%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[11pt,twoside]{article} % For LaTeX2e
%\usepackage{iclr2021_conference,times}

\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-6cm}
\setlength{\textheight}{\paperheight}
\addtolength{\textheight}{-4cm}
\addtolength{\textheight}{-1.1\headheight}
\addtolength{\textheight}{-\headsep}
\addtolength{\textheight}{-\footskip}
\setlength{\oddsidemargin}{0.5cm}
\setlength{\evensidemargin}{0.5cm}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
%\input{math_commands.tex}
\usepackage{eqnarray,amsmath, bm}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
       % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% \usepackage{subfigure}
\usepackage{epsf}
\usepackage{epsfig}
\usepackage{fancyhdr}
\usepackage{graphics}
\usepackage{graphicx}
% \usepackage{epstopdf}
\usepackage{psfrag}
\usepackage{fullpage}
\usepackage{pdfpages}
\usepackage{colortbl}
\usepackage{tcolorbox}
% \usepackage[table]{xcolor}
\usepackage{tabularx}
\usepackage{enumitem}

\usepackage{tikz}
\definecolor{cyan}{cmyk}{.3,0,0,0}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.95}


\usepackage{url}% for url's in bib
% for theorem hyperlink colors
\usepackage[colorlinks,linkcolor=magenta,citecolor=blue, pagebackref=true]{hyperref}
% \renewcommand*{\backref}[1]{\ifx#1\relax \else Page #1 \fi}
\renewcommand*{\backrefalt}[4]{%
    \ifcase #1 \footnotesize{(Not cited.)}%
    \or        \footnotesize{(Cited on page~#2.)}%
    \else      \footnotesize{(Cited on pages~#2.)}%
    \fi}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
%\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{color}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb,bbm}
\usepackage{caption}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{textcomp}
\usepackage{siunitx}
\usepackage{wrapfig}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{multicol}% colors
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\strongconvex}{\mu}
\newcommand{\smooth}{L_1}
\newcommand{\smoothprior}{L_2}
\newcommand{\subgaussian}{\sigma}
\newcommand{\barFn}{\bar{F}_n}


\newtheorem*{remark}{Remark}


\newtheorem{assumption}{Assumption}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setlength{\textwidth}{\paperwidth}
%\addtolength{\textwidth}{-6cm}
%\setlength{\textheight}{\paperheight}
%\addtolength{\textheight}{-4cm}
%\addtolength{\textheight}{-1.1\headheight}
%\addtolength{\textheight}{-\headsep}
%\addtolength{\textheight}{-\footskip}
%\setlength{\oddsidemargin}{0.5cm}
%\setlength{\evensidemargin}{0.5cm}

% baselinestretch trick to save some space
 %   \renewcommand{\baselinestretch}{0.99}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}%opening
%\newtheorem{assumption}{Assumption}
%\newtheorem{conjecture}{Conjecture}
\newenvironment{assumptionprime}[1]
  {\renewcommand{\theassumption}{\ref{#1}$'$}%
   \addtocounter{assumption}{-1}%
   \begin{assumption}}
  {\end{assumption}}


%\def\argmin{\textnormal{arg} \min}
\def\st{\textnormal{s.t.}}
\def\sgn{\texttt{sign}}
\newcommand{\todo}[1]{\textcolor{red}{[Todo: #1]}}
%\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}




\newcommand{\widgraph}[2]{\includegraphics[keepaspectratio,width=#1]{#2}}

\newcommand{\notiff}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{"}$\hidewidth\cr$\iff$}}}}


% Attempt to make hyperref and algorithmic work together better:






\input{macro_commands}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2021}
% \DeclareMathOperator*{\argmax}{arg\,max}  % in your preamble
% \DeclareMathOperator*{\argmin}{arg\,min}  % in your preamble 
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{BoMb-OT: On Batch of Mini-batches Optimal Transport}

\begin{document}

\begin{center}

{\bf{\LARGE{On Zero-Initialized Attention: Optimal Prompt and Gating Factor Estimation}}}
  
\vspace*{.2in}
{\large{
\begin{tabular}{ccccc}
Nghiem T. Diep$^{\star1,2,3}$ &
Huy Nguyen$^{\star4}$ & 
Chau Nguyen$^{\star5}$ & 
Minh Le$^{5}$ & 
Duy M. H. Nguyen$^{1,6,7}$ 
\vspace{0.2em} \\
& Daniel Sonntag$^{1,8}$ 
& Mathias Niepert$^{6,7}$ 
& Nhat Ho$^{4}$
\end{tabular}
}}

\vspace*{.2in}

\begin{tabular}{ccc}
$^1$German Research Center for Artificial Intelligence (DFKI) \\
$^2$University of Science, VNU-HCM, Ho Chi Minh City, Vietnam \\
$^3$ Viet Nam National University, Ho Chi Minh City, Vietnam \\
$^4$The University of Texas at Austin$^{\dagger}$\\
$^5$Movian AI, Vietnam$^{\diamond}$ \\
$^6$Max Planck Research School for Intelligent Systems (IMPRS-IS) \\
$^7$University of Stuttgart \\
$^8$Oldenburg University
\end{tabular}

\vspace*{.1in}
\today

\vspace*{.2in}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\blfootnote{$^\star$ Equal contribution}

\begin{abstract}
The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique for LLaMA models, leveraging zero-initialized attention to stabilize training and enhance performance. However, despite its empirical success, the theoretical foundations of zero-initialized attention remain largely unexplored. In this paper, we provide a rigorous theoretical analysis, establishing a connection between zero-initialized attention and mixture-of-expert models. We prove that both linear and non-linear prompts, along with gating functions, can be optimally estimated, with non-linear prompts offering greater flexibility for future applications. Empirically, we validate our findings on the open LLM benchmarks, demonstrating that non-linear prompts outperform linear ones. Notably, even with limited training data, both prompt types consistently surpass vanilla attention, highlighting the robustness and adaptability of zero-initialized attention.
\end{abstract}
\end{center}



\vspace{-0.2in}
\section{Introduction}
\vspace{-0.05in}
% \minh{anh Duy, could you help me write the opening paragraph to introduce the LLaMA model?}
Large Language Models (LLMs) have revolutionized natural language processing \cite{radford2019language,raffel2020exploring,anil2023palm,minaee2024large}, demonstrating remarkable understanding and generative capabilities across various tasks. While proprietary models like ChatGPT \cite{OpenAI2025a} and GPT-4 \cite{openai2024gpt} set new records in instruction-following performance \cite{peng2023instruction}, their closed-source nature and high development costs limit accessibility. To address this, several efforts have explored fine-tuning open-source LLMs, e.g., LLaMA \cite{touvron2023llama}, on large-scale instruction datasets where small human-annotated samples are expanded into massive training corpora using self-instruct methods \cite{taori2023stanford}. This approach has enabled models like LLaMA to achieve instruction-following performance comparable to GPT-3.5 and, with further advancements, shows promise in approaching GPT-4’s capabilities. However, full fine-tuning of large-scale LLMs remains computationally intensive, highlighting the need for more efficient adaptation techniques to unlock their full potential across diverse downstream applications.

% Although LLaMA demonstrates a high level of generalization, fine-tuning remains essential for optimizing its performance on new user datasets and tasks. 
% To overcome this obstacle, researchers have increasingly focused on parameter-efficient fine-tuning techniques (PEFT), which update only a small fraction of parameters while leaving most of the backbone frozen, thus facilitating effective adaptation~\cite{houlsby2019parameter, lester2021power, hu2021lora}. Among these approaches, LLaMA-Adapter~\cite{zhang2024llama} has emerged as a PEFT method specifically designed to transform LLaMA into an instruction-following model, achieving strong performance across multiple benchmarks. In addition to adaptation prompts, a key innovation of LLaMA-Adapter is the introduction of zero-initialized attention with zero gating, which allows for the incremental incorporation of new instructional prompts while preserving the model’s existing knowledge. Despite its empirical success, however, the theoretical underpinnings of zero-initialized attention remain underexplored, limiting our deeper understanding of its mechanism and potential.

To address this challenge, researchers have increasingly turned to parameter-efficient fine-tuning (PEFT) techniques, which update only a small subset of parameters while keeping most of the backbone frozen, enabling efficient and scalable adaptation~\cite{houlsby2019parameter, lester2021power, hu2021lora}. Among these approaches, the LLaMA-Adapter~\cite{zhang2024llama} has emerged as a specialized PEFT method for transforming LLaMA into an instruction-following model, demonstrating strong performance across multiple benchmarks. Beyond adaptation prompts, its key innovation lies in the \textit{introduction of zero-initialized attention with zero gating}, which facilitates the seamless integration of new instructional prompts while preserving the model’s pre-existing knowledge. However, despite its empirical success, the theoretical foundations of zero-initialized attention remain largely unexplored, limiting a deeper understanding of its underlying mechanisms and potential for further advancements.

In this work, we conduct a rigorous theoretical and empirical investigation into zero-initialized attention, showing that it is not merely an engineering trick. Building on recent findings \cite{le2024mixture, le2024revisiting} that draw connections between the attention mechanism \cite{vaswani2017attention} and the mixture of experts (MoE) model \cite{Jacob_Jordan-1991, jordan1994hierarchical, shazeer2017outrageously}, we investigate how zero-initialized attention in the LLaMA-Adapter~\cite{zhang2024llama} can be interpreted within this framework. Specifically, we show that \textit{zero-initialized attention can be viewed as a special formulation of the MoE model} and that under linear prompt settings, both the prompts and the gating factor can be optimally estimated, offering significant statistical benefits. Furthermore, we \textit{extend this analysis to scenarios in which the trainable prompts are nonlinear (\, e.g., implemented via an MLP)} and prove that the optimal estimation of the prompts and gating parameters remains achievable, thus enhancing the flexibility for future applications.

Our empirical evaluations further substantiate these theoretical findings. Across multiple datasets in the open LLM benchmark \cite{beeching2023openllm}, zero-initialized attention consistently demonstrates better performance than random-initialized attention. Additionally, non-linear prompts exhibit improved results compared to their linear counterparts, thereby offering the potential to boost the LLaMA-Adapter’s capabilities. Notably, on various small rates of limited training data, both linear and non-linear prompts combined with zero-initialized attention substantially outperform standard attention, reinforcing our theoretical findings and demonstrating the robustness of the proposed approach.

\textbf{Contribution.} Our contributions can be summarized as follows: 
\textbf{i.} We develop a theoretical framework that examines the connection between zero-initialized attention and MoE models.
\textbf{ii.} We demonstrate the statistical advantages of zero-initialized attention over conventional attention, enabling optimal estimation of prompt parameters and gating factors.
\textbf{iii.} We extend our analysis to accommodate non-linear prompts, offering increased flexibility for a broader range of applications.
\textbf{iv.} Finally, extensive experiments on multiple question-answering datasets validate our theoretical insights, highlighting the robustness and significance of our findings.


\textbf{Organization.} 
% The remainder of this paper is structured as follows: Section~\ref{section:related_work} discusses related work. In Section~\ref{section:zero_attn_moe}, we explore the connection between zero-initialized attention and the mixture of experts model. Our theoretical results are presented in Section~\ref{sec:section_4} with an extended result for the non-linear setting. In Section~\ref{sec:nonlinear_prompts} we summarize the main algorithm given the non-linear one and followed by empirical experiments in Section~\ref{section:experiment}. Finally, Section~\ref{section:conclusion} concludes the paper with a discussion of limitations and directions for future research. Detailed proofs and additional experimental information are provided in the Appendix.
The remainder of this paper is organized as follows: Section~\ref{section:related_work} reviews related work. In Section~\ref{section:zero_attn_moe}, we establish the connection between zero-initialized attention and the mixture-of-experts model. Section~\ref{sec:section_4} presents our theoretical results. In Section~\ref{sec:nonlinear_prompts}, we extend the analysis for the non-linear setting and outline the main algorithm for the non-linear case, followed by empirical evaluations in Section~\ref{section:experiment}. Finally, Section~\ref{section:conclusion} discusses limitations and future research directions. Detailed proofs and additional experimental results are provided in the Appendix.

\textbf{Notation.} For any $n\in\mathbb{N}$, $[n]$ is $\{1,2,\ldots,n\}$ . For any set $S$, $|S|$  is denoted as its cardinality. For any $u:=(u_1,u_2,\ldots,u_d) \in \mathbb{R}^{d}$ and $\alpha:=(\alpha_1,\alpha_2,\ldots,\alpha_d)\in\mathbb{N}^d$, we let $u^{\alpha}=u_{1}^{\alpha_{1}}u_{2}^{\alpha_{2}}\ldots u_{d}^{\alpha_{d}}$, $|u|:=u_1+u_2+\ldots+u_d$ and $\alpha!:=\alpha_{1}!\alpha_{2}!\ldots \alpha_{d}!$, while $\|u\|$ is $2$-norm value of $u$. Lastly, for any two positive sequences $\{a_n\}_{n\geq 1}$ and $\{b_n\}_{n\geq 1}$, we write $a_n = \mathcal{O}(b_n)$ or $a_{n} \lesssim b_{n}$ if $a_n \leq C b_n$ for all $ n\in\mathbb{N}$, where $C > 0$ is some constant. Finally, $a_{n} = \mathcal{O}_{P}(b_{n})$ means $a_{n}/b_{n}$ is stochastically bounded. 

\vspace{-0.5 em}
\section{Related Work} \label{section:related_work}

\textbf{Parameter Efficient Fine-Tuning for LLMs.} Large foundational models have demonstrated remarkable generalization capabilities across a wide range of tasks \cite{devlin2018bert, brown2020language, touvron2023llama1, touvron2023llama2}. 
However, as model sizes grow exponentially, fine-tuning all parameters in a large-scale model becomes increasingly impractical. In contrast, PEFT techniques~\cite{hu2021lora,karimi2021compacter,gao2024clip,houlsby2019parameter,li2024graphadapter,li2021prefix,zhou2022learning,mangrulkar2022peft} have emerged as a promising strategy to adapt these models to downstream tasks while freezing most of the backbone’s parameters. These approaches can be categorized into three directions, namely (i) \textit{low-rank decomposition} like LoRA \cite{hu2021lora} and its advanced versions \cite{karimi2021compacter,zhang2023adalora}; (ii) \textit{adapters} which insert
lightweight adaptation modules into each block of the transformer and have been applied across
numerous domains \cite{gao2024clip,houlsby2019parameter,li2024graphadapter}; and (iii) \textit{prompt tuning} where trainable tokens are appended to the input embeddings \cite{lester2021power,zhou2022learning} or at intermediate layers, i.e., pre-fixed tuning \cite{liu2021p,li2021prefix,shi2023dept}. 

Unlike the aforementioned PEFT methods, the LLaMA-Adapter \cite{zhang2024llama} is specifically designed to enhance instruction-following capabilities, where the model learns to generate contextually relevant responses based on natural language instructions. This is done by introducing a concept of zero-initialized attention to integrating new instructional prompts while preserving the model’s existing knowledge.  Through this mechanism, the algorithm starts with minimal impact and prevents training instability and catastrophic forgetting by selectively activating relevant information while allowing the model to incorporate instructions incrementally. In this work, we investigate comprehensive theoretical and empirical investigation into zero-initialized attention, demonstrating that it is more than just an engineering trick and uncovering its fundamental properties and advantages.
 
% For instance, prompt tuning \cite{lester2021power, li2021prefix} incorporates learnable prompt tokens into the input to serve as task-specific instructions that guide the pre-trained model. LoRA~\cite{hu2021lora} approximates weight updates with low-rank matrices, which are then added back to the backbone parameters. Adapters~\cite{houlsby2019parameter} integrate lightweight modules into each transformer block to enable task-specific adaptations.

% \minh{anh Duy, we might need a short paragraph for LLM specifically}

\textbf{Mixture of Experts (MoE) in PEFT.} 
Recent research has explored MoE in PEFT to enhance the adaptability of large pre-trained models while minimizing computational costs. MoE-based approaches, such as Switch Transformers \cite{fedus2022switch} leveraged sparse activation of expert networks to achieve efficient scaling. In the context of PEFT, techniques like AdapterDrop \cite{ruckle2020adapterdrop} and LoRA \cite{hu2021lora} have been combined with MoE to dynamically allocate resources to task-specific experts, reducing the number of trainable parameters \cite{li2024mixlora,chen2024llava,chen2023adamv}. These works demonstrate that MoE can significantly enhance parameter efficiency without compromising performance, making it a promising direction for fine-tuning large-scale models.

In another line of research, the MoE framework has been leveraged in \cite{le2024mixture,le2024revisiting} to investigate the convergence behavior of learnable prompt vectors in the context of prefix tuning method, which are attached to the key and value matrices of self-attention mechanism to learn downstream tasks. 
In particular, by showing rigorously that each row of an attention head can be represented as an MoE, they demonstrate  theoretically and empirically that the prompt convergence will be significantly accelerated if there exists a shared structure among the prompt vectors. 
However, although the zero-initialized attention has been widely used as an PEFT method, its theoretical understanding has remained missing in the literature. To close this gap, we provide a comprehensive study on the convergence of prompt vectors within the zero-initialized attention by establishing a connection between this model and MoE in Section~\ref{section:zero_attn_moe}. Our theory indicates that linear prompts and non-linear prompts share the same convergence behavior and can be both optimally estimated. On the empirical side, we observe that the non-linear prompts in zero-initalized attention work favorably compared to linear prompts in several benchmark datasets.  

Additional discussion on related work of the theory of mixture of experts is in Appendix~\ref{sec:add_related_works}.

% Although MoEs have been widely used to scale up large models, their theoretical foundations remain under active development. For example,  focused on input-free gating Gaussian MoEs and showed that under maximum likelihood estimation, the experts’ convergence rates depend on the algebraic independence of the expert functions. Next, \cite{nguyen2023demystifying} established convergence rates for both density and parameter estimation in Softmax gating Gaussian MoEs, linking these rates to the solvability of polynomial systems under Voronoi-based loss functions. More recently, \cite{nguyen2024least, nguyen2024sigmoid} employed least squares estimation to identify conditions under which expert functions are identifiable. Under these conditions, the resulting estimation rates improve substantially.

\section{Zero-initialized Attention meets Mixture of Experts} \label{section:zero_attn_moe}

% {\color{blue} Duc Minh updates the definition of zero-initialized attention and its connection to MoE.}


\textbf{Zero-initialized attention.} We revisit the zero-initialized attention formulation introduced by~\cite{zhang2024llama}. Let $\mathbf{X}_{l} = \left[ \xbm_{l, 1}, \dots, \xbm_{l, N} \right]^\top \in \RR^{N \times d}$ denote the input tokens at the $l$-th layer of LLaMA’s transformer model, where $N$ is the input sequence length, and $d$ represents the feature dimensionality. Similarly, let $\mathbf{P}_l = \left[ \pbm_{l, 1}, \dots, \pbm_{l, L} \right]^\top \in \RR^{L \times d}$ denote the learnable adaptation prompt used to fine-tune the model. This prompt is concatenated with $\mathbf{X}_{l}$ along the token dimension, serving as a prefix, where $L$ denotes the prompt length. For simplicity, we omit the subscript $l$ in subsequent notations.

Suppose the model is generating the $(N + 1)$-th word based on $\left( \mathbf{P}, \mathbf{X} \right)$ at the $l$-th layer. We denote the corresponding $(N + 1)$-th word token as $\tbm \in \RR^{d}$. Within the attention mechanism, linear projection layers are applied to the input tokens, transforming them into queries, keys, and values, defined as follows:
\begin{align}
    \Qbm &= \tbm^\top {W^Q} \in \RR^{1 \times d_k}, \\ 
    \Kbm &= \left[ \Pbf^\top, \Xbf^\top, \tbm \right]^\top W^K \in \RR^{(L + N + 1) \times d_k}, \\ 
    \Vbm &= \left[ \Pbf^\top, \Xbf^\top, \tbm \right]^\top W^V \in \RR^{(L + N + 1) \times d_v},
\end{align}
where $W^Q \in \RR^{d \times d_k}$, $W^K \in \RR^{d \times d_k}$, and $W^V \in \RR^{d \times d_v}$ are pre-trained projection matrices. The attention scores computed between $\Qbm$ and $\Kbm$ before applying the softmax function are given by:
\begin{align}
\label{eq:normal_att}
    \Sbm 
    = \frac{\Qbm \Kbm^\top}{\sqrt{d_k}} 
    = \left[ \Sbm_P, \ \Sbm_X \right] \in \RR^{1 \times (L + N + 1)},
\end{align}
where $\Sbm_P \in \RR^{1 \times L}$ and $\Sbm_X \in \RR^{1 \times (N + 1)}$ denote the attention scores of $L$ adaption prompts and $N + 1$ word tokens, respectively. Rather than applying the softmax function directly, \cite{zhang2024llama} propose an alternative approach. They suggest computing the softmax independently over the two components $\Sbm_P$ and $\Sbm_X$, and incorporating a learnable gating factor $\alpha \in \RR$ as follows:
\begin{align}
    \Sbm_g = \left[ 
    \softmax(\Sbm_P) \cdot \tanh(\alpha), \
    \softmax(\Sbm_X)
    \right]. \label{eq:zero_attn_weights}
\end{align}
This approach aims to decouple the knowledge contained within the pre-trained model and the adaptation prompts, thereby preserving the pre-trained model's original knowledge. The activation function $\tanh(\cdot)$ is used to regulate the scale of $\alpha$ within the range $[-1, 1]$. Consequently, the output of the zero-initialized attention mechanism can be expressed as:
\begin{align}
    \ybf = \Sbm_g \cdot \Vbm \in \RR^{d_v}. \label{eq:zero_attn_output}
\end{align}
During training, only the prompt parameters $\Pbf$ are optimized, while all other parameters of the pre-trained model remain frozen. Next, we examine how zero-initialized attention can be interpreted through the lens of the mixture of experts framework.


\textbf{Connection to mixture of experts.} Recent studies \cite{le2024mixture, le2024revisiting} have revealed a notable connection between the attention mechanisms and the mixture of experts (MoE) architectures~\cite{Jacob_Jordan-1991, jordan1994hierarchical}, showing that attention can be seen as a form of MoE. This perspective allows us to examine zero-initialized attention through the MoE framework, offering a valuable approach for analyzing its components.  

Specifically, let $\Xbm = \left[ \xbm_1^\top, \dots, \xbm_N^\top, \tbm^\top \right]^\top \in \RR^{(N + 1)d}$, which is the concatenation of input tokens. For $i \in [N + 1]$, define $E_i \in \RR^{d \times (N + 1)d}$ such that $E_i \Xbm = \xbm_i$ for $i = 1, \dots, N$, and $E_{N + 1} \Xbm = \tbm$. We then introduce a set of $L + N + 1$ experts $f_j: \RR^{(N + 1)d} \rightarrow \RR^{\dv}$, defined as:
\begin{align}
    &f_{j}(\Xbm) = {W^V}^\top E_j \Xbm = {W^V}^\top \xbm_j  ,\ j \in [N] \\
    &f_{N + 1}(\Xbm) = {W^V}^\top E_{N + 1} \Xbm = {W^V}^\top \tbm \\
    &f_{N + 1 + j'}(\Xbm) = {W^V}^\top \pbm_{j'}, \ j' \in [L].
\end{align}
Based on equation~\eqref{eq:zero_attn_weights}, the weights $G_j: \RR^{(N + 1)d} \rightarrow \RR$ associated with each expert are defined as follows:
\begin{align*}
    &G_{j}(\Xbm) =  
        \frac{\exp(\frac{\Xbm^\top E_{N + 1}^\top W^Q {W^K}^\top E_j \Xbm}{\sqrt{d_k}})}
        {\sum_{k = 1}^{N + 1} \exp(\frac{\Xbm^\top E_{N + 1}^\top W^Q {W^K}^\top E_k \Xbm}{\sqrt{d_k}})}, \\
    &G_{N + 1 + j'}(\Xbm) =  
        \frac{\exp(\frac{\Xbm^\top E_{N + 1}^\top W^Q {W^K}^\top \pbm_{j'}}{\sqrt{d_k}})}
        {\sum_{k' = 1}^L \exp(\frac{\Xbm^\top E_{N + 1}^\top W^Q {W^K}^\top \pbm_{k'}}{\sqrt{d_k}})},
\end{align*}
for $j' \in [L]$ and $j \in [N + 1]$. With these formulations, the output of zero-initialized attention, as described in equation~\eqref{eq:zero_attn_output}, can be expressed as:
\begin{align}
    \ybf = \sum_{j = 1}^{N + 1} G_{j}(\Xbm) \cdot f_{j}(\Xbm) + \tanh(\alpha) \times \left(
    \sum_{j' = 1}^L   G_{N + 1 + j'}(\Xbm) \cdot f_{N + 1 + j'}(\Xbm) \right) . \label{eq:connection_moe}
\end{align}
From this formulation, zero-initialized attention can be interpreted as a specialized form of a mixture of experts model. The set of experts $f_{1}, \dots, f_{N + 1}$, along with their associated weight functions, are pre-trained and require no additional training, as their parameters are encoded within the pre-trained transformer model, representing existing knowledge. In contrast, the prompt experts $f_{N + 2}, \dots, f_{N + 1 + L}$ and their weight functions work in conjunction with the pre-trained experts, effectively integrating newly acquired information into the model through learnable prompts. Viewing zero-initialized attention through this lens as a specialized mixture of experts model provides the foundation for further theoretical analysis, as demonstrated in the next section.


\vspace{-0.5 em}
\section{Optimal Prompt and Gating Factor Estimation}
\label{sec:section_4}
In this section, we provide a theoretical analysis for the prompt and gating factor estimation by leveraging the connection between the zero-initialized attention and the mixture of experts in equation~\eqref{eq:connection_moe}. By viewing the output of zero-initialized attention to be generated from a regression framework in Section~\ref{sec:linear_prompts}, we show that the linear prompts and gating factor can be optimally estimated in terms of the sample complexity. In Section~\ref{sec:nonlinear_prompts}, we extend these analyses to non-linear prompts and prove that the optimal rates are still maintained for estimating both non-linear prompts and gating factors. The theoretical benefits under the nonlinear prompt settings offer great flexibility in improving the practical performance of the zero-initialized attention, which is investigated extensively with several benchmark datasets and tasks in Section~\ref{section:experiment}.
\subsection{Analytics for Linear Prompts}
\label{sec:linear_prompts}
We first consider the original setting of zero-initialized attention when the prompts are linear.

\textbf{Problem setting.}  Assume that $(\Xbm_1,Y_1),(\Xbm_2,Y_2),\ldots,$\\$(\Xbm_n,Y_n)\in\mathbb{R}^{d} \times\mathbb{R}^{d'}$ are i.i.d. samples of size $n$ generated from the following regression model:
\allowdisplaybreaks
\begin{align}
    Y_i=f_{G_*, 
    \alpha_{*}}(\Xbm_i)+\varepsilon_i, \quad i=1,2,\ldots,n, 
    \label{eq:regression_model}
\end{align}
where the variables $\varepsilon_1,\ldots,\varepsilon_n$ are independent Gaussian noise satisfying $\bbE[{\varepsilon_{i}}|\Xbm_i] = 0$ and $\var(\varepsilon_{i}|\Xbm_i) = \sigma^2 I_{d'}$ for all $i \in [n]$. Additionally, we assume that $\Xbm_{1}, \Xbm_{2}, \ldots, \Xbm_{n}$ are i.i.d. samples from some probability distribution $\mu$.
The regression function $f_{G_{*}, \alpha_{*}}(\cdot)$ in equation~(\ref{eq:regression_model}) takes the form of the MoE model with $N$ pre-trained experts and $L$ unknown experts, which is given by:
\allowdisplaybreaks
\begin{align}
    &f_{G_{*}, \alpha_{*}}(\Xbm) := \sum_{j=1}^{N} \frac{\exp(\Xbm^{\top} \bar{A}^0_j\Xbm+\bar{a}^0_j)}{\sum_{k = 1}^{N}\exp(\Xbm^{\top}\bar{A}^0_{k}\Xbm+\bar{a}^0_{k})}\cdot h(\Xbm,\bar{\eta}^0_j) \nonumber \\
    &\hspace{-0.6em}+ \tanh(\alpha_{*})\cdot\sum_{j' = 1}^{L} \frac{\exp((\bar{B}\prompt_{*,j'})^{\top}\Xbm+\bar{b}_{*,j'})}{\sum_{k' = 1}^{L} \exp((\bar{B}\prompt_{*,k'})^{\top}\Xbm+\bar{b}_{*,k'})}\cdot \bar{C}\prompt_{*,j'}, 
    % &\times\left(\sum_{j' = 1}^{L} \frac{\exp((\bar{B}\prompt_{*,j'})^{\top}\Xbm+\bar{b}_{*,j'})}{\sum_{k' = 1}^{L} \exp((\bar{B}\prompt_{*,k'})^{\top}\Xbm+\bar{b}_{*,k'})}\cdot \bar{C}\prompt_{*,j'}\right),
    \label{eq:true_regression_function}
\end{align}
where $G_{*} := \sum_{j' = 1}^{L} \exp(\bar{b}_{*,j'}) \delta_{\prompt_{*,j'}}$ denotes a true but unknown \emph{mixing measure}, which is a weighted sum of Dirac measures $\delta$, associated with unknown biases and prompts $(\bar{b}_{*,j'},\prompt_{*,j'})_{j'=1}^{L}$ in the parameter space $\Theta\subset\mathbb{R} \times\mathbb{R}^d$. Furthermore, the \emph{gating factor} $\alpha^{*}$ is unknown and belongs to the parameter space $\Omega \subset \mathbb{R}$. In the model~\eqref{eq:true_regression_function}, the matrices $\bar{A}^0_j$, the expert parameters $\bar{\eta}^0_j$, and the bias parameters $\bar{a}^0_j$ are known for all $j \in [N]$. Finally, $\bar{B} \in \mathbb{R}^{d \times d}$ and $\bar{C} \in \mathbb{R}^{d' \times d}$ are given and they play the role of pre-trained projection matrices in the context of zero-initialized attention. 

\noindent
\textbf{Least-square estimator.} We can estimate the unknown prompts and gating factor in the regression model~\eqref{eq:regression_model} via estimating the mixing measure $G_*$ using least-square method as follows:
\begin{align}
    \label{eq:least_squared_estimator_overspecified}
    (\widehat{G}_n, \widehat{\alpha}_{n}) :=\argmin_{G\in\mathcal{G}_{L'}(\Theta), \alpha \in \Omega}\sum_{i=1}^{n}\|Y_i-f_{G, \alpha}(\Xbm_i)\|^2,
\end{align}
where $\mathcal{G}_{L'}(\Theta):=\{G=\sum_{i=1}^{\ell}\exp(\bar{b}_{i})\delta_{\prompt_{i}}:1\leq \ell\leq L', \  (b_{i},\prompt_{i})\in\Theta\}$ denotes the set of all mixing measures with at most $L'$ prompts. For practical purpose, the number of chosen prompts $L'$ is generally larger than the number of true prompts $L$, i.e., $L' \geq L$, to guarantee that the estimated prompts and gating factor from the least-square method converge to the true prompts and gating factor.

\noindent
\textbf{Convergence rate of regression estimator.} We first show that the regression estimator $f_{\widehat{G}_n,\widehat{\alpha}_n}$ can still estimate the true regression function $f_{{G}_*,\alpha_*}$ at the standard parametric rate in terms of the sample size $n$ though we overspecify the number of prompts, i.e., $L' > L$.
\begin{proposition}
    \label{theorem:regression_estimation}
     The convergence rate of the regression estimator $f_{\widehat{G}_n,\widehat{\alpha}_n}(\cdot)$ to the true regression function $f_{{G}_*,\alpha_*}(\cdot)$ under the $L^2(\mu)$ norm is of parametric order, that is,
    \begin{align}
        \label{eq:model_bound}
        \normf{f_{\widehat{G}_n, \widehat{\alpha}_{n}}-f_{G_*, \alpha_{*}}}=\mathcal{O}_{P}(\sqrt{\log(n)/n}).
    \end{align}
\end{proposition}
Proof of Proposition~\ref{theorem:regression_estimation} is in Appendix~\ref{appendix:regression_estimation}. Given the above convergence rate of the regression estimator, we aim to construct a loss function among parameters, denoted by $\mathcal{D}(\widehat{G}_n,G_*)$, such that $\normf{f_{\widehat{G}_n, \widehat{\alpha}_{n}}-f_{G_*, \alpha_{*}}}\gtrsim [\mathcal{D}(\widehat{G}_n,G_*)+|\widehat{\alpha}_n-\alpha_*|]$. Then, this lower bound together with the bound~\eqref{eq:model_bound} will lead to our desired prompt convergence rates. To this end, we will build a loss function based on the concept of Voronoi cells as in \cite{manole22refined}.
% \subsection{Prompt and Gating Estimation under the Exact-specified Prompt Setting}
% \label{subsec:exact_specified}
% \textbf{Voronoi loss function.}
% \begin{theorem}
%     \label{theorem:zero_initialized_exactspecified}
%     Assume that $L' = L$, i.e., the number of prompts is known. Given the least square estimator $(\widehat{G}_n, \widehat{\alpha}_{n})$ defined in equation~\eqref{eq:least_squared_estimator_overspecified}, we have that
%     \begin{align*}
%         \mathcal{D}(\widehat{G}_n, G_*)=\mathcal{O}_{P}(\sqrt{\log(n)/n}), \quad \quad |\widehat{\alpha}_{n} - \alpha_{*}| = \mathcal{O}_{P}(\sqrt{\log(n)/n}).
%     \end{align*}
% \end{theorem}
% \subsection{Prompt and Gating Estimation under the Over-specified Prompt Setting}
% \label{subsec:over_specified}

\textbf{Voronoi loss function.} For a mixing measure $G\in\mathcal{G}_{L'}(\Theta)$, we distribute its atoms across the Voronoi cells $\{\mathcal{C}_j\equiv
    \mathcal{C}_j(G),j\in[L]\}$ generated by the atoms of $G_*$, where
\begin{align*}
    \hspace{-0.5em}\mathcal{C}_j:=
    % \mathcal{A}_j(G):=
    \left\{
    i\in[L']:
    \| \prompt_i-\prompt_{*,j} \|
    \leq
    \| \prompt_i-\prompt_{*,\ell} \|,
    \forall \ell\neq j
    \right\}.
\end{align*}
Then, the Voronoi loss function is given by
\begin{align}
    &\mathcal{D}(G,G_*):=\sum_{j'=1}^{L}\Big|\sum_{i\in\mathcal{C}_{j'}}\exp(b_{i})-\exp(b_{*,j'})\Big| \nonumber \\
    &+\sum_{j'\in[L]:|\mathcal{C}_{j'}|=1}\sum_{i\in\mathcal{C}_{j'}}\exp(b_{i})\|\Delta \prompt_{ij'}\| \nonumber\\
    \label{eq:voronoi_loss}
    &+\sum_{j'\in[L]:|\mathcal{C}_{j'}|>1}\sum_{i\in\mathcal{C}_{j'}}\exp(b_{i})\|\Delta \prompt_{ij'}\|^{2}, 
\end{align}
where $\Delta\prompt_{ij'}=\prompt_{i}-\prompt_{*,j'}$ for all $i \in \mathcal{C}_{j'}$ and $j'\in[L]$. Given the above loss function, we are now ready to capture the convergence behavior of linear prompts in Theorem~\ref{theorem:zero_initialized_overspecified}.
\begin{theorem}
    \label{theorem:zero_initialized_overspecified}
    Assume that $L' > L$, i.e., the number of prompts is unknown and is overspecified by $L'$ prompts. Then, the least square estimator $(\widehat{G}_n, \widehat{\alpha}_{n})$ defined in equation~\eqref{eq:least_squared_estimator_overspecified} satisfies
    \begin{align*}
        \mathcal{D}(\widehat{G}_n, G_*)=\mathcal{O}_{P}(\sqrt{\log(n)/n}), \\ |\widehat{\alpha}_{n} - \alpha_{*}| = \mathcal{O}_{P}(\sqrt{\log(n)/n}).
    \end{align*}
\end{theorem}
Proof of Theorem~\ref{theorem:zero_initialized_overspecified} is in Appendix~\ref{appendix:zero_initialized_overspecified}. Putting the first bound and the formulation of the Voronoi loss function $\mathcal{D}$ together, we observe that the convergence rates of estimating linear prompts range from the order $\mathcal{O}_P([\log(n)/n]^{\frac{1}{2}})$ to $\mathcal{O}_P([\log(n)/n]^{\frac{1}{4}})$, which are optimal. Therefore, we need a polynomial number of data, either $\mathcal{O}(\epsilon^{-2})$ or $\mathcal{O}(\epsilon^{-4})$, to approximate the linear prompts with a given error $\epsilon$.

\vspace{-0.5 em}
\section{LLaMA-Adapter with Non-Linear Prompts}
\vspace{-0.1in}
\begin{figure}[!hbt]
\vspace{-0.1in}
\vskip 0.2in
    \centering % Centers the figure within the wrapfigure
    \includegraphics[width=0.45\textwidth]{Arxiv/figures/zero_init_v3.png}
    % \vspace{-0.3in}
    \vspace{-0.1in}
    \caption{LLaMA-Adapter with non-linear prompt structures. Trainable prompts are integrated into the final layers of the LLaMA model, where a zero-gating mechanism modulates the added prompts. This approach enables progressive learning of instructional knowledge while keeping the remaining model parameters frozen.}
    \vspace{-0.1in}
    \label{fig:non_linear_llama}
\end{figure}

\label{sec:nonlinear_prompts}
\subsection{Theoretical Benefits of Non-Linear Prompts} 
While the original zero-initialization approach considered only linear prompts~\cite{zhang2024llama}, most current prompt-based techniques commonly reparameterize the prompt parameters with an MLP rather than optimizing them directly, in order to enhance training stability~\cite{li2021prefix, liu2021p,le2024revisiting}. To increase both the flexibility and practical relevance of our results, we extend our analysis to zero-initialized attention equipped with non-linear prompts.


\textbf{Problem setting.} Suppose that the data $(\Xbm_1,Y_1), (\Xbm_2,Y_2),\ldots,(\Xbm_n,Y_n)\in\mathbb{R}^{d} \times\mathbb{R}^{d'}$ are i.i.d. samples of size $n$ generated from the model:
\begin{align}
    Y_i=g_{G_*, 
    \alpha_{*}}(\Xbm_i)+\varepsilon_i, \quad i=1,2,\ldots,n.
    \label{eq:regression_model_nonlinear}
\end{align}
Here, we impose the same assumptions on the noise variables $\varepsilon_i$ and the input $\Xbm_i$ as in Section~\ref{sec:linear_prompts}.
Nevertheless, the regression function $g_{G_{*}, \alpha_{*}}(\cdot)$ in equation~(\ref{eq:regression_model_nonlinear}) now takes the form of a prefix MoE model with $N$ pre-trained experts and $L$ unknown experts,
\begin{align}
    &g_{G_{*}, \alpha_{*}}(\Xbm) := \sum_{j=1}^{N} \frac{\exp(\Xbm^{\top} \bar{A}^0_j\Xbm+\bar{a}^0_j)}{\sum_{k = 1}^{N}\exp(\Xbm^{\top}\bar{A}^0_{k}\Xbm+\bar{a}^0_{k})}\cdot h(\Xbm,\bar{\eta}^0_j) \nonumber \\
    &+\tanh(\alpha_{*}) \nonumber\\
    &\hspace{-0.6em}\times\left(\sum_{j' = 1}^{L} \frac{\exp((\bar{B}\sigma(\prompt_{*,j'}))^{\top}\Xbm+\bar{b}_{*,j'})}{\sum_{k' = 1}^{L} \exp((\bar{B}\sigma(\prompt_{*,k'}))^{\top}\Xbm+\bar{b}_{*,k'})}\cdot \bar{C}\sigma(\prompt_{*,j'})\right), \label{eq:true_regression_function_nonlinear}
\end{align}
where $\sigma:\mathbb{R}^{d}\to\mathbb{R}^{d'}$ is some activation function applied element-wise to the prompts $\prompt_{*,j'}$. According to the change of the regression function, the least-square estimator of the true mixing measure $G_*$ under this setting becomes
\begin{align}
    \label{eq:least_squared_estimator_overspecified_nonlinear}
    (\widetilde{G}_n, \widetilde{\alpha}_{n}) :=\argmin_{G\in\mathcal{G}_{L'}(\Theta), \alpha \in \Omega}\sum_{i=1}^{n}\Big(Y_i-g_{G, \alpha}(\Xbm_i)\Big)^2.
\end{align}
In the following proposition, we will illustrate that the parametric convergence rate of the regression function estimator still holds true under the setting of non-linear prompts.
\begin{proposition}
    \label{theorem:regression_estimation_nonlinear}
     The convergence rate of the model estimation $g_{\widetilde{G}_n,\widetilde{\alpha}_n}(\cdot)$ to the true model $g_{{G}_*,\alpha_*}(\cdot)$ under the $L^2(\mu)$ norm is parametric on the sample size, that is,
    \begin{align}
        \label{eq:model_bound_nonlinear}
        \normf{g_{\widetilde{G}_n, \widetilde{\alpha}_{n}}-g_{G_*, \alpha_{*}}}=\mathcal{O}_{P}(\sqrt{\log(n)/n}).
    \end{align}
\end{proposition}
Similar to Section~\ref{sec:linear_prompts}, by utilizing the Voronoi loss function $\mathcal{D}$ defined in Eq.(\ref{eq:voronoi_loss}), we are able to investigate the convergence behavior of non-linear prompts in the zero-initialized attention in Theorem~\ref{theorem:zero_initialized_overspecified_nonlinear} whose proof can be found in Appendix~\ref{appendix:zero_initialized_overspecified_nonlinear}. 

% \textbf{Assumptions.} We assume that the activation function $\sigma$ meet the following assumptions:

% \emph{(A.1) (Uniform Lipschitz) Let $F(\Xbm;\prompt):=\exp((B\sigma(\prompt))^{\top}\Xbm)C\sigma(\prompt)$. Then, for any $r\in\{1,2\}$, we have
%     \begin{align*}
%         \sum_{|\alpha|=r}\Bigg|\Big(\frac{\partial^{|\alpha|}F}{\partial\prompt^{\alpha}}(\Xbm;\prompt)&-\frac{\partial^{|\alpha|}F}{\partial\prompt^{\alpha}}(\Xbm;\prompt')\Big)\gamma^{\alpha}\Bigg|\\
%         &\leq C\|(\prompt-\prompt'\|^{\zeta}\|\gamma\|^{r},
%     \end{align*}
%     for any vector $\gamma\in\mathbb{R}^{d}$ and for some positive constants $\zeta$ and $C$ which are independent of $\Xbm$ and $\prompt,\prompt'$. Here, $\alpha\in\mathbb{N}^{d}$.}

% \emph{(A.2) (Injective) If there exist parameters $\prompt$ and $\prompt'$ such that $\sigma(\prompt)=\sigma(\prompt')$, then we obtain that $\prompt=\prompt'$. }
\begin{theorem}    \label{theorem:zero_initialized_overspecified_nonlinear}
    Assume that the activation function $\sigma$ satisfies the Assumptions (A.1)-(A.2) specified in Appendix~\ref{appendix:zero_initialized_overspecified_nonlinear}. Then, the least square estimator $(\widetilde{G}_n, \widetilde{\alpha}_{n})$ defined in equation~\eqref{eq:least_squared_estimator_overspecified_nonlinear} satisfies
    \begin{align*}
        \mathcal{D}(\widetilde{G}_n, G_*)=\mathcal{O}_{P}(\sqrt{\log(n)/n}), \\ |\widetilde{\alpha}_{n} - \alpha_{*}| = \mathcal{O}_{P}(\sqrt{\log(n)/n}).
    \end{align*}
\end{theorem}
It can be seen that the convergence of prompt parameters under this setting behaves analogously to that in Theorem~\ref{theorem:zero_initialized_overspecified}. In particular, the prompt parameters $\prompt_{*,j}$ still admit the estimation rates of order $\mathcal{O}_P([\log(n)/n]^{\frac{1}{2}})$ or $\mathcal{O}_P([\log(n)/n]^{\frac{1}{4}})$. As a result, it takes a polynomial number of data ranging from $\mathcal{O}(\varepsilon^{-2})$ to $\mathcal{O}(\varepsilon^{-4})$ to achieve the prompt approximation with a given error $\epsilon$.

% \textcolor{red}{minh duy: we need to summarize a short equation for a non-linear prompt in the form of Eq.(\ref{eq:zero_attn_weights}).}
\vspace{-0.5 em}
\subsection{Non-Linear Prompt Optimization in Practice}
Motivated by the theoretical benefits of using non-linear prompts in zero-initialized attention in Eq.(\ref{eq:true_regression_function_nonlinear}), one replaces the linear prompts $\mathbf{P}$ in zero-initialized attention with non-linear prompts (Figure \ref{fig:non_linear_llama}), which is given by:
\begin{align}
    \tilde{\mathbf{P}} 
    = \sigma(\mathbf{P}) 
     \in \RR^{L \times d},
\end{align}
where $\sigma(\cdot)$ is a non-linear activation function or a lightweight MLP, in line with common practice in most current prompt-based techniques, and $\mathbf{P}$ can be a layer embedding vector or a set of embedding vectors equal to the length of prompt for each layer. For instance, we can choose $\sigma$ as a lightweight MLP with 2 layers combined with non-linear activation such as Tanh, ReLU, and Leaky-ReLU: 
% \begin{align}
% \label{eq:non_linear_p}
%     \sigma(p) = f_{2}(\sigma_1((f_{1}(p)))),
% \end{align}
% For the non-linear setting in the experiments, we define $\sigma$ is a lightweight MLP with 2 layers combined with non-linear activation such as ReLU and Leaky-ReLU: 
\begin{align}
\label{eq:non_linear_p}
    \sigma(\mathbf{P}) = f_2(\phi((f_1(\mathbf{P})))),
\end{align}
where $f_1(.)$, $f_2(.)$ are separate linear transformations, $\phi(.)$ represents the non-linear activation function, e.g., ReLU or Leaky-ReLU, and $\mathbf{P}$ is defined as a layer embedding vector. To ensure parameter efficiency and facilitate knowledge sharing across layers, this MLP can be shared among the layers that utilize the prompts.
% To be parametrically efficient and utilize the sharing of knowledge among layers, this MLP is shared among layers that use prompts.

As established in Theorem \ref{theorem:zero_initialized_overspecified_nonlinear}, this non-linear formulation retains the same estimation rates as the linear variant, thus offering greater flexibility for practical applications.

\vspace{-0.5 em}
\section{Experiments}
\label{section:experiment}
To highlight the statistical advantages of zero-initialized attention and explore the potential of non-linear prompts, we conduct a series of question-answering experiments on LLM tasks. Section~\ref{section:exp_setup} provides an overview of our experimental setup, while the main results are presented in Section~\ref{section:exp_results}. Additional details and prompt templates are included in Appendix~\ref{sec:additional_experiments}.

\vspace{-0.5 em}
\subsection{Experimental Setup} \label{section:exp_setup}

\begin{table*}
\centering
\caption{Commparison between \textit{Linear prompt} (zero-initialized mechanism) and \textit{Random-Init} prompt on 4 LLM tasks using LLaMA-7B and LLaMA-13B models.}
\vspace{0.1in}
\label{tab:linear}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} &
  \multicolumn{3}{c}{\textbf{ARC}} &
  \textbf{MMLU} &
  \textbf{Hellaswag} &
  \textbf{TruthfullQA} &
  \multirow{2}{*}{\textbf{Average}} \\ \cmidrule(lr){2-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7}
 &
  \textit{Acc (eas)} &
  \textit{Acc (cha)} &
  \textit{Acc (aver)} &
  \textit{Acc} &
  \textit{Acc} &
  \textit{Acc} &
   \\ \midrule
LLaMA-7B + zero-init    & 62.29 \textcolor{blue}{$\uparrow$\,\scriptsize{1.64}} & {43.17} \textcolor{blue}{$\uparrow$\,\scriptsize{2.47}} & 52.73 \textcolor{blue}{$\uparrow$\,\scriptsize{2.06}}  & 36.28 \textcolor{blue}{$\uparrow$\,\scriptsize{1.16}} & 76.79 \textcolor{blue}{$\uparrow$\,\scriptsize{4.17}} & 45.53 \textcolor{blue}{$\uparrow$\,\scriptsize{7.71}} & 52.83 \textcolor{blue}{$\uparrow$\,\scriptsize{3.77}} \\
LLaMA-7B + rand-init  & 60.65 & 40.7  & 50.67 & 35.12 & 72.62 & 37.82 & 49.06 \\ \midrule
LLaMA-13B + zero-init   & \underline{81.78} \textcolor{blue}{$\uparrow$\,\scriptsize{0.17}} & {64.33} \textcolor{blue}{$\uparrow$\,\scriptsize{0.42}} & \underline{73.06} \textcolor{blue}{$\uparrow$\,\scriptsize{0.3}}  & \underline{49.64} \textcolor{blue}{$\uparrow$\,\scriptsize{1.62}} & 81.21 \textcolor{blue}{$\uparrow$\,\scriptsize{0.05}} & 34.88 \textcolor{blue}{$\uparrow$\,\scriptsize{0.36}} & 59.70 \textcolor{blue}{$\uparrow$\,\scriptsize{0.58}}  \\
LLaMA-13B + rand-init & 81.61 & 63.91 & 72.76  & 48.02 & 81.16 & 34.52 & 59.12 \\ \bottomrule
\end{tabular}%
}
\vskip -0.1in
\end{table*}

\textbf{Datasets and Evaluations.} We use the Open LLM benchmarks as in \cite{beeching2023openllm}. These benchmarks evaluate the generative abilities of LLMs in four different tasks, including \texttt{(i) AI2 Reasoning Challenge} (ARC) with Easy (eas) and Challenge (cha) types \cite{clark2018think}, \texttt{(ii) HellaSwag} \cite{zellers2019hellaswag}, \texttt{(iii) MMLU} \cite{hendrycks2020measuring}, and \texttt{(iv) TruthfulQA} \cite{lin2021truthfulqa}. All these tasks evaluate the model through multiple-choice questions, where ARC and MMLU test the LLM's knowledge, HellaSwag tests the model's ability to finish sentences, and TruthfulQA measures whether an LLM is truthful in generating answers to given questions. 

We follow the experimental setup of LLaMA-Adapter \cite{zhang2024llama} by fine-tuning LLaMA on the Alpaca dataset \cite{taori2023stanford}. The model performance is evaluated on the test set by conducting a zero-shot evaluation for ARC, MMLU, and TruthfulQA while using a 10-shot setting for HellaSwag. Here,  $n$ -shot refers to incorporating  $n$ instruction-following samples into the prompt question.

\textbf{Architectures Training.} We employ  experiments on two LLaMA versions, LLaMA-7B with 32 Transformer layers and LLaMA-13B with 40 Transformer layers \cite{touvron2023llama1, touvron2023llama2}. The models are trained with $4$ A100-GPUs for $5$ epochs. The training configuration includes a warmup period of $2$ epochs, a total batch size of $64$, a learning rate of $0.009$, and a weight decay of $0.02$. With LLaMA-7B, we use a prompt with length $L = 10$ and integrate adaptation prompts into the last $K = 30$ layers. On LLaMA-13B, we use $ L= 10$ and insert prompts at the last $K=38$ layers.

\textbf{Baselines.} To access the effectiveness of zero-initialized attention and demonstrate the potential of integrating it with the proposed non-linear prompt, we conduct experiments using the following configurations: (1)\texttt{Linear prompt}: i.e., the default setting as LLaMA-Adapter~\cite{zhang2024llama}, where prompt vectors $\mathbf{P}$ are zero-initialized and used directly; (2) \texttt{Non-Linear prompt}: use zero-initialized mechanism and apply a nonlinear MLP on input prompts $\mathbf{P}$ to generate prompt vectors $ \tilde{\mathbf{P}} = \sigma(\mathbf{P})$ and the MLP layers are shared among layers; (3) \texttt{Random-Init prompt}: use the input prompts with conventional randomly-initialization, i.e., forms in Eq.(\ref{eq:normal_att}) rather than zero-initialized mechanism; (4) \texttt{Finetuning \& low-rank decomposition}: comparing with LLaMA model is fully fine-tuned on Alpaca dataset \cite{taori2023stanford} where all model's parameters are updated. Additionally, we also benchmark against \texttt{LoRA}~\cite{hu2021lora} a PEFT method using trainable rank decomposition matrices into each network weights.

% \texttt{Non-Linear prompt}: use zero-initialized mechanism and apply a nonlinear MLP on input $P$ to generate prompt vectors, and this MLP is shared among layers which use prompts; Linear prompt: the default setting from \cite{zhang2024llama}, where prompt vectors are initialized and used directly instead of being fed to a non-linear function; 

% Random-Init prompt: use the conventional attention instead of attention with zero-initialized mechanism. We also compare zero-initialized mechanism with fine-tuning techniques: Finetune: fully fine-tuning on alpaca dataset \cite{taori2023stanford} and update all model's parameters; LoRA: fine-tuning the model with LoRA technique on alpaca dataset.
\vspace{-0.5 em}
\subsection{Main Results}
\label{section:exp_results}
\textbf{I. Zero-initialized attention is essential in prompt-tuning, enhancing both \textit{robustness} and \textit{effectiveness} compared to random-initialized attention.} We begin by investigating the impact of zero-initialized prompt-tuning on LLaMA-7B and LLaMA-13B using the \texttt{linear prompt} setting, comparing its performance against conventional random-initialization strategies.

% As shown in Table \ref{tab:linear}, the zero initial mechanism significantly enhances the performance of LLaMA-Adapter compared to the conventional attention mechanism. For instance, with LLaMA-7B on Hellaswag and TruthfulQA, Linear prompt exceeds Random-Init prompt by $4.17\%$ and $9.71\%$, respectively. All other datasets and LLaMA-13B backbone also have the same trend, which clarify the effectiveness of zero-init attention in improving prefix-tuning on LLMs (LLaMA-Adapter). The findings of this experiment align with our theoretical results in Section \ref{sec:section_4}, which demonstrate the effectiveness of zero-initialized attention in enhancing the robustness and sample efficiency in parameter estimation

As shown in Table \ref{tab:linear}, the zero-initialized mechanism enhances stability in LLaMA-13B and significantly boosts the performance of LLaMA-Adapter when using LLaMA-7B, compared to conventional random-initialization strategies. For instance, with LLaMA-7B on Hellaswag and TruthfulQA, the Linear prompt surpasses the Random-Init prompt by $4.17\%$ and $7.71\%$, respectively. The varying impact of zero-initialization between LLaMA-7B and LLaMA-13B can be attributed to differences in model capacity and expressiveness. LLaMA-13B, with its larger parameter space, naturally generalizes better even with randomly initialized prompts, reducing the relative advantage of zero-initialization. In contrast, LLaMA-7B, with its lower capacity, benefits more from zero-initialization, as it relies heavily on efficient adaptation mechanisms to optimize learning.

In summary, those results align with our theoretical findings in Section \ref{sec:section_4}, which demonstrate how zero-initialized attention improves both robustness and sample efficiency in parameter estimation.

\textbf{II. Non-linear prompts provide the potential to improve the performance of the zero-initialized mechanism and achieve competitive performance with full fine-tuning.} We evaluate the impact of the \texttt{non-linear prompt} setting on the LLaMA-Adapter by comparing it with the \texttt{linear prompt} setting on LLaMA-7B and LLaMA-13B. Additionally, we benchmark against a fully fine-tuned LLaMA model and a version fine-tuned using LoRA. All models are trained with the Alpaca dataset.

% To further assess the impact of prompt structure in zero-initialized mechanism, we also compare the performance of Linear prompt where $\sigma$ is identity functions and Non-Linear prompt where $\sigma$ is a shared non-linear MLP. 

 Tables \ref{tab:non_linear} and \ref{tab:param} presents our results with the following observations: 1) The \texttt{non-linear prompt} consistently matches or outperforms the \texttt{linear prompt} across both LLaMA-7B and LLaMA-13B, with performance gains ranging from 1–2\%. Notably, on the MMLU and TruthfulQA datasets with LLaMA-13B, the non-linear prompt achieves scores of $51.32$ and $38.92$, respectively, compared to $49.64$ and $34.88$ for the linear prompt, demonstrating its effectiveness. Table \ref{tab:param} also shows that the non-linear prompt does not significantly increase training time compared to the linear version;
 2) When compared to fully fine-tuned and LoRA-based versions of LLaMA-7B, our approach using a non-linear prompt achieves performance nearly on par with full fine-tuning while surpassing LoRA by an average accuracy margin of 0.64\%. Notably, our method achieves this efficiency while updating only 2.6M parameters, compared to 4.2M for LoRA and 7B for full fine-tuning, highlighting its parameter efficiency. Additionally, with LLaMA-13B, the non-linear prompt updates just 3.3M parameters yet achieves an average score of 61.67—outperforming full fine-tuning and LoRA by 8.28\% and 9.03\%, respectively.

 
In summary, these findings validate our theoretical observations and highlight the effectiveness of combining non-linear prompts with zero-initialized attention and improving prompt-tuning performance in LLMs.
 % the Non-Linear prompt performs slightly better than Linear prompt in most cases. For instance, in LLaMA7B setting, the Non-Linear prompt outperforms Linear prompt on ARC and MMLU dataset by $0.7\%$ and $1.41\%$, respectively, and exceed by $0.45\%$ in average. In LLaMA-13B setting, the Non-Linear outperforms Linear in all datasets. Compared to fully fine-tuning in LLaMA-7B setting, the Non-Linear prompt performs comparably in average accuracy, while surpasses LoRA alpaca setting by $0.64\%$ in average accuracy. 
 


\begin{table*}
\centering
\caption{Comparison of \texttt{Non-Linear prompt}, \texttt{Linear prompt}, and various fine-tuning methods. \textbf{Params} denote the total number of parameters updated during the fine-tuning process. \textbf{Bold} values indicate better scores between linear and non-linear settings.}
\vspace{0.1in}
\label{tab:non_linear}
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} &
\multirow{2}{*}{\textbf{Params}} &
  \multicolumn{3}{c}{\textbf{ARC}} &
  \textbf{MMLU} &
  \textbf{Hellaswag} &
  \textbf{TruthfullQA} &
  \multirow{2}{*}{\textbf{Average}} \\ \cmidrule(lr){3-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8}
 & &
  \textit{Acc (eas)} &
  \textit{Acc (cha)} &
  \textit{Acc (aver)} &
  \textit{Acc} &
  \textit{Acc} &
  \textit{Acc} &
   \\ \midrule 
   \rowcolor{Gray}
LLaMA-7B, Fully Fine-tuning Alpaca  & 7B  & 67.47 & 46.25 & 56.86 & 37.25 & 77.09 & 42.35 & 53.39 \\
LLaMA-7B, LoRA Alpaca     &       4.2M      & {61.91} & {42.15} & 52.03 & 34.87 & 77.53 & 46.14 & 52.64 \\
\midrule
LLaMA-7B + zero-init + linear & 1.2M     & {62.29} & {43.17} & {52.73} & {36.28} & \textbf{76.79} & \textbf{45.53} & {52.83} \\
\rowcolor{LightCyan}
LLaMA-7B + zero-init + non-linear & 2.6M & \textbf{63.51} & \textbf{45.39} & \textbf{54.45} & \textbf{36.95} & {76.67} & {45.04} & \textbf{53.28} \\ \midrule
LLaMA-13B + zero-init + linear  & 1.9M   & 81.78 & {64.33} & 73.06 & 49.64 & {81.21} & {34.88} & {59.70}  \\
\rowcolor{LightCyan}
LLaMA-13B + zero-init + non-linear & 3.3M & \textbf{82.87} & \textbf{66.55} & \textbf{74.71} & \textbf{51.32} & \textbf{81.72} & \textbf{38.92} & \textbf{61.67} \\ \bottomrule
\end{tabular}%
}
\vskip -0.1in
\end{table*}

\textbf{III. Sample Efficiency of Zero-Initialized Attention vs. Random-Initialized Attention.} 
Figures \ref{fig:sample_7b} and \ref{fig:sample_13B} provide a systematic analysis of the sample efficiency of zero-initialized attention by evaluating its performance under varying data availability. Specifically, we randomly subsample the Alpaca dataset at different fractions \{1\%, 10\%, 30\%, 50\%, 100\%\} to simulate low-data scenarios. We then fine-tune the \texttt{Non-Linear}, \texttt{Linear}, and \texttt{Random-Init} prompts on these subsets for both LLaMA-7B and LLaMA-13B and evaluate their performance on the ARC dataset. This experiment allows us to assess how well each initialization strategy adapts to limited data and whether zero-initialized attention provides a consistent advantage in sample efficiency.

% In Figures \ref{fig:sample_7b} and \ref{fig:sample_13B}, we also investigate the sample-efficiency of zero-initialized attention in a systematic way by randomly subsampling the Alpaca dataset by a fraction $\{1\%, 10\%, 30\%, 50\%, 100\%\}$. Next, we fine-tune the Non-Linear, Linear, and Random-Init prompts in these subsets for both LLaMA-7B and LLaMA-13B and measure the performance on the ARC dataset. 

% We see that the Non-Linear and Linear prompts demonstrate the effectiveness of sample efficiency in parameter estimation for prefix-tuning in LLMs compared to the Random-Init prompt (use conventional attention). For instance, in LLaMA-7B setting \ref{fig:sample_7b}, both Non-Linear and Linear outperform Random-Init in all fractions of the Alpaca training set, such as exceed Random-Init fine-tuned on $100\%$ dataset by $2.8\%$ and $0.25\%$, respectively, and exceed by $4.72\%$ and $2.16\%$ when all versions are trained on $50\%$ dataset. Non-linear also demonstrates effectiveness when it improves the performance of zero-initialized attention in all fractions. In LLaMA-13B setting \ref{fig:sample_13B}, zero-initialized attention also has the sample trend as LLaMA-7B, and the Non-Linear slightly better than Linear in most cases, except case $1\%$ when Linear exceeds Non-Linear by $1.99\%$ in accuracy. These results align with our theoretical conclusions in Section \ref{sec:section_4} on the sample efficiency in the estimation of parameters of the zero-initialized mechanism.
We observe that both Non-Linear and Linear prompts significantly enhance sample efficiency in parameter estimation for prefix-tuning in LLMs compared to the Random-Init prompt (which uses conventional attention). In the LLaMA-7B setting (Figure \ref{fig:sample_7b}), both Non-Linear and Linear prompts outperform Random-Init across all fractions of the Alpaca training set. For example, the Non-Linear prompt exceeds Random-Init by 3.77\% when trained on 100\% of the dataset, while Linear exceeds it by 2.05\%. When trained on 50\% of the dataset, Non-Linear and Linear outperform Random-Init by 4.72\% and 2.16\%, respectively. In the LLaMA-13B setting (Figure \ref{fig:sample_13B}), a similar trend is observed, with zero-initialized attention showing consistent advantages over Random-Init, and the Non-Linear prompt slightly outperforming the Linear prompt in most cases. The only exception is at 1\% training data, where Linear surpasses Non-Linear by 1.99\% in accuracy. In short, these findings corroborate our theoretical results in Section \ref{sec:section_4}, which demonstrate the sample efficiency of the zero-initialized attention mechanism in parameter estimation.

% \vspace{-0.05in}
\vspace{-0.2in}
\begin{figure}[!hbt]
\vspace{-0.1in}
\vskip 0.2in
    \centering % Centers the figure within the wrapfigure
    \includegraphics[width=0.45\textwidth]{Arxiv/figures/llama7B.png}
    \vspace{-0.15in}
    \caption{Sample efficiency comparison of three prompt-tuning initialization strategies on the ARC Dataset with LLaMA-7B.}
    \label{fig:sample_7b}
\end{figure}
\vspace{-0.1in}
\begin{figure}[!hbt]
\vspace{-0.1in}
    \centering % Centers the figure within the wrapfigure
    \includegraphics[width=0.45\textwidth]{Arxiv/figures/llama13B.png}
    % \vspace{-0.3in}
    \vspace{-0.15in}
    \caption{Sample efficiency comparison of three prompt-tuning initialization strategies on the ARC Dataset with LLaMA-13B.}
    \vspace{-0.1in}
    \label{fig:sample_13B}
\end{figure}
\vspace{-0.1in}
\begin{table}[!htb]
\centering
\caption{Efficiency Comparison. The training is conducted on 4 GPUs A100-80GB}
\vspace{0.1in}
\label{tab:param}
\resizebox{0.48\textwidth}{!}{%
\begin{tabular}{lccc}
\toprule
\textbf{Method} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Tuned \\ Params\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Storage\\ Space\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Training \\ Time\end{tabular}} \\ \cmidrule{1-1} \cmidrule{2-4}
LLaMA-7B + zero-init + linear      & 1.2M & 4.4M & 1h54' \\
LLaMA-7B + zero-init + non-linear  & 2.6M & 9.5M &    1h54'   \\
LLaMA-13B + zero-init + linear     & 1.9M & 6.9M &  3h17'    \\
LLaMA-13B + zero-init + non-linear & 3.3M & 12M  &    3h17'    \\ \bottomrule
\end{tabular}%
}
\vspace{-0.1in}
\end{table}

\section{Conclusion and Limitations}
\label{section:conclusion}
In this paper, we demonstrate that zero-initialization prompt-tuning for adapting LLMs is not just an engineering trick but can be rigorously explained through theoretical properties by drawing connections between attention mechanisms and the mixture-of-experts perspective. Based on these insights, we introduce a novel non-linear prompt-tuning approach that outperforms linear prompts in terms of both performance and robustness while achieving competitive results compared to the full fine-tuning of LLaMA on the Alpaca dataset. Our findings are validated across several question-answering tasks, tested on both LLaMA-7B and LLaMA-13B architectures, and align with our theoretical analysis in practical settings. We believe our results will encourage further exploration into initialization techniques and their potential for improving other parameter-efficient fine-tuning methods.

Our study also has some limitations that need further exploration. First, it has only been tested on unimodal LLMs, leaving the performance on multi-modal LLMs unexamined. Multi-modal tasks may require adjustments to the framework in both theoretical perspectives and practical implementations. Second, the integration of adapters and the optimal number of prompt embeddings across layers have not been systematically explored. Further empirical experiments are needed to refine these components for better efficiency and adaptability in more complex models. Finally, the current study primarily focuses on performance metrics, but the interpretability and stability of non-linear prompts in real-world deployment scenarios require further analysis. Addressing these limitations will contribute to a deeper understanding of the robustness and generalizability of PEFT for LLaMA models.







\newpage
\appendix
\begin{center}
{}\textbf{\Large{Supplement to
``On Zero-Initialized Attention: Optimal Prompt and Gating Factor Estimation''}}
\end{center}


In this supplementary material, we provide detailed proofs of the main results in Appendix~\ref{sec:proof}. Additional discussion on related works is in Appendix~\ref{sec:add_related_works} and additional experimental details are in Appendix~\ref{sec:additional_experiments}.
\section{Proofs}
\label{sec:proof}
\subsection{Proof of Theorem~\ref{theorem:zero_initialized_overspecified}}
\label{appendix:zero_initialized_overspecified}
Based on the convergence rate of $f_{\widehat{G}_{n},\widehat{\alpha}_n}$ to $f_{{G}_{*},\alpha_*}$ in Proposition~\ref{theorem:regression_estimation}, to obtain the conclusion of Theorem~\ref{theorem:zero_initialized_overspecified}, we only need to demonstrate that 
\begin{align*}
    \|f_{G,\alpha} - f_{G_{*}, \alpha_{*}}\|_{L^{2}(\mu)} \geq C \cdot \left(\mathcal{D}(G, G_{*}) + |\alpha-\alpha_*| \right)
\end{align*} 
for any $(G, \alpha) \in \mathcal{G}_{L'}(\Theta) \times \Omega$ for some universal constant $C$.
It is equivalent to proving that:
\begin{align*}
\inf_{(G, \alpha) \in \mathcal{G}_{L'}(\Theta) \times \Omega} \normf{f_{G, \alpha}-f_{G_*, \alpha_{*}}}/(\mathcal{D}(G,G_*) + |\alpha-\alpha_*|) >0.
\end{align*}
To obtain the conclusion for the above inequality, we consider two parts: (i) local part, namely, 
\begin{align*}
    \lim_{\varepsilon\to0} \inf_{(G, \alpha) \in \mathcal{G}_{L'}(\Theta) \times \Omega: \mathcal{D}(G,G_*) + |\alpha - \alpha_{*}| \leq \varepsilon} \normf{f_{G, \alpha}-f_{G_*, \alpha_{*}}}/(\mathcal{D}(G,G_*) + |\alpha-\alpha_*|) >0;
\end{align*} 
(ii) global part, namely, for any $\varepsilon > 0$
\begin{align*}
    \inf_{(G, \alpha) \in \mathcal{G}_{L'}(\Theta) \times \Omega: \mathcal{D}(G,G_*) + |\alpha - \alpha_{*}| > \varepsilon} \normf{f_{G, \alpha}-f_{G_*, \alpha_{*}}}/(\mathcal{D}(G,G_*) + |\alpha-\alpha_*|) >0;
\end{align*} 
\paragraph{Local part:} We first start with the local part, which is equivalent to demonstrating that
\begin{align*}
    \lim_{\varepsilon\to0} \inf_{(G, \alpha) \in \mathcal{G}_{L'}(\Theta) \times \Omega: \mathcal{D}(G,G_*) + |\alpha - \alpha_{*}| \leq \varepsilon} \normf{f_{G, \alpha}-f_{G_*, \alpha_{*}}}/(\mathcal{D}(G,G_*) + |\alpha-\alpha_*|) >0.
\end{align*} 
We prove the above claim by contradiction. Assume by contrary that the above claim does not hold. It indicates that we can find a sequence of mixing measures $G_{n} := \sum_{j' = 1}^{L_n} \exp(\bar{b}_{n,j'}) \delta_{\prompt_{n,j'}}$ in $\mathcal{{G}}_{L'}(\Theta)$ and a sequence of $\alpha_{n} \in \Omega$ such that when $n \to \infty$, the following limits hold:
$$\left\{\begin{matrix}
 \mathcal{D}(G_n,{G}_*) + |\alpha_{n} - \alpha_{*}| \to 0, \\
 \normf{f_{G_n, \alpha_{n}}-f_{G_*, \alpha_{*}}}/(\mathcal{D}(G_n,{G}_*) + |\alpha_n-\alpha_*|) \to 0.
\end{matrix}\right.$$
The first limit indicates that $\mathcal{D}_{n} : = \mathcal{D}(G_n,{G}_*) \to 0$ and $\alpha_{n} \to \alpha_{*}$ as $n \to \infty$.

For the simplicity of the ensuing presentation, we denote $\mathcal{C}_j^n:= \mathcal{C}_j({G}_n)$ as a Voronoi cell of ${G}_n$ induced by the $j$-th components of ${G}_*$. Without loss of generality, we assume that those Voronoi cells do not depend on the sample size, i.e., $\mathcal{C}_j = \mathcal{C}_j^n$, which is possible since our arguments are asymptotic. Therefore, we can rewrite the Voronoi loss $\mathcal{D}_{n}$ as follows:
\begin{align*}
    \mathcal{D}_{n}:=\sum_{j'=1}^{L}\Big|\sum_{i\in\mathcal{C}_{j'}}\exp(\bar{b}_{n,i})-\exp(\bar{b}_{*,j'})\Big|&+\sum_{j'\in[L]:|\mathcal{C}_{j'}|=1}\sum_{i\in\mathcal{C}_{j'}}\exp(\bar{b}_{n,i})\|\Delta \prompt_{n,ij'}\| \nonumber\\
    &+\sum_{j'\in[L]:|\mathcal{C}_{j'}|>1}\sum_{i\in\mathcal{C}_{j'}}\exp(\bar{b}_{n,i})\|\Delta \prompt_{n,ij'}\|^{2},
\end{align*}
where $\Delta\prompt_{n,ij'}=\prompt_{n,i}-\prompt_{*,j'}$ for all $i \in \mathcal{C}_{j'}$.

From the hypothesis, we have $\mathcal{D}_{n} \to 0$, which implies that $\sum_{i\in\mathcal{C}_{j}}\exp(\bar{b}_{n,i})\to\exp(\bar{b}_{*,j})$ and $\prompt_{n,i} \to \prompt_{*,j}$ for any $i \in \mathcal{C}_{j}, j \in [L]$. To establish the contradiction, our proof consists of three main steps.
\paragraph{Step 1 - Taylor expansion.} To ease the presentation, let us denote
\begin{align*}
    f_{G_n}(\Xbm)&:=\sum_{j = 1}^{L_n} \frac{\exp((\bar{B} \prompt_{n,j})^{\top}\Xbm+ \bar{b}_{n,j})}{\sum_{k = 1}^{L_n} \exp((\bar{B}\prompt_{n,k})^{\top}\Xbm+\bar{b}_{n,k})}\cdot \bar{C}\prompt_{n,j},\\
    f_{G_*}(\Xbm)&:=\sum_{j' = 1}^{L} \frac{\exp((\bar{B}\prompt_{*,j'})^{\top}\Xbm+\bar{b}_{*,j'})}{\sum_{k' = 1}^{L} \exp((\bar{B}\prompt_{*,k'})^{\top}\Xbm+\bar{b}_{*,k'})}\cdot \bar{C}\prompt_{*,j'}.
\end{align*}
We now decompose the function $\bar{Q}_{n}(\Xbm)$ as follows:
\begin{align*}
    \bar{Q}_{n}(\Xbm):&=\Big[\sum_{k' = 1}^{L} \exp((\bar{B}\prompt_{*,k'})^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot[f_{G_n,\alpha_n}(\Xbm)-f_{G_*,\alpha_*}(\Xbm)]\\
    &=\Big[\sum_{k' = 1}^{L} \exp((\bar{B}\prompt_{*,k'})^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot\Big(\tanh(\alpha_n)f_{G_n}(\Xbm)
    -\tanh(\alpha_*)f_{G_*}(\Xbm)\Big)\\
    &=\Big[\sum_{k' = 1}^{L} \exp((\bar{B}\prompt_{*,k'})^{\top}\Xbm+ \bar{b}_{*,k'})\Big]\cdot\tanh(\alpha_n)\Big[f_{G_n}(\Xbm)-f_{G_*}(\Xbm)\Big]\\
    &+\Big[\sum_{k' = 1}^{L} \exp((\bar{B}\prompt_{*,k'})^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot[\tanh(\alpha_n)-\tanh(\alpha_*)]f_{G_*}(\Xbm)\\
    &:=\bar{Q}_{n,1}(\Xbm)+\bar{Q}_{n,2}(\Xbm).
\end{align*}
For that purpose, we will decompose the two terms $\bar{Q}_{n,1}(\Xbm)$ and $\bar{Q}_{n,2}(\Xbm)$, respectively.\\

\noindent
\textbf{Decomposition of the function $\bar{Q}_{n,1}(\Xbm)$.} We have
\begin{align}
\bar{Q}_{n,1}(\Xbm)&=\sum_{j=1}^{L}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\exp((\bar{B}\prompt_{n,i})^{\top}\Xbm)\bar{C}\prompt_{n,i}-\exp((\bar{B}\prompt_{*,j})^{\top}\Xbm)\bar{C} \prompt_{*,j}\Big] \nonumber \\
    &-\sum_{j=1}^{L}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\exp((\bar{B}\prompt_{n,i})^{\top}\Xbm)-\exp((\bar{B}\prompt_{*,j})^{\top}\Xbm)\Big]f_{G_n}(\Xbm) \nonumber \\
    &+\sum_{j=1}^{L}\tanh(\alpha_n)\Big(\sum_{i\in\mathcal{C}_j}\exp(\bar{b}_{n,i})-\exp(\bar{b}_{*,j})\Big)\exp((\bar{B}\prompt_{*,j})^{\top}\Xbm)\Big[\bar{C}\prompt_{*,j}-f_{G_n}(\Xbm)\Big] \nonumber \\
    &:= \bar{A}_n(\Xbm)- \bar{B}_n(\Xbm)+ \bar{C}_n(\Xbm).
    \label{eq:main_equation_linear}
\end{align}
We now proceed to decompose the functions $\bar{A}_{n}(.)$ and $ \bar{B}_{n}(.)$ via Taylor expansion.
\paragraph{Decomposition of the function $\bar{A}_n(\Xbm)$.} We first define the following functions $\bar{U}(\Xbm; \prompt) : = \exp((\bar{B}\prompt)^{\top}\Xbm)$ and $\bar{V}(\prompt) = \bar{C} \prompt$. Then, we denote the product of these functions as $\bar{F}(\Xbm;\prompt)= \bar{U}(\Xbm; \prompt) \bar{V}(\prompt)$. To decompose $\bar{A}_n(\Xbm)$, we separately consider Voronoi cells with exactly one element and those with more than one element. It leads to the following decomposition of the function $\bar{A}_{n}(\Xbm)$:
\begin{align*}
    \bar{A}_n(\Xbm)&=\sum_{j:|\mathcal{C}_j|=1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\bar{F}(\Xbm;\prompt_{n,i})-\bar{F}(\Xbm;\prompt_{*,j})\Big]\\
    & + \sum_{j:|\mathcal{C}_j|>1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\bar{F}(\Xbm;\prompt_{n,i})-\bar{F}(\Xbm;\prompt_{*,j})\Big]\\
    &:= \bar{A}_{n,1}(\Xbm) + \bar{A}_{n,2}(\Xbm),
\end{align*}
where we denote $\bar{A}_{n,1}(\Xbm) = \sum_{j:|\mathcal{C}_j|=1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\bar{F}(\Xbm;\prompt_{n,i})-\bar{F}(\Xbm;\prompt_{*,j})\Big]$ and $\bar{A}_{n,2}(\Xbm) = \sum_{j:|\mathcal{C}_j|>1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\bar{F}(\Xbm;\prompt_{n,i})-\bar{F}(\Xbm;\prompt_{*,j})\Big]$. 

For the function $\bar{A}_{n,1}(\Xbm)$, for any indices $i \in \mathcal{C}_{j}$ and $j$ such that $|\mathcal{C}_{j}| = 1$, the first-order Taylor expansion entails that
\begin{align*}
    \bar{U}(\Xbm; \prompt_{n,i}) & = \bar{U}(\Xbm; \prompt_{*,j}) + \sum_{|\alpha|=1} (\Delta\prompt_{n,ij} )^{\alpha} \dfrac{\partial^{|\alpha|} \bar{U}}{\partial{\prompt^\alpha}}(\Xbm;\prompt_{*,j}) + \bar{R}_{ij,1}(\Xbm), \\
    \bar{V}(\prompt_{n,i}) & = \bar{V}(\prompt_{*,j}) + \sum_{|\alpha|=1} (\Delta \prompt_{n,ij} )^{\alpha} \dfrac{\partial^{|\alpha|} \bar{V}}{\partial{\prompt^\alpha}}(\prompt_{*,j}) + \bar{R}_{ij,2},
\end{align*}
where the terms $\bar{R}_{ij,1}(\Xbm)$ and $\bar{R}_{ij, 2}$ are Taylor remainders. 

Combining the above results leads to the following formulation of the function $\bar{A}_{n,1}(\Xbm)$:
\begin{align*}
    \bar{A}_{n,1}(\Xbm) &= \sum_{j:|\mathcal{C}_j|=1}\sum_{i\in\mathcal{C}_j} \dfrac{\tanh(\alpha_n)\exp(\bar{b}_{n,i})}{\alpha!} \sum_{|\alpha|=1} \biggr\{(\Delta\prompt_{n,ij} )^{\alpha}\dfrac{\partial^{|\alpha|} \bar{U}}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) \bar{V}(\prompt_{*,j}) \\
    &\hspace{5cm} + (\Delta\prompt_{n,ij} )^{\alpha} \dfrac{\partial^{|\alpha|} \bar{V}}{\partial{\prompt^\alpha}}(\prompt_{*,j}) \bar{U}(\Xbm;\prompt_{*,j})\biggr\} + \bar{R}_{n,1}(\Xbm)\\
    &=\sum_{j:|\mathcal{C}_j|=1}\sum_{|\alpha|=1} \biggr\{ \bar{M}_{n,j,\alpha} \dfrac{\partial^{|\alpha|} \bar{U}}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) \bar{V}(\prompt_{*,j})  + \bar{M}_{n,j,\alpha} \dfrac{\partial^{|\alpha|} \bar{V}}{\partial{\prompt^\alpha}}(\prompt_{*,j}) \bar{U}(\Xbm; \prompt_{*,j})\biggr\} + \bar{R}_{n,1}(\Xbm)
\end{align*}
where the function $\bar{R}_{n,1}(\Xbm)$ is the combination of Taylor remainders and satisfies that $\bar{R}_{n,1}(\Xbm)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$ when $n \to \infty$. Furthermore, the formulations of $\bar{M}_{n,j,\alpha}$ are as follows:
\begin{align*}
\bar{M}_{n,j,\alpha}=\sum_{i\in\mathcal{C}_j} \dfrac{\tanh(\alpha_n)\exp(\bar{b}_{n,i})}{\alpha!} (\Delta\prompt_{n,ij})^{\alpha},
\end{align*}
for any $|\alpha| = 1$.


Moving to the function $\bar{A}_{n,2}(\Xbm)$, the second-order Taylor expansions of the function $\bar{U}(\Xbm; \prompt_{n,i})$ around the function $\bar{U}(\Xbm; \prompt_{*,j})$ and the function $\bar{V}(\prompt_{n,i})$ around the function $\bar{V}(\prompt_{*,j})$ for any indices $i \in \mathcal{C}_{j}$ and $j$ such that $|\mathcal{C}_{j}| > 1$, we obtain the following formulation of the function $\bar{A}_{n,2}(\Xbm)$:
\begin{align*}
\bar{A}_{n,2}(\Xbm) & = \sum_{j:|\mathcal{C}_j|>1}\sum_{1\leq |\alpha|\leq 2} \biggr\{\bar{M}_{n,j,\alpha}\dfrac{\partial^{|\alpha|} \bar{U}}{\partial {\prompt^\alpha}}(\Xbm;\prompt_{*,j}) \bar{V}(\prompt_{*,j})  + \bar{M}_{n,j,\alpha} \dfrac{\partial^{|\alpha|} \bar{V}}{\partial{\prompt^\alpha}}(\prompt_{*,j}) \bar{U}(\Xbm;\prompt_{*,j}) \biggr\} \\
& + \sum_{j:|\mathcal{C}_j|>1}\sum_{|\alpha| = 1, |\beta| = 1} \bar{M}_{n,j,\alpha, \beta} \dfrac{\partial^{|\alpha|} \bar{U}}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) \dfrac{\partial^{|\beta|} \bar{V}}{\partial{\prompt^\beta}}(\prompt_{*,j})  + \bar{R}_{n,2}(\Xbm)
\end{align*}
where the function $\bar{R}_{n,2}(\Xbm)$ is a combination of Taylor remainders and satisfies $\bar{R}_{n,2}(\Xbm)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$ when $n \to \infty$. Furthermore, we define
\begin{align*}   \bar{M}_{n,j,\alpha} = \sum_{i\in\mathcal{C}_j} \dfrac{\tanh(\alpha_n)\exp(b_{n,i})}{\alpha!} (\Delta\prompt_{n,ij})^{\alpha},
\end{align*}
for any $|\alpha| = 2$ and
\begin{align*}
    \bar{M}_{n,j,\alpha, \beta} = \sum_{i\in\mathcal{C}_j} \dfrac{\tanh(\alpha_n)\exp(\bar{b}_{n,i})}{\alpha! \beta!} (\Delta\prompt_{n,ij})^{\alpha + \beta},  
\end{align*}
for any $|\alpha| = |\beta| = 1$. From the formulations of the functions $\bar{U}(\Xbm; \prompt)$ and $\bar{V}(\prompt)$, we obtain the following explicit forms of their partial derivatives:
\begin{align*}
    \dfrac{\partial \bar{U}}{\partial {\prompt^{(u)}}}(\Xbm;\prompt) & = \exp((\bar{B}\prompt)^{\top}\Xbm) (\bar{B} 1_{u})^{\top}\Xbm, \\
     \dfrac{\partial^{2} \bar{U}}{\partial {\prompt^{(u)}}\partial {\prompt^{(v)}}}(\Xbm;\prompt) & = \exp((\bar{B}\prompt)^{\top}\Xbm) \Xbm^{\top} (\bar{B} 1_{u})(\bar{B} 1_{v})^{\top}\Xbm, \\
     \dfrac{\partial \bar{V}}{\partial {\prompt^{(u)}}}(\prompt) & = \bar{C} 1_{u}, \\
     \dfrac{\partial^2 \bar{V}}{\partial {\prompt^{(u)}}\partial {\prompt^{(v)}}}(\prompt) & = 0.
\end{align*}
In these formulations, we use $1_{u}$ to denote the vector that its $u$-th element is 1 and its other elements are 0 for $1 \leq u \leq d$. Plugging these explicit formulations of the derivatives of the functions $\bar{U}(\Xbm; \prompt)$ and $\bar{V}(\prompt)$, we can express the functions $\bar{A}_{n, 1}(\Xbm)$ and $\bar{A}_{n,2}(\Xbm)$ as follows:
\begin{align*}
& \bar{A}_{n, 1}(\Xbm) = \sum_{j:|\mathcal{C}_{j}| = 1} \exp((\bar{B}\prompt_{*,j})^{\top}\Xbm) \big[\mathcal{L}_{1,n}(\prompt_{*,j}) + \mathcal{L}_{2,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm \bigr) + \bar{R}_{n,1}(\Xbm), \\
& \bar{A}_{n, 2}(\Xbm) = \sum_{j:|\mathcal{C}_{j}| > 1} \exp((\bar{B} \prompt_{*,j})^{\top}\Xbm) \big[\bar{\mathcal{L}}_{1,n}(\prompt_{*,j}) + \bar{\mathcal{L}}_{2,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm \\
& \hspace{8 em} + (\bar{B}^{\top} \Xbm)^{\top} \bar{\mathcal{L}}_{3,n}(\prompt_{*,j}) \bar{B}^{\top} \Xbm \big] + \bar{R}_{n,2}(\Xbm),
\end{align*}
where the functions $\mathcal{L}_{1,n}(\prompt), \mathcal{L}_{2,n}(\prompt), \bar{\mathcal{L}}_{1,n}(\prompt), \bar{\mathcal{L}}_{2,n}(\prompt)$, and $\bar{\mathcal{L}}_{3,n}(\prompt)$ are defined as follows:
\begin{align*}
    \mathcal{L}_{1,n}(\prompt) & = \sum_{u = 1}^{d} \bar{M}_{n, j, 1_{u}} \bar{C} 1_{u}, \\
    \mathcal{L}_{2,n}(\prompt) & = \sum_{u = 1}^{d} \bar{M}_{n, j, 1_{u}}  1_{u} \bar{C} \prompt, \\
    \bar{\mathcal{L}}_{1,n}(\prompt) & = \sum_{u = 1}^{d} \bar{M}_{n, j, 1_{u}} C 1_{u}, \\
    \bar{\mathcal{L}}_{2,n}(\prompt) & = \sum_{u = 1}^{d} \bar{M}_{n, j, 1_{u}} 1_{u} \bar{C} \prompt + \sum_{1 \leq u,v \leq d} \bar{M}_{n, j, 1_{v}, 1_{u}}  \bar{C} 1_{u} 1_{v} \\
     \bar{\mathcal{L}}_{3,n}(\prompt) & = \sum_{1 \leq u,v \leq d} \bar{M}_{n, j, 1_{uv}} 1_{u} 1_{v}^{\top} \bar{C} \prompt.
\end{align*}
%the partial derivatives of the function $F(\Xbm; .)$ up to the second order are given by:
%\begin{align*}
%\dfrac{\partial F}{\partial \prompt^{(u)}}(\Xbm;\prompt)&= \exp((B\sigma_1(\prompt))^{\top}\Xbm) \Big[((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\Xbm)C\sigma_2(\prompt) + C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) \Big], \\
%\dfrac{\partial^2 F}{\partial \prompt^{(u)}\partial \prompt^{(v)}}(\Xbm;\prompt)&=
%\exp((B\sigma_1(\prompt))^{\top}\Xbm) \biggr\{\Big[((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\Xbm)C\sigma_2(\prompt) + C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) \Big]((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt))^{\top}\Xbm) \\
%& + ((B \dfrac{\partial^2{\sigma_{1}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt))^{\top}\Xbm)C\sigma_2(\prompt) + ((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\Xbm)C\dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(v)}}}  (\prompt) + C \dfrac{\partial^2{\sigma_{2}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt) \biggr\}
%\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)\Big[((B\sigma_1^{''}(\prompt_{*,j}))^{\top}\Xbm^{(u)}\Xbm^{(v)})C\sigma_2(\prompt_{*,j}) +C\sigma_2^{''}(\prompt_{*,j})\\
%&+((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\Xbm^{(u)})C\sigma_2^{'}(\prompt_{*,j})+ +((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\Xbm^{(v)})C\sigma_2^{'}(\prompt_{*,j})\\
%&+\left((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\right)^2\Xbm^{(u)}\Xbm^{(v)} C\sigma_2(\prompt_{*,j})  \Big]
%\end{align*}
%for any $1 \leq u, v \leq d$. Finally, we also have
%$$M_{n,j,\alpha}=\sum_{i\in\mathcal{C}_j} \dfrac{\exp(b_{n,i})}{\alpha!} (\Delta\prompt_{n,ij})^{\alpha},$$ for any $1 \leq |\alpha| \leq 2$.
In these formulations, we denote $1_{uv}$ as the matrix that its $(u,v)$-th element is 1 and its other elements are 0 for any $1 \leq u,v \leq d$. 
\paragraph{Decomposition of the function $\bar{B}_n(\Xbm)$.}  Similar to the decomposition of the function $\bar{A}_{n}(\Xbm)$, we can decompose the function $\bar{B}_n(\Xbm)$ as follows:
\begin{align*}
    \bar{B}_n(\Xbm) &=\sum_{j:|\mathcal{C}_j|=1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\bar{U}(\Xbm;\prompt_{n,i})-\bar{U}(\Xbm;\prompt_{*,j})\Big]f_{G_n}(\Xbm) \\
    & +\sum_{j:|\mathcal{C}_j|>1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\bar{U}(\Xbm;\prompt_{n,i})-\bar{U}(\Xbm;\prompt_{*,j})\Big]f_{G_n}(\Xbm) \\
    &:= \bar{B}_{n,1}(\Xbm) + \bar{B}_{n,2}(\Xbm)
\end{align*}
where we denote $\bar{B}_{n,1}(\Xbm) = \sum_{j:|\mathcal{C}_j|=1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\bar{U}(\Xbm;\prompt_{n,i})-\bar{U}(\Xbm;\prompt_{*,j})\Big]f_{G_n}(\Xbm)$ and $\bar{B}_{n,2}(\Xbm) = \sum_{j:|\mathcal{C}_j|>1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\bar{U}(\Xbm;\prompt_{n,i})-\bar{U}(\Xbm;\prompt_{*,j})\Big]f_{G_n}(\Xbm)$. 

Similar to the Taylor expansions for the functions $\bar{A}_{n,1}(\Xbm)$ and $\bar{A}_{n,2}(\Xbm)$, by using the first-order Taylor expansion to $\bar{B}_{n,1}(\Xbm)$ and the second-order Taylor expansion to $\bar{B}_{n,2}(\Xbm)$, we obtain that
\begin{align*}
    \bar{B}_{n,1}(\Xbm)&= \sum_{j:|\mathcal{C}_j|=1}\sum_{|\alpha|=1} \bar{M}_{n,j,\alpha} \dfrac{\partial^{|\alpha|}\bar{U}}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})f_{G_n}(\Xbm)+ \bar{R}_{n,3}(\Xbm)
    \\
     \bar{B}_{n,2}(\Xbm)&=\sum_{j:|\mathcal{C}_j|=1}\sum_{1 \leq |\alpha|\leq 2} \bar{M}_{n,j,\alpha} \dfrac{\partial^{|\alpha|}\bar{U}}{\partial{\prompt^\alpha}}(\Xbm;\prompt_{*,j})f_{G_n}(\Xbm)+ \bar{R}_{n,4}(\Xbm)
\end{align*}
where the functions $\bar{R}_{n,3}(\Xbm), \bar{R}_{n,4}(\Xbm)$ are Taylor remainders. Furthermore, they satisfy that $\bar{R}_{n,3}(\Xbm)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$ and $\bar{R}_{n,4}(\Xbm)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)\to 0$ when $n \to \infty$. Given the explicit formulations of the derivatives of the functions $\bar{U}(\Xbm; \prompt)$ and $\bar{V}(\prompt)$, the functions $\bar{B}_{n,1}(\Xbm)$ and $\bar{B}_{n,2}(\Xbm)$ can be then rewritten as follows:
\begin{align*}
    \bar{B}_{n,1}(\Xbm) & = \sum_{j:|\mathcal{C}_{j}| = 1} \exp((\bar{B}\prompt_{*,j})^{\top}\Xbm) \mathcal{N}_{1,n}(\prompt_{*,j})^{\top} \Xbm f_{G_{n}}(\Xbm)+ \bar{R}_{n,3}(\Xbm), \\
    \bar{B}_{n,2}(\Xbm) & = \sum_{j:|\mathcal{C}_{j}| > 1} \exp((\bar{B}\prompt_{*,j})^{\top}\Xbm) \big[\bar{\mathcal{N}}_{1,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm \\
    & \hspace{6 em} + (\bar{B}^{\top} \Xbm)^{\top} \bar{\mathcal{N}}_{2,n}(\prompt_{*,j}) (\bar{B}^{\top} \Xbm) \big]f_{G_{n}}(\Xbm) + \bar{R}_{n,4}(\Xbm).
\end{align*}
Here, the functions $\mathcal{N}_{1,n}(\Xbm)$, $\bar{\mathcal{N}}_{1,n}(\Xbm)$, and $\bar{\mathcal{N}}_{2,n}(\Xbm)$ have the following formulations:
\begin{align*}
    \mathcal{N}_{1,n}(\prompt) & = \sum_{u = 1}^{d} \bar{M}_{n, j, 1_{u}} 1_{u}, \\
    \bar{\mathcal{N}}_{1,n}(\prompt) & = \sum_{u = 1}^{d} \bar{M}_{n, j, 1_{u}} 1_{u}, \\   
    \bar{\mathcal{N}}_{2,n}(\prompt) & = \sum_{1 \leq u,v \leq d} \bar{M}_{n, j, 1_{uv}} 1_{u}  1_{v}^{\top}.
\end{align*}
Collecting all of the above results with the decomposition of the functions $\bar{A}_{n}(\Xbm)$ and $\bar{B}_{n}(\Xbm)$, we can represent the function $\bar{Q}_{n,1}(\Xbm)$ in equation~(\ref{eq:main_equation_linear}) as follows: 
\begin{align}
    \bar{Q}_{n,1}(\Xbm)
    & = \sum_{j:|\mathcal{C}_j| = 1} \exp((\bar{B} \prompt_{*,j})^{\top}\Xbm) \big[\mathcal{L}_{1,n}'(\prompt_{*,j}) + \mathcal{L}_{2,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm \bigr) \nonumber \\
    & + \sum_{j:|\mathcal{C}_j| > 1} \exp((\bar{B} \prompt_{*,j})^{\top}\Xbm) \big[\bar{\mathcal{L}}_{1,n}'(\prompt_{*,j}) + \bar{\mathcal{L}}_{2,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm + (\bar{B}^{\top} \Xbm)^{\top} \bar{\mathcal{L}}_{3,n}(\prompt_{*,j}) \bar{B}^{\top} \Xbm \big] \nonumber \\
    & - \sum_{j:|\mathcal{C}_j| = 1} \exp((\bar{B} \prompt_{*,j})^{\top}\Xbm) \big[ \bar{M}_{n,j,0_{d}} + \mathcal{N}_{1,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm \big] f_{G_{n}}(\Xbm) \nonumber \\
    & - \sum_{j:|\mathcal{C}_j| > 1} \exp((\bar{B} \prompt_{*,j})^{\top}\Xbm) \big[ \bar{M}_{n,j,0_{d}} + \bar{\mathcal{N}}_{1,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm + (\bar{B}^{\top} \Xbm)^{\top} \bar{\mathcal{N}}_{2,n}(\prompt_{*,j}) \bar{B}^{\top} \Xbm \big]f_{G_{n}}(\Xbm) \nonumber \\
    & + \bar{R}_{n,1}(\Xbm) + \bar{R}_{n,2}(\Xbm) - \bar{R}_{n,3}(\Xbm) - \bar{R}_{n,4}(\Xbm) \label{eq:main_equation_expression_linear}
\end{align}   
%\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha}\dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) - \sum_{j=1}^{L}\sum_{0 \leq |\alpha|\leq 2} M_{n,j,\alpha}\dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm) \\
%    & \hspace{14 em} + \sum_{j=1}^{2}R_{n,j} (\Xbm) - \sum_{j=3}^{4}R_{n,j} (\Xbm) \\    &=\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \Big[\dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) - \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm)\Big] \\
%    & \hspace{14 em} + \sum_{j=1}^{2}R_{n,j} (\Xbm) - \sum_{j=3}^{4}R_{n,j} (\Xbm).
%\end{align*}
where we define $\bar{M}_{n,j,0_{d}}=\tanh(\alpha_n)\Big(\sum_{i\in\mathcal{C}_j}\exp(\bar{b}_{n,i})-\exp(\bar{b}_{*,j})\Big)$ for any index $1 \leq j \leq L$, $\mathcal{L}_{1,n}'(\prompt_{*,j}) = \mathcal{L}_{1,n}(\prompt_{*,j}) + \bar{M}_{n,j,0_{d}} \bar{C} \prompt_{*,j}$, and $\bar{\mathcal{L}}_{1,n}'(\prompt_{*,j}) = \bar{\mathcal{L}}_{1,n}(\prompt_{*,j}) + \bar{M}_{n,j,0_{d}} \bar{C}\prompt_{*,j}$.\\

\noindent
\textbf{Decomposition of the function $\bar{Q}_{n,2}(\Xbm)$.} An application of the first-order Taylor expansion leads to the following expression for the function $\bar{Q}_{n,2}(\Xbm)$: 
\begin{align}
    \bar{Q}_{n,2}(\Xbm)&=[\tanh(\alpha_n)-\tanh(\alpha_*)]\cdot \Big[\sum_{k' = 1}^{L} \exp((\bar{B}\prompt_{*,k'})^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot f_{G_*}(\Xbm)\nonumber\\
    \label{eq:main_equation_expression_linear_2}
    &=(\alpha_n-\alpha_*)[1-\tanh^2(\alpha_*)]\cdot \Big[\sum_{k' = 1}^{L} \exp((\bar{B}\prompt_{*,k'})^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot f_{G_*}(\Xbm)+\bar{R}_5(\Xbm),
\end{align}
where the function $\bar{R}_{n,5}(\Xbm)$ is Taylor remainder and satisfies that $\bar{R}_{n,5}(\Xbm)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)\to0$ as $n\to\infty$.



%From the formulations of the partial derivatives of the function $F(\Xbm;.)$ up to the second order, we have
%\begin{align*}
%   \sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) = \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) \big[L_{1,n}(\prompt_{*,j}) + L_{2,n}(\prompt_{*,j})^{\top} \Xbm + \Xbm^{\top} L_{3,n}(\prompt_{*,j}) \Xbm \big],
%\end{align*}
%where we define 
%\begin{align*}
%    L_{1,n}(\prompt) & = M_{n, j, 0_{d}} C\sigma_2(\prompt) + \sum_{u = 1}^{d} M_{n, j, 1_{u}}  C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) + \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}} C \dfrac{\partial^2{\sigma_{2}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt), \\
%    L_{2,n}(\prompt) & = \sum_{u = 1}^{d} M_{n, j, 1_{u}} (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) C\sigma_2(\prompt) + \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}}  \Big[C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt)) \\
%    & \hspace{4 em} + (B \dfrac{\partial^2{\sigma_{1}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt)) C\sigma_2(\prompt) + (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) C\dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(v)}}}  (\prompt)\Big],  \\
%    L_{3,n}(\prompt) & = \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}} (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt))^{\top} C\sigma_2(\prompt). 
%\end{align*}
%{\color{blue} Linear independence of $\sigma_{2}, etc.$ does not mean $L_{1,n}(p^{*}) \neq 0$. }
%Here, we denote $1_{u}$ is the vector that its $u$-th element is 1 while its other elements are 0 for any $1 \leq u \leq d$. Furthermore, $1_{uv}$ is the matrix that its $(u,v)$-th element is 1 while its other elements are 0 for any $1 \leq u,v \leq d$. 

%Given the above equations, we obtain that
%\begin{align} \sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})/ \mathcal{D}_{2n}& \nonumber \\
%& \hspace{-15 em} = \sum_{j = 1}^{L} \big[\dfrac{L_{1,n}(\prompt_{*,j})}{\mathcal{D}_{2n}} + \dfrac{L_{2,n}(\prompt_{*,j})^{\top}}{\mathcal{D}_{2n}} \Xbm + \Xbm^{\top} \dfrac{L_{3,n}(\prompt_{*,j})}{\mathcal{D}_{2n}} \Xbm \big] \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm). \label{eq:main_equation_first}
%\end{align}
%Therefore, we can view $\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})/ D_{2n}$ as a linear combination of the linear independent functions $\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)$, $\Xbm^{(u)} \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)$, and $\Xbm^{(u)} \Xbm^{(v)} \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)$ whose elements are respectively $L_{1,n}(\prompt_{*,j})/\mathcal{D}_{2n}$, $L_{2,n}(\prompt_{*,j})^{(u)}/\mathcal{D}_{2n}$, and $L_{3,n}(\prompt_{*,j})^{(uv)}/\mathcal{D}_{2n}$ for any $1 \leq u,v \leq d$ and $1 \leq j \leq L$.

%Likewise, from the formulations of the partial derivatives of the function $E(\Xbm;.)$ up to the second order, we obtain
%\begin{align*}
%    \sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm)& \nonumber \\
%& \hspace{-11 em} =\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)  \big[N_{1,n}(\prompt_{*,j}) + N_{2,n}(\prompt_{*,j})^{\top} \Xbm + \Xbm^{\top} N_{3,n}(\prompt_{*,j}) \Xbm \big]g_{G_n}(\Xbm),
%\end{align*}
%where the formulations of $N_{1,n}(.), N_{2,n}(.)$, and $N_{3,n}(.)$ are given by:
%\begin{align*}
%    N_{1,n}(\prompt) &=M_{n,j,0_d},
%    \\
%     N_{2,n}(\prompt) &=\sum_{u = 1}^{d} M_{n, j, 1_{u}} (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) + \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}}(B \dfrac{\partial^2{\sigma_{1}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt)),
%     \\
%      N_{3,n}(\prompt) &= \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}}(B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt))^{\top}.
%\end{align*}
%Putting the above equations together leads to
%\begin{align}   \sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm) /\mathcal{D}_{2n}& \nonumber \\
%& \hspace{-21 em} = \sum_{j = 1}^{L} \big[\dfrac{N_{1,n}(\prompt_{*,j})}{\mathcal{D}_{2n}} + \dfrac{N_{2,n}(\prompt_{*,j})^{\top}}{\mathcal{D}_{2n}}\Xbm + \Xbm^{\top} \dfrac{N_{3,n}(\prompt_{*,j})}{\mathcal{D}_{2n}} \Xbm \big]\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) g_{G_n}(\Xbm).
%\label{eq:main_equation_second}
%\end{align}
%Therefore, we also can view the term $\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm)/ \mathcal{D}_{2n}$ as a linear combination of the independent functions $\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) g_{G_{n}}(\Xbm)$, $\Xbm^{(u)} \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) g_{G_{n}}(\Xbm)$, and $\Xbm^{(u)} \Xbm^{(v)} \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) g_{G_{n}}(\Xbm)$, whose elements are respectively $N_{1,n}(\prompt_{*,j})/\mathcal{D}_{2n}$, $N_{2,n}(\prompt_{*,j})^{(u)}/\mathcal{D}_{2n}$, and $N_{3,n}(\prompt_{*,j})^{(uv)}/\mathcal{D}_{2n}$ for any $1 \leq u,v \leq d$ and $1 \leq j \leq L$.

\paragraph{Step 2 - Non-vanishing coefficients.} 
 The results of equations~\eqref{eq:main_equation_expression_linear} and \eqref{eq:main_equation_expression_linear_2} indicate that $[\bar{Q}_{n}(\Xbm)-\sum_{i=1}^{5}\bar{R}_{n,i}(\Xbm)]/ (\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$ can be represented as a combination of the linearly independent functions
 $\exp((\bar{B} \prompt_{*,j})^{\top}\Xbm)$, $(\bar{B}^{\top} \Xbm)^{(u)} \exp((\bar{B} \prompt_{*,j})^{\top}\Xbm)$, $(\bar{B}^{\top} \Xbm)^{(u)} (\bar{B}^{\top} \Xbm)^{(v)} \exp((\bar{B} \prompt_{*,j})^{\top}\Xbm)$, $\exp((\bar{B} \prompt_{*,j})^{\top}\Xbm) f_{G_{n}}(\Xbm)$,

 \noindent
 $(\bar{B}^{\top} \Xbm)^{(u)} \exp((\bar{B} \prompt_{*,j})^{\top}\Xbm) f_{G_{n}}(\Xbm)$,
 $(\bar{B}^{\top} \Xbm)^{(u)} (\bar{B}^{\top} \Xbm)^{(v)} \exp((\bar{B} \prompt_{*,j})^{\top}\Xbm) f_{G_{n}}(\Xbm)$, and $[1-\tanh^2(\alpha_*)]\cdot \Big[\sum_{k' = 1}^{L} \exp((\bar{B}\prompt_{*,k'})^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot f_{G_*}(\Xbm)$ for any $1 \leq j \leq L$ and $1 \leq u, v \leq d$. 
 
 Our claim is that at least one of the coefficients of these linearly independent terms in the formulation of $[\bar{Q}_{n}(\Xbm)-\sum_{i=1}^{5}\bar{R}_{n,i}(\Xbm)]/ (\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$ does not go to 0 as $n \to \infty$. Assume by contrary that this claim does not hold, which means that all the coefficients of these linearly independent terms go to 0 as $n \to \infty$. Therefore, as $n \to \infty$ we obtain that 
 \begin{align*}
     & \mathcal{L}_{1,n}(\prompt_{*,j})/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \mathcal{L}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \bar{\mathcal{L}}_{1,n}(\prompt_{*,j})/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \\
     & \bar{\mathcal{L}}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \bar{\mathcal{L}}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \bar{\mathcal{L}}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \\
     & \bar{\mathcal{L}}_{3,n}(\prompt_{*,j})^{(uv)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \mathcal{N}_{1,n}(\prompt_{*,j})/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \bar{\mathcal{N}}_{1,n}((\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \\
     & \bar{\mathcal{N}}_{2,n}(\prompt_{*,j})^{(uv)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \bar{M}_{n,j,0_{d}}/\mathcal{D}_{n} \to 0, \ (\alpha_n-\alpha_*)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0
\end{align*} 
for any $1 \leq u,v \leq d$ and $1 \leq j \leq L$. 

As $(\alpha_n-\alpha_*)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$, we deduce that
\begin{align}
    \label{eq:alpha_converge}
    \frac{|\alpha_n-\alpha_*|}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}\to0.
\end{align}
Note that since $\alpha_n \to \alpha_{*} \neq 0$ as $n\to\infty$, we have $1/\tanh(\alpha_n)\not\to\infty$. Then, as $\bar{M}_{n,j,0_{d}}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$, it implies that
\begin{align*}
    \frac{|\sum_{i\in\mathcal{C}_j}\exp(\bar{b}_{n,i})-\exp(\bar{b}_{*,j})|}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}=\frac{1}{\tanh(\alpha_n)}\cdot\frac{|\bar{M}_{n,j,0_{d}}|}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}  \to 0,
\end{align*}
for any $1 \leq j \leq L$. By varying the index $j$ from 1 to $L$ in these limits and summing them up, we obtain that
\begin{align}
\frac{\sum_{j = 1}^{L} |\sum_{i\in\mathcal{C}_j}\exp(\bar{b}_{n,i})-\exp(\bar{b}_{*,j})|}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to 0. \label{eq:key_limits_first}
\end{align}
Now, we consider indices $j \in [L]$ such that $|\mathcal{C}_j | = 1$, i.e., the corresponding Voronoi cell has only one element. From the hypothesis, we have $\mathcal{L}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$, which leads to $\bar{M}_{n,j,1_{u}}/ (\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$. Hence, we find that
\begin{align*}
    \frac{\sum_{i \in \mathcal{C}_{j}} \exp(\bar{b}_{n,i})\|\Delta \prompt_{n,ij}\|}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}=\frac{\sum_{u = 1}^{d} |\bar{M}_{n,j,1_{u}}|}{\tanh(\alpha_n)(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to 0.
\end{align*}
That limit directly implies the following result: 
\begin{align}
    \label{eq:prompt_converge_1}
    \frac{\sum_{j: |\mathcal{C}_{j}| = 1} \sum_{i \in \mathcal{C}_{j}} \exp(\bar{b}_{n,i}) \|\Delta \prompt_{n,ij}\|}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to 0. 
\end{align}
We now move to the Voronoi cells having more than one element, namely, we consider indices $j \in [L]$ satisfying $|\mathcal{C}_{j}| > 1$. The limit $\bar{\mathcal{L}}_{3,n}(\prompt_{*,j})^{(uu)}/ (\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$ induces that 
\begin{align*}
    \frac{\sum_{i \in \mathcal{C}_{j}} \exp(\bar{b}_{n,i})\|\Delta \prompt_{n,ij}\|^2}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}= \frac{\sum_{u = 1}^{d}  \bar{\mathcal{L}}_{3,n}(\prompt_{*,j})^{(uu)}}{\tanh(\alpha_n)(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to 0. 
\end{align*}
By varying the indices $j$ in these limits over all the Voronoi cells $\mathcal{C}_{j}$ having more than one element, we find that
\begin{align}
    \label{eq:prompt_converge_2}
    \frac{\sum_{j: |\mathcal{C}_{j}| > 1} \sum_{i \in \mathcal{C}_{j}} \exp(\bar{b}_{n,i}) \|\Delta \prompt_{n,ij}\|^2}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to 0. 
\end{align}
Combining the results from equations~\eqref{eq:alpha_converge}, \eqref{eq:key_limits_first}, \eqref{eq:prompt_converge_1}, and \eqref{eq:prompt_converge_2} leads to
\begin{align*}
    1 = \frac{\mathcal{D}_{n}+|\alpha_n-\alpha_*|}{\mathcal{D}_{n}+|\alpha_n-\alpha_*|} \to 0
\end{align*}
as $n \to \infty$, which cannot hold. 
As a consequence, at least one of the coefficients of the terms in the formulations of $[Q_{n}(\Xbm)-\sum_{i=1}^{5} \bar{R}_{n,i}(\Xbm)]/ (\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$ does not go to 0 as $n \to \infty$. 

\paragraph{Step 3 - Application of the Fatou’s lemma.} We denote $m_n$ as the maximum of the absolute values of $\mathcal{L}_{1,n}'(\prompt_{*,j})/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\mathcal{L}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\bar{\mathcal{L}}_{1,n}'(\prompt_{*,j})/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\bar{\mathcal{L}}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\bar{\mathcal{L}}_{3,n}(\prompt_{*,j})^{(uv)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\mathcal{N}_{1,n}(\prompt_{*,j})/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\bar{\mathcal{N}}_{1,n}((\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\bar{\mathcal{N}}_{2,n}(\prompt_{*,j})^{(uv)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\bar{M}_{n,j,0_{d}}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, and $(\alpha_n-\alpha_*)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$ for all $1 \leq u, v \leq d$. From the result of Step 2 in the proof, we have $1/m_n \not \to \infty$ as $n \to \infty$.

Recall that $\normf{f_{G_n,\alpha_n}-f_{G_*,\alpha_*}}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$ as $n \to \infty$, which
indicates that $\normf{f_{G_n,\alpha_n}-f_{G_*,\alpha_*}}/(m_{n} (\mathcal{D}_{n}+|\alpha_n-\alpha_*|)) \to 0$. Furthermore, since the $L^2(\mu)$ norm is equivalent to the $L^1(\mu)$ norm, we have $\|f_{G_n,\alpha_n}-f_{G_*,\alpha_*}\|_{L^1(\mu)}/(m_{n} (\mathcal{D}_{n}+|\alpha_n-\alpha_*|)) \to 0$. An application of Fatou's lemma leads to
\begin{align*}
    0=\lim_{n \to \infty} \dfrac{\|f_{G_n,\alpha_n}-f_{G_*,\alpha_*}\|_{L^1(\mu)}}{m_n(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \geq  \int \liminf_{n \to \infty} \dfrac{\left\| f_{G_n,\alpha_n}(\Xbm)-f_{G_*,\alpha_*}(\Xbm)\right\|_1}{m_n(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}d\mu(\Xbm) \geq 0.
\end{align*}
It indicates that $\liminf_{n \to \infty} \dfrac{\left\| f_{G_n}(\Xbm)-f_{G_*}(\Xbm)\right\|_1}{m_n(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} = 0$ for almost surely $\Xbm$. As $n \to \infty$, we denote
\begin{align*}
    & \dfrac{\mathcal{L}_{1,n}'(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \widehat{\alpha}_{j}, \quad \dfrac{\mathcal{L}_{2,n}(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \beta_{j}, \\
    & \dfrac{\bar{\mathcal{L}}_{1,n}'(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \bar{\alpha}_{j}, \quad \dfrac{\bar{\mathcal{L}}_{2,n}(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \bar{\beta}_{j}, \quad \dfrac{\bar{\mathcal{L}}_{3,n}(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \bar{\gamma}_{j}, \\
    & \dfrac{\bar{M}_{n,j,0_{d}}}{m_n(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \tilde{\alpha}_{j}, \quad \dfrac{\mathcal{N}_{1,n}(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \tilde{\beta}_{j}, \\
    & \dfrac{\bar{\mathcal{N}}_{1,n}(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \widehat{\beta}_{j}, \quad \dfrac{\bar{\mathcal{N}}_{2,n}(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \widehat{\gamma}_{j}, \quad \dfrac{\alpha_n-\alpha_*}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}\to\tau
\end{align*}
for any $1 \leq j \leq L$. Here, from the definition of $m_{n}$, at least one coefficient among $\{\widehat{\alpha}_{j}, \beta_{j}, \tilde{\alpha}_{j}, \tilde{\beta}_{j}\}_{j: |\mathcal{C}_{j}| = 1}$, $\{\bar{\alpha}_{j}, \bar{\beta}_{j}, \bar{\gamma}_{j}, \tilde{\alpha}_{j}, \widehat{\beta}_{j}, \widehat{\gamma}_{j}\}_{j: |\mathcal{C}_{j}| > 1}$, and $\tau$ is different from 0. Then, the equation
\begin{align*}
    \liminf_{n \to \infty} \dfrac{\|Q_n(\Xbm)\|_1}{m_n(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}=\Big[\sum_{k' = 1}^{L} \exp((\bar{B}\prompt_{*,k'})^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot\liminf_{n \to \infty} \dfrac{\left\| f_{G_n,\alpha_n}(\Xbm)-f_{G_*,\alpha_*}(\Xbm)\right\|_1}{m_n(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} = 0
\end{align*}
leads to
\begin{align*}
    & \sum_{j:|\mathcal{C}_{j}| = 1} \exp((\bar{B} \prompt_{*,j})^{\top}\Xbm) (\alpha_{j} + \beta_{j}^{\top} (\bar{B}^{\top} \Xbm) \bigr) \nonumber \\
    & + \sum_{j:|\mathcal{C}_{j}| > 1} \exp((\bar{B} \prompt_{*,j})^{\top}\Xbm) \big[\bar{\alpha}_{j} + \bar{\beta}_{j}^{\top} (B^{\top} \Xbm) + (\bar{B}^{\top}\Xbm)^{\top} \bar{\gamma}_{j} (\bar{B}^{\top} \Xbm) \big] \nonumber \\
    & - \sum_{j:|\mathcal{C}_{j}| = 1} \exp((\bar{B} \prompt_{*,j})^{\top}\Xbm) (\tilde{\alpha}_{j} + \tilde{\beta}_{j}^{\top} (\bar{B}^{\top}\Xbm)) f_{G_{*}}(\Xbm) \nonumber \\
    & - \sum_{j:|\mathcal{C}_{j}| > 1} \exp((\bar{B} \prompt_{*,j})^{\top}\Xbm) \big[\tilde{\alpha}_{j} + \widehat{\beta}_{j}^{\top} (\bar{B}^{\top} \Xbm) + (\bar{B}^{\top}\Xbm)^{\top} \widehat{\gamma}_{j} \bar{B}^{\top} \Xbm \big]f_{G_{*}}(\Xbm) \\
    &+\tau[1-\tanh^2(\alpha_*)]\cdot \Big[\sum_{k' = 1}^{L} \exp((\bar{B}\prompt_{*,k'})^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot f_{G_*}(\Xbm)= \zerod
\end{align*}
for almost surely $\Xbm$. That equation only holds if and only if all the coefficients $\{\widehat{\alpha}_{j}, \beta_{j}, \tilde{\alpha}_{j}, \tilde{\beta}_{j}\}_{j: |\mathcal{C}_{j}| = 1}$, $\{\bar{\alpha}_{j}, \bar{\beta}_{j}, \bar{\gamma}_{j}, \tilde{\alpha}_{j}, \widehat{\beta}_{j}, \widehat{\gamma}_{j}\}_{j: |\mathcal{C}_{j}| > 1}$, and $\tau$ are 0, which is a contradiction. 

It indicates that we indeed have the conclusion of the local part, namely, $$\lim_{\varepsilon\to0} \inf_{G\in\mathcal{G}_{L'}(\Omega): (\mathcal{D}(G,G_*)+|\alpha-\alpha_*|)\leq \varepsilon} \normf{f_{G,\alpha}-f_{G_*,\alpha_*}}/(\mathcal{D}(G,G_*)+|\alpha-\alpha_*|) >0.$$
\paragraph{Global part:} The result of the local part implies that we can find a positive constant $\varepsilon'$ such that
$$\inf_{G\in\mathcal{G}_{L'}(\Omega): (\mathcal{D}(G,G_*)+|\alpha-\alpha_*|)\leq \varepsilon'} \normf{f_{G,\alpha}-f_{G_*,\alpha_*}}/(\mathcal{D}(G,G_*)+|\alpha-\alpha_*|) >0.$$
Therefore, to obtain the conclusion of the theorem it is sufficient to prove that
$$ \inf_{G\in\mathcal{G}_{L'}(\Omega): (\mathcal{D}(G,G_*)+|\alpha-\alpha_*|)> \varepsilon'} \normf{f_{G,\alpha}-f_{G_*,\alpha_*}}/(\mathcal{D}(G,G_*)+|\alpha-\alpha_*|) >0.$$
Assume by contrary that the above claim does not hold. Then there exists a sequence of measures $G'_{n} := \sum_{j' = 1}^{L'} \exp(\bar{b}_{n,j'}) \delta_{\prompt_{n,j'}}$ in $\mathcal{G}_{L'}(\Theta)$ and $\alpha'_n\in\Omega$ such that we have
$$\left\{\begin{matrix}
 \mathcal{D}(G'_n,G_*)+|\alpha'_n-\alpha_*| > \varepsilon'\\
 \normf{f_{G'_n,\alpha'_n}-f_{G_*,\alpha_*}}/(\mathcal{D}(G'_n,G_*)+|\alpha'_n-\alpha_*|) \to 0.
\end{matrix}\right.$$
These limits indicate that $\normf{f_{G'_n,\alpha'_n}-f_{G_*,\alpha_*}} \to 0$  as $n \to \infty$.\\
Recall that the sets $\Theta$ and $\Omega$ are compact. Therefore, there exists a mixing measure $G'$ in $\mathcal{G}_{L'}(\Omega)$ such that one of $(G'_n,\alpha'_n)$'s subsequences converges to $(G',\alpha')$. Since $\mathcal{D}(G'_n,G_*)+|\alpha'_n-\alpha_*|>\varepsilon'$, we deduce that $\mathcal{D}(G',G_*)+|\alpha'-\alpha_*|>\varepsilon'$.\\
An application of the Fatou’s lemma leads to
$$0=\lim_{n \to \infty} \normf{f_{G'_n,\alpha'_n}-f_{G_*,\alpha_*}} \geq  \int \liminf_{n \to \infty} \left\| f_{G'_n,\alpha'_n}(\Xbm)-f_{G_*,\alpha_*}(\Xbm)\right\|_2^2 d\mu(\Xbm).$$
Hence, we have $f_{G',\alpha'}(
\Xbm)=f_{G_*,\alpha_*}(\Xbm)$ for $\mu-$almost surely $\Xbm$. From the identifiability property (cf. the end of this proof), we deduce that $(G',\alpha')\equiv (G_*,\alpha_*)$. It follows that $\mathcal{D}(G',G_*)+|\alpha'-\alpha_*|=0$. It contradicts to the hypothesis that $\mathcal{D}(G',G_*)+|\alpha'-\alpha_*|>\varepsilon'>0$. \\
As a consequence, the proof of the global part is completed. We obtain the conclusion of the theorem.
\paragraph{Proof for the identifiability property.} We now
demonstrate that if $f_{\bar{G}, \bar{\alpha}}(\Xbm) = f_{\bar{G}_*, \bar{\alpha}_{*}}(\Xbm)$ for almost every $\Xbm$, then we obtain that $(\bar{G}, \bar{\alpha})  \equiv  (\bar{G}_*, \bar{\alpha}_{*})$.

To ease the presentation we denote the following notations:
\begin{align*}
    \softmax_{\bar{G}}^{\text{Pretrain}}(u)&=\dfrac{\exp(u)}{\sum_{k = 1}^{N}\exp(\Xbm^{\top}\bar{A}^0_{k}\Xbm+\bar{a}^0_{k})},\\
    \softmax_{\bar{G}}^{\text{Prompt}}(u')&=\dfrac{\exp(u)}{\sum_{j'=1}^{L'}\exp((\bar{B}\prompt_{j'})^{\top}\Xbm+\bar{b}_{j'})},\\
    \softmax_{\bar{G}_*}^{\text{Pretrain}}(u_*)&=\dfrac{\exp(u_*)}{\sum_{k = 1}^{N}\exp(\Xbm^{\top}\bar{A}^0_{k}\Xbm+\bar{a}^0_{k})}, \\
    \softmax_{\bar{G}_*}^{\text{Prompt}}(u_*')&=\dfrac{\exp(u_*)}{\sum_{j'=1}^{L}\exp((\bar{B}\prompt_{*,j'})^{\top}\Xbm+\bar{b}_{*,j'})}.
\end{align*}
Here, $u$, $u'$, $u_{*}$, and $u_{*}'$ in these equations satisfy:
\begin{align*}
    u &\in \{\Xbm^{\top} \bar{A}^0_j\Xbm+ \bar{a}^0_j: j \in [N] \}, \ u' \in \{(\bar{B} \prompt_{j'})^{\top}\Xbm+ \bar{b}_{j'}: j' \in [L']\} \\
    u_* &\in \{\Xbm^{\top} \bar{A}^0_j\Xbm+\bar{a}^0_j: j \in [N]\}, \ u_*' \in \{(\bar{B} \prompt_{*,j'})^{\top}\Xbm+ \bar{b}_{*,j'}: j' \in [L]\}.
\end{align*}
The equation $f_{\bar{G}, \bar{\alpha}}(\Xbm) = f_{\bar{G}_*, \bar{\alpha}_{*}}(\Xbm)$ for almost every $\Xbm$ indicates that
\begin{align}
    & \sum_{j=1}^{N}\softmax_{\bar{G}}^{\text{Pretrain}}(\Xbm^{\top} \bar{A}^0_j\Xbm+ \bar{a}^0_j))h(\Xbm,\bar{\eta}^0_j) + \tanh(\bar{\alpha}) \sum_{j' = 1}^{L'} \softmax_{\bar{G}}^{\text{Prompt}}((\bar{B} \prompt_{j'})^{\top}\Xbm+ \bar{b}_{j'})\bar{C} \prompt_{j'}  \nonumber \\
&  = \sum_{j=1}^{N}\softmax_{\bar{G}_*}^{\text{Pretrain}}(\Xbm^{\top} \bar{A}^0_j\Xbm+\bar{a}^0_j))h(\Xbm,\bar{\eta}^0_j) + \tanh(\bar{\alpha}_{*})\sum_{j' = 1}^{L} \softmax_{{\bar{G}}_*}^{\text{Prompt}}((\bar{B} \prompt_{*,j'})^{\top} \Xbm+\bar{b}_{*,j'})\bar{C} \prompt_{*,j'}.
\label{eq:identify_proof_first}
\end{align}
The above equation only holds when $L = L'$. Furthermore, we also have that
\begin{align*}
    \{\softmax_{\bar{G}}^{\text{Prompt}}((\bar{B} \prompt_{j'})^{\top}\Xbm+\bar{b}_{j'}):j' \in [L']\} =\{\softmax_{\bar{G}_*}^{\text{Prompt}}((\bar{B} \prompt_{*,j'})^{\top}\Xbm+\bar{b}_{*,j'}):j' \in [L]\},
\end{align*}
for almost every $\Xbm$. By relabelling the indices, we can assume that
\begin{align*}  \softmax_{\bar{G}}^{\text{Prompt}}((\bar{B} \prompt_{j'})^{\top}\Xbm+\bar{b}_{j'}) =\softmax_{\bar{G}_*}^{\text{Prompt}}((\bar{B} \prompt_{*,j'})^{\top}\Xbm+\bar{b}_{*,j'}),
\end{align*}
for almost every $\Xbm$ and any $j' \in [L]$. From the translation invariant property of the softmax function, the above equations only hold when $\bar{b}_{j'}=\bar{b}_{*,j'}+ \bar{r}$ for some $\bar{r} \in \mathbb{R}$ and any $j' \in [L]$. Given these results, equation~(\ref{eq:identify_proof_first}) leads to
\begin{align}
     \tanh(\bar{\alpha}) \sum_{j = 1}^{L}\exp{(\bar{b}_{j})}\exp{((\bar{B}\prompt_{j})^{\top}\Xbm)}\bar{C}\prompt_{j} = \tanh(\bar{\alpha}_{*})\sum_{j = 1}^{L}\exp{(\bar{b}_{*,j})}\exp{((\bar{B}\prompt_{*,j})^{\top}\Xbm)} \bar{C} \prompt_{*,j},    \label{eq:identify_proof_second}
\end{align}
for almost surely $\Xbm$.


Now, we partition the set $\{1,2, \ldots, L\}$ into $m$ subsets $\bar{K}_1, \bar{K}_2,\ldots,\bar{K}_m$ where $m\leq L$, such that $\exp{(\bar{b}_{j})}=\exp{(\bar{b}_{*,j'})}$ for any $j,j'\in \bar{K}_i$ and $i \in [m]$. It is clear that $\exp{(\bar{b}_{j})}\neq \exp{(\bar{b}_{*,j'})}$ when $j, j'$ belong to different subsets $\bar{K}_i$. Collecting these results, equation~(\ref{eq:identify_proof_second}) can be rewritten as follows:
\begin{align*}
    \tanh(\bar{\alpha}) \sum_{i = 1}^{m}\sum_{j \in \bar{K}_i}\exp{(\bar{b}_{j})}\exp{((\bar{B} \prompt_{j})^{\top}\Xbm)}\bar{C} \prompt_{j} & \nonumber \\
& \hspace{-5 em} = \tanh(\bar{\alpha}_{*}) \sum_{i = 1}^{m}\sum_{j \in \bar{K}_i}\exp{(\bar{b}_{*,j})}\exp{((\bar{B} \prompt_{*,j})^{\top}\Xbm)}\bar{C} \prompt_{*,j},
\end{align*}
for almost surely $\Xbm$. Hence, we achieve that
\begin{align*}
    \{((\bar{B} \prompt_{j})^{\top}, \prompt_{j}): j \in \bar{K}_i\} = \{((\bar{B} \prompt_{*,j})^{\top}, \prompt_{*,j}): j \in \bar{K}_i\} \ \ \text{and} \ \  \tanh(\bar{\alpha}) = \tanh(\bar{\alpha}_{*}).
\end{align*}
It naturally leads to 
\begin{align*}
    \{\prompt_{j}: j \in \bar{K}_i\} = \{\prompt_{*,j}: j \in \bar{K}_i\}.
\end{align*}
Without loss of generality, $\prompt_{j}=\prompt_{*,j}$ for all $j \in \bar{K}_i$. As a consequence, we obtain that $\bar{\alpha} = \bar{\alpha}_{*}$ and 
\begin{align*}
    \sum_{i = 1}^{m}\sum_{j \in \bar{K}_i}\exp{(\bar{b}_{j})}\delta_{\prompt_{j}} = \sum_{i = 1}^{m}\sum_{j \in \bar{K}_i}\exp{(\bar{b}_{*,j})}\delta_{\prompt_{*,j}}.
\end{align*}
It is equivalent to $(\bar{G}, \bar{\alpha})  \equiv  (\bar{G}_*, \bar{\alpha}_{*})$. We achieve the conclusion of the identifiability claim.
% \subsection{Proof of Theorem~\ref{theorem:zero_initialized_overspecified}}
% \label{appendix:zero_initialized_overspecified}
% Given the parametric convergence rate of the estimated regression function $f_{\bar{G}_{n}}$ to the true regression function $f_{\bar{G}_{*}}$ in Proposition~\ref{theorem:regression_estimation_linear}, to obtain the conclusion of Theorem~\ref{theorem:pretrained_param_rate}, it is sufficient to demonstrate that $\|f_{\bar{G}} - f_{\bar{G}_{*}}\|_{L^{2}(\mu)} \geq C' \mathcal{D}_{2}(\bar{G}, \bar{G}_{*})$ for any $\bar{G} \in \bar{\mathcal{G}}_{L'}(\Omega)$ for some universal constant $C'$.
% It is equivalent to demonstrate the following inequality:
% \begin{align*}
% \inf_{\bar{G}\in\mathcal{\bar{G}}_{L'}(\Omega)} \normf{f_{G}-f_{\bar{G}_*}}/\mathcal{D}_2(G,\bar{G}_*) >0.
% \end{align*}
% We divide the proof of the above inequality into local and global parts.
% \paragraph{Local part:} We will demonstrate that
% $$\lim_{\varepsilon\to0} \inf_{\bar{G}\in\mathcal{\bar{G}}_{L'}(\Omega): \mathcal{D}_2(G,\bar{G}_*)\leq \varepsilon} \normf{f_{\bar{G}}-f_{\bar{G}_*}}/\mathcal{D}_2(\bar{G},\bar{G}_*) >0$$
% Assume by contrary that the above claim does not hold. Then, there exists a sequence of mixing measures $\bar{G}_{n} := \sum_{j' = 1}^{L'} \exp(b_{n,j'}) \delta_{\prompt_{n,j'}}$ in $\mathcal{\bar{G}}_{L'}(\Omega)$ such that as $n \to \infty$, we have
% $$\left\{\begin{matrix}
%  \mathcal{D}_{2n}:=\mathcal{D}_2(\bar{G}_n,\bar{G}_*) \to 0, \\
%  \normf{f_{\bar{G}_n}-f_{\bar{G}_*}}/\mathcal{D}_{2n} \to 0.
% \end{matrix}\right.$$
% Denote $\mathcal{C}_j^n:= \mathcal{C}_j(\bar{G}_n)$ as a Voronoi cell of $\bar{G}_n$ generated by the $j$-th components of $\bar{G}_*$. Since our arguments are asymptotic, we may assume that those Voronoi cells do not depend on the sample size, i.e., $\mathcal{C}_j = \mathcal{C}_j^n$. Thus, the Voronoi loss $\mathcal{D}_{2n}$ can be represented as
% \begin{align*}
%     \mathcal{D}_{2n}:=\sum_{j'=1}^{L}\Big|\sum_{i\in\mathcal{C}_{j'}}\exp(b_{n,i})-\exp(b_{*,j'})\Big|&+\sum_{j'\in[L]:|\mathcal{C}_{j'}|=1}\sum_{i\in\mathcal{C}_{j'}}\exp(b_{n,i})\|\Delta \prompt_{n,ij'}\| \nonumber\\
%     &+\sum_{j'\in[L]:|\mathcal{C}_{j'}|>1}\sum_{i\in\mathcal{C}_{j'}}\exp(b_{n,i})\|\Delta \prompt_{n,ij'}\|^{2},
% \end{align*}
% where $\Delta\prompt_{n,ij'}=\prompt_{n,i}-\prompt_{*,j'}$ for all $i \in \mathcal{C}_{j'}$.

% Additionally, since $\mathcal{D}_{2n} \to 0$, we have $\sum_{i\in\mathcal{C}_{j}}\exp(b_{n,i})\to\exp(b_{*,j})$ and $\prompt_{n,i} \to \prompt_{*,j}$ for any $i \in \mathcal{C}_{j}, j \in [L]$.
% Now, we divide the proof of the local part into three sub-steps as follows.
% \paragraph{Step 1 - Taylor expansion.} In this step, we would like to decompose the quantity
% $$Q_n(\Xbm):=\Big[\sum_{j = 1}^{N}\exp(\Xbm^{\top}A^0_{j}\Xbm+a^0_{j})+\sum_{j'=1}^{L}\exp((B\prompt_{*,j'})^{\top}\Xbm+b_{*,j'})\Big]\cdot[f_{\bar{G}_n}(\Xbm)-f_{\bar{G}_*}(\Xbm)],$$  as follows:
% \begin{align}
%     Q_n(\Xbm)&=\sum_{j=1}^{L}\sum_{i\in\mathcal{C}_j}\exp(b_{n,i})\Big[\exp((B\prompt_{n,i})^{\top}\Xbm)C\prompt_{n,i}-\exp((B\prompt_{*,j})^{\top}\Xbm)C \prompt_{*,j}\Big] \nonumber \\
%     &-\sum_{j=1}^{L}\sum_{i\in\mathcal{C}_j}\exp(b_{n,i})\Big[\exp((B\prompt_{n,i})^{\top}\Xbm)-\exp((B\prompt_{*,j})^{\top}\Xbm)\Big]f_{\bar{G}_n}(\Xbm) \nonumber \\
%     &+\sum_{j=1}^{L}\Big(\sum_{i\in\mathcal{C}_j}\exp(b_{n,i})-\exp(b_{*,j})\Big)\exp((B\prompt_{*,j})^{\top}\Xbm)\Big[C\prompt_{*,j}-f_{\bar{G}_n}(\Xbm)\Big] \nonumber \\
%     &:=\bar{A}_n(\Xbm)-\bar{B}_n(\Xbm)+ \bar{C}_n(\Xbm). \label{eq:main_equation_linear}
% \end{align}
% \paragraph{Decomposition of $\bar{A}_n(\Xbm)$.} To ease the ensuing presentation, we denote $E(\Xbm; \prompt) : = \exp((B\prompt)^{\top}\Xbm)$ and $H(\prompt) = C \prompt$, and $F(\Xbm;\prompt)= E(\Xbm; \prompt) H(\prompt)$. Since each Voronoi cell $\mathcal{C}_j$ possibly has more than one element, we continue to decompose $\bar{A}_n$ as follows:
% \begin{align*}
%     \bar{A}_n(\Xbm)&=\sum_{j:|\mathcal{C}_j|=1}\sum_{i\in\mathcal{C}_j}\exp(b_{n,i})\Big[F(\Xbm;\prompt_{n,i})-F(\Xbm;\prompt_{*,j})\Big]\\
%     & \hspace{7 em} + \sum_{j:|\mathcal{C}_j|>1}\sum_{i\in\mathcal{C}_j}\exp(b_{n,i})\Big[F(\Xbm;\prompt_{n,i})-F(\Xbm;\prompt_{*,j})\Big]\\
%     &:= \bar{A}_{n,1}(\Xbm) + \bar{A}_{n,2}(\Xbm).
% \end{align*}
% By means of the first-order Taylor expansion, we have
% \begin{align*}
%     E(\Xbm; \prompt_{n,i}) & = E(\Xbm; \prompt_{*,j}) + \sum_{|\alpha|=1} (\Delta\prompt_{n,ij} )^{\alpha} \dfrac{\partial^{|\alpha|}E}{\partial{\prompt^\alpha}}(\Xbm;\prompt_{*,j}) + R_{ij,1}(\Xbm), \\
%     H(\prompt_{n,i}) & = H(\prompt_{*,j}) + \sum_{|\alpha|=1} (\Delta \prompt_{n,ij} )^{\alpha} \dfrac{\partial^{|\alpha|}H}{\partial{\prompt^\alpha}}(\prompt_{*,j}) + R_{ij,2},
% \end{align*}
% for any $i \in \mathcal{C}_{j}$ and $j$ such that $|\mathcal{C}_{j}| = 1$. Here, $R_{ij,1}(\Xbm)$ and $R_{ij, 2}$ are Taylor remainders. Putting the above results together leads to
% \begin{align*}
%     \bar{A}_{n,1}(\Xbm) &= \sum_{j:|\mathcal{C}_j|=1}\sum_{i\in\mathcal{C}_j} \dfrac{\exp(b_{n,i})}{\alpha!} \sum_{|\alpha|=1} \biggr\{(\Delta\prompt_{n,ij} )^{\alpha}\dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) H(\prompt_{*,j}) \\
%     & + (\Delta\prompt_{n,ij} )^{\alpha} \dfrac{\partial^{|\alpha|}H}{\partial{\prompt^\alpha}}(\prompt_{*,j}) E(\Xbm;\prompt_{*,j})\biggr\} + \bar{R}_{n,1}(\Xbm)\\
%     &=\sum_{j:|\mathcal{C}_j|=1}\sum_{|\alpha|=1} \biggr\{ M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) H(\prompt_{*,j}) \\
%     & + M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}H}{\partial{\prompt^\alpha}}(\prompt_{*,j}) E(\Xbm; \prompt_{*,j})\biggr\} + \bar{R}_{n,1}(\Xbm)
% \end{align*}
% where the function $\bar{R}_{n,1}(\Xbm)$ satisfies $\bar{R}_{n,1}(\Xbm)/\mathcal{D}_{2n} \to 0$ when $n \to \infty$. Furthermore, the formulations of $M_{n,j,\alpha}$ are given by:
% \begin{align*}
% M_{n,j,\alpha}=\sum_{i\in\mathcal{C}_j} \dfrac{\exp(b_{n,i})}{\alpha!} (\Delta\prompt_{n,ij})^{\alpha},
% \end{align*}
% for any $|\alpha| = 1$.


% Moving to the term $\bar{A}_{n,2}(\Xbm)$, by applying the second-order Taylor expansions to $E(\Xbm; \prompt_{n,i})$ around $E(\Xbm; \prompt_{*,j})$ and $H(\prompt_{n,i})$ around $H(\prompt_{*,j})$ for any $i \in \mathcal{C}_{j}$ and $j$ such that $|\mathcal{C}_{j}| > 1$, we get that
% \begin{align*}
% \bar{A}_{n,2}(\Xbm) & = \sum_{j:|\mathcal{C}_j|>1}\sum_{1\leq |\alpha|\leq 2} \biggr\{M_{n,j,\alpha}\dfrac{\partial^{|\alpha|}E}{\partial {\prompt^\alpha}}(\Xbm;\prompt_{*,j}) H(\prompt_{*,j}) \\
% & + M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}H}{\partial{\prompt^\alpha}}(\prompt_{*,j}) E(\Xbm;\prompt_{*,j}) \biggr\} \\
% & + \sum_{|\alpha| = 1, |\beta| = 1} M_{n,j,\alpha, \beta} \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) \dfrac{\partial^{|\beta|}H}{\partial{\prompt^\beta}}(\prompt_{*,j})  + \bar{R}_{n,2}(\Xbm)
% \end{align*}
% where the function $\bar{R}_{n,2}(\Xbm)$ satisfies $\bar{R}_{n,2}(\Xbm)/\mathcal{D}_{2n} \to 0$ when $n \to \infty$. Furthermore, we define
% \begin{align*}   M_{n,j,\alpha}=\sum_{i\in\mathcal{C}_j} \dfrac{\exp(b_{n,i})}{\alpha!} (\Delta\prompt_{n,ij})^{\alpha},
% \end{align*}
% for any $|\alpha| = 2$ and
% \begin{align*}
%     M_{n,j,\alpha, \beta} = \sum_{i\in\mathcal{C}_j} \dfrac{\exp(b_{n,i})}{\alpha! \beta!} (\Delta\prompt_{n,ij})^{\alpha + \beta},  
% \end{align*}
% for any $|\alpha| = |\beta| = 1$. Direct calculation leads to the following formulations of the partial derivatives of $E(\Xbm; \prompt)$ and $H(\prompt)$:
% \begin{align*}
%     \dfrac{\partial E}{\partial {\prompt^{(u)}}}(\Xbm;\prompt) & = \exp((B\prompt)^{\top}\Xbm) (B 1_{u})^{\top}\Xbm, \\
%      \dfrac{\partial^{2} E}{\partial {\prompt^{(u)}}\partial {\prompt^{(v)}}}(\Xbm;W_{1}\prompt) & = \exp((B\prompt)^{\top}\Xbm) \Xbm^{\top} (B 1_{u})(B 1_{v})^{\top}\Xbm, \\
%      \dfrac{\partial H}{\partial {\prompt^{(u)}}}(\prompt) & = C 1_{u}, \\
%      \dfrac{\partial^2 H}{\partial {\prompt^{(u)}}\partial {\prompt^{(v)}}}(W_{2}\prompt) & = 0.
% \end{align*}
% Here, we denote $1_{u}$ is the vector that its $u$-th element is 1 while its other elements are 0 for any $1 \leq u \leq d$. Given the above formulations, we can rewrite $\bar{A}_{n, 1}(\Xbm)$ and $\bar{A}_{n,2}(\Xbm)$ as follows:
% \begin{align*}
% & \bar{A}_{n, 1}(\Xbm) = \sum_{j:|\mathcal{C}_{j}| = 1} \exp((B\prompt_{*,j})^{\top}\Xbm) \big[L_{1,n}(\prompt_{*,j}) + L_{2,n}(\prompt_{*,j})^{\top} B^{\top} \Xbm \bigr) + \bar{R}_{n,1}(\Xbm), \\
% & \bar{A}_{n, 2}(\Xbm) = \sum_{j:|\mathcal{C}_{j}| > 1} \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) \big[\bar{L}_{1,n}(\prompt_{*,j}) + \bar{L}_{2,n}(\prompt_{*,j})^{\top} B^{\top} \Xbm \\
% & \hspace{8 em} + (B^{\top} \Xbm)^{\top} \bar{L}_{3,n}(\prompt_{*,j}) B^{\top} \Xbm \big] + \bar{R}_{n,2}(\Xbm),
% \end{align*}
% where the formulations of the functions $L_{1,n}, L_{2,n}, \bar{L}_{1,n}, \bar{L}_{2,n}$, and $\bar{L}_{3,n}$ are given by:
% \begin{align*}
%     L_{1,n}(\prompt) & = \sum_{u = 1}^{d} M_{n, j, 1_{u}} C 1_{u}, \\
%     L_{2,n}(\prompt) & = \sum_{u = 1}^{d} M_{n, j, 1_{u}}  1_{u} C \prompt, \\
%     \bar{L}_{1,n}(\prompt) & = \sum_{u = 1}^{d} M_{n, j, 1_{u}} C 1_{u}, \\
%     \bar{L}_{2,n}(\prompt) & = \sum_{u = 1}^{d} M_{n, j, 1_{u}} 1_{u} C \prompt + \sum_{1 \leq u,v \leq d} M_{n, j, 1_{v}, 1_{u}}  C 1_{u} 1_{v} \\
%      \bar{L}_{3,n}(\prompt) & = \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}} 1_{u} 1_{v}^{\top} C \prompt.
% \end{align*}
% %the partial derivatives of the function $F(\Xbm; .)$ up to the second order are given by:
% %\begin{align*}
% %\dfrac{\partial F}{\partial \prompt^{(u)}}(\Xbm;\prompt)&= \exp((B\sigma_1(\prompt))^{\top}\Xbm) \Big[((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\Xbm)C\sigma_2(\prompt) + C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) \Big], \\
% %\dfrac{\partial^2 F}{\partial \prompt^{(u)}\partial \prompt^{(v)}}(\Xbm;\prompt)&=
% %\exp((B\sigma_1(\prompt))^{\top}\Xbm) \biggr\{\Big[((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\Xbm)C\sigma_2(\prompt) + C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) \Big]((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt))^{\top}\Xbm) \\
% %& + ((B \dfrac{\partial^2{\sigma_{1}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt))^{\top}\Xbm)C\sigma_2(\prompt) + ((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\Xbm)C\dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(v)}}}  (\prompt) + C \dfrac{\partial^2{\sigma_{2}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt) \biggr\}
% %\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)\Big[((B\sigma_1^{''}(\prompt_{*,j}))^{\top}\Xbm^{(u)}\Xbm^{(v)})C\sigma_2(\prompt_{*,j}) +C\sigma_2^{''}(\prompt_{*,j})\\
% %&+((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\Xbm^{(u)})C\sigma_2^{'}(\prompt_{*,j})+ +((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\Xbm^{(v)})C\sigma_2^{'}(\prompt_{*,j})\\
% %&+\left((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\right)^2\Xbm^{(u)}\Xbm^{(v)} C\sigma_2(\prompt_{*,j})  \Big]
% %\end{align*}
% %for any $1 \leq u, v \leq d$. Finally, we also have
% %$$M_{n,j,\alpha}=\sum_{i\in\mathcal{C}_j} \dfrac{\exp(b_{n,i})}{\alpha!} (\Delta\prompt_{n,ij})^{\alpha},$$ for any $1 \leq |\alpha| \leq 2$.
% Here, $1_{uv}$ is the matrix that its $(u,v)$-th element is 1 while its other elements are 0 for any $1 \leq u,v \leq d$. 
% \paragraph{Decomposition of $\bar{B}_n(\Xbm)$.}  We can rewrite $\bar{B}_n(\Xbm)$ as follows:
% \begin{align*}
%     \bar{B}_n(\Xbm) &=\sum_{j:|\mathcal{C}_j|=1}\sum_{i\in\mathcal{C}_j}\exp(b_{n,i})\Big[E(\Xbm;\prompt_{n,i})-E(\Xbm;\prompt_{*,j})\Big]f_{\bar{G}_n}(\Xbm) \\
%     & \hspace{5 em} +\sum_{j:|\mathcal{C}_j|>1}\sum_{i\in\mathcal{C}_j}\exp(b_{n,i})\Big[E(\Xbm;\prompt_{n,i})-E(\Xbm;\prompt_{*,j})\Big]f_{\bar{G}_n}(\Xbm) \\
%     &:= \bar{B}_{n,1}(\Xbm) + \bar{B}_{n,2}(\Xbm)
% \end{align*}
% By applying the first-order and second-order Taylor expansion, we get
% \begin{align*}
%     \bar{B}_{n,1}(\Xbm)&= \sum_{j:|\mathcal{C}_j|=1}\sum_{|\alpha|=1} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})f_{\bar{G}_n}(\Xbm)+ R_{n,3}(\Xbm)
%     \\
%      \bar{B}_{n,2}(\Xbm)&=\sum_{j:|\mathcal{C}_j|=1}\sum_{1 \leq |\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}E}{\partial{\prompt^\alpha}}(\Xbm;\prompt_{*,j})f_{\bar{G}_n}(\Xbm)+ R_{n,4}(\Xbm)
% \end{align*}
% where $R_{n,3}(\Xbm), R_{n,4}(\Xbm)$ is a Taylor remainder such that $R_{n,3}(\Xbm)/\mathcal{D}_{2n} \to 0$, $R_{n,4}(\Xbm)/\mathcal{D}_{2n} \to 0$ when $n \to \infty$. Therefore, we can express the functions $\bar{B}_{n,1}(\Xbm)$ and $\bar{B}_{n,2}(\Xbm)$ as follows:
% \begin{align*}
%     \bar{B}_{n,1}(\Xbm) & = \sum_{j:|\mathcal{C}_{j}| = 1} \exp((B\prompt_{*,j})^{\top}\Xbm) N_{1,n}(\prompt_{*,j})^{\top} \Xbm f_{\bar{G}_{n}}(\Xbm)+ R_{n,3}(\Xbm), \\
%     \bar{B}_{n,2}(\Xbm) & = \sum_{j:|\mathcal{C}_{j}| > 1} \exp((B\prompt_{*,j})^{\top}\Xbm) \big[\bar{N}_{1,n}(\prompt_{*,j})^{\top} B^{\top} \Xbm \\
%     & \hspace{6 em} + (B^{\top} \Xbm)^{\top} \bar{N}_{2,n}(\prompt_{*,j}) (B^{\top} \Xbm) \big]f_{\bar{G}_{n}}(\Xbm) + R_{n,4}(\Xbm),
% \end{align*}
% where the formulations of the functions $N_{1,n}$, $\bar{N}_{1,n}$, and $\bar{N}_{2,n}$ are given by:
% \begin{align*}
%     N_{1,n}(\prompt) & = \sum_{u = 1}^{d} M_{n, j, 1_{u}} 1_{u}, \\
%     \bar{N}_{1,n}(\prompt) & = \sum_{u = 1}^{d} M_{n, j, 1_{u}} 1_{u}, \\   
%     \bar{N}_{2,n}(\prompt) & = \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}} 1_{u}  1_{v}^{\top}.
% \end{align*}



% Plugging the above expressions into equation~(\ref{eq:main_equation_linear}), we can represent $Q_n(\Xbm)$ as folows: 
% \begin{align}
%     Q_n(\Xbm)&= \sum_{j:|\mathcal{C}_j| = 1} \exp((B \prompt_{*,j})^{\top}\Xbm) \big[L_{1,n}(\prompt_{*,j}) + L_{2,n}(\prompt_{*,j})^{\top} B^{\top} \Xbm \bigr) \nonumber \\
%     & + \sum_{j:|\mathcal{C}_j| > 1} \exp((B \prompt_{*,j})^{\top}\Xbm) \big[\bar{L}_{1,n}(\prompt_{*,j}) + \bar{L}_{2,n}(\prompt_{*,j})^{\top} B^{\top} \Xbm + (B^{\top} \Xbm)^{\top} \bar{L}_{3,n}(\prompt_{*,j}) B^{\top} \Xbm \big] \nonumber \\
%     & - \sum_{j:|\mathcal{C}_j| = 1} \exp((B \prompt_{*,j})^{\top}\Xbm) N_{1,n}(\prompt_{*,j})^{\top} \Xbm f_{\bar{G}_{n}}(\Xbm) \nonumber \\
%     & - \sum_{j:|\mathcal{C}_j| > 1} \exp((B \prompt_{*,j})^{\top}\Xbm) \big[ \bar{N}_{1,n}(\prompt_{*,j})^{\top} B^{\top} \Xbm + (B^{\top} \Xbm)^{\top} \bar{N}_{2,n}(\prompt_{*,j}) B^{\top} \Xbm \big]f_{\bar{G}_{n}}(\Xbm) \nonumber \\
%     & - \sum_{j = 1}^{L} M_{n,j,0_{d}} \exp((B \prompt_{*,j})^{\top}\Xbm) f_{G_{n}}(\Xbm) + \sum_{j = 1}^{L} M_{n,j,0_{d}} \exp((B \prompt_{*,j})^{\top}\Xbm) C \prompt_{*,j} \nonumber \\
%     & + \bar{R}_{n,1}(\Xbm) + \bar{R}_{n,2}(\Xbm) - R_{n,3}(\Xbm) - R_{n,4}(\Xbm) \nonumber \\
%     & = \sum_{j:|\mathcal{C}_j| = 1} \exp((B \prompt_{*,j})^{\top}\Xbm) \big[L_{1,n}'(\prompt_{*,j}) + L_{2,n}(\prompt_{*,j})^{\top} B^{\top} \Xbm \bigr) \nonumber \\
%     & + \sum_{j:|\mathcal{C}_j| > 1} \exp((B \prompt_{*,j})^{\top}\Xbm) \big[\bar{L}_{1,n}'(\prompt_{*,j}) + \bar{L}_{2,n}(\prompt_{*,j})^{\top} B^{\top} \Xbm + (B^{\top} \Xbm)^{\top} \bar{L}_{3,n}(\prompt_{*,j}) B^{\top} \Xbm \big] \nonumber \\
%     & - \sum_{j:|\mathcal{C}_j| = 1} \exp((B \prompt_{*,j})^{\top}\Xbm) \big[ M_{n,j,0_{d}} + N_{1,n}(\prompt_{*,j})^{\top} B^{\top} \Xbm \big] f_{\bar{G}_{n}}(\Xbm) \nonumber \\
%     & - \sum_{j:|\mathcal{C}_j| > 1} \exp((B \prompt_{*,j})^{\top}\Xbm) \big[ M_{n,j,0_{d}} + \bar{N}_{1,n}(\prompt_{*,j})^{\top} B^{\top} \Xbm + (B^{\top} \Xbm)^{\top} \bar{N}_{2,n}(\prompt_{*,j}) B^{\top} \Xbm \big]f_{\bar{G}_{n}}(\Xbm) \nonumber \\
%     & + \bar{R}_{n,1}(\Xbm) + \bar{R}_{n,2}(\Xbm) - R_{n,3}(\Xbm) - R_{n,4}(\Xbm) \label{eq:main_equation_expression_linear}
% \end{align}   
    
    
% %\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha}\dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) - \sum_{j=1}^{L}\sum_{0 \leq |\alpha|\leq 2} M_{n,j,\alpha}\dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm) \\
% %    & \hspace{14 em} + \sum_{j=1}^{2}R_{n,j} (\Xbm) - \sum_{j=3}^{4}R_{n,j} (\Xbm) \\    &=\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \Big[\dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) - \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm)\Big] \\
% %    & \hspace{14 em} + \sum_{j=1}^{2}R_{n,j} (\Xbm) - \sum_{j=3}^{4}R_{n,j} (\Xbm).
% %\end{align*}
% where $M_{n,j,0_{d}}=\sum_{i\in\mathcal{C}_j}\exp(b_{n,i})-\exp(b_{*,j})$ for any $j \in [L]$, $L_{1,n}'(\prompt_{*,j}) = L_{1,n}(\prompt_{*,j}) + M_{n,j,0_{d}}C\prompt_{*,j}$, and $\bar{L}_{1,n}'(\prompt_{*,j}) = \bar{L}_{1,n}(\prompt_{*,j}) + M_{n,j,0_{d}}C\prompt_{*,j}$.

% %From the formulations of the partial derivatives of the function $F(\Xbm;.)$ up to the second order, we have
% %\begin{align*}
% %   \sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) = \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) \big[L_{1,n}(\prompt_{*,j}) + L_{2,n}(\prompt_{*,j})^{\top} \Xbm + \Xbm^{\top} L_{3,n}(\prompt_{*,j}) \Xbm \big],
% %\end{align*}
% %where we define 
% %\begin{align*}
% %    L_{1,n}(\prompt) & = M_{n, j, 0_{d}} C\sigma_2(\prompt) + \sum_{u = 1}^{d} M_{n, j, 1_{u}}  C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) + \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}} C \dfrac{\partial^2{\sigma_{2}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt), \\
% %    L_{2,n}(\prompt) & = \sum_{u = 1}^{d} M_{n, j, 1_{u}} (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) C\sigma_2(\prompt) + \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}}  \Big[C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt)) \\
% %    & \hspace{4 em} + (B \dfrac{\partial^2{\sigma_{1}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt)) C\sigma_2(\prompt) + (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) C\dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(v)}}}  (\prompt)\Big],  \\
% %    L_{3,n}(\prompt) & = \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}} (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt))^{\top} C\sigma_2(\prompt). 
% %\end{align*}
% %{\color{blue} Linear independence of $\sigma_{2}, etc.$ does not mean $L_{1,n}(p^{*}) \neq 0$. }
% %Here, we denote $1_{u}$ is the vector that its $u$-th element is 1 while its other elements are 0 for any $1 \leq u \leq d$. Furthermore, $1_{uv}$ is the matrix that its $(u,v)$-th element is 1 while its other elements are 0 for any $1 \leq u,v \leq d$. 

% %Given the above equations, we obtain that
% %\begin{align} \sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})/ \mathcal{D}_{2n}& \nonumber \\
% %& \hspace{-15 em} = \sum_{j = 1}^{L} \big[\dfrac{L_{1,n}(\prompt_{*,j})}{\mathcal{D}_{2n}} + \dfrac{L_{2,n}(\prompt_{*,j})^{\top}}{\mathcal{D}_{2n}} \Xbm + \Xbm^{\top} \dfrac{L_{3,n}(\prompt_{*,j})}{\mathcal{D}_{2n}} \Xbm \big] \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm). \label{eq:main_equation_first}
% %\end{align}
% %Therefore, we can view $\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})/ D_{2n}$ as a linear combination of the linear independent functions $\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)$, $\Xbm^{(u)} \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)$, and $\Xbm^{(u)} \Xbm^{(v)} \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)$ whose elements are respectively $L_{1,n}(\prompt_{*,j})/\mathcal{D}_{2n}$, $L_{2,n}(\prompt_{*,j})^{(u)}/\mathcal{D}_{2n}$, and $L_{3,n}(\prompt_{*,j})^{(uv)}/\mathcal{D}_{2n}$ for any $1 \leq u,v \leq d$ and $1 \leq j \leq L$.

% %Likewise, from the formulations of the partial derivatives of the function $E(\Xbm;.)$ up to the second order, we obtain
% %\begin{align*}
% %    \sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm)& \nonumber \\
% %& \hspace{-11 em} =\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)  \big[N_{1,n}(\prompt_{*,j}) + N_{2,n}(\prompt_{*,j})^{\top} \Xbm + \Xbm^{\top} N_{3,n}(\prompt_{*,j}) \Xbm \big]g_{G_n}(\Xbm),
% %\end{align*}
% %where the formulations of $N_{1,n}(.), N_{2,n}(.)$, and $N_{3,n}(.)$ are given by:
% %\begin{align*}
% %    N_{1,n}(\prompt) &=M_{n,j,0_d},
% %    \\
% %     N_{2,n}(\prompt) &=\sum_{u = 1}^{d} M_{n, j, 1_{u}} (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) + \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}}(B \dfrac{\partial^2{\sigma_{1}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt)),
% %     \\
% %      N_{3,n}(\prompt) &= \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}}(B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt))^{\top}.
% %\end{align*}
% %Putting the above equations together leads to
% %\begin{align}   \sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm) /\mathcal{D}_{2n}& \nonumber \\
% %& \hspace{-21 em} = \sum_{j = 1}^{L} \big[\dfrac{N_{1,n}(\prompt_{*,j})}{\mathcal{D}_{2n}} + \dfrac{N_{2,n}(\prompt_{*,j})^{\top}}{\mathcal{D}_{2n}}\Xbm + \Xbm^{\top} \dfrac{N_{3,n}(\prompt_{*,j})}{\mathcal{D}_{2n}} \Xbm \big]\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) g_{G_n}(\Xbm).
% %\label{eq:main_equation_second}
% %\end{align}
% %Therefore, we also can view the term $\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm)/ \mathcal{D}_{2n}$ as a linear combination of the independent functions $\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) g_{G_{n}}(\Xbm)$, $\Xbm^{(u)} \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) g_{G_{n}}(\Xbm)$, and $\Xbm^{(u)} \Xbm^{(v)} \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) g_{G_{n}}(\Xbm)$, whose elements are respectively $N_{1,n}(\prompt_{*,j})/\mathcal{D}_{2n}$, $N_{2,n}(\prompt_{*,j})^{(u)}/\mathcal{D}_{2n}$, and $N_{3,n}(\prompt_{*,j})^{(uv)}/\mathcal{D}_{2n}$ for any $1 \leq u,v \leq d$ and $1 \leq j \leq L$.

% \paragraph{Step 2 - Non-vanishing coefficients.} 
%  From equation~(\ref{eq:main_equation_expression}), we can represent $Q_{n}(\Xbm)/ \mathcal{D}_{2n}$ as a linear combination of the independent functions $\exp((B \prompt_{*,j})^{\top}\Xbm)$, $(B^{\top} \Xbm)^{(u)} \exp((B \prompt_{*,j})^{\top}\Xbm)$, $(B^{\top} \Xbm)^{(u)} (B^{\top} \Xbm)^{(v)} \exp((B \prompt_{*,j})^{\top}\Xbm), \exp((B \prompt_{*,j})^{\top}\Xbm) f_{\bar{G}_{n}}(\Xbm), (B^{\top} \Xbm)^{(u)} \exp((B \prompt_{*,j})^{\top}\Xbm) f_{\bar{G}_{n}}(\Xbm)$, and $(B^{\top} \Xbm)^{(u)} (B^{\top} \Xbm)^{(v)} \exp((B \prompt_{*,j})^{\top}\Xbm) f_{\bar{G}_{n}}(\Xbm)$ for any $1 \leq j \leq L$ and $1 \leq u, v \leq d$. 
 
%  Assume that all the coefficients of these linear independent functions in the formulation of $Q_{n}(\Xbm)/ \mathcal{D}_{2n}$ go to 0 as $n \to \infty$. It follows that $L_{1,n}(\prompt_{*,j})/\mathcal{D}_{2n}$, $L_{2,n}(\prompt_{*,j})^{(u)}/\mathcal{D}_{2n}$, $\bar{L}_{1,n}(\prompt_{*,j})/\mathcal{D}_{2n}$, $\bar{L}_{2,n}(\prompt_{*,j})^{(u)}/\mathcal{D}_{2n}$, $\bar{L}_{3,n}(\prompt_{*,j})^{(uv)}/\mathcal{D}_{2n}$, $N_{1,n}(\prompt_{*,j})/\mathcal{D}_{2n}$, $\bar{N}_{1,n}((\prompt_{*,j})^{(u)}/\mathcal{D}_{2n}$, $\bar{N}_{2,n}(\prompt_{*,j})^{(uv)}/\mathcal{D}_{2n}$, and $M_{n,j,0_{d}}/\mathcal{D}_{2n}$ approach 0 as $n \to \infty$ for any $1 \leq u,v \leq d$ and $1 \leq j \leq L$. 
 
% Then, as $M_{n,j,0_{d}}/\mathcal{D}_{2n} \to 0$, it indicates that
% \begin{align*}
%     \frac{|M_{n,j,0_{d}}|}{\mathcal{D}_{2n}} = \frac{|\sum_{i\in\mathcal{C}_j}\exp(b_{n,i})-\exp(b_{*,j})|}{\mathcal{D}_{2n}} \to 0,
% \end{align*}
% for any $1 \leq j \leq L$. By summing these limits up when varying the index $j$ from 1 to $L$, we obtain that
% \begin{align}
% \frac{\sum_{j = 1}^{L} |\sum_{i\in\mathcal{C}_j}\exp(b_{n,i})-\exp(b_{*,j})|}{\mathcal{D}_{2n}} \to 0. \label{eq:key_limits_first}
% \end{align}
% Now, we consider indices $j \in [L]$ such that its corresponding Voronoi cell has only one element, i.e. $|\mathcal{C}_j | = 1$. As $L_{2,n}(\prompt_{*,j})^{(u)}/\mathcal{D}_{2n} \to 0$, it indicates that $M_{n,j,1_{u}}/ \mathcal{D}_{2n} \to 0$. It indicates that
% \begin{align*}
%    \frac{\sum_{u = 1}^{d} \exp(b_{n,i})|M_{n,j,1_{u}}|}{\mathcal{D}_{2n}} = \frac{\sum_{i \in \mathcal{C}_{j}} \exp(b_{n,i})\|\Delta p_{n,ij}\|}{\mathcal{D}_{2n}} \to 0. 
% \end{align*}
% Putting the above results together, we find that
% \begin{align}
%     \frac{\sum_{j: |\mathcal{C}_{j}| = 1} \sum_{i \in \mathcal{C}_{j}} \exp(b_{n,i}) \|\Delta p_{n,ij}\|}{\mathcal{D}_{2n}} \to 0. 
% \end{align}
% Moving to indices $j \in [L]$ such that $|\mathcal{C}_{j}| > 1$, as $\bar{L}_{3,n}(\prompt_{*,j})^{(uu)}/ \mathcal{D}_{2n} \to 0$, we obtain that 
% \begin{align*}
%     \frac{\sum_{u = 1}^{d} \exp(b_{n,i}) \bar{L}_{3,n}(\prompt_{*,j})^{(uu)}}{\mathcal{D}_{2n}} = \frac{\sum_{i \in \mathcal{C}_{j}} \exp(b_{n,i})\|\Delta \prompt_{n,ij}\|^2}{\mathcal{D}_{2n}} \to 0. 
% \end{align*}
% Therefore, we find that
% \begin{align*}
%     \frac{\sum_{j: |\mathcal{C}_{j}| > 1} \sum_{i \in \mathcal{C}_{j}} \exp(b_{n,i}) \|\Delta p_{n,ij}\|^2}{\mathcal{D}_{2n}} \to 0. 
% \end{align*}
% Collecting all the above results, we obtain that
% \begin{align*}
%     1 = \frac{\mathcal{D}_{2n}}{\mathcal{D}_{2n}} \to 0
% \end{align*}
% as $n \to \infty$, which is a contradiction. 

% As a consequence, not all of the coefficients of the linear independent functions in the formulations of $Q_{n}(\Xbm)/ \mathcal{D}_{2n}$ go to 0 as $n \to \infty$. 
% \paragraph{Step 3 - Application of Fatou’s lemma.} In particular, let denote $m_n$ as the maximum of the absolute values of $L_{1,n}'(\prompt_{*,j})/\mathcal{D}_{2n}$, $L_{2,n}(\prompt_{*,j})^{(u)}/\mathcal{D}_{2n}$, $\bar{L}_{1,n}'(\prompt_{*,j})/\mathcal{D}_{2n}$, $\bar{L}_{2,n}(\prompt_{*,j})^{(u)}/\mathcal{D}_{2n}$, $\bar{L}_{3,n}(\prompt_{*,j})^{(uv)}/\mathcal{D}_{2n}$, $N_{1,n}(\prompt_{*,j})/\mathcal{D}_{2n}$, $\bar{N}_{1,n}((\prompt_{*,j})^{(u)}/\mathcal{D}_{2n}$, $\bar{N}_{2,n}(\prompt_{*,j})^{(uv)}/\mathcal{D}_{2n}$, and $M_{n,j,0_{d}}/\mathcal{D}_{2n}$ for all $1 \leq u, v \leq d$. From the result of Step 2, it
% follows that $1/m_n \not \to \infty$ as $n \to \infty$.

% Recall that $\normf{f_{\bar{G}_n}-f_{\bar{G}_*}}/\mathcal{D}_{2n} \to 0$ as $n \to \infty$, which
% indicates that $\normf{f_{\bar{G}_n}-f_{\bar{G}_*}}/(m_{n} \mathcal{D}_{2n}) \to 0.$ By applying Fatou's lemma, we get that
% \begin{align*}
%     0=\lim_{n \to \infty} \dfrac{\normf{f_{\bar{G}_n}-f_{\bar{G}_*}}}{m_n\mathcal{D}_{2n}} \geq  \int \liminf_{n \to \infty} \dfrac{\left| f_{\bar{G}_n}(\Xbm)-f_{\bar{G}_*}(\Xbm)\right|}{m_n\mathcal{D}_{2n}}d\mu(\Xbm) \geq 0.
% \end{align*}
% It indicates that $\liminf_{n \to \infty} \dfrac{\left| f_{\bar{G}_n}(\Xbm)-f_{\bar{G}_*}(\Xbm)\right|}{m_n\mathcal{D}_{2n}} = 0$ for almost surely $\Xbm$. As $n \to \infty$, we denote
% \begin{align*}
%     & \dfrac{L_{1,n}'(\prompt_{*,j})}{m_{n}\mathcal{D}_{2n}} \to \alpha_{j}, \quad \dfrac{L_{2,n}(\prompt_{*,j})}{m_{n}\mathcal{D}_{2n}} \to \beta_{j}, \\
%     & \dfrac{\bar{L}_{1,n}'(\prompt_{*,j})}{m_{n}\mathcal{D}_{2n}} \to \bar{\alpha}_{j}, \quad \dfrac{\bar{L}_{2,n}(\prompt_{*,j})}{m_{n}\mathcal{D}_{2n}} \to \bar{\beta}_{j}, \quad \dfrac{\bar{L}_{3,n}(\prompt_{*,j})}{m_{n}\mathcal{D}_{2n}} \to \bar{\gamma}_{j}, \\
%     & \dfrac{M_{n,j,0_{d}}}{\mathcal{D}_{2n}} \to \tilde{\alpha}_{j}, \quad \dfrac{N_{1,n}(\prompt_{*,j})}{m_{n}\mathcal{D}_{2n}} \to \tilde{\beta}_{j}, \\
%     & \dfrac{\bar{N}_{1,n}(\prompt_{*,j})}{m_{n}\mathcal{D}_{2n}} \to \widehat{\beta}_{j}, \quad \dfrac{\bar{N}_{2,n}(\prompt_{*,j})}{m_{n}\mathcal{D}_{2n}} \to \widehat{\gamma}_{j}
% \end{align*}
% for any $1 \leq j \leq L$. Here, from the definition of $m_{n}$, at least one coefficient among $\{\alpha_{j}, \beta_{j}, \tilde{\alpha}_{j}, \tilde{\beta}_{j}\}_{j: |\mathcal{C}_{j}| = 1}$, $\{\bar{\alpha}_{j}, \bar{\beta}_{j}, \bar{\gamma}_{j}, \tilde{\alpha}_{j}, \widehat{\beta}_{j}, \widehat{\gamma}_{j}\}_{j: |\mathcal{C}_{j}| > 1}$ is different from 0. Then, the equation $\liminf_{n \to \infty} \dfrac{\left| f_{\bar{G}_n}(\Xbm)-f_{\bar{G}_*}(\Xbm)\right|}{m_n\mathcal{D}_{2n}} = 0$ leads to
% \begin{align*}
%     & \sum_{j:|\mathcal{C}_j| = 1} \exp((B \prompt_{*,j})^{\top}\Xbm) (\alpha_{j} + \beta_{j}^{\top} (B^{\top} \Xbm) \bigr) \nonumber \\
%     & + \sum_{j:|\mathcal{C}_j| > 1} \exp((B \prompt_{*,j})^{\top}\Xbm) \big[\bar{\alpha}_{j} + \bar{\beta}_{j}^{\top} (B^{\top} \Xbm) + (B^{\top}\Xbm)^{\top} \bar{\gamma}_{j} (B^{\top} \Xbm) \big] \nonumber \\
%     & - \sum_{j:|\mathcal{C}_j| = 1} \exp((B \prompt_{*,j})^{\top}\Xbm) (\tilde{\alpha}_{j} + \tilde{\beta}_{j}^{\top} (B^{\top}\Xbm)) f_{\bar{G}_{*}}(\Xbm) \nonumber \\
%     & - \sum_{j:|\mathcal{C}_j| > 1} \exp((B \prompt_{*,j})^{\top}\Xbm) \big[\tilde{\alpha}_{j} + \widehat{\beta}_{j}^{\top} (B^{\top} \Xbm) + (B^{\top}\Xbm)^{\top} \widehat{\gamma}_{j} B^{\top} \Xbm \big]f_{\bar{G}_{*}}(\Xbm) = 0
% \end{align*}
% for almost surely $\Xbm$. By denoting $\boldsymbol{Z} = B^{\top} \Xbm$, this equation also holds for almost surely $\boldsymbol{Z}$. However, the new equation implies that all the coefficients $\{\alpha_{j}, \beta_{j}, \tilde{\alpha}_{j}, \tilde{\beta}_{j}\}_{j: |\mathcal{C}_{j}| = 1}$, $\{\bar{\alpha}_{j}, \bar{\beta}_{j}, \bar{\gamma}_{j}, \tilde{\alpha}_{j}, \widehat{\beta}_{j}, \widehat{\gamma}_{j}\}_{j: |\mathcal{C}_{j}| > 1}$ are 0, which is a contradiction. 

% It indicates that we indeed have the conclusion of the local part, namely, $$\lim_{\varepsilon\to0} \inf_{\bar{G}\in\mathcal{\bar{G}}_{L'}(\Omega): \mathcal{D}_2(\bar{G},\bar{G}_*)\leq \varepsilon} \normf{f_{\bar{G}}-f_{\bar{G}_*}}/\mathcal{D}_2(\bar{G},\bar{G}_*) >0.$$
% \paragraph{Global part:}From local part, there exists a positive constant $\varepsilon'$ such that
% $$\inf_{\bar{G}\in\mathcal{\bar{G}}_{L'}(\Omega): \mathcal{D}_2(\bar{G},\bar{G}_*)\leq \varepsilon'} \normf{f_{\bar{G}}-f_{\bar{G}_*}}/\mathcal{D}_2(\bar{G},\bar{G}_*) >0.$$
% Therefore, it is sufficient to prove that
% $$ \inf_{\bar{G}\in\mathcal{\bar{G}}_{L'}(\Omega): \mathcal{D}_2(\bar{G},\bar{G}_*)> \varepsilon'} \normf{f_{\bar{G}}-f_{\bar{G}_*}}/\mathcal{D}_2(G,\bar{G}_*) >0.$$
% Assume by contrary, then we can find a sequence of
% mixing measures $\bar{G}'_{n} := \sum_{j' = 1}^{L'} \exp(b_{n,j'}) \delta_{\prompt_{n,j'}}$ in $\mathcal{\bar{G}}_{L'}(\Omega)$ such that as $n \to \infty$, we have
% $$\left\{\begin{matrix}
%  \mathcal{D}_2(\bar{G}'_n,\bar{G}_*) > \varepsilon'\\
%  \normf{f_{\bar{G}'_n}-f_{\bar{G}_*}}/\mathcal{D}_2(\bar{G}'_n,\bar{G}_*) \to 0,
% \end{matrix}\right.$$
% which indicates that $\normf{f_{\bar{G}'_n}-f_{\bar{G}_*}} \to 0$  as $n \to \infty$.\\
% Recall that $\Omega$ is a compact set. Therefore, there exists a mixing measure $\bar{G}'$ in $\mathcal{\bar{G}}_{L'}(\Omega)$ such that one of $\bar{G}'_n$'s  subsequences converges to $\bar{G}'$. Since $\mathcal{D}_2(\bar{G}'_n,\bar{G}_*)>\varepsilon'$, we deduce that $\mathcal{D}_2(\bar{G}',\bar{G}_*)>\varepsilon'$.\\
% By invoking the Fatou’s lemma, we have that
% $$0=\lim_{n \to \infty} \normf{f_{\bar{G}'_n}-f_{\bar{G}_*}} \geq  \int \liminf_{n \to \infty} \left| f_{\bar{G}'_n}-f_{\bar{G}_*}\right|^2 d\mu(\Xbm).$$
% Thus, we have $f_{\bar{G}'}=f_{\bar{G}_*}$ for $\mu-$almost surely $\Xbm$. From the identifiability property (cf. the end of this proof), we deduce that $\bar{G}'\equiv \bar{G}_*$. It follows that $\mathcal{D}_2(\bar{G}',\bar{G}_*)=0$, contradicting the fact that $\mathcal{D}_2(\bar{G}',\bar{G}_*)>\varepsilon'>0$. \\
% Hence, the proof of the global part is completed.
\subsection{Proof of Theorem~\ref{theorem:zero_initialized_overspecified_nonlinear}}
\label{appendix:zero_initialized_overspecified_nonlinear}
Similar to the proof of Theorem~\ref{theorem:zero_initialized_overspecified} in Appendix~\ref{appendix:zero_initialized_overspecified}, we only need to demonstrate that 
\begin{align*}
    \|g_{G,\alpha} - g_{G_{*}, \alpha_{*}}\|_{L^{2}(\mu)} \geq C \cdot \left(\mathcal{D}(G, G_{*}) + |\alpha-\alpha_*| \right)
\end{align*} 
for any $(G, \alpha) \in \mathcal{G}_{L'}(\Theta) \times \Omega$ for some universal constant $C$.
It is equivalent to proving that:
\begin{align*}
\inf_{(G, \alpha) \in \mathcal{G}_{L'}(\Theta) \times \Omega} \normf{g_{G, \alpha}-g_{G_*, \alpha_{*}}}/(\mathcal{D}(G,G_*) + |\alpha-\alpha_*|) >0.
\end{align*}
To obtain the conclusion for the above inequality, we consider two parts: (i) local part, namely, 
\begin{align*}
    \lim_{\varepsilon\to0} \inf_{(G, \alpha) \in \mathcal{G}_{L'}(\Theta) \times \Omega: \mathcal{D}(G,G_*) + |\alpha - \alpha_{*}| \leq \varepsilon} \normf{g_{G, \alpha}-g_{G_*, \alpha_{*}}}/(\mathcal{D}(G,G_*) + |\alpha-\alpha_*|) >0;
\end{align*} 
(ii) global part, namely, for any $\varepsilon > 0$
\begin{align*}
    \inf_{(G, \alpha) \in \mathcal{G}_{L'}(\Theta) \times \Omega: \mathcal{D}(G,G_*) + |\alpha - \alpha_{*}| > \varepsilon} \normf{g_{G, \alpha}-g_{G_*, \alpha_{*}}}/(\mathcal{D}(G,G_*) + |\alpha-\alpha_*|) >0;
\end{align*} 
Since the global part can be argued in a similar fashion to Appendix~\ref{appendix:zero_initialized_overspecified}, we will focus only on proving the local part in this appendix. Additionally, we will impose the following essential yet mild assumptions in the activation function $\sigma$ to facilitate our arguments:

\textbf{Assumptions.} We assume that the activation function $\sigma$ meet the following assumptions:

\emph{(A.1) (Uniform Lipschitz) Let $F(\Xbm;\prompt):=\exp((B\sigma(\prompt))^{\top}\Xbm)C\sigma(\prompt)$. Then, for any $r\in\{1,2\}$, we have
    \begin{align*}
        \sum_{|\alpha|=r}\Bigg|\Big(\frac{\partial^{|\alpha|}F}{\partial\prompt^{\alpha}}(\Xbm;\prompt)&-\frac{\partial^{|\alpha|}F}{\partial\prompt^{\alpha}}(\Xbm;\prompt')\Big)\gamma^{\alpha}\Bigg|\leq c\|\prompt-\prompt'\|^{\zeta}\|\gamma\|^{r},
    \end{align*}
    for any vector $\gamma\in\mathbb{R}^{d}$ and for some positive constants $\zeta$ and $c$ which are independent of $\Xbm$ and $\prompt,\prompt'$. Here, $\alpha\in\mathbb{N}^{d}$.}

\emph{(A.2) (Injective) If there exist parameters $\prompt$ and $\prompt'$ such that $\sigma(\prompt)=\sigma(\prompt')$, then we obtain that $\prompt=\prompt'$. }

\paragraph{Local part:} We first start with the local part, which is equivalent to demonstrating that
\begin{align*}
    \lim_{\varepsilon\to0} \inf_{(G, \alpha) \in \mathcal{G}_{L'}(\Theta) \times \Omega: \mathcal{D}(G,G_*) + |\alpha - \alpha_{*}| \leq \varepsilon} \normf{g_{G, \alpha}-g_{G_*, \alpha_{*}}}/(\mathcal{D}(G,G_*) + |\alpha-\alpha_*|) >0.
\end{align*} 
We prove the above claim by contradiction. Assume by contrary that the above claim does not hold. It indicates that we can find a sequence of mixing measures $G_{n} := \sum_{j' = 1}^{L_n} \exp(\bar{b}_{n,j'}) \delta_{\prompt_{n,j'}}$ in $\mathcal{{G}}_{L'}(\Theta)$ and a sequence of $\alpha_{n} \in \Omega$ such that when $n \to \infty$, the following limits hold:
$$\left\{\begin{matrix}
 \mathcal{D}(G_n,{G}_*) + |\alpha_{n} - \alpha_{*}| \to 0, \\
 \normf{g_{G_n, \alpha_{n}}-g_{G_*, \alpha_{*}}}/(\mathcal{D}(G_n,{G}_*) + |\alpha_n-\alpha_*|) \to 0.
\end{matrix}\right.$$
The first limit indicates that $\mathcal{D}_{n} : = \mathcal{D}(G_n,{G}_*) \to 0$ and $\alpha_{n} \to \alpha_{*}$ as $n \to \infty$.

For the simplicity of the ensuing presentation, we denote $\mathcal{C}_j^n:= \mathcal{C}_j({G}_n)$ as a Voronoi cell of ${G}_n$ induced by the $j$-th components of ${G}_*$. Without loss of generality, we assume that those Voronoi cells do not depend on the sample size, i.e., $\mathcal{C}_j = \mathcal{C}_j^n$, which is possible since our arguments are asymptotic. Therefore, we can rewrite the Voronoi loss $\mathcal{D}_{n}$ as follows:
\begin{align*}
    \mathcal{D}_{n}:=\sum_{j'=1}^{L}\Big|\sum_{i\in\mathcal{C}_{j'}}\exp(\bar{b}_{n,i})-\exp(\bar{b}_{*,j'})\Big|&+\sum_{j'\in[L]:|\mathcal{C}_{j'}|=1}\sum_{i\in\mathcal{C}_{j'}}\exp(\bar{b}_{n,i})\|\Delta \prompt_{n,ij'}\| \nonumber\\
    &+\sum_{j'\in[L]:|\mathcal{C}_{j'}|>1}\sum_{i\in\mathcal{C}_{j'}}\exp(\bar{b}_{n,i})\|\Delta \prompt_{n,ij'}\|^{2},
\end{align*}
where $\Delta\prompt_{n,ij'}=\prompt_{n,i}-\prompt_{*,j'}$ for all $i \in \mathcal{C}_{j'}$.

From the hypothesis, we have $\mathcal{D}_{n} \to 0$, which implies that $\sum_{i\in\mathcal{C}_{j}}\exp(\bar{b}_{n,i})\to\exp(\bar{b}_{*,j})$ and $\prompt_{n,i} \to \prompt_{*,j}$ for any $i \in \mathcal{C}_{j}, j \in [L]$. Similar to the proof of Theorem~\ref{theorem:zero_initialized_overspecified},  to establish the contradiction our proof consists of three main steps.
\paragraph{Step 1 - Taylor expansion.} To ease the presentation, let us denote
\begin{align*}
    g_{G_n}(\Xbm)&:=\sum_{j = 1}^{L_n} \frac{\exp((\bar{B}\sigma(\prompt_{n,j}))^{\top}\Xbm+\bar{b}_{n,j})}{\sum_{k = 1}^{L_n} \exp((\bar{B}\sigma(\prompt_{n,k}))^{\top}\Xbm+\bar{b}_{n,k})}\cdot \bar{C}\sigma(\prompt_{n,j}),\\
    g_{G_*}(\Xbm)&:=\sum_{j' = 1}^{L} \frac{\exp((\bar{B}\sigma(\prompt_{*,j'}))^{\top}\Xbm+\bar{b}_{*,j'})}{\sum_{k' = 1}^{L} \exp((\bar{B}\sigma(\prompt_{*,k'}))^{\top}\Xbm+\bar{b}_{*,k'})}\cdot \bar{C}\sigma(\prompt_{*,j'}).
\end{align*}
We now perform the following decomposition:
\begin{align*}
    \widetilde{Q}_{n}(\Xbm):&=\Big[\sum_{k' = 1}^{L} \exp((\bar{B}\sigma(\prompt_{*,k'}))^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot[g_{G_n,\alpha_n}(\Xbm)-g_{G_*,\alpha_*}(\Xbm)]\\
    &=\Big[\sum_{k' = 1}^{L} \exp((\bar{B}\sigma(\prompt_{*,k'}))^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot\Big(\tanh(\alpha_n)g_{G_n}(\Xbm)
    -\tanh(\alpha_*)g_{G_*}(\Xbm)\Big)\\
    &=\Big[\sum_{k' = 1}^{L} \exp((\bar{B}\sigma(\prompt_{*,k'}))^{\top}\Xbm+ \bar{b}_{*,k'})\Big]\cdot\tanh(\alpha_n)\Big[g_{G_n}(\Xbm)-g_{G_*}(\Xbm)\Big]\\
    &+\Big[\sum_{k' = 1}^{L} \exp((\bar{B}\sigma(\prompt_{*,k'}))^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot[\tanh(\alpha_n)-\tanh(\alpha_*)]g_{G_*}(\Xbm)\\
    &:=\widetilde{Q}_{n,1}(\Xbm)+\widetilde{Q}_{n,2}(\Xbm).
\end{align*}
For that purpose, we will decompose the two terms $\widetilde{Q}_{n,1}(\Xbm)$ and $\widetilde{Q}_{n,2}(\Xbm)$, respectively.\\

\noindent
\textbf{Decomposition of the function $\widetilde{Q}_{n,1}(\Xbm)$.} We have
\begin{align}
\widetilde{Q}_{n,1}(\Xbm)&=\sum_{j=1}^{L}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\exp((\bar{B}\sigma(\prompt_{n,i}))^{\top}\Xbm)\bar{C}\sigma(\prompt_{n,i})-\exp((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm)\bar{C} \sigma(\prompt_{*,j})\Big] \nonumber \\
    &-\sum_{j=1}^{L}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\exp((\bar{B}\sigma(\prompt_{n,i}))^{\top}\Xbm)-\exp((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm)\Big]g_{G_n}(\Xbm) \nonumber \\
    &+\sum_{j=1}^{L}\tanh(\alpha_n)\Big(\sum_{i\in\mathcal{C}_j}\exp(\bar{b}_{n,i})-\exp(\bar{b}_{*,j})\Big)\exp((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm)\Big[\bar{C}\sigma(\prompt_{*,j})-g_{G_n}(\Xbm)\Big] \nonumber \\
    &:= \widetilde{A}_n(\Xbm)- \widetilde{B}_n(\Xbm)+ \widetilde{C}_n(\Xbm).
    \label{eq:main_equation_nonlinear}
\end{align}
\paragraph{Decomposition of the function $\widetilde{A}_n(\Xbm)$.} To ease the presentation, we define the following functions $\widetilde{U}(\Xbm; \prompt) : = \exp((\bar{B}\sigma(\prompt))^{\top}\Xbm)$ and $\widetilde{V}(\prompt) = \bar{C} \sigma(\prompt)$. Then, we denote the product of these functions as $\widetilde{F}(\Xbm;\prompt)= \widetilde{U}(\Xbm; \prompt) \widetilde{V}(\prompt)$. To decompose $\widetilde{A}_n(\Xbm)$, we separately consider Voronoi cells with exactly one element and those with more than one element. It leads to the following decomposition of the function $\widetilde{A}_{n}(\Xbm)$:
\begin{align*}   \widetilde{A}_n(\Xbm)&=\sum_{j:|\mathcal{C}_j|=1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\widetilde{F}(\Xbm;\prompt_{n,i})-\widetilde{F}(\Xbm;\prompt_{*,j})\Big]\\
    & + \sum_{j:|\mathcal{C}_j|>1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\widetilde{F}(\Xbm;\prompt_{n,i})-\widetilde{F}(\Xbm;\prompt_{*,j})\Big]\\
    &:= \widetilde{A}_{n,1}(\Xbm) + \widetilde{A}_{n,2}(\Xbm),
\end{align*}
where we denote $\widetilde{A}_{n,1}(\Xbm) = \sum_{j:|\mathcal{C}_j|=1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\widetilde{F}(\Xbm;\prompt_{n,i})-\widetilde{F}(\Xbm;\prompt_{*,j})\Big]$ and $\widetilde{A}_{n,2}(\Xbm) = \sum_{j:|\mathcal{C}_j|>1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\widetilde{F}(\Xbm;\prompt_{n,i})-\widetilde{F}(\Xbm;\prompt_{*,j})\Big]$. 

For the function $\widetilde{A}_{n,1}(\Xbm)$, for any indices $i \in \mathcal{C}_{j}$ and $j$ such that $|\mathcal{C}_{j}| = 1$, the first-order Taylor expansion entails that
\begin{align*}
    \widetilde{U}(\Xbm; \prompt_{n,i}) & = \widetilde{U}(\Xbm; \prompt_{*,j}) + \sum_{|\alpha|=1} (\Delta\prompt_{n,ij} )^{\alpha} \dfrac{\partial^{|\alpha|} \widetilde{U}}{\partial{\prompt^\alpha}}(\Xbm;\prompt_{*,j}) + \widetilde{R}_{ij,1}(\Xbm), \\
    \widetilde{V}(\prompt_{n,i}) & = \widetilde{V}(\prompt_{*,j}) + \sum_{|\alpha|=1} (\Delta \prompt_{n,ij} )^{\alpha} \dfrac{\partial^{|\alpha|} \widetilde{V}}{\partial{\prompt^\alpha}}(\prompt_{*,j}) + \widetilde{R}_{ij,2},
\end{align*}
where the terms $\widetilde{R}_{ij,1}(\Xbm)$ and $\widetilde{R}_{ij, 2}$ are Taylor remainders. 

Combining the above results leads to the following formulation of the function $\widetilde{A}_{n,1}(\Xbm)$:
\begin{align*}
    \widetilde{A}_{n,1}(\Xbm) &= \sum_{j:|\mathcal{C}_j|=1}\sum_{i\in\mathcal{C}_j} \dfrac{\tanh(\alpha_n)\exp(\bar{b}_{n,i})}{\alpha!} \sum_{|\alpha|=1} \biggr\{(\Delta\prompt_{n,ij} )^{\alpha}\dfrac{\partial^{|\alpha|} \widetilde{U}}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) \widetilde{V}(\prompt_{*,j}) \\
    &\hspace{5cm} + (\Delta\prompt_{n,ij} )^{\alpha} \dfrac{\partial^{|\alpha|} \widetilde{V}}{\partial{\prompt^\alpha}}(\prompt_{*,j}) \widetilde{U}(\Xbm;\prompt_{*,j})\biggr\} + \widetilde{R}_{n,1}(\Xbm)\\
    &=\sum_{j:|\mathcal{C}_j|=1}\sum_{|\alpha|=1} \biggr\{ \bar{M}_{n,j,\alpha} \dfrac{\partial^{|\alpha|} \widetilde{U}}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) \widetilde{V}(\prompt_{*,j})  + \bar{M}_{n,j,\alpha} \dfrac{\partial^{|\alpha|} \widetilde{V}}{\partial{\prompt^\alpha}}(\prompt_{*,j}) \widetilde{U}(\Xbm; \prompt_{*,j})\biggr\} + \widetilde{R}_{n,1}(\Xbm)
\end{align*}
where the function $\widetilde{R}_{n,1}(\Xbm)$ is the combination of Taylor remainders and satisfies that $\widetilde{R}_{n,1}(\Xbm)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$ when $n \to \infty$. Furthermore, the formulations of $\bar{M}_{n,j,\alpha}$ are as follows:
\begin{align*}
\bar{M}_{n,j,\alpha}=\sum_{i\in\mathcal{C}_j} \dfrac{\tanh(\alpha_n)\exp(\bar{b}_{n,i})}{\alpha!} (\Delta\prompt_{n,ij})^{\alpha},
\end{align*}
for any $|\alpha| = 1$.


Moving to the function $\widetilde{A}_{n,2}(\Xbm)$, the second-order Taylor expansions of the function $\widetilde{U}(\Xbm; \prompt_{n,i})$ around the function $\widetilde{U}(\Xbm; \prompt_{*,j})$ and the function $\widetilde{V}(\prompt_{n,i})$ around the function $\widetilde{V}(\prompt_{*,j})$ for any indices $i \in \mathcal{C}_{j}$ and $j$ such that $|\mathcal{C}_{j}| > 1$, we obtain the following formulation of the function $\widetilde{A}_{n,2}(\Xbm)$:
\begin{align*}
\widetilde{A}_{n,2}(\Xbm) & = \sum_{j:|\mathcal{C}_j|>1}\sum_{1\leq |\alpha|\leq 2} \biggr\{\bar{M}_{n,j,\alpha}\dfrac{\partial^{|\alpha|} \widetilde{U}}{\partial {\prompt^\alpha}}(\Xbm;\prompt_{*,j}) \widetilde{V}(\prompt_{*,j})  + \bar{M}_{n,j,\alpha} \dfrac{\partial^{|\alpha|} \widetilde{V}}{\partial{\prompt^\alpha}}(\prompt_{*,j}) \widetilde{U}(\Xbm;\prompt_{*,j}) \biggr\} \\
& + \sum_{j:|\mathcal{C}_j|>1}\sum_{|\alpha| = 1, |\beta| = 1} \bar{M}_{n,j,\alpha, \beta} \dfrac{\partial^{|\alpha|} \widetilde{U}}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) \dfrac{\partial^{|\beta|} \widetilde{V}}{\partial{\prompt^\beta}}(\prompt_{*,j})  + \widetilde{R}_{n,2}(\Xbm)
\end{align*}
where the function $\widetilde{R}_{n,2}(\Xbm)$ is a combination of Taylor remainders and satisfies $\widetilde{R}_{n,2}(\Xbm)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$ when $n \to \infty$. Furthermore, we define
\begin{align*}   \bar{M}_{n,j,\alpha} = \sum_{i\in\mathcal{C}_j} \dfrac{\tanh(\alpha_n)\exp(b_{n,i})}{\alpha!} (\Delta\prompt_{n,ij})^{\alpha},
\end{align*}
for any $|\alpha| = 2$ and
\begin{align*}
    \bar{M}_{n,j,\alpha, \beta} = \sum_{i\in\mathcal{C}_j} \dfrac{\tanh(\alpha_n)\exp(\bar{b}_{n,i})}{\alpha! \beta!} (\Delta\prompt_{n,ij})^{\alpha + \beta},  
\end{align*}
for any $|\alpha| = |\beta| = 1$. From the formulations of the functions $\widetilde{U}(\Xbm; \prompt)$ and $\widetilde{V}(\prompt)$, we obtain the following explicit forms of their partial derivatives:
\begin{align*}
    \dfrac{\partial \widetilde{U}}{\partial {\prompt^{(u)}}}(\Xbm;\prompt) & = \exp((\bar{B}\sigma(\prompt))^{\top}\Xbm) \Big(\bar{B} \dfrac{\partial{\sigma}}{\partial{\prompt^{(u)}}}(\prompt)\Big)^{\top}\Xbm, \\
     \dfrac{\partial^{2} \widetilde{U}}{\partial {\prompt^{(u)}}\partial {\prompt^{(v)}}}(\Xbm;\prompt) & = \exp((\bar{B}\sigma(\prompt))^{\top}\Xbm) \biggr\{ \Big(\bar{B} \dfrac{\partial^{2}{\sigma}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt)\Big)^{\top}\Xbm\\
     & \qquad+ \Xbm^{\top} \Big(\bar{B} \dfrac{\partial{\sigma}}{\partial{\prompt^{(u)}}}(\prompt)\Big)\Big(\bar{B} \dfrac{\partial{\sigma}}{\partial{\prompt^{(v)}}}(\prompt)\Big)^{\top}\Xbm \biggr\}, \\
     \dfrac{\partial \widetilde{V}}{\partial {\prompt^{(u)}}}(\prompt) & = \bar{C} \dfrac{\partial{\sigma}}{\partial{\prompt^{(u)}}}  (\prompt), \\
     \dfrac{\partial^2 \widetilde{V}}{\partial {\prompt^{(u)}}\partial {\prompt^{(v)}}}(\prompt) & = \bar{C} \dfrac{\partial^2{\sigma}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}  (\prompt).
\end{align*}
Plugging these explicit formulations of the derivatives of the functions $\widetilde{U}(\Xbm; \prompt)$ and $\widetilde{V}(\prompt)$, we can express the functions $\widetilde{A}_{n, 1}(\Xbm)$ and $\widetilde{A}_{n,2}(\Xbm)$ as follows:
\begin{align*}
& \widetilde{A}_{n, 1}(\Xbm) = \sum_{j:|\mathcal{C}_{j}| = 1} \exp((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm) \big[\mathcal{L}_{1,n}(\prompt_{*,j}) + \mathcal{L}_{2,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm \bigr) + \widetilde{R}_{n,1}(\Xbm), \\
& \widetilde{A}_{n, 2}(\Xbm) = \sum_{j:|\mathcal{C}_{j}| > 1} \exp((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm) \big[\bar{\mathcal{L}}_{1,n}(\prompt_{*,j}) + \bar{\mathcal{L}}_{2,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm \\
& \hspace{8 em} + (\bar{B}^{\top} \Xbm)^{\top} \bar{\mathcal{L}}_{3,n}(\prompt_{*,j}) \bar{B}^{\top} \Xbm \big] + \widetilde{R}_{n,2}(\Xbm),
\end{align*}
where the functions $\mathcal{L}_{1,n}(\prompt), \mathcal{L}_{2,n}(\prompt), \bar{\mathcal{L}}_{1,n}(\prompt), \bar{\mathcal{L}}_{2,n}(\prompt)$, and $\bar{\mathcal{L}}_{3,n}(\prompt)$ are defined as follows:
\begin{align*}
    \mathcal{L}_{1,n}(\prompt) & = \sum_{u = 1}^{d} \bar{M}_{n, j, 1_{u}}  \bar{C} \dfrac{\partial{\sigma}}{\partial{\prompt^{(u)}}}  (\prompt), \\
    \mathcal{L}_{2,n}(\prompt) & = \sum_{u = 1}^{d} \bar{M}_{n, j, 1_{u}}  \dfrac{\partial{\sigma}}{\partial{\prompt^{(u)}}}(\prompt) \bar{C} \sigma(\prompt), \\
    \bar{\mathcal{L}}_{1,n}(\prompt) & = \sum_{1 \leq u,v \leq d} \bar{M}_{n, j, 1_{uv}} \bar{C} \dfrac{\partial^2{\sigma}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt)  = \sum_{u = 1}^{d} M_{n, j, 1_{uu}}  \bar{C} \dfrac{\partial^2{\sigma}}{\partial{\prompt^{(u)}}\partial{\prompt^{(u)}}}(\prompt), \\
    \bar{\mathcal{L}}_{2,n}(\prompt) & = \sum_{u = 1}^{d} \bar{M}_{n, j, 1_{u}} \dfrac{\partial{\sigma}}{\partial{\prompt^{(u)}}}(\prompt) \bar{C}\sigma(\prompt)  + \sum_{1 \leq u,v \leq d} \big[ \bar{M}_{n, j, 1_{v}, 1_{u}}  \bar{C} \dfrac{\partial{\sigma}}{\partial{\prompt^{(u)}}}  (\prompt)  \dfrac{\partial{\sigma}}{\partial{\prompt^{(v)}}}(\prompt) \\
    & \hspace{5cm}+ \bar{M}_{n,j,1_{uv}} \dfrac{\partial^2{\sigma}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt) \bar{C} \sigma(\prompt)  \big], \\
     \bar{\mathcal{L}}_{3,n}(\prompt) & = \sum_{1 \leq u,v \leq d} \bar{M}_{n, j, 1_{uv}}  \dfrac{\partial{\sigma}}{\partial{\prompt^{(u)}}}(\prompt)  (\dfrac{\partial{\sigma}}{\partial{\prompt^{(v)}}}(\prompt))^{\top} \bar{C} \sigma(\prompt).
\end{align*}
%the partial derivatives of the function $F(\Xbm; .)$ up to the second order are given by:
%\begin{align*}
%\dfrac{\partial F}{\partial \prompt^{(u)}}(\Xbm;\prompt)&= \exp((B\sigma_1(\prompt))^{\top}\Xbm) \Big[((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\Xbm)C\sigma_2(\prompt) + C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) \Big], \\
%\dfrac{\partial^2 F}{\partial \prompt^{(u)}\partial \prompt^{(v)}}(\Xbm;\prompt)&=
%\exp((B\sigma_1(\prompt))^{\top}\Xbm) \biggr\{\Big[((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\Xbm)C\sigma_2(\prompt) + C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) \Big]((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt))^{\top}\Xbm) \\
%& + ((B \dfrac{\partial^2{\sigma_{1}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt))^{\top}\Xbm)C\sigma_2(\prompt) + ((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\Xbm)C\dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(v)}}}  (\prompt) + C \dfrac{\partial^2{\sigma_{2}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt) \biggr\}
%\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)\Big[((B\sigma_1^{''}(\prompt_{*,j}))^{\top}\Xbm^{(u)}\Xbm^{(v)})C\sigma_2(\prompt_{*,j}) +C\sigma_2^{''}(\prompt_{*,j})\\
%&+((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\Xbm^{(u)})C\sigma_2^{'}(\prompt_{*,j})+ +((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\Xbm^{(v)})C\sigma_2^{'}(\prompt_{*,j})\\
%&+\left((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\right)^2\Xbm^{(u)}\Xbm^{(v)} C\sigma_2(\prompt_{*,j})  \Big]
%\end{align*}
%for any $1 \leq u, v \leq d$. Finally, we also have
%$$M_{n,j,\alpha}=\sum_{i\in\mathcal{C}_j} \dfrac{\exp(b_{n,i})}{\alpha!} (\Delta\prompt_{n,ij})^{\alpha},$$ for any $1 \leq |\alpha| \leq 2$.
In these formulations, we use $1_{u}$ to denote the vector that its $u$-th element is 1 and its other elements are 0 for $1 \leq u \leq d$. Furthermore, we denote $1_{uv}$ as the matrix that its $(u,v)$-th element is 1 and its other elements are 0 for any $1 \leq u,v \leq d$. 
\paragraph{Decomposition of the function $\widetilde{B}_n(\Xbm)$.}  Similar to the decomposition of the function $\widetilde{A}_{n}(\Xbm)$, we can decompose the function $\widetilde{B}_n(\Xbm)$ as follows:
\begin{align*}
    \widetilde{B}_n(\Xbm) &=\sum_{j:|\mathcal{C}_j|=1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\widetilde{U}(\Xbm;\prompt_{n,i})-\widetilde{U}(\Xbm;\prompt_{*,j})\Big]g_{G_n}(\Xbm) \\
    & +\sum_{j:|\mathcal{C}_j|>1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\widetilde{U}(\Xbm;\prompt_{n,i})-\widetilde{U}(\Xbm;\prompt_{*,j})\Big]g_{G_n}(\Xbm) \\
    &:= \widetilde{B}_{n,1}(\Xbm) + \widetilde{B}_{n,2}(\Xbm)
\end{align*}
where we denote $\widetilde{B}_{n,1}(\Xbm) = \sum_{j:|\mathcal{C}_j|=1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\widetilde{U}(\Xbm;\prompt_{n,i})-\widetilde{U}(\Xbm;\prompt_{*,j})\Big]g_{G_n}(\Xbm)$ and $\bar{B}_{n,2}(\Xbm) = \sum_{j:|\mathcal{C}_j|>1}\sum_{i\in\mathcal{C}_j}\tanh(\alpha_n)\exp(\bar{b}_{n,i})\Big[\widetilde{U}(\Xbm;\prompt_{n,i})-\widetilde{U}(\Xbm;\prompt_{*,j})\Big]g_{G_n}(\Xbm)$. 

Similar to the Taylor expansions for the functions $\widetilde{A}_{n,1}(\Xbm)$ and $\widetilde{A}_{n,2}(\Xbm)$, by using the first-order Taylor expansion to $\widetilde{B}_{n,1}(\Xbm)$ and the second-order Taylor expansion to $\widetilde{B}_{n,2}(\Xbm)$, we obtain that
\begin{align*}
    \widetilde{B}_{n,1}(\Xbm)&= \sum_{j:|\mathcal{C}_j|=1}\sum_{|\alpha|=1} \bar{M}_{n,j,\alpha} \dfrac{\partial^{|\alpha|}\widetilde{U}}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm)+ \widetilde{R}_{n,3}(\Xbm),
    \\
     \widetilde{B}_{n,2}(\Xbm)&=\sum_{j:|\mathcal{C}_j|=1}\sum_{1 \leq |\alpha|\leq 2} \bar{M}_{n,j,\alpha} \dfrac{\partial^{|\alpha|}\widetilde{U}}{\partial{\prompt^\alpha}}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm)+ \widetilde{R}_{n,4}(\Xbm)
\end{align*}
where the functions $\widetilde{R}_{n,3}(\Xbm), \widetilde{R}_{n,4}(\Xbm)$ are Taylor remainders. Furthermore, they satisfy that $\widetilde{R}_{n,3}(\Xbm)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$ and $\widetilde{R}_{n,4}(\Xbm)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)\to 0$ when $n \to \infty$. Given the explicit formulations of the derivatives of the functions $\widetilde{U}(\Xbm; \prompt)$ and $\widetilde{V}(\prompt)$, the functions $\widetilde{B}_{n,1}(\Xbm)$ and $\widetilde{B}_{n,2}(\Xbm)$ can be then rewritten as follows:
\begin{align*}
    \widetilde{B}_{n,1}(\Xbm) & = \sum_{j:|\mathcal{C}_{j}| = 1} \exp((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm) \mathcal{N}_{1,n}(\prompt_{*,j})^{\top} \Xbm g_{G_{n}}(\Xbm)+ \widetilde{R}_{n,3}(\Xbm), \\
    \widetilde{B}_{n,2}(\Xbm) & = \sum_{j:|\mathcal{C}_{j}| > 1} \exp((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm) \big[\bar{\mathcal{N}}_{1,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm \\
    & \hspace{6 em} + (\bar{B}^{\top} \Xbm)^{\top} \bar{\mathcal{N}}_{2,n}(\prompt_{*,j}) (\bar{B}^{\top} \Xbm) \big]g_{G_{n}}(\Xbm) + \widetilde{R}_{n,4}(\Xbm).
\end{align*}
Here, the functions $\mathcal{N}_{1,n}(\Xbm)$, $\bar{\mathcal{N}}_{1,n}(\Xbm)$, and $\bar{\mathcal{N}}_{2,n}(\Xbm)$ have the following formulations:
\begin{align*}
    \mathcal{N}_{1,n}(\prompt) & = \sum_{u = 1}^{d} \bar{M}_{n, j, 1_{u}} \dfrac{\partial{\sigma}}{\partial{\prompt^{(u)}}}(\prompt), \\
    \bar{\mathcal{N}}_{1,n}(\prompt) & = \sum_{u = 1}^{d} \bar{M}_{n, j, 1_{u}} \dfrac{\partial{\sigma}}{\partial{\prompt^{(u)}}}(\prompt)  + \sum_{1 \leq u,v \leq d} \bar{M}_{n,j,1_{uv}} \dfrac{\partial^2{\sigma}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt), \\   
    \bar{\mathcal{N}}_{2,n}(\prompt) & = \sum_{1 \leq u,v \leq d} \bar{M}_{n, j, 1_{uv}}  \dfrac{\partial{\sigma}}{\partial{\prompt^{(u)}}}(\prompt)  \dfrac{\partial{\sigma}}{\partial{\prompt^{(v)}}}(\prompt)^{\top}.
\end{align*}
Collecting all of the above results with the decomposition of the functions $\widetilde{A}_{n}(\Xbm)$ and $\widetilde{B}_{n}(\Xbm)$, we can represent the function $\widetilde{Q}_{n,1}(\Xbm)$ in equation~(\ref{eq:main_equation_linear}) as follows: 
\begin{align}
    \widetilde{Q}_{n,1}(\Xbm)
    & = \sum_{j:|\mathcal{C}_j| = 1} \exp((\bar{B} \sigma(\prompt_{*,j}))^{\top}\Xbm) \big[\mathcal{L}_{1,n}'(\prompt_{*,j}) + \mathcal{L}_{2,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm \bigr) \nonumber \\
    & + \sum_{j:|\mathcal{C}_j| > 1} \exp((\bar{B} \sigma(\prompt_{*,j}))^{\top}\Xbm) \big[\bar{\mathcal{L}}_{1,n}'(\prompt_{*,j}) + \bar{\mathcal{L}}_{2,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm + (\bar{B}^{\top} \Xbm)^{\top} \bar{\mathcal{L}}_{3,n}(\prompt_{*,j}) \bar{B}^{\top} \Xbm \big] \nonumber \\
    & - \sum_{j:|\mathcal{C}_j| = 1} \exp((\bar{B} \sigma(\prompt_{*,j}))^{\top}\Xbm) \big[ \bar{M}_{n,j,0_{d}} + \mathcal{N}_{1,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm \big] g_{G_{n}}(\Xbm) \nonumber \\
    & - \sum_{j:|\mathcal{C}_j| > 1} \exp((\bar{B} \sigma(\prompt_{*,j}))^{\top}\Xbm) \big[ \bar{M}_{n,j,0_{d}} + \bar{\mathcal{N}}_{1,n}(\prompt_{*,j})^{\top} \bar{B}^{\top} \Xbm + (\bar{B}^{\top} \Xbm)^{\top} \bar{\mathcal{N}}_{2,n}(\prompt_{*,j}) \bar{B}^{\top} \Xbm \big]g_{G_{n}}(\Xbm) \nonumber \\
    & + \widetilde{R}_{n,1}(\Xbm) + \widetilde{R}_{n,2}(\Xbm) - \widetilde{R}_{n,3}(\Xbm) - \widetilde{R}_{n,4}(\Xbm) \label{eq:main_equation_expression_nonlinear}
\end{align}   
%\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha}\dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) - \sum_{j=1}^{L}\sum_{0 \leq |\alpha|\leq 2} M_{n,j,\alpha}\dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm) \\
%    & \hspace{14 em} + \sum_{j=1}^{2}R_{n,j} (\Xbm) - \sum_{j=3}^{4}R_{n,j} (\Xbm) \\    &=\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \Big[\dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) - \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm)\Big] \\
%    & \hspace{14 em} + \sum_{j=1}^{2}R_{n,j} (\Xbm) - \sum_{j=3}^{4}R_{n,j} (\Xbm).
%\end{align*}
where we define $\bar{M}_{n,j,0_{d}}=\tanh(\alpha_n)\Big(\sum_{i\in\mathcal{C}_j}\exp(\bar{b}_{n,i})-\exp(\bar{b}_{*,j})\Big)$ for any index $1 \leq j \leq L$, $\mathcal{L}_{1,n}'(\prompt_{*,j}) = \mathcal{L}_{1,n}(\prompt_{*,j}) + \bar{M}_{n,j,0_{d}} \bar{C} \sigma(\prompt_{*,j})$, and $\bar{\mathcal{L}}_{1,n}'(\prompt_{*,j}) = \bar{\mathcal{L}}_{1,n}(\prompt_{*,j}) + \bar{M}_{n,j,0_{d}} \bar{C}\sigma(\prompt_{*,j})$.\\

\noindent
\textbf{Decomposition of the function $\widetilde{Q}_{n,2}(\Xbm)$.} An application of the first-order Taylor expansion leads to the following expression for the function $\widetilde{Q}_{n,2}(\Xbm)$: 
\begin{align}
    \widetilde{Q}_{n,2}(\Xbm)&=[\tanh(\alpha_n)-\tanh(\alpha_*)]\cdot \Big[\sum_{k' = 1}^{L} \exp((\bar{B}\sigma(\prompt_{*,k'}))^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot g_{G_*}(\Xbm)\nonumber\\
    \label{eq:main_equation_expression_nonlinear_2}
    &=(\alpha_n-\alpha_*)[1-\tanh^2(\alpha_*)]\cdot \Big[\sum_{k' = 1}^{L} \exp((\bar{B}\sigma(\prompt_{*,k'}))^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot g_{G_*}(\Xbm)+\widetilde{R}_{n,5}(\Xbm),
\end{align}
where the function $\widetilde{R}_{n,5}(\Xbm)$ is Taylor remainder and satisfies that $\widetilde{R}_{n,5}(\Xbm)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)\to0$ as $n\to\infty$.


%From the formulations of the partial derivatives of the function $F(\Xbm;.)$ up to the second order, we have
%\begin{align*}
%   \sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j}) = \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) \big[L_{1,n}(\prompt_{*,j}) + L_{2,n}(\prompt_{*,j})^{\top} \Xbm + \Xbm^{\top} L_{3,n}(\prompt_{*,j}) \Xbm \big],
%\end{align*}
%where we define 
%\begin{align*}
%    L_{1,n}(\prompt) & = M_{n, j, 0_{d}} C\sigma_2(\prompt) + \sum_{u = 1}^{d} M_{n, j, 1_{u}}  C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) + \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}} C \dfrac{\partial^2{\sigma_{2}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt), \\
%    L_{2,n}(\prompt) & = \sum_{u = 1}^{d} M_{n, j, 1_{u}} (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) C\sigma_2(\prompt) + \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}}  \Big[C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt)) \\
%    & \hspace{4 em} + (B \dfrac{\partial^2{\sigma_{1}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt)) C\sigma_2(\prompt) + (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) C\dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(v)}}}  (\prompt)\Big],  \\
%    L_{3,n}(\prompt) & = \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}} (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt))^{\top} C\sigma_2(\prompt). 
%\end{align*}
%{\color{blue} Linear independence of $\sigma_{2}, etc.$ does not mean $L_{1,n}(p^{*}) \neq 0$. }
%Here, we denote $1_{u}$ is the vector that its $u$-th element is 1 while its other elements are 0 for any $1 \leq u \leq d$. Furthermore, $1_{uv}$ is the matrix that its $(u,v)$-th element is 1 while its other elements are 0 for any $1 \leq u,v \leq d$. 

%Given the above equations, we obtain that
%\begin{align} \sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})/ \mathcal{D}_{2n}& \nonumber \\
%& \hspace{-15 em} = \sum_{j = 1}^{L} \big[\dfrac{L_{1,n}(\prompt_{*,j})}{\mathcal{D}_{2n}} + \dfrac{L_{2,n}(\prompt_{*,j})^{\top}}{\mathcal{D}_{2n}} \Xbm + \Xbm^{\top} \dfrac{L_{3,n}(\prompt_{*,j})}{\mathcal{D}_{2n}} \Xbm \big] \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm). \label{eq:main_equation_first}
%\end{align}
%Therefore, we can view $\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})/ D_{2n}$ as a linear combination of the linear independent functions $\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)$, $\Xbm^{(u)} \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)$, and $\Xbm^{(u)} \Xbm^{(v)} \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)$ whose elements are respectively $L_{1,n}(\prompt_{*,j})/\mathcal{D}_{2n}$, $L_{2,n}(\prompt_{*,j})^{(u)}/\mathcal{D}_{2n}$, and $L_{3,n}(\prompt_{*,j})^{(uv)}/\mathcal{D}_{2n}$ for any $1 \leq u,v \leq d$ and $1 \leq j \leq L$.

%Likewise, from the formulations of the partial derivatives of the function $E(\Xbm;.)$ up to the second order, we obtain
%\begin{align*}
%    \sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm)& \nonumber \\
%& \hspace{-11 em} =\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm)  \big[N_{1,n}(\prompt_{*,j}) + N_{2,n}(\prompt_{*,j})^{\top} \Xbm + \Xbm^{\top} N_{3,n}(\prompt_{*,j}) \Xbm \big]g_{G_n}(\Xbm),
%\end{align*}
%where the formulations of $N_{1,n}(.), N_{2,n}(.)$, and $N_{3,n}(.)$ are given by:
%\begin{align*}
%    N_{1,n}(\prompt) &=M_{n,j,0_d},
%    \\
%     N_{2,n}(\prompt) &=\sum_{u = 1}^{d} M_{n, j, 1_{u}} (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) + \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}}(B \dfrac{\partial^2{\sigma_{1}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt)),
%     \\
%      N_{3,n}(\prompt) &= \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}}(B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt)) (B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt))^{\top}.
%\end{align*}
%Putting the above equations together leads to
%\begin{align}   \sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm) /\mathcal{D}_{2n}& \nonumber \\
%& \hspace{-21 em} = \sum_{j = 1}^{L} \big[\dfrac{N_{1,n}(\prompt_{*,j})}{\mathcal{D}_{2n}} + \dfrac{N_{2,n}(\prompt_{*,j})^{\top}}{\mathcal{D}_{2n}}\Xbm + \Xbm^{\top} \dfrac{N_{3,n}(\prompt_{*,j})}{\mathcal{D}_{2n}} \Xbm \big]\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) g_{G_n}(\Xbm).
%\label{eq:main_equation_second}
%\end{align}
%Therefore, we also can view the term $\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\Xbm;\prompt_{*,j})g_{G_n}(\Xbm)/ \mathcal{D}_{2n}$ as a linear combination of the independent functions $\exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) g_{G_{n}}(\Xbm)$, $\Xbm^{(u)} \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) g_{G_{n}}(\Xbm)$, and $\Xbm^{(u)} \Xbm^{(v)} \exp((B\sigma_1(\prompt_{*,j}))^{\top}\Xbm) g_{G_{n}}(\Xbm)$, whose elements are respectively $N_{1,n}(\prompt_{*,j})/\mathcal{D}_{2n}$, $N_{2,n}(\prompt_{*,j})^{(u)}/\mathcal{D}_{2n}$, and $N_{3,n}(\prompt_{*,j})^{(uv)}/\mathcal{D}_{2n}$ for any $1 \leq u,v \leq d$ and $1 \leq j \leq L$.

\paragraph{Step 2 - Non-vanishing coefficients.} 
From the results of equations~\eqref{eq:main_equation_expression_nonlinear} and \eqref{eq:main_equation_expression_nonlinear_2}, we can express $[\widetilde{Q}_{n}(\Xbm)-\sum_{i=1}^{5}\widetilde{R}_{n,i}(\Xbm)]/ (\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$ as a combination of the linearly independent terms 
 \begin{align*}
& \exp((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm), \ (\bar{B}^{\top} \Xbm)^{(u)} \exp((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm), \\ 
& (\bar{B}^{\top}\Xbm)^{(u)} (\bar{B}^{\top}\Xbm)^{(v)} \exp((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm), \ \exp((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm) g_{G_n}(\Xbm), \\
& (\bar{B}^{\top} \Xbm)^{(u)} \exp((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm) g_{G_n}(\Xbm), \ (\bar{B}^{\top}\Xbm)^{(u)} (\bar{B}^{\top}\Xbm)^{(v)} \exp((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm) g_{G_n}(\Xbm),\\
&[1-\tanh^2(\alpha_*)]\cdot \Big[\sum_{k' = 1}^{L} \exp((\bar{B}\sigma(\prompt_{*,k'}))^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot g_{G_*}(\Xbm),
\end{align*}
for any $1 \leq j \leq L$ and $1 \leq u, v \leq d$. 

 Our claim is that at least one of the coefficients of these linearly independent terms in the formulation of $[\widetilde{Q}_{n}(\Xbm)-\sum_{i=1}^{5}R_{n,i}(\Xbm)]/ (\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$ does not go to 0 as $n \to \infty$. Assume by contrary that this claim does not hold, which means that all the coefficients of these linearly independent terms go to 0 as $n \to \infty$. Therefore, as $n \to \infty$ we obtain that 
 \begin{align*}
     & \mathcal{L}_{1,n}(\prompt_{*,j})/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \mathcal{L}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \bar{\mathcal{L}}_{1,n}(\prompt_{*,j})/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \\
     & \bar{\mathcal{L}}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \bar{\mathcal{L}}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \bar{\mathcal{L}}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \\
     & \bar{\mathcal{L}}_{3,n}(\prompt_{*,j})^{(uv)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \mathcal{N}_{1,n}(\prompt_{*,j})/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \bar{\mathcal{N}}_{1,n}((\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \\
     & \bar{\mathcal{N}}_{2,n}(\prompt_{*,j})^{(uv)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0, \ \bar{M}_{n,j,0_{d}}/\mathcal{D}_{n} \to 0, \ (\alpha_n-\alpha_*)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0
\end{align*} 
for any $1 \leq u,v \leq d$ and $1 \leq j \leq L$. 

Since $(\alpha_n-\alpha_*)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, we obtain that
\begin{align}
    \label{eq:alpha_converge_nonlinear}
    \frac{|\alpha_n-\alpha_*|}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}\to0.
\end{align}
Furthermore, as $\alpha_n \not\to0$ as $n\to\infty$, we have $1/\tanh(\alpha_n)\not\to\infty$. Given that
$\bar{M}_{n,j,0_{d}}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$, it demonstrates that
\begin{align*}
    \frac{|\sum_{i\in\mathcal{C}_j}\exp(\bar{b}_{n,i})-\exp(\bar{b}_{*,j})|}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}=\frac{1}{\tanh(\alpha_n)}\cdot\frac{|\bar{M}_{n,j,0_{d}}|}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}  \to 0,
\end{align*}
for any $1 \leq j \leq L$. By varying the index $j$ from 1 to $L$ in these limits and summing them up, we achieve that
\begin{align}
\frac{\sum_{j = 1}^{L} |\sum_{i\in\mathcal{C}_j}\exp(\bar{b}_{n,i})-\exp(\bar{b}_{*,j})|}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to 0. \label{eq:key_limits_first_nonlinear}
\end{align}
Now, we first consider indices $j \in [L]$ such that its corresponding Voronoi cell $\mathcal{C}_{j}$ satisfying $|\mathcal{C}_j | = 1$. From the hypothesis, $\mathcal{L}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$. Therefore, $\bar{M}_{n,j,1_{u}}/ (\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$, which leads to
\begin{align*}
    \frac{\sum_{i \in \mathcal{C}_{j}} \exp(\bar{b}_{n,i})\|\Delta \prompt_{n,ij}\|}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}=\frac{\sum_{u = 1}^{d} |\bar{M}_{n,j,1_{u}}|}{\tanh(\alpha_n)(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to 0.
\end{align*}
The above limit indicates that
\begin{align}
    \label{eq:prompt_converge_1_nonlinear}
    \frac{\sum_{j: |\mathcal{C}_{j}| = 1} \sum_{i \in \mathcal{C}_{j}} \exp(\bar{b}_{n,i}) \|\Delta \prompt_{n,ij}\|}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to 0. 
\end{align}
Moving to indices $j \in [L]$ such that their corresponding Voronoi cells $\mathcal{C}_{j}$ satisfying $|\mathcal{C}_{j}| > 1$. The limit $\bar{\mathcal{L}}_{3,n}(\prompt_{*,j})^{(uu)}/ (\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$ leads to 
\begin{align*}
    \frac{\sum_{i \in \mathcal{C}_{j}} \exp(\bar{b}_{n,i})\|\Delta \prompt_{n,ij}\|^2}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}= \frac{\sum_{u = 1}^{d}  \bar{\mathcal{L}}_{3,n}(\prompt_{*,j})^{(uu)}}{\tanh(\alpha_n)(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to 0. 
\end{align*}
The above limit demonstrates that
\begin{align}
    \label{eq:prompt_converge_2_nonlinear}
    \frac{\sum_{j: |\mathcal{C}_{j}| > 1} \sum_{i \in \mathcal{C}_{j}} \exp(\bar{b}_{n,i}) \|\Delta \prompt_{n,ij}\|^2}{(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to 0. 
\end{align}
Collecting all the limits in equations~\eqref{eq:alpha_converge_nonlinear}, \eqref{eq:key_limits_first_nonlinear}, \eqref{eq:prompt_converge_1_nonlinear}, and \eqref{eq:prompt_converge_2_nonlinear}, we obtain that
\begin{align*}
    1 = \frac{\mathcal{D}_{n}+|\alpha_n-\alpha_*|}{\mathcal{D}_{n}+|\alpha_n-\alpha_*|} \to 0
\end{align*}
as $n \to \infty$, which is a contradiction. 
As a consequence, not all of the coefficients of the linearly independent terms in $[\widetilde{Q}_{n}(\Xbm)-\sum_{i=1}^{5}\widetilde{R}_{n,i}(\Xbm)]/ (\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$ go to 0 as $n \to \infty$. 

\paragraph{Step 3 - Application of the Fatou’s lemma.} We denote $m_n$ as the maximum of the absolute values of $\mathcal{L}_{1,n}'(\prompt_{*,j})/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\mathcal{L}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\bar{\mathcal{L}}_{1,n}'(\prompt_{*,j})/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\bar{\mathcal{L}}_{2,n}(\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\bar{\mathcal{L}}_{3,n}(\prompt_{*,j})^{(uv)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\mathcal{N}_{1,n}(\prompt_{*,j})/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\bar{\mathcal{N}}_{1,n}((\prompt_{*,j})^{(u)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\bar{\mathcal{N}}_{2,n}(\prompt_{*,j})^{(uv)}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, $\bar{M}_{n,j,0_{d}}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$, and $(\alpha_n-\alpha_*)/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)$ for all $1 \leq u, v \leq d$. From the result of Step 2 in the proof, we have $1/m_n \not \to \infty$ as $n \to \infty$.

Recall that $\normf{g_{G_n,\alpha_n}-g_{G_*,\alpha_*}}/(\mathcal{D}_{n}+|\alpha_n-\alpha_*|) \to 0$ as $n \to \infty$, which
indicates that $\normf{g_{G_n,\alpha_n}-g_{G_*,\alpha_*}}/(m_{n} (\mathcal{D}_{n}+|\alpha_n-\alpha_*|)) \to 0$. Furthermore, since the $L^2(\mu)$ norm is equivalent to the $L^1(\mu)$ norm, we have $\|g_{G_n,\alpha_n}-g_{G_*,\alpha_*}\|_{L^1(\mu)}/(m_{n} (\mathcal{D}_{n}+|\alpha_n-\alpha_*|)) \to 0$. An application of Fatou's lemma leads to
\begin{align*}
    0=\lim_{n \to \infty} \dfrac{\|g_{G_n,\alpha_n}-g_{G_*,\alpha_*}\|_{L^1(\mu)}}{m_n(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \geq  \int \liminf_{n \to \infty} \dfrac{\left\| g_{G_n,\alpha_n}(\Xbm)-g_{G_*,\alpha_*}(\Xbm)\right\|_1}{m_n(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}d\mu(\Xbm) \geq 0.
\end{align*}
It indicates that $\liminf_{n \to \infty} \dfrac{\left\| g_{G_n,\alpha_{n}}(\Xbm)-g_{G_*, \alpha_{*}}(\Xbm)\right\|_1}{m_n(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} = 0$ for almost surely $\Xbm$. As $n \to \infty$, we denote
\begin{align*}
    & \dfrac{\mathcal{L}_{1,n}'(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \widehat{\alpha}_{j}, \quad \dfrac{\mathcal{L}_{2,n}(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \beta_{j}, \\
    & \dfrac{\bar{\mathcal{L}}_{1,n}'(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \bar{\alpha}_{j}, \quad \dfrac{\bar{\mathcal{L}}_{2,n}(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \bar{\beta}_{j}, \quad \dfrac{\bar{\mathcal{L}}_{3,n}(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \bar{\gamma}_{j}, \\
    & \dfrac{\bar{M}_{n,j,0_{d}}}{m_n(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \tilde{\alpha}_{j}, \quad \dfrac{\mathcal{N}_{1,n}(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \tilde{\beta}_{j}, \\
    & \dfrac{\bar{\mathcal{N}}_{1,n}(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \widehat{\beta}_{j}, \quad \dfrac{\bar{\mathcal{N}}_{2,n}(\prompt_{*,j})}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} \to \widehat{\gamma}_{j}, \quad \dfrac{\alpha_n-\alpha_*}{m_{n}(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}\to\tau
\end{align*}
for any $1 \leq j \leq L$. Here, from the definition of $m_{n}$, at least one coefficient among $\{\widehat{\alpha}_{j}, \beta_{j}, \tilde{\alpha}_{j}, \tilde{\beta}_{j}\}_{j: |\mathcal{C}_{j}| = 1}$, $\{\bar{\alpha}_{j}, \bar{\beta}_{j}, \bar{\gamma}_{j}, \tilde{\alpha}_{j}, \widehat{\beta}_{j}, \widehat{\gamma}_{j}\}_{j: |\mathcal{C}_{j}| > 1}$, and $\tau$ is different from 0. Then, the equation
\begin{align*}
    \liminf_{n \to \infty} \dfrac{\|\widetilde{Q}_n(\Xbm)\|_1}{m_n(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)}=\Big[\sum_{k' = 1}^{L} \exp((\bar{B}\sigma(\prompt_{*,k'}))^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot\liminf_{n \to \infty} \dfrac{\left\| g_{G_n,\alpha_n}(\Xbm)-g_{G_*,\alpha_*}(\Xbm)\right\|_1}{m_n(\mathcal{D}_{n}+|\alpha_n-\alpha_*|)} = 0
\end{align*}
leads to
\begin{align*}
    & \sum_{j:|\mathcal{C}_{j}| = 1} \exp((\bar{B} \sigma(\prompt_{*,j}))^{\top}\Xbm) (\alpha_{j} + \beta_{j}^{\top} (\bar{B}^{\top} \Xbm) \bigr) \nonumber \\
    & + \sum_{j:|\mathcal{C}_{j}| > 1} \exp((\bar{B} \sigma(\prompt_{*,j}))^{\top}\Xbm) \big[\bar{\alpha}_{j} + \bar{\beta}_{j}^{\top} (B^{\top} \Xbm) + (\bar{B}^{\top}\Xbm)^{\top} \bar{\gamma}_{j} (\bar{B}^{\top} \Xbm) \big] \nonumber \\
    & - \sum_{j:|\mathcal{C}_{j}| = 1} \exp((\bar{B} \sigma(\prompt_{*,j}))^{\top}\Xbm) (\tilde{\alpha}_{j} + \tilde{\beta}_{j}^{\top} (\bar{B}^{\top}\Xbm)) f_{G_{*}}(\Xbm) \nonumber \\
    & - \sum_{j:|\mathcal{C}_{j}| > 1} \exp((\bar{B} \sigma(\prompt_{*,j}))^{\top}\Xbm) \big[\tilde{\alpha}_{j} + \widehat{\beta}_{j}^{\top} (\bar{B}^{\top} \Xbm) + (\bar{B}^{\top}\Xbm)^{\top} \widehat{\gamma}_{j} \bar{B}^{\top} \Xbm \big]g_{G_{*}}(\Xbm) \\
    &+\tau[1-\tanh^2(\alpha_*)]\cdot \Big[\sum_{k' = 1}^{L} \exp((\bar{B}\sigma(\prompt_{*,k'}))^{\top}\Xbm+\bar{b}_{*,k'})\Big]\cdot g_{G_*}(\Xbm)= \zerod
\end{align*}
for almost surely $\Xbm$. That equation only holds if and only if all the coefficients $\{\widehat{\alpha}_{j}, \beta_{j}, \tilde{\alpha}_{j}, \tilde{\beta}_{j}\}_{j: |\mathcal{C}_{j}| = 1}$, $\{\bar{\alpha}_{j}, \bar{\beta}_{j}, \bar{\gamma}_{j}, \tilde{\alpha}_{j}, \widehat{\beta}_{j}, \widehat{\gamma}_{j}\}_{j: |\mathcal{C}_{j}| > 1}$, and $\tau$ are 0, which is a contradiction. 

It indicates that we indeed have the conclusion of the local part, namely, $$\lim_{\varepsilon\to0} \inf_{G\in\mathcal{G}_{L'}(\Omega): (\mathcal{D}(G,G_*)+|\alpha-\alpha_*|)\leq \varepsilon} \normf{g_{G,\alpha}-g_{G_*,\alpha_*}}/(\mathcal{D}(G,G_*)+|\alpha-\alpha_*|) >0.$$

As a consequence, we obtain the conclusion of the theorem.

\paragraph{Proof for the identifiability property.} We now
demonstrate that if $g_{\bar{G}, \bar{\alpha}}(\Xbm) = g_{\bar{G}_*, \bar{\alpha}_{*}}(\Xbm)$ for almost every $\Xbm$, then we obtain that $(\bar{G}, \bar{\alpha})  \equiv  (\bar{G}_*, \bar{\alpha}_{*})$.

To ease the presentation we denote the following notations:
\begin{align*}
    \softmax_{\bar{G}}^{\text{Pretrain}}(u)&=\dfrac{\exp(u)}{\sum_{k = 1}^{N}\exp(\Xbm^{\top}\bar{A}^0_{k}\Xbm+\bar{a}^0_{k})},\\
    \softmax_{\bar{G}}^{\text{Prompt}}(u')&=\dfrac{\exp(u)}{\sum_{j'=1}^{L'}\exp((\bar{B}\sigma(\prompt_{j'}))^{\top}\Xbm+\bar{b}_{j'})},\\
    \softmax_{\bar{G}_*}^{\text{Pretrain}}(u_*)&=\dfrac{\exp(u_*)}{\sum_{k = 1}^{N}\exp(\Xbm^{\top}\bar{A}^0_{k}\Xbm+\bar{a}^0_{k})}, \\
    \softmax_{\bar{G}_*}^{\text{Prompt}}(u_*')&=\dfrac{\exp(u_*)}{\sum_{j'=1}^{L}\exp((\bar{B}\sigma(\prompt_{*,j'}))^{\top}\Xbm+\bar{b}_{*,j'})}.
\end{align*}
Here, $u$, $u'$, $u_{*}$, and $u_{*}'$ in these equations satisfy:
\begin{align*}
    u &\in \{\Xbm^{\top} \bar{A}^0_j\Xbm+ \bar{a}^0_j: j \in [N] \}, \ u' \in \{(\bar{B} \sigma(\prompt_{j'}))^{\top}\Xbm+ \bar{b}_{j'}: j' \in [L']\} \\
    u_* &\in \{\Xbm^{\top} \bar{A}^0_j\Xbm+\bar{a}^0_j: j \in [N]\}, \ u_*' \in \{(\bar{B} \sigma(\prompt_{*,j'}))^{\top}\Xbm+ \bar{b}_{*,j'}: j' \in [L]\}.
\end{align*}
The equation $g_{\bar{G}, \bar{\alpha}}(\Xbm) = g_{\bar{G}_*, \bar{\alpha}_{*}}(\Xbm)$ for almost every $\Xbm$ indicates that
\begin{align}
    & \sum_{j=1}^{N}\softmax_{\bar{G}}^{\text{Pretrain}}(\Xbm^{\top} \bar{A}^0_j\Xbm+ \bar{a}^0_j))h(\Xbm,\bar{\eta}^0_j) + \tanh(\bar{\alpha}) \sum_{j' = 1}^{L'} \softmax_{\bar{G}}^{\text{Prompt}}((\bar{B} \sigma(\prompt_{j'}))^{\top}\Xbm+ \bar{b}_{j'})\bar{C} \sigma(\prompt_{j'})  \nonumber \\
&  = \sum_{j=1}^{N}\softmax_{\bar{G}_*}^{\text{Pretrain}}(\Xbm^{\top} \bar{A}^0_j\Xbm+\bar{a}^0_j))h(\Xbm,\bar{\eta}^0_j) + \tanh(\bar{\alpha}_{*})\sum_{j' = 1}^{L} \softmax_{{\bar{G}}_*}^{\text{Prompt}}((\bar{B} \sigma(\prompt_{*,j'}))^{\top} \Xbm+\bar{b}_{*,j'})\bar{C} \sigma(\prompt_{*,j'}).
\label{eq:identify_proof_first_nonlinear}
\end{align}
The above equation only holds when $L = L'$. Furthermore, we also have that
\begin{align*}
    \{\softmax_{\bar{G}}^{\text{Prompt}}((\bar{B} \sigma(\prompt_{j'}))^{\top}\Xbm+\bar{b}_{j'}):j' \in [L']\} =\{\softmax_{\bar{G}_*}^{\text{Prompt}}((\bar{B} \sigma(\prompt_{*,j'}))^{\top}\Xbm+\bar{b}_{*,j'}):j' \in [L]\},
\end{align*}
for almost every $\Xbm$. By relabelling the indices, we can assume that
\begin{align*}  \softmax_{\bar{G}}^{\text{Prompt}}((\bar{B} \sigma(\prompt_{j'}))^{\top}\Xbm+\bar{b}_{j'}) =\softmax_{\bar{G}_*}^{\text{Prompt}}((\bar{B} \sigma(\prompt_{*,j'}))^{\top}\Xbm+\bar{b}_{*,j'}),
\end{align*}
for almost every $\Xbm$ and any $j' \in [L]$. From the translation invariant property of the softmax function, the above equations only hold when $\bar{b}_{j'}=\bar{b}_{*,j'}+ \bar{r}$ for some $\bar{r} \in \mathbb{R}$ and any $j' \in [L]$. Given these results, equation~(\ref{eq:identify_proof_first_nonlinear}) leads to
\begin{align}
     \tanh(\bar{\alpha}) \sum_{j = 1}^{L}\exp{(\bar{b}_{j})}\exp{((\bar{B}\sigma(\prompt_{j}))^{\top}\Xbm)}\bar{C}\sigma(\prompt_{j}) = \tanh(\bar{\alpha}_{*})\sum_{j = 1}^{L}\exp{(\bar{b}_{*,j})}\exp{((\bar{B}\sigma(\prompt_{*,j}))^{\top}\Xbm)} \bar{C} \sigma(\prompt_{*,j}),    \label{eq:identify_proof_second_nonlinear}
\end{align}
for almost surely $\Xbm$.


Now, we partition the set $\{1,2, \ldots, L\}$ into $m$ subsets $\bar{K}_1, \bar{K}_2,\ldots,\bar{K}_m$ where $m\leq L$, such that $\exp{(\bar{b}_{j})}=\exp{(\bar{b}_{*,j'})}$ for any $j,j'\in \bar{K}_i$ and $i \in [m]$. It is clear that $\exp{(\bar{b}_{j})}\neq \exp{(\bar{b}_{*,j'})}$ when $j, j'$ belong to different subsets $\bar{K}_i$. Collecting these results, equation~(\ref{eq:identify_proof_second}) can be rewritten as follows:
\begin{align*}
    \tanh(\bar{\alpha}) \sum_{i = 1}^{m}\sum_{j \in \bar{K}_i}\exp{(\bar{b}_{j})}\exp{((\bar{B} \sigma(\prompt_{j}))^{\top}\Xbm)}\bar{C} \sigma(\prompt_{j}) & \nonumber \\
& \hspace{-5 em} = \tanh(\bar{\alpha}_{*}) \sum_{i = 1}^{m}\sum_{j \in \bar{K}_i}\exp{(\bar{b}_{*,j})}\exp{((\bar{B} \sigma(\prompt_{*,j}))^{\top}\Xbm)}\bar{C} \sigma(\prompt_{*,j}),
\end{align*}
for almost surely $\Xbm$. Hence, we achieve that
\begin{align*}
    \{((\bar{B} \sigma(\prompt_{j}))^{\top}, \prompt_{j}): j \in \bar{K}_i\} = \{((\bar{B} \sigma(\prompt_{*,j}))^{\top}, \prompt_{*,j}): j \in \bar{K}_i\} \ \ \text{and} \ \  \tanh(\bar{\alpha}) = \tanh(\bar{\alpha}_{*}).
\end{align*}
It naturally leads to 
\begin{align*}
    \{\sigma(\prompt_{j}): j \in \bar{K}_i\} = \{\sigma(\prompt_{*,j}): j \in \bar{K}_i\}.
\end{align*}
Without loss of generality, $\prompt_{j}=\prompt_{*,j}$ for all $j \in \bar{K}_i$. As a consequence, we obtain that $\bar{\alpha} = \bar{\alpha}_{*}$ and 
\begin{align*}
    \sum_{i = 1}^{m}\sum_{j \in \bar{K}_i}\exp{(\bar{b}_{j})}\delta_{\prompt_{j}} = \sum_{i = 1}^{m}\sum_{j \in \bar{K}_i}\exp{(\bar{b}_{*,j})}\delta_{\prompt_{*,j}}.
\end{align*}
It is equivalent to $(\bar{G}, \bar{\alpha})  \equiv  (\bar{G}_*, \bar{\alpha}_{*})$. We achieve the conclusion of the identifiability claim.



\subsection{Proof of Proposition~\ref{theorem:regression_estimation}}
\label{appendix:regression_estimation}
Recall that the i.i.d sample $(\Xbm_1,Y_1), (\Xbm_2,Y_2),\ldots,(\Xbm_n,Y_n)\in\mathbb{R}^{d} \times\mathbb{R}^{d'}$ are generated from the model:
\begin{align*}
    Y_i=f_{G_*, 
    \alpha_{*}}(\Xbm_i)+\varepsilon_i, \quad i=1,2,\ldots,n, 
    %\label{eq:regression_model}
\end{align*}
where $\varepsilon_1,\ldots,\varepsilon_n$ are independent Gaussian noise variables such that $\bbE[{\varepsilon_{i}}|\Xbm_i] = 0$ and $\var(\varepsilon_{i}|\Xbm_i) = \sigma^2 I_{d'}$ for all $1 \leq i \leq n$. Since $\varepsilon_i|\Xbm_i\sim\mathcal{N}(\zerod,\sigma^2I_{d'})$, a least square estimator $(\widehat{G}_n,\widehat{\alpha}_n)$ defined as
\begin{align*}
    %\label{eq:least_squared_estimator_overspecified}
    (\widehat{G}_n, \widehat{\alpha}_{n}) :=\argmin_{G\in\mathcal{G}_{L'}(\Theta), \alpha \in \Omega}\sum_{i=1}^{n} \|Y_i-f_{G, \alpha}(\Xbm_i)\|^2,
\end{align*}
is exactly a maximum likelihood estimator given by
\begin{align*}
    (\widehat{G}_n,\widehat{\alpha}_n)\in\argmax_{G\in\mathcal{G}_{L'}(\Theta), \alpha \in \Omega}\frac{1}{n}\sum_{i=1}^{n}\log(\pi(Y_i|f_{G,\alpha}(\Xbm_i),\sigma^2I_{d'})),
\end{align*}
where $\pi(Y_i|f_{G,\alpha}(\Xbm_i),\sigma^2I_{d'})$ denotes the probability density function of the multivariate Gaussian distribution with mean vector $f_{G,\alpha}(\Xbm)$ and covariance matrix $\sigma^2I_{d'}$. Furthermore, it follows from the result in \cite{vandeGeer-00} that
\begin{align*}
    h(\pi(Y|f_{\widehat{G}_n,\widehat{\alpha}_n}(\Xbm),\sigma^2I_{d'}),\pi(Y|f_{G_*,\alpha_*}(\Xbm),\sigma^2I_{d'}))=\mathcal{O}_P(\sqrt{\log(n)/n}).
\end{align*}
According to Pardo et al. \cite{pardo2018statistical}, we have
\begin{align*}
    h^2(\pi(Y|\theta_1,\Sigma_1),\pi(Y|\theta_2,\Sigma_2))=1-\dfrac{\det(\Sigma_1)^{1/4}\det(\Sigma_2)^{1/4}}{\det\Big(\frac{1}{2}\Sigma_1+\frac{1}{2}\Sigma_2\Big)^{1/2}}\exp\Bigg\{-\frac{1}{8}(\theta_1-\theta_2)^{\top}\Big(\frac{1}{2}\Sigma_1+\frac{1}{2}\Sigma_2\Big)^{-1}(\theta_1-\theta_2)\Bigg\}.
\end{align*}
Therefore, we deduce that
\begin{align*}
    h^2(\pi(Y|f_{\widehat{G}_n,\widehat{\alpha}_n}(\Xbm),\sigma^2I_{d'}),\pi(Y|f_{G_*,\alpha_*}(\Xbm),\sigma^2I_{d'}))=1-\exp\Bigg\{-\frac{1}{8\sigma^2}\|f_{\widehat{G}_n,\widehat{\alpha}_n}(\Xbm)-f_{G_*,\alpha_*}(\Xbm)\|^2\Bigg\}.
\end{align*}
As a result, it follows that
\begin{align*}
    1-\exp\Bigg\{-\frac{1}{8\sigma^2}\|f_{\widehat{G}_n,\widehat{\alpha}_n}(\Xbm)-f_{G_*,\alpha_*}(\Xbm)\|^2\Bigg\}=\mathcal{O}_P(\log(n)/n).
\end{align*}
This means that with probability one, there exists some constant $C>0$ and a natural number $n_0$ such that
\begin{align*}
    &1- C\log(n_{0})/n_{0}\leq \exp\Bigg\{-\frac{1}{8\sigma^2}\|f_{\widehat{G}_n,\widehat{\alpha}_n}(\Xbm)-f_{G_*,\alpha_*}(\Xbm)\|^2\Bigg\}. 
\end{align*}
Assume that the natural number $n_0$ is large enough so that $1-C\log(n)/n\geq 1/2$ is true for all $n\geq n_0$. Then, the above inequality is equivalent to
\begin{align*}
    \|f_{\widehat{G}_n,\widehat{\alpha}_n}(\Xbm)-f_{G_*,\alpha_*}(\Xbm)\|^2&\leq 8\sigma^2\log\Big(\dfrac{1}{1-C\log(n)/n}\Big)\\
    &=8\sigma^2\log\Big(1+\dfrac{C\log(n)/n}{1-C\log(n)/n}\Big)\\
    &\leq 8\sigma^2\cdot\dfrac{C\log(n)/n}{1-C\log(n)/n}\\
    &\leq 16\sigma^2C\log(n)/n,
\end{align*}
which implies that
\begin{align*}
    \|f_{\widehat{G}_n,\widehat{\alpha}_n}(\Xbm)-f_{G_*,\alpha_*}(\Xbm)\|=\mathcal{O}_P(\sqrt{\log(n)/n}).
\end{align*}
Consequently, we have
\begin{align*}
    \normf{f_{\widehat{G}_n,\widehat{\alpha}_n}-f_{G_*,\alpha_*}}=\Big(\int_{\mathcal{X}}\|f_{\widehat{G}_n,\widehat{\alpha}_n}(\Xbm)-f_{G_*,\alpha_*}(\Xbm)\|^2\dint \mu(\Xbm)\Big)^{1/2}=\mathcal{O}_P(\sqrt{\log(n)/n}).
\end{align*}
Hence, the proof is completed.

\section{Additional Related Works}
\label{sec:add_related_works}
\textbf{Theory of Mixture of Experts.} 
While there is a surge of interest in employing MoE to scale up the model capacity, a theoretical foundation for that model has not been fully understood. Firstly, from a probabilistic perspective, a line of research on the Gaussian MoE has been investigated in \cite{ho2022gaussian,nguyen2023demystifying,yan2025contaminated} where the convergence behavior of maximum likelihood expert estimation was comprehensively analyzed, implying optimal expert structures for training MoE. Next, the expert convergence behavior was revisited in \cite{chen2022theory,nguyen2024general} but under the settings where MoE was utilized for classification tasks by formulating each expert as a classifier. In terms of MoE theory for deep learning, \cite{han2024fusemoe} leveraged MoE in multi-modal learning where each expert is trained to specialize in processing one or a few data modalities such as time series, text or images. Meanwhile, \cite{pham2024competesmoe} attempted to propose a competition routing policy to train sparse MoE effectively for a language modeling task, accompanied by a theoretical analysis to justify their method. Lastly, theories for the applications of MoE in domain adaptation and continual learning have also been provided in \cite{nguyen2025cosine} and \cite{li2024continuallearning}, respectively. However, to the best of our knowledge, the theory for MoE in zero-initialized attention has not been explored yet.

\section{Additional Experimental Details}
\label{sec:additional_experiments}
\subsection{Datasets Description}
We first fine-tune LLaMA-Adapter with all prefix-tuning settings on Alpaca dataset, which include 52K instruction-following data for training. Then, we evaluate our experiments on LLM benchmarks, including AI2 Reasoning Challenge (ARC), HellaSwag,
MMLU, and TruthfulQA. The statistics of 4 datasets about testing subset are summarized in detail in Table \ref{tab:data_stat}.

\begin{itemize}
    \item ARC dataset is a multiple-choice question-answering dataset which contain science questions in exams from grade 3 to grade 9. It has two types: Easy and Challenge. In this paper, we report performance on both types and average accuracy.
    \item HellaSwag provides multiple-choice questions to evaluate commonsense NLI of LLMs. Given a paragraph which is incomplete, the model need to find the suitable option to complete it.
    \item MMLU is a benchmark that covers 57 subjects across STEM, the social sciences, humanities, and more through multiple-choice questions. This dataset test the model on both world knowledge and problem solving ability. Its subjects range from traditional areas such as mathematics to more specialized areas like law and ethics.
    \item TruthfulQA measure whether the LLMs is truthful in generating answers given questions. It comprises 817 questions with each question has two types, generative questions and multiple-choice questions. In this paper, we evaluate all settings on multiple-choice questions.
\end{itemize}
\vspace{-0.2in}
\begin{table}[H]
\centering
\caption{Statistics of 4 LLM benchmarks about testing subset.}
\vspace{0.1in}
\label{tab:data_stat}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
        & ARC (Easy) & ARC (Challenge) & HellaSwag & MMLU  & TruthfulQA \\ \midrule
Testing & 2376       & 1172            & 10042     & 14042 & 817        \\ \bottomrule
\end{tabular}%
}
\end{table}

% \subsection{Implementation Details}
% In our experiments on open LLM benchmarks, we first fine-tune on Alpaca dataset and adopt hyperparameter configuration as in \cite{zhang2024llama}, where the values of epochs, warmup epochs, weight decay, and adapter length are 5, 2, 0.02, and 10, respectively. We also use a total batch size of 16 for LLaMA-7B and 8 for LLaMA-13B, with prefix-tuning applied to last 30 layers in LLaMA-7B and last 38 layers in LLaMA-13B, respectively. AdamW optimizer is used to optimize all backbone settings with learning rate of 9e-3. All experiments were implemented in PyTorch and executed on NVIDIA A100-80GB GPUs.
\subsection{Prompt Templates}
We also provide the prompt templates that we use for all settings to evaluate on ARC, MMLU, and TruthfulQA benchmarks in Figure \ref{fig:arc_prt}, \ref{fig:mmlu_prt}, and \ref{fig:truth_prt}, respectively. These templates are based on the fact that Alpaca Fully Fine-tuning and LLaMA-Adapter both use this prompt template structure in training and we custom a little bit for each dataset.
\begin{figure}[H] % Use [H] to keep the figure in place (requires float package)
    \centering
    \resizebox{0.7\textwidth}{!}{
    \begin{tcolorbox}[colback=white, colframe=black,
    boxrule=0.5pt, % Adjust the thickness of the frame
     % Adjust the font size of the content
    title=ARC Prompt Template
    ]

    \texttt{Below is an instruction that describes a task. Write a response that appropriately completes the multiple-choice question. \\ \\ 
    \#\#\# Instruction: \\ \\
    Question: <question> \\ \\
    Options: 
    \begin{enumerate}[label=-]
    \setlength\itemsep{0pt} % Adjust the spacing between items
    \item Option 1
    \item Option 2
    \item Option 3
    \item Option 4
    \item ...
    \end{enumerate}
    \#\#\# Response:
    }

    \end{tcolorbox}}
    \vspace{-0.1in}
    \caption{Prompt templates for ARC dataset}
    \label{fig:arc_prt}
\end{figure}

\begin{figure}[H] % Use [H] to keep the figure in place (requires float package)
    \centering
    \resizebox{0.7\textwidth}{!}{
    \begin{tcolorbox}[colback=white, colframe=black,
    boxrule=0.5pt, % Adjust the thickness of the frame
     % Adjust the font size of the content
    title=MMLU Prompt Template
    ]

    \texttt{Below is an instruction that describes a task. Write a response that appropriately completes the multiple-choice question about \{task\}. \\ \\ 
    \#\#\# Instruction: \\ \\
    Question: <question> \\ \\
    Options: 
    \begin{enumerate}[label=-]
    \setlength\itemsep{0pt} % Adjust the spacing between items
    \item Option 1
    \item Option 2
    \item Option 3
    \item Option 4
    \item ...
    \end{enumerate}
    \#\#\# Response:
    }

    \end{tcolorbox}}
    \vspace{-0.1in}
    \caption{Prompt template for MMLU dataset.}
    \label{fig:mmlu_prt}
\end{figure}

\begin{figure}[H] % Use [H] to keep the figure in place (requires float package)
    \centering
    \resizebox{0.7\textwidth}{!}{
    \begin{tcolorbox}[colback=white, colframe=black,
    boxrule=0.5pt, % Adjust the thickness of the frame
     % Adjust the font size of the content
    title=TruthfulQA Prompt Template
    ]

    \texttt{Below is an instruction that describes a task. Write a response that appropriately completes the multiple-choice question about \{task\}. \\ \\ 
    \#\#\# Instruction: \\ 
    Interpret each multiple-choice question literally, and as a multiple-choice question about the real world; check carefully to see if the question misses the concept or not and research each option and only pick one most suitable option, without falling prey to any common mythss. \\ \\
    Question: <question> \\ \\
    Options: 
    \begin{enumerate}[label=-]
    \setlength\itemsep{0pt} % Adjust the spacing between items
    \item Option 1
    \item Option 2
    \item Option 3
    \item Option 4
    \item ...
    \end{enumerate}
    \#\#\# Response:
    }

    \end{tcolorbox}}
    \vspace{-0.1in}
    \caption{Prompt templates for TruthfulQA datasets.}
    \label{fig:truth_prt}
\end{figure}
\label{sec:exp_appendix}
% To start with, it is necessary to define the notations that will be used throughout this proof. First of all, let us denote by $\mathcal{F}_{L'}(\Theta)$ the set of regression functions w.r.t mixing measures in $\mathcal{G}_{L'}(\Theta)$, that is,
% \begin{align*}
%     \mathcal{F}_{L'}(\Theta):=\{f_{G}(\Xbm):G\in\mathcal{G}_{L'}(\Theta)\}.
% \end{align*}
% Next, for each $\delta>0$, we define the $L^{2}(\mu)$ ball centered around the regression function $f_{G_*}(\Xbm)$ and intersected with the set $\mathcal{F}_{L'}(\Theta)$ as
% \begin{align*}   \mathcal{F}_{L'}(\Theta,\delta):=\left\{f \in \mathcal{F}_{L'}(\Theta): \|f -f_{G_*}\|_{L^2(\mu)} \leq\delta\right\}.
% \end{align*}
% Furthermore, \cite{vandeGeer-00} suggest capturing the size of the above set by using the following quantity:
% \begin{align}
%     \label{eq:bracket_size}
%     \mathcal{J}_B(\delta, \mathcal{F}_{L'}(\Theta,\delta)):=\int_{\delta^2/2^{13}}^{\delta}H_B^{1/2}(t, \mathcal{F}_{L'}(\Theta,t),\|\cdot\|_{L^2(\mu)})~\dint t\vee \delta,
% \end{align}
% in which $H_B(t, \mathcal{F}_{L'}(\Theta,t),\|\cdot\|_{L^2(\mu)})$ denotes the bracketing entropy \cite{vandeGeer-00} of $ \mathcal{F}_{L'}(\Theta,t)$ under the $L^{2}(\mu)$-norm and $t\vee\delta:=\max\{t,\delta\}$. 

% Subsequently, let us introduce a key result of this proof in Lemma~\ref{lemma:density_rate}, which is achieved by applying similar arguments as those in Theorem 7.4 and Theorem 9.2 in \cite{vandeGeer-00}.
% \begin{lemma}
%     \label{lemma:density_rate}
%     Take $\Psi(\delta)\geq \mathcal{J}_B(\delta, \mathcal{F}_{L'}(\Theta,\delta))$ that satisfies $\Psi(\delta)/\delta^2$ is a non-increasing function of $\delta$. Then, for some universal constant $c$ and for some sequence $(\delta_n)$ such that $\sqrt{n}\delta^2_n\geq c\Psi(\delta_n)$, we achieve that
%     \begin{align*}
%         \mathbb{P}\Big(\|f_{G_n} - f_{G_*}\|_{L^2(\mu)} > \delta\Big)\leq c \exp\left(-\frac{n\delta^2}{c^2}\right),
%     \end{align*}
%     for all $\delta\geq \delta_n$.
% \end{lemma}

% \textbf{General picture.} We begin with deriving the bracketing entropy inequality
% \begin{align}    
% H_B(\varepsilon,\mathcal{F}_{L'}(\Theta),\|\cdot\|_{L^{2}(\mu)}) \lesssim \log(1/\varepsilon), \label{eq:bracket_entropy_bound}
% \end{align}
% for any $0 < \varepsilon \leq 1/2$. Then, it follows that
% \begin{align}
%     \label{eq:bracketing_integral}
%     \mathcal{J}_B(\delta, \mathcal{F}_{L'}(\Theta,\delta))= \int_{\delta^2/2^{13}}^{\delta}H_B^{1/2}(t, \mathcal{F}_{L'}(\Theta,t),\normf{\cdot})~\dint t\vee \delta\lesssim \int_{\delta^2/2^{13}}^{\delta}\log(1/t)dt\vee\delta.
% \end{align}
% Let $\Psi(\delta)=\delta\cdot[\log(1/\delta)]^{1/2}$, then $\Psi(\delta)/\delta^2$ is a non-increasing function of $\delta$. Additionally, equation~(\ref{eq:bracketing_integral}) indicates that $\Psi(\delta)\geq \mathcal{J}_B(\delta,\mathcal{F}_{L'}(\Theta,\delta))$. Moreover, by choosing $\delta_n=\sqrt{\log(n)/n}$, we have that $\sqrt{n}\delta^2_n\geq c\Psi(\delta_n)$ for some universal constant $c$. Then, according to Lemma~\ref{lemma:density_rate}, we reach the conclusion of Theorem~\ref{theorem:regression_estimation_linear}.

% % We now demonstrate that when the expert functions are Lipschitz continuous, the following bound holds:
% % \begin{align}    
% % H_B(\varepsilon,\mathcal{F}_{L'}(\Theta),\|.\|_{L^{2}(\mu)}) \lesssim \log(1/\varepsilon), \label{eq:bracket_entropy_bound}
% % \end{align}
% % for any $0 < \varepsilon \leq 1/2$. 
% As a result, it suffices to establish the inequality in equation~(\ref{eq:bracket_entropy_bound}).

% \textbf{Proof of equation~(\ref{eq:bracket_entropy_bound}).} Let $f_{G}$ be an arbitrary regression function in $\mathcal{F}_{L'}(\Theta)$. As the prompts $\prompt_{j'}$ are both bounded, we obtain that $|f_{G}(\Xbm)| \leq M$ for all $\Xbm$ where $M>0$ is some universal constant. 

% Next, let $\tau\leq\varepsilon$ and $\{\pi_1,\ldots,\pi_{\bar{N}}\}$ be the $\tau$-cover under the $L^{\infty}$ norm of the set $\mathcal{F}_{L'}(\Theta)$ in which $\bar{N}:={N}(\tau,\mathcal{F}_{L'}(\Theta),\|\cdot\|_{L^{2}(\mu)})$ is the $\tau$-covering number of the metric space $(\mathcal{F}_k(\Theta),\|\cdot\|_{L^{\infty}(\mu)})$. Then, we construct the brackets of the form $[L_i(\Xbm),U_i(\Xbm)]$ for all $i\in[\bar{N}]$ as follows:
%     \begin{align*}
%         L_i(x)&:=\max\{\pi_i(\Xbm)-\tau,0\},\\
%         U_i(x)&:=\max\{\pi_i(\Xbm)+\tau, M \}.
%     \end{align*}
% It can be verified that $\mathcal{F}_{L'}(\Theta)\subset\cup_{i=1}^{\bar{N}}[L_i(\Xbm),U_i(\Xbm)]$. Furthermore, we also get that
% \begin{align*}
%     \normf{U_i-L_i}=\Big(\int(U_i-L_i)^2\dint\mu(\Xbm)\Big)^{1/2}\leq\Big(\int 4\tau^2\dint\mu(\Xbm)\Big)^{1/2}=2\tau,
% \end{align*}
% From the definition of the bracketing entropy, we have that
% \begin{align}
%     \label{eq:bracketing_covering}
%     H_B(2\tau,\mathcal{F}_{L'}(\Theta),\normf{\cdot})\leq\log \bar{N}=\log {N}(\tau,\mathcal{F}_{L'}(\Theta),\|\cdot\|_{L^{\infty}}).
% \end{align}
% Thus, it is sufficient to establish an upper bound for the covering number $\bar{N}$. For that purpose, we denote $\Delta =\{(b,\prompt)\in\mathbb{R}\times\mathbb{R}^{d}:(b,\prompt)\in\Theta\}$. Since $\Theta$ is a compact set, $\Delta$ is also compact. Thus, there exist $\tau$-covers for $\Delta$, denoted by $\Delta_{\tau}$, respectively. Then, we find that
% \begin{align*}
%     |\Delta_{\tau}|\leq \mathcal{O}(\tau^{-(d+1)L'})).
% \end{align*}
% For each mixing measure $G=\sum_{i=1}^{L'}\exp(b_{i})\delta_{\prompt_{i}}\in\mathcal{G}_{L'}(\Theta)$, we consider a corresponding mixing measure $\check{G}$ defined as
% \begin{align*}
%     \check{G}:=\sum_{i=1}^{L'}\exp(\check{b}_{i})\delta_{\check{\prompt}_{i}},
% \end{align*}
% where  $(\check{b}_{i},\check{\prompt}_{i})\in\Delta_{\tau}$ is the closest to $(b_{i},\prompt_{i})$ in that set. Let us denote 
% \begin{align*}
%     D:&=\sum_{i'=1}^{N}\exp(\Xbm^{\top}A^0_{i'}\Xbm+a^0_{i'})+\sum_{j'=1}^{L'}\exp((B\prompt_{j'})^{\top}\Xbm+b_{j'}),\\
%     \check{D}:&=\sum_{i'=1}^{N}\exp(\Xbm^{\top}A^0_{i'}\Xbm+a^0_{i'})+\sum_{j'=1}^{L'}\exp((B\check{\prompt}_{j'})^{\top}\Xbm+\check{b}_{j'}).
% \end{align*}
% Subsequently, we aim to show that $\normf{f_{G}-f_{\check{G}}}\lesssim\tau$. In particular, we have
% \begin{align*}
%     \normf{f_{G}-f_{\check{G}}}&=\Big\|\sum_{j=1}^{L'}\frac{\exp((B\prompt_{j})^{\top}\Xbm+b_{j})}{D}\cdot C\prompt_{j}-\sum_{j=1}^{L'}\frac{\exp((B\check{\prompt}_{j})^{\top}\Xbm+\check{b}_{j})}{\check{D}}\cdot C\check{\prompt}_{j}\Big\|_{L^2(\mu)}\\
%     &\leq\Big\|\sum_{j=1}^{L'}\frac{\exp((B\prompt_{j})^{\top}\Xbm+b_{j})}{D}\cdot C(\prompt_{j}-\check{\prompt}_{j})\Big\|_{L^2(\mu)}\\
%     &\quad +\Big\|\sum_{j=1}^{L'}\Big[\frac{\exp((B\prompt_{j})^{\top}\Xbm+b_{j})}{D}-\frac{\exp((B\check{\prompt}_{j})^{\top}\Xbm+\check{b}_{j})}{\check{D}}\Big]\cdot C\check{\prompt}_{j}\Big\|_{L^2(\mu)}\\
%     &:=T_1+T_2.
% \end{align*}
% Then, it is sufficient to demonstrate that $T_1\lesssim\tau$ and $T_2\lesssim\tau$, respectively. First of all, we get that
% \begin{align*}
%     T_1^2
%     &=\int\Bigg[\sum_{j=1}^{L'}\frac{\exp((B\prompt_{j})^{\top}\Xbm+b_{j})}{D}\cdot C(\prompt_{j}-\check{\prompt}_{j})\Bigg]^2\dint\mu(\Xbm)\\
%     &\leq L' \int\sum_{j=1}^{L'}\Bigg[\frac{\exp((B\prompt_{j})^{\top}\Xbm+b_{j})}{D}\cdot C(\prompt_{j}-\check{\prompt}_{j})\Bigg]^2~\dint\mu(\Xbm)\\
%     &\leq L'\int\sum_{j=1}^{L'}~[C(\prompt_{j}-\check{\prompt}_{j})]^2~\dint\mu(\Xbm)\lesssim L'\int\sum_{j=1}^{L'}~\tau^2~\dint\mu(\Xbm)\lesssim\tau^2,
% \end{align*}
% which is equivalent to $T_1\lesssim\tau$. Here, the second inequality is according to the Cauchy-Schwarz inequality, the third inequality occurs as the softmax weight is bounded by 1. 

% Next, we have
% \begin{align}
%     T_2^2&=\int\Big[\frac{1}{D}\Big(\sum_{i=1}^{N}\exp(\Xbm^{\top}A_{i}^{0} \Xbm+\coi)h(\Xbm,\eoi)+\sum_{j=1}^{L'}\exp((B\prompt_{j})^{\top}\Xbm+b_{j})C\check{\prompt}_{j}\nonumber\\
%     &\quad-\frac{1}{\check{D}}\Big(\sum_{i=1}^{N}\exp(\Xbm^{\top}A_{i}^{0}\Xbm+\coi)h(\Xbm,\eoi)+\sum_{j=1}^{L'}\exp((B\check{\prompt}_{j})^{\top}\Xbm+\check{b}_{j})C\check{\prompt}_{j}\Big)\Big]^2\dint\mu(\Xbm)\nonumber\\
%     &\leq \frac{1}{2}\int\Big\{\Big[\sum_{i=1}^{N}\Big(\frac{\exp(\Xbm^{\top} A_{i}^{0} \Xbm+\coi)}{D}-\frac{\exp(\Xbm^{\top}A_{i}^{0} \Xbm+\coi)}{\check{D}}\Big)h(\Xbm,\eoi)\Big]^2\nonumber\\
%     &\hspace{2cm}+\Big[\sum_{j=1}^{L'}\Big(\frac{\exp((B\prompt_{j})^{\top}\Xbm+b_{j})}{D}-\frac{\exp((B\check{\prompt}_{j})^{\top}\Xbm+\check{b}_{j})}{\check{D}}\Big)C\check{\prompt}_{j}\Big]^2\Big\}\dint\mu(\Xbm)\nonumber\\
%     &\leq\frac{N}{2}\Big(\frac{1}{D}-\frac{1}{\check{D}}\Big)^2\int\sum_{i=1}^{N}\Big[\exp(\Xbm^{\top}A_{i}^{0} \Xbm+\coi)h(\Xbm,\eoi)\Big]^2\dint\mu(\Xbm)\nonumber\\
%     \label{eq:density_bound}
%     &\hspace{1.5cm}+ \frac{L'}{2}\int\sum_{j=1}^{L'}\Bigg[\Big(\frac{\exp((B\prompt_{j})^{\top}\Xbm +b_{j})}{D}-\frac{\exp((B\check{\prompt}_{j})^{\top}\Xbm+\check{b}_{j})}{\check{D}}\Big)C\check{\prompt}_{j}\Bigg]^2~\dint\mu(\Xbm).
% \end{align}
% Now, we will bound two terms in the above right hand side. Firstly, since both the input space $\mathcal{X}$ and the parameter space $\Theta$ are bounded, we have that
% \begin{align*}
%     \frac{1}{D}-\frac{1}{\check{D}}\lesssim |D-\check{D}|
%     &=\Big|\sum_{j'=1}^{L'}\Big[\exp((B\prompt_{j'})^{\top}\Xbm+b_{j'})-\exp((B\check{\prompt}_{j'})^{\top}\Xbm+\check{b}_{j'})\Big]\Big|\\
%     &\lesssim\sum_{j'=1}^{L'}\Big[\|\prompt_{j'}-\check{\prompt}_{j'}\|\cdot\|\Xbm\|+|b_{j}-\check{b}_{j'}|\Big]\\
%     &\leq k\tau(B+1).
% \end{align*}
% As a result, we deduce that
% \begin{align}
%     \label{eq:first_term_bound}
%     \frac{N}{2}\Big(\frac{1}{D}-\frac{1}{\check{D}}\Big)^2\int\sum_{i=1}^{N}\Big[\exp(\Xbm^{\top}A_{i}^{0} \Xbm+\coi)h(\Xbm,\eoi)\Big]^2\dint\mu(\Xbm)\lesssim \frac{1}{2}N[L'\tau(B+1)]^2.
% \end{align}
% Regarding the second term, note that
% \begin{align*}
%     &\frac{\exp((B\prompt_{j})^{\top}\Xbm+b_{j})}{D}-\frac{\exp((B\check{\prompt}_{j})^{\top}\Xbm+\check{b}_{j})}{\check{D}}\\
%     =&~\exp((B\prompt_{j})^{\top}\Xbm+b_{j})\Big(\frac{1}{D}-\frac{1}{\check{D}}\Big)+\frac{1}{\check{D}}\Big[\exp((B\prompt_{j})^{\top}\Xbm+b_{j})-\exp((B\check{\prompt}_{j})^{\top}\Xbm+\check{b}_{j})\Big].
% \end{align*}
% Since we have
% \begin{align*}
%     \exp((B\prompt_{j})^{\top}\Xbm+b_{j})\Big(\frac{1}{D}-\frac{1}{\check{D}}\Big)&\lesssim \frac{1}{D}-\frac{1}{\check{D}}\lesssim L'\tau(B+1),\\
%     \frac{1}{\check{D}}\Big[\exp((B\prompt_{j})^{\top}\Xbm+b_{j})-\exp((B\check{\prompt}_{j})^{\top}\Xbm+\check{b}_{j})\Big]&\lesssim\Big[\|\prompt_{j}-\check{\prompt}_{j}\|\cdot\|\Xbm\|+|b_{j}-\check{b}_{j}|\Big]\leq \tau(B+1),
% \end{align*}
% it follows that
% \begin{align}
%     \label{eq:second_term_bound}
%     \frac{L'}{2}\int\sum_{j=1}^{L'}\Bigg[\Big(\frac{\exp((B\prompt_{j})^{\top}\Xbm+b_{j})}{D}-\frac{\exp((B\check{\prompt}_{j})^{\top}\Xbm+\check{b}_{j})}{\check{D}}\Big)h(x,\overline{\eta}_{j})\Bigg]^2~\dint\mu(\Xbm)\lesssim \frac{1}{2}(L')^2M^2[\tau(B+1)]^2
% \end{align}
% From \eqref{eq:density_bound}, \eqref{eq:first_term_bound} and \eqref{eq:second_term_bound}, we obtain that $T_2\lesssim\tau$. As a result, we achieve that 
% \begin{align*}
%     \normf{f_{G}-f_{\check{G}}}\leq T_1+T_2\lesssim\tau.
% \end{align*}
% By definition of the covering number, we deduce that
% \begin{align}
%     \label{eq:covering_bound}
%     {N}(\tau,\mathcal{F}_{L'}(\Theta),\|\cdot\|_{L^{\infty}})\leq |\Delta_{\tau}|\leq \mathcal{O}(n^{-(d+1)L'}).
% \end{align}
% Combine equations~\eqref{eq:bracketing_covering} and \eqref{eq:covering_bound}, we achieve that
% \begin{align*}
%     H_B(2\tau,\mathcal{F}_{L'}(\Theta),\normf{\cdot})\lesssim \log(1/\tau).
% \end{align*}
% Let $\tau=\varepsilon/2$, then we obtain that 
% \begin{align*}
%     H_B(\varepsilon,\mathcal{F}_{L'}(\Theta),\|.\|_{L^{2}(\mu)}) \lesssim \log(1/\varepsilon).
% \end{align*}
% Hence, the proof is completed.




\subsection{Visualize question-answering}
We provide in Table \ref{tab:visualize_vqa} illustrations on output of LLaMA-Adapter trained with different prompt-tuning strategy.
\begin{table*}[t]
\centering
\setlength{\extrarowheight}{5pt} % Add extra space between rows
\caption{Visualize some question-answering outputs with different prompt-tuning methods.}
\vskip 0.15in
\begin{tabularx}{\textwidth}{XXX}
\toprule
\texttt{\textbf{Question:} Which best explains what scientists are referring to when they use the term conservation?} \newline
\texttt{\textbf{(a)} nonliving parts of the environment.} \newline
\texttt{\textbf{(b)} living organisms in the environment.} \newline
\texttt{\textbf{(c)} health of the living organisms in the environment.} \newline
\texttt{\textbf{(d)} protection, management, and renewal of resources.} &
\texttt{\textbf{Question:} A pitcher throws a 0.15 kg baseball at 43 40 m/s towards the catcher. What is the momentum of the baseball while moving at 40 m/s?} \newline
\texttt{\textbf{(a)} 0.025 kg x m/s.} \newline
\texttt{\textbf{(b)} 3.8 kg x m/s.} \newline
\texttt{\textbf{(c)} 6.0 kg x m/s.} \newline
\texttt{\textbf{(d)} 270 kg x m/s.} &
\texttt{\textbf{Question:} Part of the east coast of South America and the west coast of Africa have matching fossils within the same series of rock layers. This provides evidence that these two continents were once:} \newline
\texttt{\textbf{(a)} separated by a much larger ocean.} \newline
\texttt{\textbf{(b)} joined together as one landmass.} \newline
\texttt{\textbf{(c)} located near the North Pole.}\newline
\texttt{\textbf{(d)} in a different hemisphere.} \\
\midrule
\texttt{\textbf{Ground Truth:} d.} &
\texttt{\textbf{Ground Truth:} c.} &
\texttt{\textbf{Ground Truth:} b.} \\
\midrule
\textbf{Non-Linear prompt:} \newline
The term conservation refers to the protection, management, and renewal of resources. &
\textbf{Non-Linear prompt:} \newline
6.0 kg x m/s &
\textbf{Non-Linear prompt:} \newline
joined together as one landmass \\
\midrule
\textbf{Linear prompt:} \newline
protection, management, and renewal of resources. &
\textbf{Linear prompt:} \newline
3.8 kg x m/s &
\textbf{Linear prompt:} \newline
The two continents were once joined together as one landmass. \\
\midrule
\textbf{Random-Init:} \newline
protection, management, and renewal of resources. &
\textbf{Random-Init:} \newline
The momentum of the baseball while moving at 40 m/s is 0.025 kg x m/s. &
\textbf{Random-Init:} \newline
located near the North Pole. \\
\bottomrule
\end{tabularx}
\label{tab:visualize_vqa}
\end{table*}


\clearpage

\bibliography{references}
\bibliographystyle{abbrv}
\end{document}




