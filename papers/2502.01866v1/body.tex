\section{Introduction}
% - intro CL
Online Continual Learning (OCL) models are trained continuously on a nonstationary data stream. The goal is to obtain a model that is accurate at any point in time, quickly adapts to new data (i.e. plasticity), and does not forget old information (i.e. stability), without any information about task start, end, or identity. In this setting, many standard Continual Learning (CL) approaches, cannot be applied \cite{aljundi2019task,mai2022online}. 
Many specific OCL methods have been developed, nearly all replay-based \cite{yoo2024layerwise}, but several shortcomings still exist: \cite{DBLP:conf/iccvw/Soutif-Cormerais23} shows that most OCL methods have high forgetting and fail to beat a simple replay baseline on some metrics; \cite{lirias4071238,DBLP:conf/iclr/CacciaAATPB22} discovered the stability gap, a sudden drop in performance at task boundaries; \cite{DBLP:journals/nature/DohareHLRMS24} even observed loss of plasticity, limiting the ability to learn in time.

% - stability-plasticity tradeoff
% - why the tradeoff is a bad metaphor
% - a natural optimizer is naturally stable and plastic
These results suggest fundamental failures of the algorithms. Most methods focus on preventing forgetting at the end of a learning task, an approach that does not ensure stable optimization throughout the entire stream (stability gap); furthermore, stability is often optimized at the expense of plasticity. We argue that a proper CL optimizer should seek to maximize both stability and plasticity, assuming that the model is large enough, at every step in time, interpreting it as a continual filtering process (filter intended as in nonstationary time series literature \cite{durbin2012time}).

% Paper goals:
% - we do OCL because it's hard
% - we carefully formalize OCL as the joint approximate optimization of past and new distributions
% - natural gradients comes to the rescue, finding suitable directions for each pdf
% - we design a bunch of tricks that actually make this stuff work
% - METHOD is sota in all the benchmarks, and we don't even have to try too hard
This paper proposes Online Curvature-Aware Replay (OCAR), a novel method designed for replay-based online continual learning, aiming at tackling the challenges in both plasticity and stability inherent in this scenario with a continual optimization approach at every step in time.
We formalize OCL as the joint optimization on past and new data, with past data approximated using a limited replay buffer, and adding an explicit constraint on the variation of KL-divergence on previous information. The Fisher Information Matrix (FIM) is used to capture the loss function's curvature, providing both plasticity and stability constraints in the model distribution space.
When the KL divergence is used as a metric, the FIM has the additional value of describing the curvature of the parameter space itself, being the Riemannian metric tensor of that space \cite{amari2016information}. We can directly adapt our gradient to the geometry of the space in stark difference with traditional CL methods that uses the FIM as a penalization term \cite{kirkpatrick2017overcoming}. 
Kronecker-factored Approximate Curvature (K-FAC) \cite{martens2015optimizing} is used to efficiently approximate the FIM, with some critical adjustments to make it work in OCL settings.

% empirical results
The main contributions of this paper are: the design of Online Curvature-Aware Replay (OCAR) as a combination of replay, second-order optimization, and information geometry; the analysis of the Tikhonov regularization and its ratio with the learning rate in the stability-plasticity tradeoff; an improvement of state-of-the-art performance in standard computer vision benchmarks across all the stability and plasticity metrics.

\section{Related Work}
\textbf{Continual Stability and OCL:} Most continual learning methods assume that stability must be kept at the expense of plasticity~\cite{DBLP:journals/pami/LangeAMPJLST22,DBLP:journals/pami/MasanaLTMBW23}, the so called plasticity-stability tradeoff, and therefore are designed to preserve knowledge about previous tasks to mitigate catastrophic forgetting~\cite{french1999catastrophic}. However, recent evidence in \cite{DBLP:conf/iclr/LangeVT23,DBLP:conf/iclr/CacciaAATPB22} suggest that even the methods with high "stability" measured at the end of tasks suffer from high instability and forgetting immediately after the task switch, and they recover the lost performance over time. \cite{DBLP:journals/corr/abs-2406-05114} found evidence of the stability gap even in incremental i.i.d. settings. In OCL settings, \cite{DBLP:conf/iccvw/Soutif-Cormerais23} showed that some state-of-the-art methods are unable to outperform a simple reservoir sampling baselines on some fundamental stability metrics. 
Furthermore, CL methods also fail at keeping plasticity, and \cite{DBLP:journals/nature/DohareHLRMS24} provides evidence of the loss of plasticity in deep continual networks. Overall, the literature suggests that CL methods fail at both stability and plasticity due to instabilities in the learning dynamics. Recently, some methods such as OnPro~\cite{wei2023online} and OCM~\cite{guo2022online} proposed novel self-supervised auxiliary losses and prototype-based classifiers, two approaches orthogonal to our optimization-based method.

% forgetting may not be a consequence of intrinsic interference in the data but instead the result of undesirable learning dynamics at the task boundaries.
\textbf{Optimization in Continual Learning:} Most CL optimization algorithms are designed to prevent forgetting by removing interfering updates. GEM \cite{DBLP:conf/nips/Lopez-PazR17,DBLP:conf/iclr/ChaudhryRRE19} models interference using the dot product of the task gradients and constrains the model updates to have positive dot products with the gradients of previous tasks. Subsequent work explored orthogonal projection methods~\cite{DBLP:conf/iclr/SahaG021,DBLP:conf/aistats/FarajtabarAML20} that either extend the idea of interfering gradients or project in the null space of the latent activations. \cite{DBLP:conf/nips/MirzadehFPG20} discusses the relationship between the curvature of the first task and the forgetting, proposing a hyperparameter schedule that implicitly regularizes the curvature. More recently, LPR\cite{yoo2024layerwise} exploits proximal optimization in the L2 space of latent activations, and it is the only projection-based optimizer compatible with replay. \cite{DBLP:journals/corr/abs-2311-04898} proposes a combination of GEM and replay as a potential mitigation for the stability gap.

\textbf{Natural Gradient and FIM in CL:} Natural gradients can be used to train neural networks thanks to efficient approximations of the Fisher Information Matrix (FIM)~\cite{martens2012training}, such as the K-FAC~\cite{martens2020new} and E-KFAC~\cite{DBLP:conf/nips/GeorgeLBBV18}. Interestingly, \citet{DBLP:conf/icml/Benzing22} showed that K-FAC seems to work better than the full FIM in some empirical settings, which is connected to a form of gradient descent on the neurons. In continual learning, the FIM is typically used to approximate the posterior of the weights with a Laplace approximation. The result is a quadratic regularizer that is combined with the loss on new data, as introduced by EWC~\cite{kirkpatrick2017overcoming} and its several extensions~\cite{DBLP:conf/eccv/ChaudhryDAT18,liu2018rotate,DBLP:journals/corr/abs-1712-03847}. \citet{DBLP:conf/iclr/MagistriTS0B24} proposed to use the FIM only for the final layer, which can be computed efficiently with a closed-form equation. As an alternative, FROMP~\cite{pan2020continual} computes a Gaussian Process posterior, which is also used to estimate the importance of samples in the replay buffer. \citet{daxberger2023improving} proposes a method that combines EWC, replay, and knowledge distillation. More relevant to our work, NCL~\cite{DBLP:conf/nips/KaoJVBH21} proposed a modified natural gradient step with a quadratic posterior as in EWC. Here the Fisher is computed only at the boundaries and the method does not support rehearsal, making it difficult to implement in OCL.

\textbf{Comparison with our work} Most CL methods use the FIM to compute a Laplace approximation of the posterior. This is not possible in OCL because the model is never assumed to be at a local minimum, not knowing task boundaries or length. In general, in the CL literature, the FIM is often restricted to its use in quadratic penalties. While quadratic regularizers are easier to use, we show that the use of the Fisher as a gradient preconditioner is a promising direction, improving the optimization path. Many of the limitations found for EWC and similar methods may be due to some suboptimal choices in the use of the FIM, such as the use of the empirical FIM, popular in the CL literature but with different properties from the FIM~\cite{kunstner2019limitations}. Another difference with the literature is that our optimizer is compatible with replay, unlike most projection-based methods. Furthermore, while most methods penalize plasticity indirectly to prevent forgetting, our approach is designed to improve both on learning speed and forgetting.


\section{Online Continual Learning}
In continual learning (CL), the model learns incrementally from nonstationary data. 
In most CL settings, we can identify a sequence of tasks $\mathcal{T}_1, \hdots, \mathcal{T}_N$, each one with its own distribution. For example, in class-incremental learning \cite{DBLP:journals/corr/abs-1904-07734}, each tasks has different classes, and tasks are seen sequentially during training. The goal of the model is to learn all the tasks seen during training. Given the model parameters $w_t$ learned at time $t$, a loss function $\mathcal{L}$, and a test set for each task $\mathcal{D}_1, \hdots, \mathcal{D}_N$, we can evaluate the model with the average task loss $\mathcal{L}^{avg}(t) = \sum_{i=1}^N \mathcal{L}(w_t, \mathcal{D}_i)$.

Online continual learning (OCL) requires some additional constraints and desiderata~\cite{DBLP:conf/iccvw/Soutif-Cormerais23,yoo2024layerwise,DBLP:journals/ijon/MaiLJQKS22}: (\emph{D1-Online training}) at each step, we do not have access to the whole training dataset for the current task, but only a small minibatch, which can be processed for a limited amount of time; (\emph{D2-Anytime inference}) the model should be ready for inference at any point in time; Consequently, (\emph{D3-Continual stability}) the model must be stable at any point in time, instead of only at the task boundaries; (\emph{D4-fast adaptation}) the model must also be able to learn quickly from new data.

% We propose to reframe the optimization problem to explicitly follow the desiderata for replay-based OCL defined by Yoo et al. \cite{DBLP:conf/iccvw/Soutif-Cormerais23,yoo2024layerwise,DBLP:journals/ijon/MaiLJQKS22}:
% \begin{description}
%     \item[D1] the network should rapidly learn from new data batches;
%     \item[D2] the network should continue
%     learning from prior (replay) data
%     \item[D3]  parameter updates should not cause sudden unnecessary performance degradation on the past data. 
% \end{description}

Following the literature, we focus on replay-based methods, which use a limited buffer of previous data for rehearsal, by combining new minibatches with samples from the buffer at each iteration. This work assumes no access to knowledge about task identities, boundaries, or length. Each single observation in the stream can be sampled from different distributions/tasks.

% \todo[inline]{CL problem, stability, plasticity, stability gap}
% \todo[inline]{empirical results on stability gap and interference?}



\section{Online Curvature-Aware Replay}
We show the building process of our method, starting from the current ER optimization and expanding it to a second-order method, then approximated with FIM and making some final adjustments to improve its CL performance.
\subsection{The optimization problem}
Online continual learning (OCL) is an online learning problem in a nonstationary setting. Common optimizers in machine learning implicitly assume stationarity, which justifies the gradient estimate from the minibatches (e.g.: ADAM \cite{DBLP:journals/corr/KingmaB14}, SGD). Instead, in OCL the distribution can change at any point in time (non i.i.d.). 
At each step, the method can only use the current minibatch and, in replay-based methods, an additional minibatch sampled from a small buffer of old data. 

\textbf{First-order optimization:}
We define our learning process as a sequence of local optimization problems solved at each step. Differently from stationary settings, these problems can be weakly dependent one on another, preventing the use of more "global" approaches (e.g, learning rate decay and momentum~\cite{lecun2015deep}). Each single step must be meaningful by itself. What is forgotten or not learned could be lost forever.

The Kullback–Leibler (KL) divergence \cite{thomas2006elements,lecun2015deep} is used as our objective, aiming to minimize the "distance" between the predicted and the real distribution. The KL is estimated on the current batch of data $\hat{KL}(y_D || f_w(x_D)) = \frac{1}{N} \sum_{i}^N KL(\hat{p}(y_{D, i}| f_{w^*}(x_{D, i})|| p(y_{D, i}| f_{w}(x_{D, i}))$. In our notation $D$ represents the batch of data, $i$ the specific sample, $y$ the target variable dependent on the observed $x$. $f_w$ is the model parametrized by $w$ that is supposed to represent the reality when $w=w^*$.The empirically observed distribution is $\hat{p}(y_{D, i}| f_{w^*}(x_{D, i})$.

Experience Replay (ER) \cite{chaudhry2019continual} keeps a small buffer $\mathcal{B}$ of past data and solves a joint optimization on a batch $N_t$ sampled from the unknown real current data distribution and a batch $B_t$ sampled from $\mathcal{B}$ :
\begin{equation}
\label{eq: ER_problem}
\begin{aligned}
& \min_{\delta_t} \quad \hat{KL}(y_{N_t} \, || \, f_{w_t}(x_{N_t})) +  \hat{KL}(y_{B_t} \, || \, f_{w_t}(x_{B_t}))\\
& \text{subject to} \quad \frac{1}{2}||\delta||_2^2 \leq \epsilon, 
\end{aligned}
\end{equation}
where $\delta_t = w_t - w_{t-1}$. Here and throughout the paper we write a sum of the two KL divergences for visual ease. Usually (and we also do this in practice) the mean is taken, combining the data in a single batch with no distinctions. The KL divergence is approximated by Taylor expansion:
\begin{align*}
    \hat{KL}(y_{D} \, || \, f_w(x_{D})) \approx  \; &\hat{KL}(y_{D} \, || \, f_{w = w_0}(x_{D})) + \\ &\nabla_{t}^T \delta_t + \delta_t^T \mathbf{H}_{t} \delta_t
\end{align*}
where $\nabla_t = \nabla_w \hat{KL}(y_{N_t} \, || \, f_{w = w_0}(x_{N_t}))$ is the gradient of the model on the current data, and $\mathbf{H}_{t} = \mathbf{H}_w \hat{KL}(y_{N_t} \, || \, f_{w = w_0}(x_{N_t}))$ the Hessian. $D$ can be both $N_t$ or $B_t$. After approximation, the solution for problem \ref{eq: ER_problem} is $\delta_t^* = -\frac{1}{\lambda} (\nabla_{N_t} + \nabla_{B_t})$ where $\lambda$ is the Lagrange multiplier of the constraint. ER actively optimizes on the buffer and the current data distributions with no distinctions and no forgetting constraint. The only stability requirement is $\frac{1}{\lambda}$ (the learning rate), limiting the movement of the weights in all directions, trading stability for plasticity.

\textbf{Failure of ER with first-order optimization:}
Unfortunately, the first-order information can be a poor approximation in CL. Often, information about the curvature is necessary to avoid catastrophic forgetting. For example, at task boundaries, the model is often close to a minimum for the previous task. In that case, buffer gradients $\nabla_B \approx 0$, while new gradients $\nabla_N$ can potentially be much higher, dominating the update direction. This issue partially explains the stability gap \cite{lirias4071238}.
Second-order methods can solve this problem by enlarging the "sight" of our optimizer with the local variation of the variation.
Using a second-order Taylor expansion problem \ref{eq: ER_problem} is solved by
\begin{equation*}
    \delta_t^* = -(\mathbf{H}_{N_t} + \mathbf{H}_{B_t} + \lambda \mathbf{I})^{-1}(\nabla_{N_t} + \nabla_{B_t}),
\end{equation*}
where the Lagrange multiplier $\lambda$, still related to the $L2$ regularization imposed on the update, now acts as a Tikhonov damping term \cite{martens2012training}. Unlike the learning rate in SGD, $\tau$ has a different effect on the eigendirections of the Hessian, which depends on their eigenvalues (see Sec. \ref{sec:car_stability}). 

\textbf{Stability constraint:}s approach already improves plasticity (D4), accelerating learning on both current and past data through Newton optimization, it still lacks an explicit stability constraint (D3) necessary for non-i.i.d. settings. Combining experience replay, second-order methods, an explicit requirement for stability on past data and $L2$ regularization on the update the problem becomes:
\begin{equation*}
\begin{aligned}
\min_{\delta} \quad &\hat{KL}(y_{N_t} \, || \, f_{w_t}(x_{N_t})) +  \hat{KL}(y_{B_t} \, || \, f_{w_t}(x_{B_t}))\\
\text{subject to}\quad  &\hat{KL}(f_{w_{t-1}}(x_{B_t}) \, || \, f_{w_t}(x_{B_t})) \leq \rho\\
& \frac{1}{2}||\delta||_2^2 \leq \epsilon. 
\end{aligned}
\end{equation*}
The term $\hat{KL}(f_{w_{t-1}}(x_{B_t}) \, || \, f_{w_t}(x_{B_t}))$ measures the variation of the model predicted distribution on buffer data, and its expansion around the pre-update parameters $w_{t-1}$ will have zero zeroth and first-order terms. The remaining term is controlled by the Hessian of the KL-divergence evaluated at $w=w_{t-1}$, which is exactly the Fisher Information Matrix \cite{ollivier2017information} \cite{martens2020new}:
\begin{equation*}
    \mathbf{F}_{B_t, ij} = \left. \frac{\partial^2 \hat{KL}(f_{w_{t-1}}(x_{B_t}) \, || \, f_{w}(x_{B_t}))}{\partial w_i \, \partial w_j} \right|_{w_t = w_{t-1}}.
\end{equation*}
The optimization problem becomes
\begin{equation}\label{eq:complete_opt}
\begin{aligned}
\min_{\delta} \quad & \nabla_{N_t}^T \delta +  \delta^T \mathbf{H}_{N_t} \delta +  \nabla_{B_t}^T \delta +  \delta^T \mathbf{H}_{B_t} \delta\\
\text{subject to}\quad &\frac{1}{2}\delta^T \mathbf{F}_{B_t} \delta \leq \rho &\ \\
&\frac{1}{2}||\delta||_2^2 \leq \epsilon,
\end{aligned}
\end{equation}
which is solved by
\begin{equation*}
    \delta_t^* = -(\mathbf{H}_{N_t} + \mathbf{H}_{B_t} + \lambda \mathbf{F_{B_t}}+\tau \mathbf{I})^{-1}(\nabla_{N_t} + \nabla_{B_t}),
\end{equation*}
where $\lambda$ controls the strength of the stability constraint, ensuring that the predictions of the model remain stable, while $\tau$ is the Tikhonov regularization controlling directions of maximum acceleration. As done in many second-order methods~\cite{martens2020new,martens2012training}, a learning rate $\alpha$ can be used to uniformly control the step size in all directions, making the final update be $\alpha \delta_t^*$. As we will show, the relation between $\alpha$ and $\tau$ has an important impact on the learning dynamics and the stability-plasticity tradeoff. 

\subsection{Estimations and approximations}
\textbf{Approximating the Hessians: }Computing two Hessians and one Fisher Information Matrix would be impractical and inverting them unfeasible. Luckily, in the second-order optimization literature, it has been shown how for some distributions (including the multivariate normal and the multinomial), the FIM is equivalent to a Generalized Gauss-Newton (GGN) matrix, both approximating the Hessian of the KL-divergence \cite{martens2020new}. They are equal to the Hessian when the model describes well the data (near optimum) and the work of Martens \cite{martens2020new} illustrates some general advantages of these approximations over the Hessian also when far from optimum.

Moreover, the use of the FIM has the additional interpretation of describing the curvature of the parameter space itself. In this space, each point represents a distribution, requiring the use of the KL divergence as "distance". The KL-divergence of infinitesimal displacements corresponds to $\frac{1}{2} F_{ij} d w_i d w_j$  giving rise to the Fisher Information as the metric tensor used in this manifold \cite{amari2016information}. More than a simple approximation of the Hessian, the FIM can be used to correctly measure distances in the manifold where we are performing the gradient descent if the steps are small enough. Assuming a small learning rate, our method would use the preconditioner not as in a Newton method (that usually requires a learning rate of 1 to directly go the the approximate optimum) but as a metric adaptation. 

We believe these two results justify the use of the Fisher information in nonstationary settings as OCL. Following this, we approximate the two Hessians matrices of our solution with the FIM, greatly simplifying the computations. The new optimal update becomes (including the learning rate):
\begin{equation*}
\label{eq:OCAR}
    \delta_t^* = -\alpha (\mathbf{F}_{N_t} + (1 + \lambda) \mathbf{F_{B_t}}+\tau \mathbf{I})^{-1}(\nabla_{N_t} + \nabla_{B_t}).
\end{equation*}
This implies the second-order information is now taken with respect to the model prediction instead of the real targets as also $\mathbf{F}_{N_T} = \mathbf{H}_w \; \hat{KL}(f_{w_{t-1}}(x_{N_t}) || f_{w}(x_{B_t})$. The gradient of our method corresponds to the usual gradient obtained from the loss function when the loss is a derivation of the KL divergence (negative log-likelihood, cross-entropy, etc...). The weighted sum (or mean) of the two FIMs can now be obtained by computing a single FIM on the batch, giving more weight to the buffer data.

There is a deep connection between our method and Natural Gradient (NG) \cite{amari1998natural}. We have to underline that our building process is much different from the one used for the original NG, thought for stationary settings. Moreover, our preconditioner is not really the FIM of the model, but a modified and regularized version stemming from our OCL second-order optimization with the use of replay. We then cannot assume all benefits of NG would apply to our case. 

\textbf{The Empirical FIM: } The Fisher is also defined as the expected value of the squared score $F = \sum_n \mathbb{E}_{y \sim p(y|f_w(x_n))}[\nabla_w \log p(y|f_w(x_n)) \nabla_w \log p(y|f_w(x_n))^T ]$. One could argue that a better preconditioner for the gradient would be the Empirical Fisher (EF) matrix, computed using the real target $y$ instead of the one sampled from the predicted distribution, in particular for approximating the Hessian of new data $H_{N_t}$. Besides getting us outside the theory about Natural Gradient and Fisher/GGN equivalence, it has been shown it is a questionable choice, even when the model is not a good description of the data \cite{kunstner2019limitations}. For these reasons, OCAR uses the "real" Fisher, unlike other traditional CL approaches \cite{kirkpatrick2017overcoming}. 

\textbf{K-FAC: }This approach has theoretical advantages, but it requires the inversion of an extremely large matrix, unfeasible for large networks. While diagonal approximations are frequently used~\cite{kirkpatrick2017overcoming,kingma2014adam}, given the particular challenges of OCL a more informative approximation is needed. We rely on the Kronecker-factored Approximate Curvature (K-FAC) in its block-diagonal version to approximate the FIM \cite{martens2015optimizing}:
\begin{equation*}
    \Tilde{F} = \text{diag}(\bar{A}_{0,0} \otimes G_{1,1}, ... ,\bar{A}_{l-1,l-1} \otimes G_{l,l}),
\end{equation*}
where each block corresponds to an approximation of the covariance matrix of the score of a specific layer, obtained by the Kronecker product of $\bar{A}_{i, i} = \mathbb{E}[\bar{a}_i \bar{a}_i^T]$ and $G_{i, i} = \mathbb{E}[g_i g_i^T]$. $\bar{a}_i$ is the vector of the layer activations (with an additional $1$ appended for the bias) and $g_i$ the gradient of the prediction with respect to the output of the layer before the activation function. The expected values of both are computed with an Exponential Moving Average (EMA) of past values. The efficiency of this method relies on the property of the Kronecker product $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$ allowing us to invert much smaller matrices for each layer, ignoring layer interactions. 

\subsection{Nuts and Bolts for OCL}\label{sec:cl_nuts}
% After extensive trial and error, we identified some best practices to make the whole method more tailored to the practical difficulties of the OCL setting without task boundaries: 

To make the method work in practice, we found some adjustments are needed. 

\textbf{Hyperparameter optimization and $\tau$ scheduling:} Usually, hyperparameter selection is done only on the first $K$ tasks of the stream, but it must generalize to longer streams during training. Our method uses three hyperparameters: the learning rate $\alpha$, the Tikhonov regularizer $\tau$, and the parameter $\gamma$ for the EMA used for Kronecker factors estimation. In hyperparameter selection, we found it beneficial to search for a value for the increase of $\tau$ instead of $\tau$ itself. $\tau$ is then initialized at the same value of the learning rate and increased by the selected value at each optimization step improving long-term stability. 

\textbf{Estimate of K-FAC factors at boundaries:} In class-incremental settings, the shape of the classifier will grow over time as new classes are observed. This means the K-FAC factor $G_{l,l}$ of the last layer will change shape, also breaking the relations with the previously observed gradients. To avoid keeping track of errors, if $G_{l,l}$ changes shape, the EMA of this factor is reset. This is not done for the other factor of the last layer $\bar{A}_{l-1,l-1}$. We assume the model has consistent representations, and that the majority of instability is happening on the classifier. 

\textbf{Scheduling of $\lambda$:} The stability constraint of problem \ref{eq:complete_opt} is estimated on the current batch extracted from the buffer, but it should represent the whole buffer. As new experiences are encountered in time, the information content of the buffer will grow, with less redundancy. At the limit, we can get a buffer where each example represent a different class or domain. Being $\lambda$ the Lagrange multiplier, it is directly connected with the constraint $\rho$. By decreasing $\rho$ to have a stronger constraint and save buffer information, we get an increase in $\lambda$. For class-incremental problems, $\lambda$ increases with the number of different classes encountered. In domain-incremental settings, it grows in time (as done with $\tau$). 

The composition of all these pieces together forms the \textbf{Online Curvature-Aware Replay (OCAR)}: at each step, the method is the optimal step of a second-order replay-based optimization problem, approximated with the FIM made computationally viable via K-FAC, with some fundamental adjustments to make it work in practice. The algorithm can be found in the Appendix \ref{app: algo}. 


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/manifold_ng_edo_best_lambda0_single_fig.pdf}
        \caption{OCAR 2D projection of the learning trajectory.}\label{fig:proj_car}       
    \end{subfigure}
    \begin{subfigure}[t]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/manifold_reservoir_single_fig.pdf}
        \caption{ER 2D projection of the learning trajectory.}\label{fig:proj_er}       
    \end{subfigure}
    \caption{2D projections of the training trajectories for ER and OCAR on Split MNIST (5 Tasks). Loss surface on the first task (left), second task (middle), and the average loss on all the $5$ tasks (right). The black stars highlight the task boundaries. More details on the 2D projections and additional plots are available in the Appendix.}\label{fig:traj}
\end{figure}



\section{Continual Stability in OCAR}\label{sec:car_stability}

In this section, we provide a qualitative analysis in a simplified setting, showing how OCAR results in a smoother continual optimization compared to ER and how $\alpha$, $\tau$, and their ratio can be used to control stability and plasticity.

\textbf{Loss landscape and model trajectories:} We can visualize the improvements in the optimization trajectory of OCAR in a simple continual learning setting. We train a small feed-forward network with ER and OCAR on Split MNIST (5 Tasks). Given the small size of the model, we can store the entire training history of the model, which allows us to plot 2D projections of the model trajectory in the loss landscape (Figure \ref{fig:traj}) (details in Appendix \ref{app: trajectory}).

Looking at their task-wise and joint loss surfaces in figures \ref{fig:proj_car} and \ref{fig:proj_er}, OCAR shows a much smoother model trajectory across all analyzed loss landscapes (Task 1, Task 2, and Average loss), which results in consistent improvements over time (plasticity) and mild forgetting (stability). Second-order information moves the optimization directly toward the next minimum (black stars in the plots), in fewer steps. On the other hand, ER always suffers from instability at the task boundaries (right after black stars in the plots) which results in an abrupt deviation from the optimal path. The learning curves on the first task (Fig. \ref{fig:curve_t0}) and the average of all tasks (Fig. \ref{fig:curve_joint}), available in Appendix \ref{app: figures}, confirm the result. OCAR maintains a smoother learning path without experiencing any stability gap on the first task and ending the stream with higher overall accuracy. ER instead, while still performing well in the basic MNIST setting, suffers from much more instability during training.

\textbf{Role of the Eigenvectors and Hyperparameters:} One approach to understand OCAR effect is to study how the eigenvalues of the matrix $\alpha (F_N + (1 + \lambda) F_B + \tau I)^{-1}$ are related to the hyperparameters $\alpha$, $\lambda$, and $\tau$. We can diagonalize  $\bar{F} = F_N + (1 + \lambda) F_B = Q \Sigma Q^T$, where $Q$ is a unitary matrix where the rows are the eigenvectors and $\Sigma$ a diagonal matrix with the eigenvalues $\sigma_i$. Therefore, we find that the eigenvalues of $\alpha (F_N + (1 + \lambda) F_B + \tau I)^{-1}$ are 
$\sigma_i^* = \frac{\alpha}{\sigma_i + \tau}.$
In the new coordinate system defined by the eigenvectors $Q$, we can interpret OCAR as slowing or accelerating directions depending on their curvature. Directions with $\sigma^*_i \ll 1$ ($\sigma^*_i \gg 1$) correspond to directions with high (low) curvature for some tasks.
The learning rate $\alpha$ and the Tikhonov regularization $\tau$ rescale these eigenvalues (as shown in Figure \ref{fig:lr_ratio_curve} in Appendix).  In particular, a learning rate $\alpha < 1$ decreases the step size. Conversely, $\tau$ mitigates the acceleration caused by small eigenvalues. When $\sigma_i \rightarrow 0$,  $\sigma^*_i \rightarrow \frac{\alpha}{\tau}$, limiting the maximum acceleration in each direction. When $\sigma_i \gg \tau$, we have $\sigma^*_i \approx \frac{\alpha}{\sigma_i}$, which is approximately independent of $\tau$.

Empirically, we find that the spectrum spans several orders of magnitude, with the smallest eigenvalues close to zero and a small set of very large values around $(10^4, 10^6)$ (consistently with the common intuition behind methods such as EWC, which expect few important parameters).  % Figure \ref{fig:lr_ratio_curve} shows the effective step size on the entire eigenvalue spectrum for different values of $\alpha$ and $\tau$.

\begin{figure}[t]
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/grid_forget.pdf}
        %\caption{}\label{fig:lr_ratio_grid}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/grid_plast.pdf}
        %\caption{}\label{fig:lr_ratio_grid}
    \end{subfigure}

    \caption{Grid search over $\alpha$ and $\frac{\alpha}{\tau}$: (left) forgetting on the first task, (right) plasticity measured as the accuracy on the final task. Metrics are computed on the test stream at the end of training.}\label{fig:lr_ratio_grid}
\end{figure}

\textbf{Stability-Plasticity tradeoff of OCAR hyperparameters:}
An interpretation of OCAR hyperparameters in the stability-plasticity tradeoff can be given.
While higher $\lambda$ gives more importance to the FIM of the buffer (stability) and higher values of $\alpha$ allow larger learning steps (plasticity), the role of $\tau$ and $\frac{\alpha}{\tau}$ is less intuitive. First, higher values of $\tau$ are needed for the introduction of new classes, that creates instability due to the FIM being the variance of gradients (See the Appendix \ref{app: Fisher computations} for a detailed explanation). Figure \ref{fig:lr_ratio_grid} shows the results of a grid search on $\alpha$ and $\frac{\alpha}{\tau}$ (Complete figure in Appendix fig \ref{fig:lr_ratio_grid_complete}). We can make some empirically-based speculations: (1) the ratio $\frac{\alpha}{\tau}$ controls the learning plasticity. As a result, bottom-left elements in Figure \ref{fig:lr_ratio_grid} show less plasticity; (2) $\frac{\alpha}{\tau}$ controls the effective step size in the eigendirections with small eigenvalues, problematic at task boundaries. If the ratio is too low, OCAR may use updates that are too large, resulting in instability (top-right); (3) Decreasing $\frac{\alpha}{\tau}$ consistently increases forgetting; (4) After a sufficient number of steps after a task drift, the FIM becomes more stable, and $\frac{\alpha}{\tau}$ becomes less important in the learning dynamics. At this point, small $\alpha$ will result in conservative updates, with low plasticity and high stability. In the figure, we notice that for equal values of the ratio, models may have a similar accuracy but a different stability-plasticity tradeoff, depending on their $\alpha$ (low=stability, high=plasticity). 

%Overall, the results show the fundamental relationship between $\alpha$, $\tau$, and their ratio. Each hyperparameter has a clear role in the optimizer stability and its continual stability-plasticity tradeoff.

%Edoardo Urettini, [27/09/2024 11:33]
% Quindi tutto si gioca su una relazione tra il ratio lr/regul che regola la plasticità iniziale (o nei momenti lontani dai minimi) e il lr da solo che invece guida la parte successiva del lr (suppongo quando l'approssimazione della Fisher inizia ad avere più senso vicino ai minimi)
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/qualitative_analysis.pdf}
    \caption{\textit{Left}: $L_p$ Cumulative loss of single batches. \textit{Right}: $L_s$ Cumulative loss measured on all previous data of the stream.}
    \label{fig:qualitative}
\end{figure}



\begin{table*}[t]
    \caption{Results on Split CIFAR100 (20 Tasks) and Split Tiny ImageNet (20 Tasks). Best in bold for the base methods. Best underlined for all methods included OCAR+OTHER.}\label{tab:tinyimnet}
    \resizebox{1.0\textwidth}{!}{%
    \begin{tabular}{l|cccc|cccc}
    \hline \multirow[t]{2}{*}{Method} & \multicolumn{4}{c}{Split-Cifar100 (20 Tasks)} & \multicolumn{4}{c}{Split-TinyImagenet (20 Tasks)} \\
    & Acc $\uparrow$ & $A A A^{\text {val }} \uparrow$ & WC-Acc ${ }^{\text {val }} \uparrow$ & Probed Acc $\uparrow$ & Acc $\uparrow$ & $A A A^{\text {val }} \uparrow$ & WC-Acc ${ }^{\text {val }} \uparrow$ & Probed Acc $\uparrow$ \\
    \midrule
    i.i.d & $35.3 \pm 1.5$ & - & - & $45.8 \pm 0.6$ & $26.5 \pm 0.6$ & - & - & $34.3 \pm 0.5$ \\ \midrule
    ER\cite{chaudhry2019continual}& $28.2 \pm 1.2$ & $36.6 \pm 2.0$ & $12.5 \pm 0.6$ & $44.9 \pm 0.9$ & $21.2 \pm 0.6$ & $33.9 \pm 1.7$ & $15.2 \pm 0.5$ & $35.6 \pm 0.6$ \\
    GDumb\cite{prabhu2020gdumb} & $18.5 \pm 0.5$ & - & - & - & $13.1 \pm 0.4$ & - & - & - \\
    AGEM\cite{DBLP:conf/iclr/ChaudhryRRE19}& $3.1 \pm 0.2$ & $10.4 \pm 0.6$ & $2.9 \pm 0.3$ & $18.7 \pm 0.8$ & $2.6 \pm 0.2$ & $7.3 \pm 0.5$ & $2.6 \pm 0.2$ & $23.3 \pm 0.6$ \\
    $\mathrm{ER}+\mathrm{LwF}$\cite{li2017learning} & $30.4 \pm 0.8$ & $39.2 \pm 2.0$ & $15.3 \pm 0.9$ & $44.4 \pm 0.8$ & $22.7 \pm 1.1$ & $34.4 \pm 2.4$ & $17.0 \pm 0.7$ & $33.8 \pm 0.9$ \\
    MIR\cite{aljundi2019online} & $29.4 \pm 1.9$ & $33.1 \pm 3.2$ & $11.6 \pm 1.6$ & $43.4 \pm 0.7$ & $21.3 \pm 0.8$ & $31.0 \pm 1.8$ & $15.2 \pm 0.5$ & $33.0 \pm 0.4$ \\
    RAR\cite{kumari2022retrospective} & $28.2 \pm 1.4$ & $38.2 \pm 1.6$ & $14.9 \pm 0.7$ & $42.3 \pm 0.9$ & $15.7 \pm 0.9$ & $27.8 \pm 2.8$ & $10.1 \pm 0.9$ & $29.8 \pm 0.9$ \\
    %EMA & $\mathbf{37.3}\pm0.3$ & $36.7\pm0.5$ & $12.3\pm0.3$ & & $\mathbf{26.0}\pm0.2$ & $33.7\pm0.4$ & $15.1\pm0.1$ & \\
    DER++ \cite{buzzega2020dark}& $29.3 \pm 0.9$ & $37.5 \pm 2.5$ & $13.4 \pm 0.7$ & $44.0 \pm 0.8$ & $22.9 \pm 0.5$ & $34.2 \pm 4.0$ & $16.3 \pm 0.3$ & $31.5 \pm 0.9$ \\    
    ER-ACE \cite{DBLP:conf/iclr/CacciaAATPB22} & $29.9 \pm 0.6$ & $38.5 \pm 1.8$ & $14.9 \pm 0.9$ & $42.4 \pm 0.6$ & $\mathbf{23.6} \pm 0.7$ & $35.0 \pm 1.5$ & $16.8 \pm 0.7$ & $34.2 \pm 0.3$ \\
    SCR\cite{mai2021supervised} & $28.3 \pm 0.8$ & $42.1 \pm 2.1$ & $20.3 \pm 0.4$ & $37.0 \pm 0.3$ & $16.9 \pm 0.4$ & $30.7 \pm 1.5$ & $12.3 \pm 0.5$ & $22.5 \pm 0.4$ \\
    OnPro & $31.7 \pm 1.2$ & $36.6 \pm 2.5$ & $12.2 \pm 1.1$ & - & $17.1 \pm 1.5$ & $24.2 \pm 0.4$ & $8.00 \pm 0.8$ & - \\
    OCM   & $30.9 \pm 0.7$ & $33.3 \pm 1.9$ & $14.9 \pm0.4$ & - & $20.6 \pm 0.6$ & $24.8 \pm 1.1$  & $10.9 \pm 0.5$ & - \\
    % ER\cite{yoo2024layerwise} & $29.4 \pm 0.5$ & $35.9 \pm 0.5$ & $12.6 \pm 0.3$ & - & $21.9 \pm 0.2$ & $33.7 \pm 0.4$ & $15.0 \pm 0.1$ & -\\
    LPR~\cite{yoo2024layerwise} & $33.3 \pm 0.6$ & $42.5 \pm 0.5$ & $19.3 \pm0.3$ & - & $23.1 \pm 0.2$ & $34.9 \pm 0.4$ & $16.2 \pm 0.2$ & - \\
    OCAR (ours) & $\mathbf{34.9} \pm 0.6$ & $\mathbf{48.2} \pm 1.2$ & $\mathbf{25.0} \pm 1.1$ & $\mathbf{46.2} \pm 0.6$ & $21.7 \pm 1.0$ & $\mathbf{38.3} \pm 1.4$ & $\mathbf{17.4} \pm 0.6$ & $\mathbf{38.3} \pm 0.6$ \\
    \midrule
    %LPR-DER++~\cite{yoo2024layerwise} & $32.3 \pm 0.5$ & $ 43.3 \pm 0.6$ & $20.2 \pm 0.3$ & - & $\underline{24.1} \pm 0.3$ & $\underline{35.3} \pm 0.5$ & $\underline{17.6} \pm 0.1$ & -\\
    OCAR-DER++ (ours) & $34.3 \pm 1.1$& $46.8 \pm 1.7$ & $25.4 \pm 0.8$ & $46.0 \pm 0.8$ & - & - & - &- \\
    %LPR-EMA\cite{yoo2024layerwise} & $38.8 \pm 0.3$ & $41.4 \pm 0.5$ & $17.6 \pm 0.3$ & - & $\mathbf{26.7} \pm 0.2$ & $35.0 \pm 0.4$ & $16.3 \pm 0.1$ & -\\
    %OCAR-EMA (ours) & $\mathbf{38.9} \pm 0.9$ & $48.0 \pm 1.2$ & $23.9 \pm 0.6$ & $45.7 \pm 0.8$ & $\mathbf{26.2} \pm 1.0$ & $38.0 \pm 1.3$ & $17.6 \pm 0.9$ & $\mathbf{38.3} \pm 0.5$ \\
    OCAR-ACE (ours) & $\underline{35.6} \pm 1.2$ & $\underline{48.7} \pm 1.7$ & $\underline{26.5} \pm 0.4$ & $44.1 \pm 0.7$& $\underline{25.6} \pm 0.4$ & $\underline{39.8} \pm 2.0$ & $\underline{21.5} \pm 0.9$ & $34.7 \pm 0.3$\\

    % ER\cite{yoo2024layerwise} & $29.4 \pm 0.5$ & $35.9 \pm 0.5$ & $12.6 \pm 0.3$ & - & $21.9 \pm 0.2$ & $33.7 \pm 0.4$ & $15.0 \pm 0.1$ & -\\
    % LPR(ER)\cite{yoo2024layerwise} & $33.3 \pm 0.6$ & $42.5 \pm 0.5$ & $19.3 \pm0.3$ & - & $23.1 \pm 0.2$ & $34.9 \pm 0.4$ & $16.2 \pm 0.2$ & - \\
    % LPR(DER)\cite{yoo2024layerwise} & $32.3 \pm 0.5$ & $ 43.3 \pm 0.6$ & $20.2 \pm 0.3$ & - & $24.1 \pm 0.3$ & $35.3 \pm 0.5$ & $17.6 \pm 0.1$ & -\\
    % LPR(EMA)\cite{yoo2024layerwise} & $38.8 \pm 0.3$ & $41.4 \pm 0.5$ & $17.6 \pm 0.3$ & - & $26.7 \pm 0.2$ & $35.0 \pm 0.4$ & $16.3 \pm 0.1$ & -\\
    % \midrule
    % OCAR & $34.9 \pm 0.6$ & $48.2 \pm 1.2$ & $25.0 \pm 1.1$ & $46.2 \pm 0.6$ & $21.7 \pm 1.0$ & $38.3 \pm 1.4$ & $17.4 \pm 0.6$ & $38.3 \pm 0.6$ \\
    % OCAR-ACE & $35.6 \pm 1.2$ & $48.7 \pm 1.7$ & $26.5 \pm 0.4$ & $44.1 \pm 0.7$& $25.6 \pm 0.4$ & $39.8 \pm 2.0$ & $21.5 \pm 0.9$ & $34.7 \pm 0.3$\\
    % OCAR-DER & $34.3 \pm 1.1$& $46.8 \pm 1.7$ & $25.4 \pm 0.8$ & $46.0 \pm 0.8$ & - & - & - &- \\
    % OCAR-EMA & $38.9 \pm 0.9$ & $48.0 \pm 1.2$ & $23.9 \pm 0.6$ & $45.7 \pm 0.8$ & $26.2 \pm 1.0$ & $38.0 \pm 1.3$ & $17.6 \pm 0.9$ & $38.3 \pm 0.5$ \\
    \bottomrule
    \end{tabular}
    }
\end{table*}

\section{Experiments}

\subsection{Comparison Between EWC, NGD, and OCAR in a Convex Setting}
First, we compare OCAR with alternative uses of the Fisher Information, that are not commonly considered for OCL, in a small-scale convex setting. In CL, EWC \cite{kirkpatrick2017overcoming} generated a cascade of derived methods~\cite{DBLP:conf/eccv/ChaudhryDAT18,liu2018rotate,DBLP:journals/corr/abs-1712-03847} based on the idea to add a quadratic regularization term to the loss, penalizing the movement of the parameters from an optimal configuration, weighted by the FIM. This approach finds some limitations in OCL when no task boundaries are provided and it's not possible to select the previous task's "best" weights \cite{mai2022online}, breaking the fundamental assumption behind the Laplace approximation. Additionally, regularizing the loss does not directly speed up non-important directions, a non-optimal approach in OCL. On the other hand, outside CL, the Natural Gradient Descent (NGD) \cite{amari1998natural} uses the FIM as a preconditioner for the gradient, slowing it down in the direction of high curvature and accelerating it in others. We believe NGD can be well-suited for OCL problems. The problem is that raw NGD is derived for i.i.d. settings. OCAR, on the other hand, is an adaptation for non-i.i.d. problems. To underline the differences, we tested the three approaches in combination with ER in an online stream of 10 small convex tasks (all details in Appendix \ref{app: qualitative}). We measure the cumulative loss experienced on single batches during the training $L_p = \sum_{t} L(y_{t}, \hat{y}_t)$ to measure the ability to adapt to current data and the cumulative loss experienced on all previous data $L_s = \sum_{t} L(y_{0:t}, \hat{y}_{0:t})$ to measure the stability of the model. The results in figure \ref{fig:qualitative} show that, while NG is much more adaptable than EWC, it is slightly less stable. EWC performance in OCL is very similar to the ones of basic ER, a result aligned with \cite{mai2022online}. OCAR, thanks to its explicit memory constraint, and its dynamic hyperparameters is able to improve both on the speed and on the stability, showing a slight optimization superiority already in this very basic setting.

% To improve replicability and make a fair comparison among different strategies, we perform our experimental evaluation with the same code, benchmarks, and setting used by Soutif–Cormerais et al. \cite{DBLP:conf/iccvw/Soutif-Cormerais23}. The same approach has been used by Yoo et al. \cite{yoo2024layerwise} making the results of the different works directly comparable. Everything is built on top of Avalanche library \cite{carta2023avalanche}. As in the original survey, we evaluate our method on two standard OCL settings: Split-CIFAR100, 100 classes split into 20 tasks, and Split-TinyImageNet, 200 classes split into 20 tasks. Both datasets do not have any repetitions of classes in the different tasks. No information about task boundaries is used. 
\subsection{Literature benchmarks and SOTA comparison}
To ensure reproducibility and a fair comparison we use the same code and experimental setup in \cite{DBLP:conf/iccvw/Soutif-Cormerais23}, later used by  LPR \cite{yoo2024layerwise}, an ICML24 paper, that represents the SOTA in OCL and our main "competitor". We use Avalanche~\cite{carta2023avalanche} and nngeometry \cite{george_nngeometry}. In line with the literature Split-CIFAR100 (20 Task) and Split-TinyImageNet (20 tasks), are used as task incremental benchmarks. Following \cite{yoo2024layerwise} we experimented also on Online CLEAR (10 tasks) \cite{lin2021clear}, a domain incremental scenario fundamentally different from the other two datasets. 
% The classifier changes its shape anytime a new class is observed and this can happen also multiple steps after the boundary. The data stream is composed of batches of 10 samples that become inaccessible when a new batch arrives. Only a class-balanced replay buffer of random observed samples can be maintained in time. The size of the buffer is 2000 samples for Split-CIFAR100 and 4000 for Split-TinyImageNet. 
All the methods use a reservoir sampling buffer with 2000 samples for Split-CIFAR100 and online CLEAR and 4000 for Split-TinyImageNet. More details about the experimental setting are available in the Appendix \ref{app:experimental setup} with specific information for CLEAR in Appendix \ref{app:clear}. Our entire code for the experiments can be found at \url{https://anonymous.4open.science/r/CAR-8412}.

We compare our approach with a large set of CL baselines (Table \ref{tab:tinyimnet}), evaluated on 4 fundamental OCL metrics: 
\textbf{WC-Acc} \cite{lirias4071238}: the worst-case accuracy is a metric of the stability of the model, measuring a trade-off between the model accuracy and the minimum accuracy among all tasks encountered. 
\textbf{AAA} \cite{DBLP:conf/iclr/CacciaAATPB22}: The Average Anytime Accuracy is a metric developed specifically for OCL measuring the mean accuracy of the model in time on all the encountered tasks. It is a measure of the whole accuracy history of the model.
 \textbf{Acc} \cite{DBLP:conf/nips/Lopez-PazR17}: The final average accuracy is a snapshot of the accuracy of the model at the end of the entire stream. 
\textbf{Probed Acc} \cite{davari2022probing}: The metric is the final accuracy after freezing the feature extractor and retraining only the linear classifier on all the training data. It is a measure of the representation quality of the model. 

\subsection{Results}

Using the same code and setup, all results of other methods are taken from \cite{DBLP:conf/iccvw/Soutif-Cormerais23} and \cite{yoo2024layerwise} (for LPR), except for OnPro~\cite{wei2023online} and OCM~\cite{guo2022online}, where we reused the original code and run the experiments following the setup in \cite{DBLP:conf/iccvw/Soutif-Cormerais23,yoo2024layerwise}. Everything is evaluated on 5 runs (except LPR in 10).

\textbf{End of training metrics:} While in Split-Cifar100 OCAR is able to achieve the best \textit{Acc} among base models, ER-ACE takes the crown in Tinyimagenet (Table \ref{tab:tinyimnet}). However, we must point out that \textit{Acc} is a very poor evaluation metric for the OCL performance. Since it is evaluated at the very final iteration it badly represents the whole training and can be affected by noise (check learning curves in \ref{app: figures}). OCAR obtains the best results on linear probing in both benchmarks, underlying the optimization improvements also on the feature extractor. This is a significant result when compared to the i.i.d. case: while the classifier suffers from forgetting, OCAR can learn a better feature representation if compared to the i.i.d. case, confirming how it learns efficiently in nonstationary settings.

\textbf{Continual metrics:} When evaluated at every step in time, OCAR obtains the best \textit{AAA} and \textit{WC-Acc} among the methods in both benchmarks. This confirms OCAR as a robust continual optimizer with high accuracy and stability at every point in time. Both metrics are significantly improved with a jump of several points. 

\textbf{Integration with other methods:} Being ER-ACE the method with the best accuracy on Tinyimagenet, we try a combination with it to see if OCAR can improve the results. Even if ER-ACE uses a slightly modified loss, the integration with OCAR works very well. The combination OCAR-ACE beats all other methods (including base OCAR) on \textit{Acc}, \textit{AAA} and \textit{WC-Acc}. Both ER-ACE and OCAR-ACE show a slightly lower probing accuracy, which suggests that ER-ACE may be tuned to prefer stability over plasticity, learning less transferable features despite the high accuracy. Possibly, the robustness of the classifier given by ER-ACE slightly interferes with deeper feature learning. Following this experiment, we also tried the combination with DER++. Unfortunately, we have found OCAR-DER to be much less stable (failing optimization on Tinyimagent), which we conjecture to be caused by the entropy regularization implicit in the DER loss, which results in a loss that is "too different" from the KL divergence and breaks the assumption for the use of the Fisher.

\textbf{Online CLEAR}
The Online CLEAR benchmark is fundamentally different: no classes are added from task to task, but all classes evolve in time, in a domain incremental fashion. This puts much more importance on forward and backward transfer and less on catastrophic forgetting due to the similarities between tasks. For this reason, the final accuracy is higher than \textit{AAA} (see figure \ref{fig:clear accuracy} in Appendix). This setting, being more similar to the standard iid one, makes much easier the estimation and the stability of the FIM, making OCAR remarkably better than the previous SOTA LPR (see table \ref{tab:clear}). This confirms OCAR robustness also on domain incremental settings, underlying its optimization improvements. 
\begin{table}[h!]
    \caption{Results on Online CLEAR (10 Tasks) domain incremental setting. 2000 Buffer size. Best in bold.}\label{tab:clear}
    {%
    \begin{tabular}{l|ccc}
    \hline \multirow[t]{2}{*}{Method} & \multicolumn{3}{c}{Online CLEAR (10 Tasks)}  \\
    & Acc $\uparrow$ & $A A A^{\text {val }} \uparrow$ & WC-Acc ${ }^{\text {val }} \uparrow$ \\
    \midrule
    ER & $63.1 \pm 0.7$ & $58.9 \pm 0.8$ & $47.7 \pm 1.6$ \\ 
    LPR & $65.2 \pm 0.9$ & $63.5 \pm 1.0$ & $62.6 \pm 0.7$ \\
    \midrule
    OCAR(Ours) & $\mathbf{75.3} \pm 0.8$ & $\mathbf{73.9} \pm 0.5$ & $\mathbf{70.3} \pm 0.5$\\
    \bottomrule
    \end{tabular}
    }
\end{table}

\textbf{Final Comment}: OCAR showed remarkable performance across all continual metrics, improving both on task-incremental and domain-incremental the previous SOTA results, including the ICML24 paper LPR \cite{yoo2024layerwise}, using the same code, setting and benchmarks. All of this is done efficiently, even improving on computational time (see Appendix \ref{app:experimental setup}) of some previous SOTA approaches. The method showed the possibility of being combined with other approaches, obtaining even stronger results.



% \subsubsection{Continual Stability and Plasticity}
% \todo[inline]{stability plots GD vs NG?}

% \subsubsection{Ablation}
% \todo[inline]{GD, NG, (KFAC vs diagonal?), NG+EMA, NG+EMA+tau}

% \todo[inline]{plot hyperparam}
% \todo[inline]{plot learning rate hyperopt SGD vs NGD + surface lr/regul NGD. Si nota che SGD ha un picco di lr buoni, NGD ha valori buoni più ampi}
% \todo[inline]{plot che motiva regul e regul-last separati. Si nota che regul vuole basso, regul-last molto alto. Relazione inversa che torna con il fatto che il classificatore tende a rompersi al drift.}
% \vspace{-0.5 em}
\section{Conclusion}
In this paper, we revisit replay-based OCL as second-order optimization with hard stability constraints and information geometry rooting. The resulting method OCAR shows consistent improvements in plasticity and continual stability with clear hyperparameters interpretation in the stability-plasticity tradeoff.
Future research directions include the comparison of different approximations of the curvature (e.g. \citet{george2018fast}), alternative derivations for the optimization problem (e.g. \citet{benzing2022gradient}), and feasible dynamic adaptation of $\alpha$ and $\tau$. We believe OCAR can be the starting point for further improvements towards a deeper understanding of continual learning dynamics.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.