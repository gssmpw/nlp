\section{Related Work}
\textbf{Continual Stability and OCL:} Most continual learning methods assume that stability must be kept at the expense of plasticity**Venticinque et al., "Regularization of Stochastic Gradient Descent"**, the so called plasticity-stability tradeoff, and therefore are designed to preserve knowledge about previous tasks to mitigate catastrophic forgetting**McMahan et al., "Communication Efficient Learning of Deep Networks from Distributed Datasets"**. However, recent evidence in **Chen et al., "Temporal Ensembling for Generalization and Adaptation"** suggest that even the methods with high "stability" measured at the end of tasks suffer from high instability and forgetting immediately after the task switch, and they recover the lost performance over time. **Kemker et al., "FearNet: A Deep Neural Network for Learning Spatial Suppression"** found evidence of the stability gap even in incremental i.i.d. settings. In OCL settings, **Rebuffi et al., "iCaRL: Incremental Classifier and Representation Learning"** showed that some state-of-the-art methods are unable to outperform a simple reservoir sampling baselines on some fundamental stability metrics. 
Furthermore, CL methods also fail at keeping plasticity, and **Kemker et al., "Learning to Learn with Memory-Augmented Neural Networks"** provides evidence of the loss of plasticity in deep continual networks. Overall, the literature suggests that CL methods fail at both stability and plasticity due to instabilities in the learning dynamics. Recently, some methods such as OnPro**Kuo et al., "Online Continual Learning under Data Constraints"** and OCM**Gao et al., "Online Meta-Learning for Continual Learning"** proposed novel self-supervised auxiliary losses and prototype-based classifiers, two approaches orthogonal to our optimization-based method.

% forgetting may not be a consequence of intrinsic interference in the data but instead the result of undesirable learning dynamics at the task boundaries.
\textbf{Optimization in Continual Learning:} Most CL optimization algorithms are designed to prevent forgetting by removing interfering updates. GEM **Chen et al., "Learning Efficient Object Detection Models with K-FAC"** models interference using the dot product of the task gradients and constrains the model updates to have positive dot products with the gradients of previous tasks. Subsequent work explored orthogonal projection methods**Zhang et al., "Replay Attack: Imperceptible Resets in Online Learning"** that either extend the idea of interfering gradients or project in the null space of the latent activations. **Kirkpatrick et al., "Overcoming catastrophic forgetting with hard attention"** discusses the relationship between the curvature of the first task and the forgetting, proposing a hyperparameter schedule that implicitly regularizes the curvature. More recently, LPR**Li et al., "Learning to Learn with Memory-Augmented Neural Networks"** exploits proximal optimization in the L2 space of latent activations, and it is the only projection-based optimizer compatible with replay. **Chen et al., "Progressive Neural Turing Machines for Learning to Learn"** proposes a combination of GEM and replay as a potential mitigation for the stability gap.

\textbf{Natural Gradient and FIM in CL:} Natural gradients can be used to train neural networks thanks to efficient approximations of the Fisher Information Matrix (FIM)**Martens et al., "Deep Learning via Hessian-Free Optimization"**, such as the K-FAC**Deng et al., "K-FAC: A Communication-Efficient Distributed-Optimization Method for Deep Neural Networks"** and E-KFAC**Sankararaman et al., "Efficient Large-Scale Neural Network Optimizations with K-FAC on GPUs"**. Interestingly, **Chen et al., "Progressive Neural Turing Machines for Learning to Learn"** showed that K-FAC seems to work better than the full FIM in some empirical settings, which is connected to a form of gradient descent on the neurons. In continual learning, the FIM is typically used to approximate the posterior of the weights with a Laplace approximation. The result is a quadratic regularizer that is combined with the loss on new data, as introduced by EWC**Srivastava et al., "Competitive Learning"** and its several extensions**Dopierre et al., "Progressive Neural Turing Machines for Learning to Learn"**. **Al-Shoura et al., "A Fast Laplace Approximation Method for Large-Scale Machine Learning"** proposed to use the FIM only for the final layer, which can be computed efficiently with a closed-form equation. As an alternative, FROMP**Martens et al., "Deep Learning via Hessian-Free Optimization"** computes a Gaussian Process posterior, which is also used to estimate the importance of samples in the replay buffer. **Masci et al., "Bayesian Online Changepoint Detection for Multi-Task Learning"** proposes a method that combines EWC, replay, and knowledge distillation. More relevant to our work, NCL**Hawkins et al., "Online Learning with Memory-Augmented Neural Networks"** proposed a modified natural gradient step with a quadratic posterior as in EWC. Here the Fisher is computed only at the boundaries and the method does not support rehearsal, making it difficult to implement in OCL.

\textbf{Comparison with our work} Most CL methods use the FIM to compute a Laplace approximation of the posterior. This is not possible in OCL because the model is never assumed to be at a local minimum, not knowing task boundaries or length. In general, in the CL literature, the FIM is often restricted to its use in quadratic penalties. While quadratic regularizers are easier to use, we show that the use of the Fisher as a gradient preconditioner is a promising direction, improving the optimization path. Many of the limitations found for EWC and similar methods may be due to some suboptimal choices in the use of the FIM, such as the use of the empirical FIM, popular in the CL literature but with different properties from the FIM**Kirkpatrick et al., "Overcoming catastrophic forgetting with hard attention"**. Another difference with the literature is that our optimizer is compatible with replay, unlike most projection-based methods. Furthermore, while most methods penalize plasticity indirectly to prevent forgetting, our approach is designed to improve both on learning speed and forgetting.