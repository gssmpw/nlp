\section{Algorithm}
\label{app: algo}
\begin{algorithm}
\caption{Online Curvature-Aware Replay (OCAR)}
\textbf{Input:} network parameters $w$, learning rate $\alpha$, per batch gradient steps count $S$, Tikhonov increase $\Delta \tau$, EMA parameter $\alpha_{EMA}$.\\
\textbf{Output:} trained network parameters $w$.
\begin{algorithmic}[1]
    \STATE Initialize replay buffer: $\mathcal{B} \gets \{\}$.
    \STATE $\tau \gets \alpha$
    \STATE $\lambda \gets 1$
    \FOR{$t \in \{1, ..., \infty\}$}
        \STATE Obtain new data batch $N_t$.
        \STATE Sample buffer data batch $B_t$
        \FOR{$s \in \{1, ..., S\}$}
            \STATE Compute loss $\mathcal{L}(w)$ using $N_t$ and $B_t$.
            \STATE Compute loss gradient $\nabla \mathcal{L}(w)$.
            \STATE $\tau \gets \tau + \Delta \tau$
            \IF{Class Incremental}
                \STATE Update known classes list with $N_t$
                \STATE Increase $\lambda$ with new classes
            \ELSE
                \STATE $\lambda \gets \lambda + \Delta\tau$
            \ENDIF
            \IF{s = 1}
                \STATE Compute K-FAC factors $A$ and $G$ with $N_t$ and $B_t$ ($B_t$ influence weighted by $\lambda$)
                \FOR{$l \in {1, ..., L}$}
                    \STATE $A_{EMA,l} \gets (1-\alpha_{EMA}) A_{EMA,l} + \alpha_{EMA} A_l$
                    \STATE $G_{EMA,l} \gets (1-\alpha_{EMA}) G_{EMA,l} + \alpha_{EMA} G_l$
                    \IF{$l = L$ and $L$ changed shape}
                    \STATE $G_{EMA,l} \gets  G_l$
                    \ENDIF
                \ENDFOR
                \STATE $F_{EMA} \gets$ ${A_{EMA}, G_{EMA}}$
                \STATE $F_{INV} \gets (F_{EMA}+\tau\mathbf{I})^{-1}$
            \ENDIF
            \STATE$ \Tilde{\nabla} \mathcal{L}(w) \gets F_{INV} \nabla \mathcal{L}(w)$
             \STATE $w \gets w - \alpha \Tilde{\nabla} \mathcal{L}(w)$
        \ENDFOR
        \STATE $B \gets$ Reservoir.update$(B, Nt, maxsize)$
    \ENDFOR
\end{algorithmic}
\end{algorithm}


The K-FAC computations are done after weighting with $\lambda$ the data related to the buffer. 

\section{Experimental Setup}
\label{app:experimental setup}
\paragraph{Code}The entire code used to perform the full experiments (CIFAR100, TinyImageNet e Clear) can be found in the anonymous repository at \url{https://anonymous.4open.science/r/CAR-8412}. The anonymization process could have removed some important parts of the code but this is highly unlikely. The code was built on a combination of the repository for the OCL survey \cite{DBLP:conf/iccvw/Soutif-Cormerais23} and the nngeometry repository \cite{george_nngeometry}.

\paragraph{Hardware}The experiments were performed in a Linux cluster equipped with Nvidia Tesla V100 16GB GPUs and Intel Xeon Gold 6140M CPUs. The training was not parallelized between the GPUs. 

\paragraph{Main training setup} All the experiments have been performed as in \cite{DBLP:conf/iccvw/Soutif-Cormerais23}. We only report the most important aspects here, referring to the original paper for all the additional details. The datasets used for the main experiments are Split-CIFAR100 and Split-TinyImageNet. Both are class-incremental benchmarks, divided in 20 tasks, with a number of new classes encountered at each new experience (5 in Split-CIFAR100 and 10 in Split-TinyImageNet). The model used for the main experiments is a Slim-ResNet18 trained using SGD. SGD can help the model adapt faster when new data are encountered after a task boundary, while ADAM \cite{kingma2014adam} will need some time to update its running statistics. The batches are tiny, with 10 examples from the current portion of the stream and 10 from the replay buffer. The replay buffer for Split-CIFAR100 was kept at 2000 samples, while for Split-TinyImageNet at 4000. The metrics computed at the end of the training (cumulative accuracy and linear probing accuracy) are computed on a test stream. The continual metrics (average anytime accuracy and worst-case accuracy) are computed after the training on each batch on a heald-out validation set. The only additional information OCAR has access to is the number of classes on each experience (5 for one and 10 for the other). This information is only used to increase the weight $\lambda$ for the computation of the Fisher Information on the buffer data. If the buffer data contains $n$ classes and the number of classes observed in the current portion of the stream is $k$, then the FIM of the buffer will weight $\frac{n}{k}$. While this information is useful, if it is not accessible, other weighting procedures can be performed, or, it can simply be estimated by watching how many classes are observed in the first few steps of the training. Usually few steps are enough to observe all the different classes present in the task. 

The same setup has been used in \cite{yoo2024layerwise}, with the only difference they used 10 seeds for their experiments instead of only 5. There is also a possible difference in the hyperparameter selection. Given the equality of the training and validation setup, we compared our methods' performances directly with the results of \cite{yoo2024layerwise} and \cite{DBLP:conf/iccvw/Soutif-Cormerais23}. This minimizes the probability of errors or bugs and uses the results obtained in the best conditions. 

\paragraph{Training time}
We report the training time to complete a single task on Split-CIFAR100 of different methods. Since OCAR and other OCL methods do not increase their computational cost over time, we report the training times on the first task to avoid unnecessary retraining of all the methods. We train on the first task without the continual evaluation to remove the evaluation overhead and provide a fair comparison between the methods. 

\begin{table}[ht]
    \centering
    \caption{Training Time for the First Task on Split-CIFAR-100.}
    \label{tab:training_time_split_cifar100}
    \begin{tabular}{@{}lc@{}}
        \toprule
        \textbf{Method} & \textbf{Training Time (seconds)} \\ \midrule
        ER & 14 \\
        ER + LWF & 15 \\
        MIR & 31 \\
        ER-ACE & 17 \\
        DER & 17 \\
        RAR & 72 \\
        SCR & 131 \\        
        LPR & 213 \\
        \midrule
        OCAR(Ours) & 38 \\
        \bottomrule
    \end{tabular}
\end{table}
OCAR is about 3 times slower than standard ER, which does not perform any additional operations apart from the basic SGD loop. It is also faster than other sophisticated methods such as MIR, SCR, and LPR. We underline that in our tests LPR resulted much slower than what the authors found in their original paper \cite{yoo2024layerwise}. We did not identify the cause of this. It can be something related to our hardware or the implementation details of our code. 

\paragraph{Hyperparameters selection}
As with everything else, the hyper-optimization is performed as in \cite{DBLP:conf/iccvw/Soutif-Cormerais23}. The best configuration is selected as the best performing in the cumulative accuracy metrics on the validation set at the end of the fourth experience. This approach is suboptimal when working on nonstationary data. We notice that in our model, this shorter stream would select a combination of parameters prone to having high plasticity and lower stability. For this reason, instead of selecting a fixed Tikhonov regularizer $\tau$, we instead selected a speed of increase for $\tau$. In this way, we found a much more robust approach. Differently from \cite{DBLP:conf/iccvw/Soutif-Cormerais23}, all our hyperparameters for OCAR have been selected after trying 100 combinations using the tree-structured Parzen estimator algorithm, instead of 200 trials used in the original survey. This can give our method a small disadvantage, but we found it to be sufficient to get good results. The only exception was OCAR-ACE on Split-TinyImageNet where 200 trials were needed due to the particular complexity of using a loss function not derived from a KL divergence on a complex dataset as TinyImageNet. We want to highlight that in the LPR paper \cite{yoo2024layerwise}, it seems the hyperparameters selection is done by training on all the experiences and selecting the best final accuracy. It is possible this can result in an advantage, avoiding the problem of the short and partial stream used in \cite{DBLP:conf/iccvw/Soutif-Cormerais23} and for our method. 




\section{Fisher Matrix Computations}
\label{app: Fisher computations}
The Fisher Information Matrix can be computed both as the variance of the score (the gradient of the log-likelihood) or as the negative expected value of the curvature of the log-likelihood. For a classification problem we assume the model $f_w$ is predicting a probability vector probability vector $\mathbf{p}=[p_1, \ldots, p_K]$ where each $p_k$ is the probability that $y_k=1$, such that $\mathbf{p}=f(x;w)$. The distribution assumed is then a categorical distribution. On a single example $x$ the FIM can be computed as:

\begin{align*}
F(w) &=\mathbf{E}_{\mathbf{y}\sim Cat(\mathbf{y}|x;w)}[\nabla_w^2  \, logCat(\mathbf{y}|x;w)] = \mathbf{E}_{\mathbf{y}\sim Cat(\mathbf{y}|x;w)} [\nabla_w^2 \, \sum_k y_k log f_w(x)_k] = \\
&=  \mathbf{E}_{\mathbf{y}\sim Cat(\mathbf{y}|x_n;w)}[
\begin{bmatrix}
           \sum_k \frac{y_{k}}{f_w(x)_k}\frac{\partial f_w(x)_k}{\partial w_1}  \\
           \vdots \\
          \sum_k \frac{y_{k}}{f_w(x)_k}\frac{\partial f_w(x)_k}{\partial w_H} 
\end{bmatrix}
\begin{bmatrix}
           \sum_k \frac{y_{k}}{f_w(x)_k}\frac{\partial f_w(x)_k}{\partial w_1}  \\
           \vdots \\
          \sum_k \frac{y_{k}}{f_w(x)_k}\frac{\partial f_w(x)_k}{\partial w_H} 
\end{bmatrix}^T] = \\
&= \mathbf{E}_{\mathbf{y}\sim Cat(\mathbf{y}|x_n;w)}[
\begin{bmatrix}
           \sum_k \frac{y_{k}^2}{f^2_w(x)_k}(\frac{\partial f_w(x)_k}{\partial w_1})^2 
           &\sum_k \frac{y_{k}^2}{f^2_w(x)_k}\frac{\partial f_w(x)_k}{\partial w_1} \frac{\partial f_w(x)_k}{\partial w_2}
           \ldots \\
          \sum_k \frac{y_{k}^2}{f^2_w(x)_k}\frac{\partial f_w(x)_k}{\partial w_1} \frac{\partial f_w(x)_k}{\partial w_2} \\         
           \vdots \\         
\end{bmatrix}] = \\
&= \begin{bmatrix}
           \sum_k \frac{1}{f_w(x)_k}(\frac{\partial f_w(x)_k}{\partial w_1})^2   
           &\sum_k \frac{1}{f_w(x)_k}\frac{\partial f_w(x)_k}{\partial w_1} \frac{\partial f_w(x)_k}{\partial w_2}
           \ldots \\
          \sum_k \frac{1}{f_w(x)_k}\frac{\partial f_w(x)_k}{\partial w_1} \frac{\partial f_w(x)_k}{\partial w_2} \\
           \vdots \\
\end{bmatrix}
\end{align*}

Note how each element is a sum of the contribution from each class, weighted by the inverse predicted probability for that class. The Fisher computed for a gradient composed of the sum (or mean) of gradients from multiple examples is the sum (or mean) of the individual Fishers. This is due to the required assumption to compute the Fisher: being at the optimum of the log-likelihood optimization, assuming the predicted parameters are the true ones. A consequence of this assumption is that the correlations between gradients of different examples will be zero: $\mathbf{V}[\nabla(\mathbf{y}_i| x_i; w) + \nabla(\mathbf{y}_j| x_j; w)] = \mathbf{V}[\nabla(\mathbf{y}_i| x_i; w) ] + \mathbf{V}[\nabla(\mathbf{y}_j| x_j; w) ]$. Then, our $\lambda$ to give more weight to the buffer data is easily implemented. 
For the same assumption about the optimum, the variance of the score is computed as the expected value of the squared gradients: the squared expected value would be equal to zero.

\paragraph{Last Layer FIM} Focusing on the FIM of the last layer and ignoring all the correlations between the weights for a moment, we can compute the diagonal element of a classifier's weight. Assuming the loss $\mathcal{L}$ is a standard cross entropy loss:
\begin{equation*}
    \frac{\partial \mathcal{L}}{\partial w_{j,i}} = h_j (p_i-y_i)
\end{equation*}
where $p_i$ is the prediction for the class $i$ and $y_i$ an indicator function for the real value. 
The variance of the score for this weight is:
\begin{equation*}
    \mathbf{E}\left[ \left(\frac{\partial \mathcal{L}}{\partial w_{j,i}}\right)^2\right] =  \mathbf{E}[h_j^2 (p_i - y_i)^2] = h_j^2 p_i (1-p_i)
\end{equation*}

If we transform the partial derivative with the inverse variance:
\begin{equation*}
     \mathbf{E}\left[ \left(\frac{\partial \mathcal{L}}{\partial w_{j,i}}\right)^2\right]^{-1} \frac{\partial \mathcal{L}}{\partial w_{j,i}} = \frac{1}{h_j}\frac{p_i - y_i}{p_i (1 - p_i)}
\end{equation*}
In class-incremental settings, at task boundaries, we can assume the probability predicted for the new classes will be pretty low due to the random initialization of the weights. In this case, the gradient of the weights connected to this new unit will be high while the variance of the gradient will be very low, greatly accelerating the gradient, and contributing to possible instabilities due to the assumption of the Fisher of predicting the "true" distribution while, particularly at task boundaries, this is not true. 
Clearly this analysis ignores all the complex cross-correlations between the weights. To visualize the complete effect, we plot in Figure \ref{fig:ratio_plot} the ratio between the norm of the gradient after being transformed with OCAR and the norm of the original cross-entropy gradient. We show only a subset of the training on Split-CIFAR100, using a small $\tau$ (lr/100). All task boundaries are clearly visible, showing the great acceleration the FIM provides when new classes arrive. This underlines the importance of the Tikhonov regularization ($\tau$) in class-incremental settings, where the Fisher can exacerbate some instabilities when the predictions of the model are completely off. 
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/ratio_last_plot.pdf}
    \caption{Ratio between the norm of the gradient after being transformed with OCAR and the norm of the original gradient when a small $\tau$ is used}
    \label{fig:ratio_plot}
\end{figure}

\section{Qualitative Experiment}
\label{app: qualitative}
The code of the qualitative experiment is included in our repository. We used a basic linear model with 10 inputs and a single output trained to solve a simple linear regression problem with an MSE loss. 10 tasks with 1000 samples per task are randomly generated. Each task has a different multivariate normal from which input data are sampled and different real weights that should be estimated by the mode. The data are accessed as a stream, with a single pass per batch. The  Standard Vitter algorithm is used to keep a buffer of old data. Each method uses a batch composed of 10 new data and 10 data sampled from the buffer. 

Natural Gradient is applied directly by estimating the full Fisher information of the model with an EMA of the FIM computed on the single batches. This FIM after the EMA is used to precondition the gradient. 

OCAR uses the exact same approach but uses a $\lambda$ to give more weight to old data and uses a scheduling of both $\lambda$ and $\tau$ (the Tikhonov regularize) to increase them both in time.
EWC is more tricky to implement in its basic form for online problems. Our approach is very similar to what is done in \cite{mai2022online}, an online extension of the EWC++ strategy \cite{chaudhry2018riemannian}. The difference is that we use also replay data for computing the gradient. Namely, the loss is computed on both old and new data, but the Fisher (being a penalization) is computed only on old data (an EMA is kept also in this case for estimation). Given that no task boundaries are accessible, the penalization of the movement of the weights is done with respect to the weights at the previous step in time. In this way, a more regularized descent should be followed, penalizing the displacement from step to step using the Fisher Information of old data. 
The loss is a basic MSE but for analysis purposes, the figure \ref{app: qualitative} shows the cumulative loss (simply the loss experienced in each batch accumulated in time) and the cumulative loss on all previous data (at each step the loss on all previous data is computed after the optimization step and accumulated in time). The first measure shows a general capability of fast adaptation while the second an ability to find an optimum stable for the entire previous stream.

A hyperparameter selection is performed to select the best setting on the sum of the final value of both these stability and plasticity measures. Then, on the same data, using the best hyperparameters, the results are averaged across 10 random seeds to test fro training stability with different optimization paths.


\section{Online CLEAR}
\label{app:clear}
Following the very recent work of \cite{yoo2024layerwise}, we tested our method also on the Online CLEAR benchmark \cite{lin2021clear}, a domain incremental learning scenario. This scenario is fundamentally different from the class-incremental, with the same classes that undergo some sort of evolution. The CL aspect is then less impactful, with much more forward/backward transfer and less catastrophic forgetting due to the similarity of the tasks in different domains. 
We followed the same settings as in LPR paper \cite{yoo2024layerwise}, that are very similar to the standard approach used for our main experiments, but with some differences: the use of the full ResNet18 instead of the Slim version and 10 gradient steps per batch. Unfortunately, we encountered some bugs when their exact code was used in ours, requiring some slight modifications. Not being able to ensure the exact same conditions, we rerun all the experiments. We decided to compare the baseline of Experience Replay, the very recent LPR approach, and our method. ER-ACE is tailored for task-incremental settings, so we avoid its use. In this scenario, we increase $\lambda$ not with the number of classes in the buffer (as in class incremental), but we increase it in time as new batches arrive. LPR was tested using 100 samples from the buffer to estimate its preconditioner.






\begin{figure*}[t!]
    \centering
    %%%%%%%%%%%%%%%%%%%%%%%%%%
    %%% OCAR
    %%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/manifold_ng_edo_best_lambda0_Task0.pdf}
        \caption{OCAR - T1.}
        %\label{fig:proj_car}       
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/manifold_ng_edo_best_lambda0_Task1.pdf}
        \caption{OCAR - T2.}
        %\label{fig:proj_car}       
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/manifold_ng_edo_best_lambda0_Task2.pdf}
        \caption{OCAR - T3.}
        %\label{fig:proj_car}       
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/manifold_ng_edo_best_lambda0_Task3.pdf}
        \caption{OCAR - T4.}
        %\label{fig:proj_car}       
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/manifold_ng_edo_best_lambda0_Task4.pdf}
        \caption{OCAR - T5.}
        %\label{fig:proj_car}       
    \end{subfigure}
    %%%%%%%%%%%%%%%%%%%%%%%%%%
    %%% ER-Reservoir
    %%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/manifold_reservoir_Task0.pdf}
        \caption{ER - T1.}
        %\label{fig:proj_car}       
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/manifold_reservoir_Task1.pdf}
        \caption{ER - T2.}
        %\label{fig:proj_car}       
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/manifold_reservoir_Task2.pdf}
        \caption{ER - T3.}
        %\label{fig:proj_car}       
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/manifold_reservoir_Task3.pdf}
        \caption{ER - T4.}
        %\label{fig:proj_car}       
    \end{subfigure}
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/manifold_reservoir_Task4.pdf}
        \caption{ER - T5.}
        %\label{fig:proj_car}       
    \end{subfigure}
    %%%%%%%%%%%%%%%%%%%%%%%%%%
    %%% Learning curves
    %%%%%%%%%%%%%%%%%%%%%%%%%%
   \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/learning_curve_T0.pdf}
        \caption{T1.}
        %\label{fig:curve_t0}
    \end{subfigure}
   \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/learning_curve_T0.pdf}
        \caption{T2.}
        %\label{fig:curve_t0}
    \end{subfigure}
   \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/learning_curve_T0.pdf}
        \caption{T3.}
        %\label{fig:curve_t0}
    \end{subfigure}
   \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/learning_curve_T0.pdf}
        \caption{T4.}
        %\label{fig:curve_t0}
    \end{subfigure}
   \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/learning_curve_T0.pdf}
        \caption{T5.}
        %\label{fig:curve_t0}
    \end{subfigure}
   
    \caption{2D projections of the training trajectories for ER and OCAR on Split MNIST (5 Tasks). The black stars highlight the task boundaries, the red star the final model. We also show learning curves on each task separately. }
    \label{fig:app_traj}
\end{figure*}

\section{Learning Trajectory on Split MNIST}
\label{app: trajectory}

The model is a small feedforward network with two hidden layers of $100$ units and ReLU activations. The model is trained online with 3 passes for each mini-batch with a small replay buffer of $100$ elements, corresponding to $10$ samples per class by the end of training.

To plot the 2D learning projections in Figure \ref{fig:app_traj}, we follow a procedure similar to \cite{DBLP:conf/iclr/MirzadehFGP021}.

We consider the 2D plane that intersects the model's initialization $w_0^*$, the model after the first task $w_1^*$, and the final model $w_5^*$. Many other possibilities were considered before this choice (e.g. using different tasks or random directions), but they all resulted in qualitatively similar plots. The coordinates system is obtained by orthonormalizing the directions $u = w_1^* - w_0^*$ and $v = w_5^* - w_0^*$, obtaining $\bar{u} = \frac{u}{\left\lVert u \right\rVert}$ and $\bar{v} = \frac{v - \cos(u, v) u}{\left\lVert v - \cos(u, v) u \right\rVert}$. Given a model $w$, its coordinates in the 2D space are the unique $\langle x, y \rangle$ such that $w = x \bar{u} + y \bar{v} + w_0^*$. Notice that each method has different values for $w_1^*$ and $w_5^*$, and therefore different 2D planes and coordinate systems are chosen for each method. Since the coordinates are not meaningful and cannot be compared across plots, we do not show them in Figure \ref{fig:app_traj}.


\section{Additional Figures}
\label{app: figures}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/mean_top1_accuracy_over_time_CIFAR.pdf}
    \caption{Accuracy over time on the validation set for Split-CIFAR100 experiment.}
    \label{fig:enter-label}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/mean_top1_accuracy_over_time_IMAGENET.pdf}
    \caption{Accuracy over time on the validation set for Split-TinyImagenet experiment.}
    \label{fig:enter-label}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/mean_top1_accuracy_over_time_CLEAR.pdf}
    \caption{Accuracy over time on the validation set for Online CLEAR experiment.}
    \label{fig:clear accuracy}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/effective_lr.pdf}
    \caption{Effective step size against the FIM eigenvalues. The step size in the directions with small eigenvalues is regularized via $\tau$, while large eigenvalues are unaffected by it. The learning rate $\alpha$ affects all the directions equally.}
    \label{fig:lr_ratio_curve}
\end{figure}


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/learning_curve.pdf}
        \caption{Average of all tasks.}
        \label{fig:curve_joint}
    \end{subfigure}   
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/manifold/learning_curve_T0.pdf}
        \caption{First Task.}
        \label{fig:curve_t0}
    \end{subfigure}
    
    \caption{Learning curves on the first task (\ref{fig:curve_t0}) and the average accuracy for all tasks (\ref{fig:curve_joint}). }\label{fig:lr_curves}
\end{figure}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/grid_avg_acc.pdf}
        %\caption{}\label{fig:lr_ratio_grid}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/grid_forget.pdf}
        %\caption{}\label{fig:lr_ratio_grid}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/grid_plast.pdf}
        %\caption{}\label{fig:lr_ratio_grid}
    \end{subfigure}

    \caption{Grid search over $\alpha$ and $\frac{\alpha}{\tau}$: (left) average accuracy, (middle) forgetting on the first task, (right) plasticity measured as the accuracy on the final task. Metrics are computed on the test stream at the end of training.}\label{fig:lr_ratio_grid_complete}
\end{figure*}
