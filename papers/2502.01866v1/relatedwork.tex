\section{Related Work}
\textbf{Continual Stability and OCL:} Most continual learning methods assume that stability must be kept at the expense of plasticity~\cite{DBLP:journals/pami/LangeAMPJLST22,DBLP:journals/pami/MasanaLTMBW23}, the so called plasticity-stability tradeoff, and therefore are designed to preserve knowledge about previous tasks to mitigate catastrophic forgetting~\cite{french1999catastrophic}. However, recent evidence in \cite{DBLP:conf/iclr/LangeVT23,DBLP:conf/iclr/CacciaAATPB22} suggest that even the methods with high "stability" measured at the end of tasks suffer from high instability and forgetting immediately after the task switch, and they recover the lost performance over time. \cite{DBLP:journals/corr/abs-2406-05114} found evidence of the stability gap even in incremental i.i.d. settings. In OCL settings, \cite{DBLP:conf/iccvw/Soutif-Cormerais23} showed that some state-of-the-art methods are unable to outperform a simple reservoir sampling baselines on some fundamental stability metrics. 
Furthermore, CL methods also fail at keeping plasticity, and \cite{DBLP:journals/nature/DohareHLRMS24} provides evidence of the loss of plasticity in deep continual networks. Overall, the literature suggests that CL methods fail at both stability and plasticity due to instabilities in the learning dynamics. Recently, some methods such as OnPro~\cite{wei2023online} and OCM~\cite{guo2022online} proposed novel self-supervised auxiliary losses and prototype-based classifiers, two approaches orthogonal to our optimization-based method.

% forgetting may not be a consequence of intrinsic interference in the data but instead the result of undesirable learning dynamics at the task boundaries.
\textbf{Optimization in Continual Learning:} Most CL optimization algorithms are designed to prevent forgetting by removing interfering updates. GEM \cite{DBLP:conf/nips/Lopez-PazR17,DBLP:conf/iclr/ChaudhryRRE19} models interference using the dot product of the task gradients and constrains the model updates to have positive dot products with the gradients of previous tasks. Subsequent work explored orthogonal projection methods~\cite{DBLP:conf/iclr/SahaG021,DBLP:conf/aistats/FarajtabarAML20} that either extend the idea of interfering gradients or project in the null space of the latent activations. \cite{DBLP:conf/nips/MirzadehFPG20} discusses the relationship between the curvature of the first task and the forgetting, proposing a hyperparameter schedule that implicitly regularizes the curvature. More recently, LPR\cite{yoo2024layerwise} exploits proximal optimization in the L2 space of latent activations, and it is the only projection-based optimizer compatible with replay. \cite{DBLP:journals/corr/abs-2311-04898} proposes a combination of GEM and replay as a potential mitigation for the stability gap.

\textbf{Natural Gradient and FIM in CL:} Natural gradients can be used to train neural networks thanks to efficient approximations of the Fisher Information Matrix (FIM)~\cite{martens2012training}, such as the K-FAC~\cite{martens2020new} and E-KFAC~\cite{DBLP:conf/nips/GeorgeLBBV18}. Interestingly, \citet{DBLP:conf/icml/Benzing22} showed that K-FAC seems to work better than the full FIM in some empirical settings, which is connected to a form of gradient descent on the neurons. In continual learning, the FIM is typically used to approximate the posterior of the weights with a Laplace approximation. The result is a quadratic regularizer that is combined with the loss on new data, as introduced by EWC~\cite{kirkpatrick2017overcoming} and its several extensions~\cite{DBLP:conf/eccv/ChaudhryDAT18,liu2018rotate,DBLP:journals/corr/abs-1712-03847}. \citet{DBLP:conf/iclr/MagistriTS0B24} proposed to use the FIM only for the final layer, which can be computed efficiently with a closed-form equation. As an alternative, FROMP~\cite{pan2020continual} computes a Gaussian Process posterior, which is also used to estimate the importance of samples in the replay buffer. \citet{daxberger2023improving} proposes a method that combines EWC, replay, and knowledge distillation. More relevant to our work, NCL~\cite{DBLP:conf/nips/KaoJVBH21} proposed a modified natural gradient step with a quadratic posterior as in EWC. Here the Fisher is computed only at the boundaries and the method does not support rehearsal, making it difficult to implement in OCL.

\textbf{Comparison with our work} Most CL methods use the FIM to compute a Laplace approximation of the posterior. This is not possible in OCL because the model is never assumed to be at a local minimum, not knowing task boundaries or length. In general, in the CL literature, the FIM is often restricted to its use in quadratic penalties. While quadratic regularizers are easier to use, we show that the use of the Fisher as a gradient preconditioner is a promising direction, improving the optimization path. Many of the limitations found for EWC and similar methods may be due to some suboptimal choices in the use of the FIM, such as the use of the empirical FIM, popular in the CL literature but with different properties from the FIM~\cite{kunstner2019limitations}. Another difference with the literature is that our optimizer is compatible with replay, unlike most projection-based methods. Furthermore, while most methods penalize plasticity indirectly to prevent forgetting, our approach is designed to improve both on learning speed and forgetting.