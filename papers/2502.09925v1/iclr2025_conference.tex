
\documentclass{article} % For LaTeX2e
\usepackage{amsmath, amssymb, bm}
\usepackage{iclr2025_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands}
\usepackage{hyperref}
\usepackage{url}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{colortbl}
% \usepackage[table]{xcolor}
\usepackage{longtable}
\usepackage{hhline}
% \usepackage{amsmath}
\usepackage{adjustbox} 
% \title{TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types}
\title{
     TaskGalaxy: \\
     Scaling Multi-modal Instruction Fine-tuning \\
     with Tens of Thousands Vision Task Types
}
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu}
% }
\author{Jiankang Chen$^{\ast}$, Tianke Zhang, Changyi Liu, Haojie Ding, Yaya Shi, Feng Cheng,\\ 
\textbf{Huihui Xiao, Bin Wen$^{\dagger}$, Fan Yang, Tingting Gao, Di Zhang} \\
Kuaishou Technology\\
\texttt{\{chenjiankang,zhangtianke,liuchangyi,wenbin,yangfan\}@kuaishou.com}
}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\vspace{-0.6cm}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{This work was done while Jiankang Chen was interning at Kuaishou.}
\footnotetext[2]{Corresponding author.} 
\begin{abstract}
Multimodal visual language models are gaining prominence in open-world applications, driven by advancements in model architectures, training techniques, and high-quality data. However, their performance is often limited by insufficient task-specific data, leading to poor generalization and biased outputs. Existing efforts to increase task diversity in fine-tuning datasets are hindered by the labor-intensive process of manual task labeling, which typically produces only a few hundred task types. To address this, we propose TaskGalaxy, a large-scale multimodal instruction fine-tuning dataset comprising 19,227 hierarchical task types and 413,648 samples. TaskGalaxy utilizes GPT-4o to enrich task diversity by expanding from a small set of manually defined tasks, with CLIP and GPT-4o filtering those that best match open-source images, and generating relevant question-answer pairs. Multiple models are employed to ensure sample quality. This automated process enhances both task diversity and data quality, reducing manual intervention. Incorporating TaskGalaxy into LLaVA-v1.5 and InternVL-Chat-v1.0 models shows substantial performance improvements across 16 benchmarks, demonstrating the critical importance of task diversity. TaskGalaxy is publicly released at \href{https://github.com/Kwai-YuanQi/TaskGalaxy}{\texttt{https://github.com/Kwai-YuanQi/TaskGalaxy}}.


\end{abstract}

\section{Introduction}

Recent breakthroughs in artificial intelligence have been fueled with the development of a large number of large multimodal models (LMMs)~\citep{llava, qwenvl, llava_v1_6, internlm2_5}. These models are typically composed of a pre-trained visual encoder~\citep{clipvit}, a pre-trained large language model~\citep{llama}, and a lightweight structure (Q-former for BLIP2~\citep{blip2}, two layers of MLP for LLaVA~\citep{llava}, etc.) connecting the above two, which have been adopted in various domains such as image captioning, object detection, visual question answering and other related fields. How to improve the model's performance in various mission scenarios is of great importance for deploying such a model into an open-world system.

To enhance the performance of LMMs in specialized and general-purpose domains, mainstream approaches focus on three key areas: optimizing model architectures~\citep{structure_2, structure_3, structure_4, blip2, llava_v1_6}, improving training strategies~\citep{mdpo, llava_rlhf, dpo_model, zhu2024model}, and constructing high-quality data~\citep{vision_flan, sharegpt4v, llava_math}. While advances in model architectures and training strategies are crucial, our work focuses on addressing the critical challenges in the data domain. Current multimodal models typically undergo a biphasic training process: a pre-training phase with image-text pairs for visual-textual alignment, followed by a supervised fine-tuning (SFT) phase with command-format data to refine multimodal abilities. However, the diversity of tasks in the pre-training phase is limited, affecting the generalization ability of visual language models~\citep{multiinstruct}. Recent research~\citep{bliva, llava_next_interleave, llava_math, multiinstruct, lamm, llava, zhao2024retrieval} has focused on expanding task diversity in the supervised fine-tuning phase to enhance instruction adherence and logical reasoning. Despite these efforts, instructional datasets still face limitations in task diversity, leading to performance bottlenecks. For example, Vision-Flan~\citep{vision_flan} proposed a comprehensive dataset with tasks like OCR and object detection but required extensive manual labeling and yielded only around 200 task types. Similarly, VisionLLM v2~\citep{visionllmv2} aggregated numerous task types but needed task-specific decoders, limiting the dataset's generalizability.
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{Pictures/performance_overview_2.png}
    \caption{\textbf{An illustration of the benefits of high task type coverage in TaskGalaxy for the SFT stage.} We presented the performance of LLaVA-v1.5-13B and InternVL-Chat-v1.0-7B models, both before and after integrating TaskGalaxy into the fine-tuning dataset.
    }
    \vspace{-0.5cm}
    \label{fig:performance}
\end{figure}

\vskip 0.17in

To address the limited task diversity in existing fine-tuning datasets and overcome current methodological constraints, we propose a novel pipeline for generating fine-tuning data. This pipeline enables the simultaneous construction of a wide variety of task types using contemporary multimodal models~\citep{hurst2024gpt, glm4v, internvlchat, internvl2_26b}. We introduce an innovative supervised fine-tuning dataset, \textbf{TaskGalaxy}, which encompasses over 19,000 hierarchical task types, significantly enhancing task diversity in multimodal scenarios. Specially, to minimize manual involvement and effectively construct multi-level multimodal task type data for fine-tuning dataset generation, our data generation pipeline is divided into five main steps as follows: (1) we initially designed several foundational multimodal graphic task types as seeds. These include tasks such as Optical Character Recognition (OCR) and Image Description for visual understanding, Logical Reasoning and Analysis for deep reasoning, as well as Multiple Choice Questions and Fill-in-the-Blank Questions for question-and-answer formats and so on. We then design appropriate prompts to enable GPT-4o to iteratively expand the range of task types both at the same level and to generate new, more detailed task types at subsequent levels. Notably, the number of task types per level is flexible and controllable, allowing for theoretically infinite generation. In our work, we have generated over 19,000 distinct types of tasks. (2) To provide image sources for all task types, we collected a diverse set of images from multiple open-source datasets. (3) We preliminarily select task types that are strongly correlated with each image using CLIP's~\citep{clipvit} similarity between image embeddings and text embeddings. Appropriate prompt for GPT-4o is then designed to further filter task types corresponding to each image. (4) GPT-4o is utilized to generate question and answer pairs related to each task type for the selected images. (5) To ensure the reasonableness and alignment of the questions, task types, and images, we employ three robust open-source multimodal models GLM4v~\citep{glm4v}, InternVL2\_26B~\citep{internvl2_26b}, and InternVL-Chat-v1.5~\citep{internvlchat} as judges for scoring and screening the generated pairs. Finally, we generate a novel high-quality fine-tuning dataset \textbf{TaskGalaxy} containing 19,227 task types and around 410k visual Q\&A samples. After fine-tuning LLaVA-v1.5-7B \& 13B and InternVL-Chat-v1.0-7B \& 13B with the \textbf{TaskGalaxy} dataset along with raw fine-tuning data, the models showed an average improvement of 4.5 \& 3.83, 3.0 \& 3.64 points across fifteen benchmarks compared to the only original data fine-tuning, and Figure~\ref{fig:performance} illustrates the performance gains achieved by LLaVA-v1.5-13B and InternVL-Chat-v1.0-7B after incorporating TaskGalaxy  during the SFT stage. Additionally, it demonstrated an increase of  68 points for LLaVA-v1.5-13B on the MME benchmark, indicating that our dataset enhances the model's generalization ability.\\
% \vskip 0.15in
% \vspace{0.3cm}

The contributions of this study are as follows.
\begin{itemize}
\item We propose a novel multi-modal instruction fine-tuning dataset, \textbf{TaskGalaxy}, which contains tens of thousands+ of vision task types and approximately 413k samples, addressing the limitation of task diversity in existing datasets.
\item An almost fully automated pipeline for creating a comprehensive fine-tuning dataset of diverse task types was designed. This pipeline can be flexibly expanded by incorporating high-quality images, task types, and question-answer samples.
\item Incorporating \textbf{TaskGalaxy} into the fine-tuning of LLaVA-v1.5 and InternVL-Chat-v1.0 resulted in improvements across all 16 benchmarks compared to fine-tuning with the original data which proves that expanding the diversity of visual task types and high-quality question-answer pairs associated with these tasks significantly enhances the generalization capabilities of multimodal models.
\end{itemize}
\vspace{-0.3cm}
\section{TaskGalaxy Dataset}
\subsection{Overview}
The TaskGalaxy dataset consists of 19,227 hierarchical task types, ranging from OCR and image description to fine-grained object recognition and complex logical reasoning and so on. These task types originate from a small set of manually defined seeds, further expanded by GPT-4o through prompt design. CLIP’s graphical similarity is used to filter task types that best match specific images. GPT-4o then generates corresponding question-answer pairs, which are evaluated by three open-source models to select the highest-quality samples. This process yields 413,648 high-quality question-answer pairs aligned with these hierarchical task types.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{Pictures/pipeline.pdf}
    \caption{\textbf{An overview of the task type and high-quality question-answer pairs generation pipeline for TaskGalaxy}. We initially define the first level of visual task types, along with a small number of second and third level task types. Subsequently, we instruct GPT-4o to extend these to a broader range of task types. We then collect image modalities from existing publicly available datasets for matching task types with images, filtering, generating question answers related to task types, and utilizing the three referee models to obtain final high-quality visual quiz pairs for various task types strongly related to images.}
    \label{fig:pipeline}
    \vspace{-0.5cm}
\end{figure}
\subsection{Dataset Generation Pipeline}
In this section, we outline the pipeline for creating the \textbf{TaskGalaxy} dataset, highlighting the reduction of human intervention and leveraging multiple advanced LLMs. The process encompasses task type expansion, precise matching between task types and images, automated question-answer generation, and referee-based filtering to ensure data quality, with the majority of these steps being fully automated. \textbf{TaskGalaxy} stands out from other fine-tuning datasets due to its extensive visual task diversity, minimal human involvement, rich and high-quality task-related Q\&A data, and scalability. The workflow is depicted in Figure~\ref{fig:pipeline}.

To ensure these key attributes of the \textbf{TaskGalaxy} dataset, we have designed a five-step pipeline:
\vspace{0.15cm}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{Pictures/prompt.pdf}
    \caption{\textbf{The prompt template used in GPT-4o API for first-level task type generation.}}
    \label{fig:prompt}
\end{figure}
\vspace{0.15cm}

\textbf{Hierarchical Task Type Generation.} 
To address the challenge of expanding the range of visual task types in the multimodal domain, manual organization is impractical due to its time-intensive nature. Instead, we leverage a robust multimodal model to automate the construction of diverse task types with minimal human intervention. Rather than randomly generating task types, we designed a hierarchical method. As shown in Figure~\ref{fig:pipeline}(a), we began by manually defining a small set of first-level task types, such as OCR, Image Description, Logical Reasoning. Secondary and tertiary task types, such as OCR$\sim$webpage OCR, Logical Reasoning$\sim$Complex Reasoning$\sim$software and coding, and Detection$\sim$Anomaly Detection, were also specified. These initial task types served as seeds, guiding GPT-4o to expand them at multiple hierarchical levels using specially designed prompts. The detailed prompt for first-level task type expansion is illustrated in Figure~\ref{fig:prompt}, and descriptions of the prompts for secondary and tertiary tasks are included in the Appendix. These prompts have been carefully designed to mitigate the risk of overlap between different task types. Specifically, as illustrated in Figure~\ref{fig:prompt}, the instruction \textquotedblleft Please expand and supplement with new primary-level categories that do not belong to the aforementioned categories\textquotedblright  ( refer to Table~\ref{table:prompt} in the Appendix for additional prompts that generate non-overlapping task types) ensures this objective. This process resulted in the generation of 19,227 hierarchical task types, making TaskGalaxy the most diverse dataset of its kind. Further details on these task types are provided in the Appendix.

\vspace{-0.15cm}
\begin{table}[!htb]
\centering
\caption{\textbf{Summary of Image Sources for the Collection.} We have curated a diverse array of image sources, ranging from natural images~\citep{coco, sa, vision_flan} to specialized~\citep{docvqa, clevr-math, dqa, geos, mapqa,tabmwp} and web-based images~\citep{laion, sbu, cc3m}. This selection aims to ensure the diversity and comprehensiveness necessary to address various hierarchical task types effectively.} 

\resizebox{0.8\linewidth}{!}{%
\label{tab:datasource}
\begin{tabular}{ccc}
\hline
 Name & Image Source & Samples \\
\hline
 ALLaVA & LAION, VFLAN & 326K\\
 Visual Genome & MSCOCO & 108K \\
 MathV360K & DocVQA, IconQA, UniGeo, CLEVR-Math, etc & 31K\\ 
 ShareGPT4V & COCO, SA, SBU, etc & 358K\\
\hline
\end{tabular}
% \end{wraptable}
}
\end{table}
\vspace{0.3cm}
\textbf{Image Collection.} 
To align with the diversity and comprehensiveness of the hierarchical task types, as illustrated in Figure~\ref{fig:pipeline}(b), we collected approximately 800k images from a variety of sources, as shown in Table~\ref{tab:datasource}, including ALLaVA~\citep{allava}, Visual Genome~\citep{vg}, MathV360K~\citep{llava_math}, and ShareGPT4V~\citep{sharegpt4v}. These sources cover a range of tasks, such as object detection (COCO~\citep{coco}), visual question answering (VFLAN~\citep{vision_flan}), segmentation (SA~\citep{sa}), and document-related tasks (DocVQA~\citep{docvqa}). Additionally, CLEVR-Math~\citep{clevr-math} contributes to mathematical problem-solving tasks, while LAION~\citep{laion} provides a broad array of web-sourced images, including artwork and watermarked content. All collected images maintain their original resolution. Additionally, to provide further insight into the image data, the sample size statistics for all image source datasets are provided in the Appendix. Our highly scalable data pipeline can accommodate diverse image types, requiring only a single modality, thus highlighting its flexibility and adaptability.  
\vspace{0.25cm}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{Pictures/qa.pdf}
    \caption{\textbf{Sample images, task types, and Q\&A in \textbf{TaskGalaxy}}. The \textbf{Task Type} refers to the visual task related to the image. \textbf{Question} and \textbf{Answers} are generated by GPT-4o and subsequently filtered by three refereeing models.}
    \label{fig:qa}
\end{figure}
\vspace{0.15cm}

\textbf{Match and Filter.} 
Given the complexity and labor-intensive nature of selecting images for a wide range of hierarchical task types, we utilize the vision language model CLIP~\citep{clipvit}, renowned for its robust text-to-image alignment capabilities, to streamline the initial screening process. CLIP is utilized to identify task type names that best correspond to each image, improving both efficiency and accuracy in matching. As illustrated in Figure~\ref{fig:pipeline}(c), for any image ${x}_i$ and task type text ${t}_j$, we pass them through CLIP's image encoder $\mathcal{I}$ and text encoder $\mathcal{T}$ to obtain their respective embeddings. The match between the image and the task type is then calculated using the following formula: 
\begin{equation}
    {s}_j = \mathcal{I}({x}_{i})\cdot\mathcal{T}({t}_{j}),
\end{equation}
where $\cdot$ denotes the cosine similarity between image embedding and text embedding, ${x}_i$ from image pool, ${t}_j$ from task type pool. To identify the most appropriate task type for a given image, we initially select the $k$ task types that exhibit the highest degree of similarity and $k=10$ in our pipeline. These selected task types form the preliminary sequence for that image. 

Given that task types with the highest $k$ similarity have been initially assigned to each image, some mismatches may still persist. To further refine and filter the task types that are more relevant to the image, we use a specially designed prompt, detailed in the appendix, to guide GPT-4o to further analyze and filter the task types from the initial sequence, identifying those that most closely match the image. The resulting list of task types will serve as the final selection for subsequent processes. Examples of images and their corresponding task type lists are shown in Appendix.

\textbf{Q\&A Generation.} 
A key factor in improving multimodal models for visual question-answering tasks is high-quality training data with diverse question-answer pairs. To address this, we developed high-quality image-matched task types, as illustrated in Figure~\ref{fig:pipeline}(d). We designed specific prompts, detailed in the appendix, to guide GPT-4o in generating questions and answers based on image content and task-type text. This approach ensures diverse question-answer pairs that cover a broad spectrum of visual Q\&A scenarios. The resulting data provide a comprehensive training resource, enabling the model to learn from a wide range of scenarios and enhance its capabilities.

\textbf{Referee Screening.} 
Despite GPT-4o’s superior performance, it may still encounter mismatches between task types, questions, and images. To improve alignment and manage the high costs of closed-source APIs, we used three high-performing open-source multimodal models~\citep{internvl2_26b, internvlchat, glm4v} to evaluate task types and questions for each image, as shown in Figure~\ref{fig:pipeline}(e). Following the principle that "three heads are better than one," these models assessed each task type and question relative to an image on a binary scale, awarding one point for a match and zero otherwise. We integrated these scores, selecting only those with a cumulative score of two or more for inclusion in the TaskGalaxy dataset. This process enhances the accuracy of task type, question, and image matching. To ensure balance across task types, we randomly selected 1-55 samples per task type from the final dataset.\begin{wrapfigure}{r}{0.5\textwidth} %r表示右侧
    \centering
    \includegraphics[width=0.6\linewidth]{Pictures/task_stastic.pdf}
    \caption{\textbf{Distribution of the number of images across the 19,227 task types in TaskGalaxy.} The ranges 1-10, 21-40 and etc. indicate the number of samples associated with different task types in TaskGalaxy. The corresponding ratios represent the proportion of task types that fall within each specified sample range. }
    \label{fig:task_stastic}
\end{wrapfigure}

Figure~\ref{fig:task_stastic} illustrates the distribution of sample counts across different task types. To provide a more comprehensive understanding of TaskGalaxy, we analyze not only the proportion of samples in each task type but also the hierarchical distribution of task types across layers. Specifically, the ratio of task types in the first three layers follows a 1:2:3 pattern, corresponding to 115, 2796, and 14,370 task types, respectively. TaskGalaxy is a high-quality, supervised fine-tuning dataset covering a diverse range of tasks. Figure~\ref{fig:qa} presents sample images along with their associated task types, questions, and answers from TaskGalaxy.

% Figure~\ref{fig:task_stastic} shows the distribution of sample counts across different task types. In order to gain a more comprehensive understanding of the nature of TaskGalaxy, in addition to the proportion of samples contained within each task type, for the distribution of task types across layers, the ratio of task types in the first three layers is 1:2:3, corresponding to 115, 2796, and 14,370, respectively. We have developed TaskGalaxy, a high-quality supervised fine-tuning dataset covering a wide range of tasks. Figure~\ref{fig:qa} displays images, task types, and their corresponding questions and answers from TaskGalaxy.
Our task types and image data are designed to be expandable. The TaskGalaxy dataset can be automatically updated with additional images and task types through the fine-tuning data pipeline described earlier. This scalability facilitates future iterations, offering opportunities for further updates. To assess the characteristics of our dataset in relation to previous studies, we provide a comparative analysis between our dataset and previous work in the Appendix. And to comprehensively present properities of TaskGalaxy, a detailed dataset card is available at \href{https://github.com/Kwai-YuanQi/TaskGalaxy/blob/main/TaskGalaxy-Data-Card.pdf}{TaskGalaxy Data Card (PDF)}.

% Figure~\ref{fig:image_task_type} illustrates the distribution of sample counts across the different task types, and Finally, we developed TaskGalaxy, a high-quality supervised fine-tuning dataset that encompasses a wide range of tasks. Figure~\ref{fig:qa} presents several images, task types, and the corresponding questions and answers in TaskGalaxy.
% Note that our task types and image data can continue to expand in future stages. The TaskGalaxy dataset can be automatically extended with more images and task types using the fine-tuning data pipeline described above. This flexible scalability allows for subsequent iterations of the dataset, bringing more possibilities for updates and enhancements.


\section{Experiment}

\subsection{Experiment Setup}
\textbf{Model Architecture.}
In the matching and filtering stage, the CLIP-L/14\cite{clipvit} model, developed by OpenAI, is employed. This model utilizes a ViT-L/14 Transformer architecture as the image encoder and a masked self-attention Transformer as the text encoder. In the stage of evaluating the TaskGalaxy dataset, We use the LLaVA~\citep{llava} and InternVL-Chat-v1.0~\citep{internvl2_26b} models. Both models feature a pre-trained visual encoder and a large language model, linked by a two-layer MLP projection layer. LLaVA employs a two-stage training: initially, a subset of the CC3M~\citep{cc3m} dataset pretrains only the projection layer for multimodal alignment. We use the model pre-trained in this phase as the basis for fine-tuning in the subsequent phase. For validation, we selected two variants: LLaVA-v1.5-7B and LLaVA-v1.5-13B. Similarly, InternVL-Chat-v1.0 undergoes two training phases: first, the MLP layers are trained with the LGS-558K~\citep{llava} dataset, followed by training the language model with the LLaVA-Mix-665K~\citep{llava_v1_6} dataset.
% We utilize the LLaVA~\citep{llava} and InternVL-Chat-v1.0~\citep{internvl2_26b}family of models to rigorously verify the validity and effectiveness of our TaskGalaxy dataset. This family of models combines a pre-trained visual encoder and a pre-trained large language model, both of which utilize a two-layer MLP as a projection layer to connect the visual and linguistic representation spaces. For the family of LLaVA, they employ a two-stage instruction fine-tuning process. In the first stage, a subset of the CC3M~\citep{cc3m} dataset is used for pre-training, during which only the projection layer is fine-tuned to achieve multimodal feature alignment. We use the pre-trained model from this initial stage as our base model. For validation, we selected two models of different scales: LLaVA-v1.5-7B and LLaVA-v1.5-13B. For the family of InternVL-Chat-v1.0, the training process consists of two phases, similar to LLaVA. Initially, the MLP layers are trained using the LGS-558K~\citep{llava} dataset. Subsequently, the large language model is trained using the LLaVA-Mix-665K~\citep{llava_v1_6} dataset. 

\textbf{TaskGalaxy Finetuning.}
We incorporated our TaskGalaxy dataset with the original supervised fine-tuning data organized by LLaVA and are gathered from publicly available academic task-oriented datasets~\citep{665_1,vg,665_2,665_3} during the second stage of model training for both models. In this stage, the visual encoder weights were frozen, while the projection layer and large language model weights were fine-tuned.


\textbf{Benchmarks.}
To design a comprehensive instruction fine-tuning dataset for various vision tasks, we selected 16 benchmarks to evaluate model performance. These include MME-Perception (MME)~\citep{mme}, which measures perception abilities across 14 subtasks; MMBench (MMB)~\citep{mmbench}, a multi-choice test covering all proficiency levels; MMBench\_CN (MMB\textsuperscript{CN})~\citep{mmbench}, the Chinese version of MMBench; POPE~\citep{pope}, which assesses illusion using subsets of COCO~\citep{coco} (random, normal, adversarial); LLaVA-in-the-wild (LLaVA\textsuperscript{W})~\citep{llava} and MMVet~\citep{mmvet}, evaluating visual conversation abilities and leveraging GPT-4o for response evaluation; TextVQA (TQA)~\citep{textvqa}, focused on text-related visual question answering; ScienceQA (SQA)~\citep{sciencevqa}, with multiple-choice questions on scientific topics requiring visual answers; MathVista~\citep{mathvista}, assessing mathematical reasoning in visual contexts; ChartQA~\citep{chartqa}, evaluating visual and logical reasoning over charts; AI2D~\citep{ai2d}, for diagram interpretation; Q-Bench~\citep{qbench}, testing low-level visual perception and understanding; Chinese-Q-Bench (Q-Bench\textsuperscript{CN}), the Chinese version of Q-Bench; HallusionBench (HalluBench)~\citep{hallubench}, focusing on visual and knowledge illusions; SEED-Bench (SEED)~\citep{seed_bench}, evaluating performance on both images and videos (with video accuracy assessed using sampled frames from LLaVA); and MMMU~\citep{mmmu}, which tests multimodal models on large-scale multidisciplinary tasks requiring advanced subject matter knowledge and reasoning skills.

\textbf{Implementation Details.}
For LLaVA-v1.5, we utilize the pre-trained projection layer weights from LLaVA to fine-tune both the projection layer and the large language model (LLM). This fine-tuning is conducted using the 665k supervised fine-tuning data from LLaVA, supplemented by our TaskGalaxy dataset. We use two LLaVA architectures, Vicuna-13B v1.5 and Vicuna-7B v1.5~\citep{vicuna}, combined with CLIP-ViT-L-336px~\citep{clipvit} and two layers of MLP as our visual-language models (VLMs). During instruction tuning, we fine-tune the MLP layer and LLM using 8 A800 GPUs, with an initial learning rate of 2e-5, a batch size of 16 per device, for 1 epoch, totaling approximately 9300 steps.
For InternVL-Chat-v1.0, we first train the MLP layers with the LGS-558K dataset using 8 A800 GPUs, with an initial learning rate of 1e-3 and a batch size of 32 per device. Subsequently, we fine-tune the MLP layers and the LLM with both the raw fine-tuning dataset and the TaskGalaxy dataset, using the same hyperparameters as for LLaVA. We employ two InternVL-Chat-v1.0 architectures, Vicuna-7B and Vicuna-13B, along with InternViT-6B~\citep{internvl2_26b}.

\subsection{Quantitative Comparison}
\begin{table*}[!bht]
\centering
\caption{\textbf{Elucidate the advantages of incorporating our TaskGalaxy dataset into SFT phase.} We present the performance comparison on existing representative benchmarks before and after incorporating TaskGalaxy. 
The results in the Baseline represent our re-implementation of the officially provided checkpoint. All the numbers are presented in \% except MME and the full score is 100\%. The indicator of MME is the perception score, the maximum value is 2000.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccccccccccc}
\hline
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Model\end{tabular}} & \multirow{2}{*}{Method} & \multicolumn{8}{c}{Benchmarks} \\ \cline{3-11}
& & MME & MMB & MMB$^{\mathrm{CN}}$ & POPE & LLaVA$^{\mathrm{W}}$ & MMVet & TQA & SQA & MathVista\\ \hline
\multicolumn{1}{c|}{\multirow{6}{*}{LLaVA-v1.5-7B}} & \multicolumn{1}{c|}{Baseline}&1506&64.69&58.07&85.9&53.0&25.9&58.21&69.51&26.7\\ 
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline+TaskGalaxy}& \cellcolor[HTML]{EFEFEF}{\textbf{1533}} & \cellcolor[HTML]{EFEFEF}{\textbf{68.04}} & \cellcolor[HTML]{EFEFEF}{\textbf{61.17}}&\cellcolor[HTML]{EFEFEF}{\textbf{86.7}}&\cellcolor[HTML]{EFEFEF}{\textbf{56.3}}&\cellcolor[HTML]{EFEFEF}{\textbf{29.9}}&\cellcolor[HTML]{EFEFEF}{\textbf{58.98}}&\cellcolor[HTML]{EFEFEF}{\textbf{71.26}}&\cellcolor[HTML]{EFEFEF}{\textbf{31.4}} \\ \cline{3-11}
\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{} & ChartQA & AI2D & Q-Bench & Q-Bench$^{\mathrm{CN}}$ & HalluBench & SEED & MMMU & \multicolumn{2}{c}{Average (w/o MME)} \\ \cline{3-11} 
 \multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline}& 14.72& 25.32 & 26.08 & 33.58 & 50.05 & 58.59 & 16.6 & \multicolumn{2}{c}{44.46}\\ 
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline+TaskGalaxy}&\cellcolor[HTML]{EFEFEF}{\textbf{20.20}} & \cellcolor[HTML]{EFEFEF}{\textbf{38.26}}& \cellcolor[HTML]{EFEFEF}{\textbf{43.58}}&\cellcolor[HTML]{EFEFEF}{\textbf{34.85}} &\cellcolor[HTML]{EFEFEF}{\textbf{51.74}}&\cellcolor[HTML]{EFEFEF}{\textbf{60.28}}&\cellcolor[HTML]{EFEFEF}{\textbf{21.8}}&\multicolumn{2}{c}{\cellcolor[HTML]{EFEFEF}{\textbf{48.96}}}\\ \cline{3-10} \hline
 
\multicolumn{1}{c|}{\multirow{6}{*}{LLaVA-v1.5-13B}} & \multicolumn{1}{c|}{} & MME & MMB & MMB$^{\mathrm{CN}}$ & POPE & LLaVA$^{\mathrm{W}}$ & MMVet & TQA & SQA & MathVista \\ \cline{3-11}
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline} & 1532&68.47&63.49&86.03&62.6&32.2&61.25&71.60&28.1\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline+TaskGalaxy}&  \cellcolor[HTML]{EFEFEF}{\textbf{1600}} & \cellcolor[HTML]{EFEFEF}{\textbf{69.85}} & \cellcolor[HTML]{EFEFEF}{\textbf{64.43}}&\cellcolor[HTML]{EFEFEF}{\textbf{86.20}}&\cellcolor[HTML]{EFEFEF}{\textbf{63.1}}&\cellcolor[HTML]{EFEFEF}{\textbf{34.4}}&\cellcolor[HTML]{EFEFEF}{\textbf{61.95}}&\cellcolor[HTML]{EFEFEF}{\textbf{73.33}}&\cellcolor[HTML]{EFEFEF}{\textbf{33.3}}\\ \cline{3-11}
\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{} & ChartQA & AI2D & Q-Bench & Q-Bench$^{\mathrm{CN}}$ & HalluBench & SEED & MMMU & \multicolumn{2}{c}{Average (w/o MME)} \\ \cline{3-11} 
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline}& 15.56 & 21.13 & 29.03 & 20.53 & 49.84 & 60.81 & 7.5 & \multicolumn{2}{c}{45.21}\\ 
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline+TaskGalaxy}&\cellcolor[HTML]{EFEFEF}{\textbf{23.44}} & \cellcolor[HTML]{EFEFEF}{\textbf{41.19}}& \cellcolor[HTML]{EFEFEF}{\textbf{29.16}}&\cellcolor[HTML]{EFEFEF}{\textbf{23.08}} &\cellcolor[HTML]{EFEFEF}{\textbf{53.21}}&\cellcolor[HTML]{EFEFEF}{\textbf{61.22}}&\cellcolor[HTML]{EFEFEF}{\textbf{17.8}}&\multicolumn{2}{c}{\cellcolor[HTML]{EFEFEF}{\textbf{49.04}}}\\ \hline


\multicolumn{1}{c|}{\multirow{6}{*}{InternVL-Chat-v1.0-7B}} & \multicolumn{1}{c|}{} & MME & MMB & MMB$^{\mathrm{CN}}$ & POPE & LLaVA$^{\mathrm{W}}$ & MMVet & TQA & SQA & MathVista \\ \cline{3-11}
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline} & 1500&65.29&57.28&86.07&51.6&26.2&56.68&66.4&27.7\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline+TaskGalaxy}&  \cellcolor[HTML]{EFEFEF}{\textbf{1532}} & \cellcolor[HTML]{EFEFEF}{\textbf{67.10}} & \cellcolor[HTML]{EFEFEF}{\textbf{60.91}}&\cellcolor[HTML]{EFEFEF}{\textbf{86.97}}&\cellcolor[HTML]{EFEFEF}{\textbf{54.8}}&\cellcolor[HTML]{EFEFEF}{\textbf{30.6}}&\cellcolor[HTML]{EFEFEF}{\textbf{57.51}}&\cellcolor[HTML]{EFEFEF}{\textbf{70.93}}&\cellcolor[HTML]{EFEFEF}{\textbf{30.8}}\\ \cline{3-11}
\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{} & ChartQA & AI2D & Q-Bench & Q-Bench$^{\mathrm{CN}}$ & HalluBench & SEED & MMMU & \multicolumn{2}{c}{Average (w/o MME)} \\ \cline{3-11} 
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline}& 15.20 & 35.96 & 44.75 & 44.88 & 52.58 & 59.29 & 27.0 & \multicolumn{2}{c}{47.79}\\ 
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline+TaskGalaxy}&\cellcolor[HTML]{EFEFEF}{\textbf{18.24}} & \cellcolor[HTML]{EFEFEF}{\textbf{38.57}}& \cellcolor[HTML]{EFEFEF}{\textbf{48.56}}&\cellcolor[HTML]{EFEFEF}{\textbf{48.16}} &\cellcolor[HTML]{EFEFEF}{\textbf{53.37}}&\cellcolor[HTML]{EFEFEF}{\textbf{60.44}}&\cellcolor[HTML]{EFEFEF}{\textbf{34.9}}&\multicolumn{2}{c}{\cellcolor[HTML]{EFEFEF}{\textbf{50.79}}}\\ \hline
\multicolumn{1}{c|}{\multirow{6}{*}{InternVL-Chat-v1.0-13B}} & \multicolumn{1}{c|}{} & MME & MMB & MMB$^{\mathrm{CN}}$ & POPE & LLaVA$^{\mathrm{W}}$ & MMVet & TQA & SQA & MathVista \\ \cline{3-11}
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline} & 1525 & 65.64 &60.31& 86.00&51.7&28.7&56.94&70.12&28.7&\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline+TaskGalaxy}&  \cellcolor[HTML]{EFEFEF}{\textbf{1534}} & \cellcolor[HTML]{EFEFEF}{\textbf{69.50}} & \cellcolor[HTML]{EFEFEF}{\textbf{63.14}}&\cellcolor[HTML]{EFEFEF}{\textbf{86.43}}&\cellcolor[HTML]{EFEFEF}{\textbf{52.9}}&\cellcolor[HTML]{EFEFEF}{\textbf{32.0}}&\cellcolor[HTML]{EFEFEF}{\textbf{59.51}}&\cellcolor[HTML]{EFEFEF}{\textbf{72.72}}&\cellcolor[HTML]{EFEFEF}{\textbf{30.5}}\\ \cline{3-11}
\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{} & ChartQA & AI2D & Q-Bench & Q-Bench$^{\mathrm{CN}}$ & HalluBench & SEED & MMMU & \multicolumn{2}{c}{Average (w/o MME)} \\ \cline{3-11} 
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline}& 16.28 & 38.55 & 56.13 & 54.86 & 50.05 & 59.35 & 25.1 & \multicolumn{2}{c}{49.90}\\ 
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline+TaskGalaxy}&\cellcolor[HTML]{EFEFEF}{\textbf{17.04}} & \cellcolor[HTML]{EFEFEF}{\textbf{52.60}}& \cellcolor[HTML]{EFEFEF}{\textbf{58.60}}&\cellcolor[HTML]{EFEFEF}{\textbf{58.26}} &\cellcolor[HTML]{EFEFEF}{\textbf{52.90}}&\cellcolor[HTML]{EFEFEF}{\textbf{60.23}}&\cellcolor[HTML]{EFEFEF}{\textbf{36.8}}&\multicolumn{2}{c}{\cellcolor[HTML]{EFEFEF}{\textbf{53.54}}}\\
\bottomrule
\hline
\end{tabular}
}
\label{tab:main_result}
\end{table*}
\vspace{0.25cm}
Table~\ref{tab:main_result} presents a quantitative comparison of LLaVA-v1.5 and InternVL-Chat-v1.0 models trained on the original fine-tuned data versus those fine-tuned with TaskGalaxy. The new models show improvements of 4.5 and 3.83 points across all 15 benchmarks for LLaVA-v1.5-7B and 13B, respectively, excluding MME. For InternVL-Chat-v1.0, the improvements are 3.0 and 3.64 points. It is worth noting that LLaVA-v1.5-13B sees a performance increase of 68 points with TaskGalaxy on the MME Benchmark.

Taking LLaVA-v1.5-7B as an example, we observed a 3.35\% and 3.1\% improvement over the original baseline on MMBench and MMBench\_{CN}, respectively. For LLaVA-in-the-wild, we achieved a 3.3 points increase, demonstrating that the TaskGalaxy fine-tuning dataset enhances the model's performance in detailed description and complex reasoning tasks. Notably, incorporating TaskGalaxy resulted in improvements of 0.77, 1.75, 4.7, 5.48, and 12.94 points on TQA, SQA, MathVista, ChartQA, and AI2D, respectively, highlighting the dataset's broad coverage. On hallucination mitigation tasks, improvements of 1 to 2 points on POPE and HalluBench suggest that a diverse range of tasks helps address hallucination issues. Additionally, on the SEED benchmark, which includes 12 evaluation latitudes and 19k questions, there was a modest 1.7 points improvement over the model trained solely on raw fine-tuned data. In low-level image evaluation tasks, Q-Bench and Chinese-Q-Bench, models fine-tuned with TaskGalaxy showed gains of 17.5 and 1.27 points, respectively. The improvements on the challenging MMMU benchmark, where the model showed a 5-point gain, are attributed to the inclusion of diverse task types, including math and humanities. We also observed a 27-point improvement on the MME benchmark. Across the 15 benchmarks excluding MME, TaskGalaxy led to an average improvement of nearly 4 points. Similar results were observed with the LLaVA-v1.5-13B, which achieved a remarkable 68-point gain on MME following TaskGalaxy fine-tuning. These improvements were consistent across InternVL-Chat-v1.0. To further demonstrate the effectiveness of TaskGalaxy, we provide additional comparative experiments which involve fine-tuning the aforementioned models, as well as more robust models, using TaskGalaxy alone and using other similar instruction-tuning datasets separately in the Appendix. The results are consistent with the findings presented earlier.

Our results demonstrate that TaskGalaxy, with its exceptionally broad coverage of task types, significantly enhances the performance of multimodal models across a wide range of tasks, offering valuable insights for the research community regarding task type diversity. 

\subsection{Ablation Study}
\begin{figure}[!tbh]
    \centering
     \includegraphics[width=1.0\linewidth]{Pictures/ablation_study.pdf}
    \caption{\textbf{Plot showing the change in baseline performance with variations in the number of tasks and the total number of samples.} Left: The effect of varying the number of task types (ranging from 2k to 19,227) on model performance, while maintaining a constant total sample size of 100k. Right: The impact on model performance of varying the number of samples across all task types, ranging from a maximum of 5 to 55 per task type (resulting in a total sample size from 76k to 413k), while keeping the number of tasks constant at 19,227.}
    \label{fig:ablation_study}
\end{figure}
\vspace{0.15cm}
\textbf{The number of task types.} 
The primary objective of TaskGalaxy is to enhance the generalization capabilities of multi-modal models by encompassing a broad array of visual-language task types. TaskGalaxy includes a diverse set of 19,227 distinct task types. In this subsection, we examine how the number of task types affects the performance of multimodal models. We selected task types in increments of 2k, 3k, 5k, 10k, 15k, 18k, and 19,227 from TaskGalaxy, maintaining a constant total of 100k images, and conducted ablation experiments using the LLaVA-v1.5-7B model. As shown in Figure~\ref{fig:ablation_study}(left), benchmarks such as LLaVA-in-the-wild, ChartQA, AI2D, Q-Bench, and MMMU consistently improved with an increasing number of task types. The 'Average' performance across the 15 benchmarks, excluding MME, also shows a clear trend of enhancement with more task types, which is corroborated by MME performance changes in Figure~\ref{fig: ablation_study_mme}. These results highlight the critical role of task type diversity in enhancing the capabilities of modern multimodal models.


\textbf{The number of samples.} 
In addition to the impact of the number of tasks, it is well established that the amount of sample data in the instruction fine-tuning dataset also significantly affects model performance. To investigate this, we conducted ablation experiments to assess the effect of varying data volumes on model performance. As depicted in Figure~\ref{fig:ablation_study}(right), we included all task types and controlled the variation in total sample size by setting the maximum number of samples for each task type, ranging from 5 up to 55 which corresponds to the final TaskGalaxy dataset. The results show that for benchmarks such as MMVeT, ChartQA, AI2D, and Q-Bench, as well as the average performance across 15 benchmarks excluding MME, performance generally improves with the increase in sample size. However, for LLaVA-in-the-wild, performance peaks at 281k samples, suggesting that the optimal sample size may vary depending on the specific benchmark. Nonetheless, there is a clear overall trend of increasing model performance with the increase in data volume.



% Numerous studies~\citep{cot_1,cot_2,cot_3,cot_4} have demonstrated the substantial impact of Chain-of-Thought (CoT) prompting on enhancing the performance of MLLMs. In this section, to further verify whether Chain-of-Thought (CoT) prompting enhances performance when task types are increased, we constrained the maximum number of samples for each task type to 5, resulting in a total sample size of 76k (as indicated by max\_5 in the Table~\ref{tab:cot_result}). We then compared the performance using the Q\&A data from the original TaskGalaxy and by generating CoT answers with GPT-4o, designed through specific prompts which can be found in Appendix (as indicated by max\_5 (CoT) in the Table~\ref{tab:cot_result}). As shown in Table~\ref{tab:cot_result}, the Chain-of-Thought (CoT) versions of the TaskGalaxy (max\_5) for MME, LLaVA-in-the-wild, and Q-Bench exhibit significant improvements over the original common responses. Additionally, the average performance across the 15 benchmarks, excluding MME, increased by approximately 1.3 points compared to the common responses. These results underscore the importance and exploratory value of incorporating CoT in multi-modal models.



\section{Related Work}\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{Pictures/mme_ablation.pdf}
    \caption{\textbf{Plot illustrating the variation in MME Perception performance scores in relation to the number of task types and the total number of samples.} The left bar represents performance across different numbers of task types, while the right bar represents performance across varying total sample sizes.}
    \label{fig: ablation_study_mme}
\end{wrapfigure}
\textbf{Large Multi-modal Models.} 
With the rapid advancement of large language models (LLMs), such as GPT-3~\citep{gpt3}, LLama2~\citep{llama2}, InternLM~\citep{internlm}, and Baichuan 2~\citep{baichuan2}, there has been a growing focus on integrating visual knowledge into LLMs, exemplified by models like CLIP~\citep{clipvit}, BLIP~\citep{blip}, and BLIP2~\citep{blip2}. While these models exhibit strong performance in graphic alignment and image captioning, they continue to face significant challenges in handling more complex visual question answering tasks.
To enhance the model's instruction adherence and content understanding in visual question answering (VQA), visual instruction fine-tuning strategies have garnered increasing attention in the training of large multi-modal models. For instance, models like LLaVA~\citep{llava}, MiniGPT-4~\citep{minigpt}, and InstructBLIP~\citep{instructblip} leverage large language models from the GPT4~\citep{gpt4} family to generate fine-tuned instruction data, thereby enhancing performance in complex VQA scenarios. Furthermore, to expand the range of VQA task scenarios, recent models such as LAMM~\citep{lamm} and MIMIC-IT~\citep{mimic}, following the example of LLaVA, have extended their VQA capabilities to encompass 3D scenarios, multi-graph tasks, videos, and other complex domains. Recently, a series of open-source large multi-modal models  with enhanced performance have consistently outperformed existing benchmarks. Notable examples include GLM-4v~\citep{glm4v}, Qwen-VL~\citep{qwenvl}, InternLM-XComposer-2.5~\citep{internlm-xcomposer-2.5}, and InternLM2~\citep{internlm2_5}, which are leading the field in various multimodal tasks. In addition to open-source models, recently developed closed-source models such as GPT-4v~\citep{gpt4v}, GPT-4o~\citep{hurst2024gpt}, and Claude-3.5~\citep{claude3.5} continue to lead the field, often matching or surpassing open-source models in various VQA tasks. To facilitate comprehensive improvements in the performance of open-source models across a wide range of Visual Question Answering (VQA) tasks, we construct fine-tuning datasets featuring diverse task types based on multiple LMMs, while also leveraging closed-source models. This approach aims to enhance the performance of open-source models in various tasks.

\textbf{Multi-modal Instruction-Tuning Datasets.} 
Data-driven instruction fine-tuning strategies have become increasingly crucial in the training of multimodal models. Recent works have introduced high-quality instruction fine-tuning datasets designed to enhance models' visual question-answering capabilities. Among these datasets, MultiInstruct~\citep{multiinstruct} is the first manually labeled multimodal instruction tuning benchmark dataset, encompassing 62 different multimodal tasks. Mini-GPT4~\citep{minigpt} constructs an instruction-following dataset by combining image-text datasets with manually crafted instruction templates. LLaVA~\citep{llava} utilized the captions from the COCO dataset and the contents of the bounding boxes, sending them to GPT-4 to construct an instruction fine-tuning dataset comprising approximately 150k samples. Similar to LLaVA, LAMM~\citep{lamm} used the GPT API to generate command-response pairs for collected images and point clouds, resulting in approximately 190k graphic command-response pairs and 10k point cloud command-response pairs. However, these instruction fine-tuning datasets do not emphasize the concept of task types specific to VQA scenarios and lack diversity in the range of task types. Considering the concept of task types, VisionLLM v2~\citep{visionllmv2} aggregated hundreds of task types of multimodal data based on existing academic datasets but it requires the design of specific decoders for different tasks, which limits the generalizability of the dataset. Recent work VFLAN~\citep{vision_flan} enabled experts to construct 187 task types based on existing datasets, resulting in a large fine-tuning instruction dataset containing approximately 1,664k samples. However, this approach requires significant specialized manpower to annotate the extended tasks and generate the associated task structures, making it both time-consuming and labor-intensive. Additionally, despite the effort, the dataset covers only around 200 task types. In contrast, we developed TaskGalaxy, a high-quality instruction fine-tuning dataset, guided by the principles of maximizing the coverage of hierarchical task types for VQA while minimizing manpower investment. We successfully generated around 20,000 hierarchical task types and 410k VQA samples. Integrating TaskGalaxy into multimodal architectures like LLaVA and InternVL-Chat resulted in substantial performance improvements.

\vspace{-0.2cm}
\section{Conclusion}
In this study, we present TaskGalaxy, a multi-modal instruction fine-tuning dataset comprising approximately 20,000 multi-modal task types and around 410k instruction Q\&A samples. Additionally, we propose a pipeline for the systematic construction and generation of a diverse range of task types and corresponding high quality instruction Q\&A samples. This approach addresses the limitations of existing multi-modal instruction fine-tuning datasets, particularly the narrow scope of task types and the excessive reliance on human intervention. TaskGalaxy encompasses an extensive range of multimodal visual Q\&A tasks and offers a highly extensible pipeline that facilitates the addition of new task types and the generation of high-quality fine-tuned instructional data. We fine-tuned the LLaVA-v1.5 and InternVL-Chat-v1.0 models using TaskGalaxy, resulting in a significant improvement compared to using only raw fine-tunning data, respectively. Adequate empirical evaluation confirms the effectiveness of our broader task type data in enhancing the performance of multimodal models, highlighting the critical importance of task type diversity in the instruction fine-tuning dataset. We hope that our approach of constructing a dataset with a broad range of task types and reduced manual labor will guide future development of multi-modal instruction fine-tuning datasets and we plan to make the dataset publicly available for community research.

\vspace{-0.2cm}
\section{Ethics Statement}
This study upholds rigorous ethical standards to ensure the credibility and confidentiality of the findings. All data underwent thorough de-identification procedures to protect privacy and maintain anonymity. The study followed ethical guidelines and obtained informed consent from participants while prioritizing their rights and autonomy. Transparency and accountability were maintained throughout the research process to minimize biases and conflicts of interest. No academic ethical issues or misconduct were encountered, and the authors affirm their unwavering commitment to upholding ethical research practices and promptly addressing any unintentional errors or oversights.
% If your paper is ultimately accepted, the statement {\tt
%   {\textbackslash}iclrfinalcopy} should be inserted to adjust the
% format to the camera ready requirements.

% The format for the submissions is a variant of the NeurIPS format.
% Please read carefully the instructions below, and follow them
% faithfully.

% \subsection{Style}

% Papers to be submitted to ICLR 2024 must be prepared according to the
% instructions presented here.

% %% Please note that we have introduced automatic line number generation
% %% into the style file for \LaTeXe. This is to help reviewers
% %% refer to specific lines of the paper when they make their comments. Please do
% %% NOT refer to these line numbers in your paper as they will be removed from the
% %% style file for the final version of accepted papers.

% Authors are required to use the ICLR \LaTeX{} style files obtainable at the
% ICLR website. Please make sure you use the current files and
% not previous versions. Tweaking the style files may be grounds for rejection.

% \subsection{Retrieval of style files}

% The style files for ICLR and other conference information are available online at:
% \begin{center}
%    \url{http://www.iclr.cc/}
% \end{center}
% The file \verb+iclr2024_conference.pdf+ contains these
% instructions and illustrates the
% various formatting requirements your ICLR paper must satisfy.
% Submissions must be made using \LaTeX{} and the style files
% \verb+iclr2024_conference.sty+ and \verb+iclr2024_conference.bst+ (to be used with \LaTeX{}2e). The file
% \verb+iclr2024_conference.tex+ may be used as a ``shell'' for writing your paper. All you
% have to do is replace the author, title, abstract, and text of the paper with
% your own.

% The formatting instructions contained in these style files are summarized in
% sections ~\ref{gen_inst}, ~\ref{headings}, and ~\ref{others} below.

% \section{General formatting instructions}
% \label{gen_inst}

% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
% Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
% preferred typeface throughout. Paragraphs are separated by 1/2~line space,
% with no indentation.

% Paper title is 17~point, in small caps and left-aligned.
% All pages should start at 1~inch (6~picas) from the top of the page.

% Authors' names are
% set in boldface, and each name is placed above its corresponding
% address. The lead author's name is to be listed first, and
% the co-authors' names are set to follow. Authors sharing the
% same address can be on the same line.

% Please pay special attention to the instructions in section ~\ref{others}
% regarding figures, tables, acknowledgments, and references.


% There will be a strict upper limit of 9 pages for the main text of the initial submission, with unlimited additional pages for citations. 

% \section{Headings: first level}
% \label{headings}

% First level headings are in small caps,
% flush left and in point size 12. One line space before the first level
% heading and 1/2~line space after the first level heading.

% \subsection{Headings: second level}

% Second level headings are in small caps,
% flush left and in point size 10. One line space before the second level
% heading and 1/2~line space after the second level heading.

% \subsubsection{Headings: third level}

% Third level headings are in small caps,
% flush left and in point size 10. One line space before the third level
% heading and 1/2~line space after the third level heading.

% \section{Citations, figures, tables, references}
% \label{others}

% These instructions apply to everyone, regardless of the formatter being used.

% \subsection{Citations within the text}

% Citations within the text should be based on the \texttt{natbib} package
% and include the authors' last names and year (with the ``et~al.'' construct
% for more than two authors). When the authors or the publication are
% included in the sentence, the citation should not be in parenthesis using \verb|\citep{}| (as
% in ``See \citep{Hinton06} for more information.''). Otherwise, the citation
% should be in parenthesis using \verb|~\citep{}| (as in ``Deep learning shows promise to make progress
% towards AI~~\citep{Bengio+chapter2007}.'').

% The corresponding references are to be listed in alphabetical order of
% authors, in the \textsc{References} section. As to the format of the
% references themselves, any style is acceptable as long as it is used
% consistently.

% \subsection{Footnotes}

% Indicate footnotes with a number\footnote{Sample of the first footnote} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches
% (12~picas).\footnote{Sample of the second footnote}

% \subsection{Figures}

% All artwork must be neat, clean, and legible. Lines should be dark
% enough for purposes of reproduction; art work should not be
% hand-drawn. The figure number and caption always appear after the
% figure. Place one line space before the figure caption, and one line
% space after the figure. The figure caption is lower case (except for
% first word and proper nouns); figures are numbered consecutively.

% Make sure the figure caption does not get separated from the figure.
% Leave sufficient space to avoid splitting the figure and figure caption.

% You may use color figures.
% However, it is best for the
% figure captions and the paper body to make sense if the paper is printed
% either in black/white or in color.
% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Sample figure caption.}
% \end{figure}

% \subsection{Tables}

% All tables must be centered, neat, clean and legible. Do not use hand-drawn
% tables. The table number and title always appear before the table. See
% Table~~\ref{sample-table}.

% Place one line space before the table title, one line space after the table
% title, and one line space after the table. The table title must be lower case
% (except for first word and proper nouns); tables are numbered consecutively.

% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

% \section{Default Notation}

% In an attempt to encourage standardized notation, we have included the
% notation file from the textbook, \textit{Deep Learning}
% ~\citep{goodfellow2016deep} available at
% \url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
% is not required and can be disabled by commenting out
% \texttt{math\_commands.tex}.


% \centerline{\bf Numbers and Arrays}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1in}p{3.25in}}
% $\displaystyle a$ & A scalar (integer or real)\\
% $\displaystyle \va$ & A vector\\
% $\displaystyle \mA$ & A matrix\\
% $\displaystyle \tA$ & A tensor\\
% $\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
% $\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
% $\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
% $\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
% $\displaystyle \ra$ & A scalar random variable\\
% $\displaystyle \rva$ & A vector-valued random variable\\
% $\displaystyle \rmA$ & A matrix-valued random variable\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Sets and Graphs}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \sA$ & A set\\
% $\displaystyle \R$ & The set of real numbers \\
% $\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
% $\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
% $\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
% $\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
% $\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
% $\displaystyle \gG$ & A graph\\
% $\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
% \end{tabular}
% \vspace{0.25cm}


% \centerline{\bf Indexing}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
% $\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
% $\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
% $\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
% $\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
% $\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
% $\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
% $\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}


% \centerline{\bf Calculus}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% % NOTE: the [2ex] on the next line adds extra height to that row of the table.
% % Without that command, the fraction on the first line is too tall and collides
% % with the fraction on the second line.
% $\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
% $\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
% $\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
% $\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
% $\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
% $\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
% $\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
% $\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
% $\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Probability and Information Theory}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
% $\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
% a variable whose type has not been specified\\
% $\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
% $\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
% $\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
% $\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
% $\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
% $\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
% $\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
% over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Functions}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
% $\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
%   $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
%   (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
% $\displaystyle \log x$ & Natural logarithm of $x$ \\
% $\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
% $\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
% $\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
% $\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
% $\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
% $\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}



% \section{Final instructions}
% Do not change any aspects of the formatting parameters in the style files.
% In particular, do not modify the width or length of the rectangle the text
% should fit into, and do not change font sizes (except perhaps in the
% \textsc{References} section; see below). Please note that pages should be
% numbered.

% \section{Preparing PostScript or PDF files}

% Please prepare PostScript or PDF files with paper size ``US Letter'', and
% not, for example, ``A4''. The -t
% letter option on dvips will produce US Letter files.

% Consider directly generating PDF files using \verb+pdflatex+
% (especially if you are a MiKTeX user).
% PDF figures must be substituted for EPS figures, however.

% Otherwise, please generate your PostScript and PDF files with the following commands:
% \begin{verbatim}
% dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
% ps2pdf mypaper.ps mypaper.pdf
% \end{verbatim}

% \subsection{Margins in LaTeX}

% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+
% from the graphicx package. Always specify the figure width as a multiple of
% the line width as in the example below using .eps graphics
% \begin{verbatim}
%    \usepackage[dvips]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.eps}
% \end{verbatim}
% or % Apr 2009 addition
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% for .pdf graphics.
% See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

% A number of width problems arise when LaTeX cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix

\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{table}{0}


\renewcommand\thesection{\Alph{section}}
\renewcommand {\thefigure} {A-\arabic{figure}}
\renewcommand {\thetable} {A-\arabic{table}}
\section{Appendix}
\subsection{Overall review of image sources}
Considering the accessibility of data sources and the task-related nature of the image data we aim to mine, we have opted for open-source image data. The approximate data sources and their corresponding sample sizes are presented in Table 1 of the main text. To provide further insight into the image data, the Table~\ref{tab:imagesource} presents the statistics of the sample sizes for the different data sources collected.
% \begin{table}[!tbh]
% \centering
% \caption{Statistics of the sample sizes for the different data sources collected.}
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{ccccccccc}
% \hline
% \multirow{1}{*}{\begin{tabular}[c]{@{}c@{}} \textbf{Dataset}\end{tabular}} & UniGeo & GEOS & CLEVR-Math& VAQ2.0 & DocVQA & Geometry3K & VQA-AS & VQA-RAD\\ \hline
% \multicolumn{1}{c}{\multirow{1}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Sample size}\end{tabular}}} & 1507 & 64 & 590 & 2327 & 2672 & 1182 & 658 & 242 &  \hline

% \multirow{1}{*}{\begin{tabular}[c]{@{}c@{}} \textbf{Dataset}\end{tabular}} & GeoQA+ & VizWiz & PlotQA& TQA & FigureQA & MapQA & A-OKVQA & PMC-VQA\\ \hline
% \multicolumn{1}{c}{\multirow{1}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Sample size}\end{tabular}}} & 2146 & 749 & 612 & 1137 & 1970 & 590 & 3391 & 4208 &  \hline

% \multirow{1}{*}{\begin{tabular}[c]{@{}c@{}} \textbf{Dataset}\end{tabular}} & TabMWP & Super-CLEVR & IconQA& DVQA & allava\_laion & allava\_vflan & Visual Genome & wikiart\_images\\ \hline
% \multicolumn{1}{c}{\multirow{1}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Sample size}\end{tabular}}} & 2538 & 971 & 2560 & 1660 & 145359 & 181393 & 108249 & 62 &  \hline

% \multirow{1}{*}{\begin{tabular}[c]{@{}c@{}} \textbf{Dataset}\end{tabular}} & \multicolumn{2}{c}{ocr\_vqa\_images} & \multicolumn{2}{c}{coco\_train2017} & \multicolumn{2}{c}{web-celeberity\_images} & \multicolumn{2}{c}{web-landmark\_images} \\ \hline
% \multicolumn{1}{c}{\multirow{1}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Sample size}\end{tabular}}} & \multicolumn{2}{c}{12614} & \multicolumn{2}{c}{118326} & \multicolumn{2}{c}{67} & \multicolumn{2}{c}{57}\\ \hline

% \multirow{1}{*}{\begin{tabular}[c]{@{}c@{}} \textbf{Dataset}\end{tabular}} & \multicolumn{3}{c}{text\_vqa\_train\_images} & \multicolumn{3}{c}{sam\_images} & \multicolumn{2}{c}{share\_textvqa\_images} \\ \hline
% \multicolumn{1}{c}{\multirow{1}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Sample size}\end{tabular}}} & \multicolumn{3}{c}{3485} & \multicolumn{3}{c}{223720} & \multicolumn{2}{c}{55}\\ \hline

% \end{tabular}%
% }
% \label{tab:imagesource}
% \end{table}

\begin{table*}[h]
\centering
\small
\caption{Statistics of the sample sizes for the different data sources collected.}
\resizebox{0.85\textwidth}{!}{%
\begin{tabular}{l r | l r}
\toprule
\textbf{Dataset} & \textbf{Sample Size} & \textbf{Dataset} & \textbf{Sample Size} \\
\midrule
UniGeo & 1,507 & GeoQA+ & 2,146 \\
GEOS & 64 & VizWiz & 749 \\
CLEVR-Math & 590 & PlotQA & 612 \\
VAQ2.0 & 2,327 & TQA & 1,137 \\
DocVQA & 2,672 & FigureQA & 1,970 \\
Geometry3K & 1,182 & MapQA & 590 \\
VQA-AS & 658 & A-OKVQA & 3,391 \\
VQA-RAD & 242 & PMC-VQA & 4,208 \\
TabMWP & 2,538 & Super-CLEVR & 971 \\
IconQA & 2,560 & DVQA & 1,660 \\
allava\_laion & 145,359 & allava\_vflan & 181,393 \\
Visual Genome & 108,249 & wikiart\_images & 62 \\
ocr\_vqa\_images & 12,614 & coco\_train2017 & 118,326 \\
web-celebrity\_images & 67 & web-landmark\_images & 57 \\
text\_vqa\_train\_images & 3,485 & sam\_images & 223,720 \\
share\_textvqa\_images & 55 & & \\
\bottomrule
\end{tabular}%
}
\label{tab:imagesource}
\end{table*}


\subsection{More Details on the Generation Pipeline Process of TaskGalaxy}
\textbf{About Prompt:} In the TaskGalaxy dataset pipeline, the first step involves using GPT-4o to continuously expand new task types based on a set of human-defined task type seeds. This process requires designing distinct prompts for different levels of task types and determining whether lower-level task types exist within each hierarchy, allowing GPT-4o to systematically expand and populate the dataset. For level-1 task types, we focus solely on extending the existing prompt, as detailed in the main text. For generating two-level task types, the approach varies depending on whether the one-leveltask types have corresponding two-level tasks; prompts are designed accordingly to either continue expansion or generate new prompts. The same methodology applies to three-leval tasks. The detailed prompts are provided in Table~\ref{table:prompt}. 

After generating a large number of hierarchical task types and collecting a substantial amount of open-source image data, the third step in our pipeline involves matching and filtering. Following the image-text cosine similarity matching conducted by CLIP, we proceed to further refine the selection of task types that are most compatible with specific images. In this stage, we employ a specially designed prompt for GPT-4o, denoted as p\_filter in Table~\ref{table:prompt_second}, to filter and select the task types that best match each particular image.

After completing the matching and filtering of task types and images in the third stage, the fourth stage involves generating question-answer pairs related to all task types matched with the images. The prompt templates guiding GPT-4o to generate these task-type-related answer texts are denoted as p\_Q\&A in Table~\ref{table:prompt_second}.

After generating all the Q\&A samples related to the task types, to further refine the selection of task types and questions that best match the images and ensure higher quality, we employ three open-source models in the final stage. This step focuses on filtering the images, task types, and their corresponding questions to identify the most suitable and coherent matches, while also considering cost-effectiveness. The prompt templates used for this filtering process by the three open-source models are listed as p\_openfilter in Table~\ref{table:prompt_second}.

In addition to the above prompt templates, in the ablation experiments, we aimed to verify the effectiveness of Chain-of-Thought (CoT) under multi-task type conditions. For a dataset with a total size of 76k, we prompted GPT-4o to articulate the reasoning process, guiding it to generate CoT answers. The prompt templates used for this purpose are denoted as p\_CoT in Table~\ref{table:prompt_second}.
\begin{table}[h]
\caption{Different prompt templates were generated for the layered task types in the first stage. Here, p2\_w denotes the continuation of expansion for a one-level task type that includes a two-level task, while p2\_w/o represents the direct generation of a two-level task for a one-level task type without an existing two-level task. Similarly, p3\_w indicates the continuation of expansion for a two-level task that includes a three-level task, and p3\_w/o represents the direct generation of a three-level task for a two-level task type without an existing three-level task. The placeholder \{...\} denotes the corresponding task type field to be populated. }
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|p{1\textwidth}|}
\hline
\multicolumn{1}{|c|}{Type} & \multicolumn{1}{|c|}{Prompt} \\ \hline
\multirow{10}{*}{\centering{p2\_w}} & You are an expert in multimodal content understanding with extensive experience in this field. I want to construct a comprehensive task label system related to multimodal content understanding, which only includes image and text modalities. In this system, the input is an image and the corresponding text, and the output is in the form of text. The primary and secondary categories are connected by '$\sim$'. The task name of the primary category that needs to be detailed currently is \{....\}, and multiple secondary categories have already been established manually for this primary category, which are: \{...\}. Please supplement other categories that do not belong to the aforementioned secondary categories to cover all task scenarios under the primary category of multi-modal content understanding. Output format: Each line corresponds to one task category, without any other characters, and different levels of task categories are connected by '$\sim$'.\\ \hline
\multirow{9}{*}{\centering{p2\_w/o}}& You are an expert in multimodal content understanding with extensive experience in this field. I want to construct a comprehensive task label system related to multimodal content understanding, which only includes image and text modalities. In this system, the input is an image and the corresponding text, and the output is in the form of text. The primary and secondary categories are connected by '$\sim$'. The task name of the primary category that needs to be detailed currently is \{...\}. Please expand the secondary task categories under this primary task category to cover all tasks included in this primary task category in the context of multimodal content understanding. Output format: Each line corresponds to one task category, without any other characters, and different levels of task categories are connected by '$\sim$'.\\ \hline
\multirow{10}{*}{\centering{p3\_w}}& You are an expert in multimodal content understanding with extensive experience in this field. I want to construct a comprehensive task taxonomy for multimodal content understanding that includes only two modalities: images and text. The input to this taxonomy will be images and their corresponding text, and the output will be in text form. The primary, secondary, and tertiary categories in this taxonomy are connected by '$\sim$'. Currently, the task name for the secondary category that needs to be detailed is \{...\}, and several tertiary categories have already been established manually for this secondary category, which are: \{...\}. Please supplement additional categories that do not belong to the aforementioned tertiary categories to cover all tasks under the secondary category in the context of multimodal content understanding. Output format: Each line should correspond to one task category, without any other characters. Different levels of task categories should be connected by '$\sim$'. \\\hline
\multirow{9}{*}{\centering{p3\_w/o}}& You are an expert in multimodal content understanding with extensive experience in this field. I want to construct a comprehensive task taxonomy for multimodal content understanding that includes only two modalities: images and text. The input to this taxonomy will be images and their corresponding text, and the output will be in text form. The primary, secondary, and tertiary categories in this taxonomy are connected by '$\sim$'. Currently, the task name for the secondary category that needs to be detailed is \{...\}. Please expand the tertiary task categories under this secondary task category to cover all tasks included in this secondary task category in the context of multimodal content understanding. Output format: Each line should correspond to one task category, without any other characters. Different levels of task categories should be connected by '$\sim$'.\\ \hline
\end{tabular}
}

\label{table:prompt}
\end{table}


\begin{table*}[h]
\caption{The prompt templates used throughout the other stages of the TaskGalaxy pipeline: further filtered in the third stage based on image and task type compatibility, employed in the fourth stage to generate question-answer pairs, and finally refined in the fifth stage through multi-referee scoring to ensure optimal matching and high-quality alignment between task types, image content, and question-answer pairs, and the prompt template for Chain-of-Thought (CoT) answers generation in the ablation study.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|p{1\textwidth}|}
\hline
\multicolumn{1}{|c|}{Type} & \multicolumn{1}{|c|}{Prompt} \\ \hline
\multirow{8}{*}{\centering{p\_filter}} & You are a multimodal content understanding expert. Given an image and multiple task labels related to multimodal content understanding, with task labels as: {init\_task\_type}. I would like to generate some question-answer pairs related to these task labels based on the image content. However, some task labels might not be relevant to the image content and thus cannot generate relevant questions. Please fully understand the image content and the meanings of the task labels, and select all the task labels that are appropriate for this task. Ensure the task labels are the same as the original. The output format should be: [task labels], without any other characters. If there are no matches, output [None].
\\ \hline
\multirow{7}{*}{\centering{p\_Q\&A}} & You are a multi-modal content understanding expert, very skilled in handling visual question answering tasks. Your task is: Given an image and task labels that are highly relevant to the content of this image, the task labels are: \{...\}. Please fully understand the content of the image and, for each task label, propose a question and answer pair related to the image content. Please try to propose some complex questions and provide answers to these questions. Please strictly follow the JSON format for the output. Each line should represent a question and answer pair corresponding to a task label in the following JSON format: \{"task\_type": "question": "answer":\}.
\\ \hline
\multirow{5}{*}{\centering{p\_filter\_
referee}} & You are an expert in multimodal content understanding, particularly skilled in handling visual question answering tasks. Your task is: Given an image and a multimodal content understanding-related task label, along with a question related to that task label, the task label is "\{task\_type\}", the question is "\{question\}", please fully understand the image content, the task label, and the question, and determine whether the task label and question are suitable for the image. If suitable, score it as 1; otherwise, score it as 0. Please only output your final score without any other characters.
\\ \hline
\multirow{5}{*}{\centering{p\_CoT}} & You are a multimodal content understanding expert and you are very good at solving visual question answering tasks, I will give you an image and a question related to this image, the question is: \{question\}, your task is to fully understand the content of the image along with the question, please fully think about how the question should be answered, please give a step by step thought process of how you solved the question and finally output the answer to the question.
\\ \hline

\end{tabular}
}
\label{table:prompt_second}
\end{table*}
% \vspace{-0.4cm}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{Pictures/task_image.pdf}
    \caption{Examples of images along with their corresponding task list pairs filtered by GPT-4o.}
    \label{fig:image_task_type}
\end{figure}
\textbf{About task types corresponding to the image:} In the third step of the data generation pipeline, the top 10 task types with the highest CLIP's image-text cosine similarity are assigned to all images. These task types are then further refined using GPT-4o to identify multiple task types corresponding to each image, which are subsequently used to generate question-answer pairs. Figure~\ref{fig:image_task_type} illustrates the list of images along with their associated task types.


\subsection{Comparision with Existing Datasets}
Table~\ref{tab:dataset_comparison} presents a comparision between existing visual instruction tunning datasets and TaskGalaxy. For existing visual instruction tunning datasets, we directly adopt the numbers of tasks and instances reported in their original papers. VL-Qwen~\cite{qwenvl} is a newly introduced large-scale dataset with human annotations; however, it is not publicly accessible. In contrast, MultiInstruct~\cite{multiinstruct} is constructed from publicly available datasets but primarily emphasizes visual grounding tasks, containing only 29 tasks that exclude region-specific information. To further expand the number of task types, the recent Vision-Flan constructed a dataset comprising 196 task types through expert collection and curation. Although this approach increases the number of task types compared to previous datasets, it heavily relies on expert involvement, making the creation process highly time-consuming, labor-intensive, and significantly limiting its scalability. In contrast, TaskGalaxy employs an almost fully automated process to construct a high-quality instruction-tuning dataset encompassing nearly 20,000 task types—hundreds of times more than previous efforts.

\begin{table}[!thb]
    \caption{Comparison between TaskGalaxy and existing visual instruction tuning datasets.}
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Dataset} & \textbf{Instances} & \textbf{\# Tasks} & \textbf{\# Source} \\
        \midrule
        LLaVA~\cite{llava} & 150K & 3 & Synthetic \\
        LAMM~\cite{lamm} & 196K & 8 & Synthetic \\
        VL-Qwen~\cite{qwenvl} & 350K & Unknown & Synthetic \\
        M3IT~\cite{m3it} & 2.4M & 40 & Private \\
        mPlug-Owl~\cite{mplug} & 150K & 3 & Synthetic \\
        Shikra~\cite{shikra} & 156K & 4 & Synthetic \\
        SVIT~\cite{svit} & 4.2M & 4 & Synthetic \\
        MultiInstruct~\cite{multiinstruct} & 510K & 62 & Synthetic \\
        VISION-FLAN~\cite{vision_flan} & 1.6M & 196 & Public \\
        TaskGalaxy (Ours) & 431K & 19227 & Public+Synthetic \\
        \bottomrule
    \end{tabular}
    \label{tab:dataset_comparison}
\end{table}
\subsection{Illustrations and Analysis of Samples Filtered During the Data Generation Pipeline}
In TaskGalaxy generation pipeline, there are two parts involved in matching and screening, and in the first part, we employ a two-phase process for matching and screening task types and images. In the first phase, task types are generated and open-source image data is collected. We use CLIP to perform an initial screening, matching task types with images based on their textual and visual similarity. However, the performance of CLIP's image-text matching is inherently limited. This sometimes leads to overestimation of similarity scores, resulting in mismatches where task types are paired with images that do not accurately represent their content. To address this limitation, the second phase involves leveraging GPT-4o with carefully designed prompts to refine the matches. This step effectively filters out task types that are not contextually related to the content of the images. Below, we provide a comparison illustrating the image-task pairings before and after the second-phase refinement by GPT-4o, demonstrating the improvement in alignment between task types and image content.

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{pictures_filter/filter_fig1.jpg}
    \caption{Example 1 of Filtered-Out Samples.}
\label{fig:filter_1}
\end{figure}
In Figure~\ref{fig:filter_1} depicting the bathroom of this hotel, the initial CLIP matching may generate terms like "public facilities", "restaurants", "smart home", "workplaces", and "museums" which are not well-aligned with the actual image content. However, after applying corrective filtering with GPT-4o, the results are more compatible with the image content. This filtering refines the matches to include task types such as detailed descriptions of the room, hygiene-related information, tourist accommodation, and accommodation reviews, all of which align well with our expectations.


In Figure~\ref{fig:filter_2} featuring the iconic building, the initial CLIP matching generated numerous task types related to communities, which were clearly not aligned with the content of the image. However, after GPT-4o filtering, only relevant task types were retained, such as those related to landmarks and weather background, ensuring a better match with the image content.
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{pictures_filter/filter_fig2.jpg}
    \caption{Example 2 of Filtered-Out Samples.}
\label{fig:filter_2}
\end{figure}


In Figure~\ref{fig:filter_3} showing the baseball sports, the initial CLIP matching generated numerous task types related to social media analysis. This may be influenced by the coexistence of social media content and sports within CLIP's training data. However, the images are more specifically aligned with the theme of sports. After GPT-4o filtering, the retained task types focus on sports recognition, player analysis, and other topics more relevant to the image content, providing a better contextual match.
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{pictures_filter/filter_fig3.jpg}
    \caption{Example 3 of Filtered-Out Samples.}
\label{fig:filter_3}
\end{figure}

In Figure~\ref{fig:filter_4} showcasing restaurant food, the initial CLIP matching assigned task types such as restaurant OCR and related information. This could be because restaurant food is often associated with menus in the training data. However, after GPT-4o filtering, only food-related tasks, such as food recognition and other more relevant task types, were retained, ensuring a closer alignment with the image content.
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{pictures_filter/filter_fig4.jpg}
    \caption{Example 4 of Filtered-Out Samples.}
\label{fig:filter_4}
\end{figure}

\textbf{Analysis:}In the third part of the data generation pipeline, we utilize CLIP to initially match task types for each image. The primary purpose of using CLIP is to identify and screen the ten images most similar to the content of a given image. Additionally, this step helps avoid excessively long text prompts, which could exceed the input token limit during the subsequent GPT-4o screening stage.
However, the effectiveness of the initial matching is influenced by the training data of CLIP, which can lead to the generation of illusory task types. For instance, during CLIP's training, food images may often co-occur with menus, sports images with media reports, natural scenery with tourism-related content, and pet images with pet products. These associations can cause CLIP to inaccurately match task types that are not directly relevant to the actual content of the image, resulting in imperfect matching in this first stage. 
To address these issues, the second stage involves GPT-4o, which refines the candidate task type list based on the actual content of the image. As demonstrated in the examples above, the filtered results effectively retain task types strongly aligned with the image content. This two-stage process ensures a higher degree of relevance and accuracy, achieving the intended purpose of the pipeline.

In the final step of the data generation pipeline, task-type-related question-answer pairs generated by GPT-4o are evaluated and screened using three open-source multimodal models. Samples are rejected in this step if at least two of the three models determine that the task type, questions, and answers are not well-aligned. The primary objective of this process is to eliminate low-quality samples that may arise from GPT-4o's hallucinations or incorrect responses. Below, we present several examples of samples that received a total score of less than 2 during the final scoring by the open-source models. These examples highlight cases where the task type, question, and answer alignment failed to meet quality expectations.
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{pictures_filter/filter_fig5.jpg}
    \caption{Example 5 of Filtered-Out Samples.}
\label{fig:filter_5}
\end{figure}

For the geometric content-related questions in Figure~\ref{fig:filter_5}, the task type is categorized as complex mathematical operations. However, based on the task type and the picture content, GPT-4o generates unreasonable values for the lengths of OA, OB, and OC in the question. Furthermore, the calculated perimeter in the answer is incorrect. As a result, the MLLM (Multimodal Language Model) both formulates and answers the question incorrectly. Open-source models uniformly scored these samples as 0 points, leading to their exclusion.

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{pictures_filter/filter_fig6.jpg}
    \caption{Example 6 of Filtered-Out Samples.}
\label{fig:filter_6}
\end{figure}
The Figure~\ref{fig:filter_6} features an artistically designed virtual vehicle, not a specific real-world brand. The response incorrectly identifies the vehicle as a Ferrari F8 Tributo. Additionally, there is no direct information about multimodal fusion embedded within the image. This aspect of the response is entirely speculative and fictional, likely influenced by the task type “data fusion,” which has led to associating unrelated information with the image. The response attempts to connect keywords from the task type (e.g., multimodal data fusion) to content that is absent in the image. As a result, it diverges from the core task of identifying the vehicle's make and model. Instead, it focuses on explaining the concept of “multimodal data fusion,” producing an answer that is misaligned with the task requirements. Due to this disconnection, the response was filtered out by the open-source model.

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{pictures_filter/filter_fig7.jpg}
    \caption{Example 7 of Filtered-Out Samples.}
\label{fig:filter_7}
\end{figure}
In Figure~\ref{fig:filter_7}, the answer states that “The cat is predominantly gray with some white markings,” but based on the content of the image, the animal in the picture is actually a dog sitting by the window, not a cat. This represents a classic case of hallucination, where the model incorrectly categorizes the animal as a cat. Additionally, the answer describes the animal as “mainly gray,” whereas the image clearly shows the dog as predominantly dark brown or black, not gray. The task was to identify the 'predominant color' of the animal, but the mention of 'white markings' in the answer is not supported by the image content. This indicates that the model likely introduced assumptions or biases unrelated to the image itself when generating the answer, resulting in a response that does not align with the factual visual content. The open-source model received a score of 0 points due to its failure to correctly identify the animal species (mistakenly identifying a dog as a cat), its incorrect description of the animal's predominant color, and its generation of an answer based entirely on hallucinations unrelated to the factual content of the image.

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{pictures_filter/filter_fig8.jpg}
    \caption{Example 8 of Filtered-Out Samples.}
\label{fig:filter_8}
\end{figure}
In Figure~\ref{fig:filter_8}, the answer avoids addressing the core of the question, which concerns “the former owners of the painting and whether it is part of a prestigious collection.” Instead, it merely states that detailed research into art records and auction histories is needed, failing to provide specific information directly related to the image content. Furthermore, the response entirely disregards key features of the image, such as the figure depicted and the style of the painting. It made no attempt to analyze or extract relevant information from the image itself. The task type, “provenance tracking,” explicitly requires specific details regarding the painting’s provenance. However, the response deviates significantly from the task's objective by offering a generalized and non-informative statement, thereby failing to meet the expectations of the task. The open-source model received a score of 0 points due to its failure to extract useful information, its lack of analysis of the image content, and its inability to fulfill the task objectives.

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{pictures_filter/filter_fig9.jpg}
    \caption{Example 9 of Filtered-Out Samples.}
\label{fig:filter_9}
\end{figure}
In Figure~\ref{fig:filter_9}, the answer provided \( z = \frac{5}{3} \) as the final solution, but through logical reasoning and algebraic calculations, \( z \) can be correctly determined as \( 222 \) or \( -\frac{5}{4} \). Thus, the answer is incorrect. The task requires logical reasoning and complex algebraic calculations to solve the problem, but the answer does not meet the expected level of complexity required for this task type. Additionally, the image content appears to be more relevant to OCR recognition of the book cover rather than the mathematical problem presented. This mismatch indicates that the problem itself may be irrelevant or disconnected from the graphical content, making the task type unsuitable. As a result, two of the three open-source models correctly judged the answer with 0 points. The Internvl model, however, provided a score, which may reflect a lack of mathematical reasoning capabilities, leading to an incorrect evaluation of the task's requirements and performance.

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{pictures_filter/filter_fig10.jpg}
    \caption{Example 10 of Filtered-Out Samples.}
\label{fig:filter_10}
\end{figure}

In Figure~\ref{fig:filter_10}, the model depicts a scenario in which “flooding inundates the marina and affects nearby buildings and facilities”. The image shows no sign of flooding at all, only a normal marina scene with moored boats. The model's responses were generated entirely on the basis of fictional content. The model may have assumed flood damage in the image by default due to the task type “damage assessment” and ignored the normal conditions of the actual image. The task asks for an assessment of “infrastructure damage due to flooding” and there is no flooding in the image, so the model should recognize this and answer with something realistic such as “no flooding or damage observed”. This mismatch between the task type and the image content led to the creation of subsequent illusory responses, which were filtered with a total score of less than two.

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{pictures_filter/filter_fig11.jpg}
    \caption{Example 11 of Filtered-Out Samples.}
\label{fig:filter_11}
\end{figure}
In Figure~\ref{fig:filter_11}, the yellow structure in the picture appears to be a discarded refrigerator, and there is no visible notice board attached to it. The model likely generated a plausible but fabricated response based on the cues "yellow structure" and "notice board." However, the information in the response is entirely fictitious. Moreover, the yellow structure on the left clearly displays the text "New York Post," indicating it is a box for distributing newspapers. This has no connection to recycling dates or garbage sorting. The model incorrectly associated the task type (public message board) with garbage sorting information, likely due to a lack of OCR capability and proper semantic understanding, resulting in a response that is detached from the actual content of the image. Two open-source models correctly scored the response 0 points, recognizing that the content was illusory and inaccurate. However, one model mistakenly scored the response 1 point, possibly because the answer aligned loosely with the task type in a superficial manner.

\textbf{Analysis:} In the final stage of open-source model scoring and judging to screen samples, an analysis and summary of the above examples reveal several key issues. Some samples are unqualified due to errors in the questions or answers generated by the MLLM (Multimodal Large Language Model) itself. Others fail because the model's answers contain hallucinations or incorrect information, while in some cases, the task type does not align with the content of the image, resulting in the model's question-answer pair being invalid. Additionally, certain issues arise when the model generates hallucinatory information that, while related to the task type, is not connected to the actual content of the image. Moreover, the model's inherent capability limitations (e.g., OCR capability, mathematical reasoning ability) also contribute to errors in the answers.
These challenges highlight the problems faced by MLLMs when generating answers. To mitigate the inclusion of such hallucinatory and erroneous samples in the final dataset, we incorporated three open-source MLLMs to evaluate and score samples. This multi-model evaluation approach helps identify unqualified samples more effectively. However, as observed in the examples above, even open-source models can sometimes misjudge samples.
To address this, we adhere to the principle of "three ignorant cobblers working together outdo a Zhuge Liang," allowing multiple models to evaluate and score samples collectively. This strategy maximizes the chances of filtering out unqualified samples and ensures the construction of a high-quality TaskGalaxy dataset.

\subsection{Licences of Data}
The licensing information for the image sources listed in Table~\ref{tab:datasource} is as follows: ALLaVA (Apache License 2.0), Visual Genome (CC BY 4.0), MathV360K (Apache License 2.0), and ShareGPT4V (CC BY-NC 4.0). The proposed dataset, upon its open-source release, will be licensed under CC BY-NC 4.0.

We acknowledge our responsibility for ensuring legal compliance in data usage. The dataset licenses have been carefully reviewed, and our release under CC BY-NC 4.0 aligns with the restrictions of certain sources. Steps have been taken to mitigate potential legal risks and ensure adherence to the respective terms.


\subsection{Evaluation Regulations}
For the evaluated benchmarks MME, MMBench, MMBench\_CN, MM-VeT, POPE, SEED, SQA, and TextVQA, we utilized the official evaluation code provided by LLaVA. For AI2D, ChartQA, HallusionBench, LLaVA-in-the-wild, MMMU, Q-Bench, and Chinese-Q-Bench, we referred to the evaluation code that follows the official evaluation protocol of InternLM-XComposer. The MathVista evaluation baseline was conducted using the official evaluation code of MathVista. In benchmarks such as MM-VeT, LLaVA-in-the-wild, and MathVista, we replaced the original GPT-4 API with GPT-4o, which offers more stringent criteria and improved performance for scoring, answer extraction, answer matching, and related tasks. 


\subsection{Experimental Evaluation Using TaskGalaxy, Baseline Dataset, and Other Instruction-Tuning Datasets Individually}
To demonstrate the impact of TaskGalaxy's task diversity on model performance, we compare the baseline fine-tuning data with TaskGalaxy fine-tuning data, as well as several other instruction-tuning datasets, including ShareGPT-4V~\cite{sharegpt4v}, LLaVA-OneVision~\cite{llava-onevision}, ALLaVA-4V~\cite{allava}, and Cambrian-1~\cite{cambrian}. For a fair comparison, we randomly sampled the same number of samples from each dataset as in TaskGalaxy for fine-tuning. The results, summarized in Table~\ref{tab:other_result}, demonstrate that TaskGalaxy consistently achieves the highest performance on most benchmarks across multiple model architectures, validating its effectiveness. 

\begin{table*}[!bht]
\centering
\caption{Experimental Evaluation of TaskGalaxy, Baseline Dataset, and Other Instruction-Tuning Datasets Separately. All the numbers are presented in \% except MME and the full score is 100\%. The indicator of MME is the perception score, the maximum value is 2000. The best results are highlighted in \textbf{bold}.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccccccccccc}
\hline
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Model\end{tabular}} & \multirow{2}{*}{Method} & \multicolumn{8}{c}{Benchmarks} \\ \cline{3-11}
& & MME & MMB & MMB$^{\mathrm{CN}}$ & POPE & LLaVA$^{\mathrm{W}}$ & MMVet & TQA & SQA & MathVista\\ \hline
\multicolumn{1}{c|}{\multirow{13}{*}{LLaVA-v1.5-7B}} & \multicolumn{1}{c|}{Baseline}&1476&63.29&56.45&86.30&47.70&24.70&57.59&68.77&28.20\\  
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{ShareGPT-4V}&1501 &  65.97 & 
 59.10 &  86.29 & 49.20 &  29.00 &  57.56 &  70.60 &  28.20 \\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{LLaVA-OneVision}& 1251& 59.79& 52.84& 83.90 & 51.20& \textbf{29.60} & 52.99 &  \textbf{73.19} & 28.20  \\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{ALLaVA-4V}& 1474 &  60.13 &  55.39 &  84.21 &  38.00 & 27.00 &  53.77 &  70.05 &  29.20 \\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Cambrian-1}& 1494 & 61.08 &  54.46 &  85.46 &  52.00 &  25.70 &  55.17 &  71.03 & 29.10 \\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{TaskGalaxy}& \textbf{1520} & \textbf{66.62} & \textbf{59.43} & \textbf{86.40} & \textbf{52.30} & 28.60 & \textbf{58.08} & 71.06 &  \textbf{29.30} \\ \cline{3-11}


\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{} & ChartQA & AI2D & Q-Bench & Q-Bench$^{\mathrm{CN}}$ & HalluBench & SEED & MMMU & \multicolumn{2}{c}{Average (w/o MME)} \\ \cline{3-11} 

\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline}& 14.40 & 25.29 & 24.89 & 31.26 & 47.95 & 58.62 & 19.70 & \multicolumn{2}{c}{43.62} \\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{ShareGPT-4V}& 17.84 &  27.08 & 
 26.22 &  32.51 & 48.79 &  59.26 &  15.60 &  \multicolumn{2}{c}{44.48}\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{LLaVA-OneVision}& 18.72 &  27.95 &  24.48 &  33.51 &  47.74 &  30.06 &  17.00 &  \multicolumn{2}{c}{41.68}\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{ALLaVA-4V}& 17.00 & 23.73 &  23.95 &  33.18 &  48.73 &  40.52 &  18.10 & \multicolumn{2}{c}{41.53}\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Cambrian-1}& 20.07 &  29.46 & 26.70 &  33.61 &  50.78 &  49.52 &  19.30 & \multicolumn{2}{c}{44.23}\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{TaskGalaxy}& 19.90 & \textbf{32.70} & \textbf{30.24} & \textbf{34.01} & \textbf{50.95} & \textbf{59.32} & \textbf{20.70} & \multicolumn{2}{c}{\textbf{46.49}} \\ \cline{1-11}
 
\multicolumn{1}{c|}{\multirow{13}{*}{InternVL-Chat-v1.0-7B}} & \multicolumn{1}{c|}{} & MME & MMB & MMB$^{\mathrm{CN}}$ & POPE & LLaVA$^{\mathrm{W}}$ & MMVet & TQA & SQA & MathVista \\ \cline{3-11}

\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline}& 1488 & 64.86 & 56.41 & 86.03 &  48.30 &  25.70 &  55.29 &  65.63 &  27.00 \\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{ShareGPT-4V}& 1191 & 47.08 & 40.63 & 82.62 & 30.00 & 17.90 & 44.21 & 64.55 & 27.20\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{LLaVA-OneVision}& 1350 & 61.23 & 54.74 &  67.94 &  32.50 &  19.40 &  37.74 &  66.29 &  25.00\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{ALLaVA-4V}& 1425& 62.76 &  52.78 & 84.50 &  21.50 &  23.50 &  48.04 &  66.29 &  29.40 &   \\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Cambrian-1}& 1481 &  60.22 & 53.01 &  84.17 &  43.70 &  26.80 & 52.61 &  67.71 &  \textbf{33.00}\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{TaskGalaxy}& \textbf{1512} & \textbf{65.03} & \textbf{57.91} & \textbf{86.23} &  \textbf{52.30}& \textbf{30.10} & \textbf{56.15} & \textbf{68.88}&  30.10 \\ \cline{3-11}


\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{} & ChartQA & AI2D & Q-Bench & Q-Bench$^{\mathrm{CN}}$ & HalluBench & SEED & MMMU & \multicolumn{2}{c}{Average (w/o MME)} \\ \cline{3-11} 

\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline}& 14.12 &  35.92 &  42.89 & 43.73 & 51.94 & 59.06 & 26.90 &  \multicolumn{2}{c}{47.17}\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{ShareGPT-4V}& 14.52 & 35.59 & 46.69 & 36.38 & 52.36 & 47.24 & 30.30& \multicolumn{2}{c}{42.48}\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{LLaVA-OneVision}& 13.76 &  22.75 &  40.08 &  42.89 &  53.39 &  40.87 &  24.60 &\multicolumn{2}{c}{40.20} \\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{ALLaVA-4V}& 12.99 &  28.28 &  42.87 &  44.16 &  51.41 &  48.36 &  27.30 & \multicolumn{2}{c}{42.94}\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Cambrian-1}& \textbf{16.00} & 36.69 &  48.00 &  41.33 &  \textbf{54.63} &  56.24 &  30.60 &  \multicolumn{2}{c}{46.98}\\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{TaskGalaxy}& 15.16 & \textbf{37.69} &  \textbf{48.21} & \textbf{46.32} & 53.00 &  \textbf{60.44} &  \textbf{32.80} & \multicolumn{2}{c}{\textbf{49.63}}\\

\bottomrule
\hline
\end{tabular}
}
\label{tab:other_result}

\end{table*}

\subsection{More advanced model architecture}
For more advanced models, we utilize the InternVL-Chat-V2.0-8B model, which has made its second-stage instruction fine-tuning data publicly available. For the comparison, we randomly sample the same number of samples as TaskGalaxy from the officially disclosed instruction fine-tuning dataset.

We fine-tune InternVL-Chat-V2.0-8B using both the original instruction fine-tuning dataset and the TaskGalaxy instruction fine-tuning dataset, ensuring that the number of samples for each is consistent with TaskGalaxy. The Table~\ref{tab:moremodel_result} shows the performance comparison between the original instruction fine-tuning dataset and the TaskGalaxy instruction fine-tuning dataset on InternVL-Chat-V2.0-8B.

\begin{table*}[!bht]
\centering
\caption{Experimental Evaluation of Fine-tuning InternVL-Chat-v1.0-8B Using TaskGalaxy and Baseline Dataset Individually. All the numbers are presented in \% except MME and the full score is 100\%. The indicator of MME is the perception score, the maximum value is 2000. The best results are highlighted in \textbf{bold}.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccccccccccc}
\hline
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Model\end{tabular}} & \multirow{2}{*}{Method} & \multicolumn{8}{c}{Benchmarks} \\ \cline{3-11}
& & MME & MMB & MMB$^{\mathrm{CN}}$ & POPE & LLaVA$^{\mathrm{W}}$ & MMVet & TQA & SQA & MathVista\\ \hline
\multicolumn{1}{c|}{\multirow{5}{*}{InternVL-Chat-V2.0-8B}} & \multicolumn{1}{c|}{Baseline}&1536&68.52&66.46&86.30&\textbf{63.20}&46.17&66.24&90.58&50.10&\\  
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{TaskGalaxy}& \textbf{1565} & \textbf{73.88} & \textbf{70.79} & \textbf{86.90} & 62.85 & \textbf{48.86} & \textbf{70.49} & \textbf{92.71} &  \textbf{52.31} \\ \cline{3-11}


\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{} & ChartQA & AI2D & Q-Bench & Q-Bench$^{\mathrm{CN}}$ & HalluBench & SEED & MMMU & \multicolumn{2}{c}{Average (w/o MME)} \\ \cline{3-11} 

\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline}& \textbf{76.64} & 75.88 & 57.79 & 56.98 & 57.51 & 62.72 & 40.50 & \multicolumn{2}{c}{65.86} \\
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{TaskGalaxy}& 76.56 & \textbf{76.75} & \textbf{59.65} & \textbf{57.12} & \textbf{58.99} & \textbf{64.25} & \textbf{41.22} & \multicolumn{2}{c}{\textbf{67.81}} \\
 
\bottomrule
\hline
\end{tabular}
}
\label{tab:moremodel_result}
\end{table*}

As shown in the Table~\ref{tab:moremodel_result}, after fine-tuning the model using the TaskGalaxy dataset and the original InternVL-Chat-V2.0-8B fine-tuned dataset, TaskGalaxy outperforms the original baseline dataset on 14 out of 16 benchmarks. For the remaining two benchmarks, ChartQA and LLaVA-in-the-wild, the performance difference compared to the baseline dataset is minimal (less than 0.5). This demonstrates that TaskGalaxy's enhancement of task diversity is also effective for more advanced models, providing a significant boost in overall performance.

\subsection{The benefits of Chain-of-Thought(CoT)}
Numerous studies~\citep{cot_1,cot_2,cot_3,cot_4} have highlighted the significant impact of Chain-of-Thought (CoT) prompting on enhancing MLLM performance. In this section, we investigate whether CoT prompting improves performance with increased task types. We constrained the maximum number of samples per task type to 5, resulting in a total of 76k samples (as indicated by max\_5 in Table~\ref{tab:cot_result}). We compared the performance using original TaskGalaxy Q\&A data with CoT-generated answers from GPT-4o, designed through specific prompts detailed in the Appendix. The results show significant improvements for the CoT versions of TaskGalaxy (max\_5) in benchmarks such as MME, LLaVA-in-the-wild, and Q-Bench. Additionally, the average performance across 15 benchmarks, excluding MME, increased by approximately 1.3 points with CoT. These findings underscore the value of incorporating CoT prompting into multimodal models. 
\begin{table*}[!bht]
\centering
\caption{\textbf{Performance comparison of CoT validity verification.} +max\_5 refers to 19,227 task types, each containing no more than 5 samples, totaling approximately 76k samples. In contrast, +max\_5 (CoT) represents the version where Chain-of-Thought (CoT) answers were generated for all the aforementioned samples.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccccccccccccccc}
\hline
\toprule
\multirow{2}{*}{\begin{tabular}[c]
{@{}c@{}}Model\end{tabular}} & \multirow{2}{*}{Method} & \multicolumn{8}{c}{Benchmarks} \\ \cline{3-10}
& & MME & MMB & LLaVA$^{\mathrm{W}}$ & MathVista & ChartQA & Q-Bench & MMMU & Average( w/o MME)\\ \hline
\multicolumn{1}{c|}{\multirow{3}{*}{LLaVA-v1.5-7B}} & \multicolumn{1}{c|}{Baseline}&1506&64.69&53.0&26.7&14.72&26.08&16.6&44.46\\  \multicolumn{1}{c|}{}&\multicolumn{1}{c|}{Baseline+max\_5}& 1506 & 65.80& 53.4&27.3&20.20&36.48&17.4&46.61 \\ 
\multicolumn{1}{c|}{}& \multicolumn{1}{c|}{Baseline+max\_5 (CoT)}& 1523 & 66.72 & 64.7&27.9&20.96&43.27&19.3&47.92\\ 
\bottomrule
\hline
\end{tabular}
}
\label{tab:cot_result}
\end{table*}


\subsection{Task types in TaskGalaxy}
One of the key challenges addressed by the TaskGalaxy instruction fine-tuning dataset is the substantial expansion of task type diversity. Initially, we manually defined a small set of task types, which was later expanded to 19,227 hierarchical task types using GPT-4o. Given the large number of task types and space constraints, we present only a selection of these hierarchical task types here. A more comprehensive list will be available in the full dataset upon its release.




\begin{longtable}{|c|p{0.45\textwidth}|}
\caption{Comprehensive Task Type Table} \label{table:task_type} \\

\hline
\multicolumn{1}{|c|}{Level-1 Task Type} & \multicolumn{1}{|c|}{Level-2, etc. Task Types} \\
\hline
\endfirsthead

\hline
\multicolumn{2}{|c|}{\textit{Continued from previous page}} \\
\hline
\multicolumn{1}{|c|}{Level-1 Task Type} & \multicolumn{1}{|c|}{Level-2, etc. Task Types} \\
\hline
\endhead

\hline
\multicolumn{2}{|c|}{\textit{Continued on next page}} \\
\hline
\endfoot

\hline
\endlastfoot

\multirow{20}{*}{\centering OCR} & bill ocr $\sim$ medical bill recognition; bill ocr $\sim$ travel expense recognition; book ocr $\sim$ metadata extraction; book ocr $\sim$ text summarization; business card ocr $\sim$ text extraction; business card ocr $\sim$ entity recognition; captcha ocr $\sim$ multi-font text recognition; chart $\sim$ bar chart ocr; chart $\sim$ violin plot ocr; comic strip ocr $\sim$ speech bubble detection; comic strip ocr $\sim$ text translation; diagram ocr $\sim$ flowchart recognition; document $\sim$ form ocr; document $\sim$ full text $\sim$ literature $\sim$ research paper ocr; font recognition $\sim$ font type identification; graffiti ocr $\sim$ text enhancement; grocery list ocr $\sim$ item price extraction; handwritten text ocr $\sim$ word recognition; infographic ocr $\sim$ caption analysis; invoice ocr $\sim$ data validation; invoice ocr $\sim$ signature detection; label ocr $\sim$ word recognition; lecture notes ocr $\sim$ speaker identification and attribution; logo recognition $\sim$ brand identification; logo recognition $\sim$ logo location detection; musical notes ocr $\sim$ time signature detection; ... \\ 
\hline

\multirow{20}{*}{\centering Image Description} & abstract-concept-based $\sim$ symbolic representation; abstract-concept-based $\sim$ aesthetic judgment; accessibility-based description $\sim$ highlight key elements; accessibility-based description $\sim$ emotionally aware description; action-based $\sim$ contextual action description; activity-based $\sim$ sports event description; activity-based $\sim$ performance or event description; advertisement-based $\sim$ testimonial integration; art-style-based $\sim$ surrealist description; attribute-based $\sim$ pattern recognition; autobiographical-based $\sim$ daily activities; bias-mitigation-based $\sim$ disability bias detection; coarse-grained $\sim$ highlight extraction; coarse-grained $\sim$ alt text generation; context-aware description correction $\sim$ factual accuracy verification; contrast-based $\sim$ emphasis on unique aspects; cross-cultural adaptability $\sim$ identify cultural context; culture-based $\sim$ architecture and landmarks; culture-based $\sim$ social norms and values; educational-content-based $\sim$ step-by-step tutorial; emotion-based $\sim$ emotion-driven storytelling; event-based $\sim$ event sentiment analysis; ... \\ 
\hline

\multirow{15}{*}{\centering Detection} & object detection $\sim$ single object detection $\sim$ firefighting equipment detection; object detection $\sim$ single object detection $\sim$ vehicle $\sim$ interior detection; target detection $\sim$ scene detection $\sim$ campus scene detection; target detection $\sim$ scene detection $\sim$ traffic scene detection; signature detection; out of stock detection; anomaly detection $\sim$ vehicle anomaly detection; anomaly detection $\sim$ behavior anomaly detection; object detection $\sim$ multiple object detection $\sim$ quantity detection; object detection $\sim$ single object detection $\sim$ road traffic signal detection; object detection $\sim$ single object detection $\sim$ public facility detection; target detection $\sim$ scene detection $\sim$ pond scene detection; signature detection; ... \\ 
\hline

\multirow{21}{*}{\centering Analysis} & color analysis $\sim$ color contrast analysis; color analysis $\sim$ dominant color detection; complex scenes $\sim$ attribute extraction; content accessibility analysis $\sim$ automatic summarization for accessibility; content personalization $\sim$ user interest profiling; content prudence analysis $\sim$ source reliability evaluation; design concepts and intentions $\sim$ evaluate layout and composition; emotional analysis $\sim$ mood classification; emotional analysis $\sim$ emotion trajectory analysis; fashion analysis $\sim$ season identification; fashion analysis $\sim$ color analysis; game rules and strategies $\sim$ goal determination; gender representation analysis $\sim$ gender stereotype identification; gender representation analysis $\sim$ gender representation in advertising analysis; language use analysis $\sim$ language complexity analysis; language use analysis $\sim$ speech act recognition; review analysis $\sim$ review summarization; review analysis $\sim$ authenticity verification; political sentiment analysis $\sim$ polarization detection; political sentiment analysis $\sim$ emotion detection; ... \\ 
\hline

\multirow{7}{*}{\centering{Image-based Knowledge Distillation}} &  fine-grained image classification; semantic segmentation; caption generation; multimodal trend analysis; image inpainting; region-based image captioning; object attribute extraction; attribute recognition; content summarization; visual relationship detection;......\\ 
\hline

\multirow{32}{*}{\centering Content Creation} & FAQ creation $\sim$ Extract information from images to create detailed FAQ answers; FAQ creation $\sim$ Determine ambiguity in text and images for FAQ refinement; advertising content creation $\sim$ brochure content creation; advertising content creation $\sim$ flyer content creation; annual report writing $\sim$ industry benchmarking; annual report writing $\sim$ risk assessment; art critique $\sim$ technique and brushwork examination; art critique $\sim$ gesture and movement evaluation; artistic inspiration writing $\sim$ mood setting; artistic inspiration writing $\sim$ genre-specific style implementation; augmented reality content creation $\sim$ virtual staging and design; augmented reality content creation $\sim$ 3D object placement; brainstorming $\sim$ graphic design concepts; brainstorming $\sim$ product ideation; brainstorming $\sim$ headline generation; children's book creation $\sim$ conflict resolution; children's book creation $\sim$ narrative voice consistency; content curation $\sim$ content diversification; corporate training content creation $\sim$ employee onboarding material creation; diversity and inclusion content creation $\sim$ celebration of cultural events; e-commerce content creation $\sim$ purchase decision support content creation; e-commerce content creation $\sim$ content creation for limited-time promotions; educational game content creation $\sim$ image captioning games; interactive content creation $\sim$ immersive simulations; letter writing $\sim$ closing statement formulation; log writing $\sim$ daily summary; log writing $\sim$ task management; ... \\ 
\hline

\multirow{13}{*}{\centering{Suggestions}}& home decor ideas; furniture design; wedding planning; recipe suggestions; seasonal decorations; party themes; workspace ergonomics; hair styling; book recommendations; interior lighting ideas; eco-friendly products; movie suggestions; fashion advice; life hacks; meditation practices; time management techniques; TV show recommendations; coding resources; online learning platforms; memory improvement exercises; volunteer opportunities; language translation aids; entertainment activities; speech writing; content creation tools; study techniques;......
\\ 
\hline

\multirow{17}{*}{\centering{Subject Question and Answer}} & mathematics; transportation\&logistics; religion; language\& literature; law; agriculture; architecture; psychology; geography; environmental studies; political science; entertainment; economics; philosophy; education; history; media\&communication; arts\&arts$\sim$music studies; arts\&arts$\sim$design studies; arts\&arts$\sim$academic theoretical studies; business$\sim$management studies; business$\sim$finance studies; business$\sim$accounting studies; health\&medicine$\sim$clinical medical studies; humanities\&social sciences$\sim$history studies; science$\sim$geography studies; science$\sim$mathematics studies; technology\&engineering$\sim$mechanical engineering studies; technology\&engineering$\sim$electrical\& energy studies;......\\ 
\hline

\multirow{28}{*}{\centering{Summarization}} & sports event summarization; pop culture summarization; policy summarization; news article summarization; book overview summarization; technical document summarization; agricultural data summarization; recipe summarization; presentation summarization; environmental impact summarization; genealogy summarization; customer journey summarization; financial report summarization; advertisement summarization; legal document summarization; travel itinerary summarization; instructional summarization; customer preferences summarization; market analysis summarization; political debate summarization; comparative summarization; multimodal dataset summarization; multilingual content summarization; event summarization; healthcare report summarization; wildlife monitoring summarization; customer service conversation summarization; event outcome summarization; climate data summarization; historical document summarization; educational content summarization; radio interview summarization; product description summarization; brand sentiment summarization; thematic summarization; visual trend summarization; image-based content summarization; product review summarization;......\\ 
\hline
\multirow{35}{*}{\centering{Logical Reasoning}} & abductive reasoning$\sim$multimodal hypothesis generation; analogy reasoning$\sim$scene relationship analogy reasoning; causal reasoning$\sim$counterfactual reasoning; complex reasoning$\sim$three-dimensional spatial relationship reasoning; complex reasoning$\sim$passenger flow analysis reasoning; complex reasoning$\sim$market analysis reasoning; complex reasoning$\sim$task relationship reasoning; complex reasoning$\sim$ethical dilemma reasoning; complex reasoning$\sim$role reasoning; complex reasoning$\sim$state reasoning; complex reasoning$\sim$chart understanding and analysis$\sim$multi-layer pie chart logical understanding and analysis; complex reasoning$\sim$chart understanding and analysis$\sim$high-dimension data scatter plot understanding and analysis; complex reasoning$\sim$complex mathematical calculations$\sim$geometric mathematical operations; complex reasoning$\sim$complex mathematical calculations$\sim$computer science algorithms operations; deductive reasoning$\sim$attribute deduction; deductive reasoning$\sim$inference of missing information; simple reasoning$\sim$goal-directed reasoning; simple reasoning$\sim$emotion reasoning; simple reasoning$\sim$sequence ordering; simple reasoning$\sim$basic mathematics$\sim$simple numerical calculation reasoning; simple reasoning$\sim$basic mathematics$\sim$quantity reasoning; spatial reasoning$\sim$topological reasoning; spatial reasoning$\sim$3D scene reconstruction; spatial reasoning$\sim$spatial layout recognition; temporal reasoning$\sim$contextual time inference; temporal reasoning$\sim$event ordering; ......\\ 
\hline
\multirow{13}{*}{\centering{Context-Aware Recommendations}} & seasonal and holiday recommendations; personalized shopping guides; context-based travel suggestions; context-specific personal assistant; visual product recommendations; real-time event notifications; location-based activity suggestions; dynamic content customization; context-sensitive educational content; contextual news delivery; adaptive learning resources; contextual target audience analysis; lifestyle-based content curation; interest-based content filtering; mood-based content suggestions; context-specific social media posts; personalized content suggestions; \\ 
\hline
\multirow{22}{*}{\centering{Refusal}} & due to animal cruelty$\sim$identification of animal types in images; due to animal cruelty$\sim$image and text alignment for cruelty evidence; due to child exploitation$\sim$identification of minors; due to child exploitation$\sim$engagement with law enforcement; due to conspiracy theories$\sim$bi-modal sentiment analysis related to conspiracies; due to conspiracy theories$\sim$detect repetitive conspiracy motifs; due to dangerous stunts$\sim$compare depicted stunts with known dangerous activities; due to dangerous stunts$\sim$analyze risk levels of described actions; due to deepfake content$\sim$classification; due to human trafficking$\sim$detecting document forgery; due to incitement of panic$\sim$cross-verify with trusted sources; due to political propaganda$\sim$identify exaggeration in political claims; due to solicitation$\sim$detecting refusal language; refusal due to illegal activities$\sim$refusal due to illegal distribution; refusal due to pornographic content$\sim$characterization of suggestive poses;......
\\ 
\hline
\multirow{23}{*}{\centering{3D Object Recognition}} & Depth estimation$\sim$Self-supervised depth estimation; Material and texture recognition$\sim$Material texture segmentation in images; Material and texture recognition$\sim$Material texture correlation between images and text; Object attribute extraction$\sim$Color detection; Object detection$\sim$Real-time object detection; Object detection$\sim$Category-based object detection; Object interaction modeling$\sim$Object spatial relationships; Object matching and retrieval$\sim$Aligning textual descriptions with visual object attributes; Object matching and retrieval$\sim$Multi-view image retrieval from textual input; Object part recognition$\sim$Part-based object localization; Object part segmentation$\sim$Part relationship analysis; Object pose estimation$\sim$Instance-level pose estimation; Object recognition in context$\sim$Semantic segmentation of objects in context; Object registration$\sim$Multi-view registration; Object tracking$\sim$Object localization; Occlusion handling$\sim$Multi-view fusion for occluded objects; Occlusion handling$\sim$Self-supervised learning for occlusion robustness;......\\ 
\hline
\multirow{28}{*}{\centering{Safety and Risk Assessment}} & aviation safety and risk assessment$\sim$runway safety monitoring; aviation safety and risk assessment$\sim$air traffic control communication analysis; biological hazard risk assessment$\sim$foodborne pathogen identification; biological hazard risk assessment$\sim$water quality assessment; biological hazard risk assessment$\sim$disease outbreak identification; chemical hazard risk assessment$\sim$incident analysis and reporting; child safety and risk assessment$\sim$age-appropriate content detection; child safety and risk assessment$\sim$explicit content filtering; construction site safety and risk assessment$\sim$safety signage compliance; construction site safety and risk assessment$\sim$emergency response preparedness; consumer product safety and risk assessment$\sim$label product hazards; cultural heritage safety and risk assessment$\sim$damage assessment; cultural heritage safety and risk assessment$\sim$environmental hazard identification; elderly care safety and risk assessment$\sim$fall risk detection; entertainment venue safety and risk assessment$\sim$first aid station location identification; environmental safety and risk assessment$\sim$wildlife impact assessment; infrastructure safety and risk assessment$\sim$bridge stability analysis; wildlife safety and risk assessment$\sim$monitoring of wildlife health;......
\\ 
\hline
\multirow{13}{*}{\centering{Image-Text Matching}} & image sequence$\sim$visual storytelling; image sequence$\sim$next image prediction; image sequence$\sim$story board generation; multiple images$\sim$event chronology; multiple images$\sim$relationship extraction; multiple images$\sim$collage interpretation; single image$\sim$visual reasoning; multiple images$\sim$visual consistency; multiple images$\sim$relationship extraction; question answer selection$\sim$visual entailment; question answer selection$\sim$scene-text based question answering; question answer selection$\sim$image-caption-based question answering;...... \\ 
\hline
\multirow{33}{*}{\centering{Science-Related}} & anomaly detection in scientific data$\sim$detecting unexpected changes in medical imaging and diagnostic reports; chart and diagram interpretation$\sim$identify underlying assumptions in data; chart and diagram interpretation$\sim$compare different data sets; chart and diagram interpretation$\sim$explain scientific concepts; citations and influence analysis$\sim$citation context analysis; citations and influence analysis$\sim$historical citation trends analysis; common knowledge question and answer$\sim$correlate text to scientific imagery; common knowledge question and answer$\sim$classify scientific categories in images; common knowledge question and answer$\sim$compare scientific phenomena in different images; common knowledge question and answer$\sim$deduce outcomes based on visual experiments; concept drifts detection$\sim$alteration in hypothesis testing; concept drifts detection$\sim$shift in research focus; concept drifts detection$\sim$updates in scientific vocabularies; conclusion extraction$\sim$pattern recognition; content paraphrasing$\sim$explanatory paraphrasing; data correlation analysis$\sim$metadata extraction; data correlation analysis$\sim$information synthesis; data correlation analysis$\sim$context-aware filtering; error detection and correction$\sim$table data correctness; experiment hypothesis generation$\sim$interactive multimodal hypothesis testing; scientific argumentation analysis$\sim$confounding factor detection; visual reasoning$\sim$flowchart analysis; visual reasoning$\sim$equation-visual correlation;......\\ 
\hline


\multirow{17}{*}{\centering{Concept Extraction}} & attribute extraction$\sim$texture recognition; attribute extraction$\sim$spatial geometry determination; attribute extraction$\sim$affordance recognition; attribute extraction$\sim$object functionality identification; caption generation$\sim$geographical context captioning; contextual similarity$\sim$contextual relationship extraction; contextual similarity$\sim$cross-modal context expansion; image-text localization$\sim$caption region association; keyphrase extraction$\sim$hierarchical keyphrase extraction; keyphrase extraction$\sim$frequency-based keyphrase extraction; relationship extraction$\sim$agent-action relationship identification; relationship extraction$\sim$causal relationship identification; relationship extraction$\sim$object-action relationship identification; summary generation$\sim$balanced summary;......\\ 
\hline



\multirow{18}{*}{\centering{Interactive Art Creation}} & 3D model generation from text$\sim$3D object reconstruction; 3D model generation from text$\sim$Image-based texture generation; adaptive theme-based art expansion using text$\sim$context-aware embellishment of art based on text; adaptive theme-based art expansion using text$\sim$coherence evaluation across multi-modal elements; adaptive theme-based art expansion using text$\sim$hierarchical theme structuring from text; art critique and suggestion$\sim$color analysis; art critique and suggestion$\sim$contextual relevance; art critique and suggestion$\sim$technical proficiency critique; artistic scene composition$\sim$color palette matching; collaborative art creation with text$\sim$crowdsourced art projects; conceptual visualization from narrative text$\sim$poetry-inspired illustrations; context-aware art adaptation$\sim$object removal;......\\ 
\hline
\multirow{32}{*}{\centering{Medical Imaging Analysis}} & document summarization; anomalous region detection$\sim$lesion detection; anomalous region detection$\sim$deformation detection; anomalous region detection$\sim$obstruction detection; biomarker identification$\sim$predictive biomarker discovery; biomarker identification$\sim$radiomic feature extraction; clinical trial matching$\sim$Imaging Biomarker Identification; clinical trial matching$\sim$Clinical Trial Summarization; clinical trial matching$\sim$Patient Profile Construction; data annotation$\sim$disease classification; data annotation$\sim$report generation; disease diagnosis$\sim$eye disease detection; disease diagnosis$\sim$tumor classification; disease diagnosis$\sim$autoimmune disease detection; disease diagnosis$\sim$diabetes-related imaging analysis; functional mapping$\sim$biomarker identification; image classification$\sim$organ segmentation; image classification$\sim$tissue type classification; image classification$\sim$image quality assessment; medical captioning$\sim$anatomical structure captioning; medical captioning$\sim$diagnostic summary captioning; medical captioning$\sim$procedure description captioning; medical captioning$\sim$lesion detection captioning; patient outcome prediction$\sim$complication risk assessment; patient outcome prediction$\sim$disease progression prediction; surgical assistance$\sim$training and simulation; surgical assistance$\sim$augmented reality visualization; treatment planning$\sim$tumor localization; treatment planning$\sim$risk assessment;......\\ 
\hline
\multirow{8}{*}{\centering{Multimodal Translation}} & text-based image description; image to motivational quote generation; image to narrative generation; image to multilingual article generation; dynamic image caption generation; fine-grained image understanding and text generation; image to paragraph generation; image-based dialogue generation; contextual image description generation;......\\ 
\hline
\multirow{6}{*}{\centering{Multiple Choice Questions}} &  text-based; image-based; combined media-based; image-text based$\sim$image focus; image-text based$\sim$balanced focus; image-text based$\sim$text focus; image-text based$\sim$contextual inference; image-text based$\sim$temporal understanding;......\\ 
\hline
\multirow{30}{*}{\centering{Scene Understanding}} & activity recognition$\sim$sleep behavior analysis; activity recognition$\sim$animal behavior recognition; activity recognition$\sim$gesture recognition; activity recognition$\sim$group activity recognition; aption generation$\sim$style-specific captioning; context reasoning$\sim$cultural context detection; event detection$\sim$event duration estimation; expression recognition$\sim$expression detection in group images; expression recognition$\sim$pose estimation for expressions; gesture recognition$\sim$gesture pose estimation; human-object interaction detection$\sim$interaction dynamics modeling; human-object interaction detection$\sim$interaction localization; human-object interaction detection$\sim$interactive object detection
scene understanding$\sim$human-object interaction detection$\sim$relationship extraction; relationship detection$\sim$human-human relationship; relationship detection$\sim$human-environment relationship; relationship detection$\sim$human-object relationship; scene classification$\sim$urban scene classification; scene classification$\sim$commercial scene classification; scene classification$\sim$recreational scene classification; scene description$\sim$object identification; scene description$\sim$scene composition evaluation; scene description$\sim$background element identification; visual reasoning$\sim$spatial reasoning; visual reasoning$\sim$contextual reasoning;......\\ 
\hline
\multirow{10}{*}{\centering{Target Recognition in Special Image Domains}} & target recognition in paintings; target recognition in sketches; target recognition in clip art; target recognition in doodles; target recognition in low-resolution images; target recognition in photographs; target recognition in cartoons; target recognition in infographics; target recognition in x-ray images; target recognition in ct images; target recognition in 3d rendered images; target recognition in low exposure images;......\\ 
\hline
\multirow{22}{*}{\centering{Topic Classification}} & business$\sim$leadership changes; business$\sim$mergers and acquisitions; business$\sim$financial reporting; business$\sim$competitive analysis; business$\sim$corporate social responsibility; education$\sim$student proficiency assessment; education$\sim$educational resource identification; education$\sim$education level identification; emotion detection$\sim$anger detection; entertainment$\sim$event classification; entertainment$\sim$comic book identification; health$\sim$exercise and fitness tracking; health$\sim$treatment recommendation; health$\sim$mental health assessment; lifestyle$\sim$health and wellness; lifestyle$\sim$hobbies and crafts; politics$\sim$election event detection; politics$\sim$political sentiment analysis; politics$\sim$political stance detection; science$\sim$geology; science$\sim$computer science; sports$\sim$match predictions; sports$\sim$transfer news; technology$\sim$software categorization; technology$\sim$industry trends; travel$\sim$travel safety information extraction; travel$\sim$travel tips extraction;......\\ 
\hline
\end{longtable}

% \begin{table}[!bht]
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{|c|p{1\textwidth}|}
% \hline
% \multicolumn{1}{|c|}{level-1 task type} & \multicolumn{1}{|c|}{level-2, etc. task types} \\ \hline
% \multirow{11}{*}{\centering{analysis}} & color analysis$\sim$color contrast analysis; color analysis$\sim$dominant color detection; complex scenes$\sim$attribute extraction; content accessibility analysis$\sim$automatic summarization for accessibility; content personalization$\sim$user interest profiling; content prudence analysis$\sim$source reliability evaluation; design concepts and intentions$\sim$evaluate layout and composition; emotional analysis$\sim$mood classification; emotional analysis$\sim$emotion trajectory analysis; fashion analysis$\sim$season identification; fashion analysis$\sim$color analysis; game rules and strategies$\sim$goal determination; gender representation analysis$\sim$gender stereotype identification; gender representation analysis$\sim$gender representation in advertising analysis; language use analysis$\sim$language complexity analysis; language use analysis$\sim$speech act recognition; review analysis$\sim$review summarization; review analysis$\sim$authenticity verification; political sentiment analysis$\sim$polarization detection; political sentiment analysis$\sim$emotion detection;...... \\ \hline
% \multirow{16}{*}{\centering{content creation}} & FAQ creation$\sim$Extract information from images to create detailed FAQ answers; FAQ creation$\sim$Determine ambiguity in text and images for FAQ refinement; advertising content creation$\sim$brochure content creation; advertising content creation$\sim$flyer content creation; annual report writing$\sim$industry benchmarking; annual report writing$\sim$risk assessment; art critique$\sim$technique and brushwork examination; art critique$\sim$gesture and movement evaluation; artistic inspiration writing$\sim$mood setting; artistic inspiration writing$\sim$genre-specific style implementation; augmented reality content creation$\sim$virtual staging and design; augmented reality content creation$\sim$3D object placement; brainstorming$\sim$graphic design concepts; brainstorming$\sim$product ideation; brainstorming$\sim$headline generation; children's book creation$\sim$conflict resolution; children's book creation$\sim$narrative voice consistency; content curation$\sim$content diversification; corporate training content creation$\sim$employee onboarding material creation; diversity and inclusion content creation$\sim$celebration of cultural events; e-commerce content creation$\sim$purchase decision support content creation; e-commerce content creation$\sim$content creation for limited-time promotions; educational game content creation$\sim$image captioning games; interactive content creation$\sim$immersive simulations; letter writing$\sim$closing statement formulation; log writing$\sim$daily summary; log writing$\sim$task management;......
% \\ \hline
% \multirow{6}{*}{\centering{suggestions}}& home decor ideas; furniture design; wedding planning; recipe suggestions; seasonal decorations; party themes; workspace ergonomics; hair styling; book recommendations; interior lighting ideas; eco-friendly products; movie suggestions; fashion advice; life hacks; meditation practices; time management techniques; TV show recommendations; coding resources; online learning platforms; memory improvement exercises; volunteer opportunities; language translation aids; entertainment activities; speech writing; content creation tools; study techniques;......
% \\ \hline
% \multirow{13}{*}{\centering{summarization}} & sports event summarization; pop culture summarization; policy summarization; news article summarization; book overview summarization; technical document summarization; agricultural data summarization; recipe summarization; presentation summarization; environmental impact summarization; genealogy summarization; customer journey summarization; financial report summarization; advertisement summarization; legal document summarization; travel itinerary summarization; instructional summarization; customer preferences summarization; market analysis summarization; political debate summarization; comparative summarization; multimodal dataset summarization; multilingual content summarization; event summarization; healthcare report summarization; wildlife monitoring summarization; customer service conversation summarization; event outcome summarization; climate data summarization; historical document summarization; educational content summarization; radio interview summarization; product description summarization; brand sentiment summarization; thematic summarization; visual trend summarization; image-based content summarization; product review summarization;......\\ \hline
% \multirow{17}{*}{\centering{logical reasoning}} & abductive reasoning$\sim$multimodal hypothesis generation; analogy reasoning$\sim$scene relationship analogy reasoning; causal reasoning$\sim$counterfactual reasoning; complex reasoning$\sim$three-dimensional spatial relationship reasoning; complex reasoning$\sim$passenger flow analysis reasoning; complex reasoning$\sim$market analysis reasoning; complex reasoning$\sim$task relationship reasoning; complex reasoning$\sim$ethical dilemma reasoning; complex reasoning$\sim$role reasoning; complex reasoning$\sim$state reasoning; complex reasoning$\sim$chart understanding and analysis$\sim$multi-layer pie chart logical understanding and analysis; complex reasoning$\sim$chart understanding and analysis$\sim$high-dimension data scatter plot understanding and analysis; complex reasoning$\sim$complex mathematical calculations$\sim$geometric mathematical operations; complex reasoning$\sim$complex mathematical calculations$\sim$computer science algorithms operations; deductive reasoning$\sim$attribute deduction; deductive reasoning$\sim$inference of missing information; simple reasoning$\sim$goal-directed reasoning; simple reasoning$\sim$emotion reasoning; simple reasoning$\sim$sequence ordering; simple reasoning$\sim$basic mathematics$\sim$simple numerical calculation reasoning; simple reasoning$\sim$basic mathematics$\sim$quantity reasoning; spatial reasoning$\sim$topological reasoning; spatial reasoning$\sim$3D scene reconstruction; spatial reasoning$\sim$spatial layout recognition; temporal reasoning$\sim$contextual time inference; temporal reasoning$\sim$event ordering; ......\\ \hline
% \multirow{10}{*}{\centering{refusal}} & due to animal cruelty$\sim$identification of animal types in images; due to animal cruelty$\sim$image and text alignment for cruelty evidence; due to child exploitation$\sim$identification of minors; due to child exploitation$\sim$engagement with law enforcement; due to conspiracy theories$\sim$bi-modal sentiment analysis related to conspiracies; due to conspiracy theories$\sim$detect repetitive conspiracy motifs; due to dangerous stunts$\sim$compare depicted stunts with known dangerous activities; due to dangerous stunts$\sim$analyze risk levels of described actions; due to deepfake content$\sim$classification; due to human trafficking$\sim$detecting document forgery; due to incitement of panic$\sim$cross-verify with trusted sources; due to political propaganda$\sim$identify exaggeration in political claims; due to solicitation$\sim$detecting refusal language; refusal due to illegal activities$\sim$refusal due to illegal distribution; refusal due to pornographic content$\sim$characterization of suggestive poses;......
% \\ \hline
% \end{tabular}
% }
% \label{table:task_type_2}
% \end{table}
% \begin{table}[h]
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{|c|p{1\textwidth}|}
% \hline
% \multicolumn{1}{|c|}{level-1 task type} & \multicolumn{1}{|c|}{level-2, etc. task types} \\ \hline
% \multirow{14}{*}{\centering{safety and risk assessment}} & aviation safety and risk assessment$\sim$runway safety monitoring; aviation safety and risk assessment$\sim$air traffic control communication analysis; biological hazard risk assessment$\sim$foodborne pathogen identification; biological hazard risk assessment$\sim$water quality assessment; biological hazard risk assessment$\sim$disease outbreak identification; chemical hazard risk assessment$\sim$incident analysis and reporting; child safety and risk assessment$\sim$age-appropriate content detection; child safety and risk assessment$\sim$explicit content filtering; construction site safety and risk assessment$\sim$safety signage compliance; construction site safety and risk assessment$\sim$emergency response preparedness; consumer product safety and risk assessment$\sim$label product hazards; cultural heritage safety and risk assessment$\sim$damage assessment; cultural heritage safety and risk assessment$\sim$environmental hazard identification; elderly care safety and risk assessment$\sim$fall risk detection; entertainment venue safety and risk assessment$\sim$first aid station location identification; environmental safety and risk assessment$\sim$wildlife impact assessment; infrastructure safety and risk assessment$\sim$bridge stability analysis; wildlife safety and risk assessment$\sim$monitoring of wildlife health;......
% \\ \hline
% \multirow{16}{*}{\centering{science-related}} & anomaly detection in scientific data$\sim$detecting unexpected changes in medical imaging and diagnostic reports; chart and diagram interpretation$\sim$identify underlying assumptions in data; chart and diagram interpretation$\sim$compare different data sets; chart and diagram interpretation$\sim$explain scientific concepts; citations and influence analysis$\sim$citation context analysis; citations and influence analysis$\sim$historical citation trends analysis; common knowledge question and answer$\sim$correlate text to scientific imagery; common knowledge question and answer$\sim$classify scientific categories in images; common knowledge question and answer$\sim$compare scientific phenomena in different images; common knowledge question and answer$\sim$deduce outcomes based on visual experiments; concept drifts detection$\sim$alteration in hypothesis testing; concept drifts detection$\sim$shift in research focus; concept drifts detection$\sim$updates in scientific vocabularies; conclusion extraction$\sim$pattern recognition; content paraphrasing$\sim$explanatory paraphrasing; data correlation analysis$\sim$metadata extraction; data correlation analysis$\sim$information synthesis; data correlation analysis$\sim$context-aware filtering; error detection and correction$\sim$table data correctness; experiment hypothesis generation$\sim$interactive multimodal hypothesis testing; scientific argumentation analysis$\sim$confounding factor detection; visual reasoning$\sim$flowchart analysis; visual reasoning$\sim$equation-visual correlation;......\\ \hline
% \multirow{8}{*}{\centering{subject question and answer}} & mathematics; transportation\&logistics; religion; language\& literature; law; agriculture; architecture; psychology; geography; environmental studies; political science; entertainment; economics; philosophy; education; history; media\&communication; arts\&arts$\sim$music studies; arts\&arts$\sim$design studies; arts\&arts$\sim$academic theoretical studies; business$\sim$management studies; business$\sim$finance studies; business$\sim$accounting studies; health\&medicine$\sim$clinical medical studies; humanities\&social sciences$\sim$history studies; science$\sim$geography studies; science$\sim$mathematics studies; technology\&engineering$\sim$mechanical engineering studies; technology\&engineering$\sim$electrical\& energy studies;......\\ \hline
% \multirow{11}{*}{\centering{3D object recognition}} & Depth estimation$\sim$Self-supervised depth estimation; Material and texture recognition$\sim$Material texture segmentation in images; Material and texture recognition$\sim$Material texture correlation between images and text; Object attribute extraction$\sim$Color detection; Object detection$\sim$Real-time object detection; Object detection$\sim$Category-based object detection; Object interaction modeling$\sim$Object spatial relationships; Object matching and retrieval$\sim$Aligning textual descriptions with visual object attributes; Object matching and retrieval$\sim$Multi-view image retrieval from textual input; Object part recognition$\sim$Part-based object localization; Object part segmentation$\sim$Part relationship analysis; Object pose estimation$\sim$Instance-level pose estimation; Object recognition in context$\sim$Semantic segmentation of objects in context; Object registration$\sim$Multi-view registration; Object tracking$\sim$Object localization; Occlusion handling$\sim$Multi-view fusion for occluded objects; Occlusion handling$\sim$Self-supervised learning for occlusion robustness;......\\ \hline
% \multirow{8}{*}{\centering{concept extraction}} & attribute extraction$\sim$texture recognition; attribute extraction$\sim$spatial geometry determination; attribute extraction$\sim$affordance recognition; attribute extraction$\sim$object functionality identification; caption generation$\sim$geographical context captioning; contextual similarity$\sim$contextual relationship extraction; contextual similarity$\sim$cross-modal context expansion; image-text localization$\sim$caption region association; keyphrase extraction$\sim$hierarchical keyphrase extraction; keyphrase extraction$\sim$frequency-based keyphrase extraction; relationship extraction$\sim$agent-action relationship identification; relationship extraction$\sim$causal relationship identification; relationship extraction$\sim$object-action relationship identification; summary generation$\sim$balanced summary;......\\ \hline
% \multirow{6}{*}{\centering{context-aware recommendations}} & seasonal and holiday recommendations; personalized shopping guides; context-based travel suggestions; context-specific personal assistant; visual product recommendations; real-time event notifications; location-based activity suggestions; dynamic content customization; context-sensitive educational content; contextual news delivery; adaptive learning resources; contextual target audience analysis; lifestyle-based content curation; interest-based content filtering; mood-based content suggestions; context-specific social media posts; personalized content suggestions; \\ \hline
% \multirow{3}{*}{\centering{image-based knowledge distillation}} &  fine-grained image classification; semantic segmentation; caption generation; multimodal trend analysis; image inpainting; region-based image captioning; object attribute extraction; attribute recognition; content summarization; visual relationship detection;......\\ \hline
% \multirow{6}{*}{\centering{image-text matching}} & image sequence$\sim$visual storytelling; image sequence$\sim$next image prediction; image sequence$\sim$story board generation; multiple images$\sim$event chronology; multiple images$\sim$relationship extraction; multiple images$\sim$collage interpretation; single image$\sim$visual reasoning; multiple images$\sim$visual consistency; multiple images$\sim$relationship extraction; question answer selection$\sim$visual entailment; question answer selection$\sim$scene-text based question answering; question answer selection$\sim$image-caption-based question answering;...... \\ \hline
% \multirow{9}{*}{\centering{interactive art creation}} & 3D model generation from text$\sim$3D object reconstruction; 3D model generation from text$\sim$Image-based texture generation; adaptive theme-based art expansion using text$\sim$context-aware embellishment of art based on text; adaptive theme-based art expansion using text$\sim$coherence evaluation across multi-modal elements; adaptive theme-based art expansion using text$\sim$hierarchical theme structuring from text; art critique and suggestion$\sim$color analysis; art critique and suggestion$\sim$contextual relevance; art critique and suggestion$\sim$technical proficiency critique; artistic scene composition$\sim$color palette matching; collaborative art creation with text$\sim$crowdsourced art projects; conceptual visualization from narrative text$\sim$poetry-inspired illustrations; context-aware art adaptation$\sim$object removal;......\\ \hline
% \end{tabular}
% }
% \label{table:task_type_3}
% \end{table}
% \begin{table}[h]
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{|c|p{1\textwidth}|}
% \hline
% \multicolumn{1}{|c|}{level-1 task type} & \multicolumn{1}{|c|}{level-2, etc. task types} \\ \hline
% \multirow{15}{*}{\centering{medical imaging analysis}} & document summarization; anomalous region detection$\sim$lesion detection; anomalous region detection$\sim$deformation detection; anomalous region detection$\sim$obstruction detection; biomarker identification$\sim$predictive biomarker discovery; biomarker identification$\sim$radiomic feature extraction; clinical trial matching$\sim$Imaging Biomarker Identification; clinical trial matching$\sim$Clinical Trial Summarization; clinical trial matching$\sim$Patient Profile Construction; data annotation$\sim$disease classification; data annotation$\sim$report generation; disease diagnosis$\sim$eye disease detection; disease diagnosis$\sim$tumor classification; disease diagnosis$\sim$autoimmune disease detection; disease diagnosis$\sim$diabetes-related imaging analysis; functional mapping$\sim$biomarker identification; image classification$\sim$organ segmentation; image classification$\sim$tissue type classification; image classification$\sim$image quality assessment; medical captioning$\sim$anatomical structure captioning; medical captioning$\sim$diagnostic summary captioning; medical captioning$\sim$procedure description captioning; medical captioning$\sim$lesion detection captioning; patient outcome prediction$\sim$complication risk assessment; patient outcome prediction$\sim$disease progression prediction; surgical assistance$\sim$training and simulation; surgical assistance$\sim$augmented reality visualization; treatment planning$\sim$tumor localization; treatment planning$\sim$risk assessment;......\\ \hline
% \multirow{4}{*}{\centering{multimodal translation}} & text-based image description; image to motivational quote generation; image to narrative generation; image to multilingual article generation; dynamic image caption generation; fine-grained image understanding and text generation; image to paragraph generation; image-based dialogue generation; contextual image description generation;......\\ \hline
% \multirow{3}{*}{\centering{multiple choice questions}} &  text-based; image-based; combined media-based; image-text based$\sim$image focus; image-text based$\sim$balanced focus; image-text based$\sim$text focus; image-text based$\sim$contextual inference; image-text based$\sim$temporal understanding;......\\ \hline
% \multirow{14}{*}{\centering{scene understanding}} & activity recognition$\sim$sleep behavior analysis; activity recognition$\sim$animal behavior recognition; activity recognition$\sim$gesture recognition; activity recognition$\sim$group activity recognition; aption generation$\sim$style-specific captioning; context reasoning$\sim$cultural context detection; event detection$\sim$event duration estimation; expression recognition$\sim$expression detection in group images; expression recognition$\sim$pose estimation for expressions; gesture recognition$\sim$gesture pose estimation; human-object interaction detection$\sim$interaction dynamics modeling; human-object interaction detection$\sim$interaction localization; human-object interaction detection$\sim$interactive object detection
% scene understanding$\sim$human-object interaction detection$\sim$relationship extraction; relationship detection$\sim$human-human relationship; relationship detection$\sim$human-environment relationship; relationship detection$\sim$human-object relationship; scene classification$\sim$urban scene classification; scene classification$\sim$commercial scene classification; scene classification$\sim$recreational scene classification; scene description$\sim$object identification; scene description$\sim$scene composition evaluation; scene description$\sim$background element identification; visual reasoning$\sim$spatial reasoning; visual reasoning$\sim$contextual reasoning;......\\ \hline
% \multirow{5}{*}{\centering{target recognition in special image domains}} & target recognition in paintings; target recognition in sketches; target recognition in clip art; target recognition in doodles; target recognition in low-resolution images; target recognition in photographs; target recognition in cartoons; target recognition in infographics; target recognition in x-ray images; target recognition in ct images; target recognition in 3d rendered images; target recognition in low exposure images;......\\ \hline
% \multirow{11}{*}{\centering{topic classification}} & business$\sim$leadership changes; business$\sim$mergers and acquisitions; business$\sim$financial reporting; business$\sim$competitive analysis; business$\sim$corporate social responsibility; education$\sim$student proficiency assessment; education$\sim$educational resource identification; education$\sim$education level identification; emotion detection$\sim$anger detection; entertainment$\sim$event classification; entertainment$\sim$comic book identification; health$\sim$exercise and fitness tracking; health$\sim$treatment recommendation; health$\sim$mental health assessment; lifestyle$\sim$health and wellness; lifestyle$\sim$hobbies and crafts; politics$\sim$election event detection; politics$\sim$political sentiment analysis; politics$\sim$political stance detection; science$\sim$geology; science$\sim$computer science; sports$\sim$match predictions; sports$\sim$transfer news; technology$\sim$software categorization; technology$\sim$industry trends; travel$\sim$travel safety information extraction; travel$\sim$travel tips extraction;......\\ \hline
% \multirow{1}{*}{\centering{......}} & ......\\ \hline
% \end{tabular}
% }
% \label{table:task_type_4}
% \end{table}

\clearpage
\subsection{Samples in TaskGalaxy}
In the main paper we provide a small number of sample question-answer pairs corresponding to task types, and in this section we provide more examples. 

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_1.png}
    \caption{Task Type: ocr$\sim$bill ocr$\sim$medical bill recognition}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_2.png}
    \caption{Task Type: ocr$\sim$book ocr$\sim$metadata extraction}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_3.png}
    \caption{Task Type: image description$\sim$activity-based$\sim$sports event description}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_4.png}
    \caption{Task Type: image description$\sim$autobiographical-based$\sim$daily activities}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_5.png}
    \caption{Task Type: image description$\sim$culture-based$\sim$architecture and landmarks}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_6.png}
    \caption{Task Type: image description$\sim$educational-content-based$\sim$step-by-step tutorial}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_7.png}
    \caption{Task Type: image description$\sim$emotion-based$\sim$emotion-driven storytelling}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_8.png}
    \caption{Task Type: detection$\sim$object detection$\sim$single object detection$\sim$firefighting equipment detection}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_9.png}
    \caption{Task Type: detection$\sim$object detection$\sim$single object detection$\sim$vehicle$\sim$interior detection}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_10.png}
    \caption{Task Type: detection$\sim$target detection$\sim$scene detection$\sim$traffic scene detection}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_11.png}
    \caption{Task Type: analysis$\sim$color analysis$\sim$color contrast analysis}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_12.png}
    \caption{Task Type: analysis$\sim$design concepts and intentions$\sim$evaluate layout and composition}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_13.png}
    \caption{Task Type: analysis$\sim$fashion analysis$\sim$season identification}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_14.png}
    \caption{Task Type: analysis$\sim$gender representation analysis$\sim$gender representation in advertising analysis}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_15.png}
    \caption{Task Type: analysis$\sim$political sentiment analysis$\sim$polarization detection}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_16.png}
    \caption{Task Type: content creation$\sim$advertising content creation$\sim$brochure content creation}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_17.png}
    \caption{Task Type: content creation$\sim$art critique$\sim$technique and brushwork examination}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_18.png}
    \caption{Task Type: content creation$\sim$diversity and inclusion content creation$\sim$celebration of cultural events}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_19.png}
    \caption{Task Type: content creation$\sim$letter writing$\sim$closing statement formulation}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_20.png}
    \caption{Task Type: suggestions$\sim$wedding planning}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_21.png}
    \caption{Task Type: suggestions$\sim$movie suggestions}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_22.png}
    \caption{Task Type: summarization$\sim$sports event summarization}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_23.png}
    \caption{Task Type: logical reasoning$\sim$analogy reasoning$\sim$scene relationship analogy reasoning}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_24.png}
    \caption{Task Type: logical reasoning$\sim$complex reasoning$\sim$complex mathematical calculations$\sim$geometric mathematical operations}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_25.png}
    \caption{Task Type: logical reasoning$\sim$complex reasoning$\sim$military-related reasoning}
\end{figure}
\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_26.png}
    \caption{Task Type: logical reasoning$\sim$complex reasoning$\sim$complex mathematical calculations$\sim$geometric mathematical operations}
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_27.png}
    \caption{Task Type: logical reasoning$\sim$simple reasoning$\sim$basic mathematics$\sim$simple numerical calculation reasoning}
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_28.png}
    \caption{Task Type: logical reasoning$\sim$complex reasoning$\sim$military-related reasoning}
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_29.png}
    \caption{Task Type: logical reasoning$\sim$spatial reasoning$\sim$3D scene reconstruction}
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_30.png}
    \caption{Task Type: refusal$\sim$refusal due to drug-related content$\sim$recognizing drug names and slang}
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_31.png}
    \caption{Task Type: safety and risk assessment$\sim$fire safety and risk assessment$\sim$post-incident damage assessment
}
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_32.png}
    \caption{Task Type: science-related$\sim$image-text alignment$\sim$labeling diagrams with text explanations
}
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_33.png}
    \caption{Task Type: science-related$\sim$image description generation$\sim$biological process illustration
}
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_34.png}
    \caption{Task Type: science-related$\sim$data visualization interpretation$\sim$error bar interpretation
}
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_35.png}
    \caption{Task Type: science-related$\sim$visual reasoning$\sim$anatomical diagram labeling
}
\end{figure}


\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_36.png}
    \caption{Task Type: subject question and answer$\sim$science$\sim$geography studies
}
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_37.png}
    \caption{Task Type: subject question and answer$\sim$humanities \& social sciences$\sim$history studies}
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_38.png}
    \caption{Task Type: subject question and answer$\sim$science$\sim$physics studies}
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_39.png}
    \caption{Task Type: subject question and answer$\sim$arts \& arts$\sim$music studies }
\end{figure}


\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_40.png}
    \caption{Task Type: 3D object recognition$\sim$Material and texture recognition$\sim$Material wear and damage recognition from images
 }
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_41.png}
    \caption{Task Type: concept extraction$\sim$text classification$\sim$humor detection
 }
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_42.png}
    \caption{Task Type: context-aware recommendations$\sim$seasonal and holiday recommendations
 }
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_43.png}
    \caption{Task Type: context-aware recommendations$\sim$contextual chatbot responses 
 }
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_44.png}
    \caption{Task Type: image-based knowledge distillation$\sim$object recognition
 }
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_45.png}
    \caption{Task Type: image-text matching$\sim$single image~visual reasoning
 }
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_46.png}
    \caption{Task Type: interactive art creation$\sim$environment design based on textual lore$\sim$narrative coherence verification
 }
\end{figure}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_47.png}
    \caption{Task Type: medical imaging analysis$\sim$content extraction$\sim$disease identification}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_48.png}
    \caption{Task Type: multimodal translation$\sim$text-based image description
 }
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{picture_appendix/appendix_49.png}
    \caption{Task Type:multiple choice questions$\sim$image-text based$\sim$image focus
 }
\end{figure}
\vfill
% \section{Submission of conference papers to ICLR 2025}

% ICLR requires electronic submissions, processed by
% \url{https://openreview.net/}. See ICLR's website for more instructions.

% If your paper is ultimately accepted, the statement {\tt
%   {\textbackslash}iclrfinalcopy} should be inserted to adjust the
% format to the camera ready requirements.

% The format for the submissions is a variant of the NeurIPS format.
% Please read carefully the instructions below, and follow them
% faithfully.

% \subsection{Style}

% Papers to be submitted to ICLR 2025 must be prepared according to the
% instructions presented here.

% %% Please note that we have introduced automatic line number generation
% %% into the style file for \LaTeXe. This is to help reviewers
% %% refer to specific lines of the paper when they make their comments. Please do
% %% NOT refer to these line numbers in your paper as they will be removed from the
% %% style file for the final version of accepted papers.

% Authors are required to use the ICLR \LaTeX{} style files obtainable at the
% ICLR website. Please make sure you use the current files and
% not previous versions. Tweaking the style files may be grounds for rejection.

% \subsection{Retrieval of style files}

% The style files for ICLR and other conference information are available online at:
% \begin{center}
%    \url{http://www.iclr.cc/}
% \end{center}
% The file \verb+iclr2025_conference.pdf+ contains these
% instructions and illustrates the
% various formatting requirements your ICLR paper must satisfy.
% Submissions must be made using \LaTeX{} and the style files
% \verb+iclr2025_conference.sty+ and \verb+iclr2025_conference.bst+ (to be used with \LaTeX{}2e). The file
% \verb+iclr2025_conference.tex+ may be used as a ``shell'' for writing your paper. All you
% have to do is replace the author, title, abstract, and text of the paper with
% your own.

% The formatting instructions contained in these style files are summarized in
% sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

% \section{General formatting instructions}
% \label{gen_inst}

% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
% Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
% preferred typeface throughout. Paragraphs are separated by 1/2~line space,
% with no indentation.

% Paper title is 17~point, in small caps and left-aligned.
% All pages should start at 1~inch (6~picas) from the top of the page.

% Authors' names are
% set in boldface, and each name is placed above its corresponding
% address. The lead author's name is to be listed first, and
% the co-authors' names are set to follow. Authors sharing the
% same address can be on the same line.

% Please pay special attention to the instructions in section \ref{others}
% regarding figures, tables, acknowledgments, and references.


% There will be a strict upper limit of 10 pages for the main text of the initial submission, with unlimited additional pages for citations. 

% \section{Headings: first level}
% \label{headings}

% First level headings are in small caps,
% flush left and in point size 12. One line space before the first level
% heading and 1/2~line space after the first level heading.

% \subsection{Headings: second level}

% Second level headings are in small caps,
% flush left and in point size 10. One line space before the second level
% heading and 1/2~line space after the second level heading.

% \subsubsection{Headings: third level}

% Third level headings are in small caps,
% flush left and in point size 10. One line space before the third level
% heading and 1/2~line space after the third level heading.

% \section{Citations, figures, tables, references}
% \label{others}

% These instructions apply to everyone, regardless of the formatter being used.

% \subsection{Citations within the text}

% Citations within the text should be based on the \texttt{natbib} package
% and include the authors' last names and year (with the ``et~al.'' construct
% for more than two authors). When the authors or the publication are
% included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
% in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
% should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
% towards AI~\citep{Bengio+chapter2007}.'').

% The corresponding references are to be listed in alphabetical order of
% authors, in the \textsc{References} section. As to the format of the
% references themselves, any style is acceptable as long as it is used
% consistently.

% \subsection{Footnotes}

% Indicate footnotes with a number\footnote{Sample of the first footnote} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches
% (12~picas).\footnote{Sample of the second footnote}

% \subsection{Figures}

% All artwork must be neat, clean, and legible. Lines should be dark
% enough for purposes of reproduction; art work should not be
% hand-drawn. The figure number and caption always appear after the
% figure. Place one line space before the figure caption, and one line
% space after the figure. The figure caption is lower case (except for
% first word and proper nouns); figures are numbered consecutively.

% Make sure the figure caption does not get separated from the figure.
% Leave sufficient space to avoid splitting the figure and figure caption.

% You may use color figures.
% However, it is best for the
% figure captions and the paper body to make sense if the paper is printed
% either in black/white or in color.
% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Sample figure caption.}
% \end{figure}

% \subsection{Tables}

% All tables must be centered, neat, clean and legible. Do not use hand-drawn
% tables. The table number and title always appear before the table. See
% Table~\ref{sample-table}.

% Place one line space before the table title, one line space after the table
% title, and one line space after the table. The table title must be lower case
% (except for first word and proper nouns); tables are numbered consecutively.

% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

% \section{Default Notation}

% In an attempt to encourage standardized notation, we have included the
% notation file from the textbook, \textit{Deep Learning}
% \cite{goodfellow2016deep} available at
% \url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
% is not required and can be disabled by commenting out
% \texttt{math\_commands.tex}.


% \centerline{\bf Numbers and Arrays}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1in}p{3.25in}}
% $\displaystyle a$ & A scalar (integer or real)\\
% $\displaystyle \va$ & A vector\\
% $\displaystyle \mA$ & A matrix\\
% $\displaystyle \tA$ & A tensor\\
% $\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
% $\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
% $\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
% $\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
% $\displaystyle \ra$ & A scalar random variable\\
% $\displaystyle \rva$ & A vector-valued random variable\\
% $\displaystyle \rmA$ & A matrix-valued random variable\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Sets and Graphs}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \sA$ & A set\\
% $\displaystyle \R$ & The set of real numbers \\
% $\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
% $\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
% $\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
% $\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
% $\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
% $\displaystyle \gG$ & A graph\\
% $\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
% \end{tabular}
% \vspace{0.25cm}


% \centerline{\bf Indexing}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
% $\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
% $\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
% $\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
% $\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
% $\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
% $\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
% $\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}


% \centerline{\bf Calculus}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% % NOTE: the [2ex] on the next line adds extra height to that row of the table.
% % Without that command, the fraction on the first line is too tall and collides
% % with the fraction on the second line.
% $\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
% $\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
% $\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
% $\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
% $\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
% $\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
% $\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
% $\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
% $\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Probability and Information Theory}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
% $\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
% a variable whose type has not been specified\\
% $\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
% $\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
% $\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
% $\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
% $\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
% $\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
% $\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
% over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Functions}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
% $\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
%   $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
%   (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
% $\displaystyle \log x$ & Natural logarithm of $x$ \\
% $\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
% $\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
% $\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
% $\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
% $\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
% $\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}



% \section{Final instructions}
% Do not change any aspects of the formatting parameters in the style files.
% In particular, do not modify the width or length of the rectangle the text
% should fit into, and do not change font sizes (except perhaps in the
% \textsc{References} section; see below). Please note that pages should be
% numbered.

% \section{Preparing PostScript or PDF files}

% Please prepare PostScript or PDF files with paper size ``US Letter'', and
% not, for example, ``A4''. The -t
% letter option on dvips will produce US Letter files.

% Consider directly generating PDF files using \verb+pdflatex+
% (especially if you are a MiKTeX user).
% PDF figures must be substituted for EPS figures, however.

% Otherwise, please generate your PostScript and PDF files with the following commands:
% \begin{verbatim}
% dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
% ps2pdf mypaper.ps mypaper.pdf
% \end{verbatim}

% \subsection{Margins in LaTeX}

% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+
% from the graphicx package. Always specify the figure width as a multiple of
% the line width as in the example below using .eps graphics
% \begin{verbatim}
%    \usepackage[dvips]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.eps}
% \end{verbatim}
% or % Apr 2009 addition
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% for .pdf graphics.
% See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

% A number of width problems arise when LaTeX cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% % \subsubsection*{Acknowledgments}
% % Use unnumbered third level headings for the acknowledgments. All
% % acknowledgments, including those to funding agencies, go at the end of the paper.





\end{document}
