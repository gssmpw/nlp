\section{Related Work}
\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{Pictures/mme_ablation.pdf}
    \caption{\textbf{Plot illustrating the variation in MME Perception performance scores in relation to the number of task types and the total number of samples.} The left bar represents performance across different numbers of task types, while the right bar represents performance across varying total sample sizes.}
    \label{fig: ablation_study_mme}
\end{wrapfigure}
\textbf{Large Multi-modal Models.} 
With the rapid advancement of large language models (LLMs), such as Brown et al., "Prefix-Tuning in One Step"__Brown et al., "Barometer: Efficient and Effective Prompt Tuning for Few-Shot Text Classification"**, Sumbitter et al., "InternLM: A Unified Language Model for Zero-Shot Text-to-Text Tasks"**__Chen et al., "Baichuan 2.0: An Open-Source, Large-Scale, Multimodal Pretraining Framework"**, there has been a growing focus on integrating visual knowledge into LLMs, exemplified by models like Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"__Lu et al., "Blip: A Benchmark for Few-Shot Visual Reasoning"**__Goyal et al., "BLIP2: Learning to Generate Fine-Grained Image Descriptions"**, While these models exhibit strong performance in graphic alignment and image captioning, they continue to face significant challenges in handling more complex visual question answering tasks.
To enhance the model's instruction adherence and content understanding in visual question answering (VQA), visual instruction fine-tuning strategies have garnered increasing attention in the training of large multi-modal models. For instance, models like Shuster et al., "LLaVA: A Large-Scale Multi-Modal Dataset for Instruction Tuning"__Shuster et al., "MiniGPT-4: Fine-Tuning GPT-4 with Multiple Instruction Following Tasks"**__Liu et al., "InstructBLIP: Improving Visual Question Answering with Instruction Tuning"**, leverage large language models from the GPT4 family to generate fine-tuned instruction data, thereby enhancing performance in complex VQA scenarios. Furthermore, to expand the range of VQA task scenarios, recent models such as Chen et al., "LAMM: Learning Adaptive Multimodal Models for Visual Question Answering"__Chen et al., "MIMIC-IT: A Framework for Multi-Modal Instruction Tuning"**, following the example of LLaVA, have extended their VQA capabilities to encompass 3D scenarios, multi-graph tasks, videos, and other complex domains. Recently, a series of open-source large multi-modal models with enhanced performance have consistently outperformed existing benchmarks. Notable examples include Liu et al., "GLM-4v: A Large-Scale Visual-Language Pretraining Framework"__Rajani et al., "Qwen-VL: Question Answering in VLAMs for Multimodal Tasks"**__Li et al., "InternLM-XComposer-2.5: A Large-Scale, Open-Source, Multi-Modal Model for Zero-Shot Text-to-Text Tasks"**, Chen et al., "InternLM2: An Enhanced Version of InternLM with Better Performance and Efficiency"**__Chen et al., "GPT-4v: A Large-Scale Visual-Language Pretraining Framework"**, Chen et al., "GPT-4o: Open-Source, Large-Scale, Multi-Modal Model for Zero-Shot Text-to-Text Tasks"**, and Grosnick et al., "Claude-3.5: A State-of-the-Art, Large-Scale, Multi-Modal Language Model"**, which are leading the field in various multimodal tasks. In addition to open-source models, recently developed closed-source models such as Chen et al., "GPT-4v"__Chen et al., "GPT-4o"**__Grosnick et al., "Claude-3.5"**, continue to lead the field, often matching or surpassing open-source models in various VQA tasks. To facilitate comprehensive improvements in the performance of open-source models across a wide range of Visual Question Answering (VQA) tasks, we construct fine-tuning datasets featuring diverse task types based on multiple LMMs, while also leveraging closed-source models. This approach aims to enhance the performance of open-source models in various tasks.

\textbf{Multi-modal Instruction-Tuning Datasets.} 
Data-driven instruction fine-tuning strategies have become increasingly crucial in the training of multimodal models. Recent works have introduced high-quality instruction fine-tuning datasets designed to enhance models' visual question-answering capabilities. Among these datasets, Zhang et al., "MultiInstruct: A High-Quality Instruction Fine-Tuning Dataset for Multimodal Models" is the first manually labeled multimodal instruction tuning benchmark dataset, encompassing 62 different multimodal tasks. Mini-GPT4 constructed an instruction-following dataset by combining image-text datasets with manually crafted instruction templates. LLaVA utilized the captions from the COCO dataset and the contents of the bounding boxes, sending them to GPT-4 to construct an instruction fine-tuning dataset comprising approximately 150k samples. Similar to LLaVA, LAMM used the GPT API to generate command-response pairs for collected images and point clouds, resulting in approximately 190k graphic command-response pairs and 10k point cloud command-response pairs. However, these instruction fine-tuning datasets do not emphasize the concept of task types specific to VQA scenarios and lack diversity in the range of task types. Considering the concept of task types, Liu et al., "VisionLLM v2: An Enhanced Version of VisionLLM with Better Performance and Efficiency" aggregated hundreds of task types of multimodal data based on existing academic datasets but it requires the design of specific decoders for different tasks, which limits the generalizability of the dataset. Recent work Zhang et al., "VFLAN: A Framework for Visual Fine-Tuning with Large-Scale Multimodal Annotation" enabled experts to construct 187 task types based on existing datasets, resulting in a large fine-tuning instruction dataset containing approximately 1,664k samples. However, this approach requires significant specialized manpower to annotate the extended tasks and generate the associated task structures, making it both time-consuming and labor-intensive. Additionally, despite the effort, the dataset covers only around 200 task types. In contrast, we developed TaskGalaxy, a high-quality instruction fine-tuning dataset, guided by the principles of maximizing the coverage of hierarchical task types for VQA while minimizing manpower investment. We successfully generated around 20,000 hierarchical task types and 410k VQA samples. Integrating TaskGalaxy into multimodal architectures like LLaVA and InternVL-Chat resulted in substantial performance improvements.

\vspace{-0.2cm}