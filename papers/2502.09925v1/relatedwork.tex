\section{Related Work}
\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{Pictures/mme_ablation.pdf}
    \caption{\textbf{Plot illustrating the variation in MME Perception performance scores in relation to the number of task types and the total number of samples.} The left bar represents performance across different numbers of task types, while the right bar represents performance across varying total sample sizes.}
    \label{fig: ablation_study_mme}
\end{wrapfigure}
\textbf{Large Multi-modal Models.} 
With the rapid advancement of large language models (LLMs), such as GPT-3~\citep{gpt3}, LLama2~\citep{llama2}, InternLM~\citep{internlm}, and Baichuan 2~\citep{baichuan2}, there has been a growing focus on integrating visual knowledge into LLMs, exemplified by models like CLIP~\citep{clipvit}, BLIP~\citep{blip}, and BLIP2~\citep{blip2}. While these models exhibit strong performance in graphic alignment and image captioning, they continue to face significant challenges in handling more complex visual question answering tasks.
To enhance the model's instruction adherence and content understanding in visual question answering (VQA), visual instruction fine-tuning strategies have garnered increasing attention in the training of large multi-modal models. For instance, models like LLaVA~\citep{llava}, MiniGPT-4~\citep{minigpt}, and InstructBLIP~\citep{instructblip} leverage large language models from the GPT4~\citep{gpt4} family to generate fine-tuned instruction data, thereby enhancing performance in complex VQA scenarios. Furthermore, to expand the range of VQA task scenarios, recent models such as LAMM~\citep{lamm} and MIMIC-IT~\citep{mimic}, following the example of LLaVA, have extended their VQA capabilities to encompass 3D scenarios, multi-graph tasks, videos, and other complex domains. Recently, a series of open-source large multi-modal models  with enhanced performance have consistently outperformed existing benchmarks. Notable examples include GLM-4v~\citep{glm4v}, Qwen-VL~\citep{qwenvl}, InternLM-XComposer-2.5~\citep{internlm-xcomposer-2.5}, and InternLM2~\citep{internlm2_5}, which are leading the field in various multimodal tasks. In addition to open-source models, recently developed closed-source models such as GPT-4v~\citep{gpt4v}, GPT-4o~\citep{hurst2024gpt}, and Claude-3.5~\citep{claude3.5} continue to lead the field, often matching or surpassing open-source models in various VQA tasks. To facilitate comprehensive improvements in the performance of open-source models across a wide range of Visual Question Answering (VQA) tasks, we construct fine-tuning datasets featuring diverse task types based on multiple LMMs, while also leveraging closed-source models. This approach aims to enhance the performance of open-source models in various tasks.

\textbf{Multi-modal Instruction-Tuning Datasets.} 
Data-driven instruction fine-tuning strategies have become increasingly crucial in the training of multimodal models. Recent works have introduced high-quality instruction fine-tuning datasets designed to enhance models' visual question-answering capabilities. Among these datasets, MultiInstruct~\citep{multiinstruct} is the first manually labeled multimodal instruction tuning benchmark dataset, encompassing 62 different multimodal tasks. Mini-GPT4~\citep{minigpt} constructs an instruction-following dataset by combining image-text datasets with manually crafted instruction templates. LLaVA~\citep{llava} utilized the captions from the COCO dataset and the contents of the bounding boxes, sending them to GPT-4 to construct an instruction fine-tuning dataset comprising approximately 150k samples. Similar to LLaVA, LAMM~\citep{lamm} used the GPT API to generate command-response pairs for collected images and point clouds, resulting in approximately 190k graphic command-response pairs and 10k point cloud command-response pairs. However, these instruction fine-tuning datasets do not emphasize the concept of task types specific to VQA scenarios and lack diversity in the range of task types. Considering the concept of task types, VisionLLM v2~\citep{visionllmv2} aggregated hundreds of task types of multimodal data based on existing academic datasets but it requires the design of specific decoders for different tasks, which limits the generalizability of the dataset. Recent work VFLAN~\citep{vision_flan} enabled experts to construct 187 task types based on existing datasets, resulting in a large fine-tuning instruction dataset containing approximately 1,664k samples. However, this approach requires significant specialized manpower to annotate the extended tasks and generate the associated task structures, making it both time-consuming and labor-intensive. Additionally, despite the effort, the dataset covers only around 200 task types. In contrast, we developed TaskGalaxy, a high-quality instruction fine-tuning dataset, guided by the principles of maximizing the coverage of hierarchical task types for VQA while minimizing manpower investment. We successfully generated around 20,000 hierarchical task types and 410k VQA samples. Integrating TaskGalaxy into multimodal architectures like LLaVA and InternVL-Chat resulted in substantial performance improvements.

\vspace{-0.2cm}