% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{subcaption}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage{xcolor}


\usepackage{amssymb}  % for \mathbb command
\usepackage{amsmath}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{float}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{booktabs} % For better looking tables
\usepackage{array} % For custom column alignmen
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{colortbl}
\usepackage[most]{tcolorbox}
% \newcommand{\veb}[1]{{\color{blue}#1}}

\newcommand{\veb}[1]{%
    \begin{tcolorbox}[
        colback=red,
        coltext=white,
        boxrule=0mm,
        arc=2mm,
        left=1mm,
        right=1mm,
        top=1mm,
        bottom=1mm,
        enhanced jigsaw,
        breakable
    ]
    #1
    \end{tcolorbox}%
}  
\newcommand{\ve}[1]{\textcolor{blue}{\textbf{#1}}}

\usepackage{comment}

% \title{Decoding the Code of Numbers in Language Models: \\ The Sublinear Mental Line}
\title{Number Representations in LLMs:\\ A Computational Parallel to Human Perception}
% \title{Language Models Encode Numbers Logarithmically}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
\begin{comment}
\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}
\end{comment}


\author{
  \normalsize \textbf{H. V. AlquBoj}$^{*}$ \hspace{5mm} \textbf{Hilal AlQuabeh}$^{*1}$ \hspace{5mm} \textbf{Velibor Bojkovic}$^{*1}$   \\ 
    \normalsize \textbf{Tatsuya Hiraoka}$^{1}$  \hspace{5mm} \textbf{Ahmed Oumar El-Shangiti}$^{1}$ \hspace{5mm} \textbf{Munachiso Nwadike}$^{1}$ 
    \\ \normalsize \textbf{Kentaro Inui}$^{1, 2, 3}$ \\[10pt]
  \normalsize $^{1}$ Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) \\
  \normalsize $^{2}$ Tohoku University, \hspace{5mm} $^{3}$ RIKEN \\
  \normalsize $^*$Amalgamation of first authors' names.
  %\\ \footnotesize{Emails: \texttt{\{hilal.alquabeh, velibor.bojkovic, tatsuya.hiraoka, ahmed.elshangiti, munachiso.nwadike, kentaro.inui\}@mbzuai.ac.ae}}
}





%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
% This paper explores the representation of numbers in large language models (LLMs) by investigating the presence of a sublinear ``mental number line.'' Using the language models, we analyze how numerical values are encoded across different layers by applying dimensionality reduction techniques such as PCA and PLS. Our findings reveal that the model's numerical representations exhibit a logarithmic-like structure, with increasing distances between values corresponding to their logarithmic scale. This suggests that LLMs, like humans, represent numbers on a sublinear scale. We also propose further experiments to refine our understanding of how tokenization influences these patterns.

Humans are believed to perceive numbers on a logarithmic mental number line, where smaller values are represented with greater resolution than larger ones. This cognitive bias, supported by neuroscience and behavioral studies, suggests that numerical magnitudes are processed in a sublinear fashion rather than on a uniform linear scale. Inspired by this hypothesis, we investigate whether large language models (LLMs) exhibit a similar logarithmic-like structure in their internal numerical representations. By analyzing how numerical values are encoded across different layers of LLMs, we apply dimensionality reduction techniques such as PCA and PLS followed by geometric regression to uncover latent structures in the learned embeddings. Our findings reveal that the modelâ€™s numerical representations exhibit sublinear spacing, with distances between values aligning with a logarithmic scale. This suggests that LLMs, much like humans, may encode numbers in a compressed, non-uniform manner\footnote{Code is available at: \url{https://github.com/halquabeh/llm_natural_log}}\footnote{Correspondence: \{hilal.alquabeh, velibor.bojkovic, kentaro.inui\}@mbzuai.ac.ae}. 
\end{abstract}


% \begin{abstract}
% This paper investigates the representation of numbers in large language models (LLMs), focusing on the presence of a sublinear "mental number line." Using dimensionality reduction techniques such as PCA and PLS, we analyze how numerical values are encoded across different layers. Our findings reveal that numerical representations in LLMs exhibit a logarithmic-like structure, with distances between values corresponding to their logarithmic scale. This parallels human cognition, where numbers are represented on a sublinear scale. Additionally, we propose further experiments to explore the influence of tokenization on these patterns. These findings highlight the role of overlapping subspaces in numerical representation, suggesting a connection to superposition. By uncovering efficient encoding mechanisms, this work opens avenues for improving model compression techniques and understanding feature entanglement in neural networks.
% \end{abstract}

\section{Introduction}
\input{sections/new_intro} 
\section{Related Works}
\input{sections/related_work} 
\section{Methodology}
\input{sections/background}
\input{sections/proposal}

%\section{Results}
%\label{Results}









%\section{Analysis}
%\input{sections/analysis}

\section{Conclusion}
\input{sections/conclusion}
% \begin{itemize}
%     \item Superposition
%     \item Numbers frequency in training phases.
%     \item Tokenizaion affect.
% \end{itemize}
% \section{Limitations}
% \input{sections/limitations}

% \section{Ethical Statement}
% \input{sections/ethical}

% \newpage

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}
\newpage

\appendix

\input{sections/appendix}


\end{document}
