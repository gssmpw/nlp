\section{Experiment 1: Identifying number line using contextualized numbers}\label{sec exp 1}
\label{experiment1}
% \subsection{Setup}
\paragraph{Setup.} 
To systematically probe the model’s numerical representations, we partition numbers into logarithmically spaced groups:
\begin{equation}\label{eq groups G}
\begin{aligned}
    &G_1 = \{ 1, 2, \dots, 20\}, \\ &G_{i+1} = \{10^i - 19, \dots, 10^i + 20\}, \quad i \geq 2.
\end{aligned}
\end{equation}
These groups ensure that larger group indices correspond to numerical magnitudes that increase exponentially with index, reflecting the logarithmic nature of the mental number line hypothesis.

To analyze the embeddings of numbers, to every number \( x \in G_i \) we assign the following prompt:
\begin{equation}\label{eqn:prompt}
    x \leftarrow \text{\texttt{a=a, b=b, c=c, x=}}
\end{equation}

\noindent where \( \text{\texttt{a, b, and c}} \) are randomly generated numbers from the groups $G_i$. This prompt structure is designed to provide the model with contextual examples, encouraging it to invoke the number \( x \) in model's hidden states representations (see Figure \ref{fig pipeline}). Such approaches have been used in prior work to probe contextual representations in language models \citet{srivastava-etal-2024-nice}.

 The hidden state representation \( f(x) \in \mathbb{R}^d \) is extracted from a designated layer of the model from the last token in the prompt, e.g. the `=` token. We use only those $x$ for which the generated output of the model is $x$ itself. 





To ensure a representative sampling, we randomly select \( k \) numbers from each group \( G_i \). These sampled numbers collectively form a dataset, denoted as \( \mathcal{X} \), which serves as the basis for our analysis. The set of hidden state representations, \( \{f(x)\}_{x \in \mathcal{X}} \), is then aggregated and analyzed to investigate patterns and properties in the embedding space.


To control for potential biases introduced by tokenization (where larger numbers often span more tokens) we conduct a complementary experiment using non-numerical sequences. Instead of numerical inputs, we construct sequences of random letters with lengths corresponding to the tokenized representations of numbers. The letter sequences are grouped to their lengths so that the grouping approximately matches one of the numbers, and the prompts corresponding to specific letter sequences are designed in a similar fashion as for the numbers (\ref{eqn:prompt}). This setting allows us to compare any observed structural patterns between the number representations and letter representations. By doing so, we can determine whether the model truly encodes numerical magnitude or if it is simply responding to surface-level features of the input.

% ensures that any observed structural patterns are not merely artifacts of sequence length or token composition. By comparing the spatial organization of embeddings for numerical and non-numerical inputs, we can determine whether the model truly encodes numerical magnitude or if it is simply responding to surface-level features of the input.

% \subsection{Motivation}
\paragraph{Motivation.} 
The goal of this experiment is twofold. We first investigate whether LLMs encode numerical values in context along a \textit{monotonic number line} in their internal representation space. Second, we test whether this number line exhibits \textit{sublinear scaling}, similar to human cognitive representations of numbers.

% To control for potential biases introduced by tokenization (where larger numbers often span more tokens) we conduct a complementary experiment using non-numerical sequences. Instead of numerical inputs, we construct sequences of random letters with lengths matched to the tokenized representations of numbers. This ensures that any observed structural patterns are not merely artifacts of sequence length or token composition. By comparing the spatial organization of embeddings for numerical and non-numerical inputs, we can determine whether the model truly encodes numerical magnitude or if it is simply responding to surface-level features of the input.

% However, a tokenization challenge arises where larger groups often assign more tokens to the numbers they contain. To address this, we extended the study to include non-numerical data, such as random letters, with longer sequences assigned to larger groups. This allows us to investigate whether the observed patterns are specific to numerical data or if they also apply to other types of inputs, regardless of semantic differences.
% This approach provides a broader perspective on the robustness and generality of the encoded number line and its scaling behavior.
% \subsection{Methodology}
\paragraph{Methodology.} 
For these purposes, we use:
\vspace{0.5\baselineskip}
% \begin{itemize}
%     \item Monotonicity metric. We apply it on a sequence $x_1,\dots,x_n$ of all the numbers in the union of groups $G_j$ defined in \eqref{eq groups G}, and their respective projections $f_{\text{LLM}}(x_1),\dots,f_{\text{LLM}}(x_n)$. Spearman rank coefficient will tell us whether the model preserves natural ordering of the numbers.
% \end{itemize}

\noindent $\bullet{}$ PCA nad PLS. After the set of hidden state representations is aggregated, we further project it into a one-dimensional space using PCA or PLS methods. In particular, for PLS we take the numbers themselves to form the target vectors, while for letters, we consider the letter sequence as a base $26$ representation of a number (with random but fixed assignment of values to the letters), and use this number as the corresponding target.

\vspace{0.5\baselineskip}
\noindent $\bullet{}$ Monotonicity metric. We apply it on a sequence $x_1,\dots,x_n$ of all the numbers in the union of groups $G_j$ defined in \eqref{eq groups G}, and their respective projections $f_{\text{LLM}}(x_1),\dots,f_{\text{LLM}}(x_n)$. Spearman rank coefficient will tell us whether the model preserves natural ordering of the numbers.

% \noindent $\bullet{}$ Scaling Rate Index. 
% To analyze the spacing between representatives of various groups, we apply SRI analysis to the sequence 
% \[\label{eq expected center}
% \bar{f}_{\text{LLM}}(i) = \underset{{x \in G_i}}{\mathbb{E}}[f_{\text{LLM}}(x)],
% \]
% i.e. we are fitting positive constants $\alpha$ and $\beta$ such that $\bar{f}_{\text{LLM}}(i)\approx \alpha\cdot\beta^i$.


% To investigate whether LLMs exhibit a similar phenomenon to human mental line hypothesis, we construct clusters of numbers spaced exponentially, with representative values centered around powers of ten, specifically \( 10^i \) for \( i = 1, \dots, 5 \) (denoted as sets \( G_i \)). At each model layer, we extract embeddings of these numbers and project them onto a one-dimensional real line using PCA. We then analyze the resulting projections to assess their monotonicity and relative spacing, determining whether LLMs encode numerical representations in a manner consistent with logarithmic scaling.  

% Following equation \eqref{eq expected center}, we take the expected projection of a group \( G_i \), defined as  
% \[
% \bar{f}_{\text{LLM}}(i) = \mathbb{E}_{x \in G_i} [f_{\text{LLM}}(x)],
% \]
% as its representative and apply SRI analysis to the sequence \( \bar{f}_{\text{LLM}}(i) \). 
\vspace{0.5\baselineskip} 
\noindent $\bullet{}$ Scaling Rate Index. The initial centers of \( G_i \), given by \( x_i = 10^i \), form a convex, exponentially growing sequence characterized by parameters \( \alpha = 9 \) and \( \beta = 10 \). These numbers serve as representative scales of the numbers within each group.

To obtain a robust estimate of how these scales are preserved in the projections under $f_{\text{LLM}}$, we compute the expectation of projections in each group. Specifically, for SRI analysis, we define the sequence

\begin{equation}\label{eq expected center}
\bar{f}_{\text{LLM}}(i) = \underset{{x \in G_i}}{\mathbb{E}}[f_{\text{LLM}}(x)],
\end{equation}

\noindent and fit positive constants $\alpha$ and $\beta$ such that $\bar{f}_{\text{LLM}}(i)\approx \alpha\cdot\beta^i$. 

In particular, the mapping 
\begin{equation}
10^i \mapsto \bar{f}_{\text{LLM}}(i) %:= \underset{x\in G_i}{}\bar{p}_T(i).
\end{equation}
allows us to examine how does $f_{\text{LLM}}$ scales numerical magnitudes. The fundamental question we seek to answer is: \textbf{What is the nature of the function \( f_{\text{LLM}} \) (logarithmic, linear, or exponential)}? 

% By aggregating over groups rather than individual points, you ensure that your analysis captures the model’s broader scaling behavior rather than artifacts of individual representations.

% The analogous mapping in LLMs thus takes the form  
% \[
% 10^i \mapsto \bar{f}_{\text{LLM}}(x_i) := \underset{x\in G_i}{}\bar{p}_T(i).
% \]
% The central question we aim to is: \textbf{What is the nature of the function \( f_{\text{LLM}} \) (logarithmic, linear, or exponential)}?  

% To determine this, we fit the parameters \( \alpha \) and \( \beta \) for the sequence \( \bar{p}_T(i) \) and analyze the results from Table \ref{tab:model_comparison_compact}: 


To answer this question, we analyze the scaling factor $\beta$ in the fitted exponential model\footnote{We can disregard $\alpha$ from the analysis since it does not influence the scaling but merely introduces a bias.}. In the following, we explain how different values of $\beta$ correspond to the underlying properties of $f_{\text{LLM}}$.

% \begin{itemize}
    $\bullet{}$ If \( \beta > 1 \), the sequence \( \bar{f}_{\text{LLM}}(i) \) is convex and exponentially increasing. This means \( f_{\text{LLM}} \) maps an exponentially increasing sequence to another exponentially increasing sequence:
    \[
    9 \cdot 10^i \mapsto \alpha \cdot \beta^i = \alpha \cdot 10^{(\log_{10} \beta) \cdot i}.
    \]
    Thus, \( f_{\text{LLM}} \) preserves the original spacing of numbers, albeit with a scaling factor \( \log_{10} \beta >0 \).  

    $\bullet{}$ If \( \beta = 1 \), the sequence \( \bar{f}_{\text{LLM}}(i) \) is linearly increasing, meaning \( f_{\text{LLM}} \) takes the form:
    \[
    10^i \mapsto \alpha \cdot i = \alpha \cdot \log_{10} 10^i.
    \]
    In this case, \( f_{\text{LLM}} \) exhibits logarithmic scaling.  

    $\bullet{}$ If \( \beta < 1 \), the sequence \( \bar{f}_{\text{LLM}}(i) \) is concave, exponentially decaying. Here, \( f_{\text{LLM}} \) follows:
    \[
    10^i \mapsto \alpha \cdot \beta^i = \alpha \cdot 10^{-(\log_{10} \frac{1}{\beta}) \cdot i}.
    \]
    Thus \( f_{\text{LLM}} \) is a \textit{sub-logarithmic} mapping.
% \end{itemize}

\begin{table}[h]
\begin{footnotesize}
    \centering
    \resizebox{.48\textwidth}{!}{ % Resize to fit within two-column width
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{Group} & \textbf{Layer} & $\rho \pm$ std & $\beta \pm$ std & $\sigma^2$ $\pm$ std \\
        \midrule
        \multirow{2}{*}{LLaMA-2-7B} & Numbers & 3 & 0.97 $\pm$ 0.00 & 0.83 $\pm$ 0.06 & 0.60 $\pm$ 0.01 \\
         & Letters & 1 & 0.45 $\pm$ 0.00 & 1.21 $\pm$ 0.00 & 0.24 $\pm$ 0.00 \\
        \midrule
        \multirow{2}{*}{Pythia-2.8B} & Numbers & 8 & 0.94 $\pm$ 0.01 & 0.54 $\pm$ 0.01 & 0.31 $\pm$ 0.01 \\
         & Letters & 11 & 0.89 $\pm$ 0.01 & 0.53 $\pm$ 0.10 & 0.16 $\pm$ 0.01 \\
        \midrule
        \multirow{2}{*}{GPT-2-L} & Numbers & 18 & 0.95 $\pm$ 0.00 & 0.58 $\pm$ 0.02 & 0.32 $\pm$ 0.00 \\
         & Letters & 5 & 0.11 $\pm$ 0.05 & 0.80 $\pm$ 0.42 & 0.21 $\pm$ 0.01 \\
        \midrule
        \multirow{2}{*}{Mistral-7B} & Numbers & 3 & 0.96 $\pm$ 0.00 & 1.05 $\pm$ 0.00 & 0.44 $\pm$ 0.00 \\
         & Letters & 14 & 0.89 $\pm$ 0.00 & 0.60 $\pm$ 0.00 & 0.22 $\pm$ 0.00 \\
        \midrule
        \multirow{2}{*}{LLaMA-3.1-8B} & Numbers & 1 & 0.41 $\pm$ 0.04 & 1.14 $\pm$ 0.05 & 0.48 $\pm$ 0.01 \\
         & Letters & 1 & 0.56 $\pm$ 0.00 & 0.16 $\pm$ 0.07 & 0.19 $\pm$ 0.01 \\
        \midrule
        \multirow{2}{*}{LLaMA-3.2-Instruct-1B} & Numbers & 4 & 0.93 $\pm$ 0.02 & 1.33 $\pm$ 0.12 & 0.35 $\pm$ 0.01 \\
         & Letters & 1 & 0.57 $\pm$ 0.06 & 0.47 $\pm$ 0.08 & 0.17 $\pm$ 0.00 \\
        \bottomrule
    \end{tabular}
    }
    \end{footnotesize}
    \caption{Comparison of several models on Numbers and Letters groups, evaluated using three metrics: $\rho$, $\beta$, and Explained Variance ($\sigma^2$). Results are reported for the layer with the highest $\sigma^2$ score. Standard deviations are included.}
    \label{tab:model_comparison_compact}
\end{table}

\begin{table}[h]
\begin{footnotesize}
    \centering
    \resizebox{.48\textwidth}{!}{ % Resize to fit within two-column width
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{Group} & \textbf{Layer} & $\rho \pm$ std & $\beta \pm$ std & $R^2 \pm$ std \\
        \midrule
        \multirow{2}{*}{Llama-3.2-1B-Instruct} & Numbers & 6 & 0.91 $\pm$ 0.00 & 1.93 $\pm$ 0.05 & 0.68 $\pm$ 0.01 \\
         & Letters & 10 & 0.93 $\pm$ 0.00 & 0.97 $\pm$ 0.03 & 0.45 $\pm$ 0.03 \\
        \midrule
        \multirow{2}{*}{Pythia-2.8b} & Numbers & 1 & 0.78 $\pm$ 0.02 & 4.65 $\pm$ 1.32 & 0.71 $\pm$ 0.01 \\
         & Letters & 20 & 0.90 $\pm$ 0.01 & 0.95 $\pm$ 0.11 & 0.46 $\pm$ 0.04 \\
        \midrule
        \multirow{2}{*}{GPT2-L} & Numbers & 17 & 0.96 $\pm$ 0.01 & 1.15 $\pm$ 0.09 & 0.67 $\pm$ 0.03 \\
         & Letters & 33 & 0.81 $\pm$ 0.03 & 0.93 $\pm$ 0.04 & 0.44 $\pm$ 0.01 \\
        \midrule
        \multirow{2}{*}{Llama-2-7b} & Numbers & 5 & 0.93 $\pm$ 0.00 & 2.62 $\pm$ 0.00 & 0.81 $\pm$ 0.00 \\
         & Letters & 27 & 0.88 $\pm$ 0.03 & 0.91 $\pm$ 0.02 & 0.45 $\pm$ 0.01 \\
        \midrule
        \multirow{2}{*}{Mistral-7B-v0.1} & Numbers & 7 & 0.88 $\pm$ 0.00 & 14.87 $\pm$ 8.28 & 0.81 $\pm$ 0.00 \\
         & Letters & 29 & 0.86 $\pm$ 0.00 & 1.62 $\pm$ 0.00 & 0.63 $\pm$ 0.00 \\
        \midrule
        \multirow{2}{*}{Llama-3.1-8B} & Numbers & 4 & 0.93 $\pm$ 0.01 & 2.00 $\pm$ 0.01 & 0.73 $\pm$ 0.01 \\
         & Letters & 16 & 0.93 $\pm$ 0.01 & 0.88 $\pm$ 0.06 & 0.45 $\pm$ 0.02 \\
        \bottomrule
    \end{tabular}
    }
\end{footnotesize}
\caption{Comparison of several models on Numbers and Letters groups, evaluated using three metrics: $\rho$, $\beta$, and $R^2$. Results are reported for the layer with the highest $R^2$. Standard deviations are included.}
\label{tab:model_comparison}
\end{table}


\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
    \includegraphics[width=\textwidth]{plots/GPT2-L.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/Llama-2.7B.pdf}
    \end{subfigure}
    
    \vspace{0.3cm}  % Adds spacing between rows
    
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/Pythia-2.8B.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/Mistral-7B.pdf}
    \end{subfigure}

\caption{Layer-wise analysis of four models on numerical groups, showing explained variance ($\sigma^2$), monotonicity ($\rho$), and Scaling Rate Index ($\beta$). The layer with maximum $\sigma^2$ aligns with peak $\rho$, indicating optimal numerical encoding.}
    \label{fig:metrics-layers-numerics}
% \vspace{-1em}
\end{figure}

\begin{figure}[ht]
    \centering
\includegraphics[width=0.9\linewidth]{plots/plot1.pdf}
\caption{Projections of numerical representations (y-axis) against their log-scaled magnitudes (x-axis) for the layer with the highest explained variance in four models. Sublinearity and monotonicity ($\rho$) are indicated above each subfigure, demonstrating consistent sublinear trends and strong monotonic relationships across models.}    \label{fig:projections-numbers}
\end{figure}

\begin{figure}[ht]
    \centering
\includegraphics[width=0.9\linewidth]{plots/plot2.pdf}
\caption{Projections of letters representations (y-axis) against their log-scaled magnitudes (x-axis) assigned proportional to their length, for the layer with the highest explained variance in four models. Sublinearity and monotonicity ($\rho$) are indicated above each subfigure, demonstrating consistent sublinear trends and strong monotonic relationships across models.}    \label{fig:projection_letters}
\end{figure}

% \subsection{Results}
\paragraph{Results.} 

The results reveal distinct yet consistent patterns in how different models encode numerical and alphabetical structures, with variations across layers (Tables \ref{tab:model_comparison_compact} and \ref{tab:model_comparison}). Despite these variations, similar trends emerge across the models, leading to consistent conclusions about their processing of numerical values (please refer to appendix \ref{app:exp} for experimental details).

\paragraph{Numerical vs. Symbolic Representations.} 
First key finding is that numerical embeddings exhibit a significantly higher explained variance ($\sigma^2$ in Table \ref{tab:model_comparison_compact} and $R^2$ in Table \ref{tab:model_comparison}) in the one-dimensional PCA and PLS transformations compared to letter-based embeddings (refer to Methodology). This suggests that numbers naturally align along a one-dimensional manifold—akin to a number line—while random sequences of letters do not display the same structured behavior.

Furthermore, the monotonicity metric ($\rho$) consistently shows higher values for numerical data, with most models achieving $\rho > 0.9$ in both PCA and PLS analyses. This supports the idea that numerical representations are not only structured, but also maintain a well-ordered progression across layers.

The resulting projections obtained using PCA for the numerical and letters groups are visualized in Figures \ref{fig:projections-numbers} and \ref{fig:projection_letters}, respectively. 

\paragraph{Sublinearity in Numerical Representations.}
The sublinearity coefficient ($\beta$) derived from PCA projections reveals notable differences across models. Some, such as LLaMA-2-7B, Pythia, and GPT-2 Large, exhibit strong sublinear (sublogarithmic) scaling with $\beta < 1$, indicating that embedding distances grow at a diminishing rate. In contrast, models like Mistral show a nearly logarithmic trend ($\beta \approx 1$), while others approach a more linear spacing pattern with higher $\beta$ values.

\paragraph{Layer-Wise Dynamics.}
Since Table 1 reports values from the layer with the highest explained variance, interpretation requires caution—other layers with comparable $\sigma^2$ values may exhibit similar trends. Figure \ref{fig:metrics-layers-numerics} provides a layer-wise analysis for four models, demonstrating how sublinearity evolves across different depths. 

% Finally, Figure \ref{fig:projections-numbers} presents scatter plots of PCA projections alongside their logarithmic transformations, offering a clear visual confirmation of monotonicity and sublinearity across all four models.

% When performing PCA and PLS on various layers of the model, in order to project 
% The results highlight distinct patterns in how different models encode numerical and alphabetical structures. As shown in Tables \ref{tab:model_comparison_compact} and \ref{tab:model_comparison}, the best-performing layers vary across models and tasks.



% A key observation is that the explained variance ($R^2$) in the one dimensional PCA (PLS) transformations is significantly higher for numerical data compared to letter-based data (refer to Methodology). This suggests that numbers naturally reside in a one-dimensional structure, similar to a number line, whereas random letters of similar length do not exhibit the same behavior.

% Additionally, the monotonicity metric ($\rho$) is notably higher for numerical data, with most models exhibiting a high degree of monotonicity ($\rho > 0.9$) in both PCA and PLS analyses. This further supports the idea that numerical representations follow a structured and ordered transformation across layers.

% An interesting trend emerges in the sublinearity coefficient ($\beta$) from PCA results. Some models, such as LLaMA-2-7B, Pythia, and GPT-2 Large, display strong sublinear (sublogarithmic) behavior with $\beta < 1$, indicating that distances between embeddings grow at a diminishing rate. In contrast, models like Mistral exhibit mild logarithmic scaling ($\beta \approx 1$), while others show larger $\beta$ values, approaching a more linear spacing pattern.

% However, since the values in Table 1 are reported for the layer with maximum explained variance, the interpretation may be slightly misleading if another layer has comparable $\sigma^2$ coefficient. Figure \ref{fig:metrics-layers-numerics} illustrates the layer-wise behavior for four models, showing that sublinearity is dynamic across layers. Finally, Figure \ref{fig:enter-label} presents a scatter plot of PCA projections alongside their logarithmic values, clearly illustrating monotonicity and sublinearity across all four models.

\paragraph{PCA vs PLS.}
The PLS method achieves high monotonicity ($\rho$) and explained variance ($R^2$) but exhibits lower sublinearity compared to PCA. This discrepancy arises because PLS operates as a supervised linear probe, where the regression target (e.g., numerical values) directly influences the projection. 
This process distorts the intrinsic spacing between points, as PLS prioritizes maximizing covariance with the target over preserving the original geometric structure. In contrast, PCA, being unsupervised, retains the relative spacing of data points in the latent space, better capturing the underlying sublinear trends. This distinction is evident in tables \ref{tab:model_comparison_compact} and \ref{tab:model_comparison}: PCA consistently reveals stronger sublinearity, while PLS achieves higher $R^2$ and $\rho$ by aligning the projection with the target variable. 

Notably, this aligns with findings in \citet{zhu2025language}, where a linear probe  failed to adequately capture the non-linear scaling of hidden states, particularly for larger numbers, where non-linearity becomes more pronounced. Our work explicitly quantify sublinearity using the Scaling Rate Index (SRI, $\beta$), which directly measures the rate of scaling in the latent space. This allows us to better capture the true geometric organization of numerical representations, especially in regimes where non-linear effects dominate.

\paragraph{Ablation study.} Finally, we perform an ablation study to examine the dependence of our results on the number of examples in the prompt for both numerical and alphabetical datasets. Figure \ref{fig:metrics-ablation} shows that the metrics exhibit greater stability for numerical data compared to alphabetical data, indicating that the model processes numerical information more consistently, while alphabetical representations are more sensitive to prompt variations.
% \paragraph{{Numerics vs Letters}}
% The resulting projections obtained using PCA for the numerical and letters groups are visualized in Figures \ref{fig:projections-numbers} and \ref{fig:projection_letters}, respectively. The geometric distribution of the examples reveals distinct patterns: the numerical group exhibits stronger sublinearity compared to the letters group, with a clear and consistent monotonic trend along the projection axis. In contrast, the letters group shows a more dispersed scattering of points along the projection (number line), indicating weaker monotonicity and less structured organization.


% \veb{Ablation study: Dependece of the results on the number of samples we are passing? Also does it make a difference if all the samples a=a, b=b x= belong to the same group or they are randomly sampled? We can include these results in case we need to fill up some space}
% For these purposes, we introduce the following metrics to analyze the representation space: (1) the Monotonicity Metric, which evaluates whether the projected representations form a coherent, ordered number line, and (2) the Sublinearity Metric, which quantifies the extent to which the spacing along this number line decreases in a logarithmic-like manner as the group index increases.

% \paragraph{1. Monotonicity Metric ($M^2$)}
% To evaluate whether $T(x)$ forms a monotonic number line, we compute the \textit{Spearman rank correlation coefficient}:
% \begin{equation}
% M^2 = \left|1 - \frac{6 \sum_{i=1}^n (R(x_i) - R(p_T(x_i)))^2}{n(n^2 - 1)}\right|,
% \end{equation}
% where $R(x_i)$ and $R(p_T(x_i))$ are the ranks of $x_i$ and $p_T(x_i)$, respectively, and $n$ is the number of examples. A value of $M^2 \approx 1$ indicates strong monotonic alignment\footnote{The metric is unaffected by the non-linearity of the number line, as it is based on ranks.}.

% \paragraph{2.Sublinearity Metric (SM)}  
% To analyze sublinearity, we consider the expected spacing between consecutive groups. Given a set of groups \( G_1, G_2, \dots, G_n \), we define the expected projection of each group as:  
% \[
% \bar{p}_T(i) = \mathbb{E}_{x \in G_i} [p_T(x)].
% \]  
% The expected distance between successive groups is then:  
% \[
% \bar{d}_T(i) = \bar{p}_T(i+1) - \bar{p}_T(i).
% \]  
% The distance is approximated by a power-law function as follow:
% \begin{equation}
%     \bar{d}_T(i) = \alpha \beta^i
% \end{equation}
% To estimate \( \beta \), we fit a linear regression on the log-transformed distances:  
% \[
% \log \bar{d}_T(i) = i \log \beta  + \log \alpha .
% \]  
% The estimated exponent \( \hat{\beta}\) allows us to compute the \textbf{Sublinearity Metric (SM)},
% $SM = \hat{\beta}$.
% \begin{itemize}
%     \item \( 0<SM \leq1  \): sublinear spacing.  
%     \item \( SM = 1 \): logarithmic spacing (\( \bar{d}_T(i) \sim \alpha \)).  
%     \item \( SM > 1 \): super-linear (spacing increases instead of decreasing).  
% \end{itemize}





% \subsection{Sub-linear Number line in Embedding Space}

% We hypothesize that the embeddings of numerical values in LLaMA 7B exhibit a form of logarithmic compression. Specifically, we predict that as the numerical values increase, the Euclidean distance between consecutive embeddings decreases, mimicking the behavior of a logarithmic scale.

% To test this hypothesis, we conduct a series of experiments by grouping numbers based on their logarithmic magnitude. The groups are designed such that the ratio of any number sampled from group $i+1$ to a randomly selected number from group $i$ is approximately 10, ensuring equal spacing along a logarithmic scale. These groups are defined as:
% \begin{itemize}
%     \item $G_1 := \{1, 2, \dots, 20\}$ for $i=1$.
%     \item $G_{i} := \{10^i - 2 \cdot 10, \dots, 10^i + 2 \cdot 10\}$ for $i > 1$.
% \end{itemize}


% For each number $x$ in these groups, we construct prompts of the form $a=a, b=b, c=c, x=$, where $a$, $b$, and $c$ are random numbers, and $x$ is the number from the group. The model’s hidden states are collected across all layers of the language model for each number in the groups. Dimensionality reduction techniques, such as PCA and PLS, are then applied to the hidden states, and the data is projected onto the first principal component suggested by these methods.

% To evaluate the alignment with a line and the sublinearity of the number line in the embedding space, we define two key metrics:
% \begin{itemize}
%     \item \textbf{Linearity Metric (LM):} This metric quantifies whether the numbers in the embedding space form a line along one principal direction. For each group $G_i$, we compute the distance between the numbers along the first principal direction of the projected data. We then assess whether the distances increase along one axis and decrease along the other. This is done by fitting a linear model to the data and measuring the goodness of fit. The closer the data points are to forming a straight line in the embedding space, the higher the Linearity Metric (LM). A high value of LM indicates that the numbers align along a single direction, with increasing values along one dimension and decreasing variance along the orthogonal dimension.

%     \item \textbf{Sublinearity Metric (SM):} This metric quantifies the extent to which the distance between consecutive numbers in the embedding space follows a sublinear, logarithmic-like pattern. For each group $i$, we compute the variance of the distances between embeddings within the group:
%     \[
%     \sigma^2_i := \mathbb{V}_{m, n \in G_i}[\tilde{d}(m, n)],
%     \]
%     and track how this variance changes as we move from smaller to larger groups. A decreasing variance along the x-axis (logarithmic scale) as $i$ increases indicates sublinearity in the embedding space, which we measure by comparing the rate of variance decay to a theoretical model of logarithmic compression.
% \end{itemize}

% These metrics will help assess whether the LLaMA 7B model’s embeddings of numerical values align with a line (increasing along one axis and decreasing along another) and whether they exhibit sublinear compression as the numbers increase.

% \begin{table}[h]
%     \centering
%     \renewcommand{\arraystretch}{1.2}
%     \setlength{\tabcolsep}{4pt}
%     \begin{tabular}{lccc|ccc}
%         \toprule
%         & \multicolumn{3}{c|}{\textbf{Numbers Group}} & \multicolumn{3}{c}{\textbf{Letters Group}} \\
%         \textbf{Model} & $\rho$ & $\beta$ & EV & $\rho$ & $\beta$ & EV \\
%         \midrule
%         LLaMA-2-7B  & 0.99 & 0.79 & 0.60 & 0.31 & 1.74 & 0.26 \\
%         Pythia-2.8B & 0.94 & 0.60 & 0.3 & 0.90 & 0.50 & 0.16 \\
%         GPT-2-L & 0.96 & 0.51 & 0.33 & 0.18 & 0.85 & 0.20 \\
%         Mistral-7B & 0.97 & 0.98 & 0.43 & 0.84 & 0.59 & 0.21 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Comparison of three models (LLaMA, Pythia, GPT-2) on two case studies: Numbers Group and Letters Group, evaluated using two metrics ($M^2$ and SRI $\beta$). We report results for the layer with highest Explained Variance (EV).}
%     \label{tab:model_comparison}
% \end{table}

% \begin{table}[h]
%     \centering
%     \resizebox{0.48\textwidth}{!}{ % Resize the table to the width of the text
%     \renewcommand{\arraystretch}{1.2}
%     \setlength{\tabcolsep}{4pt}
%     \begin{tabular}{lccc|ccc}
%         \toprule
%         & \multicolumn{3}{c|}{\textbf{Numbers Group}} & \multicolumn{3}{c}{\textbf{Letters Group}} \\
%         \textbf{Model} & $\rho$ & $\beta$ & EV & $\rho$ & $\beta$ & EV \\
%         \midrule
%         LLaMA-2-7B  & 0.99 & 0.79 & 0.60 & 0.31 & 1.74 & 0.26 \\
%         Pythia-2.8B & 0.94 & 0.60 & 0.3 & 0.90 & 0.50 & 0.16 \\
%         GPT-2-L & 0.96 & 0.51 & 0.33 & 0.18 & 0.85 & 0.20 \\
%         Mistral-7B & 0.97 & 0.98 & 0.43 & 0.84 & 0.59 & 0.21 \\
%         \bottomrule
%     \end{tabular}
%     }
%     \caption{Comparison of three models (LLaMA, Pythia, GPT-2) on two case studies: Numbers Group and Letters Group, evaluated using two metrics ($M^2$ and SRI $\beta$). We report results for the layer with highest Explained Variance (EV).}
%     \label{tab:model_comparison}
% \end{table}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
    \includegraphics[width=\textwidth]{plots/samples_4.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
    \includegraphics[width=\textwidth]{plots/examples_40.pdf}
    \end{subfigure}
    \caption{Top row: Change in metrics with respect to the number of samples in the set $\mathcal{X}$. Bottom row: Change in metrics with respect to the number of in-context examples in the prompt. Left column corresponds to the \textbf{numbers} group, and the right column corresponds to the \textbf{letters} group. Sublinearity and monotonicity trends are highlighted for each case.}
    \label{fig:metrics-ablation}
\end{figure}


\section{Experiment 2: Identifying number line using real-world tasks}
\label{experimen2}

\paragraph{Setup.} In the previous experiment (Section \ref{experiment1}) we created an artificial experimental setting to test our hypothesis. 
In this experiment, however, we want to further validate our hypothesis using real-world data. We collect names of celebrities along with their birth years and population of different cities/countries from Wikidata~\cite{Wikidata}. The task here is to investigate for similar patterns and observations seen in the previous experiment. 


\paragraph{Motivation.}
The goal of this experiment is to investigate how LLMs internally represent numerical values in real-world contexts, specifically focusing on the monotonicity and scaling of these representations. By analyzing the hidden states, we aim to uncover whether the models encode numerical information in a structured and interpretable manner.

\paragraph{Methodology.}

The experimental setup for this experiment is as follows:

% \begin{itemize}
$\bullet{}$ \textbf{Prompting the Model:} We prompt the model to provide the exact birth year or population size for each entity in our dataset, which consists of 1K samples.An example of a prompt would be ``What is the population of [country]?''
$\bullet{}$ \textbf{Collecting Model Outputs:} We collect the LLM's output answers, and filter out non-numerical and incorrect responses. 

$\bullet{}$ \textbf{Extracting Hidden States:} We extract the hidden state corresponding to the question mark token at each model layer\footnote{We divided the hidden states into four equally sized groups, ranging from the minimum to the maximum answers, to facilitate the calculation of the Scaling Rate Index (SRI).}.

$\bullet{}$ \textbf{Training PLS Models:} We train one- and two-component PLS models on the extracted hidden states to predict the birth years or population sizes of the entities. This is performed for two LLMs: \textit{Llama-3.1-8B} and \textit{Llama-3.2-1B}.

% \end{itemize}

\paragraph{Results.}

The results, as shown in table \ref{tab:model_comparison exp 2}, demonstrate a clear distinction between the two tasks, but also between the models. 

% For the \textit{birth year} task, \textit{Llama-3.1-8B} exhibits strong trends with high monotonicity ($\rho$) and $R^2$, alongside low SRI ($\beta$), indicating well-structured and predictive internal representations. In contrast, \textit{Llama-3.2-1B} shows lower monotonicity, making $\beta$ uninformative, likely due to less structured representations of earlier birth years (Figure \ref{fig:pls_model_comparison}).

% For the \textit{population size} task, both models display weaker monotonicity and lower $R^2$, suggesting population sizes are encoded less systematically. Unlike birth years, population figures are more context-dependent, influenced by geopolitical changes, reporting inconsistencies, and approximate expressions in text. Consequently, the low monotonicity makes the scaling ratio $\beta$ unreliable.

For the \textit{birth year} task, model \textit{Llama-3.1-8B} exhibit strong trends, with high monotonicity ($\rho$) and  ($R^2$), while having low SRI ($\beta)$, hence high compression. This indicates that the internal representations of birth years are well-structured and predictive, aligning with our expectations for numerical encoding in LLMs.
On the other side, \textit{Llama-3.2-1B}) shows low monotonicity score, hence the $\beta$ factor is not informative. We attribute the low monotonicity score to the non-structured internal representations of lower birth years, as can be seen in Figure \ref{fig:pls_model_comparison}.

For the \textit{population size} task, both models display weaker monotonicity and lower $R^2$, suggesting population sizes are encoded less systematically. Unlike birth years, population figures are more context-dependent, influenced by geopolitical changes, reporting inconsistencies, and approximate expressions in text. Consequently, the low monotonicity makes the scaling ratio $\beta$ unreliable.

% In contrast, for the \textit{population size} task, the results are less consistent. Both models exhibit weaker monotonicity and lower $R^2$ values, indicating that population sizes are not encoded with the same structured regularity as birth years. One possible explanation is that population sizes are inherently more context-dependent—often appearing in diverse textual settings with varying emphasis on relative or absolute scale. Unlike birth years, which follow a well-defined chronological order, population values fluctuate due to geopolitical changes, reporting inconsistencies, and rounding conventions in natural language. Given the low monotonicity scores, the scaling ratio $\beta$ becomes unreliable, as the representation space does not exhibit a consistent ordering that would allow for meaningful compression analysis.

% The results, as shown in table \ref{tab:model_comparison exp 2}, demonstrate a clear distinction between the two tasks. For the \textit{birth year} task, both models (\textit{Llama-3.1-8B} and \textit{Llama-3.2-1B}) exhibit strong positive trends, with high monotonicity ($\rho$) and  ($R^2$), while having low $SRI$ ($\beta)$. This indicates that the internal representations of birth years are well-structured and predictive, aligning with our expectations for numerical encoding in LLMs.

% In contrast, for the \textit{population size} task, the results are less consistent. Both models show weaker monotonicity and lower $R^2$ values, suggesting that they struggle to encode population sizes in a similarly structured manner. This discrepancy may arise due to the greater variability and complexity of population data, which often spans multiple orders of magnitude and lacks the linear regularity observed in birth years.

% In contrast, for the \textit{population size} task, the results are less consistent. Both models exhibit weaker monotonicity and lower $R^2$ values, indicating that population sizes are not encoded with the same structured regularity as birth years. One possible explanation is that population sizes are inherently more context-dependent—often appearing in diverse textual settings with varying emphasis on relative or absolute scale. Unlike birth years, which follow a well-defined chronological order, population values fluctuate due to geopolitical changes, reporting inconsistencies, and rounding conventions in natural language. Additionally, population figures are often presented in approximate terms (e.g., “about 10 million”), further reducing the precision of their numerical encoding. Given the low monotonicity scores, the scaling ratio 
% $\beta$ becomes unreliable, as the representation space does not exhibit a consistent ordering that would allow for meaningful compression analysis.

Finally Figure \ref{fig:pls_model_comparison} shows the examples of one and two PLS projections for two models, for birth-year dataset. 


\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.23\textwidth}
        \includegraphics[width=\linewidth]{figures/PLS_MODEL_2COMPONENTS_BIRTH_best_layer.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.23\textwidth}
        \includegraphics[width=\linewidth]{figures/PLS_MODEL_1COMPONENTS_BIRTH_best_layer.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.23\textwidth}
        \includegraphics[width=\linewidth]{figures/1B_PLS_MODEL_2COMPONENTS_BIRTH_best_layer.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.23\textwidth}
        \includegraphics[width=\linewidth]{figures/1B_PLS_MODEL_1COMPONENTS_BIRTH_best_layer.pdf}
    \end{subfigure}
    \caption{Visualization of PLS models trained on Llama-3.1-8B (top row) and Llama-3.2-1B (bottom row) model activations to predict entities' birth years using one and two dimensional PLS respectively. Each subfigure represents the layer with the highest $R^2$ score for one- and two-component PLS models.}
    \label{fig:pls_model_comparison}
\end{figure}



\begin{table}[ht]
\begin{footnotesize}
    \centering
    \resizebox{.48\textwidth}{!}{ % Resize to fit within two-column width
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{Dataset} & \textbf{Layer} & $\rho$ & $\beta$ & $R^2$ \\
        \midrule
        \multirow{2}{*}{Llamba 3.1 8B} & Birth & 29 & 0.84 & 0.50 & 0.63 \\
         & Population & 9 & 0.63 & 2.48 & 0.08 \\
        \midrule
        \multirow{2}{*}{Llamba 3.2 1B} & Birth & 12 & 0.03 & 0.72 & 0.61 \\
         & Population & 10 & 0.62 & 2.69 & 0.10 \\
        \bottomrule
    \end{tabular}
    }
\end{footnotesize}
\caption{Results for Llamba models evaluated on the Birth and Population datasets. Results are reported for the layer with the highest $R^2$, highlighting the relationship between scaling rate ($\beta$), monotonicity ($\rho$), and model performance.}
\label{tab:model_comparison exp 2}
\end{table}