% \section{Monotonicity Metric and Scaling Rate Index}\label{app sublinearity}

% \subsection{Monotonicity Metric}
% Let $X, Y\in \mathbb{R}^n$ be two real $n$-dimensional vectors and let $R(X)$ (resp. $R(Y))$ denote an $n$-dimensional vector obtained from $X$ (resp. $Y$) where the entries are substituted with their ranks in the sequence of sorted entries of $X$ (resp. $Y$). Then, Spearman rank correlation coefficients (usually denoted by $\rho$) is given by:
% \begin{equation}\label{eq spearman general}
%     \rho=\frac{\textbf{Cov}(R(X),R(Y))}{\sigma(R(x))\cdot\sigma(R(Y))},
% \end{equation}
% Where $\textbf{Cov}(R(X),R(Y))$ is the covariance between rank vectors $R(X)$ and $R(Y)$, while $\sigma(R(X))$ and $\sigma(R(Y))$ are their respective standard deviations. 

% Spearman coefficient $\rho$ is a nonparametric measure for the alignment of the two vectors. Loosely speaking, the coefficient assesses if the increment in one variable corresponds to the increase (or decrease) of the other. In particular, unlike Pearson coefficient which takes into account the value of the changes, Spearman's $\rho$ takes into account only the sign of the changes. 

% In the special situation where all the entries of vector $X$ and $Y$ have different ranks, the equation \eqref{eq spearman general} simplifies to 
% \begin{equation}\label{eq spearman special}
%     \rho = 1 - \frac{6 \cdot\sum_{i=1}^n (R(x_i) - R(y_i))^2}{n\cdot(n^2 - 1)},
% \end{equation}
% where $x_i$ and $y_i$ are $i$th entries of $X$ and $Y$, respectively. 

% In our experiments, we consider a sequence of groups $G_j$, $j=1,\dots,5$ \ve{What is $j$ in our experiments?} defined as $G_1=\{1,\dots,40\}$ and $G_j=\{10^j-19,\dots,10^j+20\}$ for $j\geq 2$. In particular, the sets $G_j$ are disjoint and each has cardinality 40. Let $x_1,\dots,x_n$ be a sequence of all the elements in $\cup_{j=1}^5G_j$ and let $p_T(x_1),\dots,p_T(x_n)$ be their projections by the map $p_T$ defined in equation \eqref{eq projection p_T}. Then, in Section \ref{sec exp 1} we test the monotonic alignment between the sequences $(x_i)_i$ and $(p_T(x_i))_i$. In particular, we empirically verify that there are no repeated ranks among the sequences (for sequence $(x_i)_i$ this is clear), and we can directly apply formula \eqref{eq spearman special}.  

% \subsection{Scaling Rate Index}

% For a general monotonically increasing sequence $x_i$, $i=1,\dots,n$, SRI is designed to measure the rate at which the sequence grows. 

% In particular, if $\beta>0$, the difference between two consecutive members $x_i$ and $x_{i+1}$ is expected to be  increasing with $i$. in other words, $x_{i+1}-x_{i}$ is expected to be smaller than $x_{i+2}-x_{i+1}$. Such sequences are commonly known as \textit{convex sequences} \cite{rockafellar2015convex} and the growth of $x_i$ is exponential (superlinear) in $i$. An example of such a sequence that is relevant to our study is the sequence defined with $x_i:=10^i$. Then, $x_{i+1}-x^i= 9\cdot 10^{i}$ and we can take $\alpha=9$ and $\beta = 10$. 

% If $\beta=1$, expression \eqref{eq sri} indicates that the difference between two consecutive members $x_i$ and $x_{i+1}$ is approximately constant, i.e. the sequence is approximately linearly increasing. For an example, one may take sequence $x_i:=i$, for which $\alpha=1=\beta$.

% Finally, $\beta<1$ implies that the difference between consecutive members is steadily decreasing or $x_{i+1}-x_{i}$ is expected to be greater than $x_{i+2}-x_{i+1}$. Such sequences are commonly known as \textit{concave sequences} \cite{rockafellar2015convex}, and the growth of the sequence is exponentially decreasing (sublinear). An example of such a sequence is given by $x_i = 1- \frac{1}{10^i}$, with $\alpha = 9$ and $\beta = \frac{1}{10}$. 

% \subsection{Relation between SRI and logarithmic number line hypothesis}

% % The logarithmic mental number line hypothesis \cite{dehaene2008log} states that humans innately space numbers on the number line in a logarithmic way. If we denote by $f$ such a map (spacing), then the hypothesis asserts that $f$ is approximately logarithmic map. 

% % Of course, such a map $f$ can hardly be mathematically defined in a precise manner, but for our purposes, we use $f\equiv\log$  as a guiding principle in our experiments. 

% % The \textit{logarithmic mental number line hypothesis} \cite{dehaene2008log} suggests that humans innately perceive numerical magnitudes on a logarithmic scale rather than a linear one. In other words, if we denote the internal mapping of numbers to their cognitive representation by \( f_h \), the hypothesis asserts that \( f_h \) is approximately a logarithmic function. While the exact form of \( f_h \) cannot be precisely defined, we adopt \( f_h \equiv \log \) as a guiding principle in our experiments.  

% % To explore an analogous phenomenon in LLMs, we construct clusters of numbers that are spaced exponentially, with representative values centered around powers of ten, specifically \( 10^i \) for \( i = 1, \dots, 5 \) (i.e. set of numbers $G_i$). At each model layer, we extract the embeddings of these numbers and project them onto a one-dimensional real line using PCA. We then analyze the resulting projections to evaluate both their monotonicity and relative spacing, assessing whether LLMs implicitly organize numerical representations in a manner consistent with logarithmic scaling. 

% % In particular, following equation \eqref{eq expected center} we take the expected projection of a group $G_i$, $\bar{p}_T(i) = \mathbb{E}_{x \in G_i} [p_T(x)],$ to be its representative and apply SRI analysis on the sequence $\bar{p}_T(i)$. We recall that the starting sequence of the centers of $G_i$, which is $x_i:= 10^i$, is convex with $\alpha=9$ and $\beta=10$, and it is growing exponentially. The equivalent of the above mapping $f_h$ then becomes $x_i\mapsto f_{LLM}(x_i):=\bar{p}_T(i)$. The main question is: What is the nature (logarithmic, linear, exponential) of the function $f_{LLM}$?

% % When fitting the parameters $\alpha$ and $\beta$ for the sequence $\bar{p}_T(i)$, the results from Table \ref{tab:model_comparison} can be interpreted as follows.

% % $\bullet{}$ $\beta>1$ implies that the sequence $\bar{p}_T(i)$ is convex and exponentially increasing. In particular, the function $f_{LLM}$ is mapping an exponentially increasing sequence to an exponentially increasing sequence $9\cdot 10^i\mapsto \alpha\cdot \beta^i=\alpha\cdot 10^{(\log_{10}\beta)\cdot i}$ and, as such, can be interpreted as preserving the spacing of the original numbers, albeit with some scaling factor in the form of $\log_{10}\beta$.

% % $\bullet{}$ $\beta=1$ implies that the sequence $\bar{p}_T(i)$ is linearly increasing, and the map $f_{LLM}$ takes the form $10^i\mapsto \alpha\cdot i = \alpha\cdot \log_{10}10^i$. In particular, in this case we can say that $f_{LLM}$ is logarithmic. 

% % $\bullet{}$ $\beta<1$ implies the sequence $\bar{p}_T(i)$ is concave, i.e. exponentially decaying. Then, $f_{LLM}$ is the mapping $10^i\mapsto \alpha\cdot \beta^i=\alpha\cdot 10^{-(\log_{10}\frac{1}{\beta})\cdot i}$, and in particular, $f_{LLM}$ can be interpreted as \textit{sub-logarithmic} map. 

% % Based on our results from Table \ref{tab:model_comparison}, we conclude that LLMs perform compression of numbers on a scale that is even stronger than what the logarithmic mental number line hypothesis suggests for humans. Our results suggest that "mental number line" in LLMs is  sub-logarithmic. 


% % \section{Logarithmic Mental Number Line in LLMs}

% The \textit{logarithmic mental number line hypothesis} \cite{dehaene2008log} suggests that humans innately perceive numerical magnitudes on a logarithmic scale rather than a linear one. Formally, if we denote the internal mapping of numbers to their cognitive representation as \( f_h \), the hypothesis asserts that \( f_h \) is approximately logarithmic. While the exact nature of \( f_h \) remains elusive, we adopt the guiding principle \( f_h \equiv \log \) in our experiments.  

% To investigate whether LLMs exhibit a similar phenomenon, we construct clusters of numbers spaced exponentially, with representative values centered around powers of ten, specifically \( 10^i \) for \( i = 1, \dots, 5 \) (denoted as sets \( G_i \)). At each model layer, we extract embeddings of these numbers and project them onto a one-dimensional real line using PCA. We then analyze the resulting projections to assess their monotonicity and relative spacing, determining whether LLMs encode numerical representations in a manner consistent with logarithmic scaling.  

% Following equation \eqref{eq expected center}, we take the expected projection of a group \( G_i \), defined as  
% \[
% \bar{p}_T(i) = \mathbb{E}_{x \in G_i} [p_T(x)],
% \]
% as its representative and apply SRI analysis to the sequence \( \bar{p}_T(i) \). The initial centers of \( G_i \), given by \( x_i = 10^i \), form a convex, exponentially growing sequence with parameters \( \alpha = 9 \) and \( \beta = 10 \). The analogous mapping in LLMs thus takes the form  
% \[
% x_i \mapsto f_{\text{LLM}}(x_i) := \bar{p}_T(i).
% \]
% The central question is: \textbf{What is the nature of the function \( f_{\text{LLM}} \) (logarithmic, linear, or exponential)}?  

% To determine this, we fit the parameters \( \alpha \) and \( \beta \) for the sequence \( \bar{p}_T(i) \) and analyze the results from Table \ref{tab:model_comparison_compact}:  

% \begin{itemize}
%     \item \textbf{If \( \beta > 1 \)}, the sequence \( \bar{p}_T(i) \) is convex and exponentially increasing. This means \( f_{\text{LLM}} \) maps an exponentially increasing sequence to another exponentially increasing sequence:
%     \[
%     9 \cdot 10^i \mapsto \alpha \cdot \beta^i = \alpha \cdot 10^{(\log_{10} \beta) \cdot i}.
%     \]
%     Thus, \( f_{\text{LLM}} \) preserves the original spacing of numbers, albeit with a scaling factor \( \log_{10} \beta >0 \).  

%     \item \textbf{If \( \beta = 1 \)}, the sequence \( \bar{p}_T(i) \) is linearly increasing, meaning \( f_{\text{LLM}} \) takes the form:
%     \[
%     10^i \mapsto \alpha \cdot i = \alpha \cdot \log_{10} 10^i.
%     \]
%     In this case, \( f_{\text{LLM}} \) exhibits logarithmic scaling.  

%     \item \textbf{If \( \beta < 1 \)}, the sequence \( \bar{p}_T(i) \) is concave, exponentially decaying. Here, \( f_{\text{LLM}} \) follows:
%     \[
%     10^i \mapsto \alpha \cdot \beta^i = \alpha \cdot 10^{-(\log_{10} \frac{1}{\beta}) \cdot i}.
%     \]
%     This suggests that \( f_{\text{LLM}} \) is a \textit{sub-logarithmic} mapping.
% \end{itemize}

% From the results in Table \ref{tab:model_comparison_compact}, we conclude that LLMs compress numerical values more aggressively than what the logarithmic mental number line hypothesis predicts for humans. Specifically, our findings indicate that the "mental number line" in LLMs follows a \textit{sub-logarithmic} pattern.





\section{Experimental details}
 \label{app:exp}
% \veb{Experimental setting: Models and their parameters (perhaps a table?), which gpus did we use, data size and samples for both Experiment 1 and Experiment 2. Anything else? }
All experiments were performed using an {NVIDIA A6000 GPU} for accelerated computation. The models were implemented in {Python} and imported from Huggingface with {PyTorch}, and standard libraries like {NumPy} and {Matplotlib} were used for data processing and visualization. We evaluated the following models:

\begin{table}[h!]
\centering
\begin{footnotesize}
\begin{tabular}{l|l|l}
\hline
\textbf{Model} & \textbf{Variants} &Ref. \\ \hline
Pythia & 2.8B &\cite{touvron2023llama1} \\ \hline
LLaMA & 2.7B, 3.1-8B, 3.2-1B & \cite{touvron2023llama1}\\ \hline
GPT-2 & Large-1.5B &\cite{radford2019language} \\ \hline
Mistral & 7B &\cite{jiang2024identifying}\\ \hline
\end{tabular}
\caption{Models evaluated in the experiments.}
\label{tab:models}
\end{footnotesize}
\end{table}

Whenever possible, results were reported as the average of three runs, along with the standard deviation (std). For experiments where repeated runs were not feasible, the random seed was fixed to {42} to ensure reproducibility. 
\section{Additional experiments}
% \subsection{Dependence on the number of examples in the context prompt}

\subsection{Layer-wise PLS analysis}
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
    \includegraphics[width=\textwidth]{plots/GPT2-L_symbols.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/Llama-2.7B_symbols.pdf}
    \end{subfigure}
    
    \vspace{0.3cm}  % Adds spacing between rows
    
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/Pythia-2.8B_symbols.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/Mistral-7B_symbols.pdf}
    \end{subfigure}

\caption{Layer-wise analysis of four models on letters groups, showing explained variance ($\sigma^2$), monotonicity ($\rho$), and Scaling Rate Index  ($\beta$).}    
\label{fig:metrics}
\end{figure}
% \subsection{\ve{What else?}}
\subsection{Birth year  and population datasets projections in all layers}


\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/PLS_MODEL_1COMPONENTS_BIRTH_all_layers.pdf}
    \caption{One component PLS model trained on Llama-3.1-8B instruct model activations to predict entities' birth year.}
    \label{fig:one-component pls-birth task}
\end{figure}


\begin{figure}%[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/PLS_MODEL_2COMPONENTS_BIRTH_all_layers.pdf} % Replace 'example-image' with your image file name
    \caption{Two components PLS model trained on Llama-3.1-8B instruct model activations to predict entities' birth year.}
    \label{fig:two-components pls-birth task}
\end{figure}




\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/1B_PLS_MODEL_1COMPONENTS_BIRTH_all_layers.pdf}
    \caption{One component PLS model trained on Llama-3.2-1B instruct model activations to predict entities' birth year.}
    \label{fig:1B_one-component pls-birth task}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/1B_PLS_MODEL_2COMPONENTS_BIRTH_all_layers.pdf}
    \caption{Two components PLS model trained on Llama-3.2-1B instruct model activations to predict entities' birth year.}
    \label{fig:1B-two-components pls-birth task}
\end{figure*}



\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/population_PLS_MODEL_1COMPONENTS_BIRTH_all_layers.pdf}
    \caption{One component PLS model trained on Llama-3.1-8B instruct model activations to predict entities' population size.}
    \label{fig:one-component pls-population task}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/population_PLS_MODEL_2COMPONENTS_BIRTH_all_layers.pdf}
    \caption{Two components PLS model trained on Llama-3.1-8B instruct model activations to predict entities' population size.}
    \label{fig:two-components pls-population task}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/1B_population_PLS_MODEL_1COMPONENTS_BIRTH_all_layers.pdf}
    \caption{One component PLS model trained on Llama-3.2-1B instruct model activations to predict entities' population size.}
    \label{fig:1B-one-components pls-population task}
\end{figure*}
% \subsection{Population dataset projections}
\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/1B_population_PLS_MODEL_2COMPONENTS_BIRTH_all_layers.pdf}
    \caption{Two component PLS model trained on Llama-3.2-1B instruct model activations to predict entities' population size.}
    \label{fig:1B-two-components pls-population task}
\end{figure*}