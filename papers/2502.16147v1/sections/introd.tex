% \veb{I will add references for neuroscience part of the work, like what is logairthmic mental line hypothesis.}
% \ve{Add in the introduction bits about monotonicity}
The \textit{logarithmic mental number line hypothesis} suggests that humans perceive and process numerical magnitudes in a nonlinear, logarithmic fashion, rather than on a uniform linear scale (see Figure \ref{fig log line}). This idea is rooted in psychophysical studies, particularly the Fechner-Weber law, which posits that perceived differences between stimuli follow a logarithmic relationship rather than an absolute one \cite{fechner1860elemente,dehaene2003neural}. Behavioral experiments have shown that young children and individuals from cultures with limited formal education tend to place numbers on a logarithmic scale when asked to map numerical values onto a spatial axis, suggesting that this mode of representation may be innate or cognitively efficient \cite{dehaene2008log, siegler2003development, moeller2009children}. As individuals receive formal mathematical training, their numerical perception gradually shifts toward a more linear representation, particularly for familiar ranges of numbers, though logarithmic encoding remains evident in certain contexts such as estimation and large-number processing.  

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/log_mental_line.png} % Replace 'example-image' with your image filename
    \caption{\textit{Logarthmic mental number line hypothesis} asserts that humans innately percieve numbers on a logarithmic scale. Image source \cite{fritz2013development}}
    \label{fig log line}
\end{figure}

Neuroscientific evidence further supports this hypothesis. Studies using functional MRI and electrophysiological recordings have found that neuronal populations in the intraparietal sulcus (IPS), a brain region involved in numerical cognition, exhibit activity patterns that align with logarithmic number encoding \cite{nieder2009representation, piazza2004tuning}. Moreover, reaction time studies reveal that numerical comparison tasks conform to a ratio-dependent effect—that is, the difficulty of distinguishing two numbers depends on their relative difference rather than their absolute difference, consistent with a logarithmic scale \cite{moyer1967time}. These findings collectively suggest that human numerical intuition is shaped by compressed, logarithmic-like representations, which may serve as a more efficient encoding strategy for dealing with large ranges of numerical values. Given these cognitive principles, we ask ourselves the following question: do artificial neural systems, such as large language models (LLMs), develop similar numerical representations?


% Large language models (LLMs) have demonstrated remarkable proficiency in a wide array of natural language processing tasks \cite{touvron2023llama1,achiam2023gpt}. However, the mechanisms by which they encode and represent abstract concepts, such as numbers, spatial relationships, and temporal information, are still poorly understood. Recent studies suggest that LLMs may implicitly construct structured, interpretable ``world models,'' capturing coherent representations of relationships within their training data \cite{petroni2019language,radford2019language}. These emergent properties raise compelling questions about the nature of representation and abstraction in these models.

% For instance, recent work has revealed that LLMs encode spatial and geographical information in structured, low-dimensional subspaces. \citet{gurneelanguage} demonstrated that models like LLaMA-2 embed geographical coordinates across layers, producing interpretable world maps that improve with model size. Similarly, \citet{godey2024scaling} observed that spatial representations are hierarchically organized, with model performance strongly correlated to the frequency of country names in the training data, further underscoring the relationship between data exposure, model scale, and representation quality.
% Beyond geography, numerical representation has also been a focus of investigation. For example, \citet{zhou2024scaling} examined the impact of tokenization strategies on numeric reasoning tasks, showing that base-10 tokenization is more data-efficient than higher bases, such as base-100 or base-1000, due to token frequency effects.  

% Recent studies on the \textit{linear hypothesis of representation} further extend this discussion by proposing that concepts are encoded in structured, linear subspaces. \citet{park2023linear} formalized three notions of linearity in representations—subspace, measurement, and intervention—and introduced the concept of a ``causal inner product'' that unifies these perspectives. These findings suggest that representations within LLMs may exhibit geometric properties that support probing, interpretability, and manipulation.
%  Similarly, \citet{heinzerling2024monotonic} examined monotonic trends in the way numeric properties are represented, while \citet{gurneelanguage} introduced the concept of linear representations of space and time.  Collectively, these studies suggest that numerical information is encoded in structured, interpretable subspaces that can be probed, manipulated, and generalized across scales.

Large language models (LLMs) have demonstrated impressive capabilities in natural language processing tasks \cite{touvron2023llama1,achiam2023gpt}, yet their internal representations of abstract concepts—such as numbers, space, and time—remain largely opaque. Recent research suggests that LLMs construct structured ``world models,'' encoding relationships in ways that can be systematically analyzed \cite{petroni2019language,radford2019language}. For instance, studies have shown that spatial and geographical information is embedded in low-dimensional subspaces, where model performance correlates with data exposure \cite{gurneelanguage,godey2024scaling}. Similarly, numerical representation is influenced by tokenization strategies, with base-10 encoding proving more efficient for numeric reasoning tasks than higher-base tokenizations \cite{zhou2024scaling}.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/overview_horizontal.pdf} % Replace 'example-image' with your image filename
    \caption{The overall graphical representation of our method. Numbers are passed to the model in form of a prompt and the internal representations are captured from the embeddings corresponding to token '='.......\ve{finish this}}
    \label{fig pipeline}
\end{figure*}

Beside these results, the linear hypothesis of representation proposes that concepts within LLMs are structured in geometric subspaces that support interpretability and manipulation \cite{park2023linear}. This framework suggests that numerical properties are encoded in systematic, monotonic trends \cite{heinzerling2024monotonic} and that space and time may be represented through linear structures \cite{gurneelanguage}. Collectively, these insights point to the existence of structured, interpretable subspaces in LLMs, raising fundamental questions about the nature of numerical representation and abstraction in artificial systems.

In this work, we explore whether LLMs encode numerical values in a manner analogous to the human logarithmic mental number line, discussed above. By analyzing hidden representations across model layers, we examine how LLMs structure numerical magnitudes using logarithmically spaced groups of numbers. Applying dimensionality reduction techniques such as PCA and PLS, we uncover monotonicity in 1-dimensional projections and a consistent sublinear encoding pattern: As group indices increase, the variance in numerical representations decreases, closely resembling the way humans perceive numbers on a compressed, logarithmic scale. These findings suggest that LLMs may develop internal numerical representations that parallel human cognitive biases, offering new insights into their emerging numerical reasoning abilities.

% Despite these advances, many questions remain about how LLMs encode numerical information and whether these representations align with cognitive patterns observed in humans. A well-documented phenomenon in cognitive psychology is the ``mental number line,'' where humans perceive numerical magnitudes on a logarithmic scale~\cite{moeller2009children}. While some recent work has demonstrated non-linear encoding tendencies in specific contexts \citet{engels2024not}, the extent to which such patterns generalize across numerical domains and tasks in LLMs remains unexplored.

% In this work, we investigate whether LLMs encode numerical values along a sublinear, human-like ``mental number line.'' Using logarithmically spaced groups of numbers, we probe the hidden representations of language models across layers using dimensionality reduction techniques such as PCA and PLS. Our findings reveal consistent sublinear encoding patterns: as group indices increase, the variance in representations decreases, mirroring the human cognitive tendency to perceive numbers on a logarithmic scale. These results provide new insights into the emergent numerical reasoning capabilities of LLMs and their potential alignment with human cognitive principles.
