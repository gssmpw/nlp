% LLMs, such as LLaMA-2, process inputs by mapping them into a high-dimensional \textit{representation space}. For a given input $x$ (e.g., a number presented in a prompt), the model generates an internal representation $f(x) \in \mathbb{R}^d$, where $f(x)$ is the hidden state from a specific layer of the model.

% The geometry of these representations can reveal how the model organizes and reasons about the inputs. By analyzing the structure of $\{f(x)\}_{x \in \mathcal{X}}$ for a set of inputs $\mathcal{X}$ (e.g., numerical values), we can investigate emergent properties such as whether the model encodes these inputs along a \textit{number line} or exhibits \textit{cognitive-like patterns}, such as sublinear scaling.

% Human perception of numbers follows a logarithmic scale, where differences are perceived more acutely for smaller values. This allows efficient processing of a wide range of numbers, with smaller differences, like between 1 and 10, feeling greater than larger ones, such as between 1000 and 1010. Rooted in the Fechner-Weber Law, this phenomenon suggests that the just noticeable difference between stimuli is proportional to the magnitude, resulting in a compressed representation of large numbers and making smaller values more distinguishable.

\subsection{General settup} 

The \textit{logarithmic mental number line hypothesis} \cite{dehaene2008log} suggests that humans innately perceive numerical magnitudes on a logarithmic scale rather than a linear one. Formally, if we denote the internal mapping of numbers to their cognitive representation as \( f_{\text{H}} \), the hypothesis asserts that \( f_{\text{H}} \) is approximately logarithmic. While the exact nature of \( f_{\text{H}} \) remains elusive, we adopt the guiding principle \( f_{\text{H}} \equiv \log \) in our experiments.

On the other side, Large language models, like LLaMA-2, process inputs by mapping them into a high-dimensional representation space, where each input \( x \) (e.g., a number) is transformed into an internal representation \( f(x) \in \mathbb{R}^d \). Analyzing the geometry of these representations across a set of inputs \( \mathcal{X} \) can reveal how the model organizes and reasons about them, shedding light on emergent properties such as whether the model encodes numerical values along a number line or exhibits cognitive-like patterns, such as sublinear scaling. 

Our goal is \textbf{to study the properties of the function $f_{\text{LLM}}$, which serves as a counterpart of the human cognitive mapping $f_{\text{H}}$ described above}. Specifically, we examine  whether $f_{\text{LLM}}$ preserves the natural ordering of numbers and how it transforms their magnitudes. 

% We do so in steps: 1) By passing numerical inputs to a model, we capture their internal representations within the model's hidden layers; 2) We project the internal representations to  one and two dimensional subspaces using PCA and PLS analysis. 3) We study the relationship between the original inputs and finally obtained projections. 

% \ve{TODO: 1. Add description of $f_h$ from below and make our objective clear (Why do we need monotonicity and SRI, what are essential properties of $f_h$ that characterize it. 2. Make the transitions readable and smooth.}
% \section{Definitions}

% \subsection{Projection Distance}  
\subsection{Definition of $f_{\text{LLM}}$}
To analyze the structure of hidden representations, we apply a projection \( T: \mathbb{R}^d \to \mathbb{R}^p \), $p=1,2$, obtained from techniques such as Principal Component Analysis (PCA) or Partial Least Squares (PLS). Then, our function is given by
% projected position of \( x \) is given by  
\begin{equation}\label{eq projection p_T}
        f_{\text{LLM}}(x) := T(f(x)),
\end{equation}
where $f$ is a map between the input number and the corresponding internal representation in the model (see Sections \ref{experiment1} and \ref{experimen2} how $f$ is defined in various settings).

For any two inputs \( x, y \in \mathcal{X} \), we define the distance between their projections following the mapping $T$ using Euclidean norm as  
\begin{equation}
d(x, y) = ||f_{\text{LLM}}(x) - f_{\text{LLM}}(y)||.
\end{equation}

% \paragraph{Inputs} \veb{TODO: Change structure of the section: It should be section Experiments general, and subsections different experiments abstract and concrete.}

\subsection{Abstract Number Line and Monotonicity Metric}  
If the projection dimension is $p=1$, one-dimensional embedding of numerical inputs forms a \emph{number line} if the projections preserve monotonicity (resp. reverse monotonicity), i.e., for \( x_1 < x_2 \) (resp. $x_1>x_2$), we have  
$
f_{\text{LLM}}(x_1) < f_{\text{LLM}}(x_2).
$
This ensures that the natural order of numerical values is maintained in the representation space.  

% \paragraph{Monotonicity Metric ($\rho$).}
% Given a sequence of real numbers $x_1,\dots,x_n$, and a function $f:\mathbb{R}\to \mathbb{R}$, we can measure the ordering alignment (monotonicity) between the sequences $x_1,\dots,x_n$ and $f(x_1),\dots,f(x_n)$ using the \textit{Spearman rank correlation coefficient}:
% \begin{equation}\label{eq spearman}
% \rho = 1 - \frac{6 \cdot\sum_{i=1}^n (R(x_i) - R(f(x_i)))^2}{n\cdot(n^2 - 1)},
% \end{equation}
% where $R(x_i)$ and $R(f(x_i))$ are the ranks (positions in the sorted sequences) of $x_i$ and $f(x_i)$, respectively. A value of $\rho \approx 1$ (resp. $\rho\approx -1$) indicates a strong (resp. reverse)  monotonic alignment\footnote{The metric is unaffected by the non-linearity of the number line, as it is based on ranks.}.

% \paragraph{Scaling Rate Index.} For a monotonically increasing sequence of positive real numbers $x_1,\dots, x_n$, we introduce Scaling Rate Index to measure the rate at which numbers grow in magnitude. More precisely, we seek for positive real constants $\alpha$ and $\beta$ that minimize the following objective function:
% \begin{equation}\label{eq sri}
%     \sum_{i=1}^{n-1}|(x_{i+1}-x_i) - \alpha\cdot \beta^i|^2.
% \end{equation}
% In particular, 

%     $\bullet{}$ $\beta\!>\!1$ indicates \textit{convex (accelerating) growth},
    
%     $\bullet{}$ $\beta\!=\!1$ indicates \textit{linear growth},
    
%     $\bullet{}$ $\beta\!<\!1$ indicates \textit{concave (decelerating) growth}.
  
% We refer to Appendix \ref{app sublinearity} for more details behind this terminology.

% \subsection{Monotonicity Metric}
To measure monotonicity properties of the function $f_{\text{LLM}}$ we use Spearman rank correlation that we briefly describe next. Let $X, Y\in \mathbb{R}^n$ be two real $n$-dimensional vectors and let $R(X)$ (resp. $R(Y))$ denote an $n$-dimensional vector obtained from $X$ (resp. $Y$) where the entries are substituted with their ranks in the sequence of sorted entries of $X$ (resp. $Y$). Then, Spearman rank correlation coefficients (usually denoted by $\rho$) is given by:
\begin{equation}\label{eq spearman general}
    \rho=\frac{\textbf{Cov}(R(X),R(Y))}{\sigma(R(x))\cdot\sigma(R(Y))},
\end{equation}
where $\textbf{Cov}(R(X),R(Y))$ is the covariance between rank vectors $R(X)$ and $R(Y)$, while $\sigma(R(X))$ and $\sigma(R(Y))$ are their respective standard deviations. 

Spearman coefficient $\rho$ is a nonparametric measure for the alignment of the two vectors. Loosely speaking, the coefficient assesses if the increment in one variable corresponds to the increase (or decrease) of the other. In particular, unlike Pearson coefficient which takes into account the value of the changes, Spearman's $\rho$ takes into account only the sign of the changes. 

% In the special situation where all the entries of vector $X$ and $Y$ have different ranks, the equation \eqref{eq spearman general} simplifies to 
% \begin{equation}\label{eq spearman special}
%     \rho = 1 - \frac{6 \cdot\sum_{i=1}^n (R(x_i) - R(y_i))^2}{n\cdot(n^2 - 1)},
% \end{equation}
% where $x_i$ and $y_i$ are $i$th entries of $X$ and $Y$, respectively. 

% In our experiments, we consider a sequence of groups $G_j$, $j=1,\dots,5$ \ve{What is $j$ in our experiments?} defined as $G_1=\{1,\dots,40\}$ and $G_j=\{10^j-19,\dots,10^j+20\}$ for $j\geq 2$. In particular, the sets $G_j$ are disjoint and each has cardinality 40. Let $x_1,\dots,x_n$ be a sequence of all the elements in $\cup_{j=1}^5G_j$ and let $f_{LLM}(x_1),\dots,f_{LLM}(x_n)$ be their projections by the map $f_{LLM}$ defined in equation \eqref{eq projection p_T}. Then, in Section \ref{sec exp 1} we test the monotonic alignment between the sequences $(x_i)_i$ and $(f_{LLM}(x_i))_i$. 
% In particular, we empirically verify that there are no repeated ranks among the sequences (for sequence $(x_i)_i$ this is clear), and we can directly apply formula \eqref{eq spearman special}.  

\subsection{Scaling Rate Index}

For a monotonically increasing sequence of positive real numbers $x_1,\dots, x_n$, we introduce the Scaling Rate Index to measure the rate at which numbers grow in magnitude. More precisely, we seek for positive real constants $\alpha$ and $\beta$ that minimize the following objective function:
\begin{equation}\label{eq sri}
    \sum_{i=1}^{n-1}|(x_{i+1}-x_i) - \alpha\cdot \beta^i|^2.
\end{equation}

% For a general monotonically increasing sequence $x_i$, $i=1,\dots,n$, SRI is designed to measure the rate at which the sequence grows. 

In particular, if $\beta>1$, the difference between two consecutive members $x_i$ and $x_{i+1}$ is expected to increase with $i$. In other words, $x_{i+1}-x_{i}$ is expected to be smaller than $x_{i+2}-x_{i+1}$. Such sequences are commonly known as \textit{convex sequences} \cite{rockafellar2015convex} and the growth of $x_i$ is exponential (superlinear) in $i$. An example of such a sequence that is relevant to our study is that defined with $x_i:=10^i$. Then, $x_{i+1}-x_i= 9\cdot 10^{i}$ and we can take $\alpha=9$ and $\beta = 10$. 

If $\beta=1$, expression \eqref{eq sri} indicates that the difference between two consecutive members $x_i$ and $x_{i+1}$ is approximately constant, i.e. the sequence is approximately linearly increasing. For example, one may take the sequence $x_i:=i$, for which $\alpha=1=\beta$.

Finally, $\beta<1$ implies that the difference between consecutive members is steadily decreasing, $x_{i+1}-x_{i}$ is expected to be greater than $x_{i+2}-x_{i+1}$. Such sequences are commonly known as \textit{concave sequences} \cite{rockafellar2015convex}, and the growth of the sequence is exponentially decreasing (sublinear). An example of such a sequence is given by $x_i = 1- \frac{1}{10^i}$, with $\alpha = 9$ and $\beta = \frac{1}{10}$. 

% \subsection{Relation between SRI and logarithmic number line hypothesis}

% The logarithmic mental number line hypothesis \cite{dehaene2008log} states that humans innately space numbers on the number line in a logarithmic way. If we denote by $f$ such a map (spacing), then the hypothesis asserts that $f$ is approximately logarithmic map. 

% Of course, such a map $f$ can hardly be mathematically defined in a precise manner, but for our purposes, we use $f\equiv\log$  as a guiding principle in our experiments. 

% The \textit{logarithmic mental number line hypothesis} \cite{dehaene2008log} suggests that humans innately perceive numerical magnitudes on a logarithmic scale rather than a linear one. In other words, if we denote the internal mapping of numbers to their cognitive representation by \( f_h \), the hypothesis asserts that \( f_h \) is approximately a logarithmic function. While the exact form of \( f_h \) cannot be precisely defined, we adopt \( f_h \equiv \log \) as a guiding principle in our experiments.  

% To explore an analogous phenomenon in LLMs, we construct clusters of numbers that are spaced exponentially, with representative values centered around powers of ten, specifically \( 10^i \) for \( i = 1, \dots, 5 \) (i.e. set of numbers $G_i$). At each model layer, we extract the embeddings of these numbers and project them onto a one-dimensional real line using PCA. We then analyze the resulting projections to evaluate both their monotonicity and relative spacing, assessing whether LLMs implicitly organize numerical representations in a manner consistent with logarithmic scaling. 

% In particular, following equation \eqref{eq expected center} we take the expected projection of a group $G_i$, $\bar{p}_T(i) = \mathbb{E}_{x \in G_i} [p_T(x)],$ to be its representative and apply SRI analysis on the sequence $\bar{p}_T(i)$. We recall that the starting sequence of the centers of $G_i$, which is $x_i:= 10^i$, is convex with $\alpha=9$ and $\beta=10$, and it is growing exponentially. The equivalent of the above mapping $f_h$ then becomes $x_i\mapsto f_{LLM}(x_i):=\bar{p}_T(i)$. The main question is: What is the nature (logarithmic, linear, exponential) of the function $f_{LLM}$?

% When fitting the parameters $\alpha$ and $\beta$ for the sequence $\bar{p}_T(i)$, the results from Table \ref{tab:model_comparison} can be interpreted as follows.

% $\bullet{}$ $\beta>1$ implies that the sequence $\bar{p}_T(i)$ is convex and exponentially increasing. In particular, the function $f_{LLM}$ is mapping an exponentially increasing sequence to an exponentially increasing sequence $9\cdot 10^i\mapsto \alpha\cdot \beta^i=\alpha\cdot 10^{(\log_{10}\beta)\cdot i}$ and, as such, can be interpreted as preserving the spacing of the original numbers, albeit with some scaling factor in the form of $\log_{10}\beta$.

% $\bullet{}$ $\beta=1$ implies that the sequence $\bar{p}_T(i)$ is linearly increasing, and the map $f_{LLM}$ takes the form $10^i\mapsto \alpha\cdot i = \alpha\cdot \log_{10}10^i$. In particular, in this case we can say that $f_{LLM}$ is logarithmic. 

% $\bullet{}$ $\beta<1$ implies the sequence $\bar{p}_T(i)$ is concave, i.e. exponentially decaying. Then, $f_{LLM}$ is the mapping $10^i\mapsto \alpha\cdot \beta^i=\alpha\cdot 10^{-(\log_{10}\frac{1}{\beta})\cdot i}$, and in particular, $f_{LLM}$ can be interpreted as \textit{sub-logarithmic} map. 

% Based on our results from Table \ref{tab:model_comparison}, we conclude that LLMs perform compression of numbers on a scale that is even stronger than what the logarithmic mental number line hypothesis suggests for humans. Our results suggest that "mental number line" in LLMs is  sub-logarithmic. 


% \section{Logarithmic Mental Number Line in LLMs}

% The \textit{logarithmic mental number line hypothesis} \cite{dehaene2008log} suggests that humans innately perceive numerical magnitudes on a logarithmic scale rather than a linear one. Formally, if we denote the internal mapping of numbers to their cognitive representation as \( f_h \), the hypothesis asserts that \( f_h \) is approximately logarithmic. While the exact nature of \( f_h \) remains elusive, we adopt the guiding principle \( f_h \equiv \log \) in our experiments.  

% \subsection{Relation between SRI and logarithmic number line hypothesis}

% To investigate whether LLMs exhibit a similar phenomenon to human mental line hypothesis, we construct clusters of numbers spaced exponentially, with representative values centered around powers of ten, specifically \( 10^i \) for \( i = 1, \dots, 5 \) (denoted as sets \( G_i \)). At each model layer, we extract embeddings of these numbers and project them onto a one-dimensional real line using PCA. We then analyze the resulting projections to assess their monotonicity and relative spacing, determining whether LLMs encode numerical representations in a manner consistent with logarithmic scaling.  

% Following equation \eqref{eq expected center}, we take the expected projection of a group \( G_i \), defined as  
% \[
% \bar{p}_T(i) = \mathbb{E}_{x \in G_i} [f_{LLM}(x)],
% \]
% as its representative and apply SRI analysis to the sequence \( \bar{p}_T(i) \). The initial centers of \( G_i \), given by \( x_i = 10^i \), form a convex, exponentially growing sequence with parameters \( \alpha = 9 \) and \( \beta = 10 \). The analogous mapping in LLMs thus takes the form  
% \[
% x_i \mapsto f_{\text{LLM}}(x_i) := \bar{p}_T(i).
% \]
% The central question is: \textbf{What is the nature of the function \( f_{\text{LLM}} \) (logarithmic, linear, or exponential)}?  

% To determine this, we fit the parameters \( \alpha \) and \( \beta \) for the sequence \( \bar{p}_T(i) \) and analyze the results from Table \ref{tab:model_comparison_compact}:  

% \begin{itemize}
%     \item If \( \beta > 1 \), the sequence \( \bar{p}_T(i) \) is convex and exponentially increasing. This means \( f_{\text{LLM}} \) maps an exponentially increasing sequence to another exponentially increasing sequence:
%     \[
%     9 \cdot 10^i \mapsto \alpha \cdot \beta^i = \alpha \cdot 10^{(\log_{10} \beta) \cdot i}.
%     \]
%     Thus, \( f_{\text{LLM}} \) preserves the original spacing of numbers, albeit with a scaling factor \( \log_{10} \beta >0 \).  

%     \item If \( \beta = 1 \), the sequence \( \bar{p}_T(i) \) is linearly increasing, meaning \( f_{\text{LLM}} \) takes the form:
%     \[
%     10^i \mapsto \alpha \cdot i = \alpha \cdot \log_{10} 10^i.
%     \]
%     In this case, \( f_{\text{LLM}} \) exhibits logarithmic scaling.  

%     \item If \( \beta < 1 \), the sequence \( \bar{p}_T(i) \) is concave, exponentially decaying. Here, \( f_{\text{LLM}} \) follows:
%     \[
%     10^i \mapsto \alpha \cdot \beta^i = \alpha \cdot 10^{-(\log_{10} \frac{1}{\beta}) \cdot i}.
%     \]
%     This suggests that \( f_{\text{LLM}} \) is a \textit{sub-logarithmic} mapping.
% \end{itemize}

% From the results in Table \ref{tab:model_comparison_compact}, we conclude that LLMs compress numerical values more aggressively than what the logarithmic mental number line hypothesis predicts for humans. Specifically, our findings indicate that the "mental number line" in LLMs follows a \textit{sub-logarithmic} pattern.
 
