
% \citet{zhou2024scaling} \cite{levy2024language} \citet{zhu2025language} \citet{heinzerling2024monotonic}   \citet{godey2024scaling}   \citet{binz2023turning}   \cite{zhang2020language}.   \cite{park2022measurements}  \cite{zhang2020language}.  \citet{park2023linear}   \citet{gurneelanguage}   \cite{gurneelanguage}.  \cite{manvi2024large,roberts2023gpt4geo} 

 
% \noindent\textbf{Linearity vs. Logarithmic Fit} \\
% \ve{TODO: Make related works slightly shorter, by the length of the last paragraph for example}
% Linearity of internal representations \cite{park2023linear} has been a motivating assumption in existing research, leading to the view that language models represent numerical values in a linear manner \cite{zhu2025language}. However, \citet{zhu2025language} also present findings that suggest a more nuanced perspective. Their analysis of partial number encoding (Appendix F) indicates that language models encode numerical sequences incrementally, with probing accuracy declining as the sequence length increases. Notably, they observe increasing difficulty in capturing precise values as numerical scale grows, a pattern often associated with logarithmic encoding, where resolution is higher for smaller numbers and diminishes for larger ones. While their interpretation frames this as a challenge in numerical encoding, these observations align with our hypothesis that language models may internally represent numbers in a compressed, logarithmic manner, similar to cognitive biases in human numerical perception.  \\

Linearity of internal representations \cite{park2023linear} has been a central assumption in existing research, suggesting that language models encode numerical values in a linear manner. However, \citet{zhu2025language} present a more nuanced perspective. Their analysis of partial number encoding (Appendix F) shows that probing accuracy declines as sequence length increases, with greater difficulty in capturing precise values at larger scales, a pattern reminiscent of logarithmic encoding, where resolution is higher for smaller numbers. Some of the conclusions in \citet{zhu2025language} are that LLMs encode numerical values in their hidden representations, yet linear probes fail to precisely reconstruct these values, as discussed in \citet[Section 3.1]{zhu2025language}. The authors there suggest that ``This phenomenon may indicate that language models use stronger non-linear encoding systems''.  Our findings support this claim and further uncover the underlying nature of this non-linearity.

% The author proposed using $log(x)$ to address the training stability of the probe but did not consider that this might stem from inherently logarithmic scaling in the representations. Unlike their approach, we do not assume logarithmic scaling but instead measure it explicitly through subtle patterns in the data.



% \noindent\textbf{Representations in Base 10}  \\
% Works, such as \citet{levy2024language, zhou2024scaling}, study number representations in LLMs in dependence on the numerical basis. provide further support for this hypothesis, demonstrating that LLMs do not encode numbers in a continuous linear space but instead rely on base-10 digit-wise representations, as revealed through circular probing techniques. While individual digits can be accurately reconstructed, accuracy declines for larger numbers (1000, 2000), suggesting that numerical representations may be structured at the digit level rather than as holistic values. 
% However, the analysis is limited to numbers up to 2000 and does not extend to larger magnitudes, leaving open the question of whether different encoding patterns emerge at higher numerical scales. Additionally, causal intervention experiments focus only on numbers up to 999, meaning the ability to manipulate internal representations is tested only on relatively small values. These findings highlight the need for further investigation into whether encoding patterns shift at larger numerical scales. Moreover, the observed tendency for errors to cluster around multiples of 10 and 100 reinforces the idea that LLMs encode numbers with varying resolution across scales, a characteristic often associated with logarithmic compression.

% Further supporting this idea, \citet{zhou2024scaling} show that LLMs trained on numeral systems with higher bases (e.g., base 100, base 1000) struggle with numerical extrapolation compared to those trained in base 10. This suggests that models implicitly leverage a compressed numerical representation, where smaller values are encoded with finer granularity than larger ones—consistent with logarithmic scaling.  \\

Recent studies such as \citet{levy2024language, zhou2024scaling} show that LLMs rely on base-10 digit-wise representations rather than encoding numbers in a continuous linear space, as revealed through circular probing techniques. While individual digits are accurately reconstructed, performance declines for larger numbers, suggesting a structured rather than holistic encoding. Furthermore, \citet{zhou2024scaling} demonstrate that LLMs trained on higher-base numeral systems struggle with numerical extrapolation, implying an implicit compressed representation where smaller values have finer granularity—consistent with logarithmic scaling. Collectively, these results align with and further substantiate the hypothesis that LLMs internally represent numbers in a non-uniform, sublinear manner.

% \ve{This paragraph feels like it does not belong here} Research in cognitive science suggests that humans perceive numerical magnitudes using a logarithmic mental number line, where smaller numbers are represented with greater precision than larger ones \cite{dehaene2003neural}. This bias is particularly evident in children and untrained adults, who demonstrate sublinear spacing when estimating numerical distances. Our findings reveal a strikingly similar pattern in LLMs, where sublinearity emerges in projected numerical spaces. This suggests that LLMs may develop representations that mirror human cognitive biases, raising intriguing questions about their internal numerical reasoning mechanisms.

% Our study bridges these perspectives by demonstrating that while numerical representations in LLMs reside in a linear subspace, they exhibit systematic sublinearity, aligning with both recent findings in machine learning and established theories in cognitive science.
% \noindent\textbf{Local Linearity vs. Global Encoding} \\
% One possible reason for the assumption that LLMs represent numbers linearly is that logarithmic functions can appear linear over small local intervals. Any smooth function—including a logarithmic mapping—can be well approximated by a linear function within a sufficiently small $\varepsilon$-neighborhood. Thus, methods such as PLS regression and activation patching, which focus on small variations in activation space \cite{heinzerling2024monotonic}, may capture local monotonicity while missing the global nonlinear structure. This suggests that previously reported linear effects could be artifacts of analyzing narrow numerical ranges, and that a more comprehensive examination across broader scales may reveal an underlying logarithmic representation. Moreover, even within the local analyses of prior work, the results do not establish that LLMs represent numbers on a strictly linear scale. Linear predictability does not imply that numerical values are encoded with equal spacing, and the presence of monotonic directions only ensures order preservation, not necessarily uniform distance between values. Thus, while prior findings demonstrate that numerical properties can be modulated linearly, they do not contradict the possibility that LLMs internally encode numbers in a compressed, sublinear manner. \\


% \noindent\textbf{Representation vs. Reasoning}\\
% Prior works, such as \citet{park2022measurements} and \citet{zhang2020language}, explore whether language models align with human intuitions about numerical quantities, but differ fundamentally from our approach. \citet{park2022measurements} focus on Numerical Reasoning over Text (NRoT)—evaluating models' ability to process explicit numerical values rather than their internal representations. Their analysis includes tasks like Unit Conversion (e.g., recognizing that "3.5g" is equivalent to "3500mg") and Reference Range Detection (e.g., determining that "85mg/dL" falls within the normal blood glucose range). In contrast, Zhang et al. \cite{zhang2020language} investigate numerical magnitude within Common Sense Reasoning (CSR), testing models on number sequence completion (e.g., "10, 20, \_\_, 40"), scalar comparisons (e.g., "Is 500 larger than 50?"), and real-world estimation (e.g., "How heavy is a dog?"). While these studies assess models' ability to reason about explicit numbers, our work examines the spatial structure of numerical representations within hidden states and how this encoding generalizes across scales.

Logarithmic functions can appear linear over small local intervals, which may explain why LLMs are often assumed to represent numbers linearly. Consequently, methods like PLS regression and activation patching \cite{heinzerling2024monotonic,elshangiti2024geometrynumericalreasoninglanguage}, which analyze small activation variations, may capture local monotonicity while missing the global nonlinear structure. This suggests that reported linear effects could stem from analyzing narrow numerical ranges, whereas a broader examination may reveal an underlying logarithmic representation. 

Finally, we also emphasize the difference between our work and prior studies on numerical reasoning \cite{park2022measurements, zhang2020language} which evaluate models' ability to process explicit numbers rather than probing their internal representations. While \citet{park2022measurements} focus on tasks like unit conversion and range detection, \citet{zhang2020language} examine numerical magnitude in common sense reasoning. Unlike these works, our study investigates the spatial structure of numerical representations within hidden states and how this encoding generalizes across scales.