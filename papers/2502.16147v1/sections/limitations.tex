Although PCA reveals systematic sublinearity in numerical representations, the underlying mechanisms driving this compression remain unclear. While the observed patterns resemble the logarithmic scaling in human numerical cognition, it is uncertain whether LLMs develop similar abstractions or if the sublinearity arises from different architectural or training biases. Further research is needed to disentangle these factors and determine the extent to which LLMs replicate human-like numerical encoding.