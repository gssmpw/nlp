\section{Related works}
\textbf{Related works:} Given the popularity and simplicity of TD learning, the work on the topic is substantial. Below we will cover a subset of recent findings.
\begin{itemize}
    \item \cite{samsonov24} establishes high-probability bounds for TD-learning with linear function approximation. This was achieved under geometric ergodicity of the markov process and data dropping. By Data dropping \cite{samsonov24} no longer follows practical use of TD(0), which does not data drop, and requires knowing the mixing constant beforehand. In contrast, we relax the geometric assumption to polynomial, do not data drop, do not need to know mixing rates, and extend beyond linear.
    \item \cite{li2024high} This work establishes high-probability bounds and sample complexity results for the TD(0) learning algorithm, extending the analysis to its off-policy counterpart (TDC) under the i.i.d. sampling assumption. The authors address the stability of random matrix products, a crucial aspect in the convergence analysis of TD learning algorithms. Their results offer insights into the convergence behavior of TD methods, with implications for the choice of step sizes in practical implementations. Our method is a proof of TD(0) convergence under universal and instance-independent step-size selection, regardless of Markovian dependencies. Li et al., while analyzing convergence in off-policy settings, do not address this generality and require step-size tuning based on problem-specific parameters.
    \item \cite{patil2023finite} studies the finite-time behavior of the TD learning algorithm when combined with tail-averaging and regularization. The authors derive finite-time bounds on the parameter error of the tail-averaged TD iterate under a step-size choice that does not require information about the eigenvalues of the matrix underlying the projected TD fixed point. Their analysis shows that tail-averaged TD converges at the optimal \(O(1/t)\) rate, both in expectation and with high probability, and exhibits a sharper rate of decay for the initial error compared to averaging all iterates. Additionally, they propose and analyze a regularized variant of TD, concluding that it is beneficial for problems with ill-conditioned features. In contrast, for out linear and non-linear models we do not require projection.
    \item \cite{asadi2024td} analyzed the convergence of temporal-difference (TD) learning through an optimization lens, providing a theoretical foundation for linear TD learning under geometric mixing and specific loss functions. However, their analysis relied on restrictive assumptions such as geometric ergodicity, instance-dependent step sizes, and algorithmic modifications like data dropping. In contrast, this work extends the theoretical guarantees of TD(0) to more general settings, including polynomial mixing, universal step sizes, and non-linear function approximation.
\end{itemize}
\begin{table*}[t]
    %\centering
    \makebox[\linewidth][l]{
    \hspace{-3cm}
    \renewcommand{\arraystretch}{1.2} % Optional: Increases row height
    \begin{tabular}{lccccccc}
        \toprule
        \textbf{Authors} & B. (\citeyear{bhandari18}) & D. (\citeyear{dalal2018finite}) & L. and S.(\citeyear{lakshminarayanan2018linear}) & P. (\citeyear{patil2023finite}) & L.(\citeyear{li2024high}) & S. (\citeyear{samsonov24}) & \textbf{This paper} \\
        \midrule
        \textbf{Algorithm type} & P-R & Last iterate & P-R & P-R & P-R & P-R & Last iterate \\
        \textbf{Step size schedule} & $1/\sqrt{t}$ & $1/k^\kappa$ & constant $\alpha$ & constant $\alpha$ & constant $\alpha$ & constant $\alpha$ & $1/t^\eta, \eta \in \left(\tfrac{1}{2},1\right]$ \\
        \textbf{Universal step size} & \cmark & \cmark & \cmark & \cmark & \xtimes & \cmark & \cmark \\
        \textbf{Markovian data} & \cmark & \xtimes & \xtimes & \cmark & \xtimes & \cmark & \cmark \\
        \textbf{High-order bounds} & \xtimes & \cmark & \xtimes & \cmark & \cmark & \cmark & \cmark \\
        \textbf{No projection} & \xtimes & \cmark & \cmark & \xtimes & \cmark & \cmark & \cmark\\
        \textbf{Mixing properties} & Geometric & \xtimes & \xtimes & Geometric & \xtimes & Geometric & Polynomial\\
        \textbf{Function} & Linear & Linear & Linear & Linear & Linear & Linear & Non-Linear\\
        \textbf{No Data Dropping} & \cmark & \cmark & \cmark & \cmark & \cmark & \xtimes  & \cmark \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of error bounds for TD(0) algorithm.}
    \label{tab:wide_table}}
\end{table*}
%\include{figs_tables/comparisons}
\paragraph{Polynomial versus Geometric Mixing Rates}
Geometric mixing is a stronger condition than polynomial mixing.
Under geometric mixing, correlations between samples from the markov chain decay exponentially.
As a result, previous work could simply wait a few samples, then data points would become i.i.d.
However, many real-world systems, such as ones with bottlenecks, sparse reward, and bottlenecks do not express geometric mixing and are better described with the more relaxed polynomial mixing.
Thus, we found it important to proof under this condition for applicability.