\section{Problem Setup}

In this section, we define the Markov Decision Process (MDP), Temporal Difference (TD) learning framework, and key quantities relevant to our analysis. The focus is on a general TD-learning formulation applicable to non-linear function approximation, including neural networks.

\subsection{Markov Decision Reward Processes (MDRPs)}
We consider an MDRP defined as a tuple \((\mathcal{S}, \mathcal{A}, P, R, \gamma)\), where:
\(\mathcal{S}\): State space, assumed to be finite or countably infinite,\(\mathcal{A}\): Action space,
\(P(s'|s, a)\): Transition probability function, giving the probability of transitioning to state \(s'\) from \(s\) under action \(a\),
\(R(s, a)\): Reward function, defining the expected reward \(r = \mathbb{E}[R(s, a)]\) for taking action \(a\) in state \(s\),
\(\gamma \in [0, 1)\): Discount factor, determining the relative weight of future rewards.

Given a stationary policy \(\pi(a|s)\) mapping states to a probability distribution over actions, the MDRP induces a Markov reward process (MRP) with transitions defined by the marginal probability \(P_\pi(s'|s) = \sum_a P(s'|s, a)\pi(a|s)\) and rewards \(R_\pi(s) = \sum_a R(s, a)\pi(a|s)\).

\begin{assumption}[Bounded Reward]
\label{ass:bounded-reward}
The reward function \(R(s, a)\) is uniformly bounded, i.e., there exists a constant \(R_{\text{max}} > 0\) such that for all \(s \in \mathcal{S}\) and \(a \in \mathcal{A}\),
\[
|R(s, a)| \leq R_{\text{max}}.
\]
Which leads to a bounded value functions as these are dependent on an exponentially decreasing summation of rewards.
\end{assumption}

\subsection{Temporal Difference Learning}
The goal is to estimate the value function \(V^\pi(s)\), defined as:
\[
V^\pi(s) = \mathbb{E}\left[ \sum_{t=0}^\infty \gamma^t r_{t+1} \,\Big|\, s_0 = s \right],
\]
where \(r_{t+1}\) is the reward observed at time \(t+1\).

\noindent
We approximate \(V^\pi(s)\) using a parameterized function \(f_\theta(s)\), where \(\theta \in \mathbb{R}^d\) represents the learnable parameters. The TD(0) algorithm updates \(\theta\) iteratively based on a temporal difference (TD) error:
\begin{equation}\label{eq:td(0)}
    \theta_{t+1} = \theta_t - \alpha_t F(\theta_t, x_t),
\end{equation}
where \(x_t\) is the state (or state-action pair) observed at time \(t\), \(\alpha_t > 0\) is the step size at iteration \(t\), \(F(\theta, x)\) represents a (sub)gradient of the TD objective, incorporating both the observed reward and the temporal-difference error.

The function \(F(\theta, x)\) encapsulates the dependence of the TD update on the non-linear parameterization of \(f_\theta(s)\). For instance, it is often defined as:
\begin{equation}\label{def:F}
    F(\theta, x) = -\delta_t \nabla_\theta f_\theta(s),
\end{equation}
where:
\begin{equation}\label{def:delta}
\delta_t = r_{t+1} + \gamma f_\theta(s_{t+1}) - f_\theta(s_t).    
\end{equation}
\noindent
This formulation accommodates non-linear function approximators, such as neural networks, where \(f_\theta(s)\) may involve complex architectures, and \(F(\theta, x)\) reflects the backpropagated gradient or subgradient of the TD objective.

\subsection{Key Definitions and Quantities}

\paragraph{TD Error:} The error at time \(t\) is defined as:
\[
e_t = \theta_t - \theta^*,
\]
where \(\theta^*\) is the fixed point of the TD(0) update, satisfying the stationarity condition:
\begin{assumption}[Stationary Solution]
\label{ass:stationary-solution}
There is a parameter vector \(\theta^*\) satisfying
\[
    \mathbb{E}\bigl[F(\theta^*,\,x)\bigr] \;=\; 0.
\]
\textit{(In general, \(\theta^*\) may be a local fixed point if the objective is non-convex.)}
\end{assumption}
under the stationary distribution of the Markov chain induced by the policy \(\pi\).

%\paragraph{High-Probability Bound:} A high-probability bound for a sequence of random variables \(\{X_t\}_{t \geq 1}\) states that there exist constants \(C_\delta > 0\) and \(\omega > 0\) such that, for any \(\delta \in (0, 1)\),
%\[
%\mathbb{P}\left( \|X_t\| \leq C_\delta t^{-\omega}, \, \forall t \geq t_0 \right) \geq 1 - \delta,
%\]
%where \(t_0\) is the smallest time for which the bound applies, and \(C_\delta\) may depend logarithmically on \(\delta\).

%\paragraph{Goal of paper: high-probability bounds}
%Our main theorems are: high-probability and high-order moment bounds of TD(0) under linear function approximation (Theorem \ref{theorem:hom-linear-bounds}, \ref{theorem:hp-linear-bounds}, ) and non-linear function approximiation (Theorem \ref{theorem:bounds-non-linear-td}).
%For each theorem, we will develop the necessary mathematical understanding and assumptions prior to formulating the proof. Due to space constraints, most proof details are deferred to appendix with brief proof sketches in main text.