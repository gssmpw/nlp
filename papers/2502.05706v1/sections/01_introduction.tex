\section{Introduction}
Temporal Difference (TD) learning is a fundamental technique in reinforcement learning, enabling agents to estimate value functions and make informed decisions in complex environments. Despite its widespread adoption, theoretical guarantees for TD algorithms, especially in more realistic and challenging settings, remains an open challenge.

The TD algorithms \cite{sutton1988learning,sutton2018reinforcement} are methods for approximating the true value function, \(V_{\pi}(s)\), of a Markov Decision Process (MDP) under a policy $\pi$, with approximation $\hat{V}_{\pi}(s)$.
We investigate the semi-gradient TD(0) (ref. TD) update rule, which for a linear function is:
\begin{align*}
\hat{V}_{\pi}(s) &=  f_\theta(\phi(s))\\
    \theta_k &\gets \theta_{k-1} + \alpha \left[ r_{t+1} + \gamma f_{\theta_{k-1}}(\phi(s_k))\right]\phi(s_k)
\end{align*}
where \(s_0, r_1, s_1, \dots \) is generated by following $\pi$ in the MDP, \(\phi(\cdot) \in \mathbb{R}^d\) is a linear feature map, $f_\theta$ is a function approximator with $\theta\in\mathbb{R}^d$ trainable weights.
TD variants, such as Q-learning \cite{watkins1992q} and PPO \cite{schulman2017proximal}, are frequently used to optimize deep learning systems.
Better theoretical understanding could provide an insight to the success of modern AI algorithms.

We achieve convergence of TD under various settings:
\begin{itemize}
    \item[\ding{72}] Linear function approximation: Convergence on Markov data without modifying the algorithm or relying on instance-dependent step sizes. 
    \item[\ding{72}] Non-linear function approximation: Convergence under HÃ¶lder continuity and generalized gradients.
\end{itemize} 
These results are all obtained on Markovian data without any algorithmic modifications and with instance-independent step sizes.

Using our method, we can bound the convergence rate of the TD algorithm as follows:
\begin{itemize}
    \item Linear: \(O(t^{-\beta/2})\)
    \item Non-linear (generalized gradiets): \(O(t^{-\beta/2} + t^{-\eta\gamma})\)
\end{itemize} 
Here $\beta>1$ represents the polynomial mixing coefficient, which characterizes the rate at which the Markov chain converges to its stationary distribution.
$\gamma\in(0,1)$ the discount factor, and $\eta=\in \left(\tfrac{1}{2},1\right]$ is the step-size decay.

Previous theoretical findings have only established convergence of TD in practical settings under under i.i.d. data sampling \cite{tsitsiklis1996analysis, samsonov24}.
Recent work addressing Markovian noise has provided high-probability bounds and sample complexity under restrictive conditions such as unknown variables at runtime \cite{li2024high} ($\theta^*$), the necessity of projections for stability \cite{korda2015td, patil2023finite}, or data dropping combined with geometric ergodicity to ensure approximate i.i.d. conditions \cite{samsonov24}.
Consequently, existing approaches either modify the TD(0) algorithm (e.g., data dropping or projections) or assume learning rates that are difficult to determine during runtime, alongside stringent Markov noise profiles.

\textbf{Contributions:} We provide statistical guarantees for the TD(0) algorithm under instance-independent step-size, polynomial mixing, h\"{o}lder continuity, and bounded gradients.
To our knowledge, these are the first results addressing polynomial Markov chains in the context of TD learning. For linear functions, our bounds are the first to ensure convergence on Markov chains without unknown parameters or algorithmic modifications. Moreover, this work represents the first theoretical analysis of non-linear functions in TD(0) learning.

%Previous state-of-the-art theoretical findings are for linear functions under geometric markov chains and requires either data-dependent learning rates \cite{li2024high} or modifying the algorithm to drop large quantities of data \cite{samsonov24}.
%Extending to non-linear functions and Deep ReLU networks we further assume either generalized gradients or bounded weights. Specifically, we
%\begin{itemize}
%    \item Relax all markov chains to polynomial and all continuity arguments to H\"older.
%    \item Linear functions: we provide High-order moment bounds \& high-probability bounds of TD(0) under minimal assumptions: no modifications and instance-indepedent learning rates.
%    \item Non-linear: adding assumptions of generalized gradients and bounded sub-gradients we provide high-probability bounds.
 %   \item Deep ReLU Network: adding assumption of bounded weights and bounded input, we can bound the gradient and achieve High-probability bounds.
%\end{itemize}