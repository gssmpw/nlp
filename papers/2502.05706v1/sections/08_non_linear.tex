\section{TD(0) High-probability Error Bounds for Non-Linear Function Approximation}

\paragraph{Non-linear function approximator (generalized gradients).}
Consider a non-smooth value function \(f_\theta\) parameterized by \(\theta\in\mathbb{R}^d\), for which the usual derivative might not exist everywhere. Instead, we rely on the concept of \emph{generalized gradients} \ref{def:generalized-gradients} and a subgradient variant of h\"older continuity \ref{ass:subgrad-holder}.

\paragraph{Goal:} Our objective is to extend the high-probability error bounds of TD(0) to the case of non-linear function approximation under generalized smoothness conditions. We introduce generalized subgradients and leveraging smoothness assumptions such as H\"older continuity a.e. (Assumption~\ref{ass:subgrad-holder}) and bounded subgradients (Assumption~\ref{ass:bounded-subgrad}). These modifications preserve the core martingale-plus-remainder decomposition and enable the derivation of high-probability convergence guarantees, similar to those established for linear and piecewise-linear (e.g., ReLU-based) models.


\subsection{Generalized Gradients}
\begin{definition}[Generalized Gradients]
\label{def:generalized-gradients}
For a non-linear function \(f(\theta)\), the generalized gradient is defined as:
\[
\partial f(\theta) = \{v \in \mathbb{R}^d : f(\theta') \geq f(\theta) + v^\top (\theta' - \theta) \text{ for all } \theta'\}.
\]
\end{definition}
\vspace{1em}
\begin{assumption}[Subgradient/H\"older Continuity (Almost Everywhere)]
\label{ass:subgrad-holder}
For each \(x\), the map \(\theta \mapsto F(\theta,x)\) (where \(F\) is the \emph{sub}gradient of the function based TD objective)  
is H\"older continuous with exponent \(\gamma \in (0,1]\) \emph{almost everywhere}:
\[
    \|F(\theta,x) - F(\theta',x)\| \;\le\; L\,\|\theta - \theta'\|^\gamma,
\]
for all \(\theta,\theta'\) \emph{except possibly on a measure-zero set} where ReLU is not differentiable.
\end{assumption}
\vspace{1em}
\begin{assumption}[Bounded Subgradients]
\label{ass:bounded-subgrad}
There exists \(G>0\) such that \(\|F(\theta,x)\| \le G\) for all \(\theta,x\).  
\textit{(This can be ensured if network weights remain in a bounded set.)}
\end{assumption}

\subsection{Error Bounds for Non-Linear TD}

\begin{theorem}[Non-linear Error Bounds]
\label{theorem:bounds-non-linear-td}
Under polynomial ergodicity \ref{theorem:polynomial-ergodicity}, coupling \ref{lemma:coupling-argument-polynomial-mixing}, bounded subgradients \ref{ass:bounded-subgrad}, Hölder continuity (almost everywhere) \ref{ass:subgrad-holder}, and generalized gradients \ref{def:generalized-gradients}, then we find that the TD error satisfies:
\[
\| \theta_t - \theta^* \| \leq C t^{-\beta/2} + C' \alpha_t^\gamma.
\]
\end{theorem}

\begin{proof}
We establish high-probability bounds for non-linear TD(0) under polynomial mixing, bounded subgradients, and (optionally) H\"older continuity. The iterate error decomposes as
\[
\theta_t - \theta^* = M_t + R_t,
\]
where \(M_t\) is a martingale difference term and \(R_t\) is the remainder.

1. \emph{Martingale Term \(M_t\):} Using Freedman’s inequality and polynomial mixing, \(M_t\) is controlled with high probability, leading to a bound of \(O(t^{-\beta/2})\), where \(\beta > 1\) is the mixing exponent.

2. \emph{Remainder Term \(R_t\):} If \(\overline{F}(\theta)\) is monotone, \(R_t = O(\sum \alpha_k^2)\). With H\"older continuity (\(\gamma\)), we refine this to \(O(t^{-\eta\gamma})\).

Applying a union bound, we obtain the final rate:
\[
\|\theta_t - \theta^*\| = O(t^{-\beta/2}) + O(t^{-\eta\gamma}).
\]
Capturing both stochastic averaging and bias decay due to step size. Details in Appendix \ref{app:proof-bounds-non-linear-td-merged}.
\end{proof}
