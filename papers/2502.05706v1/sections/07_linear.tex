\section{TD(0) High-probability Error Bounds for Linear Functions}

\paragraph{Linear function approximation.}
Consider a linear function approximation of the form
\begin{align*}
    f_\theta &= W^{(1)}\,x + b^{(1)}\,
\end{align*}
where
\(\theta = \bigl\{\,W^{(1)},\,b^{(1)}\bigr\}\)  
are the trainable parameters. 

\paragraph{Goal:}
We derive high-probability bounds for the convergence of the error \(\|e_t\| = \|\theta_t - \theta^*\|\) in TD learning. Specifically, we establish that \(\|e_t\|\) diminishes under polynomial ergodicity \ref{theorem:polynomial-ergodicity}, bounded variance of the martingale term \ref{lemma:bounded-variance-martingale}, and step-size \ref{ass:step-size-decay} for high-probability bounds.
Our results provide guarantees on the moment bounds and error decay rates, highlighting the role of step size and mixing conditions in determining convergence behavior.

\subsection{Moment Bounds}

\begin{theorem}[High-Order Moment Bounds, Linear Function]
\label{theorem:hom-linear-bounds}
For any \(p \geq 2\), the \(p\)-th moment of the TD error satisfies:
\[
\mathbb{E}[\|\theta_t - \theta^*\|^p]^{1/p} \leq C t^{-\beta/2} + C' \alpha_t^{\gamma/p}.
\]
\end{theorem}

\begin{proof}
By leveraging Lemma \ref{lemma:ho-error-bounds}, which provides high-order error bounds, and utilizing properties of martingale differences alongside an induction-based discrete Grönwall argument, we can derive the convergence rates. The approach systematically bounds each component of the parameter error recursion, ensuring that the \( p \)-th moment of the TD(0) parameter error decays at a rate influenced by the mixing properties of the underlying Markov chain and the smoothness of the value function approximation.
For the detailed analysis, see Appendix \ref{app:proof-hom-linear-bounds}
\end{proof}

\subsection{High-Probability Bounds, Linear Function}

\begin{theorem}[High-Probability Bounds]
\label{theorem:hp-linear-bounds}
Under the assumptions: polynomial ergodicity \ref{theorem:polynomial-ergodicity}, TD(0) error decomposition \ref{theorem:td-error-decomposition}, Bounded Variance of the Martingale Term \ref{lemma:bounded-variance-martingale}, Step Size Condition \ref{ass:step-size-decay}, Concentration Property \ref{theorem:concentration-block}. With probability at least \(1 - \delta\), the TD error satisfies:
\[
\|\theta_t - \theta^*\| \leq C_\delta t^{-\beta/2} + C'_\delta \alpha_t^\gamma,
\]
where \(C_\delta\) and \(C'_\delta\) depend logarithmically on \(1/\delta\).
\end{theorem}

\begin{proof}
We decompose the error as \(\theta_t - \theta^* = M_t + R_t\), where \(M_t\) is a martingale term capturing stochastic fluctuations and \(R_t\) is a remainder term controlled via step size conditions and mixing properties.  

Using the bounded variance assumption (Lemma \ref{lemma:bounded-variance-martingale}), we apply Azuma–Hoeffding’s inequality to bound \(M_t\) with high probability. Given a step size schedule \(\alpha_k = \mathcal{O}(k^{-\beta})\), the variance term satisfies \(\sum_{k=1}^t \alpha_k^2 = \mathcal{O}(t^{-\beta})\), leading to  
\[
\mathbb{P}(\|M_t\| > \epsilon) \leq 2 \exp\left(-\frac{\epsilon^2}{C t^{\beta}}\right).
\]
Setting \(\epsilon = C_\delta t^{-\beta/2}\) ensures probability at least \(1 - \delta/2\).  

For \(R_t\), polynomial mixing and Hölder continuity yield \(\|R_t\| = \mathcal{O}(\alpha_t^\gamma)\), holding with probability at least \(1 - \delta/2\). Applying a union bound, we conclude  
\[
\|\theta_t - \theta^*\| \leq C_\delta t^{-\beta/2} + C'_\delta \alpha_t^\gamma
\]
with probability at least \(1 - \delta\). The constants depend logarithmically on \(1/\delta\), completing the proof.  
Details in~\ref{app:proof-hp-linear-bounds}
\end{proof}