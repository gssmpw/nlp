\begin{abstract}
%What’s the domain?
Theoretical work on Temporal Difference (TD) learning has provided finite-sample and high-probability guarantees for data generated from Markov chains.
%What’s the issue?
However, these bounds typically require linear function approximation, instance-dependent step sizes, algorithmic modifications, and restrictive mixing rates.
%What’s your contribution?
We present theoretical findings for TD learning under more applicable assumptions, including instance-independent step sizes, full data utilization, and polynomial ergodicity, applicable to both linear and non-linear functions.
%Why is it novel?
\textbf{To our knowledge, this is the first proof of TD(0) convergence on Markov data under universal and instance-independent step sizes.}
%What’s interesting about it?
While each contribution is significant on its own, their combination allows these bounds to be effectively utilized in practical application settings.
%How does it perform? 
Our results include bounds for linear models and non-linear under generalized gradients and Hölder continuity.
%Thus, under UGE Markov Noise, TD(0) enjoys the same $n^{-1/2}$-type high probability behaviour as in the i.i.d. scenario.

%This document provides a rigorous analysis of high-order error bounds for Temporal Difference (TD) learning while using minimal assumptions. Unlike existing work, we replace strong smoothness conditions with Hölder continuity and generalized gradients, stability with polynomial mixing, and we handle data dependencies without resorting to data dropping. The framework addresses non-smooth dynamics and function approximation in Markovian settings. Results are extended to handle non-linear function approximation, and explicit treatments of dependencies are presented through coupling and block analysis. These contributions aim to answer open questions about TD learning while ensuring practical applicability under weaker conditions.
\end{abstract}