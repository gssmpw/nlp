\section{Handling Data Dependencies}
\label{sec:handling-data-dependencies}

In practical reinforcement learning (RL), data points generated by a Markov process exhibit dependencies. These dependencies can complicate the analysis and convergence properties of learning algorithms. To rigorously handle these dependencies, we employ coupling and block techniques. This section explicitly analyzes data dependencies under the assumption of polynomial ergodicity, leading to meaningful concentration results.

%\subsection{Mixing Rates: Polynomial vs. Geometric}

%\textbf{Polynomial Mixing:} A Markov chain is said to exhibit \emph{polynomial mixing} if the total variation distance to its stationary distribution decays at a polynomial rate. Formally, for a Markov chain \(\{x_t\}\) with stationary distribution \(\pi\), this is expressed as:
%\[
%\left\| \mathbb{P}(x_{t+s} \in \cdot \mid x_t = x) - \pi(\cdot) \right\|_{\text{TV}} \leq C s^{-\beta}, \quad \forall s \geq 1,
%\]
%where \(C > 0\) and \(\beta > 1\). The parameter \(\beta\) characterizes the mixing rate within this framework.

%\textbf{Geometric (Exponential) Mixing:} A Markov chain exhibits \emph{geometric mixing} if the total variation distance decays exponentially fast:
%\[
%\left\| \mathbb{P}(x_{t+s} \in \cdot \mid x_t = x) - \pi(\cdot) \right\|_{\text{TV}} \leq C \rho^s, \quad \forall s \geq 1,
%\]
%where \(C > 0\) and \(0 < \rho < 1\).

%It is important to note that geometric mixing is a stronger condition than polynomial mixing. While polynomial mixing with \(\beta > 1\) indicates a relatively faster convergence within the polynomial framework, it does not equate to the exponentially fast convergence of geometric mixing.

\subsection{Dependent Block Analysis}

\begin{lemma}[Dependent Blocks]
\label{lemma:dependent-blocks}
Let \(\{x_t\}_{t=1}^\infty\) be a Markov chain satisfying the polynomial ergodicity condition as specified in Theorem \ref{theorem:polynomial-ergodicity} with rate \(\beta > 1\). Partition the sequence into non-overlapping blocks of size \(b\):
\[
B_k = (x_{(k-1)b + 1}, \ldots, x_{kb}), \quad k = 1, 2, \ldots, \left\lfloor \frac{n}{b} \right\rfloor.
\]
For a sufficiently large block size \(b\), the blocks \(\{B_k\}\) satisfy:
\[
\left\| \mathbb{P}(B_k \mid B_{k-1}) - \mathbb{P}(B_k) \right\|_{\text{TV}} \leq C b^{-\beta},
\]
where \(C > 0\) and \(\beta > 1\) are constants derived from the polynomial ergodicity condition.
\end{lemma}

\begin{proof}
Given the polynomial ergodicity condition, for any \(s \geq 1\),
\[
\left\| \mathbb{P}(x_{t+s} \in \cdot \mid x_t = x) - \pi(\cdot) \right\|_{\text{TV}} \leq C s^{-\beta},
\]
where \(\pi\) is the stationary distribution of the Markov chain.

Consider two consecutive blocks \(B_{k-1}\) and \(B_k\). The dependency between these blocks is encapsulated in the transition from \(x_{(k-1)b}\) to \(x_{kb}\). Due to the Markov property, the distribution of \(B_k\) given \(B_{k-1}\) depends only on \(x_{(k-1)b}\).

Applying the ergodicity condition to the transition from \(x_{(k-1)b}\) to \(x_{kb}\), we have:
\begin{align*}
\left\| \mathbb{P}(B_k \mid B_{k-1}) - \mathbb{P}(B_k) \right\|_{\text{TV}} &= \left\| \mathbb{P}(B_k \mid x_{(k-1)b}) - \mathbb{P}(B_k) \right\|_{\text{TV}}\\
&\leq C b^{-\beta}.
\end{align*}
This inequality holds because the dependence between the blocks diminishes as the block size \(b\) increases, aligning with the polynomial decay rate specified by \(\beta\).

Thus, for sufficiently large \(b\), the blocks \(\{B_k\}\) are approximately independent with the total variation distance between their conditional and marginal distributions bounded by \(C b^{-\beta}\), formal details are in Appendix \ref{app:proof-block-independence-approximation}.
\end{proof}

\subsection{Coupling for Data Dependencies}

\begin{lemma}[Coupling Argument under Polynomial Mixing]
\label{lemma:coupling-argument-polynomial-mixing}
Let \(\{x_t\}_{t=1}^\infty\) and \(\{y_t\}_{t=1}^\infty\) be two Markov chains satisfying polynomial ergodicity as per Theorem \ref{theorem:polynomial-ergodicity} with rates \(\alpha_x\) and \(\alpha_y\), respectively. Assume both chains satisfy the block independence condition from Lemma \ref{lemma:dependent-blocks}. There exists a coupling such that for all \(t \geq 1\),
\[
\mathbb{P}(x_t \neq y_t) \leq C t^{-\beta},
\]
where \(C > 0\) and \(\beta = \min\{\alpha_x, \alpha_y\}\).
\end{lemma}

\begin{proof}
Construct the coupling by initializing both chains \(\{x_t\}\) and \(\{y_t\}\) independently. At each time step \(t\), generate a common random variable \(U_t\) (e.g., uniformly distributed on \([0,1]\)) to drive the transitions of both chains.

Define the coupling as follows:
\[
x_{t} = f(x_{t-1}, U_t), \quad y_{t} = f(y_{t-1}, U_t),
\]
where \(f\) is the transition function of the Markov chain.

The probability that \(x_t \neq y_t\) given \(x_{t-1} \neq y_{t-1}\) is bounded by the total variation distance between the transition kernels:
\begin{align*}
&\mathbb{P}(x_t \neq y_t \mid x_{t-1}, y_{t-1}) \leq\\
&\quad\quad\quad\quad\left\| \mathbb{P}(x_t \in \cdot \mid x_{t-1})- \mathbb{P}(y_t \in \cdot \mid y_{t-1}) \right\|_{\text{TV}}.
\end{align*}
By the polynomial ergodicity condition,
\[
\left\| \mathbb{P}(x_t \in \cdot \mid x_{t-1}) - \mathbb{P}(y_t \in \cdot \mid y_{t-1}) \right\|_{\text{TV}} \leq C t^{-\beta}.
\]
Applying induction, we obtain:
\[
\mathbb{P}(x_t \neq y_t) \leq C t^{-\beta}.
\]
This completes the coupling argument under polynomial mixing, details are in Appendix \ref{app:proof-coupling-argument-polynomial-mixing}
\end{proof}

\subsection{Covariance Between Blocks under Polynomial Ergodicity}
%% Cov Proof

\begin{lemma}[Covariance Between Blocks under Polynomial Ergodicity]
\label{lemma:covariance-between-blocks}
Let \(\{x_t\}_{t=1}^\infty\) be a Markov chain satisfying the polynomial ergodicity condition with rate \(\beta > 1\), as defined in Theorem \ref{theorem:polynomial-ergodicity}. Partition the sequence into non-overlapping blocks of size \(b\):
\[
B_k = (x_{(k-1)b + 1}, \ldots, x_{kb}), \quad k = 1, 2, \ldots, K,
\]
where \(K = \left\lfloor \frac{n}{b} \right\rfloor\). Let \(Y_k = \sum_{t=(k-1)b + 1}^{kb} x_t\) denote the sum of the \(k\)-th block.

Then, for any two distinct blocks \(B_k\) and \(B_j\) with \(k \neq j\), the covariance between their sums satisfies:
\[
|\text{Cov}(Y_k, Y_j)| \leq C b^2 |k - j|^{-\beta},
\]
where \(C > 0\) is a constant depending on the mixing parameters of the Markov chain.
\end{lemma}

\begin{proof}
Details can be found in Appendix \ref{app:proof-covariance-between-blocks}
\end{proof}

\subsection{Concentration Under Polynomial Ergodicity}

\begin{lemma}[Concentration Under Polynomial Ergodicity]
\label{theorem:concentration-block}
Let \(\{x_t\}_{t=1}^n\) be a sequence of R.V.s generated by a Markov chain under polynomial ergodicity per Theorem \ref{theorem:polynomial-ergodicity} with rate \(\beta > 1\). Assume that each \(x_t\) has bounded variance, i.e., \(\text{Var}(x_t) \leq \sigma^2\) for some constant \(\sigma^2 > 0\), and that the rewards are bounded, i.e., \(|x_t| \leq M\) almost surely for some constant \(M > 0\). Then, for any \(\epsilon > 0\),
\[
\mathbb{P}\left(\left|\frac{1}{n}\sum_{t=1}^n x_t - \mathbb{E}[x_t]\right| > \epsilon\right) \leq 2\exp\left(-\frac{n\epsilon^2}{2C_\beta}\right),
\]
where \(C_\beta\) depends on the mixing rate \(\beta\) and the bound \(M\).
\end{lemma}

\begin{proof}
Through Bernstein's inequality  \cite{vershynin2018high}, we can bound sub-gaussian depedent sequences to obtain our concentration bound, details  can be found in Appendix \ref{app:proof-concentration-block}
\end{proof}