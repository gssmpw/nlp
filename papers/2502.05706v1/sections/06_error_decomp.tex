\section{Error Decomposition}
\label{sec:error-decomposition}

In this section, we analyze the error decomposition of the Temporal Difference (TD) learning algorithm. Specifically, we focus on decomposing the TD error into a martingale term and a remainder term, facilitating a deeper understanding of the convergence properties of the algorithm under certain stochastic conditions.

\subsection{Preliminaries and Assumptions}

Before delving into the proof of the key lemma, we establish the necessary assumptions and definitions that underpin our analysis.

\begin{assumption}[Step-Size Decay]
\label{ass:step-size-decay}
The step sizes \(\{\alpha_k\}\) are defined as
\[
\alpha_k \;=\; \frac{C_\alpha}{k^\eta},\quad\sum_{k=1}^{\infty} \alpha_k = \infty,
\quad\text{and}\quad
\sum_{k=1}^{\infty} \alpha_k^2 < \infty.
\]
where \(C_\alpha > 0\) is a universal constant, and \(\eta \in \left(\tfrac{1}{2},1\right]\).
\end{assumption}

\begin{assumption}[Hölder Continuity of Feature Mapping]
\label{ass:holder-continuous-features}
The feature mapping \( \phi: S \to \mathbb{R}^d \) is Hölder continuous with exponent \( \gamma \in (0, 1] \) and constant \( C_\gamma > 0 \). Formally, for all \( s, s' \in S \),
\[
\|\phi(s) - \phi(s')\| \leq C_\gamma \|s - s'\|^\gamma.
\]
\end{assumption}

\begin{assumption}[Bounded Feature Vectors]
\label{ass:bounded-features}
There exists a universal constant \(C_\phi > 0\) such that for all states \(s \in S\),
\[
\|\phi(s)\| \leq C_\phi.
\]
\end{assumption}

\subsection{TD(0) error decomposition into Martingale Term and Remainder Term}

The TD error \( e_t \) at iteration \( t \) can be decomposed into two components: a martingale term \( M_t \) and a remainder term \( R_t \). This decomposition is crucial for analyzing the stochastic behavior of the TD learning algorithm.

\begin{lemma}[TD Error Decomposition into Martingale and Remainder]
\label{theorem:td-error-decomposition}
Under the assumptions of polynomial ergodicity \ref{theorem:polynomial-ergodicity}, step size decay \ref{ass:step-size-decay}, Hölder continuity of the feature mapping \ref{ass:holder-continuous-features}, and bounded feature vectors \ref{ass:bounded-features}, the TD error \( e_t \) can be decomposed as:
\[
e_t = M_t + R_t,
\]
where: \(M_t = \sum_{k=1}^{t} d_k\) is a martingale with \( \{d_k\} \) being a martingale difference sequence.
\( R_t \) is the remainder term capturing the systematic bias.
\end{lemma}

\begin{proof}
Under the assumptions of polynomial ergodicity, step size decay, Hölder continuity of the feature mapping, and bounded feature vectors, we show that the TD error can be decomposed as \( e_t = M_t + R_t \), where \( M_t \) is a martingale and \( R_t \) captures systematic bias.
The TD update rule is:
\[
\theta_{t+1} = \theta_t + \alpha_t \delta_t \nabla_\theta V_\theta(x_t),
\]
where \( \delta_t = r_t + \gamma V_\theta(x_{t+1}) - V_\theta(x_t) \) is the TD error. Subtracting \( \theta^* \) and summing iteratively,:
\begin{align*}
\theta_t - \theta^* = &\sum_{k=1}^t \alpha_k (\delta_k - \mathbb{E}[\delta_k | \mathcal{F}_{k-1}]) \nabla_\theta V_\theta(x_k)\\ &+ \sum_{k=1}^t \alpha_k \mathbb{E}[\delta_k | \mathcal{F}_{k-1}] \nabla_\theta V_\theta(x_k).
\end{align*}
\( M_t \), is a martingale since \( \mathbb{E}[M_t | \mathcal{F}_{t-1}] = M_{t-1} \). \( R_t \) is a remainder term, bounded using Hölder continuity and step size conditions. Decomposition enables analyzing the asymptotic behavior of TD learning under stochastic approximation.
Details and verification of the decomposition of the TD error into martingale, verification of the Martingale properties, and remainder components are established in Appendix~\ref{app:proof-TD-error}.
\end{proof}

\subsection{Bounded Variance of the Martingale Term}

A critical aspect of analyzing the convergence of the TD algorithm is bounding the variance of the martingale term \( M_t \) and convergence of the remainder term \( R_t \).

\begin{lemma}[Bounded Variance of Martingale Term]
\label{lemma:bounded-variance-martingale}
Assuming polynomial mixing \ref{ass:polynomial-mixing}, Hölder continuity of the feature mapping \ref{ass:holder-continuous-features}, bounded feature vectors \ref{ass:bounded-features}, and appropriate step size decay \ref{ass:step-size-decay}, we can use the concentration bound in theorem \ref{theorem:concentration-block} to find the martingale term \( M_t \) satisfies:
\[
\mathbb{E}[\|M_t\|^2] \leq C t^{-\beta},
\]
where \( C \) is a constant dependent on the step size and mixing parameters, and \( \beta > 0 \) characterizes the rate at which the variance decays.
\end{lemma}

\begin{proof}
Detailed proof is found in Appendix \ref{app:proof-bounded-variance-martingale}.
\end{proof}
\subsection{Bounded Remainder Term}

The remainder term $R_t$ captures the bias introduced by the TD updates. 
It is defined as the difference between the total error and the martingale component:
\[
  R_t = e_t - M_t.
\]

To analyze $R_t$, we employ the concept of H\"older continuity, which provides 
a measure of smoothness for the functions involved in the TD updates.

\begin{lemma}[Remainder Term Convergence]
\label{lemma:remainder-growth-term}
Under Hölder continuity \ref{ass:holder-continuous-features} and polynomial ergodicity \ref{theorem:polynomial-ergodicity}, step-condition \ref{ass:step-size-decay}, the remainder term satisfies:
\[
\|R_t\| \leq O(t^{-\gamma/2})
\]
where \( \gamma \) is the H\"older exponent.
\end{lemma}
\begin{proof}
This is achieved through a decomposition and recursive convergence argument, details are in Appendix \ref{app:proof-remainder-growth-term}.
\end{proof}