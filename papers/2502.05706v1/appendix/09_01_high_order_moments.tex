\subsection{Proof of Theorem \ref{theorem:hom-linear-bounds}, High-Order Moment Bounds, Linear Function Approximation}
\label{app:proof-hom-linear-bounds}
\begin{theoremapp*}[High-Order Moment Bounds, Linear Function Approximation]
For any \( p \geq 2 \), the \( p \)-th moment of the TD(0) error satisfies:
\[
\left( \mathbb{E}\left[\|\theta_t - \theta^*\|^p\right] \right)^{1/p} \leq C t^{-\beta/2} + C' \alpha_t^{\gamma/p},
\]
where \( \alpha_t = \frac{\alpha}{t} \) for some constant \( \alpha > 0 \), \( \beta > 0 \) is the mixing-based rate parameter, and \( \gamma \) is the Hölder exponent from Lemma~\ref{lemma:ho-error-bounds}.
\end{theoremapp*}
\begin{proof}
\;\newline
\paragraph{1. Summary of the Proof:}  
This proof establishes high-order moment bounds for the parameter error \( \|\theta_t - \theta^*\| \) in Temporal-Difference (TD) learning with linear function approximation. By leveraging Lemma~\ref{lemma:ho-error-bounds}, which provides high-order error bounds, and utilizing properties of martingale differences alongside an induction-based discrete Grönwall argument, we derive the convergence rates specified in Theorem~\ref{theorem:hom-linear-bounds}. The approach systematically bounds each component of the parameter error recursion, ensuring that the \( p \)-th moment of the TD(0) parameter error decays at a rate influenced by the mixing properties of the underlying Markov chain and the smoothness of the value function approximation.

\paragraph{2. Assumptions:}
\begin{itemize}
    \item \textbf{Polynomial mixing} The Markov process \((x_t)\) satisfies polynomial mixing: for any bounded measurable functions \(f\) and \(g\),
\(
|\mathbb{E}[f(x_t)g(x_{t+k})] - \mathbb{E}[f(x_t)]\mathbb{E}[g(x_{t+k})]| \leq C k^{-\beta},
\)
for some constants \(C > 0\) and \(\beta > 1\).
    \item \textbf{Bounded Features:} \ref{ass:bounded-features} The gradient \( \nabla_\theta V_\theta(x_t) \) is uniformly bounded, i.e., there exists a constant \( C > 0 \) such that \( \|\nabla_\theta V_\theta(x_t)\| \leq C \) for all \( t \).
    \item \textbf{Hölder Continuity} \ref{ass:holder-continuous-features}: The feature mapping \( f: S \to \mathbb{R}^d \) is Hölder continuous with exponent \( \gamma \in (0, 1] \) and constant \( C_\gamma > 0 \). Formally, for all \( s, s' \in S \), then we have \(\|f(s) - f(s')\| \leq C_\gamma \|s - s'\|^\gamma.\)
    \item \textbf{Step Size Conditions:} The learning rates \( \{\alpha_t\} \) satisfy \( \sum_{t=1}^\infty \alpha_t = \infty \) and \( \sum_{t=1}^\infty \alpha_t^2 < \infty \). 
\end{itemize}
%\textbf{Introduction:}  
%The objective is to establish high-order moment bounds for the parameter error \( \|\theta_t - \theta^*\| \) in the context of Temporal-Difference (TD) learning. Utilizing Lemma~\ref{lemma:ho-error-bounds}, along with properties of martingale differences and recursive inequality techniques, we derive the convergence rates as stated in Theorem~\ref{theorem:hom-linear-bounds}.
\paragraph{3) TD Update Rule and Parameter Error Recursion:}  
The TD learning update rule is given by:
\[
\theta_{t+1} = \theta_t + \alpha_t \delta_t x_t,
\]

Subtracting the optimal parameter \( \theta^* \) from both sides:
\[
\theta_{t+1} - \theta^* = \theta_t - \theta^* + \alpha_t \delta_t x_t.
\]
Taking the norm, using triangular inequality, and raising it to the \( p \)-th power:
\begin{equation}
\label{eq:parameter-error-recursion}
\|\theta_{t+1} - \theta^*\|^p \leq \left( \|\theta_t - \theta^*\| + \alpha_t \|\delta_t x_t\| \right)^p.
\end{equation}

\paragraph{4) Applying Minkowski’s Inequality:}  
Minkowski’s inequality for \( p \geq 1 \) states that:
\[
\left( \mathbb{E}\left[ \left( a + b \right)^p \right] \right)^{1/p} \leq \left( \mathbb{E}\left[ a^p \right] \right)^{1/p} + \left( \mathbb{E}\left[ b^p \right] \right)^{1/p}.
\]
Applying this to the parameter error recursion~\eqref{eq:parameter-error-recursion}:
\[
\left( \mathbb{E}\left[\|\theta_{t+1} - \theta^*\|^p\right] \right)^{1/p} \leq \left( \mathbb{E}\left[\|\theta_t - \theta^*\|^p\right] \right)^{1/p} + \alpha_t \left( \mathbb{E}\left[\|\delta_t x_t\|^p\right] \right)^{1/p}.
\]
Define
\[
E_t = \left( \mathbb{E}\left[\|\theta_t - \theta^*\|^p\right] \right)^{1/p},
\]
so the recursion becomes
\begin{equation}
\label{eq:minkowski-application}
E_{t+1} \leq E_t + \alpha_t \left( \mathbb{E}\left[\|\delta_t x_t\|^p\right] \right)^{1/p}.
\end{equation}

\paragraph{5) Bounding \( \mathbb{E}\left[\|\delta_t x_t\|^p\right]^{1/p} \):}  
Assuming that the feature vectors \( x_t \) are uniformly bounded, i.e., \( \|x_t\| \leq C_x \) for all \( t \), we have:
\[
\|\delta_t x_t\| \leq \|\delta_t\| \cdot \|x_t\| \leq C_x \|\delta_t\|.
\]
Taking the \( p \)-th moment:
\begin{equation}
\label{eq:deltax-bound}
\left( \mathbb{E}\left[\|\delta_t x_t\|^p\right] \right)^{1/p} \leq C_x \left( \mathbb{E}\left[\|\delta_t\|^p\right] \right)^{1/p}.
\end{equation}

\paragraph{6) Applying Lemma~\ref{lemma:ho-error-bounds}, High-Order Error Bounds:}
\[
\left( \mathbb{E}\left[\|\delta_t\|^p\right] \right)^{1/p} \leq C_p^{1/p} \left( 1 + \|\theta_t - \theta^*\|^{\gamma} \right).
\]
Substituting this into Equation~\eqref{eq:deltax-bound}:
\begin{equation}
\label{eq:deltax-substitution}
\left( \mathbb{E}\left[\|\delta_t x_t\|^p\right] \right)^{1/p} \leq C_x C_p^{1/p} \left( 1 + E_t^{\gamma} \right).
\end{equation}

\paragraph{7) Substituting Back into the Recursive Inequality:}  
Substituting Equation~\eqref{eq:deltax-substitution} into Equation~\eqref{eq:minkowski-application}:
\[
E_{t+1} \leq E_t + \alpha_t C_x C_p^{1/p} \left( 1 + E_t^{\gamma} \right).
\]
\label{eq:recursive-inequality}

\paragraph{8) Incorporating the Martingale Difference Property:}  
The TD update involves stochastic noise \( \delta_t \), which forms a martingale difference sequence. Specifically, by Lemma~\ref{lemma:td-error-mds}, we have:
\[
\mathbb{E}\left[ \delta_t \mid \mathcal{F}_t \right] = 0,
\]
where \( \mathcal{F}_t = \sigma(x_0, x_1, \dots, x_t) \) is the filtration up to time \( t \). This implies that the stochastic updates do not introduce bias, allowing us to focus on bounding the expected norms.

\paragraph{9) Solving the Recursive Inequality via Induction:}  
Let \( E_t = \mathbb{E}\left[\|\theta_t - \theta^*\|^p\right]^{1/p} \). The recursive inequality becomes:
\[
E_{t+1} \leq E_t + \alpha_t C_x C_p^{1/p} \left(1 + E_t^{\gamma}\right).
\]
Assuming the learning rate \( \alpha_t = \frac{\alpha}{t} \) for some \( \alpha > 0 \), we substitute:
\[
E_{t+1} \leq E_t + \frac{\alpha C_x C_p^{1/p}}{t} \left(1 + E_t^{\gamma}\right).
\]
To solve this recursion, we employ an \emph{induction-based Grönwall argument}.

\paragraph{10) Bounding \( E_t \):}  
We aim to show that \( E_t \) satisfies:
\[
E_t \leq C t^{-\beta/2} + C' \alpha_t^{\gamma/p},
\]
for appropriate constants \( C \) and \( C' \). This involves balancing the recursive additions and ensuring that the growth rate of \( E_t \) does not exceed the desired bound.

\paragraph{11) Solving the Recursive Inequality via Induction:}

To establish that
\[
E_t \leq C t^{-\tfrac{\beta}{2}} + C' \alpha_t^{\tfrac{\gamma}{p}},
\]
we employ an \emph{induction-based Grönwall argument}. Assume that the learning rate is given by
\[
\alpha_t = \frac{\alpha}{t},
\]
for some constant \( \alpha > 0 \).

\textbf{Base Case (\( t = 1 \)):}  
For \( t = 1 \), the bound
\[
E_1 \leq C \cdot 1^{-\tfrac{\beta}{2}} + C' \left(\frac{\alpha}{1}\right)^{\tfrac{\gamma}{p}} = C + C' \alpha^{\tfrac{\gamma}{p}}
\]
holds provided \( C \) and \( C' \) are chosen to accommodate the initial error \( E_1 \). Specifically, set
\[
C \geq E_1,
\quad
C' \geq 1,
\]
to satisfy the inequality.

\textbf{Inductive Step:}  
Assume that for some \( t \geq 1 \),
\[
E_t \leq C t^{-\tfrac{\beta}{2}} + C' \alpha_t^{\tfrac{\gamma}{p}}.
\]
We aim to show that
\[
E_{t+1} \leq C (t+1)^{-\tfrac{\beta}{2}} + C' \alpha_{t+1}^{\tfrac{\gamma}{p}}.
\]

Starting from the recursive inequality~\eqref{eq:recursive-inequality}:
\[
E_{t+1} \leq E_t + \frac{\kappa}{t} \left(1 + E_t^{\gamma}\right),
\]
where \( \kappa = \alpha C_x C_p^{1/p} \).

Substituting the inductive hypothesis into the above:
\begin{align*}
E_{t+1} &\leq C t^{-\tfrac{\beta}{2}} + C' \alpha_t^{\tfrac{\gamma}{p}} + \frac{\kappa}{t} \left(1 + \left(C t^{-\tfrac{\beta}{2}} + C' \alpha_t^{\tfrac{\gamma}{p}}\right)^{\gamma}\right) \\
&\leq C t^{-\tfrac{\beta}{2}} + C' \left(\frac{\alpha}{t}\right)^{\tfrac{\gamma}{p}} + \frac{\kappa}{t} \left(1 + 2^{\gamma - 1} \left(C^{\gamma} t^{-\tfrac{\beta \gamma}{2}} + C'^{\gamma} \alpha_t^{\tfrac{\gamma^2}{p}}\right)\right) \quad \text{(Using $(a+b)^\gamma \leq 2^{\gamma-1}(a^\gamma + b^\gamma)$)} \\
&= C t^{-\tfrac{\beta}{2}} + C' \alpha^{\tfrac{\gamma}{p}} t^{-\tfrac{\gamma}{p}} + \frac{\kappa}{t} + \frac{\kappa 2^{\gamma - 1}}{t} \left(C^{\gamma} t^{-\tfrac{\beta \gamma}{2}} + C'^{\gamma} \left(\frac{\alpha}{t}\right)^{\tfrac{\gamma^2}{p}}\right).
\end{align*}

\textbf{Choosing Constants \( C \) and \( C' \):}  
To satisfy the inductive step, choose \( C \) and \( C' \) such that:
\[
C \geq \kappa + \kappa 2^{\gamma - 1} C^{\gamma},
\]
and
\[
C' \geq \kappa 2^{\gamma - 1} C'^{\gamma}.
\]
These inequalities can be satisfied by appropriately choosing \( C \) and \( C' \). For instance, set
\[
C \geq \frac{\kappa}{1 - \kappa 2^{\gamma - 1}},
\]
provided that \( \kappa 2^{\gamma - 1} < 1 \), and similarly
\[
C' \geq \frac{\kappa 2^{\gamma - 1}}{1 - \kappa 2^{\gamma - 1}}.
\]
Assuming \( \kappa 2^{\gamma - 1} < 1 \), such choices of \( C \) and \( C' \) are feasible.

\paragraph{12) Concluding the Induction:}  
With \( C \) and \( C' \) chosen to satisfy the above inequalities, the inductive step holds:
\[
E_{t+1} \leq C (t+1)^{-\tfrac{\beta}{2}} + C' \alpha_{t+1}^{\tfrac{\gamma}{p}}.
\]
Thus, by induction, the bound
\[
E_t \leq C t^{-\tfrac{\beta}{2}} + C' \alpha_t^{\tfrac{\gamma}{p}}
\]
holds for all \( t \geq 1 \).

\paragraph{13) Final Bound:}  
Therefore, the \( p \)-th moment of the TD(0) error satisfies
\[
\left( \mathbb{E}\left[\|\theta_t - \theta^*\|^p\right] \right)^{1/p} \leq C t^{-\tfrac{\beta}{2}} + C' \alpha_t^{\tfrac{\gamma}{p}},
\]
as required. This completes the proof of \emph{Theorem \ref{theorem:hom-linear-bounds}}.
\end{proof}