\subsection{Proof of Lemma~\ref{lemma:dependent-blocks}: Block Independence Approximation}
\label{app:proof-block-independence-approximation}

\begin{lemmaapp*}[Block Independence]
Let \(\{x_t\}_{t=1}^\infty\) be a Markov chain satisfying the polynomial ergodicity condition as specified in Theorem \ref{theorem:polynomial-ergodicity} with rate \(\beta > 1\). Partition the sequence into non-overlapping blocks of size \(b\):
\[
B_k = (x_{(k-1)b + 1}, \ldots, x_{kb}), \quad k = 1, 2, \ldots, \left\lfloor \frac{n}{b} \right\rfloor.
\]
For a sufficiently large block size \(b\), the blocks \(\{B_k\}\) satisfy:
\[
\left\| \mathbb{P}(B_k \mid B_{k-1}) - \mathbb{P}(B_k) \right\|_{\text{TV}} \leq C b^{-\beta},
\]
where \(C > 0\) and \(\beta > 1\) are constants derived from the polynomial ergodicity condition.
\end{lemmaapp*}

\begin{proof}
\;\newline
\paragraph{1) Assumptions: Polynomial Mixing.}
By assumption, the Markov chain \( (x_t) \) satisfies a \emph{polynomial mixing} condition: there exist constants \( \alpha > 0 \) and \( C_0 > 0 \) such that for all bounded measurable functions \( f, g: \mathcal{X} \to \mathbb{R} \) and for all integers \( t \geq 0 \) and \( k \geq 1 \),
\[
  \Bigl|\,
    \mathbb{E}[\,f(x_t)\,g(x_{t+k})]
    \;-\;
    \mathbb{E}[\,f(x_t)]\,\mathbb{E}[\,g(x_{t+k})]
  \Bigr|
  \;\;\le\;\;
  C_0\,k^{-\alpha}.
\]
Intuitively, this condition implies that the dependence between \( x_t \) and \( x_{t+k} \) decays polynomially as \( k \) increases.

\paragraph{2) Partition into Blocks of Size \( b \).}
Define the blocks
\[
  B_i \;=\; (x_{(i-1)b+1}, \dots, x_{ib})
  \quad
  \text{for } i=1,2,\dots.
\]
Each block \( B_i \) consists of \( b \) consecutive states of the Markov chain. The goal is to show that consecutive blocks \( B_{i-1} \) and \( B_i \) are approximately independent in the total variation sense, with the dependence decaying as \( b^{-\alpha} \).

\paragraph{3) Expressing Conditional Distribution of \( B_i \) Given \( B_{i-1} \).}
Given the block \( B_{i-1} = (x_{(i-2)b+1}, \dots, x_{(i-1)b}) \), the distribution of \( B_i \) conditioned on \( B_{i-1} \) depends solely on the last state of \( B_{i-1} \), due to the Markov property. Specifically,
\[
  \mathbb{P}(B_i \mid B_{i-1}) = P^b(x_{(i-1)b}, \cdot),
\]
where \( P^b \) denotes the \( b \)-step transition kernel of the chain.

\paragraph{4) Relating Single-Step Mixing to Block-Level Mixing.}
To bound the total variation distance \( \Bigl\|\mathbb{P}(B_i \mid B_{i-1}) - \mathbb{P}(B_i)\Bigr\|_{TV} \), we proceed as follows:

\begin{enumerate}
  \item \textbf{Understanding \( \mathbb{P}(B_i) \):}  
    The unconditional distribution of \( B_i \) is given by
    \[
      \mathbb{P}(B_i) = \int_{\mathcal{X}} P^b(x, B_i) \, \pi(dx),
    \]
    where \( \pi \) is the invariant distribution of the chain. Since \( \pi \) is invariant, \( P^b(\pi, \cdot) = \pi \), ensuring that \( \mathbb{P}(B_i) \) aligns with the stationary behavior of the chain.

  \item \textbf{Bounding \( \Bigl\| P^b(x_{(i-1)b}, \cdot) - \pi(\cdot) \Bigr\|_{TV} \):}  
    By the polynomial mixing condition, for any \( x \in \mathcal{X} \),
    \[
      \Bigl\| P^b(x, \cdot) - \pi(\cdot) \Bigr\|_{TV}
      \;\le\;\;
      C_0\,b^{-\alpha} \cdot (1 + V(x)),
    \]
    where \( V(x) \) is a Lyapunov function ensuring that states with higher \( V(x) \) are "heavier" or more significant in some sense.

  \item \textbf{Applying the Mixing Bound to the Conditional Distribution:}  
  {Justification of the Total Variation Distance Inequality:}

By the Markov property, the distribution of block \( B_i \) given block \( B_{i-1} \) depends solely on the last state of \( B_{i-1} \), denoted as \( x_{(i-1)b} \). Therefore, we have:
\[
  \mathbb{P}(B_i \mid B_{i-1}) = P^b(x_{(i-1)b}, \cdot).
\]
Since \( \mathbb{P}(B_i) = \pi(\cdot) \) under the stationary distribution, the Total Variation Distance between the conditional and marginal distributions satisfies:
\[
  \Bigl\| \mathbb{P}(B_i \mid B_{i-1}) - \mathbb{P}(B_i) \Bigr\|_{TV}
  = \Bigl\| P^b(x_{(i-1)b}, \cdot) - \pi(\cdot) \Bigr\|_{TV}.
\]
Thus, the first inequality is an equality, and the subsequent inequality follows from applying the polynomial mixing condition:
\[
  \Bigl\| P^b(x_{(i-1)b}, \cdot) - \pi(\cdot) \Bigr\|_{TV}
  \;\le\;\;
  C_0\,b^{-\alpha} \cdot (1 + V(x_{(i-1)b})).
\]

    Given \( B_{i-1} \), the distribution of \( B_i \) is \( P^b(x_{(i-1)b}, \cdot) \). Therefore, the total variation distance between \( \mathbb{P}(B_i \mid B_{i-1}) \) and \( \mathbb{P}(B_i) \) can be bounded as
    \[
      \Bigl\| \mathbb{P}(B_i \mid B_{i-1}) - \mathbb{P}(B_i) \Bigr\|_{TV}
      \;=\;\;
      \Bigl\| P^b(x_{(i-1)b}, \cdot) - \pi(\cdot) \Bigr\|_{TV}
      \;\le\;\;
      C_0\,b^{-\alpha} \cdot (1 + V(x_{(i-1)b})).
    \]

    

  \item \textbf{Handling the Lyapunov Function \( V(x_{(i-1)b}) \):}  
    To ensure the bound is independent of \( b \), we need to control \( V(x_{(i-1)b}) \). This can be achieved by leveraging the Lyapunov condition, which ensures that \( V(x_t) \) does not grow unboundedly over time. Specifically, under the polynomial drift condition, \( \mathbb{E}[V(x_t)] \) remains bounded for all \( t \). Therefore, with high probability or in expectation, \( V(x_{(i-1)b}) \) can be bounded by a constant \( V_{\max} \).

    Thus, we have
    \[
      \Bigl\| \mathbb{P}(B_i \mid B_{i-1}) - \mathbb{P}(B_i) \Bigr\|_{TV}
      \;\le\;\;
      C_0\,b^{-\alpha} \cdot (1 + V_{\max})
      \;=\;\;
      C\,b^{-\alpha},
    \]
    where \( C = C_0 \cdot (1 + V_{\max}) \) is a constant independent of \( b \).

\end{enumerate}

\paragraph{5) Concluding the Total Variation Bound.}
Combining the above steps, we establish that for every \( i \geq 2 \),
\[
  \Bigl\|\mathbb{P}(B_i \mid B_{i-1}) - \mathbb{P}(B_i)\Bigr\|_{TV}
  \;\le\;\;
  C\,b^{-\alpha},
\]
where \( C \) is a constant independent of \( b \). This demonstrates that consecutive blocks \( B_{i-1} \) and \( B_i \) are approximately independent in the total variation sense, with the dependence decaying polynomially as \( b^{-\alpha} \).

\paragraph{6) Ensuring Uniformity Across All Blocks.}
Since the bound \( C\,b^{-\alpha} \) holds uniformly for any \( i \geq 2 \) and for all blocks of size \( b \), the lemma is proven. This uniform bound is crucial for subsequent analyses that rely on the approximate independence of blocks to apply concentration inequalities or martingale arguments.

\end{proof}
