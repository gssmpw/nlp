\subsection{Proof of Theorem \ref{theorem:hp-linear-bounds}: High-Probability Bounds, Linear Function Approximation}
\label{app:proof-hp-linear-bounds}

\begin{theoremapp*}[High-Probability Error Bounds, Linear Function Approximation]
Under the assumptions: polynomial ergodicity \ref{theorem:polynomial-ergodicity}, TD(0) error decomposition \ref{theorem:td-error-decomposition}, Bounded Variance of the Martingale Term \ref{lemma:bounded-variance-martingale}, Step Size Condition \ref{ass:step-size-decay}, Concentration Property for High-Probability Bounds \ref{theorem:concentration-block}. With probability at least \(1 - \delta\), the TD error satisfies:
\[
\|\theta_t - \theta^*\| \leq C_\delta t^{-\beta/2} + C'_\delta \alpha_t^\gamma,
\]
where \(C_\delta\) and \(C'_\delta\) depend logarithmically on \(1/\delta\).
\end{theoremapp*}

\begin{proof}
\;\newline
\paragraph{Introduction and Overview.}
We prove high-probability bounds on the parameter-estimation error
\[
  \|\theta_t - \theta^*\|
\]
by decomposing it into two terms: (1) a martingale term $M_t$ that captures fluctuations from the stochastic process (and admits an Azuma--Hoeffding-type concentration inequality), and (2) a remainder term $R_t$ whose magnitude is controlled via H\"older continuity and polynomial mixing assumptions. We then set a failure probability target $\delta$ and apply a union bound to combine these two bounds into the final result.

\medskip
\noindent\newline
\textbf{Notation and Setup:}
\begin{itemize}
\item \(\theta^*\) is the true (or optimal) parameter we compare against.  
\item \(\theta_t\) is the estimate at time \(t\).  
\item Polynomial mixing ensures that the dependence between successive samples decays at a polynomial rate, thus bounding variance-like terms and aiding Azuma--Hoeffding concentration.  
\item H\"older continuity (with exponent \(\gamma\)) allows bounding the error of certain remainder terms by a factor like \(t^{-\gamma/2}\).  
\end{itemize}

\paragraph{1) Decompose the Error:}
From Theorem \ref{theorem:td-error-decomposition}, we can write
\[
  \theta_t - \theta^*
  \;=\;
  M_t \;+\; R_t,
\]
where
\[
  M_t
  \;=\;
  \sum_{k=1}^t d_k
  \quad
  \text{(a sum of martingale differences)}, 
  \quad
  \text{and}
  \quad
  R_t
  \;\text{is the remainder term.}
\]
Typically, $M_t$ collects the ``random fluctuations'' arising from stochastic updates (e.g., stochastic gradients or TD errors), while $R_t$ accounts for the deterministic error or residual that decays due to stepsize choices, H\"older continuity, or polynomial ergodicity.

\paragraph{2) Martingale Term \(\boldsymbol{M_t}\) and Azuma--Hoeffding:}
To apply concentration inequalities, we first establish that the martingale differences \(d_k\) are conditionally sub-Gaussian. Specifically, under Assumption \ref{lemma:bounded-variance-martingale}, we have:
\[
\mathbb{E}\left[\exp\left(\lambda d_k \mid \mathcal{F}_{k-1}\right)\right] \leq \exp\left(\frac{\lambda^2 \sigma^2}{2}\right) \quad \forall \lambda \in \mathbb{R},
\]
where \(\mathcal{F}_{k-1}\) denotes the filtration up to time \(k-1\), and \(\sigma^2\) bounds the conditional variance of \(d_k\). This sub-Gaussian property allows us to apply the Azuma-Hoeffding inequality for martingales.
Under polynomial mixing and bounded (or at most $\sigma$-sub-Gaussian) increments, the sequence $\{d_k\}$ satisfies bounded conditional variances or bounded increments. By Azuma--Hoeffding's inequality for martingales (or adapted sequences),
\[\displaystyle
  \mathbb{P}\left(
    \bigl\|M_t\bigr\| > \epsilon
  \right)
  \;\le\;
  2
  \,\exp\left(-\tfrac{\displaystyle \epsilon^2}{\displaystyle 2\sum_{k=1}^t \alpha_k^2 C_\delta^2\,C_\nabla^2}\right),
\]
where:
\begin{itemize}
  \item \(\alpha_k\) is a stepsize (or learning rate) that appears inside the increments $d_k$.  
  \item $C_\delta$ and $C_\nabla$ reflect bounds on the TD errors (or gradients) under polynomial mixing and H\"older continuity. Specifically, $C_\delta$ could represent a uniform bound on the one-step TD fluctuation, and $C_\nabla$ a bound on the gradient/features.  
  \item $\sum_{k=1}^t \alpha_k^2 C_\delta^2 C_\nabla^2$ plays the role of the sum of squared step-size norms bounding the variance of $M_t$.  
\end{itemize}

Assuming a step size schedule of \(\alpha_k = \frac{\alpha_0}{k}\) for some \(\alpha_0 > 0\), we have
\[
\sum_{k=1}^t \alpha_k^2 = \alpha_0^2 \sum_{k=1}^t \frac{1}{k^2} \leq \alpha_0^2 \frac{\pi^2}{6} \leq C_3,
\]
for a constant \(C_3 > 0\). Substituting this into the concentration inequality yields
\[
\mathbb{P}\left( \|M_t\| > \epsilon \right) \leq 2 \exp\left(-\frac{\epsilon^2}{2 C_3 C_\delta^2 C_\nabla^2}\right).
\]

\paragraph{3) Setting Failure Probability to $\boldsymbol{\delta/2}$:}
To achieve a failure probability of \(\delta/2\) for the martingale term, we set
\[
2 \exp\left(-\frac{\epsilon^2}{2 C_3 C_\delta^2 C_\nabla^2}\right) = \frac{\delta}{2}.
\]
Solving for \(\epsilon\), we obtain
\[
\epsilon = C_1 \sqrt{\log\left(\frac{4}{\delta}\right)},
\]
where \(C_1 = \sqrt{2 C_3} C_\delta C_\nabla\). Therefore, with probability at least \(1 - \frac{\delta}{2}\),
\[
\|M_t\| \leq C_1 \sqrt{\log\left(\frac{4}{\delta}\right)}.
\]

\paragraph{4) Remainder Term \(\boldsymbol{R_t}\) via H\"older Continuity:}
To bound \(R_t\), we leverage H\"older continuity and polynomial ergodicity. Specifically, under Assumptions \ref{theorem:polynomial-ergodicity} and H\"older continuity with exponent \(\gamma\), we have
\[
\|R_t\| \leq C_2 t^{-\gamma/2},
\]
where \(C_2 > 0\) is a constant derived from the combination of H\"older continuity and mixing rates.

\paragraph{5) Combine Bounds and Apply the Union Bound:}
Both events \(\{\|M_t\| \leq C_1 \sqrt{\log(4/\delta)}\}\) and \(\{\|R_t\| \leq C_2 t^{-\gamma/2}\}\) hold with probabilities at least \(1 - \frac{\delta}{2}\) each. Applying the union bound, the probability that both events hold simultaneously is at least
\[
1 - \left(\frac{\delta}{2} + \frac{\delta}{2}\right) = 1 - \delta.
\]
Hence, with probability at least \(1 - \delta\),
\[
\|\theta_t - \theta^*\| = \|M_t + R_t\| \leq \|M_t\| + \|R_t\| \leq C_1 \sqrt{\log\left(\frac{4}{\delta}\right)} + C_2 t^{-\gamma/2}.
\]
Since \(\log(4/\delta) = \log(1/\delta) + \log 4 \leq 2 \log(1/\delta)\) for \(\delta \in (0,1)\), we can simplify the bound to
\[
\|\theta_t - \theta^*\| \leq C_1' \sqrt{\log\left(\frac{1}{\delta}\right)} t^{-1/2} + C_2 t^{-\gamma/2},
\]
where \(C_1' = C_1 \sqrt{2}\) absorbs the constant terms. This completes the proof of the high-probability bound:
\[
\mathbb{P}\left( \|\theta_t - \theta^*\| \leq C_1' \sqrt{\frac{\log(1/\delta)}{t}} + C_2 t^{-\gamma/2} \right) \geq 1 - \delta.
\]
\paragraph{6. Conclusion:}
Combining the martingale concentration (Azuma--Hoeffding) bound for $M_t$ with the H\"older-based decay of $R_t$, and invoking the union bound, establishes the desired probability guarantee:
\[
  \mathbb{P}\!\Bigl(
     \|\theta_t - \theta^*\|
     \;\le\;
     C_1 \sqrt{\tfrac{\log(1/\delta)}{t}}
     \;+\;
     C_2 t^{-\gamma/2}
  \Bigr)
  \;\ge\;
  1-\delta.
\]
\end{proof}
