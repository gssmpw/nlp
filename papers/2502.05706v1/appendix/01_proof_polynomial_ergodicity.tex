\subsection{Proof of Lemma \ref{theorem:polynomial-ergodicity}: Ergodicity Properties Under Polynomial Mixing}
\label{app:proof-polynomial-ergodicity}

\begin{theoremapp*}[Polynomial Ergodicity]
Under the polynomial mixing assumption \ref{ass:polynomial-mixing} and the drift condition \ref{ass:drift-condition}, the Markov process satisfies:
\[
|\mathbb{P}(x_t \in A) - \pi(A)| \leq C (1 + V(x_0)) t^{-\beta},
\]
for some constant \( C > 0 \), where \( \pi \) is the stationary distribution, \( V: \mathcal{X} \to [1,\infty) \) is a Lyapunov function, and \( \beta > 1 \).
\end{theoremapp*}

\begin{proof}
\paragraph{1) Assumptions: Lyapunov Function and Polynomial Drift Condition}

We begin by defining the Lyapunov function \( V: \mathcal{X} \to [1,\infty) \). We assume that the \emph{polynomial drift condition} holds, which is a standard assumption in the study of Markov processes. Specifically, there exist constants \( c > 0 \), \( d \in (0,1) \), and \( b > 0 \) such that for all \( x \in \mathcal{X} \):
\[
\mathbb{E}[V(x_{t+1}) \mid x_t = x] \leq V(x) - c V(x)^d + b.
\]
This condition ensures that the Lyapunov function \( V(x_t) \) decreases at a polynomial rate, which is crucial for establishing polynomial convergence of the Markov chain to its stationary distribution.

\paragraph{2) Total Variation Distance Representation}

The \emph{Total Variation (TV) distance} between the distribution of \( x_t \) and the invariant measure \( \pi \) is defined as:
\[
\|\mathbb{P}(x_t \in \cdot) - \pi(\cdot)\|_{TV} = \sup_{A \subseteq \mathcal{X}} |\mathbb{P}(x_t \in A) - \pi(A)|.
\]
To bound the TV distance, it suffices to control \( |\mathbb{P}(x_t \in A) - \pi(A)| \) uniformly over all measurable sets \( A \).

\paragraph{3) Bounding \( |\mathbb{P}(x_t \in A) - \pi(A)| \) for All Measurable Sets \( A \)}

Consider any measurable set \( A \subseteq \mathcal{X} \). We can express the difference as:
\[
|\mathbb{P}(x_t \in A) - \pi(A)| = \left| \int_{\mathcal{X}} P^t(x_0, A) \mathbb{P}(x_0) \, dx_0 - \int_{\mathcal{X}} \pi(A) \pi(dx_0) \right|.
\]
Assuming that the initial distribution is concentrated at \( x_0 \), i.e., \( \mathbb{P}(x_0) = \delta_{x_0} \), this simplifies to:
\[
|\mathbb{P}(x_t \in A) - \pi(A)| = |P^t(x_0, A) - \pi(A)|.
\]
To generalize for any initial distribution, we consider an arbitrary initial distribution \( \mathbb{P}(x_0) \). Applying the \emph{triangle inequality}, we obtain:
\[
|\mathbb{P}(x_t \in A) - \pi(A)| \leq \int_{\mathcal{X}} |P^t(x_0, A) - \pi(A)| \mathbb{P}(x_0) \, dx_0.
\]
This step utilizes the fact that the absolute difference of integrals is bounded by the integral of the absolute differences.

\paragraph{4) Applying the Polynomial Mixing Condition}

Under the \emph{polynomial mixing} assumption \ref{ass:polynomial-mixing} (as detailed in Chapters 14-16 of \cite{meyn2012markov}), there exist constants \( C > 0 \) and \( \beta > 1 \) such that for all \( x_0 \in \mathcal{X} \) and measurable sets \( A \):
\[
|P^t(x_0, A) - \pi(A)| \leq C(1 + V(x_0)) t^{-\beta}.
\]
Substituting this bound into the integral from Step 3, we obtain:
\[
|\mathbb{P}(x_t \in A) - \pi(A)| \leq \int_{\mathcal{X}} C(1 + V(x_0)) t^{-\beta} \mathbb{P}(x_0) \, dx_0 = C t^{-\beta} \int_{\mathcal{X}} (1 + V(x_0)) \mathbb{P}(x_0) \, dx_0.
\]
If the initial distribution is concentrated at \( x_0 \), the integral simplifies to:
\[
|\mathbb{P}(x_t \in A) - \pi(A)| \leq C(1 + V(x_0)) t^{-\beta}.
\]
Otherwise, for a general initial distribution, the bound becomes:
\[
|\mathbb{P}(x_t \in A) - \pi(A)| \leq C t^{-\beta} \left(1 + \mathbb{E}[V(x_0)]\right).
\]

\paragraph{5) Finalizing the Total Variation Bound}

Taking the supremum over all measurable sets \( A \) yields the \emph{Total Variation distance bound}:
\[
\|\mathbb{P}(x_t \in \cdot) - \pi(\cdot)\|_{TV} \leq C' (1 + V(x_0)) t^{-\beta},
\]
where \( C' \) is a constant that may differ from \( C \) but remains positive. This establishes that the Markov chain exhibits \emph{polynomial ergodicity} with a convergence rate of \( t^{-\beta} \).

\end{proof}