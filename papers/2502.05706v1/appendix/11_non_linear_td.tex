\subsection{Proof of Theorem \ref{theorem:bounds-non-linear-td}:
Non-linear TD(0) under Generalized Gradients, Variance Control, Polynomial Mixing, and (Optionally) H\"older Continuity}
\label{app:proof-bounds-non-linear-td-merged}

\begin{theoremapp*}[High-Probability Bounds for Non-linear TD(0)]
Let $\{x_t\}$ be a Markov chain on a state space $\mathcal{X}$ that satisfies \textbf{polynomial ergodicity} (rate $\beta>1$). 
Consider a non-linear TD(0) update of the form
\begin{align}
F(\theta, x) &= -\delta_t \nabla_\theta f_\theta(s)\\
\theta_{t+1} &= \theta_t - \alpha_t F(\theta_t, x_t)
\end{align}
where $F(\theta,x)$ is a \emph{generalized} (sub-)gradient of the possibly non-smooth TD objective.  Assume:
\begin{enumerate}
\item \textbf{Boundedness:} $\|F(\theta,x)\|\le G$ for all $(\theta,x)$,
\item \textbf{Variance / Higher-Order Moments:} $\mathrm{Var}[F(\theta,x)]\le \sigma^2$ (or $\|F(\theta,x)\|^2 \le \sigma'^2$ a.s.),
\item \textbf{Polynomial Mixing / Coupling:} The chain $\{x_t\}$ “forgets” initial conditions at a rate $t^{-\beta}$, enabling Freedman-type concentration with a small polynomial overhead,
\item \textbf{Zero-Mean at Optimum:} If $\theta^*$ is the TD fixed point / minimizer, then $\mathbb{E}[\,F(\theta^*,x)\,] = 0$,
\item \textbf{(Optional Refinement) H\"older Continuity:} 
\(
  \|\overline{F}(\theta) - \overline{F}(\theta^*)\|
  ~\le~
  L\,\|\theta - \theta^*\|^\gamma
  \quad(\gamma\in(0,1]),
\)
where $\overline{F}(\theta)=\mathbb{E}[\,F(\theta,x)\,]$ under stationarity.
\end{enumerate}
Then, under a diminishing step-size schedule $\{\alpha_t\}$ (e.g.\ $\alpha_t\sim 1/t^\omega,\,\omega>1/2$), there exist constants $C,C'>0$ such that for any $\delta\in(0,1)$, with probability at least $1-\delta$,
\[
  \|\theta_t - \theta^*\|
  ~\le~
  C\,t^{-\tfrac{\beta}{2}}
  ~+~
  C'\,\alpha_t^\gamma
  \quad
  (\forall\,t\ge1).
\]
\end{theoremapp*}

\begin{proof}
\;\newline
We organize the proof in four main steps, merging the variance-based Freedman concentration and the optional H\"older argument for the remainder term.
\paragraph{Step 1: Error Decomposition (Martingale + Remainder).}
From the TD error, we have
\begin{align*}
\theta_{t+1} - \theta^* 
&= (\theta_t - \theta^*) - \alpha_t F(\theta_t, x_t).
\end{align*}
Summing from $k=1$ to $t$, we get the standard telescoping form:
\begin{align}
\theta_t - \theta^*
&=
-\sum_{k=1}^t \alpha_k F(\theta_k, x_k) \\
&=
\underbrace{\sum_{k=1}^t \alpha_k \Bigl[F(\theta_k, x_k) 
   - \mathbb{E}(F(\theta_k, x_k) \mid \mathcal{F}_k) \Bigr]}_{\displaystyle M_t}
+ \underbrace{\sum_{k=1}^t \alpha_k \,
   \mathbb{E}(F(\theta_k, x_k) \mid \mathcal{F}_k)}_{\displaystyle R_t}.
\label{eq:theta-t-decomp}
\end{align}
\begin{proof}
Recall that
\[
  M_t 
  \;=\; 
  \sum_{k=1}^t 
  \alpha_k \Bigl[
    F(\theta_k, x_k) 
    \;-\;
    \mathbb{E}\bigl(F(\theta_k,x_k) \mid \mathcal{F}_k\bigr)
  \Bigr].
\]
Define the increments 
\[
  d_k 
  \;=\; 
  \alpha_k \Bigl[
    F(\theta_k, x_k) 
    \;-\;
    \mathbb{E}\bigl(F(\theta_k,x_k) \mid \mathcal{F}_k\bigr)
  \Bigr],
\]
so \(M_t = \sum_{k=1}^t d_k\). We claim \(\{M_t\}\) is a martingale with respect to the filtration \(\{\mathcal{F}_t\}\). Indeed,
\begin{align*}
M_{t+1} 
&=\; 
M_t + d_{t+1},\\[6pt]
\mathbb{E}\bigl[M_{t+1} \mid \mathcal{F}_t\bigr]
&=\; 
\mathbb{E}\bigl[M_t \mid \mathcal{F}_t\bigr]
\;+\;
\mathbb{E}\bigl[d_{t+1} \mid \mathcal{F}_t\bigr]
\;=\;
M_t 
\;+\;
\alpha_{t+1}\!\Bigl(
  \mathbb{E}\bigl[F(\theta_{t+1},x_{t+1}) \mid \mathcal{F}_t\bigr]
  \;-\;
  \mathbb{E}\bigl[
    \mathbb{E}[\,F(\theta_{t+1},x_{t+1}) \mid \mathcal{F}_{t+1}\,]
    \mid \mathcal{F}_t
  \bigr]
\Bigr)\\[2pt]
&=\; 
M_t 
\;+\;
\alpha_{t+1}\Bigl(
  \mathbb{E}\bigl[F(\theta_{t+1},x_{t+1}) \mid \mathcal{F}_t\bigr]
  \;-\;
  \mathbb{E}\bigl[F(\theta_{t+1},x_{t+1}) \mid \mathcal{F}_t\bigr]
\Bigr)
\;=\;
M_t.
\end{align*}
Hence \(M_t\) is a martingale.
\end{proof}
We will prove high-probability bounds on $\|M_t\|$ (variance-based Freedman) and on $\|R_t\|$ (mixing + optional H\"older arguments), then combine via union bound.
\paragraph{Step 2: Martingale Term \boldmath$M_t$ (Freedman / Higher-Order).}\;\\

\textbf{(a) Bounded Increments \& Conditional Variance.}\;\\
Define $d_k=\alpha_k[F(\theta_k,x_k)-\mathbb{E}(\cdot\mid\mathcal{F}_k)]$.  Then $\mathbb{E}[\,d_k\mid\mathcal{F}_k]=0$ and $\|d_k\|\le2\,\alpha_k G$.  Moreover,
\[
   \mathbb{E}[\|d_k\|^2 \mid \mathcal{F}_k]
   ~\le~
   \alpha_k^2(\sigma^2+G^2)
   \quad
   (\text{due to boundedness/variance of }F).
\]
\begin{proof}
Recall 
\[
  d_k \;=\; \alpha_k \Bigl[
      F(\theta_k,x_k) 
      \;-\; 
      \mathbb{E}\bigl(F(\theta_k,x_k) \mid \mathcal{F}_k\bigr)
  \Bigr].
\]
Then
\[
  \mathbb{E}\bigl[\|d_k\|^2 \mid \mathcal{F}_k\bigr]
  \;=\;
  \alpha_k^2 
  \,\mathbb{E}\Bigl[
    \bigl\|F(\theta_k,x_k) 
    \;-\; 
    \mathbb{E}\bigl(F(\theta_k,x_k) \mid \mathcal{F}_k\bigr)\bigr\|^2
    \,\Big|\,
    \mathcal{F}_k
  \Bigr].
\]
Since \(F(\theta_k,x_k)\) is almost surely bounded by \(G\) and has conditional variance at most \(\sigma^2\), we have
\[
  \mathbb{E}\Bigl[
    \bigl\|F(\theta_k,x_k) 
    - 
    \mathbb{E}\bigl(F(\theta_k,x_k) \mid \mathcal{F}_k\bigr)\bigr\|^2
    \,\Big|\,
    \mathcal{F}_k
  \Bigr]
  \;\le\;
  \sigma^2 + G^2.
\]
Thus
\[
  \mathbb{E}\bigl[\|d_k\|^2 \mid \mathcal{F}_k\bigr]
  \;\le\;
  \alpha_k^2 \,\bigl(\sigma^2 + G^2\bigr).
\]
\end{proof}
Hence the predictable variation $v_t=\sum_{k=1}^t \mathbb{E}[\|d_k\|^2\mid\mathcal{F}_{k-1}]$ is on the order of $\sum\alpha_k^2$, which is typically $O(\log t)$ or $O(1)$ if $\{\alpha_k\}$ decreases fast enough.

\textbf{(b) Freedman Concentration in a Polynomially Mixing Setting.}\;\\
While Freedman’s inequality typically requires \emph{martingale} increments, the Markov chain correlation means $\{d_k\}$ are not strictly i.i.d.  We handle this by a \emph{polynomial-ergodicity/coupling} argument from Lemma~\ref{lemma:coupling-argument-polynomial-mixing}]{...}.  Concretely:
\begin{itemize}
\item \textbf{Block Independence:} Partition time into blocks of size $b\approx t^\alpha$.  The distribution of each block is within $O(b^{-\beta})$ in total variation from stationarity.  This implies the partial sums of $d_k$ across blocks can be approximated by a block-wise martingale, introducing only a polynomially small $\approx t\,b^{-\beta}$ correction.
\item \textbf{Applying Freedman/Tropp:} Freedman’s tail bound for vector martingales \cite{tropp2011freedman,freedman1975tail} then yields
\[
  \mathbb{P}\!\bigl(\|M_t\|>\epsilon\bigr)
  \;\le\;
  d\,
  \exp\!\Bigl(
     -\tfrac{\epsilon^2}{\kappa\,v_t + b\,\epsilon}
  \Bigr)
  \;+\;
  \text{(polynomial mixing correction)}.
\]
Choosing $b\approx t^\alpha$ with $\alpha>0$ ensures the correction is $O(t\,t^{-\alpha\beta})=O(t^{1-\alpha\beta})$, which can be forced below $\delta/2$ for large $t$ if $\alpha\beta>1$.
\end{itemize}
Hence with probability at least $1-\tfrac{\delta}{2}$,
\[
  \|M_t\|
  ~\le~
  C_1\,\sqrt{\sum_{k=1}^t\alpha_k^2}\,\sqrt{\log\bigl(\tfrac{1}{\delta}\bigr)}
  ~+~
  \text{(possibly smaller correction terms)}.
\]
If $\sum_{k=1}^t\alpha_k^2=O(\log t)$, this is typically $O(\sqrt{\log t}\,\sqrt{\log(1/\delta)})$.  For many standard step-size choices, this remains bounded or slowly growing in $t$.  Denote that high-probability event by
\begin{align}
\label{eq:Mt-bound-merged}
\|M_t\|
~\le~
C_1
\quad
(\text{with probability }1-\tfrac{\delta}{2}).
\end{align}

\paragraph{Step 3: Remainder Term \boldmath$R_t$.}

Recall
\[
  R_t
  ~=~
  \sum_{k=1}^t \alpha_k\,\mathbb{E}[\,F(\theta_k,x_k)\bigr].
\]
Under stationarity (or near-stationarity), we write $\mathbb{E}[\,F(\theta_k,x_k)]\approx \overline{F}(\theta_k)$, where $\overline{F}(\theta)=\mathbb{E}_{x\sim\pi}[F(\theta,x)]$.  By definition, $\overline{F}(\theta^*)=0$ at the optimum $\theta^*$.  

\smallskip
\noindent
\begin{lemma}[Monotonicity/Negative Drift under Generalized Gradients]
\label{lemma:monotonicity}
Suppose there is a point $\theta^*$ such that $\overline{F}(\theta^*) = 0$, and assume $\overline{F}$ is \emph{monotone} in the sense that for all $\theta$,
\[
  \bigl\langle
    \overline{F}(\theta) - \overline{F}(\theta^*),\;
    \theta - \theta^*
  \bigr\rangle
  \;\ge\; 0,
  \quad\text{and}\quad
  \|\overline{F}(\theta)\|\;\le\; G.
\]
Then the ``remainder'' term
\[
  R_t
  \;=\;
  \sum_{k=1}^t \alpha_k \,\overline{F}\bigl(\theta_k\bigr)
\]
is $\mathcal{O}\!\bigl(\sum_{k=1}^t \alpha_k^2\bigr)$. In particular, if $\sum_{k=1}^\infty \alpha_k^2$ converges, then $R_t$ remains uniformly bounded as $t \to \infty$.
\end{lemma}

\begin{proof}[Proof]
By stationarity or near-stationarity, we assume
\[
  \mathbb{E}\bigl[F(\theta_k,x_k)\bigr]
  \;=\;
  \overline{F}(\theta_k).
\]
Hence
\[
  R_t
  \;=\;
  \sum_{k=1}^t \alpha_k \,\overline{F}(\theta_k).
\]
Since $\overline{F}$ is monotone and $\overline{F}(\theta^*) = 0$, we have
\[
  \bigl\langle
    \overline{F}(\theta_k),\;
    \theta_k - \theta^*
  \bigr\rangle
  \;=\;
  \bigl\langle
    \overline{F}(\theta_k)
    - \overline{F}(\theta^*),\;
    \theta_k - \theta^*
  \bigr\rangle
  \;\ge\; 0.
\]
Consider the purely ``deterministic'' update $\theta_{k+1} = \theta_k - \alpha_k \,\overline{F}(\theta_k)$.  A standard expansion gives
\[
  \|\theta_{k+1} - \theta^*\|^2
  \;=\;
  \|\theta_k - \theta^*\|^2
  - 2\,\alpha_k\,\bigl\langle
        \overline{F}(\theta_k),\,
        \theta_k - \theta^*
      \bigr\rangle
  \;+\;
  \alpha_k^2 \,\|\overline{F}(\theta_k)\|^2.
\]
Since the inner product is nonnegative by monotonicity, we have
\[
  \|\theta_{k+1} - \theta^*\|^2
  \;\le\;
  \|\theta_k - \theta^*\|^2
  + \alpha_k^2\,G^2.
\]
Summing over $k$ implies
\(
  \|\theta_k - \theta^*\|^2
\)
is bounded by
\(
  \|\theta_0 - \theta^*\|^2 + G^2\,\sum_{j=1}^k \alpha_j^2.
\)
In turn, one can show
\[
  \sum_{k=1}^t \alpha_k\,\overline{F}(\theta_k)
  \;=\;
  \underbrace{
    \sum_{k=1}^t \alpha_k \,\bigl[\overline{F}(\theta_k) - \overline{F}(\theta^*)\bigr]
  }_{\text{nonnegative by monotonicity}}
  \;+\;
  \text{(a constant depending on $\theta_0$ and the $\alpha_k^2$ terms).}
\]
Consequently,
\(
  \bigl\|\sum_{k=1}^t \alpha_k\,\overline{F}(\theta_k)\bigr\|
  = \mathcal{O}\!\bigl(\sum_{k=1}^t \alpha_k^2\bigr),
\)
as claimed.
\end{proof}
\smallskip
\noindent
\textbf{(b) \underline{Optional} H\"older Continuity Refinement.}
If we do assume
\[
  \|\overline{F}(\theta) - \overline{F}(\theta^*)\|
  ~\le~
  L\,\|\theta-\theta^*\|^\gamma,
\]
then we can improve the remainder bound.  Specifically, if we have an inductive guess \(\|\theta_k-\theta^*\|\le D\,k^{-\tfrac{\beta}{2}}+D'\,\alpha_k^\gamma\), we get
\[
  \|\overline{F}(\theta_k)\|
  ~\le~
  L\,\|\theta_k-\theta^*\|^\gamma
  ~\le~
  L\bigl(D\,k^{-\tfrac{\beta}{2}}+D'\,\alpha_k^\gamma\bigr)^\gamma.
\]
\paragraph{Inductive Argument}
Assume 
\[
  \|\overline{F}(\theta) - \overline{F}(\theta^*)\|
  \;\;\le\;\;
  L\,\|\theta - \theta^*\|^\gamma,
  \quad
  \gamma \in (0,1],
\]
and that the Markov chain is (near) stationary so that
\[
  \mathbb{E}[\,F(\theta_k,x_k)\,]
  \;=\;
  \overline{F}(\theta_k)
  \;+\;
  \mathcal{O}\!\bigl(k^{-\beta}\bigr)
  \quad
  (\text{or exact if fully stationary}).
\]

\smallskip
\noindent
\textbf{Step 1: Inductive Hypothesis.}
Suppose we aim to show 
\[
  \|\theta_k - \theta^*\|
  \;\le\;
  D\,k^{-\tfrac{\beta}{2}}
  \;+\;
  D'\,\alpha_k^\gamma
  \quad
  \text{for each }k.
\]
For $k=1$, we pick $D,D'$ large enough so that
\(\|\theta_1 - \theta^*\|\le D + D'\,\alpha_1^\gamma\),
which is always possible if the initial distance
\(\|\theta_1-\theta^*\|\) is finite.

\smallskip
\noindent
\textbf{Step 2: Inductive Step.}
Assume the bound holds at time \(k\). Then
\[
  \theta_{k+1}
  \;=\;
  \theta_k \;-\;\alpha_k\,F(\theta_k,x_k).
\]
Taking expectation (or working directly under stationarity) gives
\[
  \|\theta_{k+1} - \theta^*\|
  \;\approx\;
  \Bigl\|\,
    \theta_k - \theta^*
    \;-\;
    \alpha_k\,\overline{F}(\theta_k)
  \Bigr\|
  \;+\;
  \text{(small mixing error)}.
\]
By H\"older continuity,
\[
  \|\overline{F}(\theta_k)\|
  \;\le\;
  L\,\|\theta_k - \theta^*\|^\gamma
  \;\le\;
  L\,\Bigl(
         D\,k^{-\tfrac{\beta}{2}}
         \;+\;
         D'\,\alpha_k^\gamma
       \Bigr)^\gamma.
\]
Since $\gamma \in (0,1]$, we can simplify
\(\bigl(a+b\bigr)^\gamma \leq a^\gamma + b^\gamma\) up to constant factors, giving us terms like $k^{-\tfrac{\beta\gamma}{2}}$ and $\alpha_k^{\gamma^2}$. Thus
\[
  \alpha_k\,\|\overline{F}(\theta_k)\|
  \;\le\;
  \alpha_k \,\Bigl(
     D\,k^{-\tfrac{\beta}{2}} + D'\,\alpha_k^\gamma
  \Bigr)^\gamma
  \;=\;
  \mathcal{O}\!\bigl(
     k^{-\tfrac{\beta\gamma}{2}}
  \bigr)
  \;+\;
  \mathcal{O}\!\bigl(
     \alpha_k^{\,1+\gamma^2}
  \bigr).
\]
One checks that (for appropriate choices of $D,D'$) this is sufficient to ensure
\[
  \|\theta_{k+1} - \theta^*\|
  \;\le\;
  D\,(k+1)^{-\tfrac{\beta}{2}}
  \;+\;
  D'\,\alpha_{k+1}^\gamma,
\]
closing the induction.

\smallskip
\noindent
\textbf{Step 3: Summation for the Remainder.}
Since
\[
  R_t
  \;=\;
  \sum_{k=1}^t 
    \alpha_k \,\overline{F}(\theta_k),
\]
we use the above bound 
$\|\overline{F}(\theta_k)\| \le L\,(\dots)$ to get
\[
  \bigl\|R_t\bigr\|
  \;\le\;
  \sum_{k=1}^t \alpha_k \,\|\overline{F}(\theta_k)\|
  \;=\;
  \sum_{k=1}^t
     \mathcal{O}\!\bigl(
       \alpha_k \,k^{-\tfrac{\beta\gamma}{2}}
     \bigr)
  \;+\;
  \sum_{k=1}^t
     \mathcal{O}\!\bigl(
       \alpha_k \,\alpha_k^{\gamma^2}
     \bigr).
\]
For typical step-size schedules (e.g.\ $\alpha_k \sim 1/k^\omega$) and $\gamma\le1$, each sum yields a polynomially decaying term in $k$. Concretely, one finds
\[
  \|R_t\|
  \;=\;
  \mathcal{O}\!\bigl(t^{-\tfrac{\beta\gamma}{2}}\bigr)
  \;+\;
  \text{(smaller terms if }\gamma\le1\text{)}.
\]
Thus the remainder $R_t$ remains small at a polynomial rate, yielding the high-probability bound
\[
  \|\theta_t - \theta^*\|
  \;\le\;
  C\,t^{-\tfrac{\beta}{2}}
  \;+\;
  C'\,\alpha_t^\gamma
\]
once we also control the martingale term $M_t$ by Freedman‐type concentration.
\smallskip
\noindent
\textbf{(c) Final Form of \boldmath$R_t$.}
Combining monotonicity or the optional H\"older step with polynomial mixing ensures that $R_t$ shrinks at a polynomial rate.  If \(\gamma=1\), we get $R_t=O(t^{-\frac{\beta}{2}})$; if $0<\gamma<1$, we get $R_t=O(\alpha_t^\gamma)$ or $t^{-\tfrac{\beta\gamma}{2}}$.  Denote such a bound as
\begin{align}
\label{eq:Rt-bound-merged}
\|R_t\|
~\le~
C_2\,t^{-\tfrac{\beta\gamma}{2}}
\quad\text{(w.h.p.)},
\end{align}
noting that $\gamma$ can be replaced by $1$ (or the relevant exponent) if no further continuity assumption is used.

\paragraph{Step 4: Combine Bounds via Union Bound.}

We have, from \eqref{eq:theta-t-decomp},
\[
  \theta_t - \theta^*
  ~=~
  M_t + R_t.
\]
From \eqref{eq:Mt-bound-merged}, with probability $\ge1-\frac{\delta}{2}$,
\[
  \|M_t\|
  ~\le~
  C_1.
\]
From \eqref{eq:Rt-bound-merged}, with probability $\ge1-\frac{\delta}{2}$,
\[
  \|R_t\|
  ~\le~
  C_2\,t^{-\tfrac{\beta\gamma}{2}}
  \quad(\text{or }C_2\,\alpha_t^\gamma\text{ in simpler forms}).
\]
By a union bound, with probability $\ge1-\delta$,
\begin{align}
\|\theta_t - \theta^*\|
~\le~
C_1 + C_2\,t^{-\tfrac{\beta\gamma}{2}}
\quad(\text{plus possibly $\alpha_t^\gamma$ terms}).
\label{eq:theta-final-merged}
\end{align}
If the step-size is chosen so that $M_t=O(\alpha_t^\gamma)$ (e.g.\ $\sum\alpha_t^2=O(\alpha_t^{2\gamma})$), we get a final statement:
\[
  \|\theta_t-\theta^*\|
  ~\le~
  C\,t^{-\tfrac{\beta}{2}}
  ~+~
  C'\,\alpha_t^\gamma
\]
with probability at least $1-\delta$.  
\end{proof}
