\section{Experimental Evaluation}\label{sec:experiment}

In this section, we implement \chatiot\ and conduct extensive evaluation. 
We study the effectiveness of \chatiot\ and answer the following questions:

\noindent \textbf{Q1:} How does \datakit\ extract appropriate field selection from IoT threat datasets and convert them into well-structured documents for retrieval and LLM analysis?
What are the optimal chunking strategies for each kind of document? (\S~\ref{sec:exp-dataprocess})

\noindent \textbf{Q2:} What are the advantages of our system? Can \chatiot\ effectively generalize and improve the capabilities of the most advanced LLMs available in processing IoT security issues? (\S~\ref{sec:exp-llmeval})

\noindent \textbf{Q3:} Can \chatiot\ be a practical useful IoT assistant over using LLM alone? How about the feedback from real-world human evaluation? (\S~\ref{sec:exp-human})


\subsection{Setup}
\noindent \textbf{Testbed.}
We implement \chatiot\ in Python 3.10.13, utilizing large language models LLaMA3:8B \& 70B and LLaMA3.1:8B \& 70B provided by Groq\footnote{\scriptsize \url{https://chat.groq.com/}}, GPT-4o-mini and 4o provided by OpenAI\footnote{\scriptsize \url{https://platform.openai.com/docs/models/gpt-4o}}. All these LLMs are utilized by calling their APIs.
For building the vector store, we employed Elasticsearch 8.13.2~\cite{elasticsearch2018elasticsearch}, running on Docker Desktop 4.29.0~\cite{docker-desktop}. 
All components were integrated using the LangChain library (version 0.2.5)~\cite{langchain2024}. 
The WebApp was developed using Streamlit (version 1.33.0)~\cite{streamlit}. 
Experiments were conducted on a MacBook Pro equipped with an Apple M3 Pro CPU (11 cores) and 18 GB of RAM, running macOS 14.6.1 with the Darwin 23.6.0 kernel.

\smallskip
\noindent \textbf{Data Sources.}
We collect five kinds of IoT security and threat datasets from the public Internet:
\begin{itemize}
\item[\romannumeral1)] VARIoT vulnerabilities~\cite{VARIoT_db}: This dataset catalogs known vulnerabilities in various IoT devices, offering detailed information about the potential risks associated with each vulnerability.
\item[\romannumeral2)] VARIoT exploits~\cite{VARIoT_db}: This dataset contains exploits targeting IoT devices, providing insights into the techniques and methods attackers use to compromise these systems.
\item[\romannumeral3)] MITRE ATT\&CK ICS TTPs~\cite{strom2018mitre}: This dataset outlines the tactics, techniques, and procedures (TTPs) employed by adversaries specifically in industrial control systems (ICS), which often include IoT-related TTPs as well.
\item[\romannumeral4)] Threat reports: We collect $17$ public threat reports from VXUG\footnote{\scriptsize \url{https://vx-underground.org/}} about emerging threats and vulnerabilities, offering analysis and recommendations for mitigating potential risks.
\item[\romannumeral5)] Cybersecurity labelling schemes\footnote{\scriptsize \url{https://www.csa.gov.sg/our-programmes/certification-and-labelling-schemes}}\footnote{\scriptsize \url{https://tietoturvamerkki.fi/en/products}}
\footnote{\scriptsize \url{https://www.nemko.com}}
\footnote{\scriptsize \url{https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.02042022-2.pdf}}: These schemes provide information on the security posture of various IoT products, helping consumers and organizations assess the security standards and certifications achieved by specific devices.
\end{itemize}
These datasets provide comprehensive insights into the current landscape of IoT threats, enabling us to enhance our system's capabilities.


%For each user case defined in \S~\ref{sec:usecase}, we propose 10 queries. 
%Then, we compare the generated answers of \chatiot\ with those generated by only using the corresponding LLM without augmented retrieval.
%and the commercial LLMs such as OpenAI ChatGPT3.5 and Google Gemini, to illustrate our improvements in IoT security.


\subsection{Fields Selection \& Chunking Strategy}\label{sec:exp-dataprocess}

This section shows the experimental evaluation for field selection and chunking strategy optimization.

\input{Tables/fieldselect-eva}

\subsubsection{Fields for Page\_Content \& Metadata}\label{sec:exp-field}
Recall that for each dataset, we should first determine the fields for page\_content and metadata for documents before building self-querying retrievers (see \S~\ref{sec:toolkit}).
For each dataset, we sample 3 items, list the fields' names, and instruct the LLM to select the suitable fields. The results are as follows:
\begin{itemize}
    \item We use three LLMs: LLaMA3:8B, LLaMA3.1:70B, and GPT4-o, to select fields for VARIoT vulnerabilities, exploits, and MITRE ATT\&CK ICS.
    The experimental results are summarized in Table~\ref{tab:fieldsel-eva}.
    While there are slight variations in their selections, there is consensus on the crucial decisions:
    For instance, in the case of the VARIoT vulnerabilities, all LLMs select \texttt{title} and \texttt{description} for page\_content, and \texttt{id} and \texttt{products} for metadata.
    Similar selections are observed for the VARIoT exploits and MITRE ATT\&CK ICS datasets.

    \item For threat reports, which are typically unstructured, we use the report's content as page\_content and the title as metadata (Note we do not use self-querying retrieval for threat reports). 
    The CLS schemes consist solely of metadata with no descriptive content, so we leave page\_content blank and utilize the metadata for self-querying retrievers.
\end{itemize}
The specific fields selected for page\_content and metadata are detailed in Table~\ref{tab:fieldselect}.


\input{Tables/FieldSelect}


\subsubsection{Chunking Evaluation}\label{sec:exp-chunk}

%\input{Tables/vulchunk_test}

\input{Tables/ics_chunk}

To optimize the chunking strategy for documents' page\_content, we utilize the Ragas library~\cite{es2023ragas} in conjunction with all-MiniLM\footnote{\scriptsize \url{https://ollama.com/library/all-minilm}} (for embedding) and LLaMA3:8B (for evaluation) to search the most suitable chunking size, overlap, and splitting method for each dataset. We use content \textit{precision} and \textit{recall} as the key metrics: 
\begin{itemize}
    \item Precision measures whether all relevant items retrieved by the model are ranked higher than the irrelevant items;
    \item Recall measures how much of the relevant content is retrieved based on the annotated answers and the retrieved context.
\end{itemize}
Both precision and recall are evaluated within the range $[0,1]$, where a higher score indicates better performance.
As the datasets contain a huge number of samples, for practical efficiency, we select a subset of $1,000$ samples from each dataset except threat reports\footnote{\scriptsize We use all collected threat reports for generating testset and evaluation.}, generate a testset of $50$ items, and conduct evaluations based on the subset and testset.
This might not result in the optimal chunking size, overlap, and splitter method, but is enough to get a reasonable and useful chunking strategy for our practical applications.
As shown in Table~\ref{table:chunking}, we test the following commonly used configurations:
\begin{itemize}
    \item chunk sizes: $\{500, 1000, 1500, 2000\}$;
    \item overlaps: $\{50, 100, 150, 200\}$; 
    \item splitters: \texttt{Character}, \texttt{RecursiveCharacter}, and \texttt{TokenText}.
\end{itemize}
Our objective is to achieve high precision and recall simultaneously, ensuring that the system retrieves as many relevant documents as possible while minimizing irrelevant content. 
From the experimental results in Table~\ref{table:chunking}, it is easy to see that using the \texttt{RecursiveCharacter} splitter with a chunk size of $500$ and an overlap of $100$ is the most effective strategy for the VARIoT vulnerabilities, offering the best trade-off between precision and recall. 
Similarly, we can choose the suitable chunking strategies for VARIoT exploits, MITRE ATT\&CK ICS, and threat reports. The details are illustrated in Table~\ref{tab:chunkres}.



\begin{table}[]
    \centering
    \caption{Optimized chunking strategy for VARIoT vulnerabilities, exploits, ICS, and threat reports. \texttt{RecurChar} is short for \texttt{RecursiveCharacter}.}
    \label{tab:chunkres}
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{c|ccc}
    \toprule \toprule
     {Dataset} & {Size} & {Overlap} & {Splitter Method} \\ \midrule
     VARIoT Vulns. & $500$ & $100$ & \texttt{RecurChar}\\
     VARIoT Exps. & $1000$ & $150$ & \texttt{TokenText} \\
     ICS & $1000$ & $200$ & \texttt{Character} \\
     Threat Reports & $500$ & $200$ & \texttt{TokenText} \\
     \bottomrule \bottomrule
    \end{tabular}}
\end{table}



\subsection{LLMs-based Evaluation of Outputs}\label{sec:exp-llmeval}

\input{Tables/LLM-eval}

As there is no public Question-Answer dataset about IoT security and threat intelligence, we synthesize 50 common IoT security-related questions (10 questions for each kind of user).
To evaluate our improvements, we compare \chatiot's outputs with the answers generated by underlying LLM alone (denoted as LLM-A), which is not equipped with our introduced IoT data sources. 
And we let another LLM be the evaluator and measure the quality of outputs by four metrics: \textit{Reliability}, \textit{Relevance}, \textit{Technical}, and \textit{Friendliness} as follows:
\begin{itemize}
    \item Reliability: the trustworthiness and reliability of each answer, ensuring it is plausible and aligns with known IoT best practices and standards.
    \item Relevance: Assess how well the answer addresses the specific question and meets the user’s needs, considering their role and context in the IoT ecosystem.
    \item Technical: Judge the appropriateness and precision of technical language, including IoT research, standards, protocols, and relevant technical aspects. Ensure that the answer demonstrates a solid understanding of IoT technologies.
    \item Friendliness: Determine how easy the answer is to comprehend, focusing on clarity for the user’s role, and how well the answer provides actionable steps or solutions tailored to the user’s IoT security needs. 
\end{itemize}
All scores are in $[0,5]$, where $5$ is the highest. 
For each question, the answers generated by \chatiot\ and corresponding LLM-A are inputted into the evaluator simultaneously. 
This approach ensures that both answers for each question are evaluated within the same internal state of the evaluator. By doing so, we aim to reduce the impact of LLM randomness as much as possible and enable a fair comparison between \chatiot\ and LLM-A.
The prompt for evaluation is shown in Figure~\ref{fig:llmevalprompt}.

\input{Figures/prompteval}

\input{Tables/OutLLML}

We develop five versions of \chatiot\ using LLaMA3:8B, LLaMA3.1:8B, LLaMA3.1:70B, GPT-4o-mini, and GPT-4o, and employ LLaMA3:70B to evaluate them.
Table~\ref{tab:exp_llmsmall} shows the experimental results for moderate LLMs LLaMA3:8B, LLaMA3.1:8B, and GPT-4o-mini; and Table~\ref{tab:exp_llm} present the results for more advanced LLMs LLaMA3.1:70B and GPT-4o.
We also compute our improved scores over LLM-A in the tables.
From these results, several key observations can be made: 
\begin{itemize}
\item \chatiot\ significantly enhances the moderate LLM's performance in the IoT security domain. As shown in Table~\ref{tab:exp_llmsmall}, \chatiot\ achieves higher scores across most metrics for use cases \textit{Consumer}, \textit{Security Analyst}, \textit{Technical Officer}, and \textit{Developer}.
This is expected, as \chatiot\ integrates domain-specific IoT security knowledge, \eg, vulnerabilities and exploits, and tailors responses to be more user-friendly and relevant.
As illustrated in Table~\ref{tab:exp_llm}, \chatiot\ can also improve the advanced LLMs' capabilities in IoT problems.

\item However, \chatiot\ does not always outperform the baseline LLMs.
Taking the use case \textit{Trainer}, when using LLaMA series models, \chatiot\ even performs slightly worse; when using GPT-4o-mini and GPT-4o, the improvements achieved by \chatiot\ are much less than that for the other use cases.
This is likely due to the external data introduced in \chatiot\ focusing mainly on vulnerabilities, exploits, and TTPs, while lacking sufficient information on course training materials.
As a result, \chatiot\ excels at producing technical and security-centric content but may overlook broader aspects like training programs.
\end{itemize}

The above analysis also highlights the importance of incorporating external knowledge to bolster LLMs in specialized domains.
Fortunately, additional information, such as training materials, can easily be integrated into \chatiot\ using our \datakit\ toolkit.

\subsection{Analysis of Human Evaluation}\label{sec:exp-human}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/human.jpg}
    \caption{The human evaluation of \chatiot\ and LLM-A. The LLM of \chatiot\ and LLM-A method is GPT-4o in experiments.}
    \label{fig:human-exp}
\end{figure}


In Figure~\ref{fig:human-exp}, we present the results of human evaluations comparing \chatiot\ with GPT-4o, noting that \chatiot\ is built on top of GPT-4o in this experiment. We select four Q\&A pairs for each user type and display them as a survey. 
Human evaluators are asked to select the answer they find more suitable. 
To aid in their decisions, we provide them with evaluation metrics (as described in \S~\ref{tab:exp_llmsmall}), but do not require them to score each metric. 
This approach is intended to streamline the evaluation process, making it more akin to real-world scenarios where users prioritize ease of decision-making.

From the experimental results, it is clear that \chatiot\ consistently outperforms GPT-4o across all use cases. This aligns with expectations, as \chatiot\ integrates additional IoT-specific intelligence into the LLM. Notably, \chatiot\ demonstrates the greatest improvement for Security Analyst and the least for Technical Officer. The former result aligns with Table~\ref{tab:exp_llm}, where the comparison for Security Analyst shows the most significant difference.
Additionally, we give a use case comparison for \textit{Security Analyst} between GPT-4o and GPT-4o-based \chatiot. The formal analysis can be referred to Appendix~\ref{appendix:use-case-cmp}.
However, while Table~\ref{tab:exp_llm} suggests that the least improvement is for Trainer, the human evaluation indicates Technical Officer experiences the smallest gains. This discrepancy can be explained by: \romannumeral1) The improvements for Technical Officer, though better than those for Trainer, particularly in top metric scores, may not be as easily discernible to humans as other use cases, making it harder for them to identify notable differences; \romannumeral2) The Q\&A tasks for Technical Officer are generally more complex and technical than those for Trainer, making it easier for users to select a better answer in Trainer case even when improvements are less significant.

We acknowledge that our Q\&A survey might carry some biases since it is impossible to cover all use cases and questions/queries. However, we have made a strong effort to choose the most common and representative Q\&A pairs for this survey, which we believe adequately reflects the improvements achieved.
Nonetheless, \chatiot\ consistently provides better results over GPT-4o alone in the IoT security domain, demonstrating its enhanced capability to address IoT-specific challenges.




%Faithfulness: This measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better. (only ChatIoT) 

%Context Relevance (ConRel) (only ChatIoT)

%answer\_relevancy: measures how relevant the generated response is to the given question (ChatIoT and baselines)


%llm\_grader: leverages a general-purpose zero-shot prompt to rate responses from an LLM to a given question on a scale from 1-10; (ChatIoT and baselines) 

%Models: llama3:8b