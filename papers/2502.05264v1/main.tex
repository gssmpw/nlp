\documentclass[prl,aps,superscriptaddress,footinbib,twocolumn]{revtex4-2}\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage[unicode=true,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true]
 {hyperref}
\hypersetup{
 linkcolor=magenta, urlcolor=blue, citecolor=blue, pdfstartview={FitH}, hyperfootnotes=false, unicode=true}
\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.








\usepackage{amsfonts}\usepackage{tabularx}\usepackage{dcolumn}\usepackage{bm}\usepackage{graphicx}\usepackage{epstopdf}
\usepackage{times}
\usepackage{cleveref}
% \usepackage{braket}
\setcounter{MaxMatrixCols}{10}
\hypersetup{urlcolor=blue}

\def\CTeXPreproc{Created by ctex v0.2.12, don't edit!}

\def\ket#1{\left|#1\right\rangle}
\def\bra#1{\left\langle#1\right|}
\def\braket#1{\left\langle#1\right\rangle}
\def\ketbra#1{\left|#1\right\rangle\!\left\langle#1\right|}

\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bx}{\bm{x}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator{\Tr}{Tr}
\newtheorem{theorem}{Theorem}
\newcommand{\qy}[1]{{\color{blue}{Qi: \textbf{#1}}}}
\newcommand{\gsy}[1]{{\color{blue}{gsy: \textbf{#1}}}}
\newcommand{\wk}[1]{{\color{cyan}{Li: \textbf{#1}}}}
\newcommand{\TODO}[1]{{\color{red}{TODO: \textbf{#1}}}}
\makeatother

\begin{document}

\title{Quantum automated learning with provable and explainable trainability}

\author{Qi Ye}\thanks{These authors contributed equally}\affiliation{Center for Quantum Information, IIIS, Tsinghua University, Beijing 100084, China}\affiliation{Shanghai Qi Zhi Institute, Shanghai 200232, China}\affiliation{School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts 02138, USA}
\author{Shuangyue Geng}\thanks{These authors contributed equally}\affiliation{Center for Quantum Information, IIIS, Tsinghua University, Beijing 100084, China}\affiliation{Shanghai Qi Zhi Institute, Shanghai 200232, China}

\author{Zizhao Han}\affiliation{Center for Quantum Information, IIIS, Tsinghua University, Beijing 100084, China}
\author{Weikang Li}
\affiliation{Center for Quantum Information, IIIS, Tsinghua University, Beijing 100084, China}

\author{L.-M. Duan}
\affiliation{Center for Quantum Information, IIIS, Tsinghua University, Beijing 100084, China}
\affiliation{Hefei National Laboratory, Hefei 230088, China}


\author{Dong-Ling Deng}
\email{dldeng@tsinghua.edu.cn}
\affiliation{Center for Quantum Information, IIIS, Tsinghua University, Beijing 100084, China}\affiliation{Shanghai Qi Zhi Institute, Shanghai 200232, China}
\affiliation{Hefei National Laboratory, Hefei 230088, China}




\begin{abstract}
{Machine learning is widely believed to be one of the most promising practical applications of quantum computing. Existing quantum machine learning schemes typically employ a quantum-classical hybrid approach that relies crucially on gradients of model parameters. Such an approach lacks provable convergence to global minima and will become infeasible as quantum learning models scale up. Here, we introduce quantum automated learning, where \textit{no} variational parameter is involved and the training process is converted to quantum state preparation. In particular, we encode training data into unitary operations and iteratively evolve a random initial state under these unitaries and their inverses, with a target-oriented perturbation towards higher prediction accuracy sandwiched in between. Under reasonable assumptions, we rigorously prove that the evolution converges exponentially to the desired state corresponding to the global minimum of the loss function. We show that such a training process can be understood from the perspective of preparing quantum states by imaginary time evolution, where the data-encoded unitaries together with target-oriented perturbations would train the quantum learning model in an automated fashion. We further prove that the introduced quantum automated learning paradigm features good generalization ability with the generalization error upper bounded by the ratio between a logarithmic function of the Hilbert space dimension and the number of training samples. In addition, we carry out extensive numerical simulations on real-life images and quantum data to demonstrate the effectiveness of our approach and validate the assumptions. Our results establish an unconventional quantum learning strategy that is gradient-free with provable and explainable trainability, 
which would be crucial for large-scale practical applications of quantum computing in machine learning scenarios. 
}
\end{abstract}

\maketitle

\noindent \textbf{\large{}Introduction}{\large\par}

\noindent Machine learning, the core of artificial intelligence (AI), has achieved dramatic success~\cite{LeCun2015Deep,Goodfellow2016Deep,Wang2023Scientific}. A number of long-standing challenging problems, such as playing the game of Go~\cite{Silver2016Mastering,Silver2017Mastering}, predicting protein structures~\cite{Senior2020Improved}, and automated theorem proving at the olympiad level~\cite{Trinh2024Solving},  have been cracked in recent years, elevating AI to new scientific heights. Yet, as Moore's law is approaching the end and machine learning models become unprecedentedly large devouring a tremendous amount of resources, 
further development of AI would be subject to the limitations of computational power and energy consumption~\cite{2023AI}. 
Quantum computing promises a potential way out of this dilemma. 

Indeed, parallel to machine learning, the field of quantum computing has also made remarkable progress in the past decades~\cite{Preskill2018Quantum}, with experimental demonstrations of quantum supremacy~\cite{Arute2019Quantum,Zhong2020Quantum,Wu2021Strong} and error correction~\cite{Bluvstein2024Logical,Acharya2024Quantum} marked as the latest milestones. 
A variety of quantum algorithms have been proposed to enhance, speed up or innovate machine learning~\cite{Liu2021Rigorous,Huang2021Power}, giving rise to a new vibrant research frontier of quantum machine learning~\cite{Biamonte2017Quantum,Dunjko2018Machine,DasSarma2019Machine}. Proof-of-principle experiments have been reported with current noisy intermediate-scale quantum devices~\cite{Herrmann2022Realizing,Saggio2021Experimental,Ren2022Experimental,Hu2019Quantum,Huang2021Quantum,Gong2023Quantum,Peters2021Machine}, including these on quantum convolutional networks~\cite{Herrmann2022Realizing},   reinforcement learning~\cite{Saggio2021Experimental},   adversarial learning~\cite{Ren2022Experimental}, federated learning~\cite{Liu2025Practical}, and  continual learning~\cite{Zhang2024Quantum}. 
Most of these quantum learning schemes rely on parametrized quantum circuits and utilize gradient-based approaches for training~\cite{Mitarai2018Quantum}. They face three major difficulties when scaling up. First, in classical deep learning the gradients can be calculated in an efficient and parallel way by the backpropagation algorithm~\cite{Wythoff1993Backpropagation}. Whereas, in quantum scenarios the implementation of the backpropagation algorithm is resource demanding~\cite{Beer2020Training,Pan2023Deep} and quantum gradients are usually obtained one-by-one for each variational parameter~\cite{Mitarai2018Quantum,Schuld2019Evaluating}. This renders large-scale quantum learning impractical, especially for models as large as GPT-4~\cite{OpenAI2024GPT4} and GLaM~\cite{Du2022GLaM} with up to trillions of parameters. Second, quantum landscapes can have exponentially many local minima~\cite{Bittel2021Training} and exhibit the notorious barren plateau phenomenon~\cite{Larocca2024Review,McClean2018Barren,Cerezo2021Cost}, where the gradients vanish exponentially with the system size. Consequently, 
the gradient-based training of variational quantum learning models becomes intrinsically hard as they scale up. Third, there lacks a clear understanding of the gradient-based training dynamics to guide further development of large-scale quantum learning models. 

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig_qal.pdf}
    \caption{\textbf{Comparison between two different quantum learning paradigms}. Upper panel:  a sketch of quantum  learning and data encoding schemes. Lower left panel: an illustration of gradient-based quantum learning. In this paradigm, the input datum $\bx$ is usually first encoded into a quantum state $\ket{\phi(\bx)}$ and then fed into a parametrized quantum circuit $U(\btheta)$. For a $k$-class classification task, one measures about $\log k$ qubits and obtains the gradient of a predefined loss function on a classical computer based on the measurement results. With the obtained gradient, one updates the variation parameters $\btheta$. This process is iterated a number of times until the loss function no longer decreases (the training process). At the inference stage, one inputs $\ket{\phi(\bx)}$ for an unseen sample $\bx$ into the variational circuit with optimized parameters and then does the same measurement on $\log k$ qubits to make predictions. Lower right panel: a sketch of quantum automated learning. In this scenario,  the input datum $\bx$ is encoded into $U(\bx)$ and the training process is transferred into quantum state preparation, which in turn can be accomplished by a dissipation process with guaranteed convergence. At the inference stage, we evolve the prepared state $\ket{\psi^*}$ with $U(\bx)$ and then do the measurement to make a prediction for the unseen sample $\bx$. The quantum automated learning approach involves no variational parameter and is inherently gradient-free, thus it is more scalable to large-scale practical quantum learning applications.
}
    \label{fig:QAL}
\end{figure*}

In this paper, we circumvent these difficulties by introducing quantum automated learning (QAL) with provable and explainable trainability. Our approach involves \textit{no} variational parameter and thus is inherently gradient-free and scalable (see Fig.~\ref{fig:QAL}). Specifically, we focus our discussion on supervised learning with pre-labeled training data. We convert the training process into quantum state preparation for a target state, on which the outcome of a data-encoded measurement gives the predicted label of the data sample. To prepare the desired state, we design a dissipation process for each training sample that drives a random initial state towards the target state in an automated manner. 
We prove that such a dissipation process converges exponentially to the target state corresponding to the global minimum of the loss function. We show that this training process has a clear physical interpretation: it  essentially implements an imaginary time evolution that cools down
the system to the ground state of a data-encoded Hamiltonian. In addition, we prove that 
the generalization error of our approach is upper bounded by $\sqrt{\log (D) /N}$, where $D$ and $N$ denote the Hilbert space dimension and the number of training samples, respectively. 
To demonstrate
the effectiveness of our approach and validate the assumptions, we carry out extensive numerical simulations on real-life images (e.g., MNIST Dataset~\cite{lecun1998mnist} and Fashion MNIST Dataset~\cite{xiaoFashionMNISTNovelImage2017}) and quantum data (e.g., thermal and localized quantum many-body states). Our QAL strategy eludes the three pronounced challenges faced by conventional gradient-based quantum learning schemes, advancing towards the goal of scaling up quantum learning to large-scale practical applications. 





\vspace{.5cm}
\noindent\textbf{\large{}Quantum automated learning}{\large\par}
\noindent We first introduce the general framework for quantum automated learning. For convenience, we focus our discussion on classification tasks in the setting of supervised learning~\cite{Li2022Recent}, where we assign a label $y(\bx)\in Y$ to an input data sample $\bx \in X$, with $Y$ being a finite label set of size $k$ and $X$ the set of all possible samples. We denote the training set as $S_N=\{(\bx_1,y_1),\cdots,(\bx_N,y_N)\}$, where $\bx_i\in X$ is sampled from an unknown distribution $\cD$, $y_i\in Y$ represents the label of $\bx_i$, and $N$ is the size of the training set. The goal of classification is to predict the labels of unseen samples drawn from $\cD$ with high probability.  

A widely studied strategy in solving classification problems relies on quantum neural networks~\cite{Cerezo2021Variational,Li2022Quantum}, where the input datum is first encoded into a quantum state $|\phi (\bx)\rangle$ and then fed into a variational quantum circuit $U(\btheta)$ with parameters collectively denoted by $\btheta$. To train the quantum neural networks, one introduces a loss function that measures the discrepancy between predicted and true labels, and minimizes it on training data via the gradient descent method. The gradients are usually obtained by using finite differences or the parameter-shift rule assisted by a classical computer~\cite{Mitarai2018Quantum,Schuld2019Evaluating}. An illustration of this quantum-classical hybrid approach is sketched in the lower left panel of Fig.~\ref{fig:QAL}. Such an approach is a straightforward extension of classical neural networks to the quantum domain, yet three pronounced difficulties hinder its scalability. The first difficulty concerns the impractical time consumption in obtaining the gradients. Unlike in classical deep learning where the gradients can be calculated in parallel through the backpropagation algorithm, quantum gradients are usually obtained one-by-one for each variational parameter. In fact, even calculating the gradient for a single parameter demands  executing the variational quantum circuit thousands of times. The second difficulty originates from the pathological %undesirable 
landscapes of quantum neural networks---they typically bear exponentially many local minima and suffer from barren plateaus~\cite{Larocca2024Review,McClean2018Barren,Cerezo2021Cost}. Overcoming this difficulty requires exponentially many runs of the quantum circuit. The third difficulty regards the shortage of a physical understanding of the training dynamics under gradient descent, which precludes a prior guide for tuning hyperparameters or designing more efficient gradient-based algorithms without resorting to trial and error. These three difficulties make the quantum neural network based approach impractical when scaled up, especially for large models with trillions of parameters. 
 

Unlike the gradient-based approach discussed above, quantum automated learning exploits an entirely different strategy that involves no variational parameter and is gradient-free inherently.  As sketched in the lower right panel of Fig.~\ref{fig:QAL},  our general recipe goes as the following: (i) start with a random $n$-qubit initial state $|\psi\rangle$, where $n=O(\log |\bx|)$ 
with $|\bx|$ the dimension of the data sample $\bx$ (Supplementary Sec. II A); (ii) randomly choose a data sample $(\bx,y)$ from the training set and utilize a unitary-encoding scheme to encode $\bx$ into a unitary $U(\bx)$ (Methods and Supplementary Sec. II B); (iii) evolve the system with $U(\bx)$, a $y$-dependent perturbation $M_y$, and $U(\bx)^{\dagger}$; (iv) repeat steps (ii) and (iii) for $T$ times to drive the system to the target state $|\psi^*\rangle$; (v) for an unseen datum $\bx'$, evolve $|\psi^*\rangle$ with $U(\bx')$ and then measure $\sim\log k$ qubits to output the predicted label $y'$. In this scheme, steps (i)-(iv) constitute the training process and step (v) serves as the inference. We stress that such a scheme does not require a quantum random access memory~\cite{Giovannetti2008Quantum}, which is resource-demanding and has not been realized in experiments so far, to load classical data. Indeed, a datum $\bx$ is input into the training process through $U(\bx)$, which consists of multiple layers of single- and two-qubit gates specified by $\bx$. These gates are readily accessible in current experiments. In step (iii), the perturbation $M_y=\ketbra{y}+(1-\eta)(\mathbf{I}-\ketbra{y})$, where $\ketbra{y}$ denotes a $\log k$-qubit state that encodes the label $y$, $\eta$ is the learning rate,  and $\mathbf{I}$ is the identity matrix. This perturbation aims to suppress the probability of incorrect prediction and is implemented by block encoding into a unitary $U_y$ with an ancillary qubit combined with post-selection (Methods and Supplementary Sec. II C). In practice, one may need to compile $U_y$ into hardware-compatible elementary gates. This can be accomplished efficiently by the Solovay-Kitaev algorithm~\cite{Dawson2006TheSolovay} or the quantum compiling algorithm based on reinforcement learning~\cite{Zhang2020Topological} in general. More efficient compiling of $U_y$ exists given its special structure (Supplementary Sec. II C).  


From a high-level perspective, the QAL scheme formulates the training process as quantum state preparation, which in turn is accomplished by a data-dependent dissipation process that drives the system to the target state in a fully autonomous way. It naturally circumvents the challenges in designing appropriate quantum neural networks for given tasks and obtaining gradients to train them. When scaling up to large sizes, it is free from local minima and barren plateaus. In addition, owing to the dissipation nature of the ``training'' process, the QAL scheme features some degree of resilience to the noise in the quantum hardware, which makes it particularly suitable for current noisy quantum devices without error correction. One may worry that the success probability of preparing $|\psi^*\rangle$ would become vanishingly small due to post-selection in step (iii) as $T$ increases. This is in fact not the case as proved in Theorem~\ref{thm:analytical} under certain reasonable assumptions and verified in numerical simulations with real-life images. Another plausible (but not true) drawback has a profound relation to the quantum no-cloning theorem~\cite{Wootters1982Single}, which states that it is impossible to copy an arbitrary unknown quantum state. Due to the no-cloning theorem, one cannot make copies of $|\psi^*\rangle$ and hence it seems that for each run of the inference step (v), one has to execute the whole training process (i)-(iv). This is not necessary either, as discussed in-depth later.  








\vspace{.5cm}
\noindent \textbf{\large{}Provable convergence and physical explanation}{\large\par}

\noindent We now show that the training process of the QAL scheme introduced above can be understood from the perspective of preparing quantum states through imaginary time evolution and it converges exponentially to the global minimum of a naturally defined loss function. To this end, we define a Hamiltonian $H_{\bx}=\mathbf{I}-U(\bx)^\dagger \Pi_{y(\bx)}U(\bx)$, where $\Pi_{y(\bx)}$  denotes the measurement projection corresponding to the state encoding the label $y(\bx)$. The probability of correct prediction reads $\braket{\psi|U(\bx)^{\dagger}\Pi_{y(\bx)}U(\bx)|\psi}=1-\braket{\psi|H_{\bx}|\psi}$. 
A direct calculation shows that the training step (iii) effectively updates the state according to  (Supplementary Sec. II E):
\begin{equation}\label{equ:update rule}
    \ket{\psi}\leftarrow \frac{(\mathbf{I}-\eta H_{\bx})\ket{\psi}}{\norm{(\mathbf{I}-\eta H_{\bx})\ket{\psi}}},
\end{equation} 
where $\norm{(\mathbf{I}-\eta H_{\bx})\ket{\psi}}$ is a normalization factor whose square gives the success probability of post-selection. We define a loss function as the average failure probability to predict the label of a random training datum: $\hat{R}_S(\psi) = \E_{\bx\sim S}\braket{\psi|H_{\bx}|\psi}=\braket{\psi|H_S|\psi}$,
where $\E_{\bx\sim S}$ denotes the expectation with $\bx\sim S$ meaning $\bx$ is uniformly sampled from the training set $S$ and $H_S=\E_{\bx\sim S} H_{\bx}$ is the averaged Hamiltonian. The loss defined in this way has a clear physical meaning: it is just the energy of $\ket{\psi}$ under the Hamiltonian $H_S$. As a result, finding its global minimum is equivalent to finding the ground state energy of $H_S$. 


For convenience, we rewrite Eq.~\eqref{equ:update rule} in the density matrix formalism without normalization: $\rho\leftarrow (\mathbf{I}-\eta H_{\bx})\rho (\mathbf{I}-\eta H_{\bx})$. This way, $\Tr(\rho)$ is the success probability of the post-selection and the density matrix keeps track of the overall success probability. Another benefit of the density matrix formalism is that we can embed the randomness of the sample into the density state. Noting that $\bx$ is uniformly sampled from $S$, the averaged post-selection state after each training step (iii) reads $\rho \leftarrow e^{-\eta H_S}\rho e^{-\eta H_S} + \mathcal{O}(\eta^2)$ (Methods). Denoting the initial state as $\rho_0$, the averaged state after $T$ training steps is
\begin{equation}\label{Eq:density Tsteps}
    \rho = e^{-\eta T H_S}\rho_0 e^{-\eta T H_S} + \mathcal{O}(T\eta^2).
\end{equation}
From Eq.~\eqref{Eq:density Tsteps}, the dynamics of $\rho$ is determined by $\beta=\eta T$, the summation of learning rates in all training steps. By choosing $\eta$ sufficiently small, the higher-order terms diminish and Eq.~\eqref{Eq:density Tsteps} reduces to $\rho_{\beta} = e^{-\eta T H_S}\rho_0 e^{-\eta T H_S}$, which is exactly the imaginary-time evolution of $\rho_0$ under the Hamiltonian $H_S$. We remark that for a given machine learning task, the size of the input data sample is typically finite. As a result, $H_S$ would maintain a finite energy gap between the ground and excited states. Consequently, the training process will converge exponentially towards the ground state as $T$ increases until the higher-order terms become significant. We summarize the convergence analysis in the following theorem  (Supplementary Sec. III B):
\begin{theorem}\label{Thm1}
    Suppose $\rho_0$ has a nonzero overlap with the ground space of $H_S$. For an arbitrarily small constant $c$, we can choose an appropriate $\eta$ and $T$ such that the QAL protocol converges exponentially towards the global minimum up to higher-order corrections and the final averaged loss is upper bounded by $E_g+c$, where $E_g$ denotes the ground state energy of $H_S$.
\end{theorem}


Theorem~\ref{Thm1} implies that the convergence of the QAL protocol is guaranteed, as long as the initial state $\rho_0$ has a nonzero overlap with the ground space of $H_S$, which in turn can be assured by initializing $\rho_0$ to be the maximally-mixed state $\rho_0=\mathbf{I}/2^n$. In practice, a random state would have a nonzero overlap with the ground space of $H_S$ with almost unity probability and hence a random initial state $\rho_0$ suffices. We mention that the QAL training process can also be understood from a more familiar variational perspective in machine learning: it indeed implements gradient descent in an automated fashion (Supplementary Sec. II E). In fact, one may regard $\ket{\psi}$ as a variational state parametrized by a complex vector $\psi$. For each single datum, the gradient of $\braket{\psi|H_{\bx}|\psi}$ with respect to $\psi$ is $2H_{\bx}\ket{\psi}$. Therefore, the update rule  in Eq.~\eqref{equ:update rule} essentially implements the stochastic projected gradient descent algorithm to minimize the loss function $\hat{R}_S(\psi)$ with a batch size one. %Here we use the term ``projected'' to emphasize the normalization after each update. 
From this perspective, our QAL approach essentially exchanges the roles of data and variational parameters.  In conventional quantum neural networks, the input data are encoded into quantum states and the variational parameters specify the quantum circuits. Whereas, in the QAL approach we use data to specify the quantum circuits and treat quantum states as variational parameters. This is reminiscent of the shift from the Heisenberg picture to the Schr\"{o}dinger picture to some extent~\cite{Griffiths2019Introduction}: the conventional approach resembles the Heisenberg picture, where the ``operators'' (variational quantum circuits) are updated during the training process; in contrast, the QAL approach is more akin to the  Schr\"{o}dinger picture, where one evolves the ``states'', rather than the variational ``operators'',  to train the model.    



\vspace{.5cm}
\noindent \textbf{\large{}A subtle trade-off}{\large\par}

\noindent In the QAL protocol, we exploit post-selection to implement the desired data-dependent dissipation during the training process. This gives rise to a concern that the overall success probability may decay exponentially with the number of steps. In this section, we show that there is a subtle trade-off between the prediction accuracy and the overall post-selection success probability:  we prove that, as long as $\rho_0$ has a constant overlap with the low energy eigenspace of $H_S$, by choosing an appropriate $\beta$, we can achieve a near-optimal training loss with a constant success probability. We have the following theorem as proved in Supplementary Sec. III D:






\begin{theorem}\label{thm:analytical}
    Assume the spectrum of $H_S$ has a heavy tail, that is, has a constant proportion of low-energy eigenstates. With a random initial state $\rho_0$ in the computational basis, an appropriate learning rate $\eta$, and an appropriate number of steps $T$, the final state $\rho_{\beta}$ achieves a near-optimal training loss with a constant success probability.
\end{theorem}

We remark that the heavy-tail assumption is reasonable in machine learning scenarios---it stems from the fact that data samples with the same label should bear similar data structures and thus correspond to similar Hamiltonians. Consider a simple example of classifying images of dogs and cats, where all dogs look similar and all cats look similar. The Hamiltonian $H_S$ is approximately a mixture of two projectors of dimensions $2^{n-1}$, $H_{\text{dogs}}$ and $H_{\text{cats}}$. Regarding $H_{\text{dogs}}$ and $H_{\text{cats}}$ as random projectors, then $H_S$ has a constant proportion of near-zero eigenvalues, thus has a heavy tail. The heavy-tail assumption is further verified with real-life datasets, as shown in Fig.~\ref{fig:numerics}\textbf{d}. We note that heavy-tail assumption does not hold for typical Hamiltonians encountered in quantum physics, which in general exhibit exponential concentration of energy levels around the middle spectra. 




\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure.pdf}
    \caption{
    \textbf{Classify images in the Fashion MNIST dataset~\cite{xiaoFashionMNISTNovelImage2017} using QAL.} \textbf{a}, Example images from the dataset belonging to the classes ``trouser" and ``ankle boot". The pixel values of an image are encoded into the rotation angles of variational single-qubit gates in a quantum circuit. Here, CNOT and CZ denote the controlled-NOT and controlled-Z gates, respectively. \textbf{b}, Testing and training accuracy during the training process. We also plot the accuracy amplified by majority vote with multiple trials, see Supplementary Sec.~II D for the details of the majority vote. \textbf{c}, The trade-off between testing accuracy and the success probability of post-selection. The success probability is analytically calculated from Eq. \eqref{Eq:density Tsteps}, where we omit the higher-order term $O(T\eta^2)$. \textbf{d}, Spectrum of the Hamiltonian $H_S$ associated with the training dataset. \textbf{e}, Training performance under depolarizing noise with different two-qubit gate noise strengths. The single-qubit noise rate is set to one-tenth of the two-qubit noise rate. The green line (5\textperthousand) corresponds to the noise level of real-world superconducting quantum devices~\cite{jinObservationTopologicalPrethermal2025}. \textbf{f}, Illustration of state reusability: whenever the training loss drops below a threshold of 0.15, a prediction is made while training continues. The x-axis represents the number of steps required to recover the performance.} 
    \label{fig:numerics}
\end{figure*}


\vspace{.5cm}
\noindent \textbf{\large{}Bounded generalization error}{\large\par}
\noindent 
The above discussions have established a provable and explainable trainability for the QAL protocol. Yet, its generalization ability remains uncharted: it is unclear whether the good performance on the training dataset can be generalized to unseen data samples. We address this crucial issue in this section by proving an upper bound for the generalization error. Defining the true loss function as $R(\psi)=\E_{\bx\sim \cD}\braket{\psi|H_{\bx}|\psi}$, the following theorem bounds the generalization gap with the rigorous proof given in Supplementary Sec. III E:
\begin{theorem}\label{thm:generalization}
    Supposing the data samples are drawn randomly and independently from $\cD$, then with probability at least $1-\delta$, the generalization gap is upper bounded by
    \begin{equation}
        \max_{\psi}\big(R(\psi)-\hat{R}_S(\psi)\big) \leq \sqrt{\frac{4\ln(2^{n+1}/\delta)}{N}}.
    \end{equation}
\end{theorem}

According to this theorem, as long as the size $N$ of the training dataset is larger than $\Omega(n)$ (i.e., the logarithm of the degree of freedom), a  state $\ket{\psi}$ with low training loss has a low generalization loss with high probability. 
The good generalization of our QAL approach stems from the simple quadratic form of the loss function.



\vspace{.5cm}
\noindent \textbf{\large{}State reusability}{\large\par}
\noindent
In classical machine learning or conventional gradient-based quantum learning models, variational parameters are usually stored in a classical computer and they can be copied and reused on demand during the inference stage. In contrast, in the QAL approach what we obtain after training is a particular quantum state that carries information about the training samples. Due to the quantum no-cloning theorem~\cite{Wootters1982Single}, this state cannot be copied. This, together with the fact that the quantum measurements involved are destructive, results in a possible drawback of the QAL approach: it seems that one needs to carry out repeatedly the whole training process to prepare the desired state for each run of the inference. Fortunately, this is not the case. 


There are two complementary ways to solve this problem. First, we can incorporate quantum shadow tomography techniques ~\cite{Aaronson2018Shadow}  into step (v) of the QAL protocol. This will substantially reduce the number of copies of the desired states during inference: $\text{polylog}(N_p)$ copies would be sufficient for classifying $N_p$ unseen data samples. Second, one may reuse the state after the measurements in step (v). Theoretically, the state after training $\ket{\psi^*}$ would output correct labels for unseen sample $\bx$ with high probability, as proved above that the QAL process converges to the global minimum with bounded generalization error. Hence, the measurement $\{U(\bx)^\dagger \Pi_{y(\bx)} U(\bx)\}_y$ is in fact a gentle measurement that would not disturb $\ket{\psi^*}$ too much. Although the post-measurement state is different from $\ket{\psi^*}$, a few more training steps would recover the system to a state with high prediction accuracy. This reusability of states is verified by extensive numerical simulations on different real-life datasets (Fig.~\ref{fig:numerics}\textbf{f}). One can combine shadow tomography with state reusability to further reduce the resources required during the inference step. 




\vspace{.5cm}
\noindent \textbf{\large{}Numerical results}{\large\par}
\noindent To further illustrate and benchmark the effectiveness of QAL, we carry out extensive numerical simulations on a range of datasets, including real-life images (e.g., hand-writing digit images) and quantum data (e.g., thermal and localized quantum many-body states). In Fig.~\ref{fig:numerics}, we plot our numerical results for classifying images in the Fashion MNIST dataset~\cite{xiaoFashionMNISTNovelImage2017}. Fig.~\ref{fig:numerics}\textbf{a} shows some samples from the dataset and the encoding scheme used.  Fig.~\ref{fig:numerics}\textbf{b} plots the training and testing accuracy versus training iterations, from which it is clear that the accuracy increases rapidly and then saturates at a high value of $0.99$. In addition, we find that this approach is surprisingly efficient in the sense that one only need to run the experiment very few times to achieve a relatively high accuracy. From Fig.~\ref{fig:numerics}\textbf{b}, the accuracy for $29$ trials is already very close to that for infinite trials. In Fig.~\ref{fig:numerics}\textbf{c}, we plot the training accuracy versus the overall post-selection success probability. From this figure, the accuracy is maintained at a high level up to a sizable success probability. For this particular example, it remains about $0.99$ up to a success probability larger than $0.1$ for only $29$ trials.  Therefore, to achieve an accuracy as high as $0.99$, we only need to run the training circuit for roughly $290$ times, which can be done in less than a  second on a superconducting quantum device~\cite{Kjaergaard2020Superconducting}. As a comparison, for conventional gradient-based approaches, $290$ executions of the variational circuit is barely enough to estimate the gradient of a single parameter in one training step.



In Fig.~\ref{fig:numerics}\textbf{d}, we plot the spectrum of the averaged Hamiltonian $H_S$. From this figure, it is evident that $H_S$ indeed has a large portion of low-energy eigenstates, validating the heavy-tail assumption. Another salient feature of the QAL protocol is its robustness to experimental noise, which has an origin in the dissipation nature in preparing the target state $\ket{\psi^*}$ during training. This has been verified in our numerical simulations, as shown in Fig.~\ref{fig:numerics}\textbf{e}. In Fig.~\ref{fig:numerics}\textbf{f}, we show the state reusability for the QAL protocol. We keep training the model and predict an unseen datum whenever the training loss is below a threshold value of $0.15$. Most predictions are correct, in which case the performance for the post-measurement state is not degraded too much (as discussed above) and is recovered after about three additional training steps. When the prediction is wrong, its performance can be even worse than that for the random initial state. Nevertheless, the training steps needed to recover can be notably smaller, indicating that partial of the data information is preserved in the post-measurement state even if the measurements give wrong predictions. Our numerical simulations for classifying images of hand-written digits in the MNIST dataset and symmetry-protected topological states are plotted in Supplementary Sec. IV, where similar results are obtained.














\vspace{.5cm}

{\noindent \textbf{\large{}Discussion and outlook}{\large\par}}

\noindent 
The introduced QAL protocol is gradient-free and thus escapes the barren plateau problem inherently. It features a number of striking merits such as provable and explainable trainability with bounded generalization error.  Yet, several questions of fundamental importance remain unsolved. First, in conventional gradient-based approaches, quantum neural networks are shown to possess universal representation power. They can approximate an arbitrary function to arbitrary accuracy as long as the number of variational parameters is large enough~\cite{Goto2021Universal}. In contrast, the QAL protocol involves no variational parameter and its representation power depends on particular encoding schemes. Intuitively, one can always exploit different encoding schemes to approximate an arbitrary function. However, a rigorous proof of the universal representation power for the QAL protocol is technically challenging and remains unknown.  Second, the demonstration of quantum advantages is a long-sought-after goal in the field of quantum machine learning. It would be interesting and important to prove in theory and demonstrate in experimental quantum advantages for certain learning tasks in the QAL framework. Third, our discussions mainly focus on supervised learning. An extension of the QAL protocol to unsupervised and reinforcement learning scenarios is well worth exploring. 



Given the fact that the QAL protocol bears a certain degree of robustness to noise and does not need a quantum random access memory~\cite{Giovannetti2008Quantum} to transfer classical data into quantum states, an experimental demonstration of such a protocol with current noisy intermediate-scale quantum devices is highly feasible and desirable. Such an experiment would be a crucial step toward large-scale practical applications of quantum technologies in artificial intelligence. 




\vspace{.5cm}
\noindent\textbf{\large{}Methods}{\large\par}

\vspace{.2cm}
\noindent\textbf{Implementation of the  target-oriented perturbation}{\large\par}
\noindent
During the training process, the state $\ket{\psi}$ is updated in step (iii) by a non-unitary perturbation $M_y=\ketbra{y}+(1-\eta)(\mathbf{I}-\ketbra{y})$. Such a perturbation has a clear physical meaning of suppressing (enhancing) the probability of incorrect (correct) prediction, hence evolving the state $\ket{\psi}$ towards the target state  $\ket{\psi^*}$. This is similar to the Hamiltonian-echo-backprogagation based self-learning approach~\cite{lopez-pastorSelfLearningMachinesBased2023} for classical learning, where a small error signal is injected on top of the evaluation field to guide the training process. To implement $M_y$, we consider adding an ancillary qubit and embedding $M_y$ into a unitary $U_y$: 
\begin{equation}
    U_y = M_y\otimes Z + \sqrt{I-M_y^2}\otimes X,
\end{equation}
where $Z$ and $X$ are the Pauli matrices acting on the ancillary qubit. We initialize the ancillary qubit to state $\ket{0}$ and then apply $U_y$ to $\ket{\psi}\otimes\ket{0}$. Noting that $(\mathbf{I}\otimes \bra{0})U_y(\mathbf{I}\otimes \ket{0}) = M_y$, the state $\ket{\psi}$ will be updated by $M_y$ after we measure the ancillary qubit in the computational basis and post-select the outcome $0$. In this way, we effectively implement the non-unitary perturbation $M_y$ by adding an ancillary qubit combined with post-selection. 

It is worthwhile to remark that $U_y$ can be implemented with elementary gates very efficiently. For a $k$-class classification task, $M_y$ acts on $\lceil{\log k}\rceil$ qubits, where $\lceil\cdot\rceil$ denotes the ceiling function that outputs the least integer greater than or equal to the input. $U_y$ acts on $\lceil{\log k}\rceil+1$ qubits and is a multi-controlled gate up to single-qubit gates. It can be efficiently compiled into elementary gates with gate complexity scaling linearly in $\log k$ (See Supplementary Sec II C). We note that such a gate complexity scaling is much better than that for compiling a general unitary with the Solovay-Kitaev algorithm~\cite{Dawson2006TheSolovay}, where a $2^{O(\log k)}=\text{poly}(k)$ gate complexity is required for a given accuracy. 





\vspace{.5cm}
\noindent\textbf{Data encoding}{\large\par}


\noindent For classical classification, the data sample can be represented as a vector $\bx$ (for example, the pixel information of an image). We encode $\bx$ into a quantum circuit consisting of $\lceil{\frac{l}{3n}}\rceil$ blocks with similar structure, where $l$ denotes the dimension of $\bx$. Each block contains a layer of single-qubit rotations parametrized by components of $\bx$ and a layer of two-qubit entangling gates (see Fig.~\ref{fig:numerics}\textbf{a} and Supplementary Sec. II B). Such an encoding scheme features an intriguing merit of bypassing the use of a quantum random access memory~\cite{Giovannetti2008Quantum}, which is resource-demanding and inaccessible so far, to load classical data. This makes the QAL approach attainable for middle-size data samples ($l$ on the order of tens of thousands) with current noisy intermediate-scale quantum devices. For even larger data samples, such as the largest images in the ImageNet dataset~\cite{Deng2009ImageNet} ($l\sim 10^8$), this encoding scheme would become impractical for current quantum devices since the circuit depth is formidably large.  In fact, the required circuit depth scales roughly as $\sim l/(3n)$. In practice, the number of qubits $n$ can be chosen on the order of $\log l$ and hence the circuit depth $\sim l/(3\log l)$. For an image with $l\sim 10^8$, we need a quantum circuit with depth $\sim 10^6$ to encode this image. This is beyond the reach of current noisy quantum devices with state-of-the-art performance~\cite{Xiang2024Longlived, Jin2025Observation, Acharya2408Quantum,Bluvstein2024Logical,Iqbal2024non,Cao2023Generation}. One may use more qubits to encode $\bx$ and reduce the circuit depth.  

For the classification of Hamiltonian data, the encoding is straightforward. One can encode $H_{\bx}$ into real-time evolution $e^{-iH_{\bx}t}$, which in turn can be implemented via quantum simulation techniques~\cite{Childs2012Hamiltonian,Georgescu2014Quantum,Low2017Optimal,Clinton2021Hamiltonian}. In quantum state classification, each datum is a quantum state $\ket{\bx}$ of $s$ qubits. Fix an $(s+n)$-qubit Hamiltonian $H$, $H_{\ket{\bx}}=(\bra{\bx}\otimes \mathbf{I}_n)H(\ket{\bx}\otimes \mathbf{I}_n)$ is an $n$-qubit Hamiltonian. We then encode the state $\ket{\bx}$ into a unitary $U_{\ket{\bx}}=e^{-iH_{\ket{\bx}}}$. This unitary can be efficiently implemented using copies of $\ket{\bx}$ and real-time evolution $e^{-iHt}$, inspired by the Lloyd-Mohseni-Rebentrost protocol~\cite{Lloyd2014Quantum}. See the Supplementary Sec. II B for details. 


\vspace{.3cm}
\noindent\textbf{Numerical setup}\\
\noindent We conduct numerical simulations on various types of datasets, including classical data, Hamiltonian data, and quantum state data.
For classical data, we use the Fashion MNIST dataset~\cite{xiaoFashionMNISTNovelImage2017} and focus on the binary classification of classes ``trouser" and ``ankle boot". To reduce the computation overhead, we rescale the figures into $10\times 10$ pixels and then normalize the pixel values. The pixel values are encoded into rotation angles of single-qubit gates in a parameterized quantum circuit. We also conduct QAL protocol to classify MNIST dataset~\cite{lecun1998mnist} and present the results in the Supplementary Information.
For Hamiltonian data, we consider a binary classification of the following Aubry-Andr\'{e} Hamiltonian on $10$ qubits~\cite{aubry1980analyticity}:
\begin{equation*}
H=-\frac g2 \sum_k (\sigma_k^x\sigma_{k+1}^x+\sigma_k^y\sigma_{k+1}^y)-\frac V2 \sum_k \cos(2\pi \phi k)\sigma_k^z, 
\end{equation*}
where $g$ is the coupling strength, $\sigma_k^\alpha$ ($\alpha=x, y, z$) are Pauli operators on the $k$-th qubit, $V$ is the disorder magnitude and $\phi=(\sqrt{5}-1)/2$. This Hamiltonian exhibits a quantum phase transition at $V/g=2$, between a localized phase for $V/g>2$ and a delocalized phase for $V/g<2$. To generate the dataset, we fix $g=1$, sample $V$ in the interval $[0, 4]$ and label the Hamiltonian according to its phase. To carry out the QAL protocol, we encode the Hamiltonian into its real-time evolution $e^{-2iH}$. Our numerical results for Hamiltonian data are ploted in Fig.~S2 in the Supplementary Information. 
For quantum state data, we classify the ground state of $10$-qubit clustering-Ising model Hamiltonian~\cite{son2011quantum,smacchia2011statistical} with periodic boundary condition:
\begin{equation*}
    H(h)=-\sum_{k}\sigma_k^x\sigma_{k+1}^z\sigma_{k+2}^x+h\sum_{k}\sigma_k^y\sigma_{k+1}^y,
\end{equation*}
This Hamiltonian has a phase transition at $h=1$, between a symmetry protected topological phase ($h<1$) and an antiferromagnetic phase ($h>1$). We sample $h$ from interval $[0, 2]$ to generate the dataset. The detailed data encoding scheme and the numerical results are shown in Supplimentary Sec. IV.

In most numerical simulations, we uniformly set the learning rate to 0.1 and examine the performance on a test dataset of size 500. The only exception is the illustration of noise robustness shown in Fig.~\ref{fig:numerics}\textbf{e}, where to reduce the computational overhead of density matrix simulation, we further thumbnail the Fashion MNIST image to $5\times 5$, increase the learning rate to $0.2$ and perform the simulation with five qubits. 
%\vspace{.3cm}
%\noindent About Theorem 1 \TODO{}


\vspace{.6cm}
\noindent\textbf{\large{}Data availability}
All the data for this study will be  made publicly available for download on Zenodo/Figshare/Github upon publication.

\vspace{.6cm}
\noindent\textbf{\large{}Code availability}
The data analysis and numerical simulation codes for this study will be made publicly available for download on Zenodo/Figshare/Github upon publication.

\vspace{.5cm}
\noindent\textbf{Acknowledgement} We thank Andrew Chi-Chih Yao, Adi Shamir, Xun Gao, Sirui Lu, Zidu Liu, and Fangjun Hu for helpful discussions.  This work is supported by the National Natural Science Foundation of China (Grant No.~T2225008, No.~T24B2002, and No.~12075128), the Innovation Program for Quantum Science and Technology (Grant No.~2021ZD0302203, 2021ZD0301601, and 2021ZD0301605), the Ministry of Science and Technology of China (2021AAA0150000), Tsinghua University Initiative Scientific Research Program, and the Ministry of Education of China, the Tsinghua University Dushi Program, and the Shanghai Qi Zhi Institute Innovation Program SQZ202318 and SQZ202317. L.-M.D. acknowledges in addition support from the New Cornerstone Science Foundation through the New Cornerstone Investigator Program. 


%apsrev4-2.bst 2019-01-14 (MD) hand-edited version of apsrev4-1.bst
%Control: key (0)
%Control: author (8) initials jnrlst
%Control: editor formatted (1) identically to author
%Control: production of article title (0) allowed
%Control: page (0) single
%Control: year (1) truncated
%Control: production of eprint (0) enabled
\begin{thebibliography}{68}%
\makeatletter
\providecommand \@ifxundefined [1]{%
 \@ifx{#1\undefined}
}%
\providecommand \@ifnum [1]{%
 \ifnum #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \@ifx [1]{%
 \ifx #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \natexlab [1]{#1}%
\providecommand \enquote  [1]{``#1''}%
\providecommand \bibnamefont  [1]{#1}%
\providecommand \bibfnamefont [1]{#1}%
\providecommand \citenamefont [1]{#1}%
\providecommand \href@noop [0]{\@secondoftwo}%
\providecommand \href [0]{\begingroup \@sanitize@url \@href}%
\providecommand \@href[1]{\@@startlink{#1}\@@href}%
\providecommand \@@href[1]{\endgroup#1\@@endlink}%
\providecommand \@sanitize@url [0]{\catcode `\\12\catcode `\$12\catcode `\&12\catcode `\#12\catcode `\^12\catcode `\_12\catcode `\%12\relax}%
\providecommand \@@startlink[1]{}%
\providecommand \@@endlink[0]{}%
\providecommand \url  [0]{\begingroup\@sanitize@url \@url }%
\providecommand \@url [1]{\endgroup\@href {#1}{\urlprefix }}%
\providecommand \urlprefix  [0]{URL }%
\providecommand \Eprint [0]{\href }%
\providecommand \doibase [0]{https://doi.org/}%
\providecommand \selectlanguage [0]{\@gobble}%
\providecommand \bibinfo  [0]{\@secondoftwo}%
\providecommand \bibfield  [0]{\@secondoftwo}%
\providecommand \translation [1]{[#1]}%
\providecommand \BibitemOpen [0]{}%
\providecommand \bibitemStop [0]{}%
\providecommand \bibitemNoStop [0]{.\EOS\space}%
\providecommand \EOS [0]{\spacefactor3000\relax}%
\providecommand \BibitemShut  [1]{\csname bibitem#1\endcsname}%
\let\auto@bib@innerbib\@empty
%</preamble>
\bibitem [{\citenamefont {LeCun}\ \emph {et~al.}(2015)\citenamefont {LeCun}, \citenamefont {Bengio},\ and\ \citenamefont {Hinton}}]{LeCun2015Deep}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {LeCun}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Bengio}},\ and\ \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Hinton}},\ }\bibfield  {title} {\bibinfo {title} {Deep learning},\ }\href {https://doi.org/10.1038/nature14539} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {521}},\ \bibinfo {pages} {436} (\bibinfo {year} {2015})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Goodfellow}\ \emph {et~al.}(2016)\citenamefont {Goodfellow}, \citenamefont {Bengio},\ and\ \citenamefont {Courville}}]{Goodfellow2016Deep}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {I.}~\bibnamefont {Goodfellow}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Bengio}},\ and\ \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Courville}},\ }\href@noop {} {\emph {\bibinfo {title} {Deep {{Learning}}}}}\ (\bibinfo  {publisher} {MIT Press},\ \bibinfo {year} {2016})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Wang}\ \emph {et~al.}(2023)\citenamefont {Wang}, \citenamefont {Fu}, \citenamefont {Du} \emph {et~al.}}]{Wang2023Scientific}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Fu}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Du}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Scientific discovery in the age of artificial intelligence},\ }\href {https://doi.org/10.1038/s41586-023-06221-2} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {620}},\ \bibinfo {pages} {47} (\bibinfo {year} {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Silver}\ \emph {et~al.}(2016)\citenamefont {Silver}, \citenamefont {Huang}, \citenamefont {Maddison} \emph {et~al.}}]{Silver2016Mastering}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Silver}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Huang}}, \bibinfo {author} {\bibfnamefont {C.~J.}\ \bibnamefont {Maddison}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Mastering the game of {{Go}} with deep neural networks and tree search},\ }\href {https://doi.org/10.1038/nature16961} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {529}},\ \bibinfo {pages} {484} (\bibinfo {year} {2016})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Silver}\ \emph {et~al.}(2017)\citenamefont {Silver}, \citenamefont {Schrittwieser}, \citenamefont {Simonyan} \emph {et~al.}}]{Silver2017Mastering}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Silver}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Schrittwieser}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Simonyan}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Mastering the game of {{Go}} without human knowledge},\ }\href {https://doi.org/10.1038/nature24270} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {550}},\ \bibinfo {pages} {354} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Senior}\ \emph {et~al.}(2020)\citenamefont {Senior}, \citenamefont {Evans}, \citenamefont {Jumper} \emph {et~al.}}]{Senior2020Improved}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~W.}\ \bibnamefont {Senior}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Evans}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Jumper}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Improved protein structure prediction using potentials from deep learning},\ }\href {https://doi.org/10.1038/s41586-019-1923-7} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {577}},\ \bibinfo {pages} {706} (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Trinh}\ \emph {et~al.}(2024)\citenamefont {Trinh}, \citenamefont {Wu}, \citenamefont {Le}, \citenamefont {He},\ and\ \citenamefont {Luong}}]{Trinh2024Solving}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {T.~H.}\ \bibnamefont {Trinh}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Wu}}, \bibinfo {author} {\bibfnamefont {Q.~V.}\ \bibnamefont {Le}}, \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {He}},\ and\ \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Luong}},\ }\bibfield  {title} {\bibinfo {title} {Solving olympiad geometry without human demonstrations},\ }\href {https://doi.org/10.1038/s41586-023-06747-5} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {625}},\ \bibinfo {pages} {476} (\bibinfo {year} {2024})}\BibitemShut {NoStop}%
\bibitem [{202(2023)}]{2023AI}%
  \BibitemOpen
  \bibfield  {title} {\bibinfo {title} {{{AI}} hardware has an energy problem},\ }\href {https://doi.org/10.1038/s41928-023-01014-x} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Electron.}\ }\textbf {\bibinfo {volume} {6}},\ \bibinfo {pages} {463} (\bibinfo {year} {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Preskill}(2018)}]{Preskill2018Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Preskill}},\ }\bibfield  {title} {\bibinfo {title} {Quantum {{Computing}} in the {{NISQ}} era and beyond},\ }\href {https://doi.org/10.22331/q-2018-08-06-79} {\bibfield  {journal} {\bibinfo  {journal} {Quantum}\ }\textbf {\bibinfo {volume} {2}},\ \bibinfo {pages} {79} (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Arute}\ \emph {et~al.}(2019)\citenamefont {Arute}, \citenamefont {Arya}, \citenamefont {Babbush} \emph {et~al.}}]{Arute2019Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Arute}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Arya}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Babbush}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Quantum supremacy using a programmable superconducting processor},\ }\href {https://doi.org/10.1038/s41586-019-1666-5} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {574}},\ \bibinfo {pages} {505} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Zhong}\ \emph {et~al.}(2020)\citenamefont {Zhong}, \citenamefont {Wang}, \citenamefont {Deng} \emph {et~al.}}]{Zhong2020Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.-S.}\ \bibnamefont {Zhong}}, \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {Y.-H.}\ \bibnamefont {Deng}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Quantum computational advantage using photons},\ }\href {https://doi.org/10.1126/science.abe8770} {\bibfield  {journal} {\bibinfo  {journal} {Science}\ }\textbf {\bibinfo {volume} {370}},\ \bibinfo {pages} {1460} (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Wu}\ \emph {et~al.}(2021)\citenamefont {Wu}, \citenamefont {Bao}, \citenamefont {Cao} \emph {et~al.}}]{Wu2021Strong}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Wu}}, \bibinfo {author} {\bibfnamefont {W.-S.}\ \bibnamefont {Bao}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Cao}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Strong {{Quantum Computational Advantage Using}} a {{Superconducting Quantum Processor}}},\ }\href {https://doi.org/10.1103/PhysRevLett.127.180501} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {127}},\ \bibinfo {pages} {180501} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bluvstein}\ \emph {et~al.}(2024)\citenamefont {Bluvstein}, \citenamefont {Evered}, \citenamefont {Geim} \emph {et~al.}}]{Bluvstein2024Logical}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Bluvstein}}, \bibinfo {author} {\bibfnamefont {S.~J.}\ \bibnamefont {Evered}}, \bibinfo {author} {\bibfnamefont {A.~A.}\ \bibnamefont {Geim}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Logical quantum processor based on reconfigurable atom arrays},\ }\href {https://doi.org/10.1038/s41586-023-06927-3} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {626}},\ \bibinfo {pages} {58} (\bibinfo {year} {2024})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Acharya}\ \emph {et~al.}(2024{\natexlab{a}})\citenamefont {Acharya}, \citenamefont {Abanin}, \citenamefont {{Aghababaie-Beni}}, \citenamefont {Aleiner}, \citenamefont {Andersen}, \citenamefont {Ansmann}, \citenamefont {Arute}, \citenamefont {Arya}, \citenamefont {Asfaw}, \citenamefont {Astrakhantsev} \emph {et~al.}}]{Acharya2024Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Acharya}}, \bibinfo {author} {\bibfnamefont {D.~A.}\ \bibnamefont {Abanin}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {{Aghababaie-Beni}}}, \bibinfo {author} {\bibfnamefont {I.}~\bibnamefont {Aleiner}}, \bibinfo {author} {\bibfnamefont {T.~I.}\ \bibnamefont {Andersen}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Ansmann}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Arute}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Arya}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Asfaw}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Astrakhantsev}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Quantum error correction below the surface code threshold},\ }\href {https://doi.org/10.1038/s41586-024-08449-y} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ ,\ \bibinfo {pages} {1}} (\bibinfo {year} {2024}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Liu}\ \emph {et~al.}(2021)\citenamefont {Liu}, \citenamefont {Arunachalam},\ and\ \citenamefont {Temme}}]{Liu2021Rigorous}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Liu}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Arunachalam}},\ and\ \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Temme}},\ }\bibfield  {title} {\bibinfo {title} {A rigorous and robust quantum speed-up in supervised machine learning},\ }\href {https://doi.org/10.1038/s41567-021-01287-z} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Phys.}\ }\textbf {\bibinfo {volume} {17}},\ \bibinfo {pages} {1013} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Huang}\ \emph {et~al.}(2021{\natexlab{a}})\citenamefont {Huang}, \citenamefont {Broughton}, \citenamefont {Mohseni}, \citenamefont {Babbush}, \citenamefont {Boixo}, \citenamefont {Neven},\ and\ \citenamefont {McClean}}]{Huang2021Power}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.-Y.}\ \bibnamefont {Huang}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Broughton}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Mohseni}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Babbush}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Boixo}}, \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Neven}},\ and\ \bibinfo {author} {\bibfnamefont {J.~R.}\ \bibnamefont {McClean}},\ }\bibfield  {title} {\bibinfo {title} {Power of data in quantum machine learning},\ }\href {https://doi.org/10.1038/s41467-021-22539-9} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {12}},\ \bibinfo {pages} {2631} (\bibinfo {year} {2021}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Biamonte}\ \emph {et~al.}(2017)\citenamefont {Biamonte}, \citenamefont {Wittek}, \citenamefont {Pancotti}, \citenamefont {Rebentrost}, \citenamefont {Wiebe},\ and\ \citenamefont {Lloyd}}]{Biamonte2017Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Biamonte}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Wittek}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Pancotti}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Rebentrost}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Wiebe}},\ and\ \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Lloyd}},\ }\bibfield  {title} {\bibinfo {title} {Quantum machine learning},\ }\href {https://doi.org/10.1038/nature23474} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {549}},\ \bibinfo {pages} {195} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Dunjko}\ and\ \citenamefont {Briegel}(2018)}]{Dunjko2018Machine}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {V.}~\bibnamefont {Dunjko}}\ and\ \bibinfo {author} {\bibfnamefont {H.~J.}\ \bibnamefont {Briegel}},\ }\bibfield  {title} {\bibinfo {title} {Machine learning \& artificial intelligence in the quantum domain: A review of recent progress},\ }\href {https://doi.org/10.1088/1361-6633/aab406} {\bibfield  {journal} {\bibinfo  {journal} {Rep. Prog. Phys.}\ }\textbf {\bibinfo {volume} {81}},\ \bibinfo {pages} {074001} (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Das~Sarma}\ \emph {et~al.}(2019)\citenamefont {Das~Sarma}, \citenamefont {Deng},\ and\ \citenamefont {Duan}}]{DasSarma2019Machine}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Das~Sarma}}, \bibinfo {author} {\bibfnamefont {D.-L.}\ \bibnamefont {Deng}},\ and\ \bibinfo {author} {\bibfnamefont {L.-M.}\ \bibnamefont {Duan}},\ }\bibfield  {title} {\bibinfo {title} {Machine learning meets quantum physics},\ }\href {https://doi.org/10.1063/PT.3.4164} {\bibfield  {journal} {\bibinfo  {journal} {Physics Today}\ }\textbf {\bibinfo {volume} {72}},\ \bibinfo {pages} {48} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Herrmann}\ \emph {et~al.}(2022)\citenamefont {Herrmann}, \citenamefont {Llima}, \citenamefont {Remm} \emph {et~al.}}]{Herrmann2022Realizing}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Herrmann}}, \bibinfo {author} {\bibfnamefont {S.~M.}\ \bibnamefont {Llima}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Remm}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Realizing quantum convolutional neural networks on a superconducting quantum processor to recognize quantum phases},\ }\href {https://doi.org/10.1038/s41467-022-31679-5} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {13}},\ \bibinfo {pages} {4144} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Saggio}\ \emph {et~al.}(2021)\citenamefont {Saggio}, \citenamefont {Asenbeck}, \citenamefont {Hamann} \emph {et~al.}}]{Saggio2021Experimental}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {V.}~\bibnamefont {Saggio}}, \bibinfo {author} {\bibfnamefont {B.~E.}\ \bibnamefont {Asenbeck}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Hamann}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Experimental quantum speed-up in reinforcement learning agents},\ }\href {https://doi.org/10.1038/s41586-021-03242-7} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {591}},\ \bibinfo {pages} {229} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Ren}\ \emph {et~al.}(2022)\citenamefont {Ren}, \citenamefont {Li}, \citenamefont {Xu}, \citenamefont {Wang} \emph {et~al.}}]{Ren2022Experimental}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Ren}}, \bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Li}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Xu}}, \bibinfo {author} {\bibnamefont {Wang}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Experimental quantum adversarial learning with programmable superconducting qubits},\ }\href {https://doi.org/10.1038/s43588-022-00351-9} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Comput. Sci.}\ }\textbf {\bibinfo {volume} {2}},\ \bibinfo {pages} {711} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hu}\ \emph {et~al.}(2019)\citenamefont {Hu}, \citenamefont {Wu}, \citenamefont {Cai} \emph {et~al.}}]{Hu2019Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Hu}}, \bibinfo {author} {\bibfnamefont {S.-H.}\ \bibnamefont {Wu}}, \bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Cai}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Quantum generative adversarial learning in a superconducting quantum circuit},\ }\href {https://doi.org/10.1126/sciadv.aav2761} {\bibfield  {journal} {\bibinfo  {journal} {Sci. Adv.}\ }\textbf {\bibinfo {volume} {5}},\ \bibinfo {pages} {eaav2761} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Huang}\ \emph {et~al.}(2021{\natexlab{b}})\citenamefont {Huang}, \citenamefont {Wang}, \citenamefont {Song} \emph {et~al.}}]{Huang2021Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Huang}}, \bibinfo {author} {\bibfnamefont {Z.-A.}\ \bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Song}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Quantum generative adversarial networks with multiple superconducting qubits},\ }\href {https://doi.org/10.1038/s41534-021-00503-1} {\bibfield  {journal} {\bibinfo  {journal} {npj. Quantum. Inf.}\ }\textbf {\bibinfo {volume} {7}},\ \bibinfo {pages} {165} (\bibinfo {year} {2021}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Gong}\ \emph {et~al.}(2023)\citenamefont {Gong}, \citenamefont {Huang}, \citenamefont {Wang} \emph {et~al.}}]{Gong2023Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Gong}}, \bibinfo {author} {\bibfnamefont {H.-L.}\ \bibnamefont {Huang}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Wang}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Quantum neuronal sensing of quantum many-body states on a 61-qubit programmable superconducting processor},\ }\href {https://doi.org/10.1016/j.scib.2023.04.003} {\bibfield  {journal} {\bibinfo  {journal} {Sci. Bull.}\ }\textbf {\bibinfo {volume} {68}},\ \bibinfo {pages} {906} (\bibinfo {year} {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Peters}\ \emph {et~al.}(2021)\citenamefont {Peters}, \citenamefont {Caldeira}, \citenamefont {Ho}, \citenamefont {Leichenauer}, \citenamefont {Mohseni}, \citenamefont {Neven}, \citenamefont {Spentzouris}, \citenamefont {Strain},\ and\ \citenamefont {Perdue}}]{Peters2021Machine}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {E.}~\bibnamefont {Peters}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Caldeira}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Ho}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Leichenauer}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Mohseni}}, \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Neven}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Spentzouris}}, \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Strain}},\ and\ \bibinfo {author} {\bibfnamefont {G.~N.}\ \bibnamefont {Perdue}},\ }\bibfield  {title} {\bibinfo {title} {Machine learning of high dimensional data on a noisy quantum processor},\ }\href {https://doi.org/10.1038/s41534-021-00498-9} {\bibfield  {journal} {\bibinfo  {journal} {npj. Quantum. Inf.}\ }\textbf {\bibinfo {volume} {7}},\ \bibinfo {pages} {161} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Liu}\ \emph {et~al.}(2025)\citenamefont {Liu}, \citenamefont {Cao}, \citenamefont {Liu}, \citenamefont {Sun}, \citenamefont {Bao}, \citenamefont {Lu}, \citenamefont {Yin},\ and\ \citenamefont {Chen}}]{Liu2025Practical}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Z.-P.}\ \bibnamefont {Liu}}, \bibinfo {author} {\bibfnamefont {X.-Y.}\ \bibnamefont {Cao}}, \bibinfo {author} {\bibfnamefont {H.-W.}\ \bibnamefont {Liu}}, \bibinfo {author} {\bibfnamefont {X.-R.}\ \bibnamefont {Sun}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Bao}}, \bibinfo {author} {\bibfnamefont {Y.-S.}\ \bibnamefont {Lu}}, \bibinfo {author} {\bibfnamefont {H.-L.}\ \bibnamefont {Yin}},\ and\ \bibinfo {author} {\bibfnamefont {Z.-B.}\ \bibnamefont {Chen}},\ }\bibfield  {title} {\bibinfo {title} {Practical quantum federated learning and its experimental demonstration},\ }\href {https://arxiv.org/abs/2501.12709} {\bibfield  {journal} {\bibinfo  {journal} {arXiv:2501.12709}\ } (\bibinfo {year} {2025})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Zhang}\ \emph {et~al.}(2024)\citenamefont {Zhang}, \citenamefont {Lu}, \citenamefont {Zhao} \emph {et~al.}}]{Zhang2024Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Zhang}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Lu}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Zhao}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Quantum continual learning on a programmable superconducting processor},\ }\href {https://arxiv.org/abs/2409.09729} {\bibfield  {journal} {\bibinfo  {journal} {arXiv:2409.09729}\ } (\bibinfo {year} {2024})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Mitarai}\ \emph {et~al.}(2018)\citenamefont {Mitarai}, \citenamefont {Negoro}, \citenamefont {Kitagawa},\ and\ \citenamefont {Fujii}}]{Mitarai2018Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Mitarai}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Negoro}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Kitagawa}},\ and\ \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Fujii}},\ }\bibfield  {title} {\bibinfo {title} {Quantum circuit learning},\ }\href {https://doi.org/10.1103/PhysRevA.98.032309} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. A}\ }\textbf {\bibinfo {volume} {98}},\ \bibinfo {pages} {032309} (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Wythoff}(1993)}]{Wythoff1993Backpropagation}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {B.~J.}\ \bibnamefont {Wythoff}},\ }\bibfield  {title} {\bibinfo {title} {Backpropagation neural networks: {{A}} tutorial},\ }\href {https://doi.org/10.1016/0169-7439(93)80052-J} {\bibfield  {journal} {\bibinfo  {journal} {Chemometr. Intell. Lab. Syst.}\ }\textbf {\bibinfo {volume} {18}},\ \bibinfo {pages} {115} (\bibinfo {year} {1993})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Beer}\ \emph {et~al.}(2020)\citenamefont {Beer}, \citenamefont {Bondarenko}, \citenamefont {Farrelly}, \citenamefont {Osborne}, \citenamefont {Salzmann}, \citenamefont {Scheiermann},\ and\ \citenamefont {Wolf}}]{Beer2020Training}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Beer}}, \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Bondarenko}}, \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Farrelly}}, \bibinfo {author} {\bibfnamefont {T.~J.}\ \bibnamefont {Osborne}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Salzmann}}, \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Scheiermann}},\ and\ \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Wolf}},\ }\bibfield  {title} {\bibinfo {title} {Training deep quantum neural networks},\ }\href {https://doi.org/10.1038/s41467-020-14454-2} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {11}},\ \bibinfo {pages} {808} (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Pan}\ \emph {et~al.}(2023)\citenamefont {Pan}, \citenamefont {Lu}, \citenamefont {Wang}, \citenamefont {Hua}, \citenamefont {Xu}, \citenamefont {Li}, \citenamefont {Cai}, \citenamefont {Li}, \citenamefont {Wang}, \citenamefont {Song} \emph {et~al.}}]{Pan2023Deep}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {X.}~\bibnamefont {Pan}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Lu}}, \bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Hua}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Xu}}, \bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Li}}, \bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Cai}}, \bibinfo {author} {\bibfnamefont {X.}~\bibnamefont {Li}}, \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {Y.-P.}\ \bibnamefont {Song}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Deep quantum neural networks on a superconducting processor},\ }\href {https://doi.org/10.1038/s41467-023-39785-8} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {14}},\ \bibinfo {pages} {4006} (\bibinfo {year} {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Schuld}\ \emph {et~al.}(2019)\citenamefont {Schuld}, \citenamefont {Bergholm}, \citenamefont {Gogolin}, \citenamefont {Izaac},\ and\ \citenamefont {Killoran}}]{Schuld2019Evaluating}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Schuld}}, \bibinfo {author} {\bibfnamefont {V.}~\bibnamefont {Bergholm}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Gogolin}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Izaac}},\ and\ \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Killoran}},\ }\bibfield  {title} {\bibinfo {title} {Evaluating analytic gradients on quantum hardware},\ }\href {https://doi.org/10.1103/PhysRevA.99.032331} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. A}\ }\textbf {\bibinfo {volume} {99}},\ \bibinfo {pages} {032331} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {OpenAI}\ \emph {et~al.}(2024)\citenamefont {OpenAI}, \citenamefont {Achiam}, \citenamefont {Adler} \emph {et~al.}}]{OpenAI2024GPT4}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibnamefont {OpenAI}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Achiam}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Adler}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {{{GPT-4 Technical Report}}},\ }\Eprint {https://arxiv.org/abs/2303.08774} {arXiv:2303.08774}  (\bibinfo {year} {2024})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Du}\ \emph {et~al.}(2022)\citenamefont {Du}, \citenamefont {Huang}, \citenamefont {Dai} \emph {et~al.}}]{Du2022GLaM}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Du}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Huang}}, \bibinfo {author} {\bibfnamefont {A.~M.}\ \bibnamefont {Dai}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {{{GLaM}}: {{Efficient Scaling}} of {{Language Models}} with {{Mixture-of-Experts}}},\ }in\ \href {https://proceedings.mlr.press/v162/du22c.html} {\emph {\bibinfo {booktitle} {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}}}}\ (\bibinfo  {publisher} {PMLR},\ \bibinfo {year} {2022})\ pp.\ \bibinfo {pages} {5547--5569}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bittel}\ and\ \citenamefont {Kliesch}(2021)}]{Bittel2021Training}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Bittel}}\ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Kliesch}},\ }\bibfield  {title} {\bibinfo {title} {Training {{Variational Quantum Algorithms Is NP-Hard}}},\ }\href {https://doi.org/10.1103/PhysRevLett.127.120502} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {127}},\ \bibinfo {pages} {120502} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Larocca}\ \emph {et~al.}(2024)\citenamefont {Larocca}, \citenamefont {Thanasilp}, \citenamefont {Wang}, \citenamefont {Sharma}, \citenamefont {Biamonte}, \citenamefont {Coles}, \citenamefont {Cincio}, \citenamefont {McClean}, \citenamefont {Holmes},\ and\ \citenamefont {Cerezo}}]{Larocca2024Review}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Larocca}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Thanasilp}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Sharma}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Biamonte}}, \bibinfo {author} {\bibfnamefont {P.~J.}\ \bibnamefont {Coles}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Cincio}}, \bibinfo {author} {\bibfnamefont {J.~R.}\ \bibnamefont {McClean}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Holmes}},\ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}},\ }\bibfield  {title} {\bibinfo {title} {A {{Review}} of {{Barren Plateaus}} in {{Variational Quantum Computing}}},\ }\Eprint {https://arxiv.org/abs/2405.00781} {arXiv:2405.00781}  (\bibinfo {year} {2024})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {McClean}\ \emph {et~al.}(2018)\citenamefont {McClean}, \citenamefont {Boixo}, \citenamefont {Smelyanskiy}, \citenamefont {Babbush},\ and\ \citenamefont {Neven}}]{McClean2018Barren}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.~R.}\ \bibnamefont {McClean}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Boixo}}, \bibinfo {author} {\bibfnamefont {V.~N.}\ \bibnamefont {Smelyanskiy}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Babbush}},\ and\ \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Neven}},\ }\bibfield  {title} {\bibinfo {title} {Barren plateaus in quantum neural network training landscapes},\ }\href {https://doi.org/10.1038/s41467-018-07090-4} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {9}},\ \bibinfo {pages} {4812} (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cerezo}\ \emph {et~al.}(2021{\natexlab{a}})\citenamefont {Cerezo}, \citenamefont {Sone}, \citenamefont {Volkoff}, \citenamefont {Cincio},\ and\ \citenamefont {Coles}}]{Cerezo2021Cost}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Sone}}, \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Volkoff}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Cincio}},\ and\ \bibinfo {author} {\bibfnamefont {P.~J.}\ \bibnamefont {Coles}},\ }\bibfield  {title} {\bibinfo {title} {Cost {{Function Dependent Barren Plateaus}} in {{Shallow Parametrized Quantum Circuits}}},\ }\href {https://doi.org/10.1038/s41467-021-21728-w} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {12}},\ \bibinfo {pages} {1791} (\bibinfo {year} {2021}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {LeCun}\ \emph {et~al.}(1998)\citenamefont {LeCun}, \citenamefont {Cortes},\ and\ \citenamefont {Burges}}]{lecun1998mnist}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {LeCun}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Cortes}},\ and\ \bibinfo {author} {\bibfnamefont {C.~J.~C.}\ \bibnamefont {Burges}},\ }\href {http://yann.lecun.com/exdb/mnist/} {\bibinfo {title} {The {MNIST} database of handwritten digits}} (\bibinfo {year} {1998})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Xiao}\ \emph {et~al.}(2017)\citenamefont {Xiao}, \citenamefont {Rasul},\ and\ \citenamefont {Vollgraf}}]{xiaoFashionMNISTNovelImage2017}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Xiao}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Rasul}},\ and\ \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Vollgraf}},\ }\bibfield  {title} {\bibinfo {title} {Fashion-mnist: A novel image dataset for benchmarking machine learning algorithms},\ }\Eprint {https://arxiv.org/abs/1708.07747} {arXiv:1708.07747}  (\bibinfo {year} {2017})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Li}\ and\ \citenamefont {Deng}(2022)}]{Li2022Recent}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Li}}\ and\ \bibinfo {author} {\bibfnamefont {D.-L.}\ \bibnamefont {Deng}},\ }\bibfield  {title} {\bibinfo {title} {Recent advances for quantum classifiers},\ }\href {https://doi.org/10.1007/s11433-021-1793-6} {\bibfield  {journal} {\bibinfo  {journal} {Sci. China Phys. Mech. Astron.}\ }\textbf {\bibinfo {volume} {65}},\ \bibinfo {pages} {220301} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cerezo}\ \emph {et~al.}(2021{\natexlab{b}})\citenamefont {Cerezo}, \citenamefont {Arrasmith}, \citenamefont {Babbush}, \citenamefont {Benjamin}, \citenamefont {Endo}, \citenamefont {Fujii}, \citenamefont {McClean}, \citenamefont {Mitarai}, \citenamefont {Yuan}, \citenamefont {Cincio},\ and\ \citenamefont {Coles}}]{Cerezo2021Variational}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Arrasmith}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Babbush}}, \bibinfo {author} {\bibfnamefont {S.~C.}\ \bibnamefont {Benjamin}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Endo}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Fujii}}, \bibinfo {author} {\bibfnamefont {J.~R.}\ \bibnamefont {McClean}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Mitarai}}, \bibinfo {author} {\bibfnamefont {X.}~\bibnamefont {Yuan}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Cincio}},\ and\ \bibinfo {author} {\bibfnamefont {P.~J.}\ \bibnamefont {Coles}},\ }\bibfield  {title} {\bibinfo {title} {Variational quantum algorithms},\ }\href {https://doi.org/10.1038/s42254-021-00348-9} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Rev. Phys.}\ }\textbf {\bibinfo {volume} {3}},\ \bibinfo {pages} {625} (\bibinfo {year} {2021}{\natexlab{b}})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Li}\ \emph {et~al.}(2022)\citenamefont {Li}, \citenamefont {Lu},\ and\ \citenamefont {Deng}}]{Li2022Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Li}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Lu}},\ and\ \bibinfo {author} {\bibfnamefont {D.-L.}\ \bibnamefont {Deng}},\ }\bibfield  {title} {\bibinfo {title} {Quantum {{Neural Network Classifiers}}: {{A Tutorial}}},\ }\href {https://doi.org/10.21468/SciPostPhysLectNotes.61} {\bibfield  {journal} {\bibinfo  {journal} {SciPost Phys. Lect. Notes}\ }\textbf {\bibinfo {volume} {1}},\ \bibinfo {pages} {61} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Giovannetti}\ \emph {et~al.}(2008)\citenamefont {Giovannetti}, \citenamefont {Lloyd},\ and\ \citenamefont {Maccone}}]{Giovannetti2008Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {V.}~\bibnamefont {Giovannetti}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Lloyd}},\ and\ \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Maccone}},\ }\bibfield  {title} {\bibinfo {title} {Quantum {{Random Access Memory}}},\ }\href {https://doi.org/10.1103/PhysRevLett.100.160501} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {100}},\ \bibinfo {pages} {160501} (\bibinfo {year} {2008})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Dawson}\ and\ \citenamefont {Nielsen}(2006)}]{Dawson2006TheSolovay}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.~M.}\ \bibnamefont {Dawson}}\ and\ \bibinfo {author} {\bibfnamefont {M.~A.}\ \bibnamefont {Nielsen}},\ }\bibfield  {title} {\bibinfo {title} {The solovay-kitaev algorithm},\ }\href {https://doi.org/10.5555/2011679.2011685} {\bibfield  {journal} {\bibinfo  {journal} {Quantum Info. Comput.}\ }\textbf {\bibinfo {volume} {6}},\ \bibinfo {pages} {81} (\bibinfo {year} {2006})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Zhang}\ \emph {et~al.}(2020)\citenamefont {Zhang}, \citenamefont {Zheng}, \citenamefont {Zhang},\ and\ \citenamefont {Deng}}]{Zhang2020Topological}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.-H.}\ \bibnamefont {Zhang}}, \bibinfo {author} {\bibfnamefont {P.-L.}\ \bibnamefont {Zheng}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Zhang}},\ and\ \bibinfo {author} {\bibfnamefont {D.-L.}\ \bibnamefont {Deng}},\ }\bibfield  {title} {\bibinfo {title} {Topological {{Quantum Compiling}} with {{Reinforcement Learning}}},\ }\href {https://doi.org/10.1103/PhysRevLett.125.170501} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {125}},\ \bibinfo {pages} {170501} (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Wootters}\ and\ \citenamefont {Zurek}(1982)}]{Wootters1982Single}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {W.~K.}\ \bibnamefont {Wootters}}\ and\ \bibinfo {author} {\bibfnamefont {W.~H.}\ \bibnamefont {Zurek}},\ }\bibfield  {title} {\bibinfo {title} {A single quantum cannot be cloned},\ }\href {https://doi.org/10.1038/299802a0} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {299}},\ \bibinfo {pages} {802} (\bibinfo {year} {1982})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Griffiths}\ and\ \citenamefont {Schroeter}(2019)}]{Griffiths2019Introduction}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.~J.}\ \bibnamefont {Griffiths}}\ and\ \bibinfo {author} {\bibfnamefont {D.~F.}\ \bibnamefont {Schroeter}},\ }\href@noop {} {\emph {\bibinfo {title} {Introduction to quantum mechanics}}}\ (\bibinfo  {publisher} {Cambridge university press},\ \bibinfo {year} {2019})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Jin}\ \emph {et~al.}(2025{\natexlab{a}})\citenamefont {Jin}, \citenamefont {Jiang}, \citenamefont {Zhu}, \citenamefont {Bao}, \citenamefont {Shen}, \citenamefont {Wang}, \citenamefont {Zhu}, \citenamefont {Xu}, \citenamefont {Song}, \citenamefont {Chen} \emph {et~al.}}]{jinObservationTopologicalPrethermal2025}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Jin}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Jiang}}, \bibinfo {author} {\bibfnamefont {X.}~\bibnamefont {Zhu}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Bao}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Shen}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Zhu}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Xu}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Song}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Chen}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Observation of topological prethermal strong zero modes},\ }\Eprint {https://arxiv.org/abs/2501.04688} {arXiv:2501.04688}  (\bibinfo {year} {2025}{\natexlab{a}})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Aaronson}(2018)}]{Aaronson2018Shadow}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Aaronson}},\ }\bibfield  {title} {\bibinfo {title} {Shadow tomography of quantum states},\ }in\ \href {https://doi.org/10.1145/3188745.3188802} {\emph {\bibinfo {booktitle} {Proceedings of the 50th {{Annual ACM SIGACT Symposium}} on {{Theory}} of {{Computing}}}}},\ \bibinfo {series and number} {{{STOC}} 2018}\ (\bibinfo  {publisher} {Association for Computing Machinery},\ \bibinfo {address} {New York, NY, USA},\ \bibinfo {year} {2018})\ pp.\ \bibinfo {pages} {325--338}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Kjaergaard}\ \emph {et~al.}(2020)\citenamefont {Kjaergaard}, \citenamefont {Schwartz}, \citenamefont {Braum{\"u}ller}, \citenamefont {Krantz}, \citenamefont {Wang}, \citenamefont {Gustavsson},\ and\ \citenamefont {Oliver}}]{Kjaergaard2020Superconducting}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Kjaergaard}}, \bibinfo {author} {\bibfnamefont {M.~E.}\ \bibnamefont {Schwartz}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Braum{\"u}ller}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Krantz}}, \bibinfo {author} {\bibfnamefont {J.~I.-J.}\ \bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Gustavsson}},\ and\ \bibinfo {author} {\bibfnamefont {W.~D.}\ \bibnamefont {Oliver}},\ }\bibfield  {title} {\bibinfo {title} {Superconducting {{Qubits}}: {{Current State}} of {{Play}}},\ }\href {https://doi.org/10.1146/annurev-conmatphys-031119-050605} {\bibfield  {journal} {\bibinfo  {journal} {Annu. Rev. Condens. Matter Phys.}\ }\textbf {\bibinfo {volume} {11}},\ \bibinfo {pages} {369} (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Goto}\ \emph {et~al.}(2021)\citenamefont {Goto}, \citenamefont {Tran},\ and\ \citenamefont {Nakajima}}]{Goto2021Universal}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Goto}}, \bibinfo {author} {\bibfnamefont {Q.~H.}\ \bibnamefont {Tran}},\ and\ \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Nakajima}},\ }\bibfield  {title} {\bibinfo {title} {Universal {{Approximation Property}} of {{Quantum Machine Learning Models}} in {{Quantum-Enhanced Feature Spaces}}},\ }\href {https://doi.org/10.1103/PhysRevLett.127.090506} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {127}},\ \bibinfo {pages} {090506} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {{L{\'o}pez-Pastor}}\ and\ \citenamefont {Marquardt}(2023)}]{lopez-pastorSelfLearningMachinesBased2023}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {V.}~\bibnamefont {{L{\'o}pez-Pastor}}}\ and\ \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Marquardt}},\ }\bibfield  {title} {\bibinfo {title} {Self-{{Learning Machines Based}} on {{Hamiltonian Echo Backpropagation}}},\ }\href {https://doi.org/10.1103/PhysRevX.13.031020} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. X}\ }\textbf {\bibinfo {volume} {13}},\ \bibinfo {pages} {031020} (\bibinfo {year} {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Deng}\ \emph {et~al.}(2009)\citenamefont {Deng} \emph {et~al.}}]{Deng2009ImageNet}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Deng}} \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {{{ImageNet}}: {{A}} large-scale hierarchical image database},\ }in\ \href {https://doi.org/10.1109/CVPR.2009.5206848} {\emph {\bibinfo {booktitle} {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}}}\ (\bibinfo {year} {2009})\ pp.\ \bibinfo {pages} {248--255}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Xiang}\ \emph {et~al.}(2024)\citenamefont {Xiang}, \citenamefont {Jiang}, \citenamefont {Bao} \emph {et~al.}}]{Xiang2024Longlived}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Xiang}}, \bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Jiang}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Bao}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Long-lived topological time-crystalline order on a quantum processor},\ }\href {https://doi.org/10.1038/s41467-024-53077-9} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {15}},\ \bibinfo {pages} {8963} (\bibinfo {year} {2024})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Jin}\ \emph {et~al.}(2025{\natexlab{b}})\citenamefont {Jin}, \citenamefont {Jiang}, \citenamefont {Zhu}, \citenamefont {Bao}, \citenamefont {Shen}, \citenamefont {Wang}, \citenamefont {Zhu}, \citenamefont {Xu}, \citenamefont {Song}, \citenamefont {Chen} \emph {et~al.}}]{Jin2025Observation}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Jin}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Jiang}}, \bibinfo {author} {\bibfnamefont {X.}~\bibnamefont {Zhu}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Bao}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Shen}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Zhu}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Xu}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Song}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Chen}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Observation of topological prethermal strong zero modes},\ }\href {https://arxiv.org/abs/2501.04688} {\bibfield  {journal} {\bibinfo  {journal} {arXiv:2501.04688}\ } (\bibinfo {year} {2025}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Acharya}\ \emph {et~al.}(2024{\natexlab{b}})\citenamefont {Acharya}, \citenamefont {Aghababaie-Beni}, \citenamefont {Aleiner}, \citenamefont {Andersen}, \citenamefont {Ansmann}, \citenamefont {Arute}, \citenamefont {Arya}, \citenamefont {Asfaw}, \citenamefont {Astrakhantsev}, \citenamefont {Atalaya} \emph {et~al.}}]{Acharya2408Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Acharya}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Aghababaie-Beni}}, \bibinfo {author} {\bibfnamefont {I.}~\bibnamefont {Aleiner}}, \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Andersen}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Ansmann}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Arute}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Arya}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Asfaw}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Astrakhantsev}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Atalaya}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Quantum error correction below the surface code threshold},\ }\href {https://doi.org/10.1038/s41586-024-08449-y} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ } (\bibinfo {year} {2024}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Iqbal}\ \emph {et~al.}(2024)\citenamefont {Iqbal}, \citenamefont {Tantivasadakarn}, \citenamefont {Verresen}, \citenamefont {Campbell}, \citenamefont {Dreiling}, \citenamefont {Figgatt}, \citenamefont {Gaebler}, \citenamefont {Johansen}, \citenamefont {Mills}, \citenamefont {Moses} \emph {et~al.}}]{Iqbal2024non}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Iqbal}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Tantivasadakarn}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Verresen}}, \bibinfo {author} {\bibfnamefont {S.~L.}\ \bibnamefont {Campbell}}, \bibinfo {author} {\bibfnamefont {J.~M.}\ \bibnamefont {Dreiling}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Figgatt}}, \bibinfo {author} {\bibfnamefont {J.~P.}\ \bibnamefont {Gaebler}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Johansen}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Mills}}, \bibinfo {author} {\bibfnamefont {S.~A.}\ \bibnamefont {Moses}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Non-abelian topological order and anyons on a trapped-ion processor},\ }\href {https://www.nature.com/articles/s41586-023-06934-4} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {626}},\ \bibinfo {pages} {505} (\bibinfo {year} {2024})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cao}\ \emph {et~al.}(2023)\citenamefont {Cao}, \citenamefont {Wu}, \citenamefont {Chen}, \citenamefont {Gong}, \citenamefont {Wu}, \citenamefont {Ye}, \citenamefont {Zha}, \citenamefont {Qian}, \citenamefont {Ying}, \citenamefont {Guo} \emph {et~al.}}]{Cao2023Generation}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Cao}}, \bibinfo {author} {\bibfnamefont {B.}~\bibnamefont {Wu}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Chen}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Gong}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Wu}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Ye}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Zha}}, \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Qian}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Ying}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Guo}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {Generation of genuine entanglement up to 51 superconducting qubits},\ }\href {https://www.nature.com/articles/s41586-023-06195-1} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {619}},\ \bibinfo {pages} {738} (\bibinfo {year} {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Childs}\ and\ \citenamefont {Wiebe}(2012)}]{Childs2012Hamiltonian}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~M.}\ \bibnamefont {Childs}}\ and\ \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Wiebe}},\ }\bibfield  {title} {\bibinfo {title} {Hamiltonian {{Simulation Using Linear Combinations}} of {{Unitary Operations}}},\ }\href {https://doi.org/10.26421/QIC12.11-12} {\bibfield  {journal} {\bibinfo  {journal} {QIC}\ }\textbf {\bibinfo {volume} {12}},\ \bibinfo {pages} {901} (\bibinfo {year} {2012})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Georgescu}\ \emph {et~al.}(2014)\citenamefont {Georgescu}, \citenamefont {Ashhab},\ and\ \citenamefont {Nori}}]{Georgescu2014Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {I.~M.}\ \bibnamefont {Georgescu}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Ashhab}},\ and\ \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Nori}},\ }\bibfield  {title} {\bibinfo {title} {Quantum simulation},\ }\href {https://doi.org/10.1103/RevModPhys.86.153} {\bibfield  {journal} {\bibinfo  {journal} {Rev. Mod. Phys.}\ }\textbf {\bibinfo {volume} {86}},\ \bibinfo {pages} {153} (\bibinfo {year} {2014})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Low}\ and\ \citenamefont {Chuang}(2017)}]{Low2017Optimal}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.~H.}\ \bibnamefont {Low}}\ and\ \bibinfo {author} {\bibfnamefont {I.~L.}\ \bibnamefont {Chuang}},\ }\bibfield  {title} {\bibinfo {title} {Optimal hamiltonian simulation by quantum signal processing},\ }\href {https://doi.org/10.1103/PhysRevLett.118.010501} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {118}},\ \bibinfo {pages} {010501} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Clinton}\ \emph {et~al.}(2021)\citenamefont {Clinton}, \citenamefont {Bausch},\ and\ \citenamefont {Cubitt}}]{Clinton2021Hamiltonian}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Clinton}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Bausch}},\ and\ \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Cubitt}},\ }\bibfield  {title} {\bibinfo {title} {Hamiltonian simulation algorithms for near-term quantum hardware},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {12}},\ \bibinfo {pages} {4989} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Lloyd}\ \emph {et~al.}(2014)\citenamefont {Lloyd}, \citenamefont {Mohseni},\ and\ \citenamefont {Rebentrost}}]{Lloyd2014Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Lloyd}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Mohseni}},\ and\ \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Rebentrost}},\ }\bibfield  {title} {\bibinfo {title} {Quantum principal component analysis},\ }\href {https://doi.org/10.1038/nphys3029} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Phys.}\ }\textbf {\bibinfo {volume} {10}},\ \bibinfo {pages} {631} (\bibinfo {year} {2014})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Aubry}\ and\ \citenamefont {Andr{\'e}}(1980)}]{aubry1980analyticity}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Aubry}}\ and\ \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Andr{\'e}}},\ }\bibfield  {title} {\bibinfo {title} {Analyticity breaking and anderson localization in incommensurate lattices},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Ann. Israel Phys. Soc}\ }\textbf {\bibinfo {volume} {3}},\ \bibinfo {pages} {18} (\bibinfo {year} {1980})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Son}\ \emph {et~al.}(2011)\citenamefont {Son}, \citenamefont {Amico}, \citenamefont {Fazio}, \citenamefont {Hamma}, \citenamefont {Pascazio},\ and\ \citenamefont {Vedral}}]{son2011quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Son}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Amico}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Fazio}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Hamma}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Pascazio}},\ and\ \bibinfo {author} {\bibfnamefont {V.}~\bibnamefont {Vedral}},\ }\bibfield  {title} {\bibinfo {title} {Quantum phase transition between cluster and antiferromagnetic states},\ }\href {https://doi.org/10.1209/0295-5075/95/50001} {\bibfield  {journal} {\bibinfo  {journal} {EPL}\ }\textbf {\bibinfo {volume} {95}},\ \bibinfo {pages} {50001} (\bibinfo {year} {2011})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Smacchia}\ \emph {et~al.}(2011)\citenamefont {Smacchia}, \citenamefont {Amico}, \citenamefont {Facchi}, \citenamefont {Fazio}, \citenamefont {Florio}, \citenamefont {Pascazio},\ and\ \citenamefont {Vedral}}]{smacchia2011statistical}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Smacchia}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Amico}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Facchi}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Fazio}}, \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Florio}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Pascazio}},\ and\ \bibinfo {author} {\bibfnamefont {V.}~\bibnamefont {Vedral}},\ }\bibfield  {title} {\bibinfo {title} {Statistical mechanics of the cluster ising model},\ }\href {https://doi.org/10.1103/PhysRevA.84.022304} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. A}\ }\textbf {\bibinfo {volume} {84}},\ \bibinfo {pages} {022304} (\bibinfo {year} {2011})}\BibitemShut {NoStop}%
\end{thebibliography}%



\end{document}