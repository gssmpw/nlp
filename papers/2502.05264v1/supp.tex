\documentclass[prx,superscriptaddress]{revtex4-2}
\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{physics}
\usepackage{algorithm}  
\usepackage{algorithmicx}  
\usepackage{algpseudocode}  
\usepackage{subcaption}
\usepackage{caption}

\renewcommand{\algorithmicrequire}{\textbf{Input }}  
\renewcommand{\algorithmicensure}{\textbf{Output}}  
\usepackage{todonotes}
\usepackage[unicode=true,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true]
 {hyperref}
\hypersetup{
 linkcolor=magenta, urlcolor=blue, citecolor=blue, pdfstartview={FitH}, hyperfootnotes=false, unicode=true}
\usepackage{cleveref}
\crefname{theorem}{Theorem}{Theorems}
\crefname{definition}{Definition}{Definitions}
\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\@ifundefined{textcolor}{}
{%
 \definecolor{BLACK}{gray}{0}
 \definecolor{WHITE}{gray}{1}
 \definecolor{RED}{rgb}{1,0,0}
 \definecolor{GREEN}{rgb}{0,1,0}
 \definecolor{BLUE}{rgb}{0,0,1}
 \definecolor{CYAN}{cmyk}{1,0,0,0}
 \definecolor{MAGENTA}{cmyk}{0,1,0,0}
 \definecolor{YELLOW}{cmyk}{0,0,1,0}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

\usepackage{amsfonts,amsthm}\usepackage{tabularx}\usepackage{dcolumn}\usepackage{bm}\usepackage{graphicx}\usepackage{epstopdf}
\usepackage{times}


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{problem}{Problem}
\newtheorem{idea}{Idea}

%\theoremstyle{plain}
\setcounter{MaxMatrixCols}{10}
\hypersetup{urlcolor=blue}

\def\CTeXPreproc{Created by ctex v0.2.12, don't edit!}
\def\ket#1{\left|#1\right\rangle}
\def\bra#1{\left\langle#1\right|}
\def\braket#1{\left\langle#1\right\rangle}
\def\ketbra#1{\left|#1\right\rangle\!\left\langle#1\right|}

\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bx}{\bm{x}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\qy}[1]{{\color{blue}{Qi: \textbf{#1}}}}
\newcommand{\TODO}[1]{{\color{red}{TODO: \textbf{#1}}}}
\newcommand{\wk}[1]{{\color{cyan}{Li: \textbf{#1}}}}
\newcommand{\gsy}[1]{{\color{blue}{gsy: \textbf{#1}}}}
\makeatother
% section 1,2

\begin{document}

\title{Supplementary Information for: \\Quantum automated learning with provable and explainable trainability}

\maketitle
\tableofcontents

\makeatletter
\setcounter{figure}{0}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{definition}{0}
\setcounter{theorem}{0}
\setcounter{lemma}{0}
\setcounter{proposition}{0}
\renewcommand{\thefigure}{S\@arabic\c@figure}
\renewcommand{\thetable}{S\@arabic\c@table}
\renewcommand{\theequation}{S\@arabic\c@equation}
\renewcommand{\thedefinition}{S\@arabic\c@definition}
\renewcommand{\thetheorem}{S\@arabic\c@theorem}
\renewcommand{\thelemma}{S\@arabic\c@lemma}


\section{Theoretical Background}
\subsection{Supervised learning}
We start by introducing the basic framework of supervised learning \cite{Goodfellow2016Deep}. Let $\cX$ be the set of input data and $\cY=\{1, 2, \cdots, k\}$ be the set of labels. We assume that every input data $\bx\in\cX$ has a deterministic label $y(\bx)\in \cY$. Let $\cD$ be an unknown distribution over $\cX$. The goal of supervised learning is to find an algorithm $\cA(\cdot)$ (probably randomized in quantum machine learning) such that, input a sample $\bx\sim \cD$, output the label $y(\bx)$ with high probability. To achieve this goal, we parametrize the learning model by parameters $\btheta$ and optimize the average loss
\begin{equation}
    R(\btheta)= \E_{\bx\sim \cD} L(\bx, y(\bx); \btheta).
\end{equation}
Here $L(\bx, y; \btheta)$ is some loss function, usually a metric of the difference between the output distribution of $\cA(\bx; \btheta)$ and the correct label $y$. $R(\btheta)$ is called the risk or the prediction error of the model $\cA(\cdot~; \btheta)$. However, the distribution $\cD$ is unknown, so we cannot directly calculate $R(\btheta)$. Instead, we sample a training dataset $S=\{(\bx_i, y_i=f(\bx_i))\}_{i=1}^m$ from $\cD$, and optimize the following empirical risk or training error:
\begin{equation}
    \hat{R}_S(\btheta) = \frac{1}{m}\sum_{i=1}^m L(\bx_i, y_i; \btheta).
\end{equation}
According to the simple decomposition $R(\btheta)=\hat{R}_S(\btheta)+(R(\btheta)-\hat{R}_S(\btheta))$, the success of supervised learning depends on two important factors: trainability and generalization. In short, trainability asks whether we can efficiently find $\btheta$ with low empirical risk, while generalization asks whether the generalization gap $\mathrm{gen}_S(\btheta)=R(\btheta)-\hat{R}_S(\btheta)$ is upper bounded, i.e., whether the good performance on the training set $S$ can be generalized to unseen data.

\subsection{Variational quantum learning models}


For conventional gradient-based quantum learning approaches~\cite{Cerezo2021Variational}, a learning algorithm $\cA(\bx;\btheta)$ executes a variational quantum circuit $U(\btheta)$ to a data-encoded state $\ket{\phi(\bx)}$ before performing certain measurements to make the prediction.  
Assuming the measurement observable to be $O_M$, the output from the variational circuit is the expectation value $\braket{\phi(\bx)|U(\btheta)^\dagger O_MU(\btheta)|\phi(\bx)}$.
The loss function is often defined as a function of this value, where commonly used forms include mean square error and cross-entropy.
For a training task, the average loss value over a given set of training data is defined as the empirical risk,
where schemes based on gradient descents are widely exploited to minimize it and find the optimal parameters $\btheta^*$. 
In the quantum machine learning realm, there are various methods proposed to calculate the gradients with respect to circuit parameters, including finite differences, the parameter-shift rules, and quantum natural gradients~\cite{Mitarai2018Quantum,Schuld2019Evaluating,Stokes2020Quantum}.


Quantum neural networks have demonstrated promising generalization capabilities in various learning settings~\cite{caiSampleComplexityLearning2022,caroGeneralizationQuantumMachine2022}.
Intuitively, when the number of training data points exceeds the degrees of freedom in the parameter space, the generalization gap of the optimized parameters is typically bounded by a small constant.
However, the practical trainability of quantum neural networks remains a significant challenge.
A key bottleneck lies in the computational cost of estimating gradients with respect to the circuit parameters. 
For instance, computing the gradient of a single parameter accurately often requires executing the variational circuit thousands of times, even when employing comparably efficient parameter-shift rules.
This process becomes increasingly time-consuming and impractical as the number of parameters grows.

Furthermore, the loss landscape of quantum neural networks can be highly non-convex and challenging to navigate.
As shown in ref.~\cite{youExponentiallyManyLocal2021}, the loss function of quantum neural networks exhibits exponentially many local minima, which can trap optimization algorithms and hinder convergence.
In parallel, the phenomenon of barren plateaus, first identified in ref.~\cite{McClean2018Barren}, poses a critical issue: the gradients of the loss function tend to vanish exponentially with the number of qubits, especially in deep quantum circuits. In such cases, the loss landscape becomes effectively flat, making it extremely difficult to identify a direction for optimization. The presence of barren plateaus is closely tied to the randomness and entanglement structure of the quantum circuit, as well as the choice of cost function and initial parameterization, which severely threads the scalability and practical utility of quantum neural networks for large-scale problems~\cite{McClean2018Barren,Cerezo2021Cost,OrtizMarrero2021EntanglementInduced,Holmes2022Connecting,Wang2021Noiseinduced,Larocca2024Review}.



\section{The Automated Learning Strategy}
In this section, we provide more technical details about the quantum automated learning strategy.



\subsection{Choose an appropriate number of qubits}
To carry out the QAL protocol, the first step is to decide an appropriate number of qubits $n$. Since an $n$-qubit state lives in a $O(2^n)$-dimensional Hilbert space and thus bears $O(2^n)$ degrees of freedom, one natural choice is $n=O(\log\abs{\bx})$, where $\abs{\bx}$ is the dimension of data sample $\bx$. However, we remark that the choice of $n$ is much more flexible. For example, if we are classifying Hamiltonian data, it is more natural to set $n$ to be the system size of the Hamiltonian. If the data are  images of size $L\times L$, setting $n=L$ may align with the two-dimensional structure better. On the other hand, sometimes it is possible to set $n$ to be even much smaller than $\log(\abs{\bx})$,  since realistic data samples are usually believed to lie in a low-dimension manifold. 

\subsection{Encode data into unitaries}\label{supp_sec:data_encoding}
Once we pin down the number of qubits $n$, the next step is to encode data into $n$-qubit unitaries. Here we present the detailed data encoding schemes for quantum automated learning, which incorporates three distinct categories of data: 
classical data, Hamiltonian data, and quantum state data.
An overview of the encoding methods is provided in Fig.~\ref{fig:3data_encode}, which summarizes the key approaches before delving into the detailed descriptions of each scheme. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{3data_encode.pdf}
    \captionsetup{justification=raggedright, singlelinecheck=false}
    \caption{\textbf{Overview of three different data encoding methods.} We encode classical data $\bx$ into parameterized quantum circuit $U(\bx)$; encode Hamiltonian data $H_{\bx}$ into its real-time evolution $e^{-iH_{\bx}t}$; and encode quantum state $|\bx\rangle$ first into an $n$-qubit Hamiltonian: $H_{\ket{\bx}}=(\bra{\bx}\otimes \mathbf{I}_n)H(\ket{\bx}\otimes \mathbf{I}_n)$, and then encode it to a unitary $e^{-iH_{\ket{\bx}}}$.}
    \label{fig:3data_encode}
\end{figure}

Classical data, including images, text, or audio, can be transformed into a vector of numerical values, denoted as $\bx$. The data vector $\bx$ is encoded into parameterized quantum circuit. Specifically, each element of $\bx$ is mapped into rotation angles of single-qubit gates. A single-qubit gate is parametrized as $G(\alpha, \beta, \gamma)=R_y(\alpha)R_z(\beta)R_y(\gamma)$, where $R_y(\alpha)$ and $R_z(\beta)$ are the rotations around the $Y$ and $Z$ axes of the Bloch sphere by angle $\alpha$ and $\beta$, respectively. Therefore, for a n-qubit quantum circuit, a layer of single-qubit gates can encode up to $3n$ entries of the vector $\bx$. If we denote the dimension of $\bx$ as $l$, then it is necessary to employ $\lceil \frac{l}{3n}\rceil$ layers of single-qubit gates. More concretely, considering a $3n$-dimensional vector $\textbf{y}$, we define the encoding of a single-qubit layer as:
% $G(\textbf{x})=\otimes_{i=1}^n G_i(x_{3i-2}, x_{3i-1}, x_{3i})$, 
$G(\textbf{y})=\otimes_{i=1}^n G_i(y_{2n+i}, y_{n+i}, y_{i})$, 
where $G_i$ acts on the $i$-th qubit, as illustrated in Fig.~\ref{fig:data_encode}\textbf{a}. Then the $k$-th layer single-qubit encoding of the    data vector $\bx$ is defined as $G\left(\bx_{3n(k-1)+1:3nk}\right)$, where $\bx_{i:j}$ denotes the abbreviation of $(x_i,x_{i+1},\dots,x_j)$. In cases where the number of elements in $\bx$ does not exactly divide by $3n$, padding with zeros is used to ensure uniformity.

Between two layers of single-qubit gates, we insert a layer of two-qubit gates to entangle the qubits, leading to the spread of information. This layer of two-qubit gates is composed of a CNOT-gate block $A$ and a CZ-gate block $B$. Each block consists of two layers of two-qubit gates: in the first layer, the odd-numbered qubits act as the control qubits, while in the second layer, the even-numbered qubits serve as the control qubits. In both layers, each control qubit targets the subsequent qubit in the sequence. Mathematically, we define: $A=\left(\otimes_{i=1}^{\lfloor \frac {n-1}{2} \rfloor} \text{CNOT}_{2i,2i+1}\right) \left(\otimes_{i=1}^{\lfloor \frac n 2 \rfloor} \text{CNOT}_{2i-1,2i}\right)$ and $B=\left(\otimes_{i=1}^{\lfloor \frac {n-1}{2} \rfloor} \text{CZ}_{2i,2i+1}\right) \left(\otimes_{i=1}^{\lfloor \frac n 2 \rfloor} \text{CZ}_{2i-1,2i}\right)$, as shown in Fig.~\ref{fig:data_encode}\textbf{a}. To ensure that all elements of the data vector can influence the measured qubits used for prediction, we add additional $\lfloor \frac{n}{2} \rfloor -1$ layers of two-qubit gates. 

The final unitary encoding $U(\bx)$ for classical data is then given by a sequence of single- and two-qubit gates:

\begin{equation}
    (BA)^{\lfloor \frac{n}{2} \rfloor -1}G(\bx_{3n(d-1)+1:3nd})\cdots BAG(\bx_{3n+1:6n})BAG(\bx_{1:3n}),
\end{equation}
where $d=\lceil \frac{l}{3n}\rceil$ and $\bx$ is padded with zeros if $3nd>l$.
as illustrated in Fig.~\ref{fig:data_encode}\textbf{b}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{data_encode.pdf}
    \captionsetup{justification=raggedright, singlelinecheck=false}
    \caption{\textbf{The classical data encoding scheme.} \textbf{a}, Illustrates an encoding layer for a $3n$-dimensional vector $\textbf{y}$ ($G\left(\textbf{y}\right)$). The layer consists of three layers of single-qubit gates ($R_y, R_z, R_y$), denoted by the purple block; two layers of CNOT two-qubit gates, denoted by the green block; and two layers of CZ two-qubit gates, denoted by the yellow block. \textbf{b}, Illustrates the complete encoding for an $l$-dimensional vector $\bx$. The encoding scheme involves $\lceil \frac{l}{3n}\rceil$ layers of single-qubit and two-qubit gates ($BAG\left(\bx_{3n(k-1)+1:3nk}\right)$), followed by $\lfloor \frac n 2 \rfloor -1$ layers of entangling gates ($BA$).}
    \label{fig:data_encode}
\end{figure}


For Hamiltonian data, we encode the Hamiltonian $H_{\bx}$ through its real-time evolution $e^{-iH_{\bx} t}$. This encoding inherently captures the time evolution of quantum states governed by the Schr\"{o}dinger equation. Both $e^{-iH_{\bx} t}$ and its reverse evolution $e^{iH_{\bx} t}$ can be implemented through quantum Hamiltonian simulation techniques \cite{Childs2012Hamiltonian,Georgescu2014Quantum,Low2017Optimal,Clinton2021Hamiltonian}. One may exploit other encoding schemes for $H_{\bx}$. In practice, we find that our real-time evolution encoding works well, as shown in our numerical simulations (Fig. \ref{fig:AA}) 


For quantum state classification, each datum is a quantum state $\ket{\bx}$ of  $s$ qubits. In order to carry out the QAL protocol for classifying $\ket{\bx}$, we need to encode $\ket{\bx}$ into a unitary $U_{\ket{\bx}}$. 
We fix an $(s+n)$-qubit Hamiltonian $H$.  We first encode the quantum state $|\bx\rangle$ into an $n$-qubit Hamiltonian: $H_{\ket{\bx}}=(\bra{\bx}\otimes \mathbf{I}_n)H(\ket{\bx}\otimes \mathbf{I}_n)$, and then encode it to a unitary $U_{\ket{\bx}}=e^{-iH_{\ket{\bx}}}$. This unitary can be efficiently implemented using copies of $\ket{\bx}$ and real-time evolution $e^{-iHt}$, inspired by the Lloyd-Mohseni-Rebentrost protocol \cite{lloydQuantumPrincipalComponent2014} that implements $e^{-i\rho t}$ from copies of $\rho$. More concretely, for any state $\rho$ straightforward calculations yield
\begin{align}
    \tr_{\leq l}(e^{-iH\Delta t}(\ketbra{\bx}\otimes \rho)e^{iH\Delta t})&=\rho + \tr_{\leq l}(-iH\Delta t (\ketbra{\bx}\otimes \rho)+(\ketbra{\bx}\otimes \rho)iH\Delta t))+O((\Delta t)^2)\nonumber\\
    &= \rho - i\Delta t [H_{\ket{\bx}}, \rho] + O((\Delta t)^2)\nonumber\\
    &= e^{-iH_{\ket{\bx}}\Delta t}\rho e^{iH_{\ket{\bx}}\Delta t} + O((\Delta t)^2).
\end{align}
Therefore, if we apply $e^{-iH\Delta t}$ to $\ketbra{\bx}\otimes \rho$, we effectively apply $e^{-iH_{\ket{\bx}}\Delta t}$ to $\rho$, up to a second-order error $O((\Delta t)^2)$. Repeating this procedure $1/\Delta t$ times, we effectively apply $e^{-iH_{\ket{\bx}}}$ to $\rho$ up to error $O(\Delta t)$. By choosing $\Delta t$ sufficiently small, we can approximate $e^{-iH_{\ket{\bx}}}$ up to any precision. In our numerical simulations, we choose $H$ as a random 4-local Hamiltonian. Each term of $H$ is a 4-body Pauli interaction on 4 random positions with a random interaction strength between -1 and 1 (Fig. \ref{fig:CIM}).

\subsection{Training process and efficient compiling of $U_y$}
In this subsection, we show that the required unitary $U_y$ to implement the target-oriented perturbation can be compiled in an efficient way, with the count of CNOT gates scales logarithmically with the number of classes $k$. Recalling that $U_y=M_y\otimes Z + \sqrt{I-M_y^2}\otimes X$ acts on $\lceil\log k\rceil+1$ qubits, and $M_y = \ketbra{y} + (1-\eta)(\mathbf{I}-\ketbra{y})$, we arrive at the following decomposition:
\begin{eqnarray}
    U_y&=& \ketbra{y}\otimes Z+(I-\ketbra{y})\otimes((1-\eta)Z+\sqrt{2\eta-\eta^2}X)\\
    &=& [I\otimes((1-\eta)Z+\sqrt{2\eta-\eta^2}X)][\ketbra{y}\otimes ((1-\eta)Z+\sqrt{2\eta-\eta^2}X)Z+(I-\ketbra{y})\otimes I]. \nonumber
\end{eqnarray}
The first part of the above equation $I\otimes((1-\eta)Z+\sqrt{2\eta-\eta^2}X)$ is a single-qubit gate and can be compiled into a constant number of gates. The second part is a $\lceil\log k\rceil$-controlled $SU(2)$ gate, as $\ketbra{y}$ involves $\lceil\log k\rceil$ qubits and $(1-\eta)Z+\sqrt{2\eta-\eta^2}X)Z$ is a $SU(2)$ gate. Utilizing the techniques in Ref. \cite{valeDecompositionMulticontrolledSpecial2023}, such a multi-controlled $SU(2)$ gate can be compiled into $O(\log k)$ CNOT gates. This leads to the conclusion that the whole $U_y$ can be compiled in an efficient way with about  $\log k$ two-qubit gates.

\subsection{Prediction and evaluataion}

The predicted label of $\bx$ is the outcome of measurements in the computational basis performed on $U(\bx)\ket{\psi}$. So the probability of correct prediction (i.e., the accuracy) is $\braket{\psi|U(\bx)^{\dagger}\Pi_{f(\bx)}U(\bx)|\psi}=1-\braket{\psi|H_{\bx}|\psi}$. This accuracy can be amplified by repetition. Indeed, once the data in each step has been sampled, the training process is a fixed quantum circuit (with post-selection). So we can run the circuit multiple times to obtain $K$ copies of the final states $\ket{\psi}$. To predict the label of a new unseen data  sample $\bx$, we measure  $U(\bx)\ket{\psi}$ in the computational basis for each copy and do a majority vote. For simplicity, we consider the binary classification problem and assume $K$ is odd, then the probability of correct label (called $K$-accuracy) is 
\begin{equation*}
    p_K(\bx, \ket{\psi}) = \sum_{r=0}^{(K-1)/2}\binom{K}{r} \braket{\psi|H_{\bx}|\psi}^r (1-\braket{\psi|H_{\bx}|\psi})^{K-r}.
\end{equation*}
When $K=1$, this reduces to the single-copy accuracy $1-\braket{\psi|H_{\bx}|\psi}$. When $K= \infty$, the $K$-accuracy equals to the step function $1[\braket{\psi|H_{\bx}|\psi}<1/2]$. Throughout this paper, we call $K$ the number of trials.

\subsection{Gradient perspective}

As mentioned in the main text, the dissipation process of quantum automated learning actually implements the gradient descent algorithm in an automated manner. 
In the training step (iii), the quantum system is evolved through $U(\bx)$, $M_{y}$ and $U(\bx)^\dagger$. We define $M_{y} = \Pi_{y(\bx)} + (1-\eta) (\mathbf{I}-\Pi_{y(\bx)})$ and $H_{\bx}=\mathbf{I}-U(\bx)^\dagger \Pi_{y(\bx)}U(\bx)$, where $\Pi_{y(\bx)}$ denotes the measurement projection corresponding to the state encoding the label $y$. Then we get the updated (unnormalized) state $U(\bx)^\dagger M_{y} U(\bx) |\psi\rangle = (\mathbf{I} - \eta H_{\bx}) |\psi\rangle$. 

As mentioned above, the non-unitary perturbation $M_{y}$ in training step (iii) of the QAL protocol is implemented by block encoding into a unitary with an ancillary qubit combined with post-selection. As a result, this step effectively updates the state with unitary transformation: 
\begin{equation}\label{equ:update}
    \ket{\psi}\leftarrow \frac{(I-\eta H_{\bx})\ket{\psi}}{\norm{(I-\eta H_{\bx})\ket{\psi}}},
\end{equation} 
where $\norm{(I-\eta H_{\bx})\ket{\psi}}$ is a normalization factor whose square gives the success probability of post-selection. 





The probability of correct prediction of a datum $\bx$ reads $\braket{\psi|U(\bx)^{\dagger}\Pi_{y(\bx)}U(\bx)|\psi}=1-\braket{\psi|H_{\bx}|\psi}$. 
%This probability can be amplified by repeating the experiments multiple times and making majority vote.
As mentioned in the main text, we define the loss function as the average failure probability: $\hat{R}_S(\psi) = \E_{\bx\sim S}\braket{\psi|H_{\bx}|\psi}$,

where $\E_{\bx\sim S}$ denotes the expectation and 
$\bx\sim S$ means $\bx$ is uniformly sampled from the training set $S$. % and $H_S=\E_{\bx\sim S} H_{\bx}$. %This loss function has a clear operational meaning: the averaged failure probability to predict the label of a random training datum.
From the perspective of conventional machine learning, we may also regard $\ket{\psi}$ as a variational state parametrized by a complex vector $\bm{\psi}$. 
Given that the expectation value $\braket{\psi|H_{\bx}|\psi}$ is real, we transform the complex vector $\bm{\psi} = (\psi_1,\dots,\psi_{2^n})$ into a fully real representation: $(a_1,b_1,\dots,a_{2^n},b_{^n})$, where $a_i$ and $b_i$ denote the real and imaginary components of $\psi_i$ respectively. Owing to the Hermitian property of $H_x$, we derive the partial derivatives: $\frac{\partial \braket{\psi|H_{\bx}|\psi}}{\partial a_i} = 2Re(\sum_j(H_x)_{i,j}\psi_j)$ and $\frac{\partial \braket{\psi|H_{\bx}|\psi}}{\partial b_i} = 2Im(\sum_j(H_x)_{i,j}\psi_j)$. This allows us to define $\frac{\partial \braket{\psi|H_{\bx}|\psi}}{\partial \psi_i} = 2\sum_j(H_x)_{i,j}\psi_j$. Consequently, the gradient of $\braket{\psi|H_{\bx}|\psi}$ with respect to $\psi$ can be succinctly expressed as $2H_{\bx}\ket{\psi}$.


Therefore, the update rule  in Eq. \eqref{equ:update} essentially implements the stochastic projected gradient descent algorithm to minimize the loss function $\hat{R}_S(\psi)$ with a batch size one. Here we use the term ``projected'' to emphasize the normalization after each update. 
From the stochastic gradient descent perspective, one may conclude that an initial state $|\psi\rangle$ can exponentially converge to a local minimum through updating rule \eqref{equ:update} on expectation \cite{mohri2018foundations}.  However, a rigorous proof of convergence to the global minimum is unattainable in general. In fact, this is an inherent drawback for conventional gradient-based quantum learning approaches. Whereas, owing to the quadratic form of the loss function and the clear physical interpretation, we can rigorously prove that  $\ket{\psi}$ converges exponentially to the global minimum for the QAL protocol, as discussed in the main text and detailed in the following sections. 




\section{Physical Interpretation and Analytical Results}
Throughout this section, we use $\norm{A}_1, \norm{A}_\infty$ to denote the trace norm (the summation of singular values) and spectral norm (the largest singular value), respectively. For two Hermitian $A, B$, denote $A\preceq B$ if $B-A$ is positive semi-definite. By definition, for any $\bx$, $0\preceq H_{\bx}\preceq I$, and thus $0\preceq H_{S}\preceq I$. We will use the following fact.
\begin{lemma}\label{lem:norm_inequality}
    Let $A, B, C$ be three Hermitian matrices such that $\norm{A}_1\leq 1$, $0\preceq B, C\preceq I$. Then
    \begin{equation}
        \norm{BAB}_1\leq 1, \norm{BAC+CAB}_1\leq 2. 
    \end{equation}
\end{lemma}
\subsection{Formulation of the training process}
In this subsection, we explain the training process from a physical perspective and derive an analytical characterization of the success probability of post-selection and the performance of the final model. Observe that the empirical risk %\eqref{equ:empirical_risk} 
is the energy of $\ket{\psi}$ under the Hamiltonian $H_S$, so finding the global minimum is equivalent to finding the ground state of $H_S$. We rewrite \eqref{equ:update} in the density matrix formalism:
\begin{equation}\label{equ:update_density}
    \rho\xleftarrow{\bx} (I-\eta H_{\bx})\rho (I-\eta H_{\bx}).
\end{equation}
Here we keep the post-state $\rho$ unnormalized. Indeed, $\Tr(\rho)$ is the success probability of the post-selection. So the density matrix formalism helps us to keep track of the overall success probability. Another benefit of the density matrix formalism is that we can embed the randomness of the sample into the state. Since the datum $\bx$ is uniformly sampled from $S$, the averaged post-state up to the second order term is 
\begin{align}
    \rho &\leftarrow \E_{\bx\sim S} (I-\eta H_{\bx})\rho (I-\eta H_{\bx}) \nonumber\\
    &\approx I-\eta (H_S \rho+\rho H_S)\nonumber\\
    &\approx e^{-\eta H_S}\rho e^{-\eta H_S}. \label{equ:imaginary_time}
\end{align}
We make this approximation precise in the following lemma
\begin{lemma}\label{lem:imaginary_time_single_step}
    \begin{equation}
        \E_{\bx\sim S} (I-\eta H_{\bx})\rho (I-\eta H_{\bx}) = e^{-\eta H_S}\rho e^{-\eta H_S} + \eta^2 O,
    \end{equation}
    where $O$ is a Hermitian matrix with trace norm at most 4. 
\end{lemma}
\begin{proof}
    Let $R=(e^{-\eta H_S}-(I-\eta H_S))/\eta^2$. Since $0\preceq H_S\preceq I$, all the eigenvalues of $H_S$ are in $[0, 1]$. By the Taylor expansion of the exponential function, for any $x\in [0, 1]$, there exists $x^*\in [0, 1]$ such that $(e^{-\eta x}-(1-\eta x))/\eta^2 = (x^*)^2/2\in [0, 1/2]$. Therefore, $R$ is a Hermitian matrix such that $0\preceq R\preceq I/2$. By \Cref{lem:norm_inequality}, we have
    \begin{align}
        &\norm{\E_{\bx\sim S} (I-\eta H_{\bx})\rho (I-\eta H_{\bx}) - e^{-\eta H_S}\rho e^{-\eta H_S}}_1\nonumber\\
        =& \norm{\rho-\eta (H_S\rho +\rho H_S) + \eta^2 \E_{\bx\sim S}H_{\bx} \rho H_{\bx} - (\eta^2 R + (I-\eta H_S))\rho (\eta^2 R + (I-\eta H_S))}_1\nonumber\\
        =& \eta^2\norm{\big(\E_{\bx\sim S}H_{\bx}\rho H_{\bx} -H_S\rho H_S -\eta^2 R\rho R -(R\rho (I-\eta H_S)+(I-\eta H_S)\rho R)\big)}_1\nonumber\\
        \leq &\eta^2(1+1+\eta^2/4 +1) < 4\eta^2.\qedhere
    \end{align}
\end{proof}
Up to the second order term, \eqref{equ:imaginary_time} is the imaginary time evolution of $\rho$ under $H_S$. Suppose the initial state is $\sigma$, then the averaged state after $T$ epochs is $\rho = e^{-\eta T H_S}\sigma e^{-\eta T H_S}$. Let $\beta=\eta T$ be the summation of learning rates. We can approximate the success probability of possibility by $\tr(e^{-\beta H_S}\sigma e^{-\beta H_S})$ and the loss by $\tr(H_S\frac{e^{-\beta H_S}\sigma e^{-\beta H_S}}{\tr(e^{-\beta H_S}\sigma e^{-\beta H_S})})$. We summarize and prove the results in the following theorem.

\begin{theorem}\label{thm:formulation}
    Suppose we train the QAL model with initial state $\sigma$ for $T$ steps, with learning rate $\eta_t$ at step $t$. Define
    \begin{equation}
        \beta = \sum_{i=1}^T \eta_t,\quad \gamma = \sum_{i=1}^T \eta_t^2, \quad \sigma(\beta)=e^{-\beta H_S}\sigma e^{-\beta H_S}.
    \end{equation}
    Averaging over choice of training samples, the success probability of post-selection is 
    \begin{equation}
        \tr(\sigma(\beta))+c_1\gamma,\label{eq:success_probability}
    \end{equation}
    and the average loss conditioned on the success of post-selection is
    \begin{equation}
        \frac{\tr(H_S\sigma(\beta))+c_2\gamma}{\tr(\sigma(\beta))+c_1\gamma}. \label{eq:conditional_loss}
    \end{equation}
    Here $c_1, c_2$ are two real numbers such that $\abs{c_1}, \abs{c_2}\leq 4$.
\end{theorem}
\begin{proof}
    Let $\bx_1, \cdots, \bx_T \sim S$ be the training samples in the $T$ steps. We abbreviate $\bx_1, \cdots, \bx_t$ as $\bx_{1:t}$.
    By~\eqref{equ:update_density}, the unnormalized state after step $t$ is
    \begin{equation}
        \rho_t^{\bx_{1:t}} = (I-\eta_t H_{\bx_t})\cdots (I-\eta_1 H_{\bx_1})\sigma (I-\eta_1 H_{\bx_1})\cdots (I-\eta_t H_{\bx_t}).
    \end{equation}
    Given samples $\bx_{1:T}$, $\tr(\rho_T^{\bx_{1:T}})$ is the success probability of post-selection and $\tr(H_S\frac{\rho_T^{\bx_{1:T}}}{\tr(\rho_T^{\bx_{1:T}})})$ is the loss conditioned on the success of post-selection. We now average over the choice of samples.
    Recursively apply Lemma \ref{lem:imaginary_time_single_step} and Lemma \ref{lem:norm_inequality}, we have 
    \begin{equation}
        \E_{\bx_{1:T}\sim S^T}\rho_T^{\bx_{1:T}} = \sigma(\beta) + \gamma O,
    \end{equation}
    for some Hermitian $O$ with trace norm at most 4. The average success probability of post-selection is 
    \begin{equation}
        \E_{\bx_{1:T}\sim S^T}\tr(\rho_T^{\bx_{1:T}}) = \tr(\sigma(\beta)) + c_1\gamma,
    \end{equation}
    where $c_1=\tr(O)$ satisfies $\abs{c_1}\leq 4$. Now we calculate the averaged loss conditioned on the success of post-selection. For clarity, denote $q=\tr(\sigma(\beta)) + c_1\gamma$, $p=1/\abs{S}^T$ be the probability of sampling $\bx_1, \cdots, \bx_T$.
    Then conditioned on the success of post-selection, the conditional probability of sampling $\bx_{1:T}$ is $p\tr(\rho_T^{\bx_{1:T}})/q$. Therefore, the average loss conditioned on the success of post-selection is
    \begin{equation}
        \sum_{\bx_{1:T}\sim S^T} \frac{p\tr(\rho_T^{\bx_{1:T}})}{q} \tr(H_S\frac{\rho_T^{\bx_{1:T}}}{\tr(\rho_T^{\bx_{1:T}})}) = \frac{1}{q}\E_{\bx_{1:T}\sim S^T}\tr(H_S\rho_T^{\bx_{1:T}})=\frac{\tr(H_S\sigma(\beta)) + c_2\gamma}{\tr(\sigma(\beta)) + c_1\gamma},
    \end{equation}
    where $c_2=\tr(H_S O)$ satisfies $\abs{c_2}\leq 4$.
\end{proof}

According to the theorem, up to the second order term $c_1\gamma, c_2\gamma$, the training process behaves the same as the imaginary time evolution of $\sigma$ under $H_S$. The effect of imaginary time evolution is clearer in the eigenbasis of $H_S$. Write the spectrum decomposition of $H_S$ as $H_S=\sum_{i}E_i\ketbra{E_i}$ and define $\sigma_i=\braket{E_i|\sigma|E_i}$ as the overlap of $\sigma$ with the $i$-th eigenstate. Then 
\begin{equation}
    \sigma(\beta)=\sum_{i}\sigma_i e^{-2\beta E_i}\ketbra{E_i},~\quad
    \frac{\tr(H_S\sigma(\beta))}{\tr(\sigma(\beta))} = \frac{\sum_i E_i\sigma_i e^{-2\beta E_i}}{\sum_i \sigma_i e^{-2\beta E_i}}.
\end{equation}
The weight of $\ketbra{E_i}$, $\sigma_i e^{-2\beta E_i}$, decays exponentially with $\beta$. The decay is slower for lower energy eigenstates. Assume $\sigma$ has a non-zero overlap with the ground space. As $\beta$ goes up, eventually the weight of the ground space dominates, so $\rho_{\text{n}}(\beta)$ converges to a ground state of $H_S$ and the empirical risk converges to the global minimum. In the following, we will make this intuition rigorous in the presence of $c_1\gamma, c_2\gamma$. 

\subsection{Convergence to global minimum}
Denote the ground energy of $H_S$ (i.e., the global minimum of the loss) as $g$, the projector to the ground space as $\Pi_g$, and the gap between the ground energy and the first excited state as $\delta>0$. 
\begin{theorem}\label{thm:convergence_to_global_minimum}
    Suppose $\sigma$ has a nonzero overlap with the ground space of $H_S$ (that is, $\sigma_g=\tr(\Pi_g\sigma)>0$). For any constant $c\in (0, 1)$, we can choose an appropriate $\eta$ and $T$ such that if we train the QAL model for $T$ steps with learning rate $\eta$ in each step, the averaged loss conditioned on the success of post-selection is at most $g+c$.
    
    %For any $\beta$, we can choose efficiently small $\eta<\sigma_g/(6\beta e^{2\beta g})$ and set $T=\beta/\eta$, such that if we keep training the QAL model for $T$ steps with learning rate $\eta$ in each step, the averaged loss conditioned on the success of post-selection converges is at most $g+c_3e^{-2\beta\delta}+c_4\eta$ for some constants that depend on $g, \delta, \sigma_g, \beta$. 
\end{theorem}
\begin{proof}
    According to Theorem~\ref{thm:formulation}, we only need to upper bound~\eqref{eq:conditional_loss} for $\beta = \eta T$ and $\gamma=\eta^2 T=\beta\eta$. Since
    \begin{align}
        \frac{\tr(H_S\sigma(\beta))+c_2\beta\eta}{\tr(\sigma(\beta))+c_1\beta\eta}&\leq \frac{\sigma_g e^{-2\beta g}g + (1-\sigma_g)e^{-2\beta (g+\delta)}+4\beta\eta}{\sigma_g e^{-2\beta g} -4\beta\eta} \nonumber\\
        &=g + \frac{(1-\sigma_g)e^{-2\beta (g+\delta)}+(4+4g)\beta\eta}{\sigma_g e^{-2\beta g} -4\beta\eta}\nonumber\\
        &\leq g + \frac{e^{-2\beta \delta}+8\beta\eta e^{2\beta g}}{\sigma_g - 4\beta\eta e^{2\beta g}}\label{eq:global_minimum_1}.
    \end{align}
    Choose $\beta$ such that $e^{-2\beta\delta}<\sigma_gc/4$, and then choose $\eta$ such that $\beta\eta e^{2\beta g}<\sigma_gc/16<\sigma_g/16$. Then the right hand side of~\eqref{eq:global_minimum_1} is at most
    \begin{equation}
        g+\frac{\sigma_g c/4+\sigma_g c/2}{\sigma_g-\sigma_g/4} = g+c.\qedhere
    \end{equation}
\end{proof}

A randomly initialized state $\sigma$ has a nonzero overlap with the ground space with probability 1. According to the theorem, the QAL model will converge to the global minimum of the loss function. However, this convergence is built on the success of post-selection, whose probability exponentially decays with the number of steps. Therefore, a more realistic question is whether we can build a reasonable trade-off between the success probability and the performance of the final model. 

\subsection{Convergence with constant probability}
In this subsection, we will establish a practical trade-off between the accuracy of the final model and the success probability of post-selection when the initial state has a large overlap with the low-energy eigenspace of $H_S$.

\begin{definition}
    Let $H$ be a Hamiltonian. The $E$ low energy subspace of $H$ is the subspace spanned by the eigenstates of $H$ with energy at most $E$. Denote the projector to the $E$ low energy subspace as $\Pi_E^H$. The overlap of a state $\sigma$ with the $E$ low energy subspace is defined as $\tr(\Pi_E^H\sigma)$.
\end{definition}

Throughout this section, we focus on the Hamiltonian $H_S$ and omit the superscript $H$. 

\begin{theorem}\label{thm:constant_probability}
    Let $c_1, c_3\in (0, 1), c_2\in (0, 1/10)$ be three constants, $g$ be the ground energy of $H_S$, and $\epsilon>0$ such that $g/\epsilon \leq c_1$. Assume the overlap between the initial state $\sigma$ and the $(g+\epsilon)$ low energy eigenspace of $H_S$, namely $\tr(\sigma \Pi_{g+\epsilon})$, is at least $c_2$. Then we can choose an appropriate $\eta$ and $T$ such that if we train the QAL model with the initial state $\sigma$ for $T$ steps with learning rate $\eta$ in each step, the success probability of post-selection is at least $c_4$ and the averaged loss conditioned on the success of post-selection is at most $g+\epsilon+c_3$. Here $c_4$ is a constant that only depends on $c_1, c_2, c_3$. 
\end{theorem}
\begin{proof}
    By Theorem~\ref{thm:formulation}, we only need to lower bound the the success probability in \eqref{eq:success_probability} and the conditional loss in \eqref{eq:conditional_loss}. We will follow the notation in Theorem~\ref{thm:formulation}, so that $\beta = \eta T$, $\gamma = \eta^2 T$, and $\sigma(\beta)=e^{-\beta H}\sigma e^{-\beta H}$. Here we write $H=H_S$ for simplicity. We will prove the theorem for $\beta = 3\ln(1+c_2)/(c_3\epsilon)$, $c_4=e^{-6(1+c_1)\ln(1+c_2)/c_3}c_2/2$ and $\gamma=c_3c_4/40$. Accordingly, $\eta = \gamma/\beta$ is of order $\epsilon$ and $T=\beta^2/\gamma$ is of order $1/\epsilon^2$. 
    By~\eqref{eq:success_probability}, the success probability is at least
    \begin{align}
        \tr(\sigma(\beta)) - 4\gamma &\ge \tr(\Pi_{g+\epsilon}\sigma(\beta))-4\gamma\nonumber \\
        &=\tr(e^{-\beta H}\Pi_{g+\epsilon}e^{-\beta H}\sigma)-4\gamma\nonumber\\
        &\ge e^{-2\beta(g+\epsilon)}\tr(\Pi_{g+\epsilon}\sigma)-4\gamma\nonumber\\
        &\ge e^{-2\beta\epsilon(1+c_1)}c_2-4\gamma\nonumber\\
        &= 2c_4-4\gamma > c_4.
    \end{align}
    By~\eqref{eq:conditional_loss}, the averaged loss conditioned on the success of post-selection is at most
    \begin{align}
        \frac{\tr(H \sigma(\beta))+4\gamma}{\tr(\sigma(\beta))-4\gamma}&=g+\frac{\tr((H-gI)\sigma(\beta))}{\tr(\sigma(\beta))-4\gamma}+\frac{(4+4g)\gamma}{\tr(\sigma(\beta))-4\gamma}\nonumber\\
        &\leq g + \frac{\tr((H-gI)\sigma(\beta))}{\tr(\sigma(\beta))(1-c_3/20)}+\frac{8\gamma}{c_4}\nonumber\\
        &\leq g + \frac{c_3}{5} + (1+\frac{c_3}{5})\frac{\tr((H-gI)\sigma(\beta))}{\tr(\sigma(\beta))},\label{eq:constant_probability_proof1}
    \end{align}
    where we use $4\gamma=c_3c_4/10\leq c_3\tr(\sigma(\beta))/20$ in the second line and $(1+c_3/5)(1-c_3/20)\ge 0$ in the third line.
    So it suffices to upper bound $\tr((H-gI)\sigma(\beta))/\tr(\sigma(\beta))$. 
    Write the spectrum decomposition of $H$ as $H=\sum_i E_i\ketbra{E_i}$ and let $x_i = 2\beta(E_i-g), \sigma_i=\braket{E_i|\sigma|E_i}$. We simplify the last term of~\eqref{eq:constant_probability_proof1} to
    \begin{align}
        \frac{\tr((H-gI)\sigma(\beta))}{\tr(\sigma(\beta))} &= \frac{\sum_i (E_i-g)e^{-2\beta E_i}\sigma_i}{\sum_i e^{-2\beta E_i}\sigma_i}\nonumber\\
        &=\frac{\sum_{i}\sigma_i e^{-x_i}x_i}{\sum_{i}\sigma_i e^{-x_i}}\cdot \frac1{2\beta}.\label{eq:constant_probability_proof2}
    \end{align}
    Split the Hilbert space into low energy and high energy eigenspaces, $L=\{i: E_i\leq g+\epsilon\}$ and $H=\{i: E_i>g+\epsilon\}$. Let $p_L=\tr(\sigma \Pi_{g+\epsilon})=\sum_{i\in L}\sigma_i\ge c_2$ and $p_H=1-p_L$ be the overlaps of $\sigma$ with the two subspaces. 
    Since $f(y)=-y\ln(y)$ is concave, by Jensen's inequality, 
    \begin{equation}
        \sum_{i\in L}\frac{\sigma_i}{p_L} f(e^{-x_i})\leq f\big(\sum_{i\in L}\frac{\sigma_i}{p_L}e^{-x_i}\big).
    \end{equation}
    Let $l$ be the number such that $e^{-l}=\sum_{i\in L}\sigma_i e^{-x_i}/p_L$. The inequality becomes $\sum_{i\in L}\sigma_i e^{-x_i}x_i\leq p_L e^{-l}l$. Similarly, let $h=-\ln(\sum_{i\in H}\sigma_i e^{-x_i}/\sum_{i\in H}\sigma_i)$, then $\sum_{i\in H}\sigma_i e^{-x_i}x_i\leq p_He^{-h}h$. Therefore, 
    \begin{equation}
        \frac{\sum_{i}\sigma_i e^{-x_i}x_i}{\sum_{i}\sigma_i e^{-x_i}} \leq \frac{p_L e^{-l}l+p_He^{-h}h}{p_L e^{-l}+p_He^{-h}}.\label{eq:constant_probability_proof3}
    \end{equation}
    By definition, $x_i\in [0, 2\beta \epsilon]$ for $i\in L$ and $x_i>2\beta \epsilon $ for $i \in H$. Since $e^{-l}$ is a mixed of $e^{-x_i} (i\in L)$, we have $0\leq l\leq 2\beta\epsilon$ and similarly $h\ge 2\beta\epsilon$. Denote $y=h-l$. Insert~\eqref{eq:constant_probability_proof2} and~\eqref{eq:constant_probability_proof3} to~\eqref{eq:constant_probability_proof1}.
    \begin{align}
        \frac{\tr(H \sigma(\beta))+4\gamma}{\tr(\sigma(\beta))-4\gamma}&\leq g+\frac{c_3}{5}+ (1+\frac{c_3}{5})\frac{p_L e^{-l}l+p_He^{-h}h}{p_L e^{-l}+p_He^{-h}}\cdot\frac{1}{2\beta}\nonumber\\
        &= g + \frac{c_3}{5}+(1+\frac{c_3}{5})(l+\frac{p_H e^{-y}y}{p_L+p_He^{-y}})\cdot \frac1{2\beta}\nonumber\\
        &\leq g+\frac{c_3}{5} + (1+\frac{c_3}{5})(l+\frac{e^{-y}y}{c_2+e^{-y}})\cdot \frac1{2\beta}. \label{eq:constant_probability_proof4}
    \end{align}
    By differentiating $g(y)=e^{-y}y/(c_2+e^{-y})=y/(c_2e^{y}+1)$, we find that $g(y)\leq g(y^*)$ for the $y^*>0$ such that $c_2e^{y^*}(y^*-1)=1$. For this $y^*$ we have $g(y^*)=y^*-1$. Assume $y^*>\ln(1/c_2)>2$, then $1=c_2e^{y^*}(y^*-1)> c_2(1/c_2)(2-1)=1$, a contradiction. So $g(y)\leq g(y^*)=y^*-1\leq \ln(1/c_2)$. By~\eqref{eq:constant_probability_proof4}, the averaged loss conditioned on the success of post-selection is at most
    \begin{equation}
        g+\frac{c_3}{5} + (1+\frac{c_3}{5})(2\beta\epsilon+\ln(1/c_2))\cdot \frac1{2\beta} = g+\frac{c_3}{5}+(1+\frac{c_3}{5})(\epsilon+\frac{c_3\epsilon}{6})<g+\epsilon+c_3.\qedhere
    \end{equation}
\end{proof}

The theorem ensures the convergence of the QAL training process with a constant success probability, assuming a good initial state. Concretely, as long as the initial state has a large (at least $c_2$) overlap with the low energy eigenspace (with energy at most $g+\epsilon$), the QAL training process will converge to the low energy eigenspace (up to an arbitrarily small residue error $c_3$) with a constant success probability ($c_4$ that only depends on $c_1, c_2, c_3$). 

\subsection{Heavy-tailed Hamiltonian}
Theorem \ref{thm:constant_probability} highlights the importance of the initial state $\sigma$. However, without prior knowledge of $H_S$, we cannot do better than a random guess, or equivalently, starting from the maximally-mixed state $\sigma = I/2^n$. Therefore, we actually hope that $H_S$ has a constant proportion of low energy eigenstates that do not scale up with $n$, the dimension of $\bx$, and the size $m$ of the training dataset. We formalize this intuition in the following definition.

\begin{definition}[Heavy-tailed Hamiltonian]
    We say a Hamiltonian $H$ is $(E, c)$-heavy-tailed if the proportion of eigenstates with energy at most $E$ is at least $c$.
\end{definition}

\begin{theorem}\label{thm:heavy_tailed}
    Let $c_1, c_3\in (0, 1), c_2\in (0, 1/10)$ be three constants. Suppose $H_S$ is $(g+\epsilon, c_2)$-heavy-tailed, where $g$ is the ground energy of $H_S$ and $\epsilon>0$ such that $g/\epsilon \leq c_1$. Then we can choose an appropriate $\eta$ and $T$ such that if we train the QAL model with a maximally-mixed initial state in computational basis for $T$ steps with learning rate $\eta$ in each step, the success probability of post-selection is at least $c_4$ and the averaged loss conditioned on the success of post-selection is at most $g+\epsilon+c_3$. Here $c_4$ is a constant that only depends on $c_1, c_2, c_3$.
\end{theorem}

\begin{proof}
    By definition of heavy-tailed Hamiltonian, the overlap between the initial state $\sigma=I/2^n$ and the $(g+\epsilon)$ low eigenspace of $H_S$ is at least $c_2$. The theorem follows directly from Theorem~\ref{thm:constant_probability}.
\end{proof}

Therefore, the QAL training process is guaranteed to converge to the low energy eigenspace of a heavy-tailed $H_S$.
We now argue that when $H_S$ comes from a reasonable dataset, it is likely to be heavy-tailed due to the similarity of data.
Consider the extremely simple example of classifying dogs and cats, where all dogs look similar and all cats look similar. The Hamiltonian $H_S$ is approximately a mixture of two projectors of dimensions $2^{n-1}$, $H_{\text{dogs}}$ and $H_{\text{cats}}$. Regard $H_{\text{dogs}}$ and $H_{\text{cats}}$ as random projectors, then $H_S$ has a constant proportion of near-zero eigenvalues. This assumption is supported by the numerical simulation.

Remark that while Theorem~\ref{thm:heavy_tailed} applies to the maximally-mixed initial state, in reality we will use a random initial state in the computational basis. Once we sample an initial state better than the maximally-mixed state, we can stick to it and apply Theorem~\ref{thm:constant_probability}.

\subsection{Generalization}
The previous results establish the explainable trainability of QAL. While in training classical neural networks, each epoch is a full pass of the training dataset, in QAL, each step only involves a single datum. This indicates that the QAL model could be optimized using a few data points. In this subsection we rigorously demonstrate the generalization ability of QAL, showing that once the model achieves a good performance on a small dataset, the good performance will generalize to unseen data.

Recall that the training loss and the true loss of $\ket{\psi}$ are $\hat{R}_S(\psi)=\E_{x\sim S}\braket{\psi|H_{\bx}|\psi}$ and $R(\psi)=\E_{\bx\sim \cD}\braket{\psi|H_{\bx}|\psi}$, respectively.
\begin{theorem}\label{thm:generalization}
    With probability at least $1-\delta$ over the choice of $S$, the generalization gap is upper bounded by
    \begin{equation}
        \max_{\ket{\psi}}\big(R(\psi)-\hat{R}_S(\psi)\big) \leq \sqrt{\frac{4\ln(2^{n+1}/\delta)}{m}}.
    \end{equation}
\end{theorem}
\begin{proof}
    By definition, the left-hand side is upper bounded by the spectral norm of $\E_{x\sim_u S}H_{x}-\E_{x\sim \cD}H_{x}$, which can be bounded by matrix Bernstein inequality (see, e.g., \cite[Theorem 6.1.1]{troppIntroductionMatrixConcentration2015}): \begin{equation*}
        \Pr_S[\norm{\E_{x\sim_u S}H_x-\E_{x\sim \cD}H_x}_2\ge t]\leq 2^{n+1}\exp(-mt^2/4).
    \end{equation*}
    The theorem follows by setting $t=\sqrt{4\ln(2^{n+1}/\delta)/m}$.
\end{proof}
According to the theorem, as long as the size $m$ of the training dataset is larger than $\Omega(n)$ (i.e., a logarithm of the degree of freedom), a parameter state $\ket{\btheta}$ with low training loss has a low true loss with high probability. This is better than quantum neural networks where the training dataset size has to be larger than the degree of freedom \cite{caiSampleComplexityLearning2022,caroGeneralizationQuantumMachine2022}. 
We remark that the good generalization stems from the simple quadratic form of the loss function.

\section{More Numerical Results}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{mnist.pdf}
    \captionsetup{justification=raggedright, singlelinecheck=false}
    \caption{Classify images in the MNIST dataset \cite{lecun1998mnist} using QAL. \textbf{a}, Testing and training accuracy during the training process. \textbf{b} The trade-off between testing accuracy and the success probability of post-selection. \textbf{c}, Spectrum of the associated Hamiltonian $H_S$ of the training dataset.}
    \label{fig:mnist}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{AA_hamiltonian.pdf}
    \captionsetup{justification=raggedright, singlelinecheck=false}
    \caption{Classify Aubry-Andr\'{e} Hamiltonian \cite{aubry1980analyticity} using QAL. \textbf{a}, Testing and training accuracy during the training process. \textbf{b} The trade-off between testing accuracy and the success probability of post-selection. \textbf{c}, Spectrum of the associated Hamiltonian $H_S$ of the training dataset.}
    \label{fig:AA}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{CIM.pdf}
    \captionsetup{justification=raggedright, singlelinecheck=false}
    \caption{Classify the ground state of clustering-Ising model  \cite{son2011quantum,smacchia2011statistical} using QAL. \textbf{a}, Testing and training accuracy during the training process. \textbf{b} The trade-off between testing accuracy and the success probability of post-selection. \textbf{c}, Spectrum of the associated Hamiltonian $H_S$ of the training dataset.}
    \label{fig:CIM}
\end{figure}

Besides the numerical simulation on the Fashion MNIST dataset in the main text, we also conduct QAL protocol on MNIST dataset, Hamiltonian dataset, and quantum state dataset. The data encoding scheme for each dataset is describe in For each dataset. We keep the setting the same, so that the number of qubits is 10, the learning rate is $0.1$, and the test data size is 500. For each dataset, we plot the training performance, trade-off between accuracy and success probability of post-selection, and spectrum of data Hamiltonian as in the main text.

MNIST is a widely used benchmark dataset for handwritten digit recognition~\cite{lecun1998mnist}. Here, we follow the same data processing routine as Fashion MNIST. We focus on the binary classification of classes "1" and "9". All images are rescaled to $10\times 10$ pixels, and pixel values are normalized. The results are shown in Fig.~\ref{fig:mnist}.




For Hamiltonian data, we consider a binary classification of the following Aubry-Andr\'{e} Hamiltonian on 10 qubits~\cite{aubry1980analyticity}:
\begin{equation*}
H=-\frac g2 \sum_k (\sigma_k^x\sigma_{k+1}^x+\sigma_k^y\sigma_{k+1}^y)-\frac V2 \sum_k \cos(2\pi \phi k)\sigma_k^z, 
\end{equation*}
where $g$ is the coupling strength, $\sigma_k^\alpha$ ($\alpha=x, y, z$) are Pauli operators on the $k$-th qubit, $V$ is the disorder magnitude and $\phi=(\sqrt{5}-1)/2$. This Hamiltonian exhibits a quantum phase transition at $V/g=2$, between a localized phase for $V/g>2$ and a delocalized phase for $V/g<2$. To generate the dataset, we fix $g=1$, sample $V$ in the interval $[0, 4]$ and label the Hamiltonian according to its phase. To carry out the QAL protocol, we encode the Hamiltonian into its real-time evolution $e^{-2iH}$. Our numerical results for Hamiltonian data are ploted in Fig.~\ref{fig:AA}.

For quantum state data, we classify the ground state of 10-qubit clustering-Ising model Hamiltonian~\cite{son2011quantum,smacchia2011statistical} with periodic boundary condition:
\begin{equation*}
    H(h)=-\sum_{k}\sigma_k^x\sigma_{k+1}^z\sigma_{k+2}^x+h\sum_{k}\sigma_k^y\sigma_{k+1}^y,
\end{equation*}
This Hamiltonian has a phase transition at $h=1$, between a symmetry protected topological phase ($h<1$) and an antiferromagnetic phase ($h>1$). The dataset is generated by sampling $h$ from interval $[0, 2]$ and labeling the state according to its phase. As mentioned in Sec.~\ref{supp_sec:data_encoding}, the data encoding scheme relies on a Hamiltonian. In our numerical simulations, $H$ is a random 4-local Hamiltonian. Each term of $H$ is a 4-body Pauli interaction on 4 random positions with a random interaction strength between -1 and 1. The results are displayed in Fig.~\ref{fig:CIM}.


%apsrev4-2.bst 2019-01-14 (MD) hand-edited version of apsrev4-1.bst
%Control: key (0)
%Control: author (8) initials jnrlst
%Control: editor formatted (1) identically to author
%Control: production of article title (0) allowed
%Control: page (0) single
%Control: year (1) truncated
%Control: production of eprint (0) enabled
\begin{thebibliography}{26}%
\makeatletter
\providecommand \@ifxundefined [1]{%
 \@ifx{#1\undefined}
}%
\providecommand \@ifnum [1]{%
 \ifnum #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \@ifx [1]{%
 \ifx #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \natexlab [1]{#1}%
\providecommand \enquote  [1]{``#1''}%
\providecommand \bibnamefont  [1]{#1}%
\providecommand \bibfnamefont [1]{#1}%
\providecommand \citenamefont [1]{#1}%
\providecommand \href@noop [0]{\@secondoftwo}%
\providecommand \href [0]{\begingroup \@sanitize@url \@href}%
\providecommand \@href[1]{\@@startlink{#1}\@@href}%
\providecommand \@@href[1]{\endgroup#1\@@endlink}%
\providecommand \@sanitize@url [0]{\catcode `\\12\catcode `\$12\catcode `\&12\catcode `\#12\catcode `\^12\catcode `\_12\catcode `\%12\relax}%
\providecommand \@@startlink[1]{}%
\providecommand \@@endlink[0]{}%
\providecommand \url  [0]{\begingroup\@sanitize@url \@url }%
\providecommand \@url [1]{\endgroup\@href {#1}{\urlprefix }}%
\providecommand \urlprefix  [0]{URL }%
\providecommand \Eprint [0]{\href }%
\providecommand \doibase [0]{https://doi.org/}%
\providecommand \selectlanguage [0]{\@gobble}%
\providecommand \bibinfo  [0]{\@secondoftwo}%
\providecommand \bibfield  [0]{\@secondoftwo}%
\providecommand \translation [1]{[#1]}%
\providecommand \BibitemOpen [0]{}%
\providecommand \bibitemStop [0]{}%
\providecommand \bibitemNoStop [0]{.\EOS\space}%
\providecommand \EOS [0]{\spacefactor3000\relax}%
\providecommand \BibitemShut  [1]{\csname bibitem#1\endcsname}%
\let\auto@bib@innerbib\@empty
%</preamble>
\bibitem [{\citenamefont {Goodfellow}\ \emph {et~al.}(2016)\citenamefont {Goodfellow}, \citenamefont {Bengio},\ and\ \citenamefont {Courville}}]{Goodfellow2016Deep}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {I.}~\bibnamefont {Goodfellow}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Bengio}},\ and\ \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Courville}},\ }\href@noop {} {\emph {\bibinfo {title} {Deep {{Learning}}}}}\ (\bibinfo  {publisher} {MIT Press},\ \bibinfo {year} {2016})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cerezo}\ \emph {et~al.}(2021{\natexlab{a}})\citenamefont {Cerezo}, \citenamefont {Arrasmith}, \citenamefont {Babbush}, \citenamefont {Benjamin}, \citenamefont {Endo}, \citenamefont {Fujii}, \citenamefont {McClean}, \citenamefont {Mitarai}, \citenamefont {Yuan}, \citenamefont {Cincio},\ and\ \citenamefont {Coles}}]{Cerezo2021Variational}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Arrasmith}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Babbush}}, \bibinfo {author} {\bibfnamefont {S.~C.}\ \bibnamefont {Benjamin}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Endo}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Fujii}}, \bibinfo {author} {\bibfnamefont {J.~R.}\ \bibnamefont {McClean}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Mitarai}}, \bibinfo {author} {\bibfnamefont {X.}~\bibnamefont {Yuan}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Cincio}},\ and\ \bibinfo {author} {\bibfnamefont {P.~J.}\ \bibnamefont {Coles}},\ }\bibfield  {title} {\bibinfo {title} {Variational quantum algorithms},\ }\href {https://doi.org/10.1038/s42254-021-00348-9} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Rev. Phys.}\ }\textbf {\bibinfo {volume} {3}},\ \bibinfo {pages} {625} (\bibinfo {year} {2021}{\natexlab{a}})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Mitarai}\ \emph {et~al.}(2018)\citenamefont {Mitarai}, \citenamefont {Negoro}, \citenamefont {Kitagawa},\ and\ \citenamefont {Fujii}}]{Mitarai2018Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Mitarai}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Negoro}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Kitagawa}},\ and\ \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Fujii}},\ }\bibfield  {title} {\bibinfo {title} {Quantum circuit learning},\ }\href {https://doi.org/10.1103/PhysRevA.98.032309} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. A}\ }\textbf {\bibinfo {volume} {98}},\ \bibinfo {pages} {032309} (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Schuld}\ \emph {et~al.}(2019)\citenamefont {Schuld}, \citenamefont {Bergholm}, \citenamefont {Gogolin}, \citenamefont {Izaac},\ and\ \citenamefont {Killoran}}]{Schuld2019Evaluating}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Schuld}}, \bibinfo {author} {\bibfnamefont {V.}~\bibnamefont {Bergholm}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Gogolin}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Izaac}},\ and\ \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Killoran}},\ }\bibfield  {title} {\bibinfo {title} {Evaluating analytic gradients on quantum hardware},\ }\href {https://doi.org/10.1103/PhysRevA.99.032331} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. A}\ }\textbf {\bibinfo {volume} {99}},\ \bibinfo {pages} {032331} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Stokes}\ \emph {et~al.}(2020)\citenamefont {Stokes}, \citenamefont {Izaac}, \citenamefont {Killoran},\ and\ \citenamefont {Carleo}}]{Stokes2020Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Stokes}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Izaac}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Killoran}},\ and\ \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Carleo}},\ }\bibfield  {title} {\bibinfo {title} {Quantum {{Natural Gradient}}},\ }\href {https://doi.org/10.22331/q-2020-05-25-269} {\bibfield  {journal} {\bibinfo  {journal} {Quantum}\ }\textbf {\bibinfo {volume} {4}},\ \bibinfo {pages} {269} (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cai}\ \emph {et~al.}(2022)\citenamefont {Cai}, \citenamefont {Ye},\ and\ \citenamefont {Deng}}]{caiSampleComplexityLearning2022}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Cai}}, \bibinfo {author} {\bibfnamefont {Q.}~\bibnamefont {Ye}},\ and\ \bibinfo {author} {\bibfnamefont {D.-L.}\ \bibnamefont {Deng}},\ }\bibfield  {title} {\bibinfo {title} {Sample complexity of learning parametric quantum circuits},\ }\href {https://doi.org/10.1088/2058-9565/ac4f30} {\bibfield  {journal} {\bibinfo  {journal} {Quantum Sci. Technol.}\ }\textbf {\bibinfo {volume} {7}},\ \bibinfo {pages} {025014} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Caro}\ \emph {et~al.}(2022)\citenamefont {Caro}, \citenamefont {Huang}, \citenamefont {Cerezo}, \citenamefont {Sharma}, \citenamefont {Sornborger}, \citenamefont {Cincio},\ and\ \citenamefont {Coles}}]{caroGeneralizationQuantumMachine2022}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.~C.}\ \bibnamefont {Caro}}, \bibinfo {author} {\bibfnamefont {H.-Y.}\ \bibnamefont {Huang}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Sharma}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Sornborger}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Cincio}},\ and\ \bibinfo {author} {\bibfnamefont {P.~J.}\ \bibnamefont {Coles}},\ }\bibfield  {title} {\bibinfo {title} {Generalization in quantum machine learning from few training data},\ }\href {https://doi.org/10.1038/s41467-022-32550-3} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {13}},\ \bibinfo {pages} {4919} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {You}\ and\ \citenamefont {Wu}(2021)}]{youExponentiallyManyLocal2021}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {X.}~\bibnamefont {You}}\ and\ \bibinfo {author} {\bibfnamefont {X.}~\bibnamefont {Wu}},\ }\bibfield  {title} {\bibinfo {title} {Exponentially {{Many Local Minima}} in {{Quantum Neural Networks}}},\ }in\ \href {https://proceedings.mlr.press/v139/you21c.html} {\emph {\bibinfo {booktitle} {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}}}}\ (\bibinfo  {publisher} {PMLR},\ \bibinfo {year} {2021})\ pp.\ \bibinfo {pages} {12144--12155}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {McClean}\ \emph {et~al.}(2018)\citenamefont {McClean}, \citenamefont {Boixo}, \citenamefont {Smelyanskiy}, \citenamefont {Babbush},\ and\ \citenamefont {Neven}}]{McClean2018Barren}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.~R.}\ \bibnamefont {McClean}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Boixo}}, \bibinfo {author} {\bibfnamefont {V.~N.}\ \bibnamefont {Smelyanskiy}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Babbush}},\ and\ \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Neven}},\ }\bibfield  {title} {\bibinfo {title} {Barren plateaus in quantum neural network training landscapes},\ }\href {https://doi.org/10.1038/s41467-018-07090-4} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {9}},\ \bibinfo {pages} {4812} (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cerezo}\ \emph {et~al.}(2021{\natexlab{b}})\citenamefont {Cerezo}, \citenamefont {Sone}, \citenamefont {Volkoff}, \citenamefont {Cincio},\ and\ \citenamefont {Coles}}]{Cerezo2021Cost}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Sone}}, \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Volkoff}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Cincio}},\ and\ \bibinfo {author} {\bibfnamefont {P.~J.}\ \bibnamefont {Coles}},\ }\bibfield  {title} {\bibinfo {title} {Cost {{Function Dependent Barren Plateaus}} in {{Shallow Parametrized Quantum Circuits}}},\ }\href {https://doi.org/10.1038/s41467-021-21728-w} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {12}},\ \bibinfo {pages} {1791} (\bibinfo {year} {2021}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Ortiz~Marrero}\ \emph {et~al.}(2021)\citenamefont {Ortiz~Marrero}, \citenamefont {Kieferov{\'a}},\ and\ \citenamefont {Wiebe}}]{OrtizMarrero2021EntanglementInduced}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Ortiz~Marrero}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Kieferov{\'a}}},\ and\ \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Wiebe}},\ }\bibfield  {title} {\bibinfo {title} {Entanglement-{{Induced Barren Plateaus}}},\ }\href {https://doi.org/10.1103/PRXQuantum.2.040316} {\bibfield  {journal} {\bibinfo  {journal} {PRX Quantum}\ }\textbf {\bibinfo {volume} {2}},\ \bibinfo {pages} {040316} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Holmes}\ \emph {et~al.}(2022)\citenamefont {Holmes}, \citenamefont {Sharma}, \citenamefont {Cerezo},\ and\ \citenamefont {Coles}}]{Holmes2022Connecting}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Holmes}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Sharma}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}},\ and\ \bibinfo {author} {\bibfnamefont {P.~J.}\ \bibnamefont {Coles}},\ }\bibfield  {title} {\bibinfo {title} {Connecting ansatz expressibility to gradient magnitudes and barren plateaus},\ }\href {https://doi.org/10.1103/PRXQuantum.3.010313} {\bibfield  {journal} {\bibinfo  {journal} {PRX Quantum}\ }\textbf {\bibinfo {volume} {3}},\ \bibinfo {pages} {010313} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Wang}\ \emph {et~al.}(2021)\citenamefont {Wang}, \citenamefont {Fontana}, \citenamefont {Cerezo}, \citenamefont {Sharma}, \citenamefont {Sone}, \citenamefont {Cincio},\ and\ \citenamefont {Coles}}]{Wang2021Noiseinduced}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {E.}~\bibnamefont {Fontana}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Sharma}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Sone}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Cincio}},\ and\ \bibinfo {author} {\bibfnamefont {P.~J.}\ \bibnamefont {Coles}},\ }\bibfield  {title} {\bibinfo {title} {Noise-induced barren plateaus in variational quantum algorithms},\ }\href {https://doi.org/10.1038/s41467-021-27045-6} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {12}},\ \bibinfo {pages} {6961} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Larocca}\ \emph {et~al.}(2024)\citenamefont {Larocca}, \citenamefont {Thanasilp}, \citenamefont {Wang}, \citenamefont {Sharma}, \citenamefont {Biamonte}, \citenamefont {Coles}, \citenamefont {Cincio}, \citenamefont {McClean}, \citenamefont {Holmes},\ and\ \citenamefont {Cerezo}}]{Larocca2024Review}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Larocca}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Thanasilp}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Sharma}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Biamonte}}, \bibinfo {author} {\bibfnamefont {P.~J.}\ \bibnamefont {Coles}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Cincio}}, \bibinfo {author} {\bibfnamefont {J.~R.}\ \bibnamefont {McClean}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Holmes}},\ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}},\ }\bibfield  {title} {\bibinfo {title} {A {{Review}} of {{Barren Plateaus}} in {{Variational Quantum Computing}}},\ }\Eprint {https://arxiv.org/abs/2405.00781} {arXiv:2405.00781}  (\bibinfo {year} {2024})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Childs}\ and\ \citenamefont {Wiebe}(2012)}]{Childs2012Hamiltonian}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~M.}\ \bibnamefont {Childs}}\ and\ \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Wiebe}},\ }\bibfield  {title} {\bibinfo {title} {Hamiltonian {{Simulation Using Linear Combinations}} of {{Unitary Operations}}},\ }\href {https://doi.org/10.26421/QIC12.11-12} {\bibfield  {journal} {\bibinfo  {journal} {QIC}\ }\textbf {\bibinfo {volume} {12}},\ \bibinfo {pages} {901} (\bibinfo {year} {2012})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Georgescu}\ \emph {et~al.}(2014)\citenamefont {Georgescu}, \citenamefont {Ashhab},\ and\ \citenamefont {Nori}}]{Georgescu2014Quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {I.~M.}\ \bibnamefont {Georgescu}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Ashhab}},\ and\ \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Nori}},\ }\bibfield  {title} {\bibinfo {title} {Quantum simulation},\ }\href {https://doi.org/10.1103/RevModPhys.86.153} {\bibfield  {journal} {\bibinfo  {journal} {Rev. Mod. Phys.}\ }\textbf {\bibinfo {volume} {86}},\ \bibinfo {pages} {153} (\bibinfo {year} {2014})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Low}\ and\ \citenamefont {Chuang}(2017)}]{Low2017Optimal}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.~H.}\ \bibnamefont {Low}}\ and\ \bibinfo {author} {\bibfnamefont {I.~L.}\ \bibnamefont {Chuang}},\ }\bibfield  {title} {\bibinfo {title} {Optimal hamiltonian simulation by quantum signal processing},\ }\href {https://doi.org/10.1103/PhysRevLett.118.010501} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {118}},\ \bibinfo {pages} {010501} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Clinton}\ \emph {et~al.}(2021)\citenamefont {Clinton}, \citenamefont {Bausch},\ and\ \citenamefont {Cubitt}}]{Clinton2021Hamiltonian}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Clinton}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Bausch}},\ and\ \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Cubitt}},\ }\bibfield  {title} {\bibinfo {title} {Hamiltonian simulation algorithms for near-term quantum hardware},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {12}},\ \bibinfo {pages} {4989} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Lloyd}\ \emph {et~al.}(2014)\citenamefont {Lloyd}, \citenamefont {Mohseni},\ and\ \citenamefont {Rebentrost}}]{lloydQuantumPrincipalComponent2014}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Lloyd}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Mohseni}},\ and\ \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Rebentrost}},\ }\bibfield  {title} {\bibinfo {title} {Quantum principal component analysis},\ }\href {https://doi.org/10.1038/nphys3029} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Phys.}\ }\textbf {\bibinfo {volume} {10}},\ \bibinfo {pages} {631} (\bibinfo {year} {2014})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Vale}\ \emph {et~al.}(2023)\citenamefont {Vale}, \citenamefont {Azevedo}, \citenamefont {Ara{\'u}jo}, \citenamefont {Araujo},\ and\ \citenamefont {{da Silva}}}]{valeDecompositionMulticontrolledSpecial2023}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Vale}}, \bibinfo {author} {\bibfnamefont {T.~M.~D.}\ \bibnamefont {Azevedo}}, \bibinfo {author} {\bibfnamefont {I.~C.~S.}\ \bibnamefont {Ara{\'u}jo}}, \bibinfo {author} {\bibfnamefont {I.~F.}\ \bibnamefont {Araujo}},\ and\ \bibinfo {author} {\bibfnamefont {A.~J.}\ \bibnamefont {{da Silva}}},\ }\bibfield  {title} {\bibinfo {title} {Decomposition of {{Multi-controlled Special Unitary Single-Qubit Gates}}},\ }\Eprint {https://arxiv.org/abs/2302.06377} {arXiv:2302.06377}  (\bibinfo {year} {2023})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Mohri}\ \emph {et~al.}(2018)\citenamefont {Mohri}, \citenamefont {Rostamizadeh},\ and\ \citenamefont {Talwalkar}}]{mohri2018foundations}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Mohri}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Rostamizadeh}},\ and\ \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Talwalkar}},\ }\href@noop {} {\emph {\bibinfo {title} {Foundations of machine learning}}}\ (\bibinfo  {publisher} {MIT press},\ \bibinfo {year} {2018})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Tropp}(2015)}]{troppIntroductionMatrixConcentration2015}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.~A.}\ \bibnamefont {Tropp}},\ }\bibfield  {title} {\bibinfo {title} {An {{Introduction}} to {{Matrix Concentration Inequalities}}},\ }\Eprint {https://arxiv.org/abs/1501.01571} {arXiv:1501.01571}  (\bibinfo {year} {2015})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {LeCun}\ \emph {et~al.}(1998)\citenamefont {LeCun}, \citenamefont {Cortes},\ and\ \citenamefont {Burges}}]{lecun1998mnist}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {LeCun}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Cortes}},\ and\ \bibinfo {author} {\bibfnamefont {C.~J.~C.}\ \bibnamefont {Burges}},\ }\href {http://yann.lecun.com/exdb/mnist/} {\bibinfo {title} {The {MNIST} database of handwritten digits}} (\bibinfo {year} {1998})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Aubry}\ and\ \citenamefont {Andr{\'e}}(1980)}]{aubry1980analyticity}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Aubry}}\ and\ \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Andr{\'e}}},\ }\bibfield  {title} {\bibinfo {title} {Analyticity breaking and anderson localization in incommensurate lattices},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Ann. Israel Phys. Soc}\ }\textbf {\bibinfo {volume} {3}},\ \bibinfo {pages} {18} (\bibinfo {year} {1980})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Son}\ \emph {et~al.}(2011)\citenamefont {Son}, \citenamefont {Amico}, \citenamefont {Fazio}, \citenamefont {Hamma}, \citenamefont {Pascazio},\ and\ \citenamefont {Vedral}}]{son2011quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Son}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Amico}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Fazio}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Hamma}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Pascazio}},\ and\ \bibinfo {author} {\bibfnamefont {V.}~\bibnamefont {Vedral}},\ }\bibfield  {title} {\bibinfo {title} {Quantum phase transition between cluster and antiferromagnetic states},\ }\href {https://doi.org/10.1209/0295-5075/95/50001} {\bibfield  {journal} {\bibinfo  {journal} {EPL}\ }\textbf {\bibinfo {volume} {95}},\ \bibinfo {pages} {50001} (\bibinfo {year} {2011})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Smacchia}\ \emph {et~al.}(2011)\citenamefont {Smacchia}, \citenamefont {Amico}, \citenamefont {Facchi}, \citenamefont {Fazio}, \citenamefont {Florio}, \citenamefont {Pascazio},\ and\ \citenamefont {Vedral}}]{smacchia2011statistical}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Smacchia}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Amico}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Facchi}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Fazio}}, \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Florio}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Pascazio}},\ and\ \bibinfo {author} {\bibfnamefont {V.}~\bibnamefont {Vedral}},\ }\bibfield  {title} {\bibinfo {title} {Statistical mechanics of the cluster ising model},\ }\href {https://doi.org/10.1103/PhysRevA.84.022304} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. A}\ }\textbf {\bibinfo {volume} {84}},\ \bibinfo {pages} {022304} (\bibinfo {year} {2011})}\BibitemShut {NoStop}%
\end{thebibliography}%

\end{document}