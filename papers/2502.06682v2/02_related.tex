\section{Related Work}
\label{sec:related}
\textbf{Collaborative autonomous driving (CAV)} offers significant benefits, including extended perception range by sharing sensor information to detect objects beyond a single vehicle's field of view.
Despite these benefits, collecting large-scale, real-world datasets for CAV poses significant challenges due to the complexity and cost of deploying multiple instrumented vehicles in diverse environments. Existing datasets often have limitations that restrict the scope of CAV research. For instance, OPV2V~\cite{xu2022opv2v} and V2X-Sim~\cite{li2022v2xsim} rely on simulations using the CARLA simulator~\citep{Dosovitskiy17carla}, allowing controlled scenarios with multiple vehicle agents, yet lacking real-world variability. V2V4Real~\cite{xu2023v2v4real}, on the other hand, provides real-world data, yet it only contains two collaborating agents, which restricts the exploration of more complex multi-agent interactions. Similarly, the DAIR-V2X~\cite{dair-v2x} dataset also offers real-world data including vehicle-to-infrastructure (V2I) and V2V, but it mainly focuses on V2I scenarios and still has a limited number of collaborating vehicles. To overcome current limitations, we propose a new research direction---generating realistic point clouds given the ego's perspective. 

\nbf{3D generation} Various works have explored 3D scene generation, such as LiDARGen~\cite{zyrianov2022learning-lidargen}, R2DM~\cite{nakashima2024lidar-r2dm}, LiDM~\cite{ran2024towards}, UltraLiDAR~\citep{xiong2023learning}, and RangeLDM~\cite{hu2025rangeldm}. Additionally, LidarDM~\cite{zyrianov2024lidardm} and Text2LiDAR~\cite{wu2024text2lidar} investigate conditional scene generation, with the former relying on hand-crafted map layouts and the latter conditioning on text inputs. However, all these methods focus on ego-centric generation. We propose a new research direction for CAV, aiming to generate realistic and consistent scenes conditioned on the ego agent's real point clouds---an area that remains largely unexplored.



\nbf{Diffusion models}
Diffusion models have recently advanced generative modeling for high-quality LiDAR point clouds and images. Denoising Diffusion Probabilistic Models (DDPMs)~\cite{ho2020denoising} outperform traditional Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative} and further enhance efficiency for LiDAR generation. In autonomous driving applications, methods like RangeLDM~\cite{hu2025rangeldm}, LidarDM~\cite{zyrianov2024lidardm}, and LiDARGen~\cite{zyrianov2022learning-lidargen} apply diffusion models for realistic LiDAR scene generation. In this paper, we utilize its generation capability to study our proposed problem.
