
\section{Experiments}
\label{sec:exp}
\subsection{Setup}
\nbf{Datasets}
In this study, we primarily use three datasets: OPV2V~\citep{xu2022opv2v}, V2V4Real~\citep{xu2023v2v4real}, and WOD~\citep{sun2020waymo}. OPV2V is a simulation-based CAV dataset that includes over 70 diverse driving scenes and more than 11,000 frames. We utilize this dataset initially to validate our proposed research problem and later to guide the generation training on single-agent datasets. V2V4Real is a real-world CAV dataset with approximately 20,000 LiDAR frames, which we use to validate \ours in a real-world CAV context, treating it as a single-agent dataset (i.e., without using any labels to train \ours). Lastly, we adopt WOD to scale up \ours, one of the largest real-world single-agent datasets available.


\input{tables/01_opv2v_0_only}
\input{tables/22_v2v4real}

\begin{figure}
\centering
\includegraphics[trim={0cm, 0cm, 0cm, 0cm},clip,width=.95\linewidth]{figs/adapt.pdf}
\vspace{-4mm}
\caption{
\label{fig:adapt}
\small \textbf{Qualitative results on enhancement in the target domain.} Generated point cloud (\textcolor{customgreen}{green}) has better quality with the enhancement given ego (\textcolor{gray}{gray}) from Waymo (\cf \cref{sec:adapt}).}
\vspace{-5mm}
\end{figure}

\input{tables/11_v2v4real_ft}








\input{tables/31_dist_waymo} 
\input{tables/41_step}

\nbf{Evaluation metrics}
Following standard benchmarks~\citep{xu2022opv2v,xu2023v2v4real}, we report Average Precision (AP 0.5) across three distance ranges: 0-30 meters (short, denoted as s), 30-50 meters (medium, m), and 50-100 meters (long, l) and overall (all). 


\nbf{Implementations}
For \ours training, we voxelize point clouds within the range $[-51.2, 51.2]$ meters in x and y dimensions and 4 meters in z (range is dataset-dependent) to create 3D volumes, $\vx \in \sR^{512 \times 512 \times 20}$. This is then encoded to $\vx^f \in \sR^{64 \times 64 \times 8}$ with VQ-VAE~\citep{van2017neural}. Similarly, semantic information $\vy \in \sR^{512 \times 512 \times 1}$ is encoded to $\vy^f \in \sR^{64 \times 64 \times 3}$. We train VQ-VAE for $\vx$ 120 epochs and for $\vy$ 10 epochs, with a batch size of 6 per GPU. Next, we train LDM with a batch size of 48 per GPU for 200 epochs in the first stage and 80 epochs in the second stage. During the second stage, we freeze the U-Net from the first stage and fine-tune only the control module, following~\citep{mou2024t2i}. For adaptation, we initialize the weights of encoder-decoder for CAV data (fake) with the model trained on single-agent data (real), freeze its codebook, and fine-tune it with a discriminator comprising 4 convolutional layers. The model processes an equal amount of fake and real per batch (i.e., 3 samples each). All training is conducted on 4 NVIDIA H100 GPUs. For experiments on benchmarks, we follow the convention to report results with Pointpillars~\citep{lang2019pointpillars}. We train / fine-tune all models with $8$ P100, with batch size $4$ per GPU. For V2X-ViT, we use $4$ H100 due to resources required. 



\nbf{Inference of generation and post-processing}
We use DDIM scheduler~\citep{song2020denoising} for 200 steps. For post-processing, we apply modified Gumbel-Softmax~\citep{jang2016categorical} to the logits. Specifically, we define two thresholds, low and high. Scores below the low threshold are dropped, those above the high threshold are retained, and values in between are perturbed with Gaussian noise on logits. Hyperparameters are tuned on a small hold-out set. Finally, we convert the volume back to coordinates by using the centers of the occupied voxels.




\subsection{Do We Need Collaborative Perception?}
\label{sec:question}
To explore this question, we consistently report the performance without collaborative agents (denoted as ``ego’s gt only'' in \cref{tab:opv2v}, \cref{tab:real}, and \cref{tab:ft}). For example, in the V2V4Real dataset, a well-trained detector struggles with objects beyond 50 meters, achieving only $0.08$ AP at that range, though it performs well on nearby objects with an AP of $0.71$ (\cf first row of \cref{tab:ft}). simply combining the predictions from two under-trained detectors, we observe a notable boost in performance to $0.33$ (\cf ``scratch'' in late fusion, \cref{tab:real}), suggesting the significance of CAV.


\subsection{Point Clouds Generation on OPV2V}
\label{exp:opv2v}
\nbf{Setting}
To validate our research concept, we begin with the simulated dataset OPV2V~\citep{xu2022opv2v}. We split the original training set of $44$ scenes into two halves, using only the first $22$ scenes to train our generation model. For the remaining $22$ scenes, we generate point clouds for reference agents based on the ego agent’s point clouds and compare these results to ground-truth point clouds. We follow the benchmark to report performance on the validation set from Culver City.



\nbf{Baselines}
Following the literature~\citep{xu2022opv2v,xu2023v2v4real}, we adopt three common fusion algorithms for CAV: early, late, and intermediate fusion, which fuse input point clouds, extracted features, and predicted bounding boxes, respectively. For intermediate fusion, we adpot AttFuse~\citep{xu2022opv2v} and V2X-VIT~\citep{xu2022v2xvit}. 






\nbf{Comparison to using ground-truth LiDAR}
In \cref{tab:opv2v}, we show that training CAV with point clouds generated by \ours achieves performance comparable to that obtained with ground-truth (oracle) across all methods (\eg, $0.46~\vs~0.49$ with early, $0.50~\vs~0.55$ with late, and $0.77~\vs~0.72$ with AttFuse~\citep{xu2022opv2v}, etc.) This result highlights the quality of the generated point clouds. In the \supp, we further demonstrate that with additional labeled data (from the first half of the split used for training generation), the gap to oracle performance can be further minimized. 




\begin{figure*}[t]
\centering
\includegraphics[trim={0cm, 0cm, 0cm, 0cm},clip,width=.87\linewidth]{figs/ColWaymo.pdf}
\vspace{-3mm}
\caption{
\label{fig:colwaymo}
\small \textbf{Qualitative results on Collaborative Waymo.} The gray point clouds are from the original single-agent dataset and the green are generated by \ours conditioning on them.
}
\vspace{-3mm}
\end{figure*}

\subsection{Single-Agent to Collaborative Multi-Agent}
\label{exp:real}
\nbf{Setting} Building on our findings with simulated data (\cf \cref{exp:opv2v}), we validate our approach on the real-world dataset V2V4Real~\citep{xu2023v2v4real}, \emph{treating it as a single-agent dataset without access to reference agents’ point clouds}. Specifically, we use the model from \cref{exp:opv2v} to generate reference car's point clouds from the ego’s perspective and compare performance against ground-truth point clouds (oracle). Note that \emph{no V2V4Real data is used to train the generation}.

\nbf{Results on V2V4Real}
In \cref{tab:real}, we show the performance using generated reference point clouds is comparable to the oracle, supporting our idea of \emph{translating a single-agent dataset to multi-agent CAV}. Xu \etal previously explored OPV2V-to-V2V4Real adaptation, noting significant performance drops even with adaptation algorithms. Here, \ours offers a new solution to this challenge -- in a generative way, providing a fresh perspective on domain adaptation.
\begin{figure*}[ht]
\centering
\includegraphics[trim={0cm, 0cm, 0cm, 0cm},clip,width=.9\linewidth]{figs/qual.pdf}
\vspace{-3mm}
\caption{
\label{fig:qual}
\small \textbf{Qualitative results.} Our proposed \ours is capable of scene editing, by inputting the same point cloud but different object locations. We (b) remove and (c) add a car from (a) the original point cloud. Inspired by the idea of past traversals~\citep{you2022hindsight}, we apply completely different traffic conditions and generate (d), (e), and (f), to imagine driving through the same intersection.
}
\vspace{-5mm}
\end{figure*}


\subsection{ColWaymo: Scaling with Large-Scale Data}
\label{exp:colwaymo}

\nbf{Scaling CAV to the next level}
In \cref{exp:real}, we show the potential to create realistic reference's point clouds on real-world dataset V2V4Real~\citep{xu2023v2v4real}, through a simulation-guided training of \ours. Here, our goal is broader: to transform an existing large-scale single-agent dataset (\eg, WOD~\citep{sun2020waymo}) into a collaborative version. Using the pipeline described in \cref{sec:method} and illustrated in \cref{fig:pipeline}, we train the generative model on $17,400$ samples from the WOD training set. We then use its validation set to generate approximately $36,000$ collaborative samples by sampling reference locations from labeled vehicles. These generated samples serve to pre-train a model, which we subsequently fine-tune on V2V4Real~\citep{xu2023v2v4real}. For comparison, we also conduct pre-training on OPV2V~\citep{xu2022opv2v}.


\nbf{Results of fine-tuning on V2V4Real}
Following prior work~\citep{pan2023pre,xie2020pointcontrast,yin2022proposalcontrast,boulch2023also}, we perform pre-training followed by fine-tuning on limited labeled data to validate the potential of \ours for scaling up CAV development. As shown in \cref{tab:ft}, pre-training on our generated data yields significantly better performance than training from scratch. Additionally, compared to pre-training on the simulated dataset OPV2V, ColWaymo consistently achieves higher performance, highlighting the realism of the generated point clouds. These results demonstrate that \ours effectively transforms an ego-only dataset into a collaborative version, and potentially reducing the need for extensive CAV data collection.

\nbf{Generation quality by proposed adaptation} To assess the quality of generated data, we use two common metrics: Jensen-Shannon Divergence (JSD) and Maximum Mean Discrepancy (MMD). As shown in \cref{tab:dist}, our adaptation method improves JSD from $0.26$ to $0.16$ and MMD from $4.61$ to $1.17$ compared to the baseline without adaptation. Additionally, \cref{fig:adapt} visually illustrates this improvement.

\subsection{Qualitative Results}
We illustrate the proposed problem in \cref{fig:teaser}, which highlights the benefits of collaboration by allowing an agent to ``see'' beyond the limitations of its own sensors. \ours generates realistic point clouds from various viewpoints, enabling the agents to perceive previously occluded objects while preserving the overall scene semantics. \cref{fig:colwaymo} shows our ColWaymo, and \cref{fig:qual} further demonstrates the model’s capability for scene editing.

