\section{Introduction}
\label{sec:intro}

Seeing the world from an ego-centric perspective, a self-driving car risks being ``narrow-sighted,'' limiting its ability to respond appropriately in dynamic driving environments. For instance, it should slow down or honk if a pedestrian is about to cross the road or if another vehicle is merging into its lane. However, these actions depend on the car’s ability to detect traffic participants, which may be occluded by a large bus or a building at a sharp intersection. %
This occlusion problem can hardly be addressed by mounting a few more sensors on the car. We argue that a self-driving car must move beyond its ego-centric perspective.

One intuitive way is to collaborate with nearby ``agents'' that observe the scene from different angles, such as other sensor-equipped cars or static devices like roadside units (RSUs). When trained together to address GPS errors and synchronization delays~\citep{yoo2024learning}, collaborative perception has been shown to significantly improve the accuracy of each agent's perception, particularly in detecting occluded or distant objects~\citep{xu2022opv2v,chen2019cooper,rawashdeh2018collaborative, chen2019fcooper,wang2020v2vnet,liu2020when2com,li2021learning,hu2022where2comm,xu2022v2xvit,xu2023v2v4real,xiang2024v2x}. 
\emph{However, collecting training data for collaborative perception is never easy.} 
Unlike single-agent data, which can be collected by simply driving a car on the road, collaborative data requires the simultaneous presence of multiple agents in the same driving scene. For dynamic agents like vehicles, precise coordination is needed to ensure they are within communication range. These challenges limit existing works in scale and the number of agents (typically just two). While one may leverage simulated data by game engines~\citep{Dosovitskiy17carla}, they often fail to capture the diversity of real-world scenes. %
We thus ask:
\vspace{-2mm}
\begin{center}
    \color{blue}
    \emph{
    Can we obtain realistic data for learning collaborative perception with much less effort, ideally as easily as single-agent data?
    }
\end{center}
\vspace{-2mm}
Specifically, given the vast amount of single-agent LiDAR data already collected across various driving environments, is it possible to convert each of them into collaborative perception data by generating additional point clouds from alternative reference viewpoints within the same scene?

At first glance, \textbf{this question may seem overly ambitious for three reasons.} First, for collaborative perception to be effective, the generated point cloud must provide information that the ego-car’s point cloud cannot capture, such as occluded surfaces invisible to the ego-car. This creates a chicken-and-egg problem: if the ego-car’s point cloud is all we initially know about the scene, how can we deduce any additional information?
Second, for training purposes, the generated data must be realistic. It should replicate the constraints and data patterns of a real sensor as if it were positioned at the reference viewpoint, producing no points in occluded areas and fewer points in distant areas. 
Last but not least, in regions commonly perceivable from both the reference and ego-car's viewpoints, the generated data should align in layouts and semantics with the ego-car's data. On the surface, this may seem as simple as copying the ego-car's data, but doing so would violate the second requirement. In essence, the generated data should resemble a point cloud seen from the reference viewpoint, not the ego-car.


That said, after a deeper look at the question, \textbf{we argue that it is achievable with three key insights.} First, given semantic information around the ego-car for conditioning, existing research has shown promising progress in generating realistic point clouds~\citep{ran2024towards, hu2025rangeldm, wu2024text2lidar, nakashima2024lidar-r2dm, zyrianov2024lidardm, xiong2023ultralidar, zyrianov2022learning-lidargen}. This is especially encouraging, as most existing single-agent datasets provide 3D object labels, and they can be translated to obtain semantic information centered around the reference position. This further implies that we can generate occluded object surfaces as seen from the ego-car if they are visible from the reference viewpoint. Second, semantic information is easily editable: even if the translated object map has noticeable empty areas due to limited ego-car perception, object boxes can be manually added to make the map appear more realistic from the reference viewpoint. Lastly, in commonly perceivable regions where the generated data needs to meet two physical constraints, we could leverage simulators based on computational graphics and optics. Specifically, we can use simulated data from the two views to train a conditioned generative model that maps one viewpoint to the other.

Building upon these insights, we propose \textbf{Transfer Your Perspective (\ours)}, a novel research problem and the very first solution for \emph{generating a realistic point cloud from any viewpoint in a scene, given real ego-car's point cloud and semantic labels as conditions.} \ours assumes access to \textbf{1)} a simulated collaborative driving dataset with multiple agents perceiving the same scenes from different viewpoints and \textbf{2)} a real single-agent driving dataset; both are labeled.
Our solution involves a \textbf{conditioned latent diffusion model}~\citep{rombach2022high} and a \textbf{dedicated two-stage training process}. In the first stage, we consider a single-agent scenario and train the model using real data conditioning only on object locations. This equips the model with the ability to generate diverse and realistic scenes. We denote the learned model by $P(\vx|\vy)$, where $\vy$ stands for the semantic condition and $\vx$ for the point cloud.  
In the second stage, we incorporate the simulated data to learn \emph{how to ground generation on data from another agent's viewpoint} so that the model can produce a semantically consistent reference point cloud given the ego-car's data. 
We learn a lightweight conditioning module to turn $P(\vx|\vy)$ into $P(\vx_r|\vx_e, \vy_r)$, where $e$ and $r$ indicate ego and reference views, respectively. We note that $\vx_e$ was pre-translated to center around the reference position. 

One challenge in \ours is the domain gap between real and simulated data. Due to differences in sensor configurations and placements, data collection environments, and the sim-to-real gap, the two sets of point clouds inevitably exhibit discrepant distributions, patterns, and densities. To address this, \emph{we insert a domain adaptation step between the two training stages}. We train a separate encoder-decoder for the simulated data while enforcing a constraint to make the encoded features of simulated and real data indistinguishable \citep{tzeng2017adversarial}.
This step allows us to learn $P(\vx_r|\vx_e, \vy_r)$ in a space with reduced domain discrepancy during the second stage.

Once trained, we pair $P(\vx_r|\vx_e, \vy_r)$ with the real data's encoder-decoder to generate real-style point clouds grounding on real ego-car's data, so that \emph{we can develop collaborative perception algorithms without real collaborative data.}
More specifically, given a real ego-car's perception $\vx_e$ and label $\vy_e$, we first translate them to center around the reference position, followed by optionally injecting object labels into $\vy_e$ to make it realistic from the reference viewpoint. 


We extensively validate \ours on multiple datasets, all in an \emph{offline} setting. Empirical results demonstrate \ours's effectiveness in generating high-quality reference data to aid in the development of collaborative perception. In particular, we show that a conditioned diffusion model trained solely on simulated data (\eg, OPV2V~\citep{xu2022opv2v}) can already turn a real single-agent dataset into a real-alike collaborative one. As such, one can train collaborative perception algorithms for real test data without real training data. We further generate the ``ColWaymo'' dataset by training \ours on real single-agent Waymo data~\citep{sun2020waymo} and simulated OPV2V data~\citep{xu2022opv2v}, thus turning the former into a collaborative one. Collaborative perception backbones pre-trained on the large-scale, semi-synthetic ColWaymo data demonstrate a remarkable transferability. They notably improve collaborative perception developed in the downstream tasks (\eg, V2V4Real~\citep{xu2023v2v4real}), even in a many-shot setting.  

In sum, our key contributions are:
\begin{itemize}
\item We propose a new research direction to aid in the development of collaborative perception, \emph{generating real-looking sensory data from potentially any viewpoint in a real driving scene}, capable of turning a single-agent dataset into a collaborative one. This has the potential to scale up collaborative autonomous driving (CAV).
\item We present \ours, a two-stage training recipe with domain adaptation in between to learn the generative model using simulated collaborative data and real single-agent data.
\item Extensive experiments demonstrate \ours's effectiveness in aiding in collaborative perception across various scenarios.
\end{itemize}
