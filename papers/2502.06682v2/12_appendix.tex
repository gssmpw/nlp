In this \supp, we provide more details and experiment results in addition to the main paper:
\begin{itemize}
    \item \cref{sup:related}: provides more related works, including 3D scene generation, diffusion models, domain adaptation, and NeRF. 
    \item \cref{sup:datasets}: summarizes existing datasets for CAV.
    \item \cref{sup:opv2v}: conducts experiments on generated point clouds with additional ground-truths.
    \item \cref{sup:stat}: shows the statistical results of the experiments.
    \item \cref{sup:stage}: further demonstrates the generation quality of two-stage training with experiments on CAV setting.
    \item \cref{sup:qual}: shows more qualitative results. 
    \item \cref{sup:future}: concludes future directions.
\end{itemize}

\section{More Related Work}
\label{sup:related}

\nbf{3D Scene Generation}
As generative models have gained traction, recent research has increasingly focused on applying these methods to 3D point cloud data. Initially, the synthesis of point clouds was primarily limited to fixed-size data, such as single objects~\citep{achlioptas2018learning, fan2017point, shu20193d, valsesia2018learning}. However, recent advancements have extended beyond single-object generation to encompass entire 3D scenes. Early pioneering works in this domain employed generative adversarial networks (GANs)~\citep{goodfellow2014generative}, demonstrating the feasibility of 3D scene generation, albeit with significant challenges in quality and realism.

More recent efforts have aimed at improving the quality and realism of 3D scene generation. For instance, LiDARGen~\citep{zyrianov2022learning-lidargen} and UltraLiDAR~\citep{xiong2023ultralidar} leverage diffusion models to enhance scene quality, incorporating realistic effects like ray drop. However, these methods struggle to generate scenes based on user-defined conditions, such as specific locations or diverse traffic scenarios.

To address these limitations, works like LidarDM~\citep{zyrianov2024lidardm} have introduced more controllable scene generation using consecutive video frames and user-defined conditions. Similarly, Text2LiDAR~\citep{wu2024text2lidar} employs text prompts for conditioning, enabling diverse scene generation tailored to user inputs.

Other advancements prioritize flexibility, efficiency, and quality. R2DM~\citep{nakashima2024lidar-r2dm} proposes efficient training pipelines and a LiDAR completion framework that enhances scene quality. Meanwhile, RangeLDM~\citep{hu2025rangeldm} combines latent diffusion models with improved speed and quality for scene generation. LiDM~\citep{ran2024towards} synthesizes recent advancements to achieve state-of-the-art results in realistic 3D scene generation, balancing quality, realism, and user control with multiple conditioning inputs.

In this paper, we propose a novel research problem in the context of CAV: generating realistic point clouds for reference agents. This direction offers significant potential for the research community, addressing the critical challenge of data collection in CAV, which is inherently difficult and costly. Unlike existing works focused on ego-centric scene generation, our approach shifts the perspective to collaborative scenarios. We view this as a complementary research direction and are open to enhancing our proposed solution by adopting advancements in efficiency and controllability from ongoing work in 3D scene generation. 


\nbf{Diffusion-based Generative Models}
Diffusion models (DMs)\citep{sohl2015deep} have made significant advancements in various domains, particularly in generating high-quality images. Initially, DMs were applied directly to raw pixel data, achieving remarkable results\citep{dhariwal2021diffusion, ho2020denoising, kingma2021variational}. To improve efficiency, Latent Diffusion Models (LDMs)~\citep{rombach2022high} operate in a compressed latent space, preserving visual quality while significantly reducing computational requirements. These approaches have found widespread application across diverse tasks, including 3D scene generation, as discussed in the previous section.


\nbf{Controllable Diffusion Models} Many existing works focus on controlling generative processes through text prompts, particularly in text-to-image (T2I) synthesis~\citep{nichol2022glide,ramesh2021zero,ding2021cogview,gafni2022make,rombach2022high,saharia2022photorealistic}. The predominant strategy involves performing denoising in feature space while integrating text conditions into the denoising process via a cross-attention mechanism. While these approaches achieve impressive synthesis quality, text prompts often lack reliable structural guidance for precise generation.

To address this limitation, several works improve structural control during generation. For instance, \citep{wang2022pretraining,hertzprompt,feng2023training,balaji2022ediff} explore methods to enhance structure guidance in text-driven synthesis. Meanwhile, works like \citep{mou2024t2i,li2025controlnet,zhang2023adding} introduce additional trainable modules built upon pre-trained T2I models to provide more targeted and controllable outputs.

In this paper, we leverage the approach proposed by \citep{mou2024t2i} during the second stage of our framework. This stage grounds the generation process, ensuring that the outputs align with given semantic cues.

\input{tables/51_cav_datasets}

\nbf{Domain Adaptation}
Unsupervised domain adaptation (UDA) has been extensively studied. A common approach for domain adaptation is to learn domain-invariant embeddings by minimizing the distributional differences between source and target domains~\citep{long2015learning,sun2016deep,tzeng2015simultaneous,tzeng2014deep}. More recently, adversarial training methods have gained popularity for bridging domain gaps effectively~\citep{ajakan2014domain,ganin2015unsupervised,ganin2016domain,tzeng2017adversarial,isola2017image,lim2017geometric}. These methods leverage a discriminator to distinguish between domains, encouraging the generator to produce features or outputs that are indistinguishable across domains.

In this paper, we adopt a discriminator inspired by adversarial training-based approaches to reduce the domain gap in embedded features between multi-agent and single-agent datasets. This step ensures that the domain-adapted embeddings provide robust guidance for generation training on single-agent datasets in the second stage of our proposed method.


\nbf{Neural Radiance Fields}
Neural Radiance Fields (NeRF) have significantly advanced 2D novel view synthesis (NVS) by encoding scenes as implicit volumetric functions optimized through ray-marching~\citep{mildenhall2021nerf}. While effective in generating high-quality novel views, NeRF requires dense multi-view images and suffers from high computational costs~\citep{barron2021mip, barron2022mip}. Extensions such as Mip-NeRF~\citep{barron2021mip} improve aliasing, and depth-supervised variants reduce multi-view dependency~\citep{deng2022depth, roessle2022dense}.
In 3D LiDAR-based NVS, NeRF-inspired methods like LiDAR-NeRF~\citep{tao2024lidarnerf}, Neural LiDAR Fields~\citep{huang2023neural}, and NeRF-LiDAR~\citep{zhang2024nerf} adapt implicit representations to synthesize novel LiDAR views. These approaches enhance reconstruction but struggle with sparse data, large-scale outdoor scenes, and dynamic objects, as ray-marching is inefficient for LiDARâ€™s discrete nature~\citep{tao2024lidarnerf}.

Our work shares similarities with NeRF-based LiDAR generation~\citep{huang2023neural, tao2024lidarnerf, zhang2024nerf, zheng2024lidar4d} as we also synthesize LiDAR point clouds. However, unlike these methods focused on scene reconstruction from multiple samples (\eg, views, time frames), \ours generates collaborative driving data from a single frame. Instead of modeling implicit densities, \ours directly generates LiDAR point clouds with spatial consistency even at a long distance, enabling single-agent datasets to be converted into multi-agent data for autonomous driving.

\input{tables/01_opv2v_no_bold}

\section{Existing Datasets for CAV}
\label{sup:datasets}

We summarize existing CAV datasets in \cref{tab:datasets}, highlighting the current state of CAV research. At the time of this paper, no real-world dataset includes both dynamic and static agents and supports more than two agents, primarily due to the challenges of real-world data collection. Additionally, some datasets are limited to vehicle-only labels or a single sensor modality. These limitations drive our work, pushing boundaries and introducing a new research direction.

As shown in \cref{fig:teaser} and \cref{fig:teaser2}, \ours demonstrates strong potential to scale up the number of agents---both static and dynamic---through the proposed generation framework. Empirical results in \cref{tab:ft} further validate that the generated point clouds can enhance CAV development.

\begin{figure}[t]
\centering
\includegraphics[trim={0cm, 0cm, 0cm, 0cm},clip,width=1\linewidth]{figs/gt.pdf}
\caption{
\label{fig:gt}
\small \textbf{Visualization with validation data of OPV2V.} The generated point clouds are well-aligned with the ground-truth bounding boxes and follow the physics (\eg, occluded areas).
}
\end{figure}

\section{Results on OPV2V}
\label{sup:opv2v}

In \cref{tab:opv2v} of the main paper, we validate the quality of the generated point clouds by replacing the ground-truth point clouds of the reference agents with the generated ones on the OPV2V dataset~\citep{xu2022opv2v}. In this \supp, we investigate the impact of having access to a limited amount of labeled data.

\nbf{Setting} As outlined in \cref{exp:opv2v} of the main paper, the original training set of 44 scenes was split into two halves: the first 22 scenes were used to train the generation model, while the remaining 22 scenes were used for inference to generate point clouds of reference agents. Here, we further utilize the first split as a source of limited labeled data.  

\nbf{Results} 
Firstly, the results in \cref{tab:opv2v_full} exhibit a consistent trend with \cref{tab:opv2v} in the main paper, demonstrating that using generated point clouds achieves results comparable to those obtained with ground-truth point clouds (oracle).
Secondly, the results in \cref{tab:opv2v_full} highlight that incorporating additional limited labeled data further reduces the gap between using ground-truth and generated point clouds. For example, in Early Fusion with 22 additional labeled scenes, the performance with generated point clouds matches that of ground-truth point clouds (\ie, both achieve $0.78$).

\section{Statistical Results of Experiments}
\label{sup:stat}
\input{tables/02_opv2v_mean_and_std}

In \cref{tab:opv2v} of the main paper, we validate the quality of the generated point clouds by replacing the ground-truth point clouds of the reference agents with the generated ones on the OPV2V dataset~\citep{xu2022opv2v}. In this \supp, we extend this evaluation by conducting two additional runs (\ie, three in total) and present the statistical results in \cref{tab:opv2v_mean}. The results consistently demonstrate that using the generated point clouds achieves performance comparable to that of the ground-truth (oracle) point clouds, highlighting the robustness, consistency, and reproducibility of our approach. 


\section{Single-Stage \vs Multi-Stage Training}
\label{sup:stage}
In \cref{tab:step} of the main paper, we compare the quality of generated point clouds between single-stage and the proposed multi-stage training by evaluating the distance between generated and ground-truth samples. In this \supp, we extend this analysis by conducting CAV training using point clouds generated by the single-stage training model. 

The results in \cref{tab:opv2v_mean} demonstrate that the performance of single-stage training consistently lags behind the proposed multi-stage approach, particularly in scenarios that directly rely on point clouds (\ie, early fusion). Furthermore, the multi-stage method remains essential for translating single-agent datasets into collaborative versions, underscoring its critical role in the proposed framework (\cf \cref{sec:adapt,exp:colwaymo} in the main paper).



\section{More Qualitative Results}
\label{sup:qual}
We present additional examples of \ours in \cref{fig:teaser2}. These examples demonstrate the ability to designate \emph{any location} as a reference, effectively simulating both static and dynamic agents communicating with the ego vehicle. This flexibility overcomes the limitations of existing real-world CAV datasets, which are often constrained by specific communication types (\ie, vehicle-to-vehicle or vehicle-to-infrastructure) and a limited number of agents. Furthermore, the generated point clouds are both realistic and semantically consistent with the ego agent's perception.

In \cref{fig:colwaymo2}, we provide more examples of the collaborative version of the Waymo dataset (\ie, ColWaymo), which was utilized to pre-train the detector for CAV tasks, as discussed in \cref{exp:colwaymo,tab:ft} of the main paper. These examples further highlight the high quality of the point clouds generated by \ours and underscore its potential to significantly scale up datasets for CAV research. 



\begin{figure*}
\centering
\includegraphics[trim={0cm, 0cm, 0cm, 0cm},clip,width=.9\linewidth]{figs/teasure_v2.pdf}

\caption{
\label{fig:teaser2}
\small \textbf{Illustration of the proposed problem and solution, Transfer Your Perspective (\ours).} (a) A given sensory data captured by the {\color{red} ego-car (red triangle)}. (b) A generated sensory data by \ours, seeing from the viewpoint of \textcolor{customgreen}{another vehicle (green triangle)} in the same scene. (c) A generated sensory data, seeing from an imaginary {\color{blue} static agent like roadside units (blue icon)}. (d) Putting all the sensory data together, given or generated, \ours enables the development of collaborative perception with little or no real collaborative driving data.}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[trim={0cm, 0cm, 0cm, 0cm},clip,width=.9\linewidth]{figs/ColWaymo_v2.pdf}
\caption{
\label{fig:colwaymo2}
\small \textbf{Qualitative results on Collaborative Waymo.} The gray point clouds are from the original single-agent dataset and the green are generated by \ours conditioning on them.}

\end{figure*}

\section{Future Work}
\label{sup:future}

This paper follows the existing benchmark~\citep{xu2022opv2v, xu2023v2v4real} to focus on vehicle-like objects. However, \ours is scalable and can extend to broader object categories when semantic information is available (\cf \cref{sec:setup} and \cref{sec:input}). We are also open to exploring cross-modality generation in future research.
