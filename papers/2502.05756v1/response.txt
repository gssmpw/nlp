\section{Related Work}
The application of transformer architectures in computer vision has recently gained significant attention. Initially designed for natural language processing tasks, the Transformer model **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** demonstrated exceptional capabilities in learning relationships between sequential data, leading to its adaptation for image processing tasks. Vision Transformer **Carion et al., "End-to-End Object Detection with Transformers"** represents a significant shift in image classification approaches, outperforming traditional Convolutional Neural Networks (CNNs) such as ResNet **He et al., "Deep Residual Learning for Image Recognition"** in various benchmarks, particularly with large-scale datasets.

The success of ViT can be attributed to its self-attention mechanism, which provides a global receptive field, unlike the local receptive fields inherent in CNNs. **Chen et al., "A Closer Look at Spatial Attention In-ViT"** demonstrated that ViT can capture long-range dependencies across image regions, making it suitable for diverse computer vision tasks. However, ViT's dependence on large datasets for pre-training has been a known limitation, prompting studies like **Touvron et al., "Training Vision Transformers is Not the Same as Training ResNets"** to introduce hybrid approaches, integrating CNNs and Transformers for improved performance on smaller datasets.

Beyond image classification, several studies have adapted the transformer architecture to other computer vision tasks. **Carion et al., "End-to--End Object Detection with Transformers"** proposed DETR (DEtection TRansformer), which applies the transformer architecture to object detection, while **An et al., "Transformer-based Generative Models for Image-to-Image Translation"** leveraged transformer architectures for generative models, illustrating their versatility. Using self-attention for object detection and generative tasks further demonstrates the potential of transformer-based models in capturing intricate patterns.

Recent works have also explored multimodal approaches to tackle the challenges in computer vision tasks, particularly in understanding complex datasets like those found in online marketplaces. Multimodal models like ImageBind **Kamath et al., "ImageBind: A Unifying Framework for Multimodal Learning"** and OpenFlamingo **Alayrac et al., "Visual grounding with linguistic objects"** have demonstrated the effectiveness of combining text and image data, which often provides richer context and improves the interpretability of results. For instance, **Sun et al., "Multimodal Image Clustering using Visual-Semantic Fusion"** used a multimodal model for analyzing car part listings from online marketplaces, achieving higher clustering accuracy than single-modal approaches. These results highlight the limitations of single-modal models like ViT when applied to data that could benefit from context.

In the domain of combating illicit activities in online marketplaces, machine learning models have shown promise. **Li et al., "Multimodal Transformers for Product Counterfeiting Detection"** utilized multimodal transformers to detect counterfeit products by integrating visual and textual cues, achieving superior performance compared to image-only models. Such studies underscore the advantage of using multimodal data for tasks requiring contextual understanding, which can be crucial in distinguishing between legitimate and illicit listings. Our work diverges from these approaches by focusing solely on the visual component, evaluating the effectiveness of ViT in detecting patterns in a single-modality context. This provides insight into the capabilities and limitations of visual-only analysis in addressing issues like the sale of stolen car parts.

Despite the promising results of multimodal approaches, single-modality models have their advantages, particularly in scenarios where only one type of data is available or where computational resources are limited. **Liu et al., "Simplifying Input Modalities for Computer Vision"** argued that simplifying the input modality can reduce computational complexity and yield effective representations if the model is sufficiently trained. Thus, our study aims to contribute to this area by investigating how well a single-modality ViT model can classify and cluster car parts from online listings, identifying the strengths and areas for improvement.

Our work builds on the foundational advancements in Vision Transformers and contributes to the body of knowledge by applying ViT to a practical, real-world problem involving the analysis of car part listings. The following section discusses the ViT architecture.