\section{Related Work}
The application of transformer architectures in computer vision has recently gained significant attention. Initially designed for natural language processing tasks, the Transformer model~\cite{vaswani2023attentionneed} demonstrated exceptional capabilities in learning relationships between sequential data, leading to its adaptation for image processing tasks. Vision Transformer~\cite{dosovitskiy2021imageworth16x16words} represents a significant shift in image classification approaches, outperforming traditional Convolutional Neural Networks (CNNs) such as ResNet~\cite{he2016deep} in various benchmarks, particularly with large-scale datasets.

The success of ViT can be attributed to its self-attention mechanism, which provides a global receptive field, unlike the local receptive fields inherent in CNNs. \citet{dosovitskiy2021imageworth16x16words} demonstrated that ViT can capture long-range dependencies across image regions, making it suitable for diverse computer vision tasks. However, ViT's dependence on large datasets for pre-training has been a known limitation, prompting studies like~\citet{touvron2021training} to introduce hybrid approaches, integrating CNNs and Transformers for improved performance on smaller datasets.

Beyond image classification, several studies have adapted the transformer architecture to other computer vision tasks. \citet{carion2020endtoendobjectdetectiontransformers} proposed DETR (DEtection TRansformer), which applies the transformer architecture to object detection, while \citet{radford2016unsupervisedrepresentationlearningdeep} leveraged transformer architectures for generative models, illustrating their versatility. Using self-attention for object detection and generative tasks further demonstrates the potential of transformer-based models in capturing intricate patterns.

Recent works have also explored multimodal approaches to tackle the challenges in computer vision tasks, particularly in understanding complex datasets like those found in online marketplaces. Multimodal models like ImageBind~\cite{girdhar2023imagebind} and OpenFlamingo~\cite{alayrac2023flamingo} have demonstrated the effectiveness of combining text and image data, which often provides richer context and improves the interpretability of results. For instance, ~\citet{hamara2024latentenginemanifoldsanalyzing} used a multimodal model for analyzing car part listings from online marketplaces, achieving higher clustering accuracy than single-modal approaches. These results highlight the limitations of single-modal models like ViT when applied to data that could benefit from context.

In the domain of combating illicit activities in online marketplaces, machine learning models have shown promise. \citet{rashid2024aisafetypracticeenhancing} utilized multimodal transformers to detect counterfeit products by integrating visual and textual cues, achieving superior performance compared to image-only models. Such studies underscore the advantage of using multimodal data for tasks requiring contextual understanding, which can be crucial in distinguishing between legitimate and illicit listings. Our work diverges from these approaches by focusing solely on the visual component, evaluating the effectiveness of ViT in detecting patterns in a single-modality context. This provides insight into the capabilities and limitations of visual-only analysis in addressing issues like the sale of stolen car parts.

Despite the promising results of multimodal approaches, single-modality models have their advantages, particularly in scenarios where only one type of data is available or where computational resources are limited. \citet{wu2020visualtransformerstokenbasedimage} argued that simplifying the input modality can reduce computational complexity and yield effective representations if the model is sufficiently trained. Thus, our study aims to contribute to this area by investigating how well a single-modality ViT model can classify and cluster car parts from online listings, identifying the strengths and areas for improvement.

Our work builds on the foundational advancements in Vision Transformers and contributes to the body of knowledge by applying ViT to a practical, real-world problem involving the analysis of car part listings. The following section discusses the ViT architecture.