\section{Method}
\label{sec:method}

The overall framework is shown in Fig.~\ref{fig: framework}. First, we discuss the framework of LightFC-X (T for thermal, D for depth, E for event, and S for sonar). Then, we present the proposed ECAM module  and the STAM module. Finally, we introduce the prediction head and the training loss.


\subsection{Lightweight MultiModal Tracking framework}
\label{method rgbs}



We select LightFC \cite{lightfc} as our baseline to better balance parameters, performance, FLOPs, and speed. The modal of LightFC-S is slightly different due to the spatial misalignment of RGB-Sonar tracking, which we will discuss it separately. The inputs of our method are the RGB and X modalities (Thermal, Depth, Event, and Sonar) template images represented as  $z_{r}, z_{x} \in R^{H_{z}\times W_{z}\times 3}$ and the search area images are represented as $x_{r}, x_{x} \in R^{H_{x}\times W_{x}\times 3}$. We first use the backbone~\cite{tinyvit} to extract features and represent them as $Z_{r}, Z_{x} \in R^{C \times H_{z}^{f}\times W_{z}^{f}}$ and $X_{r}, X_{x} \in R^{C \times H_{x}^{f}\times W_{x}^{f}}$, where $C=160$ is the channels. $H_{z}^{f}=H_{z}/s, H_{x}^{f}=H_{x}/s$, where $s=16$ is the stride. During tracking, we use the proposed STAM module to aggregate the spatiotemporal feature of the target. The process is represented as $Z_{rt}=\phi(Z_{r}^{1}, Z_{r}^{i})$. Similarly, we obtain $Z_{xt}=\phi(Z_{x}^{1}, Z_{x}^{i})$.

First, we model the relationship between the template and the search area in different modalities, represented as:
\begin{equation}
A_{r}=Z_{rt}^{T}X_{r}, A_{x}=Z_{xt}^{T}X_{x}
\end{equation}
Then, we follow the framework of LightFC~\cite{lightfc} to refine the features. The refined features are represented as $X_{r}^{ts}, X_{x}^{ts} \in R^{C_{a} \times H_{x}^{f}\times W_{x}^{f}}$, where $C_{a}=96$.

The proposed efficient cross-attention module (ECAM) is represented as $X_{rx}^{st}=f(X_{r}^{ts}, X_{x}^{ts})$, where $X_{rx}^{st}$ denotes the cross-modal integrated feature. Then, we concatenate $X_{rx}^{st}$, $X_{r}$, and $X_{x}$ to supplement the original semantic lost during the fusion, represented as:
\begin{equation}
X_{rx}^{fusion}=[X_{rx}^{st}; X_{r}; X_{x}] \in R^{(2C+C_{a})\times H_{x}^{f}\times W_{x}^{f}}
\end{equation}
where $X_{fusion}$ is fed into the prediction head.

In RGB-Sonar (RGB-S) tracking, since the target is spatial misaligned in the RGB image and the sonar image~\cite{rgbs50}, we need to adjust the framework. The ECAM module for RGB-S tracking is denoted as $X_{r}^{xst}, X_{x}^{rst}=f^{'}(X_{r}^{ts}, X_{x}^{ts})$. Similarly, the feature concatenation is represented as:
\begin{equation}
\begin{split}
X_{i}^{fusion}=[X_{i}^{ist}; X_{i}] \in R^{C+C_{a}}, i\in(r,x)
\end{split}
\end{equation}
where $X_{i}^{fusion}, i\in(r,x)$ are input into the RGB prediction head and the sonar prediction head to predict the state of the target in two modalities, respectively.


\subsection{Efficient Cross-Attention Module}

As shown in Fig.~\ref{fig:ecam}, our proposed efficient cross-attention module (ECAM) consists of a lightweight cross-attention layer and a joint feature encoding module. We insert the ECAM module between the template-search area feature interaction module and the prediction head.
\input{figs/ecam}
\input{figs/stam}

\textbf{Lightweight Cross-Modal Integration.} 
We first convert the features $X_{r}^{ts}, X_{x}^{ts}$ into tokens $X_{r}', X_{x}'\in R^{N\times C_{a}}$, where $N=H_{x}^{f}W_{x}^{f}$. Then, we use an lightweight spatial cross-attention directly to the features, represented as:
\begin{equation}\small
\begin{split}
X_{r}^{crs} &=\text{Softmax}(\frac{Q_{r}K_{x}^{T}}{\sqrt{C}})V_{r}=\text{Softmax}(\frac{X_{r}'X_{x}'^{T}}{\sqrt{C}})X_{r}' \\
X_{x}^{crs} &=\text{Softmax}(\frac{Q_{x}K_{r}^{T}}{\sqrt{C}})V_{x}=\text{Softmax}(\frac{X_{x}'X_{r}'^{T}}{\sqrt{C}})X_{x}'
\end{split}
\end{equation}
where $Q, K, V$ are the Query, Key, and Value matrices. $Q_{x}K_{r}^{T}\in R^{1\times N\times N}$.

We use residual concatenation to complement the original semantic information of the target, represented as:
\begin{equation}
\begin{split}
X_{r}^{cross} = X_{r}'+X_{r}^{crs}, X_{x}^{cross} = X_{x}'+X_{x}^{crs}
\end{split}
\end{equation}
We then perform channel fusion on the features and use residual concatenation to supplement the original semantic, represented as:
\begin{equation}
\begin{split}
X_{i}^{eca} = \text{LN}^{\text{i}} (X_{i}'+\text{Conv}_{1\times 1}^{i}({X_{i}^{res}})) 
\end{split}
\end{equation}
where $i\in\{r,x\}$. Different superscripts indicate that they are independent layers.


\textbf{Joint Feature Encoding.} 
We use a joint feature encoding module to refine the features. First, we concatenate the multimodal features $X_{r}^{eca}$ and $X_{x}^{eca}$ to achieve joint encoding of the RGB modality and the X modality features, represented as $X_{rx}^{cat}=[X_{r}^{eca}; X_{x}^{eca}] \in R^{N\times 2C}$.

Then we downsample the features using a 1x1 convolutional layer, represented as:
\begin{equation}
X_{rx}^{down}=\text{Conv}_{1\times 1}^{down}(X_{rx}^{cat}) \in R^{N\times C}
\end{equation}
To improve the spatial representation of the target, we further perform a multi-level spatial aggregation as:
\begin{equation}
X_{rx}^{space}= \sum_{k\in (3,5,7)}\text{DWConv}_{k\times k}(X_{rx}^{space})
\end{equation}
To improve the nonlinearity of the model, we introduce the ReLU function to integrate the channel features as:
\begin{equation}
X_{rx}^{act}= \text{BN}(\text{Conv}_{1\times 1}(\text{ReLU}(X_{rx}^{space}))
\end{equation}
Where $\text{Conv}_{1\times 1}$ maps the feature from $R^{C}$ to $R^{C}$, $\text{BN}$ is the BatchNorm layer.

To complement the semantic information lost during encoding, we introduce downsampling and residual concatenation, represented as:
\begin{equation}
X_{rx}^{output}= \text{BN}(X_{rx}^{act}+\text{Conv}_{1\times 1}'(X_{rx}^{cat}))
\end{equation}
where $X_{rx}^{output}$ is the output feature of the proposed ECAM module. $\text{Conv}_{1\times 1}'$ is an unbiased convolutional layer.

For RGB-S tracking, we use two joint feature encoding modules to learn the refinement of the RGB feature and the sonar feature, respectively. Accordingly, the process is:
\begin{equation}
\begin{split}
X_{r}^{out}= \text{JFE}_{r}(X_{r}^{eca}), X_{x}^{out}=\text{JFE}_{x}(X_{x}^{eca})
\end{split}
\end{equation}
where $\text{JFE}$ represents the joint feature encoding module.

\subsection{Spatiotemporal Template Aggregation Module}
\label{section 3.3}
Our proposed spatiotemporal template aggregation module (STAM) includes a lightweight cross-attention layer, a feature integration module, and a linear transformation layer, as shown in Fig.~\ref{fig: stam}. 

\textbf{Spatiotemporal Feature Interaction.} The initial frame target template, considered as a fixed template is denoted as $Z_{r}^{1}$, and during tracking, the dynamic appearance template is denoted as $Z_{r}^{i}$.
\begin{equation}
    Z_{rt}^{1}, Z_{rt}^{i} = \text{CrossAttn}(Z_{r}^{1}, Z_{r}^{i})
\end{equation}
where $\text{CrossAttn}$ is the lightweight cross-attention above.

\textbf{Feature Refinement.} We then integrate the two template features using two decoupled spatial and channel enhancement layers, respectively. Since the feature processing is identical for both templates, we only show one of them. First, we model the local spatial relationships as:
\begin{equation}
    Z_{rt}^{is} = \text{DWConv}_{33}(Z_{rt}^{i})+Z_{rt}^{i}
\end{equation}
Then, we perform nonlinear channel integration as:
\begin{equation}
    Z_{rt}^{ic} = \text{Conv}_{11}^\text{down}(\text{GELU}(\text{Conv}_{11}^\text{up}(Z_{rt}^{is})))+Z_{rt}^{is}
\end{equation}
where $\text{Conv}_{11}^\text{up}$ maps the feature from $R^{C}$ to $R^{2C}$, $\text{GELU}$ represents the GeLU~\cite{gelu} function. Similarly, we get $Z_{rt}^{1c}$.

\textbf{Linear Transformation}. We then perform a joint linear transformation of $Z_{rt}^{is}$ and $Z_{rt}^{ic}$, denoted as:
\begin{equation}
    Z_{rt} = \text{LN}(\text{Linear}(Z_{rt}^{1c}+Z_{rt}^{ic})+Z_{rt}^{1c}+Z_{rt}^{ic})
\end{equation}
where $Z_{rt}$ is the output feature of the STAM module.

\textbf{Module Fine-Tuning.} During training, the joint training of the STAM module and tracker's model relies on the ability to discriminate based features from a dual-mode combination. However, due to the discrimination limitations of the lightweight model, this optimal discrimination during training may become suboptimal when tracking is disturbed. Therefore, we adopt a fine-tuning paradigm to freeze the tracker's model and train only the STAM module. This maintains the model's adaptability to individual template feature and enhances its ability to discriminate continuous changes in target appearance by incorporating additional temporal information.

\subsection{Head and Loss}
 
Following the design of LightFC~\cite{lightfc}, rep-center-head is adopted as our prediction head. For RGB-T, RGB-D, RGB-E tracking, we use only one prediction head. For RGB-S tracking, we use two prediction heads that have the same structure but do not share parameters. 

We use the weighted focal loss~\cite{weightfocalloss} and the GIoU loss~\cite{giou} to train the classification branch and the box regression branch, respectively. The total training loss is:
\begin{equation}
    L_{total}=L_{cls}+\lambda_{iou}L_{iou}+\lambda_{l_{1}}L_{1}
\end{equation}
where $\lambda_{iou}=2$ and $\lambda_{l_{1}}=5$.
