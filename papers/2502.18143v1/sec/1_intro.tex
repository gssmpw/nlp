\section{Introduction}
\label{sec:intro}

Multimodal tracking uses a multimodal template to predict the target state in subsequent multimodal frames. It significantly improves the performance of visual trackers in challenging scenarios and has a wide range of potential applications \cite{rgbt1,rgbd2,rgbe3,rgbs50}.

In recent years, with the introduction of Transformer, multimodal trackers have made significant progress. However, these trackers inevitably become heavy and expensive. Take RGB-T tracking as an example, some methods insert integration modules into ViT, such as TBSI~\cite{tbsi} and CSTNet~\cite{cstnet}, their parameters and FLOPs are 202.0M and 144.0M, 82.5G and 74.7G, respectively. In addition, some methods like ViPT~\cite{vipt} and MPLT~\cite{mplt}, they use the prompt learning paradigm with only a few additional parameters, but their baseline OSTrack~\cite{ostrack} still has 92.1M parameters and 43.0G FLOPs. Similar problems exist for RGB-D and RGB-E tracking. The parameters and FLOPs of these trackers limit their application on resource-constrained devices. 

Although some MDNet trackers \cite{manet,dafnet,macnet} achieve lightweight multimodal tracking, their speed and performance is insufficient. In addition, CMD~\cite{cmd} explores how to design a lightweight DCF tracker through knowledge distillation in RGB-T tracking. However, there still exists a performance gap between CMD~\cite{cmd} and high-performance trackers. Furthermore, in RGB-D and RGB-E tracking, lightweight multimodal trackers have not received sufficient attention, resulting in challenges for the perception modules of many small intelligent vehicles (such as UAVs, etc.) to benefit from multimodal tracking.

\input{figs/bubblemap}

To alleviate this problem, we aim to explore an efficient RGB-X tracker family using a unified tracking framework. A unified model will also facilitate the development of specific computing devices for acceleration. Therefore, the key issues are how to select an appropriate framework and how to achieve efficient cross-modal feature modeling.

% For the first point, we first analyze the lightweight visual trackers of the last two years. With similar performance, LiteTrack-B4~\cite{litetrack} has 26.19M parameters and 6.78G FLOPs, which is 4.7x and 2.7x more than those of LightFC-vit~\cite{lightfc}, which has 5.50M parameters and 2.48G FLOPs. Similarly, the parameters of HiT-Small~\cite{hit} and MixFormerV2-S~\cite{mixformerv2s} are 2x and 3x more than those of LightFC~\cite{lightfc}. Therefore, to achieve a better balance between parameters and performance, we adopt the convolutional framework of LightFC~\cite{lightfc} as our baseline.

For the first point, we first analyze the lightweight visual trackers of the last two years. With similar performance, LiteTrack-B4~\cite{litetrack} has 26.19M parameters and 6.78G FLOPs, which is 4.7x and 2.7x more than those of LightFC-vit~\cite{lightfc}, which has 5.50M parameters and 2.48G FLOPs. In addition, the operator optimization of convolution is sufficient, and most edge devices do not limit the inference efficiency of lightweight models. Although Transformer trackers display high speed on Nvidia edge devices, their speed may reduce on some embedded devices that lack attention operator optimization but are cheaper. Therefore, we adopt the convolutional framework of LightFC~\cite{lightfc} as our baseline.

For the second point, considering that the dimension of the lightweight features is small, the insertion of cross-modal feature interaction module does not significantly increase the model parameters. Moreover, some trackers \cite{tbsi,cstnet} show the benefits of inserting feature integration modules for cross-modal feature interaction, Therefore, we adopt the module insertion paradigm for cross-modal feature interaction.

In this work, we propose a family of lightweight RGB-X (Thermal, Depth, Event, and Sonar) trackers called LightFC-X. It includes a novel efficient cross-attention module (ECAM) and a novel spatiotemporal template aggregation module (STAM). The ECAM module takes the original multimodal features as Q, K, and V matrices, computes lightweight cross-modal cross-attention, and then performs joint spatial and channel integration on the multimodal features. The STAM module uses the first template as a fixed template and extracts dynamic templates in tracking, performing similar processing on the two templates as described above. For training, we first train the model with the ECAM module while omitting the STAM module to achieve spatial feature discrimination. Then, we freeze the parameters of the model and finetune the STAM module to model the spatiotemporal representation of the target.


Comprehensive experiments show that the proposed LightFC-X achieves state-of-the-art performance and an optimal balance between parameters, performance, and speed among current lightweight trackers. For example, for RGB-T tracking, LightFC-T and LightFC-T-ST surpass CMD~\cite{cmd} by 4.8\% and 5.7\% in PR, 3.7\% and 4.3\% in SR on the LasHeR~\cite{lasher}, while using 3x and 2.6x fewer parameters, respectively, and running 2.7x faster. In the DepthTrack~\cite{depthtrack} and VisEvent~\cite{visevent} benchmarks, LightFC-D-ST and LightFC-E-ST achieve similar performance to OSTrack~\cite{ostrack} while using 12x fewer parameters. Moreover, LightFC-X achieves real-time performance on a CPU at a speed of 22 \textit{fps}.

The contributions of our work are as follows:

(1) We propose a novel efficient cross-attention module (ECAM) with only 0.08M parameters. It achieves efficient cross-modal feature interaction and multimodal feature refinement.


(2) We propose a novel spatiotemporal template aggregation module (STAM). It achieves a fast transition from a spatial model to a spatiotemporal model through module fine-tuning paradigm.

(3) We propose LightFC-X, a family of lightweight RGB-X trackers. Comprehensive experiments demonstrate that LightFC-X achieves state-of-the-art performance on several multimodal tracking benchmarks.