%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}
\usepackage{natbib}
% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage{array}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{multirow}
\usepackage{float}
\usepackage{placeins}
\usepackage{xcolor} % 用于自定义颜色
\usepackage{fontawesome}
\renewcommand{\floatpagefraction}{0.9}
\renewcommand{\textfraction}{0.1}    
\renewcommand{\topfraction}{0.9}     
\renewcommand{\bottomfraction}{0.9}
% \usepackage{simbiber}
% \usepackage[colorlinks=true, linkcolor=teal, urlcolor=teal, citecolor=teal]{hyperref} 


% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
% \usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}


\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Injecting Domain-Specific Knowledge into Large Language Models: \\A Comprehensive Survey}


% % Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
% \author{
% First Author$^1$
% \and
% Second Author$^2$\and
% Third Author$^{2,3}$\And
% Fourth Author$^4$\\
% \affiliations
% $^1$First Affiliation\\
% $^2$Second Affiliation\\
% $^3$Third Affiliation\\
% $^4$Fourth Affiliation\\
% \emails
% \{first, second\}@example.com,
% third@other.example.com,
% fourth@example.com
% }
% \fi

\author{
Zirui Song$^{1,2}$\footnote{Equal Contribution}
\and
Bin Yan$^{1*}$
\and
Yuhan Liu$^3$
\and
Miao Fang$^1$
\and
Mingzhe Li$^4$
\and
Rui Yan$^{3\dag}$
\and
Xiuying Chen$^{2}$\footnote{Corresponding authors.}\\
\affiliations
$^1$Northeastern University\\
$^2$Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)\\
$^3$Gaoling School of Artificial Intelligence, Renmin University of China\\
$^4$ByteDance\\
\emails
\{zirui.song, xiuying.chen\}@mbzuai.ac.ae, \{yuhan.liu, ruiyan\}@ruc.edu.cn\\
}






\begin{document}

\maketitle

\begin{abstract}
Large Language Models (LLMs) have demonstrated remarkable success in various tasks such as natural language understanding, text summarization, and machine translation. 
However, their general-purpose nature often limits their effectiveness in domain-specific applications that require specialized knowledge, such as healthcare, chemistry, or legal analysis.
To address this, researchers have explored diverse methods to enhance LLMs by integrating domain-specific knowledge. 
In this survey, we provide a comprehensive overview of these methods, which we categorize into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. 
Each approach offers unique mechanisms to equip LLMs with domain expertise, balancing trade-offs between flexibility, scalability, and efficiency. 
We discuss how these methods enable LLMs to tackle specialized tasks, compare their advantages and disadvantages, evaluate domain-specific LLMs against general LLMs, and highlight the challenges and opportunities in this emerging field. 
For those interested in delving deeper into this area, we also summarize the commonly used datasets and benchmarks.
To keep researchers updated on the latest studies, we maintain an open-source at: \faGithub ~\href{https://github.com/abilliyb/Knowledge_Injection_Survey_Papers}{\textcolor{blue}{official-repo.com}}, dedicated to documenting research in the field of specialized LLM.
% For researchers and practitioners interested in advancing knowledge-enhanced LLMs, we provide an open-source GitHub repository that consolidates the latest studies and resources. 

\end{abstract}



\section{Introduction}
Large Language Models (LLMs) have achieved extraordinary success across various tasks, showcasing remarkable capabilities in reasoning, knowledge representation, and decision-making. 
However, despite their impressive performance in general-purpose applications, many specialized domains, such as healthcare, chemistry, and legal analysis, demand the integration of domain-specific knowledge to achieve high accuracy and reliability. 
To address this challenge, researchers have explored methods to enhance LLMs through external or embedded domain expertise, a process often referred to as \textit{\textbf{knowledge injection}}. This approach aims to bridge the gap between general-purpose language understanding and the stringent requirements of domain-specific tasks, enabling LLMs to perform effectively in highly specialized contexts.


\begin{figure*}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/intro.pdf}
    \caption{Illustration of Growth Trends in Domain-Specific Knowledge Injection into LLMs. 
The chart displays the cumulative number of papers published between October 2022 and December 2024.
Different colors and border styles represent various injection methods and domains, such as blue with a solid border denoting dynamic injection in the biomedical field.    }
    \label{fig:intro}
\end{figure*}



Building on the foundational capabilities of general-purpose LLMs, knowledge injection techniques provide an effective means to address their limitations in handling specialized applications. Compared to the generalized approach of standard LLMs, knowledge injection offers two key advantages:  
1) incorporating precise, domain-specific knowledge to improve accuracy and reliability in specialized tasks, and  
2) allowing LLMs to dynamically adapt to new information or evolving knowledge bases, ensuring up-to-date expertise.  
These techniques bridge the gap between general-purpose understanding and domain-specific demands by leveraging both structured and unstructured knowledge sources. As a result, knowledge injection methods have been successfully applied in fields such as healthcare, chemistry, and legal analysis, significantly enhancing LLM performance. For example, biomedical LLMs ~\citep{bolton2024biomedlm27bparameterlanguage,yan2023biomedical} have demonstrated superior accuracy in tasks like medical diagnostics and regulatory compliance, while domain-specific models for material science~\citep{xie2024darwin15largelanguage,antunes2024crystalstructuregenerationautoregressive,zhang2024honeycombflexiblellmbasedagent} have achieved advances in material property prediction and discovery. These dedicated models underscore the transformative potential of integrating domain knowledge into LLMs, unlocking solutions to complex, field-specific challenges.


Despite these advancements, early efforts in knowledge injection often treated domains independently, leading to a lack of standardization in methodologies and evaluation. 
As the volume of research continues to grow rapidly, with applications and studies proliferating across disciplines, the need for a comprehensive review becomes evident.
This review aims to summarize the state of knowledge injection techniques, provide a systematic blueprint for future research, and identify key challenges, such as balancing scalability with domain-specific accuracy and enabling efficient, real-time knowledge updates. 
% These considerations underscore the importance of consolidating efforts in knowledge injection research to maximize its potential and impact.


To assist readers from various backgrounds in understanding knowledge injection techniques for LLMs and to complement existing surveys by addressing unresolved questions, we organize our survey paper as follows. 
After providing the\textit{ foundational background knowledge in Section~\ref{background},} we address a pivotal question: How can domain-specific knowledge be effectively integrated into LLMs? To answer this, we present a \textit{comprehensive framework for categorizing knowledge injection methods in Section~\ref{sec:paradigms}}. We delve into this topic by discussing: 1) Dynamic Knowledge Injection, which explains how external knowledge is retrieved and incorporated in real-time during inference to enhance reasoning capabilities; 2) Static Knowledge Embedding, which describes how domain knowledge is embedded into the model during training or fine-tuning to make it an inherent part of the model; 3) Adapters, which highlight modular techniques for storing and utilizing external knowledge without altering the primary model’s parameters; and 4) Prompt Optimization, which focuses on how carefully designed prompts enable models to utilize existing knowledge without requiring changes to their architecture. Another perspective for reviewing knowledge injection studies is their application across various domains. 
\textit{In Section~\ref{application}, we categorize current application domains}, such as materials science, chemistry, biology, and law. 
To guide the identification of appropriate tools and resources, we summarize \textit{commonly used benchmarks, open-source frameworks, and analyses on performance in Section~\ref{resource}}. Based on these summaries, we discuss the \textit{challenges and opportunities in this evolving field in Section~\ref{challenges}}.
The conclusions are summarized in Section~\ref{conclusion}.




\section{Background}
% 介绍分类
\label{background}


\subsection{Domain-Specific Knowledge}

Domain-specific knowledge refers to specialized information or expertise pertinent to a specific field or application, distinguishing it from general knowledge that spans across multiple domains. 
While general knowledge enables models to understand broad contexts, domain-specific knowledge is essential for addressing specialized tasks where precise, field-specific understanding is required.
For instance, in scientific text processing~\citep{bran2023chemcrow}, models must comprehend complex scientific terminologies, concepts, and methodologies to provide accurate and relevant answers. 
Similarly, in e-commerce search~\citep{zhao2024snfinllm}, understanding domain-specific terms such as product categories, technical specifications, or colloquial shopping language is crucial for delivering relevant search results and recommendations.
In healthcare applications, LLMs must understand medical terminologies, diagnoses, treatment plans, and drug interactions. 
For example, biomedical question answering~\citep{pei2024biot5} and medical report summarization rely on integrating knowledge from medical literature like PubMed~\citep{dernoncourt2017pubmed}.

To address these needs, researchers have explored various methods for incorporating domain-specific knowledge into LLMs. 
In this paper, we aim to provide a survey of these various injection methods.


\subsection{Knowledge Representation and Encoding}
Knowledge can take different forms depending on the structure and application needs. 
For example, knowledge graphs~\citep{zhang2024knowgpt} represent information as entities and relationships in a graph format, allowing structured reasoning and inference. 
These are widely used in tasks like question-answering and recommendation systems, where relationships between entities are crucial. 
Similarly, knowledge in text form, such as Wikipedia~\citep{jeong2024adaptive}, provides a vast corpus of unstructured information. 

Knowledge can also be stored in vector space instead of readable text or graph formats. 
For instance, soft prompt tuning~\citep{singhal2023publisher} learns useful knowledge in vector form, which is concatenated with the original input to guide LLMs in performing specific downstream tasks. 
In addition to external representations, knowledge can also emerge from within the model itself.
For example, chain-of-thought prompting~\citep{yao2024tree} introduces intermediate reasoning steps that help the model break down complex tasks into manageable parts. 
By explicitly reasoning through these steps, the LLM can utilize its internally stored information more effectively, resulting in better performance on tasks requiring logical reasoning, multi-step computation, or decision-making.



\section{Paradigms of Knowledge Injection}
\label{sec:paradigms}

This section identifies four major paradigms of knowledge injection: \textit{Dynamic Knowledge Injection}, \textit{Static Knowledge Embedding}, \textit{Adapters}, and \textit{Prompt Optimization}.
These paradigms illustrate various mechanisms by which external domain-specific knowledge can be incorporated into LLMs. 
We utilize unified notations, as described in Table~\ref{form}, to systematically represent the processes.
External knowledge \(\mathcal{K}\) is integrated into LLMs either by modifying the original parameters \(\theta\), introducing additional parameters \(\phi\), or leveraging auxiliary mechanisms. 






\begin{figure*}[tb]
    \centering
    \includegraphics[width=1\linewidth]{figs/paradigm.pdf}
    \caption{
Four knowledge injection paradigms for LLMs.
(a) Dynamic Knowledge Injection retrieves external knowledge during inference for enhanced reasoning.
(b) Static Knowledge Injection embeds external knowledge into model parameters during fine-tuning.
(c) Modular Knowledge Adapters use plug-and-play modules to dynamically adapt to tasks or updates.
(d) Prompt Optimization utilizes precise prompts to guide the LLM without altering its parameters.
    }
    \label{fig:model}
\end{figure*}



\subsection{Dynamic Knowledge Injection}
% We define the Dynamic Knowledge Injection as 通过 检索知识库 /知识图谱 等外部知识库，获取 knowledge 结合prompt 作为 LLMs的输入。



\begin{table}[tb]
\centering
\small
\begin{tabular}{c|p{0.7\linewidth}}
\toprule
\textbf{Symbol} & \textbf{Description} \\
\midrule
\(\mathbf{x}\)  & Input to LLM \\ 
\(\mathbf{y}\)  & Output of LLM \\ 
\(M\) & Backbone LLM Function \\
\(\mathcal{K}\) & External domain knowledge base \\
\(\theta\)      & Parameters of LLM \\ 
\(\phi\)        & Additional parameters introduced \\ 
\(\mathcal{R}(\mathbf{x}, \mathcal{K})\) & Retrieval function fetches relevant elements of \(\mathcal{K}\) given the input \(\mathbf{x}\) \\
\(M(\mathbf{x}; \theta)\)& Represent LLM takes input \(\mathbf{x}\) and produces an output, parameterized by \(\theta\) \\
\(\Delta\theta\) & Offsets to the original LLM's parameters \\
\bottomrule
\end{tabular}

\caption{Summary of Symbols.}
\label{form}
\end{table}

We define dynamic knowledge injection as the process of first retrieving information from external knowledge bases or knowledge graphs and then combining it with the input for use in LLMs:
\begin{equation}
    \mathbf{y} = M(\mathbf{x}, \mathbf{\mathcal{R}(\mathbf{x}, \mathcal{K})}; \theta),
\end{equation}
where \(\mathbf{x}\) represents the original input, \(\mathcal{R}\) denotes the retrieval function, \(\mathcal{K}\) is the external knowledge base, and \(\theta\) are the model parameters, which remain unchanged. 
This paradigm offers several advantages, including ease of updating (hence the term "dynamic injection") and the ability to incorporate new knowledge without retraining the model.
However, it also presents challenges, such as dependency on the quality of the knowledge base \(\mathcal{K}\), the retrieval function \(\mathcal{R}\), and limitations imposed by the maximum input length of the LLM.

\subsection{Static Knowledge Embedding}

Compared with dynamic knowledge retrieval, static knowledge embedding involves embedding knowledge into the model's parameters through full or partial fine-tuning, making it less flexible to changes.
Concretely, the model learns new parameters \(\Delta\theta\) that encode domain knowledge from \(\mathcal{K}\):
\[
\Delta\theta = \textstyle \arg\min_{\theta} \sum_{(\mathbf{x_s}, \mathbf{y_s}) \in \mathcal{K}} \mathcal{L}\bigl(M(\mathbf{x_s}; \theta), \mathbf{y_s}\bigr),
\]
where \(\mathcal{K}\) is the domain-specific knowledge base containing training samples \(\mathbf{x_s}\) and \(\mathbf{y_s}\), and \(\mathcal{L}\) is a typical supervised training loss function. After optimization, the updated parameters \(\Delta\theta\) are obtained. 

At inference time, no further retrieval or external knowledge calls are required:
\[
\mathbf{y} = M\bigl(\mathbf{x}; \Delta\theta\bigr).
\]
This paradigm provides fast inference, as it eliminates additional retrieval steps and often results in stronger performance. However, it also comes with challenges, such as costly updates—requiring fine-tuning whenever domain knowledge changes—and scalability issues, as embedding large or frequently changing knowledge bases can be computationally expensive.

\subsection{Modular Knowledge Adapters}  
To address the costly updates associated with static knowledge embedding, another paradigm, known as modular knowledge adapters, introduces \textit{small}, trainable modules that can be inserted into or operate alongside the base model to store domain-specific knowledge while saving computational resources. 
In this approach, the original parameters \(\theta\) of the LLM typically remain frozen, preserving the model’s general-purpose capabilities.
Given a knowledge dataset \(\mathcal{K}\), the adapter parameters \(\phi\) are trained by minimizing the following objective:
\[
\phi = \textstyle \arg\min_{\phi} \sum_{(\mathbf{x_s}, \mathbf{y_s}) \in \mathcal{K}} \mathcal{L}\bigl(M(\mathbf{x_s}; \theta, \phi), \mathbf{y_s}\bigr),
\]
where \(M(\mathbf{x_s}; \theta, \phi)\) represents the base model’s generation function enhanced with the new adapter parameters. 
At inference time, the enhanced model generates outputs as:
\[
\mathbf{y} = M\bigl(\mathbf{x}; \theta, \phi\bigr).
\]
This paradigm offers a parameter-efficient method to adapt LLMs to specific domains without modifying the original model weights. 
By freezing the base model’s parameters, the approach seeks to preserve previously acquired knowledge while enabling the seamless incorporation of new domain-specific information.
However, this approach also introduces challenges, such as the need to design new architectural components and determine appropriate hyperparameters, including the size and number of adapters. These additional elements can increase the overall complexity of the model and its training process.

\subsection{Prompt Optimization}  
Unlike previous approaches, prompt optimization does not retrieve knowledge from external sources. 
Instead, it focuses on fully leveraging or guiding the LLM to utilize its internal, pre-existing knowledge. The process can be formalized as:  
\[
\mathbf{y} = M\bigl([\mathbf{p}, \mathbf{x}]; \theta\bigr),
\]
where \(\mathbf{p}\) represents a textual prompt containing implicit domain knowledge or specific instructions.  

Prompt optimization offers significant advantages, including eliminating dependency on external domain knowledge bases and avoiding training. 
However, it also presents challenges, as designing effective prompts can be both complex and time-consuming. 
Additionally, long prompts may reduce the available context window, potentially affecting model efficiency and performance.




\subsection{Comparison of the Four Paradigms}  


% \begin{table}[htbp]
% \centering
% \scriptsize
% \renewcommand{\arraystretch}{1} % 调整行间距
% \begin{tabular}{@{}m{1cm}m{7cm}@{}}
% \toprule
% \textbf{Paradigm} & \textbf{Strengths and Weaknesses} \\ \midrule

% \begin{tabular}[c]{@{}c@{}}Dynamic \\ Injection\end{tabular} & 
% \textbf{Strengths:} 
% \begin{itemize}
%     \item No training cost, as it leverages an external retrieval module.
%     \item Supports knowledge updates dynamically without retraining.
%     \item Offers interpretability
% \end{itemize}
% \textbf{Weaknesses:}
% \begin{itemize}
%     \item Slower inference due to retrieval latency.
%     \item Heavily dependent on the quality of the retrieval module.
% \end{itemize} \\ \midrule

% \begin{tabular}[c]{@{}l@{}}Static \\Embedding\end{tabular} & 
% \textbf{Strengths:}
% \begin{itemize}
%     \item Fixed embedding representation incurs no extra inference cost.
% \end{itemize}
% \textbf{Weaknesses:}
% \begin{itemize}
%     \item High training cost due to pretraining or fine-tuning requirements.
%     \item Knowledge is static and cannot adapt to unseen tasks.
%     \item Risks catastrophic forgetting when fine-tuned for new tasks.
% \end{itemize} \\ \midrule


% \begin{tabular}[c]{@{}c@{}}Modular \\Adapters\end{tabular} & 
% \textbf{Strengths:} 
% \begin{itemize}
%     \item Low training cost, as only a small subset of parameters is trained.
%     \item Virtually no impact on inference speed.
%     \item Easy to update specific modules without affecting the entire model.
% \end{itemize}
% \textbf{Weaknesses:}
% \begin{itemize}
%     \item Sensitive to the quality of training data for each module.
%     \item Limited generalization if the adapters are not well-aligned with the task.
% \end{itemize} \\ \midrule

% \begin{tabular}[c]{@{}l@{}}Prompt \\Engineering\end{tabular} & 
% \textbf{Strengths:}
% \begin{itemize}
%     \item No training cost; relies on existing pre-trained knowledge.
%     \item Virtually no impact on inference speed.
% \end{itemize}
% \textbf{Weaknesses:}
% \begin{itemize}
%     \item Labor-intensive process requiring task-specific manual effort.
%     \item Limited to the pre-existing knowledge within the model.
%     \item Difficult to scale for highly specialized tasks.
% \end{itemize} \\

% \bottomrule
% \end{tabular}
% \caption{Comparison of knowledge injection paradigms based on strengths and weaknesses.}
% \label{tab:knowledge_injection_strengths_weaknesses}
% \end{table}

\begin{table}[htbp]
\centering
\scriptsize
\renewcommand{\arraystretch}{1} % 调整行间距
\begin{tabular}{@{}m{1.5cm}m{2cm}m{1.5cm}m{2.2cm}@{}}

\toprule
\textbf{Paradigm} & \textbf{Training Cost} & \textbf{Inference Speed} & \textbf{Limitations} \\ \midrule
\begin{tabular}[c]{@{}c@{}}Dynamic \\ Injection\end{tabular}  &   \begin{tabular}[c]{@{}c@{}} None, but requires \\ retrieval module\end{tabular}&  \begin{tabular}[c]{@{}c@{}} Slower due to  \\ retrieval latency \end{tabular}& \begin{tabular}[c]{@{}c@{}} Relies heavily on\\retrieval quality\end{tabular} \\ \midrule
\begin{tabular}[c]{@{}l@{}}Static \\ Embedding\end{tabular}   & \begin{tabular}[c]{@{}cc@{}} High \\ (requires pretraining \\ or fine-tuning)\end{tabular} & No extra cost & \begin{tabular}[c]{@{}cc@{}} Fixed knowledge; \\ risks catastrophic \\ forgetting \end{tabular}  \\ \midrule
\begin{tabular}[c]{@{}c@{}}Modular \\ Adapters\end{tabular}   & \begin{tabular}[c]{@{}cc@{}} Low \\ (train small subset \\ of parameters)\end{tabular} & \begin{tabular}[c]{@{}c@{}} Almost  \\unaffected\end{tabular} & \begin{tabular}[c]{@{}c@{}} Sensitive to training \\ data quality\end{tabular} \\ \midrule
\begin{tabular}[c]{@{}l@{}}Prompt \\ Optimization\end{tabular} & None & \begin{tabular}[c]{@{}c@{}} Almost  \\unaffected\end{tabular} & \begin{tabular}[c]{@{}cc@{}} Labor-intensive;\\ limited to pre-existing\\ knowledge\end{tabular}  \\
\bottomrule
\end{tabular}
\caption{Guidance on selecting knowledge injection paradigms based on training cost, inference speed, and limitations.}
\label{tab:knowledge_injection}
\end{table}
Dynamic knowledge injection integrates external knowledge at runtime, offering flexibility and adaptability to new information without increasing training cost. 
However, it requires an effective retrieval module, and the inference speed depends heavily on retrieval performance, which can slow down the overall process.
Static knowledge embedding embeds domain expertise during pretraining or fine-tuning, requiring large-scale domain-specific data and significant training resources, including GPUs and time. 
While it incurs no additional inference cost, its limitations lie in the potential risks of catastrophic forgetting and its inability to adapt to evolving information.
Modular adapters serve as a middle ground, allowing plug-and-play components to enhance domain-specific capabilities with minimal training data. 
Only a small subset of parameters needs to be trained, which reduces training costs, and inference speed is largely unaffected. However, the quality of training data significantly impacts the performance of this method.
Prompt Optimization, on the other hand, avoids retraining entirely by leveraging carefully crafted inputs. 
It has no impact on inference speed but relies on extensive human effort to find optimal prompts. 
This method is limited in its ability to utilize new knowledge and primarily activates pre-existing knowledge.

% Each paradigm entails trade-offs in adaptability, training cost, inference efficiency, and scalability. The choice of an appropriate method depends on the specific task requirements, available resources, and whether real-time adaptability or static knowledge integration is prioritized. 
We summarize these comparisons in Table~\ref{tab:knowledge_injection} as a practical guide to help determine the most suitable method based on specific task requirements and scenarios.

\section{Applications}
\label{application}
\subsection{Biomedicine}

The biomedicine domain benefits from a wealth of specialized corpora, such as PubMed~\citep{dernoncourt2017pubmed} and MedQA~\citep{jin2021disease}, enabling the development of LLMs specifically trained on biomedical texts. These models often follow the static knowledge embedding approach, leveraging the domain-specific richness of biomedical data.
For instance, PMC-LLaMA~\citep{wu2023pmc} extends the LLaMA 7B model through further pretraining on 4.9 million PubMed Central articles curated from the S2ORC dataset~\citep{lo2020s2orc}, completing five epochs to embed biomedical knowledge effectively. Similarly, Med-PaLM 2~\citep{singhal2023towards} builds on PaLM 2 via instruction fine-tuning. This fine-tuning incorporates a diverse mix of medical question-answering datasets, including MedQA, MedMCQA~\citep{pal2022medmcqa}, and HealthSearchQA~\citep{singhal2023publisher}.

Beyond foundational models, integrating external tools and knowledge can further enhance performance. For example, GeneGPT~\citep{jin2024genegpt} utilizes an LLM pretrained on code tasks to tackle GeneTuring tests by employing NCBI Web APIs. This approach combines in-context learning with an augmented decoding algorithm that identifies and executes API calls. 
Similarly, Med-PaLM~\citep{singhal2023publisher} introduces vector prompts—representations that store and retrieve medical domain knowledge—to extend the capabilities of Flan-PaLM~\citep{chung2024scaling}.
\begin{table*}[h!]
\scriptsize
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c}
\toprule
\textbf{Domain} & \textbf{Model} & \textbf{Paradigms} & \textbf{Knowledge Source} & \textbf{Link}  \\ \hline
\multirow{10}{*}{Biomedicine}   
% & BianCang~\citep{wei2024biancangtraditionalchinesemedicine} & Static Knowledge Embedding & ChP-TCM & \href{https://github.com/QLU-NLP/BianCang}{\textcolor{teal}{Link}} 
% \\ \cline{2-5}
 % &  HuaTuo~\citep{wang2023huatuotuningllamamodel} & Static Knowledge Embedding & SUS & \href{https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese}{\textcolor{teal}{Link}} \\ \cline{2-5}
% & AntGLM-Med-10B~\citep{li2024beginnerexpertmodelingmedical}&Static Knowledge Embedding & PromptCBLUE, PubMedQA, MedQA, MedMCQA & \href{https://haishen-ll.github.io/projects/}{\textcolor{teal}{Link}} \\
% \cline{2-5}


&PMC-LLaMA~\citep{wu2023pmc}& Static Knowledge Embedding &PMC-OA, MedC-I, PubMedQA, MedMCQA, USMLE& \href{https://github.com/chaoyi-wu/PMC-LLaMA}{\textcolor{teal}{Link}}  \\ \cline{2-5}


&Med-PaLM 2~\citep{singhal2023towards}&Static Knowledge Embedding&MultiMed&\href{https://github.com/SHARANR26/Med-Palm2}{\textcolor{teal}{Link}}  \\ \cline{2-5}

% &Qilin-Med \citep{ye2023qilin}& \begin{tabular}[c]{@{}c@{}}Static Knowledge Embedding \\Dynamic Knowledge Injection\end{tabular} &ChiMed,CMExam test set,Huatuo-26M test set&\href{https://github.com/williamliujl/Qilin-Med}{\textcolor{teal}{Link}}  \\ \cline{2-5}


&DALK ~\citep{li2024dalk}&\begin{tabular}[c]{@{}c@{}}Dynamic Knowledge Injection \\ Prompt Optimization\end{tabular}&MedQA, MedMCQA, MMLU, QA4MRE&\href{https://github.com/David-Li0406/DALK}{\textcolor{teal}{Link}} \\ \cline{2-5}

&ChronicCareGPT~\citep{liu2024few} &Prompt Optimization& eRisk &\href{https://github.com/WangRongsheng/CareGPT}{\textcolor{teal}{Link}} \\ \cline{2-5}

&SA-MDKIF~\citep{xu2024sa}&Modular Knowledge Adapters&MedQuA,emrQA, PubMedQA, MedQA& \textbackslash{}\\ \cline{2-5}

&MaLP~\citep{zhang2024llm}&Modular Knowledge Adapters&HealthCareMagic-100k, iCliniq &\href{https://github.com/MatthewKKai/MaLP}{\textcolor{teal}{Link}} \\ \cline{2-5}
&BioMedLM ~\citep{bolton2024biomedlm27bparameterlanguage}&Static Knowledge Embedding   &PubMed,MedMCQA,MedQA,MMLU,BioASQ   &\href{https://github.com/stanford-crfm/BioMedLM}{\textcolor{teal}{Link}} \\ \cline{2-5}




&BiomedRAG~\citep{li2024biomedragretrievalaugmentedlarge}&Dynamic Knowledge Injection    &CHEMPROT,DDI,ade-corpus-v2,MTsample,ADInt,UMLS   &\href{https://github.com/ToneLi/BIoMedRAG}{\textcolor{teal}{Link}} \\ \cline{2-5}

&MedINST~\citep{han2024medinstmetadatasetbiomedical}&Static Knowledge Embedding   &MedINST   &\href{https://github.com/MedMNIST/MedMNIST}{\textcolor{teal}{Link}} \\ \cline{1-5}





 \hline
\multirow{8}{*}{Finance}
&FLANG    ~\citep{shah2022fluemeetsflangbenchmarks}&Static Knowledge Embedding   &\begin{tabular}[c]{@{}c@{}}Financial PhraseBank,FiQA 2018 Task-1,\\News Headline Classification, Named Entity Recognition,\\Structure Boundary Detection,Question Answering \end{tabular} &\href{https://huggingface.co/SALT-NLP/FLANG-BERT}{\textcolor{teal}{Link}} \\ \cline{2-5}

 




& BloomBergGPT~\citep{wu2023bloomberggpt} & \begin{tabular}[c]{@{}c@{}}Static Knowledge Embedding\end{tabular} & \begin{tabular}[c]{@{}c@{}}Finance dataset (web, news, filings, press, Bloomberg), \\
Public dataset (the Pile, C4, Wikipedia)\end{tabular} & \textbackslash{} \\ 
\cline{2-5}
 

% &XuanYuan 2.0 ~\citep{zhang2023xuanyuan20largechinese}&Static Knowledge Embedding   &FinCorpus  &\href{https://github.com/Duxiaoman-DI/XuanYuan}{\textcolor{teal}{Link}} \\ \cline{2-5}

&FinMA ~\citep{xie2023pixiu}&Static Knowledge Embedding   &\begin{tabular}[c]{@{}c@{}}FPB,FiQA-SA,Headline,NER,FinQA,\\ConvFinQA,BigData22,ACL18,CIKM18    \end{tabular}
  &\href{https://github.com/The-FinAI/PIXIU}{\textcolor{teal}{Link}} \\ \cline{2-5}



  
&FinGPT~\citep{zhang2023instruct} & Modular Knowledge Adapters & \begin{tabular}[c]{@{}c@{}}Financial news, Company filings and announcements, \\
 Social media discussions, Trends\end{tabular}
 &\href{https://github.com/AI4Finance-Foundation/FinGPT}{\textcolor{teal}{Link}}  \\ 
\cline{2-5}


 &Fin-LLaMA  ~\citep{konstantinidis2024finllamafinancialsentimentclassification}&Static Knowledge Embedding   &fin-llama-dataset   &\href{https://github.com/Bavest/fin-llama}{\textcolor{teal}{Link}} \\ \cline{2-5}
 
 &SNFinLLM~\citep{zhao2024snfinllm}  & \begin{tabular}[c]{@{}c@{}}Static Knowledge Embedding\end{tabular} & \begin{tabular}[c]{@{}c@{}}FinEval, FinanceIQ,qEQA,FinC,KQA,MRC,cMRC\end{tabular}
 &\textbackslash{} \\ 
\cline{1-5}





% &BBT-FinT5   ~\citep{lu2023bbtfincomprehensiveconstructionchinese}&Static Knowledge Embedding   &BBT-FinCorpus &\href{https://github.com/supersymmetry-technologies/BBT-FinCUGE-Applications}{\textcolor{teal}{Link}} \\ \cline{2-5}



% &Instruct-FinGPT ~\citep{zhang2023instructfingptfinancialsentimentanalysis}&Static Knowledge Embedding   &\begin{tabular}[c]{@{}c@{}}FiQA,Twitter financial news sentiment training,\\Numerical sensitivity,Financial PhraseBank (FPB)       \end{tabular}
%  &\href{https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_RAG/instruct-FinGPT}{\textcolor{teal}{Link}} \\ \cline{2-5}



\hline


\multirow{8}{*}{Materials}   

 & ChemCrow~\citep{bran2023chemcrow} & Dynamic Knowledge Injection & 18 expert-designed tools & \href{https://github.com/ur-whitelab/chemcrow-public}{\textcolor{teal}{Link}} \\
  \cline{2-5}
  




 &  ChemDFM ~\citep{zhao2024chemdfmlargelanguagefoundation} &Static Knowledge Embedding & \begin{tabular}[c]{@{}c@{}}SciQ,PIQA,PubChem,ARC,USPTO\end{tabular} &\href{https://github.com/OpenDFM/ChemDFM}{\textcolor{teal}{Link}} \\
 \cline{2-5}

 & ChemLLM~\citep{zhang2024chemllmchemicallargelanguage} &\begin{tabular}[c]{@{}c@{}}Static Knowledge Embedding\end{tabular} & \begin{tabular}[c]{@{}c@{}}ChemData,ChemBench\end{tabular} & \href{https://github.com/keyhsw/ChemLLM}{\textcolor{teal}{Link}} \\
 \cline{2-5}
 
 



 & CrystaLLM ~\citep{antunes2024crystalstructuregenerationautoregressive} &\begin{tabular}[c]{@{}c@{}}Static Knowledge Embedding\end{tabular} & \begin{tabular}[c]{@{}c@{}}Materials Project, OQMD, NOMAD\end{tabular} & \href{https://github.com/lantunes/CrystaLLM}{\textcolor{teal}{Link}} \\
 \cline{2-5}
 & ScholarChemQA ~\citep{chen2024scholarchemqa} &\begin{tabular}[c]{@{}c@{}}Static Knowledge Embedding\end{tabular} & AG News,Yahoo Answers ,Yelp-5,Amazon-5 & \href{https://github.com/iriscxy/chemmatch}{\textcolor{teal}{Link}} \\
 \cline{2-5}

& DARWIN 1.5~\citep{xie2024darwin15largelanguage} & Static Knowledge Embedding & FAIR datasets &  \href{https://github.com/MasterAI-EAM/Darwin}{\textcolor{teal}{Link}} \\ 
\cline{2-5}


  &  ChemAgent~\citep{tang2025chemagent} &\begin{tabular}[c]{@{}c@{}}Dynamic Knowledge Injection \\ Prompt Optimization\end{tabular} & \begin{tabular}[c]{@{}c@{}}Google API, Docker container, Internet,\\ Hardware API document, Physical world hardware\end{tabular} & \href{https://github.com/gersteinlab/chemagent}{\textcolor{teal}{Link}} \\
 \cline{1-5}
% &  HoneyComb ~\citep{zhang2024honeycombflexiblellmbasedagent} &\begin{tabular}[c]{@{}c@{}}Dynamic Knowledge Injection \end{tabular} & \begin{tabular}[c]{@{}c@{}}MatSciKB,MaScQA,SciQA \end{tabular} & \textbackslash{} \\ \cline{2-5}

 % & LlaSMol ~\citep{yu2024llasmoladvancinglargelanguage} &\begin{tabular}[c]{@{}c@{}}Static Knowledge Embedding\end{tabular} & \begin{tabular}[c]{@{}c@{}}SMolInstruct\end{tabular} &\href{https://github.com/OSU-NLP-Group/LLM4Chem}{\textcolor{teal}{Link}} \\
 % \cline{1-5}
\hline


\multirow{5}{*}{Mental Health}  
 &  MeChat~\citep{qiu2023smile} & Dynamic Knowledge Injection  & SMILECHAT, PsyQA & \href{https://huggingface.co/qiuhuachuan/MeChat}{\textcolor{teal}{Link}}  \\  \cline{2-5}

  &  MindChat~\citep{MindChat} & Static Knowledge Embedding & Multi-turn psychological dialogue data & \href{https://github.com/X-D-Lab/MindChat}{\textcolor{teal}{Link}}  \\  \cline{2-5}

  
 &  SoulChat~\citep{chen2023soulchat} & Static Knowledge Embedding & Long-text counseling sessions & \href{https://github.com/scutcyr/SoulChat}{\textcolor{teal}{Link}}  \\ \cline{2-5}
 
 
 
 % & CPsyExam~\citep{zhao2024cpsyexam} & Static Knowledge Embedding & CPsyExam & \href{https://github.com/CAS-SIAT-XinHai/CPsyExam}{\textcolor{teal}{Link}}  \\ \cline{2-5}

  & EmoLLM~\citep{yang2024emollm} & \begin{tabular}[c]{@{}c@{}}Static Knowledge Embedding \\ Modular Knowledge Adapters\end{tabular} &  CPsyCounD & \href{https://github.com/SmartFlowAI/EmoLLM}{\textcolor{teal}{Link}}  \\ 


  \hline
  \multirow{5}{*}{Education}  
  &  EduChat~\citep{dan2023educhat} & Static Knowledge Embedding & \begin{tabular}[c]{@{}c@{}}Textbooks Data, Open QA Data, \\
 Emotional Support Data, Socratic Teaching Data\end{tabular} & \href{https://github.com/ECNU-ICALK/EduChat}{\textcolor{teal}{Link}}  \\  \cline{2-5}

&  QiaoBan~\citep{qiaoban2023} & Prompt Optimization & Children's emotional education dialogue data & \href{https://github.com/HIT-SCIR-SC/QiaoBan?tab=readme-ov-file}{\textcolor{teal}{Link}}   \\ \cline{2-5}
      
  &  HiTA~\citep{liu2024hita} & Dynamic Knowledge Injection & Educator curated database & \textbackslash{}  \\  \cline{2-5}

& SocraticLM~\citep{liusocraticlm} & Modular Knowledge Adapters & SocraTeach dataset& \textbackslash{}  \\ \cline{2-5}


& CyberQ~\citep{agrawal2024cyberq} & \begin{tabular}[c]{@{}c@{}}Static Knowledge Embedding \\ Dynamic Knowledge Injection\end{tabular} &  AISecKG, Q\&A  & \textbackslash{}  \\ 
\hline

    
    \multirow{3}{*}{Social Science} 
  & SocialLLM~\citep{jiang2023social}  &  
\begin{tabular}[c]{@{}c@{}}Static Knowledge Embedding \\
 Prompt Optimization\end{tabular}& \begin{tabular}[c]{@{}c@{}}Covid-Political, Election2020, COVID-Morality, \\ Ukr-Rus-Suspended, Ukr-Rus-Hate,  \\ Immigration-Hate-08, Immigration-Hate-05\end{tabular} & \textbackslash{} \\ \cline{2-5}
 
  & FPS~\citep{liu2024skepticism}  &  
Prompt Optimization & Fake News Dataset, Big Five Personality Traits & \href{https://github.com/LiuYuHan31/FPS}{\textcolor{teal}{Link}} \\ \cline{2-5}

  & FUSE~\citep{liu2024tiny}  &  
Prompt Optimization & True News Dataset, Big Five Personality Traits & \textbackslash{} \\
\toprule
\end{tabular}}
\caption{Summary of the domain-specific knowledge injection studies. 
We categorize current work according to their research domain and knowledge injection method.}
\label{maintable}
\end{table*}



\subsection{Finance}  

Fine-tuned financial LLMs have demonstrated significant advancements in adapting general-purpose models to domain-specific tasks through task-specific training. 
PIXIU~\citep{xie2023pixiu} fine-tunes LLaMA on 136K instruction samples tailored to financial tasks, enabling the model to handle a wide range of domain-relevant scenarios. 
Instruct-FinGPT~\citep{zhang2023instruct} fine-tunes LLaMA on 10K instruction samples derived from two financial sentiment analysis datasets, focusing primarily on finance classification tasks. 
FinGPT~\citep{yang2023fingpt} introduces a comprehensive end-to-end framework for training and deploying FinLLMs in the financial industry. 
Utilizing the LoRA technique, FinGPT fine-tunes open-source LLMs like LLaMA and ChatGLM with approximately 50K task-specific samples, achieving efficient fine-tuning without full model retraining.

In contrast, scratch-trained financial LLMs aim to create models specifically designed for financial tasks from the ground up. BloombergGPT~\citep{wu2023bloomberggpt} leverages a subset of 5 billion tokens from Bloomberg-specific data, representing only 0.7\% of its total training corpus, to tailor its model to financial applications. 
XuanYuan 2.0~\citep{zhang2023xuanyuan} combines 366 billion tokens for pre-training with an additional 13 billion tokens for fine-tuning, creating the largest Chinese financial chat model. 
Similarly, Fin-T5~\citep{lu2023bbt} introduces a Chinese financial pre-training language model built on the T5 architecture, utilizing a 300GB financial corpus. 
Furthermore, SNFinLLM~\citep{zhao2024snfinllm} dynamically incorporates real-time financial data during inference to enhance decision-making capabilities, demonstrating the value of domain-specific pretraining and adaptability in financial LLMs.



\subsection{Materials}
In contrast to biomedicine, where significant efforts have been devoted to static knowledge embedding due to the availability of extensive corpora, research in materials and chemistry has primarily focused on utilizing task-related tools that align with the dynamic knowledge injection paradigm.

For instance,
~\cite{xie2024darwin15largelanguage} demonstrated how Darwin 1.5 leverages natural language inputs and a two-stage training strategy to achieve significant improvements in materials discovery and design tasks.
\cite{bran2023chemcrow} introduced ChemCrow, a framework that augments LLMs with chemistry-expert-designed tools for downstream tasks such as organic synthesis and drug discovery.
There are also studies on prompt optimization~\cite{tang2025chemagent}, demonstrating that better-designed planning prompts can effectively utilize the model's internal knowledge to orchestrate complex tasks.
This approach capitalizes on the planning and execution capabilities of multiple LLMs to achieve autonomy in chemical experimentation.

More recently, there has been growing interest in exploring static knowledge embedding and modular knowledge adapters within the chemistry domain. 
For example,~\cite{chen2024scholarchemqa} curated a QA dataset to fine-tune pretrained models like BERT and LLMs such as Llama, aiming to enhance their performance in chemistry-related tasks.
Similarly,~\cite{xie2024darwin15largelanguage} introduced Darwin 1.5, an open-source large language model tailored for materials science. 



\subsection{Human-Centered Science}  

The last domain we introduce is human-centered science, which encompasses a wide range of applications such as psychological counseling, financial forecasting, social behavior prediction, and legal reasoning.
All these domains center on understanding and interacting with human needs, behaviors, and decision-making processes.  

In \textit{\textbf{mental health}}, datasets like PsyQA~\citep{sun2021psyqa} provide a foundation for training models in psychological counseling scenarios.
For instance, SoulChat~\citep{chen2023soulchat}, a model fine-tuned on over 100,000 long-text counseling sessions using static knowledge embedding, is designed for empathic conversations. 
Similarly, MeChat~\citep{qiu2023smile} employs dynamic knowledge injection to adapt to real-time inputs, significantly enhancing its emotional support capabilities. 
These advancements demonstrate the potential of human-centered science in addressing complex, real-world challenges through personalized and context-aware solutions.  

In the \textit{\textbf{education domain}}, LLMs have shown immense potential in addressing challenges such as personalized learning, curriculum alignment, and interactive teaching. 
Personalized learning, for example, requires models to adapt to individual needs, providing tailored feedback and emotional support. 
EduChat~\citep{dan2023educhat} tackles this by leveraging educational theories from psychology and pedagogy through static knowledge embedding, enabling tasks like open Q\&A, composition correction, and emotional support. 
Similarly, QiaoBan~\citep{qiaoban2023} focuses on child-centered education by using prompt optimization to adapt the model’s behavior based on child psychology and emotional well-being, catering specifically to young learners. 
% In contrast, models like HiTA~\citep{liu2024hita} address curriculum alignment challenges by dynamically injecting course-specific knowledge during inference through a retrieval-augmented generation framework. 
Domain-specific education and interactive teaching have also seen advancements through LLMs. 
% CyberQ~\citep{agrawal2024cyberq} combines static and dynamic knowledge injection using knowledge graphs like AISecKG~\citep{agrawal2023aiseckg}, enabling the generation of Q\&A that reflects best practices in specialized fields like cybersecurity. 
CyberQ~\citep{agrawal2024cyberq} blends static  knowledge embedding and dynamic knowledge injection via AISecKG~\citep{agrawal2023aiseckg}, generating Q\&A based on cybersecurity best practices.
Interactive teaching, on the other hand, benefits from models like SocraticLM~\citep{liusocraticlm}, which employs adapters fine-tuned on the SocraTeach dataset to engage students in critical thinking and problem-solving. 
% These adapters enable SocraticLM to act as an effective teaching assistant, guiding students through reasoning tasks using Socratic questioning techniques.



For \textit{\textbf{social sciences}}, models like SocialLLM~\citep{jiang2023social} combine static knowledge embedding and dynamic knowledge injection to analyze human behavior in social networks. 
Adapters facilitate large-scale data integration while prompt optimization guides the model to focus on specific social behavior patterns. 
Models like FPS~\citep{liu2024skepticism} and FUSE~\citep{liu2024tiny} use prompt optimization to simulate the spread and evolution of fake news in social networks, helping understand misinformation's impact. 
% Additionally, Lawyer LLaMA~\citep{huang2023lawyer} employs similar techniques to inject legal knowledge for improved legal reasoning and decision-making.

A summary of the mainstream models and their information is provided in Table~\ref{maintable}.
More models across various domains can be found at: \href{https://github.com/abilliyb/Knowledge_Injection_Survey_Papers}{\textcolor{blue}{Survey-official-repo}}.

\section{Tools, Resources, and Analysis}
\label{resource}

\subsection{Knowledge Injection Framework}
In this section, we provide a detailed introduction to four open-source frameworks categorized under different knowledge injection methods to facilitate understanding and application:
KnowGPT~\citep{zhang2024knowgpt} for Dynamic Knowledge Injection, StructTuning~\citep{liu2024structure} for Static Knowledge Embedding, 
K-Adapter~\citep{wang2021k} for Modular Knowledge Adapters, and SelfLift~\citep{cheng2024lift} for Prompt Optimization.



KnowGPT \textit{dynamically} combines knowledge graphs with prompt optimization by leveraging reinforcement learning to extract highly relevant subgraphs from the knowledge graph. 
These subgraphs are represented as triples and transformed into natural language prompts that language models can interpret and utilize via diverse prompt templates. 
The KnowGPT framework significantly reduces the API call costs of LLMs while enhancing their performance in domain-specific tasks.

% Map-Tuning dynamically injects knowledge into model inputs during the inference phase to optimize reasoning performance. 
% By training a lightweight mapping network, Map-Tuning maps knowledge to model inputs while keeping the downstream model parameters frozen. Specifically, Map-Tuning incorporates two injection strategies: a general-purpose injection plugin applicable to multiple downstream models and a task-specific injection plugin tailored for specific downstream tasks. This framework demonstrates significant performance improvements in knowledge-driven tasks such as relation classification, entity type tagging, and question answering, achieving a balance between generality and efficiency.

StructTuning uses a structure-aware approach to \textit{embed} domain knowledge into pre-trained models with a two-stage strategy: 
Structure-Aware Continual Pre-Training encodes knowledge into the model's parameters, and Structure-Aware Supervised Fine-Tuning refines understanding through structured QA tasks.
This framework demonstrates significant performance improvements in knowledge-driven tasks such as relation classification and question answering, achieving a balance between generality and efficiency.

K-Adapter stores knowledge within \textit{adapter} modules. 
Its core method involves freezing the original model parameters and assigning an independent, task-specific adapter for each type of knowledge.
These adapters are inserted as independent modules into the intermediate layers of the model to generate enhanced representations of specific knowledge. 
This design effectively mitigates the issue of catastrophic forgetting, preventing newly injected knowledge from overwriting the model's pre-existing knowledge.


Finally, SelfLift iteratively employs a retrieval-augmented generator to create an \textit{unbounded memory pool} and uses a memory selector to choose one output as memory for the subsequent generation round. 
This is an excellent demonstration of prompt optimization, where the model's outputs are dynamically refined and reused to enhance its overall performance and coherence in subsequent tasks.

\subsection{Datasets and Benchmarks}

We summarize commonly used datasets or benchmarks for domain-specific LLM study in Table~\ref{maintable}, observing significant variation in dataset richness across domains. 
Biomedicine boasts numerous high-quality datasets, such as PubMed, PubMedQA~\citep{jin2019pubmedqa}, and BioASQ~\cite{tsatsaronis2012bioasq}, that support tasks such as question answering and clinical summarization. 
In contrast, materials and chemistry have more limited resources, and datasets like USPTO and Enzymes focus on chemical reactions.
Miscellaneous datasets are scattered across other domains, such as PsyQA and SmileChat in mental health, SocraTeach, and Children’s emotional education dialogue data dataset in education.
% BIG-Bench for diverse tasks like the Imitation Game, ChPTCM for Traditional Chinese Medicine, and MoviE Text Audio QA for multi-modal movie-related tasks.
This diversity underscores the effort to tailor LLMs to specialized fields while emphasizing the need for broader curation of benchmarks in underrepresented domains.




\subsection{Performance Comparison of Domain-specific LLM and General-domain LLM}

\begin{table}[htbp]
\centering
\small
\resizebox{0.48\textwidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Model}          & \textbf{Domain Type} & \textbf{Size}   & \textbf{MedQA} & \textbf{PubMedQA} & \textbf{MedMCQA} \\ \midrule
Med-Gemini              & Specific             & --              & 91.1           & -                 & -                \\
GPT-4                   & General              & --              & 90.2           & 80.4              & 73.7             \\
Med-PaLM 2              & Specific             & --              & 85.4           & 81.8              & 72.3             \\
PMC-LLaMA               & Specific             & 13B             & 56.3           & 77.9              & 56.0             \\
BioMedLM                & Specific             & 2.7B            & 50.3           & 74.4              & --               \\
Llama 2                 & General              & 70B             & 43.7           & 74.3              & 35.0             \\
Galactica                     & General              & 120B            & 44.4           & 77.6              & 52.9             \\
\bottomrule
\end{tabular}%
}
\caption{Performance comparison of domain-specific and general-domain model performance on medical benchmarks.}
\label{compare}
\end{table}


Since there are also general-domain strong LLMs, it is necessary to discuss the comparison between domain-specific LLMs and general ones to determine if a specific knowledge injection process is essential. 
Here, we take the biomedical domain as an example due to significant research efforts in this field, as shown in Table~\ref{compare}.
The results are collected from corresponding papers or \textit{paperswithcode.com}.

% We can draw several conclusions.
First, we can observe that closed-source LLMs are currently the most effective models, while the performance gap between general-domain and domain-specific LLMs is relatively narrow. 
For example, both GPT-4 and Med-Gemini~\citep{saab2024capabilities} achieve outstanding performance, with scores higher than 90 on the MedQA dataset.
However, due to the lack of transparency in closed-source LLMs, open-source LLM efforts should not be overlooked. 
In this field, domain-specific LLMs often outperform general-domain models.
For instance, PMC LLaMA-13B outperforms LLaMA2-70B by more than 10 points on the MedQA dataset.
This demonstrates the value of domain-specific LLMs in achieving superior performance on specialized tasks.
While general-domain models can deliver strong results, incorporating domain-specific knowledge allows for significant improvements, particularly in open-source initiatives. 
This underscores the importance of investing in domain-specific LLMs to address the unique challenges of specialized fields.



\section{Challenges and Opportunities}
\label{challenges}
\subsection{Integrated Knowledge Consistency}

Knowledge injection allows LLMs to incorporate and integrate different domain-specific knowledge. 
However, retrieved knowledge may conflict with the model’s pre-trained representations or other retrieved facts, leading to inconsistencies in outputs~\citep{xu2024knowledge}. 
For example, in healthcare or legal analysis, conflicting treatment protocols or contradictory legal precedents could arise, resulting in unreliable decisions and undermining the system’s trustworthiness.

To address this, future research must focus on detecting inconsistencies, resolving conflicts, and maintaining consistency in integrated knowledge.
Conflicts can be addressed by prioritizing reliable sources, applying domain-specific rules, or using ensemble techniques to balance multiple perspectives. 
Alignment algorithms and validation modules can further ensure that retrieved knowledge aligns with the model’s reasoning processes and is reliable before influencing outputs. 
These efforts are essential to enhance the reliability and applicability of knowledge-enhanced LLMs in complex, high-stakes domains.


\subsection{Cross-Domain Knowledge Transfer}
Cross-domain knowledge transfer involves equipping LLMs with the ability to generalize knowledge across diverse and distinct fields. 
While this significantly expands their applicability, it also introduces challenges due to the complexity and diversity of domain-specific terminologies, ontologies, and reasoning patterns.
For example, transferring knowledge from chemistry to healthcare might require reconciling differing data structures and reasoning~frameworks.

Overcoming these challenges necessitates advancements in modular knowledge representation and transfer learning techniques. 
Future efforts could explore hybrid approaches that blend static embeddings with dynamic retrieval, enabling LLMs to adapt knowledge flexibly across domains without compromising depth. 
Additionally, creating standardized cross-domain benchmarks and datasets could facilitate systematic evaluation and foster innovation in multi-domain knowledge transfer methodologies.


\section{Conclusion}
\label{conclusion}
LLMs enhanced by domain-specific knowledge have shown remarkable potential and garnered increasing research interest. 
This survey systematically reviews LLM knowledge injection systems, exploring knowledge representation methods, integration strategies, and mechanisms for preserving model generality. 
We also summarize applications across biomedicine, chemistry, and computational social science domains.
By highlighting standard datasets, benchmarks, challenges, and future opportunities, we aim to provide a valuable resource that inspires further exploration of knowledge-enhanced LLMs for domain-specific challenges.

% \appendix
% \section*{Acknowledgments}




%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}





\end{document}

