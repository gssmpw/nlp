\section{RELATED WORK}\label{sec:related}

\subsection{Weighted Low Rank Applications}
Weighted low-rank techniques have been widely applied in broad machine learning applications such as grayscale-thermal detection~\cite{lwz+16}, object detection~\cite{twzl16,cxt22}, fault detection~\cite{dcz+17}, defect detection~\cite{mww+20,jld+20}, background estimation~\cite{dl17,dlr18}, multi-task learning~\cite{flct18}, robust visual representation learning~\cite{kjn15,wzx+18,wwz+19}, adversarial learning~\cite{lbb+19}, image restoration~\cite{psdx14,cyz+20}, image clustering and classification~\cite{sjs+11,wl17,wllz18,fzcw21,fzc+22,fzc+22b,kkk+23,pcp+23}, robust principal component analysis~\cite{xxf+21}, language models training~\cite{hhw+22}, language model compression~\cite{hhc+21}, weather prediction~\cite{wtl18}, tensor training~\cite{cwc+21,zwht+22}, domain generalization~\cite{smf+24} and many more.
Weighted low-rank techniques have also been widely used in signal processing for filter design and noise removal~\cite{lpw97,lhzc10,jycl15}. 

\subsection{Weighted Low Rank in Attention Mechanism} Particularly, a line of works shows that the attention matrix may have some low-rank property, even under softmax activation function by polynomial approximation methods, e.g., \cite{as23,kll+25_var,lls+25_prune,chl+24_rope,llss24_sparse,lls+24c,lss+24,lssy24,lssz24_tat,swxl24,xsl24,as24_iclr,as24_rope,as24b,hsk+24,hwg+24,hwl+24,hwsl24}. Thus, under such conditions, we may use weighted low-rank approximations for transformers' attention acceleration. For a few distinct columns or rows, empirically, we see that the attention matrix has some good patterns \cite{jlz+24,cls+25,lls+25_grok,cll+25_icl,smn+24}. In this work, we focus on the theoretical analysis and leave its empirical justification as future work.

\subsection{Weighted Low Rank Approximation and Acceleration}
Many previous works try to solve in an efficient way empirically and theoretically~\cite{mmh03,slv04,wj06,mv07,m08,ev12,mxzz13,mu14,rsw16,llr16,d16,dl17,dl17b,bwz19,hlx+19,zqz+19,swz+20,th21,yzls22,syyz23,zyls24}. Particularly, recently, \cite{wy24} proposes an algorithm to output a slightly higher rank output as a proxy to solve the weighted low-rank approximation problem efficiently.
\cite{llr16} develops an efficient framework for alternating minimization to get the weighted low-rank approximation. Similarly, \cite{syyz23} proposes a more robust framework to get the solution. 



\subsection{Sketching for Numerical Linear Algebra}
In this work, we use the sketching technique to accelerate a submodular optimization problem. We provide a brief overview of prior sketching work across various domains. Sketching has been utilized to enhance numerous continuous optimization challenges, including linear programming \cite{cls19,song19,b20,jswz21,sy21,gs22}, empirical risk minimization \cite{lsz19,qszz23}, the cutting plane method \cite{jlsw20}, calculating the John Ellipsoid \cite{ccly19,syyz22}, and many more.
Beyond continuous optimization, sketching has also been applied to several discrete optimization issues \cite{dsw22,sxz22,z22,jlsz23}. Additionally, sketching concepts have been implemented in addressing theoretical problems in large language models, such as exponential and softmax regression \cite{lsz23,gsy23,dls23_softmax,llss24}, and reducing the feature dimension of attention matrices \cite{dms23}. Sketching techniques prove valuable in various machine learning tasks, including matrix completion \cite{gsyz23}, adversarial training \cite{gqsw22}, training over-parameterized neural tangent kernel regression \cite{bpsw21,szz21,z22,als+22,hswz22}, matrix sensing \cite{dls23_sensing,qsz23}, kernel density estimation \cite{qrs+22}, and federated learning \cite{swyz23}. Moreover, the application of sketching extends to the theory of relational databases \cite{qjs+22}.