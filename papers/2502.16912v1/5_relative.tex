\section{LOWER BOUND ON OPT}\label{sec:relative}



We assume that $W$ has $r$ distinct rows and $r$ distinct columns. Then, we get rid of the dependence on $n$ in the degree.

\begin{theorem}[Implicitly in \cite{rsw16}]\label{thm:lower_bound_on_cost}
Assuming that $W$ has $r$ distinct rows and $r$ distinct columns, each entry of $A$ and $W$ needs $n^{\gamma}$ bits to represent. Assume $\OPT >0$. Then we know that, with high probability,  
\begin{align*}
    \OPT \geq 2^{-n^{\gamma} 2^{\wt{O}(rk^2/\epsilon)} }.
\end{align*}
\end{theorem}


\begin{proof}


We use $A_{i,*} \in \R^n$ denote the $i$-th row of $A$. We use $W_{i,*} \in \R^n$ to denote the $i$-th row of $W$. Let $(U_1)_{i,*}$ denote the $i$-th row of $U_1$.  
For any $n \times k$ matrix $U_1$ and for any $k \times n$ matrix $Z_1$, we have
\begin{align*}
 & ~ \| (U_1 Z_1 - A) \circ W \|_F^2 \\
 = & ~ \sum_{i=1}^n \| (U_1)_{i,*} Z_1 \diag( W_{i,*} ) - A_{i,*} \diag( W_{i,*} ) \|_2^2.
\end{align*}

Based on the observation that $W$ has $r$ distinct rows, we use group $g_{1,1}, g_{1,2}, \cdots , g_{1,r}$ to denote $r$ disjoint sets such that
\begin{align*}
    \cup_{i=1}^r g_{1,i} = [n]
\end{align*}
For any $i \in [r]$, for any $j_1, j_2 \in g_{1,i}$, we have $W_{j_1,*} = W_{j_2,*}$.


Thus, we can have 
\begin{align*}
    & ~ \sum_{i=1}^n \| (U_1)_{i,*} Z_1 \diag( W_{i,*} ) - A_{i,*} \diag( W_{i,*} ) \|_2^2 \\
    = & ~ \sum_{i=1}^r  \sum_{\ell \in g_{1,i}} \| (U_1)_{\ell,*} Z_1 \diag( W_{\ell,*} ) - A_{\ell,*} \diag( W_{\ell,*} ) \|_2^2  .
\end{align*}
We can sketch the objective function by choosing Gaussian matrices $S_1 \in \R^{n \times s_1}$ with $s_1 = O(k/\epsilon)$.
\begin{align*}
\sum_{i=1}^n \| (U_1)_{i,*} Z_1 \diag( W_{i,*} ) S_1 - A_{i,*} \diag( W_{i,*} ) S_1 \|_2^2 .
\end{align*}
Let $\wh{U}_1$ denote the optimal solution of the sketch problem,

\begin{align*}
    & ~ \wh{U}_1 =  \arg\min_{U_1 \in \R^{n \times k}} \\
    & ~ \sum_{i=1}^n \| (U_1)_{i,*} Z_1 \diag( W_{i,*} ) S_1 - A_{i,*} \diag( W_{i,*} ) S_1 \|_2^2 .
\end{align*}
By properties of $S_1$, plugging $\wh{U}_1$ into the original problem, we obtain
\begin{align*}
& ~ \sum_{i=1}^r  \sum_{\ell \in g_{1,i}} \| (\wh{U}_1)_{\ell,*} Z_1 \diag( W_{\ell,*} ) - A_{\ell,*} \diag( W_{\ell,*} ) \|_2^2  \\
\leq & ~  (1+\epsilon) \cdot \OPT.
\end{align*}

Let $R$ denote the set of all $\S(g_{1,i})$ (for all $i \in [r]$ and $|R| = r$)
 

Note that $\wh{U}_1$ also has the following form, for each $\ell \in L \subset [n]$ (Note that $|L| = r p$.)
\begin{align*}
(\wh{U}_1)_{\ell, *} 
= & ~ A_{\ell,*} \diag(W_{\ell,*}) S_1 \cdot ( Z_1 \diag( W_{\ell,*} ) S_1 )^\dagger \\
= & ~  A_{\ell,*} \diag(W_{\ell,*}) S_1 \cdot ( Z_1 \diag( W_{\ell,*} ) S_1 )^\top \\
& ~ \cdot ( ( Z_1 \diag( W_{\ell,*} ) S_1 ) ( Z_1 \diag( W_{\ell,*} ) S_1 )^\top )^{-1}.
\end{align*}

Recall the number of different $ \diag(W_{\ell,*})$ is at most $r$.

For each $k \times s_1$ matrix $Z_1 \diag( W_{\ell,*}) S_1$, we create $k \times s_1$ variables to represent it. Thus, we create $r$ matrices,
\begin{align*}
\{ Z_1 D_{W_{i,*}} S_1 \}_{i \in R}.
\end{align*}
For simplicity, let $P_{1,i} \in \R^{k \times s_1}$ denote $Z_1 \diag(W_{i,*}) S_1$. Then we can rewrite $\wh{U}^i$ as follows
\begin{align*}
    \wh{U}_1^i = A_{i,*} \diag(W_{i,*}) S_1  \cdot P_{1,i}^\top (P_{1,i} P_{1,i}^\top )^{-1}.
\end{align*}
If $P_{1,i} P_{1,i}^\top \in \R^{k \times k}$ has rank-$k$, then we can use Cramer's rule to write down the inverse of $P_{1,i} P_{1,i}^\top$.
For the situation, it is not full rank. We can guess the rank. Let $t_i \leq k$ denote the rank of $P_{1,i}$. Then, we need to figure out a maximal linearly independent subset of columns of $P_{1,i}$. We can also guess all the possibilities, which is at most $2^{O(k)}$. Because we have $r$ different $P_{1,i}$, the total number of guesses we have is at most $2^{O(rk)}$. Thus, we can write down $(P_{1,i} P_{1,i}^\top)^{-1}$ according to Cramer's rule. Note that $(P_{1,i} P_{1,i}^\top)^{-1}$ can be view as $P_a/P_b$ where $P_a$ is a polynomial and $P_b$ is another polynomial which is essentially $\det(P_{1,i} P_{1,i}^\top)$.

 
After $\wh{U}_1$ is obtained, we fix $\wh{U}_1$. We consider 
\begin{align*}
    & ~ \wh{U}_2 =  \arg\min_{U_2 \in \R^{n \times k}}  \| (\wh{U}_1 U_2^\top - A) \circ W \|_F^2 .
\end{align*}
In a similar way, we can get and write $\wh{U}_2$. 

Overall, by creating $l = O(rk^2/\epsilon)$ variables, we have rational polynomials $\wh{U}_1(x)$ and $\wh{U}_2(x)$. Note that $\wh{U}_1(x)$ only has $rp$ different rows, and same for $\wh{U}_2(x)$.

 Indeed, now we have only $2 r$ distinct denominators (w.l.o.g., assume the first $r$ columns are distinct and the first $r$ rows are distinct),
\begin{align*}
h_{1,i}(x) & = \det ( P_{1,i} P_{1,i}^\top ), \forall i \in[r] \\
h_{2,i}(x) & = \det ( P_{2,i} P_{2,i}^\top ), \forall i \in[r].
\end{align*}



Then, we can write down the following optimization problem,
\begin{align*}
\min _{x \in \R^l} & ~ p(x) / q(x) \\
\mathrm{s.t. } & ~ h_{1,i}^2(x) \neq 0, h_{2,i}^2(x) \neq 0, \forall i \in[r], \\
& ~ q(x)=\prod_{i=1}^r h_{1,i}^2(x) h_{2,i}^2(x),
\end{align*}
where $q(x)$ has degree $O(r k)$, the maximum coefficient in absolute value is 
\begin{align*}
( 2^{n^{\gamma}} )^ {O (r k )}
\end{align*}
 and the number of variables $O (r k^2 / \epsilon )$. However, that formulation $p(x)/q(x)$ is not a polynomial, to further make it a polynomial, we introduce variable $y$:
 \begin{align*}
\min _{x \in \R^l} & ~ p(x) y \\
\mathrm{s.t. } & ~ h_{1,i}^2(x) \neq 0, h_{2,i}^2(x) \neq 0, \forall i \in[r], \\
& ~ q(x)=\prod_{i=1}^r h_{1,i}^2(x) h_{2,i}^2(x) \\
& ~ q(x)y - 1 =0.
\end{align*}

Applying Theorem~\ref{thm:jpt13}, with parameters
\begin{align*}
& \m= O(r), \v = O(rk^2/\epsilon),  \d = O(r) , \\
& \H = 2^{O( n^{\gamma} rk ) } ,  \wt{H} = O(\H) .
\end{align*}

we can achieve the following minimum nonzero cost: 
\begin{align*}
\geq  ~ 2^{-2^{3 \v \log (\d)} \log( \wt{\H}) } 
\geq  ~ 
2^{-n^\gamma 2^{\wt{O} (r k^2 / \epsilon )}  } .
\end{align*}
 Thus, we complete the proofs.
\end{proof}