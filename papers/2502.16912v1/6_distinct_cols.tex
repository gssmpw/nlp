
\section{FEW DISTINCT COLUMNS}\label{sec:few_distinct_columns}

In this section, we try to estimate the $\OPT$ value. 

\begin{theorem}\label{thm:few_distinct_columns}
Given a matrix $A$ and $W$, each entry can be written using $O(n^{\gamma})$ bits for $\gamma >0$.
Given $A \in \R^{n \times n}, W \in \R^{n \times n}, 1 \leq k \leq n$.  Assume that $W$ has $r$ distinct columns and rows.
   
Then, with high probability,  
one can output a number $\Lambda$ in time $n^{1+\gamma} \cdot p \cdot 2^{O (k^2 r / \epsilon )} $ such that 
\begin{align*}
    \OPT \leq \Lambda \leq(1+\epsilon) \OPT .
\end{align*}
\end{theorem}
\begin{proof}

Let $U_1^*, U_2^* \in \R^{n \times k}$ denote the matrices satisfying 
\begin{align*}
    \| W \circ (U_1^* (U_2^*)^\top - A) \|_F^2 = \OPT.
\end{align*}


We use $A_{i,*} \in \R^n$ denote the $i$-th row of $A$. We use $W_{i,*} \in \R^n$ to denote the $i$-th row of $W$. Let $(U_1)_{i,*}$ denote the $i$-th row of $U_1$. Let $Z_1 = (U_2^*)^\top$.  
For any $n \times k$ matrix $U_1$,
\begin{align*}
 & ~ \| (U_1 Z_1 - A) \circ W \|_F^2 \\
 = & ~ \sum_{i=1}^n \| (U_1)_{i,*} Z_1 \diag( W_{i,*} ) - A_{i,*} \diag( W_{i,*} ) \|_2^2.
\end{align*}

Based on the observation that $W$ has $r$ distinct rows, we use group $g_{1,1}, g_{1,2}, \cdots , g_{1,r}$ to denote $r$ disjoint sets such that
\begin{align*}
    \cup_{i=1}^r g_{1,i} = [n]
\end{align*}
For any $i \in [r]$, for any $j_1, j_2 \in g_{1,i}$, we have $W_{j_1,*} = W_{j_2,*}$.

Next, based on assumptions on $W$ and $A$, we use $g_{1,i,1}$, $g_{1,i,2}$, $g_{1,i,p}$ to denote $p$ groups such that 
\begin{align*}
    \cup_{j=1}^p g_{1,i,j} = g_{1,i} .
\end{align*}
For any $i \in [r]$, for any $j \in [p]$, for any $\ell_1, \ell_2 \in g_{1,i,j}$, we have  $(W_{\ell_1,*} \circ A_{\ell_1,*}) = (W_{\ell_2,*} \circ A_{\ell_2,*})$.

Let $\S(g_{1,i,j})$ denote the smallest index from set $g_{1,i,j}$

Thus, we can have 
\begin{align*}
    & ~ \sum_{i=1}^n \| (U_1)_{i,*} Z_1 \diag( W_{i,*} ) - A_{i,*} \diag( W_{i,*} ) \|_2^2 \\
    = & ~ \sum_{i=1}^r \sum_{j =1}^p \sum_{\ell \in g_{1,i,j}} \| (U_1)_{\ell,*} Z_1 \diag( W_{\ell,*} ) \\
     & ~ \quad\quad\quad\quad\quad\quad - A_{\ell,*} \diag( W_{\ell,*} ) \|_2^2 \\
    = & ~\sum_{i=1}^r \sum_{j =1}^p |g_{1,i,j}| \cdot \| (U_1)_{\S(g_{1,i,j}),*} Z_1 \diag( W_{ \S(g_{1,i,j}),*} ) \\
    & ~ \quad\quad\quad\quad\quad\quad - A_{\S(g_{1,i,j}),*} \diag( W_{\S(g_{1,i,j}),*} ) \|_2^2 .
\end{align*}
We can sketch the objective function by choosing Gaussian matrices $S_1 \in \R^{n \times s_1}$ with $s_1 = O(k/\epsilon)$.
\begin{align*}
& ~ \sum_{i=1}^r \sum_{j =1}^p |g_{1,i,j}| \cdot \| (U_1)_{\S(g_{1,i,j}),*} Z_1 \diag( W_{ \S(g_{1,i,j}),*} ) S_1 \\
& ~ \quad\quad\quad\quad\quad\quad - A_{\S(g_{1,i,j}),*} \diag( W_{\S(g_{1,i,j}),*} ) S_1 \|_2^2.
\end{align*}
Let $\wh{U}_1$ denote the optimal solution of the sketch problem,
\begin{align*}
    \wh{U}_1 = & ~ \arg\min_{U_1} \sum_{i=1}^r \sum_{j =1}^p |g_{1,i,j}| \\
    & ~ \cdot \| (U_1)_{\S(g_{1,i,j}),*} Z_1 \diag( W_{ \S(g_{1,i,j}),*} ) S_1 \\
    & ~ \quad - A_{\S(g_{1,i,j}),*} \diag( W_{\S(g_{1,i,j}),*} ) S_1 \|_2^2.
\end{align*}
By properties of $S_1$, plugging $\wh{U}_1$ into the original problem, we obtain
\begin{align*}
& ~ \sum_{i=1}^r \sum_{j =1}^p |g_{1,i,j}| \cdot \| (U_1)_{\S(g_{1,i,j}),*} Z_1 \diag( W_{ \S(g_{1,i,j}),*} ) \\
& ~ \quad\quad - A_{\S(g_{1,i,j}),*} \diag( W_{\S(g_{1,i,j}),*} ) \|_2^2 \leq (1+\epsilon) \cdot \OPT.
\end{align*}

Let $R$ denote the set of all $\S(g_{1,i})$ (for all $i \in [r]$ and $|R| = r$).

Let $L$ denote the set of all $\S(g_{1,i,j})$ (for all $i \in [r]$, $j \in [p]$ and $|L| = rp$).

Note that $\wh{U}_1$ also has the following form, for each $\ell \in L \subset [n]$ (Note that $|L| = r p$.)
\begin{align*}
(U_1)_{\ell, *} 
= & ~ A_{\ell,*} \diag(W_{\ell,*}) S_1 \cdot ( Z_1 \diag( W_{\ell,*} ) S_1 )^\dagger \\
= & ~  A_{\ell,*} \diag(W_{\ell,*}) S_1 \cdot ( Z_1 \diag( W_{\ell,*} ) S_1 )^\top \\
& ~ \cdot ( ( Z_1 \diag( W_{\ell,*} ) S_1 ) ( Z_1 \diag( W_{\ell,*} ) S_1 )^\top )^{-1}.
\end{align*}

Recall the number of different $A_{\ell,*} \diag(W_{\ell,*})$ is at most $rp$, and the number of different $ \diag(W_{\ell,*})$ is at most $r$. For each $k \times s_1$ matrix $Z_1 \diag( W_{\ell,*}) S_1$, we create $k \times s_1$ variables to represent it. Thus, we create $r$ matrices,
\begin{align*}
\{ Z_1 \diag( W_{i,*} ) S_1 \}_{i \in R}. 
\end{align*}
For simplicity, let $P_{1,i} \in \R^{k \times s_1}$ denote $Z_1 \diag(W_{i,*}) S_1$. Then we can rewrite $(\wh{U}_1)_{i,*}$ as follows
\begin{align*}
    (\wh{U}_1)_{i,*} = A_{i,*} \diag(W_{i,*}) S_1  \cdot P_{1,i}^\top (P_{1,i} P_{1,i}^\top )^{-1}.
\end{align*}
If $P_{1,i} P_{1,i}^\top \in \R^{k \times k}$ has rank-$k$, then we can use Cramer's rule to write down the inverse of $P_{1,i} P_{1,i}^\top$.
For the situation, it is not full rank. We can guess the rank. Let $t_i \leq k$ denote the rank of $P_{1,i}$. Then, we need to figure out a maximal linearly independent subset of columns of $P_{1,i}$. We can also guess all the possibilities, which is at most $2^{O(k)}$. Because we have $r$ different $P_{1,i}$, the total number of guesses we have is at most $2^{O(rk)}$. Thus, we can write down $(P_{1,i} P_{1,i}^\top)^{-1}$ according to Cramer's rule. Note that $(P_{1,i} P_{1,i}^\top)^{-1}$ can be view as $P_a/P_b$ where $P_a$ is a polynomial and $P_b$ is another polynomial which is essentially $\det(P_{1,i} P_{1,i})$.


After $\wh{U}_1$ is obtained, we will fix $\wh{U}_1$ in the next round.

For any $n \times k$ matrix $U_2$, we can rewrite 
\begin{align*}
 & ~ \| (\wh{U}_1 U_2^\top  - A ) \circ \|_F^2 \\
 = & ~ \sum_{i=1}^n \| \diag(  W_{*,i} ) \wh{U}_1   (U_2^\top)_{*,i}   - \diag(W_{*,i}) A_{*,i} \|_2^2 .
\end{align*}



Based on the observation that $W$ has $r$ columns rows, we use group $g_{2,1}, g_{2,2}, \cdots , g_{2,r}$ to denote $r$ disjoint sets such that
\begin{align*}
    \cup_{i=1}^r g_{2,i} = [n].
\end{align*}
For any $i \in [r]$, for any $j_1, j_2 \in g_{2,i}$, we have $W_{*,j_1} = W_{*,j_2}$.

Next, based on assumptions on $W$ and $A$, we use $g_{2,i,1}$, $g_{2,i,2}$, $g_{2,i,p}$ to denote $p$ groups such that 
\begin{align*}
    \cup_{j=1}^p g_{2,i,j} = g_{2,i}.
\end{align*}
For any $i \in [r]$, for any $j \in [p]$, for any $\ell_1, \ell_2 \in g_{2,i,j}$, we have  $(W_{*,\ell_1} \circ A_{*,\ell_1}) = (W_{*,\ell_2} \circ A_{*,\ell_2})$.

Let $\S(g_{2,i,j})$ denote the smallest index from set $g_{2,i,j}$

Thus, we can have 
\begin{align*}
    & ~ \sum_{i=1}^n \| \diag( W_{*,i} ) \wh{U}_1   (U_2^\top )_{*,i}  -  \diag( W_{*,i} ) A_{*,i} \|_2^2 \\
    = & ~ \sum_{i=1}^r \sum_{j =1}^p \sum_{\ell \in g_{1,i,j}} \|  \diag( W_{*,\ell} ) \wh{U}_1 (U_2^\top )_{*,\ell} \\
    & ~ \quad\quad\quad\quad\quad\quad - \diag( W_{*,\ell} ) A_{*,\ell}  \|_2^2 \\
    = & ~\sum_{i=1}^r \sum_{j =1}^p |g_{2,i,j}| \cdot \|  \diag( W_{ *, \S(g_{2,i,j}) } ) \wh{U}_1 (U_2^\top )_{*, \S(g_{2,i,j})} \\
    & ~ \quad\quad\quad\quad\quad\quad -  \diag( W_{*, \S(g_{1,i,j})} ) A_{*,\S(g_{1,i,j})} \|_2^2 .
\end{align*}
We can sketch the objective function by choosing Gaussian matrices $S_2 \in \R^{n \times s_1}$ with $s_2 = O(k/\epsilon)$.
\begin{align*}
& ~ \sum_{i=1}^r \sum_{j =1}^p |g_{2,i,j}| \cdot \| S_2  \diag( W_{ \S(g_{2,i,j}),*} ) \wh{U}_1  (U_2^\top )_{*, \S(g_{2,i,j}) } \\
& ~ \quad\quad\quad\quad - S_2 \diag( W_{*, \S(g_{2,i,j})} )  A_{*, \S(g_{2,i,j}) }  \|_2^2.
\end{align*}
Let $\wh{U}_1$ denote the optimal solution of the sketch problem,
\begin{align*}
    \wh{U}_1 = & ~ \arg\min_{U_1} \sum_{i=1}^r \sum_{j =1}^p |g_{2,i,j}| \\
    & ~  \cdot \| S_2  \diag( W_{ \S(g_{2,i,j}),*} ) \wh{U}_1  (U_2^\top )_{*, \S(g_{2,i,j}) } \\
    & ~ - S_2 \diag( W_{*, \S(g_{2,i,j})} )  A_{*, \S(g_{2,i,j}) }  \|_2^2.
\end{align*}
By properties of $S_1$, plugging $\wh{U}_2$ into the original problem, we obtain
\begin{align*}
& ~ \sum_{i=1}^r \sum_{j =1}^p |g_{2,i,j}| \cdot \|  \diag( W_{ \S(g_{2,i,j}),*} ) \wh{U}_1  ( \wh{U}_2^\top )_{*, \S(g_{2,i,j}) } \\
& ~ -  \diag( W_{*, \S(g_{2,i,j})} )  A_{*, \S(g_{2,i,j}) }  \|_2^2\leq (1+\epsilon) \cdot \OPT.
\end{align*}

Let $R$ denote the set of all $\S(g_{1,i})$ (for all $i \in [r]$ and $|R| = r$).

Let $L$ denote the set of all $\S(g_{1,i,j})$ (for all $i \in [r]$, $j \in [p]$ and $|L| = rp$).

Note that $\wh{U}_1$ also has the following form, for each $\ell \in L \subset [n]$ (Note that $|L| = r p$.)
\begin{align*}
(U_1)_{\ell, *} 
= & ~  ( S_2 \diag( W_{*,\ell} ) \wh{U}_1 )^\dagger \cdot S_2 \diag(W_{*,\ell})  A_{*,\ell} \\
= & ~ (  ( S_2 \diag( W_{*,\ell} ) \wh{U}_1 ) ( S_2 \diag( W_{*,\ell} ) \wh{U}_1)^\top )^{-1} \\
& ~ \cdot S_2 \diag( W_{*,\ell} ) \wh{U}_1 \cdot S_2 \diag(W_{*,\ell})  A_{*,\ell}.
\end{align*}

Recall the number of different $A_{*,\ell} \diag(W_{*,\ell})$ is at most $rp$, and the number of different $ \diag(W_{*,\ell})$ is at most $r$.

For each $s_2 \times k$ matrix $S_2 \diag( W_{*,i} ) \wh{U}_1$, we create $s_2 \times k$ variables to represent it. Thus, we create $r$ matrices,
\begin{align*}
\{ S_2 \diag( W_{*,i} ) \wh{U}_1 \}_{i \in R}.
\end{align*}
For simplicity, let $P_{2,i} \in \R^{k \times s_2}$ denote $S_2 \diag( W_{*,i} ) \wh{U}_1$. Then we can rewrite $\wh{U}_2$ as follows
\begin{align*}
    (\wh{U}_2^\top)_{*,i} = (P_{2,i} P_{2,i}^\top)^{-1}  P_{2,i} S_2 \diag(W_{*,i})  A_{*,i}.
\end{align*}

In a similar way, we can write $\wh{U}_2$. Overall, by creating $l = O(rk^2/\epsilon)$ variables, we have rational polynomials $\wh{U}_1(x)$ and $\wh{U}_2(x)$. Note that $\wh{U}_1(x)$ only has $rp$ different rows, and same for $\wh{U}_2(x)$.

Putting it all together, we can write the objective function,
\begin{align*}
\min_{x \in \R^l} & ~ \| ( \wh{U}_1(x) \wh{U}_2(x)^\top - A ) \circ W \|_F^2 \\
\mathrm{~s.t.~}& ~ h_{1,i}(x) \neq 0 ,  \forall i \in [r] \\
& ~ h_{2,i}(x) \neq 0, \forall i \in [r].
\end{align*}

Note that $\wh{U}_1(x) \wh{U}_{2}(x) \circ W$ only has $rp$ distinct rows. Also, $A \circ W$ only has $rp$ distinct rows. Writing down the objective function $\| ( \wh{U}_1(x) \wh{U}_2(x)^\top - A ) \circ W \|_F^2$ only requires $n (rp) \cdot \poly(kr/\epsilon)$ time.

Combining the binary search explained in Lemma~\ref{lem:warmup} 
with the lower bound on cost (Theorem~\ref{thm:lower_bound_on_cost}) we obtained, we can find the solution for the original problem in time,
\begin{align*}
(np \cdot \poly(kr/\epsilon) + n 2^{\wt{O} (rk^2 / \epsilon) }  ) \cdot n^{\gamma} = n^{1+\gamma} p \cdot 2^{\wt{O}(rk^2/\epsilon)}.
\end{align*}

\end{proof}