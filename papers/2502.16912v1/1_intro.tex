\section{INTRODUCTION}\label{sec:intro}
Weighted low-rank approximation problem~\citep{sj03} is a fundamental numerical linear algebra problem related to matrix completion~\citep{llr16}, faster SVD decomposition~\citep{bjs14,bks16}, and principal component analysis~\citep{xxf+21}. 
It is a prevalent focus over recent years and has been widely applied in machine learning in broad practical applications, such as vision detection~\citep{cxt22}, representation learning~\citep{wwz+19}, image classification~\citep{pcp+23}, language model~\citep{hhc+21}, weather prediction~\citep{wtl18}, and many more.
We can formulate the weighted low-rank approximation problem as below.
\begin{definition}
Given two $n \times n$ matrices $A$ and $W$, the goal is to find two $n \times k$ matrices $U,V$ such that $\| W \circ (UV^\top - A) \|_F^2 \leq (1+\epsilon) \min_{\{A' | \mathrm{rank}(A')=k \}} ~ \| W \circ (A' - A ) \|_F^2$. Here, $W\circ B$ denotes a matrix with entries given by $W_{i,j} B_{i,j}$, where $\circ$ is the Hadamard product operation.
\end{definition}

Here, we provide an example to show that attention computation can be formulated as a weighted low-rank approximation problem. 
Let $Q, K, V \in \mathbb{R}^{n \times d}$ be the query, key, and value matrix. Let $A := \mathsf{Activation}(Q K^\top) \in \mathbb{R}^{n \times n}$ be the attention matrix, e.g, using $\mathsf{Softmax}$ as activation. Let $W \in \mathbb{R}^{n \times n}$ be the attention mask. Then, we have the attention output as $(A \circ W) V$. We can clearly see that $A \circ W$ is the target of the weighted low-rank approximation problem.

There is some well-known hardness in the weighted low-rank approximation problem. \cite{gg11,rsw16} showed that, when we want to find an approximate solution with $\epsilon = 1/\poly(n)$, the weighted low-rank approximation problem is ``equivalent '' to $\mathsf{NP}$-$\mathsf{hard}$ problem.   
To practically fix it, under certain assumptions (e.g., the weighted matrix $W$ has $r$ distinct columns/rows for small $r$), we may have a polynomial time complexity algorithm to solve the weighted low-rank approximation problem.
Thus, there are many theoretical and empirical works studying how we are able to solve the weighted low-rank approximation problem efficiently (e.g., \cite{ev12,zqz+19,syyz23,wy24} and many more).  

However, the best previous works still need to solve weighted low-rank approximation in $\Omega(n^2)$ if $A$ has $\Omega(n^2)$ non-zero entries and $W$ has $\Omega(n^2)$ non-zero-entries. 
Therefore, in this work, we raise a natural fundamental mathematical question:
\begin{center}
{\it
Is there some dense regime setting for $A$ and $W$ so that we can solve the weighted low-rank approximation problem in almost linear $n$ time?
}
\end{center}

The answer is positive. 
When we have mild practical assumptions (see detail in Assumption~\ref{ass:W}), we can solve the weighted low-rank approximation problem in almost linear complexity, i.e., $n^{1+o(1)}$. We state our main results in the following, whose proof is in Section~\ref{sec:recover_solution}.


\begin{theorem}[Main result] \label{thm:main}
Let  $A$ and $ W$ denote two $n \times n$ matrices. 
Assume each entry of $A$ and $W$ needs $n^{\gamma}$ bits\footnote{Each entry in a matrix can be represented by $n^\gamma$ bits. In a real dataset, if we use the float32 format, then as long as $n^\gamma \ge 32$, our assumption holds.} to represent, where $\gamma = o(1)$.
Let $r$ be the number of distinct columns and rows of $W$ (Definition~\ref{def:distinct}).  
Assume $A \circ W$ has at most $r \cdot p$ distinct columns and at most $r \cdot p$ distinct rows, where $p = n^{o(1)}$. 
Assume that $k^2r= O(\log n / \log \log n)$. For every constant $\epsilon>0$, there is an algorithm running in time,   
$n^{1+o(1)}$ time which outputs a factorization (into an $n \times k$ and a $k \times n$ matrix) of a rank-$k$ matrix $\wh{A}$ for which  
\begin{align*}
\|W \circ(A-\wh{A})\|_{F}^{2} \leq(1+\epsilon) \OPT,
\end{align*}
with probability at least $0.99$, where 
\begin{align*}
    \OPT:= \min_{\{A_k| \mathrm{rank}(A_k)=k \}} \| W \circ ( A - A_k ) \|_F^2.
\end{align*}
\end{theorem}

Theorem~\ref{thm:main} shows that, with high probability, we can solve the weighted low-rank approximation problem in almost linear complexity when there is a small number of distinct rows and columns in the target matrix. Furthermore,  for any $\delta \in (0,0.1)$, if we conduct $O(\log(1/\delta))$ repetitions and take the median of results as the final output, our success probability can be boosted to $1-\delta$. Note that it is a standard way to boost the success probability from constant 0.99 to $1-\delta$ for any $\delta \in (0,0.1)$. The high-level idea is a majority vote, which is equivalent to taking the median. 


\paragraph{Comparison with Previous Works. }
Our results are beyond \cite{rsw16} in the following sense. Under the condition in Theorem~\ref{thm:main}, their algorithm requires $(\nnz(A) + \nnz(W )) \cdot n^{o(1)} +
n^{1+o(1)}$ running time complexity, where $\nnz(A)$ means number of non-zero entries in $A$.  When $A$ or $W$ are dense matrix, their complexity is $n^{2+o(1)}$, while our complexity is $n^{1+o(1)}$. 
We use a similar high-level proof framework as \cite{rsw16}. However, as our problem setup changes, all analyses need to be updated accordingly. In particular, Theorem~\ref{thm:few_distinct_columns} in Section~\ref{sec:few_distinct_columns} is brand new.
The high-level intuition is that we need to carefully handle the distinct patterns in the analysis to avoid the quadratic time complexity. Then, during the matrix product, we can rewrite the summation order. Then, by fine-grained analysis, we can group many operations together due to the few distinct patterns and bypass the quadratic complexity.



{\bf Roadmap.} 
In Section~\ref{sec:related}, we provide related work of weighted low-rank. In Section~\ref{sec:prelim}, we provide the preliminary of our problem setup and some tools from previous works. Then, in Section~\ref{sec:additive}, we provide a warm up of our analysis to introduce our technique. 
In Section~\ref{sec:relative}, we provide the lower bound of cost used in binary search. 
In Section~\ref{sec:few_distinct_columns}, we introduce how to estimate the $\OPT$ value. 
In Section~\ref{sec:recover_solution}, we can successfully recover our solutions. 
Finally, we state our conclusion in Section~\ref{sec:conclusion}.