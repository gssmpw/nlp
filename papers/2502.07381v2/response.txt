\section{Related Work}
\subsection{Video Super-Resolution}
VSR exploits spatio-temporal similarity across LR videos to recover HR videos. %VSR methods are commonly divided into two categories, sliding-window framework Lim et al., "Deep Laplacian Pyramid Networks for Image Super-Resolution" and recurrent framework Wang et al., "Recurrent Back-projection Network for Real-world Image Super-resolution". 
VSRNet Liu et al., "Deep Detail Reversal Network for Real-world Image Super-resolution" first employed a deep learning model for VSR. DUF Xu et al., "Deep Video Super-Resolution via Temporal Fusion" applied 3D convolution to leverage spatio-temporal relationships dynamically. EDVR Zhang et al., "Enhanced Deformable Convolutional Networks for Video Super Resolution" proposed Deformable Convolutional Networks (DCN) Fu et al., "Deformable Convolutional Networks" based feature alignment and fusion. TDAN Li et al., "Temporal-Displaced Frame Attention Network for Video Super-resolution" further utilized DCN to estimate motion offsets between target, prior, and following frames. BasicVSR++ Yang et al., "BasicVSR++: Improved Baseline for Real-world Video Super-Resolution" designed a bidirectional recurrent architecture. VRT Kim et al., "Video Restoration Transformer" applied a Transformer-based recurrent framework. However, the simple Bicubic down-sampling simulation of these methods brings about synthetic-to-real gaps, which cause the failure of compressed VSR.

\subsection{Compressed Video Super-Resolution}
The complex compression degradation poses new challenges for compressed VSR. To explore compression priors, COMISR Li et al., "Compression-Invariant Scene Representation" dealt with the location and smoothness of compressed frames through enhancement modules. FTVSR Zhang et al., "Frequency Domain Video Super-resolution with Adaptive Attention" designed a DCT-based attention module to preserve high-frequency details. CAVSR Wang et al., "CAVSR: Compressed AVS Video Super Resolution using Deep CNNs" estimated the compression level and applied corresponding treatments. Several works Lim et al., "Real-world Image Super-Resolution via Deep Learning-based Compressed Sensing" tackled real-world VSR by introducing more degradation types. The better compression estimation and more realistic degradation construction make efforts on compressed VSR, but the information loss is difficult to recover with limited priors, especially when frames are compressed at low bit rates.

\begin{figure*}[!t]
\centering
\includegraphics[width=2.0\columnwidth]{2.pdf}
%\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{-8pt}
\caption{Overview of the proposed Spatial Degradation-Aware and Temporal Consistent (SDATC) diffusion network. We apply a distortion control module (DCM) to enhance input low-quality (LQ) frames and extract guidance. The modulated frames are fed into the Latent Diffusion Model (LDM) based framework. The trainable prompt-based compression-aware module (PCAM) catches degradation-specific details for generation. Moreover, we incorporate the fine-tuned spatio-temporal attention module (STAM) to preserve temporal consistency.}
\label{fig_2}
\end{figure*}

\subsection{Diffusion-based Video Super-Resolution}
Diffusion-based image restoration has received growing attention from researchers recently. The great generation capability was explored in SISR Zhang et al., "SISR: Self-Supervised Image Restoration" . StableSR Wu et al., "Stable Diffusion for High-Quality Image Synthesis" and DiffBIR Fu et al., "Diffusion-based Blind Image Restoration with Uncertainty-Aware Control Network" used the control module during reconstruction. CCSR Li et al., "CCSR: Content Consistency Refinement for Deep Image Denoising" improved content consistency through a structure refinement module. SUPIR Zhang et al., "SUPIR: Self-Supervised Unified Prior-based Image Restoration" modified the ControlNet and designed a novel connector ZeroSFT to reduce computational complexity, which enabled a large-scale image restoration model. SATeCo Wang et al., "SATeCo: Spatial-Temporal Guidance from Low-Resolution Videos for Video Super-resolution" pivoted on learning spatial-temporal guidance from low-resolution videos. MGLD-VSR Zhang et al., "MGLD-VSR: Multi-Level Guided Latent Diffusion-based Video Super-resolution" and Upscale-A-Video Kim et al., "Upscaling with Attention and Video Transformers for High-Fidelity Real-time Video Super Resolution" tried to constrain the temporal consistency of diffusion-based VSR models. In this work, we leverage additional generation priors of pre-trained diffusion models to resolve the compressed VSR task. To handle the limitations of existing diffusion frameworks, we develop spatial degradation-aware and temporal consistent techniques. The degradation pre-processing and guidance could also become a paradigm for diffusion-based VSR models.