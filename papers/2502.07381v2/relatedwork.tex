\section{Related Work}
\subsection{Video Super-Resolution}
VSR exploits spatio-temporal similarity across LR videos to recover HR videos. %VSR methods are commonly divided into two categories, sliding-window framework \cite{VSRnet,VESPCN,DUF,EDVR,TDAN} and recurrent framework \cite{RLSP,RBPN,BasicVSR,BasicVSR++,VRT}. 
VSRNet \cite{VSRnet} first employed a deep learning model for VSR. DUF \cite{DUF} applied 3D convolution to leverage spatio-temporal relationships dynamically. EDVR \cite{EDVR} proposed Deformable Convolutional Networks (DCN) \cite{DCN} based feature alignment and fusion. TDAN \cite{TDAN} further utilized DCN to estimate motion offsets between target, prior, and following frames. BasicVSR++ \cite{BasicVSR++} designed a bidirectional recurrent architecture. VRT \cite{VRT} applied a Transformer-based recurrent framework. However, the simple Bicubic down-sampling simulation of these methods brings about synthetic-to-real gaps, which cause the failure of compressed VSR.

\subsection{Compressed Video Super-Resolution}
The complex compression degradation poses new challenges for compressed VSR. To explore compression priors, COMISR \cite{COMISR} dealt with the location and smoothness of compressed frames through enhancement modules. FTVSR \cite{FTVSR} designed a DCT-based attention module to preserve high-frequency details. CAVSR \cite{CAVSR} estimated the compression level and applied corresponding treatments. Several works \cite{RealVSR,RealESRGAN,RealBasicVSR} tackled real-world VSR by introducing more degradation types. The better compression estimation and more realistic degradation construction make efforts on compressed VSR, but the information loss is difficult to recover with limited priors, especially when frames are compressed at low bit rates.

\begin{figure*}[!t]
\centering
\includegraphics[width=2.0\columnwidth]{2.pdf}
%\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{-8pt}
\caption{Overview of the proposed Spatial Degradation-Aware and Temporal Consistent (SDATC) diffusion network. We apply a distortion control module (DCM) to enhance input low-quality (LQ) frames and extract guidance. The modulated frames are fed into the Latent Diffusion Model (LDM) based framework. The trainable prompt-based compression-aware module (PCAM) catches degradation-specific details for generation. Moreover, we incorporate the fine-tuned spatio-temporal attention module (STAM) to preserve temporal consistency.}
\label{fig_2}
\end{figure*}

\subsection{Diffusion-based Video Super-Resolution}
Diffusion-based image restoration has received growing attention from researchers recently. The great generation capability was explored in SISR \cite{SR3}. StableSR \cite{StableSR} and DiffBIR \cite{StableSR} used the control module during reconstruction. CCSR \cite{CCSR} improved content consistency through a structure refinement module. SUPIR \cite{SUPIR} modified the ControlNet and designed a novel connector ZeroSFT to reduce computational complexity, which enabled a large-scale image restoration model. SATeCo \cite{SATeCo} pivoted on learning spatial-temporal guidance from low-resolution videos. MGLD-VSR \cite{MGLD-VSR} and Upscale-A-Video \cite{Upscale-A-Video} tried to constrain the temporal consistency of diffusion-based VSR models. In this work, we leverage additional generation priors of pre-trained diffusion models to resolve the compressed VSR task. To handle the limitations of existing diffusion frameworks, we develop spatial degradation-aware and temporal consistent techniques. The degradation pre-processing and guidance could also become a paradigm for diffusion-based VSR models.