%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage{subfigure}
\usepackage{amsfonts} % 或者 \usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[table]{xcolor}

% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Spatial Degradation-Aware and Temporal Consistent Diffusion Model \\ for Compressed Video Super-Resolution}

\author{
Hongyu An$^{1,2}$
\and
Xinfeng Zhang$^1$\thanks{Corresponding author.}\and
Shijie Zhao$^2$\And
Li Zhang$^2$\\
\affiliations
$^1$School of Computer Science and Technology, University of Chinese Academy of Sciences\\
$^2$Bytedance Inc.\\
\emails
anhongyu22@mails.ucas.ac.cn,
xfzhang@ucas.ac.cn,
\{zhaoshijie.0526, lizhang.idm\}@bytedance.com
}

\begin{document}

\maketitle 

\begin{abstract}
Due to limitations of storage and bandwidth, videos stored and transmitted on the Internet are usually low-quality with low-resolution and compression noise. Although video super-resolution (VSR) is an efficient technique to enhance video resolution, relatively VSR methods focus on compressed videos. Directly applying general VSR approaches leads to the failure of improving practical videos, especially when frames are highly compressed at a low bit rate. Recently, diffusion models have achieved superior performance in low-level visual tasks, and their high-realism generation capability enables them to be applied in VSR. To synthesize more compression-lost details and refine temporal consistency, we propose a novel Spatial Degradation-Aware and Temporal Consistent (SDATC) diffusion model for compressed VSR. Specifically, we introduce a distortion Control module (DCM) to modulate diffusion model inputs and guide the generation. Next, the diffusion model executes the denoising process for texture generation with fine-tuned spatial prompt-based compression-aware module (PCAM) and spatio-temporal attention module (STAM). PCAM extracts features to encode specific compression information dynamically. STAM extends the spatial attention mechanism to a spatio-temporal dimension for capturing temporal correlation. Extensive experimental results on benchmark datasets demonstrate the effectiveness of the proposed modules in enhancing compressed videos. 
%The codes will be publicly available.
\end{abstract}

\section{Introduction}

Limited by the high costs of memory and transmission, videos are usually down-sampled and compressed in practical scenarios. Video super-resolution (VSR) is an efficient technique to improve video visual quality, which aims to recover continuous high-resolution (HR) frames from corresponding low-resolution (LR) frames. With the development of deep learning, sliding windows network-based %\cite{VSRnet,DUF,EDVR,TDAN} 
and recurrent network-based 
%\cite{RLSP,RBPN,BasicVSR,BasicVSR++, VRT} 
approaches have achieved great breakthroughs. Unfortunately, these methods hardly consider real-world videos stored and delivered on the Internet or mobile devices, which are always compressed with different levels \cite{CAVSR}. Therefore, existing VSR methods may regard compression artifacts as textures and enlarge them during reconstruction. Meanwhile, compression non-adaptive VSR models lead to blurred restoration.

\begin{figure}[!t]
\centering
\subfigure[Image super-resolution diffusion model.]
{\includegraphics[width=0.95\columnwidth]{1_a.pdf}\vspace{-8pt}\label{fig_1a}}\vspace{-6pt}
\subfigure[Spatial improved image super-resolution diffusion model.]{\includegraphics[width=0.95\columnwidth]{1_b.pdf}\vspace{-8pt}\label{fig_1b}}\vspace{-6pt}
\subfigure[Temporal improved video super-resolution diffusion model.]{\includegraphics[width=0.95\columnwidth]{1_c.pdf}\vspace{-8pt}\label{fig_1c}}\vspace{-6pt}
\caption{Comparison of different diffusion processes for image super-resolution. Compared with (a) StableSR, our SDATC introduces spatial (b) and temporal (c) guidance for better generation.}
\vspace{-10pt}
\end{figure}

To address the above issues, several works have investigated VSR on compressed videos. COMISR \cite{COMISR} exploited compression properties to reduce distortions. FTVSR \cite{FTVSR} proposed a frequency Transformer for compressed VSR. CAVSR \cite{CAVSR} utilized video stream information and predicted the compression ratio. Although these approaches improved compressed VSR performance with additional encoding priors beyond compressed frames, restoring truncated textures is still challenging. The quantization process during compression inevitably introduces information loss, thus the lack of low-quality (LQ) video priors makes it difficult to reconstruct pleasant details.

Inspired by the vivid generation capability of diffusion models, we apply generation priors to solve the above challenges. Recently, some studies have handled the image super-resolution (SR) task with a diffusion model. SR3 \cite{SR3} pioneeringly utilized denoising diffusion probabilistic models (DDPM) to achieve image SR. The innovative StableSR \cite{StableSR} and DiffBIR \cite{DiffBIR} leveraged ControlNet \cite{ControlNet} to balance the realism and fidelity of reconstruction results. Following works \cite{CCSR,SeeSR,CoSeR} employed content-related textual prompts as guidance. An intuitive way to realize SR for compressed videos is to recover each frame through the mentioned diffusion models. However, the stochastic diffusion process damages temporal consistency. Limited works \cite{MGLD-VSR,Upscale-A-Video} explored the temporal alignment for diffusion-based VSR.

The gap between existing diffusion models and the compressed VSR task lies in two main aspects: (1) How to improve diffusion models to generate frames with higher spatial fidelity and fewer compression artifacts? (2) How to constrain the temporal consistency of reconstructed frames? To mitigate these gaps, we design a distortion control module (DCM) to modulate diffusion model inputs. DCM eliminates interfering noise in input frames and controls the following diffusion phase based on LQ priors to prevent mistaken artifact generation. Subsequently, we insert a prompt-based compression-aware module (PCAM) at UNet and VAE decoders to incorporate compression awareness. 
%The UNet decoder accomplishes latent-space denoising and the VAE decoder completes pixel-space reconstruction. 
Based on compression feature code, PCAM provides lightweight prompts to characterize different degradation degree information. Finally, to constrain temporal consistency, we employ a spatio-temporal attention module (STAM), which explores relationships across frames with a spatial-temporal dimension fusion. 

In general, the proposed Spatial Degradation-Aware and Temporal Consistent (SDATC) diffusion model relieves compression negative impacts during spatial generation as shown in Fig.\ref{fig_1b}. In contrast to current SR diffusion models, we divide the compressed VSR task into two sub-tasks. DCM preemptively eases degradation effects and PACM guides diffusion with compression-aware prompts. As depicted in Fig.\ref{fig_1c}, STAM further takes full advantage of adjacent frames to smooth reconstruction frames.

The main technique contributions of this work can be summarized as follows: 

\begin{itemize}
\item{We propose a distortion control module (DCM) to adjust diffusion input distribution and provide controllable guidance. The end-to-end DCM reduces content-independent degradations for the generation process.}
\item{We introduce a prompt-based compression-aware module (PCAM) in UNet and VAE decoders to extract compression information from latent and reconstruction space. PCAM enables an adaptive diffusion process for frames compressed to varying degrees.}
\item{We design a spatio-temporal attention module (STAM) and optical flow-based latent features warping to enhance temporal coherence.}
\end{itemize}

\section{Related Work}

\subsection{Video Super-Resolution}
VSR exploits spatio-temporal similarity across LR videos to recover HR videos. %VSR methods are commonly divided into two categories, sliding-window framework \cite{VSRnet,VESPCN,DUF,EDVR,TDAN} and recurrent framework \cite{RLSP,RBPN,BasicVSR,BasicVSR++,VRT}. 
VSRNet \cite{VSRnet} first employed a deep learning model for VSR. DUF \cite{DUF} applied 3D convolution to leverage spatio-temporal relationships dynamically. EDVR \cite{EDVR} proposed Deformable Convolutional Networks (DCN) \cite{DCN} based feature alignment and fusion. TDAN \cite{TDAN} further utilized DCN to estimate motion offsets between target, prior, and following frames. BasicVSR++ \cite{BasicVSR++} designed a bidirectional recurrent architecture. VRT \cite{VRT} applied a Transformer-based recurrent framework. However, the simple Bicubic down-sampling simulation of these methods brings about synthetic-to-real gaps, which cause the failure of compressed VSR.

\subsection{Compressed Video Super-Resolution}
The complex compression degradation poses new challenges for compressed VSR. To explore compression priors, COMISR \cite{COMISR} dealt with the location and smoothness of compressed frames through enhancement modules. FTVSR \cite{FTVSR} designed a DCT-based attention module to preserve high-frequency details. CAVSR \cite{CAVSR} estimated the compression level and applied corresponding treatments. Several works \cite{RealVSR,RealESRGAN,RealBasicVSR} tackled real-world VSR by introducing more degradation types. The better compression estimation and more realistic degradation construction make efforts on compressed VSR, but the information loss is difficult to recover with limited priors, especially when frames are compressed at low bit rates.

\begin{figure*}[!t]
\centering
\includegraphics[width=2.0\columnwidth]{2.pdf}
%\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{-8pt}
\caption{Overview of the proposed Spatial Degradation-Aware and Temporal Consistent (SDATC) diffusion network. We apply a distortion control module (DCM) to enhance input low-quality (LQ) frames and extract guidance. The modulated frames are fed into the Latent Diffusion Model (LDM) based framework. The trainable prompt-based compression-aware module (PCAM) catches degradation-specific details for generation. Moreover, we incorporate the fine-tuned spatio-temporal attention module (STAM) to preserve temporal consistency.}
\label{fig_2}
\end{figure*}

\subsection{Diffusion-based Video Super-Resolution}
Diffusion-based image restoration has received growing attention from researchers recently. The great generation capability was explored in SISR \cite{SR3}. StableSR \cite{StableSR} and DiffBIR \cite{StableSR} used the control module during reconstruction. CCSR \cite{CCSR} improved content consistency through a structure refinement module. SUPIR \cite{SUPIR} modified the ControlNet and designed a novel connector ZeroSFT to reduce computational complexity, which enabled a large-scale image restoration model. SATeCo \cite{SATeCo} pivoted on learning spatial-temporal guidance from low-resolution videos. MGLD-VSR \cite{MGLD-VSR} and Upscale-A-Video \cite{Upscale-A-Video} tried to constrain the temporal consistency of diffusion-based VSR models. In this work, we leverage additional generation priors of pre-trained diffusion models to resolve the compressed VSR task. To handle the limitations of existing diffusion frameworks, we develop spatial degradation-aware and temporal consistent techniques. The degradation pre-processing and guidance could also become a paradigm for diffusion-based VSR models.

\section{Methodology}

Video compression (e.g., H.264 \cite{H.264}) inevitably introduces information loss which makes it difficult to reconstruct realistic details for compressed VSR. Motivated by the diffusion model success, we take advantage of generation priors in pre-trained Latent Diffusion Model (LDM) \cite{LDM} for compressed VSR. Compared with diffusion models, LDM applies Variational Auto-Encoder (VAE) to map images into latent space for decreasing training costs, which enables large-scale dataset application with abundant prior knowledge. Nevertheless, the unstable LDM generation can not handle compressed videos at unknown levels and increases temporal inconsistency. 

To tackle these challenges, we propose a Spatial Degradation-Aware and Temporal Consistent (SDATC) diffusion model with fewer unpleasant artifacts. The overall framework of SDATC is illustrated in Fig.\ref{fig_2}. We merely fine-tune UNet decoders in down-sampled latent space and VAE decoders in pixel-level reconstruction space. Such a framework with proposed modules prevents a great increase in computational complexity and improves spatio-temporal SR performance. Architecture details of the proposed modules are introduced in the following subsections. 

\subsection{Diffusion Model}
Diffusion models generate images from random noise $z$ through an iterative reverse Markovian. The data distribution learning for generation is through the $T$ steps forward process. The diffusion from a clean image $x_0$ to Gaussian noise $x_T$ can be formulated as:
\begin{equation}
x_t=\sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}\epsilon_{t-1},
\end{equation}
\begin{equation}
x_t=\sqrt{\overline{\alpha}_t}x_{0} + \sqrt{1-\overline{\alpha}_t}\epsilon,
\end{equation}
\begin{equation}
q(x_t|x_0)=N(x_t;\sqrt{\overline{\alpha}_t}x_{0},(1-\overline{\alpha}_t)I),
\end{equation}
where $t \in [1,T]$, $\epsilon \in N(0,1)$, and $\overline{\alpha}_t = \prod_{i=1}^{t} \alpha_i$. As $t$ increases, $\alpha_i$ gradually decreases, when $T \to \infty$, $x_T \in N(0,1)$. The reverse process predicts the inverse distribution based on the UNet network with the sampling process:
\begin{equation}
q(x_{t-1}|x_t,x_0)=N(x_{t-1};\tilde{\mu}(x_t,x_0),\tilde{\beta}_tI).
\end{equation}
The training goal is obtaining a denoising network $\epsilon_{\theta}$ by minimizing $\mathbb{E}_{t \in [1,T],x_0,\epsilon_t} [{\|\epsilon_t - \epsilon_{\theta}(x_t, t)\|}^2]$ to estimate the noise $\epsilon_t$. Based on the denoising network $\epsilon_{\theta}$, the model performs $T$ iterations diffusion reverse denoising process.

\subsection{Distortion Control Module}

Given a $n$ frames low-quality (LQ) video sequence ${\{x_{LQ}^1, ..., x_{LQ}^i, ..., x_{LQ}^n\}}$, we aim to recover a high quality (HQ) video sequence ${\{x_{HQ}^1, ..., x_{HQ}^i, ..., x_{HQ}^n\}}$. Most existing diffusion-based VSR methods first up-sample input frames to target resolution and then generate details through the diffusion process. Unfortunately, the widely utilized up-sample approaches Bilinear or Bicubic may destroy textures and have a negative influence on subsequent generations. 
%DiffBIR \cite{DiffBIR} introduced a restoration module after up-sampling to remove degradation but adopted a general up-sampling technique.

To address the above issues and concentrate on compression characteristics, we design a distortion control module (DCM). Notably, LDM learns input data distribution, thus original distortions in frames will interfere in the generation process with unpleasant artifacts. To prevent LDM from noise corruption, we execute DCM pre-processing and extract LQ guidance for the subsequent diffusion process. Specifically, we apply a Transformer-based network to remove distortions and increase spatial resolution as follows: 
\begin{equation}
x_{\mathit{HR}}=\text{Up}(\text{RSTB}(\text{Conv}_{3\times3}(x_{\mathit{LQ}}))),
\end{equation}
where RSTB($\cdot$) depicts Residual Swin Transformer Blocks \cite{SwinIR} and Up($\cdot$) is PixelShuffle up-sampling. Next, we encode modulated $x_{\mathit{HR}}$ in condition latent space $\mathit{C_{HR}}$ by VAE encoder. Following ControlNet \cite{ControlNet}, we input condition guidance $\mathit{C_{HR}}$ concatenated with noise $Z_{t}$ into a trainable copy of UNet encoder to encode guidance $C'_{HR}$. The UNet Decoder fine-tune training with guidance is then denoted as: 
\begin{equation}
Z'_t=\text{UNet}_\text{decoder}(\text{Cat}(Z_t,\text{Conv}_\text{zero}(C'_{HR}))),
\end{equation}
where Cat($\cdot$) stands for concatenation. Notably, we introduce zero convolution to prevent early-stage random noise fluctuation. The proposed DCM is presented in Fig. \ref{fig_3}, we design a general pre-processing module for diffusion-based VSR and encode modified conditions to guide generation. Moreover, we fine-tune DCM with LDM in an end-to-end framework to explore better generation input distribution and guidance.

\begin{figure}[!t]
\centering
\includegraphics[width=0.8\columnwidth]{3.pdf}
%\setlength{\abovecaptionskip}{6pt}
\setlength{\belowcaptionskip}{-6pt}
\caption{The distortion control module (DCM) designed as the pre-processing of the diffusion model.}
\label{fig_3}
\end{figure}

\subsection{Prompt-based Compression-Aware Module}

Prompt learning has achieved great success in natural language processing with effective context information. Recently, PromptIR \cite{PromptIR} developed prompt learning in image restoration. Inspired by PromptIR, we try to generate reasonable prompts for compression and diffusion generation. Nevertheless, the learnable weights of randomly initialized PromptIR prompts are obtained only by taking features average and simple linear mapping, which can not catch compression priors. Consequently, we leverage compression estimation to generate degradation dependence prompts. The proposed prompt-based compression-aware module (PCAM) is fine-tuned in both UNet and VAE decoders to guide in different spaces. Compared with large-scale degradation datasets pre-training or descriptions generated by complex semantic large language models, PCAM is feasible and has low computational complexity.

As depicted in Fig. \ref{fig_4}, PCAM interacts with latent feature $Z'$ via CNN and Adaptive AvgPool (AAP) to generate feature vector $v$, which encodes contextual and compression priors in prompt components $P$. We apply $v$ to weight $P$ and then upscale $P$ to the specific $Z'$ size as $P_{C}$ because we develop PCAM in each scale of UNet and VAE decoders. The compression-aware prompts generation is summarized as:
\begin{equation}
P_{C}=\text{Conv}_{3\times3}\left(\sum_{k=1}^{K} {\text{AAP}(\text{Conv}_{3\times3}(Z))_kP_k}\right),
\end{equation}where $k$ denotes prompt length. $P_{C}$ enables interaction between latent feature $Z'$ and prompt $P$ to get compression information. Finally, we concatenate $P_{C}$ with $Z'$ and exploit compression-aware prompts through a Transformer block. The feature transformation can be formulated as:
\begin{equation}
Z'_{C}=\text{Conv}_{3\times3}(\text{Transformer}(\text{Cat}(Z, P_{C}))).
\end{equation} 
%The utilized Transformer block is the same as the UNet decoder in LDM.

\begin{figure}[!t]
\centering
\includegraphics[width=0.9\columnwidth]{4.pdf}
%\setlength{\abovecaptionskip}{6pt}
\setlength{\belowcaptionskip}{-4pt}
\caption{The prompt-based compression-aware module (PCAM). PCAM extracts compression degradation information as prompts to direct better details generation.}
%\vspace{-10pt}
\label{fig_4}
\end{figure}

\begin{table*}[!t]
%\setlength{\belowcaptionskip}{-35pt}
\caption{Quantitative comparison of $\times$4 VSR on different compression level datasets. \colorbox{red!30}{\textbf{Bold}} and \colorbox{pink!50}{\underline{underlined}} values denote the best and second-best results respectively. $\uparrow$ and $\downarrow$ indicate better quality with higher and lower values correspondingly.}
\centering
\small
%\setlength{\tabcolsep}{10pt}
\renewcommand\arraystretch{0.95}{
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
%\hline
%Scale & \multicolumn{2}{c|}{$\times$ 4} & \multicolumn{2}{c|}{$\times$ 8} & \multicolumn{2}{c}{$\times$ 16} \\
\hline
\multicolumn{2}{c|}{Dataset}&\multicolumn{5}{c|}{REDS4}&\multicolumn{5}{c}{UDM10}\\
\hline
Method&CRF&DISTS$\downarrow$&FID$\downarrow$&NIQE$\downarrow$&MANIQA$\uparrow$&CLIP-IQA$\uparrow$&DISTS$\downarrow$&FID$\downarrow$&NIQE$\downarrow$&MANIQA$\uparrow$&CLIP-IQA$\uparrow$\\
\hline
%BasicVSR++ & 15 &\underline{29.83}/\underline{0.8184}&0.3517&0.1148&38.57&5.017&0.2338&0.4984\\
BasicVSR++&15&0.1148&38.57&5.017&0.2338&0.4984&0.1028&39.63&5.914&0.2264&0.4539\\
VRT&15&0.1197&39.19&5.558&0.2406&0.5015&0.1055&39.15&6.487&0.2335&0.4635\\
FTVSR&15&0.0991&\cellcolor{pink!50}\underline{33.68}&4.648&0.3385&0.6023&0.1005&\cellcolor{red!30}\textbf{37.24}&6.070&0.3258&0.5463\\
Real-ESRGAN&15&0.0944&52.40&\cellcolor{pink!50}\underline{2.631}&\cellcolor{pink!50}\underline{0.4021}&0.5953&0.1023&52.67&4.354&0.3513&0.5577\\
RealBasicVSR&15&\cellcolor{pink!50}\underline{0.0684}&36.08&2.647&0.3401&0.5295&0.1011&51.49&3.852&0.3400&0.4957\\
StableSR&15&0.0991&57.94&3.056&0.3733&\cellcolor{red!30}\textbf{0.7187}&\cellcolor{pink!50}\underline{0.0975}&51.62&4.361&0.3808&\cellcolor{pink!50}\underline{0.6538}\\
MGLD-VSR&15&0.0732&34.78&2.887&0.3905&0.5417&0.1044&46.90&\cellcolor{pink!50}\underline{3.810}&\cellcolor{pink!50}\underline{0.3887}&0.5242\\
%VRT & 15 &29.64/0.8138&0.3567&0.1197&39.19&5.558&0.2406&0.5015\\
%RealESRGAN & 15 &25.23/0.7147&0.3342&0.0944&52.40&\underline{2.631}&\underline{0.4021}&0.5953\\
%RealBasicVSR & 15 &27.87/0.7786&\textbf{0.2689}&\underline{0.0684}&36.08&2.647&0.3401&0.5295\\
%FTVSR & 15 & \textbf{30.88}/\textbf{0.8580} & 0.3078 & 0.0991 &\underline{33.68}& 4.648 & 0.3385 & 0.6023  \\
%StableSR&15&25.66/0.7325&0.3155&0.0991&57.94&3.056&0.3733&\textbf{0.7187}\\
%CCSR & 15 &25.68/0.7006&0.3087&0.0768&41.60&2.732&0.3836&\underline{0.6186}\\
%CCSR&15&0.0768&41.60&2.732&0.3836&\underline{0.6186}\\
%SeeSR & 15 &26.04&0.7112&0.3196&0.0784&37.38&3.188&0.5127&0.8112\\
%DiffBIR & 15 &26.10&0.6935&0.3257&0.0778&37.80&3.876&0.5710&0.8259\\
%MGLD&15&26.40/0.7118&0.2848&0.0732&34.78&2.887&0.3905&0.5417\\
%SDATC (Ours) & 15 &26.17/0.7137&\underline{0.2774}&\textbf{0.0636}&\textbf{31.09}&\textbf{2.613}&\textbf{0.4024}&0.6119\\
SDATC (Ours)&15&\cellcolor{red!30}\textbf{0.0636}&\cellcolor{red!30}\textbf{31.09}&\cellcolor{red!30}\textbf{2.613}&\cellcolor{red!30}\textbf{0.4024}&\cellcolor{pink!50}\underline{0.6119}&\cellcolor{red!30}\textbf{0.0940}&\cellcolor{pink!50}\underline{38.52}&\cellcolor{red!30}\textbf{3.508}&\cellcolor{red!30}\textbf{0.3935}&\cellcolor{red!30}\textbf{0.6606}\\
\hline
%BasicVSR++ & 25 &26.85/0.7173&0.4822&0.1861&94.19&6.019&0.1681&0.3637\\
BasicVSR++&25&0.1861&94.19&6.019&0.1681&0.3637&0.1403&83.33&6.412&0.1947&0.3315\\
VRT&25&0.1902&93.75&6.496&0.1706&0.3766&0.1436&82.30&6.928&0.2016&0.3519\\
FTVSR&25&0.1649&84.59&5.697&0.2952&0.4611&0.1374&88.71&6.685&0.3015&0.4013\\
Real-ESRGAN&25&0.1191&73.99&2.871&0.3565&\cellcolor{pink!50}\underline{0.5385}&0.1205&76.86&4.590&0.3139&0.4806\\
RealBasicVSR&25&0.1028&70.11&\cellcolor{pink!50}\underline{2.839}&0.3267&0.5252&0.1272&80.02&3.860&0.3065&0.4445\\
StableSR&25&0.1292&74.60&3.440&0.3212&0.5370&\cellcolor{pink!50}\underline{0.1155}&69.68&4.591&0.3498&\cellcolor{pink!50}\underline{0.5960}\\
MGLD-VSR&25&\cellcolor{pink!50}\underline{0.0975}&\cellcolor{pink!50}\underline{55.37}&2.964&\cellcolor{pink!50}\underline{0.3703}&0.5001&0.1191&\cellcolor{pink!50}\underline{67.00}&\cellcolor{pink!50}\underline{3.847}&\cellcolor{pink!50}\underline{0.3628}&0.5003\\
%VRT & 25 &\underline{26.92}/\underline{0.7196}&0.4833&0.1902&93.75&6.496&0.1706&0.3766\\
%RealESRGAN & 25 &24.89/0.6898&0.3926&0.1191&73.99&2.871&0.3565&\underline{0.5385}\\
%RealBasicVSR & 25 &25.95/0.7066&0.3534&0.1028&70.11&\underline{2.839}&0.3267&0.5252\\
%FTVSR & 25 &\textbf{28.39}/\textbf{0.7802}& 0.4186 & 0.1649 & 84.59 & 5.697 & 0.2952 & 0.4611 \\
%StableSR & 25 &25.23/0.7022&0.3859&0.1292&74.60&3.440&0.3212&0.5370\\
%CCSR & 25 &25.20/0.6724&0.3565&0.0990&54.84&2.920&0.3680&0.5355\\
%SeeSR & 25 &25.15&0.6747&0.3761&0.1030&51.92&3.368&0.5062&0.7820\\
%DiffBIR & 25 &25.36&0.6577&0.3860&0.1120&57.80&3.748&0.5248&0.7494\\
%SUPIR & 25 &  &  &  &  &  &  &  & \\
%MGLD & 25 & 25.47/0.6685 &\textbf{0.3366}&\underline{0.0975} &\underline{55.37}& 2.964 &\underline{0.3703}& 0.5001 \\
%SDATC (Ours) & 25 &25.28/0.6705&\underline{0.3488}&\textbf{0.0894}&\textbf{51.00}&\textbf{2.796}&\textbf{0.3796}&\textbf{0.5616}\\
SDATC (Ours)&25&\cellcolor{red!30}\textbf{0.0894}&\cellcolor{red!30}\textbf{51.00}&\cellcolor{red!30}\textbf{2.796}&\cellcolor{red!30}\textbf{0.3796}&\cellcolor{red!30}\textbf{0.5616}&\cellcolor{red!30}\textbf{0.1153}&\cellcolor{red!30}\textbf{66.15}&\cellcolor{red!30}\textbf{3.524}&\cellcolor{red!30}\textbf{0.3675}&\cellcolor{red!30}\textbf{0.6021}\\
\hline
%BasicVSR++ & 35 &24.24/0.6265&0.5852&0.2676&183.37&7.225&0.1037&0.2255\\
BasicVSR++&35&0.2676&183.37&7.225&0.1037&0.2255&0.2173&163.68&7.267&0.1417&0.2055\\
VRT&35&0.2686&183.05&7.345&0.1049&0.2379&0.2198&163.22&7.656&0.1455&0.2159\\
FTVSR&35&0.2526&180.28&7.190&0.1983&0.1867&0.2106&162.23&7.730&0.2319&0.1706\\
Real-ESRGAN&35&0.2241&172.29&3.993&0.2329&0.4210&0.1937&155.43&5.489&0.2375&0.3138\\
RealBasicVSR&35&0.1720&137.54&3.158&0.2715&\cellcolor{pink!50}\underline{0.4637}&0.1884&165.71&4.235&0.2649&0.3436\\
StableSR&35&0.2311&166.80&4.734&0.1576&0.2321&0.1868&149.57&5.630&0.2157&0.3378\\
MGLD-VSR&35&\cellcolor{red!30}\textbf{0.1587}&\cellcolor{red!30}\textbf{97.90}&\cellcolor{pink!50}\underline{2.873}&\cellcolor{pink!50}\underline{0.3275}&0.4598&\cellcolor{pink!50}\underline{0.1771}&\cellcolor{red!30}\textbf{117.43}&\cellcolor{pink!50}\underline{3.998}&\cellcolor{pink!50}\underline{0.2878}&\cellcolor{pink!50}\underline{0.3808}\\
%VRT & 35 &\underline{24.26}/\underline{0.6270}&0.5855&0.2686&183.05&7.345&0.1049&0.2379\\
%RealESRGAN & 35 &23.60/0.6200&0.5288&0.2241&172.29&3.993&0.2329&0.4210\\
%RealVSR & 35 &  &  &  &  &  &  &  &  \\
%RealBasicVSR & 35 &23.45/0.6078&0.4722&0.1720&137.54&3.158&0.2715&\underline{0.4637}\\
%FTVSR & 35 &\textbf{25.13}/\textbf{0.6697}& 0.5436 & 0.2526 & 180.28 & 7.190 & 0.1983 & 0.1867 \\
%StableSR & 35 &23.60/0.6260&0.5367&0.2311&166.80&4.734&0.1576&0.2321\\
%CCSR & 35 &23.60/0.5981&0.4624&0.1784&115.00&3.161&0.3165&0.4594\\
%CCSR&35&0.1784&115.00&3.161&0.3165&0.4594\\
%SeeSR & 35 &23.14&0.5932&0.4716&0.1615&99.43&3.540&0.4951&0.7816\\
%DiffBIR & 35 &23.36&0.5683&0.5042&0.2026&139.12&3.801&0.4506&0.7152\\
%SUPIR & 35 &  &  &  &  &  &  &  & \\
%MGLD & 35 & 23.36/0.5719 &\textbf{0.4272}&\textbf{0.1587}&\textbf{97.90}& \underline{2.873}&\underline{0.3275}& 0.4598 \\
SDATC (Ours)&35&\cellcolor{pink!50}\underline{0.1602}&\cellcolor{pink!50}\underline{113.19}&\cellcolor{red!30}\textbf{2.725}&\cellcolor{red!30}\textbf{0.3276}&\cellcolor{red!30}\textbf{0.5545}&\cellcolor{red!30}\textbf{0.1683}&\cellcolor{pink!50}\underline{146.33}&\cellcolor{red!30}\textbf{3.526}&\cellcolor{red!30}\textbf{0.2958}&\cellcolor{red!30}\textbf{0.4637}\\
\hline
%\multirow{2}{*}{Model} & WS-PSNR /& WS-PSNR /& WS-PSNR /& WS-PSNR /& WS-PSNR /& WS-PSNR /\\
%&WS-SSIM&WS-SSIM&WS-SSIM&WS-SSIM&WS-SSIM&WS-SSIM\\
%Bicubic & 24.62 / 0.6555 & 24.61 / 0.6459 & 19.64 / 0.5908 & 19.72 / 0.5403 & 17.12 / 0.4332 & 17.56 / 0.4638\\
\end{tabular}}
\vspace{-6pt}
\end{table*}

\subsection{Spatio-Temporal Attention Module}
Although LDM-based SR methods reconstruct each frame successfully, the multi-frame generation meets temporal inconsistency. The serious deformation of an object across adjacent frames is visually disastrous. 
%In video generation, Tune-A-Video \cite{TAV} tried to learn continuous motion by a large range attention mechanism. 
To improve compressed VSR with temporal consistency, we introduce a spatio-temporal attention module (STAM) in the VAE decoder. In particular, we expand the temporal dimension with multi-frames. Freezing pre-trained spatial residual blocks, we insert 3D CNN and temporal attention block(TAB). TAB executes self-attention among temporal dimension and the outputs of TAB are regarded as residuals. We apply learnable parameters to balance spatio-temporal branches as:
\begin{equation}
Z_0'=\text{Res}(\text{Conv}_{3\times3}({Z'_{C}}_0)),
\end{equation}
\begin{equation}
Z_0''=\alpha_T\text{Conv3D}_{3\times3}(Z_0')+(1-\alpha_T)(Z_0'),
\end{equation}
\begin{equation}
F_d=\text{Res}(\beta_T\text{TA}(Z_0'')+(1-\beta_T)(Z_0'')),
\end{equation}where Res($\cdot$) is residual block, $\alpha_T$ and $\beta_T$ denote learnable spatio-temporal tensors. As illustrated in Fig. \ref{fig_2}, we further incorporate $F_e$ from the VAE encoder and achieve a balanced outcome through Controllable Feature Warping (CFW) module \cite{StableSR}, the adjustable parameter $\omega$ controls reconstructed outputs as (a larger $\omega$ means higher fidelity):
\begin{equation}
x_{HQ}=\text{Up}(F_d + \omega\text{CFW}(F_e, F_d)).
\end{equation} 
%A larger $\omega$ means higher fidelity.

Limited by computational complexity, we explore temporal coherence in the VAE decoder. In the latent space of UNet decoder, we compute forward and backward optical flow to align features for improving consistency. As shown in Fig. \ref{fig_2}, we calculate the motion error $E_t$ at each denoising step:
\begin{equation}
E_t=\sum_{i=1}^{N-1} {\|f_{b}({Z'}_t^i)-{Z'}_t^{i+1}\|_1}+\sum_{i=2}^{N} {\|f_{f}({Z'}_t^{i})-{Z'}_t^{i-1}\|_1},
\end{equation}where $f_b$ and $f_f$ indicate backward and forward warping. The subsequent sampling process is as follows:
\begin{equation}
{Z'}_t=\text{UNet}({Z'}_{t+1})-\sigma_t^2 \triangledown _Z(\text{UNet}({Z'}_{t+1}), E_t).
\end{equation}The first item is the DDPM result and the second is the optical flow warping gradient scaled by variance $\sigma_t^2$. The gradient updating in $\text{UNet}({Z'}_{t+1})$ is based on $E_t$.

\begin{figure*}[!t]
\centering
\includegraphics[width=1.9\columnwidth]{5_.pdf}
%\setlength{\abovecaptionskip}{5pt}
%\setlength{\belowcaptionskip}{-16pt}
\vspace{-2pt}
\caption{Qualitative comparison of $\times4$ VSR on different compression level datasets.}
\label{fig_5}
\vspace{-8pt}
\end{figure*}

\section{Experiments}
\subsection{Implementation Details}
\subsubsection{Datasets} 
We train our SDATC on merged REDS \cite{REDS} training set and validation set 
%(266 sequences, each containing 100 frames with the resolution of 1280 $\times$ 720)
, the left 4 sequences (REDS4) are for testing. During training, we utilize the x264 encoder to $\times$4 down-sample and compress videos. Without loss of generality, we randomly compress videos from 10K to 100K bit rates. The x264 codec provides different compression levels, i.e., Constant Rate Factor (CRF). CRF value ranges from 0 to 51 and 0 stands for lossless compression.
%and 23 is the default. 
Following existing works \cite{COMISR,FTVSR,CAVSR}, we choose CRFs of 15, 25, and 35 to generate compressed testing videos. We also conduct evaluations on the Vid4 dataset \cite{Vid4} and the UDM10 dataset \cite{UDM10}. 
%\textcolor{red}{corresponding comparisons and the user study can be found in Appendix B.}
% which consists of 4 clips with 40 frames at 720 $\times$ 480 and 10 clips with 32 frames at 1272 $\times$ 720, respectively.

\subsubsection{Training Setting}
The DCM consists of 6 RSTB blocks with a window size of 8. We fine-tune the diffusion model on 8 NVIDIA A100 GPUs. The length of input clips is 5, the batch size is 4, and the patch size is 512. The learning rate is initialized as 5 $\times$ 10$^{-5}$ using the Adam \cite{Adam} optimizer. The trade-off parameter $\omega$ is set to 0.75.
%The noise linear schedule is set to $\eta_1$ = 0.00085 and $\eta_T$ = 0.0120 ($T$ = 1000). 
During inference, we set 50 sampling steps.

\subsubsection{Evaluation Setting}
We apply perceptual metrics for evaluation, including DISTS \cite{DISTS}, FID \cite{FID}, NIQE \cite{NIQE}, MANIQA \cite{MANIQA}, and CLIP-IQA \cite{CLIPIQA}, which concentrate on human visual preference. Moreover, we comprehensively compare the proposed SDATC with VSR (BasicVSR++ \cite{BasicVSR++}, VRT \cite{VRT}), compressed VSR (Rea-lESRGAN \cite{RealESRGAN}, RealBasicVSR \cite{RealBasicVSR}, FTVSR \cite{FTVSR}), and diffusion-based models (StableSR \cite{StableSR}, MGLD-VSR \cite{MGLD-VSR}).
%PSNR and SSIM measure the referenced pixel-wise error and similarity. We test them on the Y channel (gray-scale) for reference. 

%and corresponding ground-truth (GT) frames. 
%Besides, we conduct a user study.
%The NIQE, MANIQA, and CLIP-IQA are no-reference metrics. 

\subsection{Quantitative Comparison}
The quantitative experimental results are shown in Tab. 1. It can be observed that our SDATC achieves the best performances on different compression levels in the REDS4 dataset and UDM10 dataset. The superior results demonstrate that adopting modules with compression-aware generation priors improves visual perception. Furthermore, the higher CLIP-IQA values of SDATC reflect its strong realistic details generation capability. We also present quantitative experiments on the Vid4 dataset in Appendix 1.2. The other metrics comparison like PSNR, SSIM, and LPIPS is also mentioned.
%Although MSE-based compressed VSR models like FTVSR obtain higher PSNR and SSIM, they always produce over-smoothed outcomes. This phenomenon is more severe at a low bit rate (CRF=35).

\begin{figure}[!t]
\centering
\includegraphics[width=0.85\columnwidth]{7.pdf}
\caption{User study results of $\times4$ VSR on CRF=25 datasets.}
\vspace{-12pt}
\end{figure}

\subsection{Qualitative Comparison}
As can be seen from zoom-in regions of Fig. \ref{fig_5}, the proposed SDATC outperforms MSE-based methods BasicVSR++, VRT, and FTVSR with clearer details, especially they rebuild completely blurry results when CRF$=$35. Compared with generative approaches, SDATC restores finer text and numbers in sequence ``calendar'', more appealing textures in sequence ``archpeople'', and more natural trees, grassland, and water surface in sequence ``lake''. Note that the SOTA diffusion-based VSR method MGLD-VSR introduces grid-like artifacts in sequence archpeople and color shift artifacts in sequence lake. Alternatively, SDATC addresses such issues with compression-specific modules. More visual results on different datasets are provided in Appendix 1.2.

\subsection{User Study}
We further perform a user study to select the better videos from two reconstructed videos. In detail, we invite 15 participants to compare SDATC with RealBasicVSR, StableSR, and MGLD-VSR in pairs, respectively. The compared video sequences are set to 9. As depicted in Fig. 4, volunteers prefer the results of SDATC rather than other approaches.

%\subsection{Computational Efficiency Comparison}

\section{Ablation Study}
In this section, we further analyze the proposed SDATC diffusion network. In detail, we perform experiments on each module individually and observe their effectiveness in the spatio-temporal dimension. The experiments are conducted on the compressed REDS4 dataset (CRF=25) for $\times$4 VSR. 
%The baseline is the LDM-based architecture.

\begin{table}[!t]
\caption{Ablation study of Distortion Control Module.}
%\setlength{\abovecaptionskip}{6pt}
\vspace{-2pt}
\centering
\small
\renewcommand\arraystretch{0.95}\setlength{\tabcolsep}{4pt}{
\begin{tabular}{c|c|c|c|c}
\hline
Module&DISTS$\downarrow$&NIQE$\downarrow$&MANIQA$\uparrow$&CLIP-IQA$\uparrow$\\
\hline
Baseline&0.1551&4.104&0.1694&0.1470\\
+USM&0.1451&4.107&0.1796&0.1670\\
+DiffBIR&0.1215&3.293&0.2655&0.3169\\
+TMSA&0.1408&3.475&0.2354&0.2486\\
+DCM&\cellcolor{red!30}\textbf{0.1005}&\cellcolor{red!30}\textbf{2.964}&\cellcolor{red!30}\textbf{0.3386}&\cellcolor{red!30}\textbf{0.4371}\\
\hline
\end{tabular}
}
\vspace{-4pt}
\end{table}

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{6_a.pdf}
%\setlength{\abovecaptionskip}{-4pt}
%\setlength{\belowcaptionskip}{-5pt}
\vspace{-16pt}
\caption{LPIPS scores of different restoration methods.}
\label{fig_6}
\vspace{-4pt}
\end{figure}

\subsection{Distortion Control Module}
As depicted in Tab. 2, DCM improves perceptual quality and exceeds other enhancement methods. Specifically, Unsharpen Mask (USM) sharpens GT images to optimize training. DiffBIR \cite{DiffBIR} up-samples images by PixelShuffle and then restores them. TMSA \cite{VRT} extracts multi-frame features before up-sampling. In contrast to them, DCM achieves compression discrimination, activating higher fidelity generation results. To clarify intuitively, we calculate similarity scores of the low-quality (LQ) domain, diffusion generation (DG) domain, GT domain, and enhancement domain. The similarity is measured by LPIPS and FID, lower scores indicate closer distance. As presented in Fig. 6 (FID scores are in Appendix 1.2), basic diffusion-based VSR up-samples LQ frames by Bicubic and next executes diffusion denoising. Maintaining the generation capacity unchanged, DCM enables the diffusion model to generate results with better spatial fidelity. We also visualize the results of these methods in Appendix 1.2, DCM effectively deduces noises and provides smooth diffusion inputs.

%\begin{figure}[!t]
%\centering
%\includegraphics[width=\columnwidth]{7.pdf}
%\setlength{\abovecaptionskip}{-5pt}
%\setlength{\belowcaptionskip}{-5pt}
%\caption{Visual results of different restoration methods.}
%\label{fig_7}
%\end{figure}

\begin{table}[!t]
\caption{Ablation study of Prompt-based Compression-Aware Module.}
%\setlength{\abovecaptionskip}{-5pt}
\vspace{-2pt}
\centering
\small
\renewcommand\arraystretch{0.95}\setlength{\tabcolsep}{1.5pt}{
\begin{tabular}{c|c|c|c|c}
\hline
Module&PSNR$\uparrow$/SSIM$\uparrow$&NIQE$\downarrow$&MANIQA$\uparrow$&CLIP-IQA$\uparrow$\\
\hline
Baseline&26.26/0.7009&4.104&0.1694&0.1470\\
+Prompt&26.29/0.7006&3.882&0.1867&0.1767\\
+Softmax&26.32/0.7015&3.903&0.1861&0.1820\\
+PCAM&\cellcolor{red!30}\textbf{26.58}/\textbf{0.7110}&\cellcolor{red!30}\textbf{3.803}&\cellcolor{red!30}\textbf{0.2080}&\cellcolor{red!30}\textbf{0.2053}\\
\hline
\end{tabular}
}
\vspace{-4pt}
\end{table}

\begin{table}[!t]
\caption{Ablation study of Prompt-based Compression-Aware Module Artifacts Removal.}
\vspace{-2pt}
%\setlength{\abovecaptionskip}{-5pt}
\centering
\small
\renewcommand\arraystretch{0.95}{
\begin{tabular}{c|c|c|c}
\hline
\multirow{2}{*}{Module} & \multicolumn{3}{c}{Perception-Sensitive Pixel Loss$\downarrow$} \\
\cline{2-4}
&CRF=15&CRF=25&CRF=35\\ 
\hline
Baseline&0.2348&0.3137&0.5263\\
+PCAM&\cellcolor{red!30}\textbf{0.2058}&\cellcolor{red!30}\textbf{0.2868}&\cellcolor{red!30}\textbf{0.5072}\\
\hline
\end{tabular}
}
\vspace{-8pt}
\end{table}

\subsection{Prompt-based Compression-Aware Module}
It can be observed in Tab. 3 that PCAM achieves not only perceptual but also PSNR and SSIM gains over baseline. Note that ``+Prompt'' means the basic random initialization learnable-prompts. ``+PCAM'' is the proposed feature extraction and compression-aware prompts. ``+Softmax'' is the version of PCAM with a Softmax layer in feature extraction. PCAM provides compression-specific prompts to guide reasonable texture generation in the latent and reconstruction space. Simultaneously, the compression priors extracted from features contribute to pixel-oriented metrics. Furthermore, we perform an experiment on REDS4 dataset to quantitatively analyze compressed VSR artifacts. Following LDL \cite{LDL}, we calculate perception-sensitive pixel loss based on variances. From Tab. 4, the PCAM module effectively distinguishes compression and generation artifacts.

\subsection{Spatio-Temporal Attention Module}
To evaluate both spatial quality as well as temporal coherency, we adopt video quality assessment VMAF \cite{VMAF}. VMAF additionally introduces motion measures for temporal characteristics. From Tab. 5, ``+STAM'' receives a better VMAF score and perceptual measurements than the baseline. Meanwhile, SDATC overcomes other generation methods in VMAF. The STAM module benefits the spatio-temporal performance of the diffusion model. We also demonstrate temporal profiles in Fig. 7 to compare temporal consistency, SDATC achieves smoother multi-frame reconstruction and is closer to GT frames.

\begin{table}[!t]
\caption{Ablation study of Spatio-Temporal Attention Module.}
\centering
\small
\renewcommand\arraystretch{0.95}\setlength{\tabcolsep}{3pt}{
\begin{tabular}{c|c|c|c|c}
\hline
Module&VMAF$\uparrow$&NIQE$\downarrow$&MANIQA$\uparrow$&CLIP-IQA$\uparrow$\\
\hline
Baseline&35.70&4.104&0.1694&0.1470\\
+STAM&44.53&3.200&0.3620&0.4869\\
Real-ESRGAN&59.40&2.871&0.3565&0.5385\\
RealBasicVSR&64.66&2.839&0.3267&0.5252\\
StableSR&58.44&3.440&0.3212&0.5370\\
CCSR&51.99&2.920&0.3680&0.5355\\
MGLD-VSR&56.80&2.964&0.3703&0.5001\\
SDATC (Ours)&\cellcolor{red!30}\textbf{67.22}&\cellcolor{red!30}\textbf{2.796}&\cellcolor{red!30}\textbf{0.3796}&\cellcolor{red!30}\textbf{0.5616}\\
\hline
\end{tabular}
}
\end{table}

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{8.pdf}
%\setlength{\abovecaptionskip}{-2pt}
%\setlength{\belowcaptionskip}{-12pt}
\caption{Temporal profiles comparison. The so-called temporal profiles are acquired through concatenating rows at the a same location in continuous frames.}
\label{fig_7}
\vspace{-6pt}
\end{figure}

\section{Conclusion}
In this paper, we presented a Spatial Degradation-Aware and Temporal Consistent (SDATC) diffusion model for compressed video super-resolution. The key of SDATC was leveraging pre-trained diffusion model generation priors and extracting compression priors to improve reconstruction. Specifically, we introduced a distortion control module to modulate diffusion inputs and create controllable guidance, which alleviated negative compression impacts in the following denoising process. To further recover compression-lost details, we inserted prompt-based compression-aware modules in latent and reconstruction space to provide prompts for generation. Finally, we proposed a spatio-temporal attention module and optical flow warping to lighten flickering. Extensive experimental evaluations and visual results on benchmark datasets demonstrated the superiority of SDATC over other state-of-the-art methods. Through compression-specific optimizations, we exploited the potentials of the diffusion model in compressed video super-resolution.

%\appendix

%\section*{Ethical Statement}

%There are no ethical issues.

%\section*{Acknowledgments}


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\newpage

\section{Appendix}
Due to the lack of space in the main paper, we provide more details of the proposed Spatial Degradation-Aware and Temporal Consistent (SDATC) diffusion model in supplementary material. In Appendix 1.1, we present more information on the baseline LDM diffusion model. Additional experimental results and visual comparisons can be found in Appendix 1.2.

\subsection{Methodology}
This section supplements the section ``Methodology'' in the main paper. We present a detailed introduction to our SDATC's benchmark framework. 

\subsubsection{Color Correction}
Recent works [Choi et al. 2022] have found that diffusion models confront color shift issues. Noting that the variant network of diffusion models exhibits a more noticeable color shift issue after training, Upscale-A-Video [Zhou et al., 2024] adopted the wavelet color correction module [Wang et al., 2024] for correction. Specifically, Upscale-A-Video performed color normalization on generated images by referencing the mean and variance of LR inputs. Carrying on the same line of thought, we adopt adaptive instance normalization (adaIN) [Huang and Belongie 2017] to transform the style of reconstructed frames to have similar color and illuminations as those LQ frames in SDATC.

\subsection{Experiments}
In this section, we provide more quantitative and qualitative experimental results to compare the proposed SDATC with other state-of-the-art methods comprehensively. We also supply additional ablation study results to claim the effectiveness of the proposed modules.

\subsubsection{Quantitative Comparison}
As can be seen from Tab. 1, we introduce additional experiments on the Vid4 dataset. We also follow the compression level setting in the main paper, CRF=15, 25, and 35 for $\times4$ VSR. SDATC outperforms other methods in terms of perceptual quality, especially in MANIQA [Yang et al., 2022] and CLIP-IQA [Wang et al., 2023a], which highly correlate with human perception. Similar to other diffusion-based methods, the proposed STDAC shows limitations on certain metrics like PSNR and SSIM because these metrics are primarily designed to measure pixel-level fidelity and structural similarity, while STDAC focuses on perceptual quality. Notably, our STDAC performs better than SOTA diffusion-based SISR approach StaleSR and VSR approach MGLD-VSR on PSNR and SSIM. The comprehensive experimental results deflect the great capability of SDATC to enhance compressed videos and carry out realistic details.

\subsubsection{Qualitative Comparison}
We visualize $\times4$ VSR qualitative results in Fig. 3, Fig. 4, and Fig. 5. As can be seen in zoom-in regions, SDATC reconstructs more appealing structure textures and more complete buildings. For MSE-based methods, their reconstruction results are totally blurred, especially when CRF is 25 or 35. Although other generation models could recover objects from compressed frames, a sense of unreality from their reconstruction results has a negative impact on visual experiences. Benefiting from the strong generation priors, SDATC develops finer details and less compression artifacts.


\begin{table*}[!t]
\caption{Quantitative comparison of $\times$4 VSR on different compression levels Vid4 dataset. \colorbox{red!30}{\textbf{Bold}} and \colorbox{pink!50}{\underline{underlined}} values denote the best and second-best results respectively. $\uparrow$ and $\downarrow$ indicate better quality with higher and lower values correspondingly.}
%\vspace{-6pt}
\centering
%\small
\renewcommand\arraystretch{1.0}{
%\setlength{\tabcolsep}{0.5pt}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
\hline
\multicolumn{2}{c|}{Dataset}&\multicolumn{8}{c}{Vid4}\\
\hline
Method&CRF&PSNR$\uparrow$&SSIM$\uparrow$&LPIPS$\downarrow$&DISTS$\downarrow$&FID$\downarrow$&NIQE$\downarrow$&MANIQA$\uparrow$&CLIP-IQA$\uparrow$\\
\hline
BasicVSR++&15&\cellcolor{pink!50}\underline{25.74}&0.7381&0.3745&0.1566&\cellcolor{pink!50}\underline{68.73}&5.137&0.2155&0.4110\\
VRT&15&25.72&\cellcolor{pink!50}\underline{0.7418}&0.3747&0.1680&70.73&5.939&0.2253&0.4202\\
FTVSR&15&\cellcolor{red!30}\textbf{26.35}&\cellcolor{red!30}\textbf{0.7849}&0.3634&0.1439&68.93&5.584&0.2947&0.5597\\
Real-ESRGAN&15&22.42&0.6037&0.3838&0.1516&86.16&\cellcolor{red!30}\textbf{2.593}&0.3336&0.5924\\
RealBasicVSR&15&23.92&0.6615&\cellcolor{red!30}\textbf{0.3526}&\cellcolor{pink!50}\underline{0.1252}&72.95&\cellcolor{pink!50}\underline{2.933}&0.2922&0.6185\\
StableSR&15&22.15&0.5805&0.3762&0.1430&80.16&3.207&0.3380&\cellcolor{pink!50}\underline{0.6460}\\
MGLD-VSR&15&22.27&0.5654&0.3741&0.1321&89.46&3.247&\cellcolor{pink!50}\underline{0.3529}&0.6292\\
SDATC (Ours)&15&22.49&0.5862&\cellcolor{pink!50}\underline{0.3631}&\cellcolor{red!30}\textbf{0.1229}&\cellcolor{red!30}\textbf{65.97}&3.055&\cellcolor{red!30}\textbf{0.3714}&\cellcolor{red!30}\textbf{0.7332}\\
\hline
BasicVSR++&25&23.64&0.6210&0.4738&0.2183&137.96&5.621&0.1594&0.2703\\
VRT&25&\cellcolor{pink!50}\underline{23.79}&\cellcolor{pink!50}\underline{0.6300}&0.4717&0.2266&137.68&6.532&0.1663&0.3271\\
FTVSR&25&\cellcolor{red!30}\textbf{24.70}&\cellcolor{red!30}\textbf{0.6980}&0.4217&0.1984&131.18&6.106&0.2548&0.4861\\
Real-ESRGAN&25&21.96&0.5703&0.4206&0.1672&115.83&\cellcolor{red!30}\textbf{2.662}&0.3175&0.5899\\
RealBasicVSR&25&22.82&0.5931&0.4163&0.1588&116.50&\cellcolor{pink!50}\underline{2.809}&0.2712&0.5987\\
StableSR&25&21.85&0.5561&0.4094&0.1588&\cellcolor{pink!50}\underline{93.54}&3.416&0.3056&\cellcolor{pink!50}\underline{0.6264}\\
MGLD-VSR&25&21.77&0.5290&\cellcolor{pink!50}\underline{0.4073}&\cellcolor{pink!50}\underline{0.1507}&97.76&3.276&\cellcolor{pink!50}\underline{0.3447}&0.6051\\
SDATC (Ours)&25&21.91&0.5362&\cellcolor{red!30}\textbf{0.4055}&\cellcolor{red!30}\textbf{0.1436}&\cellcolor{red!30}\textbf{92.56}&3.157&\cellcolor{red!30}\textbf{0.3666}&\cellcolor{red!30}\textbf{0.6979}\\
\hline
BasicVSR++&35&21.57&0.4914&0.5838&0.2885&254.62&6.618&0.1114&0.1421\\
VRT&35&\cellcolor{pink!50}\underline{21.62}&\cellcolor{pink!50}\underline{0.4949}&0.5844&0.2907&252.83&7.157&0.1228&0.1806\\
FTVSR&35&\cellcolor{red!30}\textbf{22.08}&\cellcolor{red!30}\textbf{0.5412}&0.5497&0.2786&302.37&6.898&0.1813&0.1840\\
Real-ESRGAN&35&20.83&0.4874&0.5204&0.2304&235.89&3.213&0.2382&0.4272\\
RealBasicVSR&35&20.98&0.4783&0.5229&0.2229&250.18&\cellcolor{pink!50}\underline{3.113}&0.2326&0.3449\\
StableSR&35&20.89&0.4815&0.5186&0.2368&\cellcolor{pink!50}\underline{222.96}&4.246&0.2102&0.3748\\
MGLD-VSR&35&20.46&0.4392&\cellcolor{pink!50}\underline{0.5023}&\cellcolor{pink!50}\underline{0.2083}&\cellcolor{red!30}\textbf{166.07}&\cellcolor{red!30}\textbf{3.054}&\cellcolor{pink!50}\underline{0.3234}&\cellcolor{pink!50}\underline{0.4396}\\
SDATC (Ours)&35&20.27&0.4077&\cellcolor{red!30}\textbf{0.4773}&\cellcolor{red!30}\textbf{0.1919}&231.08&3.256&\cellcolor{red!30}\textbf{0.3501}&\cellcolor{red!30}\textbf{0.5994}\\
\hline
\end{tabular}}
%\vspace{-6pt}
\end{table*}

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{6_b.pdf}
%\vspace{-14pt}
\caption{FID scores of different restoration methods.}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{7_.pdf}
%\vspace{-14pt}
\caption{Visual results of different restoration methods.}
\end{figure}

\begin{figure*}[!t]
\centering
\includegraphics[width=2.1\columnwidth]{5.pdf}
\caption{Qualitative comparison of $\times4$ VSR on different compression level REDS4 dataset.}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=2.1\columnwidth]{0.pdf}
\caption{Qualitative comparison of $\times4$ VSR on different compression level datasets.}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=2.1\columnwidth]{1.pdf}
\caption{Qualitative comparison of $\times4$ VSR on different compression level datasets.}
\end{figure*}

\subsubsection{Ablation of Distortion Control Module}
As mentioned in the section ``Ablation Study'' of the main paper, we illustrate the FID [Heusel et al. 2017] scores of different methods in Fig. 1. It can be observed that DCM achieves the best FID score between the DG domain and GT domain, which means DCM allows the diffusion model to achieve the most similar outputs with GT frames. We also visualize the results of these methods in Fig. 2, DCM effectively deduces noises and provides smooth diffusion inputs.

\end{document}

