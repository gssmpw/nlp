\section{Related work}
\textbf{WaveNet:} The WaveNet series is a collection of deep learning-based speech generation models proposed by Google. These models significantly enhance the quality and naturalness of speech synthesis by directly generating waveform data. The WaveNet model is not only groundbreaking in the field of speech synthesis but has also influenced multiple related domains, such as music synthesis.
    
    WaveNet[2] is a generative model based on causal convolution that directly generates audio waveforms in the time domain. By modeling the conditional probability distribution of each sample point, WaveNet can generate speech with high fidelity and naturalness. WaveNet significantly improves the naturalness and quality of synthesized speech, with generated speech nearly reaching human-level quality. However, its generation speed is relatively slow, as it produces one sample point at a time, resulting in high computational costs and long inference times.To address the slow inference speed of WaveNet, Parallel WaveNet[3] accelerates the generation process by introducing a flow-based model. It employs a Teacher-Student Framework for training, enabling the parallel network to approximate the output distribution of WaveNet. During the generation phase, an inverse autoregressive process is used to generate waveforms in parallel, improving generation efficiency. Parallel WaveNet significantly enhances inference speed, making it capable of meeting real-time speech synthesis requirements.WaveRNN[4] further optimizes WaveNet’s computational efficiency for real-time applications. By replacing the convolutional network with a recurrent neural network (RNN) [5], it significantly reduces the number of model parameters. The advantage of this approach is a smaller model size and faster generation speed, making it suitable for resource-constrained scenarios.
   
\textbf{DeepVoice:} The DeepVoice series is a collection of end-to-end speech synthesis systems proposed by the Baidu research team. These models aim to gradually replace traditional speech synthesis pipelines through deep learning techniques, achieving high-quality and natural speech. The DeepVoice series consists of three versions, with each generation showing significant improvements in model architecture, training methods, and speech synthesis quality.

DeepVoice 1[6] built an end-to-end neural network speech synthesis system to replace traditional rule-based and statistical parameter-based TTS methods. Its advantage lies in improving the learnability between modules, enabling end-to-end training of the speech generation process. However, the naturalness of the speech generated by DeepVoice 1 was still limited, and it relied on multiple separate training modules.
DeepVoice 2[7] added support for multi-speaker speech synthesis, addressing the limitations of traditional single-speaker models. It significantly improved the model’s performance in multi-speaker speech synthesis tasks, allowing the system to quickly adapt to new speakers with minimal additional data.
DeepVoice 3[8] introduced an attention-based sequence-to-sequence (Seq2Seq) model, which greatly simplified the complexity of the speech synthesis system. The naturalness of the generated speech was significantly improved, particularly in terms of prosody and timbre, making it more realistic. It also supports a wider range of applications, including multilingual, multi-speaker, and emotional speech synthesis.

 \textbf{Tacotron:} The Tacotron series is an end-to-end speech synthesis system proposed by Google. This series of models has achieved significant advancements in speech synthesis technology, particularly in terms of naturalness, timbre diversity, and model simplification.

Tacotron[9] introduced an end-to-end model that directly generates speech spectrograms from text, replacing the complex multi-stage pipeline of traditional speech synthesis systems (such as phoneme extraction and prosody modeling). This simplification greatly reduced the complexity of conventional TTS systems and significantly improved the naturalness of generated speech, making it closer to human speech. However, Tacotron used the Griffin-Lim[10] algorithm for waveform reconstruction, which resulted in lower audio clarity. Additionally, its reliance on an attention mechanism during training could lead to alignment errors, such as syllable repetitions or omissions.
Tacotron 2[11] addressed the waveform generation issue by replacing the Griffin-Lim algorithm with WaveNet, which directly generates high-quality waveforms from spectrograms, significantly enhancing speech naturalness. Tacotron 2 became one of the most human-like speech synthesis models at the time, supporting emotional and prosody control to produce more expressive speech. However, its use of the WaveNet vocoder introduced high computational complexity, affecting real-time performance. Additionally, the model still relied heavily on large-scale training data.

 \textbf{FastSpeech:} The FastSpeech series is an efficient speech synthesis model series proposed by Zhejiang University and Microsoft Research, aimed at addressing the issues of slow generation speed, poor real-time performance, and weak prosody control in traditional end-to-end TTS models.

FastSpeech[12] improves the inference speed of TTS models and reduces the instability issues (such as attention misalignment) faced by attention-based Seq2Seq models. FastSpeech is a non-autoregressive model that bypasses the attention-based process of generating waveforms point by point. Its inference speed is more than 10 times faster than traditional autoregressive models (such as Tacotron), improving generation stability and reducing issues like misaligned or missing speech. However, its prosody and emotional expression capabilities are relatively weak, limiting the expressiveness of the generated speech.
FastSpeech 2[13] further enhances the naturalness, prosody control ability, and diversity of speech synthesis while maintaining high inference speed. It introduces more acoustic features (such as pitch, energy, etc.) as additional supervisory signals, allowing the model to control prosodic features during synthesis. It provides explicit control interfaces for adjusting speech rate, pitch, and volume. It also improves the duration predictor by incorporating alignment tools (such as Monte Carlo alignment) to achieve higher-quality phoneme duration annotations.
FastSpeech 2s[13] further shortens the generation path from text to speech, supporting direct waveform generation from text, skipping the spectrogram generation stage. This enables true end-to-end speech synthesis while maintaining high inference efficiency, achieving a better balance between inference speed and speech quality.

 \textbf{Transformer TTS:} Transformer TTS[14] is a TTS model based on the Transformer[15] architecture, proposed by Microsoft. Its core idea is to model the mapping relationship between text sequences and speech spectrograms using the self-attention mechanism while incorporating an end-to-end design to improve generation efficiency and speech quality.
Transformer TTS consists of a text encoder, a spectrogram decoder, and a vocoder. It features fast generation speed, strong stability, and support for large-scale training. Transformer TTS has been applied to various tasks, including multilingual speech synthesis, personalized speech synthesis, and emotional speech synthesis.

\textbf{NaturalSpeech:} The NaturalSpeech series is a collection of end-to-end speech synthesis models proposed by the Microsoft research team, aiming to achieve breakthroughs in generation speed, speech naturalness, diversity, and control capabilities through innovative architectures and efficient training methods.

NaturalSpeech[16] delivers high-quality speech synthesis, further enhancing the naturalness and expressiveness of speech. By adopting a non-autoregressive architecture, NaturalSpeech reduces generation steps and significantly improves speech generation speed, achieving a level of naturalness close to human speech.
NaturalSpeech 2[17] further improves the model's ability to control diverse speech features while maintaining high-quality generation, and it supports low-resource scenarios. It enhances multimodal control and data efficiency, increasing the flexibility of speech synthesis. This version can generate customized speech for specific contexts and performs better in low-resource languages or accent scenarios.

 \textbf{VITS:} VITS (Variational Inference Text-to-Speech) is an end-to-end speech synthesis model based on variational inference. The core of VITS is to combine TTS and Vocoder into a unified framework, achieving high-quality and efficient speech synthesis through end-to-end modeling.

VITS[18] introduces a parallel end-to-end text-to-speech approach, which generates audio that is more natural than current two-stage TTS models. This approach uses variational inference-enhanced normalizing flows and adversarial training to improve the expressiveness of the generative model. It also proposes a stochastic duration predictor, which can synthesize speech with different rhythms.
VITS 2[19] introduces a single-stage text-to-speech synthesis model designed to improve both the quality and efficiency of speech synthesis. It uses an adversarially trained stochastic duration predictor to synthesize more natural speech and improve efficiency. VITS 2 introduces Transformer blocks in the normalizing flow to capture long-term dependencies. It also designs a speaker-conditioned text encoder to better model speaker characteristics in multi-speaker environments.