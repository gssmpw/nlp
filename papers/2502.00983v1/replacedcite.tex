\section{Related Work}
\noindent\textbf{Offline Reinforcement Learning.} Unlike online reinforcement learning, which involves agent-environment interaction during training, offline reinforcement learning operates without such interaction, learning from fixed datasets collected by unknown behavior policies. Recent offline reinforcement learning methods____ focus on addressing distributional shifts and value overestimation due to discrepancies between target and behavior policies. Our work advances the field of offline reinforcement learning by emphasizing the learning of causal task representations and meta-policies from offline datasets.


\noindent\textbf{Offline Meta-Reinforcement Learning.} Offline meta-reinforcement learning (OMRL) aims to address the challenges of generalization and costly data collection by applying meta-learning techniques to pre-collected offline datasets. OMRL methods focus on learning task representation during the meta-training process by utilizing historical trajectories from offline datasets. 
Current OMRL methods are categorized into gradient-based and context-based approaches. Gradient-based methods ____ optimize initial policy parameters for quick task adaptation. Context-based methods ____ frame tasks as contextual Markov Decision Processes (MDPs), focusing on encoding task representations from offline data.
Our work adheres to the context-based OMRL approach, emphasizing the learning of causal task representations to enhance the generalization.


\noindent\textbf{Causal Representation Learning.} 
The central problem for causal representation learning is to discover high-level causal variables from low-level observations____. Current causal representation learning methods mostly fall into two categories: the first category methods____ realize causal representation learning under supervision of ground truth counterfactual images generated according to causal graph; the second category methods____ realize representation learning under annotations and causal graph. Although these methods have been widely applied in computer vision, the application of causal representation learning in reinforcement learning settings is still an open problem____. In this paper, we apply causal representation learning in a context-based OMRL setting to enhance the generalization ability. To the best of our knowledge, this is the first work to propose the application of causal representation learning in the context-based OMRL setting.


\noindent\textbf{Mutual Information Optimization and Contrastive Learning.}
Mutual information optimization and contrastive learning are two critical techniques for learning data representation. Specifically, mutual information optimization learns the essential data representation by enhancing the correlation between variables and maximizing their mutual information____. Contrastive learning learns data representation by maximizing the similarity between related samples and minimizing the similarity between unrelated samples____. Our work introduces mutual information optimization and contrastive learning into task representation learning for context-based OMRL, aiming to enhance the representation learned through causal representation learning.