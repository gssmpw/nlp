@article{Anil2021,
  title={Large-scale Differentially Private BERT},
  author={Anil, Rohan and Ghazi, Badih and Gupta, Vineet and Kumar, Ravi and Manurangsi, Pasin},
  journal={arXiv preprint arXiv:2108.01624},
  year={2021}
}

@article{abad2024copyright,
  title={Copyright-Protected Language Generation via Adaptive Model Fusion},
  author={Abad, Javier and Donhauser, Konstantin and Pinto, Francesco and Yang, Fanny},
  journal={arXiv preprint arXiv:2412.06619},
  year={2024}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@article{biderman2024emergent,
  title={Emergent and predictable memorization in large language models},
  author={Biderman, Stella and Prashanth, Usvsn and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raff, Edward},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@article{carlini2022quantifying,
  title={Quantifying memorization across neural language models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  journal={arXiv preprint arXiv:2202.07646},
  year={2022}
}

@article{elkin2023can,
  title={Can Copyright be Reduced to Privacy?},
  author={Elkin-Koren, Niva and Hacohen, Uri and Livni, Roi and Moran, Shay},
  journal={arXiv preprint arXiv:2305.14822},
  year={2023}
}

@article{hans2024like,
  title={Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs},
  author={Hans, Abhimanyu and Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Kazemi, Hamid and Singhania, Prajwal and Singh, Siddharth and Somepalli, Gowthami and Geiping, Jonas and Bhatele, Abhinav and others},
  journal={arXiv preprint arXiv:2406.10209},
  year={2024}
}

@misc{huang2024demystifyingverbatimmemorizationlarge,
      title={Demystifying Verbatim Memorization in Large Language Models}, 
      author={Jing Huang and Diyi Yang and Christopher Potts},
      year={2024},
      eprint={2407.17817},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.17817}, 
}

@article{ippolito2022preventing,
  title={Preventing verbatim memorization in language models gives a false sense of privacy},
  author={Ippolito, Daphne and Tram{\`e}r, Florian and Nasr, Milad and Zhang, Chiyuan and Jagielski, Matthew and Lee, Katherine and Choquette-Choo, Christopher A and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2210.17546},
  year={2022}
}

@article{jang2022knowledge,
  title={Knowledge unlearning for mitigating privacy risks in language models},
  author={Jang, Joel and Yoon, Dongkeun and Yang, Sohee and Cha, Sungmin and Lee, Moontae and Logeswaran, Lajanugen and Seo, Minjoon},
  journal={arXiv preprint arXiv:2210.01504},
  year={2022}
}

@inproceedings{kandpal2022deduplicating,
  title={Deduplicating training data mitigates privacy risks in language models},
  author={Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={10697--10711},
  year={2022},
  organization={PMLR}
}

@article{karamolegkou2023copyright,
  title={Copyright violations and large language models},
  author={Karamolegkou, Antonia and Li, Jiaang and Zhou, Li and S{\o}gaard, Anders},
  journal={arXiv preprint arXiv:2310.13771},
  year={2023}
}

@article{maini2023can,
  title={Can neural network memorization be localized?},
  author={Maini, Pratyush and Mozer, Michael C and Sedghi, Hanie and Lipton, Zachary C and Kolter, J Zico and Zhang, Chiyuan},
  journal={arXiv preprint arXiv:2307.09542},
  year={2023}
}

@article{mireshghallah2022memorization,
  title={Memorization in nlp fine-tuning methods},
  author={Mireshghallah, Fatemehsadat and Naseri, Mohammadali and Holzenberger, Nils and Mani, Pratyush and Ramaswamy, Harsha Nori and Khani, Mohammad and Tran, Daniel and Tramer, Florian},
  journal={arXiv preprint arXiv:2205.12506},
  year={2022}
}

@article{nasr2311scalable,
  title={Scalable Extraction of Training Data from (Production) Language Models. arXiv 2023},
  author={Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A Feder and Ippolito, Daphne and Choquette-Choo, Christopher A and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
  journal={arXiv preprint arXiv:2311.17035}
}

@article{pinto2024extracting,
  title={Extracting training data from document-based VQA models},
  author={Pinto, Francesco and Rauschmayr, Nathalie and Tram{\`e}r, Florian and Torr, Philip and Tombari, Federico},
  journal={arXiv preprint arXiv:2407.08707},
  year={2024}
}

@article{sakarvadia2024mitigating,
  title={Mitigating Memorization In Language Models},
  author={Sakarvadia, Mansi and Ajith, Aswathy and Khan, Arham and Hudson, Nathaniel and Geniesse, Caleb and Chard, Kyle and Yang, Yaoqing and Foster, Ian and Mahoney, Michael W},
  journal={arXiv preprint arXiv:2410.02159},
  year={2024}
}

@article{schwarzschild2024rethinking,
  title={Rethinking llm memorization through the lens of adversarial compression},
  author={Schwarzschild, Avi and Feng, Zhili and Maini, Pratyush and Lipton, Zachary C and Kolter, J Zico},
  journal={arXiv preprint arXiv:2404.15146},
  year={2024}
}

@article{shumailov2024ununlearning,
  title={Ununlearning: Unlearning is not sufficient for content regulation in advanced generative ai},
  author={Shumailov, Ilia and Hayes, Jamie and Triantafillou, Eleni and Ortiz-Jimenez, Guillermo and Papernot, Nicolas and Jagielski, Matthew and Yona, Itay and Howard, Heidi and Bagdasaryan, Eugene},
  journal={arXiv preprint arXiv:2407.00106},
  year={2024}
}

@misc{zhou2023quantifyinganalyzingentitylevelmemorization,
      title={Quantifying and Analyzing Entity-level Memorization in Large Language Models}, 
      author={Zhenhong Zhou and Jiuyang Xiang and Chaomeng Chen and Sen Su},
      year={2023},
      eprint={2308.15727},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.15727}, 
}

