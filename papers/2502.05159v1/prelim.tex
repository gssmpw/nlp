\section{Preliminaries}
\label{section:preliminaries}
\subsection{Language Models: Notation and Setup}

Language models operate over a vocabulary \(\mathcal{V} = \{v_1, \ldots, v_{|\mathcal{V}|}\}\) of typically \(|\mathcal{V}| \approx 10^5-10^6\) tokens. Given an input prompt \((x_{-l_p}, \ldots, x_{-1}) \in \mathcal{V}^{l_p}\) followed by a response sequence \((x_0, \ldots, x_{l-1}) \in \mathcal{V}^{l}\), where \(l_p\) and \(l\) denote prompt and response lengths respectively, an auto-regressive language model parametrizes the joint probability by factorizing over conditional probabilities:
%
\begin{equation}
    p(x_0, \ldots, x_l | x_{-l_p}, \ldots, x_{-1}) \;=\; \prod_{i=0}^l p(x_i \,|\, x_{<i}),
\label{eq:autoregressive}
\end{equation}
%
where for each position \(i\), the model outputs a distribution \(\mathbf{p}_i[v]\) over \(\mathcal{V}\), with \(\mathbf{p}_i[v] = p(x_i = v | x_{<i})\) for any \(v \in \mathcal{V}\).

Since language models are trained to maximize the likelihood of observed sequences, they tend to assign high probabilities to specific tokens that follow given prefixes. This increases the risk of memorization and \emph{verbatim reproduction} of training data, including sensitive or copyrighted materials. This issue extends beyond exact string matches, as models can also regenerate slightly altered yet recognizable versions of memorized content. 


\subsection{Extractable Memorization}
Memorization in language models can manifest in various ways, but a practically relevant and widely adopted framework is \emph{extractable memorization}~\citep{carlini2021extracting, carlini2022quantifying}. This definition captures when training data can be systematically reproduced via prompting, making it directly applicable to evaluating real-world risks. \citet{carlini2021extracting} demonstrated that LLMs can be induced to regurgitate training data by providing partial context, revealing vulnerabilities in deployed systems. 

\begin{definition}[Extractable Memorization]
\label{definition:memorization}
A sequence \(x = (x_0, \ldots, x_{l-1})\) of length \(l\) is considered \emph{extractable with \(l_p\) tokens of context} from a language model \(p\) if there exists a prefix \(x_{-} = (x_{-l_p}, \ldots, x_{-1})\) of length \(l_p\) such that 
\([\,x_{-} \,\|\, x\,]\) appears in the training data of \(p\), and \(p\) reproduces \(x\) via greedy decoding. 
\\
\\
Formally, for each \(i \in \{\,0,\ldots,l-1\}\):
\[
x_i \;=\;
\arg\max_{x' \in \mathcal{V}} \; p\bigl(x' \,|\, x_{<i},x_{-}\bigr).
\]
\end{definition}
%
%

This definition is practically useful because: (1) it aligns with real-world risks of copyright and memorized generation~\citep{nasr2311scalable, karamolegkou2023copyright}, (2) it provides a concrete, testable condition that can be evaluated on real models, and (3) it extends to models of different sizes, capturing the well-documented trend that larger models memorize more data~\citep{carlini2022quantifying}. This observation is important in motivating our methodology in Section~\ref{section:methodology}.

