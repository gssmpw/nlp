\section{Methodology}
\label{section:methodology}

This section builds on the observation from Section~\ref{section:preliminaries} that language models with fewer than 100M parameters (e.g., DistilGPT-2, Pythia-70M) exhibit negligible memorization, in contrast to billion-parameter models, which often reproduce long verbatim segments of training data. We introduce \sys, a lightweight, post-hoc method that combines the strengths of both model scales: it employs a smaller auxilliary model to selectively replace the token probabilities of a larger, more capable model during inference. Below, we describe the algorithm in detail (Section~\ref{subsec:proposed_algorithm}) and present theoretical guarantees (Section~\ref{subsec:theory}).

\begin{table*}[htbp]
\centering
\caption{Memorization for WritingPrompts and MathAbstracts datasets. ML: Matching Length, EMR: Exact Match Rate., Lev. : Normalized Levenshtein Distance}
\label{tab:memorization_finetuning}
\begin{tabular}{l|cccc|cccc}
\toprule
& \multicolumn{4}{c|}{WritingPrompts} & \multicolumn{4}{c}{MathAbstracts} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
Method & ML\(\downarrow\) & ROUGE-L\(\downarrow\) & Lev.\(\uparrow\) & EMR\(\downarrow\) & ML\(\downarrow\) & ROUGE-L\(\downarrow\) & Lev.\(\uparrow\) & EMR\(\downarrow\)\\
\midrule
Standard & 464.02 & 0.8910 & 0.0964 & 83.35 & 450.37 & 0.9808 & 0.0251 & 93.55 \\
CP-Fuse-mix & 280.30 & 0.5770 &0.3695	 & 49.15 & 233.73 & 0.6242 & 0.3643 & 47.10 \\
CP-Fuse-half & 12.46 & 0.1676	 & 0.7275 & 0.00 & 15.27 & 0.2644 & 0.7062 & 0.10 \\
\sys & 19.65 & 0.1929	 & 0.7064 & 0.10 & 53.01 & 0.3752 & 0.6032 & 1.80 \\
\bottomrule
\end{tabular}
\end{table*}



\subsection{Setup and Assumptions}
\label{subsec:main-assumptions}
%
%
Let $\pmain$ be a large-scale language model with billions of parameters (e.g., GPT-4 or LLaMA), trained on a dataset 
$\mathcal{D}$. Let $\paux$ be a smaller auxiliary model with significantly fewer parameters. Their probability 
distributions over tokens are denoted by $\pmain(x_t \mid x_{<t})$ and $\paux(x_t \mid x_{<t})$, respectively. 
We make the following assumption: the main model can reproduce verbatim segments from $\mathcal{D}$ due to its large 
capacity, whereas the smaller model exhibits minimal memorization and serves as a reference for modifying token probabilities.



\begin{algorithm}[t]
\caption{\sys}
\label{alg:tokenswap}
\begin{algorithmic}[1]
\REQUIRE Main model \(\pmain\), auxiliary model \(\paux\), token subset \(\gramset\), prompt \(x_{<0}\)
\FOR{\(i = 0, 1, \ldots\)}
    \STATE \(\pmain_i \gets \pmain(\cdot | x_{<i})\) \COMMENT{Get main model probabilities}
    \STATE \(\paux_i \gets \paux(\cdot | x_{<i})\) \COMMENT{Get auxiliary model probabilities}
    \STATE \(\alpha \gets \frac{\sum_{v \in \gramset} \pmain_i[v]}{\sum_{v \in \gramset} \paux_i[v]}\) \COMMENT{Compute normalization}
    \FOR{\(v \in \mathcal{V}\)}
        \STATE \(\pfinal_i[v] \gets \begin{cases}
            \pmain_i[v], & \text{if } v \notin \gramset \\
            \alpha \cdot \paux_i[v], & \text{if } v \in \gramset
        \end{cases}\)
    \ENDFOR
    \STATE \(x_i \sim \pfinal_i\) \COMMENT{Sample next token}
\ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{Algorithm Overview}
\label{subsec:proposed_algorithm}
Given a main model \(\pmain\) and an auxiliary model \(\paux\), \sys selectively replaces probabilities for a fixed subset of tokens \(\gramset \subset \mathcal{V}\). Algorithm \ref{alg:tokenswap} details the procedure.

% \babak{Here I tried to explain what $\gramset$ is ealy on. }

% \babak{At each position \(i\), \sys\ queries both \(\pmain\) and \(\paux\) to obtain probability distributions conditioned on the current context \(x_{<i}\). To balance memorization mitigation and fluency, \sys\ modifies only a specific subset of tokens \(\gramset \subset \mathcal{V}\), which are chosen to maximize disruption of memorized sequences while minimizing impact on general text generation. The selection of \(\gramset\) is critical for ensuring this trade-off and is discussed in detail below. For tokens in \(\gramset\), probabilities from the main model are replaced with scaled probabilities from the auxiliary model, ensuring that \(\pfinal\) remains a valid probability distribution. Algorithm \ref{alg:tokenswap} details the procedure.}
%5
At each position \(i\), \sys\ queries both \(\pmain\) and \(\paux\) to obtain probability distributions conditioned on the current context \(x_{<i}\). To balance memorization mitigation and fluency, \sys\ modifies only a specific subset of tokens \(\gramset \subset \mathcal{V}\), which are chosen to maximize disruption of memorized sequences while minimizing impact on general text generation. For tokens in \(\gramset\), probabilities from the main model are replaced with scaled probabilities from the auxiliary model, with scaling factor \(\alpha\) ensuring \(\pfinal\) remains a valid distribution. This prevents reproduction of memorized sequences: if any token \(x_i\) in a memorized sequence belongs to \(\gramset\), its probability under \(\pfinal\) is determined by the auxiliary model which exhibits minimal memorization, disrupting the chain of conditional probabilities required for verbatim generation of long sequences. Importantly, for tokens \(v \notin \gramset\), their probabilities remain unchanged, i.e., \(\pfinal_i[v] = \pmain_i[v]\).
%%
% %
% \paragraph{Selecting \(\gramset\) for Effective Memorization Disruption}  
% The selection of \(\gramset\) is critical to balancing memorization mitigation and model performance. By modifying the probabilities of specific tokens, \sys disrupts memorized sequences while preserving fluency. However, not all tokens are equally effective for this purpose. \(\gramset\) should consist of tokens that frequently appear in memorized text, as replacing their probabilities reduces the likelihood of exact reproduction. At the same time, modifying inappropriate tokens can degrade model performance, especially for specialized tasks.
% %
% To ensure effectiveness, \(\gramset\) should satisfy two key criteria. First, it must contain tokens that frequently occur in natural text, increasing the chance of intervention in memorized sequences. Second, it should avoid tokens where probability replacement significantly impacts the modelâ€™s capabilities. For instance, if \(\gramset\) includes numeric tokens, mathematical reasoning could degrade since smaller models often struggle with such content.
% %
% Empirical studies suggest that small models approximate the probabilities of high-frequency function words well while diverging more on rare or domain-specific terms \citep{pinto2024fair}. Additionally, restricting \(\paux\) to function words, determiners, and prepositions preserves fluency and coherence \citep{Eldan2023}. Based on these insights, we construct \(\gramset\) from high-frequency grammar tokens, ensuring effective memorization disruption while maintaining text generation quality. While this approach is well-suited for natural language, structured domains such as code may require domain-specific adaptations.

\paragraph{Selecting \(\gramset\) for Effective Memorization Disruption}  
The choice of \(\gramset\) affects both memorization and model performance. By modifying token probabilities, \sys disrupts memorized sequences while preserving fluency. However, not all tokens are equally effective for this purpose. \(\gramset\) should consist of tokens that frequently appear in memorized text, as replacing their probabilities reduces the likelihood of exact reproduction. At the same time, modifying inappropriate tokens can degrade model performance, especially for specialized tasks.

Therefore, \(\gramset\) should satisfy two key criteria. First, it must contain tokens that frequently occur in text, increasing the chance of intervention in memorized sequences. Second, it should avoid tokens where probability replacement impacts the model's capabilities. For instance, if \(\gramset\) includes numeric tokens, mathematical reasoning could degrade.

Empirical studies suggest that small models approximate the probabilities of high-frequency function words well while diverging more on rare or domain-specific terms~\citep{pinto2024fair}. Additionally, small language models (\(\approx 100M\) can generate coherent and grammatically correct text~\citep{Eldan2023}. Based on these insights, we construct \(\gramset\) from grammar-based high-frequency tokens (e.g. - 'the', 'in'). Further, since \(\gramset\) consists of high-frequency words, there exists a natural one-to-one mapping between tokens even when \(\pmain\) and \(\paux\) use different tokenizers and vocabularies. While this approach is well-suited for natural language, structured domains such as code may require domain-specific adaptations. Additional details on the construction of \(\gramset\) are provided in Section~\ref{subsection:finetuning} and Appendix~\ref{appendix:listofgramset}.


\subsection{Theoretical Guarantees}
\label{subsec:theory}
This section establishes theoretical guarantees for \sys, demonstrating that it exponentially suppresses the probability of generating long memorized sequences. Since copyright concerns often pertain to verbatim reproduction of extended text spans~\citep{karamolegkou2023copyright}, it is crucial to quantify how \sys\ affects memorization as sequence length increases. Specifically, we prove that if a sequence is extractable under \(\pmain\), its likelihood under \(\pfinal\) decays exponentially with length.

\begin{theorem}[Exponential Decay of Memorized Sequences]
\label{theorem:swap}
Let \(\ptrue\) denote the true data distribution over sequences, defined over the vocabulary \(\mathcal{V}\). Let \(\gamma\) be the fraction of tokens from \(\gramset\) in a typical sequence sampled from \(\ptrue\). Define
\[
\beta 
\;=\;
\mathbb{E}_{x \sim \ptrue}\!\Bigl[\frac{\max_{\,v \,\in\, \gramset}\; \ptrue[v]}{\sum_{v \in \gramset} \ptrue[v]}\Bigr].
\]
Consider a sequence \([\,x_{<0} \,\|\, x]\) of length \(l\) that is \emph{extractable} under \(\pmain\) as per Definition~\ref{definition:memorization}. Let \(q_{\text{final}}\) denote the probability that the same sequence is extractable under \(\pfinal\). Then, the following bound holds:
\[
q_{\text{final}}
\;\leq\;
e^{-\gamma(1-\beta)l}.
\]
Since \(\beta < 1\), the probability of extracting a memorized sequence under \sys\ through greedy sampling decays exponentially with sequence length \(l\), substantially reducing the risk of reproducing long verbatim segments.
\end{theorem}
\paragraph{Proof Sketch: }The key insight is that at each position (i) where the token belongs to \(\gramset\), the probability of continuing along the "memorized path" is dictated by the auxiliary model rather than the main model. Specifically, these tokens appear with frequency \(\gamma\) in typical sequences, and at each such position, the token's probability is determined by \(\paux\) instead of \(\pmain\). The parameter \(\beta\) captures the expected maximum probability assigned to tokens in \(\gramset\) under the true distribution - this bounds how likely the auxiliary model is to generate the memorized token at these positions. Since disruption occurs independently at each position containing a token from \(\gramset\), the overall probability decays exponentially. The formal proof is given in Appendix~\ref{appendix:proof}. 

