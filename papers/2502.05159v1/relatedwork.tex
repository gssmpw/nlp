\section{Related Work}

\subsection{Memorization in LLMs}
LLMs have been shown to memorize and potentially reproduce copyrighted information from their training data \citep{carlini2021extracting, carlini2022quantifying, karamolegkou2023copyright, sok_mem}.  This is demonstrated through prefix attacks, where models prompted with training data prefixes generate their memorized completions.\citet{schwarzschild2024rethinking} formalize this notion based on adversarial compression, requiring that any memorized sequence must be longer than the prefix used to elicit it. \citet{zhou2023quantifyinganalyzingentitylevelmemorization} and \citet{nasr2311scalable} demonstrate that large-scale training data can be extracted without access to training prefixes. Studies further indicate a correlation between model scale and memorization, with larger models regurgitating higher proportions of their training data \citep{carlini2022quantifying, zhou2023quantifyinganalyzingentitylevelmemorization, biderman2024emergent}.


\subsection{Mitigating Memorization and Verbatim Generation}

\paragraph{Pre-training}
Several methods have been developed to reduce memorization and verbatim generation during the model training process. \citet{kandpal2022deduplicating} propose de-duplication though the prevalence of near-duplicate content in large-scale datasets limits its effectiveness. Differential Privacy (DP) \citep{abadi2016deep} provides rigorous privacy guarantees by bounding the influence of individual data points on model predictions. However, DP suffers from high computational costs and negatively impacts model performance \citep{Anil2021}, while potentially being overly restrictive to mitigate verbatim generation \citep{elkin2023can}. Other approaches include excluding fixed fractions of tokens from loss computation \citep{hans2024like} and early stopping during training \citep{mireshghallah2022memorization, pinto2024extracting}. While these pre-training interventions can help reduce memorization, they are typically expensive to implement, often degrade model performance, and are not accessible to end users of the models.

\paragraph{Post-training}
Post-training approaches offer alternative strategies for addressing memorization. One line of work focuses on unlearning methods \citep{maini2023can, jang2022knowledge, sakarvadia2024mitigating}, which attempt to identify and modify hidden neurons and weights associated with memorized content. However, these models can still be prompted to reveal training data \citep{shumailov2024ununlearning} and often suffer from degraded performance \citep{huang2024demystifyingverbatimmemorizationlarge}. This has led to the development of methods that modify model outputs rather than weights. For instance, \citet{ippolito2022preventing} propose blocking generations that exactly match training data, though this approach is limited to exact matches and requires access to the training corpus. \citet{abad2024copyright} present a model fusion approach that uses the weighted sum of logits from two models. The resulting distribution is constrained to be equidistant from the two models. However, this method requires at least two LLMs (with the same vocabulary and tokenizer) trained on disjoint datasets, doubles memory requirements, and can suffer from performance degradation when the distributions of the two training sets differ or one of the models is weak.

