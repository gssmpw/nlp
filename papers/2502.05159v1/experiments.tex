

\section{Experiments}


\label{section:experiments}
In this section, we demonstrate the effectiveness of \textsc{TokenSwap}, in both controlled and real-world settings. Our experiments evaluate \sys along two dimensions:
\begin{itemize}
    \item The method's efficacy in preventing exact and approximate reproduction of training data
    \item The impact on model performance across standard language and commonsense benchmarks
\end{itemize}

We evaluate \sys across three settings to demonstrate its effectiveness. In Section~\ref{subsection:finetuning}, we deliberately induce memorization through extensive fine-tuning on small datasets to stress-test our defense. Section~\ref{subsection:wild} evaluates \sys on production-grade models including Pythia-6.9B and Llama-3-8B. Finally, in Section~\ref{subsection:goldfish}, we compare against Goldfish~\citep{hans2024like}, a pre-training method specifically designed to reduce memorization, showing that our post-hoc approach achieves comparable results without requiring model retraining.
\subsection{Extreme Memorization}
\label{subsection:finetuning}

In order to rigorously evaluate \sys, we create an extreme test case by deliberately inducing memorization through extensive fine-tuning. While \sys can be applied to pre-trained models directly, our baselines require specific experimental conditions for fair comparison. Additionally, analyzing memorization in pre-trained models is challenging since training datasets for most open-weight models like Llama are not publicly available, making it difficult to verify if generated sequences originate from training.

We address these challenges through controlled experiments where we fine-tune a Llama-3.2-3B model \citep{dubey2024llama} on 2,000-sequence subsets from two datasets: MathAbstracts~\citep{zhang2024autonomous} and WritingStories~\citep{fan2018hierarchical}. Following \citet{abad2024copyright}, we train for 50 epochs to deliberately amplify memorization beyond typical levels.

\begin{table}[!htbp]
\centering
\caption{Validation Cross-entropy loss on WritingPrompts and MathAbstracts. Lower values \(\downarrow\) indicate better performance.}
\label{tab:performance_finetuning}
\begin{tabular}{lcc}
\toprule
Method & WritingPrompts & MathAbstracts \\
\midrule
Standard & 6.68 & 4.94 \\
CP-Fuse-mixture & 9.38 & 6.89 \\
CP-Fuse-half & 9.43 & 6.67 \\
\sys & 5.98 & 4.65 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table*}[t]
\centering
\setlength{\tabcolsep}{4pt}
\caption{Memorization for LeetCode and Pile-Memorized datasets. ML: Matching Length, EMR: Exact Match Rate., Lev. : Normalized Levenshtein Distance}
\label{tab:memorization_pretrained}
\begin{tabular}{l|cccc|cccc}
\toprule
& \multicolumn{4}{c|}{LeetCode Dataset} & \multicolumn{4}{c}{Pile-Memorized Dataset} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
Method & ML \(\downarrow\) & ROUGE-L \(\downarrow\) & Levenshtein \(\uparrow\) & ROUGE-L\(>0.8\) \(\downarrow\) & ML \(\downarrow\) & ROUGE-L \(\downarrow\) & Levenshtein\(\uparrow\) & EMR \(\downarrow\) \\
\midrule
Standard & 24.57 & 0.3913 & 0.5962 & 9.65 & 151.55 & 0.7974 & 0.1817 & 65.22 \\
CP-Fuse & 19.44 & 0.3730 & 0.6179 & 7.01 & 97.05 & 0.6247 & 0.3512 & 29.35 \\
\sys & 6.04 & 0.2670 & 0.7105 & 0.96 & 35.10 & 0.3784 & 0.5580 & 5.98 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table}[t]
\centering
\caption{Cross-entropy loss comparison. Lower values (â†“) indicate better performance.}
\label{tab:performance_pretrained}
\begin{tabular}{lcc}
\toprule
Method & Pythia 6.9b \(\downarrow\) & Llama 3 8b \(\downarrow\) \\
\midrule
Standard & 2.80 & 2.38 \\
CP-Fuse & 2.81 & 2.45 \\
\sys & 2.88 & 2.52 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table*}[t]
\centering
\caption{Performance comparison on standard benchmarks. All values are accuracy percentages, higher is better (\(\uparrow\)).}
\label{tab:benchmark_results}
\begin{tabular}{l|ccccccc}
\toprule
Method & WinoGrande \(\uparrow\) & PIQA \(\uparrow\) & OpenBookQA \(\uparrow\)& BoolQ \(\uparrow\) & ARC-E \(\uparrow\) & ARC-C \(\uparrow\) \\
\midrule
Llama 3 8B & 54.69 & 64.84 & 76.56 & 70.31 & 82.03 & 82.81 \\
CP-Fuse & 54.69 & 64.84 & 77.34 &  58.59 & 83.59 & 82.03 \\
\sys & 54.69 & 64.06 & 76.56 &  70.31 & 82.81 & 82.81 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[t]
\caption{Comparison with Goldfish~\citep{hans2024like} for k \(\in\) \{3,4,32\}. }
\label{tab:goldfish}
\centering
\begin{tabular}{lccccc}
\toprule
Method & ML$\downarrow$ & ROUGE-L$\downarrow$ & Levenshtein$\uparrow$ & EMR$\downarrow$ & Cross Entropy$\downarrow$ \\
\midrule
Standard & 73.93 & 0.3796 & 0.5758	 & 7.80 & 3.435 \\
Goldfish ($k$=3) & 12.65 & 0.2265	 & 0.7167 & 0.00 & 3.541 \\
Goldfish ($k$=4) & 14.73 & 0.2332 & 0.7101 & 0.00 & 3.495 \\
Goldfish ($k$=32) & 58.10 & 0.3516 & 0.6025	 & 2.55 & 3.441 \\
\sys & 12.37 & 0.2230 & 0.7197	 & 0.10 & 3.443 \\
\bottomrule
\end{tabular}
\end{table*}


\paragraph{Memorization Metrics}
Our analysis employs both exact and approximate memorization and performance metrics to ensure a comprehensive assessment. Exact memorization is measured through \textit{Matching Length (ML)}, which the number of verbatim characters or tokens generated before first deviation, and \textit{Exact Matching Rate (EMR)}, which computes the fraction of sequences reproduced verbatim. To capture partial memorization, we use the \textit{ROUGE-L} score, which identifies the longest common non-contiguous subsequence and  gives a score between 0 and 1, and the \textit{Normalized Levenshtein Distance}, which quantifies the minimum number of edits needed to transform generated text into the original sequence. Lower scores indicate reduced memorization for Matching Length, Exact Matching Rate, and ROUGE-L. Higher scores are better for Normalized Levenshtein Distance. These metrics are widely used to evaluate memorized generation~\citep{karamolegkou2023copyright, hans2024like, abad2024copyright}. Since tokenization varies across models, the metrics are evaluated at the character level rather than the token level. 

\paragraph{Performance Metrics}
To evaluate overall model performance, we measure cross-entropy loss on a held-out validation set, with lower values indicating greater fluency. 

\paragraph{Setup and Inference-time Baselines}
We compare against CP-Fuse \citep{abad2024copyright}, which samples from weighted combinations of models trained on disjoint datasets. CP-Fuse chooses the weights adaptively to ensure that the final distribution is equidistant from both models averaged over all tokens. Standard refers to the model's default greedy decoding without any memorization mitigation techniques. Further, to evaluate CP-Fuse under realistic conditions where datasets may overlap, we implement two variants: CP-Fuse Half with perfectly disjoint sets of 1,000 sequences each, and CP-Fuse Mixture with 1,500 sequences per model and 500 sequences of overlap.

For \sys, we employ DistilGPT-2~\citep{distilgpt2} as \(\paux\). We construct \(\gramset\) with \(|\gramset| = 110\) tokens using high-frequency 'grammar-based' words. Starting with the 500 most frequent tokens from COCA~\citep{davies2010corpus}, we apply NLTK~\citep{loper2002nltk} part-of-speech filtering to retain:
\begin{itemize}
    \item Core grammatical elements: determiners (\texttt{DT}), prepositions (\texttt{IN}), conjunctions (\texttt{CC})
    \item Pronouns (\texttt{PRP}, \texttt{PRP\$}) and modal verbs (\texttt{MD})
    \item Question-related tokens: wh-words (\texttt{WDT}, \texttt{WP}, \texttt{WRB})
    \item Auxiliary verbs: \textit{be}, \textit{do}, \textit{have}
\end{itemize}
This construction prioritizes tokens with high frequency but low semantic content, ensuring syntactic fluency while minimizing impact on model capabilities. To estimate the frequency of tokens (\(\gamma\)) in \(\gramset\) empirically, we analyzed 2000 samples from the SlimPajama dataset~\citep{soboleva2023slimpajama}, finding \(\gamma = 0.233\). Appendix~\ref{appendix:listofgramset} provides the full list of words in \(\gramset\).

For all experiments and methods, a prefix of 20 tokens is used and the next 128 tokens are greedily sampled (temperature = 0.0).

\paragraph{Results}
Table~\ref{tab:memorization_finetuning} demonstrates \sys's effectiveness in reducing memorization across both datasets. For WritingPrompts, \sys reduces EMR by 800x (from 83.35\% to 0.10\%) and ROUGE-L by 4.6x (from 0.8910 to 0.1929) compared to standard generation. On MathAbstracts, EMR decreases by 50x (from 93.55\% to 1.80\%) and ROUGE-L by 2.6x (from 0.9808 to 0.3752). CP-Fuse-half achieves slightly better results but requires disjoint training sets, while CP-Fuse-mix performs significantly worse due to dataset overlap. The performance gap between WritingPrompts (EMR: 0.10\%) and MathAbstracts (EMR: 1.80\%) aligns with our intuition - \(\gramset\) was designed focusing on natural language tasks. However, \sys still achieves substantial memorization reduction for both domains while maintaining strong performance.

Table~\ref{tab:performance_finetuning} shows \sys achieves the lowest cross-entropy loss across both datasets (5.98 and 4.65 for WritingPrompts and MathAbstracts respectively). The superior performance, even compared to standard generation, suggests our method effectively disrupts memorization pathways while preserving model capabilities. 

To complement our quantitative results, we provide qualitative examples of generations from the WritingPrompts dataset in Appendix~\ref{appendix:examples}.

\subsection{Memorization in the wild}
\label{subsection:wild}
In this section, we demonstrate the efficacy of our approach on production-grade models. We assess the effectiveness of \sys on two widely used pre-trained models: Pythia-6.9B~\citep{biderman2023pythiasuiteanalyzinglarge} and Llama-3-8B~\citep{dubey2024llama}. 

\paragraph{Pile-Memorized Dataset} For Pythia-6.9B, we utilize memorized sequences identified by \citet{chang2024localization} from the Pile dataset, consisting of 32-token prefixes and 48-token suffixes. After filtering to retain only natural language content (excluding code, URLs, etc.), we obtain 184 evaluation examples.

\paragraph{LeetCode Dataset} For LLaMA-3-8B, following \citet{karamolegkou2023copyright}'s demonstration of LeetCode problem memorization, we evaluate on 1,825 LeetCode problems compiled by \citet{gzipchrist2021leetcode}. Since the exact format of LeetCode problems in LLaMA's training data is unknown, we remove punctuation while calculating the memorization metrics. Additionally, instead of exact match rate, we use ROUGE-L \(> 0.8\)  as our threshold for identifying memorized content. For this task, a prefix length of 20 tokens is used and the next 100 tokens are sampled.

\paragraph{Evaluation Setup} When comparing TokenSwap with CP-Fuse, we encountered two key limitations. First, most real-world large language models (LLMs) do not release their training data, making it impossible to verify whether the datasets used by the two models are disjoint. Even for models like Pythia, which provide access to training data, significant overlap exists across LLM training corpora, making it difficult to identify a pair of models with truly disjoint datasets. Second, CP-Fuse requires both models to have the same vocabulary size and tokenizer, which constrains the choice of the second model to those within the same model family. To ensure a fair comparison, we avoided selecting larger models as the second model, as larger models are known to memorize more. Instead, we selected smaller counterparts: Pythia-2.8B for Pythia-6.9B and LLaMA-3.2-3B for LLaMA-3-8B. In fact, this choice benefits CP-Fuse, as smaller models are less prone to memorization. We, however, do not choose very small models (\(<\)100M) since CP-Fuse requires roughly equally capable models. This is discussed further in Appendix~\ref{appendix:baselines}. The setup for the rest of the methods is identical to Section~\ref{subsection:finetuning}.

\paragraph{Memorization Metrics} We use the same Memorization metrics Section~\ref{subsection:finetuning}.

\paragraph{Performance Metrics} We evaluate model performance using cross-entropy loss on samples from \textit{Slimpajama}~\citep{soboleva2023slimpajama} and five-shot learning performance on multiple commonsense reasoning benchmarks: BoolQ \citep{clark2019boolq}, SIQA \citep{sap2019socialiqa}, PIQA \citep{bisk2020piqa}, ARC-Challenge \citep{clark2018think}, ARC-Easy \citep{clark2018think}, OBQA \citep{mihaylov2018can}, and WinoGrande \citep{sakaguchi2021winogrande}. Each task is framed as multiple-choice, comparing standard LLaMA 3 8B, \sys (LLaMA 3 8B + DistilGPT-2), and CP-Fuse (LLaMA 3 8B + LLaMA 3.2B 3B).

\paragraph{Results} Table~\ref{tab:memorization_pretrained} demonstrates \sys substantially reduces both exact and approximate memorization for both LeetCode and Pile datasets. Exact match rate decreases by over 10x compared to standard generation and 5-7x compared to CP-Fuse on both datasets. The average matching length shows similar improvements, reducing by 4-5x versus standard and 3-4x versus CP-Fuse. The consistent improvements in approximate memorization metrics (ROUGE-L and Levenshtein distance) demonstrate that \sys robustly prevents verbatim generation rather than simply introducing small perturbations. CP-Fuse shows limited effectiveness in these real-world scenarios primarily because its core assumption of disjoint training datasets does not hold. Even when using different models, the inherent overlap in web-scale training corpora prevents CP-Fuse from effectively disrupting memorized sequences. 

Table~\ref{tab:performance_pretrained} and~\ref{tab:benchmark_results} demonstrate that \sys largely maintains model performance. The minimal increase in cross-entropy loss and consistent performance across commonsense reasoning benchmarks indicate that our selective token replacement strategy successfully preserves model capabilities on language and commonsense-reasoning. 

\subsection{Comparison with Pre-training Methods}
\label{subsection:goldfish}

While previous sections demonstrate the \sys outperforms post-hoc baselines, we now compare it with Goldfish \citep{hans2024like}, a pre-training approach that reduces memorization by excluding a fraction $1/k$ of tokens from loss computation during training. Since pre-training large models using this loss is expensive, we evaluate on pre-trained goldfish models from \citet{hans2024like}. These models were trained on a subset of RedPajama~\citep{weber2024redpajama} combined with 2000 Wikipedia~\citep{bridge2001wikipedia} sequences. To induce memorization, the Wikipedia sequences were duplicated 50 times during training. We compare against models trained with $k \in \{3,4,32\}$. For \sys, we maintain the same experimental setup from Section~\ref{subsection:finetuning}. Following \citet{hans2024like}, we use identical prefix and suffix lengths for extraction of memorized sequences.

\paragraph{Results} Table~\ref{tab:goldfish} shows \sys achieves comparable or superior performance to Goldfish across all memorization metrics. Notably, \sys obtains the best Matching Length, Rouge-L and Normalized Levenshtein distance scores while maintaining better cross-entropy than the Goldfish variants for \(k = 3,4\). The effectiveness of Goldfish varies with parameter $k$ - smaller values (more aggressive token exclusion) yield stronger memorization reduction but worse performance, as evidenced by higher cross-entropy. This illustrates a key advantage of \sys: we achieve similar memorization reduction without requiring modified training or reduced training data tokens. Figure~\ref{fig:rougelwithgoldfish} further supports this finding, showing nearly identical ROUGE-L score distributions between \sys and Goldfish ($k$=3), indicating that our post-hoc approach matches the most aggressive pre-training variant.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/rougewithgoldfish.pdf}
    \caption{We compare \sys with Goldfish~\citep{hans2024like} on RougeL score distributions for Wikipedia generations~\citep{bridge2001wikipedia}. The similar distributions of \sys and Goldfish (k=3) demonstrate that our inference-time approach is comparable to expensive pre-training methods in reducing memorization. }
    \label{fig:rougelwithgoldfish}
\end{figure}
