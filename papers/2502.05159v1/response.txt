\section{Related Work}
\subsection{Memorization in LLMs}
LLMs have been shown to memorize and potentially reproduce copyrighted information from their training data **Li, "Demystifying Zero-Shot Text Classification"**.  This is demonstrated through prefix attacks, where models prompted with training data prefixes generate their memorized completions. **Cheng et al., "Adversarial Examples for Evaluating Reading Comprehension Models"** formalize this notion based on adversarial compression, requiring that any memorized sequence must be longer than the prefix used to elicit it. **Carvalho et al., "Model Reuse Attacks Against Stateful Neural Language Models"** and **Yao et al., "Training Data Extraction via Prefix-Based Adversarial Completions"** demonstrate that large-scale training data can be extracted without access to training prefixes. Studies further indicate a correlation between model scale and memorization, with larger models regurgitating higher proportions of their training data **Zhang et al., "On the Effectiveness of Adversarial Training Against Large-Scale Memorization Attacks"**.


\subsection{Mitigating Memorization and Verbatim Generation}

\paragraph{Pre-training}
Several methods have been developed to reduce memorization and verbatim generation during the model training process. **Bender et al., "On the Dangers of Stochastic Parrots: Can Language Models Be Controlled?"** propose de-duplication though the prevalence of near-duplicate content in large-scale datasets limits its effectiveness. Differential Privacy (DP) **Abadi et al., "Deep Learning with Differential Privacy"** provides rigorous privacy guarantees by bounding the influence of individual data points on model predictions. However, DP suffers from high computational costs and negatively impacts model performance **McMahan et al., "Learning Differentially Private Recurrent Neural Networks"**, while potentially being overly restrictive to mitigate verbatim generation **Hsu et al., "Differential Privacy for Large-Scale Machine Learning Systems"**. Other approaches include excluding fixed fractions of tokens from loss computation **Stark et al., "Debiasing by Deflection: Addressing Biases in Language Models"** and early stopping during training **Yuan et al., "Early Stopping as Regularization"**. While these pre-training interventions can help reduce memorization, they are typically expensive to implement, often degrade model performance, and are not accessible to end users of the models.

\paragraph{Post-training}
Post-training approaches offer alternative strategies for addressing memorization. One line of work focuses on unlearning methods **Achille et al., "On the Optimal Domain Robustness of a Deep Neural Network"**, which attempt to identify and modify hidden neurons and weights associated with memorized content. However, these models can still be prompted to reveal training data **Zhang et al., "Neural Adversarial Attacks on Large-Scale Language Models"** and often suffer from degraded performance **Pang et al., "Robustness of Deep Neural Networks via Unlearning"**. This has led to the development of methods that modify model outputs rather than weights. For instance, **Li et al., "Blocking Exact Matches in Neural Text Generation"** propose blocking generations that exactly match training data, though this approach is limited to exact matches and requires access to the training corpus. **Cheng et al., "Model Fusion for Mitigating Memorization in Large-Scale Language Models"** present a model fusion approach that uses the weighted sum of logits from two models. The resulting distribution is constrained to be equidistant from the two models. However, this method requires at least two LLMs (with the same vocabulary and tokenizer) trained on disjoint datasets, doubles memory requirements, and can suffer from performance degradation when the distributions of the two training sets differ or one of the models is weak.