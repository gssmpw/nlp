

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019}
}

@book{jurafsky2008speech,
  author       = {Daniel Jurafsky and
                  James H. Martin},
  title        = {Speech and language processing - an introduction to natural language
                  processing, computational linguistics, and speech recognition},
  series       = {Prentice Hall series in artificial intelligence},
  publisher    = {Prentice Hall},
  year         = {2000},
  isbn         = {978-0-13-095069-7},
  timestamp    = {Mon, 22 Jul 2019 16:40:55 +0200},
  biburl       = {https://dblp.org/rec/books/daglib/0004298.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{clark2019does,
  author       = {Kevin Clark and
                  Urvashi Khandelwal and
                  Omer Levy and
                  Christopher D. Manning},
  editor       = {Tal Linzen and
                  Grzegorz Chrupala and
                  Yonatan Belinkov and
                  Dieuwke Hupkes},
  title        = {What Does {BERT} Look at? An Analysis of BERT's Attention},
  booktitle    = {Proceedings of the 2019 {ACL} Workshop BlackboxNLP: Analyzing and
                  Interpreting Neural Networks for NLP, BlackboxNLP@ACL 2019, Florence,
                  Italy, August 1, 2019},
  pages        = {276--286},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/W19-4828},
  doi          = {10.18653/V1/W19-4828},
  timestamp    = {Sun, 12 Nov 2023 02:16:34 +0100},
  biburl       = {https://dblp.org/rec/conf/blackboxnlp/ClarkKLM19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{tenney2019bert,
  author       = {Ian Tenney and
                  Dipanjan Das and
                  Ellie Pavlick},
  editor       = {Anna Korhonen and
                  David R. Traum and
                  Llu{\'{\i}}s M{\`{a}}rquez},
  title        = {{BERT} Rediscovers the Classical {NLP} Pipeline},
  booktitle    = {Proceedings of the 57th Conference of the Association for Computational
                  Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
                  Volume 1: Long Papers},
  pages        = {4593--4601},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/p19-1452},
  doi          = {10.18653/V1/P19-1452},
  timestamp    = {Fri, 06 Aug 2021 00:41:01 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/TenneyDP19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}



@article{zhang2021counterfactual,
  title={Counterfactual memorization in neural language models},
  author={Zhang, Chiyuan and Ippolito, Daphne and Lee, Katherine and Jagielski, Matthew and Tram{\`e}r, Florian and Carlini, Nicholas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={39321--39362},
  year={2023}
}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{hans2024like,
  title={Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs},
  author={Hans, Abhimanyu and Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Kazemi, Hamid and Singhania, Prajwal and Singh, Siddharth and Somepalli, Gowthami and Geiping, Jonas and Bhatele, Abhinav and others},
  journal={arXiv preprint arXiv:2406.10209},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{ippolito2022preventing,
  title={Preventing verbatim memorization in language models gives a false sense of privacy},
  author={Ippolito, Daphne and Tram{\`e}r, Florian and Nasr, Milad and Zhang, Chiyuan and Jagielski, Matthew and Lee, Katherine and Choquette-Choo, Christopher A and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2210.17546},
  year={2022}
}

@inproceedings{kandpal2022deduplicating,
  title={Deduplicating training data mitigates privacy risks in language models},
  author={Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={10697--10711},
  year={2022},
  organization={PMLR}
}
@article{yu2021differentially,
  title={Differentially private fine-tuning of language models},
  author={Yu, Da and Naik, Saurabh and Backurs, Arturs and Gopi, Sivakanth and Inan, Huseyin A and Kamath, Gautam and Kulkarni, Janardhan and Lee, Yin Tat and Manoel, Andre and Wutschitz, Lukas and Dai, Shangyu and Rastogi, Vahab},
  journal={arXiv preprint arXiv:2110.06500},
  year={2021}
}
@article{mireshghallah2022memorization,
  title={Memorization in nlp fine-tuning methods},
  author={Mireshghallah, Fatemehsadat and Naseri, Mohammadali and Holzenberger, Nils and Mani, Pratyush and Ramaswamy, Harsha Nori and Khani, Mohammad and Tran, Daniel and Tramer, Florian},
  journal={arXiv preprint arXiv:2205.12506},
  year={2022}
}
@article{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}
@article{carlini2022quantifying,
  title={Quantifying memorization across neural language models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  journal={arXiv preprint arXiv:2202.07646},
  year={2022}
}
@article{schwarzschild2024rethinking,
  title={Rethinking llm memorization through the lens of adversarial compression},
  author={Schwarzschild, Avi and Feng, Zhili and Maini, Pratyush and Lipton, Zachary C and Kolter, J Zico},
  journal={arXiv preprint arXiv:2404.15146},
  year={2024}
}
@article{tirumala2022memorization,
  title={Memorization without overfitting: Analyzing the training dynamics of large language models},
  author={Tirumala, Kushal and Markosyan, Aram and Zettlemoyer, Luke and Aghajanyan, Armen},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38274--38290},
  year={2022}
}
@inproceedings{chang2024localization,
  title={Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks},
  author={Chang, Ting-Yun and Thomason, Jesse and Jia, Robin},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={3190--3211},
  year={2024}
}
@article{sakarvadia2024mitigating,
  title={Mitigating Memorization In Language Models},
  author={Sakarvadia, Mansi and Ajith, Aswathy and Khan, Arham and Hudson, Nathaniel and Geniesse, Caleb and Chard, Kyle and Yang, Yaoqing and Foster, Ian and Mahoney, Michael W},
  journal={arXiv preprint arXiv:2410.02159},
  year={2024}
}
@misc{MoE_mem,
      title={Mixture of Parrots: Experts improve memorization more than reasoning}, 
      author={Samy Jelassi and Clara Mohri and David Brandfonbrener and Alex Gu and Nikhil Vyas and Nikhil Anand and David Alvarez-Melis and Yuanzhi Li and Sham M. Kakade and Eran Malach},
      year={2024},
      eprint={2410.19034},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.19034}, 
}

@misc{rlhf_mem,
      title={Measuring memorization in RLHF for code completion}, 
      author={Aneesh Pappu and Billy Porter and Ilia Shumailov and Jamie Hayes},
      year={2024},
      eprint={2406.11715},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.11715}, 
}
@misc{mitigating-mem,
      title={Mitigating Memorization In Language Models}, 
      author={Mansi Sakarvadia and Aswathy Ajith and Arham Khan and Nathaniel Hudson and Caleb Geniesse and Kyle Chard and Yaoqing Yang and Ian Foster and Michael W. Mahoney},
      year={2024},
      eprint={2410.02159},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.02159}, 
}

@misc{sok_mem
,
      title={SoK: Memorization in General-Purpose Large Language Models}, 
      author={Valentin Hartmann and Anshuman Suri and Vincent Bindschaedler and David Evans and Shruti Tople and Robert West},
      year={2023},
      eprint={2310.18362},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.18362}, 
}

@misc{huang2024demystifyingverbatimmemorizationlarge,
      title={Demystifying Verbatim Memorization in Large Language Models}, 
      author={Jing Huang and Diyi Yang and Christopher Potts},
      year={2024},
      eprint={2407.17817},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.17817}, 
}
@misc{zhou2023quantifyinganalyzingentitylevelmemorization,
      title={Quantifying and Analyzing Entity-level Memorization in Large Language Models}, 
      author={Zhenhong Zhou and Jiuyang Xiang and Chaomeng Chen and Sen Su},
      year={2023},
      eprint={2308.15727},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.15727}, 
}
@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{biderman2024emergent,
  title={Emergent and predictable memorization in large language models},
  author={Biderman, Stella and Prashanth, Usvsn and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raff, Edward},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{karamolegkou2023copyright,
  title={Copyright violations and large language models},
  author={Karamolegkou, Antonia and Li, Jiaang and Zhou, Li and S{\o}gaard, Anders},
  journal={arXiv preprint arXiv:2310.13771},
  year={2023}
}
@article{grynbaum2023times,
  title={The Times sues OpenAI and Microsoft over AI use of copyrighted work},
  author={Grynbaum, Michael M and Mac, Ryan},
  journal={The New York Times},
  volume={27},
  year={2023}
}
@article{panwar2025generative,
  title={Generative AI and Copyright Issues Globally: ANI Media v OpenAI},
  author={Panwar, Aklovya},
  journal={Tech Policy Press},
  year={2025},
  month={jan},
  day={8},
  url={https://www.techpolicy.press/generative-ai-and-copyright-issues-globally-ani-media-v-openai/}
}@techreport{bick2024rapid,
  title={The rapid adoption of generative ai},
  author={Bick, Alexander and Blandin, Adam and Deming, David J},
  year={2024},
  institution={National Bureau of Economic Research}
}
@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}
@article{anil2021large,
  title={Large-scale differentially private BERT},
  author={Anil, Rohan and Ghazi, Badih and Gupta, Vineet and Kumar, Ravi and Manurangsi, Pasin},
  journal={arXiv preprint arXiv:2108.01624},
  year={2021}
}
@article{maini2023can,
  title={Can neural network memorization be localized?},
  author={Maini, Pratyush and Mozer, Michael C and Sedghi, Hanie and Lipton, Zachary C and Kolter, J Zico and Zhang, Chiyuan},
  journal={arXiv preprint arXiv:2307.09542},
  year={2023}
}
@article{attias2024information,
  title={Information complexity of stochastic convex optimization: Applications to generalization and memorization},
  author={Attias, Idan and Dziugaite, Gintare Karolina and Haghifam, Mahdi and Livni, Roi and Roy, Daniel M},
  journal={arXiv preprint arXiv:2402.09327},
  year={2024}
}
@article{abad2024copyright,
  title={Copyright-Protected Language Generation via Adaptive Model Fusion},
  author={Abad, Javier and Donhauser, Konstantin and Pinto, Francesco and Yang, Fanny},
  journal={arXiv preprint arXiv:2412.06619},
  year={2024}
}

@article{Kandpal2022,
  title={Deduplicating Training Data Mitigates Privacy Risks in Language Models},
  author={Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  journal={International Conference on Machine Learning},
  pages={10697--10707},
  year={2022},
  publisher={PMLR}
}



@article{Anil2021,
  title={Large-scale Differentially Private BERT},
  author={Anil, Rohan and Ghazi, Badih and Gupta, Vineet and Kumar, Ravi and Manurangsi, Pasin},
  journal={arXiv preprint arXiv:2108.01624},
  year={2021}
}

@article{Shi2022,
  title={Differential Privacy for Pretraining and Fine-tuning in Language Models},
  author={Shi, Weijia and Zhao, Mengzhou and others},
  journal={arXiv preprint arXiv:2202.04614},
  year={2022}
}

@inproceedings{Srivastava2014,
  title={Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  booktitle={Journal of Machine Learning Research},
  volume={15},
  number={56},
  pages={1929--1958},
  year={2014}
}

@article{Tirumala2022,
  title={Limitations of Regularization Techniques in Mitigating Memorization in Large Models},
  author={Tirumala, Neha and Lee, Katherine and others},
  journal={arXiv preprint arXiv:2203.01563},
  year={2022}
}

@inproceedings{Bourtoule2021,
  title={Machine unlearning},
  author={Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
  booktitle={IEEE Symposium on Security and Privacy},
  pages={141--159},
  year={2021},
  organization={IEEE}
}

@article{Liu2024,
  title={Rethinking Machine Unlearning for Large Language Models},
  author={Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and others},
  journal={arXiv preprint arXiv:2402.08787},
  year={2024}
}

@inproceedings{Maini2023,
  title={Can Neural Network Memorization Be Localized?},
  author={Maini, Pratyush and Mozer, Michael C and Sedghi, Hanie and Lipton, Zachary C and Kolter, J Zico and Zhang, Chiyuan},
  booktitle={arXiv preprint arXiv:2307.09542},
  year={2023}
}

@article{Eldan2023,
  title={Who’s Harry Potter? Approximate Unlearning in LLMs},
  author={Eldan, Ronen and Russinovich, Mark},
  journal={arXiv preprint arXiv:2310.02238},
  year={2023}
}

@inproceedings{Barbulescu2024,
  title={To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models},
  author={Barbulescu, George-Octavian and Triantafillou, Peter},
  booktitle={International Conference on Machine Learning},
  year={2024}
}

@inproceedings{Ramanujan2020,
  title={What’s Hidden in a Randomly Weighted Neural Network?},
  author={Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11893--11902},
  year={2020}
}
@dataset{gzipchrist2021leetcode,
    title = {Leetcode Problem Dataset},
    author = {gzipChrist},
    year = {2021},
    publisher = {Kaggle},
    url = {https://www.kaggle.com/datasets/gzipchrist/leetcode-problem-dataset},
    version = {1},
    description = {Dataset of 1,825 Leetcode problems with problem details, statistics, and metadata},
    license = {MIT}
}
@article{eldan2023tinystories,
  title={Tinystories: How small can language models be and still speak coherent english?},
  author={Eldan, Ronen and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2305.07759},
  year={2023}
}
@article{pinto2024fair,
  title={The Fair Language Model Paradox},
  author={Pinto, Andrea and Galanti, Tomer and Balestriero, Randall},
  journal={arXiv preprint arXiv:2410.11985},
  year={2024}
}
@article{davies2010corpus,
  title={The Corpus of Contemporary American English as the first reliable monitor corpus of English},
  author={Davies, Mark},
  journal={Literary and linguistic computing},
  volume={25},
  number={4},
  pages={447--464},
  year={2010},
  publisher={EADH: The European Association for Digital Humanities}
}

@article{pinto2024extracting,
  title={Extracting training data from document-based VQA models},
  author={Pinto, Francesco and Rauschmayr, Nathalie and Tram{\`e}r, Florian and Torr, Philip and Tombari, Federico},
  journal={arXiv preprint arXiv:2407.08707},
  year={2024}
}
@article{jang2022knowledge,
  title={Knowledge unlearning for mitigating privacy risks in language models},
  author={Jang, Joel and Yoon, Dongkeun and Yang, Sohee and Cha, Sungmin and Lee, Moontae and Logeswaran, Lajanugen and Seo, Minjoon},
  journal={arXiv preprint arXiv:2210.01504},
  year={2022}
}
@article{nasr2311scalable,
  title={Scalable Extraction of Training Data from (Production) Language Models. arXiv 2023},
  author={Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A Feder and Ippolito, Daphne and Choquette-Choo, Christopher A and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
  journal={arXiv preprint arXiv:2311.17035}
}
@article{elkin2023can,
  title={Can Copyright be Reduced to Privacy?},
  author={Elkin-Koren, Niva and Hacohen, Uri and Livni, Roi and Moran, Shay},
  journal={arXiv preprint arXiv:2305.14822},
  year={2023}
}

@article{shumailov2024ununlearning,
  title={Ununlearning: Unlearning is not sufficient for content regulation in advanced generative ai},
  author={Shumailov, Ilia and Hayes, Jamie and Triantafillou, Eleni and Ortiz-Jimenez, Guillermo and Papernot, Nicolas and Jagielski, Matthew and Yona, Itay and Howard, Heidi and Bagdasaryan, Eugene},
  journal={arXiv preprint arXiv:2407.00106},
  year={2024}
}

@article{weber2024redpajama,
  title={Redpajama: an open dataset for training large language models},
  author={Weber, Maurice and Fu, Daniel and Anthony, Quentin and Oren, Yonatan and Adams, Shane and Alexandrov, Anton and Lyu, Xiaozhong and Nguyen, Huu and Yao, Xiaozhe and Adams, Virginia and others},
  journal={arXiv preprint arXiv:2411.12372},
  year={2024}
}

@misc{cr-dataset,
      title={LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models}, 
      author={Zhiqiang Hu and Lei Wang and Yihuai Lan and Wanyu Xu and Ee-Peng Lim and Lidong Bing and Xing Xu and Soujanya Poria and Roy Ka-Wei Lee},
      year={2023},
      eprint={2304.01933},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.01933}, 
}
@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}

@article{sap2019socialiqa,
  title={Socialiqa: Commonsense reasoning about social interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09728},
  year={2019}
}
@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}
@article{bridge2001wikipedia,
  title={Wikipedia, the free encyclopedia},
  author={Bridge, Astoria-Megler},
  journal={San Francisco (CA): Wikimedia Foundation},
  year={2001}
}
@article{xie2023defending,
  title={Defending chatgpt against jailbreak attack via self-reminders},
  author={Xie, Yueqi and Yi, Jingwei and Shao, Jiawei and Curl, Justin and Lyu, Lingjuan and Chen, Qifeng and Xie, Xing and Wu, Fangzhao},
  journal={Nature Machine Intelligence},
  volume={5},
  number={12},
  pages={1486--1496},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{distilgpt2,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC$^2$ Workshop},
  year={2019}
}
@article{loper2002nltk,
  title={Nltk: The natural language toolkit},
  author={Loper, Edward and Bird, Steven},
  journal={arXiv preprint cs/0205028},
  year={2002}
}
@inproceedings{zhang2024autonomous,
  title={Autonomous data selection with language models for mathematical texts},
  author={Zhang, Yifan and Luo, Yifan and Yuan, Yang and Yao, Andrew C},
  booktitle={ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models},
  year={2024}
}
@article{fan2018hierarchical,
  title={Hierarchical neural story generation},
  author={Fan, Angela and Lewis, Mike and Dauphin, Yann},
  journal={arXiv preprint arXiv:1805.04833},
  year={2018}
}
@misc{soboleva2023slimpajama,
  title={SlimPajama: A 627B token cleaned and deduplicated version of RedPajama},
  author={Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
  year={2023},
  publisher={June}
}
@misc{biderman2023pythiasuiteanalyzinglarge,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.01373}, 
}
