\newpage
\appendix
\onecolumn
\section{Examples}%
We provide examples of text generated by standard greedy decoding and \sys on four random examples from the WritingPrompts dataset. Memorized text is in red.
\label{appendix:examples}
\definecolor{boxcolor}{RGB}{255, 240, 220}  % Light peach
\definecolor{headercolor}{RGB}{25, 25, 112}  % Dark blue

\textbf{Example 1}
% Create custom box styles
\tcbset{
    base style/.style={
        enhanced,
        colback=white,
        colframe=gray!50,
        fonttitle=\bfseries,
        arc=2mm,
        boxrule=0.5pt,
        left=3mm,
        right=3mm,
        top=2mm,
        bottom=2mm,
        breakable
    }
}

% The boxes
\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=Prefix
]
\text{\#\#\#} Prompt: [ WP ] Magic - once a real and potent force but as the world population
\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=Suffix
]
grew from millions to billions the shared mana per person is now negligible. A group of astronauts helplessly watching the Earth perish experience something... unexpected.
\\
\\
\text{\#\#\#} Story: Lauren sobbed. She was engaged to be married as soon as we got back home. We all lost loved ones. Hell, we all lost everything we ever knew. 
\\
\\
First the atmosphere started turning black. We got word that the Koreans had started a nuclear war. But within minutes of receiving news, all radio contact was lost. We didn't even have a chance to tell anyone goodbye. Now, there's only 4 of us left. 
\\
\\
Lauren hasn't
\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=Standard Generation
]
{\color{red}grew from millions to billions the shared mana per person is now negligible. A group of astronauts helplessly watching the Earth perish experience something... unexpected.
\\
\\
text{\#\#\#} Story: Lauren sobbed. She was engaged to be married as soon as we got back home. We all lost loved ones. Hell, we all lost everything we ever knew. 
\\
\\
First the atmosphere started turning black. We got word that the Koreans had started a nuclear war. But within minutes of receiving news, all radio contact was lost. We didn't even have a chance to tell anyone goodbye. Now, there's only 4 of us left. 
\\
\\
Lauren hasn't}
\end{tcolorbox}



\vspace{2mm}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=\sys Generation
]
{\color{red}grew} and the number of wizards and witches declined, the world began to suffer. Now the world suffers from a lack of magic, and the government is tasked with maintaining the only magic university in the world.
\\
\\
\text{\#\#\#} Story: `` Welcome to Magic University.'' 
\\
\\
The spell is cast, and the portal opens. Through the portal, the world is revealed to be a beautiful place. The grass is a deep emerald, and the trees sky blue. It is a place of incredible beauty, and the students here are lucky enough to have grown up in such a place. The university is built on the side of the volcano, and the students live
\end{tcolorbox}

\newpage
\textbf{Example 2}
\definecolor{boxcolor}{RGB}{255, 240, 220}  % Light peach
\definecolor{headercolor}{RGB}{25, 25, 112}  % Dark blue

\tcbset{
    base style/.style={
        enhanced,
        colback=white,
        colframe=gray!50,
        fonttitle=\bfseries,
        arc=2mm,
        boxrule=0.5pt,
        left=3mm,
        right=3mm,
        top=2mm,
        bottom=2mm,
        breakable
    }
}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=Prefix
]
\text{\#\#\#} Prompt: [ WP ] The year is 2040, and you are the last smoker
\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=Suffix
]
alive. The `` Quit Smoking '' ads get personal.
\\
\\
\text{\#\#\#} Story: `` Quit now Dan!'' said the man on the talking billboard. A finger pointing at the lone figure walking down the empty street. Empty due to everyone crossing to the other side to avoid him and his smog. 
\\
Dan paused by a waste bin, small cardboard box in hand. He looked from the box to the bin and back again. Those across the street stopped and stared, ready to witness history being made. 
\\
There was absolute silence as Dan lifted his hand, as if to throw the packet towards the bin, all eyes now on him, even the
\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=Standard Generation
]
{\color{red}alive. The `` Quit Smoking '' ads get personal.
\\
\\
\text{\#\#\#} Story: `` Quit now Dan!'' said the man on the talking billboard. A finger pointing at the lone figure walking down the empty street. Empty due to everyone crossing to the other side to avoid him and his smog. 
\\
Dan paused by a waste bin, small cardboard box in hand. He looked from the box to the bin and back again. Those across the street stopped and stared, ready to witness history being made. 
\\
There was absolute silence as Dan lifted his hand, as if to throw the packet towards the bin, all eyes now on him, even the}
\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=\sys Generation
]
{\color{red}alive. The `` Quit Smoking '' ads get personal.
\\
\\
\text{\#\#\#} Story: `` Quit now Dan!'' said the man} who was once John, who now went to work as a telephone scammer. `` You're killing yourself, and you know what? I know you're lying. I know you're lying and you're sucking your parents pockets dry. They worked hard for you know what? And you're not grateful. You never thank me know what? I never see you thanking me. I never see you thanking anyone. You just keep on doing it, killing yourself, and I'm not going to stand by you. I'm not
\end{tcolorbox}

\newpage
\textbf{Example 3}
\definecolor{boxcolor}{RGB}{255, 240, 220}  % Light peach
\definecolor{headercolor}{RGB}{25, 25, 112}  % Dark blue

\tcbset{
    base style/.style={
        enhanced,
        colback=white,
        colframe=gray!50,
        fonttitle=\bfseries,
        arc=2mm,
        boxrule=0.5pt,
        left=3mm,
        right=3mm,
        top=2mm,
        bottom=2mm,
        breakable
    }
}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=Prefix
]
\text{\#\#\#} Prompt: [ WP ] A young man killed his abusive father. The court has exoner
\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=Suffix
]
ated him, but society hasn't.
\\
\\
\text{\#\#\#} Story: I can see disapproving stares from my classmates and teachers all the time. Nobody wants to stay near me anymore. All my friends avoid me or ignore me. Every classmate stays far away from me. 
\\
\\
Eventually, my smile turns cracked. In the mirror, I see haunting eyes looking back at me. Memories and visions of my father lying there with the side of his head completely covered in blood. So this is what true loneliness feels like. 
\\
\\
I remember my father. He tortured me, hit me, locked me out of the house. I oftened stayed at
\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=Standard Generation
]
{\color{red}ated him, but society hasn't.
\\
\\
\text{\#\#\#} Story: I can see disapproving stares from my classmates and teachers all the time. Nobody wants to stay near me anymore. All my friends avoid me or ignore me. Every classmate stays far away from me. 
\\
\\
Eventually, my smile turns cracked. In the mirror, I see haunting eyes looking back at me. Memories and visions of my father lying there with the side of his head completely covered in blood. So this is what true loneliness feels like. 
\\
\\
I remember my father. He tortured me, hit me, locked me out of the house. I oftened stayed at}
\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=\sys Generation
]
{\color{red}ated him, but society hasn't.
\\
\\
\text{\#\#\#} Story: I} was abused. I was taken advantage of. I was exploited. 
\\
\\
Who is the real villain here? The one who committed the act, the one who pushed me past the point where I was comfortable, was the villain. Or the one who stood idly by and was complicit in the crime, the one who provided the weapon, the one who pushed me past the point where I was comfortable. 
\\
\\
You see, I was comfortable. I was happy. I was loved. I was cared for. 
\\
\\
But then I grew up. I learned to think about it
\end{tcolorbox}

\newpage
\textbf{Example 4}
\definecolor{boxcolor}{RGB}{255, 240, 220}  % Light peach
\definecolor{headercolor}{RGB}{25, 25, 112}  % Dark blue

\tcbset{
    base style/.style={
        enhanced,
        colback=white,
        colframe=gray!50,
        fonttitle=\bfseries,
        arc=2mm,
        boxrule=0.5pt,
        left=3mm,
        right=3mm,
        top=2mm,
        bottom=2mm,
        breakable
    }
}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=Prefix
]
\text{\#\#\#} Prompt: [ WP ] A demon and an angel meet up weekly in a coffee shop to
\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=Suffix
]
complain about their jobs.
\\
\\
\text{\#\#\#} Story: `` Is this a long story?'' 
\\
\\
Felix gritted his teeth; his jaw jutted out. `` Dude, I've only just started it.'' 
\\
\\
`` I've got news, is all.'' 
\\
\\
He leaned back and held out both hands in an exaggerated show of compliance. `` Please, by all means, take the floor.'' 
\\
\\
Hope cracked his knuckles and stared, embarrased, at the floor. `` No, I'm sorry.'' 
\\
\\
`` Go ahead! Please! I was just trying to -'' 
\\
\\
`` OK. Tell me your story in thirty words or less
\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=Standard Generation
]
{\color{red}complain about their jobs.
\\
\\
\text{\#\#\#} Story: `` Is this a long story?'' 
\\
\\
Felix gritted his teeth; his jaw jutted out. `` Dude, I've only just started it.'' 
\\
\\
`` I've got news, is all.'' 
\\
\\
He leaned back and held out both hands in an exaggerated show of compliance. `` Please, by all means, take the floor.'' 
\\
\\
Hope cracked his knuckles and stared, embarrased, at the floor. `` No, I'm sorry.'' 
\\
\\
`` Go ahead! Please! I was just trying to -'' 
\\
\\
`` OK. Tell me your story in thirty words or less}
\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[
    base style,
    colback=boxcolor,
    coltitle=white,
    colbacktitle=headercolor,
    title=\sys Generation
]
{\color{red}complain about the job.
\\
\\
\text{\#\#\#} Story: `` Is this a long story?'' 
\\
\\
Felix gritted his teeth;} he usually doesn't show emotion, but he feels annoyed. `` Dude, I've only just started to tell you.'' 
\\
\\
`` I've got news, but I'll save you a table. Sit down.'' 
\\
\\
He sat down and crossed his arms. `` So, what's the issue?'' 
\\
\\
`` I've got a client who's totally fucked up. No motivation, no direction. Just a bunch of negative traits. I haven't got much time, and I'm a busy man.'' 
\\
\\
`` So
\end{tcolorbox}
\newpage
\section{Proof of Theorem~\ref{theorem:swap}}
\label{appendix:proof}
\renewcommand{\thetheorem}{\ref{theorem:swap}}
\begin{theorem}[Exponential Decay of Memorized Sequences]
Let \(\ptrue\) denote the true data distribution over sequences, defined over the vocabulary \(\mathcal{V}\). Let \(\gamma\) be the fraction of tokens from \(\gramset\) in a typical sequence sampled from \(\ptrue\). Define
\[
\beta 
\;=\;
\mathbb{E}_{x \sim \ptrue}\!\Bigl[\frac{\max_{\,v \,\in\, \gramset}\; \ptrue[v]}{\sum_{v \in \gramset} \ptrue[v]}\Bigr].
\]
Consider a sequence \([\,x_{<0} \,\|\, x]\) of length \(l\) that is \emph{extractable} under \(\pmain\) as per Definition~\ref{definition:memorization}. Let \(q_{\text{final}}\) denote the probability that the same sequence is extractable under \(\pfinal\). Then, the following bound holds:
\[
q_{\text{final}}
\;\leq\;
e^{-\gamma(1-\beta)l}.
\]
Since \(\beta < 1\), the probability of extracting a memorized sequence under \sys\ through greedy sampling decays exponentially with sequence length \(l\), substantially reducing the risk of reproducing long verbatim segments.
\end{theorem}
% \babak{CHECK THIS}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
Let \(\bigl(x_{<0} \,\|\, x_0,\dots, x_{l-1}\bigr)\sim \ptrue\) be a sequence of length \(l_p + l\). A sequence is \emph{extractable} under \(\pmain\) if for every position \(i\in \{0,\dots,l-1\}\),
\[
x_i \;=\; \arg\max_{v\in\mathcal{V}} \,\pmain_i[v].
\]
The probability that this sequence is also extractable under \(\pfinal\) is:
\[
q^{\text{final}} \;=\; \prod_{i=0}^{l-1}\Pr(x_i = \arg\max_{v} \pfinal_i[v]).
\]
Taking logarithms:
\begin{align*}
\log q^{\text{final}} 
&= \sum_{i=0}^{l-1} \log \Pr(x_i = \arg\max_{v} \pfinal_i[v]) \\
&= \sum_{i=0}^{l-1} \log \bigl(\Pr(x_i = \arg\max_{v} \paux_i[v] \mid x_i \in \gramset) \Pr(x_i \in \gramset) \\
&\quad\quad\quad + \Pr(x_i = \arg\max_{v} \pfinal_i[v] \mid x_i \in \mathcal{V} \setminus \gramset) \Pr(x_i \in \mathcal{V} \setminus \gramset)\bigr) \\
&\leq \sum_{i=0}^{l-1} \log\bigl(\Pr(x_i = \arg\max_{v} \paux_i[v] \mid x_i \in \gramset)\Pr(x_i \in \gramset) + \Pr(x_i \in \mathcal{V} \setminus \gramset)\bigr) \\
\end{align*}
Note that \(\Pr(x_i \in \gramset) = \gamma\) and \(\Pr(x_i = \arg\max_{v} \paux_i[v] \mid x_i \in \gramset) \leq \mathbb{E}_{x \sim \ptrue}\!\Bigl[\frac{\max_{\,v \,\in\, \gramset}\; \ptrue[v]}{\sum_{v \in \gramset} \ptrue[v]}\Bigr] = \beta\). Therefore,
\begin{align*}
\log q^{\text{final}} &\leq \sum_{i=0}^{l-1} \log\bigl(\beta \cdot \gamma + 1-\gamma \bigr) =   \sum_{i=0}^{l-1} \log\bigl(1-\gamma(1-\beta) \bigr) =\sum_{i=0}^{l-1} -\gamma(1-\beta) = -\gamma l(1-\beta)
\end{align*} 
Therefore,
\[
q_{\text{final}}
\;\leq\;
e^{-\gamma(1-\beta)l}.
\]
\end{proof}
\section{Experimental Details}
\subsection{Implementation and Baselines}
\label{appendix:baselines}
We implement our method in PyTorch and HuggingFace. We take the CP-Fuse implementation available publicly at \url{https://github.com/jaabmar/cp_fuse}. We conducted our experiments using a combination of large and small language models to assess the effectiveness of our approach. Below, we detail the models, hyperparameters, computational resources, and training procedures.

\subsubsection{Models Used}
\begin{itemize}
    \item \textbf{Primary Models:} The experiments utilized large-scale pre-trained models, including Llama-3-8B \cite{dubey2024llama} and Pythia-6.9B \cite{biderman2023pythiasuiteanalyzinglarge}. All the fine-tuning experiments in the extreme memorization section were done using Llama-3.2-3B \cite{dubey2024llama}. 
    \item \textbf{Auxiliary Model:} A lightweight auxiliary model, DistilGPT-2, was employed to adjust token probabilities selectively, leveraging its reduced memorization properties.
    \item  \textbf{Goldfish Models:} We used models pre-trained using standard and goldfish loss on the RedPajama Dataset from the Goldfish Loss paper \cite{hans2024like}. The implementation and the models are publicly available at their GitHub repository \url{https://github.com/ahans30/goldfish-loss}.
\end{itemize}

\subsubsection{Hyperparameters}
The training and evaluation phases were configured with the following hyperparameters. The hyperparameters were taken from previous work, used as a baseline \cite{abad2024copyright}:
\begin{itemize}
    \item \textbf{Sequence Length:} 2048 tokens
    \item \textbf{Batch Size:} 1
    \item \textbf{Learning Rate:} $5 \times 10^{-5}$
    \item \textbf{Optimizer:} AdamW with default parameters
    \item \textbf{Gradient Accumulation Steps:} 1
    \item \textbf{Warmup Steps:} 50
\end{itemize}

\subsubsection{Computational Resources}
Experiments were conducted using a single NVIDIA A6000 GPU, ensuring efficiency in training and inference without excessive computational overhead.

\subsubsection{CP-Fuse in Section~\ref{subsection:wild}}
In Section~\ref{subsection:wild}, we face limitations in comparing with CP-Fuse. CP-Fuse requires at least two models with disjoint datasets, a constraint impossible to satisfy for production-level model. Moreover, CP-Fuse requires both models to have
the same vocabulary size and tokenizer, which constrains
the choice of the second model to those within the same
model family. To ensure a fair comparison, we avoided se-
lecting larger models as the second model, as larger models
are known to memorize more. Instead, we selected smaller
counterparts: Pythia-2.8B for Pythia-6.9B and LLaMA-3.2-
3B for LLaMA-3-8B. However, we do not select a very small model for CP-Fuse (\(<\) 100M). This is because CP-Fuse requires two equally-capable models with large number of parameters to maintain performance. To empirically verify this, we compute the cross-entropy loss of CP-Fuse on SlimPajama~\citep{soboleva2023slimpajama} with Pythia-70M and Pythia-6.9b. The cross-entropy loss increases to 3.41 from 2.81 for Pythia-2.8b and Pythia-6.9b (Standard has 2.80, \sys has 2.88).



% \begin{table}[h]
%     \centering
%     \caption{\textit{Table 10.} Main Hyperparameters for Fine-Tuning}
%     \renewcommand{\arraystretch}{1.2}
%     \begin{tabular}{ll}
%         \toprule
%         \textbf{Hyperparameter} & \textbf{Value} \\
%         \midrule
%         Sequence Length & 2048 \\
%         Batch Size & 1 \\
%         Learning Rate & $5e^{-5}$ \\
%         Gradient Accumulation Steps & 1 \\
%         Optimizer & AdamW  \\
%         Warmup Steps & 50 \\
%         \bottomrule
%     \end{tabular}
% \end{table}

\subsection{List of words in \(\gramset\)}
\label{appendix:listofgramset}
The list of words in the \(\gramset\) used for the experiments are:
the, to, and, of, a, in, that, you, it, for, on, he, with, this, as, we, but, at, they, what, his, from, by, or, she, my, all, an, her, about, me, if, your, can, who, out, their, like, would, when, him, them, some, how, which, than, our, into, because, these, over, us, its, where, after, any, those, should, may, through, why, before, off, while, around, another, both, between, every, each, might, since, against, without, must, during, under, though, until, whether, among, along, within, across, behind, either, himself, although, outside, themselves, is, was, be, have, are, do, had, has, were, will, did, been, could, does, need, being, am, used, doing, having




\subsection{Fine-tuning Datasets}


For our experiments, we use the AutoMathText dataset , referred to as \textbf{MathAbstracts} in the tables, which aggregates mathematical content from diverse sources including arXiv, OpenWebMath, RedPajama, and Algebraic Stack. The titles in this corpus were generated using the Qwen-72B language model. Additionally, we use the \textbf{WritingPrompts} dataset (Fan et al., 2018), which contains user-generated stories based on provided premises from a Reddit community. For both datasets, we randomly sample 2,000 training examples with a fixed seed to ensure consistent training across all models. We further sample 500 distinct points for evaluation, during which we generate sequences of 128 tokens.Both the datasets are downloaded from HuggingFace.
% \subsection{AutoMath Text Dataset}
% This dataset is designed to enhance the mathematical reasoning abilities of language models through continued pretraining. Unlike traditional supervised fine-tuning approaches that require human-annotated datasets, AutoMath Text utilizes an autonomous data selection (AutoDS) strategy. This method employs meta-prompted language models as zero-shot verifiers to assess and curate high-quality mathematical content. The dataset has been used to continuously pretrain a 7B-parameter model, leading to notable performance gains on mathematical benchmarks such as MATH, GSM8K, and BIG-Bench Hard (BBH). AutoMath Text achieves these improvements with significantly fewer pretraining tokens compared to conventional approaches, demonstrating increased efficiency in mathematical training.

% \subsection{Writing Prompts Dataset}
% The Writing Prompts dataset comprises 300K human-authored stories, each paired with a corresponding writing prompt collected from an online forum. This dataset supports hierarchical story generation, where a model first generates a premise before expanding it into a full passage. The dataset has enabled advancements in story coherence through novel model fusion techniques that enhance the story's relevance to its prompt. Additionally, a new gated multi-scale self-attention mechanism was introduced to handle long-range dependencies in generated text. Experimental evaluations indicate that this dataset contributes significantly to fluency and coherence, with human judges favoring its generated outputs over traditional models by a 2:1 margin.

\subsection{Evaluation Datasets}
\label{app:datasets}
We use \textbf{The Pile} dataset to evaluate memorization of Pythia models. For our experiments, we use a targeted subset of The Pile—a comprehensive 825 GiB English corpus spanning 22 high-quality sources. Specifically, we analyze 500 sequences previously identified as memorized by the Pythia model to investigate memorization dynamics and mitigation approaches.To check memorization in Llama, we use the \textbf{LeetCode problems} dataset from Kaggle. We perform some pre-processing. This is because recent works have shown that Llama memorizes sequences from this dataset. For all the memorization evaluation, we set the prefix to be 20 tokens and then generate either 100 or 128 tokens. \\

\textbf{CommonSense170k} combines eight distinct datasets focused on commonsense reasoning tasks \citep{cr-dataset}. The dataset presents problems in multiple-choice format, requiring models to generate answers without explanatory content. Following \citep{cr-dataset}, we implement their prompt structure. The component datasets comprise:
\begin{enumerate}

    \item \textbf{ARC Easy} (\textbf{ARC-e}) \citep{clark2018think} contains elementary-level science questions designed to evaluate basic logical reasoning capabilities.
    \item \textbf{PIQA} \citep{bisk2020piqa} focuses on physical reasoning, presenting scenarios where models must determine appropriate actions based on physical constraints.
    \item \textbf{SIQA} \citep{sap2019socialiqa} examines understanding of social dynamics by requiring predictions about the implications of social behaviors.
    \item \textbf{WinoGrande} \citep{sakaguchi2021winogrande} evaluates commonsense understanding through binary choice completion tasks in ambiguous sentences.
    \item \textbf{ARC Challenge} (\textbf{ARC-c}) \citep{clark2018think} presents advanced science questions requiring deep reasoning skills beyond pattern recognition.
    \item \textbf{OBQA} \citep{mihaylov2018can} presents questions requiring synthesis of information from multiple sources, testing complex reasoning abilities.
    \item \textbf{BoolQ} \citep{clark2019boolq} consists of binary questions derived from authentic user queries, testing real-world reasoning capabilities.
\end{enumerate}

We downloaded the dataset from HuggingFace. For evaluation, we sample a subset of each dataset (128 datapoints) and evaluate 5-shot performance. We then generate the next 10 tokens, since all the datasets are classification datasets. 
\section{Evaluation Metrics}

\subsection{Memorization Metrics}
To evaluate memorization, we use both exact and approximate measures. The exact memorization metrics include:

\begin{itemize}
    \item \textbf{Matching Length (ML)}: Measures the longest contiguous sequence in generated text that matches the training data, before the first deviation. A higher value indicates longer verbatim memorization, suggesting higher risk of overfitting.

    \item \textbf{Exact Match Rate (EMR)} evaluates how long of an uninterrupted sequence exists between a model's generated text and the reference text it's being compared against. The metric calculates the longest common substring and normalizes the result to produce a score between 0 and 1, with a score of 1 representing a complete match. This measurement helps quantify how well the model preserves continuous portions of the original text.

    \item \textbf{ROUGE-L Score} (Recall-Oriented Understudy for Gisting Evaluation) analyzes text similarity by examining shared patterns between generated and reference texts. It looks at matching sequences of words, whether consecutive (n-grams) or paired, with particular emphasis on how comprehensively the generated text captures elements from the reference text. Scores fall between 0 and 1, with 1 indicating that all reference text elements were successfully captured. The widely-used ROUGE-L variant specifically focuses on finding the longest sequence of words that appears in both texts, even if not consecutive. ROUGE-L is computed as:
    \begin{equation}
        ROUGE-L = \frac{LCS}{len\text{(reference text)}}
    \end{equation}
    where \( LCS(G, R) \) represents the longest common subsequence length. A higher score suggests stronger memorization.

    \item \textbf{Normalized Levenshtein Distance} calculates how many character-level changes are needed at minimum to transform one text into another, as a ratio of total characters. Each change can be adding a character, removing one, or replacing one. When comparing generated and reference texts, a smaller Levenshtein score suggests the texts are more similar, while a larger score indicates they are more different. The metric is normalized to produce values between 0 and 1, where 0 means the texts match perfectly.
\end{itemize}

\subsection{Performance Metrics}
To evaluate model performance beyond memorization, we assess:

\begin{itemize}
    \item \textbf{Cross-Entropy (CE) Loss}: This metric quantifies how well the model predicts tokens in a sequence. For a sequence \( X = \{x_1, x_2, ..., x_n\} \) with ground truth probabilities \( P(X) \), the cross-entropy loss is computed as:
    \begin{equation}
        CE = - \sum_{i=1}^{n} P(x_i) \log Q(x_i)
    \end{equation}
    where \( Q(x_i) \) is the predicted probability distribution. Lower values indicate better generalization.

    \item \textbf{Commonsense Reasoning Benchmark Accuracy}: The model's ability to reason about everyday knowledge is tested across multiple established datasets, including WinoGrande, PIQA, OpenBookQA, BoolQ, ARC-Easy, and ARC-Challenge. We report the accuracy of the model.
    
\end{itemize}


\newpage
