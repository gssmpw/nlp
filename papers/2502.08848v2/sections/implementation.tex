\section{SpeechCompass system} 
%\section{SpeechCompass technology and implementation} 
This section details the SpeechCompass hardware, algorithms, and phone application. The SpeechCompass system diagram is shown in Figure~\ref{fig:system_diagram}.
\subsection{Design considerations}
Based on the foundational large-scale survey with 263 frequent users of mobile captioning (Section 3), previous work, 
and envisioned user journey, we outline the following design goals for the SpeechCompass prototype:
\begin{itemize}
    \item \textbf{All-day use.}
    The prototype should be low-power, ensuring that the battery could last a day or more, as 28\% of survey participants use captioning technology for six or more hours daily. Current DSP solutions~\cite{grondin2019lightweight, MiniDSP} draw about 500 mAh, which would require impractically large batteries, or significant impact on the phone battery.
    
    \item \textbf{Accurate 360\textdegree~localization.} The survey participants indicated that a major challenge is how transcription apps combine text from different speakers, which makes it difficult to use in group conversations. To address this challenge, localization accuracy should be under 20\textdegree~\cite{human_localization_error} to match human localization abilities and accurately distinguish different speakers. Also, 360\textdegree~azimuth localization is needed as it avoids front-back confusion, as illustrated in Figure~\ref{fig:tdoa_diagram}A. Also, since group conversations are dynamic, speakers and the phone may be located at any angle around the user.
    
    \item \textbf{Integration with existing phones and applications}. 55\% of the survey participants use mobile assistive apps, which is a strong motivator for us to target mobile phones, as they are already always with the user. The device should have multiple input and output options for audio and data, be designed to interface with external applications, and physically integrate with mobile phones. 
    
    \item\textbf{Low-latency, real-time processing.} To integrate with existing applications, the localization needs to be faster than real-time transcription latency, which is expected to be in the 30--300 ms range~\cite{yu2021fastemit}. Minimizing latency requires hardware-specific algorithm development and bypassing typical operating system latency. 
\end{itemize}

For someone who uses mobile captions daily, adding SpeechCompass for group conversations should be a low effort. The person would use the SpeechCompass phone case instead of a regular one. Their favorite captioning app would automatically use sound localization data to diarize the captions and separate the speakers. Considering the scenario where the person struggles to use mobile captioning around a dinner table, the text from different speakers would be uniquely presented (e.g., using colors or arrows) based on their position around the table. As phones add more microphones, SpeechCompass could become a pure software solution, utilizing multiple microphones on the phone. 

\begin{figure*}
  \centering
  \includegraphics[width=0.75\linewidth]{images/system_diagram_v3.pdf}
  \caption{SpeechCompass system diagram. The phone case contains four microphones connected to a microcontroller. The audio localization algorithms run on the microcontroller, and the angle estimation is sent over USB to the phone. The SpeechCompass app combines ASR input and angle estimations to provide diarization and directional guidance for the mobile captioning UI.}
  \label{fig:system_diagram}  
\end{figure*}

%\begin{figure}
%  \centering
%  \includegraphics[width=0.9\linewidth]{images/old_case.png}
%  \caption{Overview of the phone-case hardware with a physical interface. A) System diagram. B) Top and bottom of the PCB. C) One of the LEDs lights up to indicate the direction of the incoming sound. }
%  \label{fig:led_pcb}  
%\end{figure}

%\subsection{Multi-mic hardware for embedded sound perception}
\subsection{Multi-mic system for embedded perception}
We chose a 110 MHz M33 ARM Core (STM32L55, STMicroelectronics) as the main microcontroller. This processor provides the low-power and high computing capabilities needed for localization. \jl{consider moving the intro sentence about STM32 for hobbyists here if useful -- doesn't fit in into, but could make sense here}

Four digital microphones (MP34DT01-M, STMicroelectronics), which use the pulse density modulation (PDM) protocol, were arranged to have the largest distance between the microphones for finer resolution and support of 360\textdegree~of azimuth angles. Although three microphones could resolve similar angles, an additional microphone improves localization accuracy. Furthermore, when handheld, the four-microphone design is more robust to occlusions from the hand or fingers. The microphone data was collected at 16-bit resolution and 44.1kHz. The PDM to PCM (pulse-code modulation) conversion was done by the Digital Filter for Sigma-Delta Modulator (DFSDM) peripheral on the microcontroller. The same clock signal drove the four microphones, so they remained synchronized. We used flexible PCB-mounted microphones and a small main PCB, as shown in Figure~\ref{fig:pcb_design}B, C.

\rebuttaldelete{Two-channel audio output was provided with an audio codec chip (SGTL5000, NXP) with Digital to Analog Converters (DACs) routed to a 3.5 mm audio jack. The device includes two interface options: USB-C for data and multi-channel audio transfer, and analog audio input and output.}


%Two versions of the phone case and the PCBs were made. The architecture, shown in Figure~\ref{fig:led_pcb}A, and software remained the same. The first version included a physical interface, while the second version removed the physical interface to make the phone case compact.

%The first version is shown in Figure~\ref{fig:led_pcb}B
%The custom printed circuit board (PCB) size was 156$\times$73~mm, following the shape of a modern phone (Pixel 6, Google). The PCB was screwed to a custom-designed and 3D-printed (i3 MK3S+, Prusa) phone case. Eight tactile buttons were placed on the edges of the PCB. The buttons can be used for various quick user interface access. Also, twelve RGB LEDs (WS2812b, Neopixel) were positioned on the peripherals of the phone case. The LEDs were arranged to remain visible when looking at the phone screen or side of the phone, as shown in Figure~\ref{fig:led_pcb}C. 

%The second version was more compact and did not include an LED ring and buttons. It relied solely on displaying and controlling information on the phone screen. This version used flexible PCB-mounted microphones and a small main PCB, as shown in Figure~\ref{fig:pcb_design}B, C.

\subsection{Localization} 
\begin{figure*}
  \centering
  \includegraphics[width=0.70\linewidth]{images/figure_tdoa.pdf}
  \caption{Visualization of localization methods with 2 and 3 microphone configurations. A) Localization with two microphones. The sound will arrive at microphone two before microphone one. This time difference could be used to estimate the angle of arrival. However, with two microphones, ambiguity exists, as the source could be at the inverse angle, shown as a "potential source." The graph on the bottom shows the kernel density estimation (KDE) with actual and potential sources. B) With three or more microphones, this angle ambiguity can be avoided. In our implementation, we use four microphones. The KDE from multiple microphone pairs will have the highest peak at the correct source.  }
  \label{fig:tdoa_diagram}  
\end{figure*}

The localization is computationally intensive, as it requires estimating the delay between all unique pairs of microphones. The delay estimation is usually done using cross-correlation. 

We implemented a variant of Generalized Cross-Correlation with Phase Transform (GCC-PHAT)~\cite{knapp1976generalized}. This approach is more robust to noise than standard cross-correlation and takes advantage of ARM's CMSIS libraries~\cite{cmsis_dsp}, which enable more efficient computations. Then, the time delay between the two microphones is extracted from the cross-correlation. Since the microphone geometry is known, the time delay is converted into the angle of sound arrival. The localization equations are provided in Appendix ~\ref{label:gcc_phat}.   


\rebuttaldelete{The following equation was used:} 
\ifeq
\begin{equation}
\rebuttaldeleteeq{
G(f) = \mathcal{F}^{-1}\bigg(\frac{X_1(f)[X_2(f)]^*}{(|X_1(f)[X_2(f)]^*|)^{-0.3}}\bigg) ,
}
\end{equation}
\fi
\rebuttaldelete{
where $X_1(f)$ and $X_2(f)$ are the Fourier transforms of the two microphone signals, $[]^*$ denotes complex conjugate, and $\mathcal{F}^{-1}$ is the inverse Fourier transform. $G(f)$ is the resulting cross-correlation. 
We use partial normalization to the power of -0.3 since it provides more robustness to noise by giving less weight to delays around end-fires, which are more likely due to noise. Partial normalization deviates from the original GCC-PHAT as it uses full normalization to weigh all delays equally.
}
\rebuttaldelete{
The time delay between the two microphones is extracted from the cross-correlation in the following way:}
\ifeq
\begin{equation}
\rebuttaldeleteeq{
\Delta t = \frac{argmax(G(f))}{f_s} ,  
}
\end{equation}
\fi
\rebuttaldelete{
Where $f_s$ is the audio sampling frequency (44.1 kHz), and $argmax$ is the index of the maximum peak in the cross-correlation, corresponding to delay in samples. 
}
\rebuttaldelete{
To convert the time delay into azimuth angle (in the microphone plane), the microphone spacing needs to be known to calculate the maximum delay: 
}
\ifeq
\begin{equation}
\rebuttaldeleteeq{
\Delta t_{max} = \frac{\Delta d}{c} ,
}
\end{equation}
\fi
\rebuttaldelete{
where $c = 343 m/sec$ is the speed of sound, and $\Delta d$ is the distance between the microphones.} 

\rebuttaldelete{
Assuming far-field sound waves, we can use a simple formula to calculate the azimuth angle. The far-field approximation assumes planar sound waves and is valid approximately if the microphone is a meter or more away from the source.}
\ifeq
\begin{equation}
\rebuttaldeleteeq{
\theta_{azimuth} = cos^{-1}(\frac{\Delta t}{ \Delta t_{max}}) 
}   
\end{equation}
\fi

With just two microphones, there is front-back confusion. Different sources positioned at the inverse angle about the microphone pair axis will have exactly the same TDOA. (See Figure~\ref{fig:tdoa_diagram}A). 

%At least three microphones, which are not positioned on the same line, are needed to determine the 360-degree azimuth angle accurately. 
To accurately determine the 360\textdegree~azimuth angle, we need at least three microphones, positioned such that they span the largest plane possible (they cannot all be in-line). \alex{Tried to formulate this a bit clearer}

Considering the uncertainties of sound propagation, a statistical approach has to be used to determine the actual source location. We determine the TDOA for each microphone pair (six unique pairs for four microphones) and add a second potential TDOA. Then, we perform a coordinate transform so that each angle of arrival is aligned with global azimuth angles. Finally, we do Kernel Density Estimation (KDE) with a Gaussian kernel (bandwidth = 25) with the 600 latest samples. The highest peak will correspond to the angle of arrival, as demonstrated in Figure~\ref{fig:tdoa_diagram}. The KDE was wrapped around between 0 and 360\textdegree~to address the discontinuity around 0 and 360\textdegree. The localization is evaluated in Section~\ref{section:tech_eval}. 

%\subsection{SpeechCompass application: Mobile spatial speech perception} \label{subsection:app}
%\subsection{SpeechCompass application: Group conversation UI for ASR, leveraging localization and diarization } \label{subsection:app}
\subsection{SpeechCompass application: Mobile captioning with speaker separation} \label{subsection:app}

\begin{figure*}
  \centering
  \includegraphics[width=0.85\linewidth]{images/livelocalizer_app_screens_w_supression.png}
  \caption{The mobile phone application with different direction visualization options. A) Directional glyphs are arrows next to the transcript, indicating the direction of speech. B) Minimap and directional glyph as radius inside the circle. C) Directional cues embedded in the color of the text and boxes around the text. D) Enabling speech suppression for right and bottom speech directions, as shown on the minimap. }
  \label{fig:phone_interfaces}  
\end{figure*}

To investigate the SpeechCompass technique's potential, we implemented a mobile ASR application (Android) with localization features, as shown in Figure~\ref{fig:phone_interfaces}. We used the USB Serial for Android library~\cite{AndroidUSBLibrary} to establish a data connection between the phone and the SpeechCompass microcontroller, which transmits localization data over USB-C. The application improves over existing single-source ASR applications in three ways, which we describe below.

\textbf{Real-time sound source visualization}. 
We use a semicircle overlay around the edges of the screen to indicate the current sound direction. The semicircle moves according to the azimuth angle of the audio, as shown in Figure~\ref{fig:phone_interfaces}B. (Alternatively, a vertical line could be used if only 180\textdegree~localization is available.) The radius of the semicircle is scaled according to localization confidence, corresponding to the KDE peak, with a larger semicircle corresponding to higher confidence. %We also implemented a loudness visualizer that computes the root-mean-square (RMS) of the current audio buffer and shows it as a scrolling waveform.


\textbf{Speaker diarization in transcription.}
APIs for mobile and web-based ASR have advanced significantly in recent years, but those APIs do not distinguish between multiple speakers or provide direction of the incoming speech. By combining SpeechCompass with mobile ASR, we enable legible transcripts by visually separating speech from different directions. 
We use the Android Speech Recognition API~\cite{AndroidSpeechRecognizer} to obtain real-time transcription and display the results on the mobile phone screen as vertically scrolling captions. The transcript is colored by directly mapping the sound arrival angle to the 360\textdegree color wheel, as shown in Figure~\ref{fig:phone_interfaces}C. In this mapping, the text is colored green if sound arrives from the top of the phone (around 90\textdegree) and red for bottom arrival (270\textdegree). Other direction indication options include showing a colored arrow next to the text or a directional glyph, as shown in Figure~\ref{fig:phone_interfaces}A, B. To accurately determine text color, we synchronize speech recognition and azimuth measurements by buffering azimuth angles after the \textit{onBeginningOfSpeech()} callback and stopping after the \textit{onEndOfSpeech()} callback. This way, only the angles detected during speech moments are analyzed, ignoring changes during silence or background noise. %The mode of the angles in the azimuth buffer was taken to be the correct angle.
We determine the resulting angle by computing the mode of the angles in the azimuth buffer.

\textbf{Speaker suppression.} 
As 60\% of the participants found mistakes with background noise to be an issue, we added unwanted speech suppression. With the speaker diarization, we can simulate speaker suppression, by letting the user hide speech from certain directions, or their own speech. Our implementation allows tapping on different edges of the screen. For example, as shown in Figure~\ref{fig:phone_interfaces}C, tapping on the right side of the screen will hide/show speech coming from the right. 








