\section{SpeechCompass Application: Mobile Spatial speech perception} \alex{Needs to be update for the new application, and potentially merged with the implementation section} \alex{needs to be update to remove beamforming info}

\begin{figure}
  \centering
  \includegraphics[width=0.7\linewidth]{images/livelocalizer_app_screens.jpg}
  \caption{The diagram of audio localization. A) Localization with two microphones. The sound will arrive at microphone two before microphone one. This time difference could be used to estimate the angle of arrival. However, with two microphones, there is always ambiguity, as the source could be at the inverse angle, shown as a "potential source." The graph on the bottom shows the kernel density estimation (KDE) with actual and potential sources. B) With three microphones, this angle ambiguity can be fixed. The KDE from multiple microphone pairs will have the highest peak at the correct source. }
  \Description{???}
  \label{fig:tdoa_diagram}  
\end{figure}

SpeechCompass could be used in two ways. \alex{Should we describe both prototypes?} First, by physically interacting with the phone case. LEDs on the phone case light up towards the direction of the sound source, as can be seen in Figure~\ref{fig:phone_case_led}A. Also, the buttons on the side of the phone case can be used to set the beamformer to focus in the direction of the pressed button. 
The hardware UI allows the device to both be used standalone without a phone, or as a phone case with auxilliary functionality.  


Second, SpeechCompass can be integrated with a mobile phone application. To showcase this, we implemented a mobile (Android) speech-to-text application with localization features. The phone receives localization data over USB-C, and beamformed audio via a USBC-to-audio adapter. We used a splitter/hub to support two USB-C connections on the phone. We used the USB Serial for Android library~\cite{AndroidUSBLibrary} to establish a data connection between the phone and the SpeechCompass microcontroller. The application improves over existing single-source speech-to-text applications in three ways.

\textbf{Sound source visualization}. 
We use a circle overlay around the edges of the screen to indicate sound direction. The circle moves according to the azimuth angle of the audio, as shown in Figure~\ref{fig:phone_case_led}B. Alternatively, a vertical line could be used if only 180-degree localization is available. We changed the size of the circle and line according to the localization confidence, corresponding to the peak in the KDE. A larger circle or line corresponds to higher confidence.  

\textbf{Speaker diarization in transcription}
We use the Android Speech Recognition API~\cite{AndroidSpeechRecognizer} to obtain real-time transcription, and display the results on the mobile phone screen as vertically scrolling captions. The transcript is colored by directly mapping the sound arrival angle to the 360\textdegree color wheel. In such mapping, the text is colored green if sound arrives from the top of the phone (around 90\textdegree), and red for bottom arrival (270\textdegree). To accurately determine text color, we synchronize speech recognition and azimuth measurements by buffering azimuth angles after the \textit{onBeginningOfSpeech()} callback, and stopping after the \textit{onEndOfSpeech()} callback. This way, only the angles detected during moments of speech (and not silence or background noise) are analyzed. %The mode of the angles in the azimuth buffer was taken to be the correct angle.
We determine the resulting angle by computing the mode of the angles in the azimuth buffer.

While machine learning approaches to single-source speaker diarization have been improving~\cite{park2022review}, our multi-mic approach has the advantage of lower computational cost and latency and thus is inherently suitable for real-time transcription on low-power, low-cost microcontrollers. It is also language agnostic and can work for sounds other than speech. Our approach is tied to the position of the phone and the speaker, which can be advantageous as diarization can be immediately reconfigured by moving the phone. The use of our phone-case approach would enable this functionality to work for any phone, not just those with multiple built-in microphones. However, this approach needs further characterization for scenarios with multiple \textit{simultaneous} speakers.

%\textbf{Beamforming to a selected source}. 
%The microphone array could be used to beamform an audio signal to increase the gain in a specific direction and reduce the gain in other directions. We connected the 3.5~mm audio jack output of LiveLocalizer to a mobile phone using a USBC adapter. The beamformer direction can be controlled by dragging a white circle on the edges of the touchscreen. The beamformer steering is recalculated as the white circle is moved to the new location. 

%\textbf{filtering speakers}. 

% Sharing content with different people
%https://docs.google.com/document/d/1okevQZCpvAQ8NzOKmiQZp5vUoV4m827FyZxOoVHwwz0/edit#heading=h.2kcbg1xxzdz9