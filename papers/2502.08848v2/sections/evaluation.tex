\section{Technical Evaluation}\label{section:tech_eval}
In this section, we report on our technical evaluations of the main features of SpeechCompass. First, we characterize the underlying localization technology. We evaluate localization using an off-the-shelf mobile phone (Pixel 6, Google) and 360\textdegree~localization with the SpeechCompass hardware. Then, we determine the compute time for reliable localization, as it is critical for real-time diarization and visualization. Finally, for the real-world applicability of SpeechCompass, we characterize diarization accuracy in a realistic scenario where we also measure power consumption.

\subsection{Localization experiments setup}
By its nature, localization can not be done precisely due to time resolution. For example, with 80 mm microphone spacing and 44.1 kHz audio sampling rate, the maximum delay is $\pm$10 samples (0.23 ms). This configuration provides a resolution of 9$^\circ$. However, the actual accuracy is more nuanced, as it is obtained from multiple microphone pairs with different spacing and depends on many factors, such as the sound's frequency, environment, and loudness.
To better characterize localization accuracy, we conducted a series of experiments with the phone and SpeechCompass hardware. In the setup, the device under test was mounted on a rotating platform. A servo motor (XM430-W350-T, Dynamixel) was used to rotate the device around its axis (azimuth angle) and was synchronized with the data collection. 
A stationary speaker (S120, Logitech) was mounted 1.5 meters away to provide a direct sound path. 
Using the servo mount, the device was rotated by 10 degrees from 0--350$^{\circ}$ to obtain azimuth angle error. At each position, data was collected for 15 seconds. 
The loudness was calibrated with a reference sound meter (VL6708, VLIKE).
The speech audio came from the audiobook ``Alice's Adventure in Wonderland''~\cite{wonderland}. Rain~\cite{rain} was used as a realistic environmental sound with white noise properties. Each experimental condition was measured three times, and we report the mean value. 

\subsection{Localization: Off-the-shelf smartphone}
Many smartphones today have two or three microphones, which has the potential to enable some localization. We investigate the feasibility and accuracy of localization with an unmodified, off-the-shelf phone (Pixel 6, Google). This phone has three microphones placed at the bottom, top and near the rear camera, as shown in Figure~\ref{fig:pixel_mics}. The top and bottom microphones are on the side edge of the phone, while the rear camera microphone is on the backside.

To characterize the opportunity and limitations, we ported our localization algorithm to Java Native Interface (JNI), so it could run on the Android platform. The two-mic-localization runs on the phone in real time. 


With only a 15~mm separation between microphones 1 and 2, the delay between those two microphones was too small for useful localization. However, the other two pairs --- \{mic 0, mic 2\} and \{mic 0, mic 1\} --- provided useful localization data. %The geometry of the phone mics, we could only get 180-degree localization because of from-to-back confusion since mic 2 and mic 1 are too close to each other. 
Given this geometry, only 180-degree azimuth localization is possible for this device. If the camera microphone (mic 1) had been placed on the left side of the phone (with a larger baseline distance from mic 2), 360-degree localization would have been feasible. 

The mean error of environmental sound was 23.0 degrees, and speech was 16.0 degrees.  In comparison, humans also have up to 20-degree azimuth error when localizing sound without visual cues~\cite{human_localization_error}.
This demonstrates that localization on the phone can still be useful in limited use cases, for example, if the user does not expect any sounds from behind. We also observed that with such a limited amount of data, a simple histogram performed better than the Gaussian distribution approach, as there are not enough statistics for a reliable distribution. 
\alex{Seems like this could also be used for "Samsung interview mode" in portrait with one other person, or for 2 speakers in front with phone in landscape?}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{images/pixel6_mics.png}
  \caption{Localization on a mobile phone. A) Microphone positioning and distance. B) Examples of the raw microphone data from three microphones. The top graph shows the end-fire, with the bottom microphone facing the sound source, thus providing maximum delay between the microphones. The delay between mic 0 and mic 2 and 1 can be easily seen. The bottom graph shows broadside, where the microphones are the same distance from the sound, and thus have the same delay.}
  \label{fig:pixel_mics}  
\end{figure*}

% \subsection{Localization: SpeechCompass 4-mic hardware} 
\subsection{Localization: SpeechCompass 4-mic system} 

We varied two variables; type of sound (speech and environmental), and loudness in sound pressure level (SPL). 
The loudness varied from 50--65 dB SPL in 5 dB increments, where 0 dB corresponds to the human hearing threshold. This range covered the span of loudness typically experienced in daily conditions~\cite{db_chart}, from a quiet office (50 dB) to a normal conversation (65 dB). The ambient loudness in the experiment's room was 45 dB, so experimenting at this loudness or below was not practical. 

%Lastly, we played a mix of speech and environmental sound with x dB difference on an ortogonal speaker. This demonstrated performance of speech detection with presense of noise. 

The experimental results show that sound could be localized reliably for both speech and environmental sound, with different loudness, and with 360$^{\circ}$ coverage. The overall error is reported in Figure~\ref{fig:error}A. The error was 11.1--22.1\textdegree~for normal conversational loudness (60--65 dB). In comparison, humans also have up to 20-degree errors when localizing sound~\cite{human_localization_error}. The error increased significantly as the sound got quieter. The GCC-PHAT picks up the loudest sound, thus making it difficult to estimate TDOA if the sound source is under or at the environmental noise level. 

\textit{Directional effect.}
The localization error has some dependence on the source angle, with an overall sinusoidal trend, as can be seen in Figure~\ref{fig:error}B. The lowest error was around 0, 90, 180, and 270\textdegree, whereas the highest error was observed at 45 and 135\textdegree. This sensitivity can be explained by the rectangular microphone geometry. With the current coordinate system, an angle such as 90\textdegree~results in 4 out of 6 microphone pairs in the end-fire and/or broadside in relation to the sound source. In such a configuration, the maximum and minimum delays are obtained, thus easier to measure. 

\textit{Type of sound.}
The error was lower for the environmental sound (11.3$^{\circ}$ at 65 dB) when compared to speech (18.3$^{\circ}$ at 65 dB). There are a few reasons why speech appears to be harder to localize reliably. First, speech has a very complex acoustic pattern in comparison to noise. There are many frequencies, sudden stops, and loudness variations~\cite{wilder1975articulatory}. This results in more reverberations and reflections, which can increase the error. Also, speech contains cyclic signals, which have ambiguity when subjected to cross-correlation, especially for high frequency, and speech contains signals up to 4 kHz. 

\textit{Effects of elevation angle.} In the above experiments, we test the azimuth angle error and keep the elevation angle at zero, so the audio source and microphone array are leveled. In practical use cases, the audio source might be located below or above the microphone array. For example, with the microphone array on the table, the speaker's head is usually above the array. To characterize the effect of elevation, we move the source elevation from -40 (source below mics) to +40\textdegree~(source above mics) in 20-degree increments, while keeping the azimuth angle at 90\textdegree. The distance is kept at 1.5 meters as in previous experiments and white noise is played at 65 dB. The results in Figure~\ref{fig:elevation_effects} show minor azimuth error (under 2 degrees) at -20 and +20\textdegree~elevation. The error becomes more pronounced (up to 9.2\textdegree) at -40 to +40\textdegree~elevation, as the source moves outside of the microphone 2-D plane. %The results show that calibration might be required to account for common use cases where speakers are usually above the microphone array. 
The 20\textdegree~elevation would result in a height of 0.55m, which would correspond to speakers sitting around a table with the phone 1.5 meter away. The 40\textdegree~elevation would result in a height of 1.26m, approximately corresponding to speakers standing around a table. The results therefore suggest that SpeechCompass could work well for seated conversation partners, but would need additional calibration to account for situations where speakers are further above the microphone array. \alex{Does this make sense? Please double check this reasoning!}

\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{images/elevation_error.pdf}
  \caption{Effect of source elevation angle on the azimuth angle accuracy.}
  \label{fig:elevation_effects}  
\end{figure}

% 0.55 m (22'') - 20 deg, 1.26 m (50'') - 40 deg, 1.73 m (68'')- 60 deg

\begin{figure*}
  \centering
  \includegraphics[width=0.7\linewidth]{images/LL_error_combined.pdf}
  \caption{A) SpeechCompass azimuth angle measurement errors for speech and noise sound at different loudness levels. A mean of 0--350\textdegree~azimuth measurements is reported. B) Azimuth localization error at different angles. Data from one noise and speech trail are shown. The multi-mic device was rotated around its axis from 0--350\textdegree~while the source was stationary. A 15-second window was used for each estimate. }
  \label{fig:error}  
\end{figure*}

% \begin{figure}
%   \centering
%   \includegraphics[width=0.6\linewidth]{images/latency_example.pdf}
% %  \caption{Example of angle error as the number of samples increases. The error becomes stable for fewer samples with noise signals in comparison to speech. }
%   \caption{Example of angle error as the number of samples increases. The error stabilizes faster (approx. 10 samples) with uniform noise in contrast to speech, which takes longer (approx. 25 samples). \alex{Please double-check the interpretation here -- this graph is perhaps also not the most representative since the text says a mean of 72.7 samples for speech audio? is there another plot that is closer to the mean values?}}\artem{Maybe removing this plot is better? Everyone always gets confused looking at it, so it doesn't show much }
%   \label{fig:latency_example}  
% \end{figure}

%\begin{table}
%\centering
%\caption{Measurement errors for speech and noise sound at different loudness levels. A mean of 0-350 degree azimuth measurements is reported. }
%\begin{tabular}{ll|cc}
%                                          & \multicolumn{1}{l}{} & \multicolumn{2}{l}{Error (degrees $\pm$SD)}                  \\ 
%\cline{3-4}
%                                          & \multicolumn{1}{l}{} & \multicolumn{1}{l}{noise} & \multicolumn{1}{l}{speech}  \\ 
%\cline{3-4}
%\multirow{4}{*}{\rotcell{Loundness (dB)}} & 50                   & 37.1 ($\pm$36.3)                    & 36.6 ($\pm$31.3)                         \\
%                                          & 55                   & 16.9 ($\pm$16.9)                       & 27.7 ($\pm$27.7)                         \\
%                                          & 60                   & 11.1 ($\pm$11.1)                 & 22.1 ($\pm$22.1)                         \\
%                                          & 65                   & 11.3 ($\pm$11.3)                       & 18.3 ($\pm$18.3)                        
%\end{tabular}
%\label{table:localization_error}
%\end{table}

\subsection{Compute time to estimate direction} 
It takes 2.9 ms to compute the delay between two microphones on the 110 MHz microcontroller. Therefore the time to collect one data frame for six unique microphone pairs is 17.4 ms. This provides a lower limit on time to estimate direction. 

However, in practice, the estimation time could be much higher due to the uncertain nature of sounds like speech, as multiple measurements are needed for estimation. 
Therefore, we experimented to understand how quickly the device can react to the onset of sound in making an accurate direction estimate.

In a similar setup as the localization accuracy experiment, we positioned the device 1.5~meters from the sound source. As in the localization accuracy experiment, we used speech and noise, and measured them at different angles. 
 We collected data for 10 seconds without sound, then turned on the sound and collected the data for 50 seconds at 44.1 kHz. This measurement was conducted at angles from 0--360 in 60\textdegree~increments. 

\subsubsection{Results} 
We evaluated the number of samples it took for the sound source angle to converge around the actual angle by examining the KDE maximum after each additional sample. % (See Figure~\ref{fig:latency_example}).  

With the noise audio, localization required a mean of 12.7 samples (max: 19, min: 7). This number of samples was typically collected within one frame. So the latency was just the computational latency of 17.4 ms.

The speech audio needed on average 5.7X more samples to stabilize, with a mean of 72.7 samples (max: 149, min: 24). The speech contains more sparse localization information, and most measurements (80\%) did not have measurable microphone delays. As a result, a mean of 15 frames is required; thus, the mean estimation time is about 263 ms. However, estimation time can range approximately from 70--500 ms, depending on the speech characteristics. 


\subsection{Power consumption} 
As the device is designed to be portable and powered by its own power source or the phone's battery, it is essential to understand its power consumption. The power consumption of the whole system was 28 mAh. The four microphones consumed 2.4~mA, while the microcontroller consumed 11.7~mA. The audio codec and other peripherals consumed the rest.
%\alex{was the power characterization running with the peripheral LEDs? or are these some status LEDs}\artem{I believe thats with the ring LEDs off in standby. }\alex{OK, let's remove it as that is confusing -- can be considered "other peripherals"}  
Therefore, the device could run speaker localization continuously for 18 hours with a 500 mAh battery. When powered by a smartphone, considering a representative smartphone's power consumption of 154 mA (3700 mAh battery for 24 hours), this represents an additional 18 percent of the power consumption. 

\subsection{Diarization accuracy with localization}
In this section, we evaluate diarization with the SpeechCompass hardware. A schematic view of the setup is shown in Figure~\ref{fig:dc_setup}A. 
We set up four speech sources at 0, 90, 180 and 270 degrees to mimic a turn-taking conversation with four talkers. The speech material is from Librispeech~\cite{panayotov2015librispeech}. A diffuse noise field is simulated using four loudspeakers simultaneously playing uncorrelated noise content. The noise types are babble noise (pub and cafeteria) as well as traffic noise. One synthesized conversation lasts an average of 40 seconds with each speaker speaking in turn (i.e., no overlap in the speech content). The noise level is set to create three signal-to-noise ratio (SNR) levels of 6, 12, and 18 dB respectively. A fourth scenario is generated in clean condition (i.e., no noise). For each scenario, 100 conversations are created, corresponding to about 1.2 hours of active speech.

\begin{figure*}
  \centering
  \includegraphics[width=0.6\linewidth]{images/diarization_experiment.png}
  \caption{Diarization experiment. A) Data collection setup for diarization testing. The SpeechCompass phone case is placed on a small table in the center of the room. Head and torso simulators are used to play back speech material. Four loudspeakers are placed at the corners of the room to simulate a diffuse noise field. B) Example of TDOA data overlayed with diarization from two speakers for a conversation snippet. The extracted speaker labels from TDOA precisely follow the conversation turns. For TDOA, a frame size of 512 (11.6 ms) with no overlap at 44.1 KHz was used. Then, a running histogram of 522 ms was applied to get speaker labels.}
  \label{fig:dc_setup}  
\end{figure*}

An example diarized segment overlaying TDOA values is shown in Figure~\ref{fig:dc_setup}B, indicating the TDOA follows the speaker but still has some noise. 

In addition to the SNR sweep, we compared two microphone configurations using respectively three and four microphones, resulting in a total of eight experiments. We computed the diarization error rate (DER) for each scenario using the PyAnnotate toolkit~\cite{Bredin2020}. %DER is defined as the duration of speaker confusion error, false positives, and missed detections, divided by the total sesion time.\alex{Seems helpful with a definition of DER? I pulled in and adapted this, please verify and correct :)} \mathieu{would update to "
The DER is computed by summing the durations of false alarms, missed detections, and speaker confusions, and then dividing it by the total ground truth speech duration. The DER for the four-microphone configuration performed consistently better than the three-microphone one, across all four SNR conditions, with a relative DER improvement varying from 23 to 35\% for an average of 32\% as can be seen in Table~\ref{fig:dc_setup}. A four-microphone configuration allows for more unique microphone pairs (6) than a three-microphone configuration (3). This provides extra TDOA information that can be used by the diarization algorithm to segment and tag the speech content between the four talkers more accurately.
%The improved diarization performance with four microphones is likely due to the extra information available to segment and tag the speech content given by six microphone pairs in the four-microphone configuration versus three pairs in the three-microphone configuration.

%While machine learning approaches to single-source speaker diarization have been improving~\cite{park2022review}, our multi-mic approach has the advantage of lower computational cost and latency, and thus is inherently suitable for real-time ASR applications on low-power, low-cost microcontrollers. It is also language agnostic and can work for sounds other than speech. Our approach is tied to the position of the phone and the speaker, which can be advantageous as diarization can be immediately reconfigured by moving the phone. The use of our phone-case approach would enable this functionality to work for any phone, not just those with multiple built-in microphones. However, this approach needs further characterization for scenarios with multiple \textit{simultaneous} speakers.


% \begin{table}[]
% \centering
% \caption{Diarization error rate (DER) for three and four-microphone configurations, across a sweep of SNR levels.\alex{I think caption should summarize the results/interpretation. Also, I wonder if this would look better as a chart for easier comparison}}
% \begin{tabular}{ll|cccc|}
% \cline{3-6}
% \multicolumn{2}{l|}{\multirow{2}{*}{}}                                                               & \multicolumn{4}{c|}{SNR (dB)}                                                                                                    \\ \cline{3-6} 
% \multicolumn{2}{l|}{}                                                                                & \multicolumn{1}{c|}{inf}            & \multicolumn{1}{c|}{6}              & \multicolumn{1}{c|}{12}             & 18             \\ \hline
% \multicolumn{1}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Number \\ of mics\end{tabular}}} & 3 & \multicolumn{1}{c|}{0.46 $\pm$0.10} & \multicolumn{1}{c|}{0.48 $\pm$0.11} & \multicolumn{1}{c|}{0.49 $\pm$0.09} & 0.48 $\pm$0.11 \\ \cline{2-6} 
% \multicolumn{1}{|l|}{}                                                                           & 4 & \multicolumn{1}{c|}{0.30 $\pm$0.06} & \multicolumn{1}{c|}{0.37 $\pm$0.06} & \multicolumn{1}{c|}{0.32 $\pm$0.07} & 0.32 $\pm$0.07 \\ \hline
% \end{tabular}
% \label{table:DER_table} 
% \end{table}


\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{images/der.pdf}
  \caption{Diarization error rate (DER) for three and four-microphone configurations, across a sweep of SNR levels.\alex{I think caption should summarize the results/interpretation. Also, I swapped this table to a chart for easier comparison -- makes sense?}}
  \label{fig:dc_setup}  
\end{figure}

% Imagine of the diarization setup. 

% Where I got the current track from? 
%https://www.listenagainenglish.com/exercise-and-sports.html

%Calculate diarization error rate (DER). 
%https://github.com/wq2012/SimpleDER

% Looks like we can run puannotate directly: 
%https://colab.sandbox.google.com/github/pyannote/pyannote-audio/blob/develop/tutorials/intro.ipynb





