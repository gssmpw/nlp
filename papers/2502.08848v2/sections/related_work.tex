\section{Related work}
This section first discusses previous works in relevant audio applications like diarization, real-time audio transcription, and visualization interfaces. Then, we discuss the relevant literature on audio multi-microphone processing.

\subsection{Real-time mobile speech recognition}
CART (Communication Access Realtime Translation) is a well-established method for providing real-time captions.  A trained professional, typically using specialized software and a stenography machine, transcribes speech into text as it is spoken. CART is frequently used in broadcasting scenarios such as lectures and presentations. Recent advances in machine learning enabled real-time automatic mobile speech captioning, such as Live Transcribe~\cite{LiveTranscribe} and Microsoft Translator~\cite{TranslatorMicrosoft}, and also on head-worn displays~\cite{wearable_subtitle}. One of the main motivating uses for audio-to-text translation was audio accessibility, and research has shown the benefits of real-time ASR for hearing accessibility in scenarios like classrooms~\cite{asr_deaf_classroom, tracked_asr_study}. Research has shown various improvements to real-time ASR, such as ways to display transcription confidence~\cite{perspective_on_imperfect_captions} and customizing fonts appearance to improve readability~\cite{preffered_captions_appearance}.
\jl{Add more information regarding how we build on this related work}

\subsection{Speaker separation and diarization: Transcripts that distinguish speakers} 
% \subsection{Speaker separation and diarization: Enabling transcripts that distinguish between different speakers} %\alex{Seems like we could use this additional subsection heading?}\artem{Right, that makes it more clear}

Studies suggest that there is still room for improvements in, e.g., accuracy and usability~\cite{asr_challanges_deaf}. We particularly observe that existing approaches have yet to leverage microphone arrays in mobile devices to augment ASR. We outline the existing techniques in Table~\ref{table:tech_comparison}.

\begin{table*}
  \centering
\caption{Comparison of speaker diarization and separation technologies. The comparison shows that only SpeechCompass can support diarization and visualize sound direction on mobile devices. \jl{Can you not do acoustic beamforming on mobile? Is that the only difference from SpeechCompass?}}
  \label{table:tech_comparison}  
  \includegraphics[width=0.9\linewidth]{images/comparison.pdf}
  
\end{table*}


\textit{Acoustic beamforming} relies on classical signal processing techniques such as beamforming and localization from multiple microphones~\cite{anguera2007acoustic} to separate and diarize speakers. The main challenge for speaker separation is estimating a beamformer for each speaker using localization and other cues. Recently, neural networks have been employed to successfully aid in beamforming~\cite{heymann2016neural, yang2024binaural}. However, acoustic beamforming has been mainly applied to meeting room scenarios with a static microphone array, while SpeechCompass uses a similar multimicrophone technology for localization, which is applicable to mobile ASR throughout a user's everyday life. 



\textit{Blind source separation.}
This approach separates the speakers using a single microphone without additional cues. The technique is challenging for classical signal processing, but various machine-learning techniques have been successful~\cite{erdogan2015phase, isik2016single}. Blind speaker separation is effective when done offline on the entire audio file, providing the model with access to both future and past content. This is, however, not possible in a real-time causal system, as in this paper, since the future is not accessible, and the model only processes a limited amount of past information. These constraints for real-time casual systems make blind separation inapplicable for real-time transcription of conversations.% more challenging and limits its performance. 

Recently, multi-microphone approaches have been combined with speaker separation and diarization. In~\cite{10446934} and~\cite{10508438} Taherian et al. tackle the challenge of speaker separation in multi-speaker scenarios, focusing on conversational or meeting environments. They leverage multi-channel audio and deep learning models to improve separation accuracy to enhance the performance of downstream speech applications like ASR. In one project~\cite{10446934}, the authors leverage an end-to-end diarization system to identify individual speaker activity and then use this information to guide a multi-talker separation model. In another approach~\cite{10508438} a multi-input multi-output (MIMO) complex spectral mapping model allows for robust speaker localization, and is used to reduce speaker splitting errors. The complexity of the processing and non-causal components make these solutions unsuitable for real-time processing.

\textit{Voice fingerprint audio embeddings.}
This approach extracts unique speaker embeddings from a single microphone~\cite{wang2018speaker,snyder2019speaker} and uses them for diarization. Principal Component Analysis (PCA) or other unsupervised methods are typically done on the embeddings. The speaker embedding approach has been the primary go-to for real-time diarization since it can run causally. Adding multi-microphone data to speaker embeddings has improved diarization accuracy~\cite{snyder2019speaker}. A key disadvantage of speaker embeddings is its reliance on implicit or explicit speaker enrollment, as the initial number of speakers is unknown. Requiring every conversation partner or nearby speaker to explicitly register through a voice sample is particularly impractical in dynamic mobile scenarios. Furthermore, there are privacy concerns as speaker embeddings can be considered biometric information, and asking people to enroll would be in conflict with the discreetness that is often desired for accessibility aids. 


\textit{Audio-visual.}
Another approach has been to process audio and video using a multimodal model to separate speakers~\cite{gebru2017audio,ephrat2018looking}. The camera feed can help infer the active speaker from facial and lip motion when correlated with audio. Researchers have proposed deep learning models for audiovisual speaker separation that operate in the time-frequency domain and use cross-attention for audiovisual fusion~\cite{10446297}. The audio-visual model, while outperforming an audio-only model, however does not leverage spatial information from multiple microphones. The non-causal nature of the separation model (bi-LSTM) makes it ill-suited for real-time applications. This approach works best for meeting rooms and post-processing of recordings, since for mobile ASR, users typically do not point a camera at their conversation partners. There is potential for such applications for head-worn cameras (e.g., in smart glasses), although such approaches would still be dependent on a suitable field of view, sufficient bandwidth, and computing to process video streams. Such approaches will also have power and thermal implications for embedded devices with limited battery size.

Several commercial off-the-shelf solutions exist for mobile speaker diarization. The Ava mobile application~\cite{ava}, for example, allows diarization by connecting each speaker's smartphone to a network. However, this solution requires every speaker to set up and use their phone, which adds setup overhead. Speaksee~\cite{speaksee} is another solution that utilizes clip-on microphones for each speaker, with each microphone exclusively picking up speech from its designated wearer. These microphones connect to a central hub that provides diarized transcripts. This solution, while effective, requires dedicated hardware, making it more appropriate for formal meeting room scenarios.  While our approach may be less accurate than these solutions, because we are not using a dedicated microphone for each speaker, it does not depend on instrumentation or preparation by conversation partners, which is a crucial advantage in real-life situations whether with friends or strangers. It may even be prohibitive in certain scenarios where conversations are very brief (e.g., watercooler conversation) or the person wants to be discreet with their hearing accessibility needs.





% \mathieu{There are a number of recent papers that were presented at ICASSP this year that could be relevant, like those 3 from DeLiang Wang et al. at Ohio State: https://ieeexplore.ieee.org/abstract/document/10446934, https://ieeexplore.ieee.org/abstract/document/10508438, https://ieeexplore.ieee.org/abstract/document/10446297}
% \alex{Thanks! I put the title + abstract in the comments, below, also added to bibtex -- looking forward to your integration of these and how they relate to the project, since you are the expert on speaker separation!}
% \artem{I integrated it in the above }

% Leveraging Sound Localization to Improve Continuous Speaker Separation
% https://ieeexplore.ieee.org/abstract/document/10446934

% Continuous speaker separation aims to separate overlapping speakers in real-world environments like meetings, but it often falls short in isolating speech segments of a single speaker. This leads to split signals that adversely affect downstream applications such as automatic speech recognition and speaker diarization. Existing solutions like speaker counting have limitations. This paper presents a novel multi-channel approach for continuous speaker separation based on multi-input multi-output (MIMO) complex spectral mapping. This MIMO approach enables robust speaker localization by preserving inter-channel phase relations. Speaker localization as a byproduct of the MIMO separation model is then used to identify single-talker frames and reduce speaker splitting. We demonstrate that this approach achieves superior frame-level sound localization. Systematic experiments on the LibriCSS dataset further show that the proposed approach outperforms other methods, advancing state-of-the-art speaker separation performance.

%paper 1 only
%In \cite{} Taherian et al. propose a method for continuous speaker separation that leverages sound localization information obtained from a multi-input multi-output (MIMO) complex spectral mapping model.  By utilizing the inter-channel phase relations preserved in the MIMO model, the method achieves accurate speaker localization, which is then used to reduce speaker splitting errors. However the complexity of the task that requires runnning a DNN based separation model prior to estimating the direction of each speaker makes it prohibitive for real-time applications on mobile device.


% Multi-Channel Conversational Speaker Separation via Neural Diarization
% https://ieeexplore.ieee.org/abstract/document/10508438

% When dealing with overlapped speech, the performance of automatic speech recognition (ASR) systems substantially degrades as they are designed for single-talker speech. To enhance ASR performance in conversational or meeting environments, continuous speaker separation (CSS) is commonly employed. However, CSS requires a short separation window to avoid many speakers inside the window and sequential grouping of discontinuous speech segments. To address these limitations, we introduce a new multi-channel framework called “speaker separation via neural diarization” (SSND) for meeting environments. Our approach utilizes an end-to-end diarization system to identify the speech activity of each individual speaker. By leveraging estimated speaker boundaries, we generate a sequence of embeddings, which in turn facilitate the assignment of speakers to the outputs of a multi-talker separation model. SSND addresses the permutation ambiguity issue of talker-independent speaker separation during the diarization phase through location-based training, rather than during the separation process. This unique approach allows multiple non-overlapped speakers to be assigned to the same output stream, making it possible to efficiently process long segments—a task impossible with CSS. Additionally, SSND is naturally suitable for speaker-attributed ASR. We evaluate our proposed diarization and separation methods on the open LibriCSS dataset, advancing state-of-the-art diarization and ASR results by a large margin.

% paper 2 only
% In cite{} the autors leverage neural diarization to identify individual speakers' speech activity and assign them to output streams, thereby addressing limitations of traditional continuous speaker separation methods. Their approach allows efficient processing of long segments suitable for speaker-attributed ASR, but relies on non-causal processing making it unsuitable for real-time processing. 

% \alex{artem to incorporate these 3 papers and link to bibtex}\artem{done}

% paper 1 and 2 together


% Audiovisual Speaker Separation with Full- and Sub-Band Modeling in the Time-Frequency Domain
% https://ieeexplore.ieee.org/abstract/document/10446297

% We introduce a new deep learning model for talker-independent audiovisual speaker separation in noisy conditions in the time-frequency domain. The inputs to the model include noisy multi-talker mixtures and the corresponding cropped face images. Our approach incorporates cross-attention audiovisual fusion, effectively merging audio and visual features and enabling seamless information interchange between auditory and visual modalities. These fused features drive a separator module, which separates the acoustic features of individual speakers. The separator module is based on the recently proposed TF-Gridnet, which comprises an intra-frame full-band component, a sub-band temporal module that captures frequency-specific temporal dependencies, and a cross-attention module dedicated to extracting long-term fused audiovisual features. To encourage the utilization of visual streams during training, we employ a Signal-to-Noise Ratio (SNR) scheduler. Experimental results demonstrate that the proposed model advances the state-of- the-art speaker separation performance in several audiovisual benchmark datasets.

% paper 3



%Broadly, there are three approaches to diarization. The approaches largely depend on the use cases. First, recent advances in deep learning have shown to be promising for diarization with d-vectors~\cite{wang2018speaker}. The advantage of such systems is that they can run on a system with a single microphone. However, they are computationally intensive, and their latency is high for real-time diarization use cases. They also might require speaker enrollment for accurate diarization.  
%Second, approaches that rely on beamforming and Time Difference of Arrival (TDOA) from multiple microphones~\cite{anguera2007acoustic} can have lower compute needs and latency than deep learning but require microphone arrays. 
%Third, approaches may rely on additional hardware, such as separate microphones that clip to each speaker~\cite{speaksee}, inherently separating speakers. Audio-visual systems, on the other hand, rely on visual cues to diarize~\cite{gebru2017audio}. However, the additional hardware could be cumbersome, especially outside dedicated spaces like meeting rooms.

\jl{summarize how the work is different from the others in this section, esp. given how expansive it is}

\subsection{Visualizing non-speech sounds for accessibility use cases}
Speech is not the only aspect of sound that has been transcribed. For example, sound event recognition and visual alerts are helpful in hearing accessibility~\cite{sound_awareness, HomeSound, sound_detector_app,dhruv_sound_events}. Expressive captions~\cite{expressive_captions}, for example, visually convey the speech's emotion by altering the text's rendering based on the detected emotion. Closest to this work, research has looked into displaying sound localization cues from a microphone array using head-worm displays, such as HoloLens~\cite{holo_sound} and Head-Mounted displays (HMD)~\cite{dhruv_google_glass_localization}. Another work integrates localization into a watch~\cite{kaneko2013light} and displays sound location with LEDs. These proof-of-concept research projects use off-the-shelf devices, which do not meet power consumption, ergonomics, or form factor requirements needed for all-day use in everyday lives. In contrast, we leverage mobile phones that users already carry and use daily. While we demonstrate a prototype in a phone case form factor, the embedded hardware could also be implemented in wearable devices, such as head-worn displays. 

Due to captioning's focus on speech, fine audio qualities and structures, such as rhythmicity, are lost. To communicate such qualities, researchers have proposed real-time visualizations such as audio spectrograms~\cite{greene1984recognition} or Stabilized Auditory Images (SAI)~\cite{lyon2017human}, a visualization grounded in models of the brain, or tactile cues~\cite{vhp}. However, these approaches not only require extensive user training but also do not convey sound direction or leverage spatial information. Our approach can, however, be combined with those existing methods to extend their expressiveness. 

\subsection{Sound localization strategies} 
%Sound localization technology and algorithms
Multiple research projects have investigated sound localization, which can be divided into active and passive localization. Active localization sends out a signal and listens to the response. For example, Cricket \cite{Cricket} showed an indoor localization system using ultrasound beacons and detectors around the room. Active localization does not apply to speech, which is the main target of this paper.
As used in this work, a passive localization system listens with an array of microphones. Example of passive localization includes gunshot localization~\cite{valenzise2007scream}, rendering of a sound source in virtual reality~\cite{vr_sound_localization}, robot navigation~\cite{liu2010continuous}, and people tracking~\cite{people_walking_audio_id}. The Time Difference of Arrival (TDOA) is a popular technique for localization, where measuring the arrival delay between synchronized microphone pairs, allows the use of simple geometry to estimate the angle of arrival. 

Multiple localization techniques have been proposed in the literature based on the popular and effective TDOA estimation method. Most are variations~\cite{lee2008maximum,do2007real} on  generalized cross-correlation~\cite{knapp1976generalized}, performed in the frequency domain. Other approaches have been demonstrated, such as localization with one microphone~\cite{saxena2009learning} by adding structures to the microphone that change the sound based on incoming angle, or using two thin wires that change resistance based on the direction of sound~\cite{Sound_loc_and_vis_device}. 

Recent high-end phones use two or three microphones, which allows limited localization on some phones~\cite{phone_localization, phone_localization2}. However, the microphone placement on these phones is typically optimized for phone calls and video capture, with placements that are less well suited for localization, as microphones on the front and back are blocked when the phone is held or placed on a table. 

Several development platforms offer localization. For example, MiniDSP~\cite{MiniDSP} makes eight and sixteen microphone devices for localization and beamforming. ODAS~\cite{grondin2019lightweight} is an open-source platform for microphone array localization. Those platforms are, however, not designed for portable, battery-powered devices, given that their digital signal processing (DSP) chips have high power consumption ($\sim$500 mA).
%3d visualization~\cite{wang2012evaluation}

Although mobile phone ASR has many users, previous work shows that diarization and sound localization still need to be addressed, especially to make it practical for conversations with multiple speakers. While machine learning has improved diarization, the approaches are still challenging to apply to mobile ASR due to their numerous limitations (See Table \ref{table:tech_comparison}). Traditional multi-microphone processing techniques provide promising strategies for mobile ASR, which are practical today as shown through our technical implementation and evaluations.

\jl{say more about how your work build on the prior work}

%\alex{Maybe add something concluding that wraps up related work and talks about our complementary approach}\artem{Good idea, added a paragraph}




