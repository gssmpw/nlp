\section{Introduction}
Recent advances have enabled real-time automatic speech recognition (ASR) in mobile and embedded hardware, supporting a range of conversational scenarios~\cite{LiveTranscribe, TranslatorMicrosoft}. Real-time captioning can enhance human communication in various ways, e.g., for real-time translation between languages, transcriptions during an interview, automatic subtitle generation, hearing accessibility~\cite{designing_asr_for_deaf, tracked_asr_study}, and note-taking. 
%%As well as provide a warning about a loud incoming sound. samuely@: I couldn't figure out how to work this back in.
However, there are still unsolved limitations with real-time speech-to-text --- specifically, the ability to distinguish multiple speakers (\textit{speaker diarization}) and tracking the direction of speech (\textit{localization}). 


\begin{figure*}
  \centering
  \includegraphics[width=0.9\linewidth]{images/Ui_case_v3.png}
  \caption{Overview of the SpeechCompass phone case prototype. A) A mobile phone application interface with a mounted multi-microphone phone case. B) Inside and outside view of the prototype with a flexible PCB microphone mount and a compact main printed circuit board (PCB). C) Pictures of the main PCB with a top and bottom view. }
  \label{fig:pcb_design}  
\end{figure*}

Consider the following scenario: 
\textit{Throughout the day, a person relies on a mobile phone with real-time captioning to understand speech.
However, at a dinner table with multiple people, the conversation is difficult to follow since the app cannot distinguish between speakers and concatenates all speech into a single paragraph.
Additionally, since the person needs to look at their phone regularly, they struggle with following the turn-taking across speakers and don't know where to look if there is a speaker change. Also, irrelevant nearby conversations get transcribed as well, which can cause confusion and privacy implications.  }

Many of the challenges in the scenario stem from the spatial complexities of audio, which are challenging to capture with a single-microphone ASR system. The benefits of multi-microphone topologies for localization have been demonstrated in numerous applications, such as public safety~\cite{valenzise2007scream}, virtual reality~\cite{vr_sound_localization}, robot navigation~\cite{liu2010continuous}, mobile computing~\cite{phone_localization, phone_localization2} and audio accessibility~\cite{localization_glasses,holo_sound}.
In this work, we leverage arrays of multiple microphones and apply techniques for microphone array signal processing to demonstrate how this technology could improve ASR performance and usability in such scenarios. Specifically, we identified opportunities for improvements in three areas:
\begin{enumerate}
    \item \textit{Speaker diarization}. The transcript can visually separate different speakers based on the direction of the speech. \item \textit{Localization}. For spatial sound visualization, the screen can display the direction of the incoming sound.
    \item \textit{Selective attention}. The interface can allow the user to select speech of interest and filter out self-speech.
\end{enumerate}



%Microphone array processing has traditionally been limited to high-end audio applications, and was too computationally intensive to meet the latency and power consumption requirements for mobile and wearable devices. Recent advances have, however, enabled multi-microphone algorithms to be used in low-power devices with low latency. 

In this work, we developed \textit{SpeechCompass}, a solution to add diarization and speech localization to mobile captioning. It includes three main parts. First, low-latency localization algorithms that can run on generic microcontrollers or mobile phones. Second, a compact 4-microphone phone case that allows 360-degree localization on a low-power microcontroller. \rebuttaldelete{We used the STM32 microcontroller, a popular professional choice but also available for hobbyists as it is used in many Arduino-compatible boards (e.g., Adafruit Feather Express)} Third, a mobile captioning app that shows how sound localization can be visualized in different ways and used to support multi-speaker conversations through diarized transcripts. We also run our algorithm on an off-the-shelf mobile phone with only two microphones to demonstrate that limited 180-degree localization is possible in the app without additional hardware.

While machine learning approaches to single-source speaker diarization have been improving~\cite{park2022review}, our multi-mic approach has the advantage of lower computational cost, latency, and privacy, and thus is inherently suitable for real-time ASR applications on low-power, low-cost microcontrollers. It is also language agnostic and can work for sounds other than speech. Our approach is tied to the position of the phone and the speaker, which can be advantageous as diarization can be immediately reconfigured by moving the phone. This paper shows how traditional microphone array processing can significantly benefit diarization and localization for mobile ASR. While diarization with microphone arrays has been well studied~\cite{anguera2007acoustic}, it has yet to be applied to mobile ASR and related interfaces.
%The use of our phone-case approach would enable this functionality to work for any phone, not just those with multiple built-in microphones. 
%However, this approach needs further characterization for scenarios with multiple \textit{simultaneous} speakers.
\jl{Tie this back better to UI, give CHI}

%While diarization has advanced significantly using machine learning~\cite{erdogan2015phase, isik2016single}, it is difficult to apply to mobile real-time diarization due to high compute and non-causal algorithms, as outlined in Table~\ref{table:tech_comparison}. Furthermore, the computational requirements for the models are large, so running privacy-preserving models on the device is challenging. This paper shows that diarization and mobile ASR can significantly benefit from traditional microphone array processing. Diarization with microphone arrays has been well studied~\cite{anguera2007acoustic} but has yet to be applied to mobile ASR and interactions.

\subsection{Contributions}
The contributions of this work are: 
\begin{itemize}
    \item \textbf{Real-time sound localization algorithms and optimized, embedded multi-microphone hardware}, implemented both as phone-case prototypes with a low-power microcontroller (<20 ms processing time) and on a mobile phone with constrained microphone hardware. The code and design files are available via Github.\footnote{\href{https://github.com/google/multi_mic_audio}{github.com/google/multi\_mic\_audio}}
    
     \item \textbf{Mobile captioning UIs for group conversations, enabled through speech localization and diarization}, and implemented as mobile speech-to-text applications with different visualization and interaction techniques.
        
    \item \textbf{Technical evaluation of localization and diarization using the optimized algorithms and hardware}. We characterize localization accuracy and estimation time under various signal-to-noise conditions and speaker configurations, and evaluate diarization accuracy.

    \item \textbf{Foundational large-scale survey (n=263) with frequent users of captioning technology}, showing that noise and speaker separation are important and frequent challenges with existing solutions.

    \item \textbf{Lab study (n=8) and large-scale survey (n=494) of mobile interfaces and visualizations with frequent users of captioning technology}. Informed by a large-scale survey, we designed a lab study with eight frequent users of captioning technology. We contribute insights into interface preferences, customization, and benefits since diarization and localization have not been previously studied for mobile ASR.
    
\end{itemize}

