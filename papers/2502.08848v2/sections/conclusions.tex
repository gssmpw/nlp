\section{Conclusion}
Motivated by a foundational large-scale survey with 263 users frequent users of captioning technology, this work demonstrates how microphone array signal processing can supplement ASR by measuring and visualizing the spatial dimension of audio. Although ASR technology has greatly improved, diarization and localization features are not commonly available. 
To investigate the potential of enabling mobile captioning for group conversations, we implemented a low-latency 360\textdegree~localization algorithm that can run on general-purpose low-power microcontrollers, and a custom sound perception hardware solution with four microphones. 
%, in addition  and demonstrated that our localization algorithm can run on a mobile phone's existing built-in microphones, although limited in performance by microphone topology. 

Our technical evaluation of localization and diarization with the SpeechCompass microphone array demonstrates benefits over the traditional single-microphone configuration and pure machine-learning-based approaches. 

We introduced a mobile captioning app that uses sound localization to diarize and visualize speech directions for group conversations. 
%We enable communication of audio and localization data to the ASR application on the phone. 
The integration with our embedded software and hardware brought new capabilities to mobile ASR, including sound source location, speaker diarization, and user control of the diarization, allowing the suppression of specific speech directions. 
\jl{should probably mention the other large-scale study?}
Using the developed mobile phone application, we conducted an in-person study with frequent users of mobile ASR technology to gather feedback on the techniques and different visualization styles, and their potential for improving the captioning of group conversations. All the participants found the diarization, localization, and visualization features to be useful, and particularly appreciated the combination of a directional arrow and colored text. 

This work demonstrates that low-power microphone array processing can be integrated into new and existing mobile devices, thereby leveraging audio's natural spatial properties to enhance audio experiences and the understanding of speech. 

In the future, we hope that our approach will inspire the widespread adoption of advanced microphone arrays that natively unlock the potential of spatial sound processing and perception in mobile and wearable devices.