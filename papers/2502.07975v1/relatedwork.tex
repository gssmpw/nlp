\section{Related work}
\label{sec: related}

Learning in games has a long and complex history \cite{cesa2006prediction}. In this paper we focus on fictitious play (FP) and the replicator dynamic. The study of FP began with the work of \cite{brown1949some,robinson_iterative_1951}, who showed that in zero-sum games, the empirical distribution of strategies converges to the Nash equilibrium. Further results suggested that the long-run outcomes of FP would always be Nash equilibria; FP also converges to NEs in congestion games \cite{rosenthal_class_1973}, ordinal potential games \cite{berger_two_2007,monderer_potential_1996} and $2\times n$ games \cite{berger_fictitious_2005}. However, Shapley \cite{shapley_topics_1964} demonstrated that FP does not converge to NEs in general. Despite significant further work \cite{gaunersdorfer_fictitious_1995,hofbauer_time_2009,benaim_learning_2009,benaim_perturbations_2012,viossat_no-regret_2013}, the attractors of FP remain unknown in general. Recently, however, the behavior of FP was shown to have a close relationship to the preference graph, with many classical facts being a result of graph structure \cite{biggar_preference_2025}. We explore these ideas further in Section~\ref{sec: FP}.

The replicator dynamic arose from the work of Maynard Smith on evolutionary game theory \cite{smith1973logic}, being named and formalized in \cite{taylor_evolutionary_1978}. Since then, it has retained its central role in evolutionary game theory \cite{sandholm2010population,hofbauer_evolutionary_2003} as well as online learning, where it is the continuous-time analogue of the multiplicative weight method \cite{arora_multiplicative_2012}. Finding its attractors is a central goal of the study of the replicator, both in evolutionary game theory \cite{zeeman_population_1980,sandholm2010population} and more recently in learning \cite{papadimitriou2016nash,papadimitriou_nash_2018}. Since the work of Papadimitriou and Piliouras \cite{papadimitriou_game_2019}, a line of research has developed relating the replicator and the sink equilibria. \cite{omidshafiei_-rank_2019} used the sink equilibria as an approximation of the outcome of games for the purpose of ranking the strength of game-playing algorithms. Similarly, \cite{omidshafiei_navigating_2020} used the preference graph as a tool for representing the space of games for the purposes of learning. Later, Biggar and Shames wrote a series of papers on the preference graph and its relationship to the attractors of the replicator dynamic \cite{biggar_graph_2023,biggar_replicator_2023,biggar_attractor_2024}. Another recent work \cite{hakim2024swim} explored the problem of computing the limit distributions over sink equilibria, given some prior over strategies. Our work extends the frontier of this line of investigation.