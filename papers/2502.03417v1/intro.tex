LinkedIn is the world's largest professionals network with more than 1 billion members in more than 200 countries and territories worldwide. Hundreds of millions of LinkedIn members engage on a regular basis to find opportunities and connect with other professionals.

At LinkedIn, we strive to provide our members with valuable content that can help them build professional networks, learn new skills, and discover exciting job opportunities. To ensure this content is engaging and relevant, we aim to understand each member's specific preferences. This may include interests such as keeping up with the latest news and industry trends, participating in discussions by commenting or reacting, contributing to collaborative articles, sharing career updates, learning about new business opportunities, or applying for jobs.

In this paper, we introduce a set of innovative enhancements
to model architectures, all aimed at enhancing the member experience. 
The \textbf{contribution of the paper} consists of:
\begin{itemize}[leftmargin=*]
    \item We propose a modified transformer architecture that integrates learned gated normalization and set-wise attention mechanisms for both user history and ranked items. Additionally, we introduce a method for joint scoring of items in a set-wise fashion, which naturally enhances diversity in rankings.
    \item Industry practitioners often depend on extensive manual feature engineering to provide signals for models. We demonstrate that most manually crafted features and counter features can be deprecated. Using our proposed architecture, we achieve state-of-the-art performance with only 7 features, compared to the hundreds required by the baseline model.
    \item Our architecture demonstrates effective scaling laws for ranking and retrieval systems, achieving better performance with larger models, more training data, and longer context sequences. It also outperforms the HSTU\cite{HSTU_paper_zhai24a} and Wukong\cite{zhang2024wukongscalinglawlargescale} baselines (see \S\ref{sec:experiments} and \S\ref{wukong_layer}), establishing its superiority in ranking tasks.   
\end{itemize}

To support efficient, production-grade deployment of large ranking models, we discuss techniques that scale inference effectively through single-pass processing of user history and set-wise attention. In \S\ref{sec:experiments}, we share key insights from extensive ablation studies and A/B tests conducted on Feed Ranking. The techniques presented in this work have driven significant improvements, including a 0.27\% increase in Daily Active Users (DAU) who engage with professional content. We believe this work offers practical solutions and valuable insights for engineers looking to uplevel large-scale ranking and retrieval systems.

\vspace{-0.3em}