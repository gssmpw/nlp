\subsection{Member History Modeling}\label{sec:trans_act}
To model member interactions with platform content, we adopt an approach similar to \cite{xia2023transact, chen2019behavior}. We create historical interaction sequences for each member, with item embeddings learned during optimization or via a separate model, like \cite{pancha2022pinnerformer}. These item embeddings are concatenated with action embeddings and the embedding of the item currently being scored (early fusion). A Transformer-Encoder \cite{vaswani2017attention} processes this sequence, and the max-pooling token is used as a feature in the ranking model. To enhance information, we also consider the last five sequence steps, flatten and concatenate them as additional input features for the ranking model.
From an ablation study we found that the optimal learning rate for the model with TransAct was similar to the model without TransAct. For the number of encoder layers, going from zero (just pooling) to one layer provides the largest gains, one to two layers smaller gains, and no additional gains beyond three layers. When changing the feedforward dimension as multiples of the embedding size we observe slight additional gains by going from 1/2x to 1x, 2x, and 4x. Similar trends were observed for increasing the sequence length from 25, to 50, to 100. To optimize for latency we used two encoder layers, feedforward dimension as 1/2x the embedding dimension, and sequence length 50. In ablation experiments in \S\ref{sec:feed_ablation} we refer to history modeling as TransAct.
