

\section{Lessons learnt}
Over the time of development of {\systemname} we learnt many lessons. Here we present couple of interesting examples.
\subsection{Diversity of topics in Recommendations}
%Remove diversity re-rankers - tell the story how we removed diversity rerankers \textcolor{red}{\textbf{Sudarshan \& Borja}}

Over the years, the LinkedIn feed ranking model has been a pure pointwise model and list level interactions are not taken into account by the model. This could however yield a subpar experience of the entire feed session as a whole for our members. Some examples include seeing too many updates from the same actors, back to back out of network content, back to back instances of posts that are surfaced because one of their connections liked an activity of their connection. 

Diversifying this experience through a rule based approach has helped provide a much better session experience as a whole for our members which could be seen from the metric impact. 
We did a simple ablation study of removing all the diversity rules and checked the member impact. As expected, we do see a drop in the DAU in LinkedIn (-0.18\%).

While the rule based diversity for organizing the session does help, we believe it is suboptimal and a model powered solution could yield a better experience for our members. The rules tend to assume that the same template would work for everyone. In our approach we believe that replacing this legacy solution with a model that could learn the required list level diversity attribute is a superior solution.  In fact a model based approach could help us quantify and keep diversity as an objective in the longer term. In this work, we show how setwise models could replace diversity rules with a superior member experience in LinkedIn.


\subsection{Our approach to develop {\systemname}}
%\textcolor{red}{\textbf{Lars - tell the story how we built GR}} to FILL IN

{\systemname} and the traditional DLRM approach are fundamentally different in data format, features, and model training. {\systemname} therefore required a full rebuilding of our training pipeline. The goals for this rebuilding were to use few features, to outperform the baseline, and to demonstrate scaling laws.

We started this work by verifying that our most important count features can be emulated by ID embedding features alone. Then we built a small {\systemname} model and added the top features from the DLRM model or corresponding ID feature counterparts until adding more features gave only small AUC improvements. While the resulting model had ten times less features than the baseline, we did find that with too few features the model can suffer from item cold start and is not able to beat the baseline. In order to reduce member cold start in less frequent members we also increased the user history time window, but retained item ID features only for a limited window.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/gr-progress-chart_new.png}
    \caption{GR AUC improvement over successive iterations.}
    \label{fig:gr-progress}
    \vspace{-1.0em}
\end{figure}

Having added enough features, we scaled up the model along the dimensions of sequence length, embedding dimension, and number of layers. During this phase the main challenges were to maintain training stability and manage GPU memory. To ensure stable training we used different learning rates for dense vs. sparse model parameters, dense gating of transformer layers, and  transformers with gating as shown in Figure \ref{fig:custom-transformer}.

Overall we found that aside from ensuring training stability, most architecture changes we experimented with had little impact on the prediction AUC. The majority of performance appeared to be driven by features and model scale.

