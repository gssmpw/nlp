\subsection{Generative Ranking with Setwise attention and Learnt normalization}\label{sec:Overview_architecture}
%\textcolor{red}{\textbf{Lars}} to Fill in

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{GR.png}
    \caption{{\systemname} architecture combining historical attention for feature aggregation and in-session attention.}
    \label{fig:model_arch}
\end{figure}


% GR architecture
{\systemname}, our proposed large-scale ranking framework is fully sequential similar to the generative recommender (GR) approach described in \cite{HSTU_paper_zhai24a}.
% model setup
% input
Specifically, the model input is a member's interaction history $X_0, \ldots, X_n$, where each interaction $X_i$ represents one token. An input token $X_i$ is itself a concatenation of multiple embedding features which represent entities such as the item, the creator of the item, or content embeddings.
% outputs
The model output is a sequence $y_0, \ldots, y_n$ which is the sequence of actions corresponding to the inputs. For example, $y_i$ would be a multi-hot vector representing all the actions that were taken on item $i$ by the member. Here, actions can be click, like, comment, share etc.
% interleaving
Using \cite{HSTU_paper_zhai24a}'s action interleaving scheme, we interleave inputs with their corresponding outputs (see Figure~\ref{fig:model_arch}). This allows the model to combine historical item features with member actions to learn rich internal feature representations. Causal attention prevents position $i$ to look ahead at its outputs. The model outputs corresponding to interleaved actions are masked.
% model architecture
The model consists of Transformer blocks \cite{vaswani2017attention} with pre-norm \cite{baevski2018adaptive,child2019generating,wang2019learning}. Each multi-head attention and feed-forward layer is gated with a linear projection and sigmoid activation: $h^{j+1} = h^j + F(h^j) \times \sigma(h^j W)$, where $F$ represents the multi-head attention and feed-forward layer as shown in Figure~\ref{fig:custom-transformer}. Proposed architecture has similarity to Highway transformer \cite{chai-etal-2020-highway}, but instead of applying gating on skip connections we apply it on output of multi-head attention and Feed Forward layer.

\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{GR_transformer.png}
    \caption{{\systemname} Transformer architecture using a gating skip-connection.}
    \label{fig:custom-transformer}
\end{figure}
%\vspace{-1em}


% GR + setwise
% architecture: historical attention and in-session attention based on session ID / timestamps feature
\paragraph {Setwise Ranking Layer:}
The LinkedIn Feed ranking model has historically relied on a pointwise approach with rule-based diversity re-rankers. These re-rankers enforce session-level policies, such as:  (1) a minimum gap of two items between out-of-network content, (2)
a minimum gap of two items between posts by the same actor.
These rules assume a one-size-fits-all solution, which can be limiting. To improve, we propose adding a setwise model layer 
to re-rank items within the session by leveraging user interactions throughout the session and utilizing session-level data instead of treating each item independently on a point-wise basis. These changes aim to enhance session experiences and increase user engagement with the LinkedIn Feed. 
%Transformer decoder layers with self-attention are used to learn how list-level interactions influence user preferences for diversity.
We further extend the {\systemname} model to allow for session-level information flow similar to the SetRank architecture proposed in \cite{SetRank_paper}. SetRank introduces self-attention between items in a session to facilitate list-wise ranking. Specifically, we augment the {\systemname} model with in-session attention blocks as shown in Figure~\ref{fig:model_arch}. Due to the fact, that historical sessions are of varying length, in-session attention requires an attention mask that varies depending on the session ID inputs. We achieve this efficiently using  FlexAttention \cite{flex_attention}.
%\footnote{\url{https://pytorch.org/blog/flexattention/}}.
% loss
Finally, we augment the binary cross-entropy training loss using the Attention Rank loss \cite{ai2018learning}. Again, aggregation in the Attention Rank loss happens on a session level in our case, determined by session IDs.