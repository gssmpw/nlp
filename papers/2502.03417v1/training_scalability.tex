\section{System Architecture}\label{sec:system_arch}
{\systemname} helps to significantly simplify our system. As we are able to reduce dependency on the counter features, and reduce number to less than ten features from hundreds. 
Serving of transformer model requires to store near-line member and item activity as shown on the Figure \ref{fig:system_arch}.
Within the system we utilize variety of item and member embeddings generated by GNN \cite{LiGNN_paper}, LLM \cite{PEv3_paper}, as well as ID embeddings stored and served within the model. Given the user request, the system retrieves the top candidates using the model-based retrieval system \cite{Linr_paper}, then the top K candidates are evaluated using the second layer {\systemname} scoring model. {\systemname} takes as input Items features and Member Context.


%\textcolor{red}{\textbf{Fedor}} intro to system arch TO FILL IN 
%\subsection{System design}
%\textcolor{red}{\textbf{Sudarshan / Borja}} TO FILL IN 


\subsubsection{{\systemname} training}
Training of the {\systemname} model is GPU memory intensive, requiring us to leverage several optimizations. Firstly, long user item histories interleaved with corresponding actions result in sequence lengths of up to 2048 for our largest model. We used Flash Attention and mixed precision to reduce memory consumption.

Secondly, {\systemname} leverages several ID embedding features as shown in Table \ref{tab:single_feature_evaluation}. It is important to scale up ID embedding dimensions alongside other hyper-parameters. In order to accommodate embedding tables larger than the memory of one GPU we use column-wise splitting of embedding tables and distribution across multiple devices. The latter allowed us to scale up embedding tables to arbitrarily large sizes.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/ARTAL_arch.png}
    \caption{{\systemname} system architecture.}
    \label{fig:system_arch}
\end{figure}
%\textcolor{red}{\textbf{Fedor}} TO FILL IN 

\subsubsection{Optimization of inference}\label{optimization_inference}
Naive inference can be expensive in the {\systemname} model due to the complexity of applying many transformer layers on a long user action history for many candidate items. \cite{HSTU_paper_zhai24a, efficient_transact_paper} provide ways to amortize the computation of the history across all candidate items. In our case, the historical attention implementation allows items to attend to other items only if they are from earlier sessions. For this reason we can easily infer all candidates at the same time, automatically amortizing the inference of the history across all candidates. 

%The pointwise model used multithreading for independent item scoring at inference, but introducing a setwise attention increased latency due to: %(1) additional inference calls, (2) 
%loss of multithreading since items needed to be scored together, and  latency scaling with the number of setwise-scored items. LinkedIn's ranking inference remains CPU-based, with GPU used for retrieval \cite{Linr_paper}. 

Feed metrics are sensitive to latency, requiring careful optimizations to stay within bounds: (1) Split Scoring: Setwise scoring was applied only to the top 10 posts after point-wise scoring, where we decompose point-wise and setwise model weights separately; (2) Score Combination: Setwise scores were added to pointwise scores for items 1–10, preserving the order for items 11–end; (3) Simplification: Rule-based diversity rerankers were disabled; (4) Unified Inference: Pointwise and setwise parts of the model were stitched with a rewriter, enabling single inference to score the entire model.
To scale online scoring, all items were passed using the batch size dimension, repackaged as a listwise dataset, and scored via the trained model. These optimizations reduced incremental latency cost to 10ms (p90), ensuring minimal impact on member experience.

%The pointwise model leveraged multithreading extensively to score items at inference time since scoring each item was independent. After training a new setwise model that operates on top of the existing pointwise model, the model latency goes up due to the following reasons: (1) Extra inference call, (2) Multi-threading was no longer possible since all items needed to be scored together, (3) Latency increase was proportional to the number of items that were scored set-wise.

%One point to note is that the LinkedIn inference for ranking currently is still done mostly on CPUs and GPU inference is used for retrieval \cite{Linr_paper}. Feed metrics are sensitive to latency and we need to be careful to not overshoot the latency bounds. In order to achieve it, the following optimizations were done:

%LinkedIn's ranking inference is mostly CPU-based, with GPU inference used for retrieval [5]. Feed metrics are sensitive to latency, requiring careful optimizations to stay within bounds: (1) Split Scoring: Setwise scoring is applied only to the top 10 organic updates after pointwise scoring; (2) Score Combination: Setwise scores are added to pointwise scores for items 1–10, ensuring their scores remain higher than items 11–end, preserving the pointwise order beyond the top 10; (3) Simplification: Existing rule-based diversity rerankers were disabled;
%(4) Unified Inference: Pointwise and setwise models were stitched with a rewriter, enabling a single-threaded inference call to score the entire model at once, reducing latency.

%\begin{enumerate}
 %   \item Instead of scoring all the items setwise, we chose to split point-wise and setwise models, and apply setwise scoring to the top 10 organic updates following after the pointwise scoring.
 %   \item The setwise model scores was added to the pointwise model scores from item 1-10 in order to ensure that the top 10 scores were always greater than item 11 - end and hence ordering of item 11 - end was preserved based on pointwise model score.
 %   \item Disabled existing rule based diversity rerankers.
 %   \item Instead of having 2 separate calls to the model cloud \cite{Linr_paper}, we chose to stitch the point-wise model with the set-wise model using a rewriter. During inference time, a single thread is now leveraged to score the entire model in one shot. 
%\end{enumerate}

%In industry, TF rankling is usually leveraged to train and score setwise models.

%In order to simplify our online scoring at scale, we chose to simplify the problem. The scoring engine simply passes all the items using the batch size dimension. We repackage this to a listwise dataset and then pass it through the trained model for inference. With the mentioned optimizations, we were able to bring down the latency increase to ~10ms (p90m measurement) which did not severely impact the member experience.
%However, in order to simplify our online scoring at scale, we chose to simplify the problem. The scoring engine simply passes all the items using the batch size dimension that is available for TF models. Internally in the TF model, we repackage this to a listwise dataset and then pass it through the trained model for inference. 

%With the mentioned optimizations, we were able to bring down the latency increase to ~10ms (p90m measurement) which did not severely impact the member experience.

%\subsubsection{Optimization of {\systemname} inference}\label{optimization_inference_GR}
%Naive inference can be expensive in the GR model due to the complexity of applying many transformer layers on a long user action history for many candidate items. \cite{HSTU_paper_zhai24a, efficient_transact_paper} provide ways to amortize the computation of the history across all candidate items. In our case, the historical attention implementation allows items to attend to other items only if they are from earlier sessions. For this reason we can easily infer all candidates at the same time, automatically amortizing the inference of the history across all candidates. 
