In this section, we introduce modeling approaches how we enabled ranking models on LinkedIn Feed Ranking. Within {\systemname}, a large-scale \textbf{Li}nkedIn \textbf{G}enerative \textbf{R}ecommender ranking framework developed we bring state-of-the-art transformer-based modeling architectures. We introduce a modified transformer architecture that incorporates learned gated normalization and simultaneous set-wise attention to user history and ranked items. This architecture enables several breakthrough achievements, including:
(1) the deprecation of most manually designed feature engineering, outperforming the prior state-of-the-art system using only
7 features (compared to hundreds in the baseline), (2) validation
of the scaling law for ranking systems, showing improved performance with larger models, more training data, and longer context
sequences, and (3) simultaneous joint scoring of items in a set-wise
manner, leading to automated improvements in diversity. To enable
efficient, production-grade serving of large ranking models, we
describe techniques to scale inference effectively using single-pass
processing of user history and set-wise attention. 

\input{FeedRankingArch.tex}
\input{ModelArchitecture.tex}

%\input{tuning_model_convergence.tex}

\input{semantic_ids.tex}

\input{calibration_architecture.tex}
