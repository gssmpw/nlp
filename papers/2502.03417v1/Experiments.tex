\section{Experiments} \label{sec:experiments}
We conduct offline ablation experiments across Feed Ranking and Video Recommendation surfaces, and report A/B test for deployment of Setwise part of the model. In Feed Ranking, we rely on AUC metrics, which have shown a correlation with production online A/B test results.
\subsection{Ranking}\label{ranking_experiments}

%\textcolor{red}{\textbf{Lars \& Gaurav}}: 

\textbf{Scaling law in {\systemname}}: We perform multiple experiments to observe scaling law in {\systemname}. Figure \ref{fig:ne-scaling-flops}, \ref{fig:ne-seq-length}, \ref{fig:scaling-long-dwell} and, \ref{fig:scaling-contributions} demonstrate how evaluation loss, long dwell AUC and contributions AUC scale with log of the training flops. The figures show that for every order of magnitude increase in training FLOPS, the evaluation Long Dwell AUC improves by approximately 0.015, demonstrating a consistent positive scaling effect. Figure \ref{fig:ne-seq-length} expresses these experiments in terms of number of dense parameters and sequence length. We scaled the model to include 5.4 billion sparse ID embedding parameters, utilizing 64-dimensional ID embeddings with a vocabulary size of 33 million, implemented using a collision-based embedding bag. The model also incorporates 10 million dense parameters across 16 layers of a standard Transformer architecture, enhanced with dense gating on residual connections. Historical sequences were scaled to include up to 1,024 feed interactions per member over a six-month period. Training was performed on 8 A100 GPUs using 110 million training sequences across January to August 2024, and evaluation was conducted on 6 hours of data immediately following the training period, covering approximately 4 million samples. Performance metrics for the largest scaled model are provided in Table \ref{tab:model_performance}. We find that {\systemname} leads to a 2.4\% increase in Long Dwell AUC and a 1.2\% increase in Contributions AUC. Each action is modeled as a separate task. The Long Dwell task is defined in \cite{dwell_linkedin}.

\textbf{Scaling law of {\systemname} vs HSTU \cite{HSTU_paper_zhai24a}}: We experimented by replacing {\systemname} transformer layers (Figure \ref{fig:custom-transformer}) with HSTU layer. Wherein, we observed performance drop when using HSTU layers. Figure \ref{fig:ne-scaling-flops}, \ref{fig:ne-seq-length}, \ref{fig:scaling-long-dwell} and \ref{fig:scaling-contributions} demonstrate how HSTU underperforms compared to {\systemname} across metrics such as normalized entropy, Long dwell AUC and Contributions AUC. Training FLOPS on the Figures are an approximate function of the number of dense parameters times the sequence length. For example, at the same compute ($10^{17}$ flops) the long dwell AUC drops from 76.03 for {\systemname} to 75.87 when using HSTU. Additionally, we were able to train larger models with the {\systemname} transformer layers. This is because we could use FlashAttention to optimize its memory consumption.

\textbf{Effect of sequence length, number of layers, and ID embedding dimension:}
\cite{HSTU_paper_zhai24a} noted that {\systemname} scaling laws depend on the scaling of sequence length, number of transformer layers, and ID embedding dimension in parallel. To demonstrate scaling in Figures \ref{fig:ne-scaling-flops}, \ref{fig:ne-seq-length}, \ref{fig:scaling-long-dwell}, \ref{fig:scaling-contributions} we followed a similar approach. However in this section, we discuss what happens when we scale each dimension individually. Figures \ref{fig:scaling-seq-length}, \ref{fig:scaling-layers}, \ref{fig:scaling-id-dim} show AUC and evaluation loss when scaling sequence length, number of transformer layers, or ID
embedding dimension while holding everything else constant. 

From these plots we make the following observations about scaling laws when model capacity is scaled in isolation along a specific dimension: ID embedding dimension, number of transformer layers, and sequence length. Increasing the sequence length consistently demonstrates clear scaling laws across all evaluation metrics, including Evaluation Loss, Long Dwell AUC, and Contributions AUC. This suggests that longer sequences effectively leverage the model's capacity for improved performance. Similar observation can be made from Figure \ref{fig:ne-seq-length}. In contrast, independently increasing the ID embedding dimension or the number of transformer layers does not consistently demonstrate scaling laws for Evaluation Loss. While some metrics, such as Long Dwell AUC, show an upward trend with scaling, others, like Contributions AUC, exhibit irregular or plateauing behavior. These observations suggest that to fully realize scaling laws across all metrics, it is necessary to scale multiple dimensions simultaneously rather than in isolation.

\textbf{{\systemname} Feature Ablation:}
We conducted feature ablation in two ways. First, we evaluated which individual feature achieved the highest AUC on its own (Table \ref{tab:single_feature_evaluation}). Second, we analyzed which feature performed best when combined with content embeddings (Table \ref{tab:gr_features_evaluation}). On LinkedIn Feed, the relationship between the viewer and the actor (the author of the post) is important. As expected, Actor IDs were the significant feature in both ablation studies. For efficiency, feature ablations were conducted on smaller model configurations, using 12 Transformer layers, a sequence length of 512, and an ID embedding dimension of 32. As a result, the metrics reported in Tables \ref{tab:single_feature_evaluation} and \ref{tab:gr_features_evaluation} are lower than those in Table \ref{tab:model_performance}.

\textbf{{\systemname} with In-session Setwise Attention:}
Table \ref{tab:attention_metrics_table} compares AUC between a {\systemname} model with different attention mechanisms. One model uses historical attention only. The other uses both historical attention and in-session setwise attention. The table shows that providing the model with in-session attention results in an additional 0.2\% Long Dwell AUC gain.

%* Figure Scaling Law -> GR vs HSTU vs Wukong (Fedor needs to get data from Charles about Wukong layer)
%* Borja Position related work Not using it (Residual connection) USE
%GR instead HSTU, pre-sorting Sampling and data prep

\setlength{\tabcolsep}{2pt}

\begin{table}[h]
\centering
\small
\begin{tabular}{l c c}
\hline
\textbf{Model} & \textbf{Long Dwell AUC} & \textbf{Contributions AUC} \\
\hline
Baseline & 0.755 & 0.903 \\
{\systemname} & 0.773 & 0.914 \\
\hline
Difference & +2.4\% & +1.2\%\\
\hline
\end{tabular}
\caption{Model Performance Comparison}
\label{tab:model_performance}
\vspace{-2.0em}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/Normalized_entropy_GR_vs_HSTU.png}
    \caption{Scaling of normalized evaluation entropy as a function of training FLOPS for {\systemname} and HSTU.\protect\footnotemark %Training FLOPS are an approximate function of the number of dense parameters times the sequence length.
    }
    \Description{A plot showing the normalized evaluation entropy as a function of training FLOPS for {\systemname} and HSTU models.}
    \label{fig:ne-scaling-flops}
    \centering
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/Long_dwell_new.png}
    %\caption{Scaling of Long Dwell AUC as a function of training FLOPS for {\systemname} and HSTU. 
    \caption{Scaling of Long Dwell AUC as a function of training FLOPS for {\systemname} and HSTU.\protect\footnotemark[\value{footnote}] }
     %\footnote{\label{note1}We don't show HSTU points beyond value of 18 as the open source code of HSTU went out of memory}. %Training FLOPS are an approximate function of the number of dense parameters times the sequence length.
    
    \label{fig:scaling-long-dwell}
    \centering
\end{figure}
\footnotetext{We don't show HSTU points beyond value of 18 as the open source code of HSTU went out of memory for larger model configurations. Training FLOPS are an approximate function of the number of dense parameters times the sequence length.}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/Contributions_new.png}
    \caption{Scaling of Contributions AUC as a function of training FLOPS for {\systemname} and HSTU.\protect\footnotemark[\value{footnote}]}
    %\protect\footnotemark[\ref{note1}] %Training FLOPS are an approximate function of the number of dense parameters times the sequence length.
    \label{fig:scaling-contributions}
    \centering
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/Normalized_Entropy_seq_length.png}
    \caption{Scaling of normalized evaluation entropy as a function of training FLOPS for {\systemname} and HSTU for different sequence lengths.
    %Training FLOPS are an approximate function of the number of dense parameters times the sequence length.
    }
    \Description{A plot showing the scaling of normalized evaluation entropy as a function of training FLOPS for {\systemname} and HSTU models across different sequence lengths. The x-axis represents training FLOPS, while the y-axis represents normalized evaluation entropy. %Different curves correspond to varying sequence lengths, illustrating their effect on the entropy scaling.
    }
    \label{fig:ne-seq-length}
    \centering
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/layers_scaling_both.png}
    \caption{Scaling number of transformer layers while keeping all other hyper parameters constant (id embedding dimension = 32, sequence length = 512).}
    \label{fig:scaling-layers}
    \centering
    \vspace{-1.0em}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/id_embedding_scaling_both.png}
    \caption{Scaling id embedding dimension while keeping all other hyper parameters constant (number of transformer layers = 12, sequence length = 512).}
    \label{fig:scaling-id-dim}
    \centering
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/sequence_length_scaling_both.png}
    \caption{Scaling sequence length while keeping all other hyper parameters constant (number of transformer layers = 16, id embedding dimension = 64).}
    \label{fig:scaling-seq-length}
    \centering
    \vspace{-1.0em}
\end{figure}

\begin{table}[h]
\centering
\small
\begin{tabular}{l c c}
\hline
\textbf{Single feature} & \textbf{Long Dwell} & \textbf{Contribution} \\
\hline
Post ID & 0.703 & 0.857 \\
Original Actor ID & 0.731 & 0.893 \\
Post Type (Video/Photo/..) & 0.706 & 0.883 \\
Update Age of Post & 0.680 & 0.854 \\
Activity ID of Shared Post & 0.695 & 0.856 \\
Post content embedding\cite{PEv3_paper}  & 0.703 & 0.878 \\
\hline
All features & 0.767 & 0.912 \\
Baseline & 0.755 & 0.904 \\
\hline
\end{tabular}
\caption{AUC of models trained with only one feature.}
\label{tab:single_feature_evaluation}
\vspace{-1.0em}
\end{table}

\begin{table}[h]
\centering
\small
\begin{tabular}{l c c}
\hline
\textbf{{\systemname} Features} & \textbf{Long Dwell} & \textbf{Contribution} \\
\hline
PE + Actor IDs & 0.755 & 0.911 \\
PE + Post Type (Video/Photo/..) & 0.741 & 0.897 \\
PE + Item IDs & 0.752 & 0.895 \\
\hline
All features & 0.767 & 0.912 \\
Baseline & 0.755 & 0.904 \\
\hline
\end{tabular}
\caption{Comparison of AUC of models trained with post content embeddings (PE) \cite{PEv3_paper} and one additional feature.}
\label{tab:gr_features_evaluation}
\vspace{-2.0em}
\end{table}

\begin{table}[h]
\centering
\small
\begin{tabular}{l c c c}
\hline
\textbf{Attention} & \textbf{Click} & \textbf{Long Dwell} & \textbf{Contribution} \\
\hline
%History & 0.807 & 0.7357 & 0.8968 \\
%History + in-session & 0.811 & 0.7372 & 0.8964 \\
Difference & +0.50\% & +0.20\% & -0.04\% \\
\hline
%Difference & 0.50\% & 0.20\% & -0.04\% \\
%\hline
\end{tabular}
\caption{Relative AUC improvement for history attention only compared to history and in-session attention.}
\label{tab:attention_metrics_table}
\end{table}


\textbf{{\systemname} with Semantic IDs:} In the experiments with Semantic IDs we wanted to investigate if Semantic IDs could be used to replace Post ID (database assigned IDs to posts). We train a Residual-Quantized Variational Autoencoder on post embeddings to generate semantic IDs (SIDs) for each post using three hierarchical codebooks, each with 1,000 dimensions. 

%For efficiency, we evaluated SIDs using a smaller generative recommender (GR) model, which incorporates seven previously described features, utilizes 12 Transformer layers, has a sequence length of 256, and employs an ID embedding dimension of 32.
For SIDs, we adopt a prefix encoding approach: (1) first-level SIDs: these are directly retrieved from the embedding bag as learned during training; (2) bi-gram representations: the first and second-level IDs are concatenated and mapped to their corresponding vectors in the embedding bag learned during training; (3) tri-gram representations: the concatenation of first, second, and third-level IDs is hashed into an embedding bag of size $10^{6}$. We use 32 dimensional embedding for each SID to learn representation for prefix encoding. We use concat pooling for unigram, bi-gram and tri-gram SID embeddings in the {\systemname} model. We observe the AUC lift by including SID embeddings for both long dwell and contribution objectives. We observe that Post ID feature could be safely removed and replaced by SID.




\begin{table}[ht]
\centering
\small
\label{tab:GR SID performance}
\begin{tabular}{c c c c}
\toprule
%\textbf{Setup} 
\textbf{Feature} & \textbf{Long Dwell} & \textbf{Contribution} \\ 
\midrule
All features (Table \ref{tab:single_feature_evaluation}) & - & - \\ 
Only SID & -6.5\% & -4.6\% \\
Remove Post ID, Add SID & +0.4\% & +0.11\% \\
Add SID & +0.26\% & +0.22\%  \\

%\textbf{baseline} & All features (Table \ref{tab:single_feature_evaluation}) & - & - \\ 
%\textbf{baseline} & All features (Table \ref{tab:single_feature_evaluation}) & 0.761 & 0.906 \\ 
\midrule
%\textbf{baseline2} & Remove Post ID & 0.763 & 0.906 \\ 
%\midrule

%\textbf{concat pooling} & Only SID & -6.5\% & -4.6\% \\ 
%\textbf{concat pooling} & Only SID & 0.712 & 0.864 \\ 
 %& Remove PostID, Add SID & 0.764 & 0.907 \\ 
 %& Remove PostID, Add SID & +0.4\% & +0.11\% \\ 
 %& Add SID & +0.26\% & +0.22\% \\ 
 %& Add SID & 0.763 & 0.908 \\ 
%\midrule
%\textbf{sum pooling} & Only SID & -7.0\% & -4.8\% \\ 
%\textbf{sum pooling} & Only SID & 0.707 & 0.862 \\ 
% & Remove Post ID, Add SID & +0.13\% & +0.11\% \\ 
% & Remove Post ID, Add SID & 0.764 & 0.907 \\ 
% & Add SID & -0.13\% & +0.11\% \\ 
%\bottomrule
\end{tabular}
\caption{Performance comparisons with SIDs prefix encoding.}
\vspace{-2.0em}
\end{table}


%\textcolor{red}{\textbf{Charles and Daqi - to fill in the details for Wukong}}

\subsection{{\systemname} for Retrieval}
We experimented with in house video retrieval. We used a time range of 2 weeks worth of data. %The candidate pool include 140918 unique video candidates. 
%The data was split by the last digit of member ids for training, validation and test.
The retrieval system incorporated {\systemname} into LinkedInâ€™s PyTorch GPU-based retrieval system, LiNR \cite{Linr_paper}, and tested two retrieval modes: cosine similarity distance and an MLP-based model. In the MLP model, member and video embeddings were concatenated, and a fully connected layer was applied to learn the interaction score. We explored settings where the transformer layers between the member tower and item tower are shared and if the ID of the items are added. We allocated 6 layers for item transformer layers, and additional 6 layers for Member transformer. If sharing layer is enabled, then the 6 item transformer layers will be shared both for item representation of candidates and for item representation within history of the member model. From the experiments we can see that sharing layers, using MLP model and {\systemname} as well as using video ID boosted the recall@400. The experiments results are an average of 3 duplicated runs for each setting.
\begin{table}[ht]
\centering
\small % This command makes the text of the table smaller
\begin{tabular}{l|c|c}
\hline
\textbf{Experiment set up} & Recall@400 & \makecell{Relative Lift \\ to Row Above}\\ \hline
\makecell{Baseline: No {\systemname}, not sharing layers, \\ cosine similarity  LiNR model } &  $0.0799$ & -- \\ \hline
\makecell{No {\systemname}, sharing layers, \\ cosine similarity LiNR model } &  $0.1197$ & $49.81\%$ \\ \hline
\makecell{No {\systemname}, sharing layers, \\ MLP LiNR model } &  $0.3905$ & $226.23\%$ \\ \hline
\makecell{With {\systemname}, sharing layers,\\  MLP LiNR model } &  $0.3997$ & $2.36\%$ \\ \hline
\makecell{With {\systemname}, sharing layers + Video ID, \\  MLP  LiNR model } &  $0.4435$ & $10.96\%$ \\ \hline
\end{tabular}
\caption{Retrieval {\systemname} measurements.}
\label{table:retrieval-output}
\vspace{-2.0em}
\end{table}

%\subsection{Setwise Ranking}
%\textcolor{red}{\textbf{Borja}}
%Position related work
%Not using it (Residual connection)
%USE GR instead
%HSTU, pre-sorting
%Sampling and data prep

%Setwise layer experiments were done using one month of data collected from user interactions on the LinkedIn feed. The dataset is split by dates, where the training set consists of three weeks of data, and the remaining week of data is used for the validaton/test set. We group the data by member and session IDs and keep the top K items according to the scores predicted by the pointwise model. Each record consists of a member ID, a session ID, member-level features, and item-level features for the top K items, grouped in a listwise manner.

%\textbf{Position Encoding}

%We compare the results of our offline experiments when using different position encoding methods in the setwise ranker  architecture. We consider concatenated learned position embedding and approaches that modify the attention mechanism to encode positional information - namely, ALiBi and relative attention bias (RAB). RAB performed the best in our scenario. %Finally, we experiment with replacing the transformer encoder layers with HSTU layers which include RAB.

%\begin{table}[h]
%\centering
%\small
%\begin{tabular}{l c c c}
%\hline
%\textbf{Experiment} & \textbf{PI AUC} & %\textbf{Precision@1} & \textbf{Contributions} \\
%\hline
%Baseline & 0.807 & 0.7357 & 0.8968 \\
%Concat Learned PE & 0.811 & 0.7372 & 0.8964 \\
%ALiBi & 0.811 & 0.7372 & 0.8964 \\
%RAB & 0.811 & 0.7372 & 0.8964 \\
%HSTU & 0.811 & 0.7372 & 0.8964 \\
%\hline
%Difference & 0.50\% & 0.20\% & -0.04\% \\
%\hline
%\end{tabular}
%\caption{Evaluation AUCs for history attention only compared to history and in-session attention.}
%\label{tab:attention_metrics}
%\vspace{-2.0em}
%\end{table}

\subsection{A/B tests}
%Online A/B for Setwise \textcolor{red}{\textbf{Sudarshan}}

%TODO: Add table of results (DUPI, Sessions and total dwell time on feed ?)

\begin{table}[h!]
\centering
\small
\begin{tabular}{lll} % Four columns, all left-aligned
\toprule
Model Description & DAU & Time Spent on Feed \\ % Column headers
\midrule
%Diversity rules Disabled & +0.19\%  & +0.58\% \\
%Alibi Diversity Rules Disabled & +0.27\%  & +0.28\% \\
Setwise with Diversity Rules Disabled & +0.27\%  & +0.28\% \\
\bottomrule
\end{tabular}
\caption{Online Ramp of Setwise part of {\systemname}}
\label{tab:example_table}
\vspace{-2.0em}
\end{table}
The ramp results for setwise part of the model show that incorporating a model to account for list-level interactions after pointwise ranking leads to a 0.27\% increase in the number of daily active users engaging with professional content on LinkedIn, as well as an overall increase in the time spent on the feed. To optimize latency, the setwise and pointwise components of the model are separated. In online experiments, the setwise layer is applied only to the top 10 posts during inference.

Furthermore, with this new modeling layer in place we plan to optimize for session level metrics next  (e.g. time to next session, number of impressions in session), which was not possible previously with a pure point-wise approach.

%TransAct optimization speed-up: \textcolor{red}{\textbf{Lars}}
