\subsection{{\systemname} with Semantic IDs}\label{sec:gr_semantic_IDs}
One of the challenges with ID-based models is the need to manage large vocabularies of sparse ID features. This leads to increased model sizes, making model serving more complex and requiring continuous updates to keep IDs fresh. For instance, new posts are created by LinkedIn members every second. Large models with trillions of parameters often demand sophisticated distributed serving infrastructure \cite{HSTU_paper_zhai24a}, where different parts of the model are hosted across multiple machines.
Recent research demonstrates that using semantic IDs \cite{singh2024bettergeneralizationsemanticids} can significantly reduce model size while maintaining the same level of performance. In \S\ref{ranking_experiments}, we show that this approach allows us to reduce the model size from 5.4B to 1.3B parameters without compromising model quality.
We investigate the use of Semantic IDs as additional features by leveraging the RQ-VAE model to generate discrete hierarchical IDs through unsupervised learning, as proposed in \cite{rajput2023recommendersystemsgenerativeretrieval} and \cite{singh2024bettergeneralizationsemanticids}. The residual-quantized variational auto-encoder (RQ-VAE) model receives as input a batch of a high-dimensional feature vectors $x$ and assign to each item a tuple of semantic IDs. As the first step the encoder network $f_\phi$ downscale the input embedding $x$ to a latent lower-dimensional representation $z = f_\phi(x)$
where $f_\phi$ is the encoder network parameterized by $\phi$ and $z \in \mathbb{R}^d$ is the latent representation.
%\begin{itemize}
% \item  $f_\phi$ is the encoder network parameterized by $\phi$
% \item  $z \in \mathbb{R}^d$ is the latent representation.
%\end{itemize}
The latent representation z is quantized using hierarchical residual quantization (RQ). This involves iteratively quantizing z using a series of codebooks:

\begin{itemize}
\item Step 1: Initial Quantization.
The first quantized code
$c_1$ is obtained by finding the nearest vector in the first codebook $\mathcal{C}_1$ to z, where $c_1 = \text{Quantize}(z, \mathcal{C}_1), \quad c_1 \in \mathcal{C}_1$.
The residual $r_1$ is a difference between $z$ and $c_1$: $r_1 = z - c_1$.
\item Step 2: Residual Quantization (Iterative).
The residual $r_1$ is quantized iteratively using subsequent codebooks
$\mathcal{C}_2, \mathcal{C}_3, \dots$ %, \mathcal{C}_N$
At each step i: $c_i = \text{Quantize}(r_{i-1}, \mathcal{C}_i), \quad c_i \in \mathcal{C}_i$,$r_i = r_{i-1} - c_i$,
where $c_i$ is the quantized code at the i-th step, $r_i$ is the residual after subtracting $c_i$.
%\begin{itemize}
%    \item $c_i$ is the quantized code at the i-th step,
%    \item $r_i$ is the residual after subtracting $c_i$.
%\end{itemize}

\item Final Quantized Representation.
The final quantized latent representation $z_q$ is the sum of all quantized codes:
$z_q = c_1 + c_2 + \dots + c_N = \sum_{i=1}^N c_i$
\end{itemize}
The decoder $g_\theta$ maps the quantized representation $z_q$ back to the original embedding space, reconstructing $\hat{x} = g_\theta(z_q)$,
where $g_\theta$ is the decoder network parameterized by $\theta$, $\hat{x}$ is the reconstructed embedding.
%\begin{itemize}
%    \item $g_\theta$ is the decoder network parameterized by $\theta$,
%    \item $\hat{x}$ is the reconstructed embedding.
%\end{itemize}

\textbf{Loss Functions}: the RQ-VAE is trained by minimizing the following losses: (1) Reconstruction Loss, which measures the difference between the input embedding x and the reconstructed embedding $\hat{x}$: 
$\mathcal{L}_{\text{reconstruction}} = ||x - \hat{x}||^2$;
(2) Quantization Loss, which penalizes the difference between the latent representation z and its quantized version $z_q$:
$\mathcal{L}_{\text{quantization}} = ||z - z_q||^2$.
The total loss is a weighted combination of the reconstruction and quantization losses:
$\mathcal{L} = \mathcal{L}_{\text{reconstruction}} + \beta \mathcal{L}_{\text{quantization}}$,
where $\beta$ is a hyperparameter controlling the importance of quantization losses. We use $\beta=0.25$.
%\begin{itemize}
%    \item $\beta$ is a hyperparameter controlling the importance of quantization losses. We use $\beta=0.25$.
%\end{itemize}

To stabilize training, we use vector reset approach suggested in  \cite{singh2024bettergeneralizationsemanticids, zeghidour2021soundstreamendtoendneuralaudio}. We track exponential moving averages of number of assignments for every codebook vectors. In case codebook vectors has  utilization below threshold we reset it by a randomly sampled content embedding element from the current batch. 
The best trained RQ-VAE model includes 3 codebooks, with 1,000 code vectors each. Dimension of latent vectors is 8. To force the RQ-VAE model to learn to ensure evenly distributed clusters we apply FLOPs Regularizer \cite{paria2020minimizingflopslearnefficient}. After training, we perform inference to assign each item a tuple of Semantic IDs based on content features, then respective Semantic IDs embeddings are learnt in embedding bag as part of the {\systemname} model.

%\subsection{Setwise ranking layer}\label{sec:setwise_offline}

%\textcolor{red}{\textbf{Borja}}  TO FILL IN


%The LinkedIn feed ranking model has historically been a pointwise model along with rule based diversity rerankers. The feed ranking model was discussed in detail at the baseline model section and we wont revisit it again. Following the ranking score computation, there are a set of diversity rules which enforce policies at the session level. Some example rules in place are,
%(1) min gap of 2 between out of network items,
%(2) min gap of 2 between the same actor on the LinkedIn Feed.

%These rules are arbitrary and assume a one size fits all solution. Instead, we seek to add a setwise model layer post the point-wise model which will aim to re-rank the session based on users interaction in the entire session. This model is trained with session level data instead of independent point-wise data.

%This model takes in the point-wise model's output scores along with embedding representation of the member and all posts in the session as features. To learn the user preference of diversity (or the lack of it), we leverage transformer decoder layers with self attention to learn how list level interactions impact the user experience. With this changes, we aim to provide a better session experience for LinkedIn members and to increase the total time spent on LinkedIn feed. 