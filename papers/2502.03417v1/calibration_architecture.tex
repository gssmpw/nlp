\subsection{Retrieval with {\systemname}}\label{sec:retrieval_gr}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/GR-retrieval-member-tower.pdf}
    \caption{{\systemname} member tower based on {\systemname}. Member profile features and member activity history are inputs to a transformer model, green blocks are used when {\systemname} is enabled.}
    \label{fig:gr-retrieval-member-tower}
\end{figure}
%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{figures/GR-retrieval-item-tower.pdf}
%    \caption{{\systemname} item tower in retrieval.}
%    \label{fig:gr-retrieval-item-tower}
%\end{figure}

We also conducted experiments on video retrieval tasks using {\systemname}. In the member tower (Figure \ref{fig:gr-retrieval-member-tower}), we embed the member's sparse features, concatenate them with dense features, and pass them through a feedforward network (FFN). The FFN output is treated as a virtual token for a transformer model. Additional tokens for the transformer include embeddings of activities from the member's history. The baseline does not include the "green blocks," which are activated only when generative recommendation (GR) is enabled. Under GR, a shared action head predicts the next action for an activity based on the member profile, prior activities, and their corresponding actions.

In the item tower, item features are treated as individual tokens, and a transformer model processes these tokens. The LiNR model \cite{Linr_paper} is then used to calculate the similarity between member embeddings and item embeddings. The baseline loss function is a softmax loss with one positive example and two mined negative examples per training data point. For {\systemname}, the loss combines the two-tower softmax loss with the action prediction loss, which is itself a softmax loss predicting the correct action among all possible actions for the activities.

%We also experimented with retrieval tasks with {\systemname}. 
%In the member tower \ref{fig:gr-retrieval-member-tower}, we first embed member sparse features and concat with dense features and then feed them to an FFN where the output is virtually treated as a token to a transformer model, the other tokens to the transformer model include the embeddings of the activities in the member's history. The baseline does not include the green blocks which are only enabled when generative recommendation is enabled where a shared action head is used to take predict the action of an activity given member profiles and all activities and actions prior to this action.
%In the item tower, item features are treated as individual tokens and a transformer model is applied on top.
%The LiNR model \cite{Linr_paper} is used to calculate the similarity between a member embedding and an item embedding. 
%The baseline loss function is a softmax loss with one positive example and two mined negative examples for each training data points. The loss with GR is the summation of the two tower softmax loss and the action prediction loss which itself is a softmax loss to predict the correct one action out of all possible actions on the activities.

