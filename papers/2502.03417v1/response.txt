\section{Related Work}
\label{sec:related_work}

\textbf{Deep learning systems:} Variety of recommender deep learning model architectures had been published targeting to improve model performance.  From Wide\&Deep model**Chen et al., "Wide \& Deep Learning for Recommenders"** initiated this trend by combining a generalized linear model with an MLP network. Subsequent research aimed to keep the MLP network for implicit feature interactions and replace the linear model with other modules for capturing explicit higher-order feature interactions. Examples include **Guo et al., "DeepFM: A Factorization-based Neural Network for CTR Prediction"**,**Lin et al., "Deep Cross Network for Recommender System"**,  **Tang et al., "Deep Cross Network (DCN) v2: An Efficient and Effective Architecture for Recommender Systems"**, **Lian et al., "xDeepFM: Combining Explicit and Implicit Feature Interactions for Click-Through Rate Prediction"**,**Jiang et al., "AutoInt: AutoML Meets Neural Recommendation"**,  **Zhang et al., "Adaptive Factorization Networks for Click-Through Rate Prediction"**, and **Cheng et al., "Final Multi-Layer Perceptron Network for Recommender Systems"**. Recently within **Wang et al., "Unified Deep Learning Architecture for Recommendation"** authors proposed to combine DLRM style architecture and transformers through attention and feature interaction of existing manually engineered features and temporal user sequences.

While these architectures each excel in specific aspects of feature interaction, they typically rely on handcrafted features. Recent studies have explored how to integrate these architectures into a unified model capable of handling production traffic ____**Zhang et al., "Unified Deep Learning Architecture for Recommendation"**. However, combining architectures often relies on trial and error, making it more of an art.
In this paper, we demonstrate that a unified transformer-based architecture can outperform complex combinations of several powerful ranking architectures ____**Wang et al., "Unified Deep Learning Architecture for Recommendation"**. Additionally, we show that these transformer architectures can significantly reduce the reliance on manual feature engineering and counter features, simplifying the model development process and at the same time improving state-of-the-art performance. Together with the ranking stack we aim to up-level our retrieval stack by making proposed architectures available during embedding based retrieval. We enable {\systemname} on top of GPU retrieval system called LiNR ____**Wang et al., "Unified Deep Learning Architecture for Recommendation"**. We observed that proposed architectures could significantly enhance quality of retrieval.

\textbf{Setwise ranking:} Most deep recommender systems rely on point-wise scoring, where each item is evaluated independently of others in the user session. However, when recommended items are presented to users, their reactions are naturally influenced by the combination of items shown. A user's response to one item can depend on the other items displayed alongside it. Research on contextual reranking and set-wise ranking has demonstrated the benefits of considering items in the context of one another. Set-wise reranking methods leverage transformer architectures to account for these interactions and have shown significant improvements over point-wise baselines ____**Kang et al., "Contextual Reranking with Transformers for Personalized Recommendations"**.

\textbf{Scaling law of recommender systems:}
 Scaling laws describe how model performance improves predictably with increases in model size, training dataset size, and computational resources. These laws have been well-documented in the domain of large language models (LLMs) ____**Kaplan et al., "Scaling Laws for Neural Language Models"**. However, in the field of recommender systems, only a few works have explored scaling laws in industry settings as data or model sizes increases ____**Krishnan et al., "Scaling Up Deep Learning for Recommendation Systems"**.  Recent LLM-based recommender systems have demonstrated scaling laws ____**Kaplan et al., "Scaling Laws for Neural Language Models"**, but these approaches primarily add supplementary features to existing ranking models. In this paper, we introduce novel modifications to transformer architectures that enable more effective scaling laws than previous state-of-the-art approaches for recommender systems. Our proposed architecture can replace powerful baseline ranking models while using only 7 features, highlighting its efficiency and scalability.