%\vspace{-2.7em}



\appendix
%\section{INFORMATION FOR REPRODUCIBILITY}\label{sec:reproducability}

\section{APPENDIX}

\subsection{Reproducibility notes}
Training stability emerged as a critical factor in optimizing our model architecture, and we implemented several techniques to ensure it. These included using a transformer architecture with gating, where layer normalization is applied before multi-head attention (MHA) or MLP layers, and gating applied on top of MHA/MLP outputs with a sigmoid(XW) layer. Additionally, we employed separate learning rates for embeddings (0.01) and dense layers (0.001) to further enhance stability. Architectural changes, while having limited direct impact on performance, were crucial for maintaining training stability, enabling effective scaling of model and input size — both of which proved more impactful for performance improvements.

Our exploration of features revealed that models with even a single feature could achieve performance near baseline production model levels. However, surpassing baseline performance required the inclusion of multiple features. Notably, we tracked performance on cold-start items—those with few or no interactions in the training data—and found that adding more features significantly boosted performance on these items, demonstrating the value of feature richness for addressing sparse data challenges.

Investigations into position embeddings showed no significant AUC difference between relative attention bias and learned position embeddings for history sequence, suggesting that this choice has minimal impact. Similarly, replacing MLPs with DCNv2 or incorporating DCNv2 at the input level did not result in performance gains, despite its theoretical potential to model interactions across embedding dimensions. Even removing MLPs from transformer blocks caused only minor performance drops, presenting an opportunity to trade off MLPs for additional MHA layers within the same memory budget.

Our experiments with alternative attention activation functions, such as SiLU and Sigmoid, also yielded subpar results compared to Softmax. While these activations were hypothesized to better model extreme affinity cases, Softmax's normalization proved more effective at emphasizing the relative importance of sequence elements. This aligns with our findings on HSTU, which did not improve performance and showed lower AUC. Additionally, HSTU's reliance on SiLU attention made it incompatible with FlashAttention, a critical component in larger-scale experiments.

Regarding vocabulary size, we found no AUC difference between item ID embedding vocabularies of 33 million and 66 million, despite the training data encompassing approximately 150 million unique object IDs. This suggests that a smaller vocabulary suffices without compromising performance.

%Lastly, the team's focus on metrics and rapid iteration proved instrumental in achieving and surpassing MME AUC within a single quarter. By prioritizing fast experimentation, weekly performance improvements, and deferring non-critical tasks, two engineers were able to deliver unexpected results in a remarkably short timeframe. This approach underscores the value of agility and metric-driven development in advancing model performance.

\subsection{Analysis of alternative solutions}\label{wukong_layer}
We also explored alternative methods for scaling the model. Wukong \cite{zhang2024wukongscalinglawlargescale} has demonstrated effective dense parameter scaling by allowing the model to capture higher-order feature interactions efficiently. However, directly applying Wukong to the Feed models did not result in any observable improvement in AUCs, even when we scale the number of layers to 8. Our hypothesis suggests that the numerical features may have experienced information degradation during deep feature interactions with embedding features, potentially due to feature heterogeneity. The diverse nature of features, including numerical and categorical data, likely contributed to challenges in effectively capturing complex interactions. 

We attempted to change the design for Wukong, where we employ a dual-pathway approach to feature processing to a baseline architecture (see \S\ref{sec:Overview:FeedRanking}). Embedding features are passed through a deep Wukong layer, facilitating deep vector-wise interactions. Concurrently, numerical and categorical features are fed into a two-layer DCNv2 to enable element-wise interactions. This bifurcated structure allows for specialized handling of different feature types, potentially mitigating the challenges posed by feature heterogeneity. With this updated architecture, we observed a 0.36\% increase in Contributions AUC on top of baseline model (see \S\ref{sec:Overview:FeedRanking}), when scaling the number of layers to 8, which brings additional 2 million parameters.  With a two-tower structure, we extend the scaling to accommodate multiple objectives. For example, our model can simultaneously optimize for various user interactions, such as clicks, comments, shares, and more. Additionally, we explore different mechanisms for concatenating embeddings to achieve optimal performance. It's important to note that the Wukong layer is not restricted to stacked Factorization Machines; it can also incorporate stacked CrossNet for enhanced flexibility. The original Wukong Layer includes a Linear Compress Block, which linearly recombines embeddings — an important element for performance. When incorporated into the stacked CrossNet variant of Wukong, this block is adapted into a wide layer. This provides an interesting perspective, as each layer in Wukong effectively embodies the wide and deep architecture. 

As we developed {\systemname}, our proposed architecture (Figure \ref{fig:custom-transformer}) achieved a 1.2\% increase in Contributions AUC and demonstrated better scalability compared to the 0.36\% improvement observed with the Wukong layer. We believe that Wukong could primarily be used to extend DLRM-style models \cite{DLRM19} or to be used in combination with DLRM and transformer-based models, such as \cite{zeng2024interformer}.