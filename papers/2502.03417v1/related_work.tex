\section{Related Work}\label{sec:related_work}

\textbf{Deep learning systems:} Variety of recommender deep learning model architectures had been published targeting to improve model performance.  From Wide\&Deep model\cite{wideDeep2016} initiated this trend by combining a generalized linear model with an MLP network. Subsequent research aimed to keep the MLP network for implicit feature interactions and replace the linear model with other modules for capturing explicit higher-order feature interactions. Examples include DeepFM\cite{Guo2017DeepFMAF}, which replaced the linear model with FM; deep cross network (DCN)\cite{Wang2017DeepC} and its follow-up DCNv2\cite{Wang2020DCNVI}, which introduced a cross network for high-order feature interactions; xDeepFM\cite{Lian2018xDeepFMCE}, offering compressed interaction network (CIN) for explicit vector-wise feature interactions; AutoInt\cite{Song2018AutoIntAF}, which introduced self-attention networks for explicit feature interaction; AFN\cite{Cheng2019AdaptiveFN}, exploring adaptive-order feature interactions through a logarithmic transformation layer; and FinalMLP\cite{Mao2023FinalMLPAE}, which achieved impressive performance by combining two MLPs. Recently within \cite{zeng2024interformer} authors proposed to combine DLRM style architecture and transformers through attention and feature interaction of existing manually engineered features and temporal user sequences.

While these architectures each excel in specific aspects of feature interaction, they typically rely on handcrafted features. Recent studies have explored how to integrate these architectures into a unified model capable of handling production traffic \cite{LiRank_paper}. However, combining architectures often relies on trial and error, making it more of an art.
In this paper, we demonstrate that a unified transformer-based architecture can outperform complex combinations of several powerful ranking architectures \cite{LiRank_paper}. Additionally, we show that these transformer architectures can significantly reduce the reliance on manual feature engineering and counter features, simplifying the model development process and at the same time improving state-of-the-art performance. Together with the ranking stack we aim to up-level our retrieval stack by making proposed architectures available during embedding based retrieval. We enable {\systemname} on top of GPU retrieval system called LiNR \cite{Linr_paper}. We observed that proposed architectures could significantly enhance quality of retrieval.

\textbf{Setwise ranking:} Most deep recommender systems rely on point-wise scoring, where each item is evaluated independently of others in the user session. However, when recommended items are presented to users, their reactions are naturally influenced by the combination of items shown. A user's response to one item can depend on the other items displayed alongside it. Research on contextual reranking and set-wise ranking has demonstrated the benefits of considering items in the context of one another. Set-wise reranking methods leverage transformer architectures to account for these interactions and have shown significant improvements over point-wise baselines \cite{SetRank_paper, feng2021grngenerativereranknetwork, RankFormer_paper, Non_autoregressive_paper, PRM_paper, Pear_paper}.

\textbf{Scaling law of recommender systems:}
Scaling laws describe how model performance improves predictably with increases in model size, training dataset size, and computational resources. These laws have been well-documented in the domain of large language models (LLMs) \cite{Scaling_Law_LLM_kaplan2020scalinglawsneurallanguage}. However, in the field of recommender systems, only a few works have explored scaling laws in industry settings as data or model sizes increases \cite{zhang2024wukongscalinglawlargescale, HLLMchen2024hll_paper, HSTU_paper_zhai24a}.  Recent LLM-based recommender systems have demonstrated scaling laws \cite{HLLMchen2024hll_paper}, but these approaches primarily add supplementary features to existing ranking models. In this paper, we introduce novel modifications to transformer architectures that enable more effective scaling laws than previous state-of-the-art approaches for recommender systems. Our proposed architecture can replace powerful baseline ranking models while using only 7 features, highlighting its efficiency and scalability.