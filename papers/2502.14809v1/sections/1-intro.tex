\section{Introduction}\label{sec:intro}
Differential Privacy (DP)~\citep{Dwork:2006} has become the de facto standard for privacy preserving data analysis. An important use case is that of releasing statistics about sub-populations within a dataset while protecting privacy of individual records within it. DP has seen several practical deployments in the recent years, e.g., the US census being a significant one~\citep{abowd22topdown}.

Formally, consider a dataset $D = (x_1, \ldots, x_n) \in \cX^n$ of \emph{records} $x_i \in \cX$ for some finite \emph{domain} $\cX$, and consider a \emph{query} of interest that is a function $f : \cX \to \{0, 1\}$. We are interested in privately releasing the evaluation of the statistical query (a.k.a. linear query) $f \in \cF$ on dataset $D$, namely $f(D) := \sum_{x \in D} f(x)$, where $\cF$ is a family of queries. To motivate this definition, consider a dataset where each record $x$ contains several attributes about a unique individual in a population, such as their age, gender, race, zip-code, education level, income, etc. A potential query function $f$ of interest could be such that $f(x) = 1$ if the record $x$ corresponds to a male person of age in 30-35, with income above \$50,000 in all zip-codes within a certain county, and $f(x)=0$ otherwise.

Mechanisms that satisfy DP provide randomized estimates $(\he_f)_{f \in \cF}$ of these statistical query evaluations $(f(D))_{f \in \cF}$. The quality of these estimates are measured in terms of their accuracy, with a typical approach being to measure the {\em additive error}, namely, we would say that the estimates $\hat{\be} \in \R^{\cF}$ is $(\alpha, \cF)$-accurate with respect to dataset $D$ if $|\hat e_f - f(D)| \le \alpha$ for all $f \in \cF$. Prior work has introduced numerous mechanisms for releasing statistical query evaluations, which we summarize in \Cref{tab:prior-work-our-results}; while some of these mechanisms only release the estimates $\hat{\be} \in \R^{\cF}$, others generate a synthetic dataset $\widehat{D}$ from which the estimates $\he_f := f(\widehat{D})$ can be derived for any $f \in \cF$.

These results are stated informally with $\tO(\cdot)$ notation that hides lower order polylogarithmic terms\footnote{Formally, $\tO(f)$ means a quantity that is $O(f \log^c f)$ for some constant $c$, where the asymptotics are in terms of $n, |\cX|, |\cF|, 1/\zeta, 1/\eps, 1/\delta$.} 
wherever applicable, and we also consider a small, but constant, failure probability, e.g., $0.01$. An important point to note about all the additive error bounds are that they incur an error that is polynomial in either $n$, $|\cF|$, or $|\cX|$. Moreover, such dependencies are known to be required for {\em additive-only} error.


{
\begin{table}[t]\renewcommand{\arraystretch}{1.8}
    \centering\fontsize{9pt}{10}\selectfont
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    {\bf Reference} & {\bf Error type} & {\bf Error bound} & {\bf DP} & {\bf Release} \\
    \hline
     \cite{steinke16between} & \multirow{5}{*}{Additive} & $O\prn{\frac{|\cF|}{\eps}}$ & $\eps$-DP & \multirow{2}{*}{Estimates}\\
     \cline{1-1}\cline{3-4}
    \raisebox{-5pt}{\footnotesize \shortstack{\cite{DK22}\\ \cite{GKM21}}} & & $O\prn{\frac{\sqrt{|\cF| \log \frac 1\delta}}{\eps}}$ & $(\eps, \delta)$-DP & \\
    \cline{1-1}\cline{3-5}
    {\footnotesize \cite{vadhan17complexity}} &  & $O\prn{\frac{\sqrt{|\cX| \log |\cF|}}{\eps}}$ & $\eps$-DP & \multirow{3}{*}{Syn. Data} \\
    \cline{1-1}\cline{3-4}
    \raisebox{-7pt}{\footnotesize \shortstack{\cite{BlumLR13}\\\cite{HardtR10}\\\cite{HardtLM12}}} & & $O\prn{n^{\frac23} \prn{\frac{\log |\cF| \log|\cX|}{\eps}}^{\frac13}}$ & $\eps$-DP & \\
    \cline{1-1}\cline{3-4}
    \raisebox{-5pt}{\footnotesize \shortstack{\cite{HardtR10}\\\cite{HardtLM12}}} & & $O\prn{n^{\frac12}\prn{\frac{\sqrt{\log |\cX|} \log |\cF| \log \frac1\delta}{\eps}}^{\frac12}}$ & $(\eps, \delta)$-DP & \\
    \hline
    \hline
    \Cref{thm:efficient_pure_DP_UP} (This work) & \multirow{2}{*}{Relative} & $\tO\prn{\sqrt{\frac{n}{\eps \zeta^2}\log |{\cF}|\log|{\cX}|}+\frac{1}{\eps}}$ & $\eps$-DP & \multirow{2}{*}{Syn. Data} \\
    \Cref{thm:main} (This work) & & $\tO\prn{\frac{1}{\zeta\eps} \prn{\log n \log \frac1\delta}^{\frac32} \sqrt{\log |\cX|}\log|\cF|}$ & $(\eps, \delta)$-DP & \\
    \hline
    \end{tabular}
    \caption{Additive error bounds in prior work \& relative error bounds in this work.}
    \label{tab:prior-work-our-results}
\end{table}
}

The additive nature of the error bound can however be limiting in some settings. Consider a statistical query $f(D)$ that counts the number of people matching a certain rare combination of attributes. In such cases, the additive error can completely overwhelm the true value of the statistical query $f(D)$. On the other hand, for a statistical query $f(D)$ that counts the number of records matching a common combination of attributes, the additive error will be much smaller compared to $f(D)$.%
In such cases, it can be more meaningful to have a notion of {\em relative} error, wherein the scale of error depends on the magnitude of the statistical query, thereby affording a small error when $f(D)$ is small.

\paragraph{Our Contributions.}
We consider the problem of answering statistical queries with {\em relative} error. Namely, we say that the estimates $\hat{\be} := (\he_f)_{f\in \cF}$ are \emph{$(\zeta,\alpha, \cF)$-accurate} with respect to dataset $D$ if for all $f \in \cF$ it holds that $(1 - \zeta) \cdot f(D) - \alpha \leq \he_f \leq (1 + \zeta) \cdot f(D) + \alpha$.
A (randomized) mechanism $\cA:\cX^* \mapsto\R_{\ge 0}^\cF$ is \emph{$(\zeta, \alpha, \beta, \cF)$-accurate} if $\cA(D)$ is $(\zeta, \alpha, \cF)$-accurate with respect to $D$ with probability at least $1 - \beta$.
As before, a stronger approach is to generate a synthetic dataset $\widehat{D}$ such that the derived estimates $\he_f := f(\widehat{D})$ are $(\zeta, \alpha, \beta, \cF)$-accurate with respect to $D$.



In \Cref{sec:upper-bounds}, we introduce a new framework 
of $\mathsf{PREM}$ ({\bf P}rivate {\bf R}elative {\bf E}rror {\bf M}ultiplicative weight update) that, under $(\eps,\delta)$-DP, achieves $\alpha = \tO\Big(\frac{1}{\zeta\eps} \prn{\log n \log \frac1\delta}^{\frac32} \sqrt{\log |\cX|}\log|\cF| \Big)$. Notice that this error is only polylogarithmic in $n, \frac{1}{\delta}, |\cX|, |\cF|$, which is in stark contrast to the aforementioned additive-only results, which \emph{must} be polynomial in one of the parameters.

Under $\eps$-DP, our PREM framework achieves  
$\alpha = \tO\big(\sqrt{\frac{n}{\eps \zeta^2}\log |{\cF}|\log|{\cX}|}+\frac{1}{\eps}\big)$. While this has polynomial dependency of $\sqrt{n}$ on the dataset size, it is still an improvement over the $n^{2/3}$ dependency of the best known mechanisms for additive error~\citep{BlumLR13,HardtLM12}.


In \Cref{sec:lower-bounds}, we prove lower bounds on the relative accuracy of any mechanism that releases estimates for statistical queries (not just synthetic data generation) under both approximate- and pure-DP. For $(\eps, \delta)$-DP, we show that for sufficiently large family of queries $\cF$, our algorithm is nearly optimal (up to polylog factors on $n,\log|\cF|,\log|\cX|,\frac{1}{\zeta},\frac{1}{\eps},\frac{1}{\delta}$). For $\eps$-DP, our lower bound is not as sharp compared to our upper bounds, which is a gap that is present even in the purely-additive case \citep{DPorg-open-problem-optimal-query-release}. We suspect that in order to tighten the pure-DP gaps for relative accuracy it may be necessary to first resolve the gaps on the purely-additive setting.


Finally, in \Cref{app:real-v-bin} we extend our relative-error upper bounds to hold for queries taking values in the interval $[0, 1]$. This approach is based on a general reduction that splits each real-valued statistical query into Boolean-valued threshold queries with exponentially-decreasing thresholds. This reduction induces a minimal overhead in the additive error bound. In fact, the additive error remains asymptotically the same as long as $\frac{\log n}{\zeta} \leq |\cF|^{O(1)}$. 

\paragraph{Related Work.}
Query answering and synthetic data generation are central problems in private data analysis, and there is a vast literature studying them.

The first concerns on answering counting queries over sensitive databases were the driving force behind the notion of DP~\citep{Dinur:2003,Dwork:2006}. Ever since, this problem has been a central focus in this area (see, e.g.,~\cite{Hay:2010,Li:2010matrix_mech,Hardt:2010,Gupta:2011,Nikolov:2013,Dwork:2015}). It was later observed that generating private synthetic data offers the possibility for data analysts to access datasets in `raw' form, and where any posterior analysis is privacy protected, by postprocessing properties of DP. A meaningful way to assess the quality of synthetic data is by its worst-case additive error over a set of prescribed (or adaptively generated) queries \citep{Barak:2007,BlumLR13,HardtR10,HardtLM12,GuptaRU12,Hsu:2013,Gaboardi:2014}. Both in the query answering and synthetic data settings, it is known that any DP algorithm must incur error that is polynomial in at least one of $n,|\cF|,|\cX|$ \citep{HardtPhDThesis:2011,Bun:2014}. 

Relative accuracy guarantees have become a recent focus in DP, particularly for analytics settings, where trends can be better traced by substantial changes, naturally expressed in relative terms \citep{Cormode:2012,Qardaji:2013,Zhang:2016PrivTree,EpastoMMMVZ23,Ghazi:2023}. 
However, existing works have focused on specific problems, and to our knowledge no general framework has been established in this context. We point out that among these specific settings of interest, the case of spectral and cut approximations on graphs has been studied under relative approximation in multiple works~\cite[e.g.,][]{Blocki:2012,Arora:2019}. Besides the setting of statistical queries, relative error is also the standard notion in approximation algorithms \citep[see, e.g.,][]{WS11}, which have been studied with DP~\citep{GuptaLMRT10}.