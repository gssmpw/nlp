%(Basic Intro) -- 1 page
\section{Introduction}\label{sec:intro}

The paradigm of \emph{pre-training followed by fine-tuning} has seen tremendous success in the last few years. Very large models (often referred to as foundation models) are first trained, typically using very large amounts of data and computational resources, using self-supervised learning approaches~\citep{dosovitskiy2020image, openai2023gpt, dubey2024llama, zhousolving}. When building a model for a new task (which could be a supervised learning task), the idea is to start with the foundation model and then tune its parameters, possibly after adding additional classification layers, by training using task-specific data. The pre-train then fine-tune paradigm has been shown to have significant advantages over training a new model from scratch for the new task. Often, high accuracy can be obtained using much smaller datasets for the new task. 

Despite the success, fine-tuning a model with billions of parameters requires access to heavy computational resources, even when the task datasets are fairly small. Fortunately,  studies (e.g., \citep{panigrahi2023taskspecific} and references therein) show that fine-tuning only a small fraction of parameters can be effective. Parameter-efficient fine-tuning (PEFT) methods have thus been proposed to carry out this idea and address the challenge of making fine-tuning more accessible~\citep{lialin2023scaling}. A leading PEFT approach, Low-Rank Adaptation (LoRA, \citealt{hulora}), achieves memory efficiency by simply making low-rank updates to the weight matrices in the different layers. Another class of PEFT methods is \emph{sparse} fine-tuning (SFT, \citealt{sung2021training, guo2021parameter, ansell2022composable, nikdan2024rosa}), which learns a sparse matrix, typically an unstructured one, for updating the pre-trained weights. However, SFT typically incurs higher memory costs than LoRA during the fine-tuning process, because of the unstructured sparsity. Several works aim to mitigate the memory complexity of SFT~\cite{mofrad2019multithreaded, holmes2021nxmtransformer, pmlr-v202-nikdan23a, nikdan2024rosa}, often at the cost of increased running time and more complex implementations of sparse kernels. Besides PEFTs, techniques like Zeroth-Order optimization~\cite{malladi2023fine, guo2024zeroth} and quantization~\cite{gholami2022survey, dettmers2022gpt3, dettmers2024qlora} can further enhance memory and training efficiency for fine-tuning, including LoRA and SFT. 

%Commonly used PEFT methods include  LoRA and sparse fine-tuning (SFT, \citealt{sung2021training, guo2021parameter, ansell2022composable, nikdan2024rosa}). LoRA, the most widely used PEFT, achieves memory efficiency by simply making low-rank updates to the weight matrices in the different layers. In contrast, SFT learns a sparse matrix for updates, typically an unstructured one. Due to this lack of structure, SFT methods typically have a higher memory usage during the fine-tuning process than LoRA. 

%Although numerous studies have explored efficient computation techniques for unstructured sparse matrices to achieve memory efficiency during training~\citep{mofrad2019multithreaded, holmes2021nxmtransformer, pmlr-v202-nikdan23a, nikdan2024rosa}, these approaches often involve a tradeoff, as they increase time complexity to reduce memory usage. Therefore, some approaches also leverage C++ for acceleration, as seen in works like~\citep{nikdan2024rosa, pmlr-v202-nikdan23a}\footnote{The implementation of their forward pass, backward pass, and back-propagation can be found in \url{https://github.com/IST-DASLab/spops}, where the tensor operations are mostly implemented by C++.}. 

As LLMs increase in scale, advancing efficient sparse matrix computation, PEFT, and efficient training remains a crucial problem. %Many efficiency-related approaches are working on the similar problems, such as Zeroth-Order methods~\cite{malladi2023fine, guo2024zeroth} and techniques combining low-rank decomposition and quantization~\citep{gholami2022survey, dettmers2022gpt3,dettmers2024qlora}, where model parameters are represented using 8-bit or 4-bit data types.
%
%{\color{red}may remove this paragraph} The methods above for fine-tuning resemble the literature on neural network compression or ``network pruning'' \citep{han2015learning, han2017efficient}. This line of work, starting with the seminal paper of~\cite{lecun1989optimal}, aims to develop smaller models that have the same functional behavior as a much larger neural network. The primary applications are in deploying NN models on edge devices that are power- and resource- constrained. Ideas such as low rank factorization and sparsity, combined with quantization (representing the weights using 8-bit or 4-bit data types; e.g., see~\citep{gholami2022survey}) have played a key role in NN compression. Another prominent class of methods are unstructured and structured pruning~\citep{cheng2024survey}. The former zeros out less important parameters (resulting in a sparse weight matrix), while structured pruning removes the least important neurons or channels (resulting in a smaller dimensional weight matrix). Both reduce the model's space and computational complexity without significantly degrading accuracy. Despite similarity in methods, to our knowledge, pruning techniques have not been directly useful in model fine-tuning.
%
%{\color{red}may remove this paragraph} Although numerous works have explored unstructured pruning and SFT, a key challenge persists: unstructured sparse matrices require additional implementations for the training process to achieve memory efficiency. This often involves optimizing tensor computations by selectively processing only non-zero elements, e.g. torch.sparse\footnote{The beta version of torch.sparse please see \url{https://pytorch.org/docs/stable/sparse.html}}, compressed sparse column/row (CSC/CSR, \citealp{mofrad2019multithreaded}), semi-structured formats \citep{holmes2021nxmtransformer}, etc. The tradeoff in these approaches lies in the fact that they all increase time complexity to achieve reduced memory complexity. Therefore, some approaches also leverage C++ for acceleration, as seen in works like \citep{nikdan2024rosa, pmlr-v202-nikdan23a}\footnote{The implementation of their forward pass, backward pass, and back-propagation can be found in \url{https://github.com/IST-DASLab/spops}, where the tensor operations are mostly implemented by C++.}. The necessity for such additional implementations complicates the practical application of these methods and increases the difficulty of further advancing this field. 
%
%In this work, we study the question: \emph{ Can sparse fine-tuning be improved by incorporating techniques from neural network compression and matrix decomposition to create a memory- and parameter-efficient framework, while avoiding additional implementations of sparse operations and without increasing the training time complexity?} We answer this question in the affirmative, by proposing a new SFT framework for fine-tuning LLMs and Vision Transformers that achieves memory- and parameter-efficiency while maintaining or even improving performance on downstream tasks. Our approach utilizes structured NN pruning to identify a subset of fine-tuning parameters and employs a matrix decomposition-based computation for efficient fine-tuning. This design enables the integration of ideas from model compression, SFT, and matrix decomposition methods.
%
Towards this goal, we study the question: \emph{Can sparse fine-tuning be improved to create a memory- and parameter-efficient framework, while avoiding additional implementations of sparse operations and without increasing the training time complexity?} We answer this question in the affirmative, by proposing a new SFT framework for fine-tuning LLMs and Vision Transformers that achieves memory- and parameter-efficiency while maintaining or even improving performance on downstream tasks. Our approach utilizes NN pruning techniques to identify a subset of fine-tuning parameters and employs a matrix decomposition-based computation for efficient fine-tuning. This design enables the integration of ideas from model compression, SFT, and matrix decomposition methods.

\subsection{Our Contributions}\label{sec:our-results}
At a high level, our contributions are as follows:
\begin{itemize}
    \item We leverage ideas from \emph{network pruning} to improve SFT, achieving significant memory efficiency considerably lower than the popular LoRA. Our method uses only standard tensor operations, eliminating the need for custom sparse tensor libraries. Our approach is also modular, and it allows us to integrate several existing pruning techniques (which give different neuron importance scores) and works with all layer types, including LayerNorm and BatchNorm, which LoRA cannot directly handle. 
    \item We analyze the memory assignment of several PEFT methods and suggest that \emph{model architecture and computation graphs affect memory more significantly} than the number of trainable parameters. We validate our methods across diverse fine-tuning tasks (language and vision) and provide practical guidance on training strategies to maximize efficiency and accuracy. 
    \item We propose two variants of the Taylor importance for different settings in image and language tasks: \emph{class-aware} Taylor and Zeroth-Order Taylor. The first one is designed for tasks where class-wise accuracy is important (in addition to overall accuracy), such as image classification. Zeroth-Order Taylor is designed for large language models and requires memory only \emph{equal to that of a forward pass}. In addition, we show how to effectively reduce the estimation variance of the Zeroth-Order estimator.
\end{itemize}

The rest of the paper is organized as follows. We discuss existing PEFT methods in Section~\ref{sec:related} and analyze the existing problem in memory efficiency in Section~\ref{sec:memory_trainable_parameters}. Following this, we describe our approach in detail in Section~\ref{sec:method}. Section~\ref{sec:setup} describes the settings of our experiments. We then present and discuss our results in Section~\ref{sec:results}. Section~\ref{sec:conclude} concludes with some directions for future work along our lines.

\section{Background and Related Work}\label{sec:related}

\textbf{Parameter-Efficient and Memory-Efficient Fine-Tuning}: In various language and vision tasks, the ``pre-train then fine-tune'' paradigm has been shown highly effective. PEFT methods~\cite{lialin2023scaling} fine-tune a small subset of the parameters of a large pre-trained model in order to accelerate the training process. We begin by introducing SFT and LoRA, two popular approaches for PEFT. 

\textbf{Sparse Fine-Tuning}: SFT formulates the fine-tuning process as learning another weight matrix $\W_s$: 
\begin{align}
     &\hat{\W}=\W + \W_s, \label{eq:sft_w}
     \\&\h = f(\hat{\W},\x) = f(\W + \W_s, \x), \label{eq:sft_fwd}
\end{align}
where $\h\in\R^{d_{out}}$ and $\x\in\R^{d_{in}}$ are the input and output of the hidden layer, respectively, $f(\cdot)$ is the forward function, $\W\in \R^{d_{out}\times d_{in}}$ represents the frozen pre-trained parameters, and $\hat{\W}\in \R^{d_{out}\times d_{in}}$ denotes the final parameters used during inference for the fine-tuning task.  %\iffalse $d_{in}$ and $d_{out}$ are the input and output dimensionality of the hidden layer, respectively.\fi 
The matrix $\W_s\in \R^{d_{out}\times d_{in}}$ is sparse and is initialized at $\0$. Such a decomposition is done for every layer in the neural network. SFT methods try to limit the number of parameters to fine-tune. For selecting non-zero indices, \emph{Diff pruning}~\cite{guo2021parameter} learns a mask for $\W_s$ (using a standard Backprop algorithm), while \emph{FISH Mask}~\cite{sung2021training} uses Fisher information to identify important indices in $\W$. \emph{Lottery Ticket SFT}~\cite{ansell2022composable} fine-tune the whole model for one epoch, then use $\W_s$ itself as an importance score to decide which parameters to fine-tune subsequently. \emph{Robust Adaptor}~(RoSA,~\citealt{nikdan2024rosa}) combines the above SFTs with LoRA and outperforms all these approaches.
However, the key challenge of all SFT methods is that they do not sufficiently reduce memory usage, as $\W_s$ keeps the dimensionality of $\W$, and thus standard libraries do not yield memory improvements. %leading to similar memory complexity of fine-tuning $\W$ directly.

\textbf{Techniques for Efficient Sparse Computation}: To reduce memory redundancy in sparse tensor computations, various data formats like compressed sparse column/row (CSC/CSR, \citealp{mofrad2019multithreaded, lu2024spp}) and semi-structured formats~\citep{holmes2021nxmtransformer} have been proposed. These formats enable efficient operations like Sparse Matrix Multiplication (SpMM), which is crucial for dot products and matrix multiplications. Upon these techniques, sparse backpropagation is built to improve training efficiency~\citep{zhang2020sparch, gale2020sparse, peste2021ac, schwarz2021powerpropagation, hoefler2021sparsity,jiang2022exposing, pmlr-v202-nikdan23a, xu2024survey}. Beyond sparse tensor techniques, NVIDIA also offers memory optimization techniques for efficient training\footnote{Available at \url{https://pytorch.org/torchtune/stable/tutorials/memory_optimizations.html}}.

However, these techniques come with trade-offs, particularly in terms of time complexity and implementation complexity. Achieving memory efficiency often requires a significant increase in time complexity. To mitigate this, some approaches employ optimizations implemented in C++ or lower-level languages, such as those used in \citep{gale2020sparse, pmlr-v202-nikdan23a, nikdan2024rosa}, to accelerate the training process. 

\textbf{Low-Rank Adaptation (LoRA)}: Instead of requiring $\W_s$ to be sparse, low-rank adaptation aims to find update matrices that are of small rank:
\begin{align}
    & \hat{\W}=\W + \frac{\alpha}{r}\B\A \label{eq:lora_w}, \\
    & \h = f(\hat{\W}, \x) = f(\W, \x) + f(\frac{\alpha}{r}\B\A, \x), \label{eq:lora_fwd}
\end{align}
where $\alpha$ is the LoRA scaling hyper-parameter, \iffalse $r$ is the middle dimensionality of the low-rank matrices, \fi$\B\in \R^{d_{out}\times r},\ \A\in \R^{r\times d_{in}}$ are the low-rank matrices with $r\ll d_{in},d_{out}$. During inference, the $\B \A$ term can be merged into $\W$ to maintain the inference latency of the original model. During training, owing to the fact that $f$ is additive for both the self-attention blocks and the subsequent multilayer perceptron (MLP) layers of transformers~\citep{vaswani2017attention}, backpropagation can be performed efficiently for the $\B, \A$ parameters. Due to LoRA's simplicity and effectiveness, numerous variants have been proposed to enhance the performance, e.g., QLoRA~\citep{dettmers2022gpt3,guo2024lqlora,li2024loftq,dettmers2024qlora}, DoRA~\citep{liu2024dora}, RoSA~\citep{nikdan2024rosa}, and VeRA~\citep{kopiczko2024vera}. %~\citep{dettmers2022gpt3,liu2024dora,guo2024lqlora,li2024loftq,kopiczko2024vera,nikdan2024rosa, dettmers2024qlora}. 
These methods have achieved exceptional performance, often comparable to full fine-tuning across a range of tasks.

%
%Matrix decomposition is commonly used to reduce the computations in data sequences or the model's parameter matrices. In transformer-based models \citep{vaswani2017attention}, both the self-attention blocks and the subsequent multilayer perceptron (MLP) structures consist of several linear layers, of which the forward function can be written as $f(\W,\x)=\W\x$. This structure enables the application of LoRA:
%
%: 
%Recent works also combine methods like quantization with LoRA to further reduce memory usage \citep{dettmers2022gpt3, dettmers2024qlora}. 

\textbf{Neural Network Pruning}:
Besides PEFTs, neural network pruning is another widely applied technique that exploits parameter sparsity to reduce model complexity and speed up inference~\citep{lecun1989optimal, han2015learning, han2017efficient, hoefler2021sparsity}. Most pruning methods assess \emph{importance} of neural network weights (or neurons) and remove the least important parameters. \emph{Unstructured} pruning zeros out individual weights while preserving the network architecture, whereas \emph{structured} pruning removes parameter groups like channels or neurons, which reduce model size~\citep{liu2021group, fang2023depgraph, ma2023llmpruner}. Both approaches often require retraining to recover lost accuracy during pruning. While effective for classical NNs, pruning LLMs is costly due to high memory demands for computing importance scores and the prohibitive retraining step, making memory-efficient LLM pruning an active research area~\citep{frantar2023sparsegpt,sunsimple}.

%evaluate the importance score and remove the less important parameters. In practice, unstructured pruning evaluates the parameters individually and zeros out certain parameters without modifying the network structure, whereas structured pruning finds the dependency of parameters \citep{liu2021group, fang2023depgraph, ma2023llmpruner}, evaluating the importance of parameters by group, and removing the dependent group of components like channels and neurons, thereby reducing the network's size. Both pruning methods rely on retraining the model to recover the performance, whereas retraining LLM is too expensive. In addition, most state-of-the-art pruning methods evaluate the importance score by gradient-based metrics, which are also not suitable for LLM. Therefore, memory-efficiency is also a key challenge in recent researches of LLM pruning \citep{sunsimple, frantar2023sparsegpt}.

\section{Number of Trainable Parameters Is Not Everything}\label{sec:memory_trainable_parameters}
Before introducing our approach, we want to emphasize that \emph{in PEFT research, reducing the number of trainable parameters is not the most critical factor for saving memory.} Some PEFT methods mentioned in Section~\ref{sec:related} focus on reducing the number of trainable parameters to decrease memory consumption. However, once the number of trainable parameters is sufficiently small, this reduction is no longer the most critical factor influencing memory usage.

\begin{table}[htbp]
\tiny
\begin{center}
\begin{tabular}{l|ccccc|cccccl}\toprule
 &  \multicolumn{5}{|c|}{Llama2(7B)} \\\cmidrule(lr){2-6}
PEFT & \#param & mem & dropout & param & other \\\midrule
LoRA & 159.9M(2.37\%) & 23.46GB & 6.18GB & 2.51GB & 14.77GB\\
RoSA & 157.7M(2.34\%) & 44.69GB & 6.22GB & 1.72GB & 36.75GB\\
RoSA-bf16 & 157.7M(2.34\%) & 39.55GB & 6.22GB & 0.92GB & 32.41GB\\
DoRA & 161.3M(2.39\%) & 44.85GB & 6.29GB & 2.49GB & 36.07GB\\
VeRA & 1.37M(0.02\%) & 22.97GB & 2.15GB & 0.11GB & 20.71GB\\\bottomrule

\end{tabular}
%\vspace{-0.2cm}
\caption{The requirements of computation resources for fine-tuning full precision Llama2 using LoRA~\cite{hulora}, RoSA~\cite{nikdan2024rosa}, DoRA~\cite{liu2024dora}, and VeRA~\cite{kopiczko2024vera}. In this analysis, we set $r=64$ for LoRA, DoRA, and VeRA and set $r=32, d=1.2\%$ for RoSA. The fine-tuning parameters of RoSA-bf16 are in bfloat16 under mixed-precision training~\citep{micikevicius2018mixed}, while all others are in full precision. Note that RoSA has its own official implementation, whereas LoRA, DoRA, and VeRA are integrated into the PEFT library provided by Hugging Face. This may influence the memory used by each method. `mem' traces the peak memory usage during training, excluding the memory consumed by the model itself, details please see Appendix~\ref{apdx:measure}. `dropout' accounts for the memory consumption associated with LoRA's dropout layer. `param' represents the maximum memory savings from reducing the number of trainable parameters. It is quantified by varying $r$ and $d$ for each method and calculating the memory difference attributed to trainable parameters. `other' indicates the total memory usage from any other intermediate values in backpropagation and optimizer states.} \label{tab:memory_analysis} 
\end{center}
\end{table}

In NN training, backpropagation involves caching numerous intermediate values to compute gradients efficiently for each tensor. The memory cost of these intermediate values is heavily influenced by the computation graph and the model architecture. When the number of trainable parameters is relatively small, the memory consumed by intermediate values far exceeds that of the trainable parameters themselves. We use VeRA, RoSA, and DoRA to demonstrate the influences of these factors. Table~\ref{tab:memory_analysis} provides a detailed breakdown of the memory usage across different PEFT methods and components. %For `dropout', we traced the memory usage of fine-tuning Llama2 using LoRA \footnote{Since these PEFT methods use the dropout layer in exactly the same way—placing it before the LoRA layer—the memory usage of dropout is identical to LoRA.}, with and without dropout, to estimate dropout's memory cost. 
For `dropout', we traced the memory usage of fine-tuning Llama2 using these PEFT methods, with and without dropout, to yield dropout's memory cost. Additionally, we examined memory usage under varying $r$ and $d$ to obtain the maximum potential memory savings from reducing the number of trainable parameters in each method. For instance, in the case of LoRA, fine-tuning the model with $r=64$ and $r=32$ shows a memory saving of approximately $1.25$GB when reducing $r$ from $64$ to $32$. This implies that the memory consumed by trainable parameters in LoRA with $r=64$ is approximately $1.25 \times 2 = 2.51$GB.

Among PEFT methods that build on LoRA, VeRA aims to further reduce memory by lowering the number of trainable parameters, while RoSA and DoRA focus on improving accuracy. VeRA shares a unique pair of low-rank matrices across layers, reducing trainable parameters compared to LoRA. However, as shown in Table~\ref{tab:memory_analysis}, it saves about 2.4GB from trainable parameters and about 4GB from dropout layers while introducing extra overhead from shared parameters and the corresponding intermediate values, resulting in minimal overall savings. DoRA and RoSA consume significantly more memory due to their complex computation and unstructured sparse matrices. DoRA decomposes LoRA’s matrices into magnitude and direction (see figure~\ref{fig:DoRA_graph} in Appendix~\ref{apdx:mem_require}), which significantly increases memory usage. As reflected in Table~\ref{tab:memory_analysis}, while the memory cost by trainable parameters in DoRA is similar to LoRA, the total memory cost is substantially larger. RoSA, despite employing techniques for efficient computation, still requires far more memory than LoRA. Notably, we also use RoSA to demonstrate that quantizing trainable parameters~\citep{gholami2022survey, dettmers2022gpt3, dettmers2024qlora} with mixed-precision training~\citep{micikevicius2018mixed} saves significantly more memory than simply reducing trainable parameters, as shown by the memory difference between RoSA and RoSA-bf16 in Table~\ref{tab:memory_analysis} (see also related works on quantization~\citep{dettmers2022gpt3,guo2024lqlora,li2024loftq,dettmers2024qlora}).
%Among these PEFT methods, VeRA aims to reduce memory usage by reducing the number of trainable parameters, whereas RoSA and DoRA primarily aim to enhance the accuracy of fine-tuned models. VeRA achieves parameter reduction by sharing a unique pair of low-rank matrices projected across all layers, which decreases the trainable parameters compared to LoRA. However, as shown in Table~\ref{tab:memory_analysis}, this can reduce memory usage by at most 2.51GB compared to LoRA. Moreover, VeRA retains the same computation graph as LoRA but introduces additional memory overhead for the shared parameters and their associated intermediate values. Considering these increases, the overall memory savings of VeRA are minimal. For methods like DoRA and RoSA, their significantly higher memory usage stems from their more complex computation and the unstructured sparse matrices. DoRA decomposes LoRA's matrices into magnitude and direction components to enhance performance, the computation graph is provided in Appendix~\ref{apdx:mem_require}. This decomposition substantially increases memory consumption due to the complex computation graph. As reflected in Table~\ref{tab:memory_analysis}, while the memory cost by trainable parameters in DoRA is similar to LoRA, the total memory cost is substantially larger. The other method, RoSA, requires significantly more memory than LoRA due to the use of unstructured sparse matrices, despite employing techniques for efficient computation. We also use RoSA to illustrate that applying quantization to fine-tuning parameters with mixed-precision training~\citep{micikevicius2018mixed} achieves significantly greater memory savings compared to merely reducing the number of trainable parameters~\citep{gholami2022survey, dettmers2022gpt3, dettmers2024qlora}. This is evident from the memory difference between RoSA and RoSA-bf16 in Table~\ref{tab:memory_analysis}.

These results highlight that the primary driver of memory usage in PEFT methods is often the complexity of their computation graph and intermediate caching, rather than the number of trainable parameters alone. LoRA and its variants often rely on dropout to mitigate overfitting, which significantly contributes to memory consumption. On the other hand, SFT methods, which typically do not rely on dropout, face challenges in efficiently computing unstructured matrices, further adding to their memory demands.
