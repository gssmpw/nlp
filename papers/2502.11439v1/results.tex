%We report:
%- overall accuracy numbers

%- accuracy on corrupted datasets

%- Class wise accuracy


\section{Results and Discussion}\label{sec:results}
We now present the results of fine-tuning image models and Llamas using our framework. %Whenever possible, we report three results: full-fine-tuning, our method SPruFT, and head fine-tuning (where all the layer weights are frozen and only a ``classification head'' is added in the end). We show results across several epochs to compare how training evolves for the different fine-tuning strategies. Following this, we examine the performance of our approach by utilizing various importance metrics and evaluating the impact of involving parameter \emph{dependencies}, as we will explain. Finally, we apply our approach SPruFT to fine-tune Llama models and compare the results with those obtained using LoRA. Note that memory efficiency is not emphasized for small-scale models, as dataset-related memory—particularly with large batch sizes—dominates consumption in these cases. The main advantage of our method in these cases is the reduced FLOPs due to fewer trainable parameters.
We first apply our SPruFT to fine-tune Llama2 and Llama3 and compare the results with those obtained using LoRA and its variants. Following this, we examine the performance of our approach by utilizing various importance metrics.

\subsection{Main Results of LLM} \label{subsec:results_LLM}

We apply our SPruFT method to fine-tune Llama2-7B and Llama3-8B, comparing the results with those obtained through LoRA and its variants. We select the magnitude of the neuron vector as the importance metric due to its low memory requirements, simplicity, and widely tested effectiveness. In contrast, gradient-based metrics like Taylor and Hessian are as memory-intensive as full LLM fine-tuning. While Wanda~\cite{sunsimple} offers a memory-efficient metric for pruning LLMs, it still requires one epoch of data forwarding and significantly more memory than inference to compute the input vector's norm\footnote{We encountered an OOM error when using Wanda's official implementation. When pruning a neural network, each layer computes Wanda and is pruned immediately, gradually reducing the model size. However, in the fine-tuning process, the model size remains unchanged. Additionally, storing the norm values for computing importance scores further increases memory consumption, making memory cost significantly higher than when using Wanda for pruning.}. For epochs choosing, we opt for 5 epochs to balance computational resources and performance. 

Table~\ref{tab:llm} demonstrates the exceptional memory efficiency of our approach\footnote{Also refer to Table~\ref{tab:llm_ablation} in Appendix~\ref{apdx:ranks}, even with $r=128$, our method's memory usage remains significantly lower than that of LoRA with $r=16$.} while achieving comparable accuracy. As shown, the accuracies of fine-tuned models remain similar across most PEFT methods, while memory usage varies significantly. VeRA, despite having significantly fewer trainable parameters, shows noticeably lower accuracy. Notably, our approach consistently requires substantially less memory than all other PEFT methods listed in the table.

\begin{table}[htbp]
\tiny
\begin{center}
\begin{tabular}{lccccccccccccc}\toprule
Model, ft setting & mem & \#param & Avg \\\cmidrule(lr){1-4}
Llama2(7B), LoRA, $r=16$ & 21.64GB & 40.0M(0.59\%) & 60.55\\
Llama2(7B), FA-LoRA, $r=32$ & \textbf{16.56GB} & 46.4M(0.69\%) & \textbf{60.71}\\\cmidrule(lr){2-4}
Llama2(7B), LoRA, $r=32$ & 22.21GB & 80.0M(1.19\%) & \textbf{60.92} \\
Llama2(7B), FA-LoRA, $r=64$ & \textbf{17.25GB} & 92.8M(1.38\%)  & 60.85 \\\cmidrule(lr){2-4}
Llama2(7B), SPruFT, $r=32$ & 15.57GB & 36.4M(0.54\%) & 60.52 \\
Llama2(7B), FA-SPruFT, $r=64$ & \textbf{14.48GB} & 39.3M(0.58\%) & \textbf{60.82} \\\cmidrule(lr){2-4}
Llama2(7B), SPruFT, $r=64$ & 16.20GB & 72.9M(1.08\%) & 60.67 \\
Llama2(7B), FA-SPruFT, $r=128$ & \textbf{15.21GB} & 78.6M(1.17\%) & \textbf{60.87} \\\midrule
Llama3(8B), LoRA, $r=16$ & 28.86GB & 41.9M(0.52\%) & 64.76 \\
Llama3(8B), FA-LoRA, $r=32$ & \textbf{23.89GB} & 56.6M(0.71\%) & \textbf{64.81} \\\cmidrule(lr){2-4}
Llama3(8B), LoRA, $r=32$ & 29.37GB & 83.9M(1.04\%) & 64.76 \\
Llama3(8B), FA-LoRA, $r=64$ & \textbf{24.55GB} & 113.2M(1.41\%) & \textbf{64.77} \\\cmidrule(lr){2-4}
Llama3(8B), SPruFT, $r=32$ & 22.62GB & 39.8M(0.50\%) & 64.21 \\
Llama3(8B), FA-SPruFT, $r=64$ & \textbf{21.62GB} & 46.1M(0.57\%) & \textbf{64.56} \\\cmidrule(lr){2-4}
Llama3(8B), SPruFT, $r=64$ & 23.23GB & 79.7M(0.99\%) & \textbf{64.51} \\
Llama3(8B), FA-SPruFT, $r=128$ & \textbf{22.41GB} & 92.3M(1.15\%) & 64.26 \\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{ Comparing \emph{fine-tuning all linear layers} with \emph{fine-tuning only the MLP layers}. FA indicates that we freeze attention layers, but not including MLP layers followed by attention blocks. Full table please refer to Table~\ref{tab:llm_ablation_FA} in Appendix~\ref{apdx:FA}. } \label{tab:llm_FA} 
\end{center}
\end{table}

We then demonstrate that strategically assigning trainable parameters saves more memory than merely reducing them, without compromising accuracy. We compare \emph{fine-tuning all linear layers} and \emph{fine-tuning only the MLP layers}, with results shown in Table~\ref{tab:llm_FA}. The former requires more memory for the same number of trainable parameters, as distributing trainable parameters across the model increases the need for caching intermediate values. Recalling the results from Table~\ref{tab:memory_analysis} in Section~\ref{sec:memory_trainable_parameters}, freezing attention blocks provides significantly greater memory savings than reducing LoRA's trainable parameters. Additionally, accuracy remains nearly unchanged. Given that Llama models have been pre-trained on extensive datasets, their attention blocks likely already capture crucial patterns for token interactions. Our results suggest that freezing these attention blocks maintains performance nearly equivalent to fine-tuning all layers.

%\subsection{Importance Score and Parameter Dependency} \label{subsec:results_imp_dep}
\subsection{Importance Metrics} \label{subsec:results_dep}

\iffalse
\begin{table*}[htbp]
\tiny
\begin{center}
\begin{tabular}{l|c|ccc|ccc|cccccl}\toprule
& data & \multicolumn{3}{c}{CIFAR100} & \multicolumn{3}{c}{Tiny-ImageNet} & \multicolumn{2}{c}{Caltech101} \\\cmidrule(lr){3-5}\cmidrule(lr){6-8} \cmidrule(lr){9-10} 
model & dep & $\ell^2$ & Taylor & QMTaylor  
 & $\ell^2$ & Taylor & QMTaylor & $\ell^2$ & Taylor \\\midrule
DeiT & \XSolidBrush & \textbf{88.05} & \textbf{88.70} & \underline{\textbf{89.37}} & \textbf{89.31} & \textbf{89.69} & \underline{\textbf{89.75}} & \textbf{95.01} & \underline{\textbf{95.41}}\\
& \Checkmark & 86.43 & 87.33 & 88.08 & 85.56 & 85.92 & 86.49 & 65.35 & 78.04\\\midrule
ViT & \XSolidBrush & \textbf{87.13} & \textbf{88.06} & \underline{\textbf{88.51}} & \textbf{90.78} & \textbf{90.87} & \underline{\textbf{90.90}} & \textbf{92.69} & \underline{\textbf{93.96}} \\
& \Checkmark & 85.24 & 86.83 & 87.91 & 88.83 & 88.95 & 89.67 & 56.30 & 77.82\\\midrule
RN & \XSolidBrush & \textbf{82.25} & \textbf{82.36} & \underline{\textbf{83.50}} & \textbf{79.83} & \textbf{79.66} & \underline{\textbf{80.02}} & \underline{\textbf{93.13}} & \textbf{92.56}\\
& \Checkmark & 78.63 & 78.62 & 81.18 & 69.87 & 69.24 & 72.51 & 54.68 & 52.71 \\\midrule
RNX & \XSolidBrush & \textbf{86.12} & \textbf{85.94} & \underline{\textbf{86.93}} & \textbf{83.88} & \textbf{83.88} & \underline{\textbf{84.17}} & \textbf{95.71} & \underline{\textbf{95.84}}\\
& \Checkmark & 84.71 & 85.01 & 85.48 & 79.39 & 78.95 & 79.54 & 92.13 & 91.82\\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{Fine-tuning image models by our SPruFT for 5 epochs. $\ell^2$, Taylor, and QMTaylor represent the magnitude, Taylor importance, and Quantiles-Mean Taylor importance (\Eqref{eq:qmtaylor_mean}). Note that QMTaylor is not applied to fine-tuning Caltech101 due to its significantly imbalanced labels. All reported results are validation accuracies. \textbf{Bold} indicates the superior results achieved. } \label{tab:img_taylor} 
\end{center}
\end{table*}
\fi

%We apply various importance metrics to fine-tune Llamas and image models using our approach and report the results to compare their performance. As shown in Table~\ref{tab:img_taylor} and Table~\ref{tab:llm_imp}, Quantile-Mean Taylor and ZOTaylor offer slight improvements over other importance metrics. For image tasks, while the differences among importance metrics are not substantial, the results consistently indicate that Quantile-Mean Taylor slightly outperforms standard Taylor importance. Additionally, both Quantile-Mean Taylor and standard Taylor importance outperform magnitude-based importance.

%Similarly, in the cases of Llama2 and Llama3, our findings suggest that ZOTaylor provides a slight performance boost for fine-tuned models. This improvement is likely due to ZOTaylor's ability to capture richer data information, whereas magnitude-based importance tends to focus more on identifying generally important neurons. However, the observed performance gain remains modest, potentially due to the variance of the estimates, as discussed in Section~\ref{subsubsec:ZOTaylor}. Beyond these observations, another interesting finding is that fine-tuned models with random row selection perform similarly to VeRA, probably suggesting that this accuracy level could serve as a baseline for other fine-tuning approaches.

\begin{table}[htbp]
\tiny
\begin{center}
%\begin{tabular}{l|ccc|ccc|cccccl}\toprule
%& \multicolumn{3}{c}{CIFAR100} & \multicolumn{3}{c}{Tiny-ImageNet} & \multicolumn{2}{c}{Caltech101} \\\cmidrule(lr){2-4}\cmidrule(lr){5-7} \cmidrule(lr){8-9} 
\begin{tabular}{ll|c|c|c|ccccl}\toprule
%& CIFAR100 & Tiny-ImageNet & Caltech101 \\\cmidrule(lr){2-4}
%model& $\ell^2$ & Taylor & QMTaylor  
% & $\ell^2$ & Taylor & QMTaylor & $\ell^2$ & Taylor \\\midrule
model & imp & CIFAR100 & Tiny-ImageNet & Caltech101 \\\midrule
%DeiT & 88.05 & 88.70 & \textbf{89.37} & 89.31 & 89.69 & \textbf{89.75} & 95.01 & \textbf{95.41} \\\midrule
DeiT & FFT & 90.18 & 87.55 & 97.33\\\cmidrule(lr){2-5}
& $\ell^2$ & 88.05 & 89.31 & 95.01 \\
& Taylor & 88.70 & 89.69 & \textbf{95.41} \\
& QMTaylor & \textbf{89.37} & \textbf{89.75} & - \\\midrule
%ViT & 87.13 & 88.06 & \textbf{88.51} & 90.78 & 90.87 & \textbf{90.90} & 92.69 & \textbf{93.96} \\\midrule
ViT & FFT & 90.13 & 88.45 & 97.16\\\cmidrule(lr){2-5}
& $\ell^2$ & 87.13 & 90.78 & 92.69 \\
& Taylor & 88.06 & 90.87 &\textbf{93.96} \\
& QMTaylor & \textbf{88.51} & \textbf{90.90} & - \\\midrule
%RN & 82.25 & 82.36 & \textbf{83.50} & 79.83 & 79.66 & \textbf{80.02} & \textbf{93.13} & 92.56 \\\midrule
RN & FFT & 86.21 & 77.78 & 96.50\\\cmidrule(lr){2-5}
& $\ell^2$ & 82.25 & 79.83 & \textbf{93.13} \\
& Taylor & 82.36 & 79.66 & 92.56 \\
& QMTaylor & \textbf{83.50} & \textbf{80.02} & - \\\midrule
%RNX & 86.12 & 85.94 & \textbf{86.93} & 83.88 & 83.88 & \textbf{84.17} & 95.71 & \textbf{95.84} \\\bottomrule
RNX & FFT & 87.30 & 79.51 & 97.07\\\cmidrule(lr){2-5}
& $\ell^2$ & 86.12 & 83.88 & 95.71 \\
& Taylor & 85.94 & 83.88 & \textbf{95.84} \\
& QMTaylor & \textbf{86.93} & \textbf{84.17} & - \\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{Importance metrics on fine-tuning image models by our SPruFT for 5 epochs. FFT, $\ell^2$, Taylor, and QMTaylor represent full fine-tuning, the magnitude, Taylor importance, and Quantiles-Mean Taylor importance (\Eqref{eq:qmtaylor_mean}). Note that QMTaylor is not applied to fine-tuning Caltech101 due to its significantly imbalanced labels. All reported results are validation accuracies. \textbf{Bold} indicates the superior results achieved through different importance metrics. } \label{tab:img_taylor} 
\end{center}
\end{table}
\iffalse
\begin{table}[htbp]
\tiny
\begin{center}
\begin{tabular}{lccccccccccccc}\toprule
Model, ft setting & Avg
\\\midrule
Llama2(7B), SPruFT\\ \cmidrule(lr){1-1} 
$r=128$, random & 60.25 \\
$r=128$, $\ell^2$ & 60.86 \\
$r=128$, ZOTaylor & \textbf{60.94} \\\cmidrule(lr){2-2}
$r=128$, FA, random & 60.35 \\
$r=128$, FA, $\ell^2$ & 60.87\\
$r=128$, FA, ZOTaylor & \textbf{60.87}\\\midrule
Llama3(8B), SPruFT\\ \cmidrule(lr){1-1} 
$r=128$, random & 64.00 \\
$r=128$, $\ell^2$ & 64.50 \\
$r=128$, ZOTaylor & \textbf{64.67} \\\cmidrule(lr){2-2}
$r=128$, FA, random & 63.95 \\
$r=128$, FA, $\ell^2$ & 64.26 \\
$r=128$, FA, ZOTaylor & \textbf{64.38} \\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{Importance evaluation for Llama2 and Llama3. Random row selection serves as the baseline here. We also present the results of freezing-attention blocks in this table (FA). Full table with different ranks please refer to Table~\ref{tab:llm_imp_ablation} in Appendix~\ref{apdx:ranks}.} \label{tab:llm_imp} 
\end{center}
\end{table}
\fi

\begin{table}[htbp]
\tiny
\begin{center}
\begin{tabular}{lccccccccccccc}\toprule
Settings & \multicolumn{2}{c}{Avg}
\\\midrule
 & Llama2(7B) & Llama3(8B) \\\midrule
$r=128$, random & 60.25 & 64.00 \\
$r=128$, $\ell^2$ & 60.86 & 64.50 \\
$r=128$, ZOTaylor & \textbf{60.94} & \textbf{64.67} \\\cmidrule(lr){1-3}
$r=128$, FA, random & 60.35 & 63.95 \\
$r=128$, FA, $\ell^2$ & 60.87 & 64.26 \\
$r=128$, FA, ZOTaylor & \textbf{60.87} & \textbf{64.38} \\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{Importance evaluation for Llama2 and Llama3. Random row selection serves as the baseline here. We also present the results of freezing-attention blocks in this table (FA). Full table with different ranks please refer to Table~\ref{tab:llm_imp_ablation} in Appendix~\ref{apdx:ranks}.} \label{tab:llm_imp} 
\end{center}
\end{table}

We apply various importance metrics to fine-tune Llamas and image models using our approach and report the results to compare their performance. As shown in Table~\ref{tab:img_taylor} and Table~\ref{tab:llm_imp}, Quantile-Mean Taylor and ZOTaylor offer slight improvements over other importance metrics. For image tasks, while the differences among importance metrics are not substantial, the results consistently indicate that Quantile-Mean Taylor slightly outperforms standard Taylor importance. Additionally, both Quantile-Mean Taylor and standard Taylor importance outperform magnitude-based importance.

Similarly, in the cases of Llama2 and Llama3, our findings suggest that ZOTaylor provides a slight performance boost for fine-tuned models. This improvement is likely due to ZOTaylor's ability to capture richer data information, whereas magnitude-based importance tends to focus more on identifying generally important neurons. However, the observed performance gain remains modest, potentially due to the variance of the estimates, as discussed in Section~\ref{subsubsec:ZOTaylor}. Beyond these observations, another interesting finding is that fine-tuned models with random row selection perform similarly to VeRA, likely suggesting that this accuracy level could serve as a baseline for other fine-tuning approaches.

\section{Conclusions and Future Work}\label{sec:conclude}

We propose a parameter-efficient fine-tuning (PEFT) framework that integrates various techniques and importance metrics from model compression research to enhance sparse fine-tuning (SFT). Using our method, we can fine-tune LLMs and vision transformers using significantly less computation resources than the popular LoRA (Low-Rank Adaptation) technique, while achieving similar accuracy. We also explore the effects of using different importance metrics. There are several future directions: (1) For importance metrics, while Quantile-Mean Taylor shows slight improvements, these gains are relatively minor compared to the standard Taylor metric in some cases of DeiT and ViT. We may wish to explore better metrics for classification tasks with a large number of labels. (2) Developing memory-efficient importance metrics for LLMs is another future direction. While Zeroth-Order Taylor is effective for incorporating data-specific information without requiring large memory, the large variance of estimate is a challenge. Although we reduce the variance effectively by increasing the number of estimations, exploring a simple method to reduce variance without increasing estimation times is essential for further advancements in this field. (3) Our results show that fine-tuning a small number of neurons can significantly improve model performance on downstream tasks. This observation naturally raises the question: do the selected neurons play a distinctive role in specific tasks? This question is related to the explainability of neural networks, which is an extensive area of research. It will be interesting to understand if (and how) individual neurons chosen for fine-tuning contribute to the new task. 