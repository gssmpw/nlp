@inproceedings{ansell2022composable,
  title={Composable Sparse Fine-Tuning for Cross-Lingual Transfer},
  author={Ansell, Alan and Ponti, Edoardo and Korhonen, Anna and Vuli{\'c}, Ivan},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1778--1796},
  year={2022}
}

@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{fang2023depgraph,
  title={Depgraph: Towards any structural pruning},
  author={Fang, Gongfan and Ma, Xinyin and Song, Mingli and Mi, Michael Bi and Wang, Xinchao},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16091--16101},
  year={2023}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@inproceedings{gale2020sparse,
  title={Sparse gpu kernels for deep learning},
  author={Gale, Trevor and Zaharia, Matei and Young, Cliff and Elsen, Erich},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--14},
  year={2020},
  organization={IEEE}
}

@inproceedings{guo2021parameter,
  title={Parameter-Efficient Transfer Learning with Diff Pruning},
  author={Guo, Demi and Rush, Alexander and Kim, Yoon},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@phdthesis{han2017efficient,
  title={Efficient methods and hardware for deep learning},
  author={Han, Song},
  year={2017},
  school={Stanford University}
}

@article{hoefler2021sparsity,
  title={Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={241},
  pages={1--124},
  year={2021}
}

@article{holmes2021nxmtransformer,
  title={NxMTransformer: semi-structured sparsification for natural language understanding via ADMM},
  author={Holmes, Connor and Zhang, Minjia and He, Yuxiong and Wu, Bo},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={1818--1830},
  year={2021}
}

@article{jiang2022exposing,
  title={Exposing and exploiting fine-grained block structures for fast and accurate sparse training},
  author={Jiang, Peng and Hu, Lihan and Song, Shihui},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38345--38357},
  year={2022}
}

@article{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@article{lialin2023scaling,
  title={Scaling down to scale up: A guide to parameter-efficient fine-tuning},
  author={Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2303.15647},
  year={2023}
}

@inproceedings{liu2021group,
  title={Group fisher pruning for practical network compression},
  author={Liu, Liyang and Zhang, Shilong and Kuang, Zhanghui and Zhou, Aojun and Xue, Jing-Hao and Wang, Xinjiang and Chen, Yimin and Yang, Wenming and Liao, Qingmin and Zhang, Wayne},
  booktitle={International Conference on Machine Learning},
  pages={7021--7032},
  year={2021},
  organization={PMLR}
}

@article{peste2021ac,
  title={Ac/dc: Alternating compressed/decompressed training of deep neural networks},
  author={Peste, Alexandra and Iofinova, Eugenia and Vladu, Adrian and Alistarh, Dan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={8557--8570},
  year={2021}
}

@InProceedings{pmlr-v202-nikdan23a,
  title = 	 {{S}parse{P}rop: Efficient Sparse Backpropagation for Faster Training of Neural Networks at the Edge},
  author =       {Nikdan, Mahdi and Pegolotti, Tommaso and Iofinova, Eugenia and Kurtic, Eldar and Alistarh, Dan},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {26215--26227},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/nikdan23a/nikdan23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/nikdan23a.html},
  abstract = 	 {We provide an efficient implementation of the backpropagation algorithm, specialized to the case where the weights of the neural network being trained are <em>sparse</em>. Our algorithm is general, as it applies to arbitrary (unstructured) sparsity and common layer types (e.g., convolutional or linear). We provide a fast vectorized implementation on commodity CPUs, and show that it can yield speedups in end-to-end runtime experiments, both in transfer learning using already-sparsified networks, and in training sparse networks from scratch. Thus, our results provide the first support for sparse training on commodity hardware.}
}

@article{schwarz2021powerpropagation,
  title={Powerpropagation: A sparsity inducing weight reparameterisation},
  author={Schwarz, Jonathan and Jayakumar, Siddhant and Pascanu, Razvan and Latham, Peter E and Teh, Yee},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={28889--28903},
  year={2021}
}

@article{sung2021training,
  title={Training neural networks with fixed sparse masks},
  author={Sung, Yi-Lin and Nair, Varun and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24193--24205},
  year={2021}
}

@inproceedings{sunsimple,
  title={A Simple and Effective Pruning Approach for Large Language Models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{xu2024survey,
  title={A survey of resource-efficient llm and multimodal foundation models},
  author={Xu, Mengwei and Yin, Wangsong and Cai, Dongqi and Yi, Rongjie and Xu, Daliang and Wang, Qipeng and Wu, Bingyang and Zhao, Yihao and Yang, Chen and Wang, Shihe and others},
  journal={arXiv preprint arXiv:2401.08092},
  year={2024}
}

@inproceedings{zhang2020sparch,
  title={Sparch: Efficient architecture for sparse matrix multiplication},
  author={Zhang, Zhekai and Wang, Hanrui and Han, Song and Dally, William J},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={261--274},
  year={2020},
  organization={IEEE}
}

