@inproceedings{
micikevicius2018mixed,
title={Mixed Precision Training},
author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1gs9JgRZ},
}
@inproceedings{
sun2024evaluating,
title={Evaluating the Zero-shot Robustness of Instruction-tuned Language Models},
author={Jiuding Sun and Chantal Shaib and Byron C Wallace},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=g9diuvxN6D}
}
@article{liu2024few,
  title={Few-shot adaptation of multi-modal foundation models: A survey},
  author={Liu, Fan and Zhang, Tianshu and Dai, Wenwen and Zhang, Chuanyi and Cai, Wenwen and Zhou, Xiaocong and Chen, Delong},
  journal={Artificial Intelligence Review},
  volume={57},
  number={10},
  pages={268},
  year={2024},
  publisher={Springer}
}
@article{xu2024survey,
  title={A survey of resource-efficient llm and multimodal foundation models},
  author={Xu, Mengwei and Yin, Wangsong and Cai, Dongqi and Yi, Rongjie and Xu, Daliang and Wang, Qipeng and Wu, Bingyang and Zhao, Yihao and Yang, Chen and Wang, Shihe and others},
  journal={arXiv preprint arXiv:2401.08092},
  year={2024}
}
@article{hoefler2021sparsity,
  title={Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={241},
  pages={1--124},
  year={2021}
}
@article{cheng2024survey,
  title={A survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations},
  author={Cheng, Hongrong and Zhang, Miao and Shi, Javen Qinfeng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}
@incollection{gholami2022survey,
  title={A survey of quantization methods for efficient neural network inference},
  author={Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Low-Power Computer Vision},
  pages={291--326},
  year={2022},
  publisher={Chapman and Hall/CRC}
}
@inproceedings{sunsimple,
  title={A Simple and Effective Pruning Approach for Large Language Models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
}
@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}
@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}
@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{lialin2023scaling,
  title={Scaling down to scale up: A guide to parameter-efficient fine-tuning},
  author={Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2303.15647},
  year={2023}
}
@inproceedings{hulora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}
@inproceedings{ansell2022composable,
  title={Composable Sparse Fine-Tuning for Cross-Lingual Transfer},
  author={Ansell, Alan and Ponti, Edoardo and Korhonen, Anna and Vuli{\'c}, Ivan},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1778--1796},
  year={2022}
}
@article{ansell2024scaling,
  title={Scaling sparse fine-tuning to large language models},
  author={Ansell, Alan and Vuli{\'c}, Ivan and Sterz, Hannah and Korhonen, Anna and Ponti, Edoardo M},
  journal={arXiv preprint arXiv:2401.16405},
  year={2024}
}
@article{sung2021training,
  title={Training neural networks with fixed sparse masks},
  author={Sung, Yi-Lin and Nair, Varun and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24193--24205},
  year={2021}
}
@inproceedings{guo2021parameter,
  title={Parameter-Efficient Transfer Learning with Diff Pruning},
  author={Guo, Demi and Rush, Alexander and Kim, Yoon},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}
@inproceedings{
liu2024dora,
title={Do{RA}: Weight-Decomposed Low-Rank Adaptation},
author={Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=3d5CIRG1n2}
}
@inproceedings{
guo2024lqlora,
title={{LQ}-Lo{RA}: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning},
author={Han Guo and Philip Greengard and Eric Xing and Yoon Kim},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=xw29VvOMmU}
}
@inproceedings{
li2024loftq,
title={LoftQ: Lo{RA}-Fine-Tuning-aware Quantization for Large Language Models},
author={Yixiao Li and Yifan Yu and Chen Liang and Nikos Karampatziakis and Pengcheng He and Weizhu Chen and Tuo Zhao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=LzPWWPAdY4}
}
@inproceedings{
kopiczko2024vera,
title={Ve{RA}: Vector-based Random Matrix Adaptation},
author={Dawid Jan Kopiczko and Tijmen Blankevoort and Yuki M Asano},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=NjNfLdxr3A}
}
@inproceedings{
lu2024spp,
title={{SPP}: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models},
author={Xudong Lu and Aojun Zhou and Yuhui Xu and Renrui Zhang and Peng Gao and Hongsheng Li},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=9Rroj9GIOQ}
}
@inproceedings{
nikdan2024rosa, 
title={Ro{SA}: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation},
author={Mahdi Nikdan and Soroush Tabesh and Elvir Crn{\v{c}}evi{\'c} and Dan Alistarh},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=FYvpxyS43U}
}
@InProceedings{pmlr-v202-nikdan23a,
  title = 	 {{S}parse{P}rop: Efficient Sparse Backpropagation for Faster Training of Neural Networks at the Edge},
  author =       {Nikdan, Mahdi and Pegolotti, Tommaso and Iofinova, Eugenia and Kurtic, Eldar and Alistarh, Dan},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {26215--26227},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/nikdan23a/nikdan23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/nikdan23a.html},
  abstract = 	 {We provide an efficient implementation of the backpropagation algorithm, specialized to the case where the weights of the neural network being trained are <em>sparse</em>. Our algorithm is general, as it applies to arbitrary (unstructured) sparsity and common layer types (e.g., convolutional or linear). We provide a fast vectorized implementation on commodity CPUs, and show that it can yield speedups in end-to-end runtime experiments, both in transfer learning using already-sparsified networks, and in training sparse networks from scratch. Thus, our results provide the first support for sparse training on commodity hardware.}
}
@article{schwarz2021powerpropagation,
  title={Powerpropagation: A sparsity inducing weight reparameterisation},
  author={Schwarz, Jonathan and Jayakumar, Siddhant and Pascanu, Razvan and Latham, Peter E and Teh, Yee},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={28889--28903},
  year={2021}
}
@article{peste2021ac,
  title={Ac/dc: Alternating compressed/decompressed training of deep neural networks},
  author={Peste, Alexandra and Iofinova, Eugenia and Vladu, Adrian and Alistarh, Dan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={8557--8570},
  year={2021}
}
@article{jiang2022exposing,
  title={Exposing and exploiting fine-grained block structures for fast and accurate sparse training},
  author={Jiang, Peng and Hu, Lihan and Song, Shihui},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38345--38357},
  year={2022}
}
@inproceedings{zhang2020sparch,
  title={Sparch: Efficient architecture for sparse matrix multiplication},
  author={Zhang, Zhekai and Wang, Hanrui and Han, Song and Dally, William J},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={261--274},
  year={2020},
  organization={IEEE}
}
@inproceedings{gale2020sparse,
  title={Sparse gpu kernels for deep learning},
  author={Gale, Trevor and Zaharia, Matei and Young, Cliff and Elsen, Erich},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--14},
  year={2020},
  organization={IEEE}
}
@inproceedings{mofrad2019multithreaded,
  title={Multithreaded layer-wise training of sparse deep neural networks using compressed sparse column},
  author={Mofrad, Mohammad Hasanzadeh and Melhem, Rami and Ahmad, Yousuf and Hammoud, Mohammad},
  booktitle={2019 IEEE High Performance Extreme Computing Conference (HPEC)},
  pages={1--6},
  year={2019},
  organization={IEEE}
}
@article{holmes2021nxmtransformer,
  title={NxMTransformer: semi-structured sparsification for natural language understanding via ADMM},
  author={Holmes, Connor and Zhang, Minjia and He, Yuxiong and Wu, Bo},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={1818--1830},
  year={2021}
}
@inproceedings{singh2022skipper,
  title={Skipper: Enabling efficient snn training through activation-checkpointing and time-skipping},
  author={Singh, Sonali and Sarma, Anup and Lu, Sen and Sengupta, Abhronil and Kandemir, Mahmut T and Neftci, Emre and Narayanan, Vijaykrishnan and Das, Chita R},
  booktitle={2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={565--581},
  year={2022},
  organization={IEEE}
}
@article{herrmann2019optimal,
  title={Optimal checkpointing for heterogeneous chains: how to train deep neural networks with limited memory},
  author={Herrmann, Julien and Beaumont, Olivier and Eyraud-Dubois, Lionel and Hermann, Julien and Joly, Alexis and Shilova, Alena},
  journal={arXiv preprint arXiv:1911.13214},
  year={2019}
}

@inproceedings{zhousolving,
  title={Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification},
  author={Zhou, Aojun and Wang, Ke and Lu, Zimu and Shi, Weikang and Luo, Sichun and Qin, Zipeng and Lu, Shaoqing and Jia, Anya and Song, Linqi and Zhan, Mingjie and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}
%achiam2023gpt,
@article{openai2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{frieze-kannan,
author = {Frieze, Alan and Kannan, Ravi and Vempala, Santosh},
title = {Fast monte-carlo algorithms for finding low-rank approximations},
year = {2004},
issue_date = {November 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {6},
issn = {0004-5411},
url = {https://doi.org/10.1145/1039488.1039494},
doi = {10.1145/1039488.1039494},
abstract = {We consider the problem of approximating a given m \texttimes{} n matrix A by another matrix of specified rank k, which is smaller than m and n. The Singular Value Decomposition (SVD) can be used to find the "best" such approximation. However, it takes time polynomial in m, n which is prohibitive for some modern applications. In this article, we develop an algorithm that is qualitatively faster, provided we may sample the entries of the matrix in accordance with a natural probability distribution. In many applications, such sampling can be done efficiently. Our main result is a randomized algorithm to find the description of a matrix D* of rank at most k so that holds with probability at least 1 − δ (where |·|F is the Frobenius norm). The algorithm takes time polynomial in k,1/ϵ, log(1/δ) only and is independent of m and n. In particular, this implies that in constant time, it can be determined if a given matrix of arbitrary size has a good low-rank approximation.},
journal = {J. ACM},
month = nov,
pages = {1025–1041},
numpages = {17},
keywords = {sampling, low-rank approximation, Matrix algorithms}
}

%misc{panigrahi2023taskspecific,
%      title={Task-Specific Skill Localization in Fine-tuned Language Models}, 
%      author={Abhishek Panigrahi and Nikunj Saunshi and Haoyu Zhao and Sanjeev Arora},
%      year={2023},
%      eprint={2302.06600},
%      archivePrefix={arXiv},
%      primaryClass={cs.CL},
%      url={https://arxiv.org/abs/2302.06600}, 
%}
@inproceedings{panigrahi2023taskspecific,
  title={Task-specific skill localization in fine-tuned language models},
  author={Panigrahi, Abhishek and Saunshi, Nikunj and Zhao, Haoyu and Arora, Sanjeev},
  booktitle={International Conference on Machine Learning},
  pages={27011--27033},
  year={2023},
  organization={PMLR}
}
@article{malladi2023fine,
  title={Fine-tuning language models with just forward passes},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53038--53075},
  year={2023}
}
@article{guo2024zeroth,
  title={Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity},
  author={Guo, Wentao and Long, Jikai and Zeng, Yimeng and Liu, Zirui and Yang, Xinyu and Ran, Yide and Gardner, Jacob R and Bastani, Osbert and De Sa, Christopher and Yu, Xiaodong and others},
  journal={arXiv preprint arXiv:2406.02913},
  year={2024}
}
@article{liu2020primer,
  title={A primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications},
  author={Liu, Sijia and Chen, Pin-Yu and Kailkhura, Bhavya and Zhang, Gaoyuan and Hero III, Alfred O and Varshney, Pramod K},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={5},
  pages={43--54},
  year={2020},
  publisher={IEEE}
}
@article{nesterov2017random,
  title={Random gradient-free minimization of convex functions},
  author={Nesterov, Yurii and Spokoiny, Vladimir},
  journal={Foundations of Computational Mathematics},
  volume={17},
  number={2},
  pages={527--566},
  year={2017},
  publisher={Springer}
}
@article{duchi2015optimal,
  title={Optimal rates for zero-order convex optimization: The power of two function evaluations},
  author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J and Wibisono, Andre},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={5},
  pages={2788--2806},
  year={2015},
  publisher={IEEE}
}
@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}
@inproceedings{hedebertav3,
  title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing},
  author={He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
  booktitle={The Eleventh International Conference on Learning Representations},
  year = {2023}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@inproceedings{liu2021group,
  title={Group fisher pruning for practical network compression},
  author={Liu, Liyang and Zhang, Shilong and Kuang, Zhanghui and Zhou, Aojun and Xue, Jing-Hao and Wang, Xinjiang and Chen, Yimin and Yang, Wenming and Liao, Qingmin and Zhang, Wayne},
  booktitle={International Conference on Machine Learning},
  pages={7021--7032},
  year={2021},
  organization={PMLR}
}
@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}
@inproceedings{jaiswalcompressing,
  title={Compressing LLMs: The Truth is Rarely Pure and Never Simple},
  author={JAISWAL, AJAY KUMAR and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zhangyang and Yang, Yinfei},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}
@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}
@article{tavanaei2020embedded,
  title={Embedded encoder-decoder in convolutional networks towards explainable AI},
  author={Tavanaei, Amirhossein},
  journal={arXiv preprint arXiv:2007.06712},
  year={2020}
}
% `In the Proceedings of ICLR' is OK?
@inproceedings{wang2019glue,
  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  note={In the Proceedings of ICLR.},
  year={2019}
}
@article{warstadt2018neural,
  title={Neural Network Acceptability Judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R.},
  journal={arXiv preprint 1805.12471},
  year={2018}
}
@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},
  booktitle={Proceedings of EMNLP},
  pages={1631--1642},
  year={2013}
}
@inproceedings{dolan2005automatically,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, William B and Brockett, Chris},
  booktitle={Proceedings of the International Workshop on Paraphrasing},
  year={2005}
}
@book{agirre2007semantic,
  editor    = {Agirre, Eneko and M`arquez, Llu'{i}s and Wicentowski, Richard},
  title     = {Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)},
  month     = {June},
  year      = {2007},
  address   = {Prague, Czech Republic},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{cer-etal-2017-semeval,
    title = "{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
    author = "Cer, Daniel  and
      Diab, Mona  and
      Agirre, Eneko  and
      Lopez-Gazpio, I{\~n}igo  and
      Specia, Lucia",
    editor = "Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S17-2001",
    doi = "10.18653/v1/S17-2001",
    pages = "1--14",
    abstract = "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in \textit{all language tracks}. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the \textit{STS Benchmark} is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).",
}
@inproceedings{williams2018broad,
  author    = {Williams, Adina and Nangia, Nikita and Bowman, Samuel R.},
  title = {A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  booktitle = {Proceedings of NAACL-HLT},
  year = 2018
}
@inproceedings{rajpurkar2016squad,
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  title = {{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text},
  booktitle = {Proceedings of EMNLP},
  year = {2016},
  publisher = {Association for Computational Linguistics},
  pages = {2383--2392},
  location = {Austin, Texas},
}
@incollection{dagan2006pascal,
  title={The {PASCAL} recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment},
  pages={177--190},
  year={2006},
  publisher={Springer}
}

@article{hooker2019selective,
    title={Selective Brain Damage: Measuring the Disparate Impact of Model Pruning},
    author={Sara Hooker and Aaron Courville and Yann Dauphin and Andrea Frome},
    year={2019},
    url={https://arxiv.org/abs/1911.05248},
    eprint={1911.05248},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{bar2006second,
  title={The second {PASCAL} recognising textual entailment challenge},
  author={Bar-Haim, Roy and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
  year={2006}
}
@inproceedings{giampiccolo2007third,
  title={The third {PASCAL} recognizing textual entailment challenge},
  author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, Bill},
  booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
  pages={1--9},
  year={2007},
  organization={Association for Computational Linguistics},
}
@article{bentivogli2009fifth,
  title={The Fifth {PASCAL} Recognizing Textual Entailment Challenge},
  author={Bentivogli, Luisa and Dagan, Ido and Dang, Hoa Trang and Giampiccolo, Danilo and Magnini, Bernardo},
  booktitle={TAC},
  year={2009}
}
@inproceedings{levesque2011winograd,
  title={The {W}inograd schema challenge},
  author={Levesque, Hector J and Davis, Ernest and Morgenstern, Leora},
  booktitle={{AAAI} Spring Symposium: Logical Formalizations of Commonsense Reasoning},
  volume={46},
  pages={47},
  year={2011}
}
%krizhevsky2009learning
@article{alex2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and others},
  year={2009}
}
%deng2009imagenet
@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@misc{li_andreeto_ranzato_perona_2022, 
    title={Caltech 101}, 
    DOI={10.22002/D1.20086}, 
    abstractNote={Pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. Collected in September 2003 by Fei-Fei Li, Marco Andreetto, and Marc'Aurelio Ranzato. The size of each image is roughly 300 x 200 pixels. We have carefully clicked outlines of each object in these pictures, these are included under the 'Annotations.tar'. There is also a MATLAB script to view the annotations, 'show_annotations.m'.}, 
    publisher={CaltechDATA}, 
    author={Li, Fei-Fei and Andreeto, Marco and Ranzato, Marc'Aurelio and Perona, Pietro}, 
    year={2022}, 
    month={Apr} 
    }
@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@phdthesis{han2017efficient,
  title={Efficient methods and hardware for deep learning},
  author={Han, Song},
  year={2017},
  school={Stanford University}
}
@inproceedings{radosavovic2020designing,
  title={Designing network design spaces},
  author={Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10428--10436},
  year={2020}
}
@inproceedings{fang2023depgraph,
  title={Depgraph: Towards any structural pruning},
  author={Fang, Gongfan and Ma, Xinyin and Song, Mingli and Mi, Michael Bi and Wang, Xinchao},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16091--16101},
  year={2023}
}
@inproceedings{
ma2023llmpruner,
title={{LLM}-Pruner: On the Structural Pruning of Large Language Models},
author={Xinyin Ma and Gongfan Fang and Xinchao Wang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=J8Ajf9WfXP}
}
@inproceedings{hendrycks2019robustness,
  title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
  author={Hendrycks, Dan and Dietterich, Thomas},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2019}
}
@article{bai2021transformers,
  title={Are transformers more robust than cnns?},
  author={Bai, Yutong and Mei, Jieru and Yuille, Alan L and Xie, Cihang},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={26831--26843},
  year={2021}
}
@inproceedings{loshchilov2017sgdr,
  title={SGDR: Stochastic Gradient Descent with Warm Restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
@article{rissanen1996fisher,
  title={Fisher information and stochastic complexity},
  author={Rissanen, Jorma J},
  journal={IEEE transactions on information theory},
  volume={42},
  number={1},
  pages={40--47},
  year={1996},
  publisher={IEEE}
}
@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}
@inproceedings{frankle2018lottery,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Frankle, Jonathan and Carbin, Michael},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{joseph2020going,
  title={Going beyond classification accuracy metrics in model compression},
  author={Joseph, Vinu and Siddiqui, Shoaib Ahmed and Bhaskara, Aditya and Gopalakrishnan, Ganesh and Muralidharan, Saurav and Garland, Michael and Ahmed, Sheraz and Dengel, Andreas},
  journal={arXiv preprint arXiv:2012.01604},
  year={2020}
}
@article{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}
@inproceedings{meister-cotterell-2021-language,
    title = "Language Model Evaluation Beyond Perplexity",
    author = "Meister, Clara  and
      Cotterell, Ryan",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.414",
    doi = "10.18653/v1/2021.acl-long.414",
    pages = "5328--5339",
    abstract = "We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained. We provide a framework{--}paired with significance tests{--}for evaluating the fit of language models to these trends. We find that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present). Further, the fit to different distributions is highly-dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the type{--}token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well.",
}
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}
@inproceedings{liu2022swin,
  title={Swin transformer v2: Scaling up capacity and resolution},
  author={Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12009--12019},
  year={2022}
}
@article{hooker2019compressed,
  title={What do compressed deep neural networks forget?},
  author={Hooker, Sara and Courville, Aaron and Clark, Gregory and Dauphin, Yann and Frome, Andrea},
  journal={arXiv preprint arXiv:1911.05248},
  year={2019}
}

@article{chen2024comprehensive,
  title={Comprehensive Survey of Model Compression and Speed up for Vision Transformers},
  author={Chen, Feiyang and Luo, Ziqian and Zhou, Lisang and Pan, Xueting and Jiang, Ying},
  journal={Journal of Information, Technology and Policy},
  pages={1--12},
  year={2024}
}
@article{wang2023far,
  title={How far can camels go? exploring the state of instruction tuning on open resources},
  author={Wang, Yizhong and Ivison, Hamish and Dasigi, Pradeep and Hessel, Jack and Khot, Tushar and Chandu, Khyathi and Wadden, David and MacMillan, Kelsey and Smith, Noah A and Beltagy, Iz and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={74764--74786},
  year={2023}
}
@article{gao2021framework,
  title={A framework for few-shot language model evaluation},
  author={Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and others},
  journal={Version v0. 0.1. Sept},
  volume={10},
  pages={8--9},
  year={2021}
}
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}
@article{allenai:arc,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}
@inproceedings{clark2019boolq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2924--2936},
  year={2019}
}
@inproceedings{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year={2019},
  organization={Association for Computational Linguistics}
}
@inproceedings{OpenBookQA2018,
  title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2381--2391},
  year={2018}
}
@inproceedings{Bisk2020,
  author = {Yonatan Bisk and Rowan Zellers and
            Ronan Le Bras and Jianfeng Gao
            and Yejin Choi},
  title = {PIQA: Reasoning about Physical Commonsense in
           Natural Language},
  booktitle = {Thirty-Fourth AAAI Conference on
               Artificial Intelligence},
  year = {2020},
}
@inproceedings{sap-etal-2019-social,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454/",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473",
    abstract = "We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: {\textquotedblleft}Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?{\textquotedblright} A: {\textquotedblleft}Make sure no one else could hear{\textquotedblright}). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\ensuremath{>}}20{\%} gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA)."
}
@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@article{KingBa15,
  author    = {Kingma, Diederik and Ba, Jimmy},
  journal = {International Conference on Learning Representations (ICLR)},
  title     = {Adam: A Method for Stochastic Optimization},
  year      = {2015},
  address   = {San Diega, CA, USA},
  optmonth  = {12},
}
@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}
@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}
@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}
@article{kelley1960gradient,
  title={Gradient theory of optimal flight paths},
  author={Kelley, Henry J},
  journal={Ars Journal},
  volume={30},
  number={10},
  pages={947--954},
  year={1960}
}
@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group UK London}
}
@article{spall1992multivariate,
  title={Multivariate stochastic approximation using a simultaneous perturbation gradient approximation},
  author={Spall, James C},
  journal={IEEE transactions on automatic control},
  volume={37},
  number={3},
  pages={332--341},
  year={1992},
  publisher={IEEE}
}
@book{bienayme1853considerations,
  title={Consid{\'e}rations {\`a} l'appui de la d{\'e}couverte de Laplace sur la loi de probabilit{\'e} dans la m{\'e}thode des moindres carr{\'e}s},
  author={Bienaym{\'e}, Ir{\'e}n{\'e}e-Jules},
  year={1853},
  publisher={Imprimerie de Mallet-Bachelier}
}
@book{Chebyshev1867,
  title={Des valeurs moyennes},
  author={Tch{\'e}bychef, Pafnutii},
  year={1867},
  publisher={Journal de Math{\'e}matiques Pures et Appliqu{\'e}es 2},
  volume={12},
  pages={177–184},
}
@article{j2015variance,
  title={On variance reduction in stochastic gradient descent and its asynchronous variants},
  author={J Reddi, Sashank and Hefny, Ahmed and Sra, Suvrit and Poczos, Barnabas and Smola, Alexander J},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}