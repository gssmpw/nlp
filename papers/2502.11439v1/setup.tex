\iffalse
\begin{table*}[htbp]
\tiny
\begin{center}
\begin{tabular}{lccccccccccccc}\toprule
Model, ft setting & mem & \#param & ARC-c & ARC-e & BoolQ & HS & OBQA & PIQA & rte & SIQA & WG & Avg
%\\\cmidrule(lr){2-3}\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}\cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17} 
\\\cmidrule(lr){1-13}
Llama2(7B), LoRA, $r=64$ & 23.46GB & 159.9M(2.37\%) & \textbf{44.97} & 77.02 & 77.43 & \textbf{57.75} & 32.0 & \textbf{78.45} & 62.09 & \textbf{47.75} & 68.75 & 60.69\\
Llama2(7B), SPruFT, $r=128$ & \textbf{17.62GB} & 145.8M(2.16\%) & 43.60 & \textbf{77.26} & \textbf{77.77} & 57.47 & \textbf{32.6} & 78.07 & \textbf{64.98} & 46.67 & \textbf{69.30} & \textbf{60.86} \\\cmidrule(lr){2-13}
Llama2(7B), FA-LoRA, $r=64$ & 17.25GB & 92.8M(1.38\%) & 43.77 & \textbf{77.57} & 77.74 & \textbf{57.45} & 31.0 & 77.86 & \textbf{66.06} & \textbf{47.13} & 69.06 & 60.85\\
Llama2(7B), FA-SPruFT, $r=128$ & \textbf{15.21GB} & 78.6M(1.17\%) & \textbf{43.94} & 77.22 & \textbf{77.83} & 57.11 & \textbf{32.0} & \textbf{78.18} & 65.70 & 46.47 & \textbf{69.38} & \textbf{60.87}\\\midrule
Llama3(8B), LoRA, $r=64$ & 30.37GB & 167.8M(2.09\%) & \textbf{53.07} & \textbf{81.40} & \textbf{82.32} & \textbf{60.67} & 34.2 & \textbf{79.98} & 69.68 & \textbf{48.52} & \textbf{73.56} & \textbf{64.82}\\
Llama3(8B), SPruFT, $r=128$ & \textbf{24.49GB} & 159.4M(1.98\%) & 52.47 & 81.10 & 81.28 & 60.29 & \textbf{34.6} & 79.76 & \textbf{70.04} & 47.75 & 73.24 & 64.50 \\\cmidrule(lr){2-13}
Llama3(8B), FA-LoRA, $r=64$ & 24.55GB & 113.2M(1.41\%) & \textbf{52.47} & \textbf{81.36} & \textbf{82.23} & 60.17 & \textbf{35.0} & \textbf{79.76} & \textbf{70.04} & \textbf{48.31} & \textbf{73.56} & \textbf{64.77}\\
Llama3(8B), FA-SPruFT, $r=128$ & \textbf{22.41GB} & 92.3M(1.15\%) & 52.22 & 81.19 & 81.35 & \textbf{60.20} & 34.2 & 79.71 & 69.31 & 47.13 & 73.01 & 64.26 \\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{Fine-tuning Llama on Alpaca dataset for 5 epochs and evaluating on 9 tasks from EleutherAI LM Harness. "mem" represents the memory usage, with further details provided in Appendix~\ref{apdx:measure}. \#param is the number of trainable parameters, where the difference of \#param between the two approaches depends on the architecture of Llama, as some layers have $d_{in} \neq d_{out}$. Note that 10 million trainable parameters only account for less than 0.15GB of memory requirement. FA indicates that we freeze attention layers, but not including MLP layers followed by attention blocks. HS, OBQA, and WG represent HellaSwag, OpenBookQA, and WinoGrande datasets. More details of datasets can be found in Appendix~\ref{apdx:data}. The ablation study for different $r$ and the comparison with other LoRA variants can be found in Appendix~\ref{apdx:ablation}. All reported results are accuracies on the corresponding tasks. \textbf{Bold} indicates the best results of two approaches on the same task.} \label{tab:llm} 
\end{center}
\end{table*}
\fi

\begin{table*}[htbp]
\tiny
\begin{center}
\begin{tabular}{lccccccccccccc}\toprule
Model, ft setting & mem & \#param & ARC-c & ARC-e & BoolQ & HS & OBQA & PIQA & rte & SIQA & WG & Avg
\\\cmidrule(lr){1-13}
Llama2(7B)\\ \cmidrule(lr){1-1} 
LoRA, $r=64$ & 23.46GB & 159.9M(2.37\%) & \textbf{44.97} & 77.02 & 77.43 & 57.75 & 32.0 & \textbf{78.45} & 62.09 & 47.75 & 68.75 & 60.69\\
VeRA, $r=64$ & 22.97GB & 1.374M(0.02\%) & 43.26 & 76.43 & 77.40 & 57.26 & 31.6 & 78.02 & 62.09 & 45.85 & 68.75 & 60.07\\
DoRA, $r=64$ & 44.85GB & 161.3M(2.39\%) & 44.71 & 77.02 & 77.55 & \textbf{57.79} & 32.4 & 78.29 & 61.73 & \textbf{47.90} & 68.98 & 60.71\\
RoSA, $r=32, d=1.2\%$ & 44.69GB & 157.7M(2.34\%) & 43.86 & \textbf{77.48} & \textbf{77.86} & 57.42 & 32.2 & 77.97 & 63.90 &  47.29 & 69.06 & 60.78\\
SPruFT, $r=128$ & \textbf{17.62GB} & 145.8M(2.16\%) & 43.60 & 77.26 & 77.77 & 57.47 & \textbf{32.6} & 78.07 & \textbf{64.98} & 46.67 & \textbf{69.30} & \textbf{60.86} %\\\cmidrule(lr){2-13}
%FA-LoRA, $r=64$ & 17.25GB & 92.8M(1.38\%) & 43.77 & \textbf{77.57} & 77.74 & \textbf{57.45} & 31.0 & 77.86 & 66.06 & \textbf{47.13} & 69.06 & 60.85\\
%FA-DoRA, $r=64$ & 30.61GB & 93.6M(1.39\%) & 43.94 & 77.44 & 77.49 & 57.44 & 31.0 & 77.86 & \textbf{66.43} & 46.98 & 69.14 & 60.86\\
%FA-RoSA, $r=32, d=1.2\%$ & 38.34GB & 98.3M(1.46\%) & \textbf{44.28} & 77.02 & 77.68 & 57.22 & 31.0 & 77.97 & 64.26 & 46.32 & 69.22 & 60.55\\
%FA-SPruFT, $r=128$ & \textbf{15.21GB} & 78.6M(1.17\%) & 43.94 & 77.22 & \textbf{77.83} & 57.11 & \textbf{32.0} & \textbf{78.18} & 65.70 & 46.47 & \textbf{69.38} & \textbf{60.87}
\\\midrule
Llama3(8B)\\ \cmidrule(lr){1-1} 
LoRA, $r=64$ & 30.37GB & 167.8M(2.09\%) & 53.07 & 81.40 & 82.32 & 60.67 & 34.2 & 79.98 & 69.68 & 48.52 & 73.56 & 64.82\\
VeRA, $r=64$ & 29.49GB & 1.391M(0.02\%) & 50.26 & 80.30 & 81.41 & 60.16 & 34.4 & 79.60 & 69.31 & 46.93 & 72.77 & 63.90\\
DoRA, $r=64$ & 51.45GB & 169.1M(2.11\%) & \textbf{53.33} & \textbf{81.57} & \textbf{82.45} & \textbf{60.71} & 34.2 & \textbf{80.09} & 69.31 & \textbf{48.67} & \textbf{73.64} & \textbf{64.88}\\
RoSA, $r=32, d=1.2\%$ & 48.40GB & 167.6M(2.09\%) & 51.28 & 81.27 & 81.80 & 60.18 & 34.4 & 79.87 & 69.31 & 47.95 & 73.16 & 64.36\\
SPruFT, $r=128$ & \textbf{24.49GB} & 159.4M(1.98\%) & 52.47 & 81.10 & 81.28 & 60.29 & \textbf{34.6} & 79.76 & \textbf{70.04} & 47.75 & 73.24 & 64.50 %\\\cmidrule(lr){2-13}
%FA-LoRA, $r=64$ & 24.55GB & 113.2M(1.41\%) & 52.47 & 81.36 & 82.23 & 60.17 & \textbf{35.0} & 79.76 & 70.04 & 48.31 & \textbf{73.56} & 64.77\\
%FA-DoRA, $r=64$ & 40.62GB & 114.3M(1.42\%) & \textbf{52.56} & \textbf{81.69} & \textbf{82.26} & \textbf{60.20} & 34.4 & \textbf{79.82} & \textbf{70.40} & \textbf{48.46} & 73.40 & \textbf{64.80}\\
%FA-RoSA, $r=32, d=1.2\%$ & 42.31GB & 124.3M(1.55\%) & 52.22 & 81.19 & 82.05 & 60.11 & 34.4 & 79.76 & 69.31 & 47.70 & 73.16 & 64.43\\
%FA-SPruFT, $r=128$ & \textbf{22.41GB} & 92.3M(1.15\%) & 52.22 & 81.19 & 81.35 & \textbf{60.20} & 34.2 & 79.71 & 69.31 & 47.13 & 73.01 & 64.26 
\\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{Fine-tuning Llama on Alpaca dataset for 5 epochs and evaluating on 9 tasks from EleutherAI LM Harness. ``mem" represents the memory usage, with further details provided in Appendix~\ref{apdx:measure}. \#param is the number of trainable parameters, where the difference of \#param between the two approaches depends on the architecture of Llama, as some layers have $d_{in} \neq d_{out}$. %FA indicates that we freeze attention layers, but not including MLP layers followed by attention blocks. 
HS, OBQA, and WG represent HellaSwag, OpenBookQA, and WinoGrande datasets. %More details of datasets can be found in Appendix~\ref{apdx:data}. 
The ablation study for different $r$ can be found in Appendix~\ref{apdx:ranks}. All reported results are accuracies on the corresponding tasks. \textbf{Bold} indicates the best result on the same task. } \label{tab:llm} 
\end{center}
\end{table*}

\section{Experimental Setup}\label{sec:setup}

%(0.5 page)
%Why the chosen framework?
%Some prior approaches

%- parameter settings
%- uniform across layers vs greedy ... 
%- potential transformer-specific details

%Equations about what these methods do.. 

%(0.5 page)
%Which NN architectures are used, why?
%Number of parameters, layers, ...

%(Potential prior work on compression -- )

\subsection{Datasets} \label{subsec:dataset}
We use multiple datasets for different tasks. For image classification, we fine-tune models on the training split and evaluate it on the validation split of Tiny-ImageNet~\citep{tavanaei2020embedded}, CIFAR100~\citep{alex2009learning}, and Caltech101~\citep{li_andreeto_ranzato_perona_2022}. For text generation, we fine-tune LLMs on 256 samples from Stanford-Alpaca~\citep{alpaca} and assess zero-shot performance on nine EleutherAI LM Harness tasks~\citep{gao2021framework}. See Appendix~\ref{apdx:data} for details.

\subsection{Models and Baselines} \label{subsec:models}

We fine-tune full-precision Llama-2-7B and Llama-3-8B (float32) using our SPruFT, LoRA~\citep{hulora}, VeRA~\citep{kopiczko2024vera}, DoRA~\citep{liu2024dora}, and RoSA~\citep{nikdan2024rosa}. RoSA is chosen as the representative SFT method and is the only SFT due to the high memory demands of other SFT approaches, while full fine-tuning is excluded for the same reason. We freeze Llamaâ€™s classification layers and fine-tune only the linear layers in attention and MLP blocks.

Next, we evaluate importance metrics by fine-tuning Llamas and image models, including DeiT~\citep{touvron2021training}, ViT~\citep{dosovitskiy2020image}, ResNet101~\citep{he2016deep}, and ResNeXt101~\citep{xie2017aggregated} on CIFAR100, Caltech101, and Tiny-ImageNet. For image tasks, we set the fine-tuning ratio at 5\%, meaning the trainable parameters are a total of 5\% of the backbone plus classification layers.

\subsection{Training Details} \label{subsec:training}
Our fine-tuning framework is built on torch-pruning\footnote{Torch-pruning is not required, all their implementations are based on PyTorch.}~\citep{fang2023depgraph}, PyTorch~\citep{paszke2019pytorch}, PyTorch-Image-Models~\citep{rw2019timm}, and HuggingFace Transformers~\citep{wolf2020transformers}. Most experiments run on a single A100-80GB GPU, while DoRA and RoSA use an H100-96GB GPU. We use the Adam optimizer~\citep{KingBa15} and fine-tune all models for a fixed number of epochs without validation-based model selection.

%Structured pruning often considers parameter dependencies in importance evaluation~\citep{liu2021group, fang2023depgraph, ma2023llmpruner}. This becomes the following process in our work: first, searching for dependencies by tracing the computation graph of gradient; next, evaluating the importance of parameter groups; and finally, fine-tuning the parameters within those important groups collectively. For instance, if $\W^{a}_{\cdot j}$ and $\W^{b}_{i\cdot}$ are dependent, where $\W^{a}_{\cdot j}$ is the $j$-th column in parameter matrix (or the $j$-th input channels/features) of layer $a$ and $\W^{b}_{i\cdot}$ is the $i$-th row in parameter matrix (or the $i$-th output channels/features) of layer $b$, then $\W^{a}_{\cdot j}$ and $\W^{b}_{i\cdot}$ will be fine-tuned simultaneously while the corresponding $\M^{a}_{dep}$ for $\W^{a}_{\cdot j}$ becomes column selection matrix and $\W^a_s$ becomes $\W^a_{f,dep}\M^a_{dep}$. Consequently, fine-tuning $2.5\%$ output channels for layer $b$ will result in fine-tuning additional $2.5\%$ input channels in each dependent layer. Therefore, for the $5\%$ of desired fine-tuning ratio, the fine-tuning ratio with considering dependencies is set to $2.5\%$\footnote{In some complex models, considering dependencies results in slightly more than twice the number of trainable parameters. However, in most cases, the factor is 2.} for the approach that includes dependencies. More details for dependencies of NN can be found in Appendix~\ref{apdx:dep}. 

\textbf{Image models}: The learning rate is set to $10^{-4}$ with cosine annealing decay~\citep{loshchilov2017sgdr}, where the minimum learning rate is $10^{-9}$. All image models used in this study are pre-trained on ImageNet. 

\textbf{Llama}: For LoRA and DoRA, we set $\alpha = 16$, a dropout rate of $0.1$, and a learning rate of $10^{-4}$  with linear decay (
$0.01$ decay rate). For SPruFT, we control trainable parameters using rank instead of fine-tuning ratio for direct comparison. The learning rate is $2 \cdot 10^{-5}$ with the same decay settings. Linear decay is applied after a warmup over the first $3$\% of training steps. The maximum sequence length is $2048$, with truncation for longer inputs and padding for shorter ones.

