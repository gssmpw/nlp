%\section{Appendix}
%\appendixpage
\iffalse
\section{Additivity Property}\label{apdx:add_conv}
Our method can only be memory-efficient for the forward functions satisfying Equation~\ref{eq:SPruFT_fwd}. Similarly, LoRA has the same limitation. Fortunately, in practice, linear and 1D- and 2D-convolutional layers, which are the most widely used, satisfy the additivity property. For linear layer, this is intuitive (see Equation~\ref{eq:lora_fwd}), but for convolutional layers, this is not intuitive. In signal processing, convolutional layers utilize the `cross-correlation' operator to measure the similarity of two series, while in practical implementation, this is computed by `sliding inner-product'. Consider a convolutional forward function with all `stride', `dilation', and `groups' to be $1$ (these hyperparameters will not affect the additivity property): 
\begin{align*}
    f(\w, \x)&= \sum_{j=1}^{|\x|-|\w|} \sum_{i=1}^{|\w|} \w_i\cdot\x_{i+j}+b
    \\\implies f(\w+\w^s, \x)&= \sum_{j=1}^{|\x|-|\w|} \sum_{i=1}^{|\w|} (\w_i+\w^s_i)\cdot\x_{i+j}+b+b^s
    \\&=(\sum_{j=1}^{|\x|-|\w|} \sum_{i=1}^{|\w|} \w_i\cdot\x_{i+j}+b) + (\sum_{j=1}^{|\x|-|\w|} \sum_{i=1}^{|\w|} \w^s_i\cdot\x_{i+j}+b^s),
\end{align*}
where $\w$, $\w^s$, $b$, and $b^s$ are one individual channel's weight vector and bias of $\W$ and $\W_s$. The difference between 1D- and 2D-convolutional layers is the shape of $\w$ and $\x$, which has no impact on the additivity in the equation. For the hyperparameters, `stride' controls the sliding step (e.g. $j\in\{1,2,3,\ldots\}$ or $j\in\{1,3,5,\ldots\}$), `dilation' controls the spacing between $\x$ in $\w_i\cdot\x_{i+j}$ (e.g. $\w_1\cdot\x_1+\w_2\cdot\x_3+\ldots$), `groups' controls the connection between input and output channels~\citep{NIPS2012_c399862d}, and `padding' controls the amount of padding for controlling dimensionality.
\fi
\section{Importance Metrics}\label{apdx:imp}
\textbf{Taylor importance} is the Taylor expansion of the difference between the loss of the model with and without the target neuron:
\begin{align}
    {\bm \eta}_i&=L(\mathcal{D}, \Acute{F}_{\boldsymbol{c}_i}) - L(\mathcal{D}, F)\nonumber
    \\&\approx -\w^\top \nabla_{\w} L(\mathcal{D}, F)+ \frac{1}{2}\w^\top\nabla_{\w}^2 L(\mathcal{D}, F)\w\nonumber
    \\&\overset{(*)}{\approx} \frac{1}{2}\w^\top\nabla_{\w}^2 L(\mathcal{D}, F)\w \nonumber
    \\&\overset{(**)}{\approx} \frac{1}{2}(G\w)^\top(G\w), \nonumber
\end{align}
where $G = \nabla_{\w} L(\mathcal{D}, F)$. (**) is from the result of Fisher information~\cite{rissanen1996fisher}:
\begin{equation}
\nabla_{\w}^2 L(\mathcal{D}, F) \approx \nabla_{\w} L(\mathcal{D}, F)^\top\nabla_{\w} L(\mathcal{D}, F).\nonumber
\end{equation}
Note that (*) is from $\nabla_{\w} L(\mathcal{D}, F)\approx 0$, as removing one channel/neuron from a large neural network typically results in only a negligible reduction in loss. To efficiently compute ${\bm \eta}_i$, the equation can be further derived as:
\begin{align}
    {\bm \eta}_i\approx (G\w)^\top(G\w) = \sum_{j}(\frac{1}{|\mathcal{D}|}\sum_{\x\in\mathcal{D}}\frac{\partial L(\x, F)}{\partial w_j}w_j)^2\approx \sum_{j}|\frac{1}{|\mathcal{D}|}\sum_{\x\in\mathcal{D}}\frac{\partial L(\x, F)}{\partial w_j}w_j|,\nonumber
\end{align}
where $\w = (w_1,\ldots,w_j,\ldots)$.

\textbf{Magnitude importance} is the $\ell_2$-norm of the neuron vector computed as $\sqrt{\sum_j w^2_j}$.

\textbf{Proof of Property~\ref{SPSA_property}} 

To prove Property~\ref{SPSA_property}, we first calculate the expectation and variance of SPSA. For convenience, we denote $[\nabla L(\mathbf{\theta}, \mathcal{D})]$ as $\g$. Then, the expectation is as follows:
\begin{align}
    \E[\hat{\g} ] &\approx \E[\mathbf{z}\mathbf{z}^\top \nabla L(\mathbf{\theta}, \mathcal{D})] = \E[\mathbf{z}\mathbf{z}^\top] \nabla L(\mathbf{\theta}, \mathcal{D}) = \mathbf{I}_d \nabla L(\mathbf{\theta}, \mathcal{D}). \nonumber
\end{align}
 The variance can then be derived as follows:
\begin{align}
    \Var[\hat{\g}_i] &\approx \Var[\z_i (\mathbf{z}^\top \g)] = \E[(\z_i (\mathbf{z}^\top \g))^2] - \E[\z_i (\mathbf{z}^\top \g)]^2 = \E[\z_i^2(\sum_{i=1}^d \z_i \g_i)^2] - \E[\z_i (\sum_{l=1}^d \z_l \g_l)]^2 \nonumber
    \\&=\E[\z_i^2(\sum_{i=1}^d \z_i \g_i)^2] - \E[\z_i^2\g_i +(\z_i\sum_{l\neq i, l \in [d]} \z_l\g_l)]^2 \nonumber
    \\&=\E[\z_i^2(\sum_{i=1}^d \z_i \g_i)^2] - \left(\underbrace{\E[\z_i^2]}_{1}\g_i +\sum_{l\neq i, l \in [d]} \underbrace{\E[\z_i\z_l]}_{0}\g_l\right)^2 = \E[\z_i^2(\sum_{i=1}^d \z_i \g_i)^2] - \g_i^2\nonumber
    \\&=\E[\z_i^2(\z_i\g_i + \sum_{l\neq i, l \in [d]} \z_l \g_l)^2] - \g_i^2 \nonumber
    \\&=\E[\z_i^2(\z_i^2\g_i^2 + 2\z_i\g_i(\sum_{l\neq i, l \in [d]} \z_l \g_l) +(\sum_{l\neq i, l \in [d]} \z_l \g_l)^2] - \g_i^2 \nonumber
    \\&=\E[\z_i^4\g_i^2 + 2\z_i^3\g_i(\sum_{l\neq i, l \in [d]} \z_l \g_l) +\z_i^2(\sum_{l\neq i, l \in [d]} \z_l \g_l)^2] - \g_i^2 \nonumber
    \\&=\E[\z_i^4\g_i^2] + 2\E[\z_i^3\g_i]\E[\sum_{l\neq i, l \in [d]} \z_l \g_l] +\E[\z_i^2]\E[(\sum_{l\neq i, l \in [d]} \z_l \g_l)^2] - \g_i^2 \nonumber
    \\&=\g_i^2\underbrace{\E[\z_i^4]}_{3} + 2\g_i\E[\z_i^3]\sum_{l\neq i, l \in [d]}\g_l\underbrace{\E[\z_l]}_{0} +\underbrace{\E[\z_i^2]}_{1}(\sum_{l\neq i, l \in [d]} \g_l^2 \underbrace{\E[\z_l^2]}_{1} + \sum_{k\neq l\neq i, k,l \in [d]}\g_k\g_l\underbrace{\E[\z_k\z_l]}_{0}) - \g_i^2 \nonumber
    \\&=3\g_i^2 + (\sum_{l\neq i, l \in [d]} \g_l^2)  - \g_i^2 = \g_i^2 + \sum_{l=1}^d \g_l^2 \nonumber
\end{align}

$\E[\z_l] = 0$, $\E[\z_i^2] = \Var[\z_i] = 1$, and $\E[\z_k\z_l]_{l\neq k} = 0$ are because $\z\sim \mathcal{N}(0, \mathbf{I}_d)$. $\E[\z_i^4]=3$ is obtained from the moment generating function of standard normal: 
\begin{align}
    \E[\z_i^4]& = \frac{d^4}{dt^4}(e^{\frac{t^2}{2}})\big |_{t=0} = (3e^{\frac{t^2}{2}}+6t^2e^{\frac{t^2}{2}} +t^4e^{\frac{t^2}{2}})\big |_{t=0}=3. \nonumber
\end{align}

Thus, the expectation and variance of $n$-SPSA estimate are 
\begin{align}
    \E[\frac{\sum_{j=1}^n\hat{\g}_i^{(j)}}{n} ] &= \frac{\sum_{j=1}^n\E[\hat{\g}_i^{(j)}]}{n} = \frac{\sum_{j=1}^n\g_i}{n} = \g_i. \nonumber
    \\ \Var[\frac{\sum_{j=1}^n\hat{\g}_i^{(j)}}{n} ] &= \frac{\sum_{j=1}^n\Var[\hat{\g}_i^{(j)}]}{n^2} = \frac{\Var[\hat{\g}_i]}{n} = \frac{\g_i^2 + \sum_{l=1}^d \g_l^2}{n}. \nonumber
\end{align}

Then, we are going to prove the consistency of $n$-SPSA estimate. Given any small $\epsilon>0$, we can derive the following inequality by Chebyshev's Inequality~\citep{bienayme1853considerations, Chebyshev1867}:
\begin{align}
    \Pr[|\frac{\sum_{j=1}^n\hat{\g}^{(j)}}{n}-\g|>\epsilon] \leq \frac{\Var[\frac{\sum_{j=1}^n\hat{\g}^{(j)}}{n}]}{\epsilon^2}=\frac{\g_i^2 + \sum_{l=1}^d \g_l^2}{\epsilon^2n^2} \nonumber
\end{align}
which converges to $0$ as $n\rightarrow \infty$. 
\section{Parameter Dependency}\label{apdx:dep}
Dependencies of parameters between neurons or channels across different layers exist in NNs. These include basic layer connections, residual connections, tensor concatenations, summations, and more, as shown in Figure~\ref{fig:dependency}. The black neurons connected by real lines represent the dependent parameters that are in the same group. Pruning any black neurons results in removing the parameters connected by the real lines.~\cite{liu2021group} introduced a group pruning method for CNN models that treats residual connections as grouped dependencies, evaluating and pruning related channels within the same group simultaneously. Similarly,~\cite{fang2023depgraph} proposed a novel group pruning technique named Torch-Pruning, which considers various types of dependencies and achieves state-of-the-art results.~\cite{ma2023llmpruner} further applied this procedure to pruning LLMs. Torch-Pruning can be applied to prune a wide range of neural networks, including image transformers, LLMs, CNNs, and more, making it a popular toolkit for neural network pruning.
\begin{figure}[htbp]
\begin{center}
%\vspace{-0.2cm}
\includegraphics[width=\linewidth]{figures/dependency.pdf}
%\vspace{-0.8cm}
\caption{Common dependencies of parameters in neural networks.}\label{fig:dependency} %\vspace{-0.4cm}
\end{center}
\end{figure}
\unskip

In this study, we also evaluate the influences of incorporating parameter dependency in our approach. We put the experimental results of whether incorporating parameter dependency in Appendix~\ref{apdx:dep_exp}. In the experiments, parameter dependency becomes the following process for our approach: first, searching for dependencies by tracing the computation graph of gradient; next, evaluating the importance of parameter groups; and finally, fine-tuning the parameters within those important groups collectively. For instance, if $\W^{a}_{\cdot j}$ and $\W^{b}_{i\cdot}$ are dependent, where $\W^{a}_{\cdot j}$ is the $j$-th column in parameter matrix (or the $j$-th input channels/features) of layer $a$ and $\W^{b}_{i\cdot}$ is the $i$-th row in parameter matrix (or the $i$-th output channels/features) of layer $b$, then $\W^{a}_{\cdot j}$ and $\W^{b}_{i\cdot}$ will be fine-tuned simultaneously while the corresponding $\M^{a}_{dep}$ for $\W^{a}_{\cdot j}$ becomes column selection matrix and $\W^a_s$ becomes $\W^a_{f,dep}\M^a_{dep}$. Consequently, fine-tuning $2.5\%$ output channels for layer $b$ will result in fine-tuning additional $2.5\%$ input channels in each dependent layer. Therefore, for the $5\%$ of desired fine-tuning ratio, the fine-tuning ratio with considering dependencies is set to $2.5\%$\footnote{In some complex models, considering dependencies results in slightly more than twice the number of trainable parameters. However, in most cases, the factor is 2.} for the approach that includes dependencies. 

%Section~\ref{subsec:training} has described how the parameter dependency works in our approach, we explain it in detail here. Using the same example $\W^{a}_{\cdot j}$ and $\W^{b}_{i\cdot}$. Constructing the fine-tuning parameters $\W^{b}_f$ leads to constructing the fine-tuning parameters $\W^{a}_{f,dep}$ with the corresponding $\M^{a}_{dep}$ becoming a column selection matrix and the forward function of layer $a$ becoming the following equation.

The forward function of layer $a$ for column selection mentioned above can be written as the following equation:
\begin{align*}
    f(\hat{\W}^a,\x)&= f(\W^a,\x) + f(\M^a\W^a_f,\x) + f(\W^a_{f,dep}\M^a_{dep},\x). 
\end{align*}
Note that in this example, as the dependency is connection between the output feature/channel of $b$ and the input feature/channel of $a$, the dimension $d^a_{in}$ is equal to $d^b_{out}$ where $\W^a\in \R^{d^a_{out}\times d^a_{in}}, \W^b\in \R^{d^b_{out}\times d^b_{in}}$. 

\section{Ablation Studies and Related Analysis}\label{apdx:ablation}
In this section, we first discuss the hyperparameter settings. While we do not include DeBERTaV3~\citep{hedebertav3} in the main context, we fine-tune DeBERTaV3-base~\citep{hedebertav3} on GLUE. The learning rate is set to $2 \cdot 10^{-5}$ with linear decay, where the decay rate is $0.01$. The model is fine-tuned on the full training split of 8 tasks from the GLUE benchmark. The maximum sequence length is fixed to 256 with longer sequences truncated and shorter sequences padded. Note that memory efficiency is not emphasized for small-scale models, as dataset-related memory—particularly with large batch sizes—dominates consumption in these cases. The main advantage of our method in these cases is the reduced FLOPs due to fewer trainable parameters.

Following this, we discuss the computational resource requirements for fine-tuning. Figure~\ref{fig:backprop} illustrates the computation and cache requirements during backpropagation. Next, we provide an ablation study on the impact of different rank settings for our approach and LoRA, as shown in Table~\ref{tab:llm_ablation}. Finally, Table~\ref{tab:llm_ablation_FA} demonstrates the advantages of freezing self-attention blocks to reduce memory usage while maintaining performance.

\begin{table}[htbp]
\tiny
\begin{center}
\begin{tabular}{p{4mm}p{9.5mm}p{9.5mm}p{9.5mm}p{9.5mm}p{9.5mm}p{9.5mm}p{9.5mm}p{9.5mm}p{9.5mm}}\toprule
 & \multicolumn{3}{c}{CIFAR100} & \multicolumn{3}{c}{Tiny-ImageNet} & \multicolumn{3}{c}{Caltech101} \\\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
& Full & Head & SPruFT
& Full & Head & SPruFT 
& Full & Head & SPruFT 
\\\cmidrule(lr){2-2} \cmidrule(lr){3-3}\cmidrule(lr){4-4} \cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}

 \#ep & loss, acc & loss, acc& loss, acc 
 & loss, acc & loss, acc& loss, acc 
 & loss, acc & loss, acc& loss, acc\\\midrule%\cmidrule(lr){1-1}\cmidrule(lr){2-4} \cmidrule(lr){5-7}\cmidrule(lr){8-10}
 & \multicolumn{3}{c}{DeiT} & \multicolumn{3}{c}{DeiT} & \multicolumn{3}{c}{DeiT} \\\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
\#param: & 86.0M & 0.2M & 4.6M
    & 86.1M & 0.3M & 4.8M
    & 86.0M & 0.2M & 4.6M
    \\\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4} \cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}    \cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}
  5 & \textbf{0.36}, \textbf{90.18}  & 0.76, 80.25 & \textbf{0.37}, \textbf{88.70}
        & \textbf{0.54}, \textbf{87.55} & 0.60, 85.09 & \textbf{0.40}, \textbf{89.69} & 
        0.11, 97.33 & 1.09, 89.02 & 0.30, 95.41\\
 10 & 0.44, 90.04 & 0.64, 81.83 & 0.42, 88.62 
    & 0.69, 86.32 & 0.54, 85.72 & 0.49, 88.96 
    & \textbf{0.11}, \textbf{97.55} & 0.53, 93.22 & 0.17, 96.28\\
 30 & 0.62, 89.03 & \textbf{0.55}, \textbf{83.42} & 0.64, 88.61 
    & 0.94, 84.27 & \textbf{0.52}, \textbf{86.06} & 0.72, 88.67 
    & 0.11, 97.11 & \textbf{0.22}, \textbf{95.06} & \textbf{0.12}, \textbf{96.50}\\\midrule%\cmidrule(lr){1-1}\cmidrule(lr){2-4} \cmidrule(lr){5-7}\cmidrule(lr){8-10}
 & \multicolumn{3}{c}{ViT} & \multicolumn{3}{c}{ViT} & \multicolumn{3}{c}{ViT} \\\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
 \#param: & 85.9M & 0.1M & 4.5M 
    & 86.0M& 0.2M & 4.6M
    & 85.9M & 0.1M & 45.2M\\\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4} \cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8} \cmidrule(lr){9-9}\cmidrule(lr){10-10}
  5 & \textbf{0.38}, \textbf{90.13} & 1.01, 74.78 & \textbf{0.40}, \textbf{88.13} 
    & \textbf{0.51}, \textbf{88.45} & 0.65, 84.10 & \textbf{0.36}, \textbf{90.87}
    & 0.12, 97.16 & 1.60, 85.70 & 0.43, 93.96\\
 10 & 0.45, 89.85 & 0.85, 77.05 & 0.45, 87.55 
    & 0.66, 86.78 & 0.58, 84.95 & 0.44, 90.48 
    & \textbf{0.11}, 97.20 & 0.85, 89.98 & 0.23, 95.54\\
 30 & 0.62, 88.78 & \textbf{0.71}, \textbf{79.51} & 0.69, 87.83  
    & 0.96, 84.20 & \textbf{0.55}, \textbf{85.49} &0.61, 90.56
    & 0.12, \textbf{97.24} & \textbf{0.33}, \textbf{92.65} & \textbf{0.16}, \textbf{96.02} \\\midrule%\cmidrule(lr){1-1}\cmidrule(lr){2-4} \cmidrule(lr){5-7}\cmidrule(lr){8-10}
& \multicolumn{3}{c}{ResNet101} & \multicolumn{3}{c}{ResNet101} & \multicolumn{3}{c}{ResNet101} \\\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
\#param: & 42.7M & 0.2M & 2.2M
    & 42.9M & 0.4M & 2.4M
    & 42.7M & 0.2M & 2.2M
    \\\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4} \cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8} \cmidrule(lr){9-9}\cmidrule(lr){10-10}
  5 & \textbf{0.50}, 86.21 & 1.62, 60.78 & \textbf{0.59}, 82.36 
    & \textbf{0.92}, \textbf{77.78} & 1.64, 62.06 & \textbf{0.76}, \textbf{79.66}
    & 0.14, 96.50 & 1.25, 82.33 & 0.48, 92.56\\
 10 & 0.58, \textbf{86.41} & 1.39, 63.06 & 0.60, 82.33 
    & 1.10, 76.81 & 1.50, 63.19 & 0.79, 79.54
    & \textbf{0.14}, \textbf{96.54} & 0.69, 90.24 & 0.23, 95.58\\
 30 & 0.80, 84.72 & \textbf{1.21}, \textbf{65.63} & 0.80, \textbf{82.49} 
    & 1.54, 74.09 & \textbf{1.43}, \textbf{64.47} & 1.08, 78.58
    & 0.18, 95.80 & \textbf{0.31}, \textbf{93.00} & \textbf{0.16}, \textbf{95.89}\\\midrule%\cmidrule(lr){1-1}\cmidrule(lr){2-4} \cmidrule(lr){5-7}\cmidrule(lr){8-10}
& \multicolumn{3}{c}{ResNeXt101} & \multicolumn{3}{c}{ResNeXt101} & \multicolumn{3}{c}{ResNeXt101} \\\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
 \#param: & 87.0M & 0.2M & 4.9M 
    & 87.2M& 0.4M& 5.1M 
    & 87.0M & 0.2M & 4.9M
    \\\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4} \cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8} \cmidrule(lr){9-9}\cmidrule(lr){10-10}
 5 & \textbf{0.47}, \textbf{87.30} & 1.42, 65.07 & \textbf{0.47}, 85.94 
    & \textbf{0.86}, \textbf{79.51} & 1.46, 65.59 & \textbf{0.61}, \textbf{83.88}
    & \textbf{0.12}, \textbf{97.07} & 1.25, 83.16 & 0.28, 95.84\\
 10 & 0.56, 87.17 & 1.23, 67.55 & 0.53, 86.04 
    & 1.01, 79.27  & 1.35, 66.73 & 0.69, 83.47 
    & 0.13, 96.89 & 0.68, 90.94 & 0.18, 96.28\\
 30 & 0.71, 86.59 & \textbf{1.08}, \textbf{69.45} & 0.69, \textbf{86.33 }
    & 1.41, 76.55 & \textbf{1.29}, \textbf{67.93} & 0.90, 82.83
    & 0.16, 96.63 & \textbf{0.31}, \textbf{92.87} & \textbf{0.14}, \textbf{96.76}
\\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{Fine-tuning on CIFAR100 and Tiny-ImageNet. \#ep and \#param represent the number of epochs and the number of trainable parameters, where SPruFT is our method with Taylor importance. Full and Head indicate full fine-tuning and head-finetuning, which only fine-tunes the classification layer. All reported losses and accuracies are based on validation results. \textbf{Bold} denotes the best results of each fine-tuning approach (in the same column) on the same model and dataset. } \label{tab:imagemodels} 
\end{center}
\end{table}

\subsection{Hyperparameter Settings} \label{apdx:results_diff_ft}

We report the results of three approaches over several epochs as table~\ref{tab:imagemodels} and table~\ref{tab:lmclassification}. Overall, full fine-tuning over higher epochs is more prone to overfitting, while head fine-tuning shows the exact opposite trend. Except for the results on caltech101\footnote{The inconsistent trend observed in Caltech101 results is likely due to its significantly smaller sample size.}, the loss patterns across all models consistently reflect this trend, and most accuracy results further support this conclusion. However, our approach demonstrates a crucial advantage by effectively balancing the tradeoff between performance and computational resources. %Fine-tuning only an additional 5\% of backbone parameters compared to head fine-tuning leads to remarkable improvements, with results that are comparable to or even better than full fine-tuning in just a few epochs.

Table~\ref{tab:imagemodels} clearly shows that both our approach and full fine-tuning achieve optimal results within a few epochs, while head fine-tuning requires more training. Notably, all models have been pre-trained on ImageNet-1k, which may explain the strong performance observed with head fine-tuning on Tiny-ImageNet. However, even with this advantage, full fine-tuning still outperforms head fine-tuning, and our approach surpasses both. In just 5 epochs, our approach achieves results comparable to full fine-tuning on all datasets with significantly lower trainable parameters.

\begin{table*}[htbp]
\tiny
\begin{center}
\begin{tabular}{lccccccccccccccccl}\toprule
& & task & \multicolumn{1}{c}{CoLA} & \multicolumn{1}{c}{MNLI} & \multicolumn{1}{c}{MRPC} & \multicolumn{1}{c}{QNLI} & \multicolumn{1}{c}{QQP} & \multicolumn{1}{c}{RTE} & \multicolumn{1}{c}{SST-2}& \multicolumn{1}{c}{STS-B}
%\\\cmidrule(lr){2-3}\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}\cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17} 
\\\cmidrule(lr){4-11}
& & \#train & 8.5k & 393k & 3.7k & 108k & 364k & 2.5k & 67k & 7k \\\cmidrule(lr){4-11}
method&\#param&epochs& mcc & acc& acc& acc& acc& acc& acc& corr \\\cmidrule(lr){1-11}
Full& 184.42M &3 & $\textbf{69.96}$ & $\textbf{89.42}$ & 89.71 & $\textbf{93.57}$ & $\textbf{92.08}$ & 80.14 & $\textbf{95.53}$ & 90.44 \\
Full& &5 & 69.48 & 89.29 & 87.74 & 93.36 & 92.08 & $\textbf{83.39}$ & 94.72 & 90.14 \\
Full& &10 & 68.98 & 88.55 & $\textbf{90.20}$ & 93.15 & 91.97 & 80.51 & 93.81 & $\textbf{90.71}$ \\\midrule
Head & 592.13K & 3 & 24.04 & 62.64 & \textbf{68.38} & 70.73 & 80.18 & \textbf{52.71} & 65.48 & 5.66 \\
Head & & 5 & 45.39 & 61.75 & \textbf{68.38} & \textbf{72.32} & 80.59 & 47.29 & \textbf{78.44} & 26.88 \\
Head & & 10 & \textbf{47.32} & \textbf{63.98} & \textbf{68.38} & 71.99 & \textbf{80.96} & 47.29 & 74.66 & \textbf{49.59} \\\midrule
SPruFT& 103.57M & 3 & 64.08 & 89.58 & 81.62 & 93.10 & 90.70 & 70.40 & 95.18 & 86.58\\
SPruFT& & 5 & 65.40 & \textbf{90.21} & 86.03 & \textbf{93.17} & 90.93 & 74.37 & 95.30 & 87.36\\
SPruFT& & 10 & \textbf{65.56} & 89.55 & \textbf{87.50} & 93.15 & \textbf{91.57} & \textbf{80.14} & \textbf{95.41} & \textbf{89.14} \\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{Fine-tuning DeBERTaV3 on GLUE. `mcc', `acc', and `corr' represent `Matthews correlation', `accuracy', and `Pearson correlation', respectively. \#param is the number of trainable parameters. SPruFT is our method with Taylor importance, while Full and Head indicate full fine-tuning and head-finetuning, which only fine-tunes the classification layer. All reported metrics are based on validation results, and are percentages. \textbf{Bold} denotes the best results of each fine-tuning approach on the same task. } \label{tab:lmclassification} 
\end{center}
\end{table*}

In contrast to Table~\ref{tab:imagemodels}, the results in Table~\ref{tab:lmclassification} show more variation. Although the validation loss follows a similar trend, we report only the evaluation metrics due to the different patterns observed in these metrics. One potential reason for this variation is the varying amounts of training data across the GLUE tasks. As shown in the table, tasks with fewer samples often require more epochs to achieve better performance for both full fine-tuning and our approach. Conversely, for tasks with large amounts of training data such as `MNLI', `QNLI', `QQP', and `SST-2', the results show tiny improvement from 3 to 10 epochs. Nevertheless, the results still demonstrate that our approach significantly balances the tradeoff between performance and computational resources. Our method achieves near full fine-tuning performance with remarkably less trainable parameters. 

We refer from Table~\ref{tab:lmclassification} for epochs choosing of fine-tuning Llama2 and Llama3. Table~\ref{tab:lmclassification} shows that 5 or 10 epochs are reasonable for most tasks using our approach. Given that the maximum sequence lengths of Llama are longer than DeBERTaV3, we have opted for only 5 epochs in the main experiments to balance computational resources and performance.

\subsection{Considering Dependency}\label{apdx:dep_exp}
We evaluate our approach with and without considering parameter dependency, as shown in Table~\ref{tab:img_dep_taylor} and Table~\ref{tab:lm_dep_taylor}.

\begin{table}[htbp]
\tiny
\begin{center}
\begin{tabular}{l|c|ccc|ccc|cccccl}\toprule
& data & \multicolumn{3}{c}{CIFAR100} & \multicolumn{3}{c}{Tiny-ImageNet} & \multicolumn{2}{c}{Caltech101} \\\cmidrule(lr){3-5}\cmidrule(lr){6-8} \cmidrule(lr){9-10} 
model & dep & $\ell^2$ & Taylor & QMTaylor  
 & $\ell^2$ & Taylor & QMTaylor & $\ell^2$ & Taylor \\\midrule
DeiT & \XSolidBrush & \textbf{88.05} & \textbf{88.70} & \underline{\textbf{89.37}} & \textbf{89.31} & \textbf{89.69} & \underline{\textbf{89.75}} & \textbf{95.01} & \underline{\textbf{95.41}}\\
& \Checkmark & 86.43 & 87.33 & 88.08 & 85.56 & 85.92 & 86.49 & 65.35 & 78.04\\\midrule
ViT & \XSolidBrush & \textbf{87.13} & \textbf{88.06} & \underline{\textbf{88.51}} & \textbf{90.78} & \textbf{90.87} & \underline{\textbf{90.90}} & \textbf{92.69} & \underline{\textbf{93.96}} \\
& \Checkmark & 85.24 & 86.83 & 87.91 & 88.83 & 88.95 & 89.67 & 56.30 & 77.82\\\midrule
RN & \XSolidBrush & \textbf{82.25} & \textbf{82.36} & \underline{\textbf{83.50}} & \textbf{79.83} & \textbf{79.66} & \underline{\textbf{80.02}} & \underline{\textbf{93.13}} & \textbf{92.56}\\
& \Checkmark & 78.63 & 78.62 & 81.18 & 69.87 & 69.24 & 72.51 & 54.68 & 52.71 \\\midrule
RNX & \XSolidBrush & \textbf{86.12} & \textbf{85.94} & \underline{\textbf{86.93}} & \textbf{83.88} & \textbf{83.88} & \underline{\textbf{84.17}} & \textbf{95.71} & \underline{\textbf{95.84}}\\
& \Checkmark & 84.71 & 85.01 & 85.48 & 79.39 & 78.95 & 79.54 & 92.13 & 91.82\\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{Fine-tuning image models by our SPruFT for 5 epochs. ``dep'' refers to whether parameter dependencies are involved or not. $\ell^2$, Taylor, and QMTaylor represent the magnitude, Taylor importance, and Quantiles-Mean Taylor importance (\Eqref{eq:qmtaylor_mean}). Note that QMTaylor is not applied to fine-tuning Caltech101 due to its significantly imbalanced labels. All reported results are validation accuracies. \textbf{Bold} indicates the superior results achieved through dependency searching compared to not searching. \underline{Underline} highlights the best fine-tuning results.} \label{tab:img_dep_taylor} 
\end{center}
\end{table}

We utilize various importance metrics to fine-tune both models using our approach, with and without incorporating parameter dependencies, and report the results to compare their performances. Searching for dependencies in structured pruning is natural, as dependent parameters are pruned together. However, important neurons in a given layer do not always have dependent neurons that are also important in their respective layers. As demonstrated in Table~\ref{tab:img_dep_taylor}, fine-tuning without considering parameter dependencies outperforms fine-tuning incorporating dependencies in all cases. For importance metrics, although the differences between them are not substantial, all results consistently conclude that the Quantile-Mean Taylor importance demonstrates a slight improvement over the standard Taylor importance. Furthermore, both the Quantile-Mean Taylor and standard Taylor metrics outperform the magnitude importance.

\begin{table}[htbp]
\tiny
\begin{center}
\begin{tabular}{l|c|cccccccccccccccl}\toprule
& task & \multicolumn{1}{c}{CoLA} & \multicolumn{1}{c}{MNLI} & \multicolumn{1}{c}{MRPC} & \multicolumn{1}{c}{QNLI} & \multicolumn{1}{c}{QQP} & \multicolumn{1}{c}{RTE} & \multicolumn{1}{c}{SST-2}& \multicolumn{1}{c}{STS-B}
%\\\cmidrule(lr){2-3}\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}\cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17} 
\\\cmidrule(lr){3-10}
imp & dep & mcc & acc& acc& acc& acc& acc& acc& corr \\\cmidrule(lr){1-10}
Taylor&\XSolidBrush & 65.56 & 89.55 & \textbf{87.50} & 93.15 & \textbf{91.57} & \textbf{80.14} & \textbf{95.41} & 89.14 \\
&\Checkmark& \textbf{67.49} & \textbf{89.85} & 87.25 & \textbf{93.30} & 91.63 & 79.42 & 95.07 & \textbf{89.98}\\\midrule
$\ell^2$ &\XSolidBrush & 65.40 & 89.77 & 83.33 & 92.64 & 91.34 & 74.73 & 94.04 & \textbf{88.69} \\
&\Checkmark& \textbf{66.80} & \textbf{90.22} & \textbf{84.07} & \textbf{93.94} & \textbf{91.57} & \textbf{79.06} & \textbf{95.07} & 87.39\\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{Fine-tuning DeBERTaV3 on GLUE by our SPSFT for 10 epochs. ``dep'' refers to whether parameter dependencies are involved or not. Taylor and $\ell^2$ indicate the magnitude and Taylor importance. The importance score is Taylor. We do not apply QMTaylor since the number of labels is tiny. `mcc', `acc', and `corr' represent `Matthews correlation', `accuracy', and `Pearson correlation', respectively. All reported metrics are based on validation results. \textbf{Bold} indicates the best results of whether considering dependencies.} \label{tab:lm_dep_taylor} 
\end{center}
\end{table}

Table~\ref{tab:lm_dep_taylor} suggests a slightly different conclusion: the impact of parameter dependencies on performance is minor, nearly negligible\footnote{The results of using magnitude importance on the RTE task show significant variation, but this is likely due to the small sample size and the hardness of the task, which result in the unstable performances observed in our experiments. Aside from RTE, the results on other tasks are not significantly different.}. However, searching for dependencies involves additional implementations and computational overhead. Combining the results of image models, the conclusion is not searching for the parameter dependencies. For importance metrics, this experiment shows that magnitude and Taylor importance perform similarly.

\subsection{Memory Measurement}\label{apdx:measure}
In this study, we detail the memory measurement methodology employed. The total memory requirements can be categorized into three main components: \[ \text{mem}_{\text{TTL}} = \text{mem}_{\text{M}} + \text{mem}_{\text{FT}} + \text{mem}_{\text{Aux}},\]
where: 
\begin{enumerate}
    \item $\text{mem}_{\text{TTL}}$ is the total memory consumed during training.
    \item $\text{mem}_{\text{M}}$ represents the memory consumed by the base model itself.
    \item $\text{mem}_{\text{FT}}$ corresponds to the memory required for the fine-tuning parameters and their gradients.
    \item $\text{mem}_{\text{Aux}}$ accounts for any additional memory usage, including optimizer states, caching, and other intermediate computations.
\end{enumerate}
We yield $\text{mem}_{\text{M}}$ by measuring the memory usage during inference on the training data using the pre-trained model. The combined memory usage of $\text{mem}_{\text{FT}}$ and $\text{mem}_{\text{Aux}}$ is calculated as the difference between $\text{mem}_{\text{TTL}}$ and $\text{mem}_{\text{Model}}$. For simplicity, we consistently report $\text{mem}_{\text{FT}} + \text{mem}_{\text{Aux}}$ as ``mem'' in all comparisons presented in this study.

\begin{table}[htbp]
\tiny
\begin{center}
\begin{tabular}{l|cccc|cccc|cccccl}\toprule
 & \multicolumn{4}{|c|}{Llama2(7B)} &  \multicolumn{4}{|c|}{Llama3(8B)} \\\cmidrule(lr){2-5}\cmidrule(lr){6-9} 
FT setting & \#param & $\text{mem}_{\text{TTL}}$ & $\text{mem}_{\text{M}}$ & mem & \#param & $\text{mem}_{\text{TTL}}$ & $\text{mem}_{\text{M}}$ & mem \\\midrule
LoRA, $r=64$ & 159.9M(2.37\%) & 53.33GB & 29.87GB & 23.46GB
    & 167.8M(2.09\%) & 64.23GB & 33.86GB & 30.37GB\\
RoSA, $r=32, d=1.2\%$ & 157.7M(2.34\%) & 74.56GB & 29.87GB & 44.69GB
    & 167.6M(2.09\%) & 82.26GB & 33.86GB & 48.40GB\\
DoRA, $r=64$ & 161.3M(2.39\%) & 74.72GB & 29.87GB & 44.85GB
    & 169.1M(2.11\%) & 85.31GB & 33.86GB & 51.45GB\\
SPruFT, $r=128$ & 145.8M(2.16\%) & \textbf{47.49GB} & 29.87GB & \textbf{17.62GB}
    & 159.4M(1.98\%) & \textbf{58.35GB} & 33.86GB & \textbf{24.49GB}\\\midrule
FA-LoRA, $r=64$ & 92.8M(1.38\%) & 47.12GB & 29.87GB & 17.25GB
    & 113.2M(1.41\%) & 58.41GB & 33.86GB & 30.37GB\\
FA-RoSA, $r=32, d=1.2\%$ & 98.3M(1.46\%) & 68.21GB & 29.87GB & 38.34GB
    & 124.3M(1.55\%) & 76.17GB & 33.86GB & 42.31GB\\
FA-DoRA, $r=64$ & 93.6M(1.39\%) & 60.48GB & 29.87GB & 30.61GB
    & 114.3M(1.42\%) & 74.48GB & 33.86GB & 40.62GB\\
FA-SPruFT, $r=128$ & 78.6M(1.17\%) & \textbf{45.08GB} & 29.87GB & \textbf{15.21GB}
    & 92.3M(1.15\%) & \textbf{56.27GB} & 33.86GB & \textbf{22.41GB}\\\bottomrule

\end{tabular}
%\vspace{-0.2cm}
\caption{The requirements of computation resources for fine-tuning. `mem' traces $\text{mem}_{\text{TTL}}-\text{mem}_{\text{M}}$. All fine-tuning parameters are stored in full precision. We also examined the training time and observed that DoRA requires 50\% to 100\% more time than other methods, while LoRA, RoSA, and our approach need similar training time (differing only by a few seconds). However, due to the influence of various factors on training time and the difficulty of ensuring a fair comparison, we chose not to include these results in our report.} \label{tab:resource} 
\end{center}
\end{table}

\begin{figure}[htbp]
\begin{center}
%\vspace{-0.2cm}
\includegraphics[width=0.35\linewidth]{figures/DoRA.pdf}
%\vspace{-0.8cm}
\caption{The illustration of DoRA's computation graph. Black operations occur during the forward pass, while orange operations take place during the backward pass. }\label{fig:DoRA_graph} %\vspace{-0.4cm}
\end{center}
\end{figure}
\unskip

\subsection{Resource Requirements}\label{apdx:mem_require}
Table~\ref{tab:resource} presents the resource requirements of various PEFT methods. We compare our approach with LoRA and several of its variants that maintain or surpass LoRA's performance. As shown, our method is the most resource-efficient among these approaches. The subsequent ablation study further demonstrates that our approach achieves performance comparable to LoRA. We exclude comparisons with VeRA~\citep{kopiczko2024vera}, which proposes sharing a single pair of random low-rank matrices across all layers to save memory footprint. While VeRA achieves some memory savings, its performance often deteriorates.

We note that while our approach offers significant memory efficiency, this benefit is less pronounced in small-scale models, where the primary memory consumption arises from the dataset—especially with large batch sizes. The main advantage of our method in these cases is the reduced FLOPs due to fewer trainable parameters. Therefore, we do not highlight memory efficiency in small-scale model scenarios.

In Section~\ref{sec:memory_trainable_parameters}, we explain that the memory usage of DoRA is significantly higher than that of LoRA due to its complex computation. We demonstrate the computation graph of DoRA here, as shown in Figure~\ref{fig:DoRA_graph}. DoRA decomposes $\W$ into magnitude $\mathbf{m}$ and direction $\mathbf{V}$ and computes the final parameters matrix by $\W' =\mathbf{m}\frac{\mathbf{V}+\Delta \mathbf{V}}{||\mathbf{V}+\Delta \mathbf{V}||_c}$. This complicated computation significantly increases memory usage because it requires caching a lot of intermediate values for computing gradients of $\B$, $\A$, and $\mathbf{m}$. As illustrated in Figure~\ref{fig:DoRA_graph}, each node passed by backpropagation stores some intermediate values for efficient gradient computing.

\begin{figure}[htbp]
\begin{center}
%\vspace{-0.2cm}
\includegraphics[width=\linewidth]{figures/backprop.pdf}
%\vspace{-0.8cm}
\caption{The illustration of backpropagation highlights the operations involved. Black operations occur during the forward pass, while orange operations take place during the backward pass. Blue operations highlight the benefits of our approach. Notably, since $\M$ is non-trainable, caching $\W_f\x$ during the forward pass is unnecessary, leading to significant memory savings. Additionally, in practice, PyTorch caches $\frac{\partial L}{\partial \h_{\text{right}}}$ to efficiently compute $\frac{\partial L}{\partial \B}$, although this caching is not strictly required for backpropagation. }\label{fig:backprop} %\vspace{-0.4cm}
\end{center}
\end{figure}
\unskip
\subsection{Cache Benefit}\label{apdx:cache}
In the main context, we have already shown the memory cost of dropout layers in LoRA, in this section, we will discuss some other benefits of our approach. Figure~\ref{fig:backprop} illustrates the computation and cache requirements in backpropagation~\citep{rumelhart1986learning}. For simplicity, we replace the notation $f(\cdot, \cdot)$ with different $\h$. With the same number of trainable parameters, our approach eliminates the need to cache $\h = \W_f\x$ shown in the figure. While this benefit is negligible under lower rank settings ($r$) or when the number of fine-tuning layers is small, it becomes significant as the model size and rank settings increase. Although the caching requirement for $\h$ can be addressed by recomputing $\h = \A\x$ during backpropagation, this would result in increased time complexity during training. 

\begin{table}[htbp]
\tiny
\begin{tabular}{lccccccccccccc}\toprule
Model, ft setting & mem & \#param & ARC-c & ARC-e & BoolQ & HS & OBQA & PIQA & rte & SIQA & WG & Avg
%\\\cmidrule(lr){2-3}\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}\cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17} 
\\\cmidrule(lr){1-13}
Llama2(7B), LoRA, $r=16$ & 21.64GB & 40.0M(0.59\%) & \textbf{44.71} & 76.89 & 77.49 & \textbf{57.94} & \textbf{32.2} & \textbf{78.73} & 60.65 & \textbf{47.59} & 68.75 & \textbf{60.55}\\
Llama2(7B), SPruFT, $r=32$ & \textbf{15.57GB} & 36.4M(0.54\%) & 43.34 & \textbf{77.43} & \textbf{77.80} & 57.06 & \textbf{32.2} & 78.02 & \textbf{63.18} & 46.52 & \textbf{69.14} & 60.52 \\\cmidrule(lr){2-13}
Llama2(7B), LoRA, $r=32$ & 22.21GB & 80.0M(1.19\%) & \textbf{44.28} & 76.89 & 77.37 & \textbf{57.61} & 32.0 & \textbf{78.45} & \textbf{64.62} & \textbf{47.90} & 69.14 & \textbf{60.92}\\
Llama2(7B), SPruFT, $r=64$ & \textbf{16.20GB} & 72.9M(1.08\%) & 43.69 & \textbf{77.30} & \textbf{77.83} & 57.19 & \textbf{32.2} & 78.07 & 63.54 & 47.03 & \textbf{69.22} & 60.67\\\cmidrule(lr){2-13}
Llama2(7B), LoRA, $r=64$ & 23.46GB & 159.9M(2.37\%) & \textbf{44.97} & 77.02 & 77.43 & \textbf{57.75} & 32.0 & \textbf{78.45} & 62.09 & \textbf{47.75} & 68.75 & 60.69\\
Llama2(7B), SPruFT, $r=128$ & \textbf{17.62GB} & 145.8M(2.16\%) & 43.60 & \textbf{77.26} & \textbf{77.77} & 57.47 & \textbf{32.6} & 78.07 & \textbf{64.98} & 46.67 & \textbf{69.30} & \textbf{60.86} \\\cmidrule(lr){1-13}
Llama2(7B), FA-LoRA, $r=16$ & 16.29GB & 23.2M(0.34\%) & \textbf{44.54} & \textbf{77.36} & \textbf{77.83} & \textbf{57.39} & 30.8 & 77.69 & \textbf{67.15} & \textbf{47.08} & 68.82 & \textbf{60.96}\\
Llama2(7B), FA-SPruFT, $r=32$ & \textbf{14.16GB} & 19.7M(0.29\%) & 43.69 & 77.30 & 77.55 & 57.14 & \textbf{31.6} & \textbf{78.02} & 64.62 & 46.88 & \textbf{69.38} & 60.69 \\\cmidrule(lr){2-13}
Llama2(7B), FA-LoRA, $r=32$ & 16.56GB & 46.4M(0.69\%) & \textbf{44.03} & \textbf{77.48} & 77.61 & \textbf{57.40} & 30.4 & 77.80 & \textbf{65.70} & \textbf{46.98} & 68.98 & 60.71\\
Llama2(7B), FA-SPruFT, $r=64$ & \textbf{14.48GB} & 39.3M(0.58\%) & 43.77 & 77.26 & \textbf{77.65} & 57.17 & \textbf{31.8} & \textbf{78.07} & \textbf{65.70} & 46.78 & \textbf{69.22} & \textbf{60.82}  \\\cmidrule(lr){2-13}
Llama2(7B), FA-LoRA, $r=64$ & 17.25GB & 92.8M(1.38\%) & 43.77 & \textbf{77.57} & 77.74 & \textbf{57.45} & 31.0 & 77.86 & \textbf{66.06} & \textbf{47.13} & 69.06 & 60.85\\
Llama2(7B), FA-SPruFT, $r=128$ & \textbf{15.21GB} & 78.6M(1.17\%) & \textbf{43.94} & 77.22 & \textbf{77.83} & 57.11 & \textbf{32.0} & \textbf{78.18} & 65.70 & 46.47 & \textbf{69.38} & \textbf{60.87}\\\midrule
Llama3(8B), LoRA, $r=16$ & 28.86GB & 41.9M(0.52\%) & \textbf{53.50} & \textbf{81.44} & \textbf{82.35} & \textbf{60.61} & 34.2 & \textbf{79.87} & 69.31 & \textbf{48.00} & \textbf{73.56} & \textbf{64.76}\\
Llama3(8B), SPruFT, $r=32$ & \textbf{22.62GB} & 39.8M(0.50\%) & 51.19 & 80.81 & 81.10 & 60.21 & \textbf{34.4} & 79.60 & \textbf{70.40} & 47.19 & 73.01 & 64.21  \\\cmidrule(lr){2-13}
Llama3(8B), LoRA, $r=32$ & 29.37GB & 83.9M(1.04\%) & \textbf{53.33} & \textbf{81.86} & \textbf{82.20} & \textbf{60.65} & 34.0 & \textbf{79.87} & 68.23 & \textbf{49.03} & \textbf{73.72} & \textbf{64.76}\\
Llama3(8B), SPruFT, $r=64$ & \textbf{23.23GB} & 79.7M(0.99\%) & 51.96 & 81.27 & 81.31 & 60.18 & \textbf{35.2} & 79.76 & \textbf{70.04} &47.80 & 73.09 & 64.51 \\\cmidrule(lr){2-13}
Llama3(8B), LoRA, $r=64$ & 30.37GB & 167.8M(2.09\%) & \textbf{53.07} & \textbf{81.40} & \textbf{82.32} & \textbf{60.67} & 34.2 & \textbf{79.98} & \textbf{69.68} & \textbf{48.52} & \textbf{73.56} & \textbf{64.82}\\
Llama3(8B), SPruFT, $r=128$ & \textbf{24.49GB} & 159.4M(1.98\%) & 52.47 & 81.10 & 81.28 & 60.29 & \textbf{34.6} & 79.76 & 70.04 & 47.75 & 73.24 & 64.50 \\\cmidrule(lr){1-13}
Llama3(8B), FA-LoRA, $r=16$ & 23.54GB & 28.3M(0.35\%) & \textbf{51.45} & \textbf{81.48} & \textbf{82.17} & 60.17 & \textbf{34.4} & \textbf{79.60} & 68.95 & \textbf{47.75} & \textbf{73.48} & \textbf{64.38}\\
Llama3(8B), FA-SPruFT, $r=32$ & \textbf{21.24GB} & 23.1M(0.29\%) & 50.60 & 80.85 & 81.19 & \textbf{60.22} & 34.2 & 79.54 & \textbf{70.76} & 47.24 & 73.01 & 64.18  \\\cmidrule(lr){2-13}
Llama3(8B), FA-LoRA, $r=32$ & 23.89GB & 56.6M(0.71\%) & \textbf{52.22} & \textbf{81.61} & \textbf{82.35} & \textbf{60.26} & \textbf{35.0} & \textbf{79.98} & \textbf{69.68} & \textbf{48.41} & \textbf{73.80} & \textbf{64.81}\\
Llama3(8B), FA-SPruFT, $r=64$ & \textbf{21.62GB} & 46.1M(0.57\%) & 51.79 & 81.06 & 81.22 & 60.20 & 35.2 & 79.65 & \textbf{71.48} & 47.24 & 73.16 & 64.56 \\\cmidrule(lr){2-13}
Llama3(8B), FA-LoRA, $r=64$ & 24.55GB & 113.2M(1.41\%) & \textbf{52.47} & \textbf{81.36} & \textbf{82.23} & 60.17 & \textbf{35.0} & \textbf{79.76} & \textbf{70.04} & \textbf{48.31} & \textbf{73.56} & \textbf{64.77}\\
Llama3(8B), FA-SPruFT, $r=128$ & \textbf{22.41GB} & 92.3M(1.15\%) & 52.22 & 81.19 & 81.35 & \textbf{60.20} & 34.2 & 79.71 & 69.31 & 47.13 & 73.01 & 64.26\\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{Similar to Table~\ref{tab:llm}, this is the full table with different rank settings for our SPruFT and LoRA. We also present the results of freezing-attention blocks in this table. } \label{tab:llm_ablation} 
\end{table}


\begin{table}[htbp]
\tiny
\begin{center}
\begin{tabular}{lccccccccccccc}\toprule
Model, ft setting & ARC-c & ARC-e & BoolQ & HS & OBQA & PIQA & rte & SIQA & WG & Avg
\\\cmidrule(lr){1-11}
Llama2(7B), SPruFT\\ \cmidrule(lr){1-1} 
$r=32$, random  & 43.34 & 76.43 & \textbf{77.83} & 57.18 & 31.4 & 78.13 & 62.45 & 45.80 & \textbf{69.30} & 60.21\\
$r=32$, $\ell^2$  & 43.34 & 77.43 & 77.80 & 57.06 & \textbf{32.2} & 78.02 & 63.18 & 46.52 & 69.14 & 60.52 \\
$r=32$, ZOTaylor & \textbf{43.43} & \textbf{77.44} & 77.49 & \textbf{57.79} & 32.0 & \textbf{78.35} & \textbf{64.26} & \textbf{46.67} & 68.90 & \textbf{60.70} \\\cmidrule(lr){2-11}
$r=64$, random & 43.17 & 76.39 & \textbf{77.86} & 57.20 & 31.2 & 78.07 & 62.45 & 45.91 & 69.06 & 60.15\\
$r=64$, $\ell^2$  & 43.69 & \textbf{77.30} & 77.83 & 57.19 & \textbf{32.2} & 78.07 & 63.54 & \textbf{47.03} & \textbf{69.22} & 60.67 \\
$r=64$, ZOTaylor & \textbf{44.28} & 76.89 & 77.49 & \textbf{57.85} & 32.0 & \textbf{78.67} & \textbf{65.34} & 46.72 & 68.98 & \textbf{60.92} \\\cmidrule(lr){2-11}
$r=128$, random & 43.34 & 76.30 & \textbf{77.80} & 57.22 & 31.2 &78.07 & 63.18 & 45.96 & 69.22 & 60.25\\
$r=128$, $\ell^2$ & 43.60 & \textbf{77.26} & 77.77 & 57.47 & \textbf{32.6} & 78.07 & 64.98 & \textbf{46.67} & \textbf{69.30} & 60.86 \\
$r=128$, ZOTaylor & \textbf{44.11} & 77.15 & 77.43 & \textbf{57.71} & 32.4 & \textbf{78.35} & \textbf{65.70} & \textbf{46.67} & 68.90 & \textbf{60.94} \\\cmidrule(lr){2-11}
$r=32$, FA, random  & 43.26 & 76.39 & 77.28 & 57.14 & 31.4 & \textbf{78.07} & 62.82 & 46.06 & 69.30 & 60.19\\
$r=32$, FA, $\ell^2$  & 43.69 & 77.30 & \textbf{77.55} & 57.14 & 31.6 & 78.02 & 64.62 & \textbf{46.88} & \textbf{69.38} & 60.69 \\
$r=32$, FA, ZOTaylor & \textbf{44.20} & \textbf{77.61} & 77.22 & \textbf{57.41} & \textbf{31.8} & \textbf{78.07} & \textbf{65.70} & 46.72 & 68.90 & \textbf{60.85} \\\cmidrule(lr){2-11}
$r=64$, FA, random  & 43.34 & 76.30 & 77.52 & 57.18 & 31.4 & \textbf{78.07} & 62.82 & 46.01 & 69.14 & 60.20\\
$r=64$, FA, $\ell^2$  & 43.77 & 77.26 & 77.65 & 57.17 & 31.8 & \textbf{78.07} & 65.70 & \textbf{46.78} & \textbf{69.22} & 60.82 \\
$r=64$, FA, ZOTaylor & \textbf{43.94} & \textbf{77.44} & \textbf{77.71} & \textbf{57.43} & \textbf{32.6} & 77.91 & \textbf{66.06} & 46.42 & 68.75 & \textbf{60.92} \\\cmidrule(lr){2-11}
$r=128$, FA, random  & 43.34 & 76.43 & 77.46 & 57.19 & 31.4 & 78.02 & 64.26 & 46.01 & 69.06 & 60.35\\
$r=128$, FA, $\ell^2$ & 43.94 & 77.22 & \textbf{77.83} & 57.11 & \textbf{32.0} & \textbf{78.18} & 65.70 & 46.47 & \textbf{69.38} & 60.87\\
$r=128$, FA, ZOTaylor & \textbf{44.45} & \textbf{77.82} & 77.74 & \textbf{57.41} & 31.4 & 77.80 & \textbf{66.06} & \textbf{46.57} & 68.59 & \textbf{60.87} \\\midrule
Llama3(8B), SPruFT\\ \cmidrule(lr){1-1} 
$r=32$, random  & 50.51 & 80.26 & 81.59 & 60.20 & 34.6 & 79.54 & 68.95 & 47.08 & 72.53 & 63.92\\
$r=32$, $\ell^2$  & \textbf{51.19} & 80.81 & 81.10 & \textbf{60.21} & 34.4 & \textbf{79.60} & \textbf{70.40} & \textbf{47.19} & 73.01 & 64.21 \\
$r=32$, ZOTaylor & 50.94 & \textbf{81.61} & \textbf{82.42} & 60.17 & \textbf{35.6} & \textbf{79.60} & \textbf{70.40} & \textbf{47.19} & \textbf{73.40} & \textbf{64.59} \\\cmidrule(lr){2-11}
$r=64$, random  & 50.51 & 80.09 & 81.44 & \textbf{60.22} & 34.6 & 79.60 & 69.31 & 47.24 & 72.69 & 63.97\\
$r=64$, $\ell^2$  & 51.96 & \textbf{81.27} & 81.31 & 60.18 & 35.2 & 79.76 & \textbf{70.04} & \textbf{47.80} & \textbf{73.09} & 64.51 \\
$r=64$, ZOTaylor & \textbf{52.13} & 81.14 & \textbf{82.32} & 60.19 & \textbf{35.6} & \textbf{79.82} & \textbf{70.04} & 47.03 & 73.01 & \textbf{64.59} \\\cmidrule(lr){2-11}
$r=128$, random  & 50.43 & 80.13 & 81.47 & 60.19 & \textbf{34.6} & 79.60 & 69.68 & 47.24 & 72.69 & 64.00\\
$r=128$, $\ell^2$ & \textbf{52.47} & 81.10 & 81.28 & \textbf{60.29} & \textbf{34.6} & \textbf{79.76} & \textbf{70.04} & \textbf{47.75} & \textbf{73.24} & 64.50 \\
$r=128$, ZOTaylor & 52.22 & \textbf{81.99} & \textbf{82.54} & 60.13 & \textbf{34.6} & \textbf{79.76} & \textbf{70.04} & 47.59 & 73.16 & \textbf{64.67} \\\cmidrule(lr){2-11}
$r=32$, FA, random  & 50.43 & 80.05 & 81.47 & 60.19 & 34.6 & 79.65 & 69.68 & 47.03 & 72.85 & 63.99\\
$r=32$, FA, $\ell^2$  & 50.60 & 80.85 & 81.19 & \textbf{60.22} & 34.2 & 79.54 & \textbf{70.76} & \textbf{47.24} & 73.01 & 64.18 \\
$r=32$, FA, ZOTaylor & \textbf{51.02} & \textbf{81.31} & \textbf{82.05} & 60.21 & \textbf{35.2} & \textbf{79.71} & 70.40 & 47.03 & \textbf{73.24} & \textbf{64.46} \\\cmidrule(lr){2-11}
$r=64$, FA, random  & 50.26 & 80.13 & 81.53 & 60.20 & 34.4 & 79.54 & 68.95 & 47.03 & 72.93 & 63.89\\
$r=64$, FA, $\ell^2$  & \textbf{51.79} & 81.06 & 81.22 & 60.20 & \textbf{35.2} & \textbf{79.65} & \textbf{71.48} & 47.24 & \textbf{73.16} & \textbf{64.56} \\
$r=64$, FA, ZOTaylor & 51.71 & \textbf{81.31} & \textbf{82.05} & \textbf{60.22} & 34.8 & \textbf{79.65} & 69.68 & \textbf{47.29} & 72.77 & 64.39 \\\cmidrule(lr){2-11}
$r=128$, FA, random  & 50.26 & 80.09 & 81.47 & \textbf{60.22} & \textbf{34.6} & \textbf{79.71} & 69.31 & 47.08 & 72.85 & 63.95\\
$r=128$, FA, $\ell^2$ & \textbf{52.22} & 81.19 & 81.35 & 60.20 & 34.2 & \textbf{79.71} & 69.31 & \textbf{47.13} & \textbf{73.01} & 64.26 \\
$r=128$, FA, ZOTaylor & \textbf{52.22} & \textbf{81.23} & \textbf{82.17} & 60.19 & 34.4 & 79.43 & \textbf{69.68} & \textbf{47.13} & \textbf{73.01} & \textbf{64.38} \\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{Similar to Table~\ref{tab:llm_imp}, this is the full table with different rank settings. We also present the results of freezing-attention blocks in this table.} \label{tab:llm_imp_ablation} 
\end{center}
\end{table}

\subsection{Rank Settings}\label{apdx:ranks}
We present an ablation study of rank settings here. Table~\ref{tab:llm_ablation} demonstrates that $r=16$ is sufficient for LoRA when fine-tuning Llama-2 and Llama-3. In contrast, increasing $r$ for our approach yields slight performance improvements. The most remarkable observation in Table~\ref{tab:llm_ablation} is the exceptional memory efficiency of our approach: even with $r=128$, the memory usage of our method is significantly lower than that of LoRA with $r=16$. 

Table~\ref{tab:llm_imp_ablation} is the full table of importance evaluation for Llama2 and Llama3.

\subsection{Benefit of Freezing Attention Blocks}\label{apdx:FA}
\begin{table}[htbp]
\tiny
\begin{center}
\begin{tabular}{lccccccccccccc}\toprule
Model, ft setting & mem & \#param & ARC-c & ARC-e & BoolQ & HS & OBQA & PIQA & rte & SIQA & WG & Avg
%\\\cmidrule(lr){2-3}\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}\cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17} 
\\\cmidrule(lr){1-13}
Llama2(7B), LoRA, $r=16$ & 21.64GB & 40.0M(0.59\%) & \textbf{44.71} & 76.89 & 77.49 & \textbf{57.94} & \textbf{32.2} & \textbf{78.73} & 60.65 & \textbf{47.59} & 68.75 & 60.55\\
Llama2(7B), FA-LoRA, $r=32$ & \textbf{16.56GB} & 46.4M(0.69\%) & 44.03 & \textbf{77.48} & \textbf{77.61} & 57.40 & 30.4 & 77.80 & \textbf{65.70} & 46.98 & \textbf{68.98} & \textbf{60.71}\\\cmidrule(lr){2-13}
Llama2(7B), LoRA, $r=32$ & 22.21GB & 80.0M(1.19\%) & \textbf{44.28} & 76.89 & 77.37 & \textbf{57.61} & \textbf{32.0} & \textbf{78.45} & 64.62 & \textbf{47.90} & \textbf{69.14} & \textbf{60.92}\\
Llama2(7B), FA-LoRA, $r=64$ & \textbf{17.25GB} & 92.8M(1.38\%) & 43.77 & \textbf{77.57} & \textbf{77.74} & 57.45 & 31.0 & 77.86 & \textbf{66.06} & 47.13 & 69.06 & 60.85\\\cmidrule(lr){2-13}
Llama2(7B), SPruFT, $r=32$ & 15.57GB & 36.4M(0.54\%) & 43.34 & \textbf{77.43} & \textbf{77.80} & 57.06 & \textbf{32.2} & 78.02 & 63.18 & 46.52 & 69.14 & 60.52 \\
Llama2(7B), FA-SPruFT, $r=64$ & \textbf{14.48GB} & 39.3M(0.58\%) & \textbf{43.77} & 77.26 & 77.65 & \textbf{57.17} & 31.8 & \textbf{78.07} & \textbf{65.70} & \textbf{46.78} & \textbf{69.22} & \textbf{60.82}
\\\cmidrule(lr){2-13}
Llama2(7B), SPruFT, $r=64$ & 16.20GB & 72.9M(1.08\%) & 43.69 & \textbf{77.30} & \textbf{77.83} & \textbf{57.19} & \textbf{32.2} & 78.07 & 63.54 & \textbf{47.03} & 69.22 & 60.67\\
Llama2(7B), FA-SPruFT, $r=128$ & \textbf{15.21GB} & 78.6M(1.17\%) & \textbf{43.94} & 77.22 & \textbf{77.83} & 57.11 & 32.0 & \textbf{78.18} & \textbf{65.70} & 46.47 & \textbf{69.38} & \textbf{60.87}\\\midrule
Llama3(8B), LoRA, $r=16$ & 28.86GB & 41.9M(0.52\%) & \textbf{53.50} & 81.44 & \textbf{82.35} & \textbf{60.61} & 34.2 & 79.87 & 69.31 & 48.00 & 73.56 & 64.76\\
Llama3(8B), FA-LoRA, $r=32$ & \textbf{23.89GB} & 56.6M(0.71\%) & 52.22 & \textbf{81.61} & \textbf{82.35} & 60.26 & \textbf{35.0} & \textbf{79.98} & \textbf{69.68} & \textbf{48.41} & \textbf{73.80} & \textbf{64.81}\\\cmidrule(lr){2-13}
Llama3(8B), LoRA, $r=32$ & 29.37GB & 83.9M(1.04\%) & \textbf{53.33} & \textbf{81.86} & 82.20 & \textbf{60.65} & 34.0 & \textbf{79.87} & 68.23 & \textbf{49.03} & \textbf{73.72} & 64.76\\
Llama3(8B), FA-LoRA, $r=64$ & \textbf{24.55GB} & 113.2M(1.41\%) & 52.47 & 81.36 & \textbf{82.23} & 60.17 & \textbf{35.0} & 79.76 & \textbf{70.04} & 48.31 & 73.56 & \textbf{64.77}\\\cmidrule(lr){2-13}
Llama3(8B), SPruFT, $r=32$ & 22.62GB & 39.8M(0.50\%) & 51.19 & 80.81 & 81.10 & \textbf{60.21} & 34.4 & 79.60 & 70.40 & 47.19 & 73.01 & 64.21  \\
Llama3(8B), FA-SPruFT, $r=64$ & \textbf{21.62GB} & 46.1M(0.57\%) & \textbf{51.79} & \textbf{81.06} & \textbf{81.22} & 60.20 & 35.2 & \textbf{79.65} & \textbf{71.48} & \textbf{47.24} & \textbf{73.16} & \textbf{64.56} \\\cmidrule(lr){2-13}
Llama3(8B), SPruFT, $r=64$ & 23.23GB & 79.7M(0.99\%) & 51.96 & \textbf{81.27} & 81.31 & 60.18 & \textbf{35.2} & \textbf{79.76} & \textbf{70.04} & \textbf{47.80} & \textbf{73.09} & \textbf{64.51} \\
Llama3(8B), FA-SPruFT, $r=128$ & \textbf{22.41GB} & 92.3M(1.15\%) & \textbf{52.22} & 81.19 & \textbf{81.35} & \textbf{60.20} & 34.2 & 79.71 & 69.31 & 47.13 & 73.01 & 64.26\\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{ Same results of Table~\ref{tab:llm_ablation} with a reordering of the rows. This table is for comparing \emph{fine-tuning all linear layers} with \emph{fine-tuning only the MLP layers}. } \label{tab:llm_ablation_FA} 
\end{center}
\end{table}

We now assess different fine-tuning strategies. Table~\ref{tab:llm_ablation_FA} highlights the importance of selecting fine-tuning layers strategically to minimize redundant memory usage. Freezing the self-attention blocks achieves performance comparable to fine-tuning all layers while significantly reducing memory consumption during training. This efficiency stems from reducing the need to cache intermediate outputs for gradient computation. For example, as illustrated in Figure~\ref{fig:backprop}, using LoRA, $\nabla_{out}$ must be cached to compute $\frac{\partial L}{\partial A}$ for the subsequent layer. Freezing the next layer eliminates this caching requirement, further optimizing memory usage.

\iffalse
\begin{table}[htbp]
\tiny
\begin{center}
\begin{tabular}{lccccccccccccc}\toprule
Model, ft setting & mem & \#param & ARC-c & ARC-e & BoolQ & HS & OBQA & PIQA & rte & SIQA & WG & Avg
\\\cmidrule(lr){1-13}
Llama2(7B)\\ \cmidrule(lr){1-1} 
LoRA, $r=64$ & 23.46GB & 159.9M(2.37\%) & \textbf{44.97} & 77.02 & 77.43 & 57.75 & 32.0 & \textbf{78.45} & 62.09 & 47.75 & 68.75 & 60.69\\
DoRA, $r=64$ & 44.85GB & 161.3M(2.39\%) & 44.71 & 77.02 & 77.55 & \textbf{57.79} & 32.4 & 78.29 & 61.73 & \textbf{47.90} & 68.98 & 60.71\\
RoSA, $r=32, d=1.2\%$ & 44.69GB & 157.7M(2.34\%) & 43.86 & \textbf{77.48} & \textbf{77.86} & 57.42 & 32.2 & 77.97 & 63.90 &  47.29 & 69.06 & 60.78\\
SPruFT, $r=128$ & \textbf{17.62GB} & 145.8M(2.16\%) & 43.60 & 77.26 & 77.77 & 57.47 & \textbf{32.6} & 78.07 & \textbf{64.98} & 46.67 & \textbf{69.30} & \textbf{60.86} \\\cmidrule(lr){2-13}
FA-LoRA, $r=64$ & 17.25GB & 92.8M(1.38\%) & 43.77 & \textbf{77.57} & 77.74 & \textbf{57.45} & 31.0 & 77.86 & 66.06 & \textbf{47.13} & 69.06 & 60.85\\
FA-DoRA, $r=64$ & 30.61GB & 93.6M(1.39\%) & 43.94 & 77.44 & 77.49 & 57.44 & 31.0 & 77.86 & \textbf{66.43} & 46.98 & 69.14 & 60.86\\
FA-RoSA, $r=32, d=1.2\%$ & 38.34GB & 98.3M(1.46\%) & \textbf{44.28} & 77.02 & 77.68 & 57.22 & 31.0 & 77.97 & 64.26 & 46.32 & 69.22 & 60.55\\
FA-SPruFT, $r=128$ & \textbf{15.21GB} & 78.6M(1.17\%) & 43.94 & 77.22 & \textbf{77.83} & 57.11 & \textbf{32.0} & \textbf{78.18} & 65.70 & 46.47 & \textbf{69.38} & \textbf{60.87}\\\midrule
Llama3(8B)\\ \cmidrule(lr){1-1} 
LoRA, $r=64$ & 30.37GB & 167.8M(2.09\%) & 53.07 & 81.40 & 82.32 & 60.67 & 34.2 & 79.98 & 69.68 & 48.52 & 73.56 & 64.82\\
DoRA, $r=64$ & 51.45GB & 169.1M(2.11\%) & \textbf{53.33} & \textbf{81.57} & \textbf{82.45} & \textbf{60.71} & 34.2 & \textbf{80.09} & 69.31 & \textbf{48.67} & \textbf{73.64} & \textbf{64.88}\\
RoSA, $r=32, d=1.2\%$ & 42.31GB & 167.6M(2.09\%) & 51.28 & 81.27 & 81.80 & 60.18 & 34.4 & 79.87 & 69.31 & 47.95 & 73.16 & 64.36\\
SPruFT, $r=128$ & \textbf{24.49GB} & 159.4M(1.98\%) & 52.47 & 81.10 & 81.28 & 60.29 & \textbf{34.6} & 79.76 & \textbf{70.04} & 47.75 & 73.24 & 64.50 \\\cmidrule(lr){2-13}
FA-LoRA, $r=64$ & 24.55GB & 113.2M(1.41\%) & 52.47 & 81.36 & 82.23 & 60.17 & \textbf{35.0} & 79.76 & 70.04 & 48.31 & \textbf{73.56} & 64.77\\
FA-DoRA, $r=64$ & 40.62GB & 114.3M(1.42\%) & \textbf{52.56} & \textbf{81.69} & \textbf{82.26} & \textbf{60.20} & 34.4 & \textbf{79.82} & \textbf{70.40} & \textbf{48.46} & 73.40 & \textbf{64.80}\\
FA-RoSA, $r=32, d=1.2\%$ & 42.31GB & 124.3M(1.55\%) & 52.22 & 81.19 & 82.05 & 60.11 & 34.4 & 79.76 & 69.31 & 47.70 & 73.16 & 64.43\\
FA-SPruFT, $r=128$ & \textbf{22.41GB} & 92.3M(1.15\%) & 52.22 & 81.19 & 81.35 & \textbf{60.20} & 34.2 & 79.71 & 69.31 & 47.13 & 73.01 & 64.26 \\\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\caption{Comparison with LoRA, RoSA, and DoRA. } \label{tab:other_peft} 
\end{center}
\end{table}
\subsection{Additional Comparisons}\label{apdx:additional}
The results in Table~\ref{tab:other_peft} compare our approach with other PEFT methods. While the accuracies of these approaches are similar, there are significant differences in memory efficiency. Our approach consistently achieves the best memory savings, demonstrating its advantage in resource-constrained fine-tuning scenarios.
\fi


\section{Details of Datasets}\label{apdx:data}

\subsection{Vision Benchmarks}
\textbf{CIFAR100}: CIFAR100~\citep{alex2009learning} has 100 classes with 600 images of size 32x32 per class, while the CIFAR10 has 10 classes with 6000 images per class. In this study, we use the CIFAR100 downloaded from huggingface (\url{https://huggingface.co/datasets/uoft-cs/cifar100}) with 500 training images and 100 validation images per class. In our experiments, we resize the images to 256x256, crop the center to 224x224, and normalize them using the CIFAR mean $(0.507, 0.487, 0.441)$ and standard deviation $(0.267, 0.256, 0.276)$ for the three channels.

\textbf{Tiny-ImageNet}: Tiny-ImageNet~\citep{tavanaei2020embedded} has 200 classes with images of size 64x64, while the full ImageNet-1k~\citep{imagenet} has all 1000 classes where each image is the standard size 224x224. In this study, we use the Tiny-ImageNet downloaded from huggingface (\url{https://huggingface.co/datasets/zh-plus/tiny-imagenet}) with 500 training images and 50 validation images per class. In our experiments, we resize the images to 256x256, crop the center to 224x224, and normalize them using the mean $(0.485, 0.456, 0.406)$ and standard deviation $(0.229, 0.224, 0.225)$ for the three channels.

\textbf{caltech101}: Caltech101~\citep{li_andreeto_ranzato_perona_2022} consists of 101 classes, with images of varying sizes typically having edge lengths between 200 and 300 pixels. Each class contains approximately 40 to 800 images, resulting in a total of around 9,000 images. In this study, we use the Caltech101 dataset provided by PyTorch (\url{https://pytorch.org/vision/main/generated/torchvision.datasets.Caltech101.html}), allocating 75\% of the images for training and the remaining 25\% for validation. In our experiments, we preprocess the images by resizing them to 256×256, cropping the center to 224×224, and normalizing them using the mean $(0.485, 0.456, 0.406)$ and standard deviation $(0.229, 0.224, 0.225)$ for the three channels.

\subsection{General Language Understanding Evaluation Benchmark (GLUE)}
\textbf{CoLA}: The Corpus of Linguistic Acceptability (CoLA) is a dataset for assessing linguistic acceptability~\citep{warstadt2018neural}. This task is a binary classification for predicting whether a sentence is grammatically acceptable. The dataset is primarily from books and journal articles on linguistic theory.

\textbf{MNLI}: The Multi-Genre Natural Language Inference (MultiNLI) is a dataset designed to evaluate a model's ability to perform natural language inference (NLI). The task is to predict whether the premise entails the hypothesis, contradicts the hypothesis, or neither. The data set contains 433k sentence pairs annotated with textual entailment information~\citep{williams2018broad}.

\textbf{MRPC}: The Microsoft Research Paraphrase Corpus~\citep{dolan2005automatically} is a dataset designed for evaluating paraphrase detection systems. It consists of sentence pairs, with binary labels of whether the two sentences in the pair are equivalent. The data are automatically extracted from online news and labeled by humans.

\textbf{QNLI}: The Stanford Question Answering Dataset (SQuAD) is a dataset designed for machine comprehension of text~\citep{rajpurkar2016squad}. The dataset consists of question-paragraph pairs, where one of the sentences in the paragraph contains the answer to the corresponding question. The paragraphs are from Wikipedia and the questions are written by human annotators.

\textbf{QQP}: The Quora Question Pairs (QQP) dataset is a dataset of question pairs ({\url{https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs}). The task is to determine whether two questions are semantically equivalent.

\textbf{RTE}: The Recognizing Textual Entailment (RTE) datasets are a series of challenges that evaluate models' ability to determine whether a premise can entail a given hypothesis~\citep{dagan2006pascal, bar2006second, giampiccolo2007third, bentivogli2009fifth}. The data are constructed based on the texts from Wikipedia and news. The datasets have been used to evaluate the performance of both traditional language models and the state-of-the-art LLMs.
 
\textbf{SST-2}: The Stanford Sentiment Treebank is a dataset of sentences extracted from movie reviews~\citep{socher2013recursive}. Each sentence is labeled as either positive or negative. The task is to predict whether the sentence is positive or negative. 

\textbf{STS-B}: The Semantic Textual Similarity Benchmark (STSB) is a dataset with sentence pairs collected from news headlines, video and image captions, and natural language inference data~\citep{cer-etal-2017-semeval}. The task is to predict the semantic similarity between pairs of sentences. Each pair of sentences is annotated with a similarity score ranging from 0 to 5, where 0 indicates no semantic similarity and 5 indicates semantically equivalent. 

\subsection{Text-Generation Datasets}
\textbf{Stanford Alpaca}: Alpaca is an instruction dataset designed for instruction training of pre-trained language models~\citep{alpaca}. It contains 52002 instruction-response pairs generated by OpenAI's text-davinci-003 engine or written by humans. Note that there is only a training split in this dataset. Models fine-tuned on Alpaca are often evaluated by other tasks like ``EleutherAI LM Harness''.    

\textbf{ARC}: The AI2 Reasoning Challenge (ARC) dataset consists of
grade-school level, multiple-choice science questions~\citep{allenai:arc}. ARC dataset includes a Challenge Set and an Easy Set. The easy set contains questions that can be answered with straightforward reasoning, while the challenge set requires deeper understanding and more reasoning skills. The ARC-Easy includes 2251 training samples, 570 validation samples, and 2376 test samples and the ARC-Challenge includes 1119 training samples, 299 validation samples, and 1172 test samples.

\textbf{BoolQ}: Boolean Questions (BoolQ) is a dataset of yes/no question answering~\citep{clark2019boolq} and includes 9427 training samples and 3270 validation samples. The dataset is designed to assess models' comprehension and reasoning abilities. Each example contains question, passage, answer, and title. 

\textbf{HellaSwag}: HellaSwag is a dataset designed to evaluate the models' abilities in generating reasonable contexts~\citep{zellers2019hellaswag}. It consists of prompts with a short context followed by multiple possible continuations. The goal is to find the correct or most plausible option. The training set, validation set, and test set have 39905 samples, 10042 samples, 10003 samples, respectively.

\textbf{OpenBookQA}:
OpenBookQA is a question-answering dataset~\citep{OpenBookQA2018} comprising 4957 training samples, 500 validation samples, and 500 test samples. It requires reasoning ability and a deeper understanding of common knowledge to answer questions. Each data contains a short passage with multiple possible answers. The dataset emphasizes the integration of world knowledge and reasoning skills, making it a challenging benchmark for natural language processing models. It tests models’ abilities to understand and apply factual information effectively to solve problems.


\textbf{WinoGrande}: WinoGrande is a dataset of 44k problems for choosing the right option for a given sentence~\citep{sakaguchi2021winogrande}. It includes 40938 samples in the training set, 1,267 in the validation set, and 1,267 in the test set. The dataset is designed to assess models' commonsense reasoning abilities. The examples contain sentences with fill-in-blanks that require the model to select the most appropriate option to complete the sentence. 

\textbf{SocialIQA}: The SocialIQA dataset is a benchmark designed to evaluate a model's ability to reason about social interactions, including understanding social dynamics, intentions, and the effects of human actions~\citep{sap-etal-2019-social}. SocialIQA includes 33410 samples in the training set and 1954 in the validation set. 

\textbf{PIQA}: The PIQA (Physical Interaction Question Answering) dataset is a benchmark designed to evaluate a model's ability to understand and reason about everyday physical interactions and affordances~\citep{Bisk2020}. Here are some key details about PIQA:~\citep{sakaguchi2021winogrande}. PIQA contains 16113 samples in the training set and 1838 in the validation set.
