\section{Background and Related Work}
\label{sec:related}

\textbf{Parameter-Efficient and Memory-Efficient Fine-Tuning}: In various language and vision tasks, the ``pre-train then fine-tune'' paradigm has been shown highly effective. PEFT methods~\cite{lialin2023scaling} fine-tune a small subset of the parameters of a large pre-trained model in order to accelerate the training process. We begin by introducing SFT and LoRA, two popular approaches for PEFT. 

\textbf{Sparse Fine-Tuning}: SFT formulates the fine-tuning process as learning another weight matrix $\W_s$: 
\begin{align}
     &\hat{\W}=\W + \W_s, \label{eq:sft_w}
     \\&\h = f(\hat{\W},\x) = f(\W + \W_s, \x), \label{eq:sft_fwd}
\end{align}
where $\h\in\R^{d_{out}}$ and $\x\in\R^{d_{in}}$ are the input and output of the hidden layer, respectively, $f(\cdot)$ is the forward function, $\W\in \R^{d_{out}\times d_{in}}$ represents the frozen pre-trained parameters, and $\hat{\W}\in \R^{d_{out}\times d_{in}}$ denotes the final parameters used during inference for the fine-tuning task.  %\iffalse $d_{in}$ and $d_{out}$ are the input and output dimensionality of the hidden layer, respectively.\fi 
The matrix $\W_s\in \R^{d_{out}\times d_{in}}$ is sparse and is initialized at $\0$. Such a decomposition is done for every layer in the neural network. SFT methods try to limit the number of parameters to fine-tune. For selecting non-zero indices, \emph{Diff pruning}~\cite{guo2021parameter} learns a mask for $\W_s$ (using a standard Backprop algorithm), while \emph{FISH Mask}~\cite{sung2021training} uses Fisher information to identify important indices in $\W$. \emph{Lottery Ticket SFT}~\cite{ansell2022composable} fine-tune the whole model for one epoch, then use $\W_s$ itself as an importance score to decide which parameters to fine-tune subsequently. \emph{Robust Adaptor}~(RoSA,~\citealt{nikdan2024rosa}) combines the above SFTs with LoRA and outperforms all these approaches.
However, the key challenge of all SFT methods is that they do not sufficiently reduce memory usage, as $\W_s$ keeps the dimensionality of $\W$, and thus standard libraries do not yield memory improvements. %leading to similar memory complexity of fine-tuning $\W$ directly.

\textbf{Techniques for Efficient Sparse Computation}: To reduce memory redundancy in sparse tensor computations, various data formats like compressed sparse column/row (CSC/CSR, \citealp{mofrad2019multithreaded, lu2024spp}) and semi-structured formats~\citep{holmes2021nxmtransformer} have been proposed. These formats enable efficient operations like Sparse Matrix Multiplication (SpMM), which is crucial for dot products and matrix multiplications. Upon these techniques, sparse backpropagation is built to improve training efficiency~\citep{zhang2020sparch, gale2020sparse, peste2021ac, schwarz2021powerpropagation, hoefler2021sparsity,jiang2022exposing, pmlr-v202-nikdan23a, xu2024survey}. Beyond sparse tensor techniques, NVIDIA also offers memory optimization techniques for efficient training\footnote{Available at \url{https://pytorch.org/torchtune/stable/tutorials/memory_optimizations.html}}.

However, these techniques come with trade-offs, particularly in terms of time complexity and implementation complexity. Achieving memory efficiency often requires a significant increase in time complexity. To mitigate this, some approaches employ optimizations implemented in C++ or lower-level languages, such as those used in \citep{gale2020sparse, pmlr-v202-nikdan23a, nikdan2024rosa}, to accelerate the training process. 

\textbf{Low-Rank Adaptation (LoRA)}: Instead of requiring $\W_s$ to be sparse, low-rank adaptation aims to find update matrices that are of small rank:
\begin{align}
    & \hat{\W}=\W + \frac{\alpha}{r}\B\A \label{eq:lora_w}, \\
    & \h = f(\hat{\W}, \x) = f(\W, \x) + f(\frac{\alpha}{r}\B\A, \x), \label{eq:lora_fwd}
\end{align}
where $\alpha$ is the LoRA scaling hyper-parameter, \iffalse $r$ is the middle dimensionality of the low-rank matrices, \fi$\B\in \R^{d_{out}\times r},\ \A\in \R^{r\times d_{in}}$ are the low-rank matrices with $r\ll d_{in},d_{out}$. During inference, the $\B \A$ term can be merged into $\W$ to maintain the inference latency of the original model. During training, owing to the fact that $f$ is additive for both the self-attention blocks and the subsequent multilayer perceptron (MLP) layers of transformers~\citep{vaswani2017attention}, backpropagation can be performed efficiently for the $\B, \A$ parameters. Due to LoRA's simplicity and effectiveness, numerous variants have been proposed to enhance the performance, e.g., QLoRA~\citep{dettmers2022gpt3,guo2024lqlora,li2024loftq,dettmers2024qlora}, DoRA~\citep{liu2024dora}, RoSA~\citep{nikdan2024rosa}, and VeRA~\citep{kopiczko2024vera}. %~\citep{dettmers2022gpt3,liu2024dora,guo2024lqlora,li2024loftq,kopiczko2024vera,nikdan2024rosa, dettmers2024qlora}. 
These methods have achieved exceptional performance, often comparable to full fine-tuning across a range of tasks.

%
%Matrix decomposition is commonly used to reduce the computations in data sequences or the model's parameter matrices. In transformer-based models \citep{vaswani2017attention}, both the self-attention blocks and the subsequent multilayer perceptron (MLP) structures consist of several linear layers, of which the forward function can be written as $f(\W,\x)=\W\x$. This structure enables the application of LoRA:
%
%: 
%Recent works also combine methods like quantization with LoRA to further reduce memory usage \citep{dettmers2022gpt3, dettmers2024qlora}. 

\textbf{Neural Network Pruning}:
Besides PEFTs, neural network pruning is another widely applied technique that exploits parameter sparsity to reduce model complexity and speed up inference~\citep{lecun1989optimal, han2015learning, han2017efficient, hoefler2021sparsity}. Most pruning methods assess \emph{importance} of neural network weights (or neurons) and remove the least important parameters. \emph{Unstructured} pruning zeros out individual weights while preserving the network architecture, whereas \emph{structured} pruning removes parameter groups like channels or neurons, which reduce model size~\citep{liu2021group, fang2023depgraph, ma2023llmpruner}. Both approaches often require retraining to recover lost accuracy during pruning. While effective for classical NNs, pruning LLMs is costly due to high memory demands for computing importance scores and the prohibitive retraining step, making memory-efficient LLM pruning an active research area~\citep{frantar2023sparsegpt,sunsimple}.

%evaluate the importance score and remove the less important parameters. In practice, unstructured pruning evaluates the parameters individually and zeros out certain parameters without modifying the network structure, whereas structured pruning finds the dependency of parameters \citep{liu2021group, fang2023depgraph, ma2023llmpruner}, evaluating the importance of parameters by group, and removing the dependent group of components like channels and neurons, thereby reducing the network's size. Both pruning methods rely on retraining the model to recover the performance, whereas retraining LLM is too expensive. In addition, most state-of-the-art pruning methods evaluate the importance score by gradient-based metrics, which are also not suitable for LLM. Therefore, memory-efficiency is also a key challenge in recent researches of LLM pruning \citep{sunsimple, frantar2023sparsegpt}.