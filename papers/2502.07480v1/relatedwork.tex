\section{Related work}
\label{sec: related}
\paragraph{Nadaraya-Watson kernel estimator. } The Nadaraya-Watson (NW) estimator was introduced independently in the seminal works of \citet{nadaraya1964estimating} and \citet{watson1964smooth}. Later, and again independently, in the context of reconstructing smooth surfaces, \citet{shepard1968two} used a method referred to as Inverse Distance Weighting (IDW), which is in fact a NW estimator with respect to certain kernels leading to interpolation, identical to those we consider in this work. To the best of our knowledge, \citet{devroye1998hilbert} provided the first statistical guarantees for such interpolating NW estimators (which they called the Hilbert kernel), showing that the predictor given by \eqref{eq: h_beta} with $\beta=d$ is asymptotically consistent.
For a more general discussion on so called ``kernel rules'', see \citep[Chapter 10]{devroye2013probabilistic}.
In more recent works, \citet{belkin2019does} derived non-asymptotic rates showing consistency under a slight variation of the kernel. \citet{radhakrishnan2023wide, eilers2024generalized} showed that in certain cases, neural networks in the NTK regime behave approximately as the NW estimator, and leverage this to show consistency. \citet{abedsoltan2024context} showed that interpolating NW estimators can be used in a way that enables in-context learning. 



\paragraph{Overfitting and generalization.}

There is a substantial body of work aimed at analyzing the generalization properties of interpolating predictors that overfit noisy training data.
Many works study settings in which interpolating predictors exhibit benign overfitting,
such as linear predictors \citep{bartlett2020benign,belkin2020two,negrea2020defense,koehler2021uniform,hastie2022surprises,zhou2023optimistic,shamir2023implicit},
kernel methods \citep{yang2021exact,mei2022generalizationrandomfeat,tsigler2023benign}, and other learning rules \citep{devroye1998hilbert,belkin2018overfitting,belkin2019reconciling}.

On the other hand, there is also a notable line of work studying the limitations of generalization bounds in interpolating regimes \citep{belkin2018understand,zhang2021understanding,nagarajan2019uniform}.
In particular, several works showed that various kernel interpolating methods are not consistent in any fixed dimension \citep{rakhlin2019consistency,beaglehole2023inconsistency,haas2024mind}, 
or whenever the number of samples scales
as an integer-degree polynomial with the dimension
\citep{mei2022generalization, xiao2022precise, barzilai2024generalization, zhang2024phase}.

Motivated by these results and by additional empirical evidence, \citet{mallinar2022benign} proposed a more nuanced view of interpolating predictors,
coining the term \emph{tempered overfitting} to refer to settings in which the asymptotic risk is strictly worse than optimal, but is still better than a random guess. 
A well-known example is the classic $1$-nearest-neighbor interpolating method, for which the excess risk scales linearly with the probability of a label flip \citep{cover1967nearest}.
Several works subsequently studied settings in which tempered overfitting occurs in the context of kernel methods
\citep{li2024asymptotic,barzilai2024generalization,cheng2024comprehensive},
and for other interpolation rules
\citep{manoj2023interpolation,kornowski2024tempered,harel2024provable}.

Finally, some works studied settings in which interpolating with kernels is in fact \emph{catastrophic},
meaning that the error is lower bounded by a constant which is independent of the noise level, leading to substantial risk even in the presence of very little noise \citep{kornowski2024tempered,joshi2024noisy,medvedev2024overfitting, cheng2024characterizing}.

\paragraph{Varying kernel bandwidth.}
Several prior works consider generalization bounds that hold uniformly over a family of kernels, parameterized by a number known as the bandwidth \citep{rakhlin2019consistency, buchholz2022kernel, beaglehole2023inconsistency, haas2024mind, medvedev2024overfitting}. The bandwidth plays the same role as the parameter $\beta$ in this paper, which controls how local/global the kernel is. Specifically, these works showed that in fixed dimensions various kernels are asymptotically inconsistent \emph{for all} bandwidths. \citet{medvedev2024overfitting} showed that at least with large enough noise, the Gaussian kernel with any bandwidth is at least as bad as a predictor that is constant outside the training set, which we classify as catastrophic. As far as we know, our paper gives the first known case of a kernel method exhibiting all types of overfitting behaviors in fixed dimensions by varying the bandwidth alone.