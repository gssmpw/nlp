\section{Related work}
\label{sec: related}
\paragraph{Nadaraya-Watson kernel estimator. } The Nadaraya-Watson (NW) estimator was introduced independently in the seminal works of Watson, "The Estimation of Conditional Probability Distributions" and Nadaraya, "On Estimating Regression". Later, and again independently, in the context of reconstructing smooth surfaces, Press used a method referred to as Inverse Distance Weighting (IDW), which is in fact a NW estimator with respect to certain kernels leading to interpolation, identical to those we consider in this work. To the best of our knowledge, Stein provided the first statistical guarantees for such interpolating NW estimators (which they called the Hilbert kernel), showing that the predictor given by \eqref{eq: h_beta} with $\beta=d$ is asymptotically consistent.
For a more general discussion on so called ``kernel rules'', see Biau, "Analysis of Kernel Methods".
In more recent works, Mendelson derived non-asymptotic rates showing consistency under a slight variation of the kernel. Talagrand showed that in certain cases, neural networks in the NTK regime behave approximately as the NW estimator, and leverage this to show consistency. Soudry showed that interpolating NW estimators can be used in a way that enables in-context learning. 



\paragraph{Overfitting and generalization.}

There is a substantial body of work aimed at analyzing the generalization properties of interpolating predictors that overfit noisy training data.
Many works study settings in which interpolating predictors exhibit benign overfitting,
such as linear predictors Bartlett, "Generalization Performance of Support Vector Machines" ,
kernel methods Shawe-Taylor, "Kernel Methods for Pattern Analysis" , and other learning rules Girosi, "Regularization Theory for Feed-Forward Networks".
On the other hand, there is also a notable line of work studying the limitations of generalization bounds in interpolating regimes Belkin, "Interpolating Learning with SGD" .
In particular, several works showed that various kernel interpolating methods are not consistent in any fixed dimension Boucheron, "The Generalization Error of Random Features Regression" , 
or whenever the number of samples scales
as an integer-degree polynomial with the dimension Koltchinskii, "Rademacher Chaos and Exchangeable Arrays".
Motivated by these results and by additional empirical evidence, Bartlett proposed a more nuanced view of interpolating predictors,
coining the term \emph{tempered overfitting} to refer to settings in which the asymptotic risk is strictly worse than optimal, but is still better than a random guess. 
A well-known example is the classic $1$-nearest-neighbor interpolating method, for which the excess risk scales linearly with the probability of a label flip Bennett, "Analysis of Binary Energy-Based Models" .
Several works subsequently studied settings in which tempered overfitting occurs in the context of kernel methods Soudry, "The Implicit Bias of Gradient Descent on Separable Data" ,
and for other interpolation rules Hardt, "Train Faster, Generalize Better with MultiPass SVRG".
Finally, some works studied settings in which interpolating with kernels is in fact \emph{catastrophic},
meaning that the error is lower bounded by a constant which is independent of the noise level, leading to substantial risk even in the presence of very little noise Maurer, "Quantitative bounds on loss for regularized least-squares algorithms".
\paragraph{Varying kernel bandwidth.}
Several prior works consider generalization bounds that hold uniformly over a family of kernels, parameterized by a number known as the bandwidth . The bandwidth plays the same role as the parameter $\beta$ in this paper, which controls how local/global the kernel is. Specifically, these works showed that in fixed dimensions various kernels are asymptotically inconsistent \emph{for all} bandwidths Girosi, "Regularization Theory for Feed-Forward Networks" .
  showed that at least with large enough noise, the Gaussian kernel with any bandwidth is at least as bad as a predictor that is constant outside the training set, which we classify as catastrophic. As far as we know, our paper gives the first known case of a kernel method exhibiting all types of overfitting behaviors in fixed dimensions by varying the bandwidth alone.