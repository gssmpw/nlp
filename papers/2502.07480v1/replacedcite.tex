\section{Related work}
\label{sec: related}
\paragraph{Nadaraya-Watson kernel estimator. } The Nadaraya-Watson (NW) estimator was introduced independently in the seminal works of ____ and ____. Later, and again independently, in the context of reconstructing smooth surfaces, ____ used a method referred to as Inverse Distance Weighting (IDW), which is in fact a NW estimator with respect to certain kernels leading to interpolation, identical to those we consider in this work. To the best of our knowledge, ____ provided the first statistical guarantees for such interpolating NW estimators (which they called the Hilbert kernel), showing that the predictor given by \eqref{eq: h_beta} with $\beta=d$ is asymptotically consistent.
For a more general discussion on so called ``kernel rules'', see ____.
In more recent works, ____ derived non-asymptotic rates showing consistency under a slight variation of the kernel. ____ showed that in certain cases, neural networks in the NTK regime behave approximately as the NW estimator, and leverage this to show consistency. ____ showed that interpolating NW estimators can be used in a way that enables in-context learning. 



\paragraph{Overfitting and generalization.}

There is a substantial body of work aimed at analyzing the generalization properties of interpolating predictors that overfit noisy training data.
Many works study settings in which interpolating predictors exhibit benign overfitting,
such as linear predictors ____,
kernel methods ____, and other learning rules ____.

On the other hand, there is also a notable line of work studying the limitations of generalization bounds in interpolating regimes ____.
In particular, several works showed that various kernel interpolating methods are not consistent in any fixed dimension ____, 
or whenever the number of samples scales
as an integer-degree polynomial with the dimension
____.

Motivated by these results and by additional empirical evidence, ____ proposed a more nuanced view of interpolating predictors,
coining the term \emph{tempered overfitting} to refer to settings in which the asymptotic risk is strictly worse than optimal, but is still better than a random guess. 
A well-known example is the classic $1$-nearest-neighbor interpolating method, for which the excess risk scales linearly with the probability of a label flip ____.
Several works subsequently studied settings in which tempered overfitting occurs in the context of kernel methods
____,
and for other interpolation rules
____.

Finally, some works studied settings in which interpolating with kernels is in fact \emph{catastrophic},
meaning that the error is lower bounded by a constant which is independent of the noise level, leading to substantial risk even in the presence of very little noise ____.

\paragraph{Varying kernel bandwidth.}
Several prior works consider generalization bounds that hold uniformly over a family of kernels, parameterized by a number known as the bandwidth ____. The bandwidth plays the same role as the parameter $\beta$ in this paper, which controls how local/global the kernel is. Specifically, these works showed that in fixed dimensions various kernels are asymptotically inconsistent \emph{for all} bandwidths. ____ showed that at least with large enough noise, the Gaussian kernel with any bandwidth is at least as bad as a predictor that is constant outside the training set, which we classify as catastrophic. As far as we know, our paper gives the first known case of a kernel method exhibiting all types of overfitting behaviors in fixed dimensions by varying the bandwidth alone.