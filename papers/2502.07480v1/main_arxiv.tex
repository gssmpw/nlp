\documentclass[11pt]{article}
\usepackage{fullpage,url,natbib}
\usepackage{times}
% \renewcommand\ttdefault{cmtt}
\usepackage{amsthm,amsfonts,amsmath,amssymb,epsfig,color,float,graphicx,verbatim}
\usepackage{enumitem}
\usepackage{algorithm,algorithmic}
\usepackage{footmisc}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{bbm, bm}
% \usepackage[square,numbers]{natbib}
\usepackage{natbib}
% \setcitestyle{square, numbers}
\usepackage[export]{adjustbox}
\usepackage{nicefrac}
% \usepackage{subcaption}
% \usepackage{caption}



\usepackage{hyperref}
\hypersetup{
	colorlinks   = true, %Colours links instead of ugly boxes
	urlcolor     = blue, %Colour for external hyperlinks
	linkcolor    = blue, %Colour of internal links
	citecolor   = blue %Colour of citations
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
% \newtheorem*{proposition*}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{assumption}{Assumption}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\subsecref}[1]{Subsection~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\renewcommand{\eqref}[1]{Eq.~(\ref{#1})}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\propref}[1]{Proposition~\ref{#1}}
\newcommand{\appref}[1]{Appendix~\ref{#1}}
\newcommand{\algref}[1]{Algorithm~\ref{#1}}
\newcommand{\assref}[1]{Assumption~\ref{#1}}
\newcommand{\defref}[1]{Definition~\ref{#1}}
\newcommand{\corref}[1]{Corollary~\ref{#1}}

% \renewcommand{\S}{\mathbb{S}}


\newcommand{\note}[1]{\textcolor{red}{\textbf{#1}}}
\newcommand{\gk}[1]{\textcolor{purple}{[Guy: #1]}}
\newcommand{\db}[1]{\textcolor{violet}{[Daniel: #1]}}
\newcommand{\os}[1]{\textcolor{cyan}{[Ohad: #1]}}

\input{symbols}

\makeatletter
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\makeatother





\title{Overfitting Regimes of Nadaraya-Watson Interpolators
}
%\vspace*{2pt}



\author{
Daniel Barzilai\thanks{Equal contribution.}
	\qquad
	Guy Kornowski\printfnsymbol{1}
	\qquad
	Ohad Shamir
\vspace{3pt}
\\
Weizmann Institute of Science \vspace{2pt}\\
\texttt{\{daniel.barzilai,guy.kornowski,ohad.shamir\}@weizmann.ac.il}  
}
% \date{}


\begin{document}

\maketitle



\begin{abstract}
In recent years, there has been much interest in understanding the generalization behavior of interpolating predictors, which overfit on noisy training data. Whereas standard analyses are concerned with whether a method is consistent or not, recent observations have shown that even inconsistent predictors can generalize well. In this work, we revisit the classic interpolating Nadaraya-Watson (NW) estimator (also known as Shepard's method), and study its generalization capabilities  through this modern viewpoint. In particular, by varying a single bandwidth-like hyperparameter, we prove the existence of multiple overfitting behaviors, ranging non-monotonically from catastrophic, through benign, to tempered. 
Our results highlight how even classical interpolating methods can exhibit intricate generalization behaviors. Numerical experiments complement our theory, demonstrating the same phenomena.
\end{abstract}







\section{Introduction}
The incredible success of over-parameterized machine learning models has spurred a substantial body of work, aimed at understanding the generalization behavior of interpolating methods (which perfectly fit the training data).
In particular, according to classical statistical analyses,  interpolating inherently noisy training data can be harmful in terms of test error, due to the bias-variance tradeoff. However, contemporary interpolating methods seem to defy this common wisdom \citep{belkin2019reconciling,zhang2021understanding}. 
Therefore, a current fundamental question in statistical learning is to understand when models that perfectly fit noisy training data can still achieve strong generalization performance.

The notion of what it means to generalize well has somewhat changed over the years.
Classical analysis has been mostly concerned with whether or not a method is consistent, meaning that asymptotically (as the training set size increases), the
excess risk
converges to zero.
By now, several settings have been identified where even {interpolating} models may be consistent, a phenomenon known as ``benign overfitting'' \citep{bartlett2020benign,liang2020just,frei2022benign,tsigler2023benign}. 
However, following \citet{mallinar2022benign}, a more nuanced view of overfitting has emerged,
based on the observation that not all inconsistent learning rules are necessarily unsatisfactory.

In particular, it has been argued both empirically and theoretically that in many realistic settings, benign overfitting may not occur, yet interpolating methods may still overfit in a ``tempered'' manner,
meaning that their excess risk is proportional to the
Bayes error.
On the other hand, in some situations overfitting may indeed be ``catastrophic'', leading to substantial degradation in performance even in the presence of very little noise.
The difference between these regimes is significant when the amount of noise in the data is relatively small, and in such a case, models that overfit in a tempered manner may still generalize relatively well, while catastrophic methods do not.
These observations led to several recent works aiming at characterizing which overfitting profiles occur in different settings beyond consistency, mostly for kernel regression and shallow ReLU networks
\citep{manoj2023interpolation,kornowski2024tempered,joshi2024noisy,li2024asymptotic,barzilai2024generalization,medvedev2024overfitting,cheng2024characterizing}. We note that one classical example of tempered overfitting is $1$-nearest neighbor, which asymptotically achieves at most \emph{twice} the Bayes error \citep{cover1967nearest}. Moreover, results of a similar flavor are known for $k$-nearest neighbor where $k>1$ (see \citep{devroye2013probabilistic}). However, unlike the interpolating predictors we study here, $k$-nearest neighbors do not necessarily interpolate the training data when $k>1$. 

With this modern nuanced approach in mind, we revisit in this work
one of the earliest and most classical learning rules, namely the Nadaraya-Watson (NW) estimator \citep{nadaraya1964estimating,watson1964smooth}. 
In line with recent analysis focusing on interpolating predictors, we focus on an interpolating variant of the NW estimator, for binary classification:
given (possibly noisy) classification data $S=(\bx_i,y_i)_{i=1}^{m}\subset\reals^d\times\{\pm1\}$ sampled from some continuous distribution $\Dcal$, and given some $\beta>0$, we consider the predictor
\begin{equation}\label{eq: h_beta}
\hat{h}_\beta(\bx) :=
\begin{cases}
    \sign\left(\sum_{i=1}^m \frac{y_i}{\norm{\bx - \bx_i}^{\beta}}\right) & \text{if~~}\bx \notin S \\
    y_i & \text{if~~}\bx = \bx_i\text{ for some }\bx_i\in S~.
\end{cases}
\end{equation}
The predictor in \eqref{eq: h_beta} has a long history in the literature and is known by many different names, such as Shepard's method, inverse distance weighting (IDW), the Hilbert kernel estimate, and singular kernel classification (see Section~\ref{sec: related} for a full discussion). 

Notably, for \emph{any} choice of $\beta>0$, $\hat{h}_\beta$ interpolates the training set, meaning that $\hat{h}_\beta(\bx_i)=y_i$. 
We will study the predictor's generalization in ``noisy'' classification tasks: we assume there exists a ground truth $f^*:\reals^d\to\{\pm1\}$
(satisfying mild regularity assumptions),
so that for each sampled point $\bx$, its associated label $y\in \{\pm 1\}$ satisfies $\Pr[y=f^*(\bx)\,|\,\bx]=1-p$ for some $p\in(0,\half)$.
Clearly, for this distribution, no predictor can achieve expected classification error better than $p>0$. However, interpolating predictors achieve $0$ training error on the training set, and thus by definition overfit. We are interested in studying the ability of these predictors to achieve low classification error with respect to the underlying distribution. Factoring out the inevitable error due to noise, we can measure this via the ``clean'' classification error $\Pr_{\bx\sim\Dcal_\bx}[\hat{h}_\beta(\bx)\neq f^*(\bx)]$,
which measures how well $\hat{h}_{\beta}$ captures the ground truth function $f^*$.

As our starting point, we recall that
 $\hat{h}_\beta$ is known to exhibit benign overfitting when $\beta$ precisely equals $d$:

\begin{theorem}[\citealp{devroye1998hilbert}]
Let $\beta=d$. For any noise level $p\in(0,\half)$,
it holds that
the clean classification error of $\hat{h}_\beta$ goes to zero as $m\to\infty$, i.e. $\hat{h}_\beta$ exhibits benign overfitting.
\end{theorem}

In other words, although training labels are flipped with probability $p\in(0,\half)$, the predictor is asymptotically consistent, and thus predicts according to the ground truth $f^*$.
Furthermore, \citet{devroye1998hilbert} also informally argued that setting $\beta\neq d$ is inconsistent in general, and therefore excess risk should be expected.
Nonetheless, the behavior of the predictor $\hat{h}_\beta$ beyond the benign/consistent setting is not known prior to this work.

In this paper,
in light of the recent interest in inconsistent interpolation methods, we characterize the price of overfitting in the inconsistent regime $\beta\neq d$. What is the nature of the inconsistency for $\beta\neq d$? Is the overfitting tempered, or in fact catastrophic?
As our main contribution, we answer these questions and prove the following asymmetric behavior:
\begin{theorem}[Main results, informal]
For any dimension $d\in\NN$ and noise level $p\in(0,\half)$, the following hold asymptotically as $m\to\infty$:
\begin{itemize}
    \item (``Tempered'' overfitting) For any $\beta>d$,
    the clean classification error of $\hat{h}_\beta$ is between $\Omega(\poly(p))$ and $\widetilde{\Ocal}(p)$.
    \item (``Catastrophic'' overfitting) For any $\beta<d$, there is some $f^*$ for which $\hat{h}_\beta$ will suffer constant clean classification error, independently of $p$.
\end{itemize}
\end{theorem}

We summarize the overfitting profile that unfolds in Figure~\ref{fig:overall_illustration}, with an illustration of the Nadaraya-Watson interpolator in one dimension. 

\begin{figure}[h]
    \centering
        \includegraphics[trim=0cm 16cm 0cm 16cm,clip=true, width=1.5\linewidth,center]{tempered_axis.pdf}
        \caption*{(a)}
    \includegraphics[width=\textwidth]{1d_toy.png}
    \caption*{(b)}
    \caption{
    (a): Illustration of the entire overfitting profile of the NW interpolator given by \eqref{eq: h_beta}.
    (b): Toy illustration of the NW interpolator in dimension $d=1$ with noisy data that has two label-flipped points. \textbf{(Left)} Catastrophic overfitting for $\beta < d$: the prediction at each point is influenced too heavily by far-away points, and therefore the predictor does not capture the general structure of the ground truth function $f^*$. \textbf{(Middle)} Benign overfitting for $\beta=d$: asymptotically the excess risk will be Bayes-optimal. \textbf{(Right)} Tempered overfitting for $\beta > d$, the prediction at each point is influenced too heavily by nearby points. Therefore, the predictor misclassifies large regions around label-flipped points, but only around them.
    }
    \label{fig:overall_illustration}
\end{figure}

Overall, we provide a modern analysis of a classical learning rule, uncovering a range of generalization behaviors. By varying a single hyperparameter, these behaviors range non-monotonically from catastrophic to tempered overfitting, with a delicate sliver of benign overfitting behavior in between. Our results highlight how intricate generalization behaviors, including the full range from benign to catastrophic overfitting, can appear in simple and well-known interpolating learning rules.

The paper is structured as follows. After reviewing related work and formally presenting the discussed setting in Section~\ref{sec: prelim}, in Section~\ref{sec: tempered} we resent our result for the tempered regime $\beta>d$. In Section~\ref{sec: catastrophic} we present our result for the catastrophic regime $\beta<d$.
In Section~\ref{sec: experiment} we provide some illustrative experiments to complement our theoretical findings. We conclude in Section~\ref{sec: discuss}.
All of the results in the main text include proof sketches, while full proofs appear in the Appendix.

\subsection{Related work} \label{sec: related}
\paragraph{Nadaraya-Watson kernel estimator. } The Nadaraya-Watson (NW) estimator was introduced independently in the seminal works of \citet{nadaraya1964estimating} and \citet{watson1964smooth}. Later, and again independently, in the context of reconstructing smooth surfaces, \citet{shepard1968two} used a method referred to as Inverse Distance Weighting (IDW), which is in fact a NW estimator with respect to certain kernels leading to interpolation, identical to those we consider in this work. To the best of our knowledge, \citet{devroye1998hilbert} provided the first statistical guarantees for such interpolating NW estimators (which they called the Hilbert kernel), showing that the predictor given by \eqref{eq: h_beta} with $\beta=d$ is asymptotically consistent.
For a more general discussion on so called ``kernel rules'', see \citep[Chapter 10]{devroye2013probabilistic}.
In more recent works, \citet{belkin2019does} derived non-asymptotic rates showing consistency under a slight variation of the kernel. \citet{radhakrishnan2023wide, eilers2024generalized} showed that in certain cases, neural networks in the NTK regime behave approximately as the NW estimator, and leverage this to show consistency. \citet{abedsoltan2024context} showed that interpolating NW estimators can be used in a way that enables in-context learning. 



\paragraph{Overfitting and generalization.}

There is a substantial body of work aimed at analyzing the generalization properties of interpolating predictors that overfit noisy training data.
Many works study settings in which interpolating predictors exhibit benign overfitting,
such as linear predictors \citep{bartlett2020benign,belkin2020two,negrea2020defense,koehler2021uniform,hastie2022surprises,zhou2023optimistic,shamir2023implicit},
kernel methods \citep{yang2021exact,mei2022generalizationrandomfeat,tsigler2023benign}, and other learning rules \citep{devroye1998hilbert,belkin2018overfitting,belkin2019reconciling}.

On the other hand, there is also a notable line of work studying the limitations of generalization bounds in interpolating regimes \citep{belkin2018understand,zhang2021understanding,nagarajan2019uniform}.
In particular, several works showed that various kernel interpolating methods are not consistent in any fixed dimension \citep{rakhlin2019consistency,beaglehole2023inconsistency,haas2024mind}, 
or whenever the number of samples scales
as an integer-degree polynomial with the dimension
\citep{mei2022generalization, xiao2022precise, barzilai2024generalization, zhang2024phase}.

Motivated by these results and by additional empirical evidence, \citet{mallinar2022benign} proposed a more nuanced view of interpolating predictors,
coining the term \emph{tempered overfitting} to refer to settings in which the asymptotic risk is strictly worse than optimal, but is still better than a random guess. 
A well-known example is the classic $1$-nearest-neighbor interpolating method, for which the excess risk scales linearly with the probability of a label flip \citep{cover1967nearest}.
Several works subsequently studied settings in which tempered overfitting occurs in the context of kernel methods
\citep{li2024asymptotic,barzilai2024generalization,cheng2024comprehensive},
and for other interpolation rules
\citep{manoj2023interpolation,kornowski2024tempered,harel2024provable}.

Finally, some works studied settings in which interpolating with kernels is in fact \emph{catastrophic},
meaning that the error is lower bounded by a constant which is independent of the noise level, leading to substantial risk even in the presence of very little noise \citep{kornowski2024tempered,joshi2024noisy,medvedev2024overfitting, cheng2024characterizing}.

\paragraph{Varying kernel bandwidth.}
Several prior works consider generalization bounds that hold uniformly over a family of kernels, parameterized by a number known as the bandwidth \citep{rakhlin2019consistency, buchholz2022kernel, beaglehole2023inconsistency, haas2024mind, medvedev2024overfitting}. The bandwidth plays the same role as the parameter $\beta$ in this paper, which controls how local/global the kernel is. Specifically, these works showed that in fixed dimensions various kernels are asymptotically inconsistent \emph{for all} bandwidths. \citet{medvedev2024overfitting} showed that at least with large enough noise, the Gaussian kernel with any bandwidth is at least as bad as a predictor that is constant outside the training set, which we classify as catastrophic. As far as we know, our paper gives the first known case of a kernel method exhibiting all types of overfitting behaviors in fixed dimensions by varying the bandwidth alone.

\section{Preliminaries} \label{sec: prelim}



\paragraph{Notation.} 
We use bold-faced font to denote vectors, e.g. $\bx\in\reals^d$, and denote by $\norm{\bx}$ the Euclidean norm. We let $[n]:=\{1,\dots,n\}$. Given some set $A\subseteq \R^d$ and a function $f$, we denote its restriction by $f|_A:A\to\reals$,
and by $\Unif(A)$ the uniform distribution over $A$. We let $B(\bx, r):=\{\bz\mid \norm{\bx-\bz}\leq r\}$ be the ball of radius $r$ centered at $\bx$. We use the standard big-O notation, with $\bigo(\cdot)$, $\Theta(\cdot)$ and $\Omega(\cdot)$ hiding absolute constants that do not depend on problem parameters, and $\tilde{\bigo}(\cdot)$, $\tilde{\Omega}(\cdot)$ hiding absolute constants and  additional logarithmic factors.
Given some parameter (or set of parameters) $\theta$, we denote by $c(\theta),C(\theta),C_1(\theta)$ etc. positive constants that depend on $\theta$.

\paragraph{Setting.}
Given some target function $f^*:\reals^d\to\{\pm1\}$, we consider a classification task based on noisy training data $S=(\bx_i,y_i)_{i=1}^{m}\subset\reals^d\times\{\pm1\}$, such that $\bx_1,\dots,\bx_m \sim\Dcal_\bx$ are sampled from some distribution $\Dcal_\bx$ with a density $\mu$, and for each $i\in[m]$ independently, $y_i=f^*(\bx_i)$ with probability $1-p$ or else $y_i=-f^*(\bx_i)$ with probability $p\in(0,\half)$.


Given the predictor $\hat{h}_\beta$ introduced in \eqref{eq: h_beta},
we denote the asymptotic clean classification error by\footnote{Technically, the limit may not exist in general. In that case, our lower bounds hold for the $\lim\inf_{m\to\infty}$,
while our upper bounds hold for the $\lim\sup_{m\to\infty}$,
and therefore both hold for all partial limits.} 
\[
\Lcal(\hat{h}_\beta)
=\lim_{m\to\infty}\E_S\left[\Pr_{\bx\sim\Dcal_\bx}[\hat{h}_\beta(\bx)\neq f^*(\bx)]\right]~.
\]








Throughout the paper we impose the following mild regularity assumptions
on the density $\mu$, and on the target function $f^*:$

\begin{assumption} \label{ass: f*}
We assume $\mu$ is continuous at almost every $\bx\in\reals^d$.
We also assume that for almost every $\bx\in\reals^d$, there is a neighborhood $B_\bx\supset\{\bx\}$ such that $f^*|_{B_\bx}\equiv f^*(\bx)$.
\end{assumption}

We note that the regularity assumptions above are very mild. Indeed, any density is Lebesgue integrable, whereas our assumption for $\mu$ is equivalent to it being Riemann integrable.
As for $f^*$, the assumption essentially asserts that its associated decision boundary has zero measure, ruling out pathological functions.













\paragraph{Types of overfitting.}

We study the asymptotic error guaranteed by $\hat{h}_\beta$ in a ``minimax'' sense, namely uniformly over $\mu,f^*$ that satisfy Assumption~\ref{ass: f*}.
Under the described setting with noise level $p\in(0,\half)$, we say that:

\begin{itemize}
    \item $\hat{h}_\beta$ exhibits benign overfitting if $\Lcal(\hat{h}_\beta)=0$;
    \item Else, $\hat{h}_\beta$ exhibits tempered overfitting if $\Lcal(\hat{h}_\beta)$ scales monotonically with $p$.
    Specifically, there exists $\varphi:[0,1]\to[0,1]$ non-decreasing, continuous with $\varphi(0)=0$, so that $\Lcal(\hat{h}_\beta)\leq\varphi(p)$;
    \item $\hat{h}_\beta$ exhibits catastrophic overfitting if there exist some $\mu,f^*$ satisfying the regularity assumptions such that $\Lcal(\hat{h}_\beta)=\Omega(1)$ is lower bounded by a positive constant independent of $p$.
\end{itemize}


We note that the latter definition of catastrophic overfitting slightly differs from the one of \citet{mallinar2022benign} (which called the method catastrophic only if $\Lcal(\hat{h}_\beta)= \half$). As noted by \citet{medvedev2024overfitting}, the original definition of \citet{mallinar2022benign} can result in even the most trivial predictor, a function that is constant outside the training set, being classified as tempered instead of catastrophic. We therefore find the formalization above more suitable, which also coincides with previous works \citep{manoj2023interpolation,kornowski2024tempered,barzilai2024generalization,medvedev2024overfitting,harel2024provable}.

\section{Tempered overfitting} \label{sec: tempered}

We start by presenting our main result for the $\beta>d$ parameter regime, establishing tempered overfitting of the predictor $\hat{h}_\beta:$

\begin{theorem} \label{thm: tempered}
For any $d\in\NN$, any $\beta>d$, and any density $\mu$ and target function $f^*$ satisfying Assumption~\ref{ass: f*},
    it holds that
    \[
    C_1(\beta/d)\cdot p^{c(\beta/d)}~\leq~
    \Lcal(\hat{h}_\beta)
    ~\leq~ C_2(\beta/d)\cdot\log^{\frac{1}{1-d/\beta}}(1/p)\cdot p~,
    \]
    where $c(\beta/d)=\left(\frac{2^{\beta/d}}{\beta/d-1}\right)^{\frac{1}{\beta/d-1}}>0$, and $C_1(\beta/d),C_2(\beta/d)>0$ are constants that depend only on the ratio $\beta/d$.
\end{theorem}

In particular, the theorem implies that for any $\beta>d$
\[
\Lcal(\hat{h}_\beta)=\widetilde{\Ocal}(p)~,
\]
hence in low noise regimes the error is never too large.
Moreover, we note that the lower bound (of the form $\Omega(\mathrm{poly}(p))$ for any $\beta>d$) holds for \emph{any} target function satisfying mild regularity assumptions. Therefore, the tempered cost of overfitting holds not only in a minimax sense, but for any instance.

Further note that since we know that $\beta = d$ leads to benign overfitting, one should expect the lower bound in \thmref{thm: tempered} to approach $0$ as $\beta\to d^{+}$. Indeed, the lower bound's polynomial degree satisfies,
\[
c(\beta/d)=\left(\frac{2^{\beta/d}}{\beta/d-1}\right)^{\frac{1}{\beta/d-1}}\overset{\beta\to d^{+}}{\longrightarrow}\infty~,
\]
and thus $p^{c(\beta/d)}\overset{\beta\to d^{+}}{\longrightarrow}0$.\footnote{
To be precise, one needs to make sure that the constant $C_1(\beta/d)$ does not blow up, which is indeed the case.
}


We provide below a sketch of the main ideas that appear in the proof of Theorem~\ref{thm: tempered}, which is provided in Appendix~\ref{sec: tempered proof}.
In a nutshell, the proof establishes that when $\beta>d$, the predictor $\hat{h}_\beta$ is highly \emph{local}, and thus prediction at a test point is affected by flipped labels nearby, yet only by them. The proof essentially shows that in this parameter regime, $\hat{h}_\beta$ behaves similar to the $k$ nearest neighbor ($k$-NN) method for some finite $k$ that depends on $\beta/d$ (although notably, as opposed to $\hat{h}_\beta$, $k$-NN does not interpolate), and has a similarly tempered generalization guarantee accordingly.



\begin{proof}[Proof sketch of Theorem~\ref{thm: tempered}]
Looking at some test point $\bx\in\reals^d$, we are interested in understanding the prediction $\hat{h}_\beta(\bx)$.
Clearly, by definition in \eqref{eq: h_beta}, the prediction depends on the random variables $\norm{\bx-\bx_i}^\beta$ for $i\in[m]$, so that closer datapoints have a great affect on the prediction at $\bx$.
Denote by $y_{(1)},\dots,y_{(m)}$ the labels ordered according to the distance of their corresponding datapoints,
namely $\norm{\bx-\bx_{(1)}}\leq \norm{\bx-\bx_{(2)}}$ $\leq\dots\leq \norm{\bx-\bx_{(m)}}$.
Then by analyzing the distribution of distances from the sample to $\bx$, we are able to show that with high probability:
\begin{equation}
\label{eq: h as sumE}
\hat{h}_\beta(\bx)
=\sign\left(\sum_{i=1}^{m}\frac{y_{(i)}}{(\sum_{j=1}^{i}E_j)^{\beta/d}}+\epsilon_m
\right)~,
\end{equation}
where $E_1,\dots,E_{m}\overset{i.i.d.}{\sim} \exp(1)$ are standard exponential random variables, and
$\epsilon_m\overset{m\to\infty}{\longrightarrow}0$ is some term that is asymptotically negligible, and therefore we will neglect it for the rest of the proof sketch.
Since $\E[\sum_{j=1}^{i}E_j]=i$, we use concentration bounds for sums of exponential variables to argue that with high probability $\sum_{j=1}^{i}E_j\approx i$ simultaneously over all $i\in\NN$. Under the idealized event in which $\sum_{j=1}^{i}E_j=i$, we would get that
\[
\hat{h}_\beta(\bx)\approx \sign\left(\sum_{i=1}^{m}\frac{y_{(i)}}{i^{\beta/d}}\right)~.
\]
Crucially, for any $\beta>d$, or equivalently $\beta/d>1$, the sum above converges, and therefore there exists a constant $k\in\NN$ (that depends only on the ratio $\beta/d$) so that the tail is dominated by the first $k$ summands:
\[
\left|\sum_{i=k+1}^{m}\frac{y_{(i)}}{i^{\beta/d}}\right|
~\lesssim~\sum_{i=k+1}^{\infty}\frac{1}{i^{\beta/d}}
~\lesssim~
\frac{1}{k^{\beta/d-1}} 
~\ll~
\left|\sum_{i=1}^{k}\frac{y_{(i)}}{i^{\beta/d}}
\right|~.
\]
Therefore,
the prediction depends only on the $k$ nearest neighbors, and
under the event that all nearby labels coincide, we would get that so does the predictor, namely with high probability
\[
y_{(1)}=\dots=y_{(k)}
~~\implies~~\hat{h}_\beta(\bx)=y_{(1)}=\dots=y_{(k)}~.
\]
Moreover, by \assref{ass: f*}, for sufficiently large sample size $m$ and fixed $k$, for almost every $\bx$ the $k$ nearest neighbors should be labeled the same as $\bx$, namely $f^*(\bx)=f^*(\bx_{(1)})=\dots=f^*(\bx_{(k)})$. So overall, we see that
\[
\Pr[\hat{h}_\beta(\bx)\neq f^*(\bx)]\leq
\Pr[\underset{\text{flipped label}}{\underbrace{\exists i\in[k]:~y_{(i)}\neq f^*(\bx_{(i)})}}]=1- (1-p)^{k}\leq kp~,
\]
and similarly
\[
\Pr[\hat{h}_\beta(\bx)\neq f^*(\bx)]
\geq  \Pr[\underset{\text{all $k$ labels flipped}}{\underbrace{\forall i\in[k]:~y_{(i)}\neq f^*(\bx_{(i)})}}]
=p^k~.
\]
The two inequalities above show the desired upper and lower bounds on the prediction error, respectively.

\end{proof}




\section{Catastrophic overfitting} \label{sec: catastrophic}
We now turn to present our main result for the $\beta<d$ parameter regime,
establishing that $\hat{h}_\beta$ can catastrophically overfit:

\begin{theorem} \label{thm:catastrophic}
For any $d\in\NN$ and any $0<\beta<d$, there exist a density $\mu$ and a target function $f^*$ satisfying \assref{ass: f*}, such that for some absolute constants $C_1,C_2\in (0,1)$, and $c(\beta,d) := C_1^{\beta}\cdot \left(1-\frac{\beta}{d}\right)>0$, it holds for any $p \in (0,0.49)$ that
    \begin{align*}
        \Lcal(\hat{h}_{\beta}) \geq C_2 \cdot c(\beta,d)~.
    \end{align*}
\end{theorem}

The theorem states that whenever $\beta < d$,
the error can be arbitrarily larger than the noise level, since $\Lcal(\hat{h}_{\beta}) = \Omega(1)$ even as $p\to 0$.
Note that since the benign overfitting result for $\beta = d$ holds over any distribution and target function (under the same regularity), the fact that the lower bound of \thmref{thm:catastrophic} approaches $0$ as $\beta \to d$ is to be expected.
 
We provide below a sketch of the main ideas of the proof, which is provided in Appendix~\ref{sec: catastrophic proof}.
Interestingly, the main idea behind the proof will be quite different from that of \thmref{thm: tempered}. There, the analysis was highly \emph{local}, i.e. for every test point $\bx$ we showed that we can restrict our analysis to a small neighborhood around that point. In contrast,  the reason we will obtain catastrophic overfitting for $\beta < d$ is precisely that the predictor is too \emph{global}, as we will see that for every test point $\bx$, all points $\bx_i$ in the training set have a non-negligible effect on $\hat{h}_{\beta}(\bx)$. 
The full proof can be found in the appendix.


\begin{proof}[Proof sketch of Theorem~\ref{thm:catastrophic}]
We will construct an explicit distribution and target function for which $\hat{h}_{\beta}$ exhibits catastrophic overfitting. The distribution we consider consists of an inner ball of constant probability mass labeled $-1$, and an outer annulus labeled $+1$, as illustrated in Figure~\ref{fig:lowerbound_plot}. 

Specifically, we denote $c:=c(\beta, d)=C_1^{\beta}\cdot \left(1-\beta/d\right)$ for some absolute constant $C_1>0$ to be specified later, and consider the following density and target function:
\begin{align*}
   \mu_{c}(\bx) & =
    \begin{cases}
        \frac{c}{\mathrm{Vol} \left(B\left(\zero, \frac{1}{4}\right)\right)} & \text{if~~}\norm{\bx} \leq \frac{1}{4} \\
        \frac{1-c}{\mathrm{Vol} \left(B\left(\zero, 1\right) \setminus B\left(\zero, \frac{3}{4}\right)\right)} & \text{if~~}\frac{3}{4} \leq \norm{\bx} \leq 1 \\ 
        0 & \text{else}
    \end{cases}
    , \\
    f^*(\bx) &=
    \begin{cases}
        -1 & \text{if~~}\norm{\bx} \leq \frac{1}{4} \\
        1 & \text{else}
    \end{cases}
    .
\end{align*}


\begin{figure}[t!]
    \centering
    \includegraphics[trim=0cm 8.5cm 0cm 8.5cm,clip=true, width=\textwidth]{Tempered_lowerbound.pdf}
    \caption{Illustration of the lower bound construction used in the proof of Theorem~\ref{thm:catastrophic}.
    When $\beta<d$, the inner circle will be misclassified as $+1$ with high probability, inducing constant error.
    }
    \label{fig:lowerbound_plot}
\end{figure}

We consider a test point $\bx$ with $\norm{\bx}\leq \frac{1}{4}$, and will show that for sufficiently large $m$, with high probability $\bx$ will be misclassified as $+1$.
This implies the desired result, since then
\[
\Lcal(\hat{h}_{\beta})\gtrsim \Pr_\bx\left[\|\bx\|\leq\tfrac{1}{4}\right]=c~.
\]
To that end, we decompose
\begin{align}
    \sum_{i=1}^m \frac{y_i}{\norm{\bx-\bx_i}^{\beta}} &=  \sum_{i:\norm{\bx_i}\leq \frac{1}{4}} \frac{y_i}{\norm{\bx-\bx_i}^{\beta}} + \sum_{i:\norm{\bx_i}\geq \frac{3}{4}} \frac{y_i}{\norm{\bx-\bx_i}^{\beta}} \nonumber\\
    &=\sum_{i:\norm{\bx_i}\leq \frac{1}{4}} \frac{y_i}{\norm{\bx-\bx_i}^{\beta}} + \sum_{i:\norm{\bx_i}\geq \frac{3}{4}} \frac{1-2p}{\norm{\bx-\bx_i}^{\beta}}
    +\sum_{i:\norm{\bx_i}\geq \frac{3}{4}} \frac{y_i-1+2p}{\norm{\bx-\bx_i}^{\beta}}
    \nonumber\\
    &\geq  -\underset{=:T_1}{\underbrace{\sum_{i:\norm{\bx_i}\leq \frac{1}{4}} \frac{1}{\norm{\bx-\bx_i}^{\beta}}}} 
    + \underset{=:T_2}{\underbrace{\sum_{i:\norm{\bx_i}\geq \frac{3}{4}} \frac{1-2p}{\norm{\bx-\bx_i}^{\beta}}}} + \underset{=:T_3}{\underbrace{\sum_{i:\norm{\bx_i}\geq \frac{3}{4}} \frac{y_i - 1 + 2p}{\norm{\bx-\bx_i}^{\beta}}}}, \label{eq: decompose}
\end{align}
where $T_1$ crudely bounds the contribution of points in the inner circle, $T_2$ is the expected contribution of outer points labeled $1$, and $T_3$ is a perturbation term.
Noting that $T_2>0$, our goal is to show that $T_2$ dominates the expression above, implying that \eqref{eq: decompose} is positive and thus $h_{\beta}(\bx)=1$.

Let $k:=\abs{\left\{i : \norm{\bx_i}\leq \frac{1}{4}\right\}}$ denote the number of points inside the inner ball, and note that we can expect $k\approx \E[k]=cm$. To bound $T_1$, we express its distribution using exponential random variables in a manner that is similar to the proof of \thmref{thm: tempered}. Specifically, for standard exponential random variables $E_1,\dots,E_{m}\overset{i.i.d.}{\sim} \exp(1)$, 
we show that
\begin{align*}
    -T_1 & 
    \geq -  \sum_{i:\norm{\bx_i}\leq \frac{1}{4}} \frac{ \left(\sum_{i=1}^m E_i\right)^{\beta/d}}{(\frac{1}{4 c^{1/d}})^{\beta}\left(\sum_{j=1}^i E_j\right)^{\beta/d}} \\
    & \gtrsim_{(1)} - c^{\beta/d}4^\beta \cdot m^{\beta/d} \cdot \sum_{i=k+1}^m \frac{1}{i^{\beta/d}}
    \\
    &\gtrsim - c^{\beta/d}4^\beta \cdot m^{\beta/d} \cdot \frac{k^{1-\beta/d}}{1- \beta/d} \\
    & \gtrsim_{(2)} -cm\cdot \frac{4^\beta}{(1-\beta/d)} ~,
\end{align*}
where $(1)$ uses various concentration bounds on the sums of exponential random variables to argue that $\sum_{j=1}^iE_j \approx i$, and $(2)$ follows from showing $k\approx cm$.

To show that $T_2$ is sufficiently large, we use the fact that $\norm{\bx - \bx_i} \leq \norm{\bx} + \norm{\bx_i} \leq \frac{5}{4}$, and that $\abs{\left\{i: \norm{\bx_i} \geq \frac{3}{4}\right\}} \approx (1-c) m \geq\half m$ with high probability to obtain
\begin{align*}
T_2=\sum_{i:\norm{\bx_i}\geq \frac{3}{4}} \frac{1-2p}{\norm{\bx-\bx_i}^{\beta}} \geq (1-2p)\abs{\left\{\bx_i: \norm{\bx_i} \geq \frac{3}{4}\right\}} \cdot \left(\frac{4}{5}\right)^{\beta}
\gtrsim m \cdot \left(\frac{4}{5}\right)^{\beta}.
\end{align*}
Lastly, we show that $T_3$ is asymptotically negligible, by noting that $\E[T_3]=0$ hence
$T_3=o(m)$ with high probability by Hoeffding's inequality.
Thus \eqref{eq: decompose} becomes
\begin{align*}
\hat{h}_\beta(\bx)
=\sign\left[\sum_{i=1}^m \frac{y_i}{\norm{\bx-\bx_i}^{\beta}}\right]
\gtrsim \sign\left[m\left(\left(\frac{4}{5}\right)^\beta - \frac{c \cdot4^\beta}{1-\beta/d}\right)
\right]
~.
\end{align*}
Overall, we see that the right-hand side above is positive as long as
\[
c=C_1^{\beta}\cdot \left(1-\frac{\beta}{d}\right) < \frac{1-\beta/d}{5^\beta}~,
\]
or equivalently $C_1<\frac{1}{5}$, meaning that $\hat{h}_\beta(\bx)=1$ even though $f^*(\bx) = -1$.

\end{proof}





\section{Experiments} \label{sec: experiment}

In this section, we provide some numerical simulations that illustrate and complement our theoretical findings.
In all experiments, we sample $m$ datapoints according to some distribution, flip each label independently with probability $p$, and plot the clean test error of the predictor $\hat{h}_\beta$ for various values of $\beta$. We ran each experiment $50$ times, and plotted the average error surrounded by a $95\%$ confidence interval.


In our first experiment, we considered data in dimension $d=1$ distributed according to the construction considered in the proof of Theorem~\ref{thm:catastrophic}. In particular, we consider
\begin{align} 
\Dcal_x&=\tfrac{1}{10}\cdot\Unif\left((0,\tfrac{1}{4})\right)+\tfrac{9}{10}\cdot\Unif\left((\tfrac{3}{4},1)\right)~, \label{eq: data 1d}
\\f^*(x)&=\begin{cases}
        -1 & \text{if~~}x\in(0,\frac{1}{4}) \\
        1 & \text{else}
    \end{cases}
~. \nonumber
\end{align}
In Figure~\ref{fig:beta_p_plot_d1}, on the left we plot the results for $m=2000$ and various values of $p$, and on the right we fix $p=0.04$ and vary $m$.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth, clip=true, trim=0 10 10 10]{beta_p_plot_m2K_largefont.png}
    \includegraphics[width=0.49\textwidth, clip=true, trim=10 10 0 10]{beta_m_plot_till2K_p004_largefont.png}
    \caption{The classification error of $\hat{h}_\beta$ for varying values of $\beta$, with data in dimension $d=1$ given by \eqref{eq: data 1d}.
    On the left, $m=2000$ is fixed, $p$ varies. On the right, $p=0.04$ is fixed, $m$ varies. Best viewed in color.
    }
    \label{fig:beta_p_plot_d1}
\end{figure}

As seen in Figure~\ref{fig:beta_p_plot_d1}, the generalization is highly asymmetric with respect to $\beta$. For $\beta<1$, the test error degrades independently of the noise level $p$, and quickly reaches $0.1$ in all cases, illustrating that the predictor errors on the negative labels (which have $0.1$ probability mass).
On the other hand, for $\beta>1$, the test error exhibits a gradual deterioration. Moreover, we see this deterioration is controlled by the noise level $p$, matching our theoretical finding.
The right figure illustrates all of the discussed phenomena hold similarly for moderate sample sizes, which complements our asymptotic analysis. 


Next, in our second experiment, we consider a similar distribution over the unit sphere $\mathbb{S}^2\subset\reals^3$, where the inner negatively labeled region is a spherical cap. In particular, consider the spherical cap defined by $A:=\left\{\bx=(x_1,x_2,x_3)\in \mathbb{S}^2 ~\mid~ x_3 > \sqrt{3}/2 \right\}$, and let
\begin{align} 
\Dcal_{\bx}&=
\tfrac{1}{10}\cdot\Unif(A)
+\tfrac{9}{10}\cdot\Unif(\mathbb{S}^2 \setminus A) 
~, \label{eq: data 2d}
\\
f^*(\bx)&=\begin{cases}
        -1 & \text{if~~} \bx\in A \\
        1 & \text{else}
    \end{cases}
~.
\nonumber
\end{align}
In Figure~\ref{fig:beta_p_plot_d2}, on the left we plot the results for $m=2000$ and various values of $p$, and on the right we fix $p=0.04$ and vary $m$.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth, clip=true, trim=0 10 10 10]{spherical_cap_plot_q01_theta0166_m2k_largefont.png}
    \includegraphics[width=0.49\textwidth, clip=true, trim=10 10 0 10]{spherical_cap_plot_q01_theta0166_p004_largefont.png}
    \caption{The classification error of $\hat{h}_\beta$ for varying values of $\beta$,
    with data on $\mathbb{S}^2\subset\reals^3$ given by \eqref{eq: data 2d}.
    On the left, $m=2000$ is fixed, $p$ varies. On the right, $p=0.04$ is fixed, $m$ varies. Best viewed in color.
    }
    \label{fig:beta_p_plot_d2}
\end{figure}


As seen in Figure~\ref{fig:beta_p_plot_d2}, the same asymmetric phenomenon holds in which overly large $\beta$ are more forgiving than overly small $\beta$, especially in low noise regimes.
The main difference between the first and second experiment is that the optimal ``benign'' exponent in the second case is $\beta=2$ matching the \emph{intrinsic} dimension of the sphere, even though the data is embedded in the $3$-dimensional space.
This illustrates that while our analysis focused on continuous distributions with a density in $\reals^d$, which makes the data of ``full dimension'', it should extend to distributions that are continuous with respect to a lower dimensional structure, such as having a density with respect to a smooth manifold. While this should not be difficult in principle, the proofs would become substantially more technical.

Although the observation above might not be too surprising, it illustrates a potential practical implication: If the data is suspected to lie in a lower-dimensional space of unknown dimension $d'$, it may be better to choose $\beta$ to equal some over-estimate of $d'$ (potentially resulting in tempered overfitting), rather than an under-estimate of $d'$ (potentially resulting in catastrophic overfitting). In other words, our results suggest that the NW estimator's exponent parameter is more tolerant to over-estimating the intrinsic dimension, rather than under-estimating it.






\section{Discussion} \label{sec: discuss}
In this work, we characterized the generalization behavior of the NW interpolator for any choice of the hyperparameter $\beta$. Specifically, NW interpolates in a tempered manner when $\beta>d$, exhibits benign overfitting when $\beta=d$, and overfits catastrophically when $\beta<d$. This substantially extends the classical analysis of this method, which only focused on consistency. In addition, it indicates that the NW interpolator is much more tolerant to over-estimating $\beta$ as opposed to under-estimating it. 

Our experiments suggest that the dependence on $d$ arises from the assumption that the distributions considered here have a density in $\reals^d$. It would thus be interesting to extend our analysis to distributions with lower dimensional structure, such as those supported over a low-dimensional manifold.

Overall, our results highlight how intricate generalization behaviors, including the full range from benign to catastrophic overfitting, can already appear in simple and well-known interpolating learning rules. We hope these results will further motivate revisiting other fundamental learning rules using this modern viewpoint, going beyond the classical consistency-vs.-inconsistency analyses. 




\subsection*{Acknowledgments}
This research is supported in part by European Research Council (ERC) grant 754705,
% DB is supported
by the Israeli Council for Higher Education (CHE) via the Weizmann Data Science Research Center and by research grants from the Estate of Harry Schutzman and the Anita James Rosen Foundation.
GK is supported by an Azrieli Foundation graduate fellowship.


\bibliographystyle{plainnat}
% \newpage
\bibliography{references}



\newpage

\appendix


\section{Notation and Order Statistics}\label{app: notation}
We start by introducing some notation
that we will use throughout the proofs to follow.
We denote $X_1\asymp X_2$ to abbreviate $X_1=\Theta(X_2)$, $X_1\lesssim X_2$ to abbreviate $X_1=\bigo(X_2)$ and $X_1\gtrsim X_2$ to abbreviate $X_1=\Omega(X_2)$.
Throughout the proofs we let $\alpha:=\beta/d$, and abbreviate $h=\hat{h}_\beta$.
Given some $\bx\in\mathrm{supp}(\mu)$ and $m\in\N$, we consider the one-dimensional random variables
\[
W_i^\bx:=V_d^{\alpha}\norm{\bx-\bx_i}^{\beta}
~,
\]
where $V_d$ is the volume of the $d$ dimensional unit ball, and the randomness is over $\bx_i$. We let $F_{\bx}$ be the CDF of $W_i^\bx$ (which is clearly the same for all $i\in[m]$). We also let $U_i\sim U([0,1]),\,i\in[m]$ be standard uniform random variables, and denote by $W_{(i)}^\bx$ and $U_{(i)}$ the ordered versions of the $W_i^\bx$s and $U_i$s respectively, namely
\begin{gather*}
W_{(1)}^\bx\leq W_{(2)}^\bx\leq\dots\leq W_{(m)}^\bx~,
\\
U_{(1)}\leq U_{(2)}\leq\dots\leq U_{(m)}~.
\end{gather*}
We will often omit the superscript/subscript $\bx$ where it is clear by context and use the notations $W_i, W_{(i)}$ and $F$ to denote $W_i^\bx, W_{(i)}^\bx$ and $F_{\bx}$ respectively.
Lastly, we let $F^{-1}:[0,1]\to[0,1],~F^{-1}(t)=\inf\{s:F(s)\geq t\}$ be the quantile function, and note that it satisfies $F(w)\leq u$ if and only if $w\leq F^{-1}(u)$.

\begin{lemma}[\citealp{devroye2006nonuniform}, Theorem 2.1] \label{lem:order_stats}
    For any $\bx\in\R^d$, $W_{(i)}^{\bx}=F_{\bx}^{-1}(U_{(i)})$ where the equality is in distribution.
\end{lemma}
Importantly, this implies that
\[
h(\bx)=\sign\left(\sum_{i=1}^m \frac{y_{(i)}}{F^{-1}(U_{(i)})}\right)
\]
where the equality is in distribution. The behavior of $U_{(i)}$ is best understood through the following lemma.

\begin{lemma}[\citealp{shorack2009empirical}, Chapter 8, Proposition 1]\label{lem:order_stat}
    Let $E_1, \ldots E_{m+1}$ be i.i.d. standard exponential random variables. Then
    \[
    \left(U_{(1)}, \ldots, U_{(m)}\right) \sim \frac{1}{\sum_{i=1}^{m+1}E_i}\left(\sum_{i=1}^1 E_i, \ldots, \sum_{i=1}^m E_i\right)~.
    \]
\end{lemma}

\section{Proof of Theorem~\ref{thm: tempered}}
\label{sec: tempered proof}
Throughout the proof, we will use the notation introduced in \appref{app: notation}.

\begin{lemma}\label{lem:cdf_approx}
    For almost every $\bx \in \R^d$ and $\epsilon > 0$, there exists $\delta_{\bx}>0$ such that for any $u\leq V_d^{\alpha} \delta_{\bx}^{\beta}:$
    \begin{align*}
        \frac{1}{(1+\epsilon)\mu(\bx)}u^{\alpha} \leq F^{-1}(u) \leq \frac{1}{(1-\epsilon)\mu(\bx)} u^{\alpha}~.
    \end{align*}
In particular, by letting $\epsilon=\half$, there exists $\delta_\bx>0$ such that for any $u\leq V_d^{\alpha} \delta_{\bx}^{\beta}:$
    \begin{align*}
        F^{-1}(u) \asymp \frac{u^{\alpha}}{\mu(\bx)}~.
    \end{align*}
\end{lemma}

\begin{proof}
By the Lebesgue differentiation theorem (cf. \citealp[Chapter 3]{stein2009real}), for almost every $\bx \in \R^d$ and $\epsilon > 0$, there exists some $\delta_\bx > 0$ such that 
\begin{align*}
    \sup_{0 < r \leq \delta_{\bx}} \abs{\frac{\int_{B(\bx,r)}\mu(\bz)d\bz}{V_dr^d} - \mu(\bx)} \leq \epsilon \mu(\bx).
\end{align*}
In particular, for any $0<u\leq V_d^{\alpha} \delta_{\bx}^{\beta}$, taking $r= \frac{u^{1/\beta}}{V_d^{1/d}}$ (which in particular satisfies $r\leq\delta_{\bx}$) we have 
\begin{align*}
    \abs{\frac{\int_{B(\bx, r)}\mu(\bz)d\bz}{u^{\frac{1}{\alpha}}} - \mu(\bx)} \leq \epsilon \mu(\bx).
\end{align*}
As a result, 
\begin{align*}
    F(w) &= \Pr_\bz \left(V_d^{\alpha}\norm{\bx -\bz}^{\beta} \leq w\right) 
    = \Pr_\bz\left(\norm{\bx - \bz} \leq \frac{w^{1/\beta}}{V_d^{1/d}}\right) \\
    &= \int_{B\left(\bx , r\right)} \mu(\bz)d\bz \in  \left[(1-\epsilon)\mu(\bx) w^{1/\alpha}, (1+\epsilon)\mu(\bx) w^{1/\alpha}\right].
\end{align*}
The result readily follows by inverting.
\end{proof}



\begin{lemma} \label{lem: eps_m}
For almost every $\bx\in\reals^d$, there exist some constants $C:=C(\bx)>0$ (which depends only on the test point $\bx$) and an absolute constant $c>0$, such that for any $m \in\N$, it holds with probability at least $1-2\exp(-cm)$ that
    \begin{align*}
        \sum_{i=1}^m \frac{y_{(i)}}{F^{-1}(U_{(i)})} ~\asymp~ \left(\sum_{j=1}^{m+1} E_j\right)^{\alpha} \cdot \left(\mu(\bx)\sum_{i=1}^m \frac{y_{(i)}}{\left(\sum_{j=1}^i E_j\right)^{\alpha}} + \epsilon_m\right) \quad \text{where} \quad \abs{\epsilon_m} \leq C \frac{1}{m^{\alpha-1}},
    \end{align*}
namely, there exists an absolute constant $C_1>0$ such that 
\begin{align*}
        \frac{1}{C_1}\cdot\sum_{i=1}^m \frac{y_{(i)}}{F^{-1}(U_{(i)})} \leq  \left(\sum_{j=1}^{m+1} E_j\right)^{\alpha} \cdot \left(\mu(\bx)\sum_{i=1}^m \frac{y_{(i)}}{\left(\sum_{j=1}^i E_j\right)^{\alpha}} + \epsilon_m\right)
        \leq C_1\cdot\sum_{i=1}^m \frac{y_{(i)}}{F^{-1}(U_{(i)})}
        ~.
    \end{align*}

\end{lemma}

\begin{proof}
By \lemref{lem:cdf_approx}, for almost every $\bx\in\R^d$ there exists some $\delta_{\bx}>0$ such that for all $U_{(i)}\leq u_0:= V_d^{\alpha}\delta_{\bx}^{\beta}:$
    \begin{align}\label{eq:helper1}
        \frac{y_{(i)}}{F^{-1}(U_{(i)})} 
        \asymp \frac{y_{(i)}\mu(\bx)}{U^\alpha_{(i)}}~.
    \end{align}
Denoting
    \begin{align*}
        \tilde \epsilon_m := \sum_{i\in[m]: U_{(i)} >  u_0} \left(\frac{y_{(i)}}{F^{-1}(U_{(i)})} -  \frac{y_{(i)}\mu(\bx)}{U_{(i)}^{\alpha}}\right)~,
    \end{align*}
    we get that

\begin{align}\label{eq:helper3}
\sum_{i=1}^m \frac{y_{(i)}}{F^{-1}(U_{(i)})}
&=\sum_{i\in[m]:U_{(i)}\leq u_0} \frac{y_{(i)}}{F^{-1}
(U_{(i)})}
+\sum_{i\in[m]:U_{(i)}> u_0} \frac{y_{(i)}}{F^{-1}
(U_{(i)})}
\nonumber\\
&\asymp\sum_{i\in[m]:U_{(i)}\leq u_0} \frac{y_{(i)}\mu(\bx)}{U_{(i)}^\alpha}
+\tilde{\epsilon}_m+\sum_{i:U_{(i)}> u_0} \frac{y_{(i)}\mu(\bx)}{U_{(i)}^\alpha}
\nonumber\\&=\mu(\bx)\sum_{i\in[m]} \frac{y_{(i)}}{U_{(i)}^\alpha}
+\tilde{\epsilon}_m 
\nonumber\\
&~=~ \mu(\bx) \left(\sum_{j=1}^{m+1} E_j\right)^\alpha \cdot \left(\sum_{i=1}^{m} \frac{y_{(i)}}{\left(\sum_{j=1}^i E_j\right)^{\alpha}}\right) + \tilde \epsilon_m.
\end{align}

It remains to bound the magnitude of $ \tilde \epsilon_m$. To that end, note that for any $i\in[m]:~U_{(i)} > u_0$ if and only if $F^{-1}(U_{(i)}) > F^{-1}(u_0)\asymp u_0^{\alpha}$. Thus,  
    \begin{align*}
        \abs{\tilde{\epsilon}_m}
        &\leq 
        \sum_{i\in[m]: U_{(i)} >  u_0} \abs{\frac{y_{(i)}}{F^{-1}(U_{(i)})} -  \frac{y_{(i)}\mu(\bx)}{U_{(i)}^{\alpha}}}
        \\
        &\leq\sum_{i\in[m]: U_{(i)} >  u_0} \abs{\frac{y_{(i)}}{F^{-1}(U_{(i)})}} +  \abs{\frac{y_{(i)}\mu(\bx)}{U_{(i)}^{\alpha}}}
        \\&\lesssim 
        (1+\mu(\bx))\sum_{i\in[m]:\,U_{(i)}> u_0} \frac{1}{u_0^{\alpha}}
        \\&\leq \frac{1+\mu(\bx)}{u_0^{\alpha}}m
        =\frac{1+\mu(\bx)}{u_0^{\alpha}}m^{\alpha}m^{1-\alpha}~.
    \end{align*}
    By a concentration result for the sum of exponential random variables given by \lemref{lem:exp_tail}, it holds with probability at least $1-2\exp(-cm)$ that $m^{\alpha}\leq \left(2 \sum_{j=1}^{m+1} E_j\right)^{\alpha}$, implying that
    \begin{align*}
        \abs{\tilde{\epsilon}_m} \lesssim \frac{1+\mu(\bx)}{u_0^{\alpha}} \left(2 \sum_{j=1}^{m+1} E_j\right)^{\alpha} \cdot m^{1-\alpha}.
    \end{align*}
    Taking $\epsilon_m$ such that $\left(\sum_{j=1}^{m+1} E_j\right)^{\alpha} \cdot m^{1-\alpha}\epsilon_m = \tilde{\epsilon}_m$ concludes the proof.

\end{proof}



\begin{lemma} \label{lem: only k matter}
For almost every $\bx\in\reals^d$ there exists a constant $\tilde{C}(\bx)$ such that as long as $m\geq \tilde{C}(\bx)$, the following holds: If 
$k\in[m]$ is such that the $k$ nearest neighbors of $\bx$ are all labeled the same $y_{(1)}=\dots=y_{(k)}$, then $h(\bx)=y_{(1)}$ with probability at least $1-c_1\exp(-c_2 k)-\exp(-c_{\alpha}k^{1-\frac{1}{\alpha}})$
over the randomness of $(\bx_i)_{i=1}^{m}$,
namely,
\[
y_{(1)}=\dots=y_{(k)}
~\implies~
\Pr_{\bx_1,\dots,\bx_m}[h(\bx)=y_{(1)}]\geq 1-c_1\exp(-c_2 k)-\exp(-c_{\alpha}k^{1-\frac{1}{\alpha}})~.
\]

\end{lemma}


\begin{proof}
Denote $s_i:=\frac{y_{(i)}}{(\sum_{j=1}^{i}E_j)^{\alpha}}$, and note that by Lemma~\ref{lem: eps_m} we get that
\begin{align*}
h(\bx)
=\sign\left(\sum_{i=1}^{m}s_i+\frac{\epsilon_m}{\mu(\bx)}\right)
=\sign\left(\sum_{i=1}^{k}s_i+\sum_{i=k+1}^{m}s_i+\frac{\epsilon_m}{\mu(\bx)}\right)
\end{align*}
with probability at least $1-2\exp(-\Omega(m))$,
where $\epsilon_m\overset{m\to\infty}{\longrightarrow}0$.
So let $m$ be sufficiently large so that $|\frac{\epsilon_m}{\mu(\bx)}|\ll \frac{2^{\alpha}}{(\alpha-1)k^{\alpha-1}}$.
By applying Lemma~\ref{lem:exp_tail}, we get that
with probability at least $1-c_1\exp(-c_2 k)$,
for all $i\geq k+1:~|s_i|\leq \frac{1}{(i/2)^{\alpha}}$, and therefore under this event we get
\[
\left|\sum_{i=k+1}^{m}s_i\right|\leq 2^{\alpha}\sum_{i=k+1}^{\infty}\frac{1}{i^{\alpha}}
\leq 2^{\alpha}\int_{k}^{\infty}t^{-\alpha}dt
=\frac{2^{\alpha}}{(\alpha-1)k^{\alpha-1}}~.
\]
It therefore remains to show that with high probability $\left|\sum_{i=1}^{k}s_i\right|\gg \frac{2^{\alpha}}{(\alpha-1)k^{\alpha-1}}$. To that end, a loose way to obtain this bound is by
considering the event in which even just $s_1$ is sufficiently large:
\[
\left|\sum_{i=1}^{k}s_i\right|
=\left|y_{(1)}\sum_{i=1}^{k}\frac{1}{(\sum_{j=1}^{i}E_j)^{\alpha}}\right|
=\sum_{i=1}^{k}\frac{1}{(\sum_{j=1}^{i}E_j)^{\alpha}}
\geq \frac{1}{E_1^{\alpha}}~,
\]
and noting that the latter is at least $\frac{2^{\alpha}}{(\alpha-1)k^{\alpha-1}}$ as long as $E_1\leq \frac{(\alpha-1)^{\frac{1}{\alpha}}k^{1-\frac{1}{\alpha}}}{2}$, which occurs with probability $1-\exp\left[-\frac{(\alpha-1)^{\frac{1}{\alpha}}}{2}k^{1-\frac{1}{\alpha}}\right]$.
\end{proof}








\begin{lemma} \label{lem: k neighbors match}
    Given $\bx \in \R^d$, let $A^k_\bx$ be the event in which all of $\bx$'s $k$ nearest neighbors $(\bx_{(i)})_{i=1}^{k}$ satisfy $f^*(\bx_i)= f^*(\bx)$. Then for any fixed $k$, it holds
    for almost every $\bx\in\mathrm{supp}(\mu)$ 
    that $\lim_{m\to\infty}\Pr[A^k_\bx]=1$.
\end{lemma}

\begin{proof}
Let $\bx\in\mathrm{supp}(\mu)$ be such that $\mu$ is continuous at $\bx$ (which holds for a full measure set by assumption). Since $\mu(\bx)>0$, then there exists $\rho>0$ so that $\mu|_{B(\bx,\rho)}>0$, and assume $\rho$ is sufficiently small so that $f^*|_{B(\bx,\rho)}=f^*(\bx)$. Note that $B(\bx,\rho)$ has some positive probability mass which we denote by $\phi:=\int_{B(\bx,\rho)}\mu$. 
    Under this notation, we see that
    \[
    \Pr[\lnot A^k_\bx]
    \leq\Pr\left[|\{\bx_1,\dots,\bx_m\}\cap B(\bx,\rho)|<k\right]
    =\Pr[\mathrm{Binomial}(m,\phi)<k]\overset{m\to\infty}{\longrightarrow}0
    ~.
    \]
\end{proof}

\noindent
\textbf{Proof of Theorem~\ref{thm: tempered}\quad}
We start by proving the upper bound.
Let $k:=\log^{\frac{\alpha}{\alpha-1}}(1/p)$, and for any $\bx\in\R^d$, consider the event $A^k_\bx$ in which $\bx$'s $k$ nearest neighbors $(\bx_{(i)})_{i=1}^{k}$ satisfy $f^*(\bx_i)= f^*(\bx)$
(as described in Lemma~\ref{lem: k neighbors match}).
Using the law of total expectation, we have that
\begin{align}
\E_S\left[\Pr_\bx(h(\bx)\neq f^*(\bx))\right] \nonumber
&=\E_S\E_{\bx}[\one{h(\bx)\neq f^*(\bx)}]
\\&=\E_\bx\E_S\left[\one{h(\bx)\neq f^*(\bx)}\right] \nonumber
\\
&=\E_\bx \left[ \E_S\left[\one{h(\bx)\neq f^*(\bx)}\mid  A^k_\bx\right]\cdot \Pr_S[A^k_\bx]\right] \nonumber
\\&~~~+\E_\bx\left[\E_S\left[\one{h(\bx)\neq f^*(\bx)}\mid\lnot A^k_\bx\right]\Pr_S[\lnot A^k_\bx]  \right] \label{eq: total E}
\\
&\leq \E_\bx\E_S\left[\one{h(\bx)\neq f^*(\bx)}\mid  A^k_\bx\right]+\E_\bx\Pr_S[\lnot A^k_\bx]~. \nonumber
\end{align}
Note that by Lemma~\ref{lem: k neighbors match} $\lim_{m\to\infty}\Pr_S[\lnot A^k_\bx]=0$, and therefore it remains to bound the first summand above.

To that end, we continue by temporarily fixing $\bx$. Denote by 
$B^k_\bx$ the event in which $\bx$'s $k$ nearest neighbors are all labeled correctly (namely, their labels were not flipped),
and note that $\Pr_S[B^k_\bx]=(1-p)^k\geq 1-kp$, hence $\Pr_S[\lnot B^k_\bx]<kp$.
By Lemma~\ref{lem: only k matter} we also know that for sufficiently large $m:$
\[
\Pr_{S}[h(\bx)\neq f^*(\bx)\mid A^k_\bx,B^k_\bx]\leq c_1\exp(-c_2 k)+\exp(-c_{\alpha}k^{1-\frac{1}{\alpha}})~.
\]
Therefore,
\begin{align*}
    \E_S[\one{h(\bx)\neq f^*(\bx)}\mid  A^k_\bx]
    &=\Pr_S[h(\bx)\neq f^*(\bx)\mid  A^k_\bx]
    \\&=\Pr_S[h(\bx)\neq f^*(\bx)\mid A^k_\bx,B^k_\bx]\cdot \Pr_S[B^k_\bx]
    \\&~~~+\Pr_S[h(\bx)\neq f^*(\bx)\mid  A^k_\bx,\lnot B^k_\bx]\cdot \Pr_S[\lnot B^k_\bx]
    \\
    &\leq \left(c_1\exp(-c_2 k)+\exp(-c_{\alpha}k^{1-\frac{1}{\alpha}})\right)\cdot 1+1\cdot kp
    \\
    &\leq C_{\alpha}\log^{\frac{\alpha}{\alpha-1}}(1/p)\log(p)
    ~,
\end{align*}
where the last inequality follows by our assignment of $k$.
Since this is true for any $\bx$, it is also true in expectation over $\bx$, thus completing the proof of the upper bound.

We proceed to prove the lower bound. We consider $A^k_\bx$ to be the same event as before, yet now we set $k:=k_\alpha=\left(\frac{2^{\alpha}}{\alpha-1}\right)^{\frac{1}{\alpha-1}}
$. By lower bounding \eqref{eq: total E} (instead of upper bounding it as before), we obtain 
\begin{align*}
\E_S\left[\Pr_\bx(h(\bx)\neq f^*(\bx))\right]
\geq 
\E_\bx [\underset{(\star)}{\underbrace{\E_S [\one{h(\bx)
\neq f^*(\bx)}\mid  A^k_\bx ]}}\cdot \Pr_S[A^k_\bx] ]-\E_\bx\Pr_S[\lnot A^k_\bx]~.
\end{align*}
As 
$\lim_{m\to\infty}\Pr_S[A^k_\bx]=1$ and
$\lim_{m\to\infty}\Pr_S[\lnot A^k_\bx]=0$
by Lemma~\ref{lem: k neighbors match}, it once again remains to bound $(\star)$.

To that end, we temporarily fix $\bx$, denote by 
$D^k_\bx$ the event in which the labels of $\bx$'s $k$ nearest neighbors were are all flipped. Note that since the label flips are independent of the location of the datapoints, it holds that
$\Pr_S[D^k_\bx\mid A_\bx^k]=\Pr_S[D^k_\bx]=p^k$.
By Lemma~\ref{lem: only k matter} we also know that for sufficiently large $m:$
\[
\Pr_{S}[h(\bx)\neq f^*(\bx)\mid A^k_\bx,D^k_\bx]\geq 1-c_1\exp(-c_2 k)-\exp(-c_{\alpha}k^{1-\frac{1}{\alpha}})~.
\]
Therefore,
\begin{align*}
    \E_S[\one{h(\bx)\neq f^*(\bx)}\mid  A^k_\bx]
    &=\Pr_S[h(\bx)\neq f^*(\bx)\mid  A^k_\bx]
    \\&\geq\Pr_S[h(\bx)\neq f^*(\bx)~\land~D_\bx^k\mid A^k_\bx]
    \\
    &=\Pr_S[h(\bx)\neq f^*(\bx)\mid A^k_\bx,D_\bx^k]\cdot \Pr[D_\bx^k\mid A^k_\bx]
    \\
    &\geq \left(1-c_1\exp(-c_2 k)-\exp(-c_{\alpha}k^{1-\frac{1}{\alpha}})\right)p^k
    \\
    &\geq c_\alpha p^{k}
    ~,
\end{align*}
is due to our assignment of $k$ (and the explicit form of $c_\alpha$ in Lemma~\ref{lem: only k matter}).





\section{Proof of Theorem~\ref{thm:catastrophic}}
\label{sec: catastrophic proof}

\paragraph{Setting for the proof. }\label{set:app_catastrophic}
Throughout the proof, we will use the notation introduced in \appref{app: notation}. We start by specifying the target function and distribution for which we will prove that catastrophic overfitting occurs. We will consider a slightly more general version than mentioned in the main text. Fix $R, r, c >0$ that satisfy $R > 3r$. We define a distribution on $B(\zero, R)$ whose density is given by 
\begin{align*}
\mu(\bx) ~:=~
\begin{cases}
    \frac{c}{\mathrm{Vol} \left(B(\zero, r)\right)} & \norm{\bx} < r \\
    \frac{1-c}{\mathrm{Vol} \left(B(\zero, R) \setminus B(\zero, 3r)\right)} & 3r \leq \norm{\bx} \leq R \\ 
    0 & \text{else}
\end{cases}
~=~
\begin{cases}
    \frac{c}{V_dr^d} & \norm{\bx} < r \\
    \frac{1-c}{V_d\cdot(R^d-(3r)^d)} & 3r \leq \norm{\bx} \leq R \\
    0 & \text{else}
\end{cases}
,
\end{align*}
where $V_d$ is the volume of the $d$-dimensional unit ball. We also define the target function
\[
f^*(\bx):=
\begin{cases}
    -1 & \norm{\bx} \leq r \\
    1 & \text{else}
\end{cases}~.
\]
The main lemma from we derive the proof of Theorem~\ref{thm:catastrophic} is the following:

\begin{lemma}\label{lem:catastrophic_probability}
    Under setting \ref{set:app_catastrophic} suppose that $c$ satisfies
    \begin{align*}
        c \leq \frac{1-\beta/d}{2400 \left(1 + \frac{R}{r}\right)^{\beta}}.
    \end{align*} 
    Then there exists some $m_0\in\N$, such that for any $\bx\in B(\zero, r)$, $m>m_0$ and $p \in (0,0.49)$, it holds with probability at least $1-\tilde \bigO_{m} \left(\frac{1}{m} + \frac{1}{m^{\frac{1-\beta/d}{\beta/d}}}\right)$ over the randomness of the training set $S$ that 
    \begin{align*}
        \hat{h}_{\beta}\left(\bx\right) = 1.
    \end{align*}
\end{lemma}


We temporarily defer the proof of Lemma~\ref{lem:catastrophic_probability}, and start by showing that
it easily implies the theorem:


\begin{proof}\textbf{ of Theorem \ref{thm:catastrophic}}
Fix $R>3r$, let $c=\frac{1-\beta/d}{2400(1+\frac{R}{r})^{\beta}}$ and consider the distribution and target function given by Setting \ref{set:app_catastrophic}. Using the law of total expectation, we have that
    \begin{align*}
    \E_S\left[\Pr_\bx(h(\bx)\neq f^*(\bx))\right]
    & = \E_S\E_{\bz}[\one{h(\bz)\neq f^*(\bz)}] \\
    & = \E_\bz\E_S\left[\one{h(\bz)\neq f^*(\bz)}\right] \\
    & \geq \E_\bz\left[\E_S\left[\one{h(\bz)\neq f^*(\bz)}\right] \mid \bz \in B(\zero, r)\right] \cdot \Pr\left(\bz \in B(\zero, r) \right) \\
    & = \E_\bz\left[\Pr_S\left(\one{h(\bz)\neq f^*(\bz)} \mid \bz \in B(\zero, r)\right)\right] \cdot \Pr\left(\bz \in B(\zero, r) \right) \\
    & \geq_{(*)} c\left(1-\tilde \bigO_{m} \left(\frac{1}{m} + \frac{1}{m^{\frac{1-\beta/d}{\beta/d}}}\right)\right)
    \end{align*} 
    where $(*)$ follows from \lemref{lem:catastrophic_probability}. This completes the proof by sending $m \to \infty$.
\end{proof}



\subsection{Proof of Lemma~\ref{lem:catastrophic_probability}}


Fix some $\bx$ with $\norm{\bx}< r$, we will show that for sufficiently large $m$, with high probability $\bx$ will be misclassified as $+1$.
To that end, we decompose
\begin{align}
    \sum_{i=1}^m \frac{y_i}{\norm{\bx-\bx_i}^{\beta}} &=  \sum_{i:\norm{\bx_i}\leq r} \frac{y_i}{\norm{\bx-\bx_i}^{\beta}} + \sum_{i:\norm{\bx_i}\geq 3r} \frac{y_i}{\norm{\bx-\bx_i}^{\beta}} \nonumber\\
    &=\sum_{i:\norm{\bx_i}\leq r} \frac{y_i}{\norm{\bx-\bx_i}^{\beta}} + \sum_{i:\norm{\bx_i}\geq 3r} \frac{1-2p}{\norm{\bx-\bx_i}^{\beta}}
    +\sum_{i:\norm{\bx_i}\geq 3r} \frac{y_i-1+2p}{\norm{\bx-\bx_i}^{\beta}}
    \nonumber\\
    &\geq  -\underset{=:T_1}{\underbrace{\sum_{i:\norm{\bx_i}\leq r} \frac{1}{\norm{\bx-\bx_i}^{\beta}}}} 
    + \underset{=:T_2}{\underbrace{\sum_{i:\norm{\bx_i}\geq 3r} \frac{1-2p}{\norm{\bx-\bx_i}^{\beta}}}} - \underset{=:T_3}{\underbrace{\abs{\sum_{i:\norm{\bx_i}\geq 3r} \frac{y_i - 1 + 2p}{\norm{\bx-\bx_i}^{\beta}}}}},
\end{align}
where $T_1$ crudely bounds the contribution of points in the inner circle, $T_2$ is the expected contribution of outer points labeled $1$, and $T_3$ is a perturbation term. Let $k_m:=\abs{\{i\in[m] \mid \norm{\bx_i}\leq r\}}$ denote the number of training points inside the inner ball. By \lemref{lem:k_m_bound}, whenever $c\leq \frac{1}{2}$ (we will ensure this happens) it holds with probability at least $1-2\exp\left(-\frac{m}{8}\right)$ that 
\begin{align}\label{eq:k_m_helper}
    \frac{cm}{2} \leq k_m \leq \frac{3cm}{2} \leq \frac{3m}{4}.   
\end{align} 
The rest of the proof is conditioned on this event occurring. 

\emph{Bounding $T_1$: } Using that  $\norm{\bx}< r$ and that the pdf $\mu$ is such that for all $\bx_i\notin B(\zero, r)$, $\norm{\bx - \bx_i}> 3r - r > 2r$, we have that the $k_m$ nearest neighbors $\bx_{(1)},\ldots, \bx_{(k_m)}$ are precisely the points with $\norm{\bx_i}\leq r$.

For any $w\leq (2r)^{\beta}$ and any $\bz\in B\left(\bx , w^{\frac{1}{\beta}}\right)$ it holds that $\norm{\bz}\leq 3r$, and $\mu(\bx)\leq \frac{c}{V_d r^d}$. Thus, for such a $w$,
    \begin{align*}
        F(w) :=& \Pr_\bz \left(\norm{\bz -\bx}^{\beta} \leq w\right) 
        = \Pr_\bz\left(\norm{\bz - \bx} \leq w^{\frac{1}{\alpha d}}\right) = \int_{B\left(\bx , w^{\frac{1}{\alpha d}}\right)} \mu(\bx)d\bz \\
        \leq& \int_{B\left(\bx , w^{\frac{1}{\alpha d}}\right)} \frac{c}{V_d r^d}  d\bz = \frac{c}{r^d} w^{\frac{1}{\alpha}}.
    \end{align*}
    Correspondingly, by substituting $u=\frac{c}{r^d} w^{\frac{1}{\alpha}}$, we obtain for any $u\leq 2^d c$ that $u \geq F\left(\frac{u^{\alpha}r^{\alpha d}}{c^\alpha}\right)$ and thus $F^{-1}(u) \geq \frac{u^{\alpha}r^{\alpha d}}{c^\alpha}$. Note that for any $i\in[k_m]$, $\norm{\bx - \bx_{(i)}}^\beta < (2r)^{\alpha d}$ so $W_{(i)}$ satisfies the condition that $W_{(i)} \leq (2r)^{\alpha d}$. As such, using \lemref{lem:order_stats} we obtain 
    \begin{align} \label{eq:inv_cdf_bound}
        \forall i\in[k_m], \qquad W_{(i)}=F^{-1}(U_{(i)}) \geq \frac{U_{(i)}^{\alpha}r^{\alpha d}}{c^\alpha}.
    \end{align}
    
    Now for $T_1$, we have from \eqref{eq:inv_cdf_bound}:
    \begin{align*}
        -T_1 \geq & -\frac{c^\alpha}{r^{\alpha d}} \sum_{i=1}^{k_m} \frac{1}{U_{(i)}^{\alpha}} 
        \geq_{(1)} -\frac{2\cdot 3^\alpha}{(1-\alpha)}\frac{c^\alpha}{r^{\alpha d}} \cdot m^{\alpha} k_m^{1-\alpha} 
        \geq_{(2)} -\frac{2\cdot 3^\alpha}{(1-\alpha)}\frac{c^\alpha}{r^{\alpha d}} \cdot m^{\alpha} \left(\frac{3cm}{2}\right)^{1-\alpha} 
        \\
        =& -m\cdot \frac{2^{\alpha} \cdot 3}{(1-\alpha)r^{\alpha d}} \cdot c,
    \end{align*}
    where $(1)$ holds by \lemref{lem:sum_order} with probability at least $1-\tilde \bigO_{k_m} \left(\frac{1}{k_m} + \frac{1}{k_m^{\frac{1-\alpha}{\alpha}}}\right)=\bigO_{m} \left(\frac{1}{m} + \frac{1}{m^{\frac{1-\alpha}{\alpha}}}\right)$ and $(2)$ follows from \eqref{eq:k_m_helper}.

\emph{Bounding $T_2$: } Using the fact that for any $i\in[m]$, $\norm{\bx - \bx_{i}} \leq \norm{\bx} + \norm{\bx_i} \leq R+r$, and the bound on $k_m$ from \eqref{eq:k_m_helper}, we have for any $p < 0.49$ that
    \begin{align*}
        T_2 \geq \frac{(1-2p)(m-k_m)}{(R+r)^{\alpha d }} 
        \geq \frac{(1-2p)}{(R+r)^{\alpha d }} \cdot \left(m-\frac{3}{4}m\right) > m \cdot \frac{1}{200(R+r)^{\alpha d }}.
    \end{align*}

\emph{Bounding $T_3$: } From \lemref{lem:y_pert} and \eqref{eq:k_m_helper}, it holds with probability at least $1 - 2\exp\left(-\sqrt{\frac{m}{4}}\right)$ that 
    \begin{align*}
        T_3 \leq \frac{(m-k_m)^{\frac{3}{4}}}{(2r)^{\alpha d}} 
        \leq m^{\frac{3}{4}} \cdot \frac{1}{(2r)^{\alpha d}}.
    \end{align*}

    
\emph{Putting it Together: }  For any $\epsilon > 0$ there is some $m_0 \in \N$, such that for any $m>m_0$, $-T_3 \geq -m\epsilon$.
    So overall, we obtain that with probability at least $1-\tilde \bigO_{m} \left(\frac{1}{m} + \frac{1}{m^{\frac{1-\alpha}{\alpha}}}\right)$,
    \begin{align*}
        \frac{1}{m}\sum_{i=1}^m \frac{y_i}{\norm{\bx-\bx_i}^{\alpha d}} \geq \frac{1}{m}(-T_1 + T_2 - T_3) 
        > & - \frac{2^{\alpha} \cdot 3}{(1-\alpha)r^{\alpha d}} \cdot c + \frac{1}{200(R+r)^{\alpha d }} - \epsilon \\ 
        \geq & - \frac{6}{(1-\alpha)r^{\alpha d}} \cdot c + \frac{1}{400(R+r)^{\alpha d }}, 
    \end{align*}
    where the last line follows by using that $\alpha < 1$, and by fixing some sufficiently small $\epsilon$.
    Finally, fixing some $c \leq \frac{(1-\alpha)r^{\alpha d}}{2400 (R+r)^{\alpha d}} = \frac{1-\alpha}{2400 \left(1 + \frac{R}{r}\right)^{\alpha d}}$ suffices to ensure that this is positive, implying $\hat{h}_{\beta}(\bx)=1$.


\begin{lemma}\label{lem:y_pert}
    Under Setting \ref{set:app_catastrophic}, let $\bx\in B(\zero, r)$ and  $k_m:=\abs{\{i\in[m] \mid \norm{\bx_i}\leq r\}}$. It holds with probability at least $1-2\exp\left(-\sqrt{m-k_m}\right)$ that 
     \begin{align*}
         \abs{\sum_{i:\norm{\bx_i}\geq 3r} \frac{y_i - 1 + 2p}{\norm{\bx-\bx_i}^{\beta}}} \leq \frac{(m-k_m)^{\frac{3}{4}}}{(2r)^{\alpha d}}.
     \end{align*}
\end{lemma}
\begin{proof}
    Let $\xi_i$ be the random variable representing a label flip, meaning that $\xi_i$ is $1$ with probability $p$ and $-1$ with probability $1-p$, and $y_i = f^*(\bx_i)\xi_i$ by assumption. For any $\bx_i$ with $\norm{\bx_i} \geq 3r$, it holds that $f^*(\bx_i)=1$, and that $\norm{\bx - \bx_i} \geq \norm{\bx_i} - \norm{\bx} \geq 2r$, and thus $\frac{y_i}{\norm{\bx - \bx_i}^{\alpha d}}$ are bounded as
    \begin{align*}
        \abs{\frac{y_i}{\norm{\bx - \bx_i}^{\alpha d}}} \leq \frac{1}{(2r)^{\alpha d}}.
    \end{align*}
    
    We thus apply Hoeffding's Inequality (cf. \citealp[Theorem 2.2.6]{vershynin2018high}) yielding that for any $t\geq 0$
    \begin{align*}
        \Pr\left(\abs{\sum_{i:\norm{\bx_i}\geq 3r} \frac{y_i}{\norm{\bx-\bx_i}^{\alpha d}} - \sum_{i:\norm{\bx_i}\geq 3r} \frac{1-2p}{\norm{\bx-\bx_i}^{\alpha d}}} \geq t\right) \leq & 2\exp\left(-\frac{t^2 (2r)^{2\alpha d}}{2(m-k_m)}\right).
    \end{align*}
     In particular, we have that with probability at least $1-2\exp\left(-\frac{1}{2}\sqrt{m-k_m}\right)$ that
     \begin{align*}
         \abs{\sum_{i:\norm{\bx_i}\geq 3r} \frac{y_i}{\norm{\bx-\bx_i}^{\alpha d}} - (1-2p)\sum_{i:\norm{\bx_i}\geq 3r} \frac{1}{\norm{\bx-\bx_i}^{\alpha d}}} \leq \frac{(m-k_m)^{\frac{3}{4}}}{(2r)^{\alpha d}}.
     \end{align*}
\end{proof}

\begin{lemma}\label{lem:k_m_bound}
    Under setting \ref{set:app_catastrophic}, let $k_m:=\abs{\{i: \norm{\bx_i} \leq r\}}$, then it holds with probability at least $1-2\exp(-\frac{c^2m}{2})$ that
    \begin{align*}
        \frac{cm}{2}\leq k_m \leq \frac{3cm}{2}
    \end{align*}
\end{lemma}
\begin{proof}
    We can rewrite $k_m=\sum_{i=1}^m B_i$ where $B_i=1$ if $\norm{\bx_i}\leq r$ and $0$ otherwise. Notice that each $B_i$ is a Bernoulli random variable with parameter $c$, i.e $B_i$ is $1$ with probability $c$ and $0$ with probability $1-c$. So by Hoeffding's inequality (cf. \citealp[Theorem 2.2.6]{vershynin2018high}), we have for any $t\geq 0$ that
    \begin{align*}
        \Pr\left(\abs{\sum_{i=1}^m B_i - cm} \geq t\right) \leq \exp\left(-\frac{2t^2}{m}\right).
    \end{align*}
    Taking $t=\frac{cm}{2}$ concludes the proof. 
\end{proof}

\begin{lemma}\label{lem:sum_order}
    It holds for any $k\leq m\in\N$, $0<\alpha<1$ that with probability at least $1-\tilde \bigO_{k} \left(\frac{1}{k} + \frac{1}{k^{\frac{1-\alpha}{\alpha}}}\right)$, 
    \begin{align*}
        \sum_{i=1}^{k}\frac{1}{U_{(i)}^{\alpha}} \leq \frac{2\cdot 3^\alpha}{1-\alpha}.
    \end{align*}
\end{lemma}
\begin{proof}
    Fix some $n_0\leq k$ which will be specified later. Using \lemref{lem:order_stat}, we can write
    \begin{align} \label{eq:helper_sum}
        \sum_{i=1}^{k} \frac{1}{U_{(i)}^{\alpha}} 
        =& \left(\sum_{i=1}^m E_i \right)^{\alpha}\left(\sum_{i=1}^{k}\frac{1}{\left(\sum_{j=1}^i E_j\right)^\alpha}\right) \nonumber
        \\
        =& \underset{:=T_1}{\underbrace{\left(\sum_{i=1}^m E_i \right)^{\alpha}}} 
        \left(
        \underset{:=T_2}{\underbrace{\sum_{i=1}^{n_0}\frac{1}{\left(\sum_{j=1}^i E_j\right)^\alpha}}} +
        \underset{:=T_3}{\underbrace{\sum_{i=n_0}^{k}\frac{1}{\left(\sum_{j=1}^i E_j\right)^\alpha}}}
        \right).
    \end{align}
    
    By \lemref{lem:exp_tail}, for some absolute constant $C>0$ it holds with probability $1-2(1+\frac{1}{C})\exp(-Cn_0)$ that for all $n \geq n_0,$ 
    \begin{align}\label{eq:e_i_helper}
        \frac{1}{2} \leq \sum_{i=1}^n E_i \leq \frac{3n}{2}.
    \end{align}
    Conditioned on this even occurring, we use this to bound both $T_1$ and $T_3$. For $T_1$, \eqref{eq:e_i_helper} directly implies that $T_1 \leq \left(\frac{3}{2}m\right)^{\alpha}$. For $T_3$, using both \eqref{eq:e_i_helper} as well as the integral test for convergence we obtain
    \begin{align*}
        T_3 \leq 2^\alpha \sum_{i=n_0}^{k} \frac{1}{i^{\alpha}} \leq 2^\alpha \int_{n_0-1}^k \frac{1}{i^{\alpha}} \leq 2^\alpha \frac{k^{1-\alpha} - (n_0-1)^{1-\alpha}}{1-\alpha}.
    \end{align*}
    
    It remains to bound $T_2$. By definition of an exponential random variable, for any $t\geq 0$ it holds for any $E_i$ with probability at least $\exp(-t)$ (which is $\geq 1-t$) that $E_i \geq t$. So taking $t=\left(\frac{n_0}{k^{1-\alpha}}\right)^{\frac{1}{\alpha}}$, it holds with probability at least $1- \left(\frac{n_0}{k^{1-\alpha}}\right)^{\frac{1}{\alpha}}$ that $E_1 \geq \left(\frac{n_0}{k^{1-\alpha}}\right)^{\frac{1}{\alpha}}$. As a result,
    \begin{align}\label{eq:t2_helper}
        T_2 \leq n_0 \cdot \frac{1}{E_1^{\alpha}} \leq n_0 \cdot \frac{1}{\left(\frac{n_0}{k^{1-\alpha}}\right)} = k^{1-\alpha}.
    \end{align}

    To ensure that the probability that both \eqref{eq:e_i_helper} and \eqref{eq:t2_helper} hold is sufficiently high, we take $n_0=\max\left(\frac{1}{C}\log(k), 2\right)$. As such, we obtain that with probability at least $1-\tilde \bigO \left(\frac{1}{k} + \frac{1}{k^{\frac{1-\alpha}{\alpha}}}\right)$ that \eqref{eq:helper_sum} can be bounded as
    \begin{align*}
        \sum_{i=1}^{k} \frac{1}{U_{(i)}^{\alpha}} = & T_1\left(T_2 + T_3\right) \leq \left(\frac{3}{2}m\right)^\alpha \left(2^\alpha \frac{k^{1-\alpha} - (n_0-1)^{1-\alpha}}{1-\alpha} + k^{1-\alpha}\right) \\ 
        \leq & \left(\frac{3}{2}m\right)^\alpha \cdot k^{1-\alpha} \left( \frac{2^\alpha}{1-\alpha} + 1\right) 
        \leq \left(\frac{3}{2}m\right)^\alpha \cdot k^{1-\alpha} \cdot 2\cdot \frac{2^\alpha}{1-\alpha} \\
        \leq & \frac{2\cdot 3^\alpha}{1-\alpha} \cdot m^{\alpha} k^{1-\alpha}.
    \end{align*}
\end{proof}












\section{Auxiliary Lemma}

\begin{lemma}\label{lem:exp_tail}
Suppose $(E_i)_{i\in\NN}\overset{iid}{\sim}\exp(1)$ are standard exponential random variables.
Then there exists some absolute constant $C>0$ such that:
    \begin{enumerate}
        \item For any $n\in\N$ it holds that
        \[
        \Pr\left(\frac{n}{2} \leq \sum_{i=1}^n E_i \leq \frac{3n}{2}\right) \geq 1- 2\exp(-Cn)~.
        \]
        \item For any $n_0\in\N$ it holds that 
        \[
        \Pr\left(\bigcap_{n=n_0}^\infty \left[\frac{n}{2} \leq \sum_{i=1}^n E_i \leq \frac{3n}{2}\right]\right) \geq 1 - 2\left(1+\frac{1}{C}\right)\exp(-Cn_0)~.
        \]
    \end{enumerate}
\end{lemma}

\begin{proof}
    Denote by $\norm{\,\cdot\,}_{\psi_1}$ the sub-exponential norm of a random vector (for a reminder of the definition, see for example \citealt[Definition 2.7.5]{vershynin2018high}). Each $E_i$ satisfies for any $t> 0$, $\Pr\left(E_i \geq t\right) \leq \exp(-t)$ implying that $\norm{E_i}_{\psi_1}=1$. By \citet[Remark 5.18]{vershynin2010introduction}, this implies $\norm{E_i - 1}_{\psi_1}\leq 2$. So Bernstein's inequality for sub exponential random variables \citep[Corollary 2.8.3]{vershynin2018high} states that there exists some absolute constant $C'>0$ such that for any $t\geq 0$
    \[
    \Pr\left(\abs{\left(\frac{1}{n}\sum_{i=1}^nE_i\right) - 1}\geq t\right) \leq 2\exp\left(-C'\min\left(\frac{t^2}{4}, \frac{t}{2}\right)n\right).
    \]
    Taking $t=\frac{1}{2}$ and taking $C:=\frac{C'}{16}$ yields 
    \[
    \Pr\left(\abs{\sum_{i=1}^n E_i - n} \geq \frac{n}{2}\right) \leq 2\exp\left(-Cn\right).
    \]
This proves the first statement. For the second statement, we union bound and apply the integral test for convergence, to get that
    \begin{align*}
        \Pr\left(\bigcup_{n=n_0}^\infty \left[\abs{\sum_{i=1}^n E_i - n} \geq \frac{n}{2}\right]\right) &\leq \sum_{n=n_0}^\infty \Pr\left(\abs{\sum_{i=1}^n E_i - n} \geq \frac{n}{2}\right) \\
        &\leq  2\sum_{n=n_0}^\infty \exp\left(-Cn\right)
        \\
        &\leq 2\exp(-Cn_0) + 2\int_{n_0}^\infty \exp(-Cn_0) \\
        &\leq  2\exp(-Cn_0) + \frac{2}{C}\exp(-Cn_0)~.
    \end{align*}
\end{proof}




    

\end{document}