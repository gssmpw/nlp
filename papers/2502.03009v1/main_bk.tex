
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{array}
\usepackage{booktabs} %
\usepackage{multirow}
\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{icml2024}
\usepackage{natbib}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{lipsum}                     %
\usepackage{xargs}                      %
\usepackage[pdftex,dvipsnames]{xcolor}  %
\usepackage{etoolbox}

\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}

\newenvironment{chatgpt}
{%
  \begingroup
  \textbf{\textcolor{blue}{ChatGPT generated: }}
  \color{blue}%
}
{%
\textbf{\textcolor{blue}{ ChatGPT generated.}}
  \endgroup
}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Scaling Laws for Upcycling MoE}

\begin{document}

\twocolumn[
\icmltitle{Scaling Laws for Upcycling Mixture-of-Experts Language Models}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]




\begin{abstract}
This document provides a basic paper template and submission guidelines.
If using this (optional) template, your short abstract should go here.
\end{abstract}


\section{Introduction}
\label{sec:introduction}
Large-scale neural network architectures, particularly dense transformers \cite{vaswani2017attention}, have seen remarkable success across a wide range of tasks, particularly achieving human-level capabilities in natural language processing \cite{achiam2023gpt}.
However, they often demand an enormous amount of computation, imposing challenges of computational efiiciency and scalability.
Sparse models, specifically mixture-of-experts (MoE) architectures \cite{shazeer2017outrageously,lepikhin2020gshard,fedus2022switch}, have emerged as an alternative achieving better efficiency-performance tradeoff via partial activation of neural parameters when processing the input.
Even so, MoEs still require substantial training to reach their full potential.

One direction to further accelerate training convergence is leveraging pretrained models to guide the training of MoEs.
A particularly simple method within this context is sparse upcycling \cite{komatsuzaki2022sparse}, or simply upcycling.
It works by first initializing the MoE by duplicating the multi-layer perceptron (MLP) layers, and copying other layers' weights of the dense checkpoint, followed by continual-pretraining the MoE.

Despite the promise of efficient MoE training via upcycling, the effectiveness and limitations of this technique remain unclear.
On one hand, some works \cite{wei2024skywork,he2024upcycling} already adopted it to train large-scale MoEs.
On the other hand, Muennighoff et al. \cite{muennighoff2024olmoe} mentioned that upcycling may slow down training convergence.
We believe these seemingly contradictory conclusions are due to a lack of comprehensive studies and assessments on when and how to upcycle MoEs to expedite training, hampering a wider adoption of this technique for efficient MoE pretraining.
\textcolor{blue}{\textbf{ add more desription}}

In this paper, we seek to better understand the technique of upcycling MoEs and establish guidelines of using it in practice.
We first ran extensive (hundreds) experiments up to a few hundred billion (B) training tokens and a few billion total model parameters, to establish the existence of new scaling behaviors that incorporate the number of tokens used to train the dense/upcycled MoE, and the number of active MoE parameters.
The scaling laws allow us to answer the following practical questions: 
\begin{itemize}
    \item How should we scale model size and data (training tokens) for compute-optimal upcycling? 
    \item How does the number of tokens used to train a dense model affect the upcycling effectiveness at accelerating convergence? 
\item Given a fixed total compute, or floating-point operations (FLOPs), how should we allocate them between training the dense model and the upcycled MoE? 
\end{itemize}
\textbf{Notations.}
Before giving the summary of our main results, we give the notations used throughout the paper in the following.
\begin{itemize}
    \item $A,B$: scaling factors of the power law. 
    \item $\alpha$: 
\end{itemize}

\textbf{Main results.}
The main technical results of this paper are summarized below.
Let $D_1$, $D_2$ be the number of tokens used to train the dense and upcycled MoE respectively.
Denote the cross-entropy test loss of the upcyled MoE by $L$.
We discover that upcycled MoEs with fixed numbers of model parameters satisfy the following scaling law:
\begin{equation}
    \label{eq:dataonly_law}
    L \overset{c}{=} D_1^{-\alpha_1} D_2^{-\alpha_2+\alpha_3 \log D_1} 
\end{equation}
where $\alpha_i$'s ($i=1,2,3$) are positive constants, and we denote equality up to a constant by $\overset{c}{=}$.

Moreover, let $N_2$ be the number of active parameters of the upcycled model.
We find that the upcycled MoEs, fixing the number of experts and activated experts, satisfy:
\begin{equation}
    L \overset{c}{=} N_2^{-\beta}
\end{equation}
with $\beta$ a positive constant.
These scaling laws imply the following:
\begin{itemize}
    \item Denote $C^{\rm opt}_2$ by the optimal compute to train the upcycled model given $D_1$.
    We show by combining the above scaling laws that one should scale data and model size according to how much the sunken cost ($D_1$) of training the dense model is. 
    These results indicate that to one should scale data and model size according to how much the sunken cost ($D_1$) of training the dense model is. 
    This is different from the dense model scenario, where the scaling of data and model size should be fixed to a certain ratio \cite{kaplan2020scaling,hoffmann2022training}.
    \item For a fixed FLOPs for dense model and upcycled training, we show that 
\end{itemize}
 
\section{Preliminaries and Experiments}
In this Section, we describe the model architectures used in this work, the basic formulation of existing neural scaling laws, which will be helpful for our own formulation, and how we train our models.
\subsection{Model details}
\textbf{Dense model.}
Our dense models are decoder-only transformers with architecture most similar to the Llama2 models \cite{touvron2023llama}, incorporating advances such as SwiGLU and rotary embedding positions.
\textcolor{blue}{\textbf{add full details in appendix, eg, qk norm}}
We use the Llama tokenizer of vocabulary size 32,000.

\textbf{Mixture-of-Experts model.}
The MoE in consideration is the same as our dense model, but with all MLP blocks replaced by eight blocks with the same configuration.
A router consisting of a single-layer MLP outputs the routing probability of the tokens to the experts.
Each token is routed to only two experts (top-$k$ is 2) at each layer.
Note that the same architecture is adopted by some industrial-grade publicly available LLMs, such as Mixtral \cite{jiang2024mixtral}.

We refer to the number of model parameters used for computation per token as the number of \textit{active parameters}.
To minimize imbalance in expert activation, we augment the standard cross-entropy loss for next-token prediction with the auxiliary load-balancing loss \cite{fedus2022switch}, which is, in our scenario : 
$$
\mathcal{L}_{\text{aux}} =  \frac{4\alpha}{T^2} \cdot \sum_{i=1}^{8} \left( \left( \sum_{j=1}^{T} P_{j, i} \right) \cdot Q_i \right), 
$$
where $\alpha$ is the coefficient for the auxiliary loss, $T$ is the number of tokens, $P_{j, i}$ the router output probability for token $j$ to be assigned to token $i$, and $Q_i$ is the number of token assigned to expert $i$.
We set $\alpha$ to $10^{-3}$ throughout this work.

\textbf{Sparse upcycling.}
The sparse upcycling scenario assumes that one is given a pretrained dense model and would like to train an MoE with the same configuration modulo the MLP layers \cite{komatsuzaki2022sparse}.
By replicating the dense model's MLP weight eight times to form the experts, the knowledge from the dense model can be reused to accelerate the training of the MoE compared to training the MoE from scratch (from random parameter initialization).
Furthermore, the router is initiated randomly, and other modules' weights are copied from the dense counterparts directly.
We employ this technique for our studies of the scaling laws.

\subsection{Chinchilla scaling law}
We are interested in the relationships between model parameters and training budget when training LLMs.
Hoffmann et al. \cite{hoffmann2022training} showed that when training a dense transformer with $N$ model parameters and $D$ training tokens, the cross-entropy loss, $L$, can be fitted well with the following "Chinchilla" scaling law:
\begin{equation}
    \label{eq:chinchilla}
    L = \frac{A}{D^{\alpha}}  + \frac{B}{N^{\beta}} +E
\end{equation}
where $\alpha$, $\beta$ are exponents controlling how fast the loss decreases with respect to training tokens and model size respectively; $A$, $B$ are scaling factors; and $E$ is a constant: it is the irreducible loss representing the inherent entropy of text; all of them are to be fitted with experimental observations.
In this work, we denote $N_1$ by the dense model's total number of non-embedding parameters (that is, total number of parameters minus the number of embedding and language modeling head parameters), and $N_2$ by the MoE's total number of non-embedding parameters, and incorporate them to our scaling laws.

\subsection{Experimental Design}
\textcolor{blue}{add more experimental details.}
\textbf{Infrastructure.}
Our experiments are performed on multiple nodes, each consisting of 8 NVIDIA A100 or H100 80 GB GPUs, interconnected via InfiniBand HDR. 
Experiments are conducted using the Megatron-LM library \cite{shoeybi2019megatron}, which offer multi-dimensional parallelism, distributed checkpointing, kernel fusion, and other features enabling high-speed computation and optimal memory utilization. 

\textbf{Training details.}
The model configurations (number of layers, $n_{\rm layer}$, hidden dimension $d_{\rm model}$, and  MLP hidden dimension, $d_{\rm mlp}$) used in this paper is summarized in Table \ref{tab:config}.
We employ a warmup phase (with span 1\% of total iterations) followed by cosine decay to 10\% of the maximum learning rate as our learning rate schedule.
The optimizer in use is AdamW \cite{loshchilov2017fixing}.
The context length and the global batch size are fixed to be equal to 1,024.
Details of other hyperparameters can be found in Appendix.

Our training dataset is from the CommonCrawl portion of Slimpajama-DC \cite{together2023redpajama,cerebras2023slimpajama,shen2023slimpajama}.
The test loss is calculated from the default validation set defined therein. 
We train each model with different logarithmically-spaced numbers of iteration, and obtain the final test losses as a function of two variables, dense model size and training tokens.
Our 1B model trained for approximately 100 billion tokens performs similarly to other models of around the same size when evaluated with several downstream tasks.
See ...


We then perform upcycling by initializing the MoE with the dense pretrained models, and train them  with standard cross-entropy loss augmented by the auxiliary load-balancing loss. 
We also note that we initialize the router weights from a normal distribution with zero mean and variance of $2/5d$, where $d$ is the hidden dimension \cite{le-scao-etal-2022-language,chowdhery2023palm,zeng2022glm}.
Other hyperparameters are set to be the same as the one used in training the dense models.
We again train each upcycled model with different iterations to obtain the final losses.
The dataset in use is also the CommonCrawl portion of SlimPajama-DC, but without overlapping with the one used in dense training.
This results in a $D_1 \times D_2$ grid of MoEs for each dense model size.

Additionally, we train MoEs from scratch (model parameters randomized at initialization) for a variety of model sizes and training iterations.
To this end, we obtained three sets of results in total, of which we denote by: 
\textbf{dense training} (dense model training from scratch), \textbf{MoE training} (MoE training from scratch), \textbf{upcycled training} (MoE training for $D_2$ tokens where the dense model pretrained for $D_1$ tokens has been reused for MoE initialization).
We log the test loss at each 100 iterations.

\textbf{Training results.}
We show some of the test loss curves of our upcycling experiments in Figure \ref*{fig:lossplot_0.1b}.
Some observations can be made from the plot: (a) upcycling from a dense model with more pretrained tokens leads to lower initial losses.
(b) The more overtrained a dense model is, the smaller the rate of the final loss change of the upcycled MoE becomes (that is, the exponent $\alpha$ as in Equation \ref*{eq:chinchilla} becomes smaller).
The reason for (a) is straightforward; all experts inherit the same weights from the dense model. 
The output of the MoE module is preserved from the dense model irrespective of the routing at initialization, and therefore the loss is preserved as well.
For (b), our intuitive explanation is as follows.
The duplicated experts' weights are already close to the optimal ones when they are upcycled from an overtrained dense model.
It is then harder for the experts to diversify and specialize at the MoE training stage to further lower the loss. 
As we will reveal soon, our proposed scaling law captures these phenomena as well.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.9\linewidth]{lossplot.png}
    \caption{
        \textbf{Loss curves of upcycling.}
     Intermediate test losses of the 8x0.1B MoE trained for a variety of total number of tokens, $D_2$, when upcycled from a dense model pretrained with various numbers of training tokens ($D_1$). }
    \label{fig:lossplot_0.1b}
\end{figure}






\begin{table}[htb]
    \centering
    \begin{tabular}{l|cccr} 
        \toprule
        Model name & $n_{\rm layer}$ & $d_{\rm model}$ & $n_{\rm head}$ & $N_{\rm dense}$\\
        \midrule
        5M & 6  & 224  & 4 & 4,819,808 \\
        15M & 9  & 320  & 4 &14,751,680
        \\
        44M  & 12  & 480  & 8 &44,248,800\\
        0.1B   & 15 & 640  & 8 &98,323,840 \\
        0.2B  & 21 & 832 & 8 &232,623,040 \\
        0.5B  & 26 & 1120 & 16 &521,889,760 \\
        1B& 30& 1504& 16&1,085,859,424\\
        \bottomrule
    \end{tabular}
    \caption{Dense models used in our study and their parametric details. 
    The corresponding MoE, consisting of 8 experts, is denoted with a prefix "8x" (e.g., 8x1B).
    The intermediate hidden dimension size, $d_{\rm MLP}$, is set to be $4d_{\rm model}$. }
    \label[type]{tab:config}
    \end{table}

\section{Scaling Laws}
Our major goal is to characterize scaling laws of upcycled MoEs with parameters $D$'s and $N$'s.
In this Section, we study these dependencies separately before combining them to a unified scaling law. 
We also give rationale that leads to our proposed formulae and provide numerical validation. 
\subsection{Scaling law of dense and upcycled training tokens}
The Chinchilla scaling law, or Equation \ref*{eq:chinchilla}, contains three terms describing learning capacity of LLMs due to the amount of data, model expressiveness (model parameter), and the inherent entropy of the dataset.
It is a generic expression independent of neural network parameter initialization.
We can view upcycling as a vanilla training procedure but with specific parameter initialization (instead of random initialization).
This means that the same functional form of the Chinchilla scaling law should apply to $D_2$, in place of $D$ in Equation \ref*{eq:chinchilla}, as well.

To simplify analysis, we take a slice of the model size while varying $D_{1,2}$ (we drop the subscripts of $N$ in our formulae for convenience in this subsection).
Under this simplification and based on the argument given above, we hypothesize the following form of power law with respect to $D_2$ but for specific values of $D_1$: 
\begin{equation}
    \label{eq:fit_dataonly}
     L_{D_1}(D_2) = A_{D_1} D_2^{-\alpha_{D_1}} + E'_{D_1}.
\end{equation}
As $N,D_2\to \infty$, the loss should converge to the inherent entropy of the dataset ($E$ given in Equation \ref*{eq:chinchilla}), independent of $D_1$.
We thus further assume that $E'_{D_1}=E'$ (but it may be still dependent on $N$).


With the above assumptions on the functional form of the scaling law, we make a global fit, such that all points with a specific $D_1$ are fitted with a specific set of $A_{D_1}$ and $\alpha_{D_1}$ in Equation \ref*{eq:fit_dataonly}, but with $E'$ fitted and shared among the individual fits.

\textbf{Fitting procedure.}
We use the Huber loss with $\delta=10^{-3}$ as the optimization objective, and the L-BFGS-B algorithm to perform the fit.
We fit the logarithmic loss, and use the LogSumExp trick to sum over the RHS of Equation \ref*{eq:fit_dataonly}.
We use root mean square error (RMS, lower is better. Zero indicates perfect fit) and the coefficient of determination ($R^2$, higher is better. $R^2=1$ indicates perfect fit) to quantify the goodness of fit.

\textbf{Results.}
At the top part of Figure \ref{fig:dataonly}, we show the scatter plots for the test loss and the fits of the 8x0.1B model for various $D_{1,2}$.
Indeed, we find that each fit with fixed $D_{1}$ agrees well with Equation \ref*{eq:fit_dataonly}, with RMS in the range $(1\sim 3)\times 10^{-3}$, and $R^2$ in the range $0.9915\sim 0.9995$.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.9\linewidth]{dataonly_fit_global_0.1b.png}
        \includegraphics[height=0.35\linewidth]{dataonly_fit_a_0.1b.png}
        \includegraphics[height=0.35\linewidth]{dataonly_fit_b_0.1b.png}
    \caption{
        \textbf{Scaling behavior with respect to dense and upcycled training tokens. }
    \textit{Top}: Test losses of 0.1B model upcycled to 8x0.1B MoE. We show that for fixed dense training tokens, $D_1$, the loss can be fitted to a power law with respect to upcycled training tokens, $D_2$.
    \textit{Bottom left}: The exponent of the power law fitted above can be observed to be varying logarithmically with respect to $D_1$.
    \textit{Bottom right}: The scaling factor of the power law fitted above can be seen to satisfy power law with respect to $D_1$. 
    These lead us to propose a scaling law in the form of Equation \ref{eq:dataonly_loglaw}.}
    \label{fig:dataonly}
\end{figure}

Following the above observation that upcycled MoEs with fixed $D_1$ satisfy Equation \ref*{eq:fit_dataonly}, we next consider how to incorporate $D_1$ into the scaling law, i.e., we wish to consider what the functional form of $L(D_1,D_2)$ is.
To do this, we make empirical observations of the relationships between the fitted parameters $\alpha_{D_1}$, $A_{D_1}$ and $D_1$ using scatter plots.
As shown at the bottom of Figure \ref{fig:dataonly}, we can observe and hypothesize that they satisfy the following relationships:
\begin{equation}
    \label{eq:coeff_fit}
    - \alpha(D_1) \overset{c}{=} \gamma \log D_1, \; \;  A(D_1) \propto D_1^{-\eta}
\end{equation}
Subsequently, we perform least-square fits (sufficient for these simple functional forms) to test these hypotheses.
We observe good fits, with ${\rm RMS}=6\times 10^{-4}$, $R^2=0.9985$ and ${\rm RMS}=6\times 10^{-3}$, $R^2=0.9947$ for $\alpha$ and $A$ respectively.

Now, with the functional forms of the parameters' dependence on $D_1$ well-justified, we substitute them to Equation \ref*{eq:fit_dataonly} to obtain the scaling law represented by $L(D_1, D_2)$.
Simple algebraic manipulations of the substitution leads to $L(D_1, D_2) = L'(D_1, D_2) + E'$, where 
\begin{align}
    \label{eq:dataonly_loglaw}
    \log L'(D_1, D_2) & \overset{c}{=} - \alpha_{1} \log D_1 - \alpha_{2} \log D_2  \\
    &+  \alpha_{3} \log D_1 \log D_2, \nonumber
\end{align}
where we have absorbed the separately-fitted parameters (e.g., $\gamma$) to single parameters (e.g., $\alpha_3$) for the clarity of presentation.
It can be seen that $\log L'(D_1, D_2)$ is bilinear in terms of $\log D_1$ and $\log D_2$.
Note that exponentiating both sides gives the expression shown in Equation \ref*{eq:dataonly_law}.

\subsection{Scaling law of model size}

We now discuss the scaling behavior of $N$.
Since we fix the number of experts, top-$k$ of experts, and the $d_{MLP}$/$d_{\rm model}$ ratio (equal to 4), our configuration satisfies $N_2/N_{\rm dense} \approxeq 1.75$. \footnote{Ignoring subleading contribution in $d_{\rm model}$ to the number of parameters, each dense model layer contains $16d_{\rm model}^2$ parameters ($4 d_{\rm model}^2$ and $12d_{\rm model}^2$ for the attantion and MLP modules respectively), while each layer in the MoE contains $28d_{\rm model}^2$ active parameters ($4 d_{\rm model}^2$ for attention module, and $12d_{\rm model}^2$ for each of the two active experts).
This leads to the ratio $28/16=1.75$.}
It is sufficient to consider the dependency on either of the $N_i$'s to establish the scaling law with respect to model size as they are proportional to each other.
We choose to use $N_2$.

Now, we consider the dependence on $N_2$ in Equation \ref*{eq:fit_dataonly}, of the cross-entropy loss, which is now written as $L_{D_1,D_2}(N_2)$.
To simplify the analysis, we first take a slice of $D_1,D_2$ and consider the scaling with respect only to $N_2$.
Following the argument given in the previous Subsection on Equation \ref*{eq:fit_dataonly}, $E'$ should be a variable dependent on $N_2$ but independent of $D_1,D_2$.

Based on the above argument and the Chinchilla scaling law, we hypothesize the following form of power law with respect to $N_2$:
\begin{equation}
    \label{eq:fit_modelonly}
     L_{D_1,D_2}(N_2) = B N_2^{-\beta} + E''_{D_1,D_2}.
\end{equation}

In Figure \ref*{fig:modelonly}, we show the scatter plot of $L$, fixing $D_2$ to 4B tokens, and varying $D_1,N_2$.
Indeed, the loss has a power-law behavior, and the exponent is independent of $D_1$.
\textbf{\color{blue} See appendix for dependence on D2}
In the same Figure, we show the fits of Equation \ref*{eq:fit_modelonly}, noting that $E''_{D_1,D_2}$ is fitted separately for each $D_1$, different from the previous shared $E'$ fit.
The fits fixing $D_1$ agree excellently with Equation \ref*{eq:fit_modelonly}, where we yield RMS in the range $(5\sim 9)\times 10^{-3}$, and $R^2$ in the range $0.994\sim 0.997$.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.9\linewidth]{modelonly_fit_global_d2_4b.png}
    \caption{
        \textbf{Scaling behavior with respect to model size.}
     The upcycled MoE is demonstrated to have a power-law behavior for the test loss with respect to the model size.
     Here, we fix $D_2$ to 4B tokens, and vary $D_1,N_2$.}
    \label{fig:modelonly}
\end{figure}
\subsection{Unified Scaling Law}
Now, we combine the scaling laws obtained in the previous Subsections to obtain a unified scaling law for upcycled training with $D_1,D_2,N_2$ being the variables of the loss function.
Based on Equations \ref*{eq:dataonly_loglaw} and \ref*{eq:fit_modelonly}, we hypothesize the following functional form:
\begin{align}
    L(D_1,D_2,N_2) =  \frac{A}{D_1^{\alpha_1}D_2^{\alpha_2 - \alpha_3\log D_1}} + \frac{B}{N_2^{\beta_2}} + E 
\end{align}
This scaling law unifies the expressions given in Equations \ref*{eq:dataonly_loglaw} and \ref*{eq:fit_modelonly}:
$$
L(D_1,D_2,N_2) = L_{N_2}(D_1,D_2) = L_{D_1,D_2}(N_2)
$$
It also aligns with the Chinchilla scaling law when we let either of $D_1,D_2$ as $D$ and treat the other as a constant.

Furthermore, we fit the scaling laws for dense training and MoE training, which have the functional form of Equation \ref*{eq:chinchilla}.
Note that all three scaling laws should fit the same $E$, as it represents the inherent entropy of text, which should be independent of models and training methods. 
We thus make a global fit of the three scaling laws sharing a common $E$.

\textbf{Fitting details.}
We use all the test loss data (241 instances in total) we obtained from experimenting with the models in Table \ref*{tab:config} to fit the unified scaling law.
As before, We minimize the Huber loss ($\delta=10^{-3}$) as the optimization objective with the L-BFGS-B algorithm.
Note that we fit with $N_2$ and $D_2$ in the unit of billion, which brings the fitted parameters to the same order of magnitude, and helps stabilize the fitting.

We obtain good fits, with the error metrics shown below:
\begin{align*}
    {\rm Dense : RMS }=0.053,\; R^2=0.994 \\
    {\rm MoE : RMS }=0.051, \; R^2=0.993 \\
    {\rm Upcycled : RMS }=0.035, \; R^2=0.995
\end{align*}

Furthermore, we perform bootstrapping on the dataset to obtain the estimated parameters' errors and confidence levels.
More precisely, we sample from the dataset with replacement to generate 1,000 bootstraps of 241 instances each, which are used to perform the fit.
From the bootstrap, we estimate the standard errors, and we construct the confidence intervals of the fitted parameters from the quantiles.
The resulting fits are summarized in Table \ref*{tab:param}.

\textcolor{blue}{\textbf{compare with fine-grained expert exponents? because of different expert?}}

\begin{table*}[htb]
    \centering\small
    \begin{tabular}{l|ccccccc} 
        \toprule
        Model & $A$ & $B$ & $\alpha/\alpha_1$ & $\alpha_2$ & $\alpha_3$ & $\beta$ & $E$ \\
        \midrule
        \multirow{2}{*}{Dense} & $1.230 \pm 0.063$ & $0.781 \pm 0.040$ & $0.475 \pm 0.059$ & $-$ & $-$ & $0.173 \pm 0.007$ & $1.289 \pm 0.033$ \\
         & $[1.130, 1.399]$ & $[0.771, 0.909]$ & $[0.435, 0.673]$ & $-$ & $-$ & $[0.153, 0.179]$ &$[1.215, 1.340]$ \\
        \midrule
        \multirow{2}{*}{MoE} & $1.275 \pm 0.056$ & $0.643 \pm 0.031$ & $0.441 \pm 0.057$ & $-$ & $-$ & $0.196 \pm 0.006$ & $1.289 \pm 0.033$\\
         & $[1.173, 1.394]$ & $[0.616, 0.737]$ & $[0.364, 0.613]$ & $-$ & $-$ & $[0.177, 0.205]$ & $[1.215, 1.340]$\\
        \midrule
        \multirow{2}{*}{Upcycled} & $1.110 \pm 0.018$ & $0.735 \pm 0.013$ & $0.267 \pm 0.023$ & $0.397 \pm 0.019$ & $0.048 \pm 0.008$ & $0.169 \pm 0.003$ & $1.289 \pm 0.033$\\
         & $[1.044, 1.117]$ & $[0.689, 0.738]$ & $[0.180, 0.266]$ & $[0.348, 0.423]$ & $[0.023, 0.052]$ & $[0.166, 0.181]$ & $[1.215, 1.340]$\\
        \bottomrule
    \end{tabular}
    \caption{Fitted parameters for scaling laws studied in this work, with errors and 95\% confidence intervals.
    Note that the above parameters should be interpreted with training tokens and model sizes set to be in the unit of billion.}
    \label{tab:param}
\end{table*}

\section{Applications of our Scaling Laws}
\subsection{Interpretations}
Several qualitative observations can be made from the unified scaling law.
Let us focus on The logarithm of the data part, or Equation \ref*{eq:dataonly_loglaw}.
Notice that from our fit (Table \ref*{tab:param}), $\alpha_2 > \alpha_1 > \alpha_3$.
This means that both the terms $\log D_1$ and $\log D_2$ help improve the loss, but upcycled training has a larger absolute value of exponent ($\alpha_2$), and trains faster per training token.
The interaction term $\log D_1\log D_2$ on the other hand, has a positive sign and thus negatively impacts performance.

\textbf{Upcycled MoEs have a better head start (effective scaling factor is smaller).}
Consider specific values of $D_1$ and vary the upcycled training tokens $D_2$.
We can see that the effective scaling factor for $D_2$ is $A / D_1^{\alpha_1}$.
Increasing $D_1$ lowers the effective scaling factor, and hence the loss of upcycling.
Indeed, fixing $D_2$, we see that the model performs better with increasing $D_1$ in Figure \ref*{fig:lossplot_0.1b}. 

\textbf{Upcycled MoEs train slower with larger sunken costs (effective exponent is smaller).}
Again consider specific values of $D_1$ and vary the upcycled training tokens $D_2$.
We can see that the effective scaling exponent with respect to $D_2$ is $\alpha_2 - \alpha_3 \log D_1$.
This means that the larger the sunken cost ($D_1$) is, the loss decreases more slowly with $D_2$, indicating diminishing returns from increasing $D_2$ at higher $D_1$ values.
Indeed, we observe such a phenomenon in Figure \ref*{fig:lossplot_0.1b}.

\subsection{When does upcycling help?}
\textcolor{blue}{\textbf{add a plot of from scratch vs upcycling}}


\subsection{Upcycling compute optimality for fixed $D_1$}
We are concerned with how upcycling affects the compute optimality. 
That is, fixing $D_1$, we would like to know how we scale $N_2,D_2$ given a FLOPs budget while minimizing the loss, which is now written as $L_{D_1}(D_2,N_2)$.
Noting that the compute of upcycled training, $C_2$, can be approximated by $C_2=6N_2D_2$ \cite{kaplan2020scaling}, this is equivalent to solving the following:

\begin{align*}
    \left. \frac{\partial}{\partial D_2} L_{D_1}(D_2, C_2/6D_2) \right|_{C=C^{\rm opt}_2} = 0,\\
    \left. \frac{\partial}{\partial N_2} L_{D_1}( C_2/6N_2, N_2)\right|_{C=C^{\rm opt}_2} = 0
\end{align*}

Solving the above equations leads to
\begin{align}
    C^{\rm opt}_2 &\propto D_2^{\frac{\beta}{\beta + \alpha_2 - \alpha_3 \log D_1}} \\
    C^{\rm opt}_2 &\propto N_2^{\frac{ \alpha_2 - \alpha_3 \log D_1}{\beta + \alpha_2 - \alpha_3 \log D_1}}
\end{align}

\textcolor{blue}{\textbf{explain ...}}
\subsection{Budget allocation between dense and upcycled training}
From $\alpha_{\rm MoE} > \alpha_2$ in Table \ref*{tab:param}, one can infer that in the long run, training MoEs from scratch is more efficient.
On the other hand, training an MoE is more compute demanding, as the compute is approximately 1.75 than larger than that for dense training.
An interesting question is, given a fixed compute budget, how we should allocate the budget between dense training, MoE upcycled training, or simply training from scratch to maximize performance.

In other words, we would like to consider
\begin{align}
    \min_{D_1,D_2} L(D_1,D_2,N_2) \;
{\rm s.t.} \; 6 N_1 D_1 + 6 N_2 D_2 = C,
\label{eq:minimize}
\end{align}
and compare it with the training either the dense model or MoE from scratch.



To summarize, given a large enough compute budget, one should always train the MoE from scratch to yield the best performance.
\subsection{Extension of scaling laws to other variants}
Re-using data.

Threshold effects.
\section[]{Related Work}
\textbf{Mixture-of experts.}  
While the interests in publishing open-weight MoE are recent \cite{jiang2024mixtral,hu2024minicpm,yang2024qwen2,wei2024skywork,dai2024deepseekmoe,liu2024grin,muennighoff2024olmoe,sun2024hunyuan}., the use of MoE in deep learning can be traced to Eigen et al. \cite{eigen2013learning}.
In the context of modern natural language processing, \cite{shazeer2017outrageously,lepikhin2020gshard,fedus2022switch} are some of the first studies.
See \cite{cai2024survey} for a detailed survey of modern MoEs.
The MoE architecture we experiment with is most similar to Mixtral \cite{jiang2024mixtral}, of which the dense counterpart is the Llama architecture \cite{touvron2023llama}.

\textbf{Upcycling.}
Leveraging pretrained models to expedite the training of larger dense models is known as model growth \cite{kaddour2024no,du2024stacking}.
In the context of training MoEs reusing dense pretrained models, Komatsuzaki et al. \cite{komatsuzaki2022sparse} were the first studying such a scenario with encode-decoder models.
Upcycling decoder-only models have been studied in more recent works.
For example, it is used as a baseline in \cite{sukhbaatar2024branch}, but the training details are unclear.
MiniCPM \cite{hu2024minicpm}, Qwen \cite{yang2024qwen2}, and Mixtral (rumored) \cite{lo2024closer} use some form of sparse upcycling, but unfortunately the training details are not published as well.

There are works
\cite{wei2024skywork,muennighoff2024olmoe,he2024upcycling} that offer insights into sparse upcycling of large-scale decoder-only transformers, but no concrete/quantitative guidelines to sparse upcycling are offered (Skywork-MoE \cite{wei2024skywork} made only a rough recommendation: use upcycling when the budget for training is smaller than twice the budget used for dense training).
We finally note that are reports of training MoEs from scratch at the scale of trillions of parameters from scratch \cite{fedus2022switch}.
However, recent replication attempts with more modern architectures seem less successful. \footnote{(In Japanese) \url{https://tech.preferred.jp/ja/blog/pretraining-1t/}}

\textbf{\textcolor{blue}{written by chatgpt.}}
LLMs have advanced significantly, achieving unprecedented performance across a wide range of tasks through a combination of increasing model scale, dataset size, and computational power. Scaling laws provide a framework for quantifying how these factors—model parameters, data, and compute—relate to the performance of LLMs, establishing empirical relationships that guide the development of ever-larger and more capable models. Understanding these scaling laws has become essential to optimizing LLM design and deployment strategies in an era where model size and computational efficiency are paramount.

\textbf{\textcolor{blue}{written by chatgpt.}}
Pioneering work by \cite{kaplan2020scaling} demonstrated that LLMs exhibit power-law scaling, where improvements in performance can be reliably predicted by increasing model parameters, dataset size, or computation. They observed that, for a given task, the model's performance as measured by cross-entropy loss scales predictably with these factors, although returns diminish as scale grows. This scaling behavior allows for predictive optimization, guiding decisions on model size and training data necessary to reach a desired performance level.

While these two works have different formulation of the scaling laws and reach different conclusions, they are reconcilable: the difference seems to be due to how the total number of parameters is counted, and training details such as the number of warmup steps.
\section{Discussion}
We limit ourselves to Mixtral-like MoEs (8 experts with top-k equal to 2) to narrow down the parameter space to explore.

\section*{Accessibility}
Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

\section*{Software and Data}

If a paper is accepted, we strongly encourage the publication of software and data with the
camera-ready version of the paper whenever appropriate. This can be
done by including a URL in the camera-ready copy. 

\section*{Acknowledgements}

Optional acknowledgments go here, as an unnumbered section after the main paper but before the references. 

\section*{Impact Statement}

Authors are should consider including an \textbf{optional} statement of the potential 
broader impact of their work, including its ethical aspects and future 
societal consequences. This statement should be in an unnumbered and also at the end of the paper, before the references. 



\bibliography{ref}
\bibliographystyle{plainnat}


\newpage
\appendix
\onecolumn
\section{Performance of downstream tasks}



\end{document}
