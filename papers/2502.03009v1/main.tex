
\documentclass[preprint]{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[preprint]{neurips_2023}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{lipsum}                     %
\usepackage{xargs}                      %
\usepackage[pdftex,dvipsnames]{xcolor}  %
\usepackage{etoolbox}
\usepackage{enumitem}

\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}



\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\newcounter{requirement}
\setcounter{requirement}{1} %

\author{%
  Seng Pei Liew\thanks{\texttt{sengpei.liew@sbintuitions.co.jp}}, \; 
  Takuya Kato, \;
  Sho Takase \\
  SB Intuitions \\
  Tokyo, Japan \\
}
\title{Scaling Laws for Upcycling Mixture-of-Experts Language Models}

\begin{document}
\maketitle
















\begin{abstract}
 Pretraining large language models (LLMs) is resource-intensive, often requiring months of training time even with high-end GPU clusters.
 There are two approaches of mitigating such computational demands: reusing smaller models to train larger ones (upcycling), and training computationally efficient models like mixture-of-experts (MoE).
 In this paper, we study the upcycling of LLMs to MoE models, of which the scaling behavior remains underexplored.
 Through extensive experiments, we identify empirical scaling laws that describe how performance depends on dataset size and model configuration.
 Particularly, we show that, while scaling these factors improves performance, there is a novel interaction term between the dense and upcycled training dataset that limits the efficiency of upcycling at large computational budgets.  
 Based on these findings, we provide guidance to scale upcycling, and establish conditions under which upcycling outperforms from-scratch trainings within budget constraints.
\end{abstract}


\section{Introduction}
\label{sec:introduction}
Large-scale neural network architectures, such as dense transformers \citep{vaswani2017attention}, have seen remarkable success across a wide range of tasks, particularly achieving human-level capabilities in natural language processing \citep{achiam2023gpt}.
However, they often demand an enormous amount of computation, imposing challenges of computational efficiency and scalability.
Sparse models like mixture-of-experts (MoE) architectures \citep{shazeer2017outrageously,lepikhin2020gshard}, have emerged as an alternative achieving better efficiency-performance trade-off via partial activation (routing) of neural parameters when processing the input.
Even so, MoE still requires substantial compute power to reach its full potential \citep{wei2024skywork,dai2024deepseekmoe,yang2024qwen2}.

One direction to further accelerate training convergence is leveraging smaller pretrained models to guide the training of larger MoE models.
\citet{komatsuzaki2022sparse} propose upcycling, which reuses the dense checkpoint to continued pretrain the upcycled MoE.
The MoE is expected to specialize and optimize routing rapidly by leveraging the pretrained dense model weights.

Despite the promise of efficient MoE training via upcycling, the effectiveness and limitations of this technique remain unclear.
While some \citep{wei2024skywork,he2024upcycling} have already adopted it to training large-scale MoEs, \citet{muennighoff2024olmoe} reported negative results where upcycling can slow down training convergence.
We believe these seemingly contradictory conclusions are due to insufficient comprehensive studies and assessments.
There is also a lack of guidance on how and when to upcycle MoE, hampering a wider adoption of this technique.

In this paper, we seek to better understand large language model's (LLM) upcycling to MoE via a series of controlled experiments, spanning up to a few hundred billion (B) training tokens and models up to 7B total parameters.
Specifically, we uncover precise power-law scalings for the language modeling performance (cross-entropy loss) with respect to training dataset size (for both dense and upcycled MoE training), and the model configuration, including the total number of parameters (model size).
Building on these results, we provide a framework for assessing when upcycling offers advantages over from-scratch training and how performance gains depend on dataset size and model configuration. 
\begin{figure*}[ht!]
    \centering
        \includegraphics[width=0.43\linewidth]{p_vs_active_scatter.png}
        \includegraphics[width=0.43\linewidth]{efficiency_schema.png}
    \caption{
    \textbf{Left: Upcycling improves with sparsity and the number of active parameters.}
        We find that upcycling to MoE which is sparser and has more active parameters improves performance.
        The z-axis shows the value of cross-entropy loss.
        See Section \ref{subsec:model_scaling} for details.
        \textbf{Right: Efficiency of upcycling diminishes with sunk cost and model size.}
        For all additional token budgets for upcycling a Mixtral-like MoE above the curve(s), training MoE from scratch is more efficient, whereas for all token budgets below it, upcycling is more efficient.
        Shown are the numerical (blue) and analytical (red) solutions of Equation \ref{eq:sol}.
        See Section \ref{subsec:implication} for details.
    }
        \label{fig:budget}
\end{figure*}

\textbf{Main results.}
The major technical findings of this paper are summarized below.
Let $D_1$, $D_2$ be the number of tokens used to train the dense model and upcycled MoE respectively.
Denote the cross-entropy test loss of the upcycled MoE by $L$.
We find that the upcycled MoE satisfies the following relation for a wide range of model configuration:
\begin{equation}
    \label{eq:dataonly_law}
    L = A D_1^{-\alpha_1} D_2^{-\alpha_2+\alpha_3 \log D_1} + E
\end{equation}
where $\alpha_i$'s ($i=1,2,3$) are positive scaling exponents, and $A,E$ are constants independent of $D_1,D_2$.

Moreover, the empirical performance has a power-law scaling with respect to the model size as well as its sparsity (Equation \ref{eq:modeling_law}).
These allow us to quantify the effectiveness and shortcomings of upcycling.
Particularly, our empirical results suggest that :
\begin{itemize}
    \item 
 Increasing $D_1$ (sunk cost) reduces the initial losses of the upcycled MoE but results in slower training progress with $D_2$ (upcycled MoE tokens). 
    \item Upcycled MoE benefits from increased sparsity and active parameters without noticeable trade-offs.
            See the left panel in Figure \ref{fig:budget}.
\item
We propose a joint scaling law of dataset and model sizes for Mixtral-like MoE \citep{jiang2024mixtral} (Equation \ref{eq:joint}).
While we find that the advantage of upcycling diminishes with increasing sunk cost and dense model size ($N_1$), upcycling remains effective when $D_2$ are limited.
\item
Particularly, upcycling is beneficial when $D_2$ remains below a certain threshold, where the pretrained dense model can still accelerate convergence. However, beyond this threshold, from-scratch training of MoE becomes more efficient.
The threshold $D^*$ is 
    \begin{equation}
        D^* \simeq  4\left(\frac{N_1}{10^9}\right)^{-0.7 + 0.04 \log (N_1/10^9)}\; \text{B tokens}
        \label{eq:eff_fit}
    \end{equation}
        See the right panel of Figure \ref{fig:budget}.


\end{itemize}

\textbf{Notations.}
 We summarize main notations used in the paper.
\begin{itemize}
    \setlength{\itemsep}{0pt}      
    \setlength{\parskip}{0pt}      
    \setlength{\itemindent}{0pt}   
	\setlength{\labelsep}{4pt}     
    \item $L$: cross-entropy loss in nat
    \item $D$: dataset size in token
    \item $N$: non-embedding model size
    \item $A,B,F$: scaling factors of the power law, independent of the variable under consideration
    \item $E$: irreducible loss of the power law, independent of the variable under consideration
    \item $\alpha,\beta,\gamma$: scaling exponents of the power law, independent of the variable under consideration
\end{itemize}
\section{Preliminaries}
\label{sec:prem}

\subsection{Model details}
\textbf{Dense model.}
Our dense models are decoder-only transformers pretrained with an autoregressive language modeling objective.
The architecture is most similar to Llama2 models \citep{touvron2023llama}, incorporating advances such as SwiGLU \citep{shazeer2020glu} and rotary position embedding \citep{su2024roformer}.
We use the Llama tokenizer of vocabulary size 32,000.

\textbf{Mixture-of-Experts.}
The MoE model in consideration is the same as our dense model, but with all MLP blocks replaced by multiple blocks (experts) with the same configuration \citep{fedus2022switch}.
A router consisting of a single-layer MLP outputs the routing probability of the tokens to the experts.

The model configuration has two key parameters: $n_{\rm expert}$, representing the number of experts, and $n_{\rm TopK}$, which specifies how many of the highest-probability experts each token is routed to at each layer.
The output of the experts is linearly combined and passed to the next layer.

The MoE and its corresponding dense model with model size $N_{\rm dense}$, consisting of $n_{\rm expert}$ experts, is denoted with a prefix "$n_{\rm expert}$", e.g., 8x1B where the dense correspondent is of 1B in model size.
We refer to the number of non-embedding model parameters (that is, total number of parameters minus the number of embedding and language modeling head parameters) used for computation per token as the number of \textit{active parameters}.

\textbf{Upcycling.}
The upcycling scenario assumes that one is given a pretrained dense model and would like to train an MoE with the same configuration but replacing the MLPs with the MoE modules \citep{komatsuzaki2022sparse}.
By replicating the dense model's MLP weight $n_{\rm expert}$ to form the experts, the knowledge from the dense model can be reused to accelerate the training of the MoE compared to training the MoE from scratch (from random parameter initialization).
Other modules' weights are copied from the dense counterparts directly, with the router's initial weights randomized.
See Figure \ref*{fig:overview} for an illustration of upcycling.
We employ this technique for our study.

\subsection{Power-law Ansatz}
There is an extensive literature showing that the loss of training deep learning models has a simple power-law behavior: $ L = \frac{A}{X^{\alpha}}  +E$, for single variable $X$, including model size and dataset size \citep{hestness2017deep,hestness2019beyond,rosenfeld2019constructive,henighan2020scaling}. \footnote{The actual form we assume is  $ L = \frac{A}{(X+1)^{\alpha}} +E$ such that the loss is finite at the limit $X\to 0$. 
However, since the values of $X$ we consider are often much larger than 1 ($10^6$ or more), we approximate it as $ L = \frac{A}{X^{\alpha}}  +E$ for notational convenience.}
We use this ansatz in this work. 
\begin{figure*}[ht!]
    \centering
        \includegraphics[width=0.62\linewidth]{overview2.pdf}
    \caption{
        \textbf{Upcycling and factors affecting MoE's performance.}
        Upcycling involves initiating the weights of the MoE (activating $\color{red} n_{\rm TopK}$ experts per token) by reusing the weights (duplicating the weights of MLPs $\color{red} n_{\rm expert}$ times) of an existing dense transformer of size $\color{red} N_1$ that has been trained for $\color{red} D_1$ tokens.
        The (upcycled) MoE is further trained for $\color{red} D_2$ tokens.
        Language modeling performance improves when scaling these factors. 
        We study and develop formulae (scaling laws) consisting of these factors to predict the empirical performance. 
    }
    \label{fig:overview}
\end{figure*}

\citet{hoffmann2022training} have shown that when training a dense transformer, the cross-entropy loss is well-described by the following "Chinchilla" scaling law:
\begin{equation}
    \label{eq:chinchilla}
    L = AD^{-\alpha}  + BN^{-\beta} +E
\end{equation}
The first and second terms quantify the limitation of learning due to limited dataset and model sizes respectively.
The scaling exponents, $\alpha$, $\beta$ control how fast the loss decreases with respect to dataset and model sizes respectively.
$E$ is a constant: it is the irreducible loss representing the inherent entropy of text.
All parameters are to be fitted with experimental observations.
We also assume Equation \ref*{eq:chinchilla} for models trained from scratch.


\section{Experimental Design and Results}
\label{sec:exp}
In this Section, we describe the design of our experiments, before presenting some of the experimental results.
Additional details, including ablation studies are available in Appendix \ref{app:exp}.
\subsection{Setup}

\textbf{Dense models in consideration.}
We train a suite of dense models with model sizes 15M, 44M, 0.1B, 0.2B, 0.5B and 1B.
The model configuration (number of layers, $n_{\rm layer}$, hidden dimension $d_{\rm model}$, and  MLP hidden dimension, $d_{\rm mlp}$) used in this paper is summarized in Table \ref{tab:config} of Appendix \ref{app:exp}.

We denote $N_1$ by the dense model's total number of non-embedding parameters, $N_2$ by the MoE's total number of non-embedding \textit{active} parameters, $N_{\rm total}$ by the MoE's total number of non-embedding parameters.
They are given by (ignoring subleading contribution proportional to $d_{\rm model}$)
\begin{eqnarray}
    N_1 &\approx& (4 + 12) n_{\rm layer} d^2_{\rm model} = 16 n_{\rm layer} d^2_{\rm model} \label{eqn:n_1} \\
    N_2 &\approx& (4 + 12 n_{\rm TopK})n_{\rm layer} d^2_{\rm model} \label{eqn:n_2} \\
    N_{\rm total} &\approx& (4 + 12 n_{\rm expert})n_{\rm layer} d^2_{\rm model} \label{eqn:n_total}
\end{eqnarray}
where the attention and MLP module contribute $4d^2_{\rm model}$ and $12d^2_{\rm model}$ parameters respectively per layer.

\textbf{Learning rate schedule.}
We are interested in training models with different numbers of training token budget.
Previous work \citep{hoffmann2022training} used the cosine learning rate (LR) schedule and trained separate models for \textit{each} number of training token budget, which is resource-consuming.
We instead employ the warmup-stable-decay (WSD) learning rate schedule \citep{bi2024deepseek,hu2024minicpm}, which requires only a single model sweep with a sufficiently large number of training tokens.
As the LR is constant for the majority of learning, the saved intermediate model checkpoints can be reused to emulate different numbers of training token budget; the checkpoints are continued-pretrained reusing the optimizer states from the constant LR stage with a new, shorter LR schedule.
This reduces substantially the number of new training runs required.
In Appendix \ref{app:schedule}, we show that the performance of dense and MoE training is on par with the usual cosine LR schedule. 
The maximum LR value is tuned individually for each model size and for both dense and MoE training.


\textbf{Dataset.}
We use training dataset derived from the CommonCrawl portion of Slimpajama-DC \citep{shen2023slimpajama}, containing 368B tokens in total.
The test loss is calculated from the default validation set (0.3B tokens) defined therein. 
In Appendix \ref{app:dataset}, we train models on two different datasets (Japanese language and source code datasets) to study the scaling behavior across datasets.


\subsection{Upcycled Training}
Upcycled training is performed by first initializing the MoE with the dense pretrained models as mentioned before.
Subsequently, we train the upcycled MoE with standard cross-entropy loss augmented by an auxiliary load-balancing loss with coefficient $10^{-3}$ to minimize expert's imbalance of activation.
See Appendix \ref{app:aux} for ablation of the coefficient.




To this end, we obtained three sets of results in total, of which we denote by: 
\textbf{dense training} (dense model training from scratch), \textbf{MoE training} (MoE training from scratch), \textbf{upcycled training} (MoE training for $D_2$ tokens where the dense model pretrained for $D_1$ tokens has been reused).

\subsection{Training results}
\label{subsec:exp_results}
We show some of the resulting test loss curves of our upcycling experiments in Figure \ref*{fig:lossplot_0.1b} (see model evaluation with standard benchmarks in Appendix \ref{app:eval}).
Some observations can be made from the plot: 
\begin{enumerate}[label=(\alph*)]
    \setlength{\itemsep}{0pt}      
    \setlength{\parskip}{0pt}      
    \setlength{\itemindent}{0pt}   
	\setlength{\labelsep}{4pt}     
    \item upcycling from a dense model with more pretrained tokens leads to lower initial losses.
\item The more overtrained a dense model is, the smaller the rate of the final loss change of the upcycled MoE becomes (the exponent $\alpha$ becomes smaller).
\end{enumerate}
The reason for (a) is \textit{function preservation}.
As all experts inherit the same weights from the dense model,
the output of the MoE module is preserved from the dense model irrespective of the routing at initialization, and therefore the loss is preserved as well.
For (b), our intuitive explanation is as follows.
The duplicated experts' weights are already close to the optimal ones when they are upcycled from an overtrained dense model.
It is then harder for the experts to diversify and specialize at the MoE training stage to further lower the loss. 
As we will reveal soon, our proposed scaling law captures these phenomena.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.5\linewidth]{lossplot.png}
    \caption{
        \textbf{Loss curves of upcycling.}
     Intermediate test losses of the 8x0.1B MoE (2 experts activated per token) trained for a variety of total number of tokens, $D_2$, when upcycled from a dense model pretrained with various numbers of training tokens ($D_1$) in B. }
    \label{fig:lossplot_0.1b}
\end{figure}








\section{Scaling Laws}
\label{sec:law}
Our ultimate goal is to understand the performance of upcycling with respect to various factors illustrated in Figure \ref{fig:overview}.
However, simultaneously varying all these variables to investigate the scaling behavior is computationally prohibitive.
We separately investigate two aspects closely related to upcycling: dataset sizes (Section \ref{subsec:datasize_law}) and model configuration (Section \ref{subsec:model_scaling}).
In the next Section, we further study the joint scaling behavior of dataset and model sizes with a predetermined MoE architecture.

\subsection{Scaling Law for Dataset Sizes}
\label{subsec:datasize_law}







We fix the model size while varying $D_{1,2}$ to study the scaling behavior with respect to these variables.
To determine the functional form of the scaling law, $L(D_1,D_2)$, we require it to satisfy certain properties:

\textbf{Requirement 1.} 
$L(D_1,D_2)$ follows the power law with respect to $D_2$, as shown empirically in Figure \ref{fig:dataonly}.
This aligns with the power-law ansatz, treating upcycling as analogous to standard MoE training with dataset size $D_2$, initialized with dense parameters rather than random weights:
\begin{equation}
    L(D_1,D_2)= L_{D_1}(D_2) = A D_2^{-\alpha} + E
\label{eq:power_d2}
\end{equation}

\textbf{Requirement 2.} 
As $D_2\to0$, the loss should reduce to the power-law scaling behavior of the dense counterpart with respect to $D_1$, consistent with the function-preserving initialization of upcycling (see Section \ref{subsec:exp_results}):
$${\rm lim}_{D_2\to 0} L(D_1,D_2) = A D_1^{-\alpha}+E$$



We investigate the following functional forms of power law satisfying these requirements:

\textbf{Multiplicative:}
\begin{align}
    L(D_1,D_2) &= A D_1^{-\alpha_1}(1+D_2)^{-\alpha_2 + \alpha_3\log D_1}+E \nonumber \\
    &\approx A D_1^{-\alpha_1}D_2^{-\alpha_2 + \alpha_3\log D_1}+E
    \label{eq:multiplicative}
\end{align}
\textbf{Additive:}
\begin{align}
    L(D_1,D_2) &= A D_1^{-\alpha_1} + F (1+D_2)^{-\alpha_2 + \alpha_3\log D_1}+E \nonumber \\
    &\approx A D_1^{-\alpha_1} + F D_2^{-\alpha_2 + \alpha_3\log D_1}+E
    \label{eq:additive}
\end{align}
Both forms include an \textit{interaction} term ($\alpha_3 \log D_1$) in the scaling exponent, capturing the interplay between $D_1$ and $D_2$. 

\textbf{Empirical comparisons of functional forms.}
We empirically compare these functional forms, including the special case where $D_1$ and $D_2$ have no interaction.
The optimization uses the Huber loss ($\delta=10^{-3}$) and the BFGS algorithm, fitting the logarithm of the loss via the LogSumExp trick applied to the RHS of Equations \ref*{eq:multiplicative} and \ref*{eq:additive}.
The leave-one-out root mean square error (RMS) serves as the fit metric.

The fit is performed on a 0.1B dense model upcycled to MoE architectures with $n_{\rm expert}=\{4,8\}$ and $n_{\rm TopK}=\{1,2\}$ , trained on a $5\times5$ grid of $D_1,D_2$.
The fitting results are shown in Table \ref*{tab:func}, where we can see that the multiplicative functional form (with non-zero interaction) achieves consistently the lowest leave-one-out RMS error across the experimented MoE architectures.
Henceforth, we adopt Equation \ref{eq:multiplicative} in our scaling laws.

\textbf{Multiplicative scaling law from empirical observations.}
To strengthen our proposal for Equation \ref{eq:multiplicative}, we demonstrate through a bottom-up approach that the multiplicative nature of the scaling law, including the interaction term, arises naturally from empirical data.

From Figure \ref{fig:dataonly}, we observe that the scaling exponent in Equation \ref{eq:power_d2} decreases as $D_1$ increases.
To quantify this relationship, we model the scaling exponent as a function of $D_1$, denoted as $-\alpha(D_1)$. A scatter plot of $-\alpha(D_1)$ reveals the following logarithmic relationship:
\begin{equation}
-\alpha(D_1) = \gamma \log D_1 + E'. \label{eq:log}
\end{equation}
which fits the data well, as shown in the same Figure. 
Substituting this expression to Equation \ref*{eq:power_d2}, we obtain a term of the form $ D_2^{- E'+\gamma \log D_1}$, directly supporting the interaction term in the multiplicative scaling law. 
This shows that the interaction term is not merely an artifact of theoretical derivation but is essential to explain observed empirical trends.
Additionally, we show that the multiplicative dependence, $A\propto D_1^{-\alpha_1}$ also arises from empirical observations, as detailed in Appendix \ref{app:fit_dataset_law}.
\subsubsection{Interpretations}
Several quantitative and qualitative observations can be made from the scaling law of dataset sizes.
From our fit (Table \ref*{tab:fit_arch} in Appendix \ref{app:fit_dataset_law}), we notice a trend $\alpha_2 \gtrsim \alpha_1 \gg \alpha_3$.
This means that while increasing either of $D_1$ and $D_2$ helps improve performance, as upcycled training has a slightly larger exponent ($\alpha_2$), increasing $D_2$ helps train faster.

\textbf{Upcycled MoE has a better head start (effective scaling factor is smaller).}
Fixing $D_1$, we see that the effective scaling factor for $D_2$ is $A D_1^{-\alpha_1}$.
Increasing $D_1$ lowers the effective scaling factor, and hence the loss of upcycling.
Indeed, fixing $D_2$, we see that the model performs better with increasing $D_1$ in Figure \ref*{fig:lossplot_0.1b}. 

\textbf{Upcycled MoE trains slower with larger sunk cost (effective scaling exponent is smaller).}
Again fixing $D_1$, we see that the effective scaling exponent with respect to $D_2$ is $\alpha_2 - \alpha_3 \log D_1$.
This means that the larger the sunk cost ($D_1$) is, the loss decreases more slowly with $D_2$, indicating diminishing returns from increasing $D_2$ at higher $D_1$ values, agreeing with Figure \ref*{fig:dataonly}'s results.


\begin{table}[htb]
    \centering
    \begin{tabular}{l|cccc} 
        \toprule
         ($n_{\rm expert}$,$n_{\rm TopK}$) & (4,1) & (4,2) & (8,1) & (8,2)   \\
        \midrule
        \textbf{Mul.} & \textbf{0.0111} & \textbf{0.0081} & \textbf{0.0105}& \textbf{0.0031}   \\
        \textbf{Mul. (\boldmath $\alpha_3=0$)} & 0.0169 & 0.0085 & 0.0180 & 0.0095  \\
 
        \textbf{Add.} & 0.0165 & 0.00843 & 0.0167 & 0.0093  \\
        \textbf{Add.(\boldmath$\alpha_3=0$)} & 0.0196 & 0.0117 & 0.0430 &0.0113 \\
        \bottomrule
    \end{tabular}
    \caption{
    \textbf{Multiplicative scaling law with interaction consistently achieves lowest error.}    
    Leave-one-out RMS error for fitting the loss for MoEs upcycled from a dense 0.1B model, with functional forms of Equations \ref*{eq:multiplicative}, \ref*{eq:additive}, and specific cases with $\alpha_3=0$.
    The first and second number in the bracket of the first row indicates the MoE architecture's parameter, $n_{\rm expert}$ and $n_{\rm TopK}$ respectively.}
    \label{tab:func}
\end{table}









\begin{figure*}[h]
    \centering
        \includegraphics[width=0.3\linewidth]{dataonly_fit_global_15m.png}
        \includegraphics[width=0.3\linewidth]{dataonly_fit_global_0.1b.png}
        \includegraphics[width=0.3\linewidth]{dataonly_fit_global_1b.png}
        \includegraphics[width=0.3\linewidth]{dataonly_fit_a_15m.png}
        \includegraphics[width=0.3\linewidth]{dataonly_fit_a_0.1b.png}
        \includegraphics[width=0.3\linewidth]{dataonly_fit_a_1b.png}

    \caption{
        \textbf{Top: $D_2$ has power-law scaling.} 
        We show scaling behavior of upcycled training tokens ($D_2$) for different values of dense tokens ($D_1$).
        \textbf{Bottom: Interaction term explains decreasing exponents.} The fitted exponents in the upper plots are used to fit Equation \ref{eq:log} as a function of $D_1$, and are shown to agree well with the functional form.
        }    \label{fig:dataonly}
\end{figure*}



\subsection{Scaling Law for Model Configuration}
\label{subsec:model_scaling}
Previous Subsection mainly concerns with \textit{when} to upcycle, i.e., the training tokens used when upcycling.
Here, we study \textit{how} to upcycle the dense model, i.e., the model configuration of the MoE.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.5\linewidth]{p_vs_active_fit2.png}
    \caption{
        \textbf{Fits of scaling law for model configuration.}
    }
    \label{fig:p_vs_active_fit2}
\end{figure}
To do this, we define \textit{sparsity}, $P\coloneq N_{\rm total}/N_2$, and consider the dependency of the performance on sparsity and active parameter $N_2$, of which the variables capture scaling behavior with respect to model parameters and architectural details ($n_{\rm expert}$ and $n_{\rm TopK}$).
See Equations \ref{eqn:n_1} to \ref{eqn:n_total}.

Using these variables, we hypothesize, similar to Equation \ref{eq:dataonly_law}, the following scaling relation:
\begin{eqnarray}
    L(P,N_2) = B P^{-\beta_1} N_2 ^{-\beta_2+ \beta_3\log P}+ E
    \label{eq:modeling_law}
\end{eqnarray}
To fit the above equation, we run experiments fixing $D_1=D_2=2.3\text{B}$, varying the following: dense model of size \{15M, 44M, 0.1B, 0.2B\},  $n_{\rm expert}=\{4,8,16,32\}$ and $n_{\rm TopK}=\{1,2,4\}$.
The scatter plot is shown in the left panel of Figure \ref{fig:budget}.

We obtain the following values for the exponents using the same fitting procedure in Section \ref{subsec:model_scaling}: $\beta_1=0.03\;,\beta_2=0.30\;,\beta_3=-0.01$.
See Figure \ref{fig:p_vs_active_fit2} for the fits.
Unlike dataset sizes, due to computational constraints, we are unable to generate more data points to make a more precise estimate extending the range of model configuration. 
Nevertheless, we do not observe performance trade-offs when scaling sparsity and active parameters within the range of our examination, in contrast to the scaling law of dataset sizes.
Our work suggests that:
\textbf{Performance of upcycling improves with sparsity and active parameters without noticable trade-offs.}

\section{Joint Scaling Law}
\label{sec:joint}
In this Section, we show that the upcycled MoE follows a joint scaling law with respect to dataset size and model size, provided the MoE architecture is fixed.
While increasing sparsity and active parameters generally improves performance as shown before, compute and memory constraints impose practical upper limits.
Thus, while scaling MoE configurations as much as possible is theoretically beneficial, practitioners should choose an architecture that aligns with their available compute, memory, and expected training token budget.
For our study, we adopt a widely used MoE configuration ($n_{\rm expert}=8, n_{\rm TopK}=2$), which has been implemented in several large-scale, publicly available models, including Mixtral-8x7B and 8x22B \citep{jiang2024mixtral}, and Sarashina2-8x70B \footnote{\url{https://huggingface.co/sbintuitions/sarashina2-8x70b}}.
 
To establish the scaling law with respect to model size, it is sufficient to consider the dependency solely on $N_1$ among other definitions (Equations \ref*{eqn:n_1}, \ref*{eqn:n_2}, \ref{eqn:n_total}), as they are equivalent up to a multiplicative constant, e.g., $N_2/N_1 \approx 1.75$.
To determine the scaling law, we require the functional form to satisfy the following, analogous to previous Requirements:

\textbf{Requirement 3.}
$L_{D_1,D_2}(N_1) = B_{D_1,D_2} N_1^{-\beta} + E_{D_1,D_2}$.

\textbf{Requirement 4.}
${\rm lim}_{D_2\to 0} L(D_1,D_2, N_1) = A D_1^{-\alpha}+ B N_1^{-\beta} + E$, i.e., approaching the Chinchilla scaling law.

The straightforward functional form that satisfies these requirements is as follows.
\footnote{One could extend this with Equation \ref{eq:modeling_law} for a truly unified scaling law including all model configurations, but we avoid doing so in this work as reliably fitting with that many independent variables would require extensive experiments beyond our computational reach.}
\begin{align}
    L(D_1,D_2,N_2) =  AD_1^{-\alpha_1}D_2^{-\alpha_2 + \alpha_3\log D_1} + BN_1^{-\beta_2} + E 
    \label{eq:joint}
\end{align}








\textbf{Fitting.}
Towards this end, we fit three separate scaling laws, corresponding to dense, MoE, and upcycled trainings, incorporating dataset and model sizes (the first two are fitted with the functional form of Equation \ref{eq:chinchilla}). 
We use the same fitting procedure presented in Section \ref{subsec:model_scaling}.
As the irreducible loss $E$ is universal irrespective of scaling laws under consideration, we first estimate $E$ from a joint fit of dense and MoE scaling laws, then fix it when fitting the upcycling scaling law to reduce overfitting and enhance extrapolation ability. 


In Figure \ref{fig:valid}, we show that the fitted result of upcycling scaling law extrapolates well, achieving validation RMS error of 0.015.
The fitted parameters using all data for all three scaling laws are summarized in Table \ref*{tab:param}.
We consider other fitting possibilities and present more results in Appendix \ref{app:fit}.
For the rest of this Section, we consider the application and implication of the joint scaling law.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.5\linewidth]{valid.png}
    \caption{
        \textbf{Fits of the joint upcycling scaling law.}
    }
    \label{fig:valid}
\end{figure}

\begin{table}[htb]
    \centering
    \begin{tabular}{l|cccccc} 
        \toprule
        {} & $ A$ & $ B$ & $\alpha/\alpha_1$ & $\alpha_2$ & $\alpha_3$ & $\beta$  \\
        \midrule
        \textbf{Dense} & 8.83 & 12.3 & 0.088 & $-$ & $-$ & $0.116$  \\
      
        \midrule
        \textbf{MoE} & 32.0 & 7.05 & 0.161 & $-$ & $-$ & $0.080$ \\
        \midrule
        \textbf{Upcycled} & 16.3 & 8.53 & 0.043 & 0.085 & 7.98e-4 & 0.112 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Fitted parameters for joint scaling laws.}
    Note that for MoE, we fit the parameters with variable $N_1$ (instead of $N_2$) for comparison conveniences.
    The irreducible loss applicable to all laws, $E$, is fitted to be 0.165.}
    \label{tab:param}
\end{table}



\subsection{Training MoE from Scratch versus Upcycling}
\label{subsec:implication}
Past studies \citep{komatsuzaki2022sparse,muennighoff2024olmoe} have explored the efficiency of training an MoE from scratch compared to upcycling a dense model.
\citet{komatsuzaki2022sparse} found that upcycling retains an advantage up to 120\% of the sunk cost ($D_1$).  
For instance, to match the performance of an upcycled MoE trained with an additional 0.4 trillion (T) tokens after 2T dense tokens, training an MoE from scratch would require 2.4T tokensâ€”effectively saving 2T tokens in upcycled training.
Conversely, \citet{muennighoff2024olmoe} reported that training from scratch requires less than 100\% of the sunk cost under different settings, indicating that it could be more efficient. 
These seemingly contradictory results suggest that upcycling efficiency depends on both sunk cost and model size.

To investigate this, we define $D^*$ the number of tokens required for training from scratch to match the performance of an upcycled MoE with the same sunk cost, i.e., 
\begin{equation}
L^{\rm MoE}_{N_1}(D^*)=L^{\rm Upc.}_{N_1}(D_1=D^*,D_2=D^*) \label{eq:sol}
\end{equation}
Since the above equation involves non-integer polynomial exponents, we solve it numerically and approximate the solution analytically (Equation \ref{eq:eff_fit}).
 Figure \ref{fig:budget} shows that $D^*$ decreases with increasing model size, with $D^*$ equal to 4B tokens for an 8x1B model.
When $D_{2} \lesssim D^*$, the required $D_{\rm MoE}$ to catch up is more than 100\% of $D_1$: upcycling remains more efficient than training from scratch.
However, for $D_{2} \gtrsim D^*$, the efficiency reverses, favoring training from scratch.
Our findings indicate that:
\textbf{Upcycling is more efficient than training an MoE from scratch for lower sunk cost and training token budget, but its efficiency diminishes as the sunk cost or model size increases.}



\subsection{Compute Allocation and Compute-Optimal Upcycling}
\label{subsec:com-opt}
Given a fixed total floating-point operation (FLOPs) budget $C$, we analyze how to optimally allocate compute between dense training and upcycled training. Specifically, we solve:
\begin{align*}
    \min_{D_1,D_2,N_1} L(D_1,D_2,N_1) \;
{\rm s.t.} \; {\rm FLOPs}(D_1,D_2,N_1)= C
\end{align*}
We find that for a given compute budget without any pretrained models, training a compute-optimal dense or MoE model from scratch outperforms the two-stage dense-to-upcycled MoE approach (see Figure \ref{fig:budget_app} in Appendix \ref{app:com-opt}).
This suggests:
\textbf{If no pretrained model exists, direct training is preferable to upcycling for optimal performance.}

In scenarios where a pretrained dense model is already available, we can determine the compute-optimal scaling of MoE upcycling. 
The compute cost of upcycled training is approximated as $C_2=6N_2D_2$ \citep{kaplan2020scaling}. Optimizing the compute leads to the scaling relations (see Appendix \ref{app:com-opt} for derivation):
\begin{align}
    D^{\rm opt}_2 &\propto C_2^{\frac{\beta}{\beta +\alpha_{\rm eff}}},\; \;
    N^{\rm opt}_1 \propto C_2^{\frac{ \alpha_{\rm eff}}{\beta + \alpha_{\rm eff}}}
    \label{eq:optimal_d_n}
\end{align}
 where $\alpha_{\rm eff} \coloneq \alpha_2-\alpha_3\log D_1$.
 Notably, as $D_1$ increases, $\alpha_{\rm eff}$ decreases, meaning \textbf{larger pretrained models require disproportionately more tokens for efficient upcycling.}

For instance, applying this to the Llama2 models (7B, 13B, 70B), which were trained on 2T tokens \citep{touvron2023llama}, we estimate that compute-optimal upcycling follows the scaling $D_2 \propto N_1^{1.8}$, indicating that larger models require nearly quadratic increases in upcycling data.

Overall, we find that upcycling is inefficient relative to from-scratch trainings when considering compute optimality.
      
      
      


\section[]{Related Work}
\label{sec:related}
\textbf{Mixture-of experts.}  
While interests in developing open MoE LLMs are relatively recent \citep{hu2024minicpm,yang2024qwen2,dai2024deepseekmoe,liu2024grin,sun2024hunyuan}, the use of MoE in deep learning can be traced to early 2010s \citep{eigen2013learning,bengio2013estimating}.
See \citet{cai2024survey} for a detailed survey of modern MoE models.

\textbf{Upcycling.}
Leveraging pretrained models to expedite the training of larger dense models is also known as model growth \citep{chen2015net2net}.
In the context of training MoEs reusing dense pretrained models, \citet{komatsuzaki2022sparse} are the first studying such a scenario with encode-decoder models.
There are studies
\citep{hu2024minicpm,yang2024qwen2,lo2024closer,wei2024skywork,muennighoff2024olmoe,he2024upcycling} offering insights into upcycling decoder-only transformers, systematic investigation has not been presented.
\citet{wei2024skywork} made only a rough recommendation: use upcycling when the budget for training is smaller than twice the budget used for dense training, while we provide a more general guideline.

\textbf{Scaling laws.}
Power-law scaling appears in a variety of natural and human-made phenomena.
Scaling studies that are perhaps closest to our work are those considering two-stage training, e.g., transfer learning, fine-tuning, and model growth \citep{mikami2022scaling,zhang2024scaling,du2024stacking}. 
A notable similar phenomenon in transfer learning is \textit{ossification}, where pretraining can hurt fine-tuning performance \citep{hernandez2021scaling}.
However, to our knowledge, no prior work incorporates an interaction term ($\alpha_3$ in Equation \ref{eq:dataonly_law}) to capture such effects.

In the context of MoE, \citet{clark2022unified} studied how different architectural choices affect MoE's scaling, while \citet{krajewski2024scaling} investigated fine-grained experts' scaling behavior.
The latter work fits a joint scaling law with respect to dataset and model sizes, but we find differences in the obtained parameters.
It is likely due to several differences in methodology: the largest dense model they experimented with was smaller (85M), and they did not tune the LR for each model size.
Nevertheless, our findings that MoE scales better than its dense counterpart with sufficiently large computational budget do agree with theirs.

\section{Discussion and Conclusion}
\label{sec:conclusion}

In this paper, we have presented compelling evidence that MoE upcycling follows novel scaling laws with dataset and model configuration, revealing trade-offs due to interactions between dataset sizes.

Given the complexity of upcycling, deriving a unified scaling law that allows for simultaneous optimization of dataset size and model configuration is beyond our current computational reach.
However, our findings still provide practical guidance, as the joint scaling law (Equation \ref{eq:joint}) enables accurate predictions of upcycling performance once the MoE architecture is fixed.
In practice, we recommend selecting the largest viable MoE size and sparsity within compute and memory limits while balancing this choice against the expected training token budget.
Once the architecture is set, Equation \ref{eq:joint} can guide dataset/model scaling, demonstrating reliable extrapolation of upcycling performance.

While our empirical formulae successfully capture the observed scaling behavior, the underlying mechanism, particularly the interaction term, remains theoretically unexplained. 
To our knowledge, formal justification for such interactions is lacking in the literature \citep{bahri2024explaining,paquette20244+}.
Future work could explore it through optimization dynamics or loss landscape analysis.
Additionally, extending our scaling laws to alternative MoE architectures and encoder-decoder models would help assess their generality and implications.





\bibliography{ref}
\bibliographystyle{abbrvnat}


\newpage
\appendix

\section[short]{More on architecture and experimental design}
\label{app:exp}
\subsection{Megatron-LM configuration}
\textbf{Infrastructure.}
Our experiments are performed on multiple nodes, each consisting of 8 NVIDIA H100 80 GB GPUs, interconnected via InfiniBand HDR.
The software we use for training is the Megatron-LM library \citep{shoeybi2019megatron}.

We use and modify the Megatron-LM (core v0.8.0) library for our experiments. \footnote{\url{https://github.com/NVIDIA/Megatron-LM}}
Models are trained with data type bfloat16.
Except for the largest MoE we train (8x1B), which has tensor parallelism configured to be 2, all models are trained with data and sequence parallelisms only \citep{korthikanti2023reducing}.
Other optimization libraries used include FlashAttention \citep{dao2022flashattention} and TransformerEngine. \footnote{\url{https://github.com/NVIDIA/TransformerEngine}}
See the example scripts included in the supplementary material.

\subsection{Model configuration}
Let us elaborate more on our architectural choices.
The intermediate hidden dimension size, $d_{\rm MLP}$, is set to be $4d_{\rm model}$.
We do not implement bias in the linear layers.
We also do not use techniques geared for treating training instabilities (which we did not encounter in our study), such as Z-loss or QK normalization. 
Efficiency-motivated implementations like grouped query attention are not considered as well for simplicity.
The number of attention head is chosen to increase with model size following practices in the LLM literature.
Other designs of the architecture follow Llama2's closely \citep{touvron2023llama}.
See Table \ref{tab:config} for the model configurations.
They are selected such that the ratio $n_{\rm layer}/d_{\rm model}$ lies in the range 32 to 64, as in \citet{kaplan2020scaling}.
We use the smaller models for ablation studies.

\subsection{MoE configuration}
Let us describe the routing mechanism within the MoE module studied in this work.
Denote the number of experts by $n_{\rm expert}$, the number of activated experts by $n_{\rm TopK}$, and the output of expert $i$ by $O_{{\rm exp},i}$.
At each layer, the output tokens $x$ of the attention layer are passed to a router, which consists of a single-layer perceptron with weight $W$, responsible for calculating
$(G_1(x), G_2(x), ..., G_{n_{\rm expert}}(x))$, where
\begin{equation}
   G(x) = {\rm Softmax}({\rm TopK}(W \cdot x))
\end{equation}
The TopK operation ensures that only $n_{\rm expert}$ experts are activated for each token, resulting in sparse computation.
The output of the MoE module, $O_{\rm MoE}$, is the weighted expert outputs which can be written as follows:
$$
O_{\rm MoE}= \sum_{i=1}^{n_{\rm expert}} G_i(x)O_{{\rm exp},i}(x)
$$
Note that this is also known as token-choice algorithm.
Furthermore, we do not use the token-dropping mechanism as in \citet{fedus2022switch}.
We also do not study MoE variants such as shared experts \citep{dai2024deepseekmoe} and fine-grained experts \citep{krajewski2024scaling}, as upcycling these variants is not straightforward.

Let us move to discussing the load-balancing loss.
It has the form \citep{fedus2022switch}:
$$
\mathcal{L}_{\text{aux}} =  \frac{4\eta}{T^2} \cdot \sum_{i=1}^{n_{\rm experts}} \left( \left( \sum_{j=1}^{T} P_{j, i} \right) \cdot Q_i \right), 
$$
where $\eta$ is the coefficient for the auxiliary loss, $T$ is the number of tokens, $P_{j, i}$ the router output probability for token $j$ to be assigned to token $i$, and $Q_i$ is the number of token assigned to expert $i$.
We ablate the coefficient in Appendix \ref{app:aux}.

\subsection{Ablation of learning rate schedules}
\label{app:schedule}
Here, we compare the performances of using WSD and the commonly used learning rate (LR) cosine schedules. 
Dense model and MoE used in our ablation are 0.1B and 8x44m respectively, with training configuration given in Table \ref{tab:schedule}.
We can see from Figure \ref{fig:ablate_wsd} that both schedules yield similar performances.
\begin{figure}[h]
    \centering
        \includegraphics[width=0.4\linewidth]{ablate_wsd_loss.png}
        \includegraphics[width=0.4\linewidth]{ablate_wsd_lr.png}
        \caption{\textbf{Comparing WSD and cosine schedules of learning rate.}
        Left: we see that different schedules cause little differences between the losses, for both dense and MoE training.
        Right: the learning rate schedules in use are shown.}
    \label{fig:ablate_wsd}
\end{figure}

\subsection{Training configuration}
The common setup of training is shown in Table \ref*{tab:common}, and the model-dependent setup (warmup iteration, standard deviation of the normal distribution for initializing weights, maximum iteration run, battch size, tuned LR) is shown in Table \ref{tab:train_config}.
As described in the main text, we use the WSD schedule for training.
The number of warmup steps of the WSD LR schedule is set to be roughly the same as the total model size \citep{porian2024resolving}.
Linear decay to 10\% of the maximum LR value is used in the last stage of the schedule, with the length set to be around 10\% of the training length, following \citet{hagele2024scaling}.

Logarithmically-spaced intermediate checkpoints are saved and used to emulate different numbers of token budget. 
We also increase both the training length and batch size with model size following common practices without performing precise tuning. 

\begin{table}[htb]
    \centering
    \begin{tabular}{l|cccrr} 
        \toprule
        Model  & $n_{\rm layer}$ & $d_{\rm model}$ & $n_{\rm head}$ & $N_{\rm dense}$& $N^{\rm total}_{\rm MoE}$ (8 experts)\\
        \midrule
        \textbf{15M} & 9  & 320  & 4 &14,751,680&92,189,120
        \\
        \textbf{44M} & 12  & 480  & 8 &44,248,800&276,538,080\\
        \textbf{0.1B}  & 15 & 640  & 8 &98,323,840&614,496,640 \\
        \textbf{0.2B}  & 21 & 832 & 8 &232,623,040& 1,453,845,952\\
       \textbf{0.5B}  & 26 & 1120 & 16 &521,889,760& 3,261,732,320\\
        \textbf{1B}& 30& 1504& 16&1,085,859,424&6,786,500,704\\
        \bottomrule
    \end{tabular}
    \caption{Dense models used in our study and their parametric details. 
    Note that $d_{\rm MLP}$, is set to be $4d_{\rm model}$.
    In the last column, we show the total non-embedding model parameters of the corresponding MoE with 8 experts.}
    \label[type]{tab:config}
    \end{table}

\begin{table*}
    \centering
    \begin{tabular}{l|l}
    \hline
    Configuration             & Details \\ \hline
    Context length & 1,024 \\
    Embedding tying & False \\
    Optimizer & AdamW \citep{loshchilov2017fixing} \\    
    Adam $\beta_1$                   & 0.9 \\
    Adam $\beta_2$                & 0.95 \\
    Adam $\epsilon$  & 1e-8 \\
    Weight decay                 & 0.1 \\
    Gradient clipping   & 1.0 \\
    \end{tabular}
    \caption{Training configuration used throughout the paper.}
    \label{tab:common}
    \end{table*}
\begin{table*}[htb]
    \centering
    \begin{tabular}{l|ccccc} 
        \toprule
        Model & warmup iter. & init. size & Max iter. & batch size &LR (8x)\\
        \midrule
        \textbf{15M} & 200  & 0.035  & 17,600& 128&8e-3 (2e-3) 
        \\
        \textbf{44M}  & 200  & 0.029  & 17,600& 256&4e-3 (2e-3) \\
        \textbf{0.1B}   & 200 & 0.025  & 17,600 &512&4e-3 (2e-3)  \\
        \textbf{0.2B}  & 400 & 0.022 & 35,200 &512&2e-3 (2e-3)  \\
        \textbf{0.5B}  & 800 & 0.019 & 70,400 &512&4e-4 (4e-4)  \\
        \textbf{1B}& 800& 0.016& 70,400&1024&4e-4 (4e-4)\\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Model-dependent training configuration}. 
    "init. size" refers to the standard deviation of the normal distribution used for initializing the weights.
    "Max iter." refers to the maximum iteration run on the model.
    The MoE counterpart uses the same configuration except for the learning rate (last column). }
    \label{tab:train_config}
    \end{table*}

    \begin{table*}
        \centering
        \begin{tabular}{l|l}
        \hline
        Configuration             & Details \\ \hline
        Batch size & 512 \\
        train iter.                    & 4,000 \\
        Warmup iter.               & 200 \\
        Auxiliary loss coeff.                 &  $10^{-3}$ \\
        \hline
        \end{tabular}
        \caption{Training configuration for ablation studies.}
        \label{tab:schedule}
        \end{table*}

        \subsection{Upcycled training's configuration}
        We initialize the router weights from a normal distribution with zero mean and variance of $2/5d_{\rm model}$ \citep{le-scao-etal-2022-language,chowdhery2023palm,zeng2022glm} (the same initialization is used for from-scratch trainings).

        Regarding the learning rate (LR) of upcycled training, there are several choices: using the LR at the end of dense training with a constant LR schedule, i.e., treating upcycled training as a kind of fine tuning; using the LR of dense/MoE training \citep{komatsuzaki2022sparse,he2024upcycling}.
        We consider these three choices without retuning the LR.
        With the other training settings are set to be the same as the one used in training the dense models, including the use of the WSD LR schedule, we find that using the MoE LR leads to better performance.
        See Figure \ref{fig:ablate_upcycle_lr}.

As a side note, we observe that the loss for the constant LR schedule decreases monotonically, while the loss increases initially for other cases; there is a \textit{rewarming} stage when using the WSD LR schedules, which can also be observed in Figure \ref{fig:lossplot_0.1b}.

        Let us further comment on alternative upcycling methods that could potentially further accelerate training. 
       Adding some form of noises to the dense MLP weights, or modifying partially the weights would intuitively help upcycled training generalize faster.
        Our preliminary experiments however did not see any advantages of doing so.
        Note that this observation is also consistent with previous negative reports \citep{komatsuzaki2022sparse,wei2024skywork,muennighoff2024olmoe}.
        Henceforth, we simply copy the weights directly from the dense model to perform upcycled training. 

\begin{figure}[h]
    \centering
        \includegraphics[width=0.4\linewidth]{ablate_upcycle_lr.png}
        \caption{\textbf{Ablation of LR when upcycling an 8x0.1B MoE.}
        We compare the performance of upcycled training (from a dense model trained for 2B tokens) using constant LR ($2\times 10^{-4}$, LR at the end of dense training), LR used for dense training, and LR used for MoE training.
        We find that the latter works the best. 
        The training setup follows Table \ref{tab:schedule}.
       }
    \label{fig:ablate_upcycle_lr}
\end{figure}

\subsection{Ablation of auxiliary coefficients for load-balancing loss}
\label{app:aux}
The auxiliary loss affects the load-balancing loss (smaller values meaning more balanced expert usage) as well as the cross-entropy loss.
We plot these losses in Figure \ref{fig:ablate_aux} for training a 8x0.1B MoE.

We see that, setting the coefficient to be too small leads to imbalance in expert usage, while setting the coefficient to be too large interferes with the cross-entropy loss.
We set it to around $10^{-3}$ which gives the right balance.
We did not make finer tuning of the coefficients and adopted $10^{-3}$ in our experiments.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.4\linewidth]{aux.png}
        \includegraphics[width=0.4\linewidth]{aux_lb.png}
        \caption{\textbf{Ablating auxiliary coefficients.}
        Left: Cross-entropy losses, where it can be seen that auxiliary coefficient of 1 performs worst.
        Right: Load-balancing losses, where the larger the coefficient is, the smaller the load-balancing loss becomes.
        Setting the coefficient to be $10^{-3}$ gives the right balance between these two losses.}
    \label{fig:ablate_aux}
\end{figure}

\subsection{Ablation of dataset repetition for upcycling}
In our experiments, we have used the same dataset for training both dense and upcycled models, following practices of \citep{he2024upcycling}.
As an ablation, we split our dataset to two non-overlapping portions, and perform the two-stage training with the distinct datasets.
In Figure \ref{fig:repeat}, we find that the difference in performance is very small.
This also aligns with previous investigation on data repetition, where it is shown that there is a little difference in performance up to 4 times of repetition \citep{muennighoff2023scaling}.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.4\linewidth]{data_repeat.png}
        \caption{\textbf{Ablation of data repetition when upcycling an 8x0.1B MoE.}
        We do not observe notable difference in the loss.
        The training setup follows Table \ref{tab:schedule}.
       }
    \label{fig:repeat}
\end{figure}

\subsection{Evaluation with standard benchmarks and comparison with other existing models}
\label{app:eval}
We compare the performance of our trained 1B models against existing models with similar sizes, Pythia \citep{biderman2023pythia} and TinyLlama \citep{zhang2024tinyllama}, based on standard natural language processing benchmarks, ARC \citep{clark2018think}, lambada \citep{paperno2016lambada}, logiqa \citep{liu2020logiqa}, piqa \citep{bisk2020piqa}, sciq \citep{welbl2017crowdsourcing}, and winogrande \citep{sakaguchi2021winogrande}.

Table \ref{tab:eval} shows the results.
First, we see that the dense model perform similarly to the open models, indicating that our models have been trained correctly.
Second, we see that the upcycled model achieves overall the best performance (note that the total tokens used for dense and upcycled training are around 100B, similar to those under comparison), also indicating that upcycling has progressed correctly and improved the scores. 

\begin{table}[h!]
    \centering
    \begin{tabular}{lcc|cc}
    \toprule
    \textbf{Models} & \textbf{Pythia-1B} & \textbf{TinyLlama-1.1B} & \textbf{1B} & \textbf{Upcycled 8x1B} \\
    \midrule
    \textbf{Datasets} & Pile & Slimpajama \& Starcoder & Slimpajama & Slimpajama \\
    \textbf{Tokens}& 100B & 103B & 74B & 37B \\
    \midrule
    ARC-c        & 25.59 & 24.32 & 27.65 & \textbf{30.12} \\
    ARC-e        & 47.26 & 44.91 & 52.10 & \textbf{56.14} \\
    lambada      & \textbf{53.52} & -     & 45.08 & 49.72 \\
    logiqa       & \textbf{29.49} & -     & 26.11 & 27.65 \\
    piqa         & \textbf{69.31} & 67.30 & 65.89 & 67.19 \\
    sciq         & 77.3  & -     & 78.10  & \textbf{82.00} \\
    winogrande   & 51.22 & 53.28 & 54.93 & \textbf{57.77} \\
    \midrule
    \textbf{Avg.} &50.53 & -     &49.98  &  \textbf{52.94}\\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Benchmarks' performance comparison across models.}
    Reported scores are accuracies (normalized by byte length whenever applicable).
    The first two columns are scores of existing models.
    The last two columns are evaluation results of models trained in this work.
    The upcycled 8x1B model is upcycled from the 1B model.
    Our models are evaluated with the LM Evaluation Harness v0.4.0 library \citep{eval-harness}.}
    \label{tab:eval}
    \end{table}

\subsection{Estimated Total GPU hours}
Instead of reporting the actual runtimes on our cluster, which varied in our experiments due to many factors affecting the cluster (number of available nodes, congestion, etc.), we give a theoretical estimate of total GPU hours used for obtaining the joint scaling law, which involves running the largest tested model with most training tokens in this paper.

The estimate is as follows.
We calculate the FLOPs for training the dense, MoE, upcycled MoE models with maximum iterations using the $6ND$ approximation, ignoring the additional FLOPs required to continued pretrain models with shorter iterations (as we can reuse the intermediate checkpoints).
We further assume that the per-second TFLOPs of the GPU is 400, and is the same for both dense and MoE models. 
 \footnote{\url{https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/transformer/moe/README.md}.
 Note that MoE requires more GPU memory to store its total model parameters, inducing overhead that may slow down training.}
We obtain,

\textbf{Dense model:} 
$6.14 \times 10^{17}$ FLOPs

\textbf{MoE:} 
$1.08 \times 10^{18}$ FLOPs

\textbf{Upcycled MoE:} 
$5.38 \times 10^{18}$ FLOPs

The total GPU hours are henceforth approximately 4,900.  
We note that the exact total GPU hours used are larger as we have run various additional studies as detailed in the paper.
We further note that using the cosine learning rate schedule would cost about twice more GPU hours when varying the number of training token budget.
\section{Generalization across other datasets}
\label{app:dataset}
We show that the scaling behavior studied in the main text generalizes across datasets in use.
Aside from Slimpajama \citep{together2023redpajama,cerebras2023slimpajama,shen2023slimpajama} used in the main text, we test with two additional datasets: Japanese portion of the CommonCrawl corpus, and the Stack code dataset \citep{kocetkov2022stack}.
We use a tokenizer of enlarged vocabulary size of 102,400 when training with the former dataset. \footnote{\url{https://huggingface.co/sbintuitions/sarashina2-70b}}
We upcycle a 0.1B model with the training configuration set to the same as those used for the experiments presented in the main text.
The results are presented in Figure \ref{fig:otherdata}, where the scaling behaves similarly to those presented in Figure \ref{fig:dataonly}, suggesting that our scaling law for dataset are independent of dataset in use.
We however do not further study scaling law for model configuration and the joint scaling law due to computational constraints.
\begin{figure}[h]
    \centering
        \includegraphics[width=0.4\linewidth]{dataonly_fit_global_jp.png}
        \includegraphics[width=0.4\linewidth]{dataonly_fit_global_code.png}
        \caption{\textbf{Scaling behavior generalizes across datasets used.}
        Left: Japanese dataset.
        Right: Code dataset.}
    \label{fig:otherdata}
\end{figure}




\section{More results on scaling law for dataset sizes}
\label{app:fit_dataset_law}
\subsection{Fitting across architectures}
Figure \ref{tab:fit_arch} shows the fitting of the scaling law for dataset sizes across architectures (fixing dense model size to 0.1B).
\begin{table}[htb]
    \centering
    \begin{tabular}{l|cccc} 
        \toprule
         ($n_{\rm expert}$,$n_{\rm TopK}$) & (4,1) & (4,2) & (8,1) & (8,2)   \\
        \midrule
        $\alpha_1$ &0.28  & 0.19& 0.29&0.18 \\
        $\alpha_2$ & 0.29 & 0.20 & 0.30 & 0.20  \\
       $\alpha_3$ & 0.01 & 0.008 & 0.01 & 0.008  \\
        \bottomrule
    \end{tabular}
    \caption{
    \textbf{Fitted exponents of the scaling law for dataset sizes across architectures.}}
    \label{tab:fit_arch}
\end{table}

\subsection{More validation of Equation \ref{eq:dataonly_law}}

We first make an observation from Figure \ref{fig:dataonly} that the slope of the fitted lines, that is, the scaling exponent as in Equation \ref*{eq:power_d2} is decreasing with $D_1$.  
 Let us consider the scaling factors $A$ fitted in Figure \ref{fig:dataonly}. 
The scatter plot of $A$'s in Figure \ref{fig:dataonly_add} leads us to deduce that the following relation:
\begin{equation}
A \propto  D_1^{-\eta} \label{eq:scaling}
\end{equation}
Indeed, we find that the above equation fits well in the Figure.
Substituting it to Equation \ref*{eq:power_d2}, we obtain at the RHS a term of the form $ D_1^{- \eta}D_2^{-\alpha}$, indicating that Equation \ref{eq:dataonly_law} arises not only from first principles as in the main text, but from empirical observations as well.
\begin{figure*}[h]
    \centering
        \includegraphics[width=0.3\linewidth]{dataonly_fit_global_15m.png}
        \includegraphics[width=0.3\linewidth]{dataonly_fit_global_0.1b.png}
        \includegraphics[width=0.3\linewidth]{dataonly_fit_global_1b.png}
        \includegraphics[width=0.3\linewidth]{dataonly_fit_cap_a_15m.png}
        \includegraphics[width=0.3\linewidth]{dataonly_fit_cap_a_0.1b.png}
        \includegraphics[width=0.3\linewidth]{dataonly_fit_cap_a_1b.png}

    \caption{
        \textbf{Top: Fitting plots same as those in Figure \ref{fig:dataonly}.}
        \textbf{Bottom: Fitting scaling factor has power-law behavior.} The fitted scaling factors are shown to fit well with Equation \ref{eq:scaling}.
        }    \label{fig:dataonly_add}
\end{figure*}
 \subsection{Bilinear form}
We would also like to note that by taking the logarithm of the first term of Equation \ref*{eq:multiplicative}, we have $\alpha_1 \log D_1 + \alpha_2 \log D_2 +  \alpha_3\log D_1 \log D_2$, which is \textit{bilinear} in terms of $\log D_1$ and $\log D_2$ (that is, linear with each variable separately), indicating that the multiplicative term with interaction is a natural generalization of the single-variable power law. 
\section{More Results on the Joint Scaling Law}
\label{app:fit}
\subsection{Validating Requirement 3}
We show in Figure \ref{fig:N1_scaling} that the upcycled model empirically satisfies power-law scaling with respect to $N_1$, i.e., Requirement 3, repeated below for convenience.

\textbf{Requirement 3.}
$L_{D_1,D_2}(N_1) = B_{D_1,D_2} N_1^{-\beta} + E_{D_1,D_2}$.

\begin{figure*}[h]
    \centering
        \includegraphics[width=0.5\linewidth]{N1_scaling.png}

    \caption{
        \textbf{Upcycled model has power-law behavior with respect to $N_1$.} 
        }    \label{fig:N1_scaling}
\end{figure*}

\subsection{Another functional form satisfying Requirements 3 and 4}
\label{app:other_fit}
For convenience purposes, let us repeat Requirement 4:

\textbf{Requirement 4.}
${\rm lim}_{D_2\to 0} L(D_1,D_2, N_1) = A D_1^{-\alpha}+ B N_1^{-\beta} + E$, i.e., approaching Chinchilla scaling law.

There is another functional form satisfying Requirement 3 and 4:
\begin{align}
    L(D_1,D_2,N_2) =  \frac{A}{D_1^{\alpha_1}} + \frac{B}{D_2^{\alpha_2}N_1^{\beta_2- \alpha_3\log D_2}} + E 
\end{align}
Fitting the above equation however yields negative exponents, which are, empirically, 
$\alpha_1=0.10,\alpha_2=-0.07,\beta=-0.08,\alpha_3=-0.008$.
We reject the hypothesis of this functional form as the loss is predicted to increase with dataset/model sizes, violating the power-law ansatz as well as empirical observation.

\subsection{Why fitting scaling laws separately?}
We provide reasoning on why we fit three scaling laws separately, instead of fitting an unified scaling law which holds for all three dense, MoE from-scratch, and upcycled training scenarios.

The scaling law for MoE trained from scratch is expected to have parameters different from those for upcycled training (that is, they are not equal to each other at $D_1\to 0$), because upcycled MoEs are initialized with identical MLP/expert weights from the dense models, while MoEs trained from-scratch have different initial (random) MLP/expert weights.
Thus, we expect them to scale differently.

Similarly, we expect the scaling law for dense model has parameters different from those for upcycled training at the limit $D_2\to 0$.
As can be seen from Figure \ref{fig:lossplot_0.1b}, the upcycled training undergoes a \textit{rewarming} phase in the beginning, where the loss increases initially before decreasing.
This causes a deviation from the dense model scaling law at $D_2\to 0$, although we still expect it to correlate with the dense model scaling law, i.e., Requirement 2 should hold based on function preservation and (empirical) observations that rewarming does not completely de-correlate the loss behaviors.

Finally, our preliminary investigations also show that unified scaling law does not fit well.
We consequently consider them separately.
Moreover, we note that there can be other possibilities of functional forms that we do not explore further.

\section{More implication of the Joint Scaling Law}
\subsection{From-scratch training vs Upcycling}
We have shown in the main text that upcycling is only effective when the sunk cost or the model size is small.
We further visualize this by plotting the losses for model sizes 1B, 7B and 70B, fixing $D_1$ to various values, with respect to $D_{\rm MoE}$ and $D_2$ for from-scratch and upcycled training respectively in Figure \ref{fig:budget2}.

\begin{figure*}[h]
    \centering
        \includegraphics[width=0.3\linewidth]{8x1b_budget.png}
        \includegraphics[width=0.3\linewidth]{8x7b_budget.png}
        \includegraphics[width=0.3\linewidth]{8x70b_budget.png}
    \caption{
        \textbf{Token budget of from-scratch MoE training and when it catches up with upcycled MoE's performance.}
        We compare loss-versus-token plots of from-scratch and upcycled MoE trainings at various dense training budgets (sunk costs) and model sizes. 
        Upcycling is considered to be efficient only when $D_{\rm MoE}>$ sunk cost.
        We observe that the efficiency of upcycling diminishes with sunk cost and model size. 
    }
        \label{fig:budget2}
\end{figure*}

\subsection{More on compute optimality of upcycling}
\label{app:com-opt}

As mentioned in the main text, while training the Mixtral-like MoE from scratch is shown to be more efficient in the long run, it requires more compute (1.75 times larger than dense training).
There is a compute-performance trade-off between these stages of training.

We  show in Figure \ref{fig:budget_app} that the optimization results in Section \ref{subsec:com-opt} that training from scratch can always be considered to be more performant.
\begin{figure*}[ht!]
    \centering
        \includegraphics[height=0.35\linewidth]{alloc.png}
    \caption{
         \textbf{Compute-optimal training.} Upcycled training performs worse even when $D_1,D_2,N_1$ are allocated optimally, compared to compute-optimal training of dense or MoE models. 
    }
        \label{fig:budget_app}
\end{figure*}






\subsection{Derivation of compute-optimal formula}
As in the main text, we want to scale $N_2,D_2$ optimally, $N^{\rm opt}_2,D^{\rm opt}_2$, given a FLOPs budget and fixing $D_1$, while minimizing the loss $L$, which we write as $L_{D_1}(D_2,N_1)$.
This is equivalent to solving the following:
\begin{align*}
    \left. \frac{\partial}{\partial D_2} L_{D_1}(D_2, C_2/10.5D_2) \right|_{D_2=D_2^{\rm opt}} = 0,\\
    \left. \frac{\partial}{\partial N_1} L_{D_1}( C_2/10.5N_1, N_1)\right|_{N_1=N^{\rm opt}_1} = 0
\end{align*}
where we have used $N_2=1.75N_1$ and $C_2 = 6 N_2 D_2$.
Solving the above equations leads to
\begin{align*}
    D^{\rm opt}_2 &= G \left(\frac{C_2}{10.5}\right)^{a}, \\
    N^{\rm opt}_1 &= G^{-1} \left(\frac{C_2}{10.5}\right)^{b}
\end{align*} 
where
\begin{align*}
    G &\coloneq \left( \frac{ A_{\rm eff}\alpha_{\rm eff}}{B\beta}\right)^{1/(\alpha_{\rm eff} + \beta)} \\
    a &\coloneq \frac{\beta}{\alpha_{\rm eff}+ \beta} \\
    b &\coloneq \frac{\alpha_{\rm eff}}{\alpha_{\rm eff}+ \beta}\\
    A_{\rm eff} &\coloneq A D_1^{-\alpha_1}\\
    \alpha_{\rm eff} &\coloneq \alpha_2-\alpha_3\log D_1
\end{align*}
We can henceforth relate $D^{\rm opt}_2$ and $N^{\rm opt}_1$ via 
$$
D^{\rm opt}_2 = G \left(GN^{\rm opt}_1 \right)^{a/b} \propto \left(N^{\rm opt}_1 \right)^{\beta/\alpha_2-\alpha_3\log D_1}
$$
and
$$
N^{\rm opt}_1 = G^{-1} \left(G^{-1}D^{\rm opt}_2 \right)^{b/a} \propto \left(D^{\rm opt}_2 \right)^{(\alpha_2-\alpha_3\log D_1)/\beta}
$$
\end{document}
