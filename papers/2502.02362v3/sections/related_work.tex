\section{Related work}
\label{sec:related_work}
% \paragraph{Math reasoning with LLMs:} The reasoning capabilities of large language models (LLMs) are often tested on mathematical and scientific reasoning benchmarks such as \cite{hendrycks2021measuringmassivemultitasklanguage,arora-etal-2023-llms,wang2024scibench}. Deriving a solution to a mathematical problem requires complex planning, recalling formulae, and grounding the plan to the given problem \cite{arora-etal-2023-llms} to derive the final answers. A popular approach to improve such reasoning is to first generate a series of rationales and then generate the answer \cite{wei2023chainofthoughtpromptingelicitsreasoning} instead of directly generating the final answer. Previous work also investigated reasoning in a structured fashion, rather than step-by-step, such as Tree of thoughts \cite{yao2023treethoughtsdeliberateproblem}, Graph of thoughts \cite{Besta_2024}, etc. 
% \paragraph{Evaluation of Chain-of-thoughts:} While rationale generation helps reasoning, it is important to verify whether the reasoning process was correct to have a holistic view of the reasoning capabilities of the model. 
% Although earlier work has focused on word overlap or embedding similarity to evaluate text generation, they are not equipped well enough to judge deductive reasoning capabilities \cite{celikyilmaz2021evaluationtextgenerationsurvey, reiter-2019-natural}.  Other existing efforts include Roscoe \cite{golovneva2023roscoesuitemetricsscoring}, which uses SimCSE \cite{gao2022simcsesimplecontrastivelearning} embeddings to find similarity between the context and the generated solution to identify multiple types of errors from a taxonomy. Receval \cite{prasad2023recevalevaluatingreasoningchains} extends this further with metrics based on informativeness and correctness. Socreval \cite{he2024socrevallargelanguagemodels} further extends these frameworks using GPT4 \cite{openai2024gpt4technicalreport} in the loop with in-context learning to alleviate the need for costly human annotations and proposes a completely training-free approach.
% While such methods perform well in a reference-free setting, they fail to judge correctness at a step level, and instead assign a chain level score. Furthermore, the metrics used in these methods offer limited explainability, making it difficult to clearly identify the specific error type associated with each reasoning step. In contrast, our framework provides a natural language description of both the reasoning step’s correctness and its corresponding error type.


% \paragraph{Verifiers for math reasoning:} Verifiers of the reasoning chain have multiple applications in the reasoning of LLMs. \citet{uesato2022solvingmathwordproblems} and \citet{lightman2023letsverifystepstep} showed that step-by-step process rewards generated by a verifier lead to a stable training of LLMs and improve accuracy on end tasks. Deductive Beam Search \cite{zhu2024deductivebeamsearchdecoding} trained a verifier on manually crafted datasets and used it in beam search to improve the accuracy of the end task. Although existing work has shown that a verifier in the loop can improve LLM reasoning in multiple stages of the pipeline (training/inference), a clear approach on how to train a reliable verifier is missing from prior work. We address this issue in our work. The closest to our work is \citet{ling2023deductiveverificationchainofthoughtreasoning}, however, their approach requires the models need to generate the reasoning alongside the premises in their natural program style. This has a drawback in that it requires the model to generate the reasoning chain in a certain way, making it impossible to verify a general purpose chain of thought, hurting its scalability. In contrast, our approach to identify premises as a separate step and proposing it as a task ensures generalizability across reasoning styles and is not under any assumptions. 

\paragraph{Math Reasoning with LLMs.}  
Mathematical and scientific reasoning tasks have become a primary testbed to assess the capabilities of large language models (LLMs) \cite{hendrycks2021measuringmassivemultitasklanguage,arora-etal-2023-llms,wang2024scibench}. Solving these tasks requires complex planning, recall of relevant formulae, and grounding the solution to the given problem \cite{arora-etal-2023-llms}. A widely adopted approach to improve the reasoning of LLMs is to generate intermediate rationales, commonly known as chain-of-thought (CoT), before predicting a final answer \cite{wei2023chainofthoughtpromptingelicitsreasoning}. Recent work explores alternative structures for organizing these solutions, including Tree-of-Thoughts \cite{yao2023treethoughtsdeliberateproblem} and Graph-of-Thoughts \cite{Besta_2024}.

\paragraph{Evaluation of Chain-of-Thoughts.}
Although generating rationales can significantly improve model reasoning, systematically evaluating these explanations remains a challenge. Traditional metrics focused on word overlap or embedding similarity \cite{celikyilmaz2021evaluationtextgenerationsurvey, reiter-2019-natural} fail to capture logical soundness, especially for step-by-step deductions. To address this gap, methods such as Roscoe \cite{golovneva2023roscoesuitemetricsscoring}, Receval \cite{prasad2023recevalevaluatingreasoningchains}, and Socreval \cite{he2024socrevallargelanguagemodels} offer reference-free evaluation frameworks that assess correctness and identify various categories of errors. However, these methods generally provide a single chain-level score and offer limited explainability, making it difficult to pinpoint specific error types in individual reasoning steps. In contrast, our proposed framework provides a natural language characterization of the correctness of each step, providing fine-grained insights into the precise nature of errors.

\paragraph{Verifiers for Math Reasoning.}  
Verifiers have proven effective in enhancing LLM-based reasoning. \citet{uesato2022solvingmathwordproblems} and \citet{lightman2023letsverifystepstep} demonstrate that providing reward signals for intermediate steps can stabilize training and improve final performance. Deductive Beam Search \cite{zhu2024deductivebeamsearchdecoding} similarly incorporates a trained verifier to refine model outputs during inference, leading to higher accuracy for the final task. While these studies highlight the benefits of verifiers, a clear strategy for training a robust verifier, one that generalizes across diverse problem formats, remains elusive. The closest existing effort, \citet{ling2023deductiveverificationchainofthoughtreasoning}, requires models to encode premises in a particular “natural program” style, which constrains applicability to general chains-of-thought. In contrast, our method treats premise extraction as a standalone step and refrains from making strong assumptions about how the reasoning chain is structured.

%\zhenhailong{add our distinctive contribution here compared with AssistantBench/MindSearch}

% \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,label=simplify_expression]
% \textbf{Q: Simplify the following expression:
% \[
% \frac{\sec x}{\sin x} - \frac{\sin x}{\cos x}
% \]}

% \textbf{Step 1: (Correct \textcolor{green}{\ding{51}})} expressing the given expression in terms of sine and cosine,
% \[
% \frac{\sec x}{\sin x} - \frac{\sin x}{\cos x} = \frac{1}{\sin x \cos x} - \frac{\sin x}{\cos x}.
% \]
% \textbf{Step 2: (Error \textcolor{red}{\ding{55}})} Rewriting, we get
% \[
% \frac{1}{\sin x \cos x} - \frac{\sin x}{\cos x} = \frac{1}{\sin x \cos x} - \frac{\sin x}{\cos x} \cdot \frac{1}{1}.
% \]
% \[
% = \frac{1}{\sin x \cos x} - \frac{\sin x}{\cos x \cdot \sin x}.
% \]

% \textbf{Step 3: (Accumulation Error \textcolor{red}{!!})} Simplifying further
% \[
% \frac{1}{\sin x \cos x} - \frac{\sin x}{\sin x \cos x} = \frac{1 - \sin x}{\sin x \cos x}.
% \]

% \textbf{Step 4: (Error \textcolor{red}{\ding{55}})} expressing the numerator in terms of a cosine
% \[
% \frac{1 - \sin x}{\sin x \cos x} = \frac{\cos^2 x}{\sin x \cos x}
% = \frac{\cos x}{\sin x}
% = \cot x.
% \]


% \textbf{Feedback:} There are two wrong steps in this step-by-step process. First, Step 2 incorrectly simplifies $\frac{\sin x}{\cos x} \cdot \frac{1}{1}$ to $\frac{\sin x}{\cos x \cdot \sin x}$. Further Step 4 incorrectly replaces $1 - \sin x$ with $\cos^2 x$, making the final answer correct.
% \end{tcolorbox}


% \begin{comment}
%     \begin{itemize}
%         \item WebLinx \citepp{lu2024weblinx}
%         \item WebArena \citepp{zhou2023webarena}
%         \item WebGPT \citepp{nakano2021webgpt}
%         \item Mind2Web \citepp{deng2024mind2web}
%         \item AutoGPT \citepp{}
%     \end{itemize}
% \end{comment}
    