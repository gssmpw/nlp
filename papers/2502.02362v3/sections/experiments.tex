
% \section{Experimental Details}
% \paragraph{Model} 
% For the Llama model \cite{grattafiori2024llama3herdmodels}, we used vLLM \cite{kwon2023efficientmemorymanagementlarge} for model serving and AzureOpenAI for the GPT4o and GPT4-o1 \cite{openai2024o1} models. To ensure reproducibility, all generations were performed with a temperature=0. To annotate the premise and error types, we used the GPT4o1 model. For all models, we used their instruct variant. 

% \begin{table*}[t!]
%     \centering
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{cc|cccc|cccc}
%         \hline
%         \textbf{Model} & \textbf{Con-} & \multicolumn{4}{c|}{\textbf{GSM Negatives}} & \multicolumn{4}{c}{\textbf{GSM Positives Perturbed}} \\
%         \textbf{Name}& \textbf{text} & \textbf{COR ($\uparrow$)} & \textbf{ERR ($\uparrow$)} & \textbf{AE ($\uparrow$)} & \textbf{Avg.} & \textbf{COR ($\uparrow$)} & \textbf{ERR ($\uparrow$)} & \textbf{AE ($\uparrow$)} & \textbf{Avg.} \\
%         \hline
%         Llama 3.1        & Full & 97.3 & 44.15 & 4.8 & 55.34  & 87.5  & 66.7  & 9.7 &  58.58 \\ 
%         (8B)             & Partial & 80.77 & 65.28 & 50 & 67.35 & 83.75 & 93.94 & 50.65 & 77.52 \\ 
%         Llama 3.1       & Full & 95.5  & 61.03 & 10.57 & \textbf{60.48} & 97.5  & 86.3  & 35.36 & \textbf{76.13} \\ 
%         (70B)           & Partial & 93.08 & 76.39 & 78.26 & \textbf{84.36} & 95 & 90.91 & 72.73 & 87.32 \\
%         \hline
%         Qwen 2.5         & Full &  89.62 & 19.48 & 2.8 & -  & 87.5  & 46.96 & 3.65 & - \\
%         (7B)             & Partial & - & - & - & - & - & - & - & - \\
%         Qwen2.5          & Full &  78.15 & 29.87 & 4.8 & -  & 78.75 & 65.15 & 12.19 & - \\ 
%         (72B-Instruct)   & Partial & - & - & - & - & - & - & - & - \\ 
%         \hline
%         \multirow{2}{*}{GPT-4o-mini}         & Full &  93.3  & 50.6  & 10.57 & 56.95 & 95    & 87.8  & 25.6 & 72.82 \\
%                                              & Partial & 91.54 & 54.17 & 72.83 & 76.53 & 91.25 & 90.91 & 68,83 & 84.72 \\
%         \multirow{2}{*}{GPT-4o}              & Full &  97    & 46.75 & 9.6 & 57.34  & 100   & 83.3  & 30.48 & 74.76 \\
%                                              & Partial & 93.85 & 54.17 & 82.61 & 80.62 & 96.25 & 86.36 & 80.52 & \textbf{88.91} \\
%         \hline
%     \end{tabular}
%     }
%     \caption{Classification accuracy for the step-level error identification task on GSM Negatives and GSM Positives Perturbed. COR stands for Correct, ERR for Error, and AE for Accumulated Error.}
%     \label{tab:gsm_negatives_perturbed}
% \end{table*}




% \begin{table*}[!t]
%     \centering
%     \setlength\tabcolsep{4pt}
%     \fontsize{8pt}{9pt}\selectfont
%     \begin{tabular}{cccc|ccc|ccc}
%     \toprule
%     \multirow{2}[2]{*}{\textbf{Model Name}} & \multicolumn{3}{c|}{\textbf{Positives}} & \multicolumn{3}{c|}{\textbf{Negatives}} & \multicolumn{3}{c}{\textbf{Synthetic Negatives }} \\
%     \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
%      & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 \\
%     \midrule
%     \textbf{GPT4o-mini} \\
%     Aggregative & 74.93 & 84.81 & 79.56 & 70.85 & 87.01 & 78.10 & 71.84 & 86.73 & 78.59 \\
%     Dyadic & 75.04 & 82.82 & 78.74 & 64.57 & 71.12 & 67.68 & 55.18 & 53.98 & 54.57 \\
%     \midrule
%     \textbf{GPT4o} \\
%     Aggregative & 89.38 & 93.66 & 91.47 & 84.97 & 94.95 & 89.69 & 83.43 & 94.65 & 88.72 \\
%     Dyadic & 75.32 & 99.01 & 85.55 & 67.70 & 92.68 & 78.25 & 67.27 & 79.42 & 72.84 \\
%     \bottomrule
%     \end{tabular}
    
%     \caption{Precision, Recall and F1 scores for the premise mapping task for the GSM8K dataset under the aggregative and dyadic approaches. We saw a consistent drop in F1 score for the dyadic approach, in spite of it being computationally more expensive.}
%     \label{tab:premise_identification_per_candidate_score}
% \end{table*}

\section{Results and Discussion}

    For detailed information on our experimental setup, including model configurations and implementation details, refer to Appendix \ref{appx:model-choice}.

\subsection{Premise Mapping} 
In this section we try to answer RQ1 - ``Can LLMs identify premises to convert a Linear Reasoning Chain to a PARC?"
\paragraph{Metrics:} For identification of premises, we use precision, recall, and F1 score as metrics. However, we would want to highlight to the reader that the most important metric here is Recall. Since marking an extra step as a premise will not hurt the verifiability of the step, missing one will hurt its verifiability and all subsequent steps that rely on this step as a premise. Further, we note that some reasoning chains have a higher number of reasoning steps compared to others. In order to ensure robustness of the metrics, we first compute them at the level of each data point and later average the metrics across the dataset. We tried two approaches for the task of premise mapping.

\begin{table*}[!t]
\centering
\setlength\tabcolsep{4pt}
\fontsize{8pt}{9pt}\selectfont
\begin{tabular}{lcccccccccccccccc}
\toprule
\multirow{2}[2]{*}{\textbf{Model}} & \multicolumn{4}{c}{\textbf{GSM8K}} & \multicolumn{4}{c}{\textbf{MATH}} & \multicolumn{4}{c}{\textbf{Orca Math}} & \multicolumn{4}{c}{\textbf{MetaMathQA}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13} \cmidrule(lr){14-17}
 & Neg & Syn & Pos & Avg & Neg & Syn & Pos & Avg & Neg & Syn & Pos & Avg & Neg & Syn & Pos & Avg \\
\textbf{Llama 3.1 8b} \\
Full Context & 48.87 & 57.68 & 91.9 & 58.6 & 46.91 & 60.29 & 90.61 & 60.2 & 43.26 & 51.2 & 96.7 & 55.06 & 50.47 & 55.15 & 95.17 & 59.45 \\
Model Premises & 54.61 & 76.74 & 65.44 & 64.53 & 52.16 & 59.83 & 50.96 & 54.88 & 55.57 & 59.82 & 67.36 & 59.22 & 54.52 & 69.13 & 62.63 & 61.78 \\
\midrule
\textbf{Llama 3.1 70b} \\
Full Context & 58.35 & 77.03 & 98.61 & 71.37 & 55.55 & 72.83 & 96.24 & 69.77 & 50.33 & 64.25 & 96.74 & \cellcolor{lightblue}63.52 & 55.59 & 73.13 & 97.93 & 69.49 \\
Model Premises & 74.51 & 86.22 & 94.69 & 81.92 & 70.94 & 79.29 & 77.91 & 75.45 & 64.95 & 72.28 & 93.37 & \cellcolor{darkerblue}72.52 & 65.69 & 82.74 & 89.71 & 76.52 \\
\midrule
\textbf{Qwen 2.5 7b} \\
Full Context & 40.88 & 54.13 & 100 & \cellcolor{lightblue}54.94 & 40.47 & 53.60 & 98.07 & 56.26 & 41.20 & 53.69 & 100 & \cellcolor{lightblue}55.85 & 43.36 & 53.18 & 99.39 & \cellcolor{lightblue}56.26 \\
Model Premises & 53.03 & 78.40 & 90.37 & \cellcolor{darkerblue}69.73 & 58.47 & 66.87 & 82.72 & 65.96 & 52.07 & 71.12 & 84.28 & \cellcolor{darkerblue}65.38 & 54.98 & 73.87 & 91.02 & \cellcolor{darkerblue}68.43 \\
\midrule
\textbf{Qwen 2.5 32b} \\
Full Context & 46.76 & 69.26 & 99.8 & \cellcolor{lightblue}63.56 & 49.58 & 66.5 & 98 & \cellcolor{lightblue}65.11 & 49.44 & 63.01 & 99.5 & 63.06 & 49.7 & 67.8 & 99.6 & \cellcolor{lightblue}65.02 \\
Model Premises & 65.58 & 86.34 & 96.95 & \cellcolor{darkerblue}79.37 & 68.61 & 76.96 & 88.71 & \cellcolor{darkerblue}75.57 & 61.89 & 69.24 & 95.25 & 70.26 & 62.18 & 85.60 & 97.32 & \cellcolor{darkerblue}77.40 \\
\midrule
\textbf{Qwen 2.5 72b} \\
Full Context & 47.48 & 67.17 & 100 & \cellcolor{lightblue}63.11 & 53.19 & 67.87 & 99.79 & \cellcolor{lightblue}67.52 & 50.41 & 60.33 & 100 & \cellcolor{lightblue}62.50 & 47.31 & 63.59 & 99.05 & \cellcolor{lightblue}62.20 \\
Model Premises & 69.87 & 85.12 & 96.38 & \cellcolor{darkerblue}80.56 & 69.51 & 79.92 & 90.84 & \cellcolor{darkerblue}77.28 & 69.45 & 71.34 & 94.19 & \cellcolor{darkerblue}74.41 & 64.73 & 85.52 & 98.12 & \cellcolor{darkerblue}78.53 \\
\midrule
\textbf{GPT4o-mini} \\
Full Context & 52.68 & 73.14 & 95.02 & 66.68 & 50.41 & 62.39 & 93.4 & 63.03 & 55.22 & 61.18 & 98.6 & 64.59 & 51.52 & 65.42 & 97.35 & 64.47 \\
Model Premises & 58.25 & 80.81 & 86.72 & 72.33 & 65.84 & 72.4 & 71.88 & 69.45 & 61.18 & 72.48 & 85.94 & 69.93 & 57.04 & 81.63 & 91.15 & 72.51 \\
\midrule
\textbf{GPT-4o} \\
Full Context & 53.81 & 75.3 & 98.33 & 68.52 & 50.9 & 68.41 & 98.57 & \cellcolor{lightblue}66.52 & 59.55 & 65.75 & 100 & 68.55 & 56.26 & 71.72 & 99.3 & 69.41 \\
Model Premises & 65.23 & 86.67 & 99.76 & 79.82 & 70.12 & 76.67 & 91.84 & \cellcolor{darkerblue}76.45 & 66.33 & 71.74 & 93.14 & 72.96 & 67.28 & 88.19 & 89.85 & 79.41 \\
\bottomrule
\end{tabular}
\vspace{-1ex}
\caption{Accuracy for Error identification of various models under Full Context (baseline) and Premises settings across different datasets. Note that Neg, Syn and Pos means Negative, Synthetic Negatives and Positive splits of our PERL, as explained earlier. For each dataset, we highlighted the models that benefit the most from the PARC.}
\vspace{-2ex}
\label{tab:error_identification_restructured}
\end{table*}

\paragraph{Aggregative Approach:} 
Table \ref{tab:premise_identification} contains the results of the Aggregative approach for a suite of models and datasets. From the table, it is evident that LLMs are efficient in identifying premises for a given step. In particular, Llama 3.1 70b \cite{grattafiori2024llama3herdmodels}, Qwen 2.5 32b \cite{qwen2025qwen25technicalreport} and GPT4o are very efficient in this task, all having a recall greater than 90\%. It also shows that these models can convert an LRC to a PARC with high accuracy. 

We note that the scale of the model is an important factor. The increase in recall varies from 8.2\% to 32.2\% when we scale up for the same model class, and the pattern is consistent. Tables \ref{tab:gsm8k_premise_identification_per_candidate_score}, \ref{tab:math_premise_identification_per_candidate_score}, \ref{tab:orca_math_premise_identification_per_candidate_score} and \ref{tab:meta_math_premise_identification_per_candidate_score} in the appendix contain the detailed metrics across the splits Positive, Negative and Synthetic negatives for GSM8K, MATH, OrcaMath and MetaMathQA respectively.


\paragraph{Dyadic Approach: }
Table \ref{tab:premise_identification_per_candidate_score} provides a comparative analysis of precision, recall, and F1 scores for the GSM8K dataset using two models for the aggregative and dyadic approaches.
This approach causes the LLM to be more biased to mark a step as a premise, causing precision to drop significantly. In addition, this approach has a time complexity of $O(n^2)$, where $n$ is the number of reasoning steps.   Note that for almost all cases the precision drops, and the change in recall gives a mixed view on how this approach might be helpful. 

Our analysis revealed that this methodology often led the model to incorrectly classify second- or higher-order dependencies as premises, even when the current step did not directly rely on them. This misclassification contributed to the observed degradation in precision.


% \begin{table*}[t!]
%     \centering
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{l|ccc|ccc|ccc}
%         \hline
%         \textbf{Model Name} & \multicolumn{3}{c|}{\textbf{GSM Positives}} & \multicolumn{3}{c|}{\textbf{GSM Negatives}} & \multicolumn{3}{c}{\textbf{GSM Positives Perturbed}} \\
%         & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
%         \hline
%         GPT-4o-mini          & 74.8 & 91.5  & 82.31 & 65.9  & 84.5 & 79.35 & 59.7 & 74 & 66.09 \\
%         GPT-4o               & 66.5 & 97  & 78.91 & 60.1  & 92.7 & 72.92 & 56.2  & 80.8  & 66.29 \\
%         \hline
%     \end{tabular}
%     }
%     \caption{Premise Identification Performance Metrics for Different Models on GSM Positives, GSM Negatives, and GSM Positives Perturbed for the Pairwise Approach. The pairwise approach causes overprediction of premises, which further causes a drop in precision. }
%     \label{tab:premise_identification_per_candidate_score}
% \end{table*}






% \begin{table*}[!t]
% \centering
% \setlength\tabcolsep{4pt}
% \fontsize{8pt}{9pt}\selectfont
% \begin{tabular}{lcccccccccccccccc}
% \toprule
% \multirow{2}[2]{*}{\textbf{Model}} & \multicolumn{4}{c}{\textbf{GSM8K}} & \multicolumn{4}{c}{\textbf{MATH}} & \multicolumn{4}{c}{\textbf{Orca Math}} & \multicolumn{4}{c}{\textbf{MetaMathQA}} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13} \cmidrule(lr){14-17}
%  & Neg & Syn & Pos & Avg & Neg & Syn & Pos & Avg & Neg & Syn & Pos & Avg & Neg & Syn & Pos & Avg \\
% \textbf{Llama 3.1 8b} \\
% Full Context & 48.87 & 57.68 & 91.9 & 58.6 & 46.91 & 60.29 & 90.61 & 60.2 & 43.26 & 51.2 & 96.7 & 55.06 & 50.47 & 55.15 & 95.17 & 59.45 \\
% Model Premises & 54.61 & 76.74 & 65.44 & 64.53 & 52.16 & 59.83 & 50.96 & 54.88 & 55.57 & 59.82 & 67.36 & 59.22 & 54.52 & 69.13 & 62.63 & 61.78 \\
% \midrule
% \textbf{Llama 3.1 70b} \\
% Full Context & 58.35 & 77.03 & 98.61 & 71.37 & 55.55 & 72.83 & 96.24 & 69.77 & 50.33 & 64.25 & 96.74 & \cellcolor{lightblue}63.52 & 55.59 & 73.13 & 97.93 & 69.49 \\
% Model Premises & 74.51 & 86.22 & 94.69 & 81.92 & 70.94 & 79.29 & 77.91 & 75.45 & 64.95 & 72.28 & 93.37 & \cellcolor{darkerblue}72.52 & 65.69 & 82.74 & 89.71 & 76.52 \\
% \midrule
% \textbf{Qwen 2.5 7b} \\
% Full Context & 40.88 & 54.13 & 100 & \cellcolor{lightblue}54.94 & 40.47 & 53.60 & 98.07 & 56.26 & 41.20 & 53.69 & 100 & \cellcolor{lightblue}55.85 & 43.36 & 53.18 & 99.39 & \cellcolor{lightblue}56.26 \\
% Model Premises & 53.03 & 78.40 & 90.37 & \cellcolor{darkerblue}69.73 & 58.47 & 66.87 & 82.72 & 65.96 & 52.07 & 71.12 & 84.28 & \cellcolor{darkerblue}65.38 & 54.98 & 73.87 & 91.02 & \cellcolor{darkerblue}68.43 \\
% \midrule
% \textbf{Qwen 2.5 32b} \\
% Full Context & 46.76 & 69.26 & 99.8 & \cellcolor{lightblue}63.56 & 49.58 & 66.5 & 98 & \cellcolor{lightblue}65.11 & 49.44 & 63.01 & 99.5 & 63.06 & 49.7 & 67.8 & 99.6 & \cellcolor{lightblue}65.02 \\
% Model Premises & 65.58 & 86.34 & 96.95 & \cellcolor{darkerblue}79.37 & 68.61 & 76.96 & 88.71 & \cellcolor{darkerblue}75.57 & 61.89 & 69.24 & 95.25 & 70.26 & 62.18 & 85.60 & 97.32 & \cellcolor{darkerblue}77.40 \\
% \midrule
% \textbf{Qwen 2.5 72b} \\
% Full Context & 47.48 & 67.17 & 100 & \cellcolor{lightblue}63.11 & 53.19 & 67.87 & 99.79 & \cellcolor{lightblue}67.52 & 50.41 & 60.33 & 100 & \cellcolor{lightblue}62.50 & 47.31 & 63.59 & 99.05 & \cellcolor{lightblue}62.20 \\
% Model Premises & 69.87 & 85.12 & 96.38 & \cellcolor{darkerblue}80.56 & 69.51 & 79.92 & 90.84 & \cellcolor{darkerblue}77.28 & 69.45 & 71.34 & 94.19 & \cellcolor{darkerblue}74.41 & 64.73 & 85.52 & 98.12 & \cellcolor{darkerblue}78.53 \\
% \midrule
% \textbf{GPT4o-mini} \\
% Full Context & 52.68 & 73.14 & 95.02 & 66.68 & 50.41 & 62.39 & 93.4 & 63.03 & 55.22 & 61.18 & 98.6 & 64.59 & 51.52 & 65.42 & 97.35 & 64.47 \\
% Model Premises & 58.25 & 80.81 & 86.72 & 72.33 & 65.84 & 72.4 & 71.88 & 69.45 & 61.18 & 72.48 & 85.94 & 69.93 & 57.04 & 81.63 & 91.15 & 72.51 \\
% \midrule
% \textbf{GPT-4o} \\
% Full Context & 53.81 & 75.3 & 98.33 & 68.52 & 50.9 & 68.41 & 98.57 & \cellcolor{lightblue}66.52 & 59.55 & 65.75 & 100 & 68.55 & 56.26 & 71.72 & 99.3 & 69.41 \\
% Model Premises & 65.23 & 86.67 & 99.76 & 79.82 & 70.12 & 76.67 & 91.84 & \cellcolor{darkerblue}76.45 & 66.33 & 71.74 & 93.14 & 72.96 & 67.28 & 88.19 & 89.85 & 79.41 \\
% \bottomrule
% \end{tabular}

% \caption{Accuracy for Error identification of various models under Full Context (baseline) and Premises settings across different datasets. Note that Neg, Syn and Pos means Negative, Synthetic Negatives and Positive splits of our PERL, as explained earlier. For each dataset, we highlighted the models that benefit the most from the PARC.}
% \label{tab:error_identification_restructured}
% \end{table*}


% \begin{table}[!t]
% \centering
% \setlength\tabcolsep{4pt}
% \fontsize{8pt}{9pt}\selectfont
% \begin{tabular}{lcccc}
% \toprule
% \textbf{Model} & \textbf{GSM8K} & \textbf{MATH} & \textbf{Orca-Math} & \textbf{MetaMath} \\
% \midrule
% \textbf{Llama 3.1 70b} \\
% Oracle Premises & 81.46 & 74.68 & 73.45 & 76.05 \\
% Model Premises & 81.92 & 75.45 & 72.52 & 76.52 \\
% \midrule
% \textbf{Qwen 2.5 72b} \\
% Oracle Premises & 80.58 & 79.50 & 74.87 & 79.49 \\
% Model Premises & 80.56 & 77.28 & 74.41 & 78.53 \\
% \midrule
% \textbf{GPT-4o} \\
% Oracle Premises & 78.74 & 76.79 & 75.56 & 82.88 \\
% Model Premises & 79.82 & 76.45 & 72.96 & 79.41 \\
% \bottomrule
% \end{tabular}

% \caption{Comparison of Error identification under oracle premises vs model generated premises. Since these models have high recall in premise identification, we observe that the error identification accuracy is comparable.}
% \label{tab:avg_performance_selected}
% \end{table}
\vspace{-2ex}
\subsection{Error identification}
In this section, we address RQ2 and RQ3, i.e. is error identification more robust when we only use the premises for a given step as context. We explain the experimental setup for error identification for the baseline, as well as our approach, in the previous sections. For the identification of errors under premises, we consider two settings. For the first one, we use the oracle premise annotation (from the ground truth). For the second, we first identify the premises with the LLMs and then use those premises for the identification of errors. Note that in the previous section we established that LLMs can identify premises with a high recall, and in this experiment, we exploit that. The prompts we used for our experiments are shared in Appendix \ref{sec:baseline_error_prompts}. 

\paragraph{Metrics:} For this task, we report the average accuracy for the identification of error types for each step. In our analysis, we observe that the boundaries between the error types \textit{Mathematical Error} and \textit{Logical Inconsistency} are quite thin, and models often classify one as the other, so we merge these two error categories into a single error type \textit{Error}, while keeping \textit{Accumulation Error} separate. Furthermore, since an early erroneous step for a solution containing multiple steps could cause the number of errors to skew toward \textit{accumulation errors}, we first normalize the accuracy by the number of steps in a datapoint, and later normalize across the dataset. 

\paragraph{Results:}
\begin{table}[!t]
\centering
\setlength\tabcolsep{4pt}
\fontsize{8pt}{9pt}\selectfont
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{GSM8K} & \textbf{MATH} & \textbf{Orca-Math} & \textbf{MetaMath} \\
\midrule
\textbf{Llama 3.1 70b} \\
Oracle Premises & 81.46 & 74.68 & 73.45 & 76.05 \\
Model Premises & 81.92 & 75.45 & 72.52 & 76.52 \\
\midrule
\textbf{Qwen 2.5 72b} \\
Oracle Premises & 80.58 & 79.50 & 74.87 & 79.49 \\
Model Premises & 80.56 & 77.28 & 74.41 & 78.53 \\
\midrule
\textbf{GPT-4o} \\
Oracle Premises & 78.74 & 76.79 & 75.56 & 82.88 \\
Model Premises & 79.82 & 76.45 & 72.96 & 79.41 \\
\bottomrule
\end{tabular}
\vspace{-1ex}
\caption{Comparison of Error identification under oracle premises vs model generated premises. Since these models have high recall in premise identification, we observe that the error identification accuracy is comparable.}
\vspace{-2ex}
\label{tab:premise_ablation}
\end{table}
Table \ref{tab:error_identification_restructured} presents the experimental results for the error identification task. We evaluate performance under two setups: under complete context as input (\textit{Full Context}), on the other hand, the \textit{Model Premises} setup denotes the end-to-end approach, wherein the model is tasked with first identifying the relevant premises and subsequently utilizing them. All models are provided with the question and the solutions generated up to the current step and are tasked with predicting the error type observed in the current step.

\textbf{Premises help Error Identification :} It is evident from Table \ref{tab:error_identification_restructured}, that providing only the premise steps as context (instead of providing the full context) improves the accuracy of error detection with LLMs. Figure \ref{fig:example} shows one such example, where the baseline approach (using an LRC, and error detection performed in the full context) could not identify errors, but our approach was able to successfully identify the mathematical and accumulation errors. Our results show that the models are not as biased towards marking a step as correct, when given a more precise context (no distractor). Hence, upon providing only the premises, identification of errors becomes much more robust. However, it is important to note that this also comes at the cost of a drop of accuracy for correct steps, this can be observed from comparing the Pos column for Full context and Model Premises. However, this accuracy drop is much smaller for larger models.  In addition, our approach impacts larger and more capable models more than smaller models. 

\textbf{Models struggle with Error Detection, especially Accumulation errors:} We find that, while capable of identifying correct steps, all models struggle with identifying errors. This can be attributed to the fact that models are inherently biased toward marking a step as correct. This issue was also pointed out by \citet{ling2023deductiveverificationchainofthoughtreasoning}. Identifying errors with directly connected premises as context yields significantly better performance than simply providing the full context. Moreover, models struggle to identify accumulation errors. Table \ref{tab:expanded_view} contains the accuracies across error types in the negative split of GSM8K. Note that, in full context, accuracy is quite high for correct steps, but lower for erroneous steps (mathematical and logical errors), and significantly lower for accumulation errors. However, in our approach, the identification of errors becomes significantly robust, with a minor drop in performance in identifying correct steps. 

\textbf{Synthetic negatives are easier:} The average performance of all models on synthetically generated negatives is consistently higher compared to the performance on true negatives (evident from comparing the Neg and Syn columns in Table \ref{tab:error_identification_restructured}). This observation highlights an important insight: perturbing reasoning chains does not serve as a reliable proxy for evaluating a model's ability to identify genuine errors in reasoning. While prior work has focused on such synthetic negatives, it might provide an overoptimistic view of LLMs' ability to detect errors. 




\begin{table}[!t]
\centering
\setlength\tabcolsep{4pt}
\fontsize{8pt}{9pt}\selectfont
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Correct} & \textbf{Error} & \textbf{Acc. Error} & \textbf{Avg} \\
\midrule
\textbf{Llama 3.1 70b} \\
Full Context & 96.79 & 60.87 & 12 & 58.35 \\
Oracle Premises & 90.18 & 75.25 & 55.63 & 74.41 \\
Model Premises & 88.78 & 75.25 & 57.54 & 74.51 \\
\midrule
\textbf{GPT-4o} \\
Full Context & 95.75 & 48.2 & 13.2 & 53.81 \\
Oracle Premises & 96.83 & 60.02 & 41.79 & 67.03 \\
Model Premises & 94.15 & 55.13 & 44.91 & 65.23 \\
\bottomrule
\end{tabular}
\vspace{-1ex}
\caption{Error identification accuracy for each type of steps in ground truth. Acc. Error means Accumulation Errors}
\label{tab:expanded_view}
\vspace{-2ex}
\end{table}

\textbf{Oracle Premises vs Model generated Premises:} In Table \ref{tab:premise_ablation}, we present results of an ablation, where instead of providing the model generated premises, we provide the ground truth premises (oracle premises from PERL). As anticipated, the accuracy of error identification improves when oracle premises are provided. However, it is noteworthy that for most models, the performance remains comparable to that achieved with oracle premises. This can be attributed to the fact that all these models can identify premises with a recall of higher than 90.
\label{sec:related_work}
% \heng{add some related work about information aggregation, include your SmartBook paper}\zhenhailong{added}

%\zhenhailong{add our distinctive contribution here compared with AssistantBench/MindSearch}

% \begin{comment}
%     \begin{itemize}
%         \item WebLinx \citepp{lu2024weblinx}
%         \item WebArena \citepp{zhou2023webarena}
%         \item WebGPT \citepp{nakano2021webgpt}
%         \item Mind2Web \citepp{deng2024mind2web}
%         \item AutoGPT \citepp{}
%     \end{itemize}
% \end{comment}
    