% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%     \hline
%     \textbf{Error Type}           & \textbf{Description} \\
%     \hline
%     Logical Inconsistency       & Violates logic or common sense. \\
%     Mathematical Error     & Incorrect calculations or quantitative reasoning in \(S_t\). \\
%     Accumulation Error       &  Errors in \(S_t\) due to incorrect premises. \\
%     \hline
%   \end{tabular}
%   \caption{\label{Step-Level error}
%     Description of step level errors 
%     % \textcolor{red} {You can add examples to this table and maybe their corrections.}
%   }
% \end{table*}


% \begin{table*}
%   \centering
%   \begin{tabular}{p{0.25\textwidth} p{0.65\textwidth}}
%     \hline
%     \textbf{Error Type}           & \textbf{Description} \\
%     \hline
%     Planning Error       & Reasoning steps lacks logical coherence or fails to form a valid strategy to solve the given problem. \\
%     Missing Step Error     & Reasoning steps which are necessary for reaching the correct solution are omitted. \\
%     \hline

%   \end{tabular}
%   \caption{\label{ReasoningError}
%     Description of reasoning chain level errors
%   }
% \end{table*}

\section{Method}
In this work, we explore how the establishment of premise links improves the identification of errors in reasoning chains with LLMs. Our approach works in two steps. First, we identify premises for each step and augment a linear reasoning chain (LRC) to convert it into a Premise Augmented Reasoning Chain (PARC). And then verify each step in the PARC under its premises only. Lastly, we do a graph traversal to identify steps that are logically correct but have faulty premises to identify accumulation errors. In particular, we are interested in the following research questions.

\textbf{RQ 1} Given a sequential step-by-step answer to a math word problem, can LLMs identify premises for each step?

\textbf{RQ 2} Given premise annotations, can LLMs identify errors in reasoning more faithfully?

\textbf{RQ 3} Can LLMs perform the entire process \textit{end-to-end}, i.e., given a reasoning chain, can they identify premises for each step and detect errors?
\subsection{Premise Augmented Reasoning Chains (PARC)}
Let $\mathbf{q}$ represent the question, $\hat{a}$ the predicted answer, $a$ the ground truth answer, and $\mathbf{r_{\leq t}} = [s_1, s_2, \dots, s_t]$ the generated reasoning chain composed of $t$ intermediate steps $s_i$, leading to $\hat{a}$. Using CoT reasoning, the predicted answer $\hat{a}$ is derived by first generating a reasoning chain $\mathbf{r_{\leq t}}$. The probability distribution for $\hat{a}$ can be expressed as:
\[
\mathbb{P}_{\mathrm{LM}}(\hat{a} \mid \mathbf{q}) = \mathbb{P}_{\mathrm{LM}}(\hat{a} \mid \mathbf{r_{\leq t}}) \times \mathbb{P}_{\mathrm{LM}}(\mathbf{r_{\leq t}} \mid \mathbf{q}),
\]
where the reasoning chain $\mathbf{R}$ can be decomposed into intermediate steps as:
\[
\mathbb{P}_{\text{LM}} (\mathbf{r_{\leq t}} \mid \mathbf{q}) = \mathbb{P}_{\text{LM}} (s_1 \mid \mathbf{q}) \times \prod_{i=1}^{t-1} \mathbb{P}_{\text{LM}} (s_{i+1} \mid \mathbf{q}, s_i).
\]
Our objective is to identify errors in the intermediate steps $s_i$ of the reasoning chain $\mathbf{r_{\leq t}}$.


The \textit{premises} for a step $\mathbf{s}_i$ are defined as the necessary and sufficient subset of prior steps, denoted as
\[
\mathcal{P}_i \subseteq \{s_j \forall j < i\},
\]
satisfying the following properties:

1. \textbf{Verifiability}: The correctness of $s_i$ is verifiable based on $\mathcal{P}_i$ alone:
\[
    \mathcal{F}(s_i \mid \mathcal{P}_i) = 1.
\]
2. \textbf{Minimality}: The set $\mathcal{P}_i$ is minimal such that removing any element $\mathbf{s}_j \in \mathcal{P}_i$ results in $s_i$ becoming unverifiable:
\begin{align}
    \mathcal{F}(s_i \mid \mathcal{P}_i \setminus \{s_j\}) &= 0, 
    \quad \forall s_j \in \mathcal{P}_i.
\end{align}
Given premises $\mathcal{P}_i$ for a step $\mathbf{s}_i$, we convert a linear reasoning chain into a PARC, where each step $\mathbf{s}_i$  is augmented with its premises $\mathcal{P}_i$. $\mathcal{F}$ is a function that estimates whether a step is verifiable or not.
\[
\mathbf{r_{\leq t}}' = [(s_1, \mathcal{P}_1), (s_2, \mathcal{P}_2), \dots, (s_t, \mathcal{P}_t)],
\]
where $\mathbf{r_{\leq t}}'$ represents the transformed reasoning chain. 
\subsection{Progressive Premise Mapping}
To transform a reasoning chain \(\mathbf{r_{\leq t}} = [s_1, s_2, \dots, s_t]\) into a premise-augmented reasoning chain (PARC), each step \(s_i\) must be explicitly linked to its premises \(\mathcal{P}_i\). We explore two approaches to identify these premises: \textit{Aggregative Premise Mapping} and \textit{Dyadic Premise Mapping}. 
\subsubsection{Aggregative Premise Mapping}
In the \textit{Aggregative} approach, the premises for each step \(\mathbf{s}_k\) are collectively identified by querying an LLM with the complete reasoning context up to the step. For a given question \(\mathbf{q}\), the reasoning chain so far $\mathbf{r_{< k}}$, and the next step \(s_k\), the LLM is prompted to output \(\mathcal{P}_k\).

\subsubsection{Dyadic Premise Mapping}

In the \textit{Dyadic} approach, the task of identifying premises is reformulated as a pairwise evaluation. Instead of querying the LLM to identify all premises \(\mathcal{P}_k\) for \(s_k\) at once, we evaluate whether each individual step \(s_i\) (where \(i < k\)) serves as a valid premise for \(s_k\).  For each pair \((s_i, s_k)\), the LLM is queried to compute:
\[
\mathcal{I}(s_k \mid s_i) = 
\begin{cases} 
1, & \text{if } s_i \text{ is a valid premise for } s_k, \\ 
0, & \text{otherwise}.
\end{cases}
\]
The premises for \(s_k\) are then given by:
\[
\mathcal{P}_k = \{s_i \mid \mathcal{F}(s_k \mid s_i) = 1, \, \forall i < k\}.
\]
\subsection{Error Identification}
Next, we present our setup for error identification and how premises play a significant role in this task. We introduce a taxonomy of error types that occur in math reasoning with LLMs and then describe our approach to identify them.
\subsubsection{Types of Error}
Traditionally, prior research has predominantly focused on identifying \textit{native errors}, which refer to inaccuracies inherent to individual reasoning steps. These errors often arise from issues such as incorrect mathematical calculations (defined as \textit{Mathematical Error}), logical irregularities (defined as \textit{logical inconsistencies}) and are evaluated independently of the broader reasoning context. Although significant, this focus on native errors tends to overlook another critical category of errors, called \textit{accumulation errors}. 

An \textit{accumulation error} occurs when a reasoning step is valid in isolation but is built upon one or more erroneous premises from earlier steps. Unlike native errors, accumulation errors emerge from the compounding effects of prior inaccuracies, propagating through sequential steps. Formally, in the context of the reasoning chain, a step \(s_i\) is classified as:

1. \textit{Correct}, if it contains no logical or mathematical errors, and is performed under premises that are also \textit{correct}

2. A \textit{native error} if it contains an inherent discrepancy (e.g., a miscalculation or logical inconsistency)

3. An \textit{accumulation error} if \(s_i\) is logically valid, but at least one of its premises is incorrect.

By addressing both native and accumulation errors, we can achieve a more comprehensive understanding of error dynamics in sequential reasoning processes.

\begin{algorithm}[tb]
    \small
   \caption{Constructing and Evaluating PARC}
   \label{alg:pipeline}
\begin{algorithmic}
   \STATE {\bfseries Input:} $R = [s_1, s_2, \dots, s_t]$

   {\small \texttt{// Step 1: Premise Extraction}}
   \FOR{$k=1$ {\bfseries to} $t$}
        \STATE $\mathcal{P}_k \gets \textsc{ExtractPremise}(s_k, \{s_1,\dots,s_{k-1}\})$
   \ENDFOR
   \STATE $R' \gets [(s_1, \mathcal{P}_1), \dots, (s_t, \mathcal{P}_t)]$

   {\small \texttt{// Step 2: Error Detection}}

   \FOR{$k=1$ {\bfseries to} $t$}
        \STATE $\mathcal{E}_k^{(1)} \gets \textsc{IsMathematicalError}(s_k)$
        \STATE $\mathcal{E}_k^{(2)} \gets \textsc{IsLogicallyInconsistent}(s_k, \mathcal{P}_k)$
        \STATE $\mathcal{E}_k \gets \mathcal{E}_k^{(1)} \lor \mathcal{E}_k^{(2)}$

   \ENDFOR

   {\small \texttt{// Step 3: Accumulation Error Detection}}
    \FOR{$k=1$ {\bfseries to} $t$}
        \IF{$s_k$ is correct}
            \FOR{$s_j \in \mathcal{P}_k$}
                \IF{$s_j$ is incorrect}
                    \STATE $\mathcal{E}_k \gets \textsc{AccumulationError}$
                    \STATE \textbf{break}
                \ENDIF
            \ENDFOR
        \ENDIF
    \ENDFOR

    \STATE $R' \gets [(s_1, \mathcal{P}_1, \mathcal{E}_1), \dots, (s_t, \mathcal{P}_t, \mathcal{E}_t)]$
    
\end{algorithmic}
\end{algorithm}

\subsubsection{Error Identification}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{sections/example.pdf}
    \vspace{-2.5em}
    \caption{An example where the baseline method fails to detect errors, while our verification method with established premise links successfully identifies the mathematical error in step 6, and the accumulation error in step 7.}
    % \vspace{-1.5em}
    \label{fig:example}
\end{figure*}
\paragraph{Baseline.} In the baseline approach, a large language model (LLM) is used in a zero-shot setting to classify the reasoning steps. For a given question \(\mathbf{q}\), the reasoning chain so far \(r_{<k}\), and the next step proposed \(s_k\), the model is queried to assign an error type \(\mathcal{E}_k\) for \(s_k\) according to the predefined error taxonomy (\textit{Correct} / \textit{Mathematical Error} / \textit{Logical Error} / \textit{Accumulation Error}):
% \[
% \mathcal{E}_k \in \{\texttt{Mathematical Error}, \texttt{Logical Inconsistency}, \texttt{Accumulation Error}, \texttt{None}\}.
% \]
The model processes the entire context \((\mathbf{q}, r_{< k}, s_k)\) and generates a classification directly. The zero-shot prompt explicitly defines all error types to guide the classification.

\paragraph{Proposed Approach.} Our proposed approach refines the classification process by situating each reasoning step \(\mathbf{s}_k\) explicitly within the context of its premises \(\mathcal{P}_k\). Algorithm \ref{alg:pipeline} illustrates the overall pipeline of conversion of LRC to PARC and step-by-step error classification. The prompts for the following are reported in Appendix \ref{tab:ours_math_error} and \ref{tab:ours_logical_error}.
\subparagraph{1. Mathematical Error:} To detect mathematical errors, we prompt an LLM to assess whether \(s_k\) contains a mathematical error by analyzing the step in isolation, excluding the broader reasoning chain. 
\subparagraph{2. Logical Inconsistency:} For the detection of logical inconsistencies, we restrict the evaluation to the premises \(\mathcal{P}_k\) of the reasoning step \(s_k\). The LLM is prompted to determine whether \(s_k\) is logically consistent with its premises. This ensures that the step's validity is evaluated in the context of the dependencies defined by \(\mathcal{P}_k\), without considering unrelated parts of the reasoning chain.
\subparagraph{3. Accumulation Error:} After identifying native errors, accumulation errors are identified by analyzing the dependency graph of the reasoning chain. The premises \(\mathcal{P}_k\) form directed edges in the graph, where each step depends on its premises. A Depth-First Search (DFS) traversal is performed, and a step \(s_k\) is classified as an Accumulation Error if it satisfies the following criteria: 1. \(s_k\) itself is classified as correct, and 2. At least one premise \(s_j \in \mathcal{P}_k\) (where \(j < k\)) is classified as incorrect.

% \subsection{Reasoning-chain level errors}
% For reasoning chain level errors we consider the following two error types: Planning error and Missing Step Error. Each of these errors are detailed in Table~\ref{ReasoningError}.

\begin{table*}[!t]
    \centering
    \setlength\tabcolsep{4pt}
    \fontsize{8pt}{9pt}\selectfont
    \begin{tabular}{lccc|ccc|ccc|ccc}
    \toprule
    \multirow{2}[2]{*}{\textbf{Model Name}} & \multicolumn{3}{c|}{\textbf{GSM8K}} & \multicolumn{3}{c|}{\textbf{MATH}} & \multicolumn{3}{c|}{\textbf{Orca Math}} & \multicolumn{3}{c}{\textbf{MetaMathQA}} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
     & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 \\
    \midrule
    Llama 3.1 8b & 65.64 & 89.33 & 75.66 & 51.00 & 82.40 & 62.94 & 54.65 & 79.77 & 64.85 & 53.92 & 81.42 & 64.75 \\
    Llama 3.1 70b & 81.26 & 97.55 & 88.65 & 69.80 & 96.82 & 81.11 & 71.05 & 96.73 & 81.91 & 73.10 & 96.46 & 83.05 \\
    \midrule
    Qwen 2.5 7b & 67.42 & 63.53 & 65.36 & 55.87 & 59.73 & 57.70 & 63.08 & 72.64 & 67.05 & 55.54 & 61.66 & 58.14 \\
    Qwen 2.5 32b & 84.07 & 95.35 & 89.34 & 72.56 & 88.55 & 79.74 & 74.27 & 90.63 & 81.62 & 75.57 & 90.89 & 82.49 \\
    \midrule
    GPT4o-mini & 72.54 & 86.18 & 78.75 & 57.72 & 69.29 & 62.96 & 60.12 & 73.96 & 66.31 & 60.77 & 76.70 & 67.71 \\
    GPT-4o & 85.93 & 94.42 & 89.96 & 69.40 & 89.78 & 78.28 & 71.12 & 92.17 & 80.79 & 75.49 & 90.37 & 81.85 \\
    \bottomrule
    \end{tabular}
    
    \caption{Precision, Recall and F1 scores for premise identification under the Aggregative approach. Note that ideally this needs to be a high recall system, because missing even a single premise step could hurt the verifiability of the reasoning chain.}
    \label{tab:premise_identification}
\end{table*}
\begin{table*}[!t]
    \centering
    \setlength\tabcolsep{4pt}
    \fontsize{8pt}{9pt}\selectfont
    \begin{tabular}{cccc|ccc|ccc}
    \toprule
    \multirow{2}[2]{*}{\textbf{Model Name}} & \multicolumn{3}{c|}{\textbf{Positives}} & \multicolumn{3}{c|}{\textbf{Negatives}} & \multicolumn{3}{c}{\textbf{Synthetic Negatives }} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
     & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 \\
    \midrule
    \textbf{GPT4o-mini} \\
    Aggregative & 74.93 & 84.81 & 79.56 & 70.85 & 87.01 & 78.10 & 71.84 & 86.73 & 78.59 \\
    Dyadic & 75.04 & 82.82 & 78.74 & 64.57 & 71.12 & 67.68 & 55.18 & 53.98 & 54.57 \\
    \midrule
    \textbf{GPT4o} \\
    Aggregative & 89.38 & 93.66 & 91.47 & 84.97 & 94.95 & 89.69 & 83.43 & 94.65 & 88.72 \\
    Dyadic & 75.32 & 99.01 & 85.55 & 67.70 & 92.68 & 78.25 & 67.27 & 79.42 & 72.84 \\
    \bottomrule
    \end{tabular}
    \vspace{-1ex}
    \caption{Precision, Recall and F1 scores for the premise mapping task for the GSM8K dataset under the aggregative and dyadic approaches. We saw a consistent drop in F1 score for the dyadic approach, in spite of it being computationally more expensive.}
    \label{tab:premise_identification_per_candidate_score}
\end{table*}

\section{PERL: Premises and ERrors identification in Language models}
In order to test the ability of LLMs to identify premises and error categories, we designed a testbed. We used existing datasets of math word problems to create PERL, our testbed for step-level premise and error annotations. 
\paragraph{Generating Reasoning Chains:} 
In order to generate the reasoning chains, we used two popular benchmarks (i) GSM8K \cite{cobbe2021trainingverifierssolvemath}, a collection of 8,500 grade school math word problems, and (ii) MATH \cite{hendrycksmath2021}, a dataset of 12,500 challenging competition level math word problems. In addition, we use the (iii) Orca-Math dataset by \citet{mitra2024orcamathunlockingpotentialslms}, a synthetic dataset of 200K math problems alongside solutions written by GPT4Turbo, and the (iv) MetaMathQA dataset by \citet{yu2023metamath}. We first randomly sampled 1000 examples from the GSM8k and MATH test split and the Orca-Math and MetaMathQA training split (since these are training datasets). We began by using the open weight Large Language Model Llama-3.1-8B-Instruct \cite{grattafiori2024llama3herdmodels} to generate step-by-step reasoning chains. Next, we categorized the reasoning chains as positive or negative, depending on whether the final answer matches the ground-truth answer. Next, we randomly sampled 50 positive (correct) and 50 negative (incorrect) reasoning chains. To expand our dataset, we employed GPT-4o to systematically introduce mathematical or logical errors into the correct reasoning chains, creating additional synthetic negative examples (since prior work also focused on such synthetic negatives). When introducing errors, we ensured that the subsequent steps were appropriately modified to reflect the impact of the initial error. In contrast with existing works such as \citet{golovneva2023roscoesuitemetricsscoring}, where the perturbation in a step is not followed up in subsequent steps, our synthetic negatives are more realistic.


\paragraph{Premise and Error annotation:} Next, for each step in the reasoning chain, we identified the ground truth premises for each step using the OpenAI o1-Preview model \cite{openai2024openaio1card}. Then, we use the same o1-Preview model to map each step to the predefined taxonomy of errors or mark it as correct, to create ground-truth error annotations. Upon obtaining the annotated data sets, 2 authors manually went through 10\% of the generated data points to identify the discrepancy between the human annotations and the annotations done by the o1 model. Manual inspection of 71 samples showed that, in the case of premise annotations, only 4 out of 71 had a discrepancy and 5 out of 71 had issues in error annotations. For premise identification, the discrepancy typically indicates additional steps as premises. However, for error annotation, the annotation errors do not have a clear pattern and manifest themselves in forms of misclassification of the correctness of the step. The prompts used to generate the annotations of the premises and errors are in Appendix \ref{sec:appendix_prompt}.
% This results in a total of 607 reasoning chains with 203 positives, 214 negatives, and 190 synthetic negatives. In total, we have 2,134 steps annotated as \textit{Correct}, 321 steps as \textit{Mathematical Error}, 443 steps as \textit{Logical Inconsistency}, and 741 steps as \textit{Accumulation Error}, indicating that native and accumulation errors appear at almost equal rates.
A detailed summary of the dataset statistics can be found in Appendix \ref{appx:data-stats}
%  Additionally, we model each chain-of-thought as a directed acyclic graph (PARC) based on the step-level premise annotations. Across 1,370 PARCs, each chain contains on average 7.30 steps, representing the length of the chain-of-thought; 11.27 premises, corresponding to the total premise references across steps; and 10.42 edges linking premises to conclusions. The average depth of the PARC is 6.02, measuring the longest path of dependencies, while the maximum width is 1.90, quantifying how many steps can appear at the same layer in the PARC. Finally, the branching factor is 1.37, which is the ratio of edges to nodes, indicating that each step typically spawns fewer than two subsequent steps on average.
% \paragraph{Data statistics:}  
% This results in a total of 607 reasoning chains with 203 positives, 214 negatives, and 190 synthetic negatives. In total, we have 2,134 steps annotated as \textit{Correct}, 321 steps as \textit{Mathematical Error}, 443 steps as \textit{Logical Inconsistency}, and 741 steps as \textit{Accumulation Error}, indicating that native and accumulation errors appear at almost equal rates. Additionally, we model each chain-of-thought as a directed acyclic graph (PARC) based on the step-level premise annotations. Across 1,370 PARCs, each chain contains on average 7.30 steps, representing the length of the chain-of-thought; 11.27 premises, corresponding to the total premise references across steps; and 10.42 edges linking premises to conclusions. The average depth of the PARC is 6.02, measuring the longest path of dependencies, while the maximum width is 1.90, quantifying how many steps can appear at the same layer in the PARC. Finally, the branching factor is 1.37, which is the ratio of edges to nodes, indicating that each step typically spawns fewer than two subsequent steps on average.

% \begin{table*}[!t]
%     \centering
%     \setlength\tabcolsep{4pt}
%     \fontsize{8pt}{9pt}\selectfont
%     \begin{tabular}{lccc|ccc|ccc|ccc}
%     \toprule
%     \multirow{2}[2]{*}{\textbf{Model Name}} & \multicolumn{3}{c|}{\textbf{GSM8K}} & \multicolumn{3}{c|}{\textbf{MATH}} & \multicolumn{3}{c|}{\textbf{Orca Math}} & \multicolumn{3}{c}{\textbf{MetaMathQA}} \\
%     \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
%      & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 \\
%     \midrule
%     Llama 3.1 8b & 65.64 & 89.33 & 75.66 & 51.00 & 82.40 & 62.94 & 54.65 & 79.77 & 64.85 & 53.92 & 81.42 & 64.75 \\
%     Llama 3.1 70b & 81.26 & 97.55 & 88.65 & 69.80 & 96.82 & 81.11 & 71.05 & 96.73 & 81.91 & 73.10 & 96.46 & 83.05 \\
%     \midrule
%     Qwen 2.5 7b & 67.42 & 63.53 & 65.36 & 55.87 & 59.73 & 57.70 & 63.08 & 72.64 & 67.05 & 55.54 & 61.66 & 58.14 \\
%     Qwen 2.5 32b & 84.07 & 95.35 & 89.34 & 72.56 & 88.55 & 79.74 & 74.27 & 90.63 & 81.62 & 75.57 & 90.89 & 82.49 \\
%     \midrule
%     GPT4o-mini & 72.54 & 86.18 & 78.75 & 57.72 & 69.29 & 62.96 & 60.12 & 73.96 & 66.31 & 60.77 & 76.70 & 67.71 \\
%     GPT-4o & 85.93 & 94.42 & 89.96 & 69.40 & 89.78 & 78.28 & 71.12 & 92.17 & 80.79 & 75.49 & 90.37 & 81.85 \\
%     \bottomrule
%     \end{tabular}
    
%     \caption{Precision, Recall and F1 scores for premise identification under the Aggregative approach. Note that ideally this needs to be a high recall system, cause missing even a single premise step could hurt the verifiability of the reasoning chain.}
%     \label{tab:premise_identification}
% \end{table*}