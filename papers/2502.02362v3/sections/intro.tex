% \section{Introduction}
% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=0.9\textwidth]{sections/overview.pdf}
%     \caption{Comparison between a general-purpose Chain of Thought (CoT) and a fully verifiable CoT. In the general-purpose CoT (left), errors can propagate through the reasoning steps without verification. In the fully verifiable CoT (right), premise links are explicitly established, enabling identification of correct and incorrect steps. Accumulation errors can be traced back to faulty premises, improving the interpretability and reliability of the reasoning process.}
%     \label{fig:overview}
% \end{figure*}
% There has been a well-documented success of the reasoning capabilities of large language models (LLMs) with Chain-of-Thought (CoT; \citet{wei2023chainofthoughtpromptingelicitsreasoning}). Across a plethora of applications such as embodied reasoning \cite{NEURIPS2023_3d0758f0, philipov2024simulatinguseragentsembodied}, code generation \cite{jiang2024surveylargelanguagemodels}, mathematical and scientific reasoning \cite{imani-etal-2023-mathprompter, ahn2024largelanguagemodelsmathematical}, CoT-based LLMs have been shown to produce exceptional performance. 
% However, the corresponding evaluation methods and metrics have a significant limitation: they focus solely on the correctness of the \textit{final} answer, neglecting intermediate reasoning processes and rationales that contribute to it. Although the final answer serves as a proxy for the reasoning capability, it is not sufficient to judge the model's reasoning performance. Hence, evaluating the final answer provides a narrower view of the reasoning capabilities.
% % since LLMs can produce correct answers while the rationales could be flawed.
% % The example in Figure \ref{simplify_expression} illustrates a representative situation where the generated reasoning chain incurs multiple errors while getting the final answer right.
% Since the intermediate reasoning process is equally important as getting the correct final answer \cite{huang2023reasoninglargelanguagemodels, golovneva2023roscoesuitemetricsscoring, prasad2023recevalevaluatingreasoningchains}, a comprehensive evaluation of the reasoning chains is crucial to holistically understanding the reasoning capabilities of LLMs. 


% Evaluation of a reasoning chain has been studied in literature in the context of self-verification \cite{weng-etal-2023-large}. Previous work has shown that LLMs struggle to find reasoning errors in chain-of-thought traces without the help of external verifiers \cite{stechly2024selfverificationlimitationslargelanguage, wu-etal-2024-large, tyen-etal-2024-llms}, casting doubt on the overoptimism of LLMs' self-critique abilities. 
% Existing research focusing on the evaluation of reasoning chains in LLMs can be broadly categorized into \textit{reference-based} and \textit{reference-free} methods.
% Reference-based methods, which rely on the availability of a ground truth reasoning chain \cite{welleck2021naturalproofs, han2024folionaturallanguagereasoning, tian-etal-2021-diagnosing}, are reliable but are constrained by the significant cost of human annotations, restricting their application. 
% In contrast, reference-free methods \cite{prasad2023recevalevaluatingreasoningchains, golovneva2023roscoesuitemetricsscoring, zhu2024deductivebeamsearchdecoding} bypass the need for annotations, but suffer from two major drawbacks. Firstly, most of such existing works assign a chain-level score and not target error localization. Further, they often need fine-tuning of task-specific models, restricting their generalizability. 
% One workaround could be to do reasoning with formal proof assistants like Lean \cite{10.1007/978-3-030-79876-5_37}, which natively support the verifiability of generated proofs \cite{yang2024formalmathematicalreasoningnew, yang2023leandojotheoremprovingretrievalaugmented, murphy2024autoformalizingeuclideangeometry}. However, this requires the challenging task of auto-formalizing natural language text to such proof assistants, adding an additional layer of difficulty \cite{wu2022autoformalizationlargelanguagemodels}. Another challenge for such an approach is that even for math word problems, such proof assistants need the answer to be known beforehand, thereby restructuring the problem to a proof. 

% % The challenges in evaluating LLM reasoning chains have significant implications for transparency and reliability of these models. Without systematic evaluation of intermediate reasoning steps, there is an increased risk of hallucinations or flawed deductions, which can lead to correct answers reached through faulty logic. Additionally, while reference-based methods are reliable, the high cost of human annotations restricts their use to simpler tasks, limiting their scalability. Reference-free methods, though promising, still struggle with the complex logical and scientific reasoning and fail to provide step by step evaluation. 

% In our work, we focus on reference-free verification of LLM reasoning chains in the context of mathematical reasoning. Mathematical problem solving requires a series of deductive reasoning steps, where each step is performed under some premise steps. We hypothesize that a step in a reasoning chain should be verified only under its premises. Previous work also shown that having unnecessary context hurts performance of LLMs \cite{10.5555/3618408.3619699}. Thus, by excluding unnecessary context from the verifier, we make the verification of a reasoning step only conditioned on its premises, addressing the known susceptibility of LLMs to errors under irrelevant contexts.
% We propose a framework, namely \textbf{PARC (Premise-Augmented Reasoning Chains)}, to induce a structure on a typical linear reasoning chain (LRC) to convert it into a premise-augmented format by identifying the premises of each step. Identifying those premises improves the tracability of a reasoning chain and the logical flow. By constructing corresponding dataset, called \textbf{PERL (Premises and ERrors identification in Language models)}, we verify the effectiveness of premise augmentation in annotating both premises  and errors.


% Additionally, we refine the error taxonomy for mathematical reasoning by introducing a new category: ``accumulation error" (see Fig. \ref{fig:overview} for example). This type of error occurs when a reasoning step is correct in isolation but is derived from flawed premise steps, resulting in error propagation throughout the reasoning chain. Although this type of errors are quite common in practice, existing work has not addressed this error type to the best of our knowledge. Previous work focuses on the first mistake in a reasoning chain \cite{lightman2023letsverifystepstep, zheng2024processbenchidentifyingprocesserrors, daheim2024stepwiseverificationremediationstudent}, discarding the rest of the steps, due to the ambiguity of their correctness. Identifying accumulation errors is critical for improving the reliability and interpretability of reasoning traces in LLMs. By distinguishing accumulation errors from reasoning errors, we enable a more precise understanding of error types and facilitate targeted interventions. In addition, establishing explicit premise links between reasoning steps improves both the efficiency of verification and the transparency of the reasoning process. This, in turn, strengthens the robustness and reliability of chain-of-thought reasoning, making it a more trustworthy tool for complex tasks. 

% % ds\textcolor{red}{We can mention partial credit and tie it to error classification, accumulation errors...etc}

% To this end, our contributions are as follows: 
% \begin{itemize}
%     \item We demonstrate that off-the-shelf LLMs can detect premises for a given step with high accuracy for mathematical reasoning to successfully convert a Linear Reasoning Chain (LRC) to a Premise Augmented Reasoning Chain (PARC).
%     \item We establish that the use of these premises improves the accuracy in detecting errors and their types in reasoning chains.
%     \item We propose an error taxonomy for mathematical reasoning in LLMs, notably, we introduce the use of \textit{Accumulation Errors} to classify erroneous steps beyond the first wrong step in the chain.
%     \item We create PERL, a dataset of reasoning chains, annotated with premises and error types that we will make publicly available to foster future research. 
% \end{itemize}

\section{Introduction}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{sections/overview.pdf}
    \vspace{-3ex}
    \caption{Comparison between a Linear Reasoning Chain (LRC) and our proposed PARC (Premise-Augmented Reasoning Chain). The LRC (left), is linear and there no explicit premise link between steps. In PARC (right), premise links are explicitly established, enabling better identification of correct and incorrect steps. Accumulation errors can be traced back to faulty premises. Establishing these premises helps improve error detection with LLMs.}
    \label{fig:overview}
    \vspace{-3ex}
\end{figure*}
Chain-of-thought reasoning enhances problem solving by breaking down complex tasks into a series of logical steps, improving the accuracy and clarity of decision-making processes.
There has been a well-documented success of the reasoning capabilities of large language models (LLMs) with chain of thought (CoT; \citet{wei2023chainofthoughtpromptingelicitsreasoning}) across applications such as embodied reasoning~\cite{NEURIPS2023_3d0758f0, philipov2024simulatinguseragentsembodied}, code generation~\cite{jiang2024surveylargelanguagemodels}, and mathematical and scientific reasoning~\cite{imani-etal-2023-mathprompter, ahn2024largelanguagemodelsmathematical}.  However, the corresponding evaluation methods and metrics have a significant limitation: they focus solely on the correctness of the \textit{final} answer, neglecting intermediate reasoning processes and rationales that contribute to it. Although the final answer serves as a proxy for the reasoning capability, it is not sufficient to judge the reasoning performance of the model. Hence, evaluating the final answer provides a narrower view of the reasoning capabilities. Since the intermediate reasoning process is equally important as getting the correct final answer~\cite{huang2023reasoninglargelanguagemodels, golovneva2023roscoesuitemetricsscoring, prasad2023recevalevaluatingreasoningchains}, a comprehensive evaluation of the reasoning chains is crucial to holistically understanding the reasoning capabilities of LLMs. 

Evaluation of a reasoning chain has been studied in literature in the context of self-verification~\cite{weng-etal-2023-large}. Previous work has shown that LLMs struggle to find reasoning errors in chain-of-thought traces without the help of external verifiers~\cite{stechly2024selfverificationlimitationslargelanguage, wu-etal-2024-large, tyen-etal-2024-llms}, casting doubt on the overoptimism of LLMs’ self-critique abilities. Existing research focusing on the evaluation of reasoning chains in LLMs can be broadly categorized into \textit{reference-based} and \textit{reference-free} methods. Reference-based methods, which rely on the availability of a ground truth reasoning chain~\cite{welleck2021naturalproofs, han2024folionaturallanguagereasoning, tian-etal-2021-diagnosing}, are reliable but are constrained by the significant cost of human annotations, restricting their application. In contrast, reference-free methods~\cite{prasad2023recevalevaluatingreasoningchains, golovneva2023roscoesuitemetricsscoring, zhu2024deductivebeamsearchdecoding} bypass the need for annotations, but suffer from two major drawbacks: (1) most such works assign only a chain-level score rather than localizing specific errors, and (2) they often need fine-tuning of task-specific models, restricting their generalizability. One workaround could be using formal proof assistants like Lean~\cite{10.1007/978-3-030-79876-5_37}, which natively support the verifiability of generated proofs~\cite{yang2024formalmathematicalreasoningnew, yang2023leandojotheoremprovingretrievalaugmented, murphy2024autoformalizingeuclideangeometry}, but this requires a challenging auto-formalization of natural language text~\cite{wu2022autoformalizationlargelanguagemodels} and often assumes the solution is already known for proof construction. 

In our work, we focus on reference-free verification of LLM reasoning chains in the context of mathematical reasoning. Mathematical problem solving requires a series of deductive reasoning steps, where each step is performed under a small set of premises. We hypothesize that a step in a reasoning chain should be verified only under its premises. Previous work has shown that having unnecessary context hurts the performance of LLMs for solving math word problems~\cite{10.5555/3618408.3619699}. Thus, by excluding irrelevant context from the verifier, we make the verification of a reasoning step only conditioned on its premises, addressing the known susceptibility of LLMs to errors under distractors. 

We restructure conventional linear reasoning chains (LRCs) into \textbf{PARC (Premise-Augmented Reasoning Chains)}, by identifying the premises of each step. Identifying those premises improves the traceability of a reasoning chain. We create a corresponding dataset, called \textbf{PERL (Premises and ERrors identification in Language models)} to test LLMs' capability in identifying premises as well as the effectiveness of premise augmentation in annotating both premises and errors. Additionally, we refine the error taxonomy (as proposed in \cite{golovneva2023roscoesuitemetricsscoring}) for mathematical reasoning by introducing a new category: “accumulation error.” This error arises when a reasoning step is correct in isolation but is derived from flawed premises (refer to step 5 in Fig.\ref{fig:overview}), resulting in error propagation throughout the chain; while such errors are common, existing work~\cite{lightman2023letsverifystepstep, zheng2024processbenchidentifyingprocesserrors, daheim2024stepwiseverificationremediationstudent} has not addressed them beyond discarding all subsequent steps once the first mistake appears. Distinguishing accumulation errors from inherently flawed steps (mathematical errors or logical inconsistencies) is crucial for a holistic evaluation of CoT traces. Moreover, establishing explicit premise links between steps allows each step to be verified only under its premises which reduces irrelevant information for each verification step, thus improving the error detection for mathematical problem solving. 

Our main contributions are as follows:
\begin{itemize}
    \item We demonstrate that off-the-shelf LLMs can detect premises for a given step with high accuracy for mathematical reasoning, enabling the conversion of a Linear Reasoning Chain (LRC) to a Premise-Augmented Reasoning Chain (PARC).
    \item We establish that verifying each step under its corresponding premises increases the accuracy of identifying errors and their types.
    \item We propose a refined error taxonomy for mathematical reasoning, introducing the notion of \textit{accumulation errors} for steps that are locally correct but inherit upstream errors.
    \item We introduce and will release \textbf{PERL}, a dataset of reasoning chains annotated with premises and error types, to facilitate broader research on premise-centered reasoning verification.
\end{itemize}



The rest of the paper is structured as follows: in Section 2, we discuss relevant prior work in context of mathematical reasoning and verification of reasoning chains with LLMs. Section 3 introduces the research questions, alongside the necessary background as well as mathematical definition of premises, conversion of LRC to PARC, as well as how we identify errors with premises. Section 4 details PERL, our dataset to show effectiveness of the framework, and lastly, in section 5 we discuss the results and insights from our experiments.  