\section{Introduction}

\begin{figure}[t] 
\centering 
\includegraphics[width=0.7\textwidth]{figures/fig1.pdf} 
\caption{Comparison of performance and efficiency between Full Attention model and our \method{}. Left: Despite being sparse, \method{} surpasses Full Attention baseline on average across general benchmarks, long-context tasks, and reasoning evaluation. Right: For 64k-length sequence processing, \method{} achieves substantial computational speedup compared to Full Attention in all stages: decoding, forward propagation, and backward propagation.
}
\label{fig:fig1}
\end{figure}

The research community increasingly recognizes long-context modeling as a crucial capability for next-generation large language models, driven by diverse real-world applications ranging from in-depth reasoning~\citep{star,deepseekr1}, repository-level code generation~\citep{RepoCoder,CodeAgent} and multi-turn autonomous agent systems~\citep{AutoAgent}.
Recent breakthroughs, including OpenAI's o-series models, DeepSeek-R1~\citep{deepseekr1}, and Gemini 1.5 Pro~\citep{team2024gemini}, 
enabling models to process entire codebases, lengthy documents, maintain coherent multi-turn conversations over thousands of tokens, and perform complex reasoning across long-range dependencies.
However, the high complexity~\citep{zaheer2020big} of vanilla Attention~\citep{vaswani2017attention} mechanisms emerges as a critical latency bottleneck as sequence length increases. Theoretical estimates indicate that attention computation with softmax architectures accounts for 70--80\% of total latency when decoding 64k-length contexts, underscoring the urgent need for more efficient attention mechanisms.



A natural approach to efficient long-context modeling is to take advantage of the inherent sparsity of softmax attention \citep{ge2023model, jiang2023llmlingua}, where selectively computing critical query-key pairs can significantly reduce computational overhead while preserving performance. Recent advances demonstrate this potential through diverse strategies: 
KV-cache eviction methods~\citep{h2o,snapkv,zhou2024llm}, blockwise KV-cache selection methods~\citep{quest,infllm,gao2024seerattention}, and sampling, clustering or hashing-based selection methods~\citep{magicpig,clusterkv,desai2024hashattention}.
Despite these promising strategies, existing sparse attention methods often fall short in practical deployments. Many approaches fail to achieve speedups comparable to their theoretical gains; moreover, most methods lack effective training-time support to fully exploit the sparsity patterns of attention.


To address these limitations, the deployment of effective sparse attention must tackle two key challenges: (1) \textbf{\textit{Hardware-aligned inference speedup}}: Converting theoretical computation reductions into actual speed improvements
requires hardware-friendly algorithm design during both prefilling and decoding stages to mitigate memory access and hardware scheduling bottlenecks;
(2) \textbf{\textit{Training-aware algorithm design}}: Enabling end-to-end computation with trainable operators to 
reduce training costs while maintaining model performance.
These requirements are crucial for real-world applications to achieve fast long-context inference or training.
When considering both aspects, existing methods still exhibit a noticeable gap.


To achieve more effective and efficient sparse attention, we present \method{}, a Natively trainable Sparse Attention architecture that integrates hierarchical token modeling. As shown in \cref{fig:framework}, \method{} reduces per-query computation by organizing keys and values into temporal blocks and processing them through three attention paths: compressed coarse-grained tokens, selectively retained fine-grained tokens, and sliding windows for local contextual information. Then we implement specialized kernels to maximize its practical efficiency.
\method{} introduces two core innovations corresponding to the key requirements above:
(1) Hardware-aligned system: Optimize blockwise sparse attention for Tensor Core utilization and memory access, ensuring balanced arithmetic intensity.
(2) Training-aware design: Enable stable end-to-end training through efficient algorithms and backward operators.
This optimization enables \method{} to support both efficient deployment and end-to-end training.


We evaluate \method{} through comprehensive experiments on real-world language corpora. Pretraining on a 27B-parameter transformer backbone with 260B tokens, we assess \method{}'s performance across general language evaluations, long-context evaluations, and chain-of-thought reasoning evaluation. We further compare the kernel speed on A100 GPUs with optimized Triton~\citep{tillet2019triton} implementations. Experimental results demonstrate that \method{} achieves comparable or superior performance to full attention baseline, while outperforming existing sparse attention approaches. Additionally, \method{} delivers substantial speedups across decoding, forward, and backward stages compared to Full Attention, with the speedup ratio increasing for longer sequences. These results validate that our hierarchical sparse attention design effectively balances model capability and computational efficiency.
