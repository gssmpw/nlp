\section{Methodology}

Our technical approach spans algorithm design and kernel optimization.
In the following subsections, we first introduce the background of our methodology. Then we present the overall framework of \method{}, followed by its key algorithmic components. Finally, we detail our hardware-optimized kernel design that maximizes practical efficiency.

\subsection{Background}
\textbf{Attention Mechanism} 
 is widely used in language modeling where each query token \( \mathbf{q}_t \) computes relevance scores against all preceding keys \( \mathbf{k}_{:t} \) to generate a weighted sum of values \( \mathbf{v}_{:t} \). Formally, for an input sequence of length \( t \), the attention operation is defined as:
\begin{equation}
\label{equ:attn}
    \mathbf{o}_t = \operatorname{Attn}\left(\mathbf{q}_t, \mathbf{k}_{:t}, \mathbf{v}_{:t}\right)
\end{equation}
where \( \operatorname{Attn} \) denotes the attention function:
\begin{equation}
    \operatorname{Attn}\left(\mathbf{q}_t, \mathbf{k}_{:t}, \mathbf{v}_{:t}\right) = \sum_{i=1}^t\frac{ \alpha_{t,i} \mathbf{v}_i}{\sum_{j=1}^t \alpha_{t,j}}, \quad \alpha_{t,i} = e^{\frac{\mathbf{q}_t^\top \mathbf{k}_i}{\sqrt{d_k}}}\,.
\end{equation}
Here, \( \alpha_{t,i} \) represents the attention weight between \( \mathbf{q}_t \) and \( \mathbf{k}_i \), and \( d_k \) is the feature dimension of keys. 
As sequence length increases, attention computation becomes increasingly dominant in the overall computational cost, presenting significant challenges for long-context processing.

\textbf{Arithmetic Intensity}
is the ratio of compute operations to memory accesses. It intrinsically shapes algorithm optimization on hardware. Each GPU has a critical arithmetic intensity determined by its peak compute capability and memory bandwidth, calculated as the ratio of these two hardware limits. For computation tasks, arithmetic intensity above this critical threshold becomes compute-bound~(limited by GPU FLOPS), while below it becomes memory-bound~(limited by memory bandwidth). 

Specifically for causal self-attention mechanism, during training and prefilling phases, batched matrix multiplications and attention computations exhibit high arithmetic intensity, making these stages compute-bound on modern accelerators. In contrast, auto-regressive decoding becomes memory-bandwidth constrained
because it generates one token per forward pass while requiring loading the entire key-value cache,
resulting in low arithmetic intensity. 
This leads to different optimization goals --- reducing computation cost during training and prefilling, while reducing memory access during decoding.


\subsection{Overall Framework}

To leverage the potential of attention with natural sparse pattern, we propose replacing the original key-value pairs \( \mathbf{k}_{:t}, \mathbf{v}_{:t} \) in \cref{equ:attn} with a more compact and information-dense set of representation key-value pairs $\tilde{K}_t, \tilde{V}_t$ given each query \( \mathbf{q}_t \).
Specifically, we formally define the optimized attention output as follows:
\begin{equation}
\tilde{K}_t = f_K(\mathbf{q}_t, \mathbf{k}_{:t}, \mathbf{v}_{:t}), \quad \tilde{V}_t = f_V(\mathbf{q}_t, \mathbf{k}_{:t}, \mathbf{v}_{:t})
\end{equation}
\begin{equation}
\mathbf{o}^*_t=\operatorname{Attn}\left(\mathbf{q}_t,\tilde{K}_t, \tilde{V}_t \right)
\end{equation}

 where 
 \( \tilde{K}_t, \tilde{V}_t \) are dynamically constructed based on the current query \( \mathbf{q}_t \) and the contextual memory \( \mathbf{k}_{:t}, \mathbf{v}_{:t} \). 
We can design various mapping strategies to get different categories of $\tilde{K}_t^c, \tilde{V}_t^c$, and combine them as follows:


\begin{equation}
\mathbf{o}^*_t = \sum_{c \in \mathcal{C}} g_t^c \cdot \text{Attn}(\mathbf{q}_t, \tilde{K}_t^c, \tilde{V}_t^c).
\label{equ:gate_merge}
\end{equation}
As illustrated in \cref{fig:framework}, \method{} have three mapping strategies $\mathcal{C}=\{\text{cmp},\text{slc},\text{win}\}$, representing compression, selection, and sliding window for keys and values.
$g_t^c\in [0, 1]$ is the gate score for corresponding strategy $c$, derived from input features via an MLP and sigmoid activation.
Let \( N_t \) denote the total number of remapped keys/values:
\begin{equation}
N_t = \sum_{c \in \mathcal{C}}\text{size}[\tilde{K}^c_t].
\end{equation}
We maintain a high sparsity ratio by ensuring${N_t}{} \ll t$.

\subsection{Algorithm Design}
In this subsection, we introduce the design of our remapping strategies \( f_K \) and \( f_V \): token compression, token selection, and sliding window.


\subsubsection{Token Compression} 
By aggregating sequential blocks of keys or values into block-level representations, we obtain compressed keys and values that capture the information of the entire block.
Formally, the compressed key representation is defined as:
\begin{equation}
\tilde{K}^\text{cmp}_t = f_K^\text{cmp}(\mathbf{k}_{:t}) = \left\{\phi(\mathbf{k}_{i d+1: i d+l})\middle| 0\leq i\leq\left\lfloor\frac{t-l}{d}\right\rfloor\right\}
\end{equation}
where \( l \) is the block length, \( d \) is the sliding stride between adjacent blocks, and
\( \phi \) is a learnable MLP with intra-block position encoding to map keys in a block to a single compressed key.
$\tilde{K}_t^\text{cmp}\in \mathbb{R}^{ d_k \times \left\lfloor\frac{t-l}{d}\right\rfloor }$ is tensor composed by compresion keys. 
Usually, we adopt $d < l$ to mitigate information fragmentation.
An analogous formulation holds for the compressed value representation \( \tilde{V}_t^\text{cmp} \). 
Compressed representations capture coarser-grained higher-level semantic information and reduce computational burden of attention.

\subsubsection{Token Selection} 
Using only compressed keys, values might lose important fine-grained information, motivating us to selectively preserve individual keys, values. Below we describe our efficient token selection mechanism that identifies and preserves the most relevant tokens with low computational overhead.

\textbf{Blockwise Selection.} 
Our selection strategy processes key and value sequences in spacial continuous blocks, motivated by two key factors: hardware efficiency considerations and inherent distribution patterns of attention scores. \textit{\textbf{Blockwise selection is crucial to achieve efficient computation on modern GPUs.}} That is because modern GPU architectures exhibit significantly higher throughput for continuous block accesses compared to random index-based reads. Also, blockwise computation enables optimal utilization of Tensor Cores. This architectural characteristic has established blockwise memory access and computation as a fundamental principle in high-performance attention implementations, as exemplified by FlashAttention's block-based design. \textit{\textbf{Blockwise selection follows the inherent distribution patterns of attention scores.}} Prior works~\citep{minference} have shown that attention scores often exhibit spatial continuity, suggesting that neighboring keys tend to share similar importance levels. Our visualization in \cref{appendix:vis} also shows this spatial continuous pattern.

To implement blockwise selection, we first divide key, value sequences into selection blocks. To identify the most important blocks for attention computation, we need to assign importance scores to each block. Below we present our method for computing these block-level importance scores.





\textbf{Importance Score Computation.} 
Computing block importance scores could introduce significant overhead. Fortunately, the attention computation of compression tokens produces intermediate attention scores that we can leverage to induce selection block importance scores, formulated as: % can be formulated as:
\begin{equation}
\mathbf{p}_t^\text{cmp} = \operatorname{Softmax}\left(\mathbf{q}_t^T \tilde{K}_t^\text{cmp}\right),
\end{equation}
where \( \mathbf{p}_t^\text{cmp} \in \mathbb{R}^{\left\lfloor\frac{t-l}{d}\right\rfloor+1}  \) is the attention scores between $q_t$ and  compression keys $\tilde{K}_t^\text{cmp}$. 
Let $l'$ denote the selection block size. 
When compression blocks and selection blocks share the same blocking scheme, i.e., $l'=l=d$,
we can directly obtain the selection block importance scores $\mathbf{p}_t^\text{slc}$ by $\mathbf{p}_t^\text{slc} = \mathbf{p}_t^\text{cmp}$ straightforwardly.
For cases where the blocking schemes differ, we derive the importance scores for selection blocks according to their spatial relationship. Given $l\leq l', $ $d \mid l$ and $d \mid l'$, we have:
\begin{equation}
\mathbf{p}_t^\text{slc}[j] = \sum_{m=0}^{\frac{l'}{d}-1}\sum_{n=0}^{\frac{l}{d} -1} \mathbf{p}_t^\text{cmp}\left[\frac{l'}{d}j -m -n \right],
\label{equ.8}
\end{equation}
where$[\cdot]$ denotes the indexing operator for accessing vector element. 
For models employing GQA or MQA where key-value caches are shared across query heads, consistent block selection across these heads has to be ensured to minimize KV cache loading during decoding. The shared importance scores across heads in a group are formally defined as:

\begin{equation}
{\mathbf{p}_t^{\text{slc}}}' = \sum_{h=1}^{H} \mathbf{p}_{t}^{\text{slc}, (h)},
\end{equation}
where $(h)$ in the superscript denotes the head index, and $H$ is the number of query heads in each group. This aggregation ensures consistent block selection across heads within the same group.

\textbf{Top-$\pmb{n}$ Block Selection.} After obtaining the selection block importance scores,
We retain tokens within the top-$n$ sparse blocks ranked by block importance scores, formulated as:
\begin{equation}
\mathcal{I}_t = \{i \mid \text{rank}({\mathbf{p}_t^\text{slc}}'[i]) \leq n\}
\end{equation}
\begin{equation}
\tilde{K}^\text{slc}_t =  \operatorname{Cat}\left[\{\mathbf{k}_{il'+1:(i+1)l'}|i \in \mathcal{I}_t\}\right],
\end{equation}
where rank$(\cdot)$ denotes the ranking position in descending order, with rank = 1 corresponding to the highest score, \( \mathcal{I}_t \) is the set of selected blocks' indices,
\( \operatorname{Cat} \) denotes the concatenation operation.
$\tilde{K}_t^\text{slc}\in \mathbb{R}^{ d_k \times nl' }$ is tensor composed by compresion keys. 
An analogous formulation applies to the fine-grained value \( \tilde{V}^\text{slc}_t \).
The selected keys and values then participate in the attention computation with $\mathbf{q}_t$ as defined in \cref{equ:gate_merge}.


\subsubsection{Sliding Window}
In attention mechanisms, local patterns typically adapt faster and can dominate the learning process, potentially preventing the model from effectively learning from compression and selection tokens. To address this issue, we introduce a dedicated sliding window branch that explicitly handles local context, allowing other branches (compression and selection) to focus on learning their respective features without being shortcutted by local patterns. Specifically, we maintain recent tokens $\tilde{K}_t^\text{win}=\mathbf{k}_{t-w:t}, \tilde{V}_t^\text{win}=\mathbf{v}_{t-w:t}$ in a window $w$, and isolate attention computations of different information sources (compression tokens, and selected tokens, sliding window) into separate branches. These branch outputs are then aggregated through a learned gating mechanism. 
To further prevent shortcut learning across attention branches with marginal computational overhead, we provide independent keys and values for three branches.
This architectural design enables stable learning by preventing gradient interference between local and long-range pattern recognition, while introducing minimal overhead.



After obtaining all three categories of keys and values ($\tilde{K}_t^\text{cmp}, \tilde{V}_t^\text{cmp}$; $\tilde{K}_t^\text{slc}, \tilde{V}_t^\text{slc}$; and $\tilde{K}_t^\text{win}, \tilde{V}_t^\text{win}$), we compute the final attention output following \cref{equ:gate_merge}.
Together with the compression, selection, and sliding window mechanisms described above, this forms the complete algorithmic framework of \method{}.

\subsection{Kernel Design}
To achieve FlashAttention-level speedup during the training and prefilling, we implement hardware-aligned sparse attention kernels upon Triton. Given MHA is memory-intensive and inefficient for decoding, we focus on architectures with shared KV caches like GQA and MQA following the current state-of-the-art LLMs. While compression and sliding window attention computations are readily compatible with existing FlashAttention-2 kernels, we introduce the specialized kernel design for sparse selection attention.
If we were to follow FlashAttention's strategy of loading temporally continuous query blocks into SRAM, it would result in inefficient memory access since queries within a block may require disjoint KV blocks. To address this, our key optimization lies in a different query grouping strategy: for each position on the query sequence, we load all query heads within a GQA group (they share the same sparse KV blocks) into SRAM. \cref{fig:fig3} illustrates our forward pass implementation.
The proposed kernel architecture is characterized by the following key features:
\begin{enumerate}
    \item \textbf{Group-Centric Data Loading}. For each inner loop, load all heads' queries $Q\in \mathbb{R}^{[ h, d_k]}$ in the group at position $t$ and their shared sparse key/value block indices $\mathcal{I}_t$.
    \item \textbf{Shared KV Fetching}. In the inner loop, Sequentially load continuous key/value blocks indexed by $\mathcal{I}_t$ into SRAM as $K\in\mathbb{R}^{[B_k, d_k]}, V\in\mathbb{R}^{[B_k, d_v]}$ to minimize memory loading, where $B_k$ is the kernel block size satisfying $B_k | l'$.
    \item \textbf{Outer Loop on Grid}. Since the inner-loop length (proportional to the selected block count $n$) remains nearly identical for different query blocks, we put query/output loops in Triton's grid scheduler to simplify and optimize the kernel.
\end{enumerate}


This design achieves near-optimal arithmetic intensity by (1) eliminating redundant KV transfers through group-wise sharing, and (2) balancing compute workloads across GPU streaming multiprocessors. 


\begin{figure}[h] 
\centering 
\includegraphics[width=0.5\linewidth]{figures/fig3.pdf}
\caption{Kernel design for \method{}. The kernel loads queries by GQA groups (Grid Loop), fetches corresponding sparse KV blocks (Inner Loop), and performs attention computation on SRAM. Green blocks indicate data on SRAM, while blue indicates data on HBM.}
\label{fig:fig3} 
\end{figure}