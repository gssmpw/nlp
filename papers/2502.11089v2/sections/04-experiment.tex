\section{Experiments}
\label{sec:experiment}
We evaluate \method{} through three lenses: (1) general benchmarks performance, (2) long-context benchmarks performance, and (3) chain-of-thought reasoning performance, comparing against Full Attention baseline and state-of-the-art sparse attention methods. We defer the efficiency analysis of our sparse computation paradigm to \cref{sec:time_comparsion}, where we provide detailed discussions on training and inference speed.

\subsection{Pretraining Setup} 
Following the common practice in state-of-the-art LLMs, our experiments adopt a backbone combining Grouped-Query Attention (GQA) and Mixture-of-Experts (MoE), featuring $27\text{B}$ total parameters with $3\text{B}$ active parameters. The model consists of 30 layers with a hidden dimension of 2560. For GQA, we set the number of groups to 4, with a total of 64 attention heads. For each head, the hidden dimensions of the query, key, and value are configured as $d_q = d_k = 192$ and $d_v = 128$, respectively. For MoE, we utilize the DeepSeekMoE~\citep{dai2024deepseekmoe,deepseekV2} structure, with 72 routed experts and 2 shared experts, and set the top-k experts to 6. To ensure training stability, the MoE in the first layer is replaced by an MLP in the form of SwiGLU. The proposed architecture achieves an effective trade-off between computation cost and model performance.
For \method{}, we set compression block size $l=32$, sliding stride $d=16$, selected block size $l'=64$, selected block count $n=16$ (including fixed activating the 1 initial block and 2 local blocks), and sliding window size $w=512$. Both Full Attention and sparse attention models are pretrained on $270\text{B}$ tokens of $8\text{k}$-length texts, followed by continued training and supervised fine-tuning on $32\text{k}$-length texts with YaRN~\citep{yarn} to achieve long-context adaptation. Both models are trained to full convergence to ensure fair comparison. As shown in ~\cref{fig:losscurve}, the pretraining loss curve of our \method{} and Full Attention baseline demonstrates stable and smooth decline, with \method{} consistently outperforming the Full Attention model.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/loss_curve.pdf}
    \caption{Pretraining loss comparison between Full Attention and our \method{} on 27B-parameter model. Both models exhibit stable convergence, with \method{} achieving lower loss values.}
    
    \label{fig:losscurve}
\end{figure}
\input{tables/base_eval}

\subsection{Baselines Methods} 
In addition to comparing with Full Attention, we evaluate several state-of-the-art inference-stage sparse attention methods: H2O \citep{h2o}, infLLM \citep{infllm}, Quest \citep{quest}, and Exact-Top, which first computes full attention score and select the top-$n$ scores keys corresponding to each query and then calculates attention on these positions. These methods span diverse sparse attention paradigms, including KV-cache eviction, query-aware selection, and exact top-$n$ sparse selection. 

For general evaluation, where most samples have lengths within the local context window of sparse attention baselines, these methods are effectively equivalent to Full Attention. Therefore, we present only the comparison results between \method{} and Full Attention baseline in this setting. In the long-context evaluation, we conduct comparisons across all baseline methods, with the sparsity of all sparse attention methods set to the same to ensure a fair comparison. For chain-of-thought reasoning evaluation, which requires long-text supervised fine-tuning, we limit our comparison to Full Attention, as sparse attention baselines do not support training.






\subsection{Performance Comparison}
\input{tables/longbench}

\textbf{General Evaluation.} We evaluated the pretrained \method{} and Full Attention baseline, on a comprehensive suite of benchmarks spanning knowledge, reasoning, and coding capabilities, including MMLU~\citep{hendrycks2020measuring}, MMLU-PRO~\citep{wang2024mmlu}, CMMLU~\citep{li2023cmmlu}, BBH~\citep{suzgun2022challenging}, GSM8K~\citep{cobbe2021training}, MATH~\citep{hendrycks2020measuring}, DROP~\citep{dua2019drop}, MBPP~\citep{austin2021program}, and HumanEval~\citep{chen2021evaluating}. The results are shown in \cref{tab:base_eval}. Despite its sparsity, \method{} achieves superior overall performance, outperforming all baselines including Full Attention on 7 out of 9 metrics. This indicates that although \method{} may not fully leverage its efficiency advantages on shorter sequences, it shows strong performance.
Notably, \method{} demonstrates significant gains in reasoning-related benchmarks (DROP: +0.042, GSM8K: +0.034), suggesting that 
our pretraining helps models to develop specialized attention mechanisms. This sparse attention pretraining mechanism forces model to focus on the most important information, potentially enhancing performance by filtering out noise from irrelevant attention pathways.
The consistent performance across diverse evaluations also validates \method{}'s robustness as a general-purpose architecture.

\textbf{Long-Context Evaluation.} As shown in \cref{fig:niah},
\method{} achieves perfect retrieval accuracy across all positions in 64k-context needle-in-a-haystack~\citep{kamradt2023llmtest} test. This performance stems from our hierarchical sparse attention design, which combines compression tokens for efficient global context scanning, and  
selection tokens for precise local information retrieval.  
The coarse-grained compression identifies relevant context blocks at low computational cost, while the token-level attention on selected tokens ensures the preservation of critical fine-grained information. This design enables \method{} to maintain both global awareness and local precision.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/NIAH.pdf}
    \caption{Needle-in-a-Haystack retrieval accuracy across context positions with 64k context length. \method{} achieves perfect accuracy through its hierarchical sparse attention design.}
    \label{fig:niah}
\end{figure}

We also evaluate \method{} on LongBench~\citep{bai2023longbench} against state-of-the-art sparse attention methods and Full Attention baseline. 
To ensure consistent sparsity, we set the token activated by each query in all sparse attention baselines to 2560 tokens, which corresponds to the average number of tokens activated in \method{} when handling 32k sequence lengths. Following StreamLLM~\citep{streaming}, this token budget includes the leading 128 tokens and 512 local tokens.
We exclude certain subsets from LongBench due to their low scores across all models, which may not provide meaningful comparisons. As shown in \cref{tab:longbench}, \method{} achieves the highest average score 0.469, outperforming all baselines (+0.032 over Full Attention and +0.046 over Exact-Top). This improvement arises from two key innovations: (1) our native sparse attention design, which enables end-to-end optimization of sparse patterns during pretraining, facilitates synchronized adaptation between the sparse attention module and other model components; and (2) the hierarchical sparse attention mechanism achieves a balance between local and global information processing.

Notably, \method{} demonstrates exceptional performance on tasks requiring complex reasoning over long contexts, achieving +0.087 and +0.051 improvements over Full Attention on multi-hop QA tasks (HPQ and 2Wiki), exceeding the performance of baselines on code understanding (LCC: +0.069), and outperforming other methods on passage retrieval (PassR-en: +0.075). These results validate \method{}'s capability to handle diverse long-context challenges, with its natively pretrained sparse attention providing additional benefits in learning task-optimal patterns.


\textbf{Chain-of-Thought Reasoning Evaluation.}  
To evaluate \method{}'s compatibility with advanced downstream training paradigms, we investigate its capacity to acquire chain-of-thought mathematical reasoning abilities via post-training. 
Given the limited effectiveness of reinforcement learning on smaller-scale models, we employ knowledge distillation from DeepSeek-R1, conducting supervised fine-tuning (SFT) with 10B tokens of 32k-length mathematical reasoning traces. This produces two comparable models: Full Attention-R (Full Attention baseline) and \method{}-R (our sparse variant). We assess both models on the challenging American Invitational Mathematics Examination (AIME 24) benchmark. We use a sampling temperature of 0.7 and a top-$p$ value of 0.95 to generate 16 responses for each question and obtain the average score. To validate the impact of reasoning depth, we conduct experiments with two generation context limits: 8k and 16k tokens, measuring whether extended reasoning chains improve accuracy. Example comparisons of model predictions are provided in \cref{app:aime}.
\input{tables/aime}

As shown in \cref{tab:aime}, \method{}-R achieves significantly higher accuracy than Full Attention-R under the 8k context setting (+0.075), with this advantage persisting at 16k contexts (+0.054).
These results validate two key benefits of native sparse attention: (1) The pretrained sparse attention patterns enable efficient capture of long-range logical dependencies critical for complex mathematical derivations; (2) Our architecture's hardware-aligned design maintains sufficient context density to support growing reasoning depth without catastrophic forgetting. The consistent outperformance across context lengths confirms sparse attention's viability for advanced reasoning tasks when natively integrated into the training pipeline.


\section{Efficiency Analysis }
\label{sec:time_comparsion}
We evaluate the computational efficiency of \method{} against Full Attention on an 8-GPU A100 system. In efficiency analysis, we also configure the model with GQA group $g=4$, heads per group $h=16$, query/key dimension $d_k=192$, and value dimension $d_v=128$. 
Following the same settings in \cref{sec:experiment}, we set \method{} compression block size $l=32$, sliding stride $d=16$, selected block size $l'=64$, selected block count $n=16$, and sliding window size $w=512$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/train_time.pdf}
    \caption{Comparison of Triton-based \method{} kernel with Triton-based FlashAttention-2 kernel. Our implementation significantly reduces latency across all context lengths, with the improvement becoming more pronounced as input length increases.}
    \label{fig:speed}
\end{figure}
\subsection{Training Speed}
We compare the Triton-based implementations of our \method{} attention and Full Attention with Triton-based FlashAttention-2 to ensure fair speed comparison across the same backend.
As shown in \cref{fig:speed}, our \method{} achieves progressively greater speedups as context length increases, up to 9.0$\times$ forward and 6.0$\times$ backward speedup at 64k context-length. Notably, the speed advantage becomes more pronounced with longer sequences. This speedup stems from our hardware-aligned algorithm design to maximize the efficiency of sparse attention architecture: (1) The Blockwise memory access pattern maximizes Tensor Core utilization through coalesced loads, (2) The 
delicate loop scheduling in the kernel eliminates redundant KV transfers.

\input{tables/decode_speed}
\subsection{Decoding Speed}
The decoding speed of Attention is primarily determined by the memory access bottleneck, which is closely tied to the amount of KV cache loading. In each decoding step, Our \method{} just needs to load at most $\left\lfloor\frac{s-l}{d}\right\rfloor$ compression tokens, $nl'$ selected tokens, and $w$ neighbor tokens, where $s$ is the cached sequence length.
As shown in \cref{tab:infer}, our method exhibits a significant reduction in latency as the decoding length increases, achieving up to 11.6$\times$ speedup at 64k context-length. This advantage in memory access efficiency also amplifies with longer sequences.
