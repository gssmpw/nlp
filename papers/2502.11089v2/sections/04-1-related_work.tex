\section{Related Works}
\label{app:related}


We review existing approaches that improve the efficiency of attention computation through sparse attention. These methods can be broadly categorized into three groups based on their core strategies: (1) fixed sparse pattern, (2) dynamic token pruning, and (3) query-aware selection. We introduce several representative works from each category.

\subsection{Fixed Sparse Pattern}
SlidingWindow is a commonly used approach that allows the query to compute attention only within a fixed window. 
StreamingLLM~\citep{streaming} combines attention sinks with local windows to process continuous text streams. 
MoA~\citep{moa} and DuoAttention~\citep{xiao2024duoattention} also adopt similar local and sink information for long sequence modeling.
Longformer~\citep{beltagy2020longformer} interleaves local windowed attention with global tokens to process long sequences.
Compared with them, our \method{} does not rely on pre-defined sparse patterns, but learns the patterns automatically, unlocking the potential to utilize full context. 

\subsection{Dynamic Token Pruning}
Some methods are designed to reduce memory and computation costs during inference through dynamic KV-cache pruning.
H2O~\citep{h2o}, BUZZ~\citep{zhao2024buzz}, and SepLLM~\citep{chen2024sepllm} implement adaptive approaches to reduce KV-cache memory usage during decoding. These methods dynamically evict tokens deemed less important for future predictions.
FastGen~\cite{ge2023model} and HeadKV~\citep{fu2024not} optimize computation by assigning different strategies to individual attention heads.
SnapKV~\citep{snapkv} introduces a token pruning strategy that reduces the KV-cache by selectively retaining only the most crucial features, enabling efficient memory usage. 
Unlike these inference-focused approaches, our \method{} incorporates sparsity natively during the training phase.



\subsection{Query-Aware Selection}
Other works focus on query-dependent token selection methods to reduce attention computation while preserving attention quality.
Quest~\citep{quest} employs a blockwise selection strategy where each chunk's importance is estimated by product between query and coordinate-wise min-max of the key chunks.
InfLLM~\citep{infllm} combines fixed patterns with retrieval by maintaining attention sinks, local context, and retrievable chunks. This method selects representative keys from each chunk to estimate chunk importance.
HashAttention~\citep{desai2024hashattention} formulates pivotal token identification as a recommendation problem by mapping queries and keys to Hamming space using learned functions.
ClusterKV~\citep{clusterkv} achieves sparsity by firstly clustering keys and then selecting the most relevant clusters for attention computation based on query-cluster similarity.
MInference~\citep{minference} and TokenSelect~\citep{wu2024tokenselect} select KV pairs for computation based on token-level importance scoring.
SeerAttention~\citep{gao2024seerattention} separates queries and keys into spatial blocks and performs blockwise selection for efficient computation.
Compared to these methods, our \method{} achieves hardware-aligned sparse attention computation throughout the full model lifecycle, including training, prefilling, and decoding.