\section{Discussion}
In this section, we reflect on the development process of \method{} and discuss key insights gained from our exploration of different sparse attention strategies. While our approach demonstrates promising results, understanding the challenges encountered with alternative strategies and analyzing attention patterns provides valuable context for future research directions. We first examine challenges with alternative token selection strategies that motivated our design choices, followed by visualizations that offer insights into attention distribution patterns.

\subsection{Challenges with Alternative Token Selection Strategies}
\label{app:challenge}
Before designing \method{}, we explored adapting existing sparse attention methods to the training stage. However, these attempts encountered various challenges, prompting us to design a different sparse attention architecture:

\textbf{Key-Clustering Based Strategies.} 
We examined clustering-based strategies like ClusterKV~\citep{clusterkv}. These methods store Keys and Values from the same cluster in contiguous memory regions. While theoretically feasible for training and inference, they face three significant challenges: (1) Non-trivial computational overhead introduced by dynamic clustering mechanisms; (2) Operator optimization difficulties exacerbated by inter-cluster imbalances, especially in Mixture-of-Experts (MoE) systems, where skewed Expert Parallelism (EP) group execution times lead to persistent load imbalances; (3) Implementation constraints arising from the need for mandatory periodic reclustering and chunk-sequential training protocols. These combined factors create substantial bottlenecks, significantly limiting their effectiveness for real-world deployment.
\begin{figure}[t]
    \centering
    \begin{minipage}{0.52\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/loss_curve3b.pdf}
        \caption{Compare training loss on a 3B-parameter model with Full Attention and different token selection strategies and. Our \method{} achieves better performance.}
        \label{fig:losscurve3b}
    \end{minipage}
    \hfill
    \begin{minipage}{0.44\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/visual.pdf}
        \caption{Visualization of Attention Map on a Full Attention transformer. Light-colored regions indicate higher attention values. As shown in the figure, attention scores exhibit blockwise clustering distribution.}
        \label{fig:visual}
    \end{minipage}
\end{figure}

\textbf{Other Blockwise Selection Strategies.}
We also considered blockwise key, value selection strategies different from \method{}, such as Quest~\citep{quest} and InfLLM~\citep{infllm}. These methods rely on
computing an importance score
for each KV block and selecting the top-$n$ blocks based on their similarity with $q_t$. However, existing methods face two critical issues: (1) Since the selection operation is non-differentiable, importance score computation based on neural networks relies on auxiliary loss, which increases operator overhead and often degrades model performance; (2) Heuristic parameter-free importance score computation strategy suffer from low recall rates, leading to suboptimal performance. We evaluate both approaches on a 3B-parameter model with similar architecture and compare their loss curve with \method{} and Full Attention.
For the auxiliary loss-based selection method, we introduce additional queries for each token and representative keys for each block to estimate the block importance scores.
We compute block-level supervision signals by mean-pooling attention scores within each key block, and use KL divergence to supervise block importance prediction. We maintain individual query granularity instead of block-averaged queries to accommodate efficient decoding. This auxiliary loss-based importance estimation shares conceptual similarity with SeerAttention~\citep{gao2024seerattention}.
For the heuristic parameter-free selection method, following the strategy of Quest, we implement direct selection using the product between queries and coordinate-wise min-max of the key chunks, without introducing additional parameters. We also explore a cold-start training approach where Full Attention is applied for the initial 1000 steps before transitioning to the heuristic blockwise selection.
As shown in ~\cref{fig:losscurve3b}, both methods exhibited inferior loss.

\vspace{-10pt}
\subsection{Visualization}
\label{appendix:vis}

To explore potential patterns in transformer attention distributions and seek inspiration for our design, we visualize the attention map from our pretrained 27B Full Attention model in \cref{fig:visual}. The visualization reveals interesting patterns where attention scores tend to exhibit blockwise clustering characteristics, with nearby keys often showing similar attention scores. This observation inspired our design of \method{}, suggesting that selecting key blocks based on spatial continuity might be a promising approach. The blockwise clustering phenomenon indicates that tokens adjacent in the sequence may share certain semantic relationships with query tokens, though the exact nature of these relationships requires further investigation. This observation motivated us to explore a sparse attention mechanism that operates on continuous token blocks rather than individual tokens, aiming to enhance computational efficiency and preserve high-attention patterns. 