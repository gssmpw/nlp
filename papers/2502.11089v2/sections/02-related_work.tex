\section{Rethinking Sparse Attention Methods}
\label{sec:critique}

Modern sparse attention methods have made significant strides in reducing the theoretical computational complexity of transformer models. However, most approaches predominantly apply sparsity during inference while retaining a pretrained Full Attention backbone, potentially introducing architectural bias that limits their ability to fully exploit sparse attention's advantages. Before introducing our native sparse architecture, we systematically analyze these limitations through two critical lenses.


\begin{figure*}[t] 
\centering 
\includegraphics[width=1\textwidth]{figures/fig2.pdf} 
\caption{Overview of \method{}'s architecture. Left: The framework processes input sequences through three parallel attention branches: For a given query, preceding keys and values are processed into compressed attention for coarse-grained patterns, selected attention for important token blocks, and sliding attention for local context. Right: Visualization of different attention patterns produced by each branch. Green areas indicate regions where attention scores need to be computed, while white areas represent regions that can be skipped.}
\label{fig:framework}
\end{figure*}


\subsection{The Illusion of Efficient Inference}

Despite achieving sparsity in attention computation, many methods fail to achieve corresponding reductions in inference latency, primarily due to two challenges:

\textbf{Phase-Restricted Sparsity.}
Methods such as H2O \citep{h2o} apply sparsity during autoregressive decoding while requiring computationally intensive pre-processing (e.g. attention map calculation, index building) during prefilling. In contrast, approaches like MInference \citep{minference} focus solely on prefilling sparsity. 
These methods fail to achieve acceleration across all inference stages, as at least one phase remains computational costs comparable to Full Attention.
The phase specialization reduces the speedup ability of these methods in prefilling-dominated workloads like book summarization and code completion, or decoding-dominated workloads like long chain-of-thought~\citep{cot} reasoning.

\textbf{Incompatibility with Advanced Attention Architecture.}
Some sparse attention methods fail to adapt to modern decoding efficient architectures like Mulitiple-Query Attention~(MQA) \citep{mqa} and Grouped-Query Attention~(GQA) \citep{gqa}, which significantly reduced the memory access bottleneck during decoding by sharing KV across multiple query heads. For instance, in approaches like Quest \citep{quest}, each attention head independently selects its KV-cache subset. Although it demonstrates consistent computation sparsity and memory access sparsity in Multi-Head Attention (MHA) models, it presents a different scenario in models based on architectures like GQA, where the memory access volume of KV-cache corresponds to the union of selections from all query heads within the same GQA group. This architectural characteristic means that while these methods can reduce computation operations, the required KV-cache memory access remains relatively high.
This limitation forces a critical choice: while some sparse attention methods reduce computation, their scattered memory access pattern conflicts with efficient memory access design from advanced architectures.

These limitations arise because many existing sparse attention methods focus on KV-cache reduction or theoretical computation reduction, but struggle to achieve significant latency reduction in advanced frameworks or backends.
This motivates us to develop algorithms that combine both advanced architectural and hardware-efficient implementation to fully leverage sparsity for improving model efficiency.


\subsection{The Myth of Trainable Sparsity}
Our pursuit of native trainable sparse attention is motivated by two key insights from analyzing inference-only approaches:
(1) \textbf{\textit{Performance Degradation}}: Applying sparsity post-hoc forces models to deviate from their pretrained optimization trajectory. As demonstrated by \citet{magicpig}, top 20\% attention can only cover 70\% of the total attention scores, rendering structures like retrieval heads in pretrained models vulnerable to pruning during inference.
(2)~\textbf{\textit{Training Efficiency Demands}}: 
Efficient handling of  long-sequence training is crucial for modern LLM development. This includes both pretraining on longer documents to enhance model capacity, and subsequent adaptation phases such as long-context fine-tuning and reinforcement learning. However, existing sparse attention methods primarily target inference, leaving the computational challenges in training largely unaddressed. This limitation hinders the development of more capable long-context models through efficient training. Additionally, efforts to adapt existing sparse attention for training also expose challenges:



\textbf{Non-Trainable Components.} Discrete operations in methods like ClusterKV~\citep{clusterkv} 
(includes k-means clustering) and MagicPIG~\citep{magicpig} (includes SimHash-based selecting) create discontinuities in the computational graph. These non-trainable components prevent gradient flow through the token selection process, limiting the model's ability to learn optimal sparse patterns. 

\textbf{Inefficient Back-propagation.} Some theoretically trainable sparse attention methods suffer from practical training inefficiencies. Token-granular selection strategy used in approaches like HashAttention~\citep{desai2024hashattention} leads to the need to load a large number of individual tokens from the KV cache during attention computation. 
This non-contiguous memory access prevents efficient adaptation of fast attention techniques like FlashAttention, which rely on contiguous memory access and blockwise computation to achieve high throughput.
As a result, implementations are forced to fall back to low hardware utilization, significantly degrading training efficiency.



\subsection{Native Sparsity as an Imperative}

These limitations in inference efficiency and training viability motivate our fundamental redesign of sparse attention mechanisms.
We propose \method{}, a natively sparse attention framework that addresses both computational efficiency and training requirements.
In the following sections, we detail the algorithmic design and operator implementation of \method{}.
