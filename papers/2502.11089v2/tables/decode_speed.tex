\begin{table}[t]
\centering
\resizebox{0.66\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
Context Length& ~~8192~~& ~~ 16384 ~~ & 32768 &~~65536~~\\
\midrule
 Full Attention& ~~8192~~& 16384& 32768 &~~65536~~\\
\method{}& ~~2048~~&  2560& 3584 &~~5632~~\\
\midrule
Expected Speedup& \textbf{4$\times$}& \textbf{~~6.4$\times$}& \textbf{~~9.1$\times$} &\textbf{~~11.6$\times$} \\
\bottomrule
\end{tabular}
}
\caption{Memory access volume (in equivalent number of tokens) per attention operation during decoding. Due to the low arithmetic intensity and memory-bound nature of decoding, the expected speedup is approximately linear with the volume of memory access.}

\label{tab:infer}
\end{table}
