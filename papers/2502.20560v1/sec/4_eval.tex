\section{Case Study I: General Scene Understanding}
\subsection{Setup}

We first evaluate 
the LVLM factuality framework for general scene understanding tasks.
To do so, we use 
$500$ randomly selected images from the MSCOCO~\cite{lin2014microsoft} validation set with more than three objects (same as the POPE dataset~\cite{li2023evaluating}).

\paragraph{LVLMs.}
We use four start-of-the-art LVLMs for the evaluation of the scene understanding task, including three open-sourced models \modelname{LLaVA-1.5}~\cite{liu2024visual}, \modelname{Phi-3.5-vision-instruct}~\cite{abdin2024phi}, \modelname{Llama-3.2-11B-Vision}~\cite{metaLlama32}, and one close-sourced model \modelname{GPT-4o-mini}~\cite{openaigpt4omini}.
We prompt each LVLM to generate a detailed description for each image and decompose the original response into independent claims.
Our prompts are provided in the Appendix.


\paragraph{Error Annotation and Loss Function.} 
We consider the following five types of errors for scene understanding:
(1) \textit{Object identification}: The claim involves hallucinated or wrongly identified objects;
(2) \textit{Attribute (in)accuracy}: The claim involves incorrect attributes (e.g., color, size, shape);
(3) \textit{Spatial relations}: The claim involves incorrect spatial relationships between objects;
(4) \textit{Interaction/Action (in)accuracy}: The claim involves incorrect or hallucinated action or interaction;
and (5) \textit{Quantitative information}: The claim involves incorrect numeric details (e.g., the wrong object count).
We prompt \modelname{GPT-4o} to label each claim as either correct or belonging to one or more error categories.
In practice, the loss function can be tailored according to %
specific use cases. For 
our experiments, we consider a cumulative loss function that assigns a loss score to each response based on the total error its claims contain. Specifically, all claims start with a loss score equal to $0$. For each ``Object identification'' error contained in a claim, the loss is increased by $3$; for every other type of error contained, the loss is increased by $1$. A correct claim thus receives a loss of $0$.
This choice of loss structure reflects the common consensus that hallucinating non-existing objects is typically more harmful compared to other types of hallucinations.

\paragraph{Scoring Function.}
We use two configurations of pretrained CLIP~\cite{radford2021learning} models, \modelname{CLIP-ViT-Base} with $32px$ patch size and \modelname{CLIP-ViT-Large} with $14px$ patch size, to derive the normalized image and text embeddings and compute the dot product as external confidence scores.

\begin{figure}
    \centering
\includegraphics[width=0.56\linewidth]{figs/pope_coverage_err=0.pdf}
    \caption{Alignment between empirical and desired (theoretical) coverage in \textit{scene understanding} (with $\lambda=0$).
    Vanilla LVLM (red dashed line) refers to the base setting where the LVLM-generated responses are returned to users without using \methodName.
    }
    \label{fig:pope_coverage}
\end{figure}



\subsection{Results}

\paragraph{\methodName~Achieves Any Desired Level of Coverage.}
We
examine the validity of \methodName~by measuring the empirical coverage (ratio of responses that satisfy $\gL\big(\hat{F}(\mC_{n+1}), I_{n+1})\big) \leq \lambda$ over total number of responses) under different levels of desired (theoretical) coverage determined by $1-\alpha$.
We set the error tolerance $\lambda$ to $0$ (most restrictive) and report the results over $50$ random splits of calibration ($400$ data points for establishing conformal prediction) and test data ($100$ data points for computing the empirical coverage). %
The results shown in Fig.~\ref{fig:pope_coverage} confirm that \methodName~can
achieve the desired level of coverage with all types of scoring functions and for all LVLMs considered. 
By contrast, Vanilla LVLM (i.e., responses without any filtration) leads to significantly low coverage that signals a failure in the model's reliability. 


\begin{figure}
    \centering
\includegraphics[width=0.56\linewidth]{figs/pope_avg_cont_err=0.pdf}
    \caption{Average ratio of claims filtered with varying coverage using different scoring functions in \textit{scene understanding} (with $\lambda=0$). Standard errors are marked.}
    \label{fig:pope_cont}
\end{figure}

\paragraph{\methodName~Achieves Higher Filtering Efficiency Than Baselines.}
Approaching the desired coverage level of $1-\alpha$ involves flagging and filtering out low-confidence claims and, in some cases, abstaining from providing a response.
Next, we analyze the ratio of claims being filtered out by \methodName~(Fig.~\ref{fig:pope_cont}) and the associated rate of abstention  (Fig.~\ref{fig:pope_abstent}).
We observe a general trend where the ratio of filtered claims and the abstention rate increase as the desired coverage level rises.
This is expected as the unfiltered responses from LVLMs contain a large number of non-factual claims (e.g., $87.8\%$ responses from \modelname{LLaVA-1.5} are erroneous), and thus ensuring a lower error rate requires the framework to be more conservative and filter more content. When comparing across different scoring functions, it can be seen that, in the case of \modelname{LLaVA-1.5}, the external confidence scores based on \modelname{CLIP} models 
achieve a lower ratio of filtered claims and also a lower abstention rate than
internal scores. This is notable
because \modelname{LLaVA-1.5} uses \modelname{CLIP} as the visual encoder, which implies that there may be certain deficiencies in the visual instruction tuning process.
However, such a trend is weaker or non-existent
with other LVLMs. This may be because claims generated by other LVLMs are typically longer and contain more details, which is 
more difficult to capture using \modelname{CLIP}.
When comparing across LVLMs, we see that \modelname{GPT-4o-mini} requires filtering out much fewer claims to achieve the desired coverage, which is because \modelname{GPT-4o-mini} has better empirical factuality performance compared to other models.

\begin{figure}
    \centering
\includegraphics[width=0.56\linewidth]{figs/pope_abstent_err=0.pdf}
    \caption{Abstention rate with varying coverage using different scoring functions in \textit{scene understanding} (with $\lambda=0$).}
    \label{fig:pope_abstent}
\end{figure}




Now we analyze how accurate \methodName~is in filtering out nonfactual claims.
To our knowledge, there are no existing baselines with statistical factuality guarantees. Thus, we consider a simple \textit{Random Filtering} baseline beyond Vanilla LVLM, which drops claims uniformly at random with probability $\alpha$.
As shown in Table~\ref{tab:pope_res}, for \modelname{LLaVA-1.5} with the setting of $\alpha=0.1, \lambda=0$, \methodName~achieves a high true positive rate (TPR, or Recall) of $0.953$ and a relatively high F1 score of $0.504$. Vanilla LVLM, in contrast, has a TPR of $0$ and F1 of $0$, as it does not perform any filtration. When randomly dropping $10\%$ claims, Random Filtering reaches a TPR of $0.104$ and an F1 of $0.158$, which are approximately $9\times$ and $3.2\times$ lower than those achieved by \methodName. We have similar observations for other LVLMs and these highlight the utility of \methodName~in identifying nonfactual claims. 
It is also evident that other LVLMs with \methodName~implemented demonstrate lower values of TPR and F1 compared to \modelname{LLaVA-1.5}, e.g., \modelname{GPT-4o-mini} achieves a TPR of $0.850$ and an F1 of $0.100$. This occurs because models like \modelname{GPT-4o-mini} typically generate more comprehensive descriptions of an image, leading to a higher number of claims, and, at the same time, a smaller proportion of nonfactual claims
(examples in Appendix).
In other words, the task of identifying nonfactual claims itself is much harder for \modelname{GPT-4o-mini} than for \modelname{LLaVA-1.5}. Nonetheless, \methodName~still outperforms the baseline approaches. 

\begin{table}[t]
    \centering
    \caption{Claim-level results on the \textit{scene understanding} task using \modelname{CLIP-ViT-Large} as scoring function (\methodName~with $\alpha=0.1,\lambda=0$).}
    \vspace{2mm}
    \label{tab:pope_res}
    \resizebox{0.56\linewidth}{!}{
\begin{tabular}{l|c|cc}
\toprule
LVLM                    & \textbf{Configuration}                                                                & \textbf{TPR$\uparrow$}                                    & \textbf{F1$\uparrow$}                    \\ \toprule
\modelname{LLaVA-1.5}               & Vanilla LVLM                                                                               & 0.0                                                        & 0.0                            \\
& \begin{tabular}[c]{@{}c@{}}Random Filtering\end{tabular}                                                                              & 0.104                                                       & 0.158                            \\
                        & \cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}c@{}}\methodName\end{tabular} & \cellcolor[HTML]{EFEFEF}\textbf{0.953} & \cellcolor[HTML]{EFEFEF}\textbf{0.504} \\ \midrule
\begin{tabular}[l]{@{}c@{}}\modelname{Phi-3.5-vision}\end{tabular} & Vanilla LVLM                                                                               & 0.0                                                       & 0.0                            \\
\modelname{-instruct} & \begin{tabular}[c]{@{}c@{}}Random Filtering\end{tabular}                                                                               & 0.102                                                        & 0.145                            \\
                        & \cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}c@{}}\methodName\end{tabular} & \cellcolor[HTML]{EFEFEF}\textbf{0.945}  & \cellcolor[HTML]{EFEFEF}\textbf{0.401} \\ \midrule
\modelname{Llama-3.2-11B-vision}    & Vanilla LVLM                                                                              & 0.0                                                       & 0.0                            \\
& \begin{tabular}[c]{@{}c@{}}Random Filtering\end{tabular}                                                                               & 0.099                                                       & 0.121                            \\
                        & \cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}c@{}}\methodName\end{tabular} & \cellcolor[HTML]{EFEFEF}\textbf{0.936} & \cellcolor[HTML]{EFEFEF}\textbf{0.269} \\ \midrule
\modelname{GPT-4o-mini}             & Vanilla LVLM                                                                               & 0.0                                                   & 0.0                            \\
& \begin{tabular}[c]{@{}c@{}}Random Filtering\end{tabular}                                                                               & 0.106                                                      & 0.070                            \\
                        & \cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}c@{}}\methodName\end{tabular} & \cellcolor[HTML]{EFEFEF}\textbf{0.850} & \cellcolor[HTML]{EFEFEF}\textbf{0.100} \\ \bottomrule
\end{tabular}
    }
\end{table}

\paragraph{Error Tolerance Allows Flexible Control Over Coverage-Utility Tradeoff.}
To study the impact of error tolerance, we plot the ratio of claims filtered and abstention rate using \modelname{Llama-3.2-11B-Vision} with varying $\lambda$ while keeping $\alpha=0.1$ in Fig.~\ref{fig:pope_err_a}. We observe the expected behavior that \methodName~will filter out less content and abstain less as the error tolerance increases.
In Fig.~\ref{fig:pope_err_b}, we plot the model's response distributions with different error tolerances and using \modelname{CLIP-ViT-Large} as the scoring function.
When the error tolerance is set to $\infty$, i.e., using the raw LVLM output without censoring, the model can generate detailed responses ($75\%$ responses have $\geq 10$ claims) but also have high risks of hallucination (more than $50\%$ responses have loss $\geq 3$).
As the error tolerance decreases, the loss of responses gradually reduces with the number of claims. For example, an error tolerance is set to $3$, which reduces more than $75\%$ of responses to below $2$, with the median of the number of claims being $4$.
This shows that error tolerance can serve as an additional tuning knob that allows the user to flexibly choose the desired level of error within the acceptable range of utility. 




\section{Case Study II: Medical Report Generation}
\subsection{Setup}
Next, we evaluate \methodName~on the radiology report generation task. For this evaluation, we use a subset of $500$ chest X-ray images from the MIMIC-CXR~\cite{johnson2019mimic} dataset, each from a distinct patient.

\paragraph{LVLMs.} We consider the following three medical-domain LVLMs for this task:
(1) \modelname{LlaVa-Med}~\cite{li2024llava} is a biomedical LVLM instruction-tuned on several corpora in the biomedicine domain. We use the latest v1.5 \modelname{Mistral} 7B version.
(2) \modelname{CvT2DistilGPT2}~\cite{nicolson2023improving} is LVLM based on the encoder-to-decoder architecture developed for chest X-ray report generation. The originally released model weights are trained on the MIMIC-CXR dataset. To avoid data leakage, we retrain the model on a disjoint subset of MIMIC-CXR that does not contain any patients involved in our evaluation.
(3) \modelname{MAIRA-2}~\cite{bannur2024maira} is the latest radiology-specific LVLM developed by Microsoft Research. It is based on a similar architecture as \modelname{LlaVa}, featuring a \modelname{Rad-DINO} visual encoder and a language model based on \modelname{Vicuna} 7B v1.5 for grounded report generation.

\begin{figure}
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.76\linewidth]{figs/Llama-3.2-11B-vision_vary_err.pdf}
        \caption{Ratio of claims filtered and abstention rate}
        \label{fig:pope_err_a}
    \end{subfigure}

    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.54\linewidth]{figs/pope_impact_err.pdf}
        \caption{Response distributions using \modelname{CLIP-ViT-Large} as scoring function}
        \label{fig:pope_err_b}
    \end{subfigure}
    \caption{Comparison of \modelname{Llama-3.2-11B-Vision}'s response with different error tolerances ($\lambda$) while fixing $\alpha=0.1$ in \textit{scene understanding}.}
    \label{fig:pope_err}
\end{figure}



\paragraph{Error Annotation and Loss Function.}
We leverage \modelname{GPT-4o} for error annotation. To ensure quality, we provide \modelname{GPT-4o} with the ground truth report written by qualified physicians in addition to the chest X-ray images. This eliminates the requirement for \modelname{GPT-4o} to understand the actual medical image as it can verify the veracity of each claim by just checking if it is entailed by the ground truth report.
Specifically, the errors are categorized as:
(1) \textit{Conflicting error}: The claim directly contradicts information provided in the ground truth report;
(2) \textit{Implausible error}: The claim does not directly conflict with or align with the ground truth report, and is implausible within the given context;
and (3) \textit{Plausible error}: The claim does not directly conflict with or align with the ground truth report, but remains plausible within the given context.
Similar to scene understanding, we assign each occurrence of errors (1)-(3) a loss of $3$, $2$, and $1$, respectively, and compute the accumulated loss as the final loss for each response.

\paragraph{Scoring Function.}
We use \modelname{BiomedCLIP}~\cite{zhang2023biomedclip} to compute the similarity between claim text and image pairs as the external confidence score.

\begin{figure}
    \centering
\includegraphics[width=0.8\linewidth]{figs/maira-2_merged.pdf}
    \caption{\modelname{MAIRA-2} results in \textit{medical report generation}.}
    \label{fig:mimic_merged}
\end{figure}

\subsection{Results}
Given limited space, we present the results of \modelname{MAIRA-2} and defer the results of other LVLMs to the Appendix.

We plot the empirical coverage versus desired coverage in Fig.~\ref{fig:mimic_merged}a.
The results verify that \methodName~can achieve tight error control in the revised responses after filtering. This is particularly important considering that medical report generation is a much more challenging task that requires precise control over the risk of output hallucination.

We plot the average ratio of claims filtered and abstention rate at various desired levels of coverage in Fig.~\ref{fig:mimic_merged}b and Fig.~\ref{fig:mimic_merged}c, respectively, given fixed $\lambda=0$.
We observe a trend similar to the scene understanding task, where the ratio of filtered claims and abstention rate increases with the desired coverage level. Notably, given a fixed coverage level, \modelname{BiomedCLIP} archives the lowest ratio of filtered claims and abstention rate among all scoring functions.
Fig.~\ref{fig:mimic_merged}d and Fig.~\ref{fig:mimic_merged}e present the results with varying error tolerance while keeping $\alpha=0.1$. In particular, changing $\lambda$ from $0$ to $3$ reduces the abstention rate by more than half, while still maintaining a relatively low error (compared to the median of the loss of unfiltered responses, which is $7$).







\begin{figure}
    \centering
\includegraphics[width=0.8\linewidth]{figs/llava-1.6_merged.pdf}
    \caption{\modelname{LLaVA-NeXT} results in \textit{document understanding}.}
    \label{fig:sroie_merged}
\end{figure}

\section{Case Study III: Document Understanding}
\subsection{Setup}
Finally, we evaluate \methodName~on the document understanding task, where we randomly select $500$ invoice scan/images from the SROIE~\cite{huang2019icdar2019} dataset.

\paragraph{LVLMs.}
We consider two LVLMs, \modelname{LLaVA-Next}~\cite{liu2024llavanext}, which is the latest model in the \modelname{LLaVA} family with enhanced visual reasoning and OCR capabilities, and \modelname{Phi-3.5-vision-instruct}.



\paragraph{Error Annotation and Loss Function.}
We consider the following error types for this task:
(1) \textit{Field misinterpretation}: Incorrectly identify important fields such as mistaking "Subtotal" for "Total Amount", or misrecognizing non-existing fields.
(2) \textit{Numerical and quantitative errors}: Incorrect amounts, totals, or quantity values, as well as calculation discrepancies (e.g., subtotal, tax, and total relationship).
(3) \textit{Date error}: Misrecongizing date or misinterpreting date formats.
(4) \textit{Item error}: Misrecongizing item or item details, or falsely identifying non-existing items.
(5) \textit{Other errors}: Other errors such as misspelling or misrecognizing character, layout, and alignment issues.
Each occurrence of Numerical and Date Errors gets an additional loss of $3$, each occurrence of field or item error gets an additional loss of $2$, and the occurrence of other errors gets an additional loss of $1$. Similar to other tasks, we compute the accumulated loss for each response.

\paragraph{Scoring Function.}
We use \modelname{LayoutLMv3}~\cite{huang2022layoutlmv3} which is a pre-trained multi-modal transformer to derive embeddings for the claim text and document images and compute their cosine similarity as the external confidence score.

\subsection{Results}
We now show the results of \modelname{LLaVA-NeXT} and defer the results of \modelname{Phi-3.5-vision-instruct} to the Appendix.


We first verify the alignment of empirical coverage and desired coverage in Fig.~\ref{fig:sroie_merged}a. The results show that \methodName~can achieve the precise coverage on the document understanding task as desired.



We investigate the ratio of filtered claims and abstention rate with varying $\alpha$ and fixed $\lambda=1$ in Fig.~\ref{fig:sroie_merged}b and Fig.~\ref{fig:sroie_merged}c, respectively, and with varying $\lambda$ and fixed $\alpha=0.1$ in Fig.~\ref{fig:sroie_merged}d and Fig.~\ref{fig:sroie_merged}e, respectively. Besides the general trend that increasing desired coverage or reducing the error tolerance would result in filtering out more content and more frequently abstaining, we additionally observe that \modelname{LayoutLMv3} achieves significantly lower rates compared to other scoring functions based on the LVLM's internal confidence, e.g., preserving approximately $10\%$ more content when $\alpha=0.4$. This shows that a small dedicated model is more accurate than the prominent LVLMs in terms of verifying the factuality of claims regarding document images.
