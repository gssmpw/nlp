\section{Introduction}
\label{sec:intro}
Large Vision-Language Models (LVLMs) which  combine Large Language Models (LLMs) with computer vision modules, have demonstrated 
remarkable multi-modal abilities~\cite{liu2024visual, abdin2024phi,metaLlama32,openaigpt4omini}. LVLMs are designed to receive both free text and visual content as inputs (e.g., images or videos) and generate text responses to user queries about the visual input, thus enabling a flexible conversational interface for numerous visual perception and multi-modal comprehension tasks. This capability has triggered successful developments across 
various application domains, including cross-modality agents specializing in graphical user interface understanding and planning~\cite{hong2024cogagent}, visual-language foundation models for analyzing pathology slides~\cite{lu2024visual}, and autonomous driving assistants offering real-time reasoning and decision-making~\cite{wen2023road}.
Despite the excitement and new opportunities LVLMs offer, 
they can
produce \textit{hallucinations}~\cite{bai2024hallucination}â€”text outputs that contain erroneous, or simply fabricated, assertions that deviate from the visual content.  This can significantly limit their adoption in
safety-critical fields, such as healthcare and autonomous driving.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/VLM_overview_v4.pdf}
    \caption{\textbf{Overview}: given user-specified error tolerance $\lambda$, error rate $\alpha$, and a calibration dataset, \methodName~returns a more reliable response for any new image and prompt at inference time through \textit{sampling}, \textit{decomposing} $D$, \textit{filtering} $F$, and \textit{merging} $M$, to ensure that the risk of the final response $Y^*$ is controlled with high probability. Illustrative examples, one for each application domain, are provided for outcome demonstration, where claims are highlighted to indicate \methodName's confidence using specific conformity score and error tolerance level. Unhighlighted claims correspond to low confidence in factuality check.}
    \label{fig:overview}
\end{figure*}

\paragraph{Related Work.}
To date,
the research on LVLM hallucinations has generally
focused on two
threads.
One line of work aims to build benchmarks~\cite{li2023evaluating,yin2023survey,lovenia2023negative,sun2023aligning,kaul2024throne,guan2024hallusionbench,jing2023faithscore} and metrics~\cite{rohrbach2018object,li2023evaluating,jing2023faithscore} for assessing and analyzing hallucinations in popular LVLMs.  Most studies in this area
concentrate on object hallucination in discriminative question-answering tasks~\cite{li2023evaluating,yin2023survey,lovenia2023negative},
with a lesser focus
on other types of hallucination in free-text generation tasks~\cite{jing2023faithscore,kaul2024throne,guan2024hallusionbench}.
The other line of work aims to develop strategies to mitigate LVLM hallucinations.
Notable
strategies in this line of investigation include refining the training and instruction tuning phases by optimizing the loss function~\cite{jiang2024hallucination} or alignment objective~\cite{zhao2023beyond,gunjal2024detecting,sun2023aligning}, as well as enhancing the inference phase by designing new decoding algorithms~\cite{leng2024mitigating,favero2024multi,deng2024seeing}.
However, these solutions are often resource-heavy and lack flexibility,
typically requiring model retraining or white-box access during decoding. By contrast, \emph{post hoc} correction methods offer a more flexible approach,  which improves responses from any black-box LVLMs through the assistance of external language or vision modules~\cite{yin2023woodpecker}, dedicated revisor model~\cite{zhou2023analyzing}, or self-revision techniques~\cite{lee2023volcano}. However, the current collection of methods relies solely on heuristics and lacks rigorous statistical guarantees of factuality for the revised output, a necessity in mission-critical application domains.



\paragraph{Our Contribution.}
To address these challenges, in this study, we introduce \methodName, a framework with statistical guarantees on output factuality (alignment of text response with visual context) that seamlessly integrates with LVLMs of any architecture, complexity, and purpose.
\methodName~treats LVLM-generated free text as a series of individual claims, each corresponding to a testable hypothesis. Each claim is then evaluated for its factuality using discriminative mechanisms built from the same (or auxiliary) LVLM to filter out unsupported claims and retain those that meet factual standards. To provide statistical guarantees on the factuality of retained claims, \methodName~employs a conformal prediction framework that allows control for flexible error rates, as well as error tolerance levels, defined by users
to suit specific application needs. By leveraging the generally more stable discriminative capabilities of LVLMs through calculating and ranking predefined conformity scores, \methodName~enables quantitative factual assessments. We demonstrate the effectiveness of \methodName~across three representative application domains, i.e., general scene understanding, medical radiology report generation, and document understanding,
to validate its effectiveness in ensuring a desired level of response factuality for various state-of-the-art LVLMs and explore multiple potentially useful conformity scores. Our results, which cover over $81,000$ claims generated from eight popular LVLMs, establish \methodName~as a general-purpose framework that operates with any LVLM in an assumption- and finetuning-free manner, thus promoting trustworthiness in LVLM applications broadly. Moreover, this work opens up a novel research space, where each component of \methodName~invites further innovation to improve the rigorous guarantee of hallucination mitigation for multi-modal models.








