\section{Preliminaries}
\label{sec:prelim}

\subsection{Large Vision-Language Models}




Large Vision-Language Models (LVLM) are generative models that are typically composed of a visual model $h(\cdot)$, a language model parameterized by $\vtheta$, and a fusion model $g(\cdot)$.
The most popular implementations of LVLMs, such as Llava~\cite{liu2024visual}, combine a pre-trained visual encoder (e.g., CLIP~\cite{radford2021learning}) and a pre-trained Large Language Model (LLM) (e.g., Vicuna~\cite{chiang2023vicuna}) by training a projection network as the fusion model to convert extracted visual features into the LLM's embedding space in a process known as visual instruction tuning.
During inference, an LVLM takes an input image $\mI$ and a text prompt $\mX=[\rx_1, ..., \rx_l]$, and outputs a text response $\mY=[\ry_1, ..., \ry_m]$, where $\rx_i$ and $\ry_j$ are individual tokens. This is achieved by first converting the image into a sequence of visual tokens using the visual model $\mV=[\rv_1, ..., \rv_k]=g\circ h(\mI)$ and then sampling the response from the conditional distribution in an autoregressive manner:
$p_\vtheta(\mY|\mX,\mV) = \prod_{j=1}^m p_\vtheta(\ry_j|\mX,\mV,\mY_{<j})$.

\paragraph{Hallucination of LVLMs.}
The problem of hallucination originates from the space of language models, where the generated text response is either non-factual (conflicts with verifiable facts) or unfaithful (does not follow the user's instructions). In the context of LVLMs, hallucination refers to the phenomenon where the generated text response deviates from the provided visual content. Common types of LVLM hallucinations include \textit{object} hallucination (e.g., falsely identifying non-existent objects), \textit{attribute} hallucination (e.g., wrong color, shape, or material),
and \textit{relation} hallucination (e.g., human-object interaction, relative position)~\cite{bai2024hallucination}.


\subsection{Split Conformal Prediction}



Split conformal prediction (SCP)~\cite{vovk2005algorithmic,shafer2008tutorial} is a distribution-free method for quantifying the uncertainty of black-box prediction algorithms by constructing prediction sets with finite-sample coverage properties. 

\paragraph{Coverage Guarantee.}
For a black-box prediction function $f: \gX \rightarrow \gY$, let $\{(X_i, Y_i)\}_{i=1}^{n+1}$ be an exchangeable set of feature and label pairs sampled from the joint distribution on $\gX\times\gY$. The goal of split conformal prediction is to use the calibration data $\{(X_i, Y_i)\}_{i=1}^{n}$ and $f$ to construct a prediction set $\hat{C}: \gX \rightarrow 2^\gY$ for the new data point such that it achieves valid \textit{coverage}, i.e., containing the true label with high probability
$\prob\big(Y_{n+1}\in \hat{C}(X_{n+1})\big) \geq 1-\alpha$
for any user-specified error rate $\alpha \in (0, 1)$.

\paragraph{Conformal Calibration.}
Suppose there is a \textit{conformity score} function $S(X, Y)\in \sR$ that measures how well a given sample \textit{conforms} to the observed data.
The split conformal procedure uses the calibration data set $\{(X_i, Y_i)\}_{i=1}^{n}$ to derive \textit{conformity} scores $\{S(X_i, Y_i) \}_{i=1}^n$, where a larger value indicates the model is more confident about the prediction being true. To calibrate the prediction set to the desired level of coverage, we then compute a threshold $\hat{\tau}$
that is approximately the $1-\alpha$ quantile of the conformity scores.
At the time of inference, given a new data point $X_{n+1}$, we construct the prediction set as $\hat{C}(X_{n+1}) = \{y\in\gY: S(X, y) \geq \hat{\tau} \}$. If the data are exchangeable, then this prediction set will satisfy the desired coverage property.
