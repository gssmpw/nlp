\section{Discussion}

\paragraph{Impact of Calibration Data Size.}
We have considered a fixed calibration data size of $400$.
To study the impact of calibration data size, we use \modelname{Llama-3.2-11B-Vision} as an example and vary the calibration data from $50$ to $400$ samples, each with $50$ random train-test splits, and plot the empirical coverage and ratio of claims filtered of three sets of $(\alpha, \lambda)$ parameters in Fig.~\ref{fig:pope_cali}.
We observe that \methodName~can consistently achieve the desired level of coverage while maintaining the same ratio of filtered claims regardless of the calibration data size, though a larger calibration dataset could help reduce the result variance.

\begin{figure}
    \centering
\includegraphics[width=0.8\linewidth]{figs/pope_calibration_size.pdf}
    \caption{Impact of calibration data size on the empirical coverage and utility of \modelname{Llama-3.2-11B-Vision} on the \textit{scene understanding} task (with $95\%$ CI).}
    \label{fig:pope_cali}
\end{figure}

\paragraph{Discriminative vs. Generative Models.}
In our experiments, we observe that in many cases small discriminative vision-language models (e.g., \modelname{BiomedCLIP} for medical report generation and \modelname{LayoutLMv3} for document understanding) outperform LVLMs in terms of capturing the relevance between a text claim and an image. One potential reason is that compared to large generative models, small discriminative models are easier to optimize and can learn useful representations more efficiently. This hints that besides serving as the image encoder, these models can be used as critics to censor LVLM outputs and improve factuality with lower computational costs.

\paragraph{Coverage vs. Reliability.}
Although our method can achieve the precise coverage as specified by the user, the statistical guarantee only holds marginally with split conformal prediction. However, conditional guarantees may be required for certain applications, e.g., to ensure health equity among groups of patients in healthcare. Future work could consider the integration with advanced conformal methods to achieve conditional validity~\cite{gibbs2023conformal}.
Besides coverage, investigating other important aspects of LVLM reliability, such as omission, and designing better scoring functions to achieve the same level of coverage while preserving more content are also interesting avenues for future research.



\section{Conclusion}
In this work, we propose \methodName, a framework for achieving statistical factuality guarantee of LVLM output through decomposing responses into individual verifiable hypotheses and filtering out those with low confidence given the image content. We demonstrate with three application domains that by choosing the desired error rate and tolerance, \methodName~offers users flexible control over the hallucination risk of LVLM output.
