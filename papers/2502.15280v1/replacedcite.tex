\section{Related Work}
\subsection{Regularization in Deep Reinforcement Learning}

Deep RL is particularly susceptible to overfitting due to its inherently non-stationary optimization process ____. To address overfitting, researchers have adapted regularization techniques from SL, including weight decay ____, dropout ____, and various normalization layers ____. However, these methods often prove insufficient when scaling RL models, as larger computational resources and increased model sizes can easily exacerbate overfitting ____.

To further scale computations and model sizes in RL, recent studies have explored periodic weight reinitialization strategies in RL to rejuvenate learning and escape local minima ____. 
These strategies include reinitializing weights to their initial distributions ____, interpolating between random and current weights ____, utilizing momentum networks ____, and selectively reinitializing dormant weights ____. 
While promising, reinitialization has a notable limitation: it can lead to the loss of useful information and incur significant computational overhead as model size increases.

To address these limitations, we introduce SimbaV2, an architecture that explicitly constrains parameter, feature, and gradient norms throughout training. By constraining norms through hyperspherical normalization, SimbaV2 stabilizes an optimization process and eliminates the need for weight decay or periodic weight reinitialization.


\subsection{Hyperspherical Representations in Deep Learning}

Hyperspherical representations are widely used in deep learning across image classification ____, face recognition ____, variational autoencoders ____, and contrastive learning ____. Using spherical embeddings is known to enhance feature separability ____, improving performance in tasks requiring precise discrimination. Recently, researchers have applied the hyperspherical normalization to intermediate features and weights to stabilize training in large-scale models such as diffusion models ____ and transformers ____.

In this work, we apply hyperspherical normalization to RL. Unlike previous studies that focus on training the network on stationary data distributions with discrete inputs and outputs, we demonstrate their effectiveness on non-stationary data distributions with continuous inputs and outputs.

% \begin{figure*}[!ht]
% \begin{center}
% \includegraphics[width=17cm,height=4cm]{example-image-duck}
% \end{center}
% \caption{An abnormally wide duck. Nature is truly amazing.}
% \label{fig:architecture}
% \end{figure*}