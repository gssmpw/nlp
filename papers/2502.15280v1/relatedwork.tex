\section{Related Work}
\subsection{Regularization in Deep Reinforcement Learning}

Deep RL is particularly susceptible to overfitting due to its inherently non-stationary optimization process \citep{song2019observational}. To address overfitting, researchers have adapted regularization techniques from SL, including weight decay \citep{farebrother2018l2rl}, dropout \citep{hiraoka2021dropout}, and various normalization layers \citep{gogianu2021spectral, bjorck2021spectralnormRL, lyle2023understanding_plasticity, gallici2024simplifying_td, bhatt2024crossq, lee2024simba, elsayed2024streaming, palenicek2025scaling}. However, these methods often prove insufficient when scaling RL models, as larger computational resources and increased model sizes can easily exacerbate overfitting \citep{li2023efficient, nauman2024overestimation}.

To further scale computations and model sizes in RL, recent studies have explored periodic weight reinitialization strategies in RL to rejuvenate learning and escape local minima \citep{d2023sample_breaking, nauman2024bro}. 
These strategies include reinitializing weights to their initial distributions \citep{nikishin2022primacy}, interpolating between random and current weights \citep{xu2023drm, schwarzer2023bbf}, utilizing momentum networks \citep{lee2024slow}, and selectively reinitializing dormant weights \citep{sokar2023dormant}. 
While promising, reinitialization has a notable limitation: it can lead to the loss of useful information and incur significant computational overhead as model size increases.

To address these limitations, we introduce SimbaV2, an architecture that explicitly constrains parameter, feature, and gradient norms throughout training. By constraining norms through hyperspherical normalization, SimbaV2 stabilizes an optimization process and eliminates the need for weight decay or periodic weight reinitialization.


\subsection{Hyperspherical Representations in Deep Learning}

Hyperspherical representations are widely used in deep learning across image classification \citep{salimans2016weight_norm, liu2017hyper_conv}, face recognition \citep{wang2017normface, liu2017sphereface}, variational autoencoders \citep{xu2018spherical_vae}, and contrastive learning \citep{chen2020simclr}. Using spherical embeddings is known to enhance feature separability \citep{wang2020understanding}, improving performance in tasks requiring precise discrimination. Recently, researchers have applied the hyperspherical normalization to intermediate features and weights to stabilize training in large-scale models such as diffusion models \citep{karras2024analyzing_diffusion} and transformers \citep{loshchilov2024ngpt}.

In this work, we apply hyperspherical normalization to RL. Unlike previous studies that focus on training the network on stationary data distributions with discrete inputs and outputs, we demonstrate their effectiveness on non-stationary data distributions with continuous inputs and outputs.

% \begin{figure*}[!ht]
% \begin{center}
% \includegraphics[width=17cm,height=4cm]{example-image-duck}
% \end{center}
% \caption{An abnormally wide duck. Nature is truly amazing.}
% \label{fig:architecture}
% \end{figure*}