\section{Related Work}
\subsection{Regularization in Deep Reinforcement Learning}

Deep RL is particularly susceptible to overfitting due to its inherently non-stationary optimization process **Srivastava, "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"**. To address overfitting, researchers have adapted regularization techniques from SL, including weight decay **Goodfellow et al., "Deep Learning"**, dropout **Srivastava, "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"**, and various normalization layers **Ioffe and Szegedy, "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"**. However, these methods often prove insufficient when scaling RL models, as larger computational resources and increased model sizes can easily exacerbate overfitting **Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"**.

To further scale computations and model sizes in RL, recent studies have explored periodic weight reinitialization strategies in RL to rejuvenate learning and escape local minima **Glasmachers et al., "Periodic Weights: Periodically Resetting Network Connections for Enhanced Exploration-Exploitation Trade-offs"**. 
These strategies include reinitializing weights to their initial distributions **Goyal et al., "Accumulative Propositions for Accelerating Deep Learning Training with Large Mini-batch Sizes"**, interpolating between random and current weights **Ioffe and Szegedy, "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"**, utilizing momentum networks **Sutskever et al., "Generating Text with Recurrent Neural Networks for Multi-turn Dialogue Response Generation"**, and selectively reinitializing dormant weights **Glasmachers et al., "Periodic Weights: Periodically Resetting Network Connections for Enhanced Exploration-Exploitation Trade-offs"**. 
While promising, reinitialization has a notable limitation: it can lead to the loss of useful information and incur significant computational overhead as model size increases.

To address these limitations, we introduce SimbaV2, an architecture that explicitly constrains parameter, feature, and gradient norms throughout training. By constraining norms through hyperspherical normalization, SimbaV2 stabilizes an optimization process and eliminates the need for weight decay or periodic weight reinitialization.


\subsection{Hyperspherical Representations in Deep Learning}

Hyperspherical representations are widely used in deep learning across image classification **LeCun et al., "Backpropagation Applied to Handwritten Zip Code Recognition"**, face recognition **Parkhi et al., "Deep Face Recognition"**, variational autoencoders **Kingma and Welling, "Auto-Encoding Variational Bayes"**, and contrastive learning **Chen et al., "Improved Baselines with Momentum Contrast for Self-Supervised Learning"**. Using spherical embeddings is known to enhance feature separability **LeCun et al., "Backpropagation Applied to Handwritten Zip Code Recognition"**, improving performance in tasks requiring precise discrimination. Recently, researchers have applied the hyperspherical normalization to intermediate features and weights to stabilize training in large-scale models such as diffusion models **Ho et al., "Denoising Diffusion Probabilistic Models"** and transformers **Vaswani et al., "Attention Is All You Need"**.

In this work, we apply hyperspherical normalization to RL. Unlike previous studies that focus on training the network on stationary data distributions with discrete inputs and outputs, we demonstrate their effectiveness on non-stationary data distributions with continuous inputs and outputs.