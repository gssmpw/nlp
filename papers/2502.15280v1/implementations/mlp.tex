\begin{lstlisting}[
    language=Python,
    caption={A JAX implementation of MLP block (Section~\ref{subsection:feature_encoding}).},
    label={implementation:mlp}
]
import flax.linen as nn

class SimbaV2Block(nn.Module):
    hidden_dim: int
    ffn_scaler_init: float
    ffn_scaler_scale: float
    alpha_scaler_init: float
    alpha_scaler_scale: float

    def setup(self):
        self.w1 = nn.Dense(
            features=4*self.hidden_dim,
            use_bias=False
        )(x)
        self.mlp_scaler = Scaler(
            dim=4*self.hidden_dim,
            init=ffn_scaler_init,
            scale=ffn_scaler_scale
        )
        self.w2 = nn.Dense(
            features=self.hidden_dim,
            use_bias=False
        )
        self.alpha = Scaler(
            dim=self.hidden_dim,
            init=alpha_scaler_init,
            scale=alpha_scaler_scale
        )

    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
        residual = x
        # MLP + l2-Norm
        x = self.w1(x)
        x = self.mlp_scaler(x)
        x = nn.relu(x)
        x = self.w2(x)
        x = l2normalize(x, axis=-1)
        # LERP + l2-Norm
        x = l2normalize(residual + self.alpha(x - residual), axis=-1)
        return x
\end{lstlisting}