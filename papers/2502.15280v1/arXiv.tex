 %%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% My packages
\usepackage{lipsum}
\usepackage{placeins}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{array}
\usepackage{tabularx}
\usepackage{bm}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
%\usepackage{hyperref}
\usepackage{appendix}
\usepackage{titletoc}

\usepackage[pagebackref=true]{hyperref} 
\renewcommand*\backref[1]{\ifx#1\relax \else (Cited on page #1) \fi}

% Mine
\usepackage{definition}

%\usepackage[dvipsnames]{xcolor}
%\usepackage[table]{xcolor}
\usepackage[table, dvipsnames]{xcolor}
\definecolor{ACMBlue}{cmyk}{1,0.1,0,0.1}
\definecolor{ACMYellow}{cmyk}{0,0.16,1,0}
\definecolor{ACMOrange}{cmyk}{0,0.42,1,0.01}
\definecolor{ACMRed}{cmyk}{0,0.90,0.86,0}
\definecolor{ACMLightBlue}{cmyk}{0.49,0.01,0,0}
\definecolor{ACMGreen}{cmyk}{0.20,0,1,0.19}
\definecolor{ACMPurple}{cmyk}{0.55,1,0,0.15}
\definecolor{ACMDarkBlue}{cmyk}{1,0.58,0,0.21}
\hypersetup{colorlinks,
  linkcolor=ACMDarkBlue,  %   ACMPurple,
  citecolor=ACMDarkBlue,  % ACMPurple,
  urlcolor=ACMDarkBlue,   % black,
  filecolor=ACMDarkBlue}

\usepackage{tikzducks}
\definecolor{Simbav2_dark_color}{RGB}{80, 80, 184}
\definecolor{Simba_color}{RGB}{232, 159, 0}
\definecolor{Simba_dark_color}{RGB}{170, 108, 0}
\definecolor{SimBa_light_color}{RGB}{255, 235, 190}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:

\begin{document}

\icmltitlerunning{SimbaV2}

\twocolumn[
\icmltitle{Hyperspherical Normalization for Scalable Deep Reinforcement Learning} 

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hojoon Lee}{KAIST,equal}
\icmlauthor{Youngdo Lee}{KAIST,equal}
\icmlauthor{Takuma Seno}{Sony AI}
\icmlauthor{Donghu Kim}{KAIST}
\icmlauthor{Peter Stone}{Sony AI,UT Austin}
\icmlauthor{Jaegul Choo}{KAIST}
\end{icmlauthorlist}

\icmlaffiliation{KAIST}{KAIST}
\icmlaffiliation{Sony AI}{Sony AI}
\icmlaffiliation{UT Austin}{UT Austin}

\icmlcorrespondingauthor{Hojoon Lee}{joonleesky@kaist.ac.kr}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

% Fig1 color scheme
% Simba: #F5C76A
% SimbaV2 (or HyperSimba): #648EF6

\vskip 0.3in
]



% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNoticeArXiv{\icmlEqualContribution}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Scaling up the model size and computation has brought consistent performance improvements in supervised learning. 
However, this lesson often fails to apply to reinforcement learning (RL) because training the model on non-stationary data easily leads to overfitting and unstable optimization.
In response, we introduce SimbaV2, a novel RL architecture designed to stabilize optimization by (i) constraining the growth of weight and feature norm by \textit{hyperspherical normalization}; and (ii) using a distributional value estimation with reward scaling to maintain stable gradients under varying reward magnitudes. Using the soft actor-critic as a base algorithm, SimbaV2 scales up effectively with larger models and greater compute, achieving state-of-the-art performance on 57 continuous control tasks across 4 domains. The code is available at \href{https://dojeon-ai.github.io/SimbaV2/}{\textit{dojeon-ai.github.io/SimbaV2}}.
%\vspace{-7mm}
%\vspace{-3mm}
\end{abstract}

\vspace{-3mm}
\section{Introduction}
\label{submission}

Over the past decade, a scaling law has emerged as a cornerstone in deep learning, suggesting that increasing the model size, compute, and data consistently enhance performance~\cite{kaplan2020scaling, dehghani2023scaling}. This paradigm has driven significant breakthroughs in supervised learning (SL), including large language models \cite{team2023gemini, achiam2023gpt4} and diffusion models \cite{ramesh2021clip, rombach2022stablediffusion}.

In contrast, scaling laws often fail to apply to reinforcement learning (RL) \cite{song2019observational, li2023efficient}. Unlike SL where data distributions remain static, RL agents are trained under evolving data distributions and shifting objectives \cite{sutton2018reinforcement}. This inherent non-stationarity impedes scaling efforts, as increasing the model capacity or computational resources causes the model to easily overfit to earlier data and reduces the agent’s adaptability to new tasks \cite{lyle2022capacity, dohare2023maintaining}.

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.42\textwidth]
{figures/sec1_compute_vs_performance.pdf}
\end{center}
\vspace{-4mm}
\caption{\textbf{Compute vs RL Performance.} 
Soft Actor-Critic with SimbaV2 outperforms other RL algorithms, where performance scales as compute increases. The grey numbers below each dot indicate the update-to-data (UTD) ratio. SimbaV2, with UTD = 1, achieves a performance of 0.848, surpassing TD-MPC2 (0.749), the most computationally intensive version of Simba (0.818), and BRO (0.807). The results show normalized returns, averaged over $57$ continuous control tasks from MuJoCo, DMC, MyoSuite, and HumanoidBench, each trained on 1 million samples.}
\vspace{-2mm}
\label{figure:compute_vs_performance}
\end{figure}




\begin{figure*}[ht!]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/benchmark.pdf}
\end{center}
\vspace{-3mm}
\caption{\textbf{Benchmark Summary.} \textbf{(a)} SimbaV2, with an update-to-data (UTD) ratio of 2, outperforms state-of-the-art RL algorithms across diverse continuous control benchmarks using fixed hyperparameters across all domains. \textbf{(b)} SimbaV2 delivers competitive performance in both online and offline RL while requiring significantly less training computation and offering faster inference times.}
\vspace{-1mm}
\label{figure:benchmark_summary}
\end{figure*}

To address these challenges, the RL community has explored regularization techniques from SL. 
Methods such as weight decay \cite{farebrother2018l2rl} and dropout \cite{hiraoka2021dropout} have been applied to reduce overfitting and improve generalization. 
Also, normalization techniques, such as layer normalization \cite{lei2016layer_norm, lyle2023understanding_plasticity} and RL-specific variants \cite{bhatt2024crossq, lee2024simba}, have been employed to stabilize training by constraining the feature norm. 
More recently, periodic weight re-initialization \cite{nikishin2022primacy, d2023sample_breaking, schwarzer2023bbf} has emerged as a strategy to address the instability in non-stationary optimization processes.

Despite their effectiveness, these approaches have notable limitations. Tuning regularizers is particularly challenging in RL’s dynamic settings; overly restrictive constraints impede training, while insufficient constraints fail to prevent overfitting \cite{henderson2018deeprlmatter, eimer2023hyperparameters_rl}.
Moreover, normalization layers do not strictly regularize the growth of the parameter norm, resulting in varying effective learning rates (i.e., the gradient-to-parameter norm ratio) across layers and potentially destabilizing training \cite{lyle2024normalization}. Finally, periodic weight reinitialization becomes computationally prohibitive for large models or complex tasks \cite{sokar2023dormant, lee2024slow}.

In response, we present SimbaV2, a novel RL architecture designed to stabilize non-stationary optimization by constraining weight, feature, and gradient norms. Building on the Simba architecture \cite{lee2024simba}, which uses pre-layernorm residual blocks \cite{xiong2020pre_ln} and weight decay \cite{krogh1991simple_l2} to control the weight and feature norm growth, SimbaV2 introduces two modifications:
\begin{itemize}[leftmargin=*, topsep=1pt, itemsep=0pt]
\item \textbf{Hyperspherical Normalization:} We replace all layer normalization with hyperspherical normalization (i.e.,$\ell_2$-normalization) and project weights onto the unit-norm hypersphere after each gradient update. These changes ensure consistent effective learning rates across layers and eliminate the need for tuning weight regularization.
\item \textbf{Distributional Value Estimation with Reward Scaling:} To address unstable gradient norms from varying reward scales and outliers, we integrate a distributional critic \cite{bellemare2017distributional} and apply reward scaling to ensure the unit variance of target values for both actor and critic.
\end{itemize}

Using soft actor-critic \cite{haarnoja2018sac} as a base, SimbaV2 stabilizes weight, feature, gradient norms, along with the effective learning rate throughout training (Section~\ref{section:empirical_analysis} and Figure~\ref{figure:analysis}).
We evaluate SimbaV2 on four standard online RL benchmarks, MuJoCo \cite{todorov2012mujoco}, DMC Suite \cite{tassa2018dmc}, MyoSuite \cite{caggiano2022myosuite}, and HumanoidBench \cite{sferrazza2024humanoidbench}, as well as D4RL MuJoCo benchmark \cite{fu2020d4rl} for offline RL.
As indicated by results in Figures \ref{figure:compute_vs_performance} and \ref{figure:benchmark_summary}, SimbaV2 achieves state-of-the-art performance without algorithmic modifications or hyperparameter tuning.
SimbaV2 also scales effectively with increased model size and computation, avoiding overfitting even in the absence of periodic reinitialization. We will release our code in the camera-ready version.


%We introduce a novel network architecture that enforces both features and weights to reside on a unit hypersphere, ensuring consistent effective learning rates and removing the need for normalization and regularization. Additionally, we stabilize training by normalizing value targets to unit variance and employing a distributional loss to smooth gradient updates, mitigating instability from varying reward scales and outliers.



%Deep reinforcement learning (RL) has recently shown human-level successes on various areas, including games (OpenAI Five, Gran Turismo), industries (semiconductor: https://ieeexplore.ieee.org/document/8373191/), and embodied AI..? Similarly to humans that are designed to learn through experience (ref: experiential learning), the deep RL agents also interact with environment to gather experience so as to learn the environment properties as well as the long-term effect of different action choices. However, they generally necessitate a large number of interactions -- for example, sth requires 1000M steps. Enhancing the sample efficiency of agents, therefore, has been a long-standing challenge in deep RL. 

%There are mainly three problems to be addressed to achieve human-level sample efficiency: (1) overfitting (2) overestimation (3) loss of plasiticity. 

%Prior works propose a plethora of approaches in order to mitigate these issues, such as scaling replay ratio by resetting (ref: SR-SPR), random ensemble of Q-networks (ref: REDQ, DroQ, CrossQ), and regularization (ref: A-LIX). These methods are not generally applicable or impose additional computational costs. 

%We propose \textbf{SimbaV2}. blahblah With maximal settings, our agents achieve a new state-of-the-art performance in state-based continuous controls. 


\section{Related Work}

\subsection{Regularization in Deep Reinforcement Learning}

Deep RL is particularly susceptible to overfitting due to its inherently non-stationary optimization process \citep{song2019observational}. To address overfitting, researchers have adapted regularization techniques from SL, including weight decay \citep{farebrother2018l2rl}, dropout \citep{hiraoka2021dropout}, and various normalization layers \citep{gogianu2021spectral, bjorck2021spectralnormRL, lyle2023understanding_plasticity, gallici2024simplifying_td, bhatt2024crossq, lee2024simba, elsayed2024streaming, palenicek2025scaling}. However, these methods often prove insufficient when scaling RL models, as larger computational resources and increased model sizes can easily exacerbate overfitting \citep{li2023efficient, nauman2024overestimation}.

To further scale computations and model sizes in RL, recent studies have explored periodic weight reinitialization strategies in RL to rejuvenate learning and escape local minima \citep{d2023sample_breaking, nauman2024bro}. 
These strategies include reinitializing weights to their initial distributions \citep{nikishin2022primacy}, interpolating between random and current weights \citep{xu2023drm, schwarzer2023bbf}, utilizing momentum networks \citep{lee2024slow}, and selectively reinitializing dormant weights \citep{sokar2023dormant}. 
While promising, reinitialization has a notable limitation: it can lead to the loss of useful information and incur significant computational overhead as model size increases.

To address these limitations, we introduce SimbaV2, an architecture that explicitly constrains parameter, feature, and gradient norms throughout training. By constraining norms through hyperspherical normalization, SimbaV2 stabilizes an optimization process and eliminates the need for weight decay or periodic weight reinitialization.


\subsection{Hyperspherical Representations in Deep Learning}

Hyperspherical representations are widely used in deep learning across image classification \citep{salimans2016weight_norm, liu2017hyper_conv}, face recognition \citep{wang2017normface, liu2017sphereface}, variational autoencoders \citep{xu2018spherical_vae}, and contrastive learning \citep{chen2020simclr}. Using spherical embeddings is known to enhance feature separability \citep{wang2020understanding}, improving performance in tasks requiring precise discrimination. Recently, researchers have applied the hyperspherical normalization to intermediate features and weights to stabilize training in large-scale models such as diffusion models \citep{karras2024analyzing_diffusion} and transformers \citep{loshchilov2024ngpt}.

In this work, we apply hyperspherical normalization to RL. Unlike previous studies that focus on training the network on stationary data distributions with discrete inputs and outputs, we demonstrate their effectiveness on non-stationary data distributions with continuous inputs and outputs.

% \begin{figure*}[!ht]
% \begin{center}
% \includegraphics[width=17cm,height=4cm]{example-image-duck}
% \end{center}
% \caption{An abnormally wide duck. Nature is truly amazing.}
% \label{fig:architecture}
% \end{figure*}

\section{Preliminaries}

As background, we briefly explain Soft Actor-Critic (SAC) algorithm \citep{haarnoja2018sac} and the Simba architecture \citep{lee2024simba}.
%, which serves as the foundation for SimbaV2. 
%Then, the key architectural modifications introduced by SimbaV2 will be outlined in the following section.


\subsection{Soft Actor Critic}

SAC is a prominent off-policy algorithm for continuous control. It aims to maximize both expected cumulative reward and policy entropy where $\tau = (o, a, r, o')$ represents a transition tuple. SAC comprises a stochastic policy $\pi_{\theta}(a|o)$, a Q-function $Q_{\phi}(o, a)$, and an entropy coefficient $\alpha$ that balances reward and entropy.

The policy network is optimized to maximize the expected return while encouraging entropy which is formalized as:
\begin{equation} 
\mathcal{L}_{\pi} = \mathbb{E}_{\bar{a} \sim \pi_\theta} \left[ \alpha \log \pi_\theta(\bar{a}|o) - Q_\phi(o, \bar{a}) \right].
\label{eq:policy_objective} \end{equation}

The Q-function $Q_{\phi}(o, a)$ is trained to minimize the Bellman residual: 
\begin{equation} 
\mathcal{L}_Q =(Q_\phi(o, a) - \left( r + \gamma Q_{\bar{\phi}}(o', a')-\alpha\log \pi_\theta(a'|o') \right))^2,
\label{eq:critic_objective} 
\end{equation} 
where $a'\sim \pi_{\theta}(\cdot|o')$, $\gamma \in [0, 1)$ is the discount factor, and $Q_{\bar{\phi}}$ represents the target Q-network updated via an exponential moving average of $\phi$.

\subsection{Simba Architecture}

Simba \cite{lee2024simba} is an RL architecture with normalization layers composed of the following stages:

\textbf{Input Embedding.} Given an input observation $\bm{o}_t \in \mathbb{R}^{\vert \mathcal{O} \vert}$, Simba applies Running Statistics Normalization (RSNorm) to normalize each dimension to zero mean and unit variance.

At each timestep $t$, the running mean $\bm{\mu}_t \in \mathbb{R}^{\vert \mathcal{O} \vert}$ and variance $\bm{\sigma}^2 \in \mathbb{R}^{\vert \mathcal{O} \vert}$ are updated recursively as:
\begin{equation}\label{eqn:rs}
\bm{\mu}_t = \bm{\mu}_{t-1} + \frac{1}{t}\bm{\delta}_t,   \quad
\bm{\sigma}_t^2 = \bm{\sigma}_{t-1}^2 + \frac{1}{t}(\bm{\delta}_t^2 - \bm{\sigma}_{t-1}^2)
\end{equation}
where $\bm{\delta}_t = \bm{o}_t - \bm{\mu}_{t-1}$.

Given running statistics, the observation is normalized as: 
\begin{equation}
    \bm{\bar{o}}_t = \text{RSNorm}(\bm{o}_t) = \frac{\bm{o}_t - \bm{\mu}_t}{\sqrt{\bm{\sigma}_t^2 + \epsilon}}.
\end{equation}
Then, the normalized observation, $\bar{\bm{o}}_t$, is embedded with a linear layer $\bm{W}_h^0 \in \mathbb{R}^{|\mathcal{O}| \times d_h}$ defined as:
\begin{equation}
    \bm{h}_t^0 = \bm{W}_h^0 \bar{\bm{o}}_t.
\end{equation}
\textbf{Latent Encoding.} Next, the embedding $\bm{h}_t^0$ is encoded by a stack of $L$ residual blocks with pre-layer normalization. For $l \in \{1, \dots, L\}$, each of the $l$-th block is defined as:
\begin{equation}
    \bm{h}_t^l = \bm{h}_t^{l-1} + \text{MLP}(\text{LayerNorm}(\bm{h}_t^{l-1}))
\end{equation}
After the final block, the output is normalized again to obtain the latent feature:
\begin{equation}
    \bm{z}_t = \text{LayerNorm}(\bm{h}_t^{L}).
\end{equation}
\textbf{Output Prediction:} Finally, to predict the policy or Q-value, a linear layer $\bm{W}_o \in \mathbb{R}^{d_h \times d_o}$ maps $\bm{z}_t$ to:
\begin{equation}
    \bm{p}_t = \bm{W}_o\bm{z}_t.
\end{equation}
\section{SimbaV2}

SimbaV2 builds on Simba by adding constraints on weights, features, and gradients to enhance training stability, particularly when scaling to larger models and more computation. The modifications include:
\begin{itemize}[leftmargin=*, topsep=1pt, itemsep=0pt]
\item \textbf{LayerNorm} $\bm{\rightarrow}$ \textbf{$\ell_2$-Norm}: Layer normalization is replaced with $\ell_2$-normalization, constraining intermediate features to have unit norm. 
\item \textbf{Linear} $\bm{\rightarrow}$ \textbf{Linear + Scaler}: Standard linear layer is decoupled into a linear layer with weights constrained to a unit norm hypersphere, without a bias, and a learnable scaling vector that performs element-wise scaling.
\item \textbf{Residual Connection} $\bm{\rightarrow}$ \textbf{LERP}: Residual connection is replaced with a learnable linear interpolation (LERP), which combines raw and transformed features via a learnable interpolation vector.
\item \textbf{MSE Loss} $\bm{\rightarrow}$ \textbf{KL-divergence Loss}: MSE-based Bellman loss is replaced with KL-divergence loss, using a categorical critic \cite{bellemare2017distributional}.
\item \textbf{No Reward Scaling} $\bm{\rightarrow}$ \textbf{Reward Scaling}:  Rewards are normalized with running statistics to stabilize the scale of both actor loss (Equation.\ref{eq:policy_objective}) and critic loss (Equation.\ref{eq:critic_objective}).
\end{itemize}

%We describe in detail these modifications in the following subsections.
In the following subsections, we describe these modifications in detail. % and their integration.



\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.41\textwidth]{figures/simbav2_architecture.pdf}
\end{center}
\vspace{-1mm}
\caption{\textbf{SimbaV2 architecture.} The input observation is first normalized using running statistics, then shifted along a new axis with a constant $c_\text{shift}$ to preserve magnitude information before being projected onto the unit hypersphere. The projected observation is passed through a linear layer, followed by a series of non-linear blocks and refined with LERP, serving as a residual connection. A final linear layer predicts the policy or value function.}
\label{fig:architecture}
\end{figure}

\subsection{Input Embedding}
\label{subsection:input_embedding}

Following Simba, SimbaV2 first standardize the raw observations \(\bm{o}_t \in \mathbb{R}^{|\mathcal{O}|}\) using RSNorm, yielding \(\bar{\bm{o}}_t\). To further stabilize training, we map \(\bar{\bm{o}}_t\) onto the unit hypersphere before applying a linear layer.

\paragraph{Shift + $\bm{\ell_2}$-Norm.}
Direct \(\ell_2\)-normalization can discard magnitude information (e.g., \(\bar{\bm{o}}_t = [1, 0]\) and \(\ [2, 0]\) both map to \([1, 0]\)). 
To retain magnitude information, we embed $\bar{\bm{o}}_t$ into an ($|\mathcal{O}|+1$)-dimensional vector by concatenating a positive constant $c_{\text{shift}}>0$, then apply \(\ell_2\)-normalization:
\begin{equation}
  \widetilde{\bm{o}}_t 
  \;=\; 
  \ell_2\text{-Norm}  
  (\bigl[\bar{\bm{o}}_t;\,c_{\text{shift}}\bigr]).
\end{equation}
As illustrated in Figure \ref{fig:architecture}, this additional coordinate encodes the original norm of \(\bar{\bm{o}}_t\), preserving magnitude information. 

\textbf{Linear + Scaler.} We then embed $\bm{\tilde{o}}_t$ using a linear layer $\bm{W}^0_h \in \mathbb{R}^{(|\mathcal{O}|+1) \times d_h}$ and a scaling vector $\boldsymbol{s}_h^0 \in \mathbb{R}^{d_h}$ as:
\begin{equation}
    \bm{h}_t^0 = \ell_2 \text{-Norm} (\bm{s}_h^0 \odot (\bm{W}_h^0 \; \mathrm{Norm} (\bm{\tilde{o}}_t)).
\end{equation}
where the $\ell_2$-normalization projects back to the hypersphere.


\subsection{Feature Encoding}
\label{subsection:feature_encoding}

Starting from the initial hyperspherical embedding \(\bm{h}_t^0\), we apply \(L\) consecutive blocks of non-linear transformations. Each $l$-th block transforms \(\bm{h}_t^l\) into \(\bm{h}_t^{l+1}\) as follows:

\textbf{MLP + $\bm{\ell_2}$-Norm.} Each block uses an inverted bottleneck MLP \cite{vaswani2017attention} followed by $\ell_2$-normalization to project the output back onto the unit hypersphere.
\begin{equation}
  \bm{\tilde{h}}_t^l
  = 
  \ell_2\text{-Norm} (
      \bm{W}_{h,2}^l \,\mathrm{ReLU}\bigl( 
          (\bm{W}_{h,1}^l \,\bm{h}_t^l) 
          \odot \bm{s}^l_h
      \bigr)
  ).
\end{equation}
where \(\bm{W}_{h,1}^l \in \mathbb{R}^{4d_h \times d_h}\) and \(\bm{W}_{h,2}^l \in \mathbb{R}^{d_h \times 4d_h}\) are weight matrices, and \(\bm{s}^l_h \in \mathbb{R}^{4d_h}\) is a learnable scaling vector.

\textbf{LERP + $\bm{\ell_2}$-Norm.} We then linearly interpolate between the original input \(\bm{h}_t^l\) and its non-linearly transformed output \(\bm{\tilde{h}}_t^l\), followed by another $\ell_2$-normalization:
\begin{equation}
  \bm{h}_t^{l+1}
  =
  \ell_2\text{-Norm}(
      (\bm{1} - \bm{\alpha}^l) \odot \bm{h}_t^l
      + 
      \bm{\alpha}^l \odot\bm{\tilde{h}}_t^l
    ).
\end{equation}
where \(\bm{1} \in \mathbb{R}^{d_h}\) and \(\bm{\alpha}^l \in \mathbb{R}^{d_h}\) are one vector and a learnable interpolation vector, respectively. 

LERP acts analogous to a learnable residual connection but can also be viewed as a first-order approximation of a Riemannian retraction on the hypersphere \cite{absil2008optimization}. Please refer to Appendix~\ref{appendix:details_lerp} for further discussion.

\subsection{Output Prediction}

We use a linear layer to parameterize both the policy distribution and Q-value. Because Simba’s single Q-value estimate with an MSE-based Bellman loss is susceptible to outliers, we adopt a categorical critic with KL-divergence loss \cite{bellemare2017distributional}, which provides smoother gradients and more stable optimization \cite{imani2018dist_loss}.

\textbf{Distributional Critic.} We represent the Q-value as a categorical distribution over a discrete set of returns:
\begin{equation}
    \bigl\{\delta_i=G_{\min} + (i-1)\,\frac{G_{\max} - G_{\min}}{n_\text{atom}-1}
    \;\big|\; 
    i = 1,...,n_\text{atom}\bigr\},
\end{equation}
where \(G_{\min}\) and \(G_{\max}\) denote the minimum and maximum possible returns, and \(n_\text{atom}\) is the number of discrete atoms.

Given the encoded representation \(\bm{h}_t^L\), we compute unnormalized logits \(\bm{z}_t \in \mathbb{R}^{|\mathcal{A}| \times n_\text{atom}}\) for all actions as follows:
\begin{equation}
    \bm{z}_t 
    \;=\; 
    \bm{W}_{o,2}(\bigl(\bm{W}_{o,1}\,\bm{h}_t^L\bigr)\,\odot\,\bm{s}_o),
\end{equation}
where \(\bm{W}_{o,1} \in \mathbb{R}^{d_h \times d_h}\), \(\bm{W}_{o,2} \in \mathbb{R}^{|\mathcal{A}| \times n_\text{atom} \times d_h}\), and \(\bm{s}_o \in \mathbb{R}^{d_h}\) are trainable parameters. 

For each action \(\bm{a} \in \mathcal{A}\), the categorical probability is represented by applying the softmax function to \(\bm{z}_{t, \bm{a}}\in\mathbb{R}^{n_\text{atom}}\):
\begin{equation}
    p_{t,\bm{a}} \;=\; \mathrm{softmax}\bigl(\bm{z}_{t, \bm{a}}\bigr).
\end{equation}
The resulting Q-value is the expected return under $p_{t,\bm{a}}$:
\begin{equation}
    Q(\bm{o}_t, \bm{a}) = \sum_{i=1}^{n_\text{atom}} \delta_i \, p_{t,\bm{a},i}.
\end{equation}
\textbf{Reward Bounding and Scaling.} To use a categorical critic, we first bound the target returns within $[G_{\min}, G_{\max}]$ and then scale the reward to maintain unit variance, ensuring stable gradients for both the actor and the critic. 
Unlike previous work \cite{schaul2021td_scaling}, which scaled the critic loss, we scale the reward itself, affecting both components simultaneously. 
Moreover, unlike observation normalization, we do not center the reward, as shifting the reward can alter the optimal policy in episodic tasks \cite{naik2024reward_centering}. 

Given a reward $r_t$ at time $t$ and a discounted factor $\gamma$, we track a running discounted return: 
\begin{equation}
    G_t \leftarrow (1 - \gamma) G_{t-1} + r_t
\end{equation} 
where $G_t$ is re-initialized to $0$ at the start of each episode.

Then, we track the running variance of $G_t$, denoted as $\sigma^2_{t,G}$ and maintain a running maximum:
\begin{equation}
    G_{t, \max} \leftarrow \max (G_{t,\max}, G_t).
\end{equation}
We then scale the reward as follows:
\begin{equation}
    \bar{r}_t \leftarrow \frac{r_t}{\max
    (\sqrt{\sigma_{t,G}^2 + \epsilon},\; G_{t, \max} / G_{\max})}.
\end{equation} 
This formula stabilizes gradients for both high-variance and low-variance returns, while thresholding with \(G_{t,\max}/G_{\max}\) ensures target returns remain within \([G_{\min}, G_{\max}]\).

\subsection{Initialization and Update} 
\label{subsection:init_and_update}

In this subsection, we outline how weight matrix \(\bm{W}\), scaler \(\bm{s}\), and interpolation vector \(\bm{\alpha}\) are initialized and updated. 

\textbf{Weight.} All weight vectors are initialized orthogonally and then projected onto the unit hypersphere which forms an orthonormal basis. At each gradient step, we re-project them onto the unit sphere to maintain unit norm.

Formally, let $\bm{W}$ be the weight matrix before the update, and let $\mathcal{L}$ denote the loss function. The update rule is defined as:
\begin{equation}
    \bm{W} \leftarrow \ell_2\text{-Norm} (\bm{W} - \eta \frac{\partial \mathcal{L}}{\partial \bm{W}})
\end{equation}
where $\eta > 0$ is a learning rate and $\ell_2\text{-Norm}$ is the $\ell_2$-normalization operator along the embedding axis.

\textbf{Scaler.} Following \citet{loshchilov2024ngpt}, we decouple the initialization scale of \(\bm{s}\) from its learning dynamics by using two scalars, \(\bm{s}_{\mathrm{init}}\) and \(\bm{s}_{\mathrm{scale}}\). Although \(\bm{s}\) is initialized to \(\bm{s}_{\mathrm{scale}}\), it behaves as if it was initialized to \(\bm{s}_{\mathrm{init}}\) during the forward pass by:
\vspace{-1mm}
\begin{equation} 
    \bm{s} \leftarrow \bm{s}_\mathrm{scale} \odot ( \bm{s}_{\mathrm{init}} \oslash\bm{s}_{\mathrm{scale}}) 
\end{equation}
where \(\odot\) and \(\oslash\) are element-wise product and division, respectively. This formulation lets \(\bm{s}_\mathrm{scale}\) control the learning rate of \(\bm{s}\) independently from the global learning rate \(\eta\). 

When both the feature vector \(\bm{h} \in \mathbb{R}^{d_h}\) and the randomly orthonormal initialized weight matrix \(\bm{W} \in \mathbb{R}^{d_h \times d_h}\) lie on the unit hypersphere, each component of \(\bm{W}\bm{h} \in \mathbb{R}^{d_h}\) can be approximated by \(\cos(\theta)\) with \(\mathbb{E}_{\theta}[\cos^2(\theta)] = 1/2\). Therefore, we set 
\(\bm{s}_{\mathrm{init}} = \bm{s}_{\mathrm{scale}}=(\sqrt{2 / {d_h}} ) \, \bm{1}\) to maintain unit norm after scaling at initialization. A detailed derivation is in Appendix~\ref{appendix:details_scaler}.

\textbf{Interpolation vector.} Analogous to the scaler, the interpolation vector, $\bm{\alpha}$, also has $\bm{\alpha}_{init}$ and $\bm{\alpha}_{scale}$.
Following \citet{loshchilov2024ngpt}, we initialize $\bm{\alpha}_{init}=\mathbf{1}/(L+1)$ and $\bm{\alpha}_{scale}=\mathbf{1}/\sqrt{d_h}$, to preserve residual feature and gradually integrate non-linear features.

%For a detailed implementation, please refer to Appendix~\ref{appendix:details}.

\begin{figure*}[ht!]
\centering
\begin{center}
\includegraphics[width=0.97\textwidth]{figures/sec5_analysis.pdf}
\end{center}
\vspace{-5mm}
\caption{\textbf{SimbaV2 vs. Simba Training Dynamics.} We track 4 metrics during training to understand the learning dynamics of SimbaV2: \textbf{(a)} Average normalized return across tasks. \textbf{(b)} Weighted sum of $\ell_2$-norms of all intermediate features in critic. \textbf{(c)} Weighted sum of $\ell_2$-norms of all critic parameters \textbf{(d)} Weighted sum of $\ell_2$-norms of all gradients in critic \textbf{(e)} Effective learning rate (ELR) of the critic. On both environments, SimbaV2 maintains stable norms and ELR, while Simba exhibits divergent fluctuations.}
\label{figure:analysis}
\vspace{-3mm}
\end{figure*}

\section{Experiments}

We now present a series of experiments designed to evaluate SimbaV2. Our investigation centers on three main setups:
\begin{itemize}[leftmargin=*, topsep=1pt, itemsep=0pt]
    \item \textbf{Empirical Analysis} (Section~\ref{section:empirical_analysis}). Investigates whether SimbaV2 stabilizes the non-stationary optimization process and whether scaling model capacity and computation improves performance without overfitting. 
    \item \textbf{Comparisons} (Sections~\ref{section:online_rl}). Compare SimbaV2 against state-of-the-art RL algorithms.
    \item \textbf{Design Study} (Section~\ref{section:design_study}.) Conducts ablation studies on individual architectural component of SimbaV2.
\end{itemize}

\subsection{Experimental Setup}
\label{section:experiments}

\textbf{Environment.} A total of 57 continuous-control tasks are considered across 4 domains: MuJoCo~\cite{todorov2012mujoco}, DMC Suite~\cite{tassa2018dmc}, MyoSuite~\cite{caggiano2022myosuite}, and HumanoidBench~\cite{sferrazza2024humanoidbench}. Also, two challenging subsets are defined for an empirical analysis: DMC-Hard (7 tasks involving \texttt{dog} and \texttt{humanoid} embodiments) and HBench-Hard (5 tasks: \texttt{run}, \texttt{balance-simple}, \texttt{sit-hard}, \texttt{stair}, \texttt{walk}).

\textbf{Baselines.} Comparisons include a broad range of deep RL algorithms, PPO \cite{schulman2017ppo}, SAC \cite{haarnoja2018sac}, TD3 \cite{fujimoto2018td3} TD3+OFE \cite{ota2020ofenet}, TQC \cite{kuznetsov2020tqc}, DreamerV3 \cite{hafner2023dreamerv3}, TD7 \cite{fujimoto2023td7}, TD-MPC2 \cite{hansen2023tdmpcv2}, Cross-Q \cite{bhatt2024crossq}, BRO \cite{nauman2024bro}, MAD-TD \cite{voelcker2024madtd}, MR.Q \cite{fujimoto2025mrq}, and Simba \cite{lee2024simba}. 
Whenever available, we report the results from the original paper; otherwise, we run the authors’ official code. In addition, to further compare performance before and after scaling, we evaluate BRO, Simba, and SimbaV2 under both low UTD ratios ($\leq2$) and high UTD ratios ($\leq8$). Additional details are described in Appendix~\ref{appendix:baselines_online}.

\textbf{Metrics.} To aggregate performance across diverse domains, each environment’s return is normalized to a near $[0, 1)$ range.  Specifically, MuJoCo performance is normalized by TD3~\cite{fujimoto2018td3}; DMC returns are divided by 1000; MyoSuite scores use success rates; and HumanoidBench scores are normalized by their success score.

\textbf{Training.} If possible, we tried to closely follow Simba's training configuration aiming to provide an apples-to-apples comparison. Unless otherwise specified, the actor and critic have hidden dimensions of 128 and 512, respectively (approximately 5M parameters). The model is trained for 1M environment steps, using a UTD ratio 2. We used an Adam~\cite{kingma2014adam} optimizer without weight decay and set the batch size to 256. The learning rate is linearly decayed
from $1\times10^{-4}$ to $3\times10^{-5}$. Full hyperparameter configurations are provided in Appendix~\ref{appendix:hyperparameters}.

For the parameter scaling experiments, we focus on scaling the critic as prior studies suggest that scaling the actor has limited benefits \cite{nauman2024bro, lee2024simba}. Specifically, the critic’s hidden dimension is scaled from \(\{128, 256, 512, 1024\}\), growing is parameters from \(0.3\)M to \(17.8\)M. For computation scaling experiments, the UTD ratio is scaled from \(\{1, 2, 4, 8\}\) both with and without periodic weight reinitialization. We apply reinitialization every 500k update steps as suggested by \citet{nauman2024bro}.

\input{tables/online_rl}

\subsection{Empirical Analysis}
\label{section:empirical_analysis}

\textbf{Training Dynamics.} To understand the training dynamics of SimbaV2, we measure the feature norm, weight norm, gradient norm, and the effective learning rate (ELR), defined as the ratio of the gradient norm to the weight norm \citep{kodryan2022training, lyle2024normalization} (See Appendix~\ref{appendix:effective_lr} for details).  
We weighted average each metric across layers where weights correspond to each layer’s fraction of total parameters. 
Additionally, we divide the layers into encoder layers (all layers before the output prediction) and predictor layers (those after) to analyze their respective dynamics.

Figure~\ref{figure:analysis} compares \textcolor{Simbav2_dark_color}{SimbaV2} and \textcolor{Simba_dark_color}{Simba} on DMC-Hard and HBench-Hard. 
As shown in Figure~\ref {figure:analysis}.(b)-(d), Simba exhibits large, often divergent fluctuations in feature, weight, and gradient norms between the encoder and predictor. 
Consequently, Figure~\ref{figure:analysis}.(e) shows that the encoder’s ELR trending upward while the predictor’s ELR declines.

In contrast, SimbaV2 enforces tighter constraints, stabilizing norms and ELRs throughout training. Although certain parameters (e.g., scalers or interpolation vectors) can exceed the unit norm, the majority of parameters remain on the hypersphere, resulting in more robust optimization. A standalone visualization of SimbaV2 is in Appendix~\ref{appendix:effective_lr}.


\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.46\textwidth]{figures/sec5_param_scaling.pdf}
\end{center}
\vspace{-4.5mm}
\caption{\textbf{Parameter Scaling.} We scale the number of model parameters by increasing the width of the critic network. On DMC-Hard, both Simba and SimbaV2 benefit from increased model size. On HBench-Hard, however, Simba plateaus at larger model sizes, whereas SimbaV2 continues to improve.}
\vspace{-2.2mm}
\label{figure:param_scaling}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%5
% Parameter Scaling

\textbf{Parameter Scaling.} Next, we investigate whether SimbaV2’s stable training dynamics alleviate overfitting and enhance performance as the model size scales. In Figure~\ref{figure:param_scaling}, we increase the critic network’s parameters on DMC-Hard (left) and HBench-Hard (right). Both Simba and SimbaV2 gain from a larger model on DMC-Hard. However, on HBench-Hard, despite comparable performance at the smallest scale (0.3M), Simba plateaus at larger scales, whereas SimbaV2 continues to improve. This suggests that stabilizing training dynamics can effectively leverage a larger model.

\textbf{Compute Scaling.} Finally, we explore compute scaling, a major factor in improving sample efficiency in deep RL \citep{li2023efficient}. 
While increasing the UTD ratio can enhance sample efficiency, it also raises the risk of overfitting. Previous work has addressed this with ensembling \citep{chen2021redq}, periodic reinitialization \citep{lee2024plastic, d2023sample_breaking, nauman2024bro}, or both \citep{kim2023reset_ensemble}. 
We investigate whether SimbaV2’s stable dynamics enable effective scaling without these additional methods.

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.46\textwidth]{figures/sec5_utd_scaling.pdf}
\end{center}
\vspace{-4mm}
\caption{\textbf{Compute Scaling.}. We scale compute by increasing the UTD ratio. We compare Simba and SimbaV2, both with and without periodic reset. Simba saturates at lower ratios without reset but improves with reset. In contrast, SimbaV2 scales smoothly even without reset, where using reset slightly degrades its performance.}
\label{figure:utd_scaling}
\vspace{-2mm}
\end{figure}

Figure~\ref{figure:utd_scaling} shows the effect of varying the UTD ratio on DMC-Hard (left) and HBench-Hard (right), comparing Simba and SimbaV2 with and without reinitialization (solid lines: no reinitialization; dashed lines: reinitialization).
In Simba, performance plateaus at a UTD ratio of 2 on DMC-Hard and 1 on HBench-Hard. When combined with reinitialization, but further improves with reinitialization, consistent with \citet{d2023sample_breaking}.
In contrast, SimbaV2 scales consistently as the UTD ratio increases, even without reinitialization. 
Notably, reinitialization slightly degrades SimbaV2’s performance, as it disrupts training and adds time to recover.

To assess the importance of hyperspherical normalization for UTD scaling, we test a variant in Appendix~\ref{appendix:scalability_effect} that adds distributional critics and reward scaling but omits the normalization. This variant fails to scale at higher UTD ratios, demonstrating the role of hyperspherical normalization.

\input{tables/ablation}

\subsection{Online RL}
\label{section:online_rl}

Having observed SimbaV2’s scalability, we now compare it against standard model-free and model-based RL. 

Table~\ref{table:online_rl}.(a) presents results at a UTD ratio below 2. SimbaV2 with UTD=2 attains an average normalized score of 0.892, exceeding the previous best of 0.780. 
Only except for DMC-Easy suite, SimbaV2 outperforms leading model-free (CrossQ \cite{bhatt2024crossq}, BRO \cite{nauman2024bro}, Simba \cite{lee2024simba}) and model-based (TD-MPC2 \cite{hansen2023tdmpcv2}, Mr.Q \cite{fujimoto2025mrq}) baselines, demonstrating superior sample efficiency.

Table~\ref{table:online_rl}.(b) evaluates higher UTD settings. Increasing SimbaV2's UTD from 2 to 8 further elevates its average score from 0.892 to 0.911. SimbaV2 also surpasses BRO with UTD=10, which utilizes periodic reinitialization to avoid overfitting at high update rates. These consistent gains at larger UTD ratios underscore the efficacy of hyperspherical normalization in stabilizing training.

For offline RL, we simply add a behavioral cloning loss during training with using identical configurations to the online RL. Despite minimal changes, SimbaV2 performs competitively with existing baselines (Appendix~\ref{appendix:offline_rl}).

\subsection{Design Study}
\label{section:design_study}

Table~\ref{table:design} presents the results from ablation studies isolating the contributions of various architectural choices.

\textbf{Input Projection.}  Projecting observations onto a hypersphere before passing them through the linear layer is crucial for performance (Table~\ref{table:design}.(a)), where omitting this step leads to a significant performance drop. 
Equally important design is preserving the original magnitude during projection (Table~\ref{table:design}.(b)). 
We also explore an alternative ``resize'' projection, where inputs are first divided by $c_{\rm{shift}}\sqrt{d_h}$ before being projected onto an $(n+1)$-dimensional hypersphere. 
The resize projection yields comparable performance as it can also retain magnitude information (Table~\ref{table:design}.(d)).

\textbf{Output Projection.}  Incorporating a distributional critic and reward scaling improves performance, especially in environments with high reward variance like MuJoCo (Table~\ref{table:design}.(e)–(f)). 
Bounding target returns proves essential for easier tasks (Table~\ref{table:design}.(g)), such as \texttt{cartpole} in the DMC-Easy suite (Table~\ref{table:appendix_full_dmc_easy_output_design}).
Without bounding, consistent high returns can diminish return variance, and scaling returns push target values beyond the range of the distributional critic, leading to collapse in the TD loss.

\textbf{Initialization \& Update.} 
Gradually decaying the learning rate is critical. Without decay, the model may struggle to refine its predictions during later training stages, as SimbaV2 maintains an effective constant learning rate throughout training (Table~\ref{table:design}.(i)).
Tuning initial scaler values has minimal impact on performance where the architecture remains stable by these changes (Table~\ref{table:design}.(j)–(m)).


\section{Lessons and Opportunities}

\textbf{Lessons.} Historically, RL research has relied on complex regularizations to address overfitting and scalability issues \cite{klein2024plasticity_survey}. 
Our findings suggest that suitably chosen constraints, exemplified by SimbaV2, can simplify these design complexities while retaining strong performance.

\textbf{Opportunities.} Future opportunities include deploying SimbaV2 in real-world robotics \cite{hwangbo2019robot}, where sample efficiency is crucial, and extending it to model-based \cite{hansen2023tdmpcv2} or visual RL \cite{kostrikov2020drq}. 
Furthermore, with increasing interest in RL for training large language models \cite{ouyang2022rlhf, guo2025deepseekr1}, the potential benefits of using stricter normalization for large models remain an exciting open question.



% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}
We would like to express our gratitude to Dongyoon Hwang and Hawon Jeong for their valuable feedback on this paper.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{reference}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\begin{center}
\textbf{\large Appendix}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Riemannian Optimization
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Architectural Details}
\label{appendix:details}

\subsection{LERP: A Retraction-based Approximation on Riemannian Manifolds.}
\label{appendix:details_lerp}

During the feature encoding stage in SimbaV2, the input $\bm{h}$ and its non-linearly transformed output $\bm{\tilde{h}}$ are linearly interpolated using a learnable interpolation vector $\bm{\alpha} \in \mathbb{R}^{d_h}$: \begin{equation}
    \bm{h} \leftarrow \ell_2\textrm{-Norm} ((\bm{1} - \bm{\alpha}) \odot \bm{h} + \bm{\alpha} \odot \bm{\tilde{h}}), 
\end{equation} followed by $\ell_2$-normalization. 

Intuitively, this can be interpreted as a first-order (retraction-based) approximation of the Riemannian update formula on the hypersphere. This section provides a brief introduction to the differential geometry concepts that underpin the Riemannian optimization perspective of $\bm{\alpha}$. For brevity, we omit the mathematical definitions, derivations, and proofs here. The comprehensive introduction to differential geometry and Riemannian optimization can be found in \citet{spivak1970comprehensive}, \citet{do1992riemannian}, and \citet{boumal2023introduction}.

Let $\mathbb{S}_{n-1}$ denote the $n$-dimensional hypersphere embedded in $\mathbb{R}^n$, i.e., $\mathbb{S}_{n-1} = \{ \bm{h} \in \mathbb{R}^n \mid \Vert \bm{h} \Vert_2 = 1 \}$.

\textbf{Manifold.} A \textit{manifold} $\mathcal{M}$ of dimension $n$ is a space that can locally be approximated by a Euclidean space $\mathbb{R}^n$. The simplest examples of a manifold include the open ball $U = \{ \bm{x} \in \mathbb{R}^n \mid \Vert \bm{x} \Vert_2 < r \}$ for $r \in \mathbb{R}_{> 0}$, and the hypersphere $\mathbb{S}_{n-1}$ is also a manifold in $\mathbb{R}^n$.

\textbf{Tangent Spaces.} At each point $\bm{x} \in \mathcal{M}$, the \textit{tangent space} $T_{\bm{x}} \mathcal{M}$ is an $n$-dimensional vector space that locally approximates $\mathcal{M}$ near $\bm{x}$. Tangent vectors generalize the concept of directional derivatives. For the hypersphere $\mathbb{S}_{n-1}$, the tangent space at a point $\bm{p}$ consists of all vectors orthogonal to $\bm{p}$: \begin{equation} T_{\bm{p}} \mathbb{S}_{n-1} = \{ \bm{h} \in \mathbb{R}^n \mid \langle \bm{p}, \bm{h} \rangle = 0 \} \end{equation} where $\langle \cdot, \cdot \rangle$ denotes the Euclidean inner product.

\textbf{Riemannian Metrics and Manifolds.} The tangent space $T_{\bm{x}} \mathcal{M}$ is not inherently equipped with an inner product. A \textit{Riemannian metric} $\rho$ provides a collection of inner products $\rho_{\bm{x}} (\cdot, \cdot): T_{\bm{x}} \mathcal{M} \times T_{\bm{x}} \mathcal{M} \to \mathbb{R}$ on the tangent spaces, $\rho := (\rho_{\bm{x}})_{\bm{x} \in \mathcal{M}}$, which locally define the geometry of $\mathcal{M}$. A \textit{Riemannian manifold} $(\mathcal{M}, \rho)$ is a smooth manifold $\mathcal{M}$ equipped with such a metric.  This enables us to define geometric notions such as distance, angle, length, volume, and curvature of manifold. For a detailed explanation of geometrics on Riemannian manifolds, refer to \citet{lee2006riemannian}.

\textbf{Exponential Mapping and Retraction.} Under some conditions~\citep{do1992riemannian}, the \textit{exponential map} $\exp_{\bm{x}}: T_{\bm{x}} \mathcal{M} \to \mathcal{M}$ can be defined at a point $\bm{x} \in \mathcal{M}$. $\exp_{\bm{x}} (\bm{v})$ maps a tangent vector $\bm{v} \in T_{\bm{x}} \mathcal{M}$ to a point on the manifold along the geodesic from $\bm{x}$ in the direction of $\bm{v}$. Therefore, for small $t \in \mathbb{R}$, $\exp_{\bm{x}}(t \bm{v})$ represents the shortest path on $\mathcal{M}$ starting at $\bm{x}$ with initial direction $\bm{v}$. In Euclidean space $(\mathbb{R}^n, \bm{I}_n)$, the exponential map $\exp_{\bm{x}} (\bm{v}) = \bm{x} + \bm{v}$ is simply defined as a straight path. In practice, for computational efficiency (e.g., the mappings do not have closed-form), we often approximate the exponential map $\exp_{\bm{x}}$ by a  \textit{retraction}~\citep{absil2008optimization} $R_{\bm{x}}$: 
\begin{definition}[Retraction]
    A retraction $R$ on a manifold $\mathcal{M}$ is a smooth map:
    \[
        \deffun{R : \bigcup_{\bm{x} \in \mathcal{M}} T_{\bm{x}} \mathcal{M} \to \mathcal{M} ; (\bm{x}, \bm{v}) \mapsto R_{\bm{x}}(\bm{v})}
    \]
    with the following properties:
    \[
        R_{\bm{x}} (\bm{0}) = \bm{x} \qquad \text{and} \qquad ( dR_{\bm{x}})_{\bm{0}} = \text{id}
    \]
    where $R_{\bm{x}}$ denotes the restriction of $R$ to $T_{\bm{x}} \mathcal{M}$, $(dR_{\bm{x}})_{\bm{0}}$ denotes the differential of $R_{\bm{x}}$ at $\bm{0}$, and $\text{id}$ is the identity map.
\end{definition} Intuitively, a retraction $R_{\bm{x}} (\bm{v})$ provides a first-order approximation of the exponential map $\exp_{\bm{x}}(\bm{v})$~\citep{boumal2023introduction}. Figure~\ref{figure:appendix_riemannian} illustrates the difference between the exponential map and retraction on $\mathbb{S}_2$. For the hypersphere $\mathbb{S}_{n-1}$, the retraction of a tangent vector $\bm{\xi} \in T_{\bm{h}} \mathbb{S}_{n-1}$ onto $\mathbb{S}_{n-1}$ is given by~\citep{absil2008optimization}: \begin{equation} R_{\bm{h}} (\bm{\xi}) = \ell_2\textrm{-Norm} (\bm{h} + \bm{\xi}) = \frac{\bm{h} + \bm{\xi}}{\Vert \bm{h} + \bm{\xi} \Vert_2} \end{equation} 

\begin{figure*}[ht!]
\centering
\begin{center}
\includegraphics[width=0.3\textwidth]{figures/appendix_riemannian_figure_comp.pdf}
\end{center}
\vspace{-6mm}
\caption{\textbf{Exponential Map vs. Retraction on 3-dimensional sphere.} Comparison of the exponential map $\exp_{\bm{x}}$ and the retraction $R_{\bm{x}}$ on the $3$-dimensional sphere $\mathbb{S}_2$. The exponential mapping sends a tangent vector $\bm{g} \in T_{\bm{x}} \mathbb{S}_2$ exactly along the geodesic from $\bm{x}$ to a point on the manifold, while the retraction locally approximates this mapping to first order. Figure adapted from \citet{sutti2024riemannian}.}
\label{figure:appendix_riemannian}
\end{figure*}

\textbf{Riemannian Optimization.} On Riemannian manifolds, gradient updates ideally follow the curved geodesics, rather than straight lines as in Euclidean space. To this end, \citet{bonnabel2013riemanniangd} introduce Riemannian SGD that generalizes SGD to Riemannian manifolds using exponential map: \begin{equation} \bm{h} \leftarrow \exp_{\bm{h}} (-\alpha \bm{g}) \end{equation} where $\alpha > 0$ is the global learning rate and $\bm{g} \in T_{\bm{h}} \mathcal{M}$ denotes the \textit{Riemannian gradient}.

In our case, $-(\bm{\tilde{h}} - \bm{h})$ can be viewed as the gradient $\bm{g}$ in the Euclidean space. Then, we project the gradient onto the tangent space $T_{\bm{h}} \mathbb{S}_{n-1}$: \begin{align} 
\bm{g}_\mathrm{proj} & = \bm{g} - \langle \bm{g}, \bm{h} \rangle \bm{h} \\
& = -(\bm{\tilde{h}} - \bm{h})- \langle -\bm{\tilde{h}} + \bm{h}, \bm{h} \rangle \bm{h} \\
& = - \bm{\tilde{h}} + \langle \bm{\tilde{h}} , \bm{h} \rangle \bm{h}
\end{align} Applying the retraction $\exp_{\bm{h}} (-\alpha \bm{g}_\text{proj}) \approx R_{\bm{h}} (-\alpha \bm{g}_\text{proj})$: \begin{align} 
\bm{h} & \leftarrow \ell_2\textrm{-Norm} \left(\bm{h} + \alpha ( \bm{\tilde{h}} - \langle \bm{\tilde{h}} , \bm{h} \rangle \bm{h} ) \right) \\ 
& = \ell_2\textrm{-Norm} \left((1 - \alpha \langle \bm{\tilde{h}} , \bm{h} \rangle) \bm{h} + \alpha \bm{\tilde{h}} \right)
\end{align} Thus, the LERP operation in SimbaV2 can be interpreted as a retraction-based approximation of the Riemannian update rule on the hypersphere, where the learning rate $\alpha$ is replaced by a learnable vector $\bm{\alpha}$ and the inner product $\langle \bm{\tilde{h}}, \bm{h} \rangle$ term is neglected. 
Also, \citet{loshchilov2024ngpt} empirically show that neglecting the inner product term has no significant impact on performance.

\clearpage
\subsection{Scaler Initialization} 
\label{appendix:details_scaler}

In our algorithm, the \textit{scaler} $\bm{s} \in \mathbb{R}^{d_h}$ is a learnable vector that element-wise scales the output $\bm{z}$ of the linear layer: \begin{equation}
    \bm{z} = \bm{s} \odot \bm{W} \bm{h} \in \mathbb{R}^{d_h}
\end{equation} where $\bm{W} \in \mathbb{R}^{d_h \times n}$ is the weight matrix of the linear layer, and $\bm{h} \in \mathbb{R}^n$ is the input vector. To ensure that $\bm{z}$ (approximately) maintains unit norm at initialization, we initialize $\bm{s}$ as $\bm{s} = \sqrt{\frac{2}{d_h}} \cdot \bm{1}$. The following section provides the derivation for this initialization.

We assume that each normalized embedding $\bm{w}_l \in \mathbb{R}^n$ of $\bm{W}$, and a random $n$-dimensional normalized vector $\bm{h} \in \mathbb{R}^n$, are uniformly distributed on the $n$-dimensional hypersphere $\mathbb{S}_{n-1}$. Furthermore, we assume that the vectors $\bm{w}_l$ and $\bm{h}$ are mutually independent~\citep{feller1991introduction}. We denote the angle between $\bm{w}_l$ and $\bm{h}$ by $\theta_l$, such that $\cos \theta_l = \bm{w}_l \cdot \bm{h}$ since $\Vert \bm{w}_l \Vert_2 = \Vert \bm{h}\Vert_2 = 1$.

\textbf{Distribution of the Cosine of the Angle.} For simplicity, assume that $\bm{w}_l$ is fixed. Since $\bm{h}$ is uniformly distributed on the hypersphere, the distribution of the angle $\theta_l$ depends on the \textit{solid angle}~\citep{weisstein2005solidangle} subtended by $\bm{h}$ with respect to $\bm{w}_l$. The surface area $A_{n-2}$ of an $(n-1)$-dimensional hyperspherical cap~\citep{li2010concise} leads to the probability density function $f (\theta_l)$~\citep{cai2013distributions}: \begin{equation}
    f (\theta_l) = \frac{A_{n-1}}{S_{n-1}} = \frac{\frac{2 \pi^{(n-1)/2}}{\Gamma(\frac{n-1}{2})}}{\frac{2 \pi^{n/2}}{\Gamma (\frac{n}{2})}} \sin^{n-2} (\theta_l) = \frac{\Gamma (\frac{n}{2})}{\sqrt{\pi} \Gamma (\frac{n-1}{2})} \sin^{n-2} (\theta_l)
\end{equation} where $\theta_l \in [0, \pi]$, $\Gamma$ is the gamma function and $S_{n-1}$ is the surface area of $\mathbb{S}_{n-1} = \frac{2 \pi^{n/2}}{\Gamma (\frac{n}{2})}$~\citep{weisstein2002hypersphere}. 

\textbf{Norm of Output Vector.} Let $\bm{z} = \bm{s} \odot \bm{W}\bm{h} \in \mathbb{R}^{d_h}$ be the output of the linear layer. Each element of $\bm{z}$ and $\bm{s}$, denoted by $\bm{z}_l$ and $\bm{s}_l$, respectively, corresponds to the scaled cosine of the angle $\theta_l$ between $\bm{w}_l$ and $\bm{h}$: \begin{equation}
    \bm{z} = \bm{s}\odot \bm{W}\bm{h} = \begin{bmatrix} \bm{s}_1 (\bm{w}_1 \cdot \bm{h}) \\ \bm{s}_2 (\bm{w}_2 \cdot \bm{h}) \\ \vdots \\ \bm{s}_{d_h} (\bm{w}_{d_h} \cdot \bm{h})\end{bmatrix} = \begin{bmatrix} \bm{s}_1 \cos\theta_1 \\ \bm{s}_2 \cos \theta_2 \\ \vdots \\ \bm{s}_{d_h} \cos \theta_{d_h} \end{bmatrix}
\end{equation} The expected squared norm of $\bm{z}$ is then given by: \begin{equation}
    \mathbb{E}[\Vert \bm{z} \Vert_2^2] = \sum_{l=1}^{d_h} \bm{s}_{l}^2 \mathbb{E}[\cos^2 \theta_l] 
\end{equation} Using the trigonometric identity $\cos^2 (\theta) = \frac{1 + \cos (2\theta)}{2}$ and the following integrals:
\begin{align}
    & \int_0^\pi \sin^{n-2} (\theta) \; d\theta = \frac{\Gamma (\frac{n-1}{2}) \Gamma (\frac{1}{2})}{\Gamma (\frac{n}{2})} = \frac{\sqrt{\pi} \Gamma (\frac{n-1}{2})}{\Gamma (\frac{n}{2})} \\
    & \int_0^\pi \cos(2\theta) \sin^{n-2} (\theta) \; d\theta = 0
\end{align} 
where the second integral vanishes due to the symmetry of $\cos (2\theta)$ about $\theta = \frac{\pi}{2}$, we compute the expectation: \begin{align} 
    \mathbb{E} [\cos^2 (\theta_l) ] & = \int_0^\pi \cos^2 (\theta_l) \underbrace{\frac{\Gamma (\frac{n}{2})}{\sqrt{\pi} \Gamma (\frac{n-1}{2})} \sin^{n-2} (\theta_l)}_{f (\theta_l)} \; d\theta_l \\
    & = \frac{\Gamma (\frac{n}{2})}{\sqrt{\pi} \Gamma (\frac{n-1}{2})} \int_0^\pi \cos^2 (\theta_l) \sin^{n-2} (\theta_l) d \theta \\
    & = \frac{\Gamma (\frac{n}{2})}{2\sqrt{\pi} \Gamma (\frac{n-1}{2})} \times \frac{\sqrt{\pi} \Gamma (\frac{n-1}{2})}{\Gamma (\frac{n}{2})} = \frac{1}{2}
\end{align} Thus, by setting $\bm{s}_l = \sqrt{\frac{2}{d_h}}$ for all $\ell \in \{1, \cdots, d_h\}$, we expect that the expected norm $\mathbb{E}[\Vert \bm{z} \Vert_2^2]$ is $1$ at initialization.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Implementation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Implementation Details}
\label{appendix:implementation_details}

Listings \ref{implementation:scaler}, \ref{implementation:input_embedding} and \ref{implementation:mlp} provide the Google JAX implementation of scaling vector (Section~\ref{subsection:init_and_update}), input embedding (Section~\ref{subsection:input_embedding}), and MLP block (Section~\ref{subsection:feature_encoding}), respectively.

\vspace{-1mm}
\input{implementations/scaler}
\vspace{-3mm}
\input{implementations/input_embedding}

\clearpage 

\input{implementations/mlp}

\clearpage
\section{Hyperparameters}
\label{appendix:hyperparameters}

For all experiments, we use consistent hyperparameters across benchmarks. The default settings are listed in Table~\ref{table:hyperparameters}. 
\vspace{-5mm}%
\input{tables/hyperparameter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Offline RL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage

\section{Offline RL}
\label{appendix:offline_rl}

In this section, we assess whether the SimbaV2 architecture also provide benefits in offline RL, training from a stationary distribution. 
We adopt the minimalist offline RL method from \cite{fujimoto2021td3bc}, where the behavioral cloning loss is integrated into the reinforcement learning objective. The objective is defined as:
\begin{equation}
\pi \approx \arg\max_{\pi} \mathbb{E}_{(s, a) \sim D} \left[ Q(s, \pi(s)) - \lambda \left| \mathbb{E}_{s \sim D} [Q(s, \pi(s))] \right| \cdot (\pi(s) - a)^2 \right]
\end{equation}
where we used $\lambda = 0.1$, as in \citep{fujimoto2023td7}, and no parameter tuning is performed.

\subsection{Experimental Setup}

\textbf{Environment.} We use 9 MuJoCo tasks from the D4RL \cite{fu2020d4rl} benchmark, covering 3 environments (\texttt{HalfCheetah}, \texttt{Hopper}, \texttt{Walker2d}) and 3 difficulty levels (\texttt{Medium}, \texttt{Medium-Replay}, \texttt{Medium-Expert}).

\textbf{Baselines.} We compare SimbaV2 against standard offline RL methods: Percentile BC, Decision Transformer (DT, \cite{chen2021dt}), Diffusion Q-Learning (DQL, \cite{wang2022dql}), Implicit Diffusion Q-Learning (IDQL, \cite{hansen2023idql}), Conservative Q-Learning (CQL, \cite{chen2021dt}), TD3+BC \cite{fujimoto2021td3bc}, Implicit Q-Learning (IQL, \cite{kostrikov2021iql}), Extreme Q-Learning ($\mathcal{X}$-QL, \cite{garg2023xql}), and TD7+BC \cite{fujimoto2023td7}. 

The results for Percentile BC, DT, DQL, and IDQL is from \citep{hansen2023idql}, while CQL, TD3+BC, IQL, $\mathcal{X}$-QL, and TD7 results come from \citep{fujimoto2023td7}.

\textbf{Metrics.} Following the standard offline RL protocol \cite{fu2020d4rl}, we normalize the score of each environment based on the expert trajectory in the dataset.

\textbf{Training.}  We use the same training configuration as in online RL (Appendix~\ref{appendix:hyperparameters}), with a learning rate decaying linearly from $1 \times 10^{-4}$ to $1 \times 10^{-5}$ over 100 epochs, and include an additional behavioral cloning loss.

\subsection{Results}

Table~\ref{table:offline_rl} reports the performance of SimbaV2 + BC, averaged over 10 random seeds.

\vspace{-2mm}
\input{tables/offline_rl}

With minimal changes, SimbaV2 performs highly competitively with existing offline RL algorithms, with statistically significantly better performance on \texttt{Hopper}. 
Again, this experimental results reinforces the importance of architectural design over complex algorithmic modifications.
We believe our architectural approach offers exciting future potential for bridging offline and online RL \cite{ball2023rlpd, zhou2024wsrl}.










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Baselines
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Baselines}
\label{appendix:baselines_online}

\textbf{PPO}~\citep{lillicrap2015ddpg}. Proximal Policy Optimization (PPO) is an on-policy policy gradient method that constrains updated policies to remain proximal to the old policies to circumvent performance collapse. Results for Gym - MuJoCo and DMC were obtained from \citet{fujimoto2025mrq}, which are averaged over $10$ seeds. 

\textbf{SAC}~\citep{haarnoja2018sac}. Soft Actor-Critic (SAC) is an off-policy actor-critic algorithm in which the actor simultaneously maximizes expected return and entropy, encouraging both stability and exploration. Results for Gym-MuJoCo were obtained from \citet{fujimoto2023td7}. These scores are average over $10$ random seeds. For DMC, MyoSuite, and HBench tasks, we use the results from~\citet{lee2024simba} which were obtained by running the official repository for 10 random seeds. 

\textbf{TD3}~\citep{fujimoto2018td3}. Twin Delayed DDPG (TD3) is an off-policy actor-critic algorithm that mitigates Q-overestimation bias via three key techniques: (i) clipped double Q-learning, (ii) delayed policy updates, (iii) target policy smoothing. Results for Gym-MuJoCo were obtained from Table 1 of \citet{fujimoto2023td7}. These scores are averaged over $10$ random seeds. 

\textbf{TD3+OFE}~\citep{ota2020ofenet}. By replacing the encoder with an Online Feature Extractor (OFE)—trained via a dynamics prediction task to produce high-dimensional representations of observation-action pairs—TD3+OFE outperforms the original TD3 without requiring any hyperparameter adjustments. Results for Gym-MuJoCo were obtained from Table 1 of \citet{fujimoto2023td7}. These scores are averaged over $10$ random seeds. We attach these results into Table~\ref{table:online_rl} by TD3-normalizing the scores as outlined in Appendix \ref{appendix:environments_gym}. 

\textbf{TQC}~\citep{kuznetsov2020tqc}. Truncated Quantile Critic (TQC) proposes to truncate the return distribution of the distributional critics to flexibly balance between under- and overestimation bias of Q-value. Results for Gym-MuJoCo were taken directly from Table 1 of \citet{fujimoto2023td7}. We attach these results into Table~\ref{table:online_rl} by TD3-normalizing the scores as described in Appendix \ref{appendix:environments_gym}. 

% \textbf{REDQ}~\citep{chen2021redq}. Randomized Ensembled Double Q-Learning (REDQ) expands clipped double Q-learning from two Q-networks to an ensemble of ten to control estimation bias and variance, and enhance training stability.

% \textbf{DroQ}~\citep{chen2021redq}. Dropout Q-Function (DroQ) reduces the computational burden of REDQ by using a smaller ensemble of Q functions while employing Dropout and Layer Normalization to stabilize training against Dropout-induced noise.

\textbf{DreamerV3}~\citep{hafner2023dreamerv3}. DreamerV3 encodes sensory inputs into categorical representations to build a learned world model, enabling long-horizon behavior learning in its compact latent space. Results for Gym-MuJoCo and DMC were obtained from \citet{fujimoto2025mrq}, which are averaged over $10$ seeds. For MyoSuite, and HBench tasks, we use the results from \citet{lee2024simba} which were obtained by running the official repository (\url{https://github.com/SonyResearch/simba}) over 3 random seeds.

% \textbf{SR-SAC}~\citep{doro2022rrbarrier}. Scaled-by-Resetting (SR) proposes simple replay ratio-scalable algorithms by frequently resetting all the agent parameters. For DMC hard tasks, the results were obtained by running the official repository (\url{https://github.com/proceduralia/high_replay_ratio_continuous_control}) over $5$ random seeds. We used $\text{UTD} = 32$.

\textbf{TD7}~\citep{fujimoto2023td7}. TD7 improves TD3 by combining TD3 with four key improvements: (i) state-action representation learning (SALE), (ii) prioritized experience replay, (iii) policy checkpoints, and (iv) additional behavior cloning loss for offline RL. Results for Gym-MuJoCo and DMC were obtained from \citet{fujimoto2025mrq}, which are averaged over $10$ seeds. For MyoSuite, and HBench tasks, we use the results from \citet{lee2024simba} which were obtained by running the official repository (\url{https://github.com/SonyResearch/simba}) over $5$ random seeds.

\textbf{TD-MPC2}~\citep{hansen2023tdmpcv2}. TD-MPC2 is a model-based algorithm that learns an implicit (decoder-free) world model through multiple dynamics prediction tasks and performs local trajectory optimization within the learned latent space. Results for Gym-MuJoCo and DMC were obtained from \citet{fujimoto2025mrq}, which are averaged over $10$ seeds. For MyoSuite, and HBench tasks, we use the results from \citet{lee2024simba} which were obtained by running the official repository (\url{https://github.com/SonyResearch/simba}) over 3 random seeds.

\textbf{CrossQ}~\citep{bhatt2024crossq}. CrossQ achieves superior performance and sample efficiency with low replay ratio, by removing target networks and employing careful batch normalization. Results for Gym-MuJoCo were obtained by running the official repository (\url{https://github.com/adityab/CrossQ}) for 10 random seeds.

\textbf{iQRL}~\citep{scannell2024iqrl}. Implicitly Quantized Reinforcement Learning (iQRL) is a representation learning technique of model-free RL that prevents representation collapse and improve sample-efficiency via latent quantization. For the DMC hard tasks, results averaged over 3 random seeds were obtained directly from the authors.

\textbf{BRO}~\citep{nauman2024bro}. Bigger, Regularized, Optimistic (BRO) scales the critic network of SAC by integrating distributional Q-learning, optimistic exploration, and periodic resets. Results for Gym-MuJoCo and DMC Easy were obtained by running the official repository (\url{https://github.com/naumix/BiggerRegularizedOptimistic}) for 5 random seeds. For DMC hard, MyoSuite, and HBench tasks, we use the results from \citet{lee2024simba} which were obtained by running the official repository (\url{https://github.com/SonyResearch/simba}) over $5$ random seeds for HBench tasks and 10 random seeds for DMC hard and MyoSuite tasks. Unless stated otherwise, we set update-to-data (UTD) ratio to be $2$. 

% \textbf{CQN-AS}~\citep{seo2024cqnas}. Coarse-to-fine Q-Network with Action Sequence (CQN-AS) is a value-based RL algorithm that learns a critic network outputting Q-values over an action sequence to improve learning from noisy trajectories, such as human-collected demonstrations. Results for the HBench tasks (without dexterous hands as explained in Appendix~\ref{appendix:environments_hb}) were obtained by running the official repository (\url{https://github.com/younggyoseo/CQN-AS}) over $5$ random seeds. 

\textbf{MAD-TD}~\citep{voelcker2024madtd}. Model-Augmented Data for Temporal Difference learning (MAD-TD) aims to stabilize high UTD training by mixing a small fraction $\alpha$ of model-generated on-policy data with real off-policy replay data. For the DMC hard tasks, results averaged over $10$ random seeds were obtained directly from the authors using the best algorithm setting ($\text{UTD} = 8$, $\alpha = 0.05$). 

\textbf{MR.Q}~\citep{fujimoto2025mrq}. Model-based Representations for Q-learning (MR.Q) is a model-free algorithm that uses model-based objectives, such as dynamics and reward prediction, to obtain rich representation for actor-critic agent. We use the results for Gym-MuJoCo and DMC from \citet{fujimoto2025mrq} which were obtained by running the official repository (\url{https://github.com/facebookresearch/MRQ}) over $10$ random seeds.

\textbf{Simba}~\citep{lee2024simba}. SimBa is an architecture designed to scale up parameters in deep reinforcement learning by injecting a simplicity bias with observation normalizer, residual blocks, and layer normalizations. For Gym-MuJoCo, DMC, MyoSuite, and HBench tasks, we use the results from \citet{lee2024simba} which were obtained by running the official repository (\url{https://github.com/SonyResearch/simba}) over $15$ random seeds for DMC hard tasks and $10$ random seeds otherwise. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Environment Details
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\begin{table}[ht!]
\centering
\caption{\textbf{Environment details.} We list the episode length, action repeat for each domain, total environment steps, and performance metrics used for benchmarking SimbaV2.}
%
\vspace{0.05in}
\begin{tabular}{lcccc}
\toprule
& \textbf{Gym} & \textbf{DMC} & \textbf{MyoSuite} & \textbf{HumanoidBench} \\ \midrule
Episode length      & $1,000$ & $1,000$ & $100$ & $500$ - $1,000$ \\
Action repeat       & $1$ & $2$ & $2$ & $2$ \\
Effective length    & $1,000$ & $500$ & $50$ & $250$ - $500$ \\
Total env. steps    & $1$M & $1$M & $1$M & $1$M \\
Performance metric  & Average Return & Average Return & Average Success & Average Return \\ \bottomrule
\end{tabular}%

\label{tab:appendix_environment_details}
\vspace{-0.1in}
\end{table}

\section{Environment Details}
\label{appendix:environments}

This section outlines the benchmark environments used in our evaluation. A complete list of all tasks from each benchmark, including their observation and action dimensions, is provided at the end of this section. Additionally, Table~\ref{tab:appendix_environment_details} outlines the episode length, action repeat, total number of environment steps, and performance metrics for each task domain. % Visualizations of each environment are shown in Figure~\ref{figure:appendix_environment_visualization}.

\subsection{Gym - MuJoCo}
\label{appendix:environments_gym}

Gym~\citep{brockman2016openai, towers2024gymnasium} is a suite of benchmark environments spanning finite MDPs to Multi-Joint dynamics with Contact~\citep[MuJoCo]{todorov2012mujoco} simulations. It offers a diverse range of tasks, including classic Atari games, small-scale tasks such as Toy Text and classic controls, as well as physics-based continuous robot control. For our experiments, we focus on 5 locomotion tasks within MuJoCo environments, which simulate complex physical interactions involving multi-body dynamics and contact forces. A complete list of these tasks is provided in Table~\ref{tab:appendix_gym_mujoco_tasks}. Note that we use the \texttt{v4} version.

For comparison across different score scales of each task, all MuJoCo scores are normalized using TD3 and the random score for each task, as provided in TD7~\citep{fujimoto2023td7}. 

$$
\text{TD3-Normalized}(x) := \frac{x - \text{random score}}{\text{TD3 score} - \text{random score}}
$$

\begin{table}[ht!]
\centering
\parbox{\textwidth}{
\centering
\vspace{0.05in}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Random} & \textbf{TD3} \\ \midrule
\texttt{Ant-v4} & $-70.288$ & $3942$ \\
\texttt{HalfCheetah-v4} & $-289.415$ & $10574$ \\
\texttt{Hopper-v4} & $18.791$ & $3226$ \\
\texttt{Humanoid-v4} & $120.423$ & $5165$ \\
\texttt{Walker2d-v4} & $2.791$ & $3946$ \\ \bottomrule
\end{tabular}}
%\vspace{10mm}
\end{table}


\subsection{DeepMind Control Suite}
\label{appendix:environments_dmc}

DeepMind Control Suite~\citep[DMC]{tassa2018dmc} is a standard continuous control benchmarks, encompassing a variety of locomotion and manipulation tasks with varying levels of complexity. These tasks range from simple low-dimensional settings ($\mathcal{O} \in \mathbb{R}^{3}$, $\mathcal{A} \in \mathbb{R}^{1}$) to highly complex scenarios ($\mathcal{O} \in \mathbb{R}^{223}$, $\mathcal{A} \in \mathbb{R}^{38}$). Our evaluation includes 27 DMC tasks, divided into two categories: DMC-Easy\&Medium and DMC-Hard. All Humanoid and Dog tasks are grouped as DMC-Hard, while the rest are fall under DMC-Easy\&Medium. Comprehensive lists of DMC-Easy\&Medium and DMC-Hard are available in Tables~\ref{tab:appendix_dmc_easy_medium_tasks} and~\ref{tab:appendix_dmc_hard_tasks}, respectively.

\subsection{MyoSuite}
\label{appendix:environments_myo}

MyoSuite~\citep{caggiano2022myosuite} models human motor control using musculoskeletal simulations of the human elbow, wrist, and hand, focusing on physiologically accurate movements. It provides benchmarks for intricate real-world object manipulation, ranging from simple posing tasks to the simultaneous manipulation of two Baoding balls. Our evaluation focuses on 10 MyoSuite tasks involving the  hand. As defined by the authors, each task is categorized as \texttt{hard} when the goal is randomized; otherwise the goal is fixed. The full list of MyoSuite tasks is presented in Table~\ref{tab:appendix_myosuite_tasks}.

\subsection{HumanoidBench}
\label{appendix:environments_hb}

HumanoidBench~\citep{sferrazza2024humanoidbench} serves as a high-dimensional simulated robot learning benchmark, leveraging the Unitree H1 humanoid robot equipped with dexterous hands. It encompasses a diverse set of whole-body control tasks, spanning from fundamental locomotion to complex human-like activities that require refined manipulation. In our experiments, we concentrate on 14 locomotion tasks. A comprehensive list of tasks is provided in Table~\ref{tab:appendix_hb_tasks}. 

Note that the locomotion tasks do not necessitate hand dexterity. Therefore, to reduce the complexity arising from high degrees of freedom (DoF) and complex dynamics, we streamline the environments setup by excluding the hands of humanoid. For example, in case of \texttt{walk}, this drastically declines the dimension of the observation and action spaces by approximately 66\%. 

\begin{table}[ht!]
\centering
\parbox{\textwidth}{
\centering
\vspace{0.05in}
\begin{tabular}{lcc}
\toprule
\texttt{walk} & \textbf{Without hand} & \textbf{With 2 hand} \\ \midrule
Observation dim $\vert \mathcal{O} \vert$ & $51$ & $151$ \\
Action dim $\vert \mathcal{A} \vert$ & $19$ & $61$ \\
DoF (body) & $25$ & $25$ \\
DoF (two hands) & $0$ & $50$ \\\bottomrule
\end{tabular}}
%\vspace{10mm}
\end{table}

For comparison across different score scales of each task, all HumanoidBench scores are normalized using each task's target success score provided by the authors and random score. Random scores are measured by the average undiscounted returns over $10$ episodes of random agent. Each measurement is repeated over $10$ seeds.  

$$
\text{Success-Normalized}(x) := \frac{x - \text{random score}}{\text{Target success score} - \text{random score}}
$$

\begin{table}[ht!]
\centering
\parbox{0.8\textwidth}{
\centering
\vspace{0.05in}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Random} & \textbf{Target Success} \\ \midrule
\texttt{h1-balance-simple} & $9.391$ & $800$ \\
\texttt{h1-balance-hard} & $9.044$ & $800$ \\
\texttt{h1-crawl} & $272.658$ & $700$ \\
\texttt{h1-hurdle} & $2.214$ & $700$ \\
\texttt{h1-maze} & $106.441$ & $1200$ \\
\texttt{h1-pole} & $20.09$ & $700$ \\
\texttt{h1-reach} & $260.302$ & $12000$ \\
\texttt{h1-run} & $2.02$ & $700$ \\
\texttt{h1-sit-simple} & $9.393$ & $750$ \\
\texttt{h1-sit-hard} & $2.448$ & $750$ \\
\texttt{h1-slide} & $3.191$ & $700$ \\ 
\texttt{h1-stair} & $3.112$ & $700$ \\ 
\texttt{h1-stand} & $10.545$ & $800$ \\ 
\texttt{h1-walk} & $2.377$ & $700$ \\ \bottomrule
\end{tabular}}
%\vspace{10mm}
\end{table}


\begin{table}[ht!]
\centering
\parbox{0.8\textwidth}{
\caption{\textbf{Gym-MuJoCo.} We evaluate a total of $5$ continuous control tasks from the Gym-MuJoCo benchmark. Below, we provide a list of all the tasks considered. The baseline performance for each task is reported at $1$M environment steps.}
\label{tab:appendix_gym_mujoco_tasks}
\centering
\vspace{0.05in}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Observation dim} $|\mathcal{O}|$ & \textbf{Action dim} $|\mathcal{A}|$ \\ \midrule
\texttt{Ant-v4} & $27$ & $8$ \\
\texttt{HalfCheetah-v4} & $17$ & $6$ \\
\texttt{Hopper-v4} & $11$ & $3$ \\
\texttt{Humanoid-v4} & $376$ & $17$ \\
\texttt{Walker2d-v4} & $17$ & $6$ \\ \bottomrule
\end{tabular}}
%\vspace{10mm}
\end{table}

\begin{table}[ht!]
\centering
\parbox{0.8\textwidth}{
\caption{\textbf{DMC-Easy Complete List.} We evaluate a total of $21$ continuous control tasks from the DMC-Easy benchmark. Below, we provide a list of all the tasks considered. The baseline performance for each task is reported at $1$M environment steps.}
\label{tab:appendix_dmc_easy_medium_tasks}
\centering
\vspace{0.05in}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Observation dim} $|\mathcal{O}|$ & \textbf{Action dim} $|\mathcal{A}|$ \\ \midrule
\texttt{acrobot-swingup} & $6$ & $1$ \\
\texttt{ball-in-cup-catch} & $6$ & $1$ \\
\texttt{cartpole-balance} & $5$ & $1$ \\
\texttt{cartpole-balance-sparse} & $5$ & $1$ \\
\texttt{cartpole-swingup} & $5$ & $1$ \\
\texttt{cartpole-swingup-sparse} & $5$ & $1$ \\
\texttt{cheetah-run} & $17$ & $6$ \\
\texttt{finger-spin} & $9$ & $2$ \\
\texttt{finger-turn-easy} & $12$ & $2$ \\
\texttt{finger-turn-hard} & $12$ & $2$ \\
\texttt{fish-swim} & $24$ & $5$ \\
\texttt{hopper-hop} & $15$ & $4$ \\
\texttt{hopper-stand} & $15$ & $4$ \\
\texttt{pendulum-swingup} & $3$ & $1$ \\
\texttt{quadruped-run} & $78$ & $12$ \\
\texttt{quadruped-walk} & $78$ & $12$ \\
\texttt{reacher-easy} & $6$ & $2$ \\
\texttt{reacher-hard} & $6$ & $2$ \\
\texttt{walker-run} & $24$ & $6$ \\
\texttt{walker-stand} & $24$ & $6$ \\
\texttt{walker-walk} & $24$ & $6$ \\ \bottomrule
\end{tabular}}
%\vspace{10mm}
\end{table}

\begin{table}[ht!]
\centering
\parbox{0.8\textwidth}{
\caption{\textbf{DMC-Hard Complete List.} We evaluate a total of $7$ continuous control tasks from the DMC-Hard benchmark. Below, we provide a list of all the tasks considered. The baseline performance for each task is reported at $1$M environment steps.}
\label{tab:appendix_dmc_hard_tasks}
\centering
\vspace{0.05in}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Observation dim} $|\mathcal{O}|$ & \textbf{Action dim} $|\mathcal{A}|$ \\ \midrule
\texttt{dog-run} & $223$ & $38$ \\
\texttt{dog-trot} & $223$ & $38$ \\
\texttt{dog-stand} & $223$ & $38$ \\
\texttt{dog-walk} & $223$ & $38$ \\
\texttt{humanoid-run} & $67$ & $24$ \\
\texttt{humanoid-stand} & $67$ & $24$ \\
\texttt{humanoid-walk} & $67$ & $24$ \\ \bottomrule
\end{tabular}%
}
\vspace{-0.1in}
\end{table}

\clearpage

\begin{table}[ht!]
\centering
\parbox{0.8\textwidth}{
\caption{\textbf{MyoSuite Complete List.} We evaluate a total of $10$ continuous control tasks from the MyoSuite benchmark including both fixed-goal and randomized-goal (\texttt{hard}) settings. Below, we provide a list of all the tasks considered. The baseline performance for each task is reported at $1$M environment steps.}
\label{tab:appendix_myosuite_tasks}
\centering
\vspace{0.05in}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Observation dim} $|\mathcal{O}|$ & \textbf{Action dim} $|\mathcal{A}|$ \\ \midrule
\texttt{myo-key-turn} & $93$ & $39$ \\
\texttt{myo-key-turn-hard} & $93$ & $39$ \\ 
\texttt{myo-obj-hold} & $91$ & $39$ \\
\texttt{myo-obj-hold-hard} & $91$ & $39$ \\
\texttt{myo-pen-twirl} & $83$ & $39$ \\
\texttt{myo-pen-twirl-hard} & $83$ & $39$ \\
\texttt{myo-pose} & $108$ & $39$ \\
\texttt{myo-pose-hard} & $108$ & $39$ \\
\texttt{myo-reach} & $115$ & $39$ \\
\texttt{myo-reach-hard} & $115$ & $39$ \\
\bottomrule
\end{tabular}
}
\vspace{-0.1in}
\end{table}
\vspace{10mm}

\begin{table}[ht!]
\centering
\parbox{0.8\textwidth}{
\caption{\textbf{HumanoidBench Complete List.} We evaluate a total of $14$ continuous control locomotion tasks from the HumanoidBench benchmark that simulates the UniTree H1 humanoid robot. Below, we provide a list of all the tasks considered. The baseline performance for each task is reported at $1$M environment steps.}
\label{tab:appendix_hb_tasks}
\centering
\vspace{0.05in}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Observation dim} $|\mathcal{O}|$ & \textbf{Action dim} $|\mathcal{A}|$ \\ \midrule
\texttt{h1-balance-hard} & $77$ & $19$ \\
\texttt{h1-balance-simple} & $64$ & $19$ \\
\texttt{h1-crawl} & $51$ & $19$ \\
\texttt{h1-hurdle} & $51$ & $19$ \\
\texttt{h1-maze} & $51$ & $19$ \\
\texttt{h1-pole} & $51$ & $19$ \\
\texttt{h1-reach} & $57$ & $19$ \\
\texttt{h1-run} & $51$ & $19$ \\
\texttt{h1-sit-simple} & $51$ & $19$ \\
\texttt{h1-sit-hard} & $64$ & $19$ \\
\texttt{h1-slide} & $51$ & $19$ \\ 
\texttt{h1-stair} & $51$ & $19$ \\ 
\texttt{h1-stand} & $51$ & $19$ \\ 
\texttt{h1-walk} & $51$ & $19$ \\ \bottomrule
\end{tabular}%
}
\vspace{-0.1in}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Training Stability
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Training Stability}
\label{appendix:effective_lr}

In Section~\ref{section:empirical_analysis}, we investigated the training dynamics of SimbaV2 on DMC-Hard and HBench-Hard via four metrics: feature norm, parameter norm, gradient norm, and effective learning rate (ELR) of neural networks. This section presents these standalone metrics for SimbaV2 to highlight its stable behavior throughout training.

\textbf{Effective Learning Rate.} We base our notion of ELR on the \textit{effective step size} of \citet{kodryan2022training}, omitting the global learning rate $\eta$ and using dimension-based weighting $w_i = \frac{\vert \bm{\theta}_i \vert}{\sum_{j=1}^N \vert \bm{\theta}_j \vert}$ instead of squared-parameter-norm weighting $w_i = \frac{\Vert \bm{\theta}_i \Vert^2}{\sum_{j=1}^N \Vert \bm{\theta}_j \Vert^2}$. 
\begin{definition}[Effective Learning Rate]
\label{definition:effective_learning_rate}
   Let $\bm{\theta} = \{ \bm{\theta}_i \}_{i=1}^N$ be the parameter set of a neural network, and $\bm{g}_i$ be the back-propagated gradient associated with $\bm{\theta}_i$. The \textit{(total) effective learning rate} $\mathrm{ELR}$ of the network is defined as: 
   \begin{equation} \mathrm{ELR} \triangleq \sqrt{\sum_{i=1}^N w_i \frac{\Vert \bm{g}_i \Vert^2}{\Vert \bm{\theta}_i \Vert^2}} \end{equation} 
   where $w_i = \frac{\vert \bm{\theta}_i \vert}{\sum_{j=1}^N \vert \bm{\theta}_j \vert}$. Intuitively, our $\mathrm{ELR}$ measures the ``effective'' gradient step—per parameter dimension—before scaled by the global learning rate.
\end{definition}

\textbf{Metrics.} To reflect dimensional contributions across layers, we also apply the same weighting $w_i$ when computing the feature norm, parameter norm, and gradient norm. For instance, our gradient norm is defined as: \begin{equation}
    \Vert \bm{g}_i \Vert^2 \triangleq \sum_{i=1}^N w_i \Vert \bm{g}_i \Vert_2^2
\end{equation} where $\Vert \cdot \Vert_2$ is the standard $\ell_2$-norm (Frobenius norm $\Vert \cdot \Vert_F$ in case of matrices). Analogous expressions are applied for feature and parameter norms. We separate encoder layers (all layers preceding the output) from predictor layers (all layers after) to capture their distinct roles in the network. Average returns are normalized by maximum score $1000$ for DMC-Hard, by success and random scores for HBench-Hard (Appendix~\ref{appendix:environments_hb}). 

\textbf{Results.} Figure~\ref{figure:appendix_analysis} shows the tracked metrics over 1 million training steps. Certain features (e.g., logits) and parameters (e.g., scalers and interpolation vectors) may occasionally exceed unit norm, the overall parameter norms are tightly controlled (Figure~\ref{figure:appendix_analysis}.(b)-(c)), and gradient magnitudes are consistently balanced across modules (Figure~\ref{figure:appendix_analysis}.(d)). This leads to the consistent trend and scales of their ELRs over time. We hypothesize that this stable behavior contributes to underpins \textsc{SimbaV2}’s improved performance and scalability.

\vspace{-3mm}
\begin{figure*}[ht!]
\centering
\begin{center}
\includegraphics[width=\textwidth]{figures/appendix_analysis.pdf}
\end{center}
\vspace{-6mm}
\caption{\textbf{SimbaV2 Training Dynamics.} We track 4 metrics during training to understand the learning dynamics of SimbaV2: \textbf{(a)} Average normalized return across tasks. \textbf{(b)} Weighted sum of $\ell_2$-norms of all intermediate features in critic. \textbf{(c)} Weighted sum of $\ell_2$-norms of all critic parameters \textbf{(d)} Weighted sum of $\ell_2$-norms of all gradients in critic \textbf{(e)} Effective learning rate (ELR) of the critic. On both environments, SimbaV2 maintains feature and parameter norms aligned, producing consistent gradient norms and ELRs.}
\label{figure:appendix_analysis}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Scalability Effect of Hyperspherical Normalization
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Scalability Effect of Hyperspherical Normalization}
\label{appendix:scalability_effect}

In Section~\ref{section:empirical_analysis}, we observe that SimbaV2 consistently scales with an increasing update-to-data (UTD) ratio, even without reinitialization, while Simba saturates at a ratio of $2$. However, this raises the question of whether hyperspherical normalization is critical for UTD scaling. This section investigates the effectiveness of hyperspherical normalization in scalability.

\textbf{Experimental Setup.} In this experiment, we examine a ``Simba-like'' variant, named Simba+, which incorporates distributional critic and reward scaling but only excludes the hyperspherical normalization. In other words, Simba+ is identical to SimbaV2 except that it excludes hyperspherical normalization. On DMC-Hard tasks, we compare SimbaV2, Simba, and Simba+ under varying model sizes and UTD ratios to determine the role of hyperspherical normalization in scaling performance.

\textbf{Result.} Figure~\ref{fig:appendix_utd_scaling} shows the scaling results. In Figure~\ref{fig:appendix_utd_scaling} (left), all three methods benefit from increased model capacity, but Simba+ slightly underperforms at larger parameter counts. More critically, Simba and Simba+ both plateau when the UTD ratio surpasses $2$ in Figure~\ref{fig:appendix_utd_scaling} (right), while SimbaV2 continues to improve. These results confirm that hyperspherical normalization is truly indispensable for UTD scaling.

{
\centering
\vspace{5mm}%
\includegraphics[width=0.6\textwidth]{figures/appendix_utd_scaling.pdf}%
\vspace{-4mm}
\captionof{figure}{
    \textbf{Performance Scaling under DMC-Hard.} We compare SimbaV2, Simba+, and Simba as scaling the number of model parameters by increasing the critic network width and UTD ratio. Simba+ fails to scale effectively at higher UTD ratios, highlighting the essential role of the hyperspherical normalization for scalability.
}
\vspace{-4mm}
\label{fig:appendix_utd_scaling}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Complete UTD Scaling Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Complete UTD Scaling Results}
\label{appendix:complete_utd_scalability}

\subsection{Gym - MuJoCo}
\vspace{-5mm}%
\input{tables/utd_mujoco}
{
\centering
\vspace{1mm}%
\includegraphics[width=0.99\textwidth]{figures/appendix_utd_gym_learning_curve.pdf}%
\vspace{-4mm}
\captionof{figure}{
    \textbf{Gym-MuJoCo UTD Scaling Learning Curves.} Average episode return (1k) for the Gym-MuJoCo environment. Results are averaged over $5$ random seeds, and the shaded areas indicate 95\% bootstrap confidence intervals.
    \vspace{-4mm}
    }%
\label{fig:appendix_utd_gym_learning_curve}%
}

\clearpage
\subsection{Deepmind Control Suite - Easy}
\vspace{-5mm}%
\input{tables/utd_dmc_em}

\clearpage
{
\centering
\vspace{-4mm}%
\includegraphics[width=0.9\textwidth]{figures/appendix_utd_dmc_em_learning_curve.pdf}%
\vspace{-1em}
\captionof{figure}{
    \textbf{DMC-Easy UTD Scaling Learning Curves.} Average episode return for the DMC-Easy environment. Results are averaged over $5$ random seeds, and the shaded areas indicate 95\% bootstrap confidence intervals.
    \vspace{-4mm}
    }%
\label{fig:appendix_utd_dmc_em_learning_curve}%
}

\clearpage
\subsection{Deepmind Control Suite - Hard}
\vspace{-5mm}%
\input{tables/utd_dmc_hard}
{
\centering
\vspace{1mm}%
\includegraphics[width=0.99\textwidth]{figures/appendix_utd_dmc_hard_learning_curve.pdf}%
\vspace{-4mm}
\captionof{figure}{
    \textbf{DMC-Hard UTD Scaling Learning Curves.} Average episode return for the DMC-Hard environment. Results are averaged over $5$ random seeds, and the shaded areas indicate 95\% bootstrap confidence intervals.
    \vspace{-4mm}
    }%
\label{fig:appendix_utd_dmc_hard_learning_curve}%
}

\clearpage
\subsection{MyoSuite}
\vspace{-5mm}%
\input{tables/utd_myosuite}
{
\centering
\vspace{1mm}%
\includegraphics[width=0.99\textwidth]{figures/appendix_utd_myo_learning_curve.pdf}%
\vspace{-4mm}
\captionof{figure}{
    \textbf{MyoSuite UTD Scaling Learning Curves.} Average episode success rate (\%) for the MyoSuite environment. Results are averaged over $5$ random seeds, and the shaded areas indicate 95\% bootstrap confidence intervals.
    \vspace{-4mm}
    }%
\label{fig:appendix_utd_myo_learning_curve}%
}
\clearpage

\subsection{Humanoid Bench}
\vspace{-5mm}%
\input{tables/utd_hbench}

\clearpage
\begin{figure}[p]{
\centering
\vspace{1mm}%
\includegraphics[width=0.99\textwidth]{figures/appendix_utd_hb_learning_curve.pdf}%
\vspace{-4mm}
\captionof{figure}{
    \textbf{Humanoidbench UTD Scaling Learning Curves.} Average episode return for the HumanoidBench environment. Results are averaged over $5$ random seeds, and the shaded areas indicate 95\% bootstrap confidence intervals. The black dotted line indicates the success score of each tasks (Appendix~\ref{appendix:environments_hb})
    \vspace{-4mm}
    }%
    \label{fig:appendix_utd_hb_learning_curve}%
}\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Complete Main Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Complete Main Results}
\label{appendix:complete}

This section provides learning curves and final performance for each online RL task across the evaluated algorithms.

\textbf{Learning Curve.} For visibility of learning curve, we focus on DreamerV3 \cite{hafner2023dreamerv3}, TD7 \cite{fujimoto2023td7}, TD-MPC2 \cite{hansen2023tdmpcv2}, MR.Q \cite{fujimoto2025mrq}, and Simba \cite{lee2024simba} as main baselines, selected for their strong performance and community adoption. We omit curves for algorithms with unavailable raw samples at each task.

\textbf{Confidence Interval.} The light-colored area in the figures and the gray-shaded, bracketed terms in the tables represent 95\% bootstrap confidence intervals. For each task evaluated over $n$ random seeds, the $95\%$ bootstrap confidence interval $\mathrm{CI}$ is computed as: \begin{equation*}
\mathrm{CI} = \left[\mu - 1.96 \times \frac{\sigma}{\sqrt{n}}, \mu + 1.96 \times \frac{\sigma}{\sqrt{n}}\right]
\end{equation*} where $\mu$ and $\sigma$ are the sample mean and standard deviation (with Bessel's correction) of the evaluation, respectively. For aggregated scores (mean, median, and interquartile mean), confidence intervals are computed over all $n \times T$ raw samples, where $n$ and $T$ are the number of evaluated random seeds and tasks in the benchmark, respectively. For algorithms with only average scores for each task available, we approximate the CI of aggregated scores using these averages (denoted with gray-colored $\dagger$). We caution that this estimation may be inaccurate.

\clearpage
\subsection{Gym - MuJoCo}
\vspace{-5mm}
\input{tables/full_mujoco}
{
\centering
\vspace{-3mm}%
\includegraphics[width=0.99\textwidth]{figures/appendix_gym_learning_curve.pdf}%
\
\vspace{-5mm}
\captionof{figure}{
    \textbf{Gym-MuJoCo Learning Curves.} Average episode return (1k) for the Gym-MuJoCo environment. Results are averaged over random seeds of each algorithm, and the shaded areas indicate 95\% bootstrap confidence intervals.
    \vspace{-4mm}
    }%
\label{fig:appendix_gym_learning_curve}%
}
\clearpage

\subsection{Deepmind Control Suite - Easy}
\vspace{-5mm}
\input{tables/full_dmc_em}
\newpage
{
\centering
\vspace{-4mm}%
\includegraphics[width=0.9\textwidth]{figures/appendix_dmc_em_learning_curve.pdf}%
\vspace{-1em}
\captionof{figure}{
    \textbf{DMC-Easy Learning Curves.} Average episode return for the DMC-Easy environment. Results are averaged over random seeds of each algorithm, and the shaded areas indicate 95\% bootstrap confidence intervals.
    \vspace{-4mm}
    }%
\label{fig:appendix_dmc_em_learning_curve}%
}
\clearpage

\subsection{Deepmind Control Suite - Hard}
\vspace{-5mm}
\input{tables/full_dmc_hard}
{
\centering
\vspace{1mm}%
\includegraphics[width=0.99\textwidth]{figures/appendix_dmc_hard_learning_curve.pdf}%
\vspace{-4mm}
\captionof{figure}{
    \textbf{DMC-Hard Learning Curves.} Average episode return for the DMC-Hard environment. Results are averaged over random seeds of each algorithm, and the shaded areas indicate 95\% bootstrap confidence intervals.
    \vspace{-4mm}
    }%
\label{fig:appendix_dmc_hard_learning_curve}%
}
\clearpage

\subsection{MyoSuite}
\vspace{-5mm}
\input{tables/full_myosuite}
{
\centering
\vspace{1mm}%
\includegraphics[width=0.99\textwidth]{figures/appendix_myo_learning_curve.pdf}%
\vspace{-4mm}
\captionof{figure}{
    \textbf{MyoSuite Learning Curves.} Average episode success rate (\%) for the MyoSuite environment. Results are averaged over random seeds of each algorithm, and the shaded areas indicate 95\% bootstrap confidence intervals.
    \vspace{-4mm}
    }%
\label{fig:appendix_myo_learning_curve}%
}
\clearpage

\subsection{Humanoid Bench}
\vspace{-5mm}
\input{tables/full_hbench}
\clearpage
\begin{figure}[p]{
\centering
\vspace{1mm}%
\includegraphics[width=0.99\textwidth]{figures/appendix_hb_learning_curve.pdf}%
\vspace{-4mm}
\captionof{figure}{
    \textbf{Humanoidbench Learning Curves.} Average episode return for the HumanoidBench environment. Results are averaged over random seeds of each algorithm, and the shaded areas indicate 95\% bootstrap confidence intervals. The black dotted line indicates the success score of each tasks (Appendix~\ref{appendix:environments_hb})
    \vspace{-4mm}
    }%
}\end{figure}

\clearpage

\section{Complete Ablation Results}
\label{appendix:ablation}

This section presents a per-environment analysis of the design variations discussed in Section \ref{section:design_study}. Each table includes raw scores for individual environments, with [\textcolor{gray}{bracketed values}] indicating 95\% bootstrap confidence intervals. The aggregate mean, median, and interquartile mean (IQM) are calculated based on the differences in normalized scores. To illustrate the magnitude of these differences, we use the following highlight scale:
\begin{itemize}[itemsep=0pt]
    \item \mbest{$(\geq 0.1)$}
    \item \mbetter{$[0.05, 0.1)$}
    \item \mgood{$[0.02, 0.05)$}
    \item \mbad{$[-0.02, -0.05)$}
    \item \mworse{$[-0.05, -0.1)$}
    \item \mworst{$(\leq -0.05)$}
\end{itemize}




\subsection{Gym - MuJoCo}
\input{tables/design_mujoco}
\clearpage

\subsection{Deepmind Control Suite - Easy}
\input{tables/design_dmc_easy}
\clearpage

\subsection{Deepmind Control Suite - Hard}
\input{tables/design_dmc_hard}
\clearpage

\subsection{Myosuite}
\input{tables/design_myosuite}
\clearpage

\subsection{Humanoid Bench}
\input{tables/design_hbench}


\end{document}