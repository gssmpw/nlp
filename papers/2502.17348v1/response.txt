\section{Background and Related work}
\subsection{How programmers use Code LLMs}
There is a growing literature on how software developers interact with Code LLMs, particularly focusing on in-IDE tools customized for programming like GitHub Copilot. \add{A frequent finding is that programmers use the autocomplete functionality of tools like Copilot to speed up the process of drafting code, particularly when writing formulaic or boilerplate structures **Krause, "Automating Programming"**. Programmers also use Code LLMs to reduce the cognitive load of remembering software libraries' API methods and language-specific syntax, drawing comparisons between the role of Code LLMs and Stack Overflow **Bird, "Programmers at Work"**. Additionally, programmers report using Code LLMs to "provide a good starting point to approach a programming task” ____ that the user struggles to decompose into computational steps ____. }

One close study of experienced programmers working with Code LLMs suggests these varied objectives map to at least two "modes" of interaction ____:  \textit{acceleration}, in which programmers know what they intend to write and Copilot helps them write it faster, and \textit{exploration}, in which they consider multiple possibilities for how to achieve a programming objective using Copilot to facilitate exploration. In acceleration mode, users prefer short, granular autocomplete suggestions that anticipate the design choices they have already made. In exploration mode, users deliberately prompt Code LLMs to suggest unfamiliar coding patterns, libraries, and APIs. 

\add{Considering the growing adoption of Code LLMs, it can be tempting to view them as a new form of programming that allows users to code via a higher level of abstraction than before--via natural language. If this is correct, Code LLMs should be the ultimate support for non-expert programmers like scientists. However, **Kelly, "The Chasm"** cautions that "a prompt can span the gamut from describing an entire application in a few sentences, to painstakingly describing an algorithm in step-by-step pseudocode. Thus it would be a mistake to view programming with AI assistance as another rung on the abstraction ladder. Rather, it can be viewed as a device that can teleport the programmer to arbitrary rungs of the ladder as desired.” }

\add{Programming with Code LLMs still requires users to understand and interact with code at multiple levels of abstraction due to Code LLMs' inherent unreliability: even well-crafted prompts can validly correspond to a variety of code solutions, which may be an ineradicable aspect of specifying program requirements via natural language ____. Furthermore, Code LLMs are by design statistical models of sequences rather than algebraic engines ____. Their reliability in coding tasks depends on conditional factors like the quality and coverage of training data and the length and complexity of prompts ____. For the foreseeable future, programming with the assistance of a Code LLM requires the user to check that its outputs are acceptable---a task that requires both skill and effort ____.}
 
Indeed, the ability to verify generated code appears to interact with the user's prior programming experience: a Copilot user study with less-experienced participants found that half reported difficulty understanding generated code, which made repairing errors and checking code correctness challenging ____. Out of 24 participants, the experimenters also observed 8 cases where the participant accepted generated code without any effort to validate it. Despite this, a large majority of participants (19 of 24) indicated a preference to use Copilot in their regular programming practice. Similarly, **Gardiner and Kroll, "Copilot’s Solutions"** analyzed Copilot’s solutions to a battery of programming tasks and concluded that “…Copilot can become a liability if it is used by novice developers who may fail to filter its buggy or non-optimal solutions due to a lack of expertise”.  

Surveys of early adopters of Code LLMs have similarly confirmed that understanding, debugging and validating generated code is a major hurdle, even for professional developers ____. In response to this, a research team proposed and scoped an in-IDE tool for generating code explanations ____. Unexpectedly, they found that more professional developers benefited more than student developers. In line with **Kroll et al., "Code Explanation Tool"**, the researchers noted several instances where participants chose not to try to understand generated \add{code} at all---a phenomenon they called "outsourcing comprehension to the LLM". 

This literature establishes that on a variety of programming tasks, validating generated code is challenging even for software professionals. It is yet to be established how well these findings about programming generally apply to scientific contexts, though. To use the language of researcher Diane Kelly, the space between scientific programmers and software engineers is a "chasm" ____: in addition to an array of cultural distinctions owing to radically different development incentives ____, scientists may not see value in standard engineering practices ____, may not adopt explicit verification strategies like code testing, ____, may never receive formal training in software development ____, and may not have clear upfront requirements for software when they begin developing ____. Considering these distinctions, scientists will need to be studied as a distinct user group to understand how they interact with Code LLMs.  


\subsection{How data analysts use Code LLMs}
Professional data analysts and data scientists have similarities to science researchers: they have all been classified as "end-users", programmers who seek not to develop code as a primary product but for a domain-specific goal ____ (though this view has been challenged considering the breadth of development goals modern scientists may have ____). Professional data scientists may use programming languages that are common in the sciences, such as Python and R ____, though like science researchers they often lack formal training ____. Considering these similarities, the literature on how data professionals interact with code-generating tools is relevant. 

\add{While general findings from programmer interactions with Code LLMs likely still apply, distinct research considerations should be made for this population: software libraries used by data professionals, and the types of programming problems they encounter, may not closely match the training data of popular Code LLMs ____. These users often work in specialized development environments, like Jupyter Notebooks, that interleave code, documentation, and visualizations; in-notebook interfaces for Code LLMs is therefore an active area of research ____. Specialized Code LLM interfaces have also been explored for spreadsheet computation ____ and data visualization ____.}

\add{Code LLMs also hold unique promises for data work. For example, they may lower barriers for users to implement a variety of computational techniques ____, circumventing an established bottleneck for many analysts ____. Automated approaches that help analysts rapidly explore a broad landscape of modeling decisions could lead to more robust results, with less sensitive dependence on the idiosyncrasies of the analyst's strategy  ____. At the extreme, though, **Kelly and Heiberger, "The Risks of Over-Automation"** cautions that over-automation could defeat the goal of analysis in the first place: "Without any visibility into how [an interpretation of data] was produced, the human has no opportunity to apply knowledge that is not contained in the training data to debug operations chosen by the machine. They may not feel confident in their results, and their lack of insight into how they were reached may prevent them from applying the knowledge that is output, leading to a question of whether it is knowledge at all."}

\add{There has been tremendous excitement about generative AI as a programming teaching tool ____. Yet studies so far suggest substantial variation in how effectively students use, understand, and learn from Code LLMs ____. } In one controlled study, 120 students who had completed a single introductory computer science class were tasked with solving Python problems with a custom Chat interface ____. Many found it difficult to construct natural language prompts that correctly and completely specified their intent. The authors hypothesize that students' difficulty refining a prompt strategy could relate to misconceptions about how Code LLMs work---in particular, they tended to believe the model employed rule-based keyword-lookup to retrieve information from a data structure. \add{Other studies about how novice programmers prompt suggest they misunderstand the scope of information Code LLMs can access ____ and struggle to prompt at appropriate levels of abstraction required for successful code synthesis ____. Beginners may also struggle to decompose programming problems into smaller tasks before prompting ____, which is known to improve LLM performance on a variety of benchmarks ____ }. 

\add{There is some evidence for a "widening gap" between students who can use Code LLMs in ways that productively scaffold their learning, and students who cannot ____. Close observational studies of beginners solving introductory programming tasks by **Hollingshead et al., "Maladaptive Uses of Code LLMs"** indicates that students who struggle to program may overestimate the correctness of generated code as well as the productivity gains associated with using them.  Students who exhibited maladaptive uses of Code LLMs in one study were prone to overstating their mastery of foundational programming concepts, potentially delaying their own learning.}