\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{dicta}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{subcaption}
\usepackage{caption}
\captionsetup{justification=centering}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\dictafinalcopy % *** Uncomment this line for the final submission

\def\dictaPaperID{73} % *** Enter the DICTA Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifdictafinal\pagestyle{plain}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Automatic Vehicle Detection using DETR: A Transformer-Based Approach for Navigating Treacherous Roads}

\author{
Istiaq Ahmed Fahad\\
Institute of Information Technology\\
University of Dhaka\\
Dhaka, Bangladesh\\
{\tt\small bsse1204@iit.du.ac.bd}
\and
Abdullah Ibne Hanif Arean\\
Dept. of Computer Science and Engineering\\
University of Dhaka\\
Dhaka, Bangladesh\\
{\tt\small abdullaharean2613@gmail.com}
\and
Nazmus Sakib Ahmed\\
Institute of Information Technology\\
University of Dhaka\\
Dhaka, Bangladesh\\
{\tt\small bsse1124@iit.du.ac.bd}
\and
Mahmudul Hasan\\
Dept. of Computer Science and Engineering\\
University of Dhaka\\
Dhaka, Bangladesh\\
{\tt\small mahmudul.hhh@gmail.com}
}


\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
    Automatic Vehicle Detection (AVD) in diverse driving environments presents unique challenges due to varying lighting conditions, road types, and vehicle types. Traditional methods, such as YOLO and Faster R-CNN, often struggle to cope with these complexities. As computer vision evolves, combining Convolutional Neural Networks (CNNs) with Transformer-based approaches offers promising opportunities for improving detection accuracy and efficiency. This study is the first to experiment with Detection Transformer (DETR) for automatic vehicle detection in complex and varied settings. We employ a Collaborative Hybrid Assignments Training scheme, Co-DETR, to enhance feature learning and attention mechanisms in DETR. By leveraging versatile label assignment strategies and introducing multiple parallel auxiliary heads, we provide more effective supervision during training and extract positive coordinates to boost training efficiency. Through extensive experiments on DETR variants and YOLO models, conducted using the BadODD dataset, we demonstrate the advantages of our approach. Our method achieves superior results, and improved accuracy in diverse conditions, making it practical for real-world deployment. This work significantly advances autonomous navigation technology and opens new research avenues in object detection for autonomous vehicles. By integrating the strengths of CNNs and Transformers, we highlight the potential of DETR for robust and efficient vehicle detection in challenging driving environments.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Autonomous navigation technology has witnessed remarkable advancements in recent years, revolutionizing various industries and promising safer and more efficient transportation systems. In particular, the deployment of Autonomous Vehicles (AVs) holds immense potential for transforming the way we commute and travel \cite{shalev2017formal, grigorescu2020survey}. However, the successful realization of fully autonomous vehicles requires robust object detection systems capable of accurately identifying and localizing objects in complex and diverse driving environments.

\begin{figure}[!htb]
        \centering
        \includegraphics[width=0.60\linewidth]{Bd_area_coverage_map.png}
        \caption{Districts of Bangladesh from where the data for BadODD dataset is collected}
        \label{fig:area-coverage}
\end{figure}



Object detection serves as a fundamental component in the perception stack of autonomous vehicles, enabling them to understand and interact with their surroundings effectively. Traditional object detection methods \cite{felzenszwalb2010object, viola2001rapid, dalal2005hog} often rely on handcrafted features and complex pipelines, which may struggle to generalize across different environments and exhibit limited scalability. Recent research has shifted towards end-to-end trainable deep learning-based approaches \cite{ren2015faster, redmon2016you, Carion2020DETR} to overcome these limitations, offering improved performance and adaptability.

This paper focuses on addressing the unique challenges of automatic vehicle detection in diverse driving environments. These environments are characterized by varying road conditions, diverse vehicle types, and challenging lighting conditions \cite{zhu2019visdrone, geiger2012kitti, caesar2020nuscenes}. To facilitate research and development in this domain, we used the BadODD dataset \cite{Baig2024BadODDBA}, a comprehensive dataset specifically curated for detecting autonomous driving objects in complex settings.

We fine-tuned DETR \cite{Carion2020DETR} with Collaborative Hybrid Assignments Training (Co-DETR) \cite{zong2023detrs} to enhance the performance of DETR-based object detectors. We evaluated this approach against Transformer-based YOLOv8m \cite{chen2023yolov8} models, demonstrating improvements in efficiency and effectiveness through versatile label assignment strategies. This method addresses issues related to sparse supervision and inefficient feature learning, thereby improving the model’s ability to detect vehicles accurately in challenging scenarios. By combining the BadODD dataset with this training scheme, we aim to provide a robust framework for AVD in diverse driving conditions.

Our study pioneers the use of Detection Transformer (DETR) technology \cite{Carion2020DETR} for AVD in diverse driving environments. We fine-tuned the DETR model to enhance feature learning and attention mechanisms. Through experiments on the BadODD dataset, we demonstrate superior efficiency and enhanced accuracy compared to traditional methods like YOLO and Faster R-CNN \cite{ren2015faster}. This work validates DETR’s practical application in autonomous navigation and its potential to advance object detection in challenging driving scenarios.

\begin{itemize}
    \item Pioneers the use of the Detection Transformer (DETR) method for vehicle detection in diverse driving environments.
    \item Fine-tuned DETR with Collaborative Hybrid Assignments Training (Co-DETR) to enhance feature learning and attention mechanisms.
    \item Demonstrates thorough experiments on the BadODD dataset superior efficiency, and enhanced accuracy compared to traditional methods like YOLO and Faster R-CNN.
\end{itemize}

To outline our workflow, the paper is structured as follows: Section II provides a detailed description of the BadODD dataset, including coverage of Bangladesh districts, image and object statistics, and annotation details. Section III outlines the experimental setup, detailing the hardware and software configurations used for training the models. Section IV elaborates on the methodology, including data preprocessing, model selection, model specification, and comparative analysis. Section V presents the results and discussion, showcasing the performance of YOLOv8m and Co-DETR models. Section VI concludes the paper, by summarizing the contributions and implications of the proposed approach for automatic vehicle detection in diverse driving environments. Nevertheless, in the following sections, we will use the term \textit{Co-DETR} to refer to \textit{DETRs with Collaborative Hybrid Assignments Training}.

    \begin{figure}[!htb]
        \centering
        \includegraphics[width=\linewidth]{Data_split_by_area.png}
        \caption{Place wise Train-Test Data Distribution}
        \label{fig:train-test-coverage}
    \end{figure}
    

%------------------------------------------------------------------------
\section{Related Work}

The evolution of Autonomous Vehicle Detection (AVD) has been significantly influenced by deep learning. Initially, Convolutional Neural Networks (CNNs) were the go-to approach due to their effectiveness in handling image data. One of the pioneering models, AlexNet \cite{krizhevsky2012imagenet}, demonstrated the power of deep learning in image classification, which soon found applications in AVD. This was followed by models like Faster R-CNN \cite{ren2015faster}, which introduced region proposal networks, enhancing both speed and accuracy.

Despite these advancements, CNN-based methods struggled in complex environments such as treacherous roads. Issues like slow inference times and an inability to capture the global context effectively led to the exploration of more sophisticated architectures. Single-stage detectors, such as YOLO \cite{redmon2016you}, attempted to address these problems by predicting bounding boxes and class probabilities in a single evaluation. However, these models often faltered in complex scenes and small object detection \cite{liu2016ssd}.

To overcome these challenges, the focus shifted to Transformer-based models, initially popularized in natural language processing by \textit{Vaswani et al}. \cite{vaswani2017attention}. The DETR (DEtection TRansformer) model \cite{Carion2020DETR} emerged as a groundbreaking approach for image tasks. Unlike traditional CNNs, DETR utilizes a transformer encoder-decoder architecture to predict object sets directly, capturing the global context of an image. This method eliminates the need for hand-crafted components like anchor boxes and non-maximum suppression, simplifying the detection process and improving accuracy, particularly in cluttered scenes \cite{zhu2020deformable}.

The application of DETR in AVD is still in its nascent stage. Most studies, such as those by \textit{Zhu et al.} \cite{zhu2020deformable} and Carion et al. \cite{Carion2020DETR}, have focused on general object detection and segmentation. However, recent research indicates that integrating DETR into AVD can leverage its robust set prediction capabilities and global context understanding \cite{gao2021road}. Enhancements like Collaborative Hybrid Assignments Training (Co-DETR) show promise in improving detection accuracy and efficiency in challenging driving environments, highlighting the potential of transformer-based models in AVD.

    \begin{figure}[!htb]
        \centering
        \includegraphics[width=\linewidth]{dataset2.png}
        \caption{Class Distribution of BadODD Dataset}
        \label{fig:class-dist}
    \end{figure}
    

%------------------------------------------------------------------------
\section{Dataset Description}
    
    \subsection{Coverage of Bangladesh Districts}
        The dataset, named BadODD, covers 9 districts in Bangladesh: \textbf{Sylhet, Dhaka, Rajshahi, Mymensingh, Munshiganj(Maowa), Chittagong, Sirajganj, Sherpur, and Khulna.} (Figure \ref{fig:area-coverage}) It includes various road scenarios such as urban and rural areas, highways, and expressways, providing a diverse representation of Bangladesh's road infrastructure. Data was collected using smartphone cameras to ensure authenticity and capture real-world driving conditions.
    
    \subsection{Image and Object Statistics}
        The dataset includes a total of \textit{9,825 images} showcasing various road and driving scenarios, captured during both day and night. Among these, 5,896 images are designated for training and 1,964 images for testing. Class-wise Train-Test image distribution is shown in Figure \ref{fig:train-test-coverage}. There are 78,943 objects annotated across the dataset, categorized into \textit{13 distinct classes} representing various vehicles under diverse driving conditions commonly found on Bangladeshi roads (Figure \ref{fig:diverse_driving_conditions}). Frame selection was meticulously planned to capture the dynamic qualities of urban environments. Adaptive frame-rate sampling strategies were employed based on traffic densities, similar to other autonomous vehicle datasets such as KITTI \cite{geiger2012kitti}, nuScenes \cite{caesar2020nuscenes}, and BDD100K \cite{yu2020bdd100k}.

        \begin{figure*}[!htb]
            \centering
            \begin{subfigure}{0.3\linewidth}
                \centering
                \includegraphics[width=\linewidth]{chittagong_bohoddarhat1_826.jpg}
                \caption{Day Image 1}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.3\linewidth}
                \centering
                \includegraphics[width=\linewidth]{chittagong_bohoddarhat2_12154.jpg}
                \caption{Day Image 2}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.3\linewidth}
                \centering
                \includegraphics[width=\linewidth]{chittagong_bohoddarhat1_4071.jpg}
                \caption{Day Image 3}
            \end{subfigure}
            \vskip\baselineskip
            \begin{subfigure}{0.3\linewidth}
                \centering
                \includegraphics[width=\linewidth]{dhaka_night1_1121.jpg}
                \caption{Night Image 1}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.3\linewidth}
                \centering
                \includegraphics[width=\linewidth]{dhaka_night3_24780.jpg}
                \caption{Night Image 2}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.3\linewidth}
                \centering
                \includegraphics[width=\linewidth]{dhaka_night3_51625.jpg}
                \caption{Night Image 3}
            \end{subfigure}
            \caption{Examples from the dataset illustrating diverse road conditions and vehicle types under different lighting scenarios. The top row (a-c) depicts daytime scenes, while the bottom row (d-f) showcases nighttime scenes. This dataset is intended for studying diverse traffic patterns and vehicle behavior in Bangladesh.}
            \label{fig:diverse_driving_conditions}
        \end{figure*}

    
    \subsection{Annotation Details}
        To ensure \textbf{scalability and generalization}, classes were redefined based on vehicle characteristics rather than relying solely on local or globally recognized names. This annotation strategy is inspired by previous datasets such as nuScenes \cite{caesar2020nuscenes} and BDD100K \cite{yu2020bdd100k}, which consider environmental conditions and dataset scalability.

        Annotations were conducted using the \textbf{YOLOv5 format}, which offers simplicity and efficiency in handling diverse object classes \cite{jocher2020yolov5}. The dataset’s class distribution analysis highlights imbalances, with more prevalent classes like \textbf{Person, Auto Rickshaw, and Three Wheeler}, while underrepresented classes include \textbf{Wheelchair, Train, and Construction Vehicle} shown in Figure \ref{fig:class-dist}. The YOLOv5 annotation format ensures optimized model training and improved performance in real-world scenarios.

     \begin{figure*}[!htb]
        \centering
        \begin{subfigure}{0.45\linewidth}
            \centering
            \includegraphics[width=\linewidth]{normal.png}
            \caption{Raw}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.45\linewidth}
            \centering
            \includegraphics[width=\linewidth]{HE.png}
            \caption{Histogram Equalization}
        \end{subfigure}
        \vskip\baselineskip
        \begin{subfigure}{0.45\linewidth}
            \centering
            \includegraphics[width=\linewidth]{CLAHE.png}
            \caption{CLAHE}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.45\linewidth}
            \centering
            \includegraphics[width=\linewidth]{AGC.png}
            \caption{Gamma Correction}
        \end{subfigure}
        \caption{Comparison of image enhancement techniques applied to a raw street scene. (a) Raw image, (b) Image after Histogram Equalization, (c) Image after Contrast Limited Adaptive Histogram Equalization (CLAHE), and (d) Image after Gamma Correction. These techniques improve visual clarity and highlight different features in the dataset.}
        \label{fig:image-enhancement-techniques}
    \end{figure*}


%------------------------------------------------------------------------
\section{Methodology}

%------------------------------------------------------------------------
\subsection{Data Preprocessing}

Before feeding our data into the model for training and inference, we performed preprocessing to enhance the quality of the images and facilitate better object detection. This involved applying various Image Enhancement Techniques to the image set of BadODD (Figure \ref{fig:image-enhancement-techniques}). These techniques aim to improve the visual quality of images, thereby enhancing their interoperability for both humans and machines. Specifically, we employed three different techniques:

\begin{itemize}
    \item \textbf{Histogram Equalization:} A method used to improve the contrast of an image by redistributing pixel intensities. \cite{Pizer1987HE}
    \item \textbf{Contrast Limited Adaptive Histogram Equalization (CLAHE):} An adaptive version of histogram equalization that limits the amplification of noise in regions with low contrast. \cite{Zuiderveld1994CLAHE}
    \item \textbf{Gamma Correction:} A technique used to adjust the brightness and contrast of an image by modifying the gamma value. \cite{Fung1999GammaCorrection}
\end{itemize}

These reprocessing steps ensured that the input data provided optimal conditions for object detection algorithms to perform effectively. Figure \ref{fig:image-enhancement-techniques} illustrates these techniques applied to sample images from the BadODD dataset. These enhanced images serve as the improved input data for our object detection models, contributing to more accurate and reliable detection results.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{Methodology.png}
    \caption{Workflow of the proposed methodology for automatic vehicle detection using DETR}
\end{figure*}



%------------------------------------------------------------------------
\subsection{Model Selection}

We initially adopted the YoloV8m architecture for its advancements in object detection, featuring an enhanced backbone network, adaptive training strategy, and improved detection head. YoloV8m is known for achieving superior accuracy while maintaining computational efficiency, making it a compelling choice for our study. However, our results with the current dataset did not show significant improvements.

Therefore, we opted for a transformer-based approach, known for capturing long-range dependencies and contextual information. Transformers leverage attention mechanisms, which enhance the model's understanding of global context, making them particularly suitable for complex scenes. Consequently, we explored the Co-DETR model. Co-DETR departs from traditional anchor-based methods by employing a transformer architecture and a set prediction mechanism to capture spatial relationships effectively. It predicts all object classes and their bounding boxes in a single pass, providing a robust solution for intricate scenarios.

Our investigation into both YoloV8m and Co-DETR aimed to provide a comprehensive comparative analysis of their strengths and weaknesses in object detection. We fine-tuned these models using a dataset specifically collected for vehicle detection in Bangladesh.

    \begin{figure*}[!t]
        \centering
        \includegraphics[width=\linewidth]{tranformer.jpeg}
        \caption{Framework of Collaborative Hybrid Assignment Training of DETR model (Co-DETR). The auxiliary branches are discarded during
        evaluation. \cite{Zong2022DETRsWC}}
    \end{figure*}


%------------------------------------------------------------------------
\subsection{Model Specification}

We initially opted for the YoloV8m architecture due to its advancements in object detection. YoloV8m features an enhanced backbone network, an adaptive training strategy, and an improved object detection head. Its reputation for achieving superior accuracy while maintaining computational efficiency made it a compelling choice for our study.

The training procedure involved fine-tuning the YOLOv8m model using the BadODD dataset specifically collected for detecting autonomous driving objects, especially for Bangladesh. We configured the training process with the following parameters:

\begin{itemize}
    \item \textbf{Model:} YOLOv8m
    \item \textbf{Base Learning Rate: }0.018
    \item \textbf{Momentum:} 0.933
    \item \textbf{Cosine learning rate:} True
    \item \textbf{Deterministic:} True
    \item \textbf{Seed:} 43
    \item \textbf{Evaluation Metric:} mAP
\end{itemize}

These parameters were chosen based on experimentation and best practices in the field of object detection for autonomous navigation tasks.

Before training the YoloV8m model, we conducted exploratory data analysis (EDA) to understand the dataset better. We also applied image processing techniques to enhance visualization and improve object detection accuracy. Additionally, a \textbf{validation set} comprising \textbf{20\%} of the dataset was created to ensure data integrity, where images in the training set did not overlap with those in the validation set.

The YOLOv8m model underwent fine-tuning using the diverse BadODD dataset, encompassing varied driving environments across Bangladesh. However, YOLOv8m did not perform adequately on treacherous roads. To address this, we shifted to the Transformer-based DETR model with Collaborative Hybrid Assignments Training (Co-DETR) for potentially enhanced efficiency and accuracy. Utilizing transfer learning, we adapted the pre-trained DETR model specifically for these challenging conditions, aiming for improved performance in detecting vehicles on difficult roadways.

Co-DETR represents a cutting-edge approach that merges transformer architecture with set prediction mechanisms, enhancing its ability to interpret complex scenes found in diverse driving conditions. By effectively capturing the global context, Co-DETR emerges as a promising alternative to traditional models like YoloV8m, particularly suited for environments with varied road conditions.

To optimize its performance for vehicle detection, we fine-tuned Co-DETR using a diverse dataset covering a wide range of driving scenarios in Bangladesh. We carefully adjusted key hyper-parameters, including the backbone depths and query head settings of the SwinTransformer \cite{Lin2020SwinTransformer}, to maximize its accuracy and efficiency. During training, we configured several important parameters:

\begin{itemize}
    \item \textbf{Model:} Co-DETR
    \item \textbf{Backbone:} SwinTransformer
    \begin{itemize}
        \item Pretrained image size: 384
        \item Embedding dimensions: 192
        \item Depths: [2, 2, 18, 2]
        \item Number of heads: [6, 12, 24, 48]
        \item Window size: 12
        \item MLP ratio: 4
        \item Dropout rate: 0.3
    \end{itemize}
    \item \textbf{Training Batch Size per GPU:} 1
    \item \textbf{Number of Workers for Data Loading:} 1
    \item \textbf{Learning Rate:} 0.00008
    \item \textbf{Number of Epochs:} 20
\end{itemize}

Our objective was to harness Co-DETR's advanced capabilities to significantly enhance object detection accuracy in dynamic driving environments characteristic of Bangladesh. By exploring Co-DETR, we aimed to push the boundaries of what's possible in object detection. Through rigorous experimentation and evaluation, we sought to validate its effectiveness in addressing the unique challenges posed by diverse driving scenarios.


%------------------------------------------------------------------------
\section{Experimental Setup}
All experiments were conducted on a high-performance workstation featuring a 16-core AMD Ryzen 9 5950X processor, 128GB of RAM, an NVIDIA RTX 3090 GPU with 24GB of memory, a 2TB storage drive, and running Ubuntu 22.04 LTS. The primary focus was on training time, with the YoloV8m models taking 1.2 hours and the Co-DETR model taking 20 hours to train, both for 10 epochs. The extended training time for Co-DETR is attributed to its complex transformer architecture and the collaborative hybrid assignments training scheme, which involves multiple parallel auxiliary heads supervised by one-to-many label assignments. This approach enhances the encoder's learning ability and improves training efficiency in the decoder. Importantly, these auxiliary heads are discarded during inference, resulting in no additional parameters or computational cost for the original detector.
    

%------------------------------------------------------------------------
\section{Results and Discussion}

The results and discussion section presents an evaluation of the proposed approach for object detection on Bangladesh roads using the BadODD dataset. To effectively compare the performance of YOLOv8m and Co-DETR in object detection, we examined their mean Average Precision (mAP) scores across different training epochs, as summarized in Table \ref{tab:map_scores}. Notably, the confidence threshold plays a significant role in determining the model's prediction certainty. For our evaluations, a threshold of \textbf{0.4} was found to strike a good balance between accurate predictions and comprehensive detection coverage.


    
    \begin{table}[h]
        \centering
        \caption{Mean Average Precision (mAP) Scores for YOLOv8m and Co-DETR Models Across Different Epochs}
        \begin{tabular}{ccc}
        \toprule
        \multirow{2}{*}{Epochs} & \multicolumn{2}{c}{mAP Scores} \\ \cmidrule(lr){2-3}
         & YOLOv8m & Co-DETR \\ 
        \midrule
        1 & 0.236 & 0.372 \\
        4 & 0.255 & 0.418 \\
        \textbf{9} & \textbf{0.295} & \textbf{0.438} \\
        \bottomrule
        \end{tabular}
        \label{tab:map_scores}
    \end{table}



The following table highlights the mAP scores achieved by each model at various epochs. Co-DETR consistently outperforms YOLOv8m across the board, showcasing its superior ability to detect objects in diverse environments such as those found on Bangladeshi roads. Particularly noteworthy is Co-DETR's peak performance at 9 epochs, where it achieved an mAP of 0.438, demonstrating its capability to progressively refine object detection accuracy over time.

Through iterative refinement and experimentation, Co-DETR emerges as superior in accurately detecting objects across diverse driving conditions. Co-DETR achieves a peak mAP score of \textit{0.438}, outperforming YOLOv8m's mAP score of \textit{0.295}, both at 9 epochs. These results signify significant progress in enhancing road safety and efficiency, with practical implications for autonomous vehicle development and deployment for complex scenarios. Thus, the adoption of the Co-DETR model highlights the effectiveness of Transformer-based methods in tackling the intricate challenges of object detection, fostering advancements toward safer and more efficient autonomous navigation systems.


%------------------------------------------------------------------------
\section{Conclusion}

This study investigated advanced deep learning models, specifically focusing on vehicle detection in diverse driving environments using the BadODD dataset. Co-DETR, leveraging transformer-based architecture, outperformed traditional methods like YOLOv8m, demonstrating superior accuracy and efficiency in detecting vehicles across challenging road conditions in Bangladesh. These results highlight the potential of transformer-based approaches to enhance autonomous navigation technology, setting new benchmarks for future advancements in vehicle detection. Further research can optimize Co-DETR for real-time applications and explore hybrid approaches to integrate transformer strengths with other deep learning paradigms, promising even greater advancements in autonomous vehicle technology.

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}



\end{document}
