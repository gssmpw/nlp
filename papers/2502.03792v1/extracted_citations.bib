@inproceedings{alimisis2021momentum,
  title={Momentum improves optimization on Riemannian manifolds},
  author={Alimisis, Foivos and Orvieto, Antonio and Becigneul, Gary and Lucchi, Aurelien},
  booktitle={International conference on artificial intelligence and statistics},
  pages={1351--1359},
  year={2021},
  organization={PMLR}
}

@inproceedings{anonymous2025methods,
title={Methods for Convex $(L_0,L_1)$-Smooth Optimization: Clipping, Acceleration, and Adaptivity},
author={Gorbunov, E. and Tupitsa, N. and Choudhury, S. and Aliev, A. and Richtárik, P. and Horváth, S. and Takáč, M.},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=0wmfzWPAFu}
}

@inproceedings{arora2018a,
title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
author={Sanjeev Arora and Nadav Cohen and Noah Golowich and Wei Hu},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SkMQg3C5K7},
}

@article{ba2022high,
  title={High-dimensional asymptotics of feature learning: How one gradient step improves the representation},
  author={Ba, Jimmy and Erdogdu, Murat A and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37932--37946},
  year={2022}
}

@article{baes2021low,
  title={Low-rank plus sparse decomposition of covariance matrices using neural network parametrization},
  author={Baes, Michel and Herrera, Calypso and Neufeld, Ariel and Ruyssen, Pierre},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={34},
  number={1},
  pages={171--185},
  year={2021},
  publisher={IEEE}
}

@article{casgrain2019latent,
  title={A latent variational framework for stochastic optimization},
  author={Casgrain, Philippe},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{faw2023beyond,
  title={Beyond uniform smoothness: A stopped analysis of adaptive {SGD}},
  author={Faw, Matthew and Rout, Litu and Caramanis, Constantine and Shakkottai, Sanjay},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={89--160},
  year={2023},
  organization={PMLR}
}

@article{gonon2020risk,
  title={Risk bounds for reservoir computing},
  author={Gonon, Lukas and Grigoryeva, Lyudmila and Ortega, Juan-Pablo},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={240},
  pages={1--61},
  year={2020}
}

@article{gonon2023approximation,
  title={Approximation bounds for random neural networks and reservoir systems},
  author={Gonon, Lukas and Grigoryeva, Lyudmila and Ortega, Juan-Pablo},
  journal={The Annals of Applied Probability},
  volume={33},
  number={1},
  pages={28--69},
  year={2023},
  publisher={Institute of Mathematical Statistics}
}

@article{heiss2023implicit,
  title={How (Implicit) Regularization of ReLU Neural Networks Characterizes the Learned Function--Part II: the Multi-D Case of Two Layers with Random First Layer},
  author={Heiss, Jakob and Teichmann, Josef and Wutte, Hanna},
  journal={arXiv preprint arXiv:2303.11454},
  year={2023}
}

@article{hsieh2024riemannian,
  title={Riemannian stochastic optimization methods avoid strict saddle points},
  author={Hsieh, Ya-Ping and Karimi Jaghargh, Mohammad Reza and Krause, Andreas and Mertikopoulos, Panayotis},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{jin2023implicit,
  title={Implicit bias of gradient descent for mean squared error regression with two-layer wide neural networks},
  author={Jin, Hui and Mont{\'u}far, Guido},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={137},
  pages={1--97},
  year={2023}
}

@article{mei2022generalization,
  title={The generalization error of random features regression: Precise asymptotics and the double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={4},
  pages={667--766},
  year={2022},
  publisher={Wiley Online Library}
}

@article{minh2022strong,
  title={Strong convergence of a generalized forward--backward splitting method in reflexive Banach spaces},
  author={Minh Tuyen, Truong and Promkam, Ratthaprom and Sunthrayuth, Pongsakorn},
  journal={Optimization},
  volume={71},
  number={6},
  pages={1483--1508},
  year={2022},
  publisher={Taylor \& Francis}
}

@inproceedings{mulayoff2024exact,
  title={Exact Mean Square Linear Stability Analysis for {SGD}},
  author={Mulayoff, Rotem and Michaeli, Tomer},
  booktitle={The Thirty Seventh Annual Conference on Learning Theory},
  pages={3915--3969},
  year={2024},
  organization={PMLR}
}

@article{patrascu2021stochastic,
  title={Stochastic proximal splitting algorithm for composite minimization},
  author={Patrascu, Andrei and Irofti, Paul},
  journal={Optimization Letters},
  volume={15},
  number={6},
  pages={2255--2273},
  year={2021},
  publisher={Springer}
}

@InProceedings{pmlr-v119-golikov20a,
  title = 	 {Towards a General Theory of Infinite-Width Limits of Neural Classifiers},
  author =       {Golikov, Eugene},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3617--3626},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/golikov20a/golikov20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/golikov20a.html},
  abstract = 	 {Obtaining theoretical guarantees for neural networks training appears to be a hard problem in a general case. Recent research has been focused on studying this problem in the limit of infinite width and two different theories have been developed: a mean-field (MF) and a constant kernel (NTK) limit theories. We propose a general framework that provides a link between these seemingly distinct theories. Our framework out of the box gives rise to a discrete-time MF limit which was not previously explored in the literature. We prove a convergence theorem for it, and show that it provides a more reasonable approximation for finite-width nets compared to the NTK limit if learning rates are not very small. Also, our framework suggests a limit model that coincides neither with the MF limit nor with the NTK one. We show that for networks with more than two hidden layers RMSProp training has a non-trivial discrete-time MF limit but GD training does not have one. Overall, our framework demonstrates that both MF and NTK limits have considerable limitations in approximating finite-sized neural nets, indicating the need for designing more accurate infinite-width approximations for them.}
}

@InProceedings{pmlr-v134-casgrain21a,
  title = 	 {Optimizing Optimizers: Regret-optimal gradient descent algorithms},
  author =       {Casgrain, Philippe and Kratsios, Anastasis},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {883--926},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--19 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v134/casgrain21a/casgrain21a.pdf},
  url = 	 {https://proceedings.mlr.press/v134/casgrain21a.html},
  abstract = 	 {This paper treats the task of designing optimization algorithms as an optimal control problem. Using regret as a metric for an algorithm’s performance, we study the existence, uniqueness and consistency of regret-optimal algorithms. By providing first-order optimality conditions for the control problem, we show that regret-optimal algorithms must satisfy a specific structure in their dynamics which we show is equivalent to performing \emph{dual-preconditioned gradient descent} on the value function generated by its regret. Using these optimal dynamics, we provide bounds on their rates of convergence to solutions of convex optimization problems. Though closed-form optimal dynamics cannot be obtained in general, we present fast numerical methods for approximating them, generating optimization algorithms which directly optimize their long-term regret. These are benchmarked against commonly used optimization algorithms to demonstrate their effectiveness.}
}

@InProceedings{pmlr-v178-shamir22a,
  title = 	 {The Implicit Bias of Benign Overfitting},
  author =       {Shamir, Ohad},
  booktitle = 	 {Proceedings of Thirty Fifth Conference on Learning Theory},
  pages = 	 {448--478},
  year = 	 {2022},
  editor = 	 {Loh, Po-Ling and Raginsky, Maxim},
  volume = 	 {178},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--05 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v178/shamir22a/shamir22a.pdf},
  url = 	 {https://proceedings.mlr.press/v178/shamir22a.html},
  abstract = 	 {The phenomenon of benign overfitting, where a predictor perfectly fits noisy training data while attaining low expected loss, has received much attention in recent years, but still remains not fully understood beyond simple linear regression setups. In this paper, we show that for regression, benign overfitting is “biased” towards certain types of problems, in the sense that its existence on one learning problem precludes its existence on other learning problems. On the negative side, we use this to argue that one should not expect benign overfitting to occur in general, for several natural extensions of the plain linear regression problems studied so far. We then turn to classification problems, and show that the situation there is much more favorable. Specifically, we consider a model where an arbitrary input distribution of some fixed dimension k is concatenated with a high-dimensional distribution, and prove that the max-margin predictor (to which gradient-based methods are known to converge in direction) is asymptotically biased towards minimizing the expected \emph{squared hinge loss} w.r.t. the k-dimensional distribution. This allows us to reduce the question of benign overfitting in classification to the simpler question of whether this loss is a good surrogate for the misclassification error, and use it to show benign overfitting in some new settings.}
}

@InProceedings{pmlr-v195-abbe23a,
  title = 	 {{SGD} learning on neural networks: leap complexity and saddle-to-saddle dynamics},
  author =       {Abbe, Emmanuel and Adser{\`a}, Enric Boix and Misiakiewicz, Theodor},
  booktitle = 	 {Proceedings of Thirty Sixth Conference on Learning Theory},
  pages = 	 {2552--2623},
  year = 	 {2023},
  editor = 	 {Neu, Gergely and Rosasco, Lorenzo},
  volume = 	 {195},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {12--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v195/abbe23a/abbe23a.pdf},
  url = 	 {https://proceedings.mlr.press/v195/abbe23a.html},
  abstract = 	 {   We investigate the time complexity of {SGD} learning on fully-connected neural networks with isotropic data. We put forward a complexity measure,{\it the leap}, which measures how “hierarchical” target functions are. For $d$-dimensional uniform Boolean or isotropic Gaussian data, our main conjecture states that the time complexity to learn a function $f$ with low-dimensional support is $$\Tilde \Theta (d^{\max(\mathrm{Leap}(f),2)}) \,\,.$$    We prove a version of this conjecture for a class of functions on Gaussian isotropic data and 2-layer neural networks, under additional technical assumptions on how {SGD} is run. We show that the training  sequentially learns the function support with a saddle-to-saddle dynamic. Our result departs from Abbe et al.’22 by going beyond leap 1 (merged-staircase functions), and by going beyond the mean-field and gradient flow approximations that prohibit the full complexity control obtained here.Finally, we note that this gives an {SGD} complexity for the full training trajectory that matches that of Correlational Statistical Query (CSQ) lower-bounds. }
}

@InProceedings{pmlr-v247-ma24a,
  title = 	 {Convergence of Gradient Descent with Small Initialization for Unregularized Matrix Completion},
  author =       {Ma, Jianhao and Fattahi, Salar},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {3683--3742},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/ma24a/ma24a.pdf},
  url = 	 {https://proceedings.mlr.press/v247/ma24a.html},
  abstract = 	 {We study the problem of symmetric matrix completion, where the goal is to reconstruct a positive semidefinite matrix $X^\star \in \mathbb{R}^{d\times d}$ of rank-$r$, parameterized by $UU^{\top}$, from only a subset of its observed entries. We show that the vanilla gradient descent (GD) with small initialization provably converges to the ground truth $X^\star$ without requiring any explicit regularization. This convergence result holds true even in the over-parameterized scenario, where the true rank $r$ is unknown and conservatively over-estimated by a search rank $r’\gg r$. The existing results for this problem either require explicit regularization, a sufficiently accurate initial point, or exact knowledge of the true rank $r$.  In the over-parameterized regime where $r’\geq r$, we show that, with $\widetilde\Omega(dr^9)$ observations, GD with an initial point $\|U_0\| \leq O(\epsilon)$ converges near-linearly to an $\epsilon$-neighborhood of $X^\star$. Consequently, smaller initial points result in increasingly accurate solutions. Surprisingly, neither the convergence rate nor the final accuracy depends on the over-parameterized search rank $r’$, and they are only governed by the true rank $r$. In the exactly-parameterized regime where $r’=r$, we further enhance this result by proving that GD converges at a faster rate to achieve an arbitrarily small accuracy $\epsilon&gt;0$, provided the initial point satisfies $\|U_0\| = O(1/d)$. At the crux of our method lies a novel weakly-coupled leave-one-out analysis, which allows us to establish the global convergence of GD, extending beyond what was previously possible using the classical leave-one-out analysis.}
}

@InProceedings{pmlr-v247-patel24a,
  title = 	 {The Limits and Potentials of Local {SGD} for Distributed Heterogeneous Learning with Intermittent Communication},
  author =       {Patel, Kumar Kshitij and Glasgow, Margalit and Zindari, Ali and Wang, Lingxiao and Stich, Sebastian U and Cheng, Ziheng and Joshi, Nirmit and Srebro, Nathan},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {4115--4157},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/patel24a/patel24a.pdf},
  url = 	 {https://proceedings.mlr.press/v247/patel24a.html},
  abstract = 	 {Local {SGD} is a popular optimization method in distributed learning, often outperforming mini-batch {SGD}. Despite this practical success, proving the efficiency of local {SGD} has been difficult, creating a significant gap between theory and practice. We provide new lower bounds for local {SGD} under existing first-order data heterogeneity assumptions, showing these assumptions can not capture local {SGD}’s effectiveness. We also demonstrate the min-max optimality of accelerated mini-batch {SGD} under these assumptions. Our findings emphasize the need for improved modeling of data heterogeneity.  Under higher-order assumptions, we provide new upper bounds that verify the dominance of local {SGD} over mini-batch {SGD} when data heterogeneity is low.}
}

@InProceedings{pmlr-v247-wu24b,
  title = 	 {Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency},
  author =       {Wu, Jingfeng and Bartlett, Peter L. and Telgarsky, Matus and Yu, Bin},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {5019--5073},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/wu24b/wu24b.pdf},
  url = 	 {https://proceedings.mlr.press/v247/wu24b.html},
  abstract = 	 {We consider \emph{gradient descent} (GD) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\eta$ is so large that the loss initially oscillates. We show that GD exits this initial oscillatory phase rapidly — in $O(\eta)$ steps, and subsequently achieves an $\tilde{O}(1 / (\eta t) )$ convergence rate after $t$ additional steps. Our results imply that, given a budget of $T$ steps, GD can achieve an \emph{accelerated} loss of $\tilde{O}(1/T^2)$ with an aggressive stepsize $\eta:= \Theta( T)$, without any use of momentum or variable stepsize schedulers. Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\tilde{O}(1/T^2)$ acceleration), nonlinear predictors in the \emph{neural tangent kernel} regime, and online \emph{stochastic gradient descent} ({SGD}) with a large stepsize, under suitable separability conditions.}
}

@InProceedings{pmlr-v247-zeng24a,
  title = 	 {Fast two-time-scale stochastic gradient method with applications in reinforcement learning},
  author =       {Zeng, Sihan and Doan, Thinh},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {5166--5212},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/zeng24a/zeng24a.pdf},
  url = 	 {https://proceedings.mlr.press/v247/zeng24a.html},
  abstract = 	 {Two-time-scale optimization is a framework introduced in Zeng et al. (2024) that abstracts a range of policy evaluation and policy optimization problems in reinforcement learning (RL). Akin to bi-level optimization under a particular type of stochastic oracle, the two-time-scale optimization framework has an upper level objective whose gradient evaluation depends on the solution of a lower level problem, which is to find the root of a strongly monotone operator. In this work, we propose a new method for solving two-time-scale optimization that achieves significantly faster convergence than the prior arts. The key idea of our approach is to leverage an averaging step to improve the estimates of the operators in both lower and upper levels before using them to update the decision variables. These additional averaging steps eliminate the direct coupling between the main variables, enabling the accelerated performance of our algorithm. We characterize the finite-time convergence rates of the proposed algorithm under various conditions of the underlying objective function, including strong convexity, convexity, Polyak-Lojasiewicz condition, and general non-convexity. These rates significantly improve over the best-known complexity of the standard two-time-scale stochastic approximation algorithm. When applied to RL, we show how the proposed algorithm specializes to novel online sample-based methods that surpass or match the performance of the existing state of the art. Finally, we support our theoretical results with numerical simulations in RL.}
}

@article{velikanov2024tight,
  title={Tight convergence rate bounds for optimization under power law spectral conditions},
  author={Velikanov, Maksim and Yarotsky, Dmitry},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={81},
  pages={1--78},
  year={2024}
}

@inproceedings{xu2023over,
  title = 	 {Over-Parameterization Exponentially Slows Down Gradient Descent for Learning a Single Neuron},
  author =       {Xu, Weihang and Du, Simon},
  booktitle = 	 {Proceedings of Thirty Sixth Conference on Learning Theory},
  pages = 	 {1155--1198},
  year = 	 {2023},
  editor = 	 {Neu, Gergely and Rosasco, Lorenzo},
  volume = 	 {195},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {12--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v195/xu23a/xu23a.pdf},
  url = 	 {https://proceedings.mlr.press/v195/xu23a.html},
  abstract = 	 {We revisit the canonical problem of learning a single neuron with ReLU activation under Gaussian input with square loss. We particularly focus on the over-parameterization setting where the student network has $n\ge 2$ neurons. We prove the global convergence of randomly initialized gradient descent with a $O\left(T^{-3}\right)$ rate. This is the first global convergence result for this problem beyond the exact-parameterization setting ($n=1$) in which the gradient descent enjoys an $\exp(-\Omega(T))$ rate. Perhaps surprisingly, we further present an $\Omega\left(T^{-3}\right)$ lower bound for randomly initialized gradient flow in the over-parameterization setting. These two bounds jointly give an exact characterization of the convergence rate and imply, for the first time, that \emph{over-parameterization can exponentially slow down the convergence rate}. To prove the global convergence, we need to tackle the interactions among student neurons in the gradient descent dynamics, which are not present in the exact-parameterization case. We use a  three-phase structure to analyze GD’s dynamics. Along the way, we prove gradient descent automatically balances student neurons and uses this property to deal with the non-smoothness of the objective function. To prove the convergence rate lower bound, we construct a novel potential function that characterizes the pairwise distances between the student neurons (which cannot be done in the exact-parameterization case). We show this potential function converges slowly, which implies the slow convergence rate of the loss function.}
}

@article{zhu2024stability,
  title={Stability and generalization of the decentralized stochastic gradient descent ascent algorithm},
  author={Zhu, Miaoxi and Shen, Li and Du, Bo and Tao, Dacheng},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhu2024uniform,
  title={Uniform-in-time {W}asserstein stability bounds for (noisy) stochastic gradient descent},
  author={Zhu, Lingjiong and Gurbuzbalaban, Mert and Raj, Anant and Simsekli, Umut},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

