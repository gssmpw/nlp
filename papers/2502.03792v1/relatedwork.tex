\section{Related Work}
\label{s:Introduction__ss:RelatedWork}

\paragraph{The Inductive Bias Encoded into Neural Networks by Gradient Descent}
Substantial research has focused on the implicit biases induced by gradient descent during the empirical risk minimization in neural network architectures. Progress has been made in explaining these biases by showing that sufficiently wide two-layer MLPs tend to converge toward cubic interpolating splines~\citep{jin2023implicit} and exhibit benign overfitting~\citep{pmlr-v178-shamir22a}. These properties of GD-trained networks are also reflected in the kernelized infinite-width limit for neural networks with randomized hidden layers~\citep{heiss2023implicit,mei2022generalization}, which themselves are currently well-understood~\citep{gonon2020risk,gonon2023approximation} surrogates for standard two-layer MLPs trained with GD. Indeed, it is known that there is a gap between the statistical behaviour of two-layer networks when only the final layer is trained when compared to two-layer neural networks trained using only a single GD step; the latter of which learns superior latent feature representations~\citep{ba2022high}.

\hspace{2em}In the case of deeper networks (those with more than two layers), it has been shown that when activation functions are removed (i.e., identity activations), GD training leads deep unactivated MLPs to solve a principal component analysis (PCA) problem~\citep{arora2018a}. 
Additionally, there is a large body of literature on the training dynamics of neural networks in the mean-field or infinite-width regime~\citep{pmlr-v195-abbe23a,pmlr-v119-golikov20a}. Other works examine the negative impact of overparameterization on GD convergence rates in the student-teacher setting~\cite{xu2023over}. 

\paragraph{Variations of Gradient Descent}
GD and its variants have been extensively studied in the non-convex optimization literature. Research has focused on their convergence rates to critical points of the empirical risk functional~\citep{pmlr-v247-patel24a,pmlr-v247-zeng24a}, stationary points of specific non-convex function classes~\citep{faw2023beyond,anonymous2025methods}, and under spectral conditions~\citep{velikanov2024tight}. The impact of hyperparameters, such as step size, on GD convergence~\citep{pmlr-v247-wu24b}, as well as its stability properties~\citep{mulayoff2024exact,zhu2024uniform,zhu2024stability}, has also been well explored. Additionally, GD has proven effective in solving specialized problems like matrix completion~\citep{baes2021low,pmlr-v247-ma24a}.
% 
Recent work has also explored the design of GD-type algorithms to meet meta-optimality criteria~\citep{casgrain2019latent,pmlr-v134-casgrain21a}, studying non-Euclidean settings~\citep{alimisis2021momentum,hsieh2024riemannian}, and extending GD to proximal splitting methods for objectives that combine differentiable and lower semi-continuous convex parts~\citep{patrascu2021stochastic,minh2022strong}.