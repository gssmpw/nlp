\section{Related Work}
\label{s:Introduction__ss:RelatedWork}

\paragraph{The Inductive Bias Encoded into Neural Networks by Gradient Descent}
Substantial research has focused on the implicit biases induced by gradient descent during the empirical risk minimization in neural network architectures. Progress has been made in explaining these biases by showing that sufficiently wide two-layer MLPs tend to converge toward cubic interpolating splines____ and exhibit benign overfitting____. These properties of GD-trained networks are also reflected in the kernelized infinite-width limit for neural networks with randomized hidden layers____, which themselves are currently well-understood____ surrogates for standard two-layer MLPs trained with GD. Indeed, it is known that there is a gap between the statistical behaviour of two-layer networks when only the final layer is trained when compared to two-layer neural networks trained using only a single GD step; the latter of which learns superior latent feature representations____.

\hspace{2em}In the case of deeper networks (those with more than two layers), it has been shown that when activation functions are removed (i.e., identity activations), GD training leads deep unactivated MLPs to solve a principal component analysis (PCA) problem____. 
Additionally, there is a large body of literature on the training dynamics of neural networks in the mean-field or infinite-width regime____. Other works examine the negative impact of overparameterization on GD convergence rates in the student-teacher setting____. 

\paragraph{Variations of Gradient Descent}
GD and its variants have been extensively studied in the non-convex optimization literature. Research has focused on their convergence rates to critical points of the empirical risk functional____, stationary points of specific non-convex function classes____, and under spectral conditions____. The impact of hyperparameters, such as step size, on GD convergence____, as well as its stability properties____, has also been well explored. Additionally, GD has proven effective in solving specialized problems like matrix completion____.
% 
Recent work has also explored the design of GD-type algorithms to meet meta-optimality criteria____, studying non-Euclidean settings____, and extending GD to proximal splitting methods for objectives that combine differentiable and lower semi-continuous convex parts____.