


@article{gonon2023approximation,
  title={Approximation bounds for random neural networks and reservoir systems},
  author={Gonon, Lukas and Grigoryeva, Lyudmila and Ortega, Juan-Pablo},
  journal={The Annals of Applied Probability},
  volume={33},
  number={1},
  pages={28--69},
  year={2023},
  publisher={Institute of Mathematical Statistics}
}
@article{gonon2020risk,
  title={Risk bounds for reservoir computing},
  author={Gonon, Lukas and Grigoryeva, Lyudmila and Ortega, Juan-Pablo},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={240},
  pages={1--61},
  year={2020}
}


@article{hou2024convergence,
  title={Convergence of the adapted smoothed empirical measures},
  author={Hou, Songyan},
  journal={arXiv preprint arXiv:2401.14883},
  year={2024}
}

@article {SongyangConvergence_AAP__2024,
    AUTHOR = {Acciaio, Beatrice and Hou, Songyan},
     TITLE = {Convergence of adapted empirical measures on {$\Bbb R^d$}},
   JOURNAL = {Ann. Appl. Probab.},
  FJOURNAL = {The Annals of Applied Probability},
    VOLUME = {34},
      YEAR = {2024},
    NUMBER = {5},
     PAGES = {4799--4835},
      ISSN = {1050-5164,2168-8737},
   MRCLASS = {60B10 (49Q22 62G30)},
  MRNUMBER = {4801582},
       DOI = {10.1214/24-aap2082},
       URL = {https://doi.org/10.1214/24-aap2082},
}

@article {FournierGuillin_ConvWasserstein_2015,
    AUTHOR = {Fournier, Nicolas and Guillin, Arnaud},
     TITLE = {On the rate of convergence in {W}asserstein distance of the empirical measure},
   JOURNAL = {Probab. Theory Related Fields},
  FJOURNAL = {Probability Theory and Related Fields},
    VOLUME = {162},
      YEAR = {2015},
    NUMBER = {3-4},
     PAGES = {707--738},
      ISSN = {0178-8051,1432-2064},
   MRCLASS = {60F25 (60E15 60F10)},
  MRNUMBER = {3383341},
MRREVIEWER = {Jos\'e\ Trashorras},
       DOI = {10.1007/s00440-014-0583-7},
       URL = {https://doi.org/10.1007/s00440-014-0583-7},
}

@InProceedings{pmlr-v178-shamir22a,
  title = 	 {The Implicit Bias of Benign Overfitting},
  author =       {Shamir, Ohad},
  booktitle = 	 {Proceedings of Thirty Fifth Conference on Learning Theory},
  pages = 	 {448--478},
  year = 	 {2022},
  editor = 	 {Loh, Po-Ling and Raginsky, Maxim},
  volume = 	 {178},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--05 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v178/shamir22a/shamir22a.pdf},
  url = 	 {https://proceedings.mlr.press/v178/shamir22a.html},
  abstract = 	 {The phenomenon of benign overfitting, where a predictor perfectly fits noisy training data while attaining low expected loss, has received much attention in recent years, but still remains not fully understood beyond simple linear regression setups. In this paper, we show that for regression, benign overfitting is “biased” towards certain types of problems, in the sense that its existence on one learning problem precludes its existence on other learning problems. On the negative side, we use this to argue that one should not expect benign overfitting to occur in general, for several natural extensions of the plain linear regression problems studied so far. We then turn to classification problems, and show that the situation there is much more favorable. Specifically, we consider a model where an arbitrary input distribution of some fixed dimension k is concatenated with a high-dimensional distribution, and prove that the max-margin predictor (to which gradient-based methods are known to converge in direction) is asymptotically biased towards minimizing the expected \emph{squared hinge loss} w.r.t. the k-dimensional distribution. This allows us to reduce the question of benign overfitting in classification to the simpler question of whether this loss is a good surrogate for the misclassification error, and use it to show benign overfitting in some new settings.}
}


@article{hon2022simultaneous,
  title={Simultaneous neural network approximation for smooth functions},
  author={Hon, Sean and Yang, Haizhao},
  journal={Neural Networks},
  volume={154},
  pages={152--164},
  year={2022},
  publisher={Elsevier}
}
@article{hornik1990universal,
  title={Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={3},
  number={5},
  pages={551--560},
  year={1990},
  publisher={Elsevier}
}


@article{tsuzuku2018lipschitz,
  title={Lipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks},
  author={Tsuzuku, Yusuke and Sato, Issei and Sugiyama, Masashi},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{trockman2021orthogonalizing,
title={Orthogonalizing Convolutional Layers with the Cayley Transform},
author={Asher Trockman and J Zico Kolter},
journal={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Pbj8H_jEHYv}
}


@inproceedings{arora2018a,
title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
author={Sanjeev Arora and Nadav Cohen and Noah Golowich and Wei Hu},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SkMQg3C5K7},
}

@inproceedings{araujo2023a,
title={A Unified Algebraic Perspective on Lipschitz Neural Networks},
author={Alexandre Araujo and Aaron J Havens and Blaise Delattre and Alexandre Allauzen and Bin Hu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=k71IGLC8cfc}
}
@inproceedings{wang2023direct,
  title={Direct parameterization of lipschitz-bounded deep networks},
  author={Wang, Ruigang and Manchester, Ian},
  booktitle={International Conference on Machine Learning},
  pages={36093--36110},
  year={2023},
  organization={PMLR}
}
@inproceedings{meunier2022dynamical,
  title={A dynamical system perspective for lipschitz neural networks},
  author={Meunier, Laurent and Delattre, Blaise J and Araujo, Alexandre and Allauzen, Alexandre},
  booktitle={International Conference on Machine Learning},
  pages={15484--15500},
  year={2022},
  organization={PMLR}
}


@article{hou2023instance,
  title={Instance-dependent generalization bounds via optimal transport},
  author={Hou, Songyan and Kassraie, Parnian and Kratsios, Anastasis and Krause, Andreas and Rothfuss, Jonas},
  journal={The Journal of Machine Learning Research},
  volume={24},
  number={1},
  pages={16815--16865},
  year={2023},
  publisher={JMLRORG}
}
@inproceedings{korotinwasserstein,
  title={Wasserstein-2 Generative Networks},
  author={Korotin, Alexander and Egiazarian, Vage and Asadulaev, Arip and Safin, Alexander and Burnaev, Evgeny},
  booktitle={International Conference on Learning Representations},
 year={2019}
}

@article{fazlyab2024certified,
  title={Certified robustness via dynamic margin maximization and improved lipschitz regularization},
  author={Fazlyab, Mahyar and Entesari, Taha and Roy, Aniket and Chellappa, Rama},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{singla2021improved,
  title={Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100},
  author={Singla, Sahil and Singla, Surbhi and Feizi, Soheil},
  journal={arXiv preprint arXiv:2108.04062},
  year={2021}
}


@article{zhang2022rethinking,
  title={Rethinking lipschitz neural networks and certified robustness: A boolean function perspective},
  author={Zhang, Bohang and Jiang, Du and He, Di and Wang, Liwei},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={19398--19413},
  year={2022}
}


@article{virmaux2018lipschitz,
  title={Lipschitz regularity of deep neural networks: analysis and efficient estimation},
  author={Virmaux, Aladin and Scaman, Kevin},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{limmer2024reality,
  title={Reality Only Happens Once: Single-Path Generalization Bounds for Transformers},
  author={Limmer, Yannick and Kratsios, Anastasis and Yang, Xuwei and Saqur, Raeid and Horvath, Blanka},
  journal={arXiv preprint arXiv:2405.16563},
  year={2024}
}


@article{jordan2020exactly,
  title={Exactly computing the local lipschitz constant of relu networks},
  author={Jordan, Matt and Dimakis, Alexandros G},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7344--7353},
  year={2020}
}



@article{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{persiianov2024inverse,
  title={Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization},
  author={Persiianov, Mikhail and Asadulaev, Arip and Andreev, Nikita and Starodubcev, Nikita and Baranchuk, Dmitry and Kratsios, Anastasis and Burnaev, Evgeny and Korotin, Alexander},
  journal={arXiv preprint arXiv:2410.02628},
  year={2024}
}



@inproceedings{korotin2023neural,
title={Neural Optimal Transport},
author={Alexander Korotin and Daniil Selikhanovych and Evgeny Burnaev},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=d8CBRlWNkqH}
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

@inproceedings{kolesov2024energyguided,
title={Energy-Guided Continuous Entropic Barycenter Estimation for General Costs},
author={Alexander Kolesov and Petr Mokrov and Igor Udovichenko and Milena Gazdieva and Gudmund Pammer and Anastasis Kratsios and Evgeny Burnaev and Alexander Korotin},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=JZHFRLoqDq}
}

@inproceedings{khromov2024some,
  title={Some Fundamental Aspects about Lipschitz Continuity of Neural Networks},
  author={Khromov, Grigory and Singh, Sidak Pal},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@misc{geuchen2024upperlowerboundslipschitz,
      title={Upper and lower bounds for the Lipschitz constant of random neural networks}, 
      author={Paul Geuchen and Thomas Heindl and Dominik Stöger and Felix Voigtlaender},
      year={2024},
      eprint={2311.01356},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2311.01356}, 
}

@article{Herrera2020LocalLB,
  title={Local {L}ipschitz bounds of deep neural networks},
  author={Herrera, Calypso and Krach, Florian and Teichmann, Josef},
  journal={arXiv preprint arXiv:2004.13135},
  year={2020}
}

@article{benezet2024learning,
  title={Learning conditional distributions on continuous spaces},
  author={B{\'e}n{\'e}zet, Cyril and Cheng, Ziteng and Jaimungal, Sebastian},
  journal={arXiv preprint arXiv:2406.09375},
  year={2024}
}

@article{hong2024bridging,
  title={Bridging the gap between approximation and learning via optimal approximation by ReLU MLPs of maximal regularity},
  author={Hong, Ruiyang and Kratsios, Anastasis},
  journal={arXiv preprint arXiv:2409.12335},
  year={2024}
}





@inproceedings{mulayoff2024exact,
  title={Exact Mean Square Linear Stability Analysis for {SGD}},
  author={Mulayoff, Rotem and Michaeli, Tomer},
  booktitle={The Thirty Seventh Annual Conference on Learning Theory},
  pages={3915--3969},
  year={2024},
  organization={PMLR}
}

@InProceedings{pmlr-v247-patel24a,
  title = 	 {The Limits and Potentials of Local {SGD} for Distributed Heterogeneous Learning with Intermittent Communication},
  author =       {Patel, Kumar Kshitij and Glasgow, Margalit and Zindari, Ali and Wang, Lingxiao and Stich, Sebastian U and Cheng, Ziheng and Joshi, Nirmit and Srebro, Nathan},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {4115--4157},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/patel24a/patel24a.pdf},
  url = 	 {https://proceedings.mlr.press/v247/patel24a.html},
  abstract = 	 {Local {SGD} is a popular optimization method in distributed learning, often outperforming mini-batch {SGD}. Despite this practical success, proving the efficiency of local {SGD} has been difficult, creating a significant gap between theory and practice. We provide new lower bounds for local {SGD} under existing first-order data heterogeneity assumptions, showing these assumptions can not capture local {SGD}’s effectiveness. We also demonstrate the min-max optimality of accelerated mini-batch {SGD} under these assumptions. Our findings emphasize the need for improved modeling of data heterogeneity.  Under higher-order assumptions, we provide new upper bounds that verify the dominance of local {SGD} over mini-batch {SGD} when data heterogeneity is low.}
}


@InProceedings{pmlr-v247-ma24a,
  title = 	 {Convergence of Gradient Descent with Small Initialization for Unregularized Matrix Completion},
  author =       {Ma, Jianhao and Fattahi, Salar},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {3683--3742},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/ma24a/ma24a.pdf},
  url = 	 {https://proceedings.mlr.press/v247/ma24a.html},
  abstract = 	 {We study the problem of symmetric matrix completion, where the goal is to reconstruct a positive semidefinite matrix $X^\star \in \mathbb{R}^{d\times d}$ of rank-$r$, parameterized by $UU^{\top}$, from only a subset of its observed entries. We show that the vanilla gradient descent (GD) with small initialization provably converges to the ground truth $X^\star$ without requiring any explicit regularization. This convergence result holds true even in the over-parameterized scenario, where the true rank $r$ is unknown and conservatively over-estimated by a search rank $r’\gg r$. The existing results for this problem either require explicit regularization, a sufficiently accurate initial point, or exact knowledge of the true rank $r$.  In the over-parameterized regime where $r’\geq r$, we show that, with $\widetilde\Omega(dr^9)$ observations, GD with an initial point $\|U_0\| \leq O(\epsilon)$ converges near-linearly to an $\epsilon$-neighborhood of $X^\star$. Consequently, smaller initial points result in increasingly accurate solutions. Surprisingly, neither the convergence rate nor the final accuracy depends on the over-parameterized search rank $r’$, and they are only governed by the true rank $r$. In the exactly-parameterized regime where $r’=r$, we further enhance this result by proving that GD converges at a faster rate to achieve an arbitrarily small accuracy $\epsilon&gt;0$, provided the initial point satisfies $\|U_0\| = O(1/d)$. At the crux of our method lies a novel weakly-coupled leave-one-out analysis, which allows us to establish the global convergence of GD, extending beyond what was previously possible using the classical leave-one-out analysis.}
}

@article{baes2021low,
  title={Low-rank plus sparse decomposition of covariance matrices using neural network parametrization},
  author={Baes, Michel and Herrera, Calypso and Neufeld, Ariel and Ruyssen, Pierre},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={34},
  number={1},
  pages={171--185},
  year={2021},
  publisher={IEEE}
}
@inproceedings{faw2023beyond,
  title={Beyond uniform smoothness: A stopped analysis of adaptive {SGD}},
  author={Faw, Matthew and Rout, Litu and Caramanis, Constantine and Shakkottai, Sanjay},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={89--160},
  year={2023},
  organization={PMLR}
}

@article{velikanov2024tight,
  title={Tight convergence rate bounds for optimization under power law spectral conditions},
  author={Velikanov, Maksim and Yarotsky, Dmitry},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={81},
  pages={1--78},
  year={2024}
}

@InProceedings{pmlr-v119-golikov20a,
  title = 	 {Towards a General Theory of Infinite-Width Limits of Neural Classifiers},
  author =       {Golikov, Eugene},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3617--3626},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/golikov20a/golikov20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/golikov20a.html},
  abstract = 	 {Obtaining theoretical guarantees for neural networks training appears to be a hard problem in a general case. Recent research has been focused on studying this problem in the limit of infinite width and two different theories have been developed: a mean-field (MF) and a constant kernel (NTK) limit theories. We propose a general framework that provides a link between these seemingly distinct theories. Our framework out of the box gives rise to a discrete-time MF limit which was not previously explored in the literature. We prove a convergence theorem for it, and show that it provides a more reasonable approximation for finite-width nets compared to the NTK limit if learning rates are not very small. Also, our framework suggests a limit model that coincides neither with the MF limit nor with the NTK one. We show that for networks with more than two hidden layers RMSProp training has a non-trivial discrete-time MF limit but GD training does not have one. Overall, our framework demonstrates that both MF and NTK limits have considerable limitations in approximating finite-sized neural nets, indicating the need for designing more accurate infinite-width approximations for them.}
}

@article{minh2022strong,
  title={Strong convergence of a generalized forward--backward splitting method in reflexive Banach spaces},
  author={Minh Tuyen, Truong and Promkam, Ratthaprom and Sunthrayuth, Pongsakorn},
  journal={Optimization},
  volume={71},
  number={6},
  pages={1483--1508},
  year={2022},
  publisher={Taylor \& Francis}
}

@misc{herrera2023locallipschitzboundsdeep,
      title={Local Lipschitz Bounds of Deep Neural Networks}, 
      author={Calypso Herrera and Florian Krach and Josef Teichmann},
      year={2023},
      eprint={2004.13135},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2004.13135}, 
}

@article {MR4243432,
    AUTHOR = {Petersen, Philipp and Raslan, Mones and Voigtlaender, Felix},
     TITLE = {Topological properties of the set of functions generated by
              neural networks of fixed size},
   JOURNAL = {Found. Comput. Math.},
  FJOURNAL = {Foundations of Computational Mathematics. The Journal of the
              Society for the Foundations of Computational Mathematics},
    VOLUME = {21},
      YEAR = {2021},
    NUMBER = {2},
     PAGES = {375--444},
      ISSN = {1615-3375,1615-3383},
   MRCLASS = {54H30 (52A30 68T05)},
  MRNUMBER = {4243432},
MRREVIEWER = {Bogdan\ D.\ Suceav\u a},
       DOI = {10.1007/s10208-020-09461-0},
       URL = {https://doi.org/10.1007/s10208-020-09461-0},
}

@article{patrascu2021stochastic,
  title={Stochastic proximal splitting algorithm for composite minimization},
  author={Patrascu, Andrei and Irofti, Paul},
  journal={Optimization Letters},
  volume={15},
  number={6},
  pages={2255--2273},
  year={2021},
  publisher={Springer}
}


@article{hsieh2024riemannian,
  title={Riemannian stochastic optimization methods avoid strict saddle points},
  author={Hsieh, Ya-Ping and Karimi Jaghargh, Mohammad Reza and Krause, Andreas and Mertikopoulos, Panayotis},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{alimisis2021momentum,
  title={Momentum improves optimization on Riemannian manifolds},
  author={Alimisis, Foivos and Orvieto, Antonio and Becigneul, Gary and Lucchi, Aurelien},
  booktitle={International conference on artificial intelligence and statistics},
  pages={1351--1359},
  year={2021},
  organization={PMLR}
}


@article{zhu2024uniform,
  title={Uniform-in-time {W}asserstein stability bounds for (noisy) stochastic gradient descent},
  author={Zhu, Lingjiong and Gurbuzbalaban, Mert and Raj, Anant and Simsekli, Umut},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wu2024federated,
  title={Federated conditional stochastic optimization},
  author={Wu, Xidong and Sun, Jianhui and Hu, Zhengmian and Li, Junyi and Zhang, Aidong and Huang, Heng},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhu2024stability,
  title={Stability and generalization of the decentralized stochastic gradient descent ascent algorithm},
  author={Zhu, Miaoxi and Shen, Li and Du, Bo and Tao, Dacheng},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@inproceedings{xu2023over,
  title = 	 {Over-Parameterization Exponentially Slows Down Gradient Descent for Learning a Single Neuron},
  author =       {Xu, Weihang and Du, Simon},
  booktitle = 	 {Proceedings of Thirty Sixth Conference on Learning Theory},
  pages = 	 {1155--1198},
  year = 	 {2023},
  editor = 	 {Neu, Gergely and Rosasco, Lorenzo},
  volume = 	 {195},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {12--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v195/xu23a/xu23a.pdf},
  url = 	 {https://proceedings.mlr.press/v195/xu23a.html},
  abstract = 	 {We revisit the canonical problem of learning a single neuron with ReLU activation under Gaussian input with square loss. We particularly focus on the over-parameterization setting where the student network has $n\ge 2$ neurons. We prove the global convergence of randomly initialized gradient descent with a $O\left(T^{-3}\right)$ rate. This is the first global convergence result for this problem beyond the exact-parameterization setting ($n=1$) in which the gradient descent enjoys an $\exp(-\Omega(T))$ rate. Perhaps surprisingly, we further present an $\Omega\left(T^{-3}\right)$ lower bound for randomly initialized gradient flow in the over-parameterization setting. These two bounds jointly give an exact characterization of the convergence rate and imply, for the first time, that \emph{over-parameterization can exponentially slow down the convergence rate}. To prove the global convergence, we need to tackle the interactions among student neurons in the gradient descent dynamics, which are not present in the exact-parameterization case. We use a  three-phase structure to analyze GD’s dynamics. Along the way, we prove gradient descent automatically balances student neurons and uses this property to deal with the non-smoothness of the objective function. To prove the convergence rate lower bound, we construct a novel potential function that characterizes the pairwise distances between the student neurons (which cannot be done in the exact-parameterization case). We show this potential function converges slowly, which implies the slow convergence rate of the loss function.}
}



@article{mei2022generalization,
  title={The generalization error of random features regression: Precise asymptotics and the double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={4},
  pages={667--766},
  year={2022},
  publisher={Wiley Online Library}
}


@article{heiss2023implicit,
  title={How (Implicit) Regularization of ReLU Neural Networks Characterizes the Learned Function--Part II: the Multi-D Case of Two Layers with Random First Layer},
  author={Heiss, Jakob and Teichmann, Josef and Wutte, Hanna},
  journal={arXiv preprint arXiv:2303.11454},
  year={2023}
}
@article{jin2023implicit,
  title={Implicit bias of gradient descent for mean squared error regression with two-layer wide neural networks},
  author={Jin, Hui and Mont{\'u}far, Guido},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={137},
  pages={1--97},
  year={2023}
}


@inproceedings{arnaboldi2023high,
  title={From high-dimensional \& mean-field dynamics to dimensionless odes: A unifying approach to {SGD} in two-layers networks},
  author={Arnaboldi, Luca and Stephan, Ludovic and Krzakala, Florent and Loureiro, Bruno},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={1199--1227},
  year={2023},
  organization={PMLR}
}

@article{ba2022high,
  title={High-dimensional asymptotics of feature learning: How one gradient step improves the representation},
  author={Ba, Jimmy and Erdogdu, Murat A and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37932--37946},
  year={2022}
}


@InProceedings{pmlr-v195-abbe23a,
  title = 	 {{SGD} learning on neural networks: leap complexity and saddle-to-saddle dynamics},
  author =       {Abbe, Emmanuel and Adser{\`a}, Enric Boix and Misiakiewicz, Theodor},
  booktitle = 	 {Proceedings of Thirty Sixth Conference on Learning Theory},
  pages = 	 {2552--2623},
  year = 	 {2023},
  editor = 	 {Neu, Gergely and Rosasco, Lorenzo},
  volume = 	 {195},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {12--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v195/abbe23a/abbe23a.pdf},
  url = 	 {https://proceedings.mlr.press/v195/abbe23a.html},
  abstract = 	 {   We investigate the time complexity of {SGD} learning on fully-connected neural networks with isotropic data. We put forward a complexity measure,{\it the leap}, which measures how “hierarchical” target functions are. For $d$-dimensional uniform Boolean or isotropic Gaussian data, our main conjecture states that the time complexity to learn a function $f$ with low-dimensional support is $$\Tilde \Theta (d^{\max(\mathrm{Leap}(f),2)}) \,\,.$$    We prove a version of this conjecture for a class of functions on Gaussian isotropic data and 2-layer neural networks, under additional technical assumptions on how {SGD} is run. We show that the training  sequentially learns the function support with a saddle-to-saddle dynamic. Our result departs from Abbe et al.’22 by going beyond leap 1 (merged-staircase functions), and by going beyond the mean-field and gradient flow approximations that prohibit the full complexity control obtained here.Finally, we note that this gives an {SGD} complexity for the full training trajectory that matches that of Correlational Statistical Query (CSQ) lower-bounds. }
}


@InProceedings{pmlr-v134-casgrain21a,
  title = 	 {Optimizing Optimizers: Regret-optimal gradient descent algorithms},
  author =       {Casgrain, Philippe and Kratsios, Anastasis},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {883--926},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--19 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v134/casgrain21a/casgrain21a.pdf},
  url = 	 {https://proceedings.mlr.press/v134/casgrain21a.html},
  abstract = 	 {This paper treats the task of designing optimization algorithms as an optimal control problem. Using regret as a metric for an algorithm’s performance, we study the existence, uniqueness and consistency of regret-optimal algorithms. By providing first-order optimality conditions for the control problem, we show that regret-optimal algorithms must satisfy a specific structure in their dynamics which we show is equivalent to performing \emph{dual-preconditioned gradient descent} on the value function generated by its regret. Using these optimal dynamics, we provide bounds on their rates of convergence to solutions of convex optimization problems. Though closed-form optimal dynamics cannot be obtained in general, we present fast numerical methods for approximating them, generating optimization algorithms which directly optimize their long-term regret. These are benchmarked against commonly used optimization algorithms to demonstrate their effectiveness.}
}
@article{casgrain2019latent,
  title={A latent variational framework for stochastic optimization},
  author={Casgrain, Philippe},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@inproceedings{anonymous2025methods,
title={Methods for Convex $(L_0,L_1)$-Smooth Optimization: Clipping, Acceleration, and Adaptivity},
author={Gorbunov, E. and Tupitsa, N. and Choudhury, S. and Aliev, A. and Richtárik, P. and Horváth, S. and Takáč, M.},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=0wmfzWPAFu}
}

@InProceedings{pmlr-v247-zeng24a,
  title = 	 {Fast two-time-scale stochastic gradient method with applications in reinforcement learning},
  author =       {Zeng, Sihan and Doan, Thinh},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {5166--5212},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/zeng24a/zeng24a.pdf},
  url = 	 {https://proceedings.mlr.press/v247/zeng24a.html},
  abstract = 	 {Two-time-scale optimization is a framework introduced in Zeng et al. (2024) that abstracts a range of policy evaluation and policy optimization problems in reinforcement learning (RL). Akin to bi-level optimization under a particular type of stochastic oracle, the two-time-scale optimization framework has an upper level objective whose gradient evaluation depends on the solution of a lower level problem, which is to find the root of a strongly monotone operator. In this work, we propose a new method for solving two-time-scale optimization that achieves significantly faster convergence than the prior arts. The key idea of our approach is to leverage an averaging step to improve the estimates of the operators in both lower and upper levels before using them to update the decision variables. These additional averaging steps eliminate the direct coupling between the main variables, enabling the accelerated performance of our algorithm. We characterize the finite-time convergence rates of the proposed algorithm under various conditions of the underlying objective function, including strong convexity, convexity, Polyak-Lojasiewicz condition, and general non-convexity. These rates significantly improve over the best-known complexity of the standard two-time-scale stochastic approximation algorithm. When applied to RL, we show how the proposed algorithm specializes to novel online sample-based methods that surpass or match the performance of the existing state of the art. Finally, we support our theoretical results with numerical simulations in RL.}
}



@InProceedings{pmlr-v247-wu24b,
  title = 	 {Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency},
  author =       {Wu, Jingfeng and Bartlett, Peter L. and Telgarsky, Matus and Yu, Bin},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {5019--5073},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/wu24b/wu24b.pdf},
  url = 	 {https://proceedings.mlr.press/v247/wu24b.html},
  abstract = 	 {We consider \emph{gradient descent} (GD) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\eta$ is so large that the loss initially oscillates. We show that GD exits this initial oscillatory phase rapidly — in $O(\eta)$ steps, and subsequently achieves an $\tilde{O}(1 / (\eta t) )$ convergence rate after $t$ additional steps. Our results imply that, given a budget of $T$ steps, GD can achieve an \emph{accelerated} loss of $\tilde{O}(1/T^2)$ with an aggressive stepsize $\eta:= \Theta( T)$, without any use of momentum or variable stepsize schedulers. Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\tilde{O}(1/T^2)$ acceleration), nonlinear predictors in the \emph{neural tangent kernel} regime, and online \emph{stochastic gradient descent} ({SGD}) with a large stepsize, under suitable separability conditions.}
}




@book{DudleyRealProb_2002Book,
    AUTHOR = {Dudley, R. M.},
     TITLE = {Real analysis and probability},
    SERIES = {Cambridge Studies in Advanced Mathematics},
    VOLUME = {74},
 PUBLISHER = {Cambridge University Press, Cambridge},
      YEAR = {2002},
}

@article{kloeckner2020empirical,
  title={Empirical measures: regularity is a counter-curse to dimensionality},
  author={Kloeckner, Beno{\^\i}t R},
  journal={ESAIM: Probability and Statistics},
  volume={24},
  pages={408--434},
  year={2020},
  publisher={EDP Sciences}
}




@book {VershyyninBook,
    AUTHOR = {Vershynin, Roman},
     TITLE = {High-dimensional probability},
    SERIES = {Cambridge Series in Statistical and Probabilistic Mathematics},
    VOLUME = {47},
      NOTE = {An introduction with applications in data science,
              With a foreword by Sara van de Geer},
 PUBLISHER = {Cambridge University Press, Cambridge},
      YEAR = {2018},
     PAGES = {xiv+284},
      ISBN = {978-1-108-41519-4},
   MRCLASS = {60-01 (60B05 60B20 60E15 60Fxx 62H25)},
  MRNUMBER = {3837109},
MRREVIEWER = {Sasha\ Sodin},
       DOI = {10.1017/9781108231596},
       URL = {https://doi.org/10.1017/9781108231596},
}


@article{petersen2024mathematical,
  title={Mathematical theory of deep learning},
  author={Petersen, Philipp and Zech, Jakob},
  journal={arXiv preprint arXiv:2407.18384},
  year={2024}
}


@article{JMLR:v24:22-1381,
  author  = {Guergana Petrova and Przemyslaw Wojtaszczyk},
  title   = {Limitations on approximation by deep and shallow neural networks},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {353},
  pages   = {1--38},
  url     = {http://jmlr.org/papers/v24/22-1381.html}
}

@article{Federer_GeometricMeasureTheory_1978,
    AUTHOR = {Federer, Herbert},
     TITLE = {Colloquium lectures on geometric measure theory},
   JOURNAL = {Bull. Amer. Math. Soc.},
  FJOURNAL = {Bulletin of the American Mathematical Society},
      YEAR = {1978},
}



@book{Boyd,
    title={Convex Optimization},
    author={Stephen Boyd and Lieven Vandenberghe},
    year={2009},
    pages={0--0},
    publisher={Cambridge University Press 2004},
    url={https://stanford.edu/~boyd/cvxbook/},
}


@book{Nesterov,
    author = {Yurii Nesterov},
    title = {Introductory Lectures on Convex Optimization},
    year = {2004},
    pages = {},
    publisher = {Springer Science+Business Media New York},
    url = {},
}

