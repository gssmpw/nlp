\documentclass[11pt,twoside]{article}

\setlength{\parindent}{0pt}
% \setlength{\parskip}{4pt}

% \newcommand{\gettitle}[0]{Taming Two-Layer Neural Networks Lipschitzness via Learning Rate Constraints}
\newcommand{\gettitle}[0]{Guiding Two-Layer Neural Network Lipschitzness via Gradient Descent Learning Rate Constraints}

\title{\gettitle}

\author{
Kyle Sung \\
  McMaster University\\
  {\scshape{sungk5@mcmaster.ca}} \\
   \And
  Anastasis Kratsios\\ 
McMaster University \\and the Vector Institute\\
  {\scshape{kratsioa@mcmaster.ca}} \\
\And
Noah Forman \\
   McMaster University\\
  {\scshape{formann@mcmaster.ca}} \\
}



\date{}

\setlength{\headheight}{15pt}

\usepackage{fancyhdr}
\fancyhf{}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0.4pt}
\fancyheadoffset{0pt}

\fancyhead[CO]{\gettitle}
\fancyhead[CE]{Sung, Kratsios, and Forman}

\cfoot{\thepage}


\usepackage[dvipsnames]{xcolor}
\definecolor{warmblack}{rgb}{0.0, 0.26, 0.26}
\definecolor{richblack}{rgb}{0.0, 0.25, 0.25}
\definecolor{darkcerulean}{rgb}{0.03, 0.27, 0.49}
\definecolor{smokyblack}{rgb}{0.06, 0.05, 0.03}
\definecolor{warmblack}{rgb}{0.0, 0.26, 0.26}
\definecolor{cobalt}{rgb}{0.0, 0.28, 0.67}
\definecolor{darkcobalt}{rgb}{0.1, 0.38, 0.77}
\usepackage[colorlinks=true,
            linkcolor=cobalt,
            urlcolor=darkcerulean,
            citecolor=cobalt]{hyperref}


% \setlength{\parskip}{0.01cm}


\usepackage{style}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{url}
\usepackage{algorithm2e}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{caption}
\usepackage[letterpaper, margin=1.05in]{geometry}



\newcommand{\eqdef}{\ensuremath{\,\raisebox{-1.2pt}{${\stackrel{\mbox{\upshape \scalebox{.42}{def.}}}{=}}$}}\,}

\newtheorem{gradcon}{GD LR Decay Conditions}
\newtheorem{setting}{Setting}
\newtheorem{informaltheorem}{Informal Theorem}
\newtheorem{notation}[section]{Notation}





\hypersetup{
	pdftitle={Guiding Two-Layer Neural Network Lipschitzness}, % ADD TITLE
	pdfsubject={Sung Kratsios Forman - Guiding Two-Layer Neural Network Lipschitzness}, % ADD SUBJECT
	pdfauthor={Sung Kratsios Forman}, % ADD NAMES
	pdfkeywords={Neural Networks, Lipschitz Regularity, Gradient Descent, Learning Rate Decay},
}


\numberwithin{equation}{section}
\usepackage{times}
\usepackage{svg}


                    

%%%% COMMENTING
\NewDocumentCommand{\del}{m}{
    {\color{gray}{ \tiny {#1} }}
}

\definecolor{cobaltt}{rgb}{0.0, 0.28, 0.67}
\definecolor{deepcerise}{rgb}{0.85, 0.2, 0.53}
\definecolor{darkgreen}{rgb}{0.0, 0.2, 0.13}

\NewDocumentCommand{\add}{m}{{\color{violet}{{#1}}}}
\newcommand{\change}[1]{{\color{deepcerise}{#1}}}




\usepackage{thm-restate}
\newtheorem{ass}{Assumption}



\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}


\bibliographystyle{plainnat}

\begin{document}



\maketitle

\vspace{-1em}




{ 
\centerline {\large \bfseries \scshape Abstract}
\begin{quote}
We demonstrate that applying an \textit{eventual decay} to the learning rate (LR) in empirical risk minimization (ERM)---where the mean-squared-error loss is minimized using standard gradient descent (GD) for training a two-layer neural network with Lipschitz activation functions---ensures that the resulting network exhibits a high degree of Lipschitz regularity, that is, a small Lipschitz constant. Moreover, we show that this decay does not hinder the convergence rate of the empirical risk, now measured with the Huber loss, toward a critical point of the non-convex empirical risk. From these findings, we derive generalization bounds for two-layer neural networks trained with GD and a decaying LR with a sub-linear dependence on its number of trainable parameters, suggesting that the statistical behaviour of these networks is independent of overparameterization. We validate our theoretical results with a series of toy numerical experiments, where surprisingly, we observe that networks trained with constant step size GD exhibit similar learning and regularity properties to those trained with a decaying LR. This suggests that neural networks trained with standard GD may already be highly regular learners.
\end{quote}
}



\vspace{0.65em}

\textbf{\textit{Keywords}}\;\, Neural Networks, Lipschitz Regularity, Gradient Descent, Learning Rate Decay


\section{Introduction}
\label{s:Introduction}

There has recently been a great deal of research into the Lipschitzness of neural networks~\citep{virmaux2018lipschitz,Herrera2020LocalLB,jordan2020exactly,limmer2024reality,khromov2024some}. 
%
Lipschitz constraints on neural networks promote reliable generalization~\citep{bartlett2017spectrally,hou2023instance} and guarantee robustness to noise/errors in the network's inputs~\citep{tsuzuku2018lipschitz,singla2021improved,zhang2022rethinking,fazlyab2024certified}. 
%
Moreover, these constraints are not too onerous: for $L\geq 0$, the class of $L$-Lipschitz neural networks is universal in the class of $L$-Lipschitz functions on $[0,1]^d$~\citep{hong2024bridging}. 
Lipschitz neural networks are closely connected to generative modeling via neural optimal transport~\citep{korotinwasserstein,korotin2023neural,benezet2024learning,kolesov2024energyguided,persiianov2024inverse} due to the Kantorovich-Rubinstein duality popularized in deep learning by Wasserstein GANs~\citep{arjovsky2017wasserstein}. 

\hspace{2em}In practice, neural networks are typically guided toward a desired level of Lipschitz regularity either by: (a) incorporating a differentiable regularizing penalty~\citep{fazlyab2024certified} into the objective function optimized during training, or (b) fundamentally modifying the neural network layers themselves~\citep{trockman2021orthogonalizing,meunier2022dynamical,wang2023direct,araujo2023a}. Approach (a) places these training objectives beside standard empirical risk minimization (ERM), while (b) places these modified architectures beside the scope of student deep learning models. It is, therefore, natural to ask if Lipschitzness can be simply achieved in multilayer perceptrons (MLPs) with standard activation functions trained with conventional \textit{gradient descent (GD)} with sub-Gaussian initialization of parameters. This paper, thus, studies the following question:\\
\begin{equation}
\label{eq:MainQ}
\tag{Q}
\resizebox{0.94\hsize}{!}{$
    \mbox{{\color{warmblack}{
        \textit{Can we prescribe the Lipschitzness of a two-layer MLP by tweaking the GD learning rate?}}
    }}
$}
\end{equation}
\\
To the best of our knowledge, the answer to this is only understood at the exact moment of (random) GD \textit{initialization}, for which tight bounds are known~\citep{geuchen2024upperlowerboundslipschitz}, while the general question, concerning two-layer MLPs trained with randomly initialized GD, remains open.

% \noindent
\begin{figure}[h]%[H]
% \begin{figure}[!htb]
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
            \includegraphics[width=0.9\linewidth]{Imgs/HubervsL2.png}
        % \caption{Huber Loss Function}
        \label{fig:Realistic_Contexts}
    \end{minipage}%
    ~
    \begin{minipage}{0.50\textwidth}
        \centering
        \[
            \begin{aligned}
                \ell:\mathbb{R}^2 & \rightarrow [0,\infty)
            \\
                \ell(\hat{y},y)
            & \eqdef 
                \begin{cases}
                \frac{1}{2}{|\hat{y}-y|^2}                   & \text{for } |\hat{y}-y| \le \delta, \\
                % \left(
                |\hat{y}-y| - \frac{1}{2}
                % \right) 
                & \text{otherwise.}
                \end{cases}
            \end{aligned}
                % \quad \text{for }y,\hat y\in\mathbb{R}.
        \]
        % \caption{The Huber loss function.}
        % \label{fig:Language}
    \end{minipage}
    \caption{The Huber loss $(\ell)$: a $1$-Lipschitz surrogate of the mean squared error loss.}
    \label{fig:Realistic_Contexts}
\end{figure}
Here, we study~\eqref{eq:MainQ} by training on the empirical risk under the mean squared error (MSE) loss function, illustrated by the {\color{red}{dashed red curve}} in Figure~\ref{fig:Realistic_Contexts}, and we validate and test on the risk associated to the Huber loss function, illustrated by the {\color{blue}{blue curve}} in Figure~\ref{fig:Realistic_Contexts}.  These two loss functions have the same sets of minima, with the MSE allowing for convenient training dynamics, while the Huber loss is $1$-Lipschitz, which is more convenient for statistical and optimization guarantees.  


\begin{figure}[h]%H]
% \begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\linewidth]{learning_rate_decay.png}
    \captionsetup{width=0.9\textwidth}
    \caption{\textbf{Learning Rate Decay:} A decaying LR ({\color{blue}{blue}} curve) guarantees a desired Lipschitz regularity of the trained neural network (Theorem~\ref{thrm:lr_lip}), while the standard constant step size ({\color{orange}{orange}} line) guarantees quadratic convergence to a stationary point of the ER. Following the constant {\color{orange}{orange}} line for a long enough time horizon $T>0$ and then tracing the {\color{blue}{blue}} curve's decay rate yields both quadratic convergence to a stationary point of the ER and the desired degree of Lipschitzness of the trained function (Theorem~\ref{thrm:Convergence_and_stability}). In particular, the class of two-layer neural networks trained in this manner with GD tracing this two-part {\color{green}{green}} ``hybrid'' curve generalize at a rate independent of their width (Corollary~\ref{cor:GeneralizationImplication}).}
    \label{fig:idea}
% \end{wrapfigure}
\end{figure}

\hspace{2em}This paper provides a \textbf{positive} answer to this question. As shown in Figure~\ref{fig:idea}, the key is that the learning rate must \textit{eventually decay} ({\color{blue}{blue}} curve) at a specific rate, in contrast to vanilla GD, which maintains a constant learning rate ({\color{orange}{orange}} line).  
This decay does not need to begin immediately, and can instead begin after an arbitrarily long but prescribed time horizon ($T>0$).  
Decaying the learning rate in this way implies the following special case of our main result:
\begin{informaltheorem}%[Informal]
\label{IntroTheorem}
Let $f_{\Theta_t}$ denote a two-layer neural network with $L_{\sigma}$-Lipschitz and differentiable activation function and with $P\ge d$ dimensional parameter vector at the $t^{\text{th}}$ iteration of GD denoted by $\Theta_t$ with $\Theta_0\sim \mathcal{N}(0,I_P)$.  
If the learning rate decays at $\mathcal{O}(e^{-rt})$ for some $r>0$ then
\begin{equation*}
% \label{eq:thrm_lr_lip__Bound__Intro}
% \resizebox{0.925\hsize}{!}{$
    \underset{t=0,\dots,T}{\max}
    \,
            \operatorname{Lip}(f_{\Theta_t}) 
        \in 
            \mathcal{O}
            \biggl(
                \Big( 
                L_\sigma 
                %
                \,
                    % 
                    % Note: Absorbed                 \kappa and 2\max\{C_W,C_B\} into big O
                    \sqrt{p}
                    + \frac{
                        \frac{1}{r} (2 - e^{-rT})
                        % Note 1>e^{-r}, so I upper-bound e^{-r} into 2 to keep it clean
                        % +e^{-r} 
                    }{N}
                \Big)^2
            \biggr)
% $}
\end{equation*}
with probability at least $1-4 e^{-p}$.
\end{informaltheorem}


\noindent Consequently, we find that running standard GD with a fixed learning rate up to time $T$, followed by decay at the prescribed rate, ensures both optimal convergence to a critical point of the ERM functional and that the trained two-layer MLP achieves the desired Lipschitz regularity.




\paragraph{Our Contribution}
Our first main result (Theorem~\ref{thrm:lr_lip}) shows that for any sufficiently large Lipschitz constant $L\ge 0$, a two-layer MLP with Lipschitz activation trained with our data-dependent step size limit becomes $L$-Lipschitz after $T$ iterations, provided the LR constraints in GD LR Decay Conditions~\ref{GradCons} are satisfied.

\hspace{2em}As a consequence, we derive a width-independent generalization guarantee for two-layer MLPs trained with gradient descent for $T$ iterations on $N$ i.i.d.\ noisy training instances (Corollary~\ref{cor:GeneralizationImplication}) in $\mathbb{R}^d \times \mathbb{R}$. This generalization bound converges at the non-parametric rate of $\mathcal{O}(1/\sqrt[d]{N})$ with high probability, and the constant in the bound can be explicitly controlled by adjusting $T$ and $L$.

\hspace{2em}Importantly, we show that our step size schedule is compatible with traditional GD convergence requirements. Specifically, under additional assumptions on the activation function's derivatives, an optimal convergence rate to a critical point of the ERM loss can be achieved by starting with a constant step size and letting it decay at the prescribed rate (Theorem~\ref{thrm:Convergence_and_stability}) of $\mathcal{O}(1/\sqrt{T})$. 






\subsection{Related Work}
\label{s:Introduction__ss:RelatedWork}

\paragraph{The Inductive Bias Encoded into Neural Networks by Gradient Descent}
Substantial research has focused on the implicit biases induced by gradient descent during the empirical risk minimization in neural network architectures. Progress has been made in explaining these biases by showing that sufficiently wide two-layer MLPs tend to converge toward cubic interpolating splines~\citep{jin2023implicit} and exhibit benign overfitting~\citep{pmlr-v178-shamir22a}. These properties of GD-trained networks are also reflected in the kernelized infinite-width limit for neural networks with randomized hidden layers~\citep{heiss2023implicit,mei2022generalization}, which themselves are currently well-understood~\citep{gonon2020risk,gonon2023approximation} surrogates for standard two-layer MLPs trained with GD. Indeed, it is known that there is a gap between the statistical behaviour of two-layer networks when only the final layer is trained when compared to two-layer neural networks trained using only a single GD step; the latter of which learns superior latent feature representations~\citep{ba2022high}.

\hspace{2em}In the case of deeper networks (those with more than two layers), it has been shown that when activation functions are removed (i.e., identity activations), GD training leads deep unactivated MLPs to solve a principal component analysis (PCA) problem~\citep{arora2018a}. 
Additionally, there is a large body of literature on the training dynamics of neural networks in the mean-field or infinite-width regime~\citep{pmlr-v195-abbe23a,pmlr-v119-golikov20a}. Other works examine the negative impact of overparameterization on GD convergence rates in the student-teacher setting~\cite{xu2023over}. 

\paragraph{Variations of Gradient Descent}
GD and its variants have been extensively studied in the non-convex optimization literature. Research has focused on their convergence rates to critical points of the empirical risk functional~\citep{pmlr-v247-patel24a,pmlr-v247-zeng24a}, stationary points of specific non-convex function classes~\citep{faw2023beyond,anonymous2025methods}, and under spectral conditions~\citep{velikanov2024tight}. The impact of hyperparameters, such as step size, on GD convergence~\citep{pmlr-v247-wu24b}, as well as its stability properties~\citep{mulayoff2024exact,zhu2024uniform,zhu2024stability}, has also been well explored. Additionally, GD has proven effective in solving specialized problems like matrix completion~\citep{baes2021low,pmlr-v247-ma24a}.
% 
Recent work has also explored the design of GD-type algorithms to meet meta-optimality criteria~\citep{casgrain2019latent,pmlr-v134-casgrain21a}, studying non-Euclidean settings~\citep{alimisis2021momentum,hsieh2024riemannian}, and extending GD to proximal splitting methods for objectives that combine differentiable and lower semi-continuous convex parts~\citep{patrascu2021stochastic,minh2022strong}. 



 

\subsection{Paper Outline}
\label{s:Introduction__ss:Outline}
Section~\ref{s:prelim} provides an overview of the notation, background concepts, definitions, and training dynamics of two-layer neural networks trained with GD, using decoupled learning rates for the hidden and output weights and biases. Section~\ref{s:MainResults} presents our learning rate decay conditions for GD, along with our main results: the Lipschitzness findings in subsection~\eqref{s:MainResults}, the implied generalization bounds in subsection~\ref{s:MainResults__ss:Implications} with sub-linear dependence on the number of trainable neural network parameters, and the alignment of our results with well-known optimization guarantees for GD with a fixed step size in subsection~\ref{s:MainResults__ss:Optim}. 
All major proof details are relegated to the appendix.


\hspace{2em}Our theoretical results are then validated on transparent toy experiments in Section~\ref{s:Discussion__ss:Validation}, where we confirm that two-layer neural networks trained with our learning rate decay exhibit superior Lipschitz regularity compared to those trained with constant step size, while retaining predictive power. Our toy experiments further support our findings: larger sample sizes lead to more regular networks, and the number of parameters only linearly affects the Lipschitz constant.




\section{Preliminaries} 
\label{s:prelim}
\subsection{Notation}
 \label{s:prelim__ss:notation}

Let $\mathbb{N}_+$ denote the positive integers and $\mathbb{N}\eqdef \mathbb{N}_+\cup\{0\}$.
Let $f,g:\mathbb{N}\to \mathbb{R}$; we write $f\lesssim g$ if there is a constant $c>0$ such that $f(t)\leq c\,g(t)$ for every $t\in \mathbb{N}$; equivalently, we write $f\in \mathcal{O}(g)$.
Given a sequence $\mathbf{v}\eqdef (v_t)_{t=0}^{\infty}$ of vectors in some vector space $V$, we denote the \textit{backwards difference} at a given time $t\in \mathbb{N}_+$ by $\Delta v_t\eqdef v_t-v_{t-1}$.
% 
All random variables in this paper are henceforth defined on the probability space $(\Omega,\mathcal{F},\mathbb{P})$. 



\subsection{Background}
\label{s:prelim__ss:Defs}
The definitions required to formulate our main results are aggregated here.

\paragraph{Lipschitz Functions}
In this paper, we study neural networks of a given Lipschitz regularity. We say a map $f:\mathbb{R}^d\to \mathbb{R}$ is $L$-\textit{Lipschitz} and write $\operatorname{Lip}(f) = L$, for a Lipschitz constant $L\geq 0$, if 
\begin{equation*}
\label{eq:Lipschitz_definition}
|f(\mathbf{x}_1)-f(\mathbf{x}_2)|\leq L\, \|\mathbf{x}_1-\mathbf{x}_2\|
\end{equation*}
for every $\mathbf{x}_1,\mathbf{x}_2\in \mathbb{R}^d$, where $\|\cdot\|$ denotes the Euclidean norm on $\mathbb{R}^d$. By Rademacher's Theorem~\citep%[Rademacher's Theorem - 3.1.6]
{Federer_GeometricMeasureTheory_1978}, if a function is $L$-Lipschitz then it is Lebesgue differentiable almost everywhere and the norm of its gradient is no larger than $L$ (wherever it is defined). 


\paragraph{Isotropic Sub-Gaussian Distributions}
A random vector $X$ taking values in $\mathbb{R}^p$ is said to be \textit{isotropic} if $\mathbb{E}[XX^{\top}]=I_p$ where $I_p$ is the $p\times p$ identity matrix. 
Fix a tail parameter $c>0$. A random vector is said to be $c$-\textit{sub-Gaussian} if for all ``tail levels'' $t\geq 0$ we have $\mathbb{P}(\|X\| \geq t)\leq 2e^{-t^2/c^2}$.



\paragraph{Shallow Neural Networks}
Fix a \textit{neural network width}/feature space dimension $p\in \mathbb{N}_+$ and a maximal admissible weight size $M>0$.
Following~\cite{MR4243432}, we distinguish the function realized by a neural network from its parameter space. 
We denote the \textit{parameter space} of the class of shallow neural networks of width $p$ by $\mathcal{N\!N}(d,p,M)$ as consisting of all quadruples $\Theta\eqdef (B,W,b,c)$ of a $1\times d$ weight matrix $B\in [-M,M]^{1\times p}$, a hidden weight matrix $W\in [-M,M]^{p\times d}$, a bias $c\in [-M,M]$, and a hidden bias vector $b\in [-M,M]^{p}$. That is, 
\[
        \mathcal{N\!N}(d,p,M)
    \eqdef
        [-M,M]^{1\times p}\times [-M,M]^{p\times d}\times [-M,M]\times [-M,M]^{p} 
    \leftrightarrow
        [-M,M]^P
\]
where the dimension of the parameter space is $P\eqdef (1+d)(p+1)$. We denote the parameter space of unconstrained shallow neural networks of width $p$ by $\mathcal{N\!N}(d,p)\eqdef \bigcup_{M>0}\, \mathcal{N\!N}(d,p,M)$. We will identify $\mathcal{N\!N}(d,p)$ with $\mathbb{R}^P$ and each $\mathcal{N\!N}(d,p,M)$ is identified with $[-M,M]^P$.


\begin{definition}[Realization of a Two-Layer MLP]
\label{defn:Two_LayerMLP}
Fix an activation function $\sigma:\mathbb{R}\to \mathbb{R}$, then any such $\Theta \in \mathcal{N\!N}(d,p)$ \textit{realizes} a $\sigma$-neural network two-layer multilayer perception (MLP) $f_{\Theta}:\mathbb{R}^d\to \mathbb{R}$, mapping any $\mathbf{x}\in \mathbb{R}^d$ to
\begin{equation}
\label{def:NNs}
	f_\Theta (\mathbf{x}) = B^\top \sigma \bullet (W\mathbf{x} + b) + c,
\end{equation}
where $\sigma$ acts componentwise on any vector $\mathbf{v}\in \mathbb{R}^{p}$ by 
$
\sigma \bullet \mathbf{v} 
\eqdef 
\big(
    \sigma(v^i)
\big)_{i=1}^p
$.
\end{definition}



\subsection{Training Objectives and Dynamics}
We now describe our training and evaluation objectives, as well as the dynamics of our GD policy, with variable steps used to optimize our training objectives.
\subsubsection{Training and Evaluation Objectives} 
Consider a model $f_\Theta$ where $\Theta$ represents the vector of parameters. Let $(\mathbf{x}_1, y_1),\ldots, (\mathbf{x}_N, y_N)\in \mathcal{S}$ be i.i.d.\ random variables on $\mathbb{R}^{d+1}$ drawn according to a data-generating distribution (probability measure) $\mathbb{Q}$ on $\mathbb{R}^{d+1}$. 
Our objective is to minimize the \textit{empirical risk} (ER) associated with the mean squared error (MSE) loss function\
\begin{equation}
    \label{eq:objective_raw}
	\underbrace{
    \mathcal{R}_{\mathcal{S}} (\Theta) 
    \eqdef 
        \frac{1}{N} \sum \limits_{n=1}^N \lVert f_\Theta(\mathbf{x}_n ) - y_n \rVert^2
    }_{\text{MSE-Based Empirical Risk}}
    \mbox{ and }
    \underbrace{
    \hat{\mathcal{R}}_{\mathcal{S}} (\Theta) 
    \eqdef 
        \frac{1}{N} \sum \limits_{n=1}^N 
            \ell\big(
                    f_\Theta(\mathbf{x}_n ) 
                , 
                    y_n 
            \big)
    }_{\text{Huber-Based Empirical Risk}}
\end{equation}
where $f_{\Theta}$ is a two-layer neural network whose (vectorized) parameters are recorded by $\Theta\in \mathbb{R}^P$, and $\ell$ is the Huber loss function, defined in~\eqref{fig:Realistic_Contexts}.  
% 
The out-of-sample performance of our models is quantified by their \textit{true risk}, defined for any two-layer MLP $\Theta\in \mathcal{N\!N}(d,p)$ by
\begin{equation}
\label{eq:true_Huber_risk}
    \mathcal{R}(\Theta)
    \eqdef 
    \mathbb{E}_{(X,Y)\sim \mathbb{Q}}\big[
        \ell(f_{\Theta}(X),Y)
    \big]
.
\end{equation}


Having defined the MSE-based ER, we may use it as a training objective for GD-based ERM.
\subsubsection{Gradient Descent with Variable Step Size}
Fix an initial gradient descent parameter $\Theta_0$, which is a random vector in $\mathcal{N\!N}(d,p)$ defined on $(\Omega,\mathcal{F},\mathbb{P})$. We consider \textit{variable learning rates} $\boldsymbol{\alpha}\eqdef (\alpha_t)_{t=0}^{\infty}$ where, for every $t\in \mathbb{N}_+$, the vector of learning rates at iteration $t$ for each neural network parameter belongs to $\alpha_t\eqdef (\alpha_t^W,\alpha_t^B,\alpha_t^b,\alpha_t^c)\in [0,\infty)^{4}$. For every $t\in \mathbb{N}$, $\alpha_t^W,\alpha_t^B,\alpha_t^b$, $\alpha_t^c$ respectively denote the learning rate for the weight matrices $W$ and $B$ as well as the biases $b$ and $c$.

\hspace{2em}The \textit{iterates} of the gradient descent algorithm used to minimize the MSE-empirical risk functional $\mathcal{R}_{\mathcal{S}}$, defined in~\eqref{eq:objective_raw}, over the (random) training data $\mathcal{S}$, and given a variable learning rate $\boldsymbol{\alpha}$ can be written explicitly, and define a stochastic process $\boldsymbol{\Theta}\eqdef (\Theta_t)_{t=0}^{\infty}$ taking values in $\mathcal{N\!N}(d,p)$ and defined on $(\Omega,\mathcal{F},\mathbb{P})$. 
For each $t\in \mathbb{N}_+$, the $t^{\operatorname{th}}$ iterate $\Theta_t \eqdef (W_t,B_t,b_t,c_t)$ is given \textit{recursively} by
\begin{equation}
    \label{eqn:grad_desc}
\resizebox{0.925\linewidth}{!}{$
\begin{aligned}
        W_{t}   &=   W_{t-1} - \frac{2\alpha_{t}^W}{N}  \sum_{n=1}^{N}  \left ( B_{t-1}^\top \sigma  \bullet  (W_{t-1}\mathbf{x}_n  +  b_{t-1})  +  c_{t-1}  -  y_n \right )^\top 
            %
        B_{t-1}^\top \left ( \sigma'  \bullet  (W_{t-1}\mathbf{x}_n + b_{t-1}) \right ) \mathbf{x}_n^\top \\
            %
        b_{t}   &=   b_{t-1} - \frac{2\alpha_{t}^b}{N}  \sum_{n=1}^{N}  \left ( B_{t-1}^\top \sigma  \bullet  (W_{t}\mathbf{x}_n + b_{t-1}) + c_{t-1} - y_n \right )^\top   B_{t-1}^\top 
            %
        \left( \sigma'  \bullet  (W_{t}\mathbf{x}_n + b_{t-1}) \right) \\
            %
        B_{t}   &=   B_{t-1} - \frac{2\alpha_{t}^B}{N}  \sum_{n=1}^{N}  \left ( B_{t-1}^\top \sigma  \bullet  (W_{t}\mathbf{x}_n + b_{t}) + c_{t-1} - y_n \right )   \left( \sigma  \bullet  (W_{t}\mathbf{x}_n + b_{t}) \right)^\top \\ 
            %
        c_{t}   &=   c_{t-1} - \frac{2\alpha_{t}^c}{N}  \sum_{n=1}^{N}  \left ( B_{t}^\top \sigma  \bullet  (W_{t}\mathbf{x}_n + b_{t}) + c_{t-1} - y_n \right )
.
\end{aligned}
$}
\end{equation}
Note that $\boldsymbol{\Theta}$ is measurable and adapted to the filtration $\mathbf{F}\eqdef (\mathcal{F}_t)_{t=0}^{\infty}$ where, for every $t\in \mathbb{N}$, $\mathcal{F}_t$ is the $\sigma$-algebra generated by $\{\mathcal{S}\}\cup\{\Theta_s\}_{s=0,\ldots,t}$. In particular, $\boldsymbol{\Theta}$ depends on the training data $\mathcal{S}$ and on the initial parameter distribution $\Theta_0$. 
 % 
One can couple the learning rates by setting $\alpha_t^W\eqdef \alpha_t^B\eqdef \alpha_t^b\eqdef \alpha_t^c$. Though our main result allows this common simplification, we do not require it.

\subsubsection{Rate Functions} 
We will enforce convergence to a given Lipschitz constant at a pre-specified rate: for example,\ super-exponential, exponential, polynomial, logarithmic, or even constant.
\begin{definition}[Rate Function]
\label{defn:RateFunction}
    A \textit{rate function} is a bounded, non-decreasing, and absolutely continuous map $G:[1,\infty)\to [0,\infty)$ whose (Lebesgue) a.e.\ derivative $g\eqdef G'$ is 
    % non-negative and 
    continuous.
    % is called a \textit{rate} function.
\medskip
\hfill\\
We denote the asymptotically maximal value of a rate function $G$ by $G^{\star}\eqdef \sup_{t\geq 1}\, G(t)$, which we use to extend $G$ to $[1,\infty]$ by setting $G(\infty)\eqdef G^{\star}$.
\end{definition}
\noindent The simplest examples of rate functions yield exponential, polynomial, and constant rates.
\begin{example}[Exponential and Polynomial Rate]
\label{ex:polynomial_convergence}
Fix $\lambda>0$ and $r>1$. Then, $G_{\exp:r}(t)\eqdef \lambda(1-e^{-rt})$ and $G_{\operatorname{poly}:r}(t)\eqdef \lambda\big(1-\frac{t^{1-r}}{1-r}\big)$ are rate functions satisfying $G(0)=\lambda$ and whose first derivative decays at exponential and polynomial rates, respectively.
\end{example}
Focal examples are rate functions whose derivative $g$ is illustrated by the ({\color{green}{green}} piecewise smooth curve) in Figure~\ref{fig:idea}. Such functions begin constant and then rapidly decay toward zero.
\begin{example}[Hybrid-Exponential Rate]
\label{ex:truncated_exponential}
Fix any constant $\lambda\ge 0$ and a time-horizon $\tau\in \mathbb{N}_+$. 
The hybrid-exponential rate function is given implicitly by its derivative, defined for each $t\ge 0$ by
\[
g(t)\eqdef \lambda\,
\begin{cases}
    1 & \mbox{ if } t\le \tau
    \\
    e^{-r(t-\tau)} & \mbox{ if } \tau<t
.
\end{cases}
\]
Specifically, $G(t)=\lambda + \int_0^t\, g(s)\,\mathrm{d}s$. Moreover, $G(1)=\lambda$, provided that $\tau>1$.
\end{example}


\section{Main Results}
\label{s:MainResults}


% \subsection{Setting}
% \label{s:prelim__ss:Setting}
Throughout this paper, we will always work in the following setting:
\begin{setting}
\label{setting}
Fix an \textit{input dimension} $d\in \mathbb{N}_+$, a \textit{sample size} $N\in \mathbb{N}_+$, and let 
$\mathcal{S}\eqdef (\mathbf{x}_n,y_n)_{n=1}^N$ be identically distributed (but not necessarily independant) random variables in $\mathbb{R}^{d}\times \mathbb{R}^1$ defined on some probability space $(\Omega,\mathcal{F},\mathbb{P})$ with law $\pi$. 
% We do not assume that the \textit{samples} $(\mathbf{x}_1,y_1),\dots,(\mathbf{x}_N,y_N)$ are independent nor that they are identically distributed.
% 
Fix an $L_{\sigma}$-Lipschitz and differentiable activation function $\sigma:\mathbb{R}\to \mathbb{R}$, with $L_{\sigma}\geq 0$, as well as a rate function $G$ (Definition~\ref{defn:RateFunction}).
\end{setting}

\begin{ass}[Standard GD-Initialization]
\label{ass:init}
    Let $\Theta_0=(W_0,B_0,b_0,c)$ be independent of $\mathcal{S}$, and the matrices $W_0$ and $B_0$ have independent, centered, isotropic, $c>0$-sub-Gaussian rows.
\end{ass}
In this paper, we will consider GD LRs for two-layer MLPs satisfying the following decay conditions.

\begin{gradcon}%[Gradient Decay]
\label{GradCons}
Let $G$ be a rate function, and let $C_W,C_B,C_b,C_c>0$ be free parameters. We require that the learning rates $(\alpha_s^W,\alpha_s^B,\alpha_s^b,\alpha_s^c)$ in~\eqref{eqn:grad_desc} satisfy
% the following \textit{decay conditions}:
\begin{align}
\label{eqn:alpha_1}
    \alpha_{s}^W
    % &
    \leq 
    \bar{\alpha}_s^{W,g}
    & \eqdef 
    \dfrac{C_W g(s)}{ L_{\sigma} \left \lVert B_{s-1} \right \rVert \sum \limits_{n=1}^{N} \Big [ \left \lVert B_{s-1} \right \rVert  \left \lVert \sigma \bullet (W_{s-1} \mathbf{x}_n + b_{s-1}) \right \rVert + \left \lvert c_{s-1} \right \rvert + \left \lvert y_n \right \rvert \Big ]   \left \lVert \mathbf{x}_n \right \rVert } 
    \\
\label{eqn:alpha_2} 
    \alpha_{s}^B 
        \leq 
    \bar{\alpha}_s^{B,g} 
        &\eqdef 
    \dfrac{C_B g(s)}{ \sum \limits_{n=1}^N \Big [ \left \lVert B_{s-1} \right \rVert  \left \lVert \sigma \bullet (W_{s}\mathbf{x}_n + b_{s}) \right \rVert + \left \lvert c_{s-1} \right \rvert + \left \lvert y_n \right \rvert \Big ] \left \lVert \sigma \bullet (W_{s} \mathbf{x}_n + b_{s}) \right \rVert }
    \\
    \alpha_s^b 
        &\leq 
    \dfrac{C_b g(s)}{\lVert B_{s-1} \rVert\sum \limits_{n=1}^N \left [ \lVert B_{s-1} \rVert \lVert \sigma \bullet (W_s \mathbf{x}_n + b_{s-1}) \rVert + \lvert c_{s-1}\rvert + \lvert y_n \rvert \right ] } \label{eqn:alpha_3}
    \\
    \alpha_s^c 
        &\leq 
    \dfrac{C_c g(s)}{\sum \limits_{n=1}^N \lVert B_s \rVert \lVert \sigma \bullet (W_s \mathbf{x}_n + b_s) \rVert + |c_{s-1}| + |y_n|} 
.
\label{eqn:alpha_4}
\end{align}
\end{gradcon}
% 

We are now in place to present our main results.
\subsection{Guidance toward Lipschitzness via Learning Rate Decay}
\label{s:MainResults__ss:LipGuide}
Our main result shows that, by respecting the learning rate bounds in~the GD LR Decay Conditions~\ref{GradCons}, we obtain the following high-probability bound on the Lipschitz constant of any two-layer neural network trained by gradient descent. The following is a rigorous version of Informal Theorem~\ref{IntroTheorem}. 
\begin{theorem}[Lipschitz Control Via Learning Rate Decay]
\label{thrm:lr_lip}
In Setting~\ref{setting}, suppose the GD initialization satisfies Assumption~\ref{ass:init}.
%%
If the variable learning rate $\boldsymbol{\alpha}$ satisfies the GD LR Decay Conditions~\ref{GradCons} then there is an absolute constant $\kappa>0$ such that for every $\eta>0$ and every $T \in \mathbb{N}_+$,
\begin{equation}
\label{eq:thrm_lr_lip__Bound}
\resizebox{0.925\hsize}{!}{$
    \underset{t=0,\dots,T}{\max}
    \,
            \operatorname{Lip}(f_{\Theta_t}) 
        \leq 
            L_\sigma 
            %
            \left( 
                % \big(
                    \sqrt{p}
                    +
                    \kappa\big(\sqrt{\max\{p,d\}} +\eta\big)
                % \big)
                + \frac{2\max\{C_W,C_B\}
                    (G(T) + g(1))
                }{N}
            \right)^2
$}
\end{equation}
with probability at least $1-4 e^{-\eta^2}$, and $\kappa$ depends only on the distributions of $B_0$ and $W_0$.
\end{theorem}



\noindent Our first example illustrates the decaying ({\color{blue}{blue}} curve) in Figure~\ref{fig:idea} with a polynomial decay.
\begin{example}[Lipschitz Bounds For Polynomial Learning Rate Decay]
\label{ex:Polydecay}
Consider the setting of Theorem~\ref{ex:polynomial_convergence}, $r>0$,  $p\ge d$, $C_W=C_B=1$, and let $G\eqdef G_{\operatorname{poly}:r}$ be the polynomial decay rate of Example~\ref{ex:polynomial_convergence}. 
There is an absolute constant\footnote{Set $\tilde{\kappa}\eqdef 
2\,
\max\{1,\kappa\}$ and $\eta=p$.} $\tilde{\kappa}>0$ such that for every $t\in \mathbb{N}_+$ and $\eta>0$,
\[
    \max_{t=0,\dots,T}\,
            \operatorname{Lip}(f_{\Theta_t}) 
        \leq 
            L_\sigma 
            %
            \biggl( 
                % \big(
                    \tilde{\kappa}
                    \sqrt{p}% (\sqrt{p}+\sqrt{\eta})
                % \big)
                + \frac{2
                    \lambda\big(\frac{2r-1}{1-r} -\frac{T^{1-r}}{1-r}\big)
                    % \Big)
                }{N}
            \biggr)^2
\]
holds with probability at least $1-4e^{-
p% \eta
}$. 
\end{example}

Returning to our focal example illustrated by the ({\color{green}{green}} curve) in Figure~\ref{fig:idea}, we may apply Theorem~\ref{thrm:lr_lip} to obtain Lipschitz guarantees to the hybrid-exponential rate function of Example~\ref{ex:truncated_exponential}.
\begin{example}[Lipschitz Bounds For Hybrid-Exponential Learning Rate Decay]
\label{ex:polydecay}
In the setting of Theorem~\ref{thrm:lr_lip}. If $p\ge d$, $C_W=C_B=1$, and the rate function $G$ is as in Example~\ref{ex:truncated_exponential} then there is an absolute constant $\tilde{\kappa}>0$ such that for every $T\in \mathbb{N}_+$ and $\eta>0$,
\[
    \max_{t=0,\dots,T}
    \,
    \operatorname{Lip}(f_{\Theta_t}) 
        \leq 
            L_\sigma 
            %
            \left( 
                % \big(
                    \tilde{\kappa}
                    \sqrt{p}% (\sqrt{p}+\sqrt{\eta})
                % \big)
                + \frac{2
                    \lambda \Big(1 + 
                TI_{T\le \tau} + 
                    \big(
                        \tau + \frac{\lambda}{r} \left( 1 - e^{-r(T - \tau)} \right)
                    \big)
                I_{T>\tau}
                    \Big)
                }{N}
            \right)^2
\]
holds with probability at least $1-4e^{-
p%\eta
}$. 
\end{example}
Interestingly, we remark that as the sample size $N$ grows arbitrarily large, the Lipschitz constant of the GD iterations remain uniformly bounded in time. This mirrors the convergence of GD with our step size iterations to a ``well-behaved'' subclass of neural networks given independently of the sample size. 

\hspace{2em}Our main result has several consequences in learning theory and in the optimization theory of neural networks, which hold under additional mild structural assumptions. These are now presented.
\subsection{Learning Decay Implies Generalization Bounds with Linear Parametric Dependance}
\label{s:MainResults__ss:Implications}

We first examine the learning-theoretic implications of our main result. The guarantee shows that by specifying the number of gradient steps $t\in \mathbb{N}_+$ taken and the rate function $G$, we may modulate the generalization of our GD-trained neural network, trained on the (random) training set $\mathcal{S}\eqdef \{(X_n,Y_n)\}_{n=1}^N$.

\begin{corollary}[Generalization Bounds for GD-Trained Networks with Linear Width Dependence]
\label{cor:GeneralizationImplication}
Consider the setting of Theorem~\ref{thrm:lr_lip} and suppose that $\{(X_n,Y_n)\}_{n=1}^N$ are i.i.d.\ with compactly supported law $\mathbb{Q}$ and suppose that $d+D>2$. 
%%
For every number of GD iterations $T>0$ and every ``confidence level'' $0<\delta\le 1$, the following holds with probability at least $1-\delta$:
\[
    \big|
        \mathcal{R}(f_{\Theta_T})
        -
        \hat{\mathcal{R}}^{\mathcal{S}}(f_{\Theta_T})
    \big|
\leq 
    \underbrace{
        \Lambda(T,N)
    }_{\mathcal{O}\big(
        \max\{p,d\}+\frac1{N}
    \big)}
    \,
    C_{\mathbb{Q}} 
    \,
    \underbrace{
        \biggl(
            \frac{
                C_{d+D}
            }{\sqrt[d]{N}}
            +
            \frac{
                \sqrt{\ln(8/\delta)}
            }{\sqrt{2N}}
        \biggr),
    }_{\text{Independent of No.\ Param ($p$)}}
\]
where $\Lambda(T,N)$ denotes the right-hand side of~\eqref{eq:thrm_lr_lip__Bound} with $\eta\eqdef \frac{\operatorname{diam}(\operatorname{supp}(\mathbb{Q}))}{\sqrt{2N}}$, $C_{\mathbb{Q}}\eqdef \operatorname{diam}(\operatorname{supp}(\mathbb{Q}))$, and $0<C_{d+D}\in \mathcal{O}(\sqrt{d+D})$ is a dimensional constant\footnote{See~\eqref{eq:dimensional_constant} for an explicit expression}.
% \hfill\\
Critically, $C_{d+D}$, and $\eta$ \textit{do not} depend on $p$ and $\Lambda(T,N)\in \mathcal{O}(\max\{p,d\}+G(T)/N)$.%!
\end{corollary}

We elucidate Corollary~\ref{ex:Polydecay} in the case of random inputs on the $d-1$ dimensional sphere $\mathbb{S}^{d-1}\eqdef \{x\in \mathbb{R}^d:\, \|x\|\le 1/2\}$ of radius $1/2$.
\begin{example}[Random Inputs on Spheres with Polynomial LR Decay]
\label{ex:Polydecay__Continued}
Let $\sigma=\operatorname{ReLU}$.
In the setting of Corollary~\ref{cor:GeneralizationImplication}, let $G_{\operatorname{poly}}$ be the polynomial decay rate in~\eqref{ex:Polydecay}, and suppose that $\mathbb{Q}=\mu\otimes \mu$, where $\mu$ is any probability distribution on $\mathbb{S}^{d-1}$ and $\nu$ is any probability distribution on $[0,1]$.
Then, for every number of GD iterations $T>0$ and every confidence level $0<\delta\le 1$,
\[
\resizebox{1\hsize}{!}{$
        \big|
            \mathcal{R}(f_{\Theta_T})
            -
            \hat{\mathcal{R}}^{\mathcal{S}}(f_{\Theta_T})
        \big|
    \leq 
        %% Lambda T,N
            % \underbrace{
            \biggl( 
                % \big(
                    \tilde{\kappa}
                    \sqrt{p}% (\sqrt{p}+\sqrt{\eta})
                % \big)
                + \frac{2
                    \lambda\big(\frac{2r-1}{1-r} -\frac{T^{1-r}}{1-r}\big)
                    % \Big)
                }{N}
            \biggr)^2
            % }_{\mathcal{O}(p)}
        % \,
        % C_{\mathbb{Q}} 
        % \,
        % \underbrace{
            \biggl(
                \frac{
                    C_{d+D}
                }{\sqrt[d]{N}}
                +
                \frac{
                    \sqrt{\ln(8/\delta)}
                }{\sqrt{2N}}
            \biggr)
        % }_{\text{Independent of No.\ Param ($p$)}}
    %%
    \in \mathcal{O}\Big(
     \frac{p\sqrt{\log(8/\delta)}}{\sqrt[d]{N}} 
    \Big)
$}
\]
holds with probability at least $1-\delta$.
\end{example}



We now show that our main result is compatible with existing optimization guarantees for gradient descent, allowing both convergence and the regularity and learning guarantees discussed.

\subsection{Coalescence of \texorpdfstring{Theorem~\ref{thrm:lr_lip}}{Our Main Result} with Optimization Guarantees for GD}
\label{s:MainResults__ss:Optim}

The asymptotically decaying step size required by Theorem~\ref{thrm:lr_lip} is an unorthodox choice in the optimization literature. This raises a natural question: does this eventual modification to the step size impact the convergence of GD to a \textit{critical point} of the empirical risk functional? Surprisingly, we find that our eventual step size modification comes at no cost to the GD convergence rate and only yields the additional benefit of gaining control of the Lipschitz constant of the trained neural network. 


\hspace{2em}We require the following additional mild regularity requirements on the activation functions used, as well as some regularity of the target function and the input data.
\begin{ass}[Additional Structure for Convergence]
\label{ass:2}
Suppose that our input data $(\mathbf{x}_n)_{n=1}^N$ has finite second moment, that is,\ $\mathbb{E}[\|\mathbf{x}_n\|^2]<\infty$ for $n=1,\dots,N$. Suppose also that the loss function $\ell$ has bounded gradient, that is, $\sup_{x,y}\in {\mathbb{R}^{d+1}}\, \|\nabla \ell(x,y)\|<\infty$.
% 
Additionally, we assume that there are $\sigma_{\text{max}}, \sigma^1_{\text{max}}, \sigma^2_{\text{max}}>0$ such that for every $x\in \mathbb{R}$
\begin{equation} \label{eqn:activation_bounded}
    \begin{split}
    |\sigma (x) | \leq \sigma_{\text{max}}, \;
    |\sigma ' (x) | \leq \sigma^1_{\text{max}}, \;
    |\sigma '' (x) | \leq \sigma^2_{\text{max}}
    .
    \end{split}
\end{equation}
\end{ass}
Under the structural conditions in Assumption~\ref{ass:2}, our step size modification maintains the convergence guarantee of gradient descent. Importantly, it is not at all clear, a priori, that the empirical risk functional $\mathcal{R}_{\mathcal{S}}$ is locally Lipschitz, even in view of the results of~\cite{herrera2023locallipschitzboundsdeep,JMLR:v24:22-1381}. This raises questions about defining the optimal constant GD-step size for rapid convergence, which typically depends on the reciprocal Lipschitz constant of the loss~\citep[Section 1.2.3]{Nesterov}. However, our next and final result shows that, under the GD LR Decay Conditions~\ref{GradCons}, the empirical risk is locally Lipschitz, and the hybrid learning rate decay in Figure~\ref{fig:idea} ensures both convergence and Lipschitzness of the learned network.

\begin{theorem}[Convergence at the Optimal GD-Rate]
\label{thrm:Convergence_and_stability}
In Setting~\ref{setting} and under Assumption~\ref{ass:2}, let $f_{\Theta_t}$ be a neural network as defined in~\eqref{def:NNs} optimized by GD satisfying the GD LR Decay Conditions~\ref{GradCons} to $T\geq 1$ iterations. Then, $\mathcal{R}_{\mathcal{S}}$ has a finite Lipschitz constant on $[-M,M]^P$, that is,\
% \[
\(
    \operatorname{Lip}(\mathcal{R}_{\mathcal{S}}) <\infty
.\)
% \]
% and $\mathcal{R}_{\mathcal{S}}$ is a Lipschitz function on $[-M,M]^P$, 
\\
Consequently, the following strengthened constraints are well-defined and ensure that the Lipschitz bounds in~\eqref{eq:thrm_lr_lip__Bound} hold with probability at least $1-4e^{-\eta^2}$:
\begin{equation} \label{eqn:alpha_min}
        \begin{split}
            \alpha_t^W 
            % &
            = \min \Big \{
                    1 / \operatorname{Lip}(\mathcal{R}_{\mathcal{S}})
                    \, , \,
                    \bar{\alpha}_t^{W,g}
                \Big \}
            \,\,\mbox{ and }\,\,
            % \\
            \alpha_t^B 
            % &
            = \min \Big \{
                    1 / \operatorname{Lip}(\mathcal{R}_{\mathcal{S}})
                    \, , \,
                    \bar{\alpha}_t^{B,g}
                \Big \}
.
        \end{split}
    \end{equation} 
Moreover, choosing the free parameters $C_W$ and $C_B$ large enough, the GD with backtracking line search \textit{converges} to a stationary point of $\mathcal{R}_{\mathcal{S}}$ at a rate of
\begin{equation}
\label{eq:optimal_converegence_maintained}
\min \limits_{0\leq t\leq T} \lVert \nabla_\Theta \mathcal{R}_{\mathcal{S}} (\Theta_t)\rVert \in \mathcal{O}\biggl(      
    \frac{1}{\sqrt{T+1}}
\biggr)
.
\end{equation}
\end{theorem}










\section{Experimental Ablation}
\label{s:Discussion__ss:Validation}
We now investigate the impact of GD learning rate decay on the performance of two-layer MLPs and compare it to standard GD with a constant learning rate initialized at the same value. Our analysis is based on three key experiments: (1) We explore the statistical properties of the trained networks under varying noise regimes (both low and high) for both vanilla and decaying GD. (2) We assess how the target function's characteristics influence the Lipschitz regularity of the learned function, focusing on cases where the target function exhibits oscillating or singular derivatives. (3) We verify the effect of sample size $N$—appearing on the right-hand side of our bounds in Theorem~\ref{thrm:lr_lip}—on the evolution of the Lipschitz constant throughout training.

\hspace{2em}In each experiment, the task is a non-parametric regression, modeled as
$$
    Y_n = f(X_n) + \beta\varepsilon_n,
$$
where $(X_1,\varepsilon_1),\dots,(X_N,\varepsilon_N)$ are i.i.d. random variables, with $X_n\sim \mathcal{N}(0,1)$, and each $\varepsilon_n$ an independent standard Gaussian random variable and given a ``noise level'' $\beta>0$.  

\hspace{2em}Each toy experiment is independent and only validates our theory.  Namely, that a decaying learning rate still yields expressive networks (Theorem~\ref{thrm:Convergence_and_stability}), they learn independently of their width, and thus of their number of parameter $p$ (Corollary~\ref{cor:GeneralizationImplication}), and that that the GD LR Decay Conditions~\ref{GradCons} indeed yield networks with \textit{relatively} small Lipschitz constants (Theorem~\ref{thrm:lr_lip}).

\paragraph{Effect of Sample Size}
We examine the evolution of the Lipschitz constant of a two-layer neural network during training, varying sample size $(N)$ and the number of trainable parameters ($P$). As predicted by Theorem~\ref{thrm:lr_lip}, larger sample sizes lead to neural networks with higher regularity, reflected by a smaller Lipschitz constant. The Lipschitz constant of GD-trained two-layer networks with learning rate decay scales linearly with $P$. The target function is $f(x) = x^3 + \sqrt{|x|}$. 
% Figure~\ref{fig:Lip_vs_N} shows the results, with means and standard deviations from $50$ GD runs.

\begin{figure}[!htb]%[H]
% \begin{figure}[!htb]
\vspace{-1em}
    \centering
    \begin{minipage}{0.475\textwidth}
        \includegraphics[width=1\linewidth]{img_rand_init/0_Effect_of_N/Lip_Constants_Exp_Rate_T200_runs20_noise0.03.png}
        \caption{\textbf{Effect of Sample Size ($N$)} 
        }
        \label{fig:Lip_vs_N}
    \end{minipage}%
    ~
    \begin{minipage}{0.475\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{hidden_experiment/Lip_Constants_Poly_Rate_T200_runs20_noise0.03.png}
        \caption{\textbf{Effect of No. Parameters ($P$)}}
        \label{fig:No_trinable_p}
    \end{minipage}
    \hfill\\
    \caption{As predicted by Theorem~\ref{thrm:lr_lip}, the learned two-layer neural network has a smaller Lipschitz constant for larger sample sizes (Figure~\ref{fig:Lip_vs_N}).  Furthermore, the Lipschitz constant of a two-layer neural network trained with a polynomially decaying learning rate grows linearly with the number of non-zero trainable parameters, which scales with the square root of the network's width (Figure~\ref{fig:No_trinable_p}). 
    Here, $\beta=0.03$, $p=200$, and $\alpha_t = 0.01$ for $t=1,\ldots,100$.}
    % \label{fig:Realistic_Contexts}
\end{figure}


% \begin{figure}[htp!]%
% \centering
% % \includegraphics[width=0.65\linewidth]{img_rand_init/0_Effect_of_noise/Lip_Constants_Exp_Rate_T200_runs20_noise0.03.eps}
% \includegraphics[width=0.45\linewidth]{img_rand_init/0_Effect_of_noise/Lip_Constants_Exp_Rate_T200_runs20_noise0.03.png}
%     \caption{\textbf{Effect of Sample Size ($N$):} As predicted by Theorem~\ref{thrm:lr_lip}, the learned two-layer neural network has a smaller Lipschitz constant for larger sample sizes.
%     Here, $\beta=0.03$, $p=200$, and $\alpha_t = 0.01$ for $t=1,\ldots,100$. }
%     \label{fig:Lip_vs_N}
% \end{figure}

% \begin{figure}[htp!]
%     \centering
%     \includegraphics[width=0.45\linewidth]{hidden_experiment/Lip_Constants_Poly_Rate_T200_runs20_noise0.03.png}
%     \caption{\textbf{Effect of No. Parameters ($P$):} 
%     As predicted by Theorem~\ref{thrm:lr_lip}%(particularly in Example~\ref{ex:Polydecay__Continued})
%     , the Lipschitz constant of a two-layer neural network trained with a polynomially decaying learning rate grows linearly with the number of non-zero trainable parameters, which scales with the square root of the network's width. In this case, $\beta=0.03$, and $\alpha_t = 0.01$ for $t=1,\ldots,100$.}
%     \label{fig:No_trinable_p}
% \end{figure}


\vspace{-2em}
\paragraph{Effect of Target Function}
Since our GD LR Decay Conditions~\ref{GradCons} are independent of the target function's regularity or singularity, we confirm that these constraints do not negatively impact the performance of trained two-layer neural networks. 

% \paragraph{Singular Target Function}

\begin{figure}[htp!]%[H]%
\vspace{-2em}
% \begin{figure}[!htb]
    \centering
    \begin{minipage}{0.33\textwidth}
        \centering
            \includegraphics[width=1\linewidth]{img_same_init/2_osculatory_bounded_P250_N150_noise0_05_lr_init0_01/Losses.png}
        % \caption
        {Evolution: of (MSE-based) ERM loss during GD-iterations.}
        \label{fig:Experiments_sin__Loss}
    \end{minipage}%
    ~
    \begin{minipage}{0.33\textwidth}
        \centering
            \includegraphics[width=1\linewidth]{img_same_init/2_osculatory_bounded_P250_N150_noise0_05_lr_init0_01/Lip_by_Iter.png}
        % \caption
        {Evolution: of Lipschitz constant during GD-iterations.}
        \label{fig:Experiments_sin__Lip}
    \end{minipage}%
    ~
    \begin{minipage}{0.33\textwidth}
        \centering
            \includegraphics[width=1\linewidth]{img_same_init/2_osculatory_bounded_P250_N150_noise0_05_lr_init0_01/Pred.png}
        % \caption
        {Post-GD: Learns vs.\ target function.}
        \label{fig:Experiments_sin__Pred}
    \end{minipage}%
\caption{\textbf{Target Function with Osculatory Derivative $f(x)=\sin(x)$:}
Since our two-layer networks have their Lipschitz constant implicitly restricted by the GD, it is natural to check if this inhibits their ability to learn a function with complicated, e.g.\ osculatory, derivative.  Our experiments indicate that this is not the case. 
Here, $\beta=0.05$, $p=250$, $N=150$, and $\alpha_t = 0.01$ for $t=1,\ldots, 100$.}
    \label{fig:Experiments_sin}
\end{figure}

% \noindent
\begin{figure}[H]%[htp!]%
% \begin{figure}[!htb]
    \centering
    \begin{minipage}{0.33\textwidth}
        \centering
            \includegraphics[width=1\linewidth]{img_rand_init/1_singular_target_function_P250_N150_noise0_05_lr_init0_01/Losses.png}
        % \caption
        {Evolution: of (MSE-based) ERM loss during GD-iterations.}
        \label{fig:Experiments_Singular__Loss}
    \end{minipage}%
    ~
    \begin{minipage}{0.33\textwidth}
        \centering
            \includegraphics[width=1\linewidth]{img_rand_init/1_singular_target_function_P250_N150_noise0_05_lr_init0_01/Lip_by_Iter.png}
        % \caption
        {Evolution: of Lipschitz constant during GD-iterations.}
        \label{fig:Experiments_Singular__Lip}
    \end{minipage}%
    ~
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{img_rand_init/1_singular_target_function_P250_N150_noise0_05_lr_init0_01/Pred.png}
        % \caption
        {Post-GD: Learns vs.\ target function.}
        \label{fig:Experiments_Singular__Pred}
    \end{minipage}%
\caption{\textbf{Singular Target Function $f(x)=\frac1{x}$ on $\mathbb{R}\setminus\{0\}$:}
Our LR Conditions~\ref{GradCons} are independent of the target function's continuity: the training loss, Lipschitz constant, and test performance remain unaffected by the LR decay. The performance of the vanilla GD-trained network and the one with LR decay is similar, however, the LR decay GD yields a notably smaller and less variable Lipschitz constant during training. Here, $\beta=0.05$, and $N=150$.}
    \label{fig:Experiments_Singular}
\end{figure}

As shown in Figures~\ref{fig:Experiments_sin} and~\ref{fig:Experiments_Singular}, networks trained with standard constant step size GD perform just as well in terms of training speed (MSE-ERM) and test accuracy (Huber loss) as those trained with our LR decay condition. However, networks with the LR decay exhibit a significantly smaller Lipschitz constant with less variability per training run.






























\section{Conclusion and Future Work}
\label{s:Conclusion}
 

In this paper, we showed that training two-layer neural networks with gradient descent and an eventual decay in the learning rate enforces strong Lipschitz regularity (Theorem~\ref{thrm:lr_lip}), without affecting the convergence rate of the empirical risk (measured with Huber loss) to a critical point (Theorem~\ref{thrm:Convergence_and_stability}). Additionally, we derived generalization bounds that are dependent \textit{linearly} on network width (Corollary~\ref{cor:GeneralizationImplication}), suggesting that overparameterization does not degrade the statistical performance of these networks. Our experiments also indicate that networks trained with constant learning rates exhibit similar regularity properties to those trained with standard GD, i.e.\ with constant step size, suggesting that GD may naturally promote regularity in neural networks.


\hspace{2em}In future work, we aim to extend our results to multilayer neural networks and explore GD-based algorithms with more complex dynamics, such as Conjugate Gradients or Heavy-Ball methods, or any GD-type method considered in~\cite{velikanov2024tight}. Stochastic variants and proximal splitting-type extensions could also be investigated. However, we expect the core message to remain unchanged: a small learning rate decay can ensure that GD-type algorithms optimize an ERM while yielding neural networks with high Lipschitz regularity, with high probability.


\section{Acknowledgements}
K.\ Sung was supported by a Natural Sciences and Engineering Research Council of Canada (NSERC) Undergraduate Student Research Award.
A.\ Kratsios acknowledges financial support from NSERC Discovery Grant No.\ RGPIN-2023-04482 and No.\ DGECR-2023-00230. A.\ Kratsios also acknowledges that resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute\footnote{\href{https://vectorinstitute.ai/partnerships/current-partners/}{https://vectorinstitute.ai/partnerships/current-partners/}}. N.\ Forman was supported by the NSERC Discovery Grant No.\ RGPIN-2020-06907.


\bibliography{references}

\appendix




\section{Proofs}
\begin{lemma}[Norm Bounds for GD-Trained Weight Matrices]
\label{lem:bounds_W_B}
In Setting~\ref{setting}, assume that $\sigma$ is $L_{\sigma}$-Lipschitz. For every $t\in \mathbb{N}_+$ the gradient-descent process $\boldsymbol{\Theta}$ with variable learning rate $\boldsymbol{\alpha}$ satisfies
    % {\small
\begin{align}
% \resizebox{0.91\hsize}{!}{$
    \lVert W_{t} \rVert_{op}
    %
    &\leq
    %
    \lVert W_0 \rVert_{op} + \frac{2L_\sigma}{N} \sum \limits_{s=1}^{t} \alpha_{s}^W \left \lVert B_{s-1} \right \rVert \sum \limits_{n=1}^{N} \left \lVert \mathbf{x}_n \right \rVert \Big [  \left \lVert B_{s-1} \right \rVert  \left \lVert \sigma \bullet (W_{s-1} \mathbf{x}_n + b_{s-1}) \right \rVert  +  \left \lvert c_{s-1} \right \rvert  +  \left \lvert y_n \right \rvert  \Big ] \label{eqn:lemma1_1}
    %
    \\
    %
    \lVert B_{t} \rVert
    %
    &\leq
    %
    \lVert B_0 \rVert + \frac{2}{N}\sum \limits_{s=1}^{t} \alpha_{s}^B \sum \limits_{n=1}^N \left \lVert \sigma \bullet (W_{s} \mathbf{x}_n + b_{s}) \right \rVert  \Big [  \left \lVert B_{s-1} \right \rVert  \left \lVert \sigma \bullet (W_{s}\mathbf{x}_n + b_{s}) \right \rVert + \left \lvert c_{s-1} \right \rvert + \left \lvert y_n \right \rvert  \Big ] \label{eqn:lemma1_2}
        \\
    \lVert b_t \rVert &\leq \lVert b_0 \rVert + \frac{2L_\sigma}{N} \sum \limits_{s=1}^t \alpha_t^b \sum \limits_{n=1}^N \left [ \lVert B_{t-1} \rVert \lVert \sigma \bullet (W_t \mathbf{x}_n + b_{t-1}) \rVert + \lvert c_{t-1}\rvert + \lvert y_n \rvert \right ] \lVert B_{t-1} \rVert  \label{eqn:lemma3_1}
        \\
            \lvert c_t \rvert &\leq |c_0| + \frac{2}{N}\sum \limits_{s=1}^t \alpha_t^c \sum \limits_{n=1}^N \lVert B_t \rVert \lVert \sigma \bullet (W_t \mathbf{x}_n + b_t) \rVert + |c_{t-1}| + |y_n|.  \label{eqn:lemma3_2}
% $}
    \end{align}
    % }
\end{lemma}
Having bounded the norms of the weight matrices, we may use the estimates in Lemma~\ref{lem:bounds_W_B} to deduce conditions on the variable learning rates of the gradient descent, which force the neural network's weights to grow at a given target rate. This target growth rate is encoded by a prescribed rate function.\\

\begin{proof}[Proof of Lemma~\ref{lem:bounds_W_B}]
    %
    We first show~\eqref{eqn:lemma1_1}. By the definition of $W_t$ in~\eqref{eqn:grad_desc}, taking the norm and the triangle inequality, we obtain
    %
    % { \small
    \allowdisplaybreaks
	\begin{align*}
    &\quad \; \lVert \Delta W_{t} \rVert_{op} 
    \\
    %
    &=
    %
    \frac{2\alpha_{t}^W}{N} \left \lVert \sum \limits_{n=1}^{N} (B_{t-1}^\top \sigma \bullet (W_{t-1} \mathbf{x}_n + b_{t-1}) + c_{t-1} - y_n )^\top B_{t-1}^\top (\sigma ' \bullet (W_{t-1} \mathbf{x}_n + b_{t-1} ))\mathbf{x}_n^\top \right \rVert_{op}
    %
    \\
    &\leq 
    %
    \frac{2\alpha_{t}^W}{N} \sum \limits_{n=1}^{N} \left \lVert (B_{t-1}^\top \sigma \bullet (W_{t-1} \mathbf{x}_n + b_{t-1}) + c_{t-1} - y_n )^\top B_{t-1}^\top (\sigma ' \bullet (W_{t-1} \mathbf{x}_n + b_{t-1} ))\mathbf{x}_n^\top \right \rVert_{op}
    %
    \\
    &\leq 
    %
    \frac{2\alpha_{t}^W}{N} \sum \limits_{n=1}^{N} \left \lvert B_{t-1}^\top \sigma \bullet (W_{t-1} \mathbf{x}_n + b_{t-1}) + c_{t-1} - y_n \right \rvert \left \lVert B_{t-1} \right \rVert  \left \lVert \sigma ' \bullet (W_{t-1} \mathbf{x}_n + b_{t-1} ) \right \rVert \left \lVert \mathbf{x}_n \right \rVert
    %
    \\
    &\leq 
    %
    \frac{2\alpha_{t}^W}{N} \sum \limits_{n=1}^{N} \Big [ \left \lvert B_{t-1}^\top \sigma \bullet (W_{t-1} \mathbf{x}_n + b_{t-1}) \right \rvert + \left \lvert c_{t-1} \right \rvert + \left \lvert y_n \right \rvert \Big ] \left \lVert B_{t-1} \right \rVert  \left \lVert \sigma ' \bullet (W_{t-1} \mathbf{x}_n + b_{t-1} ) \right \rVert \left \lVert \mathbf{x}_n \right \rVert
    %
    \\
    &\leq 
    %
    \frac{2\alpha_{t}^W}{N} \sum \limits_{n=1}^{N} \Big [ \left \lVert B_{t-1} \right \rVert  \left \lVert \sigma \bullet (W_{t-1} \mathbf{x}_n + b_{t-1}) \right \rVert + \left \lvert c_{t-1} \right \rvert + \left \lvert y_n \right \rvert \Big ] \left \lVert B_{t-1} \right \rVert  \left \lVert \sigma ' \bullet (W_{t-1} \mathbf{x}_n + b_{t-1} ) \right \rVert \left \lVert \mathbf{x}_n \right \rVert
    %
    \end{align*}
    % }
    %
    Noting that $\sigma$ is $L_{\sigma}$-Lipschitz, we have
    %
    \begin{equation} \label{eqn:delta_W_bound}
        \lVert \Delta W_{t} \rVert_{op}
        %
        \leq
        %
        \frac{2\alpha_{t}^W}{N} \sum \limits_{n=1}^{N} \Big [ \left \lVert B_{t-1} \right \rVert  \left \lVert \sigma \bullet (W_{t-1} \mathbf{x}_n + b_{t-1}) \right \rVert + \left \lvert c_{t-1} \right \rvert + \left \lvert y_n \right \rvert \Big ] L_{\sigma} \left \lVert B_{t-1} \right \rVert    \left \lVert \mathbf{x}_n \right \rVert
    \end{equation}
    Again, by the triangle inequality, we obtain
    \begin{equation*} 
        \lVert W_{t} \rVert_{op}
        %
        \leq
        %
        \lVert W_{t} - W_0\rVert_{op} + \lVert W_0 \rVert_{op} 
        %
        \leq
        %
        \lVert W_0 \rVert_{op} + \sum \limits_{s=1}^{t} \lVert \Delta W_s \rVert_{op} 
    \end{equation*}
    %
    Substituting~\eqref{eqn:delta_W_bound} yields~\eqref{eqn:lemma1_1}.
    
    We now show~\eqref{eqn:lemma1_2}. 
    %
    \begin{align}
        \lVert \Delta B_{t} \rVert
        %
        &=
        %
        \frac{2\alpha_{t}^B}{N} \left \lVert \sum \limits_{n=1}^N (B_{t-1}^\top \sigma \bullet (W_{t}\mathbf{x}_n + b_{t}) + c_{t-1} - y_n) (\sigma \bullet (W_{t} \mathbf{x}_n + b_{t}))^\top \right \rVert \notag
        %
        \\
        &\leq 
        %
        \frac{2\alpha_{t}^B}{N} \sum \limits_{n=1}^N \left \lVert (B_{t-1}^\top \sigma \bullet (W_{t}\mathbf{x}_n + b_{t}) + c_{t-1} - y_n) (\sigma \bullet (W_{t} \mathbf{x}_n + b_{t}))^\top \right \rVert \notag
        %
        \\
        &\leq 
        %
        \frac{2\alpha_{t}^B}{N} \sum \limits_{n=1}^N \left \lvert B_{t-1}^\top \sigma \bullet (W_{t}\mathbf{x}_n + b_{t}) + c_{t-1} - y_n \right \rvert \left \lVert \sigma \bullet (W_{t} \mathbf{x}_n + b_{t}) \right \rVert \notag
        \\
        &\leq 
        %
        \frac{2\alpha_{t}^B}{N} \sum \limits_{n=1}^N \Big [ \left \lVert B_{t-1} \right \rVert  \left \lVert \sigma \bullet (W_{t}\mathbf{x}_n + b_{t}) \right \rVert + \left \lvert c_{t-1} \right \rvert + \left \lvert y_n \right \rvert \Big ] \left \lVert \sigma \bullet (W_{t} \mathbf{x}_n + b_{t}) \right \rVert . \label{eqn:delta_B_bound}
    \end{align}
    %
    By the triangle inequality, we obtain
    %
    \begin{equation*}
        \lVert B_{t} \rVert 
        %
        \leq
        %
        \lVert B_{t} - B_0\rVert + \lVert B_0 \rVert 
        %
        \leq
        %
        \lVert B_0 \rVert + \sum \limits_{s=1}^{t} \lVert \Delta B_s \rVert .
    \end{equation*}
    %
    Substituting~\eqref{eqn:delta_B_bound} yields~\eqref{eqn:lemma1_2}.

    We next show~\eqref{eqn:lemma3_1}. By the definition of $b_t$ in~\eqref{eqn:grad_desc}, taking the norm and the triangle inequality, we obtain \begin{align}
            \lVert \Delta b_{t} \rVert \notag
        &=
            \frac{2\alpha_{t}^b}{N}\left \lVert 
                \sum \limits_{n=1}^N (B_{t-1}^\top \sigma \bullet(W_{t}\mathbf{x}_n + b_{t-1}) + c_{t-1} - y_n)^\top B_{t-1}^\top (\sigma ' \bullet (W_{t}\mathbf{x}_n + b_{t-1}))
            \right \rVert
        \\ &\leq \notag
            \frac{2\alpha_{t}^b}{N} 
                \sum \limits_{n=1}^N \left \lVert (B_{t-1}^\top \sigma \bullet(W_{t}\mathbf{x}_n + b_{t-1}) + c_{t-1} - y_n)^\top B_{t-1}^\top (\sigma ' \bullet (W_{t}\mathbf{x}_n + b_{t-1}))
            \right \rVert
        \\ &\leq \notag
            \frac{2\alpha_{t}^b}{N} \sum \limits_{n=1}^N 
            \left \lvert 
                B_{t-1}^\top \sigma \bullet (W_{t}\mathbf{x}_n + b_{t-1}) + c_{t-1} - y_n
            \right \rvert 
            \left \lVert 
                B_{t-1}^\top
            \right \rVert 
            \left \lVert 
                \sigma ' \bullet (W_{t}\mathbf{x}_n + b_{t-1} )
            \right \rVert
        \\ &\leq \notag
            \frac{2\alpha_{t}^b}{N} \sum \limits_{n=1}^N 
            \left \lvert 
                B_{t-1}^\top \sigma \bullet (W_{t}\mathbf{x}_n + b_{t-1}) + c_{t-1} - y_n
            \right \rvert 
            \left \lVert 
                B_{t-1}^\top
            \right \rVert 
            L_\sigma
        \\ &\leq \notag
            \frac{2\alpha_{t}^b}{N} \sum \limits_{n=1}^N 
            \left [ 
                \left \lvert B_{t-1}^\top \sigma \bullet (W_{t}\mathbf{x}_n + b_{t-1}) \right \rvert
                    + 
                \left \lvert c_{t-1} \right \rvert
                    + 
                \left \lvert y_n\right \rvert
            \right ] 
            \left \lVert 
                B_{t-1}^\top
            \right \rVert 
            L_\sigma
        \\ &\leq \label{align:norm_bound_b}
            \frac{2\alpha_{t}^b}{N} \sum \limits_{n=1}^N 
            \left [ 
                \left \lVert B_{t-1}^\top \right \lVert \left \lVert \sigma \bullet (W_{t}\mathbf{x}_n + b_{t-1}) \right \rVert
                    + 
                \left \lvert c_{t-1} \right \rvert
                    + 
                \left \lvert y_n\right \rvert
            \right ] 
            \left \lVert 
                B_{t-1}^\top
            \right \rVert 
            L_\sigma .
    \end{align}
    By the triangle inequality, we obtain
    %
    \begin{equation*}
        \lVert b_{t} \rVert 
        %
        \leq
        %
        \lVert b_{t} - b_0\rVert + \lVert b_0 \rVert 
        %
        \leq
        %
        \lVert b_0 \rVert + \sum \limits_{s=1}^{t} \lVert \Delta b_s \rVert .
    \end{equation*}
    Substituting~\eqref{align:norm_bound_b} yields~\eqref{eqn:lemma3_1}.
    
    Finally, we show~\eqref{eqn:lemma3_2}. We have that \begin{align}
            |\Delta c_t| 
        &=  \notag
            \frac{2\alpha_t^c}{N}\left \lvert \sum \limits_{n=1}^N B_t^\top \sigma \bullet(W_t \mathbf{x}_n + b_t)+ c_{t-1} - y_n\right \rvert
        \\ &\leq \notag
            \frac{2\alpha_t^c}{N} \sum \limits_{n=1}^N \left \lvert B_t^\top \sigma \bullet(W_t \mathbf{x}_n + b_t)+ c_{t-1} - y_n\right \rvert
        \\ &\leq \notag
            \frac{2\alpha_t^c}{N} \sum \limits_{n=1}^N \left \lvert B_t^\top \sigma \bullet(W_t \mathbf{x}_n + b_t) \right \rvert + \left \lvert c_{t-1}\right \rvert + \left \lvert y_n\right \rvert
        \\ &\leq \label{align:norm_bound_c}
            \frac{2\alpha_t^c}{N} \sum \limits_{n=1}^N \left \lVert B_t^\top \right \rVert \left \lVert \sigma \bullet(W_t \mathbf{x}_n + b_t) \right \rVert + \left \lvert c_{t-1}\right \rvert + \left \lvert y_n\right \rvert .
    \end{align}
    By the triangle inequality, we obtain
    %
    \begin{equation*}
        \lvert c_{t} \rvert 
        %
        \leq
        %
        \lvert c_{t} - c_0\rvert + \lvert c_0 \rvert 
        %
        \leq
        %
        \lvert c_0 \rvert + \sum \limits_{s=1}^{t} \lvert \Delta c_s \rvert .
    \end{equation*}
    Substituting~\eqref{align:norm_bound_c} yields~\eqref{eqn:lemma3_2}. This concludes the proof. 
\end{proof}

\begin{lemma}[Control on the Growth Rate of the Norm for GD-Trained Weight Matrices]
\label{lem:lr}
In Setting~\ref{setting}, fix a differentiable $\sigma$ and a rate function $G$.
% \hfill\\
If for every $s\in \mathbb{N}_+$ we require that the variable learning rate $\boldsymbol{\alpha}$ satisfies~\eqref{eqn:alpha_1},~\eqref{eqn:alpha_2},~\eqref{eqn:alpha_3} and~\eqref{eqn:alpha_4},
% In Setting~\ref{setting}, let $f_{\Theta_t}$ be a neural network as defined in~\eqref{def:NNs} optimized by gradient descent~\eqref{eqn:grad_desc} to $t\geq 1$ iterations. For a nonnegative and continuous function $g:\mathbb{R}\to \mathbb{R}$ and a function $\displaystyle G(t) = \int_1^t g(u) \odif{u}$ satisfying $\lim \limits_{t\to \infty} G(t) < \infty$, if for every $s \geq 0$, 
then $\lVert B_t \rVert, 
      \lVert W_t \rVert_{op} ,
      \lVert b_t\rVert, 
      |c_t| 
      \lesssim G(t)$. 
\end{lemma}

% \Kyle{With the constants witnessing $\lVert B_t \rVert, 
%       \lVert W_t \rVert_{op} ,
%       \lVert b_t\rVert, 
%       |c_t| 
%       \lesssim G(t)$ given by the same constants satisfying~\eqref{eqn:alpha_1},~\eqref{eqn:alpha_2},~\eqref{eqn:alpha_3} and~\eqref{eqn:alpha_4}}

Having gained control over the growth rate of the norms of the GD-trained weight matrices defining our neural network, we may directly deduce an upper-bound for the Lipschitz constant of the function realized by those trained neural network parameters. Together, Lemma~\ref{lem:lr} and some results from random matrix theory yield our main theorem.\\

% Next, we desire to upper bound the Lipschitz constant of the learned function, and present a quick lemma. We can do this using the upper bound on the norm of the parameters and one more lemma which finds the Lipschitz constant of the neural network defined in~\eqref{def:NNs}. 




\begin{proof}[Proof of Lemma~\ref{lem:lr}]
    First, we bound $W$. 
    %Let $g$ and $G$ be given as in the hypotheses. 
    Set \begin{equation*}
        \gamma_s^W = \alpha_{s}^W \sum \limits_{n=1}^{N} \Big [ \left \lVert B_{s-1} \right \rVert  \left \lVert \sigma \bullet (W_{s-1} \mathbf{x}_n + b_{s-1}) \right \rVert + \left \lvert c_{s-1} \right \rvert + \left \lvert y_n \right \rvert \Big ] L \left \lVert B_{s-1} \right \rVert  \left \lVert \mathbf{x}_n \right \rVert .
    \end{equation*}
    %
    This allows us to restate~\eqref{eqn:alpha_1} as $\gamma_s^W \leq C_W g(s)$ for every $s\geq 0$. Then by Lemma~\ref{lem:bounds_W_B}, \[
        \lVert W_{t} \rVert_{op}
            %
            \leq
            %
            \lVert W_0 \rVert_{op} + \frac{2}{N} \sum \limits_{s=1}^{t} \gamma_s^W .
        \]
    Combining, we have
    \[
        \lVert W_t \rVert_{op}
        \leq
        \lVert W_0 \rVert_{op} + \frac{2C_W}{N} \sum \limits_{s=1}^t g(s)
        \leq
        \lVert W_0 \rVert_{op} + \frac{2C_W}{N} (G(t) + g(1)),
    \]
    as desired.
    
    
    Now, we bound $B$. 
    %Again, let $g$ and $G$ be given as above. 
    Set \begin{equation*}
        \gamma_s^B = \alpha_{s}^B\sum \limits_{n=1}^N \Big [ \left \lVert B_{s} \right \rVert  \left \lVert \sigma \bullet (W_{s+1}\mathbf{x}_n + b_{s+1}) \right \rVert + \left \lvert c_{s} \right \rvert + \left \lvert y_n \right \rvert \Big ] \left \lVert \sigma \bullet (W_{s+1} \mathbf{x}_n + b_{s+1}) \right \rVert .
    \end{equation*}
    %
    This allows us to restate~\eqref{eqn:alpha_2} as $\gamma_s^B \leq C_B g(s)$ for every $s\geq 0$. Then, by Lemma~\ref{lem:bounds_W_B}, \[
        \lVert B_t \rVert \leq \lVert B_0 \rVert + \frac{2}{N} \sum \limits_{s=1}^{t} \gamma_s^B .
    \]
    Combining, we have
    \[
        \lVert B_t \rVert \leq \lVert B_0 \rVert + \frac{2}{N}\sum \limits_{s=1}^t g(s) \leq
        \lVert B_0 \rVert + \frac{2C_B}{N} (G(t) + g(1)),
    \]
    as desired.

    Now, we bound $b$. 
    % Let $g$ and $G$ be given as in the hypotheses. 
    Set \[
        \gamma_s^b = \alpha_s^b \sum \limits_{n=1}^N \left [ \lVert B_{t-1} \rVert \lVert \sigma \bullet (W_t \mathbf{x}_n + b_{t-1}) \rVert + \lvert c_{t-1}\rvert + \lvert y_n \rvert \right ] \lVert B_{t-1} \rVert .
    \]
    Thus allows us to restate~\eqref{eqn:alpha_3} as $\gamma_s^b \leq C_b g(s)$ for every $s\geq 0$. Then, by Lemma~\ref{lem:bounds_W_B}, \[
        \lVert b_t \rVert \leq \lVert b_0 \rVert + \frac{2}{N} \sum \limits_{s=1}^t \gamma_s^b .
    \]
    Combining, we have \[
        \lVert b_t \rVert \leq \lVert b_0 \rVert + \frac{2}{N}\sum \limits_{s=1}^t g(s) \leq \lVert b_0 \rVert + \frac{2C_b}{N} (G(t)+g(1)),
    \] as desired.

    Finally, we bound $c$. 
    % Again, let $g$ and $G$ be given as above. 
    Set \[
        \gamma_s^c = \alpha_s^c \sum \limits_{n=1}^N \lVert B_t \rVert \lVert \sigma \bullet (W_t \mathbf{x}_n + b_t) \rVert + |c_{t-1}| + |y_n| .
    \]
    This allows us to restate~\eqref{eqn:alpha_4} as $\gamma_s^c \leq C_c g(s)$ for every $s\geq 0$. Then, by Lemma~\ref{lem:bounds_W_B}, \[
        |c_t| \leq |c_t| + \frac{2}{N} \sum \limits_{s=1}^t g(s) \leq |c_0| + \frac{2C_c}{N}(G(t)+g(1)) ,
    \]
    as desired. This concludes the proof.
\end{proof}







Having bounded the norms of the weight matrices, we can derive a bound on the maximum norms on the parameter space the gradient descent process, and thus ensure that each of our parameters remain in the $M$-cube in $\mathbb{R}^p$.

\begin{lemma}[Deterministic Bounds on the Parameter Space] \label{lem:deterministic_bounds_parameters}
    In Setting~\ref{setting}, let $\sigma$ be $L_\sigma$-Lipschitz activation function for $L_\sigma \geq 0$ and fix a rate function $G$. Then, there exists $M>0$ such that $\Theta_t \in [-M,M]^p$ for every $t\in \mathbb{N}_+$.
\end{lemma}



\begin{proof}[Proof of Lemma~\ref{lem:deterministic_bounds_parameters}]
    First, we note these inequalities between $L_p$ norms, namely that for $1\leq j \leq k\leq \infty$ and $\mathbf{v}\in \mathbb{R}^d$, the following holds: \[
        \lVert \mathbf{v} \rVert_j \leq d^{\frac{1}{j}-\frac{1}{k}} \lVert \mathbf{v}\rVert_k .
    \]
    Taking $j = 2$ and $k = \infty$, we have
    \[
        %\lVert \mathbf{v}\rVert_j \leq d^{1/j} \lVert \mathbf{v}\rVert_\infty \qquad 
        \lVert \mathbf{v}\rVert_2 \leq d^{1/2} \lVert \mathbf{v}\rVert_\infty .
    \]
    By Lemma~\ref{lem:lr}, we have that \[\begin{split}
       \lVert W_t\rVert_{op} &\leq \lVert W_0\rVert_{op} + \frac{2C_W}{N} (G(t)+g(1))
    \\
       \lVert B_t\rVert &\leq \lVert B_0\rVert + \frac{2C_B    }{N}(G(t)+g(1))
    \\
       \lVert b_t\rVert &\leq \lVert b_0\rVert + \frac{2C_b    }{N}(G(t)+g(1))
    \\
       |c_t| &\leq |c_0| + \frac{2C_c}{N} (G(t)+g(1)) .
    \end{split}\]
    Let $G^{\star}$ witness boundedness of $G$. Taking $t\to \infty$, and by relationship between $L_p$ and maximum norms, we have that for every $t\in \mathbb{N}_+$, \begin{equation}\label{eqn:max_bounds_g_lemma}\begin{split}
       \lVert W_t\rVert_{op} &\leq \lVert W_0\rVert_{op} + \frac{2C_W}{N} (G^{\star}+g(1)) \leq \sqrt{d} \, \lVert W_0\rVert_\infty + \frac{2C_W}{N} (G^{\star}+g(1))
    \\
       \lVert B_t\rVert &\leq \lVert B_0\rVert + \frac{2C_B}{N}(G^{\star}+g(1)) \leq \sqrt{d} \, \lVert B_0\rVert_\infty + \frac{2C_B    }{N}(G^{\star}+g(1))
    \\
       \lVert b_t\rVert &\leq \lVert b_0\rVert + \frac{2C_b}{N}(G^{\star}+g(1)) \leq \sqrt{d} \, \lVert b_0\rVert_\infty + \frac{2C_b    }{N}(G^{\star}+g(1))
    \\
       |c_t| &\leq |c_0| + \frac{2C_c}{N} (G^{\star}+g(1)) \leq \sqrt{d} \, \lvert c_0\rvert_\infty  + \frac{2C_c}{N} (G^{\star}+g(1)).
    \end{split}
\end{equation}
    Setting 
    % $M \eqdef \max \limits_{i \in \{W, B, b, c\} } \sqrt{d} \, \lVert i_0 \rVert_\infty + \frac{2C_i}{N} (G^{\star} + g(1))$
    $M$ equal to the maximum of the expressions on the right hand side of \eqref{eqn:max_bounds_g_lemma} concludes the proof.
\end{proof}







For completeness, we record the following simple lemma. 
It essentially states that the composition of Lipschitz functions is again Lipschitz.

\begin{lemma} \label{lem:lip}
    Let $f_\Theta (\mathbf{x}) = B^\top \sigma \bullet (W\mathbf{x} + b) + c$ be a neural network as defined in~\eqref{def:NNs}. Then, the Lipschitz constant of $f_\Theta$ satisfies \begin{equation}
        \operatorname{Lip} (f_\Theta) \leq L_\sigma \lVert B \rVert \lVert W \rVert_{op}.
    \end{equation}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:lip}]
    Without loss of generality, we assume $c = 0$. Thus, \begin{equation*}
        \operatorname{Lip}(f_\Theta(\mathbf{x})) 
        =
        \operatorname{Lip}(B^\top \sigma \bullet (W\mathbf{x} + b))
        \leq
        \lVert B \rVert \operatorname{Lip}(\sigma) \lVert W\rVert_{op}.
    \end{equation*}
    By assumption, $\sigma$ is $L_\sigma$-Lipschitz, which completes the proof. 
\end{proof}
%
\noindent We record the following $\omega$-wise version of Theorem~\ref{thrm:lr_lip}.
\begin{lemma} 
\label{lem:lr_lip__omegawise}
    In Setting~\ref{setting}, fix an $L_{\sigma}\geq 0$ Lipschitz and differentiable activation function $\sigma$ and a rate function $G$.
    %%
    If, for every $s\in \mathbb{N}_+$, we require that the variable learning rate $\boldsymbol{\alpha}$ satisfies the GD LR Decay Conditions~\ref{GradCons}, then
    \begin{equation}
        \begin{split}
            \operatorname{Lip}(f_{\Theta_t}) &\leq L_\sigma \left [ \lVert W_0 \rVert_{op} + \frac{2C_W}{N} (G(t) + g(1)) \right ] \left [ \lVert B_0 \rVert + \frac{2C_B}{N} (G(t) + g(1)) \right ]
.
        \end{split}
    \end{equation}
\end{lemma}
\begin{proof}[Proof of {Lemma~\ref{lem:lr_lip__omegawise}}]
    By Lemma~\ref{lem:lr}, we have that $\lVert B_t\rVert , \lVert W_t \rVert_{op} \lesssim G(t)$. Using the Lipschitz constant from Lemma~\ref{lem:lip} completes the proof.
\end{proof}




% \section{Proofs of Theorems and Corollaries}
% \label{s:Proofs_of_Corollaries}



\subsection{Proof of Theorem~\ref{thrm:lr_lip}}
\label{s:Proofs_of_lr_lip}



\begin{proof}[Proof of Theorem~\ref{thrm:lr_lip}]
% \paragraph{Case 1 - Sub-Gaussian Initialization}
By our assumption on the weight matrices $W_0$ and $B_0$,
% in Case~1
we may apply the version of Gordon's Majorization Theorem~\citep[Theorem 4.6.1]{VershyyninBook} and a union bound to deduce that there exists a constant $\kappa>0$ depending only on the law of the first row of $B_0$ and of $W_0$ such that for every $\eta>0$,
\begin{align}
&
\label{eq:RM_Concentration___BEGIN}
        \mathbb{P}\big(
                \max\{
                    s_{\operatorname{max}}(W_0)
                ,
                    s_{\operatorname{max}}(B_0)
                \}
            \le
                \sqrt{p}
                +
                \kappa(\sqrt{\max\{p,d\}} +\eta)
        \big)
\\
\nonumber
%%% Union Bound
    & \geq 
        1-
        \mathbb{P}\big(
                    s_{\operatorname{max}}(W_0)
            \le
                \sqrt{p}
                +
                \kappa(\sqrt{\max\{p,d\}} +\eta)
        \big)
\\
\nonumber
& \qquad
        -
            \mathbb{P}\big(
                    s_{\operatorname{max}}(B_0)
            \le
                \sqrt{p}
                +
                \kappa(\sqrt{\max\{p,d\}} +\eta)
        \big)
\\
\nonumber
%%% Inclusion/Containement of Events
    & \geq 
        1-
        \mathbb{P}\big(
                    s_{\operatorname{max}}(W_0)
            \le
                \sqrt{p}
                +
                \kappa(\sqrt{d} +\eta)
        \big)
%     \\
% \nonumber
        % & 
        -
            \mathbb{P}\big(
                    s_{\operatorname{max}}(B_0)
            \le
                \sqrt{1}
                +
                \kappa(\sqrt{p} +\eta)
        \big)
%%%
\\
\label{eq:Gordon}
%%% Inclusion/Containement of Events
    & \geq 
            1
        -
            2e^{-\eta^2}
%     \\
% \nonumber
        % & 
        -
            2e^{-\eta^2}
\\
\label{eq:RM_Concentration___END}
%% Cleanup
    & \geq 
        1-4e^{-\eta^2}
\end{align}
where $s_{\operatorname{max}}(Z)$ denotes the maximal singular value of a matrix $Z$, which we recall coincides with its operator norm, and we used Gordon's Majorization Theorem to obtain~\eqref{eq:Gordon}.
%%
Since our assumptions place us squarely within the scope of Lemma~\ref{lem:lr_lip__omegawise}, then combining the estimate in~\eqref{eq:RM_Concentration___BEGIN}-\eqref{eq:RM_Concentration___END} with the estimate in Lemma~\ref{lem:lr_lip__omegawise} we deduce that for every $\eta>0$, the following holds with probability at least $1-4e^{-t^2}$:
\begin{align}
        \operatorname{Lip}(f_{\Theta_t}) 
&
\leq 
        L_\sigma 
        \Big [ \lVert W_0 \rVert_{op} + \frac{2C_W}{N} (G(t) + g(1)) \Big ] 
        \,
        \Big [ \lVert B_0 \rVert + \frac{2C_B}{N} (G(t) + g(1)) \Big ]
\\
&
\label{eq:bound_Frob_op}
\leq 
        L_\sigma 
        \Big [ \lVert W_0 \rVert_{op} + \frac{2C_W}{N} (G(t) + g(1)) \Big ] 
        \,
        \Big [ \sqrt{1}\lVert B_0 \rVert_{op} + \frac{2C_B}{N} (G(t) + g(1)) \Big ]
\\
&
\nonumber
\leq 
        L_\sigma 
        \Big [ 
            \big(
                \sqrt{p}
                +
                \kappa(\sqrt{d} +\eta)
            \big)
            + \frac{2C_W}{N} (G(t) + g(1)) \Big ] 
\\
\nonumber
\,\,
& \qquad
\times
        \Big [ 
            \big(
                    \sqrt{1}
                +
                    \kappa(\sqrt{p} +\eta)
            \big)
        + \frac{2C_B}{N} (G(t) + g(1)) \Big ]
\\
&
\nonumber
\le
    L_\sigma 
    \Big [ 
        \big(
            \sqrt{p}
            +
            \kappa(\sqrt{\max\{p,d\}} +\eta)
        \big)
        + \frac{2\max\{C_W,C_B\}}{N} (G(t) + g(1)) 
    \Big ]^2
\end{align}
where we used the bound $\|Z\|_{\text{Frobenius}}\leq \sqrt{r}\|Z\|_{op}$ for a generic $n\times n$ matrix $Z$ of rank $r\in \mathbb{N}_+$ to deduce~\eqref{eq:bound_Frob_op}.
\end{proof}






\subsection{Proof of Corollary~\ref{cor:GeneralizationImplication}}
\label{s:Proofs_of_Corollaries__Generalization}
In what follows, we use $\operatorname{Lip}(\mathbb{R}^d,\mathbb{R}^D;L)$ to denote the space of $L$-Lipschitz functions from $\mathbb{R}^d$ to $\mathbb{R}^D$. We also use $\mathcal{W}_1$ to denote the $1$-Wasserstein distance on $\mathbb{R}^{d+D}$: recall that for every pair of Borel probability measures $\mu, \nu$ on $\mathbb{R}^d$ with finite mean, we define
\begin{equation}\label{remark:wasserstein}
    \mathcal{W}_1 ( \mu, \nu) \eqdef \inf \limits_{\pi \in \Pi(\mu, \nu)} \Bigg (\int_{\mathbb{R}^{d+D}\times \mathbb{R}^{d+D}} d(x,y) \, \mathrm{d} \pi(x,y)\Bigg )
\end{equation}
where $\Pi(\mu, \nu)$ consists of all joint probability measures on $\mathbb{R}^{2d}$ with marginals $\mu$ and $\nu$.\\

\begin{proof}[{Proof of Corollary~\ref{cor:GeneralizationImplication}}]
Fix $t\in \mathbb{N}_+\cup \{\infty\}$ and define the map $\Lambda:\mathbb{N}_+\cup \{\infty\}\to [0,\infty)$
\begin{equation}
\Lambda(t,\eta)\eqdef 
    L_\sigma 
    %
    \Big( 
        \big(
            \sqrt{p}
            +
            \kappa(\sqrt{\max\{p,d\}} +\eta)
        \big)
        + \frac{2\max\{C_W,C_B\}
            (G(t) + g(1))
        }{N}
    \Big)^2
.
\end{equation}
Let $\eta>0$. Let $\mathbb{Q}^N\eqdef \frac1{N}\,\sum_{n=1}^N\, \delta_{(X_n,Y_n)}$ denote the empirical measure associated to the (random) dataset $\mathcal{S}=\{(X_n,Y_n)\}_{n=1}^N$.
By Theorem~\ref{thrm:lr_lip}, $f_{\Theta_t}\in \operatorname{Lip}(\mathbb{R}^d,\mathbb{R}^D;\Lambda(t,\eta))$ with probability at least $1-4e^{-\eta^2}$. 
Therefore, the following holds with probability at least $1-4e^{-\eta^2}$:
\allowdisplaybreaks
\begin{align}
\label{eq:theorem_applied}
    \big|
        \mathcal{R}(f_{\Theta_t})
        -
        \hat{\mathcal{R}}^{\mathcal{S}}(f_{\Theta_t})
    \big|
& =
    \big|
        \mathbb{E}_{(X,Y)\sim \mathbb{Q}}\big[
            \|f_{\Theta_t}(X)-Y\|
        \big]
        -
        \mathbb{E}_{(X,Y)\sim \mathbb{Q}^N}\big[
            \|f_{\Theta_t}(X)-Y\|
        \big]
    \big|
\\
\nonumber
&
\leq 
    \sup_{f\in \operatorname{Lip}(\mathbb{R}^d,\mathbb{R}^D;\Lambda(t,\eta))}
    \,
    \big|
        \mathbb{E}_{(X,Y)\sim \mathbb{Q}}\big[
            \|f(X)-Y\|
        \big]
        -
        \mathbb{E}_{(X,Y)\sim \mathbb{Q}^N}\big[
            \|f(X)-Y\|
        \big]
    \big|
\\
\label{eq:KR_Duality}
&
\leq 
    \Lambda(t,\eta)
    \,
    \mathcal{W}_1\big(\mathbb{Q},\mathbb{Q}^N)
\end{align}
where~\eqref{eq:KR_Duality} held by the Kantorovich-Rubinstein duality, e.g.~\citep[see][Theorem 11.8.2]{DudleyRealProb_2002Book}. 
Since we have assumed that $\mathcal{S}$ consists of $N$ i.i.d.\ samples drawn from $\mathbb{Q}$, then upon applying the concentration inequality for $\mathcal{W}_1(\mathbb{Q},\mathbb{Q}^N)$ of~\cite{kloeckner2020empirical} in the version formulated by~\citet[Lemma B.5]{hou2023instance} we deduce that for every $\varepsilon>0$, we have that
\begin{equation}
\label{eq:concentration_inequality}
        \mathcal{W}_1\big(\mathbb{Q},\mathbb{Q}^N) 
    \leq 
        \frac{
            \operatorname{diam}(\operatorname{supp}(\mathbb{Q}))
            \,
            C_{d+D}
        }{\sqrt[d]{N}}
        +
        \varepsilon
\end{equation}
hold with probability at least $1-2e^{-2N\varepsilon^2/\operatorname{diam}(\operatorname{supp}(\mathbb{Q}))^2}$, where $\operatorname{supp}(\mathbb{Q})$ denotes the (compact) support of $\mathbb{Q}$ and the dimensional constant $C_{d+D}>0$ is given by
\begin{equation}
\label{eq:dimensional_constant}
    C_{d+D}
\eqdef 
    2\Biggl(\frac{\frac{d+D}{2} - 1}{2  (1-2^{1-(d+D)/2})}\Biggr)^{2/d+D}\Biggl(1 + \frac{1}{2 \big(\frac{(d+D)}{2} - 1\big)}\Biggr)\,(d+D)^{1/2}
    \in 
    \mathcal{O}((d+D)^{1/2})
.
\end{equation}
Taking a union bound allows us to combine the concentration inequality in~\eqref{eq:concentration_inequality} with the bound in~\eqref{eq:theorem_applied}-\eqref{eq:KR_Duality} to deduce that for every $\eta,\varepsilon>0$, with probability at least:
\begin{equation}
\label{eq:raw_eta_epsilon}
    1
    -
    4
    e^{-\eta^2}
    -
    2
    e^{-2N\varepsilon^2/\operatorname{diam}(\operatorname{supp}(\mathbb{Q}))^2}
\end{equation}
we have that:
\[
    \big|
        \mathcal{R}(f_{\Theta_t})
        -
        \hat{\mathcal{R}}^{\mathcal{S}}(f_{\Theta_t})
    \big|
\leq 
    \Lambda(t,\eta)
    \,
    \biggl(
        \frac{
            \operatorname{diam}(\operatorname{supp}(\mathbb{Q}))
            \,
            C_{d+D}
        }{\sqrt[d]{N}}
        +
        \varepsilon
    \biggr)
.
\]
Since $\eta>0$ was a free parameter, we retroactively set it to be $\eta \eqdef \frac{\operatorname{diam}(\operatorname{supp}(\mathbb{Q}))}{\sqrt{2N}}$. Then, the probability in~\eqref{eq:raw_eta_epsilon} can be bounded-below by
\begin{equation}
\label{eq:raw_eta_epsilon__Bound_below}
    1
    -
    8
    e^{-2N\varepsilon^2/\operatorname{diam}(\operatorname{supp}(\mathbb{Q}))^2}
.
\end{equation}
Let $\delta>0$ be given. Similarly, we retroactively set $\varepsilon\eqdef 
\frac{
\operatorname{diam}(\operatorname{supp}(\mathbb{Q})) \sqrt{\ln(8/\delta)}
}{\sqrt{2N}}
$ yielding the claim. 
\end{proof}

\subsection{Proof of Theorem~\ref{thrm:Convergence_and_stability}}
\label{s:Proofs__ss:thrm:Convergence_and_stability}



\begin{proof}[Proof of Theorem~\ref{thrm:Convergence_and_stability}]
    First, observe that the requirement~\eqref{eqn:alpha_min} implies that~GD LR Decay Conditions~\ref{GradCons} holds. 
    Since we are in Setting~\ref{setting}, then Lemma~\ref{lem:deterministic_bounds_parameters} implies that for every $\eta>0$, there exists some $M\eqdef M(\eta)>0$ such that with probability at least $1-4e^{\eta^2}$ for every $t \in \mathbb{N}$, we have $\Theta_t \in [-M, M]^p$.
    
    \noindent Observe also that the finite second-moment condition implies that
    \[
        \mathbb{E}[\lVert(\mathbf{x}_1,\ldots, \mathbf{x}_N)\rVert_{\operatorname{Frobenius}}^2]^2
        \le \sum_{n=1}^N\, 
        \mathbb{E}[\|\mathbf{x}_n\|^2]^2
        <\infty
    ,
    \]
    hence the expected Frobenius norm of our data $\mathbb{E}[\lVert(\mathbf{x}_1,\ldots, \mathbf{x}_N)\rVert_{\operatorname{Frobenius}}^2]$ is finite. 
    Lastly, since we additionally assume that $\sigma, \sigma', \sigma''$ are bounded and that $\ell$ has a uniformly bounded derivative (i.e.\ $g_{\max}^{\prime}\eqdef \sup_{x,y}\in {\mathbb{R}^{d+1}}\, \|\nabla \ell(x,y)\|<\infty$) then all the conditions for~\citep[Corollary 3.2 and Theorem 3.3]{Herrera2020LocalLB} are met, thus the loss function $\ell$ and its gradient $\nabla \ell$ are Lipschitz with the following constants
    \begin{align}
    \label{eq:Lbound}
        0& \le L_{\ell}
        \leq
            L_{\ell}^{\operatorname{UB}}
        \eqdef 
            g_{\max}^{\prime}
            (M_{\ell}^2 M)^{
            % 2(u-1)
            2
            } (\sigma'_{\max})^{
            4%2 u
            } (S^2+1)
            + 
             % \sum_{k=1}^{u-1} (M_{\ell}^2 M)^{2(k-1)} 
             2(M_{\ell}^2 M)^{2} 
             (\sigma'_{\max})^{
             4%2 k
             } (M_{\ell} \sigma_{\max}^2 + 1)
        <
        \infty
    \\
    \label{eq:Lnalblabound}
        0& \le L_{\nabla \ell} 
        \le 
        \mathcal{O}\Big(  
            % 2%
            (\sigma_{\max}'')^2 \sigma_{\max}^4 2^{
            4% (u-1)
            } M_{\ell}^{(
            % 10u-9
            % 20-9
            11
            )} 
            \left( \sigma_{\max}' M \right)^{
            % 4(u-1)
            % 4(2-1)
            6
            } (S^4+1) 
        \Big)
        .
	\end{align}
    where $M_{\ell}\eqdef 2$ (the width of the MLP's hidden layers), as $\ell$ is bounded below by $0$, continuously differentiable and $\nabla \ell$ is $L_{\nabla \ell}$-Lipschitz. 

    To apply the results of~\citet[Section 1.2.3]{Nesterov}, it remains to verify that our learning rate is constant for the first $T$ iterations with step size $1/\operatorname{Lip}(\mathcal{R}_{\mathcal{S}})$.%$\frac1{\operatorname{Lip}(\mathcal{R}_{\mathcal{S}})}$.
    We must pick $C_W$ and $C_B$ in~GD LR Decay Conditions~\ref{GradCons} large enough (which can be done since $T$ is finite) such that for $t=1,\dots,T$ we have
    \[
                \frac1{\operatorname{Lip}(\mathcal{R}_{\mathcal{S}})}
            \le 
                \min\{
                    \bar{\alpha}_t^{W,g }
                ,
                \bar{\alpha}_t^{B,g}
                \}
    .
    \]
    Therefore, for $t=1,\dots,T$, the learning rate constraints in~\eqref{eqn:alpha_min} simplify to
    \begin{equation} \label{eqn:alpha_min__ok}
        \begin{split}
            \alpha_t^W &\leq \min \Big \{
                    1 / \operatorname{Lip}(\mathcal{R}_{\mathcal{S}})
                    \, , \,
                    \bar{\alpha}_t^{W,g}
                \Big \}
                =
                \frac1{\operatorname{Lip}(\mathcal{R}_{\mathcal{S}})}
            \\
            \alpha_t^B &\leq \min \Big \{
                    % {\color{blue}2^{-C(n,w) (p+\log M)}}
                    1 / \operatorname{Lip}(\mathcal{R}_{\mathcal{S}})
                    \, , \,
                    \bar{\alpha}_t^{B,g}
                \Big \}
                =
                \frac1{\operatorname{Lip}(\mathcal{R}_{\mathcal{S}})}
.
        \end{split}
    \end{equation} 
    Thus, for $t=1,\dots,T$ our GD has (finite) constant step size $1/\operatorname{Lip}(\mathcal{R}_{\mathcal{S}})$. Consequently, we apply the convergence result for GD with backtracking line search by~\citet[Section 1.2.3]{Nesterov}, and we have that for every $\Theta_0 \in [-M,M]^P$ and every $T\in \mathbb{N}$, for some constant $\omega>0$, we have
    \begin{align}
    \label{eq:UB_conv__setup}
        \min \limits_{0\leq t\leq T} \lVert \nabla_{\Theta} \ell (\Theta_t )\rVert &\leq \frac{1}{\sqrt{T+1}} \left ( \frac{L_{\nabla \ell}}{\omega} \ell (\Theta_0) - \inf \limits_{\Theta \in \mathbb{R}^{P}} \ell (\Theta) \right ) 
        \\
        \nonumber
        &\leq \frac{1}{\sqrt{T+1}} \left ( \frac{L_{\nabla \ell}}{\omega} \ell (\Theta_0) \right ) 
        \\
    \label{eq:UB_conv}
        &\leq \frac{L_{\nabla \ell}}{\omega \sqrt{T+1}} \sup \limits_{\Theta \in [-M,M]^P} \frac{1}{N} \sum \limits_{n=1}^N \ell (f_{\Theta}(x) , y)^2.
    \end{align}
    Now, the continuity of $\ell$, and $f_{\Theta}$ in $(\Theta,x)$ together with the compactness of $[-M,M]^P$ imply that $C\eqdef \displaystyle \sup \limits_{\Theta \in [-M,M]^P} \frac{1}{N} \sum \limits_{n=1}^N \ell (f_{\Theta}(x) , y)^2$ is finite. This, together with~\eqref{eq:UB_conv__setup}-\eqref{eq:UB_conv} imply that
    \[
        \min \limits_{0\leq t\leq T} \lVert \nabla_{\Theta} \ell (\Theta_t )\rVert \lesssim \frac{L_{\nabla \ell}}{\sqrt{T+1}}
        \in
            \mathcal{O}\Big(  
            \frac{
                % 2%
                (\sigma_{\max}'')^2 \sigma_{\max}^4 2^{
                4% (u-1)
                } M_{\ell}^{(
                % 10u-9
                % 20-9
                11
                )} 
                \left( \sigma_{\max}' M \right)^{
                % 4(u-1)
                % 4(2-1)
                6
                } (S^4+1) 
            }{
                \sqrt{T+1}
            }
        \Big)
    .
    \]
    Thus, we GD with backtracking line search and our learning rate prescription in~\eqref{eqn:alpha_min} converges to a critical point at rate $\mathcal{O}(1/{\sqrt{T}})$.
\end{proof}




% \input{REFERENCED_THEOREMS}


% \input{LitReview}


\section{Experiment Details}
\label{s:ExpDet}

We trained our model on a laptop with the following specifications. CPU: 12th Gen Intel(R) Core(TM) i7-1260P 2.10 GHz with 16.0 GB of RAM (15.7 GB usable).  
All neural networks use the $\operatorname{Swish}$ activation function given by \[
    \sigma(x) = \operatorname{Swish}(x) \eqdef \dfrac{x}{1 + e^{-x}}.
\]
\citet[Lemma 10]{limmer2024reality} obtain a general form for the $n^{\text{th}}$ derivative of $\operatorname{Swish}$, which can be shown to be bounded with bounded first and second derivatives on $[-M,M]$.

\subsection{Additional Experiments}
We include some additional variants of our toy numerical illustrations, in Section~\ref{s:Discussion__ss:Validation}, to further ablate the effect of a decaying LR on regularity and predictive power of the GD-trained two-layer neural network. 
\paragraph{Effect of Noise}
We repeat the experiment with noise level $\beta = 1$ to see if the decaying learning rate adversely impacts the learned two-layer neural networks' ability to predict.

\begin{figure}[H]%
\centering
% \includegraphics[width=0.65\linewidth]{img_rand_init/0_Effect_of_noise/Lip_Constants_Exp_Rate_T200_runs20_noise0.03.eps}
\includegraphics[width=0.65\linewidth]{experiment_noise/Lip_Constants_Poly_Rate_T200_runs20_P150_N100.png}
\caption{\textbf{Effect of Noise ($\beta$):} Here, $N=100$, $P=100$ and $\alpha_t = 0.01$ for $t=1,\ldots, 100$. We aggregate the results of 20 GD sequences and plot their means and standard deviations.
    \label{fig:Lip_vs_P}} 
\end{figure}
As shown in Figure~\ref{fig:Lip_vs_P}, the noise level $\beta$ has little to no predictable effect on the Lipschitz constant of the trained two-layer neural network.

% \paragraph{Effect of Hidden Layer Size} We observe that {\color{orange}TO DO}. 

% \begin{figure}[H]%
% \centering
% % \includegraphics[width=0.65\linewidth]{img_rand_init/0_Effect_of_noise/Lip_Constants_Exp_Rate_T200_runs20_noise0.03.eps}
% \includegraphics[width=0.65\linewidth]{hidden_experiment/Lip_Constants_Poly_Rate_T200_runs20_noise0.03.png}
% \caption{\textbf{Effect of Hidden Layer Size ($p$):} Here, $N=100$, $\beta = 0.03$, and $\alpha_t = 0.01$ for $t=1,\ldots, 100$. We aggregate the results of 20 GD sequences and plot their means and standard deviations.
%     \label{fig:Lip_vs_P}} 
% \end{figure}



\end{document}
