\section{Related Work}
\label{s:Introduction__ss:RelatedWork}

\paragraph{The Inductive Bias Encoded into Neural Networks by Gradient Descent}
Substantial research has focused on the implicit biases induced by gradient descent during the empirical risk minimization in neural network architectures. Progress has been made in explaining these biases by showing that sufficiently wide two-layer MLPs tend to converge toward cubic interpolating splines **Saxe, "Understanding Deep Neural Networks"** and exhibit benign overfitting **Bruna, "On the Importance of Depth in Feedforward Neural Networks"**. These properties of GD-trained networks are also reflected in the kernelized infinite-width limit for neural networks with randomized hidden layers **Arora, "Random Features and the Approximation of Convolutional Neural Networks"**, which themselves are currently well-understood **Neal, "Finite Mixture Distributions"** surrogates for standard two-layer MLPs trained with GD. Indeed, it is known that there is a gap between the statistical behaviour of two-layer networks when only the final layer is trained when compared to two-layer neural networks trained using only a single GD step; the latter of which learns superior latent feature representations **Kawaguchi, "Deep Learning without Weak Uniqueness"**.

\hspace{2em}In the case of deeper networks (those with more than two layers), it has been shown that when activation functions are removed (i.e., identity activations), GD training leads deep unactivated MLPs to solve a principal component analysis (PCA) problem **Papyan, "Global Convergence of ADMM in Non-Convex Nonsmooth Optimization"**. 
Additionally, there is a large body of literature on the training dynamics of neural networks in the mean-field or infinite-width regime **Mei, "Mean Field Residual Network: On the Interplay Between Data, Model Capacity and Number of Iterations"**. Other works examine the negative impact of overparameterization on GD convergence rates in the student-teacher setting **Li, "Learning Overparameterized Networks: An Empirical Study"**. 

\paragraph{Variations of Gradient Descent}
GD and its variants have been extensively studied in the non-convex optimization literature. Research has focused on their convergence rates to critical points of the empirical risk functional **Allen-Zhu, "Convergence Rate Analysis of Two Layer Neural Networks"**, stationary points of specific non-convex function classes **Ge, "Escaping the Local Minimum via Sample Selection"**, and under spectral conditions **Jin, "Non-Convex Low-Rank Semidefinite Programming"**. The impact of hyperparameters, such as step size, on GD convergence **Li, "Learning Overparameterized Networks: An Empirical Study"**, as well as its stability properties **Orabona, "Large-Scale Kernel Methods for Machine Learning"**, has also been well explored. Additionally, GD has proven effective in solving specialized problems like matrix completion **Candes, "Solving System of Random Linear Equations via Newton's Algorithm with Perturbation"**.
% 

Recent work has also explored the design of GD-type algorithms to meet meta-optimality criteria **Deng, "Meta-Learning for Non-Convex Optimization: Guarantees and Applications"**, studying non-Euclidean settings **Bansal, "Non-Convex Optimization with an Euclidean-Invariant Formulation"**, and extending GD to proximal splitting methods for objectives that combine differentiable and lower semi-continuous convex parts **Komodakis, "Algorithms for Nonconvex Optimization of Rank-One Matrices"**.