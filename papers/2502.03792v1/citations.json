[
  {
    "index": 0,
    "papers": [
      {
        "key": "jin2023implicit",
        "author": "Jin, Hui and Mont{\\'u}far, Guido",
        "title": "Implicit bias of gradient descent for mean squared error regression with two-layer wide neural networks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "pmlr-v178-shamir22a",
        "author": "Shamir, Ohad",
        "title": "The Implicit Bias of Benign Overfitting"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "heiss2023implicit",
        "author": "Heiss, Jakob and Teichmann, Josef and Wutte, Hanna",
        "title": "How (Implicit) Regularization of ReLU Neural Networks Characterizes the Learned Function--Part II: the Multi-D Case of Two Layers with Random First Layer"
      },
      {
        "key": "mei2022generalization",
        "author": "Mei, Song and Montanari, Andrea",
        "title": "The generalization error of random features regression: Precise asymptotics and the double descent curve"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "gonon2020risk",
        "author": "Gonon, Lukas and Grigoryeva, Lyudmila and Ortega, Juan-Pablo",
        "title": "Risk bounds for reservoir computing"
      },
      {
        "key": "gonon2023approximation",
        "author": "Gonon, Lukas and Grigoryeva, Lyudmila and Ortega, Juan-Pablo",
        "title": "Approximation bounds for random neural networks and reservoir systems"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ba2022high",
        "author": "Ba, Jimmy and Erdogdu, Murat A and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg",
        "title": "High-dimensional asymptotics of feature learning: How one gradient step improves the representation"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "arora2018a",
        "author": "Sanjeev Arora and Nadav Cohen and Noah Golowich and Wei Hu",
        "title": "A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "pmlr-v195-abbe23a",
        "author": "Abbe, Emmanuel and Adser{\\`a}, Enric Boix and Misiakiewicz, Theodor",
        "title": "{SGD} learning on neural networks: leap complexity and saddle-to-saddle dynamics"
      },
      {
        "key": "pmlr-v119-golikov20a",
        "author": "Golikov, Eugene",
        "title": "Towards a General Theory of Infinite-Width Limits of Neural Classifiers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "xu2023over",
        "author": "Xu, Weihang and Du, Simon",
        "title": "Over-Parameterization Exponentially Slows Down Gradient Descent for Learning a Single Neuron"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "pmlr-v247-patel24a",
        "author": "Patel, Kumar Kshitij and Glasgow, Margalit and Zindari, Ali and Wang, Lingxiao and Stich, Sebastian U and Cheng, Ziheng and Joshi, Nirmit and Srebro, Nathan",
        "title": "The Limits and Potentials of Local {SGD} for Distributed Heterogeneous Learning with Intermittent Communication"
      },
      {
        "key": "pmlr-v247-zeng24a",
        "author": "Zeng, Sihan and Doan, Thinh",
        "title": "Fast two-time-scale stochastic gradient method with applications in reinforcement learning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "faw2023beyond",
        "author": "Faw, Matthew and Rout, Litu and Caramanis, Constantine and Shakkottai, Sanjay",
        "title": "Beyond uniform smoothness: A stopped analysis of adaptive {SGD}"
      },
      {
        "key": "anonymous2025methods",
        "author": "Gorbunov, E. and Tupitsa, N. and Choudhury, S. and Aliev, A. and Richt\u00e1rik, P. and Horv\u00e1th, S. and Tak\u00e1\u010d, M.",
        "title": "Methods for Convex $(L_0,L_1)$-Smooth Optimization: Clipping, Acceleration, and Adaptivity"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "velikanov2024tight",
        "author": "Velikanov, Maksim and Yarotsky, Dmitry",
        "title": "Tight convergence rate bounds for optimization under power law spectral conditions"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "pmlr-v247-wu24b",
        "author": "Wu, Jingfeng and Bartlett, Peter L. and Telgarsky, Matus and Yu, Bin",
        "title": "Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "mulayoff2024exact",
        "author": "Mulayoff, Rotem and Michaeli, Tomer",
        "title": "Exact Mean Square Linear Stability Analysis for {SGD}"
      },
      {
        "key": "zhu2024uniform",
        "author": "Zhu, Lingjiong and Gurbuzbalaban, Mert and Raj, Anant and Simsekli, Umut",
        "title": "Uniform-in-time {W}asserstein stability bounds for (noisy) stochastic gradient descent"
      },
      {
        "key": "zhu2024stability",
        "author": "Zhu, Miaoxi and Shen, Li and Du, Bo and Tao, Dacheng",
        "title": "Stability and generalization of the decentralized stochastic gradient descent ascent algorithm"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "baes2021low",
        "author": "Baes, Michel and Herrera, Calypso and Neufeld, Ariel and Ruyssen, Pierre",
        "title": "Low-rank plus sparse decomposition of covariance matrices using neural network parametrization"
      },
      {
        "key": "pmlr-v247-ma24a",
        "author": "Ma, Jianhao and Fattahi, Salar",
        "title": "Convergence of Gradient Descent with Small Initialization for Unregularized Matrix Completion"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "casgrain2019latent",
        "author": "Casgrain, Philippe",
        "title": "A latent variational framework for stochastic optimization"
      },
      {
        "key": "pmlr-v134-casgrain21a",
        "author": "Casgrain, Philippe and Kratsios, Anastasis",
        "title": "Optimizing Optimizers: Regret-optimal gradient descent algorithms"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "alimisis2021momentum",
        "author": "Alimisis, Foivos and Orvieto, Antonio and Becigneul, Gary and Lucchi, Aurelien",
        "title": "Momentum improves optimization on Riemannian manifolds"
      },
      {
        "key": "hsieh2024riemannian",
        "author": "Hsieh, Ya-Ping and Karimi Jaghargh, Mohammad Reza and Krause, Andreas and Mertikopoulos, Panayotis",
        "title": "Riemannian stochastic optimization methods avoid strict saddle points"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "patrascu2021stochastic",
        "author": "Patrascu, Andrei and Irofti, Paul",
        "title": "Stochastic proximal splitting algorithm for composite minimization"
      },
      {
        "key": "minh2022strong",
        "author": "Minh Tuyen, Truong and Promkam, Ratthaprom and Sunthrayuth, Pongsakorn",
        "title": "Strong convergence of a generalized forward--backward splitting method in reflexive Banach spaces"
      }
    ]
  }
]