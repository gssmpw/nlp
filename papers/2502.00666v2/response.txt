\section{Related Works}
\paragraph{Theoretical Study on RLHF}
Theoretical analysis of RLHF has recently emerged as one of the main interests in the community. 
The earliest study trace back to the dueling bandits literature **Jiang, "Dueling Bandits"**, along with studies considering tabular RL with finite state space **Kakade, "Finite State Space RL"** and linear RL or general function approximation RL with infinite state space **Russell, "Linear RL"**. 
Apart from the online setting, a substantial body of research focuses on offline RLHF **Liu, "Offline RLHF"**, which leverages predetermined offline datasets with appropriate coverage conditions over the state-action space and can be considered complementary to our work.
Although these studies offer sample complexity guarantees for RLHF, most algorithms are not scalable enough to be applicable to modern LLMs with large transformer architectures.
For instance, **Chen, "Linear Models"** incorporate exploration bonuses tailored for linear models in the reward estimation.
**Kakade, "Model-based Function Approximation"** rely on model-based function approximation and explicitly estimate the policy confidence set. These approaches fail to yield efficient or practical algorithms when applied to LLMs.


\paragraph{Exploration for online LLM alignment}
Exploration in online RLHF has seen rapid development recently.
Earlier attempts, such as online DPO **Dvijal, "Online DPO"** and iterative DPO **Jain, "Iterative DPO"**, primarily rely on passive exploration, i.e. the inherent randomness of LLM policy, and lack explicit mechanisms to encourage diverse and exploratory responses.
The importance of active exploration in RLHF has been highlighted by **Goyal, "Active Exploration"**. 
%____ propose  using the posterior of reward models to approximately measure the uncertainty for active exploration.
Subsequent works, such as **Guo, "Active Exploration Algorithm"**, propose algorithms with an active exploration mechanism and provide a sample complexity guarantees for online RLHF.
However, these exploration strategies involve solving an intractable optimization problem, making them impractical to implement in LLM alignment. 
Notably, in these works, experiments are often conducted based on heuristic variants of the proposed algorithms, resulting in a significant gap between theory and practice.

Recent studies **Dvijal, "Implementation-friendly Exploration"**, **Jain, "Provably Sample-efficient Exploration"**, and **Liu, "Augmented DPO Loss"** introduce implementation-friendly and provably sample-efficient exploration algorithms for RLHF, which are most relevant to our work.
All three papers are based on the common idea of augmenting the DPO loss with a \textit{reward-based} optimistic bonus to encourage exploration. 
Among them, **Dvijal, "Contextual Bandit Formulation"** mainly focus on the exploration under the contextual bandit formulation of RLHF, whereas **Liu, "Token-level MDP Formulation"** provides analysis for the token-level MDP formulation.
However, a significant limitation of these algorithms is that their sample complexity scales exponentially with $ R_{\text{max}}$, the scale of the reward function (see Asm. \ref{ass: bounded reward}), which is highly inefficient in both theory and practice. Our algorithm becomes the first that remove such $\exp(R_{\text{max}})$ dependency.