@article{bengs2021preference,
  title={Preference-based online learning with dueling bandits: A survey},
  author={Bengs, Viktor and Busa-Fekete, R{\'o}bert and El Mesaoudi-Paul, Adil and H{\"u}llermeier, Eyke},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={7},
  pages={1--108},
  year={2021}
}

@article{cen2024value,
  title={Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF},
  author={Cen, Shicong and Mei, Jincheng and Goshvadi, Katayoon and Dai, Hanjun and Yang, Tong and Yang, Sherry and Schuurmans, Dale and Chi, Yuejie and Dai, Bo},
  journal={arXiv preprint arXiv:2405.19320},
  year={2024}
}

@inproceedings{chen2022human,
  title={Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation},
  author={Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei},
  booktitle={International Conference on Machine Learning},
  pages={3773--3793},
  year={2022},
  organization={PMLR}
}

@inproceedings{das2024active,
  title={Active preference optimization for sample efficient RLHF},
  author={Das, Nirjhar and Chakraborty, Souradip and Pacchiano, Aldo and Chowdhury, Sayak Ray},
  booktitle={ICML 2024 Workshop on Theoretical Foundations of Foundation Models},
  year={2024}
}

@article{dong2024rlhf,
  title={Rlhf workflow: From reward modeling to online rlhf},
  author={Dong, Hanze and Xiong, Wei and Pang, Bo and Wang, Haoxiang and Zhao, Han and Zhou, Yingbo and Jiang, Nan and Sahoo, Doyen and Xiong, Caiming and Zhang, Tong},
  journal={arXiv preprint arXiv:2405.07863},
  year={2024}
}

@article{dwaracherla2024efficient,
  title={Efficient exploration for llms},
  author={Dwaracherla, Vikranth and Asghari, Seyed Mohammad and Hao, Botao and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:2402.00396},
  year={2024}
}

@article{guo2024direct,
  title={Direct language model alignment from online ai feedback},
  author={Guo, Shangmin and Zhang, Biao and Liu, Tianlin and Liu, Tianqi and Khalman, Misha and Llinares, Felipe and Rame, Alexandre and Mesnard, Thomas and Zhao, Yao and Piot, Bilal and others},
  journal={arXiv preprint arXiv:2402.04792},
  year={2024}
}

@article{ji2024self,
  title={Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models},
  author={Ji, Xiang and Kulkarni, Sanjeev and Wang, Mengdi and Xie, Tengyang},
  journal={arXiv preprint arXiv:2406.04274},
  year={2024}
}

@article{liu2024provably,
  title={Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer},
  author={Liu, Zhihan and Lu, Miao and Zhang, Shenao and Liu, Boyi and Guo, Hongyi and Yang, Yingxiang and Blanchet, Jose and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2405.16436},
  year={2024}
}

@inproceedings{novoseller2020dueling,
  title={Dueling posterior sampling for preference-based reinforcement learning},
  author={Novoseller, Ellen and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={1029--1038},
  year={2020},
  organization={PMLR}
}

@article{pacchiano2021dueling,
  title={Dueling rl: reinforcement learning with trajectory preferences},
  author={Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan},
  journal={arXiv preprint arXiv:2111.04850},
  year={2021}
}

@inproceedings{saha2018battle,
  title={Battle of Bandits.},
  author={Saha, Aadirupa and Gopalan, Aditya},
  booktitle={UAI},
  pages={805--814},
  year={2018}
}

@article{wang2023rlhf,
  title={Is RLHF More Difficult than Standard RL?},
  author={Wang, Yuanhao and Liu, Qinghua and Jin, Chi},
  journal={arXiv preprint arXiv:2306.14111},
  year={2023}
}

@article{wu2023making,
  title={Making rl with preference-based feedback efficient via randomization},
  author={Wu, Runzhe and Sun, Wen},
  journal={arXiv preprint arXiv:2310.14554},
  year={2023}
}

@article{xie2024exploratory,
  title={Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF},
  author={Xie, Tengyang and Foster, Dylan J and Krishnamurthy, Akshay and Rosset, Corby and Awadallah, Ahmed and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2405.21046},
  year={2024}
}

@article{xiong2024building,
  title={Building math agents with multi-turn iterative preference learning},
  author={Xiong, Wei and Shi, Chengshuai and Shen, Jiaming and Rosenberg, Aviv and Qin, Zhen and Calandriello, Daniele and Khalman, Misha and Joshi, Rishabh and Piot, Bilal and Saleh, Mohammad and others},
  journal={arXiv preprint arXiv:2409.02392},
  year={2024}
}

@inproceedings{xiong2024iterative,
  title={Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint},
  author={Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{xu2020preference,
  title={Preference-based reinforcement learning with finite-time guarantees},
  author={Xu, Yichong and Wang, Ruosong and Yang, Lin and Singh, Aarti and Dubrawski, Artur},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18784--18794},
  year={2020}
}

@article{xu2023some,
  title={Some things are more cringe than others: Preference optimization with the pairwise cringe loss},
  author={Xu, Jing and Lee, Andrew and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2312.16682},
  year={2023}
}

@article{ye2024theoretical,
  title={A theoretical analysis of nash learning from human feedback under general kl-regularized preference},
  author={Ye, Chenlu and Xiong, Wei and Zhang, Yuheng and Jiang, Nan and Zhang, Tong},
  journal={arXiv preprint arXiv:2402.07314},
  year={2024}
}

@article{yue2012k,
  title={The k-armed dueling bandits problem},
  author={Yue, Yisong and Broder, Josef and Kleinberg, Robert and Joachims, Thorsten},
  journal={Journal of Computer and System Sciences},
  volume={78},
  number={5},
  pages={1538--1556},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{zhan2023provable,
  title={Provable offline reinforcement learning with human feedback},
  author={Zhan, Wenhao and Uehara, Masatoshi and Kallus, Nathan and Lee, Jason D and Sun, Wen},
  booktitle={ICML 2023 Workshop The Many Facets of Preference-Based Learning},
  year={2023}
}

@inproceedings{zhan2023query,
  title={How to Query Human Feedback Efficiently in RL?},
  author={Zhan, Wenhao and Uehara, Masatoshi and Sun, Wen and Lee, Jason D},
  booktitle={ICML 2023 Workshop The Many Facets of Preference-Based Learning}
}

@article{zhang2024self,
  title={Self-exploring language models: Active preference elicitation for online alignment},
  author={Zhang, Shenao and Yu, Donghan and Sharma, Hiteshi and Zhong, Han and Liu, Zhihan and Yang, Ziyi and Wang, Shuohang and Hassan, Hany and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2405.19332},
  year={2024}
}

@inproceedings{zhu2023principled,
  title={Principled reinforcement learning with human feedback from pairwise or k-wise comparisons},
  author={Zhu, Banghua and Jordan, Michael and Jiao, Jiantao},
  booktitle={International Conference on Machine Learning},
  pages={43037--43067},
  year={2023},
  organization={PMLR}
}

