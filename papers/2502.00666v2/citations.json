[
  {
    "index": 0,
    "papers": [
      {
        "key": "yue2012k",
        "author": "Yue, Yisong and Broder, Josef and Kleinberg, Robert and Joachims, Thorsten",
        "title": "The k-armed dueling bandits problem"
      },
      {
        "key": "saha2018battle",
        "author": "Saha, Aadirupa and Gopalan, Aditya",
        "title": "Battle of Bandits."
      },
      {
        "key": "bengs2021preference",
        "author": "Bengs, Viktor and Busa-Fekete, R{\\'o}bert and El Mesaoudi-Paul, Adil and H{\\\"u}llermeier, Eyke",
        "title": "Preference-based online learning with dueling bandits: A survey"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "xu2020preference",
        "author": "Xu, Yichong and Wang, Ruosong and Yang, Lin and Singh, Aarti and Dubrawski, Artur",
        "title": "Preference-based reinforcement learning with finite-time guarantees"
      },
      {
        "key": "novoseller2020dueling",
        "author": "Novoseller, Ellen and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel",
        "title": "Dueling posterior sampling for preference-based reinforcement learning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "pacchiano2021dueling",
        "author": "Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan",
        "title": "Dueling rl: reinforcement learning with trajectory preferences"
      },
      {
        "key": "chen2022human",
        "author": "Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei",
        "title": "Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation"
      },
      {
        "key": "wu2023making",
        "author": "Wu, Runzhe and Sun, Wen",
        "title": "Making rl with preference-based feedback efficient via randomization"
      },
      {
        "key": "zhan2023query",
        "author": "Zhan, Wenhao and Uehara, Masatoshi and Sun, Wen and Lee, Jason D",
        "title": "How to Query Human Feedback Efficiently in RL?"
      },
      {
        "key": "das2024active",
        "author": "Das, Nirjhar and Chakraborty, Souradip and Pacchiano, Aldo and Chowdhury, Sayak Ray",
        "title": "Active preference optimization for sample efficient RLHF"
      },
      {
        "key": "wang2023rlhf",
        "author": "Wang, Yuanhao and Liu, Qinghua and Jin, Chi",
        "title": "Is RLHF More Difficult than Standard RL?"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhu2023principled",
        "author": "Zhu, Banghua and Jordan, Michael and Jiao, Jiantao",
        "title": "Principled reinforcement learning with human feedback from pairwise or k-wise comparisons"
      },
      {
        "key": "zhan2023provable",
        "author": "Zhan, Wenhao and Uehara, Masatoshi and Kallus, Nathan and Lee, Jason D and Sun, Wen",
        "title": "Provable offline reinforcement learning with human feedback"
      },
      {
        "key": "ji2024self",
        "author": "Ji, Xiang and Kulkarni, Sanjeev and Wang, Mengdi and Xie, Tengyang",
        "title": "Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models"
      },
      {
        "key": "liu2024provably",
        "author": "Liu, Zhihan and Lu, Miao and Zhang, Shenao and Liu, Boyi and Guo, Hongyi and Yang, Yingxiang and Blanchet, Jose and Wang, Zhaoran",
        "title": "Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "pacchiano2021dueling",
        "author": "Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan",
        "title": "Dueling rl: reinforcement learning with trajectory preferences"
      },
      {
        "key": "das2024active",
        "author": "Das, Nirjhar and Chakraborty, Souradip and Pacchiano, Aldo and Chowdhury, Sayak Ray",
        "title": "Active preference optimization for sample efficient RLHF"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "chen2022human",
        "author": "Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei",
        "title": "Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation"
      },
      {
        "key": "zhan2023provable",
        "author": "Zhan, Wenhao and Uehara, Masatoshi and Kallus, Nathan and Lee, Jason D and Sun, Wen",
        "title": "Provable offline reinforcement learning with human feedback"
      },
      {
        "key": "wang2023rlhf",
        "author": "Wang, Yuanhao and Liu, Qinghua and Jin, Chi",
        "title": "Is RLHF More Difficult than Standard RL?"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "guo2024direct",
        "author": "Guo, Shangmin and Zhang, Biao and Liu, Tianlin and Liu, Tianqi and Khalman, Misha and Llinares, Felipe and Rame, Alexandre and Mesnard, Thomas and Zhao, Yao and Piot, Bilal and others",
        "title": "Direct language model alignment from online ai feedback"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "xu2023some",
        "author": "Xu, Jing and Lee, Andrew and Sukhbaatar, Sainbayar and Weston, Jason",
        "title": "Some things are more cringe than others: Preference optimization with the pairwise cringe loss"
      },
      {
        "key": "dong2024rlhf",
        "author": "Dong, Hanze and Xiong, Wei and Pang, Bo and Wang, Haoxiang and Zhao, Han and Zhou, Yingbo and Jiang, Nan and Sahoo, Doyen and Xiong, Caiming and Zhang, Tong",
        "title": "Rlhf workflow: From reward modeling to online rlhf"
      },
      {
        "key": "xiong2024building",
        "author": "Xiong, Wei and Shi, Chengshuai and Shen, Jiaming and Rosenberg, Aviv and Qin, Zhen and Calandriello, Daniele and Khalman, Misha and Joshi, Rishabh and Piot, Bilal and Saleh, Mohammad and others",
        "title": "Building math agents with multi-turn iterative preference learning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "dwaracherla2024efficient",
        "author": "Dwaracherla, Vikranth and Asghari, Seyed Mohammad and Hao, Botao and Van Roy, Benjamin",
        "title": "Efficient exploration for llms"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "dwaracherla2024efficient",
        "author": "Dwaracherla, Vikranth and Asghari, Seyed Mohammad and Hao, Botao and Van Roy, Benjamin",
        "title": "Efficient exploration for llms"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ye2024theoretical",
        "author": "Ye, Chenlu and Xiong, Wei and Zhang, Yuheng and Jiang, Nan and Zhang, Tong",
        "title": "A theoretical analysis of nash learning from human feedback under general kl-regularized preference"
      },
      {
        "key": "xiong2024iterative",
        "author": "Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong",
        "title": "Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "cen2024value",
        "author": "Cen, Shicong and Mei, Jincheng and Goshvadi, Katayoon and Dai, Hanjun and Yang, Tong and Yang, Sherry and Schuurmans, Dale and Chi, Yuejie and Dai, Bo",
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF"
      },
      {
        "key": "xie2024exploratory",
        "author": "Xie, Tengyang and Foster, Dylan J and Krishnamurthy, Akshay and Rosset, Corby and Awadallah, Ahmed and Rakhlin, Alexander",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      },
      {
        "key": "zhang2024self",
        "author": "Zhang, Shenao and Yu, Donghan and Sharma, Hiteshi and Zhong, Han and Liu, Zhihan and Yang, Ziyi and Wang, Shuohang and Hassan, Hany and Wang, Zhaoran",
        "title": "Self-exploring language models: Active preference elicitation for online alignment"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhang2024self",
        "author": "Zhang, Shenao and Yu, Donghan and Sharma, Hiteshi and Zhong, Han and Liu, Zhihan and Yang, Ziyi and Wang, Shuohang and Hassan, Hany and Wang, Zhaoran",
        "title": "Self-exploring language models: Active preference elicitation for online alignment"
      },
      {
        "key": "cen2024value",
        "author": "Cen, Shicong and Mei, Jincheng and Goshvadi, Katayoon and Dai, Hanjun and Yang, Tong and Yang, Sherry and Schuurmans, Dale and Chi, Yuejie and Dai, Bo",
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "xie2024exploratory",
        "author": "Xie, Tengyang and Foster, Dylan J and Krishnamurthy, Akshay and Rosset, Corby and Awadallah, Ahmed and Rakhlin, Alexander",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      }
    ]
  }
]