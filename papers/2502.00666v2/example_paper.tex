%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% mingyu
\usepackage{bbm}
\usepackage{multirow}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{enumitem}
\setlist[enumerate]{itemsep=0mm}
\setlist[itemize]{itemsep=0mm}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Avoiding $\exp$($R_\text{max}$) scaling in RLHF through Preference-based Exploration}

\begin{document}

%mingyu
\addtocontents{toc}{\protect\setcounter{tocdepth}{-1}}


\twocolumn[
\icmltitle{Avoiding $\mathbf{exp(R_{max})}$ scaling in RLHF through Preference-based Exploration}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Mingyu Chen}{my}
\icmlauthor{Yiding Chen}{comp}
\icmlauthor{Wen Sun}{comp}
\icmlauthor{Xuezhou Zhang}{xz}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{my}{Department of Electrical \& Computer Engineering, Boston University}
\icmlaffiliation{comp}{Department of Computer Science, Cornell University}
\icmlaffiliation{xz}{Faculty of Computing \& Data Sciences, Boston University}

\icmlcorrespondingauthor{Mingyu Chen}{mingyuc@bu.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique for large language model (LLM) alignment.
This paper studies the setting of online RLHF and focus on improving sample efficiency.
All existing algorithms in online RLHF, whether doing passive exploration or active exploration, suffer from a sample complexity that scales exponentially with the scale of the reward function.
This fundamental limitation hinders their effectiveness in scenarios with heavily skewed preferences, e.g. questions with a unique correct solution.
To address this, we introduce \textit{Self-Exploring Preference-Incentive Online Preference Optimization} (\texttt{SE-POPO}), an online RLHF algorithm that for the first time achieves a sample complexity that scales \textit{polynomially} with the reward scale, answering an open problem raised by \citet{xie2024exploratory}.
Theoretically, we demonstrate that the sample complexity of \texttt{SE-POPO} dominates that of existing exploration algorithms. Empirically, our systematic evaluation confirms that \texttt{SE-POPO} is more sample-efficient than both exploratory and non-exploratory baselines, in two primary application scenarios of RLHF as well as on public benchmarks, marking a significant step forward in RLHF algorithm design.
The code is available at \url{https://github.com/MYC000801/SE-POPO}.
\end{abstract}

\section{Introduction}\label{section::introduction}
Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique in the post-training of Large Language Models (LLMs) \cite{christiano2017deep, ziegler2019fine, ouyang2022training}. 
% With the assistance of human-annotated preference data, RLHF significantly boosts model performance and robustness via consistent alignment with human values. 
% Many advanced LLMs have demonstrated the success of RLHF in alignment, e.g., ChatGPT \cite{chatgpt}, Claude \cite{claude} and Gemini \cite{gemini}.
Earlier works on RLHF focus primarily on the offline setting \cite{ouyang2022training, rafailov2024direct}, where the preference data are pre-collected and fixed prior to the fine-tuning phase.
However, in this setting, the quality of alignment is fundamentally limited by the quality of response in the pre-collected preference dataset \cite{xiong2024iterative}.
To overcome this limitation, recent works attempt to perform RLHF in an online framework.
By continually generating and subsequently labeling new samples during training, online RLHF allow the agents to receive feedbacks on out-of-distribution (OOD) responses, and thus achieving great empirical performance \cite{dong2024rlhf}.

Similar to online reinforcement learning, the most critical challenge in online RLHF is how to balance the \textit{exploration-exploitation trade-off}.  
In naive online RLHF algorithms \cite{guo2024direct}, the exploration is carried out passively, relying solely on the inherent randomness of the LLM policy.  
Such a passive approach will still fail to sufficiently explore the prompt-response space even with many samples.  
More recently, a number of active exploration algorithms have been proposed \cite{dwaracherla2024efficient, xiong2024iterative, xie2024exploratory, cen2024value, zhang2024self}. 
By leveraging optimism-based approaches to encourage the policy to target OOD regions, active exploration has demonstrated superior performance over passive exploration in both theoretical analysis and empirical evaluations.

However, all existing online RLHF algorithms, whether with passive or active exploration, share a fundamental limitation: They remain effective only in settings with a small reward scale. 
In particular, under the Bradley–Terry (BT) model assumption, all known sample complexity bounds scales \textit{exponentially} with the reward range \cite{xie2024exploratory}.
Intuitively, this issue arises because human feedback in RLHF is given in the form of preferences rather than explicit rewards. 
Under the BT model, even if there is a significant gap in rewards between two responses, they may behave very similar in their chance of being preferred preference when pairing with another response. 
As a result, exponentially many samples are required to distinguish the quality of responses based on preference signals. 
This leads to the open question raised by \citet{xie2024exploratory}:

\begin{center}
Does there exist a sample-efficient online RLHF algorithm that remains effective under large reward scale?
\end{center}

In this work, we answer this question in the positive with a new online RLHF algorithm, \textit{Self-Exploring Preference-Incentive Online Preference Optimization} (\texttt{SE-POPO}), that for the first time achieves a sample complexity that scales \textit{polynomially} with the reward scale.
Our algorithm is provably sample-efficient, scalable and easy to implement.
We summarize our contributions below.

\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item Unlike the commonly used reward-based exploration methods, we propose a preference-based exploration technique. 
    We demonstrate that active exploration driven by this technique outperforms existing reward-based exploration. 
    Equipped with this new technique, we design a subroutine algorithm \textit{Preference-Incentive Online Preference Optimization} (\texttt{POPO}), which only requires an one-line modification on top of  \texttt{DPO}. 
    \texttt{POPO} is implementation-friendly and already achieves a sample complexity guarantee on par with existing algorithms.
\item Building upon \texttt{POPO}, we propose a self-sampler update technique that effectively prevents the sample complexity from exploding as reward range increases. 
Leveraging this idea, we develop our main algorithm \texttt{SE-POPO}, which achieves a sample complexity scaling polynomially to the reward scale.
\item
We perform a comprehensive empirical investigation to validate our theory.
We conduct evaluations across various training and testing settings as well as on major public benchmarks. In addition to that we perform ablation studies to further understand the effect of design choices made in our algorithm.
The results demonstrate that our algorithm outperforms both exploratory and non-exploratory baselines across all benchmarks with a large margin.


\end{itemize}


\section{Related Works}

\paragraph{Theoretical Study on RLHF}
Theoretical analysis of RLHF has recently emerged as one of the main interests in the community. 
The earliest study trace back to the dueling bandits literature \cite{yue2012k, saha2018battle, bengs2021preference}, along with studies considering tabular RL with finite state space \cite{xu2020preference, novoseller2020dueling} and linear RL or general function approximation RL with infinite state space \cite{pacchiano2021dueling,chen2022human, wu2023making, zhan2023query, das2024active, wang2023rlhf}. 
Apart from the online setting, a substantial body of research focuses on offline RLHF \cite{zhu2023principled, zhan2023provable, ji2024self, liu2024provably}, which leverages predetermined offline datasets with appropriate coverage conditions over the state-action space and can be considered complementary to our work.
Although these studies offer sample complexity guarantees for RLHF, most algorithms are not scalable enough to be applicable to modern LLMs with large transformer architectures.
For instance, \cite{pacchiano2021dueling, das2024active} incorporate exploration bonuses tailored for linear models in the reward estimation.
\cite{ chen2022human,zhan2023provable, wang2023rlhf} rely on model-based function approximation and explicitly estimate the policy confidence set. These approaches fail to yield efficient or practical algorithms when applied to LLMs.


\paragraph{Exploration for online LLM alignment}
Exploration in online RLHF has seen rapid development recently.
Earlier attempts, such as online DPO \cite{guo2024direct} and iterative DPO \cite{xu2023some, dong2024rlhf,xiong2024building}, primarily rely on passive exploration, i.e. the inherent randomness of LLM policy, and lack explicit mechanisms to encourage diverse and exploratory responses.
The importance of active exploration in RLHF has been highlighted by \cite{dwaracherla2024efficient}. 
%\cite{dwaracherla2024efficient} propose  using the posterior of reward models to approximately measure the uncertainty for active exploration.
Subsequent works, such as \cite{ye2024theoretical, xiong2024iterative}, propose algorithms with an active exploration mechanism and provide a sample complexity guarantees for online RLHF.
However, these exploration strategies involve solving an intractable optimization problem, making them impractical to implement in LLM alignment. 
Notably, in these works, experiments are often conducted based on heuristic variants of the proposed algorithms, resulting in a significant gap between theory and practice.

Recent studies \cite{cen2024value,xie2024exploratory,zhang2024self} introduce implementation-friendly and provably sample-efficient exploration algorithms for RLHF, which are most relevant to our work.
All three papers are based on the common idea of augmenting the DPO loss with a \textit{reward-based} optimistic bonus to encourage exploration. 
Among them, \cite{zhang2024self, cen2024value} mainly focus on the exploration under the contextual bandit formulation of RLHF, whereas \cite{xie2024exploratory} provides analysis for the token-level MDP formulation.
However, a significant limitation of these algorithms is that their sample complexity scales exponentially with $ R_{\text{max}}$, the scale of the reward function (see Asm. \ref{ass: bounded reward}), which is highly inefficient in both theory and practice. Our algorithm becomes the first that remove such $\exp(R_{\text{max}})$ dependency.

\section{RLHF Preliminaries}
In RLHF, we denote a policy by $\pi$, which generates an answer $y\in \mathcal{Y}$ given a prompt $x\in \mathcal{X}$ according to the conditional probability distribution $\pi(\cdot|x)$. 
Given two responses $y$ and $y'$ with respect to prompt $x$, we assume a preference oracle, i.e. a human evaluator, will evaluate the quality of two responses and indicate the preferred one.
Following prior works, we consider Bradley–Terry model as the preference oracle.
The mathematical definition is below.
\begin{definition}(Bradley–Terry (BT) Model)
\label{definition:1}
There exists an underlying reward function $r^\star: \mathcal{X}\times \mathcal{Y} \to \mathbb{R} $ such that for every $x,y,y'\in \mathcal{X}\times\mathcal{Y}\times \mathcal{Y}$, 
\begin{align*}
    \mathbb{P}^\star(y\succ y'|x) &= \frac{\exp(r^\star(x,y))}{\exp(r^\star(x,y))+ \exp(r^\star(x,y'))} \\
    &= \sigma(r^\star(x,y)-r^\star(x,y')),
\end{align*}
where $ \mathbb{P}^\star(y\succ y'|x)$ represents the probability that $y$ is preferred to $y'$ given $x$ and $\sigma$ represents the sigmoid function. 
% We remark that our algorithm and its preference-based regret guarantee do not rely on the BT model at all. We only use it to induce the reward-based regret to compare with prior work.
\end{definition}
\begin{assumption}[Bounded Reward]
\label{ass: bounded reward}
    For all $x,y\in \mathcal{X}\times\mathcal{Y}$, we have $r^\star(x,y)\in [-R_{\text{max}}/2,R_{\text{max}}/2]$. Without loss of generality, we assume $R_{\text{max}}\ge1 $.
\end{assumption}

\paragraph{The Two-stage RLHF pipeline}
In the classic two-stage RLHF framework \cite{christiano2017deep, ouyang2022training}, the algorithm assumes access to a dataset $\mathcal{D} = \{x_n, y_n^1, y_n^2, o_t\}_{n=1}^N$, where 
\begin{align*}
    x_n\sim \rho,\ y_n^1\sim \pi_{\text{ref}},\ y_n^2\sim \pi_{\text{ref}},\ o_n\sim \text{Ber}\left( \mathbb{P}^\star(y\succ y'|x)  \right).
\end{align*}
Here, $\rho$ denotes the underlying prompt distribution.
$\pi_{\text{ref}}$ is a reference language model, which is typically obtained via supervised fine-tuning.
$o_n$ is obtained by the preference oracle.
For simplicity, we redefine the dataset as $\mathcal{D} = \{x_n, y_n^w, y_n^l\}_{n=1}^N$, where $y_n^w$ and $y_n^l$ are assigned based on the value of $o_n$.
Given the dataset, we first estimate the reward function via maximum likelihood estimation, i.e.,
\begin{align}\label{rlhf_rm}
    \hat r&= \arg\min_{r\in \mathcal{R}} -\sum_{n=1}^N \log\sigma\left( 
 r(x_n, y_n^w) - r(x_n, y_n^l) \right) \nonumber\\
 &= \arg\min_{r\in \mathcal{R}}  \ell(r, \mathcal{D}).
\end{align}
With the learned reward function, the objective of RLHF is to fine-tune the policy $\pi$ to maximize the reward.
Following prior theoretical works on RLHF, we consider a KL-regularized reward objective, that is,
\begin{align}\label{rlhf_po}
    \hat \pi &= \arg\max_{\pi\in \Pi} \mathbb{E}_{x\sim \rho, y\sim \pi(\cdot|x)}\left[\hat r(x,y) - \beta \log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}   \right] \nonumber\\
    &=  \arg\max_{\pi\in \Pi}  J(\hat r, \pi).
\end{align}
\paragraph{The DPO pipeline}
An alternative approach of RLHF is introduced by \cite{rafailov2024direct}, namely Direct Preference Optimization (DPO).
The key motivation of DPO is from the closed-form solution of (\ref{rlhf_po}), that is, given a reward function $\hat r$, the solution $\hat \pi$ satisfies
\begin{align}\label{DPO_1}
    \hat \pi (y|x) = \frac{\pi_{\text{ref}(y|x)}\exp(\hat  r(x,y)/\beta)}{Z(r,x)},\ \forall x,y\in \mathcal{X}\times \mathcal{Y}
\end{align}
where $Z(r,x) = \sum_{y} \pi_{\text{ref}(y|x)}\exp(\hat  r(y|x)/\beta)$ is a partition function independent of $y$.
The closed form solution allows us to represent the reward by $\hat \pi$
\begin{align}\label{DPO_2}
    &\hat r(x,y) - \hat r(x,y')
    = \beta \log \frac{\hat\pi(y|x)}{\pi_{\text{ref}}(y|x)}-\beta \log \frac{\hat\pi(y'|x)}{\pi_{\text{ref}}(y'|x)}
\end{align}
for every $\forall (x,y,y')\in \mathcal{X}\times \mathcal{Y}\times \mathcal{Y}$.
By substituting (\ref{DPO_2}) into (\ref{rlhf_rm}), DPO bypasses the need for explicitly learning the reward function.
Instead, it optimizes the policy directly with objective
\begin{align*}
\min_{\pi\in \Pi}- \sum_{n=1}^N \log\sigma\left( 
 \beta \log \frac{\pi(y_n^w|x_n)}{\pi_{\text{ref}}(y_n^w|x_n)}-\beta \log \frac{\pi(y_n^l|x_n)}{\pi_{\text{ref}}(y_n^l|x_n)}\right).
\end{align*}

\paragraph{Performance metric}
The performance of a learned policy $\hat \pi$ is measured by the suboptimal gap
\begin{align*}
    \text{SubOpt}(\hat\pi) = \mathbb{E}_{x\sim \rho, y\sim \pi^\star(\cdot|x), y'\sim \hat \pi(\cdot|x)}[r^\star(x,y)-r^\star(x,y')],
\end{align*}
where $\pi^\star = \arg\max_{\pi\in \Pi} \mathbb{E}_{x\sim \rho, y\sim \pi^\star(\cdot|x)}[r^\star(x,y)]$ denotes the optimal policy.
Our goal is to propose a sample-efficient and also implementation-friendly algorithm to learn a policy $\hat \pi\in \Pi$ such that $\text{SubOpt}(\hat \pi)\le \epsilon$ for some small $\epsilon>0$.

\paragraph{Online Feedback and Exploration}
In early RLHF studies, the preference dataset $\mathcal{D}$ is typically assumed to be given. 
Although such offline RLHF has been highly successful in aligning language models, it is inherently constrained by the quality of the preference data and $\pi_{\text{ref}}$. 
To overcome these limitations, RLHF with online feedback is proposed \cite{guo2024direct}.
In the online framework, the dataset is constructed with human feedbacks on the responses generated from the language model on the fly.
Formally, online RLHF proceeds in $T$ rounds with each round as follows:
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item The agent computes $\pi_t$ using the current dataset $\mathcal{D}_t$ and samples $x_t\sim \rho, y_t^1\sim \pi_t(\cdot|x), y_t^2\sim \pi_t(\cdot|x)$.
    \item Human evaluators label responses $(x_t, y_t^1, y_t^2)\to (x_t, y_t^w, y_t^l)$. Update $\mathcal{D}_{t+1} = \mathcal{D}_t \cup \{(x_t, y_t^w, y_t^l)\}$.
\end{enumerate}
Although numerous empirical studies have demonstrated the benefits of online RLHF, the theoretical foundation has been missing.
The main reason is that existing methods rely on \textit{passive exploration} to collect data, i.e. the responses are sampled directly from the policy $\pi_t$ relying purely on the randomness of $\pi_t$ for exploration.
Motivated by this, recent works \cite{cen2024value, xie2024exploratory, zhang2024self} start to incorporate the optimism principle into RLHF, which encourages explicitly exploration in the policy $\pi_t$.
Although their implementations differ, the essence of their algorithms is to replace the MLE objectives \eqref{rlhf_rm} and \eqref{rlhf_po} in vanilla RLHF with
\begin{align}
 r_{t+1} =\ &\arg\max_{r\in \mathcal{R}} \left\{-\ell
(r, \mathcal{D}_t)+ \alpha J(r, \pi(r))\right \}, \nonumber\\
&\text{s.t. }\pi(r) = \arg\max_{\pi\in \Pi}J(r, \pi) \label{xpo-obj}
\end{align}
where $\alpha \max_{\pi\in \Pi}J(r, \pi)$ is a \textbf{reward-based exploration bonus} that encourages exploration.
Such a bonus leads to an overestimation of rewards with high uncertainty, thereby incentivizing policy to explore uncertain responses.
As shown in \cite{cen2024value, xie2024exploratory}, this design offers a practical and provably sample-efficient online exploration algorithm for RLHF with general function approximation.

\section{Preference-based Exploration}
% explain why xpo need expential blow up of reward: if you sampled second response by piref, it is inivitable.
% give the intuitions
%Interestingly, to the best of our knowledge, the theoretical guarantees of all previous RLHF works are exponential to the reward scale \cite{cen2024value, xie2024exploratory, zhang2024self}.
Although existing algorithms based on \eqref{xpo-obj} obtain theoretical sample efficiency guarantees, there is a significant gap between their bounds and what could be achieved under the standard MDP framework.
In particular, a key weakness in all existing sample complexity bounds of these algorithms is an \textit{exponential} dependency on the scale of reward $ R_{\text{max}}$.
This makes existing guarantees quite subtle, as the bound quickly becomes vacuous as soon as $R_{\text{max}}$ is moderately large.
In practical LLM applications, it is common that one response can strictly dominate another, i.e., $ \mathbb{P}^\star(y \succ y' | x) \to 1 $.
Under the BT model (Definition~\ref{definition:1}), this implies a very large $R_{\text{max}}$ and thus existing bounds will fail. Authors of prior works have admitted that this is a significant drawback of these results and in fact conjectured that the exponential dependency might be unavoidable \cite{xie2024exploratory}. In this paper, we resolve this conjecture in the negative by presenting the first algorithm that avoids such exponential dependency on the reward scale.
In what follows, we start by discussing the cause of exponential dependency on $R_{\text{max}}$ and why it's a real limitation of existing algorithms rather than merely a result of weak analysis. We then introduce our algorithm \texttt{SE-POPO} and present its theoretical properties.

\subsection{The cause of $\mathbf{exp(R_{max})}$ scaling}\label{section:1}
%Let us first consider why the exponential blow-up in $R_{\text{max}}$ arises.
Using online-to-batch technique, the sample complexity of an online algorithm can be derived from its regret, which is defined by $\sum_{t=1}^T \text{SubOpt}(\pi_t)$.
In the standard analysis of optimism online RLHF, the regret can be bounded by the sum of reward uncertainty, i.e., $\sum_{t=1}^T \mathbb{E}_{x\sim \rho, y\sim \pi_t(\cdot|x)}[|r_t(x,y)-r^\star(x,y)|]$, where $r_t$ is the induced reward function from $\pi_t$ as in DPO.
To bound the reward uncertainty, prior works reduce it to the preference uncertainty, i.e., $\sum_{t=1}^T \mathbb{E}_{x \sim \rho, y \sim \pi_t(\cdot | x), y' \sim \pi_{t}(\cdot | x)} [|\mathbb{P}_t(y \succ y' | x) - \mathbb{P}^\star(y \succ y' | x)|]$, as the preference uncertainty can be effectively bounded using concentration inequalities.
Unfortunately, this reduction is not a free lunch: due to the presence of sigmoid function in Bradley–Terry Model, for some $x,y,y'$, there is
\begin{align}
    |r_t(x,y)-r^\star(x,y)|\approx \frac{|\mathbb{P}_t(y \succ y' | x) - \mathbb{P}^\star(y \succ y' | x)|}{\nabla \sigma(r^\star(x,y)-r^\star(x,y'))}
    \label{p2r-exp}
\end{align}
Therefore, the reward uncertainty could be of order $ 1 / \nabla \sigma(R_{\text{max}})\approx \mathcal{O}(\exp(R_{\text{max}})) $ times the preference uncertainty, in the worst case where the reward gap between the two responses $y$ and $y'$ is large.
This explains where $\exp(R_{\text{max}})$ comes from in the theoretical analysis of existing works and highlights the key question in algorithm design: \textbf{How should we sample the responses $y$ and $y'$ in online RLHF?}
A number of prior works \cite{xiong2024iterative, dong2024rlhf, shi2024crucial} use $\pi_t$ to sample $y_t^1$ and use $\pi_{\text{ref}}$, or a policy distinct from $\pi_t$, to sample $y_t^2$. This, however, destine to performly poorly according to the theoretical intuition provided by \eqref{p2r-exp}. In particular, sampling $y_t^2$ using an underperformed policy, such as $\pi_{\text{ref}}$ implies that the reward gap $r^\star(x_t, y_t^1)-r^\star(x_t, y_t^2)$ would be relatively large, causing $ y_t^1 $ to be consistently favored, even if $ y_t^1$ itself is suboptimal.
As a result, such algorithms will struggle to learn the optimal response, as such comparison provides little information on how to improve based on the current best policy $\pi_t$.


\subsection{Algorithm Design}
% two points: 1). how to update the policy for second response 2). how to set the bonus.
% for the first, update it in interval, to make sure the policy is always self-improved.
% for the second, make the bonus from reward based to preference bounus, thus it is slef-bounded and more stable.
% explain balabala the high level ideas and then go to theritical proof.
Given the above intuition, we propose \textit{Preference-Incentive Online Preference Optimization with Self-updated Sampler} (\texttt{SE-POPO}), which for the first time enjoys a sample complexity bound that scales \textbf{polynomially with $R_{\text{max}}$}.
%Conceptually, \texttt{SE-POPO} differs from previous works in two main aspects: 1) to encourage diversity, \texttt{SE-POPO} applies a preference-based exploration bonus rather than a reward-based exploration bonus, 2) Instead of fixing the second sampler as $\pi_{\text{ref}}$, \texttt{SE-POPO} updates the second sampler at intervals.
Conceptually, \texttt{SE-POPO} differs from prior algorithms in two main aspects: 1) it uses a preference-based exploration bonus instead of a reward-based bonus to explore more efficiently, and 2) it updates the second sampler at intervals instead of fixing it as $\pi_{\text{ref}}$ or updating per step.
The pseudocode of the algorithms is presented in Algorithm~\ref{alg:SE-POPO} and \ref{alg:POPO}.

\texttt{SE-POPO} operates over $K$ intervals.
In each interval, \texttt{SE-POPO} selects a fixed sampler $\pi_{\text{sam}}$ to generate the second response and runs the subroutine \texttt{POPO} for $T$ iterations.
The output of \texttt{POPO} is used as the sampler for the next interval, and the output from the final interval serves as the output of \texttt{SE-POPO}.
Let us start by the subroutine \texttt{POPO}.
As illustrated in Algorithm~\ref{alg:POPO}, \texttt{POPO} shares a similar structure with existing optimism RLHF algorithms \cite{xie2024exploratory, zhang2024self, cen2024value}.
However, unlike prior designs that are tailored towards bounding the \textbf{reward-based regret}, i.e. $\sum_{t=1}^T \text{SubOpt}(\pi_t)$, \texttt{POPO} is designed to optimize the \textbf{preference-based regret} over sampler $\pi_{\text{sam}}$, i.e., $\text{Reg}_{\text{pref}}(\pi_{\text{sam}},T)=$
\begin{align}\label{eq:ref_reg}        \sum_{t=1}^T\underset{ 
 \substack{x\sim \rho\\ y^\star\sim \pi^\star(\cdot|x)\\y\sim \pi_t(\cdot|x)\\y'\sim \pi_{\text{sam}  }(\cdot|x)}}{\mathbb{E}}\bigg[\mathbb{P}^\star(y^\star\succ y'|x)-\mathbb{P}^\star(y\succ y'|x)  \bigg].
\end{align}
To achieve this, \texttt{POPO} optimizes over the following objective function instead of \eqref{xpo-obj}:
\begin{align}
 r_{t+1} =\ &\arg\max_{r\in \mathcal{R}} \left\{-\ell
(r, \mathcal{D}_t)+ \alpha G(r, \pi(r))\right \}, \nonumber\\
&\text{s.t. }\pi(r) = \arg\max_{\pi\in \Pi}J(r, \pi). \label{popo-obj}
\end{align}
Here, $G$ is the average preference rate of $\pi$ over $\pi_{\text{sam}}$
\begin{align*}
    G(r, \pi) = \mathbb{E}_{x\sim \rho, y\sim \pi(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\mathbb{P}_{r}\left(y\succ y'|x\right)  \right],
\end{align*}
where $\mathbb{P}_{r}$ denotes the preference oracle parameterized by reward $r$.
In brief, \texttt{POPO} applies a preference-based exploration bonus on the reward learning objective.
This design ensures that the optimistic exploration is conducted directly with respect to the preference, rather than the reward.
% With that, the preference-based regret can be bounded following an analysis similar to that used for bounding the reward-based regret as in prior works.
% As we shall see next, under the assumption of linear preference model, \texttt{POPO} achieves an $\tilde{\mathcal{O}}(\sqrt{T})$ bound for the preference-based regret, i.e.,
% \begin{align}
% \label{preference-regret}
%         \text{Reg}_{\text{pref}}(\pi_\text{sam},T)\le &{\mathcal{O}}\bigg( 
% \sqrt{dT\log \frac{T}{d}\log\frac{|\mathcal{R}|}{\delta}} \nonumber \\
% + \beta T & \mathbb{E}_{x\sim \rho}\left[ 
% \mathbb{D}_{\text{KL}}(\pi^\star(\cdot|x)||\pi_{\text{ref}}(\cdot|x))  \right] \bigg).
% \end{align}
   


\paragraph{Implementation-friendly Objective}
Similar to that of vanilla two-stage RLHF, the objective is a bilevel optimization involving both reward and policy, and is challenging to solve in practice. Fortunately, $\pi(r)$ remains to be the solution to the KL-regularized reward optimization objective, therefore (\ref{DPO_2}) continues to hold.
By substituting (\ref{DPO_2}) into (\ref{popo-obj}), similar to what is done in \texttt{DPO}, we can bypass the reward model and directly optimize the policy, as follows
    \begin{align}
        &\max_{ \pi\in \Pi} \sum_{s=1}^t  \log \sigma  \bigg(\beta \log \frac{\pi(y^w_s|x_s)}{\pi_{\text{ref}}(y^w_s|x_s)} - \beta \log \frac{\pi(y^l_s|x_s)}{\pi_{\text{ref}}(y^l_s|x_s)}\bigg) \nonumber\\
        &+ \alpha \underset{ 
 \substack{x\sim \rho\\y\sim \pi(\cdot|x)\\y'\sim \pi_{\text{sam}  }(\cdot|x)}}{\mathbb{E}}\left[  \sigma\bigg(\beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \beta \log \frac{\pi(y'|x)}{\pi_{\text{ref}}(y'|x)}\bigg)  \right].
        \label{popo-obj-dpo}
    \end{align}

Given the preference-based regret of \texttt{POPO}, we move on to show how \texttt{POPO} with self-updated samplers eliminates the exponential dependence on $ R_{\text{max}} $ in the reward-based regret.
The key is a novel \textit{Preference-to-Reward} reduction as follows.
%how to use POPO and self-updated sampler to get rid of the exponential dependence on $ R_{\text{max}} $ for reward-based regret.
\begin{lemma}(Preference-to-Reward reduction)
Given any prompt $x\in \mathcal{X}$, let $y^\star$ denotes the optimal response $y^\star = \arg\max_{y\in \mathcal{Y}} r^\star(x,y)$.
For every $(y,y')\in \mathcal{Y}\times\mathcal{Y}$, there is $\mathbbm{1}\{r^\star(x,y)-r^\star(x,y')\le 1\}[ r^\star(x,y^\star)-r^\star(x,y)] 
   \le 20 R_{\text{max}}\left[ \mathbb{P}^\star(y^\star \succ y'|x)-  \mathbb{P}^\star(y\succ y'|x) \right]$.
\label{Pre2Rwd-reduction}
\end{lemma}
The proof of Lemma~\ref{Pre2Rwd-reduction} is deferred to the appendix.
Intuitively, Lemma~\ref{Pre2Rwd-reduction} tells us that the exponential blow-up in preference-to-reward reduction only occurs when $ r^\star(x, y) - r^\star(x, y') $ is large. 
Assuming $ y' \sim \pi_{\text{sam}}(\cdot | x) $. 
If $ \pi_{\text{sam}} $ is ``good enough'' such that $ r^\star(x, y) -r^\star(x, y')\le 1 $ holds for all $x$, we can easily bound the reward-based regret by $ \text{Reg}_r(T) \le \mathcal{O}(R_{\max}) \text{Reg}_{\text{pref}}(\pi_{\text{sam}}, T) $, and thus get rid of the exponential dependence on $R_{\text{max}}$.


So how do we find a good enough sampler \( \pi_{\text{sam}} \)? 
An intuitive idea is to first run \texttt{POPO} to find a suboptimal policy, then use this policy as $\pi_{\text{sam}}$ and rerun \texttt{POPO}.
However, notice that finding a good enough policy by running \texttt{POPO} from scratch would still requires \(\mathcal{O}(\exp(R_{\text{max}}))\) iterations, as we would have been using \( \pi_{\text{ref}} \) as the sampler, and \( \pi_{\text{ref}} \) may be $O(R_{\text{max}})$ worse than $\pi^*$.
The trick, as shown in Algorithm~\ref{alg:SE-POPO}, is to repeat the \texttt{POPO} subroutine for many times and gradually improve $\pi_{\text{sam}}$. 
The main observation is that even if the sampler performs poorly, \texttt{POPO}'s output policy can still achieve a reward higher by a constant amount compared to the sampler.
For instance, consider \( x, y^\star, y' \) such that \( r^\star(x, y^\star) - r^\star(x, y') \) is large. 
If we use \( y' \) as the second response, after \( T \) iterations, we can find a \( y \) such that \( P^\star(y \succ y' | x) \ge P^\star(y^\star \succ y' | x) - \tilde{\mathcal{O}}(1/\sqrt{T}) \) by the preference-based regret (\ref{eq:ref_reg}). 
Since \( r^\star(x, y^\star) - r^\star(x, y') \) is large, \( P^\star(y^\star \succ y' | x) \) will be close to $1$, resulting in \( P^\star(y \succ y' | x) \) being significantly greater than $1/2$, which implies that there is a constant improvement between \( r^\star(x, y) \) and \( r^\star(x, y') \).
As we elaborate later, by repeating \texttt{POPO} $K=\mathcal{O}(R_{\text{max}})$ intervals, the sampler will finally become sufficiently effective.
%Combining all of the above, it suffices to say that the sample complexity of \texttt{SE-POPO} is polynomial in $R_{\text{max}}$.
We now present the formal results.


\begin{algorithm}[tb]
   \caption{SE-POPO: Self-Exploring Preference-Incentive Online Preference Optimization}\label{alg:SE-POPO}
\begin{algorithmic}
   \STATE {\bfseries Input:} Reference policy $\pi_{\text{ref}}$, Policy set $\Pi$, Iterations $T$, Intervals $K$
   \STATE Initialize $\pi_{\text{sam}}^1 \gets \pi_{\text{ref}}$.
   \FOR{$k=1,\dots, K-1$}
   \STATE Update the sampler $\pi_{\text{sam}}^{k+1}\gets \texttt{POPO}(\pi_{\text{ref}}, \pi_{\text{sam}}^{k},\Pi,T)$.
   \ENDFOR
   \STATE Return policy $\bar \pi = \texttt{POPO}(\pi_{\text{ref}}, \pi_{\text{sam}}^{K}, \Pi,T)$.
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[tb]
\caption{POPO: Preference-Incentive Online Preference Optimization}\label{alg:POPO}
\begin{algorithmic}
   \STATE {\bfseries Input:} Reference policy $\pi_{\text{ref}}$, Sampler $\pi_{\text{sam}}$, Policy set $\Pi$, Iterations $T$
   \STATE Initialize $\pi_1 = \pi_{\text{ref}}$.
   \FOR{$t=1,\dots,T$}
   \STATE Generate data $x_1\sim \rho, y_t^1\sim \pi_t(\cdot|x), {y}_t^2\sim \pi_{\text{sam}}(\cdot|x)$.
   \STATE Label the two responses: $(x_t, y^1_t, y^2_t)\to (x_t, y_t^w, y_t^l)$.

    \STATE Optimize objective (\ref{popo-obj-dpo}) with policies $\Pi$. Get $\pi_{t+1}$.
    

      
   \ENDFOR
   \STATE Return policy $\bar \pi = \text{Uniform}(\pi_1,\dots, \pi_t)$.
\end{algorithmic}
\end{algorithm}

\subsection{Theoretical Guarantees}
Let the regularization parameter $\beta>0$ be fixed.
We start by a \textit{reward realizability} assumption, which states that the reward class used in \texttt{SE-POPO} is sufficiently expressive.
\begin{assumption}(Reward realizability)
\label{assumption:1}
    There exists a set of reward functions $\mathcal{R}$ satisfying $r^\star\in \mathcal{R}$.
\end{assumption}
Given Assumption~\ref{assumption:1}, we define $\mathcal{P}$ as the set of preference model induced by $\mathcal{R}$, and define $\Pi$ as the optimal policies induced by $\mathcal{R}$ under KL-regularized reward objective (\ref{rlhf_po}).
Notice that $|\mathcal{P}| = |\mathcal{R}| = |{\Pi}|$ by definition.
We focus on the linear preference model settings for our theoretical result.
\begin{assumption} (Linear preference oracle)
\label{assumption:2}
    Every preference oracle $\mathbb{P}\in \mathcal{P}$ can be parameterized by
    \begin{align*}
        \mathbb{P}_\theta(y \succ y'|x) = \langle \phi(x,y,y'), \theta  \rangle,\ \forall (x,y,y')\in \mathcal{X}\times \mathcal{Y}\times \mathcal{Y},
    \end{align*}
    where $\phi(x,y,y'): \mathcal{X}\times \mathcal{Y}\times \mathcal{Y}\to \mathbb{R}^d$ is a fixed feature mapping and $\theta\in \mathbb{R}^d$ is the parameter. We further assume that $|\phi(x,y,y')|\le 1$ for all $x,y,y'$ and $\|\theta\|_2\le 1$.
\end{assumption}
%With Assumption~\ref{assumption:1} and \ref{assumption:2}, it suffices to focus on a parameter set $\Theta$ with $|\Theta| = |\mathcal{P}|$, which satisfies that for every $ \mathbb{P}\in \mathcal{P}$, there exists $\theta\in \Theta$ such that $\mathbb{P}_\theta = \mathbb{P}$.
The following is the preference-based regret bound for \texttt{POPO}.
\begin{theorem}
\label{theo:POPO}
Given Assumption~\ref{assumption:1} and \ref{assumption:2}, setting $  \alpha = \frac{1}{2}\sqrt{\frac{d\log {T}/{d}}{T\log {|\mathcal{R}|}/{\delta} }}$, with probability $1-\delta$, \texttt{POPO} guarantees that
    \begin{align*}
\text{Reg}_{\text{pref}}(\pi_{\text{sam}}, T)\le {\mathcal{O}}\left( 
\sqrt{dT\log \frac{T}{d}\log\frac{|\mathcal{R}|}{\delta}} + \beta T  C_{\text{KL}} \right),
    \end{align*}
    where $C_{\text{KL}} = \mathbb{E}_{x\sim \rho}\left[ 
\mathbb{D}_{\text{KL}}(\pi^\star(\cdot|x)||\pi_{\text{ref}}(\cdot|x))  \right]$.
\end{theorem}


Theorem~\ref{theo:POPO} established a clean $\tilde{\mathcal{O}}(\sqrt{dT})$ bound on the preference-based regret. 
This implies, for example, if one were to train against a strong baseline $\pi_{\text{sam}}$, i.e. GPT-4o, $\texttt{POPO}$ would achieve a winrate against GPT-4o similar to that of the optimal policy with a fast rate of convergence. Of course, in practice, we may not have such strong baselines at our disposal, and therefore $\texttt{SE-POPO}$ is designed to achieve a similar performance even without such baselines, by iteratively updating its $\pi_{\text{sam}}$. Our main theorem is presented as follows.
\begin{theorem}
\label{theo:SE-POPO}
Assuming $C_{\text{KL}}$ is well-bounded.
Setting $\beta =  o({1}/{\sqrt{T}})$ and $K=\lceil R_{\text{max}}\rceil $, with probability $1-\delta$, \texttt{SE-POPO} will output an $\epsilon$-optimal policy using $\tilde{\mathcal{O}}\left( \frac{d R_{\text{max}}^7\log\frac{|\mathcal{R}|}{\delta}}{\epsilon^2}   \right)$ samples.
\end{theorem}

\begin{remark}
Theorem~\ref{theo:SE-POPO} offers a significant improvement over all prior sample complexity bounds for RLHF algorithms under the BT-model, being the first sample complexity bound that scales polynomially with $R_{\text{max}}$.
Compared to prior works on online RLHF \cite{das2024provably, rosset2024direct, xie2024exploratory,zhang2024self, cen2024value}, Theorem~\ref{theo:SE-POPO} retains the same dependencies on the coverage parameter $d$, while successfully eliminating all the exponential dependence on $R_{\text{max}}$ and $1/\beta$.
Furthermore, in Appendix~\ref{appendix:generalization_bl}, we demonstrate that the theoretical results of \texttt{POPO} and \texttt{SE-POPO} can be generalized beyond linear preference oracle using a general complexity measure proposed in \cite{zhong2022gec}, extending our theoretical results to the general function approximation setting \cite{cheng2022provable, wang2023rlhf, ye2024theoretical}.
\end{remark}

\subsection{A Lightweight Implementation of \texttt{SE-POPO}}
One practical challenge we encounter when implementing \texttt{SE-POPO} is that calculating the gradient of the objective function
\eqref{popo-obj-dpo} requires sampling new responses $y\sim \pi(\cdot|x)$
\footnote{One potential solution is to apply importance sampling to estimate the gradient, i.e., sample $(y,y')\sim \pi_{\text{sam}} (\cdot|x)$ in batches and estimate the on-policy bounus by $\alpha  \frac{\pi(y|x)}{\pi_{\text{sam}}(y|x)} \sigma(\beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \beta \log \frac{\pi(y'|x)}{\pi_{\text{ref}}(y'|x)})$. However, due to the potentially significant discrepancy between \(\pi\) and \(\pi_{\text{sam}}\), the term \(\frac{\pi(y|x)}{\pi_{\text{sam}}(y|x)}\) could be highly unstable, resulting in a large variance in the bonus term. 
Indeed, we observed in our experiments that the importance sampling approach leads to very poor performance.}.
%As shown in (\ref{popo-obj-dpo}), the optimization objective is tied to the on-policy $\pi(r)$ in the exploration bonus term.
While such sampling is techniquely the same as what is required in DPO or any other on-policy RL algorithms, we empirically found that this on-policy sampling step is extremely slow in language model finetuning due to the lack of efficient LLM online inference libraries and limited computational resources at our disposal\footnote{It is worth noting that optimizing (\ref{popo-obj-dpo}) does not need to annotate the on-policy generated response. This means that, as long as an efficient online inference method is available, our algorithms can be seamlessly adapted to the iterative setting, as it only requires human preference feedback in batches.}.
%In the iterative setting, we cannot sample new responses within each iteration, making the transition from online version to iterative version for \texttt{SE-POPO} non-trivial. 
To bypass this issue, we decide to prune the first term within the bonus all together, resulting in the following objective
\begin{align}
        &\max_{ \pi\in \Pi} \sum_{s=1}^t  \log \sigma  \bigg(\beta \log \frac{\pi(y^w_s|x_s)}{\pi_{\text{ref}}(y^w_s|x_s)} - \beta \log \frac{\pi(y^l_s|x_s)}{\pi_{\text{ref}}(y^l_s|x_s)}\bigg) \nonumber\\
        &+ \alpha \underset{ 
 \substack{x\sim \rho\\y'\sim \pi_{\text{sam}  }(\cdot|x)}}{\mathbb{E}}\left[  \sigma\bigg( - \beta \log \frac{\pi(y'|x)}{\pi_{\text{ref}}(y'|x)}\bigg)  \right].\label{popo-obj-dpo-r}
\end{align}
To our surprise, we later on found out that (\ref{popo-obj-dpo-r}) still results in a sample-efficient algorithm in theory, based on the following neat observation.
\begin{lemma}
    Define 
    \begin{align*}
        H(r, \pi)&=\underset{ 
 \substack{x\sim \rho\\ y'\sim \pi_{\text{sam}  }(\cdot|x)}}{\mathbb{E}}\left[  \sigma\left( - \beta \log \frac{\pi(y'|x)}{\pi_{\text{ref}}(y'|x)}\right)  \right],
    \end{align*}
    then for every $r\in \mathcal{R}$, we have 
    \begin{align*}
        &|G(r, \pi(r))- H(r, \pi(r))|\\
        &\qquad \qquad\le \frac{\beta}{2}\mathbb{E}_{x\sim \rho}[\mathbb{D}_{\text{KL}}(\pi^\star_r(\cdot|x)||\pi_{\text{ref}}(\cdot|x))],
    \end{align*}
     where $\pi^\star_r = \arg\max_\pi \mathbb{E}_{x\sim \rho, y\sim \pi(\cdot|x)}[r(x,y)]$.
\label{lemma:POPO-efficient}
\end{lemma}
Lemma~\ref{lemma:POPO-efficient} implies that the gap between (\ref{popo-obj-dpo}) and (\ref{popo-obj-dpo-r}) scales with \( \beta \) and the KL divergence between $\pi_r^\star$ and $\pi_{\text{ref}}$.
In this case, given \( \beta = o(1/\sqrt{T}) \), replacing the optimization objective (\ref{popo-obj-dpo}) with (\ref{popo-obj-dpo-r}) still guarantees that Theorem~\ref{theo:POPO} essentially holds, i.e.,
%By Lemma~\ref{lemma:POPO-efficient}, it suffices to observe that Theorem~\ref{theo:POPO} essentially holds with objective (\ref{popo-obj-dpo-r}), i.e.
\begin{theorem}
    By replacing the optimization objective (\ref{popo-obj-dpo}) with (\ref{popo-obj-dpo-r}), with probability $1-\delta$, \texttt{POPO} guarantees that
    \begin{align*}
\text{Reg}_{\text{pref}}(\pi_{\text{sam}}, T)\le {\mathcal{O}}\left( 
\sqrt{dT\log \frac{T}{d}\log\frac{|\mathcal{R}|}{\delta}} + \beta T  C_{\text{KL}}' \right),
    \end{align*}
    where $C_{\text{KL}}' = \max_{r\in \mathcal{R}}\mathbb{E}_{x\sim \rho}\left[ 
\mathbb{D}_{\text{KL}}(\pi_r^\star(\cdot|x)||\pi_{\text{ref}}(\cdot|x))  \right]$.
\label{theo:POPO-r}
\end{theorem}
Theorem~\ref{theo:POPO-r} establishes a preference regret bound that is fundamentally consistent with Theorem~\ref{theo:POPO}, with the only difference being in the KL term.   
In particular, when \(\beta\) is sufficiently small, Theorem~\ref{theo:POPO-r} reduces to Theorem~\ref{theo:POPO} immediately.  
Therefore, assuming \(C_{\text{KL}}'\) is well-bounded, it follows that the reward regret in Theorem~\ref{theo:SE-POPO} remains valid when applying \texttt{POPO} with objective (\ref{popo-obj-dpo-r}).

\section{Experiments}
\begin{table*}[ht]
\caption{Performance comparison across multiple chat benchmarks.}
\vspace{5pt}
\centering
{%
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Model} & \multicolumn{2}{c}{\textbf{IID Data}} & \multicolumn{2}{c}{\textbf{Alpaca Data}} & \multirow{2}{*}{\textbf{AE2 LC}} & \multirow{2}{*}{\textbf{MT-Bench}}& \multirow{2}{*}{\textbf{Avg. Len. (in AE2)}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
 & WR & AvgR & WR & AvgR &  &  \\
\midrule

Llama-3-8B-SFT & - & - & 29.46 & 71.57 & 10.20 & 7.69 & 1182\\ 
\midrule
DPO-iter1 & 62.40 & -4.50 & 78.13 & -6.02 & - & - & 1645\\
DPO-iter2 & 66.59 & -3.59 & 87.14 & -3.34 & - & - & 2045\\
DPO-iter3 & 72.37 & -2.33 & 91.30 & -0.02 & 36.10 & 8.28 & 2257 \\
\midrule
XPO-iter1 & 62.58 & -4.40 & 78.26 & -5.79 & - & - & 1674\\
XPO-iter2  & 67.31 & -3.28 & 88.01 & -2.60 & - & - & 2200 \\
XPO-iter3 & 73.01  & -2.09 & 91.80 & 0.60 & 38.23 & 8.21 &2346 \\
\midrule
SE-POPO-iter1  & 62.54 & -4.32 & 80.00 & -5.68 & - & - & 1797\\
SE-POPO-iter2  & {68.24} & -3.15 & 89.06 & -2.45 & - & - & 2302\\
SE-POPO-iter3  & {\textbf{73.33}} & {\textbf{-2.03}} & {\textbf{92.42}} & {\textbf{0.61}} & {\textbf{40.12}} & {\textbf{8.39}} & 2358\\
\midrule
Llama-3-8B-Instruct  & 48.35 & -6.77 & 87.02 & -3.42 & 22.92 & 8.16 & 1899\\
    Llama-3-405B-Instruct    & - & - & - & - & 39.30  &- & 1988 \\
%GPT-4 Turbo (04/09)     & - & - & - & - & - & 55.02 & 46.12 \\
%    GPT-4o Mini (07/18)   & - & - & - & - & -  & 50.03 & 44.65 \\

\bottomrule
\end{tabular}%
}
\label{tab:comparison}
\end{table*}



\begin{figure*}[ht]
    \centering
    \begin{minipage}[t]{0.36\linewidth}  % [t] 表示 top-align
    \makeatletter
\def\@captype{table}
\makeatother
\vspace{-10pt} 
        \caption{Avg. Reward and Win Rate Comparison.}
        \vspace{10pt}  % 确保顶端对齐，如有需要可使用
        \centering
        \begin{tabular}{l|cc}
        \toprule
        \textbf{Model}  &\textbf{WR} & \textbf{AvgR}  \\
        \midrule
        $(\pi_t, \pi_t)$-iter2       & 86.95 & -3.35  \\
        $(\pi_t, \pi_{\text{ref}})$-iter2 & 86.83 & -4.09 \\
        \midrule
        $(\pi_t, \pi_t)$-iter3       & 91.24 & -2.63 \\
        $(\pi_t, \pi_{\text{ref}})$-iter3 & 89.44 & -0.02\\
        \bottomrule
        \end{tabular}
        \label{tab:2}
    \end{minipage}
    \hspace{0.01\linewidth}
    %----------------------------------
    % 右侧：两张图片
    %----------------------------------
    \begin{minipage}[t]{0.62\linewidth}  % 同样使用 [t]
        \vspace{0pt}
        \centering
        \includegraphics[width=0.48\linewidth]{figures/ab_1_84.png}
        \hspace{0.01\linewidth}
        \includegraphics[width=0.48\linewidth]{figures/ab_2_84.png}
        \caption{Rewards Distribution with Different Samplers.}
        \label{fig:img1}
    \end{minipage}
\end{figure*}

In this section, we provide a comprehensive empirical evaluation of \texttt{SE-POPO} in LLM alignment tasks. There are two primary use cases for LLM alignments in real practices:
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item \textbf{Domain-specific alignment}: This is where the goal is to fine-tune LLMs for a specific type of tasks, e.g. fashion design.
    \item \textbf{Generalist algnment}: This is where the goal is to train a general-purpose question answering AI that could answer a wide variety of questions. This is for instance what GPTs are designed for.
\end{enumerate}

Importantly, in both use cases, the preference feedback during both training and evaluation would have been provided by \textbf{the same oracle}, e.g. human evaluators. In other words, there should not be any distribution shift in the underlying preference model between training and testing. What distinguishes the two use cases is the prompt distribution during training and deployment. For use case 1, the prompts should come from the same domain during both training and deployment, i.e. no distribution shift in the prompt distribution. For use case 2, the prompt distribution between training and testing could be different.

Motivated by the real use cases discussed above, we present three sets of experiments.
For all experiments, 
our implementation build upon the iterative \texttt{DPO} codebase from \cite{dong2024rlhf}, and we use the $3$-iteration online RLHF framework following the setting in \cite{xie2024exploratory}.
Across all three experiments, we use \texttt{Llama-3-8B-SFT} as the base model, \texttt{RLHFlow-ultrafeedback} dataset as the training prompt sets, and \texttt{GRM-Llama3-8B-rewardmodel-ft} as the training preference model. More details about the experiment setup are deferred to Appendix~\ref{appendix-exp}.
The results from the three sets of experiments are shown as three columns in Table 1:
\begin{itemize}
[leftmargin=*,topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item \textbf{``IID data"} refers to the setting where the models are evaluated on a held-out test prompt set that are drawn from the same distribution as the training prompt set, and the responses are evaluated by the same preference model used during training. This is to simulate use case 1.
    \item \textbf{``Alpaca data"} refers to the setting where the models are evaluated on the AlpacaEval 2.0 dataset, but the responses are still evaluated by the same preference model used during training. This is to simulate use case 2.
    \item \textbf{Public benchmarks}: Finally, we also evaluate our algorithm on public benchmarks including AlpacaEval 2.0 and MT-bench shown in Table 1 as well as the academic benchmarks that are deferred to Table 2 in the appendix. These public benchmarks all have one common characteristic: the training and evaluation preference models are different, usually with GPT-4o as the evaluation oracle during testing. As discussed above, such a distribution shift in the preference model between training and testing rarely happen in practice. Thus, we emphasize that \textbf{performances on such benchmarks offer little insight on how well an RLHF algorithm works in practice}. Nevertheless, we include them for completeness due to their wide adoption in prior RLHF research.
\end{itemize}

%\footnote{https://huggingface.co/Ray2333/GRM-Llama3-8B-rewardmodel-ft}.
%for each iteration\footnote{https://huggingface.co/datasets/RLHFlow/ultrafeedback\_iter1, https://huggingface.co/datasets/RLHFlow/ultrafeedback\_iter2, https://huggingface.co/datasets/RLHFlow/ultrafeedback\_iter3}
%\footnote{https://huggingface.co/RLHFlow/LLaMA3-SFT} (refered to as Llama-3-8B-SFT)
%We use base model RLHFlow/LLaMA3-SFT, prompt sets RLHFlow/ultrafeedback\_iter1,2,3 for each iteration, and preference model Ray2333/GRM-Llama3-8B-rewardmodel-ft to simulate human preferences.
% The pseudocode of our implementation and detailed hyperparameters setup are .

\paragraph{Baselines} 
We compare against two baseline algorithms: iterative \texttt{DPO} \cite{dong2024rlhf}, which is the state-of-the-art passive exploration algorithm and \texttt{XPO} \cite{xie2024exploratory} which is the state-of-the-art active exploration algorithm.


\paragraph{Results} As can been seen in Table \ref{tab:comparison},
\texttt{SE-POPO} outperforms both \texttt{DPO} and \texttt{XPO} across all experiment setups.
Moreover, on the public benchmarks, \texttt{SE-POPO} achieves better performance compared to the industry-level 8B model (Llama-3-8B-Instruct) and comparable performance to model with two orders of magnitude more parameters (Llama-3-405B-Instruct).
Beyond instruction-following benchmarks, we also evaluate \texttt{SE-POPO} and the baselines on a suite of academic benchmarks, to demonstrate that our improvements in chat capabilities do not come at an additional expense of reasoning ability compared to other baselines.
The results are deferred to Appendix~\ref{appendix-exp}.
Across the $9$ academic tasks evaluated, our algorithm performs best in $4$, while DPO leads in $3$ and XPO in $2$.
These evaluation results resoundingly support the effectiveness of our algorithm.


\paragraph{Slight length exploitation in \texttt{XPO} and \texttt{SE-POPO}} 
It is worth noting that the length of the responses generated with models trained by \texttt{XPO} and \texttt{SE-POPO} are slightly longer compared to \texttt{DPO}. This makes sense in theory, considering that the exploration term in both \texttt{XPO} loss and (\ref{popo-obj-dpo-r}) encourages minimizing \( \log \frac{\pi(y'|x)}{\pi_{\text{ref}}(y'|x)} \), which inherently incentives models to generate longer responses.
We speculate that using objective (\ref{popo-obj-dpo}) can mitigate this exploitation, as the on-policy term \(\log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} \) in (\ref{popo-obj-dpo}) will encourage \( \pi \) to generate shorter responses, thereby counteracting the effect incurred by \( \log \frac{\pi(y'|x)}{\pi_{\text{ref}}(y'|x)} \).
Unfortunately, we cannot implement the version of \texttt{SE-POPO} with objective (\ref{popo-obj-dpo}) and have to defer a more comprehensive study of this phenomenon to future work.

\paragraph{Ablation study on the impact of sampler $\pi_{\text{sam}}$}
Lastly, we also conduct an ablation study to better understand the impact of samplers. 
We use iterative \texttt{DPO} as the base algorithm and consider two sampling subroutines:
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item both responses are sampled by the policy of the previous iteration, i.e., $x\sim \rho, (y^1, y^2)\sim \pi_{t}(\cdot|x)$;
    \item  one response is sampled from the previous iteration’s policy and one from the initial policy, i.e., $x\sim \rho, y_1\sim \pi_t(\cdot|x),  y^2\sim \pi_{\text{ref}}(\cdot|x)$.
\end{enumerate}
As shown in Table~\ref{tab:2}, we study two metrics: 1). the reward corresponding to the responses produced by the models, 2). the win rate with respect to the base model $\pi_{\text{ref}}$. 
Notice that for both iteration 2 and iteration 3, the difference in win rate between the two sampler settings is relatively small, whereas the discrepancy in average reward is substantial.
In addition, we plot the reward distribution of the model outputs, as illustrated in Figures~\ref{fig:img1}. 
For samplers $(\pi_t, \pi_{\text{ref}})$, the reward distribution remains relatively unchanged between iteration 2 and 3.
In contrast, samplers $(\pi_t, \pi_t)$ demonstrates a more pronounced change in the reward distribution.
These results are consistent with our theoretical intuition in Section 4.1: collecting data by $(\pi_t, \pi_{\text{ref}})$ can result in $\pi_t$ consistently winning, thereby limiting its capacity to acquire new information.
Consequently, the models can only learn a policy that is sufficiently better than $\pi_{\text{ref}}$ (with 86\% and 89\% win rate), but fail to improve any further.

\section{Conclusion}
In this work, we propose \texttt{SE-POPO}, the first practical and provably sample-efficient online exploration algorithm for RLHF with a polynomial dependence on the reward scale. 
In theory, \texttt{SE-POPO} offers a strictly superior sample complexity guarantee compared to existing online RLHF methods, while in practice, \texttt{SE-POPO} is able to match and sometimes outperform existing baselines with either passive or active exploration.

There are several open questions raised by our work.
Future directions include investigating online exploration algorithms with minimal length exploitation \cite{singhal2023long, meng2024simpo}, extending our algorithms to token-level MDP \cite{xie2024exploratory, zhong2024dpo} or multi-turn RLHF settings \cite{shani2024multi, gao2024regressing, xiong2024building}, and supporting general preference models beyond Bradley–Terry model \cite{munos2023nash, swamy2024minimaximalist, ye2024theoretical}.
\newpage

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning.
We study both of the theoretical formulation and the implied practical algorithmic designs.
The proposed algorithms can help better
align the strong LLMs with human value and preference, thus making the LLMs more helpful, controllable and contributing to the welfare of society.



\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

%\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
%\renewcommand{\contentsname}{Contents of Appendix}
%\tableofcontents
%\newpage

\section{More Related Works}
\paragraph{RLHF and RLHF algorithms}
The current RLHF framework was first
popularized by \cite{christiano2017deep}, which served to direct the attention of the deep RL community to the preference-based feedback.
Due to its significant success in LLM alignment \cite{chatgpt, touvron2023llama}, RLHF has gained substantial interest and become one of the prominent research topics in recent years.
The most widely adopted and standard RLHF framework, as described in \cite{ouyang2022training, touvron2023llama}, consists of two primary stages: 1) optimizing a reward model using the preference dataset, and 2) refining the LLM policy using PPO \cite{schulman2017proximal} based on the optimized reward model.
While this RLHF framework has achieved tremendous success in the industry, its adoption by academic and open-source communities is challenging due to the essential limitations of PPO, such as issues with reproducibility \cite{choshen2019weaknesses},  hyperparameters sensitivity \cite{engstrom2020implementation}, and its significant computational resource requirements.
Inspired by the limitations of this two-stage approach, a new line of research focuses on single-stage algorithms, including
to SLiC \cite{zhao2023slic}, DPO \cite{rafailov2024direct}, and its variants, such as IPO \cite{azar2024general}, SPPO \cite{wu2024self},
VPO \cite{cen2024value}, XPO \cite{xie2024exploratory}, and SELM \cite{zhang2024self}. 
These algorithms bypass the reward modeling step and learn a policy by optimizing a designed loss function on the preference dataset directly. 
It is observed that such algorithms are much more stable than PPO and achieve impressive performance on public benchmarks \cite{tunstall2023zephyr, dubois2024alpacafarm, zheng2023judging}.


\section{Proof of Theorem~\ref{theo:POPO}}
This proof is adapted from the proof of Theorem 1 in \cite{cen2024value}.
By the definition of $G$, there is
\begin{align*}
    \text{Reg}_{\text{pref}}(\pi_{\text{sam}}, T)&\le \sum_{t=1}^T [G(r^\star, \pi^\star) - G(r^\star, \pi_t)]
    \\&=  \underbrace{\sum_{t=1}^T [G(r^\star, \pi_\beta^\star) - G(r_t, \pi_t)]}_{\textsc{Term 1}} + \underbrace{\sum_{t=1}^T [ G(r_t, \pi_t)- G(r^\star, \pi_t)]}_{\textsc{Term 2}} + \underbrace{\sum_{t=1}^T [ G(r^\star, \pi^\star)- G(r^\star, \pi_\beta^\star)]}_{\textsc{Term 3}}
\end{align*}
where $\pi_\beta^\star = \arg\max_{\pi\in \Pi} J(r^\star, \pi)$ and $r_t$ represents the reward corresponding by $\pi_t$, i.e., $\pi_t = \arg\max_{\pi\in \Pi} J(r_t, \pi)$.

\paragraph{Bounding \textsc{Term 1}}
Notice that in objective (\ref{popo-obj}), $\pi_t$ is completely dependent on $r_t$.
In this regard, the function $G$ can be considered as a function that depends only on the reward.
By the choice of $r_t$, we have
\begin{align*}
    -\ell(r^\star, \mathcal{D}_{t-1})+\alpha G(r^\star, \pi_\beta^\star)\le -\ell(r_t, \mathcal{D}_{t-1})+\alpha G(r_t, \pi_t),
\end{align*}
thus
\begin{align*}
     G(r^\star, \pi_\beta^\star) - G(r_t, \pi_t)\le \frac{1}{\alpha}[\ell(r^\star, \mathcal{D}_{t-1})- \ell(r_t, \mathcal{D}_{t-1})].
\end{align*}
The following lemma is adapted from Lemma 2 in \cite{cen2024value}.
\begin{lemma}(MLE estimation error \cite{cen2024value, xie2024exploratory})
\label{lemma_MLE}
Let $\delta\in (0,1)$. With probability $1-\delta$, we have 
\begin{align*}
    &\ell(r^\star, \mathcal{D}_{t-1})- \ell(r_t, \mathcal{D}_{t-1})\\
    &\le -\frac{1}{2}\sum_{s=1}^{t-1} \mathbb{E}_{x\sim \rho, (y,y')\sim \pi_s\otimes \pi_{\text{sam}}(\cdot|x)}\left[\left(\mathbb{P}^\star(y\succ y'|x)- \mathbb{P}_{r_t}(y\succ y'|x)\right)^2\right]+ 2\log \frac{|\mathcal{R}|}{\delta}.
\end{align*}
\end{lemma}
Combining the above, it holds that with probability $1-\delta$ that
\begin{align*}
        \textsc{Term 1}\le -\frac{1}{2 \alpha}\sum_{t=1}^T\sum_{s=1}^{t-1}\mathbb{E}_{x\sim \rho, (y,y')\sim \pi_s\otimes \pi_{\text{sam}}(\cdot|x)}\left[\left(\mathbb{P}^\star(y\succ y'|x)- \mathbb{P}_{r_t}(y\succ y'|x)\right)^2\right]+ \frac{2}{\alpha} T\log \frac{|\mathcal{R}|}{\delta}.
\end{align*}

\paragraph{Bounding \textsc{Term 2}}
The proof completely follows that in \cite{cen2024value}.
For completeness, we provide a rewritten version here.
By Assumption~\ref{assumption:2}, we can rewrite \textsc{Term 2} into 
\begin{align*}
    \textsc{Term 2} &=\sum_{t=1}^T \mathbb{E}_{x\sim \rho, y\sim \pi_t(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\mathbb{P}_{r_t}\left(y\succ y'|x\right)-\mathbb{P}^\star\left(y\succ y'|x\right)  \right]\\
    &= \sum_{t=1}^T  \langle\theta_t-\theta^\star ,  \mathbb{E}_{x\sim \rho, y\sim \pi_t(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\phi(x,y,y')  \right] \rangle.
\end{align*}
Denote by 
\begin{align*}
    W_t = \theta_t-\theta^\star,\ X_t = \mathbb{E}_{x\sim \rho, y\sim \pi_t(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\phi(x,y,y')  \right],\  \Sigma_t = \epsilon I+ \sum_{s=1}^{t-1}X_sX_s^\top
\end{align*}
for some $\epsilon>0$, we can decompose \textsc{Term 2} into
\begin{align}
    \textsc{Term 2} &= \sum_{t=1}^T \langle W_t,  X_t \rangle \nonumber\\
    &= \sum_{t=1}^T \langle W_t,  X_t \rangle\mathbbm{1}\big\{\|X_t\|_{\Sigma_t^{-1}}\le 1\big\}+ \sum_{t=1}^T \langle W_t,  X_t \rangle\mathbbm{1}\big\{\|X_t\|_{\Sigma_t^{-1}}> 1\big\}.
    \label{appendix_term2_decompose}
\end{align}
To proceed, we recall the elliptical potential lemma.
\begin{lemma}(\cite{abbasi2011improved}, Lemma 11)
\label{lemma_abbasi2011improved}
    Let $\{X_t\}$ be a sequence in $\mathbb{R}^d$ and $\Lambda_0\in \mathbb{R}^{d\times d}$ a positive definite matrix. Define $\Lambda_t = \Lambda_0+\sum_{s=1}^{t}X_sX_s^\top $, if $\|X_t\|_2\le L$ for all $t$ , there is 
    \begin{align*}
        \sum_{t=1}^T \min\left\{ 1, \|X_t\|_{\Lambda_{t-1}^{-1}}^2  \right\}\le 2(d\log(\text{trace}(\Lambda_0)+TL^2/d)-\log \det(\Lambda_0)).
    \end{align*}
\end{lemma}
Applying this lemma we immediately have
\begin{align*}
    \sum_{t=1}^T \min\left\{ 1, \|X_t\|_{\Sigma_{t}^{-1}}^2  \right\}\le 2d\log \left(1+\frac{4T/d}{\epsilon}\right):=d(\epsilon).
\end{align*}
Now we control the term terms in \ref{appendix_term2_decompose}.
\begin{itemize}
    \item The first term is bounded by
    \begin{align*}
        &\sum_{t=1}^T \langle W_t,  X_t \rangle\mathbbm{1}\big\{\|X_t\|_{\Sigma_t^{-1}}\le 1\big\}\\
        &\le \sum_{t=1}^T \|W_t\|_{\Sigma_t}\|X_t\|_{\Sigma_t^{-1}}\mathbbm{1}\big\{\|X_t\|_{\Sigma_t^{-1}}\le 1\big\}\\
        &\le \sum_{t=1}^T \|W_t\|_{\Sigma_t} 
 \min\left\{1, \|X_t\|_{\Sigma_t^{-1}}\right\}\\
 &= \sum_{t=1}^T \left[\epsilon \|W_t\|^2+ \sum_{s=1}^{t-1} \langle W_t, X_s \rangle^2 \right]^{1/2}\bigg[\min\left\{1, \|X_t\|^2_{\Sigma_t^{-1}}\right\}\bigg]^{1/2}\\
 &\le \left\{\sum_{t=1}^T\left[\epsilon \|W_t\|^2+ \sum_{s=1}^{t-1} \langle W_t, X_s \rangle^2 \right]\right\}^{1/2}\bigg\{\sum_{t=1}^T \min\left\{1, \|X_t\|^2_{\Sigma_t^{-1}}\right\}\bigg\}^{1/2}\\
 &\le \sqrt{d(\epsilon)\epsilon T}+ \sqrt{d(\epsilon)} \left\{\sum_{t=1}^T\sum_{s=1}^{t-1} \langle W_t, X_s \rangle^2 \right\}^{1/2}\\
 &\le \sqrt{d(\epsilon)\epsilon T} + \frac{d(\epsilon)}{2\mu}+ \frac{\mu}{2}\sum_{t=1}^T\sum_{s=1}^{t-1} \langle W_t, X_s \rangle^2,
    \end{align*}
    where the third inequality is due to Cauchy–Schwarz inequality, the fourth inequality is because $\sqrt{a+b}\le \sqrt{a}+\sqrt{b}$, and the last inequality is by Young’s inequality.
\item The second term is bounded by 
\begin{align*}
    \sum_{t=1}^T \langle W_t,  X_t \rangle\mathbbm{1}\big\{\|X_t\|_{\Sigma_t^{-1}}> 1\big\}&\le \sum_{t=1}^T \mathbbm{1}\big\{\|X_t\|_{\Sigma_t^{-1}}> 1\big\}\\
    &\le \sum_{t=1}^T  \min\left\{1, \|X_t\|_{\Sigma_t^{-1}}\right\}\le d(\epsilon).
\end{align*}
\end{itemize}
Summing up the two terms we arrive at
\begin{align*}
    \textsc{Term 2}&\le d(\epsilon)+ \sqrt{d(\epsilon)\epsilon T} + \frac{d(\epsilon)}{2\mu}+ \frac{\mu}{2}\sum_{t=1}^T\sum_{s=1}^{t-1} \langle W_t, X_s \rangle^2.
\end{align*}
By the definition of $W_t$ and $X_s$, there is 
\begin{align*}
    \langle W_t, X_s \rangle^2 &= \mathbb{E}^2_{x\sim \rho, y\sim \pi_s(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\mathbb{P}_{r_t}\left(y\succ y'|x\right)-\mathbb{P}^\star\left(y\succ y'|x\right)  \right]\\
    &\le \mathbb{E}_{x\sim \rho, y\sim \pi_s(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[ \left(\mathbb{P}_{r_t}\left(y\succ y'|x\right)-\mathbb{P}^\star\left(y\succ y'|x\right) \right) ^2 \right].
\end{align*}
Thus,
\begin{align*}
    \textsc{Term 2}&\le d(\epsilon)+ \sqrt{d(\epsilon)\epsilon T} + \frac{d(\epsilon)}{2\mu}+ \frac{\mu}{2}\sum_{t=1}^T\sum_{s=1}^{t-1} \mathbb{E}_{x\sim \rho, y\sim \pi_s(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[ \left(\mathbb{P}_{r_t}\left(y\succ y'|x\right)-\mathbb{P}^\star\left(y\succ y'|x\right) \right) ^2 \right].
\end{align*}
\paragraph{Bounding \textsc{Term 3}}
By the choice of $\pi_t$ in (\ref{popo-obj}), we have $ J(r^\star, \pi^\star)\le J(r^\star, \pi^\star_\beta)$.
This implies that
\begin{align*}
    \mathbb{E}_{x\sim \rho, (y^\star, y)\sim \pi^\star\otimes \pi_\beta^\star(\cdot|x)}\left[ r^\star(x,y^\star) - r^\star(x,y) \right]\le \mathbb{E}_{x\sim \rho, (y^\star, y)\sim \pi^\star\otimes \pi_\beta^\star(\cdot|x)}\left[\beta\log \frac{\pi^\star(y^\star|x)}{\pi_{\text{ref}}(y^\star|x)}  -\beta\log \frac{\pi_\beta^\star(y|x)}{\pi_{\text{ref}}(y|x)} \right].
\end{align*}
The key observation is that for any $y'\in \mathcal{Y}$, there is
\begin{align*}
    r^\star(x,y^\star) - r^\star(x,y) \ge 4[\mathbb{P}^\star(y^\star\succ y'|x)- \mathbb{P}^\star(y\succ y'|x)].
\end{align*}
This is because $y^\star$ is always the best response, which means that $r^\star(x,y^\star) \ge r^\star(x,y)$ for sure. 
Moreover, the gradient of sigmoid function is less than $1/4$, thereby the gap between the preferences is at most $1/4$th of the gap between rewards. 
Using the inequality, we have
\begin{align*}
     \mathbb{E}_{x\sim \rho, (y^\star, y,y')\sim \pi^\star\otimes \pi_\beta^\star\otimes \pi_{\text{sam}}(\cdot|x)}\big[\mathbb{P}^\star(y^\star\succ y'|x)&- \mathbb{P}^\star(y\succ y'|x) \big]\\
     &\le \frac{1}{4} \mathbb{E}_{x\sim \rho, (y^\star, y)\sim \pi^\star\otimes \pi_\beta^\star(\cdot|x)}\left[\beta\log \frac{\pi^\star(y^\star|x)}{\pi_{\text{ref}}(y^\star|x)}  -\beta\log \frac{\pi_\beta^\star(y|x)}{\pi_{\text{ref}}(y|x)} \right]\\
     &\le \frac{1}{4}\mathbb{E}_{x\sim \rho, y^\star\sim \pi^\star(\cdot|x)}\left[\beta\log \frac{\pi^\star(y^\star|x)}{\pi_{\text{ref}}(y^\star|x)} \right]\\
     &= \frac{\beta}{4}\mathbb{E}_{x\sim \rho}\left[ \mathbb{D}_{\text{KL}}(\pi^\star(\cdot|x)||\pi_{\text{ref}}(\cdot|x))\right],
\end{align*}
Thus we have $\textsc{Term 3} \le \mathcal{O}(\beta T\mathbb{E}_{x\sim \rho}\left[ \mathbb{D}_{\text{KL}}(\pi^\star(\cdot|x)||\pi_{\text{ref}}(\cdot|x))\right])$.

\paragraph{Finishing up}
Combining the three terms , with probability $1-\delta$, there is
\begin{align*}
     \sum_{t=1}^T [G(r^\star, \pi^\star) - G(r^\star, \pi_t)]\le \frac{2}{\alpha}T \log \frac{|\mathcal{R}|}{\delta}+ d(\epsilon)+ \sqrt{d(\epsilon)\epsilon T} + \frac{d(\epsilon)}{2\mu}.
\end{align*}
as long as $ \frac{\mu}{2}\le \frac{1}{2 \alpha}$.
Setting $\epsilon=1,  \alpha = \frac{1}{2}\sqrt{\frac{d\log \frac{T}{d}}{T\log \frac{|\mathcal{R}|}{\delta} }}, \mu = 2\sqrt{\frac{T\log \frac{|\mathcal{R}|}{\delta} }{d\log \frac{T}{d}}}$, we finally arrive
\begin{align*}
    \text{Reg}_{\text{pref}}(\pi_{\text{sam}}, T)\le {\mathcal{O}}\left( 
\sqrt{dT\log \frac{T}{d}\log\frac{|\mathcal{R}|}{\delta}} + \beta T  \mathbb{E}_{x\sim \rho}\left[ 
\mathbb{D}_{\text{KL}}(\pi^\star(\cdot|x)||\pi_{\text{ref}}(\cdot|x))  \right] \right),
\end{align*}
which completes the proof.





\section{Proof of Theorem~\ref{theo:SE-POPO}}
For every $k=1,\dots, K$, by Theorem~\ref{theo:POPO}, with probability $1-\delta$, there is 
\begin{align}
        &\mathbb{E}_{x\sim \rho, (y^\star,y,y') \sim \pi^\star\otimes \pi_{\text{sam}}^{k+1} \otimes \pi_{\text{sam}}^k (\cdot|x)}\bigg[\mathbb{P}^\star(y^\star\succ y'|x)-P^\star(y\succ y'|x)  \bigg] \nonumber \\
        &= \frac{\text{Reg}_{\text{pref}}(\pi_{\text{sam}},T)}{T}
        \le {\mathcal{O}}\left( 
\sqrt\frac{{d\log \frac{T}{d}\log\frac{|\mathcal{R}|}{\delta}}}{T} + \beta  C_{\text{KL}} \right)
\label{appendix-proof-of-theorem3.3-1}
    \end{align}
By Lemma~\ref{Pre2Rwd-reduction}, we have
\begin{align}
    &\mathbb{E}_{x\sim \rho, (y^\star,y,y') \sim \pi^\star\otimes \pi_{\text{sam}}^{k+1} \otimes \pi_{\text{sam}}^k (\cdot|x)}\bigg[\mathbbm{1}\{r^\star(x,y)-r^\star(x,y')\le 1\} \left[{r^\star(x,y^\star)-r^\star(x,y)}\right]  \bigg]\nonumber \\
    &
        \le {\mathcal{O}}\left( 
R_{\text{max}}\sqrt\frac{{d\log \frac{T}{d}\log\frac{|\mathcal{R}|}{\delta}}}{T} + \beta   R_{\text{max}}
C_{\text{KL}} \right)
\label{appendix-proof-of-theorem3.3-3}
\end{align}
For notation simplicity, we denote $r^\star(x,y)- r^\star(x,y')$ by $\Delta(x,y,y')$.
To proceed, we note that
\begin{align*}
\mathbbm{1}\big\{\Delta(x,y,y')\le 1\big\}\ge \mathbbm{1}\big\{ \Delta(x,y^\star,y)> \max(R_{\text{max}}-k,1)\big\}\mathbbm{1}\big\{ \Delta(x,y^\star,y')\le \max(R_{\text{max}}-k+1,1)\big\}.
\end{align*}
This is because when $\Delta(x,y^\star,y)> \max(R_{\text{max}}-k,1)$ and $\Delta(x,y^\star,y')\le \max(R_{\text{max}}-k+1,1)$, we have
\begin{align*}
    \Delta(x,y,y') &= \Delta(x,y^\star,y')-\Delta(x,y^\star,y)\\
    &\le \max(R_{\text{max}}-k+1,1)-\max(R_{\text{max}}-k,1)\le 1.
\end{align*}
In this regard, given $r^\star(x,y^\star) - r^\star(x,y)\ge 0$ for sure, we have
\begin{align*}
    &\mathbb{E}_{x\sim \rho, (y^\star,y,y') \sim \pi^\star\otimes \pi_{\text{sam}}^{k+1} \otimes \pi_{\text{sam}}^k (\cdot|x)}\bigg[\mathbbm{1}\{r^\star(x,y)-r^\star(x,y')\le 1\} \left[{r^\star(x,y^\star)-r^\star(x,y)}\right]  \bigg]\\
    &=\mathbb{E}_{x\sim \rho, (y^\star,y,y') \sim \pi^\star\otimes \pi_{\text{sam}}^{k+1} \otimes \pi_{\text{sam}}^k (\cdot|x)}\bigg[\mathbbm{1}\{\Delta(x,y,y')\le 1\} \Delta(x,y^\star,y)  \bigg]\\
    &\ge \mathbb{E}_{x\sim \rho, (y^\star,y,y') \sim \pi^\star\otimes \pi_{\text{sam}}^{k+1} \otimes \pi_{\text{sam}}^k (\cdot|x)}\bigg[ \mathbbm{1}\big\{ \Delta(x,y^\star,y)> \max(R_{\text{max}}-k,1)\big\}\\
    &\qquad \qquad \qquad \qquad  \qquad \qquad  \mathbbm{1}\big\{ \Delta(x,y^\star,y')\le \max(R_{\text{max}}-k+1,1)\big\}\Delta(x,y^\star,y)  \bigg]\\
    &\ge\mathbb{E}_{x\sim \rho, (y^\star,y,y') \sim \pi^\star\otimes \pi_{\text{sam}}^{k+1} \otimes \pi_{\text{sam}}^k (\cdot|x)}\bigg[ \mathbbm{1}\big\{ \Delta(x,y^\star,y)> \max(R_{\text{max}}-k,1)\big\}\\
    &\qquad \qquad \qquad \qquad \qquad \qquad  \qquad \qquad \ \  \mathbbm{1}\big\{ \Delta(x,y^\star,y')\le \max(R_{\text{max}}-k+1,1)\big\}  \bigg]\\
    &\ge\mathbb{E}_{x\sim \rho, (y^\star,y') \sim \pi^\star\otimes \pi_{\text{sam}}^k (\cdot|x)}\bigg[ \mathbbm{1}\big\{ \Delta(x,y^\star,y')\le \max(R_{\text{max}}-k+1,1)\big\}\bigg]\\
    &  \qquad \qquad \qquad \qquad \    - \mathbb{E}_{x\sim \rho, (y^\star,y) \sim \pi^\star\otimes \pi_{\text{sam}}^{k+1} }\bigg[ \mathbbm{1}\big\{ \Delta(x,y^\star,y)\le \max(R_{\text{max}}-k,1)\big\}\bigg].
\end{align*}
The second inequality is because the inner term is non-zero only if $\Delta(x,y^\star,y)> \max(R_{\text{max}}-k,1)\ge 1$.
Combining this with (\ref{appendix-proof-of-theorem3.3-3}), with probability $1-K\delta$, there is
\begin{align*}
    &\mathbb{E}_{x\sim \rho, (y^\star,y) \sim \pi^\star\otimes \pi_{\text{sam}}^{K+1}(\cdot|x) }\bigg[ \mathbbm{1}\big\{ \Delta(x,y^\star,y)\le \max(R_{\text{max}}-K,1)\big\}\bigg]\\
    &\ge \mathbb{E}_{x\sim \rho, (y^\star,y') \sim \pi^\star\otimes \pi_{\text{sam}}^K (\cdot|x)}\bigg[ \mathbbm{1}\big\{ \Delta(x,y^\star,y')\le \max(R_{\text{max}}-K+1,1)\big\}\bigg] - {\mathcal{O}}\left( 
R_{\text{max}}\sqrt\frac{{d\log \frac{T}{d}\log\frac{|\mathcal{R}|}{\delta}}}{T} + \beta   R_{\text{max}}
C_{\text{KL}} \right)\\
    &\ge \mathbb{E}_{x\sim \rho, (y^\star,y') \sim \pi^\star\otimes \pi_{\text{sam}}^1 (\cdot|x)}\bigg[ \mathbbm{1}\big\{ \Delta(x,y^\star,y')\le \max(R_{\text{max}},1)\big\}\bigg] - {\mathcal{O}}\left( K R_{\text{max}}
\sqrt\frac{{d\log \frac{T}{d}\log\frac{|\mathcal{R}|}{\delta}}}{T} + \beta K R_{\text{max}}   
C_{\text{KL}} \right)\\
    &= 1- {\mathcal{O}}\left( K R_{\text{max}}
\sqrt\frac{{d\log \frac{T}{d}\log\frac{|\mathcal{R}|}{\delta}}}{T} + \beta K R_{\text{max}}   
C_{\text{KL}} \right).
\end{align*}
Setting $K=\lceil  R_{\max} \rceil-1$, we achieve that
\begin{align*}
    \mathbb{E}_{x\sim \rho, (y^\star,y) \sim \pi^\star\otimes \pi_{\text{sam}}^{\lceil  R_{\max} \rceil} }\bigg[ \mathbbm{1}\big\{ \Delta(x,y^\star,y)> 1\big\}\bigg]\le  {\mathcal{O}}\left(  R^2_{\text{max}}
\sqrt\frac{{d\log \frac{T}{d}\log\frac{|\mathcal{R}|}{\delta}}}{T} + \beta  R^2_{\text{max}}   
C_{\text{KL}} \right).
\end{align*}
This result implies that
\begin{align*}
    \mathbb{E}_{x\sim \rho, (y,y') \sim \bar \pi\otimes \pi_{\text{sam}}^{\lceil  R_{\max} \rceil} }\bigg[ \mathbbm{1}\big\{ \Delta(x,y,y')> 1\big\}\bigg]\le  {\mathcal{O}}\left(  R^2_{\text{max}}
\sqrt\frac{{d\log \frac{T}{d}\log\frac{|\mathcal{R}|}{\delta}}}{T} + \beta  R^2_{\text{max}}   
C_{\text{KL}} \right)
\end{align*}
for all $\bar \pi$. 
In this regard, it suffices to note that $\pi_{\text{sam}}^{\lceil  R_{\max} \rceil}$ is a ``good enough'' sampler: it can return a response $y'$ such that $\Delta(x,y,y')\le 1$ with high probability.
Denote by $\bar \pi = \texttt{POPO}(\pi_{\text{ref}}, \pi_{\text{sam}}^{\lceil  R_{\max} \rceil}, T)$, with probability $1-\delta$, there is 
\begin{align*}
&\mathbb{E}_{x\sim\rho, (y^\star, y)\sim\pi^\star\otimes\bar \pi(\cdot|x)}\left[r^\star(x,y^\star)-r^\star(x,y)\right]\\
    &=\mathbb{E}_{x\sim\rho, (y^\star, y,y')\sim\pi^\star\otimes\bar \pi\otimes \pi_{\text{sam}}^{\lceil  R_{\max} \rceil}(\cdot|x)}\left[\mathbbm{1}\{\Delta(x,y,y')\le 1\}[r^\star(x,y^\star)-r^\star(x,y)]\right]\\
    &+\mathbb{E}_{x\sim\rho, (y^\star, y,y')\sim\pi^\star\otimes\bar \pi\otimes \pi_{\text{sam}}^{\lceil  R_{\max} \rceil}(\cdot|x)}\left[\mathbbm{1}\{\Delta(x,y,y')>1\}[r^\star(x,y^\star)-r^\star(x,y)]\right]\\
    &\le \mathbb{E}_{x\sim\rho, (y^\star, y,y')\sim\pi^\star\otimes\bar \pi\otimes \pi_{\text{sam}}^{\lceil  R_{\max} \rceil}(\cdot|x)}\left[\mathbbm{1}\{\Delta(x,y,y')\le 1\}[r^\star(x,y^\star)-r^\star(x,y)]\right]\\
    &+R_{\max}\mathbb{E}_{x\sim\rho, (y,y')\sim\bar \pi\otimes \pi_{\text{sam}}^{\lceil  R_{\max} \rceil}(\cdot|x)}\left[\mathbbm{1}\{\Delta(x,y,y')>1\}\right]\\
    &\le {\mathcal{O}}\left(  R^3_{\text{max}}
\sqrt\frac{{d\log \frac{T}{d}\log\frac{|\mathcal{R}|}{\delta}}}{T} + \beta  R^3_{\text{max}}   
C_{\text{KL}} \right).
\end{align*}
Setting $\beta\le o\left(\frac{1}{\sqrt{T}}\right)$ and $T = \tilde{\mathcal{O}}\left( \frac{d R^6_{\max}\log \frac{|\mathcal{R}|}{\delta}}{\epsilon^2}  \right)$, it suffices to say $\bar \pi$ is an $\epsilon$-optimal policy with probability $1-\lceil {R_{\max}} \rceil\delta$.
Therefore, resizing $\delta = \delta/\lceil {R_{\max}} \rceil$, the sample complexity of $\texttt{SE-POPO}$ is 
\begin{align*}
    \lceil {R_{\max}} \rceil T = \tilde{\mathcal{O}}\left( \frac{d R^7_{\max}\log \frac{|\mathcal{R}|}{\delta}}{\epsilon^2}  \right).
\end{align*}
This completes the proof.






\section{Proof of Theorem~\ref{theo:POPO-r}}
In the proof of Theorem~\ref{theo:POPO}, the only place where we use the condition that \( \pi_{t+1} \) is the optimal solution to objective (\ref{popo-obj-dpo}) is in the proof of bounding \textsc{Term 1}. 
Therefore, it suffices to focus on \textsc{Term 1} itself.
By definition, with optimizing objective (\ref{popo-obj-dpo-r}), we have
\begin{align*}
    -\ell(r^\star, \mathcal{D}_{t-1})+\alpha H(r^\star, \pi_\beta^\star)\le -\ell(r_t, \mathcal{D}_{t-1})+\alpha H(r_t, \pi_t),
\end{align*}
Using Lemma~\ref{lemma:POPO-efficient}, it suffices to note
\begin{align*}
&-\ell(r^\star, \mathcal{D}_{t-1})+\alpha G(r^\star, \pi_\beta^\star)-\frac{\alpha\beta}{2}\max_{r\in \mathcal{R}}\mathbb{E}_{x\sim \rho}\left[ 
\mathbb{D}_{\text{KL}}(\pi_r^\star(\cdot|x)||\pi_{\text{ref}}(\cdot|x))  \right]\le
    -\ell(r^\star, \mathcal{D}_{t-1})+\alpha H(r^\star, \pi_\beta^\star)\\
    &-\ell(r_t, \mathcal{D}_{t-1})+\alpha H(r_t, \pi_t)\le -\ell(r_t, \mathcal{D}_{t-1})+\alpha G(r_t, \pi_t)+ \frac{\alpha\beta}{2}\max_{r\in \mathcal{R}}\mathbb{E}_{x\sim \rho}\left[ 
\mathbb{D}_{\text{KL}}(\pi_r^\star(\cdot|x)||\pi_{\text{ref}}(\cdot|x))  \right],
\end{align*}
thus
\begin{align*}
     G(r^\star, \pi_\beta^\star) - G(r_t, \pi_t)\le \frac{1}{\alpha}[\ell(r^\star, \mathcal{D}_{t-1})- \ell(r_t, \mathcal{D}_{t-1})]+\beta \max_{r\in \mathcal{R}}\mathbb{E}_{x\sim \rho}\left[ 
\mathbb{D}_{\text{KL}}(\pi_r^\star(\cdot|x)||\pi_{\text{ref}}(\cdot|x))  \right].
\end{align*}
This completes the proof.

\section{Proof of Lemma~\ref{lemma:POPO-efficient}}
Fix $r\in \mathcal{R}$.
Recall $\pi(r)$ is the optimal solution of the KL-regularized reward objective and $\pi^\star_r = \arg \max_{\pi} \mathbb{E}_{x\sim \rho, y\sim \pi(\cdot|x)} \newline [r(x,y)]$.
By the analysis of bounding \textsc{Term 3} in the proof of Theorem~\ref{theo:POPO}, we first note that 
\begin{align*}
    G(r, \pi_r^\star)- \frac{\beta}{4}\mathbb{E}_{x\sim \rho}[\mathbb{D}_{\text{KL}}(\pi^\star_r(\cdot|x)||\pi_{\text{ref}}(\cdot|x))]\le  G(r, \pi(r)) \le G(r, \pi_r^\star).
\end{align*}
It suffices to focus on $G(r, \pi_r^\star)$.
Then, we have
\begin{align*}
    G(r, \pi_r^\star) &= \mathbb{E}_{x\sim \rho, y\sim \pi_r^\star(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\sigma(r(x,y)- r(x,y'))  \right]\\
    &=\mathbb{E}_{x\sim \rho, y\sim \pi_r^\star(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\sigma\left(\beta \log \frac{\pi_r(y|x)}{\pi_{\text{ref}}(y|x)}- \beta \log \frac{\pi_r(y'|x)}{\pi_{\text{ref}}(y'|x)}\right)  \right]
\end{align*}
where $\pi_r=\pi(r)$.
Since here $y$ represents the response with the highest reward under $r$, it suffices to note that $\pi_r(y|x)\ge \pi_\text{ref}(y|x)$.
In this case, $\beta \log \frac{\pi_r(y|x)}{\pi_{\text{ref}}(y|x)}$ can be bounded by $[0, \beta\log \frac{1}{\pi_{\text{ref}}(y|x)}]$.
By the smoothness of sigmoid function, there is
\begin{align*}
&\mathbb{E}_{x\sim \rho, y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\sigma\left(- \beta \log \frac{\pi_r(y'|x)}{\pi_{\text{ref}}(y'|x)}\right)  \right]\\
    &\le \mathbb{E}_{x\sim \rho, y\sim \pi_r^\star(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\sigma\left(\beta \log \frac{\pi_r(y|x)}{\pi_{\text{ref}}(y|x)}- \beta \log \frac{\pi_r(y'|x)}{\pi_{\text{ref}}(y'|x)}\right)  \right]\\
    &\le\mathbb{E}_{x\sim \rho,y\sim \pi_r^\star(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\sigma\left(- \beta \log \frac{\pi_r(y'|x)}{\pi_{\text{ref}}(y'|x)}\right)  +\frac{\beta}{4}\log \frac{\pi_r(y|x)}{\pi_{\text{ref}}(y|x)}\right]\\
    &\le \mathbb{E}_{x\sim \rho, y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\sigma\left(- \beta \log \frac{\pi_r(y'|x)}{\pi_{\text{ref}}(y'|x)}\right) \right]+\frac{\beta}{4}\mathbb{E}_{x\sim \rho}[\mathbb{D}_{\text{KL}}(\pi^\star_r(\cdot|x)||\pi_{\text{ref}}(\cdot|x))]
\end{align*}
The last inequality is due to $q = \arg\max_p \sum_y q(y)\log p(y)$.
Combining the above we can conclude
\begin{align*}
    \left|G(r,\pi(r))-\mathbb{E}_{x\sim \rho, y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\sigma\left(- \beta \log \frac{\pi_r(y'|x)}{\pi_{\text{ref}}(y'|x)}\right)  \right]\right|\le \frac{\beta}{2}\mathbb{E}_{x\sim \rho}[\mathbb{D}_{\text{KL}}(\pi^\star_r(\cdot|x)||\pi_{\text{ref}}(\cdot|x))].
\end{align*}
This completes the proof.


\section{Proof of Auxiliary Lemmas}
\subsection{Proof of Lemma~\ref{lemma_MLE}}
The proof refers to the proof of Lemma 2 in \cite{cen2024value}.
To begin with, there is 
\begin{align*}
    \ell(r^\star, \mathcal{D}_{t-1}) - \ell(r_t, \mathcal{D}_{t-1}) = -\sum_{s=1}^{t-1}\log \frac{\mathbb{P}_{r^\star}(y_s^+\succ y_s^-|x_s)}{\mathbb{P}_{r_t}(y_s^+\succ y_s^-|x_s)}.
\end{align*}
Define
\begin{align*}
    X_r^s = \log \frac{\mathbb{P}_{r^\star}(y_s^+\succ y_s^-|x_s)}{\mathbb{P}_{r}(y_s^+\succ y_s^-|x_s)}.
\end{align*}
Recall a martingale exponential inequality.
\begin{lemma} (\cite{zhang2023mathematical}, Theorem 13.2)
    Let $\{X_t\}_{t=1}^\infty$ be a sequence of random variables adapted to filtration $\{\mathcal{F}_t\}_{t=1}^\infty$. It holds with probability $1-\delta$ such that for any $t\ge 1$,
    \begin{align*}
        -\sum_{s=1}^t X_s \le \sum_{s=1}^t \log \mathbb{E}[\exp(-X_s)|\mathcal{F}_{s-1}] +\log \frac{1}{\delta}.
    \end{align*}
\end{lemma}
Notice that $\{X_r^t\}_{t=1}^\infty$ is a sequence of random variables adapted to filtration $\{\mathcal{F}_t\}_{t=1}^\infty$ with $\mathcal{F}_t$ given by the $\sigma$-algebra of $\{(x_s, y_s^+, y_s^-): s\le t\}$.
Applying the above lemma and taking a union bound among all $r\in \mathcal{R}$, we have with probability $1-\delta$, for every $r\in \mathcal{R}$ and $t$, there is
\begin{align*}
    -\frac{1}{2}\sum_{s=1}^{t-1} X_r^s &\le \sum_{s=1}^{t-1} \log \mathbb{E}\left[\exp\left(-\frac{1}{2}X_r^s\right)\bigg|\mathcal{F}_{s-1}\right] +\log \frac{|\mathcal{R}|}{\delta}\\
    &\le  \sum_{s=1}^{t-1}  \left(\mathbb{E}\left[ \exp\left(-\frac{1}{2}X_r^s\right)\bigg|\mathcal{F}_{s-1}\right]-1\right) +\log \frac{|\mathcal{R}|}{\delta},
\end{align*}
where the last inequality is due to $\log(1+x)\le x$ for all $x\ge -1$.
To proceed, note that
\begin{align*}
    \mathbb{E}\left[ \exp\left(-\frac{1}{2}X_r^s\right)\bigg|\mathcal{F}_{s-1}\right]
    &\le \mathbb{E}\left[ \sqrt{\frac{\mathbb{P}_{r}(y_s^+\succ y_s^-|x_s)}{\mathbb{P}_{r^\star}(y_s^+\succ y_s^-|x_s)}}  \bigg|\mathcal{F}_{s-1}\right]\\
    &= \mathbb{E}_{x\sim\rho, (y,y')\sim \pi_s\otimes \pi_{\text{sam}}(\cdot|x), (+,-)\sim \mathbb{P}_{r^\star}(\cdot|x,y,y')}\left[ \sqrt{\frac{\mathbb{P}_{r}(y^+\succ y^-|x)}{\mathbb{P}_{r^\star}(y^+\succ y^-|x)}} \right]\\
    & = \mathbb{E}_{x\sim\rho, (y,y')\sim \pi_s\otimes \pi_{\text{sam}}(\cdot|x)}\left[ \sum_{(+,-)}\sqrt{{\mathbb{P}_{r}(y^+\succ y^-|x)}{\mathbb{P}_{r^\star}(y^+\succ y^-|x)}} \right]\\
    & = 1- \frac{1}{2} \mathbb{E}_{x\sim\rho, (y,y')\sim \pi_s\otimes \pi_{\text{sam}}(\cdot|x)}\left[ \sum_{(+,-)}\left(\sqrt{{\mathbb{P}_{r}(y^+\succ y^-|x)}}-\sqrt{{\mathbb{P}_{r^\star}(y^+\succ y^-|x)}}\right)^2 \right]\\
    &\le 1- \frac{1}{8} \mathbb{E}_{x\sim\rho, (y,y')\sim \pi_s\otimes \pi_{\text{sam}}(\cdot|x)}\left[ \sum_{(+,-)}\left({{\mathbb{P}_{r}(y^+\succ y^-|x)}}-{{\mathbb{P}_{r^\star}(y^+\succ y^-|x)}}\right)^2 \right]\\
    &=1- \frac{1}{4} \mathbb{E}_{x\sim\rho, (y,y')\sim \pi_s\otimes \pi_{\text{sam}}(\cdot|x)}\left[ \left({{\mathbb{P}_{r}(y\succ y'|x_s)}}-{{\mathbb{P}_{r^\star}(y\succ y'|x_s)}}\right)^2 \right],
\end{align*}
where the second inequality is due to $|\sqrt{x}-\sqrt{y}|\ge |x-y|/2$ for any $x,y\in [0,1]$.
The last equality is because $|{{\mathbb{P}_{r}(y\succ y'|x_s)}}-{{\mathbb{P}_{r^\star}(y\succ y'|x_s)}}| = |{{\mathbb{P}_{r}(y'\succ y|x)}}-{{\mathbb{P}_{r^\star}(y'\succ y|x)}}|$.
Combining the above, we finally have 
\begin{align*}
    \ell(r^\star, \mathcal{D}_{t-1}) - \ell(r_t, \mathcal{D}_{t-1})\le -\frac{1}{2}\sum_{s=1}^{t-1} \mathbb{E}_{x\sim\rho, (y,y')\sim \pi_s\otimes \pi_{\text{sam}}(\cdot|x)}\left[ \left({{\mathbb{P}_{r_t}(y\succ y'|x)}}-{{\mathbb{P}_{r^\star}(y\succ y'|x)}}\right)^2 \right] +2\log \frac{|\mathcal{R}|}{\delta},
\end{align*}
which completes the proof.


\subsection{Proof of Lemma~\ref{Pre2Rwd-reduction}}
    Assuming $r^\star(x,y)\le r^\star(x,y')+1$.
    In this case, we note that 
    \begin{align*}
        P^\star(y\succ y'|x)= \frac{\exp(r^\star(x,y)-r^\star(x,y'))}{1+\exp(r^\star(x,y)-r^\star(x,y'))}\le \frac{e}{1+e}\le \frac{3}{4}.
    \end{align*}
    Given this, it suffices to focus on the case where $P^\star(y^\star \succ y'|x)\le 4/5$, otherwise
    \begin{align*}
        P^\star(y^\star\succ y'|x)-P^\star(y\succ y'|x)\ge \frac{4}{5}-\frac{3}{4} \ge \frac{r^\star(x,y^\star) - r^\star(x,y)}{20R_{max}}.
    \end{align*}
    Similarly, since $P^\star(y^\star\ge y'|x)\ge 1/2$, it suffices to focus on the case where $P^\star(y \succ y'|x)\ge 9/20$, otherwise
    \begin{align*}
        P^\star(y^\star\succ y'|x)-P^\star(y\succ y'|x)\ge \frac{1}{2}-\frac{9}{20} \ge \frac{r^\star(x,y^\star) - r^\star(x,y)}{20R_{max}}.
    \end{align*}
    In this way, we obtain certain constraints on the preferences $P^\star(y^\star\succ y'|x)$ and $P^\star(y\succ y'|x)$.
This further leads to constraints on the differences in rewards, i.e.,
\begin{align*}
   0 \le r^\star(x,y^\star) - r^\star(x,y')\le \frac{3}{2},\ -\frac{1}{2} \le r^\star(x,y) - r^\star(x,y')\le 1.
\end{align*}
Thus, it suffices to focus on the interval $[-\frac{1}{2}, \frac{3}{2}]$.
It is easily to see that
\begin{align*}
&P^\star(y^\star\succ y'|x)-P^\star(y\succ y'|x)\\
&=\sigma(r^\star(x,y^\star) - r^\star(x,y')) - \sigma(r^\star(x,y) - r^\star(x,y'))\\
    &\ge \min_{\Delta\in \left[-\frac{1}{2}, \frac{3}{2}\right]} \nabla \sigma(\Delta) [r^\star(x,y^\star) - r^\star(x,y')-(r^\star(x,y) - r^\star(x,y'))]\\
    & = \frac{r^\star(x,y^\star)-r^\star(x,y)}{20}.
\end{align*}
Combining the above we have 
\begin{align*}
    r^\star(x,y^\star)-r^\star(x,y)\le 20R_{\text{max}}[P^\star(y^\star\succ y'|x)-P^\star(y\succ y'|x)]
\end{align*}
with $r^\star(x,y)- r^\star(x,y')\le 1$.
This completes the proof.








\section{Generalization beyond linear preference oracle}
\label{appendix:generalization_bl}
In this section, we extend Theorem~\ref{theo:POPO} from the linear preference oracle setting to a more general preference oracle. 
To do this, we introduce a general complexity measure— preference-based generalized eluder coefficient (PGEC)—which aligns with the complexity measures definitions in prior works \cite{xie2024exploratory, zhang2024self}.
\begin{definition}(Preference-based GEC)
\label{def:PGEC}
Given the reward class $\mathcal{R}$, we define the preference-based Generalized Eluder Coefficient (PGEC) as the smallest $d_{\text{PGEC}}$ such that for any sequence of policies $\pi_1,\dots, \pi_T\in \Pi$ and rewards $r_1,\dots, r_T\in \mathcal{R}$
    \begin{align*}
&\sum_{t=1}^T \mathbb{E}_{x\sim \rho, y\sim \pi_t(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\mathbb{P}_{r_t}\left(y\succ y'|x\right)-\mathbb{P}^\star\left(y\succ y'|x\right)  \right]\\
&\le \sqrt{d_{\text{PGEC}} \sum_{t=1}^T\sum_{s=1}^{t-1} \mathbb{E}_{x\sim \rho, y\sim \pi_s(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\left(\mathbb{P}_{r_t}\left(y\succ y'|x\right)-\mathbb{P}^\star\left(y\succ y'|x\right)\right)^2  \right]} +\sqrt{d_{\text{PGEC}}T}
\end{align*}
\end{definition}
The definition of PGEC is an variant of the Generalized Eluder Coefficient (GEC), proposed in \cite{zhong2022gec} Definition 3.4.
Specifically, here $\mathbb{E}_{x\sim \rho, y\sim \pi_t(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\mathbb{P}_{r_t}\left(y\succ y'|x\right)-\mathbb{P}^\star\left(y\succ y'|x\right)  \right]$ denotes the prediction error with respect to the preference, where $\left(\mathbb{P}_{r_t}\left(y\succ y'|x\right)-\mathbb{P}^\star\left(y\succ y'|x\right)\right)^2$ denotes the loss function in the training error.
More details about the coefficient can be found in \cite{zhong2022gec}.
By leveraging Definition~\ref{def:PGEC}, we can extend the proof of Theorem~\ref{theo:POPO} beyond the linear preference oracle. The only required modification is in the proof for bounding \textsc{Term 2}.
Notice that
\begin{align*}
&\textsc{Term 2}=\sum_{t=1}^T \mathbb{E}_{x\sim \rho, y\sim \pi_t(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\mathbb{P}_{r_t}\left(y\succ y'|x\right)-\mathbb{P}^\star\left(y\succ y'|x\right)  \right]\\
&\le \sqrt{d_{\text{PGEC}} \sum_{t=1}^T\sum_{s=1}^{t-1} \mathbb{E}_{x\sim \rho, y\sim \pi_s(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\left(\mathbb{P}_{r_t}\left(y\succ y'|x\right)-\mathbb{P}^\star\left(y\succ y'|x\right)\right)^2  \right]} +\sqrt{d_{\text{PGEC}}T}\\
&\le \frac{d_{\text{PGEC}}}{2\mu} + \frac{\mu}{2}\sum_{t=1}^T\sum_{s=1}^{t-1} \mathbb{E}_{x\sim \rho, y\sim \pi_s(\cdot|x), y'\sim \pi_{\text{sam}}(\cdot|x)}\left[\left(\mathbb{P}_{r_t}\left(y\succ y'|x\right)-\mathbb{P}^\star\left(y\succ y'|x\right)\right)^2  \right]+ \sqrt{d_{\text{PGEC}}T},
\end{align*}
which matches the bound of \textsc{Term 2} in Theorem~\ref{theo:POPO}. 
Hence, with Definition~\ref{def:PGEC}, it suffices to say that \texttt{POPO} guarantees
\begin{align*}
    \text{Reg}_{\text{pref}}(\pi_{\text{sam}}, T)\le {\mathcal{O}}\left( 
\sqrt{d_{\text{PGEC}} T\log \frac{T}{d_{\text{PGEC}}}\log\frac{|\mathcal{R}|}{\delta}} + \beta T  \mathbb{E}_{x\sim \rho}\left[ 
\mathbb{D}_{\text{KL}}(\pi^\star(\cdot|x)||\pi_{\text{ref}}(\cdot|x))  \right] \right),
\end{align*}
which also implies that the sample complexity of \texttt{SE-POPO} can be bounded by $\tilde{\mathcal{O}}\left( \frac{d_{\text{PGEC}} R_{\text{max}}^7\log\frac{|\mathcal{R}|}{\delta}}{\epsilon^2}   \right)$.








\section{Experiments Details}
\label{appendix-exp}

\begin{algorithm}[tb]
\caption{Practical Implementation of \texttt{SE-POPO}}\label{alg:POPO-V1}
\begin{algorithmic}
   \STATE {\bfseries Input:} Reference policy $\pi_{\text{ref}}$, Prompt dataset $\mathcal{D}$, Iterations $T$
   \FOR{$t=1,\dots,T$}
   \STATE Set $\mathcal{D}_t$ as the $t$-th portion of $\mathcal{D}$ and generate $(y^1, y^2) \sim \pi_{\text{ref}}(\cdot|x)$ for each prompt $x\in \mathcal{D}_t$.
    \STATE Annotate responses $(x, y^1, y^2)\to (x, y^w, y^l)$.
    \STATE Optimize
 \begin{align*}
        \pi_{t+1} =\arg\max_{ \pi} \sum_{(x, y_w, y_l)\in \mathcal{D}_t}  \log \sigma  \bigg(\beta \log \frac{\pi(y^w|x)}{\pi_{\text{ref}}(y^w|x)} - \beta \log \frac{\pi(y^l|x)}{\pi_{\text{ref}}(y^l|x)}\bigg) + \alpha  \sum_{(x, y^2)\in \mathcal{D}_t}  \sigma\bigg(- \beta \log \frac{\pi(y^2|x)}{\pi_{\text{ref}}(y^2|x)}\bigg)
    \end{align*}
    \STATE Update $\pi_{\text{ref}}\gets \pi_{t+1}$.

      
   \ENDFOR
\end{algorithmic}
\end{algorithm}


The experiments were conducted on 4 x Nvidia A100 80G GPUs.
%We mainly follow the setting used in \cite{xie2024exploratory, zhang2024self}.
The pseudocode of our algorithm's implementation is illustrated in Algorithm~\ref{alg:POPO-V1}.
In the implementation, we set $\pi_{\text{sam}}=\pi_t$ and use the chosen responses to simulate the on-policy responses. 
To accelerate training, following \cite{dong2024rlhf}, we do not restart from the initial model at each iteration but use the last-iteration model as the initial checkpoint.
Moreover, following \citet{zhang2024self}, we update $\pi_{\text{ref}}=\pi_{t+1}$ for each iteration to avoid performance regression.
For the implementations of \texttt{DPO} and \texttt{XPO}, they differ from Algorithm~\ref{alg:POPO-V1} only in the optimization objectives: \texttt{DPO} does not include the exploration bonus (i.e., $\alpha=0$), while \texttt{XPO} replaces the exploration bonus to $-\alpha \sum_{(x,y^1)\in \mathcal{D}_t} \log \frac{\pi(y^1|x)}{\pi_{\text{ref}}(y^1|x)}$.


For hyperparameters, we mainly follow the settings in \cite{xie2024exploratory} and \cite{zhang2024self}.
We set $\beta=0.1$, use a global batch size of $128$, use a learning rate of $5\times 10^{-7}$ with cosine scheduling.
For exploration coefficient $\alpha$, we employ a decreasing strategy across iterations as in \cite{xie2024exploratory} and do a grid search for $\alpha$ in the first iteration over $\{0.1,0.01,0.001, 0.0001,0.00001\}$.
Based on the empirical performance on AlphcaEval benchmark, we finally select $\{1\times10^{-3}, 5\times10^{-4}, 0\}$ for \texttt{XPO} and $\{1\times10^{-1}, 5\times10^{-2}, 0\}$ for \texttt{SE-POPO} respectively.

For academic benchmarks, following \cite{xie2024exploratory}, we select tasks MMLU \cite{hendrycks2020measuring}, AGIEval \cite{zhong2023agieval}, ANLI \cite{nie2019adversarial}, GPQA \cite{rein2023gpqa}, GSM8K \cite{cobbe2021training}, WinoGrande \cite{sakaguchi2019winogrande}, TruthfulQA \cite{lin-etal-2022-truthfulqa}, ARC Challenge \cite{Clark2018ThinkYH} and HellaSwag \cite{zellers2019hellaswag} as the benchmarks.
The results are proposed in Table~\ref{tab:academic}.
It can be observed that with increasing iterations, both \texttt{SE-POPO} and other baselines may degrade on certain benchmarks, which is known as the alignment tax \cite{askell2021general, noukhovitch2024language, lin2024mitigating}.
Nevertheless, the evaluation result suggests that our method exhibits no additional degradation compared to \texttt{DPO} and \texttt{XPO}, while still effectively improving the base model across most benchmarks.


\begin{table}[h!]
    \centering
    \caption{Performance comparison across academic benchmarks}
    \label{tab:academic}
    \begin{tabular}{lccccccccc}
        \toprule
        \textbf{Model} & \textbf{MMLU} & \textbf{AGIE} & \textbf{ANLI} & \textbf{GPQA} & \textbf{GSM8K} & \textbf{WINOG} & \textbf{TRUTH} & \textbf{ARC} & \textbf{HELLA} \\
        \midrule
Llama-3-8B-SFT & 62.56 & 39.36 & 41.80 & 32.37 & 71.80 & 75.93 & 53.46 & 56.14 & 59.91\\ 
\midrule
        DPO-iter1  & 62.75 & 40.32 & 44.00 & \textbf{32.81} & 76.64 & 76.24 & 56.18 & 55.97 & 79.58 \\
        DPO-iter2  & 63.01 & 41.00 & 44.90 & 30.80 & 77.86 & 76.40 & 57.59 & 55.63 & 80.05 \\
        DPO-iter3  & 63.11 & 41.56 & \textbf{46.90} & 31.25 & 77.55 & 76.16 & \textbf{59.48} & 54.78 & 80.33 \\
        \midrule
        XPO-iter1  & 62.65 & 40.38 & 43.90 & 32.37 & 76.35 & 76.56 & 56.17 & 55.97 & 79.64 \\
        XPO-iter2  & 63.14 & 41.38 & 45.70 & 31.25 & 77.33 & 76.95 & 58.58 & 55.38 & 80.29 \\
        XPO-iter3  & 63.09 & 41.65 & 46.10 & 31.03 & \textbf{78.24} & \textbf{77.19} & 59.43 & 54.95 & 80.43 \\
        \midrule
        POPO-iter1 & 62.80 & 40.45 & 44.00 & 32.37 & 76.80 & 76.00 & 56.21 & \textbf{56.14} & 79.80 \\
        POPO-iter2 & 62.86 & 41.39 & 45.10 & 31.70 & 77.48 & 76.87 & 57.75 & 54.95 & 80.27 \\
        POPO-iter3 & \textbf{63.13} & \textbf{41.68} & 45.60 & 31.92 & 77.63 & 76.63 & 59.14 & 54.35 & \textbf{80.67} \\
        \bottomrule
    \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.




\begin{table}[t]
\centering
{%
\begin{tabular}{l|cc}
\toprule
\textbf{Model}  &\textbf{WR} & \textbf{AvgR}  \\
\midrule
$(\pi_t, \pi_t)$-iter2 & 86.95 & -3.35  \\
$(\pi_t, \pi_{\text{ref}})$-iter2 & 86.83 & -4.09 \\
\midrule
$(\pi_t, \pi_t)$-iter3  & 91.24 & -2.63 \\
$(\pi_t, \pi_{\text{ref}})$-iter3  & 89.44 & -0.02\\
%GPT-4 Turbo (04/09)     & - & - & - & - & - & 55.02 & 46.12 \\
%    GPT-4o Mini (07/18)   & - & - & - & - & -  & 50.03 & 44.65 \\

\bottomrule
\end{tabular}%
}
\caption{Average reward and win rate comparison across different samplers.}
\label{tab:2}
\end{table}

\begin{figure}[ht]
    \centering
    % 第一个图
    \begin{minipage}[t]{0.4\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ab_1_84.png}
    \end{minipage}
    \hspace{0.02\linewidth}
    % 第二个图
    \begin{minipage}[t]{0.4\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ab_2_84.png}
    \end{minipage}
    \hspace{0.02\linewidth}
            \label{fig:img1}
    \caption{Rewards Distribution with Different Samplers}
\end{figure}



%However, in standard online RLHF, the exploration behaves in a \textit{passive} manner, as the sampling policy is not explicitly designed to target OOD regions.
%As a result, even though new data are generated at each iteration, online RLHF may still fail to sufficiently explore the entire prompt-response space.
%This leads to a main question: \textit{how to do exploration that effectively explore the prompt-response space in online RLHF}.
%A central challenge in online RLHF is how to sample responses that effectively explore the prompt-response space. 
%Remarkably, given the nearly infinite space of natural language, online exploration in RLHF cannot be trivially achieved by adapting the exploration strategies from standard online Reinforcement Learning (RL) \cite{azar2017minimax, jin2018q}.
%Recently, 

Compared to standard online Reinforcement Learning (RL), online exploration for RLHF faces two distinctive challenges: 1) given the nearly infinite states in natural language, applying common RL exploration strategy such as using state-based optimism bonus becomes highly inefficient; 
2) considering that the data are preference-annotated rather than reward-labeled, RLHF cannot directly access a reward signal, making it more difficult to identify the optimal response.
For the first challenge, several recent works have addressed it by adding an optimism term in reward optimization to guide policy towards potentially high-reward responses and OOD regions \cite{xie2024exploratory, zhang2024self, cen2024value}.
%As stated in \cite{xie2024exploratory, zhang2024self, cen2024value}, this method offers provably sample-efficient online exploration algorithm for RLHF with general function approximation.
For the second challenge, however, no prior work has attempted to provide further elaboration. 
In particular, existing studies commonly assume that as long as one response is sampled for exploration in each iteration, the resulting preference data will be sufficient to infer rewards and identify the optimal response. 
This assumption does not hold for general preference feedback scenarios. 
For example, consider three responses $a, b, c$ where both \(a\) and \(b\) are always preferred over \(c\). 
If one of the two responses in each preference pair is fixed as \(c\), no matter how the other response is explored, it remains impossible to determine which of \(a\) or \(b\) is superior. 
Consequently, current approaches are restricted to the Bradley-Terry (BT) model, where each preference corresponds directly to a unique reward value. 
Even under these conditions, existing algorithms only perform efficiently when all response preferences remain sufficient distance from the extreme values $0$ or $1$, as their sample complexity scales exponentially with the maximum difference between rewards, reflecting the intrinsic cost of converting preference-based feedback into reward signals.
This leads to a natural and important
question arises: