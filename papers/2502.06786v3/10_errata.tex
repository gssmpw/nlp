
\input{TabsNFigs/Codistil_Old_Table/table1}
\section{Errata}
\label{sec:errata}
In the first draft of the paper, we had a bug and used the following equation to train and quantize our models:
\begin{equation}
\label{eqn:slicing_old}
   S(q^c, r) = \left(\left\lfloor \frac{q^c}{2^{c - r}} \right\rceil\right) * 2^{c - r}
%   \vspace{-3mm}
\end{equation}
Equation~\ref{eqn:slicing_old} clearly allows an extra bucket to be included into the quantization range, i.e, a $r$-bit model would have $2^r + 1$ possible values instead of $2^r$. For example, consider slicing the first two MSBs from an unsigned int8 value, $234$. As per Equation~\ref{eqn:slicing}, $234$ first gets rounded to $4$, following which it gets clipped to $3$, and finally is scaled up to $3 * 64 = 192$ (Note that \alg int2 allows for $0$, $64$, $128$, $192$). However, since the clipping operation is missing in Equation~\ref{eqn:slicing_old}, $4$ is never clipped down to $3$, and $S(q^c, r)$ is now $4 * 64 = 256$ Thus, for certain int2 values in our final quantized model, we will have to store an extra bit. This is the case with int3, int4 and int6 as well where an extra bit is required to represent certain values. In Table~\ref{tab:old_omniquant}, we can see that the fraction of parameters that fall into this extra bucket is very small. However, for our 2-bit models, this additional bucket gives significant improvements in performance, for example, in Table~\ref{tab:old_omniquant} int2 Gemma-2 9B's average downstream accuracy goes up by $5$\% when trained with an additional bucket (referred to as \epalg in Table~\ref{tab:old_omniquant}). This number is further boosted to $6$\% with co-distillation, as evidenced by Table~\ref{tab:codistill_old}. We hypothesize that this additional bucket helps with capturing the outliers and thus leads to a significant performance boost. As highlighted by recent work~\citep{dettmers2023spqr, squeezellm}, it is crucial to store certain outliers full precision. Interestingly, we show that even a single bit is enough to capture several of these outliers, especially for low bit quantization. Finally, note that this performance boost is not very evident in higher precisions where there are enough buckets to account for the outliers.

\input{TabsNFigs/MixnMatch_Fig_old}

\paragraph{Mix'n'Match} As shown in Figure~\ref{fig:omniquant-mnm-old} with a strong int2 model (i.e., 2.050 bits on average), \epalg Mix’n’Match densely spans the Pareto-optimal accuracy-vs-bits-per-FFN-parameter (memory/cost) trade-off for Gemma-2 9B model trained using MatQuant with Omni-Quant – sometimes even improving on the bfloat16 model accuracy. Consequently, hardware supporting only int2 and int4 data types can still accommodate a model with a memory footprint similar to that of an int3 quantized model, and quality comparable or superior to int3; the additional bits required in the case of int2 can be packed into int2/int4. However, custom CUDA kernel would be required to enable sparse additions of these additional bits to the model weights. 

