
\section{Experiments}
\label{sec:exp}
In this section, we present an empirical evaluation of \alg working with two popular learning-based quantization methods: OmniQuant (Section~\ref{sec:exp-omniquant}) and QAT (Section~\ref{sec:exp-qat}). We demonstrate \alg's efficiency on Transformer-based LLMs. Unless otherwise mentioned, our primary focus is on weight only quantization within the parameter-intensive FFN blocks of the Transformer layer.

For our experiments, we chose the default target quantization precisions to be int8, int4, and int2. Furthermore, we showcase the interpolative nature of \alg through evaluations on int6 and int3, as well as its elastic ability to densely span the accuracy-vs-cost trade-off using layer-wise Mix'n'Match (Section~\ref{sec:exp-mnm}). Finally, we ablate on improving the performance of \alg (Sections~\ref{sec:abl-weight} and \ref{sec:abl-codistill}) and extend \alg to the quantization of FFN and Attention parameters. (Section~\ref{sec:spmatquant}). 
Further training and fine-grained evaluation details are in the Appendix.

% \paragraph{Models and Data.} We experiment on the Gemma-2~\citep{Riviere2024Gemma2I} family of models spanning 2B, 9B, 27B parameters and Mistral 7B~\citep{DBLP:mistral}. For OmniQuant experiments, we sample 128 examples with a sequence length of 2048 from the C4 dataset~\citep{raffel2020exploring} and train with a batch size of 4. We train for a total of 10M tokens for all but the int2 baseline where we train the model for 20M tokens~\cite{shao2023omniquant}. For QAT experiments, we sample a fixed set of 100M tokens from the C4 dataset and train all our models with a batch size of 16 and a sequence length of 8192 for a single epoch.
\vspace{-2mm}
\paragraph{Models and Data.} We experiment with Gemma-2~\citep{Riviere2024Gemma2I} 2B, 9B, and Mistral 7B~\citep{DBLP:mistral} models. For OmniQuant experiments, we sample 128 examples with a sequence length of 2048 from the C4 dataset~\citep{raffel2020exploring} and train using a batch size of 4. We train for a total of 10M tokens for all models except the int2 baseline, where we train the model for 20M tokens~\citep{shao2023omniquant}. For QAT experiments, we sample a fixed set of 100M tokens from the C4 dataset and train all our models using a batch size of 16 and a sequence length of 8192 for a single epoch.

% \paragraph{Baselines.} For OmniQuant and QAT, our primary baseline (referred to as ``Baseline'' in the tables and figures) are models trained explicitly for a given bit-width. When interpolating the \alg models for int3 and int6, we do not do any additional training. However, the baselines we compare with are trained explicitly for 3 and 6 bits respectively. We also compare against slicing an int8 OmniQuant (QAT) baseline model (referred to as "Sliced 8-bit" in the tables) to the required bit-width.
\vspace{-2mm}
\paragraph{Baselines.} For OmniQuant and QAT, our primary baselines (referred to as ``Baseline'' in the tables and figures) are models trained explicitly for a given precision. When interpolating the models trained with \alg for int6 and int3, we do not perform any additional training. However, the baselines are trained explicitly for 6 and 3 bits respectively. We also compare against a sliced int8 OmniQuant/QAT baseline model to the corresponding precision (referred to as ``Sliced int8'' in the tables).

% \paragraph{Baselines.} For OmniQuant and QAT, our primary baselines (referred to as ``Baseline'' in the tables and figures) are models trained explicitly for a given precision. When interpolating the \alg models for int6 and int3, we do not perform any additional training. However, the baselines we compare against are trained explicitly for 6 and 3 bits respectively. We also compare against slicing an int8 OmniQuant/QAT baseline model (referred to as ``Sliced int8'' in the tables) to the corresponding precision.

\input{TabsNFigs/QAT_Gemma2B_Gemma9B_Mistral7B_Average_logpplx/table1}
\vspace{-3mm}
\paragraph{Evaluation Datasets.}
Following recent work~\citep{frantar2022gptq, ma2024affinequant}, we evaluate all the methods based on log perplexity and average zero-shot accuracy across a collection of downstream tasks. We use C4's test set to calculate perplexity, and for downstream evaluations, we test on ARC-c, ARC-e~\citep{DBLP:arc}, BoolQ~\citep{DBLP:boolqa}, HellaSwag~\citep{DBLP:hellaswag}, PIQA~\citep{DBLP:piqa}, and Winogrande~\citep{DBLP:winogrande}.


\vspace*{-2mm}
\subsection{\alg with OmniQuant}
\label{sec:exp-omniquant}
Table~\ref{tab:omniquant-ffn} shows the efficacy of \alg when used with FFN-only OmniQuant and compared to explicitly trained OmniQuant baselines for the target precisions, i.e., int8, int4, and int2, across all the models. While the average downstream accuracy of \alg for int8 and int4 quantization is within 0.5\% of the corresponding independently trained baselines, the int2 quantized models of \alg are $1.04\%$, $3.11\%$, and $3.01\%$ more accurate for Gemma-2 2B, 9B, and Mistral 7B, respectively. Similar trends and improvements follow when measuring performance through validation log perplexity. Further, the quantized int4 and int2 models \textit{sliced} from the int8 OmniQuant baseline suffer a significant drop in accuracy around int4, demonstrating that the nested structure of int8 is not well utilized.

% Table~\ref{tab:omniquant-ffn} shows the efficacy of \alg when used with FFN-only OmniQuant and compared to explicitly trained OmniQuant baselines for the target bit-widths ie., int8, int4 \& int2 across all the models. While the average downstream accuracy of \alg for int8 and in4 quantization are within 0.5\% of the corresponding independently trained baselines, the int2 quantized models of \alg are 4.37\%, 8.01\% and 6.35\% more accurate for Gemma-2 2B, 9B and Mistral 7B respectively. Similar trends and improvements follow while measuring performance through validation log perplexity. Further, the quantized int4 and int2 models sliced from the int8 OmniQuant baseline suffer from a significant drop around int4 in accuracy showing that the nested structure of int8 is not well utilized.
\vspace{-2mm}
\paragraph{Sliced Interpolation.}
Beyond the target quantization granularities (int8, int4, and int2), \alg allows for bit-width interpolation to bit-widths not optimized during training. We find that the accuracy of the int6 and int3 models obtained by slicing the \alg models is comparable to their explicitly trained baselines.

% \paragraph{Sliced Interpolation}
% Beyond the target quantization granularities (int8, int4, int2), \alg allows for bit-width interpolation to granulaties not optimized during training. We find that the accuracy of the int6 and int3 models obtained from slicing the \alg models are as good as explictly trained baseline the both bit-widths. 



\subsection{\alg with QAT}
\label{sec:exp-qat}
To further demonstrate the generality of \alg, we experiment on the same models using the popular QAT technique. Following the trend of experimental results with OmniQuant, we show in Table~\ref{tab:qat-ffn} that the models trained using \alg with QAT are comparable to the explicitly trained baselines for all the targeted bit-widths of int8 and int4. However, int2 quantized models using \alg are $4.46\%$, $6.27\%$, and $7.02\%$ more accurate for Gemma-2 2B, 9B, and Mistral 7B, respectively.

\vspace{-2mm}
\paragraph{Sliced Interpolation.}
Models trained using \alg with QAT exhibit strong interpolative performance similar to that of \alg with OmniQuant. We find that the accuracy of the int6 and int3 models obtained by \textit{slicing} the \alg models is comparable to explicitly trained baselines for both interpolated bit-widths.

While OmniQuant only trains the auxiliary parameters needed for quantization, QAT also updates the weight parameters. This potentially results in severe overfitting to the C4 subset used in the experiments. We observe this overfitting in all the experiments presented in Table~\ref{tab:qat-ffn}, where the log perplexities improve for QAT compared to OmniQuant, while the downstream accuracies suffer. This also highlights the need for high-quality data for QAT to realize its benefits; otherwise, users are better off using resource-friendly methods like OmniQuant.

% To further show the generality of \alg, we experiment on the same models using the popular QAT technique. Following the trend of experimental results with OmniQuant, we show that the models trained using \alg with QAT are as good as the independently trained baseline for all the targeted bit-widths of int8 and int4. However, int2 quantized models using \alg are 4.69\%, 6.30\% and 6.34\% more accurate for Gemma-2 2B, 9B and Mistral 7B respectively. 

% \paragraph{Sliced Interpolation}
% Models trained using \alg with QAT exhibit strong interpolative performance similar to that of \alg with OmniQuant. We find that the accuracy of the int6 and int3 models obtained from slicing the \alg models are as good as explictly trained baseline the both bit-widths. 

% While OmniQuant only trains the auxiliary parameters needed for quantization, QAT also updates the weight parameters. This potentially results in severe overfitting to the C4 subset used in the experiments. We notice this overfitting in all the experiments presented in Table~\ref{exp:qat-ffn} where the log perplexities improve for QAT compared to OmniQuant while the downstream acccuracies suffer. This also points to the need of high-quality data for QAT to reap the benefits otherwise users are better-off using resource-friendly methods like OmniQuant.

\input{TabsNFigs/MixnMatch_Fig}
\subsection{Layerwise Mix'n'Match}
\label{sec:exp-mnm}
Alongside the strong slicing-based interpolative properties, quantization with \alg also enables another form of elastic and interpolative behavior through Mix'n'Match. Mix'n'Match provides a mechanism to obtain a combinatorial number of strong models by using different quantization granularities, from the target bit-widths -- i.e., int8, int4, and int2 across layers. Figure~\ref{fig:omniquant-mnm} shows the ability of Mix'n'Match to densely span the accuracy-vs-bits-per-FFN-parameter (memory/cost) trade-off for the Gemma-2 9B model trained using \alg with OmniQuant. While there are many more feasible models, we only showcase the best models obtained through the strategy described in Section~\ref{sec:interpolate} and further expanded in Appendix~\ref{app:td}. Interestingly, the Mix'n'Match model, with a sub-4-bit effective width, is more accurate than the 4-bit sliced model. This opens up possibilities for effective serving depending on hardware support. Section~\ref{sec:dep} continues this discussion in greater depth.

% Alongside the strong slicing based interpolative properties, quantization with \alg also enables another form of elastic and interpolative behaviour through Mix'n'Match. Mix'n'Match provides a mechanism to obtain combinatorial number of strong models by using different quanitzation granularity, from the target bit-widths ie. int8, int4, int2, across layers. Figure~\ref{fig:omniquant-mnm} shows the ability of Min'n'Match to densely span the pareto-optimal accuracy vs bits per FFN parameter trade-off for the Gemma-2 9B model trained using \alg with OmniQuant -- sometimes even improving on the bfloat16 model accuracy. While there are a lot more feasible models, we only showcase the best models obtained through the strategy described in Section~\ref{}. Interestingly, the Mix'n'Match models with effective bit-width of 3 and 6 are as accurate as models obtained with slicing. This opens up possibilities for effective serving depending on hardware support.  








% Describe

% Models: Gemma-2 (2B, 9B, 27B), Mistral 7B

% Dataset and evals, explain log pplx and the chosen downstream datasets. 

% \subsection{Experiments}
% In this section, we present experimental results showcasing the efficacy of MatQuant.

% \paragraph{Scalar Learning Omniquant}

% Plot with pplx and downstream eval number or have table for downstream and pplx plot. repeat for both and add mix'n'match

% Main results, quality neutral everywhere and XX\% gain on 2bit quant

% Mix'n'Match results




% \paragraph{weight +Scalar learning FFN Quant}
% same results. Mention about overfitting and then also mention how this translates to better eventual postrained models

% \paragraph{FFN+Attn Quant}
% Super stabilization for 2 bit. 

% \paragraph{best 2-bit IT model ever}
% only if we can





