% \vspace{-3mm}
\section{Matryoshka Quantization}
\label{sec:method}
% \ak{Aditya: Based method section is fine, refine later, merge last paragraph of RW with the preamble.}

We introduce \alg, a general-purpose, multi-scale training technique that works seamlessly with popular learning-based quantization methods such as Quantization Aware Training (QAT)~\citep{jacob2018quantization} and OmniQuant~\citep{shao2023omniquant}. As long as the model or auxiliary parameters are optimized with gradient descent, \alg's multi-scale training technique can be used across chosen bit-widths, leveraging the inherent nested structure of integer data types. In this section, we will elaborate on the preliminaries behind QAT and OmniQuant, alongside our novel proposed approach, \alg.

% We introduce \alg, a general purpose, multi-scale training technique that works seamlessly with popular learning-based quantization methods like Quantization Aware Training (QAT)~\citep{jacob2018quantization} and OmniQuant~\citep{shao2023omniquant}. As long as the the model or auxiliary parameters are optimized with gradient descent, \alg's multi-scale training technique can be used across chosen bit-widths, leveraging the inherent nested structure in the integer data types. In this section, we flesh out the preliminaries behind QAT and OmniQuant along side our novel proposed approach, \alg.

% \alg is a general purpose training technique that works seamlessly with most of the learning-based quantization methods (Section~\ref{sec:matquant}). As long as the the model/auxiliary parameters are optimized with gradient descent, \alg's multi-scale training technique can be used across chosen bit-widths, leveraging the inherent nested structure in the integer data types. In this work, we combine \alg with OmniQuant and vanilla QAT to demonstrate its efficacy in obtaining a single model that can, at any precision, match or outperform a model explicitly trained for that particular precision  while also providing powerful elastic interpolative properties (Section~\ref{sec:exp}).

% In this paper we will compare MatQuant (our new approach) with Quantized Aware Training (QAT), a popular quantization technique, and OmniQuant (another popular technique).  In this section we introduce QAT, OmniQuant, and our MatQuant quantization framework.
% Describe quantization method. Show the learnable scalars and weights -- write one equation. state that weights are optimal to learn to reduce the training time signifciantly. Then say enabling learning for both is tradtional QAT while only learning scalers leads to data and compute efficient learning. 

% Now describe MatQuant training procedure. Generalation across technqiues, take a few examples and show, but state the default training start is as follows. 

% present mixnmatch algorithm.  

% Analysis of the learnt distrbutions of baselines and MatQuant.

\vspace*{-2mm}
\subsection{Preliminaries}
\subsubsection{Quantization Aware Training}
Quantization Aware Training (QAT) learns a $c$-bit quantized model by optimizing for the end-to-end cross entropy loss using gradient descent. It uses the quantized weights for the forward pass and a straight through estimator (STE)~\citep{bengio2013estimating} to propagate gradients through the quantization operator during the backward pass.

To mathematically formulate QAT, we define MinMax quantization of a real-valued vector $w$ in $c$ bits as follows:
\begin{equation}
\label{eqn:minmax}
\begin{aligned}
Q_{\text{MM}}(w, c) = \text{clamp}\left(\left\lfloor \frac{w}{\alpha} + z\right\rceil, 0, 2^c-1\right) \\
    \alpha = \frac{\max(w) -\min(w)}{2^c-1}, \quad
    z = -\frac{\min(w)}{\alpha}
\end{aligned}
\end{equation}
where $Q_{\text{MM}}(w, c)$ is the $c$-bit quantized version of $w$, $\alpha$ is the scaling factor and $z$ is the zero point.

% Let $W_F$ represent weights of a deep neural network $F(\, \cdot\;W_F)\colon \mathcal{X} \rightarrow \mathbb{R}^d$ where $\mathcal{X}$ denotes the input domain and $d$ is the dimentionality of the output. Suppose we are given a labelled dataset $\mathcal{D}=\{(x_1, y_1), \ldots, (x_N,y_N)\}$ where $x_i\in \mathcal{X}$ is an input point and $y_i \in \mathbb{R}^d$ is the output target of $x_i$ for all $i\in[N]$. With $L_{CE}$ as the cross entropy loss, the optimization goal of QAT is:
% \begin{equation}
%     \min_{W_F} \frac{1}{N}\sum_{i\in [N]}\mathcal{L}_{\text{CE}}\left( F(x_i;Q_{\text{MM}}\left(W_F, c\right)), y_i\right)
% \end{equation}

Let $W_F$ represent weights of a Transformer LLM and let $\mathcal{D}=\{(x_1, y_1), \ldots, (x_N,y_N)\}$ be a labelled dataset where $x_i$ and $y_i$ represent the input and output respectively. With $L_{\text{CE}}$ as the cross entropy loss, the optimization of QAT is:
\begin{equation}
    \min_{W_F} \frac{1}{N}\sum_{i\in [N]}\mathcal{L}_{\text{CE}}\left( F(x_i;Q_{\text{MM}}\left(W_F, c\right)), y_i\right)
\end{equation}
where $F(\cdot)$ represents the LLM's forward pass.

\subsubsection{OmniQuant}
% \ak{Pranav: we should fix this section. QAT is very clean now. }
OmniQuant, unlike QAT, does not update the model parameters. Instead, it learns additional scaling and shifting parameters through gradient descent over layer-wise L2 error reconstruction. These auxiliary parameters aid with quantization. Similar to QAT, OmniQuant also uses a straight through estimator during optimization. However, unlike QAT, OmniQuant operates with limited data, making it much more attractive for resource-scarce settings.

OmniQuant adds two learnable scales, $\gamma$ and $\beta$, to MinMax quantization as follows:
\begin{equation}
\label{eqn:omni_quant}
\begin{aligned}
Q_{\text{Omni}}(w, c) = \text{clamp}\left(\left\lfloor \frac{w}{\alpha} + z\right\rceil, 0, 2^c-1\right) \\
    \alpha = \frac{\gamma\cdot\max(w) -\beta\cdot \min(w)}{2^c-1}, \quad
    z = -\frac{\beta\cdot\min(w)}{\alpha}
\end{aligned}
\end{equation}

OmniQuant also adds another set of learnable shifting and scaling parameters to the FFN's affine projections as follows:
% \vspace{-4mm}
\begin{equation}
    \label{eqn:omni_ffn}
    XW + b \rightarrow \left((X - \delta)\oslash s\right)\cdot Q_{\text{Omni}}(W \odot s) + b + \delta \cdot W
\end{equation}
% \ak{Pranav: what is this equation? We don't ned it I think}
where $X \in \mathbb{R}^{n \times d}$ is the input to the affine transformation, $W \in \mathbb{R}^{d \times d_{\text{o}}}$ is the linear projection associated with the affine transformation, $b \in \mathbb{R}^{d_{\text{o}}}$ is the bias vector, $\delta \in \mathbb{R}^d$ and $s \in \mathbb{R}^d$ are learnable shift and scale parameters respectively.

With the goal of optimizing the layer-wise L2 error (where a layer consists of an Attention block followed by an FFN block), OmniQuant's overall objective can be portrayed
 as follows:
\begin{equation}
    \label{omni_obj}
    \min_{\gamma,\beta,\delta,s} ||F_{l}(W_F^l), X_{l}) - F_{l}(Q_{\text{Omni}}(W_F^l), X_{l})||^2_{2}
\end{equation}
where $F_{l}(\cdot)$ represents the forward pass for a single layer $l$, $W_F^l$ represents the layer parameters and $X_{l}$ represents the layer's input. Note that the above objective is optimized independently for each of the $L$ Transformer layers.

\vspace{-2mm}
\subsection{\alg}
% \ak{Pranav: We need to refine this as well, seems like we are complicating it for no reason.}
\label{sec:matquant}
\alg is a general purpose framework to develop a single model that can do well at any precision. It is a multi-scale training technique that works with most learning-based quantization schemes like QAT and OmniQuant discussed earlier. At its core, taking inspiration from~\citet{kusupati2022matryoshka}, \alg optimizes the quantization loss for several target bit-widths jointly. 

To have a single model for various integer precisions, we nest smaller bit-widths into large ones -- leveraging the inherent Matryoshka nature of the integer data type. So, if we want to extract a $r$-bit model from a $c$-bit model ($0<r<c$), we can just \textit{slice out} the $r$ most significant bits (MSBs) -- using a right shift, followed by a left shift of the same order. Formally, the $S(q^c, r)$ operator slices the most significant $r$ bits from a $c$-bit quantized vector $q^c$:
\begin{equation}
\label{eqn:slicing}
   S(q^c, r) = \text{clamp}\left(\left\lfloor \frac{q^c}{2^{c - r}} \right\rceil, 0, 2^r - 1  \right)  * 2^{c - r}
   \vspace{-3mm}
\end{equation}

Note that $\text{clamp}(\cdot)$ is required to curtail overflows generated by rounding. More details can be found in Appdendix~\ref{app:slicing}. Once we have this structure, we can optimize for several precisions by slicing the MSBs from the largest bit-width we are optimizing for. Let $R = \{r_1, r_2, . . ., r_K\}$ be the bit-widths we want to optimize for, $Q(\cdot,)$ represent the quantization function of the base algorithm (i.e., any learning-based quantization scheme), $\mathcal{L}(\cdot)$ represent the loss function pertaining to the base algorithm, $F(\cdot)$ represent the forward pass required to compute the loss, $\theta$ represent the set of model/auxiliary parameters we are optimizing for and let $W_F$ represent the model parameters. \alg's overall objective can be formulated as follows:
\begin{equation}
\label{eqn:matquant}
    \min_{P} \frac{1}{N} \sum_{i \in [N]} \sum_{r \in R} \lambda_{r}\cdot \mathcal{L}\left(F(S(Q(\theta, c), r), x^{\prime}_i), y^{\prime}_{i}\right)
\end{equation}
where $y^{\prime}_{i} = y_i$ for QAT and $y^{\prime}_{i} =  F_l(W^l_F, X_l^i)$ for OmniQuant, and $x^{\prime}_i = x_i$ for QAT and $x^{\prime}_i = X_l^i$ for OmniQuant. $\lambda_{r}$ is the loss reweighing factor for bit-width $r$.


In this work, we default to training \alg with three bit-widths, $R = \{8, 4, 2\}$, and subsequently perform a linear search over  $\lambda_{r}$. This process aims to optimize performance such that the model performs well across all targeted precision levels. Further, while the focus of this paper is primarily on integer data types, we discuss the possibility of extending \alg to floating-point representations in Section~\ref{sec:fp}.

A key point to note is that \alg primarily alters the quantized weight distributions across precision levels compared to the base quantization algorithm (OmniQuant or QAT). Figure~\ref{fig:teaser}c illustrates the differences in the quantized weight histograms obtained with and without \alg on Gemma-2 9B using OmniQuant. Upon close observation, we find that all the distributions of \alg are shifted to the right; that is, weights quantized with \alg tend to use more higher-valued weights. While this might not significantly impact int8 or even int4 models, int2 models benefit from utilizing more of the possible quantized weights compared to the baseline. Because int2 favors higher-valued weights, this effect propagates to higher-valued weights for int4, and then to int8. This observation highlights the potential overparameterization and freedom in the int8 data type to accommodate the more stringent needs of int2 during joint training. We further explore the effects of this phenomenon in Section~\ref{sec:spmatquant} to develop a better standalone quantization technique for a single target precision.


\input{TabsNFigs/OmniQuant_Gemma2B_Gemma9B_Mistral7B_Average_logpplx/table1}
% \vspace{-4mm}
\subsubsection{Interpolative Behavior}
% \ak{Aditya: Fix this section}
\label{sec:interpolate}

\paragraph{Slicing.}
Although we explicitly train \alg for three precisions (int8, int4, int2), we find that the resulting model, when quantized to interpolated bit-widths like int6 \& int3 by slicing (Eq.~\ref{eqn:slicing}) the int8 model, performs on par with a baseline trained explicitly for that precision. It is also significantly better than slicing an int8 quantized model. We attribute this strong interpolation in bit-width space to \alg, and present more results in Sections~\ref{sec:exp-omniquant} \& \ref{sec:exp-qat}.

\vspace{-4mm}
\paragraph{Mix'n'Match.}
\alg also enables the use of different precisions at different layers through layer-wise Mix'n'Match~\citep{devvrit2023matformer}, even though we never trained for these combinatorial possibilities. These large number of models, obtained at no cost, densely span the accuracy-vs-memory trade-off. We explore several Mix'n'Match strategies and find that having a higher precision (int8) in the middle layers and a lower precision (int2) at the start and end is the most optimal among hundreds of possible models. See Section~\ref{sec:exp-mnm} for detailed experiments.

% \paragraph{Slicing.}
% Although we explicitly train \alg for three precisions (int8, int4, int2), we find that the resulting model, when quantized to interpolated bit-widths like int6 and int3 -- by \textit{slicing} (Eq~\ref{eqn:slicing}) the int8 model -- performs on par with the baseline trained explicitly for that precision and is significantly better than slicing an int8 quantized model. We attribute this strong interpolation in bit-width space to \alg, and  present more results in Sections~\ref{sec:exp-omniquant} \& \ref{sec:exp-qat}.

% \paragraph{Mix'n'Match.}
% \alg also allows for different precisions at different layers through layer-wise Mix'n'Match~\citep{devvrit2023matformer}, despite never training for these combinatorial possibilities. These large number of models, obtained for free, densely span the accuracy-vs-memory trade-off. We explore several Mix'n'Match strategies and find that having a higher precision (int8) in the middle layers and a lower precision (int2) in the start and the end is Pareto-optimal among hundreds of possible models -- see Section~\ref{sec:exp-mnm} for detailed experiments.

% \paragraph{Slicing} 
% Although we explicitly train \alg for three precision levels (int8, int4, int2), we find that the resulting model when quantized to interpolated bit-widths, such as int3 and int6 (by slicing the int8 model, as defined in Equation~\ref{eqn:slicing}), performs on par with the the baseline trained explicitly for that bit-width and is significantly better than slicing a int8 quantized model. This strong interpolation in bit-width space is primarily due to \alg and we present more results in Sections~\ref{sec:exp-omniquant} and \ref{sec:exp-qat}.

% \paragraph{Mix'n'Match}
% \alg also allows for having different precisions at different layers through layer-wise Mix'n'Match~\citep{devvrit2023matformer} despite never training for these combinatorial number of possibilities. These combinatorially large number of models, obtained for free, densely span the accuracy-vs-memory tradeoff. We explore several Mix'n'Match strategies and find that having a higher precision (int8) in the middle layers and a lower precision (int2) in the start and the end is Pareto-optimal among hundreds of possible models -- see Section~\ref{sec:exp-mnm} for detailed experiments. 


