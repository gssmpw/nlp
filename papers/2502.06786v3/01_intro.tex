% \vspace*{-15mm}
\section{Introduction}
\label{sec:intro}
% \ak{Prateek promised}
\input{TabsNFigs/Teaser_Fig}
Due to their impressive performance, there is a strong push to deploy deep learning models, particularly large language models (LLMs)~\citep{team2024gemini,dubey2024llama,achiam2023gpt} in a large number of scenarios. Due to auto-regressive nature of LLMs, decode latency tends to dominate inference cost. Decode latency itself is dominated by communication cost of transferring model weights from high-bandwidth memory (HBM) to the SRAM or due to transferring weights/activations in a distributed cluster. 

% Quantizing weights and/or activations can significantly reduce the overall communication load, and is thus one of the most popular techniques to reduce inference cost~\citep{dettmers2022gpt3}. While floating-point representations are the standard for training, integer data types like int8, int4, and int2 are appealing alternatives for inference. However, current methods for quantizing to these different integer precisions typically treat each target precision as an independent optimization problem, leading to a collection of distinct models instead of a single, versatile one. Furthermore, quantizing to extremely low-precision like int2 is known to be highly inaccurate. 

Quantizing weights and/or activations can significantly reduce the overall communication load and is, therefore, one of the most popular techniques for reducing inference costs~\citep{dettmers2022gpt3}. While floating-point representations are standard for training, integer data types such as int8, int4, and int2 are appealing alternatives for inference. However, current methods for quantizing to these varying integer precisions typically treat each target precision as an independent optimization problem, leading to a collection of distinct models rather than a single, versatile one. Furthermore, quantizing to extremely low precisions like int2 is known to be highly inaccurate. In this work, we pose the question of whether both of the above challenges can be addressed; that is, can we train a single model from which we can extract multiple accurate lower-precision models? We answer this question in the affirmative by introducing Matryoshka Quantization (\alg), a novel multi-scale training method that leverages the inherent nested (Matryoshka) structure~\citep{kusupati2022matryoshka} within integer data types (Figure~\ref{fig:teaser}a). Specifically, \textit{slicing} the most significant bits (MSBs) of an int8-quantized weight can directly yield an int4 or int2 model. Existing quantization techniques often neglect this structure, which limits the potential for multi-scale adaptable models operating at various bit-widths with optimal performance.

%But as Figure~\ref{fig:teaser}b shows, this straightforward approach is suboptimal compared to explicitly quantizing for the target precision. 



Instead, \alg simultaneously optimizes model weights across multiple precision levels (e.g., int8, int4, int2). At a high level, we represent each model parameter at different precision levels using shared MSBs, and then jointly optimize the loss for each precision level. This allows us to develop a single quantized model that can effectively operate at any of the chosen bit-widths, offering a spectrum of accuracy-vs-cost options. \alg is a general-purpose technique, applicable to most learning-based quantization methods, such as Quantization Aware Training (QAT)~\citep{jacob2018quantization} and OmniQuant~\citep{shao2023omniquant}.

%The relentless pursuit of efficient deployment for resource-intensive deep learning models, particularly large language models (LLMs)~\citep{team2024gemini,dubey2024llama,achiam2023gpt}, has made model quantization ubiquitous~\citep{dettmers2022gpt3}. Quantization usually reduces the precision of model parameters, which offers a pathway to significant memory savings, faster computation, and reduced energy consumption. While floating-point representations are the standard for training, integer data types like int8, int4, and int2 are appealing alternatives for inference. However, current methods for quantizing to these different integer precisions typically treat each target precision as an independent optimization problem, leading to a collection of distinct models instead of a single, versatile one.

%In this paper, we bring in a unique perspective on integer quantization by recognizing the inherent nested (Matryoshka) structure~\citep{kusupati2022matryoshka} within the integer data type. Specifically, the bit representation of, say, int8 inherently contains within its most significant bits (MSBs) the necessary information for constructing int4 and int2 models. Simply put, \textit{slicing} the top bits of an int8 quantized weight can directly yield an int4 or int2 model. But as Figure~\ref{fig:teaser}b shows, this straightforward approach is suboptimal compared to explicitly quantizing for the target precision. Existing quantization techniques often neglect this structure, which limits the potential for multi-scale adaptable models operating on various bit-widths with optimal performance.

%To address this, we propose Matryoshka Quantization (\alg), a novel multi-scale training method that explicitly leverages the nested structure of integer data types (Figure~\ref{fig:teaser}a). \alg optimizes model weights across multiple precision levels (e.g., int8, int4, int2) simultaneously within a single training process. This allows us to develop a single quantized model that can effectively operate at any of the chosen bit-widths, offering a spectrum of accuracy-vs-cost options. \alg is a general-purpose technique, applicable to most learning-based quantization methods, such as Quantization Aware Training (QAT)~\citep{jacob2018quantization} and OmniQuant~\citep{shao2023omniquant}.

We demonstrate the efficacy of \alg when applied to quantizing the Feed-Forward Network (FFN) parameters of standard LLMs (Gemma-2 2B, 9B, and Mistral 7B)~\citep{vaswani2017attention} -- typically, FFN is the main latency block hence the focus on improving the most significant component's latency. Our results show that \alg produces int8 and int4 models with comparable accuracy to independently trained baselines, despite the benefit of shared model parameters. Critically, the int2 models generated by \alg significantly outperform their individually trained counterparts, with $4$\% higher accuracy on downstream tasks (Figure~\ref{fig:teaser}b). We also extend \alg to quantize all weights of a Transformer layer.  In Figure~\ref{fig:teaser}c, we find that quantizing with \alg shifts the quantized weight distribution toward higher values, contributing to improved int2 performance. Finally, in Section~\ref{sec:errata}, we also demonstrate that using an extra bit to represent outliers significantly boosts the performance for our sliced int2 models.

Beyond improving chosen precision performance, \alg allows for seamless extraction of interpolative bit-widths, such as int6 and int3. \alg also admits a dense accuracy-vs-cost trade-off by enabling layer-wise Mix'n'Match of different precisions. Therefore, even if the hardware only supports int4 and int2, it's possible to serve models at various effective precisions, tailored to the deployment environment. Overall, \alg and its variants present a significant step toward developing multi-scale models with high flexibility and performance, pushing the boundaries of low-bit quantization for efficient LLM inference.


% This ensures deployment of say an effective int3 sized model even if the underlying hardware only supports int4 and int2

% The relentless pursuit of efficient deployment for resource-intensive deep learning models, particularly large language models (LLMs)~\citep{team2024gemini,dubey2024llama,achiam2023gpt}, made model quantization ubiquitous~\citep{dettmers2022gpt3}. Quantization usually reduces the precision of model parameters offers a pathway to significant memory savings, faster computation, and reduced energy consumption. While floating-point representations are the standard for training, integer data types like int8, int4, and int2 emerge as appealing alternatives for inference. However, current methods for quantizing to these different integer precisions typically treat each target precision as an independent optimization problem, leading to a collection of distinct models instead of a single, versatile one.

% In this paper, we bring in a unique perspective on integer quantization by recognizing the inherent nested (Matryoshka) structure~\citep{kusupati2022matryoshka} within the integer data type. Specifically, the bit representation of, say, int8 inherently contains within its most significant bits (MSBs) the necessary information for constructing int4 and int2 models. Simply put, \textit{slicing} the top bits of an int8 quantized weight can directly yield an int4 or int2 model. But as Figure~\ref{fig:teaser}b shows, this straightforward approach is suboptimal compared to explicitly quantizing for the target precision. Existing quantization techniques often neglect this structure, which limits the potential for multi-scale adaptable models operating on various bit-widths with optimal performance.

% To address this, we propose Matryoshka Quantization (\alg), a novel multi-scale training method that explicitly leverages the nested structure of integer data types (Figure~\ref{fig:teaser}a). \alg optimizes model weights across multiple precision levels (e.g., int8, int4, int2) simultaneously within a single training process. This allows us to develop a single quantized model that can effectively operate at any of the chosen bit-widths, offering a spectrum of accuracy-vs-cost options. \alg is a general-purpose technique, applicable to most learning-based quantization methods, such as Quantization Aware Training (QAT)~\citep{jacob2018quantization} and OmniQuant~\citep{shao2023omniquant}.

% In practice, we demonstrate the efficacy of \alg when applied to quantizing the Feed-Forward Network (FFN) block parameters of Transformer-based LLMs (e.g., Gemma-2 2B, 9B, and Mistral 7B)~\citep{vaswani2017attention}. Our results indicate that \alg produces int8 and int4 models with accuracies comparable to independently trained baselines, with the added benefit of shared model parameters. Critically, the int2 models generated by \alg significantly outperform their individually trained counterparts, achieving up to 8\% better average accuracy on downstream tasks. We extend \alg to quantizing all weights of a Transformer layer and perform ablation studies to improve the technique. Further, we also find that quantizing with \alg shifts the quantized weight distribution toward higher values, which assists with improved int2 performance given the robustness of int8 and int4 models (Figure~\ref{fig:teaser}c).

% Beyond simply improving chosen precision performance, \alg allows for seamless extraction of interpolative bit-widths, such as int6 and int3, further enhancing the model's adaptability, and opens the door for a dense accuracy-vs-cost pareto-optimal trade-off by enabling layer-wise Mix'n'Match of different precisions. Overall, \alg presents a significant step toward developing multi-scale models with high flexibility and performance, pushing the boundaries of low-bit quantization for efficient large language model inference.

% \ak{talk about the staistics}

% If there is something that is naturally nested ie., has matryoshka structure, that would be the bit representation of the int8 datatype. From a int8 weight quantized model, obtaining int4 quanitzed model means reading the 4 MSBs of the int8 model, and the same goes for obtaining a int2 model. However, despite the structure being baked into the bit representation, the modern quantization schemes for int8 do not lead to the best the best quantized int4, int2 models be it data-free or data-driven. A simple bit-specific learned quantization baseline outperforms both the int4 and int2 models obtained from a int8 quantized model. Especially, the int2 shows an improvement of at least X\% compared to naive extraction -- need to write this well. 

% Having looked at the headroom and with a predisposition to the natural nested structure of a int8 quantized model. We introduce Matryoshka Quantization, to induces the natural flexibitly into quantized models. \alg encourages the int8 quantized model weights to also have the best possible int4 and int2 slices present within its bit representation. \alg is a a general purpose training technique that relies on multi-resolution, matrysohka -style, training of the targetted bit-widths in a single training run in a single model. \alg is broadly applicable to any learning-based quantization technqiue and works across models scales effectively. While in8 and int4 models are quality neutral, the int2 models can be upto XX\% more accurate than respective SOTA baselines. 

% Further talk about generality and results. 

% Teaser figure with the bit structure and the baseline numbers. 



% Quantization is hard -- ideally a int 8 should have int4 and int2 within itself owing to the byte strcuture, however, the simple slicing results in significant drop in accuracy compared to explicit training. Baseline vs Slicing

% However, we introduce \alg a new training paradigm to enable true nested precision in a int8 quantized model. Along side training int8, training int4 and int2 results in state of the art int8, int4 and int2 models with int2 models being upto 10\% better than the baselines. 
