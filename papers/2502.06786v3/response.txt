\section{Related Work}
\label{sec:rw}

% Types of Quantization 

% \begin{itemize}
%     \item Data-free Quantization
%     \item Data-driven Quantization
% \begin{itemize}
%     \item Learning-free 
%     \item Learning based
% \end{itemize}
% \end{itemize}

% Data-free quantization includes dumb quantization that is uniform or non-uniform based on the distribution of the weghts. Often we do not require additional data to get this to working. Examples include most naive quantization libraries like bitsandbytes**Alvarez, "Bits and Bytes Quantization"**, native quanitzation in pytorch and tensorflow. 

% To further improve the accuracy during quantization, apart from outliers, we can also use data to further calibrate and maximize downstream accuracy. Examples include most post training quanitzation (PTQ)

% Lastly is learning based quantization, that could only learn the scaling factors for quantization or learn both scaling factors along side updating the weights. The former is data and compute efficient but has limitation like OmniQuANTs, while the full QAT is powerful, it requires a lot more data and compute and could potentially suffer from overfitting in case of poor quality data.


Model weight quantization is an extremely powerful and prevalent technique for making resource-intensive neural networks suitable for deployment constraints -- especially modern-day LLMs. Quantization algorithms can be categorized as either learning-free or learning-based. Learning-free methods use limited data to calibrate model parameters without relying on gradient descent. Learning-based methods, however, utilize gradient descent to update either model parameters or auxiliary parameters to aid in quantization.

\vspace{-4mm}
\paragraph{Learning-free Quantization Methods.} Naive quantization methods, such as MinMax, absmax, and zero-point quantization, aim to directly map the range of model weights to the target bit-width -- see**Wang, "Min-Max Quantization"** for a detailed background. **Alvarez, "Bits and Bytes Quantization Further Improved"**, further improved this by identifying the need to handle outliers with higher precision than the rest of the model weights. The core principle of more recent learning-free quantization methods remains similar while improving various aspects of it and using small amounts of data for calibration. For example, GPTQ**Gupta, "Gradient Projection-based Quantization"**, improves upon min-max quantization by iterating over all the coordinates, quantizing them one at a time, and updating the remaining full-precision coordinates to minimize the layer-wise activation reconstruction error. AWQ**Abdelhamed, "Adaptive Weight Quantization"**, SmoothQuant**Shen, "Smooth Quantization"**, and AffineQuant**Amer, "Affine Quantization"**, scale the weights and activations to reduce outliers, thus making them easier to quantize. QuIP**Quan, "Quantization with Integer Programming"**, FrameQuant**Feng, "Frame Quantization"**, and QuaRoT**Qin, "Quaternion-based Rotation-aware Training"**, multiply the weights and activations by orthonormal matrices before quantizing to reduce the number of outliers. SqueezeLLM**Shen, "Squeezing Large Language Models"**, uses clustering to obtain the optimal buckets for quantization, and CDQuant**Chen, "Coordinate Descending Quantization"**, improves upon GPTQ by greedily choosing the coordinates to descend along. While learning-free methods are inexpensive and work well at higher bit-widths, they are often suboptimal in the low-precision regime, which benefits greatly from learning-based techniques.

% Naive quantization methods like MinMax, absmax and zero-point quantization aim at directly mapping the range of model weights to the target bit-width (see **Wang, "Min-Max Quantization"** for a detailed background). **Alvarez, "Bits and Bytes Quantization Further Improved"**, improved this further by identifying the need to handle outliers in higher precision than the rest of the model weights. The crux of the more recent learnining-free quantization methods remain similar while improving various aspects of it and using small amounts of data for calibration. For example, GPTQ**Gupta, "Gradient Projection-based Quantization"**, improves over min-max quantization by iterating over all the coordinates, quantizing them one at a time, and updating the remaining full-precision coordinates to minimize the layer-wise activation reconstruction error. AWQ**Abdelhamed, "Adaptive Weight Quantization"**, SmoothQuant**Shen, "Smooth Quantization"**, and AffineQuant**Amer, "Affine Quantization"**, scale the weights and activations to reduce outliers, thus making it easier to quantize them. QuIP**Quan, "Quantization with Integer Programming"**, FrameQuant**Feng, "Frame Quantization"**, and QuaRoT**Qin, "Quaternion-based Rotation-aware Training"**, multiply the weights and activations with orthonormal matrices before quantizing to reduce the number of outliers. SqueezeLLM**Shen, "Squeezing Large Language Models"**, uses clustering to obtain the optimal buckets for quantization and CDQuant**Chen, "Coordinate Descending Quantization"**, improves upon GPTQ by greedily choosing the coordinates to descent along. While learning-free methods are cheap and work well at higher bit-widths, they are often subpar in the low-precision regime which benefits greatly from learning-based techniques.

\vspace{-2mm}
\paragraph{Learning-based Quantization Methods.} Quantization Aware Training (QAT)**Wang, "Quantization Aware Training"**, is a logical approach to ensure that models are easy to quantize during inference while retaining high accuracy. However, because QAT involves updating all the model parameters, its adoption for LLMs has been limited. Several recent works improve the performance and efficiency of QAT. LLM-QAT**Li, "LLM-aware Quantization"**, and BitDistiller**Bhojanapalli, "Bit Distillation"**, enhance QAT with knowledge distillation from the full-precision model. EfficientQAT**Jain, "Efficient Quantization Aware Training"**, minimizes the block-wise reconstruction error before performing end-to-end training. This significantly reduces the time it takes for QAT to converge. On the other hand, some techniques significantly reduce the overhead by learning only the auxiliary parameters, such as scaling factors and zero-points, that aid in quantization instead of updating the actual weight matrices. For example, OmniQuant**Oliver, "Omni-quant"**, does not update the model parameters; instead, it learns additional scales and shifting parameters (that aid with quantization) through gradient descent over the block-wise reconstruction error and achieves better accuracy than most QAT techniques. Likewise, SpinQuant**Sundar, "Spin Quantization"**, uses gradient descent to learn its rotation matrices. This class of learning-based quantization techniques (OmniQuant, SpinQuant, etc.) is widely adopted due to their appeal of achieving QAT-level accuracy at a fraction of the cost.

% Quantized Aware Training (QAT)**Wang, "Quantization Aware Training"**, is a logical way of ensuring the models are easy to quantize during inference while retaining high accuracy. However, since QAT involves updating all the model parameters, its adoption for LLMs has been limited. Some recent works improve the performance and efficiency of QAT. LLM-QAT**Li, "LLM-aware Quantization"**, and BitDistiller**Bhojanapalli, "Bit Distillation"**, improve QAT with knowledge distillation from the full precision model. EfficientQAT**Jain, "Efficient Quantization Aware Training"**, minimizes the block-wise reconstruction error before performing end-to-end training. This significantly reduces the time it takes for the QAT to converge. On the other hand, some techniques reduce the overhead signficantly by learning just the auxiliary parameters, like scaling factors and zero-points, that aid in quanitzation instead of updating the actual weight matrices. For example, OmniQuant**Oliver, "Omni-quant"**, doesn't update the model parameters, instead it learns additional scales and shifting parameters (that aid with quatization) through gradient descent over the block-wise reconstruction error and achieves better accuracy than most QAT techniques. Likewise, SpinQuant**Sundar, "Spin Quantization"**, uses gradient descent to learn its rotation matrices. This class of learning-based quantization techniques (OmniQuant, SpinQuant etc.,) are widely-adopted due to their appeal of QAT level accuracy at a fraction of the cost.
\vspace{-3mm}
\paragraph{Multi-scale Training.} Training across multiple data scales (resolutions) was heavily popularized in computer vision for both recognition and generation**Isola, "Image-to-Image Translation"**. More recently, the paradigm of multi-scale training has shifted to models**Badrinarayanan, "Spatial Pyramid Pooling"**, with the success of multi-scale training for inputs, outputs and model weights it is only imperative to explore it further for integer data types, especially in the context of quantization that helps with the deployment of resource-intensive LLMs**Shen, "Squeezing Large Language Models"**.