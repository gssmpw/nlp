\vspace*{-2mm}
% \input{TabsNFigs/Older_OmniQuant_main_paper_evals/table2}
\input{TabsNFigs/Older_OmniQuant_main_paper_evals/table1}

\section{Conclusions}
\label{sec:conc}
% \vspace*{-2mm}
In this work, we presented \alg, a novel multi-scale training technique that leverages the nested structure of integer data types to simultaneously optimize model weight quantization across multiple precisions (int8, int4, and int2) within a single model. This general-purpose method, applicable to learning-based quantization techniques like OmniQuant and QAT, produces models with comparable accuracy to baselines for int8 and int4, while achieving significant improvements, up to $7\%$ for int2 models. \alg further enables bit-width interpolation and layer-wise mix-and-match for flexible accuracy-cost trade-offs, promising more efficient deployment of large models across various hardware settings. Finally, \alg also helped discover \spalg, which significantly improves standalone low-bit quantization.