\section{Particulars of the Slicing Operation.}
\label{app:slicing}
To extract a $r$-bit model from a $c$-bit model, we start by slicing out the most significant $r-1$ bits. We use $1$ for the $r^{\text{th}}$ bit if the $(r+1)^{\text{th}}$, else, we use $0$. This is captured by the round function in Equation~\ref{eqn:slicing} and is done to push values to higher buckets as we expect them to be more informative~\citep{wanda}. For example, consider the the unsigned int8 value $53$. The first two MSBs are $0$s. Naively slicing them would round down $53$ to $0$, however, we want to round it up to $1$. Since the bit corresponding to $32$ is set, i.e., the $(r+1)^{\text{th}}$ MSB, instead of rounding $53$ down to $0$, we round it up to $1$. \\
The $\text{clamp}(\cdot)$ operation is also equally important. The rounding operation in Equation~\ref{eqn:slicing} will round $240$ down to $4$, however, unsigned int2 operates with only $0, 1, 2, 3$. $\text{clamp}(\cdot)$ here would make sure that $4$ is clamped down to $3$.

\section{Addition Training Details}
\label{app:td}
We run all our experiments on TPUv5e chips. For OmniQuant experiments, we use a constant learning rate of $1e-3$ and for QAT experiments, we linearly warmup the learning rate to $1e-5$ for 150 and use a consine decay schedule thereafter. For OmniQuant experiments, we sample 128 examples with a sequence length of 2048 from the C4 dataset~\citep{raffel2020exploring} and train using a batch size of 4. We train for a total of 10M tokens for all models except the int2 baseline, where we train the model for 20M tokens~\citep{shao2023omniquant}. For Co-distillation experiments where OmniQuant is the base algorithm, we train for a total of 8.3M tokens. For QAT experiments, we sample a fixed set of 100M tokens from the C4 dataset and train all our models using a batch size of 16 and a sequence length of 8192 for a single epoch. For Attn + FFN experiments with QAT, we sample a fixed set of 300M tokens from C4 and train with a batch size of 16 for a single epoch.
We use $(\lambda_8, \lambda_4, \lambda_2) = (0.1, 0.1, 1.0)$ for all our Gemma experiments unless otherwise stated. In the case of Mistral 7B,  for OmniQuant experiments, we use $(\lambda_8, \lambda_4, \lambda_2) = (0.4, 0.4, 1.0)$, and for QAT experiments we use $(\lambda_8, \lambda_4, \lambda_2) = (0.2, 0.2, 1.0)$. For all our \epalg experiments, we use $(\lambda_8, \lambda_4, \lambda_2) = (1.0, 1.0, 1.0)$.

\paragraph{Mix'n'Match} For a fixed effective bits-per-FFN layer, where each layer was quantized to either int2, int4, or int8, we explored four different quantization strategies: Pyramid, Reverse Pyramid, Increasing, and Decreasing. In the Pyramid strategy, the initial and final layers were quantized to int2, the central layers to int8, with int4 serving as an intermediate step. The Reverse Pyramid strategy followed the opposite approach, assigning int8 to the initial and final layers, int2 to the central layers, and int4 in between. The Increasing and Decreasing strategies assigned bit precision in ascending and descending order, respectively, across the layers. Our experimental results demonstrated that, for a given effective bits per FFN layer, the Pyramid strategy consistently outperformed the others. Allocating higher precision (int8) to the middle layers helped preserve critical information, while the initial and final layers performed adequately with lower bit precision (int2 and int4), leading to a more efficient and effective quantization scheme.

\section{Detailed Downstream Evaluations for OmniQuant and QAT}
Tables~\ref{tab:down_omni_2B}, \ref{tab:down_omni_9B}, \ref{tab:down_omni_7B}, \ref{tab:down_qat_2B}, \ref{tab:down_qat_9B}, and \ref{tab:down_qat_7B} present downstream evaluation results on Gemma-2 2B, Gemma-2 9B and Mistral 7B with OmniQuant and QAT.



\section{Detailed Downstream Evaluations for \alg Re-weighting}
Tables~\ref{tab:reweighting_2B}, \ref{tab:reweighting_9B}, and \ref{tab:reweighting_7B} present downstream evaluation results for OmniQuant reweighting experiments on Gemma-2 2B, Gemma-2 9B and Mistral 7B.




\section{Detailed Downstream Evaluations for Co-Distillation}
Tables~\ref{tab:omni_codistill} and \ref{tab:qat_codistill} present the downstream evaluation and perplexity results for \alg with co-distillation on Gemma-2 9B. We present results with both, OmniQuant and QAT as the base algorithms. 


\section{Detailed Evaluations for FFN + Attention Quantization}
Tables~\ref{tab:ffn_attn_9B} and \ref{tab:ffn_attn_7B} present the downstream evaluation and perplexity results for FFN + Attention quantization on Gemma-2 9B and Mistral 7B with OmniQuant and QAT.



\section{Detailed Evaluation for \spalg}
\label{app:spalg}
Tables~\ref{tab:sp_2B}, \ref{tab:sp_9B_omni}, \ref{tab:sp_9B_qat}, and \ref{tab:sp_7B} present the downstream evaluation results comparing \spalg to \alg and the \textit{Baseline} for int2 quantization of Gemma-2 2B, Gemma-2 9B and Mistral 7B with OmniQuant and QAT. Since \spalg slices 2 bits from an 8-bit model and computes loss only over the first two bits, we can evaluate the \spalg model trained for 2-bits on int4 and int8. Downstream evaluation and perplexity results for this are presented in Tables~\ref{tab:sp_9B_omni} and \ref{tab:sp_9B_qat}. We also plot the weight distribution for \spalg in Figure~\ref{fig:spmatquant}.
\input{TabsNFigs/SPMatQuant_Fig}

\section{Detailed Evaluation for \epalg}
Tables~\ref{tab:ep_2B_omni}, \ref{tab:ep_9B_omni}, and \ref{tab:ep_7B_omni} present downstream evaluation results for \epalg when applied to Gemma-2 2B, 9B, and Mistral 7B with OmniQuant as the base algorithm. Table~\ref{tab:ep_coditsill} presents downstream evaluation and perplexity results for our \epalg co-distillation experiments on Gemma-2 9B with OmniQuant as the base algorithm. 

\input{TabsNFigs/OmniQuant_Gemma2B_Gemma9B_Mistral7B_Alldatasets_Average/table1}
\input{TabsNFigs/OmniQuant_Gemma2B_Gemma9B_Mistral7B_Alldatasets_Average/table2}
\input{TabsNFigs/OmniQuant_Gemma2B_Gemma9B_Mistral7B_Alldatasets_Average/table3}
\input{TabsNFigs/QAT_Gemma2B_Gemma9B_Mistral7B_Alldatasets_Average/table1}
\input{TabsNFigs/QAT_Gemma2B_Gemma9B_Mistral7B_Alldatasets_Average/table2}
\input{TabsNFigs/QAT_Gemma2B_Gemma9B_Mistral7B_Alldatasets_Average/table3}
\input{TabsNFigs/ReweightingOmniQuant_Gemma2B_Gemma9B_Mistral7B_Alldatasets_Average/table2}
\input{TabsNFigs/ReweightingOmniQuant_Gemma2B_Gemma9B_Mistral7B_Alldatasets_Average/table1}
\input{TabsNFigs/ReweightingOmniQuant_Gemma2B_Gemma9B_Mistral7B_Alldatasets_Average/table3}

\input{TabsNFigs/Omniquant_Gemma9B_Codistil/table1}
\input{TabsNFigs/QAT_Gemma9B_Codistil/table1}

\input{TabsNFigs/QATFFNATTN_Gemma9B_Mistral7B_Alldatasets/table1}
\input{TabsNFigs/QATFFNATTN_Gemma9B_Mistral7B_Alldatasets/table2}
\input{TabsNFigs/FakeQuant_Gemma_9B/table2}

\input{TabsNFigs/FakeQuant_Gemma_9B/table5}
\input{TabsNFigs/FakeQuant_Gemma_9B/table6}
\input{TabsNFigs/FakeQuant_Gemma_9B/table4}

\input{TabsNFigs/Extra_Precision_All_Datasets_Gemma2B_Gemma9B_Mistral7B/table1}
\input{TabsNFigs/Extra_Precision_All_Datasets_Gemma2B_Gemma9B_Mistral7B/table2}
\input{TabsNFigs/Extra_Precision_All_Datasets_Gemma2B_Gemma9B_Mistral7B/table3}
\input{TabsNFigs/Codistil_Old_Table/table2}

\input{TabsNFigs/Extra_Precision_All_Datasets_Gemma2B_Gemma9B_Mistral7B/table4}