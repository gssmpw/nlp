\vspace*{-2mm}
\section{Ablations and Discussion}
\label{sec:disc}
In this section, we present design ablations to improve \alg. Section~\ref{sec:abl-weight} discusses the effect of non-uniform weighting across target precisions (int8, int4, int2), and Section~\ref{sec:abl-codistill} explores enabling co-distillation of lower precision levels (int4, int2) from the highest precision quantized model (int8). During the process of extending \alg to all Transformer parameters, not just the FFN block, we uncovered an interesting hybrid quantization algorithm (between Baseline and \alg). Section~\ref{sec:spmatquant} further details this method, called \spalg, which stabilizes the otherwise QAT baseline for all the Transformer weights. Finally, we also discuss extending \alg beyond integer data types and the considerations for effective deployment on current hardware.

% In this section, we present design ablations to further improve \alg.  Section~\ref{sec:abl-weight} discusses the effect of non-uniform weighting across target precisions (int8, int4, int2) and Section~\ref{sec:abl-codistill} dives into enabling co-distillation of lower precision levels (int4, int2) from the highest precision quantized model (int8).

% During the process of extending \alg to all Transformer parameters, not just the FFN weights, we unveiled an interesting hybrid (between baseline and \alg) quantization algorithm. Section~\ref{sec:abl-fake} discusses more about this method, called Single Precision \alg, that stabilizes the otherwise unstable baseline quantization training for all the Transformer weights.

% Finally, we also talk about extending \alg beyond integer data types and the considerations for effective deployment on the current hardware.
\input{TabsNFigs/ReweightingOmniQuant_Gemma2B_Gemma9B_Mistral7B_Alldatasets_Average/table4}
\vspace*{-3mm}
\subsection{Weightings ($\lambda_r$) for \alg}
\label{sec:abl-weight}

Depending on the constraints, we may wish to maximize the accuracy of one of the target bit-widths in \alg. Equation~\ref{eqn:matquant} provides a general formulation of \alg that supports searching over the weight $\lambda_r$ for bit-width $r$. The results in Section~\ref{sec:exp} are with the weights that have balanced performance across target precisions. Table~\ref{tab:weight} shows the weight multiplier ablation results for Gemma-2 2B, 9B, and Mistral 7B. We find that a higher relative value for $\lambda_2$ is essential in attaining good int2 performance. Increasing $\lambda_4,\lambda_8$ to improve int8 and int4 models often results in accuracy drop for the int2 models. In general, we can see that a higher relative weight for a specific precision results in increased accuracy for that bit-width. We can consider re-weighting as scaling the importance of the bits during training, and finding an optimal re-weighting recipe is an interesting research question.

% Depending on the deployment, we may wish to maximize the accuracy of one of the target bit-widths in \alg. Equation~\ref{eqn:matquant} provides a general formulation of \alg that enables a grid search on the weights $\lambda_r$ for bit-width $r$. For the results presented in Section~\ref{sec:exp}, we select the weights that maximize the accuracy of the int8 models. Table~\ref{tab:weight} shows the weight multiplier ablation results for Gemma-2 2B, 9B, and Mistral 7B models. While equal weighting for all precisions works great, we observe that higher weights for a specific precision results in increased accuracy for that bit-width. This re-weighting to improve int8 and int4 models often results in a minor accuracy drop for the int2 models. We can consider re-weighting as scaling the importance of the bits (through gradients) during training, and the optimal grid-search-free recipe is an interesting research question.

% Depending on the deployment setting, we might sometimes prefer to maximize the accuracy of one of the target bit-widths in \alg. The general-purpose formulation of \alg in Equation~\ref{eqn:matquant} enables a grid search on the weights $\lambda_r$ where $r$ is the bit-width. For the results in Section~\ref{sec:exp}, we select the weights that maximize the accuracy of the int8 models. Table~\ref{tab:weight} shows the weight multiplier ablation results for Gemma-2 2B, 9B, and Mistral 7B models. While there is no clear pattern, compared to weighting all the precisions equally, we observe that weighting int4 and int2 higher results in a better int8 model while also improving the int4 model. This re-weighting often results in minor accuracy drop for the int2 models. We can consider re-weighting as scaling the importance of the bits (through gradients) during training, and the optimal grid-search-free recipe is an interesting research question.
 
% Depending on the deployment setting, at times the users might prefer to maximize the accuracy of one of the target bit-widths in \alg. The general purpose formulation of \alg in Eq~\ref{eqn:matquant} enables for a grid search on the weights $\lambda_r$ where $r$ is the bit-width. For the results presented in Section~\ref{sec:exp} we pick the weights which maximize the accuracy of the int8 models. Table~\ref{tab:weight} shows the weight multiplier ablation results for Gemma2 2B, 9B and Mistral 7B models. While there is no clear pattern, compared to weighting all the precisions equally we notice that weighing int4 and int2 higher results in a better int8 model while also improving int4 model. This re-weighting often results in minor drops in accuracy for the int2 models. There is no clear pattern for the weighting effects on the interpolated int3 and int6 models. We can think of re-weighting as scaling the importance of the bits (through gradients) during training and the optimal grid-search-free recipe is an interesting research question.
\input{TabsNFigs/Omniquant_Gemma9B_Codistil/table2}
\vspace*{-2mm}
\subsection{Co-distillation for  \alg}
\label{sec:abl-codistill}
Given the nested nature of the models trained using \alg, we explored co-distillation, where the outputs from a higher-precision model are used as the target for the lower-precision nested model, either in a standalone fashion or alongside the ground truth target (weighted equally). Table~\ref{tab:codistill} shows the effects of co-distillation applied to \alg with both OmniQuant and QAT on Gemma-2 9B. While int8 and int4 show no significant improvement, the nested int2 model benefits substantially from the int8 supervision, reaching $0.97\%$ higher accuracy than the non-co-distilled \alg with OmniQuant. Co-distillation in \alg opens up avenues for interesting design choices that can further leverage the inherent nested structure of integer data types.

% Given the nested nature of the models trained using \alg, we explored the idea of co-distillation, where the outputs from a higher-precision model are used as the target for the lower-precision nested model, either in a standalone fashion or alongside the ground truth target (where they are weighted equally). Table~\ref{tab:codistill} shows the effects of co-distillation in \alg with both OmniQuant and QAT on Gemma-2 9B. We observe that while int8 and int4 do not improve significantly, the nested int2 model can benefit significantly from the logits of the int8 model -- 1.65\% more accurate than the non-co-distilled \alg with OmniQuant. This helps us push the int2 quantized version of Gemma-2 9B beyond 70\% average downstream accuracy for the first time across all our experiments. Co-distillation in \alg opens up avenues for interesting design choices that can further leverage the inherent nested structure of integer data types.


% Given the nested nature of the models trained using \alg, we explored the idea of co-distillation, where the outputs from a higher precision model are used as the target for the lower precision nested model in a standalone fashion or along side the ground truth target (where they are weighted equally). Table~\ref{tab:codistill} shows the effects of co-distillation in \alg with both OmniQuant and QAT on Gemma-2 9B. We notice that while int8 and int4 do not improve much, the nested int2 model can benefit significantly from the logits of int8 model -- 1.65\% more accurate than non co-distilled \alg with OmniQuant. This helps us push the int2 quantized version of Gemma-2 9B beyond 70\% average downstream accuracy for the first time across all our experiments. Co-distillation in \alg opens up avenues for interesting design choices that can further leverage the inherent nested structure of integer data types.




\input{TabsNFigs/FakeQuant_Gemma_9B/table1}
\vspace*{-2mm}
\subsection{\spalg}
\label{sec:spmatquant}
% \vspace*{-2mm}
In Tables~\ref{tab:omniquant-ffn} and~\ref{tab:qat-ffn}, \alg performs on par with the explicitly trained baselines for int4, int8, and the interpolated int3 and int6 precisions. However, the int2 models show a significant accuracy improvement. To investigate this, we conducted a simple ablation in \alg by removing the loss terms for int4 and int8 (i.e., $R = \{2\}$ in Equation~\ref{eqn:matquant} or setting $\lambda_4=\lambda_8=0$) and present the results in Table~\ref{tab:spmatquant_ffn}. We call this version of \alg as \spalg. With \spalg, we observe a further boost of up to $1.05\%$, in the accuracy of int2 models at a $\sim$2\% accuracy drop in the corresponding int4 and int8 models -- int2 is still nested within int8. This improvement likely stems from the six additional bits available during \alg-style training to optimize the int2 representation. 

In the case of \spalg, gradient descent is free to tune these six additional bits to improve the overall quality of the int2 model. In \alg, since we have additional losses to preserve the performance of the int4 and int8, the int2 performance is slightly worse than  \spalg. However, since the int4 and int8 models are typically very close in accuracy to the bfloat16 model, \alg can shift some of the weights to improve the int2 model. As int4 and int8 models have substantially more quantized buckets than int2, we hypothesize that shifting some weights into adjacent buckets may not significantly affect their performance; however, it can significantly impact int2's performance. In fact, in the weight distributions presented in Fig~\ref{fig:teaser}c, we observe that \alg results in a model where larger number of weights are assigned to the higher-valued buckets. Conclusively, \alg and \spalg inherently seem to be a better way of performing low-bit quantization.

% In Tables~\ref{tab:omniquant-ffn} and~\ref{tab:qat-ffn}, \alg performs on par with the explicitly trained baselines for int4, int8, and the interpolated int3 and int6 models. However, we observe a significant boost in the performance of int2 models. To better understand this, we conduct a simple ablation where we remove the loss terms for the int4 and int8 models (i.e., $R = \{2\}$ in Equation~\ref{eqn:matquant}) and present the results in Table~\ref{tab:spmatquant_ffn}. We call this version of \alg as \spalg. With \spalg, we observe a further boost, up to 1.67\%, in the performance of int2 models. We attribute this boost in performance to the fact that \alg-style training has 6 additional bits to further optimize for the int2 model. In the case of \spalg, gradient descent is free to tune these 6 additional bits to improve the overall quality of the int2 model. In \alg, since we have additional losses to preserve the performance of the int4 and int8 models, the int2 performance is slightly worse than that of \spalg. However, since the int4 and int8 models are typically very close in accuracy to the bfloat16 model, \alg can shift some of the weights to improve the int2 model. Given that int4 and int8 have substantially more quantized buckets than int2, we hypothesize that shifting some weights into adjacent buckets may not significantly affect their performance; however, it can significantly impact int2's performance. In fact, in the weight distributions presented in Figure~\ref{fig:teaser}, we observe that \alg results in a model where a larger number of weights are assigned to the later buckets, resulting in a more uniform distribution of weights for int2. \alg or \spalg inherently seems to be a better way to perform low-bit quantization.

% In Tables~\ref{tab:omniquant-ffn} and~\ref{tab:qat-ffn} \alg performs on par with the explicitly trained baselines for int4, int8 and the interpolated int3 and int6 models. However, we see a huge boost in the performance for int2 models. To better understand this, we run a simple ablation where we remove the loss terms for the int4 and int8 models (i.e., $R = \{2\}$ in Equation~\ref{eqn:matquant}) and present the results in Table~\ref{tab:spmatquant_ffn}. We call this version of \alg as \spalg. With \spalg we see a further boost, up to 1.67\%, in the performance for int2 models. We attribute this boost in performance to the fact that \alg style training has 6 additional bits to further optimize for the int2 model. In the case of \spalg, gradient descent is free to tune the 6 additional bits to improve the overall quality of the int2 model. In \alg, since we have additional losses to preserve the performance for the int4 and int8 models, the int2 performance is slightly worse than that of \spalg. However, since the int4 and int8 models are very close in performance to the bf16 model, \alg can move around some of the bits to improve the int2 model. Given that int4 and int8 have substantially more buckets than int2, we hypothesize that moving some weights into adjacent buckets may not affect their performance significantly, however, it can significantly impact int2's performance.  In fact, in the weight distributions presented in Figure~\ref{}, we see that \alg results in a model where a larger number of weights are assigned to the later buckets, resulting in a more uniform distribution of weights for int2. \alg or \spalg  inherently seems to be a better way to perform low bit quantization.

% As an example, let's consider the int8 value 65. On slicing the 2 MSBs we get 64. However, when slicing the 2 MSBs for 63, we get 0. For the int8 model, changing 63 to 65 may not lead to a huge difference in the performance, however, for the int2 model, it may make a huge difference. Given that the loss for the int2 model in Equation~\ref{eqn:matquant} is substantially higher, we hypothesize that \alg would prefer 65 over 63.
% \label{sec:abl-fake}

\input{TabsNFigs/QATFFNATTN_Gemma9B_Mistral7B_Average_logpplx/table1}
\vspace{-3mm}
\paragraph{FFN + Attention Weight Quantization.}

We present results for FFN + Attention quantization for QAT in Table~\ref{tab:ffn_attn_qat}. For int8, int4 and the interpolated int6 model, \alg performs on par with the \textit{Baseline}. However, we found int2 and int3 to be very unstable while quantizing both, the FFN and the Attention parameters. Most recent works that do QAT for both the blocks~\cite{DBLP:efficientqat, DBLP:llmqat, DBLP:BitDistiller} either do some form of warm starting for the quantized parameters, or have additional distillation and auxiliary loss functions. In the naive setup of minimizing the loss with respect to the ground truth, we find QAT to be very unstable at lower precisions. On the other hand, both \alg and \spalg are very stable further highlighting the benefits brought by \alg style training. 
% \ak{These tables and fakemat tables need to be consolidated}

\vspace*{-3mm}
\subsection{Deployment Considerations}
\label{sec:dep}
\vspace*{-1mm}
Current hardware accelerators have native support for serving int8 and int4 quantized models. Additionally, custom-implemented CUDA kernels can can support various low-precision bit-widths, like int2 and int3~\citep{chee2024quip,frantar2022gptq}. \alg can generate a large number of models at inference time. Depending on the serving environment, we can choose between Mix'n'Match models and homogeneous sliced models. For example, suppose the serving environment has a memory constraint equivalent to an int3 model but lacks optimized support for int3, while supporting int2. In this case, a Mix'n'Match model with a small performance drop when compared to the sliced int3 model could be deployed. More generally, as depicted in Figure~\ref{fig:omniquant-mnm}, \alg densely spans the memory-versus-accuracy curve and can be leveraged to obtain performant model for several serving constraints. \alg can enable further research on hardware software co-design to effectively support elastic bit-widths on-the-fly during inference.

% Current hardware accelerators have strong native support for serving int8 and int4 quantized models. Additionally, custom CUDA kernels can be implemented to support various low-precision bit-widths, such as int2 and int3~\citep{chee2024quip,frantar2022gptq}. \alg can generate a combinatorial number of models at inference time and depending on the serving environment, we can opt for either Mix'n'Match models or homogeneous sliced models. For example, if the serving environment requires a total memory footprint equivalent to that of an int3 model, however, it lacks an optimized support for int3 but has support for int2, one could deploy a Mix'n'Match model that performs on par with the int3 model. More generally, as depicted in Figure~\ref{fig:omniquant-mnm}, \alg densely spans the memory-versus-accuracy curve and can be leveraged to obtain the most performant model for a specific serving constraint. \alg can enable further research on hardware co-design to effectively support elastic bit-widths on-the-fly during inference time.

% Depending on the serving environment, one could opt for more homogeneous models that have the same bit-width at every layer, or one could choose Mix'n'Match models that offer the best accuracy-vs-memory trade-off. 

% Dynamically routing between several precisions can also potentially bring throughput and latency benefits. For an easier query, the router might choose to use 2-bits for most layers; for a more challenging query, the router might choose to use 8-bits for most layers. However, this kind of routing requires hardware support that enables loading only a part of the int8 representation from the ML accelerator's HBM to its SRAM.
% \ak{Pranav: Need to write about current support in hardware and how to deploy and if anyone has deployed, cite}
\vspace*{-2mm}
\subsection{Extension to Floating Point}
\label{sec:fp}
\vspace*{-1mm}
Extending \alg to floating-point representations, such as FP8 and FP4, presents significant challenges. Given that the exponent is encoded within the bit representation and contributes to the value as a power of 2 (i.e., effectively $\log_\text{2}$), slicing it results in buckets whose sizes increase exponentially, unlike the integer case, where bucket sizes are constant. For example, slicing the first two bits from int8 yields buckets of $0$, $64$, $128$, $192$. Here, the bucket size ($64$) is constant; however, this would not be the case when slicing two exponent bits from FP8. This is a promising avenue for future research that could further unlock the benefits of \alg, even during large-scale pretraining.

% We leave further investigation in this direction for future work.


% Extending \alg to Floating Points seems non-trivial at the moment. Given that the exponent is stored in $\text{Log}_\text{2}$, slicing it results buckets whose sizes increasing exponentially unlike the integer case where they are of constant size (For example, slicing the first two bits from int8 would give $0$, $64$, $128$, $192$. Here, the bucket size (of $64$) is constant, however this won't be the case when slicing 2 exponent bits from FP8). Will leave further investigation into this direction for future work.
% Additional training overheads.

% \subsection{Generality of the technique}