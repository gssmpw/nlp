
%Inference with Large Language Models (LLMs) is primarily communication bound, that is a significant chunk of decode latency is attributed to transferring model weights or activations. 
% Quantizating model weights  is critical to reducing the communication and inference cost of large models. However, quantizing models -- especially to low-precision like int4 or int2 -- require trading off model quality; int2 in particular is known to severely degrade model quality. So either we are forced to maintain multiple models with different quantization precision or serve one single model that best satisfies the quality-latency tradeoff. On the other hand, integer data types, such as int8, inherently hvae a nested (Matryoshka) structure where smaller bitwidth int4 or int2 integers, are nested in the most significant bits. Leveraging this insight, in this paper, we propose Matryoshka Quantization (\alg), a novel multi-scale quantization technique that alleviates the above challenging decision and allows us to train and maintain one single quantized model but serve with the precision demanded by application or system load etc. Furthermore, due to co-training and co-distillation regularization provided by \alg, the 2-bit precision models read-out by \alg can be as much as $\bf 8\%$ more accurate than standard 2-bit quantization (using techniques like QAT, Omniquant). This represents a significant progress in model quantization where despite following the same recipe, a 2-bit FFN quantized Gemma-2 9B model is more accurate than an 8-bit FFN quantized Gemma-2 2B model. 

Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. Leveraging this insight, in this paper, we propose Matryoshka Quantization (\alg), a novel multi-scale quantization technique that alleviates the aforementioned challenge. This technique allows us to train and maintain a single quantized model but serve it with the precision demanded by the deployment. Furthermore, leveraging \alg's co-training and co-distillation regularization, int2 precision models extracted by \alg outperform standard int2 quantization by up to to 4\% and 7\% with OmniQuant and QAT as base algorithms respectively. Finally, we demonstrate that by using an extra bit to represent outliers, a model with an effective precision of 2.05-bit gives an additional 6\% improvement with OmniQuant as the base algorithm.


%Integer data types, such as int8, inherently contain smaller bit-width integers, such as int4 and int2, within their most significant bits. Existing model quantization approaches often ignore this inherent nested structure of integers, treating each target precision (int8, int4, int2) as an independent optimization problem and resulting in multiple models. In this paper, we propose Matryoshka Quantization (\alg), a novel multi-scale training technique that leverages this nested (Matryoshka) structure and simultaneously optimizes model weight quantization across multiple precision levels (int8, int4, and int2). \alg is a general-purpose solution that can be used with any learning-based model quantization method (e.g., QAT, OmniQuant) to obtain a single model that can operate effectively across multiple bit-widths. Quantizing the Feed Forward Network (FFN) parameters of large language models (LLMs), such as the Gemma-2 2B, 9B and Mistral 7B, with \alg results in int8 and int4 models that are as accurate as their independently trained counterparts, while the int2 models can achieve up to $\bf 8\%$ higher accuracy on a collection of downstream tasks. \alg also enables the extraction of models with interpolative bit-widths (e.g., int6, int3) and further provides the ability to densely span the accuracy-vs-cost Pareto trade-off through layerwise mix'n'match of precisions. Finally, we present further ablations and investigations into the workings of the technique that led to a better standalone low-bit quantization technique.

% Finally, we present further ablations and investigations into the workings of the technique.

% Integer data types (e.g., int8) inherently contain smaller bit-width integers (e.g. int4, int2) as their most significant bits. Existing model quantization approaches ignore the inherent nested structure of integers and treat each target precision (int8, int4, int2) as an independent optimization problem resulting in multiple models. In this paper, we propose Matryoshka Quantization (\alg), a novel multi-scale training technique that leverages this nested (Matryoshka) structure and simultaneously optimizes for model weight quantization across multiple precision levels (int8, int4, int2). \alg is a general purpose solution that can be used with any learning-based model quantization method (QAT, OmniQuant etc.,) to obtain a single model that can operate effectively across multiple bit-widths. Quantizing the FFN parameters of large language models (LLMs), like Gemma 2 family 2-27B and Mistral 7B, with \alg results in int8 and int4 models that are as accurate as independently trained counterparts while the int2 models can be up to $\bf 8\%$ more accurate on a collection of downstream tasks. \alg also enables extraction of models with interpolative bit-widths (e.g., int6, int3) and further provides the ability to densely span the accuracy-vs-cost pareto trade-off through mix'n'match of precisions across layers. Finally, we also present further ablations and investigations towards the workings of the technique.

