\section{Related Work}
\label{sec:rw}

% Types of Quantization 

% \begin{itemize}
%     \item Data-free Quantization
%     \item Data-driven Quantization
% \begin{itemize}
%     \item Learning-free 
%     \item Learning based
% \end{itemize}
% \end{itemize}

% Data-free quantization includes dumb quantization that is uniform or non-uniform based on the distribution of the weghts. Often we do not require additional data to get this to working. Examples include most naive quantization libraries like bitsandbytes~\citep{dettmers2022gpt3}, native quanitzation in pytorch and tensorflow. 

% To further improve the accuracy during quantization, apart from outliers, we can also use data to further calibrate and maximize downstream accuracy. Examples include most post training quanitzation (PTQ)

% Lastly is learning based quantization, that could only learn the scaling factors for quantization or learn both scaling factors along side updating the weights. The former is data and compute efficient but has limitation like OmniQuANTs, while the full QAT is powerful, it requires a lot more data and compute and could potentially suffer from overfitting in case of poor quality data.


Model weight quantization is an extremely powerful and prevalent technique for making resource-intensive neural networks suitable for deployment constraints -- especially modern-day LLMs. Quantization algorithms can be categorized as either learning-free or learning-based. Learning-free methods use limited data to calibrate model parameters without relying on gradient descent. Learning-based methods, however, utilize gradient descent to update either model parameters or auxiliary parameters to aid in quantization.

\vspace{-4mm}
\paragraph{Learning-free Quantization Methods.} Naive quantization methods, such as MinMax, absmax, and zero-point quantization, aim to directly map the range of model weights to the target bit-width -- see~\citep{dettmers2022gpt3} for a detailed background. \citet{dettmers2022gpt3} further improved this by identifying the need to handle outliers with higher precision than the rest of the model weights. The core principle of more recent learning-free quantization methods remains similar while improving various aspects of it and using small amounts of data for calibration. For example, GPTQ~\citep{frantar2022gptq} improves upon min-max quantization by iterating over all the coordinates, quantizing them one at a time, and updating the remaining full-precision coordinates to minimize the layer-wise activation reconstruction error. AWQ~\citep{lin2023awq}, SmoothQuant~\citep{xiao2023smoothquant}, and AffineQuant~\citep{ma2024affinequant} scale the weights and activations to reduce outliers, thus making them easier to quantize. QuIP~\citep{chee2024quip}, FrameQuant~\citep{adepu2024framequant}, and QuaRoT~\citep{quarot} multiply the weights and activations by orthonormal matrices before quantizing to reduce the number of outliers. SqueezeLLM~\citep{squeezellm} uses clustering to obtain the optimal buckets for quantization, and CDQuant~\citep{DBLP:cdquant} improves upon GPTQ by greedily choosing the coordinates to descend along. While learning-free methods are inexpensive and work well at higher bit-widths, they are often suboptimal in the low-precision regime, which benefits greatly from learning-based techniques.

% Naive quantization methods like MinMax, absmax and zero-point quantization aim at directly mapping the range of model weights to the target bit-width (see \citet{dettmers2022gpt3} for a detailed background). \citet{dettmers2022gpt3} improved this further by identifying the need to handle outliers in higher precision than the rest of the model weights. The crux of the more recent learnining-free quantization methods remain similar while improving various aspects of it and using small amounts of data for calibration. For example, GPTQ~\citep{frantar2022gptq} improves over min-max quantization by iterating over all the coordinates, quantizing them one at a time, and updating the remaining full-precision coordinates to minimize the layer-wise activation reconstruction error. AWQ~\citep{lin2023awq}, SmoothQuant~\citep{xiao2023smoothquant}, and AffineQuant~\citep{ma2024affinequant} scale the weights and activations to reduce outliers, thus making it easier to quantize them. QuIP~\citep{chee2024quip}, FrameQuant~\citep{adepu2024framequant} and QuaRoT~\citep{quarot} multiply the weights and activations with orthonormal matrices before quantizing to reduce the number of outliers. SqueezeLLM~\citep{squeezellm} uses clustering to obtain the optimal buckets for quantization and CDQuant~\citep{DBLP:cdquant} improves upon GPTQ by greedily choosing the coordinates to descent along. While learning-free methods are cheap and work well at higher bit-widths, they are often subpar in the low-precision regime which benefits greatly from learning-based techniques.

\vspace{-2mm}
\paragraph{Learning-based Quantization Methods.} Quantization Aware Training (QAT)~\citep{jacob2018quantization,abdolrashidi2021pareto} is a logical approach to ensure that models are easy to quantize during inference while retaining high accuracy. However, because QAT involves updating all the model parameters, its adoption for LLMs has been limited. Several recent works improve the performance and efficiency of QAT. LLM-QAT~\citep{DBLP:llmqat} and BitDistiller~\citep{DBLP:BitDistiller} enhance QAT with knowledge distillation from the full-precision model. EfficientQAT~\citep{DBLP:efficientqat} minimizes the block-wise reconstruction error before performing end-to-end training. This significantly reduces the time it takes for QAT to converge. On the other hand, some techniques significantly reduce the overhead by learning only the auxiliary parameters, such as scaling factors and zero-points, that aid in quantization instead of updating the actual weight matrices. For example, OmniQuant~\citep{shao2023omniquant} does not update the model parameters; instead, it learns additional scales and shifting parameters (that aid with quantization) through gradient descent over the block-wise reconstruction error and achieves better accuracy than most QAT techniques. Likewise, SpinQuant~\citep{spinquant} uses gradient descent to learn its rotation matrices. This class of learning-based quantization techniques (OmniQuant, SpinQuant, etc.) is widely adopted due to their appeal of achieving QAT-level accuracy at a fraction of the cost.

% Quantized Aware Training (QAT)~\citep{jacob2018quantization,abdolrashidi2021pareto} is a logical way of ensuring the models are easy to quantize during inference while retaining high accuracy. However, since QAT involves updating all the model parameters, its adoption for LLMs has been limited. Some recent works improve the performance and efficiency of QAT. LLM-QAT~\citep{DBLP:llmqat} and BitDistiller~\citep{DBLP:BitDistiller} improve QAT with knowledge distillation from the full precision model. EfficientQAT~\citep{DBLP:efficientqat} minimizes the block-wise reconstruction error before performing end-to-end training. This significantly reduces the time it takes for the QAT to converge. On the other hand, some techniques reduce the overhead signficantly by learning just the auxiliary parameters, like scaling factors and zero-points, that aid in quanitzation instead of updating the actual weight matrices. For example, OmniQuant~\citep{shao2023omniquant} doesn't update the model parameters, instead it learns additional scales and shifting parameters (that aid with quatization) through gradient descent over the block-wise reconstruction error and achieves better accuracy than most QAT techniques. Likewise, SpinQuant~\citep{spinquant} uses gradient descent to learn its rotation matrices. This class of learning-based quantization techniques (OmniQuant, SpinQuant etc.,) are widely-adopted due to their appeal of QAT level accuracy at a fraction of the cost.
\vspace{-3mm}
\paragraph{Multi-scale Training.} Training across multiple data scales (resolutions) was heavily popularized in computer vision for both recognition and generation~\citep{adelson1984pyramid,lin2017feature,denton2015deep}. More recently, the paradigm of multi-scale training has shifted to models~\citep{rippel2014learning,yu2018slimmable,kusupati2022matryoshka,devvrit2023matformer}, where the data remains the same, and models of varying capacity, all nested within one large model, are trained jointly. This joint, nested (Matryoshka-style) learning with varying model sizes results in a smooth accuracy-vs-compute trade-off and is beneficial in many downstream applications and real-world deployments. However, the most obvious structure with a nested nature is the bit structure of the integer data type. Given the success of multi-scale training for inputs, outputs, and model weights, it is imperative to explore it further for integer data types, especially in the context of quantization, which aids in the deployment of resource-intensive LLMs. Following this idea, \citet{any_precision_dnn} have successfully trained a single model that can do well at any precision. However, the experiments were limited to ConvNets and small Neural Networks. In this paper, we extend the idea of nested precision to LLMs and show that it indeed works at scale. We also show that, for the first time, our models are quality neutral for intermediate precisions such as int3 and int6 that we never trained for, and densely span the accuracy-vs-bits trade-off. In Section~\ref{sec:spmatquant}, we show that even to train models for a fixed target precision, having loss over the sliced bits of an 8-bit model does better than training a model explicitly for that precision, indicating that \alg is a fundamentally better way to do low-bit quantization.

% Training across multiple data scales (resolutions) was heavily popularized in computer vision both for recogintion and generation~\citep{adelson1984pyramid,lin2017feature,denton2015deep}. More recently, the paradigm of multi-scale training moved to the models~\citep{rippel2014learning,yu2018slimmable,kusupati2022matryoshka,devvrit2023matformer} where the data remains the same and models of varying capacity, all nested within one large model, are trained jointly. This joint nested (Matryoshka-style) learning with varying model sizes results in smooth accuracy-vs-compute trade-off and is beneficial in many downstream applications and real-world deployment. However, the most obvious thing that has the nested structure is the bit structure of the integer data type. With the success of multi-scale training for inputs, outputs and model weights it is only imperative to explore it further for integer data types, especially in the context of quantization that helps with the deployment of resource-intensive LLMs~\citep{dettmers2023case}.