[
  {
    "index": 0,
    "papers": [
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "lin2023awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song",
        "title": "Awq: Activation-aware weight quantization for llm compression and acceleration"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "xiao2023smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "Smoothquant: Accurate and efficient post-training quantization for large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ma2024affinequant",
        "author": "Ma, Yuexiao and Li, Huixia and Zheng, Xiawu and Ling, Feng and Xiao, Xuefeng and Wang, Rui and Wen, Shilei and Chao, Fei and Ji, Rongrong",
        "title": "AffineQuant: Affine Transformation Quantization for Large Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "chee2024quip",
        "author": "Chee, Jerry and Cai, Yaohui and Kuleshov, Volodymyr and De Sa, Christopher M",
        "title": "Quip: 2-bit quantization of large language models with guarantees"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "adepu2024framequant",
        "author": "Adepu, Harshavardhan and Zeng, Zhanpeng and Zhang, Li and Singh, Vikas",
        "title": "FrameQuant: Flexible Low-Bit Quantization for Transformers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "quarot",
        "author": "Saleh Ashkboos and\nAmirkeivan Mohtashami and\nMaximilian L. Croci and\nBo Li and\nMartin Jaggi and\nDan Alistarh and\nTorsten Hoefler and\nJames Hensman",
        "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "squeezellm",
        "author": "Sehoon Kim and\nColeman Hooper and\nAmir Gholami and\nZhen Dong and\nXiuyu Li and\nSheng Shen and\nMichael W. Mahoney and\nKurt Keutzer",
        "title": "SqueezeLLM: Dense-and-Sparse Quantization"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "DBLP:cdquant",
        "author": "Pranav Ajit Nair and\nArun Sai Suggala",
        "title": "CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained\nModels using Greedy Coordinate Descent"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "lin2023awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song",
        "title": "Awq: Activation-aware weight quantization for llm compression and acceleration"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "xiao2023smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "Smoothquant: Accurate and efficient post-training quantization for large language models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "ma2024affinequant",
        "author": "Ma, Yuexiao and Li, Huixia and Zheng, Xiawu and Ling, Feng and Xiao, Xuefeng and Wang, Rui and Wen, Shilei and Chao, Fei and Ji, Rongrong",
        "title": "AffineQuant: Affine Transformation Quantization for Large Language Models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "chee2024quip",
        "author": "Chee, Jerry and Cai, Yaohui and Kuleshov, Volodymyr and De Sa, Christopher M",
        "title": "Quip: 2-bit quantization of large language models with guarantees"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "adepu2024framequant",
        "author": "Adepu, Harshavardhan and Zeng, Zhanpeng and Zhang, Li and Singh, Vikas",
        "title": "FrameQuant: Flexible Low-Bit Quantization for Transformers"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "quarot",
        "author": "Saleh Ashkboos and\nAmirkeivan Mohtashami and\nMaximilian L. Croci and\nBo Li and\nMartin Jaggi and\nDan Alistarh and\nTorsten Hoefler and\nJames Hensman",
        "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "squeezellm",
        "author": "Sehoon Kim and\nColeman Hooper and\nAmir Gholami and\nZhen Dong and\nXiuyu Li and\nSheng Shen and\nMichael W. Mahoney and\nKurt Keutzer",
        "title": "SqueezeLLM: Dense-and-Sparse Quantization"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "DBLP:cdquant",
        "author": "Pranav Ajit Nair and\nArun Sai Suggala",
        "title": "CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained\nModels using Greedy Coordinate Descent"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "jacob2018quantization",
        "author": "Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry",
        "title": "Quantization and training of neural networks for efficient integer-arithmetic-only inference"
      },
      {
        "key": "abdolrashidi2021pareto",
        "author": "Abdolrashidi, AmirAli and Wang, Lisa and Agrawal, Shivani and Malmaud, Jonathan and Rybakov, Oleg and Leichner, Chas and Lew, Lukasz",
        "title": "Pareto-optimal quantized resnet is mostly 4-bit"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "DBLP:llmqat",
        "author": "Zechun Liu and\nBarlas Oguz and\nChangsheng Zhao and\nErnie Chang and\nPierre Stock and\nYashar Mehdad and\nYangyang Shi and\nRaghuraman Krishnamoorthi and\nVikas Chandra",
        "title": "{LLM-QAT:} Data-Free Quantization Aware Training for Large Language\nModels"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "DBLP:BitDistiller",
        "author": "Dayou Du and\nYijia Zhang and\nShijie Cao and\nJiaqi Guo and\nTing Cao and\nXiaowen Chu and\nNingyi Xu",
        "title": "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "DBLP:efficientqat",
        "author": "Mengzhao Chen and\nWenqi Shao and\nPeng Xu and\nJiahao Wang and\nPeng Gao and\nKaipeng Zhang and\nYu Qiao and\nPing Luo",
        "title": "EfficientQAT: Efficient Quantization-Aware Training for Large Language\nModels"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "shao2023omniquant",
        "author": "Shao, Wenqi and Chen, Mengzhao and Zhang, Zhaoyang and Xu, Peng and Zhao, Lirui and Li, Zhiqian and Zhang, Kaipeng and Gao, Peng and Qiao, Yu and Luo, Ping",
        "title": "Omniquant: Omnidirectionally calibrated quantization for large language models"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "spinquant",
        "author": "Zechun Liu and\nChangsheng Zhao and\nIgor Fedorov and\nBilge Soran and\nDhruv Choudhary and\nRaghuraman Krishnamoorthi and\nVikas Chandra and\nYuandong Tian and\nTijmen Blankevoort",
        "title": "SpinQuant: {LLM} quantization with learned rotations"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "jacob2018quantization",
        "author": "Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry",
        "title": "Quantization and training of neural networks for efficient integer-arithmetic-only inference"
      },
      {
        "key": "abdolrashidi2021pareto",
        "author": "Abdolrashidi, AmirAli and Wang, Lisa and Agrawal, Shivani and Malmaud, Jonathan and Rybakov, Oleg and Leichner, Chas and Lew, Lukasz",
        "title": "Pareto-optimal quantized resnet is mostly 4-bit"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "DBLP:llmqat",
        "author": "Zechun Liu and\nBarlas Oguz and\nChangsheng Zhao and\nErnie Chang and\nPierre Stock and\nYashar Mehdad and\nYangyang Shi and\nRaghuraman Krishnamoorthi and\nVikas Chandra",
        "title": "{LLM-QAT:} Data-Free Quantization Aware Training for Large Language\nModels"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "DBLP:BitDistiller",
        "author": "Dayou Du and\nYijia Zhang and\nShijie Cao and\nJiaqi Guo and\nTing Cao and\nXiaowen Chu and\nNingyi Xu",
        "title": "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "DBLP:efficientqat",
        "author": "Mengzhao Chen and\nWenqi Shao and\nPeng Xu and\nJiahao Wang and\nPeng Gao and\nKaipeng Zhang and\nYu Qiao and\nPing Luo",
        "title": "EfficientQAT: Efficient Quantization-Aware Training for Large Language\nModels"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "shao2023omniquant",
        "author": "Shao, Wenqi and Chen, Mengzhao and Zhang, Zhaoyang and Xu, Peng and Zhao, Lirui and Li, Zhiqian and Zhang, Kaipeng and Gao, Peng and Qiao, Yu and Luo, Ping",
        "title": "Omniquant: Omnidirectionally calibrated quantization for large language models"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "spinquant",
        "author": "Zechun Liu and\nChangsheng Zhao and\nIgor Fedorov and\nBilge Soran and\nDhruv Choudhary and\nRaghuraman Krishnamoorthi and\nVikas Chandra and\nYuandong Tian and\nTijmen Blankevoort",
        "title": "SpinQuant: {LLM} quantization with learned rotations"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "adelson1984pyramid",
        "author": "Adelson, Edward H and Anderson, Charles H and Bergen, James R and Burt, Peter J and Ogden, Joan M",
        "title": "Pyramid methods in image processing"
      },
      {
        "key": "lin2017feature",
        "author": "Lin, Tsung-Yi and Doll{\\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge",
        "title": "Feature pyramid networks for object detection"
      },
      {
        "key": "denton2015deep",
        "author": "Denton, Emily L and Chintala, Soumith and Fergus, Rob and others",
        "title": "Deep generative image models using a laplacian pyramid of adversarial networks"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "rippel2014learning",
        "author": "Rippel, Oren and Gelbart, Michael and Adams, Ryan",
        "title": "Learning ordered representations with nested dropout"
      },
      {
        "key": "yu2018slimmable",
        "author": "Yu, Jiahui and Yang, Linjie and Xu, Ning and Yang, Jianchao and Huang, Thomas",
        "title": "Slimmable neural networks"
      },
      {
        "key": "kusupati2022matryoshka",
        "author": "Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek and Howard-Snyder, William and Chen, Kaifeng and Kakade, Sham and Jain, Prateek and others",
        "title": "Matryoshka representation learning"
      },
      {
        "key": "devvrit2023matformer",
        "author": "Devvrit, F and Kudugunta, Sneha and Kusupati, Aditya and Dettmers, Tim and Chen, Kaifeng and Dhillon, Inderjit and Tsvetkov, Yulia and Hajishirzi, Hannaneh and Kakade, Sham and Farhadi, Ali and Jain, Prateek and others",
        "title": "Matformer: Nested transformer for elastic inference"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "any_precision_dnn",
        "author": "Haichao Yu and Haoxiang Li and Humphrey Shi and Thomas S. Huang and Gang Hua",
        "title": "Any-Precision Deep Neural Networks"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "adelson1984pyramid",
        "author": "Adelson, Edward H and Anderson, Charles H and Bergen, James R and Burt, Peter J and Ogden, Joan M",
        "title": "Pyramid methods in image processing"
      },
      {
        "key": "lin2017feature",
        "author": "Lin, Tsung-Yi and Doll{\\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge",
        "title": "Feature pyramid networks for object detection"
      },
      {
        "key": "denton2015deep",
        "author": "Denton, Emily L and Chintala, Soumith and Fergus, Rob and others",
        "title": "Deep generative image models using a laplacian pyramid of adversarial networks"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "rippel2014learning",
        "author": "Rippel, Oren and Gelbart, Michael and Adams, Ryan",
        "title": "Learning ordered representations with nested dropout"
      },
      {
        "key": "yu2018slimmable",
        "author": "Yu, Jiahui and Yang, Linjie and Xu, Ning and Yang, Jianchao and Huang, Thomas",
        "title": "Slimmable neural networks"
      },
      {
        "key": "kusupati2022matryoshka",
        "author": "Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek and Howard-Snyder, William and Chen, Kaifeng and Kakade, Sham and Jain, Prateek and others",
        "title": "Matryoshka representation learning"
      },
      {
        "key": "devvrit2023matformer",
        "author": "Devvrit, F and Kudugunta, Sneha and Kusupati, Aditya and Dettmers, Tim and Chen, Kaifeng and Dhillon, Inderjit and Tsvetkov, Yulia and Hajishirzi, Hannaneh and Kakade, Sham and Farhadi, Ali and Jain, Prateek and others",
        "title": "Matformer: Nested transformer for elastic inference"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "dettmers2023case",
        "author": "Dettmers, Tim and Zettlemoyer, Luke",
        "title": "The case for 4-bit precision: k-bit inference scaling laws"
      }
    ]
  }
]