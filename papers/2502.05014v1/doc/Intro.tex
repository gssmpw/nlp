\section{Introduction}\label{section:Introduction}

High-altitude balloons (HABs) with altitude control capabilities can leverage opposing winds at different altitudes to achieve limited horizontal control.  By adjusting altitude to ``ride'' favorable winds back and forth to maintain position over an area of interest, HABs can perform station-keeping maneuvers, or with higher wind diversity, conduct more complicated trajectories. Path planning for HABs is non-trivial due to complex dynamic wind fields and partial observability of forecasts.  Wind flows constantly change in time, position, and altitude.  Some wind trends lead to more favorable and predictable station-keeping performance while some wind configurations render station-keeping infeasible.  Developing robust path-planning algorithms that can handle forecast errors also requires wind models that realistically deviate from forecasts.

Deep Reinforcement Learning (DRL) is a subset of reinforcement learning that incorporates neural networks in traditional reinforcement learning. At a high level, DRL can be used to train agents to engage in optimal behaviors in the presence of complex environments without explicit modeling of the state-action transition matrix. To determine these policies, agents are typically trained in simulation for thousands to millions of ``episodes'' depending on the problem complexity. In recent years, several state-of-the-art DRL algorithms have emerged and are actively utilized in the scientific community, such as Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and actor-critic methods such as A2C and SAC. In this work we apply DQN, a popular state-of-the-art off-policy DRL algorithm,  for completing objectives in complex environments based on past experiences. DQN was developed by Deepmind in 2013 where they successfully demonstrated beating human experts at playing Atari 2600 games \cite{mnih2013playing}. 

In traditional Q-Learning, the Bellman Equation, shown in Eq. \ref{Bellman}, is used to approximate the value function $Q(s, a)$ of the state $s$ and the action $a$ at the Markovian decision point. The value function $Q(s, a)$ represents the inherent value, looking forward in time, that a particular decision (state, action pair) can have. 

\begin{equation}
    Q(s,a) = r + \gamma max_{a'}Q(s',a')
    \label{Bellman}
\end{equation}

In situations where the state and action space are discrete, tabular representations of the value function are the most efficient. However, in situations with complex environments, high state dimensionality, and/or continuous state spaces, estimating tabular values for the value function is not feasible. DQN employs a neural network to approximate the value function and results in a more efficient and robust policy. Since the original publication, DQN and it's many variations have been implemented on autonomous and robotic systems. DQN has been implemented in many real-world complex path planning situations such as in a warehouse dispatching system \cite{yang2020multi}, with coastal ships \cite{guo2021path}, autonomous driving \cite{chen2020conditional} and drones \cite{azar2021drone}. 
%Background on Types of Altitude Controlled Balloons?
%Background on Weather Forecasts, Data Assimilation, etc. -> covered later
%Background on Reinforcement Learning

%STation Keeping without Balloons
%[NOT INCLUDE YET] https://www.sciencedirect.com/science/article/pii/S0273117722003672
%Cite the citations from google too

\subsection{Related Work}

Google Loon provided the first example of using Deep Reinforcement Learning to successfully station-keep super-pressure balloons in real-time, both in simulation and flight tested on HABs in the Pacific Ocean.  They trained HAB agents in a custom simulation environment using  Distributional Reinforcement Learning with Quantile Regression (QR-DQN) and achieved an average station-keeping time within a region of 50 km (TWR50) of approximately 55\% over multi-day, sometimes multi-week flights \cite{bellemare2020autonomous}. 
Brown et al. showed through simulation with ERA5 wind data and tree search path planning, how autonomous HABs leveraging winds in the
stratosphere can have significantly different success rates dependent on season and geographic region of operation \cite{brown2024seasonal}.
Similarly,  Schuler et. al also investigated wind diversity trends in the Western Hemisphere from over 1 million aggregated radiosonde data points over a decade with respect to controllable HABS \cite{schuler2025wind}.
Jeger et al. used the Soft Actor-Critic algorithm to successfully control short duration balloons from a start point to a predefined target in under 1.5 hours and demonstrated the algorithm on indoor and outdoor prototypes \cite{jeger2023reinforcement}. Xu et al. provide a detailed dynamic model for air-ballasted altitude control balloons and demonstrated station keeping using a Double Dueling Deep Q-Learning (D3QN) framework with priority experience replay based on high-value samples (HVS-PER) \cite{xu2022station}.
Saunders et al. created a custom simulation environment to train short-duration resource constrained weather balloons with a vent and ballast system. Using the Soft Actor-Critic (SAC) algorithm, they trained simulated HABs to station keep within TWR50 of 25\% of the flight using ECWMF wind data \cite{saunders2023}. 
Similarly,  Gannetti et al. simulated sounding balloons with an altitude control system using DQN on ERA5 winds and achieved an average simulated TWR50 of 12.5\% \cite{gannetti2023navigation}. 




%https://arxiv.org/pdf/2303.01173 
%file:///C:/Users/Tristan/Downloads/s41586-020-2939-8%20(1).pdf
%https://www.sciencedirect.com/science/article/pii/S0273117722003672
%https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10146448
%[NOT INCLUDED YET] https://www.diva-portal.org/smash/get/diva2:1364883/FULLTEXT02.pdf
%https://arxiv.org/pdf/2403.10784
%https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10189997
%[NOT INCLUDED YET] https://www.proquest.com/docview/2554893937?pq-origsite=gscholar&fromopenview=true&sourcetype=Working%20Papers
%[NOT INCLUDED YET] https://www.sciencedirect.com/science/article/pii/S1270963824003067



%Chinthan paper (Find some more balloon dynamics papers if we want to discuss this more and why we're ignroing it):
%https://www.iastatedigitalpress.com/ahac/article/8334/galley/7926/view/
%https://ntrs.nasa.gov/api/citations/20050243623/downloads/20050243623.pdf
