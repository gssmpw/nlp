\section{Discussion}\label{section:Discussion}


\subsection{Forecast Classification}\label{section:Discussion_ForecastClassification}

Station Keeping TWR success rates are completely dependent on the wind diversity for a region of interest, which can significantly vary seasonally and geographically.  To investigate how TWR success rates are affected by different regions and seasons we introduce a simple opposing winds binning algorithm to classify forecasts' wind diversity independently of trained HAB agents. The overall Forecast Score, $FS$, is a percentage of wind diversity based on the total count of opposing wind pairs in an altitude column at a specific central forecast subset coordinate, averaged across multiple time steps.


%Need to add in a calm winds check later. 
%Also potentially add a Wind Diversity metric instead of just opposing

The wind bins are defined by:  

\begin{equation}
Bin_i = \Bigl\{(i-1)\cdot \frac{360\degree}{N_b}-\theta_{C}, 
(i)\cdot \frac{360\degree}{N_b}-\theta_{C},
\mathrm{...}
\Bigr\} , i = 1,2...,8
\end{equation}

where $N_b$ is the number of bins and $\theta_C$ is an angle offset center the cardinal directions within the bins (i.e. North (0\degree) is centered in Bin 1 with a range of [-22.5\degree,22.5\degree])


The Total opposing wind score for a specific coordinate and single timestamp is then defined by:

\begin{equation}
T_{\mathrm{opposing}} = \sum_{i=1}^{N_a}(C_i+C_{i+\frac{N_b}{2}})
\end{equation}

where $N_a$ is the number of altitude levels with binned wind values and $C_i$ is the total count in each respective wind bin. 

Finally, to get an overall forecast score, $FS$, with multiple timestamps for a particular coordinate,

\begin{equation}\label{forecastScoreEquation}
FS = \frac{T_{\mathrm{opposing},t_0} + T_{\mathrm{opposing},t_1} + \mathrm{...}}{n_t}\cdot 100
\end{equation}

where $n_t$ is the number of timestamps evaluated for opposing winds within a forecast subset. 

\begin{table}[h]
\renewcommand{\arraystretch}{1.3}
\caption{Percentage of Forecast Scores of Zero from 10,000 Samples}
\centering
\begin{tabular}{|c|c|c|}
\hline
\bfseries Month & \bfseries ERA5 Model & \bfseries Synthetic Model \\
\hline%\hline
January 2023 & 34.3\% & 19.2\% \\
April 2023 & 23.3\% & 6.49\%\\
July 2023 & 8.56\% & 0.18\%\\
October 2023 & 73.4\% & 38.1\%\\
\hline
\end{tabular}
\label{tab:zeroFSScores}
\end{table}

\begin{table*}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Mean ERA5 \& Synthetic Model Forecast Scores}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\bfseries Month & \bfseries ERA5 Model & \bfseries Synthetic Model  & \bfseries Score Difference\\
\hline%\hline
January 2023 & $0.388 \pm 0.21$ & $0.529 \pm 0.26$ & $0.141 \pm 0.17$ \\
April 2023 & $0.332 \pm 0.19$ & $0.632 \pm 0.23$ & $0.299 \pm 0.19$\\
July 2023 & $0.545 \pm 0.23$ & $0.736 \pm 0.18$ & $0.191 \pm 0.19$\\
October 2023 & $0.196 \pm 0.13$ & $0.399 \pm 0.23$ & $0.203 \pm 0.19$\\
\hline
\end{tabular}
\label{tab:FSDistributionMean}
\end{table*}


We sampled 10,000 randomized forecast subsets from the Southwest United States region for 4 different evaluation months in different seasons to compare forecast score distributions. The forecast scores were simultaneously computed for both the ERA5 and synthetic wind models at each randomized coordinate. First, we sampled the 10,000 randomized forecast subsets including impossible winds for station keeping (all winds blowing in one direction, forecast score of 0).
Table \ref{tab:zeroFSScores} lists the percentage of each month's forecasts that are zero scores from an unfiltered score distribution.  The percentage of zero scores in the month of October is staggering, nearly 73.4\% of unfiltered forecasts in the ERA5 model contain no opposing winds.  


Due to the inherent bias towards zero scores during the winter and fall months, we then recomputed the Forecast Score distributions, filtering out impossible winds (Forecast Scores \> 0), shown in Figure \ref{fig:ERA5WindDistribution} and Figure \ref{fig:SynthWindDistribution}.  It is important to note that the ERA5 Forecast Score distribution shown in Fig. \ref{fig:ERA5WindDistribution} does not contain values below 0.7 because the ERA5 forecast altitude region we use only contains 7 pressure levels. %Given the forecast score calculation in Eq. \ref{forecastScoreEquation}, it is not possible for a non-zero score to exist within the [0.0, 0.0714] range. 

\begin{figure}[!htbp] % !htbp allows LaTeX to place the figure here, at the top, or at the bottom
    \centering
    % Subfigure 1
    \subfloat[ERA5 Filtered Forecast Score Distribution for SW USA]{\includegraphics[width=\linewidth]{img/ERA5_Wind_Probability.png}\label{fig:ERA5WindDistribution}}\\
    % Subfigure 2
    \subfloat[Synthetic Filtered Forecast Score Distribution for SW USA]{\includegraphics[width=\linewidth]{img/Synth_Wind_Probability.png}\label{fig:SynthWindDistribution}}
    
    \caption{Filtered Forecast Score Distributions for SW USA}
    \label{fig:windDistributions}
\end{figure}



Table \ref{tab:FSDistributionMean} shows the mean and standard deviation of the filtered distributions found in Figures \ref{fig:ERA5WindDistribution} and \ref{fig:SynthWindDistribution} as well as the mean and standard deviation of the computed difference in forecast scores between the ERA5 and synthetic wind forecasts. These distributions suggest that it may be difficult to achieve station-keeping during the fall and winter months due to the low opposing wind probability. However, it is worth noting that these seasonal distributions may vary depending on the geographic region.

\begin{table}[b]
\renewcommand{\arraystretch}{1.3}
\caption{Overall Mean and Standard Deviation TWR50 Station Keeping Performance for four months in 2023}
\label{tab:overallTWR50}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\bfseries Month   & \bfseries Mean  & \bfseries SD & \bfseries Mean & \bfseries SD \\ 
        & \bfseries TWR50 & \bfseries TWR50   & \bfseries FS   & \bfseries FS \\
\hline
January &  20\%      &  0.25 & 0.38 & 0.21   \\
April   &  39\%      &  0.31 &  0.33 & 0.2  \\
July    &  51\%       &  0.31   & 0.54  & 0.23  \\
October &  31\%         & 0.23   & 0.19  & 0.13 \\ \hline
\end{tabular}%
\end{table}

For all four months selected, the ERA5 forecast score distribution underestimates the synthetic wind model distribution. This can be attributed to several factors, namely the lesser pressure level resolution. As mentioned in Section \ref{section:SimEnvforDQN}, the ERA5 reanalysis model on individual pressure levels operates at a vertical resolution of 7 pressure levels. Conversely, the synthetic wind model operates at a vertical resolution of 46 pressure levels. As a result, the forecast score computation allows for a naturally higher score for the synthetic wind model due to the availability of more in-between pressure levels. 


As shown previously in Figures \ref{fig:01_17_50_c} and \ref{fig:07_17_50_c}, there can be significant variation in wind direction and magnitude at certain pressure levels which may also contribute to the non-uniform distribution of forecast scores. We conducted additional analysis to quantify the magnitude variation across different pressure levels in different seasons. These four figures are shown in the Appendix. 
%The analysis computed the mean difference of the wind direction of the entire Southwestern United States region at each of 5 common pressure levels found in the ERA5 and the synthetic model: 30 hPa, 50 hPa, 70 hPa, 100 hPa, 125 hPa. This analysis was conducted for each of the four seasons at 12-hour intervals, matching the true resolution of the synthetic wind model 
January and July had relatively lower variance between ERA5 and synthetic forecasts than the seasonal transition months of April and October.
 %supporting a conclusion drawn in previous work (Tristan's preprint paper)\cite{}. Cite this Once published.
 These plots also consistently show that the pressure levels of 50 hPa and 70 hPa exhibit higher variation than other pressure levels.
 %Tristan add a bit more discussion here.
 
 
%This may be due to their characterization as a transition region, in which there exist more inflection points, or turbulence, that may inhibit smooth interpolation in the synthetic wind model.

% First look for the mean difference and standard deviation in the forecast distributions for each month. These are evaluated on the same forecasts in the 10k sample. This will lead into a discussion on the actual direction difference between the ERA5 and synth models that could contribute to the differences we see in the scores as well. 
% On the distro plots
% Explain the distributions (zero not included) by prpviding the mean and standard deviation
 

\subsection{Forecast Score and TWR Evaluation}\label{section:Discussion_FS&TWR}

%Table of Months and overall TWR's


%While the Forecast Score distribution changes depending on the month as seen in Figure \ref{fig:SynthWindsDistribution}, the TWR50 scores vs Forecast Scores have similar overall probabilities with low scores resulting in low TWR50 scores and high scores resulting in high TWR50 scores, as shown in the heatmaps in Figure \ref{fig:Heatmaps}.   The biggest variation in heatmaps is in the top right corner with High Scores and High TWR50 Scores where April and October,  transition months have both higher probabilities of High TWR50 Scores, but also higher probabilities for Forecast scores in the 0.6-0.8 range compared with the months of July and January. 

%\textcolor{red}{Different months have different thresholds (where high probabilities start), the point at which the }


\begin{figure*}[!t]  % 't' places the figure at the top of the page
  \centering
  \begin{subfigure}[t]{0.25\textwidth}  % 25% width for each subfigure
    \centering
    \includegraphics[width=\textwidth]{img/January-Heatmap-Freq.png}  
    \caption{January} \label{fig1}
  \end{subfigure}%
  \begin{subfigure}[t]{0.25\textwidth}  % No space between subfigures
    \centering
    \includegraphics[width=\textwidth]{img/April-Heatmap-Freq.png}  
    \caption{April} \label{fig2}
  \end{subfigure}%
  \begin{subfigure}[t]{0.25\textwidth}  % No space between subfigures
    \centering
    \includegraphics[width=\textwidth]{img/July-Heatmap-Freq.png}  
    \caption{July} \label{fig3}
  \end{subfigure}%
  \begin{subfigure}[t]{0.25\textwidth}  % No space between subfigures
    \centering
    \includegraphics[width=\textwidth]{img/October-Heatmap-Freq.png}
    \caption{October} \label{fig4}
  \end{subfigure}

\caption{Frequency Distribution of Forecast Scores vs TWR50 evaluated with a trained DQN model on the months of January, April, July, and October 2023}
\label{fig:Heatmaps}
\end{figure*}

%10 plots Forecast Scores (label TWR, Forecast Score, Date)



\begin{figure*}[!b]  % 't' places the figure at the top of the page
  \centering
  \begin{subfigure}[t]{0.25\textwidth}  % 25% width for each subfigure
    \centering
    \includegraphics[width=\textwidth]{img/Jan-100-2.png}  
    \caption{January} \label{fig1}
  \end{subfigure}%
  \begin{subfigure}[t]{0.25\textwidth}  % No space between subfigures
    \centering
    \includegraphics[width=\textwidth]{img/Apr-100.png}  
    \caption{April} \label{fig2}
  \end{subfigure}%
  \begin{subfigure}[t]{0.25\textwidth}  % No space between subfigures
    \centering
    \includegraphics[width=\textwidth]{img/Jul-100.png}  
    \caption{July} \label{fig3}
  \end{subfigure}%
  \begin{subfigure}[t]{0.25\textwidth}  % No space between subfigures
    \centering
    \includegraphics[width=\textwidth]{img/Oct-100-2.png}
    \caption{October} \label{fig4}
  \end{subfigure}

\caption{A Sampling of Station Keeping Trajectories with High TWR50s for each of the evaluation months in 2023}
\label{fig:Trajectories}
\end{figure*}

We evaluated our best-performing preliminary DQN model for 5,000 episodes each on 4 evaluation months in different seasons (January, April, July, and October). Table \ref{tab:overallTWR50} shows the overall station-keeping performance in each of the four evaluation months, with July having the highest Mean TWR50 as well as the highest Mean Forecast Score. Interestingly, April followed by October had the next highest Mean TWR50's at 20\% and 31\% respectively while having lower mean Forecast scores than January, which had the lowest Mean TWR50 of only 20\%. Overall, the standard deviations between the TWR50 and Forecast scores were similar across all four evaluation months, even though the different months have significantly different wind profiles. While Table \ref{tab:overallTWR50} shows a high-level summary between TWR50 and Forecast Score in different months, individual months have their own unique trends. 


Figure \ref{fig:Heatmaps} shows initial TWR50 vs Forecast Score frequency distribution heatmaps from the same evaluation months on the same best-performing DQN model.  We filtered out the impossible winds for station keeping as discussed with the Forecast Score Distributions in the previous section, hence why each of the heatmaps have empty first two columns. We also filter out bins with less than 5 data points. The heatmaps forecast score distribution follows the aforementioned trends, where July has best wind diversity and October is dominated by poor winds. The occurrence of higher TWR50s also follows the same trend, with July having the most occurrences of perfect scores and October having the least. Because the Forecast Score is based on ERA5 forecasts, and not the synthetic winds, there are occurrences of perfect scores in all of the months.  Initial training on "perfect forecasts" using synthetic wind forecasts for both the wind column observation as well as the horizontal movement, suggests that significantly higher probabilities of perfect TWR50 scores (over 50\%) for high forecast scores (over 0.8) are possible with more forecast information. Figure \ref{fig:Trajectories} shows examples of unique station-keeping trajectories for HABs in each of the 4 evaluation months using the same trained DQN.  



