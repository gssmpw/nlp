\section{Station Keeping with DQN}\label{section:SimEnvforDQN}

To successfully perform station-keeping, HABs typically oscillate between altitudes with opposing winds, stay in calm wind regions ($<$ 2m/s) regions, or some combination of the two. Most altitude-controlled HAB platforms operate between 18-28 km, above the typical max altitude regulation for many countries (such as FL600 for the USA).  Luckily, this region of the atmosphere frequently has vertical wind diversity for maneuvering HABs, where different altitudes can have opposing winds. The probability of available diverse winds in a region of interest substantially differs geographically and seasonally. 
%cite david brown and Tristan Schuler papers

%Problem Formulation?
Throughout a flight, the HAB can adjust altitude any number of times, as well as leave and re-enter the region any number of times, resulting in many unique trajectories and solutions to the problem. We use deep reinforcement learning to approach the station-keeping problem as opposed to a more deterministic controller because real-world wind flows are extremely complex and dynamic, as well as often heavily differing from weather forecasts.  For a reinforcement learning approach to station keeping, we design the simulation environment in a game-like structure to learn a policy, introducing simple actions, observation states, and positive rewards for making good actions towards the station-keeping goal.  We implement the popular DQN deep reinforcement learning algorithm to train a policy that maximizes rewards based on state action pairs. Training the DQN policy involves running hundreds of thousands of individual simulations, further referred to as episodes, updating the neural network as new conditions are encountered, and building off of past experiences to determine optimal actions for any given state. For evaluating trained agents, we use a time within the region of 50 km (TWR50) metric to measure station-keeping success.

%\subsubsection{Station-Keeping Success Metric}\label{section:SimEnvforDQN_StationKeepingSuccessMetric}For evaluating trained agents, we use a time within region of 50 km (TWR50) metric to measure station-keeping success rather than the mean reward.  While rewards
%are necessary for training a DQN policy, the rewards are arbitrary values that vary between different reward functions and does not always necessarily equate to successful station-keeping.  For example, in a case where there are winds such that a HAB circled a station in a 60 km radius using the piecewise reward function, the balloon would have a high overall reward, but a TWR50 score of 0.

\subsection{Simulation Setup}\label{section:SimEnvforDQN_SimStructure}

%Our Python HAB simulation environment is wrapped in a custom gym environment, a standardized framework for testing RL algorithms. %Training agents with DQN is done using the popular StableBaselines3 codebase \cite{brockman2016openai, stable-baselines3}.  

In the simulation, we use $u$ and $v$ wind data from ERA5 Reanalysis as our forecast, and pre-generated synthetic winds (discussed in Section \ref{section:SynthForecastModeling}) as our ground truth for dictating HAB movement. ECWMF ERA5 Reanalysis forecasts provide a large collection of historical atmospheric variables through reanalysis, including wind vectors at 137 static pressure levels at a maximum horizontal resolution of 0.25\degree \cite{hersbach2018era5}. For operational balloon missions ERA5 would be replaced by a forward-looking forecast and Synthetic winds would be the true unknown winds. At the start of an episode, we prepare a HAB arena (150 x 150 x 10 km) with a subset of 3D flow fields derived from a random coordinate within the bounds of a pre-downloaded and processed master forecast (e.g., Southwestern United States).  The HAB is then initialized to a random altitude at the station coordinate. We standardized the episode length to be 20 hours long at a resolution of 1 minute. %, even though SHAB-Vs cannot fly at night, to improve the experience buffer. 
At each simulation step, the horizontal flow is applied according to the $u$ and $v$ wind components from the synthetic winds forecast, while the observation space is updated with relative wind profiles from the ERA5 forecast. 
Throughout an episode, intermediate rewards are appended to an overall score based on the current observation after executing an action; the DQN policy aims to maximize the overall reward by training the network to choose the best intermediate actions.
 Figure \ref{fig:simulator} shows a snapshot of a trained DQN agent station-keeping in the simulation environment GUI.  The simulator GUI shows the 3D trajectory, altitude profile, and corresponding ERA5 and synthetic forecasts for the simulated coordinates and timestamp.  


%Talk about seasonal differences for station keeping?
% Replacing a controller due to complex dymanic flows.  We can mention synthetic winds for making more robust, and realastic to errors. Goal, is to stay within TWR50, and we want to train and evaluate in such a way to find seasonal differenecs.  BASED ON PREVISOU WORK. Urban Sky, Tristan.  Seaonal exploration   

\subsection{Solar Balloon Dynamics}\label{section:SimEnvforDQN_SolarBalloonDynamics}

For training short-duration HABs with DQN, we selected Vented Solar High Altitude Balloons (SHAB-Vs) for simulating HAB dynamics. Solar balloons utilize a lightweight material that absorbs solar radiation, providing a free source of lift and eliminating the need for lighter than air gas or heat source. Solar balloons can fly as long as there is unobstructed sunlight; 12 hours in equatorial regions but up to 16+ hours at higher latitudes in the summer months. Researchers have logged hundreds of flight hours with the original heliotrope design as well as modified versions, lifting up to 3 kg payloads and conducting multi-hour stratospheric research \cite{bowman2020multihour, schuler2022solar, swaim2024performance, Lien2024EarthSHAB}.  Solar balloons have additionally been proposed as a feasible platform for exploring Venus' atmosphere due to the thick, highly reflective cloud layer \cite{schuler2022long}. Recent development of a novel mechanical vent for solar balloons allows SHAB-Vs to adjust altitude precisely between a typical altitude range of 15-23 km and demonstrated initial manual station-keeping in the Southwestern United States \cite{schuler2023altitude}.  




%Wand applied a simplification of the dynamics model

%\begin{equation}\label{horizontalMotion}
%    \begin{split}
%        x &= x_{0} + v_{x}\delta t \\
%        y &= y_{0} + v_{y}\delta t
%    \end{split}
%\end{equation}
 %The wind velocity vector ($v_x$, $v_y$) is applied from the pre-generated synthetic wind forecasts, discussed in Section \ref{}. %It is a function of the current location of the agent in the Geographic Coordinate System (GCS). At each time step, the Cartesian coordinate of the agent is converted into GCS latitude, longitude, and altitude. This GCS coordinate is used to query the synthetic wind data for the wind velocity vector in the Cartesian coordinate frame. 
 %At a macroscopic scale, these ascent and descent rates can apply to all heliotropes of comparable envelope volume and payload mass. \textcolor{red}{Do we need to elaborate on why we think this is true?}

 \subsection{Action Space}

At each simulation step, the SHAB-V agent can perform three discrete actions: ascend, maintain altitude, or descend. In heliotrope solar balloon flight experiments conducted in July-September 2021, Swain et al. showed that the average ascent and descent rates of non-vented heliotrope balloons are 1.8 $\pm$ 0.14 and 2.8 $\pm$ 0.3 m/s respectively \cite{swain2024heliotrope}. Additionally, SHAB-V flight experiments conducted by Schuler et al. implemented an altitude controller with standard deviation $\sigma$ = 1.25 m/s. We simulate the 3 discrete actions and reasonable errors by mapping each venting action to the normal distributions found in Table~\ref{tab:actionspace}. These distributions holistically represent the motion profile experienced via the onboard altitude controller.

% \begin{equation}\label{verticalMotionDistribution}
%     \begin{split}
%         A &\sim N(\mu = 1.80, \sigma = 0.14) \\
%         D &\sim N(\mu = 2.80, \sigma = 0.30) \\
%         S &\sim N(\mu = 0.00, \sigma = 1.25) \\
%     \end{split}
% \end{equation}

\begin{table}[h]
\renewcommand{\arraystretch}{1.2}
\caption{Action Space for Training}
\centering
\begin{tabular}{|c|c|c|}
\hline
\bfseries Action & \bfseries Sampling Distribution \\
\hline%\hline
Ascend & $1.80 \pm 0.14$ m/s\\
Descend & $2.80 \pm 0.30$ m/s\\
Stay & $0.00 \pm 1.25$ m/s\\
\hline
\end{tabular}
\label{tab:actionspace}
\end{table}

Yajima et al. showed that the lateral motion of HABs is a function of the relative velocity between the balloon and the wind and dictated by the composition of the balloon, payload mass, and varying atmospheric properties at different altitudes \cite{yajima2009ballooning}. Smaller radiosonde balloons (of mass 0.3 kg to 2 kg) require between 20 to 40 seconds to respond to a 2 m/s velocity step change \cite{yajima2009ballooning}. Over large distances and multi-hour timescales, this delay in matching lateral wind velocities creates small deviations when considering station-keeping trajectories that are constantly adapting to wind flow changes. Additionally, winds typically experience a gradual shift between altitudes of different directional flows, with turbulent regions occurring less often. With these conditions, we assume the lightweight SHAB-Vs match the wind instantaneously in our simulation environment. Horizontal wind velocities from pre-computed synthetic forecasts, discussed in Section \ref{section:SynthForecastModeling}, are applied to the SHAB-V over the course of a 60-second time step ($\delta t$). A 60-second time step allows for reasonable simulation training time and also renders the time required to reach zero relative velocity negligible.

%The value sampled from the corresponding normal distribution are then applied as instantaneous velocity $v_z$ via Eulerian integration in Eq. \ref{verticalMotion} at each simulation time step. Similar to the horizontal motion dynamics, the $\delta t$ of one minute provides a viable high-level approximation of the PID or other low-level control schema. 

%\begin{equation}\label{verticalMotion}
%    z = z_{0} + v_{z}\delta t
%\end{equation}


\begin{figure*}[t]
    \centering 
    \includegraphics[width=\textwidth]{img/Simulator.png} 
    \caption{Altitude Profile and 3D trajectory of a trained DQN HAB agent in simulation with ERA5 forecasts as observation, and synthetic winds for movement.}
    \label{fig:simulator} 
\end{figure*} 

\subsection{Observation Space}
\begin{table}[h]
\renewcommand{\arraystretch}{1.2}
\caption{Observation Space for Controller Input}
\centering
\begin{tabular}{|c|c|}
\hline
\bfseries Observation & \bfseries Range\\
\hline%\hline
HAB Altitude & $15 - 25$ km\\
Relative Distance & $0 - \infty$ km\\
Relative Bearing & $0 - 180\degree$\\
Wind Column & \{Altitude, Magnitude, \\
            & Relative Bearing\} * N \\
\hline
\end{tabular}
\label{tab:observationspace}
\end{table}

The observation space for training the DQN policy is a subset of the agent's full state and is shown in Table \ref{tab:observationspace}. This observation space does not include all simulator state information such as velocity, coordinates, or an entire forecast as DRL algorithms may take longer to train as a function of a more complex observation space. By converting coordinates to relative distance and bearing, we are able to reduce the dimensionality of the observation vector.
The relative distance and relative bearing are from the balloon to the central coordinate for station-keeping where the maximum relative bearing is 180\degree and would be a HAB traveling on a trajectory directly away from the central coordinate. The wind column is a triple that encodes relative wind velocities at the current coordinate to the central coordinate from an ERA5 forecast along with the true altitude of those coordinates. The number of levels included in the wind column remains static for the entire training set (for the altitude region of 15-25 km we include 7 levels between 20 and 150 hPa). Because ERA5 is a pressure-based forecast and we extrapolate the altitude values from the geopotential variable, altitude is not constant across a forecast and why we include it in the wind column triple.  For small forecast areas (only a few degrees of latitude and longitude across) the variance in altitude at the same pressure level is typically within $\pm$ 100 m.  By including true ERA5 altitudes in the wind column, the DRL algorithm is able to implicitly learn discrepancies between ERA5 and the synthetic winds in different seasons.
%^New sentence based on vconversation with Tony
In the future, we plan to experiment with expanding the number of altitude levels in the wind column vector based on a combination of ERA5 forecasts and previous actions, experimenting with relative altitude values, and including uncertainty estimates.

%\subsubsection{Forecasts for Training}\label{section:SimEnvforDQN_ForecastforTraining}
%The horizontal movement of the HAB agent is based on the synthetic winds discussed in Section \ref{SynthForecastModeling_SynthWindDataGeneration}, not the ERA5 based wind column in the observation space. The synthetic forecasts and ERA5 forecasts are assumed to already be downloaded and formatted correctly before initiating DRL simulations. 
%At the start of a training, a primary ERA5 and Synthetic forecast of the same large area (e.g., Southwestern United States) for the desired altitude range (15-25 km) and time frame (typically a single month) is loaded into RAM.  At the beginning of each episode, a smaller multi-dimensional forecast subset array is generated from a randomized central coordinate and includes a relative X and Y distance of forecast data; we used 150 km. This subset is used to produce the horizontal wind components as a function of altitude. The range of 150 km ensures that the HAB has sufficient spatial awareness, without requiring the use of extensive RAM for training. 

\subsection{Reward Functions}\label{section:SimEnvforDQN_RewardFunctions}

For short-duration station-keeping, we experimented with the distance-based reward proposed by Bellemare et al. in Eq. \ref{eq:google} as well as some variations \cite{bellemare2020autonomous}. For short-duration station-keeping missions we had the best performance with the piecewise reward function that introduces an inner radius as shown in Eq. \ref{piecewise_reard}.  For short-duration HAB station-keeping, if the winds have good station-keeping probabilities, they frequently do throughout the entire episode, whereas multi-day or multi-week flights can experience significantly different wind profiles over a longer time scale. The Euclidean Reward function in Eq. \ref{euclidan_reard} performed slightly worse than the piecewise reward function and about the same as Google's reward function in our simulation environment, and we believe would be the best candidate for navigating to waypoints as opposed to assuming the HAB is already in the station-keeping region. 
%^Need to explain that better

With SHAB-Vs, we ignore power as a resource constraint because the HABs can lift several kilogram payloads and can carry enough payload to continuously vent. Similarly, longer duration altitude controlled balloon platforms such as air ballasted super pressure HABs and vent and ballasted zero pressure HABs could also reduce or eliminate power constraints for short duration flights.  Unlike other HAB platforms, SHAB-Vs do not have a lighter-than-air gas as a resource constraint because the lift is generated via heating ambient atmospheric air.  Therefore, the altitude profile and horizontal motion are the most important dynamics to replicate.

Google Loon Reward Function:

\begin{equation}
r_{\mathrm{dist}}(\Delta) =
    \begin{cases}
        1.0 & \text{if } \Delta < \rho_{50km} \\
        c_{\mathrm{cliff}}2^{-(\Delta-\rho)/\tau)} & \text{otherwise }
    \end{cases}
    \label{eq:google}
\end{equation}

Piecewise Reward Function:

\begin{equation}
r_{\mathrm{dist}}(\Delta) =
    \begin{cases}
        2.0 & \text{if } \Delta \leq \rho_{25km} \\
        1.0 & \text{if }  \rho_{25km} < \Delta \leq \rho_{50km} \\
        c_{\mathrm{cliff}}2^{-(\Delta-\rho)/\tau)} & \text{otherwise }
    \end{cases}
    \label{piecewise_reard}
\end{equation}

Euclidean Reward Function:

\begin{equation}
r_{\mathrm{dist}}(\Delta) =
    \begin{cases}
        |\Delta| & \text{if } \Delta < \rho_{50km} \\
        c_{\mathrm{cliff}}2^{-(\Delta-\rho)/\tau)} & \text{otherwise }
    \end{cases}
    \label{euclidan_reard}
\end{equation}



\subsection{Training and Hyperparameter Tuning}\label{section:SimEnvforDQN_TrainingAndHyperparam}
%Do we show a reward + TWR training plot here for a few months?
%explain exploration rate. 
%Possibly explain most important hyperparameters
%cite optuna?

Figure \ref{fig:Learning} shows an example of one of the best-performing trained models for an HAB in the month of April trained for 150 million time steps and reaching an average TWR50 of 75\%. Training was only done on forecasts with decent wind diversity, so the final TWR50 is higher than the overall mean. The TWR50 and TWR75 closely follow the mean reward learning curve whereas the inner TWR25 is much lower. The mean reward started to plateau toward the end of training. However, all 3 of the TWR metrics are still increasing, suggesting longer training times could lead to improved station-keeping results.  

\begin{figure}[h]
    \centering 
    \includegraphics[width=.5\textwidth]{img/Learning.png}
    \caption{Learning curve over 1.5 million timesteps for a station-keeping DQN HAB agent} 
    \label{fig:Learning} 
\end{figure}


The best-trained models for short-duration HAB station-keeping are extremely sensitive to hyperparameters of the DQN algorithm as well as the geographic area and season where the agent was trained. We used Optuna, an open-source automated hyperparameter tuning framework, to train our initial models \cite{akiba2019optuna}. Overall, the learning rate and exploration rate had the most significant impacts on success. The best-performing models had learning rates between 1e-5 and 1e-4 and a high exploitation to exploration ratio (how often the agent takes random actions as opposed to picking actions from the trained policy.) For the exploration strategy, the best-performing models started with taking random actions approximately 25-50\% of the time at the beginning of training and linearly decayed for about a third of the overall training time to take random actions 10-20\% of the rest of the time, a more deterministic policy. 

%To avoid overfitting, we trained individually on the first month of each quarter but evaluated the monthly model on the entire region for all four months. In The Southwestern United states that are huge differences in winds between the summer and winter months.. 
%expand more here when we have better models

%Are we overfitting? 
%Should we evaluate on another year.  Should we combine all the months?

From preliminary investigations, we noticed that models trained on one region (e.g., the Southwestern United States) but evaluated on another region (e.g., an equatorial region) performed significantly worse than models trained on the other region.  The primary reason is that the HAB agent is exposed to wind flows never seen before when only training on a medium-sized region. Equatorial regions typically have significantly higher wind diversity in all dimensions (vertically, horizontally, and temporally) than the Southwestern United States, and also more deviations between ERA5 and synthetic forecasts.  Combining models, implementing curriculum learning, and other optimization strategies could help the trained DRL agents be more robust to much larger areas and timescales, but this is currently out of the scope of this paper for short-duration HABs. 















