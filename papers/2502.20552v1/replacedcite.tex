\section{Related Work}
____ explored cross-lingual AMR parsing for English, Dutch, and Spanish using both multilingual and monolingual configurations of BART-large. Their findings indicate that monolingual models consistently outperform multilingual ones, highlighting the limitations of multilingual approaches in this context.

Instruction fine-tuned language models show strong generalization to unseen tasks ____. However, standard parsing tasks like AMR are usually excluded from such instruction datasets and models. ____ experimented with FLAN-T5 instruction fine-tuned models showing significant improvement in AMR parsing over earlier BART ____ based models. Their work included a two step training approach: first a full fine-tuning and then a parameter efficient fine-tuning using Low Rank Adaptation (LoRA) ____.


____ created MASSIVE-AMR, a cross-lingual AMR dataset in the domain of question answering (QA) consisting of 1,685 utterances which are localized in 52 languages, Hungarian included. However the dataset contains some problems, as entities are inconsistently translated or left untranslated, and sometimes appear in inflected forms, making the parsing process difficult. It also lacks punctuation marks at the end of sentences, which are crucial as they often indicate parts of the AMR.

More recently, ____ experimented with cross-lingual AMR parsing across 13 languages using a meta-learning model. They found that fine-tuning a model on a small number of examples from previously unseen languages failed to improve parser performance, underscoring the challenges of cross-lingual generalization in AMR parsing.