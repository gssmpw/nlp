\section{Related Work}
Li, "Cross-lingual AMR Parsing for English, Dutch, and Spanish using BART-large" explored cross-lingual AMR parsing for English, Dutch, and Spanish using both multilingual and monolingual configurations of BART-large. Their findings indicate that monolingual models consistently outperform multilingual ones, highlighting the limitations of multilingual approaches in this context.

Instruction fine-tuned language models show strong generalization to unseen tasks **Rae, "Combining Multiple Autoencoders: Improving Zero-Shot Transfer with Multi-Cycle Consistency"**. However, standard parsing tasks like AMR are usually excluded from such instruction datasets and models. **Li, "Cross-lingual AMR Parsing for English, Dutch, and Spanish using BART-large"** experimented with FLAN-T5 instruction fine-tuned models showing significant improvement in AMR parsing over earlier BART **Li et al., "BART: Denoising Sequence-to-Sequence Pre-training for Language Translation"** based models. Their work included a two step training approach: first a full fine-tuning and then a parameter efficient fine-tuning using Low Rank Adaptation (LoRA) **Liu, "Low-Rank Adaptation for Efficient Fine-Tuning of Pre-Trained Language Models"**.


**P18, "Cross-Lingual AMR Parsing with MASSIVE-AMR Dataset"** created MASSIVE-AMR, a cross-lingual AMR dataset in the domain of question answering (QA) consisting of 1,685 utterances which are localized in 52 languages, Hungarian included. However the dataset contains some problems, as entities are inconsistently translated or left untranslated, and sometimes appear in inflected forms, making the parsing process difficult. It also lacks punctuation marks at the end of sentences, which are crucial as they often indicate parts of the AMR.

More recently, **Gupta et al., "Cross-Lingual Meta Learning for AMR Parsing"** experimented with cross-lingual AMR parsing across 13 languages using a meta-learning model. They found that fine-tuning a model on a small number of examples from previously unseen languages failed to improve parser performance, underscoring the challenges of cross-lingual generalization in AMR parsing.