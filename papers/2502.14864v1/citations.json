[
  {
    "index": 0,
    "papers": [
      {
        "key": "izacard2022few",
        "author": "Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard",
        "title": "Few-shot learning with retrieval augmented language models"
      },
      {
        "key": "zhang2024raft",
        "author": "Zhang, Tianjun and Patil, Shishir G and Jain, Naman and Shen, Sheng and Zaharia, Matei and Stoica, Ion and Gonzalez, Joseph E",
        "title": "Raft: Adapting language model to domain specific rag"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chen2022murag",
        "author": "Chen, Wenhu and Hu, Hexiang and Chen, Xi and Verga, Pat and Cohen, William W",
        "title": "Murag: Multimodal retrieval-augmented generator for open question answering over images and text"
      },
      {
        "key": "zhao2023retrieving",
        "author": "Zhao, Ruochen and Chen, Hailin and Wang, Weishi and Jiao, Fangkai and Do, Xuan Long and Qin, Chengwei and Ding, Bosheng and Guo, Xiaobao and Li, Minzhi and Li, Xingxuan and others",
        "title": "Retrieving multimodal information for augmented generation: A survey"
      },
      {
        "key": "zhao2024retrieval",
        "author": "Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Cui, Bin",
        "title": "Retrieval-augmented generation for ai-generated content: A survey"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "yao2024minicpm",
        "author": "Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others",
        "title": "MiniCPM-V: A GPT-4V Level MLLM on Your Phone"
      },
      {
        "key": "sailvl",
        "author": "Bytedance Douyin Content Team",
        "title": "SAIL-VL: Scalable Vision Language Model Training with High Quality Data Curation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ma-etal-2024-unifying",
        "author": "Ma, Xueguang  and\nLin, Sheng-Chieh  and\nLi, Minghan  and\nChen, Wenhu  and\nLin, Jimmy",
        "title": "Unifying Multimodal Retrieval via Document Screenshot Embedding"
      },
      {
        "key": "faysse2024colpali",
        "author": "Manuel Faysse and Hugues Sibille and Tony Wu and Bilel Omrani and Gautier Viaud and C\u00e9line Hudelot and Pierre Colombo",
        "title": "ColPali: Efficient Document Retrieval with Vision Language Models"
      },
      {
        "key": "yu2024visrag",
        "author": "Yu, Shi and Tang, Chaoyue and Xu, Bokai and Cui, Junbo and Ran, Junhao and Yan, Yukun and Liu, Zhenghao and Wang, Shuo and Han, Xu and Liu, Zhiyuan and others",
        "title": "Visrag: Vision-based retrieval-augmented generation on multi-modality documents"
      },
      {
        "key": "Methani_2020_WACV",
        "author": "Methani, Nitesh and Ganguly, Pritha and Khapra, Mitesh M. and Kumar, Pratyush",
        "title": "PlotQA: Reasoning over Scientific Plots"
      },
      {
        "key": "mathew2021docvqa",
        "author": "Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV",
        "title": "Docvqa: A dataset for vqa on document images"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "marino2019ok",
        "author": "Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh",
        "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge"
      },
      {
        "key": "talmor2021multimodalqa",
        "author": "Talmor, Alon and Yoran, Ori and Catav, Amnon and Lahav, Dan and Wang, Yizhong and Asai, Akari and Ilharco, Gabriel and Hajishirzi, Hannaneh and Berant, Jonathan",
        "title": "Multimodalqa: Complex question answering over text, tables and images"
      },
      {
        "key": "schwenk2022okvqa",
        "author": "Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh",
        "title": "A-okvqa: A benchmark for visual question answering using world knowledge"
      },
      {
        "key": "masry2022chartqa",
        "author": "Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul",
        "title": "Chartqa: A benchmark for question answering about charts with visual and logical reasoning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "hu2024mragbench",
        "author": "Hu, Wenbo and Gu, Jia-Chen and Dou, Zi-Yi and Fayyaz, Mohsen and Lu, Pan and Chang, Kai-Wei and Peng, Nanyun",
        "title": "MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models"
      },
      {
        "key": "li2024benchmarking",
        "author": "Li, Yangning and Li, Yinghui and Wang, Xingyu and Jiang, Yong and Zhang, Zhen and Zheng, Xinran and Wang, Hui and Zheng, Hai-Tao and Yu, Philip S and Huang, Fei and others",
        "title": "Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent"
      },
      {
        "key": "zhou2024megapairs",
        "author": "Zhou, Junjie and Liu, Zheng and Liu, Ze and Xiao, Shitao and Wang, Yueze and Zhao, Bo and Zhang, Chen Jason and Lian, Defu and Xiong, Yongping",
        "title": "MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "dong2025mmdocir",
        "author": "Dong, Kuicai and Chang, Yujing and Goh, Xin Deik and Li, Dexun and Tang, Ruiming and Liu, Yong",
        "title": "MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents"
      },
      {
        "key": "ma2024mmlongbench",
        "author": "Ma, Yubo and Zang, Yuhang and Chen, Liangyu and Chen, Meiqi and Jiao, Yizhu and Li, Xinze and Lu, Xinyuan and Liu, Ziyu and Ma, Yan and Dong, Xiaoyi and others",
        "title": "Mmlongbench-doc: Benchmarking long-context document understanding with visualizations"
      },
      {
        "key": "ding2024mvqa",
        "author": "Ding, Yihao and Ren, Kaixuan and Huang, Jiabin and Luo, Siwen and Han, Soyeon Caren",
        "title": "MVQA: A Dataset for Multimodal Information Retrieval in PDF-based Visual Question Answering"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "es2023ragas",
        "author": "Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven",
        "title": "Ragas: Automated evaluation of retrieval augmented generation"
      },
      {
        "key": "abaskohi2024fm2ds",
        "author": "Abaskohi, Amirhossein and Gella, Spandana and Carenini, Giuseppe and Laradji, Issam H",
        "title": "FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering"
      },
      {
        "key": "mathew2021docvqa",
        "author": "Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV",
        "title": "Docvqa: A dataset for vqa on document images"
      },
      {
        "key": "li2024multimodal",
        "author": "Li, Lei and Wang, Yuqi and Xu, Runxin and Wang, Peiyi and Feng, Xiachong and Kong, Lingpeng and Liu, Qi",
        "title": "Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models"
      },
      {
        "key": "wu2024synthetic",
        "author": "Wu, Ian and Jayanthi, Sravan and Viswanathan, Vijay and Rosenberg, Simon and Pakazad, Sina and Wu, Tongshuang and Neubig, Graham",
        "title": "Synthetic multimodal question generation"
      }
    ]
  }
]