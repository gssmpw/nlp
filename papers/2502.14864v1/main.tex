
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% \usepackage{hyperref}
\definecolor{uclablue}{rgb}{0.15, 0.45, 0.68}
\usepackage[
    pagebackref,
    breaklinks,
    citecolor=uclablue,
    colorlinks=true,
    % linkcolor=red
]{hyperref}

\usepackage{url}
\setlength {\marginparwidth }{2cm} 
\usepackage{multirow} 
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{float}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb} % http://ctan.org/pkg/amssymb
\usepackage{caption}
\usepackage{xparse} % data cards
\usepackage{wrapfig} % wrapped table
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{fancyhdr}
\usepackage[hang,flushmargin]{footmisc} % remove indentation in footnote
\usepackage{pifont} % http://ctan.org/pkg/pifont
\usepackage{color}
\usepackage{wrapfig} % wrapped table
\usepackage{boxedminipage}
\usepackage{framed}
\usepackage{soul}
\usepackage{xspace}
\usepackage{booktabs} % professional-quality tables
% \usepackage{courier} % light font weight
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{vcell}
\usepackage{colortbl}
\usepackage[nodisplayskipstretch]{setspace}
\usepackage{seqsplit} 
\usepackage{array}
\usepackage{todonotes}
\usepackage{CJKutf8} % Load the CJKutf8 package(Chinese)
\usepackage[normalem]{ulem}
\usepackage{enumitem}
\usepackage{etoc}
\usepackage{tcolorbox}

\newcommand{\zd}[1]{\textcolor{cyan}{\bf\small [#1 --ZD]}}
\newcommand{\zdc}[2]{\textcolor{cyan}{\sout{#1} #2}}
\newcommand{\jg}[1]{\textcolor{blue}{\bf\small [#1 --JG]}}

\hyphenpenalty=5000
\tolerance=1000


%%%%%%%%%%%%%%% Author-added commands 
\newcommand{\dataset}{\textsc{MRAG-Bench}\xspace}

\definecolor{my_green}{RGB}{51,102,0}
\definecolor{my_yellow}{RGB}{255,165,0}
\definecolor{my_red}{RGB}{204, 0, 0}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\magenta}[1]{\textcolor{magenta}{#1}}
\newcommand{\green}[1]{\textcolor{my_green}{#1}}
\newcommand{\yellow}[1]{\textcolor{my_yellow}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}

%%%%%%%%%%%%%%% Author-added commands (formats)
\newcommand{\header}[1]{\text{#1}}
% \newcommand{\header}[1]{\textbf{#1}} % table header in bold

\definecolor{backred}{RGB}{255, 190, 190}
% \definecolor{backblue}{RGB}{208, 230, 251}
\definecolor{backblue}{RGB}{210, 230, 250}
\definecolor{myblue}{RGB}{6, 174, 226}
\newcommand{\high}{\cellcolor{backblue!75}}
\newcommand{\highclass}{\cellcolor{backblue}}
\newcommand{\best}{\cellcolor{backred!50}}
\newcommand{\bestclass}{\cellcolor{backred}}

\definecolor{darkgreen}{rgb}{0.0,0.5,0.0}
\newcommand{\cmark}{\textcolor{darkgreen}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}

\definecolor{shadecolor}{RGB}{237,237,237}
\newcommand{\mybox}[1]{\vspace{0.3mm}\par\noindent\colorbox{shadecolor}
{\parbox{\dimexpr\textwidth-2\fboxsep\relax}{\vspace{-0.2mm}#1\vspace{-0.2mm}}}\vspace{0.3mm}}

% \newcommand{\gptv}[1]{{\color{blue}#1}}
% \newcommand{\gptv}[1]{{\color{black}#1}}

% \newcommand{\new}[1]{{\color{cyan}#1}}
% \newcommand{\new}[1]{{\color{blue}#1}}
% \newcommand{\new}[1]{{\color{black}#1}}

\definecolor{Gray}{gray}{0.93}
\definecolor{uclagold}{rgb}{1.0, 0.7, 0.0}
\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}
\definecolor{rosegold}{rgb}{0.72, 0.43, 0.47}
\definecolor{pastelbrown}{rgb}{0.51, 0.41, 0.33}
\definecolor{isabelline}{rgb}{0.96, 0.94, 0.93}
\definecolor{macaroniandcheese}{rgb}{0.98, 0.89, 0.83}
\definecolor{wildblueyonder}{rgb}{0.85, 0.89, 0.95}
\definecolor{mediumtaupe}{rgb}{0.4, 0.3, 0.28}
\definecolor{bluegray}{rgb}{0.4, 0.6, 0.8}
\definecolor{celestialblue}{rgb}{0.29, 0.59, 0.82}
\definecolor{darkorange}{rgb}{1.0, 0.55, 0.0}
\definecolor{cadmiumred}{rgb}{0.89, 0.0, 0.13}
\definecolor{magnolia}{rgb}{0.97, 0.96, 1.0}
\definecolor{pastelblue}{rgb}{0.68, 0.78, 0.81}
\definecolor{persiangreen}{rgb}{0.0, 0.65, 0.58}
\definecolor{steelblue}{rgb}{0.27, 0.51, 0.71}
\definecolor{bluebell}{rgb}{0.64, 0.64, 0.82}
\definecolor{dimgray}{rgb}{0.41, 0.41, 0.41}
\definecolor{splashedwhite}{rgb}{1.0, 0.99, 1.0}
\definecolor{lavendergray}{rgb}{0.77, 0.76, 0.82}
\definecolor{lightgray}{rgb}{0.83, 0.83, 0.83}
\definecolor{lavendermist}{rgb}{0.9, 0.9, 0.98}
\definecolor{lightgreen}{HTML}{f8fcf4}
\definecolor{lightblue}{HTML}{dfebf7}
% Colors used for each shot value:
\definecolor{zeroshot}{rgb}{0.9, 0.9, 0.9}
\definecolor{fourshot}{rgb}{0.8, 0.9, 0.8}
% prev: \rowcolor{isabelline}   
\definecolor{eightshot}{rgb}{0.8, 0.8, 0.9}
% prev: \rowcolor{lightblue}   
\definecolor{sixteenshot}{rgb}{0.9, 0.8, 0.8}
\usetikzlibrary{patterns}
\usepackage{subcaption}
% \usepackage{tcolorbox}
\definecolor{blue-violet}{rgb}{0.54, 0.17, 0.89}
\definecolor{coral}{HTML}{FF7F50}

\newcommand{\wh}[1]{\textcolor{coral}{\bf\small [#1 --WH]}}
\newcommand{\violet}[1]{\textcolor{purple}{\bf\small [#1 --violet]}}



% \title{Title todo: Multimodal RAG Benchmark}

% \title{\dataset: Evaluating Retrieval Augmentation Reasoning of Multimodal Foundation Models}
% \title{\dataset: Evaluating Multimodal Retrieval Augmentation in Visual Foundation Models}
\title{\dataset: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models}

% \zd{evaluating xx sounds a bit generic and less exciting. may include our uniqueness or interesting findings in the title. like Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models?}}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{\textbf{Wenbo Hu}$^{1}$, 
    \textbf{Jia-Chen Gu}$^{1}$, 
    \textbf{Zi-Yi Dou}$^{1}$, 
    \textbf{Mohsen Fayyaz}$^{1}$, 
    \textbf{Pan Lu}$^{2}$, \\
    \textbf{Kai-Wei Chang}$^{1}$, 
    \textbf{Nanyun Peng}$^{1}$ \\
    $^1$UCLA, 
    $^2$Stanford University\\
    \vspace{-3mm}
    \\
    \texttt{\{wenbohu, gujc, zdou\}@ucla.edu} \\   
    \vspace{-1mm} \\
    \textbf{\url{https://mragbench.github.io}}
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
% \setlength {\marginparwidth }{2cm}
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
%% New Version: 

Existing multimodal retrieval benchmarks primarily focus on evaluating whether models can retrieve and utilize external \textit{textual knowledge} for question answering. However, there are scenarios where retrieving visual information is either more beneficial or easier to access than textual data. %not only more beneficial but also easier than accessing textual data. 
In this paper, we introduce a \textbf{m}ultimodal \textbf{r}etrieval-\textbf{a}ugmented \textbf{g}eneration benchmark, \dataset, in which we systematically identify and categorize scenarios where visually augmented knowledge is better than textual knowledge, for instance, more images from varying viewpoints.
\dataset consists of 16,130 images and 1,353 human-annotated multiple-choice questions 
%\violet{people will likely be curious why there are much more images than questions.} 
across 9 distinct scenarios. With \dataset, we conduct an evaluation of 10 open-source and 4 proprietary large vision-language models (LVLMs). Our results show that all LVLMs exhibit greater improvements when augmented with images compared to textual knowledge, confirming that \dataset is vision-centric. 
%\violet{what does ``vision-centric'' mean here? I suggest moving this to be after you described the observation. Also, should we say vision-focused?}
Additionally, we conduct extensive analysis with \dataset, which offers valuable insights into retrieval-augmented LVLMs. Notably, the top-performing model, GPT-4o, faces challenges in effectively leveraging retrieved knowledge, achieving only a 5.82\% improvement with ground-truth information, in contrast to a 33.16\% improvement observed in human participants. 
%\violet{do we also want to comment on gold vs. retrieved images? E.g., encourage research on image retrieval (haven't checked your results, just saying)} 
These findings highlight the importance of \dataset in encouraging the community to enhance LVLMs' ability to utilize retrieved visual knowledge more effectively.

% \zd{Existing multimodal retrieval benchmarks primarily focus on evaluating whether models can retrieve external textual knowledge for question answering. However, there are scenarios where retrieving visual information is not only more beneficial but also easier than accessing textual data. In this paper, we introduce \dataset, in which we systematically define and categorize scenarios where visually augmented knowledge is better than textual knowledge. \dataset consists of 16,130 images and 1,353 human-annotated multiple-choice questions across 9 distinct tasks. Using \dataset, we conducted a comprehensive evaluation of 10 open-source and 4 proprietary large vision-language models (LVLMs). The results confirm that \dataset is vision-centric, as all LVLMs show greater improvements when augmented with images compared to textual knowledge. However, the top-performing model, GPT-4o, struggles to effectively utilize the retrieved knowledge, achieving only a 5.82\% improvement when augmented with relevant information, compared to a 33.16\% improvement demonstrated by human participants. These findings underscore the significance of \dataset in motivating the community to develop LVLMs that better utilize retrieved visual knowledge.
%  }

% Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across various tasks. However, in complex scenarios where the input image does not directly trigger \jg{Is incentivize better?} \wh{I think it's better, but reviewers should know this word?} relevant knowledge from the model’s memory, retrieval-augmented generation (RAG) of visual knowledge becomes a promising solution. Previous work has mainly focused on augmenting LVLMs with knowledge retrieved from textual sources, while neglecting the rich visual context embedded within image corpora.
% To bridge this gap, we present \dataset, a benchmark designed to evaluate the ability of LVLMs to leverage retrieved vision-centric multimodal knowledge. 
% For this purpose, we systematically defined scenarios in which visually augmented knowledge is more valuable than textual knowledge and categorized them into two aspects.
% \dataset consists of 16,130 images and 1,353 human-annotated multiple-choice questions covering 9 distinct tasks. 
% With \dataset, we conducted a comprehensive evaluation on 10 open-source and 4 proprietary LVLMs. Our results reveal that, all LVLMs benefit from being augmented with ground-truth (GT) images more than textual knowledge, indicating that retrieving and utilizing images is more promising than texts.
% % The best-performing GPT-4o model achieves an overall accuracy of 74.5\% with GT visual knowledge, surpassing human performance of 71.63\% and showcasing its comprehensive knowledge corpus. 
% Notably, all models improve with GT knowledge, but only proprietary models are able to effectively utilize noisy retrieved multimodal knowledge.  In comparison to humans, GPT-4o achieves only a 5.82\% improvement when augmented with GT knowledge and 0.28\% with retrieved knowledge, whereas humans demonstrate a 33.16\% and 22.91\% improvement, respectively. These results highlight the importance of \dataset in encouraging the community to develop LVLMs which better leverage multimodal RAG knowledge. 


% Previous work has mainly focused on augmenting large vision-language models (LVLMs) with knowledge retrieved from textual sources, while neglecting the rich visual context embedded within image corpora.
% To bridge this gap, we present \dataset, a benchmark designed to evaluate the ability of LVLMs to leverage retrieved vision-centric multimodal knowledge. 
% For this purpose, we systematically defined scenarios in which visually augmented knowledge is more valuable than textual knowledge and categorized them into two aspects.
% \dataset consists of 16,130 images and 1,353 human-annotated multiple-choice questions covering 9 distinct tasks. 
% With \dataset, we conducted a comprehensive evaluation on 10 open-source and 4 proprietary LVLMs. Our results reveal that, all LVLMs benefit from being augmented with ground-truth (GT) images more than textual knowledge, indicating that retrieving and utilizing images is more promising than texts.
% The best-performing GPT-4o model achieves an overall accuracy of 74.5\% with GT visual knowledge, surpassing human performance of 71.63\% and showcasing its comprehensive knowledge corpus. Notably, all models improve with GT knowledge, but only proprietary models are able to effectively utilize noisy retrieved multimodal knowledge.  In comparison to humans, GPT-4o achieves only a 5.82\% improvement when augmented with GT knowledge and 0.28\% with retrieved knowledge, whereas humans demonstrate a 33.16\% and 22.91\% improvement, respectively. These results highlight the importance of \dataset in encouraging the community to develop LVLMs which better leverage multimodal RAG knowledge. 

% \jg{Previous work has mainly focused on augmenting large vision-language models (LVLMs) with knowledge retrieved from textual sources, while neglecting the rich visual context embedded within image corpora.}
% Large Vision-Language Models (LVLMs) exhibit remarkable problem-solving abilities across a wide range of tasks and domains, but their potential in multimodal retrieval-augmented generation (RAG) tasks is under-explored. \zd{may delete the first sentence}
% Previous work has mainly focused on enhancing multimodal RAG inference by retrieving knowledge from textual sources, while neglecting the rich visual context embedded within image corpora.
%\wh{somthing we should do, however, previous work not doing. First illlustrate importance of visual knowledge but not focused. then give examples. Then however, existing focus on, then we constrcut benchmark. }

% Previous work has mainly focused on augmenting large vision-language models (LVLMs) with knowledge retrieved from textual sources, while neglecting the rich visual context embedded within image corpora.
% To bridge this gap, we present \dataset, a benchmark designed to evaluate the ability of LVLMs to leverage retrieved vision-centric multimodal knowledge. 
% For this purpose, we systematically defined scenarios in which visually augmented knowledge is more valuable than textual knowledge and categorized them into two aspects.
% main categories: \emph{perspective} and \emph{transformative} understanding of visual entities. \zd{not sure if perspective and transformative are very informative. maybe better to just say something like we systematically identify scenarios where retrieving vision knowledge is required/better than retrieving textual knowledge?}
% \dataset consists of 16,130 images and 1,353 human-annotated multiple-choice questions covering 9 distinct tasks. 
% % \jg{We should discuss if it is necessary to introduce the overview of \dataset, i.e., \dataset focuses on the \emph{perspective} and \emph{transformative} understanding of entities.}
% With \dataset, we conducted a comprehensive evaluation on 10 open-source and 4 proprietary LVLMs. Our results reveal that, all LVLMs benefit from being augmented with ground-truth (GT) images than textual knowledge, showing that retrieving and utilizing images is more promising than texts.
% % \zd{maybe follow JG's comment and first state our conclusion (retrieving images is better than text), then list the GPT (and maybe human?) performance} 
% The best-performing GPT-4o model achieves an overall accuracy of 74.5\% with GT visual knowledge, 
% % \jg{Here ``with ground-truth (GT) RAG knowledge'' might confuse readers about textual or image knowledge. I think ``an overall accuracy of 74.5\% augmented with ground-truth (GT) visual knowledge'' might be better.}
% surpassing human performance of 71.63\% and showcasing its comprehensive knowledge corpus. Notably, all models improve with GT knowledge, but only proprietary models are able to effectively utilize noisy retrieved multimodal knowledge.  In comparison to humans, GPT-4o achieves only a 5.82\% improvement when augmented with GT knowledge and 0.28\% with retrieved knowledge, whereas humans demonstrate a 33.16\% and 22.91\% improvement, respectively. These results highlight the importance of \dataset in encouraging the community to develop LVLMs which better leverage multimodal RAG knowledge. 


% \jg{For the results, I think we should follow this story: 1) All LVLMs benefit from being augmented with ground-truth images than textual knowledge, showing that retrieving and utilizing images is more promising than texts. 2) Even the SOTA GPT-4o still struggles with ..., calling for more follow-up work.} 

% Our results reveal that, the best-performing GPT-4o model achieves an overall accuracy of 74.5\% with ground-truth RAG, substantially outperforming the best open-source model LLaVA-OneVision by 15.52\%. \zd{comparisons between gpt4o and llava seems expected and a bit uninformative, may remove} 

% However, GPT-4o only achieves 5.82\% improvement when augmented with ground-truth image knowledge and 0.28\% improvement when using retrieved image knowledge. In contrast, humans can achieve a 33.16\% improvement with ground-truth knowledge and ?? with retrieved image knowledge. 
% These results highlights ... / suggests ... 

% the \emph{perspective} and \emph{transformative} understanding of entities.

% Large Vision-Language Models (LVLMs) exhibit remarkable problem-solving abilities across a wide range of tasks and domains, yet they still struggle with understanding irregular perspectives or physical transformations of familiar entities. % uncommon entities or irregular perspectives of familiar entities.
% A pressing research question arises: \emph{Can retrieval augmentation enhance the reasoning of multimodal foundation models?}
% \zd{i think previous papers like Retrieval-Augmented Multimodal Language Modeling has touched upon this question. we may need to better differentiate our work from others in one or two sentences. for example, we may emphasize that we focus on settings where where retrieving images is more beneficial than retrieving text (or retrieving text is not enough), and previous eval settings can be solved by just retrieving text.}
% To this end, we present \dataset, a benchmark designed to evaluate the challenges of multimodal retrieval-augmented generation (RAG). 
% It consists of 1,353 examples focusing on the \emph{perspective} and \emph{transformative} understanding of entities.
% Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging.

%\wh{1. human can utilize rag well, gpt4o can't. 2. open source met close source on other tasks, but still have a great gap in our tasks. }

\end{abstract}
\begin{figure}[h!]
  \centering
  % \vspace{-1mm}
  % \includegraphics[trim=0.3cm 6.1cm 4.6cm 0.6cm, clip, width=1.0\textwidth]{files/mmrag_teaser_new.pdf}
   \includegraphics[trim=0.1cm 3.7cm 7cm 0.0cm, clip, width=1.0\textwidth]{files/mmrag_teaser.pptx.pdf}
  % \vspace{-5mm}
  \caption{Example scenarios from \dataset. Previous benchmarks~\citep{chang2022webqamultihopmultimodalqa, encvqa, chen2023infoseek} mainly focused on retrieving from textual knowledge. However, there are scenarios where  retrieving correct textual knowledge is hard and sometimes not as useful as visual knowledge.}
  % \zd{in the figure: may replace ``in reality'' with ``however, there are scenarios where:'' also may remove `method 1/2'' just say ''retrieve from text corpus/image corpus'' and add something like ``our focus'' for ``retrieve from image corpus'' }
  
  % \vspace{-3mm}
\label{fig:mmrag qualitative teaser}
\end{figure}

\section{Introduction}

Retrieval-augmented generation (RAG) has emerged as a promising direction in large vision-language models (LVLMs)~\citep{gpt4, liu2023improvedllava, Qwen-VL, kosmos-1, chen2023shikra, hu2023bliva, internvl15, tong2024cambrian1, mckinzie2024mm1}. By incorporating external knowledge during generation, models such as Wiki-LLaVA~\citep{caffagni2024wikillavahierarchicalretrievalaugmentedgeneration} have demonstrated improved performance in knowledge-intensive question answering tasks. 
% Large Vision-Language Models (LVLMs) have demonstrated impressive problem-solving abilities across diverse tasks and domains, leveraging their capacity to process and understand both visual and textual inputs~\citep{gpt4, liu2023improvedllava, Qwen-VL, hu2023bliva, kosmos-1, chen2023shikra, internvl15, tong2024cambrian1, mckinzie2024mm1}. Retrieval-augmented generation (RAG) has emerged as a potential solution to overcome limitations in language models by incorporating external knowledge retrieval during the generation process~\citep{DBLP:conf/nips/LewisPPPKGKLYR020,DBLP:journals/corr/abs-2301-12652}. This capability has inspired efforts to extend RAG to LVLMs. For instance, Wiki-LLaVA~\citep{caffagni2024wikillavahierarchicalretrievalaugmentedgeneration} integrated an external knowledge source of multimodal documents to LVLMs and demonstrates promising results. 



% Recent open-source state-of-the-art LVLMs~\citep{internvl15, tong2024cambrian1, mckinzie2024mm1, wang2024qwen2vlenhancingvisionlanguagemodels} have shown remarkable and even surpassing GPT4V performance in various evaluation benchmarks~\citep{masry-etal-2022-chartqa, singh2019towards, mathew2021docvqa, liu2023mmbench,lu2023mathvista}, closing the gap between open-source and proprietary models and calling the need for more challenging benchmarks. 
% \zd{seems redundant. may shorten or delete this. also not sure if open-source models reaching GPT4 performance means there is a ``need for more challenging benchmarks'' and how this connects to our motivation to create a vision-centric retrieval benchmark }

% By integrating information from multiple modalities, these models have shown remarkable performance in areas such as image captioning~\citep{}, visual question answering~\citep{}, and multimodal reasoning~\citep{}.
% Despite their broad applicability, LVLMs still exhibit notable limitations, particularly when it comes to understanding complex scenarios involving irregular perspectives or physical transformations of familiar entities. 

% \zd{i think this paragraph is more focused on previous models rather than benchmarks. since we are proposing a benchmark, we may want to compare our dataset with previous benchmarks. for example, we may say previous benchmarks are text-centric and can be easily solved with text retrieval, thus it is unnecessary to retrieve vision information (we may want to have some qualitative samples in Fig. 1 and it'd be good if we have numbers in experiments too)}\wh{working on a new teaser figure with qualitative examples}

% \jg{Move some starting sentences to the first paragraph. The second paragraph can start from ``Recently, WIki-LLaVA ...'', or further ``However, as shown in Figure 2 ...''}

% \zdc{Evaluation of vision-language models that draw upon external knowledge to answer questions has been widely studied}{
There are several existing benchmarks evaluating retrieval-augmented LVLMs. For example, OK-VQA~\citep{okvqa} focused on scenarios where the image content alone is insufficient to answer the questions. A-OKVQA~\citep{schwenk2022okvqa} further extended this dataset to incorporate additional types of world knowledge. More recent works~\citep{chang2022webqamultihopmultimodalqa, chen2023infoseek,encvqa} further expanded and curated large-scale knowledge base data to evaluate pre-trained vision and language models in knowledge-intensive and information-seeking visual questions. 
However, as shown in Table~\ref{tab:comparion_exisiting_benchmarks}, these benchmarks remain text-centric, as their questions can often be resolved with related external textual knowledge. %, 
% can often be resolved solely through text retrieval, 
%rendering the use of visual source information unnecessary. 
%As the saying goes ``a picture is worth a thousand words''~\citep{gropper1963picture, hibbing2003picture}, 
In contrast, retrieving visual information is sometimes more beneficial than retrieving text, as humans often gain greater insights from it.
Specifically, we illustrate examples in Figure~\ref{fig:mmrag qualitative teaser} where retrieving correct textual knowledge can be \emph{hard} and retrieved textual knowledge can be \emph{useless}, while retrieving additional images is helpful. 
% However, , all these approaches convert visual knowledge to text for the evaluated models, neglecting the rich visual information in image corpus, 
For instance, when presented with a top-down view of a car, humans may struggle to accurately identify it; however, with a front-facing view, they can quickly recognize the vehicle and effectively leverage the visual information. 
% Inspired by humans cognition, we raise an important question: 
% vision-centric benchmark, which is motivated by the fact that retrieving visual knowledge is sometimes important/better than text, 

% \zd{i think there is a jump from the current flow to this research question. the first thing to mention should be that different from previous work, we want to create a vision-centric benchmark. then, based on this benchmark, we ask several questions, ``Can LVLMs utilize visually augmented knowledge well'' is definitely one important question, but this benchmark allows us to explore other questions like whether image retrieval models can perform well. in short, i think our focus should be our vision-centric benchmark, which is motivated by the fact that retrieving visual knowledge is sometimes important/better than text, not this research question. investigating this research question is a sub-product of our benchmark. } \emph{Can LVLMs utilize visually augmented knowledge well}? 

% \zd{i think we should emphasize more on why retrieving images is better than text for the example in Figure 1. e.g. it is harder to retrieve the correct textual knowledge, or it is harder for the model to answer the question even if the textual knowledge is correct. hopefully we can find an example that has both the issues} \wh{feels like, I can draw a new teaser figure with qualitative examples and illustrating this point.}

% Retrieval augmentation allows the model to access relevant, task-specific information from external databases or knowledge sources, enhancing its ability to provide accurate and contextually appropriate responses.

% \begin{figure}[t]%[h!]
%   \centering
%   % \vspace{-1mm}
%   \includegraphics[trim=0.1cm 1.7cm 0.1cm 0.4cm, clip, width=1.0\textwidth]{files/MMRAG_compare_figure_copy2.pptx-2.pdf}
%   % \vspace{-5mm}
%   \caption{Compared with previous works~\citep{chang2022webqamultihopmultimodalqa, encvqa, chen2023infoseek, liu2023univldr, wei2023uniir, caffagni2024wikillavahierarchicalretrievalaugmentedgeneration}, \dataset focuses on evaluating LVLMs in utilizing vision-centric retrieval-augmented multimodal knowledge.
%   }
%   % \zd{not sure if we can make it nicer. for example, the input question/image is identical for the three different categories. maybe we can combine them? it's also not super clear what the final output is for each kind of model (it's not easy to tell that the first category's output is pure text and the second category's output is pure images.} }
%   % \vspace{-3mm}
% \label{fig:comparison_teaser}
% \end{figure}


\begin{table*}[t]
\vspace{-3mm}
\centering
 \small
 \renewcommand\tabcolsep{2.5pt} % column space
 \renewcommand\arraystretch{0.95} % row space
 \resizebox{1.0\linewidth}{!}{
\begin{tabular}{l|cccc}
\toprule
 \multirow{2}{*}{\textbf{Benchmarks}}     & \textbf{Knowledge}   & \textbf{Knowledge} & \textbf{Multi-Image} & \textbf{Diverse}   \\
  &  \textbf{Modality}  & \textbf{Source}  & \textbf{Input}  & \textbf{Scenarios}  \\
\midrule
K-VQA~\citep{kvqa}    & Text & Wikipedia & \xmark  & \xmark   \\
OK-VQA~\citep{okvqa}  & Text & Wikipedia  & \xmark  & \xmark    \\
% MIMOQA~\citep{singh-etal-2021-mimoqa} \\ 
MultiModalQA~\citep{talmor2021multimodalqa}  & Text& Wikipedia & \xmark  &  \xmark   \\
ManyModalQA~\citep{Hannan_Jain_Bansal_2020}  & Text & Wikipedia  & \xmark  & \cmark  \\
A-OKVQA~\citep{schwenk2022okvqa} & Text & Common/World  & \xmark  & \xmark \\
ViQuAE~\citep{ViQuAE}  & Text & Wikipedia & \xmark  & \xmark    \\
WebQA~\citep{chang2022webqamultihopmultimodalqa} & Text/Caption & Wikipedia  & \xmark  & \xmark   \\
Encyclopedia VQA~\citep{encvqa}  & Text & Wikipedia  & \xmark  &\xmark    \\
InfoSeek~\citep{chen2023infoseek}  & Text &  Wikipedia  & \xmark  & \xmark    \\
% MMTabQA~\citep{mathur2024knowledgeawarereasoningmultimodalsemistructured} & Text/Image &  Wikipedia  & \cmark  & \xmark  \\ 
\midrule
\dataset \textbf{(Ours)} & \textbf{Image} & 
% \textbf{Web\&}  
% \includegraphics[height=1em]{files/web.png} 
\includegraphics[height=1em]{files/web.png}
% \wh{which web icon looks nicer lol }
\includegraphics[height=1em]{files/imagenet.png} \includegraphics[height=1em]{files/flower.png} \includegraphics[height=1em]{files/car.png}  & \cmark & \cmark\\

\bottomrule
\end{tabular}
}
    % \vspace{-2mm}
    \caption{Compared with previous works, \dataset focuses on
evaluating LVLMs in utilizing vision-centric retrieval-augmented multimodal knowledge. ``Diverse scenarios'' refers to whether a benchmark categorized different scenarios during evaluation.\includegraphics[height=1em]{files/web.png}: Web, \includegraphics[height=1em]{files/imagenet.png}: ImageNet~\citep{Russakovsky2015}, \includegraphics[height=1em]{files/flower.png}: Flowers102~\citep{Nilsback08}, \includegraphics[height=1em]{files/car.png}: StanfordCars~\citep{Krause_2013_ICCV_Workshops}.}

\vspace{-3mm}
\label{tab:comparion_exisiting_benchmarks}
\end{table*}

% This paradigm has been widely explored in the context of text-based tasks, but its application to multimodal scenarios, where both visual and textual information are essential, remains relatively underexplored.
% This gap in their reasoning ability raises an important research question: \emph{Can retrieval augmentation enhance the reasoning of multimodal foundation models}?
% WH flow: 
% Closing the gap between open source and close source models... on many benchmarks. 
% RAG can do .... ... 
% Recent work also used  RAG on LVLM, exploratory work, such as Wiki-LLaVA. .. 
% But they all focus on text ... 
% An image work a thound word. 
% Image corpus ... 
% Humans can utilize well. for example .. 
% but how about LVLM. 

In this paper, we introduce \dataset, a benchmark specifically designed for vision-centric evaluation for retrieval-augmented multimodal models, with visual questions typically %where it is typically easier and more beneficial to 
benefit more from retrieving visual knowledge than textual information. %for visual question answering. 
% In this paper, we introduce \dataset, a compressive
% \zd{comprehensive? i think we may not use this word as it's kind of vague and may not be very convincing} benchmark specifically designed \zdc{to rigorously evaluate LVLMs in leveraging vision-centric multimodal knowledge}{for vision-centric evaluation for retrieval-augmented multimodal models, where it is typically easier and more beneficial to retrieve visual knowledge than textual information for question answering}. 
% In response to this, we present \dataset, a benchmark specifically designed to highlight the challenges multimodal foundation models face in handling complex visual reasoning tasks and to evaluate the challenges associated with multimodal RAG. 
% \dataset provides a structured and comprehensive set of examples designed to push the boundaries of multimodal model performance and encourage further exploration of retrieval augmentation as a potential pathway to improve reasoning in these models. 
\dataset consists of 16,130 images and 1,353 human-annotated multi-choice questions spanning 9 distinctive scenarios. Focusing on utilizing visually augmented knowledge in real-world scenarios, we divide our benchmark into two aspects: 
% \zd{previously you mentioned 9 distinctive categories and here you say 2 main categories, may change the terms to avoid confusion. also 2 does not sound like a large number, may just say something like 9 distinctive categories covering two aspects}
\emph{perspective}, where changes in visual entity's perspective requiring visually augmented knowledge; and \emph{transformative}, where the visual entity undergoes transformative change physically thus requiring visually augmented knowledge. Specifically, \dataset requires models to reason about visual entities that undergo perspective changes, such as \emph{angle}, \emph{partial}, \emph{scope} and \emph{occlusion}, as well as transformative changes, such as \emph{temporal}, \emph{incomplete}, \emph{biological} and \emph{deformations}. Additionally, \dataset includes 9,673 human-selected images, which serves as the ground-truth image knowledge corpus for model evaluation. 

% These tasks involve comprehending objects and scenes from irregular or altered viewpoints.
% Specifically, \dataset requires models to reason about entities that undergo perspective changes, such as angle, crop, scope and occlusion, as well as transformative changes, such as temporal, incomplete, biological and deformations. 
% To succeed, a model must exhibit fine-grained visual understanding and advanced compositional reasoning, skills that remain challenging for even state-of-the-art multimodal foundation models.

We conduct extensive experiments on \dataset to evaluate 10 open-source and 4 proprietary LVLMs. 
The results confirm that \dataset is vision-centric, as all LVLMs show greater improvements when augmented with images compared to textual knowledge. 
Our results indicate that the best-performing GPT-4o model only achieve 68.68\% and 74.5\% of accuracy without RAG knowledge and with ground-truth (GT) RAG knowledge, respectively. This substantially outperforms the best open-source model LLaVA-OneVision by 15.39\% and 15.52\%, respectively. Notably, we observe while all models improve with GT knowledge, only proprietary models are able to effectively utilize noisy retrieved multimodal knowledge. This indicates the gap between open-source and close-source models still exists. Open-source models are falling short on their parametric knowledge and the %encouraged to enhance their knowledge bases and improve their 
ability to distinguish between high-quality and poor-quality retrieved visually augmented examples. In comparison to humans, GPT-4o achieves only a 5.82\% improvement when augmented with GT knowledge and 0.28\% with retrieved knowledge, whereas humans demonstrate a 33.16\% and 22.91\% improvement, respectively. These results highlight the importance of \dataset in encouraging the community to develop LVLMs better utilizing of visually augmented knowledge.
% \violet{if we decided to add in the abstract, here we also need to discuss about improving image retriever}

% \wh{How to connect with vision literature in like novel image class objects recognition (corresponding to our perspective category. Is this well studied or not? }
% \wh{flow of image augmented knowlege .... and analogy to text rag, give examples, like having more angles of image objects helps .... these \textbf{complex} vision question tasks .... - suggested by Kai-Wei.}





\section{\dataset}



\subsection{Benchmark Overview}
\label{sec: benchmark overview}


% Table: main statistics
\begin{figure}[t]
% \vspace{-5mm}
 \begin{minipage}{0.45\textwidth} 
 \centering
 \fontsize{8.2pt}{\baselineskip}\selectfont % font size
 \renewcommand\tabcolsep{1.0pt} % column space
 \renewcommand\arraystretch{0.8} % row space
 \begin{tabular}{lc}
 \toprule
 \textbf{Statistic} & \textbf{Number} \\
 \midrule
  Total questions & 1,353 \\
  ~- Multiple-choice questions &  1,353 (100\%) \\
  % ~- Free-form questions & 2,749 (44.8\%) \\
  % ~- Questions with annotations & 5,261 (85.6\%) \\
  ~- Questions newly annotated & 1,353 (100\%) \\
 Total Scenarios & 9 \\
Unique number of questions & 375 \\ 
  Unique number of answers & 663 \\
 \midrule
 Total number of images & 16,130\\
 % 33,237 is unique. 
 Unique number of images & 16,130 \\
 Human selected images & 9,673 \\
 Average image size (px) & 1076 x 851 \\
% Maximum image size (px) & 8688 x 5792 \\

 \midrule
 Maximum question length & 20 \\
 Maximum answer length & 9 \\
 % Maximum choice number & 4 \\
 % \midrule 
 Average question length & 8.03 \\
 Average answer length & 2.16\\
 Average choice number & 4 \\
 % Average question length & 16.09 \\
 % Average answer length & 1.21 \\
 % Average choice number & 3.40 \\
 \bottomrule
 \end{tabular}
 \captionof{table}{Key statistics of \dataset. }
 \label{tab:statistics}
 \end{minipage} 
 \hfill
 \begin{minipage}{0.56\textwidth}
 \centering
 \vspace{-1mm}
\includegraphics[trim=2cm 9.5cm 3cm 3.1cm, clip, width=0.8\linewidth]{files/mmrag_stat_chart.pdf}
\vspace{-1mm}
 \caption{Scenarios distribution of \dataset.}
 \label{fig:source_dataset}
 \end{minipage}
 % \vspace{-2mm}
\end{figure}

Our benchmark is designed for systematic evaluation of LVLM's vision-centric multimodal RAG abilities. 
% \jg{I think at this point or somewhere else in this subsection, we should have one or two sentences describing the \emph{problem formulation} along with \emph{notation definition}. For example, given a (textual question, query image) query tuple, the image retriever returns a set of relevant images. Then, LVLMs take (textual question, query image, retrieved images) as input and output the answer in a multi-choice QA format. You can refer to the following:
%       Given an input sequence $\mathbf{x}$, a target output sequence $\mathbf{y}$, and a set of $N$ retrieved documents $D$ ($[\mathbf{d}_1, \mathbf{d}_2, ..., \mathbf{d}_N]$), 
%       BRIEF compresses retrieved documents $D$ into a summary $\mathbf{s}$ which captures core information with respect to $\mathbf{x}$ with significantly fewer words. 
%       The whole architecture consists of two modules: compressor $\mathcal{C}$ and LM $\mathcal{M}$. 
%       The compressor $\mathcal{C}$ is trained on the corpora we curated in this work, while the LM $\mathcal{M}$ remains frozen and can be any off-the-shelf LM. 
%       In this work, we train an \emph{abstractive} compressor, which takes the input sequence $\mathbf{x}$ and the concatenation of retrieved document set $D$, and outputs a summary $\mathbf{s}$. The compressor $\mathcal{C}$ is intentionally designed to be substantially smaller than the LM $\mathcal{M}$, as we aim to reduce the computational cost of encoding a set of lengthy retrieved documents.
% } 
To achieve this, we focus on evaluating the model's understanding of image objects that are not commonly associated with its knowledge base, while the collected ground-truth images can help incentivize specific visual concepts within LVLMs' memory.
Therefore, we divide our benchmark into two main aspects, as illustrated in the examples in Figure~\ref{fig:mmrag qualitative teaser}:

\begin{itemize}[leftmargin=7.5mm]
\setlength{\itemsep}{1pt}
    \item \emph{perspective}, refers to the challenges in visual recognition and reasoning that arise when a visual entity is presented from varying viewpoints, scopes, or levels of visibility.
    % \wh{how about this one?}
    % where visual entity remains physically the same, but changes in perspective present challenges to the model’s ability to understand the entity \zd{where it is challenging to recognize a visual entity given an initial input, but a different viewpoint can be helpful};
    \item \emph{transformative}, refers to the challenges that arise when a visual entity undergoes fine-grained physical transformations, making it unfamiliar or not easily associated with the model's prior knowledge. 
    % \zd{where the query focuses on fine-grained physical transformations of an object that are often unfamiliar and outside the model's domain.}.
\end{itemize} 
% \wh{should we change these two definition to be the exact same as in the introduction as well?}
% \jg{I don't think so. Here we should explain the definition more detailed than that in introduction. Maybe we can make it easier to understand by presenting an example.}
\dataset consists of 16,130 images and 1,353 multiple choice questions, with key statistics shown in Table~\ref{tab:statistics}.   
\dataset adheres to the following design principles: (1) it focuses on real-world scenarios where visually augmented information is useful; (2) it incorporates 9 diverse multimodal RAG scenarios covering various types of image objects; (3) it features cleaned ground-truth images for each question that align with human knowledge;  and (4) it provides robust evaluation settings for deterministic evaluations. Unlike previous works focus on retrieving textual knowledge, evaluation on \dataset focuses on retrieving vision-centric knowledge, which can be formulated as follows: Given a query tuple $\mathbf{Q}$ composed of (query image, textual question), the multimodal retriever $\mathcal{R}$ returns a set of relevant images $\mathbf{I}$ ($[\mathbf{i}_1, \mathbf{i}_2, ..., \mathbf{i}_N]$), then the LVLM $\mathcal{M}$ take the input ($\mathbf{Q}$, $\mathbf{I}$) and output the final answer.
% \wh{I agree, we can also add the notation to figure 1 as well. Todo. }



\subsection{Benchmark Composition} 

\dataset provides a systematic evaluation across 9 distinctive multimodal RAG scenarios, with four scenarios focused on the \emph{perspective} understanding of visual entities, four on \emph{transformative} understanding, and one categorized as ``others''. As illustrated in Figure~\ref{fig:source_dataset}, each scenario comprises 7.5\% to 23.8\% of the whole benchmark. The selected examples of each scenario is shown in  Figure~\ref{fig:qualitative_examples}. The details of each scenario are introduced as follows. 
% \zd{may put this figure in this section? i think the definitions are a bit abstract and it'd be good to have concrete examples.} 

\paragraph{\emph{Perspective} understanding aspect.} First, we have \emph{perspective} aspect comprising \textsc{[Angle]}, \textsc{[Partial]}, \textsc{[Scope]}, and \textsc{[Occlusion]} dimensions.

\begin{itemize}[leftmargin=7.5mm]
\setlength{\itemsep}{1pt}
    \item \textsc{[Angle]} evaluates the ability of models to utilize visual knowledge of common shooting angles to identify and reason about less common, long-tailed viewpoints of visual entities.
    \item \textsc{[Partial]} evaluates the ability of models to use complete appearance knowledge to identify and reason when only a partial image of the visual entities is available.
    \item  \textsc{[Scope]} evaluates the ability of models to leverage high-resolution, detailed images for identifying and reasoning about visual entities in longer-scoped, low-resolution images.
    \item \textsc{[Occlusion]} evaluates the ability of models to use ground-truth image knowledge to identify and reason when visual entities are occluded or partially hidden in natural scenes.
\end{itemize} 

\begin{figure}[t]
  \centering
  % \vspace{-1mm}
  % \includegraphics[trim=0.0cm 7.6cm 2.5cm 0.0cm, clip, width=1.0\textwidth]{files/mmrag_examples_new.pdf}
    \includegraphics[trim=0.0cm 7.6cm 2.6cm 0.0cm, clip, width=1.0\textwidth]{files/mmrag_examples.pptx.pdf}
  % \vspace{-5mm}
  \caption{Qualitative examples on \dataset. For each scenario, we show the result of GPT-4o~\citep{gpt4}, Gemini Pro~\citep{team2023gemini}, LLaVA-Next-Interleave~\citep{li2024llavanextinterleavetacklingmultiimagevideo} and Mantis-8B-Siglip~\citep{jiang2024mantis}. The ground-truth answer is in \blue{blue}.}
  % \vspace{-3mm}
\label{fig:qualitative_examples}
\end{figure}


% \textsc{[Angle]} aims to evaluate the ability of models to utilize visual knowledge of common angles to identify and reason about less common, long-tailed angles of the visual entities of interest. \textsc{[Partial]} aims to evaluate the ability of models to use complete appearance knowledge to identify and reason when only a partial image of the visual entities of interest is captured. \textsc{[Scope]} aims to evaluate the ability of models to leverage high-resolution image details for identifying and reasoning about visual entities in longer-scoped, low-resolution images. \textsc{[Occlusion]} aims to evaluate the ability of models to use ground-truth image knowledge to identify and reason when visual entities of interest are occluded or partially hidden in natural scenes.

\paragraph{\emph{Transformative} understanding aspect.} On the other hand, the \emph{transformative} understanding scenarios cover \textsc{[Temporal]}, \textsc{[Deformation]}, \textsc{[Incomplete]}, and \textsc{[Biological]} dimensions.

\begin{itemize}[leftmargin=7.5mm]
\setlength{\itemsep}{1pt}
    \item \textsc{[Temporal]} evaluates the ability of models to use familiar image knowledge to identify and reason about visual entities undergoing temporal changes that may not be represented in the model’s knowledge base.
    \item \textsc{[Deformation]} evaluates the ability of models to use intact physical appearance knowledge to identify and reason when visual entities undergo deformation not captured in the model’s knowledge base.
    \item  \textsc{[Incomplete]} evaluates the ability of models to compare and contrast the complete layout and structure of image knowledge to identify and reason about missing parts and the correct layout of visual entities.
    \item  \textsc{[Biological]} evaluates the ability of models to utilize image knowledge after biological transformations of the visual entities. 
\end{itemize} 
    
% \textsc{[Temporal]} aims to assess the ability of models to use commonly seen image knowledge to identify and reason about visual entities of interest undergoing temporal changes that may not be represented in the model’s knowledge base.  \textsc{[Deformation]} aims to evaluate the ability of models to use intact physical appearance knowledge to identify and reason when visual entities of interest undergo deformation that may not be represented in the model’s knowledge base.  \textsc{[Incomplete]} aims to evaluate the ability of models to compare and contrast the complete layout and order of image knowledge to identify and reason about missing parts and the correct layout of the visual entities of interest.  \textsc{[Biological]} aims to evaluate the ability of models to utilize image knowledge after biological transformations of the visual entities of interest.

\textsc{[Others]} aims to evaluate the ability of models to leverage geographic image knowledge to accurately identify and reason about the correct regions of origin for the visual entities of interest. All these scenarios work in tandem to comprehensively evaluate LVLMs' abilities of leveraging visually augmented knowledge.  

% \jg{I think we should have a sentence to briefly summarize all these tasks at the whole benchmark-level, e.g., ``All these tasks work in tandem to comprehensively evaluate ... ''}


\subsection{Data Collection}
\label{sec: data collection}

As the guidelines discussed in $\S$~\ref{sec: benchmark overview}, our benchmark collection involves a clean ground-truth image corpus that can resonate with model's internal knowledge and a query question and image that challenge model's memory according to our definition of 9 diverse scenarios. To collect a dataset for systematic evaluation of vision-centric multimodal RAG scenarios, we manually annotate all multiple-choice question answering (MCQA) data while sourcing images from either publicly available datasets or manually scraping them from the web. 

% \zd{if there are good motivations for choosing each of the image sources, we can mention them}

\paragraph{Collection of \emph{perspective} aspect.} To collect diverse image objects and knowledge that are not extensively represented in LVLMs' memories~\citep{VLMClassifier}, 
we considered three sources of data, ImageNet~\citep{Russakovsky2015}, Oxford Flowers102~\citep{Nilsback08}, and StanfordCars~\citep{Krause_2013_ICCV_Workshops}. To construct a high quality image corpus, for each of the image class that we included in our benchmark, we examined the validation set and excluded the unqualified images which can't provide sufficient visual information for the recognition of this class. Among the selected corpus, we further humanly picked five representative examples covering the diverse aspects of each class object, as the five ground-truth examples in our experimental results (See $\S$\ref{sec: experiment}). For constructing the query images, we adhered to our scenario definitions and manually selected qualified images for the \textsc{[Angle]}, \textsc{[Scope]}, and \textsc{[Occlusion]} scenarios. For the \textsc{[Partial]} scenario, we randomly cropped images by 50\% in both height and width. Then we performed another human inspection to ensure the quality of the cropped images, filtering out examples where the visual object did not occupy the dominant area of the image. We repeated the random cropping process until satisfactory images were obtained, filtering to 20.4 GT images per question on average. 



\paragraph{Collection of \emph{transformative} aspect.} We chose to manually scrape images from the web based on the definitions of the \emph{transformative} aspect. To construct the image corpus, we employed Bing Image Search for each of the image object keyword predefined by us, please refer to Appendix~\ref{appendix:data collection} for more details. We filtered out image objects that did not form a clear transformative pair between the query image and the ground-truth image,  retaining approximately 74\% of the keyword names in the process. For ground-truth image examples, we employed automatic scripts to download the top 15 images related to its keyword names and human filtered out the unqualified image. On average, this results to 5.9 images per question and the five ground-truth images used during our evaluation are manually selected same as in \emph{perspective} aspect. 
% ~\citep{BingSearch}

According to our guidelines, additional related image object knowledge from the same geographic region can assist in identifying that region more effectively. For the \textsc{[Others]} scenario, we source the data from the GeoDE dataset~\citep{ramaswamy2022geode}. For each distinct image object category, we randomly sampled 3 out of 6 regions to serve as the answers for each question and selected the corresponding image as the query image. 

\paragraph{Quality control.} After constructing the entire benchmark, we implemented two quality control procedures: an automatic check with predefined rules and a manual examination of each instance. The automatic check verifies the correct MCQA format, assesses image validity and filters out redundant images in the corpus, more details are presented in Appendix~\ref{appendix:data collection}. The manual examination is conducted by two experts in this field, who checked the correspondence between query images and ground-truth image examples, and filtered or revised ambiguous questions and uncorrelated query image and ground-truth images. 


\begin{table*}[t]
\vspace{-3mm}
\centering
 \small
 \renewcommand\tabcolsep{2.5pt} % column space
 \renewcommand\arraystretch{0.95} % row space
 \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|l|llll|llll|l}

    \toprule
    \multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multicolumn{1}{c|}{\multirow{2}{*}{Overall}}  &\multicolumn{4}{c|}{Perspective} & \multicolumn{4}{c|}{Transformative} & \multicolumn{1}{c}{\multirow{2}{*}{Others}}  \\
      \cmidrule(lr){3-6}  \cmidrule(lr){7-10}
     & & \header{Angle} & \header{Partial} & \header{Scope} & \header{Occlusion} & \header{Temporal} & \header{Deformation} & \header{Incomplete} & \header{Biological}  \\ 
    \midrule
    % \rowcolor[rgb]{0.93,0.93,0.93} \multicolumn{11}{l}{\textit{Heuristic baselines}} \\
    Random chance & 24.83 &27.64 &23.98 &24.51 &19.44 &22.15 &25.49 &29.41 &25.49 &22.5\\
     Human performance & 38.47 & 25.16  & 34.96 & 31.37    &41.67    &
       21.48 &24.51   & 58.82 & 54.9    & 53.33   \\
    \hspace{2em}  + Retrieved RAG & 61.38$_\text{\textcolor{red}{+22.91}}$  & 62.42$_\text{\textcolor{red}{+37.26}}$  & 60.16$_\text{\textcolor{red}{+25.2}}$  & 58.82$_\text{\textcolor{red}{+27.45}}$  & 62.96$_\text{\textcolor{red}{+21.29}}$  & 54.36$_\text{\textcolor{red}{+32.88}}$  & 49.02$_\text{\textcolor{red}{+24.51}}$  & 78.43$_\text{\textcolor{red}{+19.61}}$  & 63.73$_\text{\textcolor{red}{+8.83}}$  & 62.5$_\text{\textcolor{red}{+9.17}}$ \\
    \hspace{2em}  + GT RAG  & 71.63$_\text{\textcolor{red}{+33.16}}$  & 83.85$_\text{\textcolor{red}{+58.69}}$  & 70.33$_\text{\textcolor{red}{+35.37}}$  &  66.67$_\text{\textcolor{red}{+35.3}}$  & 69.44$_\text{\textcolor{red}{+27.77}}$  & 59.73$_\text{\textcolor{red}{+38.25}}$  & 68.63$_\text{\textcolor{red}{+44.12}}$  & 83.33$_\text{\textcolor{red}{+24.51}}$  & 73.53$_\text{\textcolor{red}{+18.63}}$  & 69.17$_\text{\textcolor{red}{+15.84}}$  \\
    \midrule
    \multicolumn{11}{l}{\hfill \textit{Open-Source LVLMs}}  \\ % change to Open-source MLLMs ? \\
    \midrule
    OpenFlamingo-v2-9B  & 26.83 &27.95 &26.02 &31.37 &30.56 &29.53 &34.31 &20.59 &17.65 &21.67  \\
    \hspace{2em}  + Retrieved RAG & 28.31$_\text{\textcolor{red}{+1.48}}$  & 29.5$_\text{\textcolor{red}{+1.55}}$  & 28.86$_\text{\textcolor{red}{+2.84}}$  & 28.43$_\text{\textcolor{blue}{-2.94}}$  & 30.56$_\text{\textcolor{red}{+0.0}}$  & 34.23$_\text{\textcolor{red}{+4.7}}$  & 31.37$_\text{\textcolor{blue}{-2.94}}$  & 22.55$_\text{\textcolor{red}{+1.96}}$  & 21.57$_\text{\textcolor{red}{+3.92}}$  & 22.5$_\text{\textcolor{red}{+0.83}}$ \\ 
    \hspace{2em}  + GT RAG  & 28.90$_\text{\textcolor{red}{+2.07}}$  & 26.71$_\text{\textcolor{blue}{-1.24}}$  & 33.74$_\text{\textcolor{red}{+7.72}}$  & 28.43$_\text{\textcolor{blue}{-2.94}}$  & 33.33$_\text{\textcolor{red}{+2.77}}$  & 35.57$_\text{\textcolor{red}{+6.04}}$  & 27.45$_\text{\textcolor{blue}{-6.86}}$  & 27.45$_\text{\textcolor{red}{+6.86}}$  & 25.49$_\text{\textcolor{red}{+7.84}}$  & 18.33$_\text{\textcolor{blue}{-3.34}}$  \\

    \midrule
    Idefics2-8B & 31.04 &31.06 &33.33 &31.37 &38.89 &30.2 &35.29 &25.49 &24.51 &26.67 \\
    \hspace{2em}  + Retrieved RAG   & 30.16$_\text{\textcolor{blue}{-0.88}}$  & 29.81$_\text{\textcolor{blue}{-1.25}}$  & 27.64$_\text{\textcolor{blue}{-5.69}}$  & 29.41$_\text{\textcolor{blue}{-1.96}}$  & 36.11$_\text{\textcolor{blue}{-2.78}}$  & 36.24$_\text{\textcolor{red}{+6.04}}$  & 28.43$_\text{\textcolor{blue}{-6.86}}$  & 27.45$_\text{\textcolor{red}{+1.96}}$  & 32.35$_\text{\textcolor{red}{+7.84}}$  & 25.83$_\text{\textcolor{blue}{-0.84}}$  \\
    \hspace{2em}  + GT RAG  & 37.03$_\text{\textcolor{red}{+5.99}}$  & 36.34$_\text{\textcolor{red}{+5.28}}$  & 35.37$_\text{\textcolor{red}{+2.04}}$  & 38.24$_\text{\textcolor{red}{+6.87}}$  & 54.63$_\text{\textcolor{red}{+15.74}}$  & 47.65$_\text{\textcolor{red}{+17.45}}$  & 36.27$_\text{\textcolor{red}{+0.98}}$  & 24.51$_\text{\textcolor{blue}{-0.98}}$  & 34.31$_\text{\textcolor{red}{+9.8}}$  & 25.83$_\text{\textcolor{blue}{-0.84}}$   \\ 
    \midrule
    VILA1.5-13B  &43.68 &45.34 &41.87 &52.94 &48.15 &50.34 &38.24 &21.57 &30.39 &\high 57.5   \\
    \hspace{2em}  + Retrieved RAG & 35.48$_\text{\textcolor{blue}{-8.2}}$  & 33.54$_\text{\textcolor{blue}{-11.8}}$  & 28.86$_\text{\textcolor{blue}{-13.01}}$  & 29.41$_\text{\textcolor{blue}{-23.53}}$  & 40.74$_\text{\textcolor{blue}{-7.41}}$  & 47.65$_\text{\textcolor{blue}{-2.69}}$  & 33.33$_\text{\textcolor{blue}{-4.91}}$  & 22.55$_\text{\textcolor{red}{+0.98}}$  & 33.33$_\text{\textcolor{red}{+2.94}}$  & \high 54.17$_\text{\textcolor{blue}{-3.33}}$ \\
    \hspace{2em}  + GT RAG &47.01$_\text{\textcolor{red}{+3.33}}$  & 45.65$_\text{\textcolor{red}{+0.31}}$  & 46.75$_\text{\textcolor{red}{+4.88}}$  & 39.22$_\text{\textcolor{blue}{-13.72}}$  & 51.85$_\text{\textcolor{red}{+3.7}}$  & 53.69$_\text{\textcolor{red}{+3.35}}$  & 43.14$_\text{\textcolor{red}{+4.9}}$  & 25.49$_\text{\textcolor{red}{+3.92}}$  & 44.12$_\text{\textcolor{red}{+13.73}}$  & \high 69.17$_\text{\textcolor{red}{+11.67}}$  \\
    \midrule
    
    % \midrule
    %    InternVL2-8B  & 41.24 &41.3 &40.65 &38.24 &40.74 &46.98 &39.22 &25.49 &45.1 &50.0  \\
    % \hspace{2em}  + Retrieved RAG \\
    % \hspace{2em}  + GT RAG (Scoring keyboard) & 42.42$_\text{\textcolor{red}{+1.18}}$  & 44.72$_\text{\textcolor{red}{+3.42}}$  & 46.34$_\text{\textcolor{red}{+5.69}}$  & 40.2$_\text{\textcolor{red}{+1.96}}$  & 41.67$_\text{\textcolor{red}{+0.93}}$  & 51.01$_\text{\textcolor{red}{+4.03}}$  & 33.33$_\text{\textcolor{blue}{-5.89}}$  & 20.59$_\text{\textcolor{blue}{-4.9}}$  & 21.57$_\text{\textcolor{blue}{-23.53}}$  & 64.17$_\text{\textcolor{red}{+14.17}}$ \\
    % UPDATED BY MOHSEN 9/16/24
        % UPDATED BY MOHSEN 9/16/24
       Mantis-8B-clip-llama3  &  40.8 &45.03 &39.43 &42.16 &49.07 &49.66 &36.27 &28.43 &19.61 &45.0 \\
    \hspace{2em}  + Retrieved RAG & 36.88$_\text{\textcolor{blue}{-3.92}}$  & 36.65$_\text{\textcolor{blue}{-8.38}}$  & 34.96$_\text{\textcolor{blue}{-4.47}}$  & 42.16$_\text{\textcolor{red}{0.0}}$  & 47.22$_\text{\textcolor{blue}{-1.85}}$  & \high 50.34$_\text{\textcolor{red}{+0.68}}$  & 33.33$_\text{\textcolor{blue}{-2.94}}$  & 18.63$_\text{\textcolor{blue}{-9.8}}$  & 21.57$_\text{\textcolor{red}{+1.96}}$  & 42.5$_\text{\textcolor{blue}{-2.5}}$\\
    \hspace{2em}  + GT RAG & 44.72$_\text{\textcolor{red}{+3.92}}$  & 48.14$_\text{\textcolor{red}{+3.11}}$  & 46.75$_\text{\textcolor{red}{+7.32}}$  & 43.14$_\text{\textcolor{red}{+0.98}}$  & 54.63$_\text{\textcolor{red}{+5.56}}$  & 57.05$_\text{\textcolor{red}{+7.39}}$  & 45.1$_\text{\textcolor{red}{+8.83}}$  & 19.61$_\text{\textcolor{blue}{-8.82}}$  & 18.63$_\text{\textcolor{blue}{-0.98}}$  & 51.67$_\text{\textcolor{red}{+6.67}}$ \\
    \midrule
    
      Mantis-8B-siglip-llama3  &  45.01 &46.89 &45.12 &57.84 &58.33 &45.64 &45.1 &26.47 &29.41 &45.0 \\
    \hspace{2em}  + Retrieved RAG & 39.62$_\text{\textcolor{blue}{-5.39}}$  & 42.55$_\text{\textcolor{blue}{-4.34}}$  & 35.37$_\text{\textcolor{blue}{-9.75}}$  & 47.06$_\text{\textcolor{blue}{-10.78}}$  & 47.22$_\text{\textcolor{blue}{-11.11}}$  & 42.95$_\text{\textcolor{blue}{-2.69}}$  & 45.1$_\text{\textcolor{red}{0.0}}$  & 23.53$_\text{\textcolor{blue}{-2.94}}$  & 29.41$_\text{\textcolor{red}{0.0}}$  & 40.83$_\text{\textcolor{blue}{-4.17}}$ \\
    \hspace{2em}  + GT RAG & 48.85$_\text{\textcolor{red}{+3.84}}$  & 54.66$_\text{\textcolor{red}{+7.77}}$  & 52.85$_\text{\textcolor{red}{+7.73}}$  & 51.96$_\text{\textcolor{blue}{-5.88}}$  & 58.33$_\text{\textcolor{red}{0.0}}$  & 48.99$_\text{\textcolor{red}{+3.35}}$  & 50.0$_\text{\textcolor{red}{+4.9}}$  & 21.57$_\text{\textcolor{blue}{-4.9}}$  & 33.33$_\text{\textcolor{red}{+3.92}}$  & 49.17$_\text{\textcolor{red}{+4.17}}$ \\
    \midrule

    % UPDATED BY MOHSEN 9/16/24
    %-mPLUG-Owl3-7B-240728
    
    
    % \midrule
    %    % xgen-mm-phi3-mini-instruct-interleave 
    %  xGen-MM (BLIP-3) -Phi-3-mini & 48.71 &51.55 &54.07 &53.92 &50.93 &51.01 &46.08 &25.49 &37.25 &52.5 \\
    % \hspace{2em}  + Retrieved RAG \\
    % \hspace{2em}  + GT RAG \\
    % UPDATED BY MOHSEN 9/16/24
     Deepseek-VL-7B-chat & 43.39 &45.34 &47.56 &47.06 &45.37 &46.31 &48.04 &28.43 &20.59 &49.17 \\
    \hspace{2em}  + Retrieved RAG & 34.66$_\text{\textcolor{blue}{-8.73}}$  & 33.54$_\text{\textcolor{blue}{-11.8}}$  & 32.11$_\text{\textcolor{blue}{-15.45}}$  & 33.33$_\text{\textcolor{blue}{-13.73}}$  & 37.04$_\text{\textcolor{blue}{-8.33}}$  & 43.62$_\text{\textcolor{blue}{-2.69}}$  & 40.2$_\text{\textcolor{blue}{-7.84}}$  & 20.59$_\text{\textcolor{blue}{-7.84}}$  & 26.47$_\text{\textcolor{red}{+5.88}}$  & 45.0$_\text{\textcolor{blue}{-4.17}}$\\
    \hspace{2em}  + GT RAG & 50.33$_\text{\textcolor{red}{+6.94}}$  & 54.04$_\text{\textcolor{red}{+8.7}}$  & 56.5$_\text{\textcolor{red}{+8.94}}$  & 50.98$_\text{\textcolor{red}{+3.92}}$  & 56.48$_\text{\textcolor{red}{+11.11}}$  & 57.05$_\text{\textcolor{red}{+10.74}}$  & 50.0$_\text{\textcolor{red}{+1.96}}$  & 21.57$_\text{\textcolor{blue}{-6.86}}$  & 23.53$_\text{\textcolor{red}{+2.94}}$  & 60.83$_\text{\textcolor{red}{+11.66}}$ \\
    \midrule
LLaVA-NeXT-Interleave-7B  & 43.46 &44.41 &43.5 &40.2 &\high 64.81 &44.97 &44.12 &32.35 &26.47 &45.83  \\
    \hspace{2em}  + Retrieved RAG & 40.35$_\text{\textcolor{blue}{-3.11}}$  & 40.06$_\text{\textcolor{blue}{-4.35}}$  & 33.33$_\text{\textcolor{blue}{-10.17}}$  & 39.22$_\text{\textcolor{blue}{-0.98}}$  & 56.48$_\text{\textcolor{blue}{-8.33}}$  & 43.62$_\text{\textcolor{blue}{-1.35}}$  & 44.12$_\text{\textcolor{red}{+0.0}}$  & 27.45$_\text{\textcolor{blue}{-4.9}}$  & 36.27$_\text{\textcolor{red}{+9.8}}$  & 49.17$_\text{\textcolor{red}{+3.34}}$  \\
    \hspace{2em}  + GT RAG & 52.99$_\text{\textcolor{red}{+9.53}}$  & 54.97$_\text{\textcolor{red}{+10.56}}$  & 54.88$_\text{\textcolor{red}{+11.38}}$  & 49.02$_\text{\textcolor{red}{+8.82}}$  & 62.04$_\text{\textcolor{blue}{-2.77}}$  & 52.35$_\text{\textcolor{red}{+7.38}}$  & 47.06$_\text{\textcolor{red}{+2.94}}$  & \high 38.24$_\text{\textcolor{red}{+5.89}}$  & 48.04$_\text{\textcolor{red}{+21.57}}$  & 61.67$_\text{\textcolor{red}{+15.84}}$  \\
    \midrule 
       mPLUG-Owl3-7B  &  49.74 &48.45 &50.81 &54.9 &58.33 &\high 54.36 &51.96 &30.39 &45.1 &51.67 \\
    \hspace{2em}  + Retrieved RAG & 41.83$_\text{\textcolor{blue}{-7.91}}$  & 40.06$_\text{\textcolor{blue}{-8.39}}$  & 36.59$_\text{\textcolor{blue}{-14.22}}$  & 40.2$_\text{\textcolor{blue}{-14.7}}$  & 50.0$_\text{\textcolor{blue}{-8.33}}$  & \high 50.34$_\text{\textcolor{blue}{-4.02}}$  & 46.08$_\text{\textcolor{blue}{-5.88}}$  & 20.59$_\text{\textcolor{blue}{-9.8}}$  & 51.96$_\text{\textcolor{red}{+6.86}}$  & 46.67$_\text{\textcolor{blue}{-5.0}}$ \\
    \hspace{2em}  + GT RAG & 56.32$_\text{\textcolor{red}{+6.58}}$  & 58.39$_\text{\textcolor{red}{+9.94}}$  & 58.94$_\text{\textcolor{red}{+8.13}}$  & 58.82$_\text{\textcolor{red}{+3.92}}$  & 62.96$_\text{\textcolor{red}{+4.63}}$  & \high 61.74$_\text{\textcolor{red}{+7.38}}$  & \high 59.8$_\text{\textcolor{red}{+7.84}}$  & 26.47$_\text{\textcolor{blue}{-3.92}}$  & 50.0$_\text{\textcolor{red}{+4.9}}$  & 58.33$_\text{\textcolor{red}{+6.66}}$  \\
    \midrule
       LLaVA-OneVision &\high{53.29} & \high 58.39 &\high 56.1 &49.02 &60.19 &47.65 &\high 53.92 &37.25 &\high 52.94 &51.67   \\
    \hspace{2em}  + Retrieved RAG & \high{50.11}$_\text{\textcolor{blue}{-3.18}}$  & 50.93$_\text{\textcolor{blue}{-7.46}}$  &  \high 48.78$_\text{\textcolor{blue}{-7.32}}$  & 50.0$_\text{\textcolor{red}{+0.98}}$  & \high 60.19$_\text{\textcolor{red}{+0.0}}$  & \high 50.34$_\text{\textcolor{red}{+2.69}}$  & \high 48.04$_\text{\textcolor{blue}{-5.88}}$  & \high 33.33$_\text{\textcolor{blue}{-3.92}}$  & \high 53.92$_\text{\textcolor{red}{+0.98}}$  & \high 54.17$_\text{\textcolor{red}{+2.5}}$ \\
    \hspace{2em}  + GT RAG & 58.98$_\text{\textcolor{red}{+5.69}}$  & 62.42$_\text{\textcolor{red}{+4.03}}$  & \high 63.82$_\text{\textcolor{red}{+7.72}}$  & 59.8$_\text{\textcolor{red}{+10.78}}$  & \high 66.67$_\text{\textcolor{red}{+6.48}}$  & 59.73$_\text{\textcolor{red}{+12.08}}$  & 53.92$_\text{\textcolor{red}{+0.0}}$  & 30.39$_\text{\textcolor{blue}{-6.86}}$  & \high 57.84$_\text{\textcolor{red}{+4.9}}$  & 60.83$_\text{\textcolor{red}{+9.16}}$ \\
    \midrule
    % UPDATED BY MOHSEN 9/22/24
    Pixtral-12B & 47.97 &52.48 &45.53 &\high 58.82 &50.0 &51.68 &49.02 &\high 38.24 &42.16 &37.5 \\
    \hspace{2em}  + Retrieved RAG & 45.97$_\text{\textcolor{blue}{-2.0}}$  & \high 51.86$_\text{\textcolor{blue}{-0.62}}$  & 40.24$_\text{\textcolor{blue}{-5.29}}$  & \high 53.92$_\text{\textcolor{blue}{-4.9}}$  & 50.93$_\text{\textcolor{red}{+0.93}}$  & 49.66$_\text{\textcolor{blue}{-2.02}}$  & 47.06$_\text{\textcolor{blue}{-1.96}}$  & 19.61$_\text{\textcolor{blue}{-18.63}}$  & 47.06$_\text{\textcolor{red}{+4.9}}$  & 46.67$_\text{\textcolor{red}{+9.17}}$ \\
    \hspace{2em}  + GT RAG & \high{59.28}$_\text{\textcolor{red}{+11.31}}$  & \high 63.04$_\text{\textcolor{red}{+10.56}}$  & 63.41$_\text{\textcolor{red}{+17.88}}$  & \high 65.69$_\text{\textcolor{red}{+6.87}}$  & \high 66.67$_\text{\textcolor{red}{+16.67}}$  & \high 61.74$_\text{\textcolor{red}{+10.06}}$  & \high 59.8$_\text{\textcolor{red}{+10.78}}$  & 20.59$_\text{\textcolor{blue}{-17.65}}$  & 50.98$_\text{\textcolor{red}{+8.82}}$  & 65.0$_\text{\textcolor{red}{+27.5}}$ \\

    
    % (confused after \#3 images!) & 0.96$_\text{\textcolor{blue}{-47.75}}$  & 0.0$_\text{\textcolor{blue}{-51.55}}$  & 0.0$_\text{\textcolor{blue}{-54.07}}$  & 0.0$_\text{\textcolor{blue}{-53.92}}$  & 0.0$_\text{\textcolor{blue}{-50.93}}$  & 0.0$_\text{\textcolor{blue}{-51.01}}$  & 0.0$_\text{\textcolor{blue}{-46.08}}$  & 12.75$_\text{\textcolor{blue}{-12.74}}$  & 0.0$_\text{\textcolor{blue}{-37.25}}$  & 0.0$_\text{\textcolor{blue}{-52.5}}$ \\
    % \midrule
    % CogVLM2-llama3-chat-19B \\ 
    % \hspace{2em}  + Retrieved RAG \\
    % \hspace{2em}  + GT RAG \\
      \midrule
     \multicolumn{11}{l}{\hfill \textit{Proprietary LVLMs} } \\ % change to Proprietary Models ?
    \midrule
   
    GPT-4-Turbo & 57.21 &64.29 &59.35 &54.9 &56.48 &62.42 &47.06 &41.18 &59.8 &50.0 \\ 
    \hspace{2em}  + Retrieved RAG  &  58.95$_\text{\textcolor{red}{+1.74}}$  & 66.53$_\text{\textcolor{red}{+2.24}}$  & 59.94$_\text{\textcolor{red}{+0.59}}$  & 53.94$_\text{\textcolor{blue}{-0.96}}$  & 66.74$_\text{\textcolor{red}{+10.26}}$  & 59.73$_\text{\textcolor{blue}{-2.69}}$  & 49.06$_\text{\textcolor{red}{+2.0}}$  & \best 38.27$_\text{\textcolor{blue}{-2.91}}$  & \best 62.78$_\text{\textcolor{red}{+2.98}}$  & 58.83$_\text{\textcolor{red}{+8.83}}$  \\
    \hspace{2em}  + GT RAG & 62.85$_\text{\textcolor{red}{+5.64}}$  & 68.94$_\text{\textcolor{red}{+4.65}}$  & 69.51$_\text{\textcolor{red}{+10.16}}$  & 60.78$_\text{\textcolor{red}{+5.88}}$  & 67.59$_\text{\textcolor{red}{+11.11}}$  & 63.33$_\text{\textcolor{red}{+0.91}}$  & 51.96$_\text{\textcolor{red}{+4.9}}$  & \best 38.24$_\text{\textcolor{blue}{-2.94}}$  & 59.8$_\text{\textcolor{red}{+0.0}}$  & 62.5$_\text{\textcolor{red}{+12.5}}$   \\
     \midrule
    Gemini Pro  & 61.71 &68.01 &69.92 &\best 73.53 &71.3 &70.47 &42.16 &39.22 &53.92 &40.83 \\
     \hspace{2em}  + Retrieved RAG  & 65.93$_\text{\textcolor{red}{+4.22}}$  & 73.29$_\text{\textcolor{red}{+5.28}}$  & 69.92$_\text{\textcolor{red}{+0.0}}$  & \best 69.61$_\text{\textcolor{blue}{-3.92}}$  & 73.15$_\text{\textcolor{red}{+1.85}}$  & \best 75.84$_\text{\textcolor{red}{+5.37}}$  & 49.02$_\text{\textcolor{red}{+6.86}}$  & 34.31$_\text{\textcolor{blue}{-4.91}}$  & 56.86$_\text{\textcolor{red}{+2.94}}$  & 65.0$_\text{\textcolor{red}{+24.17}}$  \\
    \hspace{2em}  + GT RAG & 71.40$_\text{\textcolor{red}{+9.69}}$  & 77.33$_\text{\textcolor{red}{+9.32}}$  & 79.27$_\text{\textcolor{red}{+9.35}}$  & 78.43$_\text{\textcolor{red}{+4.9}}$  & 75.93$_\text{\textcolor{red}{+4.63}}$  & \best 78.52$_\text{\textcolor{red}{+8.05}}$  & 54.9$_\text{\textcolor{red}{+12.74}}$  & 36.27$_\text{\textcolor{blue}{-2.95}}$  & 61.76$_\text{\textcolor{red}{+7.84}}$  & 72.5$_\text{\textcolor{red}{+31.67}}$  \\
     \midrule
    Claude 3.5 Sonnet  & 59.87 &70.19 &57.72 &56.86 &57.41 &68.46 &48.04 &\best 49.02 &\best 62.75 &47.5\\
     \hspace{2em}  + Retrieved RAG  & 63.56$_\text{\textcolor{red}{+3.69}}$  & 73.91$_\text{\textcolor{red}{+3.72}}$  & 70.73$_\text{\textcolor{red}{+13.01}}$  & 56.86$_\text{\textcolor{red}{+0.0}}$  & 62.96$_\text{\textcolor{red}{+5.55}}$  & 70.47$_\text{\textcolor{red}{+2.01}}$  & \best 55.88$_\text{\textcolor{red}{+7.84}}$  & 31.37$_\text{\textcolor{blue}{-17.65}}$  & 62.75$_\text{\textcolor{red}{+0.0}}$  & 53.33$_\text{\textcolor{red}{+5.83}}$ \\
    \hspace{2em}  + GT RAG & 71.10$_\text{\textcolor{red}{+11.23}}$  & 78.88$_\text{\textcolor{red}{+8.69}}$  & \best 80.49$_\text{\textcolor{red}{+22.77}}$  & 76.47$_\text{\textcolor{red}{+19.61}}$  & 70.37$_\text{\textcolor{red}{+12.96}}$  & 75.17$_\text{\textcolor{red}{+6.71}}$  & 67.65$_\text{\textcolor{red}{+19.61}}$  & 36.27$_\text{\textcolor{blue}{-12.75}}$  & \best 65.69$_\text{\textcolor{red}{+2.94}}$  & 59.17$_\text{\textcolor{red}{+11.67}}$ \\
    \midrule
     GPT-4o & \best{68.68} & \best 76.09 &\best  70.42 & 69.61 & \best 74.07 & \best  73.82 & \best 61.21 & 47.62  & 58.82  & \best 65.83 \\ 
    % \wh{rerun, double check} \\
    % 70.44 &78.88 &78.46 &70.59 &74.07 &72.48 &63.73 &33.33 &64.71 &67.5
    \hspace{2em}  + Retrieved RAG & \best 68.96$_\text{\textcolor{red}{+0.28}}$  & \best 77.95$_\text{\textcolor{red}{+1.86}}$  &  \best 78.86$_\text{\textcolor{red}{+8.44}}$  & \best 69.61$_\text{\textcolor{red}{+0.0}}$  &  \best 75.0$_\text{\textcolor{red}{+0.93}}$  & 73.83$_\text{\textcolor{red}{+0.01}}$  & 54.9$_\text{\textcolor{red}{+7.28}}$  & 26.47$_\text{\textcolor{blue}{-34.74}}$  & 59.8$_\text{\textcolor{red}{+0.98}}$  & \best 68.33$_\text{\textcolor{red}{+2.5}}$  \\
    % & 63.64$_\text{\textcolor{blue}{-5.04}}$  & 75.47$_\text{\textcolor{blue}{-0.62}}$  & 65.45$_\text{\textcolor{blue}{-4.97}}$  & 62.75$_\text{\textcolor{blue}{-6.86}}$  & 73.15$_\text{\textcolor{blue}{-0.92}}$  & 70.47$_\text{\textcolor{blue}{-3.35}}$  & 53.92$_\text{\textcolor{red}{+6.3}}$  & 20.59$_\text{\textcolor{blue}{-40.62}}$  & 56.86$_\text{\textcolor{blue}{-1.96}}$  & 62.5$_\text{\textcolor{blue}{-3.33}}$  \\
    \hspace{2em}  + GT RAG & \best 74.50$_\text{\textcolor{red}{+5.82}}$  & \best 84.47$_\text{\textcolor{red}{+8.38}}$  & 77.46$_\text{\textcolor{red}{+7.04}}$  & \best 82.35$_\text{\textcolor{red}{+12.74}}$  & \best 79.63$_\text{\textcolor{red}{+5.56}}$  & 77.18$_\text{\textcolor{red}{+3.36}}$ & \best 68.62$_\text{\textcolor{red}{+7.41}}$  & 30.95$_\text{\textcolor{blue}{-16.67}}$    & 62.75$_\text{\textcolor{red}{+3.93}}$  & \best 80.0$_\text{\textcolor{red}{+14.17}}$  \\
    \bottomrule
    \end{tabular}
    }
    % \vspace{-2mm}
    \caption{Accuracy scores on \dataset. The highest scores for \colorbox{backblue!75}{open-source} models in each section and \colorbox{backred!50}{proprietary} models are highlighted in blue and red, respectively. Both Retrieved RAG and GT RAG employ top-5 image examples (except for the incomplete scenario, where a single example is intuitively sufficient). The relative difference in performance compared to the score without RAG is shown in subscript, with \blue{blue} indicating performance drops and \red{red} indicating improvements.}
    % you can use \high{20.5}  and  \best{50.5}  
\vspace{-3mm}
\label{tab:mainresults}
\end{table*}

\section{Experiments}
\label{sec: experiment}

In this section, we first introduce the experimental setup and evaluation metric ($\S~\ref{sec: experimental setup}$). Then, we present a comprehensive evaluation of 14 recent LVLMs ($\S~\ref{sec: main results}$). We demonstrate the importance of visual knowledge and discuss the critical findings revealed by the results from \dataset.

\subsection{Experimental Setup}
\label{sec: experimental setup}

We evaluate 
% \jg{I prefer using past tense through the experiment section.}\zd{present tense should be fine (and easier?) as long as it's consistent} 
14 popular LVLMs on \dataset, including 4 proprietary models and 10 open-sourced models that can accept multi-image inputs:

\begin{itemize}[leftmargin=7.5mm]
\setlength{\itemsep}{1pt}
\item \textbf{Proprietary models}: GPT-4o (0513)~\citep{gpt4}, GPT-4-Turbo~\citep{gpt4}, Gemini Pro~\citep{team2023gemini}, and Claude 3.5 Sonnet~\citep{claude35}. 
\item \textbf{Open-source models}: OpenFlamingo (v2-9B)~\citep{awadalla2023openflamingo}, Idefics (v2-8B)~\citep{idefics2}, VILA (v1.5-13B)~\citep{lin2023vila}, LLaVA-NeXT-Interleave-7B~\citep{li2024llavanextinterleavetacklingmultiimagevideo}, LLaVA-OneVision~\citep{li2024llavaonevisioneasyvisualtask}, Mantis (clip-llama3, and siglip-llama3 versions; 8B)~\citep{jiang2024mantis}, mPLUG-Owl3-7B~\citep{ye2024mplugowl3longimagesequenceunderstanding}, Deepseek-VL-7B-chat~\citep{lu2024deepseekvl}, and Pixtral-12B~\citep{pixtral12b}.
\end{itemize}

\paragraph{Evaluation setup.} We follow standard MCQA evaluation setup and employ accuracy score as our metric. We adopt default generation hyper-parameters selected by each model. Following \citet{lu2023mathvista}, we employ GPT-3.5-turbo to extract the multiple choice answer in rare cases where our pre-defined automatic extraction rules failed. We refer the readers to Appendix~\ref{appendix:data collection} and \ref{appendix:exp setup} for more details on evaluation prompts for both without multimodal RAG and with multimodal RAG scenarios, answer extraction prompt and human performance evaluation protocol.

\subsection{Main Results}
\label{sec: main results}

As shown in Table~\ref{tab:mainresults}, the average performance of the most advanced LVLMs is not better than 68.68\% without multimodal RAG knowlege, and 74.5\% with ground-truth knowledge, which demonstrates \dataset to be a challenging benchmark. The mean accuracies of open-source LVLMs are between 26.83\% and 53.29\% without RAG knowledge and between 28.90\% and 59.28\% with ground-truth knowledge, which fall behind from advanced proprietary LVLMs. Notably, \dataset proves to be knowledge-intensive as average humans achieved 38.47\% without RAG knowledge, while proprietary LVLMs generally perform well, suggesting that their extensive training data equips them with a broader knowledge base.
However, when provided with either retrieved or ground-truth knowledge, humans achieve the most significant improvements of 22.91\% and 33.16\%, respectively. This underscore the need of LVLMs to better utilize visually augmented information like humans. 

\paragraph{Can LVLMs utilize retrieved and ground-truth image knowledge well?} As illustrated in Table~\ref{tab:mainresults}, all models demonstrate improvement when ground-truth image RAG knowledge is provided. Among the open-source models, they achieve improvements ranging from 2.07\% to 11.31\% when using ground-truth RAG knowledge, whereas 5.64\% to 9.69\% improvements are observed from proprietary LVLMs. Interestingly, when images from the multimodal retriever is provided, almost all open-source LVLMs on average demonstrate a declined performance while proprietary models can still gain improvement. This indicate proprietary models possess emerging abilities to distinguish between good and bad image knowledge sources, which is a critical skill in the multimodal RAG domain. We further conducted a qualitative analysis to investigate the reasons behind this, as detailed in the following paragraphs.

% \wh{In the experiment section, we need to point out? our benchmark is a visual knowledge based benchmark. The model with highest score when not using rag, simply means it contained the most knowledge under our \emph{perspective} and \emph{transformative} evaluation. }

\paragraph{Fine-grained results.} 
We also report fine-grained scores across 9 scenarios on \dataset in Table~\ref{tab:mainresults}. Remarkably, GPT-4o surpasses most other baselines in various categories, with exceptions in problems related to partial, incomplete and biological scenarios. Notably, GPT-4o outperforms human performance on all perspective aspect as well as on temporal and deformation scenarios within the transformative aspect. We conjecture that incomplete and biological scenarios are less likely to be included in the training knowledge. Interestingly, all models exhibit a decline in performance on incomplete scenarios, with only a few exceptions, while humans find this task relatively easy, achieving 58.82\% and 83.33\% scores with ground-truth knowledge.  This further highlights the importance of leveraging retrieved visually augmented knowledge to address questions that do not directly incentivize knowledge stored in the models' memories. 

\paragraph{Why can proprietary models better utilize retrieved images?} We conduct an error analysis on an open-source model (LLaVA-Next-Interleave) and a proprietary model (Gemini Pro). For a fair comparison, we filtered results where LLaVA-Next-Interleave answered correctly without or with GT knowledge but was misled to wrong answer with retrieved examples. One example is illustrated in Figure~\ref{fig:error_analysis}, the retrieved images contain two correct examples and three false examples. While Gemini Pro is able to utilize all retrieved images, LLaVA-Next-Interleave leverages bad examples and makes wrong prediction. This example helps explain why do almost all open-source models have lower performance with retrieved knowledge.  



% \begin{figure}[t]
%   \centering
%   % \vspace{-1mm}
%   \includegraphics[width=1.0\textwidth]{files/retriver_vs_models.png}
%   % \vspace{-5mm}
%   \caption{}}
%   % \vspace{-3mm}
% \label{fig:retriver_vs_models}
% \end{figure}

\section{Analysis}




In this section, we conduct quantitative analysis addressing three important questions: 1) To what extent can LVLMs benefit more from visual knowledge than from textual knowledge on \dataset? ($\S$~\ref{sec:analysis: text vs image}) 2) How does the performance of LVLMs vary with examples retrieved from different retrievers? ($\S$~\ref{sec:analysis: different retrievers}) 3) How many ground-truth visual knowledge examples are required for LVLMs to continue benefiting? ($\S$~\ref{sec:analysis: number of ground turth examples.})

\subsection{How much can visual knowledge benefit more than textual knowledge?}
\label{sec:analysis: text vs image}


% \begin{wraptable}{r}{7cm}
% \centering
%  \resizebox{1.0\linewidth}{!}{
%   \renewcommand\tabcolsep{1.0pt} % column space
%  \renewcommand\arraystretch{0.8} % row space
%  \begin{tabular}{l|l|ll}
%  \toprule
%      % \multicolumn{1}{c|}{Model} & \multicolumn{1}{c|}{Overall}  &\multicolumn{1}{c|}{Perspective} & \multicolumn{1}{c|}{Transformative}\\
%     Model & Overall & Perspective & Transformative \\
%     \midrule
%     LLaVA-NeXT-Interleave-7B  & 43.46   & 48.23 &  36.98 \\
%     \hspace{1em}  + RAR Text RAG & 37.99$_\text{\textcolor{blue}{-5.47}}$  & 38.97$_\text{\textcolor{blue}{-9.26}}$  & 33.68$_\text{\textcolor{blue}{-3.3}}$  \\
%     \hspace{1em}  + RAR  Image RAG & 40.35$_\text{\textcolor{blue}{-3.11}}$ & 42.27$_\text{\textcolor{blue}{-5.96}}$  & 37.86$_\text{\textcolor{red}{+0.88}}$  \\
%      \hspace{1em}  + GT Text RAG & 41.09$_\text{\textcolor{blue}{-2.37}}$ & 43.92$_\text{\textcolor{blue}{-4.31}}$  & 35.02$_\text{\textcolor{blue}{-1.96}}$ \\ 
%     \hspace{1em}  + GT Image RAG & 52.99$_\text{\textcolor{red}{+9.53}}$ & 55.23$_\text{\textcolor{red}{+7.0}}$  & 46.42$_\text{\textcolor{red}{+9.44}}$  \\
%     \hspace{1em}  + GT Image \& Text RAG &  47.82$_\text{\textcolor{red}{+4.36}}$  & 49.76$_\text{\textcolor{red}{+1.53}}$  & 40.84$_\text{\textcolor{red}{+3.86}}$   \\
%     \midrule
%     GPT-4-Turbo & 57.21 & 58.76 & 52.61 \\
%     \hspace{1em}  + RAR  Text RAG & 56.61$_\text{\textcolor{blue}{-0.6}}$ & 59.82$_\text{\textcolor{red}{+1.06}}$  & 50.55$_\text{\textcolor{blue}{-2.06}}$  \\
%     \hspace{1em}  + RAR Image RAG  & 58.95$_\text{\textcolor{red}{+1.74}}$ & 60.79$_\text{\textcolor{red}{+2.03}}$  & 50.96$_\text{\textcolor{blue}{-1.65}}$  \\
%     \hspace{1em}  + GT Text RAG & 58.98$_\text{\textcolor{red}{+1.77}}$ & 65.25$_\text{\textcolor{red}{+6.49}}$  & 48.51$_\text{\textcolor{blue}{-4.1}}$  \\ 
%     \hspace{1em}  + GT Image RAG & 62.85$_\text{\textcolor{red}{+5.64}}$ & 66.7$_\text{\textcolor{red}{+7.94}}$  & 53.33$_\text{\textcolor{red}{+0.72}}$    \\
%     \hspace{1em}  + GT Image \& Text RAG & 65.11$_\text{\textcolor{red}{+7.9}}$ & 70.71$_\text{\textcolor{red}{+11.95}}$  & 54.47$_\text{\textcolor{red}{+1.86}}$  \\
%  \bottomrule
%  \end{tabular}
% }
% \caption{LVLM performance on \dataset with textual knowledge v.s visual knowledge. Here RAR means retrieved, GT means ground-truth. Both the open-source and proprietary model benefit more from image knowledge.}
% \label{tab:rag_text_baseline_shorter}
% \end{wraptable}

\begin{figure}[t]
  \centering
  % \vspace{-1mm}
  \includegraphics[trim=0.2cm 9.8cm 1.5cm 0.3cm, clip, width=1.0\textwidth]{files/Error_analysis.pptx-2.pdf}
  % \vspace{-5mm}
  \caption{Qualitative Example of Proprietary model (Gemini Pro)  identifies and utilizes correct examples, while open-source model (LLaVA-Next-Interleave) is misled by noisy retrieved information, resulting in incorrect answers.}
  % \vspace{-3mm}
\label{fig:error_analysis}
\end{figure}

\begin{table*}[t]
\vspace{-3mm}
\centering
 \small
 \renewcommand\tabcolsep{2.5pt} % column space
 \renewcommand\arraystretch{0.95} % row space
 \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|l|llll|llll|l}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multicolumn{1}{c|}{\multirow{2}{*}{Overall}}  &\multicolumn{4}{c|}{Perspective} & \multicolumn{4}{c|}{Transformative} & \multicolumn{1}{c}{\multirow{2}{*}{Others}}  \\
      \cmidrule(lr){3-6}  \cmidrule(lr){7-10}
     & & \header{Angle} & \header{Partial} & \header{Scope} & \header{Occlusion} & \header{Temporal} & \header{Deformation} & \header{Incomplete} & \header{Biological}  \\
    \midrule
    LLaVA-NeXT-Interleave-7B  & 43.46 &44.41 &43.5 &40.2 & 64.81 &44.97 &44.12 &32.35 &26.47 &45.83  \\
    \hspace{2em}  + Retrieved  Text RAG & 37.99$_\text{\textcolor{blue}{-5.47}}$  & 37.58$_\text{\textcolor{blue}{-6.83}}$  & 34.96$_\text{\textcolor{blue}{-8.54}}$  & 33.33$_\text{\textcolor{blue}{-6.87}}$  & 50.0$_\text{\textcolor{blue}{-14.81}}$  & 41.61$_\text{\textcolor{blue}{-3.36}}$  & 35.29$_\text{\textcolor{blue}{-8.83}}$  & 30.39$_\text{\textcolor{blue}{-1.96}}$  & 27.45$_\text{\textcolor{red}{+0.98}}$  & 51.67$_\text{\textcolor{red}{+5.84}}$ \\
    
    \hspace{2em}  + Retrieved  Image RAG & 40.35$_\text{\textcolor{blue}{-3.11}}$  & 40.06$_\text{\textcolor{blue}{-4.35}}$  & 33.33$_\text{\textcolor{blue}{-10.17}}$  & 39.22$_\text{\textcolor{blue}{-0.98}}$  & 56.48$_\text{\textcolor{blue}{-8.33}}$  & 43.62$_\text{\textcolor{blue}{-1.35}}$  & 44.12$_\text{\textcolor{red}{+0.0}}$  & 27.45$_\text{\textcolor{blue}{-4.9}}$  & 36.27$_\text{\textcolor{red}{+9.8}}$  & 49.17$_\text{\textcolor{red}{+3.34}}$  \\
     \hspace{2em}  + GT Text RAG & 41.09$_\text{\textcolor{blue}{-2.37}}$  & 41.93$_\text{\textcolor{blue}{-2.48}}$  & 39.02$_\text{\textcolor{blue}{-4.48}}$  & 38.24$_\text{\textcolor{blue}{-1.96}}$  & 56.48$_\text{\textcolor{blue}{-8.33}}$  & 44.97$_\text{\textcolor{red}{+0.0}}$  & 43.14$_\text{\textcolor{blue}{-0.98}}$  & 30.39$_\text{\textcolor{blue}{-1.96}}$  & 21.57$_\text{\textcolor{blue}{-4.9}}$  & 50.83$_\text{\textcolor{red}{+5.0}}$  \\ 
    \hspace{2em}  + GT Image RAG & 52.99$_\text{\textcolor{red}{+9.53}}$  & 54.97$_\text{\textcolor{red}{+10.56}}$  & 54.88$_\text{\textcolor{red}{+11.38}}$  & 49.02$_\text{\textcolor{red}{+8.82}}$  & 62.04$_\text{\textcolor{blue}{-2.77}}$  & 52.35$_\text{\textcolor{red}{+7.38}}$  & 47.06$_\text{\textcolor{red}{+2.94}}$  & 38.24$_\text{\textcolor{red}{+5.89}}$  & 48.04$_\text{\textcolor{red}{+21.57}}$  & 61.67$_\text{\textcolor{red}{+15.84}}$  \\
\hspace{2em}  + GT Image \& Text RAG & 47.82$_\text{\textcolor{red}{+4.36}}$  & 47.83$_\text{\textcolor{red}{+3.42}}$  & 48.78$_\text{\textcolor{red}{+5.28}}$  & 44.12$_\text{\textcolor{red}{+3.92}}$  & 58.33$_\text{\textcolor{blue}{-6.48}}$  & 49.66$_\text{\textcolor{red}{+4.69}}$  & 48.04$_\text{\textcolor{red}{+3.92}}$  & 30.39$_\text{\textcolor{blue}{-1.96}}$  & 35.29$_\text{\textcolor{red}{+8.82}}$  & 62.5$_\text{\textcolor{red}{+16.67}}$  \\
    \midrule

    GPT-4-Turbo & 57.21 &64.29 &59.35 &54.9 &56.48 &62.42 &47.06 &41.18 &59.8 &50.0 \\
    \hspace{2em}  + Retrieved  Text RAG & 56.61$_\text{\textcolor{blue}{-0.6}}$  & 61.8$_\text{\textcolor{blue}{-2.49}}$  & 59.35$_\text{\textcolor{red}{+0.0}}$  & 59.8$_\text{\textcolor{red}{+4.9}}$  & 58.33$_\text{\textcolor{red}{+1.85}}$  & 59.06$_\text{\textcolor{blue}{-3.36}}$  & 49.02$_\text{\textcolor{red}{+1.96}}$  & 33.33$_\text{\textcolor{blue}{-7.85}}$  & 60.78$_\text{\textcolor{red}{+0.98}}$  & 52.5$_\text{\textcolor{red}{+2.5}}$ \\
    \hspace{2em}  + Retrieved Image RAG  & 58.95$_\text{\textcolor{red}{+1.74}}$  & 66.53$_\text{\textcolor{red}{+2.24}}$  & 59.94$_\text{\textcolor{red}{+0.59}}$  & 53.94$_\text{\textcolor{blue}{-0.96}}$  & 66.74$_\text{\textcolor{red}{+10.26}}$  & 59.73$_\text{\textcolor{blue}{-2.69}}$  & 49.06$_\text{\textcolor{red}{+2.0}}$  & 38.27$_\text{\textcolor{blue}{-2.91}}$  & 62.78$_\text{\textcolor{red}{+2.98}}$  & 58.83$_\text{\textcolor{red}{+8.83}}$  \\
    \hspace{2em}  + GT Text RAG & 58.98$_\text{\textcolor{red}{+1.77}}$  & 68.01$_\text{\textcolor{red}{+3.72}}$  & 63.41$_\text{\textcolor{red}{+4.06}}$  & 65.69$_\text{\textcolor{red}{+10.79}}$  & 63.89$_\text{\textcolor{red}{+7.41}}$  & 59.73$_\text{\textcolor{blue}{-2.69}}$  & 38.24$_\text{\textcolor{blue}{-8.82}}$  & 37.25$_\text{\textcolor{blue}{-3.93}}$  & 58.82$_\text{\textcolor{blue}{-0.98}}$  & 50.83$_\text{\textcolor{red}{+0.83}}$  \\ 
    \hspace{2em}  + GT Image RAG & 62.85$_\text{\textcolor{red}{+5.64}}$  & 68.94$_\text{\textcolor{red}{+4.65}}$  & 69.51$_\text{\textcolor{red}{+10.16}}$  & 60.78$_\text{\textcolor{red}{+5.88}}$  & 67.59$_\text{\textcolor{red}{+11.11}}$  & 63.33$_\text{\textcolor{red}{+0.91}}$  & 51.96$_\text{\textcolor{red}{+4.9}}$  & 38.24$_\text{\textcolor{blue}{-2.94}}$  & 59.8$_\text{\textcolor{red}{+0.0}}$  & 62.5$_\text{\textcolor{red}{+12.5}}$   \\
\hspace{2em}  + GT Image \& Text RAG & 65.11$_\text{\textcolor{red}{+7.9}}$  & 72.05$_\text{\textcolor{red}{+7.76}}$  & 72.76$_\text{\textcolor{red}{+13.41}}$  & 67.65$_\text{\textcolor{red}{+12.75}}$  & 70.37$_\text{\textcolor{red}{+13.89}}$  & 71.81$_\text{\textcolor{red}{+9.39}}$  & 46.08$_\text{\textcolor{blue}{-0.98}}$  & 39.22$_\text{\textcolor{blue}{-1.96}}$  & 60.78$_\text{\textcolor{red}{+0.98}}$  & 57.5$_\text{\textcolor{red}{+7.5}}$  \\
    \bottomrule
    \end{tabular}
    }
    % \vspace{-2mm}
    \caption{LVLMs performance on \dataset with textual knowledge v.s visual knowledge. Both the open-source and proprietary model benefit more from image knowledge.}
    % you can use \high{20.5}  and  \best{50.5}  
\vspace{-3mm}
\label{tab:textbasline}
\end{table*}

We used the Wikipedia corpus as of 2023/07/01 as our text knowledge corpus\footnote{https://www.kaggle.com/datasets/jjinho/wikipedia-20230701}. To ensure a fair comparison, we employed the same multimodal retriever (CLIP) for retrieving either text or image knowledge. The top-5 ranked documents or images are used for augmenting the input. 
% \jg{I think right now is good and polished a little bit. top-5 documents for text and top-5 images for visual augmentation? If not, how many retrieved images are adopted here? Do we have references showing that a document and an image (or n images) have the same knowledge intensity? }
% % and provided 5 documents for text-based RAG. 
% \wh{need Jia-Chen's expertise on this writing, do you think we should describe more detail of doing this?, if so, we can put more details into appendix?} 
We selected one open-source (LLaVA-Next-Interleave) and one proprietary (GPT-4-Turbo) LVLM to examine their preference for textual knowledge versus image knowledge on \dataset. As shown in Table~\ref{tab:textbasline}, when both models utilized retrieved knowledge, LLaVA-Next-Interleave demonstrated a 2.36\% improvement with image knowledge over text knowledge, while GPT-4-Turbo showed a 2.34\% improvement. When using GT knowledge, LLaVA-Next-Interleave exhibited an 11.09\% improvement with image knowledge over text knowledge, compared to a 3.87\% improvement for GPT-4-Turbo. 
Interestingly, when both GT image and text knowledge are provided, LLaVA-Next-Interleave indicated less improvement than with GT image alone whereas GPT-4-Turbo further pushed its performance. 
All these results demonstrate that retrieving visual knowledge is more helpful than retrieving text on \dataset. 




% \begin{wrapfigure}{r}{0.5\textwidth}
%   \centering
%   \includegraphics[width=\linewidth]{files/retriver_vs_models.png}
%   \caption{LLaVA-Next-Interleaved results with 4 different multimodal retrievers on average of 9 tasks. Its performance using retrieved images correlates 95\% with retriever's Recall@5 scores.}
%   \label{fig:retriver_vs_models}
% \end{wrapfigure}

\subsection{How does retriever performance affect LVLMs?}
\label{sec:analysis: different retrievers}

We picked four recent best-performing multimodal retrievers, including CLIP~\citep{clipradford2021learningtransferablevisualmodels}, MagicLens~\citep{Zhang2024MagicLens}, E5-V~\citep{jiang2024e5vuniversalembeddingsmultimodal}, VISTA~\citep{zhou2024vistavisualizedtextembedding} and evaluated their performance (Recall@5). The detailed retriever performance can be found at Table~\ref{tab:retriever scores} in Appendix~\ref{appendix: more results}.  We selected LLaVA-Next-Interleave as the end model to assess its performance. As shown in Figure~\ref{fig:analysis}, when retrievers achieve higher Recall@5 scores (i.e., better retrieved examples),  the LVLM’s accuracy tends to improve, demonstrating a strong 95\% positive correlation. Interestingly, despite similar Recall@5 scores from CLIP and VISTA retrievers, LLaVA-Next-Interleave demonstrated a 2.07\% gap in overall accuracy. We conjecture that the order of the correctly retrieved examples may also impact the model's final performance. The sensitivity to the order of retrieved examples is a common issue that persists across various models. 
Although this phenomenon, known as position bias, has been examined in text-based RAG~\citep{lu2022fantastically,wang2024large}, its impact on visual RAG remains unexplored, presenting a promising direction for future research.
% \jg{
% The sensitivity to the order of retrieved examples is a common issue that persists across various models. 
% Although this phenomenon, known as position bias, has been examined in text-based RAG~\citep{lu2022fantastically,wang2024large}, its impact on visual RAG remains unexplored, presenting a promising direction for future research.
% }
% \wh{very convincing. can we add one citation and add them to the writing please. }
% This suggests that \dataset is well-suited as a benchmark for evaluating multimodal retrievers as well and underscores the need for the community to develop more effective models for retrieving visually augmented knowledge. 
% \wh{need advice for this claim, and maybe point this out in the intro as well, reviewers may think that retrieving on our benchmark is hard.}
% \jg{I think this claim is more helpful for retrieval end, not helpful for us. We might be challenged by the reviewers that the bottleneck of MM-RAG lies in the performance of MM retriever, instead of the robust understanding ability of LVLMs. Given a set of noisy retrieval results, how to make use of the retrieved results (e.g., have the ability to pay more attention to relevant documents/images, and avoid the distraction of irrelevant documents/images) is what we want to convey with this benchmark.}


\subsection{How many ground-truth image examples are needed? }
\label{sec:analysis: number of ground turth examples.}

For simplicity, all our experiments used five retrieved or ground-truth image examples. However, it is worth exploring how many examples LVLMs can effectively leverage. As noted in $\S$~\ref{sec: data collection}, the perspective aspect of our benchmark includes an average of 20.4 ground-truth examples. To investigate further, we perform an analysis focusing on the perspective and others aspects, covering a total of 892 questions. As shown in Figure~\ref{fig:analysis}, we evaluated LLaVA-Next-Interleave using 1, 2, 3, 5, 10, 20 GT examples, averaging the results across three random seeds for sampling the GT examples. LLaVA-Next-Interleave saw the greatest improvement of 5.64\% with just one GT example. Performance continued to increase steadily, reaching a peak at 10 GT examples, which was 0.29\% higher than with 20 GT examples. One possible explanation could be LLaVA-Next-Interleave may not able to better leverage visually augmented knowledge in long context scenarios. Moreover, the complexity of questions affects the number of images needed too, one ground-truth example sometimes help the model the most on \dataset.  We encourage the research on adaptatively deciding the number of necessary images based on the complexity of questions.

% \wh{We can do either: 1. leave as it. or 2. + claim because knowledge after some number is not that useful in our benchmark or 3. we should encourage developing models to be better utilizing long context (many images examples)? }
% \jg{I think it would be interesting to discuss the relationships between the complexity of questions and the number of necessary images. Intuitively, more complex questions need more images. We encourage the research on adaptatively deciding the number of necessary images based on the complexity of questions.}


% \begin{figure}[t]
% % \vspace{-5mm}
%  \begin{minipage}{0.58\textwidth} 
%  \centering
 % \fontsize{8.2pt}{\baselineskip}\selectfont % font size
 % \renewcommand\tabcolsep{1.0pt} % column space
 % \renewcommand\arraystretch{0.8} % row space
 % \begin{tabular}{l|l|ll}
 % \toprule
 %     % \multicolumn{1}{c|}{Model} & \multicolumn{1}{c|}{Overall}  &\multicolumn{1}{c|}{Perspective} & \multicolumn{1}{c|}{Transformative}\\
 %    Model & Overall & Perspective & Transformative \\
 %    \midrule
 %    LLaVA-NeXT-Interleave-7B  & 43.46   & 48.23 &  36.98 \\
 %    \hspace{1em}  + RAR Text RAG & 37.99$_\text{\textcolor{blue}{-5.47}}$  & 38.97$_\text{\textcolor{blue}{-9.26}}$  & 33.68$_\text{\textcolor{blue}{-3.3}}$  \\
 %    \hspace{1em}  + RAR  Image RAG & 40.35$_\text{\textcolor{blue}{-3.11}}$ & 42.27$_\text{\textcolor{blue}{-5.96}}$  & 37.86$_\text{\textcolor{red}{+0.88}}$  \\
 %     \hspace{1em}  + GT Text RAG & 41.09$_\text{\textcolor{blue}{-2.37}}$ & 43.92$_\text{\textcolor{blue}{-4.31}}$  & 35.02$_\text{\textcolor{blue}{-1.96}}$ \\ 
 %    \hspace{1em}  + GT RAG & 52.99$_\text{\textcolor{red}{+9.53}}$ & 55.23$_\text{\textcolor{red}{+7.0}}$  & 46.42$_\text{\textcolor{red}{+9.44}}$  \\
 %    \midrule
 %    GPT-4-Turbo & 57.21 & 58.76 & 52.61 \\
 %    \hspace{1em}  + RAR  Text RAG & 56.61$_\text{\textcolor{blue}{-0.6}}$ & 59.82$_\text{\textcolor{red}{+1.06}}$  & 50.55$_\text{\textcolor{blue}{-2.06}}$  \\
 %    \hspace{1em}  + RAR Image RAG  & 58.95$_\text{\textcolor{red}{+1.74}}$ & 60.79$_\text{\textcolor{red}{+2.03}}$  & 50.96$_\text{\textcolor{blue}{-1.65}}$  \\
 %    \hspace{1em}  + GT Text RAG & 58.98$_\text{\textcolor{red}{+1.77}}$ & 65.25$_\text{\textcolor{red}{+6.49}}$  & 48.51$_\text{\textcolor{blue}{-4.1}}$  \\ 
 %    \hspace{1em}  + GT Image RAG & 62.85$_\text{\textcolor{red}{+5.64}}$ & 66.7$_\text{\textcolor{red}{+7.94}}$  & 53.33$_\text{\textcolor{red}{+0.72}}$    \\
 % \bottomrule
 % \end{tabular}
 % \captionof{table}{RAG text baseline, RAR means Retrieved, GT means ground-truth .... \wh{changing this to  analysis 3 figure, may leave this as a full table. todo, add text + image rag }}
 % \label{tab:statistics}
%  \end{minipage} 
%  \hfill
%  \begin{minipage}{0.4\textwidth}
%  \centering
%  \vspace{-1mm}
%   \includegraphics[width=\linewidth]{files/retriver_vs_models.png}
%   \caption{LLaVA-Next-Interleaved results with 4 different multimodal retrievers on average of 9 tasks. Its performance using retrieved images correlates 95\% with retriever's Recall@5 scores. \wh{make axis font bigger}}
%  \label{fig:retiever_vs_model}
%  \end{minipage}
%  % \vspace{-2mm}
% \end{figure}




\begin{figure}[t]
  \centering
  % \vspace{-1mm}
  \includegraphics[width=1.0\textwidth]{files/analysis_new.png}
  % \vspace{-5mm}
  \caption{Left: LLaVA-Next-Interleave results with 4 different multimodal retrievers. Its performance using retrieved images correlates 95\% with retriever's Recall@5 scores. Right: Average results of three random seed runs. Improve the number of ground-truth RAG examples shows steady increase of model's performance, reaches the maximum with 10 examples.}
  % \vspace{-3mm}
\label{fig:analysis}
\end{figure}

\section{Related Work}

% \zd{add some sentences here. like we overview three lines of related work, xx, yy, zz}

We overview three lines of related work: 1) multimodal retrieval-augmented generation benchmarks ($\S$~\ref{sec: related works: benchmarks}), 2) large vision language models ($\S$~\ref{sec: related works: lvlm}), and 3) retrieval-augmented mutlimodal large language models ($\S$~\ref{sec: related works: rag mllm}). 

\subsection{Multimodal Retrieval-Augmented Generation Benchmarks}
\label{sec: related works: benchmarks}

A number of recent benchmarks have been developed to comprehensively assess the capabilities of LVLMs~\citep{lu2021inter,lu2022learn,li2023seed,liu2023mmbench,lu2023mathvista,yue2023mmmu,mathverse2024zhang,ying2024mmt}. There are several benchmarks well-suited for evaluating retrieval-augmented LVLMs. For instance, OK-VQA~\citep{okvqa} and A-OKVQA~\citep{schwenk2022okvqa} both focus on scenarios where external textual knowledge is required to answer visual questions. More recent works~\citep{chang2022webqamultihopmultimodalqa, chen2023infoseek, encvqa} have curated large-scale knowledge bases to evaluate models on knowledge-intensive and information-seeking visual questions. In contrast, \dataset focus on scenarios where retrieving visual information is more helpful than retrieving text. 

\subsection{Large Vision Language Models}
\label{sec: related works: lvlm}

Large Vision Language Models (LVLMs)~\citep{liu2024visual, zhu2023minigpt, dai2023instructblip, yin2023survey, hu2024matryoshka} have showcased promising results on a wide variety of vision-language tasks. Many works, such as Flamingo~\citep{alayrac2022flamingo}, Emu~\citep{sun2023generative}, Idefics~\citep{laurenccon2024obelics}, and VILA~\citep{lin2023vila}, have demonstrated in-context learning capabilities, where multiple image examples can be leveraged to improve text generation. Recent works start training LVLMs with interleaved image-text corpora, such as MMC4~\citep{zhu2024multimodal} and OBELICS~\citep{laurenccon2024obelics}, for pretraining, as well as high-quality instruction tuning in models like Mantis-Instruct~\citep{jiang2024mantis}, LLaVA-Next-Interleave~\citep{li2024llavanextinterleavetacklingmultiimagevideo}, and LLaVA-OneVision~\citep{li2024llavaonevisioneasyvisualtask}, enabling models to process and understand information from multiple images. Naturally, evaluating the ability of LVLMs to effectively leverage visually augmented knowledge becomes an important task, which is the primary focus of \dataset.


\subsection{Retrieval-Augmented Multimodal Large Language Models}
\label{sec: related works: rag mllm}

Retrieval-augmented generation (RAG) has emerged as a potential solution to overcome limitations in language models by incorporating external knowledge retrieval during the generation process~\citep{DBLP:conf/nips/LewisPPPKGKLYR020,DBLP:journals/corr/abs-2301-12652}. Reasonably, several works have focused on using multimodal knowledge to enhance the generation capabilities of Large Language Models (LLMs)~\citep{RACM3, chen-etal-2022-murag, zhao-etal-2023-retrieving-survey, cui-etal-2024-multi}. Recently, more works~\citep{caffagni2024wikillavahierarchicalretrievalaugmentedgeneration, xuan2024lemmalvlmenhancedmultimodalmisinformation, du2024vulragenhancingllmbasedvulnerability} has incorporated external knowledge to improve LVLMs' general generation abilities and the comprehensiveness of their reasoning. Although some works~\citep{chen-etal-2022-murag, Yuan2023RAMMRB} have proposed directly using image information from the web, a systematic vision-centric benchmark to evaluate LVLMs' abilities to leverage visually augmented knowledge is lacking, which is the focus of our work.

\section{Conclusion}

In this work, we introduce \dataset, a benchmark specifically designed for vision-centric
evaluation for retrieval-augmented multimodal models. Our evaluation of 14 LVLMs highlights that visually augmented knowledge brings more improvements on \dataset compared to textual knowledge. Moreover, the top-performing model, GPT-4o, struggles to effectively utilize the retrieved knowledge, achieving only a 5.82\% improvement when augmented with relevant information, compared to a 33.16\% improvement demonstrated by human participants. We further conduct extensive analysis and propose several promising directions for future research. Our findings underscore the significance of \dataset in motivating the community to develop LVLMs that better utilize retrieved visual knowledge. 




% \subsubsection*{Acknowledgments}
%if needed. 


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

% \clearpage
% \appendix
% \addcontentsline{toc}{section}{Appendices} %
% \part{Appendices} %
% \parttoc %


\clearpage
\addtocontents{toc}{\protect\setcounter{tocdepth}{-1}}
\appendix

% Reset depth to add sections and subsections to ToC
\addtocontents{toc}{\protect\setcounter{tocdepth}{3}}
% Setting colorlinks=black just for the table of contents
\hypersetup{linkcolor=black}
\tableofcontents % Lists only the appendix sections and subsections
% Revert to original link colors after table of contents
\hypersetup{linkcolor=red}

\section{\dataset Details}
\label{appendix:dataset}

% \subsection{Dataset Statistics}
% presents

\subsection{Dataset Curation Details}
\label{appendix:data collection}

\paragraph{Dataset collection of transformative aspect}

We chose to manually scrape images from the web based on the definitions of the transformative aspect. To construct the image corpus, we employed Bing Image Search for each of the image object keyword predefined by us. We filtered some of the search results where the image objects do not have a clear pair of query image and ground-truth image example, around 74\% keyword names were kept during this process. Here we listed all the keywords that are already filtered and used for search of query image except in biological scenario, it's for search of ground-truth image example. Each search keyword is composed of an ``image object'' and a ``condition''. For example, ``A young kitten image of Himalayan Cat'', here  Himalayan Cat is the image object and a young kitten is the condition. For each of keyword listed below, we searched again for its ground-truth examples (except for biological scenario, it's for query images), in which only ``image object'' is kept and ``conditon'' is removed. All searched results are further picked and downloaded by humans to ensure quality. Here is a list of the filtered keywords for transformative aspect: 

% \begin{table*}[h!]\centering
% \begin{minipage}{0.95\textwidth} 
% \centering
% \begin{tcolorbox} 
%     \centering
%       \small
%     \begin{tabular}{p{0.95\textwidth}}
% \begin{itemize}

{\bf Transformative: Temporal}  \\
- A young kitten image of Himalayan Cat \\ 
- A young kitten image of Chartreux \\ 
- A young kitten image of Burmese\\ 
- A young kitten image of Turkish Van\\ 
- A young kitten image of American Shorthair\\ 
- A young kitten image of British Shorthair\\ 
- A young kitten image of Maine Coon\\ 
- A young kitten image of Burma (Myanmar)\\ 
- A young kitten image of Selkirk Rex\\ 
- A young kitten image of Siberian\\ 
- A young kitten image of Persian\\ 
- A young kitten image of Manx\\ 
- A young kitten image of Ocicat\\ 
- A young kitten image of Russian Blue\\ 
- A young kitten image of Bengal Cat\\ 
- A young kitten image of Devon Rex\\ 
- A young kitten image of American Bobtail\\ 
- A young kitten image of Balinese\\ 
- A young kitten image of LaPerm\\ 
- A young kitten image of Egyptian Mau\\ 
- A young kitten image of Japanese Bobtail\\ 
- A young kitten image of Ragdoll\\ 
- A young kitten image of Abyssinian\\ 
- A young kitten image of American Wirehair\\ 
- A young kitten image of Oriental Shorthair\\ 
- A young kitten image of Cornish Rex\\ 
- A young kitten image of Kurilian Bobtail\\ 
- A young kitten image of Singapura Cat\\ 
- A young kitten image of Birman\\ 
- A young kitten image of Burmilla\\ 
- A young kitten image of Korat\\ 
- A young kitten image of Tonkinese\\ 
- A young kitten image of Somali Cat\\ 
- A young kitten image of Norwegian Forest Cat\\ 
- A young kitten image of Turkish Angora\\ 
- A young kitten image of Siamese\\ 
- A picture of Sainte-Chapelle under construction \\ 
- A picture of Washington Monument under construction \\ 
- A picture of Hearst Castle under construction \\ 
- A picture of Time Square under construction \\ 
- A picture of Wrigley Building under construction \\ 
- A picture of Eiffel Tower under construction \\ 
- A picture of The Arc de Triomphe under construction \\ 
- A picture of Golden Gate Bridge under construction \\ 
- A picture of White House under construction \\ 
- A picture of Palace of Versailles under construction \\ 
- A picture of Opéra Garnier under construction \\ 
- A picture of San Simeon under construction \\ 
- A picture of The Louvre under construction \\ 
- A picture of Cathédrale Notre-Dame de Paris under construction \\ 
- A picture of Sacré-Cœur Basilica under construction \\ 
- A picture of Brooklyn Bridge under construction \\ 
- A picture of Panthéon under construction \\ 
- A picture of Capitol Building under construction \\ 
- A picture of Independence Hall under construction \\ 
- A picture of Mont Saint-Michel under construction \\ 
- A picture of St Patrick's Cathedral under construction \\ 
- A picture of Space Needle under construction \\ 
- A picture of Château de Chambord under construction \\ 
- A picture of Versailles under construction \\ \\
{\bf Transformative: Deformation}  \\
- An image of Toyota Camry damaged \\
- An image of Ford F-150 damaged \\
- An image of Ferrari 458 damaged \\
- An image of Audi Q5 damaged \\
- An image of Lamborghini LP640 damaged \\
- An image of McLaren 675LT damaged \\
- An image of Mercedes SLC damaged \\
- An image of Lamborghini Aventador damaged \\
- An image of Lamborghini LP570 damaged \\
- An image of Porsche 911 GT3 RS damaged \\
- An image of Audi A6 damaged \\
- An image of Audi A4 damaged \\
- An image of Lamborghini Aventador SV damaged \\
- An image of GMC Sierra 2500 HD damaged \\
- An image of Infiniti G37 damaged \\
- An image of GMC Yukon damaged \\
- An image of Honda Accord damaged \\
- An image of Infiniti FX35 damaged \\
- An image of Tesla Model 3 damaged \\
- An image of Acura RDX 2020 damaged \\
- An image of BMW 7 Series damaged \\
- An image of Audi A5 Sportback damaged \\
- An image of Hyundai IX35 damaged \\
- An image of Cadillac XTS damaged \\
- An image of BMW M3 damaged \\
- An image of Acura MDX damaged \\
- An image of Audi A3 damaged \\
- An image of BMW X3 damaged \\
- An image of Porsche Boxster damaged \\
- An image of Mercedes CLA45 AMG damaged \\
- An image of Jaguar XJ damaged \\\\
{\bf Transformative: Incomplete}  \\
- MacBook Keyboard missing keys \\ 
- Windows Keyboard missing keys \\ 
- Laptop Keyboards (Generic) missing keys \\ 
- Mechanical Keyboard missing keys \\ 
- Ergonomic Keyboard missing keys \\ 
- Compact Keyboard missing keys \\ 
- Gaming Keyboard missing keys \\ 
- Chiclet Keyboard missing keys \\ 
- Tenkeyless (TKL) Keyboard missing keys \\ 
- Virtual Keyboard (On-screen) missing keys \\ 
- Numeric Keypad missing keys \\ 
- ISO Keyboard Layout missing keys \\ 
- ANSI Keyboard Layout missing keys \\ 
- Ortholinear Keyboard missing keys \\ 
- Bluetooth/Wireless Keyboard missing keys \\ \\
{\bf Transformative: Biological}  \\
- An image of Lime after oxidation \\
- An image of breadfruit after oxidation \\
- An image of dragonfruit after oxidation\\
- An image of starfruit after oxidation\\
- An image of Raspberry after oxidation\\
- An image of Zucchini after oxidation\\
- An image of Pear after oxidation\\
- An image of passionfruit after oxidation\\
- An image of Blackberry after oxidation\\
- An image of durian after oxidation\\
- An image of persimmon after oxidation\\
- An image of Apple after oxidation\\
- An image of bell pepper after oxidation\\
- An image of olive after oxidation\\
- An image of Mango after oxidation\\
- An image of nectarine after oxidation\\
- An image of tomato after oxidation\\
- An image of quince after oxidation\\
- An image of coconut after oxidation\\
- An image of soursop after oxidation\\
- An image of Kiwi after oxidation\\
- An image of cucumber after oxidation\\
- An image of apricot after oxidation\\
- An image of Honeydew after oxidation\\
- An image of Peach after oxidation\\
- An image of pomegranate after oxidation\\
- An image of carrot after oxidation\\
- An image of fig after oxidation\\
- An image of Papaya after oxidation\\
- An image of Blueberry after oxidation\\
- An image of Banana after oxidation\\
- An image of jackfruit after oxidation\\
- An image of Lemon after oxidation\\
- An image of tamarind after oxidation\\
- An image of lychee after oxidation\\
- An image of Pineapple after oxidation\\
- An image of Cantaloupe after oxidation\\
- An image of Orange after oxidation\\
- An image of Rambutan after oxidation\\
- An image of guava after oxidation\\
- An image of sweet potato after oxidation\\
- An image of Plum after oxidation\\
- An image of Avocado after oxidation\\
- An image of Watermelon after oxidation\\
- An image of potato after oxidation\\
- An image of Grapefruit after oxidation\\
- An image of Grapes after oxidation\\
- An image of pumpkin after oxidation\\
- An image of Cherry after oxidation\\
- An image of Strawberry after oxidation\\
- An image of custard apple after oxidation

\paragraph{Quality control}
We employ two types of quality control throughout the annotation process: an automatic check with predefined rules and a manual examination of each instance.
The automatic check verifies correct MCQA format in which each question should only have one correct answer, metadata values, assesses image validity (checking the accessibility of each image) and filters out redundant images in the corpus (images that are repetitively downloaded). 
The manual examination
is conducted by two experts in this field, who checked the correspondence between query images and
ground-truth image examples, and filtered or revised ambiguous questions and uncorrelated query
image and ground-truth images. 




\subsection{Human Evaluation Protocol}
\label{appendix:human proptol}

Three human annotators in domain conducted the human evaluation. The interface for human evaluation without RAG knowledge and with RAG knowledge are shown in Figure~\ref{fig:human_evaluation_no_rag} and Figure~\ref{fig:human_evaluation_with_rag}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{files/human_eval_norag.png}
    \caption{Human evaluation interface without RAG examples}
    \label{fig:human_evaluation_no_rag}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{files/human_eval_rag.png}
    \caption{Human evaluation interface with ground-truth RAG examples}
    \label{fig:human_evaluation_with_rag}
\end{figure}


\section{Experiment Setting Details}
\label{appendix:exp setup}

% \subsection{Model Evaluation}


\subsection{Model Prompts}
Following \citet{lu2023mathvista} and \citet{liu2023improvedllava} our prompt consists of four parts, the instruction, question, options, and a prefix of the answer. For images, we insert them into the text to form a coherent prompt as the image placeholder (\{Image\})  indicated below. The complete prompt is as follows:

\begin{tcolorbox}[colback=white, colframe=black!75!black, boxrule=0.5pt, sharp corners, title=Model Prompts for No RAG Evaluation]
\label{model_prompts_no_rag}
\small
Instruction: Answer with the option's letter from the given choices directly. 

\{Image\}

Question: \{QUESTION\}

Choices:

(A) \{OPTION\_A\}

(B) \{OPTION\_B\}

(C) \{OPTION\_C\}

(D) \{OPTION\_D\}

Answer:
\end{tcolorbox}

\begin{tcolorbox}[colback=white, colframe=black!75!black, boxrule=0.5pt, sharp corners, title=Model Prompts for RAG Evaluation] 
\label{model_prompts_with_rag}
\small
Instruction: You will be given one question concerning several images. The first image is the input image, others are retrieved examples to help you. Answer with the option's letter from the given choices directly.  

\{Image\}\{Image\}\{Image\}\{Image\}\{Image\}\{Image\}

Question: \{QUESTION\}

Choices:

(A) \{OPTION\_A\}

(B) \{OPTION\_B\}

(C) \{OPTION\_C\}

(D) \{OPTION\_D\}

Answer:
\end{tcolorbox}

\subsection{Evaluation Tool}
Following \citet{lu2023mathvista}, we first use a rule-based automatic tool to extract the exact answer. First, the tool detects if a valid option index appears in the model output. If no direct answer is found, the tool matches the output to the content of each option. If there is still no match, we employ GPT-3.5-turbo to automatically extract the answer following our prompts in Table~\ref{tab:extraction prompts}. If GPT-3.5-turbo finds there is still no match, we will randomly select an option as the answer. 


\begin{table*}[h!]\centering
\begin{minipage}{0.95\textwidth} 
\centering
\begin{tcolorbox} 
    \centering
      \small
    \begin{tabular}{p{0.95\textwidth}}
    {\bf Prompt}  \\
        
Please read the following example. Then extract the multiple choice letter with the answer corresponding to the choice list from the model response and type it at the end of the prompt. You should only output either A, B, C, or D. \\\\
 
\textcolor[rgb]{0,0.7,0}{ \{In-context examples\} }  \\ \\
 
Question: \{QUESTION\} \\ 

Choice List:  (A) \{OPTION\_A\} (B) \{OPTION\_B\} (C) \{OPTION\_C\} (D) \{OPTION\_D\} \\ 

 
Model Response:  \{Response\}

Extracted answer:

\end{tabular}
\end{tcolorbox}
\caption{Prompt template to extract multiple choice answer from model's response. \textcolor[rgb]{0,0.7,0}{ \{In-context examples\} } are in-context examples.}
    \label{tab:extraction prompts}
\end{minipage}
\end{table*}

\section{More Results}
\label{appendix: more results}

We present the Recall@5 scores per each scenarios on 4 multimodal retreivers as shown in Table~\ref{tab:retriever scores} and LLaVA-Next-Interleave's accuracy score affected by these retrievers in Table~\ref{tab:retriever scores with llava}. 

\begin{table*}[h]
\vspace{-3mm}
\centering
 \small
 \renewcommand\tabcolsep{2.5pt} % column space
 \renewcommand\arraystretch{0.95} % row space
 \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|c|cccc|cccc|c}

    \toprule
    \multirow{2}{*}{Model} & \multirow{2}{*}{Overall}  &\multicolumn{4}{c|}{Perspective} & \multicolumn{4}{c|}{Transformative} &\multirow{2}{*}{Others}  \\
      \cmidrule(lr){3-6}  \cmidrule(lr){7-10}
     & & \header{Angle} & \header{Partial} & \header{Scope} & \header{Occlusion} & \header{Temporal} & \header{Deformation} & \header{Incomplete} & \header{Biological}  \\ 
    \midrule
    % \rowcolor[rgb]{0.93,0.93,0.93} \multicolumn{11}{l}{\textit{Heuristic baselines}} \\
     % MagicLens & 31.26 &41.61 &33.33 &36.27 &36.11 &12.75 &10.78 &2.94 &29.41 &56.67\\
     % CLIP & 54.69 &64.6 &47.15 &66.67 &70.37 &35.57 &26.47 &59.8 &41.18 &74.17 \\
     % UniVL-DR  \\
     % ImageBind \\
     % UniIR (CLIP-SF)&   \\
     % UniIR (BLIP-FF)&   \\
     MagicLens & 37.03 &41.61 &33.33 &36.27 &36.11 &12.75 &10.78 &79.41 &29.41 &56.67 \\
     E5-V & 54.92 &49.69 &48.78 &61.76 &66.67 &38.93 &22.55 &73.53 &71.57 &82.50 \\ 
     % VISTA & 58.68 &64.91 &64.63 &57.84 &64.81 &34.23 &10.78 &45.10 &91.18 &80.0 \\
     VISTA & 59.65 &66.15 &67.48 &64.71 &63.89 &38.26 &8.82 &33.33 &94.12 &80.83 \\
    CLIP &  60.46 &70.19 &54.47 &71.57 &73.15 &44.30 &31.37 &67.65 &40.2 &81.67 \\
    \bottomrule
    \end{tabular}
    }
    % \vspace{-2mm}
    \caption{Recall@5 scores with 4 retriever models  on \dataset.}
    % you can use \high{20.5}  and  \best{50.5}  
\vspace{-3mm}
\label{tab:retriever scores}
\end{table*}

\begin{table*}[h]
\vspace{-3mm}
\centering
 \small
 \renewcommand\tabcolsep{2.5pt} % column space
 \renewcommand\arraystretch{0.95} % row space
 \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|c|cccc|cccc|c}

    \toprule
    \multirow{2}{*}{Model} & \multirow{2}{*}{Overall}  &\multicolumn{4}{c|}{Perspective} & \multicolumn{4}{c|}{Transformative} &\multirow{2}{*}{Others}  \\
      \cmidrule(lr){3-6}  \cmidrule(lr){7-10}
     & & \header{Angle} & \header{Partial} & \header{Scope} & \header{Occlusion} & \header{Temporal} & \header{Deformation} & \header{Incomplete} & \header{Biological}  \\ 
    \midrule
    % \rowcolor[rgb]{0.93,0.93,0.93} \multicolumn{11}{l}{\textit{Heuristic baselines}} \\
     % MagicLens & 31.26 &41.61 &33.33 &36.27 &36.11 &12.75 &10.78 &2.94 &29.41 &56.67\\
     % CLIP & 54.69 &64.6 &47.15 &66.67 &70.37 &35.57 &26.47 &59.8 &41.18 &74.17 \\
     % UniVL-DR  \\
     % ImageBind \\
     % UniIR (CLIP-SF)&   \\
     % UniIR (BLIP-FF)&   \\
     MagicLens &35.18 &34.78 &29.67 &30.39 &34.26 &40.94 &36.27 &27.45 &49.02 &39.17 \\
     E5-V & 40.06 &38.82 &39.84 &41.18 &46.3 &38.93 &41.18 &27.45 &48.04 &41.67 \\ 
     % VISTA & 58.68 &64.91 &64.63 &57.84 &64.81 &34.23 &10.78 &45.10 &91.18 &80.0 \\
     VISTA & 42.42 &40.37 &35.77 &40.2 &52.78 &45.64 &42.16 &36.27 &50.98 &48.33 \\
    CLIP &  40.35 &40.06 &33.33 &39.22 &56.48 &43.62 &44.12 &27.45 &36.27 &49.17\\
    \bottomrule
    \end{tabular}
    }
    % \vspace{-2mm}
    \caption{LLaVA-Next-Interleave accuracy scores on \dataset with 4 different retrievers. }
    % you can use \high{20.5}  and  \best{50.5}  
\vspace{-3mm}
\label{tab:retriever scores with llava}
\end{table*}

\end{document}


\section{Limitations}
\label{sec:appendix:limitation}
\subsection{Limitation and future work}
There are several limitations to this work. First, we focus our scope on 2D images, and future research can further extend the idea of work to 3D problems, and include more multi-image tasks and relation categories. 
We hope our work can guide future efforts in providing robust and faithful evaluation in multimodal benchmarks. 
Our strategie


\subsection{Societal impacts}
Our work proposes \dataset, providing a robust evaluation on multi-image tasks using multimodal LLMs. 
While it includes a comprehensive list of 12 tasks, all of them are in English and could induce bias on multilingual research settings. 
Also, if misused, the multimodal LLMs may be used to generate harmful vision and text artifacts. Nevertheless, this is not directly related to our research, and the data we curate do not contain personally identifiable information or offensive content. However, more researchers should be encouraged to get involved in research on the safety issues in a multimodal context.


% \begin{table*}[t]
% \vspace{-3mm}
% \centering
%  \small
%  \renewcommand\tabcolsep{2.5pt} % column space
%  \renewcommand\arraystretch{0.95} % row space
%  \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{l|l|llll|lllll}
%     \toprule
%     \multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multicolumn{1}{c|}{\multirow{2}{*}{Overall}}  &\multicolumn{4}{c}{Perspective} & \multicolumn{4}{c}{Transformative} & \multicolumn{1}{c}{\multirow{2}{*}{Others}}  \\
%       \cmidrule(lr){3-6}  \cmidrule(lr){7-10}
%      & & \header{Angle} & \header{Partial} & \header{Scope} & \header{Occlusion} & \header{Temporal} & \header{Deformation} & \header{Incomplete} & \header{Biological}  \\
%     \midrule
%     LLaVA-NeXT-Interleave-7B  & 43.46 &44.41 &43.5 &40.2 &64.81 &44.97 &44.12 &32.35 &26.47 &45.83  \\
%     \hspace{2em}  + Retrieved  Text RAG & 37.99$_\text{\textcolor{blue}{-5.47}}$  & 37.58$_\text{\textcolor{blue}{-6.83}}$  & 34.96$_\text{\textcolor{blue}{-8.54}}$  & 33.33$_\text{\textcolor{blue}{-6.87}}$  & 50.0$_\text{\textcolor{blue}{-14.81}}$  & 41.61$_\text{\textcolor{blue}{-3.36}}$  & 35.29$_\text{\textcolor{blue}{-8.83}}$  & 30.39$_\text{\textcolor{blue}{-1.96}}$  & 27.45$_\text{\textcolor{red}{+0.98}}$  & 51.67$_\text{\textcolor{red}{+5.84}}$ \\
    
%     \hspace{2em}  + Retrieved  Image RAG & 40.35$_\text{\textcolor{blue}{-3.11}}$  & 40.06$_\text{\textcolor{blue}{-4.35}}$  & 33.33$_\text{\textcolor{blue}{-10.17}}$  & 39.22$_\text{\textcolor{blue}{-0.98}}$  & 56.48$_\text{\textcolor{blue}{-8.33}}$  & 43.62$_\text{\textcolor{blue}{-1.35}}$  & 44.12$_\text{\textcolor{red}{+0.0}}$  & 27.45$_\text{\textcolor{blue}{-4.9}}$  & 36.27$_\text{\textcolor{red}{+9.8}}$  & 49.17$_\text{\textcolor{red}{+3.34}}$  \\
%      \hspace{2em}  + GT Text RAG & 41.09$_\text{\textcolor{blue}{-2.37}}$  & 41.93$_\text{\textcolor{blue}{-2.48}}$  & 39.02$_\text{\textcolor{blue}{-4.48}}$  & 38.24$_\text{\textcolor{blue}{-1.96}}$  & 56.48$_\text{\textcolor{blue}{-8.33}}$  & 44.97$_\text{\textcolor{red}{+0.0}}$  & 43.14$_\text{\textcolor{blue}{-0.98}}$  & 30.39$_\text{\textcolor{blue}{-1.96}}$  & 21.57$_\text{\textcolor{blue}{-4.9}}$  & 50.83$_\text{\textcolor{red}{+5.0}}$  \\ 
%     \hspace{2em}  + GT Image RAG & 52.99$_\text{\textcolor{red}{+9.53}}$  & 54.97$_\text{\textcolor{red}{+10.56}}$  & 54.88$_\text{\textcolor{red}{+11.38}}$  & 49.02$_\text{\textcolor{red}{+8.82}}$  & 62.04$_\text{\textcolor{blue}{-2.77}}$  & 52.35$_\text{\textcolor{red}{+7.38}}$  & 47.06$_\text{\textcolor{red}{+2.94}}$  & 38.24$_\text{\textcolor{red}{+5.89}}$  & 48.04$_\text{\textcolor{red}{+21.57}}$  & 61.67$_\text{\textcolor{red}{+15.84}}$  \\
% \hspace{2em}  + GT Image \& Text RAG & 47.82$_\text{\textcolor{red}{+4.36}}$  & 47.83$_\text{\textcolor{red}{+3.42}}$  & 48.78$_\text{\textcolor{red}{+5.28}}$  & 44.12$_\text{\textcolor{red}{+3.92}}$  & 58.33$_\text{\textcolor{blue}{-6.48}}$  & 49.66$_\text{\textcolor{red}{+4.69}}$  & 48.04$_\text{\textcolor{red}{+3.92}}$  & 30.39$_\text{\textcolor{blue}{-1.96}}$  & 35.29$_\text{\textcolor{red}{+8.82}}$  & 62.5$_\text{\textcolor{red}{+16.67}}$  \\
%     \midrule

%     GPT-4-Turbo & 57.21 &64.29 &59.35 &54.9 &56.48 &62.42 &47.06 &41.18 &59.8 &50.0 \\
%     \hspace{2em}  + Retrieved  Text RAG & 56.61$_\text{\textcolor{blue}{-0.6}}$  & 61.8$_\text{\textcolor{blue}{-2.49}}$  & 59.35$_\text{\textcolor{red}{+0.0}}$  & 59.8$_\text{\textcolor{red}{+4.9}}$  & 58.33$_\text{\textcolor{red}{+1.85}}$  & 59.06$_\text{\textcolor{blue}{-3.36}}$  & 49.02$_\text{\textcolor{red}{+1.96}}$  & 33.33$_\text{\textcolor{blue}{-7.85}}$  & 60.78$_\text{\textcolor{red}{+0.98}}$  & 52.5$_\text{\textcolor{red}{+2.5}}$ \\
%     \hspace{2em}  + Retrieved Image RAG  & 58.95$_\text{\textcolor{red}{+1.74}}$  & 66.53$_\text{\textcolor{red}{+2.24}}$  & 59.94$_\text{\textcolor{red}{+0.59}}$  & 53.94$_\text{\textcolor{blue}{-0.96}}$  & 66.74$_\text{\textcolor{red}{+10.26}}$  & 59.73$_\text{\textcolor{blue}{-2.69}}$  & 49.06$_\text{\textcolor{red}{+2.0}}$  & 38.27$_\text{\textcolor{blue}{-2.91}}$  & 62.78$_\text{\textcolor{red}{+2.98}}$  & 58.83$_\text{\textcolor{red}{+8.83}}$  \\
%     \hspace{2em}  + GT Text RAG & 58.98$_\text{\textcolor{red}{+1.77}}$  & 68.01$_\text{\textcolor{red}{+3.72}}$  & 63.41$_\text{\textcolor{red}{+4.06}}$  & 65.69$_\text{\textcolor{red}{+10.79}}$  & 63.89$_\text{\textcolor{red}{+7.41}}$  & 59.73$_\text{\textcolor{blue}{-2.69}}$  & 38.24$_\text{\textcolor{blue}{-8.82}}$  & 37.25$_\text{\textcolor{blue}{-3.93}}$  & 58.82$_\text{\textcolor{blue}{-0.98}}$  & 50.83$_\text{\textcolor{red}{+0.83}}$  \\ 
%     \hspace{2em}  + GT Image RAG & 62.85$_\text{\textcolor{red}{+5.64}}$  & 68.94$_\text{\textcolor{red}{+4.65}}$  & 69.51$_\text{\textcolor{red}{+10.16}}$  & 60.78$_\text{\textcolor{red}{+5.88}}$  & 67.59$_\text{\textcolor{red}{+11.11}}$  & 63.33$_\text{\textcolor{red}{+0.91}}$  & 51.96$_\text{\textcolor{red}{+4.9}}$  & 38.24$_\text{\textcolor{blue}{-2.94}}$  & 59.8$_\text{\textcolor{red}{+0.0}}$  & 62.5$_\text{\textcolor{red}{+12.5}}$   \\
% \hspace{2em}  + GT Image \& Text RAG & 65.11$_\text{\textcolor{red}{+7.9}}$  & 72.05$_\text{\textcolor{red}{+7.76}}$  & 72.76$_\text{\textcolor{red}{+13.41}}$  & 67.65$_\text{\textcolor{red}{+12.75}}$  & 70.37$_\text{\textcolor{red}{+13.89}}$  & 71.81$_\text{\textcolor{red}{+9.39}}$  & 46.08$_\text{\textcolor{blue}{-0.98}}$  & 39.22$_\text{\textcolor{blue}{-1.96}}$  & 60.78$_\text{\textcolor{red}{+0.98}}$  & 57.5$_\text{\textcolor{red}{+7.5}}$  \\
%     \bottomrule
%     \end{tabular}
%     }
%     % \vspace{-2mm}
%     \caption{RAG text baseline, RAR means Retrieved, GT means ground-truth ....}
%     % you can use \high{20.5}  and  \best{50.5}  
% \vspace{-3mm}
% \label{tab:textbasline}
% \end{table*}


