% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{amssymb} 
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{makecell}  
\usepackage{bbding}  
\usepackage{bbm}
\usepackage[ruled,linesnumbered]{algorithm2e}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\definecolor{blue}{HTML}{074799}   
\definecolor{red}{HTML}{C62E2E}   
\definecolor{lightblue}{HTML}{C4D9FF}  
\definecolor{lightred}{HTML}{FAE0DF}


\title{
Benchmarking Multimodal RAG through a Chart-based Document Question-Answering Generation Framework
}


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Yuming Yang\textsuperscript{1}, Jiang Zhong\textsuperscript{1}\thanks{Corresponding author}, \textbf{Li Jin}\textsuperscript{2*}, \textbf{Jingwang Huang}\textsuperscript{1}, Jingpeng Gao\textsuperscript{1}\\
\textbf{Qing Liu\textsuperscript{2}}, \textbf{Yang Bai\textsuperscript{2}}, \textbf{Jingyuan Zhang\textsuperscript{3}}, \textbf{Rui Jiang\textsuperscript{1}}, \textbf{Kaiwen Wei}\textsuperscript{1*}  \\
\textsuperscript{1} College of Computer Science, Chongqing University, China \\
\textsuperscript{2} Aerospace Information Research Institute, Chinese Academy of Sciences, China \\
\textsuperscript{3} Kuaishou Technology, Beijing, China \\
\text{\href{mailto:ymyang@cqu.edu.cn}{ymyang@cqu.edu.cn}, \href{mailto:zhongjiang@cqu.edu.cn}{zhongjiang@cqu.edu.cn}, \href{mailto:weikaiwen@cqu.edu.cn}{weikaiwen@cqu.edu.cn}} \\
\text{\href{mailto:jinlimails@gmail.com}{jinlimails@gmail.com}} \\
}



\begin{document}
\maketitle
\begin{abstract}
% Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning capabilities through external knowledge integration. However, existing benchmarks primarily focus on simple image-text interactions, overlooking complex visual formats like charts that are prevalent in real-world applications. 
% In this work, we introduce a novel task \textbf{Chart-based MRAG} to address this limitation. To semi-automatically and precisely generate evaluting samples, we first propose CHARt-based document question-answering GEneration (CHARGE), an automated framework that produces high-quality QA pairs through structured factual extraction, Crossmodal verification, and keypoint-based generation. Leveraging this framework, we introduce Chart-MRAG Bench, the first comprehensive benchmark for text-chart MRAG evaluation, comprising 4,738 expert-validated QA pairs across 8 domains from real-world documents.
Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning capabilities by integrating external knowledge. However, existing benchmarks primarily focus on simple image-text interactions, overlooking complex visual formats like charts that are prevalent in real-world applications. In this work, we introduce a novel task, \textbf{Chart-based MRAG}, to address this limitation. 
To semi-automatically generate high-quality evaluation samples, we propose \textbf{CHAR}t-based document question-answering \textbf{GE}neration (CHARGE), a framework that produces evaluation data  through structured keypoint extraction, crossmodal verification, and keypoint-based generation. 
By combining CHARGE with expert validation, we construct \textbf{Chart-MRAG Bench}, a comprehensive benchmark for chart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8 domains from real-world documents.
Our evaluation reveals three critical limitations in current approaches: (1) unified multimodal embedding retrieval methods struggles in chart-based scenarios, (2) even with ground-truth retrieval, state-of-the-art MLLMs achieve only 58.19\% Correctness and 73.87\% Coverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality bias during Chart-based MRAG reasoning. 
The CHARGE and Chart-MRAG Bench are released at \url{https://github.com/Nomothings/CHARGE.git}.

% In this work, we introduce a novel task \textbf{Chart-based MRAG} to address this limitation. 
% To automatically generate question answering paris, we propose \textbf{CHAR}t-based document question-answering \textbf{GE}neration (CHARGE), an automated framework that generates high-quality QA pairs through structured factual extraction, crossmodal verification, and keypoint-based generation. Leveraging this framework, we introduce \textbf{Chart-MRAG Bench}, the first comprehensive benchmark for text-chart MRAG evaluation, comprising 4,738 expert-validated question-answer pairs across 8 domains from real-world documents. 
% Our systematic evaluation reveals three critical limitations in current approaches: (1) unified multimodal embedding retrieval methods struggles in chart-based scenarios, (2) even with ground-truth retrieval, state-of-the-art MLLMs achieve only 58.19\% Correctness and 73.87\% Coverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality bias in multimodal reasoning tasks. These findings highlight significant challenges in processing information-dense visual formats and suggest important directions for improving MRAG systems. We will open-source the relevant code and dataset. 
% to advance research in MRAG.
% Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning capabilities through external knowledge integration, yet existing benchmarks primarily focus on simple image-text interactions, overlooking complex visual formats like charts that are prevalent in real-world applications. To address this limitation and facilitate large-scale evaluation data collection, we propose Chart-based Document Question-Answering Generation (CHARGE), an automated framework that generates high-quality QA pairs through structured factual extraction, crossmodal verification, and keypoint-based generation. Leveraging this framework, we introduce Chart-MRAG Bench, the first comprehensive benchmark for text-chart MRAG evaluation, comprising 4,738 expert-validated question-answer pairs across 8 domains from real-world documents. Our systematic evaluation reveals three critical limitations in current approaches: (1) unified multimodal embedding retrieval methods completely fail in chart-based scenarios, (2) even with ground-truth retrieval, state-of-the-art MLLMs achieve only 58.19\% Correctness and 73.87\% Coverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality bias in multimodal reasoning tasks. These findings highlight significant challenges in processing information-dense visual formats and suggest important directions for improving MRAG systems. We will open-source the relevant code and dataset. % to advance research in MRAG.
\end{abstract}

\section{Introduction}
% Recent advances in multimodal large language models (MLLMs) have significantly expanded language processing capabilities from text-based to image and video understanding ~\cite{openai2023gpt,zhang2024mm}. Despite their promising potential in processing multimodal data, MLLMs face crucial issues such as hallucination and outdated knowledge \cite{gao2023retrieval}. To address these problems, researchers have proposed MRAG \cite{zhao2023retrieving} to enhance model capabilities by incorporating external multimodal knowledge bases.

% 引入得不好，要提reasoning，探索检索和生成的关系
% Recent advances in multimodal large language models (MLLMs) have greatly improved the capacity for multimodal understanding and generation~\cite{openai2023gpt,zhang2024mm}. Despite their potential, MLLMs face critical challenges, such as hallucination and limited factual grounding \cite{gao2023retrieval}. To address these limitations, researchers have proposed multimodal retrieval-augmented generation (MRAG)~\cite{zhao2023retrieving}, which enhances MLLMs by integrating external multimodal knowledge sources.

% Multimodal retrieval-augmented generation (MRAG)~\cite{zhao2023retrieving} combines retrieval and generation to enhance multimodal reasoning~\cite{openai2023gpt,zhang2024mm}, where retrieval mechanisms first identify relevant external knowledge, and multimodal large language models (MLLMs) subsequently generate informed responses. This approach has demonstrated strong potential in mitigating hallucinations and improving factual grounding \cite{gao2023retrieval}. However, evaluating MRAG performance and understanding the intricate interplay between retrieval and generation remain open questions.

Multimodal retrieval-augmented generation (MRAG) \cite{zhao2023retrieving} enhances multimodal reasoning by retrieving relevant external knowledge, and leveraging multimodal large language models (MLLMs) for informed response generation~\cite{openai2023gpt,zhang2024mm}. This approach substantially mitigates hallucinations and improves factual grounding~\cite{gao2023retrieval}. 
%, yet its evaluation and the interplay between retrieval and generation remain open questions.

\begin{figure}[t]
\centering
  \includegraphics[width=\linewidth]{intro.png}
  \caption{
    Comparison of two common MRAG scenarios, image-only and text-image, and the proposed text-chart task. In the text-chart MRAG scenario, models need to capture intricate chart details and retrieve both chart and text information to generate correct answers.
  % Illustration of three MRAG scenarios across different modalities: image-only, text-image, and text-chart tasks. The text-chart scenario introduces a new challenge for evaluating complex visual formats beyond traditional image-based benchmarks.
  }
  \label{fig:intro}
\end{figure}

% The effectiveness of MRAG systems hinges on the availability of high-quality evaluation benchmarks. 
Effectively evaluating MRAG systems requires high-quality benchmarks that assess both retrieval and generation. 
Existing benchmarks such as MRAG-Bench \cite{hu2024mragbench} and Dyn-VQA \cite{li2024benchmarking} have made strides in assessing MRAG capabilities through manually curated question-answering (QA) pairs. 
However, as illustrated in Fig.\ref{fig:intro}(a) and (b), these benchmarks primarily focus on scenarios involving images or simple combinations of images and text. Such settings fail to capture the complex interactions between visual details and corresponding text, particularly when dealing dense and structured information like charts, which are widely used in real-world applications \cite{masry2022chartqa}. This leaves a critical gap in MRAG evaluation. 

To bridge this gap, we propose a new task: \textbf{Chart-based MRAG}. For a given text query, this task involves three RAG sub-tasks: (1) \textit{Text-Chart MRAG}, as illustrated in~Fig.~\ref{fig:intro}(c), both textual and chart data must be jointly retrieved to generate correct answers. In addition, to allow for the separate evaluation of each modality's contributions, it also provides (2) \textit{Text-only RAG}, where answers can only be found in textual information; and (3) \textit{Chart-only MRAG}, where answers depend  exclusively on chart data. 
To comprehensively evaluate these tasks, a major challenge is how to semi-automatically generate high-quality QA pairs that accurately capture text-chart interactions.

To overcome this challenge, we propose \textbf{CHAR}t-based document question-answering \textbf{GE}neration (CHARGE), a framework for automatically generating QA pairs from real-world chart-document data. CHARGE follows a three-stage pipeline comprising structured keypoint extraction from text and chart data, crossmodal verification for accuracy, and keypoint-based generation to model complex multimodal interactions. Moreover, to further challenge the chart-based MRAG task, MLLMs are employed to generate QA pairs that require multi-hop reasoning based on intra-document or inter-document retrieval.

Building on CHARGE, we introduce \textbf{Chart-MRAG Bench}, a high-quality, human-checked benchmark tailored for Chart-based MRAG. With CHARGE, 5,866 qualified QA pairs were initially generated, after that, 4,738 (nearly 80\%) were meticulously selected through expert evaluation based on clarity, accuracy, multimodal coherence, and ethical considerations. As shown in Table~\ref{tab:data_compare}, Chart-MRAG Bench comprises 267 documents spanning 8 domains, 8 types of questions, 1,283 paragraphs, and 627 charts, capturing complex crossmodal interactions in realistic scenarios.

% Building on CHARGE, we introduce \textbf{Chart-MRAG Bench}, a high-quality and human-checked benchmark tailored for text-chart retrieval and multimodal generation tasks. With  CHARGE, we initially generated 5,866 candidate QA pairs, from which 4,738 (nearly 80\%) were meticulously selected through expert evaluation based on clarity, accuracy, multimodal coherence, and ethical considerations.
% Finally, as shown in Table~\ref{tab:data_compare}, Chart-MRAG Bench encompasses 267 documents across 8 domains, 1,283 paragraphs, and 627 charts, capturing complex crossmodal interactions in realistic scenarios. 
%Leveraging CHARGE, we generated 5,866 candidate QA pairs, from which 4,738 (nearly 80\%) were meticulously selected through expert evaluation based on clarity, accuracy, multimodal coherence, and ethical considerations.

% To bridge this gap, we propose a new task: \textbf{Chart-based MRAG}. It consists of three sub-tasks (1) text-only RAG, ;(2) chart-only MRAG, ; (3) Text-chart MRAG, as illustrated in Fig.\ref{fig:intro}(c), 

% To bridge this gap, we propose a new task: \textbf{Chart-based MRAG}. For a given query, this task consists of three subtasks: (1) Text-chart MRAG (Fig.~\ref{fig:intro}(c)), where both text and chart data must be jointly retrieved to generate correct answers; (2) Text-only RAG, which relies solely on text; and (3) Chart-only MRAG, which uses only chart data.

% To bridge this gap, we propose a new task: \textbf{Chart-based MRAG}. For a given query, this task consists of: (1) Text-chart MRAG, as illustrated in Fig.\ref{fig:intro}(c), where both text and chart data must be jointly retrieved to generate correct answers; and to separately evaluate the contributions of each modality, we also introduce (2) text-only RAG and (3) chart-only MRAG, which only need to retrive the text or chart to generate correct answers.

% To develop a new text-chart MRAG evaluation benchmark, a big challenge is how to semi-automatically generate high-quality QA pairs that accurately capture text-chart interactions. To address this challenge, we propose Chart-based document Question-Answering Generation (\textbf{CHARGE}), a framework for automatically generating QA pairs from real-world chart-document data. CHARGE extracts and processes structured factual statements from both text and chart data, then verifies their accuracy across modalities, and finally generates QA pairs via keypoint retrieving. This approach enables capturing the complexities of real-world multimodal interactions. % 还要强调多跳
% Under this QA pair generation framwork, we introduce \textbf{Chart-MRAG Bench}, a high-quality, human-annotated benchmark tailored for text-chart retrieval and multimodal generation tasks. As illustracted in Table~\ref{tab:data_compare}, Chart-MRAG Bench captures complex crossmodal interactions in real-world scenarios, encompassing 267 documents across 8 domains, 1,283 paragraphs, and 627 charts. Leveraging the CHARGE framework, we initially generated 9,600 candidate QA pairs, from which 4,738 were meticulously selected through expert evaluation based on clarity, accuracy, multimodal coherence, and ethical considerations. 



%they mainly assess single-entity images or simple image-text interactions. This overlooks dense structured information such as charts, which are widely used in real-world applications \cite{card1999readings}, leaving a critical gap in MRAG evaluation. 
% However, as illustrated in Fig.~\ref{fig:intro}, existing benchmarks primarily focus on single-entity images or basic image-text interactions, leaving a critical gap in the evaluation of more complex visual formats. Among these, charts stand out as dense representations of structured information and play a crucial role in real-world applications~\cite{card1999readings}.
% However, as illustrated in Fig.~\ref{fig:intro }, these benchmarks predominantly focus on single-entity images or basic image-text interactions. This leaves a critical gap in evaluating complex visual formats, such as charts, which are high-density information representations. Such formats are widely used in real-world applications \cite{card1999readings}.

% Developing a new text-chart MRAG evaluation benchmark could bridge this gap, but two key challenges remain: (1) The large-scale nature of data collection requires an \textit{automated method} for generating high-quality QA pairs; 
% (2) The MRAG paradigm demands \textit{precise annotations}, as even minor errors could greatly affect evaluation outcomes.

\input{table_dataset}

% To address the first challenge, we propose Chart-Document Question-answering Generation (\textbf{CDQG}), a novel framework for automatically generating QA pairs from chart-document data. CDQG systematically extracts, processes, and categorizes crossmodal keypoints from charts and text. Specifically, we introduce a retriever that identifies relevant keypoints across modalities, collaborating with large language models (LLMs) to generate benchmarks featuring crossmodal interactions and eight distinct QA types that reflect real-world complexity. Additionally, we propose keypoint-based correctness and coverage metrics to rigorously evaluate the accuracy and comprehensiveness of the generated QA pairs.

% To tackle the second challenge, we present \textbf{Chart-MRAG Bench}, a precise human-annotated benchmark specifically designed for text-chart retrieval and multimodal generation tasks. Chart-MRAG Bench captures sophisticated crossmodal interactions in real-world contexts, comprising 267 documents across 8 domains, 1,283 paragraphs, and 627 charts. Building on the CDQG framework, we generated 9,600 initial candidate QA pairs, of which 4,738 were selected through expert evaluation based on clarity, accuracy, multimodal integration, and ethical considerations.

% To address the first challenge, we propose Chart-Document Question-answering Generation (\textbf{CDQG}), a novel framework for automatically generating QA pairs from chart-document data. CDQG systematically extracts, processes, and categorizes crossmodal keypoints from both charts and text. Specifically, we introduce a retriever that identifies relevant keypoints across modalities and collaborates with large language models (LLMs) to construct benchmarks that capture crossmodal interactions across eight distinct QA types, reflecting real-world complexity. 

% To address the first challenge, we propose Chart-based document Question-Answering Generation (\textbf{CHARGE}), a framework for automatically generating QA pairs from real-world chart-document data. CHARGE extracts and processes structured factual statements from both text and chart data, then verifies their accuracy across modalities, and finally generates QA pairs via keypoint retrieving. This approach enables capturing the complexities of real-world multimodal interactions.

% To tackle the second challenge, we introduce \textbf{Chart-MRAG Bench}, a high-quality, human-annotated benchmark tailored for text-chart retrieval and multimodal generation tasks. As illustracted in Table~\ref{tab:data_compare}, Chart-MRAG Bench captures complex crossmodal interactions in real-world scenarios, encompassing 267 documents across 8 domains, 1,283 paragraphs, and 627 charts. Leveraging the CHARGE framework, we initially generated 9,600 candidate QA pairs, from which 4,738 were meticulously selected through expert evaluation based on clarity, accuracy, multimodal coherence, and ethical considerations. 

% We conducted a systematic evaluation of mainstream retrieval methods and MLLMs on the Chart-MRAG Bench, with keypoint-based Correctness and Coverage metrics designed to rigorously assess the accuracy and comprehensiveness.
We conducted a systematic evaluation of mainstream retrieval methods and MLLMs on Chart-MRAG Bench. In our evaluation, keypoint-based Correctness and Coverage metrics were introduced to rigorously assess accuracy and comprehensiveness.
The results reveal that unified multimodal embedding retrieval methods, which rely on a single vector store, perform poorly in high-density chart scenarios. %, with a 0 recall rate for chart-based retrieval.
Furthermore, even with ground-truth retrieval, the best-performing Claude-3.5 Sonnet \cite{team2024gemini} only achieved 58.19 Correctness and 73.87 Coverage metrics, highlighting persistent challenges in text-chart multimodal reasoning. In summary, the contributions of this paper are: 
% even with ground-truth retrieval, the best-performing Claude-3.5 Sonnet model only achieved accuracy scores of Correctness 58.19 and Coverage 73.87. These findings substantiate the significant challenges that persist in crossmodal reasoning. 

1) We present Chart-based MRAG, the first extension of MRAG to chart scenarios that introduces a new dimension for evaluating crossmodal reasoning in information-dense visual contexts. 
% We present Chart-based MRAG task, the first study that extends MRAG to chart-based scenarios, introducing a novel dimension for evaluating crossmodal reasoning capabilities in information-dense visual contexts. 

% 2) We propose CDQG, an automated framework for generating high-quality QA pairs from text-chart data. It systematically extracts crossmodal keypoints and leverages LLMs to construct diverse question types that reflect real-world complexities.
2) We propose CHARGE, an automated framework for generating QA pairs in real-world scenarios through a structured pipeline of keypoint extraction, verification, and generation.

%, accompanied by two robust evaluation metrics for quality assessment. 

3) We establish Chart-MRAG Bench based on CHARGE. It is a human-verified benchmark for chart-based MRAG, covering 8 scenarios, 8 question types, and 4,738 QA pairs, with a subset designed for multi-hop reasoning. 
% We establish Chart-MRAG Bench based on CHARGE, a human-checked benchmark for chart-based MRAG that covers 8 scenarios and 8 question types with 4,738 QA pairs. 
%We establish Chart-MRAG Bench based on CHARGE, a high-quality, human-checked benchmark for chart-based MRAG, covering 8 scenarios and 8 question types with 4,738 expert-validated QA pairs. 
%We establish Chart-MRAG Bench, a precise human-annotated chart-based MRAG benchmark encompassing 8 scenarios and 8 question types, comprising 4,738 expert-validated QA pairs. %, ensuring benchmark integrity through rigorous professional verification. 

4) We introduce two robust evaluation metrics to assess MRAG quality. Extensive experiments highlight the limitations of existing retrieval and generation methods in chart-centric tasks.
%We provide two robust evaluation metrics for MRAG quality assessment. Extensive experiments on Chart-MRAG Bench reveal the limitations of unified multimodal embedding retrieval approaches in chart scenarios. %, while also identifying that mainstream MLLMs favors textual descriptions over chart data. 

% 4) Extensive experiments on Chart-MRAG Bench reveal the limitations of unified multimodal embedding retrieval approaches in chart scenarios, while also identifying that mainstream MLLMs favors textual descriptions over chart data. 

%Experiments reveal the expressive limitations of unified multimodal embedding retrieval approaches in information-dense chart scenarios, while also identifying a systematic modality bias in mainstream MLLMs that favors textual descriptions over chart data, despite the latter's superior accurate description.

% Developing evaluation datasets could address this issue, but there are still two challengs:
% （1）数据量大，如何自动化生成QA对
% (2) RAG需要非常严格的标注，一点标注错可能影响结果
% This limitation highlights the urgent need for a robust evaluation framework capable of (1) automating the generation of crossmodal QA pairs and (2) ensuring high-quality annotations for retrieval and generation processes.
%This gap underscores the need for establishing a evaluation framework that (1) automate the generation of crossmodal QA pairs and (2) ensure high-quality annotations for retrieval and generation processes.

% The advancement of MRAG is contingent upon high-quality evaluation benchmarks.  Several significant benchmarks have been proposed to assess MRAG capabilities. For example, MRAG-Bench \cite{hu2024mragbench} ensures evaluation reliability through manually curated question-answer (QA) pairs. SMMQG \cite{wu2024synthetic} extends the scope of image retrieval by generating crossmodal benchmarks that bridge image-text interactions. 
% However, as shown in Fig~\ref{fig:intro}, existing benchmarks primarily focus on dealing with single-entity images or basic image-text interactions. A significant gap exists in evaluating complex visual formats, such as charts, which serve as high-density information representations \cite{card1999readings}. This gap limits MRAG systems' evaluation in real-world applications involving data visualization tasks. As a result, there is an urgent need for establishing a evaluation framework that (1) reflect authentic text-chart scenarios by automatically generating crossmodal QA pairs and (2) ensure that both retrieval and generation processes are annotated with high quality.
% In essence, there is a pressing demand for evaluation frameworks that reflect real-world text-chart scenarios while (1) providing a robust mechanism for automatically generating crossmodal QA pairs and (2) ensuring the retrieval and generation processes have high-quality annotations.
% predominantly focus on elementary scenarios,

% In this paper, we propose Chart-Document Question-answering Generation (CDQG), a general framework for automatically generating question-answering paris from chart/document data. CDQG systematically extracts, processes, and categorizes crossmodal keypoints from charts and text. Inspired by retrieval-augmented generation (RAG) \cite{123}, we incorporate a retriever that identifies relevant keypoints across sources and modalities, working with LLMs to generate benchmarks containing crossmodal interactions and eight distinct types of QA pairs that reflect real-world complexity. Furthermore, we introduce keypoint-based correctness and coverage metrics to evaluate question accuracy and comprehensiveness, establishing a robust evaluation methodology for the generated QA pairs.

% Based on this, we present Chart-MRAG Bench, a benchmark specifically designed for text-chart retrieval and Multimodal generation tasks. Chart-MRAG Bench captures sophisticated crossmodal interactions in real-world contexts. The dataset comprises 267 documents across 8 domains, containing 1,283 paragraphs and 627 charts. With CDQG framework, we generated 9,600 initial candidate QA pairs and selected 4,738 high-quality pairs through rigorous expert evaluation focusing on clarity, accuracy, multimodal integration, and ethical considerations.

% To tackle complex chart-based QA generation, we propose Chart-Document Question-answering Generation (CDQG), a general framework for automatically generating QA paris from chart/document data. CDQG systematically extracts, processes, and categorizes crossmodal keypoints from charts and text. Inspired by RAG, we incorporate a retriever that identifies relevant keypoints across sources and modalities, working with LLMs to generate benchmarks containing crossmodal interactions and eight distinct types of QA pairs that reflect real-world complexity. Furthermore, we introduce keypoint-based correctness and coverage metrics to evaluate question accuracy and comprehensiveness, establishing a robust evaluation methodology for the generated QA pairs.

% Addressing the lack of human-checked MRAG benchmarks in dense information scenarios, we present Chart-MRAG Bench, a benchmark specifically designed for text-chart retrieval and Multimodal generation tasks. Chart-MRAG Bench captures sophisticated crossmodal interactions in real-world contexts. The dataset comprises 267 documents across 8 domains, containing 1,283 paragraphs and 627 charts. With CDQG framework, we generated 9,600 initial candidate QA pairs and selected 4,738 high-quality pairs through rigorous expert evaluation focusing on clarity, accuracy, multimodal integration, and ethical considerations.

% and quantified their performance gaps in crossmodal reasoning tasks

\begin{table}[t]
  \centering
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{lcccc}
  \toprule
  \textbf{Benchmarks} & 
  \makecell{\textbf{Target}\\\textbf{Task}} &
  \makecell{\textbf{Retrieval}\\\textbf{Modality}} &
  \makecell{\textbf{Question}\\\textbf{Types}} & 
  \makecell{\textbf{Human}\\\textbf{Annotation}} \\
  \midrule
  OK-VQA ~\cite{marino2019ok} & VQA & Visual & 1 & $\checkmark$ \\
  MMQA ~\cite{talmor2021multimodalqa} & VQA & Multimodal & 16 & $\times$ \\
  PlotQA ~\cite{Methani_2020_WACV} & VQA & Visual & 1 & $\checkmark$ \\
  ChartQA ~\cite{masry2022chartqa} & VQA & Visual & 1 & $\checkmark$ \\
  DocVQA ~\cite{mathew2021docvqa} & VQA & Visual & 9 & $\checkmark$ \\
  MRAG-Bench ~\cite{hu2024mragbench} & MRAG & Visual & 3 & $\checkmark$ \\
  SSMQG ~\cite{wu2024synthetic} & MRAG & Multimodal & 5 & $\times$ \\
  \midrule
  \textbf{Chart-MRAG} \textit{(Ours)} & MRAG & Multimodal & 8 & $\checkmark$ \\ 
  \bottomrule
  \end{tabular}%
  }
  \caption{Comparison between existing MRAG benchmarks and the proposed Chart-MRAG Bench.}
  \label{tab:data_compare}
\end{table}

\section{Related Work}
\noindent \textbf{Multimodal RAG Methods.} Recent advances in Retrieval-Augmented Generation (RAG) \cite{izacard2022few,zhang2024raft} have successfully extended to multimodal domains \cite{chen2022murag,zhao2023retrieving,zhao2024retrieval}, enabling crossmodal tasks through MLLMs \cite{yao2024minicpm, sailvl}. While researchers have proposed various approaches \cite{ma-etal-2024-unifying,faysse2024colpali,yu2024visrag,Methani_2020_WACV,mathew2021docvqa} for crossmodal retrieval, current evaluation methodologies predominantly rely on Visual Question Answering (VQA) datasets \cite{marino2019ok,talmor2021multimodalqa,schwenk2022okvqa,masry2022chartqa}. These evaluations fall short in addressing retrieval-specific challenges.

\begin{figure*}[th]
  \includegraphics[width=\textwidth]{Main_flow_diagram.png}
  \caption{The proposed CHARGE framework for creating multimodal QA pairs from document-chart data, consisting of three steps: (1) Extract keypoints from textual content and charts, (2) Perform crossmodal verification to validate keypoint modality uniqueness, (3) Generate diverse QA pairs through constrained keypoint retrieval.}
  \label{Main_flow_diagram}
\end{figure*}

\noindent \textbf{Multimodal RAG Benchmarks.} The effectiveness of MRAG systems necessitates comprehensive evaluation benchmarks. While several benchmarks \cite{hu2024mragbench,li2024benchmarking,zhou2024megapairs} explore vision-based retrieval for question answering through manual annotation, they neglect the critical dimension of crossmodal collaborative generation. Some studies \cite{dong2025mmdocir,ma2024mmlongbench,ding2024mvqa} consider hybrid modality retrieval, yet they primarily rely on manual question-answering. Furthermore, although some studies \cite{es2023ragas,abaskohi2024fm2ds,mathew2021docvqa,li2024multimodal,wu2024synthetic} have investigated automated processes for generating crossmodal QA pairs, their scope focus on simplistic natural images with singular subjects, the chart-based scenarios largely unexplored. To bridge this gap, this paper introduce Chart-MRAG Bench. Table~\ref{tab:data_compare} illustrates the differences between existing MRAG benchmarks and Chart-MRAG Bench.
\section{CHARGE Framework}

We present CHARGE, a framework for generating multimodal multi-hop QA pairs from text-chart documents. CHARGE operates in three stages: (1) extracting self-contained keypoints from both textual and visual content, (2) verifying the modality authenticity of extracted keypoints through crossmodal verification, and (3) generating diverse QA pairs by combining related keypoints across documents and modalities.

\subsection{Extract Keypoints}

As illustrated in Fig ~\ref{Main_flow_diagram}, given multimodal documents $D = \{d_1,...,d_n\}$, CHARGE process its textual content into coherent chunks $T = \{t_1,...,t_m\}$ and charts as discrete units $C = \{c_1,...,c_k\}$. We define keypoints as self-contained factual statements that capture core information from these source materials. These atomic units are extracted from both textual and visual content (e.g., "33\% of U.S. adults say they use TikTok") through:

\begin{equation}
  \label{eq1}
  K = \begin{cases}
    \phi_t(T) & \textit{for text} \\
    \phi_c(C, \psi(C)) & \textit{for chart},
  \end{cases}
\end{equation}
where $K = \{k_1,...,k_r\}$ consists of structured information units capturing factual statements, logical inferences, or conclusive summaries. For textual content, we utilize \textrm{GPT-4o} through function $\phi_t$. For visual content, we first extract numerical values using function $\psi$ (implemented with \textit{ChartOCR} \cite{luo2021chartocr}), then employ \textrm{GPT-4o} through function $\phi_c$ to jointly process the charts $C$ and the extracted values, ensuring both contextual comprehension and numerical precision. Detailed workflow is presented in Appendix ~\ref{sec: keypoints_extraction}.

\subsection{Crossmodal Verification}
To ensure the reliability of extracted keypoints, we develop a crossmodal verification  mechanism that validates whether information truly belongs to its claimed modality. Our key insight is: Authentic modality-specific keypoints should be retrievable from its source modality but not from the other.

We first categorize keypoints into two fundamental types: (1) \textit{Text-based keypoints} ($K^T$): information exclusively present in textual form; (2) \textit{Chart-only keypoints} ($K^C$): information uniquely extractable from chart visualization. While \textit{GPT-4o} performs initial classification, crossmodal Verification is crucial for complex reasoning tasks.

The verification process employs crossmodal querying with \textit{GPT-4o} serving as a judge to determine whether the queried information exists in each modality's response. Taking text-based keypoint verification as an example, for a given keypoint $k^t_i \in K^T$, we query both its source text chunk $t_i$ and the paired chart $c_i$ (with OCR information $v_i$). Let $\tilde{k}^t_i$ and $\tilde{k}^c_i$ denote the model's responses from text and chart modalities respectively. The verification criterion is formalized as:

\begin{equation}
  \label{eq:verification}
  \text{Status}(k^t_i)\!=\!\begin{cases}
    \text{Retain} & \!\!\textit{if }\tilde{k}^t_i\!=\!k^t_i \land \tilde{k}^c_i\!\neq\!k^t_i \\
    \text{Drop} & \!\!\textit{otherwise}.
  \end{cases}
\end{equation}

This automated process retains keypoints only when correctly retrieved from their source modality and absent in others. Detailed algorithms are provided in Appendix ~\ref{sec:Crossmodal_algorithms}.

\begin{figure}[tbp]
  \includegraphics[width=\columnwidth]{case.png}
  \caption{
  % One sample from Chart-MRAG Bench of inter-document text-chart QA pairs generated by the CHARGE framework.
    An inter-document multi-hop QA example from Chart-MRAG Bench, generated by CHARGE.
  }%, which integrates chart details and textual information to enable multi-step reasoning.}
  \label{case_diagram}
\end{figure}

\subsection{Question-Answer Pair Generation}

Since each keypoint represents a specific conclusion or data point, it can generate a corresponding question-answer pair (commonly referred to as Single-Point QA). In our CHARGE framework, we support this basic form of QA generation. Additionally, recognizing that most real-world queries require the integration of multiple knowledge points to be answered fully (known as Multi-hop QA), we further designed a multi-hop question answering approach: by combining semantically related keypoints to form a single question-answer pair. These types of questions cannot be completely answered using just one keypoint; they require the retrieval of all information sources (text chunks or charts) containing the constituent keypoints to be answered correctly. As illustrated in Fig~\ref{case_diagram}, CHARGE generates Multi-hop QA that requires retrieving multiple pieces of information by combining "33\% of U.S. adults say they use TikTok" (from a chart) with "62\% of U.S. adults who use TikTok say a reason they use the site is to look at product reviews or recommendations" (from text).

The generation of Multi-hop QA involves two key steps: identifying semantically related keypoints and constructing QA pairs from their combinations. Motivated by the capacity of RAG, we generate QA pairs through keypoint retrieval . Specifically, we first randomly select a keypoint (termed \textit{selected keypoint}) as the query, then retrieve candidate keypoints (termed \textit{retrieved keypoints}) by \textit{E5-Large} \cite{wang2022text}. By tracking the source documents of these retrieved keypoints, we categorize them into two types:
\begin{itemize}[noitemsep,leftmargin=*]
\item \textit{Intra-Document}: retrieved keypoints originate from the same document as the selected keypoint.
\item \textit{Inter-Document}: retrieved keypoints come from different documents than the selected keypoint.
\end{itemize}

Additionally, by tracking whether keypoints are from text or chart, we categorize their combinations into three types: 
\begin{itemize}[noitemsep,leftmargin=*]
\item \textit{Text-only}: both extracted from text paragraphs.
\item \textit{Chart-only}: both extracted from charts.
\item \textit{Text-Chart}: extracted from different modalities.
\end{itemize}

\begin{algorithm}[bp]
\SetAlgoLined
\DontPrintSemicolon
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Document set $D$; \\
       Chart keypoint set $K^C$; \\
       Text keypoint set $K^T$}
\Output{Question-Answer pair $(q, a)$}

\tcp{Step 1: Select chart keypoint}
Select $k^c_i \in K^C$ from document $d_a \in D$ \;
$(c_i, v_i) \gets (c, \psi(c))$ where $c \in d_a$ \;

\tcp{Step 2: Retrieve relevant text keypoint}
$K^T_r \gets \text{Retrieve}(k^c_i, K^T, k)$ \tcp*{top-k retrieval}
Select $k^t_j \in K^T_r$ from document $d_b \in D$ \;
$t_j \gets$ corresponding text block in $d_b$ \;

\tcp{Step 3: Generate QA pair}
$(q, a) \gets \text{MLLM}(k^c_i, k^t_j, c_i, v_i, t_j)$ \;

\Return{$(q, a)$}
\caption{\small Cross-document Text-Chart QA}
\label{alg:cross_doc_qa}
\end{algorithm}

For each combination of keypoints, we employ \textit{GPT-4o} to generate meaningful question-answer pairs by combining selected keypoint and its retrieved keypoint. These questions are designed to require multi-hop reasoning across different sources or modalities for answering. As illustrated in Table~\ref{tab:qa_types}, CHARGE naturally supports eight distinct types of QA pairs based on the document sources and modality combinations. While Algorithm~\ref{alg:cross_doc_qa} demonstrates the generation process for one specific type, comprehensive implementation details for all types are provided in Appendix~\ref{sec: qa_gen_algorithms}.

% 由于每个keypoint都是确定性的结论或数据，因此每个keypoint都可以生成对应的问答对(通常我们将单个keypoint生成的问答对称为 Single-Point QA)。在真实场景中，多数queries需要结合多个知识点来回答(named Multi-hop QA)，为了充分反映上述真实需求。我们进一步设计多跳问答：通过联立语义上相似的keypoints来构成一个问答对。这类问答仅凭单个keypoint无法被完整回答，只有充分检索到构成的所有keypoints所在的信息源(text chunks or charts)，才能正确回答。As illustrated in Fig ~\ref{case_diagram}，CHARGE通过联合"33% of U.S. adults say they use TikTok."(form chart) 和 "62% of U.S. adults who use TikTok say a reason they use the site is to look at product reviews or recommendations." (from text)。生成需要检索多个信息才能回答的Multi-hop QA.

% Multi-hop QA生成的首要任务是寻找互相联系的keypoints，然后联合keypoints生成问答对。Motivated by the capacity of RAG, we generate QA pairs through strategic keypoint retrieval. The process begins by randomly selecting a keypoint (termed \textit{selected keypoint}) as a query, followed by retrieving candidate keypoints (termed \textit{retrieved keypoints}) based on vector similarity. 通过追踪retrieved keypoints在Document中的来源，我们将其按照source分类为：\vspace{-0.5em}
% \begin{itemize}[noitemsep,leftmargin=*]
% \item Intra-Document：retrieved keypoints和selected keypoint来自于同一个文档。
% \item Inter-Document：retrieved keypoints和selected keypoint来自于不同文档。
% \end{itemize}\vspace{-0.5em}
% 进一步的，通过追踪retrieved keypoints和selected keypoint的模态来源(extracted from text or chart)，我们将其按照modal分类为：\vspace{-0.5em}
% \begin{itemize}[noitemsep,leftmargin=*]
% \item Text-only：retrieved keypoints和selected keypoint皆来自于文本段落。
% \item Chart-only：retrieved keypoints和selected keypoint皆来自于图表中提取。
% \item Text-Chart：两者分别来自于文本段落和图表。
% \end{itemize}\vspace{-0.5em}
% As inllustrated in Table ~\ref{\label{tab:qa_types}}根据 retrieved keypoints和selected keypoint的来源、模态，CHARGE可以自然的构建出8种不同类型的问答对. while Algorithm~\ref{alg:cross_doc_qa} provides 其中一种问答对的generation process. More implementation details are provided in Appendix ~\ref{sec: qa_gen_algorithms}.

% Motivated by the capacity of RAG, we generate QA pairs through strategic keypoint retrieval. The process begins by randomly selecting a keypoint (termed \textit{selected keypoint}) as a query, followed by retrieving candidate keypoints (termed \textit{retrieved keypoints}) based on vector similarity. To control the reasoning complexity and satisfy modality requirements, the retrieval process is governed by two primary constraints:


\begin{table}[t]
    \small
  \centering
  \setlength{\tabcolsep}{0pt}
  \begin{tabular*}{\columnwidth}
  {@{\extracolsep{\fill}}l@{\hspace{-0.5pt}}c@{\hspace{-0.5pt}}r@{}}
\toprule
{\small \textbf{Statistics}} & {\small \textbf{Reasoning Step}} & {\small \textbf{Number}} \\
\midrule
{\small Total questions} & {\small --} & {\small 4,738} \\
{\small ~- Single-Point Text-only} & {\small 1-hop} & {\small 499 (10.53\%)} \\
{\small ~- Single-Point Chart-only} & {\small 1-hop} & {\small 763 (16.10\%)} \\
{\small ~- Intra-Document  Text-only} & {\small 2-hop} & {\small 666 (14.06\%)} \\
{\small ~- Intra-Document Chart-only} & {\small 2-hop} & {\small 587 (12.39\%)} \\
{\small ~- Intra-Document Text-Chart} & {\small 2-hop} & {\small 746 (15.74\%)} \\
{\small ~- Inter-Document Text-only} & {\small 2-hop} & {\small 547 (11.54\%)} \\
{\small ~- Inter-Document Chart-only} & {\small 2-hop} & {\small 472 (9.96\%)} \\
{\small ~- Inter-Document Text-Chart} & {\small 2-hop} & {\small 458 (9.67\%)} \\
\bottomrule
\end{tabular*}
  \caption{Statistics of question types based on reasoning complexity and modality.}
  \label{tab:qa_types}
\end{table}

% \noindent \textbf{Source-Constrained Retrieval.} To systematically vary the reasoning complexity in question-answer pairs, we implement constraints on the document sources of retrieved keypoints relative to the selected keypoint. We define retrieval categories:\vspace{-0.5em}
% \begin{itemize}[noitemsep,leftmargin=*]
% \item Single-Point: Focuses on validating discrete factual statements or specific data points contained within a single keypoint, where $k_i \in K$.
% \item Intra-Document: Facilitates reasoning across multiple keypoints extracted from different segments within the same document, where $d_i \in D$.
% \item Inter-Document: Enables associative reasoning between keypoints sourced from two distinct documents $d_i, d_j \in D$, where $i \neq j$.
% \end{itemize}\vspace{-0.5em}

% 


% \noindent \textbf{Modality-Constrained Retrieval}. To regulate the complexity of modal interactions in question-answer pairs, we establish constraints on the modality sources of retrieved keypoints relative to the selected keypoint. We define three modal categories:\vspace{-0.5em}
% \begin{itemize}[noitemsep,leftmargin=*]
% \item Text-only: Exclusively generated from textual keypoints, where $k^t_i,k^t_j \in K^T$.
% \item Chart-only: Exclusively generated from chart-only keypoints, where $k^c_i,k^c_j \in K^C$.
% \item Text-Chart: Generated through the integration of both text-only and chart-only keypoints, where $k^t_i \in K^T, k^c_j \in K^C$.
% \end{itemize}\vspace{-0.5em}


% The orthogonal combination of these two dimensions yields 8 distinct QA categories. During retrieval, we filter candidate keypoints based on their document sources and modality types to match the intended QA category, ensuring retrieved keypoints align with both source and modality constraints. Fig~\ref{case_diagram} showcases an example of inter-document text-chart QA pairs, while Algorithm~\ref{alg:cross_doc_qa} provides the detailed generation process. More implementation details are provided in Appendix ~\ref{sec: qa_gen_algorithms}.


\section{Chart-MRAG Bench}

By utilizing the CHARGE framework, we generated an initial pool of question-answer pairs. These pairs underwent rigorous expert evaluation to ensure high quality, culminating in the Chart-MRAG Bench. This process was guided by 4 principles:

\noindent \textbf{Authenticity and Diversity.} The benchmark is based on real-world data collected from the official website\footnote{www.pewresearch.org}, a trusted source of high-quality social research. We collected data from September 2023 to September 2024, encompassing 267 documents containing 1,283 text passages and 627 charts. As illustrated in Table~\ref{tab:qa_types} and Fig~\ref{fig:theme_dist}, Chart-MRAG Bench encompasses 8 distinct domains, integrating over 10 chart types and 8 QA types.

\noindent \textbf{Annotation Reliability.} We engaged 12 expert annotators with Master's degrees. All annotators were proficient in English, with an average TOEFL score of 92 or equivalent language proficiency. The annotation process took 34 working days to complete. Our annotation protocol involved three independent reviewers evaluating each sample, achieving a Fleiss's kappa ~\cite{fleiss1973equivalence} of 0.82, indicating substantial inter-annotator agreement.

\noindent \textbf{Rigorous Quality Control.} Through meticulous manual review, we refined the dataset from 9,600 initial candidates to 5,866 validated pairs by systematically eliminating 2,631 samples with OCR errors and 1,103 redundant samples. A consensus-based sampling strategy required validation from at least two reviewers, resulting in 4,738 high-quality samples (nearly 80\% of the validated pairs).

\noindent \textbf{High Information Complexity.} Statistical analysis reveals the benchmark's sophistication: approximately 70\% of charts contain more than 8 critical information points (mean: 13.87), and over 73\% of text passages include more than 6 keypoints (mean: 8.31). This information-rich environment rigorously evaluates models' capacity to process intricate and dense data representations.

For illustrative examples of Chart-MRAG Bench question-answer pairs across different domains and reasoning types, please refer to Appendix~\ref{sec: chart-mrag_bench_cases}.

\begin{figure}[tbp]
    \centering
  \includegraphics[width=0.8\linewidth]{theme_distribution.png}
  \caption{Distribution of documents across 8 domains, representing key areas of real-world applications.}
  \label{fig:theme_dist}
\end{figure}

\section{Experiments}

\input{retrieved_tables}
\input{generation_table}
\subsection{Baselines and Evaluation Metrics}
We conduct comprehensive evaluations using 3 distinct retrieval methods and 8 diverse MLLMs. Including 
\noindent \textbf{Multimodal Retrievers}: CLIP \cite{radford2021learning}, JINA \cite{koukounas2024jina}, SigLIP \cite{zhai2023sigmoid}, BGE-M3-base/large \cite{chen2024bge} and E5-base/large \cite{wang2022text}. 
\noindent And \textbf{Backbone MLLMs}: GPT-4o (version 2024-11-20) \cite{radford2021learning}, GPT-4-Vision \cite{radford2021learning}, Gemini-1.5-Pro \cite{team2024gemini}, Claude-3.5-Sonnet (version 2024-10-22) \cite{awadalla2023openflamingo}, SAIL-VL-2B \cite{sailvl}, Qwen2-VL-7B-instruct \cite{Qwen2VL}, MiniCPM-V-2.6 (8B) \cite{yao2024minicpm}, and Llama-3.2-90B-Vision \cite{dubey2024llama}.

Following \citet{wu2024synthetic}, we evaluate multimodal retrieval models using Recall@5 (R@5) and Recall@10 (R@10). Please refer to Appendix ~\ref{sec: retrieval_setup_and_metrics} for details of the retrieval setup and metrics.  Moreover, since chart-based MRAG is a newly proposed task, existing evaluation metrics are inadequate. Therefore, we introduce Correctness and Coverage metrics to assess the quality of responses.

% \textbf{Multimodal Recall.} We  utilize Multimodal RAG Retrieval Recall metric to evaluate crossmodal retrieval. The metric operates on sentence-level text and complete charts as:
% \begin{equation}
%     \text{Recall} = \frac{1}{n}\sum_{i=1}^n \mathbb{1}(M(G_i,\mathcal{R})),
%     \label{eq:recall}
% \end{equation}
% where $n$ denotes the total number of ground truth references, $G_i$ is the $i$-th reference, and $\mathcal{R} = \{R_1, R_2, \ldots, R_k\}$ represents retrieved references. $M(G_i,\mathcal{R})$ returns true if $\mathcal{R}$ contains all sentences in $G_i$ for text, or the reference chart. 
\noindent \textbf{Correctness}. It measures the exact match between response and ground truth keypoints. Given a question-answer pair $\{Q, A, K^{gt}\}$ with ground truth keypoints $K^{gt} = \{k^{gt}_1, ..., k^{gt}_n\}$, we extract keypoints $K^r = \{k^r_1, ..., k^r_m\}$ from the model's response using an LLM. The score is defined as:
\begin{equation}
    \text{Correctness}(K^r, K^{gt}) = \mathbbm{1}[K^r \equiv K^{gt}],
\end{equation}
where $K^r \equiv K^{gt}$ implies complete keypoint matching and equal cardinality. This binary metric requires perfect accuracy, with zero tolerance for missing information or errors.

\noindent \textbf{Coverage}. It quantifies the proportion of correctly captured ground truth keypoints:
\begin{equation}
    \text{Coverage}(K^r, K^{gt}) = \frac{|K^{m}|}{|K^{gt}|},
\end{equation}
where $K^{m}$ represents matched ground truth keypoints. This continuous metric in [0,1] enables granular evaluation.

\subsection{Retrieval Performance Comparison}
Table~\ref{tab:retrieved_performance} reveals significant challenges in multimodal retrieval. While existing retrievers exhibit strong single-modal performance (JINA-CLIP achieves 77.78\% Recall@5 in text-only questions and SigLIP + E5 reaches 84.18\% Recall@5 in chart-only tasks), Inter-Document Text-Chart questions yielded only 22.32\% retrieval accuracy. The key findings demonstrate that storing and retrieving charts and text separately in the database substantially improves performance, achieving  recall rates of 42.53\% and 61.10\% at $k$=5 and $k$=10.

\noindent \textbf{Unified multimodal embeddings fail in knowledge-intensive scenarios.} While Method 1 outperforms all other approaches in pure text-only QA, it achieves zero recall (0.00\%) in chart-only QA and Text-Chart QA tasks. This phenomenon reveals a critical limitation: current unified multimodal embedding models excel at representing knowledge-sparse content (e.g., identifying a dog in an image) but struggle with knowledge-intensive scenarios (e.g., retrieving specific numerical values from charts in a multimodal repository).

\noindent \textbf{Chart captioning enables simple yet effective multimodal retrieval.} Methods 2 and 3 achieve comparable performance (Recall@5: 41.53\% vs 42.53\%), with differences primarily in chart retrieval due to the inherent limitations of text-based chart representations. However, considering the maintenance overhead of separate modal stores, caption-based retrieval provides a practical approach that preserves effectiveness while significantly reducing system complexity.

\subsection{Generative Performance Comparison}
Table~\ref{tab:generative_performance} presents the comprehensive experimental results of mainstream MLLMs, with retrieval method 3 consistently applied across all evaluations to ensure controlled comparison. The results reveal that state-of-the-art MLLMs achieve only modest performance metrics (Correctness = 3.06 and Coverage = 9.59) without multimodal RAG knowledge, highlighting Chart-MRAG Bench's exceptional challenging nature that surpasses existing benchmarks in knowledge leakage control. 
% (74.5 in MRAG Bench).

\noindent \textbf{Claude-3.5-Sonnet demonstrates superior overall performance.} The experimental results validate our keypoint-based evaluation methodology. With ground truth retrieval, Claude-3.5-Sonnet achieves Correctness of 58.19\% and Coverage of 73.87\%, outperforming mainstream MLLMs across various retrieval scenarios. It only falls behind Gemini-1.5-pro in Correctness at $k$=10. While Claude-3.5-Sonnet leads in aggregate scores, the narrow performance margins suggest potential in open-source alternatives: its Correctness (58.19) exceeds Gemini-1.5-pro by just 0.25. Moreover, in single-point text-only evaluation at recall $k$=5, qwen2-VL-7B-instruct achieves higher Correctness (50.51) compared to Claude-3.5-Sonnet (44.44).

\begin{figure}[th]
  \includegraphics[width=\columnwidth]{sankey.png}
  \caption{Impact of retrieval size $k$ across different parameter scales, demonstrating that larger models consistently benefit from increased retrieval context while smaller models show performance degradation.}
  \label{sankey_diagram}
\end{figure}

\noindent \textbf{Model performance generally scales with parameter count.} Among open-source MLLMs, Llama-3.2-90B-Vision consistently outperforms models with smaller parameters across various retrieval settings. Similarly, in proprietary MLLMs, GPT-4-Vision, with its presumably larger model size, demonstrates marginally better than GPT-4o.

\noindent \textbf{Architectural optimizations can mitigate MLLMs' parameter constraints.} By incorporating SigLip-400M and optimizing multi-image understanding, MiniCPM-V 2.6 achieves a Correctness of 46.94 and Coverage that surpasses its base model qwen2-VL-7B-instruct by 13.79 and 16.95 respectively. Most notably, despite using only 7B parameters, it approaches the performance of Llama-3.2-90B-Vision, with gaps of 3.22 in Correctness and 4.62 in Coverage, demonstrating that thoughtful architecture design can largely compensate for parameter constraints.
\begin{figure}[tbp]
  \includegraphics[width=\linewidth]{Correctness.png}
  \caption{Trade-off analysis between retrieval coverage and answer accuracy across different $k$ settings, illustrating how larger retrieval windows increase recall while compromising answer correctness.}
  \label{retrieved_diagram}
\end{figure}
\subsection{Further Analysis}

In this study, we examine the influence of retrieval rate ($k$) and modality bias of MLLMs in multimodal question answering. Our analysis shows:

\noindent \textbf{Model performance in multimodal retrieval significantly correlates with parameter scale.} Empirical analysis reveals a strong correlation between model scale and multimodal retrieval performance. We evaluated eight models of varying parameter sizes under different retrieval settings ($k$ = 2, 5, 10, 15, 20), where retrieved items were balanced between images and text (split equally for even $k$, with text receiving one additional item for odd $k$). For each model, we selected 40 question-answer pairs per category, totaling 320 pairs for comprehensive evaluation, as shown in Fig \ref{retrieved_diagram}. The results demonstrate that larger models consistently achieve superior performance across all retrieval settings. In contrast, smaller models show no significant improvement (even exhibit declining) in performance as the number of retrieved items increases. 

\begin{figure}[t]
  \includegraphics[width=\linewidth]{contrast.png}
  \caption{Analysis of modality preference in MLLMs when presented with redundant information across text and charts, revealing systematic modality bias.}
  \label{contrast_diagram}
\end{figure}

\noindent \textbf{Larger retrieval windows lead to a non-trivial trade-off between retrieval coverage and answer quality.} To systematically investigate the impact of Top\_$k$ on response generation, we conducted extended experiments as visualized in Fig ~\ref{sankey_diagram}. With $k$=5, the system achieves a 42.53 Recall and 56.17 correctness. When increasing $k$=10, although the 61.10 Recall, the answer get 49.13 correctness. Notably, while this adjustment results in an increase in absolute correct answers from 1,132 to 1,423, the improvement sacrifices precision.

\noindent \textbf{MLLMs demonstrate consistent text-over-visual modality bias.} To investigate modality bias in MLLMs, we carefully curated 100 specialized question-answer pairs where answers could be derived from both textual and visual information simultaneously, but with varying levels of granularity (e.g., "one third" in text versus "35.2\%" in charts). 
As shown in Fig.~\ref{contrast_diagram}, analysis reveals a consistent preference across models for text-only responses, even when charts contain more precise information. Notably, larger MLLMs demonstrate superior ability in detecting information redundancy and actively acknowledge this in their responses. For instance, GPT-4o proactively identified information redundancy in 23\% of its responses. In contrast, smaller models (such as SAIL-VL-2B) show limited sensitivity to such information redundancy. Detailed examples refer to Appendix ~\ref{sec: bias_case}.

\section{Conclusion}
% This paper addresses the evaluation gap of chart formats in MRAG systems. We propose CHARGE, an automated framework for generating crossmodal QA benchmarks using innovative key-point-based metrics for assessing correctness and coverage. Additionally, we present Chart-MRAG Bench, a set of human-annotated MRAG benchmarks generated via CHARGE. Our findings reveal critical limitations in current MRAG approaches, particularly in handling high-density visual information.

 This paper introduces Chart-based MRAG, a novel task to bridge the evaluation gap for chart formats in MRAG systems. To support this, we propose CHARGE, an automated framework for generating Crossmodal evaluation samples with keypoint-based metrics. Combining CHARGE with expert validation, we construct Chart-MRAG Bench, comprising 4,738 high-quality question-answer pairs across 8 domains. Our experiments reveal key limitations in current MRAG approaches, highlighting the need for specialized architectures to better handle high-density visual interactions.

 % This paper introduces Chart-based MRAG, a novel task to bridge the evaluation gap for chart formats in MRAG systems. To support this, we propose CHARGE, an automated framework for generating Crossmodal evaluation samples with keypoint-based metrics. Combining CHARGE with expert validation, we construct Chart-MRAG Bench, comprising 4,738 high-quality question-answer pairs across 8 domains. Our experiments reveal critical limitations: unified multimodal embedding retrieval methods underperform in chart-based scenarios, and even with gt retrieval, state-of-the-art MLLMs achieve only 58.19\% Correctness and 73.87\% Coverage. We also observe consistent modality bias in MLLMs, highlighting the need for specialized architectures to handle high-density visual interactions.
% This paper introduces Chart-based MRAG, a novel task to bridge the evaluation gap for chart formats in MRAG systems. To support this, we propose CHARGE, an automated framework for generating crossmodal evaluation samples, and construct Chart-MRAG Bench with 4,738 expert-validated question-answer pairs across 8 domains. Our experiments reveal critical limitations: unified multimodal embedding retrieval methods underperform in chart-based scenarios, and even with ground-truth retrieval, state-of-the-art MLLMs achieve only 58.19\% Correctness and 73.87\% Coverage scores. We also observe consistent text-over-visual modality bias in MLLMs, highlighting the need for specialized architectures to handle high-density visual interactions.

\section*{Limitations}
While our work presents promising results, we acknowledge several limitations that warrant consideration in future research.

First, although we ensured the accuracy of chart information in Chart-MRAG Bench through manual verification, the CHARGE framework would benefit from more advanced OCR techniques to further enhance the accuracy of question generation, especially in handling complex chart layouts and diverse visual elements.

Second, due to computational constraints, our evaluation was confined to a select set of MRAG methods and MLLMs. A more comprehensive evaluation across diverse model architectures and frameworks would likely yield additional insights into the generalizability of our findings and potentially reveal new directions for improvement.
% \section*{Limitations}
% While our work presents promising results, we acknowledge several limitations that warrant consideration in future research.

% First, existing advanced automatic chart extraction methods like \textit{ChartOCR} still face inherent limitations in accurately processing complex chart layouts and diverse visual elements. Although we mitigated these limitations through manual verification to ensure Chart-MRAG Bench's quality, this approach may constrains the scalability of dataset construction. This highlights the need for more robust and accurate chart extraction techniques.

% Second, due to computational constraints, our evaluation was confined to a select set of MRAG methods and MLLMs. A more comprehensive evaluation across diverse model architectures and frameworks would likely yield additional insights into the generalizability of our findings and potentially reveal new directions for improvement.

\section*{Ethical Considerations}
This research was conducted under the approval of our institution's ethics review board. All procedures were designed to ensure participant welfare and data privacy throughout the study.

\paragraph{Participant Recruitment and Compensation.} We recruited expert annotators through Amazon, a professional data annotation platform. Annotators were compensated at a rate of \$28.5 per hour. This rate was determined by:
\begin{itemize}[noitemsep,leftmargin=*]
    \item Conducting pilot studies with 5 annotators to establish an average task completion time of 45 minutes
    \item Accounting for additional training time (30 minutes) and regular breaks
    \item Considering local living wage standards across different regions
    \item Adding a 20\% premium for specialized expertise required
\end{itemize}
For a typical 8-hour workday including training, we ensure fair payment while maintaining data quality. Regular feedback from annotators confirmed the compensation was considered fair for the required expertise and effort.

\paragraph{Informed Consent and Instructions.} All annotators received comprehensive instructions detailing the task requirements, data usage policies, and potential content exposure. The instruction package included:
\begin{itemize}[noitemsep,leftmargin=*]
    \item Task objectives and annotation guidelines
    \item Examples of expected annotations
    \item Data privacy and usage policies
    \item Right to withdraw from participation
\end{itemize}
Annotators provided explicit consent for their contributions to be used in academic research and public datasets.

\paragraph{Annotator Demographics.} Our annotation team consisted of 12 professional annotators with backgrounds in data science and visualization. The annotators represented diverse geographical locations (3 North America, 3 Europe, 6 Asia) and possessed relevant domain expertise. All demographic information was self-reported during the recruitment process.

\paragraph{Data Collection and Privacy.} The datasets used in this study, including those for generating multimodal question-answer pairs, were collected and processed in compliance with GDPR and relevant data privacy regulations. We ensured that:
\begin{itemize}[noitemsep,leftmargin=*]
    \item No personally identifiable information was collected
    \item All chart data was anonymized before annotation
    \item Participants were informed about data usage and sharing plans
\end{itemize}

\paragraph{Bias Mitigation.} We implemented several measures to minimize potential biases in our dataset and evaluation metrics:
\begin{itemize}[noitemsep,leftmargin=*]
    \item Diverse annotator selection to ensure varied perspectives
    \item Regular quality checks for systematic biases in annotations
    \item Balanced representation of different chart types and domains
\end{itemize}

The resulting benchmark will be made publicly available for academic research purposes, accompanied by detailed documentation of the collection process and annotator guidelines. All materials will be released through established academic repositories to ensure transparency and reproducibility.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\section*{Acknowledgements}
We sincerely thank all the anonymous reviewers. The work is supported by the National Natural Science Foundation of China (62206267 and 62176029), Chongqing Key Project of Technological Innovation and Application Development (CSTB2023TIAD-KPX0064), China Postdoctoral Science Foundation Funded Project (2024M763867).
\bibliography{custom}

\clearpage  
\appendix
\section{Keypoints Extraction Details} 
\label{sec: keypoints_extraction}

As illustrated in Fig~\ref{keypoints_extraction_1}, we begin by extracting keypoints from text-chart pairs. Each keypoint represents an atomic unit that encapsulates a specific conclusive statement. The detailed extraction methodology is described in Appendix ~\ref{sec: prompts}.

\begin{figure*}[hp]
  \includegraphics[width=\textwidth]{keypoints_extraction_1.png}
  \caption{Demonstration of extracting atomic information units from text-chart pairs using structured prompts.}
  \label{keypoints_extraction_1}
\end{figure*}

Following the initial extraction, as shown in Fig~\ref{keypoints_extraction_2}, we implement a two-stage filtering process (preliminary screening by \textit{GPT-4o} followed by Crossmodal Verification) to categorize the keypoints into two distinct sets:
\begin{itemize}[noitemsep,leftmargin=*]
\item \textit{Text-only keypoints} ($K^T$): information exclusively present in textual form
\item \textit{Chart-only keypoints} ($K^C$): information uniquely extractable from chart visualization
\end{itemize}

\begin{figure*}[thbp]
  \includegraphics[width=\textwidth]{keypoints_extraction_2.png}
  \caption{Illustration of the keypoints classification process using \textit{GPT-4o} screening and Crossmodal Verification.}
  \label{keypoints_extraction_2}
\end{figure*}

The detailed filtering methodology is provided in prompts~\ref{prompt1} and~\ref{prompt2}.


\section{Crossmodal Vertification Algorithms}
\label{sec:Crossmodal_algorithms}

Algorithm \ref{alg:text_only} presents a robust verification mechanism for text-only keypoint identification. The algorithm validates keypoints through Crossmodal Vertification, confirming a keypoint as text-only when text-only queries can yield the correct answer.
\begin{algorithm}[h]
\SetAlgoLined
\DontPrintSemicolon
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{$K^T$: Text-only keypoints set, $k^t_i \in K^T$\\
       $T$: text chunks, $t_i \in T$\\
       $C$: chart set, $c_i \in C$\\
       $V$: chart info set, $v_i \in V$}
\Output{Updated $K^T$ with verification status}

\For{$k^t_i \in K^T$}{
    $q^t_i \gets LLM(k^t_i)$\;
    $\tilde{k}^t_i \gets LLM(q^t_i, t_i)$ \;
    $\tilde{k}^c_i \gets VLM(q^t_i, c_i, v_i)$ \;
    
    \tcp{Status determination}
    \uIf{$\tilde{k}^t_i = k^t_i$ \textbf{and} $\tilde{k}^c_i \neq k^t_i$}{
        $\text{Status}(k^t_i) \gets \text{Retain}$
    }
    \Else{
        $\text{Status}(k^t_i) \gets \text{Drop}$
    }
}
\Return{$K^T$}
\caption{\small Text-only Keypoint Verification}
\label{alg:text_only}
\end{algorithm}


Algorithm \ref{alg:chart_only} implements a symmetric verification approach for chart-only keypoint detection. Through inverse validation logic, it confirms keypoints as chart-only when only chart-only queries can produce the correct answer.

\begin{algorithm}[h]
\SetAlgoLined
\DontPrintSemicolon
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{$K^C$: Chart-only keypoints set, $k^c_i \in K^C$\\
       $T$: text chunks, $t_i \in T$\\
       $C$: chart set, $c_i \in C$\\
       $V$: chart info set, $v_i \in V$}
\Output{Updated $K^C$ with verification status}

\For{$k^c_i \in K^C$}{
    $q^c_i \gets LLM(k^c_i)$ \;
    $\tilde{k}^t_i \gets LLM(q^c_i, t_i)$ \;
    $\tilde{k}^c_i \gets VLM(q^c_i, c_i, v_i)$ \;
    
    \tcp{Status determination}
    \uIf{$\tilde{k}^c_i = k^c_i$ \textbf{and} $\tilde{k}^t_i \neq k^c_i$}{
        $\text{Status}(k^c_i) \gets \text{Retain}$
    }
    \Else{
        $\text{Status}(k^c_i) \gets \text{Drop}$
    }
}
\Return{$K^C$}
\caption{\small Chart-only Keypoint Verification}
\label{alg:chart_only}
\end{algorithm}


% Algorithm \ref{alg:mixed} introduces a dual-source verification strategy for mixed keypoint classification. The algorithm identifies mixed keypoints by verifying their accessibility through both text and chart queries, ensuring dual-modality information retrieval.
% \begin{algorithm}[h]
% \SetAlgoLined
% \DontPrintSemicolon
% \small
% \SetKwInOut{Input}{Input}
% \SetKwInOut{Output}{Output}
% \Input{$\{TK, CK\}$: keypoints;\\ $T$: text; $C$: chart; $CI$: metadata}
% \Output{$K_{mixed}$: verified mixed points}

% \tcp{Extract candidates}
% $K_{m} \gets \psi(TK, CK)$ \;
% where $K_{m} \subseteq (TK \cup CK)$ \;

% $K_{mixed} \gets \emptyset$ \;
% \For{$k_m \in K_{m}$}{
%     \tcp{Generate query}
%     $q_m \gets \text{Gen}(k_m)$ \;
%     $\text{ans}(q_m) \gets k_m$ \;
%     \tcp{Query models}
%     $\tilde{k}_t \gets \text{LLM}(q_m, T)$ \;
%     $\tilde{k}_c \gets \text{MLLM}(q_m, C, CI)$ \;
%     \tcp{Verify}
%     \If{$\tilde{k}_t = k_m \land \tilde{k}_c = k_m$}{
%         $K_{mixed} \gets K_{mixed} \cup \{k_m\}$ \;
%     }
% }
% \Return{$K_{mixed}$}
% \caption{Mixed Keypoint Verification}
% \label{alg:mixed}
% \end{algorithm}
\section{Question-Answering Pair Generation}
\label{sec: qa_gen_algorithms}
\paragraph{Single-Point Text-only QA} To generate single-point text-only question-answer pairs, we propose a simplified variant of the cross-document QA generation process. As shown in Algorithm~\ref{alg:single_point_text_qa}, the generation process consists of three main steps. First, we randomly select a text keypoint from the document collection that contains a complete, self-contained fact or statement. This approach focuses on validating discrete factual statements contained within a single text keypoint. The algorithm then leverages \textit{GPT-4o} to generate appropriate question-answer pairs based solely on the selected text keypoint and its context. This simplified approach ensures that the generated QA pairs require only single-hop reasoning, making them suitable for evaluating basic reading comprehension and fact extraction capabilities.


\begin{algorithm}[htbp]
\SetAlgoLined
\DontPrintSemicolon
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Document set $D$; \\
       Text keypoint set $K^T$}
\Output{Question-Answer pair $(q, a)$}

\tcp{Step 1: Select text keypoint}
Select $k^t_i \in K^T$ from document $d_a \in D$ \;
$t_i \gets$ corresponding text block in $d_a$ \;

\tcp{Step 2: Validate single-point constraint}
Assert $k^t_i$ contains complete fact or statement \;

\tcp{Step 3: Generate QA pair}
$(q, a) \gets \text{MLLM}(k^t_i, t_i)$ \;

\Return{$(q, a)$}
\caption{\small Single-Point Text-only QA}
\label{alg:single_point_text_qa}
\end{algorithm}

\paragraph{Single-Point Chart-only QA} For generating chart-focused question-answer pairs, we introduce a single-point variant that specializes in visual data comprehension. Algorithm~\ref{alg:single_chart_qa} begins by selecting a chart keypoint that represents a discrete observation from a visual element, such as a specific trend, comparison, or data point. Unlike Algorithm~\ref{alg:single_point_text_qa} which processes textual information, this approach extracts both the chart content and its corresponding numerical value to capture the complete visual context. The algorithm employs \textit{GPT-4o} to generate QA pairs that specifically test chart comprehension skills, ensuring that each question-answer pair is grounded in visual data interpretation without requiring cross-reference to textual content.

\begin{algorithm}[htbp]
\SetAlgoLined
\DontPrintSemicolon
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Document set $D$; \\
       Chart keypoint set $K^C$}
\Output{Question-Answer pair $(q, a)$}

\tcp{Step 1: Select chart keypoint}
Select $k^c_i \in K^C$ from document $d_a \in D$ \;
$c_i \gets$ chart content in $d_a$ \;
$v_i \gets$ corresponding value in $d_a$ \;

\tcp{Step 2: Validate single-point constraint}
Assert $k^c_i$ represents discrete chart observation \;

\tcp{Step 3: Generate QA pair}
$(q, a) \gets \text{MLLM}(k^c_i, c_i, v_i)$ \;

\Return{$(q, a)$}
\caption{\small Single-Point Chart-only QA}
\label{alg:single_chart_qa}
\end{algorithm}

\paragraph{Intra-Document Text-only QA} As illustrated in Algorithm~\ref{alg:intra_text_qa} introduces a systematic approach to constructing QA pairs that capture document-level semantic relationships. The algorithm first selects a primary text keypoint and its associated context, then identifies another relevant text keypoint from the same document to establish intra-document connections. This design ensures that the generated questions necessitate the integration of information from multiple parts of the document, testing a system's ability to perform document-level reasoning and information synthesis. The final generation step employs \textit{GPT-4o} to create questions that effectively evaluate comprehensive document understanding capabilities.
\begin{algorithm}[htbp]
\SetAlgoLined
\DontPrintSemicolon
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Document set $D$; \\
       Text keypoint set $K^T$}
\Output{Question-Answer pair $(q, a)$}

\tcp{Step 1: Select primary text keypoint}
Select $k^t_i \in K^T$ from document $d_a \in D$ \;
$t_i \gets$ corresponding text block in $d_a$ \;

\tcp{Step 2: Retrieve intra-document text keypoint}
$K^T_r \gets \{k^t \in K^T | k^t \text{ from } d_a\}$ \;
Select $k^t_j \in K^T_r$ \;
$t_j \gets$ corresponding text block in $d_a$ \;

\tcp{Step 3: Generate QA pair}
$(q, a) \gets \text{MLLM}(k^t_i, k^t_j, t_i, t_j)$ \;

\Return{$(q, a)$}
\caption{\small Intra-document Text-only QA}
\label{alg:intra_text_qa}
\end{algorithm}

\paragraph{Intra-Document Chart-only QA} Building upon our text-only approach, we propose an algorithm that focuses on reasoning across multiple chart elements within a single document. Algorithm~\ref{alg:intra_chart_qa} presents a structured methodology for generating questions that require the synthesis of information from related visual components. The algorithm initiates by selecting a primary chart keypoint and extracts its corresponding visual features, then identifies semantically related chart elements within the same document to establish comprehensive visual reasoning chains. This architecture enables the generation of questions that assess a system's capability to integrate and reason over multiple visual representations while maintaining document-level consistency. The generation process leverages \textit{GPT-4o} to construct questions that effectively evaluate sophisticated chart comprehension and cross-reference abilities.
\begin{algorithm}[htbp]
\SetAlgoLined
\DontPrintSemicolon
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Document set $D$; \\
       Chart keypoint set $K^C$}
\Output{Question-Answer pair $(q, a)$}

\tcp{Step 1: Select primary chart keypoint}
Select $k^c_i \in K^C$ from document $d_a \in D$ \;
$(c_i, v_i) \gets (c, \psi(c))$ where $c \in d_a$ \;

\tcp{Step 2: Retrieve intra-document chart keypoint}
$K^C_r \gets \{k^c \in K^C | k^c \text{ from } d_a\}$ \;
Select $k^c_j \in K^C_r$ \;
$(c_j, v_j) \gets (c, \psi(c))$ where $c \in d_a$ \;

\tcp{Step 3: Generate QA pair}
$(q, a) \gets \text{MLLM}(k^c_i, k^c_j, c_i, v_i, c_j, v_j)$ \;

\Return{$(q, a)$}
\caption{\small Intra-document Chart-only QA}
\label{alg:intra_chart_qa}
\end{algorithm}

\paragraph{Intra-Document Text-Chart QA} To further enhance the document-level reasoning capabilities, we introduce an algorithm that bridges the gap between textual and visual content within individual documents. Algorithm~\ref{alg:intra_text_chart_qa} establishes a novel approach by first selecting chart-specific keypoints and then retrieving semantically related textual descriptions from the same document. This design facilitates the generation of questions that require joint understanding of both modalities, particularly focusing on how charts and their contextual textual explanations complement each other. Through semantic retrieval between chart and text keypoints, the algorithm ensures that the generated questions capture authentic Crossmodal relationships while maintaining document coherence. The generation process employs \textit{GPT-4o} to synthesize questions that evaluate systems' ability to perform integrated reasoning over both visual and textual information sources.

\begin{algorithm}[htbp]
\SetAlgoLined
\DontPrintSemicolon
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Document set $D$; \\
       Chart keypoint set $K^C$; \\
       Text keypoint set $K^T$}
\Output{Question-Answer pair $(q, a)$}

\tcp{Step 1: Select chart keypoint}
Select $k^c_i \in K^C$ from document $d_a \in D$ \;
$(c_i, v_i) \gets (c, \psi(c))$ where $c \in d_a$ \;

\tcp{Step 2: Retrieve intra-document text keypoint}
$K^T_r \gets \{k^t \in K^T | k^t \text{ from } d_a\}$ \;
$k^t_j \gets {k^t \in K^T_r} \text{Similarity}(k^c_i, k^t)$ \;
$t_j \gets$ corresponding text block in $d_a$ \;

\tcp{Step 3: Generate QA pair}
$(q, a) \gets \text{MLLM}(k^c_i, k^t_j, c_i, v_i, t_j)$ \;

\Return{$(q, a)$}
\caption{\small Intra-document Text-Chart QA}
\label{alg:intra_text_chart_qa}
\end{algorithm}

\paragraph{Inter-Document Text-only QA} To extend our intra-document approach to a broader context, we develop an algorithm that enables reasoning across different documents. Algorithm~\ref{alg:inter_text_qa} introduces a systematic methodology for generating questions that require the integration of information from multiple source documents. The algorithm first selects a primary text keypoint from one document, then identifies semantically related text content from a different document, thereby establishing cross-document connections. This design facilitates the generation of questions that assess a system's ability to synthesize information across document boundaries while maintaining logical coherence. The generation process employs \textit{GPT-4o} to create questions that effectively evaluate comprehensive cross-document understanding and reasoning capabilities.

\begin{algorithm}[htbp]
\SetAlgoLined
\DontPrintSemicolon
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Document set $D$; \\
       Text keypoint set $K^T$}
\Output{Question-Answer pair $(q, a)$}

\tcp{Step 1: Select primary text keypoint}
Select $k^t_i \in K^T$ from document $d_a \in D$ \;
$t_i \gets$ corresponding text block in $d_a$ \;

\tcp{Step 2: Retrieve cross-document text keypoint}
$K^T_r \gets \{k^t \in K^T | k^t \text{ from } d_b \in D, d_b \neq d_a\}$ \;
Select $k^t_j \in K^T_r$ \;
$t_j \gets$ corresponding text block in $d_b$ \;

\tcp{Step 3: Generate QA pair}
$(q, a) \gets \text{MLLM}(k^t_i, k^t_j, t_i, t_j)$ \;

\Return{$(q, a)$}
\caption{\small Inter-document Text-only QA}
\label{alg:inter_text_qa}
\end{algorithm}

\paragraph{Inter-Document Chart-only QA} To further advance cross-document reasoning capabilities, we present an algorithm that enables sophisticated analysis across charts from different documents. Algorithm~\ref{alg:inter_chart_qa} establishes a methodology for generating questions that require the synthesis of visual information spanning multiple documents. The algorithm begins by selecting a primary chart keypoint and its visual features from one document, then identifies related chart elements from a different document to establish cross-document visual reasoning chains. This framework facilitates the generation of questions that evaluate a system's ability to integrate and reason over visual representations across document boundaries while maintaining semantic coherence. The generation process utilizes \textit{GPT-4o} to create questions that effectively assess advanced chart comprehension and cross-document visual reasoning abilities.

\begin{algorithm}[htbp]
\SetAlgoLined
\DontPrintSemicolon
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Document set $D$; \\
       Chart keypoint set $K^C$}
\Output{Question-Answer pair $(q, a)$}

\tcp{Step 1: Select primary chart keypoint}
Select $k^c_i \in K^C$ from document $d_a \in D$ \;
$(c_i, v_i) \gets (c, \psi(c))$ where $c \in d_a$ \;

\tcp{Step 2: Retrieve cross-document chart keypoint}
$K^C_r \gets \{k^c \in K^C | k^c \text{ from } d_b \in D, d_b \neq d_a\}$ \;
Select $k^c_j \in K^C_r$ \;
$(c_j, v_j) \gets (c, \psi(c))$ where $c \in d_b$ \;

\tcp{Step 3: Generate QA pair}
$(q, a) \gets \text{MLLM}(k^c_i, k^c_j, c_i, v_i, c_j, v_j)$ \;

\Return{$(q, a)$}
\caption{\small Inter-document Chart-only QA}
\label{alg:inter_chart_qa}
\end{algorithm}

\paragraph{Inter-Document Text-Chart QA} Extending our Crossmodal reasoning framework beyond single documents, we propose an algorithm that enables sophisticated analysis across textual and visual content from different documents. Algorithm~\ref{alg:inter_text_chart_qa} establishes an advanced approach by selecting chart-specific keypoints from one document and retrieving semantically related textual descriptions from another document. This architecture facilitates the generation of questions that require joint understanding of cross-document modalities, particularly exploring how charts and textual explanations from different sources can be synthesized for comprehensive reasoning. Through cross-document semantic retrieval between chart and text keypoints, the algorithm generates questions that evaluate systems' ability to perform integrated reasoning across both document boundaries and modality gaps. The generation process leverages \textit{GPT-4o} to create questions that assess sophisticated cross-document visual-textual understanding capabilities.

\begin{algorithm}[htbp]
\SetAlgoLined
\DontPrintSemicolon
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Document set $D$; \\
       Chart keypoint set $K^C$; \\
       Text keypoint set $K^T$}
\Output{Question-Answer pair $(q, a)$}

\tcp{Step 1: Select chart keypoint}
Select $k^c_i \in K^C$ from document $d_a \in D$ \;
$(c_i, v_i) \gets (c, \psi(c))$ where $c \in d_a$ \;

\tcp{Step 2: Retrieve cross-document text keypoint}
$K^T_r \gets \{k^t \in K^T | k^t \text{ from } d_b \in D, d_b \neq d_a\}$ \;
$k^t_j \gets {k^t \in K^T_r} \text{Similarity}(k^c_i, k^t)$ \;
$t_j \gets$ corresponding text block in $d_b$ \;

\tcp{Step 3: Generate QA pair}
$(q, a) \gets \text{MLLM}(k^c_i, k^t_j, c_i, v_i, t_j)$ \;

\Return{$(q, a)$}
\caption{\small Inter-document Text-Chart QA}
\label{alg:inter_text_chart_qa}
\end{algorithm}


\section{Chart-MRAG Bench Cases}
\label{sec: chart-mrag_bench_cases}
To illustrate the diverse chart categories in Chart-MRAG Bench, we present representative examples as shown in Figure~\ref{chart_categories}.

\begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{chart_categories.png}
  \caption{Representative visualization categories from Chart-MRAG Bench, showcasing temporal trend analysis (line charts), geospatial data visualization (choropleth maps), categorical comparisons (bar charts), compositional analysis (stacked bars), and integrated text-chart representations. The diversity of these examples demonstrates the comprehensive scope of Chart-MRAG Bench in representing complex statistical information across multiple domains and visualization paradigms.}
  \label{chart_categories}
\end{figure*}

We categorize the question-answering pairs in Chart-MRAG into eight distinct categories, as summarized in Table~\ref{tab:qa_taxonomy}, encompassing various combinations of single-point, intra-document, and inter-document scenarios across text-only, chart-only, and text-chart contexts. These categories are illustrated through representative examples: Single-Point Text-Only QA (Fig.~\ref{sample_case_1}), Single-Point Chart-Only QA (Fig.~\ref{sample_case_2}), Intra-Document Text-Only QA (Fig.~\ref{sample_case_3}), Intra-Document Chart-Only QA (Fig.~\ref{sample_case_4}), Intra-Document Text-Chart QA (Fig.~\ref{sample_case_5}), Inter-Document Text-Only QA (Fig.~\ref{sample_case_6}), Inter-Document Chart-Only QA (Fig.~\ref{sample_case_7}), and Inter-Document Text-Chart QA (Fig.~\ref{sample_case_8}).


\begin{table*}[htbp]
\centering
\setlength{\tabcolsep}{3pt}
\resizebox{\textwidth}{!}{
\small
\begin{tabular}{l|p{0.7\textwidth}}
\toprule
\multicolumn{2}{l}{\textbf{Source-Constrained and Modality-Constrained Question-Answer Categories}} \\
\midrule

\multirow{2}{*}{Single-Point Text-Only} & Questions that require reasoning about an individual textual keypoint ($k^t_i \in K^T$), focusing on discrete factual validation within a single text segment. \\
\midrule

\multirow{2}{*}{Single-Point Chart-Only} & Questions centered on an individual chart-only keypoint ($k^c_i \in K^C$), examining specific data points or visual elements within a single chart. \\
\midrule

\multirow{2}{*}{Intra-Document Text-Only} & Questions that necessitate integrative reasoning across multiple textual keypoints ($k^t_i, k^t_j \in K^T$) within the same document ($d_i \in D$). \\
\midrule

\multirow{2}{*}{Intra-Document Chart-Only} & Questions requiring comparative analysis of multiple chart-only keypoints ($k^c_i, k^c_j \in K^C$) from a single document ($d_i \in D$). \\
\midrule

\multirow{2}{*}{Intra-Document Text-Chart} & Questions involving cross-modal reasoning between textual and chart-only keypoints ($k^t_i \in K^T, k^c_j \in K^C$) within the same document ($d_i \in D$). \\
\midrule

\multirow{2}{*}{Inter-Document Text-Only} & Questions demanding associative reasoning between textual keypoints ($k^t_i, k^t_j \in K^T$) from distinct documents ($d_i, d_j \in D, i \neq j$). \\
\midrule

\multirow{2}{*}{Inter-Document Chart-Only} & Questions requiring comparative analysis of chart-only keypoints ($k^c_i, k^c_j \in K^C$) across different documents ($d_i, d_j \in D, i \neq j$). \\
\midrule

\multirow{2}{*}{Inter-Document Text-Chart} & Questions involving cross-modal and cross-document reasoning, integrating textual and chart keypoints ($k^t_i \in K^T, k^c_j \in K^C$) from different documents ($d_i, d_j \in D, i \neq j$). \\
\bottomrule
\end{tabular}
}
\caption{Taxonomy of question-answer pairs in Chart-MRAG, categorized by source constraints (Single-Point/Intra-Document/Inter-Document) and modality constraints (Text-only/Chart-only/Text-Chart).}
\label{tab:qa_taxonomy}
\end{table*}

\section{Retrieval Setup and Metrics}
\label{sec: retrieval_setup_and_metrics}

\subsection{Retrieval Setup}
For retrieval system, we designed three distinct configurations to evaluate different approaches to multimodal information retrieval:

\noindent \textbf{Unified Multimodal Embedding and Single Vector Store}. We directly embedded charts and text into a unified embedding space using vision-language models CLIP, JINA-CLIP, and SigLIP. This approach maps all content to same-dimensional vectors in a single vector store, enabling cross-modal matching between queries and documents regardless of their original modality.


\noindent \textbf{Multimodal Embeddings and Combined Vector Stores}. In this approach, charts are first converted to text summaries using \textit{GPT-4o}. Both these summaries and PDF-extracted text are then embedded using sparse BM25 and dense embedding models BGE-M3-base/large, E5-base/large into their respective vector stores. Similarity search in these embedding spaces retrieves relevant documents across both modalities.

\noindent \textbf{Multimodal Embeddings and Separate Vector Stores}. This approach maintains distinct embedding spaces for different modalities, leveraging specialized models for optimal representation. Charts are encoded using vision-language models (CLIP, JINA-CLIP, SigLIP), while textual content is processed through both sparse retrieval (BM25) and dense embedding models (BGE-M3-base/large, E5-base/large). The retrieval process operates in parallel across separate vector stores, with the final results aggregated using a weighted combination scheme.


\subsection{Retrieval Metrics}
We segment text into semantic chunks with an average length of 24.97 words, while treating each chart as an individual retrieval unit. We employ Recall@5 and Recall@10 as primary retrieval metrics. To ensure balanced representation, we implement a text-to-chart ratio of 3:2 in the final retrieval results.

Given that Chart-MRAG bench primarily consists of multi-hop questions requiring both textual and visual information, the comprehensive retrieval of all relevant sources is crucial for accurate answers. We employ $\text{Recall}@5$ and $\text{Recall}@10$ to evaluate the effectiveness and efficiency of the retrieval stage.

\textbf{Multimodal Recall.} We introduce a Multimodal RAG Retrieval Recall metric to evaluate the effectiveness of crossmodal retrieval process. For textual content, we perform sentence-level retrieval, while for charts, we treat each visualization as an individual reference unit. The Recall is formally defined as
\begin{equation}
    \text{Recall} = \frac{1}{n}\sum_{i=1}^n \mathbbm{1}(M(G_i,\mathcal{R})),
    \label{eq:recall}
\end{equation}

where $n$ is the total number of ground truth references (including both text chunks and charts), $G_i$ denotes the $i$-th ground truth reference, $\mathcal{R} = \{R_1, R_2, \ldots, R_k\}$ represents the set of retrieved references, $M(G_i,\mathcal{R})$ is a boolean function that returns true if (1) for textual references, all constituent sentences in $G_i$ are found in at least one reference in $\mathcal{R}$, or (2) for chart references, the exact chart is present in $\mathcal{R}$, and $\mathbbm{1}(\cdot)$ is the indicator function.

This metric assesses the crossmodal alignment between retrieved and ground truth references, where successful retrieval is determined by modality-specific criteria: sentence-level matching for text and exact matching for charts.

\section{Text-Over-Visual Modality Bias Case}
\label{sec: bias_case}

Section~\ref{bias_case} presents a comprehensive analysis of Text-Over-Visual Modality Bias, revealing a systematic preference for text-based processing across different model scales. Our experiments demonstrate that multimodal language models consistently favor text-only responses, even in scenarios where visual elements (particularly charts) contain more precise and relevant information. This bias raises important questions about the effective integration of multiple modalities in current AI systems.

Notably, our investigation reveals a clear correlation between model scale and the ability to handle multimodal information effectively. Larger MLLMs, particularly \textit{GPT-4o}, demonstrate sophisticated capabilities in detecting and managing information redundancy across modalities, proactively acknowledging such overlaps in 23\% of their responses. This behavior suggests a more nuanced understanding of the complementary nature of different information sources.

In contrast, smaller models exhibit significant limitations in processing multimodal inputs. For instance, SAIL-VL-2B (2B parameters) shows a stark inability to integrate information across modalities, highlighting the critical role of model scale in achieving effective multimodal reasoning.

\begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{Modality_Bias_case.png}
  \caption{A Sample Case of Text-Over-Visual Modality Bias}
  \label{bias_case}
\end{figure*}

\section{Model Prompts}
\label{sec: prompts}
CHARGE framework encompasses multiple stages, each guided by specific prompts designed to facilitate different aspects of the process. We detail these prompts according to their respective stages:

In the Extract Keypoints stage, we employ two specialized prompts: one for document keypoint extraction (Fig.~\ref{prompt1}) and another for chart keypoint extraction (Fig.~\ref{prompt2}). These prompts are designed to identify and extract crucial information points from both textual and visual components.

The Crossmodal Verification stage utilizes two key prompts: a keypoint classification prompt (Fig.~\ref{prompt3}) and a cross-modal information verification protocol (Fig.~\ref{prompt4}). These prompts work in tandem to ensure the consistency and accuracy of information across different modalities.

For Question-Answer Pair Generation, we implement two distinct protocols: a single-point generation protocol (Fig.~\ref{prompt5}) for straightforward questions, and a multi-hop generation protocol (Fig.~\ref{prompt6}) for complex questions requiring multiple reasoning steps.

The Response stage features two prompts: one designed for generating responses without retrieved information (Fig.~\ref{prompt7}), and another for responses incorporating retrieved information (Fig.~\ref{prompt8}). This dual approach enables flexible response generation based on available context.

Finally, the Evaluation stage employs two metric calculation prompts: one for assessing correctness (Fig.~\ref{prompt9}) and another for measuring coverage (Fig.~\ref{prompt10}). These prompts ensure comprehensive evaluation of the generated responses.

\label{sec:chartmrag_cases_1}
\begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{sample_case_1.png}
  \caption{A sample case of single-point text-only question answering.}
  \label{sample_case_1}
\end{figure*}

\begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{sample_case_2.png}
  \caption{A sample case of single-point chart-only question answering.}
  \label{sample_case_2}
\end{figure*}

\begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{sample_case_3.png}
  \caption{A sample case of intra-document text-only question answering.}
  \label{sample_case_3}
\end{figure*}

\begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{sample_case_4.png}
  \caption{A sample case of intra-document chart-only question answering.}
  \label{sample_case_4}
\end{figure*}

\begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{sample_case_5.png}
  \caption{A sample case of intra-document text-chart question answering.}
  \label{sample_case_5}
\end{figure*}

\begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{sample_case_6.png}
  \caption{A sample case of inter-document text-only question answering.}
  \label{sample_case_6}
\end{figure*}

\begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{sample_case_7.png}
  \caption{A sample case of inter-document chart-only question answering.}
  \label{sample_case_7}
\end{figure*}

\begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{sample_case_8.png}
  \caption{A sample case of inter-document text-chart question answering.}
  \label{sample_case_8}
\end{figure*}

\begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{Prompt_1_Document_Keypoints_Extraction.png}
  \caption{Document keypoints extraction prompt details.}
  \label{prompt1}
\end{figure*}

\begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{Prompt_2_Chart_Keypoints_Extraction.png}
  \caption{Chart keypoints extraction prompt details.}
  \label{prompt2}
\end{figure*}

 \begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{Prompt_3_Keypoint_Classification_Task.png}
  \caption{Keypoint classification task prompt details.}
  \label{prompt3}
\end{figure*}

\begin{figure*}[htbp]
  \includegraphics[width=\textwidth]{Prompt_4_Cross-Modal_Information_Verification_Protocol.png}
  \caption{Cross-modal information verification protocol prompt details.}
  \label{prompt4}
\end{figure*}

\begin{figure*}[htbp]
\includegraphics[width=\textwidth]{Prompt_5_Single-Point_Question-Answer_Generation_Protocol.png}
\caption{Single-point question-answer generation protocol prompt details.}
\label{prompt5}
\end{figure*}

 \begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{Prompt_6_Multi-Hop_Question-Answer_Generation_Protocol.png}
  \caption{Multi-hop question-answer generation protocol prompt details.}
  \label{prompt6}
\end{figure*}

 \begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{Prompt_7_Response_without_Retrieved_Information.png}
  \caption{Response without retrieved information prompt details.}
  \label{prompt7}
\end{figure*}

 \begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{Prompt_8_Response_with_Retrieved_Information.png}
  \caption{Response with retrieved information prompt details.}
  \label{prompt8}
\end{figure*}

 \begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{Prompt_9_Correctness_Metric_Calculation.png}
  \caption{Correctness metric calculation prompt details.}
  \label{prompt9}
\end{figure*}

 \begin{figure*}[htbp] 
  \includegraphics[width=\textwidth]{Prompt_10_Coverage_Metric_Calculation.png}
  \caption{Coverage metric calculation prompt details.}
  \label{prompt10}
\end{figure*}

\end{document}
