\section{Related Work}
\noindent \textbf{Multimodal RAG Methods.} Recent advances in Retrieval-Augmented Generation (RAG) \cite{izacard2022few,zhang2024raft} have successfully extended to multimodal domains \cite{chen2022murag,zhao2023retrieving,zhao2024retrieval}, enabling crossmodal tasks through MLLMs \cite{yao2024minicpm, sailvl}. While researchers have proposed various approaches \cite{ma-etal-2024-unifying,faysse2024colpali,yu2024visrag,Methani_2020_WACV,mathew2021docvqa} for crossmodal retrieval, current evaluation methodologies predominantly rely on Visual Question Answering (VQA) datasets \cite{marino2019ok,talmor2021multimodalqa,schwenk2022okvqa,masry2022chartqa}. These evaluations fall short in addressing retrieval-specific challenges.

\begin{figure*}[th]
  \includegraphics[width=\textwidth]{Main_flow_diagram.png}
  \caption{The proposed CHARGE framework for creating multimodal QA pairs from document-chart data, consisting of three steps: (1) Extract keypoints from textual content and charts, (2) Perform crossmodal verification to validate keypoint modality uniqueness, (3) Generate diverse QA pairs through constrained keypoint retrieval.}
  \label{Main_flow_diagram}
\end{figure*}

\noindent \textbf{Multimodal RAG Benchmarks.} The effectiveness of MRAG systems necessitates comprehensive evaluation benchmarks. While several benchmarks \cite{hu2024mragbench,li2024benchmarking,zhou2024megapairs} explore vision-based retrieval for question answering through manual annotation, they neglect the critical dimension of crossmodal collaborative generation. Some studies \cite{dong2025mmdocir,ma2024mmlongbench,ding2024mvqa} consider hybrid modality retrieval, yet they primarily rely on manual question-answering. Furthermore, although some studies \cite{es2023ragas,abaskohi2024fm2ds,mathew2021docvqa,li2024multimodal,wu2024synthetic} have investigated automated processes for generating crossmodal QA pairs, their scope focus on simplistic natural images with singular subjects, the chart-based scenarios largely unexplored. To bridge this gap, this paper introduce Chart-MRAG Bench. Table~\ref{tab:data_compare} illustrates the differences between existing MRAG benchmarks and Chart-MRAG Bench.