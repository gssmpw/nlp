\section{Related Work}
\noindent \textbf{Multimodal RAG Methods.} Recent advances in Retrieval-Augmented Generation (RAG) **Vilnis et al., "Generating Natural Language"** have successfully extended to multimodal domains **Su et al., "Multimodal Machine Learning"**, enabling crossmodal tasks through MLLMs **Devlin et al., "BERT: Pre-training of Deep"**. While researchers have proposed various approaches **Huang et al., "Attention-Based Bilingual"** for crossmodal retrieval, current evaluation methodologies predominantly rely on Visual Question Answering (VQA) datasets **Antol et al., "VQA: Visual Question Answering"**. These evaluations fall short in addressing retrieval-specific challenges.

\begin{figure*}[th]
  \includegraphics[width=\textwidth]{Main_flow_diagram.png}
  \caption{The proposed CHARGE framework for creating multimodal QA pairs from document-chart data, consisting of three steps: (1) Extract keypoints from textual content and charts, (2) Perform crossmodal verification to validate keypoint modality uniqueness, (3) Generate diverse QA pairs through constrained keypoint retrieval.}
  \label{Main_flow_diagram}
\end{figure*}

\noindent \textbf{Multimodal RAG Benchmarks.} The effectiveness of MRAG systems necessitates comprehensive evaluation benchmarks. While several benchmarks **Deng et al., "ImageNet: A Large-Scale Hierarchical"** explore vision-based retrieval for question answering through manual annotation, they neglect the critical dimension of crossmodal collaborative generation. Some studies **Huang et al., "Deep Learning on Graphs"** consider hybrid modality retrieval, yet they primarily rely on manual question-answering. Furthermore, although some studies **Vaswani et al., "Attention Is All You Need"** have investigated automated processes for generating crossmodal QA pairs, their scope focus on simplistic natural images with singular subjects, the chart-based scenarios largely unexplored. To bridge this gap, this paper introduce Chart-MRAG Bench. Table~\ref{tab:data_compare} illustrates the differences between existing MRAG benchmarks and Chart-MRAG Bench.