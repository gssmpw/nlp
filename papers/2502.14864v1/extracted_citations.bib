@InProceedings{Methani_2020_WACV,
author = {Methani, Nitesh and Ganguly, Pritha and Khapra, Mitesh M. and Kumar, Pratyush},
title = {PlotQA: Reasoning over Scientific Plots},
booktitle = {The IEEE Winter Conference on Applications of Computer Vision (WACV)},
month = {March},
year = {2020}
}

@article{abaskohi2024fm2ds,
  title={FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering},
  author={Abaskohi, Amirhossein and Gella, Spandana and Carenini, Giuseppe and Laradji, Issam H},
  journal={arXiv preprint arXiv:2412.07030},
  year={2024}
}

@article{chen2022murag,
  title={Murag: Multimodal retrieval-augmented generator for open question answering over images and text},
  author={Chen, Wenhu and Hu, Hexiang and Chen, Xi and Verga, Pat and Cohen, William W},
  journal={arXiv preprint arXiv:2210.02928},
  year={2022}
}

@article{ding2024mvqa,
  title={MVQA: A Dataset for Multimodal Information Retrieval in PDF-based Visual Question Answering},
  author={Ding, Yihao and Ren, Kaixuan and Huang, Jiabin and Luo, Siwen and Han, Soyeon Caren},
  journal={arXiv preprint arXiv:2404.12720},
  year={2024}
}

@article{dong2025mmdocir,
  title={MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents},
  author={Dong, Kuicai and Chang, Yujing and Goh, Xin Deik and Li, Dexun and Tang, Ruiming and Liu, Yong},
  journal={arXiv preprint arXiv:2501.08828},
  year={2025}
}

@article{es2023ragas,
  title={Ragas: Automated evaluation of retrieval augmented generation},
  author={Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven},
  journal={arXiv preprint arXiv:2309.15217},
  year={2023}
}

@misc{faysse2024colpali,
      title={ColPali: Efficient Document Retrieval with Vision Language Models}, 
      author={Manuel Faysse and Hugues Sibille and Tony Wu and Bilel Omrani and Gautier Viaud and CÃ©line Hudelot and Pierre Colombo},
      year={2024},
      eprint={2407.01449},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2407.01449}, 
}

@article{hu2024mragbench,
    title={MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models},
    author={Hu, Wenbo and Gu, Jia-Chen and Dou, Zi-Yi and Fayyaz, Mohsen and Lu, Pan and Chang, Kai-Wei and Peng, Nanyun},
    journal={arXiv preprint arXiv:2410.08182},
    year={2024}
}

@article{izacard2022few,
  title={Few-shot learning with retrieval augmented language models},
  author={Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  journal={arXiv preprint arXiv:2208.03299},
  volume={1},
  number={2},
  pages={4},
  year={2022}
}

@article{li2024benchmarking,
  title={Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent},
  author={Li, Yangning and Li, Yinghui and Wang, Xingyu and Jiang, Yong and Zhang, Zhen and Zheng, Xinran and Wang, Hui and Zheng, Hai-Tao and Yu, Philip S and Huang, Fei and others},
  journal={arXiv preprint arXiv:2411.02937},
  year={2024}
}

@article{li2024multimodal,
  title={Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models},
  author={Li, Lei and Wang, Yuqi and Xu, Runxin and Wang, Peiyi and Feng, Xiachong and Kong, Lingpeng and Liu, Qi},
  journal={arXiv preprint arXiv:2403.00231},
  year={2024}
}

@article{ma2024mmlongbench,
  title={Mmlongbench-doc: Benchmarking long-context document understanding with visualizations},
  author={Ma, Yubo and Zang, Yuhang and Chen, Liangyu and Chen, Meiqi and Jiao, Yizhu and Li, Xinze and Lu, Xinyuan and Liu, Ziyu and Ma, Yan and Dong, Xiaoyi and others},
  journal={arXiv preprint arXiv:2407.01523},
  year={2024}
}

@inproceedings{marino2019ok,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={Proceedings of the IEEE/cvf conference on computer vision and pattern recognition},
  pages={3195--3204},
  year={2019}
}

@article{masry2022chartqa,
  title={Chartqa: A benchmark for question answering about charts with visual and logical reasoning},
  author={Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  journal={arXiv preprint arXiv:2203.10244},
  year={2022}
}

@inproceedings{mathew2021docvqa,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={2200--2209},
  year={2021}
}

@misc{sailvl,
  title = {SAIL-VL: Scalable Vision Language Model Training with High Quality Data Curation},
  url = {https://huggingface.co/BytedanceDouyinContent/SAIL-VL-2B/},
  author = {Bytedance Douyin Content Team},
  month = {December},
  year = {2024}
}

@inproceedings{schwenk2022okvqa,
  title={A-okvqa: A benchmark for visual question answering using world knowledge},
  author={Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
  booktitle={European conference on computer vision},
  pages={146--162},
  year={2022},
  organization={Springer}
}

@article{talmor2021multimodalqa,
  title={Multimodalqa: Complex question answering over text, tables and images},
  author={Talmor, Alon and Yoran, Ori and Catav, Amnon and Lahav, Dan and Wang, Yizhong and Asai, Akari and Ilharco, Gabriel and Hajishirzi, Hannaneh and Berant, Jonathan},
  journal={arXiv preprint arXiv:2104.06039},
  year={2021}
}

@article{wu2024synthetic,
  title={Synthetic multimodal question generation},
  author={Wu, Ian and Jayanthi, Sravan and Viswanathan, Vijay and Rosenberg, Simon and Pakazad, Sina and Wu, Tongshuang and Neubig, Graham},
  journal={arXiv preprint arXiv:2407.02233},
  year={2024}
}

@article{yao2024minicpm,
  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}

@article{yu2024visrag,
  title={Visrag: Vision-based retrieval-augmented generation on multi-modality documents},
  author={Yu, Shi and Tang, Chaoyue and Xu, Bokai and Cui, Junbo and Ran, Junhao and Yan, Yukun and Liu, Zhenghao and Wang, Shuo and Han, Xu and Liu, Zhiyuan and others},
  journal={arXiv preprint arXiv:2410.10594},
  year={2024}
}

@article{zhang2024raft,
  title={Raft: Adapting language model to domain specific rag},
  author={Zhang, Tianjun and Patil, Shishir G and Jain, Naman and Shen, Sheng and Zaharia, Matei and Stoica, Ion and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2403.10131},
  year={2024}
}

@article{zhao2023retrieving,
  title={Retrieving multimodal information for augmented generation: A survey},
  author={Zhao, Ruochen and Chen, Hailin and Wang, Weishi and Jiao, Fangkai and Do, Xuan Long and Qin, Chengwei and Ding, Bosheng and Guo, Xiaobao and Li, Minzhi and Li, Xingxuan and others},
  journal={arXiv preprint arXiv:2303.10868},
  year={2023}
}

@article{zhao2024retrieval,
  title={Retrieval-augmented generation for ai-generated content: A survey},
  author={Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Cui, Bin},
  journal={arXiv preprint arXiv:2402.19473},
  year={2024}
}

@article{zhou2024megapairs,
  title={MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval},
  author={Zhou, Junjie and Liu, Zheng and Liu, Ze and Xiao, Shitao and Wang, Yueze and Zhao, Bo and Zhang, Chen Jason and Lian, Defu and Xiong, Yongping},
  journal={arXiv preprint arXiv:2412.14475},
  year={2024}
}

