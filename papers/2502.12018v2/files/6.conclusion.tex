\section{Conclusion}
In this paper, we introduced Atom of Thoughts (\our), a novel framework that transforms complex reasoning processes into a Markov process of atomic questions. By implementing a two-phase transition mechanism of decomposition and contraction, \our eliminates the need to maintain historical dependencies during reasoning, allowing models to focus computational resources on the current question state. Our extensive evaluation across diverse benchmarks demonstrates that \our serves effectively both as a standalone framework and as a plug-in enhancement for existing test-time scaling methods. These results validate \our's ability to enhance LLMs' reasoning capabilities while optimizing computational efficiency through its Markov-style approach to question decomposition and atomic state transitions.