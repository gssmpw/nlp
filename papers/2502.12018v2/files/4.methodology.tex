\section{The Design Details of \our}
This section details the implementation of \our's core components: decomposition and contraction, which together form one iteration of state transition in the Markov reasoning process, as illustrated in Figure \ref{fig:pipeline}. Through structured decomposition and principled contraction, our approach establishes a foundation for iterative reasoning that can flexibly integrate with other methods while balancing computational efficiency and reasoning depth.

\subsection{Decomposition}

\paragraph{Dependency Directed Acyclic Graph.}
Addressing the challenge of excessive historical information maintenance, our decomposition phase introduces an efficient dependency extraction mechanism that only temporarily captures rich structural information, which provides the foundation for subsequent simplification. This process starts with decomposing the current question into granular subquestions, then leverages LLMs' zero-shot capabilities to efficiently identify inter-question dependencies. The dependency extraction is achieved through a JSON-formatted LLM invocation that progressively labels each subquestion's dependencies by indexing its upstream questions (see Appendix~\ref{appendix:label} for annotation prompt templates).

\subsection{Contraction}

\paragraph{Subquestions Contracting.}
Based on the dependency relationships identified in DAG structure, \our performs contraction through a single LLM invocation. This process constructs an independent contracted question by selectively integrating information from independent subquestions as known conditions and incorporating the descriptions of current dependent subquestions into the main body. This process maintains answer equivalence throughout the Markov chain while continuously eliminating the test-time of solved independent subquestions in past iterations when solving the contracted question independently. The elimination of the dependency relationships from independent subquestions and the generated contracted question facilitates the transmission of key information that causes dependency. (see Appendix~\ref{appendix:contract} for contraction prompt templates).

\paragraph{Markov Property Maintenance.}
Through this contraction process, \our effectively eliminates redundant information in historical reasoning steps to reduce the test-time required for solving questions in subsequent states. The contraction mechanism ensures that each state generated in the process depends only on its immediate predecessor, preserving the Markov property while progressively simplifying inherent complexity of the question in the current state.

\subsection{Integration}

\paragraph{Iterative Process.}
The pipeline of \our operates through an iterative process where each state transition step involves question decomposition followed by contraction. The contracted question from each iteration serves as the input for the next decomposition phase. As the number of iterations increases, the test-time scales up in an attempt to achieve more robust and effective reasoning.

\paragraph{Termination Mechanism.}
To optimize test-time efficiency, \our incorporates an automated termination mechanism that uses LLM evaluation to assess solution quality through answer comparison. After each contraction step, an LLM examines three key elements: the execution results of the original question $Q_{i}$, the decomposed DAG structure $\mathcal{G}i$, and the independent execution results of contracted question $Q_{i+1}$. The LLM synthesizes these elements to generate a comprehensive answer for $Q_i$. If this synthesized answer demonstrates consistency with the answer produced by $Q_{i+1}$, the iterative process continues. Upon termination, \our combines the current contracted question with the union of independent subquestions $\mathcal{Q}_{dep} = \bigcup_{j=1}^i \mathcal{Q}_{dep_j}$ accumulated from all previous iterations to form a complete solution to the initial question $Q_0$. This structure provides a solution composed entirely of independent questions, maintaining semantic independence of each subquestion while ensuring completeness of whole solution.

\paragraph{Integration Through Configurable Termination.}
Building upon this termination mechanism, \our enables seamless integration with existing test-time scaling methods by allowing any intermediate state to serve as an entry point. This flexibility comes from \our's configurable termination strategy - often just a single decomposition-contraction cycle - before passing this simplified question to other methods (refer to the right portion of Figure~\ref{fig:pipeline}). This approach leverages \our's structural optimization capabilities as a preprocessing step while allowing other methods to operate on a more manageable question. The contracted question passed to subsequent methods maintains answer equivalence with the original question while \our's initial structural simplification helps redirect computational resources towards more direct and effective reasoning. The seamless transition between methods is facilitated by the atomic state representation in our Markov process, ensuring that essential question characteristics are preserved while unnecessary historical information is eliminated.

\begin{table*}[t!]
   \centering
   \caption{Performance Comparison Across Tasks (\%). We evaluate three variants: the base version (\our), a version integrated with FoT (\our($d$=1) + FoT($n$=2)), and a computationally intensive version (\our$^{*}$) that uses LLM to select the optimal answer from three runs. Results are reported as exact match accuracy for MATH, GSM8K, BBH, and MMLU-CF, and F1 scores for HotpotQA and LongBench.}
   \renewcommand\tabcolsep{3.2pt}
   \renewcommand\arraystretch{1.2}
   \small
   \setlength{\abovecaptionskip}{0.1cm}
   \setlength{\belowcaptionskip}{-0.2cm}
   \begin{tabular}{l|ccccccc}
        \hline
        
        \hline
        
        \hline
        
        \hline
       Method & MATH & GSM8K & BBH & MMLU-CF & HotpotQA & LongBench & Avg. \\
       \hline
       CoT & 78.3 & 90.9 & 78.3 & 69.6 & 67.2 & 57.6 & 73.7\\
       CoT-SC ($n$=5) & 81.8 & 92.0 & 83.4 & \underline{71.1} & 66.2 & 58.6 & 75.5\\
       Self-Refine & 78.7 & 91.7 & 80.0 & 69.7 & 68.3 & 58.2 & 74.4\\
       Analogical Prompting & 65.4 & 87.2 & 72.5 & 65.8 & 64.7 & 52.9 & 68.1\\
        \hline

        \hline
       AFlow & 83.0 & 93.5 & 76.0& 69.5 & 73.5 & 61.0 & 76.1\\
       FoT ($n$=8)& 82.5 & 94.0 & 82.4 & 70.6 & 66.7 & 59.1 & 75.9\\
       
        \hline

        \hline
       \rowcolor[gray]{.85}
       \our($d$=1) + FoT ($n$=2) & 82.6 & 94.2 & 82.2 & 69.7 & 67.6 & 58.4 & 75.8\\
       \rowcolor[gray]{.85}
       \our (Ours)& \underline{83.6} & \underline{95.0}& \underline{86.0} & 70.9 & \underline{80.6} & \underline{68.5} & \underline{80.8}\\
       \rowcolor[gray]{.85}
       \our$^{*}$ (Ours) & \textbf{84.9} & \textbf{95.1}& \textbf{87.4} & \textbf{71.2} & \textbf{81.0} & \textbf{68.8} & \textbf{81.4}\\
        \hline

        \hline
        
        \hline
        
        \hline
   \end{tabular}
   \label{tab:performance}
   \vspace{-1em}
\end{table*}