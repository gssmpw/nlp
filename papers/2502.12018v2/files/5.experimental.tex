\section{Experiments}
We conduct comprehensive experiments to examine \our through extensive benchmark evaluation on six standard datasets, reasoning models comparison, test-time optimization experiments, and ablation studies. Our main results demonstrate consistent improvements across different reasoning tasks, with significant gains especially in multi-hop reasoning. Through comparison with state-of-the-art reasoning models, we show \our's effectiveness as a general framework. Our test-time optimization experiments further validate \our's adaptability and efficiency. Finally, ablation studies on key components like DAG structure and decomposition mechanism confirm the essentiality of our design choices.

\subsection{Experimental Setup}
\paragraph{Datasets.}
We evaluate \our using {\texttt{gpt-4o-mini-0718}} as the backbone model, chosen for its strong performance-efficiency trade-off. Our evaluation covers four categories of reasoning tasks: mathematical reasoning (MATH~\cite{MATH2021} with numerical answers and GSM8K~\cite{gsm8k2021}), knowledge-intensive reasoning (MMLU-CF~\cite{MMLUCF2024}), logical reasoning (multiple-choice subsets of BBH~\cite{BBH2023}, see Appendix~\ref{appendix:data} for details), and multi-hop reasoning (HotpotQA~\cite{HotpotQA2018} and LongBench~\cite{longbench2024} which test models' ability to connect information across multiple contexts). We use the first 1,000 examples from each dataset's test set, except for GSM8K where we use its complete test set (1,319 examples) and LongBench where we use the combined MuSiQue~\cite{Musique2022} and 2WikiMultiHopQA~\cite{2WikiMultiHopQA} subsets (400 examples).

\paragraph{Baselines.}
Our baselines include classical prompting methods (Chain-of-Thought (CoT), CoT with Self-Consistency (CoT-SC, $n$ = 5), Self-Refine, and Analogical Reasoning~\cite{Yasunaga2024AP}) and advanced reasoning frameworks (agentic workflow AFlow~\cite{Zhang2024aflow} and Forest of Thought (FoT)). For FoT, we implement it using Tree of Thoughts with branch number $b$ = 3, chosen for its generalizability across diverse tasks. All experiments are averaged over three runs, with detailed reproduction settings in Appendix \ref{appendix:implementation_details}.

\subsection{Experimental Results and Analysis.}

\paragraph{Main Results}
As shown in Table \ref{tab:performance}, \our demonstrates consistent improvements across different reasoning tasks. \our achieves strong performance on mathematics tasks, with \our$^{*}$ reaching 84.9\% on MATH and 95.1\% on GSM8K (+1.9\% over AFlow on MATH, +1.1\% over FoT($n$=8) on GSM8K). The most notable improvements are in multi-hop QA tasks, where our base version achieves 80.6\% F1 score on HotpotQA (+7.1\% over AFlow). Similar improvements on LongBench (68.8\%, +7.5\% over AFlow) further demonstrate the effectiveness of \our's atomic state representation in long context scenarios.

\paragraph{Reasoning Models Comparison Results.}

\begin{table}[t!]
    \centering
    \caption{Comparison of Reasoning Model Performance on Multi-hop QA Tasks. Results show F1 scores and Hit rates (F1 > 0) for HotpotQA and LongBench across different models.}
    \renewcommand\arraystretch{1.1}
    \small
    \begin{tabular}{ll|cccc}
        \hline

        \hline
        \multicolumn{2}{l|}{Method} & \multicolumn{2}{c}{HotpotQA} & \multicolumn{2}{c}{LongBench} \\
        \multicolumn{2}{l|}{} & F1 & Hit & F1 & Hit \\
        \hline
        \multirow{3}{*}{CoT} 
        & QwQ & 68.1 & 82.4 & 52.7 & 65.6 \\
        & DeepSeek-R1 & 70.0 & 85.5 & 56.0 & 69.9 \\
        & o3-mini & 77.2 & 88.3 & 55.3 & \underline{70.0} \\
        \hline
        \multirow{2}{*}{\our} 
        & gpt-4o-mini & \underline{80.6} & \underline{89.8} & \underline{60.5} & 69.3 \\
        & o3-mini & \textbf{81.4} & \textbf{91.4} & \textbf{63.3} & \textbf{72.1} \\
        \hline

        \hline
    \end{tabular}
    \label{table:lrm}
\end{table}

We compare \our with several reasoning models, including \texttt{QwQ-32B-Preview}~\cite{QWQModel}, \texttt{DeepSeek-R1} \cite{deepseekR1Model}, and \texttt{o3-mini-2025-01-31}\cite{O3miniModel}. Notably, o3-mini demonstrates remarkable raw performance with a 77.2\% F1 score on HotpotQA, surpassing our previous best baseline AFlow (73.5\% F1) in the main experiments, highlighting its strength as a foundation model. When integrated into our framework, even a relatively modest model like gpt-4o-mini achieves an impressive 80.6\% F1 score. Furthermore, employing o3-mini as the backbone of \our leads to exceptional results: the F1 score increases to 81.4\% and the Hit rate reaches 91.4\% on HotpotQA. On the LongBench subset, our framework with o3-mini achieves a 63.3\% F1 score and 72.1\% Hit rate, establishing new state-of-the-art performance across all metrics. Due to the computational constraints and stability considerations, we evaluated on the first 100 examples from the Musique subset of LongBench, which may result in slightly higher scores compared to our main experiments in Table \ref{tab:performance}.

\paragraph{Test-Time Optimization Results.}

\begin{figure}[t!]
\centering
\includegraphics[width=0.45\textwidth]{images/iterations.pdf}
\vspace{-1em}
\caption{Performance scaling with transition times on MATH dataset. Darker blue indicates larger sample sizes at shallower depths, as most problems are solved with fewer decomposition steps.}
\label{fig:iterations}
\end{figure}

We investigate the test-time scaling behavior of \our through two sets of experiments. First, as shown in Figure \ref{fig:iterations}, we analyze the performance scaling of \our on MATH dataset. Unlike the dynamic iteration limit determined by problem-specific graph structures described in Section \ref{sec:preliminary}, here we set a uniform maximum of 5 iterations to explicitly examine the depth-wise scaling behavior. Since each iteration produces an evaluable solution, we can track performance across different iteration depths. All 1000 test samples naturally generate solutions at depth 1, while fewer samples proceed to deeper iterations (dropping to 207 at depth 5), as many problems achieve satisfactory solutions at earlier depths. The results demonstrate that \our exhibits consistent accuracy improvements from 83.2\% to 92.7\% as the iteration depth increases, with the performance gains gradually tapering. This pattern suggests that while deeper iterations continue to benefit overall performance, many problems can be effectively solved with fewer iterations, providing a natural trade-off between computational cost and solution quality.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{images/cost.pdf}
    \caption{Performance comparison on MATH dataset showing computational efficiency. The green line shows FoT scaling with varying tree numbers ($2^k, k=0,1,2,...$), while the gray trend line (representing other baseline methods) together demonstrate the trade-off between performance gains and computational costs. \our($d$=1) combined with FoT($n$=2) achieves slightly better performance to standalone FoT($n$=8) while requiring substantially less computation.}
    \label{fig:cost}
\end{figure}

\noindent
In our second experiment (Figure \ref{fig:cost}), we examine the effectiveness of \our as a plug-in for existing test-time scaling methods. When integrated with FoT, \our demonstrates promising efficiency. This efficiency gain stems from how \our restructures the reasoning process: by iteratively solving sub-problems and using them as known conditions for subsequent steps, it eliminates redundant derivations. This leads to substantially reduced test-time demands in the FoT phase while achieving slightly better performance, demonstrating how our approach can systematically optimize existing test-time scaling methods.

\paragraph{Cost Analysis.}
Through analyzing computational efficiency as shown in Figure \ref{fig:cost}, our \our achieves superior efficiency by reaching competitive performance at significantly lower computational costs compared to existing methods. This enhanced efficiency can be attributed to our atomic state representation that preserves only necessary information while eliminating redundant computations. Notably, \our demonstrates the steepest performance-to-cost ratio among all compared methods, indicating it achieves the highest marginal improvement in accuracy per unit of computational investment.

\paragraph{Ablation Study.}

\begin{table}[t!]
    \centering
    \caption{Ablation Study on \our Components (\%). Removing the decomposition phase causes notable performance drops, while removing the DAG structure but keeping decomposition leads to even larger degradation.}
    \begin{tabular}{lcc}
        \hline
        
        \hline
        Method & MATH & GSM8K \\
        \hline
        \our (Full) & \textbf{83.6} & \textbf{95.0} \\
        \our w/o Decomposition & \underline{82.9} & \underline{94.8} \\
        \our w/o DAG Structure & 82.7 & 94.3 \\
        \hline
        
        \hline
    \end{tabular}
    \label{tab:decompositon}
\end{table}

We conduct ablation studies to analyze the contribution of key components in \our. As shown in Table \ref{tab:decompositon}, removing the decomposition phase (i.e., no extracted independent or dependent sub-problems as guidance) causes notable performance drops, while removing the DAG structure but keeping the decomposition phase (i.e., only extracting the first semantically independent sub-problem as guidance) leads to even larger degradation. Without decomposition structure, the LLM struggles to capture crucial dependencies between subquestions in the contraction phase, resulting in contracted questions that often contain redundant information. Moreover, providing single sub-problem guidance without proper structural information disrupts the parallel relationships between sub-problems. This reveals a critical insight: imperfect structural guidance can be more detrimental than no guidance at all (see Appendix \ref{appendix:illusion} for examples).
