 \begin{figure}[!t]
    \centering
    \includegraphics[width=0.5\textwidth]{images/aot_difference.pdf} 
    \caption{\textbf{Comparison of computational resource allocation in test-time scaling methods.} Traditional test-time scaling methods allocate computational resources partially to process historical information, while \our dedicates all computational resources to reasoning directly related to the current atomic question state.}
    \label{fig:difference}
\end{figure}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{images/aot_pipeline.pdf} 
    \caption{\textbf{The overview of \our}. The left portion illustrates our Markov process where each state $Q_i$ represents an atomic reasoning state derived through DAG decomposition and contraction from its predecessor. The right portion demonstrates \our's integration capability with existing test-time scaling methods (e.g., CoT, ToT). A key feature of this integration is that any intermediate state $Q_i$ from our Markov process can serve as an entry point ($Q_0$) for other methods, enabling flexible composition while maintaining answer equivalence with the original question. This design allows \our to function both as a standalone iterative framework and as a preprocessing module that can enhance existing approaches through structural optimization.}
    \label{fig:pipeline}
\end{figure*}

\section{Introduction}

Large Language Models (LLMs) demonstrate significant scaling effects, with their capabilities showing predictable improvements as model parameters and training data increase, leading to enhanced performance across diverse domains~\cite{Kaplan2020scaling}. While this scaling law faces bottlenecks in high-quality data availability~\cite{Villalobos2024Will}, test-time scaling offers an alternative solution by using more test-time computation to improve performance on diverse tasks~\cite{Snell2024ScalingLT, muennighoff2025s1, hou2025advancing}.

However, existing test-time scaling methods excessively maintain historical information during reasoning, as they rely heavily on complex structural dependencies throughout the reasoning process. Chain-based methods must preserve the entire reasoning history to generate each subsequent step~\cite{Wei2022cot, Zhang2024autocot}, while tree-based approaches require tracking both ancestor and sibling relationships for branch selection~\cite{Yao2023tot, Zhou2024lats, Ding2023xot}. Graph-based structures further compound these challenges through arbitrary node dependencies~\cite{Besta2024got, Zhang2024dot}. As the scale of reasoning increases, the accumulation of historical dependencies not only wastes substantial computational resources but also interferes with the model's ability to reason effectively, as illustrated in Figure~\ref{fig:difference}.


Human reasoning often progresses through solving a sequence of independent subquestions, a fundamental principle established in cognitive science~\cite{simon1962architecture} and problem-solving theory~\cite{polya1945solve}. When solving a complex problem, we naturally identify and resolve self-evident subquestions first, then seamlessly incorporate these solutions to reformulate a simplified problem state, rather than maintaining detailed reasoning processes for resolved components. This progression closely resembles a Markov process~\cite{markov1906extension}, where each state represents a question, and state transitions occur through resolving partial problems to form new, independent questions. 


Inspired by this Markov nature of human reasoning, we propose Atom of Thoughts (\our), a framework that realizes the Markov-style reasoning process. \textbf{Our key insight} is that each reasoning state can be defined as a simplified problem equivalent to the original one, where partial reasoning steps are either transformed into known conditions or excluded as incorrect explorations. This definition is achieved through a two-phase state transition mechanism: first decomposing the current question into a dependency-based directed acyclic graph (DAG) to capture rich structural information, then contracting subquestions into a new independent question. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, ensuring each state transition depends only on the current state while progressively reducing problem complexity.


This design endows \our with two key advantages. First, \our eliminates the need for maintaining and computing historical information when scaling computational resources. Second, these atomic questions can be seamlessly integrated into existing test-time scaling frameworks, allowing \our to function as either a standalone framework or a plug-in enhancement for improving the overall reasoning capabilities.

In summary, our contributions are as follows:
\begin{itemize}
    \item \textbf{Atom of Thoughts.} We introduce \our, a novel reasoning framework with Markov property that progressively decomposes problems into atomic units. This approach significantly reduces computational resources wasted on historical information processing, allowing the model to focus on effective reasoning during test-time scaling.
    \item \textbf{Plug-In Enhancement.} The atomic question derived by \our can be directly integrated into existing test-time scaling methods~\cite{Bi2024fot, Wang2023cotsc}, enhancing both their performance and cost efficiency.
    \item \textbf{Extensive Evaluation.} Experiments across six benchmarks demonstrate the effectiveness of \our both as a standalone framework and as a plug-in enhancement. \our outperforms all baselines, and notably on HotpotQA dataset, enables gpt-4o-mini to surpass reasoning models: o3-mini by \textbf{3.4\%} and DeepSeek-R1 by \textbf{10.6\%}.
\end{itemize}

 