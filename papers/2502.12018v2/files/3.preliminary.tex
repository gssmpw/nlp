\section{An Overview of \our}
\label{sec:preliminary}

This section presents an overview of \our from a probabilistic modeling perspective. We first examine how traditional reasoning chains work and then introduce our dependency-based graph structures and their contraction mechanisms to enhance the modeling capability of reasoning processes. 

\subsection{Reasoning Chain}
Chain-of-Thought (CoT) prompting enables LLMs to progressively propose intermediate thoughts $T_i$ when solving a problem. As discussed earlier, this approach requires maintaining a complete reasoning history, which can be formalized as a probabilistic sampling procedure:
\begin{align}  
A \sim p(A|\mathcal{T}, Q_0) \prod_{i=0}^N p(T_i|\mathcal{T}_{<i}, Q_0)  
\end{align}
Here, $\mathcal{T} = \{T_0, T_1, \dots, T_N\}$ represents the sequence of intermediate thoughts generated by the LLM. Each thought $T_i$ depends on the previous thoughts $\mathcal{T}_{<i}$ and the initial question $Q_0$.

To explore chain-based methods with different node definitions, Least-to-Most~\cite{zhou2023least} replaces the intermediate thoughts $T_i$ with subquestions $Q_i$, resulting in a different formulation of the reasoning chain:
\begin{align}  
A \sim p(A|\mathcal{Q}) \prod_{i=0}^N p(Q_i|\mathcal{Q}_{<i})  
\end{align}  
where $\mathcal{Q} = \{Q_0, Q_1, \dots, Q_N\}$ is the sequence of subquestions.

In an ideal scenario where the reasoning chain $\mathcal{Q}$ exhibits the Markov property, each subquestion $Q_{i+1}$ would only depend on its immediate predecessor $Q_i$, similar to how humans naturally solve complex problems by resolving independent subquestions and reformulating simplified states. This leads to:
\begin{align}
A \sim p(A|Q_N) \prod_{i=0}^N p(Q_{i+1}|Q_i)
\end{align}
However, achieving true Markov property in real-world reasoning tasks is challenging. We adopt the subquestion-based node structure from reasoning chains as states while exploring a two-phase state transition mechanism consisting of decomposition and contraction to address this challenge.

\subsection{Dependency Directed Acyclic Graph}
\our utilizes temporary DAG structures to decompose the current question, unlike existing methods that maintain complex dependencies throughout the reasoning process. This DAG structure serves as a scaffold during state transitions, providing rich structural information to guide the complete state transition process, specifically functioning as the decomposition phase to facilitate the subsequent contraction phase.

The DAG $\mathcal{G}$ is defined as:

\begin{align}
\mathcal{G} = (\mathcal{Q}, E), \quad \mathcal{Q} = \{Q_i\}_{i=1}^n , \quad E \subseteq \mathcal{Q} \times \mathcal{Q} 
\end{align}

In our DAG definition, nodes represent subquestions $Q_i$, and edges $(Q_j, Q_i)$ indicate that $Q_j$ contains necessary information for solving $Q_i$. A major challenge in constructing Markov processes stems from the dependencies of various information in complex reasoning scenarios, and this definition provides structural information for identifying dependencies through rule-based determination.

Based on their dependency relationships, all subquestion nodes can be categorized into two types:

Independent subquestions $\mathcal{Q}_{\text{ind}}$ (nodes without incoming edges):

\begin{align}
\mathcal{Q}_{\text{ind}} = \{Q_i \in \mathcal{Q} \mid \nexists Q_j \in \mathcal{Q}, (Q_j, Q_i) \in E\}
\end{align}

Dependent subquestions $\mathcal{Q}_{\text{dep}}$ (nodes with incoming edges):

\begin{align}
\mathcal{Q}_{\text{dep}} = \{Q_i \in \mathcal{Q} \mid \exists Q_j \in \mathcal{Q}, (Q_j, Q_i) \in E\}
\end{align}

The key assumption of acyclicity in our DAG is guaranteed by this edge definition: since subquestions are generated following natural language order, any subquestion $Q_i$ can only depend on previously generated subquestions $Q_{<i}$. Even in the maximally connected case where each subquestion links to all its predecessors, acyclicity is maintained, as any additional edges would create cycles by connecting to future nodes while violating the natural language order.

\subsection{Contraction}
The contraction phase transforms the temporary DAG structure into the next atomic state while preserving the Markov property. To ensure this Markov process is meaningful, we must maintain state atomicity while ensuring progress in the reasoning process. As the reasoning progresses, new conclusions and information are continuously derived, necessitating the selective discarding of information to maintain atomic states. \our addresses this by treating results from $\mathcal{Q}_{\text{ind}}$ as either given conditions or eliminated process information, while contracting $\mathcal{Q}_{\text{dep}}$ into an independent question as the next state. This contracted question maintains solution equivalence to $Q_i$, ensuring the reasoning process stays on track.

The reasoning process is formally described in Algorithm \ref{algo:main}, which shows how \our iterates through decomposition and contraction steps. This iterative process continues until it reaches a maximum number $D$, which is assigned by the depth of the first generated graph $\mathcal{G}_0$ to prevent infinite decomposition. The process can be formalized as:

\begin{align}  
A \sim p(A|Q_D) \prod_{i=0}^D p(Q_{i+1}|\mathcal{G}_i)\  p(\mathcal{G}_i|Q_i)  
\end{align}


\begin{algorithm}[t]
\caption{Algorithm of \our}
\begin{algorithmic}[1]
\label{algo:main}
\small
\REQUIRE Initial question $Q_0$
\ENSURE Final answer $A$
\STATE Iteration counter $i \gets 0$
\STATE max depth $D \gets \text{None}$
\WHILE{$i < D$ or $D$ is None}
    \STATE $\mathcal{G}_i \gets \texttt{decompose}_{\text{LLM}}(Q_i)$ \\ // Generate dependency DAG
    \IF{$D$ is None}
        \STATE $D \gets \texttt{GetMaxPathLength}(\mathcal{G}_i)$ \\ // Rule-based path length calculation
    \ENDIF
    \STATE $\mathcal{Q}_{ind} \gets \{Q_i \in \mathcal{Q} \mid \nexists Q_j \in \mathcal{Q}, (Q_j, Q_i) \in E\}$
    \STATE $\mathcal{Q}_{dep} \gets \{Q_i \in \mathcal{Q} \mid \exists Q_j \in \mathcal{Q}, (Q_j, Q_i) \in E\}$
    \STATE $Q_{i+1} \gets \texttt{contract}_{\text{LLM}}(\mathcal{Q}_{ind}, \mathcal{Q}_{dep})$ \\ // Contract subquestions into a independent question
    \STATE $i \gets i + 1$
\ENDWHILE
\STATE $A \gets \texttt{solve}_{\text{LLM}}(Q_D)$ \\ // Generate final answer
\RETURN $A$
\end{algorithmic}
\end{algorithm}
