\section{Simulation Results}
\label{sec:sim}

We conduct simulation experiments across two different vision-based tasks to assess the performance of our latent safety framework. 
These experiments are designed to support the claim that latent safety filters recover performant safety-preserving policies from partial observations alone (i.e., without assuming access to ground-truth dynamics, states, or constraints), for progressively more complex safety specifications and dynamical systems. 


\subsection{How Close Does Latent Safety Get to Privileged Safety?}
We start by studying a canonical safe-control benchmark: collision-avoidance of a static obstacle with a vehicle.
Although this particular setting does not ``require'' latent-space generalizations of safety, its low-dimensionality and well-studied unsafe set allow us to 
% Because this is a standard, low-dimensional benchmark, we can 
rigorously compare the quality of the safety filter to an exact grid-based solution and 
%We first test our latent safety filter for a 3D Dubin's car, a standard benchmark for safety-critical control.
% We compare our latent safety filter against 
a privileged-state RL-based safety filter.

%The dynamics of the system are
%\begin{itemize}
%    \item[] $x^+ = x + \cos(\theta) \Delta t$
%    \item[] $y^+ = y + \sin(\theta) \Delta t$
%    \item[] $\theta^+ = a \Delta t$
%\end{itemize}

%\abnote{Our first question is how accurately can latent safety filters approximate the prividleged state one? We ground our example in a 3D Dubins' car because this is one of *the* canonical examples for low-dimensional safe control that (1) has a non-trivial solution (i.e., the shape of the Dubins' BRT is kind of interesting) and (2) we have an exact solution for with priviledged state. }

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figs/latent-vs-priv-brts.pdf}
    \caption{\textbf{Latent Safety vs. Privileged Safety.} Dubins' car collision-avoidance qualitative results. Dashed lines indicate the ground-truth set boundary. We visualize each method's failure specification and corresponding unsafe set, shown at heading slices $\theta \in \{ 0, \pi/2 \}$. While \privsafe uses the ground-truth $\state$ and $\failure_\textrm{gt}$, \ours uses the latent state from encoding the observation, $\latent = \enc_\encparam(\obs)$, and the inferred failure set $\failureLatent$. Insets on the bottom row show the observations corresponding to select privileged states $\state_1, \state_2, \state_3.$ }
    \label{fig:dubins-results}
\end{figure}

\para{Dynamical System \& Safety Specification} %\abnote{Should we pull this one part out separately throughout all the sections to make it super clear what we mean by safety? e.g., we could say smth like} 
In this experiment, the ground-truth dynamics are that of a discrete-time 3D Dubin's car with state $\state = [p^x,p^y,\theta]$ evolving as
$$\state_{t+1} = f(\state_t, \action_t) 
% [p^x_{t+1},p^y_{t+1},\theta_{t+1}] 
= \state_t + \Delta t [v \cos(\theta_t), v \sin(\theta_t), \action_t],$$ 
where the robot's action  $\action$ controls angular velocity while the longitudinal velocity is fixed at $v = \SI{1}{\meter\per\second}$, and the time-discretization is $\Delta t = \SI{0.05}{\second}$.
% as the Dubin's car colliding with a circular obstacle with radius $0.5$ centered at the origin. 
The action space for the robot is discrete and consists of $\mathcal{A} = \{-a_\mathrm{max}, 0, a_\mathrm{max} \}$ where $a_\mathrm{max} = 1.25~\mathrm{rad/s}$.  
To remain safe, the robot must avoid an obstacle of radius $r = \SI{0.5}{\meter}$ centered at the origin (see red circle in the bottom row of Figure~\ref{fig:dubins-results}).
Hence, the ground-truth failure set is a cylinder in state space, $\failure_\textrm{gt} := \{ \state : \forall \theta, \; |p^x|^2 + |p^y|^2 < r^2\}$.
The ground-truth margin function $\marginfunc_\textrm{gt}(\state) = |p^x|^2 + |p^y|^2 - 0.5^2$ captures the failure condition via its zero-sublevel set.  

\para{Baseline: Privileged Safety} The \privsafe baseline computes the HJ value function using the \emph{ground-truth} state, dynamics, and margin function, using a reinforcement learning (RL)-based solver.
Specifically, we approximate solutions to the discounted Bellman equation~(\ref{eqn:discounted-safety-bellman}) in the framework of~\cite{hsu2021safety} via DDQN~\cite{van2016deep} with Q-functions parameterized by a 3-layer MLP with 100 hidden units.

\para{Latent Safety Filter Setup}
%In this setting, the data for training the world model has near-perfect coverage of the environment.  
The \ours method does not get privileged access to ground-truth information but instead learns all model components from data.
Specifically, we train a world model from an offline dataset $\mathcal{D}_\mathrm{WM} = \{\{(o_t, a_t)\}_{t=0}^T\}^N_{i=1}$ of $N=2,000$ observation-action trajectories. The observations $\obs_t := (\img_t, \theta_t)$ consist of (128x128) RGB images $\img$ and the robot heading $\theta_t \in [-\pi, \pi]$. 
During trajectory generation, we uniformly sampled random actions. 
Each trajectory terminates after $T=100$ timesteps or if the ground-truth $x$ or $y$ coordinate left the environment bounds of $[\SI{-1}{\meter}, \SI{1}{\meter}]$. 
We trained the world model on this offline dataset using the default hyperparameters of~\cite{dreamerv3-torch}.
We used the privileged state to automatically label each observation $o_t \in \mathcal{D}_\mathrm{WM}$ as violating a constraint or not, constructing the datasets $\mathcal{D}_{\text{safe}}$ and $\mathcal{D}_{\text{unsafe}}$ for classifier training in Eq.~\ref{eq:failure-classifier-loss}. 
For solving for the HJ value function, we use the same toolbox and hyperparameters as the privileged baseline. 


One important implementation detail is dealing with the reset mechanism of the world model. Naively initializing the latent state may result in a latent state that does not correspond to any observation seen by the world model. 
In practice, we reset the world model by encoding a random observation from our offline dataset and only collect rollouts in the latent imagination for $T=25$ steps to prevent drifting too far out-of-distribution.
For \ours, we parameterize the safety classifier $\marginfunc_\ellparam(\latent)$ by a 2-layer MLP with 16 hidden units and co-train it with the world model.
The zero-sublevel set of $\marginfunc_\ellparam(\latent)$ captures the learned failure set, $\failureLatent := \{ \latent \mid \marginfunc_\ellparam(\latent) < 0\}$.



%\abnote{describe how the latent space gets a history of observations instead of the perfect state, blablabla, ...}

%\para{Failure Classifier} 

%\para{Latent World Model}

%\para{Latent Safety Filter Computation}
%\abnote{describe how we approximate it via RARL, neural network is bla, run for N epochs, lr = X, etc.}
%\abnote{state the assumptions under which we get this: perfect coverage of the world, so our world model is nearly perfect}




\para{Results: On the Quality of the Runtime Monitor, $\valfuncLatent$}
Since the ground-truth dynamics are known and its state-space is low-dimensional, we can solve for the safety value function \emph{exactly} using traditional grid-based methods \cite{mitchell2004toolbox}.
This allows us to report the accuracy of the safe/unsafe classification of the safety filter's monitor ($\valfunc$) in Table \ref{tab:filter_table} for both the privileged state value function $\valfuncPriv$ and latent state value function $\valfuncLatent$ based on their alignment (in terms of sign) with the ground-truth value function $\valfuncGt$.
Since both safety filters use the same toolbox for learning the value function, any degradation in the latent safety filter can be attributed to the quality of the learned world model.
We also qualitatively visualize the zero-sublevel set of the value function for both methods at different fixed values of $\theta$ in Figure~\ref{fig:dubins-results}. 
In summary, we find that the \emph{image-based} runtime monitor learned by our method (\ours) closely matches the accuracy of the \emph{true-state-based} of the privileged baseline (\privsafe).

% \renewcommand{\arraystretch}{1.2}
\begin{table}[h!]
\centering
\setlength{\tabcolsep}{2.5pt} % Adjust column spacing
\begin{tabular}{l|cccc|c}
\toprule
Method & True Safe & False Safe & False Unsafe & True Unsafe & $F_1$-score \\ 
\midrule
\privsafe & 0.769 & 0.013 & 0.011 & 0.206 & 0.984 \\ 
\ours     & 0.758 & 0.005 & 0.022 & 0.215 & 0.982 \\ 
\bottomrule
\end{tabular}
\caption{\textbf{Quality of the Runtime Monitor.} Performance of latent (\ours) and privileged (\privsafe) safety value functions. Note that this is computed over all three dimensions of the Dubins' car state.}
\label{tab:filter_table}
\end{table}

%%%%%%%%%

\para{Results: On the Quality of the Safety Policy, $\fallbackLatent$}
Each runtime monitor $\valfunc$ induces a corresponding safety policy: $\fallbackLatent$ for \ours and $\fallbackPriv$ for \privsafe.
To evaluate the quality of these safety policies, we check if they are capable of steering the robot away from failure. To determine initial conditions from which steering away from failure is feasible, we compute a state-based ground-truth value function, $\valfuncGt$, whose zero-sublevel set gives us a dense grid of 250 initial states for which the exact safe controller $\fallbackGt$ can guarantee safety.
For all of these states, we simulate each policy executing its best effort to keep the robot outside the privileged failure set, $\failure_\state$.
We find that our \emph{image-based} safety policy closely matches the performance of the \emph{privileged} baseline: \ours maintains safety for 240/250 (96\%) states and \privsafe maintains safety for 244/250 (97.6\%) states.

\begin{figure}[h!t]
    \centering
    \includegraphics[width=1\linewidth]{figs/latent-brts-reduced-actions.pdf}
    \caption{\textbf{Ablation: Latent Safety with Incomplete WM.}
    Unsafe set approximated by \ours using the latent space of a \emph{biased} world-model built from incomplete action coverage $\tilde{\actionSpace}=\{0, \action_\textrm{max}\}\subset \actionSpace$.
    }
    \label{fig:dubins-ablation}
\end{figure}

\para{Ablation: Effect of Incomplete Knowledge of the World}
Thus far, we have had strong coverage of all observation-action pairs when training the world model; however, complete knowledge of the world may not be achievable in reality.
% real-world robot deployment scenarios, it is unlikely to get complete coverage of all state-action pairs. 
% To investigate the effects of incomplete data coverage on the world model, 
To study this, we train our latent safety filter on top of a world model that has seen a biased dataset, wherein the robot's action space is limited to only moving straight or turning left: $\tilde{\actionSpace} = \{0, \action_\mathrm{max} \} \subset \actionSpace$.
Figure~\ref{fig:dubins-ablation} shows that the bias of the world model affects the robot's understanding of safety: since the world model did not learn about the possibility of turning right, \ours pessimistically classifies states as unsafe if they require a right turn to avoid collision. %\knnote{should we make a comment about this overconservatism not being true in general?}




\subsection{Can Latent Safety Scale to Visual Manipulation?}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figs/brt_retract_manip.pdf} %{figs/brt_retract.pdf}
    \caption{
    \textbf{Visual Manipulation: Simulation.} \textit{Top row}: Robot's observations corresponding to a known \textit{unsafe} action sequence. \textit{Middle row}: Our learned failure classifier correctly identifies only the final observations at $t=28$ as being in the failure state since the red blocks have fallen all the way over. \textit{Bottom row}: Our unsafe set (obtained via the latent-space HJ value function) correctly identifies that the robot is doomed to fail the moment that the two red blocks begin to tip over at time $t=14$. %This shows that the reachability analysis allows us to safely account for the system's dynamics.
    }
    \label{fig:sim-manip}
\end{figure*}

%Scaling Latent Safety to Visual Manipulation}
% idk what the best title is for the IsaacGym results

In our simulated manipulation setting, we adapt a contact-rich manipulation task from \cite{thananjeyan2021recovery} where a robot is tasked with grasping and lifting a green block that is placed closely between two red blocks (see Figure \ref{fig:sim-manip}).
In this setting, we generalize the safety representation to more nuanced failures, such as the red blocks falling down from aggressive interaction. 
We train a task policy for this setting that accounts for safety only via a soft constraint and compare the unfiltered behavior against two methods for safety filtering: a constrained MDP (CMDP) baseline and our latent safety filter.
%We use this environment as a testbed for deploying our method on a system with complex dynamics and a constraint that is not collision. 

%\abnote{now the safety property is a harder to specify by hand. \textbf{Stress that for this task, collision-based representation is overly conservative!} We want it to be ok if the manipulator touches or even perturbs the red blocks as long as they never fall over. In other words, the failure set only includes the red blocks being fully knocked on the ground. This is easy to "know when you see" but harder to codify. However, these complicated multi-body interaction dynamics are harder to reason about, and we are also using a manipulator. So, latent safety can help us with both specification and synthesis.}

\para{Safety Specification} 
We treat a state as a failure if either of the two red blocks is knocked down.
We categorize a block as having fallen if it is angled within \SI{1}{\radian} (measured using privileged simulator information not seen by any of the methods) of the ground plane. 
Crucially, this safety specification is \textit{not} a collision avoidance specification: the robot is allowed to touch, push, and tilt the red blocks in order to grasp the green block, as long as the red blocks do not topple over.
%\abnote{describe what we mean by safety here is that the red blocks are fallen down; Stress how this isi \textit{not} a collision-avoidance problem; this safety representation allows the robot to touch the red objects and perturb them as long as they dont fall.}

\para{Experimental Setup} 
Our nominal task policy $\policyTask$ is obtained via DreamerV3 \cite{hafner2024masteringdiversedomainsworld} trained using a dense reward for lifting the block and a sparse cost for violating constraints ($\dreamer$). 
The observation space $\obsSpace$ of the robot is given by two $3 \times 128 \times 128$ RGB camera views (table view and wrist-mounted) along with 8-dimensional proprioception (7-dimensional joint angle and gripper state) information. 
We co-train the $\dreamer$ world model with our failure classifier $\marginfunc_\ellparam(\latent)$ for 100k iterations and reuse the world model with frozen weights for our method and all other baselines. The failure classifier is implemented as a 3-layer MLP with ReLU activations. During training, we sampled batches consisting of both rollouts collected by the Dreamer policy (90$\%$ of each batch) and a dataset of 200 teleoperated demonstrations (10$\%$ of each batch) comprising both safe and unsafe behavior. 

We also compare \ours to a constrained MDP safety policy, \cmdp \cite{srinivasan2020learning, thananjeyan2021recovery}. 
This method leverages the same world model and failure classifier as \ours, but optimizes a different loss function to obtain the safety critic, $Q^\textrm{risk}$ \cite{srinivasan2020learning}:
\begin{equation}
\begin{aligned}
    \mathcal{L}(\theta) = &\mathbb{E}_{(\latent_t, \action_t, \latent_{t+1}, \action_{t+1
    }) \sim \rho_{\riskLatent}} \bigg[ \frac{1}{2}(Q^\textrm{risk}(\latent_t, \action_t) \\&- (c_t + (1-c_t))\gamma Q^\textrm{risk}(\latent_{t+1},\action_{t+1})))^2\bigg]
\end{aligned}
\end{equation}
where $\rho_{\riskLatent}$ is the state-action distribution induced by policy $\riskLatent$ and $c_t \in \{0, 1\}$ takes value $1$ if a constraint is violated at timestep $t$. 
The resulting Q-value can be interpreted as the empirical risk of violating a constraint by taking action $\action_t$ from $\latent_t$ and following policy $\riskLatent$ thereafter. 
To mimic the role of our safety filter that is agnostic to any task-driven base policy, we train the \cmdp critic with trajectories sampled from $\riskLatent(\latent) := \arg\min_{\action \in \actionSpace} Q^\textrm{risk}(\latent, \action)$ and no additional task-relevant information. 
We can then use this policy to filter any actions whose risk is higher than some threshold, $\epsilon^\textrm{risk}$. The actor and critic for both \cmdp and \ours are trained in the latent dynamics of the RSSM using DDPG~\cite{lillicrap2019continuouscontroldeepreinforcement, li2025certifiabledeeplearningreachability}. 
We again reset the latent state of the world model by encoding an observation of previously collected data. All training hyperparameters are included in the Appendix. 

During deployment, we use the actor head of $\dreamer$ to be the nominal policy, $\policyTask(\latent)$. 
To get a performant nominal policy, we tuned the \dreamer reward function by sweeping over the reward weights for the components consisting of reaching the green block, lifting the green block, action regularization, and a penalty for the red blocks falling.
%This safety critic has been used in \cite{thananjeyan2021recovery} to evaluate the risk of a task-oriented policy $\bar{\pi} = \pi^{task}$. %However in principle, once this critic is trained it can be used to shield any policy \cite{srinivasan2020learning}. 




\para{Safety Filtering} 
We instantiate our two safety filters, \ours (comprised of $\fallbackLatent(\latent)$ and $\valfuncLatent(\latent)$) and \cmdp (comprised of $\riskLatent(\latent)$ and $\valfuncRisk(\latent)$), to shield the base \dreamer policy. 
% For both safety filters, we use safe control policy $\fallbackLatent$ associated with the value function learned by method along with the \dreamer world model to perform safe control.
During each timestep $t$, we query a candidate action $\action_t$ from \dreamer that we seek to filter. 
We instantiate a modified version of the minimally-invasive safety filtering scheme described at the end of Section~\ref{sec:hj-bg}. We take the action in the world model to obtain latent state $\bar{\latent}_t$ and evaluate this latent state to obtain $\bar{\valfunc}_t$.
%To add an additional layer of robustness to our filter, we repeat the queried action $\action_t$ from the base policy $H=3$ times in the world model's imagination, evaluate the value function for each predicted latent state, and then consider only the worst-case value for our safety filter. 
This value $\bar{\valfunc}_{t}$ will serve as our monitoring signal for whether we are safe or if we should start applying our safety policy.
% for used by \ours (or \cmdp) to evaluate the safety of $\action_t$. 
For \ours, the filtered (and thus executed) action $\action^\text{exec}_{t}$ follows the filtering law: 
\begin{align}
   \action^\text{exec}_{t} = 
    \begin{cases}
    \policyTask(\latent_t) ,& \text{if } \bar{V}_t > 0.4, \\
    \fallbackLatent(\latent_t),              & \text{otherwise.}  \\ 
    \end{cases}
\end{align}
This is a least-restrictive filter that executes the safe control policy $\fallbackLatent$ whenever $\bar{V}_t \leq 0.4$. 
The \cmdp baseline follows a similar filtering control law defined by:
\begin{align}
   \action^\text{exec}_{t} = 
    \begin{cases}
    \policyTask(\latent_t) ,& \text{if } \bar{V}^{\textrm{risk}}_t < \epsilon_{\textrm{risk}}, \;\; \\
    \riskLatent(\latent_t),              & \text{otherwise.}  \\ 
    \end{cases}
\end{align}
where $\epsilon_{\textrm{risk}}$ is a manually tuned risk threshold that we ablate in our experiments to be $\epsilon_{\textrm{risk}} \in \{0.1, 0.05\}$.

%and apply safety override $\pi^\shield(\latent_t)$ if the value exceeds some threshold (\valfun(\latent) > $\epsilon_\testrm{risk}$ for \cmdp, $\valfunc(\latent) < \epsilon_\textrm{safe}$ for \ours). An illustration of the value function for \ours is shown in Figure \ref{fig:sim-manip}. Furthermore, to reduce jitter from switching between the safe control policy and nominal \dreamer policy, we scale the outputs of \dreamer by $50\%$ after the first timestep where the safe control policy was activated if the value is above the safe threshold $\epsilon$ (below for $\epsilon^\textrm{risk}$.) This can be interpreted as the nominal policy acting more cautiously after nearly violating the safety constraint.

\para{Results: Qualitative} 
First, we qualitatively studied if there was a difference between the failure set, $\failureLatent$, learned by our classifier and the unsafe set, $\unsafeSetLatent$, recovered by learning the HJ value function in this visual manipulation task. 
If $\unsafeSetLatent \supset \failureLatent$, then we have identified a non-trivial unsafe set for this high-dimensional problem. 
In Figure \ref{fig:sim-manip}, we show the observations $\obs_{0:T}$ corresponding to a known \textit{unsafe} action sequence, $\action_{0:T}$, where $T = 28$ steps. 
When the robot executes this action sequence, half-way through the robot touches the red blocks with high enough force that they end up falling over. 
We pass the observation trajectory into world model encoder to obtain a corresponding posterior latent state trajectory $\latent_{0:T}$. 
We evaluate $\text{sign}[\marginfunc_\ellparam(\latent_t)]$ and $\text{sign}[\valfuncLatent(\latent_t)], \forall t \in \{0,\hdots,T\}$ to check which latent states are in the failure set and unsafe set respectively. The two rows in Figure \ref{fig:sim-manip} correspond to each model's classification. We see that $\failureLatent$ correctly identifies that only the final observation at $t=28$ is in failure, since this is the only observation where the red blocks have fully fallen down. 
However, $\unsafeSetLatent$ detects that at timestep $t=14$, the robot has perturbed the blocks in such a way, that they are doomed to fail. 


\para{Results: Quantitative} We rollout the un-shielded base $\dreamer$ policy and the policy shielded via $\ours$ and $\cmdp$ for 50 initial conditions of the blocks randomly initialized in front of the robot within $\pm \SI{0.05}{\meter}$ in the x and y directions.
We report the success, constraint violation, and incompletion rates in Table \ref{tab:sim_comparison}. 
We define a constraint violation as any rollout where at least one of the red blocks fall, success as any rollout where the robot successfully picked up the green block without toppling a red block, and incompletion as any rollout that does not violate constraints but failing to picking up the green block.
%
\begin{table}[h!]
\centering
\setlength{\tabcolsep}{5pt} % Adjust column spacing
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \multirow{2}{*}{\textbf{\shortstack{Safe Success  \\  $\%$ ($\uparrow$)}}}& \multirow{2}{*}{\textbf{\shortstack{Constraint\\ Violation
$\%$ ($\downarrow$)}}}  & \multirow{2}{*}{\textbf{\shortstack{Incompletion  \\    $\%$ ($\downarrow$)}}} \\
& & \\ % Empty row to align the header
\midrule
\dreamer %($\policyTask$) 
&   64    &  36 & \textbf{0}   \\
\cmdp (%$\riskLatent,  
$\epsilon_{\textrm{risk}} = 0.1$) &  68 &   28   & 4 \\
\cmdp (%\riskLatent, 
$\epsilon_{\textrm{risk}} = 0.05$) &   8   &  22   & 70\\
\ours %($\fallbackLatent$)  
& \textbf{80}  &   \textbf{20} & \textbf{0}    \\
\bottomrule
\end{tabular}
\caption{\textbf{Visual Manipulation: Simulation.} Success at the task without any safety violations, constraint violations, and incompletion rates across 50 rollouts corresponding to 50 random initial conditions of the blocks. Task success is picking up the green block; constraint violation is where either of the red blocks fall down on the table.}
\label{tab:sim_comparison}
\end{table}
%



Despite the penalty for knocking over obstacles, we found that \dreamer learned to lift the block even when doing so would incur a safety violation. Although further tuning the reward function could potentially improve the behavior of the robot, reward engineering is notoriously tedious for engineers. This motivates using a safety filter to improve the safety of an unsafe task policy. We report the performance of \cmdp for two different values of $\epsilon^\textrm{risk}$ and found that \cmdp is extremely sensitive to choice of while $\epsilon^\textrm{risk}$, growing the task incompletion rate from $4\%$ when $\epsilon^\textrm{risk} = 0.1$ to $70\%$ when $\epsilon^\textrm{risk} = 0.05$ while only marginally improving safety. In contrast, \ours overrides our nominal task policy only when needed, significantly reducing the number of constraint violations while still succeeding at the task.











% ========== HARDWARE ========== %
\section{Hardware Results: \\ Preventing Hard-to-Model Robot Failures}
\label{sec:hardware}
Finally, we design a set of experiments in hardware to see if our latent safety filter can be applied in the real world (shown in Figure~\ref{fig:front-fig}). 
We use a fixed-base Franka Research 3 manipulator equipped with a 3D printed gripper from~\citep{chi2024universal}. 
The robot is tasked with interacting with an opened bag of Skittles on the table. 
We test the efficacy of our safety filter in ensuring a Diffusion Policy \cite{chi2024diffusionpolicy} can safely pick up the bag of Skittles without spilling and stress-test our policy-agnostic filter by qualitatively evaluating its capability to safeguard a teleoperator.


\para{Safety Specification} Our safety specification is to prevent the contents of the Skittles bag from falling out of the bag. 
Given only image observations and proprioception, this problem is clearly partially observed since the robot cannot directly recover the position of the Skittles in the bag.
Even if privileged state information was available, designing a function to characterize the set of failure states and a dynamics model for interactions between all relevant objects (e.g., the manipulator, the soft bag, the Skittles, and the table) would be extremely difficult. 

\para{Latent Safety Filter Setup} We use DINO-WM \cite{zhou2024dinowm}, a Vision Transformer-based world model that uses DinoV2 as an encoder \cite{oquab2023dinov2}. 
The manipulator uses a 3rd person camera and a wrist-mounted camera and records $3 \times 256 \times 256$ RGB images at 15 Hz.
For world model training, we collected a dataset  $\mathcal{D}_\text{WM}$ of 1,300 offline trajectories: 1,000 of the trajectories are generated sampling random actions drawn from a Gaussian distribution at each time step, 150 trajectories are demonstrations where the bag is grasped without spilling any Skittles, and 150 demonstrations pick up the bag while spilling candy on the table. 
We manually labeled the observations in the trajectory dataset for apparent failures. %\knnote{for appendix: details of the multi-class labeling.} 
However, in principle, we believe that this data annotation process could also be automated using alternative methods, like state-of-the-art foundation models (e.g., vision-language models). 

Our world model is trained by first preprocessing and encoding the two camera view using DINOv2 to obtain a set of dense patch tokens for each image. 
We use the DINOv2 ViT-S, the smallest DINOv2 model with 14M parameters, resulting in latent states $\latent$ of size $256 \times 384$ corresponding to 256 image patches each with embedding dimension 384. 
The transition function is implemented as a vision transformer, which takes as input the past $H=3$ patch tokens, proprioception, and actions to predict the latent. The transformer employs frame-level causal attention to ensure that predictions can only depend on previous observations. 
The model is trained via teacher-forcing minimizing mean-squared error between the ground-truth DINO embeddings of observations and proprioception information from $\mathcal{D}_\text{WM}$ and the embeddings and proprioception predicted by the model. 
Additional details on the model and hyperparameters are included in the appendix and in \cite{zhou2024dinowm}. 
After world model training, we separately learn the failure classifier (implemented as a 2-layer MLP with hidden dimension 788 and ReLU activations) on the DINO patch tokens corresponding to the manually labeled constraint-violating observations. 
For approximating the HJ value function, we use DDPG \cite{lillicrap2019continuouscontroldeepreinforcement, li2025certifiabledeeplearningreachability}. 

%\abnote{describe data for failure classifier training, data for WM, and latent reachability computation. These could be pulled apart into separate subsections too...}. 



\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/spill_dist.png}
    \caption{\textbf{Hardware Results.} %Empirical distribution of spill severity of shielded \ours vs. the base \diffpolicy. 
    Percent of bag spilled ($p$) vs number of runs that spilled at least $p\%$ of the bag.
    While \diffpolicy frequently spills a large percentage of the bag ($\sim 85\%$), \ours spills less than $5\%$ of the bag in all but one of the constraint-violating rollouts. }
    \label{fig:hardware-hist}
    \vspace{-0.5cm}
\end{figure}



\subsection{Shielding Generative Imitation-Learned Policies}
\label{subsec:diffusion}

\para{Methods} For our base task policy, $\policyTask$, we use a generative imitation-based policy \diffpolicy \cite{chi2024diffusionpolicy}. 
For training, we collect 100 high-quality teleoperated trajectories wherein the expert grasps the Skittles bag and lifts it off the table. 
During training, the Skittles bag was closed so no Skittles could fall out. The DP is a stand-in for a policy that only knew about the task (picking the bag) at training time but not about all possible failure modes. 
We compare the performance of the base un-shielded \diffpolicy to the policy shielded with our latent safety filter in a deployment scenario where the bag of Skittles is opened. The safety filter operates according to the following control law:
\begin{align}
    \action^\text{exec}_{t} = 
    \begin{cases}
    \policyTask(\latent_t) ,& \text{if } \valfunc(\bar{\latent}_{t+1}) > 0.3\\
    \fallbackLatent(\latent_t),              & \textrm{otherwise}  \\ 
    \end{cases}
\end{align}
where $\bar{\latent}_{t+1} \sim p_\dynparam(\bar{\latent}_{t+1} \mid \latent_t, \action_t)$ is a one-step rollout of the world model using the action proposed by \diffpolicy.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figs/teleop_val.pdf}
    \caption{
    \textit{Far Left}: Without a safety filter, a teleoperator lifts the closed-end of the bag too quickly and spills the Skittles. \textit{Middle Left}: By using \ours, the same action of lifting the closed-end leads to the value function $\valfuncLatent^\shield$ dipping below the safe threshold (orange) and prompting the safety policy to override the teleoperator (green); the robot does not allow the human pull the bag up sharply. 
    \textit{Middle Right}: At the same time, \ours slows down the human's attempt to move the bag side-to-side while grasping the closed end, indicating that the safety filter has a nuanced understanding of which actions will and won't violate safety.
    \textit{Right}: Grasping the bag from the open end and lifting is deemed safe and is allowed by \ours.}
    \label{fig:teleop-results}
\end{figure*}

\para{Results: Quantitative \& Qualitative} 
We measure the frequency of constraint violations (if even one Skittle falls out during an episode) and spill severity (percentange of the Skittles spilled). We deploy both \diffpolicy and \ours 15 times on the real robot. 
While \ours suffers from failure $26.4\%$ of the time, we note that 3 out of the 4 failures only spilled a single skittle. In contrast, \diffpolicy's spill rate was $73.4\%$ with many instances where a large percentage of the bag was spilled.
%For example, even if a single Skittle falls out of the bag, the aggregate spill rate marks this rollout as a failure. 
To better understand the \textit{severity} of the constraint violations, we report in Figure~\ref{fig:hardware-hist} how often each method spilled more than $p\%$ of the bag.
While \diffpolicy frequently spills a large percentage of the bag ($\sim 85\%$), the safety filter spills less than $5\%$ of the bag in all but one of the constraint-violating rollouts, where it spilled only $12\%$
. 


%\begin{table}[h!]
%\centering
%\begin{tabular}{lcc}
%\toprule
%\textbf{Method} & \textbf{Constraint Violation Rate (\%)} & \textbf{\% Bag Spilled} \\
%\midrule
%\diffpolicy &   $73.3$    &   $52.8 \pm 38.85$ \\
%\ours      &   $26.6$    &    $2.0 \pm 6.14$ \\
%\bottomrule
%\end{tabular}
%\caption{Comparison of constraint violation rate and percentage spilled when using \ours to shield \diffpolicy.}
%\label{tab:comparison}
%\end{table}



% \para{Results: Latent Safety Filters Are Policy-Agnostic} 
% \abnote{gen. across nominal policies — shielding a teleoperator, diffusion policy. This shows OOD action distributions! q}


\subsection{Shielding Human Teleoperators}


To emphasize the policy-agnostic nature of our latent safety filters, we demonstrate filtering the actions of a teleoperator.

\para{Setup} The teleoperator controls the end-effector position and gripper state via a Meta Quest pro similar to \cite{khazatsky2024droid}, and can freely move the robot around. They are tasked with interacting with the Skittles bag however they like. We use exactly the same  \textit{Latent Safety Filter} as we used to shield the \diffpolicy from Section~\ref{subsec:diffusion}. Both the teleoperation and safety filtering were executed at $15$ Hz.

\para{Results: Shielding Unsafe Grasps and Dynamic Motions} 
We visualize our qualitative results in Figure \ref{fig:teleop-results}. 
Un-shielded by our safety filter, the teleoperator can grab the opened bag of Skittles by the base and pull up sharply, spilling its contents on the table (left, Figure~\ref{fig:teleop-results}). 
By using \ours, the same behavior gets automatically overridden by the safety filter, preventing the teleoperators ``pull up'' motion from being executed and keeping the Skittles inside (center, Figure~\ref{fig:teleop-results}). 
At the same time, the latent safety filter is not overly pessimistic (right-most images in Figure~\ref{fig:teleop-results}). 
When the teleoperator moves the Skittles bag side-to-side while grasping the \textit{bottom} of the opened bag, the safety filter accurately accounts for these dynamics and minimally modifies the teleoperator to slow them down, preventing any Skittles from falling out while still allowing the general motion to be executed. 
When the teleoperator chooses a safe grasp---grabbing the bag by the top, open side---the safety filter does not activate and allows the person to complete the task safely and autonomously. 
% recognizes that such a motion will \textit{not} cause the Skittles to fall out. 
% Thus, the safety filter does not activate and allows the person to gently move the bag side-to-side. 