\section{Limitations}
Our latent HJ reachability formulation generalizes the space of failures robots can safeguard against.
% allows us to safeguard against a wide array of complex failures. 
However, it is not without its limitations. 
One limitation is that the safety filter can only protect against outcomes that the world model can predict.
This means that the world model needs to be trained on some amount of unsafe data in order to effectively predict these unsafe outcomes and compute control policies that steer clear of them. %However, these world models can also be trained on data demonstrating extremely suboptimal behavior.
Additionally, the least-restrictive safety filter mechanism we implemented returns a single control action that attempts to steer the robot away from danger maximally and at the last moment. 
% that entirely ignores the nominal task. 
More sophisticated approaches to search through the set of safe actions and align them with the base policy's task performance should be explored  \cite{hsu2023safety, wabersich2023data}. 
Finally, since we are concerned with robot safety, it is important to acknowledge what sorts of assurances we can expect from this framework. 
% the safe control community oftentimes emphasizes the notion of a safety guarantee. 
Although our method is grounded in rigorous theory, our practical implementation currently lacks formal assurances due to the combination of possible errors when learning the world model, failure classifier, and resulting HJ value function. Characterizing how the errors in one learned component propagate to the downstream safety assurances is exciting future work.

%\begin{itemize}
%    \item reliance on unsafe data to train the world model
%    \item expectation over transition, how to deal?
%    \item finally, the formal guaruntees inherited by our reachability framework are only true under a latent state $z$ that is sufficient statistic of the world, perfect dynamics, perfect failure classifier, and perfect solver. Provably bounding the suboptimality is interesting future work. 
%\end{itemize}
\section{Conclusion}
In this work, our goal was to rigorously generalize robot safety beyond collision-avoidance, accounting for hard-to-model failures like spills, items breaking, or items toppling. 
We introduce \textit{Latent Safety Filters}, a generalization of the safety filtering paradigm that operates in the learned representation of a generative world model. We instantiated our method on a suite of simulation and hardware experiments, demonstrating that our latent reachability formulation is comparable to privileged state formulations, outperforms other safe control paradigms while being less sensitive to hyperparameter selection, and protects against extremely hard-to-specify failures like spills in the real world for both generative policies and human teleoperation. 
Future work should thoroughly investigate the uncertainties within each component of our latent space safety generalization and investigate theoretical or statistical assurances on this new safety paradigm. 
% Future work should thoroughly investigate the uncertainties induced by incomplete data coverage of the world model and 
% additional sensing modalities (e.g., tactile) as inputs to the world model.
%\lpnote{not sure where the right place for this but leaving this here as a reminder: it would be good to discuss that training a WM of course needs data but this data is very "cheap" because it can be un-curated. Anecdotal evidence: the dataset of successful picking trajectories was enough to train the WM but not enough to train the diffusion policy.}

