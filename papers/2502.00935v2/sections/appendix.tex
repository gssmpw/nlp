
\subsection{Hyperparameters}

In Section \ref{sec:sim}, we showcase results for a Recurrent State Space Model~(RSSM) \cite{hafner2019learning} that uses both a deterministic recurrent state $h_t$ and stochastic component $x_t$.
\begin{equation}
\begin{aligned}
    &h_{t+1} = f_{\dynparam}(h_{t}, x_{t}, a_{t}) \;\;\;\;\;\;\;
    \hat{x}_t = p_\dynparam(\hat{x}_t | h_t) \\
        &x_{t} = \enc_\dynparam(x_{t} | h_t, o_t) \;\;\;\;\;\;\;\;\;\;\;\; \hat{o}_t = \text{dec}_\dynparam(\hat{o}_t | h_t, x_t)
\end{aligned}
 \end{equation}
 For the RSSM we define $\latent := \{h_t, x_t \}$.
 The RSSM shares weights between each module and is trained to minimize the KL divergence between the encoding of the current latent $x$ and the latent $\hat{x}_t$ predicted by the transition dynamics. The RSSM additionally utilizes an observation reconstruction objective to maintain the informativeness of its latent state. We refer the readers to \cite{hafner2019learning, hafner2020dreamerv2, hafner2024masteringdiversedomainsworld} for a more comprehensive overview of RSSMs. We use the implementation from \cite{dreamerv3-torch} and list relevant hyperparameters in Table \ref{tab:dreamer}. For Dubin's car experiments, we use a continuous stochastic latent parameterized as a 32-dimensional Gaussian. We use 32 categorical random variables with 32 classes for the high-dimensional simulation experiments.
\begin{table}[h]
    \centering
    \begin{tabular}{l l}
        \toprule
        \textbf{Hyperparameter} & \textbf{Values}  \\
        \midrule
        Image size & 128 \\
        Optimizer & Adam \\
        Learning rate (lr) & $1e-4$ \\
        Hidden dim & 512 \\ 
        Dyn deterministic  & 512 \\
        Activation fn & SiLU \\
        CNN depth & 32 \\
        Batch size & 16 \\
        Batch Length & 64 \\
        Recon loss scale & 1 \\
        Dyn loss scale & 0.5 \\
        Representation loss scale & 0.1 \\ 
    \bottomrule
    \end{tabular}
    \caption{Hyperparameters for Dreamer}
    \label{tab:dreamer}
\end{table}

In Section \ref{sec:hardware} we train DINO-WM~\cite{zhou2024dinowm}, a transformer-based world model that uses the patch-tokens of DINOv2~\cite{oquab2023dinov2} as a representation for the latent state. Here, the DINOv2 encoder is kept frozen, and we train only the parameters of a vision transformer, which is used to predict future patch tokens $\hat{\latent}_{t+1}$ conditioned on a sequence of actions $a_{t-H:t}$ and tokens $\latent_{t-H:t}$ from the previous $H$ timesteps.

\begin{equation}
\begin{aligned}
    &z_{t} = \enc_\encparam(o_t) \;\;\;\;\;\; \hat{\latent}_{t+1} = p_{\dynparam}(\latent_{t-H:t}, \action_{t-H:t})
\end{aligned}
\end{equation}
This model is trained via teacher-forcing with the following consistency loss
$\mathcal{L}(\dynparam) = || \enc_\encparam(o_{t+1}) - p_{\dynparam}(z_{t-H:t}, a_{t-H:t})||$. We tokenize both the wrist and front camera views, leading to two sets of image patches with embedding dimension 384. We additionally encode the action and proprioception into a 10-dimensional latent vector. We concatenate the image patches for both cameras, action embedding, and proprioception embedding for $H=3$ frames. We use learnable positional and temporal embeddings. We pass the output from the transformer into a wrist camera, front camera, and proprioception head, which predicts the corresponding input for the next time step. These are implemented as MLP heads with three layers, a hidden dimension of $788$, and a learning rate of $5e-5$. The remaining hyperparameters are the same as \cite{zhou2024dinowm} and are showing in Table \ref{tab:dinowm}
\begin{table}[h]
    \centering
    \begin{tabular}{l l}
        \toprule
        \textbf{Hyperparameter} & \textbf{Values}  \\
        \midrule
        Image size & 224 \\
        DINOv2 patch size & $(14 \times 14, 384)$ \\ 
        Optimizer & AdamW \\
        Predictor lr & 5e-5  \\
        Decoder lr & 3e-4 \\
        Action Encoder lr & 5e-4 \\
        Action emb dim & 10 \\
        Proprioception emb dim & 10 \\
        Batch size & 16 \\
        Training iterations & 100000 \\
        ViT depth & 6 \\
        ViT attention heads & 16 \\
        ViT MLP dim & 2048 \\
    \bottomrule
    \end{tabular}
    \caption{Hyperparameters for DINO-WM}
    \label{tab:dinowm}
\end{table}

The Dubin's car experiments use a discrete action space, so we train the value function using DDQN \cite{van2016deep} using the toolbox from \cite{hsu2021safety}. The Q-function is implemented as a 3-layer MLP with 100-d hidden dimension. The remaining hyperparameters are shown in Table \ref{tab:rarl}.
\begin{table}[h]
    \centering
    \begin{tabular}{l l}
        \toprule
        \textbf{Hyperparameter} & \textbf{Values}  \\
        \midrule
        Optimizer & AdamW \\
        Learning rate & 1e-3  \\
        Learning rate decay & 0.8 \\
        Hidden dims & $[100, 100]$ \\
        Time discount $\gamma $ & 0.9999 \\
        Activations & Tanh \\
        Batch size & 64 \\
        Training iterations & 400000 \\ 
    \bottomrule
    \end{tabular}
    \caption{Hyperparameters for DDQN HJ Reachability}
    \label{tab:rarl}
\end{table}

For both the simulation and hardware manipulation experiments, we use DDPG \cite{lillicrap2019continuouscontroldeepreinforcement} using the implementation from \cite{li2025certifiabledeeplearningreachability} using the standard discounted Bellman equation from \cite{fisac2019bridging}.

\begin{table}[h]
    \centering
    \begin{tabular}{l l}
        \toprule
        \textbf{Hyperparameter} & \textbf{Values}  \\
        \midrule
        Optimizer & AdamW \\
        Actor lr & 1e-4  \\
        Critic lr & 1e-3 \\
        Actor + Critic hidden dims & $[512, 512, 512, 512]$ \\
        Time discount $\gamma $ & 0.9999 \\
        Activations & ReLU \\
        Batch size & 512 \\
        Epochs & 50 \\ 
    \bottomrule
    \end{tabular}
    \caption{Hyperparameters for DDPG HJ Reachability}
    \label{tab:lcrl}
\end{table}

To smooth the loss landscape for reachability learning, we apply $\tanh$ to the output of $\marginfunc_\ellparam$. This is because the loss function for the failure classifier may output very different values to states that are similarly constraint violating / far from constraint violation, so long as it achieves low loss by \eqref{eq:failure-classifier-loss}. This thresholding ensures that sufficiently safe or unsafe states are evaluated similarly by $\tanh(\marginfunc_\ellparam(\latent))$.


\subsection{Common Questions}


\ques{How do we get labels for training $\failureLatent$?}

When training in simulation, we used privileged data provided by the simulator. For the real-world hardware tasks, we labeled these trajectories by hand. For our 1300 collected trajectories, this simply meant identifying a single frame in the trajectory where all subsequent timesteps were in violation of the safety constraint and all previous timesteps were safe. This took the lead author about 2.5 hours. While this scales poorly, in principle, a VLM could be used to annotate these failures automatically. Furthermore, when training the failure classifier separately from the world model, one can select a smaller subset of data to label and train the failure classifier.


%\abnote{We experimented with labelling via vision language models (e.g., Llava), this was the prompt we used, but the results weren't good so we hand-labeled. We also experimented with manual labeling: if you label even a single skittle falling as failure then the failure classifier is overly pessimistic.}

%\ques{How sensitive is the latent safety filter to the failure classifier quality?} 

%\abnote{What happens to the safety filter if the classifier is learned in better / worse ways?}

% \subsection{What happens if we have biased data for world model learning?}

% \abnote{What happens when your world model doesn't have perfect coverage of all the states, and is biased in some way?}

% \para{Sparse but uniform coverage} \abnote{something beyond just extreme coverage? ablation on dataset size -- smth like number of demonstrations in the WM vs. BRT accuracy?}

% \para{Extreme biasing (i.e., only corners of state space)} \abnote{Ken's crazy results}

%\ques{Which matters more: an accurate world model or failure calssifier?}

%\abnote{This can be a practical take on where to ``spend'' your data / effort. }

%\ques{How does the observability of the state influence the latent safety filter?}

%\abnote{A big challenge is observability of the relevant state information in the latent space. We study this in the Dubins' car setting by controlling the image size and if the heading is passed explicitly to the model. }

% The challenging part here is the heading -- that is basically what makes the BRT interesting, and we hypothesized that it may be very hard to infer heading from just the image (e.g., because image is small). We test: \textbf{[data] Large image only}, \textbf{[data] Small image only}, \textbf{[data] Perfect heading}, \textbf{[loss] Teacher-student}. 

%\ques{How do different world model architectures perform? Do pre-trained visual representations help?}

%\para{RNN vs. Transformer World Model}

%\para{Co-training encoder vs. DINOv2 encoder} We compared co-training the vision encoder with the world model as in dreamer v2 \cite{hafner2020dreamerv2} as well as using pre-trained DINOv2 features \cite{oquab2023dinov2} as in \cite{zhou2024dinowm}. 

%\ques{How does the latent safety filter react to changes of the environment?}
%\todo{other obstacle/bg changes}
%\abnote{train WM on certain types of obstacle data. Then change the features of the environment at deployment time. We keep the physical dynamics of the robot the same,  but vary the visual features of the environment: \textbf{color} is a failure-irrelevant feature, and \textbf{obstacle size and shape} is a failure-relevant feature}









