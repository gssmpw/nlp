% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=1\linewidth]{figs/sketch-pipeline.png}
%     \caption{\textbf{Latent Safety Filter Framework.} \abnote{we should show the process that one follows to instantiate this -- 1) WM, then 2) the WM embeddings are used for failure classifier training, 3) then do safety RL in the imagination and this gives you $\valfunc(\latent), \fallback(\latent)$, 4) then deploy the filter around any base policy. I was thinking we color code the WM + failure classifier to be the same color, then RL is a separate color, and deployment is separate; with highlights on the WM and the RL in the imagination since those are our key insights.}}
%     \label{fig:framework}
% \end{figure*}

\section{A Brief Background on HJ Reachability}
\label{sec:hj-bg}

Hamilton-Jacobi (HJ) reachability \cite{mitchell2005time, margellos2011hamilton} is a control-theoretic safety framework for identifying when present actions will cause future failures, and for computing best-effort policies that minimize failures.  
Traditionally, reachability assumes access to a privileged state space $\state \in \stateSpace$ and a corresponding nonlinear dynamics model $\state_{t+1} = \dyns(\state_t, \action_t)$ which evolves via the robot's control action, $\action \in \actionSpace$.
% framework for analyzing the safety of nonlinear dynamical systems. 
% Key properties of this framework include its ability to reason about non-convex constraint representations, systems with bounded control inputs, and computes minimally-invasive 
% the ability to simultaneously quantify states of inevitable failure as well as best-effort safety-preserving control laws that 

A domain expert will first specify what safety means by imposing a constraint on the state space, referred to as the \textit{failure set}, $\failure \subset \stateSpace$. 
Given the failure set, HJ reachability will automatically compute two entities: (\romannumeral 1) a safety monitor, $\valfunc: \stateSpace \rightarrow \mathbb{R}$, which quantifies if the robot is doomed to enter $\failure$ from its current state $\state$ despite the robot's best efforts, and (\romannumeral 2) a best-effort safety-preserving policy, $\fallback: \stateSpace \rightarrow \actionSpace$. 
% an unsafe set $\unsafeSet \subset \stateSpace$ from which the robot is doomed to enter $\failure$ despite it's best effort, as well as a safe policy $\fallback: \stateSpace \rightarrow \actionSpace$ as well as an which ensures that there exists a control action that the robot can take to prevent entering into F .
These two entities are co-optimized via the solution to an optimal control problem that satisfies the fixed-point safety Bellman equation \cite{fisac2019bridging}:
\begin{equation}
        \valfunc(\state) = \min \Big\{ \marginfunc(\state), ~ \max_{\action \in \actionSpace} \valfunc(\dyns(\state, \action)) \Big\},
        \label{eq:trad-fixed-pt-bellman}
\end{equation}
where the margin function $\marginfunc(s)$ encodes the safety constraint $\failure$ via its zero-sublevel set $\failure = \{ s \; | \; \marginfunc(s) < 0 \}$, typically as a signed-distance function.  
The maximally safety-preserving policy can be obtained via $\fallback(\state) := \arg\max_{\action \in \actionSpace} \valfunc(\dyns(\state, \action))$. Finally, the \textit{unsafe set}, $\unsafeSet \subset \stateSpace$, which models the set of states from which the robot is doomed to enter $\failure$, can be recovered from the zero-sublevel set of the value function: $\unsafeSet := \{\state : \valfunc(\state) < 0\}$.

At deployment time, the safety monitor and safety policy can be utilized together to perform \textit{safety filtering}: detecting an unsafe action generated by any base policy, $\policyTask$, and minimally modifying it to ensure safety.
While there are a myriad of safety filtering schemes (see surveys \cite{wabersich2023data, hsu2023safety} for details), a common minimally-invasive approach switches between the nominal and the safety policy when the robot is on the verge of being doomed to fail: $\action^\text{exec} = \mathds{1}_{\{\valfunc(\state) > 0\}} \cdot \policyTask + \mathds{1}_{\{\valfunc(\state) \le 0\}} \cdot \fallback$.

% a common choice we detail here is a least-restrictive control law: 
% % that ensures the safety of a nominal policy $\pi^{\mathrm{task}}$ by only intervening when the system is on the verge of being doomed to failure:
% \begin{equation}
%     \pi(\state) = \begin{cases}
%             \pi^{\mathrm{task}}(\state),& \text{if } V(\state) > 0 \\
%             \fallback(\state),              & V(\state) \leq 0.
%         \end{cases}
% \end{equation}

% \para{Open Challenges.} These properties make HJ reachability an appealing framework for monitoring and mitigating the system's potential failures. 





\section{Latent Safety Filters}

% and in practice it is associated with a suite of numerical synthesis techniques \cite{mitchell2004toolbox, fisac2019bridging, bansal2021deepreach}.

%In this work, our goal is to \textit{generalize} safe robot controllers to prevent failures that are hard---if not impossible---to write down by hand, like the spilling shown in Figure~\ref{fig:front-fig}.
%Our key idea is that even though these failures are difficult to represent via first-principles state space models, they can be intuitively identified from (a sequence of) high-dimensional observations. 
%However, simply detecting a failure from an observation often means that its \textit{too late} to take a recovery action. 
%For example, by the time the robot observes that the Skittles are leaking out of the bag, it is too late to slow down or reorient the bag; the Skittles are doomed to spill all the way out. 
%% often too late to stop them from spilling all the way out no matter how the manipulator slows down or reorients itself. 
%% However, just because we can identify failure from an observation isn't enough to prevent it from happening. 
%If we want to automatically generate safety-preserving actions, we need to \textit{predict} if the robot \textit{will} enter into failure in the near future and if there is anything in the robot's power to prevent this. 

To tackle both detecting and mitigating hard-to-model failures, we present a latent-space generalization of HJ reachability (from Section~\ref{sec:hj-bg}) that tractably operates on raw observation data (e.g., RGB images) by performing safety analysis in the latent embedding space of a generative world model.
This also transforms nuanced constraint specification into a classification problem in latent space and enables reasoning about dynamical consequences that are hard to simulate. 

% \para{Problem Formulation}
% Since our latent safety framework operates on high-dimensional observations, our underlying problem can be modelled as a Partially Observable Markov Decision Process defined by the tuple $<\stateSpace, \obsSpace, \actionSpace, p>$ where $\stateSpace$ and $\actionSpace$ are the state and action spaces, $\obsSpace(\obs | \state)$ determines the observations received at state $\state$, and $p(\state_{t+1} | \state_t, \action_t)$ denotes the transition probability of the next state from taking action $\action_t$ from $\state_t$.

% We wish to design a safety filter that can monitor any base policy and prevent the system from ever entering a failure set $\failure$. A key challenge in designing such a safety filter lies with the partial observability of the system (access to $\obs_t$ but not $\state_t$) and the lack of an explicit model of the transition dynamics.

\para{Setup: Environment and Latent World Models} We model the robot as operating in an environment $\env \in \envSpace$, which broadly characterizes the deployment context---e.g., in a manipulation setting, this includes the geometry and material properties of the table, objects, and gripper. The robot has a sensor $\sigma : \stateSpace \times \env \rightarrow \obsSpace$ that generates observations $\obs \in \obsSpace$
depending on the true state of the world.
While we are in a partially-observable setting and never have access to the true state, we will leverage a world model to jointly infer a lower-dimensional latent state and its associated dynamics that correspond to the high-dimensional observations.

A world model consists of an encoder that maps observations $\obs_t$ (e.g., images, proprioception, etc.) and  latent state $\hat{\latent}_t$ into a posterior latent $\latent_t$,
and a transition function that predicts the future latent state conditioned on an action. This can be mathematically described as:

\begin{itemize}
    \smallskip  
    \item[] Encoder: $\latent_t \sim \enc_\encparam(\latent_t \mid \hat{\latent}_t, \obs_t)$
    \item[] Transition Model: $\hat{\latent}_{t+1} \sim p_\dynparam(\hat{\latent}_{t+1} \mid \latent_t, \action_t)$. 
    \smallskip  
\end{itemize}
This formulation describes a wide range of world models~\cite{ha2018recurrent, hafner2019learning, hafner2020dreamerv2, hafner2024masteringdiversedomainsworld,zhou2024dinowm}, and our latent safety filter is not tied to a particular world model architecture. 
We focus on world models that are trained via self-supervised learning (observation reconstruction, teacher forcing, etc.) and do not require access to a privileged state. 
Specifically, in Section \ref{sec:sim} we use a Recurrent State Space Model (RSSM) \cite{hafner2019learning} trained with an observation reconstruction objective and in Section \ref{sec:hardware} we use DINO-WM \cite{zhou2024dinowm} which is trained with via teacher-forcing.


\para{Safety Specification: Failure Classifier on Latent State} A common approach for representing $\failure$ is to encode it as the zero-sublevel set of a function $\marginfunc(\state)$ (as in Eq.~\ref{eq:trad-fixed-pt-bellman}). Domain experts typically design this ``margin function'' to be a signed distance function to the failure set, which easily expresses constraints like collision-avoidance (e.g., distance between positional states of the robot and environment entities being less than some threshold). 
However, other types of constraints, such as liquid spills, are much more difficult to directly express with this class of functions and traditional state spaces. 
To address this, we chose to learn $\marginfunc_\ellparam(\latent)$ from data by modeling it as a classifier over latent states~$\latent\in\latentSpace$, with with learnable parameters $\ellparam$.

We train our classifier on labelled datasets of observations corresponding to safe and unsafe states, $\obs^+ \in \mathcal{D}_{\text{safe}}$ and $\obs^- \in \mathcal{D}_{\text{unsafe}}$, and optimize the following loss function inspired by~\cite{yusequential}:
\begin{equation}
\begin{aligned}
     \mathcal{L}(\ellparam) = &\frac{1}{N_\text{safe}}\sum_{\obs^+ \in \mathcal{D}_{\text{safe}}} \text{ReLU}\big(\delta - \marginfunc_\ellparam(\enc_\encparam(\obs^+))\big)\\ + &\frac{1}{N_\text{fail}}\sum_{\obs^- \in \mathcal{D}_\text{fail}} \text{ReLU}\big(\delta + \marginfunc_\ellparam(\enc_\encparam(\obs^-))\big), 
     \label{eq:failure-classifier-loss}
\end{aligned}
\end{equation}
where loss function is parameterized by $\delta \in \mathbb{R}^+$ to prevent degenerate solutions where all latent states are labeled as zero by the classifier.
Intuitively, this loss penalizes latent states corresponding to observations in the failure set from being labeled positive and vice versa. 
The learned classifier represents the failure set $\failureLatent$ in the latent space of the world model via: $\failureLatent = \{\latent \mid \marginfunc_\ellparam(\latent) < 0 \}$.  Our failure classifier can be co-trained (Section \ref{sec:sim}) or trained after (Section \ref{sec:hardware}) world model learning.

    
\para{Latent-Space Reachability in Imagination} Traditionally, reachability analysis requires either an analytic model of the robot and environment dynamics \cite{mitchell2004toolbox, bansal2021deepreach} or a high-fidelity simulator \cite{fisac2019bridging, hsu2023isaacs} to solve the fixed-point Bellman equation, both of which are currently inadequate for complex system dynamics underlying nuanced safety problems (e.g., liquid interaction). 
Instead, we propose using the latent imagination of a pretrained world model as our environment model, capturing  hard-to-design and hard-to-simulate interaction dynamics. 
% the transition function 
We introduce the latent fixed-point Bellman equation: %for computing the safety Bellman backup 
\begin{equation}
    \valfuncLatent(\latent) = \min \Big\{ \marginfunc_\ellparam(\latent), ~ \max_{\action \in \actionSpace} \mathbb{E}_{\hat{\latent}' \sim p_\dynparam(\cdot \; | \;\latent, a)} \Big[\valfuncLatent(\hat{\latent}') \Big]\Big\}.
    \label{eq:latent-fixed-pt-bellman}
\end{equation}
Note that in contrast to Equation~\ref{eq:trad-fixed-pt-bellman}, this backup operates on the latent state $\latent$ and, for full generality, includes an expectation\footnote{This expectation could also be taken to be a risk metric (e.g., CVaR) or a worst-over-N samples of the transition function to induce additional conservativeness.} over transitions to account for world models with stochastic transitions (e.g., RSSMs). For world models with deterministic transitions (e.g., DINO-WM), the expectation can be removed. 

%By using a learned world model as our transition function, we can now express a broader class of systems whose dynamics are difficult to model (e.g., particle systems or fluids), and the classifier-based margin function $\marginfunc_\ellparam(\latent)$ allows us to represent failure sets that would be extremely difficult to represent using traditional state spaces (e.g., spills). 


While the world model allows us to compress high-dimensional observations into a compact informative latent state, computing an exact solution to the latent reachability problem is still intractable due to the dimensionality of the latent embedding (e.g., our latent is a 512 dimensional vector in Section \ref{sec:sim}). This motivates the use of a learning-based approximation to the value function in Equation~\ref{eq:latent-fixed-pt-bellman}. We follow \cite{fisac2019bridging} and induce a contraction mapping for the Bellman backup by adding a time discounting factor $\gamma \in [0,1)$:
\begin{multline}
        \label{eqn:discounted-safety-bellman}
        \valfuncLatent(\latent) = (1-\gamma)\marginfunc_\ellparam(\latent) \\ + \gamma \min \Big\{ \marginfunc_\ellparam(\latent), \max_{\action \in \actionSpace}     \mathbb{E}_{\latent' \sim p_\dynparam(\cdot \; | \;\latent, a)}[\valfuncLatent(\latent')] \Big\}
\end{multline}
In theory, if solved to optimality, this latent value function would offer a safety assurance only with respect to 
% to the robot's understanding of the world (i.e., 
the data used to train the world model and the failure classifier. 
Intuitively, this implies that that the robot can only provide an assurance that it will try its hardest to avoid failure \textit{in its representation of the world}. 
In the following section, we study our overall latent safety framework and the effect of world model dataset coverage on a benchmark safe control task for which we have exact solutions. 
We then scale to high-dimensional manipulation examples in both simulation and hardware. 


