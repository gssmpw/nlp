\section{Introduction}
Imagine that a robot manipulator is deployed in your home, like shown in Figure~\ref{fig:front-fig}. 
What safety constraints should the robot reason about? 
It is common to equate robot safety with \say{collision avoidance}, but in unstructured open world environments, a robot's representation of safety should be much more nuanced. For example, the household manipulator should understand that pouring coffee too fast will cause the liquid to overflow; pulling a mug too quickly from a cupboard will cause other dishes to fall and break; or, in Figure~\ref{fig:front-fig}, aggressively pulling up from the bottom of an open bag will cause the contents to spill. 

% while these failure states are easy to identify, understanding how we got there is really difficult. Consider the black region consisting of the Skittles on the table. Before the skittles are already on the table, there are states in which they are doomed to fall out no matter what the robot does---these are the states visualized in red. 

While these failure states---like liquid overflowing, objects breaking, items spilling---are possible to directly identify from high-dimensional observations, understanding how the robot could enter those states is extremely hard. 
Consider the set of failure states visualized in the black region in Figure~\ref{fig:front-fig}. These failures correspond to states where the Skittles have \textit{already} spilled onto the table. 
But even before the spill is visible, there are states from which the robot manipulator is doomed to end up spilling no matter what it does (visualized as a red set in Figure~\ref{fig:front-fig}): for example, after yanking the bottom of the bag up too quickly, no matter if the robot slows down or reorients the bag, the Skittles are doomed to spill all the way out.
% are hard to characterize via constraints imposed on hand-designed state representations (e.g., poses, velocities), they can be intuitively identified from (a sequence of) high-dimensional observations.
% Unfortunately, simply detecting a failure from an observation often means it is \textit{too late} for the robot to autonomously recover. For example, by the time the robot observes that the Skittles are leaking out of the bag in Figure~\ref{fig:front-fig}, it is too late to slow down or reorient the bag; the Skittles are doomed to spill all the way out.

% while these failure states are easy to identify, knowing what lead to these outcomes and understaing how to preemptively mitigating them is challenging. [explain BRT intuitvely without actually saying "BRT"]



% Enabling robots to represent these complex safety specifications is key to reliably deploying robotic systems \say{in the wild} at scale.
% A safe robot must be able to both identify when it is on the verge of violating a constraint \textit{and} synthesize a safe strategy these failures. However, prior works in failure detection often rely on a human user, either in the form of a question or request for additional demonstrations \todo{cite}. A key challenge, then, is automatically synthesizing a safety-preserving control policy. 

This is where safe control theory can provide some insight. 
Frameworks like Hamilton-Jacobi (HJ) reachability analysis \cite{mitchell2005time, lygeros2004reachability} mathematically model safety constraints very generally---as arbitrary (non-convex) sets in state space---and automatically identify states from which the robot is doomed to fail in the future by solving an optimal control problem. 
However, the question remains how to practically instantiate this theoretical framework to safeguard against more nuanced failures---\emph{beyond collision-avoidance}---in robotics. 

% In this work, we bring this mathematical model of safety to reason about (and safeguard against) safety constraints that go beyond collision-avoidance. 
% reasons about their satisfaction by solving a nonlinear optimal control problem. 
% Characterizing situations in which the robot is doomed to fail---and what it's best-effort safety-preserving action should be---is a cornerstone of safe control theory. 
% While these mathematical frameworks give us a formal language and computational tools to understand when robots will fail and how to automatically prevent these failures, they 
%  However, in practice, reachability has been limited to controlled, collision-avoidance interactions between the robot and its environment. The reason why is because of how challenging
% it is to represent safety specifications—and synthesize the
% corresponding safety filter—in more nuanced interactions,
% like those encountered in manipulation. Consider the task in
% Figure 1, wherein a robotic manipulator must pick up a bag
% of opened Skittles. ..
% While in theory, foundational methods from safe control provide us a way to develop principled safety filters---that minimally modify the robot’s nominal policy to ensure that safety is preserved---these techniques have been highly limited in practice to just collision-avoidance definitions of safety. The reason why is that, to-date, we have used low-dimensional state representations to simplify the analysis of the robotic system’s dynamics. But these low-dimensional state representations commonly used in control make mathematically codifying safe sets that are semantically meaningful to us, like spills, a challenging task.


Our key insight is that the latent representations learned by generative world models \cite{hafner2020dreamerv2, zhou2024dinowm} enable safe control for constraints not traditionally expressable in handcrafted state-space representations. 
% This transforms constraint specification from hand-designing a potentially complicated function into training a failure classifier from a latent state. 
% since a large challenge has been specifying the failure by an engineer or stakeholder, this latent safety is great because all the stakeholder has to do is give labels on the image observations as ``looks safe or not''. This data is used for failure classification, btu then actually computing the safe controler and the set of unsafe states is all autonomous -- no new human data needed. 
% its much easier to know the failure than it is to know how to avoid it -- thats why its nice to just train the failure classifier from human annotated data, but learn the safety critical control and unsafe set automatically via doing HJ in latent space. 
% Within this formulation, specifying complex constraints is as easy as learning a classifier; this allows us to perform reachability in this latent space 
Within this formulation, constraint specification is as easy as learning a classifier in the latent space and HJ reachability can safely evaluate possible outcomes of different actions (to determine if the robot is doomed to fail) within the ``imagination'' of the world model. 
Ultimately, using this framework, we synthesize \textit{Latent Safety Filters}: a mechanism for detecting---and minimally modifying---unsafe actions generated by any base policy (from direct teleoperation to a pre-trained task policy), directly from high-dimensional observations. 



% Within this formulation, constraint specification is as easy as learning a classifier in the latent space 
%Combining the latent world model and the failure classifier, we perform latent-state HJ reachability and 
% to safeguard against failures that are difficult to model using traditional state-space models (e.g. modeling the complex interactions that lead to a spill), yet easy to recognize from (high-dimensional) observations (e.g. seeing a spill).
%synthesize \textit{Latent Safety Filters}: a mechanism for detecting--and minimally modifying---unsafe actions generated by any base policy (from direct teleoperation to a pre-trained task policy). 

We evaluate our approach in three vision-based safe-control tasks in both simulation and hardware: 
\begin{enumerate}
    \item a classic collision-avoidance navigation problem where we can compare our latent approximation with a privileged state solution, 
    \item a high-fidelity simulation of manipulation in clutter where the robot can touch, push, and tilt objects as long as they do not topple over, and 
    \item hardware experiments with a Franka Research 3
       arm picking up an open bag of Skittles without spilling. 
\end{enumerate}
Our quantitative results show that, without assuming access to ground-truth dynamics or hand-designed failure specifications, \textit{Latent Safety Filters} can learn a high-quality safety monitor ($F_1 \text{~score}: 0.982$) and the safety controller provides a $63.6\%$ safety failure violation decrease over a base policy trained via imitation \cite{chi2024diffusionpolicy}. 
In qualitative experiments, we also find that \textit{Latent Safety Filters} allow teleoperators to freely grasp, move, and pull up on an opened bag of Skittles, while automatically correcting any motions that would lead to spilling. 
% policy-agnostic mechanism to detect and mitigate failures in complex, partially observed environments.

% \abnote{In theory, HJ reachability is highly expressive safety framework, capable of characterizing constraints are artbitrary (non-convex) sets in state space and reasoning about their satisfaction as a function nonlinear system trajectories. 
% However, in practice, reachability has been limited to controlled, collision-avoidance interactions between the robot and its environment. 
% The reason why is because of how challenging it is to represent safety specifications---and synthesize the corresponding safety filter---in more nuanced interactions, like those encountered in manipulation. Consider the task in Figure~\ref{fig:front-fig}, wherein a robotic manipulator must pick up a bag of opened Skittles. ....}


%Robot safety is a nuanced concept.  We commonly equate safety with collision-avoidance, but in complex environments like those encountered in manipulation, it can be much more.  For example, a safe manipulator should understand that pouring coffee too fast will cause spilling; or, pulling a plate too quickly from a stack of plates will cause them to fall and break.  While in theory, foundational methods from safe control provide us a way to develop principled safety filters---that minimally modify the robot’s nominal policy to ensure that safety is preserved---these techniques have been highly limited in practice to just collision-avoidance definitions of safety. The reason why is that, to-date, we have used low-dimensional state representations to simplify the analysis of the robotic system’s dynamics. But these low-dimensional state representations commonly used in control make mathematically codifying safe sets that are semantically meaningful to us a challenging task.

%In this work, our key idea is that we can analyze the latent state and dynamics learned from world models (e.g., Dreamer) with Hamilton-Jacobi reachability to synthesize latent safety controllers that safeguard against \textit{more} than just collision; they safeguard against failures that are semantically meaningful from high-dimensional observations (e.g., seeing a spill), but would be extremely hard to write down (e.g. designing a function whose zero-sublevel set characterizes the set of \textit{all} "spills") and reason over (e.g., understanding which motions lead to a spill) in traditional state spaces for safe control.


%\knnote{in so many manipulation works, failures are defined by unsuccessful task completion. should we have an explicit disclaimer that this is not what we are doing?}
%\abnote{yeah, i think we should take care with the intro to really stress the right things (e.g., broader understanding of safety than just task failure --- this is liveness anyways --- or collision; policy-agnostic filter so it can monitor a demonstrator or nominal IL policy).}
