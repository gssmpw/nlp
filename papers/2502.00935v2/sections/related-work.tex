\section{Related Work}

%\abnote{I worry our related work is a bit too long. I propose we get rid of the ``latent world models for robotics'' section because our core contribution isn't related to the design of these WMs. We can cite these works in the method section when we introduce the generalization via WMs. Our related works can be (1) Safety Filters for Robotics — talk about HJ + CBFs in traditional state space. Ours: generalization to latent space; (2) Learning About Safety from Data — talk about there are broadly two categories: learn the spec from data (e.g., LTL spec) or learn the value function directly from demo data. Ours falls into the former; learn safety as a classifier on encoded obs; (3) Latent-Space Control — latent space control foundations (planet, td-mpc), CBFs in latent space. Ours: brings safe control to the high-D, observation-based latent space that is policy-agnostic }

\para{Safety Filtering for Robotics} 
Safety filtering is a control-theoretic approach for ensuring the safety of a robotic system in a way that is agnostic to a task-driven base policy \cite{hsu2023safety}. 
% in a way that decouples the often competing concerns of safety and performance 
A safety filter monitors a base task policy and overrides it with a safe control action if the system is on the verge of becoming unsafe. Control barrier functions \cite{ames2019control}, HJ reachability \cite{margellos2011hamilton, mitchell2005time, Fisac15}, and model-predictive shielding \cite{wabersich2022predictive} are all common ways of instantiating safety filters (see \cite{wabersich2023data} for a survey). 
The most relevant recent developments include computing safety filters from simulated rollouts of first-principles dynamics models or high-fidelity simulators via reinforcement learning or self-supervised learning \cite{hsu2023isaacs, hsu2021safety, fisac2019bridging, bansal2021deepreach, robey2020learning}, safety filters that operate on a belief-state instead of a perfect state \cite{ahmadi2019safe, bajcsy2021analyzing, hu2023deception, vahs2023belief}, and safety filters that keep a system in-distribution of a learned embedding space \cite{castaneda2023distribution}. 
Our work contributes to the relatively small body of work that attempts to bridge the gap between high-dimensional observations, such as LiDAR \cite{lin2024filterdeployallrobust, AgileButSafe} or RGB images \cite{tong2023enforcing, hsuren2022slr}, and safety filters. 
However, unlike previous work, our method does not infer hand-crafted intermediary representations of state from the high-dimensional observations nor does it represent safety as collision-avoidance. 
Instead, our safety filter operates in the latent space of a world model, reasoning directly about the embedded RGB observations and shielding against hard-to-model failure specifications.

%\abnote{add in general safety filter prior works; the point is that these filters have only been instantiated in physical state spaces and, very recently, belief state spaces. We generalize safety filters to latent state spaces learned from high-dimensional image observations.}
%\begin{itemize}
%    \item three popular ways: CBF , HJ Hamilton-Jacobi (HJ) reachability \, and MPC \cite{mellanie's papers} (for a more comprehensive treatment see these surveys \cite{wabersich2023data, hsu2023safety, dawson2022safe}). 
%    \item frontiers (approx): learning these (via RL, demonstrations are covered below) 
%    \item frontiers (representation): CBFs to keep you in distribution \cite{castaneda2023distribution}, belief-space HJ \cite{bajcsy2021analyzing, hu2023deception}
%    \item safety filters for manipulation: \cite{jung2024railreachabilityaidedimitationlearning}, backup policy is emergency stop
%\end{itemize}


\para{Learning About Safety from Expert Data}
%\abnote{here safety can mean a set of states to never enter (i.e., control invariant set), it can mean a failure set / safety specification, it can mean a value function that encodes a control invariant set.} 
There are two broad strategies for learning about safety from expert data. One approach focuses on learning the safety filter (i.e., the value function and the safety-preserving policy) directly from expert demonstration trajectories~\cite{lindemann2024learning, robey2020learning, leung2023learning, chou2020learning, chou2020learninguncertainty, scobee2019maximum, kim2024learning, lindner2024learning}. While this approach bypasses the need to explicitly model failures that are difficult to specify, this approach requires expert demonstration data and extensive coverage of state-action pairs to infer a reliable safety-preserving policy. 
%\abnote{something about why this is attractive when it comes to hard-to-model failures, but its a super hard problem for the expert -- need tons and tons of coverage of state-action pairs to directly regress the safety policy, akin to the challenges that BC approaches have.}
The other set of approaches focuses on programmatically computing the safety filter via synthesis techniques (e.g., optimal control) once the \textit{safety specification} (i.e., the failure set) is encoded or learned. One desirable property of this approach is that it reduces expert labeling efforts. 
% Compared to directly learning a safety policy from expert labels, 
Learning the failure set from expert labels only requires coverage of the state space, while directly learning the safety policy requires careful coverage of the state-action space.
% and optimal demonstrations. 
%\abnote{The idea is that it is easier (in terms of sample complexity) to learn the failure set from labeled data (since this just depends on a single state) rather than the unsafe set (since this depends on an entire trajectory). I think this is called the "verifier / generator" gap (the verifier is the failure set and the generator is the safety policy / Q-function). Maybe Gokul will have a cute theory reference here.}
% Within the regime of learning about safety, two predominant paradigms have emerged: one approach is to directly learn safe behaviors . 
% However, our work is more aligned with the approach of learning safety specifications either from 
Prior works have explored using intervention data to learn failure states in a latent space \cite{liu2023modelbasedruntimemonitoringinteractive, liumulti}, from language \cite{finucane2010ltlmop}, or complex temporal logic specifications from time-series data \cite{bartocci2022survey}. Related works include \cite{srinivasan2020learning, thananjeyan2021recovery}, which uses binary indicators of constraint violation (provided by an oracle) to synthesize the safe control policy.
%\cite{chou2020learning, chou2020learninguncertainty, scobee2019maximum, kim2024learning, lindner2024learning}, binary labels \cite{lavanakul2024safety} or from  
Our method learns hard-to-specify constraints via a binary classification problem on the embeddings of high-dimensional observations, which exhibit visually apparent failures. 
% where data is easily attainable. 
We then use reinforcement-learning-based HJ reachability to automatically synthesize the safe behavior rather than relying on a demonstrator to provide dense coverage of recovery behavior \cite{lindemann2024learning, robey2020learning}.


%\abnote{alt division: learning safety specifications from data / feedback / observations? so instead of focusing on the filter, focus on learning the spec, since that is a key thing that our latent space approach lets us do}
%\begin{itemize}
%    \item if data is demonstrations: learning constraints from demonstrations \cite{chou2020learning, chou2020learninguncertainty, scobee2019maximum, kim2024learning, lindner2024learning}, learning CBFs from demonstrations \cite{robey2020learning, lindemann2024learning}. 
%    \item if data is binary label, then learn a classifier for the boundary of the unsafe set. 
%    \item \cite{lavanakul2024safety} learn the discriminating hyperplane of safe control via RL or supervised learning (Jason's paper)
%    \item \cite{finucane2010ltlmop} learning LTL / STL specifications from natural language text descriptions 
%    \item \cite{shah2023learning} learning LTL / STL specifications from preference data (\abnote{note -- this is just a workshop paper...})
%    \item \abnote{need to stress why not learn the safety policy directly from safe data and instead synthesize the safety strategy!}
%\end{itemize}

\para{Latent Space Control} 
The model-based reinforcement learning community has recently demonstrated the potential for using generative world models for real-world robot control \cite{mendonca2023structured, wu2022daydreamer}. 
One of the advantages of world models is that they transform a control problem with partial observability into a Markov decision process in the learned latent state space.
Many methods for shaping this latent state representation exist, from observation reconstruction \cite{hafner2019learning, hafner2020dreamerv2, hafner2024masteringdiversedomainsworld}, teacher-forcing \cite{zhou2024dinowm}, or reward predictions \cite{Hansen2022tdmpc, hansen2024tdmpc2}. 
While prior works traditionally use the world model to learn a policy for a specific task, we use the world model to learn a policy-agnostic safety filter that reasons about unsafe consequences it can ``imagine'' (but are hard to model) in the latent space.

