\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[sort,numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}
\usepackage{siunitx}
\usepackage{dirtytalk}
\pdfminorversion=4
\input{macros}

% \pdfinfo{
%    /Author (Kensuke Nakamura)
%    /Title  (Latent Safety Filters: Unifying Latent World Models and Hamilton-Jacobi Reachability Analysis)
%    /CreationDate (D:20101201120000)
%    /Subject (Robots)
%    /Keywords (safe control, latent world models, reachability analysis)
% }
\begin{document}

% paper title
\title{Generalizing Safety Beyond Collision-Avoidance via Latent-Space Reachability Analysis}
%Latent/Semantic Safety Filters: Unifying Hamilton-Jacobi Reachability Analysis with Latent Dynamics and Constraints
% Latent Safety Filters: Unifying Latent World Models and Hamilton-Jacobi Reachability Analysis 
% ``I Know It When I See It'': Latent-Space Robot Safety Filters 
% Commonsense / Semantic Safety Filters: Unifying Generative Models and Hamilton-Jacobi Reachability Analysis 
% Generating Commonsense Robot Safety Strategies via Latent-Space Reachability Analysis
% Latent-Space Reachability: Synthesizing Commonsense Robot Safety Strategies 
%Latent Safety Filters: Unifying Latent World Models and Hamilton-Jacobi Reachability Analysis}
% Generalizing Robot Safety Strategies via Latent-Space Reachability Analysis
% Latent-Space Reachability: Synthesizing Commonsense Robot Safety Strategies
% Policy-Agnostic Latent Safety Filters for Preventing Hard-to-Specify Failures in Robotics

% You will get a Paper-ID when submitting a pdf file to the conference system
%\author{Author Names Omitted for Anonymous Review. Paper-ID 509}

\author{
Kensuke Nakamura \\
Carnegie Mellon University
\and
Lasse Peters \\
Delft University of Technology
\and
Andrea Bajcsy \\ 
Carnegie Mellon University
\vspace{-0.1em}
}

\makeatletter
\let\@oldmaketitle\@maketitle% Store \@maketitle
\renewcommand{\@maketitle}{\@oldmaketitle% Update \@maketitle to insert...
\setcounter{figure}{0} % To fix Figure numbering mismatch
\centering
\includegraphics[width=0.99\textwidth]{figs/frontfig_latentsafety3.pdf}
\vspace{-1em}
\captionof{figure}{
%\centering
Our \textit{Latent Safety Filter} can detect, predict, and mitigate failures that are hard to model (e.g., spilling the contents of a bag), such as those encountered in vision-based manipulation. 
Our idea is to perform approximate reachability analysis in the latent space of a world model (light grey region). 
The latent failure set is shown as a black region, with an example of an imagined failure observation shown in the upper right. 
Our method identifies latent states from which the robot is doomed to enter visually-observable failures no matter what actions it takes (larger red set shown above), and automatically overrides
% The resulting safety filter can shield arbitrary base policies. 
% At deployment time, boundary of the learned unsafe set (red set boundary), 
the base policy $\policyTask$ with safety-preserving actions from our safety policy $\fallback$ to prevent spilling the content of the bag. \\ Video results can be found %and code 
on the project website: \href{https://kensukenk.github.io/latent-safety/}{https://kensukenk.github.io/latent-safety/
}.
}
\label{fig:front-fig}
\vspace{-0.3in}
\bigskip}
\makeatother
\maketitle
% \pdfminorversion=4  


% \maketitle

\begin{abstract} 

%For robots to safely operate in the open world, it is critical to have a nuanced understanding of safety that accounts for complex interactions with the environment. While data-driven safety filters have emerged as a promising scalable method for ensuring the safety of robots systems, their application has been predominantly limited to simple hand-specified constraints such as collisions. We propose a safety filter synthesized via Hamilton-Jacobi reachability analysis leveraging the data-driven state representations and dynamics of a latent world model.
%Our latent reachability formulation allows us to handle constraints that are difficult to model mathematically, but are easy to classify, such as spills. We benchmark our method in a classical safe-control task and demonstrate its efficacy in a suite of contact-rich manipulation tasks in both simulation and hardware. %\knnote{and show XXX metric}. 

%Abstract V2:
%Robots policies deployed to the open world must be able to handle complex interactions safely.
%Hamilton-Jacobi Reachability (HJR) provides a powerful mathematical framework for computing policy-agnostic safety filters: mechanisms for detecting unsafe actions generated by \emph{any base policy} and minimally modifying them to ensure safety.
%Howerver, the conventional realization of this framework requires low-dimensional state-space representations, first-principles dynamics models, and user-defined failure specifications to remain tractable---requirements that are hard to scale beyond simple tasks such as collision avoidance.
%In this work, we seek to generalize robot safety filters to understand hazards that are hard — if not impossible — to write to write down by hand, but can intuitively be identified from high-dimensional observations: for example, a manipulator spilling the contents of a bag if the bag is grasped from the wrong angle, or manipulated too quickly.
%Our key idea is to lift the reachability problem to the latent state space learned by generative world models.
%This \emph{latent} formulation enables a data-driving intantiation of the entire safety framework.
%We instantiate our approach, \emph{Latent Safety Filters} (LSF), in a suite of tasks and for increasingly complex systems and representations of safety, demonstrating its ability to closely match ground-truth HJR perforance in collision-avoidance settings while also scaling to visual manipulation, including a hardware experiment to shield various base-policies against hard-to-model spills.

%Abstract V3:
% Safety filters, like those computed via Hamilton-Jacobi (HJ) reachability, are a mathematical framework for detecting unsafe actions generated by any base policy and minimally modifying them to prevent future failures.

Hamilton-Jacobi (HJ) reachability is a rigorous mathematical framework that enables robots to simultaneously detect unsafe states and generate actions that prevent future failures. 
% mitigation stratgies. 
% tightly couples the detection of unsafe states with failure mitigation strategies that a robot can execute online. 
While in theory, HJ reachability can synthesize safe controllers for nonlinear systems and nonconvex constraints, in practice, it has been limited to hand-engineered collision-avoidance constraints modeled via low-dimensional state-space representations and first-principles dynamics. 
% , and hand-designed failure specifications. 
In this work, our goal is to \textit{generalize} safe robot controllers to prevent failures that are hard---if not impossible---to write down by hand, but can be intuitively identified from high-dimensional observations: for example, spilling the contents of a bag.  
%Our key idea is to lift the reachability problem to the latent state space learned by generative world models.
%This latent formulation transforms safety specification a classification problem in latent space, enables reasoning about dynamical consequences that are hard to simulate (e.g., spilling), and unlocks safety filters that tractably operate directly on raw observation data (e.g., RGB images). 
We propose \textit{Latent Safety Filters}, a latent-space generalization of HJ reachability 
% reachability formulation 
that tractably operates directly on raw observation data (e.g., RGB images) by performing safety analysis in the latent embedding space of a generative world model. 
This transforms nuanced constraint specification to a classification problem in latent space and enables reasoning about dynamical consequences that are hard to simulate. 
% by leveraging generative world models.
% We instantiate our approach, \textit{Latent Safety Filters}, 
% in a suite of increasingly %complex robots, environments, and 
% representations of safety in simulation and hardware.
% In simulation and hardware experiments, we study our method in a suite of increasingly complex dynamical systems and safety representations, from collision-avoidance during navigation to spilling the contents of a bag during manipulation. 
In simulation and hardware experiments, we use Latent Safety Filters to safeguard arbitrary policies (from generative policies to direct teleoperation) from complex safety hazards, like preventing a Franka Research 3 manipulator from spilling the contents of a bag 
% if the bag is grasped from the wrong angle 
or toppling cluttered objects. 
%like collision-avoidance during navigation to spilling the contents of a bag during contact-rich manipulation. 

%We instantiate our method in both simulation and hardware on a suite of increasingly complex representations of safety,  ranging from collision avoidance to spilling the contents of a bag.

%
%
%We demonstrate that robots like 7DoF manipulators can safely interact with the environment under \textit{any control policy} (from direct teleoperation to a pre-trained generative policy) and prevent nuanced failures like spilling the contents of a bag or toppling cluttered objects. 

% [more cool stuff about our results! For collision-based representations of safety, we find that LSF can reproduce benchmark task performance! For visual manipulation, we find that LSF lets manipulators physically perturb and interact with clutter in the scene so long as the clutter doesn’t fall. Finally, we deploy LSF on hardware to sheild against hard-to-model spills; we show this for both sheilding a generative DiffusionPolicy and for sheilding teleoperators!]



%While in theory this framework is powerful, in practice safety filters have been limited to collision-avoidance representations of safety that can be modeled via traditional state-space representations and first-principles dynamics.


\end{abstract}

\IEEEpeerreviewmaketitle

\input{sections/introduction}
\input{sections/problem-formulation}
\input{sections/results}
\input{sections/related-work}
\input{sections/conclusion-limitation}
\input{sections/acks}
% Acknowledgments
% \begin{acks}
% \end{acks}

\bibliographystyle{plainnat}
\bibliography{references.bib}

\newpage 
\clearpage
\appendix
\input{sections/appendix}

\end{document}
