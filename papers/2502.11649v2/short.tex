% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
%\usepackage{subfig}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
%\usepackage{subcaption}
%\usepackage{subfig}
\usepackage{graphicx}
\usepackage{pdflscape}  % For landscape tables
\usepackage{array}       % For column formatting
\usepackage{adjustbox}
\usepackage{booktabs}    % Professional-quality tables
\usepackage{multirow}    % For multi-row cells
\usepackage{ragged2e} 
\usepackage{xcolor}
\usepackage{cellspace} % For cell padding
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{booktabs}
\newcommand{\mn}[1]{{\color{blue}[[MN: {#1}]]}}
% \newcommand{\un}[1]{{\color{blue}[[MN: {#1}]]}}
\usepackage{enumitem}

\usepackage{tikz}

\usepackage[font=footnotesize,labelformat=simple]{subcaption}
\captionsetup[sub]{labelformat=simple}\renewcommand\thesubfigure{(\alph{subfigure})}\usepackage[T1]{fontenc}


\newcommand{\rulesep}{\unskip\ \vrule\ }

\title{Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation}


\author{
 \textbf{Amin Qasmi\textsuperscript{2}},
 \textbf{Usman Naseem\textsuperscript{3}},
 \textbf{Mehwish Nasim\textsuperscript{1}}
 \\
 \textsuperscript{1}The University of Western Australia,
 \\
 \textsuperscript{2}Lahore University of Management Sciences,
 \\
 \textsuperscript{3}Macquarie University,
\\
 \small{
   \textbf{Correspondence:} \href{mailto:mehwish.nasim@uwa.edu.au}{mehwish.nasim@uwa.edu.au}
 }
}

\begin{document}
\maketitle
\begin{abstract}

%Amin (first author), Mehwish (last author)

% We introduce a non-cooperative game that incorporates elements of social psychology, including confirmation bias, resource constraints, and influence penalties, to analyse the factors that shape the formation and resistance of opinions. A key aspect of our simulation game is the introduction of penalties for Large Language Models (LLM) agents when generating messages to spread or counter misinformation, thus integrating resource optimisation into the decision-making process. Our study reveals that a higher confirmation bias leads to stronger opinion alignment but also amplify polarisation. Lower confirmation bias result in fragmented opinions with limited shifts. Investing in high-resource debunking strategy significantly boosts the population alignment with the debunking agent early in the game, but at the cost of rapid resource-depletion, limiting long-term influence.

We introduce a novel non-cooperative game to analyse opinion formation and resistance, incorporating principles from social psychology such as confirmation bias, resource constraints, and influence penalties.  Our simulation features Large Language Model (LLM) agents competing to influence a population, with penalties imposed for generating messages that propagate or counter misinformation. This framework integrates resource optimisation into the agents' decision-making process.  Our findings demonstrate that while higher confirmation bias strengthens opinion alignment within groups, it also exacerbates overall polarisation. Conversely, lower confirmation bias leads to fragmented opinions and limited shifts in individual beliefs.  Investing heavily in a high-resource debunking strategy can initially align the population with the debunking agent, but risks rapid resource depletion and diminished long-term influence.


% Our study reveals that higher 
% BCM thresholds (0.7â€“0.9) lead to stronger opinion alignment but also amplify polarisation, with nearly 90\% of agents aligning strongly with either the Red or Blue team. Lower thresholds (0.3) result in fragmented opinions with limited shifts.
% We also find that deploying high-potency debunking messages early significantly boosts Blue team alignment (reaching 52\%), but at the cost of rapid energy depletion, limiting long-term influence.

\end{abstract}

%This constraint-based approach allows for a more realistic simulation of how limited resources influence the effectiveness of misinformation mitigation strategies. By combining computational modeling with the cognitive complexities of human communication, this study provides a more realistic understanding of opinion dynamics and the trade-offs involved in countering misinformation in digital environments.

% \section{Introduction and Background}

% %mn + amin
% The study of opinion dynamics originated from efforts to understand how individuals modify their views under social influence \citep{kelman1958compliance, kelman1961american}, with broad applications in areas such as public health campaigns, conflict resolution, and combating misinformation. Within social networks, opinions spread and evolve, influenced by various factors, including peer interactions \cite{kandel1986processes}, media exposure \cite{zucker1978variable}, and group dynamics \cite{friedkin2011social}. Developing accurate models of these processes is essential not only for predicting trends like opinion polarisation \citep{Small2024} or consensus formation but also for crafting targeted interventions to mitigate harmful effects, such as the spread of misinformation or societal fragmentation \citep{hegselmann2015opinion}. Agent-based models (ABMs), which simulate interactions among individual agents as proxies for humans, serve as valuable tools for examining the emergent properties of opinion dynamics. These models offer robust frameworks for analysing complex scenarios \cite{deffuant2002can, mathias2016bounded}, evaluating strategies to reduce negative consequences, and potentially fostering constructive social influence by integrating explicit cognitive mechanisms into opinion-updating processes. This work examines how LLMs, can model human-like opinion dynamics and influence propagation within social networks. Traditional agent-based models (ABMs) often use simplified rules that fail to capture complex human communicative strategies. To address this, a non-cooperative game framework is introduced, where adversarial LLMs one spreading misinformation and the other countering it, interact. Unlike previous studies that model passive opinion evolution \cite{wang2025decoding, chuang2024simulating}, this work introduces a non-cooperative game framework where LLM agents engage in adversarial interactions, simulating the strategic spread and countering of misinformation. While previous studies focus on social media structures and mitigation via nudging, our model emphasizes resource-constrained influence operations and analyses the effectiveness of debunking strategies in competitive environments. We pose the following research questions:

\section{Introduction and Background}

The study of opinion dynamics, originating from efforts to understand how individuals modify their views under social influence \citep{kelman1958compliance, kelman1961american}, has broad applications in areas such as public health campaigns, conflict resolution, and combating misinformation.  Within social networks, opinions spread and evolve, influenced by various factors including peer interactions \cite{kandel1986processes}, media exposure \cite{zucker1978variable}, and group dynamics \cite{friedkin2011social}. Developing accurate models of these processes is essential not only for predicting trends like opinion polarisation \citep{Small2024} or consensus formation but also for crafting targeted interventions to mitigate harmful effects, such as the spread of misinformation or societal fragmentation \citep{hegselmann2015opinion}. Agent-based models (ABMs), simulating interactions among individual agents as proxies for humans, serve as valuable tools for examining the emergent properties of opinion dynamics. These models offer robust frameworks for analysing complex scenarios \cite{deffuant2002can, mathias2016bounded}, evaluating strategies to reduce negative consequences, and potentially fostering constructive social influence by integrating explicit cognitive mechanisms into opinion-updating processes.

This work investigates how Large Language Models (LLMs) can model human-like opinion dynamics and influence propagation within social networks. Traditional ABMs often employ simplified rules that fail to capture the complexity of human communicative strategies. To address this limitation, we introduce a novel non-cooperative game framework where adversarial LLMs, one spreading misinformation and the other countering it, interact. 
This work introduces a non-cooperative game where LLM agents engage in adversarial interactions to model misinformation spread and countering. Unlike prior studies \cite{wang2025decoding, chuang2024simulating} on passive opinion evolution and nudging, it focuses on resource-constrained influence operations and debunking effectiveness in competitive environments.
%Unlike previous studies that model passive opinion evolution \cite{wang2025decoding, chuang2024simulating}, this work introduces a non-cooperative game where LLM agents engage in adversarial interactions, simulating the strategic spread and countering of misinformation. While previous research has focused on social media structures and mitigation via nudging, our model emphasises resource-constrained influence operations and analyses the effectiveness of debunking strategies in competitive environments.

We pose the following research questions:

\begin{enumerate} [label=\textbf{RQ}\arabic*, nosep]
    \item What are the emergent behaviors in networks of agents influenced by competing LLMs?
    % , and how do factors such as (but not limited to) confirmation bias, cognitive dissonance, and resource constraints affect these dynamics?
    \item How does the competition between LLM agents shape the evolution of opinion clusters over time, also known as echo-chambers? 
    
    %\mn{I think here we can show how changing the BCM threshold changed polarisation and global disagreement}
  
\end{enumerate}


% \subsection{Contributions}
% \begin{enumerate}
%     \item Proposing a realistic scenario
%     \item Showing echo-chambers effect
%     \item how debunking could work with resource constraints
    
% \end{enumerate}

%\section{Related Work}

%mn
\begin{figure}[t]
    \centering
    %\includegraphics[trim= 80 290 290 40,clip, width=0.4\textwidth]{latex/src/pipelline.pdf}
        \includegraphics[width=0.40\textwidth]{latex/src/new-Methodology.pdf}
        \vspace{-0.3cm}
    \caption{In each round, the current team (Red/Blue) generates a message, which is then assigned a potency value by the judge. The network is updated as per the BCM Model's update equation. In the next round, the other team receives the results of their opponent's message in the previous round, as well as the results of their last message.}
    \label{fig:pipeline}
    \vspace{-0.3cm}
\end{figure}
\section{Methodology}

We use LLMs to simulate the propagation and debunking of misinformation on social media within a non-cooperative game framework.

% We use LLMs to simulate the propagation and debunking of misinformation on social media in a non-cooperative game setting.

\noindent\textbf{Scenario:}
Our scenario is strategically designed to reflect the asymmetric nature of contested information environments, specifically highlighting the challenges faced by the "Blue team" (i.e., those countering misinformation). This framework mirrors adversarial dynamics commonly modelled in serious games or wargames, particularly in the context of cybersecurity.  The "Red Team" and "Blue Team" construct \cite{InformationWarfighter}, familiar in cybersecurity practices (as detailed in NIST's Glossary \cite{NISTGlossaryRedTeam}), is adapted to our simulation.  The system comprises two LLM-based agents with opposing objectives: the \emph{Red Agent} (adversarial agent) disseminates misinformation, while the \emph{Blue Agent} (debunking agent) counters it and aims to restore trust. These agents operate within a directed network of neutral agents, termed \emph{Green Nodes}, representing individuals in a population. Figure \ref{fig:pipeline} shows the structure of our non-cooperative game.



% The scenario has been strategically developed to reflect the asymmetric nature of the contested information environment, highlighting the vulnerabilities facing the Blue team. This framework mirrors adversarial dynamics often modelled in serious games or wargames, particularly in cybersecurity. While the Red Team and Blue Team construct \cite{InformationWarfighter} is common in cybersecurity practices (as detailed in NIST's Glossary \cite{NISTGlossaryRedTeam}). The system comprises two LLM-based agents with opposing objectives: the \emph{Red Agent} also known as the adversarial agent, responsible for disseminating misinformation, and the \emph{Blue Agent} also known as the debunking agent, tasked with counteracting misinformation and restoring trust. These agents operate within a directed network of neutral agents, termed \emph{Green Nodes}, which represent individuals within a population. Figure \ref{fig:pipeline} shows the working of our non-cooperative game.

\noindent\textbf{Agent Roles and Mechanics:} The simulation incorporates the following agent roles:

\textbf{Red Agent:} The Red Agent aims to amplify doubt and confusion by generating misinformation messages of varying potency.  Messages of higher potency incur penalties through rejection, reflecting real-world scenarios where informed populations are sceptical of, and less susceptible to, high-strength misinformation.


% \textbf{Red agent}: The red agent aims to amplify doubts and create confusion by generating messages that contain misinformation of varying potencies. Messages with higher potencies face penalties through rejection, similar to real-world scenarios where well-informed populations are sceptical of misinformation and are less likely to be swayed by such high-strength messages.

% \textbf{Blue agent}: The blue agent dispels the misinformation spread by the red team; however, it does not have unlimited resources. The blue agent operates within an energy constraint. High-potency messages lead to greater energy loss. Therefore, the agent must keep their available resources in mind.

\textbf{Blue Agent:} The Blue Agent counters the misinformation spread by the Red Agent, but operates under a resource constraint. High-potency counter-messages incur a greater resource cost.  Therefore, the Blue Agent must strategically manage its available resources.

% \textbf{Judge Agent}: To prevent agents from assigning arbitrarily high and inflated potencies to their messages, a judge agent is responsible for assigning potencies to all generated messages based on specified criteria: 
% \emph{Clarity}: Is the message clear and well-articulated?
% \emph{Evidence}: Does the message provide credible evidence or logical reasoning?
% \emph{Relevance}: Does the message effectively address misinformation?
% \emph{Impact}: Does the message persuade or influence the audience effectively?

\textbf{Judge Agent:} To prevent agents from assigning arbitrarily high potencies to their messages, a Judge Agent assigns potencies based on specified criteria:

\begin{itemize}[noitemsep,leftmargin=*]

    \item\emph{Clarity:}Is the message clear and well-articulated?
    \item\emph{Evidence:} Does the message provide credible evidence or logical reasoning?
    \item\emph{Relevance:} Does the message effectively address the misinformation?
    \item\emph{Impact:} Does the message effectively persuade or influence the target audience?
\end{itemize}

\noindent\textbf{Simulation settings:}  The simulation begins with $n$ nodes, of which $x$ are initially aligned with the Red Agent's misinformation (pro-conspiracy), $y < n - x$ are aligned against it (anti-conspiracy), and the remaining $z = n - x - y$ are neutral. Both Red and Blue Agents generate messages, the potencies of which are determined by the Judge Agent.



% The simulation begins with $n$ nodes, of which $x$ are initially aligned with the Red Agent's misinformation (pro-conspiracy), $y < n - x$ are aligned against it (anti-conspiracy), and the remaining $z = n - x - y$ are neutral. Both Red and Blue Agents generate messages, the potencies of which are determined by the Judge Agent.
% The simulation starts with \textit{n} nodes, of which \textit{x} nodes are aligned in favor of conspiracy theory (red team), and \textit{y < n - x} nodes aligned against conspiracy theory (blue team). The remaining \textit{z = n - x - y} nodes are neutral. Both red and blue teams generate messages, and a judge determines their potency.



\noindent\textbf{Opinion Modeling:} We use the Bounded Confidence Model (BCM) \cite{mathias2016bounded} to simulate opinion dynamics.  In the BCM, a node updates its opinion if the difference between its opinion and that of a neighbouring node is less than a threshold (the confirmation bias value, $\mu$). The opinion update condition and formula are summarised below:

% We used the Bounded Confidence model \cite{mathias2016bounded} to simulate opinion dynamics. According to BCM, a node in a network updates its opinion if the difference between its opinion and that of its neighbor  is less than a constant threshold (\emph{a confirmation bias value}). The opinion update condition and formula can be summarized as follows:

\label{bcmalgorithm}
\begin{algorithmic}
\ForAll{$n \in N$}
    \ForAll{$m \in \text{Neighbours}(n)$}
        \If{$| O_m - O_n | < \mu$}
            \State $O_m \gets O_m + \mu (O_m - O_n)$
        \EndIf
    \EndFor
\EndFor
\end{algorithmic}

% \begin{algorithmic}
% \ForAll{$n \in N$} 
%     \ForAll{$m \in \text{Neighbors}(n)$}
%         \If{$| O_m - O_n | < \mu$} 
%             \State $O_m \gets O_m + \mu (O_m - O_n)$
%         \EndIf
%     \EndFor
% \EndFor
% \end{algorithmic}

where $N$ is the set of all nodes, $O_n$ is the opinion of node $n$, $O_m$ is the opinion of neighbour $m$, and $\text{Neighbours}(n)$ returns the neighbours of $n$.  Opinions range between [-1, 1]. Nodes with opinions less than -0.5 are considered aligned with the Blue Agent, those greater than 0.5 with the Red Agent, and those in between are neutral.

 % $N$ represents the set of all nodes in the network. Each node $n \in N$ has an associated opinion denoted as $O_n$. Similarly, for each neighbor $m$ of node $n$, the opinion is represented as $O_m$. The function $\text{Neighbors}(n)$ returns the set of all neighboring nodes of $n$. The parameter $\mu$ is the BCM threshold, which determines whether an opinion update should occur. The opinions of nodes range between [-1,1] where nodes having opinion values for instance less than \textit{-0.5} are aligned towards the blue team, while nodes having opinion values more than \textit{0.5} are aligned towards the red team. The in-between nodes are part of the neutral population.

\noindent\textbf{Topics Classification:} Topics for misinformation include serious debates and popular conspiracy theories (e.g., "The Earth is Flat") as well as more frivolous claims (e.g., "The Moon is made of cake").



% The topic classification includes serious debates and popular conspiracies, such as "The Earth is Flat," as well as laughable claims, such as "The Moon is made of cake."

\textbf{Models:} Our study employs GPT-4O and 4O-MINI as judges \cite{hurst2024gpt, openai2024gpt4omini}. \emph{Experiment A} compares Mixtral-8x7B-Instruct with Gemma-2-9b \cite{jiang2024mixtral, team2024gemma}, while \emph{experiment B} evaluates Mixtral-8x7B-Instruct against Gemini 1.5 Flash-8b \cite{team2024gemini}. Lastly, \emph{experiment C} contrasts Gemma-2-9b with Gemini 1.5 Flash-8b.
% The models used in our experiments are listed below. Blue indicates a debunking model, while red indicates an adversarial model.

% \begin{itemize}[noitemsep,leftmargin=*]
%     \item \textbf{Judge}
% GPT-4O/4O-MINI \cite{hurst2024gpt, openai2024gpt4omini}
%     \item \textbf{Experiment A: (Blue vs Red): } Mixtral-8x7B-Instruct-v0.1 \cite{jiang2024mixtral} vs Gemma-2-9b \cite{team2024gemma}
%     \item \textbf{Experiment B: (Blue vs Red): } Mixtral-8x7B-Instruct-v0.1 vs Gemini 1.5 flash - 8b \cite{team2024gemini}\item \textbf{Experiment C: (Blue vs Red): } Gemma-2-9b vs Gemini 1.5 flash - 8b
% \end{itemize}


% \begin{itemize}[noitemsep,leftmargin=*]
%     \item \textbf{Judge:} GPT-4O/4O-MINI \cite{hurst2024gpt, openai2024gpt4omini}
%     \item \textbf{Experiment A (Blue vs Red):} Mixtral-8x7B-Instruct-v0.1 \cite{jiang2024mixtral} vs. Gemma-2-9b \cite{team2024gemma}
%     \item \textbf{Experiment B (Blue vs Red):} Mixtral-8x7B-Instruct-v0.1 vs. Gemini 1.5 flash - 8b \cite{team2024gemini}
%     \item \textbf{Experiment C (Blue vs Red):} Gemma-2-9b vs. Gemini 1.5 flash - 8b
% \end{itemize}
%mn
%amin
\noindent\textbf{Reinforcement learning:} Reinforcement Learning from Human Feedback (RLHF) has proven effective in aligning LLMs with human preferences \cite{kaufmann2024surveyreinforcementlearninghuman}.  In our simulations, agents receive feedback after each round, including their last message and metrics such as the percentage of followers gained or lost. This feedback allows them to refine their messaging strategies.



% Reinforcement Learning from Human Feedback (RLHF) has been effectively utilized in the past to align LLMs with human preferences \cite{kaufmann2024surveyreinforcementlearninghuman}. In this approach, agents receive feedback on their outputs, enabling them to refine their responses in subsequent interactions. In our simulations, agents receive their last message along with metrics such as the percentage of followers gained or lost, allowing them to adjust their messaging strategies accordingly after each round.



\section{Simulations and Evaluation}
We conducted simulations of 100 rounds for each experiment.  The network configuration remained constant across all simulations: a directed network of 50 nodes, with 40\% (20 nodes) initially aligned with the Blue Agent (anti-conspiracy) and 20\% (10 nodes) aligned with the Red Agent (pro-conspiracy). This initial distribution reflects the observation that conspiracy theorists typically represent a minority within social media populations. The Blue Agent was initialised with a resource value of 100 and an influence factor (base potency) of 0.6, while the Red Agent's influence factor was set to 0.5.  We explored three Bounded Confidence Model (BCM) thresholds ($\mu$): 0.3, 0.7, and 0.9.

To investigate the impact of increased resource investment in debunking, we conducted a further set of experiments (using $\mu = 0.9$) where the Blue Agent generated highly potent messages during the first 20 rounds (10 messages per agent). During these initial rounds, the Blue Agent's messages had their base potency scaled by a factor of 1.2, with the final assigned potency capped at 1.0 (i.e., $\min(\text{potency} \times 1.2, 1.0)$).  This simulated a "high-resource" debunking strategy.

All simulations were run on a local machine having an Intel(R) Core(TM) i7-1355U 13th Gen CPU and 32 Gigabytes of RAM. While the machine was equipped with an integrated GPU (Intel Iris Xe Graphics (shared 15.8GB memory)), no dedicated GPU acceleration was used in this project. For all LLMs, the temperature was kept at 0.5, top\_p at 1.0, and max\_tokens were set to 100. The results of these simulations are presented in section \ref{results}.


% We ran simulations of 100 rounds for each experiment. The network configuration was kept constant: 50 nodes with 40\% initially aligned towards the Blue Team and 20\%  towards the Red Team. This was because conspiracy theorists exist in a minority on social media. The blue team was assigned an energy value of 100 and an influence factor (a potency) of 0.6, while the Red team was assigned an influence factor of 0.5 Three BCM thresholds were tested: 0.3, 0.7, and 0.9. To test whether investing more resources in debunking is rewarding, we conducted another set of experiments with the BCM threshold value 0.9 to test the effect of generating highly potent messages from the Blue team during the first 20 rounds (in which 10 messages are generated from each team). During these rounds, the messages from the Blue team were scaled by a factor of 1.2 and the final assigned potency was \textit{min(potency*1.2, 100)}. The results are discussed in the next section.

%\section{Evaluation}

\noindent\textbf{Metrics:} We employ the following metrics to analyse the results of our simulations.

% We used the following metrics for evaluation. 
%amin

\noindent\textbf{Polarisation} refers to the division of individuals or groups into opposing factions, leading to the reinforcement of extreme views within social networks. Network polarisation ($P$) is calculated \cite{chitra2020analyzing} as: \vspace{-0.3cm}


% refers to the division of individuals or groups into opposing factions, reinforcing extreme views within social networks. 
% The network polarisation \( P \) is computed as:


\begin{equation} \label{polarisation}
P = \frac{1}{N} \sum_{n \in \mathcal{V}} (O_n - \bar{O})^2
\end{equation}\vspace{-0.3cm}

where:
\vspace{-0.2cm}
\begin{itemize}[noitemsep,leftmargin=*]
    \item $N$ is the total number of nodes in the network.
    \item $\mathcal{V}$ is the set of all nodes.
    \item $O_n$ is the opinion of node $n$.
    \item $\bar{O}$ is the mean opinion across all nodes.
\end{itemize}


% \label{polarisation}
% \begin{equation} 
% \small
% P = \frac{1}{N} \sum_{n \in \mathcal{V}} (O_n - \bar{O})^2
% \end{equation}

% where
% \( N \) is the total number of nodes in the network,
% \( \mathcal{V} \) is the set of all nodes,
% \( O_n \) is the opinion of node \( n \),
% \( \bar{O} \) is the mean opinion over all nodes.

\noindent\textbf{Judge's Agreement:} To assess the consistency of the potencies assigned by Judge Agent, we used two Judge Agents in each round to independently generate potency values for the same messages.  All generated potency values were recorded, and inter-rater agreement was measured using two metrics: Intraclass Correlation Coefficient (ICC) and Krippendorff's Alpha. ICC values range from 0 to 1, where values closer to 1 indicate strong agreement and values closer to 0 indicate poor agreement. Krippendorff's Alpha ranges from -1 to 1, with higher values indicating stronger agreement.



% To understand the quality of potencies assigned by the Judge, two agents were used at each round to generate the potencies. All these values were recorded, and we used two metrics: Intraclass Correlation Coefficient (ICC) and Krippendorff's Alpha. ICC has values in the range [0,1] where a value closer to 1 indicates strong agreement and a value closer to 0 indicates poor agreement, whereas, in Krippendorff's Alpha, values lie in the range [-1, 1] where higher values suggest strong agreement between the annotators.

\begin{table}[h]
    \centering
    \resizebox{0.40\textwidth}{!}{
    \begin{tabular}{ccccc}
        \toprule
        \textbf{BCM Threshold} & \textbf{Experiment} & \textbf{ICC Value} & \textbf{Krippendorff's Alpha} \\
        \midrule
        \multirow{3}{*}{0.3} & A & 0.495 & 0.420 \\
                             & B & 0.085 & 0.035 \\
                             & C & 0.485 & 0.435 \\
        \midrule
        \multirow{3}{*}{0.7}  & A & 0.625 & 0.560 \\
                              & B & 0.615 & 0.590 \\
                             & C & 0.540 & 0.470 \\
        \midrule
        \multirow{3}{*}{0.9}   & A & 0.490 & 0.395 \\
                               & B & 0.420 & 0.400 \\
                               & C & 0.575 & 0.540 \\
        \bottomrule
    \end{tabular}
     }

        \caption{Across Topic Average Intraclass Correlations (ICC) and Krippendorff's Alpha for Different BCM Thresholds. The values represent the agreement between the agents while assigning potencies to messages.}
    \label{tab:icc_krippendorff_values}
    \vspace{-0.4cm}
\end{table}

\captionsetup[subfigure]{skip=0.5pt} % global setting for subfigure
% \captionsetup[figure]{labelfont=bf}
\begin{figure*}[t]
    %\centering

    \subfloat[\label{fig:opinions}]{\includegraphics[width=0.7\textwidth]{latex/src/opinions}}
    \hfill % Adds flexible horizontal space
\tikz{\draw[-,black, densely dashed, thick](0,-2.05) -- (0,1.05);} 
    \vspace{-1mm}
    \subfloat[\label{fig:opinions2}]{
        \includegraphics[width=0.275\textwidth]{latex/src/opinions_threshold_09} }
         \\ % Adds flexible vertical space
   
    \subfloat[\label{fig:polarisations}]{
        \includegraphics[width=0.68\textwidth]{latex/src/polarizations} }
    \hfill % Adds flexible horizontal space
 \tikz{\draw[-,black, densely dashed, thick](0,-2.05) -- (0,1.05);} 
    \vspace{-3mm}
    \subfloat[\label{fig:polarisations2}]{
        \includegraphics[width=0.275\textwidth]{latex/src/polarizations2} }
    \caption{\textbf{(a) \& (c) show population opinion percentages and average polarisation across topics for BCM thresholds 0.3, 0.7, and 0.9 over 100 rounds with models A(---), B(- - -), and C($\dots$). Red and Blue colors indicate population's alignment with adversarial and debunking agents respectively . Figures (b) \& (d) present opinions and polarisation for BCM threshold 0.9 over 50 rounds for the same experiments.}}  \vspace{-4mm}
    \label{fig:opinion_and_polarisation}
\end{figure*}

\section{Results and Discussion} \label{results}

\noindent\textbf{RQ1} explores how adversarial interactions between competing LLM-driven agents (representing misinformation and counter-misinformation) influence collective opinion dynamics. Specifically, we examine the role of cognitive biases (represented by the BCM threshold) in shaping the stability and evolution of opinion alignment. Figure \ref{fig:opinion_and_polarisation} summarises our findings.

Figure \ref{fig:opinions} illustrates the evolution of agent alignment over time for different BCM thresholds.  At a low threshold ($\mu = 0.3$), the opinion landscape becomes highly fragmented, with the Blue Agent's alignment stagnating at its initial 40\%.  In contrast, at moderate and high thresholds ($\mu = 0.7$ and $\mu = 0.9$), the Blue Agent's alignment increases to 45\% and 50\%, respectively.  The Red Agent's alignment also increases with higher thresholds, rising from 20\% to 25\%, 35\%, and 38\% at thresholds of 0.3, 0.7, and 0.9, respectively.  This demonstrates that, without resource constraints, accumulating support is more feasible.  Furthermore, the early rounds of interaction appear crucial in shaping long-term opinion trajectories, highlighting the strategic importance of early influence.

Figure \ref{fig:polarisations} shows the corresponding polarisation trends. A low threshold ($\mu = 0.3$) results in only a marginal increase in polarisation ($\sim$40\%), while higher thresholds ($\mu = 0.7$ and $\mu = 0.9$) lead to substantially higher polarisation levels ($\sim$70\% and $\sim$80\%, respectively).  These results align with the BCM update algorithm (Section \ref{bcmalgorithm}) and the polarisation calculation (equa \ref{polarisation}).  With a small BCM threshold, agents only update their opinions if they are already closely aligned, resulting in multiple localised opinion clusters rather than a single consensus.  Consequently, polarisation remains moderate as divergence occurs within sub-clusters.  However, at high BCM thresholds, interactions occur more frequently across a broader range of opinions, amplifying extreme positions.  As observed in Figure \ref{fig:opinions} (for $\mu = 0.9$), nearly 90\% of agents become strongly aligned with either the Red or Blue Agent, reflecting this sharp increase in polarisation.





\noindent\textbf{RQ2} investigates optimal strategies for the Blue Agent to effectively counter misinformation under resource constraints. Specifically, we analyse the impact of an aggressive early-game approach, where high-potency debunking messages are deployed at a substantial resource cost.  Due to the rapid resource depletion associated with this strategy, these simulations were limited to 50 rounds.

Figure \ref{fig:opinions2} shows that this aggressive strategy enabled the Blue Agent to surpass the 50\% alignment threshold, reaching a peak of 52\%.  Importantly, all three experimental conditions (A, B, and C) exhibited higher maximum alignment compared to the previous strategies, suggesting that an initial surge of high-potency messages leads to a greater overall shift towards the Blue Agent's perspective.

Figure \ref{fig:polarisations2} shows a sharp increase in polarisation during the first 20 rounds (corresponding to the high-resource debunking period), followed by a gradual convergence. This indicates that while an aggressive approach initially amplifies divisions, it eventually stabilises as the influence of misinformation diminishes. These findings highlight the trade-off between immediate impact and long-term sustainability in misinformation counter-strategies, emphasizing the importance of energy management in prolonged engagements.



% This study investigates optimal strategies for the Blue team to effectively counter misinformation while navigating energy constraints. Specifically, we analyse the impact of an aggressive early-game approach, where high-potency debunking messages are deployed at a substantial energy cost. Due to this rapid energy depletion, the simulation was limited to 50 rounds instead of the standard 100.
% As shown in Figure \ref{fig:opinions2}, this strategy enabled the Blue team to surpass the 50\% opinion alignment threshold, reaching a peak alignment of 52\%. Notably, all three experimental conditions (A, B, and C) exhibited higher maximum opinion alignment compared to previous strategies, indicating that an initial surge of high-potency messages led to a greater overall shift towards the Blue team.
% Furthermore, Figure \ref{fig:polarisations2} illustrates a sharp increase in polarisation during the first 20 roundsâ€”corresponding to the period of high-energy debunkingâ€”followed by a gradual convergence. This suggests that while an aggressive approach initially amplifies divisions, it eventually stabilizes as the influence of misinformation diminishes. These findings highlight the trade-off between immediate impact and long-term sustainability in misinformation counter-strategies, emphasizing the importance of energy management in prolonged engagements.

\section{Conclusions and Future Work}

% This study explored the dynamics of opinion polarisation and alignment in a non-cooperative game setting where adversarial LLMs engage in misinformation and counter-misinformation campaigns. Through a series of simulations, we demonstrated that higher BCM thresholds facilitate stronger alignment but also amplify polarisation, leading to greater societal fragmentation. Additionally, we examined the role of resource constraints in strategic messaging, showing that early high-potency interventions are effective at shifting opinions but come at the expense of long-term sustainability.

% Our findings suggest that in adversarial information environments, there exists a trade-off between immediate influence and sustained engagement. High-impact interventions must be carefully balanced with resource limitations to prevent premature exhaustion of influence capacity. Moreover, polarisation trends indicate that simply allowing more interactions (higher BCM thresholds) does not necessarily lead to consensus but can instead drive opinions further apart.

% These insights contribute to the broader understanding of LLM-mediated influence operations, shedding light on how misinformation campaigns and counter strategies unfold over time. Future work should refine these models by incorporating heterogeneous agent behavior, adaptive messaging strategies, and real-world social network structures to improve the robustness and applicability of findings. The simulation can be expanded by having intelligent nodes in the population (powered by LLMs) instead of 'dumb' nodes updating their opinions numerically.

This study examines opinion polarization in a non-cooperative game with adversarial LLMs spreading and countering misinformation. Higher BCM thresholds enhance faction alignment but intensify societal polarization. We identify a trade-off between immediate impact and sustainability: high-impact interventions deplete resources quickly, while frequent interactions may deepen polarization. These findings inform LLM-driven influence operations and suggest future research on adaptive agents and real-world network integration.


% This study investigated opinion polarisation in a non-cooperative game featuring adversarial LLMs disseminating and countering misinformation.  Our findings demonstrate that while higher BCM thresholds increase agent alignment within factions, they also exacerbate overall societal polarisation.  Furthermore, we observed a trade-off between immediate influence and long-term sustainability: high-impact interventions risk rapid resource depletion, and increased interaction frequency can intensify polarisation rather than promoting consensus. These insights contribute to a deeper understanding of LLM-driven influence operations and suggest avenues for future research, including the development of adaptive agents and the incorporation of real-world network structures.


% This study examined opinion polarisation in a non-cooperative game where adversarial LLMs spread misinformation and counter-misinformation, showing that higher BCM thresholds increase alignment but also amplify societal fragmentation.
% %
% Findings highlight a trade-off between immediate influence and long-term engagement, where high-impact interventions risk depleting resources, and increased interactions can intensify polarisation rather than fostering consensus.
% %
% Insights enhance understanding of LLM-driven influence operations, advocating for future models with adaptive agents and real-world network structures.

%to improve robustness and applicability.

% \appendix

\section*{Limitations}
The study is based on simulated interactions rather than real-world datasets from social media or online discourse. Validating findings with empirical data would enhance their applicability. The study relies on the BCM, which, while effective, does not capture more complex psychological and social dynamics influencing opinion formation, such as emotional contagion, identity-based biases, or network homophily.

\section*{Ethical Statement}
\label{sec:appendix}
This study, involving the simulated generation of misinformation and counter-misinformation, necessitates careful ethical considerations.  To prevent potential misuse, the specific prompts used to generate misinformation via LLMs cannot be disclosed.  Disclosure could inadvertently facilitate real-world misinformation spread.  Mitigation strategies included containing all generated content within a closed experimental environment, focusing the research objective on analysis and countermeasure development (not propagation), and ensuring that any released findings emphasise generalisable insights rather than specific prompt engineering techniques.  We underscore that responsible misinformation modelling research is paramount, ensuring that the development of countermeasures does not contribute to the problem itself.


% This study involved the generation of both misinformation and counter-misinformation within a controlled simulation environment. Due to the potential for misuse, we cannot disclose the exact prompts used to generate misinformation via LLMs. Releasing these prompts could advertently or inadvertently facilitate the spread of misinformation in real-world settings.

% To mitigate ethical risks, we ensured that:

% \begin{itemize}
%     \item The misinformation generated was contained within a closed experimental setup and was not disseminated publicly due to risks of potential misuse.
% \item The studyâ€™s goal was to analyse and counter misinformation, not to propagate it.
% \item Any release of findings will focus on generalizable insights rather than specific adversarial prompt engineering methods.
% \item We emphasize that research in misinformation modeling must be conducted responsibly, ensuring that the development of countermeasures does not contribute to the very problem it seeks to mitigate.
% \end{itemize}




% Bibliography entries for the entire Anthology, followed by custom entries
\bibliography{short}
% Custom bibliography entries only
%\bibliography{short-bib}



\end{document}
