\section{Related Work}
\label{sec:related}

\subsection{Human Pose Estimation in Images}
 	\red{With the recent advancements in neural architectures, the deep learning models (\emph{e.g.}, CNNs~\cite{he2016deep} or Transformers~\cite{dosovitskiy2020image, vaswani2017attention}) have dominated various computer vision tasks such as salient object detection~\cite{wang2022hybrid}, action recognition~\cite{cheng2022tallformer}, and human pose estimation~\cite{xu2023can, zhou2024human}.} 	
 	\red{The deep learning-based pose estimation methods can be broadly divided into two streams: bottom-up~\cite{du2022hierarchical} and top-down~\cite{fang2017rmpe}.} \textbf{(i)} \emph{Bottom-up approaches} first detect all individual body joints and then group them to form the entire human pose. OpenPose~\cite{Cao_2017_CVPR} proposes a dual-branch framework that employs cascaded convolutions to localize body joints and affinity fields to encode part-to-part associations. PifPaf~\cite{kreiss2019pifpaf} leverages a Part Intensity Field to detect human body parts and designs a Part Association Field to associate body parts with each other. \textbf{(ii)} \emph{Top-down approaches} first detect bounding boxes for all persons and then estimate the human pose within each bounding box region. HRNet~\cite{wang2020deep} introduces a high-resolution network that maintains high-resolution feature maps in all network stages. TokenPose~\cite{li2021tokenpose} proposes token representations to explicitly learn the anatomical constraints between every two joints. ViTPose~\cite{xu2023vitpose++} employs plain vision transformers to extract strong representations for pose estimation, demonstrating superior performance in multiple benchmarks.  SUNNet~\cite{xu2021sunnet} employs human parsing information to improve the performance of pose estimation. MSPose~\cite{yuan2024multi} leverages multiple supervision to explore data-limited human pose estimation. 
 	


 \subsection{Human Pose Estimation in Videos}
  Existing image-based models struggle to handle video inputs effectively as they cannot utilize temporal information across frames~\cite{liu2021deep}. To tackle this problem, several studies propose to introduce temporal representations on top of a CNN backbone. TDMI~\cite{feng2023mutual} adopts temporal feature differences to model pixel motions and employs convolutions to aggregate motion and appearance features. Flow-based methods~\cite{song2017thin, pfister2015flowing} compute dense optical flow among frames and utilize such flow-based clues to refine the heatmap estimation. \red{DCPose~\cite{liu2021deep} and PoseWarper~\cite{bertasius2019learning} compute pixel-wise motion offsets between different frames and leverage motion fields to guide accurate pose resampling.} 
   Another line of literature \cite{wang2020combining, liu2022temporal} introduces implicit motion compensation. FAMI-Pose~\cite{liu2022temporal} proposes a framework which first aligns the features of each supporting frame to the keyframe at the pixel level, and then summates the overall feature maps to estimate pose heatmaps. 
  
  \red{ As the above methods strongly rely on pixel-level dynamics and neglect semantic motion patterns, they are particularly vulnerable to image quality degradations such as occlusion or blur. Furthermore, these methods crudely fuse motion and spatial features, which cannot fully leverage these two complementary modalities. In this paper, we aim to design a novel temporal modeling paradigm to learn multi-level semantical motion dynamics that are more robust to pixel degradations. On the other hand, inspired by previous works that focus on fully integrating multi-source information (\emph{e.g.}, SDNet~\cite{liu2023transcending} fuses scene clues and object information, SRAL~\cite{liu2023distilling} combines knowledge of super-resolution and salient detection), we propose a dense spatio-temporal collaboration strategy to take full advantage of motion and spatial features for VHPE.   
}



 \begin{figure*}
\begin{center}
\includegraphics[width=.97\linewidth]{Figures/Pipeline.pdf}
\end{center}
\caption{\textbf{Overall pipeline} of our proposed framework. The goal is to estimate the human pose in the keyframe. Given the input sequence, we first extract their spatial features using a visual encoder. The resulting feature tokens are then processed via two modules (b) MLSME and (c) SMML for motion feature extraction and spatial-motion feature aggregation. Finally, a detection head is employed to produce the final pose estimation $\hat{\mb{H}}_t^{i}$.}
\label{fig:pipeline}
\end{figure*}