\section{Related Work}
\label{sec:related}

\subsection{Human Pose Estimation in Images}
 	\red{With the recent advancements in neural architectures, the deep learning models (\emph{e.g.}, CNNs____ or Transformers____) have dominated various computer vision tasks such as salient object detection____, action recognition____, and human pose estimation____.} 	
 	\red{The deep learning-based pose estimation methods can be broadly divided into two streams: bottom-up____ and top-down____.} \textbf{(i)} \emph{Bottom-up approaches} first detect all individual body joints and then group them to form the entire human pose. OpenPose____ proposes a dual-branch framework that employs cascaded convolutions to localize body joints and affinity fields to encode part-to-part associations. PifPaf____ leverages a Part Intensity Field to detect human body parts and designs a Part Association Field to associate body parts with each other. \textbf{(ii)} \emph{Top-down approaches} first detect bounding boxes for all persons and then estimate the human pose within each bounding box region. HRNet____ introduces a high-resolution network that maintains high-resolution feature maps in all network stages. TokenPose____ proposes token representations to explicitly learn the anatomical constraints between every two joints. ViTPose____ employs plain vision transformers to extract strong representations for pose estimation, demonstrating superior performance in multiple benchmarks.  SUNNet____ employs human parsing information to improve the performance of pose estimation. MSPose____ leverages multiple supervision to explore data-limited human pose estimation. 
 	


 \subsection{Human Pose Estimation in Videos}
  Existing image-based models struggle to handle video inputs effectively as they cannot utilize temporal information across frames____. To tackle this problem, several studies propose to introduce temporal representations on top of a CNN backbone. TDMI____ adopts temporal feature differences to model pixel motions and employs convolutions to aggregate motion and appearance features. Flow-based methods____ compute dense optical flow among frames and utilize such flow-based clues to refine the heatmap estimation. \red{DCPose____ and PoseWarper____ compute pixel-wise motion offsets between different frames and leverage motion fields to guide accurate pose resampling.} 
   Another line of literature ____ introduces implicit motion compensation. FAMI-Pose____ proposes a framework which first aligns the features of each supporting frame to the keyframe at the pixel level, and then summates the overall feature maps to estimate pose heatmaps. 
  
  \red{ As the above methods strongly rely on pixel-level dynamics and neglect semantic motion patterns, they are particularly vulnerable to image quality degradations such as occlusion or blur. Furthermore, these methods crudely fuse motion and spatial features, which cannot fully leverage these two complementary modalities. In this paper, we aim to design a novel temporal modeling paradigm to learn multi-level semantical motion dynamics that are more robust to pixel degradations. On the other hand, inspired by previous works that focus on fully integrating multi-source information (\emph{e.g.}, SDNet____ fuses scene clues and object information, SRAL____ combines knowledge of super-resolution and salient detection), we propose a dense spatio-temporal collaboration strategy to take full advantage of motion and spatial features for VHPE.   
}



 \begin{figure*}
\begin{center}
\includegraphics[width=.97\linewidth]{Figures/Pipeline.pdf}
\end{center}
\caption{\textbf{Overall pipeline} of our proposed framework. The goal is to estimate the human pose in the keyframe. Given the input sequence, we first extract their spatial features using a visual encoder. The resulting feature tokens are then processed via two modules (b) MLSME and (c) SMML for motion feature extraction and spatial-motion feature aggregation. Finally, a detection head is employed to produce the final pose estimation $\hat{\mb{H}}_t^{i}$.}
\label{fig:pipeline}
\end{figure*}