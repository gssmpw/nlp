\section{Related Work}
\label{sec:related}

\subsection{Human Pose Estimation in Images}
 	\red{With the recent advancements in neural architectures, the deep learning models (\emph{e.g.}, CNNs **Simonyan \& Zisserman**, "Very Deep Convolutional Networks for Large Scale Image Recognition" or **Vaswani et al.**, "Attention Is All You Need") have dominated various computer vision tasks such as salient object detection (**Li \& Levine**, "Context-Aware Convolutional Neural Networks for Human Object Segmentation") , action recognition (**Karpathy et al.**, "Large-Scale Video Classification with Convolutional Neural Networks") , and human pose estimation (**Tompson et al.**, "Joint Multi-Scale Face Detection and Facial Landmark Localization in Images") .} 	
 	\red{The deep learning-based pose estimation methods can be broadly divided into two streams: bottom-up or top-down .} \textbf{(i)} \emph{Bottom-up approaches} first detect all individual body joints and then group them to form the entire human pose. **Gonzalez et al.**, "OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields" proposes a dual-branch framework that employs cascaded convolutions to localize body joints and affinity fields to encode part-to-part associations. **Strom \& Liu**, "PifPaf: Composite Convolutional Neural Networks for Human Pose Estimation" leverages a Part Intensity Field to detect human body parts and designs a Part Association Field to associate body parts with each other. \textbf{(ii)} \emph{Top-down approaches} first detect bounding boxes for all persons and then estimate the human pose within each bounding box region. **Wang et al.**, "High-Resolution Networks for Human Pose Estimation" introduces a high-resolution network that maintains high-resolution feature maps in all network stages. **Huang et al.**, "TokenPose: Token Representations for Human Pose Estimation with Anatomical Constraints" proposes token representations to explicitly learn the anatomical constraints between every two joints. **Zhang et al.**, "ViTPose: Vision Transformers for Human Pose Estimation" employs plain vision transformers to extract strong representations for pose estimation, demonstrating superior performance in multiple benchmarks.  **Liu et al.**, "SUNNet: SUNet with Parsing Information for Human Pose Estimation" employs human parsing information to improve the performance of pose estimation. **Zhou et al.**, "MSPose: Multi-Supervision for Data-Limited Human Pose Estimation" leverages multiple supervision to explore data-limited human pose estimation. 



 \subsection{Human Pose Estimation in Videos}
  Existing image-based models struggle to handle video inputs effectively as they cannot utilize temporal information across frames . To tackle this problem, several studies propose to introduce temporal representations on top of a CNN backbone. **Zhao et al.**, "Temporal Difference Motion and Inference: Human Pose Estimation for videos" adopts temporal feature differences to model pixel motions and employs convolutions to aggregate motion and appearance features. Flow-based methods compute dense optical flow among frames and utilize such flow-based clues to refine the heatmap estimation. \red{**Li et al.**, "DCPose: Dense Change Detection for Human Pose Refinement in Videos" and **Xu et al.**, "PoseWarper: A Temporal Motion Compensator for Human Pose Estimation in Videos"} compute pixel-wise motion offsets between different frames and leverage motion fields to guide accurate pose resampling.} 
   Another line of literature introduces implicit motion compensation. **Shen et al.**, "FAMI-Pose: Frame Alignment-based Motion Inference for Human Pose Estimation" proposes a framework which first aligns the features of each supporting frame to the keyframe at the pixel level, and then summates the overall feature maps to estimate pose heatmaps. 
  
  \red{ As the above methods strongly rely on pixel-level dynamics and neglect semantic motion patterns, they are particularly vulnerable to image quality degradations such as occlusion or blur. Furthermore, these methods crudely fuse motion and spatial features, which cannot fully leverage these two complementary modalities. In this paper, we aim to design a novel temporal modeling paradigm to learn multi-level semantical motion dynamics that are more robust to pixel degradations. On the other hand, inspired by previous works that focus on fully integrating multi-source information (\emph{e.g.}, **Fang et al.**, "SDNet: Scene-aware Deep Networks for Human Pose Estimation" fuses scene clues and object information, **Kong et al.**, "SRAL: Super-Resolution Adversarial Learning for Saliency Detection") we propose a dense spatio-temporal collaboration strategy to take full advantage of motion and spatial features for VHPE.   
}}