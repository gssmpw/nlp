\section{Randomized trace estimators for parameter-dependent matrices}
\label{sec:analysis}

Consider a general parameter-dependent matrix
$\mtx{B}(t) \in \mathbb{R}^{n \times n}$ for which each
entry $b_{ij}(t)$ is a 
continuous function on a bounded interval $[a,b]$. In this section, we 
describe and analyze methods that estimate, for several values of $t$, the 
trace $\Trace(\mtx{B}(t)) = b_{11}(t)+ \cdots + b_{nn}(t)$
from matrix-vector products of $\mtx{B}(t)$ with \emph{constant} random vectors. When $\mtx{B}$ is constant, our methods reduce to known algorithms. When $\mtx{B}$ is variable, important differences arise from the fact that the random vectors do not depend on $t$. On the one hand, as we will see in~\refsec{sec:application}, this allows one to reuse computations. On the other hand, this significantly complicates the analysis, which will be the main focus of this section.

\subsection{Methods}
\label{subsec:methods}

\paragraph{Girard-Hutchinson estimator.} One of the simplest methods for trace estimation, the Girard-Hutchinson estimator~\cite{girard-1989-fast-montecarlo,hutchinson-1990-stochastic-estimator} proceeds by sampling $n_{\mtx{\Psi}}$ independent Gaussian random vectors $\vct{\psi}_1,\dots, \vct{\psi}_{n_{\mtx{\Psi}}} \in \mathbb{R}^{n}$
and computes the approximation 
\begin{equation}
    \Trace(\mtx{B}(t)) \approx \Hutch{\mtx{\Psi}}(\mtx{B}(t))
    := \frac{1}{n_{\mtx{\Psi}}} \sum_{j=1}^{n_{\mtx{\Psi}}} \vct{\psi}_j^{\top} \mtx{B}(t) \vct{\psi}_j
    = \frac{1}{n_{\mtx{\Psi}}} \Trace( \mtx{\Psi}^{\top} \mtx{B}(t) \mtx{\Psi}).
    \label{equ:hutchinson-trace-estimator}
\end{equation}
Here, $\mtx{\Psi} := [\vct{\psi}_1 ~ \cdots ~ \vct{\psi}_{n_{\mtx{\Psi}}}] \in \mathbb{R}^{n \times n_{\mtx{\Psi}}}$ is a Gaussian random matrix, that is, 
its entries are independent standard normal random variables.
There are other sensible choices for the probability distribution, such as the Rademacher distribution. Our theoretical developments, however, only apply to the Gaussian case.

The Girard-Hutchinson estimator is a popular choice for spectral density estimation; it has been used in the Haydock method \cite{haydock-1972-electronic-structure}, as well as the Delta-Gauss-Legendre and Delta-Gauss-Chebyshev methods from~\cite{lin-2016-approximating-spectral, lin-2017-randomized-estimation}. However, it shares the disadvantage of Monte Carlo methods: Its error is proportional to $n^{-\sfrac{1}{2}}_{\mtx{\Psi}}$, making it expensive in applications that require decent accuracy.

\paragraph{Nyström estimator.} Given a good low-rank approximation of a matrix, the trace can be cheaply estimated by simply taking the trace of that low-rank approximation. In the context of spectral density estimation, the use of a non-negative smoothing kernel implies that $\mtx{B}(t)$ is symmetric positive semi-definite (SPSD) for every $t$. This makes the Nyström method
from~\cite{gittens-2013-revisiting-nystrom} well suited for constructing a (randomized) low-rank approximation.
Given a Gaussian random matrix $\mtx{\Omega} \in \mathbb{R}^{n \times n_{\mtx{\Omega}}}$, Nyström computes the approximation
\begin{equation}
    \mtx{B}(t) \approx \Nystr{\mtx{\Omega}}{\mtx{B}}(t) := (\mtx{B}(t) \mtx{\Omega}) (\mtx{\Omega}^{\top} \mtx{B}(t) \mtx{\Omega})^{\dagger} (\mtx{B}(t) \mtx{\Omega})^{\top}.
    \label{equ:nystrom-approximation}
\end{equation}
Related randomized low-rank approximations for general parameter-dependent matrices have been analyzed in~\cite{kressner-2024-randomized-lowrank}.
Exploiting the invariance of the trace under cyclic permutations, taking the trace of~\refequ{equ:nystrom-approximation} gives rise to the Nyström estimator
\begin{equation}
    \Trace(\mtx{B}(t)) \approx \Trace(\Nystr{\mtx{\Omega}}{\mtx{B}}(t)) = \Trace( (\mtx{\Omega}^{\top} \mtx{B}(t) \mtx{\Omega})^{\dagger} ( \mtx{\Omega}^{\top} \mtx{B}(t)^2 \mtx{\Omega})).
    \label{equ:nystrom-trace-estimator}
\end{equation}
This estimator forms the basis of the so-called spectrum sweeping method \cite{lin-2017-randomized-estimation}. 

\paragraph{Nyström++ estimator.}
The Nyström estimator~\refequ{equ:nystrom-trace-estimator} relies on the feasibility of low-rank approximation, that is, the singular values of $\mtx{B}(t)$ need to decay sufficiently quickly.
If this property cannot be ensured, it is preferable to use an estimator that combines the Girard-Hutchinson and Nyström estimators.
The combination suggested in~\cite{lin-2017-randomized-estimation} applies Girard-Hutchinson to correct the error of the Nyström approximation~\refequ{equ:nystrom-approximation}:
\begin{equation}
    \Nystrpp{\mtx{\Omega}}{\mtx{\Psi}}(\mtx{B}(t)) = \Trace(\Nystr{\mtx{\Omega}}{\mtx{B}}(t)) + \Hutch{\mtx{\Psi}}(\mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t)),
    \label{equ:nystrompp-trace-estimator}
\end{equation}
where $\mtx{\Psi} \in \mathbb{R}^{n \times n_{\mtx{\Psi}}}$, $\mtx{\Omega} \in \mathbb{R}^{n \times n_{\mtx{\Omega}}}$ are independent Gaussian random matrices. 

For constant $\mtx{B}$, the estimator~\refequ{equ:nystrompp-trace-estimator} coincides with the Nyström++ estimator from~\cite{persson-2022-improved-variants}, which is based on the Hutch++ estimator~\cite{meyer-2021-hutch-optimal}. In this situation (constant $\mtx{B}$), these estimators were both shown to achieve a relative $\varepsilon$-error with $\mathcal{O}(\varepsilon^{-1})$ matrix-vector products only, independent of the singular value decay of $\mtx{B}$.

At this point, we acknowledge the existence of the XNysTrace estimator from \cite{epperly-2024-xtrace-making} which oftentimes outperforms the Nyström++ estimator. Moreover, a straightforward extension of the XNysTrace estimator to the parameter-dependent setting seems to be in reach. However, what distinguishes \refalg{alg:nystrom-chebyshev-pp} for estimating spectral densities are two key observations --- using the cyclic invariance of the trace and the affine linear form of Chebyshev approximations (see \refsec{subsubsec:chebyshev-nystrom-implementation} for details) --- which it exploits to significantly speed up the computation. Unfortunately, we do not see a way of marrying these observations with the efficient implementation of XNysTrace described in \cite[Section 2.2]{epperly-2024-xtrace-making}, which limits its suitability for spectral density estimation.

\subsection{Error bounds}

In this section, we derive bounds on the $L^1$-error (with respect to $t$) for each of the trace estimators from \refsec{subsec:methods}. Our bounds parallel existing results in the constant case. As in~\cite{kressner-2024-randomized-lowrank}, our proofs incorporate parameter dependence by proceeding through moments $\mathbb{E}^p[X] = (\mathbb{E}[|X|^p])^{\sfrac{1}{p}}$ of certain random variables $X$.

\subsubsection{Girard-Hutchinson estimator}
\label{subsec:hutchinson}

We first derive an $L^1$-error bound for the Girard-Hutchinson estimator~\refequ{equ:hutchinson-trace-estimator} that is relative to the $L^1$-norms of the Frobenius and spectral norms of $\mtx{B}(t)$, defined by
\begin{equation*}
 \|\mtx{B}(t)\|_{F,1} := \int_{a}^{b} \|\mtx{B}(t)\|_{F}~\mathrm{d}t, \quad  \|\mtx{B}(t)\|_{2,1} := \int_{a}^{b} \|\mtx{B}(t)\|_{2}~\mathrm{d}t.
\end{equation*}

\begin{theorem}{Girard-Hutchinson estimator for parameter-dependent matrices}{hutchinson}
For a symmetric matrix $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ that depends continuously on $t \in [a, b]$, consider the  Girard-Hutchinson estimator~\refequ{equ:hutchinson-trace-estimator} with an $n\times n_{\mtx{\Psi}}$ Gaussian random matrix $\mtx{\Psi}$. Then for any $p \geq 1$, $p \in \mathbb{R}$ and $\gamma \geq 1$, the following bound holds with probability at least $1 - \gamma^{-p}$:
\begin{equation}
    \int_{a}^{b} |\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))|~\mathrm{d}t \leq 4 \gamma \Big( \sqrt{\frac{p}{n_{\mtx{\Psi}}}}  \|\mtx{B}(t)\|_{F,1} + \frac{2 p}{n_{\mtx{\Psi}}} \|\mtx{B}(t)\|_{2,1} \Big).
    \label{equ:girard-hutchinson-bound}
\end{equation}
In particular, given $\varepsilon \in (0, 1)$ and $\delta \in (0, e^{-1}]$, the bound $\int_{a}^{b} | \Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t)) | ~\mathrm{d}t \leq \varepsilon \|\mtx{B}(t)\|_{F,1}$ holds 
    with probability at least $1-\delta$ if $n_{\mtx{\Psi}} = \mathcal{O}(\varepsilon^{-2} \log(\delta^{-1}))$.
\end{theorem}
\begin{proof}
    Let us first consider $t$ fixed. From the proof of \cite[Theorem 1]{cortinovis-2022-randomized-trace}, we know that $\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))$ is a $(2 \lVert \mtx{B}(t) \rVert _F^2 / n_{\mtx{\Psi}}, 2 \lVert \mtx{B}(t) \rVert _2 / n_{\mtx{\Psi}})$-sub-gamma random variable. Therefore, \reflem{lem:sub-gamma-moments} yields
    \begin{equation}
        \mathbb{E}^{p}[\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))] \leq 4  \sqrt{\frac{p}{n_{\mtx{\Psi}}}} \lVert \mtx{B}(t) \rVert _F + 8 \frac{p}{n_{\mtx{\Psi}}} \lVert \mtx{B}(t) \rVert _2.
        \label{equ:moment-bound-hutchinson}
    \end{equation}

    To address the parameter-dependent case, we first note that the continuity assumption implies that $\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))$ is measurable. Therefore, we can apply Minkowski's integral inequality~\cite[Theorem 202]{hardy-1952-inequalities} to conclude from the moment bound~\refequ{equ:moment-bound-hutchinson} that
    \begin{align*}
        \mathbb{E}^{p}\left[ \int_{a}^{b} |\Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t))|~\mathrm{d}t  \right]
        &\leq \int_{a}^{b} \mathbb{E}^{p}\left[ \Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t)) \right]~\mathrm{d}t \notag \\
        &\leq 4 \sqrt{\frac{p}{n_{\mtx{\Psi}}}} \lVert \mtx{B}(t) \rVert_{F,1} + 8 \frac{p}{n_{\mtx{\Psi}}} \lVert \mtx{B}(t) \rVert_{2,1}.
    \end{align*}
    This implies~\refequ{equ:girard-hutchinson-bound} by Markov's inequality.

    Bounding $\lVert \mtx{B}(t) \rVert _2 \leq \lVert \mtx{B}(t) \rVert _F$, setting $\gamma = e$, and choosing $p = \log(\delta^{-1}) \geq 1$, the second part of the theorem follows by taking,  e.g., $n_{\mtx{\Psi}} = \lceil (8 e)^2 \log(\delta^{-1}) \varepsilon^{-2} \rceil$.
\end{proof}

\refthm{thm:hutchinson} parallels an analogous result for the constant case~\cite[Lemma 2.1]{meyer-2021-hutch-optimal}, except for a slightly more restricted choice of the failure probability $\delta$.

\begin{remark}
    A variant of \refthm{thm:hutchinson} can be derived when assuming that $\mtx{B}(t)$ is SPSD and nonzero for every $t \in [a, b]$. After dividing both sides of \refequ{equ:moment-bound-hutchinson} by $\Trace(\mtx{B}(t))$, one can apply the remaining steps of the proof to deduce that
    \begin{equation*}
        \int_{a}^{b} \frac{| \Trace(\mtx{B}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t)) |}{\Trace(\mtx{B}(t))} ~\mathrm{d}t < \varepsilon
    \end{equation*}
    holds with probability at least $1 - \delta$ if $n_{\mtx{\Psi}} = \mathcal{O}(\log(\delta^{-1}) \varepsilon^{-2} \rho^2)$. Here, 
    the quantity $
        \rho := \frac{1}{b-a} \int_{a}^{b} \frac{\lVert \mtx{B}(t) \rVert _F}{\Trace(\mtx{B}(t))} ~\mathrm{d}t \in [1,n]
    $
is small when the singular values of $\mtx{B}(t)$ decay slowly for the majority of $t\in [a,b]$ and large otherwise. This parallels an analogous result for constant matrices; see~\cite[Remark 2]{cortinovis-2022-randomized-trace}.
\end{remark}

\subsubsection{Nyström estimator}
\label{subsec:nystrom}

As a consequence of~\cite{kressner-2024-randomized-lowrank}, the following result confirms the intuition that the Nystr\"om estimator is very well suited when the singular values of $\mtx{B}(t)$ decay quickly. More specifically, it shows that the $L^1$-error of the estimator stays on the level of the best rank-$r$ approximation error of $B(t)$, measured in the nuclear norm.

\begin{theorem}{Nyström estimator for parameter-dependent matrices}{nystrom}
    Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ be SPSD and continuous for $t \in [a, b]$. For integers $r \geq 2$ and $n_{\mtx{\Omega}} \geq r + 4$, consider the Nystr\"om approximation $\Nystr{\mtx{\Omega}}{\mtx{B}}(t)$ defined in~\refequ{equ:nystrom-approximation} with an $n\times n_{\mtx{\Omega}}$ Gaussian random matrix $\mtx{\Omega}$. Then, for any $\gamma \geq 1$, the bound 
    \begin{equation*}
        \int_{a}^{b} \big| \Trace(\mtx{B}(t)) - \Trace(\Nystr{\mtx{\Omega}}{\mtx{B}}(t)) \big| ~\mathrm{d}t
        \leq \gamma^2 (1 + r) \int_{a}^{b} \sum_{i = r+1}^{n} \sigma_i(\mtx{B}(t)) ~\mathrm{d}t.
    \end{equation*}
    holds with probability at least $1 - \gamma^{-(n_{\mtx{\Omega}} - r)}$.
\end{theorem}
\begin{proof}
Using that $\mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t)$ is SPSD (see, e.g.,~\cite[Lemma 2.1]{frangella-2023-randomized-nystrom}) we obtain
    \begin{align*}
        \big| \Trace(\mtx{B}(t)) - \Trace(\Nystr{\mtx{\Omega}}{\mtx{B}}(t)) \big|
        & = \big| \Trace(\mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t)) \big|
        = \lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _{\ast} \\
        & = \lVert (\mtx{I}_n - \mtx{\Pi}_{\mtx{B}(t)^{\sfrac{1}{2}} \mtx{\Omega}}) \mtx{B}(t)^{\sfrac{1}{2}} \rVert _F^2,
    \end{align*}
    where $\lVert \cdot \rVert _{\ast}$ denotes the nuclear norm and we used \cite[Theorem 1]{gittens-2011-spectral-norm} in the last equality. This allows us to apply \cite[Theorem 5]{kressner-2024-randomized-lowrank}, which immediately implies the statement claimed by the theorem.
\end{proof}

Compared to the constant case~\cite[Theorem 8.1]{tropp-2023-randomized-algorithms}, the bound of \refthm{thm:nystrom} features an additional factor $1+r$, which cannot be compensated for with oversampling.

\subsection{Nyström++ estimator for parameter-dependent matrices}
\label{subsec:nystrom-pp}

\refthm{thm:hutchinson} shows that the Girard-Hutchinson estimator achieves a relative $\varepsilon$-error when using $\mathcal{O}(\varepsilon^{-2})$ queries. The aim of this section is to show that the Nyström++ estimator~\refequ{equ:nystrompp-trace-estimator} improves this to $\mathcal{O}(\varepsilon^{-1})$ queries (that is, matrix-vector products with random vectors), \emph{without} requiring any assumption on the singular values of $\mtx{B}(t)$. This parallels existing results for the constant case~\cite{meyer-2021-hutch-optimal,persson-2022-improved-variants}. In particular, \cite[Theorem 3.4]{persson-2022-improved-variants} shows that $|\Trace(\mtx{B}) - \Nystrpp{\mtx{\Omega}}{\mtx{\Psi}}(\mtx{B})| \leq \varepsilon \Trace(\mtx{B})$ holds with high probability for (constant) SPSD $\mtx{B}$ when $n_{\mtx{\Psi}} = n_{\mtx{\Omega}} = \mathcal{O}(\varepsilon^{-1})$ in the Nyström++ estimator $\Nystrpp{\mtx{\Omega}}{\mtx{\Psi}}(\mtx{B})$ from~\refequ{equ:nystrompp-trace-estimator}. 

Our analysis mimics the strategy from~\cite{meyer-2021-hutch-optimal}, by first establishing a relative Frobenius norm error proportional to $n_{\mtx{\Omega}}^{-\sfrac{1}{2}}$ for the parameter-dependent Nyström approximation.

\begin{lemma}{Nyström approximation for parameter-dependent matrices}{nystrom}
    Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ be SPSD and continuous for $t \in [a, b]$. Consider the Nyström approximation $\Nystr{\mtx{\Omega}}{\mtx{B}}(t)$ with a Gaussian random matrix $\mtx{\Omega} \in \mathbb{R}^{n \times n_{\mtx{\Omega}}}$ and \emph{even} $n_{\mtx{\Omega}} \geq 4$.
    Then, for any $\gamma \geq 1$, the bound
    \begin{equation}
        \int_{a}^{b} \lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _F~\mathrm{d}t \leq \gamma \frac{c}{\sqrt{n_{\mtx{\Omega}}}} \int_{a}^{b} \Trace(\mtx{B}(t))~\mathrm{d}t
        \label{equ:nystrompp-theorem-bound}
    \end{equation}
        holds with probability at least $1 - \gamma^{-\sfrac{n_{\mtx{\Omega}}}{4}}$ for 
        $c = 154$. In particular, given $\varepsilon > 0$ and $\delta \in (0, 1)$, the bound $\int_{a}^{b} \lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _F~\mathrm{d}t \leq \varepsilon \int_{a}^{b} \Trace(\mtx{B}(t))~\mathrm{d}t$ holds with probability at least $1-\delta$ if $n_{\mtx{\Omega}} = \mathcal{O}(\varepsilon^{-2} + \log(\delta^{-1}))$.
\end{lemma}

\begin{proof}
For fixed $t$, consider the spectral decomposition $\mtx{B}(t) = \mtx{U} \mtx{\Lambda} \mtx{U}^{\top}$ with  
$\mtx{\Lambda} = \operatorname{diag}(\lambda_1, \dots, \lambda_n)$ and $\lambda_1 \ge \dots \ge \lambda_n \ge 0$. For $k < n_{\mtx{\Omega}}$ (chosen arbitrarily at the moment), partition
    \begin{equation*}
        \rule[\dimexpr-2ex-\ht\strutbox]{0pt}{\dimexpr2ex+4ex+\baselineskip}
        \mtx{U} = \begin{bmatrix}
            \smash{\underbrace{\mtx{U}_1}_{n \times k}} & \smash{\underbrace{\mtx{U}_2}_{n \times (n-k)}}
        \end{bmatrix}
        \quad \text{and} \quad
        \mtx{\Lambda} =
        \begin{bmatrix}
            \smash{\overbrace{\mtx{\Lambda}_1}^{k \times k}} & \\ & \smash{\underbrace{\mtx{\Lambda}_2}_{(n-k) \times (n-k)}}
        \end{bmatrix},
    \end{equation*}
    we set
    $\mtx{\Omega}_1 := \mtx{U}_1^{\top} \mtx{\Omega} \in \mathbb{R}^{k \times n_{\mtx{\Omega}}}$ and $\mtx{\Omega}_2 := \mtx{U}_2^{\top} \mtx{\Omega} \in \mathbb{R}^{(n - k) \times n_{\mtx{\Omega}}}$, which are independent Gaussian random matrices.
Applying \cite[Theorem B.1]{persson-2023-randomized-lowrank} for $f(x) = x$, see also proof of \cite[Corollary 8.2]{tropp-2023-randomized-algorithms}, yields the bound
    \begin{equation}
        \lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _F 
        \leq  \lVert \mtx{\Lambda}_2 \rVert _F + \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _{(4)}^2,
        \label{equ:nystrom-proof-persson-bonud}
    \end{equation}
    where $\lVert \cdot \rVert _{(4)}$ denotes the Schatten-4 norm. Following the proof of~\cite[Lemma 3]{meyer-2021-hutch-optimal}, we replace the the first term by a simpler bound:
    \begin{equation}
        \lVert \mtx{\Lambda}_2 \rVert _F
        \leq \sqrt{ \lambda_{k+1} (  \lambda_{k+1} + \cdots + \lambda_{n})}
        \leq \sqrt{ \Trace(\mtx{B}) / k \cdot \Trace(\mtx{B})}
        \leq \Trace(\mtx{B}) / \sqrt{k}.
        \label{equ:nystrom-proof-frobenius-trace}
    \end{equation}
    
To treat the second term in~\refequ{equ:nystrom-proof-persson-bonud}, we bound 
its $q$th moment using the independence of $\mtx{\Omega}_1$ and $\mtx{\Omega}_2$:
    \begin{equation}
        \mathbb{E}^{q}\left[ \big\| \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \big\|_{(4)}^2 \right]
        \leq \mathbb{E}^{q}\left[ \sqrt{k} \big\| \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \big\| _2^2 \right]
        \leq \sqrt{k} \mathbb{E}^{q}\left[ \big\| \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2  \big\|_2^2\right] \mathbb{E}^{q}\left[ \big\| \mtx{\Omega}_1^{\dagger} \big\|_2^2  \right].
        \label{equ:nystrom-proof-processed-tail}
    \end{equation}
    To continue from here, we set $k = n_{\mtx{\Omega}}/2$ and $q = n_{\mtx{\Omega}}/4$. By the calculations after~\cite[Eqn~(B.7)]{tropp-2023-randomized-algorithms}, one has
    \begin{equation*}
     \mathbb{E}\left[ \big\| ( \mtx{\Omega}_1 \mtx{\Omega}_1^{\top} )^{-1} \big\|_2^{\sfrac{n_{\mtx{\Omega}}}{4}} \right]
        \leq
        \left(1 + \frac{n_{\mtx{\Omega}}}{2}\right)
        \left( \frac{3}{4} n_{\mtx{\Omega}}\right)^{\sfrac{n_{\mtx{\Omega}}}{4}}
        \big( ( n_{\mtx{\Omega}} / 2 + 1)!\big)^{-\frac{n_{\mtx{\Omega}}}{2+n_{\mtx{\Omega}}}}.
    \end{equation*}
    Therefore,
    \begin{equation}
      \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}\left[ \big\| \mtx{\Omega}_1^{\dagger} \big\|_2^2 \right]
        = \mathbb{E}\left[ \big\| ( \mtx{\Omega}_1 \mtx{\Omega}_1^{\top} )^{-1} \big\|_2^{\sfrac{n_{\mtx{\Omega}}}{4}} \right]^{\sfrac{4}{n_{\mtx{\Omega}}}}
        \le \frac{3}{4}  
        \frac{e^4 n_{\mtx{\Omega}}}{(n_{\mtx{\Omega}} / 2+ 1)^2} \le 
        \frac{3}{4}  \frac{e^4}{n_{\mtx{\Omega}}},
        \label{equ:pinv-spectral-norm-bound}
    \end{equation}
    where we used $(1 + m)^{\sfrac{1}{m}} \leq e$ and $(m!)^{-\sfrac{1}{m}} \leq e/m$. Note that, in contrast 
    to the result of~\cite[Lemma B.3]{tropp-2023-randomized-algorithms}, this inequality is valid for arbitrarily large $n_{\mtx{\Omega}}$, at the expense of a slightly larger constant.

    The second factor in~\refequ{equ:nystrom-proof-processed-tail} is bounded using \reflem{lem:spectral-norm-moment} with $\mtx{A} = \mtx{\Lambda}_2^{\sfrac{1}{2}}$:
    \begin{equation*}
        \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}\left[ \big\| \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \big\|_2^2 \right]
        \leq \frac{5}{4} n_{\mtx{\Omega}} \Big( 2 \big\| \mtx{\Lambda}_2^{\sfrac{1}{2}} \big\|_2^2 + \frac{1}{n_{\mtx{\Omega}}} \big\| \mtx{\Lambda}_2^{\sfrac{1}{2}} \big\|_F^2 \Big).
    \end{equation*}
    Inserting this inequality and~\refequ{equ:pinv-spectral-norm-bound} into~\refequ{equ:nystrom-proof-processed-tail} gives
    \begin{equation*}
        \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}\left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _{(4)}^2 \right]
        \leq \frac{15 e^4}{16}  \sqrt{n_{\mtx{\Omega}}} \Big( 2 \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _2^2 + \frac{1}{n_{\mtx{\Omega}}} \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _F^2 \Big).
    \end{equation*}
    Bounding $\lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _2^2 = \lambda_{k+1}  \leq \Trace(\mtx{B})/k$ and $\lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \rVert _F^2 = \Trace(\mtx{\Lambda}_2) \le \Trace(\mtx{B})$ (recall that $k = n_{\mtx{\Omega}}/2$) yields
    \begin{equation}
        \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}\left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _{(4)}^2 \right]
        \leq \frac{15 e^4}{16}  \sqrt{n_{\mtx{\Omega}}} \Big( \frac{2}{n_{\mtx{\Omega}}} \Trace(\mtx{B}) + \frac{1}{n_{\mtx{\Omega}}} \Trace(\mtx{B}) \Big)
        \leq  \frac{154}{\sqrt{n_{\mtx{\Omega}}}} \Trace(\mtx{B}).
        \label{equ:nystrom-proof-tail-bound}
    \end{equation}
    
    Inserting~\refequ{equ:nystrom-proof-tail-bound} along with \refequ{equ:nystrom-proof-frobenius-trace} in \refequ{equ:nystrom-proof-persson-bonud}, letting $c=154$, and using the triangle inequality for $\mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}}[\cdot]$, we obtain
    \begin{equation}
        \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}} \left[\lVert \mtx{B} - \Nystr{\mtx{\Omega}}{\mtx{B}} \rVert _F \right]
        \leq \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}} \left[ \lVert \mtx{\Lambda}_2 \rVert _F \right] + \mathbb{E}^{\sfrac{n_{\mtx{\Omega}}}{4}} \left[ \lVert \mtx{\Lambda}_2^{\sfrac{1}{2}} \mtx{\Omega}_2 \mtx{\Omega}_1^{\dagger} \rVert _{(4)}^2 \right]
        \leq \frac{c}{\sqrt{n_{\mtx{\Omega}}}} \Trace(\mtx{B}).
        \label{equ:frobenius-moment-bound}
    \end{equation}
    As in \cite{kressner-2024-randomized-lowrank}, one can show that the error $\lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _F$ is measurable.
    As in the proof of~\refthm{thm:hutchinson}, the claimed bound~\refequ{equ:nystrompp-theorem-bound}
    follows from~\refequ{equ:frobenius-moment-bound} using Minkowski's integral inequality and Markov's inequality.

    Fixing $\gamma = e$ and letting $n_{\mtx{\Omega}} = \lceil(c e \varepsilon)^{-2}  + 4 \log(\delta^{-1}) \rceil$ establishes the second part of the theorem.
\end{proof}

Combining the bound on the Girard-Hutchinson estimator from~\refthm{thm:hutchinson} with the result of~\reflem{lem:nystrom} finally gives the desired error bound for the Nyström++ trace estimator.
\begin{theorem}{Nyström++ trace estimator for parameter-dependent matrices}{nystrom-pp}
    Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ be SPSD and continuous for $t \in [a, b]$. Choose $\delta \in (0, 2e^{-1}]$ and $\varepsilon \in (0, 1)$.
    Then, for $n_{\mtx{\Psi}} = n_{\mtx{\Omega}} = \mathcal{O}(\varepsilon^{-1} \log(\delta^{-1}))$,
    the inequality 
    \begin{equation*}
        \int_{a}^{b} | \Trace(\mtx{B}(t)) - \Nystrpp{\mtx{\Omega}}{\mtx{\Psi}}(\mtx{B}(t)) |~\mathrm{d}t
        \leq \varepsilon \int_{a}^{b} \Trace(\mtx{B}(t))~\mathrm{d}t
    \end{equation*}
    holds with probability at least $1 - \delta$.
\end{theorem}
\begin{proof}
    Choosing $n_{\mtx{\Psi}} = n_{\mtx{\Omega}} = \mathcal{O}(\tilde{\varepsilon}^{-2} \log(\tilde{\delta}^{-1}))$, \refthm{thm:hutchinson} and~\reflem{lem:nystrom} imply that
    \begin{align*}
        &\int_{a}^{b} | \Trace(\mtx{B}(t)) - \Nystrpp{\mtx{\Omega}}{\mtx{\Psi}}(\mtx{B}(t)) |~\mathrm{d}t 
        = \int_{a}^{b} | \Trace(\mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t)) - \Hutch{\mtx{\Psi}}(\mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t)) |~\mathrm{d}t  \\
        \leq\ & \tilde{\varepsilon} \int_{a}^{b} \lVert \mtx{B}(t) - \Nystr{\mtx{\Omega}}{\mtx{B}}(t) \rVert _F ~\mathrm{d}t       \leq \tilde{\varepsilon}^2 \int_{a}^{b} \Trace(\mtx{B}(t)) ~\mathrm{d}t 
    \end{align*}
    holds with probability at least $1 - 2\tilde{\delta}$. Taking $\varepsilon = \tilde{\varepsilon}^2$ and $\delta = 2 \tilde{\delta}$ concludes the proof.
\end{proof}

Comparing \refthm{thm:nystrom-pp} with the analogous result for constant matrices \cite[Theorem 3.4]{persson-2022-improved-variants}, we notice an additional factor of $\sqrt{\log(\delta^{-1})}$ in the required number of random vectors $n_{\mtx{\Omega}}$ and $n_{\mtx{\Psi}}$.
