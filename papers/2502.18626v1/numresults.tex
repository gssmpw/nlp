
\section{Numerical experiments}
\label{sec:results}


In this section, we verify the performance of the Chebyshev-Nyström++ algorithm proposed in this paper in various application scenarios, from electronic structure interaction, statistical thermodynamics, and neural network optimization.

While theoretically an important tool, we have observed that the preservation of non-negativity discussed in~\refsec{sec:preservnonneg} is not needed in practice. The (slight) indefiniteness of the standard Chebyshev approximation $g_{\sigma}^{(m)}(t \mtx{I}_n - \mtx{A})$ from~\refequ{equ:matrix-approximation} does not seem to impede the reliability of the Nyström++ estimator. Unless otherwise stated, we transform the input matrices such that their eigenvalues are contained in $[-1, 1]$, by approximating their spectrum with NumPy's Hermitian eigenvalue solver before we apply the methods to them. We compute spectral densities at $n_t = 100$ uniformly distributed values of the parameter $t \in [-1, 1]$. The integral involved in computing the $L^1$-errors is approximated with the composite midpoint quadrature rule using the $n_t$ values of $t$ as nodes. Both parameters introduced at the end \refsec{subsubsec:chebyshev-nystrom-implementation},the eigenvalue truncation threshold and the parameter for detecting a vanishing spectral density, are fixed to $10^{-5}$.

Our implementations are written in Python 3.12.3 using the packages NumPy 2.0.0 and SciPy 1.14.1. They are executed on a single thread of a GitHub-hosted Ubuntu runner with a 64-bit processor and 16 GB of RAM. 

\subsection{Spectral density for Hamiltonian of electronic structure}
\label{subsec:hamiltonian}

We use the electronic structure interaction example from~\cite[Section 6]{lin-2017-randomized-estimation}, which involves the second order finite difference discretization of the Hamiltonian
\begin{equation}
    \mathcal{H} = - \Delta + V
    \label{equ:electronic-hamiltonian}
\end{equation}
in three dimensions. The potential $V$ interacting with the electrons is generated by Gaussian wells
    $v(r) = v_0 e^{-\lambda r^2}$, 
with $v_0 = -4$ and $\lambda = 8$, centered in cells of side length $L=6$ which are stacked $n_c \in \mathbb{N}$ times in each spatial dimension; see \reffig{fig:gaussian-well}. The grid width of the finite difference discretization is fixed to $h=0.6$. For $n_c = 1$, this leads to a sparse matrix of size $1\,000 \times 1\,000$ and for $n_c = 2$ of size $8\,000 \times 8\,000$. This example represents an idealized model for the interaction of nuclei on a regular grid with electrons for a $k$-vector in the center of the first Brillouin zone. The distribution of the eigenvalues of the Hamiltonian --- its spectral density --- allows one to interpret the system's energy levels.

\begin{figure}[ht]
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/gaussian-well-1.pgf}
        \caption{$n_c=1$}
        \label{fig:gaussian-well-1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/gaussian-well-2.pgf}
        \caption{$n_c=2$}
        \label{fig:gaussian-well-2}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/gaussian-well-5.pgf}
        \caption{$n_c=5$}
        \label{fig:gaussian-well-5}
    \end{subfigure}
    \caption{Cross-sections of the periodic Gaussian wells potential $V$ for different $n_c$.}
    \label{fig:gaussian-well}
\end{figure}

\reffig{fig:convergence} displays the convergence behavior of Chebyshev-Nyström++ applied to the matrix $\mtx{A}$ resulting from the discretization of the Hamiltonian for $n_c = 1$ and $n_c = 2$. Unlike \cite{lin-2017-randomized-estimation}, where just a small portion of the spectral density is considered, we approximate the whole spectrum of $\mtx{A}$ (transformed to $[-1, 1]$) in accordance with our theoretical results (\refsec{subsubsec:chebyshev-nystrom-analysis}) and to demonstrate the effectiveness of our implementational particularities (\refsec{subsubsec:chebyshev-nystrom-implementation}). As a reference, we use the eigenvalues computed by NumPy's Hermitian eigenvalue solver.
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.495\textwidth}
        \input{plots/convergence-1.pgf}
        \caption{$n_c = 1.$}
        \label{fig:convergence-1}
    \end{subfigure}
    \begin{subfigure}[b]{0.495\textwidth}
        \input{plots/convergence-2.pgf}
        \caption{$n_c = 2.$}
        \label{fig:convergence-2}
    \end{subfigure}
    \caption{Error vs. number of random vectors $n_{\mtx{\Psi}} + n_{\mtx{\Omega}}$ (with fixed $\sigma=0.005$ and $m=2000$) when applying  Chebyshev-Nyström++ to the Hamiltonian matrix from~\refsec{subsec:hamiltonian}.}
    \label{fig:convergence}
\end{figure}%
As expected from \refthm{thm:hutchinson}, Girard-Hutchinson alone ($n_{\mtx{\Omega}} = 0$) requires $n_{\mtx{\Psi}} = \mathcal{O}(\varepsilon^{-2})$ samples to achieve an error of order $\varepsilon$. Because the eigenvalues of the matrix are spread out, we observe the better convergence of the Nyström approximation discussed at the end of \refsec{sec:application} once $n_{\mtx{\Omega}}$ is sufficiently large. At this point, we can observe that the errors stagnate around $10^{-6}$ as a consequence of the error made in the Chebyshev approximation. Due to the higher eigenvalue density of $\mtx{A}$ for $n_c = 2$, the stronger convergence can be observed to kick in later in this case, because a larger $n_{\mtx{\Omega}}$ is needed for a good Nyström approximation. In fact, for $n_c = 5$ the eigenvalue density is so high that to profit from this effect, $n_{\mtx{\Omega}}$ would need to be chosen so large that the computations become unfeasible.

For a fixed budget of $n_{\mtx{\Psi}} + n_{\mtx{\Omega}} = 80$ of random vectors, \reffig{fig:distribution} shows how the value of the smoothing parameter $\sigma$ the choice between Nyström ($n_{\mtx{\Omega}}$) and 
Girard-Hutchinson ($n_{\mtx{\Psi}}$). As expected, for small $\sigma$ the Nyström approximation alone suffices, but 
this is not an effective choice for large $\sigma$. Choosing $n_{\mtx{\Psi}} = n_{\mtx{\Omega}}$ constitutes a good compromise.


\begin{figure}[ht]
    \centering
    \input{plots/distribution.pgf}
    \caption{Error vs. $\sigma$ for a fixed budget of $80$ random vectors 
    when applying  Chebyshev-Nyström++ to the Hamiltonian matrix from~\refsec{subsec:hamiltonian} with $n_c = 1$.
The Chebyshev approximation error is made negligible by setting $m=16 / \sigma$ (see \reflem{lem:non-negative-chebyshev-error}).}
    \label{fig:distribution}
\end{figure}

\subsection{Comparison to Krylov-aware trace estimator}
\label{subsec:krylov-aware}

We notice that the Krylov-aware stochastic trace estimator from \cite[Algorithm 3.1]{chen-2023-krylovaware-stochastic} can also effectively be used in the setting of spectral density approximation. It also samples two Gaussian random matrices $\mtx{\Omega} \in \mathbb{R}^{n \times n_{\mtx{\Omega}}}$ and $\mtx{\Psi} \in \mathbb{R}^{n \times n_{\mtx{\Psi}}}$. $\mtx{\Omega}$. It first runs block Lanczos algorithm on $\mtx{A}$ with starting block $\mtx{\Omega}$ for $r$ iterations with reorthogonalization and subsequently $q$ without. The columns of $\mtx{\Psi}$ are then projected onto the complement of the block Krylov subspace generated by the Lanczos algorithm, and are afterwards used as starting vectors for $r$ iterations of the Lanczos algorithm on $\mtx{A}$. What is remarkable is that all these heavy computations are completely independent of the smoothing kernel $g_{\sigma}$ and the parameter $t$. Further, some algebraic manipulations show that similarly to the stochastic Lanczos quadrature \cite[Section 3]{ubaru-2017-fast-estimation}, we can compress the estimator into a quadrature. Since this quadrature is independent of $g_{\sigma}$ and $t$, it can be applied to different smoothing kernels $g_{\sigma}$ and to as many values of $t$ as needed --- at little additional cost.

We run our own, faithfully optimized implementation of the Krylov-aware stochastic trace estimator \cite[Algorithm 3.1]{chen-2023-krylovaware-stochastic} for different parameter settings on the example from \refsec{subsec:hamiltonian} and plot the approximation errors for logarithmically spaced values of the smoothing parameter $\sigma$ in \reffig{fig:krylov-aware-density}. For reference, we also report the error of Chebyshev-Nyström++ for parameters that lead to a comparable run-time. While the the Krylov-aware estimator is clearly an attractive alternative, it requires a careful choice of parameters adapted to  different regimes of $\sigma$ and still leads to a significantly larger error for moderately small values $\sigma$.

\begin{figure}[ht]
    \begin{minipage}[c]{.475\linewidth}
        \centering
        \input{plots/krylov_aware_density.pgf}
    \end{minipage}\hfill%
    \begin{minipage}[c]{.475\linewidth}
        \vspace{-35pt}
        \input{tables/krylov_aware_density_KA.tex}
        \newline
        \vspace{15pt}
        \newline
        \input{tables/krylov_aware_density_CN.tex}
    \end{minipage}
    \caption{For the example from \refsec{subsec:hamiltonian}, error vs. $\sigma$ the Krylov-Aware (KA) stochastic trace estimator with different parameter settings, compared to the Chebyshev-Nyström++ (CN++) method.}
    \label{fig:krylov-aware-density}
\end{figure}

\subsection{Spectral density of Hessian matrix of neural network}
\label{subsec:hessian}

When fitting the parameters of a neural network, it can be of interest to determine whether a (local) minimum has been found and whether this minimum is robust, i.e., whether a small perturbation of the parameters leads to a significant decrease in the fit of the neural network. Both properties are reflected in the eigenvalues of the Hessian matrix $\mtx{A}$ with respect to some loss function: if all eigenvalues are non-negative, then the loss attains a local minimum, and if, additionally, all of them are small, we talk about a flat minimum; one where small perturbations of the parameters do not cause large increases of the loss and are therefore associated with better generalization \cite{yao-2020-pyhessian-neural}. On the other hand, large eigenvalues in the spectrum indicate high sensitivity of the loss to small changes in the parameters. Those we want to avoid, which is the reason why we monitor them.

Since neural networks are usually parametrized by a large number of parameters, assembling the Hessian matrix explicitly and then computing its eigenvalues is clearly infeasible. On the other hand, one can cheaply compute matrix-vector products $\mtx{A} \vct{x}$ with the Hessian, at a cost that scales proportionally with the number of parameters \cite{pearlmutter-1994-fast-exact}.

To demonstrate that our algorithm can effectively be applied in the setting of neural network optimization, we approximate the spectral density of a small fully connected convolutional neural network with $6\,782$ parameters. We train this network on the handwritten digit classification task given by the MNIST dataset\footnote{Handwritten digit classification; taken from \url{http://yann.lecun.com/exdb/mnist}.} in PyTorch 2.4.1 in a standard way. We use the bound from \cite[Theorem 1]{zhou-2011-bounding-spectrum} to estimate bounds on the spectrum.

We plot the spectral density of the Hessian matrix of the untrained neural network, and at different stages of training in \reffig{fig:hessian-density}, as well as the corresponding mean squared error loss. It can be observed that the eigenvalues creep towards zero as the training proceeds. Furthermore, despite the loss steadily decreasing, the presence of relatively large eigenvalues in some epochs indicates a sharp minimum of the network, hence, unfavorable generalization properties.

\begin{figure}
    \begin{minipage}[c]{.49\linewidth}
        \centering
        \input{plots/hessian_density.pgf}
    \end{minipage}\hfill%
    \begin{minipage}[c]{.49\linewidth}
        \centering
        \input{plots/hessian_density_loss.pgf}
    \end{minipage}
    \caption{Mean squared error training loss (right) and corresponding approximate spectral density of the Hessian matrix of a fully connected convolutional neural network in different epochs of training on the MNIST dataset (left). The spectral density is approximated in $n_t=150$ uniformly spaced points using Chebyshev-Nyström++ with parameters $n_{\mtx{\Omega}} = 10$, $n_{\mtx{\Psi}} = 10$, $m = 1000$, and $\sigma = 0.005$.}
    \label{fig:hessian-density}
\end{figure}

\subsection{Approximation of the partition function of a quantum system}
\label{subsec:spin-chain}

To demonstrate that the Chebyshev-Nyström++ estimator is also effective for problems unrelated to spectral density estimation, we consider the problem of approximating the partition function $\Trace(\exp(-\beta \mtx{A}))$ for the $1\,048\,576 \times 1\,048\,576$ Hamiltonian $\mtx{A}$ of a planar Heisenberg spin chain from~\cite[Section 5.1]{chen-2023-krylovaware-stochastic}. We compute the pointwise error from the analytic solution as a function of the temperature $t\equiv \beta^{-1}$ and compare it to the error achieved by the Krylov-aware trace estimator in \reffig{fig:krylov-aware-spin}.

\begin{figure}[ht]
    \begin{minipage}[c]{.475\linewidth}
        \centering
        \input{plots/krylov_aware_spin.pgf}
    \end{minipage}\hfill%
    \begin{minipage}[c]{.475\linewidth}
        \vspace{-35pt}
        \input{tables/krylov_aware_spin_KA.tex}
        \newline
        \vspace{15pt}
        \newline
        \input{tables/krylov_aware_spin_CN.tex}
    \end{minipage}
    \caption{For the Heisenberg spin chain example, we compare the Chebyshev-Nyström++ (CN++) method with the Krylov-Aware stochastic trace estimator in the same configurations as in \cite[Table 5.1]{chen-2023-krylovaware-stochastic} (right). To this extent, we compute the $L^1$-error of the methods for various choices of the temperature parameter $\beta^{-1}$ (left).}
    \label{fig:krylov-aware-spin}
\end{figure}

Unlike for the spectral density example from \refsec{subsec:hamiltonian}, where the numerical rank of the matrix is relatively uniform across different $t$, this now changes changes wildly with the parameter $\beta$. Recall that the Chebyshev-Nyström++ method requires a fixed allocation of random vectors to the low-rank approximation for all values of the parameter $\beta$. Hence, both for large $\beta$, where the matrix $\exp(-\beta \mtx{A})$ has low numerical rank, and for small $\beta$, where the singular values decay very slowly, the approximation error is relatively large. Similarly, one degree $m$ for the Chebyshev approximation needs to be fixed for all $\beta$. Clearly, for large values $\beta$, where $s \mapsto \exp(-\beta s)$ becomes difficult to approximate with polynomials, the fixed degree $m$ is not sufficient for a good Chebyshev approximation, and consequently, the Chebyshev-Nyström++ method is very inaccurate.
