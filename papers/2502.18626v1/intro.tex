\section{Introduction}
\label{sec:introduction}

Spectral distributions of matrices and linear operators reveal important properties in numerous applications across physics, chemistry, engineering, and data science: For example, in electronic structure calculations, eigenvalues represent the energy levels that electrons occupy~\cite{drabold-1993-maximum-entropy, ducastelle-1970-moments-developments, haydock-1972-electronic-structure, lin-2017-randomized-estimation}, in machine learning they are indicative of the topology of the loss landscape~\cite{ghorbani-2019-investigation-neural, yao-2020-pyhessian-neural}, and in graph processing they can uncover hidden graph motifs~\cite{huang-2021-density-states}. Given a symmetric matrix $\mtx{A} \in \mathbb{R}^{n \times n}$, the distribution of its eigenvalues $\lambda_1, \dots, \lambda_n \in \mathbb{R}$ can be represented by the spectral density $\phi(t) = n^{-1} \sum_{i=1}^{n} \delta(t - \lambda_i)$, which places a rescaled Dirac delta distribution $\delta$ at each eigenvalue. Clearly, assembling this expression amounts to computing all eigenvalues of the matrix; an operation that is often prohibitively expensive. Several techniques have been developed for approximating $\phi$, including moment matching~\cite{cohen-steiner-2018-approximating-spectrum, braverman-2022-sublinear-time} and polynomial approximation, either implicitly by a Lanczos procedure \cite{lin-2016-approximating-spectral, chen-2021-analysis-stochastic}, or explicitly by expansion \cite{weisse-2006-kernel-polynomial, lin-2016-approximating-spectral}. Some of these methods have also been combined with a deflation step to yield even more efficient algorithms \cite{lin-2017-randomized-estimation, bhattacharjee-2025-improved-spectral}.

Being composed of Dirac delta distributions, the function $\phi$ itself is hard to approximate. However, as many applications only require a rough estimate of $\phi$, it usually suffices to instead approximate a smoothed spectral density of the form
\begin{equation}
    \phi_{\sigma}(t) = \frac{1}{n} \sum_{i=1}^{n} g_{\sigma}(t - \lambda_i) = \frac{1}{n} \Trace(g_{\sigma}(t \mtx{I}_n - \mtx{A})),
    \label{equ:smoothed-spectral-density}
\end{equation}
for some fixed smoothing kernel $g_{\sigma}$, typically a Gaussian~\cite{lin-2016-approximating-spectral, lin-2017-randomized-estimation} or a Lorentzian~\cite{haydock-1972-electronic-structure, lin-2016-approximating-spectral}. We thus arrive at the problem of computing the trace of the parameter-dependent matrix function $\mtx{B}(t) \equiv g_{\sigma}(t \mtx{I}_n - \mtx{A}) \in \mathbb{R}^{n \times n}$. Evaluating this matrix function \emph{exactly} requires the diagonalization of $\mtx{A}$, which does not seem to yield any gains compared to the original problem of computing its spectral distribution. The crucial point is that we can now \emph{estimate} the trace by matrix-vector products with $\mtx{B}(t)$, which --- thanks to the smoothness of $g_\sigma$ --- can be well approximated by a few matrix-vector products with $\mtx{A}$.

For a \emph{constant} matrix $\mtx{B}$, one of the most popular trace estimators is the Girard-Hutchinson estimator \cite{girard-1989-fast-montecarlo, hutchinson-1990-stochastic-estimator} along with variance reduction techniques~\cite{gambhir-2017-deflation-method, saibaba-2017-randomized-matrixfree, lin-2017-randomized-estimation, meyer-2021-hutch-optimal, persson-2022-improved-variants, chen-2023-krylovaware-stochastic, epperly-2024-xtrace-making}. Suitable extensions to parameter-dependent matrices have been considered, e.g., in~\cite{lin-2017-randomized-estimation,chen-2023-krylovaware-stochastic}, but we are not aware of an analysis providing rigorous justification and insight of these extensions. In passing, we note that dynamic trace estimation~\cite{dharangutte-2024-dynamic-trace,woodruff-2024-optimal-query} is an efficient technique for subsequently estimating the traces of matrices $\mtx{B}(t_1), \dots, \mtx{B}(t_m)$ when the increments $\mtx{B}(t_{i+1}) - \mtx{B}(t_i)$ are relatively small in norm. The potential of dynamic trace estimation appears to be limited in our setting because $\mtx{B}(t)$ may change rapidly close to eigenvalues, with $g_{\sigma}$ approximating a Dirac delta function.

All methods considered in this work are based on the following simple idea: Apply an existing randomized trace estimator to $\Trace(\mtx{B}(t))$ with \emph{constant} random vectors, that is, the same randomization is used for each value of the parameter $t$. For example, the Girard-Hutchinson estimator becomes $\mathsf{est}(t):=n_{\mtx{\Psi}}^{-1} \sum_{j=1}^{n_{\mtx{\Psi}}} \vct{\psi}_j^{\top} \mtx{B}(t) \vct{\psi}_j$ for $n_{\mtx{\Psi}}$ constant Gaussian random vectors $\vct{\psi}_1, \dots, \vct{\psi}_{n_{\mtx{\Psi}}}$. We will use the $L^1$-norm $\int_{a}^{b} | \Trace(\mtx{B}(t)) - \mathsf{est}(t) |~\mathrm{d}t$ to measure the error of such an estimator for all $t$ in an interval by $[a,b]$; similar to the approach taken in~\cite{kressner-2024-randomized-lowrank} for low-rank approximation.

\paragraph{New contributions.} We analyze three well-established randomized trace estimators when they are applied to parameter-dependent matrices with constant randomness (with respect to $t$): The Girard-Hutchinson estimator \cite{girard-1989-fast-montecarlo, hutchinson-1990-stochastic-estimator}, the trace of the Nyström low-rank approximation \cite{gittens-2013-revisiting-nystrom}, and the Nyström++ estimator \cite{persson-2022-improved-variants}. Combined with Chebyshev approximation, this allows one to reuse the majority of computations across different values of the parameter $t$, making the estimators scale favorably with respect to the number of parameter evaluations. We propose several additional modifications to the methods from \cite{lin-2017-randomized-estimation} for the special case of approximating the smoothed spectral density $\phi_{\sigma}$ from~\refequ{equ:smoothed-spectral-density}. In particular, we use a more rigorous approach that preserves non-negativity for approximating matrix functions in terms of Chebyshev polynomials. The resulting Chebyshev-Nyström++ estimator 
unifies all methods from~\cite{lin-2017-randomized-estimation} in a single algorithm.
Combining our analysis of the randomized trace estimators with standard error bounds for Chebyshev approximation leads to~\refthm{thm:chebyshev-nystrom}, which shows that $\mathcal O(\varepsilon^{-1})$ matrix-vector products suffice to guarantee an error $\varepsilon$.

\paragraph{Structure.} In \refsec{sec:analysis} we will analyze the error of three standard trace estimators --- the Girard-Hutchinson, Nyström, and Nyström++ estimators --- when they are applied to parameter-dependent matrices with constant random matrix. In \refsec{sec:application}, we propose and analyze the Chebyshev-Nyström++ estimator, which uses these estimators to approximate the smoothed spectral density of real symmetric matrices. Finally, we will illustrate and validate our developments for examples from various applications in \refsec{sec:results}.

\paragraph{Reproducibility.} \input{re-pro-badge.tex}
