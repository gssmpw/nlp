%
%
%
%
%
We bound
\begin{align}
  J(\pi) - \En_{d_{0}}\sbr*{\Qhat(s,\pi)}  
  & = \En_{d_{0},\pi}\sbr*{\Qpi(s,a) - \Qhat(s,a)}
  \\
  & = \frac{1}{1-\gamma} \En_{d^\pi}\sbr*{\Qpi(s,a) - \gamma \Qpi(s',\pi) - \Qhat(s,a) - \gamma\Qhat(s',\pi)}
  \\
  & = \frac{1}{1-\gamma} \En_{d^\pi}\sbr*{\Qpi(s,a)-\sbr*{\Tpi\Qpi}(s,a)-\Qhat(s,a) + \sbr*{\Tpi\Qhat}(s,a)}
  \\
  & =  \frac{1}{1-\gamma} \En_{d^\pi}\sbr*{\sbr*{\Tpi\Qhat}(s,a) - \Qhat(s,a)}
  \\
  & \le \frac{1}{1-\gamma} \sqrt{\Cone\cdot\En_{\data}\sbr*{\rbr*{\sbr*{\Tpi\Qhat}(s,a) - \Qhat(s,a)}^2}} 
\end{align}
where the second line follows from Bellman flow. 
Now we consider the term under the square root, 
and let $\ghat = \argmin_{g\in\cG_{\Qhat}} \ellhat(g, \Qhat)$.
\begin{align}
 \En_{\data}\sbr*{\rbr*{\sbr*{\Tpi\Qhat}(s,a) - \Qhat(s,a)}^2}
 \le 2\cdot\underbrace{\En_{\data}\sbr*{\rbr*{\sbr*{\Tpi\Qhat}(s,a) - \ghat(s,a)}}^2}_{\mathrm{(T{1})}} 
 + 2\cdot\underbrace{\En_{\data}\sbr*{\rbr*{\ghat(s,a) - \Qhat(s,a)}^2}}_{\mathrm{(T{2})}} 
\end{align}

We consider each term above individually.
$(T 1)$ is the regression error between $\ghat[Q]$
and the population regression solution $\Tpi Q$, 
which we can control using well-established bounds.
The second term $(T 2)$ measure how close the Q-value is 
to its estimated Bellman backup. 
To bound these two terms we utilize the following results.
The first controls the error between the squared-loss minimizer $\ghat[Q]$ 
and the population solution $\Tpi Q$, and is adapted from \cite{xie2020batch}.
\begin{lemma}[Lemma 9 from \cite{xie2020batch}]\label{lem:model-reg-concentration}
  Suppose that we have $\abr*{g}_\infty \le \Vmax$ 
  for all $g\in\cG_{Q}$ and $Q \in \cQ$,
  and define 
  \begin{align}
    \ghat[Q] \ldef{} \argmin_{g\in\cG_Q} \bbE_\cD \sbr*{\rbr*{g(s,a)-r-\gamma Q(s',\pi)}^2}.
  \end{align}
  Then with probability at least $1-\delta$, for all $i \in [m]$ we have 
  \begin{align}
    \En_\data \sbr*{\rbr*{ \ghat[Q](s,a) - \sbr*{\Tpi Q}(s,a)}^2} 
    \le \frac{16\Vmax^2\log\rbr*{\frac{2m}{\delta} }}{n}
    \ldef{} \vepssqs.
  \end{align}
\end{lemma}

The second controls the error of estimating the objective for choosing $\ihat$ 
from finite samples, and a proof is included at the end of this section. 
\begin{lemma}[Objective estimation error]\label{lem:model-obj-concentration}
  Suppose that we have $\nbr*{g}_\infty \le \Vmax$ 
  for all $g\in\cG_{Q}$ and $Q \in \cQ$.
  Then with probability at least $1-\delta$, 
  for all $g\in\cG_{Q}$ and $Q \in \cQ$ we have 
  \begin{align}
    &\max\Bigg\{
      \frac{1}{2}\cdot \En_{\data}\sbr*{\rbr*{g(s,a) - Q(s,a)}^2 } - \En_{\cD}\sbr*{\rbr*{g(s,a) - Q(s,a)}^2}~,
      \\
      & \qquad\quad\En_{\cD}\sbr*{\rbr*{g(s,a) - Q(s,a)}^2 } - \frac{3}{2} \cdot\En_{\data}\sbr*{\rbr*{g(s,a) - Q(s,a)}^2}
    \Bigg\}
     \le \frac{3\Vmax^2\log\rbr*{\frac{2m}{\delta}}}{n} \ldef{} \vepsobj.
  \end{align}
\end{lemma}

%
%
%
%
%
%
%
Using Lemma~\ref{lem:model-reg-concentration}, we directly obtain that with probability at $1-\delta$,
\begin{align}
  \mathrm{(T 1)} \le \vepssqs.
\end{align}

By leveraging Lemma~\ref{lem:model-obj-concentration}, we have that with probability at least $1-\delta$,
\begin{align}
  \mathrm{(T 2)}
  & = \En_{\data}\sbr*{\rbr*{\ghat(s,a) - \Qhat(s,a)}^2}
  \\
  & \le 2\cdot\vepsobj + 2\cdot\En_{\cD}\sbr*{\rbr*{\ghat[\Qhat](s,a) - \Qhat(s,a)}^2}  
  \\
  & \le 2\cdot\vepsobj + 2\cdot\En_{\cD}\sbr*{\rbr*{\ghat[\Qpi](s,a) - \Qpi(s,a)}^2}
  \\
  & \le 4\cdot\vepsobj + 3\cdot\En_{\mu}\sbr*{\rbr*{\ghat[\Qpi](s,a) - \Qpi(s,a)}^2}
  \\
  & = 4\cdot\vepsobj + 3\cdot\En_{\mu}\sbr*{\rbr*{\ghat[\Qpi](s,a) - [\Tpi\Qpi](s,a)}^2}
  \\
  & \le 4\cdot\vepsobj + 3\cdot\vepssqs
\end{align}
where in the first inequality we apply Lemma \ref{lem:model-obj-concentration}
(by lower bounding the LHS with the first expression in the $\max$); 
in the second we use the Q-value realizability assumption $\Qpi \in \cQ$
with the fact that $\Qhat$ is the minimizer of the empirical objective; 
and in the third we again apply Lemma \ref{lem:model-obj-concentration}
(now lower bounding the LHS with the second expression in the $\max$).
Then we use the identity that $\Qpi = \Tpi\Qpi$,
and apply the squared-loss regression guarantee. 
The bounds for (T1) and (T2) mean that
\begin{align}
  \En_{\data}\sbr*{\rbr*{\sbr*{\Tpi\Qhat}(s,a) - \Qhat(s,a)}^2}
  & \le 8\rbr*{\vepsobj + \vepssqs}, 
\end{align}
resulting in the final estimation bound of 
\begin{align}
 J(\pi) - \En_{d_{0}}\sbr*{\Qhat(s,\pi)} 
 & \le  \frac{1}{1-\gamma} \sqrt{\Cone\cdot\En_{\data}\sbr*{\rbr*{\sbr*{\Tpi\Qhat}(s,a) - \Qhat(s,a)}^2}} 
 \\
 & \le  \frac{1}{1-\gamma} \sqrt{8\cdot\Cone\cdot\rbr*{\vepsobj + \vepssqs}},
 \\
 & = \frac{\Vmax}{1-\gamma} \sqrt{\frac{152\cdot\Cone\cdot\log\rbr*{\frac{2m}{\delta} }}{n}},
\end{align}
which holds with probability at least $1-2\delta$.


\begin{proof}[Proof of Lemma \ref{lem:model-obj-concentration}]
  Observe that the random variable $\rbr*{g(s,a)-Q(s,a)}^2 \in [-\Vmax^2, \Vmax^2]$,
  and
  \begin{align}
    \bbV_\data\sbr*{\rbr*{g(s,a)-Q(s,a)}^2}
    & \le \En_\data\sbr*{\rbr*{g(s,a)-Q(s,a)}^4}
    \\
    & \le \Vmax^2\cdot \En_\data\sbr*{\rbr*{g(s,a)-Q(s,a)}^2}.
  \end{align}
  Then, applying Bernstein's inequality with union bound, 
  we have that, for any $g \in \cG_Q$ and $Q \in \cQ$ with probability at least $1-\delta$,
  \begin{align}
    &\abr*{ \En_{\data}\sbr*{\rbr*{g(s,a) - Q(s,a)}^2 }- \En_{\cD}\sbr*{\rbr*{g(s,a) - Q(s,a)}^2}}  
    \\
    & \le \sqrt{\frac{4\bbV_\data \sbr*{\rbr*{g(s,a) - Q(s,a)}^2}\log\rbr*{\frac{2m}{\delta}}}{n}}
    + \frac{\Vmax^2 \log\rbr*{\frac{2m}{\delta}}}{n}
    \\
    & \le  \sqrt{\frac{4\Vmax^{2}\bbE_\data \sbr*{\rbr*{g(s,a) - Q(s,a)}^2}\log\rbr*{\frac{2m}{\delta}}}{n}}
    + \frac{\Vmax^2 \log\rbr*{\frac{2m}{\delta}}}{n}
    \\
    & \le \frac{\bbE_\data \sbr*{\rbr*{g(s,a) - Q(s,a)}^2}}{2} 
    + \frac{3\Vmax^2\log\rbr*{\frac{2m}{\delta}}}{n}.
  \end{align}
  Expanding the absolute value on the LHS and rearranging, this then implies that
  \begin{align}
     \frac{1}{2} \cdot\En_{\data}\sbr*{\rbr*{g(s,a) - Q(s,a)}^2 } 
     & \le \En_{\cD}\sbr*{\rbr*{g(s,a) - Q(s,a)}^2}
     +  \frac{3\Vmax^2\log\rbr*{\frac{2m}{\delta}}}{n},
     \\
     \En_{\cD}\sbr*{\rbr*{g(s,a) - Q(s,a)}^2 } 
     & \le \frac{3}{2} \cdot\En_{\data}\sbr*{\rbr*{g(s,a) - Q(s,a)}^2}
     +  \frac{3\Vmax^2\log\rbr*{\frac{2m}{\delta}}}{n}. 
  \end{align}
  Combining these statements completes the proof. 
\end{proof}
