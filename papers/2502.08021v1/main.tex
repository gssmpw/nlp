\section{Introduction}

Offline reinforcement learning (RL) is a promising paradigm for applying RL to important application domains where perfect simulators are not available and we must learn from data \cite{levine2020offline, jiang2024offline}. Despite the significant progress made in devising more performant \textit{training} algorithms, how to perform holdout validation and model selection%
---an indispensable component of any practical machine learning pipeline---remains an open problem and has hindered the deployment of RL in real-life scenarios. 
Concretely, after multiple training algorithms (or instances of the same algorithm with different hyperparameter settings) have produced candidate policies, the \textit{primary} task (which contrasts the \textit{secondary} task which we focus on) is to select a good policy from these candidates, much like how we select a good classifier/regressor in supervised learning. To do so, we may estimate the performance (i.e., expected return) of each policy, and select the one with the highest estimated return. 

Unfortunately, estimating the performance of a new \textit{target} policy based on   data collected from a different \textit{behavior} policy is a highly challenging task, known as \textit{off-policy evaluation} (OPE). Popular OPE algorithms can be roughly divided into two categories, each with their own critical weaknesses: the first is importance sampling \cite{precup2000eligibility,jiang2016doubly,thomas2016data}, which has elegant unbiasedness guarantees but suffers variance that is \textit{exponential} in the horizon, limiting applicability beyond short-horizon settings such as contextual bandits \cite{li2011unbiased}. The second category includes algorithms such as Fitted-Q Evaluation (FQE) \cite{ernst2005tree, le2019batch, paine2020hyperparameter}, marginalized importance sampling \cite{liu2018breaking, nachum2019dualdice, uehara2019minimax}, and model-based approaches \cite{voloshin2021minimax}, which avoid the exponential variance; unfortunately, this comes at the cost of introducing their own hyperparameters (choice of neural architecture, learning rates, etc.). While prior works have reported the effectiveness of these methods \cite{paine2020hyperparameter}, they also leave a chicken-and-egg problem: \textbf{if these algorithms tune the hyperparameters of training, who tunes \textit{their} hyperparameters?} 

In this work, we make progress on this latter problem, namely model selection for OPE algorithms themselves, in multiple dimensions. %
Concretely, we consider two settings:
%
%
in the \textbf{model-based} setting, evaluation algorithms build dynamics models to evaluate a target policy. Given the uncertainty of hyperparameters in model building, we assume that multiple candidate models are given, and the task is to select one that we believe evaluates the performance of the target policy most accurately. In the \textbf{model-free} setting, 
%
evaluation algorithms only output \textit{value functions}. Similar to above, the task is to select a value function out of the candidate value functions. 
%

Our contributions are 4-fold: \vspace{-.5em}
\begin{enumerate}[leftmargin=*, itemsep=.5pt]
\item \textbf{New selectors (model-free):} We propose a new selection algorithm (or simply \textit{selector}), \lstd, for selecting between candidate value functions by approximately checking whether the function satisfies the Bellman equation. The key technical difficulty here is the infamous \textbf{\textit{double sampling}} problem \cite{baird1995residual, sutton2018reinforcement, chen2019information}. Our derivation builds on BVFT \cite{xie2020batch, zhang2021towards}, which is the only existing selector that addresses double sampling in a theoretically rigorous manner without additional function-approximation assumptions. Our new selector relies on more plausible assumptions, enjoys better statistical rates ($1/\epsilon^2$ vs.~$1/\epsilon^4$), and empirically outperforms BVFT and other baselines.
\item \textbf{New selectors (model-based):} When comparing candidate models, popular losses in model-based RL exhibit biases under stochastic transitions \cite{jiang2024note}. Instead, we propose novel estimators with theoretical guarantees, including novel adaptation of previous model-free selectors that require additional assumptions to the model-based setting \cite{antos2008learning, zitovsky2023revisiting}.  %
\item \textbf{New experiment protocol}: To empirically evaluate the selection algorithms, prior works often use FQE to prepare candidate Q-functions \cite{zhang2021towards, nie2022data}, which suffers from unstable training\footnote{For example, our preliminary investigation has found that FQE often diverges with CQL-trained policies \cite{kumar2020conservative}, which is echoed by \cite{nie2022data} in personal communications.} and lack of control in the quality of the candidate functions. We propose a new experiment protocol, where the candidate value functions are induced from variations of the groundtruth environment. This bypasses the caveats of FQE and allows for the computation of Q-values in an \textit{optimization-free} and controllable manner. Moreover, the protocol can also be used to evaluate and compare estimators for the model-based setting. Implementation-wise, we use \textbf{\textit{lazy evaluation}} and Monte-Carlo roll-outs to generate the needed Q-values.  Combined with parallelization and caching, we  reduce the computational cost and make the evaluation of new algorithms easier.
\item \textbf{Preliminary experiments}: We instantiate the protocol in Gym Hopper and demonstrate the various ways in which we can evaluate  and understand different selectors. 
\end{enumerate}



\section{Preliminaries}

\paragraph{Markov Decision Process (MDP)} 
An MDP is specified by $(\Scal, \Acal, P, R, \gamma, d_0)$, where $\Scal$ is the state space, $\Acal$ is the action space, $P: \Scal\times\Acal \to \Delta(\Scal)$ is the transition dynamics, $R: \Scal\times\Acal\to[0,\Rmax]$ is the reward function, $\gamma \in [0, 1)$ is the discount factor, and $d_0$ is the initial state distribution. A policy $\pi: \Scal\to\Delta(\Acal)$ induces a distribution over random trajectories, generated as $s_0 \sim d_0$, $a_t \sim \pi(\cdot|s_t)$, $r_t = R(s_t, a_t)$, $s_{t+1} \sim P(\cdot|s_t, a_t)$, $\forall t \ge 1$. We use $\Pr_\pi[\cdot]$ and $\EE_\pi[\cdot]$ to denote such a distribution and the expectation thereof. The performance of a policy is defined as $J(\pi):= \EE_{\pi}[\sum_{t=0}^\infty \gamma^t r_t]$, which is in the range of $[0, \Vmax]$ where $\Vmax:= \Rmax/(1-\gamma)$. %

\paragraph{Value Function and Bellman Operator} The Q-function $Q^\pi\in \RR^{\Scal\times\Acal}$ is the fixed point of $\Tcal^\pi: \RR^{\Scal\times\Acal} \to \RR^{\Scal\times\Acal}$, i.e., $Q^\pi = \Tcal^\pi Q^\pi$, where for any $f\in \RR^{\Scal\times\Acal}$, $(\Tcal^\pi f)(s,a):= R(s,a) + \gamma \EE_{s\sim P(\cdot|s,a)}[f(s',\pi)]$. We use the shorthand $f(s',\pi)$ for $\EE_{a'\sim \pi(\cdot|s')}[f(s',a')]$. 

\paragraph{Off-policy Evaluation (OPE)} OPE is about estimating the performance of a given \textit{target} policy $\pi$ in the real environment denoted as $M^\star = (\Scal, \Acal, P^\star, R, \gamma, d_0)$, namely $J_{M^\star}(\pi)$, using an offline dataset $\Dcal$ sampled from a behavior policy $\pi_b$. %
For simplicity, from now on we may drop the $M^\star$ in the subscript when referring to properties of $M^\star$, e.g., $J(\pi)\equiv J_{M^\star}(\pi)$, $Q^\pi \equiv Q_{M^\star}^\pi$, etc. 
%
As a standard simplification, our theoretical derivation assumes that the dataset $\Dcal$ consists of $n$ i.i.d.~tuples $(s,a,r,s')$ generated as $(s,a) \sim \data$, $r = R(s,a)$, $s' \sim P^\star(\cdot|s,a)$. We use $\EE_{\data}[\cdot]$ to denote the true expectation under the data distribution, and $\EE_{\Dcal}[\cdot]$ denotes the empirical approximation from $\Dcal$. 

\paragraph{Model Selection} We assume that there are multiple OPE instances that estimate $J(\pi)$, and our goal is to choose among them based on the offline dataset $\Dcal$. %
Our setup is agnostic w.r.t.~the details of the OPE instances, and view them only through the intermediate objects (dynamics model or value function) they produce. Concretely, two settings are considered:
\begin{itemize}[leftmargin=*]
\item \textbf{Model-based:} Each OPE instance %
produces an MDP $M_i$ and uses $J_{M_i}(\pi)$ as an estimate of $J(\pi)$; w.l.o.g.~we assume $M_i$ only differs from $M^\star$ in the transition $P_i$.  The  task is to select $\hat M$ from $\Mcal:=\{M_i\}_{i\in[m]}$, such that $J_{\hat M}(\pi) \approx J(\pi)$. We  assume that at least one model is close to $M^\star$, and in theoretical analyses we make the simplification that $M^\star \in \Mcal$; extension to the misspecified case ($M^\star\notin \Mcal$) is routine in RL theory \cite{amortila2023optimal,amortila2024mitigating} and orthogonal to the insights of this work. %
\item \textbf{Model-free:} Each OPE instance provides a Q-function that approximates $Q^\pi$. The validation task is to select $\hat Q$ from the candidate Q-functions $\Qcal:=\{Q_i\}_{i\in[m]}$,\footnote{In practical scenarios, the candidate models and functions, $\{M_i\}_{i\in[m]}$ and $\{Q_i\}_{i\in[m]}$, may be learned from data, and we assume $\Dcal$ is a holdout dataset  independent of the data used for producing $\{M_i\}_{i\in[m]}$ and $\{Q_i\}_{i\in[m]}$.}  such that $\EE_{s\sim d_0}[{\hat Q}(s, \pi)] \approx J(\pi)$. Similar to the model-based case, we will assume $Q^\pi \in \Qcal$ in the derivations.
\end{itemize}

The model-free setting is a more general protocol, as the model-based setting can be reduced to it: given candidate models $\{M_1, \ldots, M_m\}$, we can induce a Q-function class $\{Q_{M_1}^\pi, \ldots, Q_{M_m}^\pi\}$ and run any model-free selection algorithm over them. The model-free setup also handles broader settings, especially when we lack prior knowledge of model dynamics. In either case, we treat the base algorithms as black-boxes and interact with them only through the models and the Q-functions they produce. 

%

\section{New Model-free Selector} \label{sec:mf-select}


In this section we introduce our new model-free selector, \lstd. To start, we review the difficulties in model-free selection and the idea behind BVFT \cite{xie2020batch, zhang2021towards} which we build on. %

\subsection{Challenges of Model-free Selection and BVFT}
To select $Q^\pi$ from $\Qcal=\{Q_1, \ldots, Q_m\}$, perhaps the most natural idea is to check how much each candidate function $Q_i$ violates the Bellman equation $Q^\pi = \Tcal^\pi Q^\pi$, and choose the function that minimizes such a violation. This motivates the Bellman error (or residual) objective:
\begin{equation}\label{eq:berr}
    \EE_{\data}[(Q_i - \Tcal^\pi Q_i)^2].
\end{equation}
Unfortunately, this loss cannot be estimated due to the infamous \textit{double-sampling problem} \cite{baird1995residual, sutton2018reinforcement, chen2019information}, and the na\"ive estimation, which squares the TD error, is a biased estimation of the Bellman error (Eq.\eqref{eq:berr}) in stochastic environments:
\begin{align} \label{eq:td-sq}
\textrm{(TD-sq)} \qquad \EE_{\data}[(Q_i(s,a) - r - \gamma Q_i(s',\pi))^2].
\end{align} 
Common approaches to debiasing this objective involves additional ``helper'' classes, which we show can be naturally induced in the model-based setting; see Section~\ref{sec:mb-select} for details. 

\paragraph{BVFT} 
%
The idea behind BVFT \cite{xie2020batch} is to find an OPE algorithm for learning $Q^\pi$ from a function class $\Fcal$, such that to achieve polynomial sample-complexity guarantees, it suffices if $\Fcal$ satisfies 2 assumptions:
\begin{enumerate}[leftmargin=*, itemsep=1pt]
\item Realizability, that $Q^\pi \in \Fcal$.
\item Some \textit{structural} (as opposed to \textit{expressivity}) assumption on $\Fcal$, e.g., smoothness, linearity, etc.
\end{enumerate}
Standard learning results in RL typically require stronger expressivity assumption than realizability, such as the widely adopted \textit{Bellman-completeness} assumption ($\Tcal^\pi f\in\Fcal, \forall f\in\Fcal$). However, exceptions exist, and 
BVFT shows that \textit{they can be converted into a pairwise-comparison subroutine} for selecting between two candidates $\{Q_i, Q_j\}$, and extension to multiple candidates can be done via a tournament procedure. Crucially, we can use $\{Q_i, Q_j\}$ to \textbf{automatically create an $\Fcal$ needed by the algorithm} without additional side information or prior knowledge. We refer the readers to \cite{xie2020batch} for further details, and we will also demonstrate such a process in the next subsection. 
%
%

In short, BVFT provides a general recipe for converting a special kind of ``base'' OPE methods into selectors of favorable guarantees. Intuitively, the ``base'' method/analysis will determine the properties of the resulting selector. 
For BVFT, such a ``base'' is learning with $Q^\pi$-irrelevant abstractions \cite{li2006towards, nan_abstraction_notes}, where the structural assumption on $\Fcal$ is being piecewise-constant. Our novel insight is that for learning $Q^\pi$, there exists another algorithm, namely LSTDQ \cite{lagoudakis2003least}, which satisfies the needed criteria and has superior properties compared to $Q^\pi$-irrelevant abstractions, thus can induce better selectors than BVFT. 

\subsection{\lstd} 
We now provide a theoretical analysis of LSTDQ (which is simplified from the literature \cite{mou2023optimal,perdomo2023complete}), and show how to transform it into a selector via the BVFT recipe. 
In LSTDQ, we learn $Q^\pi$ via linear function approximation, i.e., it is assumed that a feature map $\phi: \Scal\times\Acal \to \RR^d$ is given, such that $Q^\pi(s,a) = \phi(s,a)^\top \theta^\star$, %
where $\theta^\star\in\RR^d$ is the groundtruth linear coefficient. Equivalently, this asserts that the induced linear class, $\Fcal_\phi := \{ \phi^\top \theta: \theta \in \RR^d\}$ satisfies realizability, $Q^\pi \in \Fcal_\phi$. %

LSTDQ provides a closed-form estimation of $\theta^\star$ by first estimating the following moment matrices:
\begin{align}
\Sigma := \EE_{\data}[\phi(s,a)\phi(s,a)^\top], 
\quad & \Sigcr := \EE_{\data}[\phi(s,a)\phi(s',\pi)^\top], \\ 
A := \Sigma - \gamma \Sigcr, \quad & b := \EE_{\data}[\phi(s,a) r]. \label{eq:lstd-a-b}
\end{align}
As a simple algebraic fact, $A \theta^\star = b$. Therefore, when $A$ is invertible, we immediately have that $\theta^\star = A^{-1} b$. The LSTDQ algorithm thus simply estimates $A$ and $b$ from data, denoted as $\emp A$ and $\emp b$, respectively, and estimate $\theta^\star$ as $\emp A^{-1} \emp b$. Alternatively, for any candidate $\theta$, $\|A \theta - b\|_\infty$ %
can serve as a loss function that measures the violation of the equation $A\theta^\star = b$, which we can minimize over.\footnote{When $\emp A^{-1} \emp b \in \Theta$, it will be a minimizer of the loss, so the loss-minimization version is a regularized generalization of LSTDQ.  } Its finite-sample guarantee is given below. All proofs of the paper can be found in Appendix~\ref{app:proof}.

%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{theorem} \label{thm:lstd}
%
Let $\Theta \subset \RR^d$ be a set of parameters such that $\theta^\star \in \Theta$. Assume $\max_{s,a} \nrm{\phi(s,a)}_2 \leq B_\phi$ and $\max_{\theta \in \Theta} \nrm{\theta}_2 \leq 1$. Let $\emp\theta := \argmin_{\theta\in\Theta} \|\emp A \theta - \emp b\|_\infty$. Then, with probability at least $1-\delta$, 
%
%
%
%
\begin{align}
\|Q^\pi - \hat{\phi}^\top \theta\|_\infty \le \frac{6\max\{\Rmax, B_{\phi}\}^2}{\sigma_{\min}(A)}  \sqrt{\frac{d\log(2d\abs{\Theta}/\delta)}{n}},
\end{align}
where $\sigma_{\min}(\cdot)$ is the smallest singular value. 
\end{theorem}

Besides the realizability of $Q^\pi$, the guarantee also depends on the invertibility of $A$, which can be viewed as a coverage condition, since $A$ changes with the data distribution $\data$ \cite{amortila2020variant,amortila2023optimal,jiang2024offline}. In fact, in the on-policy setting ($\data$ is an invariant distribution under  $\pi$), $\sigma_{\min}(A)$ can be shown to be lower-bounded away from $0$ \cite{mou2023optimal}. 

\paragraph{\lstd} We are now ready to describe our new selector. Recall that we first deal with the case of two candidate functions, $\{Q_i, Q_j\}$, where $Q^\pi \in \{Q_i, Q_j\}$. To apply the LSTDQ algorithm and guarantee, all we need is to create the feature map $\phi$ such that $Q^\pi$ is linearly realizable in $\phi$. In the spirit of BVFT, we design the feature map as
\begin{align} \label{eq:feature}
\phi_{i,j}(s,a):= [Q_i(s,a), Q_j(s,a)]^\top.
\end{align}
The subscript ``$i,j$'' makes it clear that the feature is created based on $Q_i$ and $Q_j$ as candidates, and we will use similar conventions for all quantities induced from $\phi_{i,j}$, e.g., $A_{i,j}, b_{i,j}$, etc. Obviously, $Q^\pi$ is linear in $\phi_{i,j}$ with $\theta^\star \in \{[1, 0]^\top, [0, 1]^\top\}$. Therefore, to choose between $Q_i$ and $Q_j$, we can calculate the LSTDQ loss of $[1, 0]^\top$ and $[0, 1]^\top$ under feature $\phi_{i,j}$ and choose the one with smaller loss. For $\theta = [1, 0]^\top$,  we have $A_{i,j} \theta - b_{i,j} =$
\begin{align}
&~ \EE_{\data}\Big\{\vc{Q_i(s,a)}{Q_j(s,a)} ([Q_i(s,a), Q_j(s,a)] \\
&~ - \gamma [Q_i(s',\pi), Q_j(s',\pi)])\Big\} \vc{1}{0} \\
&~ -  \EE_{\data}\big[[Q_i(s,a), Q_j(s,a)] \cdot r \big] \vc{1}{0} \\
= &~ \EE_{\data}\left[\vc{Q_i(s,a)}{Q_j(s,a)} (Q_i(s,a) - r - \gamma Q_i(s',\pi)) \right].
\end{align}
Taking the infinity-norm of the loss vector, we have $\|A_{i,j} \vc{1}{0} - b_{i,j}\|_\infty$
\begin{align}
= \max_{k \in \{i,j\}} |\EE_{\data}[Q_k(s,a) (Q_i(s,a) - r - \gamma Q_i(s',\pi))]|.
\end{align}

The loss for $\theta = [0, 1]^\top$ is similar, where $Q_i$ is replaced by $Q_j$. Following BVFT, we can generalize the procedure to $m$ candidate functions $\{Q_1, \ldots, Q_m\}$ by pairwise comparison and recording the worst-case loss, this leads to our final loss function: $\Lcal(Q_i; \{Q_j\}_{j\in[m]}, \pi):=$
\begin{align}\label{eq:tour}
\max_{k \in [m]} |\EE_{\data}[Q_k(s,a) (Q_i(s,a) - r - \gamma Q_i(s',\pi))]|.
\end{align}
The actual algorithm replaces $\EE_{\data}$ with the empirical estimation from data, and chooses the $Q_i$ that minimizes the loss. Building on Theorem~\ref{thm:lstd}, we have the following guarantee:

\begin{theorem}\label{thm:tournament}
Given $Q^\pi\coloneqq Q_{i^\star} \in \{Q_i\}_{i\in[m]}$, the $Q_{\emp i}$ that minimizes the empirical estimation of $\Lcal(Q_i; \{Q_j\}_{j\in[m]}, \pi)$ (Eq.\eqref{eq:tour}) satisfies that w.p.~$\ge 1-\delta$, $|J(\pi) - \EE_{s\sim d_0}[Q_{\emp i}(s, \pi)| $%
\begin{align}
\le\max_{i \in [m] \setminus \crl{i^\star}}\frac{ 24 \Vmax^3}{\sigma_{\min}(A_{i,i^\star})}\sqrt{\frac{\log(8m/\delta)}{n}}.
\end{align}
%
\end{theorem}

\begin{comment}
\begin{proof}
    \[
|J_{M^\star}(\pi) - \EE_{s\sim d_0}[Q_{\emp i}(s, \pi)| \leq \nrm{Q^\pi - Q_{\hat{i}}}_\infty
    \]

    
\end{proof}
\end{comment}

\paragraph{Comparison to BVFT \cite{xie2020batch}} BVFT's guarantee has a slow $1/\epsilon^4$ rate for OPE \cite{zhang2021towards, jia2024offline}, whereas our method enjoys the standard $1/\epsilon^2$ rate. The additional $1/\epsilon^2$ is due to an adaptive discretization step in BVFT, which also makes its implementation somewhat complicated as the resolution needs to be heuristically chosen. By comparison, the implementation of \lstd is simple and straightforward. Both methods inherit the coverage assumptions from their base algorithms and are not immediately comparable. We leave a detailed comparison of their differences for future work.

\paragraph{Variants} A key step in the derivation is to design the linearly realizable feature of Eq.\eqref{eq:feature}, but the design is not unique as any non-degenerate linear transformation would also suffice. For example, we can use $\phi_{i,j} = [Q_i/c_i, (Q_j-Q_i)/c_{j,i}]$; the ``diff-of-value'' term $Q_j -Q_i$ has shown improved numerical properties in practice \cite{kumar2020conservative, cheng2022adversarially}, and $c_i$, $c_{j,i}$ can normalize the discriminators to unit variance for further numerical stability; this will also be the version we use in the main-text experiments. %
Preliminary empirical comparison across these variants can be found in  Appendix~\ref{app:lstd}.

\section{Model-based Selectors} \label{sec:mb-select}

We now turn to the model-based setting, i.e., choosing a model from $\{M_i\}_{i\in[m]}$. This is a practical scenario when we have structural knowledge of the system dynamics and can build reasonable simulators, but simulators of complex real-world systems will likely have many design choices and knobs that cannot be set from prior knowledge alone. In some sense, the task is not very different from  system identification in control and model learning in model-based RL, except that (1) we focus on a finite and small number of plausible models, instead of a rich and continuous hypothesis class, and (2) the ultimate goal is to perform accurate OPE, and learning the model is only an intermediate step.%

\paragraph{Existing Methods} Given the close relation to model learning, a natural approach is to simply minimize the model prediction loss \cite{jiang2024note}: a candidate model $M$ is  scored by
\begin{align} \label{eq:naive}
\EE_{(s,a,s')\sim \data, \tilde s \sim P(\cdot|s,a)}[d(s', \tilde s)],
\end{align}
where $s'$ is in the data and generated according to the real dynamics $P^\star(\cdot|s,a)$, and $\tilde s$ is generated from the candidate model $M$'s dynamics $P$. $d(\cdot, \cdot)$ is a distance metric that measures the difference between states. 

Despite its wide use and simplicity \cite{nagabandi2018neural}, the method has  major caveats: first, the distance metric $d(\cdot, \cdot)$ is a design choice. When the state is represented as a real-valued vector, it is natural to use the $\ell_2$ distance as $d(\cdot, \cdot)$, which changes if we simply normalize/rescale the coordinates. Second, the metric is biased for stochastic environments as discussed in prior works \cite{jiang2024note, voelcker2023lambda}, which we will also demonstrate in the experiment section (Section~\ref{sec:exp}); essentially this is a version of the double-sampling issue but for the model-based setting \cite{amortila2024reinforcement}. 

There are alternative methods that address these issues. For example, in the theoretical literature, MLE losses are commonly used, i.e., $\EE_{\data}[\log P(s'|s,a)]$ \cite{agarwal2020flambe, uehara2021representation, liu2023optimistic}, which avoids $d(\cdot, \cdot)$ and works properly for stochastic MDPs by effectively measuring the KL divergence between $P^\star(\cdot|s,a)$ and $P(\cdot|s,a)$. Unfortunately, most complex simulators do not provide explicit probabilities $P(s'|s,a)$, making it difficult to use in practice. Moreover, when the support of $P^\star(\cdot|s,a)$ is not fully covered by $P(\cdot|s,a)$, the loss can become degenerate. 

To address these issues, we propose to estimate the Bellman error $\EE_{\data}[(Q_i - \Tcal^\pi Q_i)^2]$, where $Q_i := Q_{M_i}^\pi$. As discussed earlier, this objective suffers the double-sampling issue in the model-free setting, which we show can be addressed when we have access to candidate models $\{M_1, \ldots, M_m\}$ that contains the true dynamics $M^\star$. Moreover, the Bellman error $|Q_{M_i}^\pi(s,a) - (\Tcal^\pi Q_{M_i}^\pi)(s,a)| = $ 
$$
\gamma |\EE_{s'\sim P^\star(\cdot|s,a)}[Q_i(s',\pi)] -  \EE_{s'\sim P_i(\cdot|s,a)}[Q_i(s',\pi)]|,
$$
which can be viewed as an IPM loss \cite{muller1997integral} that measures the divergence between $P^\star(\cdot|s,a)$ and $P(\cdot|s,a)$ under $Q_i(\cdot, \pi)$ as a discriminator. IPM is also a popular choice of model learning objective in theory \cite{sun2019model, voloshin2021minimax}, and the Bellman error provides a natural discriminator relevant for the ultimate task of interest, namely OPE.


\subsection{Regression-based Selector}
Recall that the difficulty in estimating the Bellman error $\EE_{\data}[(Q_i - \Tcal^\pi Q_i)^2]$ is the uncertainty in $\Tcal^\pi$. 
%
To overcome this, we leverage the following observation from \cite{antos2008learning}, where for any $f: \Scal\times\Acal\to \RR$, %
\begin{align} \label{eq:T-argmin}
\Tcal^\pi f \in \argmin_{g:\Scal\times\Acal\to\RR} \EE_{\data}[(g(s,a) - r - \gamma f(s',\pi))^2],
\end{align}
%
%
%
which shows that we can estimate $\Tpi Q_i$ by solving a sample-based version of the above regression problem with $f=Q_i$. Statistically, however, we cannot afford to minimize the objective over all possible functions $g$; 
we can only search over a limited set $\cG_i$ that ideally captures the target $\Tpi Q_i$. Crucially, in the model-based setting 
we can generate such a set directly from the candidates $\{M_i\}_{i\in[m]}$:
%
\begin{proposition} \label{prop:gi}
Let $\Gcal_i := \{ \Tcal_{M_j}^\pi Q_i: j\in[m]\}$. Then if $M^\star \in \{M_i\}_{i\in[m]}$, it follows that $\Tcal^\pi Q_i = \Tcal_{M^\star}^\pi Q_i \in \Gcal_i$.
\end{proposition}

%
%
%
The constructed $\cG_i$ ensures that regression is statistically tractable 
given its small cardinality, $\abr{\cG_i}=m$. 
%
%
%
%
To select $Q_i$, 
we choose $Q_i$ with the smallest loss defined as follows:
%
\begin{enumerate}[leftmargin=*, itemsep=1pt, topsep=0.2pt, parsep=0pt, partopsep=0pt]
\item  $\emp g_i:=
\argmin_{g\in\Gcal_i} \EE_{\Dcal}[(g(s,a) - r - \gamma Q_i(s',\pi))^2].$
\item The loss of $Q_i$ is $\EE_{\Dcal}[(\emp g_i(s,a) - 
Q_i(s,a))^2]$.
%
\end{enumerate}
The 2nd step follows from \cite{zitovsky2023revisiting}. 
%
%
%
%
%
%
%
%
%
Alternatively, we can also use the min value of Eq.\eqref{eq:T-argmin} (instead of the argmin function) to correct for the bias in TD-squared (Eq.\eqref{eq:td-sq}) \cite{antos2008learning};  see \cite{liu2023offline} for another related variant. These approaches share similar theoretical guarantees under standard analyses \cite{xie2020batch, xie2021bellman}, and we only state the guarantee for the \cite{zitovsky2023revisiting} version below, but will include both in the experiments.
%
%
%
%
%
%
%
%
%
%
%
%

\begin{theorem}\label{thm:model}
Let $\Cone \ldef{} \En_{\pi}\sbr*{\frac{d^\pi(s,a)}{\mu(s,a)}}$.
%
For $Q_{\ihat}$ that minimizes $\EE_{\Dcal}[(\emp g_i(s,a) - Q_i(s,a))^2]$ 
we have w.p.~$\ge 1-\delta$,
\begin{align}
 J(\pi) - \En_{d_{0}}\sbr*{Q_{\ihat}(s,\pi)} 
 & \le  \frac{\Vmax}{1-\gamma} \sqrt{\frac{152\cdot\Cone\cdot\log\rbr*{\frac{4m}{\delta} }}{n}}.
\end{align}
%
\end{theorem}

\subsection{\absind}
%
We  now present another  selector that leverages the information of $\Gcal_i = \{\Tcal_{M_j}^\pi: j\in[m]\}$ in a different manner. Instead of measuring the squared Bellman error, we can also measure the absolute error, which can be written as (some $(s,a)$ argument to functions are omitted): 
%
%
%
%
%
%
%
%
%
%
%
\begin{align}
&~ \EE_{\data}[|Q_i - \Tcal_{M^\star}^\pi Q_i|] \\
= &~ \EE_{\data}[\sgn(Q_i(s,a) - (\Tcal^\pi Q_i)(s,a))(Q_i - \Tcal^\pi Q_i)] \\
= &~ \EE_{\data}[\sgn(Q_i - \Tcal^\pi Q_i)(Q_i(s,a) - r - \gamma Q_i(s',\pi))] \\  \label{eq:saber}
\le &~ \max_{g\in\Gcal_i} \EE_{\data}[\sgn(Q_i - g) (Q_i(s,a) - r - \gamma Q_i(s',\pi))]. \hspace{-.5em}
\end{align}
%
%
%
Here, the $\cG_i$ from Proposition~\ref{prop:gi}
induces a set of sign functions $\sgn(Q_i - g)$, 
which includes $Q_i - \Tpi Q_i$, 
that will negate any negative TD errors. %
%
%
The guarantee is as follows:

\begin{theorem}\label{thm:signed}
%
%
%
%
%
Let $Q_{\ihat}$ be the minimizer of the empirical estimate of Eq.\eqref{eq:saber}, and  $\Cinf \ldef{} \max_{s,a} \frac{d^\pi(s,a)}{\mu(s,a)}$. 
W.p.~$\ge 1-\delta$, 
\begin{align}
  J(\pi) - \En_{d_{0}}\sbr*{Q_{\ihat}(s,\pi)} 
  \le 4\cdot\Cinf\cdot\Vmax\sqrt{\frac{\log\rbr*{2m / \delta}}{n}}.
\end{align}
%
\end{theorem}

%

\section{A Model-based Experiment Protocol}

Given the new \sels, we would like to evaluate and compare them empirically. However, as alluded to in the introduction, current experiment protocols have various caveats and make it difficult to evaluate the estimators in well-controlled settings. 
In this section, we describe a novel model-based experiment protocol, which can be used to evaluate both model-based and model-free \sels.  

\subsection{The Protocol} %
Our protocol consists of experiment units defined by the following elements: \vspace{-.6em}
\begin{enumerate}[itemsep=.1pt]
\item Groundtruth model $M^\star$.
\item Candidate model list $\Mcal  = \{M_i\}_{i\in[m]}$.  
\item Behavior policy $\pi_b$ and offline sample size $n$.
\item Target policies $\Pi = \{\pi_1, \ldots, \pi_l\}$.
\end{enumerate} \vspace*{-.6em}
Given the specification of a unit, we will draw a dataset of size $n$ from $M^\star$ using behavior policy $\pi_b$. For each target policy $\pi \in \Pi$, we apply different selectors to choose a model $M\in \Mcal$ to evaluate $\pi$. %
Model-free algorithms will access $M$ only through its Q-function, $Q_M^\pi$, effectively choosing from the set $\Qcal=\{Q_M^\pi: M\in\Mcal\}$. Finally, the  prediction error $|J_M(\pi) - J_{M^\star}(\pi)|$ is recorded and averaged over the target policies in $\Pi$. 
%
Moreover, we may gather results from multiple units that share the same $M^\star$ but differ in $\Mcal$ and/or the behavior policy to investigate issues such as robustness to misspecification and data coverage, as we will demonstrate in the next section. 

\paragraph{Lazy Evaluation of Q-values via Monte Carlo} While the pipeline is conceptually straightforward, practically accessing the Q-function $Q_M^\pi$ is nontrivial: we could run TD-style algorithms in  $M$ to learn $Q_M^\pi$, but that invokes a separate RL algorithm that may require additional tuning and verification, and it can be difficult to control the quality of the learned function. 

Our innovation here is to note that, for all the model-free algorithms we are interested in evaluating, \textbf{they all access $Q_M^\pi$ exclusively through the value of $Q_M^\pi(s,a)$ and $Q_M^\pi(s',\pi)$ for $(s,a,r,s')$ in the offline dataset $\Dcal$}. That is, given $n$ data points in $\Dcal$, we only need to know $2n$ scalar values about $Q_M^\pi$. Therefore, we propose to directly compute these values without explicitly representing $Q_M^\pi$, and each value can be easily estimated by averaging over multiple Monte-Carlo rollouts, i.e., $Q_M^\pi(s,a) = \EE_{\pi}[\sum_{t=0}^\infty \gamma^{t} r_t | s_0 = s, a_0 = a]$ can be approximated by rolling out multiple trajectories starting from $(s,a)$ and taking actions according to $\pi$.

Moreover, for the model-based estimators proposed in Section~\ref{sec:mb-select}, we need access to quantities in the form of $(\Tcal_{M_j}^\pi Q_{M_i}^\pi)(s,a)$. This value can also be obtained by Monte-Carlo simulation: (1) start in $(s,a)$ and simulate one step in $M_j$, then (2) switch to $M_i$, simulate from step 2 onwards and rollout the rest of the trajectory. 

%

\subsection{Computational Efficiency}
Despite not involving neural-net optimization, the experiment can still be computationally intensive due to rolling out a large number of trajectories. In our code, we incorporate the following measures to reduce the computational cost:

\paragraph{Q-caching} The most intensive part of the pipeline is to roll-out Monte-Carlo trajectories for Q-value estimation. In contrast, the cost of running the actual selection algorithms is often much lower and negligible. Therefore, we generate these Monte-Carlo Q-estimates and save them to files, and retrieve them during the selection period. This makes it efficient to experiment with new selection algorithms or add extra baselines, and also enables fast experiment that involves a subset of the candidate models (see Section~\ref{sec:subgrid}). 

\paragraph{Bootstrapping} To account for the randomness due to $\Dcal$, we use bootstrapping to sample (with replacement) multiple datasets and run the algorithms on each dataset, and report the mean performance across these bootstrapped samples with 95\% confidence intervals. Using bootstrapping maximally reuses the cached Q-values and avoids the high computational costs of sampling multiple datasets and performing Q-caching in each of them, which is unavoidable if we were to repeat each experiment verbatim multiple times. %
%


\begin{figure}[t]
	\centering
	\begin{minipage}{0.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/pre_test_j_curve.pdf}
	\end{minipage}
	\begin{minipage}{0.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/pre_test_j_convergence.pdf}
	\end{minipage}
    \caption{\textbf{Left:} $J_M(\pi)$ in $M\in\Mcal_\grav$ for different target policies. \textbf{Right:} Convergence of Monte-Carlo estimates of $J(\pi)$. Each curve corresponds to a target policy. %
    \label{fig:sanity}} \vspace{-.5em}
\end{figure}

\section{Exemplification of the Protocol} \label{sec:exp}

In this section we instantiate our protocol in the Gym Hopper environment to demonstrate its utility, while also providing preliminary empirical results for our algorithms. 

\begin{comment}
    provide empirical results based on the proposed experiment protocol, with the following purposes:
%
%
(1) Demonstrate the experiment protocol and discuss important considerations in setting up the experiments.
(2) Provide preliminary empirical comparisons of the proposed new estimators.
(3) Demonstrate how to design more nuanced experiments to draw further insights.
%

\end{comment}


\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.66\textwidth}
        \includegraphics[width=.95\linewidth]{figures/mf_on_policy_sample_eff.pdf}
    \end{minipage}%
    \begin{minipage}{0.34\textwidth}
%
%
               %
                \includegraphics[width=0.647\linewidth]{figures/mf_on_policy_sample_eff_legend.pdf}
                \caption{Main results for comparing  model-free selectors in the gravity grid (\textbf{MF.G}; top row) and the noise grid (\textbf{MF.N}; bottom row). Each plot corresponds to a different $M^\star$ as indicated in the plot title. %
                ``mb\_naive'' is model-based but still included since it does not require Bellman operator rollouts. %
                \label{fig:mainfigure} 
                }
%
%
%
%
%
%
%
%
    \end{minipage}
\end{figure*}

\subsection{Experiment Setup and Main Results}
\label{sec:exp-main}
%
Our  experiments will be based on the \textit{Hopper-v4} environment. To create a variety of environments, we  add different levels of stochastic noise in the transitions and change the gravity constant (see Appendix~\ref{app:env}). Each environment is then parameterized by the gravity constant $\grav$ and noise level $\noise$. We consider arrays of such environments as the set of candidate simulator $\Mcal$: in most of our results, we consider a ``gravity grid'' (denoted using \textbf{MF.G} in the figures) $\Mcal_{\grav} := \{\envg{0} \ldots, \envg{14}\}$ (fixed noise level, varying gravity constants from $-51$ to $-9$) and a ``noise grid'' (\textbf{MF.N}) $\Mcal_{\noise} := \{\envn{0} \ldots, \envn{14}\}$ (fixed gravity constant, varying noise level from $10$ to $100$). Each array contains 15 environments, though some subsequent results may only involve a subset of them (Section~\ref{sec:subgrid}). %
Some of these simulators will also be treated as groundtruth environment $M^\star$, which determines the groundtruth performance of target policies  and produces the offline dataset $\Dcal$. 

\paragraph{Behavior and target policies} We create 15 target policies by running DDPG \cite{Lillicrap2015ContinuousCW} in one of the environments and take checkpoints.  %
For each $M^\star$, the behavior policy is the randomized version of one of the target policies; see Appendix~\ref{app:setup} for details. A dataset is collected by  sampling trajectories until $n=3200$ transition tuples are obtained. %
%
%
As a sanity check, we plot $J_M(\pi)$ for $\pi \in \Pi_{\grav}$ and  $M \in \Mcal_{\grav}$ in Figure \ref{fig:sanity}. 
%
As can be shown in the figure, the target policies have different performances, and also vary in a nontrivial manner w.r.t.~the gravity constant $\grav$. It is important to perform such a sanity check to avoid degenerate settings, such as $J_{M}(\pi)$ varies little across $M\in\Mcal$ (then even a random selection will be accurate) or across $\pi\in\Pi$. %

%
%
%


\begin{figure*}[t]
    \centering
    \begin{minipage}{0.66\textwidth}
        \includegraphics[width=.95\linewidth]{figures/mb_sample_eff.pdf}
    \end{minipage}%
    ~
    \begin{minipage}{0.34\textwidth}
 %
 %
                \includegraphics[width=0.647\linewidth]{figures/mb_sample_eff_legend.pdf}
        \caption{Main results for comparing model-based selectors. \lstd %
        is included as the best model-free selector for comparison, which surprisingly outperforms the more sophisticated model-based selectors in Section~\ref{sec:mb-select}.%
        \label{fig:mb}}
 %
 %
 %
 %
 %
 %
 %
   \end{minipage}
\end{figure*}




\begin{figure*}
    \centering
    \begin{minipage}{0.22\textwidth}
        \includegraphics[width=\linewidth]{figures/all_in_one/mf_on_policy_noise_7.pdf}
    \end{minipage}%
    \begin{minipage}{0.22\textwidth}
        \includegraphics[width=\linewidth]{figures/all_in_one/on_policy_noise_misspec.pdf}
    \end{minipage}%
    \begin{minipage}{0.46\textwidth}
        \includegraphics[width=\linewidth]{figures/all_in_one/data_coverage_0.pdf}
    \end{minipage}%
    \caption{\textbf{Left:} OPE error vs.~simulator gaps. \textbf{Middle:} OPE error vs.~misspecification. \textbf{Right:} OPE error vs.~data coverage.
    \label{fig:misc} }
\end{figure*}


\paragraph{Number of Rollouts} We then decide the two important parameters for estimating the Q-value, the number of Monte-Carlo rollouts $\numro$ and the horizon (i.e., trajectory length) $H$. %
For horizon, we set $H=1024$ which is substantially longer than typically observed trajectories from the target policies. %
For $\numro$, we plot the convergence of $J_M(\pi)$ estimation and choose $\numro=128$ accordingly (see Figure \ref{fig:sanity}R). 
%
%

%
%
%

\paragraph{Compared Methods}  We compare our methods with baselines, including TD-square (Eq.\eqref{eq:td-sq}), na\"ive model-based (Eq.\eqref{eq:naive}), BVFT \cite{zhang2021towards}, and ``average Bellman error'' $|E_{\Dcal}[Q_i(s,a) - r - \gamma Q_i(s',\pi)]|$ \cite{jiang2017contextual}, which can be viewed as our \lstd but with a trivial constant discriminator. 
%
The model-based methods in Section~\ref{sec:mb-select} require MC rollouts for $\{T_{M_j}^\pi Q_{M_i}^\pi: i,j\in[m]\}$, which requires $O(m^2)$ computational complexity. Therefore, we first compare other selectors (mostly model-free) in Figure~\ref{fig:mainfigure} with  $m=15$; the relatively large number of candidate simulators will also enable the later subgrid studies in Section~\ref{sec:subgrid}. We then perform a separate experiment with $m=5$ for the model-based selectors (Figure~\ref{fig:mb}). 

\paragraph{Main Results} Figure~\ref{fig:mainfigure} shows the main model-free results. %
Our \lstd method demonstrates strong and reliable performance. Note that while some methods sometimes outperform it, they suffer catastrophic performances when the true environment changes. For example, the na\"ive model-based method performs poorly in high-noise environment, as predicted by theory (Section~\ref{sec:mb-select}). BVFT's performance mostly coincides with TD-sq, which is a possible degeneration predicted by \cite{zhang2021towards}. This is particularly plausible when the number of data points $n$ is not large enough to allow for meaningful discretization and partition of the state space required by the method. 

Figure~\ref{fig:mb} shows the result on smaller candidate model sets (\textbf{MB.G} and \textbf{MB.N}; see Appendix~\ref{app:setup}), where we implement the 3 model-based selectors in Section~\ref{sec:mb-select} whose computational complexities grow quadratically with $|\Mcal|$. Our expectation was that (1) these algorithms should address the double-sampling issue and will outperform na\"ive model-based when the latter fails catastrophically, and (2) by having access to more information (the model, and in particular, the Bellman operators), model-based should outperform model-free algorithms under realizability. While the first prediction is largely verified, we are surprised to find that the second prediction went wrong, and our \lstd method is more robust and generally outperforms the more complicated model-based selectors. 

%

\subsection{Subgrid Studies: Gaps and Misspecifications} \label{sec:subgrid}

We now demonstrate how to extract additional insights from the Q-values cached earlier. Due to space limit we are only able to show  representative results in Figure~\ref{fig:misc}, and more comprehensive results can be found in Appendix~\ref{app:add_results}.

\paragraph{Gaps} We investigate an intellectually interesting question: is the selection problem easier if the candidate simulators are very similar to each other, or when they are very different? We argue that the answer is \textbf{neither}, and an intermediate difference (or \textit{gap}) is the most challenging: if the simulators are too similar, their $J_M(\pi)$ predictions will all be close to $J_{M^\star}(\pi)$ since $M\approx M^\star$, and any selection algorithm will perform well; if the simulators are too dissimilar, it should be easy  to tell them apart, which also makes the task easy. 

We show how we can empirically test this. We let $M^\star = M_{\noise}^7$, and run the experiments with different 3-subsets of $\Mcal_{\noise}$, including  $\{6,7,8\}$ (least gap), $\{5,7,9\}$, \ldots, $\{0, 7, 14\}$ (largest gap). Since the needed Q-values have already been cached in the main experiments, we can skip  caching and directly run the selection algorithms. We plot the prediction error as a function of gap size in Figure~\ref{fig:misc}L, and observe the down-U  curves (except for trivial methods such as  random) as predicted by theory. 

\paragraph{Misspecification} Similarly, we can study the effect of misspecification, that is, $M^\star \in \Mcal$. For example, we can take $M^\star = M_{\sigma}^0$, and consider different subsets of $\Mcal_{\noise}$: 0--4 (realizable), 1--5 (low misspecification), \ldots, 10--14 (high misspecification). Figure~\ref{fig:misc}M plots prediction error vs.~misspecification level for different methods, where we expect to observe potential difference in the sensitivity to misspecification. The actual result is not that interesting given  similar increasing trends for all methods. 

%



\subsection{Data Coverage} \label{sec:exp-coverage}
In the previous subsection, we have seen how multiple experiment units that only differ in $\Mcal$ can provide useful insights. Here we show that we can also probe the methods' sensitivity to data coverage by looking at experiment units that only differ in the dataset $\Dcal$. In Figure~\ref{fig:misc}R, we take a previous experiment setting ($\Mcal_\grav$) and isolate a particular target policy $\pi$; then, we create two datasets: (1) $\Dcal_\pi$ sampled using $\pi$; (2) $\Dcal_{\textrm{off}}$ sampled using a policy that is created to be very different from the target policies and offer very little coverage (see Appendix~\ref{app:setup}). Then, we run the algorithm with $\lambda$ fraction of data from $\Dcal_\pi$ combined with $(1-\lambda)$ from $\Dcal_{\textrm{off}}$; as predicted by theory, most methods perform better with better coverage (large $\lambda$), and performance degrades as $\lambda$ goes to $0$. %


%