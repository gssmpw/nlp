\subsection{Environment Setup: Noise and State Resetting} \label{app:env}
\paragraph{State Resetting} Monte-Carlo rollouts for Q-value estimation rely on the ability to (re)set the simulator to a particular state from the offline dataset. 
To the best of our knowledge, Mujoco environment does not natively support state resetting, and assigning values to the observation vector does not really change the underlying state.  However, state resetting can still be implemented by manually assigning the values of  the position vector \texttt{qpos} and the velocity vector \texttt{qvel}. %

\paragraph{Noise} As mentioned in Section~\ref{sec:exp}, we add noise to Hopper to create more challenging stochastic environments and create model selection tasks where candidate simulators have different levels of stochasticity. Here we provide the details about how we inject randomness into the deterministic dynamics of Hopper. 
Mujoco engine realizes one-step transition by leveraging \texttt{mjData.\{ctrl, qfrc\_applied, xfrc\_applied\}} objects \cite{mujoco_doc}, where \texttt{mjData.ctrl} corresponds to the action taken by our agent, and \texttt{mjData.\{qfrc\_applied, xfrc\_applied\}} are the user-defined perturbations in the joint space and Cartesian coordinates, respectively. %
%
To inject randomness into the transition at a noise level of $\sigma$, we first sample an isotropic Gaussian noise with variance $\sigma^2$ as the stochastic force in \texttt{mjData.xfrc[:3]} upon each transition, which jointly determines the next state with the input action \texttt{mjData.ctrl}, leaving the joint data \texttt{mjData.qfrc} intact. %

\subsection{Experiment Settings} \label{app:setup}
\paragraph{MF/MB.G/N} The settings of different experiments are summarized in Table~\ref{tab:exp_detail}. We first run DDPG in the environment of $g=-30,\sigma=32$, and obtain 15 deterministic policies $\{\pi_{0:14}\}$ from the checkpoints. The first 10 are used as target policies in MF.G/N experiments, and MB.G/N use fewer due to the high computational cost. For the main results (Section~\ref{sec:exp-main}), the choice of $M^\star$ is usually the two ends plus the middle point of the grid ($\Mcal_\grav$ or $\Mcal_\noise$). The corresponding behavior policy is an epsilon-greedy version of one of the target policies, denoted as $\pi_i^\epsilon$, which takes the deterministic action of $\pi_i(s)$ with probability 0.7, and add a unit-variance Gaussian noise to $\pi_i(s)$ with the remaining 0.3 probability. 

\paragraph{MF/MB.Off.G/N} In the above setup, the behavior and the target policies all stem from the same DDPG training procedure. While these policies still have significant differences (see Figure~\ref{fig:sanity}L), the distribution shift is relatively mild. For the data coverage experiments (Section~\ref{sec:exp-coverage}), we prepare a different set of behavior policies that intentionally offer poor coverage: these policies, denoted as $\pi_i^{\textrm{poor}}$, are obtained by running DDPG with a different neural architecture (than the one used for generating $\pi_{0:14}$) in a different environment of $g=-60, \sigma=100$. We also provide the parallel of our main experiments in Figures~\ref{fig:mainfigure} and \ref{fig:mb} under these behavior policies with poor coverage in Appendix~\ref{app:poor-coverage}. 

\paragraph{MF.T.G} This experiment is for data coverage (Section~\ref{sec:exp-coverage}), where $\Dcal$ is a mixture of two datasets, one sampled from $\pi_7$ (which is the sole target policy being considered) and one from $\pi_i^{\textrm{poor}}$ that has poor coverage. They are mixed together under different ratios as explained in Section~\ref{sec:exp-coverage}. 

\begin{table}[h]
%
    \centering
    \begin{tabular}{c l c c c c}
    \toprule
    \textbf{} & %
    \textbf{Gravity $\grav$} & \textbf{Noise Level $\sigma$} & \makecell{\textbf{Groundtruth Model $M^\star$ and}\\\textbf{Behavior Policy} $\pi_b$} & \makecell{\textbf{Target}\\\textbf{Policies $\Pi$}} \\
    \midrule
%
    MF.G \label{mf.on.g} %
    & $\text{LIN}(-51, -9, 15)$ & 100 & $\{(M_i, \pi_i^{\epsilon}), i\in \{0,7,14\}\}$ & $\{\pi_{0:9}\}$ \\
    MF.N \label{mf.on.n} %
    & -30 & $\text{LIN}(10,100,15)$ & $\{(M_i, \pi_i^{\epsilon}), i\in \{0,7,14\}\}$ & $\{\pi_{0:9}\}$ \\
        MB.G \label{mbg} %
    & $\text{LIN}(-36,-24,5)$ & 100 & $\{(M_i, \pi_i^{\epsilon}%
    ), i\in \{0,2,4\}\}$ & $\{\pi_{0:5}\}$ \\% \multirow{7}{*}{128}\\
    MB.N \label{mbn} %
    & -30 & $\text{LIN}(10,100,5)$ & $\{(M_i, \pi_i^{\epsilon}), i\in \{0,2,4\}\}$ & $\{\pi_{0:5}\}$  \\
    MF.OFF.G \label{mf.off.g} %
    & $\text{LIN}(-51, -9, 15)$ & 100 & $\{(M_i, \pi_i^{\textrm{poor}}), i\in \{0,7,14\}\}$ & $\{\pi_{0:9}\}$  \\
    MF.OFF.N \label{mf.off.n} %
    & -30 & $\text{LIN}(10,100,15)$ & $\{(M_i, \pi_i^{\textrm{poor}}), i\in \{0,7,14\}\}$ & $\{\pi_{0:9}\}$  \\
    MF.T.G \label{mf.off.g} 
    %
    & $\text{LIN}(-51, -9, 15)$ & 100 & $\{(M_i, \pi_8 \textrm{~\&~} \pi_i^{\textrm{poor}}), i\in \{0,7,14\}\}$ & $\{\pi_8\}$ \\
    \bottomrule
    \end{tabular}
    \caption{Details of experiment settings.  $\text{LIN}(a,b,n)$ (per numpy convention) refers to the arithmetic sequence with $n$ elements, starting from $a$ and ending in $b$ (e.g.~$\text{LIN}(0,1,6) = \{0, 0.2, 0.4, 0.6, 0.8, 1.0\}$).  \label{tab:exp_detail} }
\end{table}