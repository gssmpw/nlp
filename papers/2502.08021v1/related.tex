Here we review some existing works on model selection in offline RL. Most of them are not concerned about new selection algorithms with theoretical guarantees (apart from \cite{xie2020batch, zhang2021towards, zitovsky2023revisiting, liu2023offline} which are already discussed in the main text) or experiment protocol for OPE model selection (see \cite{voloshin2019empirical, kiyohara2023scope} for experiment protocol and benchmarks of OPE itself), so their focus is different and often provides insights complementary to our work. For example, \cite{nie2022data} discuss data splitting in offline model selection; this is a question we avoid by assuming a fixed holdout dataset for OPE model selection. An exception is \cite{udagawa2023policy} who studies the model selection problem for OPE itself, but focuses on the bandit case and makes heavy use of the importance sampling estimator, which we do not consider due to the focus on long-horizon tasks. 

\cite{fujimoto2022should} challenge the idea of using Bellman errors for model selection due to their surrogacy and poor correlation with actual objective; despite the valid criticisms, there are no clear alternatives that address the pain points of Bellman errors, and the poor performance is often due to lack of data coverage, which makes the task fundamentally difficult for any algorithms. We still believe that Bellman-error-like objectives (defined in a broad sense, which includes our \lstd) are promising for model selection, and the improvement on OPE error is the right goal to pursue instead of correlation (which we know could be poor due to the surrogacy). 

As mentioned above and demonstrated in our experiments, the lack of data coverage is a key factor that determines the difficulty of the selection tasks. \cite{lee2022model} propose  feature selection algorithms for offline contextual bandits that account for the different coverage effects of candidate features. On a related note, ideas from offline RL training, such as version-space-based pessimism \cite{xie2021bellman}, can also be incorporated in our method. This will unlikely improve the accuracy of OPE itself, but may be helpful if we measure performance by how OPE can eventually lead to successful selection of performant policies, which we leave for future investigation. 