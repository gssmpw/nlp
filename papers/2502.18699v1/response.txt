\section{Related Work}
\textbf{RLHF.} RLHF has proven effective across various tasks, including text summarization **Brown et al., "Fine-Tuning Pretrained Language Models: Weight Initializers, An Ensemble of Gradients"**__, translation __**, and image generation ____. Traditional RLHF pipelines involve training a reward model from human feedback and optimizing policy using reinforcement learning algorithms like Proximal Policy Optimization (PPO) ____. However, this process is often complex, unstable, and computationally intensive. To address these challenges, RL-free methods have emerged as efficient alternatives ____, aligning LLMs with average labeler preference while preserving the core principles of RLHF.

\noindent 
\textbf{Diverse Preference Alignment.} 
Single RLHF approaches often fail to capture the diversity of human preferences ____. In response, recent studies explored multi-objective settings, decomposing human feedback into distinct dimensions, fitting separate reward models to apply aggregation. MORLHF **Wang et al., "Multi-Objective Reinforcement Learning for Human Feedback"**__ employs linear scalarization, while MaxMin-RLHF **Chen et al., "MaxMin: Maximizing Minimum Reward for Preference Alignment"**__ adopts a minimization strategy to achieve equitable alignment. For additional techniques and theoretical analyses, we refer readers to ____. Another line of research __ assumes that the optimal policy can be expressed as a linear combination of language models trained on diverse preference objectives. However, this approach lacks explicit interpretation or theoretical justification for its assumption.

\vspace{-0.12in}