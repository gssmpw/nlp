[
  {
    "index": 0,
    "papers": [
      {
        "key": "ziegler2020finetuninglanguagemodelshuman",
        "author": "Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving",
        "title": "Fine-Tuning Language Models from Human Preferences"
      },
      {
        "key": "stiennon2022learningsummarizehumanfeedback",
        "author": "Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano",
        "title": "Learning to summarize from human feedback"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kreutzer2018reliabilitylearnabilityhumanbandit",
        "author": "Julia Kreutzer and Joshua Uyheng and Stefan Riezler",
        "title": "Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wu2023humanpreferencescorebetter",
        "author": "Xiaoshi Wu and Keqiang Sun and Feng Zhu and Rui Zhao and Hongsheng Li",
        "title": "Human Preference Score: Better Aligning Text-to-Image Models with Human Preference"
      },
      {
        "key": "lee2023aligningtexttoimagemodelsusing",
        "author": "Kimin Lee and Hao Liu and Moonkyung Ryu and Olivia Watkins and Yuqing Du and Craig Boutilier and Pieter Abbeel and Mohammad Ghavamzadeh and Shixiang Shane Gu",
        "title": "Aligning Text-to-Image Models using Human Feedback"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "schulman2017proximalpolicyoptimizationalgorithms",
        "author": "John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov",
        "title": "Proximal Policy Optimization Algorithms"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chen2024selfplayfinetuningconvertsweak",
        "author": "Zixiang Chen and Yihe Deng and Huizhuo Yuan and Kaixuan Ji and Quanquan Gu",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      },
      {
        "key": "liu2024statisticalrejectionsamplingimproves",
        "author": "Tianqi Liu and Yao Zhao and Rishabh Joshi and Misha Khalman and Mohammad Saleh and Peter J. Liu and Jialu Liu",
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
      },
      {
        "key": "rafailov2024directpreferenceoptimizationlanguage",
        "author": "Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "bakker2022finetuninglanguagemodelsagreement",
        "author": "Michiel A. Bakker and Martin J. Chadwick and Hannah R. Sheahan and Michael Henry Tessler and Lucy Campbell-Gillingham and Jan Balaguer and Nat McAleese and Amelia Glaese and John Aslanides and Matthew M. Botvinick and Christopher Summerfield",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences"
      },
      {
        "key": "casper2023openproblemsfundamentallimitations",
        "author": "Stephen Casper and Xander Davies and Claudia Shi and Thomas Krendl Gilbert and J\u00e9r\u00e9my Scheurer and Javier Rando and Rachel Freedman and Tomasz Korbak and David Lindner and Pedro Freire and Tony Wang and Samuel Marks and Charbel-Rapha\u00ebl Segerie and Micah Carroll and Andi Peng and Phillip Christoffersen and Mehul Damani and Stewart Slocum and Usman Anwar and Anand Siththaranjan and Max Nadeau and Eric J. Michaud and Jacob Pfau and Dmitrii Krasheninnikov and Xin Chen and Lauro Langosco and Peter Hase and Erdem B\u0131y\u0131k and Anca Dragan and David Krueger and Dorsa Sadigh and Dylan Hadfield-Menell",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "key": "zhong2024provable",
        "author": "Zhong, Huiying and Deng, Zhun and Su, Weijie J and Wu, Zhiwei Steven and Zhang, Linjun",
        "title": "Provable multi-party reinforcement learning with diverse human feedback"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wu2023finegrainedhumanfeedbackgives",
        "author": "Zeqiu Wu and Yushi Hu and Weijia Shi and Nouha Dziri and Alane Suhr and Prithviraj Ammanabrolu and Noah A. Smith and Mari Ostendorf and Hannaneh Hajishirzi",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "key": "zhou2024onepreferencefitsallalignmentmultiobjectivedirect",
        "author": "Zhanhui Zhou and Jie Liu and Jing Shao and Xiangyu Yue and Chao Yang and Wanli Ouyang and Yu Qiao",
        "title": "Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization"
      },
      {
        "key": "wang2024arithmeticcontrolllmsdiverse",
        "author": "Haoxiang Wang and Yong Lin and Wei Xiong and Rui Yang and Shizhe Diao and Shuang Qiu and Han Zhao and Tong Zhang",
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"
      },
      {
        "key": "yang2024rewardsincontextmultiobjectivealignmentfoundation",
        "author": "Rui Yang and Xiaoman Pan and Feng Luo and Shuang Qiu and Han Zhong and Dong Yu and Jianshu Chen",
        "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "chakraborty2024maxminrlhfequitablealignmentlarge",
        "author": "Souradip Chakraborty and Jiahao Qiu and Hui Yuan and Alec Koppel and Dinesh Manocha and Furong Huang and Amrit Bedi and Mengdi Wang",
        "title": "MaxMin-{RLHF}: Alignment with Diverse Human Preferences"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "bakker2022finetuninglanguagemodelsagreement",
        "author": "Michiel A. Bakker and Martin J. Chadwick and Hannah R. Sheahan and Michael Henry Tessler and Lucy Campbell-Gillingham and Jan Balaguer and Nat McAleese and Amelia Glaese and John Aslanides and Matthew M. Botvinick and Christopher Summerfield",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences"
      },
      {
        "key": "park2024rlhfheterogeneousfeedbackpersonalization",
        "author": "Chanwoo Park and Mingyang Liu and Dingwen Kong and Kaiqing Zhang and Asuman Ozdaglar",
        "title": "RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation"
      },
      {
        "key": "zhong2024provablemultipartyreinforcementlearning",
        "author": "Huiying Zhong and Zhun Deng and Weijie J. Su and Zhiwei Steven Wu and Linjun Zhang",
        "title": "Provable Multi-Party Reinforcement Learning with Diverse Human Feedback"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "chidambaram2024directpreferenceoptimizationunobserved",
        "author": "Keertana Chidambaram and Karthik Vinay Seetharaman and Vasilis Syrgkanis",
        "title": "Direct Preference Optimization With Unobserved Preference Heterogeneity"
      },
      {
        "key": "jang2023personalizedsoupspersonalizedlarge",
        "author": "Joel Jang and Seungone Kim and Bill Yuchen Lin and Yizhong Wang and Jack Hessel and Luke Zettlemoyer and Hannaneh Hajishirzi and Yejin Choi and Prithviraj Ammanabrolu",
        "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging"
      },
      {
        "key": "ji2023beavertailsimprovedsafetyalignment",
        "author": "Jiaming Ji and Mickel Liu and Juntao Dai and Xuehai Pan and Chi Zhang and Ce Bian and Chi Zhang and Ruiyang Sun and Yizhou Wang and Yaodong Yang",
        "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"
      }
    ]
  }
]