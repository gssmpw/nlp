\section{Related Work}
\textbf{RLHF.} RLHF has proven effective across various tasks, including text summarization \cite{ziegler2020finetuninglanguagemodelshuman, stiennon2022learningsummarizehumanfeedback}, translation \cite{kreutzer2018reliabilitylearnabilityhumanbandit}, and image generation \cite{wu2023humanpreferencescorebetter,lee2023aligningtexttoimagemodelsusing}. Traditional RLHF pipelines involve training a reward model from human feedback and optimizing policy using reinforcement learning algorithms like Proximal Policy Optimization (PPO) \cite{schulman2017proximalpolicyoptimizationalgorithms}. However, this process is often complex, unstable, and computationally intensive. To address these challenges, RL-free methods have emerged as efficient alternatives \cite{chen2024selfplayfinetuningconvertsweak, liu2024statisticalrejectionsamplingimproves, rafailov2024directpreferenceoptimizationlanguage}, aligning LLMs with average labeler preference while preserving the core principles of RLHF.

\noindent 
\textbf{Diverse Preference Alignment.} 
Single RLHF approaches often fail to capture the diversity of human preferences \cite{bakker2022finetuninglanguagemodelsagreement, casper2023openproblemsfundamentallimitations,zhong2024provable}. In response, recent studies explored multi-objective settings, decomposing human feedback into distinct dimensions, fitting separate reward models to apply aggregation. MORLHF \cite{wu2023finegrainedhumanfeedbackgives, zhou2024onepreferencefitsallalignmentmultiobjectivedirect, wang2024arithmeticcontrolllmsdiverse,yang2024rewardsincontextmultiobjectivealignmentfoundation} employs linear scalarization, while MaxMin-RLHF \cite{chakraborty2024maxminrlhfequitablealignmentlarge} adopts a minimization strategy to achieve equitable alignment. For additional techniques and theoretical analyses, we refer readers to \cite{bakker2022finetuninglanguagemodelsagreement, park2024rlhfheterogeneousfeedbackpersonalization, zhong2024provablemultipartyreinforcementlearning}.
Another line of research \cite{chidambaram2024directpreferenceoptimizationunobserved, jang2023personalizedsoupspersonalizedlarge,ji2023beavertailsimprovedsafetyalignment} assumes that the optimal policy can be expressed as a linear combination of language models trained on diverse preference objectives. However, this approach lacks explicit interpretation or theoretical justification for its assumption.

\vspace{-0.12in}