\section{Related Work}
\textbf{RLHF.} RLHF has proven effective across various tasks, including text summarization ____, translation ____, and image generation ____. Traditional RLHF pipelines involve training a reward model from human feedback and optimizing policy using reinforcement learning algorithms like Proximal Policy Optimization (PPO) ____. However, this process is often complex, unstable, and computationally intensive. To address these challenges, RL-free methods have emerged as efficient alternatives ____, aligning LLMs with average labeler preference while preserving the core principles of RLHF.

\noindent 
\textbf{Diverse Preference Alignment.} 
Single RLHF approaches often fail to capture the diversity of human preferences ____. In response, recent studies explored multi-objective settings, decomposing human feedback into distinct dimensions, fitting separate reward models to apply aggregation. MORLHF ____ employs linear scalarization, while MaxMin-RLHF ____ adopts a minimization strategy to achieve equitable alignment. For additional techniques and theoretical analyses, we refer readers to ____.
Another line of research ____ assumes that the optimal policy can be expressed as a linear combination of language models trained on diverse preference objectives. However, this approach lacks explicit interpretation or theoretical justification for its assumption.

\vspace{-0.12in}