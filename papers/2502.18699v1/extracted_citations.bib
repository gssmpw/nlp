@misc{bakker2022finetuninglanguagemodelsagreement,
      title={Fine-tuning language models to find agreement among humans with diverse preferences}, 
      author={Michiel A. Bakker and Martin J. Chadwick and Hannah R. Sheahan and Michael Henry Tessler and Lucy Campbell-Gillingham and Jan Balaguer and Nat McAleese and Amelia Glaese and John Aslanides and Matthew M. Botvinick and Christopher Summerfield},
      year={2022},
      eprint={2211.15006},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.15006}, 
}

@misc{casper2023openproblemsfundamentallimitations,
      title={Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback}, 
      author={Stephen Casper and Xander Davies and Claudia Shi and Thomas Krendl Gilbert and Jérémy Scheurer and Javier Rando and Rachel Freedman and Tomasz Korbak and David Lindner and Pedro Freire and Tony Wang and Samuel Marks and Charbel-Raphaël Segerie and Micah Carroll and Andi Peng and Phillip Christoffersen and Mehul Damani and Stewart Slocum and Usman Anwar and Anand Siththaranjan and Max Nadeau and Eric J. Michaud and Jacob Pfau and Dmitrii Krasheninnikov and Xin Chen and Lauro Langosco and Peter Hase and Erdem Bıyık and Anca Dragan and David Krueger and Dorsa Sadigh and Dylan Hadfield-Menell},
      year={2023},
      eprint={2307.15217},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2307.15217}, 
}

@misc{chen2024selfplayfinetuningconvertsweak,
      title={Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models}, 
      author={Zixiang Chen and Yihe Deng and Huizhuo Yuan and Kaixuan Ji and Quanquan Gu},
      year={2024},
      eprint={2401.01335},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.01335}, 
}

@misc{chidambaram2024directpreferenceoptimizationunobserved,
      title={Direct Preference Optimization With Unobserved Preference Heterogeneity}, 
      author={Keertana Chidambaram and Karthik Vinay Seetharaman and Vasilis Syrgkanis},
      year={2024},
      eprint={2405.15065},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.15065}, 
}

@misc{jang2023personalizedsoupspersonalizedlarge,
      title={Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging}, 
      author={Joel Jang and Seungone Kim and Bill Yuchen Lin and Yizhong Wang and Jack Hessel and Luke Zettlemoyer and Hannaneh Hajishirzi and Yejin Choi and Prithviraj Ammanabrolu},
      year={2023},
      eprint={2310.11564},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11564}, 
}

@misc{ji2023beavertailsimprovedsafetyalignment,
      title={BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset}, 
      author={Jiaming Ji and Mickel Liu and Juntao Dai and Xuehai Pan and Chi Zhang and Ce Bian and Chi Zhang and Ruiyang Sun and Yizhou Wang and Yaodong Yang},
      year={2023},
      eprint={2307.04657},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.04657}, 
}

@misc{kreutzer2018reliabilitylearnabilityhumanbandit,
      title={Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning}, 
      author={Julia Kreutzer and Joshua Uyheng and Stefan Riezler},
      year={2018},
      eprint={1805.10627},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1805.10627}, 
}

@misc{lee2023aligningtexttoimagemodelsusing,
      title={Aligning Text-to-Image Models using Human Feedback}, 
      author={Kimin Lee and Hao Liu and Moonkyung Ryu and Olivia Watkins and Yuqing Du and Craig Boutilier and Pieter Abbeel and Mohammad Ghavamzadeh and Shixiang Shane Gu},
      year={2023},
      eprint={2302.12192},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.12192}, 
}

@misc{liu2024statisticalrejectionsamplingimproves,
      title={Statistical Rejection Sampling Improves Preference Optimization}, 
      author={Tianqi Liu and Yao Zhao and Rishabh Joshi and Misha Khalman and Mohammad Saleh and Peter J. Liu and Jialu Liu},
      year={2024},
      eprint={2309.06657},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.06657}, 
}

@misc{park2024rlhfheterogeneousfeedbackpersonalization,
      title={RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation}, 
      author={Chanwoo Park and Mingyang Liu and Dingwen Kong and Kaiqing Zhang and Asuman Ozdaglar},
      year={2024},
      eprint={2405.00254},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.00254}, 
}

@misc{rafailov2024directpreferenceoptimizationlanguage,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2024},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.18290}, 
}

@misc{schulman2017proximalpolicyoptimizationalgorithms,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}, 
}

@misc{stiennon2022learningsummarizehumanfeedback,
      title={Learning to summarize from human feedback}, 
      author={Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
      year={2022},
      eprint={2009.01325},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2009.01325}, 
}

@misc{wang2024arithmeticcontrolllmsdiverse,
      title={Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards}, 
      author={Haoxiang Wang and Yong Lin and Wei Xiong and Rui Yang and Shizhe Diao and Shuang Qiu and Han Zhao and Tong Zhang},
      year={2024},
      eprint={2402.18571},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.18571}, 
}

@misc{wu2023finegrainedhumanfeedbackgives,
      title={Fine-Grained Human Feedback Gives Better Rewards for Language Model Training}, 
      author={Zeqiu Wu and Yushi Hu and Weijia Shi and Nouha Dziri and Alane Suhr and Prithviraj Ammanabrolu and Noah A. Smith and Mari Ostendorf and Hannaneh Hajishirzi},
      year={2023},
      eprint={2306.01693},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.01693}, 
}

@misc{wu2023humanpreferencescorebetter,
      title={Human Preference Score: Better Aligning Text-to-Image Models with Human Preference}, 
      author={Xiaoshi Wu and Keqiang Sun and Feng Zhu and Rui Zhao and Hongsheng Li},
      year={2023},
      eprint={2303.14420},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.14420}, 
}

@misc{yang2024rewardsincontextmultiobjectivealignmentfoundation,
      title={Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment}, 
      author={Rui Yang and Xiaoman Pan and Feng Luo and Shuang Qiu and Han Zhong and Dong Yu and Jianshu Chen},
      year={2024},
      eprint={2402.10207},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.10207}, 
}

@article{zhong2024provable,
  title={Provable multi-party reinforcement learning with diverse human feedback},
  author={Zhong, Huiying and Deng, Zhun and Su, Weijie J and Wu, Zhiwei Steven and Zhang, Linjun},
  journal={arXiv preprint arXiv:2403.05006},
  year={2024}
}

@misc{zhong2024provablemultipartyreinforcementlearning,
      title={Provable Multi-Party Reinforcement Learning with Diverse Human Feedback}, 
      author={Huiying Zhong and Zhun Deng and Weijie J. Su and Zhiwei Steven Wu and Linjun Zhang},
      year={2024},
      eprint={2403.05006},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.05006}, 
}

@misc{zhou2024onepreferencefitsallalignmentmultiobjectivedirect,
      title={Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization}, 
      author={Zhanhui Zhou and Jie Liu and Jing Shao and Xiangyu Yue and Chao Yang and Wanli Ouyang and Yu Qiao},
      year={2024},
      eprint={2310.03708},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.03708}, 
}

@misc{ziegler2020finetuninglanguagemodelshuman,
      title={Fine-Tuning Language Models from Human Preferences}, 
      author={Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
      year={2020},
      eprint={1909.08593},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.08593}, 
}

