\section{Related Work}
% \subsection{Retrieval-augmented LLMs}
% To enhance reasoning and prediction capabilities of LLM by retrieving relevant information from vast datasets, numerous retrieval methods have been proposed**Vaswani et al., "Attention Is All You Need"**. Early approaches primarily relied on keyword frequency, with many studies directly applying BM25 for passage-level retrieval in RAG**Karpukhin et al., "DPR: Pre-Training Open-Domain Question Answering with KGs"**. These passages were represented as bags of words and ranked using term frequency-inverse document frequency (TF-IDF)**Manning et al., "Introduction to Information Retrieval"**. Subsequently, methods based on semantic similarity emerged, which encode queries and passages into a shared vector space to train embeddings that align queries closely with relevant factual passages**Bordes et al., "Translating Embeddings for Similarity Search"**. However, these approaches are not well-suited for time-series retrieval tasks, such as predicting stock price movements, where no fixed factual passages exist for retrieval. Furthermore, the highly similar nature of time-series data poses challenges for semantic similarity-based methods, as they struggle to effectively distinguish between similar patterns. Therefore, a specialized retrieval method for time-series data is required, which our model provides.

\subsection{Stock Movement Prediction}
\textbf{Non-LLM Methods.}
Traditional approaches to stock movement prediction have focused on various aspects of financial data. One prominent category of methods analyzes stock price sequences and their corresponding technical indicators**Yao et al., "Forecasting Stock Market using Deep Learning"** to identify patterns in historical data for predicting future movements. However, due to the complexity of factors influencing stock prices, subsequent methods have incorporated additional contextual information, such as news articles**Liu et al., "News-based Stock Prediction with Attention Mechanism"** or social media posts**Kim et al., "Stock Prediction using Sentiment Analysis on Social Media"**. Despite efforts, these methods are highly susceptible to noise and struggle to analyze the vast and diverse nature of financial information effectively.

\noindent
\textbf{LLM-based Methods.}
Recent studies have explored using LLMs for financial prediction tasks, either by fine-tuning open-source models or prompting advanced models like GPT-4. However, even state-of-the-art models, including GPT-4, have achieved only random-guessing-level accuracy in stock movement prediction**Brown et al., "Language Models as a Framework for Multitask Learning"**. This highlights the inherent challenges in identifying and analyzing meaningful patterns in a domain as volatile and multifaceted as stock market prediction.

\subsection{Time-series forecasting with LLMs}
% LLMs have been increasingly applied to time-series forecasting, particularly in tasks requiring the alignment of temporal and textual data. Several approaches focus on improving this alignment through different strategies. 
\textbf{Non-retrieval Methods.}
To enhance the performance of LLMs in time-series forecasting, existing methods have primarily focused on aligning temporal and textual data, either by transforming time-series into textual formats or by encoding both modalities into a unified vector space.
For instance, a study**Li et al., "Time-series to Text"** reprograms time-series data into textual representations suitable for LLMs, enhancing prediction accuracy via declarative prompts. Similarly, some other studies**Liu et al., "Financial Forecasting with Cross-Modal Alignment"** and **Kim et al., "Time-series and Text Embeddings"**, explore cross-modal alignment: the former applies LLMs to financial forecasting by integrating stock prices with news data, while the latter introduces a cross-modality framework to align time-series with text for improved predictive performance. % Building on this, a method**Zhang et al., "Semantic Alignment of Time-series and Text"**, maps time-series and text into a shared semantic space, further boosting LLM performance through strengthened data alignment. 
% Despite these advancements, efficient methods for retrieving large-scale time-series data remain underexplored, presenting a key challenge for future research.
While these advancements improve alignment, existing methods often struggle to process large-scale time-series data comprehensively due to LLM input limitations. As a result, effectively leveraging extensive historical data remains a challenge. This highlights the need for retrieval-augmented methods, a gap our approach directly addresses.

\noindent
\textbf{RAG Methods.}
With the rapid advancement of RAG techniques in various applications of LLMs, recent research has begun exploring RAG for time-series forecasting. For instance, TimeRAG**Kadav et al., "TimeRAG: Time-Series Retrieval with Graph Neural Networks"**, integrates RAG into time-series forecasting by combining Dynamic Time Warping (DTW) with LLMs to improve prediction accuracy. However, relying solely on numeric similarity is insufficient for financial time-series forecasting, as it fails to capture deeper semantic relationships. This underscores the need for a more targeted retrieval framework tailored to the complexities of financial data, a challenge our framework effectively addresses.