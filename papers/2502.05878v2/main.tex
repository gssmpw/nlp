%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
% \documentclass[sigconf,authordraft]{acmart}
% \documentclass[sigconf,natbib=true,anonymous=true]{acmart}
%\documentclass[sigconf,natbib=true,anonymous=false]{acmart}
\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2025}
% \acmYear{2025}
% \acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{July 13--18,
%   2025}{Padua, Italy}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

\usepackage{multirow}
\usepackage{makecell}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{ulem}

\definecolor{background}{RGB}{245,245,245}
\definecolor{string}{RGB}{163,21,21}
\definecolor{keyword}{RGB}{0,0,255}
\definecolor{number}{RGB}{0,128,0}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\tiny,
  % backgroundcolor=\color{background},
  stringstyle=\color{black},
  keywordstyle=\color{string},
  numberstyle=\tiny\color{gray},
  numbersep=5pt,
  stepnumber=1,
  showstringspaces=false,
  numbers=none,
  frame=none,
  breaklines=true,
  captionpos=b,
  language=Java, % JSON can use Java syntax for highlighting
}

\setcopyright{none}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
% \title{Retrieval-augmented Large Language Models for Financial Time Series Forecasting}
\title{Enhancing Financial Time-Series Forecasting with Retrieval-Augmented Large Language Models}

\author{Mengxi Xiao}
\affiliation{%
  \institution{School of Computer Science, Wuhan University}
  \city{Wuhan}
  \country{China}}
% \email{elsashaw@whu.edu.cn}

\author{Zihao Jiang}
\affiliation{%
  \institution{School of Computer Science, Wuhan University}
  \city{Wuhan}
  \country{China}}
% \email{jiangzihao@whu.edu.cn}

\author{Lingfei Qian}
\affiliation{%
  \institution{The Fin AI}
  \country{United States}
  }
% \email{lfqian94@gmail.com}

\author{Zhengyu Chen}
\affiliation{%
  \institution{School of Computer Science, Wuhan University}
  \city{Wuhan}
  \country{China}}
% \email{2019302120293@whu.edu.cn}

\author{Yueru He}
\affiliation{%
  \institution{Columbia University}
  \country{United States}
  }
% \email{yh3507@columbia.edu}

\author{Yijing Xu}
\affiliation{%
  \institution{Columbia University}
  \country{United States}
  }
% \email{yx2770@columbia.edu}

\author{Yuecheng Jiang}
\affiliation{%
  \institution{Stevens Institute of Technology}
  \country{United States}
  }
% \email{yjiang52@stevens.edu}

\author{Dong Li}
\affiliation{%
  \institution{School of Computer Science, Wuhan University}
  \country{China}
  }
% \email{ldruth1228@gmail.com}

\author {Ruey-Ling Weng}
\affiliation{
\institution{Yale University}
\country{United States}
}
% \email{ruey-ling.weng@yale.edu}

\author{Min Peng}
\affiliation{%
  \institution{School of Computer Science, Wuhan University}
  \country{China}}
% \email{pengm@whu.edu.cn}

\author{Jimin Huang}
\affiliation{%
  \institution{The Fin AI}
  \country{United States}
  }
%\email{jimin.huang@thefin.ai}

\author{Sophia Ananiadou}
\affiliation{
\institution{University of Manchester}
\country{United Kingdom}
}
% \email{sophia.ananiadou@manchester.ac.uk}

\author{Qianqian Xie}
\authornote{Corresponding author.}
\affiliation{%
  \institution{School of Computer Science, Wuhan University}
  \country{China}
  }
\email{xieq@whu.edu.cn}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Mengxi Xiao et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
% Large language models (LLMs) have greatly benefited from Retrieval-Augmented Generation (RAG), which integrates external knowledge from diverse sources such as text, tables, and images. While recent studies have begun exploring RAG for time-series data, they often rely on simplistic distance-based retrieval methods, such as embedding distances or Dynamic Time Warping (DTW). Although these approaches show improvements in general forecasting tasks, they struggle to enhance LLMs in complex financial time-series forecasting. This is because they focus on retrieving numerically similar trends rather than identifying underlying relevance—patterns that are meaningfully connected to the forecasting objective. Additionally, the lack of clear semantic connections between historical time-series data and task contexts makes it challenging for models, or even humans, to determine which data is beneficial. To address these challenges, we introduce Financial Time-Series Retriever (FinSeer), a novel retrieval model designed to enhance LLMs' time-series forecasting capabilities. FinSeer is the first retriever specifically tailored for continuous and complex temporal sequences, leveraging historical time-series data and financial indicators to improve LLMs' performance. Guided by direct feedback from LLMs, FinSeer identifies the most relevant time-series data for the current context, effectively aligning retrieval with LLMs' objectives and enabling more accurate, context-aware predictions. Experimental results demonstrate FinSeer's effectiveness, showcasing its ability to significantly enhance forecasting performance and highlighting the potential of LLMs in time-series prediction tasks.

% Stock movement prediction, a fundamental task in financial time-series forecasting, requires identifying and retrieving critical influencing factors from vast amounts of time-series data. However, existing text-trained or numeric similarity-based retrieval methods fall short in handling complex financial analysis. To address this, we propose the first retrieval-augmented generation (RAG) framework for financial time-series forecasting, featuring three key innovations: a fine-tuned 1B parameter large language model (StockLLM) as the backbone, a novel candidate selection method leveraging LLM feedback, and a training objective that maximizes similarity between queries and historically significant sequences. This enables our retriever, FinSeer, to uncover meaningful patterns while minimizing noise in complex financial data. We also construct new datasets integrating financial indicators and historical stock prices to train FinSeer and ensure robust evaluation. Experimental results demonstrate that our RAG framework outperforms bare StockLLM and random retrieval, highlighting its effectiveness, while FinSeer surpasses existing retrieval methods, achieving an 8\% higher accuracy on BIGDATA22 and retrieving more impactful sequences. This work underscores the importance of tailored retrieval models in financial forecasting and provides a novel framework for future research.

Stock movement prediction, a critical task in financial time-series forecasting, relies on identifying and retrieving key influencing factors from vast and complex datasets. However, traditional text-trained or numeric similarity-based retrieval methods often struggle to handle the intricacies of financial data. To address this, we propose the first retrieval-augmented generation (RAG) framework specifically designed for financial time-series forecasting. Our framework incorporates three key innovations: a fine-tuned 1B large language model (StockLLM) as its backbone, a novel candidate selection method enhanced by LLM feedback, and a training objective that maximizes the similarity between queries and historically significant sequences. These advancements enable our retriever, FinSeer, to uncover meaningful patterns while effectively minimizing noise in complex financial datasets. To support robust evaluation, we also construct new datasets that integrate financial indicators and historical stock prices. Experimental results demonstrate that our RAG framework outperforms both the baseline StockLLM and random retrieval methods, showcasing its effectiveness. FinSeer, as the retriever, achieves an 8\% higher accuracy on the BIGDATA22 benchmark and retrieves more impactful sequences compared to existing retrieval methods. This work highlights the importance of tailored retrieval models in financial forecasting and provides a novel, scalable framework for future research in the field.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317.10003338.10003346</concept_id>
       <concept_desc>Information systems~Top-k retrieval in databases</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003338.10003342</concept_id>
       <concept_desc>Information systems~Similarity measures</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003338.10010403</concept_id>
       <concept_desc>Information systems~Novelty in information retrieval</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Top-k retrieval in databases}
\ccsdesc[500]{Information systems~Similarity measures}
\ccsdesc[500]{Information systems~Novelty in information retrieval}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Retrieval-augmented generation, Financial Time-series forecasting, Stock Movement Prediction, Large Language Model}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:introduction}
% Retrieval-augmented generation (RAG) has emerged as a powerful paradigm in artificial intelligence~\citep{gao2023retrieval}, combining retrieval-based systems with generative models to enhance large language models (LLMs). By integrating external knowledge, RAG enables dynamic access to vast repositories, overcoming limitations like knowledge cutoffs and factual inconsistencies. Initially successful in unstructured text~\citep{huang2024survey}, RAG has been extended to structured data, such as tables and knowledge graphs~\citep{joshi2024robust,he2024g,cui2024tabular}, showcasing its versatility across domains.
% Beyond these applications, the retrieval of time-series data is critical in many real-world scenarios, such as weather forecasting~\citep{govett2024exascale}, energy demand prediction~\citep{afzal2024building}, healthcare monitoring~\citep{reed2005heart}, and financial market analysis~\citep{nelson2017stock}. Recent studies begins to explore the use of RAG in time-series data. However, they often simplify the retrieval process to distance-based calculations. For instance, RATD~\citep{liu2024retrieval} relies on embedding distances from a pretrained encoder, while TimeRAG~\citep{yang2024timerag} directly employs Dynamic Time Warping (DTW) for retrieval. Although these approaches have shown improvements in general time-series forecasting tasks, they struggle to enhance LLMs in complex financial time-series forecasting tasks. This is because they primarily focus on retrieving numerically similar trends rather than identifying underlying relevance—patterns that are meaningfully connected to the forecasting objective.
% Therefore, there still exists significant challenges in financial series retrieval: (1) Integration Complexity: Incorporating RAG with time-series data is non-trivial. Unlike text or tabular data, where relevance is often clear, determining how a time-series segment contributes to forecasting is challenging due to the lack of a clear "ground truth" for retriever training. (2) Underlying Relevance: Retrieving relevant series involves uncovering underlying relevance rather than shallow patterns, which demands sophisticated analysis to identify meaningful trends and relationships. (3) Alignment Between Retriever and LLM: Existing time-series RAG methods often fail to bridge the gap between what the retriever deems important and what the LLM prioritizes. Without this alignment, the retrieved information may not effectively enhance the LLM's forecasting ability, limiting the overall performance.
Financial time-series forecasting plays a pivotal role in ensuring market stability and efficiency, directly impacting critical areas such as investment strategies, risk management, and the formulation of economic policies~\citep{fama2000forecasting}. One of the most prominent tasks in this domain is stock movement prediction~\citep{xie2023wall,xie2024open,xie2024finben}, which focuses on forecasting the direction of price changes, i.e. whether a stock will rise or fall. Accurate predictions of financial metrics, including stock movements, interest rates, and economic indicators, are essential for a wide range of stakeholders, from individual investors to large financial institutions. However, the inherent complexity and volatility of financial markets pose significant challenges to forecasting, necessitating the use of advanced methodologies to effectively analyze vast and often noisy datasets.

Traditional stock movement prediction methods~\citep{feng2018enhancing,qin2017dual} relied on analyzing sequences of stock prices and financial indicators\footnote{Financial indicators~\citep{jansen2020machine} are professional market analysis metrics derived from price data, where their values or crossover points signal fluctuations in the stock market, indicating rising or falling trends. However, relying on one or two indicators can introduce bias, and if an indicator only provides meaningful information at crossover points, it may fail to offer valuable insights during periods without crossings.} to identify patterns, but they often failed to capture the semantic connections between these sequences, a task at which large language models (LLMs) excel~\citep{huang2022towards}. However, LLM-based methods~\citep{xie2023pixiu,xie2024finben,xie2023wall} have primarily shifted focus to leveraging textual data such as news and tweets, often using only the past several days' closing prices as reference while overlooking the rich insights embedded in historical time-series data~\citep{bustos2020stock}. This highlights the need to effectively integrate financial time-series data with LLMs. Directly working with such data presents a challenge due to its vast scale, encompassing both the breadth of influencing variables and the depth of historical trends. To address this, a retrieval-based approach is necessary to efficiently sift through extensive time-series data, extracting meaningful information to enhance LLMs' ability to deliver more accurate and robust stock movement predictions.

Although retrieval-augmented generation (RAG) methods have been applied for various tasks~\citep{joshi2024robust,he2024g,cui2024tabular}, it is challenging to directly apply them to financial time-series retrieval. First, most embedding-based retrieval methods (retrievers)~\citep{SuSKWHOYSZ023,li2023angle,bge_embedding,zhang2023retrieve,wang2023improving} are trained on textual data and struggle to process numeric time-series data, which lacks explicit semantic information. 
% While some studies apply existing retrievers to simple RAG tasks~\citep{liu2024retrieval}, they are ineffective for complex financial forecasting. 
Second, distance-based retrieval methods like Dynamic Time Warping (DTW)~\citep{yang2024timerag} focus on numeric similarities and fail to capture deeper semantic relationships or contextual nuances essential for stock movement prediction. These limitations underscore the need for a more tailored retrieval framework designed for financial time-series data.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/framework.pdf}
    \Description{The framework of FinSeer, illustrating its components and workflow.}
    \caption{Overview of our time-series RAG framework.}
    \label{fig:framework}
\end{figure*}

To address these challenges, we introduce a retrieval-augmented framework for stock movement prediction, featuring the first dedicated retriever for financial time-series forecasting, \textbf{Fin}ancial Time-\textbf{Se}ries Retriev\textbf{er} (\textbf{FinSeer}). Unlike previous RAG methods, which rely on pre-trained encoders or distance-based metrics, our framework is the first to train a dedicated retriever tailored for continuous and complex temporal sequences. The whole retrieval framework is shown in Figure \ref{fig:framework}.



% In the data construction process, FinSeer integrates financial indicators alongside historical stock prices into the candidate pool. We first consult domain experts to identify frequently used indicators, then calculate mutual information scores between these indicators and forward returns to select the 34 most informative indicators. By incorporating domain-specific financial metrics, FinSeer embeds financial expertise into the retrieval process, allowing it to identify meaningful patterns beyond superficial trends and significantly enhancing its forecasting capabilities.
% During inference, FinSeer operates on an evolving candidate pool, updating related sequences up to the day before the query trading day.

% 原本这一段的问题：没有说新数据集的好处
To address the limitations of existing datasets, which often contain only price data, and to better align with professional financial analysis, we construct new datasets by integrating 20 groups of financial indicators. These indicators provide critical insights into market behavior that stock prices alone cannot capture\footnote{For instance, the overbought area in the KDJ indicator group~\citep{wu2015technical}, where \(K > 80\), \(D > 70\), and \(J > 90\), signals a potential falling trend.}. We begin by selecting high-trade-volume U.S. stocks, and the indicators are chosen through domain expert consultation and mutual information analysis. These indicators are then serialized and divided into train, valid and test sets. By training the retriever on datasets enriched with diverse financial indicators, our retrieval framework incorporates financial domain expertise, enabling it to interpret the implications of trending signals that uncover deeper patterns beyond surface-level trend similarities. The testing set, designed to mimic real-world economic analysis, ensures a robust evaluation of model performance in practical financial scenarios.

% 原本这一段的问题：没有具体说new training objective的优势和怎么做的
To train our retriever to identify semantically related sequences and align the retriever's priorities with those of the LLM, we introduce a novel retrieval mechanism with three key innovations: a new backbone LLM, a novel method for selecting positive and negative candidates, and a new training objective. First, to activate the LLM's inherent knowledge and ensure instruction-following capabilities, we fine-tune a 1B parameter LLM (LLaMA3.2-1B Instruct)~\citep{grattafiori2024llama3herdmodels}, creating StockLLM as the backbone model of our retriever. By using a smaller LLM, we establish a more challenging experimental setup, ensuring performance improvements are attributable to FinSeer rather than the LLM's capacity. Next, StockLLM identifies time-series segments that enhance the generative process, feedbacks from StockLLM are used to select the most beneficial sequences that could lead the model to make the correct decision as positive candidates and the least beneficial as negative candidates. This could bridge the gap between the retriever and the LLM and ensure that retrieved data aligns with the LLM's forecasting priorities. Finally, the training objective, inspired by~\citet{zhang2023retrieve}, ensures the retriever distinguishing historically significant sequences (positive candidates) from noisy sequences (negative candidates) by maximizing the similarity between the query and candidate sequences. This prioritizes meaningful patterns while minimizing irrelevant data, outperforming traditional methods that struggle to separate signal from noise in complex financial time series.

Based on FinSeer, the inference process of our RAG framework consists of three steps: (1) using FinSeer to retrieve relevant sequences from the candidate pool, (2) incorporating these sequences into StockLLM's input context, and (3) combining this enriched context with the original query to perform forecasting. This approach effectively integrates salient historical patterns with current data, improving prediction accuracy.

% To validate our approach, we focus on stock movement prediction~\citep{xie2023wall,xie2024open,xie2024finben}, a challenging task that uniquely tests time-series retrieval due to the complexity and non-stationarity of financial data. Irregular patterns, volatility, and temporal dependencies~\citep{xie2024deep} make it difficult to determine whether a time-series segment benefits the forecasting context, providing a demanding testbed for our method.

% 原本这一段的问题：our evaluation focuses on two parts: (1) the RAG framework (2) the retriever. 需要分别来写
To thoroughly evaluate the performance of our RAG framework and the retriever, we rely solely on temporal information, excluding additional textual data like news. For the RAG framework evaluation, StockLLM serves as the backbone model, integrated with different retrieval methods for comparison. 
Experimental results demonstrate that using our RAG framework outperforms bare StockLLM, indicating that the RAG framework enhances LLM performance by incorporating relevant time-series data as financial context. However, RAG with retrieval models trained on text data does not always improve performance compared to bare StockLLM. In some cases, performance even declines. This suggests that these retrieval models cannot effectively identify beneficial time-series data that could enhance the model’s predictions, which highlights a fundamental gap between text-based retrieval and time-series data.
Furthermore, RAG framework with FinSeer consistently surpasses all other retrieval models, such as Instructor~\citep{SuSKWHOYSZ023}, BGE (BAAI General Embedding)~\citep{bge_embedding}, LLM-Embedder~\citep{zhang2023retrieve}, UAE~\citep{li2023angle}, and E5-mistral-7b-instruct~\citep{wang2023improving}, demonstrating the effectiveness of our trained FinSeer in identifying relevant time-series financial data to enhance stock movement prediction.
% our RAG framework outperforms both bare StockLLM and random retrieval, highlighting its effectiveness in enhancing stock movement prediction.
% For the retriever evaluation, FinSeer is compared against various retrieval methods, including DTW-distance-based retrieval and state-of-the-art retrieval models trained on textual data, such as Instructor~\citep{SuSKWHOYSZ023}, BGE (BAAI General Embedding)~\citep{bge_embedding}, LLM-Embedder~\citep{zhang2023retrieve}, UAE~\citep{li2023angle}, and E5-mistral-7b-instruct~\citep{wang2023improving}. FinSeer outperforms these methods across most datasets, showcasing its superiority in handling time-series data.
Further analysis of retrieval results reveals that FinSeer uniquely identifies relevant financial indicators that enhance LLM performance, while other retrievers predominantly select sequences based on superficial similarities, such as close price and adjusted close price. This behavior underscores a critical limitation of text-trained models: they focus on surface-level patterns and fail to discern truly valuable time-series information. In contrast, FinSeer effectively bridges this gap, demonstrating its ability to uncover underlying relevance in financial data.

% contribution
Our contributions are summarized as follows:
\begin{enumerate}

% 1. the contribution of framework, 原本写法的问题在于太空泛
% \item We are the first to combine RAG with LLMs for financial time-series forecasting, introducing a novel framework that bridges the gap between retrieval models and LLMs by using feedbacks from LLMs to optimize the retriever.
\item We propose the first RAG framework for financial time-series forecasting, in which the retrieval mechanism features three key innovations: a fine-tuned 1B parameter LLM (StockLLM) as the backbone, a novel candidate selection method leveraging LLM feedback to identify beneficial sequences, and a training objective that maximizes similarity between queries and historically significant sequences, enabling the retriever to uncover meaningful patterns while minimizing noise in complex financial data.

% 1. separate contributions of data and retriever
% \item We propose a new retriever, FinSeer, specifically designed for time-series forecasting, featuring advanced data selection techniques and a unique data construction process. Leveraging LLM feedback, our retriever incorporates both shallow time-series patterns (e.g., price trends) and derived financial indicators (e.g., moving averages, Bollinger Bands), enabling a more comprehensive and context-aware retrieval process.

\item We construct new training and testing datasets to address the limitations of existing ones containing only price data, and better align with real-world economic analysis. Besides price data, these datasets integrate financial indicators that signal upward or downward trends, carefully selected through domain expert consultation and mutual information analysis. By using new training datasets, we train the retriever to uncover deeper patterns beyond surface-level trends during training. While the testing set, designed to mimic real-world scenarios, ensures robust evaluation of model performance in practical financial applications.

\item We propose a new retriever, FinSeer, specifically designed for time-series forecasting, trained on newly constructed datasets that integrate financial indicators alongside historical stock prices. By leveraging LLM feedback and prioritizing historically significant sequences, FinSeer uncovers deeper patterns beyond surface-level trends, setting it apart from traditional methods.

% 原本的问题：没有将RAG framework的评测结果和retriever的评测结果分开写
\item Experimental results demonstrate that our RAG framework outperforms bare StockLLM and random retrieval, highlighting its effectiveness, while our retriever, FinSeer, surpasses existing RAG methods in stock movement prediction across three datasets, achieving an 8\% higher accuracy on BIGDATA22 compared to a general-purpose LLM-feedback-based retriever. Furthermore, FinSeer retrieves more dynamic and impactful sequences, showcasing its ability to identify and leverage the most relevant patterns for forecasting.
\end{enumerate}

\section{Problem Definition and Goals}
Retrieval-augmented financial time-series forecasting~\citep{sezer2020financial} involves predicting future values or trends ($G$) based on a given query sequence ($q$) and a set of retrieved historical sequences ($c$). These sequences are collected over time at regular intervals. In the retrieval process, the goal of the retrieval model ($R$) is to efficiently identify and extract the most useful historical sequences from a large pool of candidates. By providing relevant context, the retrieval model enhances the forecasting model's ability to make accurate and reliable predictions.

\noindent
\textbf{Stock Movement Prediction Task.} In the specific task of stock movement prediction~\citep{xu2018stock}, the problem is framed as a binary classification task: predicting whether a stock's price will \textit{rise} or \textit{fall} on the next trading day. Given a query sequence \(q\), which represents the stock's price over the previous \(t\) days, the model retrieves relevant sequences as context and predicts the stock's movement \(M_{q,d}\) for the next trading day \(d\). Table \ref{tab:symbol} defines the major symbols used in this paper.

\begin{table*}[ht]
\centering
\renewcommand\arraystretch{1.2}
\caption{The definition of symbols.}
\label{tab:symbol}
\scalebox{0.7}{
\begin{tabular}{p{3.5cm}p{4cm}p{12cm}}
\hline
\makecell*[l]{General Time-series\\Symbol} & \makecell*[l]{Stock Movement\\Prediction Symbol} & Definitions
\\ \hline
$q$
& 
$q = \{ q_{d-t}, ..., q_{d-1} \}$
& 
The query time-series data. In stock movement prediction, $q$ refers to the query stock price sequence of length $t$, containing stock price data from trading day $d-t$  to trading day $d-1$.
\\ [0cm]
\makecell*[l]{$G(q)$}
& 
\makecell*[l]{$G(q,d) \in \{\textit{rise}, \textit{fall}\}$
}
&
The final output $G$ given the query $q$. In stock movement prediction, $G(q,d)$ shows the generation $G$ of the query stock $q$ on the query trading day $d$, belonging to \textit{rise} or \textit{fall}.
\\ [0cm]
$P(c) = LLM(O|q,c)$
&
$P(c) = LLM(M_d|q,c)$
&
The probability $P$ of the LLM to generate an accurate output $O$ given the query sequence $q$ and a candidate sequence $c$. In stock movement prediction, $P(c)$ refers to generating the accurate movement $M$ on the query trading day $d$.
\\ [0cm]
\multicolumn{2}{l}{$\mathbb{C_P} = \{ c_i \mid i = 1, \ldots, k \}$}
&
The set of top-k retrieved sequences as positive examples, where $P(c_i) \geq P(c_{i+1})$.
\\ [0cm]
\multicolumn{2}{l}{$\mathbb{C_N} = \{ c_i \mid i = k+1, \ldots, n \}$}
&
The set of negative retrieved sequences, where $P(c_i) \geq P(c_{i+1})$.
\\ [0cm]
\multicolumn{2}{l}{$w_i$}
&
The soft weight of the $i^{th}$ retrieved sequences, where $w_i = P(c_i), i=1, ..., k$.
\\ [0cm]
\multicolumn{2}{l}{$R$}
&
The retrieve model.
\\
\hline
\end{tabular}
}
\end{table*}

\noindent
\textbf{Rise/Fall Threshold Settings.}
To classify daily movements as rise or fall, we first calculate returns $R_{t}$, which represents the percentage change in the closing price over consecutive days.
\begin{equation}
    \scalebox{0.9}{$
        \begin{aligned}
            R_{t} 
            &= \frac{\text{adj\_close$_d$} - \text{adj\_close$_{d-1}$}}{\text{adj\_close$_{d-1}$}} * 100
        \end{aligned}
    $}
\end{equation}
Following~\citet{yoo2021accurate} and ~\citet{soun2022accurate}, we classify the movement as rise if return exceeds 0.55, fall if it is below -0.5. In line with previous experimental settings~\citep{xie2023pixiu,xie2024finben}, we do not evaluate freeze cases. However, we include sequences with $R_{t} \in [-0.5, 0.55]$ as freeze candidates in the candidate pool, ensuring a diverse and comprehensive set of historical data for context.
\begin{equation}
    \scalebox{0.9}{$
        \begin{aligned}
            M_{q,d} &= 
            \begin{cases} 
                \text{rise}, & R_{t} > 0.55 \\
                \text{fall}, & R_{t} < -0.5 \\
                \text{freeze}, &  -0.5 \leq R_{t} \leq 0.55
            \end{cases}
        \end{aligned}
    $}
\end{equation}

\noindent
\textbf{Rationale for Unbalanced Thresholds.}
The asymmetrical thresholds for classifying daily movements (0.55 for rises and -0.5 for falls) reflect the inherent dynamics of stock market behavior. Stock prices typically rise gradually due to sustained investor optimism but fall sharply during panic selling or profit-taking. The stricter rise threshold prevents minor upward fluctuations from being misclassified as significant increases, while the more lenient fall threshold ensures meaningful downward trends are captured. This approach aligns with market realities, improving the reliability of movement classifications.

\section{The RAG Framework}
% 在开头强调创新点，不要平铺直叙
To identify valuable sequences from vast amounts of stock data and conduct stock movement prediction, we propose a novel RAG framework with new datasets and a novel retrieval mechanism with a new backbone LLM. First, we construct three datasets that integrate financial indicators alongside historical stock prices, addressing the limitations of existing datasets and better aligning with real-world economic analysis. Second, we introduce StockLLM, a fine-tuned 1B parameter LLM, as the backbone to ensure instruction-following capabilities and establish a challenging experimental setup. Finally, we design a novel retrieval mechanism that leverages LLM feedback to select positive and negative candidates and employs a new training objective to prioritize historically significant sequences. This comprehensive framework bridges the gap between retrieval models and LLMs, enhancing the time-series forecasting process.
% Our method focuses on optimizing the retrieval stage to extract relevant content and seamlessly integrate it into LLMs. The framework includes raw data collection, sequence serialization and training. During candidate selection (Section \ref{sec:candidate_selection}), we classify positive candidates based on high-performance feedback from the LLM, while the remaining data are treated as negative candidates. During training, we employ knowledge distillation to teach the model how to distinguish useful time-series data (positive candidates) for a given query sequence, enabling more accurate and relevant retrieval.

\subsection{Dataset Construction}
To address the limitations of existing datasets and better align with real-world economic analysis, we construct new datasets by integrating financial indicators alongside historical stock prices, enabling deeper insights into market behavior that stock prices alone cannot capture. This section details stock selection, raw data collection (Collection of Basic Price Indicators) and financial indicator selection process(Calculation and Selection of Financial Indicators). Specifically, our datasets are built using high-trade-volume U.S. stocks, while the indicators are selected through domain expert consultation and mutual information analysis.

\noindent  
\textbf{Stock Selection.} To construct our datasets, we select high-trade-volume U.S. stocks across three periods: 2014-2015, 2017-2018, and 2022-2023. The first two periods align with two benchmark datasets: for 2014-2015, we use the same stocks as the ACL18 dataset~\citep{xu2018stock}, and for 2017-2018, we use the same stocks as the BIGDATA22 dataset~\citep{soun2022accurate}. To incorporate more recent data, we manually select high-trade-volume stocks from 2022-2023 to create the STOCK23 dataset. This ensures our training and evaluation reflect recent market conditions and provide a robust benchmark for stock movement prediction.

\noindent
\textbf{Collection of Basic Price Indicators.}  
Stock data is collected using the Yahoo Finance API~\citep{xu2014stock}, which provides basic price indicators such as the opening price, highest price, lowest price, adjusted closing price, and trading volume for each trading day. The daily price movement is then calculated as the reference answer for model training and evaluation.
% The definitions and effects of each basic price indicator are provided in Appendix \ref{append:basic_indicators}.

% \begin{itemize}
%     \item \textbf{ACL18} \citep{xu2018stock} consists of 71 stocks along with their tweets and historical price data from 2014.06.02 to 2015.12.31;
%     \item \textbf{BIGDATA22} \citep{soun2022accurate} consists of 47 stocks along with their tweets and historical price data from 2019.04.01 to 2020.12.31;
%     \item \textbf{CIKM18} \citep{wu2018hybrid} consists of 41 stocks along with their tweets and historical price data from 2017.01.03 to 2018.01.23; 
%     \item \textbf{STOCK23} consists of 51 stocks along with their historical price data from 2022.01.03 to 2023.12.31. 
% \end{itemize}

We then divide the stock data into training, validation, and test sets. To ensure temporal integrity and prevent information leakage, we partition the dataset by stocks rather than by time. This approach maintains consistent time ranges across splits while ensuring that stocks in the validation and test sets are entirely unseen during training. Unlike time-based splits, stock-based partitioning provides a more robust evaluation of the model's ability to generalize to new stocks, better reflecting real-world scenarios where models are applied to unseen entities. Table \ref{tab:dataset} summarizes the stock counts for each split in our datasets.

\begin{table}[ht]
\small
\centering
\renewcommand{\arraystretch}{1.3}
\caption{Dataset statistics.}
\label{tab:dataset}
\scalebox{0.75}{
\begin{tabular}{ccccccc}
\hline
\multirow{2}{*}{Dataset} 
& \multicolumn{3}{c}{Stock amount} 
& & \multicolumn{2}{c}{Trading dates}   
\\ 
\cline{2-4} 
\cline{6-7} 
& train 
& valid
& test
& 
& all sequences   
& query sequences
\\ \hline
ACL18~\citep{xu2018stock}   
& 33 & 5& 33     
& 
& 2014.06.02-2015.12.31
& 2015.06.03-2015.12.31 
\\
BIGDATA22~\citep{soun2022accurate}
& 22 & 3 & 22    
& 
& 2019.04.01-2020.12.31
& 2020.04.09-2020.12.31
\\
% CIKM18~\citep{wu2018hybrid}  
% & 19  & 3& 19   
% & 
% & 2017.01.03-2018.01.23 
% & 2018.01.03-2018.01.23 
% \\
STOCK23
& 24 & 3 & 24
&
& 2022.01.03-2023.12.31
& 2023.01.03-2023.12.31
\\ \hline
\end{tabular}
}
\end{table}

\noindent
\textbf{Calculation and Selection of Financial Indicators.}
In our datasets, we integrate financial indicators to provide critical insights into market behavior that stock prices alone cannot capture. Following analyzing methods in the book \textit{Machine learning for trading}~\citep{jansen2020machine}, we conduct an analysis using query sequences in all training sets to explore the relationship between commonly used financial indicators and returns.
To measure the relationship between these indicators and returns, we utilize Mutual Information (MI), a non-linear measure of dependency that quantifies the amount of information one variable contains about another. Specifically, the MI between an indicator \( X \) and returns \( R_t \) is calculated as:
\begin{equation}
    \scalebox{0.95}{$
    I(X; R_t) = \int_{x} \int_{r} p(x, r) \log \left( \frac{p(x, r)}{p(x)p(r)} \right) \, dx \, dr,$}
\end{equation}
where \( p(x, r) \) represents joint probability density function of \( X \) (indicator) and \( R_t \) (returns),
\( p(x) \) shows marginal probability density function of \( X \), and \( p(r) \) is marginal probability density function of \( R_t\).

We compute MI scores for each indicator and select the top-20 indicator groups with the highest scores as our candidates, with the detailed score ranking provided in Appendix \ref{append:mi_score}. Additionally, an example of a data piece from our datastore is illustrated in Appendix \ref{sec:datastore_example}.
% the definitions and effects of each indicator are provided in Appendix \ref{append:derived_indicators}.

\subsection{Sequence Serialization}
\label{sec:serialization}
Since stock movement prediction depends on the changes in related features rather than their exact values, we serialize stock prices and financial indicators into a time-series format. We use JSON to represent these sequences, as it has been demonstrated to effectively support LLMs in interpreting time-series data~\citep{fang2024large,singha2023tabular,yin2023finpt}.

\noindent
\textbf{Query Sequence Serialization}
We select query sequences where the query date is at least one year after the start date of the corresponding split in the dataset. This ensures that each query has enough candidate sequences for retrieval. For example, in the ACL18 dataset, which spans from 2014-06-02 to 2015-12-31, the query sequences are defined as those from 2015-06-03 to 2015-12-31.

Each query sequence contains the adjusted close price from the previous five trading days along with basic stock information. The indicator is represented as a five-day sequence, with a one-day sliding window applied across trading days. For example, when inquerying about stock MO on 2015-06-02, the query sequences contains stock name, query date, last five trading dates and their corresponding adjusted close prices. The serialized sequence is shown below:
\begin{lstlisting}[language=Java]
{ "query_stock": "MO",
  "query_date": "2015-06-02",
  "recent_date_list": ["2015-05-26", "2015-05-27", "2015-05-28", "2015-05-29", "2015-06-01"],
  "adjusted_close_list": [29.669, 29.9872, 29.8657, 29.6227, 29.6227]}
\end{lstlisting}

\noindent
\textbf{Candidate Sequence Serialization}
As new information becomes available, the candidate pool dynamically incorporates sequences from the most recent trading days. For instance, when the query date is 2015-06-03, the candidate pool includes sequences from 2014-06-02 to 2015-06-02. When the query date advances to 2015-06-04, the sequence for 2015-06-03 is added to the candidate pool without the need for additional training. This demonstrates the scalability of our method.

For each candidate stock on a specific date, we select 6 basic price indicators and 20 groups of financial indicators. Each candidate sequence includes a typical financial indicator along with basic stock information. As the same as query serialization, the indicator is represented as a five-day sequence, with a one-day sliding window applied across trading days. For example, a candidate represent stock MO on date 2014-07-02, with highest price as its indicator. The sequence includes basic information containing stock name, date, movement, last five trading dates and their corresponding highest prices. The serialized sequence is shown below:
\begin{lstlisting}[language=Java]
{ "candidate_stock": "MO",
  "candidate_date": "2014-07-02",
  "candidate_movement": "freeze",
  "recent_date_list": ["2014-06-25", "2014-06-26", "2014-06-27", "2014-06-30", "2014-07-01"],
  "high_list": [42.2, 42.0, 41.86, 42.28, 42.0]}
\end{lstlisting}

% \noindent\textbf{Basic price indicators}
% For each candidate stock on a specific date, the basic price indicators include the open price, highest price, lowest price, close price, adjusted close price, and volume. These are considered basic price indicators because they are directly provided by the stock market and do not require additional calculations. In contrast to derived indicators, such as moving averages or relative strength index (RSI), which are computed based on historical price data, these basic indicators represent raw, unprocessed market data. 

% \subsubsection{Prompt Selection}
% \label{sec:prompt_design}
% To design an effective prompt with the most useful features, we optimize three components: the task definition prompt, query sequence representation (selecting valuable features from the query sequence), candidate sequence representation (selecting valuable features from candidate sequences), and the order of these parts. To achieve this, we extract a toy dataset containing 5 rise sequences and 5 fall sequences, using all sequences from the same stock in the previous year as candidates. We then experiment with various prompts that include the task definition, each query sequence, and its corresponding candidate sequences, the detailed experiment is shown in Section \ref{sec:ablation}. Using the probability \(P(c)\) of generating the correct movement, we compute scores for each combination of task definition, query representation, and candidate representation. For each query sequence, we calculate the mean score of the top-3 \(P(c)\) to assess prompt effectiveness. An example of a prompt trial is shown in Figure \ref{fig:prompt}.

% \noindent
% \textbf{Task definition prompt}
% Utilizing the open-source LLaMA3-8b-instruct model, we first construct a fill-in-the-blank prompt designed to output only one token: 'rise' or 'fall'. This setup simplifies the calculation of the likelihood that the LLM generates the correct answer to the probability of the LLM producing the correct response at the first output index. Our selected task definition prompt is shown in Figure \ref{fig:prompt}.

% \noindent
% \textbf{Query sequence representation}
% We use the name of the stock and all recent price data to represent the query stock. We discuss how to list feature names to help the LLM better understand the referenced list. An example is highlighted in gray in Figure \ref{fig:prompt}. In these trials, we name the list of open prices in the recent five days as 'open price', 'open list', or 'recent open list'. Once we definite feature names, we use the same name for candidate features.

% \noindent
% \textbf{Candidate sequence representation}
% We discuss how different candidate features contribute to the prediction. We first provide all features and then provide the candidate sequence without each feature. After trials, we find the movement and recent movements of candidate sequences are noise for prediction. Therefore, we remove these two features, as is shown in Figure \ref{fig:prompt}.

\subsection{Retriever Training}
\label{sec:training}
To identify valuable information from vast amounts of sequences and uncover semantic relationships between them, we introduce a novel retrieval mechanism featuring three key innovations: a fine-tuned 1B parameter LLM (StockLLM) as the backbone, a candidate selection method leveraging LLM feedback to identify beneficial sequences and a training objective that maximizes similarity between queries and historically significant sequences, enabling the trained retriever to uncover meaningful patterns while minimizing noise in complex financial data. While inspired by~\citep{zhang2023retrieve}, our approach is uniquely tailored to financial time-series data, addressing the challenges of relevance assessment and alignment in a domain where traditional retrieval methods often fall short. This section details the training process of our framework, including the training of the LLM backbone, candidate scoring and selection, and the training of the retriever, which contains the training objective and knowledge distillation.
% To train our retriever, FinSeer, we introduce a novel two-stage framework that aligns the retriever with the LLM's forecasting preferences. First, we score all candidate sequences using LLM feedback to identify positive and negative examples, leveraging the LLM's ability to assess the relevance of historical sequences to the forecasting task. Second, we conduct knowledge distillation to transfer the LLM's nuanced understanding of time-series patterns to the retriever, ensuring that it prioritizes sequences that are most informative for stock movement prediction. 

\noindent
\textbf{The LLM Backbone.} 
To activate the LLM's inherent knowledge and ensure instruction-following capabilities, we fine-tune a 1B parameter LLM (LLaMA 3.2-1B-Instruct)~\citep{grattafiori2024llama3herdmodels} using the LoRA technique for efficient low-rank adaptation, resulting in StockLLM. By intentionally using a smaller backbone model, we establish a more challenging experimental setup, ensuring that performance improvements are attributable to FinSeer's retrieval capabilities rather than the LLM's capacity.  
The fine-tuning process is implemented using the LlamaFactory framework~\citep{zheng2024llamafactory}, with the following configuration: a learning rate of 5e-5, a cosine scheduler, gradient accumulation over 8 steps, mixed-precision (fp16) training, and 5 epochs of training with regular evaluation to log metrics and select the best-performing checkpoint. This setup demonstrates the robustness of our framework in extracting meaningful insights even under constrained model size, highlighting the effectiveness of FinSeer in enhancing financial forecasting tasks.

\noindent
\textbf{Candidate Scoring.}  
To determine whether a candidate sequence assists in predicting the movement of the query, we use LLM feedback to score each candidate. Specifically, for a given query \( q \), we integrate the query sequence and each candidate sequence \( c_i \) from the candidate pool as concurrent inputs to the LLM. The LLM outputs logits, which are unnormalized scores representing the model's confidence for each possible class (e.g., "rise" or "fall"). These logits are transformed into probabilities \( P(c) \) using the softmax function:
\begin{equation}
P(c) = \frac{e^{z_c}}{\sum_{j} e^{z_j}},
\end{equation}
where \( z_c \) is the logit for the correct class (e.g., "rise" if the true movement is upward) and \( z_j \) represents the logits for all possible classes. The resulting probability \( P(c) \) serves as the score for the candidate \( c_i \) with respect to the query \( q \).

\noindent
\textbf{Candidate Selection.} We rank the candidate sequences in descending order based on their scores \( P(c) \). The top-1 sequence is selected as a positive candidate, while the bottom 15 sequences are chosen as negative candidates. The sets of selected positive and negative sequences are denoted as \( \mathbb{C_P} \) and \( \mathbb{C_N} \), respectively.

\noindent
\textbf{Training Objective.}
Our retriever \( R(q) \) is designed to intelligently distinguish between historically significant sequences \( \mathbb{C_P} \) and noisy sequences \( \mathbb{C_N} \). The training objective is to ensure that \( R(q) \) prioritizes sequences from \( \mathbb{C_P} \) while minimizing attention to those from \( \mathbb{C_N} \). This is achieved by maximizing a similarity measure \( sup(q, s) \) between the query sequence \( q \) and candidate sequences \( s \). Mathematically, the retriever's objective is formulated as:
\begin{equation}
R(q) = {\arg \max}_{s \in \mathbb{C_P} \cup \mathbb{C_N}} sup(q, s).
\end{equation}
By focusing on sequences that maximize \( sup(q, s) \), the retriever ensures that the most informative and contextually relevant historical sequences are identified.

\noindent
\textbf{Knowledge Distillation.}  
To leverage the scoring derived from the LLM, we employ knowledge distillation, which transfers knowledge from the teacher model (LLM) to the student model (retriever) by mimicking the teacher's output distribution. This approach effectively captures nuanced patterns and predictions from the LLM.  
Specifically, we minimize the Kullback-Leibler (KL) divergence between the candidate distributions computed using the LLM's rewards and those predicted by the embedding model. For each query \( q \) and its candidate list \( \{ \mathbb{C_P}, \mathbb{C_N} \} \), we derive the LLM's rewards for the candidates, denoted as \( \{P(c_i), i=1, ..., n\} \). To make these rewards suitable for distillation, we normalize them using a softmax function with temperature \( \alpha \):  
\begin{equation}
w_i = \text{softmax}_R \left( \frac{P(c_i)}{\alpha} \right). 
\end{equation}
The KL divergence is then computed as follows:  
\begin{equation}
\min \sum_{c} -w_i \times \log \left( \frac{\exp \left( \left\langle \boldsymbol{e}_q, \boldsymbol{e}_{c_i} \right\rangle / \tau \right)}{\sum_{{c}^{\prime} \in \mathbb{C}} \exp \left( \left\langle \boldsymbol{e}_q, \boldsymbol{e}_{{c}^{\prime}} \right\rangle / \tau \right)} \right),
\end{equation}
where \( \boldsymbol{e}_q \) and \( \boldsymbol{e}_{c_i} \) are the embeddings of the query \( q \) and candidate \( c_i \), respectively, and \( \tau \) is a temperature parameter. This loss function optimizes the similarity between the query embedding and the embeddings of the top-ranked candidates, enhancing the retriever's ability to accurately predict stock price movements.

\subsection{Inference}  
The inference process of our RAG framework, built on FinSeer, consists of three key steps to enhance stock movement prediction. First, FinSeer retrieves relevant sequences from the candidate pool, leveraging its ability to identify historically significant patterns. Second, these retrieved sequences are incorporated into StockLLM’s input context, enriching the model’s understanding of temporal dynamics. Finally, the enriched context is combined with the original query to generate forecasts. By integrating salient historical patterns with current data, this approach bridges the gap between past trends and present conditions, significantly improving prediction accuracy and robustness in real-world financial scenarios.

\section{Experiment}
\subsection{Experimental Settings}
\noindent
\textbf{Datasets.}  
We evaluate the effectiveness of our RAG framework on the test sets of the three datasets described in Table~\ref{tab:dataset}, with ACL18 containing 2,876 query sequences, BIGDATA22 containing 2,868 queries, and STOCK23 containing 4,128 queries. These thousand-scale queries help mitigate random bias, ensuring a robust and reliable evaluation of model performance.
To ensure a comprehensive evaluation, for each query sequence, we include all sequences containing financial indicators across all stocks in the test set (not limited to the same stock), with data available up to the query date, as potential candidates. No additional restrictions are imposed, enabling a robust assessment of the models' performance in real-world financial forecasting scenarios.

\noindent
\textbf{Baselines.}  
To evaluate the efficiency of our RAG framework, we choose bare StockLLM and random retrieval as our baselines. To evaluate our retriever FinSeer, we select five competitive RAG models from the top of the MTEB English Leaderboard as baselines: (1) Instructor-large~\citep{SuSKWHOYSZ023}: A 335M instruction-finetuned text embedder that encodes sequences into 768-dimensional tensors. (2) UAE-large-v1~\citep{li2023angle}: A 335M ANGLE-optimized text embedding model that encodes sequences into 1024-dimensional tensors. (3) BGE-large-en-v1.5~\citep{bge_embedding}: A 335M general embedder pre-trained with RetroMAE~\citep{RetroMAE}, encoding sequences into 1024-dimensional tensors. (4) LLM-Embedder~\citep{zhang2023retrieve}: A 109M embedder fine-tuned from BGE-large-en-v1.5, designed as a unified embedding model to support diverse retrieval augmentation needs for LLMs. It encodes sequences into 768-dimensional tensors. (5) E5-mistral-7b-instruct~\citep{wang2023improving}: A 7111M embedder initialized from Mistral-7B-v0.1~\citep{jiang2023mistral} and fine-tuned on multilingual datasets, encoding sequences into 4096-di-mensional tensors.  

\noindent
\textbf{Evaluation Metrics.}
We employ Accuracy (ACC) and Matthews
Correlation Coefficient (MCC) \citep{matthews1975comparison} to assess the performance of FinSeer and the baseline models on the stock movement prediction task. These metrics evaluate the performance of stock movement prediction based on the distribution of positive and negative samples. 
% ACC and MCC are defined as:
% \begin{equation}
%     \scalebox{0.8}{$
%     ACC = \frac{TP +TN}{TP + TN +FP +FN}$}
% \end{equation}
% \begin{equation}
%     \scalebox{0.8}{$
%     MCC = \frac{(TP \times TN) - (FP \times FN)}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$}
% \end{equation}
% where TP denotes true positives, TN denotes true negatives, FP denotes false positives, and FN denotes false negatives.

\subsection{Main Results}
% 有rag和没有rag的方法相比，实验结果未必提升，因为抽错的会误导llm判断；在同样的llm下，我们的retriever更好
Table \ref{tab:main-result} summarizes the performance of stock movement prediction across the three datasets.
\begin{table}[ht]
\small
\centering
\renewcommand{\arraystretch}{1.3}
\caption{Results of stock movement predictions using LLMs and retrieval models. The asterisk (*) indicates the LLM employed while using retrieval models.}
\label{tab:main-result}
\scalebox{0.85}{
\begin{tabular}{ccccccccc}
\hline
{Retrieving Methods}
& \multicolumn{2}{c}{ACL18} 
&  
& \multicolumn{2}{c}{BIGDATA22} 
&  
% & \multicolumn{2}{c}{CIKM18} 
% & 
& \multicolumn{2}{c}{STOCK23} 
\\ 
\cline{2-3} 
\cline{5-6} 
\cline{8-9} 
% \cline{11-12}   
(+ StockLLM)
& ACC & MCC &  
& ACC & MCC & 
& ACC & MCC 
\\ \hline
w/o Retrieval   
& 0.496 & -0.007 &
& 0.497 & -0.008 &
% & 0.512 &  0.028 &
& 0.509 & 0.021     \\
Random Retrieve 
& 0.481 & -0.036 &
& 0.521 &  0.045 &
% & 0.475 & -0.050 &
& 0.496 & -0.004 \\
DTW
& 0.518 & \textbf{0.045} &
& 0.505 & 0.021 &
& 0.492 & -0.007 \\
Instructor     
& 0.496 & -0.008 &
& 0.497 & -0.006 &
% & 0.500 &  0.000 &
& 0.505 & 0.010 \\
UAE             
& 0.482 & -0.036 &
& 0.495 & -0.008 &
% & 0.525 &  0.050 &
& 0.494 & -0.009 \\
E5              
& 0.510 &  0.028 &
& 0.503 &  0.008 &
% & \textbf{0.537} & \textbf{0.077} &
& 0.499 & -0.001 \\
BGE             
& 0.489 & -0.019 &
& 0.487 & -0.026 &
% & 0.525 &  0.050 &
& 0.488 & -0.014 \\
LLM Embedder    
& 0.499 &  0.000 &
& 0.459 & -0.083 &
% & 0.500 &  0.000 &
& 0.503 & 0.007  
\\ \hline
FinSeer        
& \textbf{0.521} & { 0.042} &
& \textbf{0.538} & \textbf{0.079} &
% & \textbf{0.537} & 0.076 &
& \textbf{0.542} & \textbf{0.085}
\\ \hline
\end{tabular}
}
\end{table}

We analyze our framework and retriever separately to evaluate their contributions. First, our RAG framework outperforms both bare StockLLM and random retrieval across all datasets, demonstrating its effectiveness in enhancing stock movement prediction. Second, among all evaluated retrieval methods, FinSeer consistently achieves the best performance when integrated with StockLLM, showcasing its ability to retrieve valuable sequences that improve forecasting accuracy. Notably, FinSeer is the only retriever that achieves stable improvements over bare StockLLM on all datasets, while other methods exhibit inconsistent results, sometimes enhancing predictions and other times degrading them. Furthermore, our experiments reveal that incorporating a retrieval process does not always guarantee performance improvements, as retrieval errors can mislead the LLM and harm predictions. This underscores the critical need for retrieval models that align closely with the LLM's objectives, a challenge that FinSeer effectively addresses through its tailored design.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/indicator_occurence_acl18.pdf}
\caption{Indicator occurrences of different RAG models on ACL18 dataset.}
\label{fig:acl18_indicators}
\end{figure*}

The minimal performance difference between the instruction-finetuned retriever (Instructor) and the no-retrieval baselines highlights the inherent challenge of time-series retrieval. Unlike text retrieval, time-series retrieval does not solely rely on task understanding, as candidate sequences often appear similar, making it difficult to distinguish valuable ones. The angle-optimized retriever (UAE) shows inconsistent performance across datasets, as its ability to differentiate similarity between query and candidate sequences does not fully align with the LLM's perception of importance. Similarly, LLM Embedder, our backbone model trained with LLM feedback, struggles to generalize to time-series retrieval tasks, further underscoring the complexity of the problem.

The best-performing baseline is E5, which achieves competitive results despite its significantly larger model size and embedding dimensions (five times larger than FinSeer). However, FinSeer outperforms E5 on all datasets. This demonstrates that FinSeer successfully learns the preferences of LLMs and effectively enhances their time-series forecasting ability through retrieval, even under challenging conditions.

In summary, while RAG methods do not guarantee performance improvements due to the risk of retrieval errors, FinSeer stands out by consistently retrieving high-quality sequences that align with the LLM's forecasting objectives, leading to superior results compared to other retrievers.

\subsection{Ablation Study}
\label{sec:ablation}
\textbf{Indicator Occurrences}
In this part, we analyze what indicators all RAG models retrieve. We calculate indicator occurrences on ACL18 dataset and the results are shown in Figure \ref{fig:acl18_indicators}.

As shown in the figure, FinSeer is the only model that successfully extracts a diverse and comprehensive set of indicators while achieving superior performance. This clearly demonstrates its advanced temporal retrieval capabilities. Specifically, while other models like LLM Embedder and Instructor predominantly focus on basic indicators such as close price and adjusted close price, FinSeer effectively identifies and retrieves a wide range of technical indicators, including kdj crossover, MACD Histogram, Bollinger Bands, and various alpha factors. This richer set of retrieved indicators provides FinSeer with more comprehensive auxiliary information, enabling more accurate and reliable predictions.

\noindent
\textbf{Candidate Movement Correlation}
In this section, we investigate whether the LLM relies on the movement trends of retrieved sequences to make predictions. To this end, we compute the correlation between the movements of retrieved sequences and the LLM's generated results. The detailed calculation process is provided in Appendix \ref{append:candidate_movement_correlation}.

The results in Table \ref{tab:correlation_movement_count} reveal no significant correlation between the movement direction (rise or fall) of the retrieved candidate sequences and the final predictions of StockLLM. This finding indicates that StockLLM does not simply mirror the movement trends of the retrieved sequences but instead analyzes their specific content to infer the query's movement direction. This ability highlights the LLM's capacity to extract meaningful insights from complex time-series data.
\begin{table}[ht]
    \centering
    \caption{Correlation between the movements of retrieved sequences and LLM's generated results.}
    \label{tab:correlation_movement_count}
    \scalebox{0.7}{
    \begin{tabular}{cccc}
    \toprule
    \text{Dataset} & \text{RAG Model} & \text{Correlation} & \text{P-Value} \\ \hline
         \multirow{6}{*}{ACL18} & Instructor & -0.098 & 0.000 \\
                & UAE        & -0.068 & 0.000 \\
                & E5         & -0.108 & 0.000 \\
                & BGE        &  0.175 & 0.000 \\
                & LLM Embedder & 0.014 & 0.457 \\
                & FinSeer    &  0.054 & 0.004 \\
\midrule
\multirow{6}{*}{BigData22} & Instructor & -0.072 & 0.000 \\
                   & UAE        &  0.009 & 0.633 \\
                   & E5         & -0.040 & 0.032 \\
                   & BGE        &  0.093 & 0.000 \\
                   & LLM Embedder & 0.060 & 0.001 \\
                   & FinSeer    &  0.077 & 0.000 \\
\midrule
% \multirow{6}{*}{CIKM18} & Instructor &  0.019 & 0.867 \\
%                 & UAE        & -0.037 & 0.748 \\
%                 & E5         & -0.161 & 0.154 \\
%                 & BGE        & -0.034 & 0.766 \\
%                 & LLM Embedder & 0.130 & 0.249 \\
%                 & FinSeer    &  0.170 & 0.132 \\
% \midrule
\multirow{6}{*}{STOCK23} & Instructor & -0.114 & 0.000 \\
                 & UAE        & -0.093 & 0.000 \\
                 & E5         & -0.064 & 0.000 \\
                 & BGE        &  0.131 & 0.000 \\
                 & LLM Embedder & 0.015 & 0.350 \\
                 & FinSeer    &  0.032 & 0.041 \\
                 \bottomrule
    \end{tabular}}
\end{table}

This observation underscores the critical role of retrieval quality in RAG models, as the relevance and informativeness of retrieved sequences directly influence prediction performance. Notably, StockLLM is a relatively small 1B parameter LLM, making this a particularly challenging setting. Despite this, our time-series-tailored RAG model, FinSeer, successfully enhances StockLLM's performance by retrieving rich and relevant time-sensitive information. This demonstrates the effectiveness of our approach in improving prediction accuracy and showcases its potential for financial time-series forecasting.

\subsection{Case Study}
This case study illustrates the critical importance of alignment between the retriever and the LLM's forecasting preferences in financial time-series analysis. We examine the stock XOM on 2015-06-25 from the ACL18 dataset, where the adjusted close price exhibited a pronounced downward trend. The query sequence is as follows:
\begin{lstlisting}[language=Java]
{ "query_stock": "XOM",
  "query_date": "2015-06-25",
  "recent_date_list": ["2015-06-18", "2015-06-19", "2015-06-22", "2015-06-23", "2015-06-24"],
  "adjusted_close_list": [58.0813, 57.8979, 57.8707, 57.8027, 57.5377]}
\end{lstlisting}

While multiple retrievers were evaluated, only FinSeer successfully enabled StockLLM to predict the correct movement as a fall. Specifically, FinSeer retrieved five diverse indicators—close price, adjusted close price, alpha021, alpha054, and the highest price—providing a comprehensive view of the stock's behavior. Alpha021 identifies trends based on short- and long-term price averages and volume conditions, while alpha054 combines price and volume rankings to assess performance within a specific time window. These indicators allowed StockLLM to accurately assess whether the downward trend would persist or reverse, demonstrating the value of retrieving contextually relevant and diverse features.

In contrast, other retrievers, such as Instructor, BGE, LLM Embedder, and E5, extracted sequences dominated by close or adjusted close prices, all reflecting similar downward trends. While these sequences aligned with the current trend, they failed to provide actionable insights for forecasting future movements, leading StockLLM to misinterpret them as noise and incorrectly predict a rise. Similarly, UAE retrieved sequences indicating overbought and oversold conditions, including three rise and two freeze trends. Although overbought signals often suggest a potential downturn, the retrieved sequences themselves exhibited rising or frozen trends, confusing StockLLM and resulting in an erroneous prediction. This case study underscores the superiority of FinSeer in retrieving meaningful and diverse indicators that align with the LLM's forecasting logic, enabling more accurate and reliable predictions.

\subsection{Indicator Sequence Visualization}
To intuitively examine how FinSeer embeds indicator sequences, we use LargeVis~\citep{tang2016visualizing} to reduce the embeddings to two dimensions. In Figure \ref{fig:visualization}, we visualize\footnote{https://medviz.org/app/} the vector space of nine indicators, including adjusted close price, open price, high price, low price, close price, volume, movement, returns, and WAP. The results show that the green points, representing movement information, are well-clustered, indicating that FinSeer effectively captures meaningful patterns. However, this also suggests that deeper relationships between stock movements and indicators remain to be explored, highlighting the potential for further research in this area.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/acl18_group1_largevis.pdf}
    \caption{Visualization of indicator sequence embeddings on ACL18 dataset.}
    \Description{}
    \label{fig:visualization}
\end{figure}

\section{Related Work}
% \subsection{Retrieval-augmented LLMs}
% To enhance reasoning and prediction capabilities of LLM by retrieving relevant information from vast datasets, numerous retrieval methods have been proposed~\citep{fan2024survey}. Early approaches primarily relied on keyword frequency, with many studies directly applying BM25 for passage-level retrieval in RAG~\citep{chen2017reading,jiang2023active,ram2023context,xu2024recomp,zhong2022training,zhoudocprompting}. These passages were represented as bags of words and ranked using term frequency-inverse document frequency (TF-IDF)~\citep{izacard2021leveraging}. Subsequently, methods based on semantic similarity emerged, which encode queries and passages into a shared vector space to train embeddings that align queries closely with relevant factual passages~\citep{li2023mot,ludynamic,milios2023context,poesiasynchromesh,rubin2022learning,ye2023compositional}. However, these approaches are not well-suited for time-series retrieval tasks, such as predicting stock price movements, where no fixed factual passages exist for retrieval. Furthermore, the highly similar nature of time-series data poses challenges for semantic similarity-based methods, as they struggle to effectively distinguish between similar patterns. Therefore, a specialized retrieval method for time-series data is required, which our model provides.

\subsection{Stock Movement Prediction}
\textbf{Non-LLM Methods.}
Traditional approaches to stock movement prediction have focused on various aspects of financial data. One prominent category of methods analyzes stock price sequences and their corresponding technical indicators~\citep{qin2017dual,feng2018enhancing} to identify patterns in historical data for predicting future movements. However, due to the complexity of factors influencing stock prices, subsequent methods have incorporated additional contextual information, such as news articles~\citep{ding2015deep,liu2018hierarchical}  or social media posts~\citep{xu2018stock,wu2018hybrid}. Despite efforts, these methods are highly susceptible to noise and struggle to analyze the vast and diverse nature of financial information effectively.

\noindent
\textbf{LLM-based Methods.}
Recent studies have explored using LLMs for financial prediction tasks, either by fine-tuning open-source models or prompting advanced models like GPT-4. However, even state-of-the-art models, including GPT-4, have achieved only random-guessing-level accuracy in stock movement prediction~\citep{xie2023pixiu,xie2024finben,xie2023wall}. This highlights the inherent challenges in identifying and analyzing meaningful patterns in a domain as volatile and multifaceted as stock market prediction.

\subsection{Time-series forecasting with LLMs}
% LLMs have been increasingly applied to time-series forecasting, particularly in tasks requiring the alignment of temporal and textual data. Several approaches focus on improving this alignment through different strategies. 
\textbf{Non-retrieval Methods.}
To enhance the performance of LLMs in time-series forecasting, existing methods have primarily focused on aligning temporal and textual data, either by transforming time-series into textual formats or by encoding both modalities into a unified vector space.
For instance, a study~\citep{jin2023time} reprograms time-series data into textual representations suitable for LLMs, enhancing prediction accuracy via declarative prompts. Similarly, some other studies~\citep{yu2023temporal,liu2024timecma} explore cross-modal alignment: the former applies LLMs to financial forecasting by integrating stock prices with news data, while the latter introduces a cross-modality framework to align time-series with text for improved predictive performance. % Building on this, a method~\citep{pan2024s} maps time-series and text into a shared semantic space, further boosting LLM performance through strengthened data alignment. 
% Despite these advancements, efficient methods for retrieving large-scale time-series data remain underexplored, presenting a key challenge for future research.
While these advancements improve alignment, existing methods often struggle to process large-scale time-series data comprehensively due to LLM input limitations. As a result, effectively leveraging extensive historical data remains a challenge. This highlights the need for retrieval-augmented methods, a gap our approach directly addresses.

\noindent
\textbf{RAG Methods.}
With the rapid advancement of RAG techniques in various applications of LLMs, recent research has begun exploring RAG for time-series forecasting. For instance, TimeRAG~\citep{yang2024timerag} integrates RAG into time-series forecasting by combining Dynamic Time Warping (DTW) with LLMs to improve prediction accuracy. However, relying solely on numeric similarity is insufficient for financial time-series forecasting, as it fails to capture deeper semantic relationships. This underscores the need for a more targeted retrieval framework tailored to the complexities of financial data, a challenge our framework effectively addresses.

\section{Conclusion}
In this work, we introduce the first RAG framework for financial time-series forecasting, featuring a novel retrieval mechanism with three key innovations: a fine-tuned 1B parameter LLM (StockLLM), a candidate selection method leveraging LLM feedback, and a training objective that maximizes similarity between queries and historically significant sequences. We also construct new datasets integrating financial indicators and historical stock prices, selected through expert consultation and mutual information analysis, to train the retriever and ensure robust evaluation. Our retriever, FinSeer, leverages these datasets and LLM feedback to uncover deeper patterns beyond numeric similarities. Experiments demonstrate that our RAG framework outperforms bare StockLLM and random retrieval, while FinSeer surpasses existing retrieval methods, achieving an 8\% higher accuracy on BIGDATA22 and retrieving more impactful sequences. This work highlights the importance of tailored retrieval models in financial forecasting and provides a robust framework for future research.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{Data examples}
\subsection{Examples in the Datastore}
\label{sec:datastore_example}
An example in our datastore is shown below.
\begin{lstlisting}[language=Java]
{
    "stock_name":"ABBV",
    "query_date":"2014-06-05",
    "query_movement":"rise",
    "open":54.549999,
    "high":55.32,
    "low":54.360001,
    "close":55.299999,
    "adj_close":36.980961,
    "volume":4847300,
    "MACD_Histogram":0.0621141602,
    "macd_crossover":null,
    "bollinger_bands":null,
    "exceeding_upper":null,
    "exceeding_lower":null,
    "overbought_and_oversold_conditions":null,
    "kdj_crossover":null,
    "Returns":0.0131913094,
    "VWAP":54.9933333333,
    "alpha_smr":0.000549858,
    "alpha_mom":0.0121661224,
    "alpha002":null,
    "alpha006":null,
    "alpha009":-0.481476,
    "alpha012":-0.481476,
    "alpha021":1,
    "alpha023":0.0,
    "alpha024":-0.769035,
    "alpha028":null,
    "alpha032":null,
    "alpha041":-0.1554335256,
    "alpha046":-0.481476,
    "alpha049":-0.481476,
    "alpha051":-0.481476,
    "alpha053":null,
    "alpha054":126.4264005641,
    "alpha101":-18.282056485
}
\end{lstlisting}


% \section{Indicator Definitions}
% \label{append:indicators}

% \subsection{Basic Price Indicators}
% \label{append:basic_indicators}
% This section provides an overview of the basic price indicators used in our study, including their definitions and roles in stock market analysis. These indicators, directly obtained from the stock market without additional calculations, serve as fundamental inputs for time-series forecasting and offer essential insights into market trends.

% \begin{itemize}
%     \item \textbf{Open price}: Reflects the stock’s value at the start of the trading session.
%     \item \textbf{Highest and lowest prices}: Capture the price range during the day, offering insights into volatility.
%     \item \textbf{Close price}: Often the most important for technical analysis, represents the final trading price for the day. 
%     \item \textbf{Adjusted close price}: Accounts for corporate actions like dividends or stock splits, providing a more accurate long-term price history. 
%     \item \textbf{Volume}: Indicates the level of trading activity, and movement, typically the change in price from the previous day, reflects short-term price fluctuations. 
% \end{itemize}

% \subsection{Devived Indicators}
% \label{append:derived_indicators}
% This section introduces the derived indicators used in our study, including their definitions and calculation formulas. Unlike basic price indicators, derived indicators are computed from raw stock data and are designed to capture more nuanced market behaviors, trends, and patterns. These indicators often integrate historical price information with statistical or mathematical techniques, making them essential tools for advanced stock analysis and time-series forecasting.  

% \begin{itemize}
% \item \textbf{Returns}: Represents the percentage change in the closing price over consecutive days. 
% \begin{equation*}
%     \scalebox{0.9}{$
%         \begin{aligned}
%             \text{Returns} 
%             &= \frac{\text{adj\_close$_t$} - \text{adj\_close$_{t-1}$}}{\text{adj\_close$_{t-1}$}} * 100
%         \end{aligned}
%     $}
% \end{equation*}
% \item \textbf{Movement}: Represents the change in stock price between consecutive trading days.
% \begin{equation*}
%     \scalebox{0.9}{$
%         \begin{aligned}
%             \text{Movement} &= 
%             \begin{cases} 
%                 \text{rise}, & \text{Returns} > 0.55 \\
%                 \text{fall}, & \text{Returns} < -0.5 \\
%                 \text{freeze}, &  -0.5 \leq \text{Returns} \leq 0.55
%             \end{cases}
%         \end{aligned}
%     $}
% \end{equation*}

% \item \textbf{MACD (Moving Average Convergence Divergence) Group}: This group contains MACD Histogram (values) and MACD crossovers (signals). 

% MACD Histogram measures the difference between a stock's MACD line and its signal line, helping identify trends and momentum. 
% \begin{equation*}
%     \scalebox{0.9}{
%         $\begin{aligned}
%             \text{MACD Line} &= \text{EMA}_{12} - \text{EMA}_{26}  \\
%             \text{Signal Line} &= \text{EMA}_{9} \text{ of the MACD Line}  \\
%             \text{MACD Histogram} &= \text{MACD Line} - \text{Signal Line}
%         \end{aligned}$
%     }
% \end{equation*}
% MACD Crossover indicates a buy or sell signal when the MACD line crosses above or below the signal line. 
% \begin{equation*}
%     \scalebox{0.9}{
%         $\begin{aligned}
%             \text{EMA Signal} &= 
%             \begin{cases} 
%                 1, \text{EMA}_{\text{Fast}} > \text{EMA}_{\text{Slow}} \\
%                -1, \text{EMA}_{\text{Fast}} \leq \text{EMA}_{\text{Slow}}
%             \end{cases} \\
%             \text{Signal Change (sc)} &= \text{EMA Signal}_{t} - \text{EMA Signal}_{t-1} \\
%             \text{MACD Crossover} &=
%             \begin{cases} 
%                 \text{bullish\_cross}, \text{sc} = 2 \\
%                 \text{bearish\_cross}, \text{sc} = -2 \\
%                 \text{none}, \text{otherwise}
%             \end{cases}
%         \end{aligned}$
%     }
% \end{equation*}

% \item \textbf{Bollinger Bands Group}: This group includes Bollinger Bands (signals), Exceeding Upper (values), and Exceeding Lower (values). In the formulas below, STD represents the standard deviation of prices.
% \begin{equation*}
%     \scalebox{0.8}{
%         $\begin{aligned}
%             \text{Middle Band} &= \frac{1}{N} \sum_{i=t-N+1}^t \text{adj\_close}_i \\
%             \text{STD} &= \sqrt{\frac{1}{N} \sum_{i=t-N+1}^t \left(\text{adj\_close}_i - \text{Middle Band}\right)^2} \\
%             \text{Upper Band} &= \text{Middle Band} + 2 \cdot \text{STD} \\
%             \text{Lower Band} &= \text{Middle Band} - 2 \cdot \text{STD}
%         \end{aligned}$
%     }
% \end{equation*}

% Bollinger Bands define upper and lower price limits around a moving average, serving as a measure of price volatility. 
% Owing to space limitations, in the following formulas, we abbreviate Bollinger Bands as \( \text{BB} \), Exceeding Upper as \( \text{EU} \), and Exceeding Lower as \( \text{EL} \).
% \begin{equation*}
%     \scalebox{0.8}{
%     $\begin{aligned}
%         \text{BB} &=
%         \begin{cases} 
%             \text{exceeding\_upper},  \text{adj\_close}_t > \text{Upper Band} \\
%             \text{exceeding\_lower},  \text{adj\_close}_t < \text{Lower Band} \\
%             \text{none}, \text{otherwise}
%         \end{cases}
%         \\
%         \text{EU} &=
%         \begin{cases} 
%             \text{adj\_close}_t - \text{Upper Band}, \text{exceeding\_upper} \\
%             \text{none}, \text{otherwise}
%         \end{cases}
%         \\
%         \text{EL} &=
%         \begin{cases} 
%             \text{adj\_close}_t - \text{Lower Band},  \text{exceeding\_lower} \\
%             \text{None}, \text{otherwise}
%         \end{cases}
%     \end{aligned}$
%     }
% \end{equation*}

% \item \textbf{KDJ Group}: Group includes Overbought and Oversold Conditions and KDJ Crossover Signals.

% The KDJ indicator is a momentum oscillator used in technical analysis to evaluate price trends and potential reversals. It is based on the calculation of the RSV (Raw Stochastic Value) and three key components: \( K \), \( D \), and \( J \). The \( K \) and \( D \) values are smoothed exponentially weighted moving averages (EWMA) of the RSV, while \( J \) is derived from \( K \) and \( D \).
% \begin{equation*}
%     \scalebox{0.9}{
%     $\begin{aligned}
%         \text{RSV} &= \frac{\text{Close Price} - \text{Low}_n}{\text{High}_n - \text{Low}_n} \times 100
%         \\
%         K &= \text{EWMA}(\text{RSV}, \alpha = \frac{1}{3})  
%         \\
%         D &= \text{EWMA}(K, \alpha = \frac{1}{3})  
%         \\
%         J &= 3K - 2D  
%     \end{aligned}$
%     }
% \end{equation*}
% Overbought and Oversold Conditions classify the market state into "overbought" or "oversold" areas based on predefined thresholds for \( K \), \( D \), and \( J \).  
% \begin{equation*}
%     \scalebox{0.8}{
%     $\begin{aligned}
%         &\text{Overbought and Oversold Conditions}
%         \\
%         &= 
%         \begin{cases} 
%             \text{overbought\_area}, & \text{if } K > 80, D > 70, J > 90 \\  
%             \text{oversold\_area}, & \text{if } K < 20, D < 30 \\  
%             \text{None}, & \text{otherwise}  
%         \end{cases}  
%     \end{aligned}$
%     }
% \end{equation*}

% KDJ Crossover Signals identify "bullish" or "bearish" signals based on the relationship between \( K \) and \( D \).
% \begin{equation*}
%     \scalebox{0.9}{
%     $\text{KDJ Crossover} =  
%     \begin{cases} 
%     \text{bullish\_signal}, & \text{if } K > D \\  
%     \text{bearish\_signal}, & \text{if } K < D \\  
%     \end{cases}$
%     }
% \end{equation*}

% \item \textbf{VWAP (Volume Weighted Average Price)}: Calculates the average price of a stock weighted by trading volume, providing a benchmark for institutional investors.  
% \begin{equation*}
%     \scalebox{0.9}{
%     $
%     \begin{aligned}
%         \text{Typical Price}_t &= \frac{\text{High}_t + \text{Low}_t + \text{Close}_t}{3} \\
%         \text{VWAP} &= \frac{\sum_{t=1}^{n} (\text{Typical Price}_t \times \text{Volume}_t)}{\sum_{t=1}^{n} \text{Volume}_t}
%     \end{aligned}$
%     }
% \end{equation*} 

% \item \textbf{Alpha Factors}: Quantitative metrics derived from price and volume data, designed to capture specific market patterns or anomalies. 

% Alpha002 measures the negative 6-day correlation between ranked changes in logarithmic volume and ranked price movements.
% \begin{equation*}
%     \scalebox{0.9}{
%     $\begin{aligned}
%         \Delta\text{rank}_{v} &= \text{rank}(\text{ts\_delta}(\log(\text{volume}), 2)) \\
%         \Delta\text{rank}_{p} &= \text{rank}(\frac{\text{close} - \text{open}}{\text{open}}) \\
%         \text{alpha002} &= -1 \cdot \text{ts\_corr}(\Delta\text{rank}_{v}, \Delta\text{rank}_{p}, 6)
%     \end{aligned}$
%     }
% \end{equation*}

% Alpha006 computes the negative correlation between opening price and volume over a 10-day window.
% \begin{equation*}
%     \scalebox{0.9}{
%     $\begin{aligned}
%         \text{alpha006} = -1 \cdot \text{ts\_corr}(\text{open}, \text{volume}, 10)
%     \end{aligned}$
%     }
% \end{equation*}

% Alpha009 uses recent price changes to classify trends based on their range within a 5-day window.
% \begin{equation*}
%     \scalebox{0.9}{
%     $\begin{aligned}
%     \Delta\text{price} &= \text{ts\_min}(\text{ts\_delta}(\text{close}, 1), 5) \\
%        \text{alpha009} &=
%     \begin{cases}
%         \text{ts\_delta}(\text{close}, 1), &  \Delta\text{price} > 0, \\
%         \text{ts\_delta}(\text{close}, 1), &   \Delta\text{price} < 0, \\
%         -\text{ts\_delta}(\text{close}, 1), & \text{otherwise.}
%     \end{cases}
%     \end{aligned}$
%     }
% \end{equation*}

% Alpha012 relates volume change direction to the negative change in price.
% \begin{equation*}
%     \scalebox{0.9}{
%     $\begin{aligned}
%     \Delta\text{sign}_{v} &= \text{sign}(\text{ts\_delta}(\text{volume}, 1)) \\
%         \text{alpha012} &= \Delta\text{sign}_{v} \cdot (-1 \cdot \text{ts\_delta}(\text{close}, 1))
%     \end{aligned}$
%     }
% \end{equation*}

% Alpha021 identifies trends based on short- and long-term price averages and volume conditions.

% \begin{equation*}
%     \scalebox{0.8}{
%     $\begin{aligned}
%     \text{sum}_{8} = 
%          \text{ts\_mean}(\text{close}, 8) + \text{ts\_std}(\text{close}, 8) \\
%     \text{minus}_{8} = \text{ts\_mean}(\text{close}, 8) - \text{ts\_std}(\text{close}, 8) \\
%     \text{alpha021} =
% \begin{cases}
%     -1, & \text{sum}_{8} < \text{ts\_mean}(\text{close}, 2), \\
%     1, & \text{ts\_mean}(\text{close}, 2) < \text{minus}_{8}, \\
%     -1, & \frac{\text{volume}}{\text{adv20}} < 1, \\
%     1, & \text{otherwise.}
% \end{cases}
%     \end{aligned}$
%     }
% \end{equation*}

% Alpha023 tracks changes in high prices if they exceed their 20-day average.
% \begin{equation*}
%     \scalebox{0.8}{
%     $\begin{aligned}
%         \text{condition}_{23} &= \text{ts\_mean}(\text{high}, 20) < \text{high} \\
%         \text{alpha023} &=
%         \begin{cases}
%             -1 \cdot \text{ts\_delta}(\text{high}, 2), & \text{condition}_{23}, \\
%             0, & \text{otherwise.}
%         \end{cases}
%     \end{aligned}$
%     }
% \end{equation*}

% Alpha024 compares long-term and short-term price movements to identify breakouts or retracements.
% \begin{equation*}
%     \scalebox{0.8}{
%     $\begin{aligned}
%     \Delta\text{close} &= (\text{close} - \text{ts\_min}(\text{close}, 100)) \\
%     \text{condition}_{24} &= \frac{\text{ts\_delta}(\text{ts\_mean}(\text{close}, 100), 100)}{\text{ts\_lag}(\text{close}, 100)} \leq 0.05 \\
%     \text{alpha024} &=
%         \begin{cases}
%             -1 \cdot \Delta\text{close}, & \text{condition}_{24} , \\
%             -1 \cdot \text{ts\_delta}(\text{close}, 3), & \text{otherwise.}
%         \end{cases}
%     \end{aligned}$
%     }
% \end{equation*}

% Alpha028 scales the relationship between average price, low price correlation, and close price.
% \begin{equation*}
%     \scalebox{0.8}{
%     $\begin{aligned}
%         \text{relation}_{28} &= \text{ts\_corr}(\text{adv20}, \text{low}, 5) \\
%         \text{alpha028} &= \text{scale}\left(\text{relation}_{28} + \frac{\text{high} + \text{low}}{2} - \text{close}\right)
%     \end{aligned}$
%     }
% \end{equation*}

% Alpha032 combines deviations from a 7-day moving average and long-term correlation between VWAP and lagged close.
% \begin{equation*}
%     \scalebox{0.8}{
%     $\begin{aligned}
%         \text{alpha032} &= \text{scale}(\text{ts\_mean}(\text{close}, 7) - \text{close}) 
%         \\ &+ 20 \cdot \text{scale}(\text{ts\_corr}(\text{vwap}, \text{ts\_lag}(\text{close}, 5), 230))
%     \end{aligned}$
%     }
% \end{equation*}

% Alpha041 measures the difference between price range midpoint and VWAP.
% \begin{equation*}
%     \scalebox{0.8}{
%     $\begin{aligned}
%         \text{alpha041} = \sqrt{\text{high} \cdot \text{low}} - \text{vwap}
%     \end{aligned}$
%     }
% \end{equation*}

% Alpha046 analyzes trends in lagged and current 10-day price changes.
% \begin{equation*}
%     \scalebox{0.8}{
%     $\begin{aligned}
%         \text{condition}_{46} &= \frac{\text{ts\_lag}(\text{ts\_delta}(\text{close}, 10), 10)}{10} \\ &- \frac{\text{ts\_delta}(\text{close}, 10)}{10} \\
%         \text{alpha046} &=
%         \begin{cases}
%             -1, & \text{condition}_{46} > 0.25, \\
%             1, & \text{condition}_{46} < 0, \\
%             -\text{ts\_delta}(\text{close}, 1), & \text{otherwise.}
%         \end{cases}
%     \end{aligned}$
%     }
% \end{equation*}

% Alpha049 d4r5dx etects strong downtrends or reversals in lagged price movements.
% \begin{equation*}
%     \scalebox{0.8}{
%     $\begin{aligned}
%         \text{condition}_{49} &=  \frac{\text{ts\_lag}(\text{ts\_delta}(\text{close}, 10), 10)}{10} 
%         \\ &- \frac{\text{ts\_delta}(\text{close}, 10)}{10} < -0.1 \cdot \text{close} \\
%         \text{alpha049} &=
%         \begin{cases}
%             1, & \text{condition}_{49}, \\
%             -\text{ts\_delta}(\text{close}, 1), & \text{otherwise.}
%         \end{cases}
%     \end{aligned}$
%     }
% \end{equation*}

% Alpha101 normalizes price change by daily price range.
% \begin{equation*}
%     \scalebox{0.8}{
%     $\begin{aligned}
%         \text{alpha101} = \frac{\text{close} - \text{open}}{\text{high} - \text{low} + 0.001}
%     \end{aligned}$
%     }
% \end{equation*}
% \end{itemize}

\section{Addtional Experimental Results}
\subsection{Score Calculation of Financial Indicators}
\label{append:mi_score}
We compute the MI scores for each indicator using nonparametric estimation methods provided by \textit{mutual\_info\_regression} from the Scikit-learn library. Then we normalize them into range $(0,1)$. These scores provide insight into the strength of the dependency between each financial indicator and the forward return. Higher MI scores indicate stronger relationships, highlighting which indicators are most predictive of future price movements. We select the top-20 indicator groups with the highest MI scores in our candidates, as is shown in Figure \ref{fig:mi}. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/MI_features.png}
    \caption{Mutual Information(MI) scores between indicators and forward return.}
    \label{fig:mi}
\end{figure}

% \subsection{Indicator Occurences}
% \label{append:indicator_occurence}
% We calculate indicator occurrences of different RAG models on our three test sets. Figure \ref{fig:acl18_indicators}, \ref{fig:bigdata22_indicators}, \ref{fig:cikm18_indicators} and \ref{fig:STOCK23_indicators} illustrate results of ACL18, BIGDATA22, CIKM18 and STOCK23, respectively.

% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/acl18/acl18_instructor_indicator_occurrences.png}
%         \caption{ACL18-Instructor}
%         \label{fig:acl18-instructor}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/acl18/acl18_uae_indicator_occurrences.png}
%         \caption{ACL18-UAE}
%         \label{fig:acl18-uae}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/acl18/acl18_e5_indicator_occurrences.png}
%         \caption{ACL18-E5}
%         \label{fig:acl18-e5}
%     \end{subfigure}
    
%     \vspace{0.5cm}
    
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/acl18/acl18_bge_indicator_occurrences.png}
%         \caption{ACL18-BGE}
%         \label{fig:acl18-bge}
%     \end{subfigure}
%     \hfill
%      \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/acl18/acl18_llm_embedder_indicator_occurrences.png}
%         \caption{ACL18-LLM Embedder}
%         \label{fig:acl18-llm_embedder}
%     \end{subfigure}
%     \hfill
%      \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/acl18/acl18_FinSeer_indicator_occurrences.png}
%         \caption{ACL18-FinSeer}
%         \label{fig:acl18-FinSeer}
%     \end{subfigure}

%     \caption{Indicator occurrences of different RAG models on ACL18 dataset.}
%     \label{fig:acl18_indicators}
% \end{figure*}

% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/bigdata22/bigdata22_instructor_indicator_occurrences.png}
%         \caption{BIGDATA22-Instructor}
%         \label{fig:bigdata22-instructor}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/bigdata22/bigdata22_uae_indicator_occurrences.png}
%         \caption{BIGDATA22-UAE}
%         \label{fig:bigdata22-uae}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/bigdata22/bigdata22_e5_indicator_occurrences.png}
%         \caption{BIGDATA22-E5}
%         \label{fig:bigdata22-e5}
%     \end{subfigure}
    
%     \vspace{0.5cm}
    
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/bigdata22/bigdata22_bge_indicator_occurrences.png}
%         \caption{BIGDATA22-BGE}
%         \label{fig:bigdata22-bge}
%     \end{subfigure}
%     \hfill
%      \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/bigdata22/bigdata22_llm_embedder_indicator_occurrences.png}
%         \caption{BIGDATA22-LLM Embedder}
%         \label{fig:bigdata22-llm_embedder}
%     \end{subfigure}
%     \hfill
%      \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/bigdata22/bigdata22_FinSeer_indicator_occurrences.png}
%         \caption{BIGDATA22-FinSeer}
%         \label{fig:bigdata22-FinSeer}
%     \end{subfigure}

%     \caption{Indicator occurrences of different RAG models on BIGDATA22 dataset.}
%     \label{fig:bigdata22_indicators}
% \end{figure*}

% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/cikm18/cikm18_instructor_indicator_occurrences.png}
%         \caption{cikm18-Instructor}
%         \label{fig:cikm18-instructor}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/cikm18/cikm18_uae_indicator_occurrences.png}
%         \caption{cikm18-UAE}
%         \label{fig:cikm18-uae}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/cikm18/cikm18_e5_indicator_occurrences.png}
%         \caption{cikm18-E5}
%         \label{fig:cikm18-e5}
%     \end{subfigure}
    
%     \vspace{0.5cm}
    
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/cikm18/cikm18_bge_indicator_occurrences.png}
%         \caption{cikm18-BGE}
%         \label{fig:cikm18-bge}
%     \end{subfigure}
%     \hfill
%      \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/cikm18/cikm18_llm_embedder_indicator_occurrences.png}
%         \caption{cikm18-LLM Embedder}
%         \label{fig:cikm18-llm_embedder}
%     \end{subfigure}
%     \hfill
%      \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/cikm18/cikm18_FinSeer_indicator_occurrences.png}
%         \caption{cikm18-FinSeer}
%         \label{fig:cikm18-FinSeer}
%     \end{subfigure}

%     \caption{Indicator occurrences of different RAG models on CIKM18 dataset.}
%     \label{fig:cikm18_indicators}
% \end{figure*}

% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/STOCK23/STOCK23_instructor_indicator_occurrences.png}
%         \caption{STOCK23-Instructor}
%         \label{fig:STOCK23-instructor}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/STOCK23/STOCK23_uae_indicator_occurrences.png}
%         \caption{STOCK23-UAE}
%         \label{fig:STOCK23-uae}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/STOCK23/STOCK23_e5_indicator_occurrences.png}
%         \caption{STOCK23-E5}
%         \label{fig:STOCK23-e5}
%     \end{subfigure}
    
%     \vspace{0.5cm}
    
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/STOCK23/STOCK23_bge_indicator_occurrences.png}
%         \caption{STOCK23-BGE}
%         \label{fig:STOCK23-bge}
%     \end{subfigure}
%     \hfill
%      \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/STOCK23/STOCK23_llm_embedder_indicator_occurrences.png}
%         \caption{STOCK23-LLM Embedder}
%         \label{fig:STOCK23-llm_embedder}
%     \end{subfigure}
%     \hfill
%      \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[height=8cm]{figures/indicators/STOCK23/STOCK23_FinSeer_indicator_occurrences.png}
%         \caption{STOCK23-FinSeer}
%         \label{fig:STOCK23-FinSeer}
%     \end{subfigure}

%     \caption{Indicator occurrences of different RAG models on STOCK23 dataset.}
%     \label{fig:STOCK23_indicators}
% \end{figure*}

\subsection{Candidate Movement Correlation}
\label{append:candidate_movement_correlation}
For each query with retrieved candidates, we convert the movements of retrieved sequences and LLM's prediction to numerical representations.
\begin{equation*}
    \scalebox{0.9}{
        $\begin{aligned}
            {M_{c_j}, M_{q}} &= 
            \begin{cases} 
                \text{1}, & \text{rise} \\
                \text{0}, & \text{freeze} \\
                \text{-1}, & \text{fall}
            \end{cases}
        \end{aligned}$
    }
\end{equation*}
Then, we calculate the average value of the $M_{c_i}$ from the five retrieved sequences to represent the RAG-provided result, and the final prediction of the query is recorded as another numerical value:
\begin{equation*}
    \scalebox{0.9}{
        $\begin{aligned}
            x_i &= \frac{1}{5}\sum_{j=0}^4 M_{c_j}, \\
            y_i &= M_{q_i}.
        \end{aligned}$
    }
\end{equation*}
Lastly, we adopt the Pearson correlation coefficient to compute the correlation:
\begin{equation*}
    \scalebox{0.9}{
        $\begin{aligned}
            r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}.
        \end{aligned}$
    }
\end{equation*}

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
