[
  {
    "index": 0,
    "papers": [
      {
        "key": "fan2024survey",
        "author": "Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing",
        "title": "A survey on rag meeting llms: Towards retrieval-augmented large language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chen2017reading",
        "author": "Chen, Danqi  and\nFisch, Adam  and\nWeston, Jason  and\nBordes, Antoine",
        "title": "Reading {W}ikipedia to Answer Open-Domain Questions"
      },
      {
        "key": "jiang2023active",
        "author": "Jiang, Zhengbao  and\nXu, Frank  and\nGao, Luyu  and\nSun, Zhiqing  and\nLiu, Qian  and\nDwivedi-Yu, Jane  and\nYang, Yiming  and\nCallan, Jamie  and\nNeubig, Graham",
        "title": "Active Retrieval Augmented Generation"
      },
      {
        "key": "ram2023context",
        "author": "Ram, Ori  and\nLevine, Yoav  and\nDalmedigos, Itay  and\nMuhlgay, Dor  and\nShashua, Amnon  and\nLeyton-Brown, Kevin  and\nShoham, Yoav",
        "title": "In-Context Retrieval-Augmented Language Models"
      },
      {
        "key": "xu2024recomp",
        "author": "Fangyuan Xu and Weijia Shi and Eunsol Choi",
        "title": "{RECOMP}: Improving Retrieval-Augmented {LM}s with Context Compression and Selective Augmentation"
      },
      {
        "key": "zhong2022training",
        "author": "Zhong, Zexuan and Lei, Tao and Chen, Danqi",
        "title": "Training Language Models with Memory Augmentation"
      },
      {
        "key": "zhoudocprompting",
        "author": "Zhou, Shuyan and Alon, Uri and Xu, Frank F and Jiang, Zhengbao and Neubig, Graham",
        "title": "DocPrompting: Generating Code by Retrieving the Docs"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "izacard2021leveraging",
        "author": "Izacard, Gautier and Grave, {\\'E}douard",
        "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "li2023mot",
        "author": "Li, Xiaonan and Qiu, Xipeng",
        "title": "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve"
      },
      {
        "key": "ludynamic",
        "author": "Lu, Pan and Qiu, Liang and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Rajpurohit, Tanmay and Clark, Peter and Kalyan, Ashwin",
        "title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"
      },
      {
        "key": "milios2023context",
        "author": "Milios, Aristides and Reddy, Siva and Bahdanau, Dzmitry",
        "title": "In-context learning for text classification with many labels"
      },
      {
        "key": "poesiasynchromesh",
        "author": "Poesia, Gabriel and Polozov, Alex and Le, Vu and Tiwari, Ashish and Soares, Gustavo and Meek, Christopher and Gulwani, Sumit",
        "title": "Synchromesh: Reliable Code Generation from Pre-trained Language Models"
      },
      {
        "key": "rubin2022learning",
        "author": "Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan",
        "title": "Learning To Retrieve Prompts for In-Context Learning"
      },
      {
        "key": "ye2023compositional",
        "author": "Ye, Jiacheng and Wu, Zhiyong and Feng, Jiangtao and Yu, Tao and Kong, Lingpeng",
        "title": "Compositional exemplars for in-context learning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "qin2017dual",
        "author": "Qin, Yao and Song, Dongjin and Chen, Haifeng and Cheng, Wei and Jiang, Guofei and Cottrell, Garrison",
        "title": "A dual-stage attention-based recurrent neural network for time series prediction"
      },
      {
        "key": "feng2018enhancing",
        "author": "Feng, Fuli and Chen, Huimin and He, Xiangnan and Ding, Ji and Sun, Maosong and Chua, Tat-Seng",
        "title": "Enhancing stock movement prediction with adversarial training"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "ding2015deep",
        "author": "Ding, Xiao and Zhang, Yue and Liu, Ting and Duan, Junwen",
        "title": "Deep learning for event-driven stock prediction"
      },
      {
        "key": "liu2018hierarchical",
        "author": "Liu, Qikai and Cheng, Xiang and Su, Sen and Zhu, Shuguang",
        "title": "Hierarchical complementary attention network for predicting stock price movements with news"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "xu2018stock",
        "author": "Xu, Yumo and Cohen, Shay B",
        "title": "Stock movement prediction from tweets and historical prices"
      },
      {
        "key": "wu2018hybrid",
        "author": "Wu, Huizhe and Zhang, Wei and Shen, Weiwei and Wang, Jun",
        "title": "Hybrid deep sequential modeling for social text-driven stock prediction"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "xie2023pixiu",
        "author": "Xie, Qianqian and Han, Weiguang and Zhang, Xiao and Lai, Yanzhao and Peng, Min and Lopez-Lira, Alejandro and Huang, Jimin",
        "title": "PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance"
      },
      {
        "key": "xie2024finben",
        "author": "Xie, Qianqian and Han, Weiguang and Chen, Zhengyu and Xiang, Ruoyu and Zhang, Xiao and He, Yueru and Xiao, Mengxi and Li, Dong and Dai, Yongfu and Feng, Duanyu and others",
        "title": "The finben: An holistic financial benchmark for large language models"
      },
      {
        "key": "xie2023wall",
        "author": "Xie, Qianqian and Han, Weiguang and Lai, Yanzhao and Peng, Min and Huang, Jimin",
        "title": "The wall street neophyte: A zero-shot analysis of chatgpt over multimodal stock movement prediction challenges"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jin2023time",
        "author": "Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and others",
        "title": "Time-llm: Time series forecasting by reprogramming large language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yu2023temporal",
        "author": "Yu, Xinli and Chen, Zheng and Ling, Yuan and Dong, Shujing and Liu, Zongyi and Lu, Yanbin",
        "title": "Temporal Data Meets LLM--Explainable Financial Time Series Forecasting"
      },
      {
        "key": "liu2024timecma",
        "author": "Liu, Chenxi and Xu, Qianxiong and Miao, Hao and Yang, Sun and Zhang, Lingzheng and Long, Cheng and Li, Ziyue and Zhao, Rui",
        "title": "TimeCMA: Towards LLM-Empowered Time Series Forecasting via Cross-Modality Alignment"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "pan2024s",
        "author": "Pan, Zijie and Jiang, Yushan and Garg, Sahil and Schneider, Anderson and Nevmyvaka, Yuriy and Song, Dongjin",
        "title": "$ S\\^{} 2$ IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "yang2024timerag",
        "author": "Yang, Silin and Wang, Dong and Zheng, Haoqi and Jin, Ruochun",
        "title": "TimeRAG: BOOSTING LLM Time Series Forecasting via Retrieval-Augmented Generation"
      }
    ]
  }
]