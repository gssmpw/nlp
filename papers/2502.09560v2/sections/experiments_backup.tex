\section{Experiments}

% \heng{add analysis on remaining challenges, provide some insights and ideas for future directions/solutions}
\input{tables/main_high_level}
\input{tables/main_low_level}

In this section, we conduct comprehensive experiments to evaluate the performance of various MLLMs in \name, followed by ablation studies in Sections \ref{sec:language_ablation} and \ref{sec:visual_ablation} and error analysis in Section \ref{sec:error_analysis}.

\vspace{-5pt}
\subsection{Experimental Setups}
\vspace{-5pt}

% We benchmark 13 models, including leading proprietary models (GPT-4o / 4o-mini~\cite{GPT-4o,GPT-4o-mini}, Claude-3.5-Sonnet~\cite{Claude-3.5-Sonnet}, Gemini Pro / Flash~\cite{team2023gemini,team2024gemini,Gemini2.0}, and SOTA open-source models (LLaMA3.2 11B / 90B Vision Instruct~\cite{llama3.2}, InternVL 2.5 8B / 38B / 78B~\cite{internvl2.5}, Qwen2-VL 7B / 72B~\cite{wang2024qwen2}). For consistency, all models are set with a temperature of 0 and a maximum completion token length of 2048. All images are standardized to a resolution of 500$\times$500 pixels. The maximum number of environment steps is 30 for high-level tasks, 20 for EB-Navigation, and 15 for EB-Manipulation. We use the task success rate as the primary metric in our main experiments. More results and ablations are deferred to Appendix \ref{ap:additional_exp}.

We benchmark 19 models, including leading proprietary models (GPT-4o / 4o-mini~\cite{GPT-4o,GPT-4o-mini}, Claude-3.5-Sonnet~\cite{Claude-3.5-Sonnet}, Gemini Pro / Flash~\cite{team2023gemini,team2024gemini,Gemini2.0}, and Qwen-VL-Max \cite{bai2023qwen}), and SOTA open-source models (LLaMA3.2 11B / 90B Vision Instruct~\cite{llama3.2}, InternVL 2.5 8B / 38B / 78B~\cite{internvl2.5} and their MPO versions \cite{wang2024enhancing}, and Qwen2-VL and Qwen2.5-VL 7B / 72B~\cite{wang2024qwen2,bai2025qwen25vl}). For consistency, all models are set with a temperature of 0 and a maximum completion token length of 2048. All images are standardized to a resolution of 500$\times$500 pixels. The maximum number of environment steps is 30 for high-level tasks, 20 for EB-Navigation, and 15 for EB-Manipulation. We use the task success rate as the primary metric in our main experiments. More results and ablations are deferred to Appendix \ref{ap:additional_exp}.



\vspace{-5pt}

\subsection{Benchmark Results}\label{sec:benchmark_res}
% \vspace{-5pt}

\textbf{Overall Results.} Tables \ref{tb:high_level_table} and \ref{tb:low_level_table} summarize the results for high-level and low-level tasks, respectively. Overall, \textit{\textbf{current MLLMs demonstrate strong performance on high-level tasks but struggle with low-level tasks, especially EB-Manipulation.}}
Among \textbf{proprietary models}, we observe that different models excel at different task levels: Claude-3.5-Sonnet achieves the highest average accuracy on high-level tasks, with 64.0\% on EB-ALFRED and 68.0\% on EB-Habitat, while GPT-4o leads in low-level tasks, scoring 57.7\% on EB-Navigation and 28.9\% on EB-Manipulation. Gemini-1.5-Pro performs the worst among the three large proprietary models, but Gemini-1.5 / 2.0-Flash outperforms GPT-4o-mini by a large margin. For \textbf{open-source models}, InternVL2\_5 model family exhibits the best overall performance, with its largest 78B version outperforming Llama-3.2-90B-Vision-Ins and Qwen2-VL-72B-Ins across all 4 environments. Additionally, open-source models exhibit a clear scaling effect, as their performance improves with increasing model parameters. Moreover, although large open-source models are closing the gap with smaller proprietary models like GPT-4o-mini, a notable performance difference remains between large proprietary and open-source models. 


% \textbf{The Role of Vision in Embodied Agent.}
% By comparing the performance of embodied agents with and without visual information (marked as ``Lang") in Tables \ref{tb:high_level_table} and \ref{tb:low_level_table}, we observe a clear distinction between low-level and high-level tasks. \textbf{\textit{Low-level tasks show a much stronger reliance on vision compared to high-level tasks.}} For example, disabling vision causes GPT-4oâ€™s EB-Navigation performance to drop sharply from 57.7\% to 17.4\%, with long-horizon planning completely collapsing to 0\%. This sharp decline highlights the critical importance of visual signals for low-level control tasks. Conversely, high-level tasks show much less dependence on visual input. GPT-4o (Lang) and GPT-4o-mini (Lang) perform on par with or even outperform their vision-enabled counterparts in EB-ALFRED and EB-Habitat, suggesting that these tasks may rely more heavily on textual information rather than visual input. We will further investigate the impact of language-centric factors in Section \ref{sec:language_ablation}. These findings emphasize two key insights: (1) when designing MLLM-based embodied AI benchmarks, it is essential to consider action-level taxonomy, with greater attention to low-level action tasks, and (2) more advanced methods are needed to effectively leverage visual input for high-level embodied tasks.
