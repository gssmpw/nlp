\section{Conlcusion}
% We introduce \name, a comprehensive evaluation framework for vision-driven embodied agents powered by MLLMs. In our experiments, we examine the role of vision in embodied tasks and explore various impact factors. Our findings also point out promising future research directions, such as improving MLLMs' ability to understand multi-step images and advancing visual in-context learning.

We introduce \name, a comprehensive evaluation framework designed to assess MLLM-based embodied agents across tasks with varying action levels and capability-oriented subsets. Through extensive experiments, we identified key challenges, including difficulties in low-level manipulation and long-horizon planning, and the varying significance of vision input across tasks. By highlighting these areas for improvement, we hope \name will inspire and guide future research, driving the development of more capable and versatile vision-driven embodied agents. 

\textbf{Future Research Directions.}
While \name represents a significant step forward in evaluating MLLM-based embodied agents, several challenges remain, offering rich opportunities for future research. Below, we outline potential research directions:
\begin{itemize}
    \item \emph{Expanding Task Diversity.} Current benchmarks for MLLM-based embodied agents are still limited in task diversity. Future research could explore more realistic and complex environments with different action levels, such as autonomous driving \cite{gulino2024waymax,ma2024lampilot,gao2024embodiedcity}, multi-agent collaboration \cite{liu2024heterogeneous}, and human-agent interaction \cite{chang2024partnr}. These scenarios would better assess the agents' adaptability and generalization capabilities in real-world settings. 
    \item \emph{Low-Level Tasks and Spatial Reasoning.} Our findings show that current MLLM-based agents struggle with spatial reasoning and low-level control. Future research could improve these capabilities by better integrating spatial reasoning with low-level action planning, including 3D visual grounding \cite{chen2024spatialvlm,cheng2024spatialrgpt} and alignment \cite{ahn2022can,yang2024embodied}.
    \item  \emph{Long-Horizon Planning.} Long-horizon planning is still challenging for embodied agents.
    Future research can study techniques like hierarchical planning \cite{llm-planner,ajay2023compositional}, memory-augmented methods \cite{sarch2024helper}, and world models \cite{mazzaglia2024genrl} to enhance their ability to plan and execute complex, multi-step tasks more effectively.
    \item \emph{Multi-step/Multi-view Image Understanding.} Our experiments show that current MLLMs struggle with multi-step and multi-view image inputs. Future research could improve multi-frame and multi-view comprehension, temporal reasoning, and spatial awareness to enhance MLLM agents' visual perception and reasoning. One promising direction is leveraging video pretraining \cite{madan2024foundation,wang2024qwen2} to better equip embodied agents for these challenges.
    \item \emph{Visual In-context Learning (ICL).}
    Our experiments confirm the effectiveness of visual ICL \cite{zhou2024visual,sarch2024vlm} in embodied decision-making. This approach is promising because it enables adaptability and versatility without fine-tuning, allowing better use of off-the-shelf MLLMs. However, designing more effective visual ICL methods for embodied tasks remains an open problem for future research.
    % \item  \emph{Integration of Robotics Foundation Models.} Our benchmark currently relies on off-the-shelf MLLMs that are not specifically trained for embodied tasks. Future work can explore integrating pretrained robotics foundation models \cite{mu2024embodiedgpt,kim2024openvla} to enhance perception, spatial reasoning, and action generation. This integration could combine the strengths of general-purpose MLLMs for instruction understanding and reasoning with task-specific robotics models, leveraging the best of both worlds to enhance embodied decision-making.
   \item  \emph{Training Multimodal Embodied Agents.}
   While our work focuses on evaluation, fine-tuning MLLMs for embodied tasks could significantly enhance their performance \cite{mu2024embodiedgpt,szot2024multimodal,zawalski2024robotic}. Future research can explore embodied pretraining, imitation learning, and both offline and online reinforcement learning \cite{sun2023reinforcement} to better optimize MLLMs for embodied decision-making. Additionally, developing end-to-end learning approaches that seamlessly integrate perception, reasoning, and action could reduce the need for designing complex agent frameworks, leading to more adaptive and generalizable agents.
   
    \item \emph{Robustness and Generalization of MLLM Agents.} Ensuring real-world applicability requires a thorough study of MLLM agents' robustness and generalization capabilities. While related studies are emerging in other domains \cite{zou2024dynamath,xu2024robust,yang2023towards,yang2024regularizing,zhang2024out}, research on MLLM agents remains limited. Potential methods involve incorporating adversarial settings \cite{liu2024exploring,wu2024dissecting}, dynamically generated environments \cite{wang2023robogen}, or domain shifts \cite{chattopadhyay2021robustnav} to assess and enhance the ability of embodied agents to perform reliably in varying conditions.
\end{itemize}
By exploring these directions, the field can move closer to realizing the full potential of MLLM-based embodied agents in real-world applications.



% big picture, VLA, robotics foundation model, general, visual bottelneck,
%  long horizon decision making, finegrained 





