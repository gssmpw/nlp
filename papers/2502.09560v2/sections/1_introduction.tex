\section{Introduction} \label{sec:intro}
Developing embodied agents capable of solving complex tasks in real world remains a significant challenge \cite{durante2024agent}. Recent advancements in foundation models—including Large Language Models (LLMs) \cite{GPT3,achiam2023gpt,touvron2023llama,yang2024qwen2} and Multimodal Large Language Models (MLLMs) \cite{GPT-4o,reid2024gemini,liu2024llavanext,wang2024qwen2,chen2023internvl,internvl2.5}—have unlocked unprecedented potential toward this goal. These models, trained on extensive internet-scale datasets, demonstrate exceptional proficiency in understanding human knowledge and performing human-like reasoning. Based on these capabilities, researchers can now design intelligent agents that use off-the-shelf foundation models to solve complex tasks through interaction with environments \cite{huang2022language,huang2022inner,huang2023voxposer,ahn2022can,llm-planner,singh2023progprompt,liang2023code,EscapeBench2024}.


\begin{figure*}[th!]
\begin{center}
\vspace{-5pt}
% \includegraphics[width=0.94\linewidth]{pics/embodied_overview_new.pdf}
\includegraphics[width=0.97\linewidth, trim=0 0 0 15, clip]{pics/embodied_overview_new.pdf}
\end{center}
\vspace{-1.2em}
\caption{Overview of \name. Two key features of our benchmark: various action levels and capability-oriented evaluation.}\label{fig:overview}
\vspace{-1.5em}
\end{figure*}


Given the multitude of proposed algorithms, there is a pressing need for standardized and automated evaluation frameworks to enable comprehensive assessment and comparison.  To address this need, several initiatives have been exploring LLM-based embodied agent evaluation \cite{liu2023agentbench,choi2024lota,li2024embodied}. While these efforts significantly contribute to understanding LLM-based agent design, the evaluation of MLLM embodied agents remains underexplored, posing a challenge for creating more versatile agents. VisualAgentBench \cite{liu2024visualagentbench} represents the first benchmark for evaluating MLLM agents, covering embodied tasks such as household and Minecraft. However, its limited scope, focusing exclusively on high-level planning, leaves critical questions unanswered, such as \emph{the role of vision in embodied tasks and the performance of MLLM agents in low-level tasks like navigation and manipulation}. 



To address these questions, we introduce \name, a comprehensive benchmark comprising 1,128 testing instances across four environments. \name is designed with two key features that set it apart from existing benchmarks:
\textbf{1. Diverse tasks with hierarchical action levels.} Among the four environments, EB-ALFRED and EB-Habitat focus on high-level task decomposition and planning (e.g., ``put a book on the desk"), while EB-Navigation and EB-Manipulation demand planning with low-level actions (e.g., translational/rotational control) and require precise perception and spatial reasoning. 
\textbf{2. Capability-oriented evaluation.} Unlike previous benchmarks that primarily emphasize overall accuracy \cite{liu2023agentbench,choi2024lota,liu2024visualagentbench} or module-specific performance \cite{li2024embodied}, \name introduces a fine-grained evaluation framework that assesses six critical capabilities of embodied agents, including basic task solving, commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-horizon planning. 


To facilitate the evaluation of MLLMs as embodied agents, we design a unified agent framework that integrates ego-centric visual perception, few-shot in-context examples, interaction history, and environment feedback for decision-making. This powerful framework can unlock the full potential of current off-the-shelf MLLMs and tackle both high-level and low-level tasks effectively. Based on \name and our agent pipeline, we evaluate 19 leading closed-source MLLMs (e.g., GPT-4o, Gemini, Claude-3.5, and Qwen-VL-Max) and 7B–90B open-source models (e.g., Llama-3.2 Vision \cite{llama3.2}, InternVL 2.5 series \cite{internvl2.5,wang2024enhancing}, Qwen2-VL \cite{wang2024qwen2}, and Qwen2.5-VL \cite{bai2025qwen25vl}). Our evaluation yields three key findings: (1) While MLLMs excel at high-level tasks, they struggle with low-level manipulation. (2) Long-horizon planning emerges as the most challenging subset. (3) Vision input is crucial for low-level tasks, with performance degrading by 40\%–70\% when removed, whereas its impact on high-level tasks is minimal. Additionally, our ablation studies provide practical insights into MLLM agent design, particularly regarding image resolution, multi-step image input, and visual in-context learning.


Our contributions are threefold: (1) proposing a comprehensive benchmark suite for evaluating MLLM-based embodied agents with different action levels and fine-grained capability-oriented subsets, (2) the development of an efficient MLLM agent framework, (3) conducting extensive evaluations and ablation studies of leading MLLMs, providing valuable insights for vision-driven agent design. 




















\input{tables/comparison}








