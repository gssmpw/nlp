\section{Additional Related Works} \label{ap:additional_related_works}

Foundation models \cite{bommasani2021opportunities}, particularly Large Language Models (LLMs) \cite{GPT3,achiam2023gpt,touvron2023llama,yang2024qwen2,yang2024rewards} and Multi-Modal Large Language Models (MLLMs) \cite{radford2021learning, team2024gemini, wang2024qwen2, wu2024deepseek, du2025kimi, singletransformer2024,xie2024large}, fundamentally transform how embodied agents perceive, make decisions, and act in physical and simulated environments. 

The integration of these models into embodied agents evolves through several key approaches. Initially, Large Language Models (LLMs) are introduced to assist with high-level planning \cite{ahn2022can, huang2022language, huang2022inner, rana2023sayplan,gao2024physically, huang2023grounded,wang2023voyager,huang2023instruct2act,schema2023d,schema2023e,chen2023robogpt,huang2023embodied,zhou2024navgpt}. They are also adopted for low-level controls \cite{mao2023gpt,yin2024context}. MLLMs are then incorporated for perception tasks such as object attribute identification, visual relation extraction, and action recognition~\cite{xiao2022robotic, ViStruct2023, actionpatch2023,wang2023describe, gao2024physically, gu2024conceptgraphs}. Subsequently, the role of MLLMs extends into policy-making through various approaches. Some works implement MLLMs in an end-to-end manner for direct action generation \cite{shridhar2022cliport, driess2023palm, du2023video,yang2024embodied,mu2024embodiedgpt}. Others enhance policy generation by using MLLMs to create visual markers or generate constraints or guidance with visual masks \cite{sharma2023semantic, stone2023open, nasiriany2024pivot, huang2024copa, jiang2024roboexp}. A different approach involves prompting MLLMs to generate code for creating policy or value functions \cite{liang2023code, huang2023voxposer, huang2024rekep}.


%to add after accepting if needed,schema2023b,SunEACL2023,hierarchicalschema2023,simulator2024

Most recently, Vision Language Action Models (VLAs) \cite{brohan2022rt, brohan2023rt, chi2023diffusion, belkhale2024rt, team2024octo, liu2024rdt, kim2024openvla} have emerged as a promising direction. These models typically utilize MLLMs or language-conditioned diffusion models as their foundation and are trained on low-level robotics action data. Another promising direction leverages world models as action simulators \cite{xiang2024pandora, agarwal2025cosmos, liu2025physgen}. These approaches employ diffusion models conditioned on language inputs to predict future states given actions or task descriptions.

In response to the rapid advancements in this field, various simulators \cite{kolve2017ai2, puig2018virtualhome, shridhar2020alfred, xiang2020sapien, shen2021igibson, li2021igibson, li2023behavior, nasiriany2024robocasa} and evaluation benchmarks \cite{shridhar2020alfworld,shridhar2020alfred,zheng2022vlmbench,li2023behavior,szot2023large,luo2023fmb,ijcai2024p15,koh2024visualwebarena,choi2024lota,khanna2024goat,liu2024visualagentbench,li2024embodied,zhang2024vlabench,song2024towards} have been developed. However, existing benchmarks exhibit notable limitations. For instance, ALFWorld \cite{shridhar2020alfworld}, AgentBench \cite{liu2023agentbench}, Lota-bench \cite{choi2024lota}, and Embodied Agent Interface \cite{li2024embodied} lack support for multimodal input evaluation. Furthermore, most benchmarks are narrowly focused on specific domains, particularly high-level household tasks \cite{shridhar2020alfred, li2023behavior, szot2023large}, while others, such as VLMbench \cite{zheng2022vlmbench} and GOAT-bench \cite{khanna2024goat}, concentrate on low-level control for manipulation and navigation, respectively. Although VisualAgentBench \cite{liu2024visualagentbench} pioneers the evaluation of MLLMs across multiple domains, it is limited to high-level tasks like household activities and Minecraft, and does not support fine-grained capability assessment. Embodied Agent Interface \cite{li2024embodied} and VLABench \cite{zhang2024vlabench} introduce fine-grained evaluation metrics with language model support, but their focus remains primarily on LLMs and VLAs rather than MLLMs. Concurrently, EmbodiedEval \cite{cheng2025embodiedeval} introduces a multi-domain benchmark for evaluating MLLMs across navigation, object interaction, social interaction, attribute question answering, and spatial question answering. While they overlap with our work in navigation and object interaction, their benchmark lacks low-level manipulation tasks and capability-oriented evaluation. Additionally, it is limited in scale, with only 328 testing instances.




\section{Details about \name Environments and Datasets} \label{ap:detailed_task}
Below, we provide detailed descriptions of four environments and their corresponding datasets. Please note that the maximum number of environment steps varies by task: 30 steps for high-level tasks (EB-ALFRED and EB-Navigation), 20 steps for EB-Navigation, and 15 steps for EB-Manipulation. In addition to task completion and exceeding the maximum step limit, we introduce two additional stopping conditions: (1) \emph{Invalid Action Limit:} If the model generates more than 10 invalid actions in a single trajectory, indicating a lack of understanding and difficulty in producing valid actions. (2) \emph{Empty Plan Generation:} If the model generates an empty plan because it incorrectly assumes the task is complete. This issue mainly occurs in high-level tasks, and once it happens, the model tends to keep generating empty plans without making progress.  
These additional stopping conditions help reduce unnecessary computational costs and improve evaluation efficiency.

\subsection{EB-ALFRED} \label{ap:details_alfred}

\paragraph{Task Description.} We develop the EB-ALFRED tasks based on the ALFRED dataset and the AI2-THOR simulator, which are well-regarded within the embodied AI community for their diverse household tasks and scenes. These tasks aim to evaluate an agent's ability to organize and execute sequences of high-level actions in household scenarios, such as ``Put washed lettuce in the refrigerator." Each task in ALFRED can be described using the Planning Domain Definition Language (PDDL), which helps assess the agent's success in completing the task or subgoals. The ALFRED dataset includes 7 task types, \textit{Pick \& Place, Stack \& Place, Pick Two \& Place, Clean \& Place, Heat \& Place, Cool\& Place, and Examine in Light}. Our simulator is based on Lota-Bench's implementation for 8 high-level action types: ``pick up", ``open", ``close", ``turn on", ``turn off", ``slice", ``put down", and ``find". Each action can be parameterized with a specific object to form an action, e.g.,``find an apple" or ``pick up an apple". The simulation offers an egocentric view and text feedback on the validity of action execution and potential reasons for any invalid actions. For example, it may indicate ``failure to pick up an object because another object is already being held."


Despite its strengths, Lota-Bench's simulator has \textbf{three notable limitations}: (1) it does not support the \textit{Pick Two \& Place} task type due to the inability to handle multiple instances of one object type. (2) Some actions lead to incorrect task execution, such as the ``put down" action erroneously placing an object on top of the sink instead of inside it, causing a correct action but unsuccessful outcome. (3) Additionally, some instructions in the original ALFRED dataset suffer from low quality. We observe the erroneous use of ``potato" in task related to ``tomato", which prevents agents from successfully completing the tasks due to these incorrect instructions.

To enhance the simulation, we implemented several improvements. Firstly, we introduced \textit{\textbf{support for multi-instance settings}} in ALFRED by appending index suffixes to objects, such as "find a cabinet\_2," to accommodate multiple instances of the same object type. Therefore, we can support all 7 task types in ALFRED. Given the dynamic number of objects in the ALFRED dataset, we made the action space of EB-ALFRED dynamic, ranging from 171 to 298 actions. To \textit{\textbf{minimize redundancy in the action space}}, we merge all ``put down" actions into a single action, since only one object can be held at a time. Additionally, we manually \textit{\textbf{corrected bugs in the original simulation and improved the quality of language instructions}} to ensure tasks are solvable and actions can be executed more accurately. These enhancements make EB-ALFRED a high-quality benchmark for evaluating embodied agents.

\paragraph{Dataset Collection.} Following Lota-Bench \cite{choi2024lota}, we use the valid seen set from the ALFRED dataset. We first partition the dataset based on the number of steps in the oracle policy. Specifically, we select 50 samples from the subset with fewer than 15 steps, carefully refining their instructions to minimize ambiguity and improve task solvability. The commonsense and complex instruction subsets are primarily derived from this base subset, with GPT-4o augmentation tailored to specific capabilities. Additionally, we select 50 tasks with more than 15 steps to form the long-horizon subset. The visual appearance and spatial awareness subsets are chosen directly from the original dataset based on language descriptions of color/shape, or relative positions. In total, EB-ALFRED comprises 300 testing instances, evenly distributed across six subsets (50 instances each).





\subsection{EB-Habitat}
\paragraph{Task Description.}EB-Habitat is developed based on the Language Rearrangement benchmark \cite{szot2023large}, featuring 282 diverse language instruction templates designed for robotic rearrangement tasks. It leverages the Habitat 2.0 simulator \cite{szot2021habitat} and includes object data from the YCB dataset \cite{calli2015ycb} and ReplicaCAD \cite{szot2021habitat}. The benchmark focuses on planning and executing 70 high-level skills to achieve user-defined goals, such as ``Find a toy airplane and move it to the right counter." These skills are categorized into five action types: ``navigation", ``pick", ``place", ``open", and ``close", each parameterized by specific objects.


Unlike ALFRED, which permits navigation to any object, EB-Habitat constrains navigation to receptacle-type objects, requiring robots to visit multiple locations to locate target items. Task and subgoal completion are evaluated using PDDL, with agents receiving visual input and textual feedback similar to ALFRED. Given its broad range of language instructions and distinct navigation constraints, EB-Habitat serves as a complementary counterpart to EB-ALFRED, expanding the scope of our high-level embodied tasks.

\paragraph{Dataset Collection.}Habitat already provides fine-grained evaluation datasets with multiple subsets. We reorganize the subsets to formulate our dataset. Specifically, we merge ``new scenes", ``novel objects", and ``instruction rephrasing" to form our base subset; we use the ``context" set as our commonsense subset; we merge the ``conditional instructions" and ``irrelevant instruction text" as our complex instruction subset; we use the ``referring expressions" as our visual appearance subset; we use the ``spatial relationship" as our spatial awareness subset; we merge the ``multiple rearrangements" and ``multiple objects" as our long-horizon subset. Then, we sample 50 instances from each subset to form our EB-Habitat dataset, resulting in a total of 300 testing instances.



\subsection{EB-Navigation}
\paragraph{Task Description.}EB-Navigation is an evaluation suite built on AI2-THOR, designed to assess the navigation capabilities of embodied agents. In each task, the agent is placed at a starting position and must use visual observations and behavior feedback to execute low-level actions. The goal is to locate a target object and navigate to its vicinity.
The agent’s action space consists of seven actions that are executable by physical robots: 
(1) Move forward/backward by $\Delta x$.  
(2) Move rightward/leftward by $\Delta y$.  
(3) Rotate to the right/left by $\Delta \theta$ degrees.  
(4) Tilt the camera upward/downward by $\Delta \varphi$ degrees.
At the start of each task, the agent is provided with a textual description of the action space, where each action is mapped to a unique index. Then, the agent selects an action by outputting the corresponding index, which the environment then executes.


At the beginning of each step, the environment provides the agent with a first-person visual observation. Using this visual input, the agent performs planning and decision-making to choose its next action. After executing an action, the environment evaluates its validity. For example, it checks for collisions or obstacles that might cause the action to fail. The environment then provides this valid or invalid signal as feedback to the agent. This signal is the only feedback the agent receives, as it is feasible to obtain in real-world scenarios. Together with the visual observations, this feedback equips the agent with sufficient information to perform navigation tasks effectively.


\paragraph{Dataset Collection.}
We constructed the dataset based on the original dataset provided by AI2-THOR. In AI2-THOR \cite{kolve2017ai2}, there are diverse scenes including environments such as kitchens, living rooms, and bedrooms, we designed a total of 90 navigation tasks, one for each scene. Each task dataset includes the following information:  
(1) \textit{Initial Robot Pose}: Including its $(x, y, z)$ coordinates and initial orientation.  
(2) \textit{Target object information}: Specifying the object type, ID and the 3D coordinates of the object’s center.  
(4) \textit{Language navigation instruction}: A human-readable instruction specifying the target object the agent needs to navigate to.
We ensure the validness of the task dataset through the implementation of the following characteristics:  
(1) Initial distance: The agent’s starting position is carefully constrained to be at least a certain adjustable distance (denoted as $\alpha$) from the target object. This adjustable $\alpha$ allows users to customize the number of navigation steps required for each task.  
(2) Target object accessibility: All target objects are exposed in the environment, reachable without requiring the agent to leave the ground.  
(3) Task completion conditions: A task is considered complete if the agent reaches a position within a specified distance threshold from the target object or if the maximum number of steps is exceeded.
Additionally, the dataset includes an automated task-generation script. This script allows users to create custom task datasets by specifying parameters such as the target object type, initial distance threshold, and random seed for each scene. This flexibility ensures the dataset can be adapted to various research needs and scenarios.

For the capability-oriented subsets, we begin by sampling 60 instances from the original 90 tasks to form the base subset. We then use GPT-4 to perform instruction augmentation, generating more complex instructions and incorporating common sense knowledge to create the complex instruction and common sense subsets. The visual appearance subset is manually curated to include detailed descriptions of the target object's color and shape. Finally, the long horizon subset is constructed by ensuring the target object is not visible in the agent's initial view, requiring extended navigation to locate it. In total, we collect 300 testing instances across these 5 subsets (excluding the spatial awareness subset).



\subsection{EB-Manipulation}
\paragraph{Task description.} 
EB-Manipulation is an extension of VLMBench \cite{zheng2022vlmbench} using the CoppeliaSim simulator \cite{rohmer2013v} to control a 7-DoF Franka Emika Panda robotic arm. EB-Manipulation includes four task categories: (1) \textit{Pick \& Place Objects}, (2) \textit{Stack Objects}, (3) \textit{Shape Sorter Placement}, and (4) \textit{Table Wiping}, each with randomly varied instances in color, position, shape, and orientation for diverse evaluation. The action space is a 7-dimensional vector. The simulator processes these actions and performs automatic motion planning to achieve the desired position. To facilitate motion planning, the environment operates in ABS\_EE\_POSE\_PLAN\_WORLD\_FRAME mode, ensuring automatic trajectory execution from the current pose to the target pose. This simplifies the agent’s role in predicting keypoints necessary for task completion. 



Direct low-level manipulation is challenging for MLLMs due to insufficient domain-specific training. To overcome this, we implemented enhancements. \textbf{(1) Action space discretization}\cite{yin2024context}, which divides the position component into 100 bins and the orientation component into 120 bins, enabling valid actions to take forms like $[x, y, z, roll, pitch, yaw, gripper]=[57,61,20,10,60,25,1]$. Here, the first three dimensions (X, Y, Z) range from 0 to 100, while the next three (Pitch, Yaw, Gripper) range from 0 to 120. The gripper state remains binary (0.0 or 1.0). By discretizing the originally continuous action space, the model can predict actions using integer values, reducing complexity for MLLMs. \textbf{(2) Additional information} like YOLO \cite{redmon2016you} detection boxes with index markers and 3D object pose estimation for indexed objects, reducing the need for precise 3D location. Instead, the agent can focus on perceiving and reasoning about each object's relationship to the indexed objects. With these improvements and additional in-context examples, our MLLM agent effectively tackles complex low-level manipulation tasks.


% The original action space of the agent is a 6-dimensional vector ranging from -1.0 to 1.0 with an additional gripper state being 0.0 or 1.0. By modifying the pose representation, the robotic arm manipulation task involves a discretized 7-dimensional action space, where the first three dimensions (X, Y, Z) range from 0 to 100, while the next three (Pitch, Yaw, Gripper) range from 0 to 120. The gripper state remains binary (0.0 or 1.0). By discretizing the originally continuous action space, the model can predict actions using integer values, reducing complexity for LLMs.



% To assist the agent’s reasoning and decision-making on unseen tasks, in-context examples are provided. The number of examples varies based on task complexity: 2 examples for Pick \& Place, 4 examples for Stacking and Shape Sorting, and 8 examples for Table Wiping. 

% \paragraph{Observation and Feedback.}
At each step, the environment provides a front-view visual observation capturing a wooden table, a robotic arm positioned at the center corner, and multiple objects placed on the table. Each object is enclosed within a detection box labeled with a numerical index, sorted in descending order based on its Y-coordinate. A 3D XYZ coordinate system is displayed at the robot’s frame origin for spatial reference. The field of view (FOV) and image resolution are configurable, offering flexibility in visual input settings. Additionally, all visible objects in the scene are provided with discrete 3D coordinates, sorted in descending order based on their Y-coordinate, and labeled with object index (e.g., ``object 1"). This setup requires the agent to understand the correlation between objects mentioned in the instruction and their corresponding object indices. Using this position information, the agent can plan and execute action sequences to achieve the manipulation goal. To ensure validity, the environment evaluates each action, preventing constraint violations such as invalid trajectories or out-of-range movements. The validity signal serves as the sole feedback mechanism.

% The agent then plans and executes its next action based on this observation. To ensure validity, the environment evaluates each action, preventing constraint violations such as invalid trajectories or out-of-range movements. The validity signal serves as the sole feedback mechanism.
% making this system suitable for real-world deployment. Combined with visual input, this feedback provides sufficient information for effective robotic manipulation.

\paragraph{Dataset Collection. }For the base and spatial subsets, we select and curate samples from the VLMBench dataset. 
% The environment features multiple RGB-D cameras, including front, left, right, overhead, and wrist views. In this dataset, we utilize the front-view images with a resolution of $500 \times 500$ pixels. 
% The robotic arm follows motion planning to transition between waypoints, while the simulator records observations at a 50 ms time step. Each camera captures RGB-D images along with point clouds and instance masks, providing rich visual and spatial information.
% Additionally, the recorded observations include object poses at each time step and the correspondence between frames and waypoints. 
To generate instructions for each subset, we provide GPT-4o with 10 in-context examples.
The common sense, visual appearance, and complex instruction subsets are derived from the base subset, with modifications designed to assess specific capabilities. The visual appearance subset consists of 36 tasks, as the table wiping task is excluded due to the inability to distinguish objects based on appearance. Each of the remaining 4 subsets comprises 48 tasks evenly distributed across four categories, with 12 tasks per category. In total, EB-Manipulation consists of 228 testing instances.


\section{Model Versions}\label{ap:model_version}
Table \ref{tab:model_versions} lists the versions or full names of the models used in our experiments. We accessed proprietary models through API calls and open-source models via local deployment using \texttt{lmdeploy} \cite{2023lmdeploy}.


\begin{table}[h]
    \centering
    \begin{tabular}{c|cc}
    \toprule
        Model Name  & Creator & Full Name  \\
         \midrule
       GPT-4o   & OpenAI &  gpt-4o-2024-08-06 \\
       GPT-4o-mini & OpenAI & gpt-4o-mini-2024-07-18 \\
       Claude-3.5-Sonnet & Anthropic & claude-3-5-sonnet-20241022 \\
       Gemini-1.5-Pro & Google  & gemini-1.5-pro \\
       Gemini-2.0-flash  & Google & gemini-2.0-flash-exp \\
       Gemini-1.5-flash  & Google & gemini-1.5-flash \\
       Llama-3.2-90B-Vision-Ins & Meta & meta-llama/Llama-3.2-90B-Vision-Instruct \\
       Llama-3.2-11B-Vision-Ins & Meta & meta-llama/Llama-3.2-11B-Vision-Instruct \\
       InternVL2\_5-78B & OpenGVLab & OpenGVLab/InternVL2\_5-78B \\
       InternVL2\_5-38B  & OpenGVLab  & OpenGVLab/InternVL2\_5-38B \\
       InternVL2\_5-8B  & OpenGVLab & OpenGVLab/InternVL2\_5-8B \\
       Qwen2-VL-72B-Ins & Qwen & Qwen/Qwen2-VL-72B-Instruct \\
       Qwen2-VL-7B-Ins & Qwen & Qwen/Qwen2-VL-7B-Instruct \\
         \bottomrule
    \end{tabular}
    \caption{Full names of MLLMs used in our experiments.}
    \label{tab:model_versions}
\end{table}


\section{Definitions and Examples of Capability-oriented Subsets}
\label{ap:task_subset}
As listed in Table \ref{tb:subsets_complete}, we provide definitions and examples of six capability-oriented subsets in \name. 


\input{tables/subsets_appendix}




\clearpage
\section{Additional Experiment Results} \label{ap:additional_exp}
To thoroughly evaluate the performance of MLLMs as agents within \name, we present additional metric results, including subgoal success rate (Appendix \ref{ap:subgoal}) and average step counts (Appendix \ref{ap:planner_env_steps}), and conduct a series of ablation studies. These ablation studies, spanning from Appendix \ref{ap:camera_resolution} to Appendix \ref{ap:visual_icl}, focus on five critical factors: (1) varying camera resolutions, (2) the use of detection boxes, (3) multi-step images, (4) multi-view images, and (5) visual in-context learning. In the subsequent sections, we systematically analyze each of these factors, offering insights into their effectiveness and potential limitations.


\input{tables/subgoal_success_rate}
\subsection{Subgoal Success Rate}\label{ap:subgoal}
In addition to the task success rates presented in Table \ref{tb:high_level_table}, we further analyze the subgoal success rates for high-level tasks (EB-ALFRED and EB-Habitat), as detailed in Table \ref{tb:task_progress_high_level_table}. Given the use of symbolic expressions (Planning Domain Definition Language, PDDL) in high-level tasks, calculating subgoal success rates is straightforward. For instance, a task success condition can be expressed as ``condition A and condition B." Completing condition A alone results in a 50\% subgoal success rate, even though the final task success rate remains 0\%.

The results in Table \ref{tb:task_progress_high_level_table} generally align with those in Table \ref{tb:high_level_table}. For most models, the subgoal success rates are higher than their final task success rates, which is expected. Notably, Gemini-1.5-Pro achieves higher subgoal success rates than Claude-3.5-Sonnet on the EB-ALFRED benchmark, despite Gemini-1.5-Pro having a lower final task success rate. Additionally, GPT-4o demonstrates subgoal performance comparable to Claude-3.5, with a gap of less than 0.2 in both environments, despite a substantial gap in their final task success rates. These findings suggest that while models demonstrate better ability to achieve subgoals, completing the final task remains a significant challenge. Additionally, the capability to achieve subgoals may slightly differ from the ability to accomplish the entire task. Since our primary objective is to achieve the full task, future research should focus on developing strategies to improve the final task success rate.



\subsection{Average Planner and Environment Steps}\label{ap:planner_env_steps}
This section presents the results of average planner steps and environment steps, which quantify the number of model inferences and interactions with the environment, respectively. Since we employ a multi-step planning strategy, the number of environment steps exceeds that of planner steps. However, it is important to note that neither planner steps nor environment steps serve as precise metrics for evaluating agent performance, unlike the success rate. This is because the agent may generate empty plans or produce more than 10 invalid actions, potentially triggering early termination. Consequently, fewer steps do not always indicate superior planning performance. Nevertheless, meaningful insights can still be derived from Table \ref{tab:high_env_steps} and Table \ref{tab:low_env_steps}:
\begin{itemize}
    \item The multi-step planning strategy demonstrates significant efficiency in most cases, reducing average planner steps by around 50\% to 80\% compared to average environment steps. This is particularly evident in the EB-Manipulation task, where the average planner step for GPT-4o is 2.6, and the average environment step is 12.9, resulting in nearly 80\% fewer model inferences. This highlights the model's ability to generate long action sequences and we can effectively leverage this capability to minimize costs. Such efficiency is especially advantageous when utilizing expensive large proprietary MLLMs.
    \item Despite the inherent inaccuracies in average step counts, it is still possible to observe that more capable models tend to achieve smaller average planner and environment steps. For instance, Claude-3.5-Sonnet achieves the lowest planner and environment steps in both EB-ALFRED and EB-Habitat tasks, while GPT-4o records the lowest average planner and environment steps in EB-Manipulation. Additionally, larger models generally require fewer steps than their smaller counterparts, as evidenced by the comparison between GPT-4o and GPT-4o-mini, as well as Gemini-1.5-Pro and Gemini-2.0-Flash.
\end{itemize}




\begin{table}[th]
    \centering
    \renewcommand{\arraystretch}{1.2}  % Adjust row spacing for readability
    \begin{tabular}{c|cccc}  
    \toprule
    \textbf{Model} & \multicolumn{2}{c}{\textbf{EB-ALFRED}} & \multicolumn{2}{c}{\textbf{EB-Habitat}} \\
    \cmidrule(lr){2-5} 
    & \textbf{Avg Planner Step} & \textbf{Avg Env Steps} & \textbf{Avg Planner Step} & \textbf{Avg Env Steps} \\
    \midrule
    GPT-4o  & 4.4  & 16.3 & 5.5 & 13.1 \\
    GPT-4o-mini & 7.7  & 20.6 & 7.4 & 18.8 \\
    Claude-3.5-Sonnet  & 4.0  & 12.1 & 4.2 & 10.9 \\
    Gemini-1.5-Pro & 3.9  & 15.7  &  5.4 & 12.6 \\
    Gemini-2.0-flash & 4.4  & 16.3 & 6.8 & 14.8 \\
    Llama-3.2-90B-Vision-Ins & 7.3  & 16.7 & 7.3 & 16.2 \\
    InternVL2 5-78B & 5.5  & 13.9 & 6.3 & 14.1 \\
    InternVL2\_5-78B-MPO & 6.2 & 16.8 & 6.6 & 14.3 \\
    Qwen2-VL-72B-Ins & 6.1  & 13.7 & 6.8  & 14.2  \\
    \bottomrule
    \end{tabular}
    \caption{Average planner steps and environment steps in EB-ALFRED and EB-Habitat for different models.}
    \label{tab:high_env_steps}
\end{table}

\begin{table}[th]
    \centering
    \renewcommand{\arraystretch}{1.2}  % Adjust row spacing for readability
    \begin{tabular}{c|cccc}  
    \toprule
    \textbf{Model} & \multicolumn{2}{c}{\textbf{EB-Navigation}} & \multicolumn{2}{c}{\textbf{EB-Manipulation}} \\
    \cmidrule(lr){2-5} 
    & \textbf{Avg Planner Step} & \textbf{Avg Env Steps} & \textbf{Avg Planner Step} & \textbf{Avg Env Steps} \\
    \midrule
    GPT-4o & 6.2 & 15.5 & 2.6  & 12.9 \\
    GPT-4o-mini & 7.6 & 17.5 & 3.4  & 14.7 \\
    Claude-3.5-Sonnet & 6.2 & 15.6 & 2.7  & 13.3 \\
    Gemini-1.5-Pro & 8.8 & 16.5 & 2.7  & 13.4 \\
    Gemini-2.0-flash &9.2 & 16.0 & 2.8  & 14.0 \\
    Llama-3.2-90B-Vision-Ins & 7.5 & 17.4 & 3.0 & 13.9 \\
    InternVL2 5-78B & 13.2 & 17.3 & 2.9 & 13.5 \\
    InternVL2\_5-78B-MPO &6.2 &16.5 & 2.9 & 13.5 \\
    Qwen2-VL-72B-Ins &11.1 & 17.8 & 2.9 & 13.9 \\
    \bottomrule
    \end{tabular}
    \caption{Average planner steps and environment steps in EB-Navigation and EB-Manipulation for different models.}
    \label{tab:low_env_steps}
\end{table}

\subsection{Camera Resolution} \label{ap:camera_resolution}

As shown in Figure~\ref{fig:appendix_resolution}, we tested three camera resolutions—300×300, 500×500, and 700×700—on EB-ALFRED, EB-Manipulation, and EB-Navigation tasks. Our results reveal a task-dependent pattern: for EB-ALFRED, where vision plays a secondary role, increasing the resolution slightly improves performance for both GPT-4o and Claude-3.5-Sonnet, with accuracy gains of $2\% \sim 4\%$. In contrast, for EB-Manipulation and EB-Navigation, resolution is more critical, with the best performance achieved at 500×500. This suggests that while low-resolution images may lack the fine details needed for task execution, overly high resolutions can introduce unnecessary complexity, making it harder for MLLMs to focus on relevant information. These findings underscore the importance of choosing the right resolution when deploying MLLM-based embodied agents.


\begin{figure}[h!]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=1\linewidth]{pics/appendix_resolution.pdf}
\end{center}
\vspace{-1em}
\caption{Impact of different camera resolutions on \name.}
\label{fig:appendix_resolution}
\end{figure}

\subsection{Detection Boxes}

Figure~\ref{fig:low_level_detection} demonstrates the effect of using bounding boxes. The results show that detection boxes are helpful for EB-ALFRED and EB-Manipulation, improving object recognition and interaction for both GPT-4o and Claude-3.5-Sonnet. Notably, EB-Manipulation sees a significant improvement of nearly $10\%$. However, for EB-Navigation, bounding boxes tend to cause confusion, likely because they obscure spatial cues essential for path planning, resulting in lower success rates. This highlights the need to tailor visual augmentation techniques to the specific demands of each task. As a result, we only enable detection boxes by default for EB-Manipulation and exclude them for other task groups.

\begin{figure}[h!]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=1\linewidth]{pics/appendix_detection.pdf}
\end{center}
\vspace{-1em}
\caption{Impact of detection boxes on \name.}
\label{fig:low_level_detection}
\end{figure}

\subsection{Multi-step Images}


Using sequences of images is a common approach to address partial observation. As shown in Figure \ref{fig:ms_nav} and \ref{fig:ms_man}, observation images from the previous two environment steps are also included in addition to the planner’s original visual input. We explore the effectiveness of multi-step images—sequential frames shown in Figure~\ref{fig:low_level_multi_step}—where the latest three images are included as input. Surprisingly, adding temporal context does not improve decision-making; instead, it leads to a decline in performance, particularly for EB-Manipulation. This may be due to the models' struggle to interpret the relationship between multiple sequential images and their current state. These results emphasize the challenges of effectively utilizing temporal continuity in vision-language tasks.

\begin{figure}[h!]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.75\linewidth]{pics/ms-nav.pdf}
\end{center}
\vspace{-1em}
\caption{Multi-step observation example in EB-Navigation}
\label{fig:ms_nav}
\end{figure}

\begin{figure}[h!]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.75\linewidth]{pics/ms-man.pdf}
\end{center}
\vspace{-1em}
\caption{Multi-step observation example in EB-Manipulation}
\label{fig:ms_man}
\end{figure}


\begin{figure}[t]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.7\linewidth]{pics/appendix_multi_step.pdf}
\end{center}
\vspace{-1em}
\caption{Impact of multi-step images on \name.}
\label{fig:low_level_multi_step}
\end{figure}

\subsection{Multi-view Images}

In addition to multi-step images, another approach is to incorporate multi-view images from different cameras at the same time step. As shown in Figure \ref{fig:mv_low_level}, the planner receives images from two different viewpoints as input. For EB-Navigation, the input consists of a front-view image and a top-down view image. For EB-Manipulation, the planner receives a front-view image and a wrist-view image. To evaluate whether multi-view images enhance performance in EB-Manipulation and EB-Navigation, we present the results in Figure~\ref{fig:low_level_multi_view}. Surprisingly, using multi-view data also results in a performance decline, particularly for GPT-4o. While multiple viewpoints theoretically offer richer spatial context, GPT-4o and Claude-3.5-Sonnet seem to struggle with effectively integrating and leveraging these additional perspectives. This limitation may arise from challenges in multi-view feature fusion or the increased complexity of the input.

\begin{figure}[th]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=1\linewidth]{pics/mv.pdf}
\end{center}
\vspace{-1em}
\caption{Multi-view observation example in EB-Navigation (left) and EB-Manipulation (right).}
\label{fig:mv_low_level}
\end{figure}


\begin{figure}[h!]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.7\linewidth]{pics/low_level_multi_view.pdf}
\end{center}
\vspace{-1.5em}
\caption{Impact of multi-view images on \name.}
% \vspace{-0.5em}
\label{fig:low_level_multi_view}
\end{figure}

\subsection{Visual In-context Learning (ICL)}\label{ap:visual_icl}


Previous research has mainly focused on text-based in-context learning (ICL) demonstrations. In this study, we explore the impact of visual ICL for embodied agents by including image observations as part of the in-context examples for EB-Manipulation. This approach helps the model better grasp the connection between successful low-level actions and the positions of objects in the image. We provide two visual ICL examples in Figure \ref{fig:visual_icl_examples}, where the planner receives images corresponding to the textual in-context examples. To avoid overwhelming the model with excessive visual input, we limit the number of examples to two, which might slightly reduce performance compared to the main results without visual ICL.
As illustrated in Figure \ref{fig:in_context_learning}, the results show that visual ICL significantly outperforms language-only ICL, with particularly impressive gains in manipulation tasks. For instance, Claude-3.5-Sonnet achieves a 16.7\% improvement in performance. These findings highlight the potential of visual ICL as a promising direction for future research in vision-driven embodied agents.

\begin{figure}[h!]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.8\linewidth]{pics/icl.pdf}
\end{center}
\vspace{-1em}
\caption{Visual in-context learning examples for EB-Navigation \& EB-Manipulation}
\label{fig:visual_icl_examples}
\end{figure}

\begin{figure}[h!]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.7\linewidth]{pics/low_level_ICL.pdf}
\end{center}
\vspace{-1.5em}
\caption{Impact of visual in-context learning on \name.}
% \vspace{-0.5em}
\label{fig:in_context_learning}
\end{figure}


\subsection{Additional Ablation Study Conclusion}
Overall, our ablation studies reveal that while certain visual enhancements—such as moderate resolution increases, bounding-box detection, and visual in-context learning—can be beneficial, others—like extreme high-resolution inputs, multi-step/multi-view images, or detection boxes for navigation—may have limited or even negative effects. These findings highlight that the effectiveness of visual strategies heavily depends on the specific task and how additional visual information is integrated. Future research should focus on developing more advanced fusion techniques for embodied agents to better optimize the use of diverse visual inputs from multiple images.







% \clearpage
\section{Error Definitions and Additional Analysis}\label{appendix_error}
\subsection{Error Type Definition}
In this section, we define the types of errors and sub-errors encountered. We categorize errors into three main types: perception errors, reasoning errors, and planning errors. Each error type corresponds to a specific stage in our agent pipeline. For example, perception errors occur during the visual state description stage, reasoning errors arise in the reflection and reasoning stages, and planning errors occur during the language plan and executable plan generation stages. A detailed breakdown of sub-errors for each error type is provided in Table \ref{tab:error_types}.

\begin{table}[h!]
\centering
\begin{tabular}{p{5cm}|p{10cm}}
\toprule
\textbf{Error Type} & \textbf{Definition} \\
\midrule

\multicolumn{2}{c}{\textbf{Perception Errors}} \\
\midrule
\makecell[l]{\textit{Hallucination}} & Perceiving objects or attributes that are not present in the visual input \\
\makecell[l]{\textit{Wrong Recognition}} & Incorrectly identifying object types or attributes \\
\makecell[l]{\textit{Spatial Understanding}} & Misjudging object positions / depths in 3D space \\

\midrule
\multicolumn{2}{c}{\textbf{Reasoning Errors}} \\ 
\midrule
\makecell[l]{\textit{Spatial Reasoning}} & Failure to understand / reason about spatial relationships \\
\makecell[l]{\textit{Insufficient Exploration}} & Only giving a suboptimal exploration strategy through reasoning \\
\makecell[l]{\textit{Wrong Termination Decision}} & Ending task execution before completing the goal \\
\makecell[l]{\textit{Reflection Error}} & Failing to realize previous errors or adapt plans using environmental feedback \\

\midrule
\multicolumn{2}{c}{\textbf{Planning Errors}} \\
\midrule
\makecell[l]{\textit{Inaccurate Action}} & Executing actions with incorrect parameters / poses \\
\makecell[l]{\textit{Missing Steps}} & Omitting necessary actions in sequential plans \\
\makecell[l]{\textit{Invalid Action}} & Attempting physically impossible interactions \\
\makecell[l]{\textit{Action ID Mismatch}} & Misaligning action names with wrong action IDs \\

\bottomrule
\end{tabular}
\caption{Error Taxonomy with Definitions}
\label{tab:error_types}
\end{table}



\subsection{Error Analysis for EB-Navigation}
\begin{figure}[h!]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.5\linewidth]{pics/error_analysis_nav.pdf}
\end{center}
\vspace{-1.5em}
\caption{Error Analysis on EB-Navigation.}
% \vspace{-0.5em}
\label{fig:appendix_error_analysis}
\end{figure}
In evaluating the performance of MLLMs on navigation tasks, we identified three main types of errors: perception errors, reasoning errors, and planning errors. These errors significantly hinder the model's ability to successfully navigate to the target object.

\paragraph{Perception Errors.}
The first category involves the model's ability to interpret visual observations and recognize the spatial position of the target object. We observed two common failure patterns:  
\textbf{(1) Wrong Recognition}: In some cases, the model failed to identify the target object even when it was present in the visual input. This suggests limitations in object recognition, possibly due to inadequate feature extraction or attention mechanisms.  
\textbf{(2)Hallucination of the Target Object}: In other instances, the model incorrectly claimed to have detected the target object when it was not actually present. This issue is particularly problematic, as it leads to premature conclusions and incorrect decisions instead of further exploration. Ideally, the model should acknowledge its inability to locate the target and continue navigating appropriately.

% The first category of errors pertains to the model's ability to correctly interpret the visual observations and recognize the spatial position of the target object. Within this category, we observed two predominant failure patterns: \textbf{(1) Wrong recognition}: In several instances, the model was provided with a visual input containing the target object but failed to identify or recognize it. This suggests limitations in object recognition capabilities, potentially stemming from insufficient feature extraction or suboptimal attention mechanisms.
% \textbf{(2) Hallucination of the Target Object}: In other cases, the model falsely claimed to have detected the target object in the absence of its actual presence within the given observation. This hallucination issue is particularly problematic, as it leads to incorrect decision-making instead of making further exploration to locate the actual target. Ideally, the model should remain honest about its current inability to locate the target objects\cite{rtuning} and continue navigating appropriately rather than prematurely concluding that it has reached the destination.

\paragraph{Reasoning Errors.}
The second category arises from the model's limitations in reasoning. This problem appears in two main ways: flawed logical reasoning about possible paths, even when visual observations are accurate; and weak reflection on spatial structure after failed attempts or feedback from previous steps. These issues indicate a lack of strong 3D spatial reasoning. The model often struggles to build a coherent 3D representation from sequential 2D observations, resulting in poor movement decisions. Furthermore, the model is not good at refining its actions based on ongoing feedback.

\paragraph{Planning Errors.} Even when the model correctly identified the general direction, it often struggled with movement precision (``inaccurate actions"). For instance, it might overshoot the target by taking three steps instead of two. This highlights problems with spatial quantification, as the model's distance estimation and movement execution frequently did not align with real-world needs. 


These main categories of errors reveal significant limitations in the navigation capabilities of current MLLM-based agents. To address these challenges, improvements are needed in several areas: strengthening object recognition, reducing hallucinations, enhancing 3D spatial reasoning, and aligning the model output with the action space to generate accurate plans. These advancements would enable more reliable and efficient autonomous navigation.



\subsection{Format Errors}
In addition to the aforementioned error types, we also observe output format errors when generating JSON. Specifically, smaller-scale models can fail to produce valid JSON files. Table \ref{tab:format_error} shows the number of format errors across all six subsets, revealing that even proprietary models are not immune to these errors. A clear trend emerges: \textbf{larger models tend to have fewer format errors, while smaller models are more prone to such issues}. This highlights the need for further alignment of small models to improve the accuracy of structured outputs. Additionally, EB-ALFRED exhibits a higher number of errors, largely due to its greater complexity and the increased number of action steps in a trajectory.

\begin{table}[h]
    \centering
    \begin{tabular}{c|cc}
    \toprule
    Model Name & EB-ALFRED   &  EB-Habitat \\
    \midrule
       GPT-4o  & 0.0067 & 0.0067 \\
       GPT-4o-mini &  0.0867 & 0.0200 \\
       Claude-3.5-Sonnet  &  1.5400  & 0.0000 \\
       Gemini-1.5-Pro &  0.0000 & 0.0000 \\
       Gemini-2.0-flash & 0.0000 & 0.0000 \\
       Llama-3.2-90B-Vision-Ins &  1.8033 & 0.0233 \\
       Llama-3.2-11B-Vision-Ins &  1.6767  & 1.3233 \\
       InternVL2 5-78B & 1.1467 & 0.0000 \\
       InternVL2 5-38B & 2.4600 & 1.7433 \\
       InternVL2 5-8B & 8.0400 & 4.3967 \\
       Qwen2-VL-72B-Ins & 1.6100 & 0.0767 \\
       Qwen2-VL-7B-Ins &  1.6933 & 0.7067 \\
       \bottomrule
    \end{tabular}
    \caption{Format error number per trajectory in EB-ALFRED and EB-Habitat across all subsets.}
    \label{tab:format_error}
\end{table}



\newpage
\section{Input of the Vision-driven Embodied Agent}
\label{ap:planner_input_examples}
\subsection{Prompts}
\label{ap:prompts}
We provide the agent input prompts used as textual input to MLLMs for all four environments. 
\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=Prompt for EB-ALFRED, boxrule=0.3mm, width=\textwidth, arc=3mm, auto outer arc=true]

\#\# You are a robot operating in a home. Given a task, you must accomplish the task using a defined set of actions to achieve the desired outcome.
\newline
\#\# Action Descriptions and Validity Rules
\newline
• Find: Parameterized by the name of the receptacle to navigate to. So long as the object is present in the scene, this skill is always valid.
\newline
• Pick up: Parameterized by the name of the object to pick. Only valid if the robot is close to the object, not holding another object, and the object is not inside a closed receptacle.
\newline
• Put down: Parameterized by the name of the object to put down to a nearby receptacle. Only valid if the robot is holding an object.
\newline
• Drop: Parameterized by the name of the object to put down. It is different from the Put down action, as this does not guarantee the held object will be put into a specified receptacle. 
\newline
• Open: Parameterized by the name of the receptacle to open. Only valid if the receptacle is closed and the robot is close to the receptacle.
\newline
• Close: Parameterized by the name of the receptacle to close. Only valid if the receptacle is open and the robot is close to the receptacle.
\newline
• Turn on: Parameterized by the name of the object to turn on. Only valid if the object is turned off and the robot is close to the object.
\newline
• Turn off: Parameterized by the name of the object to turn off. Only valid if the object is turned on and the robot is close to the object.
\newline
• Slice: Parameterized by the name of the object to slice. Only valid if the object is sliceable and the robot is close to the object.
\newline
\#\# The available action id (0 - \{len(SKILL SET) - 1\}) and action names are: \{SKILL SET\}.
\newline
\newline
\{ICL EXAMPLES\}
\newline
\newline
\#\# Guidelines
\newline
1. **Output Plan**: Avoid generating empty plan. Each plan should include no more than 20 actions.\newline
2. **Visibility**: Always locate a visible object by the 'find' action before interacting with it.\newline
3. **Action Guidelines**: Make sure to match the action name and its corresponding action id in the output.
\newline
Avoid performing actions that do not meet the defined validity criteria. For instance, if you want to put an object in a receptacle, use 'put down' rather than 'drop' actions. \newline
4. **Prevent Repeating Action Sequences**: Do not repeatedly execute the same action or sequence of actions.
\newline
Try to modify the action sequence because previous actions do not lead to success.\newline
5. **Multiple Instances**: There may be multiple instances of the same object, distinguished by an index following their names, e.g., Cabinet\_2, Cabinet\_3. You can explore these instances if you do not find the desired object in the current receptacle.\newline
6. **Reflection on History and Feedback**: Use interaction history and feedback from the environment to refine and improve your current plan.
\newline
If the last action is invalid, reflect on the reason, such as not adhering to action rules or missing preliminary actions, and adjust your plan accordingly.
\newline
\newline
\{ACTION HISTORY \& ENVIRONMENT FEEDBACK (if available)\}
\newline
\newline
\#\# Now the human instruction is: \{TASK INSTRUCTION\} You are supposed to output in json. You need to describe the current visual state from the image, output your reasoning steps, and plan. At the end, output the action id (0 - \{len(SKILL SET) - 1\}) from the available actions to execute.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=Prompt for EB-Habitat, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc=true]

\#\# You are a robot operating in a home. Given a task, you must accomplish the task using a defined set of actions to achieve the desired outcome.
\newline
\newline
\#\# Action Descriptions and Validity Rules\newline
• Navigation: Parameterized by the name of the receptacle to navigate to. So long as the receptacle is present in the scene, this skill is always valid.\newline
• Pick: Parameterized by the name of the object to pick. Only valid if the robot is close to the object, not holding another object, and the object is not inside a closed receptacle.\newline
• Place: Parameterized by the name of the receptacle to place the object on. Only valid if the robot is close to the receptacle and is holding an object.\newline
• Open: Parameterized by the name of the receptacle to open. Only valid if the receptacle is closed and the robot is close to the receptacle.\newline
• Close: Parameterized by the name of the receptacle to close. Only valid if the receptacle is open and the robot is close to the receptacle.\newline
\newline
\#\# The available action id (0 - 69) and action names are: \{SKILL SET\}.
\newline
\newline
\{ICL EXAMPLES\}
\newline
\newline
\#\# Guidelines
\newline
1. **Output Plan**: Avoid generating empty plan. Each plan should include no more than 20 actions.\newline
2. **Visibility**: If an object is not currently visible, use the "Navigation" action to locate it or its receptacle before attempting other operations.\newline
3. **Action Validity**: Make sure to match the action name and its corresponding action id in the output.\newline Avoid performing actions that do not meet the defined validity criteria. \newline
4. **Prevent Repeating Action Sequences**: Do not repeatedly execute the same action or sequence of actions.\newline Try to modify the action sequence because previous actions do not lead to success.\newline
5. **Multiple Instances**: There may be multiple instances of the same object, distinguished by an index following their names, e.g., cabinet 2, cabinet 3. You can explore these instances if you do not find the desired object in the current receptacle.\newline
6. **Reflection on History and Feedback**: Use interaction history and feedback from the environment to refine and enhance your current strategies and actions. If the last action is invalid, reflect on the reason, such as not adhering to action rules or missing preliminary actions, and adjust your plan accordingly.
\newline
\newline
\{ACTION HISTORY \& ENVIRONMENT FEEDBACK (if available)\}
\newline
\newline
\#\# Now the human instruction is: \{TASK INSTRUCTION\} You are supposed to output in json. You need to describe the current visual state from the image, output your reasoning steps, and plan. At the end, output the action id (0 - 69) from the available actions to execute.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=Prompt for EB-Navigation at step 0, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc=true]

\#\# You are a robot operating in a home. You can do various tasks and output a sequence of actions to accomplish a given task with images of your status.\newline
\newline
\#\# The available action id (0 - 7) and action names are: \newline
action id 0: Move forward by 0.25,\newline 
action id 1: Move backward by 0.25,\newline 
action id 2: Move rightward by 0.25,\newline 
action id 3: Move leftward by 0.25,\newline 
action id 4: Rotate to the right by 90 degrees,\newline 
action id 5: Rotate to the left by 90 degrees,\newline 
action id 6: Tilt the camera upward by 30 degrees, \newline
action id 7: Tilt the camera downward by 30 degrees\newline
\newline
*** Strategy ***\newline
\newline
1. Locate the Target Object Type: Clearly describe the spatial location of the target object 
from the observation image (i.e. on the front left side, a few steps from the current standing point).\newline
\newline
2. Navigate by *** Using Move forward and Move right/left as the main strategy ***, since any point can be reached through a combination of those. 
When planning for movement, reason based on target object's location and obstacles around you. \newline
\newline
3. Focus on the primary goal: Only address invalid action when it blocks you from moving closer in the direction to target object. In other words, 
do not overly focus on correcting invalid actions when direct movement toward the target object can still bring you closer. \newline
\newline
4. *** Use Rotation Sparingly ***, only when you lose track of the target object and it's not in your view. If so, plan nothing but ONE ROTATION at a step until that object appears in your view. After the target object appears, start navigation and avoid using rotation until you lose sight of the target again.\newline
\newline
5. *** Do not complete the task too early until you can not move any closer to the object, i.e. try to be as close as possible.\newline
\newline
\{ICL EXAMPLES\}\newline
\newline
\#\# Now the human instruction is: \{TASK INSTRUCTION\}. To achieve the task, 1. Reason about the current visual state and your final goal, and 2. Reflect on the effect of previous actions. 3. Summarize how you learned from the Strategy and Examples provided. \newline 
Aim for about 2 actions in this step. !!!Notice: You cannot assess the situation until the whole plan in this planning step is finished and executed, so plan accordingly. \newline
At last, output the action id(s) (0 - 7) from the available actions to execute. \newline
\newline
The input given to you is a first-person view observation. Plan accordingly based on the visual observation.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=Prompt for EB-Navigation at remaining steps, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc=true]

\#\# You are a robot operating in a home. You can do various tasks and output a sequence of actions to accomplish a given task with images of your status.\newline
\newline
\#\# The available action id (0 - 7) and action names are: \newline
action id 0: Move forward by 0.25,\newline 
action id 1: Move backward by 0.25,\newline 
action id 2: Move rightward by 0.25,\newline 
action id 3: Move leftward by 0.25,\newline 
action id 4: Rotate to the right by 90 degrees,\newline 
action id 5: Rotate to the left by 90 degrees,\newline 
action id 6: Tilt the camera upward by 30 degrees, \newline
action id 7: Tilt the camera downward by 30 degrees\newline
\newline
*** Strategy ***\newline
\newline
1. Locate the Target Object Type: Clearly describe the spatial location of the target object 
from the observation image (i.e. on the front left side, a few steps from the current standing point).\newline
\newline
2. Navigate by *** Using Move forward and Move right/left as the main strategy ***, since any point can be reached through a combination of those. 
When planning for movement, reason based on target object's location and obstacles around you. \newline
\newline
3. Focus on the primary goal: Only address invalid action when it blocks you from moving closer in the direction to target object. In other words, 
do not overly focus on correcting invalid actions when direct movement toward the target object can still bring you closer. \newline
\newline
4. *** Use Rotation Sparingly ***, only when you lose track of the target object and it's not in your view. If so, plan nothing but ONE ROTATION at a step until that object appears in your view. After the target object appears, start navigation and avoid using rotation until you lose sight of the target again.\newline
\newline
5. *** Do not complete task too early until you can not move any closer to the object, i.e. try to be as close as possible.\newline
\newline
\{ICL EXAMPLES\}\newline
\newline
\#\# Now the human instruction is: \{TASK INSTRUCTION\}. \newline 
\newline 
\{ACTION HISTORY \& ENVIRONMENT FEEDBACK (if available)\}\newline
\newline 
To achieve the task, 1. Reason about the current visual state and your final goal, and 2. Reflect on the effect of previous actions. 3. Summarize how you learned from the Strategy and Examples provided. \newline 
Aim for about 5-6 actions in this step to be closer to the target object. !!!Notice: You cannot assess the situation until the whole plan in this planning step is finished and executed, so plan accordingly.\newline 
At last, output the action id(s) (0 - 7) from the available actions to execute. \newline 
\newline 
The input given to you is a first-person view observation. Plan accordingly based on the visual observation.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=Prompt for EB-Manipulation, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc=true]

\#\# You are a Franka Panda robot with a parallel gripper. You can perform various tasks and output a sequence of gripper actions to accomplish a given task with images of your status. The input space, output action space, and color space are defined as follows:\newline
\newline
** Input Space **\newline
- Each input object is represented as a 3D discrete position in the following format: [X, Y, Z]. \newline
- There is a red XYZ coordinate frame located in the top-left corner of the table. The X-Y plane is the table surface. \newline
- The allowed range of X, Y, Z is [0, 100]. \newline
- Objects are ordered by Y in ascending order.\newline
\newline
** Output Action Space **\newline
- Each output action is represented as a 7D discrete gripper action in the following format: [X, Y, Z, Roll, Pitch, Yaw, Gripper].\newline
- X, Y, Z are the 3D discrete positions of the gripper in the environment. It follows the same coordinate system as the input object coordinates.\newline
- The allowed range of X, Y, Z is [0, 100].\newline
- Roll, Pitch, and Yaw are the 3D discrete orientations of the gripper in the environment, represented as discrete Euler Angles. \newline
- The allowed range of Roll, Pitch, and Yaw is [0, 120] and each unit represents 3 degrees.\newline
- Gripper state is 0 for close and 1 for open.\newline
\newline
** Color space **\newline
- Each object can be described using one of the colors below:\newline
  [\texttt{"red"}, \texttt{"maroon"}, \texttt{"lime"}, \texttt{"green"}, \texttt{"blue"}, \texttt{"navy"}, \texttt{"yellow"}, \texttt{"cyan"}, \texttt{"magenta"}, \texttt{"silver"}, \texttt{"gray"}, \texttt{"olive"}, \texttt{"purple"}, \texttt{"teal"}, \texttt{"azure"}, \texttt{"violet"}, \texttt{"rose"}, \texttt{"black"}, \texttt{"white"}],\newline
\newline
Below are some examples to guide you in completing the task. \newline
\newline
\{ICL EXAMPLES\}
\newline
\newline
\#\# Now you are supposed to follow the above examples to generate a sequence of discrete gripper actions that completes the below human instruction. \newline
Human Instruction: \{TASK INSTRUCTION\} \newline
Input: \{TASK INPUT\} \newline
Output gripper actions: \{ACTION HISTORY \& ENVIRONMENT FEEDBACK (if available)\}
\end{tcolorbox}

\clearpage
\subsection{Skill Sets}
Below are the skill sets for EB-ALFRED and EB-Habitat. Note that the objects for EB-ALFRED vary depending on the scene, and the example provided here is illustrative. In contrast, the skill set for EB-Habitat remains static.  

The skill sets (or action spaces) for EB-Navigation and EB-Manipulation are already included in the planner input prompt. For detailed prompts, please refer to Appendix \ref{ap:prompts}.

\begin{table}[h!]
\centering
\begin{tabular}{p{2cm}|p{12cm}}
\toprule
\textbf{Action Type} & \textbf{Target Object} \\
\midrule
{\centering Find} & AlarmClock, Apple, Apple\_2, Apple\_3, ArmChair, BasketBall, Bathtub, Bed, Book, Box, Bread, Bread\_2, ButterKnife, ButterKnife\_2, Cabinet, Cabinet\_10, Cabinet\_2, Cabinet\_3, Cabinet\_4, Cabinet\_5, Cabinet\_6, Cabinet\_7, Cabinet\_8, Cabinet\_9, Candle, Cart, CellPhone, CD, Chair, Cloth, CoffeeMachine, CoffeeTable, CounterTop, CounterTop\_2, CounterTop\_3, CreditCard, Cup, Cup\_2, Cup\_3, Desk, DeskLamp, DishSponge, DishSponge\_2, DiningTable, Dresser, Drawer, Drawer\_2, Drawer\_3, Drawer\_4, Drawer\_5, Drawer\_6, Egg, Faucet, FloorLamp, Fork, Fork\_2, Fork\_3, Fridge, GarbageCan, Glassbottle, HandTowel, Kettle, Kettle\_2, Kettle\_3, KeyChain, Knife, Knife\_2, Ladle, Laptop, Lettuce, Lettuce\_2, Microwave, Mug, Mug\_2, Mug\_3, Newspaper, Ottoman, Pan, Pan\_2, Pan\_3, PepperShaker, PepperShaker\_2, PepperShaker\_3, Pencil, Pen, Pillow, Plate, Plunger, Potato, Potato\_2, RemoteControl, Safe, SaltShaker, SaltShaker\_2, Shelf, SideTable, Sink, SoapBar, SoapBottle, SoapBottle\_2, Sofa, Spatula, Spatula\_2, SprayBottle, Statue, StoveBurner, StoveBurner\_2, StoveBurner\_3, StoveBurner\_4, TennisRacket, TissueBox, Tomato, Toilet, ToiletPaper, ToiletPaperHanger, Vase, Watch, WateringCan, WineBottle \\
\midrule
{\centering Pick up} & AlarmClock, Apple, BaseballBat, BasketBall, Book, Bowl, Box, Bread, ButterKnife, Candle, CD, CellPhone, Cloth, CreditCard, Cup, DishSponge, Egg, Fork, Glassbottle, HandTowel, Kettle, KeyChain, Knife, Ladle, Laptop, Lettuce, Mug, Newspaper, Pan, Pen, Pencil, PepperShaker, Plate, Plunger, Potato, RemoteControl, SaltShaker, SoapBar, SoapBottle, Spatula, SprayBottle, Spoon, Statue, TennisRacket, TissueBox, Tomato, Vase, Watch, WateringCan, WineBottle \\
\midrule
{\centering Put down} & Object in hand \\
\midrule
{\centering Drop} & Object in hand \\
\midrule
{\centering Open} & Box, Cabinet, Cabinet\_10, Cabinet\_2, Cabinet\_3, Cabinet\_4, Cabinet\_5, Cabinet\_6, Cabinet\_7, Cabinet\_8, Cabinet\_9, Drawer, Drawer\_2, Drawer\_3, Drawer\_4, Drawer\_5, Drawer\_6, Fridge, Laptop, Microwave, Safe \\
\midrule
{\centering Close} & Box, Cabinet, Cabinet\_10, Cabinet\_2, Cabinet\_3, Cabinet\_4, Cabinet\_5, Cabinet\_6, Cabinet\_7, Cabinet\_8, Cabinet\_9, Drawer, Drawer\_2, Drawer\_3, Drawer\_4, Drawer\_5, Drawer\_6, Fridge, Laptop, Microwave, Safe \\
\midrule
{\centering Turn on} & DeskLamp, Faucet, FloorLamp, Microwave \\
\midrule
{\centering Turn off} & DeskLamp, Faucet, FloorLamp, Microwave \\
\midrule
{\centering Slice} & Apple, Bread, Lettuce, Potato, Tomato \\
\bottomrule
\end{tabular}
\caption{The skill set for EB-ALFRED}
\label{tab:eb-alfred}
\end{table}


\begin{table}[h]
\centering
\begin{tabular}{p{2cm}|p{12cm}}
\toprule
\textbf{Action Type} & \textbf{Target Object} \\
\midrule
{\centering Navigate to} & Cabinet 4, Cabinet 5, Cabinet 6, Cabinet 7, Chair 1, Left counter in the kitchen, Left drawer of the kitchen counter, Refrigerator, Refrigerator push point, Right counter in the kitchen, Right drawer of the kitchen counter, Sink in the kitchen, Sofa, Table 1, Table 2, TV stand \\
\midrule
{\centering Pick up} & Apple, Ball, Banana, Block, Book, Bowl, Box, Can, Clamp, Cleanser, Cup, Drill, Hammer, Knife, Lego, Lemon, Lid, Mug, Orange, Padlock, Peach, Pear, Plate, Plum, Rubik’s cube, Scissors, Screwdriver, Spatula, Spoon, Sponge, Strawberry, Toy airplane, Wrench \\
\midrule
{\centering Place at} & Chair 1, Left counter in the kitchen, Left drawer of the kitchen counter, Refrigerator, Right counter in the kitchen, Right drawer of the kitchen counter, Sink in the kitchen, Sofa, Table 1, Table 2, TV stand \\
\midrule
{\centering Open} & Cabinet 4, Cabinet 5, Cabinet 6, Cabinet 7, Refrigerator \\
\midrule
{\centering Close} & Cabinet 4, Cabinet 5, Cabinet 6, Cabinet 7, Refrigerator \\
\bottomrule
\end{tabular}
\caption{The skill set for EB-Habitat}
\label{tab:prompt}
\end{table}


\clearpage
\subsection{In-context examples}
The in-context examples provided to the agent are detailed below. Each environment includes one to ten representative examples, with the complete set in our source code.

Specifically, \textbf{EB-ALFRED} utilizes 10 examples from the training set of the original ALFRED dataset for all subsets except the long-horizon subsets. We make sure the 10 examples diversely cover all 7 task types of ALFRED. For the long-horizon subsets, we include 7 different demonstrations closely related to the task types ``pick\_heat\_then\_place\_in\_recep" and "pick\_coll\_then\_place\_in\_recep." It’s important to note that the in-context examples do not include scenarios with multiple instances of the same object. Additionally, we ensure the examples do not involve variable action indices (i.e., for actions with IDs below 162, object names remain static without suffixes, while actions with IDs 162 or higher involve changing objects). 

Similarly, \textbf{EB-Habitat} features 10 demonstrations from the 'train' set of the original Language Rearrangement dataset \cite{szot2023large} across all subsets. For \textbf{EB-Navigation}, we provide 3 demonstrations for all subsets, ensuring they are distinct from the testing instances.
In \textbf{EB-Manipulation}, the number of examples varies depending on task complexity: 2 examples for Pick \& Place, 4 examples for Stacking and Shape Sorting, and 8 examples for Table Wiping. All demonstrations are carefully selected to differ from the testing instances.



\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title={In-context examples for EB-ALFRED tasks excluding long horizon tasks}, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc=true]
\textbf{\#\# Example 1:}\\
Human instruction: Pick up the alarm clock and turn on the lamp.\\
Output:
\lstset{frame=none} % Removes bounding box from JSON
\begin{lstlisting}[language=json]
{
  "language_plan": "First, locate the alarm clock in the room. Once found, carefully pick up the alarm clock. Next, identify the location of the desk lamp. After locating the desk lamp, proceed to turn it on.",
  "executable_plan": [
    {"action_id": 57, "action_name": "find a AlarmClock" },
    {"action_id": 98, "action_name": "pick up the AlarmClock" },
    {"action_id": 63, "action_name": "find a DeskLamp" },
    {"action_id": 151, "action_name": "turn on the DeskLamp" }
  ]
}
\end{lstlisting}
\textbf{\#\# Example 2:}\\
Human instruction: Examine a credit card by the light of a tall lamp.\\
Output:
\begin{lstlisting}[language=json]
{
  "language_plan": "Start by locating the credit card in the room. Once you have found the credit card, proceed to pick it up. Then, search for the floor lamp and find its location. After finding the floor lamp, turn it on to illuminate the area so you can examine the credit card effectively.",
  "executable_plan": [
    {"action_id": 56, "action_name": "find a CreditCard" },
    {"action_id": 96, "action_name": "pick up the CreditCard" },
    {"action_id": 27, "action_name": "find a FloorLamp" },
    {"action_id": 153, "action_name": "turn on the FloorLamp" }
  ]
}
\end{lstlisting}
\textbf{\#\# Example 3:}\\
Human instruction: Use lamp light to look at the remote.\\
Output:
\begin{lstlisting}[language=json]
{
  "language_plan": "First, locate the remote control in the room. Once the remote control is found, pick it up carefully. Next, search for the floor lamp and determine its location. After you have located the floor lamp, turn it on to provide sufficient light for examining the remote control.",
  "executable_plan": [
    {"action_id": 68, "action_name": "find a RemoteControl" },
    {"action_id": 116, "action_name": "pick up the RemoteControl" },
    {"action_id": 27, "action_name": "find a FloorLamp" },
    {"action_id": 153, "action_name": "turn on the FloorLamp" }
  ]
}
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=In-context example for EB-ALFRED long horizon tasks, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc=true]
\textbf{\#\# Example 1:}\\
Human instruction: Place the microwaved slice of apple on to the white table.\\
Output:
\begin{lstlisting}[language=json]
{
  "language_plan": "Locate an apple and pick it up, find a dining table and put the apple down, look for a knife and pick it up, find the apple and slice it. After slicing, I need to find a place like the dining table to put the knife down. After that, find an apple slice and pick it up, search for a microwave and open it, put the apple in the microwave and close it, turn on the microwave to heat the apple slice, turn off the microwave and open it, pick up the microwaved apple slice, close the microwave, find the white table, and place the microwaved apple slice on the white table.",
  "executable_plan": [
    {"action_id": 22, "action_name": "find a Apple" },
    {"action_id": 107, "action_name": "pick up the Apple" },
    {"action_id": 18, "action_name": "find a DiningTable" },
    {"action_id": 133, "action_name": "put down the object in hand" },
    {"action_id": 34, "action_name": "find a Knife" },
    {"action_id": 127, "action_name": "pick up the Knife" },
    {"action_id": 22, "action_name": "find a Apple" },
    {"action_id": 160, "action_name": "slice the Apple" },
    {"action_id": 18, "action_name": "find a DiningTable" },
    {"action_id": 133, "action_name": "put down the object in hand" },
    {"action_id": 22, "action_name": "find a Apple" },
    {"action_id": 107, "action_name": "pick up the Apple" },
    {"action_id": 38, "action_name": "find a Microwave" },
    {"action_id": 143, "action_name": "open the Microwave" },
    {"action_id": 133, "action_name": "put down the object in hand" },
    {"action_id": 144, "action_name": "close the Microwave" },
    {"action_id": 149, "action_name": "turn on the Microwave" },
    {"action_id": 150, "action_name": "turn off the Microwave" },
    {"action_id": 143, "action_name": "open the Microwave" },
    {"action_id": 22, "action_name": "find a Apple" },
    {"action_id": 107, "action_name": "pick up the Apple" },
    {"action_id": 144, "action_name": "close the Microwave" },
    {"action_id": 18, "action_name": "find a DiningTable" },
    {"action_id": 133, "action_name": "put down the object in hand" }
  ]
}
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=In-context examples for EB-Habitat, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc=true]
\textbf{\#\# Example 1:}\\
Human instruction: Put both a toy airplane and a bowl onto the black table.\\
Output:
\begin{lstlisting}[language=json]
{
  "language_plan": "To achieve the goal, the robot must locate the toy airplane and the bowl in the room and then move each item to the black table. There are two tables in the room, and the robot identifies table 1 as the black table. Therefore, the objective is to place both objects on table 1. The plan is as follows: first, navigate to the sofa, pick up the airplane, move to table 1, and place the airplane there. Then, proceed to table 2, where the bowl might be, pick up the bowl, return to table 1, and set the bowl there.",
  "executable_plan": [
    {"action_id": 12, "action_name": "navigate to the sofa" },
    {"action_id": 47, "action_name": "pick up the toy airplane" },
    {"action_id": 6, "action_name": "navigate to table 1" },
    {"action_id": 50, "action_name": "place at table 1" },
    {"action_id": 7, "action_name": "navigate to table 2" },
    {"action_id": 42, "action_name": "pick up the bowl" },
    {"action_id": 6, "action_name": "navigate to table 1" },
    {"action_id": 50, "action_name": "place at table 1" }
  ]
}
\end{lstlisting}
\textbf{\#\# Example 2:}\\
Human instruction: I made a mistake and left the fridge open. Can you assist me by closing it?\\
Output:
\begin{lstlisting}[language=json]
{
  "language_plan": "The objective is for the robot to close the refrigerator. To do so, the robot first navigates to the refrigerator and then closes it.",
  "executable_plan": [
    {"action_id": 13, "action_name": "navigate to the refrigerator" },
    {"action_id": 61, "action_name": "close the refrigerator" }
  ]
}
\end{lstlisting}
\textbf{\#\# Example 3:}\\
Human instruction: Displace the sponge from the left counter.\\
Output:
\begin{lstlisting}[language=json]
{
  "language_plan": "To accomplish this task, the robot should navigate to the left counter to pick up the sponge. It should then move to another location, such as table 2, and place the sponge there.",
  "executable_plan": [
    {"action_id": 11, "action_name": "navigate to the left counter in the kitchen"},
    {"action_id": 28, "action_name": "pick up the sponge" },
    {"action_id": 7, "action_name": "navigate to table 2" },
    {"action_id": 51, "action_name": "place at table 2" }
  ]
}
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=In-context examples for EB-Navigation, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc=true]
\textbf{\#\# Example 1:}\\
Human Instruction : Navigate to the floorlamp and be as close as possible.\\
\\
Reasoning and reflection: I can see the floorlmap is at left side in front of me, next to the TVStand. Around me is a Cabinet at my left side. Following the guideline, since the floorlamp is at left side, I can approach it by a series of moving forward and left / rightward. Since currently there is an obstacle at my left, I will first move forward twice.\\
Executable plans: [[0, Move forward by 0.25], [0, Move forward by 0.25]]\\
\\
Environment Feedback: Step 1 is executed successfully; Step 2 is executed successfully;\\
\\
Reasoning and reflection: I can see now I get closer to the floorlamp. It is still located in front of me at the left. I will try move forward and move leftward in series.\\
Executable plans: [[0, Move forward by 0.25], [3, Move leftward by 0.25], [0, Move forward by 0.25], [3, Move leftward by 0.25], [0, Move forward by 0.25]]\\
\\
Environment Feedback: Step 3 is executed successfully; Step 4 is executed successfully; Step 5 is executed successfully; Step 6 is executed successfully; Step 7 is Invalid, as a StandardIslandHeight is blocking you from moving forward.\\
\\
Reasoning and reflection: I am a lot closer to the floorlamp by a series a forward and leftward movement. It is still in front of me. However, the last action is invalid, showing that I am blocked from moving forward. That means there is an obstacle in front of me and I can see it is probably a small table. Following the strategy, I can get away by first moving to the left and move forward. \\
Executable plans: [[3, Move leftward by 0.25], [3, Move leftward by 0.25], [0, Move forward by 0.25], [0, Move forward by 0.25]]\\
\\
Environment Feedback: Step 8 is executed successfully; Step 9 is executed successfully; Step 10 is executed successfully; Step 11 is executed successfully;\\
\\
Reasoning and reflection: I can see that the floorlamp is at the right side of me, slight in the front. I think it in effect of consecutively moving leftward. Also, i have got away with the obstable. Now I need to move right and I will probably be close to the floorlamp.\\
Executable plans: [[2, Move rightward by 0.25], [2, Move rightward by 0.25]]]\\
\\
Environment Feedback: Step 12 is executed successfully; Step 13 is executed successfully. \\
\\
Reasoning and reflection: Now I can see I am almost standing right next to the floorlamp, I can complete the task.\\
Executable plans: [[8, Complete the current task]]\\
\\
Task Success
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=In-context examples for EB-Manipulation, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc=true]
\textbf{\#\# Example 1:}\\
Human instruction: Pick up the star and place it into the yellow container.\\
Input: \texttt{\{'object 1': [45, 13, 18], 'object 2': [72, 20, 18], 'object 3': [50, 42, 17], 'object 4': [36, 42, 18], 'object 5': [70, 43, 17], 'object 6': [50, 77, 17]\}}\\
Output:
\begin{lstlisting}[language=json]
{
  "visual_state_description": "From left to right, I can see a purple cylinder at [45, 13, 18], a blue moon at [72, 20, 18], a green star at [50, 42, 17], a purple cube at [36, 42, 18], a yellow container at [70, 43, 17] and a purple container at [50, 77, 17].",
  "reasoning_and_reflection": "The task is to pick up the star located at [50, 42, 17] and place it into the yellow container located at [70, 43, 17]. The plan involves moving the gripper to the star's position, closing the gripper to pick it up, moving to the yellow container's position, and opening the gripper to release the cube into the container.",
  "executable_plan": [
    [50, 42, 17, 6, 61, 36, 1], 
    [50, 42, 17, 6, 61, 36, 0], 
    [47, 41, 26, 6, 61, 36, 0], 
    [69, 39, 24, 88, 32, 77, 0], 
    [69, 39, 24, 87, 32, 77, 1]
  ]
}
\end{lstlisting}
\textbf{\#\# Example 2:}\\
Human instruction: Stack the red star and the olive star in sequence.\\
Input: \texttt{\{'object 1': [42, 24, 19], 'object 2': [54, 53, 19], 'object 3': [65, 68, 19], 'object 4': [39, 73, 18]\}}\\
Output:
\begin{lstlisting}[language=json]
{
  "visual_state_description": "From left to right, I can see a purple star at [42, 24, 19], an olive star at [54, 53, 19], a grey star at [65, 68, 19] and a red star at [39, 73, 18].",
  "reasoning_and_reflection": "I need to understand the instruction first. To stack the objects in sequence, the red star at [39, 73, 18] should be at the bottom and the olive star at [54, 53, 19] should be placed on top. The plan involves moving the gripper to the olive star's position, closing the gripper to pick it up, moving to the top of the red star, and opening the gripper to release the olive star onto the red star.",
  "executable_plan": [
    [54, 51, 28, 3, 66, 37, 1], 
    [54, 54, 20, 3, 66, 37, 0], 
    [54, 51, 30, 3, 66, 37, 0], 
    [39, 74, 29, 3, 66, 7, 0], 
    [39, 74, 26, 3, 66, 7, 1]
  ]
}
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=In-context examples for EB-Manipulation, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc=true]
\textbf{\#\# Example 3:}\\
Human instruction: Put the red star into the shape sorter.\\
Input: \texttt{\{'object 1': [65, 11, 17], 'object 2': [50, 38, 17], 'object 3': [40, 64, 37], 'object 4': [45, 83, 17]\}}\\
Output:
\begin{lstlisting}[language=json]
{
  "visual_state_description": "From left to right, I can see a purple star at [65, 11, 17], a red star at [50, 38, 17], a shape sorter at [40, 64, 37] and a green star at [45, 83, 17].",
  "reasoning_and_reflection": "I need to identify the location of the star hole on the shape sorter first. I can see that the star hole is at the center of the shape sorter, which is at [40, 64, 37]. Then, the task is to pick up the red star at [50, 38, 17] and put it into the star hole on the shape sorter. The plan involves moving the gripper to the star's position, closing the gripper to pick it up, moving upwards, moving to the top of the star hole, and releasing the star into the hole.",
  "executable_plan": [
    [51, 38, 25, 119, 59, 26, 1], 
    [51, 38, 17, 119, 59, 26, 0], 
    [51, 38, 27, 119, 59, 26, 0], 
    [40, 65, 49, 0, 60, 45, 0], 
    [40, 65, 41, 0, 60, 45, 1]
  ]
}
\end{lstlisting}
\textbf{\#\# Example 4:}\\
Human instruction: Wipe the horizontal area.\\
Input: \texttt{\{'object 1': [33, 36, 19], 'object 2': [52, 38, 15], 'object 3': [59, 68, 15]\}}\\
Output:
\begin{lstlisting}[language=json]
{
  "visual_state_description": "From left to right, I can see a yellow sponge at [33, 36, 19], a green rectangle area at [52, 38, 15] and an orange rectangle area at [59, 68, 15].",
  "reasoning_and_reflection": "I need to identify which area is horizontal first. I can see that the orange rectangle area at [59, 68, 15] is horizontal since it is parallel to the green arrow representing the y-axis of the coordinate system annotated in the image. Then, the task is to pick up the sponge at [33, 36, 19] and use it to wipe the horizontal area at [59, 68, 15]. The plan involves moving the gripper to the sponge's position, closing the gripper to pick it up, moving to the side of the horizontal area, and moving the sponge along the main direction of the horizontal area to wipe it.",
  "executable_plan": [
    [32, 34, 25, 0, 60, 34, 1], 
    [32, 34, 17, 0, 60, 34, 0], 
    [32, 34, 27, 0, 60, 34, 0], 
    [60, 80, 18, 0, 61, 31, 0], 
    [61, 54, 17, 0, 61, 31, 0]
  ]
}
\end{lstlisting}
\end{tcolorbox}
\clearpage
\subsection{Output JSON Schema}
Below are the JSON schemas that guide the output structure of MLLMs.
\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title={Output JSON Schema for EB-ALFRED, EB-Habitat, and EB-Navigation}, 
boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc=true]
\begin{lstlisting}[language=json]
{
  "type": "object",
  "properties": {
    "visual_state_description": {
      "type": "string",
      "description": "Description of current state from the visual image"
    },
    "reasoning_and_reflection": {
      "type": "string",
      "description": "Summarize the history of interactions and any available environmental feedback. Additionally, provide reasoning as to why the last action or plan failed and did not finish the task."
    },
    "language_plan": {
      "type": "string",
      "description": "The list of actions to achieve the user instruction. Each action is started by the step number and the action name."
    },
    "executable_plan": {
      "type": "array",
      "description": "A list of actions needed to achieve the user instruction, with each action having an action ID and a name. Do not output an empty list.",
      "items": {
        "type": "object",
        "properties": {
          "action_id": {
            "type": "integer",
            "description": "The action ID to select from the available actions given by the prompt"
          },
          "action_name": {
            "type": "string",
            "description": "The name of the action"
          }
        },
        "required": ["action_id", "action_name"]
      }
    }
  },
  "required": [
    "visual_state_description", 
    "reasoning_and_reflection", 
    "language_plan", 
    "executable_plan"
  ]
}
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=Output JSON Schema for EB-Manipulation, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc=true]
\begin{lstlisting}[language=json]
{
  "type": "object",
  "properties": {
    "visual_state_description": {
      "type": "string",
      "description": "Describe the color and shape of each object in the detection box in numerical order in the image. Then provide the 3D coordinates of the objects chosen from input."
    },
    "reasoning_and_reflection": {
      "type": "string",
      "description": "Reason about the overall plan that needs to be taken on the target objects, and reflect on the previous actions taken if available."
    },
    "language_plan": {
      "type": "string",
      "description": "A list of natural language actions to achieve the user instruction. Each language action is started by the step number and the language action name."
    },
    "executable_plan": {
      "type": "array",
      "description": "A list of discrete actions needed to achieve the user instruction, with each discrete action being a 7-dimensional discrete action.",
      "items": {
        "type": "object",
        "properties": {
          "action": {
            "type": "string",
            "description": "The 7-dimensional discrete action in the format of a list given by the prompt."
          }
        },
        "required": ["action"]
      }
    }
  },
  "required": [
    "visual_state_description", 
    "reasoning_and_reflection", 
    "language_plan", 
    "executable_plan"
  ]
}
\end{lstlisting}
\end{tcolorbox}



\clearpage
\section{Supplementary Case Studies of Successful Planning}
\label{ap:success_planningapapp}
In this section, we present successful planning examples for Claude-3.5-Sonnet, InternVL2.5-78B, GPT-4o, and Gemini-1.5-pro across EB-ALFRED, EB-Habitat, EB-Navigation, and EB-Manipulation. Refer to Figures \ref{fig:example_alfred_claude}, \ref{fig:example_hab_internVL}, \ref{fig:example_nav_gpt4o}, and \ref{fig:example_man_gemini} for detailed reasoning and planning.

% [width=0.9\linewidth]

\begin{figure*}[h!]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.8\textwidth]{pics/alfred_claude.pdf}
\end{center}
\vspace{-1em}
\caption{Planning example of Claude-3.5-Sonnet in EB-AFRED.}
\label{fig:example_alfred_claude}
\end{figure*}

\begin{figure*}[t]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.9\linewidth]{pics/habitat_internvl.pdf}
\end{center}
\vspace{-1em}
\caption{Planning example in EB-Habitat for InternVL2.5-78B.}
\label{fig:example_hab_internVL}
\end{figure*}

\begin{figure*}[t]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.9\linewidth]{pics/navi_gpt4o.pdf}
\end{center}
\vspace{-1em}
\caption{Planning example of GPT-4o in EB-Navigation.}
\label{fig:example_nav_gpt4o}
\end{figure*}

\begin{figure*}[t]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.9\linewidth]{pics/mani_gemini.pdf}
\end{center}
\vspace{-1em}
\caption{Planning example of Gemini-1.5-pro in EB-Manipulation.}
\label{fig:example_man_gemini}
\end{figure*}


\clearpage
\section{Supplementary Case Studies of Unsuccessful Planning}

We also present failure cases across different tasks, highlighting key challenges in perception, reasoning, and planning. In EB-ALFRED, we showcase a planning error where the model fails to generate an effective executable plan to accomplish the task. In EB-Manipulation, we analyze a perception error where the agent misidentifies key objects, leading to incorrect execution. In EB-Navigation, we illustrate a reasoning error where the model struggles to interpret spatial relationships. These examples provide insights into the limitations of current models and highlight areas for improvement in object recognition, planning, and spatial reasoning. Refer to Figures \ref{fig:example_planning_error}, \ref{fig:example_perception_error}, and \ref{fig:example_reasoning_error} for details.

\label{ap:unsuccessful_task_subset}
 \begin{figure*}[h!]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.8\linewidth]{pics/planning_error_example.pdf}
\end{center}
\vspace{-1em}
\caption{Planning Error Example in EB-ALFRED: The agent was supposed to locate ``Book\_2" by the 7th action but instead continued interacting with the first book.}
\label{fig:example_planning_error}
\end{figure*}


\begin{figure*}[t]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.8\linewidth]{pics/perception_error_example.pdf}
\end{center}
\vspace{-1em}
\caption{Perception Error Example in EB-Manipulation: the agent erroneously observed the color of the object.}
\label{fig:example_perception_error}
\end{figure*}

\begin{figure*}[t]
\begin{center}
% \vspace{-15pt}
\includegraphics[width=0.8\linewidth]{pics/reasoning_error_example.pdf}
\end{center}
\vspace{-1em}
\caption{Reasoning Error Example in EB-Navigation: the agent recognized it was blocked by the countertop but failed to attempt navigating around it.}
\label{fig:example_reasoning_error}
\end{figure*}




