
\input{tables/main_high_level}
\section{Experiments}



In this section, we conduct comprehensive experiments to evaluate the performance of various MLLMs in \name, followed by ablation studies in Sections \ref{sec:language_ablation} and \ref{sec:visual_ablation} and error analysis in Section \ref{sec:error_analysis}.

\vspace{-5pt}
\subsection{Experimental Setups}
\vspace{-5pt}

% We benchmark 13 models, including leading proprietary models (GPT-4o / 4o-mini~\cite{GPT-4o,GPT-4o-mini}, Claude-3.5-Sonnet~\cite{Claude-3.5-Sonnet}, Gemini Pro / Flash~\cite{team2023gemini,team2024gemini,Gemini2.0}, and SOTA open-source models (LLaMA3.2 11B / 90B Vision Instruct~\cite{llama3.2}, InternVL 2.5 8B / 38B / 78B~\cite{internvl2.5}, Qwen2-VL 7B / 72B~\cite{wang2024qwen2}). For consistency, all models are set with a temperature of 0 and a maximum completion token length of 2048. All images are standardized to a resolution of 500$\times$500 pixels. The maximum number of environment steps is 30 for high-level tasks, 20 for EB-Navigation, and 15 for EB-Manipulation. We use the task success rate as the primary metric in our main experiments. More results and ablations are deferred to Appendix \ref{ap:additional_exp}.

We benchmark 19 models, including leading proprietary models (GPT-4o / 4o-mini~\cite{GPT-4o,GPT-4o-mini}, Claude-3.5-Sonnet~\cite{Claude-3.5-Sonnet}, Gemini Pro / Flash~\cite{team2023gemini,team2024gemini,Gemini2.0}, and Qwen-VL-Max \cite{bai2023qwen}), and SOTA open-source models (LLaMA3.2 11B / 90B Vision Instruct~\cite{llama3.2}, InternVL 2.5 8B / 38B / 78B~\cite{internvl2.5} and their MPO versions \cite{wang2024enhancing}, and Qwen2-VL and Qwen2.5-VL 7B / 72B~\cite{wang2024qwen2,bai2025qwen25vl}). For consistency, all models are set with a temperature of 0 and a maximum completion token length of 2048. All images are standardized to a resolution of 500$\times$500 pixels. The maximum number of environment steps is 30 for high-level tasks, 20 for EB-Navigation, and 15 for EB-Manipulation. We use the task success rate as the primary metric in our main experiments. More results and ablations are deferred to Appendix \ref{ap:additional_exp}.

\input{tables/main_low_level}

\vspace{-5pt}

\subsection{Benchmark Results}\label{sec:benchmark_res}
% \vspace{-5pt}

\textbf{Overall Results.} Tables \ref{tb:high_level_table} and \ref{tb:low_level_table} summarize the results for high-level and low-level tasks, respectively. Overall, \textit{\textbf{current MLLMs demonstrate strong performance on high-level tasks but struggle with low-level tasks, especially EB-Manipulation.}}
Among \textbf{proprietary models}, we observe that different models excel at different task levels: Claude-3.5-Sonnet achieves the highest average accuracy on high-level tasks, with 64.0\% on EB-ALFRED and 68.0\% on EB-Habitat, while GPT-4o leads in low-level tasks, scoring 57.7\% on EB-Navigation and 28.9\% on EB-Manipulation. Gemini-1.5-Pro performs the worst among the three large proprietary models, but Gemini-1.5 / 2.0-Flash outperforms GPT-4o-mini by a large margin. For \textbf{open-source models}, the InternVL2\_5-MPO family delivers the best overall performance. Its largest 78B version outperforms both Llama-3.2-90B-Vision-Ins and Qwen2.5-VL-72B-Ins across all four environments. Comparing the two InternVL2\_5 families also reveals a clear benefit of the MPO models \cite{wang2024enhancing}, underscoring the positive impact of preference optimization on the reasoning capabilities of embodied agents. Moreover, open-source models display a strong scaling effect, with performance improving as model parameters increase. Despite these gains, a notable performance gap remains between the top proprietary and open-source models.

% \textbf{The Role of Vision in Embodied Agent.}
% By comparing the performance of embodied agents with and without visual information (marked as ``Lang") in Tables \ref{tb:high_level_table} and \ref{tb:low_level_table}, we observe a clear distinction between low-level and high-level tasks. \textbf{\textit{Low-level tasks show a much stronger reliance on vision compared to high-level tasks.}} For example, disabling vision causes GPT-4o’s EB-Navigation performance to drop sharply from 57.7\% to 17.4\%, with long-horizon planning completely collapsing to 0\%. This sharp decline highlights the critical importance of visual signals for low-level control tasks. Conversely, high-level tasks show much less dependence on visual input. GPT-4o (Lang) and GPT-4o-mini (Lang) perform on par with or even outperform their vision-enabled counterparts in EB-ALFRED and EB-Habitat, suggesting that these tasks may rely more heavily on textual information rather than visual input. We will further investigate the impact of language-centric factors in Section \ref{sec:language_ablation}. These findings emphasize two key insights: (1) when designing MLLM-based embodied AI benchmarks, it is essential to consider action-level taxonomy, with greater attention to low-level action tasks, and (2) more advanced methods are needed to effectively leverage visual input for high-level embodied tasks.

% revised version
\textbf{The Role of Vision in Embodied Agent.}
By comparing the performance of embodied agents with and without visual information (marked as ``Lang") in Tables \ref{tb:high_level_table} and \ref{tb:low_level_table}, we observe a clear distinction between low-level and high-level tasks. \textbf{\textit{Low-level tasks show a much stronger reliance on vision compared to high-level tasks.}} For example, disabling vision causes GPT-4o’s EB-Navigation performance to drop sharply from 57.7\% to 17.4\%, with long-horizon planning completely collapsing to 0\%. This sharp decline highlights the critical importance of visual signals for low-level control tasks. Conversely, high-level tasks show much less dependence on visual input. GPT-4o (Lang) and GPT-4o-mini (Lang) perform on par with or even outperform their vision-enabled counterparts in EB-ALFRED and EB-Habitat, suggesting that these tasks may rely more heavily on textual information rather than visual input. We will further investigate the impact of language-centric factors in Section \ref{sec:language_ablation}. These findings emphasize two key insights: (1) when designing MLLM-based embodied AI benchmarks, it is essential to consider action-level taxonomy, with greater attention to low-level action tasks, and (2) more advanced methods are needed to effectively leverage visual input for high-level embodied tasks.



\textbf{Fine-grained Results across Subsets.}
We have the following findings based on our evaluation across 6 subsets.

\emph{\textbf{(1) Performance Varies across Different Subsets.}} We observe that models perform differently across various subsets. For instance, while Claude-3.5-Sonnet is the best model on EB-Habitat overall, GPT-4o surpasses it on long-horizon subsets (64\% vs. 58\%), indicating GPT-4o’s stronger ability in long-horizon planning. This trend is even more pronounced in low-level tasks. For example, Gemini-1.5-Pro scores 10 points higher than GPT-4o on the spatial awareness subset but lags significantly in other capabilities. These results highlight the importance of fine-grained evaluations to uncover nuanced limitations in current models.  

\emph{\textbf{(2) Long-Horizon Planning Is the Most Challenging Task}}. The long-horizon subset consistently proves to be the most difficult, showing the largest performance gap compared to base scores. For instance, in EB-Habitat, Claude-3.5-Sonnet achieves 96\% on the base subset but drops to 58\% on the long-horizon subset. Similarly, GPT-4o falls from 86\% to 64\%. This trend holds true across both high-level and low-level tasks, suggesting that long-horizon planning remains a significant bottleneck for current MLLM-based agents.


% \emph{\textbf{(3) Common-Sense Reasoning Suffers the Sharpest Drop in Smaller Model Variants}}.
% When comparing models within the same family but of different scales, the common-sense reasoning subset typically shows the most significant decline as the number of parameters decreases. While other capabilities, like complex instructions or spatial awareness, also degrade, the average drop in common-sense reasoning tends to be the largest. For instance, in the EB-ALFRED benchmark, the common-sense accuracy of InternVL2\_5 drops by over 20 percentage points when scaling down from 78B to 8B parameters. This decline surpasses the corresponding drops in base or visual reasoning subsets. These findings suggest that the nuanced "everyday knowledge" aspect of reasoning is far more sensitive to model scale than lower-level visual or basic instruction-following abilities.


\begin{figure}[ht]
\begin{center}
\vspace{-5pt}
\includegraphics[width=1\linewidth]{pics/high_level_ablation.pdf}
\end{center}
\vspace{-10pt}
\caption{Language-centric ablations on EB-ALFRED.}
\vspace{-1.em}
\label{fig:high_level_ablation}
\end{figure} 


% \vspace{-5pt}
\subsection{Language-centric Ablation}\label{sec:language_ablation}




% \begin{figure}[h!]
% \begin{center}
% % \vspace{-15pt}
% \includegraphics[width=0.95\linewidth]{pics/low_level_detection.pdf}
% \end{center}
% \vspace{-1em}
% \caption{Impact of detection boxes on low-level tasks.}
% \end{figure}

% \begin{figure}[h!]
% \begin{center}
% % \vspace{-15pt}
% \includegraphics[width=0.95\linewidth]{pics/low_level_multi_view.pdf}
% \end{center}
% \vspace{-1.5em}
% \caption{Effect of multi-view images on low-level tasks.}
% \vspace{-0.5em}
% \label{fig:low_level_multi_view}
% \end{figure}

% \begin{figure}[h!]
% \begin{center}
% % \vspace{-15pt}
% \includegraphics[width=0.95\linewidth]{pics/low_level_ICL.pdf}
% \end{center}
% \vspace{-1.5em}
% \caption{Influence of visual ICL examples on low-level tasks.}
% % \vspace{-1.5em}
% \label{fig:visual_in_context}
% \end{figure}

% Since visual perception is not the primary factor influencing high-level task performance, 
We explore the role of the language-centric components, specifically focusing on \textbf{environment feedback} and \textbf{the number of in-context examples}. Comparisons are conducted using the base subset of EB-ALFRED. Our findings in Figure \ref{fig:high_level_ablation} reveal that removing environment feedback—which provides critical information during interaction—causes a 10\% drop in success rate for GPT-4o and an 8\% drop for Claude-3.5-Sonnet. Furthermore, while our experiments use 10 in-context examples by default, reducing this number significantly affects performance. In a 0-shot setting, the success rate drops to around 40\%. Comparing the results in Table \ref{tb:high_level_table} and \ref{tb:low_level_table}, these results emphasize that high-level tasks depend more on textual information than on visual input.


\vspace{-5pt}
\begin{figure}[h!]
\begin{center}
% \vspace{-10pt}
\includegraphics[width=1\linewidth]{pics/visual_centric_ablation.pdf}
\end{center}
\vspace{-15pt}
\caption{Visual-centric ablations on EB-Manipulation.}
\vspace{-1.5em}
\label{fig:low_level_resolution}
\end{figure}

% \vspace{5pt}
\subsection{Visual-centric Ablation}\label{sec:visual_ablation}
\vspace{-5pt}

Visual information is critical for the performance of low-level tasks. In this section, we thoroughly analyze the impact of four factors or potential enhancements: camera resolution, detection boxes, multi-step images, and visual in-context learning. All comparisons are based on the base subset of EB-Manipulation. 
Additional ablation results can be found in Appendix \ref{ap:additional_exp}.


\textbf{Camera Resolutions.} We investigate the effect of three camera resolutions on task performance. Our results, shown in Figure \ref{fig:low_level_resolution} (a), indicate that mid-range resolutions ($500 \times 500$) achieve better results compared to both lower ($300 \times 300$) and higher ($700 \times 700$) resolutions. While low-resolution images may lack fine-grained details necessary for task execution, excessively high resolutions can introduce unnecessary complexity, making it harder for MLLMs to focus on relevant information for decision-making. These results highlight the importance of selecting an appropriate resolution when deploying MLLM-based embodied agents.

\textbf{Detection Boxes.} In EB-Manipulation, detection boxes and visual markers are used to align language instructions with visual information, helping to localize key objects in the scene. Figure \ref{fig:low_level_resolution} (b) shows that removing detection boxes reduces success rates from 39.6\% to 27.1\% for GPT-4o and from 37.5\% to 29.2\% for Claude-3.5-Sonnet, emphasizing their important role in object localization for low-level tasks.



\textbf{Multi-step Image Input.} We also explore whether incorporating multi-step historical observations can enhance performance in our agent framework, as they may help address partial observability. For EB-Manipulation, we include observations from the past two steps in addition to the current step. Two multi-step image examples are shown in Figure \ref{fig:ms_nav} and \ref{fig:ms_man}. Figure \ref{fig:low_level_resolution} (c) presents the quantitative results. Our experiments reveal that current MLLMs struggle to effectively utilize multiple image inputs, often leading to confusion about their current state. Future work could focus on developing methods to better leverage multiple images for enhanced understanding and reasoning.


% \textbf{Multi-view \& Multi-step Images.} We also explore whether multi-view images and multi-step historical observations can enhance performance in our agent framework, as they may help address partial observability. For EB-Manipulation, we consider front and wrist views, and in the multi-step setting, the agent incorporates observations from the past two steps in addition to the current step. Figures \ref{fig:low_level_resolution} (c) and \ref{fig:low_level_multi_view} present the quantitative results. Our experiments reveal that current MLLMs struggle to effectively utilize multiple image inputs, often becoming confused about their current state. Future work could focus on developing methods to efficiently leverage richer visual information for improved understanding and reasoning.


% whether multi-view images and multi-step historical observations improve task performance. In the multi-view setting, EB-Manipulation extends to front and wrist views, while EB-Navigation integrates a top-down view alongside the front view. In the multi-step setting, the agent, initially using only the current step, incorporates the past two steps' observations. Figure \ref{fig:low_level_resolution} and Figure \ref{fig:low_level_multi_view} present quantitative results. Our experiments reveal that current MLLM-based agents struggle to effectively exploit additional visual inputs. Future work could explore how to efficiently utilize richer visual information for better understanding and reasoning.

\textbf{Visual In-context Learning (ICL). }Previous work has primarily relied on text-based ICL demonstrations. In this study, we investigate the impact of visual ICL for embodied agents by including image observations as part of the in-context examples for EB-Manipulation. This approach helps the model better understand the relationship between successful low-level actions and the object positions in the image. Visual ICL examples are demonstrated in Figure \ref{fig:in_context_learning}. We limit the number of examples to two to avoid overwhelming the model with excessive visual input. This may slightly lower the baseline performance, as the main results use more than two text-based examples. As shown in Figure \ref{fig:low_level_resolution} (d), the results demonstrate that visual ICL significantly outperforms language-only ICL. For instance, Claude-3.5-Sonnet achieves a 16.7\% performance boost. These findings underscore the potential of visual ICL as a promising avenue for future research in embodied agents.


\vspace{-5pt}
\subsection{Error Analysis}\label{sec:error_analysis}
\vspace{-5pt}
\begin{figure}[t]
\begin{center}
\vspace{-5pt}
\includegraphics[width=1.0\linewidth]{pics/error_analysis.pdf}
\end{center}
\vspace{-1.5em}
\caption{Error Analysis.}
\vspace{-2em}
\label{fig:error_analysis}
\end{figure}



We conducted an error analysis on GPT-4o to identify potential failure modes in EB-ALFRED and EB-Manipulation. For each environment, we sample 10 failure episodes from each subset, resulting in a total of 110 failed episodes to be analyzed. We found three main types of errors: perception errors, reasoning errors, and planning errors. Each error category corresponds to a specific stage in our agent pipeline, with definitions of sub-errors provided in Appendix \ref{appendix_error}.

Overall, \emph{planning errors are the most common issue in both environments, while perception errors are more prevalent in low-level tasks.}
In EB-ALFRED, planning errors (55\%) and reasoning errors (41\%) dominate, while only 4\% of errors are perception errors. Among planning errors, missing steps (23\%) and invalid actions (22\%) are the most common issues, highlighting challenges in generating complete and valid plans. Reflection errors (17\%) suggest the model often fails to recognize planning mistakes in its action history. Another common failure is wrong termination errors (13\%), where the model prematurely assumes the task is complete and stops too early. For EB-Manipulation, planning errors remain the primary cause of failure (44\%), due to inaccurate actions, indicating difficulties in estimating precise gripper poses. Perception errors make up 33\% of failures, with wrong recognition errors (22\%) being the most frequent. These errors show that even with detection boxes annotated in the visual input, the model still fails to recognize object attributes correctly.  This highlights considerable room for improvement in the visual capabilities of GPT-4o.



% Reflection errors (12.8\%) and spatial reasoning errors (10.3\%) indicate that the model's reasoning process is not robust in respond to spatial relationships and action sequences.