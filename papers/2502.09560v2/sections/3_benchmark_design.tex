\section{EmbodiedBench}\label{sec:bench_design}
\vspace{-5pt}
To thoroughly assess MLLMs as embodied agents across various action levels and capabilities, we introduce \name, a benchmark comprising four environments: EB-ALFRED, EB-Habitat, EB-Navigation, and EB-Manipulation. To evaluate six core embodied agents' capabilities, we developed new datasets and enhanced existing simulators to support comprehensive assessments. Below is an overview of the four benchmark tasks, with further details available in Appendix \ref{ap:detailed_task}.




\begin{figure*}[t]
\begin{center}
% \vspace{-5pt}
\includegraphics[width=0.96\linewidth]{pics/planner_pipeline_update.pdf}
\end{center}
\vspace{-1.3em}
\caption{The vision-driven agent pipeline used in \name. This pipeline serves as a robust framework for processing multimodal inputs, reflection and reasoning, and generating executable plans. For detailed descriptions, refer to Section \ref{sec:agent}.}
\label{fig:agent_pipeline}
\vspace{-1.em}
\end{figure*}




\vspace{-5pt}
\subsection{High-level and Low-level Tasks}
\textbf{EB-ALFRED.} We develop EB-ALFRED based on the ALFRED dataset \cite{shridhar2020alfred} and the AI2-THOR simulator \cite{kolve2017ai2}. 
% ALFRED tasks are defined using the Planning Domain Definition Language (PDDL), which helps assess the agent's success in completing the task or subgoals. 
% The ALFRED dataset includes 7 task types, \textit{Pick \& Place, Stack \& Place, Pick Two \& Place, Clean \& Place, Heat \& Place, Cool\& Place, and Examine in Light}. 
Our simulator is based on Lota-Bench's implementation \cite{choi2024lota} for 8 high-level skill types: ``pick up", ``open", ``close", ``turn on", ``turn off", ``slice", ``put down", and ``find", each customizable with specific objects, for example, ``find an apple". The simulator provides an egocentric view as observation, along with textual feedback on action validity and possible failure reasons.
% For example, it may indicate ``failure to pick up an object because another object is being held."
Despite its strengths, Lota-Bench's simulator has several limitations, which we outline in Appendix \ref{ap:details_alfred}. To enhance the simulation, we introduced key improvements, such as support for multiple instances of the same object type, allowing us to cover all task types in ALFRED. Additionally, we streamlined the action space by merging ``put down" actions into a single action, since only one object can be held at a time. Due to the varying number of objects in ALFRED, the action space of EB-ALFRED is dynamic, ranging from 171 to 298 actions. Furthermore, we manually corrected simulator errors and refined instruction quality, ensuring more accurate action execution and improved task solvability. These enhancements make EB-ALFRED a high-quality benchmark for evaluating embodied agents.







\textbf{EB-Habitat.} EB-Habitat is built upon the Language Rearrangement benchmark \cite{szot2023large}, featuring 282 diverse language instruction templates. It leverages the Habitat 2.0 simulator \cite{szot2021habitat} and focuses on planning and executing 70 high-level skills to achieve user-defined goals. These skills fall into five categories: ``navigation", ``pick", ``place", ``open", and ``close", with each skill parameterized by a set of objects. Unlike ALFRED, which permits navigation to any object, EB-Habitat restricts navigation to receptacle-type objects, requiring robots to visit multiple locations to find desired items. With its wide variety of language instructions and unique navigation constraints, EB-Habitat serves as a valuable complement to EB-ALFRED.



% \subsection{Low-level Tasks}

\textbf{EB-Navigation.}
EB-Navigation is an evaluation suite based on AI2-THOR \cite{kolve2017ai2}, designed to assess embodied agents' navigation abilities. 
Each unique navigation task is primarily defined by: (1) \textit{initial Robot Pose}, (2) \textit{target object information}, and (3) \textit{language instruction} that specifies which target object to locate, such as ``navigate to the laptop". The robot can only rely on visual observations and textual feedback, without direct positioning data, to navigate to the target object.
% We set the agentâ€™s starting position to be at least a certain distance from the target object. 
% This adjustable $\alpha$ allows adjustment of the difficulty of the task. 
Success is defined as reaching within a specified distance of the target. The action space includes 8 low-level actions:
(1) Move forward/backward/left/right by $\Delta x$.   
(2) Rotate to the right/left by $\Delta \theta$ degrees.  
(3) Tilt the camera upward/downward by $\Delta \varphi$ degrees.
The environment provides textual feedback on action validity, such as collision detection. Additionally, we offer a script for automatic task generation, allowing users to create custom task datasets by specifying the configuration.





\textbf{EB-Manipulation.}
EB-Manipulation extends VLMBench \cite{zheng2022vlmbench} to evaluate MLLM-based embodied agents in low-level object manipulation. 
% It includes four task categories(1) \textit{Pick \& Place Objects}, (2) \textit{Stack Objects}, (3) \textit{Shape Sorter Placement}, and (4) \textit{Table Wiping}, each with randomly varied instances in color, position, shape, and orientation for diverse evaluation. 
The agent controls a robotic arm using a 7-dimensional action vector, specifying movement parameters. Direct low-level manipulation is challenging for MLLMs. To overcome this challenge, we implemented enhancements, as illustrated in Figure \ref{fig:agent_pipeline}: (1) action space discretization \cite{yin2024context}, which divides the position components ($x,y,z$) into 100 bins and the orientation components ($roll, pitch, yaw$) into 120 bins, enabling valid actions to take forms like $[x, y, z, roll, pitch, yaw, gripper]=[57,61,20,10,60,25,1]$; and (2) additional information like YOLO \cite{redmon2016you} detection boxes with index markers \cite{yang2023set} and object pose estimation for indexed objects, reducing the need for precise 3D location. 
% The agent can focus on perceiving and reasoning about each object's spatial relationship. 
% With these improvements and additional in-context examples, our MLLM agent effectively tackles complex low-level manipulation tasks.



\begin{figure*}[h!]
\begin{center}
\vspace{-5pt}
\includegraphics[width=0.97\linewidth]{pics/planning_example_version2.pdf}
\end{center}
\vspace{-1em}
\caption{Planning examples in EB-ALFRED and EB-Manipulation based on GPT-4o.}
\label{fig:example_alfred_man_4o}
\vspace{-1.2em}
\end{figure*}


\vspace{-0.4em}
\subsection{Capability-oriented Data Collection}
We aim to collect capability-oriented data for our four environments. To accomplish this, we have identified six capability categories, as outlined in Table \ref{tb:subsets_complete}: (1) The \textbf{Base} subset evaluates basic task-solving skills necessary for planning action sequences across tasks of low to medium difficulty. (2) The \textbf{Common Sense} subset focuses on the use of common sense knowledge to indirectly refer to objects, such as describing a refrigerator as ``a receptacle that can keep food fresh for several days." This subset evaluates the ability of embodied agents to reason using common sense. (3) The \textbf{Complex Instruction} subset includes relatively longer contexts, which can be relevant or irrelevant, to obscure the instruction. This measures an agent's ability to discern user intent from a long context. (4) The \textbf{Spatial Awareness} subset refers to objects by their location relative to other objects. (5) The \textbf{Visual Appearance} subset involves referring to objects based on their visual attributes, such as color or shape. (6) The \textbf{Long Horizon} subset comprises tasks requiring extended action sequences, typically more than 15 steps in EB-ALFRED. These subsets cover a broad range of scenarios, enabling a fine-grained evaluation of embodied agents' capabilities.

To construct a diverse dataset, we employ different data collection strategies. For EB-ALFRED and EB-Manipulation, data was gathered through a combination of manual annotation and instruction augmentation using GPT-4o \cite{GPT-4o}. For EB-Habitat, we reorganized and adapted an existing dataset from \cite{szot2023large}, aligning it with our specific objectives. Differently, data for EB-Navigation was generated entirely through automated Python programs. In summary, EB-ALFRED and EB-Habitat each include 300 test instances, with 50 instances for 6 subsets. Due to design challenges, EB-Navigation omits the spatial awareness subset and EB-Manipulation excludes the long-horizon subset. EB-Navigation consists of 300 test cases distributed across 5 subsets (60 instances each), while EB-Manipulation contains a total of 228 instances, with 48 instances for each subset except visual appearance, which includes 36 instances. Detailed data collection is provided in Appendix \ref{ap:detailed_task}.

\vspace{-7pt}


\subsection{Vision-driven Agent Design}\label{sec:agent}

To evaluate MLLMs as agents in \name, we design a unified embodied agent pipeline, illustrated in Figure \ref{fig:agent_pipeline}. 
This pipeline provides a robust framework for processing multimodal inputs, reasoning through interactions, and generating structured, executable plans composed of sequential actions. Two planning examples are provided in Figure \ref{fig:example_alfred_man_4o}, with additional examples available in Appendix \ref{ap:success_planningapapp}. Below, we outline the key components of our agent design.

\textbf{Agent Input:} 
The agent processes a variety of inputs, including language instructions, visual perceptions, in-context demonstrations, interaction history, and task-specific information. For visual perception, the agent can utilize either the current step image or a sequence of historical images within a sliding window. \emph{However, we observe that current MLLMs struggle to understand multiple historical images effectively, so we primarily rely on the current step image for efficiency.} Task-specific information varies by task type. For high-level tasks and EB-Navigation, the agent requires valid skill sets, while EB-Manipulation tasks include descriptions of the action format. Additionally, EB-Manipulation incorporates detection boxes with visual markers and object positions to help MLLMs accurately identify 3D locations. Further examples of input prompts are provided in Appendix \ref{ap:planner_input_examples}.


\textbf{Task Planner:} At each planning step, the agent: (1) generates a textual description of the current visual input; (2) reflects on past actions and environmental feedback; (3) reasons about how to achieve the goal using available information; (4) formulates a language-based plan; and (5) converts it into an executable plan in the required format. All outputs are structured in JSON. Unlike prior work planning one action per timestep \cite{liu2024visualagentbench}, we support multi-step planning, allowing the agent to dynamically decide the number of actions needed. It offers two advantages: (1) better alignment with in-context examples for sequential decision-making, and (2) reduced plan redundancy, especially in low-level tasks where single action causes limited changes in images, thereby minimizing MLLM API calls.
If a plan fails or triggers an invalid action, the agent restarts planning from the latest state.

% Further details about the agent design are available in Appendix \ref{ap:planner_input_examples}.

% (2) it minimizes redundant replanning, especially in low-level tasks where single actions cause limited changes in images, and 

% \subsection{Visual-Oriented Techniques.}