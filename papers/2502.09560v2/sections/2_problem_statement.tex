\section{Problem Formulation}\label{sec:problem_formulation}
\vspace{-5pt}
\textbf{Definition of Action Levels.}\label{sec:action_level}
In embodied agent research, actions can be systematically classified into hierarchical levels based on their executability in robotic systems \cite{ma2024survey,belkhale2024rt}. \textbf{Low-level actions} correspond to atomic commands directly executable by robots, defined as operations that specify translational or rotational displacements. For instance, a robotic armâ€™s action is often parameterized as a 7-dimensional vector:
$\label{eq:low_level_action}
    a=[X, Y, Z, \rm{Roll}, \rm{Pitch}, \rm{Yaw}, \rm{Gripper}]$,
where $(X, Y, Z)$ denote incremental translational displacements, $(\rm{Roll}, \rm{Pitch}, \rm{Yaw})$ represent rotational deltas in Euler angles, and \(\rm{Gripper}\) encodes the binary open/closed state of the end-effector. Similarly, commands like ``move forward 0.1 m" qualify as low-level actions, as they map unambiguously to kinematic transformations.  
In contrast, \textbf{high-level actions} can be decomposed into sequences of low-level primitives. Formally, a high-level action is defined as \(a^h = [a_1, a_2, \ldots, a_n]\), where each \(a_i\) is a low-level executable primitive. For example, executing \text{``find a HandTowel"} might involve iterating through low-level behaviors: rotating certain degrees, scanning for the target, and moving towards it. 

% While both action levels are critical for embodied agents, current MLLMs excel at high-level action planning due to their semantic reasoning capabilities, whereas low-level actions still remain underexplored in MLLM-based agent frameworks.  


\textbf{Vision-driven Agents.}\label{sec:def_vision_agent}
Vision-driven agents are autonomous systems that make sequential decisions based on visual perception and language instructions. This problem can be formally modeled as a Partially Observable Markov Decision Process (POMDP) augmented with language instructions, defined by the tuple $(\mathcal{S}, \mathcal{A}, \Omega, \mathcal{T}, \mathcal{O}, L, \mathcal{R})$. Here, $\mathcal{S}$ is the complete state space unobservable to the agent; $\mathcal{A}$ is the space of high-level or low-level actions for the agents; $\Omega$ is the visual perception space, where each observation $I_t \in \Omega$ corresponds to an image frame at time $t$; $\mathcal{T}$ is the transition dynamics; $\mathcal{O}$ relates the underlying states to the agent's visual observations; $L$ is the language instruction that specifies the desired goal; $\mathcal{R}$ evaluates task completion given the language instruction $L$: $r_t = \begin{cases}
1 & \text{if } s_t \models L \text{ (instruction achieved)} \\
0 & \text{otherwise}\end{cases}$.
At timestep $t$, the agent maintains a history $h_t = (a_0, I_1, ..., a_{t-1}, I_t)$ and selects actions through a policy $\pi(a_t|L, h_t)$. The objective is to maximize the probability of task success: $\max_\pi \mathbb{E}\left[r_\tau\right]$, where $\tau$ is the terminal timestep, which occurs when the task succeeds \(s_\tau \models L\) or the maximum horizon is reached. 

% Given the partially observable nature, effectively integrating planning and memory is crucial for the success of vision-driven agents.

