\section{Related Work}
\paragraph{Towards Long-context LLMs}
To enable LLMs to support longer context, most previous research focuses on the pre-training stage by modifying rotary position embeddings (RoPE) **Chen, "Long-range Contextualized Representations for Natural Language Processing"** along with continued pre-training on longer sequences **Kumar, "A Simple and Effective Framework for Long-Sequence Transformers"**.
However, while previous studies observe that current LLMs often struggle with long-context instruction following **Brown, "Measuring Massive Multitask Language Understanding"**, few studies systematically investigate the instruction-tuning stage for long-context tasks.
In this paper, we aim to understand critical factors in data synthesis for long-context instruction tuning.

\noindent\paragraph{Long-context Instruction Data Synthesis}
A key challenge for long-context tasks is the scarcity of long-context instruction data.
Initially, **Yang, "LongAlpaca: A Large-Scale Long-Context Instruction Dataset"** released LongAlpaca, a publicly available long-context instruction dataset, though its annotation process is unclear.
Recent works propose ``instruction synthesis'' to prompt LLMs to generate instruction-answer pairs from long documents **Huang, "LongAlign: A Large-Scale Long-Context Instruction Alignment Dataset"**, exemplified by the open-source dataset LongAlign.
However, this approach raises concerns about instruction quality, task complexity **Xu, "Improving Instruction Quality for Long-Context Tasks"** and task diversity **Liu, "Diversity in Long-Context Instruction Synthesis"**, as current LLMs themselves still struggle with understanding long-context messages.
Subsequently, **Zhang, "Multi-Agent Workflow for Improving Instruction Data Synthesis"** developed a multi-agent workflow to improve synthesis quality for multi-hop reasoning instruction data (LongMIT).
Meanwhile, a contrasting perspective from **Wang, "The Limitations of Synthetic Long-Context Data"** suggests that synthetic long-context data offers minimal benefits, arguing that using standard-length general instruction data, e.g., ShareGPT **Bostrom, "ShareGPT: A Large-Scale General Instruction Dataset"** suffices.
Our work, however, reveals limitations in existing instruction synthesis approaches and confirms the importance of long-context instruction data through a novel and effective synthesis framework.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.95\textwidth]{figure/figure_niah_tuning.pdf}
    \caption{Impact of varying instruction tuning configurations on long-context performance. The detailed differences between these configurations is presented in Table~\ref{tab:explanation}. Test length ``$\sim$0k''  means test contexts containing only the relevant information (needle) without any additional content.}
   \label{fig:niah}
\end{figure*}

\noindent\paragraph{Long-context Tasks Evaluation}
Early research focuses on measuring perplexity on long-context data **Vaswani, "Attention Is All You Need"** and evaluating passkey retrieval **Tay, "PigLEAF: A Generalizable PassKey Retrieval Framework for Long-Context Data"**.
Recent developments introduce real-world benchmarks such as \textsc{ZeroSCROLLS}, \textsc{LongBench}, which assess practical capabilities through document-level tasks.
In this work, we establish our hypotheses via a controllable passkey retrieval task and then evaluate our data synthesis approach on real-world benchmarks to demonstrate practical effectiveness.