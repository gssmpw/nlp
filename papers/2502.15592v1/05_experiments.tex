\section{Experiments}
\label{sec:main_results}
\subsection{Experimental Setting}

\paragraph{Evaluation Benchmark}
We conduct evaluations on \textsc{LongBench}\footnote{\url{https://github.com/THUDM/LongBench}}, focusing on three representative long-context tasks: single-document question-answering, multiple-documents question-answering and summarization\footnote{Despite being widely used in long-context evaluation, it has been argued that summarization tasks may suffer from position bias, as models can utilize the few leading sentences to achieve decent performance~\cite{nallapati2017summarunner,li2024long}.} (Table~\ref{tab:longbench}).
\begin{table}[ht]
\scriptsize
\renewcommand{\arraystretch}{1.2}
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{clcc}
\toprule
\textbf{Type}                          & \hspace{0.5cm}\textbf{Dataset}  &\textbf{Size}  & \textbf{Metric}  \\
\hline
\rowcolor{gray!15}
\multicolumn{4}{c}{\textit{Artificial Tasks}} \\
% \hline
\multirow{4}{*}{Passkey Retrieval}     & Single NIAH      & 500 & EM      \\
                                       & Multi-key NIAH   & 500 & EM      \\
                                       & Multi-query NIAH & 500 & EM      \\
                                       & Multi-value NIAH & 500 & EM      \\
\hline
\rowcolor{gray!15}
\multicolumn{4}{c}{\textit{Real-world Tasks}} \\
% \hline
\multirow{2}{*}{Single-doc QA}         & NarrativeQA    & 200  & F1      \\
                                       & Qasper         & 200  & F1      \\
% \hline
\cline{2-4} 
\multirow{3}{*}{Multi-doc QA}         & HotpotQA        & 200  & F1      \\
                                      & 2WikiMultihopQA & 200  & F1      \\
                                      & Musique         & 200  & F1      \\
\cline{2-4}
\multirow{3}{*}{Summarization}        & GovReport       & 200  & Rouge-L \\
                                      & QMSum           & 200  & Rouge-L \\
                                      & MultiNews       & 200  & Rouge-L \\
\bottomrule
\end{tabular}
}
\caption{Long-context evaluation benchmark used in our experiments. In the pilot study (Section~\ref{sec:pilot_study}), we use tasks from \textsc{RULER}. In main experiments (Section~\ref{sec:main_results}), we use real-world tasks from \textsc{LongBench}. The context length distribution of these tasks is shown in Figure~\ref{fig:length_and_score}.}
\label{tab:longbench}
\end{table}
With the open-sourced codebase~\cite{bai2024longbench}, we conduct zero-shot evaluations using greedy decoding with the provided templates and metrics.

\noindent\paragraph{Base Models}
We use \texttt{LLaMA2-7B-64k}\footnote{\url{https://huggingface.co/THUDM/LongAlign-7B-64k-base}}
and \texttt{LLaMA3.1-8B-128k}\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B}} as base models for instruction-tuning.
Both models are pre-trained to support extended context windows (64k and 128k tokens respectively).

\noindent\paragraph{Instruction Data}
For specialized context-aware instruction data, we randomly sample 200 instructions from each subtask's training set (totalling 1.6k instructions) for context synthesis\footnote{We set the number of concatenated contexts to ten and report performance with context concatenation unless otherwise stated. Performance with different concatenation sizes is reported in Appendix~\ref{sec:number-of-concatenated-context}.}.
To ensure a fair comparison, we use the corresponding contexts from these selected samples for instruction synthesis (identical sample size and domain).
We employ \texttt{GPT4o-mini}\footnote{\texttt{GPT4o-mini} refers to \texttt{gpt-4o-mini-2024-07-18}, used with default temperature and top-p settings.}~\cite{openai2024gpt4ocard} as the data synthesis engine\footnote{In the analysis section, we report perform of using other models as the synthesis engine, such as \texttt{LongWriter-8B}~\cite{bai2024longwriter} and \texttt{Qwen2.5-72B-Instruct}~\cite{qwen2025qwen25}.}.
We also compare our approach with open-source long-context instruction data as strong baselines, including LongAlpaca, LongAlign and LongMIT. 
For general-purpose instruction data, we employ ShareGPT for \texttt{LLaMA2} and UltraChat for \texttt{LLaMA3.1}\footnote{The results of using ShareGPT with \texttt{LLaMA3.1} are reported in Appendix~\ref{sec:llama3.1-sharegpt}.}.

\noindent\paragraph{Instruction-tuning Configuration}
We adopt \textit{LongAlign}\footnote{\url{https://github.com/THUDM/LongAlign}} as the codebase for instruction-tuning.
Training is performed on a single node with 8$\times$H800 GPUs.
Detailed training configuration is reported in Appendix~\ref{sec:instruction-tuning-details}.

\begingroup
\renewcommand{\arraystretch}{1.2} % Default value: 1
\begin{table*}[ht]
\centering
\scriptsize
\resizebox{\textwidth}{!}{
\begin{tabular}{p{4cm}cccccccccc}
\hline
\textbf{\hspace{1cm}Instruction Data}     & \#\textbf{Size} & \textbf{NarrativeQA} & \textbf{Qasper}      & \textbf{HotpotQA}    & \textbf{2WikiQA}    & \textbf{MuSiQue}     & \textbf{GovReport} & \textbf{QMSum}       & \textbf{MultiNews} & \textbf{Avg.}        \\
\hline
\texttt{\hspace{1cm}LLaMA2-7B}        &       &                     &                &                &                &                 &                &                &                &                      \\
ShareGPT~\cite{vicuna2023}                & 89.3k & 15.88               & 23.18          & 22.15          & 19.34          & 9.90            & 29.57          & 24.19          & 26.44          & 21.33                \\
\hdashline     
\textit{open-source long-context data}    &       &                     &                &                &                &                 &                &                &                &                      \\
+ LongAlpaca~\cite{chen2024longlora}      & 12.0k & 18.00               & 26.82          & 24.37          & 20.05          & 11.00           & 30.00          & 25.75          & 26.85          & 22.86                \\
+ LongAlign~\cite{bai2024longalign}       & 9.9k  & 17.90               & 33.21          & 30.63          & 23.91          & 11.70           & 29.81          & 22.77          & 26.61          & 24.57                \\
+ LongMIT~\cite{chen2024essential}        & 64.4k & 19.74               & 34.10          & 41.27          & 26.03          & 21.90           & 28.12          & 23.94          & 26.26          & 27.67                     \\
\hdashline 
\textit{controlled comparison}            &            &                &                &                &                &                 &                &                &                &                      \\
+ Instruction Synthesis~\cite{bai2024longalign} & 1.6k & 19.33          & 31.74          & 28.26          & 24.27          & 14.74           & 29.05          & 23.85          & 26.02          & 24.66                \\
+ Context Synthesis (ours)                & 1.6k       & \textbf{27.10} & \textbf{40.55} & \textbf{47.92} & \textbf{32.69} & \textbf{29.09}  & \textbf{32.03} & \textbf{27.17} & \textbf{30.81} & \textbf{33.42}       \\
\hline
\texttt{\hspace{1cm}LLaMA3.1-8B}     &            &                &                &                &                 &                &                &                &                &                      \\
UltraChat~\cite{ding2023enhancing}        & 515.3k     & 22.45          & 28.12          & 24.00          & 19.38           & 9.08           & 30.24          & 26.18          & 27.36          & 23.35                \\
\hdashline
\textit{open-source long-context data}    &            &                &                &                &                 &                &                &                &                &                      \\
+ LongAlpaca~\cite{chen2024longlora}      & 12.0k      & 22.80          & 28.88          & 22.94          & 22.33           & 10.57          & 30.64          & 26.18          & 27.55          & 23.99                \\
+ LongAlign~\cite{bai2024longalign}       & 9.9k       & 17.02          & 25.70          & 11.54          & 12.22           & 7.82           & 29.65          & 25.65          & 27.23          & 19.60                \\
+ LongMIT~\cite{chen2024essential}        & 64.4k      & 25.08          & 37.71          & 34.35          & 29.68           & 18.78          & 29.83          & 25.47          & 27.42          & 28.54                \\
\hdashline
\textit{controlled comparison}            &            &                &                &                &                 &                &                &                &                &                      \\
+ Instruction Synthesis~\cite{bai2024longalign} & 1.6k & 24.39          & 29.32          & 30.26          & 21.68           & 14.99          & 29.85          & 25.60          & 27.02          & 25.39                \\
+ Context Synthesis (ours)                & 1.6k       & \textbf{32.74} & \textbf{45.30} & \textbf{59.73} & \textbf{44.28}  & \textbf{32.20} & \textbf{35.82} & \textbf{27.79} & \textbf{30.70} & \textbf{38.57}       \\
\hline
\end{tabular}
}
\caption{This table illustrates model performance between using general instruction data alone and using additional long-context instruction data (rows with `+'), and compares our context synthesis approach against the previous instruction synthesis approach in a controlled setting. We also report performance using other open-source long-context instruction data for reference. Bold text denotes the highest score among instruction-tuned models.}
\label{tab:compare}
\end{table*}
\endgroup

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{figure/figure_human_gap.pdf}
    \caption{In this figure we compare model performance after instruction-tuning contrasting instruction synthesis with our approach of context synthesis. In both cases, we compare tuning without context (diagonal lines) with tuning with context (solid bars). We also illustrate the gap between synthesized data and oracle human-annotated data (red dotted line). Experiments are conducted with \texttt{LLaMA3.1-8B}.}
    \label{fig:human_gap}
\end{figure*}
\subsection{Main Results}
\label{sec:main}

\paragraph{Context Synthesis Approach Significantly Improves Long-context Performance}
In Table~\ref{tab:compare}, we first demonstrate the benefits of using long-context instruction data over using general-purpose instruction data alone.
We then compare our context synthesis approach against the instruction synthesis approach, while also reporting performance using open-source long-context instruction data for reference, though their construction processes are either transparent or do not allow for a fair comparison.
Results show that incorporating our synthesized instruction data significantly improves the model's long-context performance, achieving the highest scores across all tasks.

\noindent\paragraph{Context Synthesis Outperforms Instruction Synthesis on All Evaluated Tasks}
While instruction synthesis, which prioritizes long text lengths, shows improvement over using general instruction data alone in most cases, it yields suboptimal performance compared to the context synthesis approach (Table~\ref{tab:compare}).
Our experiments with LongAlpaca and LongAlign on the cutting-edge LLM (\texttt{LLaMA3.1}) show almost no performance improvement, which aligns with findings in \citet{gao2024train}.
We attribute this performance gap to the instruction quality, rather than suggesting context-aware instruction data is unnecessary, which will be quantitatively analyzed later (Section~\ref{sec:analysis}).

\noindent\paragraph{Context Synthesis Is Closing the Gap with Human-annotated Data}
The primary objective for synthesizing long-context instruction data is to achieve the effectiveness of human-annotated data with minimal cost.
Our experiments also enable a controlled comparison between data synthesis strategies and human-annotated long-context instruction data (Figure~\ref{fig:human_gap}).
Results show that the instruction synthesis approach significantly underperforms human-annotated data.
In contrast, our context synthesis approach achieves comparable or superior performance to human-annotated training data on single-document question-answering and summarization tasks.
Nevertheless, a performance gap persists in multi-document question-answering tasks, suggesting the need for further research in this direction.