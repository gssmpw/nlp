\section{Analysis}
\label{sec:analysis}

\paragraph{Revealing Limitations of Instruction Synthesis with Our Analytical Tool}
As introduced in Section~\ref{sec:context-free-tuning}, we employ context-free tuning as an analytical tool to assess the quality of synthesized instruction data (Figure~\ref{fig:human_gap}).
Notably, augmenting synthesized instructions with long text messages yields no additional improvements, suggesting that additional context information, although long, fails to provide meaningful learning signals.
In contrast, pairing instructions with our synthesized context leads to performance gains across different tasks, highlighting the compatibility between instructions and corresponding synthesized contexts.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/figure_length_violin.pdf}
    \caption{In the left panel, we present a task-wise performance comparison of different synthesis strategies. In the right panel, we display the context length distribution of different synthesis strategies against test sets across different tasks.}
    \label{fig:length_and_score}
\end{figure*}

\noindent\paragraph{Analysis of Length Generalization During Instruction Tuning}
Figure~\ref{fig:length_and_score} presents a comparative analysis of downstream task performance and context length distribution.
\begingroup
\renewcommand{\arraystretch}{1.3} % Default value: 1
\begin{table}[ht]
\footnotesize
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{cccc}
\toprule
\textbf{Synthesis Engine} & \textbf{Single-Doc QA} & \textbf{Multi-Doc QA} & \textbf{Summ.} \\
\midrule
\texttt{LongWriter-8B} & 38.85 & 44.68 & 30.79 \\
\texttt{Qwen2.5-72B}   & 38.30 & 44.90 & 31.53 \\
\hdashline
\texttt{GPT4o-mini}    & 39.02 & 45.40 & 31.44 \\
\bottomrule
\end{tabular}
}
\caption{Average performance of using different LLMs as context synthesis engines across three groups of downstream tasks. All experiments are conducted with \texttt{LLaMA3.1-8B}.}
\label{tab:coherent}
\end{table}
\endgroup
Even without context concatenation, our model trained on shorter contexts generalizes effectively to long-context tasks.
These findings validate our observations on length generalization.
This also reveals an efficient instruction-tuning recipe that achieves strong performance with reduced data length.

\noindent\paragraph{Comparative Analysis of Different LLMs as Data Engines}
We evaluate various LLMs as data engines for context synthesis, including both proprietary models like GPT4o-mini and open-source models such as \texttt{LongWriter-8B} and \texttt{Qwen2.5-72B-instruct}.
Experimental results in Table~\ref{tab:coherent} demonstrate comparable performance across different engines, validating the robustness of our context synthesis approach.

\noindent\paragraph{Evaluating Generalization Capability on Unseen Tasks}
Moreover, we check the generalization capability of our data synthesis approach.
We evaluate our instruction-tuned model on tasks that were not seen during training, including the most challenging variant of NIAH task (multi-value NIAH), and \textsc{ZeroSCROLLS}\footnote{For \textsc{ZeroSCROLLS}, we follow \citet{dubey2024llama} and report numbers on the validation set. For QuALITY we report exact match, for SQuALITY we report ROUGE-L.}.
As shown in Table~\ref{tab:unseen}, our data synthesis approach demonstrates strong generalization to these unseen tasks, consistently outperforming the baseline that uses only general instruction data.

\begingroup
\renewcommand{\arraystretch}{1.3}
\begin{table}[]
\footnotesize
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{\textbf{\hspace{0.25cm}Instruction Data}} & \textbf{NIAH}       & \multicolumn{2}{c}{\textbf{ZeroSCROLLS}}   \\
                                        & \textbf{Multi-Value} & \textbf{QuALITY}  & \textbf{SQuALITY}                  \\
\midrule
UltraChat                 & 91.57                  & 71.43                     & 23.02                    \\
+ our synthesis data      & \textbf{96.54} (+4.97) & \textbf{76.19} (+4.76)    & \textbf{23.65} (+0.63)   \\
\bottomrule
\end{tabular}
}
\caption{Results on unseen tasks from \textsc{RULER} and \textsc{ZeroSCROLLS}. NIAH results show the average across different tested lengths. Numbers in parentheses indicate performance improvements. All experiments are conducted with \texttt{LLaMA3.1-8B}.}
\label{tab:unseen}
\end{table}
\endgroup