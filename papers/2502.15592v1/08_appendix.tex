\clearpage
\section{Data format for NIAH Test Variants}
\label{sec:niah-data-format}
In Table~\ref{tab:niah-template}, we present the data format for NIAH tasks.
For more details, please refer to the original paper~\cite{hsieh2024ruler}.

\begingroup
\renewcommand{\arraystretch}{1.3}
\begin{table*}[]
    \footnotesize
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cp{0.95\linewidth}}
    \toprule
    \begin{tabular}{@{}c@{}}Single NIAH\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Context:} \\
    Some special magic numbers are hidden within the following text. Make sure to memorize it. I will quiz you about the numbers afterwards.\\
    \textcolor{lightgray}{Paul Graham Essays.} \\
    \textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word\}} is: \textcolor{orange}{\{uuid\}}. \textcolor{lightgray}{......}\\
    What is the special magic number for \textcolor{violet}{\{word\}} mentioned in the provided text? \\ \\
    \textbf{Answer Prefix:} \\
    The special magic number for \textcolor{violet}{\{word\}} mentioned in the provided text is\end{tabular}\\

    \midrule

    \begin{tabular}{@{}c@{}}Multi-key NIAH\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Context:} \\
    Some special magic uuids are hidden within the following text. Make sure to memorize it. I will quiz you about the uuids afterwards.\\
    \textcolor{lightgray}{Paul Graham Essays.} \\
    \textcolor{lightgray}{One of the special magic uuids for \{word-1\} is: \{uuid-1\}.} \\
    \textcolor{lightgray}{One of the special magic uuids for \{word-2\} is: \{uuid-2\}.} \\
    \textcolor{lightgray}{......} One of the special magic uuids for \textcolor{violet}{\{word-x\}} is: \textcolor{orange}{\{uuid-x\}}. \textcolor{lightgray}{......}\\
    \textcolor{lightgray}{One of the special magic uuids for \{word-n-1\} is: \{uuid-n-1\}.} \\
    \textcolor{lightgray}{One of the special magic uuids for \{word-n\} is: \{uuid-n\}.} \\
    What is the special magic number for \textcolor{violet}{\{uuid-x\}} mentioned in the provided text? \\ \\
    \textbf{Answer Prefix:} \\
    The special magic number for \textcolor{violet}{\{uuid-x\}} mentioned in the provided text is\end{tabular}\\

    \midrule
    
    \begin{tabular}{@{}c@{}}Multi-query NIAH\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Context:} \\
    Some special magic numbers are hidden within the following text. Make sure to memorize it. I will quiz you about the numbers afterwards.\\
    \textcolor{lightgray}{Paul Graham Essays.} \\
    \textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word-1\}} is: \textcolor{orange}{\{uuid-1\}}. \textcolor{lightgray}{......} \\    
    \textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word-2\}} is: \textcolor{orange}{\{uuid-2\}}. \textcolor{lightgray}{......} \\
    \textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word-3\}} is: \textcolor{orange}{\{uuid-3\}}. \textcolor{lightgray}{......} \\
    \textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word-4\}} is: \textcolor{orange}{\{uuid-4\}}. \textcolor{lightgray}{......}\\
    What are all the special magic numbers for \textcolor{violet}{\{word-1\}}, \textcolor{violet}{\{word-2\}}, \textcolor{violet}{\{word-3\}}, and \textcolor{violet}{\{word-4\}} mentioned in the provided text? \\ \\
    \textbf{Answer Prefix:} \\
    The special magic numbers for \textcolor{violet}{\{word-1\}}, \textcolor{violet}{\{word-2\}}, \textcolor{violet}{\{word-3\}}, and \textcolor{violet}{\{word-4\}} mentioned in the provided text are\end{tabular}\\

    \midrule
    
    \begin{tabular}{@{}c@{}}Multi-value NIAH\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Context:} \\
    Some special magic numbers are hidden within the following text. Make sure to memorize it. I will quiz you about the numbers afterwards.\\
    \textcolor{lightgray}{Paul Graham Essays.} \\
    \textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word\}} is: \textcolor{orange}{\{uuid-1\}}. \textcolor{lightgray}{......}\\
    \textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word\}} is: \textcolor{orange}{\{uuid-2\}}. \textcolor{lightgray}{......}\\
    \textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word\}} is: \textcolor{orange}{\{uuid-3\}}. \textcolor{lightgray}{......}\\
    \textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word\}} is: \textcolor{orange}{\{uuid-4\}}. \textcolor{lightgray}{......}\\
    What are all the special magic numbers for \textcolor{violet}{\{word\}} mentioned in the provided text? \\ \\
    \textbf{Answer Prefix:} \\
    The special magic numbers for \textcolor{violet}{\{word\}} mentioned in the provided text are\end{tabular}\\
    \bottomrule
    \end{tabular}}
    \caption{Data formats for different NIAH tasks: Single NIAH, Multi-key NIAH, Multi-query NIAH and Multi-value NIAH.}
    \label{tab:niah-template}
\end{table*}
\endgroup

\section{Template for Instruction Synthesis}
For instruction synthesis, we adopt the template shown in Figure~\ref{fig:prompt-template-qa-synthesis}, which prompts the model to generate instruction-answer pairs from given context.
While this template does not constrain the instruction type, we also experiment with a task-specific template (Figure~\ref{fig:prompt-task-template-context-synthesis}) that explicitly specifies the instruction type - generating summarization instructions for summarization tasks (GovReport, MultiNews, QMSum), generating multi-hop questions for multi-document QA tasks (2WikiMultihopQA, HotpotQA, Musique) and generating single-hop questions for single-document QA tasks (NarrativeQA, Qasper).
As shown in Table~\ref{tab:performance-task-specific-template}, the template (Figure~\ref{fig:prompt-template-qa-synthesis}) we apply in our main experiments produce higher performance.

\begin{figure}[htb]
  \centering\small
  \textbf{System prompt:}
  \vspace{1ex}
  \noindent\framebox{%
  \parbox{0.46\textwidth}{
  \texttt{Please create a question and its answer based on the background text given to you. Always begin the question with ``Question:'' and then begin the answer with ``Answer:''. Do not provide any explanation.}
  }%
  }
  \vspace{1ex}
  \textbf{User content:}
  \vspace{1ex}
  \noindent\framebox{%
  \parbox{0.46\textwidth}{
  \texttt{Context:\\
  <context>\\
  \\
  The above is a piece of text providing some background information. Write a question based on this context and then provide the corresponding answer. One must be able to infer the answer from the context information.}
  }%
  }
\caption{The prompt template for synthesizing an instruction-answer pair from a given context. The template takes a context passage as input, where \texttt{<context>} is are replaced with the actual context text. The system prompt ensures the output follows a consistent format, while the user content guides the LLM to generate instruction-answer pair based on the given context.}
\label{fig:prompt-template-qa-synthesis}
\end{figure}

\begin{figure}[ht!]
  \centering\small
  \textbf{System prompt:}
  \vspace{1ex}
  \noindent\framebox{%
  \parbox{0.46\textwidth}{
  \texttt{Please create a question and its answer based on the background text given to you. Always begin the question with ``Question:'' and then begin the answer with ``Answer:''. Do not provide any explanation.}
  }%
  }
  \vspace{1ex}
  \textbf{User content} (GovReport, MultiNews, QMSum):
  \vspace{1ex}
  \noindent\framebox{%
  \parbox{0.46\textwidth}{
  \texttt{Context:\\
  <context>\\
  \\
  The above is a piece of text providing some background information. Write a question seeking a summary across the entire context and then provide the corresponding answer. One must be able to infer the answer from the context information.}
  }%
  }
  \vspace{1ex}
  \textbf{User content} (2WikiMultihopQA, HotpotQA, Musique):
  \vspace{1ex}
  \noindent\framebox{%
  \parbox{0.46\textwidth}{
  \texttt{Context:\\
  <context>\\
  \\
  The above is a piece of text providing some background information. Write a question requiring multi-hop reasoning across the entire context and then provide the corresponding answer. One must be able to infer the answer from the context information.}
  }%
  }
  \vspace{1ex}
  \textbf{User content} (NarrativeQA, Qasper)
  \vspace{1ex}
  \noindent\framebox{%
  \parbox{0.46\textwidth}{
  \texttt{Context:\\
  <context>\\
  \\
  The above is a piece of text providing some background information. Write a question seeking information from the context and then provide the corresponding answer. One must be able to infer the answer from the context information.}
  }%
  }
\caption{The task-constrained prompt template for synthesizing an instruction-answer pair from a given context.}
\label{fig:prompt-task-template-context-synthesis}
\end{figure}


\section{Examples of Synthesized Context}
\label{sec:example-synthesized-context}
We present examples of our synthesized context from question-answering tasks (Table~\ref{tab:synthesis-context-qa-case}) and summarization tasks (Table~\ref{tab:synthesis-context-sum-case}) to help readers better understand the benefit of our approach.
Taking the first example in Table~\ref{tab:synthesis-context-qa-case}, the evidence in the synthesized context is distributed across different parts of the text, while detailed background information serve as challenging distractors.
We suggest that this context composition helps the model learn robust patterns for context utilization.

\begingroup
\renewcommand{\arraystretch}{1.3}
\begin{table*}[]
    \scriptsize
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{p{0.95\linewidth}}
    \toprule
    \textbf{<Synthesized Context>} \\
    Harry Kane, born on July 28, 1993, in Walthamstow, London, is a prominent English footballer known for his impressive goal-scoring ability and leadership on the pitch. From a young age, Kane displayed a passion for football and began his journey in the sport through local youth teams. He joined the Tottenham Hotspur academy at the age of 11, where he developed his skills and nurtured his ambition to become a professional footballer.\\
    Kane's rise through the ranks was not without challenges. He faced setbacks including being loaned out to various lower-league clubs like Leyton Orient, Millwall, Norwich City, and Leicester City, where he honed his skills but struggled to secure a permanent spot. Despite these difficulties, Kane's determination and work ethic paid off when he finally broke into the Tottenham first team during the 2014-2015 season. \\
    As a forward, Kane quickly established himself as a key player for Tottenham, displaying a remarkable ability to score goals from various positions on the pitch. His playing style is characterized by a combination of technical skill, tactical intelligence, and physicality. Additionally, he is known for his versatility, being able to play both as a striker and as a deeper-lying forward, contributing assists to his teammates. \\
    Kane's accomplishments at club level have earned him numerous accolades, including multiple Premier League Golden Boot awards, recognizing him as the league's top scorer. His remarkable performances have significantly contributed to Tottenham's competitiveness in both the Premier League and European competitions. \\
    In parallel with his club success, Kane has also made significant contributions to the England national team. He made his senior debut in 2015 and quickly became an integral part of the national squad. His leadership qualities shone through when he was appointed the captain of the national team, leading them through pivotal matches, including the FIFA World Cup 2018, where he finished as the tournament's top scorer. \\
    Throughout his career, Kane has been associated with Nike, opting to wear the Nike Hypervenom football boot, which is designed for agility and precision - a perfect match for his style of play. His affinity for these boots is a testament to his competitive edge and dedication to performing at his best. \\
    As Kane progresses in his career, he continually strives to achieve greater heights, including aspirations for club trophies and international honors. The expectations placed on him by fans, media, and himself create an environment filled with pressure, yet he remains focused on his ambitions. Balancing his professional responsibilities with personal life, Kane shares experiences with his family, which provides him with support and motivation. \\ 
    With a strong relationship with his coach and teammates, Kane fosters an atmosphere of camaraderie within the squad, encouraging younger players and leading by example. His professionalism and commitment to the sport have made him a role model for aspiring footballers across the globe. \\
    Through his journey, Kane embodies the spirit of perseverance, determination, and excellence, showcasing what it means to be a professional athlete in today's competitive sports landscape. \\
    \textbf{<Instruction>} \\
    English footballer, Harry Kane, wears the Nike Hypervenom football boot, and also plays as a forward for what teams? \\
    \textbf{<Answer>} \\
    Premier League club Tottenham Hotspur and the England national team \\
    \hline
    \textbf{<Synthesized Context>} \\
    In a recent study on the efficacy of automated question-answering systems, researchers were tasked with assessing the accuracy of a dataset compiled from various sources. The dataset consisted of 1,000 question-answer pairs related to multiple subjects, including science, history, technology, and culture. Initially, the dataset was created using a mix of crowd-sourced responses and automatically generated answers. To ensure the quality and reliability of the information, the research team set out to evaluate the correctness of each pair. \\
    To do this, they employed a team of six annotators - graduate students from several disciplines, including computer science, linguistics, and education. The annotators were trained to review the question-answer pairs based on a detailed rubric developed by the researchers. This rubric included criteria such as factual accuracy, relevance to the question, clarity of the answer, and overall coherence. \\
    The annotators were tasked with individually assessing each question-answer pair and providing feedback. After the individual evaluations, the team convened to discuss their findings and reach a consensus on the correctness of each pair. A crucial part of this process was the inter-annotator agreement, which measured the level of agreement among the annotators regarding the correctness of the pairs. \\
    Upon reviewing the pairs, it was found that the inter-annotator agreement was high, with an average agreement score of 85\%. This indicated a strong level of consistency among annotators. To further ensure quality, it was stipulated that the correctness of all the question-answer pairs had to be verified by at least two annotators before they could be deemed reliable. This measure was put in place to eliminate any potential biases and discrepancies that may arise from individual assessments. \\
    The study aimed to explore not only the reliability of the datasets used in existing automated systems but also the quality of answers provided by such systems. The goal was to contribute meaningful insights into the development of more effective AI tools that could provide accurate and relevant information to users. The researchers recognized the challenges posed by the rapidly changing nature of knowledge and how that impacts the creation of question-answer pairs. \\
    Ultimately, the reliability of the dataset was crucial, as it would be employed in further testing of the automated systems, aiming to improve their accuracy and user satisfaction. The research team hoped to publish their findings in a peer-reviewed journal, contributing to the field of artificial intelligence and educational technology by highlighting the importance of quality control in data used for training machine learning models. \\
    \textbf{<Instruction>} \\
    What was the inter-annotator agreement? \\
    \textbf{<Answer>} \\
    Correctness of all the question answer pairs are verified by at least two annotator. \\
    \bottomrule
    \end{tabular}
    }
    \caption{Examples of the synthesized background context for instruction-answer pairs generated by \texttt{GPT4o-mini} in the question-answering task. For the sake of space, we only show the relevant background context. In practice, additional unrelevant context is concatenated to reach longer length.}
    \label{tab:synthesis-context-qa-case}
\end{table*}
\endgroup

\begingroup
\renewcommand{\arraystretch}{1.3}
\begin{table*}[ht]
    \scriptsize
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{p{0.95\linewidth}}
    \toprule
    \textbf{<Synthesized Context>} \\
    In the early 2010s, the American public was becoming increasingly aware of the growing obesity epidemic and its links to unhealthy eating habits. First Lady Michelle Obama, concerned about the health of children and families in the United States, initiated the \"Let's Move!\" campaign in 2010 aimed at reducing childhood obesity and promoting healthy eating and physical activity. The focus was on increasing access to healthy foods, particularly in low-income communities, where access to fresh produce was often limited due to food deserts.\\
    During this period, Wal-Mart, the largest retailer in the United States, was facing criticism for contributing to unhealthy eating habits through the low-cost, processed foods it sold. Recognizing its potential to influence consumer choices and health outcomes significantly, Wal-Mart sought to improve its image and address these health concerns. The company realized that by modifying its product offerings and focusing on healthier options, it could not only contribute to the public health initiative but also capture a growing market of health-conscious consumers.
    Wal-Mart's CEO and management team held strategic meetings to explore how to implement healthier food options across their stores. They acknowledged that despite Wal-Mart's low prices being praised by many, the foods that gained the most sales - often high in sodium, trans fats, and sugars - had detrimental health effects, particularly for families with limited budgets. They recognized their unique position to make a difference due to their extensive reach.
    Meanwhile, Michelle Obama was seeking partnerships with major corporations to extend the reach of her campaign. She viewed initiatives with retailers like Wal-Mart as instrumental in changing the food landscape in America. After several negotiations and discussions, it was announced that Wal-Mart would reformulate their private label, Great Value, to reduce unhealthy ingredients and increase the availability of fruits, vegetables, and whole grains at lower prices.
    This partnership was publicized at a high-profile press conference attended by media outlets, health food advocates, and community leaders, including Michelle Obama. During her speech, Obama emphasized the importance of better nutrition for children and applauded Wal-Mart's commitment to making healthier options more accessible. Her statement, ``It's a victory for parents. It's a victory for families, but most of all, it's a victory for our children,'' encapsulated the larger narrative that this initiative was more than just a business decision; it was a movement towards a healthier future.
    Reports from industry analysts indicated that Wal-Mart's decision could significantly shape the market, particularly among low-income consumers who relied on Wal-Mart for affordable groceries. With the announcement, there was hope that similar efforts would follow suit from other retailers, leading to widespread changes in food production and sales practices.
    In terms of relationships, this initiative marked a pivotal point in the collaboration between corporate America and government health initiatives, showcasing how partnerships could be leveraged for public good. The dialogue surrounding the announcement also highlighted the shift in tone from the Obama administration towards businesses, encouraging them to participate in efforts that align with public health goals.
    Overall, this movement was positioned as a crucial step in addressing the health crisis affecting a significant portion of the American population, particularly children, and it sparked broader discussions about food justice, corporate responsibility, and the role of major retailers in public health initiatives. \\
    \textbf{<Instruction>} \\
    What is a summary of the news article? \\
    \textbf{<Answer>} \\
    Wal-Mart is embarking on a plan that could have a big impact on American nutrition: It will modify its house brand to make the food healthier, and lower costs for items such as fruits and veggies, and products made with whole grain. The initiative is the result of collaboration with Michelle Obama, reports the New York Times. \"It's a victory for parents. It's a victory for families, but most of all, it's a victory for our children,\" she said at today's announcement, notes ABC News. Industry analysts say that because of its reach, Wal-Mart has almost as much power as federal regulators to shape the market. Crucially, Wal-Mart, and specifically its house brand Great Value, are popular with low-income shoppers - the demographic that eats the most foods high in sodium, trans fats, and refined sugars. Related stories: Washngton Post: The move is a sign of the Obama administration's new tone toward corporate America in general and Wal-Mart in particular. Huffington Post: This could genuinely save lives, writes Dr. Michael F. Jacobson. \\
    \bottomrule
    \end{tabular}
    }
    \caption{An example of the synthesized background context for instruction-answer pairs generated by \texttt{GPT4o-mini} in the summarization task. For the sake of space, we only show the relevant background context. In practice, additional unrelevant context is concatenated to reach longer length.}
    \label{tab:synthesis-context-sum-case}
\end{table*}
\endgroup

\begingroup
\renewcommand{\arraystretch}{1.2}
\begin{table*}[ht]
\centering
\scriptsize
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\hline
\textbf{\hspace{1cm}Instruction Data} & \textbf{NarrativeQA} & \textbf{Qasper}      & \textbf{HotpotQA}    & \textbf{2WikiQA}    & \textbf{MuSiQue}     & \textbf{GovReport} & \textbf{QMSum}       & \textbf{MultiNews} & \textbf{Avg.}        \\
\hline
\texttt{\hspace{1cm}LLaMA3.1-8B}               &                 &            &           &            &               &                 &                &                  &                      \\
UltraChat~\cite{ding2023enhancing}             & 22.45                & 28.12           & 24.00          & 19.38           & 9.08               & 30.24                & 26.18                & 27.36                & 23.35                \\
+ Instruction Synthesis (template {\small\ding{172}})  & 24.39                & 29.32           & 30.26          & 21.68           & 14.99              & 29.85                & 25.60                & 27.02                & 25.39                \\
+ Instruction Synthesis (template {\small\ding{173}})  & 21.47                & 28.11           & 23.80          & 17.37           & 12.49              & 29.79                & 24.45                & 27.44                & 23.12                \\
+ Context Synthesis (ours)                     & \textbf{32.74}       & \textbf{45.30}  & \textbf{59.73} & \textbf{44.28}  & \textbf{32.20}     & \textbf{35.82}       & \textbf{27.79}       & \textbf{30.70}       & \textbf{38.57}       \\
\hline
\end{tabular}
}
\caption{This table compares model performance using different templates for instruction synthesis. Template {\small\ding{172}} refers to the constrain-free template in Figure~\ref{fig:prompt-template-qa-synthesis}, while Template {\small\ding{173}} refer to the task-specific template in Figure~\ref{fig:prompt-task-template-context-synthesis}.}
\label{tab:performance-task-specific-template}
\end{table*}
\endgroup

\section{Instruction-tuning Details}
\label{sec:instruction-tuning-details}
We use AdamW optimizer with $\beta_1$=0.9, $\beta_2$=0.95 for instruction-tuning. 
The learning rate is set to 2e-5 with a cosine decay schedule and 3\% warm-up ratio.
The models are fine-tuned for 2 epochs.
For training efficiency, we employ DeepSpeed ZeRO-3~\cite{rasley2020deepspeed} alongside a packing strategy with loss weighting~\cite{bai2024longalign}.
Specially, we adopt a packing strategy where training samples are packed together, with loss computed only on the output tokens. 
Following the default settings in \textit{LongAlign} codebase, we set the maximum sequence length per batched sample to 65536 and 32768 for \texttt{LLaMA2-7B-64k} and \texttt{LLaMA3.1-8B-128k} respectively.
Training is conducted on 8$\times$H800 GPUs with a per-device batch size of 1.

\section{Experimental Results with ShareGPT}
\label{sec:llama3.1-sharegpt}
Table~\ref{tab:sharegpt} presents the experimental results with ShareGPT on \texttt{LLaMA3.1-8B}. 
The results demonstrates similar findings to those observed in Table~\ref{tab:compare}.

\section{Impact of Context Concatenation Size}
\label{sec:number-of-concatenated-context}
In this paper, we set the concatenation size to ten, consisting of one relevant background context and nine irrelevant contexts.
Table~\ref{tab:number-of-concatenated-context} presents the performance results across different numbers of concatenated contexts.
Our results indicate that larger concatenation sizes generally yield better performance.

\begingroup
\renewcommand{\arraystretch}{1.2}
\begin{table*}[ht]
\centering
\scriptsize
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\hline
\textbf{\hspace{0.6cm}Instruction Data}    & \textbf{NarrativeQA} & \textbf{Qasper}      & \textbf{HotpotQA}    & \textbf{2WikiQA}    & \textbf{MuSiQue}     & \textbf{GovReport} & \textbf{QMSum}       & \textbf{MultiNews} & \textbf{Avg.}        \\
\hline
\texttt{\hspace{0.6cm}LLaMA3.1-8B}         &                 &            &           &            &               &                 &                &                  &                      \\
ShareGPT~\cite{vicuna2023}               & 23.35           & 23.48      & 30.98     & 24.67      & 10.19         & 29.89           & 23.60          & 28.09            & 24.28                \\
+ Instruction Synthesis                  & 26.32           & 32.19             & 28.80          & 28.86           & 14.81             & 30.78            & 24.46           &  27.58         & 26.73                \\
+ Context Synthesis (ours)               & \textbf{31.16}  & \textbf{41.02}    & \textbf{54.27} & \textbf{38.92}  & \textbf{28.03}    & \textbf{35.06}   & \textbf{26.99}  & \textbf{31.02} & \textbf{35.81}       \\
\hline
\end{tabular}
}
\caption{This table illustrates model performance between using general instruction data (ShareGPT) alone and using additional long-context intruction data (rows with `+').}
\label{tab:sharegpt}
\end{table*}
\endgroup

\begingroup
\renewcommand{\arraystretch}{1.2}
\begin{table*}[ht]
\centering
\scriptsize
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\hline
\textbf{\hspace{0.4cm}Instruction Data} & \textbf{NarrativeQA} & \textbf{Qasper}      & \textbf{HotpotQA}    & \textbf{2WikiQA}    & \textbf{MuSiQue}     & \textbf{GovReport} & \textbf{QMSum}       & \textbf{MultiNews} & \textbf{Avg.}        \\
\hline
\texttt{\hspace{0.6cm}LLaMA3.1-8B}          &                 &                &                &                 &               &                 &                &                 &                 \\
UltraChat~\cite{ding2023enhancing}             & 22.45           & 28.12          & 24.00          & 19.38           & 9.08          & 30.24           & 26.18          & 27.36           & 23.35           \\
+ Context Synthesis (n=1)               & 32.61           & 44.46          & 58.08          & 38.35           & \textbf{32.91} & 35.81          & 27.10          & \textbf{32.03}  & 37.67           \\
+ Context Synthesis (n=5)               & 32.10           & 42.08          & \textbf{60.15} & \textbf{45.34}  & 30.15          & \textbf{35.98} & 26.51          & 31.04           & 37.92           \\
+ Context Synthesis (n=10)              & \textbf{32.74}  & \textbf{45.30} & 59.73          & 44.28           & 32.20          & 35.82          & \textbf{27.79} & 30.70           & \textbf{38.57}  \\
\hline
\end{tabular}
}
\caption{Performance comparison across different concatenation sizes (n). n represents the number of concatenated contexts, including one relevant context and n-1 irrelevant contexts. Results show that larger concatenation sizes generally lead to better overall performance.}
\label{tab:number-of-concatenated-context}
\end{table*}
\endgroup

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figure/figure_without_context_llama3.1.pdf}
    \caption{In this figure we compare tuning without context (diagonal lines) with tuning with context (solid bars) and assess context-instruction coherence in synthetic instruction data (LongAlign, LongMIT). Experiments are conducted with \texttt{LLaMA3.1-8B}.}
    \label{fig:longalign-longmit}
\end{figure*}

\section{Context-Instruction Coherence Analysis}
\label{sec:longalign-longmit-context-free-tuning}
With our proposed analytic tool, we measure the context-instruction coherence in synthetic instruction data (LongAlign, LongMIT) by previous instruction sysnthesis approaches.
Experimental results are depicted in Figure~\ref{fig:longalign-longmit}.
For LongAlign, we observe minimal difference between context-included and context-free tuning, suggesting poor context-instruction coherence in their synthetic data.
While LongMIT enhances the quality of synthetic data through a carefully designed multi-agent workflow for question-answering tasks, it has limited generalizability across different tasks and achieves lower performance compared to our approach.

\section{Used Scientific Artifacts}
Below lists scientific artifacts that are used in our work. For the sake of ethic, our use of these artifacts is consistent with their intended use.
\begin{itemize} [itemsep=1pt]
    \item \textit{LongAlign (Apache-2.0 license)}, a codebase developed for long-context instruction-tuning. 
    \item \textit{RULER (Apache-2.0 license)}, a repository for generating synthetic examples to evaluate long-context language models with configurable sequence length and task complexity. 
    \item \textit{LongBench (MIT license)}, a benchmark designed for assessing the long-context capabilities of large language models.
    \item \textit{ZeroScrolls (MIT license)}, a benchmark for evaluating the long-context capabilities of large language models.
    \item \textit{LLaMA2-7B-64K (Apache-2.0 license)}, a continued pretrained version of \texttt{LLaMA2-7B} with an extended 64k context window.
    \item \textit{LLaMA-3.1 (LLaMA3.1 license)}, a large language model developed by Meta. 
    \item \textit{GPT4o-mini (Proprietary license)}, a large language model developed by OpenAI.
    \item \textit{Qwen-2.5-72B (Qwen license)}, a large language model developed by Qwen.
\end{itemize}