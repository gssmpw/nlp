% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{cattan2024can,
  title={Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations},
  author={Cattan, Arie and Jacovi, Alon and Fabrikant, Alex and Herzig, Jonathan and Aharoni, Roee and Rashkin, Hannah and Marcus, Dror and Hassidim, Avinatan and Matias, Yossi and Szpektor, Idan and others},
  journal={arXiv preprint arXiv:2406.13632},
  year={2024}
}

@article{liu2024lost,
    title = "Lost in the Middle: How Language Models Use Long Contexts",
    author = "Liu, Nelson F.  and
      Lin, Kevin  and
      Hewitt, John  and
      Paranjape, Ashwin  and
      Bevilacqua, Michele  and
      Petroni, Fabio  and
      Liang, Percy",
    journal = "Transactions of the Association for Computational Linguistics (TACL)",
    year = "2024",
    url = "https://aclanthology.org/2024.tacl-1.9",
}

@misc{levy2024same,
      title={Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models}, 
      author={Mosh Levy and Alon Jacoby and Yoav Goldberg},
      year={2024},
      eprint={2402.14848},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.14848}, 
}

@misc{hsieh2024found,
      title={Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization}, 
      author={Cheng-Yu Hsieh and Yung-Sung Chuang and Chun-Liang Li and Zifeng Wang and Long T. Le and Abhishek Kumar and James Glass and Alexander Ratner and Chen-Yu Lee and Ranjay Krishna and Tomas Pfister},
      year={2024},
      eprint={2406.16008},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.16008}, 
}

@misc{karpinska2024one,
      title={One Thousand and One Pairs: A "novel" challenge for long-context language models}, 
      author={Marzena Karpinska and Katherine Thai and Kyle Lo and Tanya Goyal and Mohit Iyyer},
      year={2024},
      url={https://arxiv.org/abs/2406.16264}, 
}

@inproceedings{karypis2024extending,
    title = "Extending Input Contexts of Language Models through Training on Segmented Sequences",
    author = "Karypis, Petros  and
      McAuley, Julian  and
      Karypis, George",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    year = "2024",
    url = "https://aclanthology.org/2024.findings-naacl.191",
}

@article{wang2024novelqa,
  title={Novelqa: A benchmark for long-range novel question answering},
  author={Wang, Cunxiang and Ning, Ruoxi and Pan, Boqi and Wu, Tonghui and Guo, Qipeng and Deng, Cheng and Bao, Guangsheng and Wang, Qian and Zhang, Yue},
  journal={arXiv preprint arXiv:2403.12766},
  year={2024}
}

@inproceedings{bai2024longalign,
    title = "{L}ong{A}lign: A Recipe for Long Context Alignment of Large Language Models",
    author = "Bai, Yushi  and
      Lv, Xin  and
      Zhang, Jiajie  and
      He, Yuze  and
      Qi, Ji  and
      Hou, Lei  and
      Tang, Jie  and
      Dong, Yuxiao  and
      Li, Juanzi",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    year = "2024",
    url = "https://aclanthology.org/2024.findings-emnlp.74",
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  url={https://arxiv.org/pdf/2407.21783},
  year={2024}
}

@misc{gao2024train,
      title={How to Train Long-Context Language Models (Effectively)}, 
      author={Tianyu Gao and Alexander Wettig and Howard Yen and Danqi Chen},
      year={2024},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2410.02660}, 
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}


@misc{chiang2023vicuna,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    year = {2023},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
}

@inproceedings{bai2024longbench,
    title = "{L}ong{B}ench: A Bilingual, Multitask Benchmark for Long Context Understanding",
    author = "Bai, Yushi  and
      Lv, Xin  and
      Zhang, Jiajie  and
      Lyu, Hongchang  and
      Tang, Jiankai  and
      Huang, Zhidian  and
      Du, Zhengxiao  and
      Liu, Xiao  and
      Zeng, Aohan  and
      Hou, Lei  and
      Dong, Yuxiao  and
      Tang, Jie  and
      Li, Juanzi",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    url = "https://aclanthology.org/2024.acl-long.172",
}


@inproceedings{hsieh2024ruler,
    title={{RULER}: What{\textquoteright}s the Real Context Size of Your Long-Context Language Models?},
    author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Boris Ginsburg},
    booktitle={First Conference on Language Modeling},
    year={2024},
    url={https://openreview.net/forum?id=kIoBbc76Sy}
}


@inproceedings{zhang2024bench,
    title = "$\infty${B}ench: Extending Long Context Evaluation Beyond 100{K} Tokens",
    author = "Zhang, Xinrong  and
      Chen, Yingfa  and
      Hu, Shengding  and
      Xu, Zihang  and
      Chen, Junhao  and
      Hao, Moo  and
      Han, Xu  and
      Thai, Zhen  and
      Wang, Shuo  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    url = "https://aclanthology.org/2024.acl-long.814",
}

@inproceedings{an2024l,
    title = "{L}-Eval: Instituting Standardized Evaluation for Long Context Language Models",
    author = "An, Chenxin  and
      Gong, Shansan  and
      Zhong, Ming  and
      Zhao, Xingjian  and
      Li, Mukai  and
      Zhang, Jun  and
      Kong, Lingpeng  and
      Qiu, Xipeng",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    url = "https://aclanthology.org/2024.acl-long.776",
}

@inproceedings{shaham2023zeroscrolls,
    title = "{Z}ero{SCROLLS}: A Zero-Shot Benchmark for Long Text Understanding",
    author = "Shaham, Uri  and
      Ivgi, Maor  and
      Efrat, Avia  and
      Berant, Jonathan  and
      Levy, Omer",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    year = "2023",
    url = "https://aclanthology.org/2023.findings-emnlp.536",
}

@article{wu2024retrieval,
      title={Retrieval Head Mechanistically Explains Long-Context Factuality}, 
      author={Wenhao Wu and Yizhong Wang and Guangxuan Xiao and Hao Peng and Yao Fu},
      year={2024},
      url={https://arxiv.org/abs/2404.15574},
      journal={arXiv preprint}
}

@inproceedings{wang2023instruct,
    title = "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
    author = "Wang, Yizhong  and
      Kordi, Yeganeh  and
      Mishra, Swaroop  and
      Liu, Alisa  and
      Smith, Noah A.  and
      Khashabi, Daniel  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2023",
    url = "https://aclanthology.org/2023.acl-long.754",
}

@misc{karpinska2024thousand,
      title={One Thousand and One Pairs: A "novel" challenge for long-context language models}, 
      author={Marzena Karpinska and Katherine Thai and Kyle Lo and Tanya Goyal and Mohit Iyyer},
      year={2024},
      url={https://arxiv.org/abs/2406.16264}, 
}

@inproceedings{mohtashami2023landmark,
    title={Landmark Attention: Random-Access Infinite Context Length for Transformers},
    author={Amirkeivan Mohtashami and Martin Jaggi},
    booktitle={Workshop on Efficient Systems for Foundation Models @ ICML2023},
    year={2023},
    url={https://openreview.net/forum?id=PkoGERXS1B}
}

@misc{chen2023extending,
      title={Extending Context Window of Large Language Models via Positional Interpolation}, 
      author={Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},
      year={2023},
      url={https://arxiv.org/abs/2306.15595}, 
}

@inproceedings{chen2024longlora,
    title={LongLo{RA}: Efficient Fine-tuning of Long-Context Large Language Models},
    author={Yukang Chen and Shengju Qian and Haotian Tang and Xin Lai and Zhijian Liu and Song Han and Jiaya Jia},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=6PmJoRfdaK}
}

@article{su2024roformer,
    title = {RoFormer: Enhanced transformer with Rotary Position Embedding},
    journal = {Neurocomputing},
    year = {2024},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231223011864},
    author = {Jianlin Su and Murtadha Ahmed and Yu Lu and Shengfeng Pan and Wen Bo and Yunfeng Liu},
}


@inproceedings{zhu2024pose,
    title={Po{SE}: Efficient Context Window Extension of {LLM}s via Positional Skip-wise Training},
    author={Dawei Zhu and Nan Yang and Liang Wang and Yifan Song and Wenhao Wu and Furu Wei and Sujian Li},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=3Z1gxuAQrA}
}

@inproceedings{tworkowski2023focused,
    title={Focused Transformer: Contrastive Training for Context Scaling},
    author={Szymon Tworkowski and Konrad Staniszewski and Miko{\l}aj Pacek and Yuhuai Wu and Henryk Michalewski and Piotr Mi{\l}o{\'s}},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=s1FjXzJ0jy}
}

@inproceedings{liu2024scaling,
    title={Scaling Laws of Ro{PE}-based Extrapolation},
    author={Xiaoran Liu and Hang Yan and Chenxin An and Xipeng Qiu and Dahua Lin}, booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=JO7k0SJ5V6}
}

@inproceedings{xiong2024effective,
    title = "Effective Long-Context Scaling of Foundation Models",
    author = "Xiong, Wenhan  and
      Liu, Jingyu  and
      Molybog, Igor  and
      Zhang, Hejia  and
      Bhargava, Prajjwal  and
      Hou, Rui  and
      Martin, Louis  and
      Rungta, Rashi  and
      Sankararaman, Karthik Abinav  and
      Oguz, Barlas  and
      Khabsa, Madian  and
      Fang, Han  and
      Mehdad, Yashar  and
      Narang, Sharan  and
      Malik, Kshitiz  and
      Fan, Angela  and
      Bhosale, Shruti  and
      Edunov, Sergey  and
      Lewis, Mike  and
      Wang, Sinong  and
      Ma, Hao",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    year = "2024",
    url = "https://aclanthology.org/2024.naacl-long.260",
}

@misc{rozière2024codellama,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2024},
      url={https://arxiv.org/abs/2308.12950}, 
}

@misc{kamradt2023needle,
  author={Gregory Kamradt},
  title={Needle In A Haystack - Pressure Testing LLMs},
  year={2023},
  url={https://github.com/gkamradt/LLMTestNeedleInAHaystack/tree/main}
}

@misc{touvron2023llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and others},
      year={2023},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    year = {2023}
}

@article{kocisky2018narrativeqa,
    title = "The {N}arrative{QA} Reading Comprehension Challenge",
    author = "Ko{\v{c}}isk{\'y}, Tom{\'a}{\v{s}}  and
      Schwarz, Jonathan  and
      Blunsom, Phil  and
      Dyer, Chris  and
      Hermann, Karl Moritz  and
      Melis, G{\'a}bor  and
      Grefenstette, Edward",
    journal = "Transactions of the Association for Computational Linguistics",
    year = "2018",
    url = "https://aclanthology.org/Q18-1023",
}

@inproceedings{zhong2021qmsum,
    title = "{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization",
    author = "Zhong, Ming  and
      Yin, Da  and
      Yu, Tao  and
      Zaidi, Ahmad  and
      Mutuma, Mutethia  and
      Jha, Rahul  and
      Awadallah, Ahmed Hassan  and
      Celikyilmaz, Asli  and
      Liu, Yang  and
      Qiu, Xipeng  and
      Radev, Dragomir",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2021",
    url = "https://aclanthology.org/2021.naacl-main.472",
}

@misc{fu2024data,
      title={Data Engineering for Scaling Language Models to 128K Context}, 
      author={Yao Fu and Rameswar Panda and Xinyao Niu and Xiang Yue and Hannaneh Hajishirzi and Yoon Kim and Hao Peng},
      year={2024},
      url={https://arxiv.org/abs/2402.10171}, 
}

@inproceedings{peng2024yarn,
    title={Ya{RN}: Efficient Context Window Extension of Large Language Models},
    author={Bowen Peng and Jeffrey Quesnelle and Honglu Fan and Enrico Shippole},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=wHBfxhZu1u}
}


@inproceedings{zhou2023lima,
 author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and YU, LILI and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {LIMA: Less Is More for Alignment},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/ac662d74829e4407ce1d126477f4a03a-Paper-Conference.pdf},
 year = {2023}
}


@article{fang2024wrong,
  title={What is Wrong with Perplexity for Long-context Language Modeling?},
  author={Fang, Lizhe and Wang, Yifei and Liu, Zhaoyang and Zhang, Chenheng and Jegelka, Stefanie and Gao, Jinyang and Ding, Bolin and Wang, Yisen},
  journal={arXiv preprint arXiv:2410.23771},
  year={2024}
}

@article{tan2024lloco,
  title={LLoCO: Learning Long Contexts Offline},
  author={Tan, Sijun and Li, Xiuyu and Patil, Shishir and Wu, Ziyang and Zhang, Tianjun and Keutzer, Kurt and Gonzalez, Joseph E and Popa, Raluca Ada},
  journal={arXiv preprint arXiv:2404.07979},
  year={2024}
}

@inproceedings{levy2024task,
    title = "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
    author = "Levy, Mosh  and
      Jacoby, Alon  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    url = "https://aclanthology.org/2024.acl-long.818",
}

@inproceedings{dao2024flashattention,
    title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
    author={Tri Dao},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=mZn2Xyh9Ec}
}

@inproceedings{ding2023enhancing,
    title = "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
    author = "Ding, Ning  and
      Chen, Yulin  and
      Xu, Bokai  and
      Qin, Yujia  and
      Hu, Shengding  and
      Liu, Zhiyuan  and
      Sun, Maosong  and
      Zhou, Bowen",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",
    url = "https://aclanthology.org/2023.emnlp-main.183",
}

@inproceedings{shaham2022scrolls,
    title = "{SCROLLS}: Standardized {C}ompa{R}ison Over Long Language Sequences",
    author = "Shaham, Uri  and
      Segal, Elad  and
      Ivgi, Maor  and
      Efrat, Avia  and
      Yoran, Ori  and
      Haviv, Adi  and
      Gupta, Ankit  and
      Xiong, Wenhan  and
      Geva, Mor  and
      Berant, Jonathan  and
      Levy, Omer",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
    url = "https://aclanthology.org/2022.emnlp-main.823",
}

@misc{yen2024helmet,
      title={HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly}, 
      author={Howard Yen and Tianyu Gao and Minmin Hou and Ke Ding and Daniel Fleischer and Peter Izsak and Moshe Wasserblat and Danqi Chen},
      year={2024},
      url={https://arxiv.org/abs/2410.02694}, 
}

@inproceedings{sennrich2016improving,
    title = "Improving Neural Machine Translation Models with Monolingual Data",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2016",
    url = "https://aclanthology.org/P16-1009",
}

@inproceedings{wei2022finetuned,
    title={Finetuned Language Models are Zero-Shot Learners},
    author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=gEZrGCozdqR}
}

@misc{bai2024longwriter,
      title={LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs}, 
      author={Yushi Bai and Jiajie Zhang and Xin Lv and Linzhi Zheng and Siqi Zhu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
      year={2024},
      url={https://arxiv.org/abs/2408.07055}, 
}

@misc{quan2024language,
      title={Language Models can Self-Lengthen to Generate Long Texts}, 
      author={Shanghaoran Quan and Tianyi Tang and Bowen Yu and An Yang and Dayiheng Liu and Bofei Gao and Jianhong Tu and Yichang Zhang and Jingren Zhou and Junyang Lin},
      year={2024},
      url={https://arxiv.org/abs/2410.23933}, 
}

@misc{krell2022efficient,
      title={Efficient Sequence Packing without Cross-contamination: Accelerating Large Language Models without Impacting Performance}, 
      author={Mario Michael Krell and Matej Kosec and Sergio P. Perez and Andrew Fitzgibbon},
      year={2022},
      url={https://arxiv.org/abs/2107.02027}, 
}

@misc{openai2024gpt4ocard,
      title={{GPT-4o} System Card}, 
      author={OpenAI},
      year={2024},
      url={https://arxiv.org/abs/2410.21276}, 
}

@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

@misc{openai2024gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{chen2024essential,
      title={What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices}, 
      author={Zhi Chen and Qiguang Chen and Libo Qin and Qipeng Guo and Haijun Lv and Yicheng Zou and Wanxiang Che and Hang Yan and Kai Chen and Dahua Lin},
      year={2024},
      url={https://arxiv.org/abs/2409.01893}, 
}

@article{xu2024chatqa,
  title={Chatqa 2: Bridging the gap to proprietary llms in long context and rag capabilities},
  author={Xu, Peng and Ping, Wei and Wu, Xianchao and Xu, Chejian and Liu, Zihan and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2407.14482},
  year={2024}
}

@inproceedings{liu2024chatqa,
    title={Chat{QA}: Surpassing {GPT}-4 on Conversational {QA} and {RAG}},
    author={Zihan Liu and Wei Ping and Rajarshi Roy and Peng Xu and Chankyu Lee and Mohammad Shoeybi and Bryan Catanzaro},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=bkUvKPKafQ}
}

@misc{qwen2025qwen25,
      title={Qwen2.5 Technical Report}, 
      author={{Qwen Team}},
      year={2025},
      url={https://arxiv.org/abs/2412.15115}, 
}

@misc{li2024long,
      title={Long-context LLMs Struggle with Long In-context Learning}, 
      author={Tianle Li and Ge Zhang and Quy Duc Do and Xiang Yue and Wenhu Chen},
      year={2024},
      eprint={2404.02060},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2404.02060}, 
}

@inproceedings{nallapati2017summarunner,
    author = {Nallapati, Ramesh and Zhai, Feifei and Zhou, Bowen},
    title = {SummaRuNNer: a recurrent neural network based sequence model for extractive summarization of documents},
    year = {2017},
    abstract = {We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.},
    booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
}

@inproceedings{pham2024suri,
    title = "{S}uri: Multi-constraint Instruction Following in Long-form Text Generation",
    author = "Pham, Chau Minh  and
      Sun, Simeng  and
      Iyyer, Mohit",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    year = "2024",
    url = "https://aclanthology.org/2024.findings-emnlp.94/",
}