\section{Introduction}
Recent advances in large language models (LLMs) have significantly extended the length of the context that they are able to ingest by addressing the problems of efficient attention and encoding of positions~\cite{su2024roformer,peng2024yarn,fu2024data,dubey2024llama}.
However, for better performance in downstream long-context tasks such as document-level question answering and summarization~\cite{shaham2023zeroscrolls,bai2024longbench,karpinska2024one}, these models still require instruction-tuning.
Although previous work has looked at synthetic instructions, we still do not understand how to best leverage existing instructions for optimal use with a long-context model.
This is a critical challenge, as it is difficult to create high quality synthetic long-context instructions.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figure/overview.pdf}
    \caption{Illustration of two long-context instruction data synthesis frameworks: \textit{instruction synthesis} and \textit{context synthesis} (ours). The light-colored blocks indicate potentially lower-quality components in the synthesized data samples.}
    \label{fig:illustration}
\end{figure}

Initial work on long-context instructions leverages off-the-shelf LLMs to generate instruction-answer pairs from existing long text passages~\cite{bai2024longalign, dubey2024llama}.
While this method prioritizes context length but overlooks other critical aspects of the synthetic data such as quality, difficulty and diversity.
These aspects are inherently constrained by an underlying paradox: the synthesized data quality relies on an LLM that can understand the lengthy input text---which is the problem it is tackling in the first place.

This paper first presents a pilot study on artificial needle-in-a-haystack tests~\cite{kamradt2023needle, hsieh2024ruler} which allows for rigorous control of different aspects of the instruction data.
This study yields three key findings: (1) instruction quality plays a crucial role in model performance; (2) models instruction-tuned on short contexts can generalize to much longer ones; (3) training with evidence embedded in distracting content helps models develop robust information extraction abilities.

We leverage these findings to design a novel instruction data synthesis approach called ``\textit{context synthesis}'' (Figure~\ref{fig:illustration}) and test it on naturally occurring tasks. 
Specifically, we prompt off-the-shelf LLMs to generate background context from existing instruction-answer pairs.
This approach offers three advantages: (1) in contrast to previous work which synthesizes instructions and target outputs, our synthetic data only forms part of the input to the model rather like back-translation for machine translation~\cite{sennrich2016improving}, preserving the quality of instructions and outputs.
(2) by generating background contexts, we can seamlessly integrate both supporting evidence and distracting information into a coherent narrative.
(3) our approach enables control over context through expansion and concatenation to harness the benefits of training on longer sequences.

We conduct experiments on real-world tasks from \textsc{LongBench}~\cite{bai2024longbench} with two base models \texttt{LLaMA2-7B-64K}~\cite{bai2024longalign} and \texttt{LLaMA3.1-8B-128K}~\cite{dubey2024llama}.
Experimental results demonstrate that our context synthesis approach significantly outperforms the instruction synthesis methods and comes close to the performance of fine-tuning with oracle human-annotated long-context instruction data.
Further analysis comparing instruction tuning with and without context reveals that the performance gains from previous instruction synthesis methods depend minimally on the paired long context, indicating their limitations in ensuring instruction-context alignment.
In contrast, training instructions with our synthesized input context enables LLMs to learn effective patterns for context utilization.
Furthermore, our instruction-tuned models demonstrate robust generalization to unseen tasks from other document-level benchmarks including \textsc{RULER}~\cite{hsieh2024ruler} and \textsc{ZeroScrolls}~\cite{shaham2023zeroscrolls}.
To summarize, our contributions are as follows: 
\begin{itemize}[itemsep=0pt, topsep=0pt, parsep=0pt]
\item We identify key factors in data synthesis for long-context instruction tuning through a controlled study, including instruction quality, context composition, and context length.
\item We propose a novel data synthesis method, context synthesis, that addresses the key factors by generating tailored background context for high-quality instructions.
\item Experimental results demonstrate that our approach outperforms the previous instruction synthesis approach and achieves performance close to using human-annotated data.
\item We devise an analytical tool which reveals the limitations of existing synthesis approaches in data quality.
\end{itemize}