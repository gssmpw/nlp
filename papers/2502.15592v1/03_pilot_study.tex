\section{Pilot Study}
\label{sec:pilot_study}

\subsection{Concept Definitions}
We first introduce essential concepts used throughout the discussion.
An instruction data instance consists of two components: a \textbf{prompt} and an \textbf{answer}.
For general-purpose instruction data, the prompt contains only an instruction that requires the LLM to generate an answer based on its parametric knowledge.
For context-aware instruction data, the prompt contains both a \textbf{context passage} and an \textbf{instruction}.
Typically, the instruction requests a part of the context (i.e., the relevant or evidence context), while the remaining irrelevant context may distract the model from generating an accurate response.
In the following analysis, we systematically control the instruction data to understand the key factors that influence the effectiveness of instruction-tuned LLMs on long-context tasks.

\subsection{Experiment Design}

\paragraph{Testing Scenario}
We adopt the needle-in-a-haystack\footnote{The target ``needles'' embedded in the context are randomly generated words and UUIDs (Universally Unique Identifiers).} (NIAH) test~\cite{kamradt2023needle} for our analysis, which challenges LLMs to locate evidence buried within long contexts.
\begingroup
\renewcommand{\arraystretch}{1.1}
\begin{table}[ht]
\centering
\resizebox{0.48\textwidth}{!}{
\footnotesize
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\makecell{\textbf{General} \\ \textbf{Instruction Data}}} & \multirow{2}{*}{\makecell{\textbf{Context-aware} \\ \textbf{Instruction Data}}} & \multicolumn{2}{c}{\textbf{Context Composition}} \\
 & &  & \textbf{rel. (needle)} & \textbf{irr. (haystack)} \\
\midrule
\texttt{Base}  &  -           & -                  & -                      & -                  \\
\texttt{SFT1}  & \checkmark             & -                  & -                      & -                  \\
\texttt{SFT2}  & \checkmark             & \checkmark         & \texttt{uuid sent}     & -                  \\
\texttt{SFT3}  & \checkmark             & \checkmark         & \texttt{uuid sent}     & \texttt{1k essay}  \\
\texttt{SFT4}  & \checkmark             & \checkmark         & \texttt{uuid sent}     & \texttt{64k essay} \\
\bottomrule
\end{tabular}}
\caption{Overview of instruction-tuning configurations for experiments in Figure~\ref{fig:niah}. The context in context-aware instruction data consists of two parts: relevant context (a sentence containing the target UUID, usually called "needle") and irrelevant context (an essay serving as distractive information, usually called "haystack"). We present the data format for these tasks in Appendix~\ref{sec:niah-data-format}.}
\label{tab:explanation}
\end{table}
\endgroup
We also incorporate its extended variants proposed in \textsc{RULER} benchmark~\cite{hsieh2024ruler}: multi-key, multi-query and multi-value NIAH tests, which requires LLMs to identify multiple pieces of information amid distracting contexts.
We pick these tasks for our pilot study due to their controllable context composition and length. % , allowing for variable control.
We conduct experiments using \texttt{LLaMA2-7B-64k}\footnote{\texttt{LLaMA2-7B-64k} is an extended version of \texttt{LLaMA2}~\cite{touvron2023llama2}, continued pre-trained by Bai et al. to support longer contexts with a context window extended to 64k tokens.} as the base model for instruction-tuning.

\noindent\paragraph{Instruction Data Control}
Table~\ref{tab:explanation} summarizes our experimental configurations for instruction-tuning.
For \texttt{SFT1}, we only use general instruction data from ShareGPT~\cite{vicuna2023}.
The remaining \texttt{SFT} models incorporate specialized context-aware instruction data with varying data composition and length.
We evaluate three training conditions: without distracting context (\texttt{SFT2}), with short distracting context (\texttt{SFT3}), and with long distracting context (\texttt{SFT4}).
To create the context-aware instruction data, we construct 200 NIAH-test-style training samples\footnote{In these training samples, we use the Paul Graham Essays~\cite{kamradt2023needle} as the haystack, consistent with the testing scenario, but incorporating newly generated contents as needles.} for each of the four subtasks, resulting in a total of 800 samples.

\subsection{Empirical Results and Insights}
\paragraph{The Need for Context-aware Instruction Data}
In Figure~\ref{fig:niah}, we compare the base model with the model trained with only general instruction data (\texttt{SFT1}) and with additional specialized context-aware instruction data (\texttt{SFT2}, \texttt{SFT3}, \texttt{SFT4}).
Results show that as the test length increases, \texttt{SFT1} exhibit significant performance divergence and even underperforms the \texttt{Base Model} on single, multi-key and multi-query NIAH tasks.
In contrast, incorporating a pinch of targeted context-aware instruction data (\texttt{SFT2}, \texttt{SFT3}, \texttt{SFT4}) leads to substantial performance improvements. 
This observation leads to our first insight: \textit{unlocking pre-trained long-context LLMs' potential requires specialized instruction data beyond general instruction data.}

\noindent\paragraph{Short-Context Training Generalizes to Long Contexts}
Next, we compare models trained with context-aware instruction data of varying context compositions and data lengths (\texttt{SFT2}, \texttt{SFT3}, \texttt{SFT4}).
As shown in Figure~\ref{fig:niah}, training with distracting context (\texttt{SFT3}, \texttt{SFT4}) significantly improves the model's performance with longer contexts.
In contrast, models trained solely with relevant context (\texttt{SFT2}) may develop shortcuts, leading to performance degradation when exposed to distracting information. 
Notably, \texttt{SFT3} maintains performance above 90\% across all NIAH tasks, even when test lengths reach 32k - far beyond its instruction-tuning context length, suggesting a potentially more efficient and cost-effective training approach.
This observation leads to our second insight: \textit{training with both evidence and distracting contexts, even short ones, is crucial for developing robust generalization to longer contexts}.

\noindent\paragraph{Training with Long Contexts Remains Optimal}
Although \texttt{SFT3} approaches the performance of \texttt{SFT4} with much shorter contexts, \texttt{SFT4} demonstrate almost perfect performance across all evaluated tasks.
The performance gap between the two models is particularly pronounced at the maximum test length in the most challenging task (Multi-value NIAH).
This highlights the important role of context length in instruction data for achieving optimal performance.
This observation leads to our third insight: \textit{training with long-context instruction data achieves optimal performance, especially for more challenging long-context tasks}.