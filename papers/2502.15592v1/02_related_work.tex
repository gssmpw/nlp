\section{Related Work}
\paragraph{Towards Long-context LLMs}
To enable LLMs to support longer context, most previous research focuses on the pre-training stage by modifying rotary position embeddings (RoPE)~\cite{su2024roformer, peng2024yarn} along with continued pre-training on longer sequences~\cite{chen2023extending,rozi√®re2024codellama,chen2024longlora,peng2024yarn,xiong2024effective,fu2024data}.
However, while previous studies observe that current LLMs often struggle with long-context instruction following~\cite{shaham2023zeroscrolls}, few studies systematically investigate the instruction-tuning stage for long-context tasks.
In this paper, we aim to understand critical factors in data synthesis for long-context instruction tuning.

\noindent\paragraph{Long-context Instruction Data Synthesis}
A key challenge for long-context tasks is the scarcity of long-context instruction data.
Initially, \citet{chen2024longlora} released LongAlpaca, a publicly available long-context instruction dataset, though its annotation process is unclear.
Recent works propose ``instruction synthesis'' to prompt LLMs to generate instruction-answer pairs from long documents~\cite{bai2024longalign,xiong2024effective,dubey2024llama}, exemplified by the open-source dataset LongAlign.
However, this approach raises concerns about instruction quality, task complexity~\cite{chen2024essential} and task diversity~\cite{quan2024language}, as current LLMs themselves still struggle with understanding long-context messages.
Subsequently, \citet{chen2024essential} developed a multi-agent workflow to improve synthesis quality for multi-hop reasoning instruction data (LongMIT).
Meanwhile, a contrasting perspective from \citet{gao2024train} suggests that synthetic long-context data offers minimal benefits, arguing that using standard-length general instruction data, e.g., ShareGPT~\cite{chiang2023vicuna} suffices.
Our work, however, reveals limitations in existing instruction synthesis approaches and confirms the importance of long-context instruction data through a novel and effective synthesis framework.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.95\textwidth]{figure/figure_niah_tuning.pdf}
    \caption{Impact of varying instruction tuning configurations on long-context performance. The detailed differences between these configurations is presented in Table~\ref{tab:explanation}. Test length ``$\sim$0k''  means test contexts containing only the relevant information (needle) without any additional content.}
   \label{fig:niah}
\end{figure*}

\noindent\paragraph{Long-context Tasks Evaluation}
Early research focuses on measuring perplexity on long-context data~\cite{chen2023extending,peng2024yarn} and evaluating passkey retrieval~\cite{fu2024data}.
Recent developments introduce real-world benchmarks such as \textsc{ZeroSCROLLS}, \textsc{LongBench}, which assess practical capabilities through document-level tasks.
In this work, we establish our hypotheses via a controllable passkey retrieval task and then evaluate our data synthesis approach on real-world benchmarks to demonstrate practical effectiveness.