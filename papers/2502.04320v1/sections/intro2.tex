

\section{Introduction}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/MMDiTvsMMDiTWithConceptAttention.pdf}
    \vspace{-0.22in}
    \caption{\textbf{\tool{} augments multi-modal DiTs with a sequence of concept embeddings that can be used to produce saliency maps.} (Left) An unmodified multi-modal attention (\layername) layer processes both \textcolor{myblue}{\textbf{prompt}} and \textcolor{myorange}{\textbf{image}} tokens. (Right) \tool{} augments these layers without impacting the image appearance to create a set of contextualized \textcolor{mygreen}{\textbf{concept}} tokens.
    }
    \label{fig:mmattn_vs_mmattn_with_concept_attention_explanatory}
\end{figure*}

% First motivate why should they care about T2I models
Diffusion models have recently gained widespread popularity, emerging as the state-of-the-art approach for a variety of generative tasks, particularly text-to-image synthesis \cite{rombach_high-resolution_2022}. These models transform random noise into photorealistic images guided by textual descriptions, achieving unprecedented fidelity and detail. Despite the impressive generative capabilities of diffusion models, our understanding of their internal mechanisms remains limited. Diffusion models operate as black boxes, where the relationships between input prompts and generated outputs are visible, but the decision-making processes that connect them are hidden from human understanding.

% Put some related work into context, then state their drawback
Existing work on interpreting T2I models has predominantly focused on UNet-based architectures \cite{podell_sdxl_2023, rombach_high-resolution_2022}, which utilize shallow cross-attention mechanisms between prompt embeddings and image patch representations. 
UNet \textit{cross attention maps} can produce high-fidelity saliency maps that predict the location of textual concepts \cite{tang_what_2022} and have found numerous applications in tasks like image editing \cite{hertz_prompt--prompt_2022, chefer_attend-and-excite_2023}. However, the interpretability of more recent  multi-modal diffusion transformers (DiTs) remains underexplored. DiT-based models have recently replaced UNets \cite{ronneberger_u-net_2015} as the state-of-the-art architecture for image generation, with models such as Flux \cite{labs_flux_2023} and SD3 \cite{esser_scaling_2024} achieving breakthroughs in text-to-image generation. The rapid advancement and enhanced capabilities of DiT-based models highlight the critical importance of methods that improve their interpretability, transparency, and safety. % Pinar; improve the ending 

% set the context for what we are doing
In this work, we propose \tool{}, a novel method that leverages the representations of multi-modal DiTs to produce high-fidelity saliency maps that localize textual concepts within images. Our method provides insight into the rich semantics of DiT representations. \tool{} is lightweight and requires no additional training, instead it repurposes the existing parameters of DiT attention layers. \tool{} works by producing a set of rich contextualized text embeddings each corresponding to visual concepts (e.g. ``dragon'', ``sun''). By linearly projecting these \textit{concept embeddings} and the image we can produce rich saliency maps that are even higher quality than commonly used cross attention maps. 


% repurposes the existing parmeters of DiT attention layers, requiring no additional training


% By probing the DiT image representations we can generate interpretable saliency maps that isolate the location of concepts within images. 
% a novel method that leverages the represntations of multi-modal diffusion transformers to


% to provide insight into the representations of large multi-modal diffusion transformers.

% Specifically, we focus on text-to-image attributionâ€”associating simple textual concepts with parts of generated images. The attention layers in DiTs iteratively process embeddings from both text and image modalities, producing richly contextualized textual embeddings that present novel opportunities for interpretability. \tool{} repurposes the parameters of DiT multi-modal attention layers with no additional training to create a set of rich text embeddings, each corresponding to different concepts. 
% \tool{} analyzes the attention mechanisms within these models to generate interpretable maps that demonstrate how specific words from the input text contribute to the formation of various image features (see Fig. \ref{fig:teaser} for an example).


%We discover that the output representations from DiT attention layers can be harnessed to create saliency maps that surpass those generated by the traditional cross-attention methods. This finding underscores the unique capabilities of DiT architectures in enhancing model transparency and interpretability. 

% motivate segmentation task 
We evaluate the efficacy of \tool{} in a zero-shot semantic segmentation task on real world images. 
% To evaluate the efficacy of \tool{}, we apply it to a semantic segmentation task on generated imagery. 
We compare our interpretative maps against annotated segmentations to measure the accuracy and relevance of the attributions generated by our method.   Our experiments and extensive comparisons demonstrate that \tool{} provides valuable insights into the inner workings of these otherwise complex black-box models. By explaining the meaning of the representations of generative models our method paves the way for advancements in interpretability, controllability, and trust in generative AI systems. 

In summary, we contribute:

\begin{itemize}
    \item \textbf{\tool{}, a method for interpreting text-to-image diffusion transformers.} Our method requires no additional training, by leveraging the representations of multi-modal DiTs to generate highly interpretable saliency maps that depict the presence of arbitrary textual concepts (e.g. ``dragon'', ``sky'', etc.) in images (as shown in Figure \ref{fig:teaser}). 
    % , without any additional training. 
    % See Figure \ref{fig:teaser} for an example of results produced by our method compared to several baselines. 
    
    \item \textbf{The novel discovery that the output vectors of attention operations produce higher-quality saliency maps than cross attentions.} \tool{} repurposes the parameters of DiT attention layers to produce a set of rich textual embeddings corresponding to different concepts, something that is uniquely enabled by multi-modal DiT architectures. By performing linear projections between these \textit{concept embeddings} and image patch representations in the attention output space we can produce high quality saliency maps. 
    % linear projections in the output space of attention layers produces higher quality saliency maps than cross attentions. We take advantage of the unique architectural characteristics of DiTs to 
    % \tool{} takes advantage of the unique architectural characteristics of DiTsWe take advnatage of tit is possible to produce more accurate saliency maps than commonly used cross attentions by performing linaer projections on the outputs of the attention operation, rather than between keys and queries.
    % not sure the following is too similar to the 1st item, we might want to combine those later
    % \item Our method leverages the unique architecture of DiTs to produce saliency maps that are more accurate and detailed than traditional cross-attention methods, offering a clearer view of how textual concepts are visually represented in generated images.
    \item  \textbf{\tool{} generalizes to achieve state-of-the-art performance in zero-shot segmentation on benchmarks like ImageNet Segmentation and Pascal VOC}. We achieve superior performance to a diverse set of zero-shot interpretability methods based on various foundation models like CLIP, DINO, and UNet-based diffusion models; this highlights the potential for the representations of DiTs to transfer to important downstream vision tasks like segmentation. 
    
    % Additionally, we apply \tool{} to image editing tasks, significantly enhancing the precision of feature localization and manipulation for more accurate edits. 
    % This versatility shows our method's broad applicability across different domains and tasks.
    \item \textbf{We make \tool{} available}, allowing researchers and practitioners to interpret and explore the intricate dynamics of text-to-image diffusion transformers. See code at: \href{https://github.com/helblazer811/ConceptAttention}{https://github.com/helblazer811/ConceptAttention}.
\end{itemize}




% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figures/UNetVersusMMAttn.pdf}
%     \vspace{-0.2in}
%     \caption{(Left) UNet-based diffusion models leverage (a) shallow projections of text embeddings into each of the UNet Blocks.  (Right) In contrast, Multi-modal DiT attention layers (b) iteratively update both text embeddings and image patches every layer.  Additionally, (c) \layername's perform a two way communication between the text and image tokens. }
%     \label{fig:enter-label}
% \end{figure*}
