
% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/VerticalQualitativeExample.pdf}
%     \vspace{-0.22 in}
%     \caption{\textbf{Comparison of \tool{} saliency maps for multiple concepts simultaneously compared to several baselines. }}
%     \label{fig:qualitative_comparison_dog_and_ball}
% \end{figure}



\begin{table*}[t!]
    \centering
    \small
    \begin{tabular}{p{0.6\columnwidth}p{0.24\columnwidth}p{0.12\columnwidth}p{0.12\columnwidth}p{0.12\columnwidth}p{0.12\columnwidth}p{0.12\columnwidth}p{0.12\columnwidth}}
        \toprule
        & &  \multicolumn{3}{c}{ImageNet-Segmentation} &  \multicolumn{3}{c}{PascalVOC (Single Class) } \\ 
        Method & Architecture & Acc $\uparrow$ & mIoU$\uparrow$ & mAP$\uparrow$ & Acc $\uparrow$ & mIoU$\uparrow$ & mAP$\uparrow$  \\
        \midrule
        LRP  \cite{binder_layer-wise_2016}    & CLIP ViT & 51.09 & 32.89 & 55.68 & 48.77 & 31.44 & 52.89 \\
        Partial-LRP \cite{binder_layer-wise_2016} & CLIP ViT & 76.31 & 57.94 & 84.67 & 71.52 & 51.39 & 84.86 \\
        Rollout \cite{abnar_quantifying_2020} & CLIP ViT & 73.54 & 55.42 & 84.76 & 69.81 & 51.26 & 85.34 \\
        ViT Attention \cite{dosovitskiy_image_2021} & CLIP ViT & 67.84 & 46.37 & 80.24 & 68.51 & 44.81 & 83.63 \\
        GradCAM \cite{selvaraju_grad-cam_2020} & CLIP ViT & 64.44 & 40.82 & 71.60 & 70.44 & 44.90 & 76.80 \\
        TextSpan  \cite{gandelsman_interpreting_2024} & CLIP ViT & 75.21 & 54.50 & 81.61 & 75.00 & 56.24 & 84.79 \\
        TransInterp \cite{chefer_transformer_2021}  & CLIP ViT & 79.70 & 61.95 & 86.03 & 76.90 & 57.08 & 86.74 \\
        % \midrule
        DINO Attention \cite{caron_emerging_2021} & DINO ViT & 81.97 & 69.44 & 86.12 & 80.71 & 64.33 & 88.90 \\
        % DINO \cite{caron_emerging_2021} & DINOv2 ViT & - & - & - \\
        % \midrule
        DAAM \cite{tang_what_2022} & SDXL UNet & 78.47 & 64.56 & 88.79 & 72.76 & 55.95 & 88.34 \\
        DAAM \cite{tang_what_2022} & SD2 UNet & 64.52 & 47.62 & 78.01 & 64.28 &  45.01 & 83.04 \\
        % DAAM (5 steps) \cite{tang_what_2022} & SDXL UNet & - & - & - & 74.51 & 58.47 & 89.67 \\
        % DAAM \cite{tang_what_2022} & SD 1.5 UNet & - & - & - \\
        % \midrule 
        Flux Cross Attention & Flux DiT & 74.92 & 59.90 & 87.23 & 80.37 & 54.77 & 89.08 \\
        % \midrule
        % \rowcolor[HTML]{FBEAE4} Ours (CA) & Flux DiT & 74.92 & 59.90 & 87.23 & 80.37 & 54.77 & 89.08 \\ 
        % \rowcolor[HTML]{FBEAE4} Ours (CA) (5 steps) & Flux DiT & - & - & - & - & - & - \\ 
        \rowcolor[HTML]{FBEAE4} \tool{} & Flux DiT & \textbf{83.07} & \textbf{71.04} & \textbf{90.45} & \textbf{87.85} & \textbf{76.45} & \textbf{90.19} \\
        % \rowcolor[HTML]{FBEAE4} Ours (Output) (5 steps) & Flux DiT & - & - & - & - & - & - \\
        \bottomrule

    \end{tabular}
    \caption{\tool{} outperforms a variety of Diffusion, DINO, and CLIP ViT interpretability methods on ImageNet-Segmentation and PascalVOC (Single Class).}
    \label{tab:imagenet_segmentation}
\end{table*}

\begin{table}
    \centering
    \small
    \begin{tabular}{p{0.15\columnwidth} p{0.15\columnwidth} p{0.1\columnwidth}p{0.1\columnwidth}p{0.09\columnwidth}}
        \toprule
        \textbf{Space} & \textbf{Softmax} & \textbf{Acc$\uparrow$} & \textbf{mIoU$\uparrow$} & \textbf{mAP$\uparrow$} 
        \\
        \midrule 
        CA & & 66.59 & 49.91 & 73.17 \\
        CA & \centering \checkmark & 74.92 & 59.90 & 87.23 \\
        Value & & 45.93 & 29.81 & 65.79  \\
        Value & \centering \checkmark & 45.78 & 29.68 & 39.61 \\
        \rowcolor[HTML]{FBEAE4} Output & & 78.75 & 64.95 & 88.39 \\
        \rowcolor[HTML]{FBEAE4} Output & \centering \checkmark & \textbf{83.07} & \textbf{71.04} & \textbf{90.45} \\
        % \tool(CA) & 78.23 & 64.24 & \textbf{90.24} \\ 
        % \tool(Output) & \textbf{87.06} & \textbf{77.09} & 86.81 \\
        \bottomrule

    \end{tabular}
    \caption{\textbf{The output space of DiT attention layers produces more transferable representations than cross attentions. } We explore the transferability of several representation spaces of a DiT: the cross attentions (CA), the value space, and the output space. We performed linear projections of the image patches and concept vectors in each of these spaces and evaluated their performance on ImageNet-Segmentation. }
    \label{tab:representation_space_imagenet_segmentation}
\end{table}



\begin{table}
    \centering
    \small
    \begin{tabular}{p{0.1\columnwidth} p{0.1\columnwidth} p{0.1\columnwidth}p{0.1\columnwidth}p{0.09\columnwidth}}
        \toprule
        \centering \textbf{CA} & \centering \textbf{SA} & \textbf{Acc$\uparrow$} & \textbf{mIoU$\uparrow$} & \textbf{mAP$\uparrow$} 
        \\
        \midrule 
         &  &  52.63 &  35.72 & 70.21 \\
         & \centering \checkmark & 51.68 & 34.85 & 69.36 \\
        \centering \checkmark &  & 76.51  & 61.96 & 86.73 \\
        \rowcolor[HTML]{FBEAE4} \centering \checkmark & \centering\checkmark & 
        \textbf{83.07} & \textbf{71.04} & \textbf{90.45}\\
        \bottomrule 
        
    \end{tabular}
    \caption{\textbf{\tool{} performs best when we utilize both cross and self attention.} We tested the effectiveness of performing just a cross attention operation between the concepts and image tokens, just a self attention among the concepts, both cross and self attention, and neither. We found that doing both operations leads to the best results. Metrics are computed on the ImageNet Segmentation benchmark.}
    \label{tab:cross_self_ablation}
\end{table}


\begin{table}
    \centering
    \small
    \begin{tabular}{p{0.36\columnwidth} p{0.1\columnwidth} p{0.1\columnwidth}}
        \toprule
        \textbf{Method} & \textbf{Acc$\uparrow$} & \textbf{mIoU$\uparrow$}
        % & \textbf{mAP$\uparrow$} 
        \\
        \midrule 
        TextSpan  & 73.84 & 38.10\\
        DAAM  & 62.89 & 10.97 \\
        Flux Cross Attention  & 79.52 & 27.04 \\
        \rowcolor[HTML]{FBEAE4} \tool{} & \textbf{86.99} & \textbf{51.39} \\
        \bottomrule 
        
    \end{tabular}
    \caption{\textbf{\tool{} outperforms alternative methods on images with multiple classes from PascalVOC.} Notably, the margin between \tool{} and other methods is even higher for this task than when a single class is in each image. }
    \label{tab:multi_class_pascal_voc}
\end{table}


\section{Experiments}

\subsection{Zero-shot Image Segmentation}

We are interested in investigating (1) the efficacy of \tool{} to generate highly localized and semantically meaningful saliency maps, and (2) understand the transferability of multi-modal DiT representations to important downstream vision tasks. Zero-shot image segmentation is a natural choice for achieving these goals. 

\paragraph{Datasets} 

We leverage two key datasets zero-shot image segmentation datasets. First, we use a commonly used \cite{gandelsman_interpreting_2024, chefer_transformer_2021} zero-shot segmentation benchmark called ImageNet-Segmentation \cite{guillaumin_imagenet_2014}. It is composed of 4,276 images from 445 categories. Each image primarily depicts a single central object or concept, which makes it a good method for comparing \tool{} to a variety of methods which generate a single saliency map that are unable to generate class-specific segmentation maps. For the second dataset we leverage PascalVOC 2012 benchmark \cite{everingham_pascal_2015}. We investigate both a single class (930 images) and multi-class split (1,449 images) of this dataset. Many methods (e.g. DINO) do not condition their saliency map on class, so for these methods we restrict our evaluation to examples only containing a single class and the background. For methods that can accept text as conditioning we evaluate on the full dataset.  

% We restricted our investigation to the examples containing only a single object class and the background for the sake of making a fair comparison to methods that do not condition on a class (e.g. DINO). 

\paragraph{Key Baseline Methods}

We compare our approach to a variety of zero-shot interpretability methods which leverage several different multi-modal foundation models. We compare to numerous interpretability methods compatible with CLIP: Layerwise Relevance Propagation (LRP) \cite{binder_layer-wise_2016}, LRP on just the final-layer of a ViT (Partial-LRP), Attention Rollout (Rollout) \cite{abnar_quantifying_2020}, Raw Vision Transformer Attention (ViT Attention) \cite{dosovitskiy_image_2021}, GradCAM \cite{selvaraju_grad-cam_2019}, TextSpan \cite{gandelsman_interpreting_2024}, and the Transformer Attribution method from \cite{chefer_transformer_2021} (TransInterp). We also compare to a UNet-based interpretability method that aggregates information from UNet cross attention layers called DAAM \cite{tang_what_2022} on both Stable Diffusion XL \cite{podell_sdxl_2023} and Stable Diffusion 2. Finally, we compare to the raw cross attention maps produced by Flux DiT models. 

\paragraph{Implementation Details}

For all of our experiments we use the Flux DiT architecture implemented in PyTorch \cite{paszke_pytorch_2019}. In particular, we use the distilled Flux-Schnell model. We encode real images with the DiT by first mapping them to the VAE latent space and then adding varying degrees of Gaussian noise before passing them through the Flux DiT. We then cache all of the concept output $o_c$ and image output vectors $o_x$ from each \layername{} layer. At the end of generation we then construct our concept saliency maps for each layer and average them over all layers of interest. In our experiments we leverage the activations from the last 10 of the 18 \layername{} layers. 

\paragraph{Single Object Image Segmentation}

For our first task we closely follow the established evaluation framework from \cite{gandelsman_interpreting_2024} and \cite{chefer_transformer_2021}. We perform this evaluation setup on both ImageNet-Segmentation and a subset of 930 PascalVOC images containing only a single class. For each method we assume the class present in the image is known and use simplified descriptions of each ImageNet class (e.g. ``Maltese dog'' $\to$ ``dog) this allows the concepts to be captured by a single token. We construct a concept vocabulary for each image composed of this target class and a set of fixed background concepts for all examples (e.g. ``background'', ``grass'', ``sky''). 

\paragraph{Quantitative Evaluation}
Each method produces a set of scalar \textit{raw scores} for each image patch which we then threshold based on the mean value to produce a binary segmentation prediction. We compare each method using standard segmentation evaluation metrics, namely: mean Intersection over Union (mIoU), patch/pixelwise accuracy (Acc), and mean Average Precision (mAP). Accuracy alone is an insufficient metric as our dataset is highly imbalanced, mIoU is significantly better, and mAP captures threshold agnostic segmentation capability. We found that \tool{} significantly out performs all of the baselines we tested across all three metrics (See \cref{tab:imagenet_segmentation}). This is true for diffusion, CLIP, and DINO based interpretability methods. 

\paragraph{Qualitative Evaluation} 

We also show qualitative results comparing the segmentation performance to each baseline in Figure \ref{fig:segmentation_qualitative_results}. We also provide more qualitative results in Appendix \ref{QualitativeAppendix}. It is worth noting that the qualitative segmentation results highlight (a) the ambiguity of zero-shot image segmentation, and (b) the limitations of human data annotation. For example, Figure \ref{fig:segmentation_qualitative_results} shows that our method does not segment the part of the dog between the ears and it's body, while the data annotation does. 

% For each method, including ours, we 
% We 

% We closely follow the evaluate approach from \cite{chefer_transformer_2021} and \cite{gandelsman_interpreting_2024}.
% For our first test we follow the evaluation framework established by \cite{chefer_transformer_2021} on ImageNet-Segmentation. 

\paragraph{Multi Object Image Segmentation}

We also wanted to evaluate the capabilities of our method at differentiating between multiple classes in an image. However, only a subset of methods produce distinct saliency maps for open ended classes. For this experiment we compare to DAAM using a SDXL backbone, TextSpan using a CLIP backbone, and the raw cross attentions of Flux. Instead of binarizing the image to produce segmentations, for each patch we predict the concept with the highest score. We used pixewlise accuracy and mIoU as our evaluation metrics and found that our method significantly out performed the baselines (See Table \ref{tab:multi_class_pascal_voc}). We also show qualitative results of our approach differentiating between multiple concepts in a single image in Figures \ref{fig:teaser}, \ref{fig:multi_class_qualitative_segmentation} and we show more results in Appendix \ref{QualitativeAppendix}. 

% \paragraph{Procedure}

% We want to demonstrate that \tool{} generates high-fidelity heatmaps for concepts that are localized to relevant regions of images. We compare \tool{} to various other zero-shot interpretability methods on the ImageNet Segmentation dataset \cite{guillaumin_imagenet_2014}, an annotated subset of ImageNet containing 4,276 images from 445 categories. We follow the experimental evaluation procedure used in \cite{chefer_transformer_2021, gandelsman_interpreting_2024}. 

% We encode real images with the DiT by first mapping images to the VAE latent space and then add Gaussian noise to the image. We then run a forward pass of the DiT with a prompt like \textit{``a \{class\}''}. We use simplified one word descriptions of each image class (e.g. ``Maltese dog'' $\to$ ``dog'') so they can be represented with single T5 tokens. We then leverage image-level group sparse coding (see Section \ref{}) on a concept dictionary containing the target class and background concepts (e.g. ``background'', ``grass'', ``sky'', etc.). Following \cite{chefer_transformer_2021}, we binarize the inferred coefficients by thresholding by the mean value.

% Our results show that \tool{} beats CLIP ViT explainability methods by a significant margin and other Flux DiT baselines on several standard image segmentation evaluation metrics (i.e. mIoU, pixelwise accuracy, and mAP). Sometimes the segmentation maps generated by our method even seem to produce better segmentation masks than the ground truth data, detecting foreground objects that occlude the class of interest (Figure \ref{}). 


\subsection{Ablation Studies}


We perform several ablation studies to investigate the impact of various architectural choices and hyperparameters on the performance of \tool{}. 

\paragraph{Impact of Layer Depth on Segmentation}

We hypothesized that deeper \layername{} layers in the DiT would have more refined representations that better transfer to segmentation. This was confirmed by our evaluation (see Figure \ref{fig:per_layer_metrics}). We pull features from each diffusion layer and evaluated the segmentation performance of these features on ImageNet Segmentation. We also compare the performance of combining all layers simultaneously, which we found performs better than any individual layer.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/per_layer_metrics.pdf}
    \vspace{-0.24in}
    \caption{\textbf{Later \layername{} layers encode richer features for zero-shot segmentation. } We investigated the impact of using features from various \layername{} layers and found that deeper layers lead to better performance on segmentation metrics like pixelwise accuracy, mIoU, and mAP. We also found that combining the information from all layers further improves performance. }
    \label{fig:per_layer_metrics}
\end{figure}

\paragraph{Impact of Diffusion Timestep on Segmentation}

We add Gaussian noise to encoded images before passing them to the DiTs, this conforms with the expectations of the models. Intuitively one might expect the later timesteps (less noise) to have much higher segmentation performance as less information about the original image is corrupted. However, we found that the middle diffusion timesteps best (See \cref{fig:per_time_metrics}). Throughout the rest of our experiments we use timestep 500 out of 1000 following this result. 

% We investigate the impact of different layers and diffusion timesteps on segmentation performance. f

% We were interested in investigating the impact of leveraging different diffusion timesteps and performing projections in different MMAttn layers. We found some interesting trends as is shown in Figure \ref{}. 


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/per_time_metrics.pdf}
    \vspace{-0.24in}
    \caption{\textbf{Optimal segmentation performance requires some noise to be present in the image. } We evaluated the performance of \tool{} by encoding samples from a variety of timesteps (determines the amount of noise). Interestingly, we found that the optimal amount of noise was not zero, but in the middle to later stages of the noise schedule. }
    \label{fig:per_time_metrics}
\end{figure}

% We investigate the impact of varying several hyper-parameters of \tool{} to verify the efficacy of our final approach. In particular, we investigate the richness of features for different DiT layers, different diffusion timesteps, and with different variations of the \tool{} attention operation. 

\paragraph{Concept Attention Operation Ablations}

We compared the performance on the ImageNet Segmentation benchmark of performing (a) just cross attention from the image patches to the concept vectors, (b) just self attention, (c) no attention operations, and (d) both cross and self attention. Our results seen in Table \ref{tab:cross_self_ablation} indicate that using a combination of both cross and self attention achieves the best performance. 

We also investigated the impact of applying a pixelwise softmax operation over our predicted segmentation coefficients. We found that it slightly improves segmentation performance in the attention output space and significantly improves performance for the cross attention maps (see Table \ref{tab:representation_space_imagenet_segmentation}. 


% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/FailureCase.pdf}
%     \vspace{-0.2in}
%     \caption{A potential failure case of \tool{} is that the saliency maps struggle to differentiate two similar classes. }
%     \label{fig:enter-label}
% \end{figure}





% \paragraph{Multi-modal DiTs Learn Contextualized Embeddings}
% TODO this set of experiments. 
% We provide evidence that the concept embeddings produced by \tool{} are actually contextualized to the particular encoded image. We demonsrate this using two key pieces of evidence. First, we encode a subset of images from the ImageNet-Segmentation dataset and produce segmentation maps in the attention output space using our \tool{} embeddings. We then perform a mean-ablation by replacing the concept embeddings generated at inference time and replacing them with the average concept embeddings over the entire set of images. Using the mean-ablated embeddings results in a substantial drop in segmentation performance. This indicates that our approach enables DiTs to infer contextualized concept embeddings. 



% perform a mean ablation on a subset of the ImageNet Segmentation dataset


% \begin{table}[t!]
%     \centering
%     \small
%     \begin{tabular}{p{0.1\columnwidth} p{0.1\columnwidth} p{0.1\columnwidth}p{0.1\columnwidth}p{0.09\columnwidth}}
%         \toprule
%         \centering \textbf{CA} & \centering \textbf{SA} & \textbf{Acc$\uparrow$} & \textbf{mIoU$\uparrow$} & \textbf{mAP$\uparrow$} 
%         \\
%         \midrule 
%          &  & 61.57 & 44.48 & 71.34 \\
%         \centering \checkmark &  & 83.49 & 71.66 & 83.03 \\
%          & \centering \checkmark & 65.61 & 48.82 & 72.59 \\
%         \centering \checkmark & \centering\checkmark & \textbf{86.66} & \textbf{76.47} & \textbf{88.21} \\
%         \bottomrule

%     \end{tabular}
%     \caption{\textbf{Ablation Study 2. } We investigate the impact of performing cross attention (CA) between concept embeddings and image tokens and self-attention (SA) among the concept tokens when using \tool{} in the output space.  }
%     \label{tab:imagenet_segmentation}
% \end{table}



% \begin{table}[t!]
%     \small
%     \begin{tabular}{l c c c c}
%         \toprule
%         \textbf{Method} &  \textbf{Time (Secs/Im)} & \textbf{Memory (GB/Im)}\\
%         \midrule
%         Flux & & & & \\
%         Flux w/ Ours & & & & \\
%         \bottomrule
%     \end{tabular}
%     \caption{\textbf{Time and memory overhead of \tool{}. } \tool{} has relatively minimal memory overhead and impact on latency compared to the vanilla Flux model. }
%     \label{tab:time_and_memory}
% \end{table}

% \subsection{Multi-class Image Segmentation}

% Our method can produce multiple contextualized embeddings for a given image. This is in contrast to methods like DINO self-attention which can not easily be applied to differentiating between multiple image concepts. 

% For this experiment, we evaluate our method on a task closer to that of semantic segmentation. Given classes present in an image (e.g. ``cat'', ``bottle'', etc.) we produce a segmentation of the given concepts. We compare this to class-specific CLIP ViT methods like TransformerExplainability \cite{chefer_transformer_2021} and diffusion based methods like DAAM \cite{}. 


% \subsection{Qualitative Results}

% We have several figures in the main paper and the appendix that compare the qualitative performance of our method to various others. In particular, in Figure \ref{fig:teaser} we compare \tool{} to the cross attention maps created by a Flux model, the saliency maps produced by DAAM \cite{tang_what_2022} on a SDXL \cite{podell_sdxl_2023}, and TextSpan on a CLIP model. 



