

\section{Preliminaries}
\subsection{Rectified-Flow Models for Image Generation}

Flux and Stable Diffusion 3 leverage multi-modal DiTs that are trained to parameterize rectified flow models. Throughout this paper we may refer to rectified flow models as diffusion models for convenience. These models attempt to generate realistic images from noise that correspond to given text prompts. Flow based models \cite{lipman_flow_2023} attempt to map a sample $x_1$ from a noise distribution $p_1$, typically $p_1 \sim \mathcal{N}(0, I)$, to a sample $x_0$ in the data distribution. Rectified flows \cite{liu_flow_2022} attempt to learn ODEs that follow straight paths between the $p_0$ and $p_1$, i.e. 
\begin{equation}
    z_t = (1 - t) x_0 + t \epsilon, \epsilon \sim \mathcal{N}(0, 1).
\end{equation}
Flux and SD3 are trained using a conditional flow matching objective which can be expressed conveniently as
\begin{equation}
    -\frac{1}{2} \mathbb{E}_{t \sim \mathcal{U}(t), \epsilon \sim \mathcal{N}(0, I) }[w_t \lambda_t'||\epsilon_\Theta(z_t, t) - \epsilon||^2]
\end{equation}
where $\lambda_t'$ corresponds to a signal-to-noise ratio and $w_t$ is a time dependent-weighting factor. Above $\epsilon_\Theta(z_t, t)$ is parameterized by a multi-modal diffusion transformer network. The architecture of this model and it's properties is of primary interest in this work. 

% Our methodology builds upon particular architectural observations of multi-modal DiT networks  

% Recent multi-modal DiT architectures like Flux \cite{} and Stable Diffusion 3 \cite{} leverage 
% Diffusion models have established themselves as the state of the art approach to generating high-dimensional modalities of data like images and video. They work by attempting to iteratively remove noise from noisy data samples and in the process generate new data samples. The choice of path between noisy data samples and realistic samples is not totally clear, and a variety of competing methods have emerged \cite{}. This work leverages a class of models called rectified-flow models \cite{liu_flow_2022}. 

% TODO Finish this. 

\subsection{The Anatomy of a Multi-modal DiT Layer}

Multi-modal DiTs like Flux and Stable Diffusion 3 leverage \textit{multi-modal attention layers (\layername)} that process a combination of textual tokens and image patches. There are two key classes of layers: one that keeps separate residual streams for each modality and one that uses a single stream. In this work, we take advantage of the properties of these dual stream layers, which we refer to as multi-modal attention layers (\layername s). 

The input to a given layer is a sequence of image patch representations $x \in \mathbb{R}^{h \times w \times d}$ and prompt token embeddings $p \in \mathbb{R}^{l \times d}$. The initial prompt embeddings at the beginning of the network are formed by taking the T5 \cite{raffel_exploring_2023} embeddings of the prompt tokens.
% $c^E =  \mathcal{E}_{p}(c)$. 

Following \cite{peebles_scalable_2023}, each \layername{} layer leverages a set of adaptive layer norm \textit{modulation layers}  \cite{xu_understanding_2019}, conditioned on the time-step and global CLIP vector. An adaptive layernorm operation is applied to the input image and text embeddings. 
% Let $m_i(v_{clip}, t)$ and $m_o(v_{clip}, t)$ correspond to our input and output modulation mechanisms respectively. Each output scale $\gamma$, shift $\beta$, and gating $\alpha$ values which control the normalization. 
% \begin{equation}
% \end{equation}
% These mechanisms transform each input modality at the beginning of the \layername{} layer and at the end. 
The final modulated outputs are then residually added back to the original input. Notably, the image and text modalities are kept in separate residual streams. The exact details of this operation are omitted for brevity. 

The key workhorse in \layername{} layers is the familiar multi-head self attention operation. The prompt and image embeddings have separate learned key, value, and query projection matrices which we refer to as $K_x, Q_x, V_x$ for images and $K_p, Q_p, V_p$ for text. The keys, queries, and values for both modalities are collectively denoted $q_{xp}$, $k_{xp}$, and $v_{xp}$, where for example $k_{xp} = [K_x x_1, \dots, K_p p_1 \dots]$. 
A self attention operation is then performed 
\begin{equation}
    o_x, o_p = \softmax(q_{xp} k_{xp}^T) v_{xp}
\end{equation}
Here we refer to $o_x$ and $o_p$ as the \textit{attention output} vectors. Another linear layer is then applied to these outputs and added to a separate residual streams weighted according to the output of the modulation layer. This gives us updated embeddings $x^{L+1}$ and $p^{L + 1}$ which are given as input to the next layer. 

% The key part of the network that is of interest to us in this paper is the familiar multi-head self attention operation. 


% First, a prompt (e.g. ``a man and a dog'') is encoded using a text encoder like T5 \cite{} to produce a set of initial prompt embeddings $p^E = \{p_1, \dots p_n \}$ where $p_i \in \mathbb{R}^{p}$. Each \layername{} applies a multi-head self-attention operation to a combined sequence of image patches  $x^L = \{x_1, \dots x_m\}$ and prompt embeddings $p^L = \{x_1, \dots x_n\}$. The prompt and image embeddings have separate learned key, value, and query projection matrices which we refer to as $K_x(x), Q_x(x), V_x(x)$ for images and $K_p(p), Q_p(p), V_p(p)$ for text. The outputs of these projections are concatenated to form joint sequences $K = [K_x(x_1), \dots, K_p(p_1), \dots K_p(p_n)]$ and a multi-head self attention operation is applied
% \begin{equation}
%     x^{L + 1}, p^{L + 1} = \softmax(QK^T) V.
% \end{equation}
% The outputs of each of these layers is integrated into the residual stream using a modulation layer. Further, after a number of \layername{} layers.

\section{Proposed Method: \tool{}}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/Concept Attention.pdf}
    \vspace{-0.3in}
    \caption{(a) \layername{} combines cross and self attention operations between the prompt and image tokens. (b) Our \tool{} allows the concept tokens to incorporate information from other concept tokens and the image tokens, but not the other way around. }
    \label{fig:multi_modal_attention_vs_concept_attention}
\end{figure}

We introduce \tool{}, a method for generating high quality saliency maps depicting the location of textual concepts in images. \tool{} works by creating a set of contextualized \textit{concept embeddings} for simple textual concepts (e.g. ``cat'', ``sky'', etc.). These concept embeddings are sequentially updated alongside the text and image embeddings, so they match the structure that each \layername{} layer expects. However, unlike the text prompt, concept embeddings do not impact the appearance of the image. We can produce high-fidelity saliency maps by projecting image patch representations onto the concept embeddings. \tool{} requires no additional training and has minimal impact on model latency and memory footprint.  A high level depiction of our methodology is shown in Figure \ref{fig:mmattn_vs_mmattn_with_concept_attention_explanatory}.

% \paragraph{Limitations of Cross Attention Maps}

% For multi-modal DiT architectures, cross attention maps have a key limitation, which is that their vocabulary is limited to the tokens in the user's prompt. Unlike UNet-based models, multi-modal DiTs sequentially update a set of prompt embeddings with each \layername{} layer. This makes it difficult to produce cross attention maps for an open-set of concepts, as you would need to add the concept tot he prompt sequence which would then change the appearance of the image. \tool{} overcomes this key limitation. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ThreeImageSegmentation.pdf}
    \vspace{-0.2in}
    \caption{\tool{} produces higher fidelity raw scores and saliency maps than alternative methods, sometimes surpassing in quality even the ground truth saliency map provided by the ImageNet-Segmentation task. Top row shows the soft predictions of each method and the bottom shows the binarized predictions. }
    \label{fig:segmentation_qualitative_results}
\end{figure*}

\subsection{Using \tool{}}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/PrecisePseudocodeConceptAttention.pdf}
%     \vspace{-0.2in}
%     \caption{\textbf{Psuedo-code of the concept attention operation. }}
%     \label{fig:concept_attention_pseudocode}
% \end{figure}

The user specifies a set of $r$ single token concepts, like ``cat'', ``sky'', etc. which are passed through a T5 encoder to produce an initial embedding $c^0$ for each concept. 
% which we call $c^E = \mathcal{E}_{p}(c)$.
% We pull out the embedding produced by T5 corresponding to each single word concept of interest.
For each \layername{} layer (indexed by $L$) we layer-normalize the input concept embeddings $c^L$ and repurpose the text prompt's projection matrices (i.e. $K_p, Q_p, V_p$), to produce a set of keys, values, and queries 
\begin{align}
    k_c &= [K_p c_1 , \dots K_p c_k] \\ 
    q_c &= [Q_p c_1 , \dots Q_p c_k] \\
    v_c &= [V_p c_1 , \dots V_p c_k]
\end{align}
each in $\mathbb{R}^{r \times d}$. 

\paragraph{One-directional Attention Operation}
We would like to update our concept embeddings so they are compatible with subsequent layers, but also prevent them from impacting the image tokens. 
% Thus, given our $k_c, q_c$ and $v_c$ vectors, we perform a \textit{one-directional} attention operation that allows the image patch representations to impact the concept vectors, but does not allow the concept vectors to alter the image patches. 
% This produces a set of contextualized embeddings that can be used for interpretability, but does not alter the appearance of generated images. 
% For our one-directional attention operation we perform a combined cross and self attention operation. 
Let $k_x$ and $v_x$ be the keys and values of the image patches $x$ respectively. We can concatenate the image and concept keys to get
\begin{equation}
    k_{xc} = [K_x x_1 \dots, K_x x_n, K_p c_1 \dots, K_p c_r]
\end{equation}
and the image and concept values to get
\begin{equation}
    v_{xc} = [V_x x_1 \dots, V_x x_n, V_p c_1 \dots, V_p c_r]
\end{equation}
We can then perform the following attention operation 
\begin{equation}
    o_{c} = \softmax(q_{c} k_{xc}^T) v_{xc}
\end{equation}
which produces a set of \textit{concept output embeddings}. 

Notice, that instead of just performing a cross attention (i.e. $\softmax(q_{c} k_{x}^T) v_{x}$) our approach leverages both cross attention from the image patches to the concepts and self attention among the concepts. We found that performing both operations improves performance on downstream evaluation tasks like segmentation (See Table \ref{tab:cross_self_ablation}).  We hypothesize this is because it allows the concept embeddings to repel from each other, avoiding redundancy between concepts.

Meanwhile, the image patch and prompt tokens ignore the concept tokens and attend only to each other as in 
\begin{equation}
    o_x, o_p = \softmax(q_{xp} k_{xp}^T) v_{xp} .
\end{equation}

% the original image patch embeddings attend only to the keys of the prompt, as in

% (TODO fill in math of regular update).

A diagram of these operations is shown in \cref{fig:multi_modal_attention_vs_concept_attention}(b).

% We found that by performing both cross attention between the concept and queries, and self-attention among the concepts improves performance on downstream evaluation tasks like segmentation as apposed to just using cross attention (See Table \ref{tab:cross_self_ablation} in results).

\paragraph{A Concept Residual Stream}

The above operations create a residual stream of concept embeddings distinct from the image and patch embeddings. Following the pretrained transformer's design, after the \layername{} we apply another projection matrix $P$ and MLP, adding the result residually to $c^{L}$.
% modulated by an adaptive layernorm mechanism. 
% We use adaptive layernorm to modulate the update to $c^L$, producing a scale $\gamma$, shift $\beta$, and some gating values $\alpha_1$ and $\alpha_2$
We apply an adaptive layernorm at the end of attention operation which outputs several values: a scale $\gamma$, shift $\beta$, and some gating values $\alpha_1$ and $\alpha_2$. The residual stream is then updated as
\begin{align}
    c^{L + 1} & \gets c^L + \alpha_1 (P o_{c}) \\
    c^{L + 1} & \gets c^{L+1} + \alpha_2 \MLP\bigg(( 1 + \gamma) \lnorm(c^{L + 1}) + \beta\bigg)
\end{align}
where $\gets$ denotes assignment. The parameters from each of our modulation, projection, and MLP layers are the same as those used to process the text prompt. 

\paragraph{Saliency Maps in the Attention Output Space}
These concept embeddings can be combined with the image patch embeddings to produce saliency maps for each layer $L$. Specifically, we found that taking a simple dot-product similarity between the image output vectors $o_x$ and concept output vectors $o_c$ produces high-quality saliency maps

% These contextualized concept embeddings for each layer $c^L$ is to interpret the meaning of the image patch embeddings $x^L$ by producing saliency maps. 
% Surprisingly, we found that taking the dot product between image output vectors $o_x$ and concept output vectors $o_c$ 
\begin{equation}
\label{eq:dot-prod-saliency}
    \phi(o_x, o_c) = \softmax(o_x o_c^T ).
\end{equation}
this is in contrast to cross attention maps which are between the image keys $k_x$ and prompt queries $q_p$.

% produces even higher quality saliency maps than cross attentions.
We can aggregate the information from multiple layers by averaging them $\frac{1}{|L|} \sum_{L = 1}^{|L|} \phi(o_x^L, o_c^L)$ where $|L|$ denotes the number of \layername{} layers (Flux has $|L| = 18$). These attention output space maps are unique to MM-DiT models as they leverage \textit{concept embeddings} corresponding to textual concepts which fundamentally can not be produced by UNet-based models.

\subsection{Limitations of Raw Cross Attention Maps}

For multi-modal DiT architectures, we could additionally consider using the raw cross attention maps  
\begin{equation}
    \phi(k_x, q_p) = \softmax(q_p k_x^T) 
\end{equation}
to produce saliency maps. However, these have a key limitation in that their vocabulary is limited to the tokens in the user's prompt. Unlike UNet-based models, multi-modal DiTs sequentially update a set of prompt embeddings with each \layername{} layer. This makes it difficult to produce cross attention maps for an open-set of concepts, as you would need to add the concept to the prompt sequence which would then change the appearance of the image. \tool{} overcomes this key limitation, and makes the additional discovery that the output space of attention mechanisms produces high-fidelity saliency maps. 

% % it is possible to create high-quality saliency maps by projections in the output space of our attention mechanism, using image output vectors $o_x$ and concept output vectors $o_c$ from the same layer $L$

% % We have described out approach to generating a set of contextualized concept embeddings $c^L$ that are updated by each \layername{} layer. 

% ---

% and adaptive layer normalization using the parameters trained for text prompt tokens to get $o_{c}'$ and update a separate residual stream containing only concepts 
% \begin{equation}
%     c^{L + 1} = c^L + s * o_{c}'
% \end{equation}
% where $s$ captures 

% \textit{one-directional} cross attention operation between the image patches and concept tokens. 

% attention operation from the image patch tokens to the concept tokens. 


% Let $k_x, k_v$ be the keys and values of the image patches for a given layer $L$. 


% a combined cross and self attention operation (See Figure \ref{fig:multi_modal_attention_vs_concept_attention}) between 




% \tool{} repurposes the parameters of \layername{} layers to create a set of concept embeddings that lay in the same space as our image representations.  


% \paragraph{One-directional Attention} 




% % % \subsection{Extending Cross Attention Maps to Any Concept}


% % vocabulary is limited to the tokens in the user prompt and (2) they produce noisy saliency maps. Unlike, 



% \paragraph{Cross Attention Maps}

% UNet-based diffusion models produce \textit{cross attention maps} by taking the dot product between a sequence of text token queries and a sequence of image patch keys. These have been shown to produce high-quality saliency maps depicting the location of textual concepts. Our desire is to interpret the representations of DiTs so a natural choice is to investigate the attention maps of \layername s. 

% We can perform a cross attention operation $\phi(q, k)$ between the text queries $q_p$ and image keys $k_p$ as $q_p k_p^T$. This can be done in parallel for multiple heads if we let $q_p \in \mathbb{R}^{m \times h \times d}$ and $k_p \in \mathbb{R}^{n \times h \times d}$ where $h$ is the number of attention heads, $m$ is the number of prompt tokens, $n$ is the number of image patches, and $d$ is the dimensionality of each head. We can then average across the head dimension to produce maps $\phi(q_p, k_p) \in \mathbb{R}^{m \times n}$. We can even obviate the need to explicitly compute these cross attentions as they correspond to a sub-matrix of the \layername's self attention operation (See Figure \todo{\ref{}}). 

% This approach produces interpretable cross attention maps for each prompt token (See Figure \ref{}). However, unlike the cross attention mechanisms of UNet-based models which leverage shallow projections of text embeddings at each cross attention layer, \layername{} layers build upon the outputs of previous \layername{} layers. \textbf{This sequential dependency limits the vocabulary of cross attention maps to the tokens in the user's text prompt.} (See Figure \ref{} (b))

% \paragraph{Concept Attention}

% % \begin{figure}
% %     \centering
% %     \includegraphics[width=\linewidth]{figures/MMDiTConceptTokens.pdf}
% %     \caption{Caption}
% %     \label{fig:enter-label}
% % \end{figure}


% % \tool{} works by turnign the
% % We introduce \tool{}, which allows us to generate interpretable saliency maps for arbitrary textual concepts.
% % % Our solution to


% % simply append a sequence of embeddings, each corresponding to a distinct textual concept (e.g. ``cat'', ``sky'', etc.), to the text prompt and use those to construct cross attention maps. 


% % \tool{} produces a sequence of \textit{concept embeddings} $c \in \mathbb{R}^{k \times d}$ each corresponding to distinct textual concepts (e.g. ``cat'', ``sky'', etc.). 

% % However, these text concepts would unfortunately influence the appearance of the image. We need an approach that produces a set of \textit{concept embeddings} that are compatible with each \layername{} layer, but also do not impact the image. 


% A naive approach to extending cross attention maps to arbitrary concepts would be to use the T5 text encoder to produce a set of textual concept embeddings $c^E$ (e.g. ``cat'', ``sky'', etc.) and concatenate these to our text and image embeddings during multi-modal attention operations. However, then the content of these embeddings would influence the appearance of the image, which is undesirable if we wish to interpret the unmodified network activations. 

% The key idea behind \tool{} is to convert the bi-directional attention mechanism between the prompt and image embeddings into a one-way attention direction from image embeddings to a set of concept embeddings. First, we leverage the T5 encoder $\mathcal{E}_{p}(\cdot)$ to produce a sequence of embeddings for our concepts which we can call $c^E = \mathcal{E}_{p}(c)$. We pull out the embedding produced by T5 corresponding to each single word concept of interest. We then apply the same modulation layer designed for the text embeddings, to the concept embeddings. For our attention operations, we apply the key, value, and query projections attuned the text embeddings to the concept embeddings. 

% For our attention operation, instead of performing a self attention between the concatenated image and concept embeddings, we perform a combined self and cross attention operation between the concept embeddings and image patches. 
% More precisely, take a set of concept keys, queries and values
% \begin{align}
%     q_c &= [Q_p c_1 , \dots Q_p c_k] \\
%     v_c &= [V_p c_1 , \dots V_p c_k] \\
%     k_c &= [K_p c_1 , \dots K_p c_k].
% \end{align}
% We concatenate the concept keys and image patch keys to get
% \begin{equation}
%     k_{cx} = [K_p c_1, \dots K_p c_k, K_x x_1 \dots, K_x x_n]
% \end{equation}
% and perform the following attention operation
% \begin{equation}
%     o_c = \softmax(q_c k_{cx}^T) V_{cx}.
% \end{equation}
% This produces a set of \textit{concept output} vectors. 
% This operation allows the information from the image patches to influence our concept embeddings, but does not allow any information to flow to the image patch representations. This operation takes advantage of the existing parameters of \layername{} layers, requiring no additional training. The cross attention maps produced by these embeddings (i.e. $q_c k_{xc}^T$) can be used to produce saliency maps for arbitrary concepts. 

% % \tool{} construct a sequence of \textit{concept embeddings} that are updated along side the image patch embeddings and text prompt embeddings. 

% % The key intuition behind \tool{} is to repurpose the parameters of \layername{} attention mechanisms, converting them from bi-directional attention mechanisms that share information between text and image modalities and instead treating them as one way cross attention mechanisms from image patches to concept embeddings. 
% % We produce a sequence of \textit{concept embeddings} $c = \{c_1, \dots c_k\}$ where $ c_i \in \mathbb{R}^p$ by repurposing the parameters of \layername{} layers without any additional training. 

% Following the \tool{} operation, we then leverage the modulation mechanism and linear projection layer designed for the text prompt to produce the final output embeddings. These embeddings are then added residually to a separate residual stream dedicated to the concept embeddings, i.e. 
% \begin{equation}
%     c^{L + 1} = c^{L} + o_c'
% \end{equation}
% where $o_c'$ represents the projected and modulated output of our attention operation. We iteratively update these concept embeddings at each layer. 

% % We then iteratively update these embeddings at each \layername{}

% % by converting the multi-modal self attention operations (See Figure \ref{} (a)) into a variation of cross attention. We leverage the projection matrices for the text prompt tokens to produce a set of concept keys, values and queries
% % \begin{align*}
% %     K_c &= [K_p(c_1), \dots K_p(c_k)] \\
% %     V_c &= [V_p(c_1), \dots V_p(c_k)] \\
% %     Q_c &= [Q_p(c_1), \dots Q_p(c_k)].
% % \end{align*}
% % We then take the image keys which where already generated for the multi-modal attention operation
% % \begin{equation}
% %     K_x = [K_x(x_1), \dots K_x(x_n)]
% % \end{equation}
% % and  cross attention operation between the concept queries and the image keys $Q_c K_x^T$. We also found that performing a self-attention operation between the concept keys and queries $Q_c K_c^T$ further improves the fidelity of our saliency maps. These self and cross attentions can be combined into a single parallel tensor operation $Q_c K_{xc}^T$ where $K_{xc}$ is a concatenation of the image and concept queries (See Figure \ref{}). This operation only allows for a one-directional flow of information from the image patches to the concepts, but not the other way around. The full attention operation becomes
% % \begin{equation}
% %     c^{L + 1} = \softmax(Q_c K_{xc}^T) V_{xc}.
% % \end{equation}
% % We can extract cross attention maps $Q_c K_x^T$ and visualize these as saliency maps. Another projection is applied to these concept embeddings and they added to a separate residual stream and passed to the next layer. 

% % erform a self-attention operation between the concept keys and queries 
% %     Q_c K_c^T
% % \end{equation}
% % These operations can be combined into a single parallel tensor operation
% % \begin{equation}
% %     Q_c K_{xc}^T 
% % \end{equation}
% % This can all be combined into a single tensor operation. 


% % to re-purposing parameters of \layername's to form a set of 

% % This limits the expressivity of these cross attention maps as a tool for explainability. 

% % The cross attention maps of UNet-based architectures 

% % leverage cross attention maps where a sequence of text tokens query a sequence of image patch keys. 

% % A natural choice for producing high quality saliency maps is to 

% % is to leverage the attention mechanisms of \layername s to produce cross attentino m

% % cross attention maps of \layername s. 
% % However, we have a self-attention operation where attention maps are formed by the matrix product $QK^T$, where we have an output matrix containing both cross-attentions between the prompt and image modalities and self attentions within each modality (see Figure \ref{}). We can pull out the cross attention map 


% % The matrix product $QK^T$ forms a matrix with $n + m$ columns and rows where $n$ is the number of text tokens in the prompt and $m$ is the number of image patches.

% \subsection{The Attention Output Space}

% We make another surprising discovery, which is that by performing linear projections in the attention output space we can produce even higher quality saliency maps than cross attentions. More precisely, let $o_c$ concept output vectors from Equation \ref{} and let $o_x$ be the attention mechanism output vectors for the image patches. We can perform linear projections $o_c o_x^T \in \mathbb{R}^{k \times n}$  to produce saliency maps for each of the $k$ concepts. Astonishingly, these maps produce saliency maps that better localize objects than cross attention maps as shown by our evaluation in Section \ref{}. These maps produced by \tool{} are unique to MM-DiT models as they leverage \textit{concept embeddings} corresponding to textual concepts which fundamentally can not be produced by UNet-based models. A qualitative comparison between the output space projections and cross attention maps can be seen in Figure \ref{}. 

% We take advantage of these properties to construct a dictionary of concept vectors $\mathcal{D} = \{d_1, \dots d_{|\mathcal{D}|}\}$ such that $d_i \in \mathbb{R}^{p}$. Given a list of textual concepts $c$ (e.g. ``man'', ``dog'', etc.) we can encode them as if they were a text prompt. 


% However, unlike UNet based models it is not possible to simply project this dictionary of concepts $\mathcal{D}^E$ using each layer's projection matrices. This is because the diffusion transformer iteratively updates the prompt tokens $p^L$. Instead we need to iteratively update the concept dictionary $\mathcal{D}^L$ in a similar fashion.  We can use the text modality projection matrices $K_p(\cdot), Q_p(\cdot), V_p(\cdot)$ from each attention layer on the concept vectors (i.e. $K_p(d_i)$). 

% One approach to processing this dictionary is to simply append them to the sequence of prompt representations and patch representations.  We then form an altered attention mechanism 
% \begin{equation}
%     x^{L + 1}, p^{L + 1}, d^{L + 1} = \softmax(QK^T) V
% \end{equation}
% % with the same parameters as the original DiT. 
% % We can then construct a sequence $x \mathbin\Vert p \mathbin\Vert d$ and perform the self attention operation. 


% A problem with this approach is that the concept vectors can now impact the appearance of the image. To solve this we only allow the image tokens to influence the concept tokens, but not the other way around. This can be achieved with two separate attention operations. The image patches and prompt vectors are processed in the same way as the original DiT
% \begin{equation}
%      x^{L + 1}, p^{L + 1} = \softmax(Q_{x, p}K_{x, p}^T) V_{x, p}
% \end{equation}
% where each of $K_{x, p}, Q_{x, p}$ and $V_{x, p}$ are concatenated sequences $K_{x, p} = [K_x(x_1), \dots, K_p(p_1), \dots K_p(p_n)]$. The concept dictionary tokens are updated as
% \begin{equation}
%     d^{L + 1} = \softmax(Q_{d} K_{x, d}^T) V_{x, d}
% \end{equation}
% where 
% \begin{align*}
%     Q_{d} &= [Q_p(d_1), \dots , Q_p(d_{|\mathcal{D}|})] \\ 
%     K_{x, d} &= [K_x(x_1), \dots, K_p(d_1), \dots K_p(d_{|\mathcal{D}|})] \\
%     V_{x, d} &= [V_x(x_1), \dots, V_p(d_1), \dots V_p(d_{|\mathcal{D}|})]    
% \end{align*}

% This amounts to a cross and self attention operation where the information from the image patches $x^L$ and the dictionary $d^L$ vectors are used to produce the next sequence of dictionary vectors $d^{L +1}$, but the dictionary does not influence the image or prompt representations. This can also be implemented in parallel with the attention mechanism for the prompt and image patches by masking the attention weights.  

