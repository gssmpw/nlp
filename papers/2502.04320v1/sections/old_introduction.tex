% \begin{figure}
%     \centering
%     \includegraphics[]{figures/code_snippet.png}
%     \caption{Caption}
%     \label{fig:enter-label}
% \end{figure} 

\section{Introduction}


Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? 
DiTs \cite{peebles_scalable_2023} have recently supplanted UNets \cite{} as the state of the art architecture for image generation. In particular, models like Flux \cite{} and SD3 \cite{} have achieved state of the art in text-to-image generation by leveraging multi-modal attention (\layername) layers, which create rich representations of both the user's text prompt and image patches. We ask the question, can we take advantage of these the rich text embeddings of \layername layers to improve the interpretability of text-to-image DiTs? 

Multi-modal DiTs incorporate text conditioning in a fundamentally different way to their UNet predecessors. UNets leverage shallow cross attention mechanisms between prompt embeddings and image patch representations. These \textit{cross attention maps} can be used to interpret UNet-based models, producing high-fidelty saliency maps depicting the presence of textual concepts \cite{tang_what_2022} and have numerous applications to tasks like image editing \cite{hertz_prompt--prompt_2022, chefer_attend-and-excite_2023}. In contrast, the attention layers of text-to-image DiTs iteratively process embeddings of both text and image modalities. This distinct approach to processing text embeddings presents new challenges as the embeddings are limited to mu

This creates rich textual contextualized embeddings that present novel opportunities for interpretability. 

Our key contributions can be summarized as follows:
\begin{enumerate}
    \item \textbf{\tool{}, a novel method for interpreting text-to-image DiTs.} We repurpose the parameters of \layername{} layers without any additional training to create a set of \textit{concept embeddings} which can be used to create high-quality saliency maps for any concept. 
    \item \textbf{We discover that DiT attention layer output representations can be used to create better saliency maps than widely adopted cross attentions. }
    \item \textbf{State of the art performance on zero-shot image segmentation benchmarks like ImageNet Segmentation and PascalVOC. }
    This provides evidence that multi-modal DiT representations are highly transferable zero-shot to important vision tasks like segmentation.
    \item \textbf{We apply \tool{} attention maps to improve the localization of image editing methods. }
    % for interpreting the representations of multi-modal DiTs that is capable of generating high-fidelity saliency maps depicting the location of textual concepts. }
    % \item \textbf{We discover that the output space of DiT attention layers can be used to construct saliency maps that are significantly better than widely adopted cross attention maps. }
\end{enumerate}

--- Ignore this older draft --


A substantial body of work on UNet-based diffusion models has observed that the cross attention maps between image patches and text-tokens can form saliency maps that locate textual concepts within images \cite{hertz_prompt--prompt_2022, tang_what_2022}. However, 



These cross attention mechanisms operate by projecting embeddings of prompt tokens into the same space as image patches. UNet architectures like Stable Diffusion \cite{} and SDXL \cite{} leverage a convolutional architecture to process the image patches and periodically incorporate cross attention layers.  



This is in contrast to UNet-based models which rely on shallow projections of text embeddings in the form of cross attention mechanisms.

We introduce \tool{}, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, \tool{} repurposes the parameters of DiT attention layers to produce highly contextualized \textit{concept embeddings}, contributing the major discovery that projecting image representations onto these embeddings in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention mechanisms.  Remarkably, \tool{} even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 10 other zero-shot interpretability methods on the ImageNet-Segmentation dataset (+5 mIoU, +4 mAP) and on a single-class subset of PascalVOC (+19 mIoU, +3 mAP). Our work contributes the first evidence that the representations of multi-modal DiT models like Flux are highly transferable to vision tasks like segmentation, even rivaling multi-modal foundation models like CLIP.





Recent models like Flux \cite{noauthor_black-forest-labsflux_nodate} and SD3 \cite{} have extended the capabilities of DiTs to text-to-image generation by incorporating \textit{multi-modal attention} (MMAttn) layers that jointly process both text and image tokens. Despite their scalability and success in generating high-fidelity images conditioned on text prompts, relatively little work has aimed to investigate the properties of the representations of multi-modal DiTs. 

% Extending beyond the text-to-image generation task, substantial effort has been devoted to training multi-modal foundation models, such as CLIP \cite{}, that learn rich representations of both text and image modalities. CLIP models learn rich features that transfer to a variety of important downstream vision tasks by learning to encode images and their text descriptions to a similar vector. However, a common criticism of CLIP models is that while they learn rich global image representations, they fail to learn high quality local image features \cite{}. This is in part due to the fact that image-text matching task encourages the model to learn only features necessary to discriminate between an image and a global caption. In this paper, we view DiTs as representation learners that learn rich encodings of images. Instead of being trained with contrastive learning, multi-modal DiTs are trained on the text-conditioned denoising task. We argue that the inductive biases of this task encourage DiTs to learn representations that are better suited to fine-grained vision tasks like image segmentation.

A substantial body of existing work investigating UNet-based diffusion models \cite{} has observed that the cross attention mechanisms between text and images can be used to create saliency maps depicting the spatial locations of textual concepts (See Figure \ref{}). These cross attention maps provide insight into how the model ``sees'' the image, and can be applied to numerous tasks like image editing \cite{}. However, unlike UNet-based models, which leverage shallow linear projections of text prompts at each cross attention layer, the \layername layers of DiTs iteratively process a concatenated sequence of both text and image tokens. This sequential dependency restricts the cross attention maps to only concepts that are explicitly present in user's text prompts, limiting their insight into the underlying representations. 

Our key contributions can be summarized as follows:
\begin{enumerate}
    \item \textbf{A method called \tool{}, that allows us to create high-quality saliency maps corresponding to arbitrary textual concepts in images}.
    We repurpose the parameters of \layername{} layers without any additional training to construct a set of contextualized concept embeddings that can be used to construct arbitrary cross attention maps, not just those in the user's text prompt. 
    
    % \layername{} layers communicate information in a bidirectional manner between the text and image tokens. We perform a one-directional attention operation between from the image tokens to a set of concept embeddings.
    % \layername{} layers allow information to flow in a bidirectional manner between the image patch tokens and prompt tokens. This allows the prompt to influence the appearance of the image, and unlike UNet based models, the image patches influence the representations of the prompt tokens. We made the observation that it is possible to leverage the existing parameters of \layername{} layers to form a one-directional \textit{concept attention}. This enables the information from the image patches to influence the concepts, but the concept embeddings can not influence the image representations.
    
    \item \textbf{We improve upon commonly used cross attention maps} by performing linear projections in the output space of \layername{} layers. These saliency maps are both qualitatively and quantitatively better than cross attention maps. 
    % With \tool{} each \layername{} layer returns a sequence of image embeddings, text prompt embeddings, and concept embeddings. 
    We found that it is possible to construct high-fidelity saliency maps by projecting the image embeddings onto our concept embeddings in the output space of \layername{} layers. This is fundamentally not possible with UNet-based model.

    \item \textbf{We evaluate \tool{} on zero-shot image segmentation on real world images and find it significantly outperforms a variety of CLIP ViT and Diffusion UNet interpretability methods}, indicating that DiTs have highly transferable representations to important vision tasks. 
    Our approach outperforms both cross attention and a CLIP ViT interpretability baselines by a significant margin on the ImageNet Segmentation benchmark (+n\% mIoU) and Pascal VOC (+n\% mIoU). 
    % \tool{} can be applied to produce segmentation maps of real world images and synthetically generated ones. 
    
    \item \textbf{We demonstrate that our concept maps can be used to generate masks for a variety of image-editing techniques to improve the quality of the edits and restrict edits to relevant image regions. } 
\end{enumerate}

% For our first major contribution, 

% For our first major contribution, we repurpose the parameters of MMAttention layers, enabling us to construct cross-attention maps for arbitrary concepts rather than being limited to the prompt tokens. We aimed to create a set of embeddings that each correspond to simple textual concepts (e.g. ``cat'', ``dog'', etc.) which we can then use to probe the DiT image representations. We call this \textit{concept attention}. This allows us to form cross-attention saliency maps for arbitrary concepts without altering the appearance of the image. 


% Our contributions can be summarized as follows:
% \begin{enumerate}
%     \item \textbf{We discover that the output space of attention layers encodes rich representations that can be used to produce high-fidelity saliency maps. }
%     \item \textbf{An extensive evaluation showing that the representaitons of DiT attention layers beat a variety of CLIP ViT methods on zero-shot image segmentaiton.}
%     \item 
% \end{enumerate}

% In summary, our contributions are. 

% \begin{enumerate}
%     \item \textbf{\tool{}, a method for producing saliency maps for arbitrary textual concepts. } \tool{} constructs a set of contextualized concept embeddings at inference time, which can be used to create both cross attention maps and output space maps. Our approach requires no additional training and has minimal impact on model latency and memory overhead. 
%     \item \textbf{We discover that the output space of DiT attention maps produces significantly higher quality saliency maps than commonly used cross attentions. } 
%     \item \textbf{We demonstrate that DiTs can serve as powerful image encoders with representations that transfer zero-shot to tasks like image segmentation. } \tool{} allows us to leverage the representations of DiTs without any additional training to produce segmentation maps of images. We significantly out perform a variety of Diffusion and CLIP ViT based interpretability methods on the ImageNet-Segmentation zero-shot image segmentation benchmark. We outperform the next best method by a margin of [Insert here]. 
%     \item \textbf{We improve the quality and disentanglement of several diffusion based image editing techniques by leveraging our segmentation maps. } 
% \end{enumerate}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/CrossAttentionTokens.pdf}
    \vspace{-0.2in}
    \caption{\textbf{Cross attention maps of the text prompt tokens of a Flux DiT model. } The cross attention maps corresponding to nouns in the provided text prompt produce saliency maps that depict the location of that noun in the generated image. }
    \label{fig:enter-label}
\end{figure}



% CLIP models learn rich features that transfer to data-scarce downstream tasks by taking advantage of cheap sources of data like image and caption pairs.  


% CLIP ViT iteratively update representations of image patches and 



% CLIP models leverage contrastive learning to encode images and their text descriptions to similar vectors. A common criticism of CLIP models is that they learn good global descriptions of images,

% aims to learn features that are highly-transferable to important downstream tasks like image segmentation by performaing contastive learning on highl 



% self-supervised or weakly supervised foundation models, such as CLIP \cite{} and DINO \cite{}. These models leverage highly available data, like image-caption pairs, to learn representations that are transferable to more data-scarce tasks like image segmentation. These models typically leverage a variation of contrastive learning. We analyze DiTs as multi-modal representation learners. 

% to learn representations that transfer effectively to important down-stream tasks. Foundation model training tasks aim to learn transferable features from available sources of data. 
% encouraging a model to learn a rich set of features that are highly relevant to down 

% We view the text-conditioned image denoising task as a pretraining task. 


% the capablities of DiTs 

% However, little investigation has been done into the particular properties of the representations of DiT models. 


% However, relatively little investigtion has been done into the



% However, despite their semantic richness, the high-dimensional representations of DiTs are difficult to interpret. This lack of interpretability limits the ability to both control and safely deploy diffusion models at scale. 




% Existing work has shown that self-supervised models like DINO \cite{} have emergent properties 




% % that by leveraging the fine-grained concept output maps produced by \tool{} it is possible to improve upon 



% % We even found that the saliency maps generated by this operation produce significantly higher quality 



% ---- 


% The cross attention layers of UNet based 

% UNet-based models like SDXL \cite{} incorporate 



% which incorporate text conditioning by performing shallow linear projections, DiTs iteratively update a joint sequence of text and image representations. 







% In this paper, we take advantage of the properties of multi-modal DiTs to produce high quality interpretations. 





% However, despite their semantic richness, the representations of DiT models are difficult to interpret. 





% Diffusion transformers (DiTs) are the architecture underpinning many recent state of the art text-to-image generation systems. However, despite their semantic richness, the high-dimensional representations of DiTs are difficult to interpret. 

% We introduce \tool{}, a method for interpreting DiTs by expressing image patches in terms of textual concepts. 

% Existing work has observed that the cross attention mechanisms between text and images can be 

% \tool{} repurposes the parameters of the DiT attention layers to allow arbitrary attention maps. Additionally, we discovered that it is possible to construct saliency maps by projecting the representations of the output space of attention mechanisms. These saliency maps are both qualitatively and quantitatively better. In fact, we demonstrate that these output saliency maps can be used to perform zero-shot image segmentation, outperforming a variety of CLIP ViT based interpretability methods. 
