\section{Conclusion}

We introduce \tool{}, a method for interpreting the rich features of multi-modal DiTs. Our approach allows a user to produce high quality saliency maps of an open-set of textual concepts that shed light on how a diffusion model ``sees'' an image. We perform an extensive evaluation of the saliency maps on zero-shot segmentation and find that they significantly outperform a variety of other zero-shot interpretability methods. Our results suggest the potential for DiT models to act as powerful and interpretable image encoders with representations that are transferable zero-shot to tasks like image segmentation. 

% \newpage

\section{Impact Statement}

Generative models for images have numerous ethical concerns: they have the potential to spread misinformation through realistic fake images (i.e. deepfakes), they may disrupt different creative industries, and have the potential to reinforce existing social biases present in their training data. Our work directly serves to improve the transparency of these models, and we believe our work could be used to understand the biases present in models. 

% the potential to spread misinformation through realistic fake images ("deepfakes"), exacerbate existing societal biases by replicating them in generated content, disrupt creative industries by automating image creation, raise concerns about privacy due to the potential for misuse of personal data, and create ethical dilemmas regarding the authenticity and veracity of visual information

\section{Acknowledgments}

This paper is supported by the National Science Foundation Graduate Research Fellowship. This work was also supported in part by Cisco, NSF \#2403297, gifts from Google, Amazon, Meta, NVIDIA, Avast, Fiddler Labs, Bosch. 