\section{Related Work}
\paragraph{Diffusion Model Interpretability}

A fair amount of existing work attempts to interpret diffusion models. Some works investigate diffusion models from an analytic lens \cite{kadkhodaie_generalization_2024, wang_diffusion_2024}, attempting to understand how diffusion models geometrically model the manifold of data. Other works attempt to understand how models memorize images \cite{carlini_extracting_2023}. An increasing body of work attempts to repurpose the representations of diffusion models for various tasks like classification \cite{li_your_2023}, segmentation \cite{karazija_diffusion_2024}, and even robotic control \cite{gupta_pre-trained_2024}. However, most relevant to our work is the substantial body of methods investigating how the representations of the neural network architectures underpinning diffusion can be used to garner insight into how these models work, steer their behavior, and improve their safety. 

Numerous papers have observed that the cross attention mechanisms of UNet-based diffusion models like Stable Diffusion \cite{rombach_high-resolution_2022} and SDXL \cite{podell_sdxl_2023} can produce interpretable saliency maps of textual concepts \cite{tang_what_2022}. Cross attention maps are used in a variety of image editing tasks like producing masks that localize objects of interest to edit \cite{dalva_fluxspace_2024}, controlling the layout of images \cite{chen_training-free_2023, epstein_diffusion_2023}, altering the appearance of an image but retaining its layout \cite{hertz_prompt--prompt_2022}, and even generating synthetic data to train instruction based editing models \cite{brooks_instructpix2pix_2023}. Other works observe that performing interventions on cross attention maps can improve the faithfulness of images to prompts by ensuring attributes are assigned to the correct objects \cite{meral_conform_2024, chefer_attend-and-excite_2023}.  Additionally, it has been observed that self-attention layers of diffusion models encode useful information about the layout of images \cite{liu_towards_2024}. 

%


\paragraph{Zero-shot Image Segmentation}
In this work, we evaluate \tool{} on the task of zero-shot image segmentation, which is a natural way to assess the accuracy of our saliency maps and the transferability of the representations of multi-modal DiT architectures to downstream vision tasks. This task also provides a good setting to compare to a variety of other interpretability methods for various foundation model architectures like CLIP \cite{radford_learning_2021}, DINO \cite{caron_emerging_2021}, and diffusion models.  

A variety of works train a diffusion models from scratch for the task of image segmentation \cite{amit_segdiff_2022, karazija_diffusion_2024} or attempt to fine-tune pretrained models \cite{baranchuk_label-efficient_2022}. Another line of work leverages diffusion models to generate synthetic data that can be used to train segmentation models that transfer zero-shot to new classes \cite{li_open-vocabulary_2023}. While effective, these methods are training-based and thus do not provide as much insight into the representations of existing text-to-image generation models, which is the key motivation behind \tool{}. 

A significant body of work attempts to improve the interpretability of CLIP vision transformers (ViTs) \cite{dosovitskiy_image_2021}. The authors of \cite{chefer_transformer_2021} develop a method for generating saliency maps for ViT models, and they introduce an evaluation protocol for assessing the effectiveness of these saliency maps. This evaluation protocol centers around the ImageNet-Segmentation dataset \cite{guillaumin_imagenet_2014}, and we extend this evaluation to the PascalVOC dataset \cite{everingham_pascal_2015}. They compare to a variety of zero-shot interpretability methods like GradCAM \cite{selvaraju_grad-cam_2019}, Layerwise-Relevance Propagation \cite{binder_layer-wise_2016}, raw attentions, and the Rollout method \cite{abnar_quantifying_2020}. The authors of \cite{gandelsman_interpreting_2024} demonstrate an approach to expressing image patches in terms of textual concepts. We also compare our approach to zero-shot diffusion based methods \cite{tang_what_2022, wang_diffusion_2024} and the self-attention maps of DINO ViT models \cite{caron_emerging_2021}. 

Another line of work attempts perform unsupervised segmentation without any class or text conditioning by performing clustering of the embeddings of models \cite{cho_picie_2021, hamilton_unsupervised_2022, tian_diffuse_2024}. Despite not producing class predictions, these models are often evaluated on semantic segmentation datasets by using approaches like Hungarian matching \cite{kuhn_hungarian_1955} to pair unlabeled segmentation predictions with the best matching ones in a multi-class semantic segmentation dataset. In contrast, \tool{} enables text conditioning so we do not compare to this family of methods. We also don't compare to models like SAM \cite{kirillov_segment_2023, ravi_sam_2024} as it is trained on a large scale dataset.  %