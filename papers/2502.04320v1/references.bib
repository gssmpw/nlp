
@misc{ravi_sam_2024,
	title = {{SAM} 2: {Segment} {Anything} in {Images} and {Videos}},
	shorttitle = {{SAM} 2},
	url = {http://arxiv.org/abs/2408.00714},
	doi = {10.48550/arXiv.2408.00714},
	abstract = {We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and Rädle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Dollár, Piotr and Feichtenhofer, Christoph},
	month = oct,
	year = {2024},
	note = {arXiv:2408.00714 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{kirillov_segment_2023,
	title = {Segment {Anything}},
	url = {http://arxiv.org/abs/2304.02643},
	doi = {10.48550/arXiv.2304.02643},
	abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02643 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{hamilton_unsupervised_2022,
	title = {Unsupervised {Semantic} {Segmentation} by {Distilling} {Feature} {Correspondences}},
	url = {http://arxiv.org/abs/2203.08414},
	doi = {10.48550/arXiv.2203.08414},
	abstract = {Unsupervised semantic segmentation aims to discover and localize semantically meaningful categories within image corpora without any form of annotation. To solve this task, algorithms must produce features for every pixel that are both semantically meaningful and compact enough to form distinct clusters. Unlike previous works which achieve this with a single end-to-end framework, we propose to separate feature learning from cluster compactification. Empirically, we show that current unsupervised feature learning frameworks already generate dense features whose correlations are semantically consistent. This observation motivates us to design STEGO (\${\textbackslash}textbf\{S\}\$elf-supervised \${\textbackslash}textbf\{T\}\$ransformer with \${\textbackslash}textbf\{E\}\$nergy-based \${\textbackslash}textbf\{G\}\$raph \${\textbackslash}textbf\{O\}\$ptimization), a novel framework that distills unsupervised features into high-quality discrete semantic labels. At the core of STEGO is a novel contrastive loss function that encourages features to form compact clusters while preserving their relationships across the corpora. STEGO yields a significant improvement over the prior state of the art, on both the CocoStuff (\${\textbackslash}textbf\{+14 mIoU\}\$) and Cityscapes (\${\textbackslash}textbf\{+9 mIoU\}\$) semantic segmentation challenges.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T.},
	month = mar,
	year = {2022},
	note = {arXiv:2203.08414 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kuhn_hungarian_1955,
	title = {The {Hungarian} method for the assignment problem},
	volume = {2},
	copyright = {Copyright © 1955 Wiley Periodicals, Inc., A Wiley Company},
	issn = {1931-9193},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109},
	doi = {10.1002/nav.3800020109},
	abstract = {Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the “assignment problem” is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.},
	language = {en},
	number = {1-2},
	urldate = {2025-01-30},
	journal = {Naval Research Logistics Quarterly},
	author = {Kuhn, H. W.},
	year = {1955},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800020109},
	pages = {83--97},
}

@misc{tian_diffuse_2024,
	title = {Diffuse, {Attend}, and {Segment}: {Unsupervised} {Zero}-{Shot} {Segmentation} using {Stable} {Diffusion}},
	shorttitle = {Diffuse, {Attend}, and {Segment}},
	url = {http://arxiv.org/abs/2308.12469},
	doi = {10.48550/arXiv.2308.12469},
	abstract = {Producing quality segmentation masks for images is a fundamental problem in computer vision. Recent research has explored large-scale supervised training to enable zero-shot segmentation on virtually any image style and unsupervised training to enable segmentation without dense annotations. However, constructing a model capable of segmenting anything in a zero-shot manner without any annotations is still challenging. In this paper, we propose to utilize the self-attention layers in stable diffusion models to achieve this goal because the pre-trained stable diffusion model has learned inherent concepts of objects within its attention layers. Specifically, we introduce a simple yet effective iterative merging process based on measuring KL divergence among attention maps to merge them into valid segmentation masks. The proposed method does not require any training or language dependency to extract quality segmentation for any images. On COCO-Stuff-27, our method surpasses the prior unsupervised zero-shot SOTA method by an absolute 26\% in pixel accuracy and 17\% in mean IoU. The project page is at {\textbackslash}url\{https://sites.google.com/view/diffseg/home\}.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Tian, Junjiao and Aggarwal, Lavisha and Colaco, Andrea and Kira, Zsolt and Gonzalez-Franco, Mar},
	month = apr,
	year = {2024},
	note = {arXiv:2308.12469 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cho_picie_2021,
	title = {{PiCIE}: {Unsupervised} {Semantic} {Segmentation} using {Invariance} and {Equivariance} in {Clustering}},
	shorttitle = {{PiCIE}},
	url = {http://arxiv.org/abs/2103.17070},
	doi = {10.48550/arXiv.2103.17070},
	abstract = {We present a new framework for semantic segmentation without annotations via clustering. Off-the-shelf clustering methods are limited to curated, single-label, and object-centric images yet real-world data are dominantly uncurated, multi-label, and scene-centric. We extend clustering from images to pixels and assign separate cluster membership to different instances within each image. However, solely relying on pixel-wise feature similarity fails to learn high-level semantic concepts and overfits to low-level visual cues. We propose a method to incorporate geometric consistency as an inductive bias to learn invariance and equivariance for photometric and geometric variations. With our novel learning objective, our framework can learn high-level semantic concepts. Our method, PiCIE (Pixel-level feature Clustering using Invariance and Equivariance), is the first method capable of segmenting both things and stuff categories without any hyperparameter tuning or task-specific pre-processing. Our method largely outperforms existing baselines on COCO and Cityscapes with +17.5 Acc. and +4.5 mIoU. We show that PiCIE gives a better initialization for standard supervised training. The code is available at https://github.com/janghyuncho/PiCIE.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Cho, Jang Hyun and Mall, Utkarsh and Bala, Kavita and Hariharan, Bharath},
	month = mar,
	year = {2021},
	note = {arXiv:2103.17070 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{labs_flux_2023,
	title = {{FLUX}},
	url = {https://github.com/black-forest-labs/flux},
	author = {Labs, Black Forest},
	year = {2023},
}

@misc{raffel_exploring_2023,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{xu_understanding_2019,
	title = {Understanding and {Improving} {Layer} {Normalization}},
	url = {http://arxiv.org/abs/1911.07013},
	doi = {10.48550/arXiv.1911.07013},
	abstract = {Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy. However, it is still unclear where the effectiveness stems from. In this paper, our main contribution is to take a step further in understanding LayerNorm. Many of previous studies believe that the success of LayerNorm comes from forward normalization. Unlike them, we find that the derivatives of the mean and variance are more important than forward normalization by re-centering and re-scaling backward gradients. Furthermore, we find that the parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases. Experiments show that a simple version of LayerNorm (LayerNorm-simple) without the bias and gain outperforms LayerNorm on four datasets. It obtains the state-of-the-art performance on En-Vi machine translation. To address the over-fitting problem, we propose a new normalization method, Adaptive Normalization (AdaNorm), by replacing the bias and gain with a new transformation function. Experiments show that AdaNorm demonstrates better results than LayerNorm on seven out of eight datasets.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Xu, Jingjing and Sun, Xu and Zhang, Zhiyuan and Zhao, Guangxiang and Lin, Junyang},
	month = nov,
	year = {2019},
	note = {arXiv:1911.07013 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{dalva_fluxspace_2024,
	title = {{FluxSpace}: {Disentangled} {Semantic} {Editing} in {Rectified} {Flow} {Transformers}},
	shorttitle = {{FluxSpace}},
	url = {http://arxiv.org/abs/2412.09611},
	doi = {10.48550/arXiv.2412.09611},
	abstract = {Rectified flow models have emerged as a dominant approach in image generation, showcasing impressive capabilities in high-quality image synthesis. However, despite their effectiveness in visual generation, rectified flow models often struggle with disentangled editing of images. This limitation prevents the ability to perform precise, attribute-specific modifications without affecting unrelated aspects of the image. In this paper, we introduce FluxSpace, a domain-agnostic image editing method leveraging a representation space with the ability to control the semantics of images generated by rectified flow transformers, such as Flux. By leveraging the representations learned by the transformer blocks within the rectified flow models, we propose a set of semantically interpretable representations that enable a wide range of image editing tasks, from fine-grained image editing to artistic creation. This work offers a scalable and effective image editing approach, along with its disentanglement capabilities.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Dalva, Yusuf and Venkatesh, Kavana and Yanardag, Pinar},
	month = dec,
	year = {2024},
	note = {arXiv:2412.09611 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_flow_2022,
	title = {Flow {Straight} and {Fast}: {Learning} to {Generate} and {Transfer} {Data} with {Rectified} {Flow}},
	shorttitle = {Flow {Straight} and {Fast}},
	url = {http://arxiv.org/abs/2209.03003},
	doi = {10.48550/arXiv.2209.03003},
	abstract = {We present rectified flow, a surprisingly simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions {\textbackslash}pi\_0 and {\textbackslash}pi\_1, hence providing a unified solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectified flow is to learn the ODE to follow the straight paths connecting the points drawn from {\textbackslash}pi\_0 and {\textbackslash}pi\_1 as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are special and preferred because they are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efficient models. We show that the procedure of learning a rectified flow from data, called rectification, turns an arbitrary coupling of {\textbackslash}pi\_0 and {\textbackslash}pi\_1 to a new deterministic coupling with provably non-increasing convex transport costs. In addition, recursively applying rectification allows us to obtain a sequence of flows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, we show that rectified flow performs superbly on image generation, image-to-image translation, and domain adaptation. In particular, on image generation and translation, our method yields nearly straight flows that give high quality results even with a single Euler discretization step.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
	month = sep,
	year = {2022},
	note = {arXiv:2209.03003 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{lipman_flow_2023,
	title = {Flow {Matching} for {Generative} {Modeling}},
	url = {http://arxiv.org/abs/2210.02747},
	doi = {10.48550/arXiv.2210.02747},
	abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Lipman, Yaron and Chen, Ricky T. Q. and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
	month = feb,
	year = {2023},
	note = {arXiv:2210.02747 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{carlini_extracting_2023,
	title = {Extracting {Training} {Data} from {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2301.13188},
	doi = {10.48550/arXiv.2301.13188},
	abstract = {Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tramèr, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13188 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{liu_flow_2022-1,
	title = {Flow {Straight} and {Fast}: {Learning} to {Generate} and {Transfer} {Data} with {Rectified} {Flow}},
	shorttitle = {Flow {Straight} and {Fast}},
	url = {http://arxiv.org/abs/2209.03003},
	doi = {10.48550/arXiv.2209.03003},
	abstract = {We present rectified flow, a surprisingly simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions {\textbackslash}pi\_0 and {\textbackslash}pi\_1, hence providing a unified solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectified flow is to learn the ODE to follow the straight paths connecting the points drawn from {\textbackslash}pi\_0 and {\textbackslash}pi\_1 as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are special and preferred because they are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efficient models. We show that the procedure of learning a rectified flow from data, called rectification, turns an arbitrary coupling of {\textbackslash}pi\_0 and {\textbackslash}pi\_1 to a new deterministic coupling with provably non-increasing convex transport costs. In addition, recursively applying rectification allows us to obtain a sequence of flows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, we show that rectified flow performs superbly on image generation, image-to-image translation, and domain adaptation. In particular, on image generation and translation, our method yields nearly straight flows that give high quality results even with a single Euler discretization step.},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
	month = sep,
	year = {2022},
	note = {arXiv:2209.03003 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{wang_diffusion_2024,
	title = {Diffusion {Models} {Learn} {Low}-{Dimensional} {Distributions} via {Subspace} {Clustering}},
	url = {http://arxiv.org/abs/2409.02426},
	doi = {10.48550/arXiv.2409.02426},
	abstract = {Recent empirical studies have demonstrated that diffusion models can effectively learn the image distribution and generate new samples. Remarkably, these models can achieve this even with a small number of training samples despite a large image dimension, circumventing the curse of dimensionality. In this work, we provide theoretical insights into this phenomenon by leveraging key empirical observations: (i) the low intrinsic dimensionality of image data, (ii) a union of manifold structure of image data, and (iii) the low-rank property of the denoising autoencoder in trained diffusion models. These observations motivate us to assume the underlying data distribution of image data as a mixture of low-rank Gaussians and to parameterize the denoising autoencoder as a low-rank model according to the score function of the assumed distribution. With these setups, we rigorously show that optimizing the training loss of diffusion models is equivalent to solving the canonical subspace clustering problem over the training samples. Based on this equivalence, we further show that the minimal number of samples required to learn the underlying distribution scales linearly with the intrinsic dimensions under the above data and model assumptions. This insight sheds light on why diffusion models can break the curse of dimensionality and exhibit the phase transition in learning distributions. Moreover, we empirically establish a correspondence between the subspaces and the semantic representations of image data, facilitating image editing. We validate these results with corroborated experimental results on both simulated distributions and image datasets.},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Wang, Peng and Zhang, Huijie and Zhang, Zekai and Chen, Siyi and Ma, Yi and Qu, Qing},
	month = dec,
	year = {2024},
	note = {arXiv:2409.02426 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{kadkhodaie_generalization_2024,
	title = {Generalization in diffusion models arises from geometry-adaptive harmonic representations},
	url = {http://arxiv.org/abs/2310.02557},
	doi = {10.48550/arXiv.2310.02557},
	abstract = {Deep neural networks (DNNs) trained for image denoising are able to generate high-quality samples with score-based reverse diffusion algorithms. These impressive capabilities seem to imply an escape from the curse of dimensionality, but recent reports of memorization of the training set raise the question of whether these networks are learning the "true" continuous density of the data. Here, we show that two DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, when the number of training images is large enough. In this regime of strong generalization, diffusion-generated images are distinct from the training set, and are of high visual quality, suggesting that the inductive biases of the DNNs are well-aligned with the data density. We analyze the learned denoising functions and show that the inductive biases give rise to a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous regions. We demonstrate that trained denoisers are inductively biased towards these geometry-adaptive harmonic bases since they arise not only when the network is trained on photographic images, but also when it is trained on image classes supported on low-dimensional manifolds for which the harmonic basis is suboptimal. Finally, we show that when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic, the denoising performance of the networks is near-optimal.},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Kadkhodaie, Zahra and Guth, Florentin and Simoncelli, Eero P. and Mallat, Stéphane},
	month = apr,
	year = {2024},
	note = {arXiv:2310.02557 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_your_2023,
	title = {Your {Diffusion} {Model} is {Secretly} a {Zero}-{Shot} {Classifier}},
	url = {http://arxiv.org/abs/2303.16203},
	doi = {10.48550/arXiv.2303.16203},
	abstract = {The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has significantly stronger multimodal compositional reasoning ability than competing discriminative approaches. Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. Our models achieve strong classification performance using only weak augmentations and exhibit qualitatively better "effective robustness" to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks. Results and visualizations at https://diffusion-classifier.github.io/},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Li, Alexander C. and Prabhudesai, Mihir and Duggal, Shivam and Brown, Ellis and Pathak, Deepak},
	month = sep,
	year = {2023},
	note = {arXiv:2303.16203 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
}

@misc{li_your_2023-1,
	title = {Your {Diffusion} {Model} is {Secretly} a {Zero}-{Shot} {Classifier}},
	url = {http://arxiv.org/abs/2303.16203},
	doi = {10.48550/arXiv.2303.16203},
	abstract = {The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has significantly stronger multimodal compositional reasoning ability than competing discriminative approaches. Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. Our models achieve strong classification performance using only weak augmentations and exhibit qualitatively better "effective robustness" to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks. Results and visualizations at https://diffusion-classifier.github.io/},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Li, Alexander C. and Prabhudesai, Mihir and Duggal, Shivam and Brown, Ellis and Pathak, Deepak},
	month = sep,
	year = {2023},
	note = {arXiv:2303.16203 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
}

@article{everingham_pascal_2015,
	title = {The {Pascal} {Visual} {Object} {Classes} {Challenge}: {A} {Retrospective}},
	volume = {111},
	issn = {1573-1405},
	shorttitle = {The {Pascal} {Visual} {Object} {Classes} {Challenge}},
	url = {https://doi.org/10.1007/s11263-014-0733-5},
	doi = {10.1007/s11263-014-0733-5},
	abstract = {The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008–2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community’s progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.},
	language = {en},
	number = {1},
	urldate = {2025-01-25},
	journal = {International Journal of Computer Vision},
	author = {Everingham, Mark and Eslami, S. M. Ali and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	month = jan,
	year = {2015},
	keywords = {Artificial Intelligence, Benchmark, Database, Object detection, Object recognition, Segmentation},
	pages = {98--136},
}

@misc{abnar_quantifying_2020,
	title = {Quantifying {Attention} {Flow} in {Transformers}},
	url = {http://arxiv.org/abs/2005.00928},
	doi = {10.48550/arXiv.2005.00928},
	abstract = {In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Abnar, Samira and Zuidema, Willem},
	month = may,
	year = {2020},
	note = {arXiv:2005.00928 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{li_open-vocabulary_2023,
	title = {Open-vocabulary {Object} {Segmentation} with {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2301.05221},
	doi = {10.48550/arXiv.2301.05221},
	abstract = {The goal of this paper is to extract the visual-language correspondence from a pre-trained text-to-image diffusion model, in the form of segmentation map, i.e., simultaneously generating images and segmentation masks for the corresponding visual entities described in the text prompt. We make the following contributions: (i) we pair the existing Stable Diffusion model with a novel grounding module, that can be trained to align the visual and textual embedding space of the diffusion model with only a small number of object categories; (ii) we establish an automatic pipeline for constructing a dataset, that consists of \{image, segmentation mask, text prompt\} triplets, to train the proposed grounding module; (iii) we evaluate the performance of open-vocabulary grounding on images generated from the text-to-image diffusion model and show that the module can well segment the objects of categories beyond seen ones at training time; (iv) we adopt the augmented diffusion model to build a synthetic semantic segmentation dataset, and show that, training a standard segmentation model on such dataset demonstrates competitive performance on the zero-shot segmentation(ZS3) benchmark, which opens up new opportunities for adopting the powerful diffusion model for discriminative tasks.},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Li, Ziyi and Zhou, Qinye and Zhang, Xiaoyun and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
	month = aug,
	year = {2023},
	note = {arXiv:2301.05221 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{baranchuk_label-efficient_2022,
	title = {Label-{Efficient} {Semantic} {Segmentation} with {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.03126},
	doi = {10.48550/arXiv.2112.03126},
	abstract = {Denoising diffusion probabilistic models have recently received much research attention since they outperform alternative approaches, such as GANs, and currently provide state-of-the-art generative performance. The superior performance of diffusion models has made them an appealing tool in several applications, including inpainting, super-resolution, and semantic editing. In this paper, we demonstrate that diffusion models can also serve as an instrument for semantic segmentation, especially in the setup when labeled data is scarce. In particular, for several pretrained diffusion models, we investigate the intermediate activations from the networks that perform the Markov step of the reverse diffusion process. We show that these activations effectively capture the semantic information from an input image and appear to be excellent pixel-level representations for the segmentation problem. Based on these observations, we describe a simple segmentation method, which can work even if only a few training images are provided. Our approach significantly outperforms the existing alternatives on several datasets for the same amount of human supervision.},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Baranchuk, Dmitry and Rubachev, Ivan and Voynov, Andrey and Khrulkov, Valentin and Babenko, Artem},
	month = mar,
	year = {2022},
	note = {arXiv:2112.03126 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{amit_segdiff_2022,
	title = {{SegDiff}: {Image} {Segmentation} with {Diffusion} {Probabilistic} {Models}},
	shorttitle = {{SegDiff}},
	url = {http://arxiv.org/abs/2112.00390},
	doi = {10.48550/arXiv.2112.00390},
	abstract = {Diffusion Probabilistic Methods are employed for state-of-the-art image generation. In this work, we present a method for extending such models for performing image segmentation. The method learns end-to-end, without relying on a pre-trained backbone. The information in the input image and in the current estimation of the segmentation map is merged by summing the output of two encoders. Additional encoding layers and a decoder are then used to iteratively refine the segmentation map, using a diffusion model. Since the diffusion model is probabilistic, it is applied multiple times, and the results are merged into a final segmentation map. The new method produces state-of-the-art results on the Cityscapes validation set, the Vaihingen building segmentation benchmark, and the MoNuSeg dataset.},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Amit, Tomer and Shaharbany, Tal and Nachmani, Eliya and Wolf, Lior},
	month = sep,
	year = {2022},
	note = {arXiv:2112.00390 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{wang_diffusion_2024-1,
	title = {Diffusion {Model} is {Secretly} a {Training}-free {Open} {Vocabulary} {Semantic} {Segmenter}},
	url = {http://arxiv.org/abs/2309.02773},
	doi = {10.48550/arXiv.2309.02773},
	abstract = {The pre-trained text-image discriminative models, such as CLIP, has been explored for open-vocabulary semantic segmentation with unsatisfactory results due to the loss of crucial localization information and awareness of object shapes. Recently, there has been a growing interest in expanding the application of generative models from generation tasks to semantic segmentation. These approaches utilize generative models either for generating annotated data or extracting features to facilitate semantic segmentation. This typically involves generating a considerable amount of synthetic data or requiring additional mask annotations. To this end, we uncover the potential of generative text-to-image diffusion models (e.g., Stable Diffusion) as highly efficient open-vocabulary semantic segmenters, and introduce a novel training-free approach named DiffSegmenter. The insight is that to generate realistic objects that are semantically faithful to the input text, both the complete object shapes and the corresponding semantics are implicitly learned by diffusion models. We discover that the object shapes are characterized by the self-attention maps while the semantics are indicated through the cross-attention maps produced by the denoising U-Net, forming the basis of our segmentation results.Additionally, we carefully design effective textual prompts and a category filtering mechanism to further enhance the segmentation results. Extensive experiments on three benchmark datasets show that the proposed DiffSegmenter achieves impressive results for open-vocabulary semantic segmentation.},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Wang, Jinglong and Li, Xiawei and Zhang, Jing and Xu, Qingyuan and Zhou, Qin and Yu, Qian and Sheng, Lu and Xu, Dong},
	month = jan,
	year = {2024},
	note = {arXiv:2309.02773 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{liu_towards_2024,
	title = {Towards {Understanding} {Cross} and {Self}-{Attention} in {Stable} {Diffusion} for {Text}-{Guided} {Image} {Editing}},
	url = {http://arxiv.org/abs/2403.03431},
	doi = {10.48550/arXiv.2403.03431},
	abstract = {Deep Text-to-Image Synthesis (TIS) models such as Stable Diffusion have recently gained significant popularity for creative Text-to-image generation. Yet, for domain-specific scenarios, tuning-free Text-guided Image Editing (TIE) is of greater importance for application developers, which modify objects or object properties in images by manipulating feature components in attention layers during the generation process. However, little is known about what semantic meanings these attention layers have learned and which parts of the attention maps contribute to the success of image editing. In this paper, we conduct an in-depth probing analysis and demonstrate that cross-attention maps in Stable Diffusion often contain object attribution information that can result in editing failures. In contrast, self-attention maps play a crucial role in preserving the geometric and shape details of the source image during the transformation to the target image. Our analysis offers valuable insights into understanding cross and self-attention maps in diffusion models. Moreover, based on our findings, we simplify popular image editing methods and propose a more straightforward yet more stable and efficient tuning-free procedure that only modifies self-attention maps of the specified attention layers during the denoising process. Experimental results show that our simplified method consistently surpasses the performance of popular approaches on multiple datasets.},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Liu, Bingyan and Wang, Chengyu and Cao, Tingfeng and Jia, Kui and Huang, Jun},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03431 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{meral_conform_2024,
	title = {{CONFORM}: {Contrast} is {All} {You} {Need} for {High}-{Fidelity} {Text}-to-{Image} {Diffusion} {Models}},
	shorttitle = {{CONFORM}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Meral_CONFORM_Contrast_is_All_You_Need_for_High-Fidelity_Text-to-Image_Diffusion_CVPR_2024_paper.html},
	language = {en},
	urldate = {2025-01-25},
	author = {Meral, Tuna Han Salih and Simsar, Enis and Tombari, Federico and Yanardag, Pinar},
	year = {2024},
	pages = {9005--9014},
}

@misc{podell_sdxl_2023,
	title = {{SDXL}: {Improving} {Latent} {Diffusion} {Models} for {High}-{Resolution} {Image} {Synthesis}},
	shorttitle = {{SDXL}},
	url = {http://arxiv.org/abs/2307.01952},
	doi = {10.48550/arXiv.2307.01952},
	abstract = {We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and Müller, Jonas and Penna, Joe and Rombach, Robin},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01952 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	doi = {10.48550/arXiv.2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_training-free_2023,
	title = {Training-{Free} {Layout} {Control} with {Cross}-{Attention} {Guidance}},
	url = {http://arxiv.org/abs/2304.03373},
	doi = {10.48550/arXiv.2304.03373},
	abstract = {Recent diffusion-based generators can produce high-quality images from textual prompts. However, they often disregard textual instructions that specify the spatial layout of the composition. We propose a simple approach that achieves robust layout control without the need for training or fine-tuning of the image generator. Our technique manipulates the cross-attention layers that the model uses to interface textual and visual information and steers the generation in the desired direction given, e.g., a user-specified layout. To determine how to best guide attention, we study the role of attention maps and explore two alternative strategies, forward and backward guidance. We thoroughly evaluate our approach on three benchmarks and provide several qualitative examples and a comparative analysis of the two strategies that demonstrate the superiority of backward guidance compared to forward guidance, as well as prior work. We further demonstrate the versatility of layout guidance by extending it to applications such as editing the layout and context of real images.},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Chen, Minghao and Laina, Iro and Vedaldi, Andrea},
	month = nov,
	year = {2023},
	note = {arXiv:2304.03373 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{epstein_diffusion_2023,
	title = {Diffusion {Self}-{Guidance} for {Controllable} {Image} {Generation}},
	url = {http://arxiv.org/abs/2306.00986},
	doi = {10.48550/arXiv.2306.00986},
	abstract = {Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. We also show that self-guidance can be used to edit real images. For results and an interactive demo, see our project page at https://dave.ml/selfguidance/},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Epstein, Dave and Jabri, Allan and Poole, Ben and Efros, Alexei A. and Holynski, Aleksander},
	month = jun,
	year = {2023},
	note = {arXiv:2306.00986 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{karazija_diffusion_2024,
	title = {Diffusion {Models} for {Open}-{Vocabulary} {Segmentation}},
	url = {http://arxiv.org/abs/2306.09316},
	doi = {10.48550/arXiv.2306.09316},
	abstract = {Open-vocabulary segmentation is the task of segmenting anything that can be named in an image. Recently, large-scale vision-language modelling has led to significant advances in open-vocabulary segmentation, but at the cost of gargantuan and increasing training and annotation efforts. Hence, we ask if it is possible to use existing foundation models to synthesise on-demand efficient segmentation algorithms for specific class sets, making them applicable in an open-vocabulary setting without the need to collect further data, annotations or perform training. To that end, we present OVDiff, a novel method that leverages generative text-to-image diffusion models for unsupervised open-vocabulary segmentation. OVDiff synthesises support image sets for arbitrary textual categories, creating for each a set of prototypes representative of both the category and its surrounding context (background). It relies solely on pre-trained components and outputs the synthesised segmenter directly, without training. Our approach shows strong performance on a range of benchmarks, obtaining a lead of more than 5\% over prior work on PASCAL VOC.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Karazija, Laurynas and Laina, Iro and Vedaldi, Andrea and Rupprecht, Christian},
	month = sep,
	year = {2024},
	note = {arXiv:2306.09316 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{abnar_quantifying_2020-1,
	title = {Quantifying {Attention} {Flow} in {Transformers}},
	url = {http://arxiv.org/abs/2005.00928},
	doi = {10.48550/arXiv.2005.00928},
	abstract = {In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Abnar, Samira and Zuidema, Willem},
	month = may,
	year = {2020},
	note = {arXiv:2005.00928 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
	number = {2},
	urldate = {2025-01-23},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	note = {arXiv:1610.02391 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {336--359},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.14294},
	doi = {10.48550/arXiv.2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = may,
	year = {2021},
	note = {arXiv:2104.14294 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{binder_layer-wise_2016,
	title = {Layer-wise {Relevance} {Propagation} for {Neural} {Networks} with {Local} {Renormalization} {Layers}},
	url = {http://arxiv.org/abs/1604.00825},
	doi = {10.48550/arXiv.1604.00825},
	abstract = {Layer-wise relevance propagation is a framework which allows to decompose the prediction of a deep neural network computed over a sample, e.g. an image, down to relevance scores for the single input dimensions of the sample such as subpixels of an image. While this approach can be applied directly to generalized linear mappings, product type non-linearities are not covered. This paper proposes an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks. We evaluate the proposed method for local renormalization layers on the CIFAR-10, Imagenet and MIT Places datasets.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Binder, Alexander and Montavon, Grégoire and Bach, Sebastian and Müller, Klaus-Robert and Samek, Wojciech},
	month = apr,
	year = {2016},
	note = {arXiv:1604.00825 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}



@misc{noauthor_black-forest-labsflux_nodate,
	title = {black-forest-labs/flux: {Official} inference repo for {FLUX}.1 models},
	url = {https://github.com/black-forest-labs/flux},
	urldate = {2025-01-19},
}

@misc{gupta_pre-trained_2024,
	title = {Pre-trained {Text}-to-{Image} {Diffusion} {Models} {Are} {Versatile} {Representation} {Learners} for {Control}},
	url = {http://arxiv.org/abs/2405.05852},
	doi = {10.48550/arXiv.2405.05852},
	abstract = {Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs. Such capabilities are difficult to learn solely from task-specific data. This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains. However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control. To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information. Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments. We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks. Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark.},
	urldate = {2025-01-19},
	publisher = {arXiv},
	author = {Gupta, Gunshi and Yadav, Karmesh and Gal, Yarin and Batra, Dhruv and Kira, Zsolt and Lu, Cong and Rudner, Tim G. J.},
	month = may,
	year = {2024},
	note = {arXiv:2405.05852 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{noauthor_ddp-homework-01-14_nodate,
	title = {{DDP}-{Homework}-01-14},
	url = {https://www.overleaf.com/project/67871fadd505ade9ad36ab62},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2025-01-15},
}

@misc{noauthor_enter_nodate,
	title = {Enter {Booking} {Details} - {Calendly}},
	url = {https://calendly.com/polochau/30min/2025-01-17T16:00:00-05:00?month=2025-01&date=2025-01-17},
	urldate = {2025-01-14},
}

@article{guillaumin_imagenet_2014,
	title = {{ImageNet} {Auto}-{Annotation} with {Segmentation} {Propagation}},
	volume = {110},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-014-0713-9},
	doi = {10.1007/s11263-014-0713-9},
	abstract = {ImageNet is a large-scale hierarchical database of object classes with millions of images.We propose to automatically populate it with pixelwise object-background segmentations, by leveraging existing manual annotations in the form of class labels and bounding-boxes. The key idea is to recursively exploit images segmented so far to guide the segmentation of new images. At each stage this propagation process expands into the images which are easiest to segment at that point in time, e.g. by moving to the semantically most related classes to those segmented so far. The propagation of segmentation occurs both (a) at the image level, by transferring existing segmentations to estimate the probability of a pixel to be foreground, and (b) at the class level, by jointly segmenting images of the same class and by importing the appearance models of classes that are already segmented. Through experiments on 577 classes and 500k images we show that our technique (i) annotates a wide range of classes with accurate segmentations; (ii) effectively exploits the hierarchical structure of ImageNet; (iii) scales efficiently, especially when implemented on superpixels; (iv) outperforms a baseline GrabCut (Rother et al. 2004) initialized on the image center, as well as segmentation transfer from a fixed source pool and run independently on each target image (Kuettel and Ferrari 2012). Moreover, our method also delivers state-of-the-art results on the recent iCoseg dataset for co-segmentation.},
	language = {en},
	number = {3},
	urldate = {2024-12-06},
	journal = {International Journal of Computer Vision},
	author = {Guillaumin, Matthieu and Küttel, Daniel and Ferrari, Vittorio},
	month = dec,
	year = {2014},
	keywords = {Artificial Intelligence, Figure-ground segmentation, ImageNet, Knowledge transfer, Large-scale computer vision, Object localization},
	pages = {328--348},
}

@misc{chefer_transformer_2021,
	title = {Transformer {Interpretability} {Beyond} {Attention} {Visualization}},
	url = {http://arxiv.org/abs/2012.09838},
	doi = {10.48550/arXiv.2012.09838},
	abstract = {Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the Deep Taylor Decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
	month = apr,
	year = {2021},
	note = {arXiv:2012.09838 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_diffusion-based_2022,
	title = {Diffusion-{Based} {Scene} {Graph} to {Image} {Generation} with {Masked} {Contrastive} {Pre}-{Training}},
	url = {http://arxiv.org/abs/2211.11138},
	abstract = {Generating images from graph-structured inputs, such as scene graphs, is uniquely challenging due to the difficulty of aligning nodes and connections in graphs with objects and their relations in images. Most existing methods address this challenge by using scene layouts, which are image-like representations of scene graphs designed to capture the coarse structures of scene images. Because scene layouts are manually crafted, the alignment with images may not be fully optimized, causing suboptimal compliance between the generated images and the original scene graphs. To tackle this issue, we propose to learn scene graph embeddings by directly optimizing their alignment with images. Specifically, we pre-train an encoder to extract both global and local information from scene graphs that are predictive of the corresponding images, relying on two loss functions: masked autoencoding loss and contrastive loss. The former trains embeddings by reconstructing randomly masked image regions, while the latter trains embeddings to discriminate between compliant and non-compliant images according to the scene graph. Given these embeddings, we build a latent diffusion model to generate images from scene graphs. The resulting method, called SGDiff, allows for the semantic manipulation of generated images by modifying scene graph nodes and connections. On the Visual Genome and COCO-Stuff datasets, we demonstrate that SGDiff outperforms state-of-the-art methods, as measured by both the Inception Score and Fr{\textbackslash}'echet Inception Distance (FID) metrics. We will release our source code and trained models at https://github.com/YangLing0818/SGDiff.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Yang, Ling and Huang, Zhilin and Song, Yang and Hong, Shenda and Li, Guohao and Zhang, Wentao and Cui, Bin and Ghanem, Bernard and Yang, Ming-Hsuan},
	month = nov,
	year = {2022},
	note = {arXiv:2211.11138 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{epstein_diffusion_2023-1,
	title = {Diffusion {Self}-{Guidance} for {Controllable} {Image} {Generation}},
	url = {http://arxiv.org/abs/2306.00986},
	abstract = {Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. We also show that self-guidance can be used to edit real images. For results and an interactive demo, see our project page at https://dave.ml/selfguidance/},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Epstein, Dave and Jabri, Allan and Poole, Ben and Efros, Alexei A. and Holynski, Aleksander},
	month = jun,
	year = {2023},
	note = {arXiv:2306.00986 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{zhang_adding_2023,
	title = {Adding {Conditional} {Control} to {Text}-to-{Image} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2302.05543},
	doi = {10.48550/arXiv.2302.05543},
	abstract = {We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small ({\textless} 50k). Moreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices. Alternatively, if powerful computation clusters are available, the model can scale to large amounts (millions to billions) of data. We report that large diffusion models like Stable Diffusion can be augmented with ControlNets to enable conditional inputs like edge maps, segmentation maps, keypoints, etc. This may enrich the methods to control large diffusion models and further facilitate related applications.},
	urldate = {2023-03-18},
	publisher = {arXiv},
	author = {Zhang, Lvmin and Agrawala, Maneesh},
	month = feb,
	year = {2023},
	note = {arXiv:2302.05543 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Human-Computer Interaction, Computer Science - Multimedia},
}

@misc{zhan_capability-aware_2024,
	title = {Capability-aware {Prompt} {Reformulation} {Learning} for {Text}-to-{Image} {Generation}},
	url = {http://arxiv.org/abs/2403.19716},
	abstract = {Text-to-image generation systems have emerged as revolutionary tools in the realm of artistic creation, offering unprecedented ease in transforming textual prompts into visual art. However, the efficacy of these systems is intricately linked to the quality of user-provided prompts, which often poses a challenge to users unfamiliar with prompt crafting. This paper addresses this challenge by leveraging user reformulation data from interaction logs to develop an automatic prompt reformulation model. Our in-depth analysis of these logs reveals that user prompt reformulation is heavily dependent on the individual user's capability, resulting in significant variance in the quality of reformulation pairs. To effectively use this data for training, we introduce the Capability-aware Prompt Reformulation (CAPR) framework. CAPR innovatively integrates user capability into the reformulation process through two key components: the Conditional Reformulation Model (CRM) and Configurable Capability Features (CCF). CRM reformulates prompts according to a specified user capability, as represented by CCF. The CCF, in turn, offers the flexibility to tune and guide the CRM's behavior. This enables CAPR to effectively learn diverse reformulation strategies across various user capacities and to simulate high-capability user reformulation during inference. Extensive experiments on standard text-to-image generation benchmarks showcase CAPR's superior performance over existing baselines and its remarkable robustness on unseen systems. Furthermore, comprehensive analyses validate the effectiveness of different components. CAPR can facilitate user-friendly interaction with text-to-image systems and make advanced artistic creation more achievable for a broader range of users.},
	language = {en},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Zhan, Jingtao and Ai, Qingyao and Liu, Yiqun and Chen, Jia and Ma, Shaoping},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19716 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval},
}

@misc{parmar_zero-shot_2023,
	title = {Zero-shot {Image}-to-{Image} {Translation}},
	url = {http://arxiv.org/abs/2302.03027},
	abstract = {Large-scale text-to-image generative models have shown their remarkable ability to synthesize diverse and high-quality images. However, it is still challenging to directly apply these models for editing real images for two reasons. First, it is hard for users to come up with a perfect text prompt that accurately describes every visual detail in the input image. Second, while existing models can introduce desirable changes in certain regions, they often dramatically alter the input content and introduce unexpected changes in unwanted regions. In this work, we propose pix2pix-zero, an image-to-image translation method that can preserve the content of the original image without manual prompting. We first automatically discover editing directions that reflect desired edits in the text embedding space. To preserve the general content structure after editing, we further propose cross-attention guidance, which aims to retain the cross-attention maps of the input image throughout the diffusion process. In addition, our method does not need additional training for these edits and can directly use the existing pre-trained text-to-image diffusion model. We conduct extensive experiments and show that our method outperforms existing and concurrent works for both real and synthetic image editing.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Parmar, Gaurav and Singh, Krishna Kumar and Zhang, Richard and Li, Yijun and Lu, Jingwan and Zhu, Jun-Yan},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03027 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{chen_training-free_2023-1,
	title = {Training-{Free} {Layout} {Control} with {Cross}-{Attention} {Guidance}},
	url = {http://arxiv.org/abs/2304.03373},
	abstract = {Recent diffusion-based generators can produce high-quality images based only on textual prompts. However, they do not correctly interpret instructions that specify the spatial layout of the composition. We propose a simple approach that can achieve robust layout control without requiring training or fine-tuning the image generator. Our technique, which we call layout guidance, manipulates the cross-attention layers that the model uses to interface textual and visual information and steers the reconstruction in the desired direction given, e.g., a user-specified layout. In order to determine how to best guide attention, we study the role of different attention maps when generating images and experiment with two alternative strategies, forward and backward guidance. We evaluate our method quantitatively and qualitatively with several experiments, validating its effectiveness. We further demonstrate its versatility by extending layout guidance to the task of editing the layout and context of a given real image.},
	urldate = {2023-07-13},
	publisher = {arXiv},
	author = {Chen, Minghao and Laina, Iro and Vedaldi, Andrea},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03373 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{murphy_learning_2012,
	title = {Learning {Effective} and {Interpretable} {Semantic} {Models} using {Non}-{Negative} {Sparse} {Embedding}},
	url = {https://www.semanticscholar.org/paper/Learning-Effective-and-Interpretable-Semantic-using-Murphy-Talukdar/0048d3c3b41cdcc16dbe6fad545030dbed9722c6},
	abstract = {In this paper, we introduce an application of matrix factorization to produce corpus-derived, distributional models of semantics that demonstrate cognitive plausibility. We find that word representations learned by Non-Negative Sparse Embedding (NNSE), a variant of matrix factorization, are sparse, effective, and highly interpretable. To the best of our knowledge, this is the first approach which yields semantic representation of words satisfying these three desirable properties. Though extensive experimental evaluations on multiple real-world tasks and datasets, we demonstrate the superiority of semantic models learned by NNSE over other state-of-the-art baselines.},
	urldate = {2024-12-02},
	author = {Murphy, B. and Talukdar, P. and Mitchell, Tom Michael},
	month = dec,
	year = {2012},
}

@misc{binder_layer-wise_2016-1,
	title = {Layer-wise {Relevance} {Propagation} for {Neural} {Networks} with {Local} {Renormalization} {Layers}},
	url = {http://arxiv.org/abs/1604.00825},
	doi = {10.48550/arXiv.1604.00825},
	abstract = {Layer-wise relevance propagation is a framework which allows to decompose the prediction of a deep neural network computed over a sample, e.g. an image, down to relevance scores for the single input dimensions of the sample such as subpixels of an image. While this approach can be applied directly to generalized linear mappings, product type non-linearities are not covered. This paper proposes an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks. We evaluate the proposed method for local renormalization layers on the CIFAR-10, Imagenet and MIT Places datasets.},
	urldate = {2024-12-01},
	publisher = {arXiv},
	author = {Binder, Alexander and Montavon, Grégoire and Bach, Sebastian and Müller, Klaus-Robert and Samek, Wojciech},
	month = apr,
	year = {2016},
	note = {arXiv:1604.00825},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{selvaraju_grad-cam_2019,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.48550/arXiv.1610.02391},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
	urldate = {2024-12-01},
	publisher = {arXiv},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = dec,
	year = {2019},
	note = {arXiv:1610.02391},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{noauthor_sparse_nodate,
	title = {Sparse {Reconstruction} of {Glucose} {Fluxes} {Using} {Continuous} {Glucose} {Monitors} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/8667648},
	urldate = {2024-12-01},
}

@misc{noauthor_chatgpt_nodate,
	title = {{ChatGPT}},
	url = {https://chatgpt.com},
	abstract = {A conversational AI system that listens, learns, and challenges},
	urldate = {2024-11-27},
}

@misc{peebles_scalable_2023,
	title = {Scalable {Diffusion} {Models} with {Transformers}},
	url = {http://arxiv.org/abs/2212.09748},
	doi = {10.48550/arXiv.2212.09748},
	abstract = {We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Peebles, William and Xie, Saining},
	month = mar,
	year = {2023},
	note = {arXiv:2212.09748},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv:1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noauthor_diffusionscope_nodate,
	title = {{DiffusionScope} {ICML} 2025},
	url = {https://www.overleaf.com/project/671ff29ce565e7e73a91b17a},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2024-11-25},
}

@misc{esser_scaling_2024,
	title = {Scaling {Rectified} {Flow} {Transformers} for {High}-{Resolution} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2403.03206},
	doi = {10.48550/arXiv.2403.03206},
	abstract = {Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and Müller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and Podell, Dustin and Dockhorn, Tim and English, Zion and Lacey, Kyle and Goodwin, Alex and Marek, Yannik and Rombach, Robin},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03206},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{chen_basis_1994,
	title = {Basis pursuit},
	volume = {1},
	url = {https://ieeexplore.ieee.org/document/471413},
	doi = {10.1109/ACSSC.1994.471413},
	abstract = {The time-frequency and time-scale communities have recently developed an enormous number of over-complete signal dictionaries, wavelets, wavelet packets, cosine packets, Wilson bases, chirplets, warped bases, and hyperbolic cross bases being a few examples. Basis pursuit is a technique for decomposing a signal into an "optimal" superposition of dictionary elements. The optimization criterion is the l/sup 1/ norm of coefficients. The method has several advantages over matching pursuit and best ortho basis, including super-resolution and stability.{\textless}{\textgreater}},
	urldate = {2024-10-30},
	booktitle = {Proceedings of 1994 28th {Asilomar} {Conference} on {Signals}, {Systems} and {Computers}},
	author = {Chen, Shaobing and Donoho, D.},
	month = oct,
	year = {1994},
	note = {ISSN: 1058-6393},
	keywords = {Chirp, Dictionaries, Explosions, Matching pursuit algorithms, Signal representations, Signal resolution, Stability, Statistics, Time frequency analysis, Wavelet packets},
	pages = {41--44 vol.1},
}

@misc{gandelsman_interpreting_2024,
	title = {Interpreting {CLIP}'s {Image} {Representation} via {Text}-{Based} {Decomposition}},
	url = {http://arxiv.org/abs/2310.05916},
	doi = {10.48550/arXiv.2310.05916},
	abstract = {We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Gandelsman, Yossi and Efros, Alexei A. and Steinhardt, Jacob},
	month = mar,
	year = {2024},
	note = {arXiv:2310.05916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}



@inproceedings{bengio_group_2009,
	title = {Group {Sparse} {Coding}},
	volume = {22},
	url = {https://papers.nips.cc/paper_files/paper/2009/hash/3b3dbaf68507998acd6a5a5254ab2d76-Abstract.html},
	abstract = {Bag-of-words document representations are often used in text, image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classification dataset show that when compact image or dictionary representations are needed for computational efficiency, the proposed approach yields better mean average precision in classification.},
	urldate = {2024-10-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bengio, Samy and Pereira, Fernando and Singer, Yoram and Strelow, Dennis},
	year = {2009},
}

@misc{bhalla_interpreting_2024,
	title = {Interpreting {CLIP} with {Sparse} {Linear} {Concept} {Embeddings} ({SpLiCE})},
	url = {https://arxiv.org/abs/2402.10376v1},
	abstract = {CLIP embeddings have demonstrated remarkable performance across a wide range of computer vision tasks. However, these high-dimensional, dense vector representations are not easily interpretable, restricting their usefulness in downstream applications that require transparency. In this work, we empirically show that CLIP's latent space is highly structured, and consequently that CLIP representations can be decomposed into their underlying semantic components. We leverage this understanding to propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE does not require concept labels and can be applied post hoc. Through extensive experimentation with multiple real-world datasets, we validate that the representations output by SpLiCE can explain and even replace traditional dense CLIP representations, maintaining equivalent downstream performance while significantly improving their interpretability. We also demonstrate several use cases of SpLiCE representations including detecting spurious correlations, model editing, and quantifying semantic shifts in datasets.},
	language = {en},
	urldate = {2024-10-08},
	journal = {arXiv.org},
	author = {Bhalla, Usha and Oesterling, Alex and Srinivas, Suraj and Calmon, Flavio P. and Lakkaraju, Himabindu},
	month = feb,
	year = {2024},
}

@misc{cunningham_sparse_2023,
	title = {Sparse {Autoencoders} {Find} {Highly} {Interpretable} {Features} in {Language} {Models}},
	url = {http://arxiv.org/abs/2309.08600},
	doi = {10.48550/arXiv.2309.08600},
	abstract = {One of the roadblocks to a better understanding of neural networks' internals is {\textbackslash}textit\{polysemanticity\}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is {\textbackslash}textit\{superposition\}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task {\textbackslash}citep\{wang2022interpretability\} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
	urldate = {2024-10-07},
	publisher = {arXiv},
	author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
	month = oct,
	year = {2023},
	note = {arXiv:2309.08600 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{olshausen_emergence_1996,
	title = {Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
	volume = {381},
	copyright = {1996 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/381607a0},
	doi = {10.1038/381607a0},
	abstract = {THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1–4 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7–12. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13–18, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
	language = {en},
	number = {6583},
	urldate = {2024-10-07},
	journal = {Nature},
	author = {Olshausen, Bruno A. and Field, David J.},
	month = jun,
	year = {1996},
	note = {Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary},
	pages = {607--609},
}

@misc{noauthor_notion_nodate,
	title = {Notion – {The} all-in-one workspace for your notes, tasks, wikis, and databases.},
	url = {https://www.notion.so},
	abstract = {A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team},
	language = {en},
	urldate = {2024-10-03},
	journal = {Notion},
}

@misc{zou_improving_2024,
	title = {Improving {Alignment} and {Robustness} with {Circuit} {Breakers}},
	url = {http://arxiv.org/abs/2406.04313},
	doi = {10.48550/arXiv.2406.04313},
	abstract = {AI systems can take harmful actions and are highly vulnerable to adversarial attacks. We present an approach, inspired by recent advances in representation engineering, that interrupts the models as they respond with harmful outputs with "circuit breakers." Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks. As an alternative to refusal training and adversarial training, circuit-breaking directly controls the representations that are responsible for harmful outputs in the first place. Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility -- even in the presence of powerful unseen attacks. Notably, while adversarial robustness in standalone image recognition remains an open challenge, circuit breakers allow the larger multimodal system to reliably withstand image "hijacks" that aim to produce harmful content. Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack. Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks.},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Zou, Andy and Phan, Long and Wang, Justin and Duenas, Derek and Lin, Maxwell and Andriushchenko, Maksym and Wang, Rowan and Kolter, Zico and Fredrikson, Matt and Hendrycks, Dan},
	month = jul,
	year = {2024},
	note = {arXiv:2406.04313 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{hong_margin-aware_2024,
	title = {Margin-aware {Preference} {Optimization} for {Aligning} {Diffusion} {Models} without {Reference}},
	url = {http://arxiv.org/abs/2406.06424},
	doi = {10.48550/arXiv.2406.06424},
	abstract = {Modern alignment techniques based on human preferences, such as RLHF and DPO, typically employ divergence regularization relative to the reference model to ensure training stability. However, this often limits the flexibility of models during alignment, especially when there is a clear distributional discrepancy between the preference data and the reference model. In this paper, we focus on the alignment of recent text-to-image diffusion models, such as Stable Diffusion XL (SDXL), and find that this "reference mismatch" is indeed a significant problem in aligning these models due to the unstructured nature of visual modalities: e.g., a preference for a particular stylistic aspect can easily induce such a discrepancy. Motivated by this observation, we propose a novel and memory-friendly preference alignment method for diffusion models that does not depend on any reference model, coined margin-aware preference optimization (MaPO). MaPO jointly maximizes the likelihood margin between the preferred and dispreferred image sets and the likelihood of the preferred sets, simultaneously learning general stylistic features and preferences. For evaluation, we introduce two new pairwise preference datasets, which comprise self-generated image pairs from SDXL, Pick-Style and Pick-Safety, simulating diverse scenarios of reference mismatch. Our experiments validate that MaPO can significantly improve alignment on Pick-Style and Pick-Safety and general preference alignment when used with Pick-a-Pic v2, surpassing the base SDXL and other existing methods. Our code, models, and datasets are publicly available via https://mapo-t2i.github.io},
	urldate = {2024-07-19},
	publisher = {arXiv},
	author = {Hong, Jiwoo and Paul, Sayak and Lee, Noah and Rasul, Kashif and Thorne, James and Jeong, Jongheon},
	month = jun,
	year = {2024},
	note = {arXiv:2406.06424 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hao_optimizing_2023,
	title = {Optimizing {Prompts} for {Text}-to-{Image} {Generation}},
	url = {http://arxiv.org/abs/2212.09611},
	doi = {10.48550/arXiv.2212.09611},
	abstract = {Well-designed prompts can guide text-to-image models to generate amazing images. However, the performant prompts are often model-specific and misaligned with user input. Instead of laborious human engineering, we propose prompt adaptation, a general framework that automatically adapts original user input to model-preferred prompts. Specifically, we first perform supervised fine-tuning with a pretrained language model on a small collection of manually engineered prompts. Then we use reinforcement learning to explore better prompts. We define a reward function that encourages the policy to generate more aesthetically pleasing images while preserving the original user intentions. Experimental results on Stable Diffusion show that our method outperforms manual prompt engineering in terms of both automatic metrics and human preference ratings. Moreover, reinforcement learning further boosts performance, especially on out-of-domain prompts. The pretrained checkpoints are available at https://aka.ms/promptist. The demo can be found at https://aka.ms/promptist-demo.},
	urldate = {2024-07-17},
	publisher = {arXiv},
	author = {Hao, Yaru and Chi, Zewen and Dong, Li and Wei, Furu},
	month = dec,
	year = {2023},
	note = {arXiv:2212.09611 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{rocamonde_vision-language_2023,
	title = {Vision-{Language} {Models} are {Zero}-{Shot} {Reward} {Models} for {Reinforcement} {Learning}},
	url = {https://openreview.net/forum?id=N0I2RtD8je},
	abstract = {Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide \_a single sentence text prompt\_ describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second "baseline" prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.},
	language = {en},
	urldate = {2024-07-11},
	author = {Rocamonde, Juan and Montesinos, Victoriano and Nava, Elvis and Perez, Ethan and Lindner, David},
	month = oct,
	year = {2023},
}

@misc{das_espresso_2024,
	title = {Espresso: {Robust} {Concept} {Filtering} in {Text}-to-{Image} {Models}},
	shorttitle = {Espresso},
	url = {http://arxiv.org/abs/2404.19227},
	doi = {10.48550/arXiv.2404.19227},
	abstract = {Diffusion-based text-to-image (T2I) models generate high-fidelity images for given textual prompts. They are trained on large datasets scraped from the Internet, potentially containing unacceptable concepts (e.g., copyright infringing or unsafe). Retraining T2I models after filtering out unacceptable concepts in the training data is inefficient and degrades utility. Hence, there is a need for concept removal techniques (CRTs) which are effective in removing unacceptable concepts, utility-preserving on acceptable concepts, and robust against evasion with adversarial prompts. None of the prior filtering and fine-tuning CRTs satisfy all these requirements simultaneously. We introduce Espresso, the first robust concept filter based on Contrastive Language-Image Pre-Training (CLIP). It identifies unacceptable concepts by projecting the generated image's embedding onto the vector connecting unacceptable and acceptable concepts in the joint text-image embedding space. This ensures robustness by restricting the adversary to adding noise only along this vector, in the direction of the acceptable concept. Further fine-tuning Espresso to separate embeddings of acceptable and unacceptable concepts, while preserving their pairing with image embeddings, ensures both effectiveness and utility. We evaluate Espresso on eleven concepts to show that it is effective ({\textasciitilde}5\% CLIP accuracy on unacceptable concepts), utility-preserving ({\textasciitilde}93\% normalized CLIP score on acceptable concepts), and robust ({\textasciitilde}4\% CLIP accuracy on adversarial prompts for unacceptable concepts). Finally, we present theoretical bounds for the certified robustness of Espresso against adversarial prompts, and an empirical analysis.},
	urldate = {2024-06-25},
	publisher = {arXiv},
	author = {Das, Anudeep and Duddu, Vasisht and Zhang, Rui and Asokan, N.},
	month = jun,
	year = {2024},
	note = {arXiv:2404.19227 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@misc{dunlap_describing_2023,
	title = {Describing {Differences} in {Image} {Sets} with {Natural} {Language}},
	url = {https://arxiv.org/abs/2312.02974v2},
	abstract = {How do two sets of images differ? Discerning set-level differences is crucial for understanding model behaviors and analyzing datasets, yet manually sifting through thousands of images is impractical. To aid in this discovery process, we explore the task of automatically describing the differences between two \${\textbackslash}textbf\{sets\}\$ of images, which we term Set Difference Captioning. This task takes in image sets \$D\_A\$ and \$D\_B\$, and outputs a description that is more often true on \$D\_A\$ than \$D\_B\$. We outline a two-stage approach that first proposes candidate difference descriptions from image sets and then re-ranks the candidates by checking how well they can differentiate the two sets. We introduce VisDiff, which first captions the images and prompts a language model to propose candidate descriptions, then re-ranks these descriptions using CLIP. To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image sets with ground truth difference descriptions. We apply VisDiff to various domains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing classification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing model failure modes (supervised ResNet), characterizing differences between generative models (e.g., StableDiffusionV1 and V2), and discovering what makes images memorable. Using VisDiff, we are able to find interesting and previously unknown differences in datasets and models, demonstrating its utility in revealing nuanced insights.},
	language = {en},
	urldate = {2024-06-07},
	journal = {arXiv.org},
	author = {Dunlap, Lisa and Zhang, Yuhui and Wang, Xiaohan and Zhong, Ruiqi and Darrell, Trevor and Steinhardt, Jacob and Gonzalez, Joseph E. and Yeung-Levy, Serena},
	month = dec,
	year = {2023},
}

@misc{yang_diffusion_2024,
	title = {Diffusion {Model} with {Cross} {Attention} as an {Inductive} {Bias} for {Disentanglement}},
	url = {https://arxiv.org/abs/2402.09712v1},
	abstract = {Disentangled representation learning strives to extract the intrinsic factors within observed data. Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion. Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive ablation studies and visualization analysis, shedding light on the functioning of this model. This is the first work to reveal the potent disentanglement capability of diffusion models with cross-attention, requiring no complex designs. We anticipate that our findings will inspire more investigation on exploring diffusion for disentangled representation learning towards more sophisticated data analysis and understanding.},
	language = {en},
	urldate = {2024-06-07},
	journal = {arXiv.org},
	author = {Yang, Tao and Lan, Cuiling and Lu, Yan and Zheng, Nanning},
	month = feb,
	year = {2024},
}

@misc{noauthor_runtimeerror_2021,
	title = {{RuntimeError}: {Trying} to create tensor with negative dimension -256},
	shorttitle = {{RuntimeError}},
	url = {https://discuss.pytorch.org/t/runtimeerror-trying-to-create-tensor-with-negative-dimension-256/109007},
	abstract = {Hi.  in torch 1.7.0 , I can’t debug this error.  class DQN(nn.Module):          def \_\_init\_\_(self, num\_states, num\_actions):         super(DQN, self).\_\_init\_\_()         self.conv1 = nn.Conv2d(3,8,kernel\_size=3,stride=2)         self.bn1 = nn.BatchNorm2d(8)         self.conv2 = nn.Conv2d(8, 16, kernel\_size=3, stride=2)         self.bn2 = nn.BatchNorm2d(16)         self.conv3 = nn.Conv2d(16, 32, kernel\_size=3, stride=2)         self.bn3 = nn.BatchNorm2d(32)          def conv2d\_size\_out(size, kerne...},
	language = {en},
	urldate = {2024-06-04},
	journal = {PyTorch Forums},
	month = jan,
	year = {2021},
}

@misc{noauthor_quantifying_nodate,
	title = {Quantifying {Bias} in {Text}-to-{Image} {Generative} {Models}},
	url = {https://arxiv.org/html/2312.13053v1},
	urldate = {2024-05-31},
}

@misc{wang_t2iat_2023,
	title = {{T2IAT}: {Measuring} {Valence} and {Stereotypical} {Biases} in {Text}-to-{Image} {Generation}},
	shorttitle = {{T2IAT}},
	url = {http://arxiv.org/abs/2306.00905},
	doi = {10.48550/arXiv.2306.00905},
	abstract = {Warning: This paper contains several contents that may be toxic, harmful, or offensive. In the last few years, text-to-image generative models have gained remarkable success in generating images with unprecedented quality accompanied by a breakthrough of inference speed. Despite their rapid progress, human biases that manifest in the training examples, particularly with regard to common stereotypical biases, like gender and skin tone, still have been found in these generative models. In this work, we seek to measure more complex human biases exist in the task of text-to-image generations. Inspired by the well-known Implicit Association Test (IAT) from social psychology, we propose a novel Text-to-Image Association Test (T2IAT) framework that quantifies the implicit stereotypes between concepts and valence, and those in the images. We replicate the previously documented bias tests on generative models, including morally neutral tests on flowers and insects as well as demographic stereotypical tests on diverse social attributes. The results of these experiments demonstrate the presence of complex stereotypical behaviors in image generations.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Wang, Jialu and Liu, Xinyue Gabby and Di, Zonglin and Liu, Yang and Wang, Xin Eric},
	month = jun,
	year = {2023},
	note = {arXiv:2306.00905 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, I.2.6},
}



@misc{wang_concept_2024,
	title = {Concept {Algebra} for ({Score}-{Based}) {Text}-{Controlled} {Generative} {Models}},
	url = {http://arxiv.org/abs/2302.03693},
	doi = {10.48550/arXiv.2302.03693},
	abstract = {This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. A key property of such models is that they can compose disparate concepts in a `disentangled' manner. This suggests these models have internal representations that encode concepts in a `disentangled' manner. Here, we focus on the idea that concepts are encoded as subspaces of some representation space. We formalize what this means, show there's a natural choice for the representation, and develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples using Stable Diffusion. Code in https://github.com/zihao12/concept-algebra-code},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Wang, Zihao and Gui, Lin and Negrea, Jeffrey and Veitch, Victor},
	month = feb,
	year = {2024},
	note = {arXiv:2302.03693 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kim_-stereotyping_2023,
	title = {De-stereotyping {Text}-to-image {Models} through {Prompt} {Tuning}},
	url = {https://openreview.net/forum?id=yNyywJln2R#all},
	abstract = {Recent text-to-image (TTI) generation models have been reported to generate images demographically stereotyped in various sensitive attributes such as gender or race. This may seriously harm the fairness of the generative model to be deployed. We propose a novel and efficient framework to de-stereotype the existing TTI model through soft prompt tuning. Utilizing a newly designed de-stereotyping loss, we train a small number of parameters consisting of the soft prompt. We demonstrate that our framework effectively balances the generated images with respect to sensitive attributes, which can also generalize to unseen text prompts.},
	language = {en},
	urldate = {2024-05-31},
	author = {Kim, Eunji and Kim, Siwon and Shin, Chaehun and Yoon, Sungroh},
	month = jun,
	year = {2023},
}

@misc{yesiltepe_mist_2024,
	title = {{MIST}: {Mitigating} {Intersectional} {Bias} with {Disentangled} {Cross}-{Attention} {Editing} in {Text}-to-{Image} {Diffusion} {Models}},
	shorttitle = {{MIST}},
	url = {http://arxiv.org/abs/2403.19738},
	doi = {10.48550/arXiv.2403.19738},
	abstract = {Diffusion-based text-to-image models have rapidly gained popularity for their ability to generate detailed and realistic images from textual descriptions. However, these models often reflect the biases present in their training data, especially impacting marginalized groups. While prior efforts to debias language models have focused on addressing specific biases, such as racial or gender biases, efforts to tackle intersectional bias have been limited. Intersectional bias refers to the unique form of bias experienced by individuals at the intersection of multiple social identities. Addressing intersectional bias is crucial because it amplifies the negative effects of discrimination based on race, gender, and other identities. In this paper, we introduce a method that addresses intersectional bias in diffusion-based text-to-image models by modifying cross-attention maps in a disentangled manner. Our approach utilizes a pre-trained Stable Diffusion model, eliminates the need for an additional set of reference images, and preserves the original quality for unaltered concepts. Comprehensive experiments demonstrate that our method surpasses existing approaches in mitigating both single and intersectional biases across various attributes. We make our source code and debiased models for various attributes available to encourage fairness in generative models and to support further research.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Yesiltepe, Hidir and Akdemir, Kiymet and Yanardag, Pinar},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19738 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{struppek_exploiting_2023,
	title = {Exploiting {Cultural} {Biases} via {Homoglyphs} in {Text}-to-{Image} {Synthesis}},
	volume = {78},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/2209.08891},
	doi = {10.1613/jair.1.15388},
	abstract = {Models for text-to-image synthesis, such as DALL-E{\textasciitilde}2 and Stable Diffusion, have recently drawn a lot of interest from academia and the general public. These models are capable of producing high-quality images that depict a variety of concepts and styles when conditioned on textual descriptions. However, these models adopt cultural characteristics associated with specific Unicode scripts from their vast amount of training data, which may not be immediately apparent. We show that by simply inserting single non-Latin characters in a textual description, common models reflect cultural stereotypes and biases in their generated images. We analyze this behavior both qualitatively and quantitatively, and identify a model's text encoder as the root cause of the phenomenon. Additionally, malicious users or service providers may try to intentionally bias the image generation to create racist stereotypes by replacing Latin characters with similarly-looking characters from non-Latin scripts, so-called homoglyphs. To mitigate such unnoticed script attacks, we propose a novel homoglyph unlearning method to fine-tune a text encoder, making it robust against homoglyph manipulations.},
	urldate = {2024-05-31},
	journal = {Journal of Artificial Intelligence Research},
	author = {Struppek, Lukas and Hintersdorf, Dominik and Friedrich, Felix and Brack, Manuel and Schramowski, Patrick and Kersting, Kristian},
	month = dec,
	year = {2023},
	note = {arXiv:2209.08891 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
	pages = {1017--1068},
}

@misc{choi_fair_2020,
	title = {Fair {Generative} {Modeling} via {Weak} {Supervision}},
	url = {http://arxiv.org/abs/1910.12008},
	doi = {10.48550/arXiv.1910.12008},
	abstract = {Real-world datasets are often biased with respect to key demographic factors such as race and gender. Due to the latent nature of the underlying factors, detecting and mitigating bias is especially challenging for unsupervised machine learning. We present a weakly supervised algorithm for overcoming dataset bias for deep generative models. Our approach requires access to an additional small, unlabeled reference dataset as the supervision signal, thus sidestepping the need for explicit labels on the underlying bias factors. Using this supplementary dataset, we detect the bias in existing datasets via a density ratio technique and learn generative models which efficiently achieve the twin goals of: 1) data efficiency by using training examples from both biased and reference datasets for learning; and 2) data generation close in distribution to the reference dataset at test time. Empirically, we demonstrate the efficacy of our approach which reduces bias w.r.t. latent factors by an average of up to 34.6\% over baselines for comparable image generation using generative adversarial networks.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Choi, Kristy and Grover, Aditya and Singh, Trisha and Shu, Rui and Ermon, Stefano},
	month = jun,
	year = {2020},
	note = {arXiv:1910.12008 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{esposito_mitigating_2023,
	title = {Mitigating stereotypical biases in text to image generative systems},
	url = {http://arxiv.org/abs/2310.06904},
	doi = {10.48550/arXiv.2310.06904},
	abstract = {State-of-the-art generative text-to-image models are known to exhibit social biases and over-represent certain groups like people of perceived lighter skin tones and men in their outcomes. In this work, we propose a method to mitigate such biases and ensure that the outcomes are fair across different groups of people. We do this by finetuning text-to-image models on synthetic data that varies in perceived skin tones and genders constructed from diverse text prompts. These text prompts are constructed from multiplicative combinations of ethnicities, genders, professions, age groups, and so on, resulting in diverse synthetic data. Our diversity finetuned (DFT) model improves the group fairness metric by 150\% for perceived skin tone and 97.7\% for perceived gender. Compared to baselines, DFT models generate more people with perceived darker skin tone and more women. To foster open research, we will release all text prompts and code to generate training images.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Esposito, Piero and Atighehchian, Parmida and Germanidis, Anastasis and Ghadiyaram, Deepti},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06904 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{friedrich_fair_2023,
	title = {Fair {Diffusion}: {Instructing} {Text}-to-{Image} {Generation} {Models} on {Fairness}},
	shorttitle = {Fair {Diffusion}},
	url = {http://arxiv.org/abs/2302.10893},
	doi = {10.48550/arXiv.2302.10893},
	abstract = {Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Friedrich, Felix and Brack, Manuel and Struppek, Lukas and Hintersdorf, Dominik and Schramowski, Patrick and Luccioni, Sasha and Kersting, Kristian},
	month = jul,
	year = {2023},
	note = {arXiv:2302.10893 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@misc{orgad_editing_2023,
	title = {Editing {Implicit} {Assumptions} in {Text}-to-{Image} {Diffusion} {Models}},
	url = {https://arxiv.org/abs/2303.08084v2},
	abstract = {Text-to-image diffusion models often make implicit assumptions about the world when generating images. While some assumptions are useful (e.g., the sky is blue), they can also be outdated, incorrect, or reflective of social biases present in the training data. Thus, there is a need to control these assumptions without requiring explicit user input or costly re-training. In this work, we aim to edit a given implicit assumption in a pre-trained diffusion model. Our Text-to-Image Model Editing method, TIME for short, receives a pair of inputs: a "source" under-specified prompt for which the model makes an implicit assumption (e.g., "a pack of roses"), and a "destination" prompt that describes the same setting, but with a specified desired attribute (e.g., "a pack of blue roses"). TIME then updates the model's cross-attention layers, as these layers assign visual meaning to textual tokens. We edit the projection matrices in these layers such that the source prompt is projected close to the destination prompt. Our method is highly efficient, as it modifies a mere 2.2\% of the model's parameters in under one second. To evaluate model editing approaches, we introduce TIMED (TIME Dataset), containing 147 source and destination prompt pairs from various domains. Our experiments (using Stable Diffusion) show that TIME is successful in model editing, generalizes well for related prompts unseen during editing, and imposes minimal effect on unrelated generations.},
	language = {en},
	urldate = {2024-05-31},
	journal = {arXiv.org},
	author = {Orgad, Hadas and Kawar, Bahjat and Belinkov, Yonatan},
	month = mar,
	year = {2023},
}

@misc{kim_training_2024,
	title = {Training {Unbiased} {Diffusion} {Models} {From} {Biased} {Dataset}},
	url = {http://arxiv.org/abs/2403.01189},
	doi = {10.48550/arXiv.2403.01189},
	abstract = {With significant advancements in diffusion models, addressing the potential risks of dataset bias becomes increasingly important. Since generated outputs directly suffer from dataset bias, mitigating latent bias becomes a key factor in improving sample quality and proportion. This paper proposes time-dependent importance reweighting to mitigate the bias for the diffusion models. We demonstrate that the time-dependent density ratio becomes more precise than previous approaches, thereby minimizing error propagation in generative learning. While directly applying it to score-matching is intractable, we discover that using the time-dependent density ratio both for reweighting and score correction can lead to a tractable form of the objective function to regenerate the unbiased data density. Furthermore, we theoretically establish a connection with traditional score-matching, and we demonstrate its convergence to an unbiased distribution. The experimental evidence supports the usefulness of the proposed method, which outperforms baselines including time-independent importance reweighting on CIFAR-10, CIFAR-100, FFHQ, and CelebA with various bias settings. Our code is available at https://github.com/alsdudrla10/TIW-DSM.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Kim, Yeongmin and Na, Byeonghu and Park, Minsang and Jang, JoonHo and Kim, Dongjun and Kang, Wanmo and Moon, Il-Chul},
	month = mar,
	year = {2024},
	note = {arXiv:2403.01189 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{yin_semantics_2019,
	title = {Semantics {Disentangling} for {Text}-to-{Image} {Generation}},
	url = {http://arxiv.org/abs/1904.01480},
	doi = {10.48550/arXiv.1904.01480},
	abstract = {Synthesizing photo-realistic images from text descriptions is a challenging problem. Previous studies have shown remarkable progresses on visual quality of the generated images. In this paper, we consider semantics from the input text descriptions in helping render photo-realistic images. However, diverse linguistic expressions pose challenges in extracting consistent semantics even they depict the same thing. To this end, we propose a novel photo-realistic text-to-image generation model that implicitly disentangles semantics to both fulfill the high-level semantic consistency and low-level semantic diversity. To be specific, we design (1) a Siamese mechanism in the discriminator to learn consistent high-level semantics, and (2) a visual-semantic embedding strategy by semantic-conditioned batch normalization to find diverse low-level semantics. Extensive experiments and ablation studies on CUB and MS-COCO datasets demonstrate the superiority of the proposed method in comparison to state-of-the-art methods.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Yin, Guojun and Liu, Bin and Sheng, Lu and Yu, Nenghai and Wang, Xiaogang and Shao, Jing},
	month = apr,
	year = {2019},
	note = {arXiv:1904.01480 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{seshadri_bias_2023,
	title = {The {Bias} {Amplification} {Paradox} in {Text}-to-{Image} {Generation}},
	url = {http://arxiv.org/abs/2308.00755},
	doi = {10.48550/arXiv.2308.00755},
	abstract = {Bias amplification is a phenomenon in which models exacerbate biases or stereotypes present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION) considerably. However, we discover that amplification can be largely attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while our prompts do not, which leads to a distribution shift and consequently inflates bias measures. Once we account for distributional differences between texts used for training and generation when evaluating amplification, we observe that amplification decreases drastically. Our findings illustrate the challenges of comparing biases in models and their training data, and highlight confounding factors that impact analyses.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Seshadri, Preethi and Singh, Sameer and Elazar, Yanai},
	month = nov,
	year = {2023},
	note = {arXiv:2308.00755 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{gallegos_self-debiasing_2024,
	title = {Self-{Debiasing} {Large} {Language} {Models}: {Zero}-{Shot} {Recognition} and {Reduction} of {Stereotypes}},
	shorttitle = {Self-{Debiasing} {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2402.01981v1},
	abstract = {Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation.},
	language = {en},
	urldate = {2024-05-20},
	journal = {arXiv.org},
	author = {Gallegos, Isabel O. and Rossi, Ryan A. and Barrow, Joe and Tanjim, Md Mehrab and Yu, Tong and Deilamsalehy, Hanieh and Zhang, Ruiyi and Kim, Sungchul and Dernoncourt, Franck},
	month = feb,
	year = {2024},
}

@inproceedings{li_discover_2021,
	address = {Montreal, QC, Canada},
	title = {Discover the {Unknown} {Biased} {Attribute} of an {Image} {Classifier}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710874/},
	doi = {10.1109/ICCV48922.2021.01470},
	language = {en},
	urldate = {2024-05-17},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Li, Zhiheng and Xu, Chenliang},
	month = oct,
	year = {2021},
	pages = {14950--14959},
}

@misc{chuang_debiasing_2023,
	title = {Debiasing {Vision}-{Language} {Models} via {Biased} {Prompts}},
	url = {http://arxiv.org/abs/2302.00070},
	abstract = {Machine learning models have been shown to inherit biases from their training datasets. This can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. The biases can be ampliﬁed and propagated to downstream applications like zero-shot classiﬁers and text-to-image generative models. In this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. In particular, we show that debiasing only the text embedding with a calibrated projection matrix sufﬁces to yield robust classiﬁers and fair generative models. The proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training. The code is available at https://github.com/chingyaoc/debias\_vl.},
	language = {en},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Chuang, Ching-Yao and Jampani, Varun and Li, Yuanzhen and Torralba, Antonio and Jegelka, Stefanie},
	month = may,
	year = {2023},
	note = {arXiv:2302.00070 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{dinca_openbias_2024,
	title = {{OpenBias}: {Open}-set {Bias} {Detection} in {Text}-to-{Image} {Generative} {Models}},
	shorttitle = {{OpenBias}},
	url = {http://arxiv.org/abs/2404.07990},
	doi = {10.48550/arXiv.2404.07990},
	abstract = {Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision Question Answering model recognizes the presence and extent of the previously proposed biases. We study the behavior of Stable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated before. Via quantitative experiments, we demonstrate that OpenBias agrees with current closed-set bias detection methods and human judgement.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {D'Incà, Moreno and Peruzzo, Elia and Mancini, Massimiliano and Xu, Dejia and Goel, Vidit and Xu, Xingqian and Wang, Zhangyang and Shi, Humphrey and Sebe, Nicu},
	month = apr,
	year = {2024},
	note = {arXiv:2404.07990 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{dinca_openbias_2024-1,
	title = {{OpenBias}: {Open}-set {Bias} {Detection} in {Text}-to-{Image} {Generative} {Models}},
	shorttitle = {{OpenBias}},
	url = {http://arxiv.org/abs/2404.07990},
	abstract = {Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision Question Answering model recognizes the presence and extent of the previously proposed biases. We study the behavior of Stable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated before. Via quantitative experiments, we demonstrate that OpenBias agrees with current closed-set bias detection methods and human judgement.},
	language = {en},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {D'Incà, Moreno and Peruzzo, Elia and Mancini, Massimiliano and Xu, Dejia and Goel, Vidit and Xu, Xingqian and Wang, Zhangyang and Shi, Humphrey and Sebe, Nicu},
	month = apr,
	year = {2024},
	note = {arXiv:2404.07990 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{shen_finetuning_2023,
	title = {Finetuning {Text}-to-{Image} {Diffusion} {Models} for {Fairness}},
	url = {https://openreview.net/forum?id=hnrB5YHoYu},
	abstract = {The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model's sampling process (adjusted DFT), which leverages an adjusted gradient to directly optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives of fairness beyond absolute equality, which is demonstrated by controlling age to a 75\% young and 25\% old distribution while simultaneously debiasing gender and race. Finally, our method is scalable: it can debias multiple concepts at once by simply including these prompts in the finetuning data. We share code and various fair diffusion model adaptors at https://sail-sg.github.io/finetune-fair-diffusion/.},
	language = {en},
	urldate = {2024-05-16},
	author = {Shen, Xudong and Du, Chao and Pang, Tianyu and Lin, Min and Wong, Yongkang and Kankanhalli, Mohan},
	month = oct,
	year = {2023},
}

@misc{namikoshi_using_2024,
	title = {Using {LLMs} to {Model} the {Beliefs} and {Preferences} of {Targeted} {Populations}},
	url = {http://arxiv.org/abs/2403.20252},
	abstract = {We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role of temperature in controlling the trade-offs between these two. Additionally, we propose and evaluate a novel loss term to improve model performance on responses that require a numeric response.},
	language = {en},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Namikoshi, Keiichi and Filipowicz, Alex and Shamma, David A. and Iliev, Rumen and Hogan, Candice L. and Arechiga, Nikos},
	month = mar,
	year = {2024},
	note = {arXiv:2403.20252 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{ding_cogview_2021,
	title = {{CogView}: {Mastering} {Text}-to-{Image} {Generation} via {Transformers}},
	shorttitle = {{CogView}},
	url = {http://arxiv.org/abs/2105.13290},
	doi = {10.48550/arXiv.2105.13290},
	abstract = {Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and Tang, Jie},
	month = nov,
	year = {2021},
	note = {arXiv:2105.13290 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{bansal_how_2022,
	title = {How well can {Text}-to-{Image} {Generative} {Models} understand {Ethical} {Natural} {Language} {Interventions}?},
	url = {http://arxiv.org/abs/2210.15230},
	abstract = {Text-to-image generative models have achieved unprecedented success in generating high-quality images based on natural language descriptions. However, it is shown that these models tend to favor speciﬁc social groups when prompted with neutral text descriptions (e.g., ‘a photo of a lawyer’). Following Zhao et al. (2021), we study the effect on the diversity of the generated images when adding ethical intervention that supports equitable judgment (e.g., ‘if all individuals can be a lawyer irrespective of their gender’) in the input prompts. To this end, we introduce an Ethical NaTural Language Interventions in Text-to-Image GENeration (ENTIGEN) benchmark dataset to evaluate the change in image generations conditional on ethical interventions across three social axes – gender, skin color, and culture. Through ENTIGEN framework, we ﬁnd that the generations from minDALL·E, DALL·E-mini and Stable Diffusion cover diverse social groups while preserving the image quality. Preliminary studies indicate that a large change in the model predictions is triggered by certain phrases such as ‘irrespective of gender’ in the context of gender bias in the ethical interventions. We release code and annotated data at https://github.com/Hritikbansal/ entigen\_emnlp.},
	language = {en},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Bansal, Hritik and Yin, Da and Monajatipoor, Masoud and Chang, Kai-Wei},
	month = oct,
	year = {2022},
	note = {arXiv:2210.15230 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multimedia},
}

@misc{ramaswamy_fair_2021,
	title = {Fair {Attribute} {Classification} through {Latent} {Space} {De}-biasing},
	url = {http://arxiv.org/abs/2012.01469},
	doi = {10.48550/arXiv.2012.01469},
	abstract = {Fairness in visual recognition is becoming a prominent and critical topic of discussion as recognition systems are deployed at scale in the real world. Models trained from data in which target labels are correlated with protected attributes (e.g., gender, race) are known to learn and exploit those correlations. In this work, we introduce a method for training accurate target classifiers while mitigating biases that stem from these correlations. We use GANs to generate realistic-looking images, and perturb these images in the underlying latent space to generate training data that is balanced for each protected attribute. We augment the original dataset with this perturbed generated data, and empirically demonstrate that target classifiers trained on the augmented dataset exhibit a number of both quantitative and qualitative benefits. We conduct a thorough evaluation across multiple target labels and protected attributes in the CelebA dataset, and provide an in-depth analysis and comparison to existing literature in the space.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Ramaswamy, Vikram V. and Kim, Sunnie S. Y. and Russakovsky, Olga},
	month = apr,
	year = {2021},
	note = {arXiv:2012.01469 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_iti-gen_2023,
	title = {{ITI}-{GEN}: {Inclusive} {Text}-to-{Image} {Generation}},
	shorttitle = {{ITI}-{GEN}},
	url = {http://arxiv.org/abs/2309.05569},
	abstract = {Text-to-image generative models often reflect the biases of the training data, leading to unequal representations of underrepresented groups. This study investigates inclusive text-to-image generative models that generate images based on human-written prompts and ensure the resulting images are uniformly distributed across attributes of interest. Unfortunately, directly expressing the desired attributes in the prompt often leads to sub-optimal results due to linguistic ambiguity or model misrepresentation. Hence, this paper proposes a drastically different approach that adheres to the maxim that “a picture is worth a thousand words”. We show that, for some attributes, images can represent concepts more expressively than text. For instance, categories of skin tones are typically hard to specify by text but can be easily represented by example images. Building upon these insights, we propose a novel approach, ITI-GEN1, that leverages readily available reference images for Inclusive Textto-Image GENeration. The key idea is learning a set of prompt embeddings to generate images that can effectively represent all desired attribute categories. More importantly, ITI-GEN requires no model fine-tuning, making it computationally efficient to augment existing text-to-image models. Extensive experiments demonstrate that ITI-GEN largely improves over state-of-the-art models to generate inclusive images from a prompt.},
	language = {en},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Zhang, Cheng and Chen, Xuanbai and Chai, Siqi and Wu, Chen Henry and Lagun, Dmitry and Beeler, Thabo and De la Torre, Fernando},
	month = sep,
	year = {2023},
	note = {arXiv:2309.05569 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zhang_iti-gen_2023,
	title = {{ITI}-{GEN}: {Inclusive} {Text}-to-{Image} {Generation}},
	shorttitle = {{ITI}-{GEN}},
	url = {http://arxiv.org/abs/2309.05569},
	abstract = {Text-to-image generative models often reflect the biases of the training data, leading to unequal representations of underrepresented groups. This study investigates inclusive text-to-image generative models that generate images based on human-written prompts and ensure the resulting images are uniformly distributed across attributes of interest. Unfortunately, directly expressing the desired attributes in the prompt often leads to sub-optimal results due to linguistic ambiguity or model misrepresentation. Hence, this paper proposes a drastically different approach that adheres to the maxim that “a picture is worth a thousand words”. We show that, for some attributes, images can represent concepts more expressively than text. For instance, categories of skin tones are typically hard to specify by text but can be easily represented by example images. Building upon these insights, we propose a novel approach, ITI-GEN1, that leverages readily available reference images for Inclusive Textto-Image GENeration. The key idea is learning a set of prompt embeddings to generate images that can effectively represent all desired attribute categories. More importantly, ITI-GEN requires no model fine-tuning, making it computationally efficient to augment existing text-to-image models. Extensive experiments demonstrate that ITI-GEN largely improves over state-of-the-art models to generate inclusive images from a prompt.},
	language = {en},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Zhang, Cheng and Chen, Xuanbai and Chai, Siqi and Wu, Chen Henry and Lagun, Dmitry and Beeler, Thabo and De la Torre, Fernando},
	month = sep,
	year = {2023},
	note = {arXiv:2309.05569 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{he_debiasing_2024,
	title = {Debiasing {Text}-to-{Image} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2402.14577},
	doi = {10.48550/arXiv.2402.14577},
	abstract = {Learning-based Text-to-Image (TTI) models like Stable Diffusion have revolutionized the way visual content is generated in various domains. However, recent research has shown that nonnegligible social bias exists in current state-of-the-art TTI systems, which raises important concerns. In this work, we target resolving the social bias in TTI diffusion models. We begin by formalizing the problem setting and use the text descriptions of bias groups to establish an unsafe direction for guiding the diffusion process. Next, we simplify the problem into a weight optimization problem and attempt a Reinforcement solver, Policy Gradient, which shows sub-optimal performance with slow convergence. Further, to overcome limitations, we propose an iterative distribution alignment (IDA) method. Despite its simplicity, we show that IDA shows efficiency and fast convergence in resolving the social bias in TTI diffusion models. Our code will be released.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {He, Ruifei and Xue, Chuhui and Tan, Haoru and Zhang, Wenqing and Yu, Yingchen and Bai, Song and Qi, Xiaojuan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14577 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kirstain_pick--pic_2023,
	title = {Pick-a-{Pic}: {An} {Open} {Dataset} of {User} {Preferences} for {Text}-to-{Image} {Generation}},
	shorttitle = {Pick-a-{Pic}},
	url = {http://arxiv.org/abs/2305.01569},
	doi = {10.48550/arXiv.2305.01569},
	abstract = {The ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public. To address this issue, we create a web app that enables text-to-image users to generate images and specify their preferences. Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image prompts and real users' preferences over generated images. We leverage this dataset to train a CLIP-based scoring function, PickScore, which exhibits superhuman performance on the task of predicting human preferences. Then, we test PickScore's ability to perform model evaluation and observe that it correlates better with human rankings than other automatic evaluation metrics. Therefore, we recommend using PickScore for evaluating future text-to-image generation models, and using Pick-a-Pic prompts as a more relevant dataset than MS-COCO. Finally, we demonstrate how PickScore can enhance existing text-to-image models via ranking.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Kirstain, Yuval and Polyak, Adam and Singer, Uriel and Matiana, Shahbuland and Penna, Joe and Levy, Omer},
	month = nov,
	year = {2023},
	note = {arXiv:2305.01569 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{parihar_balancing_2024,
	title = {Balancing {Act}: {Distribution}-{Guided} {Debiasing} in {Diffusion} {Models}},
	shorttitle = {Balancing {Act}},
	url = {http://arxiv.org/abs/2402.18206},
	abstract = {Diffusion Models (DMs) have emerged as powerful generative models with unprecedented image generation capability. These models are widely used for data augmentation and creative applications. However, DMs reflect the biases present in the training datasets. This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male). In this work, we present a method for debiasing DMs without relying on additional data or model retraining. Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution. To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation. We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes. ADP is trained with pseudo labels generated from existing attribute classifiers. The proposed Distribution Guidance with ADP enables us to do fair generation. Our method reduces bias across single/multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional diffusion models. Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data.},
	language = {en},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Parihar, Rishubh and Bhat, Abhijnya and Mallick, Saswat and Basu, Abhipsa and Kundu, Jogendra Nath and Babu, R. Venkatesh},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18206 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}


@article{chefer_attend-and-excite_2023,
	title = {Attend-and-{Excite}: {Attention}-{Based} {Semantic} {Guidance} for {Text}-to-{Image} {Diffusion} {Models}},
	volume = {42},
	issn = {0730-0301},
	shorttitle = {Attend-and-{Excite}},
	url = {https://dl.acm.org/doi/10.1145/3592116},
	doi = {10.1145/3592116},
	abstract = {Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g., colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen --- or excite --- their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts. Code is available at our project page: https://attendandexcite.github.io/Attend-and-Excite/.},
	number = {4},
	urldate = {2024-04-29},
	journal = {ACM Transactions on Graphics},
	author = {Chefer, Hila and Alaluf, Yuval and Vinker, Yael and Wolf, Lior and Cohen-Or, Daniel},
	month = jul,
	year = {2023},
	keywords = {diffusion models, image generation},
	pages = {148:1--148:10},
}

@misc{ho_classifier-free_2022,
	title = {Classifier-{Free} {Diffusion} {Guidance}},
	url = {http://arxiv.org/abs/2207.12598},
	doi = {10.48550/arXiv.2207.12598},
	abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Ho, Jonathan and Salimans, Tim},
	month = jul,
	year = {2022},
	note = {arXiv:2207.12598 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{li_gligen_2023,
	title = {{GLIGEN}: {Open}-{Set} {Grounded} {Text}-to-{Image} {Generation}},
	shorttitle = {{GLIGEN}},
	url = {http://arxiv.org/abs/2301.07093},
	doi = {10.48550/arXiv.2301.07093},
	abstract = {Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms that of existing supervised layout-to-image baselines by a large margin.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Li, Yuheng and Liu, Haotian and Wu, Qingyang and Mu, Fangzhou and Yang, Jianwei and Gao, Jianfeng and Li, Chunyuan and Lee, Yong Jae},
	month = apr,
	year = {2023},
	note = {arXiv:2301.07093 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{cao_controllable_2024,
	title = {Controllable {Generation} with {Text}-to-{Image} {Diffusion} {Models}: {A} {Survey}},
	shorttitle = {Controllable {Generation} with {Text}-to-{Image} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2403.04279},
	doi = {10.48550/arXiv.2403.04279},
	abstract = {In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. We then reveal the controlling mechanisms of diffusion models, theoretically analyzing how novel conditions are introduced into the denoising process for conditional generation. Additionally, we offer a detailed overview of research in this area, organizing it into distinct categories from the condition perspective: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at {\textbackslash}url\{https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models\}.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Cao, Pu and Zhou, Feng and Song, Qing and Yang, Lu},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04279 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cao_controllable_2024-1,
	title = {Controllable {Generation} with {Text}-to-{Image} {Diffusion} {Models}: {A} {Survey}},
	shorttitle = {Controllable {Generation} with {Text}-to-{Image} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2403.04279},
	doi = {10.48550/arXiv.2403.04279},
	abstract = {In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. We then reveal the controlling mechanisms of diffusion models, theoretically analyzing how novel conditions are introduced into the denoising process for conditional generation. Additionally, we offer a detailed overview of research in this area, organizing it into distinct categories from the condition perspective: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at {\textbackslash}url\{https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models\}.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Cao, Pu and Zhou, Feng and Song, Qing and Yang, Lu},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04279 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tang_emergent_2023,
	title = {Emergent {Correspondence} from {Image} {Diffusion}},
	url = {http://arxiv.org/abs/2306.03881},
	doi = {10.48550/arXiv.2306.03881},
	abstract = {Finding correspondences between images is a fundamental problem in computer vision. In this paper, we show that correspondence emerges in image diffusion models without any explicit supervision. We propose a simple strategy to extract this implicit knowledge out of diffusion networks as image features, namely DIffusion FeaTures (DIFT), and use them to establish correspondences between real images. Without any additional fine-tuning or supervision on the task-specific data or annotations, DIFT is able to outperform both weakly-supervised methods and competitive off-the-shelf features in identifying semantic, geometric, and temporal correspondences. Particularly for semantic correspondence, DIFT from Stable Diffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k benchmark. It even outperforms the state-of-the-art supervised methods on 9 out of 18 categories while remaining on par for the overall performance. Project page: https://diffusionfeatures.github.io},
	urldate = {2024-04-03},
	publisher = {arXiv},
	author = {Tang, Luming and Jia, Menglin and Wang, Qianqian and Phoo, Cheng Perng and Hariharan, Bharath},
	month = dec,
	year = {2023},
	note = {arXiv:2306.03881 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{shneiderman_natural_1980,
	address = {USA},
	series = {{ACL} '80},
	title = {Natural vs. precise concise languages for human operation of computers: research issues and experimental approaches},
	shorttitle = {Natural vs. precise concise languages for human operation of computers},
	url = {https://dl.acm.org/doi/10.3115/981436.981478},
	doi = {10.3115/981436.981478},
	abstract = {This paper raises concerns that natural language front ends for computer systems can limit a researcher's scope of thinking, yield inappropriately complex systems, and exaggerate public fear of computers. Alternative modes of computer use are suggested and the role of psychologically oriented controlled experimentation is emphasized. Research methods and recent experimental results are briefly reviewed.},
	urldate = {2024-01-24},
	booktitle = {Proceedings of the 18th annual meeting on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Shneiderman, Ben},
	month = jun,
	year = {1980},
	pages = {139--141},
}

@inproceedings{cohen_role_1992,
	address = {New York, NY, USA},
	series = {{UIST} '92},
	title = {The role of natural language in a multimodal interface},
	isbn = {978-0-89791-549-6},
	url = {https://dl.acm.org/doi/10.1145/142621.142641},
	doi = {10.1145/142621.142641},
	abstract = {Although graphics and direct manipulation are effective interface technologies for some classes of problems, they are limited in many ways. In particular, they provide little support for identifying objects not on the screen, for specifying temporal relations, for identifying and operating on large sets and subsets of entities, and for using the context of interaction. On the other hand, these are precisely strengths of natural language. This paper presents an interface that blends natural language processing and direct manipulation technologies, using each for their characteristic advantages. Specifically, the paper shows how to use natural language to describe objects and temporal relations, and how to use direct manipulation for overcoming hard natural language problems involving the establishment and use of context and pronominal reference. This work has been implemented in SRI's Shoptalk system, a prototype information and decision-support system for manufacturing.},
	urldate = {2024-01-24},
	booktitle = {Proceedings of the 5th annual {ACM} symposium on {User} interface software and technology},
	publisher = {Association for Computing Machinery},
	author = {Cohen, Philip R.},
	month = dec,
	year = {1992},
	pages = {143--149},
}

@misc{saharia_photorealistic_2022,
	title = {Photorealistic {Text}-to-{Image} {Diffusion} {Models} with {Deep} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2205.11487},
	doi = {10.48550/arXiv.2205.11487},
	abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
	month = may,
	year = {2022},
	note = {arXiv:2205.11487 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv:1406.2661 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{shneiderman_future_1982,
	title = {The future of interactive systems and the emergence of direct manipulation†},
	volume = {1},
	issn = {0144-929X},
	url = {https://doi.org/10.1080/01449298208914450},
	doi = {10.1080/01449298208914450},
	abstract = {This paper suggests three motivations for the strong interest in human factors' aspects of user interfaces and reviews five design issues: command language versus menu selection, response time and display rates, wording of system messages, on-line tutorials, explanations and help messages and hardware devices. Five methods and tools for system development are considered: participatory design, specification methods, software implementation tools, pilot studies and acceptance tests and evolutionary refinement based on user feedback. The final portion of the paper presents direct manipulation, an approach which promises to become widely used in interactive systems. Direct manipulation involves representation of the object of interest, rapid incremental reversible actions and physical action instead of complex syntax.},
	number = {3},
	urldate = {2024-01-23},
	journal = {Behaviour \& Information Technology},
	author = {SHNEIDERMAN, BEN},
	month = jul,
	year = {1982},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01449298208914450},
	pages = {237--256},
}

@misc{lugmayr_repaint_2022,
	title = {{RePaint}: {Inpainting} using {Denoising} {Diffusion} {Probabilistic} {Models}},
	shorttitle = {{RePaint}},
	url = {http://arxiv.org/abs/2201.09865},
	doi = {10.48550/arXiv.2201.09865},
	abstract = {Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image information. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks. RePaint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions. Github Repository: git.io/RePaint},
	urldate = {2024-01-23},
	publisher = {arXiv},
	author = {Lugmayr, Andreas and Danelljan, Martin and Romero, Andres and Yu, Fisher and Timofte, Radu and Van Gool, Luc},
	month = aug,
	year = {2022},
	note = {arXiv:2201.09865 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hertz_prompt--prompt_2022,
	title = {Prompt-to-{Prompt} {Image} {Editing} with {Cross} {Attention} {Control}},
	url = {http://arxiv.org/abs/2208.01626},
	doi = {10.48550/arXiv.2208.01626},
	abstract = {Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.},
	urldate = {2024-01-23},
	publisher = {arXiv},
	author = {Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
	month = aug,
	year = {2022},
	note = {arXiv:2208.01626 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{li_dreamedit_2023,
	title = {{DreamEdit}: {Subject}-driven {Image} {Editing}},
	shorttitle = {{DreamEdit}},
	url = {http://arxiv.org/abs/2306.12624},
	doi = {10.48550/arXiv.2306.12624},
	abstract = {Subject-driven image generation aims at generating images containing customized subjects, which has recently drawn enormous attention from the research community. However, the previous works cannot precisely control the background and position of the target subject. In this work, we aspire to fill the void and propose two novel subject-driven sub-tasks, i.e., Subject Replacement and Subject Addition. The new tasks are challenging in multiple aspects: replacing a subject with a customized one can change its shape, texture, and color, while adding a target subject to a designated position in a provided scene necessitates a context-aware posture. To conquer these two novel tasks, we first manually curate a new dataset DreamEditBench containing 22 different types of subjects, and 440 source images with different difficulty levels. We plan to host DreamEditBench as a platform and hire trained evaluators for standard human evaluation. We also devise an innovative method DreamEditor to resolve these tasks by performing iterative generation, which enables a smooth adaptation to the customized subject. In this project, we conduct automatic and human evaluations to understand the performance of DreamEditor and baselines on DreamEditBench. For Subject Replacement, we found that the existing models are sensitive to the shape and color of the original subject. The model failure rate will dramatically increase when the source and target subjects are highly different. For Subject Addition, we found that the existing models cannot easily blend the customized subjects into the background smoothly, leading to noticeable artifacts in the generated image. We hope DreamEditBench can become a standard platform to enable future investigations toward building more controllable subject-driven image editing. Our project homepage is https://dreameditbenchteam.github.io/.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Li, Tianle and Ku, Max and Wei, Cong and Chen, Wenhu},
	month = aug,
	year = {2023},
	note = {arXiv:2306.12624 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{couairon_diffedit_2022,
	title = {{DiffEdit}: {Diffusion}-based semantic image editing with mask guidance},
	shorttitle = {{DiffEdit}},
	url = {http://arxiv.org/abs/2210.11427},
	doi = {10.48550/arXiv.2210.11427},
	abstract = {Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Couairon, Guillaume and Verbeek, Jakob and Schwenk, Holger and Cord, Matthieu},
	month = oct,
	year = {2022},
	note = {arXiv:2210.11427 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_speech_nodate,
	title = {Speech and gestures for graphic image manipulation {\textbar} {Proceedings} of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	url = {https://dl.acm.org/doi/10.1145/67449.67496},
	urldate = {2024-01-17},
}

@misc{noauthor_pixeltone_nodate,
	title = {{PixelTone} {\textbar} {Proceedings} of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	url = {https://dl.acm.org/doi/10.1145/2470654.2481301},
	urldate = {2024-01-17},
}

@article{oviatt_mulitmodal_1997,
	title = {Mulitmodal {Interactive} {Maps}: {Designing} for {Human} {Performance}},
	copyright = {Copyright Taylor and Francis Group, LLC},
	shorttitle = {Mulitmodal {Interactive} {Maps}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/07370024.1997.9667241},
	language = {EN},
	urldate = {2024-01-17},
	journal = {Human–Computer Interaction},
	author = {Oviatt, Sharon},
	month = mar,
	year = {1997},
	note = {Publisher: Lawrence Erlbaum Associates, Inc.},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{min_metaicl_2022,
	title = {{MetaICL}: {Learning} to {Learn} {In} {Context}},
	shorttitle = {{MetaICL}},
	url = {http://arxiv.org/abs/2110.15943},
	doi = {10.48550/arXiv.2110.15943},
	abstract = {We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different meta-training/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task, and outperforms much bigger models with nearly 8x parameters. Finally, we show that MetaICL is complementary to human-written instructions, and the best performance can be achieved by combining both approaches.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
	month = may,
	year = {2022},
	note = {arXiv:2110.15943 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{ren_multiscale_2023,
	title = {Multiscale {Structure} {Guided} {Diffusion} for {Image} {Deblurring}},
	url = {http://arxiv.org/abs/2212.01789},
	doi = {10.48550/arXiv.2212.01789},
	abstract = {Diffusion Probabilistic Models (DPMs) have recently been employed for image deblurring, formulated as an image-conditioned generation process that maps Gaussian noise to the high-quality image, conditioned on the blurry input. Image-conditioned DPMs (icDPMs) have shown more realistic results than regression-based methods when trained on pairwise in-domain data. However, their robustness in restoring images is unclear when presented with out-of-domain images as they do not impose specific degradation models or intermediate constraints. To this end, we introduce a simple yet effective multiscale structure guidance as an implicit bias that informs the icDPM about the coarse structure of the sharp image at the intermediate layers. This guided formulation leads to a significant improvement of the deblurring results, particularly on unseen domain. The guidance is extracted from the latent space of a regression network trained to predict the clean-sharp target at multiple lower resolutions, thus maintaining the most salient sharp structures. With both the blurry input and multiscale guidance, the icDPM model can better understand the blur and recover the clean image. We evaluate a single-dataset trained model on diverse datasets and demonstrate more robust deblurring results with fewer artifacts on unseen data. Our method outperforms existing baselines, achieving state-of-the-art perceptual quality while keeping competitive distortion metrics.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Ren, Mengwei and Delbracio, Mauricio and Talebi, Hossein and Gerig, Guido and Milanfar, Peyman},
	month = dec,
	year = {2023},
	note = {arXiv:2212.01789 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sheynin_emu_2023,
	title = {Emu {Edit}: {Precise} {Image} {Editing} via {Recognition} and {Generation} {Tasks}},
	shorttitle = {Emu {Edit}},
	url = {http://arxiv.org/abs/2311.10089},
	doi = {10.48550/arXiv.2311.10089},
	abstract = {Instruction-based image editing holds immense potential for a variety of applications, as it enables users to perform any editing operation using a natural language instruction. However, current models in this domain often struggle with accurately executing user instructions. We present Emu Edit, a multi-task image editing model which sets state-of-the-art results in instruction-based image editing. To develop Emu Edit we train it to multi-task across an unprecedented range of tasks, such as region-based editing, free-form editing, and Computer Vision tasks, all of which are formulated as generative tasks. Additionally, to enhance Emu Edit's multi-task learning abilities, we provide it with learned task embeddings which guide the generation process towards the correct edit type. Both these elements are essential for Emu Edit's outstanding performance. Furthermore, we show that Emu Edit can generalize to new tasks, such as image inpainting, super-resolution, and compositions of editing tasks, with just a few labeled examples. This capability offers a significant advantage in scenarios where high-quality samples are scarce. Lastly, to facilitate a more rigorous and informed assessment of instructable image editing models, we release a new challenging and versatile benchmark that includes seven different image editing tasks.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Sheynin, Shelly and Polyak, Adam and Singer, Uriel and Kirstain, Yuval and Zohar, Amit and Ashual, Oron and Parikh, Devi and Taigman, Yaniv},
	month = nov,
	year = {2023},
	note = {arXiv:2311.10089 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_gligen_2023-1,
	title = {{GLIGEN}: {Open}-{Set} {Grounded} {Text}-to-{Image} {Generation}},
	shorttitle = {{GLIGEN}},
	url = {http://arxiv.org/abs/2301.07093},
	doi = {10.48550/arXiv.2301.07093},
	abstract = {Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms that of existing supervised layout-to-image baselines by a large margin.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Li, Yuheng and Liu, Haotian and Wu, Qingyang and Mu, Fangzhou and Yang, Jianwei and Gao, Jianfeng and Li, Chunyuan and Lee, Yong Jae},
	month = apr,
	year = {2023},
	note = {arXiv:2301.07093 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{liu_visual_2023,
	title = {Visual {Instruction} {Tuning}},
	url = {http://arxiv.org/abs/2304.08485},
	doi = {10.48550/arXiv.2304.08485},
	abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	month = dec,
	year = {2023},
	note = {arXiv:2304.08485 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{feng_layoutgpt_2023,
	title = {{LayoutGPT}: {Compositional} {Visual} {Planning} and {Generation} with {Large} {Language} {Models}},
	shorttitle = {{LayoutGPT}},
	url = {http://arxiv.org/abs/2305.15393},
	doi = {10.48550/arXiv.2305.15393},
	abstract = {Attaining a high degree of user controllability in visual generation often requires intricate, fine-grained inputs like layouts. However, such inputs impose a substantial burden on users when compared to simple text inputs. To address the issue, we study how Large Language Models (LLMs) can serve as visual planners by generating layouts from text conditions, and thus collaborate with visual generative models. We propose LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs. LayoutGPT can generate plausible layouts in multiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation. When combined with a downstream image generation model, LayoutGPT outperforms text-to-image models/systems by 20-40\% and achieves comparable performance as human users in designing visual layouts for numerical and spatial correctness. Lastly, LayoutGPT achieves comparable performance to supervised methods in 3D indoor scene synthesis, demonstrating its effectiveness and potential in multiple visual domains.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Feng, Weixi and Zhu, Wanrong and Fu, Tsu-jui and Jampani, Varun and Akula, Arjun and He, Xuehai and Basu, Sugato and Wang, Xin Eric and Wang, William Yang},
	month = oct,
	year = {2023},
	note = {arXiv:2305.15393 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{schramowski_safe_2023,
	title = {Safe {Latent} {Diffusion}: {Mitigating} {Inappropriate} {Degeneration} in {Diffusion} {Models}},
	shorttitle = {Safe {Latent} {Diffusion}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Schramowski_Safe_Latent_Diffusion_Mitigating_Inappropriate_Degeneration_in_Diffusion_Models_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-01-15},
	author = {Schramowski, Patrick and Brack, Manuel and Deiseroth, Björn and Kersting, Kristian},
	year = {2023},
	pages = {22522--22531},
}

@misc{hu_tifa_2023,
	title = {{TIFA}: {Accurate} and {Interpretable} {Text}-to-{Image} {Faithfulness} {Evaluation} with {Question} {Answering}},
	shorttitle = {{TIFA}},
	url = {http://arxiv.org/abs/2303.11897},
	doi = {10.48550/arXiv.2303.11897},
	abstract = {Despite thousands of researchers, engineers, and artists actively working on improving text-to-image generation models, systems often fail to produce images that accurately align with the text inputs. We introduce TIFA (Text-to-Image Faithfulness evaluation with question Answering), an automatic evaluation metric that measures the faithfulness of a generated image to its text input via visual question answering (VQA). Specifically, given a text input, we automatically generate several question-answer pairs using a language model. We calculate image faithfulness by checking whether existing VQA models can answer these questions using the generated image. TIFA is a reference-free metric that allows for fine-grained and interpretable evaluations of generated images. TIFA also has better correlations with human judgments than existing metrics. Based on this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse text inputs and 25K questions across 12 categories (object, counting, etc.). We present a comprehensive evaluation of existing text-to-image models using TIFA v1.0 and highlight the limitations and challenges of current models. For instance, we find that current text-to-image models, despite doing well on color and material, still struggle in counting, spatial relations, and composing multiple objects. We hope our benchmark will help carefully measure the research progress in text-to-image synthesis and provide valuable insights for further research.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Hu, Yushi and Liu, Benlin and Kasai, Jungo and Wang, Yizhong and Ostendorf, Mari and Krishna, Ranjay and Smith, Noah A.},
	month = aug,
	year = {2023},
	note = {arXiv:2303.11897 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ruiz_dreambooth_2023,
	title = {{DreamBooth}: {Fine} {Tuning} {Text}-to-{Image} {Diffusion} {Models} for {Subject}-{Driven} {Generation}},
	shorttitle = {{DreamBooth}},
	url = {http://arxiv.org/abs/2208.12242},
	doi = {10.48550/arXiv.2208.12242},
	abstract = {Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for "personalization" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
	month = mar,
	year = {2023},
	note = {arXiv:2208.12242 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@article{hutchins_direct_1985,
	title = {Direct {Manipulation} {Interfaces}},
	volume = {1},
	issn = {0737-0024},
	url = {https://doi.org/10.1207/s15327051hci0104_2},
	doi = {10.1207/s15327051hci0104_2},
	abstract = {Direct manipulation has been lauded as a good form of interface design, and some interfaces that have this property have been well received by users. In this article we seek a cognitive account of both the advantages and disadvantages of direct manipulation interfaces. We identify two underlying phenomena that give rise to the feeling of directness. One deals with the information processing distance between the user's intentions and the facilities provided by the machine. Reduction of this distance makes the interface feel direct by reducing the effort required of the user to accomplish goals. The second phenomenon concerns the relation between the input and output vocabularies of the interface language. In particular, direct manipulation requires that the system provide representations of objects that behave as if they are the objects themselves. This provides the feeling of directness of manipulation.},
	number = {4},
	urldate = {2024-01-14},
	journal = {Human–Computer Interaction},
	author = {Hutchins, Edwin L. and Hollan, James D. and Norman, Donald A.},
	month = dec,
	year = {1985},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1207/s15327051hci0104\_2},
	pages = {311--338},
}

@misc{noauthor_direct_nodate,
	title = {Direct {Manipulation}: {A} {Step} {Beyond} {Programming} {Languages} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/1654471},
	urldate = {2024-01-14},
}

@article{van_dam_post-wimp_1997,
	title = {Post-{WIMP} user interfaces},
	volume = {40},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/253671.253708},
	doi = {10.1145/253671.253708},
	number = {2},
	urldate = {2024-01-14},
	journal = {Communications of the ACM},
	author = {van Dam, Andries},
	month = feb,
	year = {1997},
	pages = {63--67},
}

@misc{dong_survey_2023,
	title = {A {Survey} on {In}-context {Learning}},
	url = {http://arxiv.org/abs/2301.00234},
	doi = {10.48550/arXiv.2301.00234},
	abstract = {With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.},
	urldate = {2024-01-14},
	publisher = {arXiv},
	author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Li, Lei and Sui, Zhifang},
	month = jun,
	year = {2023},
	note = {arXiv:2301.00234 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{petroni_language_2019,
	title = {Language {Models} as {Knowledge} {Bases}?},
	url = {http://arxiv.org/abs/1909.01066},
	doi = {10.48550/arXiv.1909.01066},
	abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.},
	urldate = {2024-01-14},
	publisher = {arXiv},
	author = {Petroni, Fabio and Rocktäschel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
	month = sep,
	year = {2019},
	note = {arXiv:1909.01066 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@incollection{perrault_chapter_1988,
	title = {Chapter 4 - {Natural}-{Language} {Interfaces}},
	isbn = {978-0-934613-67-5},
	url = {https://www.sciencedirect.com/science/article/pii/B9780934613675500083},
	abstract = {Since the early 1960s when support decreased for machine translation, much of the research on natural-language processing in North America has been motivated by its potential use for communicating with software systems. Natural-language systems have been developed to extract information from databases, to control (simulated) robots, to interact with graphic systems, to specify simulation problems, and to communicate with systems embodying expertise in some task or problem area. This chapter discusses the interfaces to database management systems. Apart from being among the earliest interface systems developed, interfaces to databases account for most of the natural-language interfaces (NLIs) implemented and they are the subject of a substantial literature. The chapter discusses the main system architectures used in NLIs and the body of techniques developed for them. In doing so, it distinguishes between the task of an interface and its domain. Natural language is but one of the methods available for human–machine interaction. The reasons for its attractiveness are obvious. They are: it provides an immediate vocabulary for talking about the contents of the database and a means of accessing information in the database independently of its structure and encodings, shields the user from the formal access language of the underlying system, and is available with a minimum of training to both novice and occasional user.},
	urldate = {2024-01-14},
	booktitle = {Exploring {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann},
	author = {Perrault, C. Raymond and Grosz, Barbara J.},
	editor = {Shrobe, Howard E. and {the American Association for Artificial Intelligence}},
	month = jan,
	year = {1988},
	doi = {10.1016/B978-0-934613-67-5.50008-3},
	pages = {133--172},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2024-01-14},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{brown_language_2020-1,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2024-01-14},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{lee_picture_1991,
	title = {Picture algebra for spatial reasoning of iconic images represented in {2D} {C}-string},
	volume = {12},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/016786559190325G},
	doi = {10.1016/0167-8655(91)90325-G},
	abstract = {A new spatial knowledge representation 2D C-string with accompanied cutting mechanism and a set of spatial operators are proposed. 2D C-string is characteristic of spatial knowledge embedded in images and is efficient in representation and manipulation of images. In this paper, transitive laws, distributive laws and manipulation laws of picture algebra are presented. All the binary relationships among objects in an image can be derived from 2D C-string. The picture algebra provides the theoretic basis for spatial reasoning and pictorial query inference.},
	number = {7},
	urldate = {2024-01-10},
	journal = {Pattern Recognition Letters},
	author = {Lee, Suh-Yin and Hsu, Fang-Jung},
	month = jul,
	year = {1991},
	keywords = {2D C-string, Image database, pictorial query, picture algebra, spatial knowledge, spatial reasoning},
	pages = {425--435},
}



@misc{brooks_instructpix2pix_2023,
	title = {{InstructPix2Pix}: {Learning} to {Follow} {Image} {Editing} {Instructions}},
	shorttitle = {{InstructPix2Pix}},
	url = {http://arxiv.org/abs/2211.09800},
	doi = {10.48550/arXiv.2211.09800},
	abstract = {We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models -- a language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.},
	urldate = {2023-12-21},
	publisher = {arXiv},
	author = {Brooks, Tim and Holynski, Aleksander and Efros, Alexei A.},
	month = jan,
	year = {2023},
	note = {arXiv:2211.09800 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{sun_dreamsync_2023,
	title = {{DreamSync}: {Aligning} {Text}-to-{Image} {Generation} with {Image} {Understanding} {Feedback}},
	shorttitle = {{DreamSync}},
	url = {http://arxiv.org/abs/2311.17946},
	doi = {10.48550/arXiv.2311.17946},
	abstract = {Despite their wide-spread success, Text-to-Image models (T2I) still struggle to produce images that are both aesthetically pleasing and faithful to the user's input text. We introduce DreamSync, a model-agnostic training algorithm by design that improves T2I models to be faithful to the text input. DreamSync builds off a recent insight from TIFA's evaluation framework -- that large vision-language models (VLMs) can effectively identify the fine-grained discrepancies between generated images and the text inputs. DreamSync uses this insight to train T2I models without any labeled data; it improves T2I models using its own generations. First, it prompts the model to generate several candidate images for a given input text. Then, it uses two VLMs to select the best generation: a Visual Question Answering model that measures the alignment of generated images to the text, and another that measures the generation's aesthetic quality. After selection, we use LoRA to iteratively finetune the T2I model to guide its generation towards the selected best generations. DreamSync does not need any additional human annotation. model architecture changes, or reinforcement learning. Despite its simplicity, DreamSync improves both the semantic alignment and aesthetic appeal of two diffusion-based T2I models, evidenced by multiple benchmarks (+1.7\% on TIFA, +2.9\% on DSG1K, +3.4\% on VILA aesthetic) and human evaluation.},
	urldate = {2023-12-10},
	publisher = {arXiv},
	author = {Sun, Jiao and Fu, Deqing and Hu, Yushi and Wang, Su and Rassin, Royi and Juan, Da-Cheng and Alon, Dana and Herrmann, Charles and van Steenkiste, Sjoerd and Krishna, Ranjay and Rashtchian, Cyrus},
	month = nov,
	year = {2023},
	note = {arXiv:2311.17946 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chakrabarty_learning_2023,
	title = {Learning to {Follow} {Object}-{Centric} {Image} {Editing} {Instructions} {Faithfully}},
	url = {http://arxiv.org/abs/2310.19145},
	abstract = {Natural language instructions are a powerful interface for editing the outputs of text-to-image diffusion models. However, several challenges need to be addressed: 1) underspecification (the need to model the implicit meaning of instructions) 2) grounding (the need to localize where the edit has to be performed), 3) faithfulness (the need to preserve the elements of the image not affected by the edit instruction). Current approaches focusing on image editing with natural language instructions rely on automatically generated paired data, which, as shown in our investigation, is noisy and sometimes nonsensical, exacerbating the above issues. Building on recent advances in segmentation, Chain-of-Thought prompting, and visual question answering, we significantly improve the quality of the paired data. In addition, we enhance the supervision signal by highlighting parts of the image that need to be changed by the instruction. The model fine-tuned on the improved data is capable of performing fine-grained object-centric edits better than state-of-the-art baselines, mitigating the problems outlined above, as shown by automatic and human evaluations. Moreover, our model is capable of generalizing to domains unseen during training, such as visual metaphors.},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Chakrabarty, Tuhin and Singh, Kanishk and Saakyan, Arkadiy and Muresan, Smaranda},
	month = oct,
	year = {2023},
	note = {arXiv:2310.19145 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{han_improving_2023,
	title = {Improving {Tuning}-{Free} {Real} {Image} {Editing} with {Proximal} {Guidance}},
	url = {http://arxiv.org/abs/2306.05414},
	abstract = {DDIM inversion has revealed the remarkable potential of real image editing within diffusion-based methods. However, the accuracy of DDIM reconstruction degrades as larger classifier-free guidance (CFG) scales being used for enhanced editing. Null-text inversion (NTI) optimizes null embeddings to align the reconstruction and inversion trajectories with larger CFG scales, enabling real image editing with cross-attention control. Negative-prompt inversion (NPI) further offers a training-free closed-form solution of NTI. However, it may introduce artifacts and is still constrained by DDIM reconstruction quality. To overcome these limitations, we propose proximal guidance and incorporate it to NPI with cross-attention control. We enhance NPI with a regularization term and reconstruction guidance, which reduces artifacts while capitalizing on its training-free nature. Additionally, we extend the concepts to incorporate mutual self-attention control, enabling geometry and layout alterations in the editing process. Our method provides an efficient and straightforward approach, effectively addressing real image editing tasks with minimal computational overhead.},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Han, Ligong and Wen, Song and Chen, Qi and Zhang, Zhixing and Song, Kunpeng and Ren, Mengwei and Gao, Ruijiang and Stathopoulos, Anastasis and He, Xiaoxiao and Chen, Yuxiao and Liu, Di and Zhangli, Qilong and Jiang, Jindong and Xia, Zhaoyang and Srivastava, Akash and Metaxas, Dimitris},
	month = jul,
	year = {2023},
	note = {arXiv:2306.05414 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{miyake_negative-prompt_2023,
	title = {Negative-prompt {Inversion}: {Fast} {Image} {Inversion} for {Editing} with {Text}-guided {Diffusion} {Models}},
	shorttitle = {Negative-prompt {Inversion}},
	url = {http://arxiv.org/abs/2305.16807},
	abstract = {In image editing employing diffusion models, it is crucial to preserve the reconstruction quality of the original image while changing its style. Although existing methods ensure reconstruction quality through optimization, a drawback of these is the significant amount of time required for optimization. In this paper, we propose negative-prompt inversion, a method capable of achieving equivalent reconstruction solely through forward propagation without optimization, thereby enabling much faster editing processes. We experimentally demonstrate that the reconstruction quality of our method is comparable to that of existing methods, allowing for inversion at a resolution of 512 pixels and with 50 sampling steps within approximately 5 seconds, which is more than 30 times faster than null-text inversion. Reduction of the computation time by the proposed method further allows us to use a larger number of sampling steps in diffusion models to improve the reconstruction quality with a moderate increase in computation time.},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Miyake, Daiki and Iohara, Akihiro and Saito, Yu and Tanaka, Toshiyuki},
	month = may,
	year = {2023},
	note = {arXiv:2305.16807 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{rafailov_direct_2023,
	title = {Direct {Preference} {Optimization}: {Your} {Language} {Model} is {Secretly} a {Reward} {Model}},
	shorttitle = {Direct {Preference} {Optimization}},
	url = {http://arxiv.org/abs/2305.18290},
	abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
	month = may,
	year = {2023},
	note = {arXiv:2305.18290 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{cao_masactrl_2023,
	title = {{MasaCtrl}: {Tuning}-{Free} {Mutual} {Self}-{Attention} {Control} for {Consistent} {Image} {Synthesis} and {Editing}},
	shorttitle = {{MasaCtrl}},
	url = {http://arxiv.org/abs/2304.08465},
	abstract = {Despite the success in large-scale text-to-image generation and text-conditioned image editing, existing methods still struggle to produce consistent generation and editing results. For example, generation approaches usually fail to synthesize multiple images of the same objects/characters but with different views or poses. Meanwhile, existing editing methods either fail to achieve effective complex non-rigid editing while maintaining the overall textures and identity, or require time-consuming fine-tuning to capture the image-specific appearance. In this paper, we develop MasaCtrl, a tuning-free method to achieve consistent image generation and complex non-rigid image editing simultaneously. Specifically, MasaCtrl converts existing self-attention in diffusion models into mutual self-attention, so that it can query correlated local contents and textures from source images for consistency. To further alleviate the query confusion between foreground and background, we propose a mask-guided mutual self-attention strategy, where the mask can be easily extracted from the cross-attention maps. Extensive experiments show that the proposed MasaCtrl can produce impressive results in both consistent image generation and complex non-rigid real image editing.},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Cao, Mingdeng and Wang, Xintao and Qi, Zhongang and Shan, Ying and Qie, Xiaohu and Zheng, Yinqiang},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08465 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{couairon_zero-shot_nodate,
	title = {Zero-{Shot} {Spatial} {Layout} {Conditioning} for {Text}-to-{Image} {Diffusion} {Models}},
	abstract = {Large-scale text-to-image diffusion models have significantly improved the state of the art in generative image modeling and allow for an intuitive and powerful user interface to drive the image generation process. Expressing spatial constraints, e.g. to position specific objects in particular locations, is cumbersome using text; and current text-based image generation models are not able to accurately follow such instructions. In this paper we consider image generation from text associated with segments on the image canvas, which combines an intuitive natural language interface with precise spatial control over the generated content. We propose ZestGuide, a “zero-shot” segmentation guidance approach that can be plugged into pre-trained text-to-image diffusion models, and does not require any additional training. It leverages implicit segmentation maps that can be extracted from cross-attention layers, and uses them to align the generation with input masks. Our experimental results combine high image quality with accurate alignment of generated content with input segmentations, and improve over prior work both quantitatively and qualitatively, including methods that require training on images with corresponding segmentations. Compared to Paint with Words, the previous state-of-the art in image generation with zero-shot segmentation conditioning, we improve by 5 to 10 mIoU points on the COCO dataset with similar FID scores.},
	language = {en},
	author = {Couairon, Guillaume and Careil, Marlene and Cord, Matthieu and Lathuiliere, Stephane and Verbeek, Jakob},
}

@misc{lee_multimodal_2023,
	title = {Multimodal {Prompting} with {Missing} {Modalities} for {Visual} {Recognition}},
	url = {http://arxiv.org/abs/2303.03369},
	abstract = {In this paper, we tackle two challenges in multimodal learning for visual recognition: 1) when missing-modality occurs either during training or testing in real-world situations; and 2) when the computation resources are not available to finetune on heavy transformer models. To this end, we propose to utilize prompt learning and mitigate the above two challenges together. Specifically, our modality-missing-aware prompts can be plugged into multimodal transformers to handle general missing-modality cases, while only requiring less than 1\% learnable parameters compared to training the entire model. We further explore the effect of different prompt configurations and analyze the robustness to missing modality. Extensive experiments are conducted to show the effectiveness of our prompt learning framework that improves the performance under various missing-modality cases, while alleviating the requirement of heavy model re-training. Code is available.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Lee, Yi-Lun and Tsai, Yi-Hsuan and Chiu, Wei-Chen and Lee, Chen-Yu},
	month = mar,
	year = {2023},
	note = {arXiv:2303.03369 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cai_leveraging_2023,
	title = {Leveraging {Large} {Language} {Models} for {Scalable} {Vector} {Graphics}-{Driven} {Image} {Understanding}},
	url = {http://arxiv.org/abs/2306.06094},
	abstract = {Recently, large language models (LLMs) have made significant advancements in natural language understanding and generation. However, their potential in computer vision remains largely unexplored. In this paper, we introduce a new, exploratory approach that enables LLMs to process images using the Scalable Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions of SVG representations instead of raster images, we aim to bridge the gap between the visual and textual modalities, allowing LLMs to directly understand and manipulate images without the need for parameterized visual components. Our method facilitates simple image classification, generation, and in-context learning using only LLM capabilities. We demonstrate the promise of our approach across discriminative and generative tasks, highlighting its (i) robustness against distribution shift, (ii) substantial improvements achieved by tapping into the in-context learning abilities of LLMs, and (iii) image understanding and generation capabilities with human guidance. Our code, data, and models can be found here https://github.com/mu-cai/svg-llm.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Cai, Mu and Huang, Zeyi and Li, Yuheng and Wang, Haohan and Lee, Yong Jae},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06094 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{wei_dialogpaint_2023,
	title = {{DialogPaint}: {A} {Dialog}-based {Image} {Editing} {Model}},
	shorttitle = {{DialogPaint}},
	url = {http://arxiv.org/abs/2303.10073},
	abstract = {We present DialogPaint, an innovative framework that employs an interactive conversational approach for image editing. The framework comprises a pretrained dialogue model (Blenderbot) and a diffusion model (Stable Diffusion). The dialogue model engages in conversation with users to understand their requirements and generates concise instructions based on the dialogue. Subsequently, the Stable Diffusion model employs these instructions, along with the input image, to produce the desired output. Due to the difficulty of acquiring fine-tuning data for such models, we leverage multiple large-scale models to generate simulated dialogues and corresponding image pairs. After fine-tuning our framework with the synthesized data, we evaluate its performance in real application scenes. The results demonstrate that DialogPaint excels in both objective and subjective evaluation metrics effectively handling ambiguous instructions and performing tasks such as object replacement, style transfer, color modification. Moreover, our framework supports multi-round editing, allowing for the completion of complicated editing tasks.},
	urldate = {2023-09-29},
	publisher = {arXiv},
	author = {Wei, Jingxuan and Wu, Shiyu and Jiang, Xin and Wang, Yequan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.10073 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{avrahami_break--scene_2023,
	title = {Break-{A}-{Scene}: {Extracting} {Multiple} {Concepts} from a {Single} {Image}},
	shorttitle = {Break-{A}-{Scene}},
	url = {http://arxiv.org/abs/2305.16311},
	abstract = {Text-to-image model personalization aims to introduce a user-provided concept to the model, allowing its synthesis in diverse contexts. However, current methods primarily focus on the case of learning a single concept from multiple images with variations in backgrounds and poses, and struggle when adapted to a different scenario. In this work, we introduce the task of textual scene decomposition: given a single image of a scene that may contain several concepts, we aim to extract a distinct text token for each concept, enabling fine-grained control over the generated scenes. To this end, we propose augmenting the input image with masks that indicate the presence of target concepts. These masks can be provided by the user or generated automatically by a pre-trained segmentation model. We then present a novel two-phase customization process that optimizes a set of dedicated textual embeddings (handles), as well as the model weights, striking a delicate balance between accurately capturing the concepts and avoiding overfitting. We employ a masked diffusion loss to enable handles to generate their assigned concepts, complemented by a novel loss on cross-attention maps to prevent entanglement. We also introduce union-sampling, a training strategy aimed to improve the ability of combining multiple concepts in generated images. We use several automatic metrics to quantitatively compare our method against several baselines, and further affirm the results using a user study. Finally, we showcase several applications of our method. Project page is available at: https://omriavrahami.com/break-a-scene/},
	urldate = {2023-09-29},
	publisher = {arXiv},
	author = {Avrahami, Omri and Aberman, Kfir and Fried, Ohad and Cohen-Or, Daniel and Lischinski, Dani},
	month = may,
	year = {2023},
	note = {arXiv:2305.16311 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{wang_caption_2023,
	title = {Caption {Anything}: {Interactive} {Image} {Description} with {Diverse} {Multimodal} {Controls}},
	shorttitle = {Caption {Anything}},
	url = {http://arxiv.org/abs/2305.02677},
	abstract = {Controllable image captioning is an emerging multimodal topic that aims to describe the image with natural language following human purpose, \${\textbackslash}textit\{e.g.\}\$, looking at the specified regions or telling in a particular text style. State-of-the-art methods are trained on annotated pairs of input controls and output captions. However, the scarcity of such well-annotated multimodal data largely limits their usability and scalability for interactive AI systems. Leveraging unimodal instruction-following foundation models is a promising alternative that benefits from broader sources of data. In this paper, we present Caption AnyThing (CAT), a foundation model augmented image captioning framework supporting a wide range of multimodel controls: 1) visual controls, including points, boxes, and trajectories; 2) language controls, such as sentiment, length, language, and factuality. Powered by Segment Anything Model (SAM) and ChatGPT, we unify the visual and language prompts into a modularized framework, enabling the flexible combination between different controls. Extensive case studies demonstrate the user intention alignment capabilities of our framework, shedding light on effective user interaction modeling in vision-language applications. Our code is publicly available at https://github.com/ttengwang/Caption-Anything.},
	urldate = {2023-09-29},
	publisher = {arXiv},
	author = {Wang, Teng and Zhang, Jinrui and Fei, Junjie and Zheng, Hao and Tang, Yunlong and Li, Zhe and Gao, Mingqi and Zhao, Shanshan},
	month = jul,
	year = {2023},
	note = {arXiv:2305.02677 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_anydoor_2023,
	title = {{AnyDoor}: {Zero}-shot {Object}-level {Image} {Customization}},
	shorttitle = {{AnyDoor}},
	url = {http://arxiv.org/abs/2307.09481},
	abstract = {This work presents AnyDoor, a diffusion-based image generator with the power to teleport target objects to new scenes at user-specified locations in a harmonious way. Instead of tuning parameters for each object, our model is trained only once and effortlessly generalizes to diverse object-scene combinations at the inference stage. Such a challenging zero-shot setting requires an adequate characterization of a certain object. To this end, we complement the commonly used identity feature with detail features, which are carefully designed to maintain texture details yet allow versatile local variations (e.g., lighting, orientation, posture, etc.), supporting the object in favorably blending with different surroundings. We further propose to borrow knowledge from video datasets, where we can observe various forms (i.e., along the time axis) of a single object, leading to stronger model generalizability and robustness. Extensive experiments demonstrate the superiority of our approach over existing alternatives as well as its great potential in real-world applications, such as virtual try-on and object moving. Project page is https://damo-vilab.github.io/AnyDoor-Page/.},
	urldate = {2023-09-27},
	publisher = {arXiv},
	author = {Chen, Xi and Huang, Lianghua and Liu, Yu and Shen, Yujun and Zhao, Deli and Zhao, Hengshuang},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09481 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_multimodal_2023,
	title = {Multimodal {Chain}-of-{Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2302.00923},
	abstract = {Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16 percentage points (75.17\%-{\textgreater}91.68\% accuracy) on the ScienceQA benchmark and even surpasses human performance. Code is publicly available available at https://github.com/amazon-science/mm-cot.},
	urldate = {2023-09-27},
	publisher = {arXiv},
	author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00923 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{su_dual_2023,
	title = {Dual {Diffusion} {Implicit} {Bridges} for {Image}-to-{Image} {Translation}},
	url = {http://arxiv.org/abs/2203.08382},
	abstract = {Common image-to-image translation methods rely on joint training over data from both source and target domains. The training process requires concurrent access to both datasets, which hinders data separation and privacy protection; and existing models cannot be easily adapted for translation of new domain pairs. We present Dual Diffusion Implicit Bridges (DDIBs), an image translation method based on diffusion models, that circumvents training on domain pairs. Image translation with DDIBs relies on two diffusion models trained independently on each domain, and is a two-step process: DDIBs first obtain latent encodings for source images with the source diffusion model, and then decode such encodings using the target model to construct target images. Both steps are defined via ordinary differential equations (ODEs), thus the process is cycle consistent only up to discretization errors of the ODE solvers. Theoretically, we interpret DDIBs as concatenation of source to latent, and latent to target Schrodinger Bridges, a form of entropy-regularized optimal transport, to explain the efficacy of the method. Experimentally, we apply DDIBs on synthetic and high-resolution image datasets, to demonstrate their utility in a wide variety of translation tasks and their inherent optimal transport properties.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Su, Xuan and Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
	month = mar,
	year = {2023},
	note = {arXiv:2203.08382 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{shi_dragdiffusion_2023,
	title = {{DragDiffusion}: {Harnessing} {Diffusion} {Models} for {Interactive} {Point}-based {Image} {Editing}},
	shorttitle = {{DragDiffusion}},
	url = {http://arxiv.org/abs/2306.14435},
	abstract = {Precise and controllable image editing is a challenging task that has attracted significant attention. Recently, DragGAN enables an interactive point-based image editing framework and achieves impressive editing results with pixel-level precision. However, since this method is based on generative adversarial networks (GAN), its generality is upper-bounded by the capacity of the pre-trained GAN models. In this work, we extend such an editing framework to diffusion models and propose DragDiffusion. By leveraging large-scale pretrained diffusion models, we greatly improve the applicability of interactive point-based editing in real world scenarios. While most existing diffusion-based image editing methods work on text embeddings, DragDiffusion optimizes the diffusion latent to achieve precise spatial control. Although diffusion models generate images in an iterative manner, we empirically show that optimizing diffusion latent at one single step suffices to generate coherent results, enabling DragDiffusion to complete high-quality editing efficiently. Extensive experiments across a wide range of challenging cases (e.g., multi-objects, diverse object categories, various styles, etc.) demonstrate the versatility and generality of DragDiffusion. Code: https://github.com/Yujun-Shi/DragDiffusion.},
	urldate = {2023-09-14},
	publisher = {arXiv},
	author = {Shi, Yujun and Xue, Chuhui and Pan, Jiachun and Zhang, Wenqing and Tan, Vincent Y. F. and Bai, Song},
	month = jul,
	year = {2023},
	note = {arXiv:2306.14435 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}



@misc{zhang_hive_2023,
	title = {{HIVE}: {Harnessing} {Human} {Feedback} for {Instructional} {Visual} {Editing}},
	shorttitle = {{HIVE}},
	url = {http://arxiv.org/abs/2303.09618},
	abstract = {Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of instructional image editing. We conduct extensive empirical experiments quantitatively and qualitatively, showing that HIVE is favored over previous state-of-the-art instructional image editing approaches by a large margin.},
	urldate = {2023-09-12},
	publisher = {arXiv},
	author = {Zhang, Shu and Yang, Xinyi and Feng, Yihao and Qin, Can and Chen, Chia-Chih and Yu, Ning and Chen, Zeyuan and Wang, Huan and Savarese, Silvio and Ermon, Stefano and Xiong, Caiming and Xu, Ran},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09618 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@misc{noauthor_convex_nodate,
	title = {Convex {Optimization} {HW2}},
	url = {https://www.overleaf.com/project/64f5f8fb15603e0237196eb7},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2023-09-08},
}

@misc{mou_dragondiffusion_2023,
	title = {{DragonDiffusion}: {Enabling} {Drag}-style {Manipulation} on {Diffusion} {Models}},
	shorttitle = {{DragonDiffusion}},
	url = {http://arxiv.org/abs/2307.02421},
	abstract = {Despite the ability of existing large-scale text-to-image (T2I) models to generate high-quality images from detailed textual descriptions, they often lack the ability to precisely edit the generated or real images. In this paper, we propose a novel image editing method, DragonDiffusion, enabling Drag-style manipulation on Diffusion models. Specifically, we construct classifier guidance based on the strong correspondence of intermediate features in the diffusion model. It can transform the editing signals into gradients via feature correspondence loss to modify the intermediate representation of the diffusion model. Based on this guidance strategy, we also build a multi-scale guidance to consider both semantic and geometric alignment. Moreover, a cross-branch self-attention is added to maintain the consistency between the original image and the editing result. Our method, through an efficient design, achieves various editing modes for the generated or real images, such as object moving, object resizing, object appearance replacement, and content dragging. It is worth noting that all editing and content preservation signals come from the image itself, and the model does not require fine-tuning or additional modules. Our source code will be available at https://github.com/MC-E/DragonDiffusion.},
	urldate = {2023-08-31},
	publisher = {arXiv},
	author = {Mou, Chong and Wang, Xintao and Song, Jiechong and Shan, Ying and Zhang, Jian},
	month = jul,
	year = {2023},
	note = {arXiv:2307.02421 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{luccioni_whats_2021,
	address = {Online},
	title = {What's in the {Box}? {An} {Analysis} of {Undesirable} {Content} in the {Common} {Crawl} {Corpus}},
	shorttitle = {What's in the {Box}?},
	url = {https://aclanthology.org/2021.acl-short.24},
	doi = {10.18653/v1/2021.acl-short.24},
	abstract = {Whereas much of the success of the current generation of neural language models has been driven by increasingly large training corpora, relatively little research has been dedicated to analyzing these massive sources of textual data. In this exploratory analysis, we delve deeper into the Common Crawl, a colossal web corpus that is extensively used for training language models. We find that it contains a significant amount of undesirable content, including hate speech and sexually explicit content, even after filtering procedures. We discuss the potential impacts of this content on language models and conclude with future research directions and a more mindful approach to corpus collection and analysis.},
	urldate = {2023-08-09},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Luccioni, Alexandra and Viviano, Joseph},
	month = aug,
	year = {2021},
	pages = {182--189},
}

@misc{szegedy_intriguing_2014,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	doi = {10.48550/arXiv.1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	urldate = {2023-08-09},
	publisher = {arXiv},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	month = feb,
	year = {2014},
	note = {arXiv:1312.6199 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{meng_sdedit_2022,
	title = {{SDEdit}: {Guided} {Image} {Synthesis} and {Editing} with {Stochastic} {Differential} {Equations}},
	shorttitle = {{SDEdit}},
	url = {http://arxiv.org/abs/2108.01073},
	abstract = {Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort. The key challenge is balancing faithfulness to the user input (e.g., hand-drawn colored strokes) and realism of the synthesized image. Existing GAN-based methods attempt to achieve such balance using either conditional GANs or GAN inversions, which are challenging and often require additional training data or loss functions for individual applications. To address these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE). Given an input image with user guide of any type, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit significantly outperforms state-of-the-art GAN-based methods by up to 98.09\% on realism and 91.72\% on overall satisfaction scores, according to a human perception study, on multiple tasks, including stroke-based image synthesis and editing as well as image compositing.},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Meng, Chenlin and He, Yutong and Song, Yang and Song, Jiaming and Wu, Jiajun and Zhu, Jun-Yan and Ermon, Stefano},
	month = jan,
	year = {2022},
	note = {arXiv:2108.01073 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lee_aligning_2023,
	title = {Aligning {Text}-to-{Image} {Models} using {Human} {Feedback}},
	url = {http://arxiv.org/abs/2302.12192},
	abstract = {Deep generative models have shown impressive results in text-to-image synthesis. However, current text-to-image models often generate images that are inadequately aligned with text prompts. We propose a fine-tuning method for aligning such models using human feedback, comprising three stages. First, we collect human feedback assessing model output alignment from a set of diverse text prompts. We then use the human-labeled image-text dataset to train a reward function that predicts human feedback. Lastly, the text-to-image model is fine-tuned by maximizing reward-weighted likelihood to improve image-text alignment. Our method generates objects with specified colors, counts and backgrounds more accurately than the pre-trained model. We also analyze several design choices and find that careful investigations on such design choices are important in balancing the alignment-fidelity tradeoffs. Our results demonstrate the potential for learning from human feedback to significantly improve text-to-image models.},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Lee, Kimin and Liu, Hao and Ryu, Moonkyung and Watkins, Olivia and Du, Yuqing and Boutilier, Craig and Abbeel, Pieter and Ghavamzadeh, Mohammad and Gu, Shixiang Shane},
	month = feb,
	year = {2023},
	note = {arXiv:2302.12192 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{lee_aligning_2023-1,
	title = {Aligning {Text}-to-{Image} {Models} using {Human} {Feedback}},
	url = {http://arxiv.org/abs/2302.12192},
	doi = {10.48550/arXiv.2302.12192},
	abstract = {Deep generative models have shown impressive results in text-to-image synthesis. However, current text-to-image models often generate images that are inadequately aligned with text prompts. We propose a fine-tuning method for aligning such models using human feedback, comprising three stages. First, we collect human feedback assessing model output alignment from a set of diverse text prompts. We then use the human-labeled image-text dataset to train a reward function that predicts human feedback. Lastly, the text-to-image model is fine-tuned by maximizing reward-weighted likelihood to improve image-text alignment. Our method generates objects with specified colors, counts and backgrounds more accurately than the pre-trained model. We also analyze several design choices and find that careful investigations on such design choices are important in balancing the alignment-fidelity tradeoffs. Our results demonstrate the potential for learning from human feedback to significantly improve text-to-image models.},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Lee, Kimin and Liu, Hao and Ryu, Moonkyung and Watkins, Olivia and Du, Yuqing and Boutilier, Craig and Abbeel, Pieter and Ghavamzadeh, Mohammad and Gu, Shixiang Shane},
	month = feb,
	year = {2023},
	note = {arXiv:2302.12192 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{black_training_2023,
	title = {Training {Diffusion} {Models} with {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2305.13301},
	abstract = {Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation.},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Black, Kevin and Janner, Michael and Du, Yilun and Kostrikov, Ilya and Levine, Sergey},
	month = may,
	year = {2023},
	note = {arXiv:2305.13301 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{noauthor_training-free_nodate,
	title = {Training-{Free} {Structured} {Diffusion} {Guidance} for {Compositional} {Text}-to-{Image} {Synthesis} {\textbar} {OpenReview}},
	url = {https://openreview.net/forum?id=PUIqjT4rzq7},
	urldate = {2023-06-07},
	keywords = {Important},
}

@misc{li_gligen_2023-2,
	title = {{GLIGEN}: {Open}-{Set} {Grounded} {Text}-to-{Image} {Generation}},
	shorttitle = {{GLIGEN}},
	url = {http://arxiv.org/abs/2301.07093},
	doi = {10.48550/arXiv.2301.07093},
	abstract = {Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms that of existing supervised layout-to-image baselines by a large margin.},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Li, Yuheng and Liu, Haotian and Wu, Qingyang and Mu, Fangzhou and Yang, Jianwei and Gao, Jianfeng and Li, Chunyuan and Lee, Yong Jae},
	month = apr,
	year = {2023},
	note = {arXiv:2301.07093 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Important},
}

@misc{li_guiding_2023,
	title = {Guiding {Text}-to-{Image} {Diffusion} {Model} {Towards} {Grounded} {Generation}},
	url = {http://arxiv.org/abs/2301.05221},
	doi = {10.48550/arXiv.2301.05221},
	abstract = {The goal of this paper is to augment a pre-trained text-to-image diffusion model with the ability of open-vocabulary objects grounding, i.e., simultaneously generating images and segmentation masks for the corresponding visual entities described in the text prompt. We make the following contributions: (i) we insert a grounding module into the existing diffusion model, that can be trained to align the visual and textual embedding space of the diffusion model with only a small number of object categories; (ii) we propose an automatic pipeline for constructing a dataset, that consists of \{image, segmentation mask, text prompt\} triplets, to train the proposed grounding module; (iii) we evaluate the performance of open-vocabulary grounding on images generated from the text-to-image diffusion model and show that the module can well segment the objects of categories beyond seen ones at training time; (iv) we adopt the guided diffusion model to build a synthetic semantic segmentation dataset, and show that training a standard segmentation model on such dataset demonstrates competitive performance on zero-shot segmentation(ZS3) benchmark, which opens up new opportunities for adopting the powerful diffusion model for discriminative tasks.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Li, Ziyi and Zhou, Qinye and Zhang, Xiaoyun and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
	month = jan,
	year = {2023},
	note = {arXiv:2301.05221 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Important},
}

@misc{huang_composer_2023,
	title = {Composer: {Creative} and {Controllable} {Image} {Synthesis} with {Composable} {Conditions}},
	shorttitle = {Composer},
	url = {http://arxiv.org/abs/2302.09778},
	abstract = {Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability. This work offers a new generation paradigm that allows flexible control of the output image, such as spatial layout and palette, while maintaining the synthesis quality and model creativity. With compositionality as the core idea, we first decompose an image into representative factors, and then train a diffusion model with all these factors as the conditions to recompose the input. At the inference stage, the rich intermediate representations work as composable elements, leading to a huge design space (i.e., exponentially proportional to the number of decomposed factors) for customizable content creation. It is noteworthy that our approach, which we call Composer, supports various levels of conditions, such as text description as the global information, depth map and sketch as the local guidance, color histogram for low-level details, etc. Besides improving controllability, we confirm that Composer serves as a general framework and facilitates a wide range of classical generative tasks without retraining. Code and models will be made available.},
	urldate = {2023-07-13},
	publisher = {arXiv},
	author = {Huang, Lianghua and Chen, Di and Liu, Yu and Shen, Yujun and Zhao, Deli and Zhou, Jingren},
	month = feb,
	year = {2023},
	note = {arXiv:2302.09778 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@article{mou_dragondiffusion_nodate,
	title = {{DragonDiffusion}: {Enabling} {Drag}-style {Manipulation} on {Diffusion} {Models}},
	language = {en},
	author = {Mou, Chong and Wang, Xintao and Song, Jiechong and Shan, Ying and Zhang, Jian},
}

@article{shi_dragdiffusion_nodate,
	title = {{DragDiffusion}: {Harnessing} {Diffusion} {Models} for {Interactive} {Point}-based {Image} {Editing}},
	language = {en},
	author = {Shi, Yujun and Xue, Chuhui and Pan, Jiachun and Zhang, Wenqing and Tan, Vincent Y F and Bai, Song},
}

@misc{shi_dragdiffusion_2023-1,
	title = {{DragDiffusion}: {Harnessing} {Diffusion} {Models} for {Interactive} {Point}-based {Image} {Editing}},
	shorttitle = {{DragDiffusion}},
	url = {https://arxiv.org/abs/2306.14435v3},
	abstract = {Precise and controllable image editing is a challenging task that has attracted significant attention. Recently, DragGAN enables an interactive point-based image editing framework and achieves impressive editing results with pixel-level precision. However, since this method is based on generative adversarial networks (GAN), its generality is upper-bounded by the capacity of the pre-trained GAN models. In this work, we extend such an editing framework to diffusion models and propose DragDiffusion. By leveraging large-scale pretrained diffusion models, we greatly improve the applicability of interactive point-based editing in real world scenarios. While most existing diffusion-based image editing methods work on text embeddings, DragDiffusion optimizes the diffusion latent to achieve precise spatial control. Although diffusion models generate images in an iterative manner, we empirically show that optimizing diffusion latent at one single step suffices to generate coherent results, enabling DragDiffusion to complete high-quality editing efficiently. Extensive experiments across a wide range of challenging cases (e.g., multi-objects, diverse object categories, various styles, etc.) demonstrate the versatility and generality of DragDiffusion. Code: https://github.com/Yujun-Shi/DragDiffusion.},
	language = {en},
	urldate = {2023-07-11},
	journal = {arXiv.org},
	author = {Shi, Yujun and Xue, Chuhui and Pan, Jiachun and Zhang, Wenqing and Tan, Vincent Y. F. and Bai, Song},
	month = jun,
	year = {2023},
}

@misc{mou_dragondiffusion_2023-1,
	title = {{DragonDiffusion}: {Enabling} {Drag}-style {Manipulation} on {Diffusion} {Models}},
	shorttitle = {{DragonDiffusion}},
	url = {https://arxiv.org/abs/2307.02421v1},
	abstract = {Despite the ability of existing large-scale text-to-image (T2I) models to generate high-quality images from detailed textual descriptions, they often lack the ability to precisely edit the generated or real images. In this paper, we propose a novel image editing method, DragonDiffusion, enabling Drag-style manipulation on Diffusion models. Specifically, we construct classifier guidance based on the strong correspondence of intermediate features in the diffusion model. It can transform the editing signals into gradients via feature correspondence loss to modify the intermediate representation of the diffusion model. Based on this guidance strategy, we also build a multi-scale guidance to consider both semantic and geometric alignment. Moreover, a cross-branch self-attention is added to maintain the consistency between the original image and the editing result. Our method, through an efficient design, achieves various editing modes for the generated or real images, such as object moving, object resizing, object appearance replacement, and content dragging. It is worth noting that all editing and content preservation signals come from the image itself, and the model does not require fine-tuning or additional modules. Our source code will be available at https://github.com/MC-E/DragonDiffusion.},
	language = {en},
	urldate = {2023-07-11},
	journal = {arXiv.org},
	author = {Mou, Chong and Wang, Xintao and Song, Jiechong and Shan, Ying and Zhang, Jian},
	month = jul,
	year = {2023},
}

@article{hong_learning_2022,
	title = {Learning to {Compose} and {Reason} with {Language} {Tree} {Structures} for {Visual} {Grounding}},
	volume = {44},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/1906.01784},
	doi = {10.1109/TPAMI.2019.2911066},
	abstract = {Grounding natural language in images, such as localizing "the black dog on the left of the tree", is one of the core problems in artificial intelligence, as it needs to comprehend the fine-grained and compositional language space. However, existing solutions rely on the association between the holistic language features and visual features, while neglect the nature of compositional reasoning implied in the language. In this paper, we propose a natural language grounding model that can automatically compose a binary tree structure for parsing the language and then perform visual reasoning along the tree in a bottom-up fashion. We call our model RVG-TREE: Recursive Grounding Tree, which is inspired by the intuition that any language expression can be recursively decomposed into two constituent parts, and the grounding confidence score can be recursively accumulated by calculating their grounding scores returned by sub-trees. RVG-TREE can be trained end-to-end by using the Straight-Through Gumbel-Softmax estimator that allows the gradients from the continuous score functions passing through the discrete tree construction. Experiments on several benchmarks show that our model achieves the state-of-the-art performance with more explainable reasoning.},
	number = {2},
	urldate = {2023-07-01},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Hong, Richang and Liu, Daqing and Mo, Xiaoyu and He, Xiangnan and Zhang, Hanwang},
	month = feb,
	year = {2022},
	note = {arXiv:1906.01784 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {684--696},
}

@misc{chang_muse_2023,
	title = {Muse: {Text}-{To}-{Image} {Generation} via {Masked} {Generative} {Transformers}},
	shorttitle = {Muse},
	url = {http://arxiv.org/abs/2301.00704},
	abstract = {We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Chang, Huiwen and Zhang, Han and Barber, Jarred and Maschinot, A. J. and Lezama, Jose and Jiang, Lu and Yang, Ming-Hsuan and Murphy, Kevin and Freeman, William T. and Rubinstein, Michael and Li, Yuanzhen and Krishnan, Dilip},
	month = jan,
	year = {2023},
	note = {arXiv:2301.00704 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{liu_visual_2023-1,
	title = {Visual {Instruction} {Tuning}},
	url = {http://arxiv.org/abs/2304.08485},
	abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08485 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{mokady_null-text_nodate,
	title = {{NULL}-{Text} {Inversion} for {Editing} {Real} {Images} {Using} {Guided} {Diffusion} {Models}},
	abstract = {Recent large-scale text-guided diffusion models provide powerful image generation capabilities. Currently, a massive effort is given to enable the modification of these images using text only as means to offer intuitive and versatile editing tools. To edit a real image using these state-of-theart tools, one must first invert the image with a meaningful text prompt into the pretrained model’s domain. In this paper, we introduce an accurate inversion technique and thus facilitate an intuitive text-based modification of the image. Our proposed inversion consists of two key novel components: (i) Pivotal inversion for diffusion models. While current methods aim at mapping random noise samples to a single input image, we use a single pivotal noise vector for each timestamp and optimize around it. We demonstrate that a direct DDIM inversion is inadequate on its own, but does provide a rather good anchor for our optimization. (ii) Null-text optimization, where we only modify the unconditional textual embedding that is used for classifier-free guidance, rather than the input text embedding. This allows for keeping both the model weights and the conditional embedding intact and hence enables applying prompt-based editing while avoiding the cumbersome tuning of the model’s weights. Our null-text inversion, based on the publicly available Stable Diffusion model, is extensively evaluated on a variety of images and various prompt editing, showing high-fidelity editing of real images.},
	language = {en},
	author = {Mokady, Ron and Hertz, Amir and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
}

@misc{noauthor_gligenopen-set_nodate,
	title = {{GLIGEN}:{Open}-{Set} {Grounded} {Text}-to-{Image} {Generation}.},
	url = {https://gligen.github.io/},
	urldate = {2023-06-30},
}

@misc{chen_uniter_2020,
	title = {{UNITER}: {UNiversal} {Image}-{TExt} {Representation} {Learning}},
	shorttitle = {{UNITER}},
	url = {http://arxiv.org/abs/1909.11740},
	abstract = {Joint image-text embedding is the bedrock for most Vision-and-Language (V+L) tasks, where multimodality inputs are simultaneously processed for joint visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream V+L tasks with joint multimodal embeddings. We design four pre-training tasks: Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). Different from previous work that applies joint random masking to both modalities, we use conditional masking on pre-training tasks (i.e., masked language/region modeling is conditioned on full observation of image/text). In addition to ITM for global image-text alignment, we also propose WRA via the use of Optimal Transport (OT) to explicitly encourage fine-grained alignment between words and image regions during pre-training. Comprehensive analysis shows that both conditional masking and OT-based WRA contribute to better pre-training. We also conduct a thorough ablation study to find an optimal combination of pre-training tasks. Extensive experiments show that UNITER achieves new state of the art across six V+L tasks (over nine datasets), including Visual Question Answering, Image-Text Retrieval, Referring Expression Comprehension, Visual Commonsense Reasoning, Visual Entailment, and NLVR\${\textasciicircum}2\$. Code is available at https://github.com/ChenRocks/UNITER.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and Kholy, Ahmed El and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
	month = jul,
	year = {2020},
	note = {arXiv:1909.11740 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{kim_diffusionclip_2022,
	title = {{DiffusionCLIP}: {Text}-{Guided} {Diffusion} {Models} for {Robust} {Image} {Manipulation}},
	shorttitle = {{DiffusionCLIP}},
	url = {http://arxiv.org/abs/2110.02711},
	abstract = {Recently, GAN inversion methods combined with Contrastive Language-Image Pretraining (CLIP) enables zero-shot image manipulation guided by text prompts. However, their applications to diverse real images are still difficult due to the limited GAN inversion capability. Specifically, these approaches often have difficulties in reconstructing images with novel poses, views, and highly variable contents compared to the training data, altering object identity, or producing unwanted image artifacts. To mitigate these problems and enable faithful manipulation of real images, we propose a novel method, dubbed DiffusionCLIP, that performs text-driven image manipulation using diffusion models. Based on full inversion capability and high-quality image generation power of recent diffusion models, our method performs zero-shot image manipulation successfully even between unseen domains and takes another step towards general application by manipulating images from a widely varying ImageNet dataset. Furthermore, we propose a novel noise combination method that allows straightforward multi-attribute manipulation. Extensive experiments and human evaluation confirmed robust and superior manipulation performance of our methods compared to the existing baselines. Code is available at https://github.com/gwang-kim/DiffusionCLIP.git.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Kim, Gwanghyun and Kwon, Taesung and Ye, Jong Chul},
	month = aug,
	year = {2022},
	note = {arXiv:2110.02711 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{wu_unifying_2022,
	title = {Unifying {Diffusion} {Models}' {Latent} {Space}, with {Applications} to {CycleDiffusion} and {Guidance}},
	url = {http://arxiv.org/abs/2210.05559},
	doi = {10.48550/arXiv.2210.05559},
	abstract = {Diffusion models have achieved unprecedented performance in generative modeling. The commonly-adopted formulation of the latent code of diffusion models is a sequence of gradually denoised samples, as opposed to the simpler (e.g., Gaussian) latent space of GANs, VAEs, and normalizing flows. This paper provides an alternative, Gaussian formulation of the latent space of various diffusion models, as well as an invertible DPM-Encoder that maps images into the latent space. While our formulation is purely based on the definition of diffusion models, we demonstrate several intriguing consequences. (1) Empirically, we observe that a common latent space emerges from two diffusion models trained independently on related domains. In light of this finding, we propose CycleDiffusion, which uses DPM-Encoder for unpaired image-to-image translation. Furthermore, applying CycleDiffusion to text-to-image diffusion models, we show that large-scale text-to-image diffusion models can be used as zero-shot image-to-image editors. (2) One can guide pre-trained diffusion models and GANs by controlling the latent codes in a unified, plug-and-play formulation based on energy-based models. Using the CLIP model and a face recognition model as guidance, we demonstrate that diffusion models have better coverage of low-density sub-populations and individuals than GANs. The code is publicly available at https://github.com/ChenWu98/cycle-diffusion.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Wu, Chen Henry and De la Torre, Fernando},
	month = dec,
	year = {2022},
	note = {arXiv:2210.05559 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{vinker_concept_2023,
	title = {Concept {Decomposition} for {Visual} {Exploration} and {Inspiration}},
	url = {http://arxiv.org/abs/2305.18203},
	abstract = {A creative idea is often born from transforming, combining, and modifying ideas from existing visual examples capturing various concepts. However, one cannot simply copy the concept as a whole, and inspiration is achieved by examining certain aspects of the concept. Hence, it is often necessary to separate a concept into different aspects to provide new perspectives. In this paper, we propose a method to decompose a visual concept, represented as a set of images, into different visual aspects encoded in a hierarchical tree structure. We utilize large vision-language models and their rich latent space for concept decomposition and generation. Each node in the tree represents a sub-concept using a learned vector embedding injected into the latent space of a pretrained text-to-image model. We use a set of regularizations to guide the optimization of the embedding vectors encoded in the nodes to follow the hierarchical structure of the tree. Our method allows to explore and discover new concepts derived from the original one. The tree provides the possibility of endless visual sampling at each node, allowing the user to explore the hidden sub-concepts of the object of interest. The learned aspects in each node can be combined within and across trees to create new visual ideas, and can be used in natural language sentences to apply such aspects to new designs.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Vinker, Yael and Voynov, Andrey and Cohen-Or, Daniel and Shamir, Ariel},
	month = may,
	year = {2023},
	note = {arXiv:2305.18203 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{han_greedy_2021,
	address = {Montreal, QC, Canada},
	title = {Greedy {Gradient} {Ensemble} for {Robust} {Visual} {Question} {Answering}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9711436/},
	doi = {10.1109/ICCV48922.2021.00161},
	abstract = {Language bias is a critical issue in Visual Question Answering (VQA), where models often exploit dataset biases for the final decision without considering the image information. As a result, they suffer from performance drop on out-of-distribution data and inadequate visual explanation. Based on experimental analysis for existing robust VQA methods, we stress the language bias in VQA that comes from two aspects, i.e., distribution bias and shortcut bias. We further propose a new de-bias framework, Greedy Gradient Ensemble (GGE), which combines multiple biased models for unbiased base model learning. With the greedy strategy, GGE forces the biased models to over-fit the biased data distribution in priority, thus makes the base model pay more attention to examples that are hard to solve by biased models. The experiments demonstrate that our method makes better use of visual information and achieves state-of-the-art performance on diagnosing dataset VQACP without using extra annotations.},
	language = {en},
	urldate = {2023-06-28},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Han, Xinzhe and Wang, Shuhui and Su, Chi and Huang, Qingming and Tian, Qi},
	month = oct,
	year = {2021},
	pages = {1564--1573},
}

@misc{noauthor_notion_nodate,
	title = {Notion – {The} all-in-one workspace for your notes, tasks, wikis, and databases.},
	url = {https://www.notion.so},
	abstract = {A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team},
	urldate = {2023-06-28},
	journal = {Notion},
}

@inproceedings{schramowski_safe_2023-1,
	title = {Safe {Latent} {Diffusion}: {Mitigating} {Inappropriate} {Degeneration} in {Diffusion} {Models}},
	shorttitle = {Safe {Latent} {Diffusion}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Schramowski_Safe_Latent_Diffusion_Mitigating_Inappropriate_Degeneration_in_Diffusion_Models_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-27},
	author = {Schramowski, Patrick and Brack, Manuel and Deiseroth, Björn and Kersting, Kristian},
	year = {2023},
	pages = {22522--22531},
}

@article{johnson_coupling-regeneration_1998,
	title = {A {Coupling}-{Regeneration} {Scheme} for {Diagnosing} {Convergence} in {Markov} {Chain} {Monte} {Carlo} {Algorithms}},
	volume = {93},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2669620},
	doi = {10.2307/2669620},
	abstract = {Here I propose a convergence diagnostic for Markov chain Monte Carlo (MCMC) algorithms based on couplings of a Markov chain with an auxiliary chain that is periodically restarted from a fixed parameter value. The diagnostic provides a mechanism for estimating the specific constants governing the rate of convergence of geometrically and uniformly ergodic chains, and provides a lower bound on the effective sample size of a MCMC run. It also provides a simple procedure for obtaining what is, with high probability, an independent sample from the stationary distribution.},
	number = {441},
	urldate = {2023-06-27},
	journal = {Journal of the American Statistical Association},
	author = {Johnson, Valen E.},
	year = {1998},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {238--248},
}

@article{feng_training-free_2023,
	title = {{TRAINING}-{FREE} {STRUCTURED} {DIFFUSION} {GUIDANCE} {FOR} {COMPOSITIONAL} {TEXT}-{TO}-{IMAGE} {SYNTHESIS}},
	abstract = {Large-scale diffusion models have achieved state-of-the-art results on text-to-image synthesis (T2I) tasks. Despite their ability to generate high-quality yet creative images, we observe that attribution-binding and compositional capabilities are still considered major challenging issues, especially when involving multiple objects. Attribute-binding requires the model to associate objects with the correct attribute descriptions, and compositional skills require the model to combine and generate multiple concepts into a single image. In this work, we improve these two aspects of T2I models to achieve more accurate image compositions. To do this, we incorporate linguistic structures with the diffusion guidance process based on the controllable properties of manipulating cross-attention layers in diffusion-based T2I models. We observe that keys and values in cross-attention layers have strong semantic meanings associated with object layouts and content. Therefore, by manipulating the cross-attention representations based on linguistic insights, we can better preserve the compositional semantics in the generated image. Built upon Stable Diffusion, a SOTA T2I model, our structured cross-attention design is efficient that requires no additional training samples. We achieve better compositional skills in qualitative and quantitative results, leading to a significant 5-8\% advantage in head-to-head user comparison studies. Lastly, we conduct an in-depth analysis to reveal potential causes of incorrect image compositions and justify the properties of cross-attention layers in the generation process.},
	language = {en},
	author = {Feng, Weixi and He, Xuehai and Fu, Tsu-jui and Jampani, Varun and Akula, Arjun and Narayana, Pradyumna and Basu, Sugato and Wang, Xin Eric and Wang, William Yang},
	year = {2023},
}

@misc{chefer_attend-and-excite_2023-1,
	title = {Attend-and-{Excite}: {Attention}-{Based} {Semantic} {Guidance} for {Text}-to-{Image} {Diffusion} {Models}},
	shorttitle = {Attend-and-{Excite}},
	url = {http://arxiv.org/abs/2301.13826},
	abstract = {Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g., colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen - or excite - their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Chefer, Hila and Alaluf, Yuval and Vinker, Yael and Wolf, Lior and Cohen-Or, Daniel},
	month = may,
	year = {2023},
	note = {arXiv:2301.13826 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{wang_image_2022,
	title = {Image as a {Foreign} {Language}: {BEiT} {Pretraining} for {All} {Vision} and {Vision}-{Language} {Tasks}},
	shorttitle = {Image as a {Foreign} {Language}},
	url = {http://arxiv.org/abs/2208.10442},
	abstract = {A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked "language" modeling on images (Imglish), texts (English), and image-text pairs ("parallel sentences") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and Wei, Furu},
	month = aug,
	year = {2022},
	note = {arXiv:2208.10442 [cs]
version: 2},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xie_visual_2019,
	title = {Visual {Entailment}: {A} {Novel} {Task} for {Fine}-{Grained} {Image} {Understanding}},
	shorttitle = {Visual {Entailment}},
	url = {http://arxiv.org/abs/1901.06706},
	abstract = {Existing visual reasoning datasets such as Visual Question Answering (VQA), often suffer from biases conditioned on the question, image or answer distributions. The recently proposed CLEVR dataset addresses these limitations and requires fine-grained reasoning but the dataset is synthetic and consists of similar objects and sentence structures across the dataset. In this paper, we introduce a new inference task, Visual Entailment (VE) - consisting of image-sentence pairs whereby a premise is defined by an image, rather than a natural language sentence as in traditional Textual Entailment tasks. The goal of a trained VE model is to predict whether the image semantically entails the text. To realize this task, we build a dataset SNLI-VE based on the Stanford Natural Language Inference corpus and Flickr30k dataset. We evaluate various existing VQA baselines and build a model called Explainable Visual Entailment (EVE) system to address the VE task. EVE achieves up to 71\% accuracy and outperforms several other state-of-the-art VQA based models. Finally, we demonstrate the explainability of EVE through cross-modal attention visualizations. The SNLI-VE dataset is publicly available at https://github.com/ necla-ml/SNLI-VE.},
	urldate = {2023-06-26},
	publisher = {arXiv},
	author = {Xie, Ning and Lai, Farley and Doran, Derek and Kadav, Asim},
	month = jan,
	year = {2019},
	note = {arXiv:1901.06706 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{agarwal_estimating_2022,
	title = {Estimating {Example} {Difficulty} {Using} {Variance} of {Gradients}},
	url = {http://arxiv.org/abs/2008.11600},
	abstract = {In machine learning, a question of great interest is understanding what examples are challenging for a model to classify. Identifying atypical examples ensures the safe deployment of models, isolates samples that require further human inspection and provides interpretability into model behavior. In this work, we propose Variance of Gradients (VoG) as a valuable and efficient metric to rank data by difficulty and to surface a tractable subset of the most challenging examples for human-in-the-loop auditing. We show that data points with high VoG scores are far more difficult for the model to learn and over-index on corrupted or memorized examples. Further, restricting the evaluation to the test set instances with the lowest VoG improves the model's generalization performance. Finally, we show that VoG is a valuable and efficient ranking for out-of-distribution detection.},
	urldate = {2023-06-26},
	publisher = {arXiv},
	author = {Agarwal, Chirag and D'souza, Daniel and Hooker, Sara},
	month = jun,
	year = {2022},
	note = {arXiv:2008.11600 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{song_clip_2022,
	title = {{CLIP} {Models} are {Few}-shot {Learners}: {Empirical} {Studies} on {VQA} and {Visual} {Entailment}},
	shorttitle = {{CLIP} {Models} are {Few}-shot {Learners}},
	url = {http://arxiv.org/abs/2203.07190},
	abstract = {CLIP has shown a remarkable zero-shot capability on a wide range of vision tasks. Previously, CLIP is only regarded as a powerful visual encoder. However, after being pre-trained by language supervision from a large amount of image-caption pairs, CLIP itself should also have acquired some few-shot abilities for vision-language tasks. In this work, we empirically show that CLIP can be a strong vision-language few-shot learner by leveraging the power of language. We first evaluate CLIP's zero-shot performance on a typical visual question answering task and demonstrate a zero-shot cross-modality transfer capability of CLIP on the visual entailment task. Then we propose a parameter-efficient fine-tuning strategy to boost the few-shot performance on the vqa task. We achieve competitive zero/few-shot results on the visual question answering and visual entailment tasks without introducing any additional pre-training procedure.},
	urldate = {2023-06-26},
	publisher = {arXiv},
	author = {Song, Haoyu and Dong, Li and Zhang, Wei-Nan and Liu, Ting and Wei, Furu},
	month = mar,
	year = {2022},
	note = {arXiv:2203.07190 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhang_domain-robust_2021,
	address = {Nashville, TN, USA},
	title = {Domain-robust {VQA} with diverse datasets and methods but no target labels},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577857/},
	doi = {10.1109/CVPR46437.2021.00697},
	abstract = {The observation that computer vision methods overﬁt to dataset speciﬁcs has inspired diverse attempts to make object recognition models robust to domain shifts. However, similar work on domain-robust visual question answering methods is very limited. Domain adaptation for VQA differs from adaptation for object recognition due to additional complexity: VQA models handle multimodal inputs, methods contain multiple steps with diverse modules resulting in complex optimization, and answer spaces in different datasets are vastly different. To tackle these challenges, we ﬁrst quantify domain shifts between popular VQA datasets, in both visual and textual space. To disentangle shifts between datasets arising from different modalities, we also construct synthetic shifts in the image and question domains separately. Second, we test the robustness of different families of VQA methods (classic two-stream, transformer, and neuro-symbolic methods) to these shifts. Third, we test the applicability of existing domain adaptation methods and devise a new one to bridge VQA domain gaps, adjusted to speciﬁc VQA models. To emulate the setting of real-world generalization, we focus on unsupervised domain adaptation and the open-ended classiﬁcation task formulation.},
	language = {en},
	urldate = {2023-06-26},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhang, Mingda and Maidment, Tristan and Diab, Ahmad and Kovashka, Adriana and Hwa, Rebecca},
	month = jun,
	year = {2021},
	pages = {7042--7052},
}

@article{stoica_zipit_nodate,
	title = {{ZipIt}! {Merging} {Models} from {Different} {Tasks} without {Training}},
	abstract = {Typical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difﬁcult problem of combining completely distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then adds them together. While this works for models trained on the same task, we ﬁnd that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce “ZipIt!”, a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren’t shared between models, we expand the model merging problem to additionally allow for merging features within each model by deﬁning a general “zip” operation. Second, we add support for partially zipping the models up until a speciﬁed layer, naturally creating a multi-head model. We ﬁnd that these two changes combined account for a staggering 20-60\% improvement over prior work, making the merging of models trained on disjoint tasks feasible.},
	language = {en},
	author = {Stoica, George and Bolya, Daniel and Bjorner, Jakob and Hearn, Taylor and Hoffman, Judy},
}

@misc{vig_multiscale_2019,
	title = {A {Multiscale} {Visualization} of {Attention} in the {Transformer} {Model}},
	url = {http://arxiv.org/abs/1906.05714},
	abstract = {The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {Vig, Jesse},
	month = jun,
	year = {2019},
	note = {arXiv:1906.05714 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@article{vig_bertviz_2019,
	title = {{BERTVIZ}: {A} {TOOL} {FOR} {VISUALIZING} {MULTI}-{HEAD} {SELF}-{ATTENTION} {IN} {THE} {BERT} {MODEL}},
	abstract = {We introduce BertViz, an open-source tool for visualizing self-attention in the BERT language representation model. BertViz extends earlier work by visualizing attention at three levels of granularity: the attention-head level, the model level, and the neuron level. We describe how each of these views can help to interpret the BERT model, and we demonstrate a debugging use case.},
	language = {en},
	author = {Vig, Jesse},
	year = {2019},
}

@misc{cho_visual_2023,
	title = {Visual {Programming} for {Text}-to-{Image} {Generation} and {Evaluation}},
	url = {http://arxiv.org/abs/2305.15328},
	abstract = {As large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can only handle predefined object classes. We demonstrate that our VPGen has improved control in counts/spatial relations/scales of objects than state-of-the-art T2I generation models. Second, we introduce VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming. Unlike previous T2I evaluations with a single scoring model that is accurate in some skills but unreliable in others, VPEval produces evaluation programs that invoke a set of visual modules that are experts in different skills, and also provides visual+textual explanations of the evaluation results. Our analysis shows VPEval provides a more human-correlated evaluation for skill-specific and open-ended prompts than widely used single model-based evaluation. We hope our work encourages future progress on interpretable/explainable generation and evaluation for T2I models. Website: https://vp-t2i.github.io},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Cho, Jaemin and Zala, Abhay and Bansal, Mohit},
	month = may,
	year = {2023},
	note = {arXiv:2305.15328 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{amini_hooked_2018,
	address = {Castiglione della Pescaia Grosseto Italy},
	title = {Hooked on data videos: assessing the effect of animation and pictographs on viewer engagement},
	isbn = {978-1-4503-5616-9},
	shorttitle = {Hooked on data videos},
	url = {https://dl.acm.org/doi/10.1145/3206505.3206552},
	doi = {10.1145/3206505.3206552},
	abstract = {Pictographic representations and animation techniques are commonly incorporated into narrative visualizations such as data videos. General belief is that these techniques may enhance the viewer experience, thus appealing to a broad audience and enticing the viewer to consume the entire video. However, no study has formally assessed the effect of these techniques on data insight communication and viewer engagement. In this paper, we first propose a scale-based questionnaire covering five factors of viewer engagement we identified from multiple application domains such as game design and marketing. We then validate this questionnaire through a crowdsourcing study on Amazon’s Mechanical Turk to assess the effect of animation and pictographs in data videos. Our results reveal that each technique has an effect on viewer engagement, impacting different factors. In addition, insights from these studies lead to design considerations for authoring engaging data videos.},
	language = {en},
	urldate = {2023-06-19},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Advanced} {Visual} {Interfaces}},
	publisher = {ACM},
	author = {Amini, Fereshteh and Riche, Nathalie Henry and Lee, Bongshin and Leboe-McGowan, Jason and Irani, Pourang},
	month = may,
	year = {2018},
	pages = {1--9},
}

@misc{noauthor_hooked_nodate,
	title = {Hooked on data videos {\textbar} {Proceedings} of the 2018 {International} {Conference} on {Advanced} {Visual} {Interfaces}},
	url = {https://dl.acm.org/doi/abs/10.1145/3206505.3206552},
	urldate = {2023-06-19},
}

@article{tversky_animation_2002,
	title = {Animation: can it facilitate?},
	volume = {57},
	issn = {1071-5819},
	shorttitle = {Animation},
	url = {https://www.sciencedirect.com/science/article/pii/S1071581902910177},
	doi = {10.1006/ijhc.2002.1017},
	abstract = {Graphics have been used since ancient times to portray things that are inherently spatiovisual, like maps and building plans. More recently, graphics have been used to portray things that are metaphorically spatiovisual, like graphs and organizational charts. The assumption is that graphics can facilitate comprehension, learning, memory, communication and inference. Assumptions aside, research on static graphics has shown that only carefully designed and appropriate graphics prove to be beneficial for conveying complex systems. Effective graphics conform to the Congruence Principle according to which the content and format of the graphic should correspond to the content and format of the concepts to be conveyed. From this, it follows that animated graphics should be effective in portraying change over time. Yet the research on the efficacy of animated over static graphics is not encouraging. In cases where animated graphics seem superior to static ones, scrutiny reveals lack of equivalence between animated and static graphics in content or procedures; the animated graphics convey more information or involve interactivity. Animations of events may be ineffective because animations violate the second principle of good graphics, the Apprehension Principle, according to which graphics should be accurately perceived and appropriately conceived. Animations are often too complex or too fast to be accurately perceived. Moreover, many continuous events are conceived of as sequences of discrete steps. Judicious use of interactivity may overcome both these disadvantages. Animations may be more effective than comparable static graphics in situations other than conveying complex systems, for example, for real time reorientations in time and space.},
	language = {en},
	number = {4},
	urldate = {2023-06-19},
	journal = {International Journal of Human-Computer Studies},
	author = {Tversky, BARBARA and Morrison, JULIE BAUER and Betrancourt, MIREILLE},
	month = oct,
	year = {2002},
	pages = {247--262},
}

@article{heer_animated_2007,
	title = {Animated {Transitions} in {Statistical} {Data} {Graphics}},
	volume = {13},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2007.70539},
	abstract = {In this paper we investigate the effectiveness of animated transitions between common statistical data graphics such as bar charts, pie charts, and scatter plots. We extend theoretical models of data graphics to include such transitions, introducing a taxonomy of transition types. We then propose design principles for creating effective transitions and illustrate the application of these principles in DynaVis, a visualization system featuring animated data graphics. Two controlled experiments were conducted to assess the efficacy of various transition types, finding that animated transitions can significantly improve graphical perception.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Heer, Jeffrey and Robertson, George},
	month = nov,
	year = {2007},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Animation, Collaboration, Data visualization, Drilling, Graphics, Guidelines, Information analysis, Marketing and sales, Scattering, Statistical data graphics, Taxonomy, animation, design, experiment, information visualization, transitions},
	pages = {1240--1247},
}

@article{kahng_gan_2019,
	title = {{GAN} {Lab}: {Understanding} {Complex} {Deep} {Generative} {Models} using {Interactive} {Visual} {Experimentation}},
	volume = {25},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {{GAN} {Lab}},
	url = {https://ieeexplore.ieee.org/document/8440049/},
	doi = {10.1109/TVCG.2018.2864500},
	abstract = {Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the ﬁrst interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process’s intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN’s structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.},
	language = {en},
	number = {1},
	urldate = {2023-06-16},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kahng, Minsuk and Thorat, Nikhil and Chau, Duen Horng Polo and Viegas, Fernanda B. and Wattenberg, Martin},
	month = jan,
	year = {2019},
	pages = {310--320},
}

@misc{noauthor_exploring_nodate,
	title = {Exploring the role of visualization and engagement in computer science education {\textbar} {ACM} {SIGCSE} {Bulletin}},
	url = {https://dl.acm.org/doi/10.1145/782941.782998},
	urldate = {2023-06-16},
}

@article{brown_techniques_1985,
	title = {Techniques for {Algorithm} {Animation}},
	volume = {2},
	issn = {0740-7459},
	doi = {10.1109/MS.1985.229778},
	language = {English},
	number = {1},
	journal = {IEEE Software},
	author = {Brown, M.H. and Sedgewick, R.},
	year = {1985},
	pages = {28--39},
}

@article{brown_techniques_1985-1,
	title = {Techniques for {Algorithm} {Animation}},
	volume = {2},
	issn = {0740-7459},
	url = {https://collaborate.princeton.edu/en/publications/techniques-for-algorithm-animation-2},
	doi = {10.1109/MS.1985.229778},
	language = {English (US)},
	number = {1},
	urldate = {2023-06-16},
	journal = {IEEE Software},
	author = {Brown, Marc H. and Sedgewick, Robert},
	month = jan,
	year = {1985},
	note = {Publisher: IEEE Computer Society},
	pages = {28--39},
}

@misc{liu_cross-modal_2023,
	title = {Cross-{Modal} {Causal} {Relational} {Reasoning} for {Event}-{Level} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2207.12647},
	abstract = {Existing visual question answering methods often suffer from cross-modal spurious correlations and oversimplified event-level reasoning processes that fail to capture event temporality, causality, and dynamics spanning over the video. In this work, to address the task of event-level visual question answering, we propose a framework for cross-modal causal relational reasoning. In particular, a set of causal intervention operations is introduced to discover the underlying causal structures across visual and linguistic modalities. Our framework, named Cross-Modal Causal RelatIonal Reasoning (CMCIR), involves three modules: i) Causality-aware Visual-Linguistic Reasoning (CVLR) module for collaboratively disentangling the visual and linguistic spurious correlations via front-door and back-door causal interventions; ii) Spatial-Temporal Transformer (STT) module for capturing the fine-grained interactions between visual and linguistic semantics; iii) Visual-Linguistic Feature Fusion (VLFF) module for learning the global semantic-aware visual-linguistic representations adaptively. Extensive experiments on four event-level datasets demonstrate the superiority of our CMCIR in discovering visual-linguistic causal structures and achieving robust event-level visual question answering. The datasets, code, and models are available at https://github.com/HCPLab-SYSU/CMCIR.},
	urldate = {2023-06-10},
	publisher = {arXiv},
	author = {Liu, Yang and Li, Guanbin and Lin, Liang},
	month = jun,
	year = {2023},
	note = {arXiv:2207.12647 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_mplug_2022,
	title = {{mPLUG}: {Effective} and {Efficient} {Vision}-{Language} {Learning} by {Cross}-modal {Skip}-connections},
	shorttitle = {{mPLUG}},
	url = {http://arxiv.org/abs/2205.12005},
	abstract = {Large-scale pretrained foundation models have been an emerging paradigm for building artificial intelligence (AI) systems, which can be quickly adapted to a wide range of downstream tasks. This paper presents mPLUG, a new vision-language foundation model for both cross-modal understanding and generation. Most existing pre-trained models suffer from the problems of low computational efficiency and information asymmetry brought by the long visual sequence in cross-modal alignment. To address these problems, mPLUG introduces an effective and efficient vision-language architecture with novel cross-modal skip-connections, which creates inter-layer shortcuts that skip a certain number of layers for time-consuming full self-attention on the vision side. mPLUG is pre-trained end-to-end on large-scale image-text pairs with both discriminative and generative objectives. It achieves state-of-the-art results on a wide range of vision-language downstream tasks, such as image captioning, image-text retrieval, visual grounding and visual question answering. mPLUG also demonstrates strong zero-shot transferability when directly transferred to multiple video-language tasks.},
	urldate = {2023-06-10},
	publisher = {arXiv},
	author = {Li, Chenliang and Xu, Haiyang and Tian, Junfeng and Wang, Wei and Yan, Ming and Bi, Bin and Ye, Jiabo and Chen, Hehong and Xu, Guohai and Cao, Zheng and Zhang, Ji and Huang, Songfang and Huang, Fei and Zhou, Jingren and Si, Luo},
	month = may,
	year = {2022},
	note = {arXiv:2205.12005 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hu_tifa_2023,
	title = {{TIFA}: {Accurate} and {Interpretable} {Text}-to-{Image} {Faithfulness} {Evaluation} with {Question} {Answering}},
	shorttitle = {{TIFA}},
	url = {http://arxiv.org/abs/2303.11897},
	abstract = {Despite thousands of researchers, engineers, and artists actively working on improving text-to-image generation models, systems often fail to produce images that accurately align with the text inputs. We introduce TIFA (Text-to-Image Faithfulness evaluation with question Answering), an automatic evaluation metric that measures the faithfulness of a generated image to its text input via visual question answering (VQA). Specifically, given a text input, we automatically generate several question-answer pairs using a language model. We calculate image faithfulness by checking whether existing VQA models can answer these questions using the generated image. TIFA is a reference-free metric that allows for fine-grained and interpretable evaluations of generated images. TIFA also has better correlations with human judgments than existing metrics. Based on this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse text inputs and 25K questions across 12 categories (object, counting, etc.). We present a comprehensive evaluation of existing text-to-image models using TIFA v1.0 and highlight the limitations and challenges of current models. For instance, we find that current text-to-image models, despite doing well on color and material, still struggle in counting, spatial relations, and composing multiple objects. We hope our benchmark will help carefully measure the research progress in text-to-image synthesis and provide valuable insights for further research.},
	language = {en},
	urldate = {2023-06-10},
	publisher = {arXiv},
	author = {Hu, Yushi and Liu, Benlin and Kasai, Jungo and Wang, Yizhong and Ostendorf, Mari and Krishna, Ranjay and Smith, Noah A.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11897 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{karthik_if_2023,
	title = {If at {First} {You} {Don}'t {Succeed}, {Try}, {Try} {Again}: {Faithful} {Diffusion}-based {Text}-to-{Image} {Generation} by {Selection}},
	shorttitle = {If at {First} {You} {Don}'t {Succeed}, {Try}, {Try} {Again}},
	url = {http://arxiv.org/abs/2305.13308},
	abstract = {Despite their impressive capabilities, diffusion-based text-to-image (T2I) models can lack faithfulness to the text prompt, where generated images may not contain all the mentioned objects, attributes or relations. To alleviate these issues, recent works proposed post-hoc methods to improve model faithfulness without costly retraining, by modifying how the model utilizes the input prompt. In this work, we take a step back and show that large T2I diffusion models are more faithful than usually assumed, and can generate images faithful to even complex prompts without the need to manipulate the generative process. Based on that, we show how faithfulness can be simply treated as a candidate selection problem instead, and introduce a straightforward pipeline that generates candidate images for a text prompt and picks the best one according to an automatic scoring system that can leverage already existing T2I evaluation metrics. Quantitative comparisons alongside user studies on diverse benchmarks show consistently improved faithfulness over post-hoc enhancement methods, with comparable or lower computational cost. Code is available at https://github.com/ExplainableML/ImageSelect.},
	language = {en},
	urldate = {2023-06-10},
	publisher = {arXiv},
	author = {Karthik, Shyamgopal and Roth, Karsten and Mancini, Massimiliano and Akata, Zeynep},
	month = may,
	year = {2023},
	note = {arXiv:2305.13308 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cho_visual_2023-1,
	title = {Visual {Programming} for {Text}-to-{Image} {Generation} and {Evaluation}},
	url = {http://arxiv.org/abs/2305.15328},
	doi = {10.48550/arXiv.2305.15328},
	abstract = {As large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can only handle predefined object classes. We demonstrate that our VPGen has improved control in counts/spatial relations/scales of objects than state-of-the-art T2I generation models. Second, we introduce VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming. Unlike previous T2I evaluations with a single scoring model that is accurate in some skills but unreliable in others, VPEval produces evaluation programs that invoke a set of visual modules that are experts in different skills, and also provides visual+textual explanations of the evaluation results. Our analysis shows VPEval provides a more human-correlated evaluation for skill-specific and open-ended prompts than widely used single model-based evaluation. We hope our work encourages future progress on interpretable/explainable generation and evaluation for T2I models. Website: https://vp-t2i.github.io},
	urldate = {2023-06-08},
	publisher = {arXiv},
	author = {Cho, Jaemin and Zala, Abhay and Bansal, Mohit},
	month = may,
	year = {2023},
	note = {arXiv:2305.15328 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{hu_tifa_2023-1,
	title = {{TIFA}: {Accurate} and {Interpretable} {Text}-to-{Image} {Faithfulness} {Evaluation} with {Question} {Answering}},
	shorttitle = {{TIFA}},
	url = {http://arxiv.org/abs/2303.11897},
	doi = {10.48550/arXiv.2303.11897},
	abstract = {Despite thousands of researchers, engineers, and artists actively working on improving text-to-image generation models, systems often fail to produce images that accurately align with the text inputs. We introduce TIFA (Text-to-Image Faithfulness evaluation with question Answering), an automatic evaluation metric that measures the faithfulness of a generated image to its text input via visual question answering (VQA). Specifically, given a text input, we automatically generate several question-answer pairs using a language model. We calculate image faithfulness by checking whether existing VQA models can answer these questions using the generated image. TIFA is a reference-free metric that allows for fine-grained and interpretable evaluations of generated images. TIFA also has better correlations with human judgments than existing metrics. Based on this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse text inputs and 25K questions across 12 categories (object, counting, etc.). We present a comprehensive evaluation of existing text-to-image models using TIFA v1.0 and highlight the limitations and challenges of current models. For instance, we find that current text-to-image models, despite doing well on color and material, still struggle in counting, spatial relations, and composing multiple objects. We hope our benchmark will help carefully measure the research progress in text-to-image synthesis and provide valuable insights for further research.},
	urldate = {2023-06-08},
	publisher = {arXiv},
	author = {Hu, Yushi and Liu, Benlin and Kasai, Jungo and Wang, Yizhong and Ostendorf, Mari and Krishna, Ranjay and Smith, Noah A.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11897 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lian_llm-grounded_2023,
	title = {{LLM}-grounded {Diffusion}: {Enhancing} {Prompt} {Understanding} of {Text}-to-{Image} {Diffusion} {Models} with {Large} {Language} {Models}},
	shorttitle = {{LLM}-grounded {Diffusion}},
	url = {http://arxiv.org/abs/2305.13655},
	abstract = {Recent advancements in text-to-image generation with diffusion models have yielded remarkable results synthesizing highly realistic and diverse images. However, these models still encounter difficulties when generating images from prompts that demand spatial or common sense reasoning. We propose to equip diffusion models with enhanced reasoning capabilities by using off-the-shelf pretrained large language models (LLMs) in a novel two-stage generation process. First, we adapt an LLM to be a text-guided layout generator through in-context learning. When provided with an image prompt, an LLM outputs a scene layout in the form of bounding boxes along with corresponding individual descriptions. Second, we steer a diffusion model with a novel controller to generate images conditioned on the layout. Both stages utilize frozen pretrained models without any LLM or diffusion model parameter optimization. We validate the superiority of our design by demonstrating its ability to outperform the base diffusion model in accurately generating images according to prompts that necessitate both language and spatial reasoning. Additionally, our method naturally allows dialog-based scene specification and is able to handle prompts in a language that is not well-supported by the underlying diffusion model.},
	urldate = {2023-06-08},
	publisher = {arXiv},
	author = {Lian, Long and Li, Boyi and Yala, Adam and Darrell, Trevor},
	month = may,
	year = {2023},
	note = {arXiv:2305.13655 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{seita_gpt-4_nodate,
	title = {{GPT}-4 + {Stable}-{Diffusion} = ?: {Enhancing} {Prompt} {Understanding} of {Text}-to-{Image} {Diffusion} {Models} with {Large} {Language} {Models}},
	shorttitle = {{GPT}-4 + {Stable}-{Diffusion} = ?},
	url = {http://bair.berkeley.edu/blog/2023/05/23/lmd/},
	abstract = {The BAIR Blog},
	urldate = {2023-06-08},
	journal = {The Berkeley Artificial Intelligence Research Blog},
	author = {Seita, Daniel and Darrell, Boyi Li, Adam Yala, Trevor, Long Lian},
}

@misc{gavrikov_visualkeras_2020,
	title = {visualkeras},
	copyright = {MIT},
	url = {https://github.com/paulgavrikov/visualkeras},
	abstract = {Visualkeras is a Python package to help visualize Keras (either standalone or included in TensorFlow) neural network architectures. It allows easy styling to fit most needs. This module supports layered style architecture generation which is great for CNNs (Convolutional Neural Networks), and a graph style architecture, which works great for most models including plain feed-forward networks.},
	urldate = {2023-06-08},
	author = {Gavrikov, Paul},
	year = {2020},
	note = {original-date: 2020-09-25T10:11:42Z},
}

@article{cass_top_2021,
	title = {Top {Programming} {Languages}: {Our} {Eighth} {Annual} {Probe} into {What}'s {Hot} and {Not}},
	volume = {58},
	issn = {1939-9340},
	shorttitle = {Top {Programming} {Languages}},
	doi = {10.1109/MSPEC.2021.9563957},
	abstract = {Learn Python. That's the biggest takeaway we can give you from its continued dominance of IEEE Spectrum's annual online interactive rankings of the top programming languages. You don't have to become a dyed-in-the-wool Pythonista, but learning the language well enough to use one of the vast number of libraries written for it is probably worth your time.},
	number = {10},
	journal = {IEEE Spectrum},
	author = {Cass, Stephen},
	month = oct,
	year = {2021},
	note = {Conference Name: IEEE Spectrum},
	keywords = {Libraries, Python},
	pages = {17--17},
}

@misc{httpswwwfacebookcom48576411181_top_nodate,
	title = {Top {Programming} {Languages} 2022 - {IEEE} {Spectrum}},
	url = {https://spectrum.ieee.org/top-programming-languages-2022},
	abstract = {Python’s still No. 1, but employers love to see SQL skills},
	language = {en},
	urldate = {2023-06-08},
	author = {https://www.facebook.com/48576411181},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	doi = {10.48550/arXiv.1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2023-06-08},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {arXiv:1912.01703 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
}

@misc{shi_instantbooth_2023,
	title = {{InstantBooth}: {Personalized} {Text}-to-{Image} {Generation} without {Test}-{Time} {Finetuning}},
	shorttitle = {{InstantBooth}},
	url = {http://arxiv.org/abs/2304.03411},
	abstract = {Recent advances in personalized image generation allow a pre-trained text-to-image model to learn a new concept from a set of images. However, existing personalization approaches usually require heavy test-time finetuning for each concept, which is time-consuming and difficult to scale. We propose InstantBooth, a novel approach built upon pre-trained text-to-image models that enables instant text-guided image personalization without any test-time finetuning. We achieve this with several major components. First, we learn the general concept of the input images by converting them to a textual token with a learnable image encoder. Second, to keep the fine details of the identity, we learn rich visual feature representation by introducing a few adapter layers to the pre-trained model. We train our components only on text-image pairs without using paired images of the same concept. Compared to test-time finetuning-based methods like DreamBooth and Textual-Inversion, our model can generate competitive results on unseen concepts concerning language-image alignment, image fidelity, and identity preservation while being 100 times faster.},
	urldate = {2023-06-08},
	publisher = {arXiv},
	author = {Shi, Jing and Xiong, Wei and Lin, Zhe and Jung, Hyun Joon},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03411 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{thrush_winoground_2022,
	title = {Winoground: {Probing} {Vision} and {Language} {Models} for {Visio}-{Linguistic} {Compositionality}},
	shorttitle = {Winoground},
	url = {http://arxiv.org/abs/2204.03162},
	abstract = {We present a novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which we call Winoground. Given two images and two captions, the goal is to match them correctly - but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set of fine-grained tags to assist in analyzing model performance. We probe a diverse range of state-of-the-art vision and language models and find that, surprisingly, none of them do much better than chance. Evidently, these models are not as skilled at visio-linguistic compositional reasoning as we might have hoped. We perform an extensive analysis to obtain insights into how future work might try to mitigate these models' shortcomings. We aim for Winoground to serve as a useful evaluation set for advancing the state of the art and driving further progress in the field. The dataset is available at https://huggingface.co/datasets/facebook/winoground.},
	urldate = {2023-06-08},
	publisher = {arXiv},
	author = {Thrush, Tristan and Jiang, Ryan and Bartolo, Max and Singh, Amanpreet and Williams, Adina and Kiela, Douwe and Ross, Candace},
	month = apr,
	year = {2022},
	note = {arXiv:2204.03162 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_diffusiondet_2022,
	title = {{DiffusionDet}: {Diffusion} {Model} for {Object} {Detection}},
	shorttitle = {{DiffusionDet}},
	url = {http://arxiv.org/abs/2211.09788},
	abstract = {We propose DiffusionDet, a new framework that formulates object detection as a denoising diffusion process from noisy boxes to object boxes. During training stage, object boxes diffuse from ground-truth boxes to random distribution, and the model learns to reverse this noising process. In inference, the model refines a set of randomly generated boxes to the output results in a progressive way. The extensive evaluations on the standard benchmarks, including MS-COCO and LVIS, show that DiffusionDet achieves favorable performance compared to previous well-established detectors. Our work brings two important findings in object detection. First, random boxes, although drastically different from pre-defined anchors or learned queries, are also effective object candidates. Second, object detection, one of the representative perception tasks, can be solved by a generative way. Our code is available at https://github.com/ShoufaChen/DiffusionDet.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Chen, Shoufa and Sun, Peize and Song, Yibing and Luo, Ping},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09788 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhu_relclip_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{RelCLIP}: {Adapting} {Language}-{Image} {Pretraining} for {Visual} {Relationship} {Detection} via {Relational} {Contrastive} {Learning}},
	shorttitle = {{RelCLIP}},
	url = {https://aclanthology.org/2022.emnlp-main.317},
	abstract = {Conventional visual relationship detection models only use the numeric ids of relation labels for training, but ignore the semantic correlation between the labels, which leads to severe training biases and harms the generalization ability of representations. In this paper, we introduce compact language information of relation labels for regularizing the representation learning of visual relations. Specifically, we propose a simple yet effective visual Relationship prediction framework that transfers natural language knowledge learned from Contrastive Language-Image Pre-training (CLIP) models to enhance the relationship prediction, termed RelCLIP. Benefiting from the powerful visual-semantic alignment ability of CLIP at image level, we introduce a novel Relational Contrastive Learning (RCL) approach which explores relation-level visual-semantic alignment via learning to match cross-modal relational embeddings. By collaboratively learning the semantic coherence and discrepancy from relation triplets, the model can generate more discriminative and robust representations. Experimental results on the Visual Genome dataset show that RelCLIP achieves significant improvements over strong baselines under full (provide accurate labels) and distant supervision (provide noise labels), demonstrating its powerful generalization ability in learning relationship representations. Code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/RelCLIP.},
	urldate = {2023-06-07},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Yi and Zhu, Zhaoqing and Lin, Bingqian and Liang, Xiaodan and Zhao, Feng and Liu, Jianzhuang},
	month = dec,
	year = {2022},
	pages = {4800--4810},
}

@misc{du_compositional_2020,
	title = {Compositional {Visual} {Generation} and {Inference} with {Energy} {Based} {Models}},
	url = {http://arxiv.org/abs/2004.06030},
	abstract = {A vital aspect of human intelligence is the ability to compose increasingly complex concepts out of simpler ideas, enabling both rapid learning and adaptation of knowledge. In this paper we show that energy-based models can exhibit this ability by directly combining probability distributions. Samples from the combined distribution correspond to compositions of concepts. For example, given a distribution for smiling faces, and another for male faces, we can combine them to generate smiling male faces. This allows us to generate natural images that simultaneously satisfy conjunctions, disjunctions, and negations of concepts. We evaluate compositional generation abilities of our model on the CelebA dataset of natural faces and synthetic 3D scene images. We also demonstrate other unique advantages of our model, such as the ability to continually learn and incorporate new concepts, or infer compositions of concept properties underlying an image.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Du, Yilun and Li, Shuang and Mordatch, Igor},
	month = dec,
	year = {2020},
	note = {arXiv:2004.06030 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{lee_codi_2023,
	title = {{CoDi}: {Co}-evolving {Contrastive} {Diffusion} {Models} for {Mixed}-type {Tabular} {Synthesis}},
	shorttitle = {{CoDi}},
	url = {http://arxiv.org/abs/2304.12654},
	abstract = {With growing attention to tabular data these days, the attempt to apply a synthetic table to various tasks has been expanded toward various scenarios. Owing to the recent advances in generative modeling, fake data generated by tabular data synthesis models become sophisticated and realistic. However, there still exists a difficulty in modeling discrete variables (columns) of tabular data. In this work, we propose to process continuous and discrete variables separately (but being conditioned on each other) by two diffusion models. The two diffusion models are co-evolved during training by reading conditions from each other. In order to further bind the diffusion models, moreover, we introduce a contrastive learning method with a negative sampling method. In our experiments with 11 real-world tabular datasets and 8 baseline methods, we prove the efficacy of the proposed method, called CoDi.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Lee, Chaejeong and Kim, Jayoung and Park, Noseong},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12654 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{mai_csp_2023,
	title = {{CSP}: {Self}-{Supervised} {Contrastive} {Spatial} {Pre}-{Training} for {Geospatial}-{Visual} {Representations}},
	shorttitle = {{CSP}},
	url = {https://arxiv.org/abs/2305.01118v2},
	abstract = {Geo-tagged images are publicly available in large quantities, whereas labels such as object classes are rather scarce and expensive to collect. Meanwhile, contrastive learning has achieved tremendous success in various natural image and language tasks with limited labeled data. However, existing methods fail to fully leverage geospatial information, which can be paramount to distinguishing objects that are visually similar. To directly leverage the abundant geospatial information associated with images in pre-training, fine-tuning, and inference stages, we present Contrastive Spatial Pre-Training (CSP), a self-supervised learning framework for geo-tagged images. We use a dual-encoder to separately encode the images and their corresponding geo-locations, and use contrastive objectives to learn effective location representations from images, which can be transferred to downstream supervised tasks such as image classification. Experiments show that CSP can improve model performance on both iNat2018 and fMoW datasets. Especially, on iNat2018, CSP significantly boosts the model performance with 10-34\% relative improvement with various labeled training data sampling ratios.},
	language = {en},
	urldate = {2023-06-07},
	journal = {arXiv.org},
	author = {Mai, Gengchen and Lao, Ni and He, Yutong and Song, Jiaming and Ermon, Stefano},
	month = may,
	year = {2023},
}

@misc{ma_contrastive_2021,
	title = {Contrastive {Learning} of {Global}-{Local} {Video} {Representations}},
	url = {http://arxiv.org/abs/2104.05418},
	abstract = {Contrastive learning has delivered impressive results for various tasks in the self-supervised regime. However, existing approaches optimize for learning representations specific to downstream scenarios, i.e., {\textbackslash}textit\{global\} representations suitable for tasks such as classification or {\textbackslash}textit\{local\} representations for tasks such as detection and localization. While they produce satisfactory results in the intended downstream scenarios, they often fail to generalize to tasks that they were not originally designed for. In this work, we propose to learn video representations that generalize to both the tasks which require global semantic information (e.g., classification) and the tasks that require local fine-grained spatio-temporal information (e.g., localization). We achieve this by optimizing two contrastive objectives that together encourage our model to learn global-local visual information given audio signals. We show that the two objectives mutually improve the generalizability of the learned global-local representations, significantly outperforming their disjointly learned counterparts. We demonstrate our approach on various tasks including action/sound classification, lip reading, deepfake detection, event and sound localization (https://github.com/yunyikristy/global{\textbackslash}\_local).},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Ma, Shuang and Zeng, Zhaoyang and McDuff, Daniel and Song, Yale},
	month = oct,
	year = {2021},
	note = {arXiv:2104.05418 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{agrawal_sort_2016,
	address = {Austin, Texas},
	title = {Sort {Story}: {Sorting} {Jumbled} {Images} and {Captions} into {Stories}},
	shorttitle = {Sort {Story}},
	url = {https://aclanthology.org/D16-1091},
	doi = {10.18653/v1/D16-1091},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Agrawal, Harsh and Chandrasekaran, Arjun and Batra, Dhruv and Parikh, Devi and Bansal, Mohit},
	month = nov,
	year = {2016},
	pages = {925--931},
}

@misc{iyyer_amazing_2017,
	title = {The {Amazing} {Mysteries} of the {Gutter}: {Drawing} {Inferences} {Between} {Panels} in {Comic} {Book} {Narratives}},
	shorttitle = {The {Amazing} {Mysteries} of the {Gutter}},
	url = {http://arxiv.org/abs/1611.05118},
	abstract = {Visual narrative is often a combination of explicit information and judicious omissions, relying on the viewer to supply missing details. In comics, most movements in time and space are hidden in the "gutters" between panels. To follow the story, readers logically connect panels together by inferring unseen actions through a process called "closure". While computers can now describe what is explicitly depicted in natural images, in this paper we examine whether they can understand the closure-driven narratives conveyed by stylized artwork and dialogue in comic book panels. We construct a dataset, COMICS, that consists of over 1.2 million panels (120 GB) paired with automatic textbox transcriptions. An in-depth analysis of COMICS demonstrates that neither text nor image alone can tell a comic book story, so a computer must understand both modalities to keep up with the plot. We introduce three cloze-style tasks that ask models to predict narrative and character-centric aspects of a panel given n preceding panels as context. Various deep neural architectures underperform human baselines on these tasks, suggesting that COMICS contains fundamental challenges for both vision and language.},
	urldate = {2023-06-06},
	publisher = {arXiv},
	author = {Iyyer, Mohit and Manjunatha, Varun and Guha, Anupam and Vyas, Yogarshi and Boyd-Graber, Jordan and Daumé III, Hal and Davis, Larry},
	month = may,
	year = {2017},
	note = {arXiv:1611.05118 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bordes_high_2022,
	title = {High {Fidelity} {Visualization} of {What} {Your} {Self}-{Supervised} {Representation} {Knows} {About}},
	url = {http://arxiv.org/abs/2112.09164},
	abstract = {Discovering what is learned by neural networks remains a challenge. In self-supervised learning, classification is the most common task used to evaluate how good a representation is. However, relying only on such downstream task can limit our understanding of what information is retained in the representation of a given input. In this work, we showcase the use of a Representation Conditional Diffusion Model (RCDM) to visualize in data space the representations learned by self-supervised models. The use of RCDM is motivated by its ability to generate high-quality samples -- on par with state-of-the-art generative models -- while ensuring that the representations of those samples are faithful i.e. close to the one used for conditioning. By using RCDM to analyze self-supervised models, we are able to clearly show visually that i) SSL (backbone) representation are not invariant to the data augmentations they were trained with -- thus debunking an often restated but mistaken belief; ii) SSL post-projector embeddings appear indeed invariant to these data augmentation, along with many other data symmetries; iii) SSL representations appear more robust to small adversarial perturbation of their inputs than representations trained in a supervised manner; and iv) that SSL-trained representations exhibit an inherent structure that can be explored thanks to RCDM visualization and enables image manipulation.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Bordes, Florian and Balestriero, Randall and Vincent, Pascal},
	month = aug,
	year = {2022},
	note = {arXiv:2112.09164 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{williams_hierarchical_2020,
	title = {Hierarchical {Quantized} {Autoencoders}},
	url = {https://arxiv.org/abs/2002.08111v3},
	abstract = {Despite progress in training neural networks for lossy image compression, current approaches fail to maintain both perceptual quality and abstract features at very low bitrates. Encouraged by recent success in learning discrete representations with Vector Quantized Variational Autoencoders (VQ-VAEs), we motivate the use of a hierarchy of VQ-VAEs to attain high factors of compression. We show that the combination of stochastic quantization and hierarchical latent structure aids likelihood-based image compression. This leads us to introduce a novel objective for training hierarchical VQ-VAEs. Our resulting scheme produces a Markovian series of latent variables that reconstruct images of high-perceptual quality which retain semantically meaningful features. We provide qualitative and quantitative evaluations on the CelebA and MNIST datasets.},
	language = {en},
	urldate = {2023-06-01},
	journal = {arXiv.org},
	author = {Williams, Will and Ringer, Sam and Ash, Tom and Hughes, John and MacLeod, David and Dougherty, Jamie},
	month = feb,
	year = {2020},
}

@misc{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1911.05722},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = mar,
	year = {2020},
	note = {arXiv:1911.05722 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{brack_sega_2023,
	title = {{SEGA}: {Instructing} {Diffusion} using {Semantic} {Dimensions}},
	shorttitle = {{SEGA}},
	url = {http://arxiv.org/abs/2301.12247},
	abstract = {Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on a variety of tasks and provide evidence for its versatility and flexibility.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Brack, Manuel and Friedrich, Felix and Hintersdorf, Dominik and Struppek, Lukas and Schramowski, Patrick and Kersting, Kristian},
	month = jan,
	year = {2023},
	note = {arXiv:2301.12247 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{liu_deepfashion_2016,
	address = {Las Vegas, NV, USA},
	title = {{DeepFashion}: {Powering} {Robust} {Clothes} {Recognition} and {Retrieval} with {Rich} {Annotations}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {{DeepFashion}},
	url = {http://ieeexplore.ieee.org/document/7780493/},
	doi = {10.1109/CVPR.2016.124},
	abstract = {Recent advances in clothes recognition have been driven by the construction of clothes datasets. Existing datasets are limited in the amount of annotations and are difﬁcult to cope with the various challenges in real-world applications. In this work, we introduce DeepFashion1, a large-scale clothes dataset with comprehensive annotations. It contains over 800,000 images, which are richly annotated with massive attributes, clothing landmarks, and correspondence of images taken under different scenarios including store, street snapshot, and consumer. Such rich annotations enable the development of powerful algorithms in clothes recognition and facilitating future researches. To demonstrate the advantages of DeepFashion, we propose a new deep model, namely FashionNet, which learns clothing features by jointly predicting clothing attributes and landmarks. The estimated landmarks are then employed to pool or gate the learned features. It is optimized in an iterative manner. Extensive experiments demonstrate the effectiveness of FashionNet and the usefulness of DeepFashion.},
	language = {en},
	urldate = {2023-05-22},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Liu, Ziwei and Luo, Ping and Qiu, Shi and Wang, Xiaogang and Tang, Xiaoou},
	month = jun,
	year = {2016},
	pages = {1096--1104},
}

@misc{wu_fashion_2020,
	title = {Fashion {IQ}: {A} {New} {Dataset} {Towards} {Retrieving} {Images} by {Natural} {Language} {Feedback}},
	shorttitle = {Fashion {IQ}},
	url = {http://arxiv.org/abs/1905.12794},
	abstract = {Conversational interfaces for the detail-oriented retail fashion domain are more natural, expressive, and user friendly than classical keyword-based search interfaces. In this paper, we introduce the Fashion IQ dataset to support and advance research on interactive fashion image retrieval. Fashion IQ is the first fashion dataset to provide human-generated captions that distinguish similar pairs of garment images together with side-information consisting of real-world product descriptions and derived visual attribute labels for these images. We provide a detailed analysis of the characteristics of the Fashion IQ data, and present a transformer-based user simulator and interactive image retriever that can seamlessly integrate visual attributes with image features, user feedback, and dialog history, leading to improved performance over the state of the art in dialog-based image retrieval. We believe that our dataset will encourage further work on developing more natural and real-world applicable conversational shopping assistants.},
	urldate = {2023-05-22},
	publisher = {arXiv},
	author = {Wu, Hui and Gao, Yupeng and Guo, Xiaoxiao and Al-Halah, Ziad and Rennie, Steven and Grauman, Kristen and Feris, Rogerio},
	month = nov,
	year = {2020},
	note = {arXiv:1905.12794 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{jiang_talk--edit_2021,
	title = {Talk-to-{Edit}: {Fine}-{Grained} {Facial} {Editing} via {Dialog}},
	shorttitle = {Talk-to-{Edit}},
	url = {http://arxiv.org/abs/2109.04425},
	abstract = {Facial editing is an important task in vision and graphics with numerous applications. However, existing works are incapable to deliver a continuous and fine-grained editing mode (e.g., editing a slightly smiling face to a big laughing one) with natural interactions with users. In this work, we propose Talk-to-Edit, an interactive facial editing framework that performs fine-grained attribute manipulation through dialog between the user and the system. Our key insight is to model a continual "semantic field" in the GAN latent space. 1) Unlike previous works that regard the editing as traversing straight lines in the latent space, here the fine-grained editing is formulated as finding a curving trajectory that respects fine-grained attribute landscape on the semantic field. 2) The curvature at each step is location-specific and determined by the input image as well as the users' language requests. 3) To engage the users in a meaningful dialog, our system generates language feedback by considering both the user request and the current state of the semantic field. We also contribute CelebA-Dialog, a visual-language facial editing dataset to facilitate large-scale study. Specifically, each image has manually annotated fine-grained attribute annotations as well as template-based textual descriptions in natural language. Extensive quantitative and qualitative experiments demonstrate the superiority of our framework in terms of 1) the smoothness of fine-grained editing, 2) the identity/attribute preservation, and 3) the visual photorealism and dialog fluency. Notably, user study validates that our overall system is consistently favored by around 80\% of the participants. Our project page is https://www.mmlab-ntu.com/project/talkedit/.},
	urldate = {2023-05-22},
	publisher = {arXiv},
	author = {Jiang, Yuming and Huang, Ziqi and Pan, Xingang and Loy, Chen Change and Liu, Ziwei},
	month = sep,
	year = {2021},
	note = {arXiv:2109.04425 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bansal_universal_2023,
	title = {Universal {Guidance} for {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2302.07121},
	abstract = {Typical diffusion models are trained to accept a particular form of conditioning, most commonly text, and cannot be conditioned on other modalities without retraining. In this work, we propose a universal guidance algorithm that enables diffusion models to be controlled by arbitrary guidance modalities without the need to retrain any use-specific components. We show that our algorithm successfully generates quality images with guidance functions including segmentation, face recognition, object detection, and classifier signals. Code is available at https://github.com/arpitbansal297/Universal-Guided-Diffusion.},
	urldate = {2023-05-22},
	publisher = {arXiv},
	author = {Bansal, Arpit and Chu, Hong-Min and Schwarzschild, Avi and Sengupta, Soumyadip and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07121 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{deng_arcface_2022,
	title = {{ArcFace}: {Additive} {Angular} {Margin} {Loss} for {Deep} {Face} {Recognition}},
	volume = {44},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {{ArcFace}},
	url = {http://arxiv.org/abs/1801.07698},
	doi = {10.1109/TPAMI.2021.3087709},
	abstract = {Recently, a popular line of research in face recognition is adopting margins in the well-established softmax loss function to maximize class separability. In this paper, we first introduce an Additive Angular Margin Loss (ArcFace), which not only has a clear geometric interpretation but also significantly enhances the discriminative power. Since ArcFace is susceptible to the massive label noise, we further propose sub-center ArcFace, in which each class contains \$K\$ sub-centers and training samples only need to be close to any of the \$K\$ positive sub-centers. Sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Based on this self-propelled isolation, we boost the performance through automatically purifying raw web faces under massive real-world noise. Besides discriminative feature embedding, we also explore the inverse problem, mapping feature vectors to face images. Without training any additional generator or discriminator, the pre-trained ArcFace model can generate identity-preserved face images for both subjects inside and outside the training data only by using the network gradient and Batch Normalization (BN) priors. Extensive experiments demonstrate that ArcFace can enhance the discriminative feature embedding as well as strengthen the generative face synthesis.},
	number = {10},
	urldate = {2023-05-21},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Deng, Jiankang and Guo, Jia and Yang, Jing and Xue, Niannan and Kotsia, Irene and Zafeiriou, Stefanos},
	month = oct,
	year = {2022},
	note = {arXiv:1801.07698 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {5962--5979},
}

@misc{noauthor_210317249_nodate,
	title = {[2103.17249] {StyleCLIP}: {Text}-{Driven} {Manipulation} of {StyleGAN} {Imagery}},
	url = {https://arxiv.org/abs/2103.17249},
	urldate = {2023-05-21},
}

@misc{liu_compositional_2022,
	title = {Compositional {Visual} {Generation} with {Composable} {Diffusion} {Models}},
	url = {https://arxiv.org/abs/2206.01714v6},
	abstract = {Large text-guided diffusion models, such as DALLE-2, are able to generate stunning photorealistic images given natural language descriptions. While such models are highly flexible, they struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects. In this paper, we propose an alternative structured approach for compositional generation using diffusion models. An image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. To do this, we interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALLE-2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation. Project page: https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/},
	language = {en},
	urldate = {2023-05-21},
	journal = {arXiv.org},
	author = {Liu, Nan and Li, Shuang and Du, Yilun and Torralba, Antonio and Tenenbaum, Joshua B.},
	month = jun,
	year = {2022},
}

@misc{poole_dreamfusion_2022,
	title = {{DreamFusion}: {Text}-to-{3D} using {2D} {Diffusion}},
	shorttitle = {{DreamFusion}},
	url = {https://arxiv.org/abs/2209.14988v1},
	abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
	language = {en},
	urldate = {2023-05-20},
	journal = {arXiv.org},
	author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
	month = sep,
	year = {2022},
}

@misc{liu_compositional_2023,
	title = {Compositional {Visual} {Generation} with {Composable} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2206.01714},
	abstract = {Large text-guided diffusion models, such as DALLE-2, are able to generate stunning photorealistic images given natural language descriptions. While such models are highly flexible, they struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects. In this paper, we propose an alternative structured approach for compositional generation using diffusion models. An image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. To do this, we interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALLE-2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation. Project page: https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Liu, Nan and Li, Shuang and Du, Yilun and Torralba, Antonio and Tenenbaum, Joshua B.},
	month = jan,
	year = {2023},
	note = {arXiv:2206.01714 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{wang_cnn_2021,
	title = {{CNN} {Explainer}: {Learning} {Convolutional} {Neural} {Networks} with {Interactive} {Visualization}},
	volume = {27},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {{CNN} {Explainer}},
	url = {http://arxiv.org/abs/2004.15004},
	doi = {10.1109/TVCG.2020.3030418},
	abstract = {Deep learning's great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN's structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern deep learning techniques.},
	number = {2},
	urldate = {2023-05-19},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},
	month = feb,
	year = {2021},
	note = {arXiv:2004.15004 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	pages = {1396--1406},
}

@misc{wang_self-consistency_2023,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{lazaridou_internet-augmented_2022,
	title = {Internet-augmented language models through few-shot prompting for open-domain question answering},
	url = {http://arxiv.org/abs/2203.05115},
	abstract = {In this work, we aim to capitalize on the unique few-shot capabilities of large-scale language models (LSLMs) to overcome some of their challenges with respect to grounding to factual and up-to-date information. Motivated by semi-parametric language models (LMs), which ground their decisions in external retrieved evidence, we use few-shot prompting to learn to condition LMs on information returned from the web using Google Search, a broad and constantly updated knowledge source. Our approach does not involve fine-tuning or learning additional parameters, thus making it applicable to any LM, offering therefore a strong baseline. Indeed, we find that LMs conditioned on the web surpass performance of closed-book models of similar, or even larger, model sizes in open-domain question answering. Finally, we find that increasing the inference-time compute of models, achieved via using multiple retrieved evidences to generate multiple answers followed by a reranking stage that uses scores generated by the same LMs, leads to better performance and alleviates lower performance of smaller few-shot LMs. All in all, our findings suggest that it might be beneficial to slow down the race towards the biggest model and instead shift attention towards finding more effective ways to use models, including but not limited to, better prompting or increasing inference-time compute.},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Lazaridou, Angeliki and Gribovskaya, Elena and Stokowiec, Wojciech and Grigorev, Nikolai},
	month = may,
	year = {2022},
	note = {arXiv:2203.05115 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{roich_pivotal_2021,
	title = {Pivotal {Tuning} for {Latent}-based {Editing} of {Real} {Images}},
	url = {http://arxiv.org/abs/2106.05744},
	abstract = {Recently, a surge of advanced facial editing techniques have been proposed that leverage the generative power of a pre-trained StyleGAN. To successfully edit an image this way, one must first project (or invert) the image into the pre-trained generator's domain. As it turns out, however, StyleGAN's latent space induces an inherent tradeoff between distortion and editability, i.e. between maintaining the original appearance and convincingly altering some of its attributes. Practically, this means it is still challenging to apply ID-preserving facial latent-space editing to faces which are out of the generator's domain. In this paper, we present an approach to bridge this gap. Our technique slightly alters the generator, so that an out-of-domain image is faithfully mapped into an in-domain latent code. The key idea is pivotal tuning - a brief training process that preserves the editing quality of an in-domain latent region, while changing its portrayed identity and appearance. In Pivotal Tuning Inversion (PTI), an initial inverted latent code serves as a pivot, around which the generator is fined-tuned. At the same time, a regularization term keeps nearby identities intact, to locally contain the effect. This surgical training process ends up altering appearance features that represent mostly identity, without affecting editing capabilities. We validate our technique through inversion and editing metrics, and show preferable scores to state-of-the-art methods. We further qualitatively demonstrate our technique by applying advanced edits (such as pose, age, or expression) to numerous images of well-known and recognizable identities. Finally, we demonstrate resilience to harder cases, including heavy make-up, elaborate hairstyles and/or headwear, which otherwise could not have been successfully inverted and edited by state-of-the-art methods.},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Roich, Daniel and Mokady, Ron and Bermano, Amit H. and Cohen-Or, Daniel},
	month = jun,
	year = {2021},
	note = {arXiv:2106.05744 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_character-centric_2022,
	title = {Character-{Centric} {Story} {Visualization} via {Visual} {Planning} and {Token} {Alignment}},
	url = {http://arxiv.org/abs/2210.08465},
	abstract = {Story visualization advances the traditional text-to-image generation by enabling multiple image generation based on a complete story. This task requires machines to 1) understand long text inputs and 2) produce a globally consistent image sequence that illustrates the contents of the story. A key challenge of consistent story visualization is to preserve characters that are essential in stories. To tackle the challenge, we propose to adapt a recent work that augments Vector-Quantized Variational Autoencoders (VQ-VAE) with a text-tovisual-token (transformer) architecture. Specifically, we modify the text-to-visual-token module with a two-stage framework: 1) character token planning model that predicts the visual tokens for characters only; 2) visual token completion model that generates the remaining visual token sequence, which is sent to VQ-VAE for finalizing image generations. To encourage characters to appear in the images, we further train the two-stage framework with a character-token alignment objective. Extensive experiments and evaluations demonstrate that the proposed method excels at preserving characters and can produce higher quality image sequences compared with the strong baselines. Codes can be found in https://github.com/sairin1202/VP-CSV},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Chen, Hong and Han, Rujun and Wu, Te-Lin and Nakayama, Hideki and Peng, Nanyun},
	month = oct,
	year = {2022},
	note = {arXiv:2210.08465 [cs]
version: 4},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{khattab_demonstrate-search-predict_2023,
	title = {Demonstrate-{Search}-{Predict}: {Composing} retrieval and language models for knowledge-intensive {NLP}},
	shorttitle = {Demonstrate-{Search}-{Predict}},
	url = {http://arxiv.org/abs/2212.14024},
	doi = {10.48550/arXiv.2212.14024},
	abstract = {Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple "retrieve-then-read" pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120\%, 8-39\%, and 80-290\% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
	month = jan,
	year = {2023},
	note = {arXiv:2212.14024 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{noauthor_three_nodate,
	title = {Three trajectories for narrative visualisation - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S2468502X2100019X},
	urldate = {2023-05-18},
}

@inproceedings{niemeyer_giraffe_2021,
	title = {{GIRAFFE}: {Representing} {Scenes} {As} {Compositional} {Generative} {Neural} {Feature} {Fields}},
	shorttitle = {{GIRAFFE}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Niemeyer_GIRAFFE_Representing_Scenes_As_Compositional_Generative_Neural_Feature_Fields_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-04-30},
	author = {Niemeyer, Michael and Geiger, Andreas},
	year = {2021},
	pages = {11453--11464},
}

@misc{noauthor_text2live_nodate,
	title = {{Text2LIVE}: {Text}-{Driven} {Layered} {Image} and {Video} {Editing}},
	url = {https://text2live.github.io/},
	urldate = {2023-04-22},
}

@misc{tang_what_2022,
	title = {What the {DAAM}: {Interpreting} {Stable} {Diffusion} {Using} {Cross} {Attention}},
	shorttitle = {What the {DAAM}},
	url = {http://arxiv.org/abs/2210.04885},
	doi = {10.48550/arXiv.2210.04885},
	abstract = {Large-scale diffusion neural networks represent a substantial milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce pixel-level attribution maps, we upscale and aggregate cross-attention word-pixel scores in the denoising subnetwork, naming our method DAAM. We evaluate its correctness by testing its semantic segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. We then apply DAAM to study the role of syntax in the pixel space, characterizing head--dependent heat map interaction patterns for ten common dependency relations. Finally, we study several semantic phenomena using DAAM, with a focus on feature entanglement, where we find that cohyponyms worsen generation quality and descriptive adjectives attend too broadly. To our knowledge, we are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future lines of research. Our code is at https://github.com/castorini/daam.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Tang, Raphael and Liu, Linqing and Pandey, Akshat and Jiang, Zhiying and Yang, Gefei and Kumar, Karun and Stenetorp, Pontus and Lin, Jimmy and Ture, Ferhan},
	month = dec,
	year = {2022},
	note = {arXiv:2210.04885 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noauthor_what_nodate,
	title = {What the {DAAM}: {Interpreting} {Stable} {Diffusion} {Using} {Cross} {Attention} - {Google} {Search}},
	url = {https://www.google.com/search?q=What+the+DAAM%3A+Interpreting+Stable+Diffusion+Using+Cross+Attention&oq=What+the+DAAM%3A+Interpreting+Stable+Diffusion+Using+Cross+Attention&aqs=chrome..69i57j69i61.148j0j7&sourceid=chrome&ie=UTF-8},
	urldate = {2023-04-14},
}

@misc{li_grounded_2021,
	title = {Grounded {Language}-{Image} {Pre}-training},
	url = {https://arxiv.org/abs/2112.03857v2},
	abstract = {This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code is released at https://github.com/microsoft/GLIP.},
	language = {en},
	urldate = {2023-04-12},
	journal = {arXiv.org},
	author = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
	month = dec,
	year = {2021},
}

@misc{yan_universal_2023,
	title = {Universal {Instance} {Perception} as {Object} {Discovery} and {Retrieval}},
	url = {https://arxiv.org/abs/2303.06674v1},
	abstract = {All instance perception tasks aim at finding certain objects specified by some queries such as category names, language expressions, and target annotations, but this complete field has been split into multiple independent subtasks. In this work, we present a universal instance perception model of the next generation, termed UNINEXT. UNINEXT reformulates diverse instance perception tasks into a unified object discovery and retrieval paradigm and can flexibly perceive different types of objects by simply changing the input prompts. This unified formulation brings the following benefits: (1) enormous data from different tasks and label vocabularies can be exploited for jointly training general instance-level representations, which is especially beneficial for tasks lacking in training data. (2) the unified model is parameter-efficient and can save redundant computation when handling multiple tasks simultaneously. UNINEXT shows superior performance on 20 challenging benchmarks from 10 instance-level tasks including classical image-level tasks (object detection and instance segmentation), vision-and-language tasks (referring expression comprehension and segmentation), and six video-level object tracking tasks. Code is available at https://github.com/MasterBin-IIAU/UNINEXT.},
	language = {en},
	urldate = {2023-04-12},
	journal = {arXiv.org},
	author = {Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Luo, Ping and Yuan, Zehuan and Lu, Huchuan},
	month = mar,
	year = {2023},
}

@misc{subramanian_reclip_2022,
	title = {{ReCLIP}: {A} {Strong} {Zero}-{Shot} {Baseline} for {Referring} {Expression} {Comprehension}},
	shorttitle = {{ReCLIP}},
	url = {http://arxiv.org/abs/2204.05991},
	doi = {10.48550/arXiv.2204.05991},
	abstract = {Training a referring expression comprehension (ReC) model for a new visual domain requires collecting referring expressions, and potentially corresponding bounding boxes, for images in the domain. While large-scale pre-trained models are useful for image classification across domains, it remains unclear if they can be applied in a zero-shot manner to more complex tasks like ReC. We present ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a state-of-the-art large-scale model, for ReC. Motivated by the close connection between ReC and CLIP's contrastive pre-training objective, the first component of ReCLIP is a region-scoring method that isolates object proposals via cropping and blurring, and passes them to CLIP. However, through controlled experiments on a synthetic dataset, we find that CLIP is largely incapable of performing spatial reasoning off-the-shelf. Thus, the second component of ReCLIP is a spatial relation resolver that handles several types of spatial relations. We reduce the gap between zero-shot baselines from prior work and supervised models by as much as 29\% on RefCOCOg, and on RefGTA (video game imagery), ReCLIP's relative improvement over supervised ReC models trained on real images is 8\%.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Subramanian, Sanjay and Merrill, William and Darrell, Trevor and Gardner, Matt and Singh, Sameer and Rohrbach, Anna},
	month = may,
	year = {2022},
	note = {arXiv:2204.05991 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{luddecke_image_2022,
	title = {Image {Segmentation} {Using} {Text} and {Image} {Prompts}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Luddecke_Image_Segmentation_Using_Text_and_Image_Prompts_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-03-30},
	author = {Lüddecke, Timo and Ecker, Alexander},
	year = {2022},
	pages = {7086--7096},
}

@misc{wang_cris_2022,
	title = {{CRIS}: {CLIP}-{Driven} {Referring} {Image} {Segmentation}},
	shorttitle = {{CRIS}},
	url = {http://arxiv.org/abs/2111.15174},
	doi = {10.48550/arXiv.2111.15174},
	abstract = {Referring image segmentation aims to segment a referent via a natural linguistic expression.Due to the distinct data properties between text and image, it is challenging for a network to well align text and pixel-level features. Existing approaches use pretrained models to facilitate learning, yet separately transfer the language/vision knowledge from pretrained models, ignoring the multi-modal corresponding information. Inspired by the recent advance in Contrastive Language-Image Pretraining (CLIP), in this paper, we propose an end-to-end CLIP-Driven Referring Image Segmentation framework (CRIS). To transfer the multi-modal knowledge effectively, CRIS resorts to vision-language decoding and contrastive learning for achieving the text-to-pixel alignment. More specifically, we design a vision-language decoder to propagate fine-grained semantic information from textual representations to each pixel-level activation, which promotes consistency between the two modalities. In addition, we present text-to-pixel contrastive learning to explicitly enforce the text feature similar to the related pixel-level features and dissimilar to the irrelevances. The experimental results on three benchmark datasets demonstrate that our proposed framework significantly outperforms the state-of-the-art performance without any post-processing. The code will be released.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Wang, Zhaoqing and Lu, Yu and Li, Qiang and Tao, Xunqiang and Guo, Yandong and Gong, Mingming and Liu, Tongliang},
	month = mar,
	year = {2022},
	note = {arXiv:2111.15174 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{gu_open-vocabulary_2022,
	title = {Open-vocabulary {Object} {Detection} via {Vision} and {Language} {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2104.13921},
	doi = {10.48550/arXiv.2104.13921},
	abstract = {We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask AP\$\_r\$ with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 AP\$\_r\$. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP\$\_\{50\}\$ on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild.},
	urldate = {2023-03-28},
	publisher = {arXiv},
	author = {Gu, Xiuye and Lin, Tsung-Yi and Kuo, Weicheng and Cui, Yin},
	month = may,
	year = {2022},
	note = {arXiv:2104.13921 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{yao_cpt_2021,
	title = {{CPT}: {Colorful} {Prompt} {Tuning} for {Pre}-trained {Vision}-{Language} {Models}},
	shorttitle = {{CPT}},
	url = {https://arxiv.org/abs/2109.11797v3},
	abstract = {Pre-Trained Vision-Language Models (VL-PTMs) have shown promising capabilities in grounding natural language in image data, facilitating a broad variety of cross-modal tasks. However, we note that there exists a significant gap between the objective forms of model pre-training and fine-tuning, resulting in a need for large amounts of labeled data to stimulate the visual grounding capability of VL-PTMs for downstream tasks. To address the challenge, we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual grounding into a fill-in-the-blank problem with color-based co-referential markers in image and text, maximally mitigating the gap. In this way, CPT enables strong few-shot and even zero-shot visual grounding capabilities of VL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs outperform their fine-tuned counterparts by a large margin (e.g., 17.3\% absolute accuracy improvement, and 73.8\% relative standard deviation reduction on average with one shot in RefCOCO evaluation). We make the data and code for this paper publicly available at https://github.com/thunlp/CPT.},
	language = {en},
	urldate = {2023-03-28},
	journal = {arXiv.org},
	author = {Yao, Yuan and Zhang, Ao and Zhang, Zhengyan and Liu, Zhiyuan and Chua, Tat-Seng and Sun, Maosong},
	month = sep,
	year = {2021},
}

@misc{qiu_vt-clip_2022,
	title = {{VT}-{CLIP}: {Enhancing} {Vision}-{Language} {Models} with {Visual}-guided {Texts}},
	shorttitle = {{VT}-{CLIP}},
	url = {http://arxiv.org/abs/2112.02399},
	doi = {10.48550/arXiv.2112.02399},
	abstract = {Contrastive Language-Image Pre-training (CLIP) has drawn increasing attention recently for its transferable visual representation learning. However, due to the semantic gap within datasets, CLIP's pre-trained image-text alignment becomes sub-optimal on downstream tasks, which severely harms its transferring performance. To better adapt the cross-modality embedding space, we propose to enhance CLIP via Visual-guided Texts, named VT-CLIP. Specifically, we guide textual features of different categories to adaptively explore informative regions on the image and aggregate visual features by attention mechanisms. In this way, the texts become visual-guided, namely, more semantically correlated with downstream images, which greatly benefits the category-wise matching process. In few-shot settings, we evaluate our VT-CLIP on 11 well-known classification datasets to demonstrate its effectiveness.},
	urldate = {2023-03-26},
	publisher = {arXiv},
	author = {Qiu, Longtian and Zhang, Renrui and Guo, Ziyu and Zeng, Ziyao and Li, Yafeng and Zhang, Guangnan},
	month = nov,
	year = {2022},
	note = {arXiv:2112.02399 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{lin_space_2023,
	title = {{SPACE}: {Unsupervised} {Object}-{Oriented} {Scene} {Representation} via {Spatial} {Attention} and {Decomposition}},
	shorttitle = {{SPACE}},
	url = {https://openreview.net/forum?id=rkl03ySYDH},
	abstract = {The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a uniﬁed probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page},
	language = {en},
	urldate = {2023-03-26},
	author = {Lin, Zhixuan and Wu, Yi-Fu and Peri, Skand Vishwanath and Sun, Weihao and Singh, Gautam and Deng, Fei and Jiang, Jindong and Ahn, Sungjin},
	month = jan,
	year = {2023},
}

@misc{shen_how_2021,
	title = {How {Much} {Can} {CLIP} {Benefit} {Vision}-and-{Language} {Tasks}?},
	url = {http://arxiv.org/abs/2107.06383},
	doi = {10.48550/arXiv.2107.06383},
	abstract = {Most existing Vision-and-Language (V\&L) models rely on pre-trained visual encoders, using a relatively small set of manually-annotated data (as compared to web-crawled data), to perceive the visual world. However, it has been observed that large-scale pretraining usually can result in better generalization performance, e.g., CLIP (Contrastive Language-Image Pre-training), trained on a massive amount of image-caption pairs, has shown a strong zero-shot capability on various vision tasks. To further study the advantage brought by CLIP, we propose to use CLIP as the visual encoder in various V\&L models in two typical scenarios: 1) plugging CLIP into task-specific fine-tuning; 2) combining CLIP with V\&L pre-training and transferring to downstream tasks. We show that CLIP significantly outperforms widely-used visual encoders trained with in-domain annotated data, such as BottomUp-TopDown. We achieve competitive or better results on diverse V\&L tasks, while establishing new state-of-the-art results on Visual Question Answering, Visual Entailment, and V\&L Navigation tasks. We release our code at https://github.com/clip-vil/CLIP-ViL.},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Shen, Sheng and Li, Liunian Harold and Tan, Hao and Bansal, Mohit and Rohrbach, Anna and Chang, Kai-Wei and Yao, Zhewei and Keutzer, Kurt},
	month = jul,
	year = {2021},
	note = {arXiv:2107.06383 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{xia_tedigan_2021,
	title = {{TediGAN}: {Text}-{Guided} {Diverse} {Face} {Image} {Generation} and {Manipulation}},
	shorttitle = {{TediGAN}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Xia_TediGAN_Text-Guided_Diverse_Face_Image_Generation_and_Manipulation_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-03-24},
	author = {Xia, Weihao and Yang, Yujiu and Xue, Jing-Hao and Wu, Baoyuan},
	year = {2021},
	pages = {2256--2265},
}

@misc{noauthor_semantic_nodate,
	title = {Semantic {Image} {Retrieval} {Using} {Region} {Based} {Inverted} {File} {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/abstract/document/5384974/},
	urldate = {2023-03-24},
}

@misc{burgess_monet_2019,
	title = {{MONet}: {Unsupervised} {Scene} {Decomposition} and {Representation}},
	shorttitle = {{MONet}},
	url = {http://arxiv.org/abs/1901.11390},
	doi = {10.48550/arXiv.1901.11390},
	abstract = {The ability to decompose scenes in terms of abstract building blocks is crucial for general intelligence. Where those basic building blocks share meaningful properties, interactions and other regularities across scenes, such decompositions can simplify reasoning and facilitate imagination of novel scenarios. In particular, representing perceptual observations in terms of entities should improve data efficiency and transfer performance on a wide range of tasks. Thus we need models capable of discovering useful decompositions of scenes by identifying units with such regularities and representing them in a common format. To address this problem, we have developed the Multi-Object Network (MONet). In this model, a VAE is trained end-to-end together with a recurrent attention network -- in a purely unsupervised manner -- to provide attention masks around, and reconstructions of, regions of images. We show that this model is capable of learning to decompose and represent challenging 3D scenes into semantically meaningful components, such as objects and background elements.},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Burgess, Christopher P. and Matthey, Loic and Watters, Nicholas and Kabra, Rishabh and Higgins, Irina and Botvinick, Matt and Lerchner, Alexander},
	month = jan,
	year = {2019},
	note = {arXiv:1901.11390 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{johnson_clevr_2016,
	title = {{CLEVR}: {A} {Diagnostic} {Dataset} for {Compositional} {Language} and {Elementary} {Visual} {Reasoning}},
	shorttitle = {{CLEVR}},
	url = {http://arxiv.org/abs/1612.06890},
	doi = {10.48550/arXiv.1612.06890},
	abstract = {When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Zitnick, C. Lawrence and Girshick, Ross},
	month = dec,
	year = {2016},
	note = {arXiv:1612.06890 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{chang_muse_2023,
	title = {Muse: {Text}-{To}-{Image} {Generation} via {Masked} {Generative} {Transformers}},
	shorttitle = {Muse},
	url = {http://arxiv.org/abs/2301.00704},
	doi = {10.48550/arXiv.2301.00704},
	abstract = {We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Chang, Huiwen and Zhang, Han and Barber, Jarred and Maschinot, A. J. and Lezama, Jose and Jiang, Lu and Yang, Ming-Hsuan and Murphy, Kevin and Freeman, William T. and Rubinstein, Michael and Li, Yuanzhen and Krishnan, Dilip},
	month = jan,
	year = {2023},
	note = {arXiv:2301.00704 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{huang_composer_2023,
	title = {Composer: {Creative} and {Controllable} {Image} {Synthesis} with {Composable} {Conditions}},
	shorttitle = {Composer},
	url = {http://arxiv.org/abs/2302.09778},
	doi = {10.48550/arXiv.2302.09778},
	abstract = {Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability. This work offers a new generation paradigm that allows flexible control of the output image, such as spatial layout and palette, while maintaining the synthesis quality and model creativity. With compositionality as the core idea, we first decompose an image into representative factors, and then train a diffusion model with all these factors as the conditions to recompose the input. At the inference stage, the rich intermediate representations work as composable elements, leading to a huge design space (i.e., exponentially proportional to the number of decomposed factors) for customizable content creation. It is noteworthy that our approach, which we call Composer, supports various levels of conditions, such as text description as the global information, depth map and sketch as the local guidance, color histogram for low-level details, etc. Besides improving controllability, we confirm that Composer serves as a general framework and facilitates a wide range of classical generative tasks without retraining. Code and models will be made available.},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Huang, Lianghua and Chen, Di and Liu, Yu and Shen, Yujun and Zhao, Deli and Zhou, Jingren},
	month = feb,
	year = {2023},
	note = {arXiv:2302.09778 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@inproceedings{arad_hudson_compositional_2021,
	title = {Compositional {Transformers} for {Scene} {Generation}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/4eff0720836a198b6174eecf02cbfdbf-Abstract.html},
	abstract = {We introduce the GANformer2 model, an iterative object-oriented transformer, explored for the task of generative modeling. The network incorporates strong and explicit structural priors, to reflect the compositional nature of visual scenes, and synthesizes images through a sequential process. It operates in two stages: a fast and lightweight planning phase, where we draft a high-level scene layout, followed by an attention-based execution phase, where the layout is being refined, evolving into a rich and detailed picture. Our model moves away from conventional black-box GAN architectures that feature a flat and monolithic latent space towards a transparent design that encourages efficiency, controllability and interpretability. We demonstrate GANformer2's strengths and qualities through a careful evaluation over a range of datasets, from multi-object CLEVR scenes to the challenging COCO images, showing it successfully achieves state-of-the-art performance in terms of visual quality, diversity and consistency. Further experiments demonstrate the model's disentanglement and provide a deeper insight into its generative process, as it proceeds step-by-step from a rough initial sketch, to a detailed layout that accounts for objects' depths and dependencies, and up to the final high-resolution depiction of vibrant and intricate real-world scenes. See https://github.com/dorarad/gansformer for model implementation.},
	urldate = {2023-03-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Arad Hudson, Dor and Zitnick, Larry},
	year = {2021},
	pages = {9506--9520},
}

@inproceedings{koh_text--image_2021,
	title = {Text-to-{Image} {Generation} {Grounded} by {Fine}-{Grained} {User} {Attention}},
	url = {https://openaccess.thecvf.com/content/WACV2021/html/Koh_Text-to-Image_Generation_Grounded_by_Fine-Grained_User_Attention_WACV_2021_paper.html},
	language = {en},
	urldate = {2023-03-24},
	author = {Koh, Jing Yu and Baldridge, Jason and Lee, Honglak and Yang, Yinfei},
	year = {2021},
	pages = {237--246},
}

@misc{tan_text2scene_2019,
	title = {{Text2Scene}: {Generating} {Compositional} {Scenes} from {Textual} {Descriptions}},
	shorttitle = {{Text2Scene}},
	url = {http://arxiv.org/abs/1809.01110},
	doi = {10.48550/arXiv.1809.01110},
	abstract = {In this paper, we propose Text2Scene, a model that generates various forms of compositional scene representations from natural language descriptions. Unlike recent works, our method does NOT use Generative Adversarial Networks (GANs). Text2Scene instead learns to sequentially generate objects and their attributes (location, size, appearance, etc) at every time step by attending to different parts of the input text and the current status of the generated scene. We show that under minor modifications, the proposed framework can handle the generation of different forms of scene representations, including cartoon-like scenes, object layouts corresponding to real images, and synthetic images. Our method is not only competitive when compared with state-of-the-art GAN-based methods using automatic metrics and superior based on human judgments but also has the advantage of producing interpretable results.},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Tan, Fuwen and Feng, Song and Ordonez, Vicente},
	month = jun,
	year = {2019},
	note = {arXiv:1809.01110 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{pan_synthesizing_2022,
	title = {Synthesizing {Coherent} {Story} with {Auto}-{Regressive} {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2211.10950},
	doi = {10.48550/arXiv.2211.10950},
	abstract = {Conditioned diffusion models have demonstrated state-of-the-art text-to-image synthesis capacity. Recently, most works focus on synthesizing independent images; While for real-world applications, it is common and necessary to generate a series of coherent images for story-stelling. In this work, we mainly focus on story visualization and continuation tasks and propose AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images. Moreover, AR-LDM can generalize to new characters through adaptation. To our best knowledge, this is the first work successfully leveraging diffusion models for coherent visual story synthesizing. Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the newly introduced challenging dataset VIST containing natural images. Large-scale human evaluations show that AR-LDM has superior performance in terms of quality, relevance, and consistency.},
	urldate = {2023-03-23},
	publisher = {arXiv},
	author = {Pan, Xichen and Qin, Pengda and Li, Yuhong and Xue, Hui and Chen, Wenhu},
	month = nov,
	year = {2022},
	note = {arXiv:2211.10950 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{maharana_improving_2021,
	title = {Improving {Generation} and {Evaluation} of {Visual} {Stories} via {Semantic} {Consistency}},
	url = {http://arxiv.org/abs/2105.10026},
	doi = {10.48550/arXiv.2105.10026},
	abstract = {Story visualization is an under-explored task that falls at the intersection of many important research directions in both computer vision and natural language processing. In this task, given a series of natural language captions which compose a story, an agent must generate a sequence of images that correspond to the captions. Prior work has introduced recurrent generative models which outperform text-to-image synthesis models on this task. However, there is room for improvement of generated images in terms of visual quality, coherence and relevance. We present a number of improvements to prior modeling approaches, including (1) the addition of a dual learning framework that utilizes video captioning to reinforce the semantic alignment between the story and generated images, (2) a copy-transform mechanism for sequentially-consistent story visualization, and (3) MART-based transformers to model complex interactions between frames. We present ablation studies to demonstrate the effect of each of these techniques on the generative power of the model for both individual images as well as the entire narrative. Furthermore, due to the complexity and generative nature of the task, standard evaluation metrics do not accurately reflect performance. Therefore, we also provide an exploration of evaluation metrics for the model, focused on aspects of the generated frames such as the presence/quality of generated characters, the relevance to captions, and the diversity of the generated images. We also present correlation experiments of our proposed automated metrics with human evaluations. Code and data available at: https://github.com/adymaharana/StoryViz},
	urldate = {2023-03-22},
	publisher = {arXiv},
	author = {Maharana, Adyasha and Hannan, Darryl and Bansal, Mohit},
	month = may,
	year = {2021},
	note = {arXiv:2105.10026 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{gafni_make--scene_2022,
	title = {Make-{A}-{Scene}: {Scene}-{Based} {Text}-to-{Image} {Generation} with {Human} {Priors}},
	shorttitle = {Make-{A}-{Scene}},
	url = {http://arxiv.org/abs/2203.13131},
	doi = {10.48550/arXiv.2203.13131},
	abstract = {Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of 512x512 pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: (i) Scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote.},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Gafni, Oran and Polyak, Adam and Ashual, Oron and Sheynin, Shelly and Parikh, Devi and Taigman, Yaniv},
	month = mar,
	year = {2022},
	note = {arXiv:2203.13131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{chen_re-imagen_2022,
	title = {Re-{Imagen}: {Retrieval}-{Augmented} {Text}-to-{Image} {Generator}},
	shorttitle = {Re-{Imagen}},
	url = {http://arxiv.org/abs/2209.14491},
	doi = {10.48550/arXiv.2209.14491},
	abstract = {Research on text-to-image generation has witnessed significant progress in generating diverse and photo-realistic images, driven by diffusion and auto-regressive models trained on large-scale image-text data. Though state-of-the-art models can generate high-quality images of common entities, they often have difficulty generating images of uncommon entities, such as `Chortai (dog)' or `Picarones (food)'. To tackle this issue, we present the Retrieval-Augmented Text-to-Image Generator (Re-Imagen), a generative model that uses retrieved information to produce high-fidelity and faithful images, even for rare or unseen entities. Given a text prompt, Re-Imagen accesses an external multi-modal knowledge base to retrieve relevant (image, text) pairs and uses them as references to generate the image. With this retrieval step, Re-Imagen is augmented with the knowledge of high-level semantics and low-level visual details of the mentioned entities, and thus improves its accuracy in generating the entities' visual appearances. We train Re-Imagen on a constructed dataset containing (image, text, retrieval) triples to teach the model to ground on both text prompt and retrieval. Furthermore, we develop a new sampling strategy to interleave the classifier-free guidance for text and retrieval conditions to balance the text and retrieval alignment. Re-Imagen achieves significant gain on FID score over COCO and WikiImage. To further evaluate the capabilities of the model, we introduce EntityDrawBench, a new benchmark that evaluates image generation for diverse entities, from frequent to rare, across multiple object categories including dogs, foods, landmarks, birds, and characters. Human evaluation on EntityDrawBench shows that Re-Imagen can significantly improve the fidelity of generated images, especially on less frequent entities.},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Chen, Wenhu and Hu, Hexiang and Saharia, Chitwan and Cohen, William W.},
	month = nov,
	year = {2022},
	note = {arXiv:2209.14491 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{gafni_make--scene_2022-1,
	title = {Make-{A}-{Scene}: {Scene}-{Based} {Text}-to-{Image} {Generation} with {Human} {Priors}},
	shorttitle = {Make-{A}-{Scene}},
	url = {http://arxiv.org/abs/2203.13131},
	doi = {10.48550/arXiv.2203.13131},
	abstract = {Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of 512x512 pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: (i) Scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote.},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Gafni, Oran and Polyak, Adam and Ashual, Oron and Sheynin, Shelly and Parikh, Devi and Taigman, Yaniv},
	month = mar,
	year = {2022},
	note = {arXiv:2203.13131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{vicol_moviegraphs_2018,
	title = {{MovieGraphs}: {Towards} {Understanding} {Human}-{Centric} {Situations} from {Videos}},
	shorttitle = {{MovieGraphs}},
	url = {http://arxiv.org/abs/1712.06761},
	doi = {10.48550/arXiv.1712.06761},
	abstract = {There is growing interest in artificial intelligence to build socially intelligent robots. This requires machines to have the ability to "read" people's emotions, motivations, and other factors that affect behavior. Towards this goal, we introduce a novel dataset called MovieGraphs which provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions. In addition, most interactions and many attributes are grounded in the video with time stamps. We provide a thorough analysis of our dataset, showing interesting common-sense correlations between different social aspects of scenes, as well as across scenes over time. We propose a method for querying videos and text with graphs, and show that: 1) our graphs contain rich and sufficient information to summarize and localize each scene; and 2) subgraphs allow us to describe situations at an abstract level and retrieve multiple semantically relevant situations. We also propose methods for interaction understanding via ordering, and reason understanding. MovieGraphs is the first benchmark to focus on inferred properties of human-centric situations, and opens up an exciting avenue towards socially-intelligent AI agents.},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Vicol, Paul and Tapaswi, Makarand and Castrejon, Lluis and Fidler, Sanja},
	month = apr,
	year = {2018},
	note = {arXiv:1712.06761 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{papalampidi_screenplay_2020,
	title = {Screenplay {Summarization} {Using} {Latent} {Narrative} {Structure}},
	url = {http://arxiv.org/abs/2004.12727},
	doi = {10.48550/arXiv.2004.12727},
	abstract = {Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront. As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document. When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient. In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models. We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes). Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries.},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Papalampidi, Pinelopi and Keller, Frank and Frermann, Lea and Lapata, Mirella},
	month = apr,
	year = {2020},
	note = {arXiv:2004.12727 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_neural_nodate,
	title = {Neural scene representation and rendering},
	url = {https://www.deepmind.com/blog/neural-scene-representation-and-rendering},
	abstract = {There is more than meets the eye when it comes to how we understand a visual scene: our brains draw on prior knowledge to reason and to make inferences that go far beyond the patterns of light that hit our retinas. For example, when entering a room for the first time, you instantly recognise the items it contains and where they are positioned. If you see three legs of a table, you will infer that there is probably a fourth leg with the same shape and colour hidden from view. Even if you can’t see everything in the room, you’ll likely be able to sketch its layout, or imagine what it looks like from another perspective.},
	language = {en},
	urldate = {2023-03-19},
}

@inproceedings{martin_event_2018,
	title = {Event representations for automated story generation with deep neural nets},
	volume = {32},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Martin, Lara and Ammanabrolu, Prithviraj and Wang, Xinyu and Hancock, William and Singh, Shruti and Harrison, Brent and Riedl, Mark},
	year = {2018},
	note = {Issue: 1},
}

@misc{ruiz_dreambooth_2023,
	title = {{DreamBooth}: {Fine} {Tuning} {Text}-to-{Image} {Diffusion} {Models} for {Subject}-{Driven} {Generation}},
	shorttitle = {{DreamBooth}},
	url = {http://arxiv.org/abs/2208.12242},
	doi = {10.48550/arXiv.2208.12242},
	abstract = {Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for "personalization" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
	month = mar,
	year = {2023},
	note = {arXiv:2208.12242 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{gal_image_2022,
	title = {An {Image} is {Worth} {One} {Word}: {Personalizing} {Text}-to-{Image} {Generation} using {Textual} {Inversion}},
	shorttitle = {An {Image} is {Worth} {One} {Word}},
	url = {http://arxiv.org/abs/2208.01618},
	doi = {10.48550/arXiv.2208.01618},
	abstract = {Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new "words" in the embedding space of a frozen text-to-image model. These "words" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},
	month = aug,
	year = {2022},
	note = {arXiv:2208.01618 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{wei_elite_2023,
	title = {{ELITE}: {Encoding} {Visual} {Concepts} into {Textual} {Embeddings} for {Customized} {Text}-to-{Image} {Generation}},
	shorttitle = {{ELITE}},
	url = {http://arxiv.org/abs/2302.13848},
	doi = {10.48550/arXiv.2302.13848},
	abstract = {Despite unprecedented ability in imaginary creation, large text-to-image models are further expected to express customized concepts. Existing works generally learn such concepts in an optimization-based manner, yet bringing excessive computation or memory burden. In this paper, we instead propose a learning-based encoder for fast and accurate concept customization, which consists of global and local mapping networks. In specific, the global mapping network separately projects the hierarchical features of a given image into multiple ``new'' words in the textual word embedding space, i.e., one primary word for well-editable concept and other auxiliary words to exclude irrelevant disturbances (e.g., background). In the meantime, a local mapping network injects the encoded patch features into cross attention layers to provide omitted details, without sacrificing the editability of primary concepts. We compare our method with prior optimization-based approaches on a variety of user-defined concepts, and demonstrate that our method enables more high-fidelity inversion and robust editability with a significantly faster encoding process. Our code will be publicly available at https://github.com/csyxwei/ELITE.},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Wei, Yuxiang and Zhang, Yabo and Ji, Zhilong and Bai, Jinfeng and Zhang, Lei and Zuo, Wangmeng},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13848 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kumari_multi-concept_2022,
	title = {Multi-{Concept} {Customization} of {Text}-to-{Image} {Diffusion}},
	url = {http://arxiv.org/abs/2212.04488},
	doi = {10.48550/arXiv.2212.04488},
	abstract = {While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning ({\textasciitilde}6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple, new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms several baselines and concurrent works, regarding both qualitative and quantitative evaluations, while being memory and computationally efficient.},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Kumari, Nupur and Zhang, Bingliang and Zhang, Richard and Shechtman, Eli and Zhu, Jun-Yan},
	month = dec,
	year = {2022},
	note = {arXiv:2212.04488 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{lin_microsoft_2015,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	doi = {10.48550/arXiv.1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2023-03-18},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = feb,
	year = {2015},
	note = {arXiv:1405.0312 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{conwell_testing_2022,
	title = {Testing {Relational} {Understanding} in {Text}-{Guided} {Image} {Generation}},
	url = {http://arxiv.org/abs/2208.00005},
	doi = {10.48550/arXiv.2208.00005},
	abstract = {Relations are basic building blocks of human cognition. Classic and recent work suggests that many relations are early developing, and quickly perceived. Machine models that aspire to human-level perception and reasoning should reflect the ability to recognize and reason generatively about relations. We report a systematic empirical examination of a recent text-guided image generation model (DALL-E 2), using a set of 15 basic physical and social relations studied or proposed in the literature, and judgements from human participants (N = 169). Overall, we find that only {\textasciitilde}22\% of images matched basic relation prompts. Based on a quantitative examination of people's judgments, we suggest that current image generation models do not yet have a grasp of even basic relations involving simple objects and agents. We examine reasons for model successes and failures, and suggest possible improvements based on computations observed in biological intelligence.},
	urldate = {2023-03-18},
	publisher = {arXiv},
	author = {Conwell, Colin and Ullman, Tomer},
	month = jul,
	year = {2022},
	note = {arXiv:2208.00005 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{cha_learning_2022,
	title = {Learning to {Generate} {Text}-grounded {Mask} for {Open}-world {Semantic} {Segmentation} from {Only} {Image}-{Text} {Pairs}},
	url = {http://arxiv.org/abs/2212.00785},
	doi = {10.48550/arXiv.2212.00785},
	abstract = {We tackle open-world semantic segmentation, which aims at learning to segment arbitrary visual concepts in images, by using only image-text pairs without dense annotations. Existing open-world segmentation methods have shown impressive advances by employing contrastive learning (CL) to learn diverse visual concepts and adapting the learned image-level understanding to the segmentation task. However, these methods based on CL have a discrepancy since it only considers image-text level alignment in training time, while the segmentation task requires region-text level alignment at test time. In this paper, we propose a novel Text-grounded Contrastive Learning (TCL) framework to directly align a text and a region described by the text to address the train-test discrepancy. Our method generates a segmentation mask associated with a given text, extracts grounded image embedding from the masked region, and aligns it with text embedding via TCL. The framework addresses the discrepancy by letting the model learn region-text level alignment instead of image-text level alignment and encourages the model to directly improve the quality of generated segmentation masks. In addition, for a rigorous and fair comparison, we present a unified evaluation protocol with widely used 8 semantic segmentation datasets. TCL achieves state-of-the-art zero-shot segmentation performance with large margins in all datasets. Code is available at https://github.com/kakaobrain/tcl.},
	urldate = {2023-03-18},
	publisher = {arXiv},
	author = {Cha, Junbum and Mun, Jonghwan and Roh, Byungseok},
	month = dec,
	year = {2022},
	note = {arXiv:2212.00785 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sarukkai_collage_2023,
	title = {Collage {Diffusion}},
	url = {http://arxiv.org/abs/2303.00262},
	doi = {10.48550/arXiv.2303.00262},
	abstract = {Text-conditional diffusion models generate high-quality, diverse images. However, text is often an ambiguous specification for a desired target image, creating the need for additional user-friendly controls for diffusion-based image generation. We focus on having precise control over image output for scenes with several objects. Users control image generation by defining a collage: a text prompt paired with an ordered sequence of layers, where each layer is an RGBA image and a corresponding text prompt. We introduce Collage Diffusion, a collage-conditional diffusion algorithm that allows users to control both the spatial arrangement and visual attributes of objects in the scene, and also enables users to edit individual components of generated images. To ensure that different parts of the input text correspond to the various locations specified in the input collage layers, Collage Diffusion modifies text-image cross-attention with the layers' alpha masks. To maintain characteristics of individual collage layers that are not specified in text, Collage Diffusion learns specialized text representations per layer. Collage input also enables layer-based controls that provide fine-grained control over the final output: users can control image harmonization on a layer-by-layer basis, and they can edit individual objects in generated images while keeping other objects fixed. Collage-conditional image generation requires harmonizing the input collage to make objects fit together--the key challenge involves minimizing changes in the positions and key visual attributes of objects in the input collage while allowing other attributes of the collage to change in the harmonization process. By leveraging the rich information present in layer input, Collage Diffusion generates globally harmonized images that maintain desired object locations and visual characteristics better than prior approaches.},
	urldate = {2023-03-18},
	publisher = {arXiv},
	author = {Sarukkai, Vishnu and Li, Linden and Ma, Arden and Ré, Christopher and Fatahalian, Kayvon},
	month = mar,
	year = {2023},
	note = {arXiv:2303.00262 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{gal_image_2022-1,
	title = {An {Image} is {Worth} {One} {Word}: {Personalizing} {Text}-to-{Image} {Generation} using {Textual} {Inversion}},
	shorttitle = {An {Image} is {Worth} {One} {Word}},
	url = {http://arxiv.org/abs/2208.01618},
	doi = {10.48550/arXiv.2208.01618},
	abstract = {Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new "words" in the embedding space of a frozen text-to-image model. These "words" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io},
	urldate = {2023-03-18},
	publisher = {arXiv},
	author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},
	month = aug,
	year = {2022},
	note = {arXiv:2208.01618 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@inproceedings{maharana_storydall-e_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{StoryDALL}-{E}: {Adapting} {Pretrained} {Text}-to-{Image} {Transformers} for {Story} {Continuation}},
	isbn = {978-3-031-19836-6},
	shorttitle = {{StoryDALL}-{E}},
	doi = {10.1007/978-3-031-19836-6_5},
	abstract = {Recent advances in text-to-image synthesis have led to large pretrained transformers with excellent capabilities to generate visualizations from a given text. However, these models are ill-suited for specialized tasks like story visualization, which requires an agent to produce a sequence of images given a corresponding sequence of captions, forming a narrative. Moreover, we find that the story visualization task fails to accommodate generalization to unseen plots and characters in new narratives. Hence, we first propose the task of story continuation, where the generated visual story is conditioned on a source image, allowing for better generalization to narratives with new characters. Then, we enhance or ‘retro-fit’ the pretrained text-to-image synthesis models with task-specific modules for (a) sequential image generation and (b) copying relevant elements from an initial frame. We explore full-model finetuning, as well as prompt-based tuning for parameter-efficient adaptation, of the pretrained model. We evaluate our approach StoryDALL-E on two existing datasets, PororoSV and FlintstonesSV, and introduce a new dataset DiDeMoSV collected from a video-captioning dataset. We also develop a model StoryGANc based on Generative Adversarial Networks (GAN) for story continuation, and compare with the StoryDALL-E model to demonstrate the advantages of our approach. We show that our retro-fitting approach outperforms GAN-based models for story continuation. We also demonstrate that the ‘retro-fitting’ approach facilitates copying of visual elements from the source image and improved continuity in visual frames. Finally, our analysis suggests that pretrained transformers struggle with comprehending narratives containing multiple characters, and translating them into appropriate imagery. Our work encourages future research into story continuation and large-scale models for the task (Code and data are available at https://github.com/adymaharana/storydalle).},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Maharana, Adyasha and Hannan, Darryl and Bansal, Mohit},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	pages = {70--87},
}

@misc{noauthor_storygan_nodate,
	title = {‪{Storygan}: {A} sequential conditional gan for story visualization‬},
	shorttitle = {‪{Storygan}},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FhMF8dkAAAAJ&citation_for_view=FhMF8dkAAAAJ:W7OEmFMy1HYC},
	abstract = {‪Y Li, Z Gan, Y Shen, J Liu, Y Cheng, Y Wu, L Carin, D Carlson, J Gao‬, ‪Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern …, 2019‬ - ‪Cited by 127‬},
	urldate = {2023-03-18},
}

@misc{noauthor_dalle_nodate,
	title = {{DALL}·{E}: {Creating} images from text},
	url = {https://openai.com/research/dall-e},
	urldate = {2023-03-18},
}

@misc{li_storygan_2019,
	title = {{StoryGAN}: {A} {Sequential} {Conditional} {GAN} for {Story} {Visualization}},
	shorttitle = {{StoryGAN}},
	url = {http://arxiv.org/abs/1812.02784},
	doi = {10.48550/arXiv.1812.02784},
	abstract = {We propose a new task, called Story Visualization. Given a multi-sentence paragraph, the story is visualized by generating a sequence of images, one for each sentence. In contrast to video generation, story visualization focuses less on the continuity in generated images (frames), but more on the global consistency across dynamic scenes and characters -- a challenge that has not been addressed by any single-image or video generation methods. We therefore propose a new story-to-image-sequence generation model, StoryGAN, based on the sequential conditional GAN framework. Our model is unique in that it consists of a deep Context Encoder that dynamically tracks the story flow, and two discriminators at the story and image levels, to enhance the image quality and the consistency of the generated sequences. To evaluate the model, we modified existing datasets to create the CLEVR-SV and Pororo-SV datasets. Empirically, StoryGAN outperforms state-of-the-art models in image quality, contextual consistency metrics, and human evaluation.},
	urldate = {2023-03-18},
	publisher = {arXiv},
	author = {Li, Yitong and Gan, Zhe and Shen, Yelong and Liu, Jingjing and Cheng, Yu and Wu, Yuexin and Carin, Lawrence and Carlson, David and Gao, Jianfeng},
	month = apr,
	year = {2019},
	note = {arXiv:1812.02784 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
