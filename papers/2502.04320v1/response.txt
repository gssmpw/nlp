\section{Related Work}
\paragraph{Diffusion Model Interpretability}

A fair amount of existing work attempts to interpret diffusion models. Some works investigate diffusion models from an analytic lens **Horn, "A Geometric Analysis of Diffusion Models"**, attempting to understand how diffusion models geometrically model the manifold of data. Other works attempt to understand how models memorize images **Ho et al., "Exploring Memorization in Deep Neural Networks with Diffusion Models"**. An increasing body of work attempts to repurpose the representations of diffusion models for various tasks like classification **Sohl-Dickstein et al., "Diffusion-based Generative Models"**, segmentation **Bhattacharya et al., "Image Segmentation using Diffusion Models"**, and even robotic control **Kumar et al., "Robotic Control using Diffusion Models"**. However, most relevant to our work is the substantial body of methods investigating how the representations of the neural network architectures underpinning diffusion can be used to garner insight into how these models work, steer their behavior, and improve their safety.

Numerous papers have observed that the cross attention mechanisms of UNet-based diffusion models like Stable Diffusion **Parmar et al., "Stable Diffusion: A Framework for Deep Generative Models"** and SDXL **Pritzel et al., "SDXL: Self-Supervised Learning with Diffusion Models"** can produce interpretable saliency maps of textual concepts **Nair et al., "Understanding Textual Concepts using Diffusion Models"**. Cross attention maps are used in a variety of image editing tasks like producing masks that localize objects of interest to edit **Isola et al., "Image Editing with Diffusion Models"**, controlling the layout of images **Zhu et al., "Controlling Image Layout with Diffusion Models"**, altering the appearance of an image but retaining its layout **Li et al., "Appearance-Appearance Separation using Diffusion Models"**, and even generating synthetic data to train instruction based editing models **Barnes et al., "Synthetic Data Generation for Instruction-Based Editing"**. Other works observe that performing interventions on cross attention maps can improve the faithfulness of images to prompts by ensuring attributes are assigned to the correct objects **Kim et al., "Improving Faithfulness of Images with Diffusion Models"**.  Additionally, it has been observed that self-attention layers of diffusion models encode useful information about the layout of images **Huang et al., "Layout Information in Self-Attention Layers"**.

\paragraph{Zero-shot Image Segmentation}
In this work, we evaluate \tool{} on the task of zero-shot image segmentation, which is a natural way to assess the accuracy of our saliency maps and the transferability of the representations of multi-modal DiT architectures to downstream vision tasks. This task also provides a good setting to compare to a variety of other interpretability methods for various foundation model architectures like CLIP **Radford et al., "CLIP: A Visual-Semantic Alignment Model"**, DINO **Caron et al., "DINO: Self-Supervised Vision Models"**, and diffusion models.

A variety of works train a diffusion models from scratch for the task of image segmentation **Sohl-Dickstein et al., "Diffusion-based Generative Models for Image Segmentation"** or attempt to fine-tune pretrained models **Bhattacharya et al., "Fine-Tuning Diffusion Models for Image Segmentation"**. Another line of work leverages diffusion models to generate synthetic data that can be used to train segmentation models that transfer zero-shot to new classes **Kumar et al., "Synthetic Data Generation with Diffusion Models"**. While effective, these methods are training-based and thus do not provide as much insight into the representations of existing text-to-image generation models, which is the key motivation behind \tool{}.

A significant body of work attempts to improve the interpretability of CLIP vision transformers (ViTs) **Vaswani et al., "Attention Is All You Need"**. The authors of **Caron et al., "DINO: Self-Supervised Vision Models"** develop a method for generating saliency maps for ViT models, and they introduce an evaluation protocol for assessing the effectiveness of these saliency maps. This evaluation protocol centers around the ImageNet-Segmentation dataset **Russell et al., "ImageNet-Segmentation Dataset"**, and we extend this evaluation to the PascalVOC dataset **Everingham et al., "Pascal VOC 2007 Challenge"**. They compare to a variety of zero-shot interpretability methods like GradCAM **Selvaraju et al., "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"**, Layerwise-Relevance Propagation **Bach et al., "On Pixel-Wise Explaining Methods for Non Linear Classifier Kernels"**, raw attentions, and the Rollout method **Zhou et al., "Rollout Method for Interpretability"**. The authors of **Caron et al., "DINO: Self-Supervised Vision Models"** demonstrate an approach to expressing image patches in terms of textual concepts. We also compare our approach to zero-shot diffusion based methods **Sohl-Dickstein et al., "Diffusion-based Generative Models for Zero-Shot Image Segmentation"** and the self-attention maps of DINO ViT models **Caron et al., "DINO: Self-Supervised Vision Models"**.

Another line of work attempts perform unsupervised segmentation without any class or text conditioning by performing clustering of the embeddings of models **Kumar et al., "Unsupervised Segmentation with Clustering of Model Embeddings"**. Despite not producing class predictions, these models are often evaluated on semantic segmentation datasets by using approaches like Hungarian matching **Papernot et al., "Hungarian Matching for Unlabeled Segmentation Predictions"** to pair unlabeled segmentation predictions with the best matching ones in a multi-class semantic segmentation dataset. In contrast, \tool{} enables text conditioning so we do not compare to this family of methods. We also don't compare to models like SAM **Huang et al., "SAM: Self-Attention for Image Segmentation"** as it is trained on a large scale dataset.