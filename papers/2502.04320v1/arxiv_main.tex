%

\documentclass{article}

%
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %

%
%
%
%
\usepackage{hyperref}
\usepackage[table]{xcolor}

%
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\tool}[0]{\textsc{ConceptAttention}}
\newcommand{\layername}[0]{\textsc{MMAttn}}

%
%

%
\usepackage[accepted]{template/icml2024}

%
\usepackage{amsmath}
\usepackage{float}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%
\usepackage[capitalize,noabbrev]{cleveref}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\MLP}{MLP}
\DeclareMathOperator{\layernorm}{layernorm}
\DeclareMathOperator{\lnorm}{lnorm}

\definecolor{myorange}{RGB}{253,158,104}
\definecolor{myblue}{RGB}{104,133,253}
\definecolor{mygreen}{RGB}{100,214,112}

%
%
%
\usepackage[textsize=tiny]{todonotes}


%
%
\icmltitlerunning{ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features}

\begin{document}

\twocolumn[
\icmltitle{ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features}

%
%
%
%

%
%
%
%

%
%
%
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Alec Helbling}{gt}
\icmlauthor{Tuna Han Salih Meral}{vt}
\icmlauthor{Ben Hoover}{gt,ibm}
\icmlauthor{Pinar Yanardag}{vt}
\icmlauthor{Duen Horng (Polo) Chau}{gt}
%
%
\end{icmlauthorlist}

\icmlaffiliation{gt}{Georgia Tech}
\icmlaffiliation{vt}{Virginia Tech}
\icmlaffiliation{ibm}{IBM Research}

\icmlcorrespondingauthor{Alec Helbling}{alechelbling@gatech.edu}
%
%
%
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]


%

%
%
%
%
%

\printAffiliationsAndNotice{}  %
%


\begin{abstract}
%

%


%
Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce \tool{}, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images\footnote[4]{Code: \href{https://alechelbling.com/ConceptAttention/}{https://alechelbling.com/ConceptAttention/}}. Without requiring additional training, \tool{} repurposes the parameters of DiT attention layers to produce highly contextualized \textit{concept embeddings}, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention mechanisms. Remarkably, \tool{} even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 11 other zero-shot interpretability methods on the ImageNet-Segmentation dataset and on a single-class subset of PascalVOC. Our work contributes the first evidence that the representations of multi-modal DiT models like Flux are highly transferable to vision tasks like segmentation, even outperforming multi-modal foundation models like CLIP. 


%

%


%

%

%

%
%

%
%
%
%
%

%

%
%
%

%

%
%
%

%
%
%
%
%


%


%



%

%

%


%

%


%

%

%

%

%
%
%
%
%
%
%


%



%

%

%

%



%

%

%

%

%
%

%

%

%
%
%
%
%
%
%

%



%



%




%


%



%

%



%


%


%

%

%





%

%

%


%

%



%


%


%



%





%

%
%


%


%


%

%



%

%




%







%

%

%



%



%



%




%


%


%

\end{abstract}

\label{submission}

%

%
%
%
%
%
%
%


%
%
%
%
%
%
%


\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/CrownJewelDragon.png}
    \vspace{-0.25in}
    \caption{\textbf{\tool{} produces saliency maps that precisely localize the presence of textual concepts in images.}  We compare Flux raw cross attention, DAAM \cite{tang_what_2022} with SDXL, and TextSpan \cite{gandelsman_interpreting_2024} for CLIP. }
    \label{fig:teaser}
\end{figure}

 
%


\section{Introduction}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/MMDiTvsMMDiTWithConceptAttention.pdf}
    \vspace{-0.22in}
    \caption{\textbf{\tool{} augments multi-modal DiTs with a sequence of concept embeddings that can be used to produce saliency maps.} (Left) An unmodified multi-modal attention (\layername) layer processes both \textcolor{myblue}{\textbf{prompt}} and \textcolor{myorange}{\textbf{image}} tokens. (Right) \tool{} augments these layers without impacting the image appearance to create a set of contextualized \textcolor{mygreen}{\textbf{concept}} tokens.
    }
    \label{fig:mmattn_vs_mmattn_with_concept_attention_explanatory}
\end{figure*}

%
Diffusion models have recently gained widespread popularity, emerging as the state-of-the-art approach for a variety of generative tasks, particularly text-to-image synthesis \cite{rombach_high-resolution_2022}. These models transform random noise into photorealistic images guided by textual descriptions, achieving unprecedented fidelity and detail. Despite the impressive generative capabilities of diffusion models, our understanding of their internal mechanisms remains limited. Diffusion models operate as black boxes, where the relationships between input prompts and generated outputs are visible, but the decision-making processes that connect them are hidden from human understanding.

%
Existing work on interpreting T2I models has predominantly focused on UNet-based architectures \cite{podell_sdxl_2023, rombach_high-resolution_2022}, which utilize shallow cross-attention mechanisms between prompt embeddings and image patch representations. 
UNet \textit{cross attention maps} can produce high-fidelity saliency maps that predict the location of textual concepts \cite{tang_what_2022} and have found numerous applications in tasks like image editing \cite{hertz_prompt--prompt_2022, chefer_attend-and-excite_2023}. However, the interpretability of more recent  multi-modal diffusion transformers (DiTs) remains underexplored. DiT-based models have recently replaced UNets \cite{ronneberger_u-net_2015} as the state-of-the-art architecture for image generation, with models such as Flux \cite{labs_flux_2023} and SD3 \cite{esser_scaling_2024} achieving breakthroughs in text-to-image generation. The rapid advancement and enhanced capabilities of DiT-based models highlight the critical importance of methods that improve their interpretability, transparency, and safety. %

%
In this work, we propose \tool{}, a novel method that leverages the representations of multi-modal DiTs to produce high-fidelity saliency maps that localize textual concepts within images. Our method provides insight into the rich semantics of DiT representations. \tool{} is lightweight and requires no additional training, instead it repurposes the existing parameters of DiT attention layers. \tool{} works by producing a set of rich contextualized text embeddings each corresponding to visual concepts (e.g. ``dragon'', ``sun''). By linearly projecting these \textit{concept embeddings} and the image we can produce rich saliency maps that are even higher quality than commonly used cross attention maps. 


%


%
%


%

%
%


%

%
We evaluate the efficacy of \tool{} in a zero-shot semantic segmentation task on real world images. 
%
We compare our interpretative maps against annotated segmentations to measure the accuracy and relevance of the attributions generated by our method.   Our experiments and extensive comparisons demonstrate that \tool{} provides valuable insights into the inner workings of these otherwise complex black-box models. By explaining the meaning of the representations of generative models our method paves the way for advancements in interpretability, controllability, and trust in generative AI systems. 

In summary, we contribute:

\begin{itemize}
    \item \textbf{\tool{}, a method for interpreting text-to-image diffusion transformers.} Our method requires no additional training, by leveraging the representations of multi-modal DiTs to generate highly interpretable saliency maps that depict the presence of arbitrary textual concepts (e.g. ``dragon'', ``sky'', etc.) in images (as shown in Figure \ref{fig:teaser}). 
    %
    %
    
    \item \textbf{The novel discovery that the output vectors of attention operations produce higher-quality saliency maps than cross attentions.} \tool{} repurposes the parameters of DiT attention layers to produce a set of rich textual embeddings corresponding to different concepts, something that is uniquely enabled by multi-modal DiT architectures. By performing linear projections between these \textit{concept embeddings} and image patch representations in the attention output space we can produce high quality saliency maps. 
    %
    %
    %
    %
    \item  \textbf{\tool{} generalizes to achieve state-of-the-art performance in zero-shot segmentation on benchmarks like ImageNet Segmentation and Pascal VOC}. We achieve superior performance to a diverse set of zero-shot interpretability methods based on various foundation models like CLIP, DINO, and UNet-based diffusion models; this highlights the potential for the representations of DiTs to transfer to important downstream vision tasks like segmentation. 
    
    %
    %
    \item \textbf{We make \tool{} available}, allowing researchers and practitioners to interpret and explore the intricate dynamics of text-to-image diffusion transformers. See code at: \href{https://github.com/helblazer811/ConceptAttention}{https://github.com/helblazer811/ConceptAttention}.
\end{itemize}




%
%
%
%
%
%
%
 %
%
\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/QualitativeExamples.pdf}
    \vspace{-0.1in}
    \caption{\textbf{\tool{} can generate high-quality saliency maps for multiple concepts simultaneously. } Additionally, our approach is not restricted to concepts in the prompt vocabulary. }
    \label{fig:multi_class_qualitative_segmentation}
\end{figure*}

\section{Related Work}

\paragraph{Diffusion Model Interpretability}

A fair amount of existing work attempts to interpret diffusion models. Some works investigate diffusion models from an analytic lens \cite{kadkhodaie_generalization_2024, wang_diffusion_2024}, attempting to understand how diffusion models geometrically model the manifold of data. Other works attempt to understand how models memorize images \cite{carlini_extracting_2023}. An increasing body of work attempts to repurpose the representations of diffusion models for various tasks like classification \cite{li_your_2023}, segmentation \cite{karazija_diffusion_2024}, and even robotic control \cite{gupta_pre-trained_2024}. However, most relevant to our work is the substantial body of methods investigating how the representations of the neural network architectures underpinning diffusion can be used to garner insight into how these models work, steer their behavior, and improve their safety. 

Numerous papers have observed that the cross attention mechanisms of UNet-based diffusion models like Stable Diffusion \cite{rombach_high-resolution_2022} and SDXL \cite{podell_sdxl_2023} can produce interpretable saliency maps of textual concepts \cite{tang_what_2022}. Cross attention maps are used in a variety of image editing tasks like producing masks that localize objects of interest to edit \cite{dalva_fluxspace_2024}, controlling the layout of images \cite{chen_training-free_2023, epstein_diffusion_2023}, altering the appearance of an image but retaining its layout \cite{hertz_prompt--prompt_2022}, and even generating synthetic data to train instruction based editing models \cite{brooks_instructpix2pix_2023}. Other works observe that performing interventions on cross attention maps can improve the faithfulness of images to prompts by ensuring attributes are assigned to the correct objects \cite{meral_conform_2024, chefer_attend-and-excite_2023}.  Additionally, it has been observed that self-attention layers of diffusion models encode useful information about the layout of images \cite{liu_towards_2024}. 

%


\paragraph{Zero-shot Image Segmentation}
In this work, we evaluate \tool{} on the task of zero-shot image segmentation, which is a natural way to assess the accuracy of our saliency maps and the transferability of the representations of multi-modal DiT architectures to downstream vision tasks. This task also provides a good setting to compare to a variety of other interpretability methods for various foundation model architectures like CLIP \cite{radford_learning_2021}, DINO \cite{caron_emerging_2021}, and diffusion models.  

A variety of works train a diffusion models from scratch for the task of image segmentation \cite{amit_segdiff_2022, karazija_diffusion_2024} or attempt to fine-tune pretrained models \cite{baranchuk_label-efficient_2022}. Another line of work leverages diffusion models to generate synthetic data that can be used to train segmentation models that transfer zero-shot to new classes \cite{li_open-vocabulary_2023}. While effective, these methods are training-based and thus do not provide as much insight into the representations of existing text-to-image generation models, which is the key motivation behind \tool{}. 

A significant body of work attempts to improve the interpretability of CLIP vision transformers (ViTs) \cite{dosovitskiy_image_2021}. The authors of \cite{chefer_transformer_2021} develop a method for generating saliency maps for ViT models, and they introduce an evaluation protocol for assessing the effectiveness of these saliency maps. This evaluation protocol centers around the ImageNet-Segmentation dataset \cite{guillaumin_imagenet_2014}, and we extend this evaluation to the PascalVOC dataset \cite{everingham_pascal_2015}. They compare to a variety of zero-shot interpretability methods like GradCAM \cite{selvaraju_grad-cam_2019}, Layerwise-Relevance Propagation \cite{binder_layer-wise_2016}, raw attentions, and the Rollout method \cite{abnar_quantifying_2020}. The authors of \cite{gandelsman_interpreting_2024} demonstrate an approach to expressing image patches in terms of textual concepts. We also compare our approach to zero-shot diffusion based methods \cite{tang_what_2022, wang_diffusion_2024} and the self-attention maps of DINO ViT models \cite{caron_emerging_2021}. 

Another line of work attempts perform unsupervised segmentation without any class or text conditioning by performing clustering of the embeddings of models \cite{cho_picie_2021, hamilton_unsupervised_2022, tian_diffuse_2024}. Despite not producing class predictions, these models are often evaluated on semantic segmentation datasets by using approaches like Hungarian matching \cite{kuhn_hungarian_1955} to pair unlabeled segmentation predictions with the best matching ones in a multi-class semantic segmentation dataset. In contrast, \tool{} enables text conditioning so we do not compare to this family of methods. We also don't compare to models like SAM \cite{kirillov_segment_2023, ravi_sam_2024} as it is trained on a large scale dataset.  %


\section{Preliminaries}
\subsection{Rectified-Flow Models for Image Generation}

Flux and Stable Diffusion 3 leverage multi-modal DiTs that are trained to parameterize rectified flow models. Throughout this paper we may refer to rectified flow models as diffusion models for convenience. These models attempt to generate realistic images from noise that correspond to given text prompts. Flow based models \cite{lipman_flow_2023} attempt to map a sample $x_1$ from a noise distribution $p_1$, typically $p_1 \sim \mathcal{N}(0, I)$, to a sample $x_0$ in the data distribution. Rectified flows \cite{liu_flow_2022} attempt to learn ODEs that follow straight paths between the $p_0$ and $p_1$, i.e. 
\begin{equation}
    z_t = (1 - t) x_0 + t \epsilon, \epsilon \sim \mathcal{N}(0, 1).
\end{equation}
Flux and SD3 are trained using a conditional flow matching objective which can be expressed conveniently as
\begin{equation}
    -\frac{1}{2} \mathbb{E}_{t \sim \mathcal{U}(t), \epsilon \sim \mathcal{N}(0, I) }[w_t \lambda_t'||\epsilon_\Theta(z_t, t) - \epsilon||^2]
\end{equation}
where $\lambda_t'$ corresponds to a signal-to-noise ratio and $w_t$ is a time dependent-weighting factor. Above $\epsilon_\Theta(z_t, t)$ is parameterized by a multi-modal diffusion transformer network. The architecture of this model and it's properties is of primary interest in this work. 

%

%
%

%

\subsection{The Anatomy of a Multi-modal DiT Layer}

Multi-modal DiTs like Flux and Stable Diffusion 3 leverage \textit{multi-modal attention layers (\layername)} that process a combination of textual tokens and image patches. There are two key classes of layers: one that keeps separate residual streams for each modality and one that uses a single stream. In this work, we take advantage of the properties of these dual stream layers, which we refer to as multi-modal attention layers (\layername s). 

The input to a given layer is a sequence of image patch representations $x \in \mathbb{R}^{h \times w \times d}$ and prompt token embeddings $p \in \mathbb{R}^{l \times d}$. The initial prompt embeddings at the beginning of the network are formed by taking the T5 \cite{raffel_exploring_2023} embeddings of the prompt tokens.
%

Following \cite{peebles_scalable_2023}, each \layername{} layer leverages a set of adaptive layer norm \textit{modulation layers}  \cite{xu_understanding_2019}, conditioned on the time-step and global CLIP vector. An adaptive layernorm operation is applied to the input image and text embeddings. 
%
%
%
%
The final modulated outputs are then residually added back to the original input. Notably, the image and text modalities are kept in separate residual streams. The exact details of this operation are omitted for brevity. 

The key workhorse in \layername{} layers is the familiar multi-head self attention operation. The prompt and image embeddings have separate learned key, value, and query projection matrices which we refer to as $K_x, Q_x, V_x$ for images and $K_p, Q_p, V_p$ for text. The keys, queries, and values for both modalities are collectively denoted $q_{xp}$, $k_{xp}$, and $v_{xp}$, where for example $k_{xp} = [K_x x_1, \dots, K_p p_1 \dots]$. 
A self attention operation is then performed 
\begin{equation}
    o_x, o_p = \softmax(q_{xp} k_{xp}^T) v_{xp}
\end{equation}
Here we refer to $o_x$ and $o_p$ as the \textit{attention output} vectors. Another linear layer is then applied to these outputs and added to a separate residual streams weighted according to the output of the modulation layer. This gives us updated embeddings $x^{L+1}$ and $p^{L + 1}$ which are given as input to the next layer. 

%


%
%
%
%
%

\section{Proposed Method: \tool{}}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/Concept_Attention.pdf}
    \vspace{-0.3in}
    \caption{(a) \layername{} combines cross and self attention operations between the prompt and image tokens. (b) Our \tool{} allows the concept tokens to incorporate information from other concept tokens and the image tokens, but not the other way around. }
    \label{fig:multi_modal_attention_vs_concept_attention}
\end{figure}

We introduce \tool{}, a method for generating high quality saliency maps depicting the location of textual concepts in images. \tool{} works by creating a set of contextualized \textit{concept embeddings} for simple textual concepts (e.g. ``cat'', ``sky'', etc.). These concept embeddings are sequentially updated alongside the text and image embeddings, so they match the structure that each \layername{} layer expects. However, unlike the text prompt, concept embeddings do not impact the appearance of the image. We can produce high-fidelity saliency maps by projecting image patch representations onto the concept embeddings. \tool{} requires no additional training and has minimal impact on model latency and memory footprint.  A high level depiction of our methodology is shown in Figure \ref{fig:mmattn_vs_mmattn_with_concept_attention_explanatory}.

%

%


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ThreeImageSegmentation.pdf}
    \vspace{-0.2in}
    \caption{\tool{} produces higher fidelity raw scores and saliency maps than alternative methods, sometimes surpassing in quality even the ground truth saliency map provided by the ImageNet-Segmentation task. Top row shows the soft predictions of each method and the bottom shows the binarized predictions. }
    \label{fig:segmentation_qualitative_results}
\end{figure*}

\subsection{Using \tool{}}

%
%
%
%
%
%
%

The user specifies a set of $r$ single token concepts, like ``cat'', ``sky'', etc. which are passed through a T5 encoder to produce an initial embedding $c^0$ for each concept. 
%
%
For each \layername{} layer (indexed by $L$) we layer-normalize the input concept embeddings $c^L$ and repurpose the text prompt's projection matrices (i.e. $K_p, Q_p, V_p$), to produce a set of keys, values, and queries 
\begin{align}
    k_c &= [K_p c_1 , \dots K_p c_k] \\ 
    q_c &= [Q_p c_1 , \dots Q_p c_k] \\
    v_c &= [V_p c_1 , \dots V_p c_k]
\end{align}
each in $\mathbb{R}^{r \times d}$. 

\paragraph{One-directional Attention Operation}
We would like to update our concept embeddings so they are compatible with subsequent layers, but also prevent them from impacting the image tokens. 
%
%
%
Let $k_x$ and $v_x$ be the keys and values of the image patches $x$ respectively. We can concatenate the image and concept keys to get
\begin{equation}
    k_{xc} = [K_x x_1 \dots, K_x x_n, K_p c_1 \dots, K_p c_r]
\end{equation}
and the image and concept values to get
\begin{equation}
    v_{xc} = [V_x x_1 \dots, V_x x_n, V_p c_1 \dots, V_p c_r]
\end{equation}
We can then perform the following attention operation 
\begin{equation}
    o_{c} = \softmax(q_{c} k_{xc}^T) v_{xc}
\end{equation}
which produces a set of \textit{concept output embeddings}. 

Notice, that instead of just performing a cross attention (i.e. $\softmax(q_{c} k_{x}^T) v_{x}$) our approach leverages both cross attention from the image patches to the concepts and self attention among the concepts. We found that performing both operations improves performance on downstream evaluation tasks like segmentation (See Table \ref{tab:cross_self_ablation}).  We hypothesize this is because it allows the concept embeddings to repel from each other, avoiding redundancy between concepts.

Meanwhile, the image patch and prompt tokens ignore the concept tokens and attend only to each other as in 
\begin{equation}
    o_x, o_p = \softmax(q_{xp} k_{xp}^T) v_{xp} .
\end{equation}

%

%

A diagram of these operations is shown in \cref{fig:multi_modal_attention_vs_concept_attention}(b).

%

\paragraph{A Concept Residual Stream}

The above operations create a residual stream of concept embeddings distinct from the image and patch embeddings. Following the pretrained transformer's design, after the \layername{} we apply another projection matrix $P$ and MLP, adding the result residually to $c^{L}$.
%
%
We apply an adaptive layernorm at the end of attention operation which outputs several values: a scale $\gamma$, shift $\beta$, and some gating values $\alpha_1$ and $\alpha_2$. The residual stream is then updated as
\begin{align}
    c^{L + 1} & \gets c^L + \alpha_1 (P o_{c}) \\
    c^{L + 1} & \gets c^{L+1} + \alpha_2 \MLP\bigg(( 1 + \gamma) \lnorm(c^{L + 1}) + \beta\bigg)
\end{align}
where $\gets$ denotes assignment. The parameters from each of our modulation, projection, and MLP layers are the same as those used to process the text prompt. 

\paragraph{Saliency Maps in the Attention Output Space}
These concept embeddings can be combined with the image patch embeddings to produce saliency maps for each layer $L$. Specifically, we found that taking a simple dot-product similarity between the image output vectors $o_x$ and concept output vectors $o_c$ produces high-quality saliency maps

%
%
\begin{equation}
\label{eq:dot-prod-saliency}
    \phi(o_x, o_c) = \softmax(o_x o_c^T ).
\end{equation}
this is in contrast to cross attention maps which are between the image keys $k_x$ and prompt queries $q_p$.

%
We can aggregate the information from multiple layers by averaging them $\frac{1}{|L|} \sum_{L = 1}^{|L|} \phi(o_x^L, o_c^L)$ where $|L|$ denotes the number of \layername{} layers (Flux has $|L| = 18$). These attention output space maps are unique to MM-DiT models as they leverage \textit{concept embeddings} corresponding to textual concepts which fundamentally can not be produced by UNet-based models.

\subsection{Limitations of Raw Cross Attention Maps}

For multi-modal DiT architectures, we could additionally consider using the raw cross attention maps  
\begin{equation}
    \phi(k_x, q_p) = \softmax(q_p k_x^T) 
\end{equation}
to produce saliency maps. However, these have a key limitation in that their vocabulary is limited to the tokens in the user's prompt. Unlike UNet-based models, multi-modal DiTs sequentially update a set of prompt embeddings with each \layername{} layer. This makes it difficult to produce cross attention maps for an open-set of concepts, as you would need to add the concept to the prompt sequence which would then change the appearance of the image. \tool{} overcomes this key limitation, and makes the additional discovery that the output space of attention mechanisms produces high-fidelity saliency maps. 

%

%

%

%
%
%
%
%

%

%


%


%




%


%




%


%



%

%

%

%

%

%
%
%
%
%
%


%
%
%


%


%

%


%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%

%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%


%

%

%

%

%

%

%
%


%

%

%

%


%

%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

 %

%
%
%
%
%
%
%



\begin{table*}[t!]
    \centering
    \small
    \begin{tabular}{p{0.6\columnwidth}p{0.24\columnwidth}p{0.12\columnwidth}p{0.12\columnwidth}p{0.12\columnwidth}p{0.12\columnwidth}p{0.12\columnwidth}p{0.12\columnwidth}}
        \toprule
        & &  \multicolumn{3}{c}{ImageNet-Segmentation} &  \multicolumn{3}{c}{PascalVOC (Single Class) } \\ 
        Method & Architecture & Acc $\uparrow$ & mIoU$\uparrow$ & mAP$\uparrow$ & Acc $\uparrow$ & mIoU$\uparrow$ & mAP$\uparrow$  \\
        \midrule
        LRP  \cite{binder_layer-wise_2016}    & CLIP ViT & 51.09 & 32.89 & 55.68 & 48.77 & 31.44 & 52.89 \\
        Partial-LRP \cite{binder_layer-wise_2016} & CLIP ViT & 76.31 & 57.94 & 84.67 & 71.52 & 51.39 & 84.86 \\
        Rollout \cite{abnar_quantifying_2020} & CLIP ViT & 73.54 & 55.42 & 84.76 & 69.81 & 51.26 & 85.34 \\
        ViT Attention \cite{dosovitskiy_image_2021} & CLIP ViT & 67.84 & 46.37 & 80.24 & 68.51 & 44.81 & 83.63 \\
        GradCAM \cite{selvaraju_grad-cam_2020} & CLIP ViT & 64.44 & 40.82 & 71.60 & 70.44 & 44.90 & 76.80 \\
        TextSpan  \cite{gandelsman_interpreting_2024} & CLIP ViT & 75.21 & 54.50 & 81.61 & 75.00 & 56.24 & 84.79 \\
        TransInterp \cite{chefer_transformer_2021}  & CLIP ViT & 79.70 & 61.95 & 86.03 & 76.90 & 57.08 & 86.74 \\
        %
        DINO Attention \cite{caron_emerging_2021} & DINO ViT & 81.97 & 69.44 & 86.12 & 80.71 & 64.33 & 88.90 \\
        %
        %
        DAAM \cite{tang_what_2022} & SDXL UNet & 78.47 & 64.56 & 88.79 & 72.76 & 55.95 & 88.34 \\
        DAAM \cite{tang_what_2022} & SD2 UNet & 64.52 & 47.62 & 78.01 & 64.28 &  45.01 & 83.04 \\
        %
        %
        %
        Flux Cross Attention & Flux DiT & 74.92 & 59.90 & 87.23 & 80.37 & 54.77 & 89.08 \\
        %
        %
        %
        \rowcolor[HTML]{FBEAE4} \tool{} & Flux DiT & \textbf{83.07} & \textbf{71.04} & \textbf{90.45} & \textbf{87.85} & \textbf{76.45} & \textbf{90.19} \\
        %
        \bottomrule

    \end{tabular}
    \caption{\tool{} outperforms a variety of Diffusion, DINO, and CLIP ViT interpretability methods on ImageNet-Segmentation and PascalVOC (Single Class).}
    \label{tab:imagenet_segmentation}
\end{table*}

\begin{table}
    \centering
    \small
    \begin{tabular}{p{0.15\columnwidth} p{0.15\columnwidth} p{0.1\columnwidth}p{0.1\columnwidth}p{0.09\columnwidth}}
        \toprule
        \textbf{Space} & \textbf{Softmax} & \textbf{Acc$\uparrow$} & \textbf{mIoU$\uparrow$} & \textbf{mAP$\uparrow$} 
        \\
        \midrule 
        CA & & 66.59 & 49.91 & 73.17 \\
        CA & \centering \checkmark & 74.92 & 59.90 & 87.23 \\
        Value & & 45.93 & 29.81 & 65.79  \\
        Value & \centering \checkmark & 45.78 & 29.68 & 39.61 \\
        \rowcolor[HTML]{FBEAE4} Output & & 78.75 & 64.95 & 88.39 \\
        \rowcolor[HTML]{FBEAE4} Output & \centering \checkmark & \textbf{83.07} & \textbf{71.04} & \textbf{90.45} \\
        %
        %
        \bottomrule

    \end{tabular}
    \caption{\textbf{The output space of DiT attention layers produces more transferable representations than cross attentions. } We explore the transferability of several representation spaces of a DiT: the cross attentions (CA), the value space, and the output space. We performed linear projections of the image patches and concept vectors in each of these spaces and evaluated their performance on ImageNet-Segmentation. }
    \label{tab:representation_space_imagenet_segmentation}
\end{table}



\begin{table}
    \centering
    \small
    \begin{tabular}{p{0.1\columnwidth} p{0.1\columnwidth} p{0.1\columnwidth}p{0.1\columnwidth}p{0.09\columnwidth}}
        \toprule
        \centering \textbf{CA} & \centering \textbf{SA} & \textbf{Acc$\uparrow$} & \textbf{mIoU$\uparrow$} & \textbf{mAP$\uparrow$} 
        \\
        \midrule 
         &  &  52.63 &  35.72 & 70.21 \\
         & \centering \checkmark & 51.68 & 34.85 & 69.36 \\
        \centering \checkmark &  & 76.51  & 61.96 & 86.73 \\
        \rowcolor[HTML]{FBEAE4} \centering \checkmark & \centering\checkmark & 
        \textbf{83.07} & \textbf{71.04} & \textbf{90.45}\\
        \bottomrule 
        
    \end{tabular}
    \caption{\textbf{\tool{} performs best when we utilize both cross and self attention.} We tested the effectiveness of performing just a cross attention operation between the concepts and image tokens, just a self attention among the concepts, both cross and self attention, and neither. We found that doing both operations leads to the best results. Metrics are computed on the ImageNet Segmentation benchmark.}
    \label{tab:cross_self_ablation}
\end{table}


\begin{table}
    \centering
    \small
    \begin{tabular}{p{0.36\columnwidth} p{0.1\columnwidth} p{0.1\columnwidth}}
        \toprule
        \textbf{Method} & \textbf{Acc$\uparrow$} & \textbf{mIoU$\uparrow$}
        %
        \\
        \midrule 
        TextSpan  & 73.84 & 38.10\\
        DAAM  & 62.89 & 10.97 \\
        Flux Cross Attention  & 79.52 & 27.04 \\
        \rowcolor[HTML]{FBEAE4} \tool{} & \textbf{86.99} & \textbf{51.39} \\
        \bottomrule 
        
    \end{tabular}
    \caption{\textbf{\tool{} outperforms alternative methods on images with multiple classes from PascalVOC.} Notably, the margin between \tool{} and other methods is even higher for this task than when a single class is in each image. }
    \label{tab:multi_class_pascal_voc}
\end{table}


\section{Experiments}

\subsection{Zero-shot Image Segmentation}

We are interested in investigating (1) the efficacy of \tool{} to generate highly localized and semantically meaningful saliency maps, and (2) understand the transferability of multi-modal DiT representations to important downstream vision tasks. Zero-shot image segmentation is a natural choice for achieving these goals. 

\paragraph{Datasets} 

We leverage two key datasets zero-shot image segmentation datasets. First, we use a commonly used \cite{gandelsman_interpreting_2024, chefer_transformer_2021} zero-shot segmentation benchmark called ImageNet-Segmentation \cite{guillaumin_imagenet_2014}. It is composed of 4,276 images from 445 categories. Each image primarily depicts a single central object or concept, which makes it a good method for comparing \tool{} to a variety of methods which generate a single saliency map that are unable to generate class-specific segmentation maps. For the second dataset we leverage PascalVOC 2012 benchmark \cite{everingham_pascal_2015}. We investigate both a single class (930 images) and multi-class split (1,449 images) of this dataset. Many methods (e.g. DINO) do not condition their saliency map on class, so for these methods we restrict our evaluation to examples only containing a single class and the background. For methods that can accept text as conditioning we evaluate on the full dataset.  

%

\paragraph{Key Baseline Methods}

We compare our approach to a variety of zero-shot interpretability methods which leverage several different multi-modal foundation models. We compare to numerous interpretability methods compatible with CLIP: Layerwise Relevance Propagation (LRP) \cite{binder_layer-wise_2016}, LRP on just the final-layer of a ViT (Partial-LRP), Attention Rollout (Rollout) \cite{abnar_quantifying_2020}, Raw Vision Transformer Attention (ViT Attention) \cite{dosovitskiy_image_2021}, GradCAM \cite{selvaraju_grad-cam_2019}, TextSpan \cite{gandelsman_interpreting_2024}, and the Transformer Attribution method from \cite{chefer_transformer_2021} (TransInterp). We also compare to a UNet-based interpretability method that aggregates information from UNet cross attention layers called DAAM \cite{tang_what_2022} on both Stable Diffusion XL \cite{podell_sdxl_2023} and Stable Diffusion 2. Finally, we compare to the raw cross attention maps produced by Flux DiT models. 

\paragraph{Implementation Details}

For all of our experiments we use the Flux DiT architecture implemented in PyTorch \cite{paszke_pytorch_2019}. In particular, we use the distilled Flux-Schnell model. We encode real images with the DiT by first mapping them to the VAE latent space and then adding varying degrees of Gaussian noise before passing them through the Flux DiT. We then cache all of the concept output $o_c$ and image output vectors $o_x$ from each \layername{} layer. At the end of generation we then construct our concept saliency maps for each layer and average them over all layers of interest. In our experiments we leverage the activations from the last 10 of the 18 \layername{} layers. 

\paragraph{Single Object Image Segmentation}

For our first task we closely follow the established evaluation framework from \cite{gandelsman_interpreting_2024} and \cite{chefer_transformer_2021}. We perform this evaluation setup on both ImageNet-Segmentation and a subset of 930 PascalVOC images containing only a single class. For each method we assume the class present in the image is known and use simplified descriptions of each ImageNet class (e.g. ``Maltese dog'' $\to$ ``dog) this allows the concepts to be captured by a single token. We construct a concept vocabulary for each image composed of this target class and a set of fixed background concepts for all examples (e.g. ``background'', ``grass'', ``sky''). 

\paragraph{Quantitative Evaluation}
Each method produces a set of scalar \textit{raw scores} for each image patch which we then threshold based on the mean value to produce a binary segmentation prediction. We compare each method using standard segmentation evaluation metrics, namely: mean Intersection over Union (mIoU), patch/pixelwise accuracy (Acc), and mean Average Precision (mAP). Accuracy alone is an insufficient metric as our dataset is highly imbalanced, mIoU is significantly better, and mAP captures threshold agnostic segmentation capability. We found that \tool{} significantly out performs all of the baselines we tested across all three metrics (See \cref{tab:imagenet_segmentation}). This is true for diffusion, CLIP, and DINO based interpretability methods. 

\paragraph{Qualitative Evaluation} 

We also show qualitative results comparing the segmentation performance to each baseline in Figure \ref{fig:segmentation_qualitative_results}. We also provide more qualitative results in Appendix \ref{QualitativeAppendix}. It is worth noting that the qualitative segmentation results highlight (a) the ambiguity of zero-shot image segmentation, and (b) the limitations of human data annotation. For example, Figure \ref{fig:segmentation_qualitative_results} shows that our method does not segment the part of the dog between the ears and it's body, while the data annotation does. 

%
%

%
%

\paragraph{Multi Object Image Segmentation}

We also wanted to evaluate the capabilities of our method at differentiating between multiple classes in an image. However, only a subset of methods produce distinct saliency maps for open ended classes. For this experiment we compare to DAAM using a SDXL backbone, TextSpan using a CLIP backbone, and the raw cross attentions of Flux. Instead of binarizing the image to produce segmentations, for each patch we predict the concept with the highest score. We used pixewlise accuracy and mIoU as our evaluation metrics and found that our method significantly out performed the baselines (See Table \ref{tab:multi_class_pascal_voc}). We also show qualitative results of our approach differentiating between multiple concepts in a single image in Figures \ref{fig:teaser}, \ref{fig:multi_class_qualitative_segmentation} and we show more results in Appendix \ref{QualitativeAppendix}. 

%

%

%

%


\subsection{Ablation Studies}


We perform several ablation studies to investigate the impact of various architectural choices and hyperparameters on the performance of \tool{}. 

\paragraph{Impact of Layer Depth on Segmentation}

We hypothesized that deeper \layername{} layers in the DiT would have more refined representations that better transfer to segmentation. This was confirmed by our evaluation (see Figure \ref{fig:per_layer_metrics}). We pull features from each diffusion layer and evaluated the segmentation performance of these features on ImageNet Segmentation. We also compare the performance of combining all layers simultaneously, which we found performs better than any individual layer.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/per_layer_metrics.pdf}
    \vspace{-0.24in}
    \caption{\textbf{Later \layername{} layers encode richer features for zero-shot segmentation. } We investigated the impact of using features from various \layername{} layers and found that deeper layers lead to better performance on segmentation metrics like pixelwise accuracy, mIoU, and mAP. We also found that combining the information from all layers further improves performance. }
    \label{fig:per_layer_metrics}
\end{figure}

\paragraph{Impact of Diffusion Timestep on Segmentation}

We add Gaussian noise to encoded images before passing them to the DiTs, this conforms with the expectations of the models. Intuitively one might expect the later timesteps (less noise) to have much higher segmentation performance as less information about the original image is corrupted. However, we found that the middle diffusion timesteps best (See \cref{fig:per_time_metrics}). Throughout the rest of our experiments we use timestep 500 out of 1000 following this result. 

%

%


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/per_time_metrics.pdf}
    \vspace{-0.24in}
    \caption{\textbf{Optimal segmentation performance requires some noise to be present in the image. } We evaluated the performance of \tool{} by encoding samples from a variety of timesteps (determines the amount of noise). Interestingly, we found that the optimal amount of noise was not zero, but in the middle to later stages of the noise schedule. }
    \label{fig:per_time_metrics}
\end{figure}

%

\paragraph{Concept Attention Operation Ablations}

We compared the performance on the ImageNet Segmentation benchmark of performing (a) just cross attention from the image patches to the concept vectors, (b) just self attention, (c) no attention operations, and (d) both cross and self attention. Our results seen in Table \ref{tab:cross_self_ablation} indicate that using a combination of both cross and self attention achieves the best performance. 

We also investigated the impact of applying a pixelwise softmax operation over our predicted segmentation coefficients. We found that it slightly improves segmentation performance in the attention output space and significantly improves performance for the cross attention maps (see Table \ref{tab:representation_space_imagenet_segmentation}. 


%
%
%
%
%
%
%





%
%
%



%


%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%

%

%

%


%

%



 %
\section{Conclusion}

We introduce \tool{}, a method for interpreting the rich features of multi-modal DiTs. Our approach allows a user to produce high quality saliency maps of an open-set of textual concepts that shed light on how a diffusion model ``sees'' an image. We perform an extensive evaluation of the saliency maps on zero-shot segmentation and find that they significantly outperform a variety of other zero-shot interpretability methods. Our results suggest the potential for DiT models to act as powerful and interpretable image encoders with representations that are transferable zero-shot to tasks like image segmentation. 

%

\section{Impact Statement}

Generative models for images have numerous ethical concerns: they have the potential to spread misinformation through realistic fake images (i.e. deepfakes), they may disrupt different creative industries, and have the potential to reinforce existing social biases present in their training data. Our work directly serves to improve the transparency of these models, and we believe our work could be used to understand the biases present in models. 

%

\section{Acknowledgments}

This paper is supported by the National Science Foundation Graduate Research Fellowship. This work was also supported in part by Cisco, NSF \#2403297, gifts from Google, Amazon, Meta, NVIDIA, Avast, Fiddler Labs, Bosch.  
%
%
%
\bibliography{references}
\bibliographystyle{template/icml2024}


%
%
%
%
%
%
\newpage
\appendix
\onecolumn

\section{More In-depth Explanation of Concept Attention}

We show pseudo-code depicting the difference between a vanilla multi-modal attention mechanism and a multi-modal attention mechanism with concept attention added to it. 

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/CodeDifference.pdf}
    \vspace{-0.3in}
    \caption{\textbf{Pseudo-code depicting the (a) multi-modal attention operation used by Flux DiTs and (b) our \tool{} operation.} We leverage the parameters of a multi-modal attention layer to construct a set of contextualized concept embeddings. The concepts query the image tokens (cross-attention) and other concept tokens (self-attention) in an attention operation. The updated concept embeddings are returned in addition to the image and text embeddings. }
    %
    \label{fig:concept_attention_code}
\end{figure*}

\newpage
\section{More Qualitative Results}
\label{QualitativeAppendix}

Here we show a variety of qualitative results for our method that we could not fit into the original paper. 

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/supplemental_imagenet_segmentations/QualitativeComparisonFigure.png}
    \vspace{-0.2in}
    \caption{A qualitative comparison between our method and several others. }
    \label{fig:enter-label}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/supplemental_imagenet_segmentations/QualitativeComparisonFigure2.pdf}
    \vspace{-0.2in}
    \caption{A qualitative comparison between our method and several others. }
    \label{fig:enter-label}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/supplemental_imagenet_segmentations/QualitativeComparisonFigure3.pdf}
    \vspace{-0.2in}
    \caption{A qualitative comparison between our method and several others. }
    \label{fig:enter-label}
\end{figure*}




%
%
%
%
%
%
%

%
%
%
%
%
%
%



%
%
%
%
%
%


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/supplemental_imagenet_segmentations/supplemental_6.png}
    \caption{A qualitative comparison between numerous baselines on ImageNet Segmentation Images. The top row shows the soft predictions of each method and the bottom shows the binarized segmentation predictions. }
    \label{fig:enter-label}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/supplemental_imagenet_segmentations/supplemental_3.png}
    \caption{A qualitative comparison between numerous baselines on ImageNet Segmentation Images. The top row shows the soft predictions of each method and the bottom shows the binarized segmentation predictions. }
    \label{fig:enter-label}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/supplemental_imagenet_segmentations/supplemental_4.png}
    \caption{A qualitative comparison between numerous baselines on ImageNet Segmentation Images. The top row shows the soft predictions of each method and the bottom shows the binarized segmentation predictions. }
    \label{fig:enter-label}
\end{figure*}


%
%
%
%
%
%
 
%
%


\end{document}
