
\subsection{Rigidity Lower Bound via Spectral Method}

Our lower bound is built on the spectral method that was previously used to bound the bounded rigidity $\R_A^{\C}(r, \theta)$. To explain the high-level idea of this method, suppose the matrix $A$ is approximated by a low-rank matrix $L$ in the bounded sense; that is, $L$ differs from $A$ in at most $s$ entries, with the magnitude of the difference in each differing entry pair bounded by $\theta$. Then, the Hamming distance $s$ between $L$ and $A$ can be lower bounded by the Frobenius distance as $\norm{A -L}_F^2 \le \theta^2 s$, which can be further analyzed using spectral tools.

There are three technical challenges of using the spectral method to prove a nearly tight regular or Boolean rigidity lower bound $\boolRigidity{A}{r} \ge N^2 \cdot (1/2 - o(1))$ over a finite field $\F_p$:
\begin{itemize}
  \item First, the Frobenius distance between matrices is defined only over the field $\C$.
  \item Second, to make the Frobenius distance upper bounded by the Hamming distance, we crucially rely on the fact that the difference between $A$ and $L$ is entry-wisely bounded by $\theta$ in magnitude. In particular, since $\norm{A -L}_F^2$ scales with both $s$ and $\theta$, even small constant factor changes in $\theta$ can make it difficult to achieve the leading constant of $1/2$ in the sparsity $s$ that we aim to achieve in our lower bound.
  In particular, to get the exact coefficient $1/2$ in prior work, we require $\theta$ to be at most $2$, which is a strong requirement.
  \item Finally, to get a lower bound on the Frobenius distance between $A$ and a rank-$r$ matrix $L$, prior work needs to use that the low-rank decomposition $U, V$ of $L$, i.e., the two $r \times n$ matrices such that $L = U^{\top}V$, have each entry bounded as well. This does not immediately follow, even from bounded rigidity upper bounds, but this additional property was obtained via a tool from convex geometry called John's Theorem \cite{john2014extremum} in the previous state-of-the-art work \cite{rashtchian2016bounded}. (We will ultimately replace this with a \emph{simpler} bound in the finite field setting.)
\end{itemize}

In this work, we show, surprisingly, that we can address these three challenges at the same time by a new, powerful generic transformation from a low-rank approximation over $\F_p$ to a low-rank approximation over $\C$. Specifically, for a matrix $A \in \BK{-1,1}^{N \times N}$, if it can be approximated by a rank-$r$ matrix $L$ over $\F_p$, then we construct another matrix $\tilde{L} \in \BK{-1,1}^{N \times N}$ with rank $\tilde{r}$ over $\C$ that approximates $A$ within the same accuracy as $L$.

As each entry of $\tilde{L}$ is either $1$ or $-1$, $\tilde{L}$ approximates $A$ in the bounded sense with $\theta = 2$, which will allow us to make use of some tools from the spectral approach of prior work. Moreover, we prove that the matrix $\tilde{L}$ admits a natural low-rank decomposition $U, V \in \C^{r\times n}$ such that each entry of them is bounded by $2^{O(r)}$. Defining $\tilde{L}$ and proving this requires a careful polynomial interpolation in conjunction with other algebraic manipulations. With all these properties, we can apply a variant on the spectral method on $\tilde{L}$ to lower bound the error by $N^2 (1/2 - o(1))$.




\subsection{Hardness Amplifications for Kronecker Product}
\label{subsec:overview_amplification_kro}
In this subsection, we will overview our techniques for hardness amplification. 
Suppose we are given a rank-$r$ matrix $L$ that approximates the matrix $A \in \BK{-1, 1}^{q \times q}$ within a constant error rate $\delta$. Our goal is to construct a matrix $\tilde{L} \in \F_p^{q^n \times q^n}$ from $L$ without a large blow up in the rank, such that $\tilde{L}$ approximates $\kro{A}$ with an accuracy $(1/2 + 2^{-O(n)})$, meaning that $\tilde{L}$ agree with $\kro{A}$ on $(1/2 + 2^{-O(n)})$-fraction of entries.

The most natural way for constructing $\tilde{L}$ is to consider $\kro{L}$. As $L$ is a low-rank approximation of ${A}$, we can think of $\kro{L}$ as approximating $\kro{A}$ within certain accuracy. Following some careful calculation, one can check $\kro{L}$ does approximate $\kro{A}$ with an accuracy $(1/2 + 2^{-O(n)})$. The only issue is that $\kro{L}$ has a much higher rank $r^n$ than what we want.

In this paper, we construct a matrix $\tilde{L}$ with a much lower rank $2nr$, without harming the asymptotic accuracy $(1/2 + 2^{-O(n)})$.
To define the low-rank approximation $\tilde{L}$ of $\kro{A}$, we consider $\kro{L}$ as an intermediate matrix and intuitively think of giving the approximation in two steps:
\begin{itemize}
    \item $\kro{L}$ approximates $\kro{A}$ with an accuracy $(1/2 + 2^{-O(n)})$, although the rank of $\kro{L}$ is high.
    \item Let $\pi: \F_p^n \to \F_p$ be the multiplication function, i.e., $\pi(z_1, \ldots, z_n) \defeq z_1 \cdots z_n$. Then, each entry of $\kro{L}$ can be represented as
        $\kro{L}[x,y] = \pi\bk*{L[x_1, y_1], \ldots, L[x_n,y_n]}.$
     We can further use a degree-$1$ polynomial $\tilde{\pi}$ to approximate the high-degree function $\pi$. Inspired by the fact that when $z_1, \ldots, z_n \in \BK{-1,1}$, the function $\pi(z_1, \ldots, z_n)$ counts the parity of the number of $-1$'s among its inputs, we construct $\tilde{\pi}$ using a known construction of probabilistic polynomials for the PARITY function~\cite{razborov1987lower, smolensky1987algebraic}. Using it, we define a low-rank matrix $\tilde{L}$ by 
    $
        \tilde{L}[x,y] = \tilde{\pi}\bk*{L[x_1, y_1], \ldots, L[x_n,y_n]}.
    $
    This low-rank matrix $\tilde{L}$ approximates $\kro{L}$ within a certain accuracy.
\end{itemize}
Now as $\kro{L}$ approximates $\kro{A}$, and $\tilde{L}$ further approximates $\kro{L}$ in some sense, it would be natural to use a union bound to conclude that $\tilde{L}$ approximates $\kro{A}$ well. However, both parts of the approximations are actually very weak: for example, for the second part of the approximation, the best degree-1-polynomial approximation $\tilde{\pi}$ for $\pi$ can only achieve an accuracy of $(1/2 + o(1))$, meaning that $\tilde{L}$ only agrees with $\kro{L}$ on a $(1/2 + o(1))$-fraction of entries. We cannot use a union bound to compose such two weak approximations. 
Fortunately, we can address this issue by carefully considering these two approximations in terms of \emph{Boolean rigidity}: When both the approximations fail at some entry $(x,y)$, their composition yields a correct approximation at $(x,y)$, i.e., $\bool(\tilde{L}[x,y]) = -\bool(\kro{L}[x,y])= \bool (\kro{A}[x,y])$. A careful calculation will show that $\bool(\tilde{L})$ approximates $\bool(\kro{A})$ within accuracy $(1/2 + 2^{-O(n)})$, relying on the fact that the errors in these two approximations are essentially independent.

\subsection{Hardness Amplification for Majority Product}
The Kronecker product has the property that $\kro{A} = \kro[n/k]{(\kro[k]{A})}$, and we implicitly made use of this above to reduce from $\kro{A}$ to $\kro[k]{A}$. Here, we also need to reduce from $\maj{A}$ to $\maj[k]{A}$, but unfortunately, in general, $\maj[n/k]{(\maj[k]{A})} \neq \maj{A}$. We begin by calculating the number of entries in which these differ, which contributes to our final error.

Suppose $A \in \BK{-1, 1}^{q \times q}$ and we are given, via a rigidity upper bound, a rank-$r$ matrix $L$ that approximates the matrix $\maj[k]{A} \in \BK{-1, 1}^{q^k \times q^k}$ within a constant error rate $\delta$. Our goal is to construct a matrix $\tilde{L}$ with a similar rank as $L$ that approximates $\maj{A}$ with an accuracy $(1/2 + \Omega\bk{\sqrt{k/n}})$. 

We follow the same framework of constructing $\tilde{L}$ as for the Kronecker powers: 
\begin{itemize}
  \item As $L$ approximates $\maj[k]{A}$ well, and $\maj{A}$ is similar to the $(n/k)$-th majority power of $\maj[k]{A}$ (as calculated earlier), we use the relatively high-rank matrix $\maj[(n/k)]{L}$ to approximate $\maj{A}$. 
  \item Then, we use a degree-$1$ polynomial $\tilde\Maj$ to approximate the majority function $\Maj\bk{z_1, \ldots, z_{n/k}}$ and define the low-rank matrix $\tilde{L}$ that approximates $\maj[(n/k)]{L}$ correspondingly. In the case of majority product, the degree-$1$ polynomial can be simply chosen as $\tilde{\Maj}(z_1, \ldots, z_{n/k}) = z_1$. One can calculate that this simple dictator function will approximate the majority function with an accuracy $(1/2 + \Omega\bk{\sqrt{k/n}})$.
\end{itemize} 


Similar to before, we may naturally aim to use Boolean rigidity to compose these two approximations. However, this is unfortunately impossible: the composed approximation, that uses $\tilde{L}$ to approximate $\maj{A}$, is not guaranteed to be have an accuracy $(1/2 + \Omega\bk{\sqrt{k/n}})$.
To see why this is the case, we consider another intermediate matrix $\tilde{A} \in \BK{-1,1}^{q^n \times q^n}$, which is the analogue of $\tilde{L}$ but with $A$ in place with $L$ in its definition. Specifically, 
\begin{align*}
  \tilde{A}[x,y] \defeq \maj[k]{A}\bk{x_{[1,k]}, y_{[1,k]}} = \Maj\bk{A[x_1, y_1], \ldots, A[x_k, y_k]}, \quad \forall x, y \in [q]^n,
\end{align*}
where $x_{[1,k]}$ represents a prefix of length $k$ in $x = (x_1, \ldots, x_n)$.
The errors in the whole approximation can be classified into two types: the inherent error incurred by the given low-rank matrix $L$ (i.e., the error between $\tilde{L}$ and $\tilde{A}$), and the approximation error (i.e., the error between $\tilde{A}$ and $\maj{A}$). 
The previous composition argument for the Kronecker product crucially relies on the fact that the two types of errors are independent, which is not true for the majority product, as the matrix $L$ only approximates $\maj[k]{A}$ in a coarse sense. For example, if we consider indices $(x,y) \in [q]^n \times [q]^n$ sampled uniformly at random, then:
\begin{itemize}
  \item The random variable $\indicator{\tilde{A}[x,y] = \maj{A}[x,y]}$ is correlated with the random variable $S \defeq A[x_1, y_1] + \cdots + A[x_k, y_k]$. When $S > 0$ (meaning that $\tilde{A}[x,y] = 1$), the larger $S$ is, the more likely that $\maj{A}[x,y] = 1$.
  \item The random variable $\indicator{\tilde{A}[x,y] = \tilde{L}[x,y]} = \indicator{\maj[k]{A}[x_{[1,k]}, y_{[1,k]}] = L[x_{[1,k]}, y_{[1,k]}]}$ can also possibly be correlated with the random variable $S$. This is because the given matrix $L$ is only guaranteed to approximate $\maj[k]{A}$ \emph{on average}, so the error can be distributed arbitrarily.
\end{itemize}
Thus, the two errors given by $\indicator{\tilde{A}[x,y] \neq \maj{A}[x,y]}$ and $\indicator{\tilde{A}[x,y] \neq \tilde{L}[x,y]}$ can be correlated, making it hard to compose them. To address this issue, we need to assume the stronger condition that the \emph{probabilistic Boolean rank} with error rate $\delta$ of $\maj[k]{A}$ is at most $r$. This means the errors in $L$ are now uniformly distributed, allowing the two errors mentioned above to be independent.

