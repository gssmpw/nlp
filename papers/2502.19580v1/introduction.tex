The \defn{rigidity} of a $N \times N$ matrix $A$ over a field $\F$ for some rank $r$, denoted by $\R_A^{\F}(r)$, is defined as the minimum number of entries needed to be changed in $A$ in order to reduce the rank of $A$ to at most $r$. 
This concept, introduced by Valiant \cite{valiant1977graphtheoretic}, has broad connections to many topics in complexity theory including circuit lower bounds \cite{valiant1977graphtheoretic,goldreich2016matrix,alman2017probabilistic}, communication complexity \cite{razborov1989rigid,wunderlich2012theorem}, and data structure lower bounds \cite{dvir2019static,natarajanramamoorthy2020equivalence}. In all these connections, researchers have shown that proving a \emph{lower bound} on the rigidity of a matrix implies a breakthrough lower bound for computing that matrix in other models of computation. 

In these applications, the goal is to prove a rigidity lower bound on an \defn{explicit} family of matrices. We say a family of matrices $\BK{A_N}_{N\in \mathbb{N}}$ is explicit, if there is a uniform polynomial-time algorithm that computes all the entries of the matrix $A_N \in \F^{N \times N}$ within time $\poly(N)$. Focusing on explicit matrices is important since lower bounds, for both rigidity and applications, are typically not hard or as interesting to prove for non-explicit matrices. As an example, a counting argument~\cite{valiant1977graphtheoretic} shows that a random $\{-1,1\}$ matrix is very rigid, enough to imply lower bounds via all the known connections, but this is not particularly exciting since counting arguments also show that a random function does not have, for instance, small circuits or efficient communication protocols. 

Moreover, one typically focuses on particular families of explicit matrices which are of interest in the applications. For instance, much work has focused on the rigidity of the Walsh-Hadamard transform, since this is a matrix that is used frequently, both as a linear transformation in circuits, and as the inner product mod 2 function in communication complexity.

\subsection{Rank Parameter Regimes}

Different applications of matrix rigidity focus on different choices of the rank parameter $r$. 
There are three main parameter regimes which have previously been studied. 

The \defn{high-rank regime} where $r \ge N^{\Omega(1)}$ was first considered by Valiant \cite{valiant1977graphtheoretic}, as a tool for proving circuit lower bounds. Valiant proved that, for any matrix family $\BK{A_N}_{N \in \mathbb{N}}$ where $A_N \in \F^{N \times N}$, if there exists a constant $\varepsilon>0$ such that the rigidity of each $A_N$ is lower bounded by $\R^{\F}_{A_N} (N/ \log \log N) \ge N^{1 + \varepsilon}$, then the linear transformation that takes a vector $x \in \F^N$ as an input and outputs $A_N x$ cannot be computed by an arithmetic circuit of depth $O(\log N)$ and size $O(N)$. 
We say the matrix family $\BK{A_N}_{N\in \mathbb{N}}$ is \defn{Valiant rigid}, if it satisfies this rigidity lower bound.
In this high-rank regime, many candidate matrices that were previously conjectured to be Valiant rigid, including some of the most important matrices in this context like the Walsh-Hadamard transform and discrete Fourier transform, were proven to be non-rigid, by a recent line of work \cite{alman2017probabilistic,dvir2019matrix,dvir2020fourier,alman2021kronecker}.

The \defn{mid-rank regime} where $N^{o(1)} > r > \log^{\omega(1)} N$ was first considered by Razborov \cite{razborov1989rigid} (see also \cite{wunderlich2012theorem}), exploring the connection between matrix rigidity and communication complexity of Boolean functions. In the two-party communication complexity model, Alice and Bob want to compute a Boolean function $f: \BK{0,1}^{n} \times \BK{0,1}^n \to \BK{0,1}$ where each party owns $n$ of the $2n$ bits of the input. Many different models of communication have been defined~\cite{goos2018landscapea}, and one typically aims to minimize how much the parties need to communicate with each other to evaluate $f$. Razborov proved that the communication complexity to compute the function $f$ is related to the rigidity of the \emph{communication matrix} $A_f$ of the function $f$. $A_f$ is a $2^n \times 2^n$ matrix whose columns and rows are indexed by strings in $\BK{0,1}^n$, and whose entries are given by $A_f[x,y] = f(x,y)$ for any $(x,y) \in \BK{0,1}^{n} \times \BK{0,1}^n$. Razborov proved that, if the rigidity of each $A_f$ is lower bounded by $\R^{\F}_{A_f} \bk{2^{\log^{\omega(1)} n}} \ge \Omega(4^n)$, then the Boolean function $f$ cannot be computed in $\textsf{PH}^{cc}$, the analogue of polynomial hierarchy in communication complexity. We say the matrix family $\BK{A_N}_{N\in \mathbb{N}}$ is \defn{Razborov rigid}, if it satisfies this rigidity lower bound. Hence, proving rigidity lower bound in the mid-rank regime can imply super strong communication complexity lower bounds.

Despite the importance of rigidity lower bounds in the mid-rank regime, our known rigidity lower bounds for explicit matrices appear quite far from proving that any explicit matrix is Razborov rigid. The best lower bound for an explicit matrix is only $\R^{\F}_{A_f} \bk{2^{\log^{\omega(1)} n}} \ge \Omega(4^n / 2^{\log^{\omega(1)} n})$, which seems to be a big gap from the goal of $\Omega(4^n)$. In this paper, we will give a new reduction from mid-rank to lower rank rigidity, where just a small improvement to our known lower bounds will suffice to achieve Razborov rigidity.


More recently, researchers \cite{alman2021kronecker,alman2023smaller} have started to explore applications of rigidity in the \defn{low-rank regime} where $r = \log^{O(1)} N$. Alman \cite{alman2021kronecker} related the rigidity of a Kronecker power matrix $\kro{A}$ in the {low-rank regime} with the size of \emph{constant-depth} linear circuits that compute the linear transformation $\kro{A}: x \mapsto \kro{A} x$. For instance, \cite{alman2021kronecker} gave the rigidity upper bound $\R^{\F}_{H_4} (1) \le 96$, and used it to obtain a depth-2 linear circuit of size only $O(N^{1.476})$ for the $N \times N$ Walsh-Hadamard transform $H_n$, giving the first improvement on the folklore fast Walsh-Haramard transform algorithm which achieves $O(N^{1.5})$. In this paper, we will prove new, strong rigidity lower bounds in this low-rank regime, proving 
 limits on this approach to designing smaller circuits.


\subsection{Matrices of Interest}

In this paper, we return to the challenge of understanding the rigidity of important families of explicit matrices. 
We focus in particular on two classes of explicit matrix families: the matrix family formed by the Kronecker powers of a small matrix, such as the Walsh-Hadamard transform, and the matrix family formed by a new variant on the Kronecker power we introduce, called the Majority power, including a special family of matrices called the ``distance matrix'', defined as follows:
\begin{itemize}
    \item The $n$-th \defn{Kronecker power} $\kro{A}$ of a matrix $A \in \F^{q \times q}$ is a $q^n \times q^n$ matrix with its columns and rows being indexed by vectors from $[q]^n$,\footnote{We use the notation $[q]$ to represent the set $\BK{0,1,\ldots,q-1}$ for any positive integer $q$.} and entries being $\kro{A}[x, y] \defeq \prod_{i=1}^n A[x_i, y_i]$ for any $x = (x_1, \ldots, x_n) \in [q]^n$ and $y = (y_1, \ldots, y_n) \in [q]^n$.
    \item  In particular, the $n$-th \defn{Walsh-Hadamard matrix} is defined as $H_n \defeq  \bk*{\begin{matrix}
    1 & 1\\
    1 & -1
    \end{matrix}}^{\otimes n} $.
    \item The $n$-th \defn{Majority power} $\maj{A}$ of a matrix $A \in \BK{-1,1}^{q \times q}$ is a $q^n \times q^n$ matrix, with its entry being $\maj{A}[x, y] \defeq \Maj\bk{A[x_1, y_1],\ldots,A[x_n, y_n]}$ for any $x = (x_1, \ldots, x_n) \in [q]^n$ and $y = (y_1, \ldots, y_n) \in [q]^n$, where $\Maj: \BK{-1, 1}^n \to \BK{-1, 1}$ is the majority function.
    \item In particular, the $n$-th \defn{distance matrix} $M_n$ is the $n$-th Majority power of $M_1 \defeq \bk*{\begin{matrix}
    1 & -1\\
    -1 & 1
    \end{matrix}} $. In other words, the entry $M_n[x,y]$ calculates whether the Hamming distance between $x$ and $y$ is greater than $n/2$, for any vectors $x, y \in \BK{0,1}^n$.
\end{itemize}

We focus on these matrix families because of their prominence in applications. For instance, in communication complexity, the Walsh-Hadamard transform is the communication matrix for the ``inner product mod $2$'' function, and Kronecker powers more generally can capture any function which is the parity of constant-sized functions. Similarly, the distance matrix is the communication matrix for testing whether the Hamming distance of the players' inputs is at most a threshold, and the Majority power can capture more general communication problems about thresholds of constant-sized functions \cite{yao2003power,gavinsky2004quantum,huang2006communication}. %

We will prove two types of results for these matrices. First, we will prove new rigidity lower bounds, which for low enough rank are essentially tight and improve on the state of the art for rigidity lower bounds on explicit matrices. Second, we will establish hardness amplification arguments, showing that even a small improvement on our new rigidity lower bound may be challenging: a slight improvement will imply the Razborov rigidity of these matrices, a breakthrough in the fields of matrix rigidity and communication complexity.


\subsection{Previous Rigidity Lower Bounds}
There has been plenty of work on rigidity lower bounds for explicit matrices including Walsh-Hadamard matrices \cite{lokam2001spectral,kashin1998improved,dewolf2006lower}, discrete Fourier transform matrices \cite{shparlinski1999}, Cauchy matrices \cite{shokrollahi1997remark} and generator matrices for linear codes \cite{friedman1993note,pudlak1994combinatorialalgebraic,shokrollahi1997remark}. The best known rigidity lower bound for any explicit matrix over any field is $\R_{A}^{\F_p} (r) \ge \Omega\bk*{\frac{N^2}{r} \cdot \log \frac{N}{r}}$. In particular, for Walsh-Hadamard matrix, the best known rigidity lower bound is $\R_{H_n}^{\F} (r) \ge \frac{N^2}{4r}$, for any field $\F$ which does not have characteristic $2$. (Note that over characteristic $2$, the matrix $H_n$ has rank $1$.) 

\paragraph*{Untouched submatrix argument.}
All the aforementioned lower bounds for explicit matrices are based on a combinatorial approach which we call an \defn{untouched submatrix argument}. The high-level idea of this argument is that, to reduce the rank of a matrix $A$ to $r$, we need to change at least one entry of $A$ in \emph{every} submatrix of $A$ with rank more than $r$. For example, if the matrix $A$ has all of its submatrices being full-rank (such $A$ is called a \defn{totally regular matrix}), then the rigidity of $A$ for rank $r$, $\R_A^{\F}(r)$, is at least the number of entries we need to choose to touch every $r \times r$ submatrix of $A$. We can bound this by applying known results from extremal graph theory (Zarankiewicz problem \cite{kovari1954problem,bollobas2004extremal}), and prove that the rigidity of any totally regular matrix is lower bounded by $\R_A^{\F}(r) \ge \Omega\bk*{\frac{N^2}{r} \cdot \log \frac{N}{r}}$, for rank $r$ with $\Omega(\log N) \le r \le N/2$. There are many known examples of explicit totally regular matrices, such as the Cauchy matrix and generator matrices for some linear codes~\cite{friedman1993note,pudlak1994combinatorialalgebraic,shokrollahi1997remark}.

However, the untouched submatrix argument has a known limitation, and cannot prove a rigidity lower bound better than $\Omega\bk*{\frac{N^2}{r} \cdot \log \frac{N}{r}}$: As shown in \cite{lokam2000rigidity,lokam2009complexity}, there exist totally regular matrices where this lower bound is tight, i.e., $\R_A^{\F}(r) \le O\bk*{\frac{N^2}{r} \cdot \log \frac{N}{r}}$. Hence, in order to prove strong rigidity lower bounds required by the applications (e.g., Valiant rigidity and Razborov rigidity lower bounds), we need to consider new techniques.

\paragraph*{Spectral method.}
In this paper, we will also build on a technique that was previously introduced for bounding a \emph{restricted} version of matrix rigidity called \defn{bounded rigidity}. For a matrix $A$, rank parameter $r$, and positive real number $\theta$, we define $\R_A^{\C}(r, \theta)$ as the minimum number of entries that we need to change in $A$ to reduce the rank to at most $r$, where we are only allowed to change each of those entries by adding or subtracting a value which is \emph{at most $\theta$ in magnitude}. Prior work has given stronger bounds on this bounded rigidity by using spectral tools~\cite{lokam2001spectral,dewolf2006lower,rashtchian2016bounded}. Indeed, if $A$ is approximated by a low-rank matrix $L$ in this bounded sense, then we can control the error in other distances, e.g., the Frobenius distance $\norm{A - L}_{F}$, and then use tools from spectral analysis to further bound this soft version of rigidity.


There are two main caveats of this approach. First, to have the spectral tools and the soft version of rigidity, we need to be in the number field $\C$, hence there is no known rigidity lower bound over a finite field $\F_p$ from this approach. Second, because this only works for bounded rigidity\footnote{Spectral methods have also been used to show that submatrices of $H_n$ or other matrices have high rank, to be used in conjunction with the untouched submatrix method~\cite{kashin1998improved, lokam2001spectral,dewolf2006lower}. However, to our knowledge, spectral methods have only been applied to the sparse error matrix $A-L$ when studying bounded rigidity.}, it does not yield most of the applications of matrix rigidity. In particular, one of the main applications of bounded rigidity was proving ``bounded-coefficient'' arithmetic circuit lower bounds for linear transforms like the Walsh-Hadmard transform. This was seen as a barrier to designing smaller circuits, since all previous circuit constructions were bounded-coefficient. However, a recent line of work~\cite{alman2021kronecker,alman2023smaller} actually showed that such bounded-coefficient lower bounds can be overcome by designing much smaller unbounded-coefficient circuits (by using the low-rank rigidity upper bounds we mentioned earlier!), making bounded-coefficient lower bounds less interesting.

In this paper, we will establish new techniques to get rid of these two caveats, and prove a (regular) rigidity lower bound over $\F_p$ via a new type of spectral method.


\subsection{Boolean Rigidity}
For technical reasons, in this paper, we will focus on a notion of rigidity called \defn{Boolean rigidity}. This notion appears implicitly in many prior works on rigidity and Boolean function analysis, although we're not aware of any that explicitly define it. For each element $x$ in a finite field $\F_p$, we define the \defn{Booleanization} of $x$ as 
    \begin{align*}
        \bool(x) \defeq \begin{cases}
            1 & \text{if } x \equiv 1 \pmod p,\\
            -1 & \text{otherwise,}
        \end{cases}
    \end{align*}
which maps the element $x \in \F_p$ into a Boolean value. The Booleanization of a matrix $A$ is defined as the matrix obtained by taking the Booleanization of each entry of $A$. 
We consider the following variant on rigidity where the low-rank matrix only needs to approximate the given matrix in this Booleanized sense:
\begin{definition}[Boolean rigidity]
    The \defn{Boolean rigidity} of a matrix $A \in \F_p^{N \times N}$ of rank $r$ over $\F_p$, denoted by $\boolRigidity{A}{r}$ is defined as the minimum $s$, such that there is a matrix $L \in \F_p^{N \times N}$ with $\rank(L) \le r$ and a subset of indices $I \subset [N]^2$ with $|I| \ge N^2 - s$, satisfying
    \begin{align*}
        \bool(A[i,j]) = \bool(L[i,j]), \quad \forall (i,j) \in I.
    \end{align*}
\end{definition}

The concept of Boolean rigidity is very closely related to the (regular) rigidity in the following sense: if all the entries of $A$ are $1$ or $-1$ in $\F_p$, then for any rank $r$, we have $$\R_A^{\F_p}(r) \ge \boolRigidity{A}{r} \ge \R_{A}^{\F_p}(O(r^{p-1})).$$ The first inequality is because any rank-$r$ matrix $L$ that is close to $A$ within Hamming distance $s$ must also be close to $A$ within Boolean distance $s$. The second inequality is because if a rank-$r$ matrix $L$ is close to $A$ within Boolean distance $s$, we can construct a matrix $\tilde{L}$ defined as $\tilde{L}[i,j] \defeq \bool\bk{L[i,j]}$ for each entry $(i,j) \in [N]^2$, which is close to $A$ within Hamming distance $s$ as each entry of $A$ is either $1$ or $-1$. By a standard argument using Fermat's little theorem (see \cref{rmk:boolean_close_to_regular} below) we see that $\rank(\tilde{L}) \leq O((\rank(L))^{p-1})$.


Based on this relationship between the Boolean rigidity and the (regular) rigidity, one can see that a Boolean rigidity lower bound which we will prove is stronger than a (regular) rigidity lower bound with the same parameter. Moreover, these two rigidity concepts are equivalent regarding to Razborov rigidity: A $2^n \times 2^n$ matrix $A$ is Razborov rigid, if and only if it satisfies $\boolRigidity{A}{2^{\log^{\omega(1)} n}} \ge \Omega(4^n)$. 
We focus here on Boolean rigidity, even though it is nearly equivalent to (regular) rigidity, because it will simplify the statements of our hardness amplification results below.

The two most closely-related notions of rigidity that have been previously studied are `toggle rigidity' and `sign-rank rigidity'. In toggle rigidity, the low-rank matrix must only have entries among $\{-1,1\}$, whereas in sign rank rigidity, the low-rank matrix must only have the same sign as $A$ in each entry to be considered correct. These notions have previously been considered for both communication complexity and circuit complexity~\cite{razborov1989rigid,wunderlich2012theorem,rashtchian2016bounded,alman2017probabilistic}. 


\subsection{Our Results}

In this paper, we give new rigidity lower bounds in the low-rank regime, and new hardness magnification results which show that a substantial improvement to our low-rank lower bounds would give a breakthrough lower bound in communication complexity.

\paragraph*{Nearly tight rigidity lower bound in the low-rank regime.}
Our first result is a generic rigidity lower bound via the spectral method. It shows that the aforementioned limitations of the spectral method in matrix rigidity can be overcome, and spectral arguments can give tight lower bounds for (regular or Boolean) rigidity over a finite field.
\begin{restatable}{theorem}{RigidityLbFromSingularValue}
    \label{thm:rigidity_lb_from_singular_value}
    Let $p$ be a constant prime number and $A \in \BK{-1,1}^{N \times N}$ be a matrix with largest singular value $\sigma_1$ (over $\mathbb{C}$). Then, there is a constant $c >1 $ (depending only on $p$) such that for any rank $r$,
    \begin{align*}
        \boolRigidity{A}{r} \ge N^2 \bk*{\frac{1}{2} - \frac{c^r \sigma_1}{N}}.
    \end{align*}
\end{restatable}

We will see below that \Cref{thm:rigidity_lb_from_singular_value} is tight for many matrices of interest. That said, even generically, we can see that it is nearly tight for small rank $r$ for \emph{any} matrix $A \in \BK{-1,1}^{N \times N}$. Indeed, for any such matrix, its \emph{rank-1} rigidity is at most $N^2/2$, since either the all-$1$ matrix or the all-$(-1)$ matrix will match $A$ in at least half its entries. 

Applying this rigidity lower bound to Kronecker powers and the distance matrix, we obtain the following rigidity lower bounds. (Recall that a Boolean rigidity lower bound also implies the same (regular) rigidity lower bound.)

\begin{restatable}{theorem}{KroneckerLb}
    \label{thm:kronecker_lb}
    Let $p$ be a constant prime number and $q$ be a constant integer. Suppose $A \in \BK{-1,1}^{q \times q}$ is a matrix with $\rank(A) > 1$. Then, there are $c_1 > 0$, $c_2 \in (0,1)$  such that for all positive integers $n$, the Kronecker power $\kro{A}$ has the rigidity lower bound
    \begin{align*}
        \boolRigidity{\kro{A}}{c_1 n} \ge q^{2n} \bk*{\frac{1}{2} - c_2^n}.
    \end{align*}
\end{restatable}

\begin{restatable}{theorem}{HammingLb}
    \label{thm:hamming_lb}
    Let $p$ be a constant prime and $n \in \N$ be a positive integer. 
    For any constant $\eps > 0$, there is a constant $\beta > 0$, such that the distance matrix $M_n$ has the rigidity lower bound
    \begin{align*}
        \boolRigidity{M_n}{\beta \log n} \ge 4^n \bk*{\frac{1}{2} - \frac{1}{n^{1/2 - \eps}}}.
    \end{align*}
\end{restatable}

As the $2^n \times 2^n$ Walsh-Hadamard matrix $H_n$ is the $n$-th Kronecker power of the $2 \times 2$ Walsh-Hadamard matrix $H_1$, \cref{thm:kronecker_lb} gives a nearly tight rigidity lower bound (up to the constant $c_2$) for Walsh-Hadamard matrices for small ranks over finite fields:
\begin{cor} 
\label{cor:walsh_hadamard_lb}
For all constant primes $p$, and all $n$, 
$$4^n\bk*{\frac{1}{2} - 2^{-O(n)}} \le \R_{H_n}^{\F_p} (\Theta(n)) \le \R_{H_n}^{\F_p}(1) \le 4^n \bk*{\frac{1}{2} - 2^{-\Omega(n)}}.$$
\end{cor}
Here, the lower bound is from our \cref{thm:kronecker_lb}, and the upper bound is from prior work on designing constant-depth circuits~\cite{alman2023smaller}. Interestingly, this shows that the rigidity for rank $1$ is hardly smaller than the rigidity for rank $\Theta(n)$ for the $2^n \times 2^n$ Walsh-Hadamard matrix $H_n$. \Cref{thm:hamming_lb} is similarly tight up to the $\varepsilon$ in the exponent, as $\R_{M_n}^{\F_p}(1) \leq 4^n(1/2 - \Theta(1/\sqrt{n}))$ by an upper bound similar to our \Cref{thm:amplification_maj} below.

Note that the lower bound in Corollary~\ref{cor:walsh_hadamard_lb} improves on all the previously known rigidity lower bounds for explicit matrices in the small rank regime: the previous state-of-the-art lower bound 
\cite{friedman1993note,pudlak1994combinatorialalgebraic,shokrollahi1997remark} is $\Omega\bk*{\frac{N^2}{r} \log \frac{N}{r}} = \Omega\bk{N^2}$ when $r = \Theta(\log N)$, where the $\Omega$ hides a small constant. Our Corollary~\ref{cor:walsh_hadamard_lb} improves the leading constant of $N^2$ to the maximum possible $1/2$, and applies in particular to the Walsh-Hadmard transform, for which the previous best lower bound at rank $r = \Theta(\log N)$ was $N^2 / 4r = \Theta(N^2 / \log N)$~\cite{dewolf2006lower}.


As mentioned previously, the low-rank regime has recently been studied extensively in the context of designing smaller low-depth linear circuits. For instance, recent improved circuits for $H_n$ are based on rigidity upper bounds for particular constant sizes: $\R_{H_4}(1) \le 96$~\cite{alman2021kronecker} and $\R_{H_6}(1) \le 1792$~\cite{alman2023smaller}. This is perhaps surprising, as most other applications of matrix rigidity make use of \emph{asymptotic} constructions, rather than constant-sized constructions. As one corollary of our rigidity lower bound, we explain this phenomenon: $H_n$ is too rigid for low ranks when $n$ is super-constant to give improved circuits using the approach of Alman~\cite{alman2021kronecker}, so only constant-sized constructions may give improvements. This motivates searching for techniques other than matrix rigidity upper bounds to get any further improved circuit constructions. See \Cref{sec:obstructiondepth2} for more details.


\paragraph*{Hardness amplification of Boolean rigidity}
Our second, complementary results are new hardness amplification arguments. We give separate arguments for the Kronecker product and the Majority product, as these products yield matrices which are rigid for slightly different parameters. We begin with the Kronecker power.

\begin{restatable}{theorem}{KroneckerAmp}
    \label{thm:amplification_kro}
    Suppose $A \in \BK{-1,1}^{q \times q}$ is a matrix with Boolean rigidity 
      $
      \boolRigidity{A}{r} \le \delta \cdot q^2,
      $
    where $r \le q$ and $\delta\in (0,1/2)$ is a parameter. 
    Assume $A$ has roughly the same number of $1$ and $-1$ entries, i.e., the fraction of $1$'s and the fraction of $-1$'s differ by at most $\alpha \in \bk{0,1}$ in $A$, with $2 \alpha + \delta < {1}/{2}$.
    Then, for any integer parameter $n$, the Boolean rigidity of $\kro{A}$ is at most 
    \begin{align*}
      \boolRigidity{\kro{A}}{2rn} \le q^{2n} \bk*{\frac{1}{2} - \frac{1}{2} \cdot \bk*{\frac{1}{2} - \alpha - \delta}^n}.
    \end{align*}
\end{restatable}

\Cref{thm:amplification_kro} shows that rigidity lower bounds for lower rank parameters can imply rigidity lower bounds for substantially higher rank parameters. To demonstrate, we apply \cref{thm:amplification_kro} to get:

\begin{restatable}{theorem}{StrongerKro}
    \label{thm:stronger_lb_kro_imply_Razborov}
      Let $p$ be a constant prime number, $q$ be a constant integer, and $A \in \BK{-1,1}^{q \times q}$ be a matrix with $\rank(A) > 1$. Suppose we can prove for some constant parameter $\eps > 0 $ that, for infinitely many positive integers $n$,
    \begin{align*}
        \label{ineq:kro_rigidity_lb_for_larger_rank}
        \boolRigidity{\kro{A}}{n^{1 + \eps}} \ge q^{2n} \bk*{\frac{1}{2} - \frac{1}{2^{n/2^{(\log n)^{o(1)}}}}}.\numberthis
    \end{align*}
    Then, the matrix family $\BK{A_n}_{n \in \mathbb{N}}$ is Razborov rigid.
\end{restatable}

In particular, \cref{thm:stronger_lb_kro_imply_Razborov} implies that if we can improve our rigidity lower bound $\boolRigidity{H_n}{\Omega(n)} \ge 4^n \bk*{1/2 - 2^{-O(n)}}$ for the Walsh-Hadamard matrix $H_n$ to work for a slightly larger rank $n^{1 + \eps}$ and a slightly higher error rate $4^n \bk*{1/2 - 2^{-n/2^{\log^{o(1)} n}}}$, it would prove that Walsh-Hadamard matrices are Razborov rigid. This would give a breakthrough lower bound for the communication complexity class $\textsf{PH}^{cc}$.

We emphasize that the higher error rate is \emph{easier} to achieve, but in exchange, the higher rank is \emph{harder} to achieve. More generally, there is a trade-off between the required error and rank parameters one must achieve, depending on how large $n$ is compared to $q$ when one applies \Cref{thm:amplification_kro}. Our \cref{thm:stronger_lb_kro_imply_Razborov} states one example of the trade-off.

This may help to explain why our best rigidity lower bounds for explicit matrices $A$ are stuck at about $\R_A(r) \geq \Omega\bk*{\frac{N^2}{r} \log \frac{N}{r}}$. Currently, for the rank regime $r=2^{\log^{\omega(1)} n}$ needed for Razborov rigidity, there is a sizeable gap between the lower bound $\Omega(N^2 / 2^{\log^{\omega(1)} n})$ we can prove for explicit matrices, and the required lower bound of $\Omega(N^2)$. However, \Cref{thm:stronger_lb_kro_imply_Razborov} gives a setting where there is a small gap between the two: one must only prove a rigidity lower bound for a slightly larger rank as Corollary~\ref{cor:walsh_hadamard_lb}, and it need not even have as low an error parameter. In particular, a large improvement to the best-known rigidity lower bound across all rank regimes may be difficult, but it might be more fruitful to aim for larger improvements focused on specific rank regimes, as we do here in the low-rank regime.


Our second, similar hardness amplification result works for the distance matrix as well as a more general class obtained by taking the Majority power of a small matrix. For technical reasons, our amplification argument needs to start with a matrix with small \emph{probabilistic Boolean rank}, which is 
the ``{worst-case probabilistic}'' version of Boolean rigidity: 
Rather than aiming for a single low-rank matrix which differs from matrix $A$ on at most $\delta$ fraction of entries, we need a \emph{distribution} on low-rank matrices such that each entry of $A$ is correct with error at most $\delta$.
Note that any Boolean function within the communication complexity class $\textsf{PH}^{cc}$ also admits a communication matrix with low probabilistic Boolean rank~\cite{razborov1989rigid,wunderlich2012theorem,alman2017probabilistic,alman2022efficient}, so this suffices for proving that rigidity lower bounds in the low-rank regime would imply a breakthrough communication lower bound:
\begin{restatable}{theorem}{MajorityAmp}
    \label{thm:amplification_maj}
    Let $A \in \BK{-1,1}^{q \times q}$ be a matrix with exactly half its entries equal to $1$. Let $k < n$ be parameters. Suppose the probabilistic Boolean rank of $\maj[k]{A}$ for error rate $\delta \in \bk{0, 1/2}$ is  $r$. Then, the Boolean rigidity of $\maj{A}$ can be bounded as 
    \begin{align*}
      \boolRigidity{\maj{A}}{r} \le q^{2n} \bk*{\frac{1}{2} - \Omega\bk*{(1 - 2\delta) \cdot \frac{\sqrt{k}}{\sqrt{n}}}},
    \end{align*}
  where the $\Omega$ is hiding an absolute constant.
  \end{restatable}




Similar to above, we can apply \Cref{thm:amplification_maj} to get:

\begin{restatable}{theorem}{StrongerMaj}
    \label{thm:stronger_lb_maj_imply_Razborov}
    Let $p$ be a constant prime number, and $\BK{M_n}_{n\in \mathbb{N}}$ be the family of the distance matrices. Suppose we can prove for some constant $\beta > 0$ that, for infinitely many positive integers $n$, 
    \begin{align*}
        \label{ineq:maj_rigidity_lb_for_larger_rank}
        \boolRigidity{M_n}{\beta \log n} \ge 4^n \bk*{\frac{1}{2} - \frac{2^{\bk*{\log \log n}^{o(1)}}}{n^{1/2}}}. \numberthis
    \end{align*}   
    Then, the matrix family $\BK{M_n}_{n \in \mathbb{N}}$ is Razborov rigid.
\end{restatable}

This again shows that a slight improvement to our rigidity lower bound in \Cref{thm:hamming_lb} would yield a breakthrough communication lower bound. Notably, both the rigidity lower bound of \Cref{thm:hamming_lb}, and the required lower bound of \Cref{thm:stronger_lb_maj_imply_Razborov}, focus on the same, lower error parameter (of about $1/2 - 1/\sqrt{n}$) compared to \Cref{thm:stronger_lb_kro_imply_Razborov} (which focuses on about $1/2 - 1/2^{\Theta(n)}$).

\subsection{Other Related Works}

Matrix rigidity has been studied extensively since its introduction by Valiant~\cite{valiant1977graphtheoretic}. We refer the reader to surveys by Lokam~\cite{lokam2009complexity}, Ramya~\cite{ramya2020recent}, and Golovnev~\cite{golovnev2020matrix} about known upper and lower bounds on rigidity, and connections with other areas of computer science. We briefly expand below on a few of the works most related to ours.

\paragraph{Rigidity upper bounds.}
Alman and Williams~\cite{alman2017probabilistic} showed that the Walsh-Hadamard transform is not Valiant rigid. Letting $N = 2^n$ denote the side-length of $H_n$, they showed that for every $\varepsilon>0$, $\R_{H_n}(N^{1 - \Theta(\varepsilon^2 / \log ( 1/\varepsilon))}) \leq N^{1 + \varepsilon}.$
They also showed an upper bound for lower rank but higher sparsity: for every parameter $s$, they showed
$\R_{H_n}((n/\log s)^{O(\sqrt{n \log s})}) \leq N^2 / s.$
These two upper bounds hold over any field. Later work by Alman~\cite{alman2021kronecker} showed that they also hold for $M_n$, and more generally any Kronecker power or Majority power matrix family.
A folklore upper bound uses the singular values of $H_n$ to prove that $H_n$ is not very rigid for very high rank: $\R^{\mathbb{C}}_{H_n}(N/2) \leq N.$
Finally, Alman, Guan and Padaki~\cite{alman2023smaller} gave an upper bound on the rank-1 rigidity of $H_n$ as part of their construction of low-depth arithmetic circuits:
$\R_{H_n}(1) \leq N^2 \cdot \left( \frac12 - \frac{\Theta(1)}{\sqrt{N}} \right).$

\paragraph{Semi-explicit rigid matrices.}

A recent line of work~\cite{alman2022efficient,bhangale2024rigid,chen2020almost,chen2021inverse,huang2021averagecase}, gave a ``somewhat-explicit'' construction of a matrix family which is Razborov rigid. They proved that there is a $\mathsf{P^{NP}}$ machine, which, on input $1^n$, outputs an $n \times n$ matrix $A_n$ over $\mathbb{F}_2$ such that 
$\R_{A_n}(2^{\Omega(\log N / \log \log N)}) \geq N^2 \cdot \left(  \frac12 - o(1) \right).$ This is the only other line of work we're aware of that gives rigidity lower bounds with maximal sparsity parameters $N^2 \cdot \left(  \frac12 - o(1) \right)$ for non-random matrices.
