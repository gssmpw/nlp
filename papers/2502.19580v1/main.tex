\documentclass[11pt]{article}
\usepackage[letterpaper, margin=1in]{geometry}
\input{head}

\title{Low Rank Matrix Rigidity: \\ Tight Lower Bounds and Hardness Amplification}
\author{Josh Alman\thanks{Columbia 
University. \texttt{josh@cs.columbia.edu}. Work supported in part by NSF Grant CCF-2238221.} 
    \and 
    Jingxun Liang\thanks{Carnegie Mellon University. \texttt{jingxunl@andrew.cmu.edu}. Work performed while the author was visiting Columbia University and supported in part by NSF Grant CCF-2238221.}}
\date{}

\begin{document}

\maketitle

\begin{abstract}
For an $N \times N$ matrix $A$, its rank-$r$ rigidity, denoted $\R_A(r)$, is the minimum number of entries of $A$ that one must change to make its rank become at most $r$. Determining the rigidity of interesting explicit families of matrices remains a major open problem, and is central to understanding the complexities of these matrices in many different models of computation and communication. We focus in this paper on the Walsh-Hadamard transform and on the `distance matrix', whose rows and columns correspond to binary vectors, and whose entries calculate whether the row and column are close in Hamming distance. Our results also generalize to other Kronecker powers and `Majority powers' of fixed matrices. We prove two new results about such matrices.

First, we prove new rigidity lower bounds in the low-rank regime where $r < \log N$. For instance, we prove that over any finite field, there are constants $c_1, c_2 > 0$ such that the $N \times N$ Walsh-Hadamard matrix $H_n$ satisfies $$\R_{H_n}(c_1 \log N) \geq N^2 \left( \frac12 - N^{-c_2} \right),$$ and a similar lower bound for the other aforementioned matrices. This is tight, and is the new best rigidity lower bound for an explicit matrix family at this rank; the previous best was $\R(c_1 \log N) \geq c_3 N^2$ for a small constant $c_3>0$. It also rules out an improvement to the current best constant-depth linear circuits for $H_n$ and other Kronecker powers by using asymptotic improvements to low-rank rigidity upper bounds; the current best uses the rank-1 rigidity of constant-sized matrices.

Second, we give new hardness amplification results, showing that rigidity lower bounds for these matrices for slightly higher rank would imply breakthrough rigidity lower bounds for much higher rank. For instance, if one could prove $$\R_{H_n}(\log^{1 + \varepsilon} N) \geq N^2 \left( \frac12 - N^{-1/2^{(\log \log N)^{o(1)}}} \right)$$ over any finite field for some $\varepsilon>0$, this would imply that $H_n$ is \emph{Razborov rigid}, giving a breakthrough lower bound in communication complexity. This may help explain why our best rigidity lower bounds for explicit matrices are stuck: there seems to be a big gap between what is known and what is required in the rank regime for Razborov rigidity, but our new required lower bound for rank $\log^{1 + \varepsilon} N$ appears close to what current techniques can prove.
\end{abstract}

\thispagestyle{empty}
\newpage\pagenumbering{arabic}
\section{Introduction}
\label{sec:introduction}
\input{introduction.tex}

\section{Technical Overview}
\label{sec:tec_overview}
\input{overview.tex}

\section{Preliminaries}
\label{sec:preliminaries}
\input{preliminaries}

\section{Nearly Tight Rigidity Lower bounds}
\label{sec:lower_bound}
\input{rigidity_lb.tex}


\section{Hardness Amplification for Rigidity}
\label{sec:amplification}
\input{hardness_amplification.tex}

\section{Discussion}
\label{sec:discussion}
\input{discussion}


{
\bibliographystyle{alpha}
\bibliography{reference.bib}
}



\end{document}
