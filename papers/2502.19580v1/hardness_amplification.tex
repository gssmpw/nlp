We now move on to proving our hardness amplification results.

\subsection{Hardness Amplification for Kronecker Products}

\KroneckerAmp*

In the remainder of this subsection, we prove Theorem~\ref{thm:amplification_kro}.

    As $\boolRigidity{A}{r} \le \delta \cdot q^2$, there is a rank-$r$ matrix $L \in \F_p^{q \times q}$, such that
    \begin{align*}
        \bool(A[i,j]) = \bool(L[i,j]), \quad \forall (i,j) \in I,
    \end{align*}
    where $I \subset [q]^2$ is a set of indices with $|I| \ge (1 - \delta) \cdot q^2$. The goal of this proof is to construct a low-rank matrix $\tilde{L} \in \F_p^{q^n \times q^n}$ that approximates $\kro{A}$.

    We index the rows and columns of $\kro{A}$ by vectors in $[q]^n$. Specifically, for any $x = (x_1, \ldots, x_n)$, and  $y = (y_1, \ldots, y_n) \in [q]^n$, the corresponding entry in $A$ is
    \begin{align*}
        \kro{A} [x,y] = A[x_1, y_1] \cdots A[x_n, y_n].
    \end{align*}

    
    Guided by the intuition introduced in \cref{subsec:overview_amplification_kro}, the matrix $\tilde{L}$ will be the low-rank approximation of the matrix $\kro{L}$ (which further approximates $\kro{A}$), constructed via a degree-$1$ polynomial $\tilde{\pi}$ that approximates the multiplication function $\pi: \F_p^n \to \F_p$, defined as follows.
    \begin{lemma}
    \label{lm:linear_polynomial_approx}
        Let $a$ be a uniformly random vector from $\F_p^n$. Define $\tilde{\pi}_a : \F_p^n \to \F_p$ as
        \begin{align*}
            \tilde{\pi}_a(z_1, z_2, \ldots, z_n) \defeq 1 + \sum_{i=1}^n a_i (z_i - 1).
        \end{align*}
        Then, for $z_1, \cdots, z_n \in \F_p$ that are not all $1$'s, over all random seeds $a$,
        \begin{align*}
            \Pr_{a \in \F_p^n}\Bk*{\bool\bk*{\tilde{\pi}_a (z_1, \cdots, z_n)} = 1} = \frac{1}{p}. 
        \end{align*}
        Moreover, for $z_1 = \cdots = z_n = 1$, 
        \begin{align*}
            \Pr_{a \in \F_p^n}\Bk*{\bool\bk*{\tilde{\pi}_a (z_1, \cdots, z_n)} = 1} = 1. 
        \end{align*}
    \end{lemma}
    \begin{proof}
        When $z_1, \ldots, z_n$ are not all $1$, $\tilde{\pi}_a(z_1, z_2, \ldots, z_n)$ is uniformly distributed over $\F_p$ over all random seeds $a$, hence $\Pr_{a \in \F_p^n}\Bk*{\bool\bk*{\tilde{\pi}_a (z_1, \cdots, z_n)} = 1} = {1}/{p}$. When $z_1 = \cdots = z_n = 1$, $\tilde{\pi}_a(z_1, z_2, \ldots, z_n)$ takes a fixed value $1$ hence $\Pr_{a \in \F_p^n}\Bk*{\bool\bk*{\tilde{\pi}_a (z_1, \cdots, z_n)} = 1} = 1. $
    \end{proof}

    Now, the low-rank matrix $\tilde{L}$ is defined as 
    \begin{align*}
        \tilde{L}[x,y] \defeq \tilde{\pi}_a \bk{L[x_1, y_1], \ldots, L[x_n, y_n]}
    \end{align*}
    for some seed $a \in \F_p^n$ to be determined. From the definition of $\tilde{\pi}_a$ in Lemma~\ref{lm:linear_polynomial_approx} above, we see that $\rank\bk{\tilde{L}} \le nr + 1 \le 2nr$, as $\tilde{L}$ can be represented as a linear combination of $n$ matrices of rank $r$ and an all-one matrix. Below, we prove that $\tilde{L}$ approximates $\kro{A}$ well in expectation over the choice of the seed $a$. Specifically, define
    \begin{align*}
        s \defeq \abs*{\BK*{(x,y) : \bool\bk{\tilde{L}[x,y]} \neq \bool\bk{\kro{A}[x,y]}}}, 
    \end{align*}
    and we will show that 
        $\E_{a \in \F_p^n}\Bk{s} \le q^{2n}\bk*{\frac{1}{2} - \frac{1}{2} \cdot \bk*{\frac{1}{2} - \alpha - \delta}^n},$
    i.e., 
    \begin{align*}
    \label{ineq:low-rank-approximates-well-to-prove}
        \Pr_{\substack{(x,y) \in [q]^{n} \times [q]^n \\ a \in \F_p^n}}\Bk*{\bool\bk{\tilde{L}[x,y]} \neq \bool\bk{\kro{A}[x,y]}} \le \frac{1}{2} - \frac{1}{2} \cdot \bk*{\frac{1}{2} - \alpha - \delta}^n, \numberthis
    \end{align*}
    where we view $x,y$ as uniformly chosen random variables from $[q]^{n}$. 
    Assuming that \eqref{ineq:low-rank-approximates-well-to-prove} holds, then there must be a specific choice of $a \in \F_p^n$ which achieves at most the expected number of errors, i.e., such that $s \le q^{2n}\bk*{\frac{1}{2} - \frac{1}{2} \cdot \bk*{\frac{1}{2} - \alpha - \delta}^n}$, and so the matrix $\tilde{L}$ with that seed $a$ is a good low-rank approximation of $\kro{A}$, which concludes the desired rigidity upper bound for $\kro{A}$. Hence, in order to conclude our proof of Theorem~\ref{thm:amplification_kro}, it suffices to prove \eqref{ineq:low-rank-approximates-well-to-prove}.

    We conclude \eqref{ineq:low-rank-approximates-well-to-prove} from the following lemma, where the random variables $A_i$'s and $L_i$'s are the Booleanization of matrix entires $A[x_i,y_i]$ and $L[x_i, y_i]$, respectively, with random $(x,y) \in [q]^n \times [q]^n$.
    \begin{lemma}
        Let $(A_1, L_1), \ldots, (A_n, L_n) \in \BK{-1, 1}^2$ be  independent pairs of random variables, which are identically distributed with $\Pr\Bk{A_i \neq L_i} \le \delta$ and $\abs{\Pr\Bk{A_i = 1} - \Pr\Bk{A_i = -1}} \le \alpha$ for each $i$. Let $a \in \F_p^n$ be drawn uniformly at random. Then,
        \begin{align*}
        \label{ineq:approx_in_RVs}
            \Pr\Bk{\tilde{\pi}_a(L_1, \ldots, L_n) \neq \pi(A_1, \ldots, A_n)} \le \frac{1}{2} -\frac{1}{2} \cdot \bk*{\frac{1}{2} - \alpha - \delta}^n. \numberthis
        \end{align*}
    \end{lemma}
    \begin{proof}
        Let $K$ be the number of $-1$'s in $A_1, \ldots, A_n$ and $L$ be the number of $-1$'s in $L_1, \ldots, L_n$. Let $p_j \defeq \Pr\Bk{A_i = j}$ and $\delta_j = \Pr\Bk{L_i \neq A_i \mid A_i = j}$ for $j \in \BK{-1,1}$. Then, $\abs{p_1 - p_{-1}} \le \alpha$ and $p_1 \delta_1 + p_{-1} \delta_{-1} \le \delta$.

        To compute the probability in the left hand side of \eqref{ineq:approx_in_RVs}, we enumerate the possible values of $K$ and $T$:
        \begin{itemize}
            \item For any $k \le n$, $\Pr\Bk{K = k} = \binom{n}{k} p_{-1}^k p_{1}^{n-k}$. Given that $K = k$, it follows that $\pi(A_1, \ldots, A_n)=(-1)^k$.
            \item Conditioned on $K = k$, we have $\Pr\Bk{T = 0 \mid K = k} = \delta_{-1}^k \bk{1 - \delta_1}^{n-k}$.
            \item Conditioned on $T = 0$ and $K = k$, according to \cref{lm:linear_polynomial_approx}, we have $\tilde{\pi}_a(L_1, \ldots, L_n) = 1$ with probability 1, which means $\tilde{\pi}_a(L_1, \ldots, L_n) = \pi(A_1, \ldots, A_n)$ with probability $\frac{1 + (-1)^k}{2}$.
            \item Conditioned on $T \neq 0$ and $K = k$, according to \cref{lm:linear_polynomial_approx}, we have $\tilde{\pi}_a(L_1, \ldots, L_n) = 1$ with probability $1/p$, which means $\tilde{\pi}_a(L_1, \ldots, L_n) = \pi(A_1, \ldots, A_n)$ with probability $\frac{1}{p} \frac{1 + (-1)^k}{2} + \frac{p-1}{p} \frac{1 - (-1)^k}{2} = \frac{1}{2} + \frac{(2-p)(-1)^k}{2p}$.
        \end{itemize}
        Combining these probabilities together, for any $k \le n$, we have 
        \begin{align*}
            &\Pr\Bk{\tilde{\pi}_a(L_1, \ldots, L_n) = \pi(A_1, \ldots, A_n) \mid K = k}\\
            {}={}& \Pr\Bk{T = 0 \mid K = k} \cdot \frac{1 + (-1)^k}{2} + \Pr\Bk{T \neq 0 \mid K = k} \cdot \bk*{\frac{1}{2} + \frac{(2-p)(-1)^k}{2p}}\\
            {}={}& \delta_{-1}^k \bk{1 - \delta_1}^{n-k} \cdot \frac{1 + (-1)^k}{2} + \bk*{1 - \delta_{-1}^k \bk{1 - \delta_1}^{n-k}} \cdot \bk*{\frac{1}{2} + \frac{(2-p)(-1)^k}{2p}}\\
            {}={}& \frac{1}{2} + \frac{(2-p)(-1)^k}{2p} + \delta_{-1}^k \bk{1 - \delta_1}^{n-k} (-1)^k \cdot \frac{p-1}{p},
        \end{align*}
        hence,
        \begin{align*}
            &\Pr\Bk{\tilde{\pi}_a(L_1, \ldots, L_n) \neq \pi(A_1, \ldots, A_n)}\\
            {}={}& \sum_{k=1}^n \Pr\Bk{K = k} \cdot \Pr\Bk{\tilde{\pi}_a(L_1, \ldots, L_n) \neq \pi(A_1, \ldots, A_n) \mid K = k}\\
            {}={}& \sum_{k=1}^n \binom{n}{k}p_{-1}^k p_1^{n-k} \bk*{\frac{1}{2} - \frac{(2-p)(-1)^k}{2p} - \delta_{-1}^k \bk{1 - \delta_1}^{n-k} (-1)^k \cdot \frac{p-1}{p}}\\
            {}={}& \frac{1}{2} - \frac{2-p}{2p}\bk*{-p_{-1} + p_1}^n - \frac{p-1}{p}\bk*{p_1(1 - \delta_1) - p_{-1}\delta_{-1}}^n\\
            {}\le{}& \frac{1}{2} + \frac{p-2}{2p}\alpha^n - \frac{p-1}{p}\bk*{\frac{1}{2} - \alpha - \delta}^n
            {}\le{} \frac{1}{2}  - \frac{1}{2} \cdot \bk*{\frac{1}{2} - \alpha - \delta}^n,
        \end{align*}
        where we used that $2\alpha + \delta < 1/2$ which implies $\alpha < 1/2 - \alpha - \delta$.
    \end{proof}


This concludes the proof of Theorem~\ref{thm:amplification_kro}.

\subsection{Hardness Amplification for Majority Product}

\MajorityAmp*


\begin{proof}
    As $\maj[k]{A}$ has probabilistic boolean rank $r$, there is a distribution $\mathcal{L}$ on matrices in $\F_p^{q^k \times q^k}$ with rank $r$, such that for any $(i,j) \in [q]^k \times [q]^k$ and a random matrix $L$ sampled from the distribution $\mathcal{L}$, 
    \[
    \label{ineq:probabilistic_rank}
        \Pr_{L \sim \mathcal{L}} \Bk*{\bool\bk{L[i,j]} \neq \bool\bk{\maj[k]{A}[i,j]}} \le \delta. \numberthis
    \]
    Below, we define a rank-$r$ matrix $\tilde{L} \in \F_p^{q^n \times q^n}$ that approximates $\maj{A}$ well.

    Recall the indices of rows and columns of $\maj{A}$ can be viewed as vectors in $[q]^n$. For any $x = (x_1, \ldots, x_n) \in [q]^n$, we define $x_\pre = (x_1, \ldots, x_k) \in [q]^k$ as the prefix of $x$ of length $k$. We define $\tilde{L}$ as 
    \begin{align*}
        \tilde{L}[x,y] = L[x_\pre, y_\pre], \quad \forall (x,y) \in [q]^n \times [q]^n,
    \end{align*}
    where $L$ is a matrix to be determined from the support of the distribution $\mathcal{L}$. By construction, $\tilde{L}$ has the same rank $r$ as $L$. To conclude a Boolean rigidity upper bound of $\maj{A}$, we need to show that if $L$ is drawn from $\mathcal{L}$, then  $\bool\bk{\tilde{L}[x,y]}$ disagrees with $\bool\bk{\maj{A}[x,y]}$ in at most $q^{2n} \bk*{\frac{1}{2} - \Omega\bk*{(1 - 2\delta) \cdot \frac{\sqrt{k}}{\sqrt{n}}}}$ entries in expectation, i.e.,
    \begin{align*}
    \label{ineq:low_rank_for_maj_to_prove}
        \Pr_{\substack{(x,y) \in [q]^n \times [q]^n\\L \sim \mathcal{L}}}\Bk*{\bool\bk{\tilde{L}[x,y]} \neq \bool\bk{\maj{A}[x,y]}} \le \frac{1}{2} -  \Omega\bk*{(1 - 2\delta) \cdot \frac{\sqrt{k}}{\sqrt{n}}}.\numberthis
    \end{align*}
    We can then fix $L$ be the best matrix in the support of $\mathcal{L}$ to achieve the desired error.
    
    To prove \eqref{ineq:low_rank_for_maj_to_prove}, recall that 
        $\maj{A}[x,y] = \Maj \bk{A[x_1, y_1], \ldots, A[x_n, y_n]}.$
    Let  $A_1, \ldots, A_n \in \BK{-1,1}$ be random variables obtained by taking random $(x,y) \in [q]^n\times[q]^n$ and then setting $A_i = A[x_i, y_i]$ for all $i$. Then, $A_1, \ldots, A_n$ are independent random variables with $\Pr\Bk{A_i = 1} = 1/2$.
    Moreover, by \eqref{ineq:probabilistic_rank}, for any possible choice of $A_1, \ldots, A_k$, over the randomness of $L$, we have 
    \[
        \Pr_{L \sim \mathcal{L}} \Bk*{\bool\bk{L[x_\pre, y_\pre]} \neq \Maj\bk{A_1, \ldots, A_k}} \le \delta.
    \]
    Hence, there is a random variable $\Delta \in \BK{-1,1}$ such that $\bool\bk{L[x_\pre, y_\pre]} = \Maj\bk{A_1, \ldots, A_k} \cdot \Delta$, with $\Pr_{L \sim \mathcal{L}}\Bk{\Delta = -1} \le \delta$ \emph{conditioned on} any possible choice of $A_1, \ldots, A_k$.
    
    We conclude \eqref{ineq:low_rank_for_maj_to_prove} from the following lemma.
    \begin{lemma}
         Let $A_1, \ldots, A_n \in \BK{-1, 1}$ be identically-distributed  independent random variables with $\Pr\Bk{A_i = 1} = 1/2$. Let $\Delta \in \BK{-1, 1}$ be a random variable which is independent of $A_{k+1}, \ldots, A_n$, such that $\Pr\Bk{\Delta = -1 | A_1 = A_1^*, \ldots, A_k = A_k^*} \le \delta$ for any possible choices of $A_1^*, \ldots, A_k^*$. Then, 
        \begin{align*}
        \label{ineq:approx_maj_in_RVs}
            \Pr\Bk*{\bk{\Delta \cdot \Maj\bk{A_1, \ldots, A_k}}  \neq \Maj\bk{A_1, \ldots, A_n}} \le \frac{1}{2} - \Omega\bk*{(1 - 2\delta) \cdot \frac{\sqrt{k}}{\sqrt{n}}}. \numberthis
        \end{align*}
    \end{lemma}
    \begin{proof}
        The first step of the proof is to assume without loss of generality that $\Pr\Bk{\Delta = -1 \mid A_1 = A_1^*, \ldots, A_k = A_k^*} = \delta$, i.e., $\Delta$ is independent to $A_1, \ldots, A_n$ with $\Pr\Bk{\Delta  = -1} = \delta$. This is because, conditioned on any possible $A_1^*, \ldots, A_k^*$, if we let $p^* \ge 1/2$ denote the probability that $\Maj\bk{A_1, \ldots, A_n}$ agrees with $\Maj\bk{A_1, \ldots, A_k}$ and $\delta^* \le \delta$ denote the probability that $\Delta = -1$, then
        \begin{align*}
            &\Pr\Bk*{\bk{\Delta \cdot \Maj\bk{A_1, \ldots, A_k}}  \neq \Maj\bk{A_1, \ldots, A_n} \mid A_1 = A_1^*, \ldots, A_k = A_k^*}\\
            {}={} &(1 - \delta^*) (1 - p^*) + \delta^* p^*
            {}={}  (1 - p^*) + (2p^* - 1) \delta^*.
        \end{align*}
        This probability is maximized when $\delta^*$ is equal to $\delta$. Hence, we can assume without loss of generality that $\Delta$ is independent of $A_1, \ldots, A_n$ and $\Pr\Bk{\Delta = -1} = \delta$. Then,
        \begin{align*}
        \label{ineq:prob_with_independent_error}
            &\Pr\Bk*{\bk{\Delta \cdot \Maj\bk{A_1, \ldots, A_k}}  \neq \Maj\bk{A_1, \ldots, A_n}} = (1-p) + (2p-1) \delta, \numberthis
        \end{align*}
        where $p \defeq \Pr\Bk{\Maj\bk{A_1, \ldots, A_k} = \Maj\bk{A_1, \ldots, A_n}}$. We use the following fact about binomial random variables to bound $p$:
        \begin{fact}
        \label{fact:concentration_of_binomial}
            Let $X_1, \ldots, X_n \in \BK{-1,1}$ be identically-distributed independent random variables with $\Pr\Bk{X_i = 1} = 1/2$. Let $X\defeq X_1 + \cdots + X_n$. Then, for any $a \ge 1$,
            \begin{align*}
                \Pr\Bk{X \ge a} = \frac{1}{2} - \Theta\bk*{\frac{a}{\sqrt{n}}}.
            \end{align*}
        \end{fact}
        To bound $p$, let $\mathcal{E}$ be the event that $A_1+ \cdots + A_k \ge \eps\sqrt{k}$ where $\eps >0$ is a small constant
        so that, by applying \cref{fact:concentration_of_binomial} on $A_1, \ldots, A_k$, we get $\Pr\Bk{\mathcal{E}}\ge \Omega(1)$. Moreover, conditioned on $\mathcal{E}$ happening, we know that $\Maj\bk{A_1, \ldots, A_k} = 1$, and so
        \begin{align*}
            &\Pr\Bk{\Maj\bk{A_1, \ldots, A_n} = 1}
            = \Pr\Bk{A_1 + \cdots + A_n \ge 0}\\
            {}\ge{} &\Pr\Bk*{A_{k+1} + \cdots + A_n \ge -\eps{\sqrt{k}}}
            {}\ge{} \frac{1}{2} + \Omega\bk*{\frac{\sqrt{k}}{\sqrt{n}}},
        \end{align*}
        where in the last inequality we apply \cref{fact:concentration_of_binomial} over $-A_{k+1}, \ldots, -A_{n}$. Hence,
        \begin{align*}
            \Pr\Bk[\big]{\Maj\bk{A_1, \ldots, A_k} = \Maj\bk{A_1, \ldots, A_n} \mid \mathcal{E}} \ge \frac{1}{2} + \Omega\bk*{\frac{\sqrt{k}}{\sqrt{n}}}.
        \end{align*}
        On the other hand, conditioned on $\mathcal{E}$ not happening, we still have 
        \begin{align*}
            \Pr\Bk[\big]{\Maj\bk{A_1, \ldots, A_k} = \Maj\bk{A_1, \ldots, A_n} \mid \overline{\mathcal{E}}} \ge \frac{1}{2}.
        \end{align*}
        Combining these probabilities together, we get
        \begin{align*}
            p = \Pr\Bk[\big]{\Maj\bk{A_1, \ldots, A_k} = \Maj\bk{A_1, \ldots, A_n}} 
            \ge \frac{1}{2} + \Pr\Bk{\mathcal{E}} \cdot \Omega\bk*{\frac{\sqrt{k}}{\sqrt{n}}} 
            \ge \frac{1}{2} + \Omega\bk*{\frac{\sqrt{k}}{\sqrt{n}}}.
        \end{align*}
        Substituting this bound into \eqref{ineq:prob_with_independent_error}, we get 
        \begin{align*}
            &\Pr\Bk*{\bk{\Delta \cdot \Maj\bk{A_1, \ldots, A_k}}  \neq \Maj\bk{A_1, \ldots, A_n}}\\
            {}={}& (1-p) + (2p-1) \delta
            {}={} \frac{1}{2} - \bk*{p - \frac{1}{2}} ( 1-2\delta)\\
            {}\le{}& \frac{1}{2} - \Omega\bk*{(1 - 2\delta) \cdot \frac{\sqrt{k}}{\sqrt{n}}}. \qedhere
        \end{align*}
    \end{proof}
\end{proof}




