\section{Method}

\subsection{Problem Formulation} \label{sec:problem-formulation}

% \begin{table}[t]
% \caption{Binary table of different types of speakers. The columns represent the Mixed Audio, and the Positive and Negative Audio Enrollment. Each row represents one type of speaker. In each table cell, $\surd$ means the speaker's voice exist in the audio of that column, and $\times$ means the contrary. We expect the model to extract only the \textcolor{green}{\textit{Classical Target}} speaker's voice from the mixed audio, and remove all \textcolor{red}{\textit{Disturbing}} speakers' voice. Whether the \textit{Undefined} speaker's voice exist in the output is dependent on the application scenario.}
% \label{speaker-type}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% % \begin{sc}
% \resizebox{1.0\columnwidth}{!}{
% \begin{tabular}{lcccc}
% \toprule
% Speaker Type & Mixed & Positive & Negative \\
% \midrule
% \textcolor{red}{Classical Disturbing} & $\surd$ & $\surd$ & $\surd$ \\
% \textcolor{LimeGreen}{Classical Target} & $\surd$ & $\surd$ & $\times$ \\
% \textcolor{red}{Additional Disturbing} & $\surd$ & $\times$ & $\surd$ \\
% Undefined & $\surd$ & $\times$ & $\times$ \\
% \textcolor{red}{Silent Disturbing Type 1} & $\times$ & $\surd$ & $\surd$ \\
% \textcolor{LimeGreen}{Silent Target} & $\times$ & $\surd$ & $\times$ \\
% \textcolor{red}{Silent Disturbing Type 2} & $\times$ & $\times$ & $\surd$ \\
% Irrelevant & $\times$ & $\times$ & $\times$ \\
% \bottomrule
% \end{tabular}}
% % \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}


\begin{figure}[!t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{src/img/venn-diagram.pdf}}
\caption{Venn diagram of different types of speakers.}
\label{fig:venn-diagram}
\end{center}
\vskip -0.3in
% \vspace{-10pt}
\end{figure}

\begin{figure*}[!t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{src/img/main-branch.pdf}}
\caption{Encoding and Extraction Branch model architecture and training pipeline.}
\label{fig:main-branch}
\end{center}
\vskip -0.3in
\end{figure*}

\begin{figure}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{src/img/encoder-pretraining.pdf}}
\caption{Encoding Branch and pretraining pipeline.}
\label{fig:encoder-pretraining}
\end{center}
\vskip -0.4in
\end{figure}


% In a target speaker extraction task, multiple speakers talk simultaneously in the scene. The target is to extract one or a subset of speakers' voice from the audio mixture. We refer to this audio mixture subject to extraction as Mixed Audio in the following explanation. Let $S = \{p_1, ..., p_N\}$ be the set of speakers in the scene, $S^M \subseteq S$ be the set of speakers present in the Mixed Audio, and $S^P, S^N \subseteq S$ be the set of speakers present in the Positive and Negative Audio Enrollment (by present in an audio mixture, we mean the speaker spoke when the audio mixture is recorded). We write the voice of each speaker $i$ in the Mixed, Positive, and Negative Audio as $a^{\{M, P, N\}}_i$, and the background noise of the scene in the three audio mixtures as $n^{\{M, P, N\}}$. We model the mixture of audio as addition of each speaker's voice and the background noise. Thus, the Mixed Audio $A^M$, the Positive Enrollment $A^P$, and the Negative Enrollment $A^N$ are: 
% $$A^M = sum(\{ a^M_i | i \in S^M\}) + n^M \,,$$
% $$A^P = sum(\{ a^P_i | i \in S^P\}) + n^P \,,$$
% $$A^N = sum(\{ a^N_i | i \in S^N\}) + n^N \,.$$ 

% The set of target speakers is defined as the difference $S^T = S^P - S^N$ between the positive and negative speaker set, and the set of disturbing speakers is $S^N - (S^P - S^N)$. Thus, the target speakers' voice in the positive audio enrollment is 
% $$A_{clean} = sum(\{ a^P_i | i \in S^T\}) \,,$$
% and the target speakers' voice in the mixed audio to be extracted (i.e. model prediction ground truth) is 
% $$A_{tgt} = sum(\{ a^M_i | i \in S^T\}) \,.$$





In a target speaker extraction task, multiple speakers talk simultaneously in the scene. The aim is to extract one or a subset of the speakers' voices from the audio mixture. We refer to this audio mixture subject to extraction as Mixed Audio in the following explanation. Let $S = \{p_1, ..., p_N\}$ be the set of speakers in the scene and $S^M \subseteq S$ be the set of speakers present in the Mixed Audio (by present in an audio mixture, we mean the speaker spoke when the audio mixture is recorded). To obtain the target speaker's voice characteristics, we record Positive and Negative Enrollments capturing the audio mixtures with or without the target speaker's voice. Both of these two enrollments contain a mixture of multiple disturbing speakers talking at the same time. We define the speakers whose voice is recorded in the Positive Enrollment as $S^P$, and speakers in the Negative Enrollment as $S^N$. The set of target speakers is defined as the set difference $S^T = S^P - S^N$. 

% The set of positive speakers containing the target speaker is $S^P$, and the set of negative speakers containing all the disturbing speakers is $S^N$. Thus, we could obtain the set of target speaker from set difference $S^T = S^P - S^N$. \textcolor{red}{this is confusing, should not say positive speaker containing target, because this is like positive speaker only contain target}

% Our task is to learn the characteristics of the target speaker through comparing the , to identify the speaker who talked in the positive but not the negative example, and extract the target speaker's voice from another audio mixture of the target speaker talking with the same set of disturbing speakers talking in the background.

Each speaker may speak different content in the Positive Enrollment, Negative Enrollment, and Audio Mixture. We write the voice of each speaker $i$ in the Mixed, Positive, and Negative Audio as $a^{\{M, P, N\}}_i$, and the background noise of the scene in the three audio mixtures as $n^{\{M, P, N\}}$. The mixture of audio is modeled as the addition of each speaker's voice and the background noise. Thus, the Mixed Audio $a^M$, the Positive Enrollment $a^P$, and the Negative Enrollment $a^N$ are: 
\begin{equation}\scriptsize
a^M = \sum_{i \in S^M} a^M_i + n^M, \,
a^P = \sum_{i \in S^P} a^P_i + n^P ,\, 
a^N = \sum_{i \in S^N} a^N_i + n^N \,.
\end{equation}
% \vspace{-8pt}
% \begin{equation}
% a^P = \sum_{i \in S^P} a^P_i + n^P ,\, 
% \end{equation}
% \vspace{-4pt}
% \begin{equation}
% a^N = \sum_{i \in S^N} a^N_i + n^N \,.
% \end{equation}
% \vspace{-4pt}

The target speakers' voice $a_{clean}$ in the positive audio enrollment and the target speakers' voice $a_{tgt}$ in the Mixed Audio (i.e. model prediction ground truth) are

\begin{equation}
a_{clean} = \sum_{i \in S^T} a^P_i , \quad a_{tgt} = \sum_{i \in (S^T \cap S^M)} a^M_i \,.
\end{equation}





% Different application scenarios could be formulated by considering the set difference between $S^M, S^P$ and $S^N$. For example, when $|S^P - S^N| > 1$, it means there are multiple target speakers, when $|S^N - S^M| > 0$, it means there are disturbing speakers captured in the negative audio enrollment but not present in the mixed audio. 


% We only require the $S^M \subseteq (S^P \cup S^N)$, this means that the speaker in the mixed audio should present in either the positive or the negative speaker set. Otherwise, it is a speaker not defined as either the target or the disturbing speaker. Depending on the application scenario, user may want to remove or also extract such unknown speaker's voice from the mixture. We show the model performance when not explicitely trained on samples with such undefined speakers presenting in Sec ???. 

Overall, we draw a Venn diagram (Figure \ref{fig:venn-diagram}) to show the different types of speakers. In detail, based on whether a speaker's voice exists in the Positive/Negative/Mixed Audio, speakers are classified into 8 types. In addition to the \textit{Classical Target} speaker, whose voice is expected to be the sole voice present in the extracted audio, speakers belonging to the four \textit{Disturbing} speaker types should not be present in the extracted audio. Even though \textit{Silent Target}, \textit{Silent Disturbing Type 1}, and \textit{Silent Disturbing Type 2} speakers are not present in the Mixed Audio, their presence in the Positive and Negative Enrollments might affect the model performance on encoding the target speaker's voice. We perform an investigation on the influence in Section \ref{sec:special-spk} and in Appendix \ref{app:special-spk}. Additionally, the model's behaviour concerning \textit{Undefined} speakers is discussed in detail in Section \ref{sec:special-spk}.

% We only expect the \textit{Classical Target} speaker's voice to present in the extract audio. 

% The ... speakers are the target speakers, because they present in the Positive Enrollment but not in the Negative Enrollment. However, since \textit{Silent Target} speaker does not speak in the Mixed Audio, only \textit{Classical Target} speakers' voice should present in the extracted audio. 

% The speakers are the disturbing speakers, because they exist in the Negative Enrollment. Model should remove their voice in the Mixed Audio. 

% The ... speakers do not exist in the Mixed Audio, thus, their voice should never appear in the extracted audio. However, since their voice is in the input Positive and Negative Enrollments, the extracted target speaker's embedding quality might be affected. We show our model's performance when these speakers present in the experiment section and in appendix. 




% Combination of different number of speakers of each type represents different extraction scenarios. For example, the combination of \textit{one Classical Target speaker, two Classical Disturbing speakers, and three Silent Disturbing Type 2 speakers} represents a six speaker extraction scenario. In the positive audio enrollment and extraction mixture, three voices are present (one target and two disturbing speakers). In the negative audio enrollment, five voices are present (two disturbing and three Silent Disturbing Type 2 speakers). The model aims to isolate the target speaker's voice from the mixture, using the positive and negative audio inputs as context. In the experiment section, we explain the scenario each experiment focuses on by indicating the combination of speaker types at the start of each experiment explanation. 


% In training stage, we only train our model on samples with 1 \textit{Target} speaker and 2 \textit{Disturbing} speakers. In Sec ???, we show our model performance when different types of speakers present.As a result, to show our model performance under the presents of special speakers, we fixed the scene to contain 1 \textit{Target} speaker and 1 \textit{Disturbing} speaker, and add variable number of special speaker of the same type. 



% However, since multiple speakers could share the same speaker type, and different types of speakers could exist at the same time, this result in significantly large number of user type combinations. We leave exploration on model performance when more complicated combinations of different types of speakers present for further work. 

% \textcolor{red}{this paragraph needs change: the introduction on speaker type table should apear in problem formulation, but [what combination of speaker type did we use in training] should appear in Training Method section, and [how did we eval the model] should appear in ablation section }

\subsection{Model Architecture}


As shown in Figure \ref{fig:main-branch}, we adopt TF-GridNet \cite{tfgridnet} as our \textbf{Encoding} and \textbf{Extraction Branches}' backbone, and introduce an attention-based \textbf{Pos-Neg Fusion Module} and an \textbf{Extraction Branch Fusion Module} to effectively integrate enrollment information into the extraction model.

The model architecture and training method for encoding the target speaker's feature is shown in Figure \ref{fig:encoder-pretraining}. \textbf{The Encoding Branch} adopts the TF-GridNet architecture as the backbone, which consists of a 2D convolution layer followed by stacks of three TF-GridNet blocks. For encoding, we directly use the output from the last TF-GridNet block as the enrollment representation. Each block processes the features through three submodules sequentially: the Intra-frame Spectral Module, which models inter-frequency information within each frame (i.e., timestep), the Sub-band Temporal Module, which models temporal information within sub-bands, and the Full-band Self-attention Module, which captures the long-range frame information between frames. 
% Both the Intra-frame Spectral Module and Sub-band Temporal Module are composed of a BiLSTM layer. After these two modules, the output feature has a shape of $[C \times T \times F]$, where $C$, $T$, and $F$ denote the channel number, frame number, and frequency bin number, respectively. The Full-band Self-attention Module flattens the feature into $[T, C \times F]$ shape and performs self-attention along the temporal dimension to capture long-range frame information between frames. The output of a TF-Gridnet block could be considerred as a length $T$ audio embeddings sequence, where each embedding vector has dimension $C \times F$. 
Positive and Negative Enrollments are encoded using the same TF-GridNet encoder, producing two sequences of embeddings $E_{pos}$ and $E_{neg}$ with shapes $[T_{pos}, C \times F]$ and $[T_{neg}, C \times F]$, respectively, where $C$ and $F$ denote the channel and the frequency bin number, and $T_{pos}, T_{neg}$ are the frame numbers in the temporal dimension of the positive and negative enrollment embeddings. 

% Let $R \in \mathbf{R}^{2 \times T \times F}$ be the input enrollment audio STFT spectrogram with $T$ frames and $F$ frequency bins. The channel size of 2 is the result of concatenating the real and imaginary part of the STFT along the channel dimension. 
% Each block process the feature with 3 submodules sequentially: the Intra-frame Spectral Module, which models inter-frequency information using BiLSTM; the Sub-band Temporal Module, which models temporal information within sub-bands using BiLSTM; and the 
% In the first Intra-frame Spectral Module, the feature is unfolded along the temporal dimension to obtain stacked embedding of nearby frames at each time step. The feature then passes through a Bidirectional-LSTM layer to model the inter-frequency information within each frame. In the second Sub-band Temporal Module submodule, the unfold operation is performed along the frequency dimension and BiLSTM is applied in the temporal dimension to model temporal information within each sub-band. Finally, an self-attention operation is performed in the temporal dimension to capture the long-range global information between frames.
% . The output of a TF-Gridnet block could be considerred as a sequence of audio embedding, where sequence length is proportional to the frame number of the STFT of the audio enrollments. \textcolor{red}{img left top: tfgridnet block} 

\textbf{The Pos-Neg Fusion Module} extracts the target speaker's voice embedding through comparing the positive and negative enrollment embeddings $E_{\text{pos}}, E_{\text{neg}}$. We first element-wise-add each embedding in both sequences with a learnable segmentation embedding, to indicate whether it encodes frames of the Positive or Negative Enrollment. The two embedding sequences then concatenate along the temporal dimension to form a feature of shape $[T_{\text{pos}} + T_{\text{neg}}, C \times F]$, and pass through two layers of Full-band Self-attention Module. By performing self-attention calculation, the Full-band Self-attention Module learns the difference between embeddings of the positive and negative enrollments and encodes the target speaker that only exists in the positive enrollment by removing the disturbing speakers' characteristics. We reshape the output embeddings corresponding to the positive enrollment as the extracted target speaker embedding, which results in a $[C, T_{\text{pos}}, F]$ shape embedding $E_{\text{pos-neg}}$ being extracted after the Pos-Neg Fusion Module. 

% We adopt the Pos-Neg Fusion Module to extract the clean target speaker's voice embedding from the positive and negative enrollment embeddings $R_{pos}, R_{neg}$.  We first reshape the embedding to shape $???$, and elemwise-add the positive and negative enrollment embedding with a learnable segmentation token, to indicate whether the token belongs to the positive or the negative enrollment, then we concat two sequences of embeddings along the temporal dimension, to obtain an input sequence of shape $[C, T_{pos} + T_{neg}, F]$. We then pass through a single layer of Full-band Self-attention Module. Through calculating the attention value between an positive embedding and all the positive and negative embeddings, the model learns to remove the disturbing speakers' characteristics from the encoding, which results in only the target speaker's voice characteristics remained in the token. We use the refined audio embedding corresponding to the positive audio embedding input as the extracted target speaker embedding, which result in a $[C, T_{pos}, F]$ shape embedding being extracted. 

% In order to obtain the target speaker's embedding from the encoded Positive and Negative Enrollments, we adopt the Pos-Neg Fusion Module to extract the target speaker's embedding from the encoded positive and negative enrollment embeddings. 

% design the encoder head model to perform self-attention on the concatenated sequence of positive and negative audio embedding sequence along the temporal dimension. After one layer of self-attention calculation, we use the refined audio embedding corresponding to the positive audio embedding input as the extracted enrollment embedding. 

In the \textbf{Extraction Branch}, we modify the TF-GridNet block to perform causal inference by adopting the same modification as LookOnceToHear \cite{Veluri2024lookonce}. In particular, we remove the global layer normalization after the first convolution layer, change the BiLSTM in the TF-GridNet blocks to unidirectional LSTM, and constrain the Full-band Self-attention Modules to calculate the causal attention value of a one-time frame with only frames before it. Three causal TF-GridNet blocks modified as above are used in the Extraction Branch. 

\textbf{Extraction Branch Fusion Blocks} are added after the first two causal TF-GridNet blocks to integrate the target speaker embedding with the extraction model. To reduce computation time, we apply non-overlapping average pooling with 20 kernel size along the temporal dimension to the extracted target speaker embedding. We discuss the effect of different kernel sizes on the model performance and inference time in Section \ref{sec:ablation}. In the Extraction Branch Fusion Block, we adopt three $1 \times 1$ convolution layers to ensure the pooled target speaker embedding $E_{\text{pos-neg}}$ and the output from the previous TF-GridNet block $E_{\text{layer l}}$ have the same channel number $H$. The target speaker embeddings are reshaped to $[T_{\text{pos}}, H \times F]$ and used as the Key and Value feature, while the output from the previous TF-GridNet block is reshaped to $[T_{\text{mix}}, H \times F]$ and serves as the Query. Information fusion between Query, Key, and Value is performed using the multi-head attention calculation in the Full-band Self-attention Module. 


% We perform cross-attention to fuse both information by using the pooled target speaker embedding $???$ as Key Value, and the output $???$ from the previous TF-GridNet block as Query. The cross attention is achieved by changing the Key and Value matrix in the Full-band Self-attention Module to the key and vau

% The detailed cross-attention architecture is shown in Appendix ???.
% In particular, following \cite{tfgridnet}, we obtain the cross-attention result of each head by passing $???$ through $1 \times 1$ 2D convolution layers, PReLU activations, Layer Normalizations, and reshape to obtain the Key $???$, Query $???$, and Value $???$. We calculate the cross-attention result as $???$. 
% We aggregate the attention result from each head through concatnetation along the ??? dimension, obtaining a $???$ shape feature. Finally, we refine the multi-head attention result using one $1 \times 1$ 2D convolution layer with PReLU activation and Layer Normalization. 

In comparison to encoding the target speaker with a fixed-sized embedding, using cross-attention allows the target speaker to be represented as a longer sequence of embeddings, which helps the model achieve better extraction quality (as shown in Section \ref{sec:ablation}). In addition, the memory complexity of the cross-attention fusion is $O(T_{\text{mix}}\times T_{\text{pos}} + T_{\text{mix}}\times H \times F)$ with respect to the Mixed Audio frame number $T_{\text{mix}}$, and the computation complexity is $O(T_{\text{mix}}\times T_{\text{pos}} \times H \times F)$. As the length of the audio mixture subject to extraction increases, the memory and time complexity of the fusion module scale linearly. Moreover, by selecting models with different pooling sizes for the target speaker embedding, we can achieve a balance between extraction quality and inference time/memory requirements. This flexibility broadens the model architecture's applicability, from lightweight audio extraction on portable devices with limited computation and storage capabilities to professional audio editing tools where higher computational power and relaxed speed constraints are available.


% Through controlling the pooling size Sec ???, we could control the  achieve real-time target speaker extraction without experiencing significant performance drop. 




% The fusino block use target speaker embedding as Key and Values, and preceding TF-Gridnet block outputs as Query, to performs cross attention fusion. The fused feature output is unflattened back ... and pass through the next TF-Gridnet block input. We used 3 TF-Gridnet blocks in the main extraction branch, and only performs fusion after the first 2 blocks. \textcolor{red}{reference bottum image, show in inference training, use dashed lines to show training stages}


\subsection{Training Method}

The training pipeline includes two stages. As shown in Figure \ref{fig:main-branch}, the first stage pretrains the encoder and the Pos-Neg Fusion Module from scratch to extract the target speaker's embedding from the positive and negative audio enrollments. We obtain the encoding of the target speaker's embedding by passing the clean target speaker's voice through a trained frozen TF-GridNet encoder. 
% In other words, we pretrain the encoder and the Pos-Neg Fusion Module by distilling it to predict the clean target speaker's embedding. 
Formally, the pretraining loss is written as 
\begin{equation}
L_{pretrain} = \|E_{\text{clean}} - E_{\text{pos-neg}}\|^2 \,,
\end{equation}

where $E_{\text{pos-neg}}$ is the target speaker's embedding extracted by our model from input pair $(a^P, a^N)$, and $E_{\text{clean}}$ is the embedding of the clean target speaker voice $a_{\text{clean}}$ encoded by a trained TF-GridNet encoder from \cite{Veluri2024lookonce}.

In the second stage, we train the Extraction Branch to extract the target speaker from the Audio Mixture. The training loss for the second stage is the negative SNR value of the extracted audio $\hat{a}_{tgt}$ and the ground truth target speaker speech $a_{tgt}$: 

\begin{equation}
L_{snr} = -\text{SNR}(\hat{a}_{tgt}, a_{tgt}).
\end{equation}


More training and model details are discussed in the Appendix. 

% The same dataset is used in both training stages. 

% The $m_{frozen}$ parameters are initialized from the LookOnceToHear \cite{Veluri2024lookonce} encoder and frozen throughout the training. 

% The first stage is introduced to reduce the instability in training. As shown in Sec ???, removing the first training stage results in significantly slower convergence speed (??? and more unstable convergence). 

% TODO: explain in detail the noise in training as motivation of pretraining

% include the main extraction model in the training to extract the target speaker from the audio mixture. 
% Such two staged training allows our model to achieve higher convergence 



% We supervise our model with negative SNR value of the extracted audio and the ground truth target speaker speech:

% However, training our model from scratch result in significantly unstable learning . Such unstability is caused by 

% Simple training  our model needs to encode the target speaker's characteristic from the audio enrollment before performing extraction


% Such two-staged extraction architecture result in unstable learning, especially when the audio enrollment is a mixture, we thus pretrain the encoder before using its output to train the extraction model. 
