\section{Experiment}

\begin{table*}[!t]
\caption{Comparison between our method and the baselines' performance in one \textit{Classical Target} and multiple \textit{Classical Disturbing} speakers scenario. The audio type column indicates whether the model is trained and tested on monaural(Mono) or binaural(Bi) audio.}
\label{tab:main-monaural}
\vskip -0.3in
% \vskip 0.15in
\begin{center}
% \begin{small}
% \begin{sc}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lccccccc}
\toprule
% \specialrule{2pt}{0pt}{0pt}
\textbf{Method} & \textbf{Audio Type} & \textbf{Metric} & \textbf{2 Speakers} & \textbf{3 Speakers} & \textbf{4 Speakers} & \textbf{5 Speakers} & \textbf{6 Speakers} \\
\midrule
% Ours & SNR    & 7.40$\pm$2.17 & 5.59$\pm$2.23 & 3.68$\pm$2.47 & 2.44$\pm$2.44 & 1.13$\pm$2.36 \\
\multirow{2}{*}{Ours (Classical-monaural)} & \multirow{2}{*}{Mono} & SNRi   & \textbf{10.81$\pm$2.43} & \textbf{10.90$\pm$2.68} & \textbf{10.34$\pm$2.76} & \textbf{10.24$\pm$2.73} & \textbf{9.81$\pm$2.74} \\
                      % & SI-SNR & 6.49$\pm$3.41 & 3.99$\pm$3.95 & 0.72$\pm$5.42 & -1.99$\pm$6.23 & -5.46$\pm$7.13 \\
                      & & SI-SNRi & \textbf{9.90$\pm$3.42} & \textbf{9.31$\pm$3.99} & \textbf{7.38$\pm$5.19} & \textbf{5.80$\pm$5.88} & \textbf{3.22$\pm$6.42} \\
\midrule
% Ours (Film Fusion) & SNR    & 6.59$\pm$2.11 & 4.53$\pm$2.38 & 3.27$\pm$2.34 & 1.78$\pm$2.39 & 0.82$\pm$2.36 \\
\multirow{2}{*}{Ours (Film fusion)} & \multirow{2}{*}{Mono} & SNRi   & 10.01$\pm$2.55 & 9.98$\pm$2.68 & 9.93$\pm$2.69 & 9.55$\pm$2.61 & 9.38$\pm$2.50 \\
                     % & SI-SNR & 5.46$\pm$3.41 & 2.22$\pm$4.71 & 0.03$\pm$5.14 & -3.05$\pm$5.69 & -5.51$\pm$6.39 \\
                     & & SI-SNRi & 8.88$\pm$3.52 & 7.66$\pm$4.50 & 6.67$\pm$4.92 & 4.73$\pm$4.94 & 3.03$\pm$5.61 \\
\midrule
% TCE        & SNR    & 4.89$\pm$2.44 & 2.68$\pm$2.54 & 1.22$\pm$2.36 & -0.11$\pm$2.30 & -0.83$\pm$2.08 \\
\multirow{2}{*}{TCE \cite{Chen2024tce}} & \multirow{2}{*}{Mono} & SNRi   & 8.46$\pm$2.57 & 8.24$\pm$2.58 & 8.19$\pm$2.55 & 7.73$\pm$2.10 & 7.75$\pm$2.05 \\
                     % & SI-SNR & 3.18$\pm$3.82 & -0.42$\pm$4.57 & -3.28$\pm$5.10 & -5.89$\pm$4.84 & -8.00$\pm$4.88 \\
                     & & SI-SNRi & 6.75$\pm$3.66 & 5.16$\pm$4.11 & 3.69$\pm$4.39 & 1.94$\pm$3.47 & 0.57$\pm$3.80 \\
\midrule
% SpeakerBeam & SNR    & -3.72$\pm$1.34 & -3.77$\pm$1.29 & -3.74$\pm$1.39 & -3.76$\pm$1.32 & -3.68$\pm$1.34 \\
\multirow{2}{*}{SpeakerBeam \cite{speakerbeam}} & \multirow{2}{*}{Mono} & SNRi   & -0.42$\pm$2.15 & 1.51$\pm$2.41 & 3.10$\pm$2.52 & 3.85$\pm$2.65 & 4.91$\pm$2.72 \\
                     % & SI-SNR & -6.32$\pm$6.22 & -8.18$\pm$5.33 & -9.64$\pm$4.44 & -10.37$\pm$4.28 & -11.65$\pm$4.15 \\
                     & & SI-SNRi & -3.01$\pm$5.98 & -2.90$\pm$5.08 & -2.81$\pm$3.96 & -2.75$\pm$3.68 & -3.03$\pm$3.38 \\
\midrule
% NMF & SNR       & 0.78$\pm$1.60 & 0.37$\pm$1.33 & 0.12$\pm$1.15 & -0.14$\pm$1.05 & -0.20$\pm$0.94 \\
\multirow{2}{*}{NMF \cite{NMF}} & \multirow{2}{*}{Mono} & SNRi      & 4.24$\pm$1.60 & 5.65$\pm$1.85 & 6.87$\pm$2.23 & 7.55$\pm$2.22 & 8.29$\pm$2.40 \\
                     % & SI-SNR    & -5.12$\pm$4.84 & -6.77$\pm$4.63 & -8.48$\pm$5.23 & -10.49$\pm$6.06 & -11.76$\pm$7.03 \\
                     & & SI-SNRi   & -1.65$\pm$3.78 & -1.50$\pm$3.57 & -1.71$\pm$3.84 & -2.80$\pm$5.05 & -3.28$\pm$5.74 \\
\midrule \midrule
\multirow{2}{*}{Ours (Classical-binaural)} & \multirow{2}{*}{Bi} & SNRi   & \textbf{8.97$\pm$2.39} & \textbf{8.83$\pm$2.64} & \textbf{9.04$\pm$2.53} & \textbf{9.28$\pm$2.51} & \textbf{10.06$\pm$2.67} \\
                    % & SI-SNR & 3.52$\pm$3.27 & -0.09$\pm$4.40 & -2.73$\pm$4.14 & -5.22$\pm$4.41 & -7.97$\pm$3.62 \\
                    & & SI-SNRi & \textbf{7.18$\pm$3.25} & \textbf{5.42$\pm$4.26} & \textbf{4.16$\pm$3.82} & \textbf{2.66$\pm$3.85} & \textbf{1.46$\pm$3.22} \\
\midrule
% Lookonce & SNR    & 3.58$\pm$2.33 & 2.30$\pm$2.66 & 1.51$\pm$2.05 & 0.91$\pm$1.92 & 0.12$\pm$1.73 \\
\multirow{2}{*}{LookOnceToHear \cite{Veluri2024lookonce}} & \multirow{2}{*}{Bi} & SNRi   & 7.10$\pm$3.16 & 7.72$\pm$3.16 & 8.34$\pm$2.75 & 8.62$\pm$2.72 & 9.28$\pm$2.61 \\
                     % & SI-SNR & 0.48$\pm$5.60 & -2.55$\pm$6.60 & -4.54$\pm$6.14 & -6.53$\pm$6.09 & -9.29$\pm$5.38 \\
                     & & SI-SNRi & 4.05$\pm$5.80 & 2.93$\pm$6.45 & 2.32$\pm$6.02 & 1.34$\pm$5.71 & 0.09$\pm$5.04 \\

\bottomrule
% \specialrule{2pt}{0pt}{0pt}
\end{tabular}}
% \end{sc}
% \end{small}
\end{center}
\vskip -0.2in
\end{table*}

\subsection{Datasets and Baselines}

\paragraph{Datasets}  We select the LibriSpeech dataset \cite{librispeech} to construct our Positive, Negative, and Mixed Audio. The background noise $n^{\{M, P, N\}}$ in the Positive, Negative, and Mixed Audio are from the WHAM noise dataset \cite{wham}. 
Besides, following the experiment configuration in LookOnceToHear \cite{Veluri2024lookonce}, we also construct binaural Positive/Negative/Mixed Audio samples by convolving speech from each speaker with the binaural RIR data provided in the ASH-Listening-Set dataset \cite{ASH-brir}. We refer to the model trained on monaural data as \textit{Classical-Monaural}, and on binaural data as \textit{Classical-Binaural}. Both models are trained on scenarios containing one \textit{Classical Target} and two \textit{Classical Disturbing} speakers. We evaluate by calculating the SNR and SI-SNR of the extracted speech, along with the improvement in SNR (SNRi) and SI-SNR (SI-SNRi), which are extracted speech's metric minus the input Mixed Audio's metric. We report the average and standard deviation of these metrics on 2000 randomly generated samples from the LibriSpeech dataset \texttt{test-clean} component.

\paragraph{Baselines} We compare our monaural model's performance on monaural target speech extraction with TCE \cite{Chen2024tce}, SpeakerBeam \cite{speakerbeam}, and non-negative matrix factorization (NMF) method \cite{NMF}. We compare our binaural model's reverberant target speech extraction performance with the LookOnceToHear model \cite{Veluri2024lookonce}. 

1. TCE \cite{Chen2024tce} considers speakers whose voices do not overlap with a given speaker as the target speakers and extracts their voices. The model takes in a d-vector embedding of the given speaker and removes all disturbing speakers who talk at the same time as the given speaker. 

% In other word, the model identify the timeframes when the given speaker is speaking, and extract the speaker that present in the other timesteps when the given speaker is not talking. In other word, the model obtain the negative audio examples using the audio embedding of one of the speakers in the negative embedding. 

2. LookOnceToHear \cite{Veluri2024lookonce} is a binaural target speech extraction method based on noisy audio examples. This method uses beamforming to extract the target speaker's characteristics from the noisy positive enrollment, which contains audio from multiple additional speakers. 

3. SpeakerBeam \cite{speakerbeam} is a target speech extraction model using clean target speaker enrollment as the extraction condition. We include this model as a baseline to show the influence of having disturbing speakers in the audio enrollment on the model performance.

4. NMF \cite{NMF} is a non-deep-learning-based audio separation technique. Since the method could not perform target speaker extraction conditioned on noisy audio examples, we report its extraction quality by selecting the audio with the highest SNR among its output as its extracted audio. 

\subsection{Result on Monaural and Binaural Reverberant Target Speech Extraction} \label{sec:main-monaural}

\textcolor{gray}{Experiment Scenario: one \textit{Classical Target} and N \textit{Classical Disturbing} speakers.}

We compare our model with baseline methods under scenarios where different numbers of speakers are present.
% (i.e. we focus on scenario where only one \textit{Classical Target} and multiple \textit{Classical Disturbing} speakers exist. The number of \textit{Classical Target} speakers varies between 1 to 5).
Table \ref{tab:main-monaural} shows the monaural target speaker extraction performance. SpeakerBeam \cite{speakerbeam} fails to perform when multiple speakers are present in the audio enrollments. TCE \cite{Chen2024tce} achieves better performance than SpeakerBeam \cite{speakerbeam} baseline since it does not rely on the clean target speaker's enrollment. However, the TCE model shows worse performance than our method under all different numbers of disturbing speakers. 

% \subsection{Result on Binaural Reverberant Target Speech Extraction} \label{sec:main-binaural}

% \begin{table}[t]
% \caption{Binaural main result.}
% \label{tab:main-binaural}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% % \begin{sc}
% \resizebox{1.0\columnwidth}{!}{
% \begin{tabular}{lcccccr}
% \toprule
% \textbf{Method} & \textbf{Metric} & \textbf{2 Speakers} & \textbf{3 Speakers} & \textbf{4 Speakers} & \textbf{5 Speakers} & \textbf{6 Speakers} \\
% \midrule
% % Our Method & SNR    & 5.30$\pm$1.98 & 3.34$\pm$1.87 & 2.14$\pm$1.51 & 1.40$\pm$1.28 & 0.65$\pm$0.78 \\
% \multirow{2}{*}{Ours} & SNRi   & \textbf{8.97$\pm$2.39} & \textbf{8.83$\pm$2.64} & \textbf{9.04$\pm$2.53} & \textbf{9.28$\pm$2.51} & \textbf{10.06$\pm$2.67} \\
%                     % & SI-SNR & 3.52$\pm$3.27 & -0.09$\pm$4.40 & -2.73$\pm$4.14 & -5.22$\pm$4.41 & -7.97$\pm$3.62 \\
%                     & SI-SNRi & \textbf{7.18$\pm$3.25} & \textbf{5.42$\pm$4.26} & \textbf{4.16$\pm$3.82} & \textbf{2.66$\pm$3.85} & \textbf{1.46$\pm$3.22} \\
% \midrule
% % Lookonce & SNR    & 3.58$\pm$2.33 & 2.30$\pm$2.66 & 1.51$\pm$2.05 & 0.91$\pm$1.92 & 0.12$\pm$1.73 \\
% \multirow{2}{*}{LookOnceToHear \cite{Veluri2024lookonce}} & SNRi   & 7.10$\pm$3.16 & 7.72$\pm$3.16 & 8.34$\pm$2.75 & 8.62$\pm$2.72 & 9.28$\pm$2.61 \\
%                      % & SI-SNR & 0.48$\pm$5.60 & -2.55$\pm$6.60 & -4.54$\pm$6.14 & -6.53$\pm$6.09 & -9.29$\pm$5.38 \\
%                      & SI-SNRi & 4.05$\pm$5.80 & 2.93$\pm$6.45 & 2.32$\pm$6.02 & 1.34$\pm$5.71 & 0.09$\pm$5.04 \\
% \bottomrule
% \end{tabular}}
% % \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% \textcolor{gray}{Experiment Scenario: one \textit{Classical Target} Speaker and N \textit{Classical Disturbing} Speakers}

In the multi-channel target speech extraction task, prior work \cite{Veluri2024lookonce} has explored using beamforming to extract the target speaker's characteristics. In this section, we show that additionally including the Negative Enrollment helps improve model performance. As shown in Table \ref{tab:main-monaural}, our method achieves better extraction performance in all scenarios with 1 to 5 disturbing speakers. 

In contrast to the monaural experiment results, the binaural model's SNRi performance increases as the number of disturbing speakers increases. This shows that the binaural audio input helps the model estimate the target speaker's voice intensity more effectively. However, this does not indicate better extraction quality, as the SI-SNRi of both our model and the baseline method decreases.
% Since we adopted the same encoder and separator model architecture as the lookonce[] model, this shows the effectiveness of the proposed fusion method in leveraging the negative audio example to improve extraction quality. 

In addition, in comparison to the monaural extraction SI-SNRi result, our model trained on binaural data shows around 3 dB lower SI-SNRi performance. This is because the binaural reverberant target speech extraction is more difficult than monaural speech extraction. In binaural speech extraction, the Positive and Negative Enrollments do not provide information on the reverberation effect of the target speaker in the Mixed Audio. To predict the binaural reverberation effect, the model needs to implicitly first identify the direction of the target speaker using the encoded target speaker characteristic, before extracting the reverberation effect coming from the estimated target speaker direction. This leads to the worse model performance on binaural target speaker extraction tasks. 
% Nonetheless, the extraction result shows that our model is capable of implicitly performing join target speaker extraction and reverberation estimation in a certain direction. 


% In this section, we demonstrate our model's effectiveness under multi-channel target speech extraction. We trained our model from scratch on the binaural data (simulated as explained in Sec. ???). We compared our model with the lookonce[] baseline, which extracts the target speaker from beamforming the input binaural audio at ??? degree. 


\subsection{Ablation Study} \label{sec:ablation}

In the ablation experiments, we modify our model architecture and training pipeline to investigate the proposed methods' effectiveness. Unless otherwise specified, all the reported results are tested on the experiment scenario with \textcolor{gray}{one \textit{Classical Target} and two \textit{Classical Disturbing} speakers}. 

\paragraph{Cross-Attention based Fusion over Film Fusion Method}

Film fusion is widely applied in the prior target audio extraction works \cite{film-fusion}. However, Film fusion passes the condition embedding through a linear layer, and element-wise multiplies the output with the input tensor to perform fusion. This limits the model to use fixed embedding size, which might not be capable of encoding the fine-grained details of the target speaker's characteristics. In this section, we show that our proposed attention-based fusion method is more optimum for our target speech extraction task.

We keep the training method and the rest of the model architecture intact and only change the fusion block of our model to a Film fusion block. In particular, we perform global average pooling on the encoder head output along the temporal dimension to obtain an embedding of fixed dimension as the condition input to the Film fusion block. As shown in Table \ref{tab:main-monaural}, our attention-based fusion method achieves higher performance in all application scenarios. This shows the cross-attention-based fusion method is more preferable for the proposed target speech extraction task.

\paragraph{Embedding Pooling Size}

\begin{table}[t]
\caption{Model performance with different pooling sizes. The inference time is the average time taken for the model main branch to extract 1-second audio on an Intel Xeon Silver 4314 @ 2.40GHz CPU. We report the average time taken for extraction in 50 repeated experiments. }
\label{tab:pooling}
\vskip 0.1in
\begin{center}
% \begin{small}
% \begin{sc}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
% \specialrule{2pt}{0pt}{0pt}
\textbf{Pooling} & \multirow{2}{*}{\textbf{Metric}} & \multirow{2}{*}{\textbf{2 Speakers}} & \multirow{2}{*}{\textbf{3 Speakers}} & \multirow{2}{*}{\textbf{4 Speakers}} & \multirow{2}{*}{\textbf{5 Speakers}} & \multirow{2}{*}{\textbf{6 Speakers}} & \textbf{Inf.}\\
\textbf{Size(\#)} & & & & & & & \textbf{Time(s)}\\
\midrule
% 10 Pooling & SNR       & 7.53$\pm$1.82 & 5.66$\pm$2.03 & 3.88$\pm$2.48 & 2.51$\pm$2.44 & 1.44$\pm$2.48 \\
\multirow{2}{*}{10}  & SNRi      & 10.99$\pm$2.44 & 11.02$\pm$2.58 & 10.67$\pm$2.86 & 10.04$\pm$2.64 & 9.90$\pm$2.52 & \multirow{2}{*}{0.386} \\
           % & SI-SNR    & 6.83$\pm$2.28 & 4.04$\pm$4.00 & 1.07$\pm$5.29 & -1.77$\pm$6.16 & -4.50$\pm$7.00 \\
           & SI-SNRi   & 10.28$\pm$2.67 & 9.40$\pm$4.01 & 7.83$\pm$5.00 & 5.77$\pm$5.63 & 3.96$\pm$6.20 \\
\midrule
% 20 Pooling & SNR       & 7.40$\pm$2.17 & 5.59$\pm$2.23 & 3.68$\pm$2.47 & 2.44$\pm$2.44 & 1.13$\pm$2.36 \\
\multirow{2}{*}{20}  & SNRi      & 10.81$\pm$2.43 & 10.90$\pm$2.68 & 10.34$\pm$2.76 & 10.24$\pm$2.73 & 9.81$\pm$2.74 & \multirow{2}{*}{0.365} \\
           % & SI-SNR    & 6.49$\pm$3.41 & 3.99$\pm$3.95 & 0.72$\pm$5.42 & -1.99$\pm$6.23 & -5.46$\pm$7.13 \\
           & SI-SNRi   & 9.90$\pm$3.42 & 9.31$\pm$3.99 & 7.38$\pm$5.19 & 5.80$\pm$5.88 & 3.22$\pm$6.42 \\
\midrule
% 40 Pooling & SNR       & 7.44$\pm$2.35 & 5.43$\pm$2.11 & 3.73$\pm$2.43 & 2.02$\pm$2.58 & 1.22$\pm$2.27 \\
\multirow{2}{*}{40} & SNRi      & 10.82$\pm$2.67 & 10.78$\pm$2.59 & 10.47$\pm$2.83 & 9.74$\pm$2.64 & 9.80$\pm$2.66 & \multirow{2}{*}{0.360} \\
           % & SI-SNR    & 6.47$\pm$3.74 & 3.76$\pm$3.99 & 0.35$\pm$6.03 & -3.11$\pm$6.82 & -5.17$\pm$7.01 \\
           & SI-SNRi   & 9.85$\pm$3.79 & 9.10$\pm$4.03 & 7.09$\pm$6.03 & 4.62$\pm$6.10 & 3.41$\pm$6.54 \\
\bottomrule
% \specialrule{2pt}{0pt}{0pt}
\end{tabular}}
% \end{sc}
% \end{small}
\end{center}
\vskip -0.2in
\end{table}

To reduce the model computation time in the Extraction Branch, we perform an average pooling of 20 on the embedding extracted by the encoder head. In this section, we compare our model performance and inference time with models using 10 and 40 average pooling sizes. As shown in Table \ref{tab:pooling}, we train two model variants with 10 and 40 average pooling sizes. Smaller pooling size results in less information loss, thus achieving better performance. However, a smaller pooling size also results in a longer sequence of audio embedding being extracted, which leads to a longer inference time. As a result, we selected 20 pooling sizes in the final model configuration. 


\paragraph{Effectiveness of Pretraining the Encoding Branch}

\begin{figure}[!t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{src/img/converge-speed.pdf}}
\caption{Validation loss values for the optimization step. The curve for the model with a pretraining stage begins at the 200k step to account for the 200k optimization steps performed during the pretraining stage. Due to the significantly large optimization step number required by the model trained from scratch, the plot only shows the learning curve of the first 600k optimization steps.}
\label{fig:converge-speed}
\end{center}
\vskip -0.3in
\end{figure}

Since our model needs to extract the target speaker's identity by comparing two noisy audio examples, training the model from scratch will result in slow and unstable convergence. In this section, we show the effectiveness of the pretraining step by showing the validation loss curve of our model with and without the first pretraining stage. As shown in Figure \ref{fig:converge-speed}, the model trained from scratch reaches 5 SNR-dB on the validation set after 520k optimization steps (around 108 hours), while the combined training time for the two-staged training takes 240k optimization steps (around 50 hours) based on single Nvidia A10 24GB GPU. 

\subsection{Model Performance in Challenging Application Scenarios} \label{sec:special-spk}
\begin{table}[!t]
\caption{Model performance under different monaural enrollment lengths. We vary the conditional Positive and Negative Enrollment length between 1 to 10 seconds.}
\label{tab:mono-enroll-length}
% \vskip 0.1in
\begin{center}
% \begin{small}
% \begin{sc}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{lccccc}
\toprule
% \specialrule{2pt}{0pt}{0pt}
\textbf{Condition} & \textbf{Metric} & \textbf{Pos. 1 sec} & \textbf{Pos. 3 sec} & \textbf{Pos. 5 sec} & \textbf{Pos. 10 sec} \\
\midrule
% Neg 1 sec & SNR     & 4.79$\pm$2.58 & 5.26$\pm$2.48 & 5.21$\pm$2.71 & 5.16$\pm$2.44 & 1.25$\pm$2.60 \\
\multirow{2}{*}{{Neg. 1 sec}} & SNRi    & 10.21$\pm$3.11 & 10.66$\pm$2.91 & 10.57$\pm$2.88 & 10.53$\pm$2.91 \\
          % & SI-SNR  & 2.45$\pm$5.40 & 3.16$\pm$5.63 & 3.01$\pm$5.78 & 3.23$\pm$4.62 & -2.94$\pm$5.01 \\
          & SI-SNRi & 7.88$\pm$5.56 & 8.56$\pm$5.65 & 8.37$\pm$5.60 & 8.60$\pm$4.61\\
\midrule
% Neg 3 sec & SNR     & 5.06$\pm$2.36 & 5.59$\pm$2.23 & 5.60$\pm$2.27 & 5.63$\pm$2.19 & 2.68$\pm$2.54 \\
\multirow{2}{*}{{Neg. 3 sec}} & SNRi    & 10.32$\pm$2.68 & 10.90$\pm$2.68 & 10.95$\pm$2.74 & 10.90$\pm$2.70\\
          % & SI-SNR  & 3.09$\pm$4.36 & 3.99$\pm$3.95 & 3.95$\pm$4.05 & 4.03$\pm$3.96 & -0.42$\pm$4.57 \\
          & SI-SNRi & 8.34$\pm$4.33 & 9.31$\pm$3.99 & 9.29$\pm$4.09 & 9.30$\pm$4.07 \\
\midrule
% Neg 5 sec & SNR     & 5.05$\pm$2.40 & 5.48$\pm$2.08 & 5.80$\pm$2.07 & 5.78$\pm$2.28 & 2.42$\pm$2.48 \\
\multirow{2}{*}{{Neg. 5 sec}} & SNRi    & 10.37$\pm$2.71 & 10.98$\pm$2.68 & 10.95$\pm$2.47 & 11.02$\pm$2.74 \\
          % & SI-SNR  & 3.01$\pm$4.76 & 3.82$\pm$3.85 & 4.26$\pm$4.05 & 4.34$\pm$3.99 & -0.87$\pm$4.69 \\
          & SI-SNRi & 8.32$\pm$4.78 & 9.32$\pm$4.01 & 9.40$\pm$4.07 & 9.58$\pm$3.96 \\
\midrule
% Neg 10 sec & SNR    & 4.94$\pm$2.45 & 5.57$\pm$2.07 & 5.60$\pm$2.08 & 5.79$\pm$2.08 & 2.49$\pm$2.42 \\
\multirow{2}{*}{{Neg. 10 sec}} & SNRi   & 10.32$\pm$2.87 & 11.07$\pm$2.59 & 11.08$\pm$2.69 & 11.07$\pm$2.50 \\
           % & SI-SNR & 2.94$\pm$4.42 & 3.97$\pm$3.69 & 4.09$\pm$3.39 & 4.34$\pm$3.56 & -0.58$\pm$4.32 \\
           & SI-SNRi & 8.30$\pm$4.38 & 9.47$\pm$3.86 & 9.58$\pm$3.53 & 9.62$\pm$3.56 \\
\bottomrule
% \specialrule{2pt}{0pt}{0pt}
\end{tabular}}
% \end{sc}
% \end{small}
\end{center}
\vskip -0.1in
\end{table}

\paragraph{Model Performance under Different Lengths of Positive and Negative Enrollment} 

\begin{table*}[!t]
% \vskip 0.15in
\begin{center}
% \begin{small}
\begin{minipage}{0.7\textwidth}
\caption{Model performance when extracting multiple monaural target speakers' voices.}
\vskip 0.1in
\label{tab:multi-tgt}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccc}
\toprule
% \specialrule{2pt}{0pt}{0pt}
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Metric}} &  \textbf{3 Speakers} & \multicolumn{2}{c}{\textbf{4 Speakers}} & \multicolumn{3}{c}{\textbf{5 Speakers}} \\
\cmidrule(lr){3-3} \cmidrule(lr){4-5} \cmidrule(lr){6-8}
 & & \textbf{2 Target Speakers} & \textbf{2 Target Speakers} & \textbf{3 Target Speakers} & \textbf{2 Target Speakers} & \textbf{3 Target Speakers} & \textbf{4 Target Speakers} \\
\midrule
            & SNR & $3.19 \pm 1.72$ & $2.83 \pm 1.59$ & $2.43 \pm 1.13$ & $2.39 \pm 1.54$ & $2.23 \pm 1.04$ & $2.17 \pm 0.90$ \\
Ours (Classical- & SNRi & $3.42 \pm 2.73$ & $4.89 \pm 2.43$ & $0.80 \pm 2.37$ & $5.69 \pm 2.30$ & $2.26 \pm 1.97$ & $-0.82 \pm 2.23$ \\
monaural)& SI-SNR & $0.42 \pm 3.13$ & $-0.21 \pm 3.11$ & $-1.10 \pm 2.53$ & $-1.32 \pm 3.75$ & $-1.48 \pm 2.37$ & $-1.76 \pm 2.31$ \\
            & SI-SNRi & $0.66 \pm 3.81$ & $1.85 \pm 3.49$ & $-2.72 \pm 3.35$ & $1.98 \pm 3.92$ & $-1.46 \pm 2.74$ & $-4.75 \pm 3.14$ \\
\midrule
Ours (Fine-tuned & SNR & \textbf{6.74 $\pm$ 1.93} & \textbf{4.78 $\pm$ 1.79} & \textbf{6.14 $\pm$ 1.50} & \textbf{3.42 $\pm$ 1.71} & \textbf{4.36 $\pm$ 1.59} & \textbf{5.74 $\pm$ 1.37} \\
for multiple & SNRi & \textbf{6.81 $\pm$ 2.06} & \textbf{6.68 $\pm$ 2.14} & \textbf{4.61 $\pm$ 1.85} & \textbf{6.80 $\pm$ 2.01} & \textbf{4.73 $\pm$ 1.95} & \textbf{2.85 $\pm$ 1.77} \\
target speaker & SI-SNR & \textbf{5.88 $\pm$ 2.47} & \textbf{3.32 $\pm$ 2.51} & \textbf{5.14 $\pm$ 1.97} & \textbf{1.16 $\pm$ 2.93} & \textbf{2.65 $\pm$ 2.40} & \textbf{4.62 $\pm$ 1.85} \\
extraction) & SI-SNRi & \textbf{5.94 $\pm$ 2.31} & \textbf{5.22 $\pm$ 2.41} & \textbf{3.61 $\pm$ 2.04} & \textbf{4.54 $\pm$ 2.52} & \textbf{3.02 $\pm$ 2.33} & \textbf{1.73 $\pm$ 1.93} \\
\bottomrule
% \specialrule{2pt}{0pt}{0pt}
\end{tabular}}
\end{minipage}
\hfill
\begin{minipage}{0.29\textwidth}
\caption{Model performance when undefined speakers are present.}
\vskip 0.1in
\label{tab:undefined}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcc}
\toprule
% \specialrule{2pt}{0pt}{0pt}
\textbf{Ground Truth} & \textbf{Metric} & \textbf{Performance}\\
\midrule
\multirow{2}{*}{Undefined + Classical Target} & SNRi & 4.48$\pm$2.70 \\
 & SI-SNRi & 0.94$\pm$4.31 \\
\midrule
\multirow{2}{*}{Undefined} & SNRi & 5.47$\pm$3.38 \\
 & SI-SNRi & -9.40$\pm$8.12 \\
\midrule
\multirow{2}{*}{Classical Target} & SNRi & \textbf{10.19$\pm$2.86} \\
 & SI-SNRi & \textbf{7.12$\pm$4.98}\\
\bottomrule
% \specialrule{2pt}{0pt}{0pt}
\end{tabular}}
\end{minipage}
% \end{small}
\end{center}
\vskip -0.1in
\end{table*}

\textcolor{gray}{Experiment Scenario: one \textit{Classical Target} and two \textit{Classical Disturbing} speakers}.

We show our model's performance under different lengths of Positive and Negative Enrollments. We train our model solely on samples with 3 seconds of Positive and Negative Enrollments and test the model performance on different enrollment lengths between 1 to 10 seconds. As shown in Table \ref{tab:mono-enroll-length}, the model performance improves slightly as the enrollments' lengths increase above 3 seconds, but drops noticeably when only 1 second of Positive or Negative Enrollment is used. This shows that providing enrollments of more than 3 seconds is important for our model to achieve higher performance. This suggests that users can enhance extraction quality by providing longer positive and negative enrollments if the results are unsatisfactory. This adaptability makes the model architecture suitable for developing a user-friendly extraction application.
% We show our binaural extraction model performance under different Positive and Negative Enrollment lengths in Appendix \ref{???}.



\paragraph{Multiple Target Speaker Extraction}

% \begin{table}[t]
% \caption{Performance comparison of monaural different target speaker number.}
% \label{tab:multi-tgt}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% % \begin{sc}
% \resizebox{1.0\columnwidth}{!}{
% \begin{tabular}{llcccccc}
% \toprule
% \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Metric}} &  \textbf{3 Speaker} & \multicolumn{2}{c}{\textbf{4 Speaker}} & \multicolumn{3}{c}{\textbf{5 Speaker}} \\
% \cmidrule(lr){3-3} \cmidrule(lr){4-5} \cmidrule(lr){6-8}
%  & & \textbf{2 Target Speaker} & \textbf{2 Target Speaker} & \textbf{3 Target Speaker} & \textbf{2 Target Speaker} & \textbf{3 Target Speaker} & \textbf{4 Target Speaker} \\
% \midrule
%             & SNR & $3.19 \pm 1.72$ & $2.83 \pm 1.59$ & $2.43 \pm 1.13$ & $2.39 \pm 1.54$ & $2.23 \pm 1.04$ & $2.17 \pm 0.90$ \\
% \multirow{4}{*}{Ours} & SNRi & $3.42 \pm 2.73$ & $4.89 \pm 2.43$ & $0.80 \pm 2.37$ & $5.69 \pm 2.30$ & $2.26 \pm 1.97$ & $-0.82 \pm 2.23$ \\
%             & SI-SNR & $0.42 \pm 3.13$ & $-0.21 \pm 3.11$ & $-1.10 \pm 2.53$ & $-1.32 \pm 3.75$ & $-1.48 \pm 2.37$ & $-1.76 \pm 2.31$ \\
%             & SI-SNRi & $0.66 \pm 3.81$ & $1.85 \pm 3.49$ & $-2.72 \pm 3.35$ & $1.98 \pm 3.92$ & $-1.46 \pm 2.74$ & $-4.75 \pm 3.14$ \\
% \midrule
%             & SNR & $6.74 \pm 1.93$ & $4.78 \pm 1.79$ & $6.14 \pm 1.50$ & $3.42 \pm 1.71$ & $4.36 \pm 1.59$ & $5.74 \pm 1.37$ \\
% Ours        & SNRi & $6.81 \pm 2.06$ & $6.68 \pm 2.14$ & $4.61 \pm 1.85$ & $6.80 \pm 2.01$ & $4.73 \pm 1.95$ & $2.85 \pm 1.77$ \\
% (Finetuned) & SI-SNR & $5.88 \pm 2.47$ & $3.32 \pm 2.51$ & $5.14 \pm 1.97$ & $1.16 \pm 2.93$ & $2.65 \pm 2.40$ & $4.62 \pm 1.85$ \\
%             & SI-SNRi & $5.94 \pm 2.31$ & $5.22 \pm 2.41$ & $3.61 \pm 2.04$ & $4.54 \pm 2.52$ & $3.02 \pm 2.33$ & $1.73 \pm 1.93$ \\
% \bottomrule
% \end{tabular}}
% % \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

\textcolor{gray}{Experiment Scenario: N \textit{Classical Target} and M \textit{Classical Disturbing} speakers}.

In this scenario, we show our model performance when more than one target speakers talk simultaneously. We vary the number of \textit{Classical Target} speakers, and test model performance on extracting the overlapped sound of these speakers in the mixed audio. As shown in Table \ref{tab:multi-tgt}, due to the distribution shift between training and testing data, our model performance drops significantly when more than one target speaker is present. We thus finetune our model for 180k optimization steps on samples with a total number of speakers varying between 4 and 6 and constrain the number of target speakers to 2. The fine-tuning of the model achieves significantly better performance. This shows that our model architecture is capable of performing multi-target speaker extraction. 

In addition, we notice that as the number of target speakers increases, the model performance drops significantly. We argue that this is caused by the change in input audio quality. As the number of target speakers increases, the input audio is dominated by the target speakers' voices, leading to the extracted audio from our model having less improvement over the input audio. This is supported by the increase in the extracted audio's SNR and SI-SNR metrics in Table~\ref{tab:multi-tgt}. 
% We show the multi-target speaker extraction performance for binaural audio in Appendix ???.  



\paragraph{Model Performance when Undefined Speaker Present}

% \begin{table}[t]
% \caption{Model performance when undefined speakers present.\textcolor{red}{make this 1x3 table, and merge with other 2 tables}}
% \label{tab:undefined}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% % \begin{sc}
% \resizebox{1.0\columnwidth}{!}{
% \begin{tabular}{lccc}
% \toprule
% Metric & SNR of Undefined + TGT & SNR of Undefined & SNR of TGT \\
% \midrule
% % SNR     & 2.76$\pm$1.66 & -1.57$\pm$2.13 & 4.83$\pm$2.55 \\
% SNRi    & 2.69$\pm$2.94 & 3.76$\pm$3.30 & \textbf{10.16$\pm$2.82} \\
% % SI-SNR  & -0.47$\pm$3.55 & -17.21$\pm$8.14 & 2.72$\pm$4.61 \\
% SI-SNRi & -0.54$\pm$4.68 & -11.87$\pm$8.20 & \textbf{8.05$\pm$4.47} \\
% \bottomrule
% \end{tabular}}
% % \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

\textcolor{gray}{
Experiment Scenario: one \textit{Classical Target}, two \textit{Classical Disturbing}, and one \textit{Undefined} speakers}.

The \textit{Undefined} speaker belongs to neither target nor disturbing speakers. Depending on the application scenario, the user may want to extract the undefined speaker's voice (if the speaker is someone who wants to join the discussion at a cocktail party), or the user may want to remove them to only listen to the target speaker's voice. In this scenario, we demonstrate the model's performance when such undefined speakers are present if not explicitly trained to remove or extract the \textit{Undefined} speakers' voice. 

We select the 4-speaker scenario, which consists of one target, two disturbing, and one undefined speaker. We compare the SNRi and SI-SNRi between the extracted audio with the following audios: \textit{Classical Target} + \textit{Unknown} speaker's voice, only \textit{Unknown} speaker's voice, and only \textit{Classical Target} speaker's voice. As shown in Table \ref{tab:undefined}, model output is most similar to the \textit{Classical Target} speaker's voice. This shows that the model trained with no unknown speakers presenting in the training samples tends to remove the unknown speakers in its extraction in inference. On the other hand, fine-tuning is required if the model is expected to also extract the \textit{Undefined} Speaker's voice.
