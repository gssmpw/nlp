\newpage
\appendix
\onecolumn
\section*{Appendix}
\section{Data Simulation Method}

In this section, we detail the method for simulating the training and testing audio. 

We generate our training, validation, and testing data from the \texttt{train-clean-360}, \texttt{dev-clean}, and \texttt{test-clean} components, respectively. The sampling rate is 16000.  
% The training dataset consists of 360 hours of clean speech with 439 female and 482 male speakers. 
We train our model on data samples containing one \textit{Classical Target} and two \textit{Classical Disturbing} speakers. 
We construct our monaural training samples for such a scenario by first selecting three distinct speakers from the LibriSpeech dataset \cite{librispeech}. Two audio samples of the target speaker are randomly sampled to form the $a_{\text{tgt}}$ and $a_{\text{clean}}$ audio defined in Section \ref{sec:problem-formulation}. For each of the disturbing speakers, we randomly sample three speech examples of them to be their voice in the Audio Mixture, the Positive Enrollment, and the Negative Enrollment. In order to prevent pauses and leading zeros in audio samples affecting the actual number of speakers in the synthesized audio samples, we use WebRTC Voice Activity Detector \cite{pywebrtcvad} to detect and remove zeros in the LibriSpeech dataset samples, before constructing the mixed, positive, and negative audio examples. 


To simulate the background environment noise, we use the WHAM noise dataset. Different noise samples from the WHAM noise dataset are recorded in different environments. As a result, additional disturbing sounds might exist in one WHAM noise example but not the other. For instance, some WHAM noise dataset samples contain music playing in the background while others don't. Using a WHAM noise sample containing music in the Positive Enrollment and a WHAM sample of ambient environment noise in the Negative Enrollment will confuse our model on whether the music is the target sound to be extracted. We resolve such ambiguity by sampling the $n^{\{M, P, N\}}$ from the same WHAM noise sample but at different temporal segments. 
Selecting random segments from the WHAM noise prevents the model from assuming the background noise is the same across the Positive, Negative, and Mixed Audio. All the Positive, Negative, and Mixed Audio are three seconds long audio in both the training and testing stages, but our model architecture can process arbitrary long audio longer than one second. We scaled the WHAM noise to be 0 SNR with respect to the ground truth target speaker's voice in each sample. 

To simulate the binaural reverberant training and testing samples, we convolve each speaker's voice with BRIR from the ASH-Listening-Set dataset \cite{ASH-brir}. The binaural RIRs used for constructing one data sample are randomly selected such that 1. All BRIR used in a single data sample are from the same scene in the ASH-Listening-Set dataset \cite{ASH-brir}, and 2. The BRIR for the target speaker in the Positive Enrollment has the direction of arrival of 0 degrees. 
The pre-processing strategy for each speaker's voice and the generation of background noise remains the same as the monaural dataset.


\section{Model Architecture and Training Detail}

\begin{wraptable}{r}{0.4\columnwidth} % 'r' for right, 'l' for left
\vskip -0.3in
\caption{The number of learnable parameters of each model component.}
\label{tab:parameter-size}
\vskip 0.1in
\centering
% \begin{small}
\resizebox{\linewidth}{!}{ % Ensures it scales properly
\begin{tabular}{lcc}
\toprule
% \specialrule{2pt}{0pt}{0pt}
 & \textbf{Encoding Branch} & \textbf{Extraction Branch} \\
\midrule
Parameter(\#)  & 1.39 M & 0.49 M \\
\bottomrule
% \specialrule{2pt}{0pt}{0pt}
\end{tabular}}
% \end{small}
% \vskip -0.1in
\end{wraptable}

The TF-GridNet Encoder in the Encoding Branch uses the following configuration: $4 \times 4$ kernel size and $1 \times 1$ stride in the first Conv2D, 64 hidden units in all three BiLSTM layers, 8 attention head numbers in the Full-band Self-attention Module. The input audio is processed by Short-Time Fourier Transform (STFT) with 128 window size and 64 hop length. The causal TF-GridNet blocks in the Extraction Branch use the same configuration as above, apart from using  $1 \times 1$ kernel size in its first Conv2D layer. The number of learnable parameters in the Encoding Branch and Extraction Branch is shown in Table \ref{tab:parameter-size}. 

All the training is done on a single Nvidia A10 24GB GPU with a batch size of 2. In both training stages, we use the Adam optimizer. We use an initial learning rate of 2e-3 and decay the learning rate by half if validation loss does not decrease for more than 50 epochs. 500 epochs (200k optimization steps) are used in the first pretraining stage, and 1000 epochs (400k optimization steps) are used in the second stage. 

Since TCE is not trained on the LibriSpeech dataset, we fine-tune the TCE model on our simulated data till convergence (for 120k optimization steps) using the Adam optimizer with 5e-4 learning rate. To train and test the TCE model on the proposed extraction task, we modify our dataloader's output to match TCE's application scenario by adding a known speaker's voice in the Negative Enrollment and concatenating the Mixed Audio with this modified Negative Enrollment. The model then performs extraction conditioned on the known speaker's d-vector embedding.


\section{SNR and SI-SNR of the Extracted Audio}

\begin{table}[t]
\caption{Comparison between our method and the baselines' extraction performance. In addition to the result reported in the main paper, we include the SNR and SI-SNR metrics of the extracted audio in this table.}
\label{tab:main-full-result}
\vskip 0.15in
\begin{center}
% \begin{small}
% \begin{sc}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\textbf{Method} & \textbf{Audio Type} & \textbf{Metric} & \textbf{2 Speakers} & \textbf{3 Speakers} & \textbf{4 Speakers} & \textbf{5 Speakers} & \textbf{6 Speakers} \\
\midrule
\multirow{4}{*}{Ours (Classical-monaural)} & Mono & SNR    & \textbf{7.40$\pm$2.17} & \textbf{5.59$\pm$2.23 } & \textbf{3.68$\pm$2.47 } & \textbf{2.44$\pm$2.44 } & \textbf{1.13$\pm$2.36} \\
     & Mono & SNRi & \textbf{10.81$\pm$2.43 } & \textbf{10.90$\pm$2.68 } & \textbf{10.34$\pm$2.76 } & \textbf{10.24$\pm$2.73 } & \textbf{9.81$\pm$2.74} \\
     & Mono & SI-SNR & \textbf{6.49$\pm$3.41 } & \textbf{3.99$\pm$3.95 } & \textbf{0.72$\pm$5.42 } & \textbf{-1.99$\pm$6.23 } & \textbf{-5.46$\pm$7.13} \\
     & Mono & SI-SNRi & \textbf{9.90$\pm$3.42 } & \textbf{9.31$\pm$3.99 } & \textbf{7.38$\pm$5.19 } & \textbf{5.80$\pm$5.88 } & \textbf{3.22$\pm$6.42} \\
\midrule
\multirow{4}{*}{Ours (Film fusion))} & Mono & SNR    & 6.59$\pm$2.11 & 4.53$\pm$2.38 & 3.27$\pm$2.34 & 1.78$\pm$2.39 & 0.82$\pm$2.36 \\
            & Mono & SNRi   & 10.01$\pm$2.55 & 9.98$\pm$2.68 & 9.93$\pm$2.69 & 9.55$\pm$2.61 & 9.38$\pm$2.50 \\
            & Mono & SI-SNR & 5.46$\pm$3.41 & 2.22$\pm$4.71 & 0.03$\pm$5.14 & -3.05$\pm$5.69 & -5.51$\pm$6.39 \\
            & Mono & SI-SNRi & 8.88$\pm$3.52 & 7.66$\pm$4.50 & 6.67$\pm$4.92 & 4.73$\pm$4.94 & 3.03$\pm$5.61 \\
\midrule
\multirow{4}{*}{TCE \cite{Chen2024tce}}        & Mono & SNR    & 4.89$\pm$2.44 & 2.68$\pm$2.54 & 1.22$\pm$2.36 & -0.11$\pm$2.30 & -0.83$\pm$2.08 \\
           & Mono & SNRi   & 8.46$\pm$2.57 & 8.24$\pm$2.58 & 8.19$\pm$2.55 & 7.73$\pm$2.10 & 7.75$\pm$2.05 \\
           & Mono & SI-SNR & 3.18$\pm$3.82 & -0.42$\pm$4.57 & -3.28$\pm$5.10 & -5.89$\pm$4.84 & -8.00$\pm$4.88 \\
           & Mono & SI-SNRi & 6.75$\pm$3.66 & 5.16$\pm$4.11 & 3.69$\pm$4.39 & 1.94$\pm$3.47 & 0.57$\pm$3.80 \\
\midrule
\multirow{4}{*}{SpeakerBeam \cite{speakerbeam}} & Mono & SNR    & -3.72$\pm$1.34 & -3.77$\pm$1.29 & -3.74$\pm$1.39 & -3.76$\pm$1.32 & -3.68$\pm$1.34 \\
            & Mono & SNRi   & -0.42$\pm$2.15 & 1.51$\pm$2.41 & 3.10$\pm$2.52 & 3.85$\pm$2.65 & 4.91$\pm$2.72 \\
            & Mono & SI-SNR & -6.32$\pm$6.22 & -8.18$\pm$5.33 & -9.64$\pm$4.44 & -10.37$\pm$4.28 & -11.65$\pm$4.15 \\
            & Mono & SI-SNRi & -3.01$\pm$5.98 & -2.90$\pm$5.08 & -2.81$\pm$3.96 & -2.75$\pm$3.68 & -3.03$\pm$3.38 \\
\midrule
\multirow{4}{*}{NMF \cite{NMF}} & Mono & SNR       & 0.78$\pm$1.60 & 0.37$\pm$1.33 & 0.12$\pm$1.15 & -0.14$\pm$1.05 & -0.20$\pm$0.94 \\
    & Mono & SNRi      & 4.24$\pm$1.60 & 5.65$\pm$1.85 & 6.87$\pm$2.23 & 7.55$\pm$2.22 & 8.29$\pm$2.40 \\
    & Mono & SI-SNR    & -5.12$\pm$4.84 & -6.77$\pm$4.63 & -8.48$\pm$5.23 & -10.49$\pm$6.06 & -11.76$\pm$7.03 \\
    & Mono & SI-SNRi   & -1.65$\pm$3.78 & -1.50$\pm$3.57 & -1.71$\pm$3.84 & -2.80$\pm$5.05 & -3.28$\pm$5.74 \\
\midrule \midrule
\multirow{4}{*}{Ours (Classical-binaural)} & Bi & SNR   & \textbf{5.30$\pm$1.98} & \textbf{3.34$\pm$1.87} & \textbf{2.14$\pm$1.51} & \textbf{1.40$\pm$1.28} & \textbf{0.65$\pm$0.78} \\
       & Bi & SNRi & \textbf{8.97$\pm$2.39} & \textbf{8.83$\pm$2.64} & \textbf{9.04$\pm$2.53} & \textbf{9.28$\pm$2.51} & \textbf{10.06$\pm$2.67} \\
       & Bi & SI-SNR & \textbf{3.52$\pm$3.27} & \textbf{-0.09$\pm$4.40} & \textbf{-2.73$\pm$4.14} & \textbf{-5.22$\pm$4.41} & \textbf{-7.97$\pm$3.62} \\
       & Bi & SI-SNRi & \textbf{7.18$\pm$3.25} & \textbf{5.42$\pm$4.26} & \textbf{4.16$\pm$3.82} & \textbf{2.66$\pm$3.85} & \textbf{1.46$\pm$3.22} \\
\midrule
\multirow{4}{*}{LookOnceToHear \cite{Veluri2024lookonce}} & Bi & SNR    & 3.58$\pm$2.33 & 2.30$\pm$2.66 & 1.51$\pm$2.05 & 0.91$\pm$1.92 & 0.12$\pm$1.73 \\
         & Bi & SNRi   & 7.10$\pm$3.16 & 7.72$\pm$3.16 & 8.34$\pm$2.75 & 8.62$\pm$2.72 & 9.28$\pm$2.61 \\
         & Bi & SI-SNR & 0.48$\pm$5.60 & -2.55$\pm$6.60 & -4.54$\pm$6.14 & -6.53$\pm$6.09 & -9.29$\pm$5.38 \\
         & Bi & SI-SNRi & 4.05$\pm$5.80 & 2.93$\pm$6.45 & 2.32$\pm$6.02 & 1.34$\pm$5.71 & 0.09$\pm$5.04 \\
\bottomrule
% \specialrule{2pt}{0pt}{0pt}
\end{tabular}}
% \end{sc}
% \end{small}
\end{center}
\vskip -0.1in
\end{table}

The main paper showed our model's SNRi and SI-SNRi performance. We show our model's full performance by additionally showing the SNR and SI-SNR metric in Table \ref{tab:main-full-result}.


\section{Model performance with respect to Input Audio Quality}

\begin{figure*}[t]
\centering
\begin{small}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\columnwidth]{src/img/mixed-sisnr-dis.pdf}
    \caption{SI-SNR of the extracted audio with respect to the Mixed Audio. The dashed black line represents the zero-improvement line.}
    \label{fig:mixed-sisnr-dis}
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\columnwidth]{src/img/pos-sisnr-dis.pdf}
    \caption{SI-SNRi of the extracted audio with respect to the Positive Enrollment SI-SNR.}
    \label{fig:pos-sisnr-dis}
\end{minipage}
\end{small}
\vskip -0.1in
\end{figure*}




In this section, we show our model performance relative to the quality of input Mixed Audio and Enrollments. 

\paragraph{Model Performance under different Mixed Audio quality} As shown in Figure \ref{fig:mixed-sisnr-dis}, we plot the extracted audio's SI-SDR values for 2000 randomly generated test samples as a function of their input SI-SDR. 97.0\% of the audio mixtures' SI-SNR were improved after the extraction.

\paragraph{Model Performance under different Enrollment Audio quality} Figure \ref{fig:pos-sisnr-dis} presents our model's extraction performance as a function of enrollment audio quality. To evaluate this, we scale the clean enrollment audio of the target speaker (Positive Enrollment) to specific SI-SNR values, while leaving the Negative Enrollment and the Mixed Audio unchanged. We then measure the average of 2000 test samples' extracted audios' SI-SNRi at each Positive Enrollment SI-SNR value.

The model achieves optimal performance when the Positive Enrollment SI-SNR is between -2.5 dB to 5 dB, and the performance deteriorates significantly when the SI-SNR of the Positive Enrollment falls below -12.5 dB. To address this limitation, we fine-tune our model by scaling the training data's Positive Enrollment to -20 and -15 SI-SNR dB. As shown by the red line and the blue line in Figure \ref{fig:pos-sisnr-dis}, this fine-tuning shifts the model's peak performance to lower SI-SNR regions, which means the model achieves optimum extraction performance under worse Positive Enrollment quality. Further research is necessary to develop a model with strong performance across the full SI-SNR range of the Positive Enrollment.

% \section{Binaural audio extraction performance under different enrollment length and target speaker number.}

% \begin{table}[t]
% \caption{Performance comparison of binaural different enrollment length.}
% \label{tab:binaural-enroll-len}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% % \begin{sc}
% \resizebox{1.0\columnwidth}{!}{
% \begin{tabular}{llcccc}
% \toprule
% Method & Metric & Pos 1 Second & Pos 3 Second & Pos 5 Second & Pos 10 Second \\
% \midrule
% % \multirow{4}{*}{Neg 1 Second} & SNR & $2.90 \pm 1.89$ & $3.23 \pm 1.75$ & $3.10 \pm 1.86$ & $3.24 \pm 2.05$ \\
% \multirow{2}{*}{Neg 1 Second} & SNRi & $8.41 \pm 2.67$ & $8.80 \pm 2.41$ & $8.74 \pm 2.56$ & $8.72 \pm 2.57$ \\
%                               % & SI-SNR & $-1.04 \pm 4.73$ & $-0.09 \pm 3.82$ & $-0.51 \pm 4.22$ & $-0.25 \pm 4.62$ \\
%                               & SI-SNRi & $4.48 \pm 4.68$ & $5.49 \pm 3.54$ & $5.14 \pm 4.12$ & $5.23 \pm 4.38$ \\
% \midrule
% % \multirow{4}{*}{Our Method, Neg 3 Second} & SNR & $2.89 \pm 1.82$ & $3.33 \pm 1.87$ & $3.33 \pm 1.86$ & $3.36 \pm 1.92$ \\
% \multirow{2}{*}{Neg 3 Second} & SNRi & $8.28 \pm 2.47$ & $8.83 \pm 2.63$ & $8.88 \pm 2.43$ & $9.08 \pm 2.43$ \\
%                               % & SI-SNR & $-0.95 \pm 4.34$ & $-0.08 \pm 4.39$ & $0.01 \pm 4.15$ & $0.14 \pm 4.01$ \\
%                               & SI-SNRi & $4.45 \pm 4.22$ & $5.42 \pm 4.26$ & $5.57 \pm 3.99$ & $5.86 \pm 3.69$ \\
% \midrule
% % \multirow{4}{*}{Neg 5 Second} & SNR & $3.10 \pm 1.91$ & $3.28 \pm 1.88$ & $3.46 \pm 1.96$ & $3.28 \pm 2.00$ \\
% \multirow{2}{*}{Neg 5 Second} & SNRi & $8.55 \pm 2.58$ & $8.94 \pm 2.60$ & $8.83 \pm 2.52$ & $8.79 \pm 2.45$ \\
%                               % & SI-SNR & $-0.53 \pm 4.24$ & $-0.06 \pm 3.94$ & $0.25 \pm 4.11$ & $-0.05 \pm 4.23$ \\
%                               & SI-SNRi & $4.91 \pm 4.10$ & $5.58 \pm 3.89$ & $5.62 \pm 3.83$ & $5.42 \pm 3.88$ \\
% \midrule
% % \multirow{4}{*}{Neg 10 Second} & SNR & $3.03 \pm 2.01$ & $3.34 \pm 1.93$ & $3.43 \pm 1.85$ & $3.31 \pm 1.94$ \\
% \multirow{2}{*}{Neg 10 Second} & SNRi & $8.49 \pm 2.50$ & $8.89 \pm 2.37$ & $8.93 \pm 2.50$ & $8.67 \pm 2.34$ \\
%                                % & SI-SNR & $-0.80 \pm 4.62$ & $-0.00 \pm 4.26$ & $0.29 \pm 3.75$ & $-0.04 \pm 4.26$ \\
%                                & SI-SNRi & $4.65 \pm 4.24$ & $5.53 \pm 3.96$ & $5.78 \pm 3.55$ & $5.32 \pm 3.78$ \\
% % \multirow{4}{*}{LookonceToHear (No Negative Enrollment)} & SNR & $1.88 \pm 2.31$ & $2.29 \pm 2.66$ & $2.40 \pm 2.32$ & $2.45 \pm 2.25$ \\
% % LookonceToHear & SNRi & $7.49 \pm 3.00$ & $7.71 \pm 3.15$ & $7.81 \pm 2.94$ & $7.92 \pm 2.96$ \\
% %                       % & SI-SNR & $-3.92 \pm 6.85$ & $-2.54 \pm 6.59$ & $-2.23 \pm 6.34$ & $-2.20 \pm 6.20$ \\
% % (No Negative Enrollment) & SI-SNRi & $1.72 \pm 6.80$ & $2.93 \pm 6.44$ & $3.20 \pm 6.16$ & $3.32 \pm 6.11$ \\
% \bottomrule
% \end{tabular}}
% % \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}


% \begin{table}[t]
% \caption{Performance comparison of binaural different target speaker number.}
% \label{tab:multi-tgt-binaural}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% % \begin{sc}
% \resizebox{1.0\columnwidth}{!}{
% \begin{tabular}{llcccccc}
% \toprule
% \multirow{2}{*}{Method} & \multirow{2}{*}{Metric} &  3 Speaker & \multicolumn{2}{c}{4 Speaker} & \multicolumn{3}{c}{5 Speaker} \\
% \cmidrule(lr){3-3} \cmidrule(lr){4-5} \cmidrule(lr){6-8}
%  & & 2 Target Speaker & 2 Target Speaker & 3 Target Speaker & 2 Target Speaker & 3 Target Speaker & 4 Target Speaker \\
% \midrule
% \multirow{4}{*}{Ours} & SNR & $3.04 \pm 1.44$ & $2.06 \pm 1.25$ & $2.27 \pm 1.20$ & $1.59 \pm 1.02$ & $1.72 \pm 0.90$ & $1.87 \pm 0.81$ \\
%                       & SNRi & $3.28 \pm 2.60$ & $4.09 \pm 2.50$ & $0.60 \pm 2.77$ & $5.19 \pm 2.27$ & $2.03 \pm 2.30$ & $-1.04 \pm 2.46$ \\
%                          & SI-SNR & $0.09 \pm 3.11$ & $-2.27 \pm 3.49$ & $-0.84 \pm 3.10$ & $-3.51 \pm 3.29$ & $-2.02 \pm 2.81$ & $-0.91 \pm 2.38$ \\
%                          & SI-SNRi & $0.33 \pm 3.88$ & $-0.23 \pm 4.01$ & $-2.51 \pm 4.13$ & $0.08 \pm 3.51$ & $-1.70 \pm 3.42$ & $-3.82 \pm 3.37$ \\
% \midrule
%      & SNR & $3.93 \pm 1.99$ & $2.46 \pm 1.90$ & $4.22 \pm 1.54$ & $1.62 \pm 1.63$ & $2.96 \pm 1.43$ & $4.19 \pm 1.32$ \\
% Ours & SNRi & $4.12 \pm 2.57$ & $4.69 \pm 1.92$ & $2.73 \pm 2.26$ & $4.95 \pm 1.90$ & $3.16 \pm 1.92$ & $1.25 \pm 2.29$ \\
% (FineTuned) & SI-SNR & $1.65 \pm 3.29$ & $-0.92 \pm 3.32$ & $2.35 \pm 2.70$ & $-2.73 \pm 3.35$ & $-0.06 \pm 2.85$ & $2.89 \pm 2.29$ \\
%      & SI-SNRi & $1.83 \pm 3.51$ & $1.32 \pm 2.83$ & $0.87 \pm 2.83$ & $0.58 \pm 3.03$ & $0.14 \pm 2.66$ & $-0.05 \pm 2.47$ \\
% \bottomrule
% \end{tabular}}
% % \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Table ??? shows our model's binaural reverberant audio extraction performance under different Positive and Negative Enrollment lengths. The model performance increases as the length of Positive Enrollment increase. However, the model performance does not change significantly as the Negative Enrollment length increase. This might because the model exploit the fact that the target speaker is in the 0 degree of arrival angle in the Positive Enrollment, thus, it does not require the Negative Enrollment to obtain the target speaker's characteristics. 

% Table ??? shows our model's performance on extracting multiple target speakers' binaural reverberant speech. The model performance is significantly worse than single target speaker's binaural speech extraction. This problem could also be caused by the fact that the 

% Even though we performed fine-tuning on 

\begin{table*}[t]
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{minipage}{0.49\textwidth}

\caption{Model performance when different numbers of \textit{Silent Target} (ST) speakers are present.}
\vspace{10pt}
\label{tab:st}
% \begin{sc}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Metric} & \textbf{0 ST} & \textbf{1 ST} & \textbf{2 ST} & \textbf{3 ST} \\
\midrule
 & SNR & \textbf{5.59$\pm$2.23} & \textbf{4.94$\pm$2.34} & \textbf{4.78$\pm$2.69} & \textbf{4.55$\pm$2.61} \\
Ours (Classical- & SNRi & \textbf{10.90$\pm$2.68} & \textbf{10.27$\pm$2.87} & \textbf{10.06$\pm$3.09} & \textbf{9.92$\pm$3.03} \\
monaural) & SI-SNR & \textbf{3.99$\pm$3.95} & \textbf{2.48$\pm$4.92} & 1.88$\pm$6.05 & 1.43$\pm$6.23 \\
 & SI-SNRi & \textbf{9.31$\pm$3.99} & \textbf{7.82$\pm$4.99} & 7.17$\pm$6.16 & 6.81$\pm$6.25 \\
\midrule
Ours (Fine-tuned & SNR & 4.38$\pm$2.36 & 4.40$\pm$2.25 & 4.24$\pm$2.12 & 4.04$\pm$2.43 \\
for multiple & SNRi & 9.59$\pm$2.57 & 9.64$\pm$2.44 & 9.56$\pm$2.50 & 9.66$\pm$2.46 \\
target speaker & SI-SNR & 2.59$\pm$3.36 & 2.41$\pm$3.52 & \textbf{2.19$\pm$3.41} & \textbf{1.75$\pm$4.09} \\
extraction) & SI-SNRi & 7.80$\pm$3.24 & 7.65$\pm$3.26 & \textbf{7.52$\pm$3.43} & \textbf{7.36$\pm$3.69} \\
\bottomrule
\end{tabular}}

\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}

\caption{Model performance when different numbers of \textit{Additional Disturbing} (AD) speakers are present.}
\label{tab:ad}
\vskip 0.15in
% \begin{sc}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Metric} & \textbf{0 AD} & \textbf{1 AD} & \textbf{2 AD} & \textbf{3 AD} \\
\midrule
& SNR & \textbf{5.59$\pm$2.23} & \textbf{3.89$\pm$2.29} & 2.47$\pm$2.54 & 1.62$\pm$2.42 \\
Ours (Classical-& SNRi & \textbf{10.90$\pm$2.68} & \textbf{10.53$\pm$2.52} & 10.22$\pm$2.62 & \textbf{10.17$\pm$2.39} \\
monaural)& SI-SNR & \textbf{3.99$\pm$3.95} & \textbf{0.85$\pm$5.10} & -1.96$\pm$6.33 & -3.51$\pm$6.02 \\
& SI-SNRi & \textbf{9.31$\pm$3.99} & \textbf{7.51$\pm$4.76} & 5.79$\pm$5.83 & 5.03$\pm$5.21 \\
\midrule
Ours (Fine- & SNR & 5.40$\pm$2.16 & 3.86$\pm$2.19 & \textbf{3.08$\pm$2.04} & \textbf{4.16$\pm$2.45} \\
tuned when & SNRi & 10.73$\pm$2.56 & 10.50$\pm$2.70 & \textbf{10.59$\pm$2.54} & 9.62$\pm$2.88 \\
special speakers & SI-SNR & 3.50$\pm$4.29 & 0.68$\pm$5.35 & \textbf{-1.06$\pm$5.58} & \textbf{1.07$\pm$5.89} \\
exist) & SI-SNRi & 8.83$\pm$4.28 & 7.32$\pm$5.30 & \textbf{6.45$\pm$5.30} & \textbf{6.52$\pm$5.81} \\
\bottomrule
\end{tabular}}

\end{minipage}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\section{Model Performance in the presence of \textit{Silent Target} speakers}

\textcolor{gray}{Experiment Scenario: one \textit{Classical Target}, two \textit{Classical Disturbing}, and N \textit{Silent Target} speakers}.

In a multi-target speaker extraction scenario, some of the target speakers might keep silent in the Mixed Audio. In this scenario, the model should extract only the voice of the remaining target speakers from the Mixed Audio.

We evaluate our multi-target speaker extraction model on samples where only one of the target speakers is speaking. In other words, the model is tested on single-speaker extraction, despite being enrolled to extract up to four target speakers. As shown in Table \ref{tab:st}, the model's performance is not significantly affected by the presence of the \textit{Silent Target} speakers. Even when three of the four enrolled target speakers are silent in the Mixed Audio, our model's extraction performance only decreases by 0.44 dB in SI-SNRi (shown in the 3 ST column in Table \ref{tab:st}). In comparison, the model trained to encode and extract a single speaker's voice shows around 2.52 dB lower SI-SNRi performance when the number of silent target speakers increases from zero to three. 


\section{Model performance under presence of \textit{Silent Disturbing Type 1}, \textit{Silent Disturbing Type 2}, and \textit{Additional Disturbing} Speakers} \label{app:special-spk}

In this section, we analyze the impact of other three types of special speakers (\textit{Silent Disturbing Type 1}, \textit{Silent Disturbing Type 2}, and \textit{Additional Disturbing} speakers) on the extraction performance. We focus on experiment scenarios containing one \textit{Classical Target} speaker, two \textit{Classical Disturbing} speakers, and N special speakers where $0 \leq N \leq 3$. We show our model performance when trained on samples containing only one \textit{Classical Target} and two \textit{Classical Disturbing} speakers. Additionally, we fine-tune our model on training data that additionally contain zero to three special speakers of the same type. 

\paragraph{Model performance under presence of \textit{Additional Disturbing} speakers}
\textcolor{gray}{Experiment Scenario: one \textit{Classical Target}, two \textit{Classical Disturbing}, and N \textit{Additional Disturbing} speakers.}

Disturbing speakers may not be present when the user records the target speaker's voice for the Positive Enrollment. Through capturing these disturbing speakers' voices in the Negative Enrollment, we expect the model to also remove these disturbing speakers from the Mixed Audio. This type of disturbing speaker is referred to as \textit{Additional Disturbing} speakers. We investigate the presence of this type of speaker in the model performance in this section. 

As shown in Table \ref{tab:ad}, when trained on samples containing only \textit{Classical Target} and \textit{Classical Disturbing} speakers, our model performance is significantly affected by the additional speakers. The SI-SNRi performance decreases by around 1 dB each time one more \textit{Additional Disturbing} speakers exist in the recorded audio. The fine-tuned model shows less performance degradation as the number of \textit{Additional Disturbing} speakers increases, and outperforms the model before fine-tuning when there are more than one \textit{Additional Disturbing} speakers present. 



\paragraph{Model Performance under the presence of Silent Disturbing speakers}

\begin{table*}[t]
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{minipage}{0.49\textwidth}

\caption{Model performance when different numbers of \textit{Silent Disturbing Type 1} (SDT1) speakers are present.}
\label{tab:sdt1}
\vskip 0.15in
% \begin{sc}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Metric} & \textbf{0 SDT1} & \textbf{1 SDT1} & \textbf{2 SDT1} & \textbf{3 SDT1} \\
\midrule
& SNR & \textbf{5.59$\pm$2.23} & \textbf{5.22$\pm$2.43} & \textbf{4.71$\pm$2.62} & \textbf{4.24$\pm$2.90} \\
Ours (Classical-& SNRi & \textbf{10.90$\pm$2.68} & \textbf{10.59$\pm$2.82} & \textbf{10.25$\pm$3.05} & \textbf{9.74$\pm$3.28} \\
Monaural) & SI-SNR & \textbf{3.99$\pm$3.95} & \textbf{2.95$\pm$5.18} & \textbf{1.86$\pm$6.14} & \textbf{0.60$\pm$7.34} \\
& SI-SNRi & \textbf{9.31$\pm$3.99} & \textbf{8.33$\pm$5.19} & \textbf{7.41$\pm$6.14} & \textbf{6.11$\pm$7.39} \\
\midrule
Ours (Fine-& SNR & 5.40$\pm$2.16 & 4.92$\pm$2.09 & 4.28$\pm$2.50 & 3.72$\pm$2.49 \\
tuned when & SNRi & 10.73$\pm$2.56 & 10.34$\pm$2.68 & 9.71$\pm$2.92 & 9.29$\pm$2.98 \\
special speakers & SI-SNR & 3.50$\pm$4.29 & 2.81$\pm$4.32 & 1.28$\pm$6.26 & 0.19$\pm$6.65 \\
exist) & SI-SNRi & 8.83$\pm$4.28 & 8.22$\pm$4.44 & 6.72$\pm$6.21 & 5.76$\pm$6.68 \\
\bottomrule
\end{tabular}}

\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}

\caption{Model performance when different numbers of \textit{Silent Disturbing Type 2} (SDT2) speakers are present.}
\label{tab:sdt2}
\vskip 0.15in
% \begin{sc}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Metric} & \textbf{0 SDT2} & \textbf{1 SDT2} & \textbf{2 SDT2} & \textbf{3 SDT2} \\
\midrule
& SNR & \textbf{5.59$\pm$2.23} & \textbf{5.22$\pm$2.51} & \textbf{4.91$\pm$2.69} & \textbf{4.36$\pm$3.01} \\
Ours (Classical-& SNRi & \textbf{10.90$\pm$2.68} & \textbf{10.64$\pm$2.94} & \textbf{10.25$\pm$3.09} & \textbf{9.85$\pm$3.18} \\
Monaural) & SI-SNR & \textbf{3.99$\pm$3.95} & \textbf{2.83$\pm$5.58} & \textbf{2.06$\pm$6.65} & 0.83$\pm$7.48 \\
& SI-SNRi & \textbf{9.31$\pm$3.99} & \textbf{8.25$\pm$5.64} & \textbf{7.39$\pm$6.74} & 6.32$\pm$7.28 \\
\midrule
Ours (Fine- & SNR & 5.40$\pm$2.16 & 5.07$\pm$2.47 & 4.61$\pm$2.49 & 4.28$\pm$2.69 \\
tuned when & SNRi & 10.73$\pm$2.56 & 10.29$\pm$2.88 & 10.00$\pm$3.05 & 9.82$\pm$3.13 \\
special speakers & SI-SNR & 3.50$\pm$4.29 & 2.76$\pm$5.38 & 1.80$\pm$5.92 & \textbf{0.88$\pm$6.89} \\
exist) & SI-SNRi & 8.83$\pm$4.28 & 7.99$\pm$5.39 & 7.19$\pm$6.16 & \textbf{6.44$\pm$6.87} \\
\bottomrule
\end{tabular}}

\end{minipage}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

% When sampling negative audio examples, novel disturbing speaker not presenting in the positive audio enrollment might present. In this section, we show our model performance on handling such scenario. 
\textcolor{gray}{Experiment Scenario: one \textit{Classical Target}, two \textit{Classical Disturbing}, and N \textit{Silent Disturbing Type 1 or Type 2} speakers.}


When a disturbing speaker remains silent in the Mixed Audio but speaks during the enrollment stage, its presence will affect the Encoding Branch's ability to extract the target speaker's embedding. Such speakers are classified as \textit{Silent Disturbing Type 1} speakers if they exist in both the Positive and Negative Enrollment or as \textit{Silent Disturbing Type 2} if they only exist in the Negative Enrollment. 

As shown in Table \ref{tab:sdt1} and Table \ref{tab:sdt2}, in comparison to the \textit{Additional Disturbing} speaker cases, our model shows less significant performance degradation as the number of \textit{Silent Disturbing Type 1} or \textit{Silent Disturbing Type 2} speakers increase from zero to three. However, the model still shows around 3.2 dB decrease in SI-SNRi performance when three speakers of \textit{Silent Disturbing Type 1 or Type 2} are present. Furthermore, while fine-tuning optimizes the model to achieve better performance under the presence of \textit{Additional Disturbing} speakers, it results in worse extraction performance when \textit{Silent Disturbing} speaker exists.

In conclusion, the presence of special speakers results in a noticeable decrease in model performance, and fine-tuning training data that contain these speakers results in improvements mainly on the scenario of \textit{Silent Target} speakers. In addition, since multiple speakers could share the same speaker type, and different types of speakers could exist at the same time, this results in a significantly large number of speaker type combinations, which is impractical to fully demonstrate in this research. We leave the exploration on better training methods and model architecture for target speaker extraction under the presence of multiple types of special speakers for future work. 


\section{Audio Visualization and Failure Cases}

\textcolor{gray}{Experiment Scenario: one \textit{Classical Target} and two \textit{Classical Disturbing} speakers.}

\begin{figure}[!t]
\centering
% \vskip 0.2in
\begin{center}

\centerline{\includegraphics[width=\linewidth]{src/img/success-case.pdf}}
\caption{Successful cases of our model. We selected four samples where the model SI-SNRi performance is above 11 dB. We report the extracted audio SI-SNRi under the case number.}
\label{fig:success-case}
\end{center}
\vskip -0.3in
\end{figure}

\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{src/img/fail-case.pdf}}
\caption{Failure cases of our model. We selected four samples where the model SI-SNRi performance is below 8 dB. We highlight the timesteps where the model has poor performance with red boxes.}
\label{fig:fail-case}
\end{center}
\vskip -0.3in
\end{figure}


In this section, we show four successful cases and four failing cases of our model. For each case, we visualize the waveform and STFT spectrogram of the Mixed Audio, the extraction ground truth, and the model prediction of a one-second audio segment. We focus on the experiment scenario containing one \textit{Classical Target} speaker and two \textit{Classical Disturbing} speakers.

As shown in Figure~\ref{fig:success-case}, our model correctly predicts silent sound in the extracted audio when the target speaker is not speaking and extracts the target speaker's voice despite the frequency range largely overlaps with the disturbing speakers. 

In several cases, our model has less optimum performance, with less than 8 dB SI-SNRi performance. We show four of these cases in Figure~\ref{fig:fail-case}. In these cases, the model fails to extract the target speaker's voice in some timesteps (Fail Case 1), fails to extract the speaker's whispering speech (Fail Case 2), includes disturbing the speaker's voice in its extraction (Fail Case 3), and fails to extract the non-verbal sound (sigh sound) of the target speaker (Fail Case 4). These non-exhaustive failure cases serve as a reference for future improvements.
