\section{Introduction}
In the target speaker extraction task, the model is required to extract the target speaker's voice from an audio mixture. To specify the characteristics of the target speaker, prior works have explored using conditional information of the target speaker in multiple modalities, including visual \cite{pan2022selectivelistening, muse}, textural \cite{ma2024clapsep, typingtolistern}, or acoustic modality \cite{speakerbeam, spex, spex+, zhang2024multilevelspeakerrepresentationtarget}.
% Due to the small computation complexity, numerous prior works in real-time target speaker extraction have leveraged the conditional information in the acoustic modality. 

Though prior works using conditional information in the acoustic modality have achieved significant performance, most of these works only considered using clean audio examples of the target speaker \cite{he2024hierarchicalspeaker, meng2024binauralselective, zhao2024continuoustargetspeech, pham2024wannahearvoiceadaptive}. This strong assumption prevents these models from performing well when only noisy audio examples are available. For example, consider a cocktail party, where the user meets a stranger who has not obtained the audio enrollments before. To extract the target stranger speaker's voice from the noisy environment to assist conversation, the user will have to ask the speaker to step outside to record the clean audio enrollment of the target speaker's voice. Enrolling target speakers in this way is often impractical in real-world applications.

\begin{figure}[!t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{src/img/task-outline_v2.pdf}}
\caption{Task outline of the three real application scenarios. We extract the voice of the target speaker, who only talked in the \textbf{Positive Enrollment} but not the \textbf{Negative Enrollment}. In this figure, the red icons represent the \textcolor{red!70}{\textbf{Disturbing Speakers (A, B \& C)}} and the green icon represents the \textcolor{LimeGreen}{\textbf{Target Speaker (T)}} to be extracted from the mixed audio.}
\label{fig:task-outline}
\end{center}
\vskip -0.4in
\end{figure}

In this work, we present a method to perform target speaker extraction conditioned on noisy audio enrollments where both the target speaker and the disturbing speakers exist. Performing target speaker extraction conditioned on such audio enrollments is an ill-posed problem since the model does not know which speaker in the audio enrollment is the target speaker required by the user. We resolve such ambiguity by additionally using negative audio enrollment that captures the disturbing speakers' characteristics, and trains the model to extract the target speaker's embedding by learning from the difference between positive and negative audio enrollments, as shown in Figure \ref{fig:task-outline}. Note that all three audio inputs to our model are audio mixtures containing multiple speakers. 

By addressing target speaker extraction in this manner, our model achieves a broader range of real-world applications. To perform target speaker extraction in real-time conversation, the user could press one button on the device to record the positive audio enrollments when the user visually observes the target speaker speaking, and press another button to record negative audio enrollments when the target speaker does not speak. Thanks to the causal extraction model architecture, our model could perform causal target speaker extraction for such application scenarios. 
To perform non-real-time target speaker extraction for arbitrary audio in an audio editing application, users could label some audio segments as positive or negative when they do or do not identify the desired speaker in the segment, and then use our model to extract the target speaker's voice in the whole audio mixture. Note that users do not have to label all the segments in the audio mixture that contains the target speaker. 
Through comparing the difference between the positive and negative audio enrollments, our model could obtain the target speaker's voice identity and perform the extraction in both of the above two application scenarios.

In conclusion, our contributions include: 

% 1. We define a new target speech separation task without any hard-collected positive speaker.
1. Address the problem of target speaker extraction when disturbing speakers exist in the enrollment stage, through comparing the positive enrollment (where the target speaker and disturbing speakers are present) and negative audio enrollment (where only disturbing speakers are present).

2. Design a fusion module and an associated pretraining method for the proposed task. Ablation experiments show that the proposed fusion method achieves 1.67 dB SI-SNRi improvement over the previously commonly used fusion method in three speaker extraction scenarios, and the proposed training method allows our model to achieve the same level of performance (5 SNR-dB on the validation set) with 280k less optimization steps. 

3. Demonstrate and discuss our model's effectiveness under various difficult and realistic application scenarios, including an increased number of disturbing speakers in the scene, speech extraction for multiple target speakers, and different lengths of positive and negative audio enrollments. 


