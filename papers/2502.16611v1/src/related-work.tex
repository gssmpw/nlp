\section{Related Work}

% Separating clean audio from different sound sources has been a long-standing task in computer science. Sound separation \cite{sudo-rmrf, tfgridnet, sepformer} focuses on isolating all sounds in an audio mixture, while target sound extraction \cite{ma2024clapsep, soloaudio} extracts sounds of a specific category. In contrast, our work addresses target speaker extraction, which isolates a specific speaker's voice from an audio mixture with interfering speakers. 

% Target speaker extraction is inherently more challenging than sound separation or target sound extraction. This is because the frequency ranges of different speakers are often less distinct compared to sound of different categories. Furthermore, the model must effectively encode the characteristics of the target speaker's voice to extract the desired speaker's voice from the mixture. In the following sections, we detail the similarity and difference between our method and the prior works in the target speaker extraction field. 



\subsection{Target Speaker Extraction with Clean Enrollment}

In target speaker extraction, prior works have explored using visual \cite{pan2022selectivelistening, muse}, textural \cite{ma2024clapsep, typingtolistern}, and audio examples \cite{speakerbeam, spex, spex+, zhang2024multilevelspeakerrepresentationtarget} of the target speaker as extraction condition. Our work belongs to the third category, which learns the acoustic characteristics from given audio examples.
% \textcolor{red}{o leverage the inherent consistency of the target speaker’s voice for AV-TSE. As illustrated by the blue lines in Figure 2, we expect that each frame of the extracted speech exhibits a high degree of correlation with the others due to the consistent voiceprint.}
Prior works in this category have attempted to improve extraction quality by modifying the fusion method \cite{he2024hierarchicalspeaker, zeng2024useftse}, leveraging multi-channel information in the audio mixture input \cite{meng2024binauralselective, pandey2024neurallowlatency}, and processing audio in the temporal domain \cite{spex, spex+}. 


% [Hierarchical Speaker Representation for Target Speaker Extraction] 
% - 使用多层fusion
% He et al. perform fusion between target speaker embedding and separation model at different granularity, instead of limiting the target speaker embedding as a fixed dimension vector \cite{he2024hierarchicalspeaker}. 
% [Binaural Selective Attention Model for Target Speaker Extraction]
% - binaural音频分离，condition为单通道音频
Meng et al. (\citeyear{meng2024binauralselective}) experimented on binaural target extraction. Experiments show that models leveraging directional information achieve better performance than monaural separation. Similarly, 
% [All Neural Low-latency Directional Speech Extraction] 
Pandey et al. (\citeyear{pandey2024neurallowlatency}) extracted audio from a specified direction-of-arrival (DOA), using multichannel audio recorded by an 8-microphone circular array. In our experiment, we show that our model is capable of implicitly leveraging the directional information of the target speaker to extract the target speech and the associated binaural reverberation effect. 

\begin{table}[!t]
% \caption{Conditional inputs used by the prior works and ours. Each column represents whether the model uses clean target speaker/sound enrollment, the target enrollment with disturbing speakers/sounds present, and/or the enrollment of disturbing speakers/sounds as an extraction condition. The last column shows if the conditional information is monaural (Mono) or binaural (Bi).}
\caption{Conditional inputs used by the prior works and ours. The last column shows if the conditional information is monaural (Mono) or binaural (Bi).}
\vskip -0.1in
\label{tab:related-work-condition-info}
% \vskip 0.15in
\begin{center}
% \begin{small}
% \begin{sc}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
% \specialrule{2pt}{0pt}{0pt}
\textbf{Method} & \textbf{Clean Target} & \textbf{Noisy Target} & \textbf{Disturbing} & \textbf{Mono/Bi} \\
\midrule
\cite{he2024hierarchicalspeaker, meng2024binauralselective} & \multirow{2}{*} {\ding{52}} & \multirow{2}{*}{\ding{56}} & \multirow{2}{*}{\ding{56}} & \multirow{2}{*}{Mono} \\
\cite{zhao2024continuoustargetspeech, pham2024wannahearvoiceadaptive} \\
\midrule
ClapSep \cite{ma2024clapsep} &  {\ding{52}} & {\ding{56}} & {\ding{52}} & Mono \\
\midrule
LookOnceToHear & \multirow{2}{*}{{\ding{56}}} & \multirow{2}{*}{{\ding{52}}} & \multirow{2}{*}{{\ding{56}}} & \multirow{2}{*}{Bi} \\
\cite{Veluri2024lookonce} \\
\midrule
TCE \cite{Chen2024tce} & {\ding{56}} & {\ding{56}} & {\ding{52}} & Mono \\
\midrule
\multirow{2}{*}{Ours} & \multirow{2}{*}{{\ding{56}}} & \multirow{2}{*}{{\ding{52}}} & \multirow{2}{*}{{\ding{52}}} & Mono \\  
 & & & & or Bi \\
\bottomrule
% \specialrule{2pt}{0pt}{0pt}
\end{tabular}}
% \end{sc}
% \end{small}
\end{center}
\vskip -0.2in
\end{table}

\subsection{Target Speaker Extraction in Challenging Scenarios}

To improve target speaker extraction models' robustness and applicability in real-world applications, prior works have addressed the model performance in extracting multiple target speakers' speech simultaneously \cite{rikhye2021multiuservoicefilter, ma2024clapsep, zeng2023simultaneousspeech}, transferring to extract speech for different languages \cite{pham2024wannahearvoiceadaptive}, extracting speech from a variable number of disturbing speakers in the audio mixture \cite{zhao2024continuoustargetspeech}, and disentangling irrelevant audio characteristics (e.g. reverberation effect) from the enrollment \cite{liu2024tsecurriculum, Ranjan2018CurriculumLearning, Heo2024CentroidEstimation, pandey2023AttentiveTraining, borsdorf2024wTIMIT2mix, mu2024selfsuperviseddisentangle, luo2024disentangleinstrument}.

Rikhye et al. (\citeyear{rikhye2021multiuservoicefilter}) performed target speaker extraction for multiple speakers simultaneously. The model attentively focuses on one of the embeddings from multiple target speakers. Similarly, ClapSep \cite{ma2024clapsep} simultaneously extracts sound for multiple sources. Clean audio of multiple targets and disturbing sound sources are used as the model's extraction condition. Zhao et al. (\citeyear{zhao2024continuoustargetspeech}) addressed the problem of variable disturbing speaker numbers in the audio mixture to be separated. In our experiment, we also show our model's generalizability to extract with different numbers of target and disturbing speakers. 

Ranjan et al. (\citeyear{Ranjan2018CurriculumLearning}) adopted curriculum learning to gradually increase the extraction difficulty. After a fixed number of epochs, samples with a lower signal-noise ratio (SNR) and higher similarity between target and disturbing speakers are added to the training dataset. Our method adopts a similar training paradigm to pretraining the audio encoder. Both ours and the prior work from Ranjan et al. (\citeyear{Ranjan2018CurriculumLearning}) show that the pretraining stage helps improve the training stability and results in faster converge speed.



% % [multi-user voicefilter-lite via attentive speaker embedding] 
% % - 同时分离多个声源
% Rikhye et al. \cite{rikhye2021multiuservoicefilter} performs target speaker extraction for multiple speakers simutaneously. The model attentively focus on one of the embeddings from multiple target speakers. Similarly, clapsep[] simutaneously extracts sound for multiple sources. Clean audio of multiple target and disturbing sound sources are used as extraction condition. 
% % [simutaneous speech extraction for multiple target speaker under the meeting scenarios] 
% % - 使用speaker diarization得到single talker sceneario得到每一speaker特征
% Zeng et al. \cite{zeng2023simultaneousspeech} used speaker diarization model to identify the single talker audio segments in the mixed audio. 

% % [wanna hear your voice: adaptive, effective, and language-agnostic approach in voice extraction] 
% % - 进行不同语言的target speaker extraction
% Pham et al. \cite{pham2024wannahearvoiceadaptive} proposed a universal target speaker extraction model for English and Vietnamese languages. 

% % [continuous target speech extraction: enhancing personalized diarization and extraction on complex recordings] 
% % - 考虑mixture中不同时刻的speaker个数不同
% Zhao et al. \cite{zhao2024continuoustargetspeech} addressed the problem of variable disturbing speaker number in the audio mixture to be separated. In our experiment, we also show our model's generalizability to extract with different number of disturbing speakers. Though trained with fixed number of disturbing speakers, our model shows stronger generalizability to scenarios with different number of disturbing speakers present in the scene. 



% % [Target Speaker Extraction with Curriculum Learning] 
% Liu et al. \cite{liu2024tsecurriculum} and 
% % [curriculum learning based approaches for noise robust speaker recognition] 
% % - pretraining
% Ranjan et al. \cite{Ranjan2018CurriculumLearning} adopts curriculum learning to gradually increase the extraction difficulty. After a fixed number of epochs, samples with lower Signal to Noise Ratio and higher similarity between target and disturbing speakers are added into the training dataset. Our method adopts a similar training paridium to pretraining the audio encoder. Both ours and the prior work from [] shows that the pretraining stage helps improving the training stability and better final performance.
% % [Attentive Training: A New Training Framework for Speech Enhancement]
% % - 使用音频样本进行target sound extraction
% Pandey et al. \cite{pandey2023AttentiveTraining} considerred the first speaker in the input audio mixture as the target speaker, and trains the attentive recurrent network[] to extract
% % [Centroid Estimation with Transformer-Based Speaker Embedder for Robust Target Speaker Extraction]
% % - 考虑reverberation, noise, speech style
% Heo et al. \cite{Heo2024CentroidEstimation} trains a transformer-based enrollment encoder with data augmentation and contrastive loss. Experiment shows the proposed pretraining stages improves separation model's performance under change in microphone characteristic, environment noise, and reverberation between the enrollment and the ground truth audio.
% % Disentangling Multi-instrument Music Audio for Source-level Pitch and Timbre Manipulation] 
% % - 修改一混合音频中的乐器音调，非抽取任务
% manipulates the target instrument's pitch and timbre with in a given audio mixture. 
% Borsdorf et al. \cite{borsdorf2024wTIMIT2mix} [wTIMIT2mix: A Cocktail Party Mixtures Database to Study Target Speaker Extraction for Normal and Whispered Speech]
% Mu et al. \cite{mu2024selfsuperviseddisentangle} [self supervised disentangled representation learning for robust target speech extraction]



% recognition:
% In addition to prior works on target speech extraction, prior works in  target speech recognition have also considerred the . 

% [RIR-SF: Room Impulse Response Based Spatial Feature for Target Speech Recognition in Multi-Channel Multi-Speaker Scenarios]
% - 使用混合 reverberant音频 + 某一声源rir做输入，进行音频分离

% [Self-Supervised Learning for ASR Pre-Training with Uniquely Determined Target Labels and Controlling Cepstrum Truncation for Speech Augmentation]

% convoifilter: a case study of doing cocktail party speech recognition
% - 微调convoifilter和asr模型，使得recognition效果更好




\subsection{Target Audio Extraction Conditioned on Enrollments with Disturbing Speakers}

Though prior works have achieved significant progress in target audio extraction, most of these works assume the availability of clean audio enrollment. However, in real-world application scenarios, such enrollment examples are not necessarily available. 

Among those prior works that attempted to extract target speech conditioned on noisy audio enrollments, TCE \cite{Chen2024tce} explored the turn-taking dynamics in the conversation. TCE model accepts an audio encoding of the user's clean voice and performs extraction by considering the speakers who cross-talk with the user as the disturbing speakers. However, TCE is limited to performing target speaker extraction when the user is participating in the conversation and does not allow specifying audio segments where the target speaker is present. 
% [OR-TSE: An Overlap-Robust Speaker Encoder for Target Speech Extraction]
OR-TSE \cite{zhang2024ortse} addressed the overlap between the target and the disturbing speakers at the start and end of the enrollment. A speech diarization model is used to discard the audio segments with multiple speakers, and the enrollment encoder still relies on the non-overlapping regions in the enrollment to extract the target speaker's characteristics. 
% lookonce
LookOnceToHear \cite{Veluri2024lookonce} extracts the target speaker when multiple other disturbing speakers are present. To obtain the target speaker's identity, the model requires the user to look at the target speaker when sampling enrollment audio. Through beamforming at a 90-degree azimuthal angle, the model disambiguate the target speaker from disturbing speakers talking simultaneously during the enrollment stage. 

In comparison to the previous target speaker extraction methods, our model does not assume knowledge of any speaker in the mixed audio, or the target speaker's spatial location in the enrollment stage. This allows our model to extract the target speaker from an arbitrary mono-channel sound mixture, and only use easily obtained binary labels of positive and negative audio segments in the temporal domain to extract the target speech. To the best of our knowledge, we are the first work to address monaural target speaker extraction without relying on any clean audio enrollment input. We summarize the difference in conditional information used by our method and the prior works in Table \ref{tab:related-work-condition-info}.



