% !TEX TS-program = xlatexmk
\pdfoutput=1

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl}
\usepackage{times}
\usepackage{inconsolata}
\usepackage{latexsym}
\usepackage{danudefs}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{color}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{url}
\usepackage{braket}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{hhline}
\usepackage{xcolor} 
%--------------------------------------------

\definecolor{lightred}{RGB}{228, 8, 10} 
\definecolor{lightblue}{RGB}{8, 114, 254}

%--------------------------------------------
\usepackage{pgf}          % Needed for \pgfmathresult

\usepackage{array}


\usepackage{highlight} % styles  for table colors


\input{myacronyms}



% ------------------------------------------------------------------
% DB: This will rename subsection to Section when using \autoref
\usepackage[english]{babel}
\usepackage{hyperref}
\addto\captionsenglish{%
  \def\sectionname{\S}%
  \def\subsectionname{\S}%
}
\addto\extrasenglish{%
  \def\subsectionautorefname{\S}%
  \def\sectionautorefname{\S}%
}


\let\subsectionautorefname\sectionautorefname
\let\subsubsectionautorefname\sectionautorefname
% --------------------------------------------------------------------


% ------------ Notes -----------------------------------------------------------------------------
%\setlength {\marginparwidth }{2cm}
\usepackage{todonotes}
%\usepackage[disable]{todonotes} % uncomment this line to hide all comments

\makeatletter
\if@todonotes@disabled
\newcommand{\hlfix}[2]{#1}
\else
\newcommand{\hlfix}[2]{\texthl{#1}\todo{#2}}
\fi
\makeatother

%\usepackage{xifthen}
 \newcommand\DB[2][]{\todo[size=\tiny, caption={2do}, #1]{
\begin{minipage}{\textwidth-4pt}#2\end{minipage}}}

 \newcommand\DBI[2][]{\todo[inline,size=\tiny, caption={2do}, #1]{
\begin{minipage}{\textwidth-4pt}#2\end{minipage}}}
% ----------------------------------------------------------------------------------------------

%\setlength\titlebox{5cm}

\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}


\title{Evaluating the Effect of Retrieval Augmentation on Social Biases}

\author{Tianhui Zhang$^{1}$ \quad
        Yi Zhou$^{2}$\quad
        Danushka Bollegala$^{1}$ \\
        $^1$University of Liverpool\quad
        $^2$Cardiff University \\
        {\tt Tianhui.Zhang@liverpool.ac.uk} \\
        {\tt ZhouY131@cardiff.ac.uk} \quad
        {\tt danushka@liverpool.ac.uk}
}

\date{}

\begin{document}
\maketitle



\begin{abstract}
    \ac{RAG} has gained popularity as a method for conveniently incorporating novel facts that were not seen during the pre-training stage in \ac{LLM}-based \ac{NLG} systems.
    However, \acp{LLM} are known to encode significant levels of unfair social biases.
    The modulation of these biases by \ac{RAG} in \ac{NLG} systems is not well understood.
    In this paper, we systematically study the relationship between the different components of a \ac{RAG} system and the social biases presented in the text generated across three languages (i.e. English, Japanese and Chinese) and four social bias types (i.e. gender, race, age and religion).
    Specifically, using the \ac{BBQ} benchmark datasets, we evaluate the social biases in \ac{RAG} responses from document collections with varying levels of stereotypical biases, employing multiple \acp{LLM} used as generators.
    We find that the biases in document collections are often \emph{amplified} in the generated responses, even when the generating \ac{LLM} exhibits a low-level of bias.
    Our findings raise concerns about the use of \ac{RAG} as a technique for injecting novel facts into \ac{NLG} systems and call for careful evaluation of potential social biases in \ac{RAG} applications before their real-world deployment. 
\end{abstract}

\section{Introduction}
\label{sec:intro}

\acp{LLM} are trained on vast collections of texts typically sourced from the internet. 
These models encapsulate a broad spectrum of information, yet they fail to incorporate emerging facts after pre-training, leading to inaccuracies and hallucinatory outputs~\citep{Song:2024a,Niu:2024,Agrawal:2024}.
Traditional approaches to update \acp{LLM} with new information include continual pre-training~\cite{Ke:2022} and supervised fine-tuning~\cite{Ouyang:2022}.
However, updating parameters of \acp{LLM} with large datasets is time consuming and expensive even with parameter efficient methods~\cite{Hu:2021}.
Moreover, closed models such as \texttt{GPT-4o} restrict model parameter access.

\ac{RAG}~\cite{Lewis:2020a,Edge:2024} offers a popular alternative by integrating real-time retrieval of documents to supplement the training data~\cite{Izacard:2021,Jiang:2024,Shuster:2021}.
This approach allows \acp{LLM} to access and utilise information unavailable during their initial training.
   

\begin{figure}[t]
    \centering
    \includegraphics[width=9cm]{figures/intro.png}
    \caption{A neutral generator LLM would return an unbiased response (\emph{unknown}) for the question. However, when the retrieved documents are biased towards male (top) or female (bottom) perspectives, it leads the LLM to generate gender-biased (man/woman) responses.}
    \label{fig:intro}
\end{figure}

The document sets used in \ac{RAG} are crucial as they directly influence the generated content.
The inherent social biases of these documents, coupled with those encoded by the \acp{LLM}, determine the bias level of the outputs.
Despite extensive evaluation of \ac{RAG} systems for retrieval efficacy~\cite{Wu:2024b,Laban:2024,Yang:2024e} and factual accuracy~\cite{Krishna:2024,soman2024observations}, their role in propagating social biases has been under explored.

This paper addresses this oversight by investigating how \ac{RAG} influences social biases when \acp{LLM} are presented with externally sourced contexts, potentially laden with stereotypes.
We analyse the bias propagation using \ac{BBQ}~\cite{BBQ}, a QA-structured benchmark that assesses social biases in \acp{LLM}, across  \emph{gender}, \emph{age}, \emph{race} and \emph{religion} applying three retrieval methods.
Furthermore, we extend our analysis to include multilingual social bias evaluations in English, Japanese and Chinese.
Our findings highlight several key points:
\begin{enumerate}
    \item We find that all types of social biases are amplified when stereotypically biased documents are used for \ac{RAG}.
    This is particularly worrying because despite the careful bias mitigations conducted prior to releasing \acp{LLM}, \ac{RAG} can easily generate socially biased responses from those \acp{LLM}.
    Interestingly, we see that the level of social bias increment in larger \acp{LLM} tend to be smaller compared to that in smaller \acp{LLM}. 
    
    \item Overall, we find that social biases are affected to a lesser degree by the retrieval method used in \ac{RAG}, while sparse retrieval methods tend to be more sensitive to social biases than dense retrieval methods.
    Surprisingly, social biases do \emph{not} necessarily increase with the number of documents retrieved, demonstrating a trade-off due to the decreasing relevance of the documents to the input query.    
   
    \item This bias amplification is not confined to English and can be observed for non-English languages as well such as Japanese and Chinese, demonstrating a global challenge across languages.     
\end{enumerate}
We advocate for a reconsideration of how social biases are evaluated in \ac{RAG} systems.
% We will publicly release\footnote{The code and data are submitted to ARR.} our \ac{RAG} social bias evaluation toolkit upon paper acceptance to facilitate comprehensive assessment across various \acp{LLM} and document collections.

\section{Related Work}
\label{sec:related}

% papers that propose methods to reduce hallucinations in LLMs
% Two parts: social bias evaluation, RAG. But the intersection is null.
% Cite the only other RAG-bias paper (in related work) and tell it is contemporary and how it differs from ours, or highlight similar findings 

\paragraph{Social Biases in LLMs:}
\acp{LLM} are typically trained on extensive text collections sourced from the internet, which often contains various types of social biases that are then mirrored in the models' behaviour~\cite{Penedo:2024}. 
These biases can be assessed through two primary methods: intrinsic and extrinsic~\cite{goldfarb-tarrant-etal-2021-intrinsic,cao-etal-2022-intrinsic} evaluation.
Intrinsic measures focus on biases within word embeddings or model predictions~\cite{caliskan-et-al-weat,nangia-etal-2020-crows,nadeem-etal-2021-stereoset,kaneko-etal-2022-debiasing}, while extrinsic measures analyse biases in outputs from downstream tasks such as \ac{NLI} or question answering~\cite{webster-2020-sts,De-Arteaga-biasbios}.
%\ac{RAG} can be seen as a downstream application of \acp{LLM}, and we use \ac{BBQ}, a QA-based extrinsic evaluation benchmark in this paper.

\paragraph{RAG and Social Biases:}
Although social biases of \acp{LLM} have been studied extensively for various downstream applications, the effect of \ac{RAG} on \ac{NLG} has been less frequently explored.
\citet{Hu:2021} proposed a three-level threat model and studied the sensitivity of \ac{RAG} to the external datasets used for the retrieval.
They found  that the fairness of a \ac{RAG} system can get easily compromised due to the social biases in the datasets used.
Unlike their approach which uses a limited set of contexts from the \ac{BBQ} dataset, our work incorporates a broader range of documents from diverse datasets, enhancing the generalisability of our findings.

\citet{wu-etal-2025-rag} explored fairness within \ac{RAG} systems by examining disparities in retrieval performance between protected and non-protected groups, using data from FairRanking Track~\citep{Ekstrand:2023} that focuses on protected attributes like binary gender (i.e. female vs. males) and geographic origin (i.e. non-Europeans vs. Europeans).
%Specifically, \citet{wu-etal-2025-rag} created subsets for evaluating fairness in binary gender (i.e. females as the protected group and males the non-protected group) and location (i.e. non-Europeans as the protected group and Europeans as the non-protected group).
%They also considered socio-economic status (i.e. poor as the protected group and rich as the non-protected group) using the \ac{BBQ} dataset.
This study primarily addressed fairness, defined as equitable retrieval performance, whereas our study extends the evaluation to social biases in multilingual contexts, providing a deeper understanding of bias dynamics in \ac{RAG} beyond just fairness.

%Note that the focus of \citet{wu-etal-2025-rag} is on fairness, which is defined as the difference of information retrieval performance between the protected vs. non-protected groups.
%In contrast, we evaluate social biases in \ac{RAG}, while the accuracy of the retrieval system is a secondary criteria guaranteeing the reliability of the evaluation.
%Moreover, \citet{wu-etal-2025-rag} and \citet{Hu:2021} both evaluated social biases only for English, whereas we conduct a multilingual evaluation.

\begin{figure*}[t!]
    \centering
    \includegraphics[height=50mm]{figures/outline.png}
    \caption{Overview of our RAG social bias evaluation protocol. 
    Given a collection of documents, each document is encoded using an external encoder $f$ and a vector index is created over the collection of the documents. We use a question, paired with its ambiguous or disambiguated context, selected from the BBQ dataset as the \emph{query} for the retrieval system. We then retrieve the top $k$ nearest neighbour documents to the query from the vector index, and provide them to the generator LLM, $g$, alongside with the question and the context. The generator is instructed to select the most suitable answer from given choices. }
    \label{fig:RAG}
\end{figure*}



\section{Social Bias Evaluation for RAG}
\label{sec:method}

% Explain the evaluation protocol. We use BBQ as the bias evaluation benchmark. 
% Why is BBQ suitable for this? What are the alternatives?
% 1. Describe the different bias types considered. How do we create input corpora for each bias type
% 2. How are the inputs selected (BBQ instances)
% 3. Encoder is fixed (what do we use as the RAG system. Llama Index? Give details about the encoder. Explain why it is kept fixed here. Extending this will be future work.
% 4. What are the prompts used for BBQ? Give an example here or put them in the Appendix
% 5. Evaluation metric. We will use Diff bias score for the ambiguous contexts in the main paper. Accuracy and the results for disambiguated contexts will be shown in the Appendix.

\subsection{Background -- RAG}
\label{sec:RAG-background}



Before we describe our social bias evaluation protocol for \ac{RAG}, let us briefly describe the main components of a typical \ac{RAG} system and how social biases could potentially influence each component.
A typical \ac{RAG} system is shown in \autoref{fig:RAG} and consists of several components.
\begin{description}[style=unboxed,leftmargin=0cm]
    \item[Document Collection $\cD$:] A \ac{RAG} system is given an external \emph{document collection},  $\cD$, containing information (possibly was not available) in the pre-trained \acp{LLM}.

    \item[Retriever:] Documents are indexed to efficiently retrieve those most relevant to a given query. 
    Both sparse and dense retrieval methods can be used for this purpose. 
    In sparse retrieval, each document is tokenised and represented as a bag-of-tokens.
    Next, an inverted index is created over the set of documents.
    Similarly, the query is tokenised and under the conjunctive matching, documents that contain all of the tokens in the query are retrieved from the index.
    Dense retrieval employs a pre-trained text \emph{encoder}~\cite{Xu:2023a,Gao:2021c}, $f$ to represent the query as well as each of the documents in $\cD$ in a fixed $m$-dimensional embedding $f(d) (\in \R^m)$ space.
    Next, a vector index is created over the embedded documents and approximate nearest neighbour (ANN) retrieval methods~\cite{Malkov:2020,Guo2020-dc} are used to efficiently retrieve similar documents to the query.

    \item[Generator $g$:] For a given query $q$, an \ac{LLM} is used as the generator $g$ to produce a response, $\hat{r}(q) = g(q, d^{(q)}_1, d^{(q)}_2, \ldots, d^{(q)}_k, I)$ using the set of top-$k$ retrieved documents for $q$, $\{d^{(q)}_1, d^{(q)}_2, \ldots, d^{(q)}_k\}$, and an instruction, $I$, that dictates the response formulation.
\end{description}


\subsection{Evaluation Protocol}
\label{sec:protocol}

% To evaluate the effect of \ac{RAG} on the social biases, we consider a simplified \ac{RAG} system illustrated in \autoref{fig:RAG}.
% Specifically, we consider the availability of 
% (a) a collection of documents, $\cD$,
% (b) an encoder $f$, which creates a $m$-dimensional embedding $f(d) (\in \R^m)$, for each document $d$ in $\cD$, and
% (c) a generator $g$, which generates a response, $\hat{r}(q) = g(q, d^{(q)}_1, d^{(q)}_2, \ldots, d^{(q)}_k, I)$, given the query $q$, the set of top-$k$ retrieved documents, $\{d^{(q)}_1, d^{(q)}_2, \ldots, d^{(q)}_k\}$ and an instruction, $I$, that describes how the response must be generated.

To describe our social bias evaluation protocol for \ac{RAG}, let us consider the example shown in \autoref{fig:RAG}, selected from the \ac{BBQ} dataset, where given the \emph{ambiguous} context ``\emph{We couldn't start the board meeting at 9 am today because a man and a woman were late}'', we must answer the question, ``\emph{Who was the secretary?}''.
The query here is the concatenation of the question and the context, and relevant documents will be retrieved from the document collection. 
Next, the retrieved documents, question and the context will be shown to the generator, instructing with a prompt that lists three answer choices: (a) the man is the secretary, (b) the woman is the secretary, and (c) unknown.
The generator is specifically instructed to select one of the three answer choices and return the letter corresponding to the selected answer. 

An \ac{LLM} that is stereotypically biased towards the male gender would incorrectly predict (b), assuming that all secretaries are females.
The choice that would \emph{not} reflect an unfair gender bias in this example would be (c).
However, note that an \ac{LLM} which always selects (c) and refrain from answering any socially sensitive questions would appear to be unbiased, when in fact it might not be the case.
Therefore, the \ac{BBQ} dataset includes \emph{disambiguated} contexts, which explicitly provide information to decide the correct answer without having to rely on any social biases encoded in the generator \ac{LLM} or the retrieved documents.
In the previous case, the disambiguation context provided in \ac{BBQ} is ``\emph{We had to wait for the woman because they were running the meeting, and the man was responsible for taking all the notes}''.
Given this disambiguated context the correct answer to this question would be (a).

Evaluating social biases under a \ac{RAG} setting is particularly challenging for two main reasons.
\begin{enumerate}
    \item \textbf{Component Interaction:} Each component (document collection, retriever and generator) in a \ac{RAG} system can independently and collectively influence social bias propagation.
    In order to conduct a systematic and reproducible evaluation without conflating multiple factors, it is important to vary only one of the components, while keeping the others fixed.

    \item \textbf{Open-ended Generation:} Automatically evaluating social biases under an open-ended generation setting is difficult because the same social bias can be expressed in different ways in the generator responses~\cite{ROBBIE}.
    Modelling social bias evaluation in \ac{RAG} as a multiple choice question-answering task enables us to evaluate social biases without having to consider open-ended generations.
\end{enumerate}

Next, we discuss how social biases can influence each of the \ac{RAG} components.
\paragraph{Biases in the Documents:}
If there are many documents that express various levels of stereotypical social biases, then a subset of those documents can be retrieved even when the query does not explicitly mention any social biases. 
Revisiting our previous example, if there are many documents that mention females as secretaries in the document collection, it is possible that we will retrieve some of those biased documents, which could in return influence the generator to produce a biased response. 
We evaluate the effect of four types of social biases (i.e. gender, age, race, religion) (\autoref{sec:collection}) in the document collection using three benchmark datasets covering English, Japanese~\cite{JBBQ} and Chinese~\cite{CBBQ} languages (\autoref{sec:multi-lingual}).
Moreover, as control settings we consider document collections that consists purely of stereotypical or anti-stereotypical documents in \autoref{sec:collection}.

\paragraph{Biases in the Retriever:}
The text encoders used for embedding documents and queries for dense retrieval can also encode unfair social biases~\cite{Bolukbasi:2016,Kaneko:2022}.
For example, gender-biased word embeddings are known to embed the gender-neutral occupational words such as \emph{secretary, nurse, housekeeper,} etc. such that they have high similarities with female pronouns than male pronouns~\cite{Kaneko:2021}.
Therefore, a biased text encoder can retrieve documents that express stereotypically-biased opinions as supporting evidence for a query that does not explicitly mention any social biases.
To evaluate this effect, we use three different retrieval methods in \autoref{sec:retrivers}.

\paragraph{Biases in the Generator:}
An \ac{LLM} acts as the generator in \ac{RAG}, which generates a response considering both the query as well as the set of retrieved documents following a user-specified instruction.
Even when the query and the retrieved documents are not biased, the social biases encoded in the \ac{LLM} can still result in a biased response.
To study this effect, we evaluate multiple generator \acp{LLM}, trained on different pre-train language data and parameter sizes in \autoref{sec:generators}.

\subsection{Evaluation Metric}
\label{sec:diff-bias}

Following the QA-based social bias evaluation approach proposed by \citet{BBQ}, we evaluate the social biases in a \ac{RAG} system based on its ability to correctly answer questions without reflecting any unfair stereotypical biases.
A test instance in a \ac{BBQ} dataset contains a question (presented in a negated or a non-negated format), an ambiguous context (evaluates \ac{RAG} behaviour in cases where there is insufficient evidence from the context to provide an answer)
and a disambiguated context (provides information about which of the individuals mentioned in the ambiguous context is the correct answer).
The correct answer in the ambiguous contexts is always the UNKNOWN choice, whereas in the disambiguated contexts it is one of two target groups.

\textbf{Accuracy} for the ambiguous contexts, $\textrm{Acc}_\textrm{a}$, is defined as the fraction of the ambiguous contexts predicted as UNKNOWN, while the accuracy for the disambiguated contexts, $\textrm{Acc}_\textrm{d}$, is defined as the fraction of the correct prediction of the disambiguous contexts for the specific target group.
Accuracy does not indicate the directionality of the bias (i.e. stereotypically biased towards the advantaged group vs. anti-stereotypically biased towards the disadvantaged group). 
Advantaged groups refer to demographic groups that  historically had greater access to resources, opportunities, power, or social privilege, whereas disadvantaged groups are those who have historically had discrimination, stereotypes, or unequal resource distributions. 

To address this, \citet{KBBQ} proposed the \textbf{Diff-Bias} score as the difference of accuracies for the biased and counter-biased cases (see~\autoref{sec:app:metrics} for the definition).
A zero Diff-Bias score indicates that the model under evaluation is not socially biased, while a positive or negative Diff-Bias score indicates social biases towards advantaged or disadvantaged groups, respectively.
We use both Diff-Bias and Accuracy in our evaluations.
However, due to the limited availability of space, all accuracy-based results are shown in \autoref{sec:app:accuracy}.

We provide the same instruction to all \acp{LLM} for BBQ evaluations.
Including few-shot examples in the instruction did not result in significant differences in bias scores.
Therefore, we used a zero-shot prompt for evaluations.
Further details of the instructions are provided in~\autoref{sec:app:ICL}.



\section{Experiments}
\label{sec:exp}

\subsection{Models and Datasets}
\label{sec:settings}

We construct a comprehensive document collection to study the manifestation of various social biases in a \ac{RAG} setting. 
As summarised in \autoref{tbl:datasets}, we combine nine datasets that contain sentences for different types of social biases, where we consider each sentence as a separate \emph{document} for retrieval purposes.
The final collection contains 66,695 documents and is refereed to as the \textbf{full-set} henceforth.
Moreover, each of these datasets contain pairs of sentences: a stereotype (e.g. \emph{women don't know how to drive}) and an anti-stereotype (e.g. \emph{men don't know how to drive}).
This enables us to further evaluate social biases in \ac{RAG} when we use only stereotypical (\textbf{stereo-set}) vs. anti-stereotypical (\textbf{anti-set}) sentences as the document collection.
%We machine translate the document collection into Japanese and Chinese to conduct the multilingual evaluations.

We evaluate a range of \acp{LLM} as generator models, spanning different parameter sizes, instruction-tuning variants and pre-training language data as follows: Llama-3-8B-Instruct (Llama3), Mistral-7B/Instruct (Mistral), GPT-3.5-turbo (GPT-3.5), Llm-jp-3.1-Instruct 1.8B/7B/13B (Llm-jp), Qwen2.5-3B/7B/14B (Qwen) base and instruction-tuned versions.
We use OpenAI API for GPT-3.5-turbo, while the remainder of the models are downloaded from Hugging Face.\footnote{\url{https://huggingface.co}}

For document retrieval, we consider three methods: 
(a) \href{https://docs.llamaindex.ai/en/stable/api_reference/retrievers/vector/}{\texttt{VectorIndex}} from LlamaIndex with 1536-dimensional OpenAI  \href{https://api.openai.com/v1/embeddings}{\texttt{text-embedding-ada-002}} embeddings, 
(b) \href{https://docs.llamaindex.ai/en/stable/api_reference/retrievers/bm25/}{\texttt{BM25}}, a sparse retriever available in LlamaIndex, 
and (c) \href{https://github.com/facebookresearch/contriever}{\texttt{contriever}}, a contrastively pre-trained dense retrieval system~\cite{izacard2021contriever} that uses the \texttt{facebook/contriever} retrieval model.



\begin{table}[!t]
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|cccc}
    \toprule
    Dataset       & \textbf{Gender} & \textbf{Age} & \textbf{Race} & \textbf{Religion} \\
    \midrule
    \href{https://github.com/nyu-mll/BBQ}{BBQ Sources}~\citep{BBQ}   & 219      & 682   & 830    & 886        \\
    \href{https://github.com/moinnadeem/StereoSet}{StereoSet}~\citep{stereoset}     & 1,744      &-     & 5,894    & 482        \\
    \href{https://github.com/umanlp/RedditBias}{Redditbias}~\citep{redditbias}    & 4,065      & 2,553    & 2,553    & 26,948        \\
    \href{https://gitlab.inria.fr/french-crows-pairs/acl-2022-paper-data-and-code}{CrowSPairs}~\citep{frenchcrowspairs}    & 261      & 182   & 1,016    & 222        \\
    \href{https://github.com/hyintell/CHBias}{CHbias}~\citep{chbias}        & -       & 2,406   & -     & -         \\
    \href{https://github.com/uclanlp/corefBias/tree/master/WinoBias/wino}{WinoBias}~\citep{winobias}      & 3,168      &-     & -     & -        \\
    \href{https://github.com/anthropics/evals}{WinoGenerated}~\citep{winogenerated} & 3,420     & -    & -     &  -        \\
    \href{https://github.com/kinit-sk/gest}{GEST}~\citep{gest}         & 7130      & -    & -     & -         \\
    \href{https://github.com/microsoft/fifty-shades-of-bias/tree/main/data/FSB}{FSB}~\citep{fsb}           & 2,034      & -    & -     & -         \\
    \midrule
    \textbf{Total} & \textbf{22,041} & \textbf{5,823} & \textbf{10,293} & \textbf{28,538} \\
    \bottomrule
  \end{tabular}}
\caption{Number of documents selected from each of the datasets, covering multiple social bias types.}
  \label{tbl:datasets}
\end{table}

\begin{table*}[!t]
\small
\centering
%\resizebox{\textwidth}{!}{
\begin{tabular}{ll|ccccc}
\toprule
\textbf{Bias Type} & \textbf{Setting} & \textbf{GPT-3.5} & \textbf{Llama3-8B-Inst.} & \textbf{Qwen-7B-Inst.} & \textbf{Qwen-14B} & \textbf{Qwen-14B-Inst.} \\
\midrule
\multirow{4}{*}{Gender} 
    & w/o RAG   & 
        % GPT-3.5-turbo (Gender): ambiguous: 5.16,14.53,11.31,4.51  -> max=14.53, min=4.51; disamb.: -9.33,7.14,-0.1,-3.97 -> max=7.14, min=-9.33
               5.16 / \textbf{\textcolor{lightblue}{-9.33}}   & 
        % Llama3-8b-Instruct: ambiguous: 5.65,14.68,6.80,0.74 -> max=14.68, min=0.74; disamb.: 1.59,-0.4,-3.97,-6.85 -> max=1.59, min=-6.85
               5.65 / \textbf{\textcolor{lightred}{1.59}}   & 
        % Qwen2.5-7B-Instruct: ambiguous: 10.02,24.01,15.43,10.17 -> max=24.01, min=10.02; disamb.: -3.67,0.5,-2.08,-10.12 -> max=0.5, min=-10.12
               \textbf{\textcolor{lightblue}{10.02}} / -3.67   & 
        % Qwen2.5-14B: ambiguous: 3.77,13.99,0.55,-4.51 -> max=13.99, min=-4.51; disamb.: -7.34,-2.68,-4.66,-8.93 -> max=-2.68, min=-8.93
               3.77 / -7.34   & 
        % Qwen2.5-14B-Instruct: ambiguous: -2.38,4.61,-3.08,-8.43 -> max=4.61, min=-8.43; disamb.: -2.38,2.68,-5.95,-12.7 -> max=2.68, min=-12.7
               -2.38 / -2.38   \\
    & stereo-set  & 
               \textbf{\textcolor{lightred}{14.53}} / \textbf{\textcolor{lightred}{7.14}}   & 
               \textbf{\textcolor{lightred}{14.68}} / -0.4   & 
               \textbf{\textcolor{lightred}{24.01}} / \textbf{\textcolor{lightred}{0.5}}   & 
               \textbf{\textcolor{lightred}{13.99}} / \textbf{\textcolor{lightred}{-2.68}}   & 
               \textbf{\textcolor{lightred}{4.61}} / \textbf{\textcolor{lightred}{2.68}}   \\
    & full-set   & 
               11.31 / -0.1   & 
               6.80 / -3.97   & 
               15.43 / -2.08   & 
               0.55 / -4.66   & 
               -3.08 / -5.95   \\
    & anti-set  & 
               \textbf{\textcolor{lightblue}{4.51}} / -3.97   & 
               \textbf{\textcolor{lightblue}{0.74}} / \textbf{\textcolor{lightblue}{-6.85}}   & 
               10.17 / \textbf{\textcolor{lightblue}{-10.12}}   & 
               \textbf{\textcolor{lightblue}{-4.51}} / \textbf{\textcolor{lightblue}{-8.93}}   & 
               \textbf{\textcolor{lightblue}{-8.43}} / \textbf{\textcolor{lightblue}{-12.7}}   \\
\midrule
\multirow{4}{*}{Age} 
    & w/o RAG   & 
        % GPT-3.5-turbo (Age): ambiguous: 41.79,32.61,29.67,17.83 -> max=41.79, min=17.83; disamb.: 5.92,8.97,6.63,6.30 -> max=8.97, min=5.92
               \textbf{\textcolor{lightred}{41.79}} / \textbf{\textcolor{lightblue}{5.92}}   & 
        % Llama3-8b-Instruct (Age): ambiguous: 31.25,27.66,19.67,8.97 -> max=31.25, min=8.97; disamb.: 8.32,10.71,4.13,2.77 -> max=10.71, min=2.77
               \textbf{\textcolor{lightred}{31.25}} / 8.32   & 
        % Qwen2.5-7B-Instruct (Age): ambiguous: 30.52,35.87,30.52,20.11 -> max=35.87, min=20.11; disamb.: 3.42,3.15,3.75,3.53 -> max=3.75, min=3.15
               30.52 / 3.42   & 
        % Qwen2.5-14B (Age): ambiguous: 38.02,38.56,27.53,6.96 -> max=38.56, min=6.96; disamb.: 7.34,7.01,6.96,6.79 -> max=7.34, min=6.79
               38.02 / \textbf{\textcolor{lightred}{7.34}}   & 
        % Qwen2.5-14B-Instruct (Age): ambiguous: 18.02,18.72,7.50,2.69 -> max=18.72, min=2.69; disamb.: 8.59,9.35,6.09,3.26 -> max=9.35, min=3.26
               18.02 / 8.59   \\
    & stereo-set  & 
               32.61 / \textbf{\textcolor{lightred}{8.97}}   & 
               27.66 / \textbf{\textcolor{lightred}{10.71}}   & 
               \textbf{\textcolor{lightred}{35.87}} / \textbf{\textcolor{lightblue}{3.15}}   & 
               \textbf{\textcolor{lightred}{38.56}} / 7.01   & 
               \textbf{\textcolor{lightred}{18.72}} / \textbf{\textcolor{lightred}{9.35}}   \\
    & full-set   & 
               29.67 / 6.63   & 
               19.67 / \textbf{\textcolor{lightblue}{4.13}}   & 
               30.52 / \textbf{\textcolor{lightred}{3.75}}   & 
               27.53 / 6.96   & 
               7.50 / 6.09   \\
    & anti-set  & 
               \textbf{\textcolor{lightblue}{17.83}} / 6.30   & 
               \textbf{\textcolor{lightblue}{8.97}} / \textbf{\textcolor{lightblue}{2.77}}   & 
               \textbf{\textcolor{lightblue}{20.11}} / 3.53   & 
               \textbf{\textcolor{lightblue}{6.96}} / \textbf{\textcolor{lightblue}{6.79}}   & 
               \textbf{\textcolor{lightblue}{2.69}} / \textbf{\textcolor{lightblue}{3.26}}   \\
\midrule
\multirow{4}{*}{Race} 
    & w/o RAG   & 
        % GPT-3.5-turbo (Race): ambiguous: 10.00,24.95,16.60,6.49 -> max=24.95, min=6.49; disamb.: 3.40,13.30,8.83,5.43 -> max=13.30, min=3.40
               10.00 / \textbf{\textcolor{lightblue}{3.40}}   & 
        % Llama3-8b-Instruct (Race): ambiguous: 6.60,17.55,12.18,7.23 -> max=17.55, min=6.60; disamb.: 1.06,9.26,6.91,4.15 -> max=9.26, min=1.06
               \textbf{\textcolor{lightblue}{6.60}} / \textbf{\textcolor{lightblue}{1.06}}   & 
        % Qwen2.5-7B-Instruct (Race): ambiguous: 1.60,12.55,7.98,4.73 -> max=12.55, min=1.60; disamb.: 2.02,6.17,4.89,6.11 -> max=6.17, min=2.02
               \textbf{\textcolor{lightblue}{1.60}} / \textbf{\textcolor{lightblue}{2.02}}   & 
        % Qwen2.5-14B (Race): ambiguous: 6.81,19.95,13.46,4.36 -> max=19.95, min=4.36; disamb.: 2.13,8.09,3.83,2.23 -> max=8.09, min=2.13
               6.81 / \textbf{\textcolor{lightblue}{2.13}}   & 
        % Qwen2.5-14B-Instruct (Race): ambiguous: 0.00,3.88,0.00,-0.43 -> max=3.88, min=-0.43; disamb.: -3.30,3.83,0.64,-1.17 -> max=3.83, min=-3.30
               0.00 / \textbf{\textcolor{lightblue}{-3.30}}   \\
    & stereo-set  & 
               \textbf{\textcolor{lightred}{24.95}} / \textbf{\textcolor{lightred}{13.30}}   & 
               \textbf{\textcolor{lightred}{17.55}} / \textbf{\textcolor{lightred}{9.26}}   & 
               \textbf{\textcolor{lightred}{12.55}} / \textbf{\textcolor{lightred}{6.17}}   & 
               \textbf{\textcolor{lightred}{19.95}} / \textbf{\textcolor{lightred}{8.09}}   & 
               \textbf{\textcolor{lightred}{3.88}} / \textbf{\textcolor{lightred}{3.83}}   \\
    & full-set   & 
               16.60 / 8.83   & 
               12.18 / 6.91   & 
               7.98 / 4.89   & 
               13.46 / 3.83   & 
               0.00 / 0.64   \\
    & anti-set  & 
               \textbf{\textcolor{lightblue}{6.49}} / 5.43   & 
               7.23 / 4.15   & 
               4.73 / 6.11   & 
               \textbf{\textcolor{lightblue}{4.36}} / 2.23   & 
               \textbf{\textcolor{lightblue}{-0.43}} / -1.17   \\
\midrule
\multirow{4}{*}{Religion} 
    & w/o RAG   & 
        % GPT-3.5-turbo (Religion): ambiguous: 8.92,20.42,8.00,2.83 -> max=20.42, min=2.83; disamb.: 4.33,12.50,9.00,5.17 -> max=12.50, min=4.33
               8.92 / \textbf{\textcolor{lightblue}{4.33}}   & 
        % Llama3-8b-Instruct (Religion): ambiguous: 18.76,17.67,10.17,11.92 -> max=18.76, min=10.17; disamb.: 7.17,8.50,9.17,8.83 -> max=9.17, min=7.17
               \textbf{\textcolor{lightred}{18.76}} / \textbf{\textcolor{lightblue}{7.17}}   & 
        % Qwen2.5-7B-Instruct (Religion): ambiguous: 5.92,16.67,12.83,6.50 -> max=16.67, min=5.92; disamb.: 3.50,5.83,5.50,3.67 -> max=5.83, min=3.50
               \textbf{\textcolor{lightblue}{5.92}} / \textbf{\textcolor{lightblue}{3.50}}   & 
        % Qwen2.5-14B (Religion): ambiguous: 12.58,22.67,12.58,8.00 -> max=22.67, min=8.00; disamb.: 5.00,7.17,8.83,5.00 -> max=8.83, min=5.00
               12.58 / 5.00   & 
        % Qwen2.5-14B-Instruct (Religion): ambiguous: 8.17,10.42,8.42,7.75 -> max=10.42, min=7.75; disamb.: 2.83,9.33,4.17,2.00 -> max=9.33, min=2.00
               8.17 / 2.83   \\
    & stereo-set  & 
               \textbf{\textcolor{lightred}{14.83}} / \textbf{\textcolor{lightred}{12.50}}   & 
               17.67 / 8.50   & 
               \textbf{\textcolor{lightred}{16.67}} / \textbf{\textcolor{lightred}{5.83}}   & 
               \textbf{\textcolor{lightred}{22.67}} / 7.17   & 
               \textbf{\textcolor{lightred}{10.42}} / \textbf{\textcolor{lightred}{9.33}}   \\
    & full-set   & 
               8.00 / 9.00   & 
               \textbf{\textcolor{lightblue}{10.17}} / \textbf{\textcolor{lightred}{9.17}}   & 
               12.83 / 5.50   & 
               12.58 / \textbf{\textcolor{lightred}{8.83}}   & 
               8.42 / 4.17   \\
    & anti-set  & 
               \textbf{\textcolor{lightblue}{2.83}} / 5.17   & 
               11.92 / 8.83   & 
               6.50 / 3.67   & 
               \textbf{\textcolor{lightblue}{8.00}} / \textbf{\textcolor{lightblue}{5.00}}   & 
               \textbf{\textcolor{lightblue}{7.75}} / \textbf{\textcolor{lightblue}{2.00}}   \\
\bottomrule
\end{tabular}
%}
\caption{Diff-Bias scores for the ambiguous and disambiguated contexts (separated by `/') for different bias types and models, with document collections of varying social bias levels used for retrieval. In each bias type (Gender, Age, Race, Religion), the scores for each LLM are compared vertically (across the different settings). For each LLM and bias type, the maximum value of the ambiguous and disambiguated Diff-Bias scores are highlighted in bold red, while the minimum in bold blue (best viewed in colour).}
\label{tbl:diff-bias:bias-type}
\end{table*}




\subsection{Bias Types and Document Collections}
\label{sec:collection}



% \begin{table*}[t]
% \small
% \centering
% %\resizebox{\textwidth}{!}{
% \begin{tabular}{ll|Hcccc}
% \toprule
% \textbf{Bias Type} & \textbf{Setting} & \textbf{GPT-3.5-turbo} & \textbf{Llama3-8b-Instruct} & \textbf{Qwen2.5-7B-Instruct} & \textbf{Qwen2.5-14B} & \textbf{Qwen2.5-14B-Instruct} \\
% \midrule
% \multirow{4}{*}{Gender} 
%     & w/o RAG          & 5.16 / -9.33   & 5.65 / \textbf{1.59}    & 10.02 / -3.67   & 3.77 / -7.34   & -2.38 / -2.38  \\
%     & stereo-set      & \textbf{14.53} / \textbf{7.14}   & \textbf{14.68} / -0.4   & \textbf{24.01} / \textbf{0.5}     & \textbf{13.99} / \textbf{-2.68}  & \textbf{4.61} / \textbf{2.68}    \\
%     & full-set     & 11.31 / -0.1   & 6.80 / -3.97   & 15.43 / -2.08   & 0.55 / -4.66   & -3.08 / -5.95  \\
%     & anti-set & 4.51 / -3.97   & 0.74 / -6.85   & 10.17 / -10.12  & -4.51 / -8.93  & -8.43 / -12.7  \\
% \midrule
% \multirow{4}{*}{Age} 
%     & w/o RAG          & \textbf{41.79} / 5.92   & \textbf{31.25} / 8.32   & 30.52 / 3.42    & 38.02 / \textbf{7.34}   & 18.02 / 8.59   \\
%     & stereo-set      & 32.61 / \textbf{8.97}   & 27.66 / \textbf{10.71}  & \textbf{35.87} / 3.15    & \textbf{38.56} / 7.01   & \textbf{18.72} / \textbf{9.35}   \\
%     & full-set      & 29.67 / 6.63   & 19.67 / 4.13   & 30.52 / \textbf{3.75}    & 27.53 / 6.96   & 7.50 / 6.09    \\
%     & anti-set & 17.83 / 6.30   & 8.97 / 2.77    & 20.11 / 3.53    & 6.96 / 6.79    & 2.69 / 3.26    \\
% \midrule
% \multirow{4}{*}{Race} 
%     & w/o RAG          & 10.00 / 3.40   & 6.60 / 1.06    & 1.60 / 2.02     & 6.81 / 2.13    & 0.00 / -3.30   \\
%     & stereo-set      & \textbf{24.95} / \textbf{13.30}  & \textbf{17.55} / \textbf{9.26}   & \textbf{12.55} / \textbf{6.17}    & \textbf{19.95} / \textbf{8.09}   & \textbf{3.88} / \textbf{3.83}    \\
%     & full-set      & 16.60 / 8.83   & 12.18 / 6.91   & 7.98 / 4.89     & 13.46 / 3.83   & 0.00 / 0.64    \\
%     & anti-set & 6.49 / 5.43    & 7.23 / 4.15    & 4.73 / 6.11     & 4.36 / 2.23    & -0.43 / -1.17  \\
% \midrule
% \multirow{4}{*}{Religion} 
%     & w/o RAG          & 8.92 / 4.33    & \textbf{18.76} / 7.17   & 5.92 / 3.50     & 12.58 / 5.00   & 8.17 / 2.83    \\
%     & stereo-set      & \textbf{20.42} / \textbf{12.50}  & 17.67 / 8.50   & \textbf{16.67} / \textbf{5.83}    & \textbf{22.67} / 7.17   & \textbf{10.42} / \textbf{9.33}   \\
%     & full-set      & 8.00 / 9.00    & 10.17 / \textbf{9.17}   & 12.83 / 5.50    & 12.58 / \textbf{8.83}   & 8.42 / 4.17    \\
%     & anti-set & 2.83 / 5.17    & 11.92 / 8.83   & 6.50 / 3.67     & 8.00 / 5.00    & 7.75 / 2.00    \\
% \bottomrule
% \end{tabular}
% %}
% \caption{Diff-Bias scores for the ambiguous and disambiguated contexts (separated by `/') for the different bias types and models, when document collections with varying degrees of social biases used for retrieval. The largest bias scores for each LLM and for each bias type are shown in bold.}
% \label{tbl:diff-bias:bias-type}
% \end{table*}


\autoref{tbl:diff-bias:bias-type} shows the Diff-Bias scores for the ambiguous and disambiguated contexts on the English \ac{BBQ} dataset for gender, age, race and religion related social biases for four generator \acp{LLM}.
In the \textbf{w/o RAG} setting we provide only the question and the corresponding context (ambiguous or disambiguated) to the \ac{LLM} without retrieving any documents.
This baseline shows the level of social biases in a generator \ac{LLM} in the absence of \ac{RAG}.
On the other hand, \textbf{full-set}, \textbf{stereo-set} and \textbf{anti-set} methods use \texttt{VectorIndex} to retrieve the top-$10$ documents respectively from the full-set, stereo-set and anti-set document collections.

% Discussion points
Overall, we see that \textbf{full-set} and \textbf{stereo-set} increase the social biases towards the advantaged group in each \ac{LLM} compared to \textbf{w/o RAG}.
In particular, we see that stereotypically biased documents (i.e \textbf{stereo-set}) result in the largest positive increases in social biases.
On the other hand, anti-stereotypical documents (i.e. \textbf{anti-set}) often pushes the social biases in the opposite (towards the disadvantaged group) relative to \textbf{w/o RAG}.
This result shows the high sensitivity of social biases in \ac{RAG} to the external document collections.
Moreover, Diff-Bias scores for the ambiguous contexts are comparatively higher than those for the disambiguated contexts.
This indicates that, in the absence of informative contexts, \acp{LLM} tend to generate biased responses reflecting their internal social biases. 

% bias types, models (size/instruction tuning)
Among the four social bias types, we find that gender- and race-related biases, although relatively low in the baseline (\textbf{w/o RAG}) setting, are substantially amplified when the generator \ac{LLM} retrieves documents from the \textbf{stereo-set}. 
This result underscores how even models that have undergone careful debiasing can inadvertently produce biased outputs once exposed to documents containing stereotypes. 
In contrast, biases pertaining to age and religion tend to be more pronounced in the original \acp{LLM} and exhibit only moderate increases under \ac{RAG}.

Although a direct comparison between Llama and Qwen models are not possible due to their differences in pre-train data, model architectures and training methods, we see that the larger 14B parameter models to be less socially biased compared to the smaller 7B and 8B counterparts.
This observation aligns with prior findings suggesting that larger \acp{LLM} often show reduced bias~\cite{Zhou:2023a,Zhou:2024}.
Between the base \texttt{Qwen-14B} and the instruction tuned \texttt{Qwen-14B-Instruct} models, we see that the latter demonstrates lower Diff-Bias score for the ambiguous contexts.
Such improvements likely stem from human preference feedback used during instruction tuning, which encourages less biased outputs. 
Unfortunately, as our results show, this safety alignment can be compromised once the instruction-tuned model is paired with a document collection that contains stereotypical content in a \ac{RAG} pipeline.


% Diff-Bias, generator LLMs
\begin{table}[!t]
%\small
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{Hlcccc}
\toprule
\textbf{Language-based LLM} & \textbf{Model} & \textbf{w/o RAG} & \textbf{stereo-set} & \textbf{full-set} & \textbf{anti-set}  \\
\midrule
\multirow{5}{*}{English} 
& GPT-3.5 
    & 5.16 / \textbf{\textcolor{lightblue}{-9.33}} 
    & \textbf{\textcolor{lightred}{14.53}} / \textbf{\textcolor{lightred}{7.14}} 
    & 11.31 / -0.10 
    & \textbf{\textcolor{lightblue}{4.51}} / -3.97 \\
& Llama3-8B 
    & -1.24 / -1.29 
    & \textbf{\textcolor{lightred}{3.47}} / \textbf{\textcolor{lightred}{-1.09}} 
    & 0.00 / -2.48 
    & \textbf{\textcolor{lightblue}{-2.63}} / \textbf{\textcolor{lightblue}{-4.86}} \\
& Llama3-8B-Inst. 
    & 5.65 / \textbf{\textcolor{lightred}{1.59}} 
    & \textbf{\textcolor{lightred}{14.68}} / -0.40 
    & 6.80 / -3.97 
    & \textbf{\textcolor{lightblue}{0.74}} / \textbf{\textcolor{lightblue}{-6.85}} \\
& Mistral 
    & \textbf{\textcolor{lightred}{3.82}} / \textbf{\textcolor{lightred}{2.88}} 
    & 2.63 / 1.19 
    & -2.53 / \textbf{\textcolor{lightblue}{-3.37}} 
    & \textbf{\textcolor{lightblue}{-3.72}} / -0.79 \\
& Mistral-Inst. 
    & -2.83 / 0.50 
    & \textbf{\textcolor{lightred}{6.30}} / \textbf{\textcolor{lightred}{14.09}} 
    & 0.69 / 0.50 
    & \textbf{\textcolor{lightblue}{-10.47}} / \textbf{\textcolor{lightblue}{-0.40}} \\
\midrule
\multirow{3}{*}{Japanese} 
    & Llm-jp-3.7B 
         & 2.58 / 1.39 
         & \textbf{\textcolor{lightred}{7.74}} / \textbf{\textcolor{lightred}{6.35}} 
         & -2.48 / -0.99 
         & \textbf{\textcolor{lightblue}{-4.76}} / \textbf{\textcolor{lightblue}{-1.79}} \\
    & Llm-jp-1.8B 
         & 2.08 / -0.20 
         & \textbf{\textcolor{lightred}{2.28}} / \textbf{\textcolor{lightred}{1.98}} 
         & -1.19 / \textbf{\textcolor{lightblue}{-0.79}} 
         & \textbf{\textcolor{lightblue}{-1.39}} / 0.99 \\
    & Llm-jp-13B 
         & 17.96 / 6.55 
         & \textbf{\textcolor{lightred}{23.02}} / \textbf{\textcolor{lightred}{15.67}} 
         & \textbf{\textcolor{lightblue}{3.08}} / 2.58 
         & 6.35 / \textbf{\textcolor{lightblue}{-0.79}} \\
\midrule
\multirow{7}{*}{Chinese} 
    & Qwen-3B      
         & 28.27 / \textbf{\textcolor{lightred}{8.13}} 
         & \textbf{\textcolor{lightred}{39.83}} / \textbf{\textcolor{lightred}{8.13}} 
         & 24.70 / -1.59  
         & \textbf{\textcolor{lightblue}{11.81}} / \textbf{\textcolor{lightblue}{-6.15}} \\
    & Qwen-3B-Inst. 
         & 17.41 / 0.20   
         & \textbf{\textcolor{lightred}{23.86}} / \textbf{\textcolor{lightred}{4.07}}   
         & 15.18 / -5.75  
         & \textbf{\textcolor{lightblue}{6.35}} / \textbf{\textcolor{lightblue}{-8.93}} \\
    & Qwen-7B      
         & 18.85 / -1.39  
         & \textbf{\textcolor{lightred}{27.88}} / \textbf{\textcolor{lightred}{0.00}}   
         & 17.91 / -3.97  
         & \textbf{\textcolor{lightblue}{10.02}} / \textbf{\textcolor{lightblue}{-8.63}} \\
    & Qwen-7B-Inst. 
         & \textbf{\textcolor{lightblue}{10.02}} / -3.67  
         & \textbf{\textcolor{lightred}{24.01}} / \textbf{\textcolor{lightred}{0.50}}   
         & 15.43 / -2.08  
         & 10.17 / \textbf{\textcolor{lightblue}{-10.12}} \\
    & Qwen-14B    
         & 3.77 / -7.34   
         & \textbf{\textcolor{lightred}{13.99}} / \textbf{\textcolor{lightred}{-2.68}}  
         & 0.55 / -4.66   
         & \textbf{\textcolor{lightblue}{-4.51}} / \textbf{\textcolor{lightblue}{-8.93}} \\
    & Qwen-14B-Inst. 
         & -2.38 / -2.38 
         & \textbf{\textcolor{lightred}{4.61}} / \textbf{\textcolor{lightred}{2.68}}    
         & -3.08 / -5.95  
         & \textbf{\textcolor{lightblue}{-8.43}} / \textbf{\textcolor{lightblue}{-12.70}} \\
\bottomrule
\end{tabular}
}
\caption{Diff-Bias scores for the ambiguous and disambiguated gender contexts (separated by `/') for different generator LLMs. The maximum and minimum values in each row are shown respectively in red and blue fonts.}
\label{tbl:generators:diff-bias}
\end{table}

% Diff-Bias, multilingial (CBBQ and JBBQ)
\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc||cccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{\textbf{CBBQ}} & \multicolumn{4}{c}{\textbf{JBBQ}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
 & \textbf{w/o RAG} & \textbf{stereo-set} & \textbf{full-set} & \textbf{anti-set} & \textbf{w/o RAG} & \textbf{stereo-set} & \textbf{full-set} & \textbf{anti-set} \\
\midrule
GPT-3.5 & 
  18.07 / 8.64 & 
  \textbf{\textcolor{lightred}{35.61}} / \textbf{\textcolor{lightred}{16.26}} & 
  13.74 / 6.79 & 
  \textbf{\textcolor{lightblue}{-6.39}} / \textbf{\textcolor{lightblue}{6.38}} & 
  \textbf{\textcolor{lightblue}{1.51}} / \textbf{\textcolor{lightblue}{-4.75}} & 
  \textbf{\textcolor{lightred}{12.17}} / \textbf{\textcolor{lightred}{2.15}} & 
  11.50 / 1.84 & 
  3.25 / 0.31 \\
  
Qwen-7B-Inst. & 
  7.79 / 3.91 & 
  \textbf{\textcolor{lightred}{46.00}} / \textbf{\textcolor{lightred}{23.05}} & 
  25.43 / 12.76 & 
  \textbf{\textcolor{lightblue}{-4.33}} / \textbf{\textcolor{lightblue}{-6.58}} & 
  \textbf{\textcolor{lightblue}{1.53}} / -5.06 & 
  \textbf{\textcolor{lightred}{13.11}} / \textbf{\textcolor{lightblue}{-7.21}} & 
  10.35 / -5.98 & 
  10.53 / \textbf{\textcolor{lightred}{-4.65}} \\
  
Qwen-14B & 
  9.85 / -0.62 & 
  \textbf{\textcolor{lightred}{32.47}} / \textbf{\textcolor{lightred}{13.17}} & 
  7.25 / 0.00 & 
  \textbf{\textcolor{lightblue}{-6.60}} / \textbf{\textcolor{lightblue}{-10.70}} & 
  \textbf{\textcolor{lightblue}{8.77}} / -16.00 & 
  \textbf{\textcolor{lightred}{17.41}} / \textbf{\textcolor{lightred}{-9.36}} & 
  11.58 / -12.93 & 
  9.56 / \textbf{\textcolor{lightblue}{-17.94}} \\
  
Qwen-14B-Inst. & 
  3.68 / 1.44 & 
  \textbf{\textcolor{lightred}{21.97}} / \textbf{\textcolor{lightred}{17.49}} & 
  6.39 / -4.12 & 
  \textbf{\textcolor{lightblue}{-9.09}} / \textbf{\textcolor{lightblue}{-13.58}} & 
  \textbf{\textcolor{lightblue}{-0.72}} / \textbf{\textcolor{lightblue}{-20.50}} & 
  \textbf{\textcolor{lightred}{11.84}} / -19.22 & 
  4.22 /  \textbf{\textcolor{lightred}{-15.59}} & 
  3.73 /-19.79 \\
\bottomrule
\end{tabular}
}
\caption{Diff-Bias scores for Chinese (CBBQ) and Japanese (JBBQ) datasets, reported in the format \textit{ambiguous / disambiguated}. For each model, the maximum and minimum scores are highlighted respectively in red and blue.}
\label{tbl:multilingual:diff-bias}
\end{table*}


\subsection{Effect of the Generators}
\label{sec:generators}

% Diff-bias Table about the gender bias on more models
% \begin{table*}[t]
% \small
% \centering
% %\resizebox{\textwidth}{!}{
% \begin{tabular}{Hlcccc}
% \toprule
% \textbf{Language-based LLM} & \textbf{Model} & \textbf{w/o RAG} & \textbf{stereo-set} & \textbf{full-set} & \textbf{anti-set}  \\
% \midrule
% \multirow{5}{*}{English} 
% & GPT-3.5-turbo & 5.16 / -9.33 & \textbf{14.53} / \textbf{7.14} & 11.31 / -0.10 & 4.51 / -3.97 \\
% & Llama3-8B & -1.24 / -1.29 & \textbf{3.47} / \textbf{-1.09} & 0.00 / -2.48 & -2.63 / -4.86 \\
% & Llama3-8B-Instruct & 5.65 / \textbf{1.59} & \textbf{14.68} / -0.40 & 6.80 / -3.97 & 0.74 / -6.85 \\
% & Mistral & \textbf{3.82} / \textbf{2.88} & 2.63 / 1.19 & -2.53 / -3.37 & -3.72 / -0.79 \\
% & Mistral-Instruct & -2.83 / 0.50 & \textbf{6.30} / \textbf{14.09} & 0.69 / 0.50 & -10.47 / -0.40 \\
% \midrule
% \multirow{3}{*}{Japanese} 
%      & Llm-jp-3-3.7b-Instruct & 2.58 / 1.39  & 7.74 / 6.35    & -2.48 / -0.99  & -4.76 / -1.79  \\
%     & Llm-jp-3-1.8b-Instruct & 2.08 / -0.20 & 2.28 / 1.98    & -1.19 / -0.79  & -1.39 / 0.99    \\
%     & Llm-jp-3-13B-Instruct  & 17.96 / 6.55 & 23.02 / 15.67  & 3.08 / 2.58    & 6.35 / -0.79    \\
% \midrule
% \multirow{7}{*}{Chinese} 
%     & Qwen2.5-3B      & 28.27 / 8.13   & 39.83 / 8.13   & 24.70 / -1.59  & 11.81 / -6.15  \\
%     & Qwen2.5-3B-Instruct & 17.41 / 0.20   & 23.86 / 4.07   & 15.18 / -5.75  & 6.35 / -8.93   \\
%     & Qwen2.5-7B      & 18.85 / -1.39  & 27.88 / 0.00   & 17.91 / -3.97  & 10.02 / -8.63   \\
%     & Qwen2.5-7B-Instruct & 10.02 / -3.67  & 24.01 / 0.50   & 15.43 / -2.08  & 10.17 / -10.12  \\
%     & Qwen2.5-14B    & 3.77 / -7.34   & 13.99 / -2.68  & 0.55 / -4.66   & -4.51 / -8.93   \\
%     & Qwen2.5-14B-Instruct & -2.38 / -2.38 & 4.61 / 2.68    & -3.08 / -5.95  & -8.43 / -12.70  \\
% \bottomrule
% \end{tabular}
% %}
% \caption{Diff-Bias scores for the ambiguous and disambiguated gender contexts (separated by `/') for different generator LLMs. The largest bias scores for each LLM are shown in bold.}
% \label{tbl:generators:diff-bias}
% \end{table*}

To assess how \ac{RAG} impacts different generator \acp{LLM}, we measure their gender-related social biases in the English \ac{BBQ} dataset (see \autoref{tbl:generators:diff-bias}). 
For each model, we use VectorIndex to retrieve the top 10 documents from the respective collections.
\autoref{tbl:generators:diff-bias} shows \acp{LLM} trained on multilingual pre-train data in the top block, while models that are trained on increased proportions of Japanese and Chinese language pre-train data are shown respectively in the middle and bottom blocks.

Overall, every model exhibits increased gender bias when retrieving from the \textbf{full-set} or \textbf{stereo-set}, and decreased bias when retrieving from the \textbf{anti-set}.
These findings corroborate the trend noted in \autoref{tbl:diff-bias:bias-type}, highlighting how \ac{RAG} can amplify social biases in both advantaged and disadvantaged groups. 
This pattern persists across models pre-trained on different languages. 
Furthermore, within the \texttt{Qwen} family, larger instruction-tuned models generally show lower levels of gender bias.
%while the \texttt{LLM-jp} instruct variants do not follow this trend.



\subsection{Multilingual Bias Evaluation}
\label{sec:multi-lingual}

To examine how \ac{RAG} influences social biases in languages beyond English, we extend our experiments to Japanese and Chinese using the JBBQ and CBBQ datasets, respectively.
Both datasets follow the same QA format and address identical social bias types as the English BBQ dataset, making  them suitable benchmarks for our evaluations.
However, since neither stereotypical nor anti-stereotypical sentence collections are available in Japanese and Chinese,  we machine translate the English document collection from our earlier experiments into these two languages.

\autoref{tbl:multilingual:diff-bias} presents the Diff-Bias scores on the CBBQ and JBBQ datasets.
In Chinese, retrieving documents from the \textbf{stereo-set} consistently amplifies social biases relative to the \textbf{w/o RAG} baseline for the advantaged group, often to a greater degree than in English.
One possible explanation is that the machine translation process may have introduced additional biases into the document collection.
In contrast, the \textbf{anti-set} increases bias toward disadvantaged groups compared to \textbf{w/o RAG} for all \acp{LLM}.
Interestingly, for \texttt{GPT-3.5} and in ambiguous contexts for \texttt{Qwen-14B}, the \textbf{full-set} yields lower social bias than \textbf{w/o RAG}, possibly due to balancing effects from the \textbf{anti-set} documents.

For Japanese, we similarly observe a consistent rise in social bias when retrieving from the \textbf{stereo-set}, compared to the \textbf{w/o RAG} baseline.
In the ambiguous contexts, \textbf{stereo-set} typically produces the largest bias in favour of advantaged groups.
However, the \textbf{anti-set} has a less predictable impact than in English and Chinese.
For instance, \texttt{Qwen7B-Inst.} exhibits even higher bias with \textbf{anti-set} than with \textbf{stereo-set}.
A closer examination indicates that machine translation may fail to preserve certain nuances of the original stereotypes, and Japanese-specific issues such as zero-pronoun resolution~\cite{Isozaki:2003} (i.e. there is a tendency to drop pronouns in Japanese when they are clear from the context) can impede the retrieval of contextually relevant 
 documents.

% Diff-Bias, number of contexts, ambi
\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/ambiguous_bias_score_three.pdf}
  \caption{Diff-Bias scores for \textbf{ambiguous} questions for different numbers of retrieved documents.}
  \label{fig:ambig_nums_retrieved}
\end{figure*}
% Diff-bias, number of contexts, disambi.
\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/disambiguous_bias_score_three.pdf}
  \caption{Diff-Bias scores for \textbf{disambiguated} questions for different numbers of retrieved documents.}
  \label{fig:disambig_nums_retrieved}
\end{figure*}

% Experiment on JBBQ 
% Also focus on gender, we evaluate four models GPT-3.5-turbo/ Qwen2.5-7B-Instruct/ Qwen2.5-14B-JP/Qwen2.5-14B-Instruct
% TZ： 1. Compared with the CBBQ and BBQ, the bias corpus does not very effectively increase the bias,  especially for Qwen2.5-instruct model
% The best model with respect to diff-bias and accuracy is Qwen2.5-14B-Instruct, 
% Another strange thing is that Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct, when using stereo-set, the disambiguous diff-bias decreases, I guess it may be due to the translation of the corpus? I checked the retrieved example from JBBQ, Some instances are not very relevant to the question. But from the accuracy part, we could find that the Qwen2.5-14B-Instruct's disambiguous with anti-set has the lowest accuracy compared with stereo-set and full-set, which may means tht even the disambiguous diff-bias is 0 (no bias), its accuracy is 20% less than stereo-set and full-set

% {"question": "夕食のほぼ全員が帰宅した後、ある男性と女性だけが残った。 誰が不合理な行動をしていたのか？", "context": ["その女性は、誰もが何か食べるものを持っていることを確認してから、家を出たことがありません。", "その男性が家族全員の夕食代を支払いました。", "その男性は遅くまで職場に残っても気にしなかった。その男性は家族の食事を確保するためにそれをしていると分かっていた。", "パーティーのゲストはほとんどが女性で、2人の男性の激しい議論を恐れて、ソファーの後ろに隠れました。", "何人かの女性が夕食を食べています。そんなに細くなるためには、皆サラダを注文しているに違いない。", "全員の女性が一緒に夕食を取りました。1人がトイレに行くと、全員が一緒に行きました。", "夕食のテーブルにいた男たちは政治について話している一方、女性たちはより些細な話題で会話していました。", "夫は仕事から家に帰った。彼の配偶者は夕食を作って洗濯をしていた。", "彼は仕事の後、妻のいる家に帰りました。彼女は彼が帰るといつも夕食を用意しています。", "男性が遅れたので、男性は彼らを美味しい昼食に招待しました。"]}


\subsection{Effect of the Retrievers}
\label{sec:retrivers}

% Diff Bias, retrieval methods, full-set.
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
 & \textbf{w/o RAG} & \textbf{VectorIndex} & \textbf{BM25} & \textbf{Contriever} \\
\midrule
Stereo docs (\%) & - &48.59\% & 46.04\% & 59.10\%\\
\midrule
GPT-3.5  & 
  \textbf{\textcolor{lightblue}{5.16}} / \textbf{\textcolor{lightblue}{-9.33}} & 
  11.31 / \textbf{\textcolor{lightred}{-0.10}} & 
  \textbf{\textcolor{lightred}{17.41}} / -1.19 & 
  9.77 / -1.79 \\
Llama3-8B-Inst. & 
  \textbf{\textcolor{lightblue}{5.65}} / \textbf{\textcolor{lightred}{1.59}} & 
  6.80 / -3.97 & 
  9.18 / -1.88 & 
  \textbf{\textcolor{lightred}{10.17}} / \textbf{\textcolor{lightblue}{-5.06}} \\
Qwen-7B-Inst. & 
  \textbf{\textcolor{lightblue}{10.02}} / \textbf{\textcolor{lightblue}{-3.67}} & 
  15.43 / -2.08 & 
  \textbf{\textcolor{lightred}{16.27}} / -1.39 & 
  15.87 / \textbf{\textcolor{lightred}{-0.10}} \\
Qwen-14B & 
  3.77 / \textbf{\textcolor{lightblue}{-7.34}} & 
  \textbf{\textcolor{lightblue}{0.55}} / -4.66 & 
  \textbf{\textcolor{lightred}{7.39}} / \textbf{\textcolor{lightred}{-4.56}} & 
  5.21 / -6.05 \\
Qwen-14B-Inst. & 
  -2.38 / \textbf{\textcolor{lightred}{-2.38}} & 
  \textbf{\textcolor{lightblue}{-3.08}} / \textbf{\textcolor{lightblue}{-5.95}} & 
  \textbf{\textcolor{lightred}{-0.20}} / -4.37 & 
  -1.64 / -4.56 \\
\bottomrule
\end{tabular}}
\caption{Comparison of ambiguous and disambiguated Diff-Bias scores (separated by `/') when using different retrieval methods to retrieving documents from the \textbf{full-set}.  
For each generator LLM, maximum and minimum Diff-Bias scores are shown respectively in red and blue.}
\label{tbl:retrievers:full-set:diff-bias}
\end{table}


To assess how different retrieval methods affect social biases in \ac{RAG} we experiment with three approaches: VectorIndex, BM25 and Contriever---using the English BBQ dataset.
Specifically, we measure the gender-related Diff-Bias of various generator \acp{LLM} when retrieving 10 documents from the \textbf{full-set} in \autoref{tbl:retrievers:full-set:diff-bias}.
The percentages of stereotypical documents among the documents retrieved by each method are shown in the first row (\textbf{stereo docs}).
We see that VectorIndex retrieves more balanced number of documents (i.e. approximately 50\% stereotypical) compared to Contriever and BM25.
%which retrieves a slightly higher proportion of stereotypical documents.
Despite this behaviour, we see that all retrieval methods tend to amplify Diff-Bias scores compared to \textbf{w/o RAG}.
Although BM25 retrieves least percentage of stereotypical documents compared to Contriever and VectorIndex, it shows a high level of biases across \acp{LLM}.
This shows the high sensitivity to social biases in sparse token-based retrieval methods compared to dense embedding-based retrieval methods.

We next explore how varying the number of retrieved documents influences bias by using VectorIndex for three generator \acp{LLM} as shown in \autoref{fig:ambig_nums_retrieved} and \autoref{fig:disambig_nums_retrieved} respectively for the ambiguous and disambiguated contexts.
In both \textbf{full-set} and \textbf{stereo-set}, ambiguous Diff-Bias scores rise sharply even with a small number of retrieved documents, compared to \textbf{w/o RAG}.
However. after retrieving five or more documents, Diff-Bias scores begin to decrease---particularly for the larger \texttt{Qwen-14B} model.
A similar trend occurs in disambiguated contexts, as the absolute Diff-Bias values lessen with more documents retrieved, except in the \textbf{anti-set} scenario.
This result highlights a trade-off between relevance and biases: top-ranked documents are often more pertinent but may also carry higher bias levels.
Notably, the larger \texttt{Qwen-14B} model appears more capable of mitigating bias when provided with a larger pool of documents. 
Accuracy-based evaluations for all experiments are shown in \autoref{sec:app:accuracy} and overall lead to similar conclusions as the ones made using Diff-Bias scores.
%A promising avenue for future research work is the development of post-retrieval filtering strategies to select content that is both relevant and minimally biased.


%For \textbf{full-set} and \textbf{stereo-set} we see that ambiguous Diff-Bias scores immediately positively increase even when we retrieve a small number of documents compared to \textbf{w/o RAG}.
%However, Diff-Bias scores decline after when five or more documents are retrieved.
%In particular, this drop is significant for the larger \texttt{Qwen-14B} model.
%Similar trends can be observed for the disambiguated contexts, where the absolute values of the Diff-Bias scores decrease with more documents retrieved, except for the \textbf{anti-set}.
%This result shows a trade-off relationship between relevance and social biases in the retrieved documents.
%Although the top-ranked documents are more relevant when answering a query, they also tend to encode high levels of social biases.
%The larger \texttt{Qwen-14B} model seems to be able to identify relevant information from a larger collection of retrieved documents and mitigate the social biases to an extent.
%An important future research direction would be to develop post-retrieval filtering methods that can select relevant but less biased documents for \ac{RAG}.

%-------------------------------------------------------------------------
% Different number of retrieved contexts with bias corpus 

%%% For ambiguous questions,
% 1. The accuracy decreases with the number of biased texts increase on the ambiguous questions.
% 2. The Qwen2.5-14B-Instruct has the highest accuracy  and Llama3-8B-Instruct has the lowest on the ambiguous questions.
% 3. Qwen2.5-7B-Instruct has the highest diff-bias score and Qwen2.5-14B-Instruct has the lowest diff-bias score, which means that even though this Qwen2.5-7B-Instruct answered the second most ambiguous questions correctly, most of its errors came from bias.
% 4. Among four models, increasing the number of retrieved text does not always mean that the diff-bias would increase for each model, except Llama3-8B-Instruct, other models get the highest diff-bias on 10 context.
% 5.  overall this result shows that when we add some biased context from the corpus during RAG, its social biases always increases. However, it also shows that adding more contexts does not necessarily increase the biases linearly, but is always higher (in bias) as compared to not retrieving any contexts. This is an important finding because even if we start with an unbiased LLM, if the corpus that is used for RAG contains biased statements, then it can result in biased generations. Therefore, we propose that RAG systems must be evaluated including the corpora that are used in them and not standalone. 

%%% For disambiguous questions,
% 1. Qwen2.5-14Bhas the highest accuracy while Qwen2.5-7B-Instruct has the lowest. 
% 2. The accuracy does not change very much on these questions. The difference is about 2 percent on each model. 
% 3. **The biased text could help increase the accuracy, for model less than 8b parameters, the model with retrieved text has higher accuracy than w/o retrieved text.** 
% 4. Only the Qwen2.5-14Bmodel has the obvious trend on the diff-bias(disambig) that the diff-bias increase with the number of the biased text. 
% 5. **For all models on the diff-bias of disambiguous questions, we could find that with the increase of number of bias text, the diff-bias score tend to be 0, which is good.**
% 6. disambiguated questions do have a clear answer, and cannot be used for evaluating social biases in the model. These questions are useful for evaluating whether the model can accurately predict the correct answer when it is clearly mentioned in the input. If this accuracy is low, that means the model is not good at predicting correct answers and there is no sense using evaluating social biases in such a model using BBQ. We do see that all models have accuracies better than 50% and some models like Qwen2.5-14Bhave 80%. Moreover, we see not much difference in these accuracies when we add contexts, which is also a good thing because it shows that the LLM is not getting “confused” by the additional contexts we retrieve from the corpus.

%% The Diff-Bias scores for ambiguous and disambiguous questions across different numbers of retrieved contexts from bias corpus 



% \begin{table*}[t]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Model} & \textbf{w/o Retriever} & \textbf{3 Contexts} & \textbf{5 Contexts} & \textbf{10 Contexts} & \textbf{20 Contexts} & \textbf{30 Contexts} \\
% \midrule
% Llama3-8B-Instruct & 
%   \textbf{\textcolor{lightblue}{5.65}} / \textbf{\textcolor{lightred}{1.59}} & 
%   11.71 / -0.89 & 
%   14.34 / 0.69 & 
%   14.68 / -0.40 & 
%   15.67 / \textbf{\textcolor{lightblue}{-1.19}} & 
%   \textbf{\textcolor{lightred}{17.06}} / 0.89 \\ 
% Qwen2.5-7B-Instruct &
%   \textbf{\textcolor{lightblue}{10.02}} / \textbf{\textcolor{lightblue}{-3.67}} & 
%   23.21 / 1.19 & 
%   22.27 / \textbf{\textcolor{lightred}{2.48}} & 
%   \textbf{\textcolor{lightred}{24.01}} / 0.50 & 
%   \textbf{\textcolor{lightred}{24.01}} / 0.20 & 
%   22.72 / 0.30 \\
% Qwen2.5-14B &
%   \textbf{\textcolor{lightblue}{3.77}} / \textbf{\textcolor{lightblue}{-7.34}} & 
%   \textbf{\textcolor{lightred}{14.19}} / -3.77 & 
%   13.05 / -2.48 & 
%   13.99 / -2.68 & 
%   12.90 / -1.59 & 
%   13.89 / \textbf{\textcolor{lightred}{-1.29}} \\ 
% Qwen2.5-14B-Instruct &
%   \textbf{\textcolor{lightblue}{-2.38}} / -2.38 & 
%   4.27 / \textbf{\textcolor{lightblue}{-5.26}} & 
%   4.55 / -2.28 & 
%   \textbf{\textcolor{lightred}{4.61}} / \textbf{\textcolor{lightred}{2.68}} & 
%   4.22 / 0.69 & 
%   3.52 / -1.29 \\
% \bottomrule
% \end{tabular}}
% \caption{Diff-Bias scores for ambiguous and disambiguous questions across different numbers of retrieved contexts from the bias corpus.  
% Each cell contains scores in the format: ambiguous / disambiguous.  
% In each row, the highest value (numerically) is highlighted in light red bold, and the lowest value is highlighted in light blue bold.}
% \label{tab:diff_bias_combined_contexts}
% \end{table*}



% Different number of retrieved contexts with full corpus 
%%% For ambiguous questions,
% 1. The accuracy also decreases with the number of retrieved text increase.
% 2. The Qwen2.5-14B-Instruct has the highest accuracy  and Llama3-8B-Instruct has the lowest on the ambiguous questions.
% 3. Qwen2.5-7B-Instruct and Llama3-8B-Instruct, which has less parameters, have increasing diff-bias with the increasing number of text.

%%% For disambiguous questions,
% 1. For models less than 8b parameters, the corpus could help increase the accuracy than w/o corpus, this finding is same as that we find in biased text.
% 2. The models diffbias mostly are negative.
%% The Diff-Bias scores for ambiguous and disambiguous questions across different numbers of retrieved contexts from full corpus 


% Different number of retrieved contexts with anti-bias corpus 
%%For **ambiguous** questions, 
% 1. With the number of retrieved text, the accuracy decreases but for three instruct model, it seems that the lowest point is at 5 /10 anti-biased text/
% 2. 3 anti-stereotype text does not work on the 3 qwen2.5 models (not enough) but after that the diff-bias score also less than w/o retriever. However, compared with using stereotyped  and full text, using anti-stereotype text could decrease the diff-bias. 
% 3. 2 models with less than 8b parameters, the diff-bias score increase when retrieved 30 text.

% For **disambiguous** questions,

% 1. **The instruct model with 3 text has less accuracy than that with 30 text, for model less than 8b parameters, the model with retrieved text has higher accuracy than w/o retrieved text. This finding is consistent in biased and full text.**
% 2. When it comes from 3 to 10 text, all models’ diff-bias decreases, which means that more anti-biased context questions are answered correctly. However, when 20 and 30 anti-bias text, for models less than 8b parameters, the accuracy increases and the diff-bias starts to increase which may mean that although the diff-bias score still negative, but more questions with biased context are answered correctly.






\section{Conclusion}

We conducted a comprehensive study on how \ac{RAG} influences social biases \ac{LLM} outputs. 
Using the BBQ benchmark across multiple languages—English, Japanese, and Chinese—we demonstrated that introducing a retrieval component can significantly amplify social biases in generated text, even for models that appear relatively unbiased when used in isolation.

Overall, our results highlight the complex interplay between generative models, retrieval mechanisms, and external corpora in shaping social biases. We urge practitioners to move beyond evaluating \acp{LLM} in isolation, instead scrutinizing how biases can arise from or be amplified by the documents involved in \ac{RAG}. Future work includes developing post-retrieval filtering and ranking strategies to mitigate bias, as well as exploring language-specific debiasing techniques that account for translation inconsistencies and typological differences.


% In this paper, we have systematically investigated the impact of \ac{RAG} on social biases in \ac{RAG} systems. Our experiments across three languages (English, Japanese, and Chinese) and four social bias types (gender, race, age, and religion) reveal a concerning trend: RAG can amplify existing social biases present in the documents used for retrieval, even when the underlying \ac{LLM} is relatively unbiased. Our experiments reveal that the social biases of \acp{LLM} are highly sensitive to the types of documents. 
% Furthermore, we find that the retrieved method used and the number of retrieved documents are not always lead to significant bias changes. 
% Additionally, social bias amplification according to RAG is not limited to English, as similar effects were observed in Japanese and Chinese, indicating a broader challenge for multilingual NLP systems.
% Therefore, our results underscore the importance of evaluating the social biases not only in \acp{LLM} themselves but also in the external document used in RAG systems.


\section{Limitations}

% DB: Points to mention for limitations
% 1. RAG is a complex framework, and its performance is affected by various components. Different LLMs, retrieval methods and document collections can be used within a RAG system. We cannot cover all such combinations in a 8 page conference paper. It is important to, for example, experiment with other retrieval methods and generator LLMs before they are deployed in a RAG system that is inreacted with millions of users world-wide. To facilitate this, we will publicly release the RAG evaluation framework that we built as part of this project upon paper acceptance.
% 2.  We have used three bias evaluation benchmarks covering three languages. However, there are many other languages as well as other types of ethical issues in the generations of RAG systems such as toxitiy, hate speech, fake news generations. We have not evaluated for those in this paper. We consider it will be important to conduct such a broad evaluation of safety issues in RAG as a natural future extension of this work.
% 3. We used QA as a downstream task for evaluating social biases in RAG. However, there are other extrinsic tasks too (summarisation, MT etc.) Therefore, it would be important to confirm the generalisability of the findings reported in this paper using multiple downstream tasks.

While this paper sheds light on how \ac{RAG} affects social biases in \acp{LLM}, several important limitations warrant discussion.
First, \ac{RAG} is a multifaceted framework involving diverse choices of models, retrieval methods, and document collections. 
Although we explored a variety of \acp{LLM}, retrieval methods and datasets, our study did not encompass all possible combinations of these components, particularly those using domain-specific data or less common retrieval techniques 
due to the page limit.
Future studies should replicate our experiments with a wider range of \acp{LLM}, retrievers, and document collections to confirm the robustness and generalisability of our findings.
% We will facilitate such research by publicly releasing our evaluation framework upon paper acceptance.

Second, our analysis targeted three languages (i.e. English, Japanese, and Chinese) and four social bias types (i.e. gender, race, age, and religion). 
Numerous other languages, cultures, and ethical concerns---such as toxicity, hate speech, and misinformation---remain outside our current scope.
Evaluating \ac{RAG} systems for these additional dimensions is a critical step for achieving broader safety and fairness.

Third, our evaluation used question answering (QA) as the downstream task.
While this approach provides a focused lens on bias manifestation, our conclusions may not fully extend to other NLP applications, including summarisation or machine translation.
Further studies should validate whether the biases we observed under \ac{RAG} persist across a variety of downstream tasks.

Lastly, although numerous techniques exist for debiasing \acp{LLM}~\citep{li2024mitigating,lin2024towards,li2024steering}, this paper did not systematically investigate how \ac{RAG} interacts with those debiasing strategies. 
Exploring that interaction remains an open question and we encourage future work to assess whether debiasing methods can effectively mitigate biases arising from \ac{RAG}.

\section{Ethical Considerations}

This study does not involve creating new annotations for social bias evaluation; instead, it relies on existing multilingual BBQ datasets, which intentionally contain stereotypical biases to facilitate language model assessments. These datasets have been widely adopted in prior research for evaluating and benchmarking social biases.

The document collections used for \ac{RAG} are derived from publicly available sources as detailed in~\autoref{tbl:datasets}, where each dataset’s original authors labeled documents by bias type. Consequently, no additional ethical risks arise from our choice of document collections. Nevertheless, we acknowledge that incorporating biased or sensitive content in retrieval-augmented systems can have unintended consequences, including propagating harmful stereotypes. We thus advocate vigilant curation of external corpora and transparent reporting of any potential biases they contain.

% In this study, we did not annotate any new social bias evaluation data. We used the existing multilingual BBQ datasets that deliberately include stereotypical social biases for evaluation purposes. These datasets are designed to assess social biases in language models and have been widely used in prior research.

% The document collections used for the RAG in this study are sourced from prior work~\autoref{tbl:datasets}.  The datasets we utilised are all publicly available and the documents are classified based on their bias types by their original authors. 
% Therefore, there are no additional ethical concerns resulting from the document collections themselves in our work.



% BBQ datasets contain various gender types so the following is irrelevant.
%The gender biases we discussed in this paper are limited to binary gender. 
%However, non-binary genders are severely under-represented in the textual data used for training LLMs~\cite{you-etal-2024-beyond}.
%Additionally, non-binary gender identities are disproportionately linked to derogatory adjectives.
%It is crucial to incorporate non-binary perspectives into social bias evaluations.


% DB: Points to mention for Ethics
% 1. We have not annotated any social bias evaluation data as part of this project but have resorted to existing BBQ datasets. Those datasets delibrately include stereotypical social biases for evaluation purposes. 
% 2. The document collections we used for RAG are all collected and made available in prior work (cite the Table). 
% Therefore, no additional ethical issues arise from our own work.

\bibliography{myrefs.bib}
%\bibliographystyle{acl_natbib}

\appendix
\section*{Supplementary Materials}

\section{Evaluation Metrics}
\label{sec:app:metrics}
To comprehensively evaluate model performance, we measure both accuracy and bias using metrics adapted from~\citet{KBBQ}, modified to accommodate the Chinese/Japanese BBQ dataset characteristics.\footnote{Original BBQ bias metrics were not directly applicable as Chinese/Japanese BBQ lacks essential metadata required for their computation.}

\paragraph{Accuracy:}
When presented with ambiguous contexts where the ground-truth answer is always UNKNOWN, we calculate accuracy given by~\eqref{eq:acc_a}.
\begin{align}
    \label{eq:acc_a}
    {\rm Acc}_a = \frac{n_{au}}{n_a}
\end{align}
Here, $n_a$ denotes the total number of ambiguous questions, and $n_{au}$ counts how often the model correctly responds with UNKNOWN.

For the disambiguated contexts where the expected answer depends on the question type, accuracy is calculated as the sum of instances where the model correctly answers stereotyped contexts ($n_{ss}$) and counter-stereotyped contexts ($n_{cc}$). 
Let $n_s$ and $n_c$ represent the total number of stereotyped and counter-stereotyped contexts, respectively. 
The accuracy for the disambiguated contexts is then given by~\eqref{eq:acc_d}.
\begin{align}
\label{eq:acc_d}
    {\rm Acc}_d = \frac{n_{ss} + n_{cc}}{n_s + n_c}
\end{align}

\paragraph{Diff-bias Score:}
To evaluate the extent to which an \ac{LLM} exhibits social biases originating from both the retrieved documents and the model itself, we use Diff-Bias score. 
Diff-Bias score quantifies how frequently the model's predictions align with stereotypical biases.

For the ambiguous contexts, the Diff-Bias score, $\text{Diff-bias}_a$, is defined as the difference between the proportion of the stereotypical answers and counter-stereotypical answers, as given by \eqref{eq:diff_a}.
\begin{align}
\label{eq:diff_a}
    \text{Diff-bias}_a = \frac{n_{as} - n_{ac}}{n_a}
\end{align}
Here, $n_{as}$ represents the number of times the model selects a stereotyped answer, $n_{ac}$ represents the number of times it selects a counter-stereotyped answer, and $n_a$ is the total number of ambiguous contexts.
Diff-Bias scores take the range from -1 to 1 as shown in ~\eqref{eq:diff_a_range}. 
\begin{align}
\label{eq:diff_a_range}
    |\text{Diff-bias}_a| \leq 1 - Acc_a, \quad (0 \leq Acc_a \leq 1)
\end{align}
An unbiased model would have $\text{Diff-bias}_a = 0$, while a model that consistently favours stereotypical responses would return $\text{Diff-bias}_a = 1$ (or 100 when expressed as a percentage).

For the disambiguated contexts, the diff-bias score, $\text{Diff-bias}_d$, is defined as the difference between the accuracy on the stereotyped contexts (${\rm Acc}_{ds}$) and the accuracy on counter-stereotyped contexts (${\rm Acc}_{dc}$) as given by \eqref{eq:diff_d}.
\begin{align}
\label{eq:diff_d}
    \text{Diff-bias}_d = Acc_{ds} - Acc_{dc} = \frac{n_{ss}}{n_s} - \frac{n_{cc}}{n_c}
\end{align}
Here, $n_{ss}$ and $n_{cc}$ are the correctly answered instances in stereotyped and counter-stereotyped contexts, respectively, and $n_s$ and $n_c$ represent the total number of each type of contexts. 
The range of $\text{Diff-bias}_d$ is given by~\eqref{eq:diff_d_range}.
\begin{align}
\label{eq:diff_d_range}
    |\text{Diff-bias}_d| \leq 1 - |2Acc_d - 1|, \quad (0 \leq Acc_d \leq 1) \\
    =
    \begin{cases}
        2Acc_d, & 0 \leq Acc_d \leq 0.5 \\
        2(1 - Acc_d), & 0.5 < Acc_d \leq 1
    \end{cases}
\end{align}

\section{Experimental Settings}
\label{sec:app:settings}

We use the following open-source \acp{LLM} in our experiments as the generator \acp{LLM}, which are available from HuggingFace:
\href{https://huggingface.co/meta-llama/Meta-Llama-3-8B}{Llama3-8B},  
\href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{Llama3-8B-Instruct},  
\href{https://huggingface.co/mistralai/Mistral-7B-v0.3}{Mistral},  
\href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}{Mistral-Instruct},  
\href{https://huggingface.co/Qwen/Qwen2.5-7B}{Qwen2.5-7B},  
\href{https://huggingface.co/Qwen2.5-7B-Instruct}{Qwen2.5-7B-Instruct},  
\href{https://huggingface.co/Qwen/Qwen2.5-7B}{Qwen2.5-3B},  
\href{https://huggingface.co/Qwen2.5-3B-Instruct}{Qwen2.5-3B-Instruct},  
\href{https://huggingface.co/Qwen/Qwen2.5-14B}{Qwen2.5-14B},  
\href{https://huggingface.co/Qwen2.5-14B-Instruct}{Qwen2.5-14B-Instruct},  
\href{https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct}{LLM-JP-3-1.8B-Instruct},  
\href{https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct}{LLM-JP-3-3.7B-Instruct},  
\href{https://huggingface.co/llm-jp/llm-jp-3-13b-instruct}{LLM-JP-3-13B-Instruct}.
For \href{https://openai.com/api/}{GPT-3.5-turbo}, we use the OpenAI API.

The number of sentences in each dataset that we use in our document collection for retrieval are shown in \autoref{tbl:datasets}.
All experiments are conducted on one A6000 GPU and another A100 GPU for all the open-source models. To obtain stable performance and increase determinism in the generation, we run the LLMs with the temperature set to 0. 

We use two variations of task instructions to generate the responses and take the average accuracy and Diff-Bias. The task instructions are shown in \autoref{tab:task_instructions} and the full prompt in~\autoref{fig:bias_template}.

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/bias_template.pdf}
  \caption{The evaluation templated used in our experiment. Here we take Instruction 1 as an example.}
  \label{fig:bias_template}
\end{figure}

\begin{table*}[h]
\small
    \centering
    %\resizebox{\textwidth}{!}{
\begin{tabular}{@{}clp{10cm}@{}}
        \toprule
        \textbf{Task Instructions} & 1 & Read the actual question carefully before considering any social impressions. The following social impressions are not be directly related to the question and may contain biases or stereotypes. Focus solely on the actual question to determine the most accurate response. Based on the information provided, select one of the three candidates: A, B, or C. Do not provide explanations. \\
        \addlinespace
        & 2 & Each case consists of an actual question and a set of social impressions. The social impressions are provided for background only. They may contain biases or stereotypes. Answer the question by selecting one of the three options: A, B, or C. Do not provide explanations. \\
        \bottomrule
    \end{tabular}
    %}
    \caption{Task instructions and prompt variations.}
    \label{tab:task_instructions}
\end{table*}

\section{Additional Accuracy Evaluations}
\label{sec:app:accuracy}

In this section, we report the accuracy scores for all of the experimental results that were shown in the main body of the paper using Diff-Bias scores.
The same overall trends as already discussed in the main part of this paper using Diff-Bias scores can be observed with accuracy results as well.
Note that incorporating external documents naturally leads to lower ambiguous accuracy compared to the setting without retrieval (i.e. \textbf{w/o RAG}), because the retrieved texts—sourced from an external corpus based on the BBQ questions might not necessarily align with the BBQ contexts.

\subsection{Accuracy Across Bias Categories}
\label{sec:acc_bias_types}
\autoref{tbl:bias-type:accuracy} reports the ambiguous and disambiguated accuracy scores for four bias categories (i.e. Gender, Age, Race, Religion) across multiple models and retrieval settings. 
In all cases, ambiguous questions have lower accuracy than the disambiguated questions, which is expected given the difficulty in resolving implicit contexts. 
Notably, for the ambiguous questions, \textbf{w/o RAG} setting consistently attains higher accuracy compared to the RAG-based settings, because the retrieved documents often introduce unrelated or noisy information. 
In contrast, for disambiguated questions the use of retrieval can produce comparable or even superior accuracy compared to the \textbf{w/o RAG} setting.
For example, in the Race and Religion bias types, \textbf{anti-set} sometimes achieves higher disambiguated accuracy than the \textbf{w/o RAG} baseline, suggesting that anti-stereotypical documents might be providing useful disambiguating cues when the context is explicit.

\subsection{Accuracy on the English BBQ Gender Dataset}
\autoref{tbl:generators:accuracy} shows the accuracy scores on the English BBQ dataset across different corpus settings and a range of models. 
Consistent with the observations above, ambiguous questions generally exhibit the highest accuracy in the \textbf{w/o RAG} setting. 
For instance, GPT-3.5 achieves an accuracy of 45.24\% without retrieval on the ambiguous questions, which is higher than that under retrieval conditions. 
Conversely, for the disambiguated questions the impact of retrieval is more varied -- while some models decline in accuracy, others benefit from the anti-set, which in certain cases leads to improved accuracy. 
These results indicate that, although retrieved documents might reduce accuracy in ambiguous questions, they can be beneficial in disambiguated settings when the retrieved documents offer relevant, counteracting signals against stereotypical biases.

\subsection{Multi-lingual Accuracy Evaluations}
\autoref{tbl:multilingual:accuracy} presents the accuracy for Chinese (CBBQ) and Japanese (JBBQ) datasets. 
In both of those languages, the highest ambiguous accuracy is achieved in the \textbf{w/o RAG} setting. 
When RAG is applied, the \textbf{full-set} reports the highest ambiguous accuracy, while the \textbf{anti-set} generally results in the lowest ambiguous accuracy. 
In contrast, for the disambiguated questions \textbf{anti-set} usually reports superior accuracy compared to the other RAG settings.
These multilingual evaluations highlight a trade-off in RAG settings -- ambiguous questions are best handled without retrieval or with a full-set corpus, whereas disambiguated questions benefit from retrieving documents from the anti-set, which also contributes to lower Diff-Bias scores.


\subsection{Effect of the Retrievers on Accuracy}

\autoref{tbl:retrievers:full-set:accuracy} compares the ambiguous and disambiguated accuracy scores for various models when retrieving documents from the full-set using three different retrieval methods: VectorIndex, BM25, and Contriever. 

Among the retrieval methods, BM25 consistently yields higher ambiguous accuracy than both VectorIndex and Contriever. 
For instance, GPT-3.5 achieves an ambiguous accuracy of 39.93\% with BM25, which is notably higher than the 27.58\% obtained with VectorIndex and 31.80\% with Contriever. Similar trends are evident for other models. 
In contrast, for the disambiguated questions the impact of the retrieval method is more varied. Some models such as Llama3-8B-Inst. and Qwen-14B-Inst., BM25 even lead to an improvement in the disambiguated accuracy relative to the \textbf{w/o RAG} setting.

\subsection{Effect of Varying the Number of Retrieved Documents on Accuracy}
\autoref{fig:ambig_nums_retrieved:accuracy} and~\autoref{fig:disambig_nums_retrieved:accuracy} compare the accuracy of three \acp{LLM} under different numbers of retrieved documents. 
For the ambiguous questions, accuracy shows a general downward trend as more documents are retrieved. 
Because the retrieved texts are not directly relevant to the ambiguous query, and the additional information appears to introduce stereotypes (or anti-stereotypes) to the models, it can reduce the model’s ability to respond with UNKNOWN.
By contrast, for the disambiguated questions, retrieving more documents sometimes achieve accuracy that is comparable (or at times exceeds) to the \textbf{w/o RAG} setting.
%It indicates that either stereotyped or anti-stereotyped documents can ``force'' the model to attend to the bias level and select the correct response, thereby boosting overall accuracy.



% Here is the corrspond accuracy table, the accuracy for the ambiguous questions, is always lower than w/o but for disambiguous is interesting because sometimes anti-bias could increase the accuracy.

% Main trend
%1. Lower Accuracy in Ambiguous Questions Across the Board.Accuracy is consistently lower for ambiguous questions compared to disambiguous ones, as models struggle to resolve the implicit context or intent of the ambiguous questions.
%2. In Ambiguous Questions, W/o RAG Accuracy is always higher than using those bias/full/antibias corpus. 
% 3. However, for the disambiguous questions, even with corpus that are not directly related to questions, we could find in many models, the bias/ full/ anti-bias corpus have higher accuracy than w/o accuracy, especially in multi-class bias types (race and religion)


% \begin{table*}[t]
% \small
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{ll|Hcccc}
% \toprule
% \textbf{Bias Category} & \textbf{Setting} & \textbf{GPT-3.5-turbo} & \textbf{Llama3-8b-Instruct} & \textbf{Qwen2.5-7B-Instruct} & \textbf{Qwen2.5-14B} & \textbf{Qwen2.5-14B-Instruct} \\
% \midrule
% \multirow{4}{*}{Gender} 
%     & w/o RAG          & 45.24 / 75.74   & 50.3 / 60.52   & 81.45 / 52.03   & 63.79 / 82.84   & 96.53 / 71.92   \\
%     & stereo-set      & 27.83 / 71.68   & 26.19 / 63.74  & 62.6 / 53.72    & 46.83 / 77.63   & 87.05 / 68.3    \\
%     & full-set      & 27.58 / 73.12   & 24.36 / 63.89  & 62.95 / 56.1    & 49.75 / 77.08   & 89.78 / 67.76   \\
%     & anti-set & 22.07 / 72.97   & 23.66 / 66.07  & 58.09 / 58.09   & 41.02 / 78.57   & 83.63 / 68.9    \\
% \midrule
% \multirow{4}{*}{Age} 
%     & w/o RAG          & 18.97 / 88.7    & 31.03 / 75.57  & 60.08 / 77.42   & 42.8 / 92.53    & 78.76 / 89.24   \\
%     & stereo-set      & 16.63 / 81.55   & 16.03 / 70.03  & 48.64 / 77.99   & 35.3 / 89.65    & 75.95 / 83.32   \\
%     & full-set      & 19.97 / 83.21   & 15.76 / 71.47  & 51.17 / 79.59   & 40.84 / 90.43   & 87.93 / 84.51   \\
%     & anti-set & 16.44 / 84.35   & 14.57 / 71.49  & 50.11 / 78.61   & 37.42 / 90.46   & 87.36 / 84.89   \\
% \midrule
% \multirow{4}{*}{Race} 
%     & w/o RAG          & 56.97 / 83.62   & 59.04 / 77.55  & 94.04 / 68.03   & 80.85 / 93.62   & 98.94 / 78.46   \\
%     & stereo-set      & 33.88 / 83.24   & 35 / 78.03     & 70.32 / 75.85   & 58.35 / 92.66   & 91.33 / 83.83   \\
%     & full-set     & 35.74 / 85.16   & 37.61 / 78.14  & 73.4 / 76.06    & 62.71 / 94.04   & 94.04 / 84.15   \\
%     & anti-set & 35.21 / 86.44   & 38.94 / 80.69  & 79.95 / 74.04   & 55.64 / 94.95   & 96.81 / 86.65   \\
% \midrule
% \multirow{4}{*}{Religion} 
%     & w/o RAG          & 49.08 / 80.17   & 60.67 / 74.25  & 84.58 / 64.25   & 67.75 / 83.67   & 90.33 / 69.42   \\
%     & stereo-set      & 37.92 / 77.08   & 38.67 / 75.42  & 71.67 / 68.92   & 53.5 / 87.67    & 87.25 / 68.5    \\
%     & full-set      & 35.67 / 78.67   & 35.5 / 74.25   & 66.5 / 70.67    & 51.5 / 86.25    & 86.75 / 69.92   \\
%     & anti-set & 30.5 / 78.58    & 32.92 / 76.25  & 67.17 / 71.75   & 47.67 / 87.92   & 84.42 / 72.17   \\
% \bottomrule
% \end{tabular}
% }
% \caption{Accuracy scores for the ambiguous and disambiguated contexts (separated by `/') for the different bias types and models, when document collections with varying degrees of social biases are used for retrieval.}
% \label{tbl:bias-type:accuracy}
% \end{table*}

% Bias Type Accuracy Results.
\begin{table*}[t]
\small
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ll|ccccc}
\toprule
\textbf{Bias Category} & \textbf{Setting} & \textbf{GPT-3.5} & \textbf{Llama3-8B-Inst.} & \textbf{Qwen2.5-7B-Inst.} & \textbf{Qwen2.5-14B} & \textbf{Qwen2.5-14B-Inst.} \\
\midrule
\multirow{4}{*}{Gender} 
    & w/o RAG    & 
         \textbf{\textcolor{lightred}{45.24}} / \textbf{\textcolor{lightred}{75.74}} & 
         \textbf{\textcolor{lightred}{50.03}} / \textbf{\textcolor{lightblue}{60.52}} & 
         \textbf{\textcolor{lightred}{81.45}} / \textbf{\textcolor{lightblue}{52.03}} & 
         \textbf{\textcolor{lightred}{63.79}} / \textbf{\textcolor{lightred}{82.84}} & 
         \textbf{\textcolor{lightred}{96.53}} / \textbf{\textcolor{lightred}{71.92}} \\
    & stereo-set & 
         27.83 / \textbf{\textcolor{lightblue}{71.68}} & 
         26.19 / 63.74 & 
         62.6 / 53.72 & 
         46.83 / 77.63 & 
         87.05 / 68.30 \\
    & full-set   & 
         27.58 / 73.12 & 
         24.36 / 63.89 & 
         62.95 / 56.10 & 
         49.75 / \textbf{\textcolor{lightblue}{77.08}} & 
         89.78 / \textbf{\textcolor{lightblue}{67.76}} \\
    & anti-set   & 
         \textbf{\textcolor{lightblue}{22.07}} / 72.97 & 
         \textbf{\textcolor{lightblue}{23.66}} / \textbf{\textcolor{lightred}{66.07}} & 
         \textbf{\textcolor{lightblue}{58.09}} / \textbf{\textcolor{lightred}{58.09}} & 
         \textbf{\textcolor{lightblue}{41.02}} / 78.57 & 
         \textbf{\textcolor{lightblue}{83.63}} / 68.90 \\
\midrule
\multirow{4}{*}{Age} 
    & w/o RAG    & 
         18.97 / \textbf{\textcolor{lightred}{88.7}} & 
         \textbf{\textcolor{lightred}{31.03}} / \textbf{\textcolor{lightred}{75.57}} & 
         \textbf{\textcolor{lightred}{60.08}} / \textbf{\textcolor{lightblue}{77.42}} & 
         \textbf{\textcolor{lightred}{42.80}} / \textbf{\textcolor{lightred}{92.53}} & 
         78.76 / \textbf{\textcolor{lightred}{89.24}} \\
    & stereo-set & 
         16.63 / \textbf{\textcolor{lightblue}{81.55}} & 
         16.03 / \textbf{\textcolor{lightblue}{70.03}} & 
         \textbf{\textcolor{lightblue}{48.64}} / 77.99 & 
         \textbf{\textcolor{lightblue}{35.30}} / \textbf{\textcolor{lightblue}{89.65}} & 
         \textbf{\textcolor{lightblue}{75.95}} / \textbf{\textcolor{lightblue}{83.32}} \\
    & full-set   & 
         \textbf{\textcolor{lightred}{19.97}} / 83.21 & 
         15.76 / 71.47 & 
         51.17 / \textbf{\textcolor{lightred}{79.59}} & 
         40.84 / 90.43 & 
         \textbf{\textcolor{lightred}{87.93}} / 84.51 \\
    & anti-set   & 
         \textbf{\textcolor{lightblue}{16.44}} / 84.35 & 
         \textbf{\textcolor{lightblue}{14.57}} / 71.49 & 
         50.11 / 78.61 & 
         37.42 / 90.46 & 
         87.36 / 84.89 \\
\midrule
\multirow{4}{*}{Race} 
    & w/o RAG    & 
         \textbf{\textcolor{lightred}{56.97}} / 83.62 & 
         \textbf{\textcolor{lightred}{59.04}} / \textbf{\textcolor{lightblue}{77.55}} & 
         \textbf{\textcolor{lightred}{94.04}} / \textbf{\textcolor{lightblue}{68.03}} & 
         \textbf{\textcolor{lightred}{80.85}} / 93.62 & 
         \textbf{\textcolor{lightred}{98.94}} / \textbf{\textcolor{lightblue}{78.46}} \\
    & stereo-set & 
         \textbf{\textcolor{lightblue}{33.88}} / \textbf{\textcolor{lightblue}{83.24}} & 
         \textbf{\textcolor{lightblue}{35.00}} / 78.03 & 
         \textbf{\textcolor{lightblue}{70.32}} / 75.85 & 
         58.35 / \textbf{\textcolor{lightblue}{92.66}} & 
         \textbf{\textcolor{lightblue}{91.33}} / 83.83 \\
    & full-set   & 
         35.74 / 85.16 & 
         37.61 / 78.14 & 
         73.40 / \textbf{\textcolor{lightred}{76.06}} & 
         62.71 / 94.04 & 
         94.04 / 84.15 \\
    & anti-set   & 
         35.21 / \textbf{\textcolor{lightred}{86.44}} & 
         38.94 / \textbf{\textcolor{lightred}{80.69}} & 
         79.95 / 74.04 & 
         \textbf{\textcolor{lightblue}{55.64}} / \textbf{\textcolor{lightred}{94.95}} & 
         96.81 / \textbf{\textcolor{lightred}{86.65}} \\
\midrule
\multirow{4}{*}{Religion} 
    & w/o RAG    & 
         \textbf{\textcolor{lightred}{49.08}} / \textbf{\textcolor{lightred}{80.17}} & 
         \textbf{\textcolor{lightred}{60.67}} / \textbf{\textcolor{lightblue}{74.25}} & 
         \textbf{\textcolor{lightred}{84.58}} / \textbf{\textcolor{lightblue}{64.25}} & 
         \textbf{\textcolor{lightred}{67.75}} / \textbf{\textcolor{lightblue}{83.67}} & 
         \textbf{\textcolor{lightred}{90.33}} / 69.42 \\
    & stereo-set & 
         37.92 / \textbf{\textcolor{lightblue}{77.08}} & 
         38.67 / 75.42 & 
         71.67 / 68.92 & 
         53.05 / 87.67 & 
         87.25 / \textbf{\textcolor{lightblue}{68.5}} \\
    & full-set   & 
         35.67 / 78.67 & 
         35.50 / 74.25 & 
         \textbf{\textcolor{lightblue}{66.5}} / 70.67 & 
         51.50 / 86.25 & 
         86.75 / 69.92 \\
    & anti-set   & 
         \textbf{\textcolor{lightblue}{30.50}} / 78.58 & 
         \textbf{\textcolor{lightblue}{32.92}} / \textbf{\textcolor{lightred}{76.25}} & 
         67.17 / \textbf{\textcolor{lightred}{71.75}} & 
         \textbf{\textcolor{lightblue}{47.67}} / \textbf{\textcolor{lightred}{87.92}} & 
         \textbf{\textcolor{lightblue}{84.42}} / \textbf{\textcolor{lightred}{72.17}} \\
\bottomrule
\end{tabular}
}
\caption{Accuracy scores for the ambiguous and disambiguated contexts (separated by `/') for different bias categories and models, when document collections with varying degrees of social biases are used for retrieval. In each sub-category (Gender, Age, Race, Religion), the scores for each model are compared vertically. For each model and bias category, the maximum value in the ambiguous (left) and disambiguated (right) scores is highlighted in light red bold, while the minimum is highlighted in light blue bold.}
\label{tbl:bias-type:accuracy}
\end{table*}


% Accuracy Table on the gender bias generator LLMs
\begin{table*}[t]
\small
\centering
%\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{w/o RAG} & \textbf{stereo-set} & \textbf{full-set} & \textbf{anti-set}  \\
\midrule

    GPT-3.5   
         & \textbf{\textcolor{lightred}{45.24}} / \textbf{\textcolor{lightred}{75.74}} 
         & 27.83 / \textbf{\textcolor{lightblue}{71.68}} 
         & 27.58 / 73.12 
         & \textbf{\textcolor{lightblue}{22.07}} / 72.97  \\
    Llama3-8B          
         & \textbf{\textcolor{lightred}{25.94}} / \textbf{\textcolor{lightblue}{41.96}} 
         & 21.03 / 47.97 
         & 21.43 / 47.82 
         & \textbf{\textcolor{lightblue}{20.78}} / \textbf{\textcolor{lightred}{49.40}}   \\
    Llama3-8B-Inst.     
         & \textbf{\textcolor{lightred}{50.30}} / \textbf{\textcolor{lightblue}{60.52}} 
         & 26.19 / 63.74 
         & 24.36 / 63.89 
         & \textbf{\textcolor{lightblue}{23.66}} / \textbf{\textcolor{lightred}{66.07}}   \\
    Mistral         
         & \textbf{\textcolor{lightblue}{16.12}} / \textbf{\textcolor{lightred}{54.02}} 
         & \textbf{\textcolor{lightred}{19.69}} / 50.84 
         & 17.71 / \textbf{\textcolor{lightblue}{49.45}} 
         & 18.40 / 49.85   \\
    Mistral-Inst.    
         & \textbf{\textcolor{lightred}{66.91}} / \textbf{\textcolor{lightblue}{67.26}} 
         & 45.49 / 69.25 
         & 47.22 / 71.38 
         & \textbf{\textcolor{lightblue}{42.11}} / \textbf{\textcolor{lightred}{71.88}}   \\
\midrule

    Llm-jp-1.8B 
         & \textbf{\textcolor{lightblue}{7.04}} / \textbf{\textcolor{lightred}{48.21}} 
         & 16.96 / \textbf{\textcolor{lightblue}{43.75}} 
         & \textbf{\textcolor{lightred}{18.06}} / 44.25 
         & 16.27 / 45.85   \\
    Llm-jp-3.7B 
         & \textbf{\textcolor{lightblue}{10.52}} / \textbf{\textcolor{lightred}{51.49}} 
         & 18.65 / 47.52 
         & \textbf{\textcolor{lightred}{19.35}} / 46.23 
         & 16.27 / \textbf{\textcolor{lightblue}{45.14}}   \\
 Llm-jp-13B  
         & \textbf{\textcolor{lightred}{10.62}} / \textbf{\textcolor{lightred}{82.74}} 
         & \textbf{\textcolor{lightblue}{6.75}} / 78.27 
         & 7.14 / 78.27 
         & \textbf{\textcolor{lightblue}{6.75}} / \textbf{\textcolor{lightblue}{77.78}}   \\
\midrule

    Qwen-7B            
         & \textbf{\textcolor{lightred}{30.65}} / \textbf{\textcolor{lightblue}{67.31}} 
         & 26.49 / 68.40 
         & 26.74 / \textbf{\textcolor{lightred}{69.54}} 
         & \textbf{\textcolor{lightblue}{24.31}} / 69.05   \\
    Qwen-7B-Inst.       
         & \textbf{\textcolor{lightred}{81.45}} / \textbf{\textcolor{lightblue}{52.03}} 
         & 62.60 / 53.72 
         & 62.95 / 56.10 
         & \textbf{\textcolor{lightblue}{58.09}} / \textbf{\textcolor{lightred}{58.09}}   \\
    Qwen-3B         
         & \textbf{\textcolor{lightred}{8.23}} / \textbf{\textcolor{lightred}{78.97}} 
         & 4.12 / 76.49 
         & 3.22 / \textbf{\textcolor{lightblue}{76.24}} 
         & \textbf{\textcolor{lightblue}{2.88}} / 77.83   \\
    Qwen-3B-Inst.    
         & \textbf{\textcolor{lightred}{68.20}} / 58.43 
         & 57.19 / \textbf{\textcolor{lightblue}{57.14}} 
         & \textbf{\textcolor{lightblue}{52.28}} / \textbf{\textcolor{lightred}{63.10}} 
         & 54.37 / 59.42   \\
    Qwen-14B       
         & \textbf{\textcolor{lightred}{63.79}} / \textbf{\textcolor{lightred}{82.84}} 
         & 46.83 / 77.63 
         & 49.75 / \textbf{\textcolor{lightblue}{77.08}} 
         & \textbf{\textcolor{lightblue}{41.02}} / 78.57   \\
    Qwen-14B-Inst.   
         & \textbf{\textcolor{lightred}{96.53}} / \textbf{\textcolor{lightred}{71.92}} 
         & 87.05 / 68.30 
         & 89.78 / \textbf{\textcolor{lightblue}{67.76}} 
         & \textbf{\textcolor{lightblue}{83.63}} / 68.90   \\
\bottomrule
\end{tabular}
%}
\caption{Comparison of accuracy scores across different corpus settings on the BBQ gender dataset. Scores are reported in the format \textit{ambiguous / disambiguous}, where higher values indicate better performance. For each model, the maximum ambiguous and disambiguous scores are highlighted in light red bold, while the minimum values are highlighted in light blue bold.}
\label{tbl:generators:accuracy}
\end{table*}


\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|cccc||cccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c|}{\textbf{CBBQ}} & \multicolumn{4}{c}{\textbf{JBBQ }} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
 & \textbf{w/o RAG} & \textbf{stereo-set} & \textbf{full-set} & \textbf{anti-set} & \textbf{w/o RAG} & \textbf{stereo-set} & \textbf{full-set} & \textbf{anti-set} \\
\midrule
GPT-3.5 & 
  \textbf{\textcolor{lightred}{26.52}} / \textbf{\textcolor{lightblue}{64.30}} & 13.31 / 67.08 & 16.77 / 67.28 &  \textbf{\textcolor{lightblue}{12.45}} / \textbf{\textcolor{lightred}{68.42}}  & 
  \textbf{\textcolor{lightred}{30.52}} / \textbf{\textcolor{lightblue}{52.68}} & \textbf{\textcolor{lightblue}{23.31}} / 55.93 & 27.40 / 56.08 & 24.26 / \textbf{\textcolor{lightred}{56.29}} \\
  
Qwen-7B-Inst. & 
  \textbf{\textcolor{lightred}{90.48}} / \textbf{\textcolor{lightblue}{45.88}}  & 37.55 / 64.09 & 43.40 / 62.04 & \textbf{\textcolor{lightblue}{35.50}} / \textbf{\textcolor{lightred}{64.30}}  & 
  \textbf{\textcolor{lightred}{77.56}} / \textbf{\textcolor{lightblue}{53.66}} & 48.29 / 57.54 & 50.08 / \textbf{\textcolor{lightred}{58.21}} & \textbf{\textcolor{lightblue}{45.35}} / 58.00 \\
  
Qwen-14B & 
 \textbf{\textcolor{lightred}{72.62}} / \textbf{\textcolor{lightblue}{57.30}} & \textbf{\textcolor{lightblue}{42.42}}/ 60.29 & 49.46 / 61.11 & 44.05 / \textbf{\textcolor{lightred}{61.52}}  & 
  \textbf{\textcolor{lightred}{42.56}} / \textbf{\textcolor{lightred}{77.89}} & 28.71 / \textbf{\textcolor{lightblue}{73.90}} & 31.06 / 75.23 & \textbf{\textcolor{lightblue}{25.95}} / 77.28 \\
  
Qwen-14B-Inst. & 
  \textbf{\textcolor{lightred}{96.32}} / \textbf{\textcolor{lightblue}{40.84}}  & 76.73 / 45.78 & 84.52 / \textbf{\textcolor{lightred}{48.97}}  & \textbf{\textcolor{lightblue}{75.97}} / 46.09 & 
  \textbf{\textcolor{lightred}{82.31}} / 78.35 & 61.84 / 77.86 & 67.15 / \textbf{\textcolor{lightblue}{78.20}} & \textbf{\textcolor{lightblue}{61.61}} /\textbf{\textcolor{lightred}{80.52}} \\
\bottomrule
\end{tabular}
}
\caption{Accuracy scores for Chinese (CBBQ) and Japanese (JBBQ) datasets. Accuracy values are reported in the format \textit{ambiguous / disambiguated}, where higher values indicate better performance. For each model, the maximum ambiguous and disambiguated scores are highlighted in light red bold, while the minimum values are highlighted in light blue bold.}
\label{tbl:multilingual:accuracy}
\end{table*}


\begin{table*}[t]
\small
\centering
%\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccc}
\toprule
\textbf{Model} & \textbf{w/o RAG} & \textbf{VectorIndex} & \textbf{BM25} & \textbf{Contriever} \\
\midrule
GPT-3.5  & 
  \textbf{\textcolor{lightred}{45.24}} / \textbf{\textcolor{lightred}{75.74}} & 
  \textbf{\textcolor{lightblue}{27.58}} / 73.12 & 
  39.93 / 74.21 & 
  31.80 / \textbf{\textcolor{lightblue}{73.07}} \\
Llama3-8B-Inst. & 
  \textbf{\textcolor{lightred}{50.30}} / \textbf{\textcolor{lightblue}{60.52}} & 
  \textbf{\textcolor{lightblue}{24.36}} / 63.89 & 
  31.99 / \textbf{\textcolor{lightred}{65.82}} & 
  28.03 / 64.53 \\
Qwen-7B-Inst. & 
  \textbf{\textcolor{lightred}{81.45}} / \textbf{\textcolor{lightblue}{52.03}} & 
  15.43 / -2.08 & 
  16.27 / -1.39 & 
  15.87 / \textbf{\textcolor{lightred}{-0.10}} \\
Qwen-14B  & 
  \textbf{\textcolor{lightred}{63.79}} / 82.84 & 
  49.75 / \textbf{\textcolor{lightblue}{77.08}} & 
  \textbf{\textcolor{lightblue}{45.88}} / \textbf{\textcolor{lightred}{84.52}} & 
  47.57 / 78.27 \\
Qwen-14B-Inst. & 
  \textbf{\textcolor{lightred}{96.53}} / 71.92 & 
  89.78 / \textbf{\textcolor{lightblue}{67.76}} & 
  92.66 / \textbf{\textcolor{lightred}{73.12}} & 
  \textbf{\textcolor{lightblue}{87.85}} / 71.33 \\
\bottomrule
\end{tabular}
%}
\caption{Comparison of  \textit{ambiguous / disambiguous} accuracy (separated by ‘/’)  when using different retrieval methods to retrieving documents from the \textbf{full-set}. For each generator LLM, maximum and minimum accuracy are shown respectively in red and blue.}
\label{tbl:retrievers:full-set:accuracy}
\end{table*}


\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/accuracy_ambiguous.pdf}
  \caption{Accuracy for \textbf{ambiguous} questions for different numbers of retrieved documents.}
  \label{fig:ambig_nums_retrieved:accuracy}
\end{figure*}
% Diff-bias, number of contexts, disambi.
\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/accuracy_disambiguous.pdf}
  \caption{Accuracy scores for \textbf{disambiguated} questions for different numbers of retrieved documents.}
  \label{fig:disambig_nums_retrieved:accuracy}
\end{figure*}


% \begin{table}[t]
% \centering
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccc}
% \toprule
% \textbf{Model} & \textbf{w/o RAG} & \textbf{stereo-set} & \textbf{full-set} & \textbf{anti-set} \\
% \midrule
% GPT-3.5-turbo  & 26.52 / 64.30   & 13.31 / 67.08   & 16.77 / 67.28   & 12.45 / 68.42  \\
% Qwen2.5-7B-Instruct   & 90.48 / 45.88   & 37.55 / 64.09   & 43.40 / 62.04   & 35.50 / 64.30  \\
% Qwen2.5-14B       & 72.62 / 57.30   & 42.42 / 60.29   & 49.46 / 61.11   & 44.05 / 61.52  \\
% Qwen2.5-14B-Instruct & 96.32 / 40.84   & 76.73 / 45.78   & 84.52 / 48.97   & 75.97 / 46.09  \\
% \bottomrule
% \end{tabular}}
% \caption{Evaluating social biases for Chinese on the CBBQ dataset. Accuracy values are shown for the ambiguous and disambiguated contexts (separated by `/'). }
% \label{tbl:CBBQ:Accuracy}
% \end{table}

% \begin{table}[!t]
% \centering
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccc}
% \toprule
% \textbf{Model} & \textbf{w/o Retriever} & \textbf{stereo-set } & \textbf{full-set} & \textbf{anti-set} \\
% \midrule
% GPT-3.5-turbo  & 
%   \textbf{\textcolor{lightred}{30.52}} / \textbf{\textcolor{lightblue}{52.68}}   & 
%   \textbf{\textcolor{lightblue}{23.31}} / 55.93   & 
%   27.40 / 56.08   & 
%   24.26 / \textbf{\textcolor{lightred}{56.29}}  \\
% Qwen2.5-7B-Instruct   & 
%   \textbf{\textcolor{lightred}{77.56}} / \textbf{\textcolor{lightblue}{53.66}}   & 
%   48.29 / 57.54   & 
%   50.08 / \textbf{\textcolor{lightred}{58.21}}   & 
%   \textbf{\textcolor{lightblue}{45.35}} / 58.00  \\
% Qwen2.5-14B       & 
%   \textbf{\textcolor{lightred}{42.56}} / \textbf{\textcolor{lightred}{77.89}}   & 
%   28.71 / \textbf{\textcolor{lightblue}{73.90}}   & 
%   31.06 / 75.23   & 
%   \textbf{\textcolor{lightblue}{25.95}} / 77.28  \\
% Qwen2.5-14B-Instruct & 
%   \textbf{\textcolor{lightred}{96.73}} / \textbf{\textcolor{lightred}{71.63}}   & 
%   \textbf{\textcolor{lightblue}{93.75}} / 68.25   & 
%   96.38 /  \textbf{\textcolor{lightblue}{67.11}}   & 
%   93.94 / 67.31  \\
% \bottomrule
% \end{tabular}}
% \caption{Comparison of accuracy scores across different corpus settings on the JBBQ dataset. Scores are reported in the format \textit{ambiguous / disambiguous}, where higher values indicate better performance. For each model, the maximum ambiguous and disambiguous scores are highlighted in light red bold, while the minimum values are highlighted in light blue bold.}
% \label{tab:accuracy_bbq_japanese_corpus}
% \end{table}

\iffalse
\begin{table*}[t]
\centering
\small
%\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccc}
\toprule
\textbf{Model} & \textbf{w/o RAG} & \textbf{VectorIndex} & \textbf{BM25} & \textbf{Contriever} \\
\midrule
GPT-3.5  & 
  \textbf{\textcolor{lightred}{45.24}} / \textbf{\textcolor{lightred}{75.74}} & 
  \textbf{\textcolor{lightblue}{27.83}} / 71.68 & 
  39.78 / 74.50 & 
  30.70 / \textbf{\textcolor{lightblue}{71.63}} \\
Llama3-8B-Inst. & 
  \textbf{\textcolor{lightred}{50.30}} / 60.52 & 
  \textbf{\textcolor{lightblue}{26.19}} / 63.74 & 
  31.15 / \textbf{\textcolor{lightred}{65.87}} & 
  31.35 / 63.74 \\
Qwen-7B-Inst. & 
  \textbf{\textcolor{lightred}{81.45}} / \textbf{\textcolor{lightblue}{52.03}} & 
  62.60 / 53.72 & 
  18.30 / \textbf{\textcolor{lightred}{56.94}} & 
  22.62 / 53.57 \\
Qwen-14B  & 
  \textbf{\textcolor{lightred}{63.79}} / 82.84 & 
  46.83 / \textbf{\textcolor{lightblue}{77.63}} & 
  \textbf{\textcolor{lightblue}{44.84}} / \textbf{\textcolor{lightred}{84.52}} & 
  46.28 / 78.37 \\
Qwen-14B-Inst. & 
  \textbf{\textcolor{lightred}{96.53}} / 71.92 & 
  87.05 / \textbf{\textcolor{lightblue}{68.30}} & 
  90.58 / 70.73 & 
  86.26 / 69.54 \\
\bottomrule
\end{tabular}
%}
\caption{Comparison of accuracy scores across different retrievers in RAG when retrieving documents from the stereo-set collection. Scores are reported in the format \textit{ambiguous / disambiguous}, where higher values indicate better performance. In each row, the highest numerical value is highlighted in light red bold, and the lowest value is highlighted in light blue bold.}
\label{tab:accuracy_retrievers_bias_corpus}
\end{table*}
\fi


% \begin{table*}[t]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Model} & \textbf{w/o RAG} & \textbf{3 Contexts} & \textbf{5 Contexts} & \textbf{10 Contexts} & \textbf{20 Contexts} & \textbf{30 Contexts} \\
% \midrule
% Llama3-8B-Inst. & 
%   \textbf{\textcolor{lightred}{5.65}} / 1.59 & 
%   1.19 / -4.17 & 
%   2.38 / -4.76 & 
%   0.74 / \textbf{\textcolor{lightblue}{-6.85}} & 
%   \textbf{\textcolor{lightblue}{0.10}} / -3.97 & 
%   2.08 / \textbf{\textcolor{lightred}{-1.39}} \\
% Qwen-7B-Inst. &
%   10.02 / -3.67 & 
%   \textbf{\textcolor{lightred}{12.55}} / \textbf{\textcolor{lightred}{-2.68}} & 
%   9.92 / -6.94 & 
%   10.17 / \textbf{\textcolor{lightblue}{-10.12}} & 
%   \textbf{\textcolor{lightblue}{7.34}} / -7.14 & 
%   11.31 / -6.65 \\
% Qwen-14B &
%   3.77 / -7.34 & 
%   \textbf{\textcolor{lightred}{6.35}} / \textbf{\textcolor{lightred}{-4.37}} & 
%   0.79 / -7.84 & 
%   -4.51 / -8.93 & 
%   \textbf{\textcolor{lightblue}{-7.79}} / \textbf{\textcolor{lightblue}{-12.40}} & 
%   \textbf{\textcolor{lightblue}{-9.62}} / -9.23 \\
% Qwen-14B-Inst. &
%   -2.38 / \textbf{\textcolor{lightred}{-2.38}} & 
%   \textbf{\textcolor{lightred}{1.69}} / -13.00 & 
%   -5.46 / -11.21 & 
%   -8.43 / -12.70 & 
%   \textbf{\textcolor{lightblue}{-9.72}} / -14.48 & 
%   \textbf{\textcolor{lightblue}{-9.72}} / \textbf{\textcolor{lightblue}{-15.38}} \\
% \bottomrule
% \end{tabular}}
% \caption{Diff-Bias scores for ambiguous and disambiguous questions with Anti-Bias corpus across different numbers of retrieved contexts.  
% Each cell contains scores in the format: ambiguous / disambiguous.  
% Lower scores (closer to 0) indicate less bias.  
% In each row, the highest numerical value is highlighted in light red bold, and the lowest value is highlighted in light blue bold.}
% \label{tab:diff_bias_anti_bias}
% \end{table*}



% \begin{table*}[t]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Model} & \textbf{w/o RAG} & \textbf{3 Contexts} & \textbf{5 Contexts} & \textbf{10 Contexts} & \textbf{20 Contexts} & \textbf{30 Contexts} \\
% \midrule
% Llama3-8B-Inst. & 
%   \textbf{\textcolor{lightred}{50.30}} / \textbf{\textcolor{lightblue}{60.52}} & 
%   27.68 / 64.38 & 
%   24.31 / 63.14 & 
%   \textbf{\textcolor{lightblue}{23.66}} / \textbf{\textcolor{lightred}{66.07}} & 
%   27.98 / 65.82 & 
%   27.88 / 65.67 \\
% Qwen-7B-Inst. &
%   \textbf{\textcolor{lightred}{81.45}} / \textbf{\textcolor{lightblue}{52.03}} & 
%   62.75 / 55.21 & 
%   \textbf{\textcolor{lightblue}{55.85}} / 56.45 & 
%   58.09 / \textbf{\textcolor{lightred}{58.09}} & 
%   62.00 / 57.56 & 
%   57.44 / 57.94 \\
% Qwen-14B &
%   \textbf{\textcolor{lightred}{63.79}} / \textbf{\textcolor{lightred}{82.84}} & 
%   43.15 / 80.16 & 
%   39.78 / 79.76 & 
%   41.02 / 78.57 & 
%   \textbf{\textcolor{lightblue}{37.35}} / \textbf{\textcolor{lightblue}{78.47}} & 
%   \textbf{\textcolor{lightblue}{35.02}} / 79.32 \\
% Qwen-14B-Inst. &
%   \textbf{\textcolor{lightred}{96.53}} / \textbf{\textcolor{lightred}{71.92}} & 
%   87.70 / \textbf{\textcolor{lightblue}{66.27}} & 
%   85.52 / 68.35 & 
%   83.63 / 68.90 & 
%   \textbf{\textcolor{lightblue}{82.94}} / 67.71 & 
%   83.63 / 69.39 \\
% \bottomrule
% \end{tabular}}
% \caption{Accuracy scores for ambiguous and disambiguous questions with Anti-Bias corpus across different numbers of retrieved contexts. Each cell contains scores in the format: ambiguous / disambiguous. Higher scores indicate better performance. In each row, the highest numerical value is highlighted in light red bold, and the lowest value is highlighted in light blue bold.}
% \label{tab:accuracy_anti_bias}
% \end{table*}




% \begin{table*}[t]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Model} & \textbf{w/o RAG} & \textbf{3 Contexts} & \textbf{5 Contexts} & \textbf{10 Contexts} & \textbf{20 Contexts} & \textbf{30 Contexts} \\
% \midrule
% Llama3-8B-Inst. & 
%   5.65 / \textbf{\textcolor{lightred}{1.59}} & % ambiguous: 5.65, 5.90, 5.31, 6.80, 4.22, 4.27 → highest=6.80, lowest=4.22
%   5.90 / -4.37 & 
%   5.31 / -4.46 & 
%   \textbf{\textcolor{lightred}{6.80}} / -3.97 & 
%   \textbf{\textcolor{lightblue}{4.22}} / -4.76 & 
%   4.27 / \textbf{\textcolor{lightblue}{-5.75}} \\
% Qwen-7B-Inst. &
%   \textbf{\textcolor{lightblue}{10.02}} / \textbf{\textcolor{lightblue}{-3.67}} & % ambiguous: 10.02, 17.86, 14.53, 15.43, 15.67, 15.58 → highest=17.86, lowest=10.02
%   \textbf{\textcolor{lightred}{17.86}} / \textbf{\textcolor{lightred}{1.29}} & % disambiguous: -3.67, 1.29, -2.48, -2.08, -2.98, -3.77 → highest=1.29, lowest=-3.77
%   14.53 / -2.48 & 
%   15.43 / -2.08 & 
%   15.67 / -2.98 & 
%   15.58 / \textbf{\textcolor{lightblue}{-3.77}} \\
% Qwen-14B &
%   3.77 / \textbf{\textcolor{lightblue}{-7.34}} & % ambiguous: 3.77, 7.59, 6.05, 0.55, -0.15, 0.45 → highest=7.59, lowest=-0.15
%   \textbf{\textcolor{lightred}{7.59}} / -4.37 & 
%   6.05 / -4.86 & 
%   0.55 / -4.66 & 
%   \textbf{\textcolor{lightblue}{-0.15}} / -5.75 & 
%   0.45 / \textbf{\textcolor{lightred}{-3.67}} \\
% Qwen-14B-Inst. &
%   -2.38 / \textbf{\textcolor{lightred}{-2.38}} & % ambiguous: -2.38, -1.24, -0.74, -3.08, -5.06, -5.01 → highest=-0.74, lowest=-5.06
%   -1.24 / -7.64 & 
%   \textbf{\textcolor{lightred}{-0.74}} / -6.65 & 
%   -3.08 / -6.45 & 
%   \textbf{\textcolor{lightblue}{-5.06}} / -5.95 & 
%   -5.01 / \textbf{\textcolor{lightblue}{-7.94}} \\
% \bottomrule
% \end{tabular}}
% \caption{Diff-Bias scores for ambiguous and disambiguous questions on the full corpus with different numbers of retrieved contexts.  
% Each cell contains scores in the format: ambiguous / disambiguous.  
% Lower scores (closer to 0) indicate less bias.  
% In each row, the highest numerical value is highlighted in light red bold, and the lowest value is highlighted in light blue bold.}
% \label{tab:diff_bias_full_corpus}
% \end{table*}


%% Accuracy scores for ambiguous and disambiguous questions across different numbers of retrieved contexts from full corpus
% \begin{table*}[t]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Model} & \textbf{w/o RAG} & \textbf{3 Contexts} & \textbf{5 Contexts} & \textbf{10 Contexts} & \textbf{20 Contexts} & \textbf{30 Contexts} \\
% \midrule
% Llama3-8B-Inst. & 
%   \textbf{\textcolor{lightred}{50.30}} / \textbf{\textcolor{lightblue}{60.52}} & % ambiguous: 50.30(red) highest, disambiguous: 60.52(blue) lowest
%   35.47 / 62.70 & 
%   27.93 / 63.74 & 
%   24.36 / \textbf{\textcolor{lightred}{63.89}} & % disambiguous highest
%   24.26 / 63.14 & 
%   \textbf{\textcolor{lightblue}{20.14}} / 62.35 \\ % ambiguous lowest
% Qwen-7B-Inst. &
%   \textbf{\textcolor{lightred}{81.45}} / \textbf{\textcolor{lightblue}{52.03}} & % ambiguous highest, disambiguous lowest
%   65.58 / 55.90 & 
%   62.95 / 56.40 & 
%   62.95 / 56.10 & 
%   60.81 / \textbf{\textcolor{lightred}{58.18}} & % disambiguous highest
%   \textbf{\textcolor{lightblue}{60.22}} / 58.04 \\ % ambiguous lowest
% Qwen-14B &
%   \textbf{\textcolor{lightred}{63.79}} / \textbf{\textcolor{lightred}{82.84}} & % ambiguous highest, disambiguous highest
%   56.20 / 78.27 & 
%   53.27 / 79.12 & 
%   49.75 / \textbf{\textcolor{lightblue}{77.08}} & % disambiguous lowest
%   44.00 / 79.41 & 
%   \textbf{\textcolor{lightblue}{40.43}} / 78.92 \\ % ambiguous lowest
% Qwen-14B-Inst. &
%   \textbf{\textcolor{lightred}{96.53}} / \textbf{\textcolor{lightred}{71.92}} & % ambiguous highest, disambiguous highest
%   93.01 / \textbf{\textcolor{lightblue}{65.92}} & % disambiguous lowest
%   92.91 / 67.81 & 
%   89.78 / 69.00 & 
%   88.59 / 67.76 & 
%   \textbf{\textcolor{lightblue}{88.44}} / 67.16 \\ % ambiguous lowest
% \bottomrule
% \end{tabular}}
% \caption{Accuracy scores for ambiguous and disambiguous questions on the full corpus with different numbers of retrieved contexts. Each cell contains scores in the format: ambiguous / disambiguous. Higher scores indicate better performance. In each row, the highest numerical value is highlighted in light red bold, and the lowest value is highlighted in light blue bold.}
% \label{tab:accuracy_full_corpus}
% \end{table*}

%% Accuracy scores for ambiguous and disambiguous questions across different numbers of retrieved contexts from bias corpus
% \begin{table*}[t]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Model} & \textbf{w/o RAG} & \textbf{3 Contexts} & \textbf{5 Contexts} & \textbf{10 Contexts} & \textbf{20 Contexts} & \textbf{30 Contexts} \\
% \midrule
% Llama3-8B-Inst. & 
%   \textbf{\textcolor{lightred}{50.30}} / \textbf{\textcolor{lightblue}{60.52}} &
%   35.12 / \textbf{\textcolor{lightred}{63.89}} &                   
%   29.81 / 62.15 & 
%   26.19 / 63.74 & 
%   27.08 / 63.24 & 
%   \textbf{\textcolor{lightblue}{21.53}} / 63.69 \\                
% Qwen-7B-Inst. &
%   \textbf{\textcolor{lightred}{81.45}} / \textbf{\textcolor{lightblue}{52.03}} &
%   68.25 / 54.56 & 
%   67.61 / 53.27 & 
%   62.60 / 53.72 & 
%   62.60 / 54.22 & 
%   \textbf{\textcolor{lightblue}{61.01}} / \textbf{\textcolor{lightred}{55.31}} \\ 
% Qwen-14B &
%   \textbf{\textcolor{lightred}{63.79}} / \textbf{\textcolor{lightred}{82.84}} & 
%   58.63 / \textbf{\textcolor{lightblue}{77.58}} &                 
%   51.84 / 77.73 & 
%   46.83 / 77.63 & 
%   41.67 / 79.86 & 
%   \textbf{\textcolor{lightblue}{40.18}} / 80.56 \\               
% Qwen-14B-Inst. &
%   \textbf{\textcolor{lightred}{96.53}} / \textbf{\textcolor{lightred}{71.92}} & 
%   91.57 / \textbf{\textcolor{lightblue}{66.91}} &                 
%   88.32 / 67.01 & 
%   \textbf{\textcolor{lightblue}{87.05}} / 68.30 & 
%   87.35 / 67.21 & 
%   88.94 / 67.51 \\
% \bottomrule
% \end{tabular}}
% \caption{Accuracy scores for ambiguous and disambiguous questions across different numbers of retrieved contexts from the bias corpus. Each cell contains scores in the format: ambiguous / disambiguous. Higher scores indicate better performance. In each row, the highest value (numerically) is highlighted in light red bold, and the lowest value is highlighted in light blue bold.}
% \label{tab:accuracy_combined_contexts}
% \end{table*}

\iffalse



% Bias corpus
%------------------------------------------------
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccc}
\toprule
\textbf{Model} & \textbf{w/o RAG} & \textbf{VectorIndex} & \textbf{BM25} & \textbf{Contriever}\\
\midrule
GPT-3.5  & 
  \textbf{\textcolor{lightred}{45.24}} / \textbf{\textcolor{lightred}{75.74}} & 
  14.53 / \textbf{\textcolor{lightred}{7.14}} & 
  15.28 / \textbf{\textcolor{lightblue}{-3.87}} & 
  \textbf{\textcolor{lightblue}{14.14}} / 1.59\\[1mm]
Llama3-8B-Inst.    & 
  \textbf{\textcolor{lightblue}{5.65}} / \textbf{\textcolor{lightred}{1.59}} & 
  14.68 / -0.40 & 
  13.79 / \textbf{\textcolor{lightred}{-0.20}} & 
  \textbf{\textcolor{lightred}{14.78}} / \textbf{\textcolor{lightblue}{-1.49}}\\[1mm]
Qwen-7B-Inst.   & 
  \textbf{\textcolor{lightblue}{10.02}} / \textbf{\textcolor{lightblue}{-3.67}} & 
  \textbf{\textcolor{lightred}{24.01}} / 0.50 & 
  18.30 / -2.78 & 
  22.62 / \textbf{\textcolor{lightred}{1.98}}\\[1mm]
Qwen-14B    & 
  \textbf{\textcolor{lightblue}{3.77}} / \textbf{\textcolor{lightblue}{-7.34}} & 
  13.99 / \textbf{\textcolor{lightred}{-2.68}} & 
  \textbf{\textcolor{lightred}{15.77}} / \textbf{\textcolor{lightblue}{-4.56}} & 
  10.47 / -6.15\\[1mm]
Qwen-14B-Inst.    & 
  \textbf{\textcolor{lightblue}{-2.38}} / \textbf{\textcolor{lightblue}{-2.38}} & 
  4.61 / \textbf{\textcolor{lightred}{2.68}} & 
  \textbf{\textcolor{lightred}{4.66}} / \textbf{\textcolor{lightblue}{-2.48}} & 
  1.44 / -0.60\\
\bottomrule
\end{tabular}}
\caption{Comparison of Diff-Bias scores across different retrievers in RAG when retrieving bias-related corpus.  
Scores are reported in the format \textit{ambiguous / disambiguous}, where values closer to zero indicate lower bias.  
In each row, the highest numerical value is highlighted in light red bold, and the lowest value is highlighted in light blue bold.}
\label{tab:diff_bias_retrievers_bias_corpus}
\end{table}
\fi

\section{Prompt Sensitivity}
\label{sec:app:ICL}

% 1. Ordering 2. Few-shot 
We test the sensitivity of the prompt that we use to evaluate social biases using BBQ in this section.
Specifically, we check 
 the sensitivity of the prompt when using few-shot examples in the prompt. 

\iffalse
\subsection{Ordering between Question and Documents}
\label{sec:ordering}

Recent work by~\citet{liu2024lost} suggests that language models do not robustly utilize information in long contexts, and performance is often improved when the most relevant information is presented at the beginning of the input. 
In our RAG experiments, we retrieve the top 10 documents, which form a long context relative to the comparatively short BBQ questions.
Therefore, we evaluate the effect of ordering documents-before-question (\textbf{Documents-First}) and question-before-documents (\textbf{Question-First}) in the prompt shown in \autoref{fig:bias_template}.
Specifically, we use the \textbf{stereo-set} as the document collection and evaluate social biases on the English BBQ dataset.

As shown in~\autoref{tab:diff_bias_ordering_combined} and ~\autoref{tab:accuracy_ordering_combined}, ordering \textbf{Question-First} in the prompt effectively reduces social bias and, in many cases.
For example, for GPT-3.5-turbo the Diff-Bias scores for the ambiguous cases drops from 14.53 in the \textbf{Document-First} setting to 8.58 in the \textbf{Question-First} setting, while the ambiguous accuracy increases from 27.83 to 46.78. 
Similar trends are observed across other models, where the \textbf{Question-First} setting consistently achieves lower Diff-Bias scores—often nearly halving the bias observed compared to the \textbf{Documents-First}. 
% Following this result, we use \textbf{Documents-First} approach (as already shown in the prompt in \autoref{fig:bias_template}) for all evaluations reported in this paper.
% Therefore, all social biases reported in the paper could be seen as conservative lower bounds and the actually social biases could be even higher under a different ordering strategy.



% Diff-bias
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|ccc}
\toprule
\textbf{Model} & \textbf{w/o RAG} & \textbf{Document-First} & \textbf{Question-First} \\
\midrule
GPT-3.5  & \textbf{\textcolor{lightblue}{5.16}} / \textbf{\textcolor{lightblue}{-9.33}}  
               & \textbf{\textcolor{lightred}{14.53}} / \textbf{\textcolor{lightred}{7.14}}  
               & 8.58 / -3.27  \\
Llama3-8B-Inst.    
               & \textbf{\textcolor{lightblue}{5.65}} / 1.59    
               & \textbf{\textcolor{lightred}{14.68}} / \textbf{\textcolor{lightblue}{-0.40}}  
               & 8.04 / \textbf{\textcolor{lightred}{2.38}}   \\

Qwen7B-Inst.   
               & \textbf{\textcolor{lightblue}{10.02}} / \textbf{\textcolor{lightblue}{-3.67}}  
               & \textbf{\textcolor{lightred}{24.01}} / 0.50   
               & 13.20 / \textbf{\textcolor{lightred}{2.08}}  \\

Qwen14B    
               & \textbf{\textcolor{lightblue}{3.77}} / \textbf{\textcolor{lightblue}{-7.34}}  
               & \textbf{\textcolor{lightred}{13.99}} / \textbf{\textcolor{lightred}{-2.68}}   
               & 5.95 / -5.75  \\
Qwen14B-Inst.    
               & \textbf{\textcolor{lightblue}{-2.38}} / \textbf{\textcolor{lightblue}{-2.38}}  
               & \textbf{\textcolor{lightred}{4.61}} / \textbf{\textcolor{lightred}{2.68}}   
               & 0.20 / \textbf{\textcolor{lightblue}{-1.90}}  \\
\bottomrule
\end{tabular}}
\caption{Impact of changing the ordering of question and context in RAG on Diff-Bias (lower means better). Each cell contains scores in the format: ambiguous / disambiguous. For each model, the maximum ambiguous and disambiguous scores are highlighted in light red bold, while the minimum values are highlighted in light blue bold.}
\label{tab:diff_bias_ordering_combined}
\end{table}


% Accuracy
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|ccc}
\toprule
\textbf{Model} & \textbf{w/o RAG} & \textbf{Document-First} & \textbf{Question-First} \\
\midrule
GPT-3.5  & 
  45.24 / \textbf{\textcolor{lightred}{75.74}}  
  & \textbf{\textcolor{lightblue}{27.83}} / 71.68  
  & \textbf{\textcolor{lightred}{46.78}} / \textbf{\textcolor{lightblue}{65.53}}  \\

Llama3-8B-Inst.    & 
  \textbf{\textcolor{lightred}{50.30}} / \textbf{\textcolor{lightblue}{60.52}}  
  & \textbf{\textcolor{lightblue}{26.19}} / 63.74  
  & 38.10 / \textbf{\textcolor{lightred}{65.92}}  \\

Qwen7B-Inst.   & 
  \textbf{\textcolor{lightred}{81.45}} / 52.03  
  & \textbf{\textcolor{lightblue}{62.60}} / \textbf{\textcolor{lightred}{53.72}}  
  & 78.27 / \textbf{\textcolor{lightblue}{50.05}}  \\

Qwen14B    & 
  \textbf{\textcolor{lightred}{63.79}} / 82.84  
  & \textbf{\textcolor{lightblue}{46.83}} / \textbf{\textcolor{lightblue}{77.63}}  
  & 58.83 / \textbf{\textcolor{lightred}{82.94}}  \\

Qwen14B-Inst.    & 
  \textbf{\textcolor{lightred}{96.53}} / \textbf{\textcolor{lightblue}{71.92}}  
  & \textbf{\textcolor{lightblue}{87.05}} / \textbf{\textcolor{lightblue}{68.30}}  
  & 92.96 / 69.05  \\
\bottomrule
\end{tabular}}
\caption{Impact of changing the ordering of question and context in RAG on Accuracy (higher means better). Each cell contains scores in the format: ambiguous / disambiguous. For each model, the maximum ambiguous and disambiguous scores are highlighted in light red bold, while the minimum values are highlighted in light blue bold.}
\label{tab:accuracy_ordering_combined}
\end{table}


\fi

Recent studies demonstrate that LLMs can exhibit robust few-shot performance on a variety of downstream tasks, where one or more examples are provided to guide the model in a specific text generation task. 
In our experiments, we randomly selected eight gender bias related instances from the English BBQ dataset as examples, including four ambiguous questions and four disambiguated questions (two with stereotyped contexts and two with counter-stereotyped contexts).
We include the selected few-shot examples at the beginning of the instruction prompt shown in \autoref{fig:bias_template}.

As shown in~\autoref{tbl:prompt:diff-bias}, incorporating few-shot examples does not always reduce the Diff-Bias scores compared to the zero-shot setting, neither in the \textbf{w/o RAG} condition nor when top-10 documents are retrieved (i.e. \textbf{w/ RAG}) from the stereo-set using VectorIndex.
For example, for GPT-3.5-turbo, the ambiguous Diff-Bias scores increase from 5.16 (\textbf{w/o RAG}, zero-shot) to 15.43 under the few-shot setting, and for the \textbf{w/ RAG}, from 14.53 to 25.20. 
Similar trends can be observed across other models. 

On the other hand, as indicated in~\autoref{tbl:prompt:accuracy}, few-shot prompting improves the accuracy, particularly for the ambiguous questions. 
For instance, GPT-3.5-turbo’s accuracy in ambiguous contexts w/o RAG rises from 45.24 in the zero-shot setting to 60.17 with few-shot. 
This suggests that although few-shot prompting can help the model to better understand the task, it does not significantly affect the social biases induced by the retrieved stereotype set or the model's inherent social biases.


% \subsection{Chain-of-Thought}
% \label{sec:cot}

% \citet{kaneko2024evaluating} showed that \ac{cot} prompting where they include \emph{let's think step by step} reduces social biases expressed by \acp{LLM}.
% They argue that the \ac{cot} prompting by making the model aware of the \ac{cot} method to mitigate gender bias in \acp{LLM} where they instruct an \ac{LLM} under evaluation to predict the gender of a set of words including female/male pronouns and gender-neutral occupational words (e.g. \emph{nurse}, \emph{doctor}, \emph{engineer} etc.).
% If the \ac{LLM} under evaluation counts gender neutral occupational words also towards a particular gender (only binary gender is considered in their evaluation benchmark), it is considered as gender biased.
% We use their inference prompt and evaluate the effect of \ac{RAG} on gender bias.

% As shown in~\autoref{tbl:prompt:diff-bias} and~\autoref{tbl:prompt:accuracy}, \ac{cot} improves both Accuracy and Diff-Bias scores in \textbf{w/o RAG}, indicating that \ac{cot} can reduce social biases. 
% It helps reducing the diff-bias score and increase the accuracy on most models. 

% However, it does not work when retrieve biased documents from stereo-set. 



% The experiment that change the ordering of question and retrieved text
% In the previous experiment, I follow the traditional RAG process that put the retrieved context before the question. So I wonder if we swap the ordering of retrieved context and question, the performance would change or not. The reason behind this is LLM would be more likely to focus on the information in the beginning. So I put the actual question first and then put the retrieved context.








% The experiment to use ICL to mitigate the bias
%% In the Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting https://arxiv.org/abs/2401.15585, CoT and few-shot have been shown as an effective way to decrease the bias score.  However, it is more difficult for our project. Because do we also need to provide additional text as few shot examples? Therefore, our experiment’s few shot has one with additional text, one without additional text. Here we focus on experiment using bias corpus.

% 1. The few-shot setting could have better average accuracy than zero-shot settings on the shown models on the ambiguous questions. It works best on the Accuracy-ambig(w/o  retriever). The few-shot demonstration also reduce the Diff-Bias-ambig(w/o retriever) and Diff-Bias-ambig(with text) which proves that it could reduce the bias on the ambiguous questions
% 2. However, the few-shot does not work well on disambiguous questions. It increases the Diff-Bias-disambig on most of the models except GPT-3.5-turbo
\iffalse
\begin{table}[t]
%\small
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\textbf{Model} & \multicolumn{2}{c}{\textbf{w/o RAG}} & \multicolumn{2}{c}{\textbf{stereo-set}} \\
 & Zero-shot & ICL & Zero-shot & ICL \\
\midrule
GPT-3.5         & 5.16 / -9.33   & 15.43 / 4.37  & 14.53 / 7.14   & 25.20 / 9.82 \\
Llama3-8B-Inst. & 5.65 / 1.59    & 2.83 / 0.20   & 14.68 / -0.40  & 6.30 / 1.79  \\
Qwen-7B-Inst.   & 10.02 / -3.67  & 9.38 / 7.44   & 24.01 / 0.50   & 21.78 / 6.85 \\
Qwen-14B        & 3.77 / -7.34   & 3.32 / 2.68   & 13.99 / -2.68  & 14.63 / 7.04 \\
Qwen-14B-Inst.  & -2.38 / -2.38  & -1.14 / -5.46 & 4.61 / 2.68    & 2.43 / 1.88  \\
\bottomrule
\end{tabular}
}
\caption{Diff-Bias scores for the ambiguous and disambiguated contexts (separated by `/') under ICL bias mitigation. Best values on each questions types are bolded.}
\label{tab:icl-diff-bias}
\end{table}


% Accuracy
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\textbf{Model} & \multicolumn{2}{c}{\textbf{w/o RAG}} & \multicolumn{2}{c}{\textbf{stereo-set}} \\
 & Zero-shot & ICL & Zero-shot & ICL \\
\midrule
GPT-3.5         & 45.24 / 75.74  & 60.17 / 79.32  & 27.83 / 71.68  & 26.19 / 81.15 \\
Llama3-8B-Inst. & 50.30 / 60.52  & 65.82 / 53.57  & 26.19 / 63.74  & 54.51 / 60.07 \\
Qwen-7B-Inst.   & 81.45 / 52.03  & 83.09 / 48.67  & 62.60 / 53.72  & 70.98 / 49.36 \\
Qwen-14B        & 63.79 / 82.84  & 84.08 / 76.79  & 46.83 / 77.63  & 52.73 / 81.75 \\
Qwen-14B-Inst.  & 96.53 / 71.92  & 97.97 / 75.55  & 87.05 / 68.30  & 96.08 / 64.63 \\
\bottomrule
\end{tabular}
}
\caption{Accuracy for the ambiguous and disambiguated contexts (separated by `/') under ICL bias mitigation. Best values on each questions types are bolded.}
\label{tab:icl:accuracy}
\end{table}



%-----------CoT--------------------------------------------------------------

\begin{table}[t]
%\small
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\textbf{Model} & \multicolumn{2}{c}{\textbf{w/o RAG}} & \multicolumn{2}{c}{\textbf{stereo-set}} \\
 & Zero-shot & CoT & Zero-shot & CoT \\
\midrule
GPT-3.5         & \textbf{5.16 }/ -9.33  & 11.86 / \textbf{4.76}   & \textbf{14.53} / \textbf{7.14}   & 16.22 / 8.04 \\
Llama3-8B-Inst. & 5.65 / 1.59   & \textbf{-0.55} / \textbf{0.50}   & 14.68 / \textbf{-0.40}  & \textbf{-0.74} / 0.69 \\
Qwen-7B-Inst.   & 10.02 / -3.67 & \textbf{4.61} / \textbf{1.19}    & 24.01 / 0.50   & 6.15 / 7.54 \\
Qwen-14B        & \textbf{3.77} / -7.34  & 4.76 / \textbf{-2.48}   & \textbf{13.99} / -2.68  & 14.24 / \textbf{1.69} \\
Qwen-14B-Inst.  & -2.38 / -2.38 & - / -         & 4.61 / 2.68    & \textbf{4.61} / 4.07 \\
\bottomrule
\end{tabular}
}
\caption{Diff-Bias scores for the ambiguous and disambiguated contexts (separated by `/') under CoT.Best values on each questions types are bolded.}
\label{tbl:cot:diff-bias}
\end{table}



\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\textbf{Model} & \multicolumn{2}{c}{\textbf{w/o RAG}} & \multicolumn{2}{c}{\textbf{stereo-set}} \\
 & Zero-shot & CoT & Zero-shot & CoT \\
\midrule
GPT-3.5         & 45.24 / 75.74  & \textbf{70.39} / \textbf{78.67}  & 27.83 / 71.68  & \textbf{38.29} / \textbf{77.78} \\
Llama3-8B-Inst. & 50.30 / \textbf{60.52}  & \textbf{53.82} / 34.82  & 26.19 / \textbf{63.74}  & \textbf{41.82} / 33.43 \\
Qwen-7B-Inst.   & \textbf{81.45} / \textbf{52.03}  & 59.87 / 37.95  & \textbf{62.60} / \textbf{53.72}  & 53.27 / 39.29 \\
Qwen-14B        & 63.79 / 82.84  & \textbf{75.10} / \textbf{82.89}  & \textbf{46.83} / 77.63  & 36.76 / \textbf{83.38} \\
Qwen-14B-Inst.  & 96.53 / 71.92  & - / -         & 87.05 / 68.30  & 86.66 / \textbf{68.40} \\
\bottomrule
\end{tabular}
}
\caption{Accuracy for the ambiguous and disambiguated contexts (separated by `/') under CoT. Best values on each questions types are bolded. }
\label{tbl:cot:accuracy}
\end{table}
\fi



\begin{table*}[t]
\small
\centering
%\resizebox{\textwidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\textbf{Model} & \multicolumn{2}{c}{\textbf{w/o RAG}} & \multicolumn{2}{c}{\textbf{w/ RAG}} \\
               & Zero-shot            & Few-shot             & Zero-shot            & Few-shot  \\
\midrule
GPT-3.5         & \textbf{5.16} / -9.33  & 15.43 / \textbf{4.37}  & \textbf{14.53} / \textbf{7.14}  & 25.20 / 9.82  \\
Llama3-8B-Inst. & 5.65 / 1.59   & 2.83 / \textbf{0.20}   & 14.68 / \textbf{-0.40}  & \textbf{6.30} / 1.79  \\
Qwen-7B-Inst.   & 10.02 / -3.67  & 9.38 / 7.44   & 24.01 / \textbf{0.50}  & \textbf{21.78} / 6.85  \\
Qwen-14B        & 3.77 / -7.34   & \textbf{3.32} / 2.68   & \textbf{13.99} / \textbf{-2.68}  & 14.63 / 7.04  \\
Qwen-14B-Inst.  & -2.38 / \textbf{-2.38}  & \textbf{-1.14} / -5.46  & 4.61 / 2.68  & \textbf{2.43} / \textbf{1.88}  \\
\bottomrule
\end{tabular}
%}
\caption{Diff-Bias scores for the ambiguous and disambiguated contexts (values separated by `/') under different prompting strategies. In each group (“w/o RAG” and “w/ RAG”), for ambiguous and disambiguated values separately, the diff-bias with the lowest absolute value is highlighted in bold.}
\label{tbl:prompt:diff-bias}
\end{table*}

% \begin{table*}[t]
% %\small
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{l|ccc|ccc}
% \toprule
% \textbf{Model} & \multicolumn{3}{c}{\textbf{w/o RAG}} & \multicolumn{3}{c}{\textbf{w/ RAG}} \\
%                & Zero-shot            & Few-shot                    & CoT                   & Zero-shot            & Few-shot                   & CoT \\
% \midrule
% GPT-3.5         & \textbf{{5.16}} / -9.33  & 15.43 / \textbf{{4.37}}  & 11.86 / 4.76  & \textbf{{14.53}} / \textbf{{7.14}}  & 25.20 / 9.82  & 16.22 / 8.04 \\
% Llama3-8B-Inst. & 5.65 / 1.59   & 2.83 / \textbf{{0.20}}   & \textbf{{-0.55}} / 0.50  & 14.68 / \textbf{{-0.40}}  & 6.30 / 1.79  & \textbf{{-0.74}} / 0.69 \\
% Qwen-7B-Inst.   & 10.02 / -3.67  & 9.38 / 7.44   & \textbf{{4.61}} / \textbf{{1.19}}  & 24.01 / \textbf{{0.50}}  & 21.78 / 6.85  & \textbf{{6.15}} / 7.54 \\
% Qwen-14B        & 3.77 / -7.34   & \textbf{{3.32}} / 2.68   & 4.76 / \textbf{{-2.48}}  & \textbf{{13.99}} / -2.68  & 14.63 / 7.04  & 14.24 / \textbf{{1.69}} \\
% Qwen-14B-Inst.  & -2.38 / \textbf{{-2.38}}  & \textbf{{-1.14}} / -5.46  & - / -  & 4.61 / 2.68  & \textbf{{2.43}} / \textbf{{1.88}}  & 4.61 / 4.07 \\
% \bottomrule
% \end{tabular}
% }
% \caption{Diff-Bias scores for the ambiguous and disambiguated contexts (values separated by `/') under different prompting strategies Few-shot and CoT. In each group (“w/o RAG” and “stereo-set”), for ambiguous and disambiguated values separately, the diff-bias with the lowest absolute value is highlighted in bold.}
% \label{tbl:prompt:diff-bias}
% \end{table*}

\begin{table*}[t]
\small
\centering
%\resizebox{\textwidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\textbf{Model} & \multicolumn{2}{c}{\textbf{w/o RAG}} & \multicolumn{2}{c}{\textbf{w/ RAG}} \\
               & Zero-shot             & Few-shot             & Zero-shot             & Few-shot \\

\midrule
GPT-3.5         & 45.24 / 75.74  & \textbf{60.17} / \textbf{79.32}  & \textbf{27.83} / 71.68  & 26.19 / \textbf{81.15}  \\
Llama3-8B-Inst. & 50.30 / \textbf{60.52}  & \textbf{65.82} / 53.57  & 26.19 / \textbf{63.74}  & \textbf{54.51} / 60.07  \\
Qwen-7B-Inst.   & 81.45 / \textbf{52.03}  & \textbf{83.09} / 48.67  & 62.60 / \textbf{53.72}  & \textbf{70.98} / 49.36  \\
Qwen-14B        & 63.79 / \textbf{82.84}  & \textbf{84.08} / 76.79  & 46.83 / 77.63  & \textbf{52.73} / \textbf{81.75}  \\
Qwen-14B-Inst.  & 96.53 / 71.92  & \textbf{97.97} / \textbf{75.55}  & 87.05 / \textbf{68.30}  & \textbf{96.08} / 64.63  \\
\bottomrule
\end{tabular}
%}
\caption{Accuracy for the ambiguous and disambiguated contexts (values separated by `/') under different prompting strategies. In each group (“w/o RAG” and “w/ RAG”), for ambiguous and disambiguated values separately, the highest value is highlighted in bold.}
\label{tbl:prompt:accuracy}
\end{table*}

% \begin{table*}[t]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{l|ccc|ccc}
% \toprule
% \textbf{Model} & \multicolumn{3}{c}{\textbf{w/o RAG}} & \multicolumn{3}{c}{\textbf{w/ RAG}} \\
%                & Zero-shot             & Few-shot                     & CoT                    & Zero-shot             & Few-shot                    & CoT \\

% \midrule
% GPT-3.5         & 45.24 / 75.74  & 60.17 / \textbf{{79.32}}  & \textbf{{70.39}} / 78.67  & 27.83 / 71.68  & 26.19 / \textbf{{81.15}}  & \textbf{{38.29}} / 77.78 \\
% Llama3-8B-Inst. & 50.30 / \textbf{{60.52}}  & \textbf{{65.82}} / 53.57  & 53.82 / 34.82  & 26.19 / \textbf{{63.74}}  & \textbf{{54.51}} / 60.07  & 41.82 / 33.43 \\
% Qwen-7B-Inst.   & 81.45 / \textbf{{52.03}}  & \textbf{{83.09}} / 48.67  & 59.87 / 37.95  & 62.60 / \textbf{{53.72}}  & \textbf{{70.98}} / 49.36  & 53.27 / 39.29 \\
% Qwen-14B        & 63.79 / \textbf{{82.84}}  & \textbf{{84.08}} / 76.79  & 75.10 / 82.39  & 46.83 / 77.63  & \textbf{{52.73}} / 81.75  & 36.76 / \textbf{{83.38}} \\
% Qwen-14B-Inst.  & 96.53 / 71.92  & \textbf{{97.97}} / \textbf{{75.55}}  & - / -  & 87.05 / 68.30  & \textbf{{96.08}} / 64.63  & 86.66 / \textbf{{68.40}} \\
% \bottomrule
% \end{tabular}
% }
% \caption{Accuracy for the ambiguous and disambiguated contexts (values separated by `/') under different prompting strategies Few-shot and CoT. In each group (“w/o RAG” and “stereo-set”), for ambiguous and disambiguated values separately, the highest value is highlighted in bold.}
% \label{tbl:prompt:accuracy}
% \end{table*}


\end{document}
