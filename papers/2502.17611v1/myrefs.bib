@INPROCEEDINGS{Zhou:2023a,
  title     = "A predictive factor analysis of social biases and
               task-performance in pretrained masked language models",
  author    = "Zhou, Yi and Camacho-Collados, Jose and Bollegala, Danushka",
  editor    = "Bouamor, Houda and Pino, Juan and Bali, Kalika",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "11082--11100",
  month     =  dec,
  year      =  2023,
  url       = "https://aclanthology.org/2023.emnlp-main.683.pdf",
  keywords  = "My Papers"
}

@INPROCEEDINGS{Zhou:2024,
  title     = "Evaluating short-term temporal fluctuations of social biases in
               social media data and masked language models",
  author    = "Zhou, Yi and Bollegala, Danushka and Camacho-Collados, Jose",
  booktitle = "Proceedings of the 2024 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "19693--19708",
  abstract  = "Yi Zhou, Danushka Bollegala, Jose Camacho-Collados. Proceedings
               of the 2024 Conference on Empirical Methods in Natural Language
               Processing. 2024.",
  month     =  nov,
  year      =  2024,
  url       = "https://aclanthology.org/2024.emnlp-main.1098.pdf",
  file      = "Other/Zhou et al. 2024 - Evaluating short-term temporal fluctuations of social biases in social media data and masked language models.pdf"
}

@INPROCEEDINGS{Isozaki:2003,
  title     = "Japanese Zero Pronoun Resolution based on Ranking Rules and
               Machine Learning",
  author    = "Isozaki, Hideki and Hirao, Tsutomu",
  booktitle = "Proceedings of the 2003 Conference on Empirical Methods in
               Natural Language Processing",
  pages     = "184--191",
  abstract  = "Hideki Isozaki, Tsutomu Hirao. Proceedings of the 2003 Conference
               on Empirical Methods in Natural Language Processing. 2003.",
  year      =  2003,
  url       = "https://aclanthology.org/W03-1024.pdf",
  file      = "Other/Isozaki and Hirao 2003 - Japanese Zero Pronoun Resolution based on Ranking Rules and Machine Learning.pdf"
}



@INPROCEEDINGS{CrowsPairs,
  title     = "{C}row{S}-pairs: A challenge dataset for measuring social biases
               in masked language models",
  author    = "Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman,
               Samuel R",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in
               Natural Language Processing (EMNLP)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1953--1967",
  abstract  = "Pretrained language models, especially masked language models
               (MLMs) have seen success across many NLP tasks. However, there is
               ample evidence that they use the cultural biases that are
               undoubtedly present in the corpora they are trained on,
               implicitly creating harm with biased representations. To measure
               some forms of social bias in language models against protected
               demographic groups in the US, we introduce the Crowdsourced
               Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508
               examples that cover stereotypes dealing with nine types of bias,
               like race, religion, and age. In CrowS-Pairs a model is presented
               with two sentences: one that is more stereotyping and another
               that is less stereotyping. The data focuses on stereotypes about
               historically disadvantaged groups and contrasts them with
               advantaged groups. We find that all three of the widely-used MLMs
               we evaluate substantially favor sentences that express
               stereotypes in every category in CrowS-Pairs. As work on building
               less biased models advances, this dataset can be used as a
               benchmark to evaluate progress.",
  month     =  nov,
  year      =  2020,
  url       = "https://www.aclweb.org/anthology/2020.emnlp-main.154.pdf",
  file      = "Other/Nangia et al. 2020 - CrowS-pairs - A challenge dataset for measuring social biases in masked language models.pdf",
  keywords  = "Unbiased"
}

@misc{izacard2021contriever,
      title={Unsupervised Dense Information Retrieval with Contrastive Learning}, 
      author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
      year={2021},
      url = {https://arxiv.org/abs/2112.09118},
      doi = {10.48550/ARXIV.2112.09118},
}

@INPROCEEDINGS{CrowsPairs,
  title     = "{C}row{S}-pairs: A challenge dataset for measuring social biases
               in masked language models",
  author    = "Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman,
               Samuel R",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in
               Natural Language Processing (EMNLP)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1953--1967",
  abstract  = "Pretrained language models, especially masked language models
               (MLMs) have seen success across many NLP tasks. However, there is
               ample evidence that they use the cultural biases that are
               undoubtedly present in the corpora they are trained on,
               implicitly creating harm with biased representations. To measure
               some forms of social bias in language models against protected
               demographic groups in the US, we introduce the Crowdsourced
               Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508
               examples that cover stereotypes dealing with nine types of bias,
               like race, religion, and age. In CrowS-Pairs a model is presented
               with two sentences: one that is more stereotyping and another
               that is less stereotyping. The data focuses on stereotypes about
               historically disadvantaged groups and contrasts them with
               advantaged groups. We find that all three of the widely-used MLMs
               we evaluate substantially favor sentences that express
               stereotypes in every category in CrowS-Pairs. As work on building
               less biased models advances, this dataset can be used as a
               benchmark to evaluate progress.",
  month     =  nov,
  year      =  2020,
  url       = "https://www.aclweb.org/anthology/2020.emnlp-main.154.pdf",
  file      = "Other/Nangia et al. 2020 - CrowS-pairs - A challenge dataset for measuring social biases in masked language models.pdf",
  keywords  = "Unbiased"
}


@INPROCEEDINGS{BBQ,
  title     = "{BBQ}: A hand-built bias benchmark for question answering",
  author    = "Parrish, Alicia and Chen, Angelica and Nangia, Nikita and
               Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut,
               Phu Mon and Bowman, Samuel",
  editor    = "Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline",
  booktitle = "Findings of the Association for Computational Linguistics: ACL
               2022",
  publisher = "Association for Computational Linguistics",
  address   = "Dublin, Ireland",
  pages     = "2086--2105",
  abstract  = "It is well documented that NLP models learn social biases, but
               little work has been done on how these biases manifest in model
               outputs for applied tasks like question answering (QA). We
               introduce the Bias Benchmark for QA (BBQ), a dataset of
               question-sets constructed by the authors that highlight attested
               social biases against people belonging to protected classes along
               nine social dimensions relevant for U.S. English-speaking
               contexts. Our task evaluate model responses at two levels: (i)
               given an under-informative context, we test how strongly
               responses reflect social biases, and (ii) given an adequately
               informative context, we test whether the model`s biases override
               a correct answer choice. We find that models often rely on
               stereotypes when the context is under-informative, meaning the
               model`s outputs consistently reproduce harmful biases in this
               setting. Though models are more accurate when the context
               provides an informative answer, they still rely on stereotypes
               and average up to 3.4 percentage points higher accuracy when the
               correct answer aligns with a social bias than when it conflicts,
               with this difference widening to over 5 points on examples
               targeting gender for most models tested.",
  month     =  may,
  year      =  2022,
  url       = "https://aclanthology.org/2022.findings-acl.165/",
  file      = "Other/Parrish et al. 2022 - BBQ - A hand-built bias benchmark for question answering.pdf"
}

@ARTICLE{KBBQ,
  title     = "{KoBBQ}: Korean Bias Benchmark for Question Answering",
  author    = "Jin, Jiho and Kim, Jiseon and Lee, Nayeon and Yoo, Haneul and Oh,
               Alice and Lee, Hwaran",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press",
  address   = "Cambridge, MA",
  volume    =  12,
  pages     = "507--524",
  abstract  = "Warning: This paper contains examples of stereotypes and biases.
               The Bias Benchmark for Question Answering (BBQ) is designed to
               evaluate social biases of language models (LMs), but it is not
               simple to adapt this benchmark to cultural contexts other than
               the US because social biases depend heavily on the cultural
               context. In this paper, we present KoBBQ, a Korean bias benchmark
               dataset, and we propose a general framework that addresses
               considerations for cultural adaptation of a dataset. Our
               framework includes partitioning the BBQ dataset into three
               classes---Simply-Transferred (can be used directly after cultural
               translation), Target-Modified (requires localization in target
               groups), and Sample-Removed (does not fit Korean culture)---and
               adding four new categories of bias specific to Korean culture. We
               conduct a large-scale survey to collect and validate the social
               biases and the targets of the biases that reflect the stereotypes
               in Korean culture. The resulting KoBBQ dataset comprises 268
               templates and 76,048 samples across 12 categories of social bias.
               We use KoBBQ to measure the accuracy and bias scores of several
               state-of-the-art multilingual LMs. The results clearly show
               differences in the bias of LMs as measured by KoBBQ and a
               machine-translated version of BBQ, demonstrating the need for and
               utility of a well-constructed, culturally aware social bias
               benchmark.",
  year      =  2024,
  url       = "https://aclanthology.org/2024.tacl-1.28/",
  file      = "Other/Jin et al. 2024 - KoBBQ - Korean Bias Benchmark for Question Answering.pdf;Other/Jin et al. 2024 - Jin et al. 2023 - KoBBQ - Korean Bias Benchmark for Question Answering.pdf"
}



@INPROCEEDINGS{Kaneko:2022,
  title     = "Gender Bias in Meta-Embeddings",
  author    = "Kaneko, Masahiro and Bollegala, Danushka and Okazaki, Naoaki",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP
               2022",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "3118--3133",
  abstract  = "Different methods have been proposed to develop meta-embeddings
               from a given set of source embeddings. However, the source
               embeddings can contain unfair gender-related biases, and how
               these influence the meta-embeddings has not been studied yet.We
               study the gender bias in meta-embeddings created under three
               different settings:(1) meta-embedding multiple sources without
               performing any debiasing (Multi-Source No-Debiasing),(2)
               meta-embedding multiple sources debiased by a single method
               (Multi-Source Single-Debiasing), and(3) meta-embedding a single
               source debiased by different methods (Single-Source
               Multi-Debiasing).Our experimental results show that
               meta-embedding amplifies the gender biases compared to input
               source embeddings.We find that debiasing not only the sources but
               also their meta-embedding is needed to mitigate those
               biases.Moreover, we propose a novel debiasing method based on
               meta-embedding learning where we use multiple debiasing methods
               on a single source embedding and then create a single unbiased
               meta-embedding.",
  month     =  dec,
  year      =  2022,
  url       = "https://aclanthology.org/2022.findings-emnlp.227.pdf"
}

@INPROCEEDINGS{CBBQ,
  title     = "{CBBQ}: A Chinese Bias Benchmark Dataset Curated with Human-{AI}
               Collaboration for Large Language Models",
  author    = "Huang, Yufei and Xiong, Deyi",
  editor    = "Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and
               Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen",
  booktitle = "Proceedings of the 2024 Joint International Conference on
               Computational Linguistics, Language Resources and Evaluation
               (LREC-COLING 2024)",
  publisher = "ELRA and ICCL",
  address   = "Torino, Italia",
  pages     = "2917--2929",
  abstract  = "Holistically measuring societal biases of large language models
               is crucial for detecting and reducing ethical risks in highly
               capable AI models. In this work, we present a Chinese Bias
               Benchmark dataset that consists of over 100K questions jointly
               constructed by human experts and generative language models,
               covering stereotypes and societal biases in 14 social dimensions
               related to Chinese culture and values. The curation process
               contains 4 essential steps: bias identification, ambiguous
               context generation, AI-assisted disambiguous context generation,
               and manual review and recomposition. The testing instances in the
               dataset are automatically derived from 3K+ high-quality templates
               manually authored with stringent quality control. The dataset
               exhibits wide coverage and high diversity. Extensive experiments
               demonstrate the effectiveness of the dataset in evaluating model
               bias, with all 12 publicly available Chinese large language
               models exhibiting strong bias in certain categories.
               Additionally, we observe from our experiments that fine-tuned
               models could, to a certain extent, heed instructions and avoid
               generating harmful outputs, in the way of \textquotedblleftmoral
               self-correction\textquotedblright. Our dataset is available at
               https://anonymous.4open.science/r/CBBQ-B860/.",
  month     =  may,
  year      =  2024,
  url       = "https://aclanthology.org/2024.lrec-main.260/",
  file      = "Other/Huang and Xiong 2024 - CBBQ - A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models.pdf;Other/Huang and Xiong 2024 - Huang and Xiong 2023 - CBBQ - A Chinese Bias Benc ... dataset curated with human-AI collaboration for large language models.pdf"
}


@ARTICLE{JBBQ,
  title         = "Analyzing social biases in Japanese Large Language Models",
  author        = "Yanaka, Hitomi and Han, Namgi and Kumon, Ryoma and Lu, Jie
                   and Takeshita, Masashi and Sekizawa, Ryo and Kato, Taisei and
                   Arai, Hiromi",
  journal       = "arXiv [cs.CL]",
  abstract      = "With the development of Large Language Models (LLMs), social
                   biases in the LLMs have become a crucial issue. While various
                   benchmarks for social biases have been provided across
                   languages, the extent to which Japanese LLMs exhibit social
                   biases has not been fully investigated. In this study, we
                   construct the Japanese Bias Benchmark dataset for Question
                   Answering (JBBQ) based on the English bias benchmark BBQ, and
                   analyze social biases in Japanese LLMs. The results show that
                   while current Japanese LLMs improve their accuracies on JBBQ
                   by instruction-tuning, their bias scores become larger. In
                   addition, augmenting their prompts with warning about social
                   biases reduces the effect of biases in some models.",
  month         =  jun,
  year          =  2024,
  url           = "http://arxiv.org/abs/2406.02050",
  file          = "Other/Yanaka et al. 2024 - Analyzing social biases in Japanese Large Language Models.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}


@INPROCEEDINGS{Bolukbasi:2016,
  title     = "Man is to Computer Programmer as Woman is to Homemaker? Debiasing
               Word Embeddings",
  author    = "Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and
               Saligrama, Venkatesh and Kalai, Adam T",
  booktitle = "Proc. of NuerIPs",
  pages     = "4349--4357",
  year      =  2016,
  file      = "Other/Bolukbasi et al. 2016 - Man is to Computer Programmer as Woman is to Homemaker - Debiasing Word Embeddings.pdf;Other/Bolukbasi et al. 2016 - Man is to Computer Programmer as Woman is to Homemaker - Debiasing_BolukbasiChangZouSaligramaKalai.pdf;Other/Bolukbasi et al. 2016 - Man is to Computer Programmer as Woman is to Homemaker - Debiasing_BolukbasiChangZouSaligramaKalai0.pdf"
}


@ARTICLE{Kaneko:2021,
  title    = "Unmasking the mask - evaluating social biases in masked Language
              Models",
  author   = "Kaneko, Masahiro and Bollegala, D",
  journal  = "National Conference on Artificial Intelligence",
  pages    = "11954--11962",
  abstract = "Masked Language Models (MLMs) have shown superior performances in
              numerous downstream Natural Language Processing (NLP) tasks.
              Unfortunately, MLMs also demonstrate significantly worrying levels
              of social biases. We show that the previously proposed evaluation
              metrics for quantifying the social biases in MLMs are problematic
              due to the following reasons: (1) prediction accuracy of the
              masked tokens itself tend to be low in some MLMs, which leads to
              unreliable evaluation metrics, and (2) in most downstream NLP
              tasks, masks are not used; therefore prediction of the mask is not
              directly related to them, and (3) high-frequency words in the
              training data are masked more often, introducing noise due to this
              selection bias in the test cases. Therefore, we propose All
              Unmasked Likelihood (AUL), a bias evaluation measure that predicts
              all tokens in a test case given the MLM embedding of the unmasked
              input and AUL with Attention weights (AULA) to evaluate tokens
              based on their importance in a sentence. Our experimental results
              show that the proposed bias evaluation measures accurately detect
              different types of biases in MLMs, and unlike AUL and AULA,
              previously proposed measures for MLMs systematically overestimate
              the measured biases and are heavily influenced by the unmasked
              tokens in the context.",
  month    =  apr,
  year     =  2021,
  url      = "https://ojs.aaai.org/index.php/AAAI/article/view/21453",
  keywords = "My Papers, Unbiased"
}


@INPROCEEDINGS{Guo2020-dc,
  title     = "Accelerating Large-Scale Inference with Anisotropic Vector
               Quantization",
  author    = "Guo, Ruiqi and Sun, Philip and Lindgren, Erik and Geng, Quan and
               Simcha, David and Chern, Felix and Kumar, Sanjiv",
  editor    = "Iii, Hal Daumé and Singh, Aarti",
  booktitle = "Proceedings of the 37th International Conference on Machine
               Learning",
  publisher = "PMLR",
  address   = "Virtual",
  volume    =  119,
  pages     = "3887--3896",
  abstract  = "Quantization based techniques are the current state-of-the-art
               for scaling maximum inner product search to massive databases.
               Traditional approaches to quantization aim to minimize the
               reconstruction error of the database points. Based on the
               observation that for a given query, the database points that have
               the largest inner products are more relevant, we develop a family
               of anisotropic quantization loss functions. Under natural
               statistical assumptions, we show that quantization with these
               loss functions leads to a new variant of vector quantization that
               more greatly penalizes the parallel component of a datapoint's
               residual relative to its orthogonal component. The proposed
               approach, whose implementation is open-source, achieves
               state-of-the-art results on the public benchmarks available at
               ann-benchmarks.com.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2020,
  url       = "https://proceedings.mlr.press/v119/guo20h.html",
  file      = "Other/Guo et al. 2020 - Accelerating Large-Scale Inference with Anisotropic Vector Quantization.pdf;Other/Guo et al. 2020 - Accelerating Large-Scale Inference with Anisotropic-Guo-1.pdf"
}

@ARTICLE{ROBBIE,
  title     = "{ROBBIE}: Robust bias evaluation of large generative language
               models",
  author    = "Esiobu, David and Tan, X and Hosseini, Saghar and Ung, Megan and
               Zhang, Yuchen and Fernandes, Jude and Dwivedi-Yu, Jane and
               Presani, Eleonora and Williams, Adina and Smith, Eric Michael",
  editor    = "Bouamor, Houda and Pino, Juan and Bali, Kalika",
  journal   = "Empir Method Nat Lang Process",
  publisher = "Association for Computational Linguistics",
  address   = "Singapore",
  pages     = "3764--3814",
  abstract  = "As generative large language models (LLMs) grow more performant
               and prevalent, we must develop comprehensive enough tools to
               measure and improve their fairness. Different prompt-based
               datasets can be used to measure social bias across multiple text
               domains and demographic axes, meaning that testing LLMs on more
               datasets can potentially help us characterize their biases more
               fully, and better ensure equal and equitable treatment of
               marginalized demographic groups. In this work, our focus is
               two-fold: (1) Benchmarking: a comparison of 6 different
               prompt-based bias and toxicity metrics across 12 demographic axes
               and 5 families of generative LLMs. Out of those 6 metrics,
               AdvPromptSet and HolisticBiasR are novel datasets proposed in the
               paper. The comparison of those benchmarks gives us insights about
               the bias and toxicity of the compared models. Therefore, we
               explore the frequency of demographic terms in common LLM
               pre-training corpora and how this may relate to model biases. (2)
               Mitigation: we conduct a comprehensive study of how well 3
               bias/toxicity mitigation techniques perform across our suite of
               measurements. ROBBIE aims to provide insights for practitioners
               while deploying a model, emphasizing the need to not only measure
               potential harms, but also understand how they arise by
               characterizing the data, mitigate harms once found, and balance
               any trade-offs. We open-source our analysis code in hopes of
               encouraging broader measurements of bias in future LLMs.",
  month     =  nov,
  year      =  2023,
  url       = "https://aclanthology.org/2023.emnlp-main.230.pdf",
  file      = "Other/Esiobu et al. 2023 - ROBBIE - Robust bias evaluation of large generative language models.pdf"
}



@ARTICLE{Malkov:2020,
  title     = "Efficient and robust approximate nearest neighbor search using
               hierarchical Navigable Small World graphs",
  author    = "Malkov, Yu A and Yashunin, D A",
  journal   = "IEEE Trans. Pattern Anal. Mach. Intell.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  42,
  number    =  4,
  pages     = "824--836",
  abstract  = "We present a new approach for the approximate K-nearest neighbor
               search based on navigable small world graphs with controllable
               hierarchy (Hierarchical NSW, HNSW). The proposed solution is
               fully graph-based, without any need for additional search
               structures (typically used at the coarse search stage of the most
               proximity graph techniques). Hierarchical NSW incrementally
               builds a multi-layer structure consisting of a hierarchical set
               of proximity graphs (layers) for nested subsets of the stored
               elements. The maximum layer in which an element is present is
               selected randomly with an exponentially decaying probability
               distribution. This allows producing graphs similar to the
               previously studied Navigable Small World (NSW) structures while
               additionally having the links separated by their characteristic
               distance scales. Starting the search from the upper layer
               together with utilizing the scale separation boosts the
               performance compared to NSW and allows a logarithmic complexity
               scaling. Additional employment of a heuristic for selecting
               proximity graph neighbors significantly increases performance at
               high recall and in case of highly clustered data. Performance
               evaluation has demonstrated that the proposed general metric
               space search index is able to strongly outperform previous
               opensource state-of-the-art vector-only approaches. Similarity of
               the algorithm to the skip list structure allows straightforward
               balanced distributed implementation.",
  month     =  apr,
  year      =  2020,
  url       = "https://doi.org/10.1109%2Ftpami.2018.2889473",
  file      = "Other/Malkov and Yashunin 2020 - Efficient and robust approximate nearest neighbor search using hierarchical Navigable Small World graphs.pdf",
  language  = "en"
}


@INPROCEEDINGS{Xu:2023a,
  title     = "{S}im{CSE}++: Improving contrastive learning for sentence
               embeddings from two perspectives",
  author    = "Xu, Jiahao and Shao, Wei and Chen, Lihui and Liu, Lemao",
  editor    = "Bouamor, Houda and Pino, Juan and Bali, Kalika",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "12028--12040",
  month     =  dec,
  year      =  2023,
  url       = "https://aclanthology.org/2023.emnlp-main.737.pdf"
}

@INPROCEEDINGS{Gao:2021c,
  title     = "{S}im{CSE}: Simple Contrastive Learning of Sentence Embeddings",
  author    = "Gao, Tianyu and Yao, Xingcheng and Chen, Danqi",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "6894--6910",
  abstract  = "This paper presents SimCSE, a simple contrastive learning
               framework that greatly advances state-of-the-art sentence
               embeddings. We first describe an unsupervised approach, which
               takes an input sentence and predicts itself in a contrastive
               objective, with only standard dropout used as noise. This simple
               method works surprisingly well, performing on par with previous
               supervised counterparts. We find that dropout acts as minimal
               data augmentation, and removing it leads to a representation
               collapse. Then, we propose a supervised approach, which
               incorporates annotated pairs from natural language inference
               datasets into our contrastive learning framework by using
               ``entailment'' pairs as positives and ``contradiction'' pairs as
               hard negatives. We evaluate SimCSE on standard semantic textual
               similarity (STS) tasks, and our unsupervised and supervised
               models using BERT base achieve an average of 76.3\% and 81.6\%
               Spearman's correlation respectively, a 4.2\% and 2.2\%
               improvement compared to the previous best results. We also show
               -- both theoretically and empirically -- that the contrastive
               learning objective regularizes pre-trained embeddings'
               anisotropic space to be more uniform, and it better aligns
               positive pairs when supervised signals are available.",
  month     =  apr,
  year      =  2021,
  url       = "https://github.com/princeton-nlp/SimCSE",
  file      = "Other/Gao et al. 2021 - SimCSE - Simple Contrastive Learning of Sentence Embeddings.pdf",
  keywords  = "Contrastive Learning"
}


@inproceedings{De-Arteaga-biasbios,
    author = {De-Arteaga, Maria and Romanov, Alexey and Wallach, Hanna and Chayes, Jennifer and Borgs, Christian and Chouldechova, Alexandra and Geyik, Sahin and Kenthapadi, Krishnaram and Kalai, Adam Tauman},
    title = {Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting},
    year = {2019},
    isbn = {9781450361255},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3287560.3287572},
    doi = {10.1145/3287560.3287572},
    booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
    pages = {120–128},
    numpages = {9},
    keywords = {algorithmic fairness, automated hiring, compounding injustices, gender bias, Supervised learning, online recruiting},
    location = {Atlanta, GA, USA},
    series = {FAT* '19}
}

@INPROCEEDINGS{Agrawal:2024,
  title     = "Can Knowledge Graphs Reduce Hallucinations in {LLM}s? : A Survey",
  author    = "Agrawal, Garima and Kumarage, Tharindu and Alghamdi, Zeyad and
               Liu, Huan",
  editor    = "Duh, Kevin and Gomez, Helena and Bethard, Steven",
  booktitle = "Proceedings of the 2024 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Mexico City, Mexico",
  pages     = "3947--3960",
  month     =  jun,
  year      =  2024,
  url       = "https://aclanthology.org/2024.naacl-long.219.pdf"
}

@INPROCEEDINGS{Penedo:2024,
  title     = "The {FineWeb} datasets: Decanting the web for the finest text
               data at scale",
  author    = "Penedo, Guilherme and Kydlíček, Hynek and Allal, Loubna Ben and
               Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von
               Werra, Leandro and Wolf, Thomas",
  booktitle = "Proceedings of the NeurIPS 2024 Track on Benchmarks and Datasets",
  abstract  = "The performance of a large language model (LLM) depends heavily
               on the quality and size of its pretraining dataset. However, the
               pretraining datasets for state-of-the-art open LLMs like Llama 3
               and Mixtral are not publicly available and very little is known
               about how they were created. In this work, we introduce FineWeb,
               a 15-trillion token dataset derived from 96 Common Crawl
               snapshots that produces better-performing LLMs than other open
               pretraining datasets. To advance the understanding of how best to
               curate high-quality pretraining datasets, we carefully document
               and ablate all of the design choices used in FineWeb, including
               in-depth investigations of deduplication and filtering
               strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion
               token collection of educational text filtered from FineWeb. LLMs
               pretrained on FineWeb-Edu exhibit dramatically better performance
               on knowledge- and reasoning-intensive benchmarks like MMLU and
               ARC. Along with our datasets, we publicly release our data
               curation codebase and all of the models trained during our
               ablation experiments.",
  month     =  jun,
  year      =  2024,
  file      = "Other/Penedo et al. 2024 - The FineWeb datasets - Decanting the web for the finest text data at scale.pdf"
}



@ARTICLE{Dubey:2024,
  title         = "The Llama 3 herd of models",
  author        = "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and
                   Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and
                   Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan,
                   Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang,
                   Aobo and Mitra, Archi and Sravankumar, Archie and Korenev,
                   Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and
                   Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava
                   and Roziere, Baptiste and Biron, Bethany and Tang, Binh and
                   Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and
                   Bi, Chloe and Marra, Chris and McConnell, Chris and Keller,
                   Christian and Touret, Christophe and Wu, Chunyang and Wong,
                   Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and
                   Allonsius, Damien and Song, Daniel and Pintz, Danielle and
                   Livshits, Danny and Esiobu, David and Choudhary, Dhruv and
                   Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and
                   Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and
                   Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and
                   Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and
                   Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme
                   and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and
                   Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron,
                   Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and
                   Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and
                   Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana
                   and Park, Jason and Mahadeokar, Jay and Shah, Jeet and van
                   der Linde, Jelmer and Billock, Jennifer and Hong, Jenny and
                   Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu
                   and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton,
                   Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph
                   and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and
                   Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak,
                   Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and
                   El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and
                   Chiu, Kuenley and Bhalla, Kunal and Rantala-Yeary, Lauren and
                   van der Maaten, Laurens and Chen, Lawrence and Tan, Liang and
                   Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo,
                   Lubo and Blecher, Lukas and Landzaat, Lukas and de Oliveira,
                   Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh,
                   Mannat and Paluri, Manohar and Kardas, Marcin and Oldham,
                   Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur,
                   Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar
                   and Hassan, Mona and Goyal, Naman and Torabi, Narjes and
                   Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji,
                   Niladri and Duchenne, Olivier and Çelebi, Onur and Alrassy,
                   Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar
                   and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and
                   Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and
                   He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and
                   Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo
                   Silveira and Stojnic, Robert and Raileanu, Roberta and
                   Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and
                   Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and
                   Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar
                   and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean
                   and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang
                   and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and
                   Wan, Shengye and Bhosale, Shruti and Zhang, Shun and
                   Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and
                   Sootla, Sten and Collot, Stephane and Gururangan, Suchin and
                   Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and
                   Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and
                   Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and
                   Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and
                   Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent
                   and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and
                   Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers,
                   Whitney and Martinet, Xavier and Wang, Xiaodong and Tan,
                   Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang,
                   Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei,
                   Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li,
                   Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan,
                   Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh,
                   Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey,
                   Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria,
                   Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay
                   and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei
                   and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and
                   Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples,
                   Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and
                   Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and
                   Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel,
                   Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan,
                   Azadeh and James, Beau and Maurer, Ben and Leonhardi,
                   Benjamin and Huang, Bernie and Loyd, Beth and De Paola, Beto
                   and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu
                   and Hancock, Braden and Wasti, Bram and Spence, Brandon and
                   Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and
                   Parker, Carl and Burton, Carly and Mejia, Catalina and Wang,
                   Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and
                   Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and
                   Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and
                   Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins,
                   David and Xu, David and Testuggine, Davide and David, Delia
                   and Parikh, Devi and Liskovich, Diana and Foss, Didem and
                   Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling,
                   Edward and Jamil, Eissa and Montgomery, Elaine and Presani,
                   Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik
                   and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and
                   Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat
                   and Caggioni, Francesco and Guzmán, Francisco and Kanayet,
                   Frank and Seide, Frank and Florez, Gabriela Medina and
                   Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and
                   Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov,
                   Grigory and {Guangyi} and {Zhang} and Lakshminarayanan, Guna
                   and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha,
                   Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk,
                   Helen and Aspegren, Henry and Goldman, Hunter and Damlaj,
                   Ibrahim and Molybog, Igor and Tufanov, Igor and Veliche,
                   Irina-Elena and Gat, Itai and Weissman, Jake and Geboski,
                   James and Kohli, James and Asher, Japhet and Gaya,
                   Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan,
                   Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul,
                   Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and
                   Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie,
                   Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang,
                   Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and
                   Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun
                   and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena,
                   Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and
                   Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg,
                   Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and
                   Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich,
                   Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani,
                   Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus,
                   Martynas and Hasson, Matan and Lennie, Matthew and Reso,
                   Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya
                   and Keneally, Meghan and Seltzer, Michael L and Valko, Michal
                   and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and
                   Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang,
                   Mike and Hermoso, Miquel Jubert and Metanat, Mo and
                   Rastegari, Mohammad and Bansal, Munish and Santhanam,
                   Nandhini and Parks, Natascha and White, Natasha and Bawa,
                   Navyata and Singhal, Nayan and Egebo, Nick and Usunier,
                   Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and
                   Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart,
                   Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent,
                   Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and
                   Rittner, Pedro and Bontrager, Philip and Roux, Pierre and
                   Dollar, Piotr and Zvyagina, Polina and Ratanchandani,
                   Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad
                   and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham
                   and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan,
                   Rebekkah and Battey, Robin and Wang, Rocky and Maheswari,
                   Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh
                   and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon,
                   Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh
                   and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun
                   and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha,
                   Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and
                   Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and
                   Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and
                   Chen, Stephen and Kehoe, Steve and Satterfield, Steve and
                   Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin
                   and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and
                   Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best,
                   Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe
                   and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and
                   Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and
                   Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish
                   and Mangla, Vishal and Albiero, Vítor and Ionescu, Vlad and
                   Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov,
                   Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and
                   Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and
                   Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia,
                   Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye
                   and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and
                   Zhang, Ying and Adi, Yossi and Nam, Youngjin and {Yu} and
                   {Wang} and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait,
                   Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo
                   and Yang, Zhenyu and Zhao, Zhiwei",
  journal       = "arXiv [cs.AI]",
  abstract      = "Modern artificial intelligence (AI) systems are powered by
                   foundation models. This paper presents a new set of
                   foundation models, called Llama 3. It is a herd of language
                   models that natively support multilinguality, coding,
                   reasoning, and tool usage. Our largest model is a dense
                   Transformer with 405B parameters and a context window of up
                   to 128K tokens. This paper presents an extensive empirical
                   evaluation of Llama 3. We find that Llama 3 delivers
                   comparable quality to leading language models such as GPT-4
                   on a plethora of tasks. We publicly release Llama 3,
                   including pre-trained and post-trained versions of the 405B
                   parameter language model and our Llama Guard 3 model for
                   input and output safety. The paper also presents the results
                   of experiments in which we integrate image, video, and speech
                   capabilities into Llama 3 via a compositional approach. We
                   observe this approach performs competitively with the
                   state-of-the-art on image, video, and speech recognition
                   tasks. The resulting models are not yet being broadly
                   released as they are still under development.",
  month         =  jul,
  year          =  2024,
  url           = "http://arxiv.org/abs/2407.21783",
  file          = "Other/Dubey et al. 2024 - The Llama 3 herd of models.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}


@INPROCEEDINGS{Song:2024a,
  title     = "{RAG}-{HAT}: A hallucination-aware tuning pipeline for {LLM} in
               retrieval-augmented generation",
  author    = "Song, Juntong and Wang, Xingguang and Zhu, Juno and Wu, Yuanhao
               and Cheng, Xuxin and Zhong, Randy and Niu, Cheng",
  booktitle = "Proceedings of the 2024 Conference on Empirical Methods in
               Natural Language Processing: Industry Track",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1548--1558",
  abstract  = "Juntong Song, Xingguang Wang, Juno Zhu, Yuanhao Wu, Xuxin Cheng,
               Randy Zhong, Cheng Niu. Proceedings of the 2024 Conference on
               Empirical Methods in Natural Language Processing: Industry Track.
               2024.",
  month     =  nov,
  year      =  2024,
  url       = "https://aclanthology.org/2024.emnlp-industry.113.pdf",
  file      = "Other/Song et al. 2024 - RAG-HAT - A hallucination-aware tuning pipeline for LLM in retrieval-augmented generation.pdf"
}

@INPROCEEDINGS{Niu:2024,
  title     = "{RAGT}ruth: A Hallucination Corpus for Developing Trustworthy
               Retrieval-Augmented Language Models",
  author    = "Niu, Cheng and Wu, Yuanhao and Zhu, Juno and Xu, Siliang and
               Shum, Kashun and Zhong, Randy and Song, Juntong and Zhang, Tong",
  editor    = "Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Bangkok, Thailand",
  pages     = "10862--10878",
  month     =  aug,
  year      =  2024,
  url       = "https://aclanthology.org/2024.acl-long.585.pdf",
  file      = "ACL-24/Niu et al. 2024 - RAGTruth - A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models.pdf"
}


@inproceedings{goldfarb-tarrant-etal-2021-intrinsic,
    title = "Intrinsic Bias Metrics Do Not Correlate with Application Bias",
    author = "Goldfarb-Tarrant, Seraphina  and
      Marchant, Rebecca  and
      Mu{\~n}oz S{\'a}nchez, Ricardo  and
      Pandya, Mugdha  and
      Lopez, Adam",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.150",
    doi = "10.18653/v1/2021.acl-long.150",
    pages = "1926--1940",
    abstract = "Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech.",
}

@techreport{webster-2020-sts,
title	= {Measuring and Reducing Gendered Correlations in Pre-trained Models},
author	= {Kellie Webster and Xuezhi Wang and Ian Tenney and Alex Beutel and Emily Pitler and Ellie Pavlick and Jilin Chen and Ed H. Chi and Slav Petrov},
year	= {2020},
URL	= {https://arxiv.org/abs/2010.06032}
}

@inproceedings{kaneko-etal-2022-debiasing,
    title = "Debiasing Isn{'}t Enough! {--} on the Effectiveness of Debiasing {MLM}s and Their Social Biases in Downstream Tasks",
    author = "Kaneko, Masahiro  and
      Bollegala, Danushka  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.111",
    pages = "1299--1310",
    abstract = "We study the relationship between task-agnostic intrinsic and task-specific extrinsic social bias evaluation measures for MLMs, and find that there exists only a weak correlation between these two types of evaluation measures. Moreover, we find that MLMs debiased using different methods still re-learn social biases during fine-tuning on downstream tasks. We identify the social biases in both training instances as well as their assigned labels as reasons for the discrepancy between intrinsic and extrinsic bias evaluation measurements. Overall, our findings highlight the limitations of existing MLM bias evaluation measures and raise concerns on the deployment of MLMs in downstream applications using those measures.",
}

@inproceedings{nadeem-etal-2021-stereoset,
    title = "{S}tereo{S}et: Measuring stereotypical bias in pretrained language models",
    author = "Nadeem, Moin  and
      Bethke, Anna  and
      Reddy, Siva",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.416",
    doi = "10.18653/v1/2021.acl-long.416",
    pages = "5356--5371",
    abstract = "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.",
}

@inproceedings{nangia-etal-2020-crows,
    title = "{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.154",
    doi = "10.18653/v1/2020.emnlp-main.154",
    pages = "1953--1967",
    abstract = "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",
}

@article{caliskan-et-al-weat,
    title = "Semantics derived automatically from language corpora contain human-like biases",
    keywords = "cs.AI, cs.CL, cs.CY, cs.LG",
    author = "Aylin Caliskan and Bryson, {Joanna J} and Arvind Narayanan",
    year = "2017",
    month = apr,
    day = "14",
    doi = "10.1126/science.aal4230",
    language = "English",
    volume = "356",
    pages = "183--186",
    journal = "Science",
    issn = "0036-8075",
    publisher = "American Association for the Advancement of Science",
    number = "6334",
}

@inproceedings{cao-etal-2022-intrinsic,
    title = "On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations",
    author = "Cao, Yang Trista  and
      Pruksachatkun, Yada  and
      Chang, Kai-Wei  and
      Gupta, Rahul  and
      Kumar, Varun  and
      Dhamala, Jwala  and
      Galstyan, Aram",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.62",
    doi = "10.18653/v1/2022.acl-short.62",
    pages = "561--570",
    abstract = "Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.",
}

@inproceedings{
soman2024observations,
title={Observations on Building {RAG} Systems for Technical Documents},
author={Sumit Soman and Sujoy Roychowdhury},
booktitle={The Second Tiny Papers Track at ICLR 2024},
year={2024},
url={https://openreview.net/forum?id=RFujq4HoV4}
}

@ARTICLE{Yang:2024e,
  title         = "{CRAG} -- Comprehensive {RAG} benchmark",
  author        = "Yang, Xiao and Sun, Kai and Xin, Hao and Sun, Yushi and
                   Bhalla, Nikita and Chen, Xiangsen and Choudhary, Sajal and
                   Gui, Rongze Daniel and Jiang, Ziran Will and Jiang, Ziyu and
                   Kong, Lingkun and Moran, Brian and Wang, Jiaqi and Xu, Yifan
                   Ethan and Yan, An and Yang, Chenyu and Yuan, Eting and Zha,
                   Hanwen and Tang, Nan and Chen, Lei and Scheffer, Nicolas and
                   Liu, Yue and Shah, Nirav and Wanga, Rakesh and Kumar, Anuj
                   and Yih, Wen-Tau and Dong, Xin Luna",
  journal       = "arXiv [cs.CL]",
  abstract      = "Retrieval-Augmented Generation (RAG) has recently emerged as
                   a promising solution to alleviate Large Language Model
                   (LLM)'s deficiency in lack of knowledge. Existing RAG
                   datasets, however, do not adequately represent the diverse
                   and dynamic nature of real-world Question Answering (QA)
                   tasks. To bridge this gap, we introduce the Comprehensive RAG
                   Benchmark (CRAG), a factual question answering benchmark of
                   4,409 question-answer pairs and mock APIs to simulate web and
                   Knowledge Graph (KG) search. CRAG is designed to encapsulate
                   a diverse array of questions across five domains and eight
                   question categories, reflecting varied entity popularity from
                   popular to long-tail, and temporal dynamisms ranging from
                   years to seconds. Our evaluation of this benchmark highlights
                   the gap to fully trustworthy QA. Whereas most advanced LLMs
                   achieve <=34\% accuracy on CRAG, adding RAG in a
                   straightforward manner improves the accuracy only to 44\%.
                   State-of-the-art industry RAG solutions only answer 63\% of
                   questions without any hallucination. CRAG also reveals much
                   lower accuracy in answering questions regarding facts with
                   higher dynamism, lower popularity, or higher complexity,
                   suggesting future research directions. The CRAG benchmark
                   laid the groundwork for a KDD Cup 2024 challenge and
                   attracted thousands of participants and submissions. We
                   commit to maintaining CRAG to serve research communities in
                   advancing RAG solutions and general QA solutions. CRAG is
                   available at https://github.com/facebookresearch/CRAG/.",
  month         =  jun,
  year          =  2024,
  url           = "http://arxiv.org/abs/2406.04744",
  file          = "Other/Yang et al. 2024 - CRAG - Comprehensive RAG benchmark.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}


@ARTICLE{Laban:2024,
  title         = "Summary of a haystack: A challenge to long-context {LLMs} and
                   {RAG} systems",
  author        = "Laban, Philippe and Fabbri, Alexander R and Xiong, Caiming
                   and Wu, Chien-Sheng",
  journal       = "arXiv [cs.CL]",
  abstract      = "LLMs and RAG systems are now capable of handling millions of
                   input tokens or more. However, evaluating the output quality
                   of such systems on long-context tasks remains challenging, as
                   tasks like Needle-in-a-Haystack lack complexity. In this
                   work, we argue that summarization can play a central role in
                   such evaluation. We design a procedure to synthesize
                   Haystacks of documents, ensuring that specific
                   \textit{insights} repeat across documents. The ``Summary of a
                   Haystack'' (SummHay) task then requires a system to process
                   the Haystack and generate, given a query, a summary that
                   identifies the relevant insights and precisely cites the
                   source documents. Since we have precise knowledge of what
                   insights should appear in a haystack summary and what
                   documents should be cited, we implement a highly reproducible
                   automatic evaluation that can score summaries on two aspects
                   - Coverage and Citation. We generate Haystacks in two domains
                   (conversation, news), and perform a large-scale evaluation of
                   10 LLMs and corresponding 50 RAG systems. Our findings
                   indicate that SummHay is an open challenge for current
                   systems, as even systems provided with an Oracle signal of
                   document relevance lag our estimate of human performance
                   (56\%) by 10+ points on a Joint Score. Without a retriever,
                   long-context LLMs like GPT-4o and Claude 3 Opus score below
                   20\% on SummHay. We show SummHay can also be used to study
                   enterprise RAG systems and position bias in long-context
                   models. We hope future systems can equal and surpass human
                   performance on SummHay.",
  month         =  jul,
  year          =  2024,
  url           = "http://arxiv.org/abs/2407.01370",
  file          = "Other/Laban et al. 2024 - Summary of a haystack - A challenge to long-context LLMs and RAG systems.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}


@ARTICLE{Wu:2024b,
  title         = "Pandora's box or Aladdin's lamp: A comprehensive analysis
                   revealing the role of {RAG} noise in large language models",
  author        = "Wu, Jinyang and Che, Feihu and Zhang, Chuyuan and Tao,
                   Jianhua and Zhang, Shuai and Shao, Pengpeng",
  journal       = "arXiv [cs.CL]",
  abstract      = "Retrieval-Augmented Generation (RAG) has emerged as a crucial
                   method for addressing hallucinations in large language models
                   (LLMs). While recent research has extended RAG models to
                   complex noisy scenarios, these explorations often confine
                   themselves to limited noise types and presuppose that noise
                   is inherently detrimental to LLMs, potentially deviating from
                   real-world retrieval environments and restricting practical
                   applicability. In this paper, we define seven distinct noise
                   types from a linguistic perspective and establish a Noise RAG
                   Benchmark (NoiserBench), a comprehensive evaluation framework
                   encompassing multiple datasets and reasoning tasks. Through
                   empirical evaluation of eight representative LLMs with
                   diverse architectures and scales, we reveal that these noises
                   can be further categorized into two practical groups: noise
                   that is beneficial to LLMs (aka beneficial noise) and noise
                   that is harmful to LLMs (aka harmful noise). While harmful
                   noise generally impairs performance, beneficial noise may
                   enhance several aspects of model capabilities and overall
                   performance. Our analysis offers insights for developing more
                   robust, adaptable RAG solutions and mitigating hallucinations
                   across diverse retrieval scenarios.",
  month         =  aug,
  year          =  2024,
  url           = "http://arxiv.org/abs/2408.13533",
  file          = "Other/Wu et al. 2024 - Pandora's box or Aladdin's lamp - A comprehensive analysis revealing the role of RAG noise in large language models.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}


@ARTICLE{Krishna:2024,
  title         = "Fact, fetch, and reason: A unified evaluation of
                   retrieval-augmented generation",
  author        = "Krishna, Satyapriya and Krishna, Kalpesh and Mohananey, Anhad
                   and Schwarcz, Steven and Stambler, Adam and Upadhyay, Shyam
                   and Faruqui, Manaal",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large Language Models (LLMs) have demonstrated significant
                   performance improvements across various cognitive tasks. An
                   emerging application is using LLMs to enhance
                   retrieval-augmented generation (RAG) capabilities. These
                   systems require LLMs to understand user queries, retrieve
                   relevant information, and synthesize coherent and accurate
                   responses. Given the increasing real-world deployment of such
                   systems, comprehensive evaluation becomes crucial. To this
                   end, we propose FRAMES (Factuality, Retrieval, And reasoning
                   MEasurement Set), a high-quality evaluation dataset designed
                   to test LLMs' ability to provide factual responses, assess
                   retrieval capabilities, and evaluate the reasoning required
                   to generate final answers. While previous work has provided
                   datasets and benchmarks to evaluate these abilities in
                   isolation, FRAMES offers a unified framework that provides a
                   clearer picture of LLM performance in end-to-end RAG
                   scenarios. Our dataset comprises challenging multi-hop
                   questions that require the integration of information from
                   multiple sources. We present baseline results demonstrating
                   that even state-of-the-art LLMs struggle with this task,
                   achieving 0.40 accuracy with no retrieval. The accuracy is
                   significantly improved with our proposed multi-step retrieval
                   pipeline, achieving an accuracy of 0.66 (>50\% improvement).
                   We hope our work will help bridge evaluation gaps and assist
                   in developing more robust and capable RAG systems.",
  month         =  sep,
  year          =  2024,
  url           = "http://arxiv.org/abs/2409.12941",
  file          = "Other/Krishna et al. 2024 - Fact, fetch, and reason - A unified evaluation of retrieval-augmented generation.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}


@ARTICLE{Edge:2024,
  title         = "From local to global: A graph {RAG} approach to query-focused
                   summarization",
  author        = "Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley,
                   Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and
                   Larson, Jonathan",
  journal       = "arXiv [cs.CL]",
  abstract      = "The use of retrieval-augmented generation (RAG) to retrieve
                   relevant information from an external knowledge source
                   enables large language models (LLMs) to answer questions over
                   private and/or previously unseen document collections.
                   However, RAG fails on global questions directed at an entire
                   text corpus, such as ``What are the main themes in the
                   dataset?'', since this is inherently a query-focused
                   summarization (QFS) task, rather than an explicit retrieval
                   task. Prior QFS methods, meanwhile, fail to scale to the
                   quantities of text indexed by typical RAG systems. To combine
                   the strengths of these contrasting methods, we propose a
                   Graph RAG approach to question answering over private text
                   corpora that scales with both the generality of user
                   questions and the quantity of source text to be indexed. Our
                   approach uses an LLM to build a graph-based text index in two
                   stages: first to derive an entity knowledge graph from the
                   source documents, then to pregenerate community summaries for
                   all groups of closely-related entities. Given a question,
                   each community summary is used to generate a partial
                   response, before all partial responses are again summarized
                   in a final response to the user. For a class of global
                   sensemaking questions over datasets in the 1 million token
                   range, we show that Graph RAG leads to substantial
                   improvements over a naive RAG baseline for both the
                   comprehensiveness and diversity of generated answers. An
                   open-source, Python-based implementation of both global and
                   local Graph RAG approaches is forthcoming at
                   https://aka.ms/graphrag.",
  month         =  apr,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.16130",
  file          = "Other/Edge et al. 2024 - From local to global - A graph RAG approach to query-focused summarization.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}


@ARTICLE{Ekstrand:2023,
  title         = "Overview of the {TREC} 2022 Fair Ranking Track",
  author        = "Ekstrand, Michael D and McDonald, Graham and Raj, Amifa and
                   Johnson, Isaac",
  journal       = "arXiv [cs.IR]",
  abstract      = "The TREC Fair Ranking Track aims to provide a platform for
                   participants to develop and evaluate novel retrieval
                   algorithms that can provide a fair exposure to a mixture of
                   demographics or attributes, such as ethnicity, that are
                   represented by relevant documents in response to a search
                   query. For example, particular demographics or attributes can
                   be represented by the documents topical content or authors.
                   The 2022 Fair Ranking Track adopted a resource allocation
                   task. The task focused on supporting Wikipedia editors who
                   are looking to improve the encyclopedia's coverage of topics
                   under the purview of a WikiProject. WikiProject coordinators
                   and/or Wikipedia editors search for Wikipedia documents that
                   are in need of editing to improve the quality of the article.
                   The 2022 Fair Ranking track aimed to ensure that documents
                   that are about, or somehow represent, certain protected
                   characteristics receive a fair exposure to the Wikipedia
                   editors, so that the documents have an fair opportunity of
                   being improved and, therefore, be well-represented in
                   Wikipedia. The under-representation of particular protected
                   characteristics in Wikipedia can result in systematic biases
                   that can have a negative human, social, and economic impact,
                   particularly for disadvantaged or protected societal groups.",
  month         =  feb,
  year          =  2023,
  url           = "http://arxiv.org/abs/2302.05558",
  file          = "Other/Ekstrand et al. 2023 - Overview of the TREC 2022 Fair Ranking Track.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR"
}


@inproceedings{wu-etal-2025-rag,
    title = "Does {RAG} Introduce Unfairness in {LLM}s? Evaluating Fairness in Retrieval-Augmented Generation Systems",
    author = "Wu, Xuyang  and
      Li, Shuowei  and
      Wu, Hsin-Tai  and
      Tao, Zhiqiang  and
      Fang, Yi",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.669/",
    pages = "10021--10036",
    abstract = "Retrieval-Augmented Generation (RAG) has recently gained significant attention for its enhanced ability to integrate external knowledge sources into open-domain question answering (QA) tasks. However, it remains unclear how these models address fairness concerns, particularly with respect to sensitive attributes such as gender, geographic location, and other demographic factors. First, as language models evolve to prioritize utility, like improving exact match accuracy, fairness considerations may have been largely overlooked. Second, the complex, multi-component architecture of RAG methods poses challenges in identifying and mitigating biases, as each component is optimized for distinct objectives. In this paper, we aim to empirically evaluate fairness in several RAG methods. We propose a fairness evaluation framework tailored to RAG, using scenario-based questions and analyzing disparities across demographic attributes. Our experimental results indicate that, despite recent advances in utility-driven optimization, fairness issues persist in both the retrieval and generation stages. These findings underscore the need for targeted interventions to address fairness concerns throughout the RAG pipeline. The dataset and code used in this study are publicly available at this GitHub Repository."
}


@software{Liu_LlamaIndex_2022,
author = {Liu, Jerry},
doi = {10.5281/zenodo.1234},
month = {11},
title = {{LlamaIndex}},
url = {https://github.com/jerryjliu/llama_index},
year = {2022}
}

@ARTICLE{Ouyang:2022,
  title         = "{T}raining language models to follow instructions with human
                   feedback",
  author        = "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo
                   and Wainwright, Carroll L and Mishkin, Pamela and Zhang,
                   Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex
                   and Schulman, John and Hilton, Jacob and Kelton, Fraser and
                   Miller, Luke and Simens, Maddie and Askell, Amanda and
                   Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe,
                   Ryan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Making language models bigger does not inherently make them
                   better at following a user's intent. For example, large
                   language models can generate outputs that are untruthful,
                   toxic, or simply not helpful to the user. In other words,
                   these models are not aligned with their users. In this paper,
                   we show an avenue for aligning language models with user
                   intent on a wide range of tasks by fine-tuning with human
                   feedback. Starting with a set of labeler-written prompts and
                   prompts submitted through the OpenAI API, we collect a
                   dataset of labeler demonstrations of the desired model
                   behavior, which we use to fine-tune GPT-3 using supervised
                   learning. We then collect a dataset of rankings of model
                   outputs, which we use to further fine-tune this supervised
                   model using reinforcement learning from human feedback. We
                   call the resulting models InstructGPT. In human evaluations
                   on our prompt distribution, outputs from the 1.3B parameter
                   InstructGPT model are preferred to outputs from the 175B
                   GPT-3, despite having 100x fewer parameters. Moreover,
                   InstructGPT models show improvements in truthfulness and
                   reductions in toxic output generation while having minimal
                   performance regressions on public NLP datasets. Even though
                   InstructGPT still makes simple mistakes, our results show
                   that fine-tuning with human feedback is a promising direction
                   for aligning language models with human intent.",
  month         =  mar,
  year          =  2022,
  url           = "http://arxiv.org/abs/2203.02155",
  file          = "Other/Ouyang et al. 2022 - Training language models to follow instructions with human feedback.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}


@INPROCEEDINGS{Ke:2022,
  title     = "Continual Pre-training of Language Models",
  author    = "Ke, Zixuan and Shao, Yijia and Lin, Haowei and Konishi, Tatsuya
               and Kim, Gyuhak and Liu, Bing",
  booktitle = "The Eleventh International Conference on Learning Representations",
  abstract  = "Language models (LMs) have been instrumental for the rapid
               advance of natural language processing. This paper studies
               continual pre-training of LMs, in particular, continual
               domain-adaptive pre-training (or continual DAP-training).
               Existing research has shown that further pre-training an LM using
               a domain corpus to adapt the LM to the domain can improve the
               end-task performance in the domain. This paper proposes a novel
               method to continually DAP-train an LM with a sequence of
               unlabeled domain corpora to adapt the LM to these domains to
               improve their end-task performances. The key novelty of our
               method is a soft-masking mechanism that directly controls the
               update to the LM. A novel proxy is also proposed to preserve the
               general knowledge in the original LM. Additionally, it contrasts
               the representations of the previously learned domain knowledge
               (including the general knowledge in the pre-trained LM) and the
               knowledge from the current full network to achieve knowledge
               integration. The method not only overcomes catastrophic
               forgetting, but also achieves knowledge transfer to improve
               end-task performances. Empirical evaluation demonstrates the
               effectiveness of the proposed method.",
  month     =  sep,
  year      =  2022,
  url       = "https://openreview.net/pdf?id=m_GDIItaI3o",
  file      = "Other/Ke et al. 2022 - Continual Pre-training of Language Models.pdf"
}


@ARTICLE{Lewis:2020a,
  title     = "Retrieval-augmented generation for knowledge-intensive {NLP}
               tasks",
  author    = "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandara and
               Petroni, F and Karpukhin, Vladimir and Goyal, Naman and Kuttler,
               Heinrich and Lewis, M and Yih, Wen-Tau and Rocktäschel, Tim and
               Riedel, Sebastian and Kiela, Douwe",
  editor    = "Larochelle, H and Ranzato, M and Hadsell, R and Balcan, M F and
               Lin, H",
  journal   = "Adv. Neural Inf. Process. Syst.",
  publisher = "Curran Associates, Inc.",
  volume    = "abs/2005.11401",
  pages     = "9459--9474",
  abstract  = "Large pre-trained language models have been shown to store
               factual knowledge in their parameters, and achieve
               state-of-the-art results when fine-tuned on downstream NLP tasks.
               However, their ability to access and precisely manipulate
               knowledge is still limited, and hence on knowledge-intensive
               tasks, their performance lags behind task-specific architectures.
               Additionally, providing provenance for their decisions and
               updating their world knowledge remain open research problems.
               Pre-trained models with a differentiable access mechanism to
               explicit non-parametric memory can overcome this issue, but have
               so far been only investigated for extractive downstream tasks. We
               explore a general-purpose fine-tuning recipe for
               retrieval-augmented generation (RAG) -- models which combine
               pre-trained parametric and non-parametric memory for language
               generation. We introduce RAG models where the parametric memory
               is a pre-trained seq2seq model and the non-parametric memory is a
               dense vector index of Wikipedia, accessed with a pre-trained
               neural retriever. We compare two RAG formulations, one which
               conditions on the same retrieved passages across the whole
               generated sequence, the other can use different passages per
               token. We fine-tune and evaluate our models on a wide range of
               knowledge-intensive NLP tasks and set the state-of-the-art on
               three open domain QA tasks, outperforming parametric seq2seq
               models and task-specific retrieve-and-extract architectures. For
               language generation tasks, we find that RAG models generate more
               specific, diverse and factual language than a state-of-the-art
               parametric-only seq2seq baseline.",
  month     =  may,
  year      =  2020,
  url       = "http://arxiv.org/abs/2005.11401",
  file      = "Other/Lewis et al. 2020 - Retrieval-augmented generation for knowledge-intensive NLP tasks.pdf"
}

@INPROCEEDINGS{Izacard:2021,
  title     = "Leveraging passage retrieval with generative models for open
               domain question answering",
  author    = "Izacard, Gautier and Grave, Edouard",
  booktitle = "Proceedings of the 16th Conference of the European Chapter of the
               Association for Computational Linguistics: Main Volume",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "874--880",
  abstract  = "Generative models for open domain question answering have proven
               to be competitive, without resorting to external knowledge. While
               promising, this approach requires to use models with billions of
               parameters, which are expensive to train and query. In this
               paper, we investigate how much these models can benefit from
               retrieving text passages, potentially containing evidence. We
               obtain state-of-the-art results on the Natural Questions and
               TriviaQA open benchmarks. Interestingly, we observe that the
               performance of this method significantly improves when increasing
               the number of retrieved passages. This is evidence that
               sequence-to-sequence models offers a flexible framework to
               efficiently aggregate and combine evidence from multiple
               passages.",
  month     =  apr,
  year      =  2021,
  url       = "https://aclanthology.org/2021.eacl-main.74.pdf",
  file      = "Other/Izacard and Grave 2021 - Leveraging passage retrieval with generative models for open domain question answering.pdf"
}

@INPROCEEDINGS{Jiang:2024,
  title     = "On Large Language Models{'} Hallucination with Regard to Known
               Facts",
  author    = "Jiang, Che and Qi, Biqing and Hong, Xiangyu and Fu, Dayuan and
               Cheng, Yang and Meng, Fandong and Yu, Mo and Zhou, Bowen and
               Zhou, Jie",
  editor    = "Duh, Kevin and Gomez, Helena and Bethard, Steven",
  booktitle = "Proceedings of the 2024 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Mexico City, Mexico",
  pages     = "1041--1053",
  month     =  jun,
  year      =  2024,
  url       = "https://aclanthology.org/2024.naacl-long.60.pdf",
  file      = "Other/Jiang et al. 2024 - On Large Language Models' Hallucination with Regard to Known Facts.pdf"
}

@INPROCEEDINGS{Shuster:2021,
  title     = "Retrieval Augmentation Reduces Hallucination in Conversation",
  author    = "Shuster, Kurt and Poff, Spencer and Chen, Moya and Kiela, Douwe
               and Weston, Jason",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP
               2021",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "3784--3803",
  abstract  = "Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston.
               Findings of the Association for Computational Linguistics: EMNLP
               2021. 2021.",
  month     =  nov,
  year      =  2021,
  url       = "https://aclanthology.org/2021.findings-emnlp.320.pdf",
  file      = "Other/Shuster et al. 2021 - Retrieval Augmentation Reduces Hallucination in Conversation.pdf"
}

@ARTICLE{Hu:2021,
  title         = "{L}o{RA}: {L}ow-{R}ank {A}daptation of {l}arge {l}anguage
                   {m}odels",
  author        = "Hu, Edward J and Shen, Yelong and Wallis, Phillip and
                   Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang,
                   Lu and Chen, Weizhu",
  journal       = "Int Conf Learn Represent",
  abstract      = "An important paradigm of natural language processing consists
                   of large-scale pre-training on general domain data and
                   adaptation to particular tasks or domains. As we pre-train
                   larger models, full fine-tuning, which retrains all model
                   parameters, becomes less feasible. Using GPT-3 175B as an
                   example -- deploying independent instances of fine-tuned
                   models, each with 175B parameters, is prohibitively
                   expensive. We propose Low-Rank Adaptation, or LoRA, which
                   freezes the pre-trained model weights and injects trainable
                   rank decomposition matrices into each layer of the
                   Transformer architecture, greatly reducing the number of
                   trainable parameters for downstream tasks. Compared to GPT-3
                   175B fine-tuned with Adam, LoRA can reduce the number of
                   trainable parameters by 10,000 times and the GPU memory
                   requirement by 3 times. LoRA performs on-par or better than
                   fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and
                   GPT-3, despite having fewer trainable parameters, a higher
                   training throughput, and, unlike adapters, no additional
                   inference latency. We also provide an empirical investigation
                   into rank-deficiency in language model adaptation, which
                   sheds light on the efficacy of LoRA. We release a package
                   that facilitates the integration of LoRA with PyTorch models
                   and provide our implementations and model checkpoints for
                   RoBERTa, DeBERTa, and GPT-2 at
                   https://github.com/microsoft/LoRA.",
  month         =  jun,
  year          =  2021,
  url           = "http://arxiv.org/abs/2106.09685",
  file          = "Other/Hu et al. 2021 - LoRA - Low-Rank Adaptation of large language models.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}
%--------------------Documents used to retrieve------------------------
@inproceedings{stereoset,
    title = "{S}tereo{S}et: Measuring stereotypical bias in pretrained language models",
    author = "Nadeem, Moin  and
      Bethke, Anna  and
      Reddy, Siva",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.416/",
    doi = "10.18653/v1/2021.acl-long.416",
    pages = "5356--5371",
    abstract = "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at \url{https://stereoset.mit.edu}."
}

@inproceedings{redditbias,
    title = "{R}eddit{B}ias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models",
    author = "Barikeri, Soumya  and
      Lauscher, Anne  and
      Vuli{\'c}, Ivan  and
      Glava{\v{s}}, Goran",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.151/",
    doi = "10.18653/v1/2021.acl-long.151",
    pages = "1941--1955",
    abstract = "Text representation models are prone to exhibit a range of societal biases, reflecting the non-controlled and biased nature of the underlying pretraining data, which consequently leads to severe ethical issues and even bias amplification. Recent work has predominantly focused on measuring and mitigating bias in pretrained language models. Surprisingly, the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited to only a few types of bias, artificially constructed resources, and completely ignores the impact that debiasing methods may have on the final perfor mance in dialog tasks, e.g., conversational response generation. In this work, we present REDDITBIAS, the first conversational data set grounded in the actual human conversations from Reddit, allowing for bias measurement and mitigation across four important bias dimensions: gender,race,religion, and queerness. Further, we develop an evaluation framework which simultaneously 1)measures bias on the developed REDDITBIAS resource, and 2)evaluates model capability in dialog tasks after model debiasing. We use the evaluation framework to benchmark the widely used conversational DialoGPT model along with the adaptations of four debiasing methods. Our results indicate that DialoGPT is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance."
}

@inproceedings{frenchcrowspairs,
    title = "{F}rench {C}row{S}-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than {E}nglish",
    author = {N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Dupont, Yoann  and
      Bezan{\c{c}}on, Julien  and
      Fort, Kar{\"e}n},
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.583/",
    doi = "10.18653/v1/2022.acl-long.583",
    pages = "8521--8531",
    abstract = "Warning: This paper contains explicit statements of offensive stereotypes which may be upsetting. Much work on biases in natural language processing has addressed biases linked to the social and cultural experience of English speaking individuals in the United States. We seek to widen the scope of bias studies by creating material to measure social bias in language models (LMs) against specific demographic groups in France. We build on the US-centered CrowS-pairs dataset to create a multilingual stereotypes dataset that allows for comparability across languages while also characterizing biases that are specific to each country and language. We introduce 1,679 sentence pairs in French that cover stereotypes in ten types of bias like gender and age. 1,467 sentence pairs are translated from CrowS-pairs and 212 are newly crowdsourced. The sentence pairs contrast stereotypes concerning underadvantaged groups with the same sentence concerning advantaged groups. We find that four widely used language models (three French, one multilingual) favor sentences that express stereotypes in most bias categories. We report on the translation process from English into French, which led to a characterization of stereotypes in CrowS-pairs including the identification of US-centric cultural traits. We offer guidelines to further extend the dataset to other languages and cultural environments."
}

@inproceedings{chbias,
    title = "{CHB}ias: Bias Evaluation and Mitigation of {C}hinese Conversational Language Models",
    author = "Zhao, Jiaxu  and
      Fang, Meng  and
      Shi, Zijing  and
      Li, Yitong  and
      Chen, Ling  and
      Pechenizkiy, Mykola",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.757/",
    doi = "10.18653/v1/2023.acl-long.757",
    pages = "13538--13556",
    abstract = "\textit{ \textbf{redWarning:} This paper contains content that may be offensive or upsetting.}Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. However, there are still limited bias categories in current research, and most of them only focus on English. In this paper, we introduce a new Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese conversational language models.Apart from those previous well-explored bias categories, CHBias includes under-explored bias categories, such as ageism and appearance biases, which received less attention. We evaluate two popular pretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias. Furthermore, to mitigate different biases, we apply several debiasing methods to the Chinese pretrained models. Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases, and debiasing methods using the proposed dataset can make response generation less biased while preserving the models' conversational capabilities."
}

@inproceedings{winobias,
    title = "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods",
    author = "Zhao, Jieyu  and
      Wang, Tianlu  and
      Yatskar, Mark  and
      Ordonez, Vicente  and
      Chang, Kai-Wei",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2003/",
    doi = "10.18653/v1/N18-2003",
    pages = "15--20",
    abstract = "In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets."
}

@inproceedings{winogenerated,
    title = "Discovering Language Model Behaviors with Model-Written Evaluations",
    author = "Perez, Ethan  and
      Ringer, Sam  and
      Lukosiute, Kamile  and
      Nguyen, Karina  and
      Chen, Edwin  and
      Heiner, Scott  and
      Pettit, Craig  and
      Olsson, Catherine  and
      Kundu, Sandipan  and
      Kadavath, Saurav  and
      Jones, Andy  and
      Chen, Anna  and
      Mann, Benjamin  and
      Israel, Brian  and
      Seethor, Bryan  and
      McKinnon, Cameron  and
      Olah, Christopher  and
      Yan, Da  and
      Amodei, Daniela  and
      Amodei, Dario  and
      Drain, Dawn  and
      Li, Dustin  and
      Tran-Johnson, Eli  and
      Khundadze, Guro  and
      Kernion, Jackson  and
      Landis, James  and
      Kerr, Jamie  and
      Mueller, Jared  and
      Hyun, Jeeyoon  and
      Landau, Joshua  and
      Ndousse, Kamal  and
      Goldberg, Landon  and
      Lovitt, Liane  and
      Lucas, Martin  and
      Sellitto, Michael  and
      Zhang, Miranda  and
      Kingsland, Neerav  and
      Elhage, Nelson  and
      Joseph, Nicholas  and
      Mercado, Noemi  and
      DasSarma, Nova  and
      Rausch, Oliver  and
      Larson, Robin  and
      McCandlish, Sam  and
      Johnston, Scott  and
      Kravec, Shauna  and
      El Showk, Sheer  and
      Lanham, Tamera  and
      Telleen-Lawton, Timothy  and
      Brown, Tom  and
      Henighan, Tom  and
      Hume, Tristan  and
      Bai, Yuntao  and
      Hatfield-Dodds, Zac  and
      Clark, Jack  and
      Bowman, Samuel R.  and
      Askell, Amanda  and
      Grosse, Roger  and
      Hernandez, Danny  and
      Ganguli, Deep  and
      Hubinger, Evan  and
      Schiefer, Nicholas  and
      Kaplan, Jared",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.847/",
    doi = "10.18653/v1/2023.findings-acl.847",
    pages = "13387--13434",
    abstract = "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100{\%} of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user`s preferred answer ({\textquotedblleft}sycophancy{\textquotedblright}) and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors."
}

@inproceedings{gest,
    title = "Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling",
    author = "Pikuliak, Mat{\'u}{\v{s}}  and
      Oresko, Stefan  and
      Hrckova, Andrea  and
      Simko, Marian",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.173/",
    doi = "10.18653/v1/2024.findings-emnlp.173",
    pages = "3060--3083",
    abstract = "We present GEST {--} a new manually created dataset designed to measure gender-stereotypical reasoning in language models and machine translation systems. GEST contains samples for 16 gender stereotypes about men and women (e.g., Women are beautiful, Men are leaders) that are compatible with the English language and 9 Slavic languages. The definition of said stereotypes was informed by gender experts. We used GEST to evaluate English and Slavic masked LMs, English generative LMs, and machine translation systems. We discovered significant and consistent amounts of gender-stereotypical reasoning in almost all the evaluated models and languages. Our experiments confirm the previously postulated hypothesis that the larger the model, the more stereotypical it usually is."
}

@inproceedings{fsb,
    title = "{\textquotedblleft}Fifty Shades of Bias{\textquotedblright}: Normative Ratings of Gender Bias in {GPT} Generated {E}nglish Text",
    author = "Hada, Rishav  and
      Seth, Agrima  and
      Diddee, Harshita  and
      Bali, Kalika",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.115/",
    doi = "10.18653/v1/2023.emnlp-main.115",
    pages = "1862--1876",
    abstract = "Language serves as a powerful tool for the manifestation of societal belief systems. In doing so, it also perpetuates the prevalent biases in our society. Gender bias is one of the most pervasive biases in our society and is seen in online and offline discourses. With LLMs increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative. Prior work often treats gender bias as a binary classification task. However, acknowledging that bias must be perceived at a relative scale; we investigate the generation and consequent receptivity of manual annotators to bias of varying degrees. Specifically, we create the first dataset of GPT-generated English text with normative ratings of gender bias. Ratings were obtained using Best{--}Worst Scaling {--} an efficient comparative annotation framework. Next, we systematically analyze the variation of themes of gender biases in the observed ranking and show that identity-attack is most closely related to gender bias. Finally, we show the performance of existing automated models trained on related concepts on our dataset."
}


%------------------------------------
% Bias supression
@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

% ICL debias
@article{kaneko2024evaluating,
  title={Evaluating gender bias in large language models via chain-of-thought prompting},
  author={Kaneko, Masahiro and Bollegala, Danushka and Okazaki, Naoaki and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2401.15585},
  year={2024}
}

%ICL
@article{GPT3:2020,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{you-etal-2024-beyond,
    title = "Beyond Binary Gender Labels: Revealing Gender Bias in {LLM}s through Gender-Neutral Name Predictions",
    author = "You, Zhiwen  and
      Lee, HaeJin  and
      Mishra, Shubhanshu  and
      Jeoung, Sullam  and
      Mishra, Apratim  and
      Kim, Jinseok  and
      Diesner, Jana",
    editor = "Fale{\'n}ska, Agnieszka  and
      Basta, Christine  and
      Costa-juss{\`a}, Marta  and
      Goldfarb-Tarrant, Seraphina  and
      Nozza, Debora",
    booktitle = "Proceedings of the 5th Workshop on Gender Bias in Natural Language Processing (GeBNLP)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.gebnlp-1.16/",
    doi = "10.18653/v1/2024.gebnlp-1.16",
    pages = "255--268"
}

@article{li2024mitigating,
  title={Mitigating social biases of pre-trained language models via contrastive self-debiasing with double data augmentation},
  author={Li, Yingji and Du, Mengnan and Song, Rui and Wang, Xin and Sun, Mingchen and Wang, Ying},
  journal={Artificial Intelligence},
  volume={332},
  pages={104143},
  year={2024},
  publisher={Elsevier}
}

@article{lin2024towards,
  title={Towards trustworthy LLMs: a review on debiasing and dehallucinating in large language models},
  author={Lin, Zichao and Guan, Shuyan and Zhang, Wending and Zhang, Huiyan and Li, Yugang and Zhang, Huaping},
  journal={Artificial Intelligence Review},
  volume={57},
  number={9},
  pages={243},
  year={2024},
  publisher={Springer}
}

@inproceedings{li2024steering,
  title={Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework},
  author={Li, Jingling and Tang, Zeyu and Liu, Xiaoyu and Spirtes, Peter and Zhang, Kun and Leqi, Liu and Liu, Yang},
  booktitle={ICLR 2024 Workshop on Reliable and Responsible Foundation Models},
year ={2024}
}