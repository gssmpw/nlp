@inproceedings{De-Arteaga-biasbios,
    author = {De-Arteaga, Maria and Romanov, Alexey and Wallach, Hanna and Chayes, Jennifer and Borgs, Christian and Chouldechova, Alexandra and Geyik, Sahin and Kenthapadi, Krishnaram and Kalai, Adam Tauman},
    title = {Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting},
    year = {2019},
    isbn = {9781450361255},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3287560.3287572},
    doi = {10.1145/3287560.3287572},
    booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
    pages = {120–128},
    numpages = {9},
    keywords = {algorithmic fairness, automated hiring, compounding injustices, gender bias, Supervised learning, online recruiting},
    location = {Atlanta, GA, USA},
    series = {FAT* '19}
}

@ARTICLE{Ekstrand:2023,
  title         = "Overview of the {TREC} 2022 Fair Ranking Track",
  author        = "Ekstrand, Michael D and McDonald, Graham and Raj, Amifa and
                   Johnson, Isaac",
  journal       = "arXiv [cs.IR]",
  abstract      = "The TREC Fair Ranking Track aims to provide a platform for
                   participants to develop and evaluate novel retrieval
                   algorithms that can provide a fair exposure to a mixture of
                   demographics or attributes, such as ethnicity, that are
                   represented by relevant documents in response to a search
                   query. For example, particular demographics or attributes can
                   be represented by the documents topical content or authors.
                   The 2022 Fair Ranking Track adopted a resource allocation
                   task. The task focused on supporting Wikipedia editors who
                   are looking to improve the encyclopedia's coverage of topics
                   under the purview of a WikiProject. WikiProject coordinators
                   and/or Wikipedia editors search for Wikipedia documents that
                   are in need of editing to improve the quality of the article.
                   The 2022 Fair Ranking track aimed to ensure that documents
                   that are about, or somehow represent, certain protected
                   characteristics receive a fair exposure to the Wikipedia
                   editors, so that the documents have an fair opportunity of
                   being improved and, therefore, be well-represented in
                   Wikipedia. The under-representation of particular protected
                   characteristics in Wikipedia can result in systematic biases
                   that can have a negative human, social, and economic impact,
                   particularly for disadvantaged or protected societal groups.",
  month         =  feb,
  year          =  2023,
  url           = "http://arxiv.org/abs/2302.05558",
  file          = "Other/Ekstrand et al. 2023 - Overview of the TREC 2022 Fair Ranking Track.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR"
}

@ARTICLE{Hu:2021,
  title         = "{L}o{RA}: {L}ow-{R}ank {A}daptation of {l}arge {l}anguage
                   {m}odels",
  author        = "Hu, Edward J and Shen, Yelong and Wallis, Phillip and
                   Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang,
                   Lu and Chen, Weizhu",
  journal       = "Int Conf Learn Represent",
  abstract      = "An important paradigm of natural language processing consists
                   of large-scale pre-training on general domain data and
                   adaptation to particular tasks or domains. As we pre-train
                   larger models, full fine-tuning, which retrains all model
                   parameters, becomes less feasible. Using GPT-3 175B as an
                   example -- deploying independent instances of fine-tuned
                   models, each with 175B parameters, is prohibitively
                   expensive. We propose Low-Rank Adaptation, or LoRA, which
                   freezes the pre-trained model weights and injects trainable
                   rank decomposition matrices into each layer of the
                   Transformer architecture, greatly reducing the number of
                   trainable parameters for downstream tasks. Compared to GPT-3
                   175B fine-tuned with Adam, LoRA can reduce the number of
                   trainable parameters by 10,000 times and the GPU memory
                   requirement by 3 times. LoRA performs on-par or better than
                   fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and
                   GPT-3, despite having fewer trainable parameters, a higher
                   training throughput, and, unlike adapters, no additional
                   inference latency. We also provide an empirical investigation
                   into rank-deficiency in language model adaptation, which
                   sheds light on the efficacy of LoRA. We release a package
                   that facilitates the integration of LoRA with PyTorch models
                   and provide our implementations and model checkpoints for
                   RoBERTa, DeBERTa, and GPT-2 at
                   https://github.com/microsoft/LoRA.",
  month         =  jun,
  year          =  2021,
  url           = "http://arxiv.org/abs/2106.09685",
  file          = "Other/Hu et al. 2021 - LoRA - Low-Rank Adaptation of large language models.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Penedo:2024,
  title     = "The {FineWeb} datasets: Decanting the web for the finest text
               data at scale",
  author    = "Penedo, Guilherme and Kydlíček, Hynek and Allal, Loubna Ben and
               Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von
               Werra, Leandro and Wolf, Thomas",
  booktitle = "Proceedings of the NeurIPS 2024 Track on Benchmarks and Datasets",
  abstract  = "The performance of a large language model (LLM) depends heavily
               on the quality and size of its pretraining dataset. However, the
               pretraining datasets for state-of-the-art open LLMs like Llama 3
               and Mixtral are not publicly available and very little is known
               about how they were created. In this work, we introduce FineWeb,
               a 15-trillion token dataset derived from 96 Common Crawl
               snapshots that produces better-performing LLMs than other open
               pretraining datasets. To advance the understanding of how best to
               curate high-quality pretraining datasets, we carefully document
               and ablate all of the design choices used in FineWeb, including
               in-depth investigations of deduplication and filtering
               strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion
               token collection of educational text filtered from FineWeb. LLMs
               pretrained on FineWeb-Edu exhibit dramatically better performance
               on knowledge- and reasoning-intensive benchmarks like MMLU and
               ARC. Along with our datasets, we publicly release our data
               curation codebase and all of the models trained during our
               ablation experiments.",
  month     =  jun,
  year      =  2024,
  file      = "Other/Penedo et al. 2024 - The FineWeb datasets - Decanting the web for the finest text data at scale.pdf"
}

@article{caliskan-et-al-weat,
    title = "Semantics derived automatically from language corpora contain human-like biases",
    keywords = "cs.AI, cs.CL, cs.CY, cs.LG",
    author = "Aylin Caliskan and Bryson, {Joanna J} and Arvind Narayanan",
    year = "2017",
    month = apr,
    day = "14",
    doi = "10.1126/science.aal4230",
    language = "English",
    volume = "356",
    pages = "183--186",
    journal = "Science",
    issn = "0036-8075",
    publisher = "American Association for the Advancement of Science",
    number = "6334",
}

@inproceedings{cao-etal-2022-intrinsic,
    title = "On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations",
    author = "Cao, Yang Trista  and
      Pruksachatkun, Yada  and
      Chang, Kai-Wei  and
      Gupta, Rahul  and
      Kumar, Varun  and
      Dhamala, Jwala  and
      Galstyan, Aram",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.62",
    doi = "10.18653/v1/2022.acl-short.62",
    pages = "561--570",
    abstract = "Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.",
}

@inproceedings{goldfarb-tarrant-etal-2021-intrinsic,
    title = "Intrinsic Bias Metrics Do Not Correlate with Application Bias",
    author = "Goldfarb-Tarrant, Seraphina  and
      Marchant, Rebecca  and
      Mu{\~n}oz S{\'a}nchez, Ricardo  and
      Pandya, Mugdha  and
      Lopez, Adam",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.150",
    doi = "10.18653/v1/2021.acl-long.150",
    pages = "1926--1940",
    abstract = "Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech.",
}

@inproceedings{kaneko-etal-2022-debiasing,
    title = "Debiasing Isn{'}t Enough! {--} on the Effectiveness of Debiasing {MLM}s and Their Social Biases in Downstream Tasks",
    author = "Kaneko, Masahiro  and
      Bollegala, Danushka  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.111",
    pages = "1299--1310",
    abstract = "We study the relationship between task-agnostic intrinsic and task-specific extrinsic social bias evaluation measures for MLMs, and find that there exists only a weak correlation between these two types of evaluation measures. Moreover, we find that MLMs debiased using different methods still re-learn social biases during fine-tuning on downstream tasks. We identify the social biases in both training instances as well as their assigned labels as reasons for the discrepancy between intrinsic and extrinsic bias evaluation measurements. Overall, our findings highlight the limitations of existing MLM bias evaluation measures and raise concerns on the deployment of MLMs in downstream applications using those measures.",
}

@inproceedings{nadeem-etal-2021-stereoset,
    title = "{S}tereo{S}et: Measuring stereotypical bias in pretrained language models",
    author = "Nadeem, Moin  and
      Bethke, Anna  and
      Reddy, Siva",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.416",
    doi = "10.18653/v1/2021.acl-long.416",
    pages = "5356--5371",
    abstract = "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.",
}

@inproceedings{nangia-etal-2020-crows,
    title = "{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.154",
    doi = "10.18653/v1/2020.emnlp-main.154",
    pages = "1953--1967",
    abstract = "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",
}

@techreport{webster-2020-sts,
title	= {Measuring and Reducing Gendered Correlations in Pre-trained Models},
author	= {Kellie Webster and Xuezhi Wang and Ian Tenney and Alex Beutel and Emily Pitler and Ellie Pavlick and Jilin Chen and Ed H. Chi and Slav Petrov},
year	= {2020},
URL	= {https://arxiv.org/abs/2010.06032}
}

@inproceedings{wu-etal-2025-rag,
    title = "Does {RAG} Introduce Unfairness in {LLM}s? Evaluating Fairness in Retrieval-Augmented Generation Systems",
    author = "Wu, Xuyang  and
      Li, Shuowei  and
      Wu, Hsin-Tai  and
      Tao, Zhiqiang  and
      Fang, Yi",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.669/",
    pages = "10021--10036",
    abstract = "Retrieval-Augmented Generation (RAG) has recently gained significant attention for its enhanced ability to integrate external knowledge sources into open-domain question answering (QA) tasks. However, it remains unclear how these models address fairness concerns, particularly with respect to sensitive attributes such as gender, geographic location, and other demographic factors. First, as language models evolve to prioritize utility, like improving exact match accuracy, fairness considerations may have been largely overlooked. Second, the complex, multi-component architecture of RAG methods poses challenges in identifying and mitigating biases, as each component is optimized for distinct objectives. In this paper, we aim to empirically evaluate fairness in several RAG methods. We propose a fairness evaluation framework tailored to RAG, using scenario-based questions and analyzing disparities across demographic attributes. Our experimental results indicate that, despite recent advances in utility-driven optimization, fairness issues persist in both the retrieval and generation stages. These findings underscore the need for targeted interventions to address fairness concerns throughout the RAG pipeline. The dataset and code used in this study are publicly available at this GitHub Repository."
}

