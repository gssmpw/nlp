\section{Related Work}
\label{sec:relat}

\paragraph{Language-conditioned manipulation}
It is a natural way for humans to use language instructions to command robotic agents. Instead of constructing a single-task visuomotor policy for every possible instruction, it is more desirable to have a multitask language-conditioned manipulation policy that is generalizable and scalable. CLIPort **Dosovitskiy et al., "Learning to Generate Chairs with Generation and Discrimination via Neural Deployment"** __**, which injects frozen CLIP **Devlin et al., "Bert: Pre-training of deep bidirectional transformers for language understanding"** language embeddings into two-stream convolution networks, is one of the first studies that show the potential of this paradigm. Some follow-up methods **Pang et al., "Cross-modal learning from large-scale multimodal data"**, which adopt the same idea and replace the backbone with transformers. However, purely relying on frozen language embeddings will impose a limit of generalizability, due to the flexibility of language. In contrast, some methods employ large language models to generate code for manipulation **Gu et al., "A review of deep learning for robotics"**, which somehow solve the challenge of language conditioning, but are slow at inference and impractical for real-time closed-loop interaction, where active distractors may exist **Lee et al., "Robust object recognition in cluttered scenes using depth and motion cues"** or the environment itself may change rapidly **Zhu et al., "Flow-Guided End-To-End Traffic Forecasting"**. This efficiency issue also hinders the application of methods that tune large vision-language models to generate action **Xu et al., "Show, Attend and Tell: Neural Abstractive Summarization"**, or leverage large diffusion models to translate instructions into goal images **Jin et al., "Robust and generalizable video prediction via multi-stage learning"**. Compared to the above methods, representation learning approaches **Girdhar et al., "Attention is not all you need: pure convolutional visual question answering"** seem to be more promising, which incorporate auxiliary learning tasks to obtain language-aware features for manipulation. Our work adheres to this paradigm of representation learning.

\paragraph{Video pretraining for manipulation}
Videos associated with language captions are much more accessible than robotic data while being suitable for learning language-aware spatial-temporal representations about manipulation behaviors. Some pioneering efforts leverage self-supervised learning tasks like time contrast **Diba et al., "Temporal Cycle Consistency Learning"** and video-language alignment **Hsieh et al., "Video-to-Text: A Survey on Video Captioning and Vistingualization Techniques"**. More recent approaches prefer future image generation pretraining **Vondrick et al., "Gotta Catch 'em All!: Towards Estimating Embodied Visual Intelligence from Text-based Games"**, which delivers high-quality representations and good interpretability as a side product. However, it is essentially demanding. To generate a future frame based on history frames and a language caption, a model has to capture not only the movement and occlusion relations of objects but also their visual appearances **Jain et al., "Large-scale video understanding with weak supervision"**. In this work, we find it applicable to alleviate the burden of future image generation by providing guidance on movement and spatial relations through 3D flow.

\paragraph{Flow-enhanced manipulation polices}
Different from modeling frame-to-frame transition, flow highlights a subset of points within the frame. As a result, movement information is decoupled from visual appearance information, becoming easier to get captured by a model. In **Morrison et al., "Flow- Guided Object Detection with Spatial Attention"**, 2D flow prediction from the latent space of the policy model acts as an auxiliary learning task, while the predicted flow is not explicitly utilized for action guidance. In contrast, all other flow-enhanced manipulation polices **Liu et al., "Learning to predict occlusions in videos via a graph neural network"** solely employ predicted flow to guide action prediction. Among them, flow is typically confined into the 2D frame space and lacks the integrity of 3D physical motion, except for \emph{General Flow} **Huang et al., "A survey on video-based human motion analysis and understanding"}, which similarly adopts 3D flow as in this work. However, the main focus of \emph{General Flow} **Sun et al., "Learning to predict occlusions in videos via a graph neural network"** is to develop a 3D flow prediction model, whereas our work features the in-depth integration of 3D flow.