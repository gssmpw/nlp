\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{amsmath, amssymb}
\usepackage{wasysym}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{tablefootnote}
\usepackage{subfigure}
\usepackage{pifont}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{placeins}

\pdfinfo{
   /Author (Homer Simpson)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

\IEEEoverridecommandlockouts
\usepackage{graphicx}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage[font=small,tableposition=top]{caption}

\usepackage{blindtext}
\title{ManiTrend: Bridging Future Generation and Action Prediction with 3D Flow for Robotic Manipulation}

\let\oldtwocolumn\twocolumn
\renewcommand\twocolumn[1][]{%
    \oldtwocolumn[{#1}{
    \begin{center}
        \includegraphics[width=0.92\textwidth]{intro.pdf}
           \captionof{figure}{Visualization of 3D flow, the future motion of particles within the 3D space, annotated with the SpatialTracker tool \cite{SpatialTracker}. The trajectory of each sampled particle is colored, with its end point marked in black. We highlight the value of 3D flow as a bridge between pixel-wise spatial-temporal modeling and fine-grained action prediction.}
           \label{fig:intro}
        \end{center}
    }]
}

\begin{document}

\author{Yuxin He \quad Qiang Nie\\
The Hong Kong University of Science and Technology (Guangzhou)\\
21S051047@stu.hit.edu.cn\\
qiangnie@hkust-gz.edu.cn
}


% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\authorblockN{Michael Shell\authorrefmark{1},
%Homer Simpson\authorrefmark{2},
%James Kirk\authorrefmark{3}, 
%Montgomery Scott\authorrefmark{3} and
%Eldon Tyrell\authorrefmark{4}}
%\authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
%\authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}

\maketitle

\begin{abstract}
Language-conditioned manipulation is a vital but challenging robotic task due to the high-level abstraction of language. To address this, researchers have sought improved goal representations derived from natural language. In this paper, we highlight 3D flow - representing the motion trend of 3D particles within a scene - as an effective bridge between language-based future image generation and fine-grained action prediction. To this end, we develop ManiTrend, a unified framework that models the dynamics of 3D particles, vision observations and manipulation actions with a causal transformer. Within this framework, features for 3D flow prediction serve as additional conditions for future image generation and action prediction, alleviating the complexity of pixel-wise spatiotemporal modeling and providing seamless action guidance. Furthermore, 3D flow can substitute missing or heterogeneous action labels during large-scale pretraining on cross-embodiment demonstrations. Experiments on two comprehensive benchmarks demonstrate that our method achieves state-of-the-art performance with high efficiency. Our code and model checkpoints will be available upon acceptance.

\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:intro}

The ability to manipulate under language instruction is significant for a generalist robot. However, given the high-level abstract nature of language and the domain gap between language and vision, it is challenging to effectively condition visuomotor policy on language instruction \cite{pmlr-v164-jang22a}. 

Recently, many approaches have been introduced to address this challenge from different perspectives. Among them, five paradigms lead the trend, including: 1) text-image alignment pretraining \cite{nair2023r3m, ma2023liv, karamcheti2023voltron, nguyen2024robotic}; 2) future image generation pretraining \cite{wuunleashing, zeng2024learning, yang2024spatiotemporal, he2024large}; 3) leveraging large vision-language models (VLMs); 4) conditioning on generated goal images \cite{blackzero, shridhargenerative, li2024gr}; 5) conditioning on predicted flow \cite{vecerik2024robotap, wen2023any, bharadhwaj2024track2act, xuflow}. Despite the promising results they have achieved, each of them has its own drawback. Text-image alignment pretraining focuses on learning static semantic features while neglecting temporal features. Future image generation pretraining can deliver spatially and temporally aware representation but enforces modeling all pixels, many of which are irrelevant to physical motion. Large VLMs and text-to-image models pretrained on Internet data excel at language understanding and can generate code/low-level actions for control or goal images for guidance, but all of them are slow to generate and infeasible for real-time interaction, where active distractors may exist \cite{yu2024hierarchical} or the environment may change rapidly \cite{zhang2024catch}.

Flow-conditioning methods \cite{bharadhwaj2024gen2act, vecerik2024robotap, wen2023any, bharadhwaj2024track2act, xuflow} avoid the above drawbacks by leveraging the future motion of a set of points predicted from language instruction and image observation to effectively and efficiently guide policy. However, existing studies all leave the flow prediction subtask to another standalone model, which prohibits its potential effect on enhancing representation learning. Besides, most of them use 2D flow within the image frame space, lacking the integrity of 3D motion.

In this work, we come up with \textbf{ManiTrend}, a novel framework that models the dynamics of 3D particles, vision observations and demonstrated actions with a single transformer. By pretraining on captioned videos and using 3D flow prediction as an auxiliary learning task, our model develops a motion-aware space-time representation space under lingual context. During imitation finetuning, 3D flow further bridges future image generation and fine-grained action prediction, leveraging its intermediate granularity. The integration of 3D flow alleviates the challenge of modeling pixel dynamics solely based on language, while also offering direct guidance for Cartesian end-effector actions in 3D space.

To label the motion of 3D particles on RGB videos, we leverage SpatialTracker \cite{SpatialTracker} to track the position of densely-sampled grid points within a projected 3D space during data preprocessing. The labels of 3D flow (as illustrated in Figure \ref{fig:intro}) are then used to train our model to predict the future motion of a set of particles conditioned on language and observations. Features for 3D flow prediction also serve as additional condition for future image generation and action prediction, injected through various techniques such as self-attention and adaptive layer normalization.

To verify the proposed framework, we conduct pretraining across four real-world datasets and one simulated dataset with videos of human/robot manipulation, and carry out downstream experiments on two benchmark datasets (CALVIN \cite{mees2022calvin}, LIBERO \cite{liu2024libero}). Experimental results show that our method achieves high success rates that are competitive to or better than state-of-the-art (SoTA) while being more efficient.

Our contributions can be summarized as follows:
\begin{itemize}
    \item We rediscover 3D flow as suitable a bridge between future generation and action prediction for robotic manipulation because of its intermediate granularity.
    \item We propose ManiTrend, a unified framework that tracks the dynamics of 3D particles, vision observations and manipulation actions with a single model.
    \item Experiments on two challenging benchmarks empirically show that our method achieves SoTA performance and is efficient by leveraging 3D flow. Our project will be fully open-source.
\end{itemize}

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{model.pdf}
    \caption{An overview of ManiTrend, the proposed end-to-end framework that tracks the dynamics of 3D particles, vision observations and manipulation actions in a unified manner. Better viewed in color.}
    \label{fig:model}
\end{figure*}

\section{Related Work}
\label{sec:relat}

\paragraph{Language-conditioned manipulation}
It is a natural way for humans to use language instructions to command robotic agents. Instead of constructing a single-task visuomotor policy for every possible instruction, it is more desirable to have a multitask language-conditioned manipulation policy that is generalizable and scalable. CLIPort \cite{shridhar2022cliport}, which injects frozen CLIP \cite{radford2021learning} language embeddings into two-stream convolution networks, is one of the first studies that show the potential of this paradigm. Some follow-up methods \cite{bharadhwaj2024roboagent} adopt the same idea and replace the backbone with transformers. However, purely relying on frozen language embeddings will impose a limit of generalizability, due to the flexibility of language. In contrast, some methods employ large language models to generate code for manipulation \cite{10160591,huang2023instruct2act}, which somehow solve the challenge of language conditioning, but are slow at inference and impractical for real-time closed-loop interaction, where active distractors may exist \cite{yu2024hierarchical} or the environment itself may change rapidly \cite{zhang2024catch}. This efficiency issue also hinders the application of methods that tune large vision-language models to generate action \cite{brohan2023rt,kim2024openvla}, or leverage large diffusion models to translate instructions into goal images \cite{blackzero, shridhargenerative, li2024gr}. Compared to the above methods, representation learning approaches \cite{nair2023r3m, ma2023liv, karamcheti2023voltron, nguyen2024robotic, wuunleashing, zeng2024learning, yang2024spatiotemporal, bharadhwaj2024gen2act} seem to be more promising, which incorporate auxiliary learning tasks to obtain language-aware features for manipulation. Our work adheres to this paradigm of representation learning.

\paragraph{Video pretraining for manipulation}
Videos associated with language captions are much more accessible than robotic data while being suitable for learning language-aware spatial-temporal representations about manipulation behaviors. Some pioneering efforts leverage self-supervised learning tasks like time contrast \cite{nair2023r3m,mavip} and video-language alignment \cite{pmlr-v164-jang22a,karamcheti2023voltron}. More recent approaches prefer future image generation pretraining \cite{wuunleashing, zeng2024learning, yang2024spatiotemporal, he2024large}, which delivers high-quality representations and good interpretability as a side product. However, it is essentially demanding. To generate a future frame based on history frames and a language caption, a model has to capture not only the movement and occlusion relations of objects but also their visual appearances \cite{wen2023any}. In this work, we find it applicable to alleviate the burden of future image generation by providing guidance on movement and spatial relations through 3D flow.

\paragraph{Flow-enhanced manipulation polices}
Different from modeling frame-to-frame transition, flow highlights a subset of points within the frame. As a result, movement information is decoupled from visual appearance information, becoming easier to get captured by a model. In \cite{bharadhwaj2024gen2act}, 2D flow prediction from the latent space of the policy model acts as an auxiliary learning task, while the predicted flow is not explicitly utilized for action guidance. In contrast, all other flow-enhanced manipulation polices \cite{vecerik2024robotap, wen2023any, bharadhwaj2024track2act, xuflow} solely employ predicted flow to guide action prediction. Among them, flow is typically confined into the 2D frame space and lacks the integrity of 3D physical motion, except for \emph{General Flow} \cite{xuflow}, which similarly adopts 3D flow as in this work. However, the main focus of \emph{General Flow} \cite{xuflow} is to develop a 3D flow prediction model, whereas our work features the in-depth integration of 3D flow.

\section{Method}
\label{sec:method}

A language-conditioned action-chunking policy $\pi_\theta(\mathbf{a}_{t:t+K-1} | \mathbf{o}_{t-T+1:t}, \mathbf{c})$ takes a language command $\mathbf{c}$ and observations $\mathbf{o}_{t-T+1:t}$ (main/wrist-view images, robot proprioception states) with historical window size $T$ as input, and outputs a chunk of actions $\mathbf{a}_{t:t+K-1}$ of length $K$. The three variables belong to quite distinct modalities, which is why auxiliary objectives that enhance representation learning are necessary. ManiTrend incorporates the prediction of 3D flow $\boldsymbol{\tau}_{t:t+L-1} \in \mathbb{R}^{L \times P \times 3}$ and future images $\mathbf{i}^\text{main/wrist}_{t+S}$ in main/wrist views, where $P$ is the number of track points, $L$ is the flow length, $S$ is the time shift of future images. In our formulation, the three dimensions of 3D flow correspond to x, y (in pixel coordinates), and absolute depth, respectively. The overall framework of our method is shown in Figure \ref{fig:model}.

Following previous research \cite{wuunleashing, li2024gr}, the language command is encoded into a vector $\mathbf{c} \in \mathbb{R}^d$ ($d$ is the model dimension) with CLIP \cite{radford2021learning} text encoder and a linear projection layer. Each main/wrist-view image is encoded into a matrix $\mathbb{I} \in \mathbb{R}^{(1+r) \times d}$ with MAE \cite{he2022masked} and a perceiver resampler \cite{jaegle2021perceiver}, where the first vector is the \emph{CLS} token and the rest $r$ vectors are resampled from the patch tokens. The robot proprioception state, which includes the 6D end-effector pose and the binary gripper status (closed or open), is embedded into a vector $\mathbf{p} \in \mathbb{R}^d$ with linear layers.

The construction of queries for flow, future images and action chunk is introduced below: 

\paragraph{Flow Query}
During training and inference, the model predicts the future 3D motion of grid points or randomly-sampled points that are near to grids. We initialize $l$ learnable vectors $\mathbb{Q}^\text{flow} \in \mathbb{R}^{l \times d}$, as the embedding vectors of flow query. To include information about which points to track, the starting pixel coordinates of the sampled points are encoded into a vector via a linear layer, which is added to each flow query vector.

\paragraph{Future Query}
Future query for main/wrist-view image is instantiated as $1+r$ learnable vectors. The number of future query vectors is equal to the number of image embedding vectors, so appropriate capacity is equipped to reconstruct the representation of a future image.

\paragraph{Action Chunk Query}
In GR-1 \cite{wuunleashing}, a single vector is responsible for predicting a single-step action. To expand that for action chunking (predicting a sequence of actions \cite{zhao2023learning}), we additionally initialize $K$ learnable position embedding vectors. And a global learnable vector is added to each position embedding vector to form the final action chunk query.

\subsection{Causal Dynamics Modeling with 3D Flow}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.88\linewidth]{env.pdf}
    \caption{Environments for our experiments. CALVIN involves 34 manipulation tasks and 4 scenes of different colors, textures and object placements. LIBERO features 4 evaluation task suites that challenge different dimensions of capability.}
    \label{fig:env}
\end{figure*}

The language/vision/proprioception input along with various queries for each timestep are organized into a sequence. And the sequences from timestep $t-T+1$ to timestep $t$ are concatenated and fed into a causal transformer. The causal transformer is essentially a transformer decoder, equipped with a carefully designed attention mask. Within it, all language/vision/proprioception tokens attend to their historical counterparts to establish temporal correlation. All queries attend to current and historical language/vision tokens, whereas action chunk queries additionally attend to current and historical proprioception tokens.

The interaction between various queries is centered on 3D flow, as detailed below.

\subsubsection{3D Flow for Future Image Generation}
Since the time shift $S$ of the target future image is typically smaller than the flow length $L$ (as it is harder to predict the future image than the future point position), we take a fraction of latent representation of flow query as an extra condition for future image generation. Concretely, within each transform block, the first $\lceil\frac{l(S+1)}{L}\rceil$ flow query vectors are pooled into a feature vector via 1D convolution. An MLP then turned the feature vector into gate, shift and scale vectors for the adaptive layer norm applied to future query before the Attention/MLP layer of the transform block. We employ adaptive layer norm \cite{perez2018film} for conditioning because it is proven to be more effective than other approaches by recent research \cite{peebles2023scalable,zhang2024tora}.
% \begin{gather}
%     [\boldsymbol{\alpha}_1^{i} , \boldsymbol{\beta}_1^{i}, \boldsymbol{\gamma}_1^i, \boldsymbol{\alpha}_2^{i} , \boldsymbol{\beta}_2^{i}, \boldsymbol{\gamma}_2^i] = \mathrm{Linear}(\mathrm{SiLU}(\mathbf{x}^i)) \\
%     \mathbf{x}^i =\mathbf{x}^i+(1+\boldsymbol{\alpha}_1^{i})\mathrm{MHA}(\boldsymbol{\gamma}_1^i\mathrm{LayerNorm}(\mathbf{x}^i+\boldsymbol{\beta}_1^i)),\\
%     \mathbf{x}^i =\mathbf{x}^i+(1+\boldsymbol{\alpha}_2^{i})\mathrm{FFN}(\boldsymbol{\gamma}_2^i\mathrm{LayerNorm}(\mathbf{x}^i+\boldsymbol{\beta}_2^i))
% \end{gather}

\subsubsection{3D Flow for Action Chunk Prediction}
By default, we set the flow length $L$ to be the same as the action chunk length $K$, since the two modalities are so similar in nature. And we enhance action prediction with the flow representation in a straightforward way, i.e., letting all action query vectors attend to all flow query vectors of the same timestep within each transform block.

\subsection{Output Decoding}
Queries are decoded into prediction of 3D flow, images and action chunk by downstream decoders, as described below.
\subsubsection{\textbf{3D Flow Prediction}}
The flow decoder resembles the track transformer proposed in \cite{wen2023any}, which first initiates a set of masked patch tokens from starting pixel coordinates, then updates patch representation conditioned on context and linearly transforms the final patch representation into predicted flow $\hat{\boldsymbol{\tau}}_{t:t+L-1} \in \mathbb{R}^{L \times P \times 3}$. The main difference is that the context used by \cite{wen2023any} is the language/vision representation, whereas the context here is the final representation of flow query.

\subsubsection{Future Image Generation}
The image decoder works in a manner similar to the flow decoder, except that the input patch tokens are predefined as the sum of a learnable mask vector and 2D sin-cos positional embedding, which is a common practice in image reconstruction domain \cite{esser2021taming,he2022masked}.

\subsubsection{Action Chunk Prediction}
The $K$ action query vectors are directly transformed into $\hat{\mathbf{a}}_{t:t+K-1}$, predicted actions corresponding to different current/future timesteps through an MLP layer. In this paper, we adopt the widely-used delta Cartesian action space, where an action includes translation (delta of $x, y, z$), rotation (delta of roll, pitch, yaw), and target binary closeness of the end-effector.

\subsection{Learning Loss}
The end-to-end learning loss $\mathcal{L}$ over a data frame is the combination of 3D flow prediction loss $\mathcal{L}_\text{flow}$, future image generation $\mathcal{L}_\text{img}$ and action chunk prediction loss $\mathcal{L}_\text{act}$, defined as follows:
\begin{align}
    \mathcal{L}_\text{flow} = & \mathrm{MSE}(\hat{\boldsymbol{\tau}}_{t:t+L-1}, \boldsymbol{\tau}_{t:t+L-1}) \\
    \mathcal{L}_\text{img} = & \mathrm{MSE}(\hat{\mathbf{i}}^\text{main}_{t+S}, \mathbf{i}^\text{main}_{t+S}) + \mathrm{MSE}(\hat{\mathbf{i}}^\text{wrist}_{t+S}, \mathbf{i}^\text{wrist}_{t+S})\\
    \mathcal{L}_\text{act} = & \alpha \cdot \mathrm{SmoothL1}(\hat{\mathbf{a}}_{t:t+K-1}[:6], {\mathbf{a}}_{t:t+K-1}[:6]) \\
    & + \mathrm{BCE}(\hat{\mathbf{a}}_{t:t+K-1}[6], {\mathbf{a}}_{t:t+K-1}[6]) \nonumber\\
    \mathcal{L} = & \beta \cdot \mathcal{L}_\text{flow} + \mathcal{L}_\text{img} + \mathcal{L}_\text{act}
\end{align}
where $\mathrm{MSE}$, $\mathrm{SmoothL1}$ and $\mathrm{BCE}$ mean Mean Squared Error, Smooth L1 and Binary Cross Entropy respectively. 
By default, we empirically set $\alpha=100$ and $\beta=5$.

\begin{table*}[htb]
	\centering
    \caption{Overall performance of ManiTrend and baseline methods on the CALVIN benchmark under the in-domain setting (D$\rightarrow$D, training and testing on scene D). During evaluation, 1000 chains of task instructions are randomly sampled, and the success rates of consecutive five tasks are recorded and averaged over the 1000 chains.}
    \label{d_calvin}
    \scalebox{1.0}[1.0]{
	\begin{tabular}{lcccccc}
		\toprule
        \multirow{2}{*}{Method} & \multicolumn{5}{c}{No. Instructions in a Row (1000 Chains)} & \multirow{2}{*}{Avg. \# Solved Tasks} \\
        \cmidrule(lr){2-6} & {\qquad1\qquad} & {\qquad2\qquad} & {\qquad3\qquad} & {\qquad4\qquad} & {\qquad5\qquad} & \\
        \midrule
        HULC \cite{mees2022matters} & 82.7\% & 64.9\% & 50.4\% & 38.5\% & 28.3\%	& 2.64 \\
        RoboFlamingo \cite{li2023vision} & 86.0\% & 71.4\% & 58.5\% & 46.0\% & 34.9\% & 2.97 \\
        MDT \cite{reuss2024multimodal} & 93.7\% & 84.5\% & 74.1\% & 64.4\% & 55.6\% & 3.72 \\
        RoboUniView \cite{liu2024robouniview} & \textbf{96.2\%} & \textbf{88.8\%} & \textbf{77.6\%} &  \underline{66.6\%} &  \underline{56.3\%} &  \underline{3.85} \\
        ManiTrend (Ours) &  \underline{95.1\%} &  \underline{86.3\%} &  \underline{76.4\%} & \textbf{68.3\%} & \textbf{60.6\%} & \textbf{3.87} \\
		\bottomrule
	\end{tabular}
    }
\end{table*}

% \begin{table}[htb]
% 	\centering
%     \caption{Overall performance of ManiTrend and baseline methods on the LIBERO benchmark. For each task suite, the success rate is calculated over 50 rollouts for each task within the task suite. The overall success rate is averaged over the results on the four task suites.}
%     \label{main_libero}
%     \scalebox{1.0}[1.0]{
% 	\begin{tabular}{lccccc}
% 		\toprule
%         Method & Spatial & Object & Goal & Long & Avg. \\
%         \midrule
%         Transformer-BC \cite{liu2024libero} & 71.8\% & 71.0\% & 76.3\% & 24.2\% & 60.8\% \\
%         DP-Language \cite{chi2023diffusion,khazatsky2024droid} & 78.3\% & 92.5\% & 68.3\% & 50.5\% & 72.4\% \\
%         OpenVLA \cite{kim2024openvla} & 84.7\% & 88.4\% & 79.2\% & 53.7\% & 76.5\% \\
%         GR-1 \cite{wuunleashing} & 93.4\% & 93.4\% & 89.0\% & 84.2\% & 90.1\% \\
%         ATM \cite{wen2023any} & 84.0\% & 89.4\% & 79.6\% & 65.2\% & 79.6\% \\
%         ManiTrend (Ours) & 93.8\% & 95.0\% & 93.8\% & 88.6\% & 92.8\% \\
% 		\bottomrule
% 	\end{tabular}
%     }
% \end{table}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{main_libero.png}
    \caption{Performance comparison on the 4 task suites of LIBERO benchmark. For each task suite, the success rate is calculated over 50 rollouts for each task within the task suite. The overall success rate is averaged over the results on the four task suites.}
    \label{main_libero}
\end{figure*}

\subsection{Cross-embodiment Pretraining}
Our framework can be easily extended for large-scale pretraining on cross-embodiment manipulation demonstrations. Here, we do not bother to align the heterogeneous action labels, which is almost impossible because most datasets have different coordinates but do not provide relevant meta-data. Instead, we take advantage of 3D flow as an intermediary to learn the modes of physical motion within a scene. To avoid the negative impact of heterogeneous proprioception, we mask out the proprioception state tokens. Since wrist-view observation is unavailable in many datasets, we exclude wrist-view images from input and output during pretraining.



\section{Experiments}
\label{sec:exp}

Our experiments aim at addressing the following research questions:

\begin{itemize}
    \item \textbf{Q1:} How is the performance of ManiTrend compared to SoTA language-conditioned manipulation policy models?
    \item \textbf{Q2:} Does modeling 3D flow help future generation?
    \item \textbf{Q3:} Does modeling 3D flow help action prediction?
    \item \textbf{Q4:} Is 3D flow superior to 2D flow?
    \item \textbf{Q5:} How much does cross-embodiment pretraining contribute to the performance of ManiTrend?
\end{itemize}

\begin{table*}[htb]
	\centering
    \caption{Comparison of zero-shot scene transfer ability on CALVIN (ABC$\rightarrow$D, training on scenes A, B, C and testing on scene D).}
    \label{abc_calvin}
    \scalebox{1.0}[1.0]{
	\begin{tabular}{lcccccc}
		\toprule
        \multirow{2}{*}{Method} & \multicolumn{5}{c}{No. Instructions in a Row (1000 Chains)} & \multirow{2}{*}{Avg. \# Solved Tasks} \\
        \cmidrule(lr){2-6} & {\qquad1\qquad} & {\qquad2\qquad} & {\qquad3\qquad} & {\qquad4\qquad} & {\qquad5\qquad} & \\
        \midrule
        RoboFlamingo \cite{li2023vision} & 82.4\% & 61.9\% & 46.6\% & 33.1\% & 23.5\%	& 2.47 \\
        GR-1 \cite{wuunleashing} & 85.4\% & 71.2\% & 59.6\% & 49.7\% & 40.1\% & 3.06 \\
        3D Diffuser Actor \cite{ke20243d} & 92.2\% & 78.7\% & 63.9\% & 51.2\% & 41.2\% & 3.27 \\ 
        RoboUniView \cite{liu2024robouniview} & 94.2\% & 84.2\% & 73.4\% & 62.2\% & 50.7\% & 3.64 \\
        GR-MG \cite{li2024gr} & 96.8\% & 89.3\% & 81.5\% & 72.7\% & 64.4\% & 4.04 \\
        ManiTrend (Ours) & 95.0\% & 86.6\% & 77.9\% & 71.1\% & 63.0\% & 3.96 \\
		\bottomrule
	\end{tabular}
    }
\end{table*}

\begin{table}[htb]
	\centering
        \caption{Comparison with SoTA language-conditioned manipulation policies in terms of inference speed. ``T2I" stands for pretrained text-to-image models, e.g. Stable Diffusion.}
    \label{infer_speed}
    \scalebox{1.0}[1.0]{
	\begin{tabular}{lccc}
		\toprule
        Method & W. Large VLMs & W. Large T2I & Latency (ms) \\
        \midrule
        OpenVLA \cite{kim2024openvla} & \checkmark & & 176 \\
        RoboUniView \cite{liu2024robouniview}  & \checkmark & & 105 \\
        GR-MG \cite{li2024gr}  & & \checkmark & 121 \\
        ATM \cite{wen2023any}  & & & 38 \\
        ManiTrend (Ours) & & & 42 \\
		\bottomrule
	\end{tabular}
    }
\end{table}

\subsection{Implementation Details}

When preprocessing training data with SpatialTracker \cite{SpatialTracker}, we employ a sampling strategy that results in around $\frac{1}{4}$ of the sampled points moving in the video and the others staying still. We do so in order to maintain a trade-off balance between teaching the model the motion of particles and narrowing the training-inference gap (since track points are just uniformly sampled from grids during inference).

We instantiate ManiTrend with 12 transformer blocks and model dimension of 384. The model weight is initialized with the GR-1 \cite{wuunleashing} checkpoint, which shares a similar architecture (except for the 3D flow related parts) and has been pretrained on Ego4D \cite{grauman2022ego4d} videos. Our pretraining is then conducted on 44K trajectories from 5 datasets (RH20T \cite{fang2023rh20t}, Bridge \cite{walke2023bridgedata}, Berkeley UR5 \cite{BerkeleyUR5Website} , Mutex \cite{shah2023mutex} and LIBERO \cite{liu2024libero}) that cover 4 kinds of embodiments (humans, Franka robots, UR5 robots and WidowX robots). The pretraining process lasts for 35 epochs, which takes 3 days on 4 NVIDIA 4090 GPUs. After that, we finetune and evaluate ManiTrend on downstream manipulation benchmarks CALVIN and LIBERO (Figure \ref{fig:env}).

\textbf{CALVIN} \cite{mees2022calvin} encompasses 34 manipulation tasks and 4 scenes (A, B, C, D) that are different in table colors, textures and object placements. The scenes along with a 7-DOF Franka robot are simulated via PyBullet. 5K expert trajectories (collected via teleoperation) with language instructions are provided for each scene. The benchmark features a challenging evaluation protocol: for each round, a chain of 5 task instructions is randomly sampled, and once the robot fails in one task, the following tasks are regarded as failed. Typically, there are 1000 rounds and the success flags of the consecutive five tasks are recorded for calculating the stage-wise success rates.

\textbf{LIBERO} \cite{liu2024libero} involves 130 tasks and 5 task suites (Spatial, Object, Goal, Long and 90). Each task suite has 10 tasks, except for the last task suite, which has 90 tasks. 50 (scripted) expert trajectories are provided for each task. Although the benchmark is originally designed for the life-long learning regime, it is also a good testbed for imitation learning algorithms. Following previous research \cite{wen2023any,kim2024openvla}, we conduct finetuning and evaluation within each of the Spatial, Object, Goal, Long task suites.

\subsection{In-domain Evaluation on CALVIN and LIBERO}

For in-domain evaluation, language-conditioned multitask policies are trained with all expert data provided for a scene and tested on the same scene.

\subsubsection{Baselines}
\begin{itemize} 
    \item \textbf{HULC} \cite{mees2022matters} is a hierarchical language-conditioned policy that leverages a global latent plan and representations refined with temporal visuo-lingual alignment.
    \item \textbf{RoboFlamingo} \cite{li2023vision} utilizes VLMs for single-step vision language comprehension, and LSTM for modeling history information and predicting action.
    \item \textbf{RoboUniView} \cite{liu2024robouniview} enhances RoboFlamingo with a unified view representation, which is learned from the 3D occupancy task.
    \item \textbf{MDT} \cite{reuss2024multimodal} is a transformer-based diffusion policy with the language representation aligned with the goal image representation.
    \item \textbf{GR-1} \cite{wuunleashing} is a GPT-style policy pretrained with text-to-video generation. It is originally tested on CALVIN and we additionally test it on LIBERO.
    \item \textbf{GR-MG} \cite{li2024gr} enhances GR-1 with goal images generated by a finetuned InstructPix2Pix \cite{brooks2023instructpix2pix} model.
    \item \textbf{Transformer-BC} \cite{liu2024libero} is a vanilla language-conditioned transformer-based policy.
    \item \textbf{DP-Language} \cite{chi2023diffusion, khazatsky2024droid} is a variant of diffusion policy, where language embedding is injected via concatenation.
    \item \textbf{OpenVLA} \cite{kim2024openvla} directly extends large VLMs to predict discretized action by finetuning on OXE \cite{o2023open} data.
    \item \textbf{ATM} \cite{wen2023any} incorporates 2D flow predicted by a track transformer to guide a transformer policy. We reproduce the results of it on LIBERO by training with full data.
\end{itemize}

\subsubsection{Results}
The results of comparison between ManiTrend and baselines under the in-domain setting (D$\rightarrow$D, training and testing on scene D) of CALVIN are shown in Table \ref{d_calvin}. It can be seen that ManiTrend surpasses SoTA methods MDT \cite{reuss2024multimodal} and RoboUniView \cite{liu2024robouniview} by solving more consecutive tasks in average. The higher success rates on the last two tasks of a chain suggests that ManiTrend are more robust to unseen object placements and proprioception states (since the placement and state get more random as the rollout persists).

Figure \ref{main_libero} displays the in-domain evaluation results on the Spatial, Object, Goal and Long task suites of LIBERO. According to the results, ManiTrend consistently surpasses all baselines on the 4 task suites by a large margin. Interestingly, GR-1 \cite{wuunleashing} outperforms ATM \cite{wen2023any}, suggesting the guidance of 2D flow not necessarily works better than learning future image generation. And the superior performance of ManiTrend compared to GR-1 and ATM demonstrates the advantage of organically combining the two paradigms.

\begin{table*}[htb]
	\centering
    \caption{Results of Ablating 3D Flow for Future Generation.}
    \label{ablate_flow_for_future}
    \scalebox{1.0}[1.0]{
	\begin{tabular}{cc|cccccc}
		\toprule
        \multirow{2}{*}{3D Flow Prediction} & \multirow{2}{*}{3D Flow Conditioning} & \multicolumn{3}{c}{Pretrain (Validation)} & \multicolumn{3}{c}{CALVIN (D$\rightarrow$D, Validation)} \\
        \cmidrule(lr){3-5} \cmidrule(lr){6-8} & & PSNR$\uparrow$ & SSIM$\uparrow$ & FID$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & FID$\downarrow$ \\
        \midrule
         &  & 22.80 & 0.6951 & 299.5 & 27.84 & 0.8608 & 110.3\\
		\checkmark & & 22.92 & 0.6966 & 295.7 & 27.95 & 0.8619 & 107.2  \\
        \checkmark & \checkmark & 23.29 & 0.7011 & 289.2 & 28.29 & 0.8670 & 102.2 \\
		\bottomrule
	\end{tabular}
    }
\end{table*}

\begin{table*}[htb]
	\centering
    \caption{Results of Ablating 3D Flow for Action Prediction on CALVIN (D$\rightarrow$D).}
    \label{ablate_flow_for_act}
    \scalebox{0.95}[0.95]{
	\begin{tabular}{ccc|cccccc}
		\toprule
        \multirow{2}{*}{Dimension of Flow} & \multirow{2}{*}{Flow Prediction} & \multirow{2}{*}{Flow Conditioning} & \multicolumn{5}{c}{No. Instructions in a Row (1000 Chains)} & \multirow{2}{*}{Avg. \# Solved Tasks} \\
        \cmidrule(lr){4-8} & & & {\qquad1\qquad} & {\qquad2\qquad} & {\qquad3\qquad} & {\qquad4\qquad} & {\qquad5\qquad} & \\
        \midrule
        - &  &  & 90.8\% & 81.1\% & 73.4\% & 67.4\% & 59.9\% & 3.80 \\
        3D & \checkmark &  & 93.3\% & 85.3\% & 76.6\% & 69.0\% & 61.0\% & 3.84 \\
        3D & \checkmark & \checkmark & {95.1\%} & {86.3\%} & {76.4\%} & {68.3\%} & {60.6\%} & {3.87} \\
        2D & \checkmark & \checkmark & 94.0\% & 85.6\% & 76.3\% & 68.5\% & 60.6\% & 3.84 \\
		\bottomrule
	\end{tabular}
    }
\end{table*}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.85\linewidth]{ablate_pretrain.png}
    \caption{Performance on CALVIN (ABC$\rightarrow$D) of MainTrend pretrained with different amounts of data.}
    \label{ablate_pretrain}
\end{figure*}

\subsection{Zero-shot Scene Transfer on CALVIN}

To evaluate the scene generalization capability of ManiTrend, we conduct zero-shot scene transfer experiments on CALVIN (ABC$\rightarrow$D, training on scenes A, B, C and testing on scene D). The evaluation results are illustrated in Table \ref{abc_calvin}. We can see that ManiTrend outperforms most SoTA methods except for GR-MG, which takes advantage of the guidance of goal image generated by InstructPix2Pix \cite{brooks2023instructpix2pix}, a large image editing diffusion model pretrained on high-quality Internet data. It is not surprising, as is known that Internet data breed the generalization ability of modern foundation models. And we believe that if pretraining ManiTrend with data of equal diversity and quality, ManiTrend will achieve similar level of generalization. Moreover, our method is much faster than GR-MG as analyzed in Section \ref{infer_efficiency}.

\subsection{Inference Efficiency}
\label{infer_efficiency}

In addition to the task success rate, inference efficiency is also an important dimension for evaluating a manipulation policy, which is always overlooked or concealed by recent research. If a policy is slow, it will not be able to react timely to a changing environment. Hence, a generalist manipulation policy should be a fast one. We compare the inference latencies of ManiTrend and SoTA language-conditioned manipulation policies running with a NVIDIA 4090 GPU. As shown in Table \ref{infer_speed}, methods that rely on large VLMs or text-to-image models to gain generalization ability, e.g. OpenVLA, RoboUniView and GR-MG tend to be quite slow, with latencies above 100 ms. Note that if we running these methods on an inferior on-board GPU instead of RTX 4090, the latencies will multiply, making it hard to even reach an operation frequency of 3 Hz. In contrast, methods that leverage flow for guidance, e.g. ManiTrend and ATM are able to operate about 3 times faster.


\subsection{The Effect of 3D Flow on Future Generation}

To verify the contribution of 3D flow to future generation, we compare the future image generation performance of ManiTrend with and without 3D flow prediction/conditioning. The results are illustrated in Table \ref{ablate_flow_for_future}. After removing the adaptive layer norm module that conditions future generation on 3D flow feature, the measured quality of generated future images on both the validation sets of Pretrain data and CALVIN D$\rightarrow$D data substantially degrades. Further removing the 3D flow prediction module leads to further degradation, but by a much smaller margin. This indicates that simply learning to predict the 3D flow slightly contributes to learning future generation. Whereas injecting 3D flow feature into the future generation module does help a lot.

% \begin{table*}[htb]
% 	\centering
%     \caption{Performance on CALVIN (ABC$\rightarrow$D) of MainTrend pretrained with different amounts of data.}
%     \label{ablate_pretrain}
%     \scalebox{1.0}[1.0]{
% 	\begin{tabular}{ccc|cccccc}
% 		\toprule
%         \multirow{2}{*}{\# Datasets} & \multirow{2}{*}{\# Traj.} & \multirow{2}{*}{\# Frames} & \multicolumn{5}{c}{No. Instructions in a Row (1000 Chains)} & \multirow{2}{*}{Avg. \# Solved Tasks} \\
%         \cmidrule(lr){4-8} & & & {\qquad1\qquad} & {\qquad2\qquad} & {\qquad3\qquad} & {\qquad4\qquad} & {\qquad5\qquad} & \\
%         \midrule
%          - & - & - & 89.2\% & 78.9\% & 70.1\% & 61.0\% & 51.3\% & 3.50 \\
%          2 & 37K & 2129K & 92.9\% & 84.3\% & 75.1\% & 67.6\% & 60.8\% & 3.81 \\
%          3 & 41K & 2765K & 93.7\% & 85.0\% & 76.3\% & 68.7\% & 61.8\% & 3.87 \\
%          5 & 44K & 3237K & 95.0\% & 86.6\% & 77.9\% & 71.1\% & 63.0\% & 3.96 \\
% 		\bottomrule
% 	\end{tabular}
%     }
% \end{table*}

\subsection{The Effect of 3D Flow on Action Prediction}
We conduct ablation study on in-domain CALVIN evalation setting (D$\rightarrow$D) to look into the effect of 3D flow on action prediction. As shown in Table \ref{ablate_flow_for_act}, both 3D flow prediction and the attention from 3D flow query to action chunk query contribute to action prediction. Interestingly, adding the attention from 3D flow query substantially increases the success rates of first 2 tasks, but somehow hurts the performance on last few tasks, which means the model tends to overfit the object placements and proprioception states seen in training data. It is reasonable, considering the deep correlation between 3D flow and particles of objects/robots.

After replacing 3D flow with 2D flow, the task success rates decrease overall, demonstrating the superiority of 3D flow over 2D flow on guiding fine-grained control.

\subsection{The Effect of Cross-embodiment Pretraining}
To quantify the contribution of cross-embodiment pretraining to downstream performance, especially to generalization ability, we ablate the amount of pretraining data and measure the resulting performance on CALVIN (ABC$\rightarrow$D). It can be observed from Figure \ref{ablate_pretrain} that:
1) When the amount of pretraining data goes from 0 to 2129K frames, the overall performance is greatly boosted, indicating the effectiveness of our pretraining. 2) The performance gain is more significant when adding 2 extra datasets than when adding 1 dataset with similar number of frames. This indicates that the diversity of pretraining data is more important than the quantity.

\section{Conclusion and Discussion}
In this paper, we introduce ManiTrend, an end-to-end framework for modeling the dynamics of 3D particles, vision observations and manipulation actions in a uniform manner. The design is motivated by the observation that 3D flow is a suitable intermediate for bridging future image generation and fine-grained action prediction, which is verified by our ablation study. As a flexible framework, ManiTrend supports cross-embodiment pretraining on manipulation demonstrations without using action labels. In-domain and zero-shot transfer evaluations on LIBERO and CALVIN demonstrate that ManiTrend achieves SoTA manipulation performance. In addition, it is much faster than SoTA methods that rely on large VLMs or text-to-image models. Experimental results also bring out two interesting discoveries: 1) letting action chunk query attend to flow query does help action prediction, but at the risk of overfitting the object placements and proprioception states seen in training data; 2) increasing the diversity of pretraining data is more helpful than increasing the quantity. Both facts indicate that diversity should be the primary consideration for scaling up pretraining/finetuning data. In the future, we will extend ManiTrend for bimanual manipulation and incorporate more sensory input, e.g. depth and force/torque.

\section{Limitations}
We summarize the limitations of our work as follows:
\begin{itemize}
    \item The quality of the 3D flow label highly depends on the tool we use, SpatialTracker \cite{SpatialTracker}, which is far from mature. It is likely that current performance of ManiTrend is restricted by this. But we believe that the fast-developing 3D Tracking technology will resolve this limitation.
    \item We have not investigated different model architectures, especially the structures of conditioning future generation and action prediction on 3D flow. A thorough empirical study on this may uncover better implementations that yield better performance.
    \item The 3D flow in this work only accounts for main view observation, and we have not looked into the influence of the length and density of flow.
    \item Some modalities of sensory input, e.g. depth, are not covered in this work, which may benefit 3D flow modeling.
    \item The manipulation tasks discussed in this work only involve a single arm.
\end{itemize}


% \section*{Acknowledgments}

%% Use plainnat to work nicely with natbib. 

\bibliographystyle{plainnat}
\bibliography{references}

\appendices

\begin{table*}[htb]
	\centering
    \caption{Statistics of datasets used in our pretraining.}
    \label{pt_data}
    \scalebox{1.0}[1.0]{
	\begin{tabular}{lcccccc}
		\toprule
        Dataset & Subset? & Embodiment & \# Tasks & \# Traj. & \# Frames & Avg. Len. \\
        \midrule
        RH20T \cite{fang2023rh20t} & Cfg 4, Cfg 5 & Human, UR5, Franka & 149 & 18264 & 1625166 & 89 \\
        Bridge \cite{walke2023bridgedata} & Filtered & WidowX & 13 & 18373 & 615772 & 34\\
        LIBERO \cite{liu2024libero} & LIBERO-90 & Franka & 90 & 4500 & 673386 & 150 \\
        Berkeley UR5 \cite{BerkeleyUR5Website} & Full Data & UR5 & 5 & 1000 & 96939 & 97 \\
        Mutex \cite{shah2023mutex} & Full Data & Franka & 100 & 1380 & 157095 & 114 \\
		\bottomrule
	\end{tabular}
    }
\end{table*}

\begin{table*}[htb]
	\centering
    \caption{Hyperparameter settings}
    \label{hyperparam}
    \scalebox{1.0}[1.0]{
	\begin{tabular}{lccc}
		\toprule
        Hyperparameter & Pretraining & Finetuning (CALVIN) & Finetuning (LIBERO) \\
        \midrule
        Epochs & 35 & 40 & 50\\
        Warmup epochs & 2 & 1 & 1 \\ 
        Batchsize & 960 & 560 & 288 \\
        Learning rate & 5e-4 & 1e-3 & 5e-4 \\
        Gradient clip & 0.1 & - & - \\
        Augment prob. & 0.9 & 0.9 & 0.9 \\
        History window $T$ & 10 & 10 & 10 \\
        Time shift $S$ & 3 & 3 & 3 \\
        Flow length $L$ & 7 & 7 & 7 \\
        \# track point $P$ & 50 & 50 & 50 \\
        Action chunk size $K$ & 7 & 7 & 7 \\
        \# resampled vectors $r$ & 9 & 9 & 9 \\
        \# flow query vectors $l$ & 4 & 4 & 4 \\
		\bottomrule
	\end{tabular}
    }
\end{table*}

\section{Pretraining Data}

Data for pretraining includes 44K trajectories from 5 datasets (RH20T \cite{fang2023rh20t}, Bridge \cite{walke2023bridgedata}, Berkeley UR5 \cite{BerkeleyUR5Website} , Mutex \cite{shah2023mutex} and LIBERO \cite{liu2024libero}), which cover 4 kinds of embodiments (humans, Franka robots, UR5 robots and WidowX robots). For RH20T, we only use the data in configs 4 and 5. For Bridge, we filter out data without wrist-view observation (although we do not utilize wrist-view observation during pretraining, we filter the data for upward compatibility). For LIBERO, we use the data in LIBERO-90 task suite. It may be beneficial to add data from the other 4 task suites of LIBERO, but we leave it for future work. Detailed statistics of the pretraining data are listed in Table \ref{pt_data}.

\section{Network and Training Details}
\label{details}

There are about 200M parameters in ManiTrend. The backbone of ManiTrend is a causal transformer with 12 layers, 12 attention heads, a dropout rate of 0.1 and a latent size of 384. The number of resampled image feature vectors $r$ and the number of flow query vectors $l$ are set to 9 and 4 respectively. Both the flow decoder and future image decoder are made up of self-attention blocks and MLP layers. The action decoder is a three-layer MLP, where the last layer splits into two heads for predicting end-effector Cartesian actions and binary gripper closeness actions.

By default, we set historical window size $T$ as 10, future time shift $S$ as 3, flow length $L$ as 7, the number of track points $P$ as 50, and action chunk size $K$ as 7 (only applicable to finetuning). Throughout pretraining and finetuning, images are augmented with random shift. The pretraining process lasts for 35 epochs, which takes about 3 days on 4 RTX 4090 GPUs. The finetuning process on CALVIN lasts for 40 epochs, taking around 0.5 day /1.5 day on 4 RTX 4090 for D$\rightarrow$D / ABC$\rightarrow$D. The finetuning process on each task suite of LIBERO lasts for 50 epochs, taking about 10 hours on 2 RTX 4090. We employ the AdamW optimizer together with the cosine warmup strategy. Detailed hyperparameter settings are available in Table \ref{hyperparam}.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=\linewidth]{flow_calvin_d_d.pdf}
    \caption{Visualization of predicted 3D flow on CALVIN (D$\rightarrow$D). The images in the upper row are main-view observations with rendered flow (only x, y are considered). Whereas the images in lower row visualize 3D flow in 3D space (for visual clarity, we only visualize the flow of moving particles that get sampled).}
    \label{flow_calvin_d_d}
\end{figure*}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=\linewidth]{flow_libero.pdf}
    \caption{Visualization of predicted 3D flow on the Spatial, Object and Goal task suites of LIBERO.}
    \label{flow_libero}
\end{figure*}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=\linewidth]{future_calvin_d_d.pdf}
    \caption{Generated future main-view and wrist-view images on CALVIN (D$\rightarrow$D). }
    \label{future_calvin_d_d}
\end{figure*}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=\linewidth]{future_libero.pdf}
    \caption{Generated future main-view and wrist-view images on the Spatial, Object and Goal task suites of LIBERO.}
    \label{future_libero}
\end{figure*}

\section{Qualitative Analysis of Predicted 3D Flow}
\label{vis_flow}

We look into the 3D flow prediction performance on CALVIN and LIBERO. Qualitative results are shown in Figure \ref{flow_calvin_d_d} and Figure \ref{flow_libero}. It can be observed that:
\begin{itemize}
    \item The quality of predicted 3D flow is overall satisfying, demonstrating the effectiveness of learning to model 3D flow.
    \item ManiTrend is good at modeling the flow of particles on the robotic arm, but weak at distinguishing between particles nearby the robotic arm and particles on the robotic arm.
    \item The density of moving particles that get sampled is relatively small. It is unclear whether increasing the density of tracking points will improve performance or not.
    \item 3D flow conveys valuable depth information, which is useful for tasks that involve rotation and longitudinal movement. In contrast, 2D flow misses depth information in certain direction.
\end{itemize}

\section{Qualitative Analysis of Generated Future Images}
\label{vis_future}

Figures \ref{future_calvin_d_d} and \ref{future_libero} display the future images generated during rollouts on CALVIN and LIBERO. It can be observed that:
\begin{itemize}
    \item The overall quality of the generated future images is satisfying, showcasing the effectiveness of spatiotemporal modeling.
    \item The distortion of the shapes and colors of objects is noticeable. And some images exhibit rasterization artifacts. These are caused by the suboptimal design of our image decoder. Using a stronger image decoder (such as the one in VQGAN \cite{esser2021taming}) may help address the problem.
\end{itemize}
\end{document}


