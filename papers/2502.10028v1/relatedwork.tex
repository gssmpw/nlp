\section{Related Work}
\label{sec:relat}

\paragraph{Language-conditioned manipulation}
It is a natural way for humans to use language instructions to command robotic agents. Instead of constructing a single-task visuomotor policy for every possible instruction, it is more desirable to have a multitask language-conditioned manipulation policy that is generalizable and scalable. CLIPort \cite{shridhar2022cliport}, which injects frozen CLIP \cite{radford2021learning} language embeddings into two-stream convolution networks, is one of the first studies that show the potential of this paradigm. Some follow-up methods \cite{bharadhwaj2024roboagent} adopt the same idea and replace the backbone with transformers. However, purely relying on frozen language embeddings will impose a limit of generalizability, due to the flexibility of language. In contrast, some methods employ large language models to generate code for manipulation \cite{10160591,huang2023instruct2act}, which somehow solve the challenge of language conditioning, but are slow at inference and impractical for real-time closed-loop interaction, where active distractors may exist \cite{yu2024hierarchical} or the environment itself may change rapidly \cite{zhang2024catch}. This efficiency issue also hinders the application of methods that tune large vision-language models to generate action \cite{brohan2023rt,kim2024openvla}, or leverage large diffusion models to translate instructions into goal images \cite{blackzero, shridhargenerative, li2024gr}. Compared to the above methods, representation learning approaches \cite{nair2023r3m, ma2023liv, karamcheti2023voltron, nguyen2024robotic, wuunleashing, zeng2024learning, yang2024spatiotemporal, bharadhwaj2024gen2act} seem to be more promising, which incorporate auxiliary learning tasks to obtain language-aware features for manipulation. Our work adheres to this paradigm of representation learning.

\paragraph{Video pretraining for manipulation}
Videos associated with language captions are much more accessible than robotic data while being suitable for learning language-aware spatial-temporal representations about manipulation behaviors. Some pioneering efforts leverage self-supervised learning tasks like time contrast \cite{nair2023r3m,mavip} and video-language alignment \cite{pmlr-v164-jang22a,karamcheti2023voltron}. More recent approaches prefer future image generation pretraining \cite{wuunleashing, zeng2024learning, yang2024spatiotemporal, he2024large}, which delivers high-quality representations and good interpretability as a side product. However, it is essentially demanding. To generate a future frame based on history frames and a language caption, a model has to capture not only the movement and occlusion relations of objects but also their visual appearances \cite{wen2023any}. In this work, we find it applicable to alleviate the burden of future image generation by providing guidance on movement and spatial relations through 3D flow.

\paragraph{Flow-enhanced manipulation polices}
Different from modeling frame-to-frame transition, flow highlights a subset of points within the frame. As a result, movement information is decoupled from visual appearance information, becoming easier to get captured by a model. In \cite{bharadhwaj2024gen2act}, 2D flow prediction from the latent space of the policy model acts as an auxiliary learning task, while the predicted flow is not explicitly utilized for action guidance. In contrast, all other flow-enhanced manipulation polices \cite{vecerik2024robotap, wen2023any, bharadhwaj2024track2act, xuflow} solely employ predicted flow to guide action prediction. Among them, flow is typically confined into the 2D frame space and lacks the integrity of 3D physical motion, except for \emph{General Flow} \cite{xuflow}, which similarly adopts 3D flow as in this work. However, the main focus of \emph{General Flow} \cite{xuflow} is to develop a 3D flow prediction model, whereas our work features the in-depth integration of 3D flow.