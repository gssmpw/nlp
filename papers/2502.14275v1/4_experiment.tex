\section{Experiments and Analysis}
\label{sec:exp}

\begin{figure*}[!ht]
    \vspace{-1.2em}
    \centering
    \resizebox{0.85\textwidth}{!}{
        \includegraphics[width=\textwidth]{figures/zo_performances.pdf}
    }
    \caption{Accuracies of LLMs on the \mkj dataset with zero-shot prompting.}
    \label{fig:zo_performaces}
    \vspace{-1.3em}
\end{figure*}

\subsection{Experiment setup}
% \todo{gpt4o. not mini version. Experiments.}
We evaluate representative open-source and closed-source LLMs of various sizes and architectures, with an intention on LLMs with smaller sizes. Closed-source models include \gptthree, \gptfour, \gptfouro~\cite{openai2023gpt4}, \claudea and \claudeb~\cite{claude2024}. 
Open-source LLMs include Llama-3 series (including \llamaa, \llamab, \llamac, \llamad)~\cite{dubey2024llama}, \ministral~\cite{ministral2024}, Qwen2.5 series (including \qwena, \qwenb, \qwenc)~\cite{qwen2.5}, and Phi-3 series (including \phia, \phib, \phic)~\cite{abdin2024phi}. We also include open-source medical LLMs \meditron~\cite{chen2023meditron70b} and \mellama~\cite{xie2024me}. 

More information can be found in Appendix~\ref{app:model_details}.
For the inputs to LLMs, we utilize zero-shot prompting technique to LLMs for their responses. Detailed prompts can be found in Appendix~\ref{app:prompt}.
We employ the most deterministic settings (i.e., \texttt{temperature=0} or \texttt{top\_k=1}) for all models. 


\subsection{Evaluation}
Each datapoint in the \mkj{} dataset contains 1 positive judgments and $k$ negative judgments (we set $k$=3). Therefore, we have three categories of evaluation metrics, positive accuracy $\text{Acc}_\text{pos}$ (accuracy on the positive judgment questions), negative accuracy $\text{Acc}_\text{neg}$ (accuracy on the negative judgment questions), and accuracy $\text{Acc}$ (regular accuracy over all samples in $\text{Acc}_\text{pos}$ and $\text{Acc}_\text{neg}$ combined), which can be formally defined as:
\begin{equation*}
\text{Acc} = \frac{1}{k+1} \text{Acc}_\text{pos} + \frac{k}{k+1} \text{Acc}_\text{neg}.
\end{equation*}

% It is also worth mentioning that our goal is not to compare different LLMs. Instead, by examining the metrics by different LLMs, we aim to observe the common patterns among the representative LLMs.

\subsection{RQ1: To what extent can LLMs accurately perform medical judgments?}
\label{sec:rq1}
Figure~\ref{fig:zo_performaces} shows the overall accuracies Acc of LLMs with zero-shot prompting, from which we can observe that \gptfouro and \gptfour achieve the best results, and medical LLMs \mellama and \meditron also display strong performances. However, general-domain open-source LLMs fall behind and their performances generally decreases as model sizes becomes smaller.

\input{tables/acc_gap}

The results of positive accuracies $\text{Acc}_\text{pos}$ and negative accuracies $\text{Acc}_\text{neg}$ are shown in Table~\ref{tab:acc_gap}. 
We also calculate the accuracy gap between $\text{Acc}_\text{pos}$ and $\text{Acc}_\text{neg}$, which is denoted as $\text{Acc}_\text{gap} = \text{Acc}_\text{pos} - \text{Acc}_\text{neg}$. 
For LLM families with more than one models tested in our experiment, we report the averaged results (round to two decimal places) for convenience and denote these aggregated values with the suffix \textsc{-Series} (e.g., \llamas).

As shown in Table~\ref{tab:acc_gap}, most LLMs exhibit negative $\text{Acc}_\text{gap}$ values, indicating that $\text{Acc}_\text{neg}$ consistently exceeds $\text{Acc}_\text{pos}$. 
This pattern reflects LLMs' inherent bias toward skepticism, showing a stronger inclination to reject rather than accept medical judgments, which aligns with previous research findings~\cite{sun-etal-2024-head, xiong2024can}. 
This behavior may be due to the model’s intrinsic uncertainty in the domain-specific knowledge required to validate medical statements, which suggests a shortage of knowledge in medicine and healthcare. 
% The observed behavior likely stems from the models' underlying uncertainty regarding domain-specific medical knowledge, highlighting a significant gap in their understanding of medicine and healthcare concepts.
% suggests that LLMs demonstrate a conservative approach in their responses


\subsection{RQ2: How well are LLMs calibrated in medical and healthcare contexts?}
\label{sec:rq2_calibration}

Given the bias in accuracies observed in Section~\ref{sec:rq1}, we aim to assess the calibration of LLMs~\cite{guo2017calibration, minderer2021revisiting, xiao2022uncertainty} on the \mkj dataset in this section.
% evaluating how well their confidence scores align with actual accuracies. 

Miscalibration is commonly quantified in terms of Expected Calibration Error (ECE)~\cite{naeini2015obtaining}, which measures the absolute difference between predictive confidence and accuracy.
Here we directly present the calibration curve, which is constructed by plotting the pairs of confidence score and accuracy, instead of further calculating the ECE.

To illustrate, considering \(N\) samples with predicted probabilities \(p_i\) and true labels \(y_i\) (\(y_i \in \{0,1\}\)), we plot the calibration curve as follows. 
Firstly, the interval \([0,1]\) is divided into \(M\) equal-width bins (we set $M=20$ in our experiment). Define the \(k\)-th bin \(B_k\) as 
\begin{equation*}
B_k = \{ i \mid p_i \in \left[\frac{k-1}{M}, \frac{k}{M}\right) \}, k = 1, 2, \dots, M.
\end{equation*}
% we compute the bin averages.
Then for each bin \(B_k\), we compute the average predicted probability (confidence score) as 
\begin{equation*}
\hat{p}_k = \frac{1}{|B_k|} \sum_{i \in B_k} p_i,
\end{equation*}
and the observed positive rate (accuracy) as 
\begin{equation*}
\hat{y}_k = \frac{1}{|B_k|} \sum_{i \in B_k} y_i.
\end{equation*}
The calibration curve is then constructed by plotting the pairs \(\{ (\hat{p}_k, \hat{y}_k) \}_{k=1}^{M}\). 
A perfectly calibrated model would have these points lie on the diagonal $y=x$.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.233\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/calibration/calibration_curve_llama-3.2-3B.pdf}
        \caption{Calibration curve for \llamad.}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.233\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/calibration/calibration_curve_ministral-8B.pdf}
        \caption{Calibration curve for \ministral.}
    \end{subfigure}
    
    \vspace{0.5cm}
    
    \begin{subfigure}{0.233\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/calibration/calibration_curve_qwen2.5-3B.pdf}
        \caption{Calibration curve for \qwenc.}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.233\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/calibration/calibration_curve_phi-3.5-mini.pdf}
        \caption{Calibration curve for \phic.}
    \end{subfigure}
    
    \vspace{0.5cm}
    
    \begin{subfigure}{0.233\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/calibration/calibration_curve_meditron-7B.pdf}
        \caption{Calibration curve for \meditron.}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.233\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/calibration/calibration_curve_MeLLaMA-13B.pdf}
        \caption{Calibration curve for \mellama.}
    \end{subfigure}
    
    \caption{Calibration curves for representative models from four different open-source LLM families (Llama, Mistral, Qwen, and Phi), as well as Medical LLMs (\meditron and \mellama).}
    \label{fig:calibration_curves}
    \vspace{-1.2em}
\end{figure}

The calibration curves of representative models from four general-domain LLM families and medical LLMs are shown in Figure~\ref{fig:calibration_curves}. 
The results for all LLMs can be found in Figure~\ref{fig:app_calibration_curve_1} and Figure~\ref{fig:app_calibration_curve_2} in the Appendix~\ref{app:calibration}.

The LLMs tested in this study
% including \llamad, \ministral, \qwenc, \phic, \meditron, and \mellama, 
illustrate varying degrees of calibration.
Across the models, we can observe that general-domain LLMs exhibit significant disparities from the perfect calibration line, while medical LLMs demonstrate better calibration performances.

For general-domain LLMs \ministral, \qwenc, and \phic all reveal erratic status, exhibiting both overconfidence and underconfidence patterns.
% exhibiting descending accuracy with growing confidence, 
For \llamad, it is observed that although the model occasionally approaches the ideal calibration line when confidence is low, its uncertainty level is still high when the confidence increases.

For medical LLMs, the calibration of \meditron is far from satisfactory, particularly in the mid-confidence range (0.4–0.8), where it tends to be overconfident. Its accuracy improves at higher confidence scores but still remains inconsistent. As for \mellama, the curve shows better calibration overall, with predictions closely aligned with the perfect calibration in mid-to-high confidence range (0.5-1.0) although it still tends to be overconfident at low confidence levels (<0.5). 

Overall, our analysis reveals pervasive calibration deficiencies across all evaluated models on the \mkj dataset. Poor calibration can result in overconfidence, which may lead to misplaced trust in incorrect predictions~\cite{xiong2024can, chen-etal-2023-close}, or underconfidence, which could cause clinicians to overlook valuable insights~\cite{yang2010nurses, jiang2012calibrating}. This points to calibration as a critical area for LLMs in healthcare and medicine.


\subsection{RQ3: What are the underlying reasons behind LLMs’ failure to retain certain critical medical knowledge?}
\label{sec:rq3}

% The constructed \mkj dataset evaluates a diverse range of medical entities, prompting the question of whether LLMs demonstrate uniform competence across these varied judgments. 
Given that LLMs exhibit problems observed in Section \ref{sec:rq1} and \ref{sec:rq2_calibration}, a natural question is raised, "What are the underlying reasons behind LLMs' failure to retain or recall certain medical knowledge?"

\input{tables/top_5_case}
\vspace{-1em}

To explore this, we analyze the performances of LLMs when tasked with assessing entities categorized under different semantic types, and summarize the accuracies accordingly in the Table~\ref{tab:top_5_case}, where we find the performance gap can be as large as 50\%, and identify that questions involving \textit{Sign or Symptom} and \textit{Hormone} categories exhibit notably higher error rates in the \mkj dataset.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/acc_and_freq_new.pdf}
    \caption{Overall accuracy (Acc) and frequency across most and least error-prone semantic types that are denoted via abbreviations as shown in Table~\ref{tab:top_5_case}.}
    \label{fig:acc_and_freq}
    \vspace{-1em}
\end{figure}


\input{tables/RAG_sparse_dense}


Our analysis indicates that the performance degradation primarily stems from the presence of rare medical entities within these categories. Through comprehensive cross-referencing with medical databases and literature~\cite{chen2024rarebench, wang2024assessing}, we identify the prevalence of rare medical conditions as a significant factor contributing to LLM failures. This finding is particularly evident in the most error-prone cases, which include uncommon medical phenomena such as ``Swallow syncope'' and ``Asterixis'' within the semantic type \textit{Sign or Symptom}, as well as rare pharmaceutical compounds like ``Prednisolone hexanoate'' and ``Fluprednisolone'' in the semantic type \textit{Hormone}.

To further quantify the rarity of the medical terms encountered, we leverage the PubMed API\footnote{https://www.ncbi.nlm.nih.gov/books/NBK25497/} to analyze their prevalence in the medical literature. Specifically, we calculate the averaged frequencies of the terms in the questions per semantic type as shown in Figure~\ref{fig:acc_and_freq}, from which we can observe certain correlations between the accuracies and frequencies, where semantic types with low accuracies are more likely to have low frequencies.

% To sum up, we identify that rare disease can be one of the reasons that [...]

This finding indicates a key limitation in their training data coverage, highlighting performance disparities across semantic categories due to under-representation of rare medical concepts.



\subsection{RQ4: What strategies can enhance the response accuracy of LLMs?}
\label{sec:rq4}

Current results by zero-shot prompting show that there still exists a gap in response accuracy.
To explore more responsible approaches for utilizing LLMs, we experiment the technique of retrieval-augmented generation (RAG)~\cite{guu2020retrieval, karpukhin-etal-2020-dense, lewis2020retrieval}.

\paragraph{Experiment setup} The experiment setup for RAG is outlined as below.
\begin{itemize}[left=0pt, topsep=0pt, itemsep=0pt, partopsep=0pt,parsep=0pt]
    \item Retriever. We incorporates both sparse retrieval method BM25~\cite{robertson2009probabilistic} and dense retrieval method Sentence-BERT~\cite{reimers-2019-sentence-bert, thakur-2020-AugSBERT}.
    \item Documents. We utilize the knowledge base $\mathcal{D}$ constructed and described in Section~\ref{sec:data_collect} as our retrieval document corpus.
    % The constructed knowledge base $D$ is utilized as the documents for retrieval, which is introduced in section~\ref{sec:data_collect}.
    \item Implementation. We prepend the retrieved grounding documents to the input of the LLMs~\cite{ram2023context, shi-etal-2024-replug}. Detailed prompts can be found in Appendix~\ref{app:prompt}.
\end{itemize}

The experiment outputs are listed in Table~\ref{tab:sparse_dense_rag}. The performance differentials between RAG and zero-shot approaches are indicated in parentheses, computed as RAG results minus zero-shot results.
Performances improved through RAG are displayed in green with upward arrows ($\uparrow$), while performance decrements are shown in red with downward arrows ($\downarrow$).

It is demonstrated that RAG enhances the response accuracy compared with zero-shot prompting on most cases, reaching 83.05\% accuracy with sparse retrieval and 81.85\% with dense retrieval on average, which greatly helps the application of LLMs in real-life settings.

However, we also observe that some small-size models exhibit performance disparities between Acc$_\text{pos}$ and Acc$_\text{neg}$ such as \qwena and \llamac, which match the overconfidence patterns observed in Section~\ref{sec:rq2_calibration} and Figure~\ref{fig:app_calibration_curve_1}.
This reveals that models with too small scales present inherent limitations in calibration and properly handling external documents.

Our experiment underscores the importance of retrieval methods to complement the inherent strengths and weaknesses of LLMs~\cite{chen2024benchmarking}. 
By integrating effective RAG systems, we can significantly improve LLMs' ability in real-life medical QA scenarios.


% \paragraph{Model-Specific Variations.} LLMs like \gptfour and \gptthree consistently achieved higher overall accuracy and reliability across both sparse and dense retrieval frameworks. These results suggest that advanced models benefit more substantially from augmented retrieval strategies, which further enhance their already robust baseline performance. In contrast, some models with small sizes such as \llamac, \llamad, \qwena and \qwenb, cannot make precise judgments even though given high-quality retrieved triplets as documents to reference, which suggests that model size is a critical factor that influences the general capability of LLMs.

% In summary, while zero-shot prompting alone falls short of making LLMs viable for real-world medical applications, RAG offers a promising pathway for improvement. By carefully optimizing retrieval techniques and integrating high-quality medical knowledge bases, we can unlock the full potential of LLMs as reliable tools for healthcare professionals and patients alike.
