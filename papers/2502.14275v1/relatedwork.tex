\section{Related Work}
\begin{figure*}[!ht]
    \centering
    \vspace{-1em}
    \resizebox{\textwidth}{!}{
        \includegraphics[width=\textwidth]{figures/dataset_pipeline.pdf}
    }
    % \includegraphics[0.5\textwidth]{figures/dataset_pipeline.pdf}
    \caption{Pipeline for the \mkj dataset construction.}
    \label{fig:pipeline}
    \vspace{-1em}
\end{figure*}


\paragraph{Benchmarks} Most works that study the factuality of LLMs use existing question-answering (QA) benchmarks such as WebQuestions~\cite{berant2013semantic}, TriviaQA~\cite{joshi2017triviaqa}, LC-QuAD~\cite{trivedi2017lc, dubey2019lc}, Natural Questions~\cite{kwiatkowski2019natural}, and EntityQuestions~\cite{sciavolino2021simple}. 
There are also new QA benchmarks assessing LLMs' long-tail knowledge factuality~\cite{mallen2023not, kumar2024automatic}. 
For medical QA datasets, \citet{qiu2024towards} build MMedBench, a multilingual medical multi-choice QA benchmark with rationales, which involves the aggregation of various medical multi-choice QA datasets from multiple languages~\cite{jin2021disease, kasai2023evaluating, labrak-etal-2022-frenchmedmcqa, vilares-gomez-rodriguez-2019-head, blinov2022rumedbench}. 
There are also some benchmarks involving more challenging medical questions such as the clinical topics datasets of MMLU~\cite{hendrycks2020measuring} and MedMCQA~\cite{pal2022medmcqa, malaviya-etal-2024-expertqa}. However, these datasets mostly contain multi-hop or indirect relationship questions, assessing more about skilled reasoning abilities of LLMs.
Compared with these benchmarks, \mkj{} focuses on fundamental evaluation by one-hop direct medical knowledge judgments and contributes to the growing body of work on LLM evaluation by systematically leveraging knowledge graphs.
% to assess LLMs' inherent medical expertise.




\paragraph{LLMs in Medicine}
To apply LLMs on different tasks in general medical and healthcare contexts, some approaches are adopted such as prompt engineering~\cite{shi2024ehragent, chen2024cod, singhal2025toward} and fine-tuning LLMs with domain-specific data~\cite{singhal2025toward, xu2023baize, xie2024me, chen2023meditron70b, shi2024mgh, xiong2023doctorglm, zhao2024helene}.
There are also some work integrating medical knowledge graphs to assist LLMs on tasks such as medical question answering~\cite{yang2024kg, yasunaga2022deep} and diagnosis prediction~\cite{afshar2024role, gao2023leveraging}.
While existing efforts have focused on enhancing LLMs' performances across diverse tasks, a fundamental assessment of their capabilities to internalize and apply medical knowledge remains relatively limited. To address this gap, we introduce the \mkj dataset.
% However, these works focus more on improving the performances across various tasks and a fundamental assessment of LLMs' capabilities to internalize medical knowledge remains somewhat limited. To this end, we propose \mkj.