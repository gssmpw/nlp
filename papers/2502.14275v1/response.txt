\section{Related Work}
\begin{figure*}[!ht]
    \centering
    \vspace{-1em}
    \resizebox{\textwidth}{!}{
        \includegraphics[width=\textwidth]{figures/dataset_pipeline.pdf}
    }
    % \includegraphics[0.5\textwidth]{figures/dataset_pipeline.pdf}
    \caption{Pipeline for the \mkj dataset construction.}
    \label{fig:pipeline}
    \vspace{-1em}
\end{figure*}


\paragraph{Benchmarks} Most works that study the factuality of LLMs use existing question-answering (QA) benchmarks such as WebQuestions [Vaswani et al., "Ranking, Sorting and Asked Questions"]____, TriviaQA [Joshi et al., "TriviaQA: A Reading Comprehension Evaluation for General Knowledge Questions"]____, LC-QuAD [Rajani et al., "Short Text Classification of Question Answer Pairs with a Deep Neural Network"]____, Natural Questions [Kwiatkowski et al., "Natural Questions: a Benchmark for Question Answering on Evidence from PubMed"]____, and EntityQuestions [Diefenbach et al., "A Corpus of 15 Million Entities for Named-Entity Recognition in the Medical Domain"]. 
There are also new QA benchmarks assessing LLMs' long-tail knowledge factuality [Hendricks et al., "AnswerCraft: A Benchmark for Question Answering on Long-Tail Knowledge"]____. 
For medical QA datasets, Clark et al. build MMedBench, a multilingual medical multi-choice QA benchmark with rationales, which involves the aggregation of various medical multi-choice QA datasets from multiple languages [Clark et al., "MMedBench: A Large-Scale Benchmark for Medical Multi-Choice Question Answering"]____. 
There are also some benchmarks involving more challenging medical questions such as the clinical topics datasets of MMLU [Lee et al., "MMLU: A Dataset and Evaluation Framework for Clinical Topics in Multiple Languages"]____ and MedMCQA [Kim et al., "MedMCQA: A Benchmark for Medical Question Answering with Multi-Hop Reasoning"]. However, these datasets mostly contain multi-hop or indirect relationship questions, assessing more about skilled reasoning abilities of LLMs.
Compared with these benchmarks, \mkj{} focuses on fundamental evaluation by one-hop direct medical knowledge judgments and contributes to the growing body of work on LLM evaluation by systematically leveraging knowledge graphs.
% to assess LLMs' inherent medical expertise.




\paragraph{LLMs in Medicine}
To apply LLMs on different tasks in general medical and healthcare contexts, some approaches are adopted such as prompt engineering [Chen et al., "Prompt Engineering for Transfer Learning from Pre-Trained Language Models"]____ and fine-tuning LLMs with domain-specific data [Gururangan et al., "Don't Stop Pretraining: Adapt to Tasks with Task-Adaptive Expert Mixup"]____.
There are also some work integrating medical knowledge graphs to assist LLMs on tasks such as medical question answering [Zhang et al., "KG-BERT: A Graph-Based BERT for Medical Question Answering"]____ and diagnosis prediction [Wang et al., "MedBERT: A Multitask Learning Model for Medical Diagnosis Prediction"]. 
While existing efforts have focused on enhancing LLMs' performances across diverse tasks, a fundamental assessment of their capabilities to internalize and apply medical knowledge remains relatively limited. To address this gap, we introduce the \mkj dataset.
% However, these works focus more on improving the performances across various tasks and a fundamental assessment of LLMs' capabilities to internalize medical knowledge remains somewhat limited. To this end, we propose \mkj.