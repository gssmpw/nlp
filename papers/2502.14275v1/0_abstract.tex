\begin{abstract}
Large language models (LLMs) have been widely adopted in various downstream task domains.
% However, how knowledgeable are LLMs in medical domain is still under exploration. 
However, their ability to directly recall and apply factual medical knowledge remains under-explored. Most existing medical QA benchmarks assess complex reasoning or multi-hop inference, making it difficult to isolate LLMsâ€™ inherent medical knowledge from their reasoning capabilities. Given the high-stakes nature of medical applications, where incorrect information can have critical consequences, it is essential to evaluate how well LLMs encode, retain, and recall fundamental medical facts.
%how well can LLMs adapt to medical and healthcare contexts directly is still under researched to some extent.

% In this paper, we aim to ask this research question: \textit{"How knowledgeable are LLMs in medical and healthcare problems?"}
To bridge this gap, we introduce the \textbf{Medical Knowledge Judgment Dataset (\mkj)}, a dataset specifically designed to measure LLMs' one-hop factual medical knowledge. \mkj is constructed from the Unified Medical Language System (UMLS), a large-scale repository of standardized biomedical vocabularies and knowledge graphs. 
We frame knowledge assessment as a binary judgment task, requiring LLMs to verify the correctness of medical statements extracted from reliable and structured knowledge sources.

Our experiments reveal that LLMs struggle with factual medical knowledge retention, exhibiting significant performance variance across different semantic categories, particularly for rare medical conditions. 
Furthermore, LLMs show poor calibration, often being overconfident in incorrect answers. 
To mitigate these issues, we explore retrieval-augmented generation, demonstrating its effectiveness in improving factual accuracy and reducing uncertainty in medical decision-making.

\end{abstract}
