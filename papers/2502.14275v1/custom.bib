% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{sun-etal-2024-head,
    title = "Head-to-Tail: How Knowledgeable are Large Language Models ({LLM}s)? {A}.{K}.{A}. Will {LLM}s Replace Knowledge Graphs?",
    author = "Sun, Kai  and
      Xu, Yifan  and
      Zha, Hanwen  and
      Liu, Yue  and
      Dong, Xin Luna",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.18",
    doi = "10.18653/v1/2024.naacl-long.18",
    pages = "311--325",
    abstract = "Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs?To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.",
}

@article{afshar2024role,
  title={On the role of the UMLS in supporting diagnosis generation proposed by Large Language Models},
  author={Afshar, Majid and Gao, Yanjun and Gupta, Deepak and Croxford, Emma and Demner-Fushman, Dina},
  journal={Journal of Biomedical Informatics},
  volume={157},
  pages={104707},
  year={2024},
  publisher={Elsevier}
}

@article{gao2023leveraging,
  title={Leveraging a medical knowledge graph into large language models for diagnosis prediction},
  author={Gao, Yanjun and Li, Ruizhe and Caskey, John and Dligach, Dmitriy and Miller, Timothy and Churpek, Matthew M and Afshar, Majid},
  journal={arXiv preprint arXiv:2308.14321},
  year={2023}
}

@article{yang2024kg,
  title={Kg-rank: Enhancing large language models for medical qa with knowledge graphs and ranking techniques},
  author={Yang, Rui and Liu, Haoran and Zeng, Qingcheng and Ke, Yu He and Li, Wanxin and Cheng, Lechao and Chen, Qingyu and Caverlee, James and Matsuo, Yutaka and Li, Irene},
  journal={arXiv preprint arXiv:2403.05881},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{bang2023multitask,
  title={A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  booktitle={Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={675--718},
  year={2023}
}

@article{zhang2023siren,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@inproceedings{sun2024head,
  title={Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? AKA Will LLMs Replace Knowledge Graphs?},
  author={Sun, Kai and Xu, Yifan and Zha, Hanwen and Liu, Yue and Dong, Xin Luna},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={311--325},
  year={2024}
}

@article{weimeasuring,
  title={Measuring short-form factuality in large language models},
  author={Wei, Jason and Karina, Nguyen and Chung, Hyung Won and Jiao, Yunxin Joy and Papay, Spencer and Glaese, Amelia and Schulman, John and Fedus, William}
}

@article{li2022neural,
  title={Neural natural language processing for unstructured data in electronic health records: a review},
  author={Li, Irene and Pan, Jessica and Goldwasser, Jeremy and Verma, Neha and Wong, Wai Pan and Nuzumlal{\i}, Muhammed Yavuz and Rosand, Benjamin and Li, Yixin and Zhang, Matthew and Chang, David and others},
  journal={Computer Science Review},
  volume={46},
  pages={100511},
  year={2022},
  publisher={Elsevier}
}

@article{bi2024decoding,
  title={Decoding by Contrasting Knowledge: Enhancing LLMs' Confidence on Edited Facts},
  author={Bi, Baolong and Liu, Shenghua and Mei, Lingrui and Wang, Yiwei and Ji, Pengliang and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2405.11613},
  year={2024}
}

@article{yang2023large,
  title={Large language models in health care: Development, applications, and challenges},
  author={Yang, Rui and Tan, Ting Fang and Lu, Wei and Thirunavukarasu, Arun James and Ting, Daniel Shu Wei and Liu, Nan},
  journal={Health Care Science},
  volume={2},
  number={4},
  pages={255--263},
  year={2023},
  publisher={Wiley Online Library}
}

@article{zhang2024generalist,
  title={A generalist vision--language foundation model for diverse biomedical tasks},
  author={Zhang, Kai and Zhou, Rong and Adhikarla, Eashan and Yan, Zhiling and Liu, Yixin and Yu, Jun and Liu, Zhengliang and Chen, Xun and Davison, Brian D and Ren, Hui and others},
  journal={Nature Medicine},
  pages={1--13},
  year={2024},
  publisher={Nature Publishing Group US New York}
}

@inproceedings{malaviya-etal-2024-expertqa,
    title = "{E}xpert{QA}: Expert-Curated Questions and Attributed Answers",
    author = "Malaviya, Chaitanya  and
      Lee, Subin  and
      Chen, Sihao  and
      Sieber, Elizabeth  and
      Yatskar, Mark  and
      Roth, Dan",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.167",
    doi = "10.18653/v1/2024.naacl-long.167",
    pages = "3025--3045",
    abstract = "As language models are adopted by a more sophisticated and diverse set of users, the importance of guaranteeing that they provide factually correct information supported by verifiable sources is critical across fields of study. This is especially the case for high-stakes fields, such as medicine and law, where the risk of propagating false information is high and can lead to undesirable societal consequences. Previous work studying attribution and factuality has not focused on analyzing these characteristics of language model outputs in domain-specific scenarios. In this work, we conduct human evaluation of responses from a few representative systems along various axes of attribution and factuality, by bringing domain experts in the loop. Specifically, we collect expert-curated questions from 484 participants across 32 fields of study, and then ask the same experts to evaluate generated responses to their own questions. In addition, we ask experts to improve upon responses from language models. The output of our analysis is ExpertQA, a high-quality long-form QA dataset with 2177 questions spanning 32 fields, along with verified answers and attributions for claims in the answers.",
}

@inproceedings{abacha2017overview,
  title={Overview of the Medical Question Answering Task at TREC 2017 LiveQA},
  author={Abacha, Asma Ben and Agichtein, Eugene and Pinter, Yuval and Demner-Fushman, Dina},
  booktitle={26th Text REtrieval Conference, TREC 2017},
  year={2017}
}

@inproceedings{claude2024,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={},
  url={https://api.semanticscholar.org/CorpusID:268232499}
}

@misc{ministral2024,
    author = {Mistral, AI team},
    title = {Un Ministral, des Ministraux},
    year = 2024
}

@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@inproceedings{berant2013semantic,
  title={Semantic parsing on freebase from question-answer pairs},
  author={Berant, Jonathan and Chou, Andrew and Frostig, Roy and Liang, Percy},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1533--1544},
  year={2013}
}

@inproceedings{joshi2017triviaqa,
  title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1601--1611},
  year={2017}
}

@inproceedings{trivedi2017lc,
  title={Lc-quad: A corpus for complex question answering over knowledge graphs},
  author={Trivedi, Priyansh and Maheshwari, Gaurav and Dubey, Mohnish and Lehmann, Jens},
  booktitle={The Semantic Web--ISWC 2017: 16th International Semantic Web Conference, Vienna, Austria, October 21-25, 2017, Proceedings, Part II 16},
  pages={210--218},
  year={2017},
  organization={Springer}
}

@inproceedings{dubey2019lc,
  title={Lc-quad 2.0: A large dataset for complex question answering over wikidata and dbpedia},
  author={Dubey, Mohnish and Banerjee, Debayan and Abdelkawi, Abdelrahman and Lehmann, Jens},
  booktitle={The Semantic Web--ISWC 2019: 18th International Semantic Web Conference, Auckland, New Zealand, October 26--30, 2019, Proceedings, Part II 18},
  pages={69--78},
  year={2019},
  organization={Springer}
}

@article{ngomo20189th,
  title={9th challenge on question answering over linked data (QALD-9)},
  author={Ngomo, Ngonga},
  journal={language},
  volume={7},
  number={1},
  pages={58--64},
  year={2018}
}

@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{sciavolino2021simple,
  title={Simple Entity-Centric Questions Challenge Dense Retrievers},
  author={Sciavolino, Christopher and Zhong, Zexuan and Lee, Jinhyuk and Chen, Danqi},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={6138--6148},
  year={2021}
}

@inproceedings{mallen2023not,
  title={When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories},
  author={Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9802--9822},
  year={2023}
}

@article{kumar2024automatic,
  title={Automatic question-answer generation for long-tail knowledge},
  author={Kumar, Rohan and Kim, Youngmin and Ravi, Sunitha and Sun, Haitian and Faloutsos, Christos and Salakhutdinov, Ruslan and Yoon, Minji},
  journal={arXiv preprint arXiv:2403.01382},
  year={2024}
}

@inproceedings{xu2023baize,
  title={Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data},
  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={6268--6278},
  year={2023}
}

@misc{chen2023meditron70b,
      title={MEDITRON-70B: Scaling Medical Pretraining for Large Language Models}, 
      author={Zeming Chen and Alejandro Hernández-Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas Köpf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},
      year={2023},
      eprint={2311.16079},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{xiongcan,
  title={Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs},
  author={Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and LI, YIFEI and Fu, Jie and He, Junxian and Hooi, Bryan},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@inproceedings{chen2023hallucination,
  title={Hallucination detection: Robustly discerning reliable answers in large language models},
  author={Chen, Yuyan and Fu, Qiang and Yuan, Yichen and Wen, Zhihao and Fan, Ge and Liu, Dayiheng and Zhang, Dongmei and Li, Zhixu and Xiao, Yanghua},
  booktitle={Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
  pages={245--255},
  year={2023}
}

@article{ji2023survey,
  title={Survey of Hallucination in Natural Language Generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={Association for Computing Machinery (ACM)}
}

@inproceedings{li-etal-2024-dawn,
    title = "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models",
    author = "Li, Junyi  and
      Chen, Jie  and
      Ren, Ruiyang  and
      Cheng, Xiaoxue  and
      Zhao, Xin  and
      Nie, Jian-Yun  and
      Wen, Ji-Rong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.586",
    doi = "10.18653/v1/2024.acl-long.586",
    pages = "10879--10899",
    abstract = "In the era of large language models (LLMs), hallucination (the tendency to generate factually incorrect content) poses great challenges to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucinations, focused on the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and design a simple yet effective detection method for LLM hallucinations. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucinations. Finally, we implement and examine a series of widely used techniques to mitigate the hallucinations in LLMs. Our work has led to several important findings to understand the hallucination origin and mitigate the hallucinations in LLMs.",
}

@article{xie2024me,
  title={Me-LLaMA: Foundation Large Language Models for Medical Applications},
  author={Xie, Qianqian and Chen, Qingyu and Chen, Aokun and Peng, Cheng and Hu, Yan and Lin, Fongci and Peng, Xueqing and Huang, Jimin and Zhang, Jeffrey and Keloth, Vipina and others},
  journal={Research square},
  pages={rs--3},
  year={2024}
}

@article{qiu2024towards,
  title={Towards building multilingual language model for medicine},
  author={Qiu, Pengcheng and Wu, Chaoyi and Zhang, Xiaoman and Lin, Weixiong and Wang, Haicheng and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  journal={Nature Communications},
  volume={15},
  number={1},
  pages={8384},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{jin2021disease,
  title={What disease does this patient have? a large-scale open domain question answering dataset from medical exams},
  author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  journal={Applied Sciences},
  volume={11},
  number={14},
  pages={6421},
  year={2021},
  publisher={MDPI}
}

@article{kasai2023evaluating,
  title={Evaluating gpt-4 and chatgpt on japanese medical licensing examinations},
  author={Kasai, Jungo and Kasai, Yuhei and Sakaguchi, Keisuke and Yamada, Yutaro and Radev, Dragomir},
  journal={arXiv preprint arXiv:2303.18027},
  year={2023}
}

@inproceedings{labrak-etal-2022-frenchmedmcqa,
    title = "{F}rench{M}ed{MCQA}: A {F}rench Multiple-Choice Question Answering Dataset for Medical domain",
    author = "Labrak, Yanis  and
      Bazoge, Adrien  and
      Dufour, Richard  and
      Daille, Beatrice  and
      Gourraud, Pierre-Antoine  and
      Morin, Emmanuel  and
      Rouvier, Mickael",
    editor = "Lavelli, Alberto  and
      Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio",
    booktitle = "Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.louhi-1.5/",
    doi = "10.18653/v1/2022.louhi-1.5",
    pages = "41--46",
    abstract = "This paper introduces FrenchMedMCQA, the first publicly available Multiple-Choice Question Answering (MCQA) dataset in French for medical domain. It is composed of 3,105 questions taken from real exams of the French medical specialization diploma in pharmacy, mixing single and multiple answers. Each instance of the dataset contains an identifier, a question, five possible answers and their manual correction(s). We also propose first baseline models to automatically process this MCQA task in order to report on the current performances and to highlight the difficulty of the task. A detailed analysis of the results showed that it is necessary to have representations adapted to the medical domain or to the MCQA task: in our case, English specialized models yielded better results than generic French ones, even though FrenchMedMCQA is in French. Corpus, models and tools are available online."
}

@inproceedings{vilares-gomez-rodriguez-2019-head,
    title = "{HEAD}-{QA}: A Healthcare Dataset for Complex Reasoning",
    author = "Vilares, David  and
      G{\'o}mez-Rodr{\'i}guez, Carlos",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1092/",
    doi = "10.18653/v1/P19-1092",
    pages = "960--966",
    abstract = "We present HEAD-QA, a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. We then consider monolingual (Spanish) and cross-lingual (to English) experiments with information retrieval and neural techniques. We show that: (i) HEAD-QA challenges current methods, and (ii) the results lag well behind human performance, demonstrating its usefulness as a benchmark for future work."
}

@inproceedings{blinov2022rumedbench,
  title={RuMedBench: A Russian Medical Language Understanding Benchmark},
  author={Blinov, Pavel and Reshetnikova, Arina and Nesterov, Aleksandr and Zubkova, Galina and Kokh, Vladimir},
  booktitle={International Conference on Artificial Intelligence in Medicine},
  pages={383--392},
  year={2022}
}

@article{singhal2023towards,
  title={Towards expert-level medical question answering with large language models},
  author={Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres, Rory and Wulczyn, Ellery and Hou, Le and Clark, Kevin and Pfohl, Stephen and Cole-Lewis, Heather and Neal, Darlene and others},
  journal={arXiv preprint arXiv:2305.09617},
  year={2023}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@inproceedings{pal2022medmcqa,
  title={Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering},
  author={Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  booktitle={Conference on health, inference, and learning},
  pages={248--260},
  year={2022},
  organization={PMLR}
}

@article{yasunaga2022deep,
  title={Deep bidirectional language-knowledge graph pretraining},
  author={Yasunaga, Michihiro and Bosselut, Antoine and Ren, Hongyu and Zhang, Xikun and Manning, Christopher D and Liang, Percy S and Leskovec, Jure},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37309--37323},
  year={2022}
}

@article{shi2024mgh,
  title={Mgh radiology llama: A llama 3 70b model for radiology},
  author={Shi, Yucheng and Shu, Peng and Liu, Zhengliang and Wu, Zihao and Li, Quanzheng and Li, Xiang},
  journal={arXiv preprint arXiv:2408.11848},
  year={2024}
}

@article{singhal2025toward,
  title={Toward expert-level medical question answering with large language models},
  author={Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres, Rory and Wulczyn, Ellery and Amin, Mohamed and Hou, Le and Clark, Kevin and Pfohl, Stephen R and Cole-Lewis, Heather and others},
  journal={Nature Medicine},
  pages={1--8},
  year={2025},
  publisher={Nature Publishing Group US New York}
}

@article{shi2024ehragent,
  title={Ehragent: Code empowers large language models for complex tabular reasoning on electronic health records},
  author={Shi, Wenqi and Xu, Ran and Zhuang, Yuchen and Yu, Yue and Zhang, Jieyu and Wu, Hang and Zhu, Yuanda and Ho, Joyce and Yang, Carl and Wang, May D},
  journal={arXiv preprint arXiv:2401.07128},
  year={2024}
}

@article{xiong2023doctorglm,
  title={Doctorglm: Fine-tuning your chinese doctor is not a herculean task},
  author={Xiong, Honglin and Wang, Sheng and Zhu, Yitao and Zhao, Zihao and Liu, Yuxiao and Huang, Linlin and Wang, Qian and Shen, Dinggang},
  journal={arXiv preprint arXiv:2304.01097},
  year={2023}
}

@article{chen2024cod,
  title={CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis},
  author={Chen, Junying and Gui, Chi and Gao, Anningzhe and Ji, Ke and Wang, Xidong and Wan, Xiang and Wang, Benyou},
  journal={arXiv preprint arXiv:2407.13301},
  year={2024}
}

@inproceedings{xiong2024can,
title={Can {LLM}s Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in {LLM}s},
author={Miao Xiong and Zhiyuan Hu and Xinyang Lu and YIFEI LI and Jie Fu and Junxian He and Bryan Hooi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gjeQKFxFpZ}
}

@article{zhao2024helene,
  title={HELENE: Hessian Layer-wise Clipping and Gradient Annealing for Accelerating Fine-tuning LLM with Zeroth-order Optimization},
  author={Zhao, Huaqin and Li, Jiaxi and Pan, Yi and Liang, Shizhe and Yang, Xiaofeng and Liu, Wei and Li, Xiang and Dou, Fei and Liu, Tianming and Lu, Jin},
  journal={arXiv preprint arXiv:2411.10696},
  year={2024}
}

@inproceedings{chen-etal-2023-close,
    title = "A Close Look into the Calibration of Pre-trained Language Models",
    author = "Chen, Yangyi  and
      Yuan, Lifan  and
      Cui, Ganqu  and
      Liu, Zhiyuan  and
      Ji, Heng",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.75/",
    doi = "10.18653/v1/2023.acl-long.75",
    pages = "1343--1367",
    abstract = "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs' calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don`t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs' confidence in wrong predictions."
}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}

@inproceedings{xiao2022uncertainty,
  title={Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis},
  author={Xiao, Yuxin and Liang, Paul Pu and Bhatt, Umang and Neiswanger, Willie and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={7273--7284},
  year={2022}
}

@article{minderer2021revisiting,
  title={Revisiting the calibration of modern neural networks},
  author={Minderer, Matthias and Djolonga, Josip and Romijnders, Rob and Hubis, Frances and Zhai, Xiaohua and Houlsby, Neil and Tran, Dustin and Lucic, Mario},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={15682--15694},
  year={2021}
}

@article{yang2010nurses,
  title={Nurses’ risk assessment judgements: A confidence calibration study},
  author={Yang, Huiqin and Thompson, Carl},
  journal={Journal of Advanced Nursing},
  volume={66},
  number={12},
  pages={2751--2760},
  year={2010},
  publisher={Wiley Online Library}
}

@article{jiang2012calibrating,
  title={Calibrating predictive model estimates to support personalized medicine},
  author={Jiang, Xiaoqian and Osl, Melanie and Kim, Jihoon and Ohno-Machado, Lucila},
  journal={Journal of the American Medical Informatics Association},
  volume={19},
  number={2},
  pages={263--274},
  year={2012},
  publisher={BMJ Group}
}

@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}

@inproceedings{thakur-2020-AugSBERT,
  title = "Augmented {SBERT}: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks",
  author = "Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes  and Gurevych, Iryna",
  booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
  month = jun,
  year = "2021",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/2021.naacl-main.28",
  pages = "296--310",
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{karpukhin-etal-2020-dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550/",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
}

@article{ram2023context,
  title={In-context retrieval-augmented language models},
  author={Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={1316--1331},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{shi-etal-2024-replug,
    title = "{REPLUG}: Retrieval-Augmented Black-Box Language Models",
    author = "Shi, Weijia  and
      Min, Sewon  and
      Yasunaga, Michihiro  and
      Seo, Minjoon  and
      James, Richard  and
      Lewis, Mike  and
      Zettlemoyer, Luke  and
      Yih, Wen-tau",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.463/",
    doi = "10.18653/v1/2024.naacl-long.463",
    pages = "8371--8384",
    abstract = "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3{\%}, as well as the performance of Codex on five-shot MMLU by 5.1{\%}. Code is publicly released at github.com/swj0419/REPLUG."
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}

@inproceedings{chen2024benchmarking,
  title={Benchmarking large language models in retrieval-augmented generation},
  author={Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={17754--17762},
  year={2024}
}

@article{wang2024assessing,
  title={Assessing and enhancing large language models in rare disease question-answering},
  author={Wang, Guanchu and Ran, Junhao and Tang, Ruixiang and Chang, Chia-Yuan and Chuang, Yu-Neng and Liu, Zirui and Braverman, Vladimir and Liu, Zhandong and Hu, Xia},
  journal={arXiv preprint arXiv:2408.08422},
  year={2024}
}

@inproceedings{chen2024rarebench,
  title={RareBench: Can LLMs Serve as Rare Diseases Specialists?},
  author={Chen, Xuanzhong and Mao, Xiaohao and Guo, Qihan and Wang, Lun and Zhang, Shuyang and Chen, Ting},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={4850--4861},
  year={2024}
}

@article{liu2023evaluating,
  title={Evaluating large language models for radiology natural language processing},
  author={Liu, Zhengliang and Zhong, Tianyang and Li, Yiwei and Zhang, Yutong and Pan, Yi and Zhao, Zihao and Dong, Peixin and Cao, Chao and Liu, Yuxiao and Shu, Peng and others},
  journal={arXiv preprint arXiv:2307.13693},
  year={2023}
}

@article{yan2023multimodal,
  title={Multimodal ChatGPT for medical applications: an experimental study of GPT-4V},
  author={Yan, Zhiling and Zhang, Kai and Zhou, Rong and He, Lifang and Li, Xiang and Sun, Lichao},
  journal={arXiv preprint arXiv:2310.19061},
  year={2023}
}

@inproceedings{naeini2015obtaining,
  title={Obtaining well calibrated probabilities using bayesian binning},
  author={Naeini, Mahdi Pakdaman and Cooper, Gregory and Hauskrecht, Milos},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={29},
  number={1},
  year={2015}
}

@article{moor2023foundation,
  title={Foundation models for generalist medical artificial intelligence},
  author={Moor, Michael and Banerjee, Oishi and Abad, Zahra Shakeri Hossein and Krumholz, Harlan M and Leskovec, Jure and Topol, Eric J and Rajpurkar, Pranav},
  journal={Nature},
  volume={616},
  number={7956},
  pages={259--265},
  year={2023},
  publisher={Nature Publishing Group UK London}
}