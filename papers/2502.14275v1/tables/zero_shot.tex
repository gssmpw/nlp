\begin{table*}[t!]

\centering
\resizebox{\textwidth}{!}{
    \begin{tabular}
    {>{\raggedright\arraybackslash}p{4cm}>{\centering\arraybackslash}p{3cm}>{\centering\arraybackslash}p{4cm}>{\centering\arraybackslash}p{4cm}}
        \toprule
        \textbf{Models} & \textbf{Accuracy} & \textbf{Positive Accuracy} & \textbf{Negative Accuracy} \\
        \midrule
        gpt-3.5-turbo & 84.25 & 78.00 & 86.33 \\
        gpt-4o-mini & 82.50 & 70.00 & 86.67 \\
        claude-3-haiku & 71.75 & 88.00 & 66.33 \\
        claude-3-sonnet & 82.75 & 68.00 & 87.67 \\
        \midrule
        Llama-3-8B-Instruct & 81.00 & 61.00 & 87.67 \\
        Llama-3.1-8B-Instruct & 78.25 & 61.00 & 84.00 \\
        Llama-3.2-1B-Instruct & 74.75 & 12.00 & 95.67 \\
        Llama-3.2-3B-Instruct & 76.75 & 52.00 & 85.00 \\
        Ministral-8B-Instruct & 81.25 & 48.00 & 92.33 \\
        Qwen2.5-0.5B & 66.00 & 60.00 & 68.00 \\
        Qwen2.5-1B & 63.00 & 83.00 & 56.33 \\
        Qwen2.5-3B & 78.25 & 16.00 & 99.00 \\
        Phi-3-mini-4k-instruct & 79.50 & 74.00 & 81.33 \\
        Phi-3-mini-128k-instruct & 81.75 & 68.00 & 86.33 \\
        Phi-3.5-mini-instruct & 79.25 & 77.00 & 80.00 \\
        \bottomrule
    \end{tabular}
}
\caption{LLM performances with zero-shot prompting. All numbers are in percentage~(\%).}
\label{tab:zero_shot_not_used}
\end{table*}