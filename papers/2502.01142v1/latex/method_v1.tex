\section{Adaptive Inference-time Compute Generation}

% In this section, we will introduce the pipeline of SRAG and formulate it as a Markov Decision Process (MDP).

% \subsection{SRAG Pipeline}
% In SRAG pipeline, there are three key actions: search and thought, and answer.
% % 
% In the search action, LLM generates a query to retrieve relevant knowledge from the knowledge base.  
% % 1. 思考 基于问题、检索回来的信息以及内部信息 进行融合和思考从而推理出正确答案
% In the thought action, LLM analyzes the question, integrate the retrieved information with its internal knowledge, reason through the problem, and then decide whether to continue the retrieval process or provide a direct answer.
% In the answer action, LLM the LLM provides a standardized response based on the given question and all the information gathered through the previous steps.

% For instance, given a question, the following are common scenarios that may occur:
% 1) Internal Knowledge Utilization: The LLM can directly provide an answer based on its pre-existing internal knowledge, without the need to query an external knowledge base.  
% 2) External Knowledge Retrieval: The question requires information beyond the LLM's internal knowledge, necessitating the retrieval of external knowledge. In this case, the LLM queries an external knowledge source, either once or multiple times, to gather the necessary information for answering the question.  
% 3) Hybrid Knowledge Integration: The question demands both internal and external knowledge to formulate an accurate response. In this scenario, the LLM must retrieve relevant external information and integrate it with its internal knowledge to infer the final answer.

% \subsection{Fomulation}
% We formulate this process as a Markov Decision Process (MDP) with state transitions governed by three possible actions. 
% % 
% The key components of the MDP are defined as follows: states, actions, transition function, policy, and reward function. 
% % 
% This framework encapsulates the interaction between the LLM, its internal knowledge, and external knowledge sources to generate answers.

% At any time step \( t \), the state \( s_t \) represents the complete reasoning chain, encompassing the initial question \( q \) and the history of actions and observations. 
% It can be defined as follows:

% \begin{equation}
% s_t = \{ q, (a_1, o_1), (a_2, o_2), \dots, (a_{t-1}, o_{t-1}) \}
% \end{equation}
% where \( q \) denotes the question, \( a_i \) represents the action taken at each time step (which may include search, thought, or answer), and \( o_i \) refers to the output resulting from executing action \( a_i \), such as the retrieved documents, intermediate inference information, or the final answer.

% The state transition function \( T \) defines how the state \( s_t \) evolves to \( s_{t+1} \) after action \( a_t \) is executed, formally expressed as:

% \begin{equation}
% s_{t+1} = T(s_t,a_t)=s_t\cup \{(a_t, o_t)\}
% \end{equation}

% The reward function is composed of two main components: the accuracy of the final answer and the efficiency of the retrieval process. It is defined as follows:

% \begin{align}
% R(s_t, &a_t, o_t) =  -\alpha \cdot \sum_{t=1}^{T} \mathbb{I}(a_t = \text{search}) \notag \\
% & + \beta \cdot \mathbb{I}(a_T = \text{answer}) \cdot \text{Score}(o_T)
% \end{align}
% where \( \mathbb{I} \) is the indicator function, \( \alpha \) and \( \beta \) are weighting parameters, and \( \text{Score}(o_T) \) quantifies the accuracy of the final output \( o_T \).

% \section{Method}
% In this section, we will detail how to enable LLM to learn the adaptive RAG method and find its knowledge boundary.
% % 
% As illustrated in Figure~\ref{fig:srag}, SRAG comprises two stages. 
% In Stage I, LLM acquires the SRAG pipeline through imitation learning. 
% % 
% In Stage II, the LLM develops the optimal trajectory for efficient and effective learning by leveraging self-evolution. This two-stage approach ensures that SRAG not only learns foundational patterns but also continuously improves its learning strategies to achieve superior performance.

% \subsection{Imitation Learning}
% % 为了将LLM的行为规范到 search，thought， answer这三个action下，我们构造相关的数据让LLM进行模仿学习。
% % 具体的，我们通过ICL的方式让模型学习到什么时候调用检索
% % Your approach should flexibly decide whether to search for additional information or to answer the question directly based on the context and your internal knowledge.

% % In this way, 我们筛选出LLM正确推理出答案的case。为了maximize数据利用率，同时提高模型的推理能力，对于错误的数据，我们将答案给模型，让模型重新探索检索的query和推理过程，构造伪推理路径，让模型学习。


% To guide the behavior of the LLM in performing search, thought, and answer actions, we construct relevant datasets for imitation learning. The dataset can be formulate as follows:
% \begin{equation}
%     D_{imi} = \{ q^{(i)}, a_1^{(i)}, o_1^{(i)},  a_2^{(i)}, \dots, a_{T_i}^{(i)}, o_{T_i}^{(i)} \}_{i=1}^{N}
% \end{equation}


% % 
% Specifically, we employ In-Context Learning (ICL) to teach the model when to invoke retrieval. 
% % 
% The approach enables the model to flexibly decide whether to search for additional information or provide an answer directly based on the context and its internal knowledge.

% Through this process, we identify instances where the LLM successfully infers the correct answer. To maximize data efficiency and enhance the model's reasoning ability, for incorrect cases, we provide the correct answer and allow the model to re-explore the retrieval query and reasoning process, generating pseudo-reasoning paths for further learning.

% Through the two steps outlined above, the training dataset includes some sub-optimal adaptive decision-making steps within the SRAG framework. 
% This dataset helps the language model better understand the decision-making process and the transitions between the three actions. 
% To further enhance the model's adaptive decision-making capabilities, we fine-tune the LLM on this dataset. 
% Specifically, we apply a masking technique to the loss associated with the searched document in order to mitigate the impact of noise introduced during document retrieval, the loss function is as follows:

% \begin{align}
% &\mathcal{L}(D_{imi}; \theta) = \frac{1}{N} \sum_{i=1}^{N} \left[ \sum_{t=1}^{T_i} \log p_{\theta}(a_t^{(i)} | s_t^{(i)}) \right.  \\
% &\quad \left. + \mathbb{I}\left(a_t^{(i)} \neq \text{search}\right) \cdot \log p_{\theta}\left(o_t^{(i)} \mid s_t^{(i)}, a_t^{(i)}\right) \right]  \notag
% \end{align}

% \subsection{Decision Evolution via Direct Preference Optimization}
% % To futher expolore the knowledge boudary of LLM meanwhile optimize the reward function $R(s_t,a_t,o_t)$, we 进一步使用强化学习方法来优化模型的表现。
% % Specifically，我们让模型自己探索知识边界。
% % after imitation learning，模型以及具备了初步的adaptive rag的能力，然而，仅仅通过模仿学习无法让他他们完全认识到知识边界，这通常会增加检索开销。更严重地，引入了不必要的检索会引入噪声，导致原本模型能答对的问题反而答错了。

% % 因此，我们通过采样模型的不同candidate responses，并基于reward function去构造数据。
% % 我们的优化目标主要有effective 和 efficient，因此as shown in fig 我们构造两大类数据，一类表示effective偏好，一类表示efficient偏好，基于此，我们使用直接偏好优化来帮助模型探索知识边界。

% To further explore the knowledge boundaries of the LLM while optimizing the reward function \( R(s_t, a_t, o_t) \), we extend the model's capabilities using reinforcement learning techniques. Specifically, we enable the model to autonomously refine its decision-making policy through Direct Preference Optimization (DPO).

% After imitation learning, the model has acquired a basic level of adaptive RAG capabilities. However, relying solely on imitation learning is insufficient for fully recognizing its knowledge boundaries, often leading to increased retrieval costs. More critically, unnecessary retrievals can introduce noise, causing the model to incorrectly answer questions that it would otherwise answer correctly.

% To address this challenge, we sample multiple candidate responses from the model and use the reward function to generate preference data for further optimization. Our optimization objectives prioritize two key aspects: (1) effectiveness, in terms of answer accuracy, and (2) efficiency, in terms of retrieval cost. 
% % 
% As illustrated in Figure~\ref{fig:srag}, we construct two types of preference pairs: one that reflects preferences based on answer accuracy, and another based on retrieval efficiency. Through Direct Preference Optimization, we guide the model to explore its knowledge boundaries and optimize the balance between these two objectives.

% Based on above, we define two prefence dataset $D_{acc}$ and $D_{ret}$. In accuracy-based dataset $D_{acc} = \{( q^{(i)}, s^{(i)}_w, s_l^{(i)}\})_{i=1}^{M}$, $s^{(i)}_w$ represents the exploration state that leads to the correct answer, while $s^{(i)}_l$ represents the state that results in an incorrect answer. In efficiency-based dataset $D_{ret}$, both $s^{(i)}_w$ and $s^{(i)}_l$ lead to the correct answer, but $s^{(i)}_w$ corresponds to a more efficient exploration, leading to a lower retrieval cost compared to $s^{(i)}_l$.

% Then, the two datasets are used to guid the model's optimization based on Direct Preference Optimization.
% The  loss function is defined as:

% \begin{align}
% &\mathcal{L}_{\text{SRAG}} = 
% -\mathbb{E}_{(x, y_w, y_l) \sim \{D_{acc},D_{ret}\}} \\
% &\left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_{\theta}(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right] \notag
% \end{align}


