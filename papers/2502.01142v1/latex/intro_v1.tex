\section{Introduction}

% 大模型已经展现出remarkable能力。
% 然而，由于模型容量和能力的限制，模型经常生成错误信息
% 一个promising的方法来解决幻觉就是RAG
Large Language Models (LLMs) have gained increasing prominence in the field of artificial intelligence.
However, limited by the capacity and capabilities of LLM, it still suffers from severe factual hallucination problems due to the lack of intrinsic knowledge.
Retrieval-augmented generation (RAG) presents a promising approach to mitigate these hallucinations, thereby enhancing the factual reliability of LLM outputs.

% RAG的流程
% 理想情况下，通过RAG，模型可以利用外部信息来弥补内部信息的不足。
% 然而在实际应用中，1. 外部信息有噪声 会影响模型的生成 2. 外部信息和内部信息有冲突，导致模型不知道该信任谁。
% 基于此，最近研究focus 在adaptive-rag，which aiming to adaptive decide when and what to retrieve on demand.
% The core of adaptive RAG is to 
% 例如，在case 中，对于xxx问题，which 对应于2个子问题，对于LLM会的地方不用检索，只要检索不会的地方

 
Ideally, RAG enables models to access pertinent external information, thereby enhancing the reliability of generated responses. 
% 
However, the effectiveness of RAG is impeded by two primary challenges. 
First, the presence of noise in external data sources can degrade the quality of the generated content. 
Second, conflicts between external and internal knowledge bases can create uncertainty for models in determining which information to trust. 
% 
Consequently, recent studies have concentrated on adaptive RAG methods, which aim to dynamically decide when and what to retrieve based on the specific requirements of each query and the varying knowledge backgrounds of LLMs.
% 
The core of adaptive RAG lies in optimizing the balance between utilizing internal knowledge and incorporating external information to enhance both the accuracy and efficiency of the retrieval process.
% 
For instance, in [lack case] certain scenarios where traditional approaches typically require two retrievals, an adaptive RAG system might identify sub-questions that the LLM can address using its internal knowledge and selectively retrieve information only for the aspects that exceed its capabilities.

\input{table/compare}

% 为了实现adaptive-rag从而最大化对内部知识的利用来提高检索效率，最近做了很多研究，
% 一方面，基于分类器的方法(SKR, UAR) 首先探针模型的内部知识情况，再训练分类器来根据内部知识决定是否要检索。然而，这样的pipeline不可避免的会有误差传播问题，并且这类方法在决策时通常只基于query或hidden states等部分信息，失去了对推理过程的感知。
% 另一方面，基于不确定探究模型内部知识的方法，通常需要借助各种中间指标（例如启发式指标、设置threshold），使得它非常没有鲁棒性。

To optimize the utilization of internal knowledge and enhance retrieval efficiency, recent research has explored a variety of strategies. On one hand, classifier-based approaches (e.g., SKR, UAR) investigate the model's internal knowledge and train classifiers to determine the necessity of retrieval. However, these methods are vulnerable to error propagation and often rely exclusively on partial information, such as queries or hidden states, thereby lacking comprehensive insight into the reasoning process. On the other hand, approaches that assess uncertainty within the model’s internal knowledge typically depend on heuristic metrics or predefined threshold settings, which compromise their overall robustness.


% In this paper, 我们提出了srag，一种新的框架让模型通过self-evolve来自适应的决定检索的时机和检索的内容，从而达到检索正确率和效率的最优。
% 具体来说，SRAG 定义了 search 和 thought 行为，given a query，模型自适应的选择采取thought行为进行思考，根据内部知识和已有的外部信息决定 直接回答 or 继续检索 ，或使用search行为生成检索query，查询外部知识库。
% 进一步，我们使用强化学习方法，让llm自己探索决策路径，鼓励模型在正确回答问题的同时尽可能降低检索次数，从而最大化内部知识与上下文的利用。


In this paper, we introduce SRAG, a novel framework that enables models to self-evolve and adaptively determine both the timing and content of information retrieval. SRAG defines three primary actions: ``search'', ``thought'', and ``answer''.
Given a query, the LLM can adaptively choose to engage in the ``thought'' action to analyze the question, integrate the retrieved information with its internal knowledge, reason through the problem, and then decide whether to continue the retrieval process or provide a direct answer.
Alternatively, the LLM may transition to the ``search'' action to generate retrieval queries and access external knowledge sources.
If the ``thought'' action determines that it is appropriate to answer directly, the LLM will provide a response based on all available information and conclude the inference process.
% 
Specifically, we first enable LLM to learn the inference pattern by imitation learning (Stage I). 
Furthermore, we employ reinforcement learning techniques to allow the LLM to explore decision-making pathways autonomously (Stage II). 
This approach encourages the model to answer questions accurately while minimizing the number of retrieval operations, thereby optimizing the integration of internal knowledge and contextual information.


% 基于subset of hotpotqa， wikihop, asqa 我们构建了训练数据来提高xxx。我们conduct experiment on 3 in-distribution dataset and 1 out-of-ditribution data。实验结果表明，通过self-evolve，我们的方法能够显著的提高balabala

% In summary,  the contribution are as follows:
%   - 我们提出了让LLM自我探索能力边界的adaptive-rag的方法来提高rag能力并优化检索效率
  % - 衡量adaptive-rag的指标【effective+efficient】
  % - 实验结果好

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/srag.pdf}
    \caption{Caption}
    \label{fig:srag}
\end{figure*}



To enhance the effectiveness of SRAG, we developed training datasets derived from training subsets of HotpotQA, 2WikiHop, and ASQA. We conducted experiments on three in-distribution datasets and one out-of-distribution dataset. The results demonstrate that our self-evolving approach significantly improves both the accuracy and efficiency of retrieval processes.


In summary, the contributions of this paper are as follows:
\begin{enumerate}
    \item We propose an adaptive RAG framework that enables LLMs to autonomously explore their knowledge boundaries, thereby enhancing RAG capabilities and optimizing retrieval efficiency.
    \item Experimental on 3 in-distribution datasets and 1 out-of-distribution dataset validate that our method can significantly enhance both the accuracy and efficiency of retrieval processes with great generalization ability.
\end{enumerate}

