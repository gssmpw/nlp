\section{Related Work}
% - adaptive RAG 
%   - classifier 
%     - Adaptive RAG, UAR
%     - 分类器方法引入额外参数，并且只对部分信息探针
%   - confidence 
%     - FLARE, DRAGIN
%     - 置信度方法基于阈值
%   - LLM verbolize 
%     - Note-enhanced, ReAct, critic, self-rag， retrieval QA
%     - veribolize和内部知识不符合
% - 强化学习
%   - 最近的研究表明强化学习（例如，rlhf,dpo）通过比模仿学习可以让模型效果更好。
%   - 然而，rlhf复杂且耗时，同时有研究表明基于模型response进行dpo可以达到类似的效果，因此本文采用dpo作为强化学习的手段

  
\paragraph{Adaptive Retrieval-Augmented Generation}
% adaptive RAG方法通常可以分为三类 基于classifier 的方法、基于probing的方法、基于LLM 生成的方法。
% 
% 基于分类器的方法，利用分类器来决定是否需要检索，Adaptive RAG使用分类器将query按检索难度分为不检索、单次检索、和多次检索；UAR通过对query从用户意图、时间-aware、knowledge-aware、self-aware四个角度构建分类数据，并训练分类器来决定检索、不检索。
% 
% 基于置信度的方法,通过模型内部状态对于query的不确定性进行决策，FLARE initiates retrieval when any token in a generated sentence has a probability below a certain threshold, while DRAGIN (Su et al., 2024) considers both token uncertainty and the attention weights between successive tokens to determine when to   retrieve additional information.
% 
% 基于LLM生成的方法，让LLM生成是否需要决策，self-rag, retrival QA通过判断模型是否过生成需要检索的token来决定检索。
% 
% 我们的方法基于LLM-based 方法，在学习检索决策的基础上，我们通过强化学习最大化其对知识边界的利用。

% Adaptive Retrieval-Augmented Generation~\cite{jeong2024adaptive} methods are typically categorized into three types: classifier-based, probing-based, and LLM-based generative approaches.

% Existing adaptive RAG approaches can be broadly categorized into three types.
% Classifier-based methods~\cite{cheng2024unified,jeong2024adaptive} leverage classifiers to decide if retrieval is needed.
% % 
% Confidence-based methods~\cite{jiang2023flare,su-etal-2024-dragin} utilize model uncertainty metrics.
% % 
% LLM-based methods~\cite{asai2023self,zhang2024retrievalqa}. allow models to generate their own retrieval decisions.
% % 
% Our method aims to enhance model performance and knowledge utilization during the retrieval-augmented process, achieving both effectiveness and efficiency.


Existing adaptive RAG approaches can be broadly categorized into three types: classifier-based methods \cite{cheng2024unified,jeong2024adaptive} requiring additional linear head training for retrieval decisions, confidence-based methods \cite{jiang2023flare,su-etal-2024-dragin,dhole2025retrieveretrieveuncertaintydetection} relying heavily on threshold-dependent uncertainty metrics, and LLM-based methods~\cite{asai2023self,zhang2024retrievalqa} generating retrieval decisions but often fail to accurately recognize their knowledge boundaries, making it unreliable to delegate retrieval timing decisions to the model.
% 
Our method leverages the inherent generative capabilities of LLMs to explore knowledge boundaries in RAG settings. This design maintains the model's native generation abilities while eliminating the need for additional parameters or unreliable uncertainty metrics.




% Our method optimizes both model performance and knowledge utilization during the retrieval-augmented process, achieving a balance between effectiveness and efficiency.




% Recently, more and more work incorpoare reasoining into RAG. Self-RAG and Auto-RAG incroparete a automatic data sytehsis method to teach models to reason in RAG.
% search-o1 incorpate retrieval in reasoning models 的到一个agentic model.
% AirRAG incorpating Monte Carlo Tree Search  and self-consistency.

% 和他们消耗过度检索和推理开销以及大规模reasoning models不同，DeepRAG 通过dynamic coginition decisiotns to achieving both effectiveness and efficiency.

\paragraph{Reasoning in Retrieval-Augmented Generation}
Recent advances in RAG have increasingly focused on incorporating reasoning capabilities. 
Self-RAG~\cite{asai2023self} and Auto-RAG~\cite{yu2024autorag} leverage automatic data synthesis to enhance reasoning within retrieval-augmented frameworks. 
Search-o1~\cite{li2025search} incorporates retrieval into inference to construct an agentic system, though its applicability is limited to o1-like large reasoning models.
AirRAG~\cite{feng2025airragactivatingintrinsicreasoning} combines Monte Carlo Tree Search and self-consistency.
% 
In contrast to these approaches that rely heavily on extensive retrieval operations or large reasoning models, DeepRAG provides an end-to-end method, enabling an arbitrary model to think to retrieval step by step on demand.


\paragraph{Knowledge Boundary}
% https://arxiv.org/pdf/2402.11493v1  Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation
% 知识边界是一个很重要的问题。By helping LLM exploring knowledge boundary, it can know what they know and what they don't know. In this way, LLM can reduce its hallucination to a large extend. 
% Typically, these works attempt to delineate the boundary of model knowledge through explicit prompting (Zhang et al., 2023a; Yang et al., 2023; Cheng et al., 2024; Wan et al., 2024), self eliciting (Chen et al., 2024a; Lin et al., 2024), self-evaluation (Zhang et al., 2024) or by probing the model’s internal states (Liang et al., 2024). wen etal  uses a on-policy fine-grained knolwedge feedback to mitigate hallucination by helping LLM aware thier knowledge boundary.

% Knowledge boundaries are crucial for reducing hallucinations in LLMs. By exploring these boundaries, LLMs can distinguish what they know from what they don’t, improving accuracy.
% Help LLMs realize its knowledge boundary can reduce hallucinations.~\cite{yin2024benchmarking}
% % 
% xxx已经表明基于explicit prompting的方法存在明显缺陷，模型内外知识不匹配。
% 通常需要进一步的微调 or 准确的探针来calibrate 认知。
% Considering LLM的自然生成的天性，我们的方法旨在保持语言模型原有能力的情况下探索其在RAG场景下的知识边界。
% 
% Help LLMs realize its knowledge boundary can reduce hallucinations.~\cite{}
% 
% Prior wor has shown that explicit prompting-based methods have significant limitations, as they often lead to mismatches between internal knowledge and its output.
LLMs struggle to accurately distinguish between what they know and what they don't know~\cite{yin2023large,kapoor2024large,yin2024benchmarking}.
% 
Additional fine-tuning~\cite{kapoor-etal-2024-calibration} or precise probing~\cite{cheng2024unified} is typically required to calibrate the model's cognition. 
% 
Our approach explores knowledge boundaries in RAG settings.


% takes a more efficient route by implementing dynamic cognition decisions, effectively balancing both performance and computational efficiency.


% In contrast, DeepRAG achieves better efficiency through dynamic cognition decisions, avoiding the computational overhead of extensive retrieval and reasoning operations.


% \subsection{Reinforcement Learning}

% % https://arxiv.org/pdf/2405.08448
% % https://arxiv.org/pdf/2409.02795

% % online dpo
% % https://arxiv.org/pdf/2402.04792 DPO from AI feedback
% % https://arxiv.org/pdf/2309.06657 RFT
% % https://arxiv.org/pdf/2312.16682 iterative DPO

% % 从人类反馈中进行强化学习已成为一种值得注意的方法，在LLM领域取得了巨大成功。
% % The complexity and instability of the RLHF pipeline have driven recent efforts to develop RL-free methods that can compete with RLHF (Rafailov et al., 2024; Song et al., 2024; Liu et al., 2023).
% % Nowadays，DPO 已经成为了a popular strategy for enhancing LLM abilities.
% % 此外，~\cite{understanding,xxx} 表明，基于on-policy 数据的优化能够提高优化的效率和效果。因此本文采用探索模型genereated response的方法从而提升模型在adaptive rag的能力。


% % Reinforcement learning from human feedback (RLHF) has emerged as a prominent method, achieving significant success in the field of LLMs. 
% % However, the complexity and instability of the RLHF pipeline have led to recent efforts to develop RL-free methods that can rival RLHF performance (Rafailov et al., 2024; Song et al., 2024; Liu et al., 2023).
% Recently, Direct Preference Optimization (DPO) has become a popular RL-free method that can rival RLHF performance (Rafailov et al., 2024; Song et al., 2024; Liu et al., 2023).
% Additionally, studies (e.g., \cite{understanding, xxx}) suggest that on-policy data optimization can improve both the efficiency and effectiveness of training.
% % 
% Thus, in this paper, we adopt a model-exploration approach based on generated responses to enhance the LLM’s performance in adaptive RAG.
