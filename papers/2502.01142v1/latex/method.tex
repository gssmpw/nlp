\section{Thinking to Retrieval Step by Step}

In this section, we introduce our proposed method DeepRAG.
At its core, DeepRAG treats the process of question decomposition, atomic decisions, and final answer generation as a Markov Decision Process (MDP).
% 
As shown in Figure~\ref{fig:main}, our framework comprises three key steps: 
1) Binary Tree Search, which constructs a binary tree for each subquery related to the given question, exploring paths based on either parametric knowledge or external knowledge base; 
2) Imitation Learning, which extracts the reasoning process that arrives at the correct final answer with minimum retrieval cost for imitation learning; 
3) Chain of Calibration, which calibrates the LLM's internal knowledge by calibrating each atomic decision. 
% 
Specifically, given a set of supervised datasets, we first employ binary tree search to synthesize data for imitation learning, enabling the model to learn effective retrieval patterns. 
Subsequently, we use binary tree search to construct preference data for further calibrating the LLM's awareness of its knowledge boundaries. 
% 
% All these phases rely solely on input data and answers without additional supervision, enabling us to calibrate LLM's knowledge boundaries in RAG scenarios with any supervised dataset. 
In the following subsections, we will describe each component of DeepRAG in detail.


\subsection{Overview of the MDP Modeling}
\label{overview}
We formalize the step by step reasoning process for retrieval-augmented generation as a Markov Decision Process (MDP) defined by the tuple \((\mathcal{S}, \mathcal{A}, P, R)\), which comprises a set of states \(\mathcal{S}\), actions \(\mathcal{A}\), transition dynamics \(P\), and a reward function \(R\).


\paragraph{States.}
At each step \(t\), the state \(s_t \in \mathcal{S}\) represents the partial solution to the original question. We denote
$s_t = \bigl[x,\,(q_1, r_1),\;\dots,\,(q_{t}, r_{t})\bigr]$,
where \(x\) is the input question, and \((q_i, r_i)\) captures the \(i\)-th subquery along with the intermediate answer (and any retrieved documents). 

\paragraph{Actions.}
At state \(s_t\), the model selects an action \(a_{t+1} = (\sigma_{t+1}, \delta_{t+1}) \in \mathcal{A}\), which consists of two sub-decisions:

1. \emph{Termination decision}:  Given the partial solution \(s_t\), the model makes a binary decision \(\sigma_{t+1} \in \{\texttt{continue}, \texttt{terminate}\}\) to determine whether to proceed with generating the next subquery \(q_{t+1}\) or finalize the answer \(o\). 

2. \emph{Atomic decision}: For each subquery \(q_{t+1}\), the model decides whether to retrieve external knowledge or rely solely on its parametric knowledge. Formally, this decision is represented as \(\delta_{t+1} \in \{\texttt{retrieve}, \texttt{parametric}\}\).


\paragraph{Transitions.}  
After executing the action \(a_{t+1} = (\sigma_{t+1}, \delta_{t+1})\) in state \(s_t\), the environment updates the state to \(s_{t+1}\).  

Specifically, if \(\sigma_t = \texttt{terminate}\), the process concludes by generating the final answer \(o\), resulting in the terminal state \(s_{t+1} = \bigl[x,\,(q_1,r_1),\;\dots,\,(q_{t}, r_{t}), o\bigr]\). Otherwise, it generates the next subquery \(q_{t+1}\).  

If \(\delta_{t+1} = \texttt{retrieve}\), the model retrieves documents \(d_{t+1}\) and generates an intermediate answer \(ia_{t+1}\) for subquery \(q_{t+1}\). Otherwise, it relies on parametric knowledge to generate the intermediate answer. 
The response \(r_{t+1}\) is set as \([d_{t+1}, ia_{t+1}]\) (if retrieved) or \(ia_{t+1}\) (if not). The updated state is  
\(s_{t+1} = \bigl[x,\,(q_1,r_1),\;\dots,\,(q_{t+1}, r_{t+1})\bigr]\).

\paragraph{Rewards.}  
The reward function evaluates the state based on answer correctness and retrieval cost, applied only after generating the final answer \(o\). Formally,  
$R\bigl(s_{t+1} = s_t+[o]\bigr) = -C(o) \times T(s_t)$,
where \(C(o)\) indicates correctness (\(1\) if correct, \(\infty\) otherwise), and \(T(s_t)\) represents the total retrieval cost in state \(s_t\).

\subsection{Binary Tree Search}

In Section~\ref{overview}, we model the step-by-step reasoning process as a Markov decision process, where the LLM iteratively decomposes a given question into subqueries, each derived from previously acquired information. The detailed generation instruction is outlined in Appendix~\ref{template}, with the answer format presented below.  

Building on this formulation, we implement a binary tree search to construct reasoning paths that integrate different retrieval strategies for each subquery. As illustrated in Figure~\ref{fig:main}, given a question, the model generates the \(i\)-th subquery and explores two answering strategies: directly leveraging parametric knowledge (blue node) or retrieving external documents (green node). This approach not only decomposes the question into a sequence of forward-dependent subqueries but also thoroughly examines the influence of retrieval choices on the final answer.
\begin{figure}[h]
\begin{tcolorbox}[colframe=cyan!40!black,title=\textbf{Answer format}]
\small
\textbf{Question}: <Question>

\textbf{Follow up}: <Subquery1>

Let's search the question in Wikipedia.

Context: <Paragraph Text>

\textbf{Intermediate answer}: <Intremediate Answer1>

\textbf{Follow up}: <Subquery2>

\textbf{Intermediate answer}: <Intermediate Answer2>

......

\textbf{So the final answer is}: <Answer>
\end{tcolorbox}
\end{figure}


\subsection{Imitation Learning}
In this section, we present an algorithm that leverages binary trees to identify the optimal reasoning process that leads to the correct final answer while minimizing retrieval costs, corresponding to the highest reward as defined in Section~\ref{overview}.
% 
Based on the synthesized optimal reasoning data, we fine-tune the model to improve its termination and atomic decisions while enhancing its query decomposition capabilities and generating faithful intermediate answers, thereby enabling a more comprehensive and coherent \textit{retrieval narrative} process.


\input{latex/imi_algorithm}

\paragraph{Synthesizing Data}  As illustrated in Algorithm~\ref{algorithm:imi}, we utilize a priority queue to efficiently explore potential reasoning trajectories. This approach allows us to prioritize paths with lower retrieval costs, balancing accuracy and computational efficiency. 
The algorithm iteratively constructs and evaluates reasoning paths until either discovering a process that generates the correct answer or exhausting all viable options within specified constraints.

% 讲loss
Through the synthesis process above, the training dataset obtained contains an adaptive inference process, which can be used to facilitate arbitrary language models in gaining the capacity for adaptive inference-time compute generation.


\paragraph{Training Objective} Specifically, we implement a masked loss function for the retrieved documents to prevent the model from learning irrelevant or noisy text that could negatively impact its performance. In this way, we hope the model to enhance the ability to decompose subqueries and retrieve them based on demand. For each instance, the loss function is formulated as follows:

\begin{small}
    
\begin{align}
&\mathcal{L}= -\sum_{1\leq i\leq n}\text{log }[\text{Pr}(q_i| s_{i-1}) + \text{Pr}(a_i| s_{i-1}, q_i, d_i)] \notag
\end{align}
\end{small}
$d_i$ refers to \textit{null} if there is no reieval for $i$th reasoning step, $n$ refers to the total iteration.


\subsection{Chain of Calibration}
Building on the Markov process in Section~\ref{overview}, we identify four key optimization aspects for DeepRAG: termination and atomic decisions, query decomposition, and intermediate answer generation. Unlike the others, \textit{atomic decisions} require the model to recognize its own knowledge boundaries to make precise judgments.  

% Inspired by \citet{zhang2024chain}, we propose an approach that dynamically optimizes retrieval decisions for each subquery rather than training LLMs on complete reasoning paths. Our method consists of two main components:
% % 
% Our approach consists of two key components:
% % 
% 1) Synthesizing preference data: We generate preference data for each subquery, determining the necessity of retrieval.
% % 
% 2) Chain of Calibration training: We fine-tune the LLM with the synthesized preference data to enhance its ability to make informed retrieval decisions based on its internal knowledge boundaries.

We propose a method that dynamically optimizes atomic decisions for each subquery, rather than training LLMs on complete reasoning paths. Our approach consists of two key components: (1) synthesizing preference data to determine when retrieval is necessary, and (2) fine-tuning the LLM with this data using Chain of Calibration training to enhance its ability to make informed atomic decisions based on its internal knowledge boundaries.

% 
% This method maximizes the information gained from each question, including insights from suboptimal paths, enhancing the LLM's performance in knowledge-intensive tasks.

% \paragraph{Synthesizing Data}
% TODO
% 和imitation learning中的数据合成方法不同，我们在这里对所有subquery探索是否需要检索，从而得到一个完全二叉树。
% Then， 根据the tree，我们基于每个step构建preference data。
% 对于第 i 个subquery，我们教模型应该选择use internal or external knowledge来获得intermediate answer.
% 我们遵循一个简单的原则：如果 我们用intermediate完成这个节点，且后续存在一条路径导致正确路径，那么对于第i个subquery我们为其选择使用内部知识，反之我们prefer 检索。
% 特别的，我们使用不同的生成片段来区分这两个决策，一个是直接生成“Intermediate Answer", 一个是"Let's search the question on the Wikipedia."
% Based on it, 我们可以对树上的每一个节点构造偏序对。

% We introduce a calibration process to enhance the model's ability to discern when to utilize parametric knowledge versus retrieving external information. 

% Firstly, we obtain an optimal path with least retrival with method similar to Algorithm~\ref{algorithm:imi}. Based on it , we obtain a trajectory with optimum dicision for each subquery 用检索还是不检索来回答。
% 基于此，我们为每个subquery是否使用检索构造偏序对。
% 
% For example， 在Figure1中，我们获得了一条路径where 对第一个subquery应当利用内部知识回答问题，而对于第二个subquery 应当利用检索文档回答问题。因此对于第一个subquery我们可以构造偏序对 偏好内部知识而不是检索，对于第二个subquery我们可以构造偏序对偏好检索而不是内部知识。

\paragraph{Synthesizing Preference Data}
First, we identify an optimal path with minimal retrieval based on Algorithm~\ref{algorithm:imi} using the model trained in Stage I. This provides the optimal atomic decision for each subquery, determining whether retrieval is necessary.  
% 
From this path, we construct preference pairs for each subquery to indicate the preferred retrieval choice. For example, in Figure~\ref{fig:main}, the optimal path may suggest answering the first subquery using parametric knowledge while requiring document retrieval for the second. Accordingly, we generate preference pairs: one favoring parametric knowledge over retrieval for the first subquery and another favoring retrieval over parametric knowledge for the second.  
% 
This process enables the LLM to learn when to retrieve external information, thereby improving its ability to maximize the use of parametric knowledge while minimizing unnecessary retrieval.


% We start by exploring the necessity of retrieval for all subqueries.
% 
% Specifically, we modify Algorithm~\ref{algorithm:imi} to explore all potential reasoning paths exhaustively, even after identifying a correct trajectory. 
% This exploration continues until all paths satisfy their respective stopping criteria, thereby constructing a complete binary tree of reasoning trajectories.
% Then, we derive preference data for each decision step, enabling us to calibrate the model's decision-making regarding the use of internal versus external knowledge.





% As illustrated in Figure~\ref{fig:main}, for the $i$-th subquery, the model is trained to decide between relying on internal knowledge or retrieving external information to derive the intermediate answer. 
% We follow a simple principle: \textit{if completing the node with an intermediate answer leads to a reasoning path that culminates in the correct final answer, the model is calibrated to prefer internal knowledge for the $i$-th subquery.}
% 



\paragraph{Chain of Calibration Objective}

We fine-tune the LLM using a Chain of Calibration objective on our synthesized preference data. 

Given the $i$-th subquery and a state $s_i = [x,q_1, r_1, \cdots, q_{i-1}, r_{i-1}]$, we have two distince intermediate answer $r_i^1 = a_i^1$ and $r_i^2 = (d_i, a_i^2)$. Based on the process above, we have known which $r_i$ is preferred.  As a result, the training objective can be formulated as follows:


\begin{small}
\begin{equation}
\mathcal{L} = 
-  \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_w \mid s_i, q_i)}{\pi_{\text{ref}}(y_w \mid s_i, q_i)} - \beta \log \frac{\pi_{\theta}(y_l \mid s_i, q_i)}{\pi_{\text{ref}}(y_l \mid s_i, q_i)} \right)  \notag
\end{equation}
\end{small}
where $\sigma$ is the logistic function, the hyperparameter $\beta$ regulates the penalty imposed for the deviations
from the base reference model $\pi_{ref}$.
The terms \( y_w \) and \( y_l \) refer to the generated snippets for direct answers and retrieved answers, respectively. Specifically, the snippet ``Intermediate Answer:'' corresponds to a direct answer, while the snippet ``Let's search the question on Wikipedia'' corresponds to retrieval-based answers.

% $y_w$ and $y_l$ refers to the to generation snippets of direct answer or retrieved answer. Specifically, snippet ``Intermediate Answer:'' for direct answer and ``Let's search the question on Wikipedia'' for retrieval.


