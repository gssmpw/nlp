
\section{Experiment}
\subsection{Datasets}
We use five open-domain QA datasets for our experiments. We split the datasets used for training our models as the in-distribution dataset, while those not used for training are considered the out-of-distribution dataset.
% 
The in-distribution datasets include HotpotQA~\cite{yang2018hotpotqa}, and 2WikMultihopQA (2WMQA)~\cite{ho-etal-2020-constructing}, and the out-of-distribution datasets consist of CAG~\cite{pan2024not}, PopQA~\cite{mallen2022not}, and WebQuestions~\cite{berant2013semantic}. 
Specifically, we employ the time-sensitive subset of CAG to evaluate temporal reasoning capabilities.
Furthermore, WebQuestions is built upon Freebase to assess model robustness when information may be absent from the knowledge base.


\subsection{Baselines}
% 
We use the following baselines to evaluate the performance: 
\textbf{CoT}~\cite{wei2022chain} and \textbf{CoT*}, which employ 8-shot examples extracted from the training dataset. The asterisk (*) indicates that the model output was trained using the same data employed for training the DeepRAG.
\textbf{CoT-Retrieve} and \textbf{CoT-Retrieve*} augment the eight examples in the context with retrieved relevant documents based on the query.
\textbf{IterDRAG}~\cite{yue2024inference} refers to decomposing question and answer step by step based on in-context learning.
\textbf{UAR}~\cite{cheng2024unified} employs a trained classifier to determine when retrieval is necessary.
\textbf{FLARE}~\cite{jiang2023flare} and \textbf{DRAGIN}~\cite{su-etal-2024-dragin} are confidence-based method that decide the timing of retrieval based on token importance and uncertainty.
\textbf{TAARE}~\cite{zhang2024retrievalqa} allows the LLM itself to determine when retrieval is needed.
\textbf{AutoRAG}~\cite{yu2024autorag} uses trained models to iteratively decompose questions and retrieve relevant documents for answering.
\input{table/main-exp}

\subsection{Implementation Details}

% Firstly, we consturct experiment on the training subset of 3 QA datasets: Hotpot QA, 2WikMultihopQA, ASQA. 
% For Stage I, 我们对这三个数据集随机抽取了4000,4000,3000条数据，并利用Section4.1所述方法构造数据进行imitation。
% For Stage II，我们对这三个数据集分别抽取1000条数据，并利用Section 4.2的方法进行decision evolve。
% Then, we evaluate on the three datasets on 对应的测试集上，特别的，对于asqa数据集. Moreover, we evaluate on a out-of-distribution dataset to show the effective of our method. 

We construct training datasets using the training subsets of 2 QA datasets: HotpotQA, and 2WMQA. 
For imitation learning, we randomly sampled 4,000 data from HotpotQA, and 2WMQA respectively. 
For chain of calibration, we individually sampled 1,000 data points from each of the two datasets.
We evaluate our method on the corresponding test sets of these datasets with Exact Match (EM) and F1 score as evaluation metrics. 

% LLM, Retriever
Following \citet{su-etal-2024-dragin}, we adopt BM25 as our retrieval model. 
For the external knowledge corpus, we utilize Wikipedia\footnote{\url{https://github.com/facebookresearch/DPR/tree/main}}, with each article segmented into 100-token passages. 
We selected Llama-3-8B-Instruct~\cite{dubey2024llama} and Qwen-2.5-7B~\cite{qwen2} as our base model.


\subsection{Overall Results}
We evaluate DeepRAG on two in-distribution datasets and three out-of-distribution datasets. The results in Table~\ref{tab:main-exp} demonstrate DeepRAG's superior performance and robustness across different scenarios.

% - DeepRAG效果相比其它baseline较好，尤其是
%     - 置信度方法自定义的指标非常不靠谱
%     - Auto-RAG总是不停的循环
% - 在时间敏感数据（CAG），检索文档不全面（Web Question），OOD（PopQA）三类挑战下表现较好
% - Single-retrieve 方法相比adaptive方法依然较为鲁棒，另一方面现有数据集query比较简单
% - TAARE在多数时候不如直接检索，体现了其内部外部知识的mismatch
% - 不恰当的微调会导致模型效果损害，如vanilla 中的sft。我们仅在保留模型能力的基础上微调，因此不会引入额外的幻觉

\textbf{Our method demonstrates superior performance across most datasets via thinking to retrieval step by step.} 
Our method consistently outperforms existing approaches by enabling step-by-step retrieval reasoning.
Compared to reasoning-based and adaptive RAG baselines, DeepRAG achieves improvements across all datasets, demonstrating the effectiveness of the structured \textit{retrieval narrative} and its reliable, on-demand  \textit{atomic decisions}.
% 
Specifically, the poor performance of IterDRAG highlights the necessity of learning both query decomposition and faithful answering.
In contrast, confidence-based methods like FLARE struggle to determine the optimal retrieval timing due to their reliance on unstable, predefined metrics. 
Moreover, we observe that such confidence-based methods suffer from instability, as their performance is highly sensitive to threshold selection. 
Meanwhile, iterative retrieval methods like Auto-RAG often fall into continuous retrieval loops when no highly relevant information is found.
% 
It is worth noting that the CoT-Retrieve method outperforms on CAG. We attribute this to the fact that CAG consists of straightforward, one-hop questions, where direct question-relevant retrieval proves more effective.

\textbf{Our DeepRAG approach exhibits remarkable generalization capabilities and robustness in time-sensitive and out-of-distribution settings.}
In the time-sensitive dataset CAG, DeepRAG performs well compared to other adaptive retrieval methods. 
Furthermore, DeepRAG achieves substantial F1 score improvements of 2.63 and 4.57 on PopQA and WebQuestions respectively, even in scenarios where relevant information may be sparse or missing from the knowledge base.

\textbf{By learning from self-synthesized data, DeepRAG effectively explores knowledge boundaries while minimizing hallucination risks.} 
We observe that TAARE often underperforms direct retrieval methods, highlighting the mismatch between its internal knowledge and verbose.
Moreover, aggressive fine-tuning approaches like CoT* and CoT-Retrieve* can actually degrade model performance by forcing the model to learn knowledge beyond its natural capabilities.
In contrast, our approach carefully preserves model capabilities during fine-tuning by leveraging self-synthesized data, effectively preventing additional hallucination while maintaining performance.


% \begin{enumerate}[1)]
%     \item \textbf{Our method demonstrates superior performance across most datasets via adaptive inference-time computation.} 
%     Compared with the adaptive RAG baseline, our DeepRAG method gains improvements on all these datasets. This indicates that our adaptive inference-time computing method is more effective due to its reliable, on-demand decisions regarding when and what to retrieve. 
%     In contrast, confidence-based methods like FLARE may incorrectly estimate the optimal time for retrieval due to reliance on an unstable, predefined metric. 
%     Additionally, we found the confidence-based method suffers from instability due to its sensitivity to pre-defined thresholds, and the iterative retrieval method Auto-RAG always enters a continuous retrieval loop when no expected relevant information is identified.
%     % 
%     It is worth noting that the CoT-Retrieve method outperformed on CAG. We attribute this to the fact that CAG consists of straightforward, one-hop questions, which makes the question-relevant method more effective.

%     \item \textbf{Our DeepRAG approach exhibits remarkable generalization capabilities and robustness in time-sensitive and out-of-distribution settings.}
%     In the time-sensitive dataset CAG, DeepRAG performs well compared to other adaptive retrieval methods. 
%     Furthermore, DeepRAG achieves substantial F1 score improvements of 2.63 and 4.57 on PopQA and WebQuestions respectively, even in scenarios where relevant information may be sparse or missing from the knowledge base.
    
%     \item \textbf{By learning from self-synthesized data, DeepRAG effectively explores knowledge boundaries while minimizing hallucination risks.} 
%     We observe that TAARE often underperforms direct retrieval methods, highlighting the mismatch between its internal knowledge and verbose.
%     Moreover, aggressive fine-tuning approaches like CoT* and CoT-Retrieve* can actually degrade model performance by forcing the model to learn knowledge beyond its natural capabilities.
%     In contrast, our approach carefully preserves model capabilities during fine-tuning by leveraging self-synthesized data, effectively preventing additional hallucination while maintaining performance.

%     % \item \textbf{SRAG Effectively Handles Tasks Requiring complex reasoning and comprehensive knowledge.}
%     % As demonstrated in Table~\ref{tab:main-exp}, SRAG outperforms other methods on Hotpot QA and 2WikiMultihopQA, which require multi-hop reasoning capabilities. Moreover, it also excels on ASQA, a dataset that demands extensive knowledge to answer questions, further showcasing SRAG’s ability to manage tasks that involve both reasoning and comprehensive knowledge.
%     % Single-retrieve methods show competitive robustness compared to adaptive approaches, partly due to the relatively simple nature of queries in existing datasets. 
    

% \end{enumerate}

