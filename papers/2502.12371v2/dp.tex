\documentclass[conference]{IEEEtran}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
% \usepackage[bookmarks=true]{hyperref}
\usepackage[pagebackref=true,breaklinks=true,bookmarks=true,colorlinks]{hyperref}
\input{format/packages}
\input{format/macros}

\pdfinfo{
   /Author (Homer Simpson)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

\begin{document}

% paper title
\title{Diffusion Policy}
\title{Diffusion Policy: \\ \LARGE{ Visuomotor Policy Learning via Action Diffusion}}
%\title{Diffusion Policy: \\ \LARGE{Learning Visuomotor Policies with Conditional Diffusion Process}}
% You will get a Paper-ID when submitting a pdf file to the conference system
% \author{Author Names Omitted for Anonymous Review. Paper-ID [6]}
\author{
Cheng Chi$^1$, Siyuan Feng$^2$, Yilun Du$^3$, Zhenjia Xu$^1$, Eric Cousineau$^2$, Benjamin Burchfiel$^2$, Shuran Song$^1$ \\ 
$^1$ Columbia University \quad\quad 
$^2$ Toyota Research Institute \quad\quad
$^3$ MIT
\\ \href{https://diffusion-policy.cs.columbia.edu}{https://diffusion-policy.cs.columbia.edu}
}

%\maketitle

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figure/DP_teaser.pdf}
%     \caption{\textbf{Different Form of Policy Representations.} a) Explicit policy with different types of action representations.  b) Implicit policy learns an energy function conditioned on both action and observation and optimizes for actions that minimize the energy landscape c) Diffusion policy learns a gradient field to refine actions. \todo{update the figure with the same example. }  }
%     \label{fig:policy_rep}
%     %https://docs.google.com/drawings/d/1SNd5_khk3RsYuE9JCwUVjmRED-eF3UrO78XnzbOwE4Y/edit?usp=sharing
% \end{figure*}

\twocolumn[{%
	\renewcommand\twocolumn[1][]{#1}%
	\maketitle
        \vspace{-5mm}
	\begin{center}
		\includegraphics[width=0.95\textwidth]{figure/DP_teaser.pdf}
		\captionof{figure}{\textbf{Policy Representations.} \label{fig:policy_rep} a) Explicit policy with different types of action representations.  b) Implicit policy learns an energy function conditioned on both action and observation and optimizes for actions that minimize the energy landscape c) Diffusion policy refines noise into actions via a learned gradient field. This formulation provides stable training, allows the learned policy to accurately model multimodal action distributions, and accommodates high-dimensional action sequences. 
        % \todo{update figure to make b and c consistent, both 2D or both 3D. Make it clear c is gradient of b change J (a) -> E (a)}
        } 
	%https://docs.google.com/drawings/d/1SNd5_khk3RsYuE9JCwUVjmRED-eF3UrO78XnzbOwE4Y/edit?usp=sharing
	\end{center}
}]


\begin{abstract}
%This consistent performance boost provides strong evidence of the Diffusion Policy's effectiveness, which is attributed to its inherited properties of the diffusion formulation. 
%By learning the gradient field of an implicit action score and performing Stochastic Langevin Dynamics sampling on this gradient field, Diffusion policy is able to accurately model the multimodal action distribution, scalable to high-dimensional action space, and achieve stable training.  
This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. 
Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps.
We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability.
To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. 
We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details will be publicly available.

% that are often designed specifically for those benchmarks.  
%we introduce a new form of policy that represents a robot's visuomotor policy as a ``conditional diffusion denoising process on action'', Diffusion Policy. In this formulation, the policy learns to infer the gradient field of the action score for K denoising iterations conditioned on its visual observation. The output action sequence can be then inferred by performing Stochastic Langevin Dynamics sampling on this learned gradient field. 
% This formulation allows the learned policy to accurately model the multimodal action distribution, scalable to high-dimensional action space, and achieve stable training.
% We systematically evaluate the algorithm over 11 tasks from 4 different benchmarks that range from both simulated and real-world environments, single- and multi-task benchmarks, and fully- and under-actuated systems.
% The consistent performance boost against state-of-the-art methods across all benchmarks, tasks, and scenarios provides strong evidence of the effectiveness of the Diffusion Policy. We also provide detailed analysis to carefully examine the characteristics of the proposed algorithm and the impacts of the key design decisions. 
% The code, data, and training details will be publicly available.
\end{abstract}

\IEEEpeerreviewmaketitle



\input{text/introduction.tex}
\input{text/method.tex}
\input{text/evaluation_v2.tex}
\input{text/relatedwork.tex}
\input{text/conclusion.tex}


\bibliographystyle{plainnat}
\bibliography{references}

\appendix
\input{text/supp.tex}

\end{document}


