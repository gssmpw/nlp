

\section{Related Work}

We organise our discussion of prior work based on three key behaviour cloning desiderata for real-world robotics: capturing multi-modal behaviour, achieving sample efficiency, and enabling fast inference. While existing works address each of these aspects individually, relatively few approaches attempt to satisfy all three simultaneously.

\subsection{Multi-Modality in Behaviour Cloning}

Behaviour cloning has been widely explored as a means of enabling robots to learn from human demonstrations, achieving success across various manipulation tasks \cite{zhang2018deep, florence2019self, mandlekar2020learning, mandlekar2020iris, zeng2021transporter, rahmatizadeh2018vision, avigal2022speedfolding, atkeson1997robot, argall2009survey, ravichandar2020recent, chen2024diffusion, chiDiffusionPolicyVisuomotor2023b, Chen-RSS-24, Chi-RSS-24}. Capturing the multi-modal nature of human demonstrations is a fundamental challenge in behaviour cloning, and generative models have been extensively applied to address this issue. Methods such as Conditional Variational Autoencoders (CVAEs) \cite{lynch2020learning, zhaoLearningFineGrainedBimanual2023}, Energy-Based Models (EBMs) \cite{ibc}, and Vector Quantization \cite{leebehavior} have been used to model multi-modal action distributions, though each comes with trade-offs such as mode collapse or complex multi-stage training.

Recently, de-noising diffusion models have emerged as a dominant approach due to their stable training dynamics and strong performance in multi-modal imitation learning \cite{chen2024diffusion, chiDiffusionPolicyVisuomotor2023b, Chen-RSS-24, Chi-RSS-24, liu2024rdt}. However, these models require an iterative sampling process during inference, making them computationally expensive. 

An alternative class of generative models, IMLE (Implicit Maximum Likelihood Estimation), has remained largely unexplored in robotics despite its effectiveness in learning expressive distributions. In this paper, we introduce an IMLE-based approach for behaviour cloning, demonstrating its ability to efficiently model complex multi-modal action distributions in a single inference step, offering a simpler and more efficient alternative to existing diffusion-based methods.

\subsection{Sample Efficiency of Behaviour Cloning}
Despite the widespread success of diffusion models in robotics, these methods typically require large datasets for effective training \cite{zhao2024aloha}, which is a significant limitation in imitation learning, where expert demonstrations are expensive to collect. Improving the sample efficiency of behaviour cloning remains an active research area. 

Current research directions for improving sample efficiency involve optimising input representations. For example, 3D Diffusion Policy utilises point clouds to improve generalisation in 3D space \cite{ze20243d}, while approaches leveraging SO(2) and SIM(3) equivariances improve learning efficiency in tasks with structured transformations \cite{wangequivariant, yang2024equibot}. Affordance-centric representations have also been explored to improve generalisation from limited demonstrations \cite{rana2024affordance}. 

Our approach is orthogonal to these representation-based methods, as it focuses on improving the core learning algorithm itself to enhance sample efficiency. This allows IMLE Policy to be complementary to existing strategies, meaning it can be combined with improved input representations for even greater efficiency gains. Additionally, we conduct a comprehensive study on sample efficiency, explicitly examining the relationship between dataset size and performance. While dataset size is often an overlooked factor in behaviour cloning research, we highlight its direct impact on policy performance, providing insights into how different behaviour cloning methods scale with available data.

\subsection{Inference Speed and Behaviour Cloning}
A key limitation of most state-of-the-art behaviour cloning approaches is their multi-step inference process, which requires iterative de-noising or auto-regressive steps to generate actions. This significantly increases computational cost during inference, limiting real-time applicability in robotics. Several approaches aim to improve upon this. Consistency Models distill multi-step diffusion policies into single-step policies while maintaining performance \cite{prasad2024consistency, song2023consistency}. Streaming Diffusion modifies the denoising process to allow earlier actions to require fewer de-noising steps, speeding up inference \cite{h√∏eg2024streamingdiffusionpolicyfast}. Flow Matching provides an alternative continuous-time generative modelling framework with more efficient probability paths, reducing inference steps \cite{lipmanflow, liuflow}, though in practice, it still requires multiple steps to prevent mode collapse when applied to behaviour cloning \cite{hu2024adaflow, black2024pi_0, zhang2024affordance}.

While these methods aim to speed up inference, they either require additional distillation steps or do not fully eliminate iterative sampling. In contrast, our approach natively enables single-step inference while still capturing complex, multi-modal action distributions. This allows IMLE Policy to perform fast, real-time inference without requiring multi-stage training or distillation, making it a promising alternative for computationally constrained settings.

