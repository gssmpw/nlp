
\section{Evaluation}

\begin{table}[!b]
\centering
\setlength{\tabcolsep}{4pt} % Adjust column spacing
\renewcommand{\arraystretch}{1.2} % Adjust row spacing
\begin{tabular}{@{}lccccccc@{}}
\toprule
Task & \#Rob & \#Obj & ActD & \#Demo  & Steps & Img? & HiPrec \\ 
\midrule
\multicolumn{8}{l}{\cellcolor[HTML]{C0C0C0}Simulation Benchmark} \\
Lift       & 1 & 1 & 7  & 200  & 400 & Yes & No  \\
Can        & 1 & 1 & 7  & 200 & 400 & Yes & No  \\
Square     & 1 & 1 & 7  & 200  & 400 & Yes & Yes \\
Transport  & 2 & 3 & 14 & 200  & 700 & Yes & No  \\
ToolHang   & 1 & 2 & 7  & 200   & 700 & Yes & Yes \\
Push-T     & 1 & 1 & 2  & 200    & 300 & Yes & Yes \\
UR3 Block Push  & 1 & 2 & 2  & 1000    & 350 & No  & No  \\
Kitchen    & 1 & 7 & 9  & 18   & 280 & No  & No  \\ 
\midrule
\multicolumn{8}{l}{\cellcolor[HTML]{C0C0C0}Realworld Benchmark} \\
Push-T             & 1 & 1 & 2 & 35    & 600 & Yes & Yes \\
Shoe Rack          & 1 & 3 & 7 & 35   & 800 & Yes & Yes  \\ 
\bottomrule
\end{tabular}
\caption{\textbf{Tasks Summary.} \#Rob: number of robots, \#Obj: number of objects, ActD: action dimension, \#Demo: number of demonstrations, Steps: max number of rollout steps, HiPrec: whether the task has a high precision requirement.}
\label{tab:sim_env_info}
\end{table}

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{images/tasks_imle.pdf}
    \caption{\textbf{Simulation Environments.} Visualisation of the environments used to evaluate IMLE Policy. The first 5 tasks are part of the Robomimic benchmark \cite{robomimic} and the other environments include Push-T \cite{chi2023diffusion}, UR3 BlockPush \cite{kim2022automating} and Franka Kitchen \cite{gupta2019relay}.}
    \label{fig:tasks}
\end{figure*}

We evaluate IMLE Policy to understand how effective it is in modelling behaviours across different datasets and environments when compared to prior state-of-the-art methods. We focus our evaluation on the core requirements for real-world robotics applications particularly: sample efficiency, multi-modal expressivity and inference speed. Concretely, we seek to answer the following questions through our experiments:

\begin{enumerate}
    \item How well does IMLE Policy perform on the respective benchmarks for behaviour cloning as a single-step generative model?
    \item How well does IMLE Policy perform when under data constraints?
    \item How well does IMLE capture multiple modes seen in demonstrations?
    \item Does IMLE Policy scale beyond simulation environments?
    \item What hyper-parameters of IMLE Policy make the most impact on its performance?
\end{enumerate}

We systematically evaluate IMLE Policy to answer the above questions through a series of experiments across both simulated and real-world environments. For all experiments, we evaluate the following policies: 

\begin{itemize}
\item \noindent\textit{IMLE Policy: } Our proposed algorithm given in Algorithm \ref{alg:inference_temporal_consistency} which leverages temporal consistency during inference.
\item\textit{IMLE Policy (w/out consistency): } Our proposed algorithm without temporal consistency during inference.
\item\textit{Diffusion Policy: } Vanilla Diffusion Policy proposed by \citet{chi2023diffusion} trained using DDPM and 100 denoising steps
\item\textit{Flow Matching (1-step): } We modify the Diffusion Policy implementation from \citet{chi2023diffusion} to utilise the Flow Matching objective and evaluate against the 1-step setting as a comparative baseline to the 1-step performance of our method.
\end{itemize}









\begin{table*}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}l
>{\columncolor[HTML]{C0C0C0}}c 
>{\columncolor[HTML]{C0C0C0}}c 
>{\columncolor[HTML]{C0C0C0}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c @{}}
\toprule
 &
  \multicolumn{3}{c}{\cellcolor[HTML]{C0C0C0}High Multimodality} &
  \multicolumn{5}{c}{\cellcolor[HTML]{FFFFFF}Low Multimodality} \\ \midrule
\multicolumn{1}{c}{} &
  Push-T &
  UR3 Block Push&
  Kitchen &
  Lift &
  Can &
  Square &
  Tool Hang &
  Transport \\ \midrule
\multicolumn{9}{c}{\cellcolor[HTML]{EFEFEF}\textbf{100\% Dataset}} \\
\cellcolor[HTML]{FFFFFF}\textbf{IMLE Policy} &
  \textbf{0.59/0.54} &
  0.82/0.73 &
  \textbf{0.11/0.09} &
  \textbf{1.00/1.00} &
  0.98/0.97 &
  0.82/0.80 &
  0.81/0.69 &
  0.90/0.80 \\
\cellcolor[HTML]{FFFFFF}\textbf{IMLE Policy (w/out consistency)} &
  0.56/0.53 &
  \textbf{0.90/0.77} &
  0.07/0.04 &
  \textbf{1.00/1.00} &
  0.96/0.95 &
  \textbf{0.86/0.82} &
  0.74/0.52 &
  0.92/0.80 \\
\cellcolor[HTML]{FFFFFF}Diffusion Policy &
  0.57/0.52 &
  0.74/0.73 &
  0.10/0.06 &
  \textbf{1.00/1.00} &
  \textbf{0.99/0.98} &
  0.84/0.84 &
  \textbf{0.84/0.81} &
  0.92/0.90 \\
\cellcolor[HTML]{FFFFFF}Flow Matching Policy &
  0.36/0.34 &
  0.87/0.81 &
  0.09/0.05 &
  \textbf{1.00/1.00} &
  0.96/0.96 &
  0.82/0.79 &
  0.78/0.68 &
  \textbf{0.93/0.87} \\ 
\multicolumn{9}{c}{\cellcolor[HTML]{EFEFEF}\textbf{20 Demos}} \\
\cellcolor[HTML]{FFFFFF}\textbf{IMLE Policy} &
  \textbf{0.10/0.07} &
  \textbf{0.34/0.32} &
  - &
  \textbf{1.00/0.99} &
  \textbf{0.50/0.41} &
  \textbf{0.18/0.14} &
  \textbf{0.03/0.00} &
  \textbf{0.28/0.20} \\
\cellcolor[HTML]{FFFFFF}\textbf{IMLE Policy (w/out consistency)} &
  0.05/0.03 &
  0.34/0.31 &
  - &
  1.00/0.98 &
  0.44/0.40 &
  0.11/0.09 &
  0.02/0.00 &
  0.27/0.22 \\
\cellcolor[HTML]{FFFFFF}Diffusion Policy &
  0.03/0.03 &
  0.26/0.21 &
  - &
  \textbf{1.00/0.99} &
  0.47/0.47 &
  0.16/0.16 &
  0.00/0.00 &
  0.27/0.24 \\
\cellcolor[HTML]{FFFFFF}Flow Matching Policy &
  0.012/0.00 &
  0.21/0.14 &
  - &
  \textbf{1.00/0.99} &
  0.48/0.39 &
  0.16/0.08 &
  0.02/0.00 &
  0.26/0.15 \\ \bottomrule
\end{tabular}%
}
\caption{\textbf{Behaviour Cloning Benchmark} We present success rates in the format of (max success rate) / (average of last 3 checkpoints), with each averaged across 3 training seeds and 50 different environment initial conditions.}
\label{tab:sim_results}
\end{table*}






\subsection{Simulation Environments}

IMLE Policy is evaluated across 8 different simulation tasks (Figure \ref{fig:tasks}) from 4 benchmark environments \cite{chi2023diffusion,robomimic,kim2022automating,gupta2019relay}. We categorise our sets of tasks based on the level of multi-modality exhibited by the dataset with the \textit{high multi-modality} tasks exhibiting the highest variance across the demonstrations. A high-level description of each task is provided below and specific details are summarised in Table \ref{tab:sim_env_info}.

\subsubsection{Push-T } We utilise the variant of this task provided by \citet{chi2023diffusion} which involves pushing a T-shaped block (gray) to a designated target position (red) using a circular end-effector (blue). Variation is introduced through randomized initial positions of the T block and the end-effector. Successfully completing the task requires leveraging complex, contact-rich object dynamics to precisely push the T block using point contacts to sufficiently overlap with the target.

\subsubsection{Robomimic }A large-scale robotic manipulation benchmark \cite{robomimic}  designed for studying imitation learning and offline reinforcement learning (RL). It includes five tasks and we utilise the Proficient Human (PH) teleoperated demonstration dataset for all evaluations. Successful completion of these tasks occurs when each of the objects is placed in the target locations.

\subsubsection{UR3 Block Push } In this task, a UR3 robot pushes two blocks to goal circles on the opposite side of the table \cite{kim2022automating}. The dataset exhibits multimodal behaviour, as either block can be moved first. Success is evaluated based on whether each block reaches the goal. 

\subsubsection{Franka Kitchen } This environment, based on the Franka Kitchen manipulation task \cite{gupta2019relay}, uses a Franka Panda robotic arm with a 7-dimensional action space and 18 successful human-collected demonstrations. It features seven possible tasks, with each trajectory completing four tasks in varying sequences making it highly multimodal. Successful completion in this environment is considered when a series of 4 unique tasks are completed sequentially. 





\subsection{Real World Tasks} \label{sec:realworldtasks}

We additionally evaluate IMLE Policy on two real robot manipulation tasks to demonstrate its capability of learning manipulation tasks in the real world. To investigate real-world sample-efficiency, we only collect 35 demonstrations and train on 35 and 17  demonstration subsets.

\subsubsection{Real World Push-T}
This task is a real world variant of the simulated Push-T task, with a 3D printed T block and a cylindrical robot end-effector being controlled on a 2D plane. We use 2 camera views, one on the wrist of the robot, and the other a side view scene camera. We record a successful trial when the robot has successfully placed the T-Block to sufficiently overlap with the target and then return to an end state within a 2 minute time frame, as shown in Figure \ref{fig:real_pusht}.

\subsubsection{Real World Shoe Racking Task}
We evaluate our method on an additional multi-modal real-world shoe racking task, where the robot must pick and place two shoes and place them side-by-side on the shoe rack. Either shoe can be selected first, and the shoes can be placed anywhere on the rack, with demonstrations including a variety of these behaviours, making this task highly multi-modal. Wrist and scene cameras are used to provide full view of the task scene. A successful trial is recorded when both shoes are placed correctly side-by-side (i.e. left shoe on the left of the right shoe) on the rack and the robot has returned to its home position within a 1 minute time frame, as shown in Figure \ref{fig:real_shoe}


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/sample_efficiency_plot.pdf}
    \caption{\textbf{Dataset Size Study.}  We evaluate how each of the methods performs when trained with different size subsets of the full dataset used in the Push-T benchmark task. The dashed line indicates that to achieve the same reward of 0.5 on the task, IMLE Policy requires less than 29\% of the data, while Diffusion Policy requires approximately 43\% while Flow Matching Policy requires over 80\% of the full dataset.}
    \label{fig:simpusht-sweep}
    % \vspace{-0.7cm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/ablation_plot.pdf}
    \caption{\textbf{Robustness to Hyper-parameters.} We evaluate the performance of IMLE Policy across varying different values for two key hyper-parameters used by the IMLE objective: rejection sampling radius $\epsilon$ and the number of samples used per condition. Note how the algorithm remain relatively stable across the entire range of values for both tasks.}
    \label{fig:hyper_sweep}
    % \vspace{-0.7cm}
\end{figure}



\subsection{Experiments}
\subsubsection{Dataset Size Study}

We conduct a study to evaluate how well IMLE can learn under data constrains when compared to the baselines. We use the Push-T simulated task for this, dividing the full dataset provided by \citet{chi2023diffusion} in 10 subsets, starting with 10\% of the data up to 100\%. We train each method on these datasets for 1000 epochs across 3 seeds and report the average evaluation performance for each method in Figure \ref{fig:simpusht-sweep}. This experiment gives us a detailed view of how performance improves as data increases.

\subsubsection{Benchmark Evaluation}

We evaluate all methods across the full datasets provided by each respective benchmarks indicated as \textit{100\% Dataset} in Table \ref{tab:sim_results}. To understand how our policy operates in the low data regime, we additionally conduct an evaluation when all methods are trained on 20 randomly sampled demonstrations from the full dataset. These 20 demonstrations are kept consistent across all methods. While 20 demonstrations might not be enough data to learn to successfully complete each task, it allows us to gauge the relative sample efficiency of all methods. For each setting, we report the best success rate achieved by the method across 1000 training epochs. The success rates are reported every 50 epochs and are calculated as the average success across 50 different initialisations within that environment. We additionally report the average of the last 3 success rates from the run. All results are averaged across 3 seeds and are summarised in Table \ref{tab:sim_results}.

\subsubsection{Real World Validation} We evaluate all methods on the real-world Push-T and shoe racking tasks described in \ref{sec:realworldtasks}. Our evaluation protocol tests both sample efficiency and overall performance by training on 17 and 35 demonstrations for each task. For both tasks, we conduct 20 evaluation trials per method and report the success rates based on the completion criteria outlined in the environment descriptions. This evaluation framework allows us to assess how effectively each method can learn from limited real-world demonstrations while handling both precise manipulation requirements in Push-T and the multi-modal decision space in the shoe racking task. Results are summarised in Figure \ref{fig:realresults}

\subsubsection{Mode Capturing Ablation} In this qualitative ablation study we demonstrate how IMLE Policy can capture multiple modes even in states where certain modes appear less frequently in the dataset. We conduct this study in the Push-T simulation benchmark where we gradually sweep the location of the robot's end effector from one side of the T block's top edge to the other. The idea here is to capture points where the demonstration data exhibits a high level of multi-modality (at the centre) and less multi-modality (towards the corners) where majority of the demonstrations will bias towards one side of the T block when in one of these particular states. The key results are illustrated in Figure \ref{fig:pusht_multimodality}.


\subsubsection{Real World Inference Speed Evaluation} We evaluate the computational efficiency of our approach by measuring inference speed across different policy architectures for the shoe racking task. Using a standard Dell Precision 3680 i7 workstation equipped with an NVIDIA GeForce RTX 3090 GPU, we measure the average time required to generate a sequence of actions. For each method, we conduct 30 separate generations and report the average inference speed in Hertz. We summarise the results in Table \ref{tab:realworldspeed}. 

% Our results, presented in Table \ref{tab:realworldspeed}, demonstrate that the IMLE policy achieves inference speeds (111 Hz) comparable to the flow matching approach (110 Hz), while significantly outperforming the diffusion policy (1.8 Hz). Notably, removing the consistency term from our IMLE policy yields a marginal speed improvement (123 Hz), suggesting that our consistency mechanism introduces minimal computational overhead while providing its benefits to policy learning.

\subsubsection{Hyperparameter Ablation Study} Finally we evaluate how robust our algorithm is to the two key hyperparameters used by the IMLE Policy objective. The first parameter pertains to the rejection sampling radius, $\epsilon$ which determines how far candidate generations have to be away from a data point to be considered in the loss computation. The second parameter pertains to the number of samples we use per condition in order to conduct our nearest-neighbour search in the objective. For each hyper-parameter we sweep over a set of values for two different tasks from our simulation benchmark (Push-T and Square) and report the maximum success rate achieved by each when trained over 1000 epochs. The results are summarised in Figure \ref{fig:hyper_sweep}.



\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/real_world_plot.pdf}
    \caption{\textbf{Real World Task Results.} Across both tasks we train each method on both 17 (left) and 35 (right) demonstrations and report the average success rate achieved across 20 trials. We find that IMLE Policy consistently outperforms baselines in the low data regime, while maintaining competitive in the higher data regime.}
    \label{fig:realresults}
    % \vspace{-0.7cm}
\end{figure}



\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lcccc@{}}
\toprule
 &
  \textbf{\begin{tabular}[c]{@{}c@{}}IMLE \\ Policy\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}IMLE Policy \\ (w/out consistency)\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Diffusion \\ Policy\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Flow Matching \\ Policy (1-step)\end{tabular}} \\ \midrule
\textbf{Inference Speed (Hz)} &
  111 &
  123 &
  1.8 &
  110 \\ \bottomrule
\end{tabular}%
}
\caption{\textbf{Real World Inference Speed.} Using the two images from shoe racking task as input, we average the inference speed required to generate a sequence of actions across 30 generations.}
\label{tab:realworldspeed}  
\end{table}


\subsection{Key Findings}

\textbf{IMLE Policy can learn with significantly less data when compared to baselines.} The dataset size study shown in Figure \ref{fig:simpusht-sweep} demonstrates how IMLE Policy consistently achieves higher max rewards across all dataset percentages and a significant advantage in the lower data settings. Notably, IMLE Policy achieves a reward of 0.5 with less than 30\% of the data, while Diffusion Policy requires nearly twice as much data to reach the same performance. This underscores the ability of IMLE Policy to generalise effectively with minimal data. Furthermore, as the dataset size increases, IMLE Policy continues to maintain its advantage, achieving near-optimal performance faster, while converging to comparable performance to Diffusion Policy when the full dataset is available. 

\textbf{IMLE Policy can maximise the utility of only a few demonstrations, while scaling to larger datasets.} As shown in Table \ref{tab:sim_results}, across all benchmark environment tasks, IMLE Policy demonstrates a clear advantage, particularly in the challenging setting of learning from only 20 randomly selected demonstrations. Under this constrained scenario, IMLE Policy consistently outperforms all baseline methods,  achieving at least 8 more successful evaluation runs on average compared to the baselines. On the full dataset benchmark, IMLE Policy continues to demonstrate competitive or superior performance, outperforming the baselines in at least 5 of the 8 tasks. 

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{images/pusht_task_real.pdf}
    \caption{\textbf{Real World Push-T Task Setup.} Hardware setup and illustration of the task. The robot needs to \ding{172} precisely push the T-Shaped block into the target region, and \ding{173} move the end-effector to the end-zone.}
    \label{fig:real_pusht}
\end{figure}



\textbf{IMLE Policy is highly multi-modal.} Figure \ref{fig:pusht_multimodality} shows that IMLE Policy maintains multi-modality across varying initial conditions of the end effector. Unlike Diffusion Policy, which biases towards majority modes, and Flow Matching Policy (one-step), which collapses modes and produces averaged trajectories, IMLE Policy captures diverse trajectories that reflect the full distribution of the demonstration data. This is particularly important in low-data regimes, where capturing all modes without overfitting to dominant behaviours enables efficient learning. Figure \ref{fig:main} further supports this, showing that IMLE Policy preserves multi-modality and accurately captures all modes, even with sparse or imbalanced data.

\textbf{IMLE Policy is robust to hyperparameter variations.} Figure \ref{fig:hyper_sweep} demonstrates that IMLE Policy consistently performs well across a wide range of hyperparameter settings, with stable success rates even as the rejection sampling threshold ($\epsilon$) and the number of samples per condition are varied. This robustness ensures that IMLE Policy adapts effectively without significant degradation in performance, making it practical for real-world applications where hyperparameter searches or fine-tuning may be infeasible. Notably, for the simulation benchmark evaluations, hyperparameters were not optimized per task but held constant throughout, yet IMLE Policy still outperformed baseline methods. 

\textbf{IMLE Policy is well suited for real world robotics.}
IMLE Policy demonstrates impressive traits that make it suitable for real-world robotics by achieving the best performance across both real-world visuomotor tasks as shown in Figure \ref{fig:realresults}. Even with as few as 17 demonstrations, IMLE Policy learns performant policies, outperforming both baseline methods by a significant margin. The robustness of IMLE Policy in low-data regimes makes it particularly valuable in real-world scenarios where collecting large datasets is often impractical. Additionally, as shown in Table \ref{tab:realworldspeed} IMLE Policy exhibits fast single-step inference, achieving up to 97.3\% faster inference speeds compared to vanilla Diffusion Policy, while either outperforming it or maintaining competitive performance. This efficiency enables real-time control and is well suited for robotics applications where computational resources can be limited. While its inference speed is similar to that of single-step Flow Matching we note that IMLE Policy outperforms Flow Matching, particularly in the low data regime and consistently across the simulation benchmarks. We provide videos demonstrating how our policy perform across both tasks when compared to the baselines and additional videos to demonstrate its robustness in the real world in the attached supplementary material.

\textbf{High multi-modality calls for temporal consistency.} While IMLE Policy demonstrates impressive multi-modal expressivity even from a limited number of demonstrations, a key empirical insight we identified was the tendency for the policy to occasionally switch between modes during execution in highly multi-modal tasks. This behaviour arises because the model is designed to represent all modes present in the data, and without explicit guidance, the policy may select a different mode at each decision point. This can result in inconsistent trajectories, particularly in tasks requiring sequential and coherent actions. Temporal consistency played an important role in ensuring that once a mode was selected, the policy adhered to it across subsequent timesteps. This not only stabilises and smooths executed trajectories but also prevents the policy from getting stuck between conflicting modes. As demonstrated in both the real-world and simulation experiments, temporal consistency significantly improves success rates, particularly in tasks where precise and sustained adherence to a trajectory is critical. The smooth and consistent nature of these trajectories can be viewed in the supplementary real world video attached to this submission. 







\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{images/shoe_rack_task.pdf}
    \caption{\textbf{Real World Shoe Racking Task Setup.} Hardware setup and illustration of the task. The robot needs to \ding{172} grasp either shoe and \ding{173} place it anywhere continuously along the top of the rack, this is repeated for the second shoe with the caveat that the shoe has to be placed either to the left or right with respect to the other shoe in order to be successful; finally \ding{174} the robot must return to its home position.}
    \label{fig:real_shoe}
\end{figure}





\subsection{Implementation Details}
We base our implementation on the available Diffusion Policy training code \cite{chi2023diffusion}. As in Diffusion Policy, the model takes in an input noise vector, with the same dimensionality as the action sequence, however in our case we do not denoise this gradually into a clean trajectory, but rather use it as the latent sample space and directly output the trajectory. Therefore the model architecture is a 1D UNet, which is the same as Diffusion Policy, modified only to remove the timestep embedding. We use an action prediction horizon $T_p$ of $16$, and action execution horizon $T_a$ of $8$ and an observation horizon $T_o$ of 2. For all experiments we use fixed hyperparameters, with a value of $0.03$ for $\epsilon$, $20$ generated samples per datapoint. We train all simulation benchmark tasks for $1000$ epochs while all real world tasks were trained for a fixed time of 12 hours. For inference, when using consistency, we set the reset period $C$ to 10, which gives a good balance of consistency and responsiveness. Although we keep this value the same across tasks, note that it can be tuned for the time horizon and desired level of responsiveness of specific tasks. 

% Here, $T_o$ is referred to as the observation horizon, $T_p$ as the action prediction horizon, and $T_a$ as the action execution horizon.