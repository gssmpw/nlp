\section{Conclusion} 
\label{sec:conclusion}


In this work, we introduced IMLE Policy, a novel imitation learning algorithm based on a conditional variant of RS-IMLE, designed to efficiently capture multi-modal action distributions while enabling fast, single-step inference. Through extensive experiments, we demonstrated state-of-the-art sample efficiency in both simulated and real-world robotic manipulation tasks, showing that IMLE Policy can learn effective policies from limited demonstrations. We conducted a thorough evaluation across varying dataset sizes, addressing an underexplored area in behaviour cloning research. Additionally, we proposed a variant that encourages temporal consistency without modifying the training procedure, enhancing execution stability in multi-modal settings. IMLE Policy exhibits promising characteristics for future research, including reinforcement learning fine-tuning, diverse behaviour generation for model predictive control (MPC) and RL exploration, and deployment in resource-constrained settings. Its efficiency and simplicity make it especially relevant for open-source robotics and real-world applications.