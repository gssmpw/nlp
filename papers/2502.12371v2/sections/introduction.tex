
\section{Introduction}


Learning policies from demonstrations is becoming a key approach for enabling robots to perform complex tasks. At its core, this problem involves mapping observations to actions in a way that captures the nuanced and often multi-modal nature of human behaviour. However, leveraging behaviour cloning for real-world robotics applications presents unique challenges including the ability to learn these highly multi-modal action distributions from limited demonstration data which can be expensive to collect, and the need for computational efficiency to enable real-time operation. Recent advancements in generative modelling, such as diffusion models \cite{sohldickstein2015nonequilibrium, chi2023diffusion} and flow matching \cite{lipmanflow, black2024pi_0}, have demonstrated promise in capturing complex multi-modal action distributions for behaviour cloning. However, these approaches are computationally expensive during inference, as they require iterative sampling processes to generate actions. Additionally, they often demand large datasets to train performant models effectively. Addressing these challenges is crucial for enhancing the practicality and adoption of behaviour cloning in real-world tasks.

% Generative adversarial networks (GANs) offer an alternative to diffusion models with efficient single-step inference and better sample efficiency, making them appealing for scenarios with limited data. However, GANs suffer from training instability and mode collapse, where parts of the data distribution are ignored, limiting their ability to effectively model complex, multi-modal behaviours. To address these shortcomings, recent advances in GAN-based training have introduced Implicit Maximum Likelihood Estimation (IMLE) \cite{imle}, a simple objective that ensures every data point is well-represented by at least one generated sample. 

Generative Adversarial Networks (GANs) are an alternative approach to generative modelling with efficient single-step inference, but fell out of favour due to their issues with training stability and mode collapse, where parts of the data distribution are ignored, limiting their ability to effectively model complex, multi-modal distributions. To address these shortcomings, Implicit Maximum Likelihood Estimation (IMLE) \cite{imle} was introduced as an alternative training objective for single-step generative models, based on a simple objective that ensures every datapoint is well-represented by at least one generated sample. In essence, IMLE minimises the distance between each data point and its nearest generated sample, guaranteeing comprehensive mode coverage and maintaining stable training, even with smaller datasets. This makes IMLE a compelling alternative to diffusion models for developing efficient and faster behaviour cloning methods suitable for real-world robotics. 
% Further improvements have been made, such as Rejection Sampling IMLE \cite{rsimle}, which greatly improves sample-efficiency, but so far these training objectives have gone largely unexplored in robotics applications.

In this work, we propose IMLE Policy, a novel extension of Implicit Maximum Likelihood Estimation tailored for conditional behaviour cloning.
We leverage a particular instantiation of IMLE that utilises rejection sampling in the training objective \cite{rsimle}, which further improves sample efficiency while effectively enabling the model to sample all possible modes during inference. IMLE Policy offers several key advantages that make it particularly promising for real-world robotic systems:

\begin{itemize}
    \item \textbf{Expressive multi-modal action distributions:} 
    By optimising the policy to generate samples close to every possible data point, IMLE Policy by nature does not drop modes, enabling highly expressive multi-modal action generations for a given state (Figure \ref{fig:main} (Bottom)).

    \item \textbf{Sample efficiency:} IMLE Policy can accurately model complex multi-modal distributions using 38\% less data on average than Diffusion Policy and single-step Flow-Matching to achieve similar performance (Figure \ref{fig:main} (Top)).

    \item \textbf{Fast inference:} IMLE Policy can generate multi-modal actions using only a single forward pass -- a 97.3\% increase in inference speed when compared to vanilla Diffusion Policy without collapsing modes when compared to single step Flow Matching.
    
\end{itemize}



We evaluate IMLE Policy across diverse simulation benchmarks and multi-modal real-world tasks, demonstrating its effectiveness as an alternative to existing generative model-based behaviour cloning approaches. IMLE Policy meets the critical \textit{desiderata} for real-world robotics: \textbf{sample efficiency, computational speed, and expressiveness}. \textit{(1) IMLE Policy outperforms state-of-the-art baselines in low-data regimes, while matching performance when more data is available}, learning performant real-world policies from as few as \textbf{17 demonstrations}. \textit{(2) IMLE Policy captures multi-modal action distributions in a single step}, generating \textbf{diverse} trajectories across varying conditions, avoiding mode collapse seen in other single-step methods. \textit{(3) IMLE Policy enables fast real-time inference}, reducing latency by \textbf{97.3\%} compared to iterative diffusion models while maintaining competitive task success rates. We further analyse sample efficiency, multi-modal expressivity, and key design choices to understand IMLE Policyâ€™s strengths. All code, datasets, and training details will be released to ensure reproducibility and support further research. We provide supplementary videos to demonstrate the real world performance of our system when compared to the baselines.




\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/pusht_multimodality.pdf}
    \caption{\textbf{Qualitative Analysis of Mode Capturing 
    Performance.} Illustrating the trajectories generated by different methods (rows) for varying initial end-effector positions (columns) in the Push-T task. The goal is to push the T-shaped block from its initial position to align with the target (green). At the top of the T, the dataset exhibits multi-modal behaviour: in the centre, demonstrations are evenly split between pushing left and right, while towards the edges, the majority of demonstrations push towards the closer side, with only a few moving in the opposite direction. IMLE Policy successfully captures all modes without collapsing or biasing, maintaining trajectory diversity even in underrepresented modes. In contrast, Diffusion Policy tends to bias towards one mode when close to an edge, while Flow Matching collapses modes, producing averaged and unimodal trajectories.}
    \label{fig:pusht_multimodality}
\end{figure*}



% \todo{iterate on what our core contributions are here}

% \krishan{old intro below WIP}

% The recent impressive progress in imitation learning has been fueled by advances in generative modeling, with techniques such as diffusion enabling the stable training of policies with expressive multimodal action distributions. However, diffusion models typically require large datasets, as is apparent in applications such as image generation \cite{}, as well as generalist robotic manipulation \cite{zhao2024aloha}. Human demonstration data however is particularly expensive, requiring human effort and robot time, with recent works demonstrating that months worth of data and human effort is required to achieve robust performance. To this end, we aim to take a step towards sample-efficient policy learning by leveraging a recently proposed, sample-efficient generative model class, RS-IMLE, which has been shown to generate realistic images merely 100 training samples, orders of magnitude less than diffusion models are typically trained with, while avoiding issues that GAN style models typically face such as mode collapse.


% Furthermore, diffusion models require solving a SDE through multiple iterations to generate actions, making them poorly suited to real time applications such as robotics. While there have been many attempts to reduce the number of steps required for generation, such as consistency models \cite{} and flow matching \cite{}, current methods still typically require several steps \cite{}. Our approach, based on a generator architecture, generates actions with a single model forward pass. The training process is also significantly simpler than denoising approaches, while maintaining the ability to model expressive multi modal distributions.

% Generative modeling has seen wide ranging success with recent years seeing Diffusion Models overtaking GANs as the dominant approach for applications such as image generation, due to their ability to model complex and high dimensional data distributions without mode collapse. However, this approach comes at the cost of being technically complex to implement, while also relying on multiple iterations to generate a single output. Implicit Maximum Likelihood Estimation is a newly introduced method for training GAN style generative models without mode collapse that selects a generated nearest neighbor to training data samples from multiple options before computing the loss, ensuring each mode is captured, without requiring multiple generation iterations \cite{imle}. Rejection Sampling IMLE improves this by adding a threshold to each datapoint, preventing samples that are already close to training datapoints, greatly improving the sample-efficiency and maintaining the modes present in the dataset \cite{rsimle}. Inspired by this new sample-efficient approach for learning unconditional image generation, we extend this method to the conditional setting, and demonstrate its potential for learning robot policies, an application where sample-efficiency and real-time inference speed are particularly important. Across a range of experiments in simulation and the real world, we find impressive benefits in sample efficiency in complex multimodal tasks, with RS-IMLE policy requiring up to 50\% less data to achieve the same performance as prior state of the art methods.

% This paper makes several contributions:
% \begin{itemize}
%     \item We propose a conditional variant of IMLE and apply it as a policy in an imitation learning setting, which we call IMLE Policy. 
%     \item We demonstrate the sample efficiency improvements of our method via simulated and real world experiments with varying dataset size.
%     \item We demonstrate improved inference speed over multi-step denoising methods, while maintaining the ability to model expressive multi-modal distributions.    
%     \item We additionally propose a simple extension to our algorithm that encourages consistency across timesteps, which prevents stalling at decision points with multiple trajectory options.
    
% \end{itemize}
