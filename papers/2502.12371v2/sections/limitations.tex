
\section{Limitations}

While IMLE Policy demonstrates strong performance in multi-modal behaviour cloning, certain limitations remain. Its strong ability to capturing all modes makes it more sensitive to the quality of demonstrations, meaning inconsistencies or suboptimal behaviours in the dataset can negatively impact performance. Additionally, in highly multi-modal tasks, IMLE Policy may occasionally switch between modes during execution, necessitating consistency mechanisms such as temporal coherence; however, this can sometimes reinforce suboptimal behaviours if the selected future actions contain errors.
% , as observed in UR3, Square, and Transport.
Recent concurrent work, such as the BID Diffusion Policy \cite{liu2024bidirectional}, introduces bidirectional decoding to bridge the tradeoff between long-term consistency and short-term reactivity by coupling past decisions with forward-looking strategies. Exploring similar decoding strategies could further enhance the robustness of IMLE Policy in multi-modal and stochastic environments. While IMLE Policy is computationally efficient at inference, training incurs some additional overhead due to nearest-neighbour (NN) searches. While we do not optimise this in our work, since we did not find the additional training cost to be too significant, the NN search could be improved using more efficient NN techniques as in prior IMLE research \cite{imle}. Furthermore, while our method outperforms baselines in most settings particularly in the low data regime, it does not consistently outperform Diffusion Policy in every benchmark task in the full dataset setting, though we note that Diffusion Policy still requires iterative inference, whereas IMLE Policy is significantly faster.
% Another potential limitation is the scalability of IMLE Policy to much larger datasets, where factors such as batch size selection and memory efficiency in rejection sampling could introduce new challenges. 


% -


% IMLE Policy is designed specifically to accurately model action distributions without mode collapse. We find that it is very effective at capturing the modes of behaviour present in the demonstration data. This can be seen in Figure \ref{fig:simpusht-sweep}, where IMLE Policy tends to maintain multiple modes of behaviour from a variety of states. While this is in general a positive attribute for many generative modelling settings, we find that this shifts more weight onto the quality of the data, since the model will capture the all modes present. Therefore if the dataset contains a significant number of human errors, the model may be more likely to capture these modes of behaviour. When more modes are captured by the model, having the policy select consistent trajectories becomes more important to avoid getting stuck between selecting conflicting modes. We address this with a simple approach for selecting consistent trajectories, but more exploration should be done to quantify the effect of poor demonstrations on policy performance. 

% \todo{- consistency can sometimes hurt performance as we are using noisy future actions to determine nearest neighbour to what we would like to execute next. In the case where these future actions are highly errorneous we persisnet on utulising these behaviours which occasionalluy impacted performance as shown in UR3, Square and Transport. More exploration could be done in our consistecny can be implemetned with recent concurrent work proposing... (talk about BID Diffusion Policy paper)
% - Policy training is slightly slower as we have to compute NN for each element in the batch. WE note here that we did not utilise and optimised NN search and simply cdist, while the orginal IMLE paper leveraged as highly optmised NN search which could be incorpirate in future work.
% - Still gets outperformed in some cases by diffusion policy in some becnmarks however we note that we are faster, and IMLE was not optimised per task.
% - 
% }




\section{Future Work}

IMLE Policy offers a promising foundation for advancing behaviour cloning in real-world robotics and presents several avenues for further research and applications. To facilitate exploration, we will release all code and datasets, encouraging the robotics and machine learning communities to investigate and build upon this work in a variety of domains.

\textbf{Exploring the use of IMLE Policy for low-cost robotics.} IMLE Policy’s efficient single-step inference and sample-efficiency presents an opportunity for adoption in low-cost, open-source robotics initiatives, such as LeRobot \cite{cadene2024lerobot}, where computational constraints limit the use of complex methods. Future research could investigate how IMLE Policy can be effectively leveraged in such settings to democratise access to high-quality robotic learning and enable broader adoption.

\textbf{Investigating reinforcement learning fine-tuning with IMLE Policy.} The simplicity of IMLE Policy's single-step inference makes it a promising candidate for reinforcement learning (RL) fine-tuning, potentially streamlining the adaptation of imitation-trained policies to new environments. Future work could explore how IMLE Policy simplifies RL finetuning pipelines compared to iterative methods like Diffusion Policy, which make RL fine-tuning difficult \cite{dppo}.

\textbf{Utilising IMLE Policy for downstream robotics applications.} IMLE Policy’s expressive multi-modal capabilities open the door to higher-level control processes, such as Model Predictive Control (MPC), where diverse trajectory proposals could enhance decision-making or potentially facilitate meaningful exploration in reinforcement learning.

\textbf{Scaling IMLE Policy to large and diverse datasets.} While IMLE Policy has shown strong performance in low-data regimes, an important area for exploration is its scalability to large-scale and diverse datasets, such as the Open-X Embodiment dataset \cite{o2024open}. Understanding its generalisation across multi-task scenarios would be an interesting area to explore.



% With its efficient single-step inference, IMLE Policy is particularly well-suited for low-cost, open-source robotics initiatives such as LeRobot, where computational constraints limit the deployment of more complex methods. We encourage the open-source community to explore IMLE Policy in such settings to democratise access to high-quality robotic learning.

% Additionally, IMLE Policy's single-step inference unlocks new possibilities for reinforcement learning (RL) fine-tuning. Unlike methods such as Diffusion Policy, which require iterative de-noising steps that complicate backpropagation, IMLE Policy’s simplicity makes it a natural candidate for direct RL optimisation. This could streamline the next stage of robot learning, enabling imitation-trained policies to be efficiently adapted to new environments via RL.

% Beyond imitation and RL, IMLE Policy’s expressive multi-modal capabilities have significant potential for higher-level decision-making processes. Future research could investigate its use in Model Predictive Control (MPC), where diverse trajectory proposals could enhance decision-making, or in informed exploration strategies for reinforcement learning agents by generating meaningful, high-coverage behaviours. Furthermore, IMLE Policy's ability to perform real-time inference makes it a strong candidate for deployment in dynamic environments, where rapid adaptation is critical for success.

% Finally, while IMLE Policy has proven highly effective in low-data regimes, an important direction for future work is to explore how well it scales to large and diverse datasets, such as the Open-X Embodiment dataset. Understanding its generalisation across cross-embodiment and large-scale robotic datasets will be key to determining its potential as a scalable foundation model for robotics.

