





\section{Method}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/imle_policy_main.pdf}
    \caption{\textbf{IMLE Policy Overview}  a) Training: The policy takes in a sequence of past observations $\mathcal{O}$ and $m$ sampled latents $\textbf{z}$ for which the policy generates $m$ sequences of predicted actions $\mathcal{A}$. Generated trajectories that lie within the rejection sampling threshold $\epsilon$ are rejected. From the remaining trajectories, the nearest-neighbour to the ground truth trajectory is selected for training. We minimise the the distance between this trajectory and the ground truth trajectory to optimise the policy. As the loss focuses on each data sample, it ensures that all modes are captured even from small datasets. b) When compared to baselines with similar multi-modal capturing capabilities, IMLE can generate actions with a single inference step as opposed to multi-step de-noising processes. c) For highly multi-modal tasks, we enhance the performance of IMLE Policy by introducing a simple inference procedure to induce consistency upon mode choice based on a nearest-neighbour search over batch-generated action proposals with the previously executed action sequence.}
    \label{fig:enter-label}
\end{figure*}



\subsection{Background}
In the context of generative modelling, the aim is to learn the probability distribution of samples $p(x)$, which would allow us to then synthesise new samples via the trained model. We can represent the generator as a function $T_\theta : Z \rightarrow X$ that transforms samples from the latent space $Z$ to the space of target samples $X$, implemented as a neural network with parameters $\theta$. Such models have been historically trained via a generative adversarial objective (as in GANs), but this approach is prone to mode collapse, where only some modes of the target distribution are modelled.

A more recent approach, Implicit Maximum Likelihood Estimation (IMLE) has been introduced, providing an alternative training objective for the generator that avoids mode collapse \cite{imle}. The IMLE objective ensures that each training sample is well represented by the generator with samples generated nearby, alleviating the mode collapse issue. 

The IMLE training objective \cite{imle} is written as:
\begin{align}
    \theta_{\text{IMLE}}
    &= \argmin_{\theta} \mathbb{E}_{z_1,...,z_m \sim \mathcal{N}(0, I)}  \left [\sum_{i=1}^n \operatorname{min}\limits_{j\in [m]} d\left( {x}_i, T_\theta({z}_j)\right) \right ],
\label{eqn:imle}
\end{align}

where $d(\cdot,\cdot)$ is a distance metric, $n$ is the number of data samples, and $m$ is the number of generated samples. 

During training, $m$ number of samples $z_j$ are drawn from the latent prior distribution, a standard Gaussian distribution. These are transformed by the generator  $T_\theta$ into synthesised samples. For each training sample, the nearest synthetic sample is selected according to the distance metric $d(\cdot,\cdot)$. While this objective is effective for avoiding mode collapse, the selection procedure results in certain latent samples being rarely selected, even if they have a high likelihood under the latent prior distribution.

Rejection Sampling IMLE (RS-IMLE) \cite{rsimle} alleviates this issue by rejecting samples from the selection process if $d(x_i,T_\theta(z)) < \epsilon$, meaning they are too close to the training data sample. The remaining samples are then used in the IMLE training objective as before. Intuitively, this prevents the selection process from repeatedly selecting similar samples after they have already converged to fitting the data sample with some accuracy, defined by the parameter $\epsilon$.


% Training Algorithm
\begin{algorithm}[t]
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\caption{\textbf{IMLE Policy Training}}
\label{alg:training}

\Input{Training dataset $\mathcal{D} = \{(\mathcal{O}_i, \mathcal{A}_i)\}_{i=1}^n$, number of latents $m$, rejection threshold $\epsilon$, distance metric $d(\cdot, \cdot)$, generator $\pi_\theta(z, y)$.}
\Output{Trained policy $\pi_\theta$.}

\ForEach{$(\mathcal{O}_i, \mathcal{A}_i) \in \mathcal{D}$}{
    Sample $m$ latent vectors $\{z_j\}_{j=1}^m \sim \mathcal{N}(0, I)$\;
    Generate $m$ trajectories $\{\mathcal{A}_j\}_{j=1}^m$, where $\mathcal{A}_j = \pi_\theta(z_j, \mathcal{O}_i)$\;
    Compute distances $d_j = d(\mathcal{A}_i, \mathcal{A}_j)$ for $j \in [m]$\;
    Filter valid trajectories $\mathcal{V} = \{j \in [m] : d_j \geq \epsilon\}$\;
    Select nearest trajectory $j^* = \argmin_{j \in \mathcal{V}} d_j$\;
    Update $\theta$ to minimise $d(\mathcal{A}_i, \mathcal{A}_{j^*})$\;
}
\Return{$\pi_\theta$}
\end{algorithm}

% Inference Algorithm
\begin{algorithm}[t]
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\caption{\textbf{Inference with Temporal Consistency in IMLE-Policy}}
\label{alg:inference_temporal_consistency}

\Input{Policy $\pi_\theta$, observation $\mathcal{O}$, number of latents $m$, distance metric $d(\cdot, \cdot)$, previously executed trajectory $\mathcal{A}_{\text{prev}[\mathcal{T}_{a}:\mathcal{T}_{p}]}$.}
\Output{Action sequence $\mathcal{A}_{[0:\mathcal{T}_{p}]}$.}

Sample $m$ latent vectors $\{z^j\}_{j=1}^m \sim \mathcal{N}(0, I)$\;
Batch generate $m$ trajectories $\{\mathcal{A}^j\}_{j=1}^m$, where $\mathcal{A}^j = \pi_\theta(z^j, \mathcal{O})$\;

Compute overlaps $o^j = d(\mathcal{A}_{\text{prev}[\mathcal{T}_{a}:\mathcal{T}_{p}]}, \mathcal{A}^j_{[0:\mathcal{T}_{a}]})$ for $j \in [m]$\;
Select trajectory $j^* = \argmin_{j \in [m]} o^j$\;

\Return{$\mathcal{A}_{j^*}$}
\end{algorithm}

\subsection{Conditional RS-IMLE}
% RS-IMLE \cite{rsimle} was originally developed for unconditional image generation and in order to adapt it to the behaviour cloning setting we extend this formulation to the conditional setting.
We extend the RS-IMLE framework \cite{rsimle}, initially developed for unconditional image generation, to the conditional setting, enabling its application to behavior cloning. In this case, our generator is augmented to include an additional conditioning variable $y$, and instead of the $m$ samples shared between all data points, we now select $m$ samples to synthesise \textit{for each} data point, each sharing the conditioning value of that point.

%Therefore each synthetic sample is $x_k = T_\theta(z_k,y_i)$ . 

Formally, for each training sample ($x_i, y_i$), we draw $m$ latent vectors $\{z_j\}_{j=1}^m$ from the standard Gaussian prior. These latent vectors are then transformed and filtered based on the rejection sampling criterion, resulting in a set of valid latent vectors $\mathcal{V}_i$:


\begin{align}
\mathcal{V}_i &= \{{z}_j \sim \mathcal{N}(0, I) : d({x}_i, T_\theta({z}_j, y_i)) \geq \epsilon, j=1,...,m\}
\end{align}


The valid latent vectors in $\mathcal{V}_i$ are transformed by the conditional generator $T_\theta({z}, {y}_i)$ to produce candidates, all conditioned on $y_i$. The nearest neighbour selection is then performed within this set of candidates specific to ${x}_i$, according to:  $\operatorname{min}\limits_{{z} \in \mathcal{V}_i} d\left( {x}_i, T_\theta({z}, y_i\right)) $. This per-example conditioning ensures that all candidates generated for a given training example share the same conditional context $y_i$, allowing the model to learn the conditional relationships present in the training data.

The overall objective for the Conditional RS-IMLE is:
\begin{align}
\label{eqn:crsimle}
\theta_{\text{C-RS-IMLE}} = \argmin_{\theta} \sum_{i=1}^n \mathbb{E}_{\mathcal{V}_i} \left[ \min_{{z} \in \mathcal{V}_i} d\left( {x}_i, T_\theta({z}, y_i)\right) \right],
\end{align}

\noindent where $\theta_{\text{C-RS-IMLE}}$ represents the optimal parameters of the model $T_\theta$ under the Conditional RS-IMLE objective. These parameters minimize the sum of expected minimum distances between each real data point ${x}_i$ and the generated outputs $T_\theta({z}, {y}_i)$ over the valid latent vectors ${z}$ in the corresponding set $\mathcal{V}_i$.

\subsection{IMLE Policy}
With our conditional variant of RS-IMLE, we can now apply this training objective (Eq. \ref{eqn:crsimle}) to behaviour cloning, where we formulate our policy as a generator $\pi(z,\mathbf{o}) \mapsto \mathbf{a}$, where our conditioning variable $\mathbf{o}$ takes the form of image and robot state information and $\mathbf{a}$ represents the generated actions. For the distance function $d(\cdot,\cdot)$ we use Euclidean distance. We utilise the same action generation procedure as \cite{chi2023diffusion}, generating a sequence of actions which we can utilise for closed-loop receding-horizon control. At each time step $t$, the policy processes the latest $T_o$ steps of observation data, $\mathcal{O}_t$, as input and predicts $T_p$ steps of future actions. Out of these, $T_a$ steps are executed on the robot before re-planning occurs. Here, $T_o$ is referred to as the observation horizon, $T_p$ as the action prediction horizon, and $T_a$ as the action execution horizon.
Although we use the same UNet architecture as Diffusion in this work, the approach is generally applicable to other architectures such as transformers. 
We summarise the IMLE Policy training algorithm in Algorithm \ref{alg:training}

% \subsection{Temporal Consistency}
% \todo{add in the math notation to stay consistent with the algorithm block and the figure 3}
% We notice that since our method is able to capture multiple modes of behaviour, and each step of inference is independent from the previously selected actions, certain decision points during tasks with multiple options can cause the policy to get stuck, selecting one mode and then a different mode the next time the policy is queried. To remedy this, we propose a simple fix where we batch generate several trajectory options with different latent samples, and select the proposed trajectory with the nearest overlap from the remaining steps in the previously selected trajectory. We reset the trajectory selection every $C$ steps by randomly selection from the generated trajectories, to avoid committing to a mode for too long if it happens to be a poor behaviour. We note that all methods that are capable of accurately expressing multimodal action distributions are susceptible to this issue. Other works have explored more involved strategies for consistency \cite{new paper on consistency,høeg2024streamingdiffusionpolicyfast} Since our method is a single step generator model, batching a set of multiple trajectories is very straightforward and computationally efficient. 


\subsection{Temporal Consistency}

IMLE Policy effectively models multi-modal action distributions; however, since each inference step is independent, the policy may exhibit \textit{mode-switching} at decision points where multiple valid behaviour exist. This can lead to oscillatory behaviour, particularly in long-horizon tasks where smooth and consistent execution is necessary. To mitigate this, we introduce a \textit{batched trajectory selection mechanism} that enforces temporal consistency while maintaining the policy’s multi-modal expressivity during inference.  

At each time step \( t \), we generate a batch of \( m \) latent-conditioned trajectories,  
\begin{align}
\mathcal{A}^j = \pi_\theta(z^j, \mathcal{O}_t), \quad \text{for } j \in [m],
\end{align}
\noindent where \( z^j \sim \mathcal{N}(0, I) \) are sampled latent vectors, and \( \mathcal{A}^j \) represents a candidate trajectory. To maintain temporal consistency, we select the trajectory that minimises deviation from the previously generated trajectory over the overlapping horizon:

\begin{align}
j^* = \argmin_{j \in [m]} d(\mathcal{A}_{\text{prev}[\mathcal{T}_{a}:\mathcal{T}_{p}]}, \mathcal{A}^j_{[0:\mathcal{T}_{a}]}),
\end{align}

\noindent where \( d(\cdot, \cdot) \) is a distance metric, \( \mathcal{A}_{\text{prev}[\mathcal{T}_{a}:\mathcal{T}_{p}]} \) represents the unexecuted segment of the previously generated trajectory and  \( \mathcal{A}^j_{[0:\mathcal{T}_{a}]} \) represents the executable $T_a$ actions from the generated action sequence. As in training, we use Euclidean distance here for $d(\cdot,\cdot)$. The first $T_a$ steps of the selected trajectory \( \mathcal{A}^{j^*} \) are then executed at timestep \( t \).  

To prevent the policy from overcommitting to a suboptimal trajectory, we introduce a periodic trajectory reset. Every \( C \) steps, a new trajectory is randomly selected from the candidate batch:
\begin{align}
j^* = \text{Uniform}([m]).
\end{align}

\noindent This ensures adaptability while preventing long-term commitment to potentially poor behaviours.  

Notably, mode-switching is a challenge inherent to any model that accurately captures multi-modal behaviour, as the policy must balance between expressivity and trajectory consistency. Prior works have explored alternative strategies such as bidirectional decoding and streaming diffusion   \cite{liu2024bidirectional,høeg2024streamingdiffusionpolicyfast}. However, since IMLE Policy is a single-step generative model, our batch-based selection strategy is straightforward and computationally efficient and can run at $>100$ Hz.
We summarise inference using IMLE policy with consistency in Algorithm \ref{alg:inference_temporal_consistency}.
