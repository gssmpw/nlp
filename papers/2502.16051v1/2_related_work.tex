
\section{Related Work}
\label{sec:2_related_work}

Numerous benchmarks and datasets have been introduced to train or evaluate AI systems for medical applications
ranging from genetics, radiology, cardiology, and EMR applications \cite{Hou2023, Zambrano2023, Oh2024} to medical exam-like content such as MedQA \cite{Jin2021}, MMMU \cite{Yue2023}, NEJM Image Challenges \cite{NEJM2024}, and Path-VQA \cite{He2020}, alongside exam-based tasks like MedMCQA \citep{pmlr-v174-pal22a} and MMLU \citep{hendrycks2021measuring}. 
Broader efforts include MedS-bench \citep{Wu2025} for LMs in clinical contexts and Google’s Gemini initiative \cite{Saab2024} or state-of-the-art graduate-level and human expert benchmarks \citep{rein2024gpqa, phan2025humanitysexam}. 

In mental health, researchers have compiled datasets of counseling sessions \citep{Adhikary2024}, explored AI-driven diagnostic reasoning \citep{karthikesalingam2024_14}, and automated clinical documentation \citep{falcetta2023_15, axios2024_16}. 
They have also investigated therapy referrals \citep{sin2024_36, habicht2024_37}, peer support \citep{sharma2023_38}, patient attitudes \citep{pataranutaporn2023_39}, and augmented care via automated psychotherapy, diagnosis, and biometric stress analysis \citep{higgins2023_17, thieme2023_18, li2023_41, balan2024_42, kasula2023_19, ates2024_40}, with broader safety considerations \citep{ganguli2022red, wang2023decodingtrust, zhang2023safetybench, liu2024mmsafetybench}, concrete safety concerns in mental health emergencies \citep{grabb2024risks}, and demographic biases \citep{gabriel_can_2024} remaining active concerns.

Unlike the existing exam-style benchmarks and multi-specialty medical datasets, our work focuses specifically on capturing the everyday ambiguities of mental healthcare tasks that often lack a single “correct” answer supported by extensive human expert input without intentionally contaminating the data with LM assistance. 
Thus, our work complements large datasets \citep[e.g.][]{Wu2025} that focus on scale.
While prior efforts have explored broader medical applications or aggregated data from exams, clinical notes, and research publications, our evaluation-first approach emphasizes diverse expert annotations, real-life psychiatric decision-making, and open-source availability, specifically within mental health. 
Finally, we evaluate the impact of demographic diversity on a wide variety of tasks such as triage and documentation—an analysis often overlooked by more extensive, general-purpose medical benchmarks.

% % Copied from TAIMH and Declan's Text


% There is a significant body of prior work regarding AI benchmarks and data sets for medical applications.
% For example, in genetics, radiology, cardiology, and electronic medical record data processing\cite{Hou2023, Zambrano2023, Oh2024}, 



% In the domain of mental health datasets and summarization, Adhikary et al. introduced a dataset comprising 191 counseling sessions with associated summaries \cite{Adhikary2024}




% Google published *Capabilities of Gemini Models in Medicine*, incorporating several prominent medical benchmarks \cite{Saab2024}. The authors highlight the novelty of their work as “the most comprehensive benchmarking of multimodal medical models to date” based on their use of 14 different medical benchmarks \cite{Saab2024}. 

% These benchmarks include ECG-QA \cite{Oh2024}, MedQA \cite{Jin2021}, GeneTuring \cite{Hou2023}, MMMU (health medicine) \cite{Yue2023}, NEJM Image Challenges \cite{NEJM2024}, Path-VQA \cite{He2020}, and others. Additionally, an effort was made to enhance the clinical relevance of these findings by evaluating the models’ ability to summarize complex medical information and generate referral letters for specialists \cite{Saab2024}. 

% the most popular medical benchmarks, such as medical question answering (MedQA), medical multiple-choice question answering (MedMCQA \citep{pmlr-v174-pal22a}), and massive multitask language understanding (MMLU \citep{hendrycks2021measuring}), are derivatives of medical exams such as USMLE or the MCAT

% A related study, published in 2025, identified 11 high-level clinical tasks and created a large benchmark (MedS-bench \citep{Wu2025}) which aggregates 58 open-source biomedical natural language processing datasets from five text sources, including exams, clinical texts, academic papers, medical knowledge bases, and daily conversations to create a instruction tuning dataset


% Google recently published its Artificial Medical Intelligence Explorer tool, which it describes as an “AI system based on a LLM and optimized for diagnostic reasoning and conversations” in primary care settings \citep{karthikesalingam2024_14}. 
% Companies utilize AI to automate administrative tasks in healthcare and assist providers in completing necessary documentation \citep{falcetta2023_15, axios2024_16}.

% Researchers are exploring AI's potential impact on various aspects of mental healthcare delivery, including its ability to automate therapy referrals \citep{sin2024_36, habicht2024_37}, empower peer support specialists \citep{sharma2023_38}, and how the technology is viewed by patients \citep{pataranutaporn2023_39}. 
% They have also discussed its ability to augment clinical decision-making \citep{higgins2023_17}, deliver treatment in the form of increasingly automated psychotherapy or conversational agent \citep{thieme2023_18, li2023_41, balan2024_42}, aid in psychiatric diagnosis \citep{kasula2023_19}, and investigated the use of AI to analyze biometric data in the setting of stress \citep{ates2024_40}. 

% Other work creates data sets to evaluate physical/medical-related safety concerns \citep{ganguli2022red, wang2023decodingtrust, zhang2023safetybench, liu2024mmsafetybench}, but do not study mental health-related issues or only partially touch on the topic of mental health treatment.



% \citet{gabriel_can_2024} uses human expert evaluation with trained clinicians to show that LLMs can use implicit and explicit cues to infer patient demographics like race. 


% SOTA benchmarks on graduate level tasks and problem-solving created by human experts
% \citep[e.g.][]{rein2024gpqa, phan2025humanitysexam}