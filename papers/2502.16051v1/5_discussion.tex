
\section{Discussion and Limitations}
\label{sec:5_discussion}

The MENTAT dataset is a critical step in advancing AI evaluation for real-world psychiatric decision-making. Unlike traditional medical AI benchmarks emphasizing fact recall, MENTAT captures the inherent ambiguities and complexities of mental healthcare tasks. 
To the best of our knowledge, MENTAT is the first dataset of its kind, relying fully on expert-guided design and annotation for mental healthcare.
This dataset provides a more realistic evaluation of AI capabilities by incorporating expert-created decision-making scenarios across diagnosis, treatment, monitoring, triage, and documentation. Our experiments reveal that while models perform well on structured tasks (diagnosis, treatment), they struggle significantly with ambiguous real-world tasks such as triage and documentation, underscoring the limitations of current AI models in handling uncertainty.

\textbf{Limitations}: Despite its contributions, MENTAT has several limitations. First, the dataset is small and U.S.-centric, excluding fine-tuning applications and other healthcare systems. Second, while we ensured diverse annotators and thorough annotation processing, biases or errors may persist. Third, structured multiple-choice and free-form evaluations do not fully capture the dynamic nature of real-world psychiatric decision-making. Finally, there is a risk that AI systems could be prematurely deployed in psychiatric care, potentially leading to harmful, biased, or unreliable clinical decisions.

\textbf{Future Directions}: Future efforts could expand MENTAT to include more questions and annotators. Also, AI models should be evaluated in conversational and interactive settings, reflecting real-world psychiatric interactions. Additionally, further research is needed to mitigate demographic biases and ensure AI models make equitable, safe, and clinically useful decisions.