

\begin{figure}[h!]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{figures/final_eval_results_by_base.pdf}}
    \caption{Using the core dataset of MENTAT ($\mathcal{D}_0$), we evaluate eleven off-the-shelf instruction-tuned and three (mental) healthcare fine-tuned models for their task-specific accuracy. The random baseline is 0.2 due to all questions having five answer options.}
    \label{fig:final_eval_results_by_base}
    \end{center}
    \vskip -0.2in
\end{figure}

\section{Experiments}
\label{sec:4_experiments}

We demonstrate some of the different use cases of MENTAT outlined in \Cref{sec:3_4_dataapplication}:
Evaluating decision-making accuracy across MENTAT’s five categories and how performance is impacted by patient demographic information, and using MENTAT as a ground-truth reference for evaluating free-form LM outputs.

\subsection{Setup, Data, and Models}
\label{sec:4_1_setup}

\textbf{Data:} To evaluate a selection of off-the-shelf and fine-tuned language models in \textit{multiple-choice QA} settings in \Cref{sec:4_2_taskaccuracy} and \Cref{sec:4_3_taskaccuracydemographic}, we use the MENTAT evaluation dataset to create four separate evaluation datasets. 
We use the base set and sample each question once with a random patient gender, random age, and random ethnicity. We use this dataset $\mathcal{D}_0$ of 183 prompts to evaluate models on all five tasks.
To capture more variety for evaluating the impact of patient demographic information on accuracy, we create three additional datasets: $\mathcal{D}_\text{G}$ with 549 prompts, by including each question once for each gender option, $\mathcal{D}_\text{A}$ with 915 prompts, by including each question five times with a different random age, and $\mathcal{D}_\text{N}$ with 1098 prompts, by including each question six times with a different random ethnicity.
For the multiple-choice QA setting, we sample each tested LM at temperature $T = 0$. 

For \Cref{sec:4_4_consistency}, we collect \textit{free-form responses} by also using the base set and removing the multiple-choice options to get a dataset $\mathcal{D}_\text{FF}$ of 183 prompts.
We only use questions in the categories of triage, diagnosis, and treatment, prompting the models to respond in one sentence and sample 10 responses from each tested LM for each question at sampling temperature $T = 1$. 
Prompting details for all datasets are stated in \Cref{app:prompting}.

\textbf{Models:} We evaluate eleven off-the-shelf instruction-tuned LMs and three LMs that have been fine-tuned for mental health applications. Specifically, we evaluate 
\begin{itemize}
    \item GPT-4o-mini (\textit{gpt-4o-mini-2024-07-18}), GPT-4o (\textit{gpt-4o-2024-08-06}), o1 (\textit{o1-2024-12-17}), and o1-mini (\textit{o1-mini-2024-09-12}) \citep{openai2025models},
    \item Claude 3.5 Sonnet (\textit{claude-3-5-sonnet-20241022}), Claude 3.5 Haiku (\textit{claude-3-5-haiku-20241022}), Claude 3 Opus (\textit{claude-3-opus-20240229}), Claude 3 Haiku (\textit{claude-3-haiku-20240307}) \citep{anthropic2025models}, 
    \item Llama2-7b (\textit{llama2-7b-chat}) \citep{touvron2023llama}, Llama3.1-8b (\textit{llama3.1-8b-instruct}), Llama3.2-3b (\textit{llama3.2-3b-instruct}) \citep{grattafiori2024llama3herdmodels}, 
    \item \textit{PMC-LLaMA-13B} \citep{pmcllama}, \textit{Meditron-7b} \citep{meditron}, \textit{MentaLLaMa-7b-chat} \citep{MentaLLaMA}, and \textit{MMedS-Llama-3-8B} \citep{Wu2025}.
\end{itemize}
Please note that none of the model developers recommend deploying their models in a clinical setting.
Due to the lack of datasets, we could not find open-source models that were fine-tuned for mental healthcare decision-making, mainly Chatbots fine-tuned for therapy-like conversations with practitioners.
Hence, MENTAT represents a critical step toward filling this gap, offering a rigorous, open dataset designed to evaluate and advance LM-based solutions for mental healthcare decision-making.

\begin{table}[t]
    \caption{Average task-specific accuracy (95\% CL) across all models and separately for only OpenAI and Anthropic models, uncertainties estimated from bootstrap resampling, and calculated with weighted arithmetic means.}
    \label{tab:average_accuracy_base}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{l|cc}
    \toprule
    \makecell{[Mean Accuracy]($\uparrow$)} & \makecell{All\\Models} & \makecell{Only OpenAI\\ \& Anthropic} \\
    \midrule
    Diagnosis    & \textbf{0.82$\pm$0.03} & 0.91$\pm$0.04 \\
    Monitoring    & 0.67$\pm$0.03 & 0.78$\pm$0.04 \\
    Treatment    & 0.76$\pm$0.03 & \textbf{0.93$\pm$0.03} \\ 
    Triage    & 0.44$\pm$0.02 & 0.48$\pm$0.03 \\
    Documentation    & 0.40$\pm$0.02 & 0.45$\pm$0.02 \\
    \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table}
% Diagnosis	 0.817 +- 0.034
% Monitoring	 0.669 +- 0.032
% Treatment	 0.762 +- 0.027
% Triage	 0.438 +- 0.024
% Documentation	 0.404 +- 0.019

% Diagnosis	 0.905 +- 0.035
% Monitoring	 0.784 +- 0.035
% Treatment	 0.926 +- 0.031
% Triage	 0.477 +- 0.031
% Documentation	 0.450 +- 0.023




\subsection{Task-Specific Accuracy}
\label{sec:4_2_taskaccuracy}

Using the dataset $\mathcal{D}_0$, we evaluate all models for their task-specific accuracy and showcase the results in \Cref{fig:final_eval_results_by_base}.
Due to restrictions of most closed-source models, we can only compare all models by relying on accuracy instead of using log probabilities to enable more nuanced analyses with, e.g., cross-entropy loss or Brier score.
Unsurprisingly, the significantly larger closed-source models outperform smaller open-source models, and newer, more refined, and capable models tend to outperform their predecessors across categories.
The fine-tuned open source models do not outperform their Llama2 and Llama3 counterparts with statistical significance.
\footnote{We omit Meditron-7b results due to performance issues (95\% confidence interval includes random baseline in all categories).}

Using the bootstrap resampled uncertainties, we can estimate symmetric Gaussian uncertainties at a 95\% confidence level and calculate the average accuracy per category across multiple models with the maximum likelihood estimator for the weighted arithmetic mean.
We do this calculation for all models together and again separately for the closed-source models from Anthropic and OpenAI.
The results are shown in \Cref{tab:average_accuracy_base}.
We find that models perform best in the diagnosis and treatment category, followed by monitoring.
Finally, all models perform around 50\% accuracy for triage and documentation, with the open-source model confidence intervals even including the 20\% random baseline, showcasing significant room for improvement and further research.

\begin{table}[t]
    \caption{Average accuracy (95\% CL) across tasks and all models and separately for only OpenAI and Anthropic models, uncertainties estimated from bootstrap resampling, and calculated with weighted arithmetic means.}
    \label{tab:average_accuracy_demographic}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{l|cc}
    \toprule
    \makecell{[Mean Accuracy]($\uparrow$)} & \makecell{All\\Models} & \makecell{Only OpenAI\\ \& Anthropic} \\
    \midrule
    Using $\mathcal{D}_\text{G}$ & &\\
    Female    & 0.63$\pm$0.02 & 0.72$\pm$0.02 \\
    Male    & 0.65$\pm$0.01 & 0.74$\pm$0.02 \\
    Non-Binary    & \textbf{0.66$\pm$0.01} & \textbf{0.78$\pm$0.02} \\
    \midrule
    Using $\mathcal{D}_\text{N}$ & &\\
    African American    & 0.65$\pm$0.02 & 0.77$\pm$0.02 \\
    Native American    & 0.67$\pm$0.02 & \textbf{0.82$\pm$0.02} \\
    White    & \textbf{0.69$\pm$0.02} & 0.78$\pm$0.02 \\
    Black    & \textbf{0.69$\pm$0.02} & 0.80$\pm$0.02 \\
    Asian    & 0.65$\pm$0.02 & 0.77$\pm$0.02 \\
    Hispanic    & 0.68$\pm$0.01 & 0.81$\pm$0.02 \\
    \midrule
    Using $\mathcal{D}_\text{A}$ & &\\
    18-33 Years    & \textbf{0.70$\pm$0.01} & \textbf{0.80$\pm$0.02} \\
    33-49 Years    & 0.64$\pm$0.01 & 0.75$\pm$0.02 \\
    49-65 Years    & 0.64$\pm$0.01 & 0.74$\pm$0.02 \\
    \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

% \renewcommand{\arraystretch}{1.3} % Increase row spacing for readability
\begin{table*}[h!]
\caption{Deviation (inconsistency) scores from the omitted multiple-choice answer options for different models across diagnosis, treatment, and triage tasks. We also list the accuracy results from \Cref{fig:final_eval_results_by_base} for comparisons.}
\label{tab:uncertainty-table}
\vskip 0.15in
\begin{center}
\begin{small} % Use smaller text to fit within column width
\begin{sc}
\begin{tabular}{l|cc|cc|cc}
\toprule
Model & \multicolumn{2}{c|}{Diagnosis} & \multicolumn{2}{c|}{Treatment} & \multicolumn{2}{c}{Triage} \\
 & \makecell{($\downarrow$) Free-Form\\Inconsistency } & \makecell{($\uparrow$) MCQA\\Accuracy}  & \makecell{($\downarrow$) Free-Form\\Inconsistency} & \makecell{($\uparrow$) MCQA\\Accuracy} & \makecell{($\downarrow$) Free-Form\\Inconsistency} & \makecell{($\uparrow$) MCQA\\Accuracy}  \\
\midrule
GPT-4o & $0.55^{+0.05}_{-0.05}$ & $0.93^{+0.07}_{-0.09}$ & $0.82^{+0.04}_{-0.04}$ & $0.98^{+0.02}_{-0.05}$  & $0.75^{+0.04}_{-0.04}$ & $0.42^{+0.19}_{-0.19}$  \\
o1 & $0.40^{+0.05}_{-0.05}$ & $0.96^{+0.04}_{-0.07}$ & $0.77^{+0.04}_{-0.04}$ & $0.95^{+0.05}_{-0.07}$ & $0.77^{+0.04}_{-0.05}$ & $0.46^{+0.19}_{-0.19}$ \\
\midrule
Claude 3.5 &&&&&&\\
Haiku & $0.75^{+0.04}_{-0.04}$ & $0.89^{+0.07}_{-0.07}$ &  $0.88^{+0.03}_{-0.03}$ & $0.93^{+0.07}_{-0.10}$ & $0.79^{+0.04}_{-0.04}$ & $0.50^{+0.19}_{-0.19}$  \\
Sonnet & $0.74^{+0.03}_{-0.03}$ & $0.85^{+0.11}_{-0.11}$ & $0.84^{+0.04}_{-0.04}$ & $0.95^{+0.05}_{-0.07}$ & $0.77^{+0.05}_{-0.05}$ & $0.54^{+0.19}_{-0.19}$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small} % Close small text
\end{center}
\vskip -0.1in
\end{table*}

\subsection{Impact of Demographic Patient Information}
\label{sec:4_3_taskaccuracydemographic}

We repeat the evaluation of all models but use the datasets $\mathcal{D}_\text{G}$, $\mathcal{D}_\text{A}$, and $\mathcal{D}_\text{N}$ to see how model performance is affected by different patient demographic information.
In \Cref{app:more_experiment_results}, we illustrate the results for different patient gender coding in \Cref{fig:final_eval_results_by_gender}, different patient ethnicity in \Cref{fig:final_eval_results_by_nat}, and different patient age groups in \Cref{fig:final_eval_results_by_age}.
We also calculate the average accuracy across all categories for different patient demographic information as in the previous section and present them in \Cref{tab:average_accuracy_demographic}.

With statistical significance, we see that when we look at all models together or when only considering the tested closed-source models, that 
the accuracy is higher for non-binary-coded patients compared to female-coded patients; the accuracy is lower for patients with an ``Asian" or "African American" background compared to other backgrounds, and the accuracy is higher for patients states in the age group 18 to 33 years compared to all other age groups.
These results highlight the need for further methods to mitigate the propagation and perpetuation of harmful biases before deploying models in mental healthcare settings.
Determining the exact cause of these results is complex, given the significant impact differences in pre- and post-training data have on models, as seen in other works studying tendencies and biases \citep[e.g.][]{lamparth2024human, moore-etal-2024-large}.

\subsection{Consistency of Free-Form Decisions}
\label{sec:4_4_consistency}

Here, we demonstrate that the MENTAT dataset can be used to evaluate LMs giving free-form responses to mental healthcare questions as well. 
Specifically, we test how consistent free-form LM responses are to the correct expert-annotated answer choice as defined by the highest preference probability for a question using $\mathcal{D}_\text{FF}$.
To measure free-form consistency, we use the methodology and code from \citet{shrivastava2024measuring} (MIT license).
\citet{shrivastava2024measuring} showed that it is possible to use $1 -$ BERTScore \citep{Zhang2020BERTScore} with the DeBERTa xlarge embedding model \citep{he2020deberta} fine-tuned with MNLI \citep{williams2017broad} to measure free-form decision-making inconsistency in different settings, including replicating human expert classification labels of safe and unsafe responses of users in mental health emergencies interacting with LMs \citep{grabb2024risks} \footnote{\citet{shrivastava2024measuring} also check the robustness of the inconsistency metric to systematic effects like text length.}.

By taking $1 -$ BERTScore as an inconsistency metric, we can measure how far models deviate in free-form responses from the annotated expert answer options.
Note, that this deviation could also increase for good answers not specified in the existing answer options.
We can compute each response's inconsistency with the expert-annotated correct annotation, average over all samples and questions, and estimate the uncertainty with bootstrap resampling between the average score of each question.
The results in \Cref{tab:uncertainty-table} show that a high multiple-choice accuracy score does not correlate with producing similar answers in free-form response prompting.
While all models also have a high inconsistency score for the triage category where they have a lower accuracy, this is not true for the OpenAI models in the diagnosis category.
All models generate responses that are very inconsistent with the original answer options in the treatment category.
In summary, although a model can achieve high multiple-choice accuracy, its free-form answers may deviate significantly from the expert “correct” options, highlighting the importance of evaluating decision-making in multiple-choice settings and with free-form responses rather than relying solely on exam-style questions about recalling fact-based knowledge.