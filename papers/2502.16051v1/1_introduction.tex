
\section{Introduction}
\label{sec:1_introduction}

\begin{figure}[ht]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=0.75\columnwidth]{figures/mentat_logo.pdf}}
    \caption{
    Designed and annotated by mental health clinicians, the MENTAT (\textit{MENtal health Tasks AssessmenT}) dataset contains 203 base questions and answers of day-to-day mental healthcare decision-making across five categories: Diagnosis, documentation, treatment, triage, and monitoring, offers variation of non-decision-relevant patient demographic information, and captures task-specific ambiguity in the uncertainty of expert preferences.}
    \label{fig:mentat_logo}
    \end{center}
    \vskip -0.2in
\end{figure}

Benchmarks in medical AI are pivotal for gauging progress and guiding model development. Evaluations typically rely on medical student or specialty board-style exams \citep[e.g.][]{Jin2021, pmlr-v174-pal22a}. 
However, even for humans, numerous studies indicate that success in these standardized tests only weakly correlates with clinicians’ real-world performance \cite{Saguil2015}, a disconnect that can be especially problematic in psychiatry, where diagnosis and management hinge on subjective judgments and interpersonal nuances. 
Recent findings underscore this need for more grounded, task-specific benchmarks in mental health \cite{Raji2025}. 
Although traditional exams emphasize factual knowledge, effective psychiatric practice demands a broader range of skills, from titrating medication to deciding on emergent hospitalization (see \Cref{app:medqa_to_mentat} for an extensive discussion on the limitations of medical exam-style questions).
While newer benchmarks such as MedS-bench \citep{Wu2025} emphasize high-level clinical tasks, psychiatry-specific evaluations remain limited, particularly those co-created by clinicians and human experts who navigate the daily ambiguities inherent to mental healthcare.

To address this gap, we introduce MENTAT (\textit{MENtal health Tasks AssessmenT})—a dataset and evaluation framework focused squarely on the pragmatic, real-world tasks in psychiatry, see \Cref{fig:mentat_logo}. 
Our expert-curated approach departs from standardized exam-style questions in several ways: 
(1) it emphasizes genuine clinical tasks such as triage, diagnosis, treatment, monitoring, and documentation; 
(2) it captures the inherent ambiguities in mental healthcare via multiple plausible answer options and preference annotations rather than enforcing a single “correct” fact-based response for two categories (triage and documentation); and 
(3) it leverages a diverse team of practicing psychiatrists to mitigate biases and ensure the relevance of each question to everyday clinical practice.

In this paper, we present MENTAT, describe its design and creation process, and demonstrate its utility
% In this paper, we present MENTAT, its design and creation process, and demonstrate its utility by 
comparing eleven off-the-shelf and four fine-tuned language models (LMs) in multiple-choice and free-form settings, with a specific focus on patient demographic sensitivity in decision-making performance. We also examine how MENTAT can serve as a ``ground-truth" reference for gauging model consistency in open-ended clinical responses. 
In contrast to most medical benchmarks that assess fact recall, our dataset targets decision-making performance—a critical yet challenging aspect of real-world psychiatry.
In summary, our key contributions are:

\begin{itemize}
    \item We introduce MENTAT, an expert-curated dataset that emphasizes real-world psychiatric ambiguities over exam-like fact recall across five mental healthcare practice domains: diagnosis, treatment, monitoring, triage, and documentation.
    \item We provide a hierarchical annotation pipeline, open licensing, and detailed coverage that allow for straightforward adjustments and support multiple evaluation paradigms to empower future work.
    \item We outline several use cases of MENTAT and demonstrate its applicability by evaluating decision-making accuracy across MENTAT’s five categories, how performance is impacted by patient demographic information, and how using MENTAT as a ground-truth reference can be valuable when evaluating free-form LM outputs.
\end{itemize}

% The full data set and the annotation processing and analysis pipeline will be publicly available on Github upon publication.
The dataset and annotation processing pipeline are publicly available on GitHub\footnote{\href{https://github.com/maxlampe/mentat}{github.com/maxlampe/mentat}} (MIT license).