
\section{MENTAT Dataset}
\label{sec:3_dataset}

The base data and all generated datasets, as well as the processing and generation code, are publicly available on \href{https://github.com/maxlampe/mentat}{GitHub}.
In this section, we communicate our design choices and assumptions to allow for custom adjustments in the code pipeline of MENTAT.

\subsection{Dataset Design and Creation}
\label{sec:3_1_datadesign}

Many, if not all, existing benchmarks and datasets for LMs in healthcare focus on medical exam-style questions (see \Cref{sec:2_related_work}), prioritizing recalling fact-based knowledge over evaluating pragmatic clinical decision-making and practicing psychiatric care.
Thus, our MENTAT dataset aims to capture the ambiguities encountered and daily actions taken by 
% mental health practitioners 
psychiatrists with human expert-designed questions, answer options, and annotations.
Our dataset captures human expert decision-making in five categories, allowing the open-source community to accurately assess and evaluate LM capabilities and training methods.
These five categories include 
\begin{itemize}
    \item \textbf{diagnosis} (utilizing information available to render a most likely diagnosis as outlined by the DSM-5-TR), 
    \item \textbf{treatment} (developing treatment plans for a patient's diagnosis and symptoms, often including detailed responses like medication dose that are often absent from medical exams and common benchmarks), 
    \item \textbf{triage} (determining the acuity of a presentation and escalating appropriately to higher levels of care), 
    \item \textbf{monitoring} (assessing the efficacy of various treatments and severity of conditions), 
    \item and \textbf{documentation} (recording clinical events in an amenable form for electronic medical records).
\end{itemize}
While this list of tasks is not exhaustive, it includes some of the most commonly occurring actions psychiatrists perform in delivering mental healthcare. 

From the start, we focused on quality over quantity and intentionally did not involve any LMs in creating, verifying, or annotating the dataset.
MENTAT contains 203 base questions (50 for diagnosis, 47 for treatment, 28 for triage, 49 for monitoring, and 29 for documentation) with five answer options each.
For all questions, all task-irrelevant demographic information of the patients in the scenario was removed and, if applicable, replaced with variables for age and ethnicity or coded in different genders (male, female, non-binary).
As demonstrated in \Cref{sec:4_experiments}, this allows for a nuanced evaluation of LM performance on different tasks and scaling the dataset for different applications.

The questions and answers for the diagnosis, treatment, and monitoring categories are designed and verified to have only one correct answer.
% In contrast, the questions and answer options in the triage and documentation categories are designed to be ambiguous and to have multiple plausible answer options, even for human experts, to capture the challenges and nuances in practicing these tasks. 
In contrast, the questions and answer options in the triage and documentation categories are designed to be ambiguous—featuring multiple plausible answers, even for human experts—to reflect the challenges and nuances of these tasks while still including a designated best answer as defined by the question creator.
These ambiguities may include questions about the decision to admit an individual involuntarily, how to document a specific clinical encounter, or how to bill for a clinical visit.

These specific tasks are ambiguous for many reasons:
In the case of billing, there are many components that psychiatrists incorporate into deciding upon the final billing code; these include the number of problems discussed/managed in the visit, the risk of the encounter, the duration, and the complexity of the encounter \citep{schmidt2011procedure}. 
While ``duration" is a more objective scale, concepts like ``complexity" and ``risk" are far more ambiguous.
Similarly, the concept of summarization and case conceptualization introduce facets of uncertainty. 
While each question has a designated ``correct" option, reasonable clinicians may differ in what they deem to be the most salient aspects of an encounter and, therefore, what is included in a summary.
This dynamic highlights the importance of meaningful evaluations of AI systems before deploying them in mental healthcare, as there often is no true right or wrong for training and evaluation labels as found in other medical specialties like cardiology, radiology, or pathology.

Due to these ambiguities, it is crucial to accurately represent and collect different expert opinions and avoid perpetuating harmful racial, gender, sexuality-based, or other biases in mental healthcare.
The MENTAT dataset is developed and overseen by a diverse group of practicing clinicians in the U.S.
% ethnic, sexual, and gender orientations, as well as different specializations in psychiatric care.
Because all nine question designers and annotators are practitioners and M.D.s in the U.S. psychiatric care system, MENTAT is limited to the scope of U.S. healthcare doctrine. 

We want to highlight that we do not conduct any human participant studies. 
Instead, we split our team into an analysis and expert team of psychiatric practitioners (``\textit{annotators}''), and we adopt the practices and methodologies informed by human behavioral studies to ensure robust annotation results. 
See \Cref{sec:3_2_data_annotaiton} for further details on annotation and processing.
During question and answer creation, a team of five 
% expert 
annotators propose questions with answers and outline a correct answer option, and the questions are then verified by someone else on the 
% expert 
annotator team. 
Conflicts are resolved via debate.
For turning annotations into preference scores to create labels for the ambiguous answer options in the triage and documentation category, a team of eight experts annotates randomized questions. 
The question-and-answer creation team and annotation team of experts overlap.
See \Cref{sec:3_2_data_annotaiton} for further annotation details.

While we try to follow AI benchmark design practices and standards \citep[e.g.][]{mcintosh2024inadequacies, reuel_hardy_2024}, MENTAT is intentionally an evaluation dataset and not a benchmark.
We split the base dataset into 90\% (183 questions) evaluation and designate 10\% (20 questions) for uses like few-shot prompting.
By prioritizing expert verification over volume and not limiting the dataset to a specific performance metric for the evaluation, we ensure MENTAT remains a robust and precise evaluation-first dataset, as a basis for future research and applications (see \Cref{sec:3_4_dataapplication}).

\subsection{Dataset Annotation and Analysis}
\label{sec:3_2_data_annotaiton}

\begin{figure}[ht]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=0.9\columnwidth]{figures/annotation_processing_raw_vs_hbt_127.pdf}}
    \caption{(Top) Mean annotation score example with 95\% confidence interval aggregated over all annotations for question 127 from the triage category. (Bottom) Resulting preference probabilities calculated via hierarchical Bradley-Terry model to be used as evaluation labels, e.g., to calculate accuracy or cross-entropy loss.}
    \label{fig:annotation_processing_raw_vs_hbt}
    \end{center}
    \vskip -0.2in
\end{figure}


To \textbf{collect annotations} for questions in the triage and documentation category, we asked eight 
% expert 
annotators to rate individual answer options with a web interface\footnote{Code vailable at \href{https://github.com/maxlampe/mentat_annotate}{github.com/maxlampe/mentat\_annotate}} using the \textit{jsPsych} library \citep{de_Leeuw2023} (MIT license).
In each annotation batch, a single expert annotates one random question at a time and 20 questions in total. 
We collect a total of 657 annotations for the 57 questions in the triage and documentation categories, averaging 11.5 annotations per question.

For each multiple-choice question, the annotators are instructed to read the question and all five answer options carefully, then independently rate each option on a scale from 0 to 100 to represent how valid they consider that answer to be. 
Since more than one option can be correct, incorrect, or somewhere in between, annotators are asked to treat each answer independently.
While all annotators are domain experts and highly willing to engage with the material, the web interface randomizes the \emph{starting position} of each validity slider, the \emph{order} in which answer options appear, and, if applicable, the \emph{patient gender} (though the shown patient gender is tracked). 
Interaction with every slider is required before progressing to the next question, and annotators may leave comments to flag any issues with a question or its answers. Figure~\ref{fig:mentat_q36_combined} in Appendix~\ref{app:annotator_interface} illustrates how the interface appears for one example question.

In \Cref{fig:annotator_scores_hbt_pars} in \Cref{app:annotation_details}, we show the
average annotation score with uncertainties for each annotator and that they are sufficiently different from a random baseline.
In \Cref{fig:annotation_processing_raw_vs_hbt}, we show the mean annotation score with bootstrap resampled uncertainties for one example question.
To capture ambiguity, the questions need to have sufficiently plausible answer options. 
Thus, we need to verify that the 
% expert 
annotators do not converge on one answer option and that there is inter-annotator disagreement.
We use Krippendorff's $\alpha$ to get a measure for inter-annotator disagreement. 
Krippendorff's $\alpha$ is designed to measure inter-rater reliability (``\textit{Do annotators produce consistent labels (or scores) for the same item?}") with $\alpha = 1$ indicating perfect agreement. 
Given our design choices, we expect $\alpha$ to be naturally low as our goal is not to measure the presence of a single ground truth and low $\alpha$ values ($\alpha \leq 0.5$) will not tell us how useful a set of annotations is—only that experts statistically disagree. 
We show the distribution of $\alpha$ for triage and documentation questions in \Cref{fig:raw_annotation_krippendorf} in \Cref{app:annotation_details}.
We verify that all $\alpha$ values are between slightly negative and 0.8. 
% We keep questions with low inter-annotator agreement because, by design, we want to have disagreement, and discarding items with very low alpha might remove exactly the ambiguous items we wanted to capture.
We do not discard any questions based on $\alpha$, e.g., due to low inter-annotator agreement, because, by design, we want to have disagreement and discarding items with very low alpha might remove exactly the ambiguous items we wanted to capture.

Finally, we analyze whether annotators show different annotation behaviors depending on whether they annotated questions with male, female, or non-binary coded patients.
Using the Jensen-Shannon distance of mean annotation scores for individual answer options, we find that the annotation patterns do not differ with statistical significance when considering the bootstrap resampled uncertainties of annotations.
However, this does not rule out any subconscious annotator bias and would require more annotations for a decisive result.

% Use triage and annotator bias to demonstrate challenges
% Why are the other categories with one true answer not biased? Annotation was subconscious bias, and one-true answer check was conscious
% JS distances between annotations for different genders (without uncertainty)
% 0.474 +0.050 -0.051
% 0.489 +0.058 -0.058
% 0.546 +0.057 -0.058

After collecting the raw annotation scores, we need to \textbf{process the annotations into a preference dataset}.
We use a hierarchical Bradley-Terry model to extract the expert annotator preferences for a question $k$ for different answer options $i$ from unprocessed annotation scores.
In a \textit{regular} Bradley-Terry model, the probability of answer option $i$ being preferred over $j$ is given by
%
\begin{align}
    P_k(i \succ j) 
    &= \frac{e^{\beta_{ik}}}{\,e^{\beta_{ik}} + e^{\beta_{jk}}\,}
    = \frac{1}{\,1 + e^{\beta_{jk} - \beta_{ik}}\,},
    \label{eq:bt-prob}
\end{align}
%
with $\beta_{ik}$ being the latent preference parameter for answer option $i$. 
This approach has the benefit of only using (scale-less) pairwise comparisons, thus eliminating issues arising from individual annotator numerical biases for one question $k$. 
We assume that most variations between annotator behavior are legitimate (i.e., some experts are more ``inclusive" with potential answers, while others are more strict), and we believe that difference captures real phenomena in their domain expertise. 
Part of what we might be learning from the data is that some experts hold stricter or more lenient criteria.
These assumptions also highlight the importance of a diverse annotation group to avoid perpetuating harmful biases.

% Another challenge of annotators rating five answer options simultaneously can be that they might have a clear ``winning" option in one annotation and might neglect other answer options by giving them equally low scores.
Simultaneously, we want to use all available information, including annotator-specific behavior \textit{across} questions and not just the differences between annotators for an individual question $k$.
Another challenge of annotators rating five answer options simultaneously can be that they might have a clear ``winning" option in one annotation and might neglect other answer options by giving them equally low scores.
To mitigate these issues and conservatively smoothen the data, we introduce an annotator-specific offset $\gamma_a$ and slope $\alpha_a$ for each annotator $a$ to turn \Cref{eq:bt-prob} into a hierarchical Bradley-Terry model:
%
\begin{align}
    P(i \succ j \mid a) 
    &= \frac{1}{\,1 + \exp\bigl[-\bigl(\gamma_a + \alpha_a \,(\beta_i - \beta_j)\bigr)\bigr]}.
    \label{eq:hbt-prob}
\end{align}
%
Introducing a slope and an offset can capture how strongly annotators separate options, tend to break (or not break) ties, and tend to prefer choosing fewer answers overall.
For the joint optimization of the $\beta_{ik}$ and individual annotator parameters $\gamma_a$ and $\alpha_a$, we use the negative log-likelihood with regularization for the annotator parameters as
%
\begin{align}
    \begin{split}
    -\log &\mathcal{L}(\boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\alpha}) 
    = 
    \\& \sum_{k}\sum_{a}
    \sum_{(i,j)\,\in\,\mathcal{D}_{ak}}
    \Bigl[
    y_{a,ij}\,\bigl(-\log P(i \succ j \mid a)\bigr)
    \;\\&+\;
    (1 - y_{a,ij})\,\bigl(-\log \bigl[1 - P(i \succ j \mid a)\bigr]\bigr)\Bigr]
     \\& + \lambda_0 \|\gamma_a\|^2 + \lambda_1 \|1 - \alpha_a\|^2.
    \end{split}
    \label{eq:hbt-nll}
\end{align}
%
Here, $y_{a,ij} = 1$ if annotator $a$ says item $i$ beats item $j$, and $0$ otherwise. 
The set $\mathcal{D}_{ak}$ is the collection of comparisons from annotator $a$ of question $k$.

Besides regularization, we bound the individual annotator parameters ($\gamma_a \in [-3.0, 3.0]$, $\alpha_a \in [0.5, 2.0]$) during the optimization to balance the goal of slightly de-noising the resulting preference dataset while keeping the majority of differences between individual annotator preferences \footnote{Our results do not significantly change without these bounds. We set them conservatively to reduce the risk of inducing bias.}.
These bounds prevent the model from fixing contradictory data by pushing a parameter to an extreme and we show the fitted parameters in \Cref{fig:annotator_scores_hbt_pars} in \Cref{app:annotation_details}.
To allow for a different set of assumptions about how to process the expert annotations for future use cases, our accompanying data pipeline code of MENTAT also allows the use of a regular Bradley-Terry model or modular replacements with alternative preference methods, e.g., Plackett-Luce.

\begin{figure}[ht]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=0.9\columnwidth]{figures/frac_ct_in_topk_bt_vs_hbt.pdf}}
    \caption{Comparing the probability for the original creator truth answer to be in the top-$k$ answers as defined by their preference probability when using a regular or a hierarchical Bradley-Terry model.}
    \label{fig:frac_ct_in_topk_bt_vs_hbt}
    \end{center}
    \vskip -0.2in
\end{figure}

Finally, we \textbf{calculate the overall probability} $p$ of an answer $i$ being preferred using the softmax function $p = \sigma(\boldsymbol{\beta})_i$ to create the final preference labels for each question.
To compare results with a regular and a hierarchical Bradley-Terry model, we check for how many questions the original question creator-preferred answer is in the top-$k$ ($k \in [1, 5]$) answer options as defined by their resulting preference probability in \Cref{fig:frac_ct_in_topk_bt_vs_hbt}.
While not an ideal metric, the original creator truth is always in the top-3 answer options defined by the hierarchical Bradley-Terry model, which is only the case for the regular model when looking at all answer options (top-5).

% Using one of the experts, we verify that objectively wrong answers have a final preference probability less than the random baseline, i.e., $p \leq 0.2$, including their 95\% confidence interval.
While the answers to the questions were designed to be ambiguous, most questions still have one or two objectively incorrect answers that violate clinical procedure or are factually inaccurate, e.g., incorrect billing codes for specific cases.
Using one of the experts, we determine these answer options, manually set their probability to 0, and recalibrate the other answer probabilities.
We do this at the end to get all individual annotator-specific behaviors across questions to determine the parameters with \Cref{eq:hbt-nll}.
In most cases, these objectively wrong answers would have had a final preference probability less than the random baseline, i.e., $p \leq 0.2$.
Our accuracy-based evaluations in \Cref{sec:4_experiments} are not affected by this post-processing step.


% \subsection{Dataset Pipeline}
% \label{sec:3_3_datapipeline}

\subsection{Use Cases and Applications}
\label{sec:3_4_dataapplication}

Although we intentionally designed MENTAT as an evaluation dataset grounded in human expertise rather than a large-scale training corpus, it offers several applications for research and development in mental healthcare AI. 

For example, researchers can directly evaluate LM decision-making via multiple-choice questions across MENTAT’s five categories, as demonstrated in \Cref{sec:4_2_taskaccuracy} and \Cref{sec:4_3_taskaccuracydemographic}. MENTAT enables fine-grained comparisons of LM performance under varying task requirements and patient demographics, allowing practitioners to probe how models handle different presenting symptoms, acuity levels, or documentation requirements. 
Furthermore, as illustrated in \Cref{sec:4_4_consistency}, MENTAT can serve as a ground-truth reference for evaluating free-form LM outputs, providing important references for dynamic evaluations of increasingly agentic AI systems. Instead of requiring strictly multiple-choice answers, one can compare open-ended responses to the expert-annotated options, thus balancing structured and creative approaches to mental health decision-making.
However, both applications share the caveat that MENTAT only partially captures the nuances of real-world interactions, such as unstructured patient interviews or free-form model responses exceeding the scope of predefined expert-annotated choices.

Beyond standard accuracy metrics, MENTAT’s multiple-choice format and preference annotations permit novel evaluation strategies, such as computing cross-entropy or Brier Scores from LM log probabilities. 
These more nuanced techniques facilitate assessments of model confidence, enabling alignment methods that account for expert uncertainty and disagreement. For instance, our hierarchical annotation scheme (see \Cref{sec:3_2_data_annotaiton}) yields probabilities that can serve as “soft” labels for calibrating or training alignment models\footnote{Practical clinical deployments often rely on a much broader context than a single question/answer pair, so these metrics should be viewed as indicative rather than definitive.}. 
Finally, MENTAT’s emphasis on capturing expert disagreement encourages ongoing research into techniques for modeling inter-annotator bias, validating novel prompting methods that handle ambiguous psychiatric scenarios, and investigating how demographic anchoring (e.g., age, ethnicity, or gender) shifts model outputs. 

% However, for any analysis based on the direct annotator scores or when requiring that absolute differences in annotator scores remain constant, the variations are most likely caused by different numeric rating scale biases which would require normalization or re-scaling. To this end, our API allows for rescaling of all annotators with a global function or based on their individual scaling behavior.

% canary string