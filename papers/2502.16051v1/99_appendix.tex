
\section{How is MENTAT Different from Medical Exam Questions?}
\label{app:medqa_to_mentat}

For years, medical AI benchmarks have focused on fact-based assessments. Most medical evaluations for LMs rely on board exams and medical student tests, primarily measuring knowledge recall rather than real-world clinical decision-making. These exams have little correlation with actual clinical practice, as passing them does not equate to the ability to manage patients effectively even in humans \cite{Saguil2015}.

\begin{figure}[ht]
    \begin{framed}
    A 32-year-old woman with type 1 diabetes mellitus has had progressive renal failure during the past 2 years. 
    She has not yet started dialysis. Examination shows no abnormalities. Her hemoglobin concentration is 9 g/dL, 
    hematocrit is 28\%, and mean corpuscular volume is 94 $\mu$m\textsuperscript{3}. 
    A blood smear shows normochromic, normocytic cells. 
    Which of the following is the most likely cause?
    
    (A) Acute blood loss \\
    (B) Chronic lymphocytic leukemia\\
    (C) Erythrocyte enzyme deficiency\\
    (D) Erythropoietin deficiency\\
    (E) Immunohemolysis\\
    (F) Microangiopathic hemolysis\\
    (G) Polycythemia vera \\
    (H) Sickle cell disease \\
    (I) Sideroblastic anemia \\
    (J) $\beta$-Thalassemia trait\\
    \textbf{(Answer: D)}
    \end{framed}
    \caption{USMLE board exam question example }
    \label{fig:usmle_example_q}
\end{figure}




For example, \Cref{fig:usmle_example_q} presents a classic USMLE board exam question \cite{USMLE2021}, which tests an AI model’s ability to recall factual knowledge rather than apply practical decision-making skills. The question may assess the recognition of a laboratory abnormality in diabetes, but it does not evaluate whether the model can adjust insulin regimens, recognize psychosocial factors, or determine hospitalization needs—key components of real-world patient care. As highlighted in previous research, medical licensing exams do not strongly correlate with clinical competency, reinforcing the need for benchmarks that evaluate accurate decision-making skills rather than memorization.

\begin{table}[h]
    \centering
    \begin{tabular}{llp{10cm}}
        \toprule
        \textbf{Question type} & \textbf{Attribute type} & \textbf{Example template question} \\
        \midrule
        \multirow{6}{*}{Single-Verify} 
        & SCP Code & Does this ECG show symptoms of \textbf{non-specific ST changes}? \\
        & Noise & Does this ECG show \textbf{baseline drift in lead I}? \\
        & Stage of infarction & Does this ECG show \textbf{early stage of myocardial infarction}? \\
        & Extra systole & Does this ECG show \textbf{ventricular extrasystoles}? \\
        & Heart axis & Does this ECG show \textbf{left axis deviation}? \\
        & Numeric feature & Does the \textbf{RR interval} of this ECG fall \textbf{within the normal range}? \\
        \bottomrule
    \end{tabular}
    \caption{Example template questions for different ECG attributes.}
    \label{tab:ecg_questions}
\end{table}
\begin{table}[h]
    \centering
    \begin{tabular}{lp{3.cm}p{3.5cm}p{1.5cm}p{3.5cm}}
        \toprule
        \textbf{Category} & \textbf{Task} & \textbf{Prompt} & \textbf{Result} & \textbf{AI Response} \\
        \midrule
        \multirow{2}{*}{Sequence alignment} 
        & DNA sequence alignment to human genome 
        & Align the DNA sequence to the human genome: \texttt{TGGGCTCA AGTGATCATA……} 
        & chr7 
        & As a language model AI, I do not have the capability to align a DNA sequence to the human genome…… 
        \\
        \midrule
        & DNA sequence alignment to multiple species 
        & Which organism does the DNA sequence come from: \texttt{CGTACACC ATTGGTGC……} 
        & yeast 
        & The organism from which the DNA sequence comes cannot be determined based solely on the DNA sequence…… 
         \\
        \bottomrule
    \end{tabular}
    \caption{DNA Sequence Alignment Tasks and AI Responses}
    \label{tab:sequence_alignment}
\end{table}


\Cref{tab:ecg_questions} and \Cref{tab:sequence_alignment} illustrate additional examples of widely used AI benchmarks, such as ECG-QA \cite{Oh2024} and GeneTuring \cite{Hou2023}, which focus on highly structured, fact-based medical knowledge. These datasets and others like MedQA \cite{Jin2021} have been leveraged by major AI companies, including Google’s Gemini initiative \cite{Saab2024}, to highlight model performance. While these benchmarks evaluate text-based and multimodal AI capabilities, they focus heavily on fact memorization rather than applied clinical reasoning.

Unlike traditional medical AI benchmarks, MENTAT is designed by practicing psychiatrists to reflect real-world clinical scenarios. The dataset also includes ambiguous, multi-choice decision-making tasks rather than a single correct answer, simulating the complex nature of psychiatric practice. Furthermore, MENTAT aims to reduce bias by empowering a diverse group of clinicians in its development from the start, making it less likely to reinforce harmful racial, gender, or sexuality-based biases in mental healthcare. In summary, MENTAT differs from medical exam questions by moving beyond fact recall to assess practical clinical decision-making in mental healthcare. While traditional benchmarks test AI models on medical knowledge, MENTAT evaluates whether AI can handle real-world psychiatric tasks, manage patient uncertainty, and make informed decisions in complex clinical environments.




% For years, benchmark evaluations have been utilized in medical AI to track the progress of new and updated models. However, they have largely focused on genetics, radiology, cardiology, and electronic medical record data processing\cite{Hou2023, Zambrano2023, Oh2024}. Little work has thus far been invested in the creation of benchmark evaluations and datasets for mental healthcare. Most medical evaluations of language and multi-modal models have also only focused on specialty board exams and exams intended for medical students. Both categories of exams assess knowledge but have been noted to have relatively little correlation to the real-world practice of medicine\cite{Saguil2015}. Every medical specialty would benefit from a creation of a dataset of question-answer pairs tailored specifically to clinical practice as opposed to the fact-based assessment that is common in licensing and medical student exams. As an example, Figure 1 demonstrates a classic board exam question directly from the USMLE website \cite{USMLE2021}, a medical licensing exam all medical students must take. It does not assess knowledge about pragmatic clinical management of diabetes; rather, it focuses on fact-based knowledge. Just as Saguil et al highlighted a lack of correlation between medical licensing exams and clinical skills, a model’s ability to answer Figure 1 correctly does not correlate to its ability to care for an individual with diabetes. Construction of datasets that test practical medical knowledge are necessary to robustly evaluate language models’ appropriateness in clinical settings. Furthermore, clinicians have recently called for moving beyond a medical exam benchmark, stating, "it is essential to move beyond medical exams and adopt more grounded, task-specific approaches for evaluation" \cite{Raji2025}. A related study, published in 2025, identified 11 high-level clinical tasks and created a benchmark (MedS-bench) meant to "address this gap" as current benchmarks and evaluation datasets "fail to adequately reflect the practical utility of LLMs in real-world clinical scenarios". Our work is different, but complementary, as our dataset applies this concept (testing for skills required to practice as a clinician as opposed to esoteric medical facts) to mental healthcare. In the realm of mental healthcare, researchers have expanded evaluations of LLM's into the realm of psychotherapy, which is complementary to our approach of evaluating LLM performance in the related field of psychiatry.


% As previously mentioned, most investigations into the evaluation of AI models in the healthcare setting have focused on pre-existing fact-based datasets such as the USMLE exams and specialty-specific board exams. In practice, these knowledge-based tests (e.g., USMLE Step 1) are designed to assess whether human trainees have acquired sufficient baseline knowledge to enter post-graduate training in a selected medical specialty. However, passing these tests alone is not considered sufficient for practicing as a physician in the United States. Residency training is required, during which trainees apply fundamental medical knowledge to real-world clinical cases \cite{Mowery2015}. 
% %
% Similarly, AI models must progress beyond simple recall of medical facts. They should be trained and evaluated on real-world clinical tasks that require the application of baseline medical knowledge. To maximize external validity, datasets should be created and vetted by actively practicing clinicians. Some of the most widely used benchmarks in medical AI are frequently leveraged by major companies to showcase the clinical capabilities of their fine-tuned models. For instance, in 2024, Google published *Capabilities of Gemini Models in Medicine*, incorporating several prominent medical benchmarks \cite{Saab2024}. The authors highlight the novelty of their work as “the most comprehensive benchmarking of multimodal medical models to date” based on their use of 14 different medical benchmarks \cite{Saab2024}. These benchmarks include ECG-QA \cite{Oh2024}, MedQA \cite{Jin2021}, GeneTuring \cite{Hou2023}, MMMU (health medicine) \cite{Yue2023}, NEJM Image Challenges \cite{NEJM2024}, Path-VQA \cite{He2020}, and others. Additionally, an effort was made to enhance the clinical relevance of these findings by evaluating the models’ ability to summarize complex medical information and generate referral letters for specialists \cite{Saab2024}. 
% %
% Examples of questions from these datasets are provided in Figures 3 and 4, which illustrate excerpts from ECG-QA and GeneTuring, respectively. MedQA, on the other hand, is best represented by the example in Figure 1. In the domain of mental health datasets and summarization, Adhikary et al. introduced a dataset comprising 191 counseling sessions with associated summaries \cite{Adhikary2024}. 
% %
% Despite the breadth of existing benchmarks, there is still no robust, clinician-led, and clinician-vetted mental healthcare benchmark for AI models. Current medical benchmarks remain overly narrow and fact-based, limiting their external validity and clinical relevance. As noted earlier, a high score on the MedQA benchmark does not equate to excellence in clinical care. Our dataset shifts the paradigm by moving beyond fact-based assessments (e.g., USMLE exams) and introducing a comprehensive evaluation of clinician-level decision-making in mental healthcare. 
% %
% Our benchmark uniquely assesses an AI model’s ability to **triage, diagnose, treat, and monitor mental health conditions**, establishing a new category of medical benchmarks that we hope other specialties will adopt. Furthermore, because our dataset is developed and overseen by a diverse group of practicing clinicians, it is significantly less likely to perpetuate harmful racial, gender, or sexuality-based biases in mental healthcare. It has been validated by [insert number] practicing psychiatrists.

\newpage

\section{Further Annotation Processing Results}
\label{app:annotation_details}

% \begin{figure}[ht]
%     \vskip 0.2in
%     \begin{center}
%     \centerline{\includegraphics[width=0.5\columnwidth]{figures/raw_annotation_krippendorf.pdf}}
%     \caption{Test.}
%     \label{fig:raw_annotation_krippendorf}
%     \end{center}
%     \vskip -0.2in
% \end{figure}

\begin{figure}[ht!]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/annotator_scores_hbt_pars.pdf}
        \caption{(Top) We show the average raw annotation score with with bootstrapped (95\% CL) uncertainties for each annotator. All of them deviate from 50 with statistical significance (the random baseline). 
        (Bottom) Fitted individual annotator parameters from the hierarchical Bradley-Terry model.
        Besides regularization in the log-likelihood objective, we bound the individual annotator parameters ($\gamma_a \in [-3.0, 3.0]$, $\alpha_a \in [0.5, 2.0]$) during the optimization to balance the goal of slightly de-noising the resulting preference dataset while keeping the majority of differences between individual annotator preferences.
        These bounds prevent the model from fixing contradictory data by pushing a parameter to an extreme.
        The fact that all annotators have a positive offset $\gamma_a$ indicates that they all tend to choose one answer option to prefer over all others in a single annotation of one question.}
        \label{fig:annotator_scores_hbt_pars}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/raw_annotation_krippendorf.pdf}
        \caption{
        We show the distribution of  Krippendorff's $\alpha$ for raw triage and documentation question annotations.
        We verify that the expert annotators do not converge on one answer option and that there is sufficient inter-annotator disagreement.
        Given our design choices, we expect $\alpha$ to be naturally low as our goal is not to measure the presence of a single ground truth and low $\alpha$ values ($\alpha \leq 0.5$) will not tell us how useful a set of annotations is—only that experts statistically disagree. 
        }
        \label{fig:raw_annotation_krippendorf}
    \end{minipage}
\end{figure}
% \begin{figure}[ht]
%     \vskip 0.2in
%     \begin{center}
%     \centerline{\includegraphics[width=0.5\columnwidth]{figures/annotator_scores_hbt_pars.pdf}}
%     \caption{Test.}
%     \label{fig:annotator_scores_hbt_pars}
%     \end{center}
%     \vskip -0.2in
% \end{figure}
% \begin{figure}[ht]
%     \vskip 0.2in
%     \begin{center}
%     \centerline{\includegraphics[width=0.5\columnwidth]{figures/frac_ct in topk_bt_vs_hbt.pdf}}
%     \caption{Test.}
%     \label{fig:frac_ct in topk_bt_vs_hbt}
%     \end{center}
%     \vskip -0.2in
% \end{figure}

\newpage

\section{Language Model Prompts}
\label{app:prompting}

\begin{figure}[ht]
    \centering
    \begin{minipage}[b]{0.38\textwidth}
        \begin{framed}
        \texttt{
        f"Question: \{q\}\textbackslash n\textbackslash n"\\
        f"A: \{answer\_list[0]\}\textbackslash n"\\
        f"B: \{answer\_list[1]\}\textbackslash n"\\
        f"C: \{answer\_list[2]\}\textbackslash n"\\
        f"D: \{answer\_list[3]\}\textbackslash n"\\
        f"E: \{answer\_list[4]\}\textbackslash n\textbackslash n"\\
        "Answer (single letter): "
        }
        \end{framed}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.62\textwidth}
        % \centering
        \begin{framed}
        \texttt{
        f"Question: \{q\}\textbackslash n\textbackslash n"\\
        f"A: \{answer\_list[0]\}\textbackslash n"\\
        f"B: \{answer\_list[1]\}\textbackslash n"\\
        f"C: \{answer\_list[2]\}\textbackslash n"\\
        f"D: \{answer\_list[3]\}\textbackslash n"\\
        f"E: \{answer\_list[4]\}\textbackslash n\textbackslash n"\\
        "Answer (only reply with a single letter!): "
        }
        \end{framed}
    \end{minipage}
    \caption{(Left) Prompt text MCQA variation A (as used for \textit{gpt-4o-mini-2024-07-18}, \textit{gpt-4o-2024-08-06}, \textit{o1-2024-12-17}, and \textit{o1-mini-2024-09-12}).
    (Right) Prompt text MCQA variation B (all other models).
By looking at the responses from models evaluated with variation A, we verified that the recorded accuracy difference caused by using different promtps was $\leq 1$\%.
The only exception was \textit{o1-mini-2024-09-12}, for which we corrected the evaluation.}
    \label{fig:eval_prompts_mcqa}
\end{figure}

\begin{figure}[ht]
    \vskip 0.2in
    \begin{framed}
        \texttt{
        f"Question: \{q\}\textbackslash n\textbackslash n"\\
        "Answer (write your reply in only one short sentence!): "
        }
        \end{framed}
        \caption{Prompt text free-form (as used for the models evaluated in \Cref{sec:4_4_consistency}).}
    \vskip -0.2in
\end{figure}

\newpage

\section{Annotator Interface}
\label{app:annotator_interface}

\begin{figure}[ht]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/mentat_q36_question.png}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/mentat_q36_answers.png}
    \end{minipage}
    \caption{Example of the online annotation interface using the \textit{jsPsych} library \citep{de_Leeuw2023} (MIT license). There is also a comment box below the sliders for feedback/comments, that is not shown.}
    \label{fig:mentat_q36_combined}
\end{figure}

\newpage

\section{Further Evaluation Results}
\label{app:more_experiment_results}

\begin{figure}[ht]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/final_eval_results_by_gender.pdf}
        \caption{Using the $\mathcal{D}_\text{G}$ dataset, we evaluate eleven off-the-shelf instruction-tuned and three (mental) healthcare fine-tuned models for overall accuracy and how it is impacted by different patient genders.}
        \label{fig:final_eval_results_by_gender}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/final_eval_results_by_age.pdf}
        \caption{Using the $\mathcal{D}_\text{A}$ dataset, we evaluate eleven off-the-shelf instruction-tuned and three (mental) healthcare fine-tuned models for overall accuracy and how it is impacted by different patient ages.}
        \label{fig:final_eval_results_by_age}
    \end{minipage}
\end{figure}

% \begin{figure}[ht]
%     \vskip 0.2in
%     \begin{center}
%     \centerline{\includegraphics[width=0.5\columnwidth]{figures/final_eval_results_by_gender.pdf}}
%     \caption{Test.}
%     \label{fig:final_eval_results_by_gender}
%     \end{center}
%     \vskip -0.2in
% \end{figure}
% \begin{figure}[ht]
%     \vskip 0.2in
%     \begin{center}
%     \centerline{\includegraphics[width=0.5\columnwidth]{figures/final_eval_results_by_age.pdf}}
%     \caption{Test.}
%     \label{fig:final_eval_results_by_age}
%     \end{center}
%     \vskip -0.2in
% \end{figure}

\begin{figure}[ht!]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=0.47\columnwidth]{figures/final_eval_results_by_nat.pdf}}
    \caption{Using the $\mathcal{D}_\text{N}$ dataset, we evaluate eleven off-the-shelf instruction-tuned and three (mental) healthcare fine-tuned models for overall accuracy and how it is impacted by different patient ethnicities.}
    \label{fig:final_eval_results_by_nat}
    \end{center}
    \vskip -0.2in
\end{figure}
