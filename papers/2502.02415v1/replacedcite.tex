\section{Related Work}
\label{sec:related-work}
\method builds on the concept of graph filtration and incorporates noise augmentation. It is fine-tuned via reinforcement learning to mitigate exposure bias. In the following, we provide a brief overview of related graph generative models, approaches to address exposure bias, and applications of graph filtration.

\paragraph{Graph Generation.} GraphRNN____ made the first advances towards deep generative graph models by autoregressively generating nodes and their incident edges to build up an adjacency matrix row-by-row. In a similar fashion, DeepGMG____ iteratively builds a graph node-by-node. ____ proposed a more efficient autoregressive model, GRAN, by generating multiple nodes at a time in a block-wise fashion, leveraging mixtures of multivariate Bernoulli distributions. 
GraphArm____ introduced an autoregressive model that reverses a diffusion process in which nodes and their incident edges decay to an absorbing state.
These models share the fact that they build graphs via node addition. Hence, they autoregressively generate an increasing sequence of \emph{induced subgraphs}. In comparison, the subgraphs we consider in our work do not necessarily need to be induced. Moreover, \method may generate sequences that are not monotonic. 
In contrast to autoregressive node-addition methods, approaches by ____ and____ generate graphs through edge-addition following a pre-defined edge ordering. While this strategy bears similarities to our proposed filtration method, our approach distinctly differs by allowing for edge deletion as well as addition, leading to possibly non-monotonic sequences.

Diffusion models for graphs such as EDP-GNN____ and GDSS____, based on score matching, or DiGress____, based on discrete denoising diffusion____, have emerged as powerful generators. However, they require many iterative denoising steps, making them slow during sampling. Hierarchical approaches____ and absorbing state processes____ have subsequently been proposed to allow diffusion models to be scaled to large graphs. In contrast to the noise processes in denoising diffusion models, the filtration processes we consider are in general \emph{non-Markovian}, necessitating a full autoregressive modeling.


Graph variational autoencoders____ generate all edges at one time, thereby reducing computational costs during inference. However, these methods struggle to model complicated distributions and may fail in the presence of isomorphic nodes____.
Generative adversarial networks (GANs)____ are likelihood-free and avoid the node-orderings and graph matching algorithms required in autoregressive models and VAEs. However, they can be unstable during training and may suffer from issues such as mode collapse____.

\paragraph{Exposure Bias.} Exposure bias____ refers to the train-test discrepancies autoregressive models face when they are exposed to their own predictions during inference. Errors may accumulate during sampling, leading to a distribution shift and degrading performance. To mitigate this phenomenon in natural language generation, ____ proposed a data augmentation strategy to expose models to their predictions during training. In a similar effort, ____ proposed training in free-running mode using reinforcement learning to optimize sequence-level performance metrics. SeqGAN____, which is the most relevant to our work, also trains models in free-running mode using reinforcement learning. Instead of relying on extrinsic metrics like____, it adversarially trains a discriminator to provide feedback to the generative model. GCPN____ adopts a hybrid framework for generating small molecules, combining adversarial and domain-specific rewards.

\paragraph{Graph Filtration.} %
Filtration is commonly used in the field of persistent homology____ to extract features of geometric data structures at different resolutions. Previously, graph filtration has mostly been used to construct graph kernels____ or extract graph representations that can be leveraged in downstream tasks such as classification____. While filtration has also been used for evaluating graph generative models____, \emph{to the best of our knowledge, our work presents the first model that directly leverages filtration for generation.}