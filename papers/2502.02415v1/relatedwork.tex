\section{Related Work}
\label{sec:related-work}
\method builds on the concept of graph filtration and incorporates noise augmentation. It is fine-tuned via reinforcement learning to mitigate exposure bias. In the following, we provide a brief overview of related graph generative models, approaches to address exposure bias, and applications of graph filtration.

\paragraph{Graph Generation.} GraphRNN~\citep{you2018graphrnn} made the first advances towards deep generative graph models by autoregressively generating nodes and their incident edges to build up an adjacency matrix row-by-row. In a similar fashion, DeepGMG~\citep{li2018deepgmg} iteratively builds a graph node-by-node. \citet{lia2019gran} proposed a more efficient autoregressive model, GRAN, by generating multiple nodes at a time in a block-wise fashion, leveraging mixtures of multivariate Bernoulli distributions. 
GraphArm~\citep{kong2023grapharm} introduced an autoregressive model that reverses a diffusion process in which nodes and their incident edges decay to an absorbing state.
These models share the fact that they build graphs via node addition. Hence, they autoregressively generate an increasing sequence of \emph{induced subgraphs}. In comparison, the subgraphs we consider in our work do not necessarily need to be induced. Moreover, \method may generate sequences that are not monotonic. 
In contrast to autoregressive node-addition methods, approaches by \citet{goyal2020graphgen} and~\citet{bacciu2020edgebased} generate graphs through edge-addition following a pre-defined edge ordering. While this strategy bears similarities to our proposed filtration method, our approach distinctly differs by allowing for edge deletion as well as addition, leading to possibly non-monotonic sequences.

Diffusion models for graphs such as EDP-GNN~\citep{niu2020edpgnn} and GDSS~\citep{jo2022gdss}, based on score matching, or DiGress~\citep{vignac2023digress}, based on discrete denoising diffusion~\citep{austin2021d3pm}, have emerged as powerful generators. However, they require many iterative denoising steps, making them slow during sampling. Hierarchical approaches~\citep{bergmeister2024efficientscalable} and absorbing state processes~\citep{chen2023edge} have subsequently been proposed to allow diffusion models to be scaled to large graphs. In contrast to the noise processes in denoising diffusion models, the filtration processes we consider are in general \emph{non-Markovian}, necessitating a full autoregressive modeling.


Graph variational autoencoders~\citep{kipf2016graphvae,simonovsky2018graphvae} generate all edges at one time, thereby reducing computational costs during inference. However, these methods struggle to model complicated distributions and may fail in the presence of isomorphic nodes~\citep{zhang2021labeling}.
Generative adversarial networks (GANs)~\citep{bojchevski2018netgan,cao2018molgan,martinkus2022spectre} are likelihood-free and avoid the node-orderings and graph matching algorithms required in autoregressive models and VAEs. However, they can be unstable during training and may suffer from issues such as mode collapse~\citep{martinkus2022spectre}.

\paragraph{Exposure Bias.} Exposure bias~\citep{bengio2015exposure_bias,ranzato2016sequencelevel} refers to the train-test discrepancies autoregressive models face when they are exposed to their own predictions during inference. Errors may accumulate during sampling, leading to a distribution shift and degrading performance. To mitigate this phenomenon in natural language generation, \citet{bengio2015exposure_bias} proposed a data augmentation strategy to expose models to their predictions during training. In a similar effort, \citet{ranzato2016sequencelevel} proposed training in free-running mode using reinforcement learning to optimize sequence-level performance metrics. SeqGAN~\citep{yu2017seqgan}, which is the most relevant to our work, also trains models in free-running mode using reinforcement learning. Instead of relying on extrinsic metrics like~\citet{ranzato2016sequencelevel}, it adversarially trains a discriminator to provide feedback to the generative model. GCPN~\citep{you2018gcpn} adopts a hybrid framework for generating small molecules, combining adversarial and domain-specific rewards.

\paragraph{Graph Filtration.} %
Filtration is commonly used in the field of persistent homology~\citep{edelsbrunner2002filtration} to extract features of geometric data structures at different resolutions. Previously, graph filtration has mostly been used to construct graph kernels~\citep{zhao2019persistenceclassification,schulz2022filtrationkernel} or extract graph representations that can be leveraged in downstream tasks such as classification~\citep{obray2021filtrationcurves}. While filtration has also been used for evaluating graph generative models~\citep{southern2023ggevaluation}, \emph{to the best of our knowledge, our work presents the first model that directly leverages filtration for generation.}