
\documentclass{article}

\input{math_commands.tex}
\input{macros}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %



\usepackage{hyperref}




\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Towards Fast Graph Generation via Autoregressive Noisy Filtration Modeling}

\input{extrapackages}

\renewcommand \thepart{}
\renewcommand \partname{}

\begin{document}
\doparttoc %
\faketableofcontents %



\twocolumn[
\icmltitle{Towards Fast Graph Generation via Autoregressive Noisy Filtration Modeling}




\begin{icmlauthorlist}
\icmlauthor{Markus Krimmel}{mlsb}
\icmlauthor{Jenna Wiens}{umich}
\icmlauthor{Karsten Borgwardt}{mlsb}
\icmlauthor{Dexiong Chen}{mlsb}
\end{icmlauthorlist}

\icmlaffiliation{mlsb}{Max Planck Institute of Biochemistry
82152 Martinsried, Germany}
\icmlaffiliation{umich}{University of Michigan, Ann Arbor, MI, USA}
\icmlcorrespondingauthor{Markus Krimmel}{krimmel@biochem.mpg.de}
\icmlcorrespondingauthor{Dexiong Chen}{dchen@biochem.mpg.de}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{} %

\begin{abstract}
Graph generative models often face a critical trade-off between learning complex distributions and achieving fast generation speed. We introduce Autoregressive Noisy Filtration Modeling (\method), a novel approach that addresses both challenges. \method leverages filtration, a concept from topological data analysis, to transform graphs into short sequences of monotonically increasing subgraphs. This formulation extends the sequence families used in previous autoregressive models. To learn from these sequences, we propose a novel autoregressive graph mixer model. Our experiments suggest that exposure bias might represent a substantial hurdle in autoregressive graph generation and we introduce two mitigation strategies to address it: noise augmentation and a reinforcement learning approach. Incorporating these techniques leads to substantial performance gains, making \method competitive with state-of-the-art diffusion models across diverse synthetic and real-world datasets. Notably, \method produces remarkably short sequences, achieving a 100-fold speedup in generation time compared to diffusion models. This work marks a significant step toward high-throughput graph generation. %
\end{abstract}




\section{Introduction}

Graphs are fundamental structures that model relationships in various domains, from social networks and molecular structures to transportation systems and neural networks. The ability to generate realistic and diverse graphs is crucial in many applications, such as drug discovery~\citep{liu2018moleculevae,vignac2023digress}, network simulation~\citep{yu2019traffic}, and protein design~\citep{ingraham2019proteingraphgen}. 
The space of drug-like molecules and protein conformations is, for practical purposes, infinite, limiting the effectiveness of in-silico screening of existing libraries~\citep{polishchuk2013chemicalspace,Levinthal1969HowTF}.
Consequently, high-throughput graph generation---the task of efficiently creating new graphs that faithfully emulate properties similar to those observed in a given domain---has thus emerged as a critical challenge in machine learning and generative artificial intelligence~\citep{gangwal2024generative,grisoni2021combining}.

Recent deep learning-based approaches, particularly autoregressive~\citep{you2018graphrnn,lia2019gran,kong2023grapharm} and diffusion models~\citep{vignac2023digress,bergmeister2024efficientscalable}, have shown promise in generating increasingly realistic graphs.
However, many current diffusion-based approaches rely on iterative refinement processes involving a large number of steps. This computational burden hinders their potential for high-throughput  applications~\citep{Gentile2022virtualscreening,gomesbobarelli2016ledscreening,polishchuk2013chemicalspace}. While autoregressive models are more efficient during inference, they have underperformed in terms of generation quality. Moreover, they might be susceptible to exposure bias~\citep{ranzato2016sequencelevel}, where performance deteriorates as errors accumulate during sampling and a train-test discrepancy consequently arises.

Recent work has explored the use of topological data analysis, particularly persistent homology and filtration~\citep{edelsbrunner2002filtration, zomordian2005persistenthom}, for graph representation. A filtration provides a multi-scale view of a graph structure by constructing a nested sequence of subgraphs.
This approach has shown potential in various graph analysis tasks, including classification and similarity measurement~\citep{obray2021filtrationcurves,schulz2022filtrationkernel}. In the context of generative modeling, filtration-based representations have been used to develop more expressive tools for generative model evaluation~\citep{southern2023ggevaluation}. However, the application of filtration-based methods for graph generation remains unexplored. In this work, we propose filtrations as a generalization of graph sequence families used in prior autoregressive models~\citep{you2018graphrnn, lia2019gran}, offering a flexible framework to construct sequences for generation. Nonetheless, modeling filtration sequences in a naive manner remains prone to exposure bias.

To address this, we introduce Autoregressive Noisy Filtration Modeling (\method), a novel approach to fast graph generation that models noise-augmented filtration sequences autoregressively. 
To generate a target graph, our method produces a short sequence of increasingly dense and detailed intermediate graphs, which interpolate the target graph and the fully disconnected graph. Compared to diffusion models~\citep{vignac2023digress,bergmeister2024efficientscalable}, \method requires fewer iterations during sampling, resulting in significantly faster inference speed. 
By adding noise to the filtration sequences, \method learns to simultaneously remove or add edges. As a result, it is able to recover from errors during sampling. Additionally, we further mitigate exposure bias with adversarial fine-tuning using reinforcement learning (RL). Our method offers a promising balance between efficiency and accuracy in graph generation, providing a 100-fold speedup over diffusion-based approaches, while substantially outperforming existing autoregressive models in terms of generation quality.%

In summary, our contributions are as follows:
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item We propose a novel autoregressive graph generation framework that leverages graph filtration. Our formulation generalizes the graph sequences used by previous autoregressive models that operate via node addition.
    \item We introduce a specialized autoregressive model architecture designed to operate on these graph sequences.
    \item We identify exposure bias as a potential challenge in autoregressive graph generation and propose noise augmentation and adversarial fine-tuning as effective strategies to mitigate this issue.
    \item We conduct ablation studies to evaluate the impact of different components within our framework, %
    demonstrating that noise augmentation and adversarial fine-tuning substantially improve performance.  %
    \item Our empirical results highlight the strong performance and efficiency of our model compared to recent baselines. Notably, our model achieves inference speed 100 times faster than existing diffusion-based models.
\end{itemize}

\section{Related Work}
\label{sec:related-work}
\method builds on the concept of graph filtration and incorporates noise augmentation. It is fine-tuned via reinforcement learning to mitigate exposure bias. In the following, we provide a brief overview of related graph generative models, approaches to address exposure bias, and applications of graph filtration.

\paragraph{Graph Generation.} GraphRNN~\citep{you2018graphrnn} made the first advances towards deep generative graph models by autoregressively generating nodes and their incident edges to build up an adjacency matrix row-by-row. In a similar fashion, DeepGMG~\citep{li2018deepgmg} iteratively builds a graph node-by-node. \citet{lia2019gran} proposed a more efficient autoregressive model, GRAN, by generating multiple nodes at a time in a block-wise fashion, leveraging mixtures of multivariate Bernoulli distributions. 
GraphArm~\citep{kong2023grapharm} introduced an autoregressive model that reverses a diffusion process in which nodes and their incident edges decay to an absorbing state.
These models share the fact that they build graphs via node addition. Hence, they autoregressively generate an increasing sequence of \emph{induced subgraphs}. In comparison, the subgraphs we consider in our work do not necessarily need to be induced. Moreover, \method may generate sequences that are not monotonic. 
In contrast to autoregressive node-addition methods, approaches by \citet{goyal2020graphgen} and~\citet{bacciu2020edgebased} generate graphs through edge-addition following a pre-defined edge ordering. While this strategy bears similarities to our proposed filtration method, our approach distinctly differs by allowing for edge deletion as well as addition, leading to possibly non-monotonic sequences.

Diffusion models for graphs such as EDP-GNN~\citep{niu2020edpgnn} and GDSS~\citep{jo2022gdss}, based on score matching, or DiGress~\citep{vignac2023digress}, based on discrete denoising diffusion~\citep{austin2021d3pm}, have emerged as powerful generators. However, they require many iterative denoising steps, making them slow during sampling. Hierarchical approaches~\citep{bergmeister2024efficientscalable} and absorbing state processes~\citep{chen2023edge} have subsequently been proposed to allow diffusion models to be scaled to large graphs. In contrast to the noise processes in denoising diffusion models, the filtration processes we consider are in general \emph{non-Markovian}, necessitating a full autoregressive modeling.


Graph variational autoencoders~\citep{kipf2016graphvae,simonovsky2018graphvae} generate all edges at one time, thereby reducing computational costs during inference. However, these methods struggle to model complicated distributions and may fail in the presence of isomorphic nodes~\citep{zhang2021labeling}.
Generative adversarial networks (GANs)~\citep{bojchevski2018netgan,cao2018molgan,martinkus2022spectre} are likelihood-free and avoid the node-orderings and graph matching algorithms required in autoregressive models and VAEs. However, they can be unstable during training and may suffer from issues such as mode collapse~\citep{martinkus2022spectre}.

\paragraph{Exposure Bias.} Exposure bias~\citep{bengio2015exposure_bias,ranzato2016sequencelevel} refers to the train-test discrepancies autoregressive models face when they are exposed to their own predictions during inference. Errors may accumulate during sampling, leading to a distribution shift and degrading performance. To mitigate this phenomenon in natural language generation, \citet{bengio2015exposure_bias} proposed a data augmentation strategy to expose models to their predictions during training. In a similar effort, \citet{ranzato2016sequencelevel} proposed training in free-running mode using reinforcement learning to optimize sequence-level performance metrics. SeqGAN~\citep{yu2017seqgan}, which is the most relevant to our work, also trains models in free-running mode using reinforcement learning. Instead of relying on extrinsic metrics like~\citet{ranzato2016sequencelevel}, it adversarially trains a discriminator to provide feedback to the generative model. GCPN~\citep{you2018gcpn} adopts a hybrid framework for generating small molecules, combining adversarial and domain-specific rewards.

\paragraph{Graph Filtration.} %
Filtration is commonly used in the field of persistent homology~\citep{edelsbrunner2002filtration} to extract features of geometric data structures at different resolutions. Previously, graph filtration has mostly been used to construct graph kernels~\citep{zhao2019persistenceclassification,schulz2022filtrationkernel} or extract graph representations that can be leveraged in downstream tasks such as classification~\citep{obray2021filtrationcurves}. While filtration has also been used for evaluating graph generative models~\citep{southern2023ggevaluation}, \emph{to the best of our knowledge, our work presents the first model that directly leverages filtration for generation.}



\section{Background}
In the following, we consider unlabeled and undirected graphs, denoted by $G=(V, E)$, where $V$ is the set of vertices and $E \subseteq V\times V$ is the set of edges. Without loss of generality, we assume $V=\{1,2,\dots,n\}$ and denote by $e_{ij}$ the edge between nodes $i,j\in V$. We assume that only connected graphs are presented to our model during training and filter training sets if necessary. Our approach is fundamentally based on the concept of graph filtration.

\paragraph{Graph Filtration.} A filtration of a graph $G$ is defined as a nested sequence of subgraphs:
\begin{equation}\label{eq:filtration}
    G = G_T \supseteq G_{T-1} \supseteq \dots \supseteq G_{1} \supseteq G_0 = (V, \emptyset)
\end{equation}
where each $G_t = (V, E_t)$ is a graph sharing the same node set as $G_T:=G$. The filtration satisfies the following properties: (1) $E_t \subseteq E_{t'}$ for all $t < t'$ and (2) $G_0$ is the fully disconnected graph, \ie $E_0 = \emptyset$. The hyper-parameter $T$ controls the length of the sequences, which is selected to be typically small in our experiments ($T\leq 32$).

\paragraph{Filtration Function and Schedule.} A convenient method to define a filtration of $G$ involves specifying two key components~\citep{obray2021filtrationcurves}: a \emph{filtration function} defined on the edge set $f: E \to \R$ and a non-decreasing sequence of scalars $(a_0, a_1,\dots,a_T)$ with $-\infty=a_0 \leq a_1 \leq \dots \leq a_{T-1} \leq a_{T}=+\infty$. Given these components, we can define the edge sets $E_t$ as nested sub-levels of the function $f$ for $t=1,\dots,T-1.$:
\begin{equation}
    E_t := f^{-1}((-\infty, a_t]) = \{e \in E \::\: f(e) \leq a_t\}   %
\end{equation}
The sequence $(a_t)_{t=0}^{T}$ is referred to as the \emph{filtration schedule sequence}. The choice of the filtration function and the schedule sequence plays a crucial role in effective graph generation. We present a visual example of the filtration process in Figure~\ref{subgfig:a-filtration-sequence}.

\section{Autoregressive Noisy Filtration Modeling}
In this section, we present the Autoregressive Noisy Filtration Modeling (\method) approach for graph generation. Given a node set $V$, our objective is to generate a sequence of increasingly dense graphs $\tilde{G}_0, \tilde{G}_1, \dots, \tilde{G}_T$ on $V$. The final graph $\tilde{G}_T$ should plausibly represent a sample from the target data distribution. 
To achieve this goal, \method will be trained to reverse a noise augmented filtration process.

This section is organized as follows: We present two filtration strategies in Sec.~\ref{subsec:filtration-strategies}. To mitigate exposure bias, we propose a noise-augmentation of the resulting graph sequences in Sec.~\ref{subsec:noise-augmentation-of-filtrations}. We then introduce in Sec.~\ref{subsec:generative-model} our autoregressive model that reverses this noisy filtration process. Finally, in Sec.~\ref{subsec:training-algo}, we propose a two-staged training scheme for \method, introducing an adversarial fine-tuning stage to further address exposure bias. 

\subsection{Filtration Strategies}
\label{subsec:filtration-strategies}
In the following, we discuss two primary strategies for the filtration function and schedule. Alternative choices are investigated in Appendix~\ref{appendix:additional-ablations}.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[c]{0.73\textwidth}
        \includegraphics[width=\textwidth,trim={0cm 0.41cm 0cm 0.41cm},clip]{figures/filtration.drawio.pdf}
        \subcaption{A filtration sequence}
        \label{subgfig:a-filtration-sequence}
    \end{subfigure} \\
    \begin{subfigure}[b]{0.42\textwidth}
        \includegraphics[width=\textwidth,trim={2cm 0.25cm 2cm 0.25cm},clip]{figures/teacher-forcing-small.drawio.pdf}
        \subcaption{Training stage I (teacher-forcing)}
        \label{subfig:teacher-forcing-training}
    \end{subfigure} \hfill
    \begin{subfigure}[b]{0.42\textwidth}
        \includegraphics[width=\textwidth,trim={0.95cm 0.35cm 1cm 0.25cm},clip]{figures/seqgan-small.drawio.pdf}
        \subcaption{Training stage II (adversarial fine-tuning)}
        \label{subfig:adversarial-fine-tuning}
    \end{subfigure}
    \caption{Top: A graph is transformed into a sequence of subgraphs (filtration) via edge-deletion. 
    Bottom left: the generator is trained via teacher-forcing to reverse the filtration process. Bottom right: the generator is fine-tuned in free-running mode via reinforcement learning on a reward signal output by a discriminator in a SeqGAN-like framework (c.f. Appendix~\ref{appendix:adversarial-finetuning}).}
    \label{fig:full-pipeline}
    \vskip-.2in
\end{figure*}

\paragraph{Filtrations from Node Orderings.} 
Many existing autoregressive models operate via node addition and thereby model sequences of nested induced subgraphs~\citep{you2018graphrnn,li2018deepgmg,lia2019gran,kong2023grapharm}. We show that similar sequences may be obtained via filtration. In this sense, the filtration framework generalizes the sequences considered by these previous works. Given a graph $G$, let $g: V \to \{1, \dots, n\}$ be a node ordering, \ie a bijection. Models that operate via node addition generate a graph sequence
\begin{equation}
    (\emptyset, \emptyset) = G[V_0] \subseteq \dots \subseteq G[V_{T}] = G,
\end{equation}
where $V_0, \dots, V_T$ are monotonically increasing sub-levels of $g$ with $V_t = g^{-1}\left([(-\infty, a_t]\right)$ for some scalars $-\infty = a_0 \leq \dots \leq a_T = +\infty$, and $G[V_t]$ denotes the induced subgraph whose node set is $V_t$.
Now we consider the following filtration function $f: E \to \R$:
\begin{equation}
    f(\{u, v\}) := \max \{g(u), g(v)\} \qquad \forall \{u, v\} \in E.
\end{equation}
It is not hard to verify that this filtration function combined with the filtration schedule $\left(a_t\right)_{t=0}^T$, yields a filtration sequence $(G_t)_{t=0}^T$ in which the edge set of $G_t$ coincides with the edge set of $G[V_t]$ for any $t=0,\dots, T$. Hence, this filtration closely mirrors the sequence of induced subgraphs, differing only in that the node set does not change over time. While a filtration function $f$ may be derived from any node ordering, we focus on depth-first search (DFS) orderings in this work, as shown to be optimal among several orderings for autoregressive graph generation in GRAN~\citep{lia2019gran}. For filtrations derived from DFS orderings, we choose the filtration schedule $a_t$ to linearly increase from a minimum edge weight at $t=1$ ($a_1=2$) to a maximum edge weight at $t=T$ ($a_T=|V|$). Unlike in previous autoregressive models~\citep{you2018graphrnn, lia2019gran, kong2023grapharm}, $T$ is fixed across all graphs.

\paragraph{Line Fiedler Filtrations.}
In contrast to the family of graph sequences leveraged by node-addition approaches, the filtration framework is not limited to sequences of induced subgraphs. We present the \emph{line Fiedler} filtration function, which is one particular choice that, in general, yields a sequence of non-induced subgraphs. This function is derived from the second smallest eigenvector (Fiedler vector) of the symmetrically normalized Laplacian of the line graph $\mathrm{L}(G)$. In $\mathrm{L}(G)$, nodes represent edges in the original graph $G$, and two nodes are connected if their corresponding edges in $G$ share a common vertex. The line Fiedler filtration function captures global structural information and tends to assign similar values to neighboring edges. For $0 \leq t \leq T$, we propose to define the filtration schedule $a_t$ as:
\begin{equation}
        a_t := \inf \left\{a \in \R \::\: \frac{|f^{-1}((-\infty, a_t])|}{|E|} \geq  \gamma(t / T) \right\},  %
\end{equation}
where $f$ is the line Fielder filtration function, and $\gamma: [0, 1] \to [0, 1]$ is a monotonic function governing the rate at which edges are added in the filtration sequence. We choose $\gamma(t) := t$, leading to an approximately linear increase in the number of edges throughout the graph sequence. We investigate other choices of $\gamma$ in Appendix~\ref{appendix:additional-ablations}.

\subsection{Noise Augmentation of Filtrations}
\label{subsec:noise-augmentation-of-filtrations}
Our goal is to autoregressively generate sequences of graphs that approximately reverse the filtration processes above. To mitigate exposure bias in autoregressive modeling,
previous works have proposed data augmentation schemes to make models more robust to the distribution shift occurring during inference~\citep{bengio2015exposure_bias}.
We propose a simpler yet effective strategy: namely, randomly perturbing intermediate graphs in a filtration sequence $G_0, \dots, G_T$ during the first training stage to expose the model to erroneous transitions. For each intermediate graph $G_t$ with $0 < t < T$, we generate a perturbed graph $\tilde{G}_t$ with edge set $\tilde{E}_t$ by including each possible edge $e$ independently with probability 
\begin{equation}
    \mathbb{P}[e \in \tilde E_t] := \left\{\begin{matrix}\begin{aligned}(1 - \lambda_t) + \lambda_t \rho_t& \quad \mathrm{if}\quad e \in E_t \\ \lambda_t \rho_t&\quad \mathrm{else}\end{aligned}\end{matrix}\right\},
\end{equation}
where $\lambda_t\in [0,1]$ controls stochasticity and $\rho_t:=\nicefrac{|E_t|}{\binom{|V|}{2}}$ is the density of $G_t$. In practice, we decrease $\lambda_t$ linearly as $t$ increases and include multiple perturbations of each filtration sequence in the training set. The choice of hyper-parameters, such as $\lambda_t$ and the number of included perturbations, is detailed in Appendix~\ref{appendix:hyper-parameters}. We note that this augmentation yields possibly non-monotonic noisy filtration sequences $(\tilde{G_t})_{t=0}^T$ during training. Hence, our autoregressive model is trained to allow for edge deletions.


\subsection{Autoregressive Modeling of Graph Sequences} 
\label{subsec:generative-model}
The generative model will be trained to reverse the noise-augmented filtration process detailed above.
We formulate the generative process using an autoregressive model, expressing the joint likelihood as follows:
\begin{equation}
    p_\theta(\tilde{G}_T, \dots, \tilde{G}_0) = p(\tilde{G}_0)\prod_{t=1}^{T}p_\theta(\tilde{G}_{t} | \tilde{G}_{t-1}, \dots, \tilde{G}_0),
\end{equation}
where $p(\tilde{G}_0)$ represents the distribution over initial graphs, defined as a point mass on the fully disconnected graph $(V,\emptyset)$.
In the following, we will detail our implementation of the autoregressive model $p_\theta$, including the architecture and training procedure.
While existing autoregressive models typically utilize RNNs~\citep{you2018graphrnn,lia2019gran,goyal2020graphgen,bacciu2020edgebased} or a first-order autoregressive structure~\citep{kong2023grapharm}, our model architecture for implementing $p_\theta$ is a novel and efficient design inspired by MLP-Mixers~\citep{tolstikhin2021mlpmixer}. 

\paragraph{Backbone Architecture.}The graph sequences we consider can be viewed as dynamic graphs with constant node sets but evolving edge sets.
Our backbone architecture operates on this structure by alternating between two types of information processing layers. The first type, called structural mixing, consists of a GNN that processes graph structures $\smash{\tilde{G}_0,\shortellipsis, \tilde{G}_{T-1}}$ independently, with weights shared across time steps. The second type, called temporal mixing, consists of a transformer decoder (TRDecoder) that processes node representations along the temporal axis, with weights shared across nodes. Our model inherits the causal structure of the transformer decoder, ensuring that node representations at timestep $t$ only depend on the graphs $\smash{\tilde{G}_0, \shortellipsis, \tilde{G}_t}$.
Formally, given input node representations $\smash{v_{i}^{(t)} \in \R^D}$ for nodes $i\in V$ and time steps $t\in[T-1]$, a single mixing operation in our backbone model produces new representations $\smash{u_{i}^{(t)}}$ and is defined as:
\begin{equation*}\label{eq:mixing-operations}
\small
\begin{aligned}
     \left(w_i^{(t)}\right)_{i=1}^{|V|} &:= \operatorname{GNN}_\theta\left(\left(v_i^{(t)}\right)_{i=1}^{|V|},\, \tilde{E}_t,\, t\right) \forall \:t=0,\shortellipsis,T - 1, \\
    \left(u_i^{(t)}\right)_{t=1}^{T - 1} &:= \operatorname{TRDecoder}_\theta\left(\left(w_i^{(t)}\right)_{t=1}^{T - 1}\right) \forall \:i=1,\shortellipsis,|V|,
\end{aligned}
\normalsize
\end{equation*}
where the first equation defines a structural mixing operation and the second equation defines a temporal mixing operation.
For the structural mixing, we use Structure-Aware-Transformer layers~\citep{chen2022sat}. Additionally, we incorporate both the timestep $t$ and cycle counts in $\tilde{G}_t$ using FiLM~\citep{perez2018film}. These structural features were used previously in other graph generative models such as DiGress~\citep{vignac2023digress}. Multiple mixing operations are stacked to form the backbone model. 

\paragraph{Edge Decoder.} 
To model $p_\theta(\tilde{G}_t|\tilde{G}_{t-1},\dots,\tilde{G}_0)$, we produce a distribution over possible edge sets of $\tilde{G}_t$. We use a mixture of multivariate Bernoulli distributions to capture dependencies between edges, similar to previous works~\citep{lia2019gran,kong2023grapharm}. 
Given $K \geq 1$ mixture components, we infer $K$ Bernoulli parameters for each node pair $i, j \in V$ from the node representations $v_i$ produced by the backbone model at timestep $t-1$: 
\begin{equation}
    p_k^{(i, j)} := D_{k, \theta}(v_i, v_j) \in [0, 1],%
\end{equation}
where $k=1,\shortellipsis, K$ and $D_{\cdot, \theta}$ is some neural network. We enforce that $\smash{p_k^{(i, j)}}$ is symmetric and that the probability of self-loops is zero. In addition, we produce a mixture distribution $\pi \in \R^K$ in the $K-1$ dimensional probability simplex from pooled node representations. 
The architectural details of $D_{\cdot, \theta}$ are provided in Appendix~\ref{appendix:edge-decoder}. The final likelihood is defined as:
\begin{equation*}
\small
    p_\theta(\tilde{E}_t | \tilde{G}_{t-1},\shortellipsis,\tilde{G}_0):=\sum_{k=1}^K \pi_k \prod_{i < j}\left\{\begin{matrix}\begin{aligned}
p_k^{(i, j)}& \; \mathrm{if} \; e_{ij} \in \tilde{E}_t \\
1 - p_k^{(i, j)}& \; \mathrm{else}
\end{aligned}
\end{matrix}
\right\}
\label{eq:edge-likelihood}
\normalsize
\end{equation*}
In contrast to existing autoregressive graph generators~\citep{you2018graphrnn,lia2019gran,goyal2020graphgen,bacciu2020edgebased,kong2023grapharm}, \emph{our model introduces a key innovation: the ability to generate non-monotonic graph sequences.} This means it can both add and delete edges. 
We argue that this capability is crucial for mitigating error accumulation during sampling (i.e., exposure bias). 
Consider, for instance, the task of generating tree structures. If a cycle is inadvertently introduced into an intermediate graph $\tilde{G}_t$ (where $t < T$), traditional autoregressive approaches would be unable to rectify this error. Our model, however, can potentially delete the appropriate edges in subsequent timesteps, thus recovering from such mistakes.
The noise augmentation approach from Sec.~\ref{subsec:noise-augmentation-of-filtrations} exposes \method to such erroneous transitions during training. We show empirically in Sec.~\ref{subsec:ablations} that this augmentation substantially improves performance.

\paragraph{Input Node Representations.} The initialization of node representations is a crucial step preceding the forward pass through the mixer architecture above. We compute initial node representations from positional and structural features in a similar fashion as~\citet{vignac2023digress}. Moreover, we add learned positional embeddings based on a node ordering derived from the filtration function. We refer to Appendix~\ref{appendix:node-individualization} for further details. 



\paragraph{Asymptotic Complexity.} We provide a detailed analysis of the asymptotic runtime complexity of our method in Appendix~\ref{appendix:complexity-analysis}. Asymptotically, \method's complexity of sampling a graph with $N$ nodes is $\mathcal{O}(T^2N + TN^3)$, where we recall that $T$ denotes the number of filtration steps. Although cubic in the number of nodes, we found that the efficiency of \method is largely driven by our ability to use a small $T$ ($T \leq 32$), while diffusion-based models generally require a much larger number of iterations. 

\subsection{Training Algorithm}
\label{subsec:training-algo}

\paragraph{Teacher-Forcing.} We employ teacher-forcing~\citep{williams1989learning} to train our generative model $p_\theta$ in a first training stage. We illustrate this training scheme in Figure~\ref{subfig:teacher-forcing-training}. Teacher-forcing allows the model to learn from complete sequences of graph evolution, providing a good initialization for subsequent reinforcement learning-based fine-tuning. Given a dataset of graphs $\mathcal{D}$, we convert it into a dataset of noisy filtration sequences, denoted as $\tilde{\mathcal{D}}$. Our objective is to maximize the log-likelihood of these sequences under our model:
\begin{equation}
    \mathcal{L}(\theta) := \mathbb{E}_{(\tilde{G}_0,\dots,\tilde{G}_T)\sim \tilde{\mathcal{D}}}\left[\log p_\theta(\tilde{G}_0, \dots, \tilde{ G}_T)\right].
    \label{eq:autoregressive-log-likelihood}
\end{equation}
In practice, this objective is implemented as a cross-entropy loss.
While the noise augmentation introduced in Sec.~\ref{subsec:noise-augmentation-of-filtrations} improves the overall quality of generated graphs after teacher-forcing training, it still falls short in generating graphs with high structural validity. To further mitigate exposure bias, we propose an RL-based fine-tuning stage to refine the model trained with teacher-forcing.

\paragraph{Adversarial Fine-tuning with RL.} Adapting the SeqGAN framework~\citep{yu2017seqgan}, we implement a generator-discriminator architecture where the generator (our mixer model) operates in inference mode as a stochastic policy and is thereby exposed to its own predictions during training. The discriminator is a graph transformer, namely GraphGPS~\citep{rampasek2022graphgps}. During training, the generator produces graph samples, which the discriminator evaluates for plausibility. The generator is updated using Proximal Policy Optimization (PPO)~\citep{schulman2018ppo} based on the discriminator's feedback, while the discriminator is trained adversarially to distinguish between generated and training set graphs. This training scheme is illustrated in Figure~\ref{subfig:adversarial-fine-tuning}. It is worth noting that only the final generated graph is presented to the discriminator. Therefore, the generator is trained to maximize a terminal reward without constraints on intermediate graphs. We provide pseudo-code in Appendix~\ref{appendix:adversarial-finetuning}.


\section{Experiments}
\label{sec:experiments}
We empirically evaluate our method on synthetic and real-world datasets. We investigate the filtration strategy based on depth-first search node orderings (DFS) and the line Fiedler function (Fdl.). In Sec.~\ref{subsec:small-synthetic}, we first present results on the commonly used small benchmark datasets~\citep{martinkus2022spectre}, comparing our method to a variety of baselines. We then demonstrate in Sec.~\ref{subsec:large-synthetic} that we can improve upon these results by using a more realistic setting with more training examples. Additionally, we present results for inference efficiency. Finally, in Sec.~\ref{subsec:real-world-exp}, we demonstrate that our model is applicable to real-world data, namely larger protein graphs~\citep{dobson2003proteins} and drug-like molecules~\citep{brown2019guacamol}. In Sec.~\ref{subsec:ablations}, we present ablation studies demonstrating the efficacy of noise augmentation and adversarial fine-tuning.

\paragraph{Evaluation.}
We follow established practices from previous works~\citep{you2018graphrnn,martinkus2022spectre,vignac2023digress} in our evaluation. We compare a set of model-generated samples to a test set via maximum mean discrepancy (MMD)~\citep{gretton2012mmd}, based on various graph descriptors. These descriptors include histograms of node degrees (Deg.), clustering coefficients (Clus.), orbit count statistics (Orbit), and eigenvalues (Spec.).

In previous works~\citep{martinkus2022spectre,vignac2023digress}, very few samples are generated for the evaluation of graph generative models. In Appendix~\ref{appendix:variance-and-bias}, we show theoretically and empirically that this leads to high bias and variance in the reported metrics. In Sec.~\ref{subsec:large-synthetic} and~\ref{subsec:real-world-exp}, we generate 1024 samples for evaluation to mitigate this, while we generate 40 samples in Sec.~\ref{subsec:small-synthetic} to fairly compare to previous methods. For synthetic datasets, we follow previous works by reporting the ratio of generated samples that are valid, unique, and novel (VUN). In Sec.~\ref{subsec:large-synthetic} and~\ref{subsec:real-world-exp}, we report inference speed, measured as the time needed to generate 1024 graphs on an H100 GPU, normalized to a per-graph cost. 

\paragraph{Baselines.} 
We aim to demonstrate that our method is competitive with state-of-the-art diffusion models in terms of sample quality while outperforming them in terms of inference speed. Hence, we compare our method to two recent diffusion models, namely DiGress~\citep{vignac2023digress} and ESGG~\citep{bergmeister2024efficientscalable}. DiGress first introduced discrete diffusion to the area of graph generation and remains one of the most robust baselines. ESGG is acutely relevant to our work, as it aims to improve inference speed. In addition, we also present results on an autoregressive model, GRAN~\citep{lia2019gran}, which focuses on efficiency during inference. For details on model selection and hyper-parameters for these baselines, we refer to \cref{appendix:esgg-selection,appendix:gran-hyperparameters,appendix:digress-hyperparameters,appendix:esgg-hyperparameters,appendix:gran-model-selection}. In Sec.~\ref{subsec:small-synthetic}, we report baseline results from the literature, also comparing to the hierarchical HiGen~\citep{karami2024higen} approach, the scalable EDGE~\citep{chen2023edge} diffusion model, the autoregressive GraphRNN model~\citep{you2018graphrnn}, and the GAN-based SPECTRE model~\citep{martinkus2022spectre}.


\subsection{Experiments with Small Synthetic Datasets}
\label{subsec:small-synthetic}
\begin{table}[htp]
    \small
    \centering
    \caption{Performance of models on small synthetic SPECTRE datasets. Results on GraphRNN, GRAN and SPECTRE taken from~\citet{martinkus2022spectre}. Results on DiGress, ESGG and EDGE from~\citet{bergmeister2024efficientscalable}.}
    \input{tables/table-spectre-datasets}
    \label{tab:small-spectre-datasets}
\end{table}
As a first demonstration of our method, we present results on the planar and SBM datasets by~\citet{martinkus2022spectre}. Since the training set consists of only 128 graphs, we find that \method models using the line Fiedler function tend to overfit during the teacher-forcing training stage. To mitigate this issue, we introduce some small stochastic perturbations to node orderings used for initializing node representations. We discuss this in more detail in Appendix~\ref{appendix:node-individualization}. Model selection is performed based on the minimal validation loss. 
Table~\ref{tab:small-spectre-datasets} illustrates that, in terms of VUN on the planar graph dataset, our models outperform GraphRNN~\citep{you2018graphrnn}, GRAN~\citep{lia2019gran}, EDGE~\cite{chen2023edge}, and SPECTRE~\citep{martinkus2022spectre}. They perform slightly worse than DiGress~\citep{vignac2023digress} and ESGG~\citep{bergmeister2024efficientscalable}. While the line Fiedler variant outperforms the DFS variant on the planar graph dataset, the DFS variant performs substantially better on the SBM dataset. Notably on the SBM dataset, the DFS variant outperforms all baselines w.r.t. VUN and most MMD metrics.
On both datasets, \method is competitive with the two diffusion-based approaches, DiGress and ESGG.
\subsection{Experiments with Expanded Synthetic Datasets}
\label{subsec:large-synthetic}
\begin{table}[t]
    \small
    \centering
    \caption{Performance on expanded synthetic datasets, evaluated on 1024 model samples. Showing a single run for the baselines and DFS, the median across three runs for the line Fiedler variant. *ESGG evaluation modified to draw graph sizes from empirical training distribution and use 100 refinement steps for determining SBM validity.}\label{tab:large-synthetic-datasets}
    \input{tables/table-large-synthetic-wo-unique-novel}
\end{table}
We supplement the results presented above by training our model on larger synthetic datasets. Namely, we generate training sets consisting of $8192$ graphs and corresponding validation and test sets consisting of 256 graphs each. We use the same data generation approach as in~\citet{martinkus2022spectre} to obtain expanded planar and SBM datasets. Additionally, we produce an expanded dataset of lobster graphs using NetworkX~\citep{hagberg2008networkx}, as done in~\citep{lia2019gran}. We perform one training run per dataset for the DFS variant and three independent runs for the line Fiedler variant to illustrate robustness. We present the deviations observed across the three runs in Appendix~\ref{appendix:comprehensive-large-synthetic} and visualize samples from our model in Appendix~\ref{appendix:qualitative-samples}. In Table~\ref{tab:large-synthetic-datasets}, we compare our method to our three baselines. For reasons of brevity, we only report the median performance here. All models reach perfect uniqueness and novelty scores on the expanded planar and SBM datasets and comparable uniqueness and novelty performance on the expanded lobster dataset (c.f. Appendix~\ref{appendix:comprehensive-large-synthetic}).
We find that \method is substantially faster during inference than the diffusion models, consistently achieving at least a 100-fold speedup in comparison to DiGress and ESGG.
Moreover, we find that our method appears competitive with respect to sample quality, outperforming the two diffusion models on the expanded SBM dataset in terms of validity. For ESGG, we note that we obtain a surprisingly low validity score on the expanded SBM dataset and refer to Appendix~\ref{appendix:esgg-selection} for further discussion on this. We find that \method substantially outperforms the autoregressive baseline, GRAN, in terms of validity and MMD metrics.

\subsection{Experiments with Real-World Data}
\label{subsec:real-world-exp}
In this subsection, we present empirical results on the protein graph dataset introduced by~\citet{dobson2003proteins} and the GuacaMol~\citep{brown2019guacamol} dataset consisting of drug-like molecules. 

\paragraph{Proteins.} While results have been reported for the protein dataset in previous works, we re-evaluate the baselines on 1024 model samples to reduce bias and variance in the reported metrics. We use a trained GRAN checkpoint provided by~\citet{lia2019gran} but re-train ESGG and DiGress, as no trained models are available.
\begin{table}[ht]
    \small
    \centering
    \caption{Performance of models on protein graph dataset. *Graph sizes are drawn from empirical training distribution.}
    \resizebox{\columnwidth}{!}{
    \input{tables/table-protein}
    }
    \label{tab:protein-dataset}
    \vskip-.1in
\end{table}
We find that our model is again substantially faster than the diffusion-based baselines. Moreover, it is also 10 times faster than GRAN while outperforming it with respect to all MMD metrics. In comparison to the diffusion-based models, the sample quality of our approach appears slightly worse by most MMD metrics.

\paragraph{GuacaMol.} So far, we have focused on generating un-attributed graphs. However, molecular structures require node and edge labels to represent atom and bond types, respectively. Inspired by the approach of~\citet{Li2020deepscaffold}, we first train ANFM to generate un-attributed topologies and then assign node and edge labels using a simple VAE. Details of this post-hoc labeling procedure can be found in \cref{appendix:post-hoc-labeling}. In Table~\ref{tab:guacamol}, we compare the performance of \method to several baselines from~\citet{vignac2023digress}, namely an LSTM model trained on SMILES strings~\citep{brown2019guacamol}, graph MCTS~\citep{brown2019guacamol}, and NAGVAE~\citep{Kwon2020nagvae}. Notably, \method demonstrates competitive performance with DiGress.
\begin{table}[ht]
    \small
    \centering
    \caption{Performance of models on GuacaMol benchmark.}
    \input{tables/guacamol}
    \label{tab:guacamol}
    \vskip-.1in
\end{table}

\subsection{Ablation Studies}
\label{subsec:ablations}
In this subsection, we demonstrate that noise augmentation and adversarial fine-tuning are crucial components of our method. We interpret this as a strong indication that exposure bias affects our autoregressive model. Additionally, we study the impact of the filtration granularity, as determined by the hyper-parameter $T$. In Appendix~\ref{appendix:additional-ablations}, we present extensive additional ablations.
\begin{table}[ht]
    \centering
    \small
    \caption{Two ablation studies using the line Fiedler variant on the expanded planar graph dataset. Showing median $\pm$ maximum deviation across three runs. For the noise ablation, we train for 100k steps in stage I. For the finetuning ablation, we train for 200k steps in stage I.}
    \include{tables/ablations/pretrained-vs-gan-and-noise-fused}
    \label{tab:ablations-fused}
    \vskip-.15in
\end{table}
\paragraph{Noise Augmentation.} We find that noise augmentation of intermediate graphs substantially improves performance during training with teacher forcing (stage I). We illustrate this using the line Fiedler variant on the expanded planar graph dataset in Table~\ref{tab:ablations-fused}. A corresponding experiment for the DFS variant can be found in Appendix~\ref{appendix:additional-ablations}.

\paragraph{GAN Tuning.} In Table~\ref{tab:ablations-fused}, we compare performance after training with teacher-forcing and subsequent adversarial fine-tuning on the expanded planar graph dataset. We find that adversarial fine-tuning substantially improves performance, both in terms of validity and MMD metrics. A corresponding analysis for the DFS variant and the expanded SBM and lobster datasets can be found in Appendix~\ref{appendix:additional-ablations}. 

\paragraph{Filtration Granularity.} In Figure~\ref{fig:filtration-granularity}, we study the impact of filtration granularity, \ie the number of steps $T$, on generation quality and speed of \method. Analogously, we investigate how the number of denoising steps influences quality and speed in DiGress. We re-train the models with varying $T$. \method consistently outperforms DiGress in computational efficiency across all $T$. While DiGress only achieves a maximum VUN of 41\% for our largest considered $T$, \method achieves a VUN of 81\% for $T=30$. 
\begin{figure}[t]
    \centering
    \begin{subfigure}[c]{0.48\columnwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/vun_reputtal.pdf}
        \subcaption{Planar VUN vs $T$}
        \label{subgfig:vun-vs-t}
    \end{subfigure}  \hfill
    \begin{subfigure}[c]{0.48\columnwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/time_rebuttal.pdf}
        \subcaption{Inference time vs $T$}
        \label{subgfig:time-vs-t}
    \end{subfigure} \\
    \caption{Performance of \method and DiGress on the expanded planar dataset as number of generation steps varies.}
    \label{fig:filtration-granularity}
    \vskip-.15in
\end{figure}

\section{Conclusion} 
We proposed \method, an efficient autoregressive graph generative model that relies on graph filtration. 
The filtration framework generalizes previous autoregressive approaches that operate via node addition, providing additional flexibilities in the choice of graph sequences.
\method generates high-quality graphs, outperforming existing autoregressive models and rivaling discrete diffusion approaches in terms of quality while being substantially faster at inference. Various ablations demonstrated the configurability of \method and indicated that exposure bias is an important challenge for autoregressive graph modeling. 

While we have demonstrated that attributes may be sampled in a post-hoc fashion using a VAE, direct modeling within \method remains an interesting area for future investigation. 
We also highlight issues with existing benchmark datasets, where small sample sizes during evaluation lead to unreliable results with high variance and bias. Systematically addressing these challenges is essential for enabling more reliable and fair benchmarking in future research.

\section*{Acknowledgements}
This work was supported by the Max Planck Society. We thank Nina Corvelo Benz for her insightful feedback on the manuscript.

\section*{Impact Statement}
We have introduced \method, a method for high-throughput graph generation. There are many well-established societal implications of advancing the field of machine learning in general, which also apply to our work. The use of graph generative models in drug discovery represents one particularly impactful application with potentially significant benefits for public health. As these advancements shape the future of medicine, it is crucial to promote their responsible use and ensure that their benefits remain widely accessible.


\bibliography{main}
\bibliographystyle{icml2025}

\clearpage

\appendix
\onecolumn


\addcontentsline{toc}{section}{Appendix} %
\part{Appendix} %
\parttoc %

This appendix is organized as follows: We discuss additional related work in \cref{appendix:extended-related-work} and provide details on post-hoc attribute generation in \cref{appendix:post-hoc-labeling}. In \cref{appendix:complexity-and-first-order-variant}, we analyze the runtime complexity of \method and investigate a variant that is asymptotically more efficient w.r.t. $T$. In \cref{appendix:a-bound-on-model-evidence}, we show that we optimize a lower-bound on the data log-likelihood in training stage I. Details on hyperparameters and model architecture are provided in \cref{appendix:hyper-parameters-and-advice,appendix:architecture}. We provide evaluation results across several runs and qualitative model samples in \cref{appendix:extended-evaluation-results}. \cref{appendix:baselines} provides details on our baselines. Additional ablations on filtration function, schedule, node individualization, etc., are presented in \cref{appendix:additional-ablations}. We discuss the shortcomings of established evaluation approaches in \cref{appendix:variance-and-bias}. Finally, we detail the adversarial finetuning algorithm in \cref{appendix:adversarial-finetuning}.


\section{Extended Related Work}
\label{appendix:extended-related-work}
In this section, we extend Sec.~\ref{sec:related-work} and provide additional comparative analyses to previous works. 

\paragraph{Other Graph Generative Models.} In concurrent work, \citet{zhao2024pard} introduce a hybrid graph generative model, termed Pard, combining autoregressive and discrete diffusion components. Similar to \method, Pard generates graphs by building a sequence of increasingly large subgraphs. In contrast to the method we present here, Pard is limited to the generation of \emph{induced} subgraphs and uses a shared diffusion model to sample them. While the authors state efficiency as one motivation for their approach, they do not present runtime measurements during inference. 

\paragraph{Graph Denoising Diffusion.} Similar to graph denoising diffusion models~\citep{vignac2023digress}, we propose a corrupting process to transform graph samples $G_T$ into graphs $G_0$ from some convergent distribution (in our case the point-mass at the completely disconnected graph). However, in contrast to denoising diffusion models, the process we are proposing is not Markov.

\paragraph{Absorbing State Diffusion.} Absorbing state graph diffusion~\citep{chen2023edge,kong2023grapharm} resembles our approach in that it also generates a sequence of increasingly dense graphs. We aim to increase efficiency by generating substantially shorter sequences than previous works. In practice, we choose to generate graphs within 15 or 30 steps. EDGE~\citep{chen2023edge} requires between 64 and 512 denoising steps, depending on the dataset. To generate a graph on $N$ nodes, GraphARM~\citep{kong2023grapharm} requires $N$ denoising steps, as exactly one node decays to the absorbing state at a time. The larger number of denoising steps in these methods may increase inference time and necessitates a first-order autoregressive structure. Additionally, as detailed above, our proposed method does not readily fit into the framework of denoising diffusion models. Moreover, \method may generate non-monotonic graph sequences.

\paragraph{RL in Graph Generation.} 
In the context of graph generation, reinforcement learning has been mostly used for molecular graphs. ~\citet{you2018gcpn} train a generative model for molecules via reinforcement learning, combining adversarial and domain-specific rewards. In contrast to our work, they only consider molecular graphs and do not use teacher-forcing for training. Taiga~\citep{eyal2023moleculerl} uses reinforcement learning to optimize the chemical properties of molecules obtained from a language model pre-trained on SMILES strings. Diffusion models have also been shown to be amenable to RL finetuning, allowing extrinsic non-differentiable metrics to be optimized~\citep{liu2024graphdiffusionpolicy}.


\section{Generating Attributed Graphs}
\label{appendix:post-hoc-labeling}
\subsection{Variational Inference}
To generate attributed graphs $(G, l_V, l_E)$ with topology $G$ and node and edge labels $l_V$ and $l_E$, we propose to first generate the un-attributed graph topology using \method. Subsequently, we label nodes and edges using a separate model. I.e., we decompose the joint distribution as:
\begin{equation}
    p_\theta(G, l_V, l_E) = p_\theta^{\mathrm{label}}(l_E, l_V \:|\: G) \cdot p_\theta^{\mathrm{\method}}(G)
\end{equation}
where $p_\theta^{\mathrm{label}}$ generates node and edge attributes.
We take inspiration from~\citet{Li2020deepscaffold} and train $p_\theta^{\mathrm{label}}$ as a variational autoencoder.

\paragraph{Variational Inference.} Let $G := (V, E)$ be a graph topology from the training dataset and let $l_V : V \to \mathcal{X}$ and $l_E: E \to \mathcal{Y}$ be ground-truth node and edge labels for $G$, respectively. An encoder $q_\phi$ maps the labeled graph $(G, l_V, l_E)$ to a distribution over $D$-dimensional latent node representations $\bm{z} \in \R^{V \times D}$. A decoder $p_\theta$ maps a tuple $(G, \bm{z})$, i.e. the graph topology combined with a latent representation, to a distribution over node and edge attributes. In practice, we consider discrete attributes and $p_\theta$ parametrizes a product distribution across nodes and edges. If $\mathcal{X}$ or $\mathcal{Y}$ factorize into a product of simpler spaces (i.e., when there are multiple node or edge attributes), $p_\theta$ parametrizes a corresponding product distribution over $\mathcal{X}$ or $\mathcal{Y}$. The encoder $q_\phi$ parametrizes a Gaussian distribution with a diagonal covariance matrix. We formulate the typical evidence lower bound~\citep{kingma2014vae}:
\begin{equation}
    p_\theta(l_E, l_V \:|\: G) \geq \mathbb{E}_{\bm{z} \sim q_\phi(\:\cdot\:|\:G, l_E, l_V)}\left[p_\theta (l_E, l_V \:|\: G, \bm{z}) \right] - \KL\left(q_\phi(\: \cdot \:|\: G, l_E, l_V) \:|\: \mathcal{N}(\bm{0}, I) \right)
    \label{eq:elbo-post-hoc-labeling}
\end{equation}

\paragraph{Architecture.} We implement $q_\phi$ and $p_\theta$ as GraphGPS~\citep{rampasek2022graphgps} models. Since $q_\phi$ operates on edge-labeled graphs, we use GINE message passing layers~\citep{hu2020gine} within its graph transformer. The decoder $p_\theta$, on the other hand, only operates on node representations, and we therefore use GIN layers~\citep{xu2019gin}. The two GNNs are provided with Laplacian and random walk positional embeddings~\citep{dwivedi2022randomwalkpe} and we use 5\% dropout layers~\citep{srivastava2014dropout}. We feed the node representations produced by $p_\theta$ through MLPs to generate distributions over node attributes. To generate a distribution over the attribute of an edge $\{u, v\}$, we add the node representations corresponding to $u$ and $v$ and feed the resulting vector through an MLP.

\paragraph{Training.} We train $q_\phi$ and $p_\theta$ jointly by maximizing a re-weighted version of the ELBO in~\eqref{eq:elbo-post-hoc-labeling}. Specifically, we divide the loss for a given datum $(G, l_E, l_V)$ by $|V|$, the cardinality of the node set of $G$. 

\paragraph{Sampling.} To sample attributes, we draw latent node representations $\bm{z}$ from the standard normal distribution. We select the maximum likelihood node and edge attributes from $p_\theta(\:\cdot\:|\:G, \bm{z})$. The encoder $q_\phi$ may be discarded after training.


\subsection{Details on GuacaMol Experiments}
\label{appendix:guacamol-details}
Below, we provide details on the molecule graph generation presented in Sec.~\ref{subsec:real-world-exp}. As described in Appendix~\ref{appendix:post-hoc-labeling}, we train \method on the un-attributed molecule topologies of GuacaMol. Independently, we train a VAE to generate attributes conditioned on a topology. To sample a molecule, we first generate a graph using \method and label nodes and edges using the VAE.

\paragraph{Attributes.} We produce edge labels that indicate the bond type between heavy atoms, distinguishing between single, double, triple, and aromatic bonds. To reconstruct the correct molecule from its graph representation, we find that we require require several node attributes. Namely, given an atom (i.e., node), we generate its type (i.e., the element), the number of explicit hydrogens bound to it, the number of radical electrons, and its partial charge.

\paragraph{\method Hyperparameters.} We train an \method model using the DFS filtration strategy. In stage I, we use similar hyper-parameters as for the other datasets. We use $T=30$ filtration steps, a learning rate of $10^{-5}$, a local batch size of 64 on two GPUs, and 300k training steps. Throughout stage I training, we monitor validation loss and ensure that the model does not overfit. During training stage II, we use a batch size of 32 and a learning rate of $2.5\times 10^{-8}$ for the generative model. The discriminator has 3 layers and a hidden dimension of 32. The other hyperparameters of the discriminator and value model match those used in the other experiments.

\paragraph{VAE Hyperparameters.} We use 5-layer GraphGPS~\citep{rampasek2022graphgps} models for the encoder and decoder, respectively. The hidden dimension is 256 while we choose the latent dimension $D$ to be 8. The Laplacian positional embedding is 3-dimensional while the random walk positional embedding is 8-dimensional. We train for 250 epochs on the GuacaMol dataset, using a batch size of 512, an Adam optimizer~\citep{kingma2014adam} and a learning rate of $10^{-4}$.

\paragraph{Model Samples.} In Figure~\ref{fig:guacamol-samples}, we show uncurated samples from the \method model.
\begin{figure}
    \centering
    \begin{subfigure}[c]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-guacamol/molecule_0.pdf}
    \end{subfigure} \hfill
    \begin{subfigure}[c]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-guacamol/molecule_1.pdf}
    \end{subfigure} \hfill
        \begin{subfigure}[c]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-guacamol/molecule_2.pdf}
    \end{subfigure} \hfill
    \begin{subfigure}[c]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-guacamol/molecule_3.pdf}
    \end{subfigure} \hfill 
    \begin{subfigure}[c]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-guacamol/molecule_4.pdf}
    \end{subfigure} \\
    \begin{subfigure}[c]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-guacamol/molecule_5.pdf}
    \end{subfigure} \hfill
        \begin{subfigure}[c]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-guacamol/molecule_6.pdf}
    \end{subfigure} \hfill
    \begin{subfigure}[c]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-guacamol/molecule_7.pdf}
    \end{subfigure} \hfill
        \begin{subfigure}[c]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-guacamol/molecule_8.pdf}
    \end{subfigure} \hfill
    \begin{subfigure}[c]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-guacamol/molecule_9.pdf}
    \end{subfigure}
    \caption{Uncurated samples from \method model trained on GuacaMol.}
    \label{fig:guacamol-samples}
\end{figure}


\section{Sampling Complexity of \method}
\label{appendix:complexity-and-first-order-variant}
\subsection{Complexity Analysis}
\label{appendix:complexity-analysis}
In the following, we analyze the asymptotic runtime complexity of sampling a graph from our proposed model and the baselines we studied in Section~\ref{sec:experiments}.
\begin{proposition}
The asymptotic runtime complexity for sampling a graph with $N$ nodes from an \method with $T$ timesteps is:
\begin{equation}
    \mathcal{O}(T^2N + TN^3)
\end{equation}
\end{proposition}
\begin{proof}
    To sample a graph from an \method, one has to perform $T$ forward passes through our proposed mixer architecture. These forward passes are preceded by the computation of various graph features, including laplacian eigenvalues and eigenvectors. This eigendecomposition has complexity $\mathcal{O}(N^3)$. At timestep $0 \leq t < N$, the structural mixing layers have complexity $\mathcal{O}(N^2)$ due to the self-attention component of SAT. The temporal mixing layers, on the other hand, have complexity $\mathcal{O}(N(t+1))$, as each node attends to its representations at timesteps $0, \dots, t$. We bound this complexity by $\mathcal{O}(NT)$. Hence, aggregating these complexities across all $T$ timesteps, we obtain the following runtime complexity:
    \begin{equation}
        \mathcal{O}(T^2N + TN^2 + TN^3) = \mathcal{O}(T^2N +  TN^3)
    \end{equation}
\end{proof}

Below we show that the asymptotic complexity of \method differs from the complexity of DiGress only in the quadratic term w.r.t. $T$: 
\begin{proposition}
The asymptotic runtime complexity for sampling a graph with $N$ nodes from a DiGress model with $T$ denoising steps is:
\begin{equation}
    \Omega(TN^3)
\end{equation}
\end{proposition}
\begin{proof}
    Similar to \method, DiGress performs an eigendecomposition of the graph laplacian in each denoising step. Hence, one obtains a complexity of $\Omega(N^3)$ in each timestep, resulting in an overall complexity of $\Omega(TN^3)$.
\end{proof}

We further analyze the asymptotic complexities of our other baselines of Sec.~\ref{subsec:large-synthetic} and Sec.~\ref{subsec:real-world-exp}.
\begin{proposition}
    The asymptotic runtime complexity for sampling a graph with $N$ nodes from a GRAN model is $\Omega(N^2)$.
\end{proposition}
\begin{proof}
    GRAN explicitly constructs a dense adjacency matrix with $N^2$ entries.
\end{proof}

\begin{proposition}
    The asymptotic runtime complexity of sampling a graph with $N$ nodes and $M$ edges from an ESGG model is $\Omega(N + M)$.
\end{proposition}
\begin{proof}
This bound should trivially be satisfied by any generative model, as one already needs $\Omega(N + M)$ bits to represent a graph with $M$ edges on $N$ nodes. We refer to~\citep{bergmeister2024efficientscalable} for a discussion on how tight this bound is.
\end{proof}

In Table~\ref{tab:runtime-complexities}, we summarize these asymptotic complexities. 
\begin{table}[ht]
    \centering
    \caption{Asymptotic runtime complexities for sampling from different graph generative models.}
    \begin{tabular}{l|l}
        \toprule
        Method & Sampling complexity \\
        \midrule
        \method & $\mathcal{O}(T^2N + TN^3)$ \\
        DiGress & $\Omega(TN^3)$ \\
        GRAN & $\Omega(N^2)$ \\
        ESGG & $\Omega(N + M)$ \\
        \bottomrule        
    \end{tabular}
    \label{tab:runtime-complexities}
\end{table}
While this analysis may suggest that \method does not scale well to extremly large graphs, we caution the reader that the asymptotic behavior may not accurately reflect efficiency in practice: Firstly, multiplicative constants and lower-order terms are ignored. Hence, it remains unknown in which regimes the asymptotic behavior governs inference time. Secondly, the analysis was made under the assumption that hyper-parameter choices (i.e. depth, width, etc.) is kept constant as $N$ and $M$ increase. It is reasonable to expect that more expressive networks are required to model large graphs.

\subsection{A First-Order Autoregressive Variant}
\label{appendix:first-order-autoregressive-variant}
As we demonstrated in Appendix~\ref{appendix:complexity-analysis}, the runtime of \method is quadratic in the number of generation steps $T$ due to the temporal mixing operations which are implemented as transformer decoder layers. Analogously, one may verify that the memory complexity of sampling from \method is linear in $T$. In this subsection, we study a simplified variant of \method in which we use a first-order autoregressive structure. I.e., we enforce:
\begin{equation}
    p_\theta(\tilde{G}_{t+1} | \tilde{G}_t, \dots, \tilde{G}_0) = p_\theta(\tilde{G}_{t+1} | \tilde{G}_t)
\end{equation}
We implement this by ablating the causally masked self-attention mechanism from the transformer layers in our mixer model, leaving only the feed-forward modules.
The resulting first-order variant of \method has space complexity which is independent of $T$ and runtime complexity which is linear in $T$. 

We train such a first-order variant of \method on the expanded planar graph dataset, using the line Fiedler filtration strategy and the same hyperparameters as for the transformer-based variant (see Appendix~\ref{appendix:hyper-parameters}). Using the first-order variant, we observe training instabilities after the first 100k training steps of stage I. While reducing the learning rate rectifies this instability, we find that this slows learning progress substantially. Instead, we use a model checkpoint at 100k steps and continue with training stage II.  

In Table~\ref{tab:first-order-variant-stage1}, we compare the performance of the transformer-based and the first-order variants after 100k steps of stage I training. In Table~\ref{tab:first-order-variant-stage2}, we compare the performance after the subsequent stage II training. While we perform only 100k training steps in stage I for the first-order variant, we perform 200k training steps for the transformer-based variant, as it did not exhibit instabilities. 
\begin{table}[ht]
    \centering
    \caption{Performance of two \method variants on the expanded planar graph dataset after 100k steps of stage I training. Showing median across three runs for the transformer-based variant and a single run for the first-order variant. All models reach perfect uniqueness and novelty scores.}
    \include{tables/ablations/first-order-variant-stage1}
    \label{tab:first-order-variant-stage1}
\end{table}
\begin{table}[ht]
    \centering
        \caption{Performance of two \method variants on the expanded planar graph dataset after stage II training. The transformer-based variant was trained for 200k steps in stage I while the first-order variant was trained for 100k steps in stage I. Showing median across three runs for the transformer-based variant and a single run for the first-order variant. All models reach perfect uniqueness and novelty scores.}
        \include{tables/ablations/first-order-variant-stage2}
    \label{tab:first-order-variant-stage2}
\end{table}

Generally, we observe that the transformer-based \method variant slightly outperforms the first-order variant in terms of quality. However, the first-order variant remains competitive after stage II trainig and, thus, may be a suitable alternative in cases where a large $T$ is chosen. In our setting ($T=30$), however, we find that the first-order variant is not substantially faster during inference, indicating that the runtime is not governed by the quadratic complexity in $T$. 


\section{A Bound on Model Evidence in \method}
\label{appendix:a-bound-on-model-evidence}
Given a graph $G_T$, let $q(G_{T-1}, \dots G_1 | G_T)$ be the data distribution over noisy filtrations of this graph, determined by the choice of filtration function, scheduling, and noise augmentation. We assume that $G_0$ is deterministically the completely disconnected graph. Moreover, we note that by applying our noise augmentation strategy we ensure that $q$ is supported everywhere. Given some graph $G_T$, we can now derive the following evidence lower bound:
\begin{equation}
\begin{aligned}
\log p_\theta(G_T)
&=&& \log \sum_{G_1, \dots, G_{T-1} \in \mathcal{G}} p_\theta(G_T, \dots, G_0) \\ 
&=&& \log \sum_{G_1, \dots, G_{T-1} \in \mathcal{G}} q(G_{T-1}, \dots, G_1 | G_T) \frac{p_\theta(G_T, \dots, G_0)}{q(G_{T-1}, \dots, G_1 | G_T)} \\
&\geq&& \mathbb{E}_{(G_{T-1}, \dots, G_1) \sim q(\,\cdot\, | G_T)} \left[\log \frac{p_\theta(G_T, \dots, G_0)}{q(G_{T-1}, \dots, G_1 | G_T)}\right]  \\
&=&& \mathbb{E}_{(G_{T-1}, \dots, G_1) \sim q(\,\cdot\, | G_T)}\biggl[\sum_{t=1}^{T} \log p_\theta(G_{t} | G_{t - 1}, \dots, G_0) - \log q(G_{T-1}, \dots G_1 | G_T)\biggr]
\end{aligned}
\end{equation}
We note that this lower bound is (up to sign and a constant that does not depend on $\theta$) exactly the autoregressive loss we use in training stage I. Hence, while we train \method to model sequences of graphs, we do actually optimize an evidence lower bound for the final graph samples $G_T$.  

\section{Hyperparameters}
\label{appendix:hyper-parameters-and-advice}
\subsection{Pracical Advice on Hyperparameter Choice}
In the following, we provide some pracitcal advice on choosing some of the most important hyper-parameters in \method. Generally, we tuned few hyper-parameters in our experiments. We found the number of generation steps $T$ to be one of the most impactful hyper-parameters.

\paragraph{Filtration Function.} The filtration function $f: E \to \R$ is the main component determining the structure of the graph sequence during stage I training. We recommend that $f$ should convey meaningful information about the structure and assign (mostly) distinct values to distinct edges. We note that if $f$ fails to assign disstinct values to edges, many edges may be added in a single generation step, regardless of the choice of $T$. We found both the edge Fiedler function and the DFS filtrations to perform well in many settings. We recommend that practioners utilize these filtration strategies and perform experiments with further filtration functions that incorporate domain-specific inductive biases. In the case of generating protein graphs, for instance, one may consider a filtration function that quantifies the distance of residues in the sequence (this filtration would first generate a backbone path, followed by increasingly long-range interactions of residues).

\paragraph{Filtration Granularity.} As we demonstrate in Sec.~\ref{subsec:ablations}, the choice of the number of generation steps $T$ has a substantial influence on sampling efficiency and generation quality. Generally, $T$ can be chosen substantially smaller than in other autoregressive models. In our experiments, we chose $T \leq 32$. We recommend that practitioners experiment with different values in this order of magnitude. We further caution that increasing $T$ does not necessarily improve sample quality, and may actually harm it.

\paragraph{Scheduling Function.} The filtration schedule governs the rate at which edges are added at different timesteps. In the case of the line Fiedler filtration function, it is determined by $\gamma$, which should be monotonically increasing with $\gamma(0) = 0$ and $\gamma(1) = 1$. We found the heuristic choice of $\gamma(t) := t$ to work well in many settings. However, as we demonstrate in Appendix~\ref{appendix:additional-ablations}, the concave schedule may be a promising alternative. We recommend that practitioners validate stage I training with a convex, linear, and concave scheduling function. We note that the scheduling function is no longer used during stage II training, as the model is left free to generate arbitrary intermediate graphs. Hence, performance after stage I training may be a suitable metric for selecting a scheduling function.

\paragraph{Noise Augmentation.} We use noise augmentation during training stage I to counteract exposure bias, i.e. the accumulation of errors in the sampling trajectory. Manual inspection of the graph sequence $\tilde{G}_0, \dots, \tilde{G}_T$ may be difficult. However, we found that inspecting the development of the edge density over this graph sequence can provide a simple tool for diagnosing exposure bias. In models that do not utilize noise augmentation, we can observe that, after some generation steps, the edge density oftentimes deviates from its expected behavior (e.g. by suddenly increasing or dropping substantially). In this case we expect that noise augmentation can rectify exposure bias. In our experiments, we find that we do not need to tune the noise schedule. Instead, we fix a single schedule that is shared across all models. For details on this schedule, we refer to Appendix~\ref{appendix:hyper-parameters}.

\paragraph{Perturbation of Node Orderings.} During training stage I, \method variants using the line Fiedler filtration function may overfit on small datasets. This manifests as an increase in validation loss, while the validation MMD metrics continue to improve. We observe this behavior only on the small datasets in Sec.~\ref{subsec:small-synthetic} and find that it can be mostly attributed to the node ordering used to derive initial node representations (c.f. Appendix~\ref{appendix:node-individualization}). 
We recommend to monitor validation losses during stage I training. If the validation loss starts to slowly increase while the training loss continues to decrease, we recommend to randomly perturb the node ordering, as described in Appendix~\ref{appendix:node-individualization}. The noise scale $\sigma$ should be increased until no over-fitting can be observed. We find that the DFS filtration strategy is less prone to overfitting, as DFS node orderings are not unique. Hence, we may include many distinct filtrations of a single graph $G_T$ in our training set.




\subsection{\method Hyperparameters}
\label{appendix:hyper-parameters}
In Tables~\ref{tab:hyper-parameters} and~\ref{tab:hyper-parameters-dfs}, we summarize the most important hyperparameters of the generative model used in our experiments, including the number of filtration steps ($T$), mixture components ($K$), learning rate (LR), batch size (BS) in the format $\texttt{num\_gpus}\times\texttt{grad\_accumulation}\times\texttt{local\_bs}$, and the number of perturbed filtration sequences we produce per graph in our training set (\# Perturbations).
\begin{table}[htp]
    \centering
    \scriptsize
    \caption{Hyper-parameters for line Fiedler variant of \method.}
    \include{tables/hyper-parameters}
    \label{tab:hyper-parameters}
\end{table}
\begin{table}[htp]
    \centering
    \scriptsize
    \caption{Hyper-parameters for DFS variant of \method. Parameters that do not appear in this table exactly match the corresponding parameters for the line Fiedler variant, c.f. Table~\ref{tab:hyper-parameters}.}
    \include{tables/hyper-parameter-dfs}
    \label{tab:hyper-parameters-dfs}
\end{table}

In Table~\ref{tab:hyper-parameters-gan}, we additionally provide the most important hyperparameters of the discriminator and value model trained during the adversarial fine-tuning stage.
\begin{table}[htp]
    \centering
    \scriptsize
    \caption{Hyper-parameters of discriminator and value model used during adversarial fine-tuning.}
    \include{tables/hyper-parameters-gan}
    \label{tab:hyper-parameters-gan}
\end{table}



\section{Details on Architecture}
\label{appendix:architecture}
\subsection{Input Node Representations}
\label{appendix:node-individualization}
We define the input node representations as:
\begin{equation}\label{eq:node_indivisualization}
    v_i^{(t)} := f_\theta(G_t)_i + W_i^{\mathrm{node}},
\end{equation}
where $f_\theta$ produces node features from Laplacian positional encodings~\citep{dwivedi2023benchmarking}, random walk positional encodings~\citep{dwivedi2022randomwalkpe}, and cycle counts following DiGress~\citep{vignac2023digress}. The matrix $W^\mathrm{node} \in \R^{N \times D}$ is a trainable embedding layer where $N$ denotes the cardinality of the largest vertex set seen during training. 
It is important to note that the computation of input node representations requires a specific node ordering to index $W^{\mathrm{node}}$. While the permutation equivariance of our model and the symmetry of the initially empty graph $G_0$ allow for arbitrary ordering during inference, we employ a structured approach during teacher-forcing training. This ordering is derived from the structure of the graph $G_T$ and depends on the filtration strategy.

\paragraph{Node Ordering for DFS Variant.} The filtration function of the DFS variant is derived from a DFS node ordering of $G_T$. Hence, we simply use this node ordering to assign positional embeddings.

\paragraph{Node Ordering for Line Fiedler Variant.} For the line Fiedler variant, we propose a node weighting scheme $h: V \rightarrow \mathbb{R}$ defined as:
\begin{equation}
    h(i) := \frac{1}{|\mathcal{N}_G(i)|} \sum_{j \in \mathcal{N}_G(i)} f(e_{ij}), \qquad \forall \: i \in V,
\end{equation}
where $\mathcal{N}_G(i)$ represents the neighborhood of node $i$ in $G$. This weighting assigns to each node the average weight of its incident edges, as determined by the filtration function $f$. We then establish a node ordering such that $h$ is non-increasing. The impact of different ordering strategies on model performance is further studied and compared in Appendix~\ref{appendix:additional-ablations}.

When training on small datasets, such as those introduced by~\citet{martinkus2022spectre}, we find that the node individualization in~\eqref{eq:node_indivisualization} can lead to overfitting. This manifests as an increase in validation loss, while the evaluation metrics (i.e. MMD and VUN) continue to improve. As a data augmentation strategy to avoid overfitting, we propose to add Gaussian noise to the node weights $h_G$ defined in~\eqref{eq:node_indivisualization} when training on small datasets. I.e., we use the perturbed node weights 
\begin{equation}
    h_G(s) + \mathcal{N}(0, \sigma^2)
\end{equation} 
for sorting the nodes. We emphasize that this measure is independent of the perturbation of intermediate graphs introduced in Sec.~\ref{subsec:noise-augmentation-of-filtrations}. Moreover, we perturb node orderings only in the experiments on the small SPECTRE datasets (i.e., in Sec~\ref{subsec:small-synthetic}).

\subsection{Edge Decoder Architecture}
\label{appendix:edge-decoder}
In this subsection, we present details on the edge decoder $D_{\cdot, \theta}$. While our approach is in principle applicable to discretely labeled edges, we concentrate on predicting distributions over unlabeled edges here. Fix some timestep $0 \leq t < T$. Assume that for this timestep, we are given some node representations $(v_i)_{i=1}^{|V|}$ produced by the backbone model. The edge decoder contains $K$ submodules that produce multivariate Bernoulli distributions. Assuming that the node-representations produced by the backbone are $D$-dimensional, let $\operatorname{Dense}_{k, \theta}^{(1)}: \R^D \to \R^{2D}$ and $\operatorname{Dense}_{k, \theta}^{(2)}, \operatorname{Dense}_{k, \theta}^{(3)}: \R^{2D} \to \R^{2D}$ be fully connected layers learned for each component $k$. Define corresponding MLPs:
\begin{equation}
    \operatorname{MLP}_{k, \theta} := \operatorname{ReLU} \circ \operatorname{Dense}^{(2)}_{k, \theta} \circ \operatorname{ReLU} \circ \operatorname{Dense}^{(1)}_{k, \theta} 
\end{equation}
For each $k$, we process the node representations $v_i$ separately and split the resulting vectors into two $D$-dimensional halves:
\begin{equation}
    (x_{i}^{(k)}, y_{i}^{(k)}) := \operatorname{MLP}_{k, \theta}(v_i) \qquad  (\hat x_{i}^{(k)}, \hat y_{i}^{(k)}) := \operatorname{Dense}^{(3)}_k\left((x_{i}^{(k)}, y_{i}^{(k)})\right)
\end{equation}
We define the logit $l_{k, i, j}$ for the presence of an edge and the logit $r_{k, i, j}$ for the absence of an edge:
\begin{equation}
    l_{k, i, j} := \frac{{x_{i}^{(k)}}^\top \hat x_{j}^{(k)} + {x_{j}^{(k)}}^\top \hat x_{i}^{(k)}}{2}    \qquad r_{k, i, j} := \frac{{y_{i}^{(k)}}^\top \hat y_{j}^{(k)} + {y_{j}^{(k)}}^\top \hat y_{i}^{(k)}}{2}
    \label{eq:symmetrize}
\end{equation}
Finally, we define the likelihood of the presence of an edge as:
\begin{equation}
    D_{k, \theta}(v_i, v_j) := \frac{\exp(l_{k, i, j})}{\exp(l_{k, i, j}) + \exp(r_{k, i, j})}
\end{equation}
While this modeling of the mixture distributions is quite involved, it allows the edge decoder to be easily extended to produce distributions over labeled edges by producing logits for labels of node pairs (instead of producing logits for presence and absence of edges).

Finally, we compute a mixture distribution $\pi \in \Delta^{K - 1}$ via $D_{\mathrm{mix}, \theta}$. To this end, we learn a node-level MLP:
\begin{equation}
    \operatorname{MLP}_{\mathrm{mix}, \theta}^{(1)} := \operatorname{ReLU} \circ \operatorname{Dense}_{\mathrm{mix}, \theta}^{(1)}
\end{equation}
and a graph-level MLP:
\begin{equation}
     \operatorname{MLP}_{\mathrm{mix}, \theta}^{(2)} := \operatorname{Dense}_{\mathrm{mix}, \theta}^{(3)} \circ \operatorname{ReLU} \circ \operatorname{Dense}_{\mathrm{mix}, \theta}^{(2)}
\end{equation}
where $\operatorname{Dense}_{\mathrm{mix}, \theta}^{(3)}: \R^D \to \R^K$. We then define:
\begin{equation}
    D_{\mathrm{mix}, \theta}\left((v_i)_{i=1}^{|V|}\right) := \operatorname{softmax}\left(\operatorname{MLP}_{\mathrm{mix}, \theta}^{(2)}\left(\frac{1}{|V|} \sum_{i=1}^{|V|} \operatorname{MLP}_{\mathrm{mix}, \theta}^{(1)}(v_i)\right)\right)
\end{equation}

In the following, we discuss how our approach, and the edge decoder in particular, may be extended to edge-attributed and directed graphs.

\paragraph{Edge Attributes.} While we only present experiments on un-attributed graphs, we note that our approach (in particular the edge decoder) is naturally extendable to discretely edge-attributed graphs. Assuming that one has $S$ possible edge labels (where one edge label encodes the absence of an edge), one would predict $S$ logits $l_{k, i, j}^{(s)}$ instead of predicting only two logits $l_{k, i, j}$ and $r_{k, i, j}$. Then, for fixed $i, j, k$, the vector 
\begin{equation}
\operatorname{softmax}_s \left(l_{k, i, j}^{(s)}\right)_{s=1}^{S} \qquad \in \qquad \Delta^{S - 1}
\end{equation}
would provide a distribution over labels for edge $\{v_i, v_j\}$. This distribution would be incorporated into a mixture (over $k$) of categorical distributions as above. \eqref{eq:edge-likelihood} would be adjusted to quantify the likelihood of edge labels instead of the likelihood of edge presence/absence. 

\paragraph{Directed Graphs.} In our experiments, we only consider applications of \method to undirected graphs. However, our approach is also naturally extendable to directed graphs. Concretely, one would first adjust all GNNs in \method to take edge directionality into account. One would additionally modify the product in \eqref{eq:edge-likelihood} to run over the entire adjacency matrix instead of only considering the upper triangle. I.e., one would get:
\begin{equation}
    p_\theta(E_t | G_{t-1}, \dots, G_0) := \sum_{k=1}^K \pi_k \prod_{i, j}\left\{\begin{matrix}\begin{aligned}
p_k^{(i, j)}& \quad \mathrm{if} \quad e_{ij} \in E_t \\
1 - p_k^{(i, j)}& \quad \mathrm{else}
\end{aligned}
\end{matrix}
\right\}.
\end{equation}
Finally, the edge decoder would be adjusted in \eqref{eq:symmetrize} to drop the symmetrization of $l_{k, i, j}$ and $r_{k, i, j}$ w.r.t. $i$ and $j$ (i.e., one no longer enforces the presence of the edge $(v_i, v_j)$ to have the same probability as the presence of $(v_j, v_i)$). 


\section{Extended Evaluation Results}
\label{appendix:extended-evaluation-results}
\subsection{Comprehensive Evaluation Results on Expanded Synthetic Datasets}
\label{appendix:comprehensive-large-synthetic}
In Table~\ref{tab:comprehensive-large-synthetic}, we present the deviations observed across the three training runs discussed in Sec.~\ref{subsec:large-synthetic}. Additionally, in Table~\ref{tab:synthetic-uniqueness-novelty}, we present uniqueness and novelty metrics for \method and our baselines.
\begin{table}[ht]
    \centering
    \small
    \caption{Full evaluation results for the Fiedler variant of \method trained on expanded synthetic datasets. Showing median across three runs $\pm$ maximum deviation.}
    \include{tables/comprehensive-large-synthetic}
    \label{tab:comprehensive-large-synthetic}
\end{table}
\begin{table}[ht]
    \centering
    \small
    \caption{Uniqueness and novelty metrics.}
    \include{tables/large-synthetic-unique-novel}
    \label{tab:synthetic-uniqueness-novelty}
\end{table}

\subsection{Qualitative Model Samples}
\label{appendix:qualitative-samples}
In Figure~\ref{fig:uncurated-samples}, we present uncurated samples from the different models described in Sec.~\ref{sec:experiments}.
\begin{figure}[htp]
\begin{subfigure}[b]{\textwidth}
    \centering
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-planar/sample_0.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-planar/sample_1.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-planar/sample_2.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-planar/sample_3.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-planar/sample_4.pdf}
    \end{subfigure}
    \subcaption{Uncurated samples from \method model trained on expanded planar graph dataset.}
    \label{fig:large-planar-samples}
\end{subfigure}
\\
\begin{subfigure}[b]{\textwidth}
    \centering
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-sbm/sample_0.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-sbm/sample_1.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-sbm/sample_2.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-sbm/sample_3.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-sbm/sample_4.pdf}
    \end{subfigure}
    \subcaption{Uncurated samples from \method model trained on expanded SBM dataset.}
    \label{fig:large-sbm-samples}
\end{subfigure}
\\
\begin{subfigure}[b]{\textwidth}
    \centering
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-lobster/sample_0.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-lobster/sample_1.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-lobster/sample_2.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-lobster/sample_3.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-large-lobster/sample_4.pdf}
    \end{subfigure}
    \subcaption{Uncurated samples from \method model trained on expanded lobster dataset.}
    \label{fig:large-lobster-samples}
\end{subfigure}
\\
\begin{subfigure}[b]{\textwidth}
    \centering
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-protein/sample_0.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-protein/sample_1.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-protein/sample_2.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-protein/sample_3.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/samples-protein/sample_4.pdf}
    \end{subfigure}
    \subcaption{Uncurated samples from \method model trained on protein dataset.}
    \label{fig:large-protein-samples}
\end{subfigure}
\caption{Uncurated samples from \method (line Fiedler variant).}
\label{fig:uncurated-samples}
\end{figure}



\section{Baselines}
\label{appendix:baselines}
\subsection{GRAN Hyperparameters}
\label{appendix:gran-hyperparameters}
For our experiments on the expanded lobster dataset, we use the hyperparameters provided by~\citet{lia2019gran} for their own (smaller) lobster dataset. For experiments on the expanded planar graph dataset, we utilize the same hyper-parameter setting but reduce the batchsize to 16. For experiments on the SBM dataset, we further reduce the batchsize to 8 and use 2 gradient accumulation steps. For the experiments on the protein dataset, we utilize the pretrained model provided at \url{http://www.cs.toronto.edu/~rjliao/model/gran_DD.pth}. We perform inference with a batch size of 20.

\subsection{DiGress Hyperparameters}
\label{appendix:digress-hyperparameters}
For our experiments on the expanded planar graph and SBM datasets, we use the hyperparameters provided by~\citet{vignac2023digress} for the corresponding SPECTRE datasets. On the lobster dataset, we use the same hyperparameters as for the expanded SBM dataset (8 layers and batch size 12). On the protein dataset, we use similar hyperparameters as for the expanded SBM dataset but reduce the batch size to 4 due to GPU memory constraints. We use the same inference approach as~\citet{vignac2023digress}, performing generation with a batch size that is twice as large as the batch size used for training. In all cases, we follow~\citet{vignac2023digress} in using 1000 diffusion steps.

\subsection{ESGG Hyperparameters}
\label{appendix:esgg-hyperparameters}
For our experiments on the expanded planar graph and SBM datasets, we use the hyperparameters provided by~\citet{bergmeister2024efficientscalable} for the corresponding SPECTRE datasets. For the expanded lobster dataset, we use the hyperparameters used by~\citet{bergmeister2024efficientscalable} for their tree dataset. We use the test batch sizes provided by~\citet{bergmeister2024efficientscalable} in their hyperparameter configurations.

\subsection{GRAN Model Selection}
\label{appendix:gran-model-selection}
\paragraph{Expanded Planar.} In Table~\ref{tab:gran-planar-model-selection}, we present validation results of the GRAN model trained on the expanded planar graph dataset. We observe no clear development in model performance past 500 steps. We select the checkpoint at 1000 steps.
\begin{table}[ht]
    \centering
    \small
    \caption{Validation results for GRAN model trained on expanded planar graph dataset. Evaluated on 260 model samples. }
    \include{tables/gran-planar-model-selection}
    \label{tab:gran-planar-model-selection}
\end{table}

\paragraph{Expanded SBM.} In Table~\ref{tab:gran-sbm-model-selection}, we present validation results of the GRAN model trained on the expanded SBM dataset. We find that, overall, the checkpoint at 200 steps appears to perform best and select it.
\begin{table}[ht]
    \centering
    \small
    \caption{Validation results for GRAN model trained on expanded SBM dataset. Evaluated on 260 model samples.}
    \include{tables/gran-sbm-model-selection}
    \label{tab:gran-sbm-model-selection}
\end{table}

\paragraph{Expanded Lobster.} In Table~\ref{tab:gran-lobster-model-selection}, we present validation results of the GRAN model trained on the expanded lobster dataset. We observe no improvement in validity past 2500 steps and select this checkpoint. 
\begin{table}[ht]
    \centering
    \small
    \caption{Validation results for GRAN model trained on expanded lobster graph dataset. Evaluated on 260 model samples.}
    \include{tables/gran-lobster-model-selection}
    \label{tab:gran-lobster-model-selection}
\end{table}

\subsection{ESGG Model Selection}
\label{appendix:esgg-selection}
While ESGG maintains exponential moving averages of model weights during training, we choose to only evaluate non-smoothed model weights (i.e. the EMA weights with decay parameter $\gamma=1$), as validation is compute-intensive. 
\paragraph{SBM Dataset.} In our experiments, we obtain worse performance on the expanded SBM dataset than was reported on the smaller SPECTRE SBM dataset in~\citep{bergmeister2024efficientscalable}. In Figure~\ref{fig:sbm-validity-esgg}, we show the development of validity throughout training, which lasted over 4.5 days on an H100 GPU. Throughout training, we fail to match the validity reported in~\citep{bergmeister2024efficientscalable}. Although the validity estimate is quite noisy, it appears to plateau. We select a model checkpoint at 4.8M steps.
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/sbm-validity-esgg.pdf}
    \caption{SBM validity during training of ESGG on expanded SBM dataset. Validity is computed using 1000 refinement steps in validation but 100 refinement steps during testing to remain consistent with other baselines.}
    \label{fig:sbm-validity-esgg}
\end{figure}
\paragraph{Protein Dataset.} Model selection on the protein graph dataset is challenging, as the MMD metrics computed during validation are noisy, and generating model samples is time-consuming. We take a structured approach and evaluate model checkpoints at 1-4M training steps using the same validation approach as~\citet{bergmeister2024efficientscalable}. Namely, for each graph in the validation set, we generate a corresponding model sample with the same number of nodes. We present the resulting MMD metrics in Table~\ref{tab:validation-esgg}. 
\begin{table}[ht]
    \centering
    \small
    \caption{Validation results of ESGG trained on protein dataset.}
    \include{tables/esgg-protein-model-selection}
    \label{tab:validation-esgg}
\end{table}
Based on these results, we select the model checkpoint at 2M steps. 




\section{Additional Ablations}
\label{appendix:additional-ablations}
\paragraph{Noise Augmentation.} We ablate noise augmentation for the DFS variant of \method, supplementing the results previously reported in Table~\ref{tab:ablations-fused}. We run 100k training steps of stage I with DFS filtrations on the expanded planar graph dataset and compare performance with and without noise augmentation. Consistent with our previous observations, we find in Table~\ref{tab:noise-ablation-dfs} that noise augmentation substantially boosts performance.
\begin{table}[htp]
    \centering
    \small
    \caption{Performance of DFS variant of AFNM after 100k steps of stage I training on expanded planar graph dataset with and without noise augmentation.}
    \include{tables/ablations/dfs-filtration-noise}
    \label{tab:noise-ablation-dfs}
\end{table}

\paragraph{GAN Tuning.} We supplement the results on the effectiveness of adversarial finetuning we presented in Table~\ref{tab:ablations-fused}. In Table~\ref{tab:finetuned-vs-pretrained-planar-dfs}, we present ablation results for the DFS variant on the expanded planar graph dataset. In Tables~\ref{tab:finetuned-vs-pretrained-sbm} and~\ref{tab:finetuned-vs-pretrained-lobster}, we compare models after training stage I and II on the expanded SBM and lobster datasets from Sec.~\ref{subsec:large-synthetic}. Again, we observe that adversarial fine-tuning substantially improves performance in terms of validity and MMD metrics.
\begin{table}[htp]
    \centering
    \small
    \caption{Performance of DFS variant of AFNM after stage I (200k steps) and stage II on expanded planar graph dataset. For results on Fiedler variant, see Table~\ref{tab:ablations-fused}.}
    \include{tables/ablations/pretraining-vs-gan-planar}
    \label{tab:finetuned-vs-pretrained-planar-dfs}
\end{table}
\begin{table}[htp]
    \centering
    \small
    \caption{Performance of \method models after stage I (200k steps) and stage II on expanded SBM dataset. Showing median $\pm$ maximum deviation across three runs for Fiedler variant and a single run for DFS variant. All models attain perfect uniqueness and novelty scores.}
    \include{tables/ablations/pretraining-vs-gan-sbm}
    \label{tab:finetuned-vs-pretrained-sbm}
\end{table}
\begin{table}[htp]
    \centering
    \small
    \caption{Performance of models after stage I (100k steps) and stage II on expanded lobster dataset. Showing median $\pm$ maximum deviation across three runs.}
    \include{tables/ablations/pretraining-vs-van-lobster}
    \label{tab:finetuned-vs-pretrained-lobster}
\end{table}

\paragraph{Filtration Function.} In Table~\ref{tab:edge-weight-ablation}, we study alternative filtration functions. We compare the line fiedler function to centrality-based filtration functions.
Following~\citet{anthonisse1971rush,brandes2008betweenness}, we let $\sigma(i, j)$ denote the number of shortest paths between two nodes $i,j\in V$, and $\sigma(i, j\, |\, e)$ denote the number of these paths passing through an edge $e\in E$. Then, we define the betweenness centrality function as:
        \begin{equation}
            f_{\text{between}}(e) := \sum_{i, j\in V} \frac{\sigma(i, j | e)}{\sigma(i, j)},   \qquad \forall \:e\in E.
        \end{equation}
Based on this, we define the remoteness centrality as $f_\mathrm{remote}(e) = -f_\mathrm{between}(e)$. We use the same filtration scheduling approach as for the line Fiedler function.
We observe that the line fiedler function appears to out-perform the two alternatives in our setting.
\begin{table}[htp]
    \centering
    \small
    \caption{Performance after training stage I with different filtration functions for 100k steps on expanded planar graph dataset. Showing median of three runs for spectral variant and one run each for betweenness and remoteness variants.}
    \include{tables/ablations/edge-weights}
    \label{tab:edge-weight-ablation}
\end{table}
\begin{figure}
    \centering
    \hfill
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth,trim={2.75cm 1cm 0.5cm 0cm}, clip]{figures/edge-weight-functions/planar_graph_fied.pdf}
        \subcaption{Line Fiedler}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth,trim={2.75cm 1cm 0.5cm 0cm}, clip]{figures/edge-weight-functions/planar_graph_dfs.pdf}
        \subcaption{DFS}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth,trim={2.75cm 1cm 0.5cm 0cm}, clip]{figures/edge-weight-functions/planar_graph_btw.pdf}
        \subcaption{Betweenness}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth,trim={2.75cm 1cm 0.5cm 0cm}, clip]{figures/edge-weight-functions/planar_graph_rem.pdf}
        \subcaption{Remoteness}
    \end{subfigure}
    \caption{Visualization of different filtration functions on a planar graph}
    \label{fig:visualization-filtration-functions}
\end{figure}

\paragraph{Scheduling.} 
We recall that the filtration schedule of the line Fiedler variant depends on a mononotonically increasing function $\gamma: [0, 1] \to [0, 1]$. Here, we study the performance of the three choices for $\gamma$: 
\begin{equation}
    \begin{aligned}
        &\mathrm{Linear:} &\quad &\gamma(t) := t\\
        &\mathrm{Convex:}  &\quad &\gamma(t) := 1 - \cos\left(\frac{\pi t}{2}\right) \\
        &\mathrm{Concave:} &\quad & \gamma(t) := \sin\left(\frac{\pi t}{2}\right)
    \end{aligned}
\end{equation}
We present results on the planar graph dataset in Table~\ref{tab:schedule-ablation}.
\begin{table}[htp]
    \centering
    \small
    \caption{Performance after training stage I with different filtration schedules for 100k steps on expanded planar graph dataset with line Fiedler variant. All models attain perfect uniqueness and novelty scores. Showing median of three runs for linear variant and one run each for convex and concave variants.}
    \include{tables/ablations/scheduling}
    \label{tab:schedule-ablation}
\end{table}
We find that no single variant performs consistently best across all evaluation metrics. However, the concave variant attains the highest validity score.

\paragraph{Node Individualization.} In Table~\ref{tab:ablation-individualization}, we study different node individualization techniques for the line Fiedler variant of \method. We refer to the ordering scheme we describe in Appendix~\ref{appendix:node-individualization} as the \emph{derived ordering}, as it is based on the values of the line Fiedler filtration function. Additionally, we study \emph{random orderings} and node orderings according to a \emph{depth first search (DFS)}. Moreover, we compare to node individualizations that do not consist of positional embeddings w.r.t. a node orderings but instead i.i.d. \emph{gaussian noise} that is re-applied in each time-step. Finally, we also consider a variant in which no individualization is applied, i.e., the embedding matrix $W^\mathrm{node}$ is fixed to be all-\emph{zeros}.
\begin{table}[htp]
    \centering
    \small
    \caption{Performance after training stage I of the line Fiedler variant with different node individualization techniques for 100k steps on expanded planar graph dataset. All models attain perfect uniqueness and  novelty scores. Showing median of three runs for derived ordering and one run each for all other variants.}
    \include{tables/ablations/planar-indiv}
    \label{tab:ablation-individualization}
\end{table}
We find that individualizing nodes with learned embeddings based on some ordering (either random, derived from the line Fiedler filtration function, or a DFS search) appears to be beneficial. On the planar graph dataset, there is no clear benefit of the derived ordering over random orderings. However, we observe a clear advantage on the SBM dataset, as can be seen in Table~\ref{tab:ablation-individualization-sbm}.
\begin{table}[htp]
    \centering
    \small
    \caption{Performance after training stage I of the line Fiedler variant with derived and random node ordering after 100k steps on expanded SBM datasets. Showing median $\pm$ maximum deviation across three runs for derived ordering and one run for random ordering.}
    \include{tables/ablations/sbm-indiv}
    \label{tab:ablation-individualization-sbm}
\end{table}

\paragraph{Stage I.} While the ablation study in Sec.~\ref{subsec:ablations} demonstrates that stage II training substantially boosts performance, we now show that, similarly, stage I is crucial too. To this end, we perform stage II training on a very early checkpoint of stage I training. Specifically, we use a checkpoint obtained after 10k steps of stage I training on the expanded planar graph dataset with the line Fiedler variant. We present the results in Table~\ref{tab:stage1-ablation}. We observe that performing stage II training on a premature checkpoint from stage I substantially harms performance. Hence, stage I training is a crucial part of our method.
\begin{table}[htp]
    \centering
    \caption{Performance on expanded planar graph dataset of line Fiedler \method variant with different training durations during stage I. Showing median across three runs for 200k steps and a single run for 10k steps.}
    \include{tables/ablations/stage2-only}
    \label{tab:stage1-ablation}
\end{table}
\FloatBarrier


\section{Bias and Variance of Estimators}
\label{appendix:variance-and-bias}
Previous works~\citep{martinkus2022spectre,vignac2023digress,bergmeister2024efficientscalable} evaluate their graph generative models on as few as 40 samples. In this section, we investigate how this practice impacts the variance and bias of the estimators used in model evaluation and argue that a higher number of test samples should be chosen.

\subsection{Variance of Validity Estimation}
On synthetic datasets such as those introduced in~\citep{martinkus2022spectre}, one may verify whether model samples are "valid", i.e., whether they satisfy a property that is fulfilled by (almost) all samples of the true data distribution. By taking the ratio of valid graphs out of $n$ model samples, previous works have estimated the probability of obtaining valid graphs from the generator.

\FloatBarrier
\begin{definition}
    Let the random variable $G$ denote a sample from a graph generative model and let $\operatorname{valid}: \mathcal{G} \to \{0, 1\}$ a measurable binary function that determines whether a sample is valid. Then the models true validity ratio is defined as:
    \begin{equation}
        \mathbb{P}[\operatorname{valid}(G) = 1]
    \end{equation}
    For i.i.d. samples $G_1,\dots,G_n$, we introduce the following estimator:
    \begin{equation}
        V := \frac{\sum_{i=1}^n \operatorname{valid}(G_i)}{n}
    \end{equation}
\end{definition}
\FloatBarrier

Given the simplicity of the validity metric, we can very easily derive the uncertainty of the estimator used for evaluation. We make this concrete in Proposition~\ref{prop:validity-variance}.
\begin{proposition}
    \label{prop:validity-variance}
    For a generative model with a true validity ratio of $p \in [0, 1]$, the validity estimator on $n$ samples is unbiased and has standard deviation $\sqrt{p(1 -p)} / \sqrt{n}$.
\end{proposition}
\begin{proof}
    Assuming that the random variables $G_1,\dots,G_n$ are i.i.d. samples from the generative model, then the random variables $\operatorname{valid}(G_1),\dots,\operatorname{valid}(G_n)$ are i.i.d. according to $\operatorname{Bernoulli}(p)$. The validity estimator is given as:
    \begin{equation}
        V = \frac{\sum_{i=1}^n \operatorname{valid}(G_i)}{n}
    \end{equation}
    By the linearity of expectation, we have
    \begin{equation}
        \mathbb{E}[V] = \frac{\sum_{i=1}^n \mathbb{E}[\operatorname{valid}(G_i)]}{n} = \frac{np}{p} = p
    \end{equation}
    which shows that the estimator is unbiased. The variance is given by:
    \begin{equation}
    \begin{aligned}
        \Var[V] &= \frac{\Var\left[\sum_{i=1}^n \operatorname{valid}(G_i)\right]}{n^2} = \frac{\sum_{i=1}^n \Var[\operatorname{valid}(G_i)]}{n^2} \\
        &= \frac{p(1-p)}{n}
    \end{aligned}
    \end{equation}
    where we used the independence assumption in the first line. Taking the square root, we obtain the standard deviation from the proposition.
\end{proof}
From Proposition~\ref{prop:validity-variance}, we note that the standard deviation of the validity estimate can be as high as $1/(2\sqrt{n})$, which is achieved at $p=0.5$. For $n=40$, we find that the standard deviation can therefore be as high as $7.9$ percentage points. %

\subsection{Bias and Variance of MMD Estimation}
\begin{definition}
    Let $(\mathcal{X}, d)$ be a metric space and let $k: \mathcal{X} \times \mathcal{X} \to \R$ be a measurable, symmetric kernel which is bounded but not necessarily positive-definite. Let $X := [x_1, \dots, x_n]$ be i.i.d. samples from a Borel distribution $p_x$ on $\mathcal{X}$ and $Y:=[y_1, \dots, y_n]$ be i.i.d. samples from a distribution $p_y$. Assume $X$ and $Y$ to be independent. 
    Following~\citep{gretton2012mmd}, define the squared MMD of $p_x$ and $p_y$ as:
    \begin{equation}
        \mathrm{MMD}^2(p_x, p_y) := \mathbb{E}[k(x_1, x_2)] + \mathbb{E}[k(y_1, y_2)] - 2\mathbb{E}[k(x_1, y_1)]
    \end{equation}
    and note that this is well-defined by our assumptions.
    Finally, introduce the following estimator for the squared MMD:
    \begin{equation}
        M := \frac{1}{n^2}\sum_{i,j=1}^n k(x_i, x_j) + \frac{1}{m^2}\sum_{i,j=1}^m k(y_i, y_j) - \frac{2}{nm}\sum_{i=1}^n\sum_{j=1}^m k(x_i, y_j)
    \end{equation}
\end{definition}


We empirically study bias and variance of the MMD estimates on the planar graph dataset. We generate 8192 samples from one of our trained model and repeatedly compute the MMD between the test set and a random subset of those samples. We vary the size of the random subsets and run 64 evaluations for each size, computing mean and standard deviation of the MMD metrics across the 64 evaluations. We report the results in Table~\ref{tab:mmd-variance}.
\begin{table}[ht]
    \centering
    \small
    \caption{Mean MMD $\pm$ standard deviation across 64 evaluation runs of a single model. The test set contains 256 planar graphs, while a varying number of model samples is used, as indicated on the left. The MMD and its variance decrease substantially with larger numbers of model samples.}
    \input{tables/table-mmd-variance}
    \label{tab:mmd-variance}
\end{table}
We observe that on average the MMD is severly over-estimated when using fewer than 256 model samples. At the same time, the variance between evaluation runs is large when few samples are used, making the results unreliable.

\FloatBarrier
\section{Adversarial Finetuning Details} 
\label{appendix:adversarial-finetuning}
We provide pseudocode for the adversarial fine-tuning stage in Algorithm~\ref{alg:adversarial-finetuning}. We note that we do not make all procedures explicit and that many hyper-parameters must be chosen (including the number of steps and epochs in $\textsc{TrainGeneratorAndValueModel}$). 

\paragraph{Generator.} The generator operates in inference mode, meaning that all dropout layers are disabled and batch normalization modules utilize the (now frozen) moving averages from training stage I. Hence, the behavior of the generative model becomes reproducible. It acts as a stochastic policy in a higher-order MDP, where the graphs $G_0, \dots, G_T$ are the states. It receives a terminal reward for the plausibility of the final sample $G_T$.   

\paragraph{Discriminator.} The discriminator is implemented as a GraphGPS~\citep{rampasek2022graphgps} model which performs binary classification on graph samples $G_T$, distinguishing real samples from generated samples. It is trained via binary cross-entropy on batches consisting in equal proportions of generated graphs and graphs from the dataset $\mathcal{D}$. For a given graph $G_T$, the discriminator produces a probability of "realness" by applying the sigmoid function to its logit. Following SeqGAN~\citep{yu2017seqgan}, the log-sigmoid of the logit then acts as a terminal reward for the generative model. We emphasize that only the final graph $G_T$ is presented to the discriminator. 

\paragraph{Value Model.} The value model uses the same backbone architecture as our generative model and regresses scalars from pooled node representations. It is trained via least squares regression. The value model is used to compute baselined reward-to-go values.

\paragraph{Training Outline.} While Algorithm~\ref{alg:adversarial-finetuning} provides a technical description of the training algorithm, we also provide a rougher outline here. At the start of training stage II, the generator is initialized with the weights learned in training stage I, while the discriminator and value model are initialized randomly. Before entering the main training loop, we pre-train discriminator and value model to match the generator. Namely, we first pre-train the discriminator to classify graphs as either "real" or "generated". The log-likelihood of "realness" acts as a terminal reward of the generative model. The discriminator is then pre-trained to regress the reward-to-go. After pre-training is finished, we proceed to the training loop, which consists of alternating training of (i) the generator and value model and (ii) the discriminator. As described above, the generator is trained via PPO to maximize the terminal reward provided by the discriminator. The value model is used to baseline the reward and is continuously trained to regress the reward-to-go. The discriminator, on the other hand, continues to be trained on generated and real graph samples via binary cross-entropy.


\begin{algorithm}[p]
\caption{Adversarial Finetuning}\label{alg:adversarial-finetuning}
\begin{algorithmic}

\Procedure{TrainGeneratorAndValueModel}{$p_\theta$, $d_\varphi$, $v_\vartheta$}

\For{$i = 1\dots,N_\mathrm{steps}$}
    \State $\mathcal{S} \gets [\:]$                    \Comment{List of sampled filtrations}
    \State $r \gets 0 \: \in \: \R^{N_\mathrm{samples}}$         \Comment{Terminal rewards}
    \For{$j=1\dots,N_\mathrm{samples}$}
        \State $G_0^{(j)}, \dots, G_T^{(j)} \gets \Call{SampleFiltration}{p_{\theta}}$
        \State $\mathcal{S}\operatorname{.append}\left(\left(G_0^{(j)}, \dots, G_T^{(j)}\right)\right)$
        \State $r_j \gets \operatorname{logsigmoid}(d_\varphi(G_T^{(j)}))$
        \State $r_j \gets \max(r_j, R_\mathrm{lower})$          \Comment{Reward clamping}
    \EndFor
    \State $r \gets \Call{Whiten}{r}$           \Comment{Whiten rewards using EMA of mean and std}
    \State $g_{j, t} \gets 0 \qquad \forall j=1, \dots, N_\mathrm{samples} \: \forall t=0, \dots, T-1$  \Comment{Rewards-to-go}
    \For{$j=1\dots,N_\mathrm{samples}$}
        \For{$t=0, \dots, T-1$}
            \State $g_{j, t} \gets r_j - v_\vartheta(G_0^{(j)}, \dots, G_{t}^{(j)})$      \Comment{Compute baselined RTG}
        \EndFor
    \EndFor
    \State $\Call{TrainValueModel}{v_\vartheta, \mathcal{S}, r}$
    \For{$k=1\dots,N_\mathrm{epoch}$}
        \State $l_{j, t}^{(k)} \gets -\log p_\theta (G_t^{(j)} | G_{t-1}^{(j)}, \dots, G_0^{(j)}) \qquad \forall j=1, \dots,N_\mathrm{samples} \quad \forall t=1, \dots, T$
        \State $u_{j, t} \gets \exp(\operatorname{sg}[l_{j, t}^{(1)}] - l_{j, t}^{(k)}) \qquad \forall j, t$
        \State $\mathcal{L}_{j, t}^{(1)} \gets - u_{j, t} \cdot g_{j, t-1} \qquad \forall j, t$
        \State $\mathcal{L}_{j, t}^{(2)} \gets - \operatorname{clamp}(u_{j, t}, 1 - \epsilon, 1 + \epsilon) \cdot g_{j, t-1} \qquad \forall j, t$
        \State $\mathcal{L} \gets \sum_{j, t} \max(\mathcal{L}_{j, t}^{(1)}, \mathcal{L}_{j, t}^{(2)})$
        \State $\theta \gets \theta - \delta \nabla_\theta \mathcal{L}$            \Comment{Backpropagate and update parameters}
    \EndFor
\EndFor
\EndProcedure


\bigskip

\Procedure{GANTuning}{$p_\theta$, $\mathcal{D}$}  \Comment{Takes generator from training stage I and graph dataset}
\State $d_\varphi \gets $ new GNN         \Comment{Initialize discriminator}
\State $\Call{TrainDiscriminator}{p_\theta, d_\varphi, \mathcal{D}}$           \Comment{Pre-train discriminator}
\State $v_\vartheta \gets$ new mixer model        
\State $\mathcal{S} \gets \Call{GenerateFiltrations}{p_\theta}$
\State $r \gets \Call{GradeSamples}{\mathcal{S}, d_\varphi}$
\State $\Call{TrainValueModel}{v_\vartheta, \mathcal{S}, r}$            \Comment{Pre-train value model}
\While{not converged}
    \State $\Call{TrainGeneratorAndValueModel}{p_\theta, d_\varphi, v_\vartheta}$
    \State $\Call{TrainDiscriminator}{p_\theta, d_\varphi, \mathcal{D}}$
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}




\end{document}
