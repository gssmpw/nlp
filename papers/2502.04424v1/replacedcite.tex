\section{Related Work}
\label{app:related}
\subsection{Multimodal Large Language Models}
With the success of LLMs in various natural language processing (NLP) tasks, such as reasoning ____ and euphemism detection ____, numerous efforts have been made to extend LLMs to multimodal areas, i.e., MLLMs, enabling them to process additional types of information, including images, videos, and audio ____. MLLMs excel in multimodal perception and reasoning and handle more diverse tasks with inputs from different multimodality ____. 
For instance, Qwen2-Audio ____ specializes in integrating audio and text, demonstrating strong performance in auditory perception tasks. 
The MiniCPM-V ____, LongVA ____, GLM ____, InternVL ____, and InternVideo2 ____ have made significant strides in vision understanding and multimodal dialogue generation.
Video-LLaMA2 ____ not only focuses on vision understanding but also enhances audio-video understanding capabilities. Additionally, the Gemini ____, an LLM natively supporting multimodal capabilities, can seamlessly understand, manipulate, and integrate information from different modalities. 
Moreover, some works further improve vision reasoning ability in MLLMs by visual dependency ____, in-context learning ____.


\subsection{Evaluation of Emotional Intelligence}
Given that EI is essential for understanding and responding to human emotions, many studies have focused on evaluating the EI capabilities of LLMs. MERBench ____ standardizes evaluation for multimodal emotion recognition by addressing inconsistencies in feature extractors and offering a unified framework. It introduces MER2023 ____, a dataset focused on the Chinese language, emphasizing multi-label learning and robustness analysis.
Moreover, MC-EIU ____ offers a joint evaluation of emotion and intent in multimodal conversations.
MOSABench ____ introduces a novel method for multi-object sentiment analysis, emphasizing the challenges MLLMs face in handling spatial complexities.
____ evaluates GPT-4's visual capabilities in emotion recognition tasks but reveals limitations in recognizing micro-expressions and leveraging temporal data effectively. EmotionBench ____ employs emotional appraisal theory to evaluate LLMs, exposing misalignments between LLM responses and human emotional behaviors. To deep dive into EI of LLM,  EIBench ____ and EmoBench ____ are based on established psychological theories to evaluate LLM with various EI tasks, and they expose significant gaps between current LLMs and human-like emotional intelligence. In addition, EQ-Bench ____ and SOUL ____ focus on nuanced EI aspects, including emotion intensity prediction and justification generation, revealing performance disparities between small and large models. 



\subsection{Multimodal Benchmarks for LLMs}
The rapid development of Multimodal Large Language Models (MLLMs) has necessitated the creation of diverse benchmarks to systematically evaluate their capabilities across perception, reasoning, and application domains. MME ____ and MMT-Bench ____ serve as comprehensive benchmarks for foundational multimodal tasks and general-purpose intelligence across varied domains. MultiTrust ____ evaluates trustworthiness, focusing on truthfulness, safety, robustness, fairness, and privacy risks. Moreover, HumanVBench ____ and MVBench ____ center on human-centric and temporal understanding in video content, exposing gaps in MLLMs' abilities to align cross-modal and temporal dynamics effectively. Specialized benchmarks have emerged to tackle domain-specific challenges. For instance, MathScape ____ targets multimodal mathematical reasoning, while M3SciQA ____ focuses on scientific question answering. BenchLMM ____ evaluates models under diverse style shifts, and BLINK ____ targets core visual perception tasks that remain challenging for multimodal models. ____ evaluate the medical diagnostic capabilities and generalization abilities of LVLMs. Furthermore, SEED-Bench-2-Plus ____ assesses MLLMs' abilities in text-rich visual scenarios, such as interpreting charts and maps. 






\begin{table*}[!t]\small
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Evaluation Scenario} & \textbf{Data Source} & \textbf{Task} & \textbf{num} & \textbf{Metric} \\
\hline\midrule
\rowcolor{gray!25}\multicolumn{5}{c}{\textbf{Foundational Emotion Recognition}} \\\midrule
Song Emotion Recognition & RAVDESS(song) ____ & 6-CLS & 500 & ACC, WAF \\
Speech Emotion Recognition & RAVDESS(speech) ____ & 8-CLS & 500 & ACC, WAF \\
Opinion Sentiment Analysis & CMU-MOSI ____ & 3-CLS & 500 & ACC, WAF \\
Emotion Intensity Analysis & CMU-MOSEI ____ & 3-CLS & 500 & ACC, WAF \\
Stock Comment Emotion Analysis & FMSA-SC ____ & 5-CLS & 250 & ACC, WAF \\
\hline\midrule
\rowcolor{gray!25}\multicolumn{5}{c}{\textbf{Conversational Emotion Understanding}} \\\midrule
Fine-Grained Dialog Emotion Analysis & MER2023 ____ & 6-CLS & 411 & ACC, WAF \\
Presentation Emotion Analysis & CH-SIMSv2 ____ & 3-CLS & 500 & ACC, WAF \\
Face-Centric Dialog Emotion Analysis & CH-SIMS ____ & 3-CLS & 457 & ACC, WAF \\
Conversational Emotion \& Intent Analysis & MC-EIU ____ & 7-\&8-CLS & 500 & ACC, WAF \\
Multi-Party Dialog Emotion Recognition & MELD ____ & 7-CLS & 500 & ACC, WAF \\
\hline\midrule
\rowcolor{gray!25}\multicolumn{5}{c}{\textbf{Socially Complex Emotion Analysis}} \\\midrule
Humor Understanding & UR-FUNNY ____ & 2-CLS & 448 & ACC, WAF \\
Sarcasm Detection & MUStARD ____ & 2-CLS & 500 & ACC, WAF \\
Laughter Reasoning & SMILE ____ & GEN & 80 & LLM \\
\bottomrule
\end{tabular}}
\vspace{-2mm}
\caption{\small Emotion Recognition Tasks and Metrics. ``n-CLS'' denotes an n-class classification task, and ``GEN'' represents a generation task. ``num'' indicates the number of samples. ``ACC'' and ``WAF'' denote accuracy and Weighted Average F-score ____, respectively. For the ``Laughter Reasoning'', we employ a LLM as the evaluator, and evaluation prompts are shown in Appendix~\ref{app:evalprompt}. To ensure fair and consistent comparisons in future research, we adopted the open-source Qwen2.5-72B-Instruct ____. Details of the specific categories in the classification tasks are provided in the Appendix~\ref{app:dataset}. Details of prompt are in Appendix~\ref{app:prompt}.} 
\label{tab:emotion_tasks}
\vspace{-2mm}
\end{table*}

%