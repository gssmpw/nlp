\section{Related Work}
\label{app:related}
\subsection{Multimodal Large Language Models}
With the success of LLMs in various natural language processing (NLP) tasks, such as reasoning \cite{ThoT} and euphemism detection \cite{Euphemism}, numerous efforts have been made to extend LLMs to multimodal areas, i.e., MLLMs, enabling them to process additional types of information, including images, videos, and audio \cite{VideoLLaMA_2,MiniCPM-V}. MLLMs excel in multimodal perception and reasoning and handle more diverse tasks with inputs from different multimodality \cite{MiniGPT4,InternVideo2}. 
For instance, Qwen2-Audio \cite{Qwen2-Audio} specializes in integrating audio and text, demonstrating strong performance in auditory perception tasks. 
The MiniCPM-V \cite{MiniCPM-V}, LongVA \cite{LongVA}, GLM \cite{GLM-4}, InternVL \cite{InternVL}, and InternVideo2 \cite{InternVideo2} have made significant strides in vision understanding and multimodal dialogue generation.
Video-LLaMA2 \cite{VideoLLaMA_2} not only focuses on vision understanding but also enhances audio-video understanding capabilities. Additionally, the Gemini \cite{Gemini1, Gemini, Gemini2}, an LLM natively supporting multimodal capabilities, can seamlessly understand, manipulate, and integrate information from different modalities. 
Moreover, some works further improve vision reasoning ability in MLLMs by visual dependency \cite{VisualDependency}, in-context learning \cite{VICL}.


\subsection{Evaluation of Emotional Intelligence}
Given that EI is essential for understanding and responding to human emotions, many studies have focused on evaluating the EI capabilities of LLMs. MERBench \cite{MERBench} standardizes evaluation for multimodal emotion recognition by addressing inconsistencies in feature extractors and offering a unified framework. It introduces MER2023 \cite{mer2023}, a dataset focused on the Chinese language, emphasizing multi-label learning and robustness analysis.
Moreover, MC-EIU \cite{MC-EIU} offers a joint evaluation of emotion and intent in multimodal conversations.
MOSABench \cite{MOSABench} introduces a novel method for multi-object sentiment analysis, emphasizing the challenges MLLMs face in handling spatial complexities.
\citet{GPT-4V} evaluates GPT-4's visual capabilities in emotion recognition tasks but reveals limitations in recognizing micro-expressions and leveraging temporal data effectively. EmotionBench \cite{EmotionBench} employs emotional appraisal theory to evaluate LLMs, exposing misalignments between LLM responses and human emotional behaviors. To deep dive into EI of LLM,  EIBench \cite{Both_Matter} and EmoBench \cite{EmoBench} are based on established psychological theories to evaluate LLM with various EI tasks, and they expose significant gaps between current LLMs and human-like emotional intelligence. In addition, EQ-Bench \cite{EQ-Bench} and SOUL \cite{SOUL} focus on nuanced EI aspects, including emotion intensity prediction and justification generation, revealing performance disparities between small and large models. 



\subsection{Multimodal Benchmarks for LLMs}
The rapid development of Multimodal Large Language Models (MLLMs) has necessitated the creation of diverse benchmarks to systematically evaluate their capabilities across perception, reasoning, and application domains. MME \cite{MME} and MMT-Bench \cite{MMT-Bench} serve as comprehensive benchmarks for foundational multimodal tasks and general-purpose intelligence across varied domains. MultiTrust \cite{MultiTrust} evaluates trustworthiness, focusing on truthfulness, safety, robustness, fairness, and privacy risks. Moreover, HumanVBench \cite{humanvbench} and MVBench \cite{MVBench} center on human-centric and temporal understanding in video content, exposing gaps in MLLMs' abilities to align cross-modal and temporal dynamics effectively. Specialized benchmarks have emerged to tackle domain-specific challenges. For instance, MathScape \cite{MathScape} targets multimodal mathematical reasoning, while M3SciQA \cite{M3SciQA} focuses on scientific question answering. BenchLMM \cite{BenchLMM} evaluates models under diverse style shifts, and BLINK \cite{BLINK} targets core visual perception tasks that remain challenging for multimodal models. \citet{MedLVLM} evaluate the medical diagnostic capabilities and generalization abilities of LVLMs. Furthermore, SEED-Bench-2-Plus \cite{SEED-Bench-2-Plus} assesses MLLMs' abilities in text-rich visual scenarios, such as interpreting charts and maps. 






\begin{table*}[!t]\small
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Evaluation Scenario} & \textbf{Data Source} & \textbf{Task} & \textbf{num} & \textbf{Metric} \\
\hline\midrule
\rowcolor{gray!25}\multicolumn{5}{c}{\textbf{Foundational Emotion Recognition}} \\\midrule
Song Emotion Recognition & RAVDESS(song) \cite{ryerson} & 6-CLS & 500 & ACC, WAF \\
Speech Emotion Recognition & RAVDESS(speech) \cite{ryerson} & 8-CLS & 500 & ACC, WAF \\
Opinion Sentiment Analysis & CMU-MOSI \cite{MOSI} & 3-CLS & 500 & ACC, WAF \\
Emotion Intensity Analysis & CMU-MOSEI \cite{CMU-MOSEI} & 3-CLS & 500 & ACC, WAF \\
Stock Comment Emotion Analysis & FMSA-SC \cite{FMSA-SC} & 5-CLS & 250 & ACC, WAF \\
\hline\midrule
\rowcolor{gray!25}\multicolumn{5}{c}{\textbf{Conversational Emotion Understanding}} \\\midrule
Fine-Grained Dialog Emotion Analysis & MER2023 \cite{mer2023} & 6-CLS & 411 & ACC, WAF \\
Presentation Emotion Analysis & CH-SIMSv2 \cite{CH-SIMSv2} & 3-CLS & 500 & ACC, WAF \\
Face-Centric Dialog Emotion Analysis & CH-SIMS \cite{CH-SIMS} & 3-CLS & 457 & ACC, WAF \\
Conversational Emotion \& Intent Analysis & MC-EIU \cite{MC-EIU} & 7-\&8-CLS & 500 & ACC, WAF \\
Multi-Party Dialog Emotion Recognition & MELD \cite{MELD} & 7-CLS & 500 & ACC, WAF \\
\hline\midrule
\rowcolor{gray!25}\multicolumn{5}{c}{\textbf{Socially Complex Emotion Analysis}} \\\midrule
Humor Understanding & UR-FUNNY \cite{UR-FUNNY} & 2-CLS & 448 & ACC, WAF \\
Sarcasm Detection & MUStARD \cite{MUStARD} & 2-CLS & 500 & ACC, WAF \\
Laughter Reasoning & SMILE \cite{SMILE} & GEN & 80 & LLM \\
\bottomrule
\end{tabular}}
\vspace{-2mm}
\caption{\small Emotion Recognition Tasks and Metrics. ``n-CLS'' denotes an n-class classification task, and ``GEN'' represents a generation task. ``num'' indicates the number of samples. ``ACC'' and ``WAF'' denote accuracy and Weighted Average F-score \cite{MELD}, respectively. For the ``Laughter Reasoning'', we employ a LLM as the evaluator, and evaluation prompts are shown in Appendix~\ref{app:evalprompt}. To ensure fair and consistent comparisons in future research, we adopted the open-source Qwen2.5-72B-Instruct \cite{yang2024qwen2}. Details of the specific categories in the classification tasks are provided in the Appendix~\ref{app:dataset}. Details of prompt are in Appendix~\ref{app:prompt}.} 
\label{tab:emotion_tasks}
\vspace{-2mm}
\end{table*}

%