%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage{natbib}
\usepackage{cite}
\pdfpagewidth=8.5in
\pdfpageheight=11in
\usepackage{microtype}
\usepackage[usenames,svgnames]{xcolor}
% \usepackage[colorlinks, citecolor=NavyBlue, linkcolor=NavyBlue, urlcolor=NavyBlue]{hyperref}
\usepackage{setspace}
\usepackage{booktabs}
% \usepackage[switch]{lineno}
\usepackage{mydef}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{framed}
\usepackage{bbding}
\usepackage{subfloat}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{multirow}


% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Connector-S: A Survey of Connectors in Multi-modal Large Language Models}


% Single author syntax
\author{
Xun Zhu~\textsuperscript{1, \dag} \and
Zheng Zhang~\textsuperscript{1,\dag} \and
Xi Chen~\textsuperscript{1, \dag} \and
Yiming Shi~\textsuperscript{1} \and
Miao Li~\textsuperscript{1, \ding{41}}\And
Ji Wu~\textsuperscript{1, 2, 3, 4, \ding{41}}
\affiliations
\textsuperscript{1} Department of Electronic Engineering, Tsinghua University \quad
\textsuperscript{2} College of AI, Tsinghua University \\
\textsuperscript{3} Beijing National Research Center for Information Science and Technology \\
\textsuperscript{4} Center for Big Data and Clinical Research, Institute for Precision Medicine, Tsinghua University \\
\dag\ Equal Contribution \qquad
\ding{41} Corresponding Author
\emails
\{zhu-x24, zzhang24, chenxi24, sym23\}@mails.tsinghua.edu.cn,
\{miao-li, wuji\_ee\}@tsinghua.edu.cn
%\footnote{\dag\ Equal contribution. \ding{41} Corresponding author.}
}


% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
\iffalse
\author{
First Author$^1$
\and
Second Author$^2$\and
Third Author$^{2,3}$\And
Fourth Author$^4$\\
\affiliations
$^1$First Affiliation\\
$^2$Second Affiliation\\
$^3$Third Affiliation\\
$^4$Fourth Affiliation\\
\emails
\{first, second\}@example.com,
third@other.example.com,
fourth@example.com
}
\fi

\begin{document}

\maketitle

\begin{abstract} 
With the rapid advancements in multi-modal large language models (MLLMs), connectors play a pivotal role in bridging diverse modalities and enhancing model performance.
However, the design and evolution of connectors have not been comprehensively analyzed, leaving gaps in understanding how these components function and hindering the development of more powerful connectors.
In this survey, we systematically review the current progress of connectors in MLLMs and present a structured taxonomy that categorizes connectors into atomic operations (mapping, compression, mixture of experts) and holistic designs (multi-layer, multi-encoder, multi-modal scenarios), highlighting their technical contributions and advancements. 
Furthermore, we discuss several promising research frontiers and challenges, including high-resolution input, dynamic compression, guide information selection, combination strategy, and interpretability.
This survey is intended to serve as a foundational reference and a clear roadmap for researchers, providing valuable insights into the design and optimization of next-generation connectors to enhance the performance and adaptability of MLLMs.

\end{abstract}

\section{Introduction}
With the remarkable advancements in large language models (LLMs) propelling progress towards general-purpose AI, there has been a significant growing focus on extending these models to multi-modal domains, leading to the development of multi-model large language models (MLLMs).
The current mainstream MLLM architectures follow a similar paradigm~\citep{zhang2024mm}:
the modality encoder, which compresses raw information, such as image or audio, into a more compact representation;
the connector, which alleviates the gap between modalities, thus facilitating the adaptation of multi-modal inputs to LLMs;
the LLM backbone, which generates the text responses in an auto-regressive manner;
and the generator, which is optional for generating more modalities output besides text.

Rather than training from scratch, a common and logical approach is to start with a pre-trained encoder and a pre-trained LLM, aiming to enhance the efficacy of modality representation and mitigate computational expenses of LLM pre-training, respectively~\citep{yin2023survey}.
In the above paradigm, the connector plays a crucial role in aligning the multi-modal inputs into a consistent form and space~\citep{cha2024honeybee}:
On the one hand, the connector bridges modalities and the language model by translating modality features into tokens that the language model can understand.
The quality of the conveyed tokens directly impacts the overall performance of the MLLM.
On the other hand, compared to the fixed architectures of encoders and LLMs, the connector is more lightweight and can be easily improved, offering greater flexibility and efficiency.

In one of the pioneering works in MLLMs, LLaVA~\citep{liu2024visual}, the connector is merely a simple linear layer that provides basic mapping functionality.
As the demand for more powerful capabilities and the complexity of scenarios increased, advanced connectors incorporate various mechanisms, such as improved mapping~\citep{liu2024improved}, compression based on spatial relation~\citep{chen2023minigpt} or semantic perception~\citep{li2023blip}, and mixture of experts (MoE)~\citep{li2025cumo} to better handle the intricacies of multi-modal inputs.
In addition, Dense Connector~\citep{yao2024dense} and Uni-Med~\citep{zhu2024uni} have optimized multi-layer feature utilization and multi-task joint learning conflicts by exploring connector design, respectively.
The evolution reflects increasing recognition of the key role of connectors in improving the overall performance of MLLM and adaptability to various complex scenarios.

\paragraph{Motivations.}
With the widespread application of MLLMs in various domains, the design of the connector is constantly updated to meet the increasing demands for more powerful capabilities and complex scenarios.
\citet{song2023bridge} has explored modality alignment methods for MLLMs, due to time constraints, many methods and scenarios have not yet appeared and been discussed.
Several studies~\citep{yin2023survey,zhang2024mm} have conducted surveys on the entire flow of MLLMs, while offering relatively brief and insufficient coverage of connectors.
The lack of a comprehensive survey on connectors makes it hard for researchers to establish a clear cognition of the underlying mechanisms of different connectors, thereby impeding the development of next-generation connectors.
Therefore, we propose a more fine-grained taxonomy to systematically review and summarize the current status of connectors in MLLMs.

\input{figures/f1}

\paragraph{Contributions.}
The contributions of this work can be summarized from the following three aspects:
(1) \textit{A structured taxonomy}.
A broad overview of the field is presented with a structured taxonomy that covers atomic operations and holistic designs.
(2) \textit{A comprehensive review}. 
Based on the proposed taxonomy, the current research progress of connectors in MLLMs is systematically delineated.
(3) \textit{Some research frontiers and directions}. 
We discuss promising research frontiers and point out possible future directions.


\section{Taxonomy}
To enhance our understanding of the dynamic evolution of connectors in MLLMs, we identify pivotal research endeavors, analyze their motivations, and succinctly encapsulate their primary technical contributions. 
As illustrated in Figure~\ref{f1}, this survey establishes a new taxonomy, which firstly examines the connector design of these works from two different perspectives, i.e., atomic operations in basic scenarios and holistic designs in complex scenarios.
Then, we briefly introduce these two perspectives as follows:
\begin{itemize}
\item \textbf{Atomic connector operations} refer to the basic components of MLLM connectors, which are designed as simple yet versatile units tailored to different functional requirements of basic scenarios.
By utilizing these atomic operations, connectors can achieve mapping, compression, and expert integration.
Furthermore, they can be combined to create more complex connectors, bridging the modality gap in a targeted and flexible way.
\item \textbf{Holistic connector designs} focus on the challenges of enhancing the capabilities of MLLMs in sophisticated scenarios from the multi-layer features, multi-encoder outputs to multi-modal contexts.
Effective holistic designs help MLLMs consider the interplay between different layers of visual features, combine insights from different visual encoders, and handle diverse modalities in a coherent and efficient manner.
\end{itemize}
In the following sections, we present a comprehensive survey along the two perspectives and corresponding categories of our taxonomy for connectors in MLLMs.
Due to some MLLMs possibly employing multiple methods in connectors, there may be instances of overlap among these models.
Finally, based on the previous summary, we open the discussion for challenges and opportunities of future connector research, including high-resolution input, dynamic compression, guide information selection, combination strategy, and interpretability.


\section{Atomic Connector Operations}
\subsection{Mapping}
Since the LLM backbones are primarily trained on generic text, there is an inevitable semantic gap exists when dealing with multi-modal features. 
Mapping operations first flatten 2D or 3D features into 1D in a specific order and directly align the dimension of representations from other modalities with textual token embeddings.

\paragraph{Linear.}
As the basic connector method, the simplicity of linear mapping makes it an attractive choice for initial feature transformation.
LLaVA~\citep{liu2024visual} uses a linear projection matrix to connect the representation produced by the visual encoder to the LLM backbone.
mPLUG-Owl3~\citep{ye2024mplug} still adopts a linear layer to further process long visual sequence inputs, while Vitron~\citep{fei2024vitron} utilizes three independent linear mappings to process image, video, and sketch features, respectively.

\paragraph{MLP.}
The multi-layer perceptron (MLP) consist of multiple linear layers connected by activation functions, introducing non-linearity into the feature transformation process.
LLaVA-1.5~\citep{liu2024improved} finds that changing from the original linear projection to MLP can improve LLaVA's multi-modal capabilities.
Yi-VL~\citep{young2024yi} and 3DMIT~\citep{li20243dmit} use a two-layer MLP and a three-layer MLP with layer normalizations to align 2D image features and 3D object features, respectively.

The primary advantages of mapping operations lie in the simplicity and lightweight nature, which facilitates fast convergence during alignment training.
However, mapping operations are limited in their ability to compress redundant information, which may lead to long token sequences that impose computational burden on the subsequent LLM backbone.

\subsection{Compression}
The complete set of tokens derived from other modalities encompasses both useful and redundant information.
To achieve the optimal balance between information representation and the number of tokens, it is crucial to implement efficient compression operations through the connector.
The current mainstream compression operations can be categorized into two main types: spatial relation and semantic perception.

\subsubsection{Spatial Relation}
The core idea behind spatial relation compression is rooted in observation: features from adjacent regions tend to be more similar in the original modality representation. 
Leveraging the spatial proximity, this category of compression operations can be further divided into several subcategories:

\paragraph{Simple Operation.}
Pooling and dimensionality reduction via token concatenation and projection both serve as simple operations to compress the number of tokens.
For example, MiniGPT-v2~\citep{chen2023minigpt} simply concatenates 4 adjacent visual tokens in the embedding space and projects them together into one single token with the textual embedding dimension.
PLLaVA~\citep{xu2024pllava} processes video features with the average pooling strategy to smooth the feature distribution along the temporal dimension.
DeCo~\citep{yao2024deco} compresses $N$ visual tokens by reshaping them into a 2D tensor of size $\left(N^{1/2}, N^{1/2}\right)$, applying adaptive average pooling to reduce the size to $\left(M^{1/2}, M^{1/2}\right)$, flattening into $M$ tokens, and projecting through a linear layer to match the textual embedding dimension.

\paragraph{CNN.}
Convolutional neural networks (CNNs) can preserve local spatial structures and enhance spatial understanding through hierarchical feature extraction.
For instance, Honeybee~\citep{cha2024honeybee} proposes C-Abstractor that consists of stacked ResNet blocks and adaptive pooling, injecting locality-aware design into the compression process.
MM1~\citep{mckinzie2025mm1} adopts C-Abstractor as the connector, verifying the effectiveness of the connector design compared with average pooling and attention pooling.

\paragraph{Variants.}
Considering that CNN introduces overly strict inductive biases for locality, Honeybee~\citep{cha2024honeybee} also presents D-Abstractor,
which enhances the locality-awareness while maintaining flexibility based on deformable attention. 
To avoid feature mismatch, MoME~\citep{shen2024mome} proposes adaptive deformable transformation, which combines adaptive average pooling and deformable attention to obtain compressed and self-enhanced visual features.

\subsubsection{Semantic Perception}
Features with similar semantics always tend to exhibit stronger correlations across modalities.
By focusing on semantic content rather than just spatial proximity, semantic perception compression aims to extract and preserve the most semantically relevant information while reducing redundancy.
Leveraging semantic understanding, this category can be further divided into several subcategories:

\paragraph{Q-Former.}
First proposed in BLIP-2 \citep{li2023blip}, Q-Former introduces a fixed number of learnable tokens as queries, which retrieve relevant information from other modality representations through cross-attention.
Since Q-Former involves more parameters and typically requires additional training, some studies use the pre-trained Q-Former to continue training for specific tasks. 
MiniGPT-4~\citep{zhu2023minigpt} utilizes the frozen Q-Former with a linear layer to enhance its visual understanding capabilities.
PlanLLM~\citep{yang2024planllm} promotes video procedure planning by continuing to train Q-Former, integrating sample-specific visual state information and textual step knowledge.

\paragraph{Resampler.}
First introduced in Flamingo~\citep{alayrac2022flamingo}, resampler
insert additional cross-attention layers between pre-trained and frozen LLM layers, whre the keys and values are obtained from the compressed vision features while the queries are derived from the language inputs.
Voila-A~\citep{yan2023voila} further adapts resampler by using the gaze heatmap features as the keys within the attention mechanism, optimizing feature expression.
InfiMM~\citep{liu2024infimm} keeps the resampler trainable in three-stage training strategies for vision-language alignment, visual question answering knowledge injection and unreshing conversation ability.

\paragraph{Variants.}
To reduce the computational and memory complexity of the transformer, Q-Mamba \citep{eom2024query} introduces a bidirectional Mamba layer to process visual features from a pre-trained vision encoder. These features are then projected into a fixed number of learnable queries using cross-attention, enhanced by a causal Mamba prior.
ParGo~\citep{wang2024pargo} employs learnable local and global tokens to extract information separately, enhancing regional relationships while controlling token count.

Compression operations help distill essential information while eliminating redundancy, thus improving both the efficiency and effectiveness of the model.
Spatial relation compression provides simple and direct, albeit somewhat crude methods for reducing token count.
Semantic perception compression offers deeper semantic understanding at the cost of increased complexity and training requirements.
% Balancing these trade-offs is crucial for optimizing the performance of MLLMs in various applications.
% attention-based modules may result in the loss of important information.

\subsection{Mixture of Experts}
Mixture of Experts (MoE) is a powerful architectural paradigm composed of two key components: a set of experts, each specializing in distinct aspects of the multi-modal features, and a router that dynamically selects and combines their outputs according to the input guidance information.
With the shared goal of achieving optimal alignment of multi-modal features, compression operations focus on extracting a minimal number of tokens, while MoE aims to leverage the specialized representations of multiple experts.
MoE's ability to capture a wide range of features and patterns makes it particularly suitable for the complex and varied nature of multi-modal data. 

\subsubsection{Vanilla MoE}
Vanilla MoE refers to a standard MoE framework where each multi-modal token is independently processed by the router to compute expert assignment weights. 
CuMo~\citep{li2025cumo} employs four expert models, each consisting of a two-layer MLP. 
The router is a linear layer that computes expert weights based on each input token, applying a top-2 gating mechanism.
This means that only the top two experts with the highest weights are selected for each token, allowing the connector to focus on the most relevant information while maintaining computational efficiency.
ChartMoE~\citep{xu2024chartmoe} adopts four experts of the linear layer with a top-2 gating router, achieving distinct preferences and effective processes in the fused information of background, table, json and code.
To enhance surgical multi-modal understanding, SurgFC~\citep{chen2024surgfc} extends the vanilla MoE connector to the medical domain, utilizing MLP for both experts and router.

\subsubsection{X-Guided MoE}
X-guided MoE expands the input to the router beyond the individual multi-modal token itself. 
The improved router strategy of X-guided MoE can leverage additional information to make more informed routing decisions, thereby improving the connector's ability to dynamically allocate resources and select the most relevant experts. 

\paragraph{Modality-Guided.}
Modality information enables the connector to quickly classify tokens by their source modality, facilitating efficient expert allocation, especially in the case of input of multiple modalities.
For example, OneLLM~\citep{han2024onellm} addresses the challenge of aligning eight modalities, including image, audio, video, point cloud, depth map, normal map, IMU and fMRI brain activity, to language within a unified framework.
To facilitate modality switching, OneLLM introduces a set of learnable tokens for each modality.
When tokens from different modalities are inputted, their corresponding modality tokens are concatenated with the input tokens and fed into the router. 

\paragraph{Text-Guided.}
For connectors, understanding the textual information such as instructions or descriptions corresponding to other modality tokens helps them understand how to convert tokens according to requirements.
Q-MoE~\citep{wang2024q} introduces a text-guided MoE connector, which routes different query tokens to pre-defined task experts by making use of cross-attention between the text representation and output of each expert.


\paragraph{Task-Guided.}
In a unified multi-task learning framework, task-specific information provides critical context for the connector to classify tokens and determine the most suitable experts.
Uni-Med~\citep{zhu2024uni} introduces a unified medical foundation model that employs C-MoE, a well-designed router which calculates routing weights based on concatenated visual and task-specific tokens and activates different experts for each task, efficiently addressing the multi-task interference problem in MLLMs.

\subsubsection{Variant MoE}
Given the expectation that different experts will exhibit distinct preferences for different types of tokens, experts in the MoE framework can also be designed differently, which has promoted the development of variant MoE.
For example, the connector of V*~\citep{wu2024v} consists of two projection experts: a linear layer and a resampler.
And V* designs a search algorithm as the router to flexibly switch between these two projection modules:
1) none searched target: the linear layer;
2) one or two searched targets: the linear layer for searched targets, the resampler for global image;
3) more than two searched targets: the resampler.

The inherent variability of multi-modal data necessitate a flexible and adaptive approach to feature transmission and integration.
MoE provides this flexibility by dynamically allocating resources and selecting the most appropriate expert for different parts of the input, thereby enhancing the model's ability to handle diverse tasks and scenarios.

\section{Holistic Connector Designs}
Atomic connector operations have utilized single-layer features, typically the last or penultimate layer derived from the modality encoder, to explore and provide basic alignment functions.
In contrast, holistic connector designs are proposed for more complex scenarios which are essentially multi-dimensional and heterogeneous in features.
These scenarios arise from three main sources:
1) multi-layer features, extracted from various layers of the original encoder, each sharing the same dimensionality but offering different levels of abstraction;
2) multi-encoder features, obtained from different encoders, each with its own dimensionality and semantic focus;
3) multi-modal features, from additional modalities, further enriching the representation space.
The core challenge lies in effectively integrating these diverse features to align with the textual embedding space.

\begin{figure*}[!ht] 
\centerline{\includegraphics[scale=0.53]{figures/f2.pdf}}
\caption{The schematic diagram of the fusion strategies: (a) token concatenation. (b) channel concatenation. (c) channel weighted addition. (d) local cross-attention. (e) global cross-attention. (f) local cross-attention (learnable queries). (g) global cross-attention (learnable queries).}
\label{f2}
\end{figure*}

\begin{table*}
%\scriptsize
\tiny
\tabcolsep= 0.5cm
\centering
\begin{tabular}{cccc}
\toprule
\textbf{Model} & \textbf{Visual Encoder} & \textbf{Multi-Layer Selection} & \textbf{Fusion Strategy} \\ 
\midrule
LION~\citep{chen2024lion} & EVA-CLIP-ViT-G & {[}16, 32, 47{]} & Global Cross-Attention→Token Concatenation\\
Dense Connector~\citep{yao2024dense} STI & CLIP-ViT-L & {[}8, 16, 24{]} & Token Concatenation \\
Dense Connector~\citep{yao2024dense} SCI & CLIP-ViT-L & {[}8, 16, 24{]} & Channel Concatenation \\
Dense Connector~\citep{yao2024dense} DCI & CLIP-ViT-L / SigLIP-ViT-SO & {[}1-12, 13-24{]} / {[}1-13, 14-26{]} & Weighted Channel Addition→Channel Concatenation \\
TokenPacker~\citep{li2024tokenpacker} & CLIP-ViT-L & {[}24{]} (Q) {[}12, 16, 22, 23{]} (K, V) & Local Cross-Attention \\
MMFuser~\citep{cao2024mmfuser} & CLIP-ViT-L & {[}23{]} (Q) {[}3, 8, 13, 18{]} (K, V) & Token Concatenation→Global Cross-Attention \\ \bottomrule
\end{tabular}
\caption{A summary of representative connectors in the multi-layer scenario according to the multi-layer selection and fusion strategy.}
\label{t1}
\end{table*}

\begin{table*}[!]
%\scriptsize
\tiny
\tabcolsep= 0.4cm
\centering
\begin{tabular}{ccc}
\toprule
\textbf{Model} & \textbf{Multi-Encoder Selection} & \textbf{Fusion Strategy} \\
\midrule
COMM~\citep{jiang2023clip} & CLIP, DINOv2 & Weighted Channel Addition→Token Concatenation \\
Eyes Wide Shut~\citep{tong2024eyes} A-MoF & CLIP-ViT-L, DINOv2-ViT-L & Weighted Channel Addition→Token Concatenation \\
Eyes Wide Shut~\citep{tong2024eyes} I-MoF &  CLIP-ViT-L, DINOv2-ViT-L & Token Concatenation \\
% Cambrian-1~\citep{tong2024cambrian} & CLIP-ViT-L/14, SigLIP-ViT-SO400M, ConvNeXt-XXL, DINOv2-ViT-L & Local Cross-Attention(Learnable Queries) \\
MoME~\citep{shen2024mome} & EVA-CLIP-G, DINOv2-ViT-L, Pix2Struct-Base & Weighted Channel Addition \\
% EAGLE~\citep{shi2024eagle} & CLIP, ConvNeXt-XXL, Pix2Struct-02-Large, EVA-02-L-Det, SAM-ViT-L, DINOv2-ViT-L & Channel Concatenation \\
LLaVA-Ultra~\citep{guo2024llava} & CLIP-ViT-L, SAM-ViT-L & Weighted Channel Addition\\ 
\multirow{2}{*}{Eagle~\citep{shi2024eagle}} & CLIP-ViT-L, SigLIP-ViT-SO400M, & \multirow{2}{*}{Channel Concatenation} \\
 & EVA-02-L-Det, SAM-ViT-L, DINOv2-ViT-L &  \\
\multirow{2}{*}{Cambrian-1~\citep{tong2024cambrian}} & CLIP, ConvNeXt-XXL, Pix2Struct-02-Large, & \multirow{2}{*}{Local Cross-Attention (Learnable Queries)} \\
 & ConvNeXt-XXL, DINOv2-ViT-L &  \\
BRAVE~\citep{kar2025brave} & EVA-CLIP-G, CLIP-ViT-L, SILC-ViT-G, ViT-e, DINOv2-ViT-L & Token Concatenation→Global Cross-Sttention (Learnable Queries) \\
\bottomrule
\end{tabular}
\caption{A summary of representative connectors in the multi-encoder scenario according to the multi-encoder selection and fusion strategy.}
\label{t2}
\end{table*}


We observe that existing popular fusion strategies, despite their variations in designs, can be broadly represented by the following several categories:
1) token concatenation, directly concatenating tokens from different layers or encoders to form a longer sequence;
2) channel concatenation, concatenating the tokens along the channel dimension, which may require resizing to ensure the number of tokens remains consistent;
3) channel weighted addition, using learnable weights to weight and sum features along the channel dimension without increasing both the sequence length and channel dimension;
4) local cross-attention, using features of specific layers as queries, dividing features of other layers into sub-regions as keys and values, and aggregating each query with the keys and values derived from its corresponding sub-region;
5) global cross-attention, using global features of specific layers as queries and the global features of other layers as keys and values for aggregation;
6) local cross-attention (learnable queries), where features of all extracted layers are divided into sub-regions respectively, and each query aggregates with the keys and values derived from its corresponding sub-regions.
Note that the number of sub-regions per layer must be divisible by the number of queries.
7) global cross-attention (learnable queries), where global features of all extracted layers are used as keys and values for aggregation.
As shown in Figure~\ref{f2}, we visualize the schematic diagram of the fusion strategies for easier understanding.

\begin{figure*}[!ht] 
\centerline{\includegraphics[scale=0.47]{figures/f3.pdf}}
\caption{Three fusion types of the connector in multi-modal scenario: (a) early fusion. (b) intermediate fusion. (c) late fusion.}
\label{f3}
\end{figure*}

\subsection{Multi-Layer Scenario}
Recent studies show that different layers of the same visual encoder capture distinct levels of abstraction and emphasize different regions of interest.
Taking \textit{CLIP} as an example, the shallow layer features focus on low-level details such as edges and textures, while the deep layer features are superior at high-level semantics such as objects and structures.
In the multi-layer scenario, the connector aims to effectively integrate features from multiple layers of a single encoder before performing its alignment role.
By leveraging the distinct strengths of different layers, ranging from low-level details to high-level semantics, the connector enhances feature representation through various fusion strategies, making alignment with textual embedding space more robust.
As shown in Table~\ref{t1}, we list the representative connectors in the multi-layer scenario according to the different properties.

LION~\citep{chen2024lion} proposes the vision aggregator which consists of two attention blocks to fuse the multi-level features from the selected [16, 32, 47] layers, facilitating the fine-grained knowledge learned based on visual grounding tasks.
In addition, LION also uses a Q-Former to process features of the penultimate layer and combines the results of two branches through token concatenation.
Dense Connector~\citep{yao2024dense} explores three intuitive instantiations for multi-layer fusion.
The first is sparse token integration (STI), which preserves the final layer features unchanged while downsampling additional visual features from other layers by using average pooling, followed by token concatenation.
The second is sparse channel integration (SCI), which directly concatenates features from the selected [8, 16, 24] layers along the channel dimension.
The third is dense channel integration (DCI), which partitions the features of all layers into groups, implements element-wise averaging within each group, and concatenates them with the last layer features along the channel dimension.
TokenPacker~\citep{li2024tokenpacker} utilizes the low-resolution interpolated visual features as point-based queries and injects the corresponding sub-region features of multiple layers into it using a point-to-region cross-attention operation.
In MMFuser~\citep{cao2024mmfuser}, deep [23] layer features are employed as query elements, while shallow and intermediate [3,8,13,18] layers features are concatenated to form key and value elements. 



\subsection{Multi-Encoder Scenario}
Each encoder, trained on different datasets and optimized for specific tasks, provides a unique perspective on the input data.
For instance, \textit{CLIP}, pre-trained on text-image pairs using contrastive learning, excels in aligning textual and visual information, whereas \textit{DINO}, trained on pure images, performs exceptionally well in object detection tasks.
While the multi-layer scenario focuses on enhancing feature representations by combining different layers of a single encoder, the multi-encoder scenario takes this a step further by integrating more comprehensive and complementary features from multiple encoders. 
In the multi-encoder scenario, the connector aims to simultaneously achieve internal alignment of all encoder outputs and external alignment with the textual space.
As shown in Table~\ref{t2}, we list the representative connectors in the multi-encoder scenario according to the multi-encoder selection and fusion strategy.

As one of the pioneer works in combining complement multiple encoders, COMM~\citep{jiang2023clip} extracts features of all layers from \textit{CLIP} and deep layers from \textit{DINOv2}, then applies learnable coefficients to weight them separately before concatenating.
Eyes Wide Shut~\citep{tong2024eyes} utilizes a coefficient to control the portion of \textit{CLIP} and \textit{DINOv2} in the A-MoF connector, while interleaving two kinds of features along the token dimension in the I-MoF connector.
MoME~\citep{shen2024mome} uses an adaptive deformable transformation module to align the output of three encoders in the token and channel dimensions.
Then, based on the specific instructions of each sample, a soft router is applied to generate customized fusion ratios for different encoders.
Considering the demands of fine-grained visual information medical domain, LLaVA-Ultra~\citep{guo2024llava} extends visual encoders from \textit{CLIP} to \textit{SAM} and uses the weighted channel addition strategy to fuse the features.
Eagle~\citep{shi2024eagle} explores different combination schemes and fusion strategies in five visual encoders, and ultimately adopts direct channel concatenation due to its efficiency, scalability, and performance.
Cambrian-1~\citep{tong2024cambrian} partitions the feature maps of five visual encoders and facilitates the information aggregation via a set of learnable queries, taking the partitioned visual features of multiple encoders as keys and values.
BRAVE~\citep{kar2025brave} proposes MEQ-Former, which derives tokens from a set of learnable queries and takes the concatenated features from five different encoders as keys and values.


\subsection{Multi-Modal Scenario}
Driven by the abundance of image-text and video-text paired data, current MLLMs primarily focus on vision-text multi-modal tasks.
However, the true advance towards general-purpose AI demands the capability to handle a variety of modalities simultaneously.
Different modality-text pairs often exhibit significant differences in data distribution and representation spaces.
A key challenge lies in effectively fusing information from these diverse modalities into a cohesive, unified representation space.
In this multi-modal scenario, the connector is crucial:
if not designed properly, the integration of information from different modalities can lead to interference, ultimately undermining the model's performance. 

As illustrated in Figure~\ref{f3}, we summarize three fusion types of the connector in multi-modal scenario.
The first type is early fusion, which requires a powerful and robust multi-modal encoder capable of effectively aligning diverse modalities into a unified space before they are fed to the connector.
Therefore, the connector's task is relatively simple, as it only needs to align the unified space with the textual space.
For example, PandaGPT~\citep{su2023pandagpt} adopts ImageBind as the unified encoder and only uses the linear projection to align the representations.
The second type is intermediate fusion, which processes each modality through its own encoder before integrating the features at the connector. 
The connector should handle the challenging task of fusing and mapping features from different modalities, which often have significant disparities in their representation spaces, ensuring that each modality aligns well with the textual space.
MACAW-LLM~\citep{lyu2023macaw} and MEERKAT~\citep{chowdhury2024meerkat} are typical cases of intermediate fusion connectors through well-designed cross-modal attention mechanisms.
The third type is late fusion, which involves separate encoders and connectors for each modality.
Each connector aligns its respective modality with the textual space independently, and the aligned features are concatenated before being fed into the LLM.
AnyMAL~\citep{moon2024anymal} chooses resampler for image branch and linear layers for other modalities' branch.
GroundingGPT~\citep{li2024groundinggpt} applies MLP for image branch and Q-former for video and audio branch.
Note that CAT~\citep{ye2025cat} utilizes a combination of the three types mentioned above.

\section{Future Directions and Challenges}
As a crucial and effective component in the realization of MLLMs, studies on connector frameworks and applications are advancing rapidly, giving rise to numerous challenges and opportunities. 
We have identified several critical challenges and potential areas for future studies.

\paragraph{High-Resolution Input.}
To unlock the high-resolution image processing capabilities of MLLM and enhance fine-grained perception, current works such as InternVL 1.5~\citep{chen2024far} and HiRED~\citep{arif2024hired} typically partition high-resolution images into low-resolution patches and rely on general image encoders and common connectors for processing. 
However, pre-partitioning may disrupt spatial relationships, potentially distorting visual information. 
Therefore, rethinking how connectors handle relationships and interactions between different partitioned patches presents a potential optimization direction.
% improve the fidelity of high-resolution image processing

\paragraph{Dynamic Compression.}
Previous works usually compress tokens using a fixed number or rate, which lacks the flexibility to adapt to varying input complexities and contextual demands. 
This rigidity can lead to suboptimal performance, as fixed-length or fixed-rate compression may either discard critical information from rich inputs or introduce unnecessary redundancy for simple data.
FocusLLaVA~\citep{zhu2024focusllava} employs adaptive downsampling on different image patches based on local and global information, while DocKylin~\citep{zhang2024dockylin} utilizes K-means clustering to categorize and aggregate tokens through similarity-weighted summation.
There remains significant room for developing more adaptive compression mechanisms that can dynamically adjust to the unique characteristics of each input.

\paragraph{Guide Information Selection.}
Inspired by human cognition, where the brain selectively focuses on regions of interest based on guidance, connectors greatly benefit from guide information selection. 
This is exemplified in X-guided MoE connectors, which explore various guide information to determine expert weights.
Future connector designs should consider to facilitate deeper interactions between guide information and token representations.
For example, PPLLaVA~\citep{liu2024ppllava} employs a prompt-guided pooling module to extract video features relevant to user prompts.
World Knowledge~\citep{zhai2024world} proposes an instruction-guided interaction module to better align the visual features with the natural language instructions.
By selecting optimal guide information, connectors can focus on task-relevant features, alleviating the burden on the LLM to manage these interactions.

\paragraph{Combination Strategy.}
As discussed in Section 4, recent works have increasingly focused on using multi-layer or multi-encoder outputs to capture more comprehensive and multi-dimensional representations of the multi-modal information.
While these approaches enhance the richness of the feature representations, they also introduce the challenge of information redundancy.
Cambrian-1~\citep{tong2024cambrian} presents the SVA module to aggregate features from multiple visual encoders.
However, it requires repeated aggregation across multiple LLM layers to iteratively access and integrate necessary visual information. 
Eagle~\citep{shi2024eagle} has explored several combination strategies and surprisingly found that the simple channel concatenation was the most effective. 
These findings underscore the vast potential for improving combination strategies in connectors to balance the trade-off between representational richness and efficiency.

\paragraph{Interpretability.}
While the connector is pivotal in bridging the gap between modalities, its effectiveness has not been fully explored from an interpretability perspective.
Investigating interpretability can provide valuable insights into the inner mechanisms of the connector, guiding the design of more robust and efficient architectures while addressing potential weaknesses.
For instance, MMNeuron~\citep{huo2024mmneuron} analyzes the distribution of domain-specific neurons across different modules, suggesting that the number of such neurons reflects their understanding capabilities.
% facilitating the interpretability of the connector.
Deco~\citep{yao2024deco} uses R-GAE relevance maps to trace the flow of information from the generated textual tokens back to raw visual patches and intermediate connector output, revealing potential semantic deficiencies in the Q-former connector, such as the loss of fine-grained attribute and spatial locality.
We believe that future research on the interpretability of connectors will play a key role in unleashing the full potential of MLLMs.

\section{Conclusion}
The development of connectors in multi-modal large language models has become a critical area of research as the field progresses towards general AI.
In this survey, we aim to provide an in-depth overview of the connector in existing MLLMs.
Firstly, we introduce a new taxonomy that categorizes connectors into atomic operations and holistic designs.
Secondly, we systematically review the representative studies according to the taxonomy, highlighting the advancements in mapping, compression, and mixture of experts, as well as the holistic approaches for handling multi-layer, multi-encoder, and multi-modal scenarios.
Finally, we discuss some challenges and highlight several future research
directions. 
We hope this survey can serve as a useful reference for researchers, encouraging further exploration and innovation in the design of more efficient and robust connectors for MLLMs.

\section*{Acknowledgments}
This paper is supported by the Natural Science Foundation of Beijing with Grant No.25D30216.

\newpage
{
\fontsize{9}{10.5}\selectfont
\bibliographystyle{named}
\bibliography{cit}
}


\end{document}

