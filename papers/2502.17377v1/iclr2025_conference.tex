 
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\newtheorem{theorem}{Proposition}
\usepackage{wrapfig}


% \title{GraphGS: Pose-Free 3D Gaussians for Efficient Open Scene Reconstruction}
\title{Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Chong Cheng$^{1\thanks{Equal contribution.}}$\enskip\enskip
Gaochao Song$^{2^*}$\enskip\enskip
Yiyang Yao$^{3}$\enskip\enskip
Qinzheng Zhou$^{4}$\\
{\bf Gangjian Zhang$^{1}$\enskip\enskip
Hao Wang$^{1}$}\thanks{Corresponding author.} \\
$^{1}$HKUST(GZ)\enskip\enskip
$^{2}$HKU\enskip\enskip
$^{3}$SCUT\enskip\enskip
$^{4}$UC Berkeley\enskip\enskip\\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\iclrfinalcopy
%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\begin{abstract}
This paper investigates an open research challenge of reconstructing high-quality, large 3D open scenes from images. It is observed existing methods have various limitations, such as requiring precise camera poses for input and dense viewpoints for supervision. To perform effective and efficient 3D scene reconstruction, we propose a novel graph-guided 3D scene reconstruction framework, GraphGS. Specifically, given a set of images captured by RGB cameras on a scene, we first design a spatial prior-based scene structure estimation method. This is then used to create a camera graph that includes information about the camera topology. Further, we propose to apply the graph-guided multi-view consistency constraint and adaptive sampling strategy to the 3D Gaussian Splatting optimization process. This greatly alleviates the issue of Gaussian points overfitting to specific sparse viewpoints and expedites the 3D reconstruction process. We demonstrate GraphGS achieves high-fidelity 3D reconstruction from images, which presents state-of-the-art performance through quantitative and qualitative evaluation across multiple datasets. Project page: \url{https://3dagentworld.github.io/graphgs/}.
\end{abstract}

\section{Introduction}
3D scene reconstruction aims to transform 2D images into realistic 3D scenes. This technology has many practical applications such as Augmented Reality (AR) and Virtual Reality (VR)~\citep{ucnerf,guo2023streetsurf, mi2023switchnerf, xiangli2022bungeenerf, xu2023gridguided}. The emergence of Neural Radiance Fields (NeRF)~\citep{nerf} and 3D Gaussian Splatting (3DGS) \citep{kerbl3Dgaussians} enables differentiable novel-view synthesis and real-time rendering \citep{luiten2023dynamic, wu20234dgaussians, yang2023deformable3dgs}. Recent studies have applied NeRF and 3DGS to unbounded environments such as street views and urban areas~\citep{meganerf, tancik2022blocknerf, wang2023fegr, lin2024vastgaussian, yan2023streetgaussians}, broadening the application scenario.

However, achieving high-quality 3D scene reconstruction remains a challenging task.
It is observed existing works require precise camera poses for input and dense viewpoints for supervision \citep{lin2024vastgaussian,chen2024periodic,guo2023streetsurf}.
Notably, public benchmark datasets such as those referenced in \citep{kitti,meganerf,Sun} often experience pose inaccuracies and sparse viewpoints for boundaries. These issues are typically attributed to motion-induced vibrations and limitations in the positioning equipment used in vehicles or drones.
Moreover, if one would like to use mobile devices such as smartphones and DSLR cameras to capture images for 3D reconstruction, obtaining accurate camera poses and dense viewpoints can be challenging and troublesome.


Although tools like COLMAP have been developed to lower the entry barrier for 3D reconstruction by providing pose estimation, the heavy optimization costs and camera matching failures limit their practicality \citep{schoenberger2016sfm,schoenberger2016mvs}. Additionally, methods that incorporate pose estimation strategies for object reconstruction, struggle with broader scene applications \citep{freenerf,Diet,wang2023sparsenerf,kim2023upnerf}. The limited number of viewpoints and the vast number of images in outdoor scenes further complicate the issue. 

To address the challenges of reconstructing large open scenes from uncalibrated images, this paper proposes GraphGS, a framework specifically tailored for large 3D open scene reconstruction. GraphGS utilizes 3DGS combined with our proposed spatial prior-based scene structure estimation to enhance the speed and accuracy of pose estimation, even for image collections comprising thousands of images. During the process of scene structure estimation, the matching relationship of cameras is also recorded, which is represented as a camera graph. The camera graph provides useful topology information for scene cameras and benefits for gaussian optimization.

GraphGS innovatively proposes to apply the constructed camera graph to guide the optimization process of large 3DGS scenes, through multi-view consistency constraints and adaptive sampling strategy. This ensures a more accurate 3D Gaussian point distribution. Besides, our proposed approach prevents Gaussian points from overfitting to given sparse viewpoints, thereby enhancing reconstruction quality. Furthermore, our adaptive sampling strategy reduces the number of iterations for camera graph nodes, significantly accelerating the reconstruction of large open 3DGS scenes and reducing training time by nearly 50\%.


The main contributions of this paper are summarized as follows:
\begin{itemize}
    \item We introduce 3DGS in conjunction with spatial prior-based structure estimation method to efficiently and accurately estimate structures from uncalibrated images.
    % , enabling fast processing and real-time rendering capabilities.
    \item Through our proposed camera graph-guided 3D Gaussian optimization, GraphGS not only improves the reconstruction quality, but also greatly accelerates the reconstruction process.
    % GraphGS not only mitigates overfitting but also rapidly reconstructs large scenes with high quality.
    \item With GraphGS framework, our method achieves state-of-the-art (SOTA) performance in several large benchmarks without using ground truth camera poses.
\end{itemize}

\section{RELATED WORK}
\subsection{Scene Reconstruction}
The advent of NeRF~\citep{nerf} has ushered in a golden age for 3D scene construction. Numerous studies have improved its efficiency~\citep{hedman2021snerg, instantngp, Reiser2023SIGGRAPH} and generalization~\citep{yu2021pixelnerf, wang2021ibrnet, chen2021mvsnerf}. Mip-NeRF~\citep{barron2021mipnerf} and Zip-NeRF~\citep{barron2023zipnerf} have tackled aliasing issues, while InstantNGP~\citep{instantngp} integrates grid pyramid technologies to optimize sub-volumes. UC-NeRF~\citep{ucnerf} targets outdoor scenes, enhancing image consistency through color correction and pose refinement. StreetSurf~\citep{guo2023streetsurf} and EmerNeRF~\citep{yang2023emernerf} introduce novel approaches for multi-view reconstructions through disentanglement and self-guided learning. Concurrently, PVG~\citep{chen2024periodic} utilizes 3DGS~\citep{kerbl3Dgaussians} to advance scene reconstruction with techniques like time-dependent transparency and scene-flow smoothing.

Furthermore, to extend reconstruction techniques to larger-scale scenes, methods like Block-NeRF~\citep{tancik2022blocknerf}, Mega-NeRF~\citep{meganerf}, and Switch-NeRF~\citep{mi2023switchnerf} employed a divide-and-conquer strategy. Mega-NeRF clustered pixels based on 3D sampling distances, Block-NeRF organized images into street blocks, and Switch-NeRF utilized a sparse network for large scene synthesis. These methods improved scalability and flexibility but faced limitations in real-time rendering of large outdoor environments. VastGaussian~\citep{lin2024vastgaussian} incorporated 3DGS to enhance detail presentation and rendering speed in large scenes. While these methods have advanced scene reconstruction, they typically rely on precise camera poses and initial data like LiDAR, which can be difficult to obtain in real-world applications, especially in expansive outdoor settings.

\subsection{Pose Optimization}
To address issues with pose accuracy, many studies seek to bypass the slow and occasionally imprecise COLMAP process by concurrently optimizing camera pose and scene representation using the original MLP-based NeRF \citep{wang2021nerfmm}, such as GARF~\citep{chng2022gaussian} and BARF~\citep{lin2021barf}. Joint-TensoRF\citep{Joint-TensoRF} focuses on refining camera poses and 3D scenes using decomposed low-rank tensors. These methods have been proven effective in recovering object structures and poses from imperfect or unknown camera positions, although their application to broader scene reconstruction remains challenging. In 3DGS, COLMAP~\citep{schoenberger2016sfm,schoenberger2016mvs} is utilized for pose reconstruction and generating sparse initial points via Structure-from-Motion (SfM)~\citep{schoenberger2016sfm}. However, COLMAP's dense matching time increases exponentially with the number of images, and its success rate is limited, which poses significant challenges for outdoor scene reconstruction.

Building on the discussions above, this paper is dedicated to proposing a low-pose-requirement, rapid, high-precision 3D reconstruction method suitable for both general and large scenes.

\section{Method}
As shown in Fig. \ref{main-fig}, we firstly design spatial prior-based scene structure estimation to improve efficiency of SfM pipeline with the input of thousands of images. During the process, we record the matching information of cameras and form a camera graph. The graph contains topology information of scene cameras, which is suitable for guiding gaussian optimization to improve reconstruction quality and efficiency. In addition, to accelerate the training process and decreases GPU memory consumption, an octree point initialization strategy is introduced to reduce initialization points.


\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{images/main_v6.pdf}
  \caption{Framework of the GraphGS method for efficient large 3D scene reconstruction. The process begins with spatial prior-based structure estimation, followed by octree-based efficient organization of initialization points. The camera graph is obtained at the end of structure estimation, which contains topology information of scene camera. The information in camera graph will be further used for the following gaussian optimization.
  }
  \label{main-fig}
\end{figure*}

\subsection{Spatial Prior-based Structure Estimation}\label{SPSE}
In the reconstruction of 3DGS scenes from thousands to tens of thousands of images, rapidly and accurately obtaining camera poses represents a significant challenge. Traditional Structure from Motion (SfM) pipeline(COLMAP \citep{schoenberger2016sfm,schoenberger2016mvs}), typically require days to accurately process such extensive datasets. We locate this problem in exhaustive matcher of COLMAP pipeline, which takes all possible image pairs for feature matching and causes time bottleneck. To address this issue, we have developed an innovative spatial analysis framework for estimating scene structure. Initially, we acquire approximate camera poses from pre-trained fast relative pose estimation models. Subsequently, utilizing concentric nearest neighbor pairing (Sec. \ref{CNNP}) and quadrant filter (Sec. \ref{QF}), we filter and prioritize image pairs that are crucial for accurate camera pose estimation. This approach dramatically reduces computational load, facilitating swift and precise reconstructions that transform processes from days-long endeavors to tasks completed within hours. Additionally, we have constructed a camera graph to guide the optimization process in 3DGS, further enhancing the efficiency and precision of the reconstruction.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{images/key-pair-select_inv.pdf}
  \caption{Illustration of spatial prior-based structure estimation, including two parts: Concentric Nearest Neighbor Pairing (left) and Quadrant Filter (right). For Concentric Nearest Neighbor Pairing, we first select $c_i$'s nearest $r$ cameras for matching to guarantee stability of local bundle adjustment, then we select $w$ cameras from every $h+w$ camera based on distance order, forming a series of concentric circles. For quadrant filter of 2D case, camera $c_i$ is posed at the center of the coordinate system, pointing towards the y-axis. For other cameras (we show 12 cameras), the relative position and orientation to $c_i$ contains $4\times4=16$ states. 
  }
  \label{fig:key-pair}
\end{figure}

\subsubsection{Concentric Nearest Neighbor Pairing}
\label{CNNP}
Compared to selecting all possible image pairs, opting for a sparser set can significantly accelerate the process of scene structure estimation. However, improper selections may lead to failures in matching cameras with the scene during the estimation phase. Our principle is, the distribution of selected image pairs should reflect both local and global structure. To handle this issue, we propose concentric nearest neighbor pairing. For camera $c_i,c_j$ from $N$ cameras altogether, we define $\mathbf{s}(i,j)$ as the sorted order from nearest to farthest of camera $c_j$ relative to camera $c_i$, then the set of all matched pairs $S_{all}$ is given by:
\begin{gather}\label{eq-CNNP}
    S_{all}=\bigcup \limits _{i=1}^N{(S_{\text{neighbor}}^{(i)} \cup S_{\text{concentric}}^{(i)} \cup S_{\text{connection}}^{(i)}) } \\
    S_{\text{neighbor}}^{(i)}=\{(c_i,c_j) | s(i,j) \leq r, j=1,2,...,N, j\neq i \} \notag\\
    S_{\text{concentric}}^{(i)}=\{(c_i,c_j) | 0 \leq (s(i,j)-r) \% (h+w) < w , j=1,2,...,N, j\neq i \} \notag \\
    S_{\text{connection}}^{(i)}=\{(c_i,c_j) | j=i-1, j\geq 1 \} \notag
\end{gather}
where the process of $S_{\text{neighbor}}^{(i)}$, $S_{\text{concentric}}^{(i)}$ are illustrated in Fig. \ref{fig:key-pair} (left). Specifically, we traverse all of the cameras from 1 to $N$ (supposing the current camera is $c_i$) and calculate which camera $c_j$ (from another loop) should form a matching pair with $c_i$. Then we have the following steps: (1) For $c_i$, we sort other cameras based on their distance to $c_i$, and add $c_i$'s nearest $r$ cameras as $S_{\text{neighbor}}^{(i)}$. (2) We add $w$ cameras from every $h+w$ cameras based on the same sorted order as $S_{\text{concentric}}^{(i)}$. (3) We add $c_{i-1}$ in the main loop to match $c_i$ as $S_{\text{connection}}^{(i)}$. 

This approach allows for a smooth transition from local to global matching pairs, ensuring sparsity that enhances the efficiency of feature matching. Notice there are repetitive elements (e.g. camera pair $(c_1,c_3)$ is equivalent to $(c_3,c_1)$), we eliminate them via HashSet at the end. Also, we implement this method via K-DTree (~\citep{kdtree}) for highly efficient storage and retrieval of camera distance relationships to save pair selection time.

\textbf{Remark.} When treating cameras as a graph node, and adding edges for every matched pair in $S_{all}$, an undirected graph will be formed, which we refer to as \textit{camera graph}. 
%To guarantee the success of BA, 
To guarantee the success of scene structure estimation, camera graph should be \textit{connected graph}. However, this may not be satisfied when we only have $S_{\text{neighbor}}^{(i)}$, $ S_{\text{concentric}}^{(i)}$, and the proof is presented in Appendix \ref{appendix-connect}. To address this issue, we add additional $S_{\text{connection}}^{(i)}$ and pairs in this set will not be affected by the following quadrant filter strategy.

\subsubsection{Quadrant Filter}
\label{QF}

In reconstructing 3DGS scenes from a large number of images, selecting appropriate image pairs to accelerate the estimation of scene structure is essential. Given two camera pairs which have little or no view intersection with each other, it is inefficient to mark them as matching pairs. Besides, these pairs introduce noise for global refinement. In this section we introduce quadrant filter strategy to filter these noise pairs. 

For one coarse camera $c_i$, we denote its global position as $p^{(i)}=[x_i,y_i,z_i]$ and orientation as $d^{(i)}=[v^{(i)}_x,v^{(i)}_y,v^{(i)}_z]$. Broadly speaking, for arbitrary two camera $c_i,c_j$ with varying position and orientation in 3D space, their relative position and relative orientation have $8$ states respectively, totally with $8\times 8=64$ states. This already provides enough information to filter noise pairs.

To record the relative position of camera $c_j$ to $c_i$, we apply binary encoding as follows:
\begin{equation}
    \mathcal{B}_p^{(i,j)}=\{sgn(x_j-x_i),sgn(y_j-y_i),sgn(z_j-z_i)\}
\end{equation}
with the sign function $sgn(x)$:
\begin{equation}
    sgn(x)= \begin{cases}
    1, & \text{if } x > 0 \\
    0, & \text{if } x \leq 0
\end{cases}
\end{equation}
where $\mathcal{B}_p^{(i,j)}$ is a three-digit binary number, representing $8$ quadrants. For example, when $\mathcal{B}_p^{(i,j)}$=\{0,1,1\}, it represents binary number \texttt{011}, corresponding to the 3rd quadrant. $(i,j)$ represents the position of $c_j$ relative to $c_i$, meaning $c_i$ is posed on the origin of the 3D coordinate system, pointing towards z-axis.

% \textbf{Relative Orientation.} 
To record the relative orientation, a direct way is to calculate a rotation matrix to transform them in the same standard, and follow the same approach like recording relative position. However, this would be time-consuming since we have to solve a system of linear equations for every potential camera pairs. To address this issue, we have proposition as follows:

\begin{theorem}\label{prop-1}
  Given two spacial vectors $d^{(i)}=[v^{(i)}_x,v^{(i)}_y,v^{(i)}_z]$, $d^{(j)}=[v^{(j)}_x,v^{(j)}_y,v^{(j)}_z]$, their relative orientation have $8$ states respectively, corresponding to $8$ quadrants in 3D coordinate system. The quadrant of relative orientation can be directly calculated via one cross product and one inner product:
  \begin{equation}
      \mathcal{B}_d^{(i,j)}=\{sgn(e_x[d_{\times}^{(i)}]d^{T(j)}),sgn(e_y[d_{\times}^{(i)}]d^{T(j)}),sgn(d^{(i)}d^{T(j)})\}
  \end{equation}
  where $\mathcal{B}_d^{(i,j)}$ is a three-digit binary number, representing $8$ quadrants. $[d_{\times}^{(i)}]$ represents the anti-symmetric matrix of cross product:
\begin{equation}
    [d_{\times}^{(i)}]=\begin{bmatrix}
0 & -v_z^{(i)} & v_y^{(i)} \\
v_z^{(i)} & 0 & -v_x^{(i)} \\
-v_y^{(i)} & v_x^{(i)} & 0
\end{bmatrix}
\end{equation}
and $e_x=[1,0,0], e_y=[0,1,0]$ represent unit vector of $x,y$ axis. 

The full proof is provided in Appendix \ref{appendix-propprof}.
  
\end{theorem}
% \textbf{Proof:} See Appendix B.



Proposition \ref{prop-1} provides an efficient way to record 8 states of relative orientation. 
% \textbf{All states.} 
After encoding both position and orientation, we concatenate them to get final pose binary encoding with 6-bit:
\begin{equation}
    \mathcal{B}^{(i,j)}=\mathcal{B}_p^{(i,j)} \oplus \mathcal{B}_d^{(i,j)}
\end{equation}
this enables us to directly query a predefined state table to filter the noise camera pairs. As shown in Fig. \ref{fig:key-pair} (right), a 2D example, in this case, the relative position and relative orientation have $4$ states respectively (that is, $4$ quadrants). For different camera $c_j$ relative to $c_i$, the $c_1$ (green) should be included in matching if it appears in $S^{(i)}$ of Equation \ref{eq-CNNP}, while the $c_2$ (red) should be excluded. For $c_3$ (yellow), it has slight view intersection with $c_i$, for this state, we set strict/loose mode of the state table to handle it in practice. Notice even in the loose mode (we include all the yellow cameras like $c_3$ to matching), our method can also filter more than $38\%$ noise camera pairs for cameras with random poses. The detailed quadrant division of 3D coordinate system, full state table and analysis of strict/loose mode are all presented in Appendix \ref{appendix-state-table}.

\subsubsection{Octree Point Initialization}
The design of this module is based on the observation that, in the scene reconstruction process using 3DGS, not all initial points contribute equally to the ultimate quality of the reconstruction. By efficiently managing these points, computational efficiency is optimized without compromising reconstruction quality. Drawing inspiration from PlenOctrees~\citep{yu2021plenoctrees}, we employ an Octree~\citep{octree} structure for spatial detail management. Nodes in the octree are evaluated based on the Level of Detail (LOD) of the initial points, with those falling below a detail threshold $\tau$—deemed minimally contributory to quality—being pruned to simplify the model.



\subsection{Graph-guided Gaussian Optimization}
\label{online}
\subsubsection{Camera Graph Definition}
Based on the observation that viewpoint coverage in open scenes is relatively sparse compared to indoor scenes or object reconstruction, supervision from a single viewpoint can lead to Gaussian points overfitting near specific cameras. This results in an inaccurate Gaussian distribution and learning predominantly from a single image rather than the entire scene. To address these challenges and improve the efficiency, accuracy, and robustness of 3DGS reconstruction in open scenes, we have specifically designed a weighted undirected camera graph based on structured estimation results. The construction of this graph adheres to the following rules: 1) Each node corresponds to a view camera’s rotation matrix R and translation matrix T; 2) Edges are formed based on the camera pairs selected in Section \ref{SPSE}. We aim to use this meticulously designed graph structure to precisely reflect the spatial relationships between cameras, thereby effectively guiding the optimization process of 3DGS in open scenes.

\subsubsection{Graph-guided Multi-view Consistency Constraint} 
\begin{wrapfigure}{r}{0.4\textwidth} % 'r'表示右侧，'0.5\textwidth'表示图形宽度为文本宽度的一半  
    \centering  
    \includegraphics[width=0.4\textwidth]{images/graph_weights.pdf} % 插入图片，替换为实际图片文件名  
    \caption{Example of edge weights.}  
    \label{fig:graph-consist}  
    % \vspace{-10pt}
\end{wrapfigure} 

We know that adjacent multi-view scenes with minimal directional differences should exhibit nearly identical photometric values \citep{Kloukiniotis_2022_CVPR}. Consequently, we designed edge weight $w_e(i,j)$ of camera graph to measure this directional differences as follows:
\begin{equation}\label{eq-weight-edge}
    w_e(i,j)=\frac{e^{-k\left\|p^{(i)}-p^{(j)}\right\|_2}}{1-e^{-d^{(i)}d^{T(j)}}}
\end{equation}
where $p^{(i)}$, $p^{(j)}$ and $d^{(i)}$, $d^{(j)}$ are refined camera position and orientation from rotation and translation matrix $R$, $T$. Take Fig. \ref{fig:graph-consist} for example, supposing there are $3$ edges $(3,4),(3,6),(3,7)$ connecting to camera node $3$. When calculating the edge weights, Eq. \ref{eq-weight-edge} will take relative distance $d_{36},d_{37},d_{34}$ and relative orientation $\theta_{36},\theta_{37},\theta_{34}$ into consideration, while the edge with smaller distance and smaller angle obtains larger weight.

Based on the edge weights, we designed a multi-view consistency photometric loss $\mathcal{L}_\text{cons}$ to enhance the robustness of 3DGS optimization process as follows:

\begin{equation}
    \mathcal{L}_\text{cons} = \lambda \sum_{i,j} \sum_{p} \| I_i(p) - I_j(\mathbf{K}_j (R_{ji} \mathbf{K}_i^{-1} p + T_{ji})) \|
\end{equation}
where \(I_i(p)\) represents the intensity or color at pixel \(p\) in the image from camera \(i\). The matrices \(R_{ji}\) and \(T_{ji}\) describe the rotation and translation from camera \(i\) to camera \(j\), respectively, while \(\mathbf{K}_i\) and \(\mathbf{K}_j\) are the intrinsic matrices, and  $\lambda$ is an adjustment factor. To obtain camera $j$ given camera $i$, we traverse all edges of camera $i$ and select the camera with the maximum edge weight to camera $i$ as target camera $j$.

Counterintuitively, while target camera $j$ may provide rendered results that are significantly worse than the ground truth images for supervision, they enhance the reconstruction outcome. We believe this may act similar to data augmentation method, helping to prevent overfitting.



\subsubsection{Adaptive Sampling Optimization} 
% For every graph node $i$, we assign an importance weight:
We propose an adaptive sampling strategy that dynamically adjusts sampling rates during optimization based on node importance in the graph. For nodes with less overlap with other viewpoints, sampling frequency is reduced according to their weights. To assess the importance of each node within the graph, we devise specific importance weights. We find that performing fewer iterations on certain peripheral nodes does not significantly degrade the overall quality of reconstruction and helps prevent overfitting due to insufficient supervision signals. The design of node weights primarily considers two criteria: 1) Degree centrality, which calculates the number of neighbors connected to each camera node; 2) Betweenness centrality, assessing the importance of each node across all shortest paths, where nodes with higher centrality are considered more significant. Degree centrality focuses on the activity level of nodes, while betweenness centrality is concerned with the control a node exerts or its role as a bridge. In our experiments, we found that using only betweenness centrality yielded better results, so we design node weight  $w_n(i)$  based on Betweenness centrality:


\begin{equation}\label{eq-weight-node}
    w_n(i)=\sum_{i\neq j\neq k} \frac{\sigma_{jk}(i)}{\sigma_{jk}}
\end{equation}

where $\sigma_{jk}$ represents the number of shortest path between node $j$,$k$ of camera graph. $\sigma_{jk}(i)$ represents the number of shortest path passing node $i$ between node $j$,$k$. After getting node weights, we set a probability function $P(i)$ for camera $c_i$ during iteration:
\begin{equation}
    P(i)=w_n(i)/\text{Max}(\{w_n(i)\}_{i=1}^{N})
\end{equation}
where the denominator represents the maximum value of all node weights. The function $P(i)$ represents the probability of view camera $i$ participating in gaussian optimization. This design is noteworthy as it significantly reduces the number of optimization iterations, which accelerates the reconstruction process. Despite this reduction, it effectively enhances the geometric representation of Gaussian points, substantially improving the overall quality of the reconstruction.




\section{Experiments}



\begin{table}
\centering
\small
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{@{}l|c|ccc|c|ccc@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c|}{Waymo} & \multicolumn{4}{c}{KITTI} \\
\cmidrule(r){2-5} \cmidrule(l){6-9}
& FPS $\uparrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & FPS $\uparrow$ & PSNR $\uparrow$& SSIM $\uparrow$& LPIPS $\downarrow$\\
\midrule
Mip-NeRF 360 \citep{mipnerf360} & 0.042 & 22.42 & 0.698 & 0.471 & 0.053 & 20.68 & 0.650 & 0.480\\
S-NeRF \citep{snerf}    & 0.001 & 19.22 & 0.515 & 0.400 & 0.008 & 18.71 & 0.606 & 0.352 \\
StreetSurf \citep{guo2023streetsurf} & 0.097 & 23.78 & 0.822 & 0.401 & 0.037 & 22.48 & 0.763 & 0.304 \\
Zip-NeRF \citep{barron2023zipnerf} & 0.500  & 26.21  & 0.815 & 0.389 & 0.610  & 21.41  & 0.665 & 0.470 \\
UC-NeRF \citep{ucnerf} & 0.032 & 26.72  & 0.800   & 0.375  & 0.051 & 24.05  & 0.721   & 0.400              \\
BARF \citep{lin2021barf} & 0.041 & 9.07 & 0.235 & 1.021 & 0.071 & 10.68 & 0.250 & 0.990\\
SPARF \citep{truong2023sparf} & - & - & - & - & - & - & - & -\\
UP-NeRF \citep{kim2023upnerf} & 0.120 & 26.16 & 0.876 & 0.375 & - & - & - & -\\
EmerNeRF \citep{yang2023emernerf}   & 0.043  & 25.92 & 0.763 & 0.384 & 0.28  & 25.24 & 0.801 & 0.237 \\
3DGS \citep{kerbl3Dgaussians}       & \textbf{63}    & 25.08 & 0.822 & 0.319 & \textbf{125}    & 19.54 & 0.776 & 0.224 \\
PVG \citep{chen2024periodic}       & 50     & 28.11 & 0.849 & 0.279 & 59     & 26.63 & 0.885 & \textbf{0.127} \\
\toprule
Ours       & 52     & \textbf{29.43} & \textbf{0.899} & \textbf{0.217} & 79     & \textbf{26.98} & \textbf{0.887} & 0.157 \\
\bottomrule
\end{tabular}
\vspace{-5pt}
\caption{Quantitative comparison of novel view synthesis results on the Waymo and KITTI. Our method demonstrates a competitive edge in rendering quality with higher FPS, PSNR, SSIM, and lower LPIPS scores, affirming its efficacy in synthesizing realistic views in comparison to the current state-of-the-art methods.}
\vspace{-10pt}
\label{tab:1}
\end{table}

\begin{figure*}
  \vspace{-10pt}
  \centering
  \includegraphics[width=0.95\textwidth]{images/1.pdf}
  \vspace{-10pt}
  \caption{Qualitative comparison of novel view synthesis results on the Waymo and KITTI, showcasing the Ground Truth alongside results from our GraphGS method, StreetSurf~\citep{guo2023streetsurf}, PVG~\citep{chen2024periodic}, and 3DGS~\citep{kerbl3Dgaussians} for comprehensive evaluation. Our approach yields closer fidelity to the Ground Truth, highlighting the effectiveness of our reconstruction method in various urban scene complexities.}
  \label{fig:1}
  \vspace{-10pt}
\end{figure*}





\subsection{Experimental Setup}
\textbf{Datasets and Metrics.}
We evaluate our method on three datasets: Waymo \citep{Sun}, KITTI \citep{kitti}, and Mill-19, which includes large scenes like Buildings and Rubble \citep{meganerf}. For Waymo, we use images from three cameras (front, front-left, front-right) across 32 scenes, totaling about 600 images per scene block. For KITTI, each scene block contains approximately 100 images. Consistent with prior research, we select one out of every eight images for testing, using the remainder for training. Our method, designed for scenarios lacking pose data, does not use ground truth (GT) poses from the datasets.

We compare our method on the Waymo and KITTI datasets against various NeRF-based methods (UC-NeRF~\citep{ucnerf}, Mip-NeRF 360 \citep{mipnerf360}, Zip-NeRF~\citep{barron2023zipnerf}, StreetSurf~\citep{guo2023streetsurf}, EmerNeRF~\citep{yang2023emernerf}, SNeRF \citep{snerf}) and methods based on 3DGS (3DGS \citep{kerbl3Dgaussians}, PVG~\citep{chen2024periodic}). We specifically evaluate against both pose optimization (UC-NeRF \citep{ucnerf}) and pose-free reconstruction methods (e.g., BARF \citep{lin2021barf}, SPARF \citep{truong2023sparf}, UP-NeRF \citep{kim2023upnerf}). For the large Mill-19 dataset, we compare with Mega-NeRF \citep{meganerf}, Switch-NeRF \citep{mi2023switchnerf}, and VastGaussian \citep{lin2024vastgaussian}, segmenting scenes for 3DGS due to memory limits. For some methods, we use published results for comparisons.

\begin{table}
\centering
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}lccc|ccc@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{Building} & \multicolumn{3}{c}{Rubble} \\
\cmidrule(r){2-4} \cmidrule(l){5-7}
& PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\
\midrule
Mega-NeRF \citep{meganerf} & 20.93 & 0.547 & 0.349 & 24.05 & 0.553 & 0.373 \\
Switch-NeRF \citep{mi2023switchnerf} & 21.54 & 0.579 & 0.294 & 24.31 & 0.562 & 0.329 \\
3DGS \citep{kerbl3Dgaussians}     & 23.01 & 0.769 & 0.164 & 26.78 & 0.800 & 0.161 \\
VastGaussian \citep{lin2024vastgaussian}     & 23.50 & 0.804 & \textbf{0.130} & 26.92 & 0.823 & \textbf{0.132} \\
\toprule
Ours                            & \textbf{26.60} & \textbf{0.854} & 0.163 & \textbf{27.03} & \textbf{0.869} & 0.185 \\
\bottomrule
\end{tabular}
\vspace{-5pt}
\caption{Quantitative comparison of novel view synthesis on the Mill 19 large scene dataset.}
\vspace{-25pt}
\label{tab:2}
\end{table}

\begin{figure*}
  \centering
  \includegraphics[width=0.95\textwidth]{images/2.pdf}
  \vspace{-10pt}
  \caption{Qualitative comparison of novel view synthesis in the Mill 19 large scene dataset~\citep{meganerf}, showcasing the Ground Truth alongside the results from our method and other state-of-the-art methods including Mega-NeRF~\citep{meganerf}, Switch-NeRF~\citep{mi2023switchnerf}, and 3DGS~\citep{kerbl3Dgaussians}.
  }
  \label{fig:2}
  \vspace{-10pt}
\end{figure*}



\textbf{Implementation.}
Experiments were conducted using an NVIDIA RTX 3090 GPU and an AMD EPYC 7542 CPU. For image sets lacking pose or sequence information, we use a pre-trained model \citep{wang2024dust3r} to estimate relative poses in about 0.01 seconds per pair. Although not highly precise, it effectively outlines the coarse distribution of the images. We then apply our scene structure strategy to estimate scene structures, generate camera graphs, and calculate consistency and importance weights. Additionally, we optimize memory and training efficiency by using an octree to prune redundant points from approximately 300k to 100k. 


For the quantitative comparison of our Spatial Prior-based Structure Estimation with Colmap, the Colmap setup included a vocabulary tree containing 256K visual words, pre-built using the Flickr100k dataset (available on the COLMAP project page). Further details and training specifics are available in the Appendix.

\begin{table}
% \vspace{-10pt}
\centering
\small
\begin{tabular}{cc|cc|ccc}
\toprule
\multicolumn{2}{c|}{Strategy} & \begin{tabular}[c]{@{}c@{}}Matching\\ Time\end{tabular} $\downarrow$ & \begin{tabular}[c]{@{}c@{}}BA\\ Time\end{tabular} $\downarrow$ & PSNR $\uparrow$  & SSIM $\uparrow$ & LPIPS $\downarrow$\\ 
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{0.6k}} & COLMAP (Ex) & 62 min & 140 min & 30.05 & 0.89 & \textbf{0.23}  \\
\multicolumn{1}{c|}{}                      & COLMAP (Vo) & 50 min & 104 min & 29.02 & 0.88 & 0.26  \\ 
\cline{2-7} 
\multicolumn{1}{c|}{}                      & Ours        & \textbf{3 min}  & \textbf{20 min}  & \textbf{30.18} & \textbf{0.90} & 0.24  \\ 
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{2k}}   & COLMAP (Ex) & \multicolumn{2}{c|}{\textgreater{}24 h} & - & - & - \\
\multicolumn{1}{c|}{}                      & COLMAP (Vo) & \multicolumn{2}{c|}{\textgreater{}24 h} & - & - & - \\ 
\cline{2-7} 
\multicolumn{1}{c|}{}                      & Ours        & \textbf{12 min} & \textbf{194 min} & \textbf{26.60} & \textbf{0.85} & \textbf{0.16}  \\ 
\midrule
\end{tabular}
\vspace{-10pt}
\caption{Quantitative comparison of our Spatial Prior-based Structure Estimation for 0.6k and 2k images. At 0.6k, COLMAP-Exhaustive (Ex) and COLMAP-VocabTree (Vo) are benchmarked. At the 2k scene, COLMAP fails consistently, with SfM processing times exceeding 24 hours, yielding no results. Our approach demonstrates significantly faster while maintaining comparable accuracy.}
\label{tab:modify}
\vspace{-15pt}
\end{table}

\subsection{Experimental Results}
\textbf{Waymo, KITTI and Mill-19.} Table \ref{tab:1} and Figure \ref{fig:1} show that our method outperforms others in both quantitative and qualitative measures, driven by precise structure estimation and graph-guided optimization. It accurately captures fine details such as tree branches without noticeable blurring, while maintaining superior shape, geometry, and color fidelity. Notably, BARF does not converge on two datasets, and SPARF, is suitable only for sparse image sets. UP-NeRF also struggles with convergence on the KITTI dataset. For large scenes like Mill-19, presented in Table \ref{tab:2} and Figure \ref{fig:2}, our method continues to demonstrate clear edges and detailed clarity in elements like rubble, grass, and stones.




\textbf{Spatial Prior-based Structure Estimation.}
% We will demonstrate the efficiency of our spatial prior-based structure estimation method via QF and CNNP in Section \ref{CNNP}, and highlight the importance of refined camera pose initialization for higher quality Gaussian splatting reconstruction. As shown in Table \ref{tab:modify}, thanks to our highly efficient structure estimation strategy, we achieve a 20x speed improvement in matching time and a 5x faster bundle adjustment with around 600 images compared to traditional strategies (exhaustive matching or vocabtree \citep{vocabtree} matching). This advantage becomes even more pronounced with large datasets, where naive 3DGS-based methods using COLMAP for initialization are impractical due to lengthy Structure from Motion times (typically >24 hours and always fails), while our method structure estimation stage in a few hours. Additionally, our method yields higher quality in the final Gaussian reconstruction, as we obtain more accurate camera poses while other strategies incorporate all cameras into pose calculation and cannot avoid the inclusion of invalid camera pairs.
We demonstrate the efficiency of our spatial prior-based method through Concentric Nearest Neighbor Pairing (CNNP \ref{CNNP}) and Quadrant Filter (QF \ref{QF}). Table \ref{tab:modify} shows our method increases matching speed 20x and reduces bundle adjustment time 5x compared to traditional approaches like exhaustive or vocabtree \citep{vocabtree} matching. For large datasets, where naive 3DGS methods using COLMAP are impractical due to long Structure from Motion times (typically $>$24 hours and always fails), our method completes structure estimation in just a few hours and achieves higher-quality reconstructions by accurately determining camera poses and avoiding invalid camera pairs.

\begin{figure}[t]% 'r'表示右侧，'0.5\textwidth'表示图形宽度为文本宽度的一半  
    \centering  
    \includegraphics[width=0.93\textwidth]{images/abof.pdf} % 插入图片，替换为实际图片文件名  
    \vspace{-15pt}
    \caption{Ablation Experiments on Adaptive Sampling Optimization. This figure highlights the impact of excluding our adaptive sampling optimization.}
    \label{abof}  
    \vspace{-5pt}
\end{figure} 

\begin{table}[h]
\centering
\small
\begin{tabular}{l|ccc}
\toprule
Method  & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\ \midrule


w/o QF    & 29.14 & 0.89 & 0.25  \\
w/o CNNP & 24.33 & 0.82 & 0.33  \\
w/o Structure Estimation                   & 24.77          & 0.803           & 0.444              \\
w/o Multi-view Consistency                   & 29.42          & 0.834           & 0.297              \\
%w/o \begin{tabular}[c]{@{}l@{}}Gaussian Selection\end{tabular} & 29.45          & 0.835          & 0.295              \\
\midrule
GraphGS(Ours)                         & \textbf{30.36}  & \textbf{0.891}  & \textbf{0.267} \\ 
\bottomrule    
\end{tabular}
\vspace{-5pt}
\caption{Quantitative Comparison of Ablation Experiments for Submodules.}
\label{abl:main}
\vspace{-10pt}
\end{table}

\begin{table}
\centering
\small
\setlength{\tabcolsep}{1pt} % 减少列间距
{
\begin{tabular}{l|cc|ccc}
\toprule
Method & \begin{tabular}[c]{@{}c@{}} Iterations\end{tabular} & \begin{tabular}[c]{@{}c@{}}Training\\ Time\end{tabular} $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\ 
\midrule
\begin{tabular}[c]{@{}l@{}}w/o Adaptive\\ Sampling\end{tabular} & 30000 & 54 min & 28.81 & 0.884 & 0.253 \\
\midrule
\begin{tabular}[c]{@{}l@{}}w Adaptive\\ Sampling\end{tabular} & \textbf{16830} & \textbf{28 min} & \textbf{30.36} & \textbf{0.891} & \textbf{0.267} \\
\bottomrule    
\end{tabular}
}
\vspace{-5pt}
\caption{Quantitative comparison of Ablation Experiments on Adaptive Sampling Optimization.}
\label{abl:adaptiv}
\vspace{-15pt}
\end{table}





\subsection{Ablation Study}
We conducted ablation studies to assess the contributions of our method’s components to the reconstruction process. This included evaluations of structural estimation and its sub-modules CNNP and QF, as well as the effects of graph-guided multi-view consistency constraint and adaptive sampling optimization on reconstruction quality.



Table \ref{abl:main} confirms the impact of each module on reconstruction quality. On the Waymo dataset, omitting the QF leads to reduced accuracy due to the inclusion of non-intersecting camera pairs in pose estimation, affecting camera pose precision. Similarly, excluding the CNNP module results in a notable decline in quality because it fails to accurately capture global poses through local matching. Integrating structure estimation significantly enhances reconstruction quality by precisely estimating the global scene structure. Additionally, the inclusion of graph-guided multi-view consistency constraints further improves outcomes by maintaining consistency across multiple views.

Table \ref{abl:adaptiv} demonstrates that the adaptive sampling optimization method significantly reduces optimization time and improves the quality of reconstruction. As illustrated in Figure \ref{abof}, this approach mitigates overfitting to specific viewpoints by optimizing the distribution of Gaussian points. This enhancement not only prevents the occlusion of other perspectives but also reduces severe distortions within the reconstructed geometry. For more ablation experiments, see Appendix \ref{add:abl}.



\section{Conclusion}
In conclusion, through our proposed structure estimation and graph-guided optimization methods, GraphGS can achieve high-quality, rapid reconstruction of large scenes from image sets without ground truth poses. This capability is particularly meaningful for multimedia applications such as virtual reality, gaming, and the metaverse. GraphGS not only meets the growing demand for fast and reliable scene reconstruction but also provides a scalable and accessible solution.

\textbf{Limitations.} The absence of rear camera data and the anisotropic nature of the method limit the comprehensive capture and reconstruction of reverse scenes. Additionally, the reliance on feature points may cause the matching strategy to fail randomly, particularly in scenes with numerous dynamic objects. The effects on distant views and the blurring caused by large Gaussian points in certain shots still need improvement and resolution. For more discussion, see Appendix \ref{App:limi}.


\section*{Acknowledgment}
This research is supported by the National Natural Science Foundation of China (No. 62406267), Tencent Rhino-Bird Focused Research Program, Guangzhou-HKUST(GZ) Joint Funding Program (Grant No.2025A03J3956), the Guangzhou Municipal Science and Technology Project (No. 2025A04J4070), the Guangzhou Municipal Education Project (No. 2024312122) and Education Bureau of Guangzhou Municipality.


\clearpage
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\clearpage
\appendix




\section{Appendix: Proof of Graph connectivity}
\label{appendix-connect}


For Equation. \ref{eq-CNNP}, if we remove $S_{\text{connection}}^{(i)}$, the $S_{all}$ would be as follows:
\begin{gather}\label{eq-CNNP-wo3}
    S_{all}=\bigcup \limits _{i=1}^N{(S_{\text{neighbor}}^{(i)} \cup S_{\text{concentric}}^{(i)})} \\
    S_{\text{neighbor}}^{(i)}=\{(c_i,c_j) | s(i,j) \leq r, j=1,2,...,N, j\neq i \} \notag\\
    S_{\text{concentric}}^{(i)}=\{(c_i,c_j) | 0 \leq (s(i,j)-r) \% (h+w) < w , j=1,2,...,N, j\neq i \} \notag 
\end{gather}
In the scenario, the camera graph may not be connected graph. It would be fairly troublesome to give direct proof. However, when considering proof by contradiction, one counter example for one special case of $r$,$h$,$w$ is enough. 

\textbf{Proof:}

We choose the special case when $r=2$, $h=N-3$, $w=1$, which means for every camera node in graph, we only add edges to its nearest camera and farthest camera. In this scenario, one counter example is illustrated in Fig. \ref{fig:supp-connect}. The graph nodes represent cameras while the position of graph nodes also represents the actual position of cameras. For example, camera node $1$ connected camera $2$ and camera $8$ since they are the nearest and farthest cameras to camera $1$ respectively. After the edge adding algorithm done, the camera $1,2,7,8$ and $3,4,5,6$ form two different connected components, while the entire camera graph is not a connected graph.

\begin{figure}
  \centering
% \vspace{-10pt}
  \includegraphics[width=0.3\textwidth]{images/graph_appendix.pdf}
  % \vspace{-10pt}
  \caption{Counter example for one special case of $r$,$h$,$w$. In the special case, $r=2$, $h=N-3$, $w=1$. This means for every camera, we will only add its nearest camera and farthest camera as graph edge. We list one counter example with $N=8$. In the counter example, the red nodes and yellow nodes are disconnected, while all the nodes follow the matching algorithm.}
  \label{fig:supp-connect}
  % \vspace{-10pt}
\end{figure}

\section{Appendix: State Table of Quadrant Filter}
\label{appendix-state-table}
In this part, we introduce quadrant division and the state table for filtering noise camera pairs as described in Section \ref{QF} of the main paper. 

\textbf{Quadrant Division.} As illustrated in Figure \ref{fig:supp-qf}, we adopt the coordinate system of OpenCV, which is a right-handed system. Within this camera coordinate system, it is assumed that the camera is facing $+z$ axis. We adhere to this assumption when calculating the relative orientation of cameras.
\begin{figure}
  \centering
% \vspace{-10pt}
  \includegraphics[width=0.5\textwidth]{images/supp-qf.pdf}
  % \vspace{-10pt}
  \caption{Illustration of quadrant division: When calculating the relative position of cameras $c_i$ and $c_j$, we assume that camera $c_i$ is positioned at the origin. In determining the relative direction, we assume that the orientation of camera $c_i$ is facing the $+z$ axis.}
  \label{fig:supp-qf}
  % \vspace{-10pt}
\end{figure}


\textbf{State Table.} For state table, we identify 64 potential configurations describing the relative relationship between two cameras. For the conciseness of representation, we only illustrate the successful matching pairs, as shown in Table \ref{tab:supp-state}. The "position quadrant" column lists all possible quadrants of the relative position between cameras $c_i$ and $c_j$, while the "orientation quadrant" column indicates the quadrant of relative orientation that enables successful matching. For instance, if $c_j$ is positioned in the right upper rear quadrant of $c_i$ (quadrant 1 in Fig. \ref{fig:supp-qf}), only a facing orientation in quadrant 7 (relative orientation) satisfies the matching condition under strict mode. Practically, we employ Loose Mode for vehicle data due to the elongated and narrow camera track. For other situations, we apply Strict Mode.

\textbf{Probability of filter under Strict/Loose Mode.} For the situation of random camera position and orientation, the strict and loose mode of state table have different probabilities to filter irrelevant camera pairs, which can be calculated theoretically to measure efficiency. For Strict mode, the probability of filter is
\begin{equation}
    P_{\text{strict}}=\frac{7}{8}\times \frac{1}{8} \times 4 + \frac{6}{8}\times \frac{1}{8}\times 4 = \frac{13}{16} \approx 81.3\%
\end{equation}
For Loose mode, the probability of filter is
\begin{equation}
    P_{\text{loose}}=\frac{4}{8}\times \frac{1}{8} \times 4 + \frac{2}{8}\times \frac{1}{8}\times 4 = \frac{6}{16} \approx 37.5\%
\end{equation}
It is notable even in loose mode, the probability of filter is still a high number, meaning more than $1/3$ noise camera pairs are filtered. This greatly reduced the burden of calculation in the following structure pipeline compared to the situation without quadrant filter. 


\begin{table}
\centering
\begin{tabular}{c|cc}

\toprule
\multirow{2}{*}{Position Quadrant} & \multicolumn{2}{c}{Orientation Quadrant}       \\ \cline{2-3} 
                                   & \multicolumn{1}{c|}{Strict Mode} & Loose Mode  \\ \midrule
1                                  & \multicolumn{1}{c|}{7}           & 2, 3, 6, 7     \\
2                                  & \multicolumn{1}{c|}{7, 8}         & 2, 3, 4, 6, 7, 8 \\
3                                  & \multicolumn{1}{c|}{5, 6}         & 1, 2, 3, 5, 6, 7 \\
4                                  & \multicolumn{1}{c|}{6}           & 2, 3, 6, 7     \\
5                                  & \multicolumn{1}{c|}{3}           & 2, 3, 6, 7     \\
6                                  & \multicolumn{1}{c|}{3, 4}         & 2, 3, 4, 6, 7, 8 \\
7                                  & \multicolumn{1}{c|}{1, 2}         & 1, 2, 3, 5, 6, 7 \\
8                                  & \multicolumn{1}{c|}{2}           & 2, 3, 6, 7     \\ \bottomrule
\end{tabular}
\caption{State Table Illustration.}
\label{tab:supp-state}
% \vspace{-10pt}
\end{table}



\section{Appendix: Proof of Proposition}
\label{appendix-propprof}
In the Proposition \ref{prop-1} of the main paper, we utilize the internal relations of cross product and inner product within a right-handed 3D coordinate system to calculate the relative orientation without the need to solve the linear system:
\begin{equation}\label{eq-comp}
    \mathcal{B}_d^{(i,j)}=\{sgn(e_x[d_{\times}^{(i)}]d^{T(j)}),sgn(e_y[d_{\times}^{(i)}]d^{T(j)}),sgn(d^{(i)}d^{T(j)})\}
\end{equation}
where $[d_{\times}^{(i)}]$ represents the anti-symmetric matrix of cross product:
\begin{equation}
    [d_{\times}^{(i)}]=\begin{bmatrix}
0 & -v_z^{(i)} & v_y^{(i)} \\
v_z^{(i)} & 0 & -v_x^{(i)} \\
-v_y^{(i)} & v_x^{(i)} & 0
\end{bmatrix}
\end{equation}
and $e_x=[1,0,0], e_y=[0,1,0]$ represent unit vector of $x,y$ axis. However, its mathematical principles are omitted due to limited space. Here we will explain the rationality of our method.

\textbf{Proof:}

For cameras $c_i$ and $c_j$ with absolute orientations $d^{(i)}$ and $d^{(j)}$, we use their cross product $d^{(i)} \times d^{(j)} = [v_x^\times, v_y^\times, v_z^\times]$ to reflect the relative orientation to some extent. We envision transforming them into a standard coordinate system as illustrated in Figure \ref{fig:supp-qf} through several steps:
\begin{itemize}
\item \textbf{Step 1:} Translate them to the origin of the standard coordinate. This process does not alter the cross product or their relative orientation.
\item \textbf{Step 2:} From an overhead view of the standard coordinate (where the $y$-axis points into the page), we note that the plane formed by $d^{(i)}$ and $d^{(j)}$ divides the $x$-$z$ plane into two parts. We then transform $d^{(i)}$ and $d^{(j)}$ such that $d^{(i)} \times d^{(j)}$ changes from $[v_x^\times, v_y^\times, v_z^\times]$ to $[v_x^\times, v_y^\times, 0]$, while maintaining their relative orientation. This transformation is feasible because the linear system we aim to solve is not full rank. After this transformation, the plane of $d^{(i)}$ and $d^{(j)}$ coincides with the $z$-$y$ plane.
\item \textbf{Step 3:} We rotate $d^{(i)}$ and $d^{(j)}$ around the $x$-axis so that $d^{(i)}$ aligns with the $z$-axis.
\end{itemize}

\begin{figure}
  \centering
% \vspace{-10pt}
  \includegraphics[width=0.6\textwidth]{images/supp-steps.pdf}
  % \vspace{-10pt}
  \caption{Imagined Transform Illustration: Throughout the entire process, the relative orientation of $d^{(i)}$ and $d^{(j)}$ is maintained without additional calculation.}
  \label{fig:supp-steps}
  % \vspace{-15pt}
\end{figure}

After the aforementioned steps, as depicted in Figure \ref{fig:supp-steps}, we have successfully translated $d^{(i)}$ and $d^{(j)}$ from an arbitrary situation to the standard coordinate with $d^{(i)}$ pointing toward the $+z$ axis. In this process, no transformation matrix is required, while $v_x^\times$, $v_y^\times$, and the relative orientation of $d^{(i)}$ and $d^{(j)}$ are preserved. This sets the stage for further analysis.

\begin{figure}
  \centering
% \vspace{-10pt}
  \includegraphics[width=0.5\textwidth]{images/supp-oriqf2.pdf}
  % \vspace{-10pt}
  \caption{Four situations for the plane of $d^{(j)}$, which depends on the sign of $v_x^\times,v_y^\times$.}
  \label{fig:supp-4}
  % \vspace{-10pt}
\end{figure}

We then consider all potential positions of $d^{(j)}$. As shown in Figure \ref{fig:supp-4}, $d^{(j)}$ lies on a green plane that can rotate around the $z$-axis. Viewing the coordinate system from the front, where the $z$-axis points into the page, and due to the constraints of the right-handed rule, the potential positions of $d^{(j)}$ are limited to half of the green plane, contingent upon one of four scenarios based on $[v_x^\times, v_y^\times, 0]$. Thus, we can limit the relative orientation of $d^{(j)}$ to two of the eight quadrants through $v_x^\times$ and $v_y^\times$.

Finally, we simply calculate the inner product of $d^{(i)}$ and $d^{(j)}$ to determine the relative orientation along the $z$-axis (either quadrant $2, 3, 6, 7$ or $1, 4, 5, 8$). Based on the derivation above, the final $\mathcal{B}_d^{(i,j)}$ can be rewritten in a more direct form without a matrix:
\begin{equation}\label{eq-easy}
\mathcal{B}_d^{(i,j)} = \{\operatorname{sgn}(v_x^\times), \operatorname{sgn}(v_y^\times), \operatorname{sgn}(d^{(i)} \cdot d^{(j)})\}
\end{equation}
The Equation \ref{eq-easy} and Equation \ref{eq-comp} are actually equivalent, while we keep Equation \ref{eq-comp} in the main text for the consistency of context.


\section{Appendix: Additional Experimental Results}
\label{add:exper}
\begin{table}[ht]
\centering
\label{tab:comparison}
\begin{tabular}{lcccc}
\hline
\textbf{Method} & \textbf{Time (min)} & \textbf{PSNR} & \textbf{SSIM} & \textbf{LPIPS} \\ \hline
ACEZero         & 10                  & 17.50         & 0.725         & 0.354          \\
PixSfM          & 130                 & 28.75         & 0.847         & 0.366          \\
GLOMAP          & 28                  & 27.65         & 0.824         & 0.388          \\
COLMAP          & 154                 & 29.14         & 0.89          & 0.250          \\
\textbf{Ours}   & \textbf{23}         & \textbf{30.36} & \textbf{0.891} & \textbf{0.267} \\ \hline
\end{tabular}
\label{add:sfmcom}
\caption{Quantitative Comparison of SfM on Waymo dataset.}
\end{table}
As shown in Table \ref{add:sfmcom}, our method demonstrates significant improvements over other state-of-the-art SfM systems in terms of processing time and accuracy metrics. Despite the emergence of numerous effective Structure from Motion (SfM) methods, COLMAP continues to be widely recognized for its robustness and accuracy in the community, thereby serving as a benchmark for comparisons among incremental SfM \citep{pan2024globalstructurefrommotionrevisited}. Notably, VGGSfM \citep{wang2023visualgeometrygroundeddeep} was excluded from detailed comparisons due to memory limitations when processing large image datasets. In contrast, ACEZero \cite{brachmann2024scenecoordinatereconstructionposing}, although faster, faces convergence issues in larger datasets. Our method not only surpasses others in terms of results from novel view synthesis but also maintains a faster processing speed.


\section{Appendix: Additional Ablation Studies}
\label{add:abl}

\textbf{Impact of Octree Initialization on reconstruction quality and training time.}
Quantitative results in Table \ref{abl:octree} show that our octree point initialization strategy cuts training time in half without compromising reconstruction quality. Further reductions in initial points did not significantly decrease training times, likely due to the densification and point operations.

\begin{table}
\centering
{
\begin{tabular}{l|cc|ccc}
\toprule
Method & \begin{tabular}[c]{@{}c@{}}Initial \\Points Number\end{tabular} & \begin{tabular}[c]{@{}c@{}}Training\\ Time\end{tabular} $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\ 
\midrule
\begin{tabular}[c]{@{}l@{}}w/o Octree Point\\ Initialization\end{tabular} & 245820 & 54 min & 29.74 & 0.841 & \textbf{0.291} \\
\midrule
\begin{tabular}[c]{@{}l@{}}w Octree Point\\ Initialization\end{tabular} & 100000 & \textbf{36 min}  & \textbf{29.75} & \textbf{0.843} & 0.292 \\
\bottomrule   
\end{tabular}
}
\caption{Quantitative comparison of Ablation Experiments on Octree Point Initialization.}
\label{abl:octree}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{images/3.pdf}
  \vspace{-10pt}
  \caption{Qualitative comparison of novel view synthesis with added random Gaussian noise to simulate imprecise pose data. Our method retains clarity and definition, while alternative approaches like PVG~\citep{chen2024periodic} and 3DGS~\citep{wu20234dgaussians} show distortions under similar conditions.
  }
  \label{fig:cam-dist}
\end{figure}

\begin{table}
\centering
\begin{tabular}{l|ccc}
% \small
\toprule
Method                                & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\ \midrule
3DGS ($\mathbf{R}$*0.3 \& $\mathbf{T}$*0.3) & 12.676           & 0.510           & 0.613              \\
PVG ($\mathbf{R}$*0.3 \& $\mathbf{T}$*0.3) & 12.695           & 0.512           & 0.615              \\
\toprule
Our ($\mathbf{R}$*0.3 \& $\mathbf{T}$*0.3)  & \textbf{28.53}  & \textbf{0.886}  & \textbf{0.221}     \\ \bottomrule
\end{tabular}
\smallskip
\par
\caption{Quantitative comparison of novel view synthesis results on Waymo dataset with additional noise. $\mathbf{R}$*0.3 and $\mathbf{T}$*0.3 represent the addition of 0.3 units of rotation and translation noise, respectively.}
\label{tab:cam-dist}
% \vspace{-15pt}
\end{table}

\textbf{Camera Disturbance.}
To simulate real-world conditions where camera poses are uncertain, we introduce disturbances by adding random noise from $0$ to $0.3$ radians to camera orientations and positions in a selected Waymo scene. We assess the robustness of our methods by comparing them with 3DGS and PVG under these conditions.




% Compared to current methods, our approach remains robust when handling disturbances in camera poses, a common issue in real-world applications such as road bumps or loose equipment. As illustrated in Figure \ref{fig:cam-dist} and Table \ref{tab:cam-dist}, disturbances are detrimental to methods based on traditional Structure from Motion pipelines, as the quality of Gaussian splatting heavily relies on accurate camera pose information. In Figure \ref{fig:cam-dist}, other methods experience severe blurring, making it difficult to distinguish scene objects; this flaw is also evident in Table \ref{tab:cam-dist}. In contrast, our method shows no significant decline in quality, as we recalculate refined camera poses based on key camera pairs.
Our method remains robust against disturbances in camera poses, common in real-world scenarios like road bumps. Figure \ref{fig:cam-dist} and Table \ref{tab:cam-dist} show that traditional 3DGS methods degrade significantly under such disturbances, with heavy blurring that obscures scene objects. In contrast, our approach maintains quality by calibrating camera poses using key camera pairs.

\textbf{Comparison with naive overlapping frusta method}
We conduct an experiment with the naive frusta method and compare it to our Concentric Nearest Neighbor Pairing (CNNP), Quadrant Fileter (QF) methods when selecting matched pairs. We implement the frusta algorithm with the following steps:
\begin{itemize}
    \item The intrinsic matrix of camera is obtained to calculate the near-far plane.
    \item The frusta of cameras is transformed to world coordinate system.
    \item We check the intersection of their bounding box to judge the view intersection.
\end{itemize}

As shown in Tab. \ref{tab:cnnpqfcomp}, and further analysis are presented as follows:
\begin{itemize}
    \item  \textbf{Universality}. Notably, our matching strategy only needs the position and orientation of coarse cameras, i.e., only the 3-rd, 4-nd colume of extrinsic matrix. Moreover, we do not need the information of intrinsic matrix. This indicates the universality of our method.
    \item  \textbf{Efficiency}. As shown in QF section and Appendix \ref{appendix-propprof}, we calculate the relative orientation with only 1 inner product and 1 cross-product based on fully convincing mathematical proof. As shown in column of pairing time, this way greatly improves our efficiency compared to frusta method, which contains time-consuming coordinate transform and intersection checking process.
    \item  \textbf{Reconstruction Quality}. In terms of reconstruction quality, there is no obvious difference between our method and frusta, since pose accuracy is well-solved in BA procedure. Our approach offers better compatibility in cases where initial poses are poor.
    \item  \textbf{Reason why frusta failed}. Though the naive frusta method sounds reasonable, however, in practice, especially in the driving scenario with long and narrow camera track, the frusta method cannot filter image pairs efficiently since all of the camera orientation are extremely similar.
    
\end{itemize}


\begin{table}[]
\begin{tabular}{c|c|ccc|ccc}
\toprule
Images & Method         & Pairing Time & Matching Time     & BA Time           & PSNR  & SSIM & LPIPS \\ \midrule
0.6 k  & Ours  & 0.2 min      & 3 min             & 20 min            & 30.18 & 0.90 & 0.24  \\ \midrule
0.6 k  & Frusta         & 2 min        & 45 min            & 86 min            & 30.15 & 0.89 & 0.24  \\ \midrule
2 k    & Ours  & 2.1 min      & 12 min            & 194 min           & 26.60 & 0.85 & 0.16  \\ \midrule
2 k    & Frusta         & 20 min       & \textgreater 24 h & \textgreater 24 h & -     & -    & -     \\ \bottomrule
\end{tabular}
\caption{Comparation of our Concentric Nearest Neighbor Pairing (CNNP), Quadrant Fileter (QF) and naive frusta method on Waymo (0.6 k) and Mill 19 (2 k) datasets.}
\label{tab:cnnpqfcomp}
\end{table}



\section{Appendix: Experimental Details}
\label{App:Experimental}
Experiments were conducted using an NVIDIA RTX 3090 GPU and an AMD EPYC 7542 CPU. The Waymo includes 600 images from three viewpoints (left, center, right), covering 599 image pairs. The Kitti includes 100 images with 99 image pairs. The Mill 19 comprises 2000 images with 1999 pairs included.

For the CNNP configuration, the settings of $r=5$, $h=20$, $w=1$ were employed. Under this setting, approximately $\sim 18k / C^2_{600}$ pairs will be selected for 0.6k images, and $\sim 200k / C^2_{2000}$ pairs will be selected for 2k images. The setting $r=5$ designates the five nearest cameras as matching candidates for each camera $c_i$. The configuration $h=20$, $w=1$ means that one camera is chosen as a match for $c_i$ out of every 20 cameras based on proximity. 

To optimize multi-view consistency and balance the coefficients, we set the coefficient $\lambda$ to 0.07, which was empirically found to be optimal in our experiments. Additionally, we adopted a graph-guided optimization setup, where the sampling probability is determined by the weights of the graph nodes. We set the minimum sampling probability at 0.5 to ensure that nodes with lower weights are not overlooked.



\section{Appendix: Discussion of Limitations and Challenges.}
\label{App:limi}
Our research focuses on 3DGS reconstruction of outdoor, unbounded scenes, primarily facing two challenges: 1) Accurate pose estimation outdoors is difficult due to the unpredictability and complexity of outdoor environments; 2) Outdoor scenes generally have sparser camera coverage and less overlap between images compared to indoor settings, resulting in insufficient constraints during training. To address these challenges, we introduced two key modules: Spatial Prior-Based Structure Estimation and a graph-guided Gaussian optimization strategy. These modules are designed to efficiently and accurately complete the reconstruction of outdoor scenes.

However, our method shows limited improvement in 3DGS reconstruction of objects. This is primarily because datasets in this category usually have precise ground truth poses, and the camera setups often rotate 360 degrees around the object, providing sufficient image overlap and constraints to aid convergence. This results in nearly equal importance weights for graph nodes, which diminishes the impact of our optimization.

Moreover, our approach relies on the accuracy of rough spatial priors. If the initial camera pose distribution in the xy plane is inaccurate, it may lead to Bundle Adjustment (BA) failure. In such cases, our method might not effectively handle pose estimation errors.



\end{document}
