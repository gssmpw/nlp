\section{Related Work}
\subsection{Vertebral Landmarks' Localization}
Over the past few decades, numerous attempts have been made to accurately locate vertebral landmarks on spine images. However, there has been limited research specifically focused on DXA imaging, especially for LSIs. Early efforts were primarily based on hand-crafted features. For example, Elmasri et al. \cite{k_elmasri} used 56 landmarks to train an active appearance model for localizing the lumbar region and the abdominal aorta in DXA LSIs. Chaplin et al. \cite{t_cootes} later trained a statistical model on Sagittal CT images using 30 landmarks to locate the lumbar vertebrae, which was then applied to DXA LSIs. Unfortunately, these methods showed limited success. Recently, deep learning-based approaches have gained popularity due to their superior performance over traditional methods. Sun et al. \cite{sun} introduced a multitask framework for VLL in Anterior-Posterior (AP) view X-ray images, while Wu et al. \cite{wu} proposed BoostNet, which achieved robust VLL by eliminating harmful outliers in the feature space. Yang et al. \cite{yang} developed a VLL framework based on heatmaps and incorporated a Markov Random Field model for refining landmarks. Payer et al. \cite{payer} proposed a CNN model that learns the spatial configuration of landmarks, improving robustness by eliminating the need for heatmap-based localization. Yi et al. \cite{yi} further advanced this by generating three outputs for landmark localization: center offset, corner offset, and heatmap-based center localization. Guo et al. \cite{guo} introduced a vertebra segmentation task using a key-point-based transformer architecture to capture the relationships between the global spine structure and local vertebrae. Most recently, Huang et al. \cite{zixun} presented the NFDP model, a generative distribution prior-based model that significantly improved results in AP view X-ray images. However, most of this work has focused on X-Ray images. Apart from Elmasri et al. \cite{k_elmasri} and Chaplin et al. \cite{t_cootes}, the only deep learning-based model targeting DXA LSIs for landmark localization is by Ilyas et al. \cite{zaid}. Their proposed framework localized T12 to L5 vertebrae to generate IVGs that are essential for AAC 24 scoring, but was limited by a small dataset of 197 DXA images from a single Hologic machine. It also struggled to capture both local and global contexts. \par

\begin{figure}[t]
\centerline{\includegraphics[width=\columnwidth]{fig2.png}}
\caption{(a) A DXA LSI example with red arrows marking the location of AAC. (b) An illustration of Kauppila's AAC-24 scoring method~\cite{kauppila}. (c) A DXA image example showing unclear vertebral boundaries, with (d) indicating the intended placement of IVGs needed for AAC-24 scoring.}
\label{fig2}
\end{figure}

\subsection{Transformer Models for Vertebral Landmarks' Localization}
Transformers were initially developed for Natural Language Processing (NLP) tasks, with self-attention as the core mechanism that allows the model to capture long-range dependencies within input sequences~\cite{attention}. In computer vision, the Vision Transformer (ViT) was the first transformer model introduced and demonstrated outstanding performance by dividing the input image into patches and applying self-attention to them~\cite{vit}. Transformers have also excelled in medical imaging tasks like classification and segmentation. However, there has been limited research on using transformer models for VLL from X-ray images. Guo \cite{guo} introduced a keypoint transformer model to measure the Cobb angle in AP view X-ray spine images. Zhao et al. \cite{moxin} developed SpineHRformer, a hybrid CNN-transformer model for landmark localization in AP view X-ray images. To the best of our knowledge, no transformer-based model has been proposed for VLL in lateral view DXA images. \par