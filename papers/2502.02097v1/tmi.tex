\documentclass[journal]{IEEEtran}  % Use standard IEEEtran class
\usepackage{cite}                   % For references
\usepackage{amsmath,amssymb}        % For math symbols and equations
\usepackage{graphicx}               % For including graphics
\usepackage{multirow}               % For multirow tables
\usepackage{url}                    % For URLs
\usepackage{comment}
\markboth{Ilyas \MakeLowercase{\textit{et al.}}: VerteNet --- A Multi-Context Hybrid CNN Transformer for Accurate Vertebral Landmark Localization in DXA Images}
{Ilyas et al.}

\begin{document}
\title{VerteNet --- A Multi-Context Hybrid CNN Transformer for Accurate Vertebral Landmark Localization in Lateral Spine DXA Images}
\author{Zaid Ilyas, Arooba Maqsood, Afsah Saleem, Erchuan Zhang\textsuperscript{$\star$}, David Suter, Parminder Raina, Jonathan M. Hodgson, John T. Schousboe, William D. Leslie, Joshua R. Lewis\textsuperscript{$\dagger$} and Syed Zulqarnain Gilani\textsuperscript{$\dagger$} 
\thanks{This work was supported by the Health Partners Institutional Review Board (\#A20-149), the Edith Cowan University Human Research Ethics Committee (Project Number: 20513 HODGSON), the Health Research Ethics Board at the University of Manitoba (HREB H2004:017L, HS20121), the Manitoba Health Information Privacy Committee 
signed consent (HIPC 2016/2017–29), and the National Health and Medical Research Council of Australia Ideas Grant (APP1183570). Additional funding was provided by the Rady Innovation Fund at the University of Manitoba, the Raine Medical Research Foundation of Australia, and the National Heart Foundation of Australia Future Leader Fellowship (ID: 102817). JRL`s salary was supported by the National Heart Foundation of Australia Future Leader Fellowship (ID: 102817). SZG's salary was partially covered by the Raine Medical Research Foundation through the Raine Priming Grant.}
\thanks{Zaid Ilyas, Arooba Maqsood, Afsah Saleem, David Suter, Joshua R. Lewis, and Syed Zulqarnain Gilani are with Center for AI \& ML, School of Science, and Nutrition and Health Innovation Research Institute, Edith Cowan University, Australia (e-mail: z.ilyas@ecu.edu.au). }
\thanks{Syed Zulqarnain Gilani is with Computer Science and
Software Engineering, The University of Western Australia.}
\thanks{Erchuan Zhang is with the School of Science, Sun Yat-sen University, China.}
\thanks{Jonathan M. Hodgson is with the School of Medical and Health Sciences, and Nutrition and Health Innovation Research Institute, Edith Cowan University, Australia.}
\thanks{John T. Schousboe is with Park Nicollet Clinic and HealthPartners Institute, Minneapolis, USA.}
\thanks{William D. Leslie is with the Department of Medicine and Radiology, University of Manitoba, Canada.}
\thanks{Parminder Raina is with the Department of Health Sciences, McMaster University, Canada.}
\thanks{$\star$ Corresponding Author (e-mail: zhangerch@mail.sysu.edu.cn)}
\thanks{$\dagger$ Joint Last Authors}
}



\maketitle



\begin{abstract}
Lateral Spine Image (LSI) analysis is important for medical diagnosis, treatment planning, and detailed spinal health assessments. Although modalities like Computed Tomography and Digital X-ray Imaging are commonly used, Dual Energy X-ray Absorptiometry (DXA) is often preferred due to lower radiation exposure, seamless capture, and cost-effectiveness. Accurate Vertebral Landmark Localization (VLL) on LSIs is important to detect spinal conditions like kyphosis and lordosis, as well as assessing Abdominal Aortic Calcification (AAC) using Inter-Vertebral Guides (IVGs). Nonetheless, few automated VLL methodologies have concentrated on DXA LSIs. We present VerteNet, a hybrid CNN-Transformer model featuring a novel dual-resolution attention mechanism in self and cross-attention domains, referred to as Dual Resolution Self-Attention (DRSA) and Dual Resolution Cross-Attention (DRCA). These mechanisms capture the diverse frequencies in DXA images by operating at two different feature map resolutions. Additionally, we design a Multi-Context Feature Fusion Block (MCFB) that efficiently integrates the features using DRSA and DRCA. We train VerteNet on 620 DXA LSIs from various machines and achieve superior results compared to existing methods. We also design an algorithm that utilizes VerteNet's predictions in estimating the Region of Interest (ROI) to detect potential abdominal aorta cropping, where inadequate soft tissue hinders calcification assessment. Additionally, we present a small proof-of-concept study to show that IVGs generated from VLL information can improve inter-reader correlation in AAC scoring, addressing two key areas of disagreement in expert AAC-24 scoring: IVG placement and quality control for full abdominal aorta assessment. The code for this work can be found at https://github.com/zaidilyas89/VerteNet.
\end{abstract}

\begin{IEEEkeywords}
Vertebral Landmarks, Dual-Energy X-Ray Absorptiometry, Dual Resolution Cross Attention, VerteNet, Multi-Context Feature Fusion Block.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{L}{ateral} Spine Images (LSIs) can be captured through modalities such as Computed Tomography (CT), Digital X-Ray Imaging (DXI), Magnetic Resonance Imaging (MRI) or Dual-Energy X-ray Absorptiometry (DXA). LSIs play an important role in medical diagnostics and treatment planning. Some important applications include spinal alignment assessment to diagnose abnormal curvature conditions such as kyphosis~\cite{kyphosis} and lordosis~\cite{lordosis}, Vertebral Fractures Assessment (VFA)~\cite{vfa}, Bone Mineral Density (BMD) calculation for osteoporosis analysis~\cite{osteoporosis}, and Abdominal Aortic Calcification (AAC) detection~\cite{aac1, aac2}. A comparison of CT, DXI, and DXA LSI scans is shown in Figure~\ref{fig1}. While both CT and DXI are effective for LSI analysis, DXA is the fastest, most affordable, and least radiation-intensive option among radiation-based methods. As a result, it is becoming the preferred choice for VFA in routine osteoporosis screening. DXA imaging is conventionally used for the calculation of BMD~\cite{osteoporosis}, however, recent studies have shown that DXA LSIs can also be used to detect AAC, a stable marker for the development of Cardiovascular Diseases (CVD)~\cite{aac1, aac2, showattend, afsah, naeha, zilyas_miccai2024}. Quantification of AAC on LSIs typically employs, the most widely adopted, Kauppila's AAC-24 point scoring method~\cite{kauppila} as shown in the Figure~\ref{fig2}. An important part of this scoring technique is the identification of intervertebral boundaries between vertebrae T12, L1, L2, L3, L4, and L5, and this process requires localization of vertebral boundaries. 

\begin{figure*}[!t]
\centerline{\includegraphics[width=1\textwidth]{fig1.png}}
\caption{(a) CT Lateral Spine Imaging – the gold standard, slower with highest radiation exposure \cite{ct}. (b) Digital X-Ray Imaging – a faster option with lower radiation exposure than CT \cite{dxi}. (c) Hologic DXA SE variant – quickest with lowest radiation exposure, though susceptible to artifacts such as bowel gas. (d) and (e) GE DXA SE and DE variants – equipped with radiation-reducing technology (black regions), offering comparatively fast imaging with low radiation exposure.}
\label{fig1}
\end{figure*}

After identification of vertebral boundaries, AAC is quantified on a scale of 0-24 (For details, please refer to~\cite{zaid}). The entire AAC-24 scoring procedure, which encompasses the identification of vertebral boundaries (whether mentally or digitally drawn) and the detection of AAC, is challenging, time-intensive, and requires specific expertise. This difficulty arises from factors such as artifacts from kidney stones, bowel gas, and occasionally ambiguous vertebral boundaries. In the context of routine clinical analysis, the scans presented in Figures~\ref{fig1}(c), (d), and (e) illustrate cases where unclear vertebral boundaries complicate the accurate localization of vertebral landmarks. This challenge requires extra time and effort for image processing to improve quality and conduct analysis.\par
Recent efforts have focused on automating two clinically relevant procedures related to analyzing DXA LSI, that is, VLL~\cite{k_elmasri, t_cootes, wu, sun, yang, payer, zixun, zaid} and AAC quantification~\cite{k_elmasri, t_cootes, dxapilot, showattend, afsah, naeha, zilyas_miccai2024}. However, there is still a need for improvement in both procedures, especially in the domain of VLL which is still underexplored and is the focus of this work. Preliminary endeavours by Elmasri et al.~\cite{k_elmasri} and Chaplin et al.~\cite{t_cootes} employed traditional active appearance and shape-based models for vertebral localization. Nevertheless, these approaches demonstrated suboptimal performance due to their reliance on constrained datasets and conventional techniques. To the best of our knowledge, the only deep learning-based work that has considered VLL in DXA LSIs is by Ilyas et al.~\cite{zaid} who proposed `GuideNet', a Convolutional Neural Network (CNN) based model trained on 197 DXA images from a Hologic machine. Although GuideNet achieved favourable results, it exhibited insufficient generalization capabilities when processing images from various DXA machines. Additionally, its performance was notably deficient in scenarios involving complex image backgrounds. \par

To accurately categorize or localize components within an image that have a complicated background, low contrast, low signal-to-noise ratio, and artifacts, it is essential to consider both the overall context and the local fine-grained information. For example, without the information of curvature, location, and shape of the spine, vertebral boundaries can be miscategorized as an artifact, or an artifact can be misclassified as AAC. Similarly, without fine-grained textural information, it is difficult to accurately localize the corners of the vertebrae or properly quantify the small amounts of AAC. This can be understood in terms of frequencies embedded in medical images. Low Frequencies (LF) are associated with information such as the shape and curvature of the spine, and High Frequencies (HF) are associated with fine-grained textural information. Both categories of frequency play an important role in informing the decision-making process in image analysis. To the best of our knowledge, our research~\cite{zilyas_miccai2024} represents the only effort in the domain of lateral view DXA image analysis that incorporates both LF and HF components. However, its principal focus remains on the quantification of AAC through the innovative application of Dual Resolution Self Attention (DRSA). This paper extends the concept of~\cite{zilyas_miccai2024} to VLL in lateral spine DXA images by proposing `VerteNet', which is an encoder-decoder model with skip connections. The novelty is in the design of a Multi-Context Feature Fusion Block (MCFB) in the decoder part that operates at different hierarchical levels and utilizes DRSA~\cite{zilyas_miccai2024}, a new Dual Resolution Cross Attention (DRCA) block and Channel-wise Self-Attention (CSA)~\cite{restormer, csa2}.\par

We conduct training and evaluation of VerteNet on a dataset of 620 DXA LSIs obtained from three distinct devices: the iDXA GE machine (100 Single Energy (SE) and 100 Dual Energy (DE)), the Hologic 4500A machine (100 SE), and the Hologic Horizon machine (320 SE). We also compare its performance with GuideNet~\cite{zaid}, HRNet~\cite{hrnet}, and the recently proposed NFDP model~\cite{zixun}. Our proposed model VerteNet significantly out-performs  the three models mentioned above. \par

To highlight the importance of precise vertebral landmark localization (VLL) in DXA images of the lateral spine, we propose an algorithm that uses VLL to estimate the Region of Interest (ROI) near the spine, where the abdominal aorta may be located. If the ROI is obscured by the image edge or blackout regions, due to radiation reduction technology in certain DXA scans, it indicates potential abdominal aorta cropping. The algorithm also provides the location and percentage of abdominal aorta cropping. The comparative analysis of our algorithm with expert-annotated images demonstrates the algorithm's accuracy in detecting cropping. It efficiently processes large volumes of images, saving both time and resources. 

We also demonstrated that accurate vertebral landmarks can improve IVG generation, enhancing consistency in AAC-24 scoring on DXA LSIs. A preliminary proof-of-concept study yielded encouraging results; however, further validation and clinical trials are necessary. \par

In summary, our contributions in this work are as follows:

\begin{itemize}
    \item A Dual Resolution Self Attention (DRSA) block designed to capture both High Frequency (HF) and Low Frequency (LF) components across different spatial contexts of the input feature map, adapted from our previous work~\cite{zilyas_miccai2024}, which focused on AAC prediction without landmark detection capability.
    \item A Dual Resolution Cross Attention (DRCA) block, using the same dual-resolution attention concept as DRSA, allows the model to integrate and align LF and HF information from different feature maps in various spatial contexts.
    \item An Multi-Context Feature Fusion Block (MCFB) for VerteNet's decoder, effectively utilizing DRSA, DRCA, and Channel-wise Self-Attention (CSA) to capture various interactions between elements of the combined feature maps.
    \item A hybrid CNN-Transformer architecture `VerteNet', using a pre-trained CNN backbone as the encoder and an MCFB-equipped decoder, achieving state-of-the-art results in Vertebral Landmarks Localization (VLL) for DXA LSI scans.
    \item An abdominal aorta crop detection algorithm leveraging VerteNet’s precise VLL to enable automated detection of abdominal aorta cropping in large datasets. 
    \item A proof of concept showing that IVGs generated by VerteNet could enhance inter-reader agreement among novice readers using the AAC-24 scoring method for detailed AAC evaluation.
\end{itemize}

\section{Related Work}
\subsection{Vertebral Landmarks' Localization}
Over the past few decades, numerous attempts have been made to accurately locate vertebral landmarks on spine images. However, there has been limited research specifically focused on DXA imaging, especially for LSIs. Early efforts were primarily based on hand-crafted features. For example, Elmasri et al. \cite{k_elmasri} used 56 landmarks to train an active appearance model for localizing the lumbar region and the abdominal aorta in DXA LSIs. Chaplin et al. \cite{t_cootes} later trained a statistical model on Sagittal CT images using 30 landmarks to locate the lumbar vertebrae, which was then applied to DXA LSIs. Unfortunately, these methods showed limited success. Recently, deep learning-based approaches have gained popularity due to their superior performance over traditional methods. Sun et al. \cite{sun} introduced a multitask framework for VLL in Anterior-Posterior (AP) view X-ray images, while Wu et al. \cite{wu} proposed BoostNet, which achieved robust VLL by eliminating harmful outliers in the feature space. Yang et al. \cite{yang} developed a VLL framework based on heatmaps and incorporated a Markov Random Field model for refining landmarks. Payer et al. \cite{payer} proposed a CNN model that learns the spatial configuration of landmarks, improving robustness by eliminating the need for heatmap-based localization. Yi et al. \cite{yi} further advanced this by generating three outputs for landmark localization: center offset, corner offset, and heatmap-based center localization. Guo et al. \cite{guo} introduced a vertebra segmentation task using a key-point-based transformer architecture to capture the relationships between the global spine structure and local vertebrae. Most recently, Huang et al. \cite{zixun} presented the NFDP model, a generative distribution prior-based model that significantly improved results in AP view X-ray images. However, most of this work has focused on X-Ray images. Apart from Elmasri et al. \cite{k_elmasri} and Chaplin et al. \cite{t_cootes}, the only deep learning-based model targeting DXA LSIs for landmark localization is by Ilyas et al. \cite{zaid}. Their proposed framework localized T12 to L5 vertebrae to generate IVGs that are essential for AAC 24 scoring, but was limited by a small dataset of 197 DXA images from a single Hologic machine. It also struggled to capture both local and global contexts. \par

\begin{figure}[t]
\centerline{\includegraphics[width=\columnwidth]{fig2.png}}
\caption{(a) A DXA LSI example with red arrows marking the location of AAC. (b) An illustration of Kauppila's AAC-24 scoring method~\cite{kauppila}. (c) A DXA image example showing unclear vertebral boundaries, with (d) indicating the intended placement of IVGs needed for AAC-24 scoring.}
\label{fig2}
\end{figure}

\subsection{Transformer Models for Vertebral Landmarks' Localization}
Transformers were initially developed for Natural Language Processing (NLP) tasks, with self-attention as the core mechanism that allows the model to capture long-range dependencies within input sequences~\cite{attention}. In computer vision, the Vision Transformer (ViT) was the first transformer model introduced and demonstrated outstanding performance by dividing the input image into patches and applying self-attention to them~\cite{vit}. Transformers have also excelled in medical imaging tasks like classification and segmentation. However, there has been limited research on using transformer models for VLL from X-ray images. Guo \cite{guo} introduced a keypoint transformer model to measure the Cobb angle in AP view X-ray spine images. Zhao et al. \cite{moxin} developed SpineHRformer, a hybrid CNN-transformer model for landmark localization in AP view X-ray images. To the best of our knowledge, no transformer-based model has been proposed for VLL in lateral view DXA images. \par





\section{Methodology}
\subsection{Overall Framework}
The proposed framework, VerteNet, shown in Fig.~\ref{fig3}(a), includes an encoder and a decoder, connected by skip connections, and three output blocks: heatmap, center offset, and corner offset. It also features an IVG generation module that uses information from these blocks to generate IVGs on the image. The encoder consists of a pre-trained CNN backbone like EfficientNetV2S, which performs convolutional operations followed by activation functions to learn complex patterns. Downsampling is done using pooling layers to reduce spatial dimensions and increase the depth (number of channels) of the feature map, resulting in a compact representation of the input image while preserving essential features. Feature maps from various stages of the network are passed to the decoder through skip connections. The decoder then upsamples the feature maps, combining data from the skip connections.

The novelty in VerteNet lies in its feature fusion method in the decoder block, called the Multi Context Feature Fusion Block (MCFB). Unlike the conventional simple fusion approach shown in Fig.~\ref{fig3}(e), where feature maps are simply concatenated along the channel dimension and adjusted through convolution, MCFB addresses the limitations of simple fusion. Simple fusion lacks critical information about global interactions among feature map elements in both spatial and channel domains, doesn’t consider the importance of elements in one feature map relative to the other, and overlooks the different frequency information inherent in an image.

MCFB overcomes these limitations by using DRSA and DRCA at different feature map resolutions, along with the concept of CSA~\cite{restormer, csa2}, as shown in Fig.~\ref{fig3}(d). These modules, which will be explained in the following sections, have proven effective and significantly improved VLL results in DXA LSIs.

VerteNet also utilizes the same approach as GuideNet~\cite{zaid} for heatmap, center offset, and corner offset calculations. The heatmap module provides the approximate location of each vertebra's center, the center offset module refines this location, and the corner offset module calculates the coordinates of the vertebrae corners. For more details on these modules, please refer to GuideNet~\cite{zaid}. The following subsections will first explain DRSA, then the working principle of DRCA, and finally the details of MCFB.       

\begin{figure*}[!t]
\centerline{\includegraphics[width=0.70\textwidth]{fig3.png}}
\caption{(a) Proposed Framework VerteNet (b) Dual Resolution Self Attention (DRSA) (c) Dual Resolution Cross Attention (DRCA) - takes two feature maps as input and generates Query from one feature map and Keys and Values from the other feature map. (d) Multi-Context Feature Fusion Block (MCFB) that employs DRSA and DRCA to calculate self-attention within, and cross attention among features from skip connection of layer \textit{n} and upscaled feature map from layer \textit{n}+1. Next, it utilizes a channel-wise self-attention transformer block to calculate inter-correlation among channels. (e) Conventional feature fusion approach that simply concatenates features along channels, and then performs convolution operations with activation functions (lack both inter and intra-correlation information among elements of the feature maps).}
\label{fig3}
\end{figure*}

\subsection{Dual Resolution Self Attention (DRSA)}
Self-attention has proven effective in capturing long-range dependencies and inter-correlations among elements within a single feature map~\cite{attention, vit, hilosa}. However, it still misses important information due to some limitations of attention calculation. Usually, SA is calculated using the concept of patches to control the quadratic complexity of attention calculation. This limits the receptive field of SA limiting the context of these operations. In natural images, including medical images, a broad range of frequencies exists holding essential details about the objects in the image. HFs relate to fine-grained details like texture, while LFs are related to global structures, such as the size, shape, and curvature of the object. Capturing all these details is important, but simple patch-wise attention mechanisms with limited context lose global context ~\cite{swin, dattention}. On the other hand, increasing the patch size to extend the context of SA increases the computational cost and parameter count quadratically. DRSA is designed to address these challenges. The first objective is to capture the diverse frequency information inherent in DXA images, and the second objective is to expand the spatial context of the windowed SA. DRSA splits the conventional Multi-Head Self-Attention (MHSA) into two sections. One section uses local windowed-SA on the input feature map, on its actual size, to encode the HF interactions, and the other section first downsamples the input feature map using average pooling operation which is a low-pass filter operation in nature, and then applies windowed SA on it. This section captures the information about the LF interaction. We use the same-sized windows in both DRSA sections which increases the spatial context of windowed SA in LF section and also introduces an overlap when the encoded feature maps are combined eventually. The DRSA block has been illustrated in Fig.~\ref{fig3}(b). Given an input feature map \textit{X} $\in$ $\mathbb{R}^{H \times W \times C}$, where \textit{H}, \textit{W}, and \textit{C} are the height, width, and channels of the input feature map, respectively, the DRSA down-samples it using avg. pooling to generate a low-resolution feature map \textit{X'} $\in$ $\mathbb{R}^{H/r \times W/r \times C}$, where \textit{r} is the reduction factor. Separate Query (Q), Key (K), and Value (V) embeddings are generated for \textit{X} and \textit{X'} using linear layers. Considering a single head, the SA for \textit{X} and \textit{X'}, abbreviated as $\textit{SA}_{H}$ and $\textit{SA}_{L}$ respectively are calculated as follows:
\begin{equation}
\textit{SA}_{H} = \text{softmax}\left(\frac{Q_HK_H^T}{\beta_{1}}\right) V_H
\end{equation}
\begin{equation}
\textit{SA}_{L} = \text{softmax}\left(\frac{Q_LK_L^T}{\beta_{2}}\right) V_L
\end{equation}

where $Q_H$, $K_H$, $V_H$, $Q_L$, $K_L$, and $V_L$ are the Query, Key, and Value embeddings for \textit{X} and \textit{X'} feature maps, with respective self-attentions $\textit{SA}_{H}$ and $\textit{SA}_{L}$. $\beta_{1}$ and $\beta_{2}$ are learnable parameters that control the SA. 

\begin{figure*}[!t]
\centerline{\includegraphics[width=0.7\textwidth]{fig4.png}}
\caption{(a) HiLo Attention~\cite{hilosa} - Uses the same window count \textit{s} in both the low and actual resolution SA paths which limits the context to individual patches. (b) DRSA (Our Proposed) - Uses the same window size in both resolution SA paths which increases the context in low-resolution SA path, and also introduces overlapping. In (a) and (b), \textit{r} is the size reduction factor, and \textit{s} is the window count.}
\label{fig4}
\end{figure*}

%\begin{equation}
%\text{SA}_{h}(Q_h, K_h, V_h) = \text{softmax}\left(\frac{Q_hK_h^T}{\alpha}\right) V_h 
%\end{equation}

%\begin{equation}
%\text{SA}_{l}(Q_l, K_l, V_l) = \text{softmax}\left(\frac{Q_lK_l^T}{\alpha}\right) V_l
%\end{equation}

\begin{equation}
\textit{SA}_{Total} = \text{Concat}(\textit{SA}_{H}, \text{Upsample}(\textit{SA}_{L}))W_p
\end{equation}

where \textit{SA$_{Total}$} is the overall self-attention, calculated by first up-sampling the $\textit{SA}_{L}$ using a transposed convolution layer (learnable), then concatenating it with $\textit{SA}_{H}$, and finally passing it through a linear layer $W_p$.

Our DRSA concept is inspired by HiLo SA~\cite{hilosa} but it uses a different approach to cover the limitations of HiLo SA which was mainly designed for fast transformer models. The architectural design of DRSA covers the limitation of HiLo SA i.e. Firstly, HiLo SA uses the same window size in both Hi and Lo attention sections which restricts spatial context. Secondly, the HiLo SA avoids overlap of patches while ``paying attention''. The working principle comparison of our proposed DRSA and the HiLo SA is shown in Fig.~\ref{fig4}.

\subsection{Dual Resolution Cross Attention (DRCA)}
Self-attention captures long-range dependencies and inter-correlations among elements within a single feature map. However, when combining two feature maps, the concept of SA can be extended in a cross direction i.e. \textit{Q} embeddings are generated from one feature map, and \textit{K} and \textit{V} embeddings are generated from the other feature map. This concept of Cross-Attention (CA) plays a critical role in managing inter-correlations and inter-dependencies between elements from different feature maps~\cite{attention, bert}. Instead of using a naive CA approach, we propose DRCA which utilizes the similar concept of dual resolution attention in the CA domain. The DRCA block has been illustrated in Fig.~\ref{fig3}(c). Given two input feature maps \textit{X} and \textit{Y}, both of the shape $\mathbb{R}^{H \times W \times C}$, DRCA splits the conventional MHSA into two sections. In one section, the DRCA down-samples the feature maps \textit{X} and feature maps \textit{Y} using avg. pooling to generate a low-resolution feature map \textit{X'} and \textit{Y'} $\in$ $\mathbb{R}^{H/r \times W/r \times C}$, where \textit{r} is the reduction factor. Separate Query embeddings, $\textit{Q}^{X}_{H}$ and $\textit{Q}^{X'}_{L}$, are generated for \textit{X} and \textit{X'}. Now different from DRSA, Key and Value embeddings are generated from feature map \textit{Y} i.e. $\textit{K}^{Y}_{H}$, $\textit{K}^{Y'}_{L}$, $\textit{V}^{Y}_{H}$, $\textit{V}^{Y'}_{L}$. Considering a single head, the CA abbreviated as $\textit{CA}_{H}$ and $\textit{CA}_{L}$ respectively are calculated as follows:

\begin{equation}
\textit{CA}_{H} = \text{softmax}\left(\frac{Q^{X}_{H}K^{Y^{T}}_{H}}{\beta_{1}}\right)V^{Y}_{H}
\end{equation}
\begin{equation}
\textit{CA}_{L} = \text{softmax}\left(\frac{Q^{X'}_{L}K^{Y'^{T}}_{L}}{\beta_{2}}\right)V^{Y'}_{L}
\end{equation}

where $\textit{Q}^{X}_{H}$, and $\textit{Q}^{X'}_{L}$ are the Query embeddings generated from feature map \textit{X} and its scaled down version \textit{X'} respectively. $\textit{K}^{Y}_{H}$, $\textit{K}^{Y'}_{L}$, $\textit{V}^{Y}_{H}$, $\textit{V}^{Y'}_{L}$ are the Key and Value embeddings generated from the feature map \textit{Y} and its scaled down version \textit{Y'}. $\beta_{1}$ and $\beta_{2}$ are learnable parameters that control the CA. After calculating $\textit{CA}_{H}$ and $\textit{CA}_{L}$, the overall cross attention \textit{$CA_{Total}$} is calculated by first up-sampling the $\textit{CA}_{L}$ using a transposed convolution layer (learnable), then concatenating it with $\textit{CA}_{H}$, and finally passing it through a linear layer $W_p$.   
\begin{equation}
\textit{CA}_{Total} = \text{Concat}(\textit{CA}_{H}, \text{Upsample}(\textit{CA}_{L}))W_p
\end{equation}

\begin{figure*}[!t]
\centerline{\includegraphics[width=1\textwidth]{fig5.png}}
\caption{(a) and (b) describe the types of input images that our algorithm can process, originating from two different machines: one with black regions (GE machine) and one without (Hologic machines). (c) illustrates the block-level structure of the abdominal aorta crop detection algorithm. The image classifier categorizes the input images into two groups: those with black regions and those without (d). (e) shows the corresponding outputs for image types (a) and (b), indicating both the location and the percentage of the abdominal aorta crop detected.}
\label{fig_algo}
\end{figure*}

\subsection{Multi-Context Feature Fusion Block (MCFB)}
For effective utilization of our proposed modules DRSA and DRCA when combining the feature maps from skip connections and the lower layers of the decoder module, an efficient approach is required that incorporates multi-context information along with rich inter and intra-correlation information. So, we propose a third module MCFB, which effectively utilizes DRSA and DRCA in addition to CSA for this purpose. It is shown in Fig.~\ref{fig3}(a) and explained in Fig.~\ref{fig3}(d). It takes feature maps from the lower layer \textit{n}+1 of the decoder part and the skip connection from the encoder layer \textit{n}. First, it upsamples the feature map from \textit{n}+1 using interpolation, and performs a convolutional operation followed by an activation function. MCFB passes both the feature maps from independent DRSA-based transformer blocks. Next, MCFB passes the feature maps through DRCA-based independent transformer blocks. After adding both self and cross-dual resolution attention, MCFB concatenates the feature maps along the channel direction. To find inter-correlation among the channels of the concatenated feature maps, MCFB passes the resultant feature map through a transformer block with transposed self-attention/ CSA, adding channel-wise self-attention. Given two feature maps $\text{X}_{S}$ and $\text{X}_{D}$ $\in$ $\mathbb{R}^{H \times W \times C}$, where $\text{X}_{S}$ is the feature map from the skip connection and $\text{X}_{D}$ is the feature map from the decoder layer after upsampling, convolution, and activation function, the operation of MCFB is as follows:
\begin{comment}
\begin{equation}
F_{S} = \text{DRSA}(\textit{X}_{S})W_s + \textit{X}_{S}
\end{equation}
\begin{equation}
F_{D} = \text{DRSA}(\textit{X}_{D})W_d + \textit{X}_{D}
\end{equation}
\begin{equation}
\textit{F}_{S'} = \text{GDFN}(\textit{F}_{S})W_p + \textit{F}_{S}
\end{equation}
\begin{equation}
\textit{F}_{D'} = \text{GDFN}(\textit{F}_{D})W_q + \textit{F}_{D}
\end{equation}
\begin{equation}
F_{SD} = \text{DRCA}(\textit{F}_{S'})W_sd + \textit{F}_{S'}
\end{equation}
\begin{equation}
F_{DS} = \text{DRCA}(\textit{F}_{D'})W_ds + \textit{X}_{D'}
\end{equation}
\begin{equation}
\textit{F}_{SD'} = \text{GDFN}(\textit{F}_{SD})W_p' + \textit{F}_{SD}
\end{equation}
\begin{equation}
\textit{F}_{DS'} = \text{GDFN}(\textit{F}_{DS})W_q' + \textit{F}_{DS}
\end{equation}
\begin{equation}
\textit{F}_{C} = \text{Concat}(\textit{F}_{SD'}, \textit{F}_{DS'})W_r
\end{equation}
\begin{equation}
    F_{A} = \text{TSA}(\textit{F}_{C})W_s + \textit{F}_{C}
\end{equation}
\begin{equation}
\textit{F}_{O} = \text{GDFN}(\textit{F}_{A})W_p + \textit{F}_{A}
\end{equation}
\begin{equation}
    F_{O'} = \text{ReLU}(\text{BN}(\textit{F}_{O}W_s))
\end{equation}
\end{comment}


\begin{equation}
\begin{split}
\textit{F}_{S} = \textit{GDFN}(\textit{DRSA}(\textit{X}_{S})W_s + \textit{X}_{S})W_p + (\textit{DRSA}(\textit{X}_{S})W_s + \\
\textit{X}_{S})    
\end{split}
\end{equation}
\begin{equation}
\begin{split}
\textit{F}_{D} = \textit{GDFN}(\textit{DRSA}(\textit{X}_{D})W_d + \textit{X}_{D})W_q + (\textit{DRSA}(\textit{X}_{D})W_d + \\
\textit{X}_{D})    
\end{split}
\end{equation}
\begin{equation}
\begin{split}
\textit{F}_{SD} = \textit{GDFN}(\textit{DRCA}(\textit{F}_{S},\textit{F}_{D})W_a + \textit{F}_{S})W_p' + \\
(\textit{DRCA}(\textit{F}_{S},\textit{F}_{D})W_a + \textit{F}_{S})    
\end{split}
\end{equation}
\begin{equation}
\begin{split}
\textit{F}_{DS} = \textit{GDFN}(\textit{DRCA}(\textit{F}_{D},\textit{F}_{S})W_b + \textit{X}_{D})W_q' + \\
(\textit{DRCA}(\textit{F}_{D},\textit{F}_{S})W_b + \textit{X}_{D})
\end{split}
\end{equation}
\begin{equation}
\textit{F}_{C} = \textit{Concat}(\textit{F}_{SD}, \textit{F}_{DS})W_r
\end{equation}

where $\textit{F}_{S}$ and $\textit{F}_{D}$ are the feature maps $\text{X}_{S}$ and $\text{X}_{D}$ after passing through DRSA based transformer blocks. $\textit{F}_{SD}$ is the feature map $\textit{F}_{S}$ after incorporating dual resolution cross attention with respect to $\textit{F}_{D}$ through transformer block with DRCA. Similarly, $\textit{F}_{DS}$ is the feature map $\textit{F}_{D}$ after incorporating dual resolution cross attention with respect to $\textit{F}_{S}$, through a separate transformer block with DRCA. $\textit{F}_{C}$ is the resultant feature map after concatenation of $\textit{F}_{DS}$ and $\textit{F}_{SD}$. The resultant feature map $\textit{F}_{C}$ has multi-context spatial information added to it using DRSA and DRCA operations, however, for calculating inter-correlation of elements among channels direction, a modified version of SA i.e. CSA is used. Given the generated \textit{Q}, \textit{K}, \textit{V} $\in$ $\mathbb{R}^{2C \times HW}$ embeddings generated from the feature map $\textit{F}_{C}$ $\in$ $\mathbb{R}^{H \times W \times 2C}$, the CSA is calculated using the following equation:

\begin{equation}
   \textit{CSA} = V^{T} \cdot \text{softmax}\left(\frac{KQ^{T}}{\beta}\right)
\end{equation}

The overall equation for the transformer block employing CSA is as follows:

\begin{equation}
\textit{F}_{O} = \textit{GDFN}(\textit{CSA}(\textit{F}_{C})W_t + \textit{F}_{C})W_o + (\textit{CSA}(\textit{F}_{C})W_t + \textit{F}_{C})
\end{equation}
\begin{equation}
    F_{O'} = \textit{ReLU}(\textit{BN}(\textit{F}_{O}W_c))
\end{equation}

Lastly, the MCFB block passes the resultant $\textit{F}_{O}$ through the convolutional layer $W_c$, batch normalization layer, and ReLU activation layer to generate final output $\textit{F}_{O'}$. In all the above equations $W_s$, $W_p$, $W_d$, $W_q$, $W_a$, $W_p'$, $W_b$, $W_q'$, $W_r$, $W_o$, and $W_t$ are the linear layers with learnable parameters. $\textit{GDFN}$, used in DRSA, DRCA and CSA, is Gated-Dconv Feed-Forward Network~\cite{restormer} that regulates the flow of information to concentrate on details that complement the others.

\subsection{Abdominal Aorta Crop Detection Algorithm}
Based on the outputs of `VerteNet', we propose an algorithm that accurately determines whether the input image contains a cropped abdominal aorta. The algorithm, depicted in Fig.~\ref{fig_algo}(c), facilitates the efficient analysis of large cohorts of VFA images acquired from various DXA machines, thereby significantly reducing the workload of image readers. It is compatible with images from both GE machines, shown in Fig.~\ref{fig_algo}(a), which exhibit black regions due to radiation reduction technology, and conventional Hologic machines, illustrated in Fig.~\ref{fig_algo}(b). In addition to detecting abdominal aorta cropping, the algorithm quantifies the percentage and identifies the specific location of cropping. The detailed steps of the algorithm for detecting potential abdominal aorta cropping are provided below:

\begin{itemize}
    \item \textbf{Image Categorization and Pre-processing:} The input image is processed by a CNN-based classifier to determine if it contains black regions. If black regions are detected, a pre-processing step applies dilation and erosion to smooth the black regions.
    \item \textbf{Determine Line Equations:} The equations for lines connecting anterior and posterior landmarks of inter- (red) and intra- (yellow) vertebral guides are computed (see images Fig.~\ref{fig_algo}(d) and Fig.~\ref{fig_algo}(e)).
    
    \item \textbf{Find Points at Distance $d$:} Using these line equations, the coordinates and pixel values of points located at a predefined distance \(d\) from all anterior landmarks (red dots in images Fig.~\ref{fig_algo}(d) and Fig.~\ref{fig_algo}(e)) are determined. The distance \(d\) is defined as a function of the mean vertebral width, i.e., \(d = \textit{Factor} \times (1+\textit{Mean Vertebral Width})\), with \textit{Factor} being a predefined constant.
    
    \item \textbf{Fit a Cubic Spline:} A cubic spline is fitted to the points at a distance $d$ from the anterior landmarks to derive the spline equation. 

    \item \textbf{Calculate Pixel Values:} Using the cubic spline equation, the pixel values for $x$ number of evenly spaced points (e.g., 500 points) are calculated, represented as a blue line in images Fig.~\ref{fig_algo}(d) and Fig.~\ref{fig_algo}(e).
    
    \item \textbf{Check for Cropping in Image Fig.~\ref{fig_algo}(e):} For cases like image (e), if the coordinates of any of the $x$ points exceed the image width, this indicates potential abdominal aorta cropping, as shown in the image Fig.~\ref{fig_algo}(e).
    
    \item \textbf{Check for Cropping in Image Fig.~\ref{fig_algo}(d):} For cases like image Fig.~\ref{fig_algo}(d), if the image contains black regions, check two conditions:  
    \begin{itemize}
        \item Whether the pixel values of any of the $x$ points fall within these black regions.  
        \item Whether the coordinate values of any of the $x$ points exceed the image width.
    \end{itemize}
    In either case, this suggests a possibility of abdominal aorta cropping. Finally, the percentage and location of the cropping is calculated.
    
\end{itemize}




\section{Experimental Setup}
De-identified images were sourced from vertebral fracture assessment studies, with ethics approvals obtained from relevant boards, including HealthPartners, Edith Cowan University, the University of Manitoba, and the Manitoba Health Information Privacy Committee (details on the first page). We selected and annotated 620 scans from three different machines, i.e. 200 scans from the iDXA GE machine (100 SE and 100 DE), 100 SE scans from Hologic 4500A, and 320 SE scans from the Hologic Horizon machine. The SE and DE datasets of the GE iDXA machine are image pairs from the same patients, however, the generated images originate from distinctly different methodologies used during the scan, resulting in varied distributions of pixel values. The dimensions of the images were set to 1024$\times$512. We performed 10-fold cross-validation keeping the same percentage of scans from all the three machines. The proposed model is implemented in the Pytorch framework and trained on an NVIDIA A6000 GPU. We used a batch size of 12, a learning rate of 0.0001, and trained the model for 100 epochs. Data Augmentation techniques i.e. random cropping, random expanding, brightness, and contrast distortion were used to reduce overfitting and further enhance the generalization of the model. We used Focal Loss\cite{focalloss} for Heatmap output and L1 loss for Center and Corner Offset outputs. For evaluation purposes of landmark localization, we used normalized mean and normalized median errors as evaluation metrics, and for aorta crop detection performance, we used classification among `cropped' and `not cropped' categories as the performance metric. 
\section{Experimental Results and Discussion}
\subsection{Landmark Localization Results}
The Vertebral Level Localization (VLL) in DXA Lumbar Spine Images (LSIs) remains an underexplored area. To the best of our knowledge, the only prior work in this domain is GuideNet~\cite{zaid}, which was initially trained on a limited dataset comprising 197 DXA scans acquired from the Hologic machine. For a fair evaluation, we retrained GuideNet on the same dataset used to train VerteNet and conducted a comparative performance analysis. Additionally, we evaluated the performance of our model against HRNet~\cite{hrnet} and NFDP~\cite{zixun}. HRNet, a well-established framework for landmark localization, was originally designed for human pose estimation but has also been widely applied in medical image landmark localization tasks. NFDP, a recent state-of-the-art (SOTA) model, was primarily trained on diverse X-ray images, including AP view X-ray spine images, X-ray cephalograms, and X-ray hand images. NFDP's SOTA performance in these tasks can be attributed to its innovative use of generative distribution priors. To ensure a fair comparison, we trained both HRNet and NFDP on the same dataset used for VerteNet and subsequently compared their performances. As shown in Table~\ref{table 1}, our proposed model outperformed these models.

\begin{table}[]
\caption{Landmark Localization Results}
\label{table 1}
\begin{center}
\begin{tabular}{lcc}
\hline
\textbf{Method} & \textbf{\begin{tabular}[c]{@{}c@{}}Normalized \\ Mean Error\\ (Pixels)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Normalized \\ Median Error\\ (Pixels)\end{tabular}} \\ \hline
GuideNet~\cite{zaid}        & 7.78                                                                                 & 2.96                                                                                   \\
HRNet~\cite{hrnet}           & 5.67                                                                                 & 2.49                                                                                   \\
NFDP~\cite{zixun}             & 5.02                                                                                 & 2.44                                                                                   \\ \hline
VerteNet (Ours) & 4.92                                                                                 & 2.35                                                                                   \\ \hline
\end{tabular}
\end{center}
\end{table}

\subsection{Abdominal Aorta Crop Detection Results}
To comprehensively assess the performance of our algorithm, we conducted two experiments. In the first experiment, we selected 70 labeled images from the Hologic Horizon machine: 35 labeled as having insufficient soft tissue on the LSI (with a cropped aorta) and 35 labeled as having adequate soft tissue for evaluating AAC-24 on the LSI. The labeling was performed by an expert clinician (J.T.S.) with over 15 years of experience in analyzing DXA LSIs. We tested our method with \textit{Factor} values ranging from 0.8 to 1.5 (as shown in Table~\ref{table 3}) and found that a factor value of 1.2 provided results that perfectly matched the expert’s labeling with 100$\%$ accuracy. In the second experiment, we applied our algorithm to 200 images and, without disclosing the algorithm’s results, asked J.T.S. to independently classify the images as either having a cropped abdominal aorta or not. A comparison of the results revealed strong alignment, confirming the accuracy of the algorithm's predictions at a factor value close to 1.0. The results are presented in Table~\ref{table 3_1}. Fig.~\ref{fig_results} showcase examples of successful cases where our algorithm performed accurately. However, the algorithm's accuracy is dependent on the accurate landmarks' placement, and in cases where landmarks are not accurately positioned, the algorithm fails, as demonstrated in Fig.~\ref{fig_failure}.


\begin{table}[]
\caption{Experiment 1: Comparison of the proposed algorithm's performance with human-labeled abdominal aorta crop detection on 70 Hologic DXA images. True positives (TP) correctly identify cropped aortas, true negatives (TN) identify uncropped aortas, false positives (FP) misclassify uncropped as cropped, and false negatives (FN) misclassify cropped as uncropped.}
\label{table 3}
\begin{center}
\begin{tabular}{cccccc}
\hline
\textbf{Factor} & \textbf{FP} & \textbf{FN} & \textbf{TP} & \textbf{TN} & \textbf{Accuracy ($\%$)} \\ \hline
0.8             & 8           & 0           & 27          & 35          & 88.57             \\
0.9             & 3           & 0           & 32          & 35          & 95.71             \\
1.0             & 2           & 0           & 33          & 35          & 97.14             \\
1.1             & 1           & 0           & 34          & 35          & 98.57             \\ \hline
1.2             & 0           & 0           & 35          & 35          & 100               \\ \hline
1.3             & 0           & 1           & 35          & 34          & 98.57             \\
1.4             & 0           & 3           & 35          & 32          & 95.74             \\
1.5             & 0           & 8           & 35          & 27          & 88.57             \\ \hline
\end{tabular}
\end{center}
\end{table}




\begin{table}[]
\caption{Experiment 2: Comparison of the performance of the proposed algorithm with human-labeled abdominal aorta crop detection on a dataset of 200 Hologic DXA images.}
\label{table 3_1}
\begin{center}
\begin{tabular}{ccccc}
\hline
\textbf{Factor} & \textbf{Accuracy (\%)} & \textbf{Sensitivity} & \textbf{Specificity} & \textbf{F1-Score} \\ \hline
0.9             & 86.0                                                            & 0.68                 & 0.98                 & 0.25              \\ \hline
\textbf{1.0}    & \textbf{96.0}                                                   & \textbf{0.93}        & \textbf{0.98}        & \textbf{0.95}     \\ \hline
1.1             & 92.0                                                            & 1.0                  & 0.87                 & 0.92              \\ \hline
\end{tabular}
\end{center}
\end{table}

\begin{figure*}[h]
\centerline{\includegraphics[width=0.65\textwidth]{fig6.png}}
\caption{(a) Possible abdominal crop detection (both based on black region and image width) in DE DXA image from GE machine. (b) No aorta crop detection in the range L1 to L4 in the SE DXA image from GE machine. (c)(d) No abdominal aorta crop in SE Hologic machine images.}
\label{fig_results}
\end{figure*}

\begin{figure*}[h]
\centerline{\includegraphics[width=0.65\textwidth]{fig7.png}}
\caption{(a) and (b) represent the failure cases of the proposed algorithm, where improper localization of landmarks in images from Hologic and GE machines leads to suboptimal performance, as the algorithm relies on accurate landmark detection.}
\label{fig_failure}
\end{figure*}


\subsection{Impact of IVGs on Inter-Reader Agreement in Granular AAC Scoring}
This proof-of-concept study investigated whether IVGs improve the correlation between two readers with limited training (2-day course) in evaluating granular AAC scores from DXA LSIs. Readers typically use manual or imaginary IVGs to divide the abdominal aorta into four regions corresponding to L1–L4 vertebrae. While experienced professionals can position IVGs and categorize regions accurately, less experienced readers may misclassify regions, e.g., assigning calcification to L3 instead of L4, or L2 instead of L1. Such errors are unlikely to affect the overall AAC score (0–24) however, it definitely would affect granular scores. In this study, two authors (A.M. and A.S.) with limited training in Kauppila's AAC-24 scoring evaluated 32 DXA images from a Hologic Horizon machine twice: first without IVGs and later with IVGs. Table~\ref{table 2_1} and Table~\ref{table 2} present experimental results, showing improved inter-reader agreement in correlation and Cohen's weighted kappa values.


\begin{table}[h]
\caption{Correlation Coefficient - Inter-Reader}
\label{table 2}
\begin{center}
\begin{tabular}{cclcl}
\hline
\multicolumn{5}{c}{\textbf{Correlation Coefficient - Inter-Reader}}                                                                                                    \\ \hline
\multicolumn{1}{l}{}                                                     & \multicolumn{2}{c}{\textbf{without IVGs}}   & \multicolumn{2}{c}{\textbf{with IVGs}}          \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}Vertebral Region \\ (AAC Score Range: 0-6)\end{tabular}} & \multicolumn{2}{c}{\textbf{Value (95$\%$ CI)}} & \multicolumn{2}{c}{\textbf{Value (95$\%$ CI)}}     \\ \hline
L1                                                                       & \multicolumn{2}{c}{0.82 (0.64 - 0.91)}      & \multicolumn{2}{c}{\textbf{0.94 (0.89 - 0.97)}} \\
L2                                                                       & \multicolumn{2}{c}{0.88 (0.75 - 0.94)}      & \multicolumn{2}{c}{\textbf{0.90 (0.79 - 0.95)}} \\
L3                                                                       & \multicolumn{2}{c}{0.83 (0.66 - 0.92)}      & \multicolumn{2}{c}{\textbf{0.84 (0.68 - 0.92)}} \\
L4                                                                       & \multicolumn{2}{c}{0.91 (0.81 - 0.96)}      & \multicolumn{2}{c}{\textbf{0.94 (0.89 - 0.97)}} \\ \hline
\end{tabular}
\end{center}
\end{table}




\begin{table}[h]
\caption{Cohen's weighted kappa - Inter-Reader}
\label{table 2_1}
\begin{center}
\begin{tabular}{cclcl}
\hline
\multicolumn{5}{c}{\textbf{Cohen's weighted kappa - Inter-Reader}}                                                                                                  \\ \hline
\multicolumn{1}{l}{}                                                     & \multicolumn{2}{c}{\textbf{without IVGs}}   & \multicolumn{2}{c}{\textbf{with IVGs}}          \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}Vertebral Region \\ (AAC Score Range: 0-6)\end{tabular}} & \multicolumn{2}{c}{\textbf{Value (95$\%$ CI)}} & \multicolumn{2}{c}{\textbf{Value (95$\%$ CI)}}     \\ \hline
L1                                                                       & \multicolumn{2}{c}{0.57 (0.24 - 0.91)}      & \multicolumn{2}{c}{\textbf{0.82 (0.54 - 1.09)}} \\
L2                                                                       & \multicolumn{2}{c}{0.54 (0.29 - 0.79)}      & \multicolumn{2}{c}{\textbf{0.58 (0.33 - 0.84)}} \\
L3                                                                       & \multicolumn{2}{c}{0.57 (0.37 - 0.76)}      & \multicolumn{2}{c}{\textbf{0.61 (0.41 - 0.80)}} \\
L4                                                                       & \multicolumn{2}{c}{0.66 (0.49 - 0.82)}      & \multicolumn{2}{c}{\textbf{0.78 (0.63 - 0.92)}} \\ \hline
\end{tabular}
\end{center}
\end{table}





\begin{comment}
    

\begin{table}[]
\caption{Inter Reader Pearson Correlation}
\label{table 2}
\begin{center}
\begin{tabular}{lcc}
\hline
\multicolumn{3}{c}{\textbf{Inter Observer Pearson Correlation}}                                  \\ \hline
\multicolumn{1}{c}{\textbf{Category (value range)}} & \textbf{without IVGs} & \textbf{with IVGs} \\ \hline
L1 Ant (0-3)                                        & 1                     & 1                  \\
L1 Post (0-3)                                       & 0.84                  & \textbf{0.90}      \\
L2 Ant (0-3)                                        & \textbf{0.85}         & \textbf{0.85}      \\
L2 Post (0-3)                                       & 0.69                  & \textbf{0.72}      \\
L3 Ant (0-3)                                        & \textbf{0.89}         & 0.87               \\
L3 Post (0-3)                                       & 0.62                  & \textbf{0.64}      \\
L4 Ant (0-3)                                        & 0.75                  & \textbf{0.78}      \\
L4 Post (0-3)                                       & 0.77                  & \textbf{0.80}      \\
Overall Score (0-24)                                & 0.90                  & \textbf{0.91}      \\ \hline
\end{tabular}
\end{center}
\end{table}
\end{comment}





\subsection{Ablation Studies}
We conducted ablation experiments to validate our architecture. First, we analyzed the impact of the reduction factor \textit{r} and patch size \textit{p} in DRSA and DRCA on performance. Table~\ref{table 4} shows various \textit{r} and \textit{p} combinations at hierarchical levels. Using a larger reduction factor (\textit{r} = 4) in shallower layers significantly reduced performance due to excessive down-sampling and loss of information. For patch size, the best results were obtained with \textit{p} = 10. Larger patch sizes increased parameters and risked overfitting due to insufficient training data. Next, we evaluated different CNN backbones. As shown in Table~\ref{table 5}, EfficientNetV2S outperformed other backbones. Finally, we tested different decoder configurations: a basic U-Net design, DRSA-only, DRCA-only, and our proposed MCFB (DRSA, DRCA, and CSA). Table~\ref{table 6} demonstrates that MCFB achieved the best performance.


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\caption{Effect of reduction factor (r) and window size (p) in DRSA and DRCA on Model's Performance}
\label{table 4}
\begin{center}
\begin{tabular}{lllcc}
\hline
\multirow{2}{*}{\textbf{Lvl2}} & \multirow{2}{*}{\textbf{Lvl3}} & \multirow{2}{*}{\textbf{Lvl4}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Normalized \\ Mean Error\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Normalized \\ Median Error\end{tabular}}} \\
                               &                                &                                &                                                                                            &                                                                                              \\ \hline
r=4                            & r=4                            & r=2                            & \multirow{2}{*}{5.64}                                                                      & \multirow{2}{*}{2.51}                                                                        \\
p=20                           & p=20                           & p=10                           &                                                                                            &                                                                                              \\
r=4                            & r=4                            & r=2                            & \multirow{2}{*}{5.3}                                                                       & \multirow{2}{*}{2.42}                                                                        \\
p=10                           & p=10                           & p=10                           &                                                                                            &                                                                                              \\
r=2                            & r=2                            & r=2                            & \multirow{2}{*}{5.08}                                                                      & \multirow{2}{*}{2.39}                                                                        \\
p=20                           & p=20                           & p=10                           &                                                                                            &                                                                                              \\
r=2                            & r=2                            & r=2                            & \multirow{2}{*}{4.92}                                                                      & \multirow{2}{*}{2.35}                                                                        \\
p=10                           & p=10                           & p=10                           &                                                                                            &                                                                                              \\ \hline
\end{tabular}
\end{center}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
\caption{Effect of Backbone on Model's Performance}
\label{table 5}
\begin{center}
\begin{tabular}{lcc}
\hline
\multirow{2}{*}{\textbf{Backbone}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Normalized \\ Mean Error\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Normalized \\ Median Error\end{tabular}}} \\
                                   &                                                                                            &                                                                                              \\ \hline
ResNet34                           & 6.12                                                                                       & 2.67                                                                                         \\
EfficientNetB3                     & 5.32                                                                                       & 2.44                                                                                         \\
EfficientNetV2S                    & 4.92                                                                                       & 2.35                                                                                         \\ \hline
\end{tabular}
\end{center}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
\caption{Effect of different configurations of decoder on Model's Performance}
\label{table 6}
\begin{center}
\begin{tabular}{lcc}
\hline
\multirow{2}{*}{\textbf{Decoder's Configuration}}                                   & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Normalized \\ Mean Error\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Normalized \\ Median Error\end{tabular}}} \\
                                                                                    &                                                                                            &                                                                                              \\ \hline
Without DRCA or DRSA                                                                & 6.12                                                                                       & 2.62                                                                                         \\
With DRSA blocks only                                                               & 5.78                                                                                       & 2.57                                                                                         \\
With DRCA blocks only                                                               & 5.67                                                                                       & 2.45                                                                                         \\
\begin{tabular}[c]{@{}c@{}}With both DRCA and DRSA\\ blocks (Proposed MCFB)\end{tabular} & 4.92                                                                                       & 2.35                                                                                         \\ \hline
\end{tabular}
\end{center}
\end{table}



\section{Conclusion and Future Work}
In this study, we introduced VerteNet, a deep learning architecture that incorporates a novel multi-context feature fusion block utilizing dual-resolution self- and cross-attention mechanisms. Trained on images from various DXA machines, VerteNet achieved state-of-the-art performance. VerteNet's VLL by showing its ability to estimate ROI in images to detect potential abdominal aorta cropping. This automated method ensures adequate soft tissue regions are captured to assess calcification across the entire abdominal aorta.  Furthermore, as a proof of concept study, we illustrated that vertebral landmarks can generate accurate and precise IVGs, which  make it easier and quicker for readers to score AAC-24 and may help standardize the assessment between readers and improve consistency among individuals assessing AAC-24 scores on DXA LSIs. For this, although the findings are promising, the small sample size limits the generalizability, highlighting the need for further studies with larger and more diverse datasets to confirm these results. Future research could include validating this proof of concept through additional clinical experiments, examining the influence of IVGs on inter-reader variability across varying levels of expertise, or extending the proposed framework to applications like kyphosis detection and severity quantification using identified landmarks.



\appendices
\begin{comment}
\section*{Appendix and the Use of Supplemental Files}

\footnote{Supplementary materials are available in the supporting documents/multimedia tab.
Further instructions on footnote usage are in the Footnotes section on the next page.}    
\end{comment}





\begin{comment}
\section*{References and Footnotes}
    \subsection{References}

\section{References}

\end{comment}

\begin{comment}
    \begin{thebibliography}{99}

\bibitem{ct} S. Bouomrani, I. Lamloum, A. Hammami, W. Mahdhaoui, A. Naffeti, R. Mesfar, and D. Amri, ``Extensive aortic mediacalcosis: an unusual presentation of primary hyperparathyroidism,'' presented at the Department of Internal Medicine, Military Hospital of Gabes, University of Sfax, Tunisia.

\bibitem{dxi} R. Setiawati, F. Di Chio, P. Rahardjo, M. Nasuto, F. J. Dimpudus, and G. Guglielmi, ``Quantitative assessment of abdominal aortic calcifications using lateral lumbar radiograph, dual-energy X-ray absorptiometry, and quantitative computed tomography of the spine,'' Journal of Clinical Densitometry, vol. 19, no. 2, pp. 242-249, 2016.

\bibitem{kyphosis} C. Liu, R. Ge, H. Li, Z. Zhu, W. Xia, and H. Liu, ``Thoracolumbar/Lumbar Degenerative Kyphosis—The Importance of Thoracolumbar Junction in Sagittal Alignment and Balance,'' Journal of Personalized Medicine, vol. 14, no. 1, p. 36, 2023.

\bibitem{lordosis} M. Tekeli, H. Erdem, N. Kilic, N. Boyan, O. Oguz, and R. W. Soames, ``Evaluation of lumbar lordosis in symptomatic individuals and comparative analysis of six different techniques: a retrospective radiologic study,'' European Spine Journal, vol. 32, no. 12, pp. 4118-4127, 2023.

\bibitem{vfa} M. Tekeli, H. Erdem, N. Kilic, N. Boyan, O. Oguz, and R. W. Soames, ``Evaluation of lumbar lordosis in symptomatic individuals and comparative analysis of six different techniques: a retrospective radiologic study,'' European Spine Journal, vol. 32, no. 12, pp. 4118-4127, 2023.

\bibitem{osteoporosis} G. M. Blake and I. Fogelman, ``The role of DXA bone density scans in the diagnosis and treatment of osteoporosis,'' Postgraduate Medical Journal, vol. 83, no. 982, pp. 509-517, 2007.

\bibitem{aac1} J. T. Schousboe, J. R. Lewis, and D. P. Kiel, ``Abdominal aortic calcification on dual-energy X-ray absorptiometry: methods of assessment and clinical significance,'' Bone, vol. 104, pp. 91-100, 2017.

\bibitem{aac2} J. T. Schousboe, K. E. Wilson, and D. P. Kiel, ``Detection of abdominal aortic calcification with lateral spine imaging using DXA,'' Journal of Clinical Densitometry, vol. 9, pp. 302–308, 2006.

\bibitem{kauppila} L. I. Kauppila, J. F. Polak, L. A. Cupples, M. T. Hannan, D. P. Kiel, and P. W. Wilson, ``New indices to classify location, severity and progression of calcific lesions in the abdominal aorta: a 25-year follow-up study,'' Atherosclerosis, vol. 132, no. 2, pp. 245-250, 1997.

\bibitem{k_elmasri} K. Elmasri, Y. Hicks, X. Yang, X. Sun, R. Pettit, and W. Evans, ``Automatic detection and quantification of abdominal aortic calcification in dual energy X-ray absorptiometry,'' Procedia Computer Science, vol. 96, pp. 1011-1021, 2016.

\bibitem{t_cootes} L. Chaplin and T. Cootes, ``Automated scoring of aortic calcification in vertebral fracture assessment images,'' Medical Imaging 2019: Computer-Aided Diagnosis, vol. 10950, International Society for Optics and Photonics, 2019.

\bibitem{sun} H. Sun, X. Zhen, C. Bailey, P. Rasoulinejad, Y. Yin, and S. Li, ``Direct estimation of spinal cobb angles by structured multi-output regression,'' in International Conference on Information Processing in Medical Imaging, Springer, 2017, pp. 529-540.

\bibitem{wu} H. Wu, C. Bailey, P. Rasoulinejad, and S. Li, ``Automatic landmark estimation for adolescent idiopathic scoliosis assessment using boostnet,'' in MICCAI, Springer, 2017, pp. 127-135.

\bibitem{yang} D. Yang, T. Xiong, D. Xu, Q. Huang, D. Liu, S. K. Zhou, Z. Xu, J. Park, M. Chen, T. D. Tran, et al., ``Automatic vertebra labeling in large-scale 3D CT using deep image-to-image network with message passing and sparsity regularization,'' in International Conference on Information Processing in Medical Imaging, Springer, 2017, pp. 633-644.

\bibitem{payer} C. Payer, D. Štern, H. Bischof, and M. Urschler, ``Integrating spatial configuration into heatmap regression based CNNs for landmark localization,'' Medical Image Analysis, vol. 54, pp. 207-219, 2019.

\bibitem{yi} J. Yi, P. Wu, Q. Huang, H. Qu, and D. N. Metaxas, ``Vertebra-focused landmark detection for scoliosis assessment,'' in ISBI, IEEE, 2020, pp. 736-740. 

\bibitem{guo} Y. Guo, Y. Li, X. Zhou, and W. He, ``A keypoint transformer to discover spine structure for Cobb angle estimation,'' in ICME, IEEE, 2021, pp. 1-6.

\bibitem{zixun} Z. Huang, R. Zhao, F. H. Leung, S. Banerjee, K. M. Lam, Y. P. Zheng, and S. H. Ling, ``Landmark Localization from Medical Images with Generative Distribution Prior,'' IEEE Transactions on Medical Imaging, 2024.

\bibitem{restormer} S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M. H. Yang, ``Restormer: Efficient transformer for high-resolution image restoration,'' in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5728-5739.

\bibitem{csa2} Z. Chen, Y. Zhang, J. Gu, L. Kong, X. Yang, and F. Yu, ``Dual aggregation transformer for image super-resolution,'' in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 12312-12321.

\bibitem{zaid} Z. Ilyas, N. Sharif, J. T. Schousboe, J. R. Lewis, D. Suter, and S. Z. Gilani, ``GuideNet: Learning Inter-Vertebral Guides in DXA Lateral Spine Images,'' in 2021 Digital Image Computing: Techniques and Applications (DICTA), IEEE, November 2021, pp. 01-07.

\bibitem{dxapilot} S. Reid, J. T. Schousboe, D. Kimelman, B. A. Monchka, M. J. Jozani, and W. D. Leslie, ``Machine learning for automated abdominal aortic calcification scoring of DXA vertebral fracture assessment images: A pilot study,'' Bone, vol. 148, 115943, 2021.

\bibitem{showattend} S. Z. Gilani, N. Sharif, D. Suter, J. T. Schousboe, S. Reid, W. D. Leslie, and J. R. Lewis, ``Show, Attend and Detect: Towards Fine-Grained Assessment of Abdominal Aortic Calcification on Vertebral Fracture Assessment Scans,'' in 2022 International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), pp. 439-450, 2022.

\bibitem{afsah} A. Saleem, Z. Ilyas, D. Suter, G. M. Hassan, S. Reid, J. T. Schousboe, R. Prince, W. D. Leslie, J. R. Lewis, and S. Z. Gilani, ``SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,'' in 2023 International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), pp. 273-283, 2023.

\bibitem{naeha} N. Sharif, S. Z. Gilani, D. Suter, S. Reid, P. Szulc, D. Kimelman, B. A. Monchka, M. J. Jozani, J. M. Hodgson, M. Sim, and K. Zhu, ``Machine learning for abdominal aortic calcification assessment from bone density machine-derived lateral spine images,'' EBioMedicine, vol. 94, 2023.

\bibitem{zilyas_miccai2024} Z. Ilyas, A. Saleem, D. Suter, J. T. Schousboe, W. D. Leslie, J. R. Lewis, and S. Z. Gilani, ``A Hybrid CNN-Transformer Feature Pyramid Network for Granular Abdominal Aortic Calcification Detection from DXA Images,'' in International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2024.

\bibitem{attention} A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, ``Attention is All You Need,'' in Advances in Neural Information Processing Systems, vol. 30, 2017.

\bibitem{bert} J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, ``BERT: Pre-training of deep bidirectional transformers for language understanding,'' arXiv preprint arXiv:1810.04805, 2018.

\bibitem{vit} A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, and J. Uszkoreit, ``An image is worth 16x16 words: Transformers for image recognition at scale,'' arXiv preprint arXiv:2010.11929, 2020.

\bibitem{moxin} M. Zhao, N. Meng, J. P. Y. Cheung, C. Yu, P. Lu, and T. Zhang, ``SpineHRformer: A Transformer-Based Deep Learning Model for Automatic Spine Deformity Assessment with Prospective Validation,'' Bioengineering, vol. 10, no. 11, p. 1333, 2023.

\bibitem{hilosa} Z. Pan, J. Cai, and B. Zhuang, ``Fast vision transformers with HiLo attention,'' in Advances in Neural Information Processing Systems, vol. 35, 2022, pp. 14541-14554.

\bibitem{swin} Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, ``Swin transformer: Hierarchical vision transformer using shifted windows,'' in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 10012-10022.

\bibitem{dattention} Z. Xia, X. Pan, S. Song, L. E. Li, and G. Huang, ``Vision transformer with deformable attention,'' in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 4794-4803.

\bibitem{hrnet} K. Sun, B. Xiao, D. Liu, and J. Wang, ``Deep high-resolution representation learning for human pose estimation,'' in *Proceedings of the CVPR*, IEEE, June 2019, pp. 5693–5703.

\bibitem{nfdp} Z. Huang, R. Zhao, F. H. Leung, S. Banerjee, K. M. Lam, Y. P. Zheng, and S. H. Ling, ``Landmark Localization from Medical Images with Generative Distribution Prior,'' in *IEEE Transactions on Medical Imaging*, 2024.

\bibitem{focalloss} T. Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, ``Focal loss for dense object detection,'' in Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.

\end{thebibliography}

\end{comment}


\begin{thebibliography}{99}

\bibitem{ct} J. Wasserthal, ``Dataset with segmentations of 117 important anatomical structures in 1228 CT images,'' Zenodo, Oct. 27, 2023, doi: 10.5281/zenodo.10047292.




\bibitem{dxi} M. Fraiwan, Z. Audat, and T. Manasreh, ``A dataset of scoliosis, spondylolisthesis, and normal vertebrae X-ray images,'' Mendeley Data, vol. 1, 2022, doi: 10.17632/xkt857dsxk.1.



\bibitem{kyphosis} C. Liu, R. Ge, H. Li, Z. Zhu, W. Xia, and H. Liu, ``Thoracolumbar/Lumbar Degenerative Kyphosis—The Importance of Thoracolumbar Junction in Sagittal Alignment and Balance,'' Journal of Personalized Medicine, vol. 14, no. 1, p. 36, 2023.

\bibitem{lordosis} M. Tekeli, H. Erdem, N. Kilic, N. Boyan, O. Oguz, and R. W. Soames, ``Evaluation of lumbar lordosis in symptomatic individuals and comparative analysis of six different techniques: a retrospective radiologic study,'' European Spine Journal, vol. 32, no. 12, pp. 4118-4127, 2023.

\bibitem{vfa} M. Tekeli, H. Erdem, N. Kilic, N. Boyan, O. Oguz, and R. W. Soames, ``Evaluation of lumbar lordosis in symptomatic individuals and comparative analysis of six different techniques: a retrospective radiologic study,'' European Spine Journal, vol. 32, no. 12, pp. 4118-4127, 2023.

\bibitem{osteoporosis} G. M. Blake and I. Fogelman, ``The role of DXA bone density scans in the diagnosis and treatment of osteoporosis,'' Postgraduate Medical Journal, vol. 83, no. 982, pp. 509-517, 2007.



\bibitem{kauppila} L. I. Kauppila, J. F. Polak, L. A. Cupples, M. T. Hannan, D. P. Kiel, and P. W. Wilson, ``New indices to classify location, severity and progression of calcific lesions in the abdominal aorta: a 25-year follow-up study,'' Atherosclerosis, vol. 132, no. 2, pp. 245-250, 1997.

\bibitem{k_elmasri} K. Elmasri, Y. Hicks, X. Yang, X. Sun, R. Pettit, and W. Evans, ``Automatic detection and quantification of abdominal aortic calcification in dual energy X-ray absorptiometry,'' Procedia Computer Science, vol. 96, pp. 1011-1021, 2016.

\bibitem{t_cootes} L. Chaplin and T. Cootes, ``Automated scoring of aortic calcification in vertebral fracture assessment images,'' Medical Imaging 2019: Computer-Aided Diagnosis, vol. 10950, SPIE, 2019.

\bibitem{sun} H. Sun, X. Zhen, C. Bailey, P. Rasoulinejad, Y. Yin, and S. Li, ``Direct estimation of spinal cobb angles by structured multi-output regression,'' in IPMI, Springer, 2017, pp. 529-540.

\bibitem{wu} H. Wu, C. Bailey, P. Rasoulinejad, and S. Li, ``Automatic landmark estimation for adolescent idiopathic scoliosis assessment using boostnet,'' in MICCAI, Springer, 2017, pp. 127-135.

\bibitem{yang} D. Yang, T. Xiong, D. Xu, Q. Huang, D. Liu, S. K. Zhou, Z. Xu, J. Park, M. Chen, T. D. Tran, et al., ``Automatic vertebra labeling in large-scale 3D CT using deep image-to-image network with message passing and sparsity regularization,'' in IPMI, Springer, 2017, pp. 633-644.

\bibitem{payer} C. Payer, D. Štern, H. Bischof, and M. Urschler, ``Integrating spatial configuration into heatmap regression based CNNs for landmark localization,'' Medical Image Analysis, vol. 54, pp. 207-219, 2019.

\bibitem{yi} J. Yi, P. Wu, Q. Huang, H. Qu, and D. N. Metaxas, ``Vertebra-focused landmark detection for scoliosis assessment,'' in ISBI, IEEE, 2020, pp. 736-740. 

\bibitem{guo} Y. Guo, Y. Li, X. Zhou, and W. He, ``A keypoint transformer to discover spine structure for Cobb angle estimation,'' in ICME, IEEE, 2021, pp. 1-6.

\bibitem{zixun} Z. Huang, R. Zhao, F. H. Leung, S. Banerjee, K. M. Lam, Y. P. Zheng, and S. H. Ling, ``Landmark Localization from Medical Images with Generative Distribution Prior,'' IEEE TMI, 2024.

\bibitem{restormer} S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M. H. Yang, ``Restormer: Efficient transformer for high-resolution image restoration,'' in Proceedings of the IEEE/CVF CVPR, 2022, pp. 5728-5739.

\bibitem{csa2} Z. Chen, Y. Zhang, J. Gu, L. Kong, X. Yang, and F. Yu, ``Dual aggregation transformer for image super-resolution,'' in Proceedings of the IEEE/CVF ICCV, 2023, pp. 12312-12321.

\bibitem{zaid} Z. Ilyas, N. Sharif, J. T. Schousboe, J. R. Lewis, D. Suter, and S. Z. Gilani, ``GuideNet: Learning Inter-Vertebral Guides in DXA Lateral Spine Images,'' in 2021 DICTA, IEEE, November 2021, pp. 01-07.

\bibitem{dxapilot} S. Reid, J. T. Schousboe, D. Kimelman, B. A. Monchka, M. J. Jozani, and W. D. Leslie, ``Machine learning for automated abdominal aortic calcification scoring of DXA vertebral fracture assessment images: A pilot study,'' Bone, vol. 148, 115943, 2021.

\bibitem{aac1} J. T. Schousboe, J. R. Lewis, and D. P. Kiel, ``Abdominal aortic calcification on dual-energy X-ray absorptiometry: methods of assessment and clinical significance,'' Bone, vol. 104, pp. 91-100, 2017.

\bibitem{aac2} J. T. Schousboe, K. E. Wilson, and D. P. Kiel, ``Detection of abdominal aortic calcification with lateral spine imaging using DXA,'' Journal of Clinical Densitometry, vol. 9, pp. 302–308, 2006.

\bibitem{showattend} S. Z. Gilani, N. Sharif, D. Suter, J. T. Schousboe, S. Reid, W. D. Leslie, and J. R. Lewis, ``Show, Attend and Detect: Towards Fine-Grained Assessment of Abdominal Aortic Calcification on Vertebral Fracture Assessment Scans,'' in MICCAI, Springer, 2022, pp. 439-450.

\bibitem{afsah} A. Saleem, Z. Ilyas, D. Suter, G. M. Hassan, S. Reid, J. T. Schousboe, R. Prince, W. D. Leslie, J. R. Lewis, and S. Z. Gilani, ``SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,'' in MICCAI, Springer 2023, pp. 273-283.

\bibitem{naeha} N. Sharif, S. Z. Gilani, D. Suter, S. Reid, P. Szulc, D. Kimelman, B. A. Monchka, M. J. Jozani, J. M. Hodgson, M. Sim, and K. Zhu, ``Machine learning for abdominal aortic calcification assessment from bone density machine-derived lateral spine images,'' EBioMedicine, vol. 94, 2023.

\bibitem{zilyas_miccai2024} Z. Ilyas, A. Saleem, D. Suter, J. T. Schousboe, W. D. Leslie, J. R. Lewis, and S. Z. Gilani. ``A Hybrid CNN-Transformer Feature Pyramid Network for Granular Abdominal Aortic Calcification Detection from DXA Images.'' In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 14-25. Cham: Springer Nature Switzerland, 2024.

\bibitem{attention} A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, ``Attention is All You Need,'' in Advances in NeurIPS, vol. 30, 2017.

\bibitem{bert} J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, ``BERT: Pre-training of deep bidirectional transformers for language understanding,'' arXiv preprint arXiv:1810.04805, 2018.

\bibitem{vit} A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, and J. Uszkoreit, ``An image is worth 16x16 words: Transformers for image recognition at scale,'' arXiv preprint arXiv:2010.11929, 2020.

\bibitem{moxin} M. Zhao, N. Meng, J. P. Y. Cheung, C. Yu, P. Lu, and T. Zhang, ``SpineHRformer: A Transformer-Based Deep Learning Model for Automatic Spine Deformity Assessment with Prospective Validation,'' Bioengineering, vol. 10, no. 11, p. 1333, 2023.

\bibitem{hilosa} Z. Pan, J. Cai, and B. Zhuang, ``Fast vision transformers with HiLo attention,'' in Advances in NeurIPS, vol. 35, 2022, pp. 14541-14554.

\bibitem{swin} Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, ``Swin transformer: Hierarchical vision transformer using shifted windows,'' in Proceedings of the IEEE/CVF ICCV, 2021, pp. 10012-10022.

\bibitem{dattention} Z. Xia, X. Pan, S. Song, L. E. Li, and G. Huang, ``Vision transformer with deformable attention,'' in Proceedings of the IEEE/CVF CVPR, 2022, pp. 4794-4803.

\bibitem{hrnet} K. Sun, B. Xiao, D. Liu, and J. Wang, ``Deep high-resolution representation learning for human pose estimation,'' in Proceedings of the CVPR, IEEE, June 2019, pp. 5693–5703.
\begin{comment}
    

\bibitem{nfdp} Z. Huang, R. Zhao, F. H. Leung, S. Banerjee, K. M. Lam, Y. P. Zheng, and S. H. Ling, ``Landmark Localization from Medical Images with Generative Distribution Prior,'' in IEEE TMI, 2024.
\end{comment}
\bibitem{focalloss} T. Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, ``Focal loss for dense object detection,'' in Proceedings of the IEEE ICCV, 2017.

\end{thebibliography}


\end{document}

