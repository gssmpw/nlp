% \vspace{-3mm}
\section{Experiments} \label{sec:experiment}




\subsection{Experiment Setup}


\textbf{Datasets.}
We conduct experiments on the widely used 3d cross-domain benchmark PointDA~\cite{qin2019pointdan} and PointSegDA~\cite{achituve2021self} for shape classification and part segmentation, respectively. PointDA collects shapes of 10 categories from ModelNet (M)~\cite{wu20153d}, ShapeNet (S)~\cite{chang2015shapenet}, and ScanNet (S*)~\cite{dai2017scannet}, with all objects aligned in the up-right direction and down-sampled into 1024 points. We perform six cross-domain classification tasks on PointDA, including {M$\to$S}, {M$\to$S*}, {S$\to$M}, {S$\to$S*}, {S*$\to$M}, and {S*$\to$S}. Twelve cross-domain tasks are conducted on PointSegDA. This dataset consists of several human body shapes that are collected with different point distributions, poses, and scanned humans from four subdomains: Adobe, Faust, Mit, and Scape. The shapes share eight categories of human body parts (head, hand, feet, \etc) while down-sampling into 2048 points.
We strictly follow the same dataset splitting (80\% for training and 20\% for testing) for PointDA as~\cite{qin2019pointdan} and the same data split for PointSegDA as~\cite{achituve2021self} to implement our method under the orientation-aware 3D domain generalization setting.



\noindent\textbf{Evaluation and Metrics.}
To simulate the orientational shift, we randomly generate a series of SO(3) rotation matrices for each point cloud by uniformly sampling the rotation angles of the three-axises in the SO(3) space over $\left[-\pi, \pi\right]$, and rebuild the dataset with free-axis rotated augmentation for training. To comprehensively evaluate the performance and generalizability of each method under the orientation-aware 3D domain generalization setting, we sample 4 equidistant rotation angles per axis, \ie, $[\frac{\pi}{2}, \pi, \frac{3\pi}{2}, 2\pi]$, and construct 64 rotation augmented testing series. Each method is evaluated to obtain the average performance together with variance over the 64 tests. The statistics are presented by the mean of 5 times training for each method. We report the results of PointDA in the form of macro-average precision score (\textit{Avg.}) while measuring the mean Intersection-over-Union (\textit{mIoU}) for PointSegDA to evaluate the segmentation performance.


\noindent\textbf{Compared Methods.}
We compare our method with both 3DDG (Metasets~\cite{huang2021metasets}, PDG~\cite{wei2022learning}) and 3DDA (PointDAN~\cite{qin2019pointdan}, DefRec~\cite{achituve2021self}, GAST~\cite{zou2021geometry}, MLSP~\cite{liang2022point}, SDDA~\cite{cardace2023self}, and PCFEA~\cite{wang2024progressive}) methods on both cross-domain generalization classification and part segmentation tasks. Since they originally ignored the orientational shift, we obtained the experiment results by executing their official codes under the orientation-aware 3D domain generalization setting for fair comparison. To evaluate the robustness of our method towards orientational shift, we also compare with state-of-the-art rotation-equivalent and rotation-invariant point cloud analysis methods by applying them in the cross-domain scenario, including EOMP~\cite{luo2022equivariant}, VN~\cite{Deng_2021_ICCV}, SVN~\cite{su2022svnet}, SPRIN~\cite{you2021prin}, RIPCA~\cite{li2021closer}, RIConv++~\cite{zhang2022riconv}, PaRI~\cite{chen2022devil}, and LocoTrans~\cite{chen2024local}. 


\noindent\textbf{Implementation Details.}
We implement our framework in Fig.~\ref{fig:pipeline} with the widely used rotation-invariant model PointNet~\cite{qi2017pointnet} and non-invariant model DGCNN~\cite{wang2019dynamic} as our backbones, training with 200 epochs and batch size 32 on a single RTX3090 GPU. We employ Adam~\cite{kingma2014adam} as the optimizer. The update period $T$ of the intricate orientation set is set as 20. The learning rate is initialized to $10^{-3}$ with a decay weight of $10^{-4}$ and scheduled by a degradation function $(1 + \gamma (ep/ep_{max}))^{-\beta}$ during the training phase, where $ep_{max}$ is the maximum number of training epochs, and $\gamma, \beta$ are set as 10 and 0.75, respectively. The temperature factor $\tau_s$, $\tau_t$ in Eq.~\ref{eq5} are empirically set to 0.5 and $\tau'$ in Eq.~\ref{eq6} and Eq.~\ref{eq7} is set to 0.07. 
The number of repeat times $AT$ is set as 10, while the number of the intricate variants $V$ in Eq.~\ref{eq6} are set as 5 due to limited memory. $\lambda_{oc}$ and $\lambda_{ms}$ in Eq.~\ref{eq9} are both set as 0.01 by observing the performance changes on M$\to$S (Please refer to the hyper-parameter sensitivity analysis in the supplementary material.). For part segmentation, we simply equip the backbone networks in our framework with an extra decoder for dense prediction and extend the $L_{cls}$ in Eq.~\ref{eq9} into a pixel-wise cross-entropy loss. All our experiments share the same set of hyperparameters. 
For the compared SDDG and SDDA methods, we trained their models with the vanilla architectures of PointNet/DGCNN. For rotation-equivalent methods and rotation-invariant methods (\eg, EOMP~\cite{luo2022equivariant}, SPRIN~\cite{you2021prin}, and RIPCA~\cite{li2021closer}), we strictly follow their official designs on network architectures and augmentation strategies without modifications. This adherence is crucial as the unique modules proposed in these methods are integral to maintaining the rotational invariance of point features.
The augmentation strategies we adopted are the same as~\cite{qin2019pointdan}, including random points down-sampling, random rotation, and point jittering, which are also used in the compared methods except for Metasets and PDG, as we empirically found that maintaining their original augmentation setting yields the best performance for these two methods. 

\input{table_main_exp_pointsegda_miou.tex}

\vspace{-2mm}
\subsection{Experimental Results}

In this section, we report the compared results on classification (Table~\ref{tab:pointda10_avg}) and part segmentation tasks (Table~\ref{tab:pointda10_mIoU}), respectively. The values after $\pm$ denote the standard deviation over the 64 rotation tests, indicating the consistency of the evaluation results towards orientational shift. The smaller the values are, the higher the stability of recognition concerning various orientations.

\noindent\textbf{Orientation-aware 3D Domain Generalization for Classification.}
By averaging the performance over the six domain generalization tasks, our method achieves 8.3\% and 7.7\% improvements on the \textit{Avg.} compared to the DGCNN and the PointNet baselines, respectively, and also outperforms all other methods compared.
As shown in Table~\ref{tab:pointda10_avg}, our framework achieves the best classification results in five out of the six tasks using DGCNN as the backbone, which exceeds both the current state-of-the-art 3D domain generation and rotation-invariant methods by a large margin. When adopting PointNet as the backbone, our performance even exceeds all the other methods over the six cross-domain tasks, demonstrating a more balanced discriminability. Additionally, the relatively small values of the standard deviations across different rotations also validate our model's consistency \wrt rotations.

\input{table_ablation}

\noindent\textbf{Orientation-aware 3D Domain Generalization for Part Segmentation.}
For part segmentation, we compare with several outstanding competitors in Table~\ref{tab:pointda10_avg}, and the results are presented in Table~\ref{tab:pointda10_mIoU}. Our method achieves a 6.5\% increase in the average \textit{mIoU} over the twelve part segmentation tasks. More concretely, our method achieves the best segmentation quality in eight out of the twelve scenes and the second-best results in the rest four scenes, demonstrating its flexibility for a broader range of cross-domain point cloud analysis applications beyond classification.

\noindent\textbf{Comparison with SDDG and SDDA Methods. }
Compared to other 3DDG works, our method demonstrates significant superiority, with 20.4\% and 24.4\% improvement in \textit{Avg.} over PDG and Metasets, respectively. Metasets are more prone to overfitting under the orientational shift as shown in Table~\ref{tab:pointda10_avg}, which can be attributed to the heavy augmentations during meta-training.
We also notice that the performance of PDG is much more unstable and even inferior to the baseline.
The possible reason is that the part-based representation adopted by PDG concentrates on the local geometric features, which are more delicate under the shift of rotation.
Compared with 3DDA methods, our method still shows great power even though they accessed the target data during the training phase. Given MLSP as a reference, our method achieves prominent performance gains, with an improvement of 6.4\% on \textit{Avg.} and 19.8\% on \textit{mIoU}, respectively. This phenomenon is subject to the design of most state-of-the-art 3DDA methods. They focus on learning the geometric information of shape via self-supervised tasks, which are highly sensitive to rotation priors. Generally, all the 3DDA methods have large variances on different rotated test sequences, which indicates that they are less stable under the perturbation of arbitrary orientations. For GAST, which highly relies on accurate supervision of point normals, the performance deteriorates as the error in estimating normals is amplified by unpredictable rotations.


\noindent\textbf{Comparison with RE and RI Methods.} The RE and RI methods are powerful in solving in-domain rotation ambiguity, however, they lack further consideration of the cross-domain scene. This may result in a drastic performance drop when applying the model to other scenes. As shown in Tables~\ref{tab:pointda10_avg} and~\ref{tab:pointda10_mIoU}, the performance of our method in both cross-domain classification and part segmentation still exceeds existing RE and RI methods. Our method achieves 3.0\% improvement in \textit{Avg.} over the best RI classification method RIPCA, and 6.5\% improvement in \textit{mIoU} compared to the best method SPRIN for part segmentation. The standard deviations of our method are also comparable to these methods, indicating considerable rotational robustness.




\vspace{-1mm}
\subsection{Ablation Study and Analysis}

In this section, we delve into the effectiveness of each proposed component of our method, including the intricate orientation mining (IOM), the orientation consistent loss (OC), and the margin separation loss (MS). The ablations are conducted on M$\to$S using both DGCNN and PointNet as our backbones. We also visualize the effects of different values of $\lambda_{oc}$ and $\lambda_{ms}$ in Eq.~\ref{eq9}. Additionally, we investigate the efficacy of intricate orientation mining during the optimization process and provide an evaluation of the time complexity of our method.


\noindent\textbf{Effectiveness of Each Component.} We conduct ablation studies by designing four variants on different backbones, including directly disabling part of the components for V1 to V3 and replacing the IOM with random rotation augmentation in V4. Apart from the \textit{Acc.} and \textit{Avg.}, an extra metric (\textit{Cst.}) is reported to reflect the consistency of the model. For each metric, we report the expectation value of the whole testing set. Specifically, we augment each point cloud 64 times and calculate the \textit{Cst.} by measuring the mean KL-divergence over their output probabilities. The smaller the \textit{Cst.} is, the more consistent the model will be. As illustrated in Table~\ref{tab:ablation}, directly training with intricate samples from intricate orientation mining can significantly enhance the consistency of the model (baseline vs. V1). Incorporating inner-sample contrastive learning on samples with intricate orientation augmentation (V1 vs. V2) could further improve the consistency and promote features's expressiveness that are beneficial for classification. MS collaborates with the representative features learned by OC and further enhances discriminability by explicitly increasing the margin between each category (V2 vs. Ours), however, solely applying MS leads to inconsistent predictions of multiple point cloud's rotated variants, which increases the difficulty of learning a robust classifier (V1 vs. V3). Finally, the overfitting problem of random rotation is evidenced by V4, which intensifies the biased learning problem by applying OC and MS directly on the random rotated point clouds (V4 vs. Ours), demonstrating the effectiveness of IOM in tackling rotational shift under the 3D domain generalization setting.
\input{supp_figure_tsne.tex}

\noindent\textbf{Visualization of Point Cloud Features.} 
For a more comprehensive understanding of the effects of the proposed orientation-aware contrastive training on improving the generalizability of learned point cloud features, in Fig.~\ref{fig:feature}, we employ t-SNE~\cite{van2008visualizing} to visualize the feature spaces generated by our proposed method before and after applying the orientation-aware contrastive training (V1 vs. Ours) in the M$\to$S experiment. For each subdomain, we select the first 50 samples from each category and augment them by uniformly sampling 10 out of the 64 testing rotation series.



The features extracted using V1 are depicted in Fig.~\ref{fig:feature}(a), where we can observe that despite training with the intricately augmented version of each point cloud, the resulting feature space remains non-separable, with features of the same category fragmenting into multiple small clusters, such as the orange and purple points. However, notable spatial proximity can be observed \wrt the source features (`$\star$') and the target features (`$+$') in the feature space. This phenomenon corroborates our findings in Section~\ref{sec:analysis}: while augmenting with intricate orientations mitigates the domain gap induced by arbitrary rotations, there is still significant room for enhancing feature discriminability and generalizability. 
As revealed in Fig.~\ref{fig:feature}(b), upon integrating our orientational-aware contrastive training, the resulting feature space exhibits enhanced discriminability. The consistency between a given point cloud and its rotated variants is also markedly improved. Compared to V1, the feature clusters of the same category from both source and target domains become closer, with more clear boundaries between adjacent categories. The feature spaces of both domains are now more separable, benefiting the learning of a more generalizable classifier in cross-domain scenarios. 


