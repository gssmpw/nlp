\documentclass[journal,compsoc]{IEEEtran}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm, algorithmic}

\usepackage{diagbox}
\usepackage{float}
\usepackage{afterpage}
\usepackage{bm}
\usepackage{subfig}

%\usepackage{tabu}
\usepackage{multirow}
\usepackage{color}
\usepackage{tablefootnote}
\usepackage{adjustbox}
\usepackage{wrapfig}

\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{times}
\usepackage{epsfig}
%\usepackage{tabu}
%\usepackage{overpic}
\usepackage{bbding}
\usepackage{etoolbox}
\usepackage{paralist}
\usepackage{ulem}
\usepackage{tikz}

\usepackage{makecell}

\usepackage{xcolor,colortbl}

% \usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\newcolumntype{Y}{p{0.5cm}<{\centering}}
\newcommand{\mc}[2]{\multicolumn{#1}{c}{#2}}
\definecolor{Gray}{gray}{0.5}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{b}{>{\columncolor{white}}c}



\DeclareMathOperator*{\cat}{Cat}


\def\H{\operatorname{H}}
\def\I{\operatorname{I}}
\def\KL{\operatorname{KL}}


\def\etal{\textit{et al}.}
\def\ie{\textit{i.e.}}
\def\eg{\textit{e.g.}}
\def\etc{\textit{etc}}
\def\wrt{\textit{w.r.t. }}

\def\bz{\textcolor{blue}}
\def\xc{\textcolor{red}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\bc}[1]{\textcolor[RGB]{192,0,0}{#1}}
\newcommand{\rc}[1]{\textcolor{blue}{#1}}
% \newcommand{\rb}[1]{\textcolor{teal}{#1}}
% \newcommand{\bb}[1]{\textcolor{blue}{#1}}
\newcommand{\bb}[1]{\textcolor[RGB]{192,0,0}{\textbf{#1}}}
\newcommand{\rb}[1]{\textcolor{blue}{\textbf{#1}}}
\newcommand{\todo}[1]{{\color{blue}{[TODO: #1]}}}

% \newcommand{\rev}[1]{\textcolor{red}{#1}}
\newcommand{\rev}[1]{{#1}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}


\normalem
\begin{document}

\title{Rotation-Adaptive Point Cloud Domain Generalization via Intricate Orientation Learning \\
—— Supplementary Material —— }

\author{{Bangzhen~Liu,~Chenxi~Zheng,~Xuemiao~Xu,~Cheng Xu,~Huaidong~Zhang, \\ and~Shengfeng~He,~\IEEEmembership{Senior Member,~IEEE}}

\thanks{ Bangzhen Liu,~Chenxi~Zheng, and~Xuemiao~Xu are with the School of Computer Science and Engineering, South China University of Technology, Guangzhou, China. E-mail: liubz.scut@gmail.com,~cszcx@mail.scut.edu.cn, and~xuemx@scut.edu.cn.}
\thanks{ Cheng Xu is with the Centre for Smart Health, The Hong Kong Polytechnic University, Hong Kong. E-mail: cschengxu@gmail.com}
\thanks{ Huaidong Zhang is with the School of Future Technology, South China University of Technology, Guangzhou, China. E-mail: huaidongz@scut.edu.cn.}
\thanks{ Shengfeng He is with the School of Computing and Information Systems, Singapore Management University, Singapore. E-mail: shengfenghe@smu.edu.sg.}
}

\markboth{IEEE Transactions on Pattern Analysis and Machine Intelligence}%
{Shell \MakeLowercase{\textit{Liu et al.}}: Rotation-Adaptive Point Cloud Domain Generalization via Intricate Orientation Learning}


\maketitle

\IEEEdisplaynontitleabstractindextext

\IEEEpeerreviewmaketitle


\section{More Experimental Results} \label{sec1}

It is worth noting that the three sub-datasets used in PointDA are all category-wise imbalanced, as shown in Table~\ref{table:dataset}, which indicates that the micro-average precision score (\textit{Acc.}) reported by previous studies is inappropriate to assess the generalizability of cross-domain classification. In the main paper, we instead report the results of PointDA in the form of the macro-average precision score (\textit{Avg.}) for a more convincing evaluation. We also report the extra evaluations in the form of \textit{Acc.} in Table~\ref{tab:pointda10_acc} for reference. Our method still outperforms all the competitors in the average metric over the six cross-domain tasks.

\input{supp_table_dataset}

\noindent\textbf{Evaluation on Aligned Dataset.} {We additionally implement our method under the traditional aligned data scenario, where the rotation only happens on the z-axis. In this case, we adapt our intricate orientation mining approach to specifically identify the most intricate orientations along the z-axis. 
The comparisons with state-of-the-art 3DDG methods are shown in Table~\ref{tab:align}, where the results of competitors are directly borrowed from their papers. Our method surpasses the baselines on all six tasks, demonstrating its effectiveness. The proposed orientation-aware contrastive training enables the model to gain a more comprehensive understanding of point clouds from various challenging perspectives, thereby enhancing the generalizability of the learned features. We notice that our method is slightly inferior on M$\to$S* and S$\to$S*. Since the orientational shift is our major concern, we do not have a special design for capturing geometric information under self-occlusions. However, in this case, our method still outperforms the two 3DDG methods on three out of the six tasks, while achieving the best average accuracy. Furthermore, the experimental results also reveal the presence of rotational shifts in the aligned data scenes, demonstrating the potential of our method for solving this problem.}

\input{supp_table_exp_pointda10_nonrot.tex}

\input{supp_table_exp_pointda10_acc.tex}

\noindent\textbf{Analysis of Hyper-parameter Sensitivity.} 
We evaluate the effects of varying $\lambda_{oc}$ and $\lambda_{ms}$, by changing the value while keeping the other frozen as 0.1. As Fig.~\ref{fig:ablation}(a) and Fig.~\ref{fig:ablation}(b) show, $\lambda_{oc}$ is insensitive across a large range, while larger $\lambda_{ms}$ may slightly decrease the performance of our model. According to the variation of performance curves, we choose $\lambda_{oc}=0.01$ and $\lambda_{ms}=0.01$ as the model setting in our main paper.
\input{supp_figure_plot}

\noindent\textbf{Analysis of Training Stability.} 
We plot the curves of the proposed orientation consistency loss and the marginal separation loss over the training stage to demonstrate the convergence of our intricate orientational learning. As Fig.~\ref{fig:plot}(a) and Fig.~\ref{fig:plot}(b) show, all the losses gradually decrease and converge to a convincing degree. The blue curves are the orientation consistency loss, which periodically bursts every 20 epochs. This is due to the update of the intricate orientation set, which gradually adapts the model to all the intricate orientations. At the end of the training stage, the amplification tends to be stable, indicating the consistency of the object towards various rotations.

\input{supp_figure_loss.tex}


\noindent\textbf{Analysis of Time Complexity.} {We report the computational costs of training/testing one batch of data in milliseconds for different compared methods in Table~\ref{tab:complexity}. The results are obtained by accumulating the running times within a single training/testing epoch and calculating the mean value w.r.t. one batch.} Due to the process of diversifying the intricate orientation set, our method introduces extra computational costs in the training phase. Nonetheless, our method yields the best performance among these methods while achieving the second-best inferencing speed, which is more efficient than the other RE and RI methods that require extra time-consuming modules for practical applications. 

\input{supp_table_complexity}


\section{Extra Visualizations and Analysis} \label{sec2}

\noindent\textbf{The Learned Intricate Augmented Samples.} {In Fig.~\ref{fig:intricat_angle}, we select several point clouds and provide visualizations of how their intricate orientations evolve during training. We trained the model on ModelNet and optimized the intricate set on the testing set every 20 epochs. Each row of the point cloud sequence shows the current pose of the given point cloud augmented by its corresponding intricate orientation at that specific epoch. 
Beneath each sequence, we also visualize the distribution of predicted probabilities and the consistency of prediction over different testing orientations. 
Specifically, for each point cloud, we obtain the predicted probabilities of its 64 testing variants $P = {\{P_a|P_a = \left[p^1_a, ..., p^C_a\right]\}}^A_{a=1}$, where $A=64$ is the number of testing orientation series and $C=10$ is the number of categories. 
The visualized probabilities' distribution $P_m$ is calculated by averaging the predictions over the 64 testing rotation series, such that $P_m = \left[\frac{1}{A}\sum_{j=1}^{A}p^1_j, ..., \frac{1}{A}\sum_{j=1}^{A}p^C_j\right]$. 
To evaluate the predicted consistency, we adopt the entropy as the metric and calculate the consistency $Ent_m$ over the 64 predicted probabilities by 
\begin{equation*}
  Ent_m = \left[\frac{1}{A}\sum_{j=1}^{A}p^1_j log p^1_j, ..., \frac{1}{A}\sum_{j=1}^{A}p^C_j log p^C_j\right].
\end{equation*}
As the number of training epochs increases, both the confidence and output consistency of the model are enhanced. For samples located near the decision boundaries, such as row 6 and row 9, learning with intricate orientation mining could significantly alleviate the ambiguity of learned features, thereby producing a more robust and generalizable classifier for downstream tasks.
}



\input{supp_figure_confusion_mat.tex}

\input{supp_figure_intricat_angle.tex}


\noindent\textbf{Confusion Matrices.} {we provide the evaluation results for Metasets~\cite{huang2021metasets}, PDG~\cite{wei2022learning}, and our method in the form of confusion matrix on the target domain. ShapeNet is a dataset whose samples are highly imbalanced across different categories, while ModelNet is much more balanced. The confusion matrices of the three approaches are shown in  Fig.~\ref{fig:visualization_m2s} and Fig.~\ref{fig:visualization_s2m}. Compared with the other two 3D domain generalization methods, our method has more compact confusion matrices under the orientation shift. For M$\to$S, both our method and Metaset are separated relatively well while PDG has much inaccurate classification on class "Plant". The inner reason is that the part-based feature utilized by PDG may encounter confusing local expressions, such as the plane of the table and the bottom of a potted plant. For S$\to$M, our method achieves more balanced and concise results. We observe that the sample of class "monitor" is much easier to misclassify into "bed" due to the similar plane structure of their surface. Similar trends happen for the categories "table" and "cabinet", which have less discriminative features in the view of shape. }

{In summary, the single shape cannot serve as a discriminative representation in some cases. This is the limitation of shape representation under the orientation shift since there are a lot of objects whose shapes are similar but belong to different categories. In this case, extra visual (\eg, texture or color), linguistic information, or spatial cues are important to provide complement representation, which may benefit the problem of cross-domain generalization under orientation shift. We will plan to investigate the function of these features in our future work.  }

\section{Gradient of the rotation parameters} \label{sec3}
In this section, we provide detailed calculations about the optimizable parameters $\Theta$ concerning a given model $F$. Considering the objective of optimizing $\Theta$ within a standard classification task, we have the following objective:
\begin{equation}
  \hat{\Theta} = \mathop{\arg\max_{\Theta}}L(w_{opt}, \hat{P}, y), 
\end{equation}
where $w_{opt}$ is the freeze parameter of $F$, $(\hat{P}, y)$ are the augmented point cloud and label:
\begin{equation}
  \begin{split}
    ~\hat{P}&=f(\hat{\Theta}, P)  \\
    &=R_{\theta_{x}}\cdot R_{\theta_{y}}\cdot R_{\theta_{z}}\cdot P.
  \end{split}
\end{equation}
According to the chain rules, the gradient of $\hat{\Theta}$ is calculated by:
\begin{equation}
  \begin{split}
  \frac{\partial L}{\partial \hat{\Theta}} &= \frac{\partial L}{\partial \hat{P}} \frac{\partial \hat{P}}{\partial \hat{\Theta}} \\
  &=\frac{\partial L}{\partial \hat{P}}
  \left(
  \frac{\partial R_{\theta_{x}}}{\partial \theta_{x}}
  R_{\theta_{y}}
  R_{\theta_{z}} \quad
  R_{\theta_{x}}
  \frac{\partial R_{\theta_{y}}}{\partial \theta_{y}}
  R_{\theta_{z}} \quad
  R_{\theta_{x}}
  R_{\theta_{y}}
  \frac{\partial R_{\theta_{z}}}{\partial \theta_{z}}
  \right)P,
  \end{split}
\end{equation}
where 
\begin{equation}
  \begin{split}
R_{\theta_{x}} = 
\begin{pmatrix}
  1 & 0 & 0 \\
  0 & \cos\theta_{x} & -\sin\theta_{x} \\
  0 & \sin\theta_{x} & \cos\theta_{x} 
\end{pmatrix},
\\
R_{\theta_{y}} = 
\begin{pmatrix}
  \cos\theta_{y} & 0 & \sin\theta_{y} \\
  0 & 1 & 0 \\
  -\sin\theta_{y} & 0 & \cos\theta_{y}
\end{pmatrix},
\\
R_{\theta_{z}} = 
\begin{pmatrix}
  \cos\theta_{z} & -\sin\theta_{z} & 0  \\
  \sin\theta_{z} & \cos\theta_{z} & 0 \\
  0 & 0 & 0
\end{pmatrix},
\end{split}
\end{equation}
and 
\begin{equation}
    \begin{split}
\frac{\partial R_{\theta_{x}}}{\partial \theta_{x}} = 
\begin{pmatrix}
  0 & 0 & 0 \\
  0 & -\sin\theta_{x} & -\cos\theta_{x} \\
  0 & \cos\theta_{x} & -\sin\theta_{x} 
\end{pmatrix}, \\
\frac{\partial R_{\theta_{y}}}{\partial \theta_{y}} = 
\begin{pmatrix}
  -\sin\theta_{y} & 0 & \cos\theta_{y} \\
  0 & 0 & 0 \\
  -\cos\theta_{y} & 0 & -\sin\theta_{y}
\end{pmatrix}, \\
\frac{\partial R_{\theta_{z}}}{\partial \theta_{z}} = 
\begin{pmatrix}
  -\sin\theta_{z} & -\cos\theta_{z} & 0  \\
  \cos\theta_{z} & -\sin\theta_{z} & 0 \\
  0 & 0 & 1
\end{pmatrix}.
\end{split}
\end{equation}



\section{Theoretical Analysis for Rotation-Adaptive Point Cloud Domain Generalization} \label{sec4}

In this section, we provide theoretical proof demonstrating how orientational consistency functions to bridge the domain gap, analyzed from the perspective of mutual information reduction.

Let $X\!=\!(U, V)$ represent a 3D point cloud, where $U$ corresponds to orientation-dependent variables and $V$ to orientation-independent variables. In our case, we assume that the ranges of $U$ and $V$ remain consistent across domains.
For $X_s\!\sim\!p_\mathrm{src}(x)$, where $p_\mathrm{src}(x)$ denotes the source domain data distribution, the marginal distributions \wrt $U_s$ and $V_s$ are expressed by:
\begin{equation}
 p_\mathrm{src}(u)=\int p_\mathrm{src}(x) \mathrm{d}v, \quad p_\mathrm{src}(v)=\int p_\mathrm{src}(x) \mathrm{d}u.
\end{equation}
Considering the data distribution $X_a\!\sim\!p_\mathrm{aug}(x)$ after augmentation, where each sample is assumed to be uniformly sampled \wrt orientations, the marginal distributions \wrt $U_a$ and $V_a$ are given by:
\begin{equation}
 p_\mathrm{aug}(u)=\mathcal{U}(\mathcal{D}_{U_a}), \quad p_\mathrm{aug}(v)=p_\mathrm{src}(v),
\end{equation}
where $\mathcal{U}(\cdot)$ denotes a uniform distribution over the measurable domain $\mathcal{D}_{U_a}$ of ${U_a}$. For simplicity, the subscript of $U_a$ in $\mathcal{D}_{U_a}$ is omitted without causing ambiguity in the subsequent analysis. In this work, we adopt the proposed orientation-aware contrastive learning framework to approximately achieve this, where ${U_a}$ is represented by Euler angles and $\mathcal{D}_{U}:=[-\pi, \pi)^3$. 



Based on the definition of joint entropy, the entropy of $p_\mathrm{src}(x)$ can be expressed in terms of its marginal entropies \wrt $U_s$ and $V_s$, along with an additional term presenting the mutual information between these two components:
\begin{equation}
\begin{aligned}
    \H(X_s) 
 =& \H(U_s)+\H(V_s)-\I(U_s;V_s) \\
 =& \mathbb{E}_{u\sim p_\mathrm{src}(u)}[-\log p_\mathrm{src}(u)] + \mathbb{E}_{v\sim p_\mathrm{src}(v)}[-\log p_\mathrm{src}(v)] \\
    &- \mathbb{E}_{x\sim p_\mathrm{src}(x)}\log \frac{p_\mathrm{src}(x)}{p_\mathrm{src}(u)p_\mathrm{src}(v)},
\end{aligned}
\end{equation}
where $\I(U_s;V_s)$ represents the mutual information between $U_s$ and $V_s$ in $p_\mathrm{src}(x)$. 
Since $p_\mathrm{aug}(u)$ follows a uniform distribution and $U_a$ and $V_a$ of $p_\mathrm{aug}(x)$ are independent under this setting, the entropy of $p_\mathrm{aug}(x)$ is given by $\I_\mathrm{aug}(U_a;V_a)\!=\!0$, and the entropy of $p_\mathrm{aug}(u)$ corresponds to the measure of $\mathcal{D}_{U}$, denoted as $m(\mathcal{D}_{U})$. 
Thus, the entropy of $p_\mathrm{aug}(x)$ can be simplified as follows:
\begin{equation}
\begin{aligned}
    \H(X_a) &= \H(U_a)+\H(V_a) \\
    &= \log m(\mathcal{D}_{U}) + \mathbb{E}_{v\sim p_\mathrm{aug}(v)}[-\log p_\mathrm{aug}(v)],
\end{aligned}
\end{equation}
where $m(\mathcal{D}_{U})\!=\!(2\pi)^3$ in our case.

We use the KL divergence to quantify the distributional shift between the source and the target distribution.
For any $X_t\!\sim\!p_\mathrm{tgt}(x)$, where $p_\mathrm{tgt}(x)$ represents the target domain distribution, the KL divergence between $p_\mathrm{tgt}(x)$ and $p_\mathrm{src}(x)$ (or $p_\mathrm{aug}(x)$) can be computed once the cross-entropy between them is known. 
However, directly calculating the cross-entropy between $p_\mathrm{tgt}(x)$ and $p_\mathrm{src}(x)$ (or $p_\mathrm{aug}(x)$) is intractable, and it is often treated as an optimization objective to minimize. Notably, the cross-entropy between $p_\mathrm{tgt}(x)$ and $p_\mathrm{src}(x)$ (or $p_\mathrm{aug}(x)$) shares the same upper bound, as the samples $X_s$, $X_a$, and $X_t$ all share the same dimensionality:
\begin{equation}
    \sup_{p_\mathrm{src}}{\H(p_\mathrm{tgt}, p_\mathrm{src})} = \sup_{p_\mathrm{aug}}{\H(p_\mathrm{tgt}, p_\mathrm{aug})} = \log ({m(\mathcal{D}_U) \times m(\mathcal{D}_V)}).
\end{equation}
Here, $\mathcal{D}_V$ is the measurable domain of $V_s$, $V_a$, and $V_t$.
It is straightforward to prove that $\H_\mathrm{aug}(X_a) > \H_\mathrm{src}(X_s)$, as the mutual information is non-negative and entropy reaches its upper bound when the distribution is uniform.
Therefore, the relation between the upper bound of the KL divergence from $p_\mathrm{tgt}(x)$ to $p_\mathrm{src}(x)$ and from $p_\mathrm{tgt}(x)$ to $p_\mathrm{aug}(x)$ can be expressed as:
\begin{equation}
\begin{aligned}
    \sup_{p_\mathrm{src}}{\KL(p_\mathrm{tgt}||p_\mathrm{src})} &= \sup_{p_\mathrm{src}}{\H(p_\mathrm{tgt};p_\mathrm{src})} - \sup_{p_\mathrm{src}}{\H(X_\mathrm{s})} \\
    &> \sup_{p_\mathrm{aug}}{\H(p_\mathrm{tgt};p_\mathrm{aug})} - \sup_{p_\mathrm{aug}}{\H(X_\mathrm{a})} \\
    &= \sup_{p_\mathrm{aug}}{\KL(p_\mathrm{tgt}||p_\mathrm{aug})}. \label{eq:ieq}
\end{aligned}
\end{equation}
As revealed in Eq.~\ref{eq:ieq}, the upper bound of $\KL(p_\mathrm{tgt}||p_\mathrm{aug})$ is consistently lower than $\KL(p_\mathrm{tgt}||p_\mathrm{src})$, demonstrating the effectiveness of orientation invariance in reducing the domain shift under the disturbance of varying rotations. Consequently, the final upper bound of $\KL(p_\mathrm{tgt}||p_\mathrm{aug})$ is formally given as follows:
\begin{equation}
    \sup_{p_\mathrm{aug}}{\KL(p_\mathrm{tgt}||p_\mathrm{aug})} = \log m(\mathcal{D}_V) - \mathbb{E}_{v\sim p_\mathrm{aug}(v)}[-\log p(v)].
\end{equation}


\section{Limitation and Future Work} 
Although our method shows commendable advantages in handling cross-domain orientational shifts, it faces challenges with other complex types of domain shifts, such as heavy occlusions. This is because our framework does not offer an explicit design for tackling these domain shifts. Addressing this limitation, possibly through constructing a more powerful and versatile feature space resilient to multiple domain shifts via self-supervised pre-training, is a goal for future work.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document} 
