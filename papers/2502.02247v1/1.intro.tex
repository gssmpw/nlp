
\section{Introduction}

\indent3D scene understanding and reasoning are crucial in various applications~\cite{leibe2007dynamic, brostow2008segmentation} such as autonomous driving, architectural design, and virtual/augmented reality. 3D Point cloud representation has gained popularity due to its simplicity and effectiveness, particularly with the advancements in deep learning~\cite{wang2019dynamic, qi2017pointnet, zhang2022self}. However, most existing 3D point cloud analysis models hold a strong \textit{i.i.d.} (independent and identically distributed) assumption between training and testing domains. This assumption can lead to significant performance degradation in real-life out-of-distribution scenarios. Factors such as varying sensor parameters and environmental conditions introduce distributional divergence between training and testing domains, limiting the practical usage of these models. 


Prevailing methods focus on addressing different types of domain shifts in point clouds, such as shape variance, non-uniform point density, sensor noise, and self-occlusion, through point cloud domain adaptation (3DDA)~\cite{qin2019pointdan, liang2022point} and domain generalization (3DDG)~\cite{huang2021metasets, wei2022learning}. 
However, they make the naive assumption that objects from both source and target domains are strictly aligned, meaning they have the same upright pose across all objects and categories. This assumption disregards the orientation of objects and can result in a model that is biased toward the aligned dataset. As a consequence, even slight variations in orientation may incur catastrophic misclassification by the model (Fig.~\ref{fig:teaser}(a)). In real-world scenarios, objects often originate from various domains with diverse orientations, ensuring an accurate understanding of these targets imposes great demands on a generalizable and rotational robust 3D recognition systems. This motivates us to investigate the orientational gap and study a new problem called \textit{orientation-aware 3D domain generalization}, which aims at exploring the robustness of 3D representations under the disturbance of varying rotations.

To ensure the robustness of a model against arbitrary rotational variations, random rotation augmentation~\cite{qi2017pointnet,wang2019dynamic} is intuitively applied. However, generalizing to arbitrary rotations is challenging due to the numerous possible rotated angles in 3D space~\cite{zhao2019rotation}. Sparse sampling over the dense rotation space often leads to imbalanced learning (Fig.~\ref{fig:teaser}(b)), where the model tends to memorize the easy tasks but struggles with unseen challenges. In practice, tackling challenging tasks can enhance one's ability to draw inferences for similar problems, a process known as extrapolation. Deep models exhibit a similar talent for extracting rich information from samples that are difficult to separate~\cite{schroff2015facenet}. This motivates our focus on challenging orientations, which will be discussed with supporting experimental evidence in Section~\ref{sec:analysis}. Building upon these observations, we aim to enhance the model's generalizability to arbitrary rotations by learning from intricate samples.

\begin{figure}[t]
    % \flushleft
    \centering
    \subfloat[Trained on aligned data]{%[b]{0.45\textwidth}
        \label{fig:teaser_a}
        % \centering
        \includegraphics[width=0.55\linewidth]{./resources/teaser/teaser_a.pdf}
    }
    \subfloat[w/ rotation augmentation]{%[b]{0.45\textwidth}
        \label{fig:teaser_b}
        % \centering
        \includegraphics[width=0.42\linewidth]{./resources/teaser/teaser_b.pdf}
        \hspace{-20mm}

        \includegraphics[width=0.2\linewidth]{./resources/teaser/legend.pdf}

    }
    \vspace{-2mm}
    \caption{t-SNE visualization of the feature spaces in the geometric encoder of DGCNN~\cite{wang2019dynamic} trained on ModelNet~\cite{wu20153d} before and after random rotation.
(a) Random rotation perturbation leads to a chaotic cluster in the feature space, rendering it non-discriminative.
(b) Even with rotation augmentation, the rotated shapes tend to be located in the low-density region of the feature space, which remains non-discriminative and less consistent. }
\vspace{-5mm}
    \label{fig:teaser}
\end{figure}


Based on this idea, we propose to address the problem of orientation-aware 3D domain generalization through intricate orientation mining. Our approach alternatively optimizes between multiple steps. First, we identify the most aggressive rotation for each point cloud and construct an intricate set by optimizing intricate orientations. Next, we create the hardest sample pairs based on the intricate set and utilize contrastive learning to obtain categorically discriminative and generalizable representations with rotation consistency.
More specifically, we optimize the intricate rotation matrix, which is parameterized by three optimizable parameters, by maximizing the prediction error of the current model. Once we obtain the intricate set, we construct sample-wise and category-wise pairs for contrastive training. The sample-wise pair consists of the original point cloud along with its rotated variant, where the rotation angles are sampled from the intricate set. The category-wise pair is formed by selecting point clouds from different classes that are close together in the representation space.


In summary, our contributions are three folds:
\begin{itemize}
\item We introduce the novel task of orientation-aware 3D domain generalization, which more accurately captures real-world domain shifts involving unpredictable rotations, and delve into the importance of challenging orientations in reducing orientational shift.
\item We propose a novel approach that enhances rotational robustness through intricate orientation mining, which further incorporates a contrastive framework that utilizes intricate samples to improve the generalizability of feature learning via inner- and intra-categorically contrastive learning.
\item Experimental results on widely used benchmarks demonstrate that our method achieves state-of-the-art performance on several point cloud analysis tasks under the orientation-aware 3D domain generalization setting.
\end{itemize}

