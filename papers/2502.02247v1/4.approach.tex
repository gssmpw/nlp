

% \vspace{-3mm}
\section{Intricate Orientation Learning} \label{sec:method}

\input{figure_analysis_2.tex}

The pipeline of our framework is depicted in Fig.~\ref{fig:pipeline}. It consists of an iterative optimization process that alternates between intricate orientation mining and orientation-aware contrastive training. We first generate an alternative set of intricate rotation angles through intricate orientation mining. Then we augment the training point clouds by applying the selected angles from the alternative set, creating a series of intricate sample pairs. To enhance the generalizability of the learned representations, we employ orientation-aware contrastive training using these intricate pairs. Our framework incorporates an orientation consistency loss, which encourages the learning of more representative features while promoting their consistency \wrt random rotations. Additionally, we introduce a margin separation loss to further improve the categorical discriminability of the learned representation space.
In the following sections, we elaborate on each component of our framework in detail.


\input{figure_pipeline}

% \vspace{-3mm}
\subsection{Intricate Orientation Mining}
Let us first consider a standard classification problem over the given data distribution $D = {\{(P_i, y_i)}\}^{n}_{i=1}$. Suppose we have a point cloud classification model, the goal is to search the model parameters $\omega^*$ that optimize the empirical risk $E_{(P, y)\sim D}\left[L(w, P, y)\right]$, where $L$ is a proper loss function. To improve the robustness, the most adopted technique is data augmentation, such that the model can resist possible perturbations on the input data, resulting in the following objective:
\begin{equation}
    \omega^*=\mathop{\arg\min_{\omega}} \mathbb{E}_{(P, y)\sim D} \left[L(\omega, f(P), y)\right],
\end{equation}
where $f$ is the perturbation function. Considering the orientational shift only, the perturbation function of a given point cloud $P_i$ can be regarded as augmenting the aligned pose with a rotation matrix $M_i$. According to Euler's rotation theorem, the rotation matrix is defined in $\mathbb{R}^3$ as $M_i = R_{\theta_{x_i}}\cdot R_{\theta_{y_i}}\cdot R_{\theta_{z_i}}$, 
where $R_{\theta_j}$ is the rotation matrix about the axis-j by angle $\theta_{j} \in \left[-\pi, \pi\right)$ over the cartesian coordinates. In this case, the rotation matrix is parameterized by $\Theta_i=\left[\theta_{x_i}, \theta_{y_i}, \theta_{z_i}\right]$. Since the trigonometric function is differential in the scope $\left[-\pi, \pi\right)$, we can optimize $\Theta_i$ through gradient descent.



The essence of intricate orientation mining lies in the deliberate search for intricate orientations, instead of directly optimizing the model with random rotations. This endeavor aims to identify a rotation matrix $\hat{\Theta}_i$ that introduces perturbations to the point cloud, such that the 3D model is confounded:
\begin{equation}
    \hat{\Theta}_i = \mathop{\arg\max_{\Theta_i}}L(\omega^*, f(\Theta_i, P_i), y_i), \label{formula:iom}
\end{equation}
where $f(\Theta_i, P_i)=M_i\cdot P_i$, and $L$ is a task-specific optimization function~(\eg, cross-entropy loss for classification task, hereafter referred to as $L_{cls}$). By iteratively maximizing this objective, we can calculate the most intricate orientations for all the point clouds within $D$. The details of gradient calculation are provided in the supplement. We adopt Projected Gradient Descent~\cite{madrytowards} to ensure that the optimized $\Theta_i$ can generate a pure rotation matrix. By applying intricate orientation mining to the whole training dataset, we obtain the intricate orientation set $I=\{\hat{\Theta}_{i}\}^{n}_{i=1}$, which is essential for the construction of intricate sample pairs. Note that each orientation is specified for the according sample. To increase the diversity of $I$, we further initialize $\Theta$ with random values and repeat the optimization multiple times. Finally, the intricate set is defined as $I=\{\{\hat{\Theta}^j_{i}\}^{AT}_{j=1}\}^{n}_{i=1}$, where $AT$ is the number of augmentation times. During the training stage, we periodically update the intricate set $I$ for every $T$ epochs. 




% \vspace{-3mm}
\subsection{Orientation-aware Contrastive Training}

Based on the intricate orientation set $I$, we construct the intricate sample pairs and build a contrastive learning framework upon them to obtain categorically discriminative and generalizable features with rotational consistency. As Fig.~\ref{fig:pipeline} shows, our model consists of an optimizable student network $F_s\!=\!C_s\circ E_s$ and a frozen teacher network $F_t\!=\!C_t\circ E_t$, where $E_{s/t}$ is a feature extractor which encodes point cloud to the representation space and derives the final classification result through a linear classifier $C_{s/t}$. The teacher network is optimized by Exponential Moving Average (EMA)~\cite{tarvainen2017mean}.

\noindent\textbf{Inner-sample Rotational Consistency.} The main idea is to maximize the agreement between the original shape and its intricate augmented variants via self-contrastive learning. Each time we sample a mini-batch of $N$ point clouds $\left\{P_{m}\right\}^N_{m=1}$ and randomly select one intricate rotation $\hat{\Theta}_m$ from $I$ for each point cloud. Then we construct the intricate sample pairs $B_{it} = \{(P_{m}, \hat{P}_m )\}^N_{m=1}$, where $\hat{P}_m = f(\hat{\Theta}_{m}, P_m)$ is the augmented version of the original point cloud with the intricate rotation. $B_{it}$ serve as the input of (${F_t}$, $F_s$) and the output logits of $(P_{m}, \hat{P}_m )$ are denoted as $p_m$ and $\hat{p}_m$, respectively. To ensure the consistency of the intricate pairs, we perform knowledge distillation~\cite{hinton2015distilling} on the output probabilities, the orientation consistency loss is therefore formulated as:
\begin{equation}\label{eq5-0}
    \sigma(p/\tau) = \frac{exp(p/\tau)}{\sum_{j=1}^{K}{exp(p^{(j)}/\tau)}},
\end{equation}
\begin{equation}
    L_{oc} = -\frac{1}{N}\sum_{m=1}^{N} \sigma(\hat{p}_m/\tau_t)\log(\sigma(p_m /\tau_s)), \label{eq5}
\end{equation}
where $K$ is the number of categories and $\sigma(\cdot)$ is the softmax function. $\tau \in \mathbb{R^+}$ is the temperature parameter controlling the magnitude of the output logits, where the subscripts in $\left[\tau_s, \tau_t\right]$ denote the source and target domains, respectively. By iteratively refining the distance between the aligned object and its intricate variant, the model can gradually adapt to the harder rotational variations and learn common knowledge concerning different rotations.


\noindent\textbf{Intra-category Discriminability.} To further enhance discriminability, we aim for the learned representation space to maintain rotational consistency while improving the classification ability. To achieve this, we introduce a margin separation loss that yields a classifier with a concise and accurate boundary, capable of handling the unforeseen rotational perturbations. Specifically, we first construct the intra-category intricate pairs by considering the relationships between different categories in the same mini-batch. The intricate pairs can be divided into two groups, including the positive pairs and the negative pairs.
Simply, the positive pairs can be found by traversing the training batch and sorting out samples that belong to the same class. But the embedding space under multiple orientations may not be well simulated by the originally aligned point clouds. Instead of learning from only one intricate orientation, we further augment each point cloud $P_{i}$ with $V$ number of intricate angles sampled from $\{\hat{\Theta}^l_{i}\}^{AT}_{l=1}$ to better simulate the feature space. The loss is designed to minimize the cosine distance between the positive samples as follows:
\begin{equation}
    L_{pos}\!=\!- \frac{1}{K}\sum_{k=1}^{K} \!\frac{1}{V{N^k_p}^2}\!\!\!\sum_{i=1,j=1 \atop j\neq i}^{N^k_{p}, N^k_{p}}\!\sum_{v=1}^{V} \cos(\frac{E_s(\hat{P}_{i,v})}{\tau'},\! \frac{E_s(\hat{P}_j)}{\tau'}),
    \label{eq6}
\end{equation}
where $\cos(\cdot, \cdot)$ denotes the cosine similarity between the two inputs, $N^k_{p}$ is the number of samples of class $k$ within the mini-batch and $\tau'$ is the temperature parameter.

The negative pairs are formed by samples with different labels.
To enlarge the discrepancy of different categories in the representation space, we plan to maximize the distance between each negative pair with:
\begin{equation}
    L_{neg}\!=\!\frac{1}{K}\sum_{k=1}^{K}  \frac{1}{{N^k_p}{N^k_n}}\sum_{i=1}^{N^k_p} \sum_{j=1}^{N^k_{n}} \cos( \frac{E_s(\hat{P}_i)}{\tau'}, \frac{E_s(\hat{P}_j)}{\tau'}),
    \label{eq7}
\end{equation}
where $N^k_{n}$ is the number of negative samples.
The overall margin separation loss is summarized as follows:
\begin{equation}
    L_{ms} = L_{pos} + L_{neg}.
\end{equation}Through contrastive tasks, the point representations under varying rotations are further clustered within the same category, while the other categories are pushed away. A more compact and consistent representation space is formed, thereby enhancing the discriminability for rotated shape classification.

\input{table_main_exp_pointda10_avg}


The entire framework is optimized in an alternative scheme between the intricate orientation mining and the orientation-aware contrastive training.
For the intricate orientation mining, the final objective is the same as Eq.~\ref{formula:iom}, where $L$ is the standard cross-entropy function over the originally aligned point clouds.
The intricate set is periodically updated after $T$ epochs of training to avoid overfitting to the current intricate rotations.
For orientation-aware contrastive training, the final loss function is
\begin{equation}
    L_{final} = L_{cls} + \lambda_{oc} L_{oc} + \lambda_{ms} L_{ms},
    \label{eq9}
\end{equation}
where $L_{cls}$ is the cross-entropy loss on the intricate point clouds. 
