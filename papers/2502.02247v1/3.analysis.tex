
\section{Orientational Shift Analysis} \label{sec:analysis}


\textbf{Problem Definition.}
In practical scenarios, objects often originate from various domains, and concurrently, present diverse orientations. To achieve an accurate understanding of these targets, our goal is to grant 3D recognition systems with cross-domain generalizability and robustness to rotational transformations. To this end, we introduce a novel task: orientation-aware 3D domain generalization, and explore the applicability of existing 3D recognition systems under orientational shifts.
Let $\mathbb{X}$ and $\mathbb{Y}$ be the input and label spaces, respectively. We consider a labeled source domain $D_s = {\{(P_i^s, y_i^s)}\}^{n_s}_{i=1}$ with $n_s$ samples and an unlabeled target domain $D_t = \{{P_i^t}\}^{n_t}_{i=1}$. Here, $y^s\in \mathbb{Y}$ is the source labels, and $P^s, P^t \in \mathbb{X}$ represent point clouds with a specified orientation relative to the world coordinate system, where each $P_i \in \mathbb{R}^{N_c \times 3}$ consists of $N_c$ points. The orientation is defined by a $3\times3$ rotation matrix $M_i \in \mathcal{O} \subseteq$ SO(3), where $\mathcal{O}$ is the set of orientations in the given domain and SO(3) represents all rotations in $\mathbb{R}^3$. The objective of orientation-aware 3D domain generalization is to learn a projection function $g: \mathbb{X} \to \mathbb{Y}$ that can be applied to any given target domain $D_t$ with arbitrary orientations in $\mathcal{O}$, by solely training on the labeled source domain $D_s$.


\input{figure_analysis_1}

\noindent\textbf{Generalization Analysis of Orientations.}
To accurately understand 3D shapes, information from multiple perspectives is often necessary. Some of the perspectives are easy-to-understand by deep models but less informative, incurring a phenomenon known as ``taking the whole from a part''. Specifically, in the context of rotation-robust point cloud analysis, these learning perspectives refer to those rotated variants with small gradients during training. Random rotation attempts to capture comprehensive information through a Monte Carlo approach, but the vast number of possible rotated angles introduces difficulties and leads to imbalanced learning.
This biased learning problem could give rise to inaccurate selection of point features, making false decisions. As shown in Fig.~\ref{fig:feature_statement}(a), the learned model cannot well distinguish the rotated sample from ``table" and ``chair", as they may contain similar features such as slim legs, which are learned from easy-to-understand samples. 
On the other hand, intricate samples lying beyond current cognitive boundaries can offer complementary knowledge for learning a more accurate discriminative boundary.

We empirically validate these insights by employing Maximum Mean Discrepancy (MMD)~\cite{gretton2012kernel} as a measure of distributional shifts. The MMD calculates the distance between source and target distributions in the Reproducing Kernel Hilbert Space (RKHS). We train a DGCNN~\cite{wang2019dynamic} on the ModelNet (source domain) and test it on ShapeNet (target domain), augmenting the data with random rotations and intricate orientations, respectively. The intricate orientations of the training dataset are obtained by optimizing the rotational angles of each point cloud to maximize the cross-entropy loss with a baseline model trained on aligned data. Fig.~\ref{fig:analysis}(a) depicts the per-class MMD values, demonstrating that training with intricate orientations reduces orientational shifts more effectively. To evaluate discriminability and consistency under varying rotations, we further train a linear SVM on the source domain and test it on the target domain. We augment each point cloud 64 times as illustrated in Section~\ref{sec:experiment} and measure consistency by calculating the mean KL-divergence between their output probabilities. As shown in Fig.~\ref{fig:analysis}(a), the discriminability of the model augmented with intricate orientations matches that of the randomly rotated version, while better preserving consistency across different rotations.
Based on these findings, we aim to leverage intricate samples to reduce orientational shift, enhance output consistency, improve model discriminability to arbitrary rotations, and finally obtain a better classifier that is generalizable towards various domains (Fig.~\ref{fig:analysis}(b)).
