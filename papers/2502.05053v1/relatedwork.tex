\section{Related Work}
% RUSS
\subsection{Robotic Ultrasound Systems}
Ultrasound image quality depends heavily on acquisition parameters such as force, orientation, and surface contact. To address this, Pierrot~\emph{et al.}~\cite{pierrot1999hippocrate} implemented a force control schema in a teleoperative RUSS to maintain constant scanning force. Jiang~\emph{et al.}~\cite{jiang2020automatic} introduced an orientation optimization algorithm based on contact force for better visualization.
To ensure that the ultrasound probe is in good contact with the surface, Chatelain~\emph{et al.}~\cite{chatelain2017confidence} integrated ultrasound confidence map~\cite{karamalis2012ultrasound} into the control loop and presented confidence-driven control. However, the proposed control algorithm was designed for convex probe. In the sight of such limitation, Jiang~\emph{et al.}~\cite{jiang2022precise} proposed an orientation correction method based on the confidence map for the linear probes. The experimental results clearly demonstrated its effectiveness, but the theoretical justification is missing.

\par
Recent advancements in RUSS for vascular applications have gained significant attention. Jiang~\emph{et al.}~\cite{jiang2021autonomous} proposed an autonomous scanning framework for peripheral vascular diseases using real-time vessel segmentation. Bi~\emph{et al.}~\cite{bi2022vesnet} applied reinforcement learning to autonomously navigate the probe to the longitudinal view of the carotid artery. Huang~\emph{et al.}~\cite{huangQ2024robot} developed a system to autonomously perform carotid scans by imitating clinical protocols, while Goel~\emph{et al.}~\cite{goel2022autonomous} introduced a Bayesian Optimization-based path planning framework for femoral artery screening.

\par
\revision{
Most existing RUSSs for vascular scanning do not address the challenge of handling multiple vessels and often require additional tools to capture the doctor's intention for automatic maneuvering. Guidance can be implemented teleoperatively, with an expert operating remotely while the robot on-site follows~\cite{fu2022robot}, or through virtual reality, providing virtual guidance to the human operator~\cite{black2024human}. Another approach involves pre-planned scanning paths, where the robot provides virtual fixtures to enable reproducible ultrasound scanning for follow-up validations~\cite{huang2024robot}. However, among these strategies, gaze signal—an intuitive and hands-free interaction method—remains unexplored. This approach is particularly advantageous in intra-operative scenarios, as it allows surgeons to perform tasks without altering workflow.}

% Gaze-guided Medical Robotic Systems
\subsection{Gaze-guided Medical Robotic Systems}
Initial attempts to integrate gaze information into medical robotic systems have been made in various scenarios, especially for laparoscopic surgeries. 
Noonan~\emph{et al.}~\cite{noonan2010gaze} used eye tracking to control an articulated robotic laparoscope for stable, hands-free visualization. Fujii~\emph{et al.}~\cite{fujii2018gaze} enhanced the gaze-guided laparoscope maneuverability by using a robotic arm instead of a rigid fixture. Clancy~\emph{et al.}~\cite{clancy2011gaze} combined gaze tracking with a liquid lens in the da Vinci system for automatic focus adjustment during minimally invasive surgeries. Gaze tracking has also been applied to constrain laparoscopic surgical tools for tissue safety~\cite{mylonas2012gaze} and to improve collaboration in multi-robot surgeries by visualizing fixation points~\cite{kwok2012collaborative}. Li~\emph{et al.}~\cite{li2018free} further optimized gaze tracking accuracy by compensating for head movements.
Beyond laparoscopic applications, Guo~\emph{et al.}~\cite{guo2019novel} utilized gaze to control needle insertion in CT-guided interventions, while Kogkas~\emph{et al.}~\cite{kogkas2019free} introduced a gaze-guided robotic scrub nurse to deliver surgical tools. To the best of our knowledge, initial attempt to integrate the gaze tracker into RUSS has not yet occurred.

% Gaze-guided Network
\subsection{Gaze-guided Medical Image Analysis}\label{sec_gaze_med_IA}
Unlike optical images, medical images are often challenging to interpret, requiring solid biological knowledge for accurate diagnostics. To enhance the robustness of medical image analysis networks, human experts' gaze signals are frequently integrated as guidance. Cai~\emph{et al.}~\cite{cai2020spatio} proposed Temporal SonoEyeNet, which predicts sonographers' visual attention and ultrasound standard planes, demonstrating that understanding visual attention complements standard plane detection. Similarly, Wang~\emph{et al.}~\cite{wang2022follow} used class activation maps~\cite{zhou2016learning} to align network attention with human gaze.
In another application, Men~\emph{et al.}~\cite{men2023gaze} leveraged gaze tracking to improve probe movement prediction for obstetric standard plane navigation, while Alsharid~\emph{et al.}~\cite{alsharid2022gaze} showed its benefits in ultrasound video captioning. Most of these works aim to align networks with human attention to enhance reasoning. Beyond implicit guidance, gaze signals also serve as an intuitive tool for human-machine interaction. For instance, Khosravan~\emph{et al.}~\cite{khosravan2019collaborative} developed a collaborative lesion segmentation system that analyzes and filters doctors' gaze data to improve segmentation and reduce false positives.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%