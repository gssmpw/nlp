%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, journal, twoside]{IEEEtran}  % Comment this line out if you need a4paper
% \documentclass[letterpaper, 10 pt, conference]{ieeeconf}

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command
\pdfminorversion=4
% \overrideIEEEmargins  
% Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphicx} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage[usenames, dvipsnames]{color}
\usepackage{siunitx}
\usepackage{cases}
\usepackage{lipsum}
\usepackage{color}
\usepackage{cite}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{diagbox} 
\usepackage{tabu}
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{empheq}
\usepackage{threeparttable}
\usepackage{tabularx}
\usepackage{hyperref}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\todo}[1]{\textcolor{red}{\textbf{\emph{TODO:~{#1}}}}}
\newcommand{\checkme}[1]{\textcolor{blue}{\textbf{\emph{{#1}}}}}
\newcommand{\revision}[1]{\textcolor{black}{#1}} %for displaying blue texts
\newcommand{\final}[1]{\textcolor{black}{#1}} %for displaying blue texts
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
% \linespread{0.97}

% \title{
% Automatic Normal Positioning of Robotic Ultrasound Probe based only on Confidence Map Optimization and Force Measurement
% }

\title{
Gaze-Guided Robotic Vascular Ultrasound Leveraging Human Intention Estimation
% Stabilized Gaze-Guided Robotic Vascular Ultrasound
% Gaze-Guided Robotic Vascular Ultrasound
% Gaze-Guided Robotic Ultrasound System for Vascular Screening
}



\author{Yuan Bi$^{1, 2}$, Yang Su$^{1}$, Nassir Navab$^{1, 2}$, \textit{Fellow, IEEE}, and Zhongliang Jiang$^{1, 2}$ % <-this % stops a space

\thanks{Manuscript received: September 10, 2024; Revised: December 7, 2024; Accepted: January 15, 2025.}%Use only for final RAL version
\thanks{This paper was recommended for publication by Editor Jessica Burgner-Kahrs upon evaluation of the Associate Editor and Reviewers' comments.}
% \thanks{$^{*}$ Authors are with equal contributions.}
\thanks{This work was partially supported by SINO-German Mobility Project (M-0221). (Corresponding Author: Zhongliang Jiang, zl.jiang@tum.de)}
\thanks{$^{1}$Y. Bi, Y. Su, N. Navab, and Z. Jiang are with the Chair for Computer-Aided Medical Procedures and Augmented Reality, Technical University of Munich, Boltzmannstr. 3, 85748 Garching, Germany}%% 
\thanks{$^{2}$Y. Bi, N. Navab, and Z. Jiang are with the Munich Center for Machine Learning, Arcisstraße 21, 80333 M\"unchen, Germany}
% \thanks{The authors would like to thank Dr. Reza Ghotbi from vascular surgery department of Helios Klinikum M\"unchen West and Mr. Wei Zhang from Klinikum rechts der Isar for their feedback and discussions.}
\thanks{Digital Object Identifier (DOI): see top of this page.}
}


\markboth{IEEE Robotics and Automation Letters. Preprint Version. Accepted JANUARY, 2025}
{Bi \MakeLowercase{\textit{et al.}}: Gaze-Guided Robotic Vascular Ultrasound Leveraging Human Intention Estimation} 




\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Medical ultrasound has been widely used to examine vascular structure in modern clinical practice. However, traditional ultrasound examination often faces challenges related to inter- and intra-operator variation. The robotic ultrasound system (RUSS) appears as a potential solution for such challenges because of its superiority in stability and reproducibility.
Given the complex anatomy of human vasculature, multiple vessels often appear in ultrasound images, or a single vessel bifurcates into branches, complicating the examination process.
To tackle this challenge, this work presents a gaze-guided RUSS for vascular applications. A gaze tracker captures the eye movements of the operator. The extracted gaze signal guides the RUSS to follow the correct vessel when it bifurcates. Additionally, a gaze-guided segmentation network is proposed to enhance segmentation robustness by exploiting gaze information.
However, gaze signals are often noisy, requiring interpretation to accurately discern the operator's true intentions. To this end, this study proposes a stabilization module to process raw gaze data. The inferred attention heatmap is utilized as a region proposal to aid segmentation and serve as a trigger signal when the operator needs to adjust the scanning target, such as when a bifurcation appears. To ensure appropriate contact between the probe and surface during scanning, an automatic ultrasound confidence-based orientation correction method is developed.
In experiments, we demonstrated the efficiency of the proposed gaze-guided segmentation pipeline by comparing it with other methods. Besides, the performance of the proposed gaze-guided RUSS was also validated as a whole on a realistic arm phantom with an uneven surface.
% A confidence-based probe orientation correction method is proposed to control the robot establish proper contact with the scanning surface
\end{abstract}

% \markboth{IEEE Robotics and Automation Letters. Preprint Version. Accepted April, 2022}
% {Bi \MakeLowercase{\textit{et al.}}: VesNet-RL: Simulation-based Reinforcement Learning for Real-World US Probe Navigation} 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{1em}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{IEEEkeywords}
Robotic ultrasound, Gaze tracker, Gaze-guided system, Ultrasound segmentation
\end{IEEEkeywords}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bstctlcite{IEEEexample:BSTcontrol}
% \input{introduction}
% \input{text}    
\input{text}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section*{ACKNOWLEDGMENT}
%This work partially received funding by the project IOTMA supported by the central innovation program for SMEs (ZIM), the BayMED project 5G-MedServices funded by the Bavarian state, as well as from the European Union’s Horizon 2020 research and innovation program EDEN2020 under Grant Agreement No. 688279. The motion capture data used in this project was obtained from mocap.cs.cmu.edu which was created with funding from NSF EIA-0196217. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used here. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}




\end{document}