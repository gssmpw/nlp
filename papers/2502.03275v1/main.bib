@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@article{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{deng2024explicit,
  title={From explicit cot to implicit cot: Learning to internalize cot step by step},
  author={Deng, Yuntian and Choi, Yejin and Shieber, Stuart},
  journal={arXiv preprint arXiv:2405.14838},
  year={2024}
}

@article{goyal2023think,
  title={Think before you speak: Training language models with pause tokens},
  author={Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh},
  journal={arXiv preprint arXiv:2310.02226},
  year={2023}
}

@article{jiang2023h,
  title={H-GAP: Humanoid Control with a Generalist Planner},
  author={Jiang, Zhengyao and Xu, Yingchen and Wagener, Nolan and Luo, Yicheng and Janner, Michael and Grefenstette, Edward and Rockt{\"a}schel, Tim and Tian, Yuandong},
  journal={arXiv preprint arXiv:2312.02682},
  year={2023}
}

@article{jiang2022efficient,
  title={Efficient planning in a compact latent action space},
  author={Jiang, Zhengyao and Zhang, Tianjun and Janner, Michael and Li, Yueying and Rockt{\"a}schel, Tim and Grefenstette, Edward and Tian, Yuandong},
  journal={arXiv preprint arXiv:2208.10291},
  year={2022}
}

@article{hao2024training,
  title={Training Large Language Models to Reason in a Continuous Latent Space},
  author={Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong},
  journal={arXiv preprint arXiv:2412.06769},
  year={2024}
}

@article{su2024dualformer,
  title={Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces},
  author={Su, DiJia and Sukhbaatar, Sainbayar and Rabbat, Michael and Tian, Yuandong and Zheng, Qinqing},
  journal={arXiv preprint arXiv:2410.09918},
  year={2024}
}

@article{saparov2022language,
  title={Language models are greedy reasoners: A systematic formal analysis of chain-of-thought},
  author={Saparov, Abulhair and He, He},
  journal={arXiv preprint arXiv:2210.01240},
  year={2022}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{yu2023metamath,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

@article{tang2024mathscale,
  title={Mathscale: Scaling instruction tuning for mathematical reasoning},
  author={Tang, Zhengyang and Zhang, Xingxing and Wang, Benyou and Wei, Furu},
  journal={arXiv preprint arXiv:2403.02884},
  year={2024}
}

@article{saxton2019analysing,
  title={Analysing mathematical reasoning abilities of neural models},
  author={Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:1904.01557},
  year={2019}
}

@inproceedings{chen2023theoremqa,
  title={Theoremqa: A theorem-driven question answering dataset},
  author={Chen, Wenhu and Yin, Ming and Ku, Max and Lu, Pan and Wan, Yixin and Ma, Xueguang and Xu, Jianyu and Wang, Xinyi and Xia, Tony},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={7889--7901},
  year={2023}
}

@article{he2024olympiadbench,
  title={Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems},
  author={He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen Leng and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.14008},
  year={2024}
}

@article{tong2024dart,
  title={Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving},
  author={Tong, Yuxuan and Zhang, Xiwen and Wang, Rui and Wu, Ruidong and He, Junxian},
  journal={arXiv preprint arXiv:2407.13690},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}


@inproceedings{
lehnert2024beyond,
title={Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping},
author={Lucas Lehnert and Sainbayar Sukhbaatar and DiJia Su and Qinqing Zheng and Paul McVay and Michael Rabbat and Yuandong Tian},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=SGoVIC0u0f}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{cot1,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{cot2,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}


@article{ss1,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{prompt1,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}


@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}

@article{deng2023implicit,
  title={Implicit chain of thought reasoning via knowledge distillation},
  author={Deng, Yuntian and Prasad, Kiran and Fernandez, Roland and Smolensky, Paul and Chaudhary, Vishrav and Shieber, Stuart},
  journal={arXiv preprint arXiv:2311.01460},
  year={2023}
}

@article{wang2024chainofthoughtreasoningprompting,
      title={Chain-of-Thought Reasoning Without Prompting}, 
      author={Xuezhi Wang and Denny Zhou},
      year={2024},
      eprint={2402.10200},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.10200}, 
}

@article{feng2024towards,
  title={Towards revealing the mystery behind chain of thought: a theoretical perspective},
  author={Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{li2024chainthoughtempowerstransformers,
      title={Chain of Thought Empowers Transformers to Solve Inherently Serial Problems}, 
      author={Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma},
      year={2024},
      eprint={2402.12875},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12875}, 
}


@article{cot_dynamics_2,
  title={From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency},
  author={Wen, Kaiyue and Zhang, Huaqing and Lin, Hongzhou and Zhang, Jingzhao},
  journal={arXiv preprint arXiv:2410.05459},
  year={2024}
}

@article{cot_dynamics_1,
  title={Towards a Theoretical Understanding of the'Reversal Curse'via Training Dynamics},
  author={Zhu, Hanlin and Huang, Baihe and Zhang, Shaolun and Jordan, Michael and Jiao, Jiantao and Tian, Yuandong and Russell, Stuart},
  journal={arXiv preprint arXiv:2405.04669},
  year={2024}
}


@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}

@article{gandhi2024stream,
  title={Stream of Search (SoS): Learning to Search in Language},
  author={Gandhi, Kanishk and Lee, Denise and Grand, Gabriel and Liu, Muxin and Cheng, Winson and Sharma, Archit and Goodman, Noah D},
  journal={arXiv preprint arXiv:2404.03683},
  year={2024}
}

@article{yue2023mammoth,
  title={Mammoth: Building math generalist models through hybrid instruction tuning},
  author={Yue, Xiang and Qu, Xingwei and Zhang, Ge and Fu, Yao and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
  journal={arXiv preprint arXiv:2309.05653},
  year={2023}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{zebralogic2024,
    title={ZebraLogic: Benchmarking the Logical Reasoning Ability of Language Models},
    author={Bill Yuchen Lin and Ronan Le Bras and Yejin Choi},
    url={https://huggingface.co/spaces/allenai/ZebraLogic},
    year={2024}
}

@article{dziri2024faith,
  title={Faith and fate: Limits of transformers on compositionality},
  author={Nouha Dziri and Ximing Lu and Melanie Sclar and Xiang Lorraine Li and Liwei Jian and Bill Yuchen Lin and Peter West and Chandra Bhagavatula and Ronan Le Bras and Jena D. Hwang and Soumya Sanyal and Sean Welleck and Xiang Ren and Allyson Ettinger and Za{\"i}d Harchaoui and Yejin Choi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{hart1968formal,
  title={A formal basis for the heuristic determination of minimum cost paths},
  author={Hart, Peter E and Nilsson, Nils J and Raphael, Bertram},
  journal={IEEE transactions on Systems Science and Cybernetics},
  volume={4},
  number={2},
  pages={100--107},
  year={1968},
  publisher={IEEE}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{2020t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}


@article{pfau2024let,
  title={Let's Think Dot by Dot: Hidden Computation in Transformer Language Models},
  author={Pfau, Jacob and Merrill, William and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2404.15758},
  year={2024}
}

@article{deng2024explicitcotimplicitcot,
      title={From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step}, 
      author={Yuntian Deng and Yejin Choi and Stuart Shieber},
      year={2024},
      eprint={2405.14838},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.14838}, 
}

@article{deng2023implicitchainthoughtreasoning,
      title={Implicit Chain of Thought Reasoning via Knowledge Distillation}, 
      author={Yuntian Deng and Kiran Prasad and Roland Fernandez and Paul Smolensky and Vishrav Chaudhary and Stuart Shieber},
      year={2023},
      eprint={2311.01460},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.01460}, 
}

@article{pagnoni2024bytelatenttransformerpatches,
      title={Byte Latent Transformer: Patches Scale Better Than Tokens}, 
      author={Artidoro Pagnoni and Ram Pasunuru and Pedro Rodriguez and John Nguyen and Benjamin Muller and Margaret Li and Chunting Zhou and Lili Yu and Jason Weston and Luke Zettlemoyer and Gargi Ghosh and Mike Lewis and Ari Holtzman and Srinivasan Iyer},
      year={2024},
      eprint={2412.09871},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.09871}, 
}

@article{lcmteam2024largeconceptmodelslanguage,
      title={Large Concept Models: Language Modeling in a Sentence Representation Space}, 
      author={LCM team and Loïc Barrault and Paul-Ambroise Duquenne and Maha Elbayad and Artyom Kozhevnikov and Belen Alastruey and Pierre Andrews and Mariano Coria and Guillaume Couairon and Marta R. Costa-jussà and David Dale and Hady Elsahar and Kevin Heffernan and João Maria Janeiro and Tuan Tran and Christophe Ropers and Eduardo Sánchez and Robin San Roman and Alexandre Mourachko and Safiyyah Saleem and Holger Schwenk},
      year={2024},
      eprint={2412.08821},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.08821}, 
}

@article{jiang2023efficientplanningcompactlatent,
      title={Efficient Planning in a Compact Latent Action Space}, 
      author={Zhengyao Jiang and Tianjun Zhang and Michael Janner and Yueying Li and Tim Rocktäschel and Edward Grefenstette and Yuandong Tian},
      year={2023},
      eprint={2208.10291},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.10291}, 
}


@article{zhang2024evaluatingperformancelargelanguage,
      title={Evaluating the Performance of Large Language Models on GAOKAO Benchmark}, 
      author={Xiaotian Zhang and Chunyang Li and Yi Zong and Zhengyu Ying and Liang He and Xipeng Qiu},
      year={2024},
      eprint={2305.12474},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.12474}, 
}


@inproceedings{math_dd,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}
}

@article{gsm8k_dd,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@article{yu2024distilling,
  title={Distilling system 2 into system 1},
  author={Yu, Ping and Xu, Jing and Weston, Jason and Kulikov, Ilia},
  journal={arXiv preprint arXiv:2407.06023},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{azerbayev2023llemma,
  title={Llemma: An open language model for mathematics},
  author={Azerbayev, Zhangir and Schoelkopf, Hailey and Paster, Keiran and Santos, Marco Dos and McAleer, Stephen and Jiang, Albert Q and Deng, Jia and Biderman, Stella and Welleck, Sean},
  journal={arXiv preprint arXiv:2310.10631},
  year={2023}
}

@article{kim2023cot,
  title={The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning},
  author={Kim, Seungone and Joo, Se June and Kim, Doyoung and Jang, Joel and Ye, Seonghyeon and Shin, Jamin and Seo, Minjoon},
  journal={arXiv preprint arXiv:2305.14045},
  year={2023}
}


@article{kvcache,
      title={Deliberation in Latent Space via Differentiable Cache Augmentation}, 
      author={Luyang Liu and Jonas Pfeiffer and Jiaxing Wu and Jun Xie and Arthur Szlam},
      year={2024},
      eprint={2412.17747},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.17747}, 
}

@inproceedings{zeng2024mr,
  title={MR-ben: A meta-reasoning benchmark for evaluating system-2 thinking in LLMs},
  author={Zeng, Zhongshen and Liu, Yinhong and Wan, Yingjia and Li, Jingyao and Chen, Pengguang and Dai, Jianbo and Yao, Yuxuan and Xu, Rongwu and Qi, Zehan and Zhao, Wanru and others},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}


@article{barrault2024large,
  title={Large Concept Models: Language Modeling in a Sentence Representation Space},
  author={Barrault, Lo{\"\i}c and Duquenne, Paul-Ambroise and Elbayad, Maha and Kozhevnikov, Artyom and Alastruey, Belen and Andrews, Pierre and Coria, Mariano and Couairon, Guillaume and Costa-juss{\`a}, Marta R and Dale, David and others},
  journal={arXiv e-prints},
  pages={arXiv--2412},
  year={2024}
}

@misc{lozhkov2024finemath,  
    author       = { Lozhkov, Anton and Ben Allal, Loubna and Bakouch, Elie and von Werra, Leandro and Wolf, Thomas },  
    title        = { FineMath: the Finest Collection of Mathematical Content }, 
    year         = 2024,  
    url          = { https://huggingface.co/datasets/HuggingFaceTB/finemath },  
    doi          = { 10.57967/hf/3847 },
    publisher    = { Hugging Face }
}

@inproceedings{jiang2024forward,
  title={Forward-backward reasoning in large language models for mathematical verification},
  author={Jiang, Weisen and Shi, Han and Yu, Longhui and Liu, Zhengying and Zhang, Yu and Li, Zhenguo and Kwok, James},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={6647--6661},
  year={2024}
}