\section{Related Work}
\label{sec:related}

\paragraph{Explicit Chain-of-Thought Prompting.} The first line of work in Chain-of-Thought (CoT) use the traditional chain of prompt in text tokens ____.  Research works demonstrated that by adding few-shot examples to the input prompt or even zero-shot, the model can perform better in question answering ____.
To further improve the model reasoning performance, there has been research effort into prompting with self-consistency ____. Here the model is prompted to generate multiple responses and select the best one based on majority voting. On the other hand, research has shown that top-$k$ alternative tokens in the beginning of the prompt can also improve the model's reasoning capability ____. \looseness=-1
On top of these empirical results, there has been research on theoretical understanding of why CoT improves the model's performance through the lens of expressivity____ or training dynamics____. In a nutshell, CoT improves the model's effective depth because the generated output is being fed back to the original input. CoT is also important for LLMs to perform multi-hop reasoning according to the analysis of training dynamics____.





   

\paragraph{Learning with CoT Data.} In addition to the success of CoT prompting,
an emerging line of works have explored training LLMs on data with high-quality reasoning traces, for example, the works of ____.
There is also a surge of interest in synthesizing datasets with diverse intermediate steps for solving problems in various domains, see, e.g.,  
the works of ____. ____ also theoretically studies how training with reasoning trace can improve the sample complexity of certain tasks.




\paragraph{LLM Reasoning in Latent Space.}
There has been research investigating LLM reasoning in the latent space. ____ have proposed to use the last hidden state of a language model as the next input embeddings, allowing the model to continue reasoning within a continuous latent space. The authors show that this approach effectively captures multiple reasoning paths simultaneously, mimicking a breadth-first-search strategy. ____ proposes to insert learnable pause tokens into the original text, in order to delay the generation. As a result, the model can leverage additional computation before providing the final answer. Parallel to this, ____ have explored filler tokens, which are used to solve computational tasks that are otherwise unattainable without intermediate token generation. 
In addition, ____ propose a latent coprocessor method that operates on the transformer's key-value cache to improve the LLM performance.  
Nevertheless, none of these methods have shown good performance when integrated into modern-sized LLMs and tested on real-world LLM datasets instead of synthetic ones.
Orthogonal to these works, ____ proposes a tokenization-free architecture that encodes input bytes into continuous patch representations, which is then used to train a latent Transformer, and ____ perform autoregressive sentence prediction in an embedding space. While these two works both leverage continuous latent spaces, our work focuses on the direct use of discrete latent tokens.