[
  {
    "index": 0,
    "papers": [
      {
        "key": "cot1",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      },
      {
        "key": "cot2",
        "author": "Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others",
        "title": "Show your work: Scratchpads for intermediate computation with language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "prompt1",
        "author": "Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W",
        "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks"
      },
      {
        "key": "kojima2022large",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large language models are zero-shot reasoners"
      },
      {
        "key": "chung2024scaling",
        "author": "Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others",
        "title": "Scaling instruction-finetuned language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ss1",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2024chainofthoughtreasoningprompting",
        "author": "Xuezhi Wang and Denny Zhou",
        "title": "Chain-of-Thought Reasoning Without Prompting"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "feng2024towards",
        "author": "Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei",
        "title": "Towards revealing the mystery behind chain of thought: a theoretical perspective"
      },
      {
        "key": "li2024chainthoughtempowerstransformers",
        "author": "Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma",
        "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "cot_dynamics_1",
        "author": "Zhu, Hanlin and Huang, Baihe and Zhang, Shaolun and Jordan, Michael and Jiao, Jiantao and Tian, Yuandong and Russell, Stuart",
        "title": "Towards a Theoretical Understanding of the'Reversal Curse'via Training Dynamics"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "cot_dynamics_1",
        "author": "Zhu, Hanlin and Huang, Baihe and Zhang, Shaolun and Jordan, Michael and Jiao, Jiantao and Tian, Yuandong and Russell, Stuart",
        "title": "Towards a Theoretical Understanding of the'Reversal Curse'via Training Dynamics"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "nye2021show",
        "author": "Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others",
        "title": "Show your work: Scratchpads for intermediate computation with language models"
      },
      {
        "key": "azerbayev2023llemma",
        "author": "Azerbayev, Zhangir and Schoelkopf, Hailey and Paster, Keiran and Santos, Marco Dos and McAleer, Stephen and Jiang, Albert Q and Deng, Jia and Biderman, Stella and Welleck, Sean",
        "title": "Llemma: An open language model for mathematics"
      },
      {
        "key": "lehnert2024beyond",
        "author": "Lucas Lehnert and Sainbayar Sukhbaatar and DiJia Su and Qinqing Zheng and Paul McVay and Michael Rabbat and Yuandong Tian",
        "title": "Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping"
      },
      {
        "key": "su2024dualformer",
        "author": "Su, DiJia and Sukhbaatar, Sainbayar and Rabbat, Michael and Tian, Yuandong and Zheng, Qinqing",
        "title": "Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces"
      },
      {
        "key": "yu2024distilling",
        "author": "Yu, Ping and Xu, Jing and Weston, Jason and Kulikov, Ilia",
        "title": "Distilling system 2 into system 1"
      },
      {
        "key": "yang2024qwen2",
        "author": "Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others",
        "title": "Qwen2. 5 Technical Report"
      },
      {
        "key": "deng2023implicit",
        "author": "Deng, Yuntian and Prasad, Kiran and Fernandez, Roland and Smolensky, Paul and Chaudhary, Vishrav and Shieber, Stuart",
        "title": "Implicit chain of thought reasoning via knowledge distillation"
      },
      {
        "key": "deng2024explicit",
        "author": "Deng, Yuntian and Choi, Yejin and Shieber, Stuart",
        "title": "From explicit cot to implicit cot: Learning to internalize cot step by step"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kim2023cot",
        "author": "Kim, Seungone and Joo, Se June and Kim, Doyoung and Jang, Joel and Ye, Seonghyeon and Shin, Jamin and Seo, Minjoon",
        "title": "The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning"
      },
      {
        "key": "tong2024dart",
        "author": "Tong, Yuxuan and Zhang, Xiwen and Wang, Rui and Wu, Ruidong and He, Junxian",
        "title": "Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving"
      },
      {
        "key": "yu2023metamath",
        "author": "Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang",
        "title": "Metamath: Bootstrap your own mathematical questions for large language models"
      },
      {
        "key": "yue2023mammoth",
        "author": "Yue, Xiang and Qu, Xingwei and Zhang, Ge and Fu, Yao and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu",
        "title": "Mammoth: Building math generalist models through hybrid instruction tuning"
      },
      {
        "key": "lozhkov2024finemath",
        "author": " Lozhkov, Anton and Ben Allal, Loubna and Bakouch, Elie and von Werra, Leandro and Wolf, Thomas ",
        "title": " FineMath: the Finest Collection of Mathematical Content "
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "cot_dynamics_2",
        "author": "Wen, Kaiyue and Zhang, Huaqing and Lin, Hongzhou and Zhang, Jingzhao",
        "title": "From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "hao2024training",
        "author": "Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong",
        "title": "Training Large Language Models to Reason in a Continuous Latent Space"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "goyal2023think",
        "author": "Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh",
        "title": "Think before you speak: Training language models with pause tokens"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "pfau2024let",
        "author": "Pfau, Jacob and Merrill, William and Bowman, Samuel R",
        "title": "Let's Think Dot by Dot: Hidden Computation in Transformer Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "kvcache",
        "author": "Luyang Liu and Jonas Pfeiffer and Jiaxing Wu and Jun Xie and Arthur Szlam",
        "title": "Deliberation in Latent Space via Differentiable Cache Augmentation"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "pagnoni2024bytelatenttransformerpatches",
        "author": "Artidoro Pagnoni and Ram Pasunuru and Pedro Rodriguez and John Nguyen and Benjamin Muller and Margaret Li and Chunting Zhou and Lili Yu and Jason Weston and Luke Zettlemoyer and Gargi Ghosh and Mike Lewis and Ari Holtzman and Srinivasan Iyer",
        "title": "Byte Latent Transformer: Patches Scale Better Than Tokens"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "barrault2024large",
        "author": "Barrault, Lo{\\\"\\i}c and Duquenne, Paul-Ambroise and Elbayad, Maha and Kozhevnikov, Artyom and Alastruey, Belen and Andrews, Pierre and Coria, Mariano and Couairon, Guillaume and Costa-juss{\\`a}, Marta R and Dale, David and others",
        "title": "Large Concept Models: Language Modeling in a Sentence Representation Space"
      }
    ]
  }
]