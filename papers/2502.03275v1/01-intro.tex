\section{Introduction}

Reasoning capabilities are increasingly recognized as a critical component of Artificial General Intelligence (AGI) systems. Recent research has demonstrated that Large Language Models (LLMs) can exhibit sophisticated reasoning and planning abilities using chain-of-thought (CoT) methodologies, including prompting LLMs 
with examples where complex problems are broken down into explicit reasoning steps~\cite{wei2022chain, chen2022program, yao2024tree}.
More recently, a number of studies have further shown that when models are trained to articulate the intermediate steps of a reasoning process~\cite{nye2021show, lehnert2024beyond}, they achieve significantly higher accuracy. The effectiveness of this approach has been demonstrated across multiple domains, including mathematical problem-solving~\cite{yue2023mammoth, gandhi2024stream, yu2023metamath, tong2024dart}, logical inference~\cite{zebralogic2024, dziri2024faith}, multistep planning tasks~\cite{lehnert2024beyond, su2024dualformer}, etc. 

However, training with explicit reasoning traces in text space comes with notable computational costs~\cite{deng2023implicit, deng2024explicit}, as the models must process lengthy input sequences. In fact, much of the text serves primarily to maintain linguistic coherence, rather than conveying core reasoning information. Several works have attempted to mitigate this issue. For example,
\citet{hao2024training} investigate reasoning in continuous latent space as a means of compressing the reasoning trace, and
\citet{deng2024explicit} explore internalizing the intermediate steps through iterative CoT eliminations, see~\Cref{sec:related} for more examples. Nonetheless, these approaches rely on multi-stage training procedures that resemble curriculum learning,  which still incur significant computational costs, and their final performances fall behind models trained with complete reasoning traces.

To tackle this challenge, we propose to use discrete latent tokens to abstract the initial steps of the reasoning traces. These latent tokens, obtained through a vector-quantized variational autoencoder (VQ-VAE), provide a compressed representation of the reasoning process by condensing surface-level details. 
More precisely, we replace the text tokens with their corresponding latent abstractions from left to right until a pre-set location, leaving the remaining tokens unchanged. We then fine-tune LLMs with reasoning traces with such \emph{assorted tokens}, allowing the models to learn from both abstract representations of the thinking process and detailed textual descriptions. 
One technical challenge posed for the fine-tuning is that the vocabulary is now extended and contains unseen latent tokens.
To facilitate quick adaptation to those new tokens, we employ a \emph{randomized replacement} strategy: during training, we randomly vary the number of text tokens being substituted by latent tokens for each sample. Our experiments confirm that this simple strategy leads to straightforward accommodation of unseen latent tokens. %, without curriculum learning.

We conduct a comprehensive evaluation of our approach on a diverse range of benchmarks spanning multiple domains.
Specifically, we assess its performance on multistep planning tasks (Keys-Finding Maze) and logical reasoning benchmarks (ProntoQA~\cite{saparov2022language}, ProsQA~\cite{hao2024training}) for training T5 or GPT-2 models from scratch.
In addition, we fine-tune different sizes of LLama-3.1 and LLama-3.2 models using our approach and evaluate them
on a number of mathematical reasoning benchmarks, including GSM8K~\cite{cobbe2021training}, Math~\cite{hendrycksmath2021}, and OlympiadBench-Math~\cite{he2024olympiadbench}, see Section~\ref{sec:expr} for more details. Across all these tasks and model architectures, our models consistently outperform baseline models trained with text-only reasoning traces, demonstrating the effectiveness of compressing the reasoning process with assorted tokens.