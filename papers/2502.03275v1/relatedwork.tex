\section{Related Work}
\label{sec:related}

\paragraph{Explicit Chain-of-Thought Prompting.} The first line of work in Chain-of-Thought (CoT) use the traditional chain of prompt in text tokens \citep{cot1, cot2}.  Research works demonstrated that by adding few-shot examples to the input prompt or even zero-shot, the model can perform better in question answering \citep{prompt1, kojima2022large, chung2024scaling}.
To further improve the model reasoning performance, there has been research effort into prompting with self-consistency \citep{ss1}. Here the model is prompted to generate multiple responses and select the best one based on majority voting. On the other hand, research has shown that top-$k$ alternative tokens in the beginning of the prompt can also improve the model's reasoning capability \cite{wang2024chainofthoughtreasoningprompting}. \looseness=-1
On top of these empirical results, there has been research on theoretical understanding of why CoT improves the model's performance through the lens of expressivity~\citep{feng2024towards, li2024chainthoughtempowerstransformers} or training dynamics~\citep{cot_dynamics_1}. In a nutshell, CoT improves the model's effective depth because the generated output is being fed back to the original input. CoT is also important for LLMs to perform multi-hop reasoning according to the analysis of training dynamics~\citep{cot_dynamics_1}.





   

\paragraph{Learning with CoT Data.} In addition to the success of CoT prompting,
an emerging line of works have explored training LLMs on data with high-quality reasoning traces, for example, the works of \citet{nye2021show, azerbayev2023llemma, lehnert2024beyond, su2024dualformer, yu2024distilling, yang2024qwen2, deng2023implicit, deng2024explicit}.
There is also a surge of interest in synthesizing datasets with diverse intermediate steps for solving problems in various domains, see, e.g.,  
the works of \citet{kim2023cot, tong2024dart, yu2023metamath, yue2023mammoth, lozhkov2024finemath}. \citet{cot_dynamics_2} also theoretically studies how training with reasoning trace can improve the sample complexity of certain tasks.




\paragraph{LLM Reasoning in Latent Space.}
There has been research investigating LLM reasoning in the latent space. \citet{hao2024training} have proposed to use the last hidden state of a language model as the next input embeddings, allowing the model to continue reasoning within a continuous latent space. The authors show that this approach effectively captures multiple reasoning paths simultaneously, mimicking a breadth-first-search strategy. \citet{goyal2023think} proposes to insert learnable pause tokens into the original text, in order to delay the generation. As a result, the model can leverage additional computation before providing the final answer. Parallel to this, \citet{pfau2024let} have explored filler tokens, which are used to solve computational tasks that are otherwise unattainable without intermediate token generation. 
In addition, \citet{kvcache} propose a latent coprocessor method that operates on the transformer's key-value cache to improve the LLM performance.  
Nevertheless, none of these methods have shown good performance when integrated into modern-sized LLMs and tested on real-world LLM datasets instead of synthetic ones.
Orthogonal to these works, \citet{pagnoni2024bytelatenttransformerpatches} proposes a tokenization-free architecture that encodes input bytes into continuous patch representations, which is then used to train a latent Transformer, and \citet{barrault2024large} perform autoregressive sentence prediction in an embedding space. While these two works both leverage continuous latent spaces, our work focuses on the direct use of discrete latent tokens.