\section{Experiments}
\label{sec:expr}
We empirically evaluate our approach on two categories of benchmarks: 
\vspace{-5pt}
\begin{enumerate}\itemsep0pt
    \item[\textbf{(1)}] Synthetic datasets including the Keys-Finding Maze, ProntoQA~\cite{saparov2022language}, and ProsQA~\cite{hao2024training}, where we pretrain T5 or GPT-2 models from scratch using the method in \Cref{sec:algo};
    \item[\textbf{(2)}] Real-world mathematic reasoning problems, where we fine-tune Llama models~\cite{dubey2024llama} on the MetaMathQA~\cite{yu2023metamath} or the Dart-MATH~\cite{tong2024dart} dataset, and then test on in-domain datasets Math and GSM-8K, along with out-of-domain datasets including Fresh-Gaokao-Math-2023, DeepMind-Math, College-Math, OlympiaBench-Math, and TheoremQA. 
\end{enumerate}
The detailed setup is introduced in \Cref{sec:expr_benchmark}.

We compare our approach to the following baselines:
\vspace*{-10pt}
\begin{enumerate}[leftmargin=0pt]\itemsep0pt
    \item[] \textbf{Sol-Only}:  the model is trained with samples that only contains questions and solutions, without any reasoning steps;
    \item[] \textbf{CoT}: the model is trained with samples with complete CoT tokens; \looseness=-1
    \item[] \textbf{iCoT}~\citep{deng2024explicit}: a method that utilizes curriculum learning to gradually eliminate the need of CoT tokens in reasoning;
    \item[] \textbf{Pause Token}~\citep{goyal2023think}:  a method that injects a learnable \texttt{pause} token into the sample during training, in order to offer extra computation before giving out the final answer.

\end{enumerate}



\subsection{Benchmarks}
\label{sec:expr_benchmark}
\subsubsection{Synthetic Benchmarks}

\textbf{Keys-Finding Maze} is a complex navigation environment designed to evaluate an agent's planning capabilities. The agent is randomly positioned within a maze comprising 4 $3 \times 3$ interconnected rooms, with the objective of reaching a randomly placed goal destination. To successfully reach the destination, the agent must collect keys (designated with green, red, and blue colors) that correspond to matching colored doors. These keys are randomly distributed among the rooms, requiring the agent to develop sophisticated planning strategies for key acquisition and door traversal. The agent is only allowed to take one key at a time. This environment poses a substantial cognitive challenge, as the agent must identify which keys are necessary for reaching the destination, and optimize the order of key collection and door unlocking to establish the most efficient path to the goal. Following \citet{lehnert2024beyond,su2024dualformer}, we generate intermediate search traces using the nondeterministic A* algorithm~\cite{hart1968formal}. The dataset contains 100k training samples. See \Cref{app:maze} for more information and graphical illustrations.

\textbf{ProntoQA}~\cite{saparov2022language} is a dataset consists of $9000$ logical reasoning problems derived from ontologies - formal representations of relationships between concepts. Each problem in the dataset is constructed to have exactly one correct proof or reasoning path. One distinctive feature of this dataset is its consistent grammatical and logical structure, which enables researchers to systematically analyze and evaluate how LLMs approach reasoning tasks. 

\textbf{ProsQA}~\cite{hao2024training} is a more difficult benchmark building on top of ProntoQA. It contains 17,886 logical problems curated by randomly generated directed acyclic graphs. %The advantage of this dataset is that it
It has larger size of distracting reasoning paths in the ontology, and thus require more complex reasoning and planning capabilities.
% to solve it.

\subsubsection{Mathematical Reasoning}
We fine-tune pretrained LLMs using the MetaMathQA~\cite{yu2023metamath} or the Dart-MATH~\cite{tong2024dart} dataset. 
MetaMathQA is a curated dataset that augments the existing \texttt{Math} ~\cite{math_dd} and \texttt{GSM8K} ~\cite{gsm8k_dd} datasets by various ways of question bootstrapping,
such as (i) rephrasing the question and generating the reasoning path; (ii) generating backward questions,  self-verification questions, FOBAR questions~\cite{jiang2024forward}, etc. This dataset contains 395k samples in total, where 155k samples are bootstrapped from \texttt{Math} and the remaining 240k come from \texttt{GSM8K}. We rerun the MetaMath data pipeline by using Llama-3.1-405B-Inst to generate the response. 
Dart-MATH~\cite{tong2024dart} also synthesizes responses for questions in \texttt{Math} and \texttt{GSM8K}, with the focus on difficult questions via difficulty-aware rejection tuning.
For evaluation, we test the models on the original \texttt{Math} and \texttt{GSM8K} datasets, which are in-domain,
and also the following out-of-domain benchmarks:
\vspace{-5pt}
\begin{itemize}[leftmargin=*]\itemsep0pt
    \item  \textbf{College-Math}~\cite{tang2024mathscale}
consists of 2818 college-level math problems taken from 9 textbooks. These problems cover over 7 different areas such as linear algebra, differential equations, and so on. They are designed to evaluate how well the language model can handle complicated mathematical reasoning problems in different field of study.

    \item  \textbf{DeepMind-Math}~\cite{saxton2019analysing} consists of 1000 problems based on the national school math curriculum for students up to 16 years old. It examines the basic mathematics and reasoning skills across different topics.

    \item  \textbf{OlympiaBench-Math}~\cite{he2024olympiadbench} 
is a text-only English subset of Olympiad-Bench focusing on advanced level mathematical reasoning. It
contains 675 highly difficult math problems from competitions. 

    \item  \textbf{TheoremQA}~\cite{chen2023theoremqa} contains 800 problems focuses on solving problems in STEM fields (such as math, physics, and engineering) using mathematical theorems.


    \item \textbf{Fresh-Gaokao-Math-2023} ~\cite{tang2024mathscale} contains 30 math questions coming from  Gaokao, or the National College Entrance Examination, which is a national standardized test that plays a crucial role in the college admissions process.
\end{itemize}

\subsection{Main Results}
\label{sec:expr_main}
We employ a consistent strategy for training VQ-VAE and replacing CoT tokens with latent discrete codes across all our experiments, as outlined below.
The specific model architecture and key hyperparameters used for LLM training are presented alongside the results for each category of benchmarks.
All the other details are deferred to \Cref{app:model}. \looseness=-1

\paragraph{VQ-VAE Training} For each benchmark, we train a VQ-VAE for 100k steps using the Adam optimizer, with learning rate $10^{-5}$ and batch size 32.
We use a codebook of size $1024$ and compress every chunk of $L=16$ tokens into a single latent token (i.e., the compression rate $r=16$).


\paragraph{Randomized Latent Code Replacement} We introduce a stochastic procedure for partially replacing CoT tokens with latent codes. 
Specifically, we define a set of predetermined numbers \( \mathcal{M} = \{0, 72, 128, 160, 192, 224, 256\}\), which are all multipliers of $L=16$.
For each training example, we first sample $m_{\max} \in \mathcal{M}$ then sample an integer $m \in [0, 16, 32, \ldots, m_{\max}]$ uniformly at random.
The first $m$ CoT tokens are replaced by their corresponding latent discrete codes, while the later ones remain as raw text. 
This stochastic replacement mechanism exposes the model to a wide range of latent-text mixtures, enabling it to effectively learn from varying degrees of latent abstraction.


\begin{table*}[t]
\centering
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\bf{Model}} & \multicolumn{2}{c}{\bf Keys-Finding Maze} & \multicolumn{2}{c}{\bf ProntoQA} & \multicolumn{2}{c}{\bf ProsQA} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
 & 1-Feasible-10 (\%) & Num. Tokens &  Accuracy & Num. Tokens & Accuracy & Num. Tokens \\ 
\midrule
Sol-Only & 3 & 645 & 93.8 & 3.0 & 76.7 & 8.2 \\
CoT & \underline{43}& 1312.0 & \underline{98.8} & 92.5 & \underline{77.5} & 49.4 \\

\bf{Latent (ours)}  & \bf{62.8 \increase{19.8}} & 374.6 & \bf{100 \increase{1.2}} & 7.7 & \textbf{96.2 \increase{18.7}} & 10.9 \\
\bottomrule
\end{tabular}
}
\caption{Our latent approach surpasses the other baselines on Keys-Finding Maze, ProntoQA and ProsQA with a large margin
. We use top-$k$ ($k=10$) decoding for Keys-Finding Maze and greedy decoding for ProntoQA and ProsQA. In terms of token efficiency, 
our latent approach also generates much shorter reasoning traces than the CoT baseline, closely tracking or even outperforming the Sol-Only approach.
\textbf{Bold: best results}. \underline{Underline: second best results}. \increase{Performance gain compared with the second best result.}
}
\label{table:synthetic}
\end{table*} 


\begin{table*}[t]
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lllllllllll}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{\bf Model}} & \multicolumn{2}{c}{\bf In-Domain} & \multicolumn{5}{c}{\bf Out-of-Domain} & \multicolumn{1}{c}{\bf Average} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-9} \cmidrule(lr){10-10}
& & Math & GSM8K & Gaokao-Math-2023 & DM-Math & College-Math & Olympia-Math & TheoremQA & All Datasets \\ \midrule
\multirow{6}{*}{\bf Llama-3.2-1B}
& Sol-Only  & 4.7 & 6.8 & 0.0 & 10.4 & 5.3 & 1.3 & 3.9 & 4.6 \\
& CoT  & \underline{10.5} & \underline{42.7} & \bf{10.0} & 3.4 & \underline{17.1} & 1.5 & 9.8 & \underline{14.1} \\
& iCoT  & 8.2 & 10.5 & 3.3 & \underline{11.3} & 7.6 & \textbf{2.1} & \underline{10.7} & 7.7 \\
& Pause Token & 5.1 & 5.3 & 2.0  & 1.4 &  0.5  & 0.0 &  0.6 & 2.1\\

& \textbf{Latent (ours)} & \textbf{14.7 \increase{4.2}} & \textbf{48.7 \increase{6}} & \textbf{10.0}  & \textbf{14.6 \increase{3.3}}  & \textbf{20.5 \increase{3.4}} & \underline{1.8}  & \textbf{11.3 \increase{0.6}}  & \textbf{17.8 \increase{3.7}}  \\
\midrule
\multirow{6}{*}{\bf Llama-3.2-3B}
& Sol-Only  & 6.1 & 8.1 & 3.3 & 14.0 & 7.0 & 1.8 & 6.8 & 6.7\\
& CoT  & \underline{21.9} & \underline{69.7} & \underline{16.7} & \textbf{27.3} & \underline{30.9} & 2.2 & 11.6 & \underline{25.2} \\
& iCoT  & 12.6 & 17.3 & 3.3 & 16.0 & 14.2 & \textbf{4.9} & \textbf{13.9} & 11.7 \\
& Pause Token & 25.2 & 53.7 & 4.1 & 7.4 &  11.8 & 0.7 &  1.0 & 14.8\\

& \textbf{Latent (ours)} & \textbf{26.1 \increase{4.2}}  & \textbf{73.8 \increase{4.1}}  & \textbf{23.3 \increase{6.6}}  & \underline{27.1}  & \textbf{32.9 \increase{2}}  & \underline{4.2}  & \underline{13.5}   & \textbf{28.1 \increase{2.9}} \\
\midrule
\multirow{6}{*}{\bf Llama-3.1-8B}
& Sol-Only  & 11.5 & 11.8 & 3.3 & 17.4 & 13.0 & 3.8 & 6.7 & 9.6 \\
& CoT  & {32.9} & \underline{80.1} & \underline{16.7} & \underline{39.3} & \underline{41.9} & 7.3 & \underline{15.8 } & \underline{33.4} \\
& iCoT  & 17.8 & 29.6 & 16.7 & 20.3 & 21.3 & \underline{7.6} & 14.8 & 18.3 \\
& Pause Token & \textbf{39.6} & 79.5 & 6.1  & 25.4 &   25.1 & 1.3 &  4.0 & 25.9\\


& \textbf{Latent (ours)} & \underline{37.2}  & \textbf{84.1 \increase{4.0}}  & \textbf{30.0 \increase{13.3}}  & \textbf{41.3 \increase{2}}  & \textbf{44.0 \increase{2.1}}  & \textbf{10.2 \increase{2.6}}  & \textbf{18.4 \increase{2.6}}  & \textbf{37.9 \increase{4.5}}  \\
 

\bottomrule
\end{tabular}
\end{adjustbox}
\caption{
Our latent approach outperforms the baselines on various types of mathematical reasoning benchmarks. The models are fine-tuned on the MetaMathQA~\cite{yu2023metamath} dataset. The Math and GSM8K are in-domain datasets since they are used to generate MetaMathQA, while the others are out-of-domain. \textbf{Bold: best results}. \underline{Underscore: second best results}. \textcolor{darkgreen}{$\uparrow$ +: \hspace{0.2em}Performance gain compared with the second best result.}
}
\label{table:LLMtable}
\end{table*}


\begin{table*}[t]
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{llccccccccc}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{\bf Model}} & \multicolumn{2}{c}{\bf In-Domain (\# of tokens)} & \multicolumn{5}{c}{\bf Out-of-Domain (\# of tokens)} & \multicolumn{1}{c}{\bf Average} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-9} \cmidrule(lr){10-10}
& & Math & GSM8K & Gaokao-Math-2023 & DM-Math & College-Math & Olympia-Math & TheoremQA & All Datasets \\ \midrule
\multirow{6}{*}{\bf Llama-3.2-1B}
& Sol-Only  & 4.7 & 6.8 & 0.0 & 10.4 & 5.3 & 1.3 & 3.9 & 4.6 \\
& CoT & 646.1 & 190.3 & 842.3 & 578.7 & 505.6 & 1087.0 & 736.5 & 655.2 \\
& iCoT & 328.4 & 39.8 & 354.0 & 170.8 & 278.7 & 839.4 & 575.4 & 369.5 \\
& Pause Token & 638.8 & 176.4 & 416.1 & 579.9 & 193.8 & 471.9 & 988.1 & 495\\
% & Dualformer &  \\
& \textbf{Latent (ours)} & 501.6 \decrease{22\%} & 181.3 \decrease{5\%} & 760.5 \decrease{11\%} & 380.1 \decrease{34\%} & 387.3 \decrease{23\%} & 840.0 \decrease{22\%} & 575.5 \decrease{22\%} & 518 \decrease{21\%} \\
\midrule
\multirow{6}{*}{\bf Llama-3.2-3B}
& Sol-Only  & 6.1 & 8.1 & 3.3 & 14.0 & 7.0 & 1.8 & 6.8 & 6.7\\
& CoT & 649.9  & 212.1  & 823.3 & 392.8 & 495.9 & 1166.7 & 759.6 & 642.9 \\
& iCoT & 344.4 & 60.7 & 564.0 & 154.3 & 224.9 & 697.6 & 363.6 & 344.2 \\
& Pause Token & 307.9 & 162.3 & 108.9 & 251.5 & 500.96 & 959.5 & 212.8 & 354.7 \\
% & Dualformer &  \\
& \textbf{Latent (ours)} & 516.7 \decrease{20\%} & 198.8 \decrease{6\%} & 618.5 \decrease{25\%} & 340.0 \decrease{13\%} & 418.0 \decrease{16\%} & 832.8 \decrease{29\%} & 670.2 \decrease{12\%} & 513.6 \decrease{20\%}\\
\midrule
\multirow{6}{*}{\bf Llama-3.1-8B}
& Sol-Only  & 11.5 & 11.8 & 3.3 & 17.4 & 13.0 & 3.8 & 6.7 & 9.6 \\
& CoT & 624.3 & 209.5 & 555.9 & 321.8 & 474.3 & 1103.3 & 760.1 & 578.5 \\
& iCoT & 403.5 & 67.3 & 444.8 & 137.0 & 257.1 & 797.1 & 430.9 & 362.5 \\
& Pause Token & 469.4 & 119.0 & 752.6 & 413.4 & 357.3 & 648.2 &600.1 &  480\\

& \textbf{Latent (ours)} & 571.9 \decrease{9 \%} & 193.9 \decrease{8 \%} & 545.8 \decrease{2 \%} & 292.1 \decrease{10\%} & 440.3 \decrease{8\%} & 913.7 \decrease{17 \%} & 637.2 \decrease{16 \%} & 513.7 \decrease{10\%}\\


\bottomrule
\end{tabular}
\end{adjustbox}
\caption{The average number of tokens in the generated responses. Compared with the CoT baseline, our latent approach achieves an $17\%$ reduction in response length on average, while surpassing it in final performance according to~\Cref{table:LLMtable}. The iCoT method generates shorter responses than our approach, yet performs significantly worse, see~\Cref{table:LLMtable}. \textcolor{darkgreen}{$\downarrow$ -:\hspace{0.2em}Trace length reduction rate compared with CoT.} }
\label{table:LLM-token}
\end{table*}


\subsubsection{Synthetic Benchmarks}

\paragraph{Hyperparameters and Evaluation Metric}  

For our experiments on the ProntoQA and ProsQA datasets, we fine-tune the pretrained GPT-2 model~\cite{radford2019language} for $16$k steps, where we use a learning rate of $10^{-4}$ with linear warmup for 100 steps, and the batch size is set to 128. 
To evaluate the models, we use greedy decoding and check the exact match with the ground truth.

For Keys-Finding Maze, due to its specific vocabulary, we trained a T5 model~\cite{2020t5} from scratch for 100k steps with a learning rate of $7.5 \times 10^{-4}$ and a batch size of 1024. We evaluate the models by the \emph{1-Feasible-10} metric. Namely, for each evaluation task, we randomly sample 10 responses with top-$k$ ($k$=10) decoding and check if any of them is feasible and reaches the goal location. 

\paragraph{Results}
As shown in \Cref{table:synthetic}, our latent approach performs better than the baselines
for both the Keys-Finding Maze and ProntoQA tasks.
Notably, the absolute improvement is 15\% for the Keys-Finding Maze problem, 
and we reach 100\% accuracy on the relatively easy ProntoQA dataset.
For the more difficult ProsQA, the CoT baseline only obtains 77.5\% accuracy,
the latent approach achieves $17.5\%$ performance gain.



\begin{table*}[t]
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lllllllllll}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{\bf Model}} & \multicolumn{2}{c}{\bf In-Domain} & \multicolumn{5}{c}{\bf Out-of-Domain} & \multicolumn{1}{c}{\bf Average} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-9} \cmidrule(lr){10-10}
& & math & GSM8K & Fresh-Gaokao-Math-2023 & DeepMind-Mathematics & College-Math & Olympia-Math & TheoremQA & All Datasets \\ \midrule
\multirow{3}{*}{\bf Llama-3.2-1B}

& {All-Replace} & 6.7 & 4.2 & 0.0 & 11.8 & 6.0 & {2.1} & 8.5 & 5.6 \\
& {Curriculum-Replace} & {7.1} & \underline{9.8} & \underline{3.3} & \underline{13.0} & 
{7.9} & \bf{2.4} & \underline{10.5} & {7.8} \\
& Poisson-Replace & \underline{13.9 } & \textbf{49.5} & {10.0}  & {12.2}  & \underline{18.9 } & \underline{2.3}  & {9.0 }  & \underline{15.1 }   \\
& \textbf{Latent-AR (ours)} & \textbf{14.7 } & \underline{48.7 } & \textbf{10.0}  & \textbf{14.6 }  & \textbf{20.5} & 1.8  & \textbf{11.3 }  & \textbf{17.8 }   \\


\midrule
\multirow{3}{*}{\bf Llama-3.2-3B}

& {All-Replace} & {10.7} & 12.8 & {10.0} & \underline{19.4} & 12.8 & \bf{5.3} & 11.8 & {11.8} \\
& {Curriculum-Replace} & 10.2 & {14.9} & 3.3 & 16.8 & {12.9} & 3.9 & \bf{14.4} & 10.9 \\
& Poisson-Replace & \underline{23.6 } & \underline{65.9 } & \underline{13.3}  & {17.9 }  & \underline{28.9 } & 2.9  & {11.2 }  & \underline{20.5 }   \\
& \textbf{Latent (ours)} & \textbf{26.1 }  & \textbf{73.8 }  & \textbf{23.3 }  & \textbf{27.1 }  & \textbf{32.9 }  & \underline{4.2}  & \underline{13.5}   & \textbf{28.1 } \\



\midrule
\multirow{3}{*}{\bf Llama-3.1-8B}

& {All-Replace} & {15.7} & 19.9 & 6.7 & {21.1} & {19.5} & {5.0} & {17.5} & 15.0 \\
& {Curriculum-Replace} & 14.6 & {23.1} & {13.3} & 20.3 & 18.7 & 3.9 & 16.6 & {15.8} \\
& Possion-Replace & \textbf{37.9 } & \underline{83.6 }  & \underline{16.6 }  & \textbf{42.7 }  & \textbf{44.7 }  & \underline{9.9 }  & \textbf{19.1 }  & \underline{36.3 }  \\
& \textbf{Latent (ours)} & \underline{37.2 } & \textbf{84.1 }  & \textbf{30.0 }  & \underline{41.3 }  & \underline{44.0 }  & \textbf{10.2 }  & \underline{18.4 }  & \textbf{37.9 }  \\




\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Our latent token replacement strategy significantly outperforms the alternative choices: All-Replace (where all the textual CoT tokens are replaced by latent tokens at once), Curriculum-Replace (where we gradually replace the text tokens for the entire CoT subsequence by latent tokens over the course of training) and Poisson-Replace (where individual chunks of text tokens are replaced with probabilities 0.5).}
\label{table:ablation_replacement}
\end{table*}




\subsubsection{Mathematical Reasoning}

\paragraph{Hyperparameters and Evaluation Metrics}
We considered 3 different sizes of LLMs from the LLaMa herd:  Llama-3.2-1B, Llama-3.2-3B and Llama-3.1-8B models. For all the models, we fine-tune them on the MetaMathQA dataset for 1 epoch. To maximize training efficiency, we use a batch size of 32 with a sequence packing of 4096.
We experiment with different learning rates $10^{-5}, 2.5 \times 10^{-5}, 5 \times 10^{-5}, 10^{-4}$ and select the one with the lowest validation error. 
The final choices are $10^{-5}$ for the 8B model and $2.5 \times 10^{-5}$ for the others. For all the experiments, we use greedy decoding for evaluation.

\paragraph{Accuracy Comparison} \Cref{table:LLMtable} presents the results. Our latent approach consistently outperforms all the baselines across nearly all the tasks, for models of different sizes. For tasks on which we do not observe improvement, our approach is also comparable to the best performance. The gains are more pronounced in specific datasets such as Gaokao-Math-2023. On average, we are observing a $+5.3$ points improvement for the 8B model, $+2.9$ points improvement for the 3B model, and +3.7 points improvement for the 1B model. 


\paragraph{Tokens Efficiency Comparison}
Alongside the accuracy, we also report the number of tokens contained in the generated responses in \Cref{table:LLM-token}, which is the dominating factor of the inference efficiency. 
Our first observation is that for all the approaches, the model size has little influence on the length of generated responses.
Overall, the CoT method outputs the longest responses, while the Sol-Only method outputs the least number of tokens, since it is trained to generate the answer directly. The iCoT method generates short responses as well ($42.8\%$ reduction compared to CoT), as the CoT data has been iteratively eliminated in its training procedure. However, this comes at the cost of significantly degraded model performance compared with CoT, as shown in \Cref{table:LLMtable}. Our latent approach shows an average $17\%$ reduction in token numbers compared with CoT while surpassing it in prediction accuracy.


\subsection{Ablation \& Understanding Studies}
\label{sec:expr_ablation}

\paragraph{Replacement Strategies}
Our latent approach partially replaces the leftmost $m$ CoT tokens, where the value of $m$ varies for each sample. We call such replacement strategies \textbf{AR-Replace}. Here we consider three alternative strategies:
\begin{enumerate}[topsep=0pt]
    \item[(1)] \textbf{All-Replace}: all the text CoT tokens are replaced by the latent tokens.
    \item[(2)] \textbf{Curriculum-Replace}: the entire CoT subsequence are gradually replaced over the course of training, similar to the training procedure used by iCoT and COCONUT~\cite{hao2024training}. We train the model for 8 epochs. Starting from the original dataset, in each epoch we construct a new training dataset whether we further replace the leftmost 16 textual CoT tokens by a discrete latent token.
    \item[(3)] \textbf{Poisson-Replace}: instead of replacing tokens from left to right, we conduct a \emph{Poisson sampling} process to select CoT tokens to be replaced: we split the reasoning traces into chunks consisting of 16 consecutive text tokens, where each chunk is randomly replaced by the latent token with probability 0.5. 
    
\end{enumerate}


\Cref{table:ablation_replacement} reports the results.  Our \textbf{AR-Replace} strategy demonstrate strong performance, outperforming the other two strategies with large performance gap. Our intuition is as follows.
When all the textual tokens are removed, the model struggles to align the latent tokens with the linguistic and semantic structures it learned during pretraining. 

In contrast, partial replacement offers the model a bridge connecting text and latent spaces: the remaining text tokens serve as anchors, helping the model interpret and integrate the latent representations more effectively. 
Interestingly, the curriculum learning strategy is bridging the two spaces very well, where \textbf{All-Replace} and \textbf{Curriculum-Replace} exhibit similar performance. This is similar to our observation that iCoT performs remarkably worse than CoT for mathematical reasoning problems.
\textbf{Poisson-Replace} demonstrates performance marginally worse to our \textbf{AR-Replace} strategy on the 1B and 8B models, but significantly worse on the 3B model. Our intuition is that having a fix pattern of replacement (starting from the beginning and left to right) is always easier for the model to learn. This might be due to the limited finetuning dataset size and model capacity. 



\paragraph{Attention Weights Analysis}
To understand the reason why injecting latent tokens enhanced the model's reasoning performance, we randomly selected two questions from the Math and Collegue-Math dataset  and generate responses, then analyze the attention weights over the input prompt tokens:
\begin{enumerate}
    \item \texttt{What is the positive difference between \$120\%\$ of 30 and \$130\%\$ of 20?}
    \item \texttt{Mark has \$50 in his bank account. He earns \$10 per day at his work. If he wants to buy a bike that costs \$300, how many days does Mark have to save his money?}
\end{enumerate}
Specifically, we take the last attention layer, compute the average attention weights over different attention heads and show its relative intensity over the prompt tokens\footnote{We first compute the average attention weights across multiple heads. This gives us a single lower triangular matrix. Then,
we take the column sum of this matrix to get an aggregated attention weights for each token. Last, we normalize the weights by their average to obtain the relative intensity. A one line pseudocode is: \texttt{column\_sum(avg(attention\_matrices)) / avg(column\_sum(avg(attention\_matrices)))}. 
 }. We compare the averaged attention weights of our model with the CoT model in \Cref{fig:attention}.
Interestingly, our model learns to grasp a stronger attention to numbers and words representing mathematical operations. Both \cref{fig:entry_1} and \cref{fig:entry_2} show that the latent model focus more on the numbers, such as \texttt{120}, \texttt{30}, and \texttt{130} for the first question.
For the second question, our latent model shows a larger attention weights on numbers including \texttt{50}, \texttt{10}, and \texttt{300}, and also tokens semantically related to mathematical operations such as \texttt{earns} (means addition) and \texttt{cost} (means subtraction). 
This suggests that, by partially compressing the reasoning trace into a mix of latent and text tokens, we allow the model to effectively focus on important tokens that build the internal logical flow. See \Cref{app:generated_text_attention} for the exact response generated by our approach and the CoT baseline.


\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{\columnwidth}
       \includegraphics[width=8cm]{plots/entry_1.png}
  \caption{Prompt: \texttt{What is the positive difference between 
  \$120\%\$ of 30 and \$130\%\$ of 20?}}
  \label{fig:entry_1}
  \end{subfigure}

  \begin{subfigure}[b]{\columnwidth}
       \includegraphics[width=8cm]{plots/entry_7746.png}
  \caption{Prompt: \texttt{Mark has \$50 in his bank account. He earns \$10 per day at his work. If he wants to buy a bike that costs \$300, how many days does Mark have to save his money?}}
  \label{fig:entry_2}
  \end{subfigure}
  \caption{Comparing with the CoT model, our latent approach have high attention weights on numbers and text tokens representing mathematical operations.}
  \label{fig:attention}
\end{figure}
 

\paragraph{Additional Experiments} We provide 4 additional example responses for questions in the Math and TheoremQA datasets in \Cref{app:generated_text_others}. In \Cref{app:additional_experiments}, we compare all the approaches when the model is trained on the DART-MATH~\cite{tong2024dart} 
dataset, where similar trends are observed.
