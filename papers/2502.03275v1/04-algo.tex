\section{Methodology}
\label{sec:algo}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.35\columnwidth]{plots/replacement.pdf}
    \caption{An example illustrating our replacement strategy. With chunk size $L=16$ and compression rate $r=16$, we encode 32 textual CoT tokens into 2 discrete latent tokens from left to right. The other CoT tokens will remain in their original forms. 
    }
    \label{fig:replacement}
\end{figure*}
In this section, we describe our methodology to enable LLMs to reason with discrete latent tokens. The notations are summarized in \Cref{app:notations}.
Let $X = P \oplus C \oplus S$ denote a sample input,
where $P = (p_1, p_2, \ldots, p_{t_p})$ are the prompt tokens, $C = (c_1, c_2, \ldots, c_{t_c})$ are the reasoning step (chain-of-thought) tokens,
$S = (s_1, s_2, \ldots, s_{t_s})$ are the solution tokens, and $\oplus$ denotes concatenation. Our training procedure consists of two stages:
\begin{enumerate}[leftmargin=*]\itemsep0em
    \item \textbf{Learning latent discrete tokens to abstract the reasoning steps}, where we train a model to convert $C$ into a sequence of latent tokens $Z = (z_1, z_2, \ldots, z_{t_z})$ such that $t_z < t_c$. The compression rate $r = t_c / t_z$ controls the level of abstraction.

    \item \textbf{Training the LLM with a partial and high-level abstract of the reasoning steps}, where we 
    construct a modified input $\Xtilde$ by
    replacing the first $m$ tokens of $C$ by the corresponding latent abstractions:
    \begin{equation}
        \Xtilde = P \oplus [z_1, \ldots, z_{\frac{m}{r}}, c_{m+1}, \ldots, c_{t_c}] \oplus S.
        \label{eq:X_replacement}
    \end{equation}
    \Cref{fig:replacement} illustrates this replacement strategy. We randomize the value of $m$ during training.
\end{enumerate}



\subsection{Learning Latent Abstractions}
We employ a vector-quantized variable autoencoder (VQ-VAE)~\cite{van2017neural} type of architecture to map CoT tokens \(C\) into discrete latent tokens \(Z\). % VQ-VAE is a powerful model capable of capturing high-dimensional semantic structures from text data.
To enhance abstraction performance, our VQ-VAE is trained on the whole input sequence $X$, but only applied to $C$ in the next stage. Following~\citet{jiang2022efficient, jiang2023h}, we split $X$ into chunks of length \(L\) and encode each chunk into $\frac{L}{r}$ latent codes, where $r$ is a preset compression rate. More precisely, our architecture consists of the following five components:
\vspace{-5pt}
\begin{itemize}\itemsep0pt
    \item $\E:$ a codebook containing $|\E|$ vectors in $\R^d$.
    \item $\fenc: \V^L \mapsto \R^{d \times \frac{L}{r}} $ that encodes a sequence of $L$ text tokens to $\frac{L}{r}$ latent embedding vectors $\bar{X} = \bar{x}_1, \ldots, \bar{x}_{\frac{L}{r}}$,  where $\V$ is the vocabulary of text tokens.
   %  and $\Z$ are the vocabularies of text and latent tokens, respectively. 
   \item $q: \R^{d} \mapsto \E$: the quantization operator that replaces the encoded embedding $\bar{x}$ by the nearest neighbor in $\E$: $q(\bar{x}) = \argmin_{e_i \in \E} \norm{e_i - \bar{x}}^2_2$.
    \item $g: \V^K \mapsto \R^d$ that maps $K$ text tokens to a $d$-dimensional embedding vector. We use $g$ to generate a continuous embedding of the prompt $P$.
    \item $\fdec: \R^{d \times \frac{L}{r}} \times \R^k \mapsto \V^L$ that decodes latent embeddings back to text tokens, conditioned on prompt embedding.
\end{itemize}
In particular, each continuous vector $e \in \E$ in the codebook has an associated latent token $z$, which we use to construct the latent reasoning steps $Z$\footnote{To decode a latent token $z$, we look up the corresponding embedding $e \in \E$ and feed it to $\fdec$.}.


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{plots/vqvae_latentformer.pdf}
    \caption{A graphical illustration of our VQ-VAE. $\fenc$ encodes the text tokens into latent embeddings, which are quantized by checking the nearest neighbors in the codebook. $\fdec$ decodes those quantized embeddings back to text tokens. When applying the VQ-VAE to compress the text tokens, the discrete latent tokens $Z$ are essentially the index of corresponding embeddings in the codebook.} 
    \label{fig:vqvae}
\end{figure}


For simplicity, we assume the lengths of the input $X$ and the prompt $P$ are $L$ and $K$ exactly.
Similar to \citet{van2017neural}, we use an objective $\mathcal{L}$ composed of 3 terms: 
% . Given an input sequence \(X_t = (x_1, x_2, \ldots, x_T)\) and the discrete latent tokens \(Z_t = (z_1, z_2, \ldots, z_{\tfrac{M T}{L}})\), the total loss  is:
\begin{equation}
\begin{aligned}
& \mathcal{L}(X) = \underbrace{ \log p(X | \fdec( q(\bar{X}) | g(P) ))}_{\text{reconstruction loss}} + \\
&  \hskip5pt \sum_{i=1}^L \underbrace{ \| \sg[\bar{X}_i] - q(\bar{X}_i) \|_2^2}_{\text{VQ loss}} +  \underbrace{\beta \| \bar{X}_i - \sg[q(\bar{X}_i)] \|_2^2}_{\text{commitment loss}},
\end{aligned}
\end{equation}
where $\bar{X} = \fenc(X)$, $\sg[\cdot]$ is the stop-gradient operator, and \(\beta\) is a hyperparameter controlling the strength of the commitment loss.
The VQ loss and the commitment loss ensure that the encoder outputs remain close to the codebook, while the reconstruction loss concerns with the decoding efficacy. As standard for VQ-VAE, we pass the gradient $\nabla_{\fdec}(L)$ unaltered to $\fenc$ directly as the quantization operator $q(\cdot)$ is non-differentiable. \Cref{fig:vqvae} illustrates our architecture. In practice, we use a causal Transformer for both $\fenc$ and $\fdec$, the model details are discussed in Appendix~\ref{app:model}.


Thus far we obtain a latent representation both semantically meaningful and conducive to reconstruction, setting the stage for the subsequent training phase where
the LLM is trained to perform reasoning with abstractions.


\subsection{Reasoning with Discrete Latent Tokens}


In this second stage, we apply the obtained VQ-VAE to form modifed samples $\Xtilde$ with latent abstractions as in \Cref{eq:X_replacement}, then train an LLM to perform next token prediction. 
Below, we outline the major design choices that are key to our model's performance, and ablate them in \Cref{sec:expr}.

\textbf{Partial Replacement}. Unlike previous planning works~\cite{jiang2022efficient, jiang2023h} that project the whole input sequence onto a compact latent space, we only replace $m < t_c$ CoT tokens with their latent abstractions, leaving the remaining tokens unchanged.  We delimit the latent tokens by injecting a special \texttt{<boLatent>} and \texttt{<eoLatent>} tokens to encapsulate them.
% The constructed $\Xtilde$ becomes a fixed mixture of early latent tokens and later text tokens.

\textbf{Left-to-Right (AR) Replacement}. We replace the leftmost $m$ tokens of $C$, rather than subsampling tokens at different locations. 

\textbf{Mixing Samples with Varying Values of $m$}. For fine-tuning an existing LLM on the reasoning dataset with latent tokens, one remarkable challenge is to deal with the extended vocabulary. As the LLM is pretrained with trillions of tokens,
it is very hard for it to quickly adapt to tokens (and corresponding embeddings) beyond the original vocabulary. Previous works that aim to replace or eliminate CoT tokens~\cite{deng2024explicit, hao2024training} employ a multistage curriculum training approach, where those operations are gradually applied to the entire input sequence. In the context of our approach, this means we increase the values of $m$ in each stage until it reaches a pre-set cap value. However, such training procedure is complex and computationally inefficient, where dedicated optimization tuning is needed. In this work, we employ a simple single stage training approach where the value of $m$ is randomly set for each sample. Surprisingly, this not only makes our training more efficient, but also leads to enhanced performance. 