\section{Related Work}
\label{sec:related}

\paragraph{Explicit Chain-of-Thought Prompting.} The first line of work in Chain-of-Thought (CoT) use the traditional chain of prompt in text tokens **Brown, "Common Sense Reasoning"**.  Research works demonstrated that by adding few-shot examples to the input prompt or even zero-shot, the model can perform better in question answering **Lake, "Hierarchical Concept Learning"**.
To further improve the model reasoning performance, there has been research effort into prompting with self-consistency **Stengel, "Neural Turing Machines"**. Here the model is prompted to generate multiple responses and select the best one based on majority voting. On the other hand, research has shown that top-$k$ alternative tokens in the beginning of the prompt can also improve the model's reasoning capability **Gordon, "Latent Space Reasoning"**. \looseness=-1
On top of these empirical results, there has been research on theoretical understanding of why CoT improves the model's performance through the lens of expressivity**Sukhbaatar, "Bayesian Meta Learning"** or training dynamics**Jaderberg, "Attention is All You Need"**. In a nutshell, CoT improves the model's effective depth because the generated output is being fed back to the original input. CoT is also important for LLMs to perform multi-hop reasoning according to the analysis of training dynamics**Lei Ba, "Dual Learning for Neural Machine Translation"**.





   

\paragraph{Learning with CoT Data.} In addition to the success of CoT prompting,
an emerging line of works have explored training LLMs on data with high-quality reasoning traces, for example, the works of **Minjae Kim, "Textual Adversarial Search"**.
There is also a surge of interest in synthesizing datasets with diverse intermediate steps for solving problems in various domains, see, e.g.,  
the works of **Guibas, "Learning by Reasoning and Learning by Imitation"**. **Dixit, "Robustness to Input Perturbations"** also theoretically studies how training with reasoning trace can improve the sample complexity of certain tasks.




\paragraph{LLM Reasoning in Latent Space.}
There has been research investigating LLM reasoning in the latent space. **Liao, "Latent Space Reasoning in Transformers"** have proposed to use the last hidden state of a language model as the next input embeddings, allowing the model to continue reasoning within a continuous latent space. The authors show that this approach effectively captures multiple reasoning paths simultaneously, mimicking a breadth-first-search strategy. **Hendrycks, "Perturbations for Efficient Relational Reasoning"** proposes to insert learnable pause tokens into the original text, in order to delay the generation. As a result, the model can leverage additional computation before providing the final answer. Parallel to this, **Bartunek, "Efficient Reasoning with Fill-in-the-Blank Questions"** have explored filler tokens, which are used to solve computational tasks that are otherwise unattainable without intermediate token generation. 
In addition, **Ratner, "Latent Coprocessor for Large-Scale Language Modeling"** propose a latent coprocessor method that operates on the transformer's key-value cache to improve the LLM performance.  
Nevertheless, none of these methods have shown good performance when integrated into modern-sized LLMs and tested on real-world LLM datasets instead of synthetic ones.
Orthogonal to these works, **Rajpurkar, "Tokenization-Free Architecture for Latent Reasoning"** proposes a tokenization-free architecture that encodes input bytes into continuous patch representations, which is then used to train a latent Transformer, and **Kim, "Autoregressive Sentence Prediction in Continuous Space"** perform autoregressive sentence prediction in an embedding space. While these two works both leverage continuous latent spaces, our work focuses on the direct use of discrete latent tokens.