\section{Related Work}
% \paragraph{Reward Model for LLM Mathematical Reasoning}
% Recently reasoning LLMs \cite{chen2024step,} leverage external verifiers to select the best reasoning path from multiple decoded candidates, thereby improving reasoning quality and reliability.
% Recent research\cite{setlur2024rewardingprogressscalingautomated,wang2024mathshepherd,guan2025rstar} indicates leveraging external reward models as verifiers to select the best reasoning path from multiple decoded candidates can improve reasoning in large language models.
% The Outcome Reward Model (ORM) \cite{cobbe2021ORM,yang2024qwen2.5math} assigns rewards based on the final output rather than intermediate steps, optimizing for end-task performance. \textcolor{blue}{However, this focus on terminal outputs neglects the intermediate reasoning steps that are crucial for tasks requiring fine-grained logical consistency. } 

% \textcolor{blue}{To mitigate this issue}, Process Reward Models (PRMs) \cite{lightman2023lets_verify,zhang2025lessons,wang2024mathshepherd} evaluate step-by-step reasoning, making them more suitable for tasks that require fine-grained assessment of logical consistency.
% Generative Verifiers (GenRM) \cite{zhang2024generativeverifiersrewardmodeling} extend traditional verifiers by jointly verifying and generating solutions using the next-token prediction objective, which also integrates chain-of-thought (CoT)\cite{wei2022cot} reasoning. However, GenRM is computationally more expensive than ORM and PRM, as it generates verification rationales instead of providing a direct "Yes" or "No" classification for correctness.
% \textcolor{blue}{To mitigate such issue, Process Reward Models (PRMs) \cite{lightman2023lets_verify,zhang2025lessons,wang2024mathshepherd} rapidly output a binary yes/no for each reasoning step. While this offers granular insights, PRMs perform best on complete traces, because of the lack of deeper analysis. In contrast, Generative Verifiers (GenRM) \cite{zhang2024generativeverifiersrewardmodeling} combine chain-of-thought (CoT) reasoning with next-token predictions to both verify and generate solutions \cite{wei2022cot}. However, their high computational cost and reliance on detailed verification rationales, as opposed to direct correctness predictions, make them less suitable for time-critical applications.}

%To balance computational efficiency and verification performance, we propose the Dynamic Verifier (DyVer), which aims to optimize both reasoning accuracy and resource utilization by combining GenRM and PRM.
% \textcolor{blue}{In contrast, our DyVer framework merges the strengths of PRMs and GenRMs using Kahneman’s dual-system theory: a rapid, token-level System 1 check for clear cases coupled with a detailed System 2 review for potential errors.}

Recent research~\cite{setlur2024rewardingprogressscalingautomated,wang2024mathshepherd,guan2025rstar} shows that external reward models can improve LLM reasoning by selecting the best path from multiple candidates. Outcome Reward Models (ORMs)~\cite{cobbe2021ORM,yang2024qwen2.5math} optimize for final outputs but overlook vital intermediate steps. Process Reward Models (PRMs)~\cite{lightman2023lets_verify,zhang2025lessons,wang2024mathshepherd} provide rapid binary validations for each step, yet struggle with a deeper analysis of incomplete traces. In contrast, Generative Verifiers (GenRMs)~\cite{zhang2024generativeverifiersrewardmodeling} combine chain-of-thought reasoning with next-token predictions to verify and generate solutions, although at a high computational cost. To balance these trade-offs, our DyVe framework merges the strengths of PRMs and GenRMs using Kahneman's dual system theory.
%: a fast, token-level System 1 check for clear cases coupled with a detailed System 2 review for potential errors.



% \paragraph{Step-level Mathematical Reasoning Data Generation with Monte Carlo Estimation}
%Recent advancements in step-level mathematical reasoning data generation have improved multi-step problem-solving by leveraging reward models and process supervision with Monte Carlo estimation for enhanced accuracy and scalability. 
%\textcolor{blue}{High-quality step-level supervision is critical for training effective process verifiers. Recent works have leveraged Monte Carlo estimation to generate such data. For example, } OmegaPRM \cite{luo2024improve_omegaprm} automates the collection of process supervision data by a novel divide-and-conquer style Monte Carlo Tree Search (MCTS) algorithm, \textcolor{blue}{thereby lifting the need of} manual annotations.
%ReST-MCTS* \cite{zhang2024restmcts} introduces a self-training approach that combines MCTS with process reward guidance to collect high-quality reasoning traces, enhancing the fine-tuning of policy and reward models for improved accuracy.
%\textcolor{blue}{Similarly, ReST-MCTS* \cite{zhang2024restmcts} leverages MCTS with process reward guidance to curate reasoning traces. Since Monte Carlo estimation yields noisy annotations—and \citet{zheng2023judging} show that LLM-as-a-Judge outperforms PRMs for error detection—\citet{zhang2025lessons} employs consensus filtering between LLM-as-a-Judge~\cite{Gu2024ASOJudge} and a critic model to remove noise. We extend this approach with step-wise consensus filtered process supervision, using a reasoning LLM to annotate each filtered sample step-by-step and generate high-quality step-level data. This refined strategy is key to DyVer, as it reliably distinguishes steps that require rapid validation from those needing deeper analysis.}
High-quality step-level supervision is crucial for training process verifiers, yet human annotations (e.g., PRM800k~\cite{Lightman2023LetsVS800k}) are prohibitively expensive. To avoid this, OmegaPRM~\cite{luo2024improve_omegaprm} employs a divide-and-conquer Monte Carlo Tree Search (MCTS) to generate annotations, although our experiments show that these labels are often noisy and weak. To address this issue, we adopt consensus filtering with an LLM-as-a-Judge~\cite{Gu2024ASOJudge} to eliminate unreliable samples~\cite{zhang2025lessons}, and further extend this approach with step-wise flagging, where a reasoning LLM conducts step-by-step analysis to identify steps that require System 2 verification.



 


% Math-shepherd \cite{wang2024mathshepherd} provides a process-oriented math reward model that uses automatically generated supervision data to assign step-level rewards, improving both output verification and reinforcement learning for LLMs..


%\subsection{Process-based Verifiers Guided Search}

%Recent research \cite{snell2025scaling} shows that applying a “compute-optimal” scaling strategy can significantly improve test-time compute efficiency, surpassing the performance of the best-of-N baseline.
%Tree of Thoughts (ToT) \cite{yao2024treeofthought} extends Chain-of-Thought reasoning by enabling structured exploration over multiple reasoning paths, outperforming best-of-N by considering alternative intermediate steps and evaluating the most promising paths.
%SVPO \cite{chen2024step} extends Direct Preference Optimization (DPO)\cite{rafailov2024dpo} by using MCTS \cite{browne2012mcts_survey} to annotate step-level preferences, resulting in improved model performance on complex reasoning tasks.
%rStar-Math \cite{guan2025rstar} demonstrates that small language models can achieve state-of-the-art math reasoning by incorporating MCTS-guided deep thinking, along with innovations in data synthesis and process reward modeling to iteratively improve reasoning. Inspired by these advancements, we extend DyVer for LLM guided search during reasoning inference, leveraging process-based verifiers to enhance step-level decision-making.
%\textcolor{blue}{Recent research \cite{snell2025scaling} demonstrates that a “compute-optimal” strategy can be achieved by combining a proposer LLM with a process verifier. However, due to limitations in current System 1 verifiers, the best-of-\(N\) approach remains optimal for medium to hard problems. More advanced searches, such as Beam Search~\cite{yao2024treeofthought} and MCTS, often exploit spurious features from noisy Monte Carlo estimation. In contrast, rStar-Math \cite{guan2025rstar} shows that small language models can achieve competitive math reasoning by incorporating a specialized program-of-thought (PoT) verifier. These findings indicate that reliable verification is vital for reasoning systems, though efficiency remains a challenge. This motivates our proposal of DyVer Search, which integrates context-aware error recovery mechanisms to balance performance and efficiency.}


% \begin{figure*}[t]
%     \centering
%     \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{images/pipeline.pdf}}
%     % \includegraphics[width=\linewidth]{latex/figs/framework.pdf} \hfill
%     \caption{Comparison of Different Process Verification Approaches. Top: Reasoning LLM with self-reflection shows instability in verification. Middle: Traditional verification approaches (GenRM and GenRM-CoT) with static offline verification. Bottom: Our proposed DyVer framework demonstrating dynamic online verification with adaptive parallel reflection and efficient error correction capabilities.}
%     \label{fig:pipeline}
% \end{figure*}


% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.48\linewidth]{images/tt-scaling.png} \hfill
%     \includegraphics[width=0.42\linewidth]{images/inference_speed_comparison.png}
%     \caption{(\textbf{Left}) Comparison of Dyve and various process verifiers when combined with Best-of-N. (\textbf{Right}) Inference speed comparison by dataset and model, illustrating time per sample in seconds for System-1, Dyve, and R1-14B across GSM8K, MATH, OlympiadBench, and OmniMATH.}
%     \label{fig:inference}
% \end{figure*}