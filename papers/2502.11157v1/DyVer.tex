% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
\usepackage{ulem}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{url}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
% \usepackage{microtype}
% \usepackage{lmodern}
\usepackage[most]{tcolorbox}
\usepackage{verbatim}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{booktabs}
% \usepackage{verbatim}
% \usepackage{fullpage}
% \tcbuselibrary{listings, breakable}
% \usepackage{listings}
% \usepackages{xcolor}

% Configure listings to use a teletype font, enable breaklines, and flexible columns
% \lstset{
%   basicstyle=\ttfamily\footnotesize,
%   breaklines=true,
%   columns=flexible,
%   postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}
% }

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%\title{DyVer: Dynamic Process Verification for Adaptive Multi-Step Reasoning in Large Language Models}
% \title{DyveTrace: Dynamic Process Verification for Robust LLM Reasoning}
\title{Dyve: Thinking Fast and Slow for Dynamic Process Verification}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
  \textbf{Jianyuan Zhong\thanks{\ \ Equal contribution.}}, 
  \textbf{Zeju Li\footnotemark[1]}, 
  \textbf{Zhijian Xu}, 
  \textbf{Xiangyu Wen}, 
  \textbf{Qiang Xu\thanks{\ \ Corresponding author.}} 
  \\
  The Chinese University of Hong Kong \\
  \texttt{\{jyzhong, zjli24, zjxu21, xywen22, qxu\}@cse.cuhk.edu.hk}
}



\begin{document}
\maketitle
\begin{abstract}

%We present \textbf{DyVer}, a dynamic process verification framework that empowers large language models with robust, multi-stage error detection. Inspired by Kahneman’s dual-system theory, DyVer fuses rapid, single-token System 1 validation with deeper, corrective System 2 analysis to reliably assess both complete and partial reasoning traces. Unlike traditional binary verifiers that focus solely on final outcomes, our approach leverages a novel step-wise consensus-filtered process supervision technique, using Monte Carlo estimation combined with both LLM-as-a-Judge and reasoning models, to curate high-quality, granular training signals from noisy rollouts. Experiments on ProcessBench demonstrate that DyVer not only significantly improves error detection during advanced search operations (e.g., beam search and Monte Carlo Tree Search) but also maintains computational efficiency by smartly allocating verification resources. This work lays the foundation for more resilient and adaptable LLM reasoning systems in complex, multi-step problem solving.

%We introduce \textbf{DyveTrace}, a dynamic process verification framework that enhances large language models (LLMs) with multi-stage, fine-grained error detection. Inspired by Kahneman’s dual-system theory, DyveTrace combines rapid, single-token validation (System 1) with deeper, corrective reasoning (System 2) to assess both full reasoning traces and partial steps. Unlike traditional verifiers that only evaluate final outputs, DyveTrace employs a \textbf{step-wise consensus-filtered supervision} strategy, leveraging Monte Carlo estimation, LLM-as-a-Judge, and specialized reasoning models to extract high-quality training signals from noisy rollouts. Experiments on ProcessBench demonstrate that DyveTrace significantly improves error detection during advanced search operations (e.g., beam search and Monte Carlo Tree Search) while maintaining computational efficiency by strategically allocating verification resources. 

We present \textbf{\textit{Dyve}}, a dynamic process verifier that enhances reasoning error detection in large language models by integrating fast and slow thinking, inspired by Kahneman's Systems Theory. Dyve adaptively applies immediate token-level confirmation (\textit{System 1}) for straightforward steps and comprehensive analysis (\textit{System 2}) for complex ones. Leveraging a novel step-wise consensus-filtered process supervision technique, combining Monte Carlo estimation, LLM-as-a-Judge, and specialized reasoning models, we curates high-quality supervision signals from noisy data for Dyve. Experimental results on ProcessBench and the MATH dataset confirm that Dyve significantly outperforms existing process-based verifiers and boosts performance in Best-of-N settings. Our code, data and model are released at: \url{https://github.com/staymylove/Dyve}

\end{abstract}


\section{Introduction}

Large Language Models (LLMs) have significantly enhanced their reasoning capabilities by shifting from rapid, intuitive System 1 responses to more deliberate, extended System 2 thinking~\cite{team2025kimik1.5, arrieta2025earlyexternalsafetytestingO3, guo2025deepseekr1}. While enabling more complex problem-solving in math and scientific reasoning, this has also introduced new challenges in process verification, particularly in the reliable evaluation of incomplete reasoning traces. 
%Existing verification methods, which were once sufficient for simpler tasks, now struggle to effectively monitor and validate the evolving, multi-stage reasoning processes of modern LLMs.

Process-based verifiers (PRMs) are essential for detecting process errors. However, becuase human annotations for process supervision~\cite{Lightman2023LetsVS800k} are prohibitively expensive, researchers increasingly use Monte Carlo estimation methods~\cite{wang2024mathshepherd, luo2024improve_omegaprm} to annotate process labels, even though these labels are noisy and weak~\cite{zhang2025lessons}. Moreover, most verifiers rely on a simplistic "System 1" binary yes/no prediction, which is insufficient for capturing complex process errors.

 %This approach fails to accurately detect process errors. The most widely used approach for combining Process-based Verifiers and Proposor LLMs is the Best-of-N method~\cite{Lightman2023LetsVS800k, zhang2024restmcts}. This technique treats Process-based Verifiers as Outcome Verifiers by only examining the complete reasoning trace. In contrast, when combining with more sophisticated search algorithms like beam search and Monte Carlo Tree Search (MCTS), which require the ability to verify incomplete processes, tend to degrade system performance~\cite{snell2025scaling, zhang2025lessons}.

% The prevailing approach for integrating process-based verifiers with proposer LLMs is the Best-of-N method~\cite{Lightman2023LetsVS800k, zhang2024restmcts}. This technique treats process-based verifiers solely as outcome verifiers by examining only complete reasoning traces. In contrast, when integrating these verifiers with more sophisticated search algorithms, such as beam search and Monte Carlo Tree Search (MCTS), which require the capability to verify incomplete processes, overall system performance tends to degrade~\cite{snell2025scaling, zhang2025lessons}.

% This section discusses the process error detection capabilities of newly released reasoning LLMs.
% In contrast, recently released reasoning LLMs, such as OpenAI O1~\cite{jaech2024openaio1} and DeepSeek R1~\cite{guo2025deepseekr1}, have begun to demonstrate promising process error detection capabilities through reinforcement learning. Their reasoning traces often include metacognitive cues like \textit{“hmm”} and \textit{“wait, let’s check,”} indicating the emergence of a rudimentary process verification mechanism, an \textit{"aha"} moment. However, since process verification was not the primary design objective, these abilities are not always reliable. Moreover, the reliance on a System 2–style self-correction process tends to induce overthinking~\cite{chen2025overthinkingo1like}, which ultimately reduces the efficiency of the search process.
Recently released reasoning LLMs, such as OpenAI O1~\cite{jaech2024openaio1} and DeepSeek R1~\cite{guo2025deepseekr1}, show promise in detecting process errors through reinforcement learning. Their reasoning traces include metacognitive cues (e.g. `hmm', `wait, let's check') that hint at a rudimentary verification mechanism, a kind of `aha' moment. However, since process verification was not the primary design goal, these abilities can be unreliable. Moreover, their reliance on a System 2–style self-correction process often leads to overthinking~\cite{chen2025overthinkingo1like} and reduce efficiency.

% Our work introduces \textit{\textbf{DyVer}} (Dynamic Verification), a cognitively-inspired process verification framework that employs an \textit{Adaptive-Process Verification Mechanism} based on Kahneman's Systems Theory~\cite{kahneman_thinking_2012}. This mechanism comprises two distinct components: \textit{System 1 Verification}, which provides immediate, single-token confirmation for evidently correct steps, and \textit{System 2 Analysis}, which performs a comprehensive error analysis for potential mistakes. This dual-system approach enables dynamic computational budgeting by allowing finetuned reasoning modes to determine which steps require rapid verification and which necessitate more in-depth analysis. 
% %Moreover, we implement \textit{context-aware error recovery mechanisms} that empower DyVer to more effectively guide the proposer LLM during beam search and Monte Carlo Tree Search (MCTS). 

% % This section introduces our step-wise consensus-filtered process supervision technique
% To ensure reliable process labeling, we introduce a novel \textbf{\textit{step-wise consensus-filtered process supervision}} technique. Our method uses Monte Carlo estimation to generate multiple rollouts along with associated labels for each reasoning step, and then leverages an LLM-as-a-Judge~\cite{Gu2024ASOJudge} to evaluate the entire reasoning trace. This is followed by ulitizing a reasoning LLM to perform step-by-step analysis to identify the specific steps that require slower verification. Using this approach, we curate approximately 117k high-quality training examples from 1.2M noisy Monte Carlo rollouts, and our experimental results demonstrate that quality, rather than quantity, is crucial for effective process-based verifier training.

Our work introduces \textbf{\textit{Dyve}} (\textbf{Dy}namic Process \textbf{Ve}rifier), a specialized reasoning language model that dynamically detects process errors using fast and slow thinking, inspired by Kahneman's Systems Theory~\cite{kahneman_thinking_2012}. For reasoning traces from step 1 to $t$, Dyve adaptively applies either \textbf{\textit{System 1}}, which supplies single-token confirmation for clearly correct steps, or \textbf{\textit{System 2}} for deeper analysis to complex ones. 
%This dual approach enables dynamic computational budgeting by assigning rapid verification to simple steps and deeper analysis to complex ones. 
To support this adaptive mechanism, we introduce a novel \textbf{\textit{step-wise consensus-filtered process supervision}} technique. Our method leverages Monte Carlo estimation to generate multiple rollouts per query, uses an LLM-as-a-Judge~\cite{Gu2024ASOJudge} to assess the full reasoning trace, and employs a reasoning LLM for step-by-step analysis to flag steps that require further verification. In doing so, we curate approximately 117K high-quality training examples from 1.2M noisy Monte Carlo rollouts, demonstrating that quality, not quantity, is key to effectively train an process-based verifier.

Experimental results on ProcessBench~\cite{zheng2024processbench} show that Dyve significantly outperforms existing PRMs and other reasoning models in detecting process errors in complete or incomplete reasoning traces. Furthermore, when combined with a proposer language model, Dyve yields better performances under Best-of-N then other PRMs.

% Our contributions include:

% \begin{itemize}
%     \item We introduce \textbf{DyVer}, a dynamic process verification framework that combines rapid System 1 validation with comprehensive System 2 correction and context-aware error recovery.
%     \item We propose a novel \textbf{\textit{step-wise consensus-filtered process supervision}} technique that employs Monte Carlo estimation and an LLM-as-a-Judge to reliably label each reasoning step.
%     \item We demonstrate that DyVer outperforms existing PRMs and LLM-as-judges on ProcessBench by improving search performance during beam search and MCTS while achieving greater efficiency with XX\% System 1 verification.
% \end{itemize}

\section{Related Work}

% \paragraph{Reward Model for LLM Mathematical Reasoning}
% Recently reasoning LLMs \cite{chen2024step,} leverage external verifiers to select the best reasoning path from multiple decoded candidates, thereby improving reasoning quality and reliability.
% Recent research\cite{setlur2024rewardingprogressscalingautomated,wang2024mathshepherd,guan2025rstar} indicates leveraging external reward models as verifiers to select the best reasoning path from multiple decoded candidates can improve reasoning in large language models.
% The Outcome Reward Model (ORM) \cite{cobbe2021ORM,yang2024qwen2.5math} assigns rewards based on the final output rather than intermediate steps, optimizing for end-task performance. \textcolor{blue}{However, this focus on terminal outputs neglects the intermediate reasoning steps that are crucial for tasks requiring fine-grained logical consistency. } 

% \textcolor{blue}{To mitigate this issue}, Process Reward Models (PRMs) \cite{lightman2023lets_verify,zhang2025lessons,wang2024mathshepherd} evaluate step-by-step reasoning, making them more suitable for tasks that require fine-grained assessment of logical consistency.
% Generative Verifiers (GenRM) \cite{zhang2024generativeverifiersrewardmodeling} extend traditional verifiers by jointly verifying and generating solutions using the next-token prediction objective, which also integrates chain-of-thought (CoT)\cite{wei2022cot} reasoning. However, GenRM is computationally more expensive than ORM and PRM, as it generates verification rationales instead of providing a direct "Yes" or "No" classification for correctness.
% \textcolor{blue}{To mitigate such issue, Process Reward Models (PRMs) \cite{lightman2023lets_verify,zhang2025lessons,wang2024mathshepherd} rapidly output a binary yes/no for each reasoning step. While this offers granular insights, PRMs perform best on complete traces, because of the lack of deeper analysis. In contrast, Generative Verifiers (GenRM) \cite{zhang2024generativeverifiersrewardmodeling} combine chain-of-thought (CoT) reasoning with next-token predictions to both verify and generate solutions \cite{wei2022cot}. However, their high computational cost and reliance on detailed verification rationales, as opposed to direct correctness predictions, make them less suitable for time-critical applications.}

%To balance computational efficiency and verification performance, we propose the Dynamic Verifier (DyVer), which aims to optimize both reasoning accuracy and resource utilization by combining GenRM and PRM.
% \textcolor{blue}{In contrast, our DyVer framework merges the strengths of PRMs and GenRMs using Kahneman’s dual-system theory: a rapid, token-level System 1 check for clear cases coupled with a detailed System 2 review for potential errors.}

Recent research~\cite{setlur2024rewardingprogressscalingautomated,wang2024mathshepherd,guan2025rstar} shows that external reward models can improve LLM reasoning by selecting the best path from multiple candidates. Outcome Reward Models (ORMs)~\cite{cobbe2021ORM,yang2024qwen2.5math} optimize for final outputs but overlook vital intermediate steps. Process Reward Models (PRMs)~\cite{lightman2023lets_verify,zhang2025lessons,wang2024mathshepherd} provide rapid binary validations for each step, yet struggle with a deeper analysis of incomplete traces. In contrast, Generative Verifiers (GenRMs)~\cite{zhang2024generativeverifiersrewardmodeling} combine chain-of-thought reasoning with next-token predictions to verify and generate solutions, although at a high computational cost. To balance these trade-offs, our DyVe framework merges the strengths of PRMs and GenRMs using Kahneman's dual system theory.
%: a fast, token-level System 1 check for clear cases coupled with a detailed System 2 review for potential errors.



% \paragraph{Step-level Mathematical Reasoning Data Generation with Monte Carlo Estimation}
%Recent advancements in step-level mathematical reasoning data generation have improved multi-step problem-solving by leveraging reward models and process supervision with Monte Carlo estimation for enhanced accuracy and scalability. 
%\textcolor{blue}{High-quality step-level supervision is critical for training effective process verifiers. Recent works have leveraged Monte Carlo estimation to generate such data. For example, } OmegaPRM \cite{luo2024improve_omegaprm} automates the collection of process supervision data by a novel divide-and-conquer style Monte Carlo Tree Search (MCTS) algorithm, \textcolor{blue}{thereby lifting the need of} manual annotations.
%ReST-MCTS* \cite{zhang2024restmcts} introduces a self-training approach that combines MCTS with process reward guidance to collect high-quality reasoning traces, enhancing the fine-tuning of policy and reward models for improved accuracy.
%\textcolor{blue}{Similarly, ReST-MCTS* \cite{zhang2024restmcts} leverages MCTS with process reward guidance to curate reasoning traces. Since Monte Carlo estimation yields noisy annotations—and \citet{zheng2023judging} show that LLM-as-a-Judge outperforms PRMs for error detection—\citet{zhang2025lessons} employs consensus filtering between LLM-as-a-Judge~\cite{Gu2024ASOJudge} and a critic model to remove noise. We extend this approach with step-wise consensus filtered process supervision, using a reasoning LLM to annotate each filtered sample step-by-step and generate high-quality step-level data. This refined strategy is key to DyVer, as it reliably distinguishes steps that require rapid validation from those needing deeper analysis.}
High-quality step-level supervision is crucial for training process verifiers, yet human annotations (e.g., PRM800k~\cite{Lightman2023LetsVS800k}) are prohibitively expensive. To avoid this, OmegaPRM~\cite{luo2024improve_omegaprm} employs a divide-and-conquer Monte Carlo Tree Search (MCTS) to generate annotations, although our experiments show that these labels are often noisy and weak. To address this issue, we adopt consensus filtering with an LLM-as-a-Judge~\cite{Gu2024ASOJudge} to eliminate unreliable samples~\cite{zhang2025lessons}, and further extend this approach with step-wise flagging, where a reasoning LLM conducts step-by-step analysis to identify steps that require System 2 verification.



 


% Math-shepherd \cite{wang2024mathshepherd} provides a process-oriented math reward model that uses automatically generated supervision data to assign step-level rewards, improving both output verification and reinforcement learning for LLMs..


%\subsection{Process-based Verifiers Guided Search}

%Recent research \cite{snell2025scaling} shows that applying a “compute-optimal” scaling strategy can significantly improve test-time compute efficiency, surpassing the performance of the best-of-N baseline.
%Tree of Thoughts (ToT) \cite{yao2024treeofthought} extends Chain-of-Thought reasoning by enabling structured exploration over multiple reasoning paths, outperforming best-of-N by considering alternative intermediate steps and evaluating the most promising paths.
%SVPO \cite{chen2024step} extends Direct Preference Optimization (DPO)\cite{rafailov2024dpo} by using MCTS \cite{browne2012mcts_survey} to annotate step-level preferences, resulting in improved model performance on complex reasoning tasks.
%rStar-Math \cite{guan2025rstar} demonstrates that small language models can achieve state-of-the-art math reasoning by incorporating MCTS-guided deep thinking, along with innovations in data synthesis and process reward modeling to iteratively improve reasoning. Inspired by these advancements, we extend DyVer for LLM guided search during reasoning inference, leveraging process-based verifiers to enhance step-level decision-making.
%\textcolor{blue}{Recent research \cite{snell2025scaling} demonstrates that a “compute-optimal” strategy can be achieved by combining a proposer LLM with a process verifier. However, due to limitations in current System 1 verifiers, the best-of-\(N\) approach remains optimal for medium to hard problems. More advanced searches, such as Beam Search~\cite{yao2024treeofthought} and MCTS, often exploit spurious features from noisy Monte Carlo estimation. In contrast, rStar-Math \cite{guan2025rstar} shows that small language models can achieve competitive math reasoning by incorporating a specialized program-of-thought (PoT) verifier. These findings indicate that reliable verification is vital for reasoning systems, though efficiency remains a challenge. This motivates our proposal of DyVer Search, which integrates context-aware error recovery mechanisms to balance performance and efficiency.}


% \begin{figure*}[t]
%     \centering
%     \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{images/pipeline.pdf}}
%     % \includegraphics[width=\linewidth]{latex/figs/framework.pdf} \hfill
%     \caption{Comparison of Different Process Verification Approaches. Top: Reasoning LLM with self-reflection shows instability in verification. Middle: Traditional verification approaches (GenRM and GenRM-CoT) with static offline verification. Bottom: Our proposed DyVer framework demonstrating dynamic online verification with adaptive parallel reflection and efficient error correction capabilities.}
%     \label{fig:pipeline}
% \end{figure*}


% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.48\linewidth]{images/tt-scaling.png} \hfill
%     \includegraphics[width=0.42\linewidth]{images/inference_speed_comparison.png}
%     \caption{(\textbf{Left}) Comparison of Dyve and various process verifiers when combined with Best-of-N. (\textbf{Right}) Inference speed comparison by dataset and model, illustrating time per sample in seconds for System-1, Dyve, and R1-14B across GSM8K, MATH, OlympiadBench, and OmniMATH.}
%     \label{fig:inference}
% \end{figure*}


\section{Method}

\subsection{Overview}
%Inspired by Kahneman’s dual-system theory, we propose \textbf{DyVer}, which decouples verification into two specialized modules. (1) A fast, token-level System 1 quickly validates clear steps, while a dedicated System 2 performs comprehensive error analysis on ambiguous cases. Our novel (2) step-wise consensus-filtered process supervision pipeline curates high-quality step-level data from noisy Monte Carlo rollouts, and (3) context-aware error recovery mechanisms dynamically guide verification during sophisticated guided search (e.g., beam search and MCTS). This dual-system design balances efficiency and accuracy in process verification.
Dyve can assess the correctness of multi-step reasoning trace generated by a language model. Given a problem \(P\) and its reasoning steps \(\{s_1, s_2, \ldots, s_T\}\), Dyve sequentially verifies each step:
\[
r_t = \text{Dyve}(s_{1:t}; \theta)
\]
where the response \(r_t\), varying from 1 to 8192 tokens based on System 1 or System 2 usage, is parsed by \(\text{Parse}(\cdot)\) to yield a binary outcome. If \(\text{Parse}(r_t) = 0\), the process halts, returning the erroneous step index and intermediate generations; otherwise, verification proceeds to the next step.

\begin{figure}
    \centering
    \includegraphics[width=1.1\linewidth]{images/pipeline.pdf}
    \caption{(1) LLM self-reflection is unreliable (2) Binary verification lacks depth, (3) Chain-of-Thought (CoT) verification is deeper but more expensive, (4) GenRM with CoT combines generation and verification without step-wise assessment, (5) Dyve, our proposed framework that dynamically combines fast System 1 and deep System 2 verification.}
    \label{fig:enter-label}
    \vspace{-15pt}
\end{figure}

\subsection{Step-wise Consensus-Filtered Process Supervision}
We introduce a novel step-wise consensus-filtered process supervision technique to enable adaptive verification within Dyve. The pipeline includes:

\paragraph{Queries Collection}
We gather query-response pairs from datasets like GSM8k~\cite{cobbe2021gsm8k} and MATH~\cite{Hendrycks2021MeasuringMP}, totaling 15K queries. 

\paragraph{Monte Carlo Rollouts Generation}
Using OmegaPRM~\cite{luo2024improve_omegaprm}, we generate 20 rollouts per query. We also gather open-souce PRM data from MathShepherd~\cite{wang2024mathshepherd} and RLHFlow, excluding PRM800k~\cite{Lightman2023LetsVS800k} to prevent data leakage, yielding approximately 1.2 million positive and negative rollouts with noisy labels.

\paragraph{Consensus Filtering with LLM-as-Judges}
We prompt DeepSeek V3 to verify the initial error steps identified by OmegaPRM. This filtering removes about 50\% of noisy rollouts. We then create a dataset of 117K high-quality examples by re-balancing the number of positive and negative step labels.

\paragraph{Step-Level Analysis with Reasoning LLMs}
A reasoning model performs step-by-step analysis on curated rollouts. Correct steps are marked with a “+” token, while uncertain steps undergo further detailed evaluation, ensuring alignment with high-quality reasoning traces.

\subsection{Training}
We train the {deepseek-ai/DeepSeek-R1-Distill-Qwen-14B} model using supervised fine-tuning on our curated dataset. This enables the model to learn rapid System 1 verification and comprehensive System 2 correction. The training objective minimizes the cross-entropy loss:

\begin{equation}
\mathcal{L}(\theta) = -\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T^{(i)}} \log p_\theta \left( y_t^{(i)} \mid x^{(i)}, y_{<t}^{(i)} \right),
\end{equation}
where \(\theta\) indicates the model parameters, \(x^{(i)}\) is the input query, and \(y^{(i)}\) is the target label for the \(i\)-th example.


% \subsection{Step-wise Consensus-Filtered Process Supervision}
% We propose a step-wise consensus-filtered process supervision technique as a data preparation pipeline designed to enable adaptive System 1 and System 2 verification within DyVer. The pipeline consists of the following stages:

% \paragraph{Raw Data Collection}
% We collect query-response pairs from widely used datasets such as GSM8k~\cite{cobbe2021gsm8k} and MATH~\cite{Hendrycks2021MeasuringMP}, which contribute 15k pairs of problems and golden answers. For process-based reward model (PRM) data, we further curate open-source datasets from MathShepherd~\cite{wang2024mathshepherd} and RLHFlow, while deliberately excluding PRM800k~\cite{Lightman2023LetsVS800k} to prevent data leakage with the MATH test set. \textcolor{blue}{We leverge open-source datasetgenerated by various models, becuase we want DyVer to profiling capapbility of all models to have good generalization.}

% \paragraph{Monte Carlo Rollouts Generation}
% We employ OmegaPRM~\cite{luo2024improve_omegaprm} to generate 20 rollouts and corresponding process labels for each query. OmegaPRM automates the collection of process supervision data using a novel divide-and-conquer Monte Carlo Tree Search (MCTS) algorithm that leverages binary search for efficient value estimation. \textcolor{blue}{We use that to effectively yield approximately 2M postive and negative rollouts with noisy labels. We encourage the reader to read ~\ref{sec:effmcts} for more detail.}


% \paragraph{Consensus Filtering with LLM-as-Judges}
% We then use an LLM-as-a-Judge, specifically DeepSeek V3, to verify whether the initial error step identified by OmegaPRM is truly incorrect. If the judgment confirms the error, the rollout is retained; otherwise, it is discarded. 
% % This filtering procedure removes roughly 90\% of the noisy rollouts, resulting in approximately 230k high-quality examples.
% %zeju:  why 90\%? balance! with query statistic
% Through this filtering process, approximately 50\% of noisy rollouts are removed. However, we observe an imbalance in step labels (+ vs. -) with a ratio of 5:1. To address this, we rebalance the step labels, resulting in a final dataset of 230K high-quality examples, covering 79K unique math and scientific problems.

% \paragraph{Step-Level Analysis with Reasoning LLMs}
% Finally, we apply a reasoning model to perform a detailed, step-by-step analysis of the curated rollouts. Correct steps are marked with a “+” token, while steps deemed uncertain trigger an in-depth analysis; only if the detailed evaluation aligns with the high-quality reasoning trace is the step retained. This process facilitates adaptive verification by selectively applying more thorough scrutiny where necessary. \textcolor{blue}{Reader may refer to Appendix~\ref{sec:DataExp}} for example training data.

% \subsection{DyVer Training}

% We train the {deepseek-ai/DeepSeek-R1-Distill-Qwen-14B} model using supervised fine-tuning (SFT) on a dataset curated by our step-wise consensus-filtered process supervision pipeline. This dataset, consisting of approximately 200k high-quality examples, enables the model to learn rapid System 1 verification for clearly correct steps as well as comprehensive System 2 correction for deeper error analysis. Our training objective is to minimize the cross-entropy loss between the model’s predictions and the curated ground-truth labels. Specifically, given a dataset \(\mathcal{D} = \{(x^{(i)}, y^{(i)})\}_{i=1}^{N}\), the loss function is defined as:

% \begin{equation}
% \mathcal{L}(\theta) = -\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T^{(i)}} \log p_\theta \left( y_t^{(i)} \mid x^{(i)}, y_{<t}^{(i)} \right),
% \end{equation}

% where \(\theta\) denotes the model parameters, \(x^{(i)}\) represents the input query, and \(y^{(i)}\) is the target reasoning process label for the \(i\)-th example.

%\subsection{DyVer Search with Context-Aware Error Recovery}

%\paragraph{Context-Aware Error Recovery Mechanism}
%To enhance inference performance, we design context-aware error recovery mechanisms that dynamically integrate with sophisticated search strategies. As illustrated in Figure~\ref{fig:pipeline}, DyVer monitors incomplete reasoning paths from the proposer LLM and determines whether to employ System 1 or System 2 verification. Upon error detection, it appends detailed analysis to help the reasoning framework recover from potential mistakes, enabling adaptive course correction during the reasoning process.

%\paragraph{DyVer Search}
%We design DyVer Search to efficiently estimate MCTS values by leveraging our dual-system verification approach. Inspired by OmegaPRM~\cite{luo2024improve_omegaprm}, we extend their method by replacing binary search with DyVer's more precise error detection capabilities. Since DyVer is trained on our carefully curated dataset, it provides more accurate step-level verification than traditional binary search, leading to more efficient MCTS value estimation and improved search performance. The detailed process of DyVer Search is illustrated in Algorithm~\ref{alg:DyverSearch}.


% \begin{algorithm}[H]
% \SetAlgoLined
% \caption{DyVerSearch with Context-Aware Error Recovery}
% \label{alg:DyverSearch}
% \textbf{Input:} 
% $Q$: Question string,
% $A$: Expected answer,
% $N$: Maximum iterations,
% $R$: Rollout budget,
% $LM$: Proposer LM,
% \textbf{Output:} Collected solution paths
% \BlankLine
% $s_0 \gets$ State($Q$)\tcp*[r]{init tree $T$}
% $n \gets 1$\;
% $\mathcal{C} \gets s_0$\tcp*[r]{init candidate pool}
% \BlankLine
% \textbf{while} {$(n < N)$\ \textbf{and} $(|r| < R)$ \textbf{and} $(\mathcal{C} \neq \emptyset)$}{
%     \BlankLine
%     $(s, r) \gets$ SelectionPhase()\;
    
%     \If{$s$ is None}{
%         \textbf{break}\;
%     }
    
%     \BlankLine
%     $e \gets$ DyVer($s$, $\theta$)\tcp*[l]{Context-Aware Error Recovery}
    
%     \If{$e$ is detected}{
%          expand($s$, $e$) \tcp*[r]{Refine $s$ at the identified error step}
%     }
    
%     Update search tree $T$ with refined $s$\;
% }
% \BlankLine

% $data \gets$ Extract solution paths from $T$\;
% \Return $data$\;
% \end{algorithm}

\begin{table*}[h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{llcccc}
\hline
Model                                               &                                        & \multicolumn{1}{l}{GSM8K} & \multicolumn{1}{l}{MATH} & \multicolumn{1}{l}{OlympiadBench} & \multicolumn{1}{l}{OmniMATH} \\ \hline
\multicolumn{1}{l|}{Qwen2.5-Math-7B-PRM}        & \multicolumn{1}{l|}{System1}           & 39.4$^*$                     & 52.2$^*$                     & 39.4$^*$                              & 33.1$^*$                         \\
\multicolumn{1}{l|}{Math-Shepherd-PRM-7B}           & \multicolumn{1}{l|}{System1}           & 47.9                      & 29.5                     & 24.8                              & 23.8                         \\
\multicolumn{1}{l|}{RLHFlow-PRM-Mistral-8B}         & \multicolumn{1}{l|}{System1}           & 50.4                      & 33.4                     & 13.8                              & 15.8                         \\
\multicolumn{1}{l|}{RLHFlow-PRM-Deepseek-8B}        & \multicolumn{1}{l|}{System1}           & 38.8                      & 33.8                     & 16.9                              & 16.9                         \\
\multicolumn{1}{l|}{Skywork-PRM-1.5B}               & \multicolumn{1}{l|}{System1}           & 59.0                      & 48.0                     & 19.3                              & 19.2                         \\
\multicolumn{1}{l|}{Skywork-PRM-7B}                 & \multicolumn{1}{l|}{System1}           & 64.1$^*$                       & 43.2$^*$                     & 16.2$^*$                              & 17.9$^*$                         \\ \hline
\multicolumn{1}{l|}{Llama-3.1-8B-Instruct}          & \multicolumn{1}{l|}{LLM-as-Judge}      & 27.5$^*$                      & 26.7$^*$                     & 18.5$^*$                              & 19.2$^*$                         \\
\multicolumn{1}{l|}{GPT-4o}                         & \multicolumn{1}{l|}{LLM-as-Judge}      & 61.9$^*$                      & 53.9$^*$                     & 48.3$^*$                              & 44.6$^*$                         \\
\multicolumn{1}{l|}{QwQ-32B-Preview}                        & \multicolumn{1}{l|}{LLM-as-Judge}      & 62.3$^*$                      & 52.7$^*$                     & 46.2$^*$                              & 43.9$^*$                         \\
\multicolumn{1}{l|}{DeepSeek-R1-Distill-Qwen-14B} & \multicolumn{1}{l|}{LLM-as-Judge}      & 67.3$^*$                      & 38.8$^*$                     & 29.9$^*$                              & 32.1$^*$                         \\ \hline
\multicolumn{1}{l|}{\textbf{Dyve 14B}}                  & \multicolumn{1}{l|}{System1 + System2} & \textbf{68.5}             & \textbf{58.3}            & \textbf{49.0}                     & \textbf{47.2}                \\ \hline
\end{tabular}
}
\caption{Performance comparison on ProcessBench. F1 scores, computed from accuracies on erroneous and correct samples, are reported for four benchmarks: GSM8K, MATH, OlympiadBench, and OmniMATH. Dyve 14B leverages a dual reasoning approach (fast System1 and slow System2) to achieve superior performance, with scores of 68.5, 58.3, 49.0, and 47.2, respectively, and it shows enhanced generalization on Olympiad-level mathematics. Models marked with a $^*$ are evaluated using our custom implementation to align with our experimental settings in the absence of an official evaluation script.}
\label{tab:value_head_prm}
\end{table*}

\section{Experiments}
To evaluate Dyve's capabilities, we conduct experiments in two main areas. First, we assess Dyve's ability to identify process errors. Second, we integrate Dyve with Proposer LLMs using a Best-of-N approach to evaluate its synergy within a reasoning framework. All experiments are conducted on 8 $\times$ NVIDIA A800-SXM4-80GB GPUs. Interested Readers may refer to Appendix ~\ref{sec:expSetup} for detailed experimental setup.

\subsection{Benchmarks}

\paragraph{ProcessBench}~\cite{zheng2024processbench} comprises four sets of test data derived from GSM8K~\cite{cobbe2021gsm8k}, MATH~\cite{Hendrycks2021MATH}, OlympiadBench~\cite{he2024olympiadbench}, and OmniMATH~\cite{Gao2024OmniMATHAU}. It includes 3,400 test cases, covering high-school to Olympiad-level math problems. Each case provides a step-by-step solution with error locations annotated by experts. \textit{Models are given \(s_{1:t}\), from the first to the last step, and must identify the earliest error or confirm that all steps are correct.} For each ProcessBench subset, we calculate the accuracies for erroneous and correct samples and compute their harmonic mean as the F1 score.

\paragraph{MATH-500} \cite{Lightman2023LetsVS800k} evaluates Dyve's integration with a Proposer LLM. We measure performance using maj@k and rm@k metrics as defined in \cite{yang2024qwen2.5math} and apply a Best-of-N decoding strategy. Due to inconsistent results from different evaluation tools, we manually verified all reported outcomes.


\subsection{Processbench}
% \subsubsection{Setup}
% For each subset of ProcessBench, we compute the accuracy on both erroneous and correct samples, and report the harmonic mean of these values as the F1 score. The F1 score is our primary evaluation metric as it balances the trade-off between being overly critical and failing to identify errors, ensuring that models can effectively handle both types of errors while maintaining overall reasoning accuracy. 
% ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts.
% Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. 


% For each subset of ProcessBench, we calculate the accuracies on erroneous and correct samples,
% respectively, and additionally compute their harmonic mean as the F1 score. We primarily refer to F1 scores to compare model performance, as it balances model behaviors between being overly critical and being incapable of identifying errors.
 
% \subsubsection{Baselines}
% We evaluate our model against several open-source PRMs, including Math-Shepherd, Qwen2.5-Math-7B-PRM800K, and Skywork-PRM, as well as zero-shot large models using critic prompts, such as GPT-4, QwQ 32B, and Deepseek-R1-Distill-Qwen-14B. 


% \begin{table*}[h]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{l|c|c|c|c}
%         \toprule
%         \hline
%         Model & GSM8K & MATH & OlympiadBench & OmniMATH \\
%         \hline
%         \midrule
%         Math-Shapherd (llama) & 49.8 & 33.4 & 13.5 & 15.8 \\
%         Qwen2.5-Math-7B-PRM800K & 39.4 & 52.2 & 39.4 & 33.1 \\
%         Math-Shepherd-PRM-7B & 47.9 & 29.5 & 24.8 & 23.8  \\
%         RLHFlow-PRM-Mistral-8B & 50.4 & 33.4 & 13.8 & 15.8  \\
%         % RLHFlow-PRM-Deepseek-8B & 38.8 & 33.8 & 16.9 & 16.9  \\
%         Skywork-PRM-1.5B & 59.0 & 48.0 & 19.3 & 19.2  \\
%         Skywork-PRM-7B & 70.8 & 53.6 & 22.9 & 21.0  \\
%         \hline
%         Zero Shot (GPT-4o) & 61.9 & 53.9 & 50.4 & 44.6 \\
%         Zero Shot (QwQ 32B) & 62.3 & 52.7 & 46.2 & 43.9 \\
%         \hline
%         Zero Shot (Llama 3.1 8b Instruct) & 27.5 & 26.7 & 18.5 & 19.2 \\
%         DyVer System1 (unfilter data) + Llama 3.1 8b Instruct & 35.6 & 28.3 & 10.9 & 13.3 \\
%         % filtered (llama ep1) & 49.3 & 40.2 & 22.3 & 22.4 \\
%         DyVer System1 + Llama 3.1 8b Instruct & 53.1 & 44.1 & 26.9 & 25.7 \\
%         \hline
%         Zero Shot (DeepSeek-R1-Distill-Qwen-14B) & 67.3 & 38.8 & 29.9 & 32.1 \\
%         DyVer System1 (unfilter data) + DeepSeek-R1-Distill-Qwen-14B & 59.1 & 34.7 & 15.0 & 11.2 \\
%         DyVer System1 + DeepSeek-R1-Distill-Qwen-14B & 66.3 & 56.0 & 36.1 & 37.7 \\
%         DyVer System2 + DeepSeek-R1-Distill-Qwen-14B & \textbf{68.5} & \textbf{58.3} & \textbf{49.0} & \textbf{47.2} \\
%         \hline
%         \bottomrule
%     \end{tabular}
%     }
%     \caption{Performance comparison on ProcessBench. We report the F1 score of the respective accuracies on erroneous and correct samples.}
%     \label{tab:value_head_prm}
% \end{table*}



% \begin{table*}[h]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{l|c|c|c|c|c}
%         \toprule
%         \hline
%         Methods & 2^0 & 2^1 & 2^2 & 2^3 & 2^4\\
%         \hline
%         \midrule
        % Best\_of\_N + DeepSeek-R1-Distill-Qwen-1.5B &  &  &  &  & \\
        % Best\_of\_N + DeepSeek-R1-Distill-Qwen-7B &  &  &  &  & \\
        % Best\_of\_N + DeepSeek-R1-Distill-Qwen-14B &  &  &  &  & \\
        % \hline
        % DyVer System1 + DeepSeek-R1-Distill-Qwen-1.5B &  &  &  &  & \\
        % DyVer System1 + DeepSeek-R1-Distill-Qwen-7B &  &  &  &  &  \\
        % DyVer System1 + DeepSeek-R1-Distill-Qwen-14B &  &  &  &  &  \\
        % \hline
        % DyVer (System1 + System2) + DeepSeek-R1-Distill-Qwen-1.5B &  &  &  &  & \\
        % DyVer (System1 + System2) + DeepSeek-R1-Distill-Qwen-7B &  &  &  &  &  \\
        % DyVer (System1 + System2) + DeepSeek-R1-Distill-Qwen-14B &  &  &  &  &  \\
        % \hline
        % Skywork-PRM-7B + DeepSeek-R1-Distill-Qwen-1.5B &  &  &  &  & \\
        % Skywork-PRM-7B + DeepSeek-R1-Distill-Qwen-7B &  &  &  &  & \\
        % Skywork-PRM-7B + DeepSeek-R1-Distill-Qwen-14B &  &  &  &  & \\
%         \hline
%         \bottomrule
%     \end{tabular}
%     }
%     \caption{xxx}
%     \label{tab:bestofn}
% \end{table*} 


\paragraph{Results and Analysis}
Dyve achieves the highest F1 scores across all benchmark subsets, outperforming all baselines. Despite being trained primarily on high-school and college-level mathematics, its dual reasoning system generalizes effectively to Olympiad-level problems. In contrast, LLM-as-Judge with DeepSeek-R1-Distill-Qwen-14B shows weaker performance on OlympiadBench and OmniMATH, indicating less reliable process error detection.

% \vspace{-15pt}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/inference_speed_comparison.png}
    % \vspace{5pt}
    \caption{Inference speed comparison on ProcesBench, time per sample in seconds, for System-1, Dyve, and DeepSeek-R1-14B.}
    \label{fig:time}
    \vspace{10pt}
\end{figure}
% \vspace{-20pt}

\paragraph{Camparison on Inference Time}
According to Figure~\ref{fig:time}, the inference speed comparison in ProcesBench, highlights model efficiency. System-1 is the fastest, maintaining minimal latency. Dyve, slightly slower, balances speed and performance, excelling in complex datasets like OlympiadBench and OmniMATH. R1-14B has the longest inference times, suggesting a bottleneck for rapid processing. This analysis highlights Dyve's ability to deliver competitive performance with efficient inference times, making it well-suited for applications demanding both accuracy and speed.

\begin{figure}[t]
    \vspace{-10pt}
    \centering
    \includegraphics[width=\columnwidth]{images/side_by_side_base_and_ablation14B.png}
    \caption{Impact of model choice and step-wise consensus filtering on performance across GSM8K, MATH, OlympiadBench, and OmniMATH. The figure illustrates improvements achieved through consensus filtering and step-wise flagging, highlighting the superior performance of the 14B reasoning model over the 7B Llama.}
    \label{fig:abl}
    \vspace{-15pt}
\end{figure}

\paragraph{Model Choice and Step-wise Consensus Filtering}
The ablation study in Figure~\ref{fig:abl} illustrates the impact of model selection and step-wise consensus filtering in ProcessBench. For Llama-3.1-8B-Instruct, consensus filtering significantly improves performance, boosting scores from 35.6 to 49.3 on GSM8K and from 28.3 to 40.2 on MATH. Similarly, DS-R1-Distill-Qwen-14B sees substantial gains, with MATH scores increasing from 34.7 to 56.0 and OmniMATH from 11.2 to 37.7. Step-wise flagging further amplifies performance, achieving scores of 68.5 on GSM8K and 58.3 on MATH. These results underscore the effectiveness of these techniques and highlight the superior reasoning capabilities of the 14B model compared to the 7B Llama, validating our choice of DeepSeek-R1-Distill-Qwen-14B.

% We evaluate Dyve 14B on four benchmarks: GSM8K, MATH, OlympiadBench, and OmniMATH, with F1 scores for both correct and erroneous samples. Dyve 14B outperforms all baselines, achieving 68.5 on GSM8K, 58.3 on MATH, 49.0 on OlympiadBench, and 47.2 on OmniMATH. In comparison, Skywork-PRM-7B performs well on GSM8K (70.8) and MATH (53.6), but Dyve 14B surpasses it on OlympiadBench and OmniMATH, demonstrating the effectiveness of our dual approach, combining fast System 1 and slow System 2 reasoning.

% Zero-shot models like GPT-4o and QwQ-32B-Preview perform well on GSM8K, but their performance drops on more complex tasks like OlympiadBench and OmniMATH. DeepSeek-R1-Distill-Qwen-14B also performs well on GSM8K (67.3) but struggles on other benchmarks, reinforcing the limitations of zero-shot models in multi-step reasoning tasks. Dyve 14B consistently outperforms both process-based and zero-shot models, particularly on MATH (58.3) and OlympiadBench (49.0), confirming the benefit of our adaptive reasoning approach.

% While Dyve 14B excels across most benchmarks, it faces challenges with OmniMATH, suggesting room for improvement in handling even more complex reasoning tasks. Overall, Dyve 14B sets a new benchmark in mathematical reasoning error detection, demonstrating significant advancements over existing models.


% In this experiment, we compare the performance of various models on four mathematical reasoning benchmarks: GSM8K, MATH, OlympiadBench, and OmniMATH. The table reports F1 scores for each model, which reflect the accuracy on both correct and erroneous samples. Our analysis focuses on the performance of our DyVer comparing with our different baselines across the benchmark. Our experiments show that DyVer consistently outperforms other baselines on ProcessBench.

% Among the PRMs, Skywork-PRM-7B achieves the best performance, with F1 scores of 70.8 on GSM8K and 53.6 on MATH. However, DyVer fine-tuned by our filtered dataset, surpasses these models, achieving 66.3 on GSM8K and 56.0 on MATH, showcasing the effectiveness of our filtering approach. This demonstrates our method improves the model’s reasoning accuracy, especially on complex ProcessBench.

% To further prove the ability of DyVer, we choose large models with critic prompt, such as GPT-4 and QwQ 32B, show competitive performance but are still outperformed by DyVer. For example, GPT-4 achieves an F1 score of 61.9 on GSM8K, but DyVer improves to 66.3. Similarly, on MATH, GPT-4 scores 53.9, while our model achieves 56.0, highlighting the advantage of DyVer fine-tuning with our dataset.


\begin{figure}[h]
    \vspace{-20pt}
    \centering
    \includegraphics[width=1\columnwidth]{images/accuracy_vs_exponent.png}
    \caption{Comparison of Dyve, Dyve System1 and Majority Vote with different generation budget when integrating with Proposer LLMs (DeepSeek-R1-Distill-Qwen-14B as solid line, Qwen2.5-MATH-7B-Instruct as dotted line).}
    \label{fig:integarte_dyve}
    \vspace{-15pt}
\end{figure}


\subsection{Integrating Dyve with Proposer LLMs}
We integrate Dyve as a process verifier to assist Proposer LLMs (Qwen-Math-7B and Deepseek-R1-Distill-Qwen-14B) on MATH-500. For fairness, we compare three setups across Best-of-N (N = 1, 2, 4, 8) decoding settings: Dyve verification, System 1 only, and Majority Vote (no verification).

\vspace{-5pt}
\paragraph{Results and Analysis}
% As illustrated in Figure \ref{fig:integarte_dyve}, integrating Dyve with Best-of-N results in substantial performance gains compared to System 1 only. This highlights the effectiveness of combining both fast and slow reasoning, improving the accuracy of outputs generated by Proposer LLMs. The Best-of-N approach further enhances this effect by selecting the most accurate reasoning path, showcasing the synergy between the verifier and proposer models. This experiment emphasizes the potential of leveraging Dyve's dual reasoning framework alongside Best-of-N to advance performance in complex reasoning tasks.
As shown in Figure \ref{fig:integarte_dyve}, Dyve's combination of fast and slow verification outperforms both Majority Voting and System 1 verification when integrated with Best-of-N decoding. When the generation budget is N = 8, Dyve with DeepSeek-R1-Distill-Qwen-14B achieves 95.5\% accuracy, while Dyve with Qwen2.5-MATH-7B-Instruct reaches 90.4\%, outperforming both baselines.
This demonstrates how our dual-system with fast and slow thinking, approach effectively guides Proposer LLMs to select more accurate reasoning paths, showcasing the synergy between the Dyve and proposer models.


% As shown in Figure \ref{fig:integarte_dyve}, integrating Dyve with Best-of-N decoding leads to significant performance gains over both Majority Voting and System 1 only.  These results highlight how Dyve’s dual-system verification—combining fast token-level confirmation and slow step-by-step reasoning—effectively guides Proposer LLMs toward selecting more accurate reasoning paths. This demonstrates the synergistic effect of integrating Dyve with Proposer LLMs, further validating its role in enhancing structured reasoning.


% \vspace{-5pt}
\section{Conclusion}
% \vspace{-5pt}
%In this study, we demonstrate the effectiveness of Dyve in process verification mathematical reasoning, outperforming existing models through a dual reasoning approach. The consensus filtering and step-wise flagging significantly enhanced model accuracy and robustness. Our ablation study confirmed the superiority of the 14B model over smaller counterparts, justifying its selection for complex reasoning tasks. These advancements highlight Dyve's potential for applications requiring both precision and efficiency in error detection and reasoning.
Our study demonstrates Dyve's, with a dual reasoning approach, superior performance in mathematical reasoning verification. The consensus filtering and step-wise flagging significantly enhanced model accuracy and robustness. Ablation studies confirm the 14B model's advantages over smaller variants for complex reasoning tasks, establishing Dyve as an effective solution for precise and efficient error detection.




% \begin{table*}[h]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{l|c|c|c|c|c}
%         \toprule
%         \hline
%         & Training Dataset & GSM8K & MATH & OlympiadBench & OmniMATH \\
%         \hline
%         \midrule

%         Zero Shot (GPT-4o) & &62.9 & \sout51.1 & 47.3 & 44.9 \\
%         Zero Shot (QwQ 32B) & &- & - & - & - \\
%         Zero Shot (llama 3.1 8b instruct) & &\sout{46.8} & \sout{38.0} & \sout{32.5} & \sout{32.3} \\
%         Zero Shot (DS-R1-Distill-Qwen-14B)& &\sout{49.5} 67.3 & \sout{28.5} 38.8 & \sout{21.5} 29.9 & \sout{25.3} 32.1 \\
%         \hline
        
%         unfiltered (llama) & DatasetV1-120k(only 15k query) & 35.6 & 28.3 & 10.9 & 13.3 \\
%         filtered (llama ep3)& DatasetV1-17k(only 15k query) & 53.1 & 44.1 & 26.9 & 25.7 \\
%          \hline
%         unfiltered (llama) & DatasetV2-270k(100k query) & - & - & - & - \\
%         filtered (llama ep3)& DatasetV2-TBD(100k query) & - & - & - & - \\
%         \hline

%         unfiltered (DS-R1-Distill-Qwen-14B)& DatasetV1-120k(only 15k query) & 59.1 & 34.7 & 15.0 & 11.2 \\
%         filtered (DS-R1-Distill-Qwen-14B)& DatasetV1-17k(only 15k query) & \textbf{66.2} & \textbf{53.2} & 32.7 & 28.7 \\
%         filtered COT RM (DS-R1-Distill-Qwen-14B) & DatasetV1-17k(only 15k query) & \sout{31.2} \textbf{68.5} & \sout{30.5} \textbf{58.3} & \sout{21.3} \textbf{49.0} & \sout{22.5} \textbf{47.2} \\
%         new data filtered COT RM (DS-R1-Distill-Qwen-14B) & DatasetV1-23k(79k query) & \sout{} \textbf{} & \sout{} \textbf{} & \sout{} \textbf{} & \sout{} \textbf{} \\
%         \hline
%         \bottomrule
%     \end{tabular}
%     }
%     \caption{Training Scaling Up for Value-head PRM}
%     \label{tab:value_head_prm}
% \end{table*}




% \newpage



% \begin{table*}[h]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{lcccccc}
%         \toprule
%         \hline
%         & GSM8K & MATH & AIME 2024 & OlympiadBench & College Math & Omni-MATH \\
%         \midrule
%         \hline
%         LLama 3.1 8B Ins-direct & 39.6 & 39.4 &  & 12.4 & 20.7 &  \\
%         LLama 3.1 8B Ins-COT & 38.4 & 39.0 &  & 12.4 & 20.7 &  \\
%         LLama 3.1 8B Ins-MCTS &  &  &  &  &  &  \\
%         LLama 3.1 8B Ins-BON &  &  &  &  &  &  \\
%         LLama 3.1 8B Ins-MCTS+PRM &  &  &  &  &  &  \\
%         \hline
%         DS R1 distill 14B pass@1 &  & 86.8 &  &  &  &  \\
%         DS R1 distill 14B—COT &  &  &  &  &  &  \\
%         DS R1 distill 14B—MCTS + Oracle Verifier &  &  &  &  &  &  \\
%         DS R1 distill 14B+BON &  &  &  &  &  &  \\
%         DS R1 distill 14B—MCTS + PRM &  & 90.5 &  &  &  &  \\
%         \hline
%         \bottomrule
%     \end{tabular}
%     }
%     \caption{Evaluation Results of baselines Models}
%     \label{tab:evaluation_results}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{lcccccc}
%         \toprule
%         \hline
%         & GSM8K & MATH & AIME 2024 & OlympiadBench & College Math & Omni-MATH \\
%         \midrule
%         \hline

%         LLama 3.1 8B Ins-MCTS+PRM &  &  &  &  &  &  \\
%         Rstar Qwen-math 1.5B -MCTS+PRM &  &  &  &  &  &  \\
%         Rstar Qwen-math 1.5B -MCTS+ our PRM &  &  &  &  &  &  \\
%         DS R1 distill 14B + pass@1 &  & 86.8 &  &  &  &  \\
%         DS R1 distill 14B + DyVerSearch@8 &  & 90.5 &  &  &  &  \\
%         DS R1 distill 14B + DyVerSearch@16 &  & 93.5 &  &  &  &  \\
%         \hline
%         \bottomrule
%     \end{tabular}
%     }
%     \caption{Evaluation Results of baselines Models}
%     \label{tab:evaluation_results}
% \end{table*}




\label{submission}

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
% \newpage

\section{Broader Ethical Impact}

Our method is centered on rigorous verification of AI reasoning, ensuring each step is systematically validated for enhanced reliability and transparency. By exclusively using publicly available datasets under their proper licenses, we adhere to responsible research practices. We believe that improving verification in AI reasoning not only boosts system robustness but also exemplifies ethical AI development.

\section{Limitations}

While Dyve demonstrates strong performance, it shares several limitations common to verification-based systems. Its effectiveness naturally depends on the complexity of the reasoning tasks, and more intricate multi-step problems may require further adaptation or deeper analysis. In addition, although our consensus-filtered process supervision considerably enhances signal quality, a modest level of noise remains inherent in any automated estimation process. Finally, the overall performance is influenced by the quality and diversity of the training data, suggesting that further efforts in data curation and filtering could yield even more robust results. These aspects offer promising directions for future research.



\bibliography{custom}

\appendix
\section{Appendix}
\label{sec:appendix}

\subsection{Detailed Experiment Setup}
\label{sec:expSetup}

\paragraph{Training}
\subsection{Training Details}
Our model processes inputs with a maximum token length of 2048, ensuring robust contextual understanding. To further enhance efficiency, we employ Low-Rank Adaptation (LoRA) configured with a rank of 16, an alpha value of 16, and a dropout rate of 0.1. The training regimen spans three epochs, using a per-device batch size of 2 and leveraging gradient accumulation over 8 steps. The learning rate is set to \(2 \times 10^{-5}\) and a weight decay of 0.01 is applied. Training is executed with mixed precision (fp16), optimizing computational resources without sacrificing performance.

\paragraph{Inference}
During inference, our model leverages a multi-step reasoning process to evaluate each problem instance. The procedure begins by formulating a sequence of conversational prompts that encapsulate both the problem statement and its progressive steps. At each step, the Dyve model is queried via its custom chat interface, and the generated response is examined for specific response patterns — such as the presence of a "\(+\)" symbol signaling a correct evaluation. This iterative mechanism continues until a response fails to meet the designated correctness criteria, at which point the process halts. To ensure efficiency, the inference is executed concurrently using a pool of 32 parallel workers, processing various configurations from the ProcessBench dataset (including \texttt{gsm8k}, \texttt{math}, \texttt{olympiadbench}, and \texttt{omnimath}). For every evaluated problem, all intermediate responses (or generations) and the final step classification are recorded. These results are then systematically saved in JSON Lines format, facilitating subsequent analysis and serving as a robust foundation for further evaluation.

\subsection{Efficient Estimation of MCTS}
\label{sec:effmcts}

In this section, we detail our approach to efficiently utilize Monte Carlo Tree Search (MCTS) for sampling rollouts, which are crucial for training process-based verifiers.

\subsubsection*{Overview}

Our method leverages MCTS to construct a state-action tree representing detailed reasoning paths for a given question. This approach allows us to collect Process-based Reward Model (PRM) training examples by exploring various reasoning paths and identifying errors efficiently.

\subsubsection*{State-Action Tree Construction}

Each state \( s \) in the tree corresponds to a question and its preceding reasoning steps, with the root state being the question without any reasoning steps. An action \( a \) is a potential next step, and the state transition function is defined as \( s' = \text{Concatenate}(s, a) \). Each node \( s \) stores the visit count \( N(s) \), Monte Carlo estimation \( MC(s) \), and rollout value function \( Q(s, r) \).

\subsubsection*{MCTS Process}

\paragraph{Selection}

We maintain a pool of rollouts with \( 0 < MC(s) < 1 \). During selection, a rollout is chosen based on tree statistics using a variant of the PUCT algorithm:
\[
U(s) = c_{\text{puct}} \frac{\sqrt{\sum_i N(s_i)}}{1 + N(s)}
\]
This strategy initially favors rollouts with low visit counts, gradually shifting preference towards those with high rollout values.

\paragraph{Binary Search}

A binary search identifies the first error location in the selected rollout. Rollouts with \( 0 < MC(s) < 1 \) are added to the selection pool. The search process divides and examines rollouts to pinpoint errors, updating the tree with new states.

\paragraph{Maintenance}

After binary search, update the statistics \( N(s) \), \( MC(s) \), and \( Q(s, r) \). Increment \( N(s) \) for the selected state-action pair and update \( MC(s) \) and \( Q(s, r) \) based on the binary search results.

\paragraph{Tree Construction}

Repeat the above process to construct the state-action tree. The process ends when the search count reaches a predetermined limit or no additional rollouts are available.

\subsection{Finetuning Data Example}
\label{sec:DataExp}
\begin{figure*}[t]
\begin{minipage}{\textwidth}
\begin{tcolorbox}[
    colback=blue!5!white,
    colframe=blue!75!black,
    title=Example Data Sample,
    width=\textwidth,
    enhanced,
    boxrule=1pt,
    arc=4mm,
    auto outer arc,
    fonttitle=\bfseries
]
\footnotesize
\textbf{LLM Proposer:}
\begin{verbatim}
Step: Cowboy Mickey and cowgirl Minnie train horses. Mickey mounts six less than twice as 
many horses per day as Minnie, while Minnie mounts three more horses per day than there are days in 
a week. How many horses does Mickey mount per week?

Step 1: Minnie mounts 7+3=<<7+3=10>>10 horses per day.

Is this step correct? Answer with '+' for correct or '-' for incorrect.
\end{verbatim}

\textbf{DyVer:}
\begin{verbatim}
<think>
</think> +
\end{verbatim}

\textbf{LLM Proposer:}
\begin{verbatim}
Step 2: Mickey mounts 12-6=<<12-6=6>>6 horses per day.

Is this step correct? Answer with '+' for correct or '-' for incorrect.
\end{verbatim}

\textbf{DyVer:}
\begin{verbatim}
<think>
Let's verify Step 2 .... 

Mickey mounts six less than twice Minnie's count. Minnie mounts 10 horses, so Mickey should mount 
2*10 - 6 = 14. Step 2 is incorrect.
</think>
Answer: -
\end{verbatim}
\end{tcolorbox}
\end{minipage}
\end{figure*}

\end{document}
