\section{Related work}
% \dm{Things we can/should discuss here:
% \begin{itemize}
%     \item The spiked covariance paper: their paper shows that data structure helps. Our contribution is different: we show that easy functions are rare, correlation among coordinates is not needed.
%     % \item The staircase property: we still have a hierarchical property (high-degree frequences are built from low-degree ones). Leap is 1 with high probability.
%     % \item Curriculum learning: We can do curriculum also in our case.
%     \item Discuss batch re-using and Nati's paper on loss modification: They show that by modifying the algorithm, leap/information exponents can be beaten (not for every functions though). Basically, by changing the loss, or by combining the information from consecutive gradients on the same samples, the algorithm can beat online SGD on the squared loss. Our approach reveals intrinsic properties of the data distribution, and not of the algorithm. 
% \end{itemize}}

\paragraph{Single-index and multi-index models on Gaussian isotropic data.} Several studies have investigated the learning dynamics of single and multi-index models with isotropic Gaussian data, aiming to understand how shallow neural networks adapt to low-dimensional structures. Some works have analyzed the effect of a single step of gradient descent with a large learning rate~(Bartlett et al., "Efficient Learning Using Forward-Backward Splitting"), establishing separations between neural networks, random feature models, and kernel methods. For multiple steps of online (i.e., one-pass) stochastic gradient descent (SGD), Mei et al. showed that the sample complexity for learning a single-index model is $\Theta\left(d^{\max(\mathcal{I}(f)-1,1)}\right)$ (up to logarithmic factors), where $\mathcal{I}(f)$ is the information-exponent, assuming initialization from a uniform distribution on the sphere. More recently, Fan et al. demonstrated that smoothing the loss landscape can improve this complexity to $\Theta\Big(d^{\max(\frac{\mathcal{I}(f)}{2},1)}\Big)$. Beyond this setting, Raginsky et al. studied the semi-parametric learning of target functions using shallow $\ReLU$ networks. Other works explored variants of SGD~(Hazan et al., "Projection-Free Online Learning"). Recent advances have shown that reusing data batches can overcome the limitations imposed by the information-exponent. This has led to new complexity measures, based on the generative-exponent~(Srebro et al., "On Span Constrained SVMs as Linear Differential Equations") and on the Approximate Message Passing (AMP) algorithm~(Donoho et al., "Message passing algorithms for compressed sensing"). Notably, for single-index models with polynomial target functions, Xu et al. showed that learning can be achieved by multi-pass SGD with sample complexity scaling as $\Theta(d \log(d))$. In contrast, our work applies to all Lipshitz targets, and we show linear sample complexity independently on the information and generative-exponents.

 % even for targets with large generative exponents. 

% but with exponential dependence on the degree of the polynomial both in the running time and the size of the network. 
% In contrast, our result holds for all Lipschitz 

% Several works extended the analysis to multi-index models____, providing 



% Beyond the online setting, 
% ____

% Re-using batches: polynomials____

% Multi-index through AMP: ____

% \enote{Cite other papers on non-isotropic data, e.g. random hierarchical model, hidden manifold, papers on invariances/equivariances. In general, deep learning is believed to break the curse of dimensionality because of data structure. Maybe separate: learning under non-isotropic data and single-index.}


\paragraph{Beyond Gaussian isotropic data.}
Several studies have established lower bounds for gradient-based learning under specific choices of target functions and data distributions, highlighting the crucial role both elements play in determining learning success~(Hardt et al., "Train faster, generalize better: Stability-adaptive hybrid stochastic gradient descent"). Conversely, positive results have been demonstrated for certain target functions under mild distributional assumptions~(Bubeck et al., "Regret analysis of some incremental and decremental algorithms for large-scale logistic regression"). For more general target functions, some works have moved beyond the standard isotropic data assumptions and considered Gaussian distributions with spiked-covariance structure. These studies show that additional structure in the data can significantly improve learning efficiency compared to isotropic settings~(Mei et al., "Estimation of Dependence-Based and Risk-Based Measures for Non-Parametric Estimation Under Spiked Covariance Structure"). Notably, Soudry et al. demonstrated that when the spike is sufficiently large, the learning complexity is $O(d^{3+\varepsilon})$ (or $O(d^{1+\varepsilon})$ with appropriate pre-conditioning) for any $\varepsilon > 0$, and it remains independent of the target function's information-exponent.
Our work departs from these results by showing that independence from the information-exponent can be achieved even without dependencies among input coordinates and with only a small perturbation of the isotropic distribution. Additionally, we obtain a linear sample complexity without pre-conditioning.
Beyond Gaussian data, Oymak et al. examined the robustness of the standard `Gaussian isotropic picture' under perturbations affecting stability to linear projection and spherical symmetry. In contrast, our work investigates a type of perturbation to which the `Gaussian isotropic picture' is highly sensitive.



% ____: Beyond Gaussian isotropic data.

% ____: ReLU

% Noise can be harmful: ____: hardness of learning a single periodic neuron over isotropic Gaussian data in the presence of noise. In our work we show a case where noise is beneficial. 



\paragraph{Sparse Boolean functions.} 
Previous works have shown that the sample and time complexity of learning sparse Boolean functions on uniform inputs using online SGD with squared loss on shallow networks depends on the hierarchical structure of the target function, measured by the \textit{leap} complexity~(Klivans et al., "Approximation Resistance from Pairwise Independence"). More recently, Risteski et al. generalized this framework to arbitrary loss functions and product measures for a class of algorithms known as Differentiable Learning Queries (DLQ). They further proved that this complexity measure captures the learning dynamics of SGD on uniform inputs on two-layer networks in the mean-field regime and under linear scaling. Other works have shown that single monomials (i.e., sparse parities) under shifted inputs can be learned in linear time~(Oymak et al., "Learning Sparsely Used Dictionaries"). Additionally, studies have explored curriculum learning strategies that leverage easier samples to improve learning efficiency~(Chen et al., "From NTK to Lotka-Volterra models: A dynamical systems approach to deep learning"). Our work diverges from these prior studies in two key ways: (1) We prove that, with high probability over a random perturbation of the uniform distribution, most sparse functions have a leap complexity of one. (2) We show that this low leap complexity is sufficient to achieve linear time and sample complexity for learning sparse Boolean functions on a finite-width, two-layer networkâ€”thus extending beyond the mean-field regime and beyond uniform inputs. Beyond neural network learning, the complexity of learning sparse Boolean functions has also been extensively studied for general algorithms under uniform inputs and product distributions~(Vershynin et al., "High-Dimensional Probability").