

@article{ortiz2021linearized,
  doi = {10.48550/ARXIV.2106.06770},
  
  url = {https://arxiv.org/abs/2106.06770},
  
  author = {Ortiz-Jiménez, Guillermo and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {What can linearized neural networks actually say about generalization?},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{liu2018intriguing,
  doi = {10.48550/ARXIV.1807.03247},
  
  url = {https://arxiv.org/abs/1807.03247},
  
  author = {Liu, Rosanne and Lehman, Joel and Molino, Piero and Such, Felipe Petroski and Frank, Eric and Sergeev, Alex and Yosinski, Jason},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{
li2021why,
title={Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?},
author={Zhiyuan Li and Yi Zhang and Sanjeev Arora},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=uCY5MuAxcxU}
}

@article{malach2020computational,
  title={Computational separation between convolutional and fully-connected networks},
  author={Malach, Eran and Shalev-Shwartz, Shai},
  journal={arXiv preprint arXiv:2010.01369},
  year={2020}
}

@misc{mok2022demystifying,
  doi = {10.48550/ARXIV.2203.14577},
  
  url = {https://arxiv.org/abs/2203.14577},
  
  author = {Mok, Jisoo and Na, Byunggook and Kim, Ji-Hoon and Han, Dongyoon and Yoon, Sungroh},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Demystifying the Neural Tangent Kernel from a Practical Perspective: Can it be trusted for Neural Architecture Search without training?},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@misc{park2020towards,
  doi = {10.48550/ARXIV.2011.06006},
  
  url = {https://arxiv.org/abs/2011.06006},
  
  author = {Park, Daniel S. and Lee, Jaehoon and Peng, Daiyi and Cao, Yuan and Sohl-Dickstein, Jascha},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards {NNGP}-guided Neural Architecture Search},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{10.2307/2048015,
 ISSN = {00029939, 10886826},
 URL = {http://www.jstor.org/stable/2048015},
 abstract = {We find upper and lower bounds for $\operatorname{Pr}(\sum \pm x_n \geq t)$, where x1, x2,... are real numbers. We express the answer in terms of the K-interpolation norm from the theory of interpolation of Banach spaces.},
 author = {S. J. Montgomery-Smith},
 journal = {Proceedings of the American Mathematical Society},
 number = {2},
 pages = {517--522},
 publisher = {American Mathematical Society},
 title = {The Distribution of Rademacher Sums},
 volume = {109},
 year = {1990}
}

@book{o'donnell_2014, place={Cambridge}, title={Analysis of Boolean Functions}, DOI={10.1017/CBO9781139814782}, publisher={Cambridge University Press}, author={O'Donnell, Ryan}, year={2014}}

@inproceedings{AS20,
 author = {Abbe, Emmanuel and Sandon, Colin},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {20061--20072},
 title = {On the universality of deep learning},
volume = {33},
 year = {2020}
}

@article{Winkelbauer2012MomentsAA,
  title={Moments and Absolute Moments of the Normal Distribution},
  author={Andreas Winkelbauer},
  journal={ArXiv},
  year={2012},
  volume={abs/1209.4340}
}

@inproceedings{abbe2021staircase,
  title={The staircase property: How hierarchical structure can guide deep learning},
  author={Abbe, Emmanuel and Boix Adsera, Enric and Brennan, Matthew and Bresler, Guy and Nagaraj, Dheeraj},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{malach2019deeper,
 author = {Malach, Eran and Shalev-Shwartz, Shai},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {6429--6438},
 title = {Is Deeper Better only when Shallow is Good?},
 volume = {32},
 year = {2019}
}

@unpublished{allen2020backward,
  title={Backward feature correction: {H}ow deep learning performs deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  note={arXiv:2001.04413},
  year={2020}
}

@inproceedings{blum1994weakly,
  title={Weakly learning {DNF} and characterizing statistical query learning using {F}ourier analysis},
  author={Blum, Avrim and Furst, Merrick and Jackson, Jeffrey and Kearns, Michael and Mansour, Yishay and Rudich, Steven},
  booktitle={Symposium on Theory of Computing (STOC)},
  pages={253--262},
  year={1994}
}


@inproceedings{abbe2021power,
  title={On the Power of Differentiable Learning versus {PAC} and {SQ} Learning},
  author={Abbe, Emmanuel and Kamath, Pritish and Malach, Eran and Sandon, Colin and Srebro, Nathan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@inproceedings{failing,
  author = {Liu, Rosanne and Lehman, Joel and Molino, Piero and Such, Felipe Petroski and Frank, Eric and Sergeev, Alex and Yosinski, Jason},
  biburl = {https://www.bibsonomy.org/bibtex/28095313389e6b84c74d3a018e670e93e/dblp},
  booktitle = {NeurIPS},
  ee = {http://papers.nips.cc/paper/8169-an-intriguing-failing-of-convolutional-neural-networks-and-the-coordconv-solution},
  interhash = {687147375fc30b3e4b9e8f67055d86a9},
  intrahash = {8095313389e6b84c74d3a018e670e93e},
  keywords = {dblp},
  pages = {9628-9639},
  timestamp = {2020-03-07T11:50:35.000+0100},
  title = {An intriguing failing of convolutional neural networks and the {C}oord{C}onv solution},
  url = {http://dblp.uni-trier.de/db/conf/nips/nips2018.html#LiuLMSFSY18},
  year = 2018
}



@InProceedings{quantifying,
  title = 	 {Quantifying the Benefit of Using Differentiable Learning over Tangent Kernels},
  author =       {Malach, Eran and Kamath, Pritish and Abbe, Emmanuel and Srebro, Nathan},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {7379--7389},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/malach21a/malach21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/malach21a.html},
  abstract = 	 {We study the relative power of learning with gradient descent on differentiable models, such as neural networks, versus using the corresponding tangent kernels. We show that under certain conditions, gradient descent achieves small error only if a related tangent kernel method achieves a non-trivial advantage over random guessing (a.k.a. weak learning), though this advantage might be very small even when gradient descent can achieve arbitrarily high accuracy. Complementing this, we show that without these conditions, gradient descent can in fact learn with small error even when no kernel method, in particular using the tangent kernel, can achieve a non-trivial advantage over random guessing.}
}


@inproceedings{shalev2021computational,
  title={Computational Separation Between Convolutional and Fully-Connected Networks},
  author={Shalev-Shwartz, Shai and Malach, Eran},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@unpublished{abbe2020poly,
  title={Poly-time universality and limitations of deep learning},
  author={Abbe, Emmanuel and Sandon, Colin},
  note={arXiv:2001.02992},
  year={2020}
}

@article{Zhang2021PointerVR,
  title={Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization},
  author={Chiyuan Zhang and Maithra Raghu and Jon M. Kleinberg and Samy Bengio},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.12580}
}

@unpublished{EnricDraft,
    author={Enric Boix-Adsera},
    note={Personal communication},
    year={2021}
}

@inproceedings{Das20,
author = {Das, Abhimanyu and Gollapudi, Sreenivas and Kumar, Ravi and Panigrahy, Rina},
title = {On the Learnability of Random Deep Networks},
year = {2020},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {In this paper we study the learnability of random deep networks both theoretically and experimentally. On the theoretical front, assuming the statistical query model, we show that the learnability of random deep networks with sign activation drops exponentially with their depths; under plausible conjectures, our results extend to ReLu and sigmoid activations. The core of the arguments is that even for highly correlated inputs, the outputs of deep random networks are near-orthogonal. On the experimental side, we find that the learnability of random networks drops sharply with depth even with the state-of-the-art training methods.},
booktitle = {Proceedings of the Thirty-First Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {398–410},
numpages = {13},
location = {Salt Lake City, Utah},
series = {SODA '20}
}

@misc{tan2021cautionary,
  doi = {10.48550/ARXIV.2110.09626},
  
  url = {https://arxiv.org/abs/2110.09626},
  
  author = {Tan, Yan Shuo and Agarwal, Abhineet and Yu, Bin},
  
  keywords = {Machine Learning (stat.ML), Information Theory (cs.IT), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A cautionary tale on fitting decision trees to data from additive models: generalization lower bounds},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Das2019learnability,
  doi = {10.48550/ARXIV.1904.03866},
  
  url = {https://arxiv.org/abs/1904.03866},
  
  author = {Das, Abhimanyu and Gollapudi, Sreenivas and Kumar, Ravi and Panigrahy, Rina},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the Learnability of Deep Random Networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{abbeINAL,
  title={An initial alignment between neural network and target is needed for gradient descent to learn},
  author={Abbe, Emmanuel and Cornacchia, Elisabetta and Hazla, Jan and Marquis, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={33--52},
  year={2022},
  organization={PMLR}
}


@misc{bietti2020deep,
  doi = {10.48550/ARXIV.2009.14397},
  
  url = {https://arxiv.org/abs/2009.14397},
  
  author = {Bietti, Alberto and Bach, Francis},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Equals Shallow for ReLU Networks in Kernel Regimes},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{misiakiewicz2021learning,
  doi = {10.48550/ARXIV.2111.08308},
  
  url = {https://arxiv.org/abs/2111.08308},
  
  author = {Misiakiewicz, Theodor and Mei, Song},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning with convolution and pooling operations in kernel methods},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{bietti2021approximation,
  doi = {10.48550/ARXIV.2102.10032},
  
  url = {https://arxiv.org/abs/2102.10032},
  
  author = {Bietti, Alberto},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Approximation and Learning with Deep Convolutional Models: a Kernel Perspective},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{shamir2016distribution,
  doi = {10.48550/ARXIV.1609.01037},
  
  url = {https://arxiv.org/abs/1609.01037},
  
  author = {Shamir, Ohad},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Distribution-Specific Hardness of Learning Neural Networks},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{malach2020hardness,
  doi = {10.48550/ARXIV.2008.08059},
  
  url = {https://arxiv.org/abs/2008.08059},
  
  author = {Malach, Eran and Shalev-Shwartz, Shai},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {When Hardness of Approximation Meets Hardness of Learning},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}

@inproceedings{malach2021quantifying,
  title={Quantifying the benefit of using differentiable learning over tangent kernels},
  author={Malach, Eran and Kamath, Pritish and Abbe, Emmanuel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={7379--7389},
  year={2021},
  organization={PMLR}
}

@article{zaidi2022does,
  title={When Does Re-initialization Work?},
  author={Zaidi, Sheheryar and Berariu, Tudor and Kim, Hyunjik and Bornschein, J{\"o}rg and Clopath, Claudia and Teh, Yee Whye and Pascanu, Razvan},
  journal={arXiv preprint arXiv:2206.10011},
  year={2022}

}

@inproceedings{johnson2017clevr,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2901--2910},
  year={2017}
}

@misc{lee2020finitevsinfinite,
  doi = {10.48550/ARXIV.2007.15801},
  
  url = {https://arxiv.org/abs/2007.15801},
  
  author = {Lee, Jaehoon and Schoenholz, Samuel S. and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Finite Versus Infinite Neural Networks: an Empirical Study},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{li2020convolutional,
  title={Why are convolutional nets more sample-efficient than fully-connected nets?},
  author={Li, Zhiyuan and Zhang, Yi and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2010.08515},
  year={2020}
}

@article{elsken2019neural,
  title={Neural architecture search: A survey},
  author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1997--2017},
  year={2019},
  publisher={JMLR. org}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{xu2021knas,
  title={KNAS: green neural architecture search},
  author={Xu, Jingjing and Zhao, Liang and Lin, Junyang and Gao, Rundong and Sun, Xu and Yang, Hongxia},
  booktitle={International Conference on Machine Learning},
  pages={11613--11625},
  year={2021},
  organization={PMLR}
}

@article{chen2021neural,
  title={Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective},
  author={Chen, Wuyang and Gong, Xinyu and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2102.11535},
  year={2021}
}

@article{amid2022learning,
  title={Learning from Randomly Initialized Neural Network Features},
  author={Amid, Ehsan and Anil, Rohan and Kot{\l}owski, Wojciech and Warmuth, Manfred K},
  journal={arXiv preprint arXiv:2202.06438},
  year={2022}
}

@article{park2020towards,
  title={Towards nngp-guided neural architecture search},
  author={Park, Daniel S and Lee, Jaehoon and Peng, Daiyi and Cao, Yuan and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:2011.06006},
  year={2020}
}

@inproceedings{mok2022demystifying,
  title={Demystifying the Neural Tangent Kernel from a Practical Perspective: Can it be trusted for Neural Architecture Search without training?},
  author={Mok, Jisoo and Na, Byunggook and Kim, Ji-Hoon and Han, Dongyoon and Yoon, Sungroh},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11861--11870},
  year={2022}
}

@article{ortiz2021can,
  title={What can linearized neural networks actually say about generalization?},
  author={Ortiz-Jim{\'e}nez, Guillermo and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8998--9010},
  year={2021}
}

@inproceedings{mei2021learning,
  title={Learning with invariances in random features and kernel models},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  pages={3351--3418},
  year={2021},
  organization={PMLR}
}

@article{bietti2021sample,
  title={On the sample complexity of learning under geometric stability},
  author={Bietti, Alberto and Venturi, Luca and Bruna, Joan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18673--18684},
  year={2021}
}

@inproceedings{elesedy2021provably,
  title={Provably strict generalisation benefit for equivariant models},
  author={Elesedy, Bryn and Zaidi, Sheheryar},
  booktitle={International Conference on Machine Learning},
  pages={2959--2969},
  year={2021},
  organization={PMLR}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{abbe2022non-universality,
  title={On the non-universality of deep learning: quantifying the cost of symmetry},
  author={Abbe, Emmanuel and Boix-Adsera, Enric},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17188--17201},
  year={2022}
}

@article{abbe2022learning,
  title={Learning to Reason with Neural Networks: Generalization, Unseen Data and Boolean Measures},
  author={Abbe, Emmanuel and Bengio, Samy and Cornacchia, Elisabetta and Kleinberg, Jon and Lotfi, Aryo and Raghu, Maithra and Zhang, Chiyuan},
  journal={Accepted at NeurIPS 2022},
  year={2022}
}

@article{blum2003noise,
  title={Noise-tolerant learning, the parity problem, and the statistical query model},
  author={Blum, Avrim and Kalai, Adam and Wasserman, Hal},
  journal={Journal of the ACM (JACM)},
  volume={50},
  number={4},
  pages={506--519},
  year={2003},
  publisher={ACM New York, NY, USA}
}

@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017},
  organization={PMLR}
}

@article{barak2022hidden,
  title={Hidden progress in deep learning: Sgd learns parities near the computational limit},
  author={Barak, Boaz and Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Malach, Eran and Zhang, Cyril},
  journal={arXiv preprint arXiv:2207.08799},
  year={2022}
}

@article{frei2022random,
  title={Random feature amplification: Feature learning and generalization in neural networks},
  author={Frei, Spencer and Chatterji, Niladri S and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2202.07626},
  year={2022}
}

@article{daniely2020learning,
  title={Learning parities with neural networks},
  author={Daniely, Amit and Malach, Eran},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20356--20365},
  year={2020}
}

@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@article{garban2010fourier,
  title={The Fourier spectrum of critical percolation},
  author={Garban, Christophe and Pete, G{\'a}bor and Schramm, Oded},
  journal={Acta Mathematica},
  volume={205},
  number={1},
  pages={19--104},
  year={2010},
  publisher={Institut Mittag-Leffler}
}

@incollection{schramm2011quantitative,
  title={Quantitative noise sensitivity and exceptional times for percolation},
  author={Schramm, Oded and Steif, Jeffrey E},
  booktitle={Selected Works of Oded Schramm},
  pages={391--444},
  year={2011},
  publisher={Springer}
}

@article{refinetti2022neural,
  title={Neural networks trained with SGD learn distributions of increasing complexity},
  author={Refinetti, Maria and Ingrosso, Alessandro and Goldt, Sebastian},
  journal={arXiv preprint arXiv:2211.11567},
  year={2022}
}

@article{jacot2022feature,
  title={Feature Learning in $ L\_ $\{$2$\}$ $-regularized DNNs: Attraction/Repulsion and Sparsity},
  author={Jacot, Arthur and Golikov, Eugene and Hongler, Cl{\'e}ment and Gabriel, Franck},
  journal={arXiv preprint arXiv:2205.15809},
  year={2022}
}


@article{kalimeris2019sgd,
  title={Sgd on neural networks learns functions of increasing complexity},
  author={Kalimeris, Dimitris and Kaplun, Gal and Nakkiran, Preetum and Edelman, Benjamin and Yang, Tristan and Barak, Boaz and Zhang, Haofeng},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@article{duminil-copin_manolescu_2022, title={Planar random-cluster model: scaling relations}, volume={10}, DOI={10.1017/fmp.2022.16}, journal={Forum of Mathematics, Pi}, publisher={Cambridge University Press}, author={Duminil-Copin, Hugo and Manolescu, Ioan}, year={2022}, pages={e23}}

@inproceedings{dudeja2018learning,
  title={Learning single-index models in {G}aussian space},
  author={Dudeja, Rishabh and Hsu, Daniel},
  booktitle={Conference On Learning Theory},
  pages={1887--1930},
  year={2018},
  organization={PMLR}
}

@article{maurer2021concentration,
  title={Concentration inequalities under sub-{G}aussian and sub-exponential conditions},
  author={Maurer, Andreas and Pontil, Massimiliano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={7588--7597},
  year={2021}
}

@article{saglietti2022analytical,
  title={An analytical theory of curriculum learning in teacher--student networks},
  author={Saglietti, Luca and Mannelli, Stefano Sarao and Saxe, Andrew},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2022},
  number={11},
  pages={114014},
  year={2022},
  publisher={IOP Publishing}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}

@article{wang2021survey,
  title={A survey on curriculum learning},
  author={Wang, Xin and Chen, Yudong and Zhu, Wenwu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  publisher={IEEE}
}

@inproceedings{jiang2015self,
  title={Self-paced curriculum learning},
  author={Jiang, Lu and Meng, Deyu and Zhao, Qian and Shan, Shiguang and Hauptmann, Alexander G},
  booktitle={Twenty-ninth AAAI conference on artificial intelligence},
  year={2015}
}

@article{toneva2018empirical,
  title={An empirical study of example forgetting during deep neural network learning},
  author={Toneva, Mariya and Sordoni, Alessandro and Combes, Remi Tachet des and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J},
  journal={arXiv preprint arXiv:1812.05159},
  year={2018}
}

@article{pashler2013does,
  title={When does fading enhance perceptual category learning?},
  author={Pashler, Harold and Mozer, Michael C},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={39},
  number={4},
  pages={1162},
  year={2013},
  publisher={American Psychological Association}
}

@article{elio1984effects,
  title={The effects of information order and learning mode on schema abstraction},
  author={Elio, Renee and Anderson, John R},
  journal={Memory \& cognition},
  volume={12},
  number={1},
  pages={20--30},
  year={1984},
  publisher={Springer}
}

@article{shafto2014rational,
  title={A rational account of pedagogical reasoning: Teaching by, and learning from, examples},
  author={Shafto, Patrick and Goodman, Noah D and Griffiths, Thomas L},
  journal={Cognitive psychology},
  volume={71},
  pages={55--89},
  year={2014},
  publisher={Elsevier}
}

@article{ross1990generalizing,
  title={Generalizing from the use of earlier examples in problem solving.},
  author={Ross, Brian H and Kennedy, Patrick T},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={16},
  number={1},
  pages={42},
  year={1990},
  publisher={American Psychological Association}
}

@book{allgower2003introduction,
  title={Introduction to numerical continuation methods},
  author={Allgower, Eugene L and Georg, Kurt},
  year={2003},
  publisher={SIAM}
}

@article{elman1993learning,
  title={Learning and development in neural networks: The importance of starting small},
  author={Elman, Jeffrey L},
  journal={Cognition},
  volume={48},
  number={1},
  pages={71--99},
  year={1993},
  publisher={Elsevier}
}

@article{krueger2009flexible,
  title={Flexible shaping: How learning in small steps helps},
  author={Krueger, Kai A and Dayan, Peter},
  journal={Cognition},
  volume={110},
  number={3},
  pages={380--394},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{pentina2015curriculum,
  title={Curriculum learning of multiple tasks},
  author={Pentina, Anastasia and Sharmanska, Viktoriia and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5492--5500},
  year={2015}
}

@inproceedings{sarafianos2017curriculum,
  title={Curriculum learning for multi-task classification of visual attributes},
  author={Sarafianos, Nikolaos and Giannakopoulos, Theodore and Nikou, Christophoros and Kakadiaris, Ioannis A},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={2608--2615},
  year={2017}
}

@article{xiong2021modeling,
  title={Modeling multi-species RNA modification through multi-task curriculum learning},
  author={Xiong, Yuanpeng and He, Xuan and Zhao, Dan and Tian, Tingzhong and Hong, Lixiang and Jiang, Tao and Zeng, Jianyang},
  journal={Nucleic acids research},
  volume={49},
  number={7},
  pages={3719--3734},
  year={2021},
  publisher={Oxford University Press}
}

@inproceedings{dong2017multi,
  title={Multi-task curriculum transfer deep learning of clothing attributes},
  author={Dong, Qi and Gong, Shaogang and Zhu, Xiatian},
  booktitle={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={520--529},
  year={2017},
  organization={IEEE}
}

@article{campos2021curriculum,
  title={Curriculum learning for language modeling},
  author={Campos, Daniel},
  journal={arXiv preprint arXiv:2108.02170},
  year={2021}
}

@article{shi2015recurrent,
  title={Recurrent neural network language model adaptation with curriculum learning},
  author={Shi, Yangyang and Larson, Martha and Jonker, Catholijn M},
  journal={Computer Speech \& Language},
  volume={33},
  number={1},
  pages={136--154},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{shi2013k,
  title={K-component recurrent neural network language models using curriculum learning},
  author={Shi, Yangyang and Larson, Martha and Jonker, Catholijn M},
  booktitle={2013 IEEE Workshop on Automatic Speech Recognition and Understanding},
  pages={1--6},
  year={2013},
  organization={IEEE}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{avrahami1997teaching,
  title={Teaching by examples: Implications for the process of category acquisition},
  author={Avrahami, Judith and Kareev, Yaakov and Bogot, Yonatan and Caspi, Ruth and Dunaevsky, Salomka and Lerner, Sharon},
  journal={The Quarterly Journal of Experimental Psychology Section A},
  volume={50},
  number={3},
  pages={586--606},
  year={1997},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{zaremba2014learning,
  title={Learning to execute},
  author={Zaremba, Wojciech and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1410.4615},
  year={2014}
}

@inproceedings{andoni2014learning,
  title={Learning polynomials with neural networks},
  author={Andoni, Alexandr and Panigrahy, Rina and Valiant, Gregory and Zhang, Li},
  booktitle={International conference on machine learning},
  pages={1908--1916},
  year={2014},
  organization={PMLR}
}

@article{soviany2022curriculum,
  title={Curriculum learning: A survey},
  author={Soviany, Petru and Ionescu, Radu Tudor and Rota, Paolo and Sebe, Nicu},
  journal={International Journal of Computer Vision},
  pages={1--40},
  year={2022},
  publisher={Springer}
}

@article{refinetti2022neural,
  title={Neural networks trained with SGD learn distributions of increasing complexity},
  author={Refinetti, Maria and Ingrosso, Alessandro and Goldt, Sebastian},
  journal={arXiv preprint arXiv:2211.11567},
  year={2022}
}

@inproceedings{abbe2022merged,
  title={The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks},
  author={Abbe, Emmanuel and Boix-Adsera, Enric and Misiakiewicz, Theodor},
  booktitle={Conference on Learning Theory},
  pages={4782--4887},
  year={2022},
  organization={PMLR}
}

@inproceedings{weinshall2018curriculum,
  title={Curriculum learning by transfer learning: Theory and experiments with deep networks},
  author={Weinshall, Daphna and Cohen, Gad and Amir, Dan},
  booktitle={International Conference on Machine Learning},
  pages={5238--5246},
  year={2018},
  organization={PMLR}
}

@article{weinshall2020theory,
  title={Theory of curriculum learning, with convex loss functions},
  author={Weinshall, Daphna and Amir, Dan},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={222},
  pages={1--19},
  year={2020}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{shalev2012online,
  title={Online learning and online convex optimization},
  author={Shalev-Shwartz, Shai and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={4},
  number={2},
  pages={107--194},
  year={2012},
  publisher={Now Publishers, Inc.}
}

@article{kearns1998efficient,
  title={Efficient noise-tolerant learning from statistical queries},
  author={Kearns, Michael},
  journal={Journal of the ACM (JACM)},
  volume={45},
  number={6},
  pages={983--1006},
  year={1998},
  publisher={ACM New York, NY, USA}
}

@article{mossel2004learning,
  title={Learning functions of k relevant variables},
  author={Mossel, Elchanan and O'Donnell, Ryan and Servedio, Rocco A},
  journal={Journal of Computer and System Sciences},
  volume={69},
  number={3},
  pages={421--434},
  year={2004},
  publisher={Elsevier}
}

@article{bshouty2005learning,
  title={Learning DNF from random walks},
  author={Bshouty, Nader H and Mossel, Elchanan and O’Donnell, Ryan and Servedio, Rocco A},
  journal={Journal of Computer and System Sciences},
  volume={71},
  number={3},
  pages={250--265},
  year={2005},
  publisher={Elsevier}
}

@article{arpe2008agnostically,
  title={Agnostically learning juntas from random walks},
  author={Arpe, Jan and Mossel, Elchanan},
  journal={arXiv preprint arXiv:0806.4210},
  year={2008}
}

@inproceedings{alekhnovich2003more,
  title={More on average case vs approximation complexity},
  author={Alekhnovich, Michael},
  booktitle={44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.},
  pages={298--307},
  year={2003},
  organization={IEEE}
}

@article{kalimeris2019sgd,
  title={Sgd on neural networks learns functions of increasing complexity},
  author={Kalimeris, Dimitris and Kaplun, Gal and Nakkiran, Preetum and Edelman, Benjamin and Yang, Tristan and Barak, Boaz and Zhang, Haofeng},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@inproceedings{cornacchia2023mathematical,
  title={A mathematical model for curriculum learning for parities},
  author={Cornacchia, Elisabetta and Mossel, Elchanan},
  booktitle={International Conference on Machine Learning},
  pages={6402--6423},
  year={2023},
  organization={PMLR}
}

@inproceedings{abbe2023sgd,
  title={{SGD} learning on neural networks: leap complexity and saddle-to-saddle dynamics},
  author={Abbe, Emmanuel and Adsera, Enric Boix and Misiakiewicz, Theodor},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={2552--2623},
  year={2023},
  organization={PMLR}
}

@article{arous2021online,
  title={Online stochastic gradient descent on non-convex losses from high-dimensional inference},
  author={Ben Arous, Gerard and Gheissari, Reza and Jagannath, Aukosh},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={4788--4838},
  year={2021},
  publisher={JMLRORG}
}

@inproceedings{hacohen2019power,
  title={On the power of curriculum learning in training deep networks},
  author={Hacohen, Guy and Weinshall, Daphna},
  booktitle={International Conference on Machine Learning},
  pages={2535--2544},
  year={2019},
  organization={PMLR}
}


@inproceedings{abbe2023generalization,
  title={Generalization on the unseen, logic reasoning and degree curriculum},
  author={Abbe, Emmanuel and Bengio, Samy and Lotfi, Aryo and Rizk, Kevin},
  booktitle={International Conference on Machine Learning},
  pages={31--60},
  year={2023},
  organization={PMLR}
}


@inproceedings{cv1,
  title={Easy samples first: Self-paced reranking for zero-example multimedia search},
  author={Jiang, Lu and Meng, Deyu and Mitamura, Teruko and Hauptmann, Alexander G},
  booktitle={Proceedings of the 22nd ACM international conference on Multimedia},
  pages={547--556},
  year={2014}
}

@inproceedings{cv2,
  title={Curriculumnet: Weakly supervised learning from large-scale web images},
  author={Guo, Sheng and Huang, Weilin and Zhang, Haozhi and Zhuang, Chenfan and Dong, Dengke and Scott, Matthew R and Huang, Dinglong},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={135--150},
  year={2018}
}

@inproceedings{cv3,
  title={Webly supervised learning of convolutional networks},
  author={Chen, Xinlei and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1431--1439},
  year={2015}
}


@inproceedings{cv4,
 author = {Saxena, Shreyas and Tuzel, Oncel and DeCoste, Dennis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Data Parameters: A New Family of Parameters for Learning a Differentiable Curriculum},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/926ffc0ca56636b9e73c565cf994ea5a-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{nlp1,
  title={Curriculum learning and minibatch bucketing in neural machine translation},
  author={Kocmi, Tom and Bojar, Ondrej},
  journal={arXiv preprint arXiv:1707.09533},
  year={2017}
}

@article{nlp2,
  title={An empirical exploration of curriculum learning for neural machine translation},
  author={Zhang, Xuan and Kumar, Gaurav and Khayrallah, Huda and Murray, Kenton and Gwinnup, Jeremy and Martindale, Marianna J and McNamee, Paul and Duh, Kevin and Carpuat, Marine},
  journal={arXiv preprint arXiv:1811.00739},
  year={2018}
}

@article{nlp3,
  title={Competence-based curriculum learning for neural machine translation},
  author={Platanios, Emmanouil Antonios and Stretcu, Otilia and Neubig, Graham and Poczos, Barnabas and Mitchell, Tom M},
  journal={arXiv preprint arXiv:1903.09848},
  year={2019}
}

@inproceedings{graves2017automated,
  title={Automated curriculum learning for neural networks},
  author={Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  booktitle={international conference on machine learning},
  pages={1311--1320},
  year={2017},
  organization={PMLR}
}

@article{narvekar2020curriculum,
  title={Curriculum learning for reinforcement learning domains: A framework and survey},
  author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={7382--7431},
  year={2020},
  publisher={JMLRORG}
}

@inproceedings{florensa2017reverse,
  title={Reverse curriculum generation for reinforcement learning},
  author={Florensa, Carlos and Held, David and Wulfmeier, Markus and Zhang, Michael and Abbeel, Pieter},
  booktitle={Conference on robot learning},
  pages={482--495},
  year={2017},
  organization={PMLR}
}


@article{kumar2010self,
  title={Self-paced learning for latent variable models},
  author={Kumar, M and Packer, Benjamin and Koller, Daphne},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}


@inproceedings{spitkovsky2010baby,
  title={From baby steps to leapfrog: How “less is more” in unsupervised dependency parsing},
  author={Spitkovsky, Valentin I and Alshawi, Hiyan and Jurafsky, Dan},
  booktitle={Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={751--759},
  year={2010}
}


@article{tan2019online,
  title={Online stochastic gradient descent with arbitrary initialization solves non-smooth, non-convex phase retrieval},
  author={Tan, Yan Shuo and Vershynin, Roman},
  journal={arXiv preprint arXiv:1910.12837},
  year={2019}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{abbe_cpam,
 title={Poly-time universality and limitations of deep learning},
 author={Abbe, Emmanuel and Sandon, Colin},
 journal={to appear in CPAM},
 year={2023}
}


@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}




@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}


@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}


@article{vaswani2017attention-transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}

@inproceedings{damian2024computational,
  title={Computational-Statistical Gaps in {G}aussian Single-Index Models},
  author={Damian, Alex and Pillaud-Vivien, Loucas and Lee, Jason and Bruna, Joan},
  booktitle={The Thirty Seventh Annual Conference on Learning Theory},
  pages={1262--1262},
  year={2024},
  organization={PMLR}
}

@article{lee2024neural,
  title={Neural network learns low-dimensional polynomials with {SGD} near the information-theoretic limit},
  author={Lee, Jason D and Oko, Kazusato and Suzuki, Taiji and Wu, Denny},
  journal={arXiv preprint arXiv:2406.01581},
  year={2024}
}


@article{dandi2024benefits,
  title={The benefits of reusing batches for gradient descent in two-layer networks: Breaking the curse of information and leap exponents},
  author={Dandi, Yatin and Troiani, Emanuele and Arnaboldi, Luca and Pesce, Luca and Zdeborov{\'a}, Lenka and Krzakala, Florent},
  journal={arXiv preprint arXiv:2402.03220},
  year={2024}
}

@article{mannelli2024tilting,
  title={Tilting the Odds at the Lottery: the Interplay of Overparameterisation and Curricula in Neural Networks},
  author={Mannelli, Stefano Sarao and Ivashinka, Yaraslau and Saxe, Andrew and Saglietti, Luca},
  journal={arXiv preprint arXiv:2406.01589},
  year={2024}
}

@article{bietti2023learning,
  title={On learning {G}aussian multi-index models with gradient flow},
  author={Bietti, Alberto and Bruna, Joan and Pillaud-Vivien, Loucas},
  journal={arXiv preprint arXiv:2310.19793},
  year={2023}
}

@article{bruna2023single,
  title={On single index models beyond {G}aussian data},
  author={Bruna, Joan and Pillaud-Vivien, Loucas and Zweig, Aaron},
  journal={arXiv preprint arXiv:2307.15804},
  year={2023}
}

@article{mousavi2023gradient,
  title={Gradient-based feature learning under structured data},
  author={Mousavi-Hosseini, Alireza and Wu, Denny and Suzuki, Taiji and Erdogdu, Murat A},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={71449--71485},
  year={2023}
}

@article{bietti2022learning,
  title={Learning single-index models with shallow neural networks},
  author={Bietti, Alberto and Bruna, Joan and Sanford, Clayton and Song, Min Jae},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9768--9783},
  year={2022}
}

@article{ba2024learning,
  title={Learning in the presence of low-dimensional structure: a spiked random matrix perspective},
  author={Ba, Jimmy and Erdogdu, Murat A and Suzuki, Taiji and Wang, Zhichao and Wu, Denny},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{berthier2024learning,
  title={Learning time-scales in two-layers neural networks},
  author={Berthier, Rapha{\"e}l and Montanari, Andrea and Zhou, Kangjie},
  journal={Foundations of Computational Mathematics},
  pages={1--84},
  year={2024},
  publisher={Springer}
}

@article{mahankali2024beyond,
  title={Beyond {NTK} with vanilla gradient descent: A mean-field analysis of neural networks with polynomial width, samples, and time},
  author={Mahankali, Arvind and Zhang, Haochen and Dong, Kefan and Glasgow, Margalit and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{damian2024smoothing,
  title={Smoothing the landscape boosts the signal for {SGD}: Optimal sample complexity for learning single index models},
  author={Damian, Alex and Nichani, Eshaan and Ge, Rong and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{danditwo,
  title={How Two-Layer Networks Learn, One (Giant) Step at a Time},
  author={Dandi, Yatin and Krzakala, Florent and Loureiro, Bruno and Pesce, Luca and Stephan, Ludovic},
year = {2023}
}

@article{collins2023hitting,
  title={Hitting the high-dimensional notes: An ode for {SGD} learning dynamics on {GLM}s and multi-index models},
  author={Collins-Woodfin, Elizabeth and Paquette, Courtney and Paquette, Elliot and Seroussi, Inbar},
  journal={arXiv preprint arXiv:2308.08977},
  year={2023}
}
@book {evans1998partial,
	AUTHOR = {Evans, Lawrence C.},
	TITLE = {Partial differential equations},
	SERIES = {Graduate Studies in Mathematics},
	VOLUME = {19},
	PUBLISHER = {American Mathematical Society, Providence, RI},
	YEAR = {1998},
	PAGES = {xviii+662},
	ISBN = {0-8218-0772-2},
	MRCLASS = {35-01},
	MRNUMBER = {1625845},
	MRREVIEWER = {Luigi\ Rodino},
	DOI = {10.1090/gsm/019},
	URL = {https://doi.org/10.1090/gsm/019},
}
@article {nazarov2003local,
	AUTHOR = {Nazarov, F. and Sodin, M. and Volberg, A.},
	TITLE = {Local dimension-free estimates for volumes of sublevel sets of
	analytic functions},
	JOURNAL = {Israel J. Math.},
	FJOURNAL = {Israel Journal of Mathematics},
	VOLUME = {133},
	YEAR = {2003},
	PAGES = {269--283},
	ISSN = {0021-2172,1565-8511},
	MRCLASS = {26D07 (26D15 31B05 32A40)},
	MRNUMBER = {1968431},
	MRREVIEWER = {Anthony\ Carbery},
	DOI = {10.1007/BF02773070},
	URL = {https://doi.org/10.1007/BF02773070},
}
@article{brudnyi2001distribution,
	title={The distribution of values of analytic functions on convex sets},
	author={Brudnyi, Alexander},
	journal={arXiv preprint math/0104271},
	volume={27},
	year={2001}
}

@article{carbery2001distributional,
    AUTHOR = {Carbery, Anthony and Wright, James},
     TITLE = {Distributional and {$L^q$} norm inequalities for polynomials
              over convex bodies in {$\Bbb R^n$}},
   JOURNAL = {Math. Res. Lett.},
  FJOURNAL = {Mathematical Research Letters},
    VOLUME = {8},
      YEAR = {2001},
    NUMBER = {3},
     PAGES = {233--248},
      ISSN = {1073-2780},
   MRCLASS = {26D15 (42B20 46E30)},
  MRNUMBER = {1839474},
MRREVIEWER = {Lars\ Erik\ Persson},
       DOI = {10.4310/MRL.2001.v8.n3.a1},
       URL = {https://doi.org/10.4310/MRL.2001.v8.n3.a1},
}

@article {ledoux1993inegalite,
    AUTHOR = {Ledoux, Michel},
     TITLE = {In\'egalit\'es isop\'erim\'etriques en analyse et
              probabilit\'es},
      NOTE = {S\'eminaire Bourbaki, Vol.\ 1992/93},
   JOURNAL = {Ast\'erisque},
  FJOURNAL = {Ast\'erisque},
    NUMBER = {216},
      YEAR = {1993},
     PAGES = {Exp. No. 773, 5, 343--375},
      ISSN = {0303-1179,2492-5926},
   MRCLASS = {60B11 (46B09 46N30 60E15 60G15)},
  MRNUMBER = {1246403},
}

@article{goldt2020modeling,
  title={Modeling the influence of data structure on learning in neural networks: The hidden manifold model},
  author={Goldt, Sebastian and M{\'e}zard, Marc and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={Physical Review X},
  volume={10},
  number={4},
  pages={041044},
  year={2020},
  publisher={APS}
}

@article{joshi2024complexity,
  title={On the complexity of learning sparse functions with statistical and gradient queries},
  author={Joshi, Nirmit and Misiakiewicz, Theodor and Srebro, Nathan},
  journal={arXiv preprint arXiv:2407.05622},
  year={2024}
}

@article{abbe2023provable,
  title={Provable advantage of curriculum learning on parity targets with mixed inputs},
  author={Abbe, Emmanuel and Cornacchia, Elisabetta and Lotfi, Aryo},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={24291--24321},
  year={2023}
}

@inproceedings{nitanda2024improved,
  title={Improved statistical and computational complexity of the mean-field {L}angevin dynamics under structured data},
  author={Nitanda, Atsushi and Oko, Kazusato and Suzuki, Taiji and Wu, Denny},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{arnaboldi2024repetita,
  title={Repetita iuvant: Data repetition allows {SGD} to learn high-dimensional multi-index functions},
  author={Arnaboldi, Luca and Dandi, Yatin and Krzakala, Florent and Pesce, Luca and Stephan, Ludovic},
  journal={arXiv preprint arXiv:2405.15459},
  year={2024}
}

@article{troiani2024fundamental,
  title={Fundamental limits of weak learnability in high-dimensional multi-index models},
  author={Troiani, Emanuele and Dandi, Yatin and Defilippis, Leonardo and Zdeborov{\'a}, Lenka and Loureiro, Bruno and Krzakala, Florent},
  journal={arXiv preprint arXiv:2405.15480},
  year={2024}
}

@inproceedings{damian2022neural,
  title={Neural networks can learn representations with gradient descent},
  author={Damian, Alexandru and Lee, Jason and Soltanolkotabi, Mahdi},
  booktitle={Conference on Learning Theory},
  pages={5413--5452},
  year={2022},
  organization={PMLR}
}

@article{dandi2023two,
  title={How two-layer neural networks learn, one (giant) step at a time},
  author={Dandi, Yatin and Krzakala, Florent and Loureiro, Bruno and Pesce, Luca and Stephan, Ludovic},
  journal={arXiv preprint arXiv:2305.18270},
  year={2023}
}

@article{ba2022high,
  title={High-dimensional asymptotics of feature learning: How one gradient step improves the representation},
  author={Ba, Jimmy and Erdogdu, Murat A and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37932--37946},
  year={2022}
}


@article{zweig2024single,
  title={On Single-Index Models beyond {G}aussian Data},
  author={Zweig, Aaron and Pillaud-Vivien, Loucas and Bruna, Joan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ben2022high,
  title={High-dimensional limit theorems for {SGD}: Effective dynamics and critical scaling},
  author={Ben Arous, Gerard and Gheissari, Reza and Jagannath, Aukosh},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25349--25362},
  year={2022}
}

@inproceedings{chen2023learning,
  title={Learning narrow one-hidden-layer {ReLU} networks},
  author={Chen, Sitan and Dou, Zehao and Goel, Surbhi and Klivans, Adam and Meka, Raghu},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={5580--5614},
  year={2023},
  organization={PMLR}
}

@inproceedings{yehudai2020learning,
  title={Learning a single neuron with gradient methods},
  author={Yehudai, Gilad and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={3756--3786},
  year={2020},
  organization={PMLR}
}

@article{song2021cryptographic,
  title={On the cryptographic hardness of learning single periodic neurons},
  author={Song, Min Jae and Zadik, Ilias and Bruna, Joan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={29602--29615},
  year={2021}
}

@inproceedings{goel2020superpolynomial,
  title={Superpolynomial lower bounds for learning one-layer neural networks using gradient descent},
  author={Goel, Surbhi and Gollakota, Aravind and Jin, Zhihan and Karmalkar, Sushrut and Klivans, Adam},
  booktitle={International Conference on Machine Learning},
  pages={3587--3596},
  year={2020},
  organization={PMLR}
}

@article{shamir2018distribution,
  title={Distribution-specific hardness of learning neural networks},
  author={Shamir, Ohad},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={32},
  pages={1--29},
  year={2018}
}

@article{soltanolkotabi2017learning,
  title={Learning relus via gradient descent},
  author={Soltanolkotabi, Mahdi},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{frei2020agnostic,
  title={Agnostic learning of a single neuron with gradient descent},
  author={Frei, Spencer and Cao, Yuan and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5417--5428},
  year={2020}
}

@inproceedings{wu2022learning,
  title={Learning a single neuron for non-monotonic activation functions},
  author={Wu, Lei},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4178--4197},
  year={2022},
  organization={PMLR}
}
@article{kou2024matching,
  title={Matching the Statistical Query Lower Bound for k-sparse Parity Problems with Stochastic Gradient Descent},
  author={Kou, Yiwen and Chen, Zixiang and Gu, Quanquan and Kakade, Sham M},
  journal={arXiv preprint arXiv:2404.12376},
  year={2024}
}

@inproceedings{valiant2012finding,
  title={Finding correlations in subquadratic time, with applications to learning parities and juntas},
  author={Valiant, Gregory},
  booktitle={2012 IEEE 53rd Annual Symposium on Foundations of Computer Science},
  pages={11--20},
  year={2012},
  organization={IEEE}
}

