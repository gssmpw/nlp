\documentclass[11pt]{article}
\usepackage{authblk}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath,amsthm,amssymb}
\usepackage[margin=1in]{geometry}
\usepackage[colorlinks=true]{hyperref}
\usepackage{thmtools, thm-restate}
\usepackage{algorithm}
%\usepackage{tikz}
%\usepackage{svg}
%\usepackage[inkscape=inkscape]{svg}
\usepackage{algorithm}
\usepackage{algpseudocode}

\declaretheorem{theorem}
\usepackage{float}

\newtheorem{lemma}{Lemma} 
\newtheorem{claim}{Claim} 
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{question}{Question}
\newtheorem{hypothesis}{Hypothesis} 

\newtheorem*{definition*}{Definition}


\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}

\newcommand{\FF}{\mathbb{F}}

\newcommand{\be}[1]{\begin{equation}\label{#1}}
\newcommand{\ee}{\end{equation}}

\newcommand{\A}{\mathcal{A}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\M}{\mathcal{M}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\G}{\mathcal{G}}



\newcommand{\NN}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}

\newcommand{\emptystr}{\epsilon}

\DeclareMathOperator{\nbocc}{NbOcc}
\DeclareMathOperator{\freq}{Freq}
\DeclareMathOperator{\bfreq}{BFreq}
\DeclareMathOperator{\outp}{Out}
\DeclareMathOperator{\good}{Balanced}
\DeclareMathOperator{\select}{Select}
\DeclareMathOperator{\capital}{Capital}

\newcommand{\casestext}[2][2in]{\parbox[t]{#1}{\strut\ignorespace #2\strut}}

\title{The Agafonov and Schnorr-Stimm theorems for probabilistic automata}
%\author{}

\author{Laurent Bienvenu, Hugo Gimbert, Subin Pulari}
%\affil[1]{
%Universit\'e de Bordeaux, CNRS, Bordeaux INP, LaBRI, UMR 5800, F-33400, Talence, France
%}


\date {\today}

\begin{document}

\maketitle
\begin{abstract}
	
	For a fixed alphabet $A$, an infinite sequence $X$ is said to be normal if every word $w$ over $A$ appears in $X$ with the same frequency as any other word of the same length. A classical result of Agafonov (1966) relates normality to finite automata as follows: a sequence $X$ is normal if and only if any subsequence of $X$ selected by a finite automaton is itself normal. Another theorem of Schnorr and Stimm (1972) gives an alternative characterization: a sequence $X$ is normal if and only if no gambler can win large amounts of money by betting on the sequence $X$ using a strategy that can be described by a finite automaton. Both of these theorems are established in the setting of deterministic finite automata. This raises the question as to whether they can be extended to the setting of probabilistic finite automata. In the case of the Agafonov theorem, this question was positively answered by L\'echine et al.\ (2024) in a restricted case of probabilistic automata with rational transition probabilities. 

In this paper, we settle the full conjecture by proving that both the Agafonov and the Schnorr-Stimm theorems hold true for arbitrary probabilistic automata. Specifically, we show that a sequence $X$ is normal if and only if any probabilistic automaton selects a normal subsequence of $X$ with probability $1$. We also show that a sequence $X$ is normal if and only if a probabilistic finite-state gambler fails to win on $X$ with probability $1$. 

\end{abstract}

\section{Introduction}

Given a finite alphabet~$A$ of $k$ letters, an infinite sequence~$X$ of letters is said to be \emph{normal} if every word of $A^*$ appear as sub-word of~$X$ with the same frequency as any other word of the same length, namely, $(1/k)^{|w|}$. The famous Champernowne sequence 
\[
01234567891011121314151617181920212223242526\ldots
\]
can be shown to be normal over the alphabet $A=\{0, \ldots, 9\}$. The number~$\pi$, for example, is conjectured to have a normal expansion in every base, though this very much remains an open question. Normal sequences are plentiful, and an easy way to generate a normal sequence~$X$ is to draw each letter $X(n)$ at random in the alphabet $A$ (all letters having the same probability $1 / |A|$) independently of the other chosen letters $X(m)$. The law of large numbers tells us that we obtain a normal sequence with probability~$1$. \\

Of course, there are also plenty of examples of non-normal sequences: \\
\begin{itemize}
\item Periodic, or ultimately periodic, sequences can never be normal: indeed if the period of~$X$ is~$k$ there are only $k$ possible sub-words of~$X$ of length~$k$ hence most words of length~$k$ will not appear and their frequency will be~$0$. 
\item Sturmian sequences, which are sequences with only $k+1$ different sub-words of length~$k$ (such as the Fibonacci sequence 100101001001010010100100101001001... obtained by iterating the morphism $0 \mapsto 01$ and $1 \mapsto 0$) are not normal for the same reason. 
\item The Thue-Morse sequence $01101001100101101001011001101001...$ obtained by iterating the morphism $0 \mapsto 01$ and $1 \mapsto 10$ is not normal because $000$ and $111$ do not appear as sub-words. 
\item A sequence of $0$ and $1$'s generated at random where each bit is chosen equal to the previous one with probability $2/3$ will have, with probability~$1$, all possible finite words as sub-words but will not (still with probability $1$) be normal as for example the word $00$ will appear with frequency $1/3$ instead of $1/4$. 
\end{itemize}



It turns out that normality has a nice interpretation in terms of finite automata. Indeed, two classical results, one due to Agafonov and the other due to Schnorr and Stimm, assert that an infinite sequence is normal if and only if it cannot be predicted by a finite automaton with better-than-average accuracy. Of course, one needs to specify what `predicted' means. We consider two prediction models. 

\begin{itemize}
\item[(I)] In the Agafonov model, an automaton reads the infinite sequence one letter at a time and updates its state in the usual way. Some of its states have a `select' tag on them. When the current state has such a tag, the next letter will be selected and added to a subsequence~$Y$. We consider the automaton successful at predicting~$X$ if the subsequence~$Y$ built in this process is infinite and some letter of $A$ does not have asymptotic frequency $1/|A|$ in~$Y$. This means that the automaton has exhibited a statistical anomaly in the sequence~$X$ and isolated this anomaly in the subsequence~$Y$. 

\item[(II)] In the Schnorr-Stimm model, the predictor is still an automaton but this time is viewed as a gambling strategy. The gambler starts with a capital of $\$1$. Each state~$q$ is labeled with a betting function $\gamma_q: A \rightarrow \R^{\geq 0}$. This function represents the amount by which the predictor would like her capital to be multiplied by depending on the value of the next bit. For example, suppose the player plays against a sequence $X \in \{a,b,c\}^\omega$. If her current capital is $\$2$ and  the current state~$q$ is labelled by a betting function $\gamma_q$ such that $\gamma_q(a)=0.7$, $\gamma_q(b)=1.1$ and $\gamma(c)=1.2$, if the next letter is $a$, her new capital will be $\$1.4$, if it is $b$ her new capital will be $\$ 2.2$ and if it is~$c$ her new capital will be $\$2.4$. For the game to be fair, each betting function $\gamma_q$ must satisfy $\frac{1}{|A|}\sum_{a \in A} \gamma_q(a) = 1$. We say that the predictor wins if the capital of the player takes arbitrarily large values throughout the (infinite) game. That is, the predictor has spotted some type of statistical anomaly and is exploiting it to get rich!
\end{itemize}


Both of these models lead to the same conclusion: an infinite sequence $X$ is normal if and only if it is unpredictable (by a finite automaton). 

\begin{theorem}[Agafonov~\cite{Agafonov1968}]
For $X \in A^\omega$, the following are equivalent.
\item[(i)] $X$ is normal. 
\item[(ii)] For any automaton $\A$ that selects a subsequence~$Y$ of~$X$ as in model~I, either~$Y$ is finite, or every letter of $A$ appears in~$Y$ with asymptotic frequency $1/|A|$.
\item[(iii)] For any automaton $\A$ that selects a subsequence~$Y$ of~$X$ as in model~I, either~$Y$ is finite, or $Y$ is normal. 
\end{theorem}

(see also Carton~\cite{Carton2020} and Seiller and Simonsen~\cite{SeillerS2020} for a modern account of this theorem). 


\begin{theorem}[Schnorr-Stimm~\cite{SchnorrS1972}]
For $X \in A^\omega$, the following are equivalent.
\item[(i)] $X$ is normal. 
\item[(ii)] Any automaton $\A$ betting on~$X$ according to model~II does not win. 
\end{theorem}


In both of these theorems, the finite automata used for prediction are assumed to be deterministic. Would the situation changed if one allowed probabilistic automata? In principle, one would not expect an unpredictable sequence to become predictable in the presence of a random source. Indeed, given a sequence~$X$ and a random source~$R$ it seems, informally speaking, that almost surely~$R$ will not `know' anything about~$X$ and thus will not help predicting~$X$. Surprisingly, this intuition is wrong in the setting where the predictors are not finite automata but Turing machines, as shown by Bienvenu et al.~\cite{BienvenuDS2022} who built a sequence that is unpredictable by deterministic Turing machines (in either prediction model of selection or gambling) and becomes predictable (in either model) if one allows probabilistic Turing machines. Nonetheless, finite automata are much weaker than Turing machines and Bienvenu et al.'s construction cannot work for such a memoryless model of computation. And indeed, recently, Léchine et al.\ showed that Agafonov's theorems holds for probabilistic automata in the restricted case where the transition probabilities are rational. 

\begin{theorem}[Léchine et al.~\cite{LechineSS2024}]
For $X \in A^\omega$, the following are equivalent.
\item[(i)] $X$ is normal. 
\item[(ii)] For any probabilistic automaton $\A$ with rational probabilities, almost surely, $\A$ selects a subsequence~$Y$ of~$X$ that is either finite, or every letter of $A$ appears in~$Y$ with asymptotic frequency $1/|A|$.
\item[(iii)] For any probabilistic automaton $\A$ with rational probabilities, almost surely, $\A$ selects a subsequence~$Y$ of~$X$ such that either~$Y$ is finite, or $Y$ is normal. 
\end{theorem}


This led them to conjecture that the probabilistic version of Agafonov's theorem holds in the general case. In this paper, we prove this conjecture and also prove the probabilistic version of the Schnorr-Stimm theorem. Additionally, we establish a probabilistic version of the Schnorr-Stimm dichotomy regarding the winning rates of probabilistic gamblers.

It is worth noting that Léchine et al.'s proof is a reduction of the rational probabilistic case to the deterministic case. Our proof is also a reduction to the deterministic case, but we will need an extension of the deterministic case to Bernoulli measures (an extension which was proved by Seiller and Simonsen~\cite{SeillerS2020}), which will be presented in the next section. \\

\subsection*{Notation and terminology}


We finish this introduction by formalizing the concepts discussed so far and gathering the notation and terminology that will be used in the rest of the paper. \\


Given an alphabet $A$, we denote by $A^*$ the set of finite words over $A$, by $A^n$ the set of words of length~$n$, by $A^\omega$ the set of infinite sequences of letters and by $A^{\leq \omega}$ the set $A^* \cup A^\omega$. For $X \in A^{\leq \omega}$, we denote by $X(i)$ the $i$-th letter of $X$ (by convention there is a $0$-th letter) and by $X[i,j]$ the word $X(i)X(i+1)\ldots X(j)$. Let $\lambda$ denote the empty string.\\

Given a word~$u$ of length~$k$ and a word~$w$ of length $n \geq k$, we denote by $\nbocc(u,w)$ the number of occurrences of the word~$u$ in~$w$, i.e.,
\[
\nbocc(u,w) = \# \{i : 0 \leq i \leq n-k, \ w[i,i+k-1]=u\}
\]
and the frequency of occurrence $\freq(u,w)$ of~$u$ in~$w$ is naturally defined by 
\[
\freq(u,w) = \frac{\nbocc(u,w)}{n-k+1}
\]
When~$X$ is an infinite sequence, we define
\[
\freq^-(u,X) = \liminf_{n \rightarrow \infty}\, \freq(u,X[0..n]) ~ ~ \text{and} ~ ~ \freq^+(u,X) = \limsup_{n \rightarrow \infty}\,  \freq(u,X[0..n])
\]
When $\freq^-(u,X)$ and $\freq^+(u,X)$ have the same value, we simply call this common value $\freq(u,X)$. \\

Given $X \in A^\omega$, we say that $X$ is \emph{balanced} if all letters appear in~$X$ with the expected frequency, i.e., $\freq(a,X)=1/|A|$ for all~$a \in A$. We say that $X$ is \emph{normal} if all words appear in $X$ with the expected frequency, i.e., $\freq(w,X)=|A|^{-|w|}$ for all $w \in A^*$. \\


A \emph{deterministic automaton (DFA)} is a tuple $(Q,A,q_I,\delta)$ where $Q$ is a finite set of states, $A$ a finite alphabet, $q_I$ the initial state and $\delta: Q \times 	A \rightarrow Q$ the transition function (in this paper, runs of automata are meant to be infinite hence there is no need for final states). We denote by $\delta^*$ the function from $Q \times A^*$ to $Q$ defined inductively by $\delta^*(q,\emptystr)=q$ where $\emptystr$ is the empty word and for $w \in A^*$ and $a \in A$, $\delta^*(q,w \cdot a) = \delta(\delta^*(q,w),a)$, where $\cdot$ is the concatenation of words. 

An \emph{automatic selector} (or \emph{selector} for short) is a tuple $(Q,A,q_I,\delta,S)$ where $(Q,A,q_I,\delta)$ is a DFA and $S$ is a subset of $Q$, representing the selection states. 

Given a selector $\S=(Q,A,q_I,\delta,S)$, we define the selection function from $A^*$ to $A^*$ inductively by:
\[
\select(\S,\emptystr)=\emptystr
\]
and for $w \in A^*$ and $a \in A$:
\[
\select(\S,w \cdot a) = \left \{
\begin{array}{lr}
\select(\S,w) \cdot a &  \text{ if } \delta^*(q_I,w) \in S\\
\select(\S,w) &  \text{ if } \delta^*(q_I,w) \notin S\\
\end{array}
\right.
\]

If $X$ is an infinite sequence in $A^\omega$, the sequence of words $\select(\S,X[0..n])$ is non-decreasing with respect to the prefix order and thus converges to a sequence $Y \in A^{\leq \omega}$ which we call \emph{the selected subsequence of $X$ selected by $\S$} and denote by $\select(\S,X)$. \\

An \emph{automatic gambler} (or \emph{gambler} for short) is a tuple $(Q,A,q_I,\delta,\gamma)$ where $(Q,A,q_I,\delta)$ is a DFA and $\gamma$ is a function from $Q \times A$ to $\R^{\geq 0}$ such that for all~$q$, $\frac{1}{|A|}\sum_{a \in A} \gamma(q,a)=1$. As said above, the value of $\gamma(q,a)$ should be interpreted as the multiplier by which the gambler, being currently in state~$q$, would like her capital to be multiplied by if the next read letter is~$a$. The condition $\frac{1}{|A|} \sum_{a \in A} \gamma(q,a)=1$ ensures that the game is fair. \\


Given a gambler $\G=(Q,A,q_I,\delta,\gamma)$ and $w \in A^*$, we define $\capital(\G,w)$ inductively by
\[
\capital(\G,\emptystr)=1
\] 
and for $w \in A^*$ and $a \in A$,
\[
\capital(\G,w \cdot a)=\capital(\G,w) \cdot \gamma(\delta^*(q_I,w),a)
\]
and we say that a gambler $\G$ \emph{wins against $X \in A^\omega$} if 
\[
\limsup_{n \rightarrow +\infty}\,  \capital(\G,X[0..n]) = +\infty
\]
(otherwise we say that $\G$ loses). 




\section{Deterministic prediction for Bernoulli measures}

In classical normality, all letters of the alphabet occur with the same frequency. We can however consider the extension of normality to Bernoulli measures. A Bernoulli measure over $A^\omega$ is a probability measure where letters of an infinite sequence~$X$ are drawn at random independently of one another but the distribution over the alphabet $A$ is non-uniform. 

\begin{definition}
Let  $\mu: A \rightarrow [0,1]$ be a distribution over the alphabet $A$ (hence satisfies $\sum_{a \in A} \mu(a) = 1$). The Bernoulli measure induced my~$\mu$, which we also denote by $\mu$ by abuse of notation, is the unique probability measure such that for all~$i,k$, for every word $w=a_0, \ldots a_k \in A$,
\[
\Pr_{X \sim \mu} \Big[ X[i,i+k]=w \Big]= \prod_{j=0}^k \mu(a_{j})
\]
We also denote by $\mu(w)$ the quantity $\prod_{j=0}^k \mu(a_{j})$.    
\end{definition}

Normality generalizes very naturally to Bernoulli measures. 


\begin{definition}
Let $\mu$ be a Bernoulli measure. A sequence~$X \in A^\omega$ is \emph{$\mu$-balanced} if $\freq(a,X)=\mu(a)$ for all~$a \in A$. It is \emph{$\mu$-normal} if for all words~$w \in A^*$, $\freq(w,X)=\mu(w)$.
\end{definition}

We say that a Bernoulli measure~$\mu$ is \emph{positive} when $\mu(a)>0$ for every letter~$a$. In the rest of the paper, all Bernoulli measures will be assumed to be positive, and we simply say `Bernoulli measure' to mean `positive Bernoulli measure'. 


%In our proof of the Schnorr-Stimm theorem for Bernoulli measures, we require the block-wise characterization of $\mu$-normality. Let $\bfreq(u,w)$ denote the frequency of block-wise occurrences of the word $u$ in $w$. For $w \in A^*$, let $\bfreq^-(w,X)$, $\bfreq^+(w,X)$ and $\bfreq(w,X)$ denote the lower, upper and limit block frequency of $w$ in $X$ defined similarly as $\freq^-(w,X)$, $\freq^+(w,X)$ and $\freq(w,X)$. A sequence $X$ is \emph{$\mu$-block normal} if for all words~$w \in A^*$, $\bfreq(w,X)=\mu(w)$. The following was shown in Simonsen and Seiller.
%
%\begin{lemma}[\cite{SeillerS2020}]
%	If a sequence $X \in A^\omega$ is $\mu$-normal then $X$ is $\mu$-block normal.
%\end{lemma}
%
%We note that the converse implication  also holds.
%
%\begin{lemma}
%\label{lem:blocknormalimpliesnormal}
%	If a sequence $X \in A^\omega$ is $\mu$-block normal then $X$ is $\mu$-normal.
%\end{lemma}
%
%We give a proof of Lemma \ref{lem:blocknormalimpliesnormal} in the appendix. The above lemmas completes the proof of the following equivalence theorem between $\mu$-normality and $\mu$-block normality.
%
%\begin{theorem}
%	A sequence $X \in A^\omega$ is $\mu$-normal if and only if $X$ is $\mu$-block normal.
%\end{theorem}


The Agafonov theorem can be extended to Bernoulli measures, as proven by Seiller and Simonsen~\cite{SeillerS2020}. It is this theorem that we will use in the next section to obtain a proof of the Agafonov theorem for probabilistic selectors. 


%NOTE: WE USE BLOCK NORMALITY LATER SO THIS NEEDS TO BE DISCUSSED\\


%
%Given an automaton $\A$ over an alphabet $A$ whose set of states is~$Q$ and a Bernoulli measure~$\mu$ over $A$, we denote by $\A^\mu$ the Markov chain over $Q$ with initial state the initial state of $\A$ and where random transitions correspond to the process of sequentially picking letters at random according to $\mu$ and apply the transition of $\A$. 
%
%TODO: maybe a small picture here? 
%
%
%\begin{theorem} 
%Let $\A$ be a DFA over an alphabet $A$, $\mu$ a Bernoulli measure over $A$ and assume that the Markov chain $\A^\mu$ is irreducible. Let $X$ be a $\mu$-normal sequence. Then, every state $q$ of $\A$ is visited with asymptotic frequency $\xi(q)$ during the run of $\A$ on~$X$, where $\xi$ is the invariant measure of $\A^\mu$.  
%\end{theorem}
%
%
%TODO: PROOF \\
%
%TODO: COROLLARY: Agafonov's theorem for deterministic automata and Bernoulli measures. (TODO: mention that it is already proven in~\cite{SeillerS2020} and also implicit in Carton but we prove it here for completeness)


\begin{theorem}[Agafonov theorem for Bernoulli measures~\cite{SeillerS2020}] \label{thm:agafonov-bernoulli}
For $X \in A^\omega$, the following are equivalent.
\item[(i)] $X$ is $\mu$-normal. 
\item[(ii)] For any selector $\S$ that selects a subsequence~$Y$ of~$X$, either~$Y$ is finite or $Y$ is $\mu$-balanced.
\item[(iii)] For any selector $\S$ that selects a subsequence~$Y$ of~$X$, either~$Y$ is finite or $Y$ is $\mu$-normal.
\end{theorem}


We can also easily generalize the notion of gambler to the setting of Bernoulli measures: it suffices to define a $\mu$-gambler $\G=(Q,A,q_I,\delta,\gamma)$ as before but with the fairness condition on~$\gamma$ replaced by $\sum_{a \in A} \mu(a)\gamma(q,a) =1$ for every~$q$. The function $\capital$ and the notion of success are defined as before. \\ 

We will now prove that the Schnorr-Stimm theorem, just like the Agafonov theorem, can also be extended to Bernoulli measures. 


\begin{theorem}[Schnorr-Stimm theorem for Bernoulli measures] \label{thm:schnorr-stimm-bernoulli}
For $X \in A^\omega$ and $\mu$ a Bernoulli measure, the following are equivalent.
\item[(i)] $X$ is $\mu$-normal. 
\item[(ii)] No $\mu$-gambler $\G$ wins by betting on~$X$.  
\end{theorem}

\begin{proof}
$(i) \Rightarrow (ii)$. Suppose that $X \in A^\omega$ is $\mu$-normal and consider a $\mu$-gambler $\G=(Q,A,q_I,\delta,\gamma)$. 

We can assume that the $\mu$-gambler only has one state on which it places a non-trivial bet. Indeed, define for every $q \in Q$ the $\mu$-gambler $\G^{[q]}=(Q,A,q_I,\delta,\gamma^{[q]})$ where $\gamma^{[q]}(q,a)=\gamma(q,a)$ for all~$a$ and $\gamma^{[q]}(q',a)=1$ for $q' \not = q$. That is, $\G^{[q]}$ is the gambler $\G$ where all states but state~$q$ are neutralized (no bet is placed while on them). By the multiplicative nature of the function $\capital$, we have for all~$n$:
\[
\capital(\G,X[0..n]) = \prod_{q \in Q} \capital(\G^{[q]},X[0..n])
\]
Thus if we can show that all quantities $\capital(\G^{[q]},X[0..n])$ are bounded, we are done. Let thus assume there is a state~$r$ that is the unique state on which $\G$ bets. If instead of $\G$ we consider the selector $\S=(Q,A,q_I,\delta,\{r\})$ with only $r$ as selecting state, we know by Agafonov's theorem for Bernoulli measures (Theorem~\ref{thm:agafonov-bernoulli}) that the subsequence $Y$ of $X$ selected by $\S$ is $\mu$-normal, hence in particular $\mu$-balanced. But this subsequence is precisely the values of $X$ on which $\G$ bets! 

We can further assume that $Y$ is infinite, otherwise it means that the run of $\G$ on~$X$ passes by~$r$ finitely often, hence $\G$ certainly cannot win as other states are not betting states. Now, suppose that at stage $n$ of the run on~$X$ the state~$r$ has been visited $k=k(n)$ times. We have 
\[
\capital(\G,X[0..n+1]) = \prod_{a \in A} \gamma(r,a)^{\nbocc(a,Y[0..k])}
\] 
But since $Y$ is $\mu$-normal we have, for every~$a$, $\nbocc(a,Y[0..k]) = \mu(a)k + o(k)$. Thus,
\[
\capital(\G,X[0..n+1]) = \prod_{a \in A} \gamma(r,a)^{\mu(a)k + o(k)}
\] 
or equivalently 
\[
\log \capital(\G,X[0..n+1]) = (k+o(k)) \cdot \sum_{a \in A} \mu(a)\cdot  \log \gamma(r,a) 
\] 
(here we assume that all values $\gamma(r,a)$ involved in the product are positive for if not then the capital falls to~$0$ and we are done). Since we have  $\sum_{a \in A} \mu(a)=1$ ($\mu$ being a distribution), we can use the strict concavity of the function $\log$ on $(0,+\infty)$ to apply Jensen's inequality and get 
\[
\sum_{a \in A} \mu(a)\cdot  \log \gamma(r,a) \leq \log \left( \sum_{a \in A} \mu(a) \gamma(r,a)\right)
\]
with strict inequality when not all $\gamma(q,a)$ are equal (which is the case where $\G$ makes non-trivial bets). But by the fairness condition, we have $\sum_{a \in A} \mu(a) \gamma(r,a)=1$ hence we see that $\log \capital(\G,X[0..n+1])$ is either $0$ or ultimately negative which either way means that $\G$ does not win. \\

$(ii) \Rightarrow (i)$ Assume that $X$ is not $\mu$-normal. This means that there is some word~$w$ such that $\freq(w,X[0..n])$ does not converge to $\mu(w)$. Let us assume that $w$ is a minimal such word and write $w=ux$ with $u \in A^*$ and $x \in A$. 

Consider the sequence of vectors $f_n$ defined be
\[
f_n = \left( \frac{\freq(ua,X[0..n])}{\sum_{b \in A} \freq(ub,X[0..n])} \right)_{a \in A}
\]
 
or equivalently, 
\[
f_n = \left( \frac{\nbocc(ua,X[0..n])}{\sum_{b \in A} \nbocc(ub,X[0..n])} \right)_{a \in A} 
\]


All of these vectors belong to the set $\Gamma=\{f: A \rightarrow [0,1] : \sum_{a \in A} f(a)=1\}$. This is a compact set, hence the sequence $f_n$ must have cluster points. By definition of $u$ and $x$, we know that $f_n$ does not converge to $\mu$ because $f_n(x)$ does not converge to $\mu(x)$: Indeed, in the definition of $f_n(x)$, the denominator converges to $\freq(u,X)$ which by minimality of~$w$ is defined and equal to $\mu(u)$, while the numerator is equal to $\freq(w,X[0..n])$ which by definition of $w$ does not converge to $\mu(w)=\mu(u)\mu(x)$. 


Therefore, the sequence $(f_n)$ must have at least one cluster point~$\nu$ different from $\mu$. Fix such a cluster point $\nu$. 

We now build our gambler $\G=(Q,A,q_I,\delta,\gamma)$. The idea is that the gambler will record the last $|u|$ bits it read and will only place bets when these exactly form the word~$u$. Let us thus take $Q=\{q_v : v \in A^*, |v| \leq |u|\}$ initial state $q_I=q_\emptystr$ and define $\delta$ by 
\begin{align*}
\delta(q_v,a)= \begin{cases}
		q_{va}, & \text{if } \lvert v \rvert < |u|\\
		q_{v'a} & \text{ if } |v|=|u| \text{ and } v=xv' \text{ with } x \in A		\end{cases}
\end{align*}

Now, define $\gamma(q_v,a)=1$ whenever $v \not= u$ and $\gamma(q_u,a) = \nu(a)/\mu(a)$ for all~$a \in A$ 
%
%(if $\mu(a)=0$, then we can by convention pick $\nu(a)/\mu(a)$ to be $1$ if $\nu(a)=0$ as well, otherwise we set it to be $+\infty$, with the convention $0 \cdot (+\infty)=0$ in the verification of the fairness condition). 


Observe that this is a valid $\mu$-gambler as the fairness condition is satisfied: $\sum_{a \in A} \nu(a)/\mu(a) \cdot \mu(a) = \sum_a \nu(a) = 1$.\\

Suppose that after reading $n$ letters of~$X$ the state $q_u$ has been visited~$k=k(n)$ times. 

First, observe that $k(n)$ tends to $+\infty$. Indeed, the state $q_u$ is visited whenever $u$ is seen as a subword of $X$. But we assumed that $u$ appears in~$X$ with frequency $\mu(u)$, by minimality of $w$. 

%If $\mu(u)$ were equal to $0$, then $u$ would appear with frequency $0$ and thus $w$, which has $u$ as prefix, would also appear with frequency $0=\mu(u)\mu(x)=\mu(w)$, contradicting the choice of~$w$. Thus $u$ appears in~$X$ with frequency $\mu(u)>0$ and a fortiori it appears infinitely many times. 

Second, unfolding the definition of $f_k$, we have,
\[
\capital(\G,X[0,n+1]) = \prod_{a \in A} \left( \frac{\nu(a)}{\mu(a)} \right)^{k \cdot f_k(a)}
\] 
This gives
\[
\log \capital(\G,X[0,n+1])= k \cdot \sum_{a \in A} f_k(a) \log \left( \frac{\nu(a)}{\mu(a)} \right)
\]
Since $\nu$ is a cluster point of the sequence $f_k$, for any fixed $\varepsilon>0$ there are infinitely many~$n$ (or $k$) such that
\[
\sum_{a \in A} f_k(a) \log \left( \frac{\nu(a)}{\mu(a)} \right) \geq \sum_{a \in A} \nu(a) \log \left( \frac{\nu(a)}{\mu(a)} \right)  - \varepsilon
\]
But the term $\sum_{a \in A} \nu(a) \log \left( \frac{\nu(a)}{\mu(a)} \right)$ is the relative entropy from $\mu$ to $\nu$ (also known as Kullback-Liebler divergence, see for example~\cite{CoverT2006}) $\mathrm{D}_{KL}(\nu || \mu)$. This quantity is nonnegative in general and is positive when $\nu \not= \mu$, which is the case here. We have thus established that for any fixed $\varepsilon$, there are arbitrarily large~$n$ and $k$ such that 
\[
\log \capital(\G,X[0,n+1]) \geq k \left( \mathrm{D}_{KL}(\nu || \mu) - \varepsilon \right)
\]
Taking any $\varepsilon < \mathrm{D}_{KL}(\nu || \mu)$, this shows that 
\[
\limsup_{n \rightarrow +\infty} \capital(\G,X[0,n]) = +\infty
\]


\end{proof}



Let us note that this last proof actually gives us a finer analysis of normality, in terms of the rate of failure or success of the gambler. This was already observed by Schnorr and Stimm in their seminal paper, where they proved the following.

\begin{theorem}[Schnorr-Stimm dichotomy theorem~\cite{SchnorrS1972}]
Let $X$ be an infinite sequence in $A^\omega$. \\
(i) If $X$ is normal and $\G$ is a gambler, then the capital of $\G$ throughout the game either is ultimately constant or decreases at an exponential rate.\\
(ii) If $X$ is not normal, then there exists a gambler~$\G$ which wins against~$X$ at an `infinitely often' exponential rate (i.e., $\limsup_n \log(\capital)/n >0$). 
\end{theorem}

As a byproduct of our proof of~Theorem~\ref{thm:schnorr-stimm-bernoulli}, we have the same dichotomy for positive Bernoulli measures (i.e., Bernoulli measures such that $\mu(a)>0$ for every letter):

\begin{theorem}[Schnorr-Stimm dichotomy theorem for Bernoulli measures] \label{thm:schnorr-stimm-dicho-bernoulli}
\label{thm:bernoullidichotomy}
Let $X$ be an infinite sequence in $A^\omega$ and $\mu$ a positive Bernoulli measure. \\
(i) If $X$ is $\mu$-normal and $\G$ is a $\mu$-gambler, then the capital of $\G$ throughout the game either is ultimately constant or decreases at an exponential rate.\\
(ii) If $X$ is not $\mu$-normal, then there exists a $\mu$-gambler~$\G$ which wins against~$X$ at an `infinitely often' exponential rate.
\end{theorem}

Our proof of~Theorem~\ref{thm:schnorr-stimm-bernoulli} almost establishes this, but we do need an additional technical lemma:

\begin{lemma}
\label{lem:ergodiconnormal}
	Let $(Q,A,q_I,\delta)$ be a finite-state automaton, $\mu$ be a positive Bernoulli measure and let $V_q(n,X)$ denote the number of times the state $q$ is visited upon running the automaton using the first $n$ bits of $X \in A^\omega$. 
	
	Then, for every $q \in Q$, there exists a real number $\pi_q \geq 0$ such that, for every $\mu$-normal sequence $X \in A^\omega$:
	\begin{itemize}
	\item either $V_q(n,X)$ is ultimately constant (i.e., $q$ is visited only finitely often during the run on~$X$)
	\item or, $	\lim\limits_{n \to \infty} \frac{V_q(n,X)}{n} = \pi_q$ (i.e., the state~$q$ is visited with asymptotic frequency $\pi_q$) and this second case can only happen when $\pi_q>0$.
		\end{itemize} 
	
	
	
\end{lemma}

\begin{proof}
	Running an automaton upon a normal sequence, starting from any state, a strongly connected component must be reached in finitely many steps. Similar to \cite{SeillerS2020}, let us consider the Markov chain corresponding to the $\lvert Q \rvert \times \lvert Q \rvert$ stochastic matrix $\mathbf{P}$ where,
\begin{align*}
\mathbf{P}_{ij}=\sum\limits_{a \in A} \mu(a) \cdot 1_{\delta(i,a)=j}.	
\end{align*}
The proof follows using the Ergodic Theorem for Markov chains and the same steps in the proof of Lemma 4.5 from \cite{SchnorrS1972} by replacing the uniform measure with the positive Bernoulli measure induced by $\mu$. We note that for this line of proof uses the notion of \emph{$\mu$-block normality} defined using the block-wise occurrences of a words within an infinite sequence instead of the notion of normality we used in this paper. However, the two notions are equivalent. See section \ref{sec:blocknormality} for a full proof.  
\end{proof}



\begin{proof}[Proof of Theorem~\ref{thm:schnorr-stimm-dicho-bernoulli}]
In part $(i) \Rightarrow (ii)$ of our proof of~Theorem~\ref{thm:schnorr-stimm-bernoulli}, we showed that on a given state~$r$, either $r$ is not a betting state  ($\gamma$ is the constant~$1$ on this state) or it is and then the gambler  loses money exponentially fast in the number of times this state is visited, the exponent being $\alpha_r = \sum_{a \in A} \mu(a)\cdot  \log \gamma(r,a)$ which we proved to be negative. By Lemma~\ref{lem:ergodiconnormal}, betting states are either visited finitely often or with positive asymptotic density. If they are all visited finitely often, the capital stabilizes after the last bet is made. Otherwise, if betting states $r$ are visited with frequency $\pi_r$ and at least one $\pi_r$ is positive, then the gambler loses at an exponential rate, where the exponent is $\sum_r \pi_r \alpha_r$.\\

In part $(ii) \Rightarrow (i)$, under the assumption of non-$\mu$-normality of~$X$, we built a gambler $\G$ satisfying the following: for any fixed $\varepsilon$, there are arbitrarily large~$n$ and $k$ such that $
\log \capital(\G,X[0,n+1]) \geq k \left( \mathrm{D}_{KL}(\nu || \mu) - \varepsilon \right)$. Here, $k$ in the number of visits to a state $q_u$. In the proof of Theorem \ref{thm:schnorr-stimm-bernoulli} part $(ii)$, this state $q_u$ is visited with frequency $\mu(u)$ for large enough $n$. Hence the gambler has an `infinitely often' exponential rate of success with exponent $\mu(u) D_{KL}(\nu || \mu)$. 
\end{proof}

%In order to prove Theorem \ref{thm:bernoullidichotomy} we require the following technical lemma.
%\begin{lemma}
%\label{lem:ergodiconnormal}
%	Let $(Q,A,q_I,\delta)$ be a finite-state automaton, $\mu$ be a Bernoulli measure and let $V_q(n,X)$ denote the number of times the state $q$ is visited upon running the automaton using the first $n$ bits of $X \in A^\omega$. Then, for every $q \in Q$, there exist real numbers $\pi_q \geq 0$ such that for any $\mu$-normal sequence $X \in A^\omega$ and $q \in Q$,
%	\begin{align*}
%	\lim\limits_{n \to \infty} \frac{V_q(n,X)}{n} = \pi_q.	
%	\end{align*}
%\end{lemma}
%

%
%Now, we prove Theorem \ref{thm:bernoullidichotomy}.
%
%\begin{proof}[Proof of Theorem \ref{thm:bernoullidichotomy}]
%\textit{(i).} Consider the case when $X$ is $\mu$-normal. Consider the gambler in proof of part \textit{(i)} of Theorem \ref{thm:schnorr-stimm-bernoulli}. We showed that 
%\[
%\log \capital(\G,X[0..n+1]) = (k+o(k)) \cdot \sum_{a \in A} \mu(a)\cdot  \log \gamma(r,a) 
%\]
%where $k=k(n)$ is the number of times the gambler visits a state $r$, where the gambler places non-trivial bets. Using Lemma \ref{lem:ergodiconnormal}, we obtain $k=\pi_r \cdot n +o(n)$. Using the fairness condition, we obtain that $\log \capital(\G,X[0..n+1])$ is either $0$ or ultimately less than $\pi_r \cdot n \cdot c$ for some negative constant $c$. This completes the proof of part \textit{(i)}.
%
%\textit{(ii).} Consider the case when $X$ is not $\mu$-normal. In the proof of Theorem \ref{thm:schnorr-stimm-bernoulli} we constructed a gambler $\G$ such that for infinitely many $n$, $\log \capital(\G,X[0,n\ell-1])= n \left( \mathrm{D}_{KL}(\nu || \mu) - \varepsilon \right)$. Hence, $\G$ which wins against~$X$ at an `infinitely often' exponential rate.
%\end{proof}




\section{The Agafonov theorem for PFAs}

We now want to prove the extension of Theorem~\ref{thm:agafonov-bernoulli} to probabilistic automata/selectors. A \emph{probabilistic finite automaton (PFA)} is a tuple $(Q,A,q_I,\delta)$ where $Q$ is a finite set of states, $A$ a finite alphabet, $q_I$ the initial state and $\delta: Q \times 	A \rightarrow \Delta(Q)$ is a probabilistic transition function, that is, $\Delta(Q)$ is the set of probability distributions over~$Q$. In this setting, we define inductively the random variables $\delta^*(q,w)$ by $\delta^*(q,\emptystr)=q$ and for $w \in A^*$ and $a \in A$, the event $\delta^*(q,w \cdot a) = q'$ is defined as the union
\[
\bigcup_{r \in Q} \left[ \delta^*(q,w)=r \wedge \delta_{|w|+1}(r,a)=q' \right]
\]
where $\{\delta_n(r,b) : n \in \mathbb{N}, r \in Q, b \in A\}$ is a family of independent random variables such that for all $(n,r,b)$, the distribution of $\delta_n(r,b)$ is $\delta(r,b)$. \\

Modulo this change of type of transition, probabilistic selectors are defined as before as well as $\select$. This makes $\select(\S,X)$ a random variable for every given~$X \in A^{\leq \omega}$. \\




In order to lift the deterministic Agafonov theorem for Bernoulli measures to the probabilistic case, we will need some preliminary lemmas about normality. \\

Given two alphabets $A$ and $B$, and given two sequences $X \in A^\omega$ and $Y \in B^\omega$, we denote by $X \otimes Y$ the sequence $Z$ over the alphabet $A \times B$ where $Z(n)=(X(n),Y(n))$. The product $v \otimes w$ for $v$ and $w$ two words of the same length over $A$ and $B$ respectively is defined in the same way. Likewise, if $\mu$ and $\nu$ are Bernoulli measures over $A$ and $B$ respectively, $\mu \otimes \nu$ is the Bernoulli measure $\xi$ over $A \times B$ where $\xi((a,b))=\mu(a)\nu(b)$ for all $(a,b) \in A \times B$. Finally, if $Z$ is a sequence over a product alphabet $A \times B$, we denote by $\pi_0$ and $\pi_1$ its first and second projection respectively (in other words, $\pi_0(X \otimes Y)=X$ and $\pi_1(X \otimes Y)=Y$ for all $X,Y$). 

\begin{lemma}\label{lem:random-join}
Let $A$ and $B$ be two alphabets and $\mu$, $\nu$ two Bernoulli measures over $A$ and $B$ respectively. If a sequence $X \in A^\omega$ is $\mu$-normal and a sequence $Y \in B^\omega$ is drawn at random according to $\nu$, then $\nu$-almost surely, $X \otimes Y$ is $\mu \otimes \nu$-normal. 
\end{lemma}

\begin{proof}
Let $w = u \otimes v$ be a non-empty word of $(A \times B)^*$.
Let $N$ be a large integer.
We split $X\otimes Y$ into blocks of length $N$ :
\[
X\otimes Y = (X_1\otimes Y_1) \cdot (X_2\otimes Y_2) \cdot  \ldots
\]
with $|X_i|=|Y_i|=N , i \geq 1$.
Introduce the random variables $(B_i)_{i\geq 1}$
which count the number of occurrences of $u \otimes v$
in each block:
\[
B_i = \nbocc(u \otimes v, X_i\otimes Y_i), {i\geq 1}\enspace.
\]
For every integer $n\in \NN$, 
within $(X\otimes Y)[0\ldots n]$
there are $\lfloor n / N \rfloor$ complete blocks.
Some occurrences of $u \otimes v$ in $(X\otimes Y)[0\ldots n]$ do occur inside a block
$X_i\otimes Y_i$ while some other do not because they overlap two contiguous blocks.
There can be at most $|w|$ such overlapping occurrences between two given blocks.
That observation leads to two ways to count occurrences of $u \otimes v$: the exact way $C_n$
and the $N$-block way $C'_{n,N}$, two random variables satisfying:
\begin{align}
\notag
&C_n =  \nbocc(u \otimes v, (X\otimes Y)[0\ldots n])\\
\notag
&C'_{n,N} =  \sum_{i\in 1 \ldots \lfloor n / N \rfloor } B_i \\
\label{lem:random-joinbounds}& C'_{n,N} \leq C_{n} \leq C'_{n+N,N} + |w|\cdot \lfloor n / N \rfloor
\enspace.
\end{align}
Let us focus on $C'_{n,N}$ first.
%The left part of $u \otimes v$, i.e. the word~$u$,
%appears with frequency $\mu(u)$ in $X$. 
%When $u$ appears in a block, i.e., $X[kn,(k+1)n-1]=u$ for some~$k$, the $\nu$-probability that the corresponding block $(X \otimes Y) [kn,(k+1)n-1]$ equals $w = u \otimes v$ is $\nu(v)$. And this event is independent on what happens on other blocks of length~$n$. By the law of large numbers, $\nu$-almost surely, an asymptotic fraction $\nu(v)$ of the blocks~$u$ present in~$X$ will yield the block $u \otimes v$ in $X \otimes Y$. Hence, the total frequency of $u \otimes v$ in $X \otimes Y$ is the product of the frequencies, namely $\mu(u)\nu(v)$. 
%Then
%\begin{equation}
%\label{lem:random-join1}
%\left(\sum_{i\in 1 \ldots \lfloor n / N \rfloor} B_i \right) \leq C_n  \leq \left(\sum_{i\in 1 \ldots \lceil n / N \rceil} B_i \right) + |w|\cdot \lfloor n / N \rfloor\enspace.
%\end{equation}
The variables $B_i$ can be grouped into $|A|^N$ different buckets,
with respect to the corresponding value of $X_i$.
For every $x\in A^{N}$, set $I_n(x)= \{ 1 \leq i \leq \lfloor n / N \rfloor \mid X_i = x\}$,
which is non-empty for $n$ large enough, since $X$ is normal.
The random variables $(X_i\otimes Y_i)$ are mutually independent,
and so are the random variables $(B_i)_{i\geq 1}$.
Moreover, for a fixed $x \in A^{N}$,
the random variables $(B_i)_{i\in I_\infty(x)}$ are distributed identically,
denote by~$\xi(x)$ the corresponding probability distribution on $0\ldots N-|w|+1$.
Then $\mathbb{E}\left[ \xi(x) \right]$ measures the expected number of
occurrences of $u \otimes v$ in a sequence where the left part is fixed, equal to $x$,
and the right part is independently generated according to $\nu$. Thus
\[
\mathbb{E}\left[ \xi_N(x) \right]  = \nbocc(u, x) \cdot \nu(v) \enspace.
\]
According to the Law of Large Numbers, 
$\frac{1}{| I_n(x)  |}\sum_{i\in I_n(x) } B_i\to_n  \nbocc(u, x) \cdot \nu(v)$.
Since $X$ is normal, every word $x \in A^N$ occurs with frequency $\mu(x)$ in $X_1,X_2,\ldots$
thus, almost-surely,
\begin{equation}
\label{lem:random-join222}
\lim_n \frac{1}{n} C'_{n,N} = \left(\sum_{x \in A^N} \mu(x)\cdot  \nbocc(u, x)\right)\cdot\nu(v)\enspace.
\end{equation}
%Denote $L_N = \lim_n \frac{1}{n}C'_n$.
Since $X$ is normal, the right part in~\eqref{lem:random-join222} converges to  $\mu(u)\cdot \nu(v)$
when $N$ grows large.
%\[
%\] $L_N \to_N \mu(u)\cdot \nu(v) = (\mu \otimes \nu)(u \otimes v)$.
%Note that there is no dependency of $(C_n)_{n\geq 1}$ upon $N$,
%hence 
%Using~\eqref{lem:random-joinbounds} leads to 
%\[
%\liminf_N L_N \leq \liminf_n \frac{1}{n}C_n \leq \limsup_n \frac{1}{n}C_n \leq \limsup_N L_N\enspace.
%\]
Using~\eqref{lem:random-joinbounds}, we get
the desired result: 
\[
\frac{1}{n}C_n \to_n \lim_N \left(\lim_n C'_{n,N}\right)  = (\mu \otimes \nu)(u \otimes v)\enspace.
\]
\end{proof}

\begin{lemma}\label{lem:projection}
If a sequence $Z$ is $\mu \otimes \nu$-normal over $A \times B$, then $\pi_0(Z)$ and $\pi_1(Z)$ are $\mu$- and $\nu$- normal respectively. 
\end{lemma}

\begin{proof}
Suppose $Z \in (A \times B)^\omega$ is $\mu \otimes \nu$-normal. We only need to show that $\pi_0(Z)$ is $\mu$-normal, the proof of the $\nu$-normality of $\pi_1(Z)$ being the same by symmetry. Let $w \in A^n$. We have
%\begin{eqnarray*}
\[
\freq^+(w,\pi_0(Z))  =  \sum_{w' \in B^n} \freq^+(w \otimes w',Z)
\]
which, by $\mu \otimes \nu$-normality of $Z$, implies 
\[
\freq^+(w,\pi_0(Z))  =  \sum_{w' \in B^n} \mu \otimes \nu(w \otimes w')=  \sum_{w' \in B^n} \mu(w)\nu(w') = \mu(w)
\]
The same holds for $\freq^-(w,\pi_0(Z))$, hence we have proven that $\freq(w,\pi_0(Z))=\mu(w)$, which is what we wanted. 

\end{proof}



We are now ready to prove Agafonov's theorem for PFAs. 

\begin{theorem}[Agafonov's theorem for PFA]\label{thm:agafonov-pfa}
Let $X \in A^\omega$, and $\mu$ a Bernoulli measure over~$A$. The following are equivalent.
\item[(i)] $X$ is $\mu$-normal. 
\item[(ii)] For any probabilistic selector $\S$ that selects a subsequence~$Y$ of~$X$, almost surely, either~$Y$ is finite, or $Y$ is $\mu$-normal. 
\item[(iii)] For any probabilistic selector $\S$ that selects a subsequence~$Y$ of~$X$, almost surely, either~$Y$ is finite, or $Y$ is $\mu$-normal. 
\end{theorem}

\begin{proof}
Since deterministic selectors (for which we already have Agafonov's theorem) are a subset of the probabilistic ones, all is left to prove is $(i) \Rightarrow (iii)$. 

Let $\S=(Q,A,q_I,\delta,S)$ be a probabilistic selector. Recall that each transition $\delta(q,a)$ (where $q \in Q$ and $a \in A$) is some probability distribution over $Q$. Consider the set $\mathcal{T}$ of all functions $Q \times A \rightarrow Q$. We can put a distribution $\tau$ over~$\mathcal{T}$ such that if $t$ is chosen according to $\tau$, for every $(q,a)$, the marginal distribution of $t(q,a)$ is $\delta(q,a)$. An easy way to do this is to take $\tau= \bigotimes_{(q,a)\in Q \times A} \delta(q,a)$.

This construction means that, for a fixed sequence~$X \in A^\omega$, an equivalent way to simulate the probabilistic run of $\S$ on~$X$ is, every time we are in a state~$q$ and read a letter~$a$, to pick $t$ at random according to $\tau$ and move to state $t(q,a)$. But $\mathcal{T}$ is a finite set and $\tau$ a Bernoulli measure over it (this Bernoulli measure might not be positive. If it is not, we simply remove from $\mathcal{T}$ all functions whose $\tau$-probability is~$0$). Reformulating slightly, yet another equivalent way to simulate the run of~$\S$ over~$X$ is to do the following:
\begin{itemize}
\item[1.] First choose $T \in \mathcal{T}^\omega$ at random with respect to the Bernoulli measure $\tau$. 
\item[2.] Then run on the sequence $X \otimes T$ the deterministic selector $\hat{\mathcal{S}}$ whose set of states is~$Q$ and transition $\hat{\delta}$ is defined by $\hat{\delta}(q,(a,t))=t(q,a)$. 
\end{itemize}

Now, the two following random variables have the same distribution: 
\begin{itemize}
\item The subsequence $Y$ of $X$ selected by~$\S$
\item The sequence $\pi_0(\hat{Y})$, where $\hat{Y}$ is the subsequence of $X \otimes T$ selected by $\hat{\S}$ when $T$ is chosen randomly according to $\tau$. 
\end{itemize}

Since $X$ is $\mu$-normal, by Lemma~\ref{lem:random-join}, $X \otimes T$ is $\mu \otimes \tau$-normal $\tau$-almost surely. Thus, by Agafonov's theorem for deterministic selectors and Bernoulli measures (Theorem~\ref{thm:agafonov-bernoulli}), almost surely, the subsequence $\hat{Y}$ selected by $\hat{\S}$ is $\mu \otimes \tau$-normal. Finally, by Lemma~\ref{lem:projection}, this implies that almost surely, the subsequence $\pi_0(\hat{Y})$ of $X$ is $\mu$-normal. This concludes the proof. 

\end{proof}

\section{The Schnorr-Stimm theorem for probabilistic gamblers}

Finally, we establish the Schnorr-Stimm theorem for probabilistic finite-state automata. The definition of probabilistic ($\mu$-)gambler is the same as the definition of deterministic ($\mu$-)gambler except that it is based on PFAs instead of DFAs. In this setting, the quantities $\capital(\G,w)$ become random variables. \\


\begin{theorem}[Schnorr-Stimm theorem for PFAs]
For $X \in A^\omega$ and $\mu$ a Bernoulli measure, the following are equivalent.
\item[(i)] $X$ is $\mu$-normal. 
\item[(ii)] Any probabilistic $\mu$-gambler $\G$ betting on~$X$ loses almost surely.   
\end{theorem}

\begin{proof}
We already have $(ii) \Rightarrow (i)$ by Theorem~\ref{thm:schnorr-stimm-bernoulli} (deterministic gamblers being a subset of probabilistic ones). 

For $(i) \Rightarrow (ii)$, the idea is similar to the proof of Theorem~\ref{thm:agafonov-pfa}. To simulate the run of a probabilistic $\mu$-gambler $\G=(Q,A,q_I,\delta,\gamma)$ on a sequence~$X$, one can equivalently run on the sequence $X \otimes T$, where $T \in \mathcal{T}^\omega$ is $\tau$-random sequence ($\mathcal{T}$ and $\tau$ being defined in the same way as in Theorem~\ref{thm:agafonov-pfa}) the deterministic gambler $\hat{\G}=(Q,A,q_I,\hat{\delta},\hat{\gamma})$ where again $\hat{\delta}(q,(a,t))=t(q,a)$ for all~$(q,a,t) \in Q \times A \times \mathcal{T}$ and $\hat{\gamma}(q,(a,t)) = \gamma(q,a)$ (indeed the bet placed by a gambler at a given stage does not take into account which state will be reached next). Note that the fairness condition is respected since for every~$q$,
\[
\sum_{(a,t) \in A \times \mathcal{T}} (\mu \otimes \tau)(a,t) \cdot \hat{\gamma}(q,(a,t)) = \sum_{a \in A, t \in \mathcal{T}} \mu(a) \tau(t) \gamma(q,a) = \sum_{t \in \mathcal{T}} \tau(t) \sum_{a \in A} \mu(a) \gamma(q,a) = \sum_{t \in \mathcal{T}} \tau(t) = 1
\]
(the second-to-last equality holds by fairness condition on~$\gamma$ and the last one because $\tau$ is a distribution).

In this way, the two following random variables will have the same distribution:
\begin{itemize}
\item $\capital(\G,X[0..n])$
\item $\capital(\hat{\G},(X \otimes T)[0..n])$ where $T \in \mathcal{T}^\omega$ is chosen at random according to $\tau$. 
\end{itemize} 

Since $X$ is $\mu$-normal, by Lemma~\ref{lem:random-join} again, $X \otimes T$ is $\mu \otimes \tau$-normal $\tau$-almost surely. Thus, by Theorem~\ref{thm:schnorr-stimm-bernoulli}, for $\tau$-almost all~$T$, $\capital(\hat{\G},(X \otimes T)[0..n])$ is bounded by a constant~$C$ independent on~$n$. By the equivalence of the two random variables above, this means that almost surely, there exists a constant~$C$ such that $\capital(\G,X[0..n])<C$ for all~$n$. In other words, almost surely, $\G$ loses on the sequence~$X$. 

\end{proof}


We remark that the dichotomy theorem for Bernoulli measures yields the following dichotomy for probabilistic $\mu$-gamblers.

\begin{theorem}[Schnorr-Stimm dichotomy theorem for PFAs]
\label{thm:probabilisticdichotomy}
Let $X$ be an infinite sequence in $A^\omega$ and $\mu$ a Bernoulli measure. \\
(i) If $X$ is $\mu$-normal and $\G$ is a $\mu$-gambler, then almost surely the capital of $\G$ throughout the game either is ultimately constant or decreases at an exponential rate.\\
(ii) If $X$ is not $\mu$-normal, then there exists a $\mu$-gambler~$\G$ which wins against~$X$ at an `infinitely often' exponential rate almost surely.
\end{theorem}


\section{Alternate proof of Agafonov's theorem for PFA}
\label{sec:agafonovalternateproof}

For the curious reader,
we provide another proof of Theorem~\ref{thm:agafonov-pfa},
along the lines of Carton's techniques~\cite{Carton2020},
with a different presentation based on the notion of \emph{balanced input words}.

\subsection{The strongly connected case}

In the whole section we assume that 
\[
\text{$\A$ is strongly connected}
\]
i.e. for all states $q,r \in Q$,
there is a computation in $\A$ from $q$ to $r$.
We also assume that $\A$ has at least one selecting state i.e. $S \neq \emptyset$.
We prove
\begin{lemma}[The strongly connected probabilistic case]
\label{lem:scpc}
Let $\A$ be a PFA over an alphabet $A$, 
$\mu$ a Bernoulli measure over $A$ 
and assume that the Markov chain $\A^\mu$ is irreducible. 
Let $X$ be a $\mu$-normal sequence.
The word output by $\A$ is almost-surely finite or $\mu$-normal.
\end{lemma}

The proof of Lemma~\ref{lem:scpc}, given at the end of the section,
relies on two ingredients:
\begin{itemize}
\item
\emph{balanced input words} (cf. Definition~\ref{def:balancedness}),
do appear frequently 
when the input word of the PFA is generated with $\mu$
(cf. Lemma~\ref{lem});
\item
and a generic result about Markov processes
(cf. Lemma~\ref{lem:tool}).
\end{itemize}

The notion of balanced input words
isolates a frequent behaviour 
of the Markov chain $\M$ with state space $Q \cup Q\times A$,
obtained by synchronizing the $\mu$-generator of letters and the probabilistic automaton
reading those letters:
from a state $q\in Q$, a new letter $a \in A$ is selected according to $\mu$
 and the new state is $(q,a)$. Then a transition to $Q$ occurs with probabilities $p_\A$.
This homogeneous Markov process is fully described by the random variables $Q_0,A_1,Q_1,A_2,\ldots$.
The computation at step $k$ is
\[
\pi_k = Q_0,A_1,Q_1,A_2,\ldots  ,Q_k\enspace.
\]
For $1\leq i \leq j$, the output of the chain between dates $i$  and $j$, both included, is the random variable $\outp_{i\ldots j}$ with values in $A^*$ defined by:
\begin{multline*}
\outp_{i\ldots j} =  \text{ the subword of $A_i\ldots A_{j}$} \\
\text{obtained by selecting letters at all dates $k$ such that $i \leq k \leq j$ and $Q_{k-1}\in S$}\enspace.
\end{multline*}

When running $\M$,
the output in $A^\omega$
is generated according to $\mu$, at an average output rate $0 < \lambda < 1$.
The ergodic theorem describes what happens almost-surely in $\M$.
Since $\A$ is strongly connected,
every state of $\M$ belongs to the same recurrence class,
and is visited at a fixed positive frequency.
Every time one of the selecting states is visited, say $Q_k \in S$ at step $k$,
then the letter $A_{k+1}$ is output,
it is picked up at random,
independently of the rest of the computation $\pi_k$,
including $Q_k$.
If one fixes some output length $m\in \NN$,
all words in $A^m$ are expected to appear
in the output at the same frequency $\frac{1}{|A|^m}$.
The output rate $\lambda$ is
equal to the frequency of visits to $S$: 
every $N$ input letters, the Markov chain produces
$\lambda N$ output letters on average.
In the long run, the chain almost-surely generates an infinite output 
\[
\outp_{1,\infty} = \lim_N \outp_{1\ldots N} \in A^\omega\enspace.
\]
according to distribution~$\mu$.

When the automaton $\A$ runs on a fixed finite input word $z=a_1,a_2,\ldots,a_N$
then the output is in general not generated according to the distribution~$\mu$: due to the deterministic constraint on the input word,
the corresponding Markov chain is not homogeneous anymore since the letters are picked deterministically, on a time-dependent basis.
However, we show that if the input word is $\mu$-normal,
the computation of $\A$ still resembles 
the computation of $\M$,
at least when it comes to counting the occurrences
of a (small) word $w\in A^m$  in the output.

We denote,
for every non-empty word $w\in A^+$,
and non-empty integer interval $1  \leq i \leq j$:
\begin{multline*}
\nbocc_{i\ldots j}(w) =  \text{ the number of indices $k\in i\ldots j$ }\\
\text{such that 
$Q_{k-1}\in S$ and
$w$ is a prefix of $\outp_{k\ldots j}$}\enspace.
\end{multline*}
As already discussed,
in $\M$ the frequency of appearance of some word $w$ in the output is typically
equal to $\mu(w)$. This is also approximately the case
 in $\A$, provided the input is \emph{balanced}.
 
\begin{definition}[$(m,\epsilon)$-balanced input words]
\label{def:balancedness}
Let $m\in \NN$ and $\epsilon >0$.
Denote $\lambda$ the frequency of visits to $F$ in $\M$.
For every word $w\in A^+$, and $1 \leq i < j$
we define the event 
\[
\good_{i\ldots j}(w,\epsilon)=
\left\{
\left | \frac{\nbocc_{i\ldots j}(w)}{|\outp_{i\ldots j}|} -  \mu(w)\right | \leq \epsilon
\right\}
\land
\left\{
\left |\frac{|\outp_{i\ldots j}|}{j -i} - \lambda \right|
\leq  \epsilon 
\right\}
\enspace,
\]
and for every $m\in\NN$,
\[
\good_{i\ldots j}(m,\epsilon)=
\bigwedge_{w \in A^m}\good_{i\ldots j}(w,\epsilon) \enspace.
\]
 A finite word $a_1 \ldots a_N$ is said to be $(m,\epsilon)$-balanced if
 for every initial state $q_0\in Q$, 
 \begin{equation}\label{defbalanced}
 \PP_{\M}\left(  \good_{1\ldots N}(m,\epsilon)  \mid A_1=a_1,\ldots ,A_N=a_N, Q_0 = q_0\right) \geq 1 - \epsilon\enspace.
 \end{equation}
\end{definition}

%\begin{remark*}\label{rem:balanced}
In the specific case where $\A$ is deterministic,
then there is a single possible computation on $a_1\ldots , a_N$,
hence the probability in~\eqref{defbalanced} is either $0$
or $1$. In that case, provided $\epsilon <1$ the 
property $\good_{1\ldots N}(m,\epsilon)$
is guaranteed to hold for sure
whenever the word $a_1\ldots , a_N$ is 
balanced.
%\end{remark*}

The following lemma shows that among input words that are long enough,
the proportion of $(m,\epsilon)$-balanced words is arbitrarily close to $1$.

\begin{lemma}\label{lem}
Let $m\in \NN$ and $\epsilon >0$.
For every state $q_0\in Q$,
\[
\PP_{\M,q_0}\left(A_1\ldots A_N \text{ is  $(m,\epsilon)$-balanced }\right)\to_N 1\enspace.
\]
\end{lemma}
\begin{proof}
In this proof, $\PP_{\M,q_0}$ is simply denoted as $\PP$.
We make use of
\[
\good(m,\epsilon, \infty)= \bigcup_{M\in\NN} \bigcap_{M \leq N}  \good(m,\epsilon, N)\enspace.
\]
By standard ergodic properties of Markov chains,
$\PP\left(\good(m,\epsilon, \infty)\right)=1$.
The event $\good(m,\epsilon,\infty)$ is a tail event,
thus according to Lévy's law,
%almost-surely
$
 \PP\left(
 \PP\left(  \good(m,\epsilon, \infty) \mid A_1,\ldots , A_N\right)  \to_N 1
 \right) = 1.
$
%thus by definition of $ \good(m,0)$,
%\[
% \PP_{q_0}\left(
% \PP_{q_0}\left(\forall w \in A^m, 
%% \frac{1}{|A|^m} - \epsilon \leq \freq_{\leq N}(w)  \leq \frac{1}{|A|^m} + \epsilon 
%\freq_{\leq N}(w)  \geq \frac{1}{|A|^m} - \epsilon 
%  \mid A_1,\ldots , A_N\right)  \to_N 1
% \right)=1
% \enspace.
%\]
Choose any $\epsilon' $ such that $0 < \epsilon' < \epsilon$.
Then, for $N$ large enough,
\[
 \PP\left(
 \PP\left(  \good(m,\epsilon, N) \mid A_1,\ldots , A_N\right)  \geq 1 - \epsilon
 \right) 
 \geq 
 1-\epsilon'
 .
\]
%\[
% \PP_{q_0}\left(  \forall w \in A^m, 
%%  \frac{1}{|A|^m} - \epsilon \leq  \freq_{\leq N}(w)  \leq \frac{1}{|A|^m} + \epsilon 
%  \freq_{\leq N}(w)  \geq \frac{1}{|A|^m} - \epsilon 
%\mid A_1,\ldots , A_N\right)  \geq 1-\epsilon'\enspace.
%\]
Thus
$
\PP_{q_0}\left(A_1\ldots A_N \text{ is  $(m,\epsilon)$-balanced }\right)\geq  1-\epsilon'.
$
This holds for $N$ large enough whatever $\epsilon'$, hence the conclusion.
\end{proof}

The second ingredient in the proof of Lemma~\ref{lem:scpc} 
is a generic property of Markov processes:
\begin{lemma}
\label{lem:tool}
Let $X_1,X_2,\ldots$ be a Markov process,
$1 = i_1< i_2<\ldots $ a sequence of indices and
$B_1,B_2\ldots$ a sequence of events
and $\epsilon>0$
such that
for every $\ell \geq 1$,
\begin{align}
&B_\ell \text{ is } (X_{i_\ell},\ldots, X_{i_{\ell+1}-1})-\text{measurable}\\
& \forall x \in X, \PP(B_\ell \mid X_{i_\ell} = x)\geq (1- \epsilon)\enspace.
\end{align}
For every $k\in \NN$, denote
$
N_k = \sum_{\ell \in 1 \ldots k} 1_{B_\ell}\enspace.
$
Then
\[
\PP\left(\frac{N_k}{k} > \left(1-2\epsilon\right) \right) \to_k 1\enspace.
\]
\end{lemma}
\begin{proof}
For every $k\in \NN$ set
\[
G_k = N_k - (1-\epsilon) \cdot k\enspace.
\]
and let
$\mathcal{F}_k$ the $\sigma$-algebra generated by $X_{1},\ldots, X_{i_{k+1}-1}$.
Then $(G_k)_{k\in \NN}$ is a submartingale for the filtration $(\mathcal{F}_k)_{k\in \NN}$:
\begin{align*}
\EE[G_{k+1} \mid \mathcal{F}_k]
&= \EE[N_{k+1} \mid \mathcal{F}_k] - (1-\epsilon) \cdot (k+1)\\
&\geq (1-\epsilon)(N_k +1) + \epsilon N_k - (1-\epsilon) \cdot (k+1)\\
&= M_{k} 
\end{align*}
Notice that $\forall k\in \NN, |G_{k+1} - G_k| \leq \max\{\epsilon, (1 - \epsilon)\}$
and by hypothesis $\epsilon \leq 1$ hence $|G_{k+1} - G_k|\leq 1$.
We make use of:
\begin{theorem}[Azuma's Inequality for Submartingales]\label{thm:azuma}
Let $(X_n)_{n \geq 0}$ be a submartingale with respect to a filtration $(\mathcal{F}_n)_{n \geq 0}$, i.e., 
$
\mathbb{E}[X_{i+1} \mid \mathcal{F}_i] \geq X_i \quad \text{for all } i \geq 0.
$
Assume there exist constants $c_1, c_2, \ldots, c_n$ such that 
$
|X_i - X_{i-1}| \leq c_i  \text{almost-surely for all } i \geq 1.
$
Then, for any $t > 0$, the following bound holds:
\[
\mathbb{P}\left(X_n - X_0 \leq -t\right) \leq \exp\left(-\frac{t^2}{2 \sum_{i=1}^n c_i^2}\right).
\]
\end{theorem}
Apply Azuma's inequality with $c_k=(1 - \epsilon))$ and $t=-k\epsilon$ and get
\[
%\forall k \in \NN,
\mathbb{P}\left(N_k \leq (1 -2\epsilon)\cdot k \right)
=
\mathbb{P}\left(G_k \leq -k\epsilon \right) \leq e^{-\frac{k^2\epsilon^2}{2 k}}
=
e^{-k\frac{\epsilon^2}{2 }}
\to_k 0
\enspace.
\]
\end{proof}


\begin{proof}[Proof of Lemma~\ref{lem:scpc}]
Take some normal input word $z$.
Fix a word length $m>0$ and a precision $\epsilon>0$ and compute the corresponding $N$ from Lemma~\ref{lem}.
Cut $z$ in chunks of length $N$, so that $z=u_1u_2\cdots$ where each $|u_i|=N$.

For every $k\geq 1$, we consider the concatenation  of the first $k$ chunks:
\[
z_k = u_1 u_2 \cdots u_k\enspace,
\]
and we shall approximate the expected number of occurrences of $w\in A^m$ in 
the corresponding output $\outp_{1\ldots N\cdot k}$ on $z_k$, when $k$ is large enough, as well as the length of this output.

Let us enumerate all indices where $u_\ell$ is normal as
\[
\{\ell \geq 0 \mid u_\ell \text{ is normal}\} = \{i_0 < i_1 < i_2 \ldots\}\enspace.
\]
By choice of $N$,
among all possible words in $A^N$, at most an $\epsilon$-fraction are not $(m,\epsilon)$-balanced.
Since $z$ is normal,  in the sequence $u_1,u_2,\ldots$ every possible word in $A^N$ occurs with the same limit frequency
$\frac{1}{|A|^N}$.
Thus $\limsup_k \frac{i_k - k}{i_k } \leq \epsilon$ hence 
\begin{equation}\label{eq:ik}
\exists k_0, \forall k \geq k_0,  ~ (1 -2\epsilon) i_k \leq k \leq i_k 
\end{equation}

For every $\ell\geq 0$, denote $I_\ell$ the interval of integers of length $N$ defined by 
\[
I_\ell=1+ (N\cdot i_\ell) \ldots (N+1)\cdot i_\ell
\]
% and the corresponding output
%\[
%O_k =\outp_{I_k}\enspace.
%\]
%and the event of balancedness,
%\[
%B_k = 1_{\good_{I_k}(m,\epsilon)}\enspace.
%\]
and let us evaluate balancedness at step $k$ as,
\[
M_k = \mid \{ \ell \in 0\ldots k \mid  \good_{I_\ell}(m,\epsilon) \}\mid \enspace.
\]
By definition of balancedness,
for every index $1 \leq \ell \leq k$,
\[
\PP(\good_{I_\ell}(m,\epsilon)\mid Q_0,A_1,Q_1,\ldots, Q_{i_\ell\cdot N} ) \geq 1-\epsilon\enspace.
\]
According to Lemma~\ref{lem:tool}:
\[
\PP\left( \frac{M_k}{k} \geq (1-2\epsilon) \right) \to_k 1 \enspace,
\]
which with~\eqref{eq:ik} implies:
\[
\PP\left(
\frac{| \{ \ell \in 0\ldots k \mid  \good_{\ell * N\ldots (\ell+1) * N-1}(m,\epsilon) \}|}{k}
\geq (1-2\epsilon)^2 
\right)
\to_k 1
\enspace.
\]




By definition of balancedness, for $k$ large enough, for every $w\in A^m$,
\begin{align}
&\label{lowerb}
\PP_{\A,z}\left(
\nbocc_{1\ldots k\cdot N}(w)
\geq
(1-2\epsilon)^2
\left(\mu(w)- \epsilon\right)
%\approx_{\epsilon \to 0} \frac{kN}{|A|^{|w|}}
\cdot \lambda \cdot k \cdot N
\right)
\geq  1 - \epsilon
\\
\label{upperb}
&\PP_{\A,z}\left(
|\outp_{1\ldots k\cdot N}|
\geq
(1-2\epsilon)^2\cdot (\lambda -\epsilon) \cdot k\cdot N
\right )\geq  1 - \epsilon
\enspace.
\end{align}
And on every non balanced words the output size is at most $N$, hence
\begin{align}
&\PP_{\A,z}\left(
|\outp_{1\ldots k\cdot N}|
\leq 
(1-2\epsilon)^2\cdot (\lambda -\epsilon) \cdot k\cdot N +
(1 - (1-2\epsilon)^2) \cdot k\cdot N 
\right )
%\\
%&\approx_{\epsilon\to 0}\lambda\cdot k\cdot N
\label{upperb2}
\end{align}


Combining~\eqref{lowerb}~--~\eqref{upperb2}, for $\epsilon$ arbitrarily small
(hence $k$ arbitrarily large)
we obtain:
\begin{align}
&\PP_{\A,z}\left(
\frac{|\outp_{1\ldots k\cdot N}|}{k\cdot N}
\to_k
\lambda
\right)
=1\\
&\PP_{\A,z}
\left(
\liminf_k 
\frac{\nbocc_{1\ldots k\cdot N}(w)}{|\outp_{1\ldots k\cdot N}|}
\geq
\mu(w)
\right)
=1
\enspace.
\end{align}
Those two limits are taken over $k\cdot N\in \{N,2N,3N,\ldots\}$
but 
since $\frac{N}{k}$ is negligible for large $k$, the stronger limits over $m\in \NN$ do hold as well:
\begin{align}
\label{lowerb3}
&\PP_{\A,z}
\left(
\liminf_n
\frac{\nbocc_{1\ldots n}(w)}{|\outp_{1\ldots n}|}
\geq
\mu(w)
\right)
=1
%\\
%&\PP_{\A,z}\left(
%\frac{|\outp_{1\ldots n}|}{n}
%\to_k
%\lambda
%\right)
%=1
\enspace.
\end{align}

Now we are  done,
since $\PP_{\A,z}$-almost-surely,
\begin{align*}
%\sum_{w \in A^k} \limsup_n \frac{\nbocc_{1\ldots n}(w)}{|\outp_{1\ldots n}|}
%&=
\limsup_n   \frac{\sum_{w \in A^k} \nbocc_{1\ldots n}(w)}{|\outp_{1\ldots n}|}
&\leq
1& 
\text{(by def. of $\nbocc$ and $\outp$)}\\
&=
\sum_{w \in A^k}
\mu(w)&
\\
&\leq
\sum_{w \in A^k}
\liminf_n
\frac{\nbocc_{1\ldots n}(w)}{|\outp_{1\ldots n}|}
&\text{(by~\eqref{lowerb3})}\\
&\leq
\liminf_n
\sum_{w \in A^k}
\frac{\nbocc_{1\ldots n}(w)}{|\outp_{1\ldots n}|}
&
\end{align*}
hence those three inequalities are $\PP_{\A,z}$-almost-surely equalities.
As a consequence
\begin{align}
&\PP_{\A,z}
\left(
\frac{\nbocc_{1\ldots n}(w)}{|\outp_{1\ldots n}|}
\to_n
\mu(w)
\right)
=1\enspace.
\end{align}
This holds for every $w\in A^+$,
hence the output is a.s. normal.
\end{proof}


\subsection{The general case}

\begin{proof}[Alternate proof of Theorem~\ref{thm:agafonov-pfa}]
We extend Lemma~\ref{lem:scpc}
to the general case
where the automaton has a non empty set of transient states $T$.
By definition from every state in $T$ there is a word and a computation
on this word which leaves $T$ and enter a BSCC,
from which there is no way back to $T$.
We show
\[
\PP_{\A,z}(\exists n , Q_n \not \in T) = 1\enspace.
\]
From every state $q\in T$,
there is at least one word of length $\leq |T|$ which 
exits $T$ 
with nonzero probability,
bounded from below by ${p_m}^{|T|}$ where $p_m$
is the smallest non-zero probability appearing in $p$.
We can concatenate $|T|$ such words in order to obtain a single word
$u$ which guarantees probability $\geq {p_m}^{|T|^2}$ to leave $T$,
 \emph{whatever the initial state}. 
Since the input word $z$ is normal, $u$ appears infinitely often in $z$,
thus $T$ is eventually left almost-surely.

Once $T$ is left, the suffix of the computation
fits the hypotheses of Lemma~\ref{lem:scpc}.
Normality is a tail property,
hence any suffix of the input word is normal as well,
hence almost-surely a suffix of the output word is normal as well, according to Lemma~\ref{lem:scpc},
hence the output is normal, since normality is a tail property.
\end{proof}


\section{$\mu$-block normality and $\mu$-normality}
\label{sec:blocknormality}

The proof of Lemma \ref{lem:ergodiconnormal} requires the block-wise characterization of $\mu$-normality. Let $\bfreq(u,w)$ denote the frequency of block-wise occurrences of the word $u$ in $w$. For $w \in A^*$, let $\bfreq^-(w,X)$, $\bfreq^+(w,X)$ and $\bfreq(w,X)$ denote the lower, upper and limit block frequency of $w$ in $X$ defined similarly as $\freq^-(w,X)$, $\freq^+(w,X)$ and $\freq(w,X)$. A sequence $X$ is \emph{$\mu$-block normal} if for all words~$w \in A^*$, $\bfreq(w,X)=\mu(w)$. The following was shown in Simonsen and Seiller.

\begin{lemma}[\cite{SeillerS2020}]
	If a sequence $X \in A^\omega$ is $\mu$-normal then $X$ is $\mu$-block normal.
\end{lemma}

We note that the converse implication  also holds.

\begin{lemma}
\label{lem:blocknormalimpliesnormal}
	If a sequence $X \in A^\omega$ is $\mu$-block normal then $X$ is $\mu$-normal.
\end{lemma}

 The above lemmas completes the proof of the following equivalence theorem between $\mu$-normality and $\mu$-block normality.

\begin{theorem}
	A sequence $X \in A^\omega$ is $\mu$-normal if and only if $X$ is $\mu$-block normal.
\end{theorem}


The proof of Lemma \ref{lem:blocknormalimpliesnormal} uses a counting trick from the proof of Theorem 3.1 from \cite{NandakumarPVV2021} which in turn is based on the proof of the main theorem in \cite{Maxfield1952}.

\begin{proof}[Proof of Lemma \ref{lem:blocknormalimpliesnormal}]
As in the proof of Theorem 3.1 from \cite{NandakumarPVV2021}, for any finite length string
	$w=a_1a_2a_3\dots a_k\in A^k$ and large enough $n$, 
	\begin{align}
		\label{eq:pillaistheorem_one}
		\freq(w,X[0..n])=
		f_1(n) + f_2(n) +\dots+
		f_{(1+\left \lfloor\log_2  \frac{n}{k} \right \rfloor)}(n) + {\frac{(k-1). O(\log n) }{n-k+1}}
	\end{align}
	where $f_p(n)$ are defined as follows:
	\begin{align*}
		f_{p}(n)= \begin{cases}
		\frac{	|\{i~ \mid~ X[ki,k(i+1)-1]=w
				~,~ 0 ~\leq ~i ~\leq ~\lfloor n/k \rfloor \}|}{n-k+1}, & \text{if } ~p=1\\
			\sum_{j=1}^{k-1}\frac{  |\{i \mid X[2^{p-1}ki,2^{p-1}k(i+1)-1] \in S_j
				, 0 \leq i \leq n/2^{p-1}k \}|}{n-k+1}, &\text{if } 1<p \leq
			(1+\left \lfloor\log_2(n/k)\right \rfloor)\\ 
			0, \text{ otherwise.}
		\end{cases}
	\end{align*}
	In the above definition, $S_{j}$ is the set of strings of the form,
	$u\ a_1a_2 \dots a_k\ v$ where $u$ is some string of length $2^{p-2}k-j$ and $v$ is some string of length $2^{p-2}k-k+j$. 
	Since $X$ is $\mu$-block normal, 
	\begin{align*}
		\lim_{n\to\infty} f_1(n) = \lim_{n\to\infty}\frac{	|\{i~ \mid~ X[ki,k(i+1)-1]=w
				~,~ 0 ~\leq ~i ~\leq ~\lfloor n/k \rfloor \}|}{n-k+1} =
	\frac{\mu(w)}{k}.
	\end{align*}
	Now, when $p \leq 1+ \left\lfloor\log_2(\frac{n}{k}) \right\rfloor$,
	\begin{align*}
		\lim_{n\to\infty} f_p(n)
		=&  \sum_{j =1}^{k-1}\lim_{n\to\infty}
		\frac{  |\{i \mid X[2^{p-1}ki,2^{p-1}k(i+1)-1] \in S_j
				, 0 \leq i \leq n/2^{p-1}k \}|}{n-k+1}= \frac{\mu(w)}{2^{p-1}k}.(k-1)
	\end{align*}
	Since
	$\langle \sum_{i=1}^{m} f_i(n) \rangle_{m \in \mathbb{N}}$ is uniformly
	convergent, we have
\begin{align*}
		\freq(w,X[0..n]) = \lim_{n\to\infty} \sum_{i=1}^{\infty} f_{i}(n)=  \sum_{i=1}^{\infty} \lim_{n\to\infty} f_{i}(n).
	\end{align*}
	Therefore from (\ref{eq:pillaistheorem_one}),
	\begin{align*}
		\lim_{n\to\infty} \freq(w,X[0..n])
		&= \frac{\mu(w)}{k} + (k-1)\mu(w) \Big[ \sum_{i=1}^\infty \frac{1}{2^i k} \Big] = \mu(w).
	\end{align*}
	Hence, $X$ is $\mu$-normal.
\end{proof}



\section{Conclusion}
The main contributions of this paper are a generalization
of the Agafonov theorem for PFA with arbitrary transition probabilities which settles the open question posed by L\'echine et al. \cite{LechineSS2024},
and an extension of Schnorr-Stimm theorem to probabilistic gamblers.  

While we proved the probabilistic Agafonov theorem (Theorem~\ref{thm:agafonov-pfa} ) by reduction to the deterministic setting, it is also possible to follow with a more direct approach (i.e., without appealing to the Seiller-Simonsen result), similar to the one followed by Carton to generalize Agafonov's theorem for DFA~\cite{Carton2020}. This however makes the argument somewhat more complicated. For the curious reader, we provide this alternate proof in section \ref{sec:agafonovalternateproof}.

An interesting direction for future research is to explore whether the `uselessness of randomness' also holds for pushdown automata. These are a more powerful model of computation and indeed some normal sequences can be predicted by pushdown automata (some in a rather dramatic way, as proven by Carton and Perifel~\cite{CartonP2024}\footnote{The result proven by Carton and Perifel considers a slightly different paradigm, namely compression (which we did not discuss in this paper) instead of prediction.}). We can for example ask: If some probabilistic pushdown selector selects a biased subsequence from a sequence~$X$, does there necessarily exist a deterministic pushdown selector which also selects a biased subsequence?. Similarly, if some probabilistic pushdown gambler wins against a sequence~$X$, does there necessarily exist a deterministic pushdown gambler which wins against that same sequence~$X$?

%
%An interesting direction for future research is to explore whether probabilistic pushdown transducers offer greater compressive power than their deterministic counterparts. A recent work by Carton and Perifel~\cite{CartonP2024} shows the existence of normal sequences that compressible by deterministic pushdown transducers. This raises the question: \emph{Does there exist sequences that are compressible by probabilistic pushdown transducers but are incompressible using deterministic pushdown transducers?}
%
A related question concerns the speed of success in the gambling model. In the case of finite-state automata, Schnorr and Stimm proved that either a sequence~$X$ cannot be predicted or some gambler wins on it at an exponential rate. This dichotomy no longer holds for pushdown automata, but one may ask the following question: \emph{If some probabilistic pushdown gambler wins at an exponential rate against a sequence~$X$, does there necessarily exist a pushdown gambler which wins against that sequence~$X$ at an exponential rate?}
 
\bibliographystyle{plain}
\bibliography{main}







\end{document}
