@inproceedings{AKT,
  title={Context-aware attentive knowledge tracing},
  author={Ghosh, Aritra and Heffernan, Neil and Lan, Andrew S},
  booktitle={Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={2330--2339},
  year={2020}
}

@inproceedings{AdaptKT,
  title={AdaptKT: A domain adaptable method for knowledge tracing},
  author={Cheng, Song and Liu, Qi and Chen, Enhong and Zhang, Kai and Huang, Zhenya and Yin, Yu and Huang, Xiaoqing and Su, Yu},
  booktitle={Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
  pages={123--131},
  year={2022}
}

@inproceedings{BEKT,
  title={BEKT: deep knowledge tracing with bidirectional encoder representations from transformers},
  author={Tiana, Zejie and Zhengc, Guangcong and Flanaganb, Brendan and Mic, Jiazhi and Ogatab, Hiroaki},
  booktitle={Proceedings of the 29th International Conference on Computers in Education},
  volume={2},
  number={5},
  pages={6--2},
  year={2021}
}

@article{BKT,  
 title={Knowledge tracing: Modeling the acquisition of procedural knowledge}, 
 url={http://dx.doi.org/10.1007/bf01099821}, 
 DOI={10.1007/bf01099821}, 
 journal={User Modelling and User-Adapted Interaction}, 
 author={Corbett, Albert T. and Anderson, John R.}, 
 year={1995}, 
 month={Jan}, 
 pages={253–278}, 
 language={en-US} 
}

@article{Bi-CLKT,
  title={Bi-CLKT: Bi-graph contrastive learning based knowledge tracing},
  author={Song, Xiangyu and Li, Jianxin and Lei, Qi and Zhao, Wei and Chen, Yunliang and Mian, Ajmal},
  journal={Knowledge-Based Systems},
  volume={241},
  pages={108274},
  year={2022},
  publisher={Elsevier}
}

@InProceedings{BiDKT,
author="Tan, Weicong
and Jin, Yuan
and Liu, Ming
and Zhang, He",
editor="Bao, Wei
and Yuan, Xingliang
and Gao, Longxiang
and Luan, Tom H.
and Choi, David Bong Jun",
title="BiDKT: Deep Knowledge Tracing with BERT",
booktitle="Ad Hoc Networks and Tools for IT",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="260--278",
abstract="Deep knowledge Tracing is a family of deep learning models that aim to predict students' future correctness of responses for different subjects (to indicate whether they have mastered the subjects) based on their previous histories of interactions with the subjects. Early deep knowledge tracing models mostly rely on recurrent neural networks (RNNs) that can only learn from a uni-directional context from the response sequences during the model training. An alternative for learning from the context in both directions from those sequences is to use the bidirectional deep learning models. The most recent significant advance in this regard is BERT, a transformer-style bidirectional model, which has outperformed numerous RNN models on several NLP tasks. Therefore, we apply and adapt the BERT model to the deep knowledge tracing task, for which we propose the model BiDKT. It is trained under a masked correctness recovery task where the model predicts the correctness of a small percentage of randomly masked responses based on their bidirectional context in the sequences. We conducted experiments on several real-world knowledge tracing datasets and show that BiDKT can outperform some of the state-of-the-art approaches on predicting the correctness of future student responses for some of the datasets. We have also discussed the possible reasons why BiDKT has underperformed in certain scenarios. Finally, we study the impacts of several key components of BiDKT on its performance.",
isbn="978-3-030-98005-4"
}

@inproceedings{DCL4KT-A,
  author       = {Unggi Lee and
                  Sungjun Yoon and
                  Joon Seo Yun and
                  Kyoungsoo Park and
                  Younghoon Jung and
                  Damji Stratton and
                  Hyeoncheol Kim},
  editor       = {Nicoletta Calzolari and
                  Min{-}Yen Kan and
                  V{\'{e}}ronique Hoste and
                  Alessandro Lenci and
                  Sakriani Sakti and
                  Nianwen Xue},
  title        = {Difficulty-Focused Contrastive Learning for Knowledge Tracing with
                  a Large Language Model-Based Difficulty Prediction},
  booktitle    = {Proceedings of the 2024 Joint International Conference on Computational
                  Linguistics, Language Resources and Evaluation, {LREC/COLING} 2024,
                  20-25 May, 2024, Torino, Italy},
  pages        = {4891--4900},
  publisher    = {{ELRA} and {ICCL}},
  year         = {2024},
  url          = {https://aclanthology.org/2024.lrec-main.438},
  timestamp    = {Thu, 23 May 2024 16:47:05 +0200},
  biburl       = {https://dblp.org/rec/conf/coling/LeeYYPJSK24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DGEKT,
author = {Cui, Chaoran and Yao, Yumo and Zhang, Chunyun and Ma, Hebo and Ma, Yuling and Ren, Zhaochun and Zhang, Chen and Ko, James},
title = {DGEKT: A Dual Graph Ensemble Learning Method for Knowledge Tracing},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3638350},
doi = {10.1145/3638350},
abstract = {Knowledge tracing aims to trace students’ evolving knowledge states by predicting their future performance on concept-related exercises. Recently, some graph-based models have been developed to incorporate the relationships between exercises to improve knowledge tracing, but only a single type of relationship information is generally explored. In this article, we present a novel Dual Graph Ensemble learning method for Knowledge Tracing (DGEKT), which establishes a dual graph structure of students’ learning interactions to capture the heterogeneous exercise–concept associations and interaction transitions by hypergraph modeling and directed graph modeling, respectively. To combine the dual graph models, we introduce the technique of online knowledge distillation. This choice arises from the observation that, while the knowledge tracing model is designed to predict students’ responses to the exercises related to different concepts, it is optimized merely with respect to the prediction accuracy on a single exercise at each step. With online knowledge distillation, the dual graph models are adaptively combined to form a stronger ensemble teacher model, which provides its predictions on all exercises as extra supervision for better modeling ability. In the experiments, we compare DGEKT against eight knowledge tracing baselines on three benchmark datasets, and the results demonstrate that DGEKT achieves state-of-the-art performance.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {78},
numpages = {24},
keywords = {Knowledge tracing, dual graph structure, graph convolutional networks, online knowledge distillation}
}

@article{DKT,
  title={Deep knowledge tracing},
  author={Piech, Chris and Bassen, Jonathan and Huang, Jonathan and Ganguli, Surya and Sahami, Mehran and Guibas, Leonidas J and Sohl-Dickstein, Jascha},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{DKT+Forget,
  title={Augmenting knowledge tracing by considering forgetting behavior},
  author={Nagatani, Koki and Zhang, Qian and Sato, Masahiro and Chen, Yan-Ying and Chen, Francine and Ohkuma, Tomoko},
  booktitle={The world wide web conference},
  pages={3101--3107},
  year={2019}
}

@inproceedings{DKVMN,  
 title={Dynamic Key-Value Memory Networks for Knowledge Tracing}, 
 url={http://dx.doi.org/10.1145/3038912.3052580}, 
 DOI={10.1145/3038912.3052580}, 
 booktitle={Proceedings of the 26th International Conference on World Wide Web}, 
 author={Zhang, Jiani and Shi, Xingjian and King, Irwin and Yeung, Dit-Yan}, 
 year={2017}, 
 month={Apr}, 
 language={en-US} 
}

@inproceedings{EERNN,
  title={Exercise-enhanced sequential modeling for student performance prediction},
  author={Su, Yu and Liu, Qingwen and Liu, Qi and Huang, Zhenya and Yin, Yu and Chen, Enhong and Ding, Chris and Wei, Si and Hu, Guoping},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{EKT,
  title={Ekt: Exercise-aware knowledge tracing for student performance prediction},
  author={Liu, Qi and Huang, Zhenya and Yin, Yu and Chen, Enhong and Xiong, Hui and Su, Yu and Hu, Guoping},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={33},
  number={1},
  pages={100--115},
  year={2019},
  publisher={IEEE}
}

@inproceedings{GKT,  
 title={Graph-based Knowledge Tracing: Modeling Student Proficiency Using Graph Neural Network}, 
 url={http://dx.doi.org/10.1145/3350546.3352513}, 
 DOI={10.1145/3350546.3352513}, 
 booktitle={IEEE/WIC/ACM International Conference on Web Intelligence}, 
 author={Nakagawa, Hiromi and Iwasawa, Yusuke and Matsuo, Yutaka}, 
 year={2019}, 
 month={Oct}, 
 language={en-US} 
}

@inproceedings{HawkesKT,
  title={Temporal cross-effects in knowledge tracing},
  author={Wang, Chenyang and Ma, Weizhi and Zhang, Min and Lv, Chuancheng and Wan, Fengyuan and Lin, Huijie and Tang, Taoran and Liu, Yiqun and Ma, Shaoping},
  booktitle={Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
  pages={517--525},
  year={2021}
}

@article{IRT,  
 title={A general solution for the latent class model of latent structure analysis}, 
 volume={16}, 
 url={http://dx.doi.org/10.1007/bf02289112}, 
 DOI={10.1007/bf02289112}, 
 number={2}, 
 journal={Psychometrika}, 
 author={Green, Bert F.}, 
 pages={151–166}, 
 language={en-US} 
}

@inproceedings{LPKT,
  title={Learning process-consistent knowledge tracing},
  author={Shen, Shuanghong and Liu, Qi and Chen, Enhong and Huang, Zhenya and Huang, Wei and Yin, Yu and Su, Yu and Wang, Shijin},
  booktitle={Proceedings of the 27th ACM SIGKDD conference on knowledge discovery \& data mining},
  pages={1452--1460},
  year={2021}
}

@InProceedings{MLFBK,
author="Li, Zhaoxing
and Jacobsen, Mark
and Shi, Lei
and Zhou, Yunzhan
and Wang, Jindi",
editor="Viberg, Olga
and Jivet, Ioana
and Mu{\~{n}}oz-Merino, Pedro J.
and Perifanou, Maria
and Papathoma, Tina",
title="Broader and Deeper: A Multi-Features with Latent Relations BERT Knowledge Tracing Model",
booktitle="Responsive and Sustainable Educational Futures",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="183--197",
abstract="Knowledge tracing aims to estimate students' knowledge state or skill mastering level over time, which is evolving into an essential task in educational technology. Traditional knowledge tracing algorithms generally use one or a few features to predict students' behaviour and do not consider the latent relations between these features, which could be limiting and disregarding important information in the features. In this paper, we propose MLFBK: A Multi-Features with Latent Relations BERT Knowledge Tracing model, which is a novel BERT based Knowledge Tracing approach that utilises multiple features and mines latent relations between features to improve the performance of the Knowledge Tracing model. Specifically, our algorithm leverages four data features (student{\_}id, skill{\_}id, item{\_}id, and response{\_}id, as well as three meaningful latent relations among features to improve the performance: individual skill mastery, ability profile of students (learning transfer across skills), and problem difficulty. By incorporating these explicit features, latent relations, and the strength of the BERT model, we achieve higher accuracy and efficiency in knowledge tracing tasks. We use t-SNE as a visualisation tool to analyse different embedding strategies. Moreover, we conduct ablation studies and activation function evaluation to evaluate our model. Experimental results demonstrate that our algorithm outperforms baseline methods and demonstrates good interpretability.",
isbn="978-3-031-42682-7"
}

@article{MRT-KT,
  title={Fine-grained interaction modeling with multi-relational transformer for knowledge tracing},
  author={Cui, Jiajun and Chen, Zeyuan and Zhou, Aimin and Wang, Jianyong and Zhang, Wei},
  journal={ACM Transactions on Information Systems},
  volume={41},
  number={4},
  pages={1--26},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@inproceedings{PEBG,
  title={Improving knowledge tracing via pre-training question embeddings},
  author={Liu, Yunfei and Yang, Yang and Chen, Xianyu and Shen, Jian and Zhang, Haifeng and Yu, Yong},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={1577--1583},
  year={2021}
}

@article{ProKT,
  title={Progressive knowledge tracing: Modeling learning process from abstract to concrete},
  author={Sun, Jianwen and Wei, Mengqi and Feng, Jintian and Yu, Fenghua and Li, Qing and Zou, Rui},
  journal={Expert Systems with Applications},
  volume={238},
  pages={122280},
  year={2024},
  publisher={Elsevier}
}

@article{RKT,
  title={RKT: Relation-Aware Self-Attention for Knowledge Tracing},
  author={Shalini Pandey and Jaideep Srivastava},
  journal={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:221370579}
}

@article{RoBERT,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{SAKT,
  title={A self-attentive model for knowledge tracing},
  author={Pandey, Shalini and Karypis, George},
  journal={arXiv preprint arXiv:1907.06837},
  year={2019}
}

@inproceedings{bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}

@article{lstm_bert,
  title={Integrating lstm and bert for long-sequence data analysis in intelligent tutoring systems},
  author={Li, Zhaoxing and Yang, Jujie and Wang, Jindi and Shi, Lei and Stein, Sebastian},
  journal={arXiv preprint arXiv:2405.05136},
  year={2024}
}

@inproceedings{tong2020exercise,
  title={Exercise hierarchical feature enhanced knowledge tracing},
  author={Tong, Hanshuang and Zhou, Yun and Wang, Zhen},
  booktitle={Artificial Intelligence in Education: 21st International Conference, AIED 2020, Ifrane, Morocco, July 6--10, 2020, Proceedings, Part II 21},
  pages={324--328},
  year={2020},
  organization={Springer}
}

