%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf,natbib=true,anonymous=false]{acmart}

\usepackage{times}
\usepackage{soul}
\usepackage{url}
% \usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
% \usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{multirow}
\usepackage{tabu}
\usepackage{boldline}
\usepackage{hhline}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{subfigure}
\usepackage{makecell}
% \usepackage{mathrsfs}
\usepackage{utfsym}
\usepackage{bbding}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage[inline]{enumitem}
\usepackage{enumitem}
\usepackage{IEEEtrantools}
\usepackage{stmaryrd}
\usepackage{colortbl}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[XXX'25]{The XXX}{XXX,
  2025}{XXX, XXX}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
% \usepackage{tabularx} 
% \usepackage{arydshln}
\usepackage[most]{tcolorbox}
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
% \texttt{LLM-KT}
\title{LLM-KT: Aligning Large Language Models with Knowledge Tracing using a Plug-and-Play Instruction}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Ben Trovato}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
% }

% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}


\author{Ziwei Wang$^{1}$, Jie Zhou$^{1*}$, Qin Chen$^{1}$, Min Zhang$^{1}$, Bo Jiang$^{1}$, Aimin Zhou$^{1}$, Qinchun Bai$^{2}$, Liang He$^{1}$
}
\thanks{*Corresponding author, jzhou@cs.ecnu.edu.cn.}
% \authornote{Jie Zhou is the corresponding authors of this paper.}
\affiliation{
  \institution{$^{1}$ School of Computer Science and Technology, East China Normal University, China}
  \country{} 
  \institution{$^{2}$ Shanghai Open University, China}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The knowledge tracing (KT) problem is an extremely important topic in personalized education, which aims to predict whether students can correctly answer the next question based on their past question-answer records.
Prior work on this task mainly focused on learning the sequence of behaviors based on the IDs or textual information. However, these studies usually fail to capture students' sufficient behavioral patterns without reasoning with rich world knowledge about questions.
In this paper, we propose a large language models (LLMs)-based framework for KT, named \texttt{\textbf{LLM-KT}}, to integrate the strengths of LLMs and traditional sequence interaction models.
For task-level alignment, we design Plug-and-Play instruction to align LLMs with KT, leveraging LLMs' rich knowledge and powerful reasoning capacity.
For modality-level alignment, we design the plug-in context and sequence to integrate multiple modalities learned by traditional methods.
To capture the long context of history records, we present a plug-in context to flexibly insert the compressed context embedding into LLMs using question-specific and concept-specific tokens.
Furthermore, we introduce a plug-in sequence to enhance LLMs with sequence interaction behavior representation learned by traditional sequence models using a sequence adapter.
Extensive experiments show that \texttt{\textbf{LLM-KT}} obtains state-of-the-art performance on four typical datasets by comparing it with approximately 20 strong baselines. 
% We will release the code and datasets on Github.
\end{abstract}


\vspace{-1mm}
%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010179</concept_id>
       <concept_desc>Computing methodologies~Natural language processing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010405.10010489.10010495</concept_id>
       <concept_desc>Applied computing~E-learning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003227.10003351</concept_id>
       <concept_desc>Information systems~Data mining</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Natural language processing}
\ccsdesc[500]{Applied computing~E-learning}
\ccsdesc[500]{Information systems~Data mining}

\vspace{-2mm}
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Knowledge Tracing, Intelligent Education, Large Language Models}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

% \vspace{-2mm}
\section{Introduction}
% ``\verb|acmart|''  {\itshape very}
Knowledge tracing \cite{abdelrahman2023knowledge,zanellati2024hybrid,shen2024survey} aims to infer students' performance based on their historical question-answer records for personalized education. 
This technique can help teachers and education systems understand the knowledge status of students, such as their skills and forgetting behavior. By doing so, it provides more accurate teaching plans and resources. Effectively solving the knowledge tracing problem can significantly enhance the efficiency of computer-aided education. 

Traditional deep learning-based models mainly focus on modeling the interaction between questions using IDs (e.g., Question IDs or Concept IDs) to learn the sequence behavior information (See Figure \ref{fig:intro}). 
Sequence learning models (e.g., LSTM \cite{hochreiter1997long} and Transformer \cite{vaswani2017attention}) are utilized to capture the representation of problem-solving records \cite{DKT,SAKT}. 
\citet{DKT+Forget,HawkesKT} integrate the time factor into the model to further learn the sequence information.
Additionally, to learn the relationships among questions and concepts, graph neural networks are adopted \cite{Bi-CLKT,PEBG}. 
These models enhance the interaction representations of an ID sequence using extra knowledge, such as time features and graph structure. 
However, the questions' textual information that contains rich semantic knowledge is not well explored. 
% Furthermore, the textual information of the concepts and questions are also 

\begin{figure}[!t]
% \vspace{-5mm}
\begin{center}
\includegraphics[width=0.48\textwidth]{imgs/intro-new.pdf}
\end{center}
% \vspace{-3mm}
\caption{The advantages of traditional models and LLMs for knowledge tracing. Traditional models are good at learning the sequence of interaction behavior, while LLMs are good at reasoning with rich world knowledge.} 
% \vspace{-4mm}
\label{fig:intro}
\end{figure}


% Meanwhile, with the development of pre-trained models, many researchers have explored the capabilities of large language models (LLMs) in various fields. 
Recently, several studies have incorporated pre-trained language models (PLMs, such as BERT \cite{bert}) into knowledge tracing to model the textual information of the question \cite{BiDKT,tong2020exercise}.
For instance, BiDKT \cite{BiDKT} adapts BERT to trace knowledge by predicting the correctness of randomly masked responses within sequences.  
MLFBK \cite{MLFBK} and LBKT \cite{lstm_bert} leverage BERT to mine complex data relations.  
These methods use the question representations obtained from PLMs to enhance the traditional sequence models.  
Though PLMs are good at natural language understanding, they cannot effectively mine the logic and reasons behind the sequence of questions due to their limited reasoning and world knowledge acquisition abilities.  
Recently, large language models (LLMs) like LLaMA \cite{llama2} have achieved great success in various natural language processing tasks due to their abilities in generation, instruction following, and reasoning.  
In this paper, we aim to integrate LLMs into knowledge tracing to better utilize the world knowledge and powerful reasoning ability of large language models.  
% For example, we can input the instruction with the problem-solving records to predict the performance of the target question (See Figure \ref{fig:intro}).

However, there are two primary challenges to applying LLMs for knowledge tracing. 
First (\textbf{C1}), LLMs struggle to capture sequential interaction behaviors from a series of IDs, which reflect students' knowledge states. Large language models interpret question IDs merely as numbers and fail to comprehend user behavior. This often results in splitting IDs into multiple tokens, causing a loss of semantic information associated with those IDs. Experiments indicate that fine-tuning LLaMA can enhance performance, but the gains are limited (refer to Table \ref{table:main results.}). 
Second (\textbf{C2}), LLMs have difficulty accurately capturing the long textual context of comprehensive problem-solving records. The textual content of questions and concepts is crucial for understanding user behavior. However, historical records may include over 200 questions, with each question averaging around 77 tokens. Experimental findings also suggest that the existing strong LLMs like LLaMA and GPT-4o cannot effectively learn students' states from this textual context.

To leverage the strengths of traditional sequence models and LLMs, we propose an LLM-based framework for knowledge tracing, referred to as \texttt{\textbf{LLM-KT}}. Specifically, we create a plug-and-play instruction that incorporates various modalities (such as textual data and IDs) utilizing specific tokens to align LLMs with knowledge tracing from the task level. For \textbf{C1}, we introduce a Plug-in Sequence that translates the embedding of sequence behavior learned from traditional knowledge tracing models into the LLM space. The traditional knowledge tracing model enhances the LLM's capability to comprehend the semantic and interaction information contained in the sequence of IDs. For \textbf{C2}, we propose a Plug-in Context module to substitute the specific token with representations of an extended context for questions. By aligning the compressed context embedding through a context adapter, it successfully captures the semantic textual information of questions. We perform extensive experiments across four standard datasets, with results demonstrating that our model surpasses all strong baselines in the majority of cases. Ablation studies and further analyses also confirm the effectiveness of \texttt{\textbf{LLM-KT}} and its key components. 

The main contribution of this paper can be summarized as follows:
\begin{itemize}[leftmargin=*, align=left]
    \item For task-level alignment, we propose \texttt{\textbf{LLM-KT}} to align LLMs with knowledge tracing using a Plug-and-Play Instruction. We introduce question- and concept-specific tokens to insert embeddings of texts and IDs flexibly.
    \item For modality-level alignment, we design a plug-in sequence to integrate the sequence interaction representations learned by the traditional sequence modeling algorithms with LLMs. Additionally, we present a plug-in context to capture the long textual context of questions.
    \item A series of experiments indicate that our model obtains the new SOTA performance over four benchmark datasets by comparing it with several strong baselines, which indicate the great advantages of our \texttt{\textbf{LLM-KT}} model. 
\end{itemize}

The rest of the paper is presented as follows. First, we review the most related studies in Section \ref{sect:related work}. Then, we introduce the details of our \texttt{\textbf{LLM-KT}} model in Section \ref{sect:methods}. Furthermore, we present the experimental settings of datasets, evaluation metrics, baselines and implementation details in Section \ref{sect:Experimental Settings}. Finally, we give the experimental analysis and conclusions in Section \ref{sect:Experimental Analysis} and \ref{sec:Conclusions and Further Work}.   

% By Jie Zhou
% Adding more related work 

\section{Related Work}
\label{sect:related work}
% ``\verb|acmart|'' {\itshape template style} \url{https://www.acm.org/publications/proceedings-template}
\subsection{Deep learning-based Knowledge Tracing}
Traditional knowledge tracking algorithms are mainly based on machine learning algorithms, such as Bayesian Knowledge Tracing (BKT) \cite{BKT} and Item Response Theory (IRT) \cite{IRT}. 
With the continuous development and progress of neural networks, deep learning-based knowledge tracing algorithms have emerged to model the sequence interaction \cite{DKT,MRT-KT,LPKT,AdaptKT}. 
DKT \cite{DKT}, or Deep Knowledge Tracing, is the first model to apply deep learning to the field of knowledge tracing, which learns the features of students' historical problem-solving records using Long Short-Term Memory (LSTM). 
% SAKT \cite{SAKT} and AKT \cite{AKT} employ a transformer framework to learn question-answer interaction sequences using an advanced attention mechanism. 
SAKT \cite{SAKT} utilized the self-attention mechanism to address the problem of insufficient generalization ability existing in the processing of sparse data. 
AKT \cite{AKT} further introduced a new monotonic attention mechanism and the classic Rasch-model in psychometrics to better understand students' knowledge mastery status and learning processes.
BEKT \cite{BEKT} proposed a multi-layer bidirectional transformer encoder with a self-attention mechanism and bidirectional analysis, to understand the student's past learning logs.
\citet{DKVMN} proposed a new structure called Dynamic Key-Value Memory Networks (DKVMN), which can utilize the relationships between underlying concepts and directly output the mastery level of each concept by students.  

% To further consider the time factor, DKT-Forget \cite{DKT+Forget} and HawkesKT \cite{HawkesKT} incorporated the time feature into the model.
To further evaluate the time aspect, DKT-Forget \cite{DKT+Forget} enhances DKT by translating the time interval into a numerical value. This value, along with learning interaction data like answering questions, is fed into the neural network. In contrast, HawkesKT \cite{HawkesKT} leverages the intensity function and mechanisms of the Hawkes process to measure the triggering effects of events across different time points. This approach clarifies how learning events temporally influence the probability of subsequent occurrences and the knowledge state.
Addressing limitations in the learning process, which is vital for KT tasks, LPKT \cite{LPKT} assesses students’ knowledge states by modeling their learning journey, capturing knowledge gains while also considering the phenomenon of forgetting. Simultaneously, \citet{ProKT} offers a novel perspective in the KT field by developing the Progressive Knowledge Tracing model. This model emphasizes the learning journey through students’ sequential thought processes and divides it into three relatively independent, yet progressively advanced stages: concept mastery, question-solving, and answering behavior, effectively modeling the transition from abstract reasoning to concrete responses.

Furthermore, graph neural networks are used to model the relationships between different questions or knowledge points in the field of knowledge tracing \cite{GKT,Bi-CLKT,PEBG,DGEKT}. 
GKT \cite{GKT} constructs a knowledge graph based on knowledge points or questions, and utilizes Graph Neural Networks (GNNs) to explore and take advantage of these underlying relational structures. 
% For example, BI-CLKT \cite{Bi-CLKT} designs a two-layer contrastive learning scheme based on the ``exercise-to-exercise" relation subgraph and  
% % , enabling it to effectively establish the spatial association and complex structure of nodes
% PEBG \cite{PEBG} represents the question-skill relationship as a bipartite graph to learn pre-trained question embeddings. 
% BI-CLKT \cite{Bi-CLKT} designs a two-layer contrastive learning scheme based on the ``exercise-to-exercise" relation subgraph and 
BI-CLKT \cite{Bi-CLKT} designs a two-layer comparative learning scheme on an ``exercise-to-exercise" (E2E) relational subgraph for node-level and graph-level contrastive learning to get discriminative representations of exercises and concepts. Additionally, two variants with different prediction layers (RNN and memory-augmented neural networks) are explored to improve representations.
PEBG \cite{PEBG} puts forward a pre-training embedding method through a bipartite graph (PEBG), leveraging edge information (including question difficulty, explicit question-skill relationships, implicit question similarity, and skill similarity) to learn low-dimensional embeddings for each question. 
DGEKT \cite{DGEKT} innovatively constructs a dual graph structure of students' learning interactions, using a concept association hypergraph and a directed transition graph to capture heterogeneous relationships. Additionally, it employs online knowledge distillation to adaptively combine the dual graph models, forming a stronger ensemble teacher model for enhanced modeling ability.



\begin{figure*}[!t]
% \vspace{-5mm}
\begin{center}
\includegraphics[width=1\textwidth]{imgs/LLM-KT-new.pdf}
\end{center}
% \vspace{-1mm}
\caption{The framework of \texttt{\textbf{LLM-KT}}. We propose a Plug-and-Play Instruction to combine the strengths of LLMs and traditional sequence models for knowledge tracing by inserting multiple modalities into LLMs. Particularly, we design a Plug-in Context module to capture the long context of students' problem-solving records. Then, we introduce the Plug-in Sequence to align the sequence interaction representation learned by the traditional model with LLMs.} 
\label{fig:main}
% \vspace{-2mm}
\end{figure*}



\subsection{PLMs-Enhanced Knowledge Tracing}
% BERT~\cite{bert} is renowned for its proficiency in generating high-quality embeddings, which play a vital role in numerous natural language processing tasks. A multitude of BERT variants have been applied in diverse deep learning fields, showcasing their remarkable performances. For instance, ConvBERT extends the application of the original BERT architecture to the image processing domain; BERT4Rec utilizes the BERT model to enhance recommendation systems, LakhNES employs the BERT model to augment Music Generation.
In the field of knowledge tracing, Pre-trained Language Models (PLMs), such as BERT \cite{bert}, RoBERT \cite{RoBERT}, are used to enhance the semantic representation for knowledge tracing \cite{BiDKT,MLFBK,tong2020exercise,DCL4KT-A}. 
For instance, BiDKT \cite{BiDKT} adapts BERT to trace knowledge by predicting the correctness of randomly masked responses within sequences.
MLFBK \cite{MLFBK} leverages the power of BERT to mine latent relations among multiple explicit features, such as individual skill mastery, students' ability profiles, and problem difficulty.
%Two text classification systems, named KDES and
% DFES, are designed to predict the knowledge distribution and difficulty of the
% exercise respectively. The semantic feature extractor system(SFES) could be
% considered as an unsuperviesed clusering problems.
Furthermore, \citet{tong2020exercise} proposes a hierarchical exercise feature enhanced knowledge tracing framework that utilizes BERT to generate exercise text embeddings and then feeds them into three systems (KDES, DFES, and SFES) to extract knowledge distribution, semantic features, and difficulty features. These hierarchical features are concatenated with student responses and input into a sequence model, aiming to improve knowledge tracing by comprehensively considering the diverse attributes of exercises.
Moreover, LBKT \cite{lstm_bert} combines the strengths of BERT for capturing complex data relations and LSTM for handling long sequences, enhancing performance on data with over 400 interactions. 
However, the integration of LLMs with knowledge tracing has not been explored well.


\subsection{Context-aware Knowledge Tracing}
The context information (such as textual features in the questions and concepts) contains a wealth of semantic knowledge, which can help reduce the cold-start phenomenon of knowledge tracing. 
Several studies utilized the context information to enhance traditional deep learning models \cite{RKT,EERNN}. 
For example, RKT \cite{RKT} used the textual information of the questions to capture relations between exercises. 
EERNN \cite{EERNN} and EKT \cite{EKT} considered the text of the questions to learn a good question representation for knowledge tracing. 
Additionally, \citet{PEBG} proposed a pre-training method called PEBG, which learns question embeddings with rich relational information using the bipartite graph of question-skill relations. 
Moreover, \citet{DCL4KT-A} proposed a difficulty-centered contrastive learning method based on the question representations using BERT. 

Unlike previous research, we propose \texttt{\textbf{LLM-KT}} to combine the great advantages of LLMs and traditional sequence learning models for knowledge tracing. 
We design a plug-and-play instruction to align the context and sequence representations with LLMs.

% Relation-aware self-attention for knowledge tracing
% (RKT) and hierarchical graph knowledge tracing
% (HGKT) also extract features from the textual information of questions to learn question representations in their models. 

% Adaptable knowledge tracing (AdaptKT), which transfers knowledge from the source domain to the target one, and 
% In Exercise Hierarchical Feature Enhanced Knowledge Tracing utilize Bert has been proposed \cite{AdaptKT,tong2020exercise}. 

% Yet, previous research has not considered much
% about the latent representation of the textual features in the questions and concepts. Text-aware
% KT models are motivated by leveraging the textual
% features of questions and concepts to enhance performance in tackling KT tasks.



\section{Our Proposed Method}
\label{sect:methods}
In this section, we propose our \texttt{\textbf{LLM-KT}} framework, a novel LLM-based framework specifically designed to handle knowledge tracing tasks (See Figure \ref{fig:main}).  
To align the LLMs with knowledge tracing from the task level, we design a plug-and-play instruction with specific tokens to flexibly integrate multiple modalities (e.g., long context and IDs) into LLMs. 
In this way, we combine the advantages of LLMs, which contain rich knowledge and strong reasoning capacities, with traditional sequence models that excel at learning sequence interaction behavior.  
For the modality level, we first propose a plug-in context to capture the long context of the sequence of questions using a context encoder and align the vector representations with LLMs via a context adapter.  
Then, we introduce a plug-in sequence to enhance the LLMs with the sequence interactive representation learned from traditional sequence models for knowledge tracing.

% The task formulation is given as follows. 
Knowledge tracing involves predicting whether a student will answer a new question correctly based on their historical question-answer records. Formally, given a student's exercise history as $H=(e_1, e_2, ..., e_i, ..., e_{N})$, where $N$ is the number of historical exercises. Here, $e_i = (q_i, a_i)$, where $q_i$ represents the information of the $i$-th question the student answered and $a_i$ indicates the student's response to this question ($a_i = 1$ means the student answered correctly, and $a_i = 0$ means the student answered incorrectly). The goal of the task is to predict the value of $a_{N+1}$ (also defined as $y$) when the student answers $q_{N+1}$.

% \subsection{Overview}
% The use of parameter-efficient fine-tuning (PEFT) \cite{PEFT-1,PEFT-2,PEFT-3,PEFT-4} methods is primarily to address the issue of large-scale parameters in modern deep learning models. Traditional fine-tuning methods typically require adjusting all parameters, which not only demands significant computational resources but also extensive labeled data. This approach is often impractical when resources are limited. Therefore, we choose PEFT methods to adjust large language models. 


\subsection{Plug-and-Play Instruction}
To utilize the rich world knowledge and reasoning capacity, we design a Plug-and-Play Instruction for \texttt{\textbf{LLM-KT}} to guide LLMs in helping the model accurately capture changes in student learning behaviors and knowledge states.  
Specifically, we develop instructions with question-specific and concept-specific slots to seamlessly integrate knowledge from various modalities, including IDs and contextual information. 
We insert specific tokens and replace the token embeddings with representations of long context and IDs that contain rich semantic and interaction information.
The Plug-and-Play Instruction organizes data into (input, output) pairings using designated tokens. 
The input consists of a student's prior question-answer records along with information about the current question requiring an answer. The output indicates the LLM's predicted evaluation of the student's expected answer's accuracy. Specifically, the instruction with input prompt $x$ and output $y$ is structured as follows:
\\
\noindent\par
% \vspace{2mm}
\noindent\resizebox{.99\columnwidth}{!}{
\begin{tcolorbox}[left=1mm,right=1mm,top=1mm,bottom=1mm]
\textbf{Input $x$.} \\
The student has previously, in chronological order, answered \textcolor{blue}{\textbf{\{}}question QID=38 [\textcolor{red}{QuesEmbed$_{38}$}] involving concept CID=219 [\textcolor[RGB]{0,205,102}{ConcEmbed$_{219}$}] correctly, ..., question QID=57 [\textcolor{red}{QuesEmbed$_{57}$}] involving concept CID=204 [\textcolor[RGB]{0,205,102}{ConcEmbed$_{204}$}] incorrectly\textcolor{blue}{\textbf{\}}$_\text{HistoryRecord}$}. \\
Please predict whether the student will answer the next \textcolor{cyan}{\textbf{\{}}question QID=55 [\textcolor{red}{QuesEmbed$_{55}$}] involving concept CID=245 [\textcolor[RGB]{0,205,102}{ConcEmbed$_{245}$}] correctly\textcolor{cyan}{\textbf{\}$_\text{TargetQues}$}}. Response with 'Yes' or 'No'. Response:
% \hline

\textbf{Output $y$.} \\
Yes/No
\end{tcolorbox}
}


In this template $x$, \textcolor{blue}{\textbf{\{\}}$_\text{HistoryRecord}$} represents the student's historical question-answer records, which consist of a series of (question, answer) pairs. Each question includes specific attributes such as question ID and knowledge concepts, and the answer indicates whether the student answered correctly or incorrectly; for example, ``question QID=38 [\textcolor{red}{QuesEmbed$_{38}$}] involving concept CID=219 [\textcolor[RGB]{0,205,102}{ConcEmbed$_{219}$}] correctly.'' Here, [\textcolor{red}{QuesEmbed$_{38}$}] and [\textcolor[RGB]{0,205,102}{ConcEmbed$_{219}$}] mean the specific tokens of question $38$ and concept $219$, where the embeddings are learned by Plug-in Context and Plug-in Sequence. \textcolor{cyan}{\textbf{\{\}$_\text{TargetQues}$}} represents the information about the target question to be predicted without indicating whether the answer is correct or incorrect. This approach allows the LLMs to understand the context of each question and the student's performance related to it.

Then, we input the $x$ into an LLM $\mathcal{M}_{\theta}$ to predict the label $\hat{y}$, where $\theta$ is the trainable parameters of the model. $\hat{y}$=``Yes'' if the student answer the target question $q_{N+1}$ correctly, otherwise $\hat{y}$=``No''. 
In the training phase, we use the language model with cross-entropy loss as the training objective.
% \begin{equation}
%     \mathcal{L} = - \log P(y|x; \theta)
% \end{equation}
Specifically, we adopt Low-Rank Adaptation (LoRA) \cite{LoRA}, which is a popular parameter-efficient fine-tuning method. The core idea of LoRA is to adapt model parameters through low-rank decomposition. 
LoRA adds two low-rank matrices, $A$ and $B$, to the model's weight matrix. 
During fine-tuning, only these two low-rank matrices are updated ($\theta=\{A, B\}$), while the original weight matrix remains unchanged. 
This approach significantly reduces the number of parameters that need to be updated, thereby lowering computational and storage costs. 


Furthermore, the question representation [\textcolor{red}{QuesEmbed$_{QID}$}] = $e_{Ques} \in \mathbb{R}^{d^e}$ and concept representation [\textcolor[RGB]{0,205,102}{ConcEmbed$_{CID}$}] = $e_{Conc} \in \mathbb{R}^{d^e}$ consist of context and sequence embeddings that are learned by Plug-in Context $f_{\text{cont}}$ (Section \ref{sect:plug-in context}) and Plug-in Sequence $f_{\text{seq}}$ (Section \ref{sect:plug-in sequence}), where ${d^e}$ is the dimension of LLM's embedding layer.
\begin{equation}
\label{equ:combine}
\begin{aligned}
    e_{Ques} = g(f_{\text{cont}}(QText), f_{\text{seq}}(QID)) \\
    e_{Conc} = g(f_{\text{cont}}(CText), f_{\text{seq}}(CID))
\end{aligned}
\end{equation}
where $g(a,b)$ is a function to combine $a$ and $b$, such as concatenation, average and addition. $QText$ and $CText$ are the textual information of the question and concept, $QID$ and $CID$ are the ids of question and concept.

In the inference phase, we calculate the probability distribution as follows:
\begin{equation}
p(\hat{y}=``\mathrm{Yes}"|x)=\frac{e^{\mathcal{M}_{\theta}(``\mathrm{Yes}"|x)}}{e^{\mathcal{M}_{\theta}(``\mathrm{Yes}"|x)}+e^{\mathcal{M}_{\theta}(``\mathrm{No}"|x)}}
\end{equation}
where $\mathcal{M}_{\theta}(``\mathrm{Yes}"|x)$ and $\mathcal{M}_{\theta}(``\mathrm{No}"|x)$ are the output probabilities of token ``Yes'' and ``No'' of $\mathcal{M}_\theta$.

\subsection{Plug-in Context}
\label{sect:plug-in context}
Due to the challenge of processing excessively long texts with large models, it is impractical to model the complete history of all questions. 
% Due to the record's long history and the question's long context, it is difficult for LLMs to capture the semantic information well. 
Thus, we propose a Plug-in Context $f_{\text{cont}}$ to model the textual information of question and concept. Particularly, we adopt a context encoder to model the text, and then a context adapter is used to align the representation with LLMs.


\subsubsection{Context Encoder}
We leverage a context encoder to encode question and knowledge concept texts into vector representations. 
Here, we use a pre-trained language model (e.g., LLaMA2 \cite{llama2}, BERT \cite{bert}, and all-mpnet-base-v2 \cite{Mpnet}) as the context encoder to obtain the textual question representation $r_{QText} \in \mathbb{R}^{d^t}$ and concept representation $r_{QText} \in \mathbb{R}^{d^t}$, where $d^t$ is the hidden dimension of context encoder. 
These representations help knowledge tracing models capture the semantic relationships between questions and knowledge concepts.
\begin{equation}
\begin{aligned}
    r_{QText} = \mathrm{ContextEncoder}(QText)  \\
    r_{CText} = \mathrm{ContextEncoder}(CText)
\end{aligned}
\end{equation}

\subsubsection{Context Adapter}
Then, we align these representations with the semantic space of the large model to ensure compatibility and effective knowledge tracing.
To achieve this alignment, we design a context adapter to map the text representation learned by the context encoder to LLMs. 
\begin{equation}
\begin{aligned}
    h_{QText} = \mathrm{ContAdapter}(r_{QText}) \\
    h_{CText} = \mathrm{ContAdapter}(r_{CText})
\end{aligned}
\end{equation}
where the $h_{QText} \in \mathbb{R}^{d^e}$ and $h_{CText} \in \mathbb{R}^{d^e}$ are the representations of question and concept after alignment.
Particularly, we used a simple multi-layer perceptron (MLP) layer as a context adapter to translate the space.

\subsection{Plug-in Sequence}
\label{sect:plug-in sequence}
Large language models have great abilities in natural language understanding and generation by training on large-scale text data. 
To integrate new modalities like ID sequence into LLMs, we propose a Plug-in Sequence $f_{\text{seq}}$ to better capture the interactive information.
In this way, we can utilize the strengths of LLMs and traditional models to learn both semantic and interaction behavior simultaneously.
Similarly, we adopt traditional models as a sequence encoder to learn the sequence representation. Then, a sequence adapter is used to convert the sequence representation into the space of LLMs.

\subsubsection{Sequence Encoder}
We treat the sequence of IDs as a new modality to be injected into LLMs. We adapt existing traditional sequence learning models (e.g., DKT \cite{DKT}, AKT \cite{AKT}) for knowledge tracing to learn question ID embeddings $r_{QID} \in \mathbb{R}^{d^s}$ and concept ID embeddings $r_{CID} \in \mathbb{R}^{d^s}$, where $d^s$ means the dimension of sequence encoder. 
These models are good at learning the sequence interaction based on the question IDs and concept IDs.
\begin{equation}
\begin{aligned}
    r_{QID} = \mathrm{SeqEncoder}(QID)  \\
    r_{CID} = \mathrm{SeqEncoder}(CID)
\end{aligned}
\end{equation}

\subsubsection{Sequence Adapter}
However, due to the semantic space of the traditional model being quite different from that of the large model, we introduce a sequence adapter to align the ID representations from the traditional knowledge tracer with LLMs. This approach ensures better integration and enhances the performance of the knowledge tracing task.
\begin{equation}
\begin{aligned}
    h_{QID} = \mathrm{SeqAdapter}(r_{QID})  \\
    h_{CID} = \mathrm{SeqAdapter}(r_{CID})
\end{aligned}
\end{equation}
where $h_{QID} \in \mathbb{R}^{d^e}$ and $h_{CID} \in \mathbb{R}^{d^e}$ are the aligned representations of question ids and concept ids.


\begin{table*}[t]
\centering
% \small
\caption{The statistical information of the datasets. QID and CID mean the ID of the question and concept. KCs means the number of knowledge concepts. QuesCont and ConcCont represent the context of the question and concept.}
\label{table:statistic of datasets}
\vspace{-1mm}
% \resizebox{.99\columnwidth}{!}{
\setlength{\tabcolsep}{3.0mm}{
\begin{tabular}{lcccccccc} 
\hlineB{4}
Dataset & Students & Questions & KCs & Interactions & QID & CID & QuesCont & ConcCont  \\ \hline
Assist2009 & 4,151 & 16,891 & 110 & 325,637  & \ding{51} & \ding{51} & \ding{55} & \ding{51} \\ 
Assist2015 & 19,840 & - & 100 & 683,801  & \ding{55} & \ding{51} & \ding{55} & \ding{55}\\ 
Junyi & 1,000 & 834 & - & 972,855 & \ding{51} & \ding{55} & \ding{55} & \ding{55} \\ 
Nips2020 & 5,310 & 110 & 17 & 428,596  &  \ding{51} &\ding{51} &\ding{51} &\ding{51} \\ \hlineB{4}
\end{tabular}
}
\vspace{-1mm}
\end{table*}

\section{Experimental Settings}
\label{sect:Experimental Settings}
In this section, we first introduce the datasets and evaluation in Section \ref{sect:Datasets and Evaluation}. Then, we list the baselines in Section \ref{sect:Baseines} and provide the implementation details in Section \ref{sect:Implementation Details}.

\subsection{Datasets and Evaluation}
\label{sect:Datasets and Evaluation}
\subsubsection{Datasets} 
To evaluate the effectiveness of our \texttt{\textbf{LLM-KT}}, we conduct experiments on four commonly used benchmark datasets for knowledge tracing. The statistical information of these datasets is listed in Table \ref{table:statistic of datasets}.

\begin{itemize}[leftmargin=*, align=left]
    \item ASSISTments2009 (Assist2009) \cite{AKT} collects the exercises of 4151 students during the 2009 to 2010 school year. The same as \citet{AKT}, we use the skill builder data version of this dataset. 
    To ensure the validity of the data, we only retain those records where both the skill\_name and skill\_id fields are not empty. 
    \item ASSISTments2015 (Assist2015) \cite{AKT} comprises responses from students on 100 distinct questions. Different from Assist2009, this dataset does not provide metadata of questions.
    \item Junyi Academy (Junyi) \cite{Edudata} is provided by Junyi Academy - the premier online learning platform in Taiwan, consisting of over 16 million exercise attempt logs. 
    These logs are contributed by more than 72,000 students, spanning a year, specifically from August 2018 to July 2019. 
    We use the dataset provided by \citet{Edudata}, which is processed specifically for knowledge tracing.
    \item NeurIPS 2020 Education Challenge (Nips2020) \cite{Nips2020} is released by the NeurIPS 2020 Education Challenge. 
    In this paper, we use the datasets from Challenge Task 3 \& 4 and extract the records of the top 150 most frequently appearing questions. 
    Note that, to obtain the textual information of the questions, we convert the figures into text manually.
\end{itemize}

\subsubsection{Evaluation} 
% \subsection{Evaluation}
% For all datasets, they are strategically partitioned into training, validation, and testing subsets following an 8:1:1 ratio, thereby allocating the majority of the data towards the training phase. Moreover, the careful selection of pertinent evaluation metrics is fundamental for accurately gauging our model's performance. 
Following \cite{MRT-KT,AKT,LPKT}, we use two widely-used metrics: Area Under the Curve (AUC) and Accuracy (ACC) to evaluate the effectiveness of our model. 
% To counteract the influence of randomness on the results, we employ the mean value of five distinct random seeds as the ultimate outcome.


\subsection{Baselines}
\label{sect:Baseines}
In our research, we compare our proposed approach with several strong baseline methodologies to assess its efficacy and performance. We split these baselines into four parts: deep-learning (DL)-based, pre-trained language models (PLMs)-based, context-aware and LLMs-based methods.

\subsubsection{DL-based Methods} 
DL-based methods learn the interactions among students' records effectively by taking the relationships and times into account. Here, we select 7 typical baselines as follows: 
\begin{itemize}[leftmargin=*, align=left]
    \item \textbf{DKT} \cite{DKT} uses RNNs(\cite{RNN} to model temporal dependencies in student learning, capturing the evolution of knowledge states.
    \item \textbf{DKVMN} \cite{DKVMN} implements a dynamic key-value memory network, where static matrices store knowledge concepts and dynamic matrices update mastery levels, enhancing the modeling of concept relationships.
    \item \textbf{SAKT} \cite{SAKT} employs a self-attention mechanism(\cite{vaswani2017attention}) to identify key knowledge concepts (KCs) from past interactions.
    \item \textbf{AKT} \cite{AKT} utilizes a monotonic attention mechanism to build context-aware representations of student interactions, capturing performance over appropriate time scales.
    \item \textbf{LPKT} \cite{LPKT} models the learning process by formalizing learning cells and incorporating gates for managing retention and forgetting over time.
    \item \textbf{LBKT$^\dag$} \cite{LBKT} analyzes the interplay of learning behaviors (e.g., speed, attempts, hints) and uses a forgetting factor to update learners’ knowledge states.
    \item \textbf{MRT-KT} \cite{MRT-KT} employs a multi-relational transformer with a novel relation encoding scheme to model fine-grained interactions between question-answer pairs in knowledge tracing.
\end{itemize}

\subsubsection{PLMs-based Methods}
PLMs-based methods improve the performance of knowledge tracing via the rich knowledge and powerful natural language understanding of PLMs. Here, we adopt the following baselines:
\begin{itemize}[leftmargin=*, align=left]
    \item \textbf{LBKT$^\ddag$} \cite{lstm_bert} addresses long-sequence data in knowledge tracing by integrating a BERT-based architecture with Rasch model embeddings for difficulty levels and an LSTM for sequential processing.
    \item \textbf{MLFBK} \cite{MLFBK} utilizes BERT to incorporate explicit features and latent relations, enhancing prediction efficiency in knowledge tracing.
    \item \textbf{BiDKT} \cite{BiDKT} adapts BERT for knowledge tracing by leveraging bidirectional context in interaction histories, unlike traditional RNN-based models.
\end{itemize}
\subsubsection{Context-Aware Methods}
For context-aware methods, they utilize the context of questions to learn semantic knowledge. Particularly, we select the following four algorithms:
\begin{itemize}[leftmargin=*, align=left]
    \item \textbf{EERNN} \cite{EKT} combines student records and exercise content into a single vector, processed by a bidirectional LSTM, with two variants: EERNNM (Markov property) and EERNNA (Attention mechanism).
    \item \textbf{EKT} \cite{EKT} extends EERNN by using a knowledge state matrix, which captures the impact of exercises on multiple concepts, while a memory network tracks concept mastery.
    \item \textbf{RKT} \cite{RKT} uses relation-aware self-attention to integrate contextual information from exercises and performance data. It also includes a forgetting model with an exponentially decaying kernel to address interactions and forgetfulness.
    \item \textbf{DCL4KT-A} \cite{DCL4KT-A} introduces a difficulty-centered contrastive learning method and leverages LLMs to optimize and predict difficulty from unseen data.
\end{itemize}

\subsubsection{LLMs-based Methods}
Furthermore, LLM-based methods have good reasoning abilities with rich commonsense knowledge. We conduct four versions of LLMs based on both fine-tuning and prompting:
\begin{itemize}[leftmargin=*, align=left]
    \item \textbf{LLM-FT$_\mathrm{ID}$} finetunes the LLaMA model with instructions using QIDs and/or CIDs depending on the dataset.
    \item \textbf{LLM-FT$_\mathrm{TokenID}$} finetunes the LLaMA model by treating QID and CID as specific tokens, where their embeddings are updated during training.
    \item \textbf{LLM-FT$_\mathrm{Text}$} finetunes the LLaMA model using textual information of questions and concepts.
    \item \textbf{GPT-4o} inputs the same textual information as LLM-FT$_\mathrm{Text}$ directly into the GPT-4o framework.
\end{itemize}

% By Jie Zhou
% Add more details about why we miss some many results in Table
For DL-based, PLMs-based, and Context-aware Methods, we only report the results from the original papers or other relevant experimental papers to ensure the reliability of the experimental results.
For LLMs-based Methods, since Assist2015 and Junyi have no textual information of the question and concept, we don't provide the results of LLM-FT$_{Text}$ and GPT-4o.
Note that we remove the information missed in the corresponding dataset (such as QID in Assist2015) from the prompt template in our experiments. 
% Detailed information of the prompt templates is provided in the Appendix.

% \begin{table*}[hbt!]
% \caption{The prompt templates for LLM-KT(Ours)}
% \label{table:llm-kt-templates}
% \centering
% % \small
% \begin{tabular}{| >{\centering\arraybackslash}m{0.1\textwidth} | >{\raggedright\arraybackslash}m{0.8\textwidth} |}
% \hline
% \rowcolor{lightgray} % 设置第一行背景颜色
% \textbf{Type} & \multicolumn{1}{c|}{\textbf{Template}} \\
% \hline
% 1 & The student has previously, in chronological order, answered question with ID=74 [WrapQEmb]
% involving concept ID=6 [WrapCEmb] correctly, ...,
% question with ID=42 [WrapQEmb] involving concept ID=5 [WrapCEmb] incorrectly.
% Please predict whether the student will answer the next question with ID=44 [NextWrapQEmb]
% involving concept ID=5 [NextWrapCEmb] correctly. 
% Response with ‘Yes’ or ‘No’. \\
% \hline
% 2 & The student has previously, in chronological order, answered question with ID=3117 [QidEmb]
% correctly, question with ID=2964 [QidEmb] correctly, 
% question with ID=5627 [QidEmb] incorrectly, ...,
% question with ID=5532 [QidEmb] correctly.
% Please predict whether the student will answer the next question with ID=5707 [NextQidEmb] correctly.
% Response with ‘Yes’ or ‘No’. \\
% \hline
% 3 & The student has previously, in chronological order, answered question involving concept ID=15 [CidEmb]
% correctly, ...,
% question involving concept ID=30 [NextCidEmb] correctly.
% Please predict whether the student will answer the next question involving concept ID=30 correctly.
% Response with ‘Yes’ or ‘No’. \\
% \hline
% \end{tabular}
% \end{table*}

% \begin{table*}[hbt!]
% \caption{The prompt templates for LLM-FT$_\mathrm{ID}$}
% \label{table:llm-kt-id-templates}
% \centering
% % \small
% \begin{tabular}{| >{\centering\arraybackslash}m{0.1\textwidth} | >{\raggedright\arraybackslash}m{0.8\textwidth} |}
% \hline
% \rowcolor{lightgray} % 设置第一行背景颜色
% \textbf{Type} & \multicolumn{1}{c|}{\textbf{Template}} \\
% \hline
% 1 & The student has previously, in chronological order, answered question with ID=74
% involving concept ID=6 correctly, question with ID=80 involving concept ID=6 correctly,
% ...,
% question with ID=42 involving concept ID=5 incorrectly.
% Please predict whether the student will answer the next question with ID=44
% involving concept ID=5 correctly.
% Response with ‘Yes’ or ‘No’. \\
% \hline
% 2 & The student has previously, in chronological order, answered question with ID=3117
% correctly, question with ID=2964 correctly,
% ...,
% question with ID=5532 correctly.
% Please predict whether the student will answer the next question with ID=5707 correctly.
% Response with ‘Yes’ or ‘No’. \\
% \hline
% 3 & The student has previously, in chronological order, answered question involving concept ID=15
% correctly, question involving concept ID=15 correctly,
% ...,
% question involving concept ID=30 correctly.
% Please predict whether the student will answer the next question involving concept ID=30 correctly.
% Response with ‘Yes’ or ‘No’. \\
% \hline
% \end{tabular}
% \end{table*}

% \begin{table*}[hbt!]
% \caption{The prompt templates for LLM-FT$_\mathrm{TokenID}$}
% \label{table:llm-kt-tokenid-templates}
% \centering
% % \small
% \begin{tabular}{| >{\centering\arraybackslash}m{0.1\textwidth} | >{\raggedright\arraybackslash}m{0.8\textwidth} |}
% \hline
% \rowcolor{lightgray} % 设置第一行背景颜色
% \textbf{Type} & \multicolumn{1}{c|}{\textbf{Template}} \\
% \hline
% 1 & The student has previously, in chronological order, answered question with ID=[qid$_{74}$] involving concept ID=[cid$_{6}$] correctly, question with ID=[qid$_{80}$] involving concept ID=[cid$_{6}$] correctly, ..., question with ID=[qid$_{42}$] involving concept ID=[cid$_{5}$] incorrectly. Please predict whether the student will answer the next question with ID=[qid$_{44}$] involving concept ID=[cid$_{5}$] correctly. Response with `Yes' or `No'. \\
% \hline
% 2 & The student has previously, in chronological order, answered question with ID=[qid$_{3117}$] correctly, question with ID=[qid$_{2964}$] correctly, question with ID=[qid$_{5627}$] incorrectly, ..., question with ID=[qid$_{5532}$] correctly. Please predict whether the student will answer the next question with ID=[qid$_{5707}$] correctly. Response with `Yes' or `No'. \\
% \hline
% 3 & The student has previously, in chronological order, answered question involving concept ID=[cid$_{15}$] correctly, question involving concept ID=[cid$_{15}$] correctly, question involving concept ID=[cid$_{30}$] incorrectly, ..., question involving concept ID=[cid$_{30}$] correctly. Please predict whether the student will answer the next question involving concept ID=[cid$_{30}$] correctly. Response with `Yes' or `No'. \\
% \hline
% \end{tabular}
% \end{table*}

% \begin{table*}[hbt!]
% \caption{The prompt templates for LLM-FT$_\mathrm{Text}$}
% \label{table:llm-kt-text-templates}
% \centering
% % \small
% \begin{tabular}{| >{\centering\arraybackslash}m{0.1\textwidth} | >{\raggedright\arraybackslash}m{0.8\textwidth} |}
% \hline
% \rowcolor{lightgray} % 设置第一行背景颜色
% \textbf{Type} & \multicolumn{1}{c|}{\textbf{Template}} \\
% \hline
% 4 & In this task, we aim to determine whether the student can answer the question correctly based on the student's history record of academic exercises. \newline
%     The student's history record of academic exercises is given as follows: \newline
%     1) How would this calculation be written? Pic$_{290-0}$ \newline
%     % A:8+(2÷5)=2; \\ B:(8+2)÷5=2; \\ C:8+2÷5=2; \\ D:(8+2÷5)=2 \\
%     A:8+(2÷5)=2 B:(8+2)÷5=2 C:8+2÷5=2 D:(8+2÷5)=2 \newline
%     Related knowledge concepts: Basic Arithmetic \newline
%     The student answered this question correctly \newline
%     2) Which symbol belongs in the box? Pic$_{749-0}$ \newline
%     % A:$>$; \\ B:$<$; \\ C:$=$; \\ D:$\ge$ \\
%     A:$>$  B:$<$  C:$=$ D:$\ge$ \newline
%     Related knowledge concepts: Basic Arithmetic \newline
%     The student answered this question correctly \newline
%     % ..., \newline
%     3) What is the output of this Function Machine? Pic$_{836-0}$ \newline
%     % A:10p; \\ B:7p; \\ C:5(p+2); \\ D:5p+2 \\
%     A:10p B:7p  C:5(p+2)  D:5p+2 \newline
%     Related knowledge concepts: Writing Expressions \newline
%     The student answered this question incorrectly \newline
%     The target question is given as follows: \newline
%     Tom and Katie are arguing about the result of this Function Machine: Pic$_{856-0}$. Tom says the output is: 3n-12. Katie says the output is:3(n-4). Who is correct? \newline
%     % A:Only Tom; \\ B:Only Katie; \\ C:Both Tom and Katie; \\ D:Neither is correct \\
%     A:Only Tom B:Only Katie C:Both Tom and Katie  D:Neither is correct \newline
%     Related knowledge concepts: Writing Expressions \newline
%     Please predict whether the student would answer the target question correctly. Response with `Yes' or `No'. \\
% \hline
% 5 & The student has previously, in chronological order, answered question involving concept ``Basic Arithmetic" correctly, question involving concept ``Basic Arithmetic" correctly, ..., question involving concept ``Basic Arithmetic" incorrectly, question involving concept ``Basic Arithmetic" correctly, ..., question involving concept ``Ordering Negative Numbers" incorrectly, question involving concept ``Ordering Negative Numbers" correctly. Please predict whether the student will answer the next question involving concept ``Ordering Negative Numbers" correctly. Response with `Yes' or `No'. \\
% \hline
% \end{tabular}
% \end{table*}



\begin{table*}
% \small
% \resizebox{.99\columnwidth}{!}{
\caption{Main results of our models and selected baselines. We give the results not reported by the original paper from \citet{MRT-KT,RKT,DKT}. Imp. means the relative improvement over the baseline LLM-FT$_\mathrm{ID}$. The best and suboptimal results are emphasized in \textbf{bold} and \underline{underline}.}
\label{table:main results.}
\vspace{-1mm}
\centering
\begin{tabular}{llcccccccc}
\hlineB{4}
& & \multicolumn{2}{c}{Assist2009} & \multicolumn{2}{c}{Assist2015} & \multicolumn{2}{c}{Junyi} & \multicolumn{2}{c}{Nips2020} \\ 
&  & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC \\ \hline
\multirow{8}{*}{DL-based Methods}
&  DKT      & 0.7084 & 0.7221 & 0.7093 & 0.7542 & 0.8013 & 0.7200 & 0.7406 & 0.6878 \\
&  DKVMN    & 0.8157 & - & 0.7268 & - & 0.8027 & - & 0.7673 & 0.7016 \\
&  SAKT     & 0.8480 & - & 0.8540 & - & 0.8340 & 0.7570 & 0.7517 & 0.6879 \\
&  AKT      & 0.7767 & 0.7532 & 0.7211 & 0.7518 & \underline{0.8948} & 0.8215 & 0.7494 & 0.6930 \\
&  LPKT     & 0.7788 & 0.7325  & - & - & 0.7689  & \underline{0.8344}  & - & - \\
&  LBKT$^\dag$  & 0.7863 & 0.7380 & - & - & 0.7723  & \textbf{0.8362}  & - & - \\
&  AT-DKT   & 0.7574 & 0.7172 & - & - & 0.7581 & 0.8325 & 0.7816 & 0.7145 \\
&  MRT-KT   & 0.8223 & 0.7841 & - & - & - & - & - & -  \\ \hline
\multirow{3}{*}{PLMs-based Methods} &  BiDKT    & 0.7651 & - & 0.6766 & - & - & - & - & - \\
&  MLFBK    & \underline{0.8524} & - & - & - & - & - & - & - \\
&  LBKT$^\ddag$     & - & - & - & - & 0.8510 & 0.8320 & - & -\\ \hline
\multirow{4}{*}{Context-Aware Methods}
&  DCL4KT-A   & 0.8153 & - & - & - & - & - & - & - \\ 
&  EERNN   & - & - & - & - & 0.8370 & 0.7580 & - & - \\ 
&  EKT     & - & - & - & - & 0.8420 & 0.7590 & - & - \\ 
&  RKT     & - & - & - & - & 0.8600 & 0.7700 & - & - \\ \hline
\multirow{4}{*}{LLMs-based Methods} 
&  LLM-FT$_\mathrm{ID}$   & 0.8393 & 0.7592 & \underline{0.9092} & \underline{0.9092} & {0.8841} & 0.8071 & \underline{0.7890} & 0.6870 \\ % using the original ids
&  LLM-FT$_\mathrm{TokenID}$   & 0.8143 & 0.7954 & 0.8386 & 0.8813 & 0.8663 & 0.8050 & 0.7774 & 0.5962 \\
&  LLM-FT$_\mathrm{Text}$   & 0.8407 & \underline{0.8119} & - & - & - & -  & 0.7762 & \underline{0.7211} \\ 
 &  GPT-4o   & - & 0.7274 & - & - & - & - & - & 0.6694 \\
\hline
\multirow{2}{*}{Ours} &  \texttt{\textbf{LLM-KT}}                 & \textbf{0.8870} & \textbf{0.8168} & \textbf{0.9356} & \textbf{0.9185} & \textbf{0.9018} & {0.8294} & \textbf{0.8291} & \textbf{0.7561} \\
&  \textcolor{blue}{Imp. (\%)}    & \textcolor{blue}{+5.68} & \textcolor{blue}{+7.59} & \textcolor{blue}{+2.90} & \textcolor{blue}{+1.02} & \textcolor{blue}{+2.00} & \textcolor{blue}{+2.76} & \textcolor{blue}{+5.08} & \textcolor{blue}{+10.06} \\ \hlineB{4}
\end{tabular}
% }
\vspace{-2mm}
% \vspace{-2mm}
\end{table*}


\subsection{Implementation Details}
\label{sect:Implementation Details}
% In our experiments, We use the deep learning framework--PyTorch Lightning with precision in {16,bf16} and amp backend "native". Our \textbf{\texttt{LLM-KT}} model is based on LLaMA2-7B \cite{llama2}. We update our model using a parameter-efficient method LoRA with rank $r=32$. We train each task for $10$ epochs using a batch size of $1$ with accumulate grad batches=$32$. We use AdamW as the optimizer with learning rate of $3e-4$ and weight decay of $1e-5$, using a cosine learning rate scheduler. 
% We mainly use LLaMA2 as the context encoder and AKT as the sequence encoder.
In our experiments, we use the deep learning framework PyTorch Lightning for its ease of use and efficient management of training processes. Following MRT-KT \cite{MRT-KT}, we divide the student dataset in a ratio of 8:1:1 for training, validation, and test sets. Then, we train on the training set, select the best model based on the validation set, and evaluate it on the test set. Our \textbf{\texttt{LLM-KT}} model is based on LLaMA2 \cite{llama2}, which is an advanced and commonly used open-source LLM with over 9,000 citations. We update our model using the parameter-efficient method LoRA, where the rank is $32$, alpha is $32$, and dropout is $0.1$. Each task is trained for a maximum of 10 epochs with a batch size of 32, using the gradient accumulation strategy. For the function $g$ used to merge the representations of context and sequence, we employ the addition operation. We utilize early stopping to avoid overfitting. Additionally, we use Adam as the optimizer with a learning rate of $3 \times 10^{-4}$ and a weight decay of $1 \times 10^{-5}$, utilizing a cosine learning rate scheduler. The sequence length of historical records is 100. We adopt LLaMA2-7B as the context encoder and AKT as the sequence encoder.

To elucidate the distinctions among diverse models within LLMs-based methods more perspicuously, we initially streamline the template in Section \ref{sect:methods} as follows.
% \\
\noindent\par
% \vspace{2mm}
\noindent\resizebox{.99\columnwidth}{!}{
\begin{tcolorbox}[left=1mm,right=1mm,top=1mm,bottom=1mm]
\textbf{Input $x$.} \\
The student has previously, in chronological order, answered \underline{HistoryQues$_1$, HistoryQues$_2$,..., HistoryQues$_n$}. Please predict whether the student will answer \underline{TargetQues} correctly. Response with 'Yes' or 'No'. Response:
% \hline
\end{tcolorbox}
}

For LLM-FT$_\mathrm{ID}$, we simply replace each ``HistoryQues'' in the template with the format ``question with ID=QID involving concept ID=CID correctly.'' At the same time, we change ``TargetQues'' to ``the next question with ID=QID involving concept ID=CID correctly.'' Note that the actual values of these IDs depend on the specific responses shared by students.
For LLM-FT$_\mathrm{TokenID}$, we adjust ``HistoryQues'' in the template with ``question with ID=[qid74] involving concept ID=[cid6] correctly'' and turn ``TargetQues'' into ``the next question with ID=[qid44] involving concept ID=[cid5] correctly.'' Here, [qid74]/[cid6]/[qid44]/[cid5] represents a newly introduced token that is finetuned on the target dataset to learn the semantic and interaction information in the given record. The number of newly added QID/CID tokens exactly matches the number of questions and concepts. 
As for LLM-FT$_\mathrm{Text}$ and GPT-4o, we replace ``HistoryQues'' and ``TargetQues'' in the template with specific textual questions. For example, we use ``Which symbol belongs in the box? Pic$_{749-0}$ A:$>$  B:$<$  C:$=$ D:$\ge$ Related knowledge concepts: Basic Arithmetic The student answered this question correctly.'' We then input these into LLaMA and GPT-4o, respectively.

% As for the LLM-KT we put forward, we replace "historyQues" with "question with ID=74 [WrapQEmb] involving concept ID=6 [WrapCEmb] correctly", and substitute "TargetQues" with "the next question with ID=44 [NextWrapQEmb] and involving concept ID=5 [NextWrapCEmb] correctly". [WrapQEmb] and [WrapCEmb] respectively denote the amalgamated representations of historical question and concept text, along with sequence information. Meanwhile, [NextWrapQEmb] and [NextWrapCEmb] stand for the combined representations of target question and concept text together with sequence information.

% \begin{table*}[hbt!]
% \caption{The prompt templates for LLM-KT(Ours)}
% \label{table:llm-kt-templates}
% \centering
% % \small
% \begin{tabular}{| >{\centering\arraybackslash}m{0.1\textwidth} | >{\raggedright\arraybackslash}m{0.8\textwidth} |}
% \hline
% \rowcolor{lightgray} % 设置第一行背景颜色
% \textbf{Type} & \multicolumn{1}{c|}{\textbf{Template}} \\
% \hline
% 1 & The student has previously, in chronological order, answered question with ID=74 [WrapQEmb]
% involving concept ID=6 [WrapCEmb] correctly, ...,
% question with ID=42 [WrapQEmb] involving concept ID=5 [WrapCEmb] incorrectly.
% Please predict whether the student will answer the next question with ID=44 [NextWrapQEmb]
% involving concept ID=5 [NextWrapCEmb] correctly. 
% Response with ‘Yes’ or ‘No’. \\
% \hline
% 2 & The student has previously, in chronological order, answered question with ID=3117 [QidEmb]
% correctly, question with ID=2964 [QidEmb] correctly, 
% question with ID=5627 [QidEmb] incorrectly, ...,
% question with ID=5532 [QidEmb] correctly.
% Please predict whether the student will answer the next question with ID=5707 [NextQidEmb] correctly.
% Response with ‘Yes’ or ‘No’. \\
% \hline
% 3 & The student has previously, in chronological order, answered question involving concept ID=15 [CidEmb]
% correctly, ...,
% question involving concept ID=30 [NextCidEmb] correctly.
% Please predict whether the student will answer the next question involving concept ID=30 correctly.
% Response with ‘Yes’ or ‘No’. \\
% \hline
% \end{tabular}
% \end{table*}

\section{Experimental Analysis}
\label{sect:Experimental Analysis}
In this section, we conduct extensive experiments to evaluate the effectiveness of our \textbf{\texttt{LLM-KT}}. To be specific, we compare our model with the strong baselines in Section \ref{sect:Main Results}. Then, we explore the performance of the main components in Section \ref{sect:Ablation Studies} and investigate the influence of sequence length in Section \ref{sect:Influence of Sequence Length}. Finally, we analyze the influence of context encoder, sequence encoder, and ensemble function $g$ in Section \ref{sect:Further Analysis}.

\subsection{Main Results}
\label{sect:Main Results}
In this section, we report the results of our \textbf{\texttt{LLM-KT}} and the selected baselines across four benchmark datasets in terms of AUC and ACC (Table \ref{table:main results.}).  
To evaluate the effectiveness of our model, we compare it with four categories: DL-based methods, PLMs-based methods, context-aware methods, and LLMs-based methods.

From these results, we obtain the following observations. 
\textbf{First}, our proposed \textbf{\texttt{LLM-KT}} obtains the best performance in most cases. Particularly, we compare our model with four kinds of baselines, where DL-based methods focus on learning the sequence of IDs, PLMs/LLMs-based methods adopt PLMs/LLMs to improve the performance, and context-based methods integrate the textual information for KT. Our model outperforms all these baselines over three datasets, which indicates that it can integrate other modalities with LLMs effectively for knowledge tracing.
\textbf{Second}, \textbf{\texttt{LLM-KT}} captures the knowledge of the long context and interaction sequence effectively. Our proposed model outperforms the models that are fine-tuned using IDs and the original textual information (e.g., LLM-FT$_\mathrm{ID}$ and LLM-FT$_\mathrm{Text}$). 
\textbf{Third}, LLMs are not good at learning the sequence information or long context directly. For LLM$_\mathrm{TokenID}$, we regard the question ID as a specific token and fine-tune the LLMs to learn the representation of the question. We also input the long textual context of history records into GPT-4o. From the results, we find that they are not good at predicting the student's performance.

% \textbf{Note} 
% To ensure the reliability of the experimental results we only report the results from the original papers or other relevant experimental papers for DL-based methods, PLMs-based methods, and Context-aware methods. 
% Regarding Assist2015 and Junyi, we don't provide the results of LLM-FT-Text and GPT-4o since these two datasets lack question or concept text (See Table \ref{table:statistic of datasets}).
% The number in the upper-right corner of LBKT means two different models: LBKT$^1$ \cite{LBKT} and LBKT$^2$ \cite{lstm_bert}, as in the baseline introduction.


\begin{table}[t!]
\caption{The results of ablation studies.}
\label{table:ablation studies.}
\vspace{-1mm}
\centering
% \small
% \resizebox{.99\columnwidth}{!}{
\begin{tabular}{lcccc}
\hlineB{4}
  & \multicolumn{2}{c}{Assist2009}  & \multicolumn{2}{c}{Nips2020} \\ 
   & AUC & ACC & AUC & ACC \\ \hline
LLM-KT                 & 0.8870 & 0.8168 & 0.8291 & 0.7561 \\ \hline
\multicolumn{5}{l}{\emph{Data Source}}     \\ \hline
- Question             & 0.8788 & 0.7937 & 0.7983 & 0.7439 \\
- Concept              & 0.8635 & 0.8053 & 0.8101 & 0.7276 \\ \hline
\multicolumn{5}{l}{\emph{Model Structure}}     \\ \hline
- Sequence             & 0.8616 & 0.8119 & 0.7958 & 0.7249 \\
- Context              & 0.8788 & 0.7937 & 0.8056 & 0.7358 \\
\hlineB{4}
\end{tabular}
% }
\vspace{-2mm}
\end{table}


\subsection{Ablation Studies}
\label{sect:Ablation Studies}
To investigate the performance of the main components contained in our proposed \texttt{\textbf{LLM-KT}} (Table \ref{table:ablation studies.}). 
From the data source, we remove the ID and text of the question (- Question), and the ID and text of the concepts (- Concept) from our model. From the model structure, we remove the Plug-in Sequence (- Sequence) and Plug-in Context (- Context). 
Due to missing questions or concepts, we mainly conduct the experiments on the Assist2009 and Nips2020.

From the results, we observe that both question and concept information can help the model understand the student's state from the history record to improve the performance of knowledge tracing. For instance, removing questions from inputs will reduce 3.08 points in terms of AUC (0.8291 vs 0.7983). Additionally, our plug-in sequence and plug-in context effectively capture the sequence interaction behaviors and long textual context. Removing any one of them from our \texttt{\textbf{LLM-KT}} will reduce the performance. The textual context helps the model learn the complex semantic relationships between the questions and concepts, and the sequence information helps the model capture the interaction behaviors based on a sequence of IDs.


\begin{figure}[t!]
    \centering
    \begin{minipage}[b]{0.235\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/AUC-Assist2009.pdf}
        %\caption{Figure 1}
        %\label{fig:1}
    \end{minipage}
    % \hfill
    \begin{minipage}[b]{0.235\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/AUC-Assist2015.pdf}
        %\caption{Figure 2}
        %\label{fig:2}
    \end{minipage}
    \\
    \begin{minipage}[b]{0.235\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/AUC-Junyi.pdf}
        %\caption{Figure 3}
        %\label{fig:3}
    \end{minipage}
    % \hfill
    \begin{minipage}[b]{0.235\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/AUC-Nips2020.pdf}
        %\caption{Figure 4}
        %\label{fig:4}
    \end{minipage}
    \vspace{-2mm}
    \caption{Influence of sequence length on four different datasets in terms of AUC.}
    \label{fig:sequence length auc}
    \vspace{-1mm}
\end{figure}


\begin{figure}[t!]
    \centering
    \begin{minipage}[b]{0.235\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/ACC-Assist2009.pdf}
        %\caption{Figure 1}
        %\label{fig:1}
    \end{minipage}
    % \hfill
    \begin{minipage}[b]{0.235\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/ACC-Assist2015.pdf}
        %\caption{Figure 2}
        %\label{fig:2}
    \end{minipage}
    % \hfill
    \\
    \begin{minipage}[b]{0.235\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/ACC-Junyi.pdf}
        %\caption{Figure 3}
        %\label{fig:3}
    \end{minipage}
    % \hfill
    \begin{minipage}[b]{0.235\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/ACC-Nips2020.pdf}
        %\caption{Figure 4}
        %\label{fig:4}
    \end{minipage}
    \vspace{-1mm}
    \caption{Influence of sequence length on four different datasets in terms of ACC.}
    \label{fig:sequence length acc}
    \vspace{-2mm}
\end{figure}


\begin{table*}
\caption{Influence of sequence encoder. Average means the average score over four datasets.}
\label{table:sequence encoder}
\vspace{-1mm}
\centering
% \scriptsize
% \setlength{\tabcolsep}{0.6mm}{
\begin{tabular}{lcccccccccc}
\hlineB{4}
    & \multicolumn{2}{c}{Assist2009} & \multicolumn{2}{c}{Assist2015} & \multicolumn{2}{c}{Junyi} & \multicolumn{2}{c}{Nips2020} & \multicolumn{2}{c}{Average} \\
    & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC      \\
    \hline
LLM-FT$_\text{TokenID}$ & 0.8143 & {0.7954} & 0.8386 & 0.8813 & 0.8663 & 0.8050 & 0.7774 & 0.5962  & 0.8242  & 0.7695\\
DKT & 0.8759 & \textbf{0.8218} & 0.9350  & \textbf{0.9204} & \textbf{0.9027} & \textbf{0.8303} & 0.8149 & 0.7453   & 0.8821 & 0.8295 \\
AKT & \textbf{0.8870} & 0.8168 & \textbf{0.9356}  & 0.9185 & 0.9018 & 0.8294 & \textbf{0.8291} & \textbf{0.7561}   & \textbf{0.8884} & \textbf{0.8302} \\
\hlineB{4}
\end{tabular}
% }
% \vspace{-1mm}
\end{table*}

\subsection{Influence of Sequence Length} 
\label{sect:Influence of Sequence Length}
In this section, we examine how sequence length affects knowledge tracing.
We present our model's results alongside several robust baselines, measured by AUC and ACC (Figure \ref{fig:sequence length auc} and \ref{fig:sequence length acc}). 
We define the sequence lengths as 20, 50, or 100.


From the results, we find that our model outperforms the other methods across all sequence lengths, which indicates that \texttt{\textbf{LLM-KT}} can capture the students' states with long text and ID sequences effectively. Specifically, our model improves by more than 4 points of AUC compared to DKT and AKT on Nips2020.
With the increase in sequence length, our model can effectively improve performance in most cases, while the improvements in previous studies are limited and may even decline. For example, \texttt{\textbf{LLM-KT}} with a sequence length of 100 outperforms the one with 20 questions by about 10 points in terms of AUC over Assist2015.
The Junyi dataset primarily focuses on short-term records, and extending the sequence length may reduce performance due to unrelated questions in the history record.
Though our model experiences a performance drop when increasing the length, the decrease is subtle. 
For DKT, it is an LSTM-based model that is not effective at capturing long sequences. 
We would like to explore how to reduce the influence of unrelated information in lengthy sequences in the future.

\begin{table}[t!]
\caption{Influence of context encoder.}
\label{table:context encoder}
\vspace{-1mm}
\centering
% \small
\setlength{\tabcolsep}{1.3mm}{
\begin{tabular}{ccccccc}
\hlineB{4}
\multicolumn{1}{l}{} & \multicolumn{2}{c}{Assist2009} & \multicolumn{2}{c}{Nips2020} & \multicolumn{2}{c}{Average} \\ \hline
\multicolumn{1}{l}{} & AUC & ACC & AUC & ACC  & AUC & ACC       \\ \hline
BERT         & 0.8770         & 0.7987        & 0.8116      & 0.7331    & 0.8443  &  0.7659 \\ 
MPNET       & 0.8749         & 0.8135        & 0.8231      & \textbf{0.7561}   & 0.8490 & 0.7848 \\ 
LLaMA2       & \textbf{0.8870}         & \textbf{0.8168}        & \textbf{0.8291}      & \textbf{0.7561}  & \textbf{0.8581} & \textbf{0.7865} \\ \hlineB{4}
\end{tabular}}
\vspace{-1mm}
\end{table}



\subsection{Further Analysis}
\label{sect:Further Analysis}
\subsubsection{Influence of Context Encoder}
We investigate the influence of different context encoders and provide their average performance for both AUC and ACC (Table \ref{table:context encoder}). 
For the context encoder, we select three typical sentence encoders: LLaMA2\footnote{\url{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf}}, BERT\footnote{\url{https://huggingface.co/google-bert/bert-base-uncased}}, and all-mpnet-base-v2 (MPNET)\footnote{\url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}}. 
We input the entire question and concepts into the Context-Encoder to generate a vector $r_{QText}$ and $r_{CText}$. Consequently, we derive a single vector for each question and all associated concepts, which remains unaffected by increases in question length or the number of concepts. Specifically, the vectors are derived by averaging the hidden states from the last layer along the sequence length for the BERT and LLaMA2 models, respectively. For the MPNET model, we calculate the the vector using SentenceTransformer. The dimensions $d^t$ are 768, 768, and 4096 for BERT, MPNET, and the LLaMA2-based Context-Encoder, respectively. We develop a Context-Adapter to convert $d^t$ into the embedding layer dimension $d^e$ of LLaMA2 (e.g., 4096).


Among the models, the LLaMA2-enhanced model achieves the highest average AUC (0.8581) and ACC (0.7865), demonstrating its superiority in capturing long context and making accurate predictions. 
In contrast, the corresponding model with BERT shows the lowest overall performance (AUC: 0.8443, ACC: 0.7659), particularly struggling with the Nips2020 dataset where it achieves the lowest AUC (0.8116) and ACC (0.7331). 
Particularly, LLaMA2 achieves a 2-point increase in ACC on average when compared to BERT (0.7865 vs 0.7659).
By training on a large-scale corpus with huge parameters, LLaMA2 learns powerful text embeddings, which help LLMs acquire the long textual knowledge of the questions. 
For MPNET, it is fine-tuned on 1B sentence pairs using a contrastive learning objective to improve the sentence representation. 
Based on the experiments, we recommend using MPNET, which is the same size as BERT and achieves competitive performance to LLaMA2.


\subsubsection{Influence of Sequence Encoder}
We investigate the influence of different sequence encoders and compare them with LLM-FT$_\text{TokenID}$ over four datasets (Table \ref{table:sequence encoder}).  
Particularly, we employ two sequence encoders: DKT \cite{DKT} and AKT \cite{AKT} to model the sequence of IDs.  
For LLM-FT$_\text{TokenID}$, it initiates the embedding of the question- and concept-specific token randomly and updates the embeddings by fine-tuning on the training dataset.  
Unlike LLM-FT$_\text{TokenID}$, we use representations of question IDs and concept IDs learned by DKT and AKT to initiate the tokens' embeddings.  

The findings indicate that models utilizing DKT and AKT perform better than LLM-FT$_\text{TokenID}$, showcasing their robust ability to model knowledge tracing tasks effectively. 
The AKT-based model achieves over a 15-point enhancement in terms of ACC over Nips2020 when compared to LLM-FT$_\text{TokenID}$. 
It suggests that the embeddings derived from traditional sequence models capture extensive semantic and interaction knowledge. 
Also, LLMs cannot capture the knowledge well by simply fine-tuning on sequence of IDs.
Additionally, DKT and AKT obtain comparable results over these four datasets (0.8295 vs 0.8302 for average ACC).



\begin{table}[t!]
\caption{Influence of function $g$.}
\label{table:merge methods}
\vspace{-1mm}
\centering
% \small
\setlength{\tabcolsep}{1.3mm}{
\begin{tabular}{clcccccc}
\hlineB{4}
\multicolumn{2}{c}{}                      & \multicolumn{2}{c}{Assist2009} & \multicolumn{2}{c}{Nips2020}  & \multicolumn{2}{c}{Average} \\
\multicolumn{2}{c}{}                      & AUC & ACC & AUC & ACC  & AUC & ACC      \\ \hline
\multicolumn{2}{c}{Concat}    & 0.8851         & 0.8102        & 0.8168      & 0.7466   & 0.8510 & 0.7784  \\ 
\multicolumn{2}{c}{Avg}       & 0.8763         & \textbf{0.8185}        & 0.8262      & 0.7466    & 0.8513 &  0.7826  \\ 
\multicolumn{2}{c}{Add}       & \textbf{0.8870}         & 0.8168        & \textbf{0.8291}      & \textbf{0.7561}   &  \textbf{0.8581} &   \textbf{0.7865} \\ \hlineB{4}
\end{tabular}}
\vspace{-1mm}
\end{table}


\subsubsection{Influence of Function $g$}
To combine the representations of context and sequence, we use a function $g$ to merge them, as mentioned in Equation \ref{equ:combine}. 
In our experiment, we explore the influence of different methods on two datasets, Assist2009 and Nips2020, using AUC and ACC as performance metrics (Table \ref{table:merge methods}).
To be specific, we evaluate the performance of three strategies, including concatenation (Concat), average (Avg), and addition (Add). 
For the Assist2009 dataset, the Add strategy achieves the highest AUC (0.8870) and ties for the best ACC (0.8168) with the Concat strategy. In contrast, for the Nips2020 dataset, the Add strategy again outperforms others in both AUC (0.8291) and ACC (0.7561). When averaging the performance across both datasets, the Add strategy consistently demonstrates superior results, achieving the highest overall AUC (0.8581) and ACC (0.7865). These findings indicate that the Add strategy is the most effective method for combining features across the evaluated scenarios. 



\section{Conclusions and Further Work}
\label{sec:Conclusions and Further Work}
In this paper, we propose a large language models-based framework for knowledge tracing (\texttt{\textbf{LLM-KT}}), leveraging the Plug-and-Play Instruction to elegantly translate the sequential tracing task into a language modeling problem. This approach incorporates multiple modalities, such as the textual information of the questions and the interaction behavior information from traditional sequence models, into the large model. Extensive experiments demonstrate that our method achieves superior performance, surpassing previous state-of-the-art results. Ablation studies confirm the effectiveness of injecting both textual information and sequence behavior information. Additionally, we analyze the impact of sequence length and the roles of the context or sequence encoders.
In the future, we would like to explore alternative integration methods and alignment strategies. It is also interesting to design a more effective algorithm to capture the long-term sequence and reduce the impact of unrelated information.

% \clearpage
% \newpage



\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\input{Appendix}
\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
