\section{Related Work}
\label{sect:related work}
% ``\verb|acmart|'' {\itshape template style} \url{https://www.acm.org/publications/proceedings-template}
\subsection{Deep learning-based Knowledge Tracing}
Traditional knowledge tracking algorithms are mainly based on machine learning algorithms, such as Bayesian Knowledge Tracing (BKT) \cite{BKT} and Item Response Theory (IRT) \cite{IRT}. 
With the continuous development and progress of neural networks, deep learning-based knowledge tracing algorithms have emerged to model the sequence interaction \cite{DKT,MRT-KT,LPKT,AdaptKT}. 
DKT \cite{DKT}, or Deep Knowledge Tracing, is the first model to apply deep learning to the field of knowledge tracing, which learns the features of students' historical problem-solving records using Long Short-Term Memory (LSTM). 
% SAKT \cite{SAKT} and AKT \cite{AKT} employ a transformer framework to learn question-answer interaction sequences using an advanced attention mechanism. 
SAKT \cite{SAKT} utilized the self-attention mechanism to address the problem of insufficient generalization ability existing in the processing of sparse data. 
AKT \cite{AKT} further introduced a new monotonic attention mechanism and the classic Rasch-model in psychometrics to better understand students' knowledge mastery status and learning processes.
BEKT \cite{BEKT} proposed a multi-layer bidirectional transformer encoder with a self-attention mechanism and bidirectional analysis, to understand the student's past learning logs.
\citet{DKVMN} proposed a new structure called Dynamic Key-Value Memory Networks (DKVMN), which can utilize the relationships between underlying concepts and directly output the mastery level of each concept by students.  

% To further consider the time factor, DKT-Forget \cite{DKT+Forget} and HawkesKT \cite{HawkesKT} incorporated the time feature into the model.
To further evaluate the time aspect, DKT-Forget \cite{DKT+Forget} enhances DKT by translating the time interval into a numerical value. This value, along with learning interaction data like answering questions, is fed into the neural network. In contrast, HawkesKT \cite{HawkesKT} leverages the intensity function and mechanisms of the Hawkes process to measure the triggering effects of events across different time points. This approach clarifies how learning events temporally influence the probability of subsequent occurrences and the knowledge state.
Addressing limitations in the learning process, which is vital for KT tasks, LPKT \cite{LPKT} assesses students’ knowledge states by modeling their learning journey, capturing knowledge gains while also considering the phenomenon of forgetting. Simultaneously, \citet{ProKT} offers a novel perspective in the KT field by developing the Progressive Knowledge Tracing model. This model emphasizes the learning journey through students’ sequential thought processes and divides it into three relatively independent, yet progressively advanced stages: concept mastery, question-solving, and answering behavior, effectively modeling the transition from abstract reasoning to concrete responses.

Furthermore, graph neural networks are used to model the relationships between different questions or knowledge points in the field of knowledge tracing \cite{GKT,Bi-CLKT,PEBG,DGEKT}. 
GKT \cite{GKT} constructs a knowledge graph based on knowledge points or questions, and utilizes Graph Neural Networks (GNNs) to explore and take advantage of these underlying relational structures. 
% For example, BI-CLKT \cite{Bi-CLKT} designs a two-layer contrastive learning scheme based on the ``exercise-to-exercise" relation subgraph and  
% % , enabling it to effectively establish the spatial association and complex structure of nodes
% PEBG \cite{PEBG} represents the question-skill relationship as a bipartite graph to learn pre-trained question embeddings. 
% BI-CLKT \cite{Bi-CLKT} designs a two-layer contrastive learning scheme based on the ``exercise-to-exercise" relation subgraph and 
BI-CLKT \cite{Bi-CLKT} designs a two-layer comparative learning scheme on an ``exercise-to-exercise" (E2E) relational subgraph for node-level and graph-level contrastive learning to get discriminative representations of exercises and concepts. Additionally, two variants with different prediction layers (RNN and memory-augmented neural networks) are explored to improve representations.
PEBG \cite{PEBG} puts forward a pre-training embedding method through a bipartite graph (PEBG), leveraging edge information (including question difficulty, explicit question-skill relationships, implicit question similarity, and skill similarity) to learn low-dimensional embeddings for each question. 
DGEKT \cite{DGEKT} innovatively constructs a dual graph structure of students' learning interactions, using a concept association hypergraph and a directed transition graph to capture heterogeneous relationships. Additionally, it employs online knowledge distillation to adaptively combine the dual graph models, forming a stronger ensemble teacher model for enhanced modeling ability.



\begin{figure*}[!t]
% \vspace{-5mm}
\begin{center}
\includegraphics[width=1\textwidth]{imgs/LLM-KT-new.pdf}
\end{center}
% \vspace{-1mm}
\caption{The framework of \texttt{\textbf{LLM-KT}}. We propose a Plug-and-Play Instruction to combine the strengths of LLMs and traditional sequence models for knowledge tracing by inserting multiple modalities into LLMs. Particularly, we design a Plug-in Context module to capture the long context of students' problem-solving records. Then, we introduce the Plug-in Sequence to align the sequence interaction representation learned by the traditional model with LLMs.} 
\label{fig:main}
% \vspace{-2mm}
\end{figure*}



\subsection{PLMs-Enhanced Knowledge Tracing}
% BERT~\cite{bert} is renowned for its proficiency in generating high-quality embeddings, which play a vital role in numerous natural language processing tasks. A multitude of BERT variants have been applied in diverse deep learning fields, showcasing their remarkable performances. For instance, ConvBERT extends the application of the original BERT architecture to the image processing domain; BERT4Rec utilizes the BERT model to enhance recommendation systems, LakhNES employs the BERT model to augment Music Generation.
In the field of knowledge tracing, Pre-trained Language Models (PLMs), such as BERT \cite{bert}, RoBERT \cite{RoBERT}, are used to enhance the semantic representation for knowledge tracing \cite{BiDKT,MLFBK,tong2020exercise,DCL4KT-A}. 
For instance, BiDKT \cite{BiDKT} adapts BERT to trace knowledge by predicting the correctness of randomly masked responses within sequences.
MLFBK \cite{MLFBK} leverages the power of BERT to mine latent relations among multiple explicit features, such as individual skill mastery, students' ability profiles, and problem difficulty.
%Two text classification systems, named KDES and
% DFES, are designed to predict the knowledge distribution and difficulty of the
% exercise respectively. The semantic feature extractor system(SFES) could be
% considered as an unsuperviesed clusering problems.
Furthermore, \citet{tong2020exercise} proposes a hierarchical exercise feature enhanced knowledge tracing framework that utilizes BERT to generate exercise text embeddings and then feeds them into three systems (KDES, DFES, and SFES) to extract knowledge distribution, semantic features, and difficulty features. These hierarchical features are concatenated with student responses and input into a sequence model, aiming to improve knowledge tracing by comprehensively considering the diverse attributes of exercises.
Moreover, LBKT \cite{lstm_bert} combines the strengths of BERT for capturing complex data relations and LSTM for handling long sequences, enhancing performance on data with over 400 interactions. 
However, the integration of LLMs with knowledge tracing has not been explored well.


\subsection{Context-aware Knowledge Tracing}
The context information (such as textual features in the questions and concepts) contains a wealth of semantic knowledge, which can help reduce the cold-start phenomenon of knowledge tracing. 
Several studies utilized the context information to enhance traditional deep learning models \cite{RKT,EERNN}. 
For example, RKT \cite{RKT} used the textual information of the questions to capture relations between exercises. 
EERNN \cite{EERNN} and EKT \cite{EKT} considered the text of the questions to learn a good question representation for knowledge tracing. 
Additionally, \citet{PEBG} proposed a pre-training method called PEBG, which learns question embeddings with rich relational information using the bipartite graph of question-skill relations. 
Moreover, \citet{DCL4KT-A} proposed a difficulty-centered contrastive learning method based on the question representations using BERT. 

Unlike previous research, we propose \texttt{\textbf{LLM-KT}} to combine the great advantages of LLMs and traditional sequence learning models for knowledge tracing. 
We design a plug-and-play instruction to align the context and sequence representations with LLMs.

% Relation-aware self-attention for knowledge tracing
% (RKT) and hierarchical graph knowledge tracing
% (HGKT) also extract features from the textual information of questions to learn question representations in their models. 

% Adaptable knowledge tracing (AdaptKT), which transfers knowledge from the source domain to the target one, and 
% In Exercise Hierarchical Feature Enhanced Knowledge Tracing utilize Bert has been proposed \cite{AdaptKT,tong2020exercise}. 

% Yet, previous research has not considered much
% about the latent representation of the textual features in the questions and concepts. Text-aware
% KT models are motivated by leveraging the textual
% features of questions and concepts to enhance performance in tackling KT tasks.