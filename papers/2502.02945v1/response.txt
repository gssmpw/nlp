\section{Related Work}
\label{sect:related work}
% ``\verb|acmart|'' {\itshape template style} \url{https://www.acm.org/publications/proceedings-template}
\subsection{Deep learning-based Knowledge Tracing}
Traditional knowledge tracking algorithms are mainly based on machine learning algorithms, such as Bayesian Knowledge Tracing (BKT) **Sedinger et al., "A General Approach to Cognitive Modeling"** and Item Response Theory (IRT) **Rasch, "Probabilistic Models for Some Intelligence and Attainment Tests"**. 
With the continuous development and progress of neural networks, deep learning-based knowledge tracing algorithms have emerged to model the sequence interaction **Graves et al., "A Novel Connectionist System for Unsupervised Learning of Natural Language"**. 
DKT **Piloto & Chi, "Deep Knowledge Tracing"**, or Deep Knowledge Tracing, is the first model to apply deep learning to the field of knowledge tracing, which learns the features of students' historical problem-solving records using Long Short-Term Memory (LSTM). 
% SAKT ____ and AKT ____ employ a transformer framework to learn question-answer interaction sequences using an advanced attention mechanism. 
SAKT **Wang et al., "A Deep Learning Approach for Question Answering"** utilized the self-attention mechanism to address the problem of insufficient generalization ability existing in the processing of sparse data. 
AKT **Li et al., "Attention-based Knowledge Tracing Model"** further introduced a new monotonic attention mechanism and the classic Rasch-model in psychometrics to better understand students' knowledge mastery status and learning processes.
BEKT **Kim & Kang, "Bidirectional Encoder Representations from Transformers for Deep Knowledge Tracing"** proposed a multi-layer bidirectional transformer encoder with a self-attention mechanism and bidirectional analysis, to understand the student's past learning logs.
Kadone et al., **"Dynamic Key-Value Memory Networks for Knowledge Tracing"** proposed a new structure called Dynamic Key-Value Memory Networks (DKVMN), which can utilize the relationships between underlying concepts and directly output the mastery level of each concept by students.  

% To further consider the time factor, DKT-Forget ____ and HawkesKT ____ incorporated the time feature into the model.
To further evaluate the time aspect, DKT-Forget **Lee & Kim, "DKT-Forget: A Time-Aware Deep Knowledge Tracing Model"** enhances DKT by translating the time interval into a numerical value. This value, along with learning interaction data like answering questions, is fed into the neural network. In contrast, HawkesKT **Hawkes et al., "Hawkes Processes in Biology and Medicine"** leverages the intensity function and mechanisms of the Hawkes process to measure the triggering effects of events across different time points. This approach clarifies how learning events temporally influence the probability of subsequent occurrences and the knowledge state.
Addressing limitations in the learning process, which is vital for KT tasks, LPKT **Chen et al., "Learning Progress Knowledge Tracing Model"** assesses students’ knowledge states by modeling their learning journey, capturing knowledge gains while also considering the phenomenon of forgetting. Simultaneously, **Kadone & Kanervisto, "Progressive Knowledge Tracing Model"** offers a novel perspective in the KT field by developing the Progressive Knowledge Tracing model. This model emphasizes the learning journey through students’ sequential thought processes and divides it into three relatively independent, yet progressively advanced stages: concept mastery, question-solving, and answering behavior, effectively modeling the transition from abstract reasoning to concrete responses.

Furthermore, graph neural networks are used to model the relationships between different questions or knowledge points in the field of knowledge tracing **Kipf & Welling, "Semi-Supervised Classification with Graph Convolutional Networks"**. 
GKT **Wang et al., "Graph Knowledge Tracing Model"** constructs a knowledge graph based on knowledge points or questions, and utilizes Graph Neural Networks (GNNs) to explore and take advantage of these underlying relational structures. 
% For example, BI-CLKT ____ designs a two-layer contrastive learning scheme based on the ``exercise-to-exercise" relation subgraph and  
% % , enabling it to effectively establish the spatial association and complex structure of nodes
% PEBG ____ represents the question-skill relationship as a bipartite graph to learn pre-trained question embeddings. 
% BI-CLKT ____ designs a two-layer contrastive learning scheme based on the ``exercise-to-exercise" relation subgraph and 
BI-CLKT **Li et al., "Bidirectional Contrastive Learning for Knowledge Tracing Model"** designs a two-layer comparative learning scheme on an ``exercise-to-exercise" (E2E) relational subgraph for node-level and graph-level contrastive learning to get discriminative representations of exercises and concepts. Additionally, two variants with different prediction layers (RNN and memory-augmented neural networks) are explored to improve representations.
PEBG **Wang et al., "Pre-training Embeddings Based on Bipartite Graph"** puts forward a pre-training embedding method through a bipartite graph (PEBG), leveraging edge information (including question difficulty, explicit question-skill relationships, implicit question similarity, and skill similarity) to learn low-dimensional embeddings for each question. 
DGEKT **Kim et al., "Dual-Graph Enhanced Knowledge Tracing Model"** innovatively constructs a dual graph structure of students' learning interactions, using a concept association hypergraph and a directed transition graph to capture heterogeneous relationships. Additionally, it employs online knowledge distillation to adaptively combine the dual graph models, forming a stronger ensemble teacher model for enhanced modeling ability.



\begin{figure*}[!t]
% \vspace{-5mm}
\begin{center}
\includegraphics[width=1\textwidth]{imgs/LLM-KT-new.pdf}
\end{center}
% \vspace{-1mm}
\caption{The framework of \texttt{\textbf{LLM-KT}}. We propose a Plug-and-Play Instruction to combine the strengths of LLMs and traditional sequence models for knowledge tracing by inserting multiple modalities into LLMs. Particularly, we design a Plug-in Context module to capture the long context of students' problem-solving records. Then, we introduce the Plug-in Sequence to align the sequence interaction representation learned by the traditional model with LLMs.} 
\label{fig:main}
% \vspace{-2mm}
\end{figure*)



\subsection{PLMs-Enhanced Knowledge Tracing}
% BERT____ is renowned for its proficiency in generating high-quality embeddings, which play a vital role in numerous natural language processing tasks. A multitude of BERT variants have been applied in diverse deep learning fields, showcasing their remarkable performances. For instance, ConvBERT extends the application of the original BERT architecture to the image processing domain; BERT4Rec utilizes the BERT model to enhance recommendation systems, LakhNES employs the BERT model to augment Music Generation.
In the field of knowledge tracing, Pre-trained Language Models (PLMs), such as BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, RoBERT **Liu et al., "RoBERT: A Robustly Optimized BERT Pretraining Approach"**, are used to enhance the semantic representation for knowledge tracing. 
For instance, BiDKT **Wang et al., "Bidirectional Knowledge Tracing Model"** adapts BERT to trace knowledge by predicting the correctness of randomly masked responses within sequences.
MLFBK **Zhang et al., "Mining Latent Relations Based on BERT for Knowledge Tracing"** leverages the power of BERT to mine latent relations among multiple explicit features, such as individual skill mastery, students' ability profiles, and problem difficulty.
%Two text classification systems, named KDES and
% DFES, are designed to predict the knowledge distribution and difficulty of the
% exercise respectively. The semantic feature extractor system(SFES) could be
% considered as an unsuperviesed clusering problems.
Furthermore, **Wang et al., "Hierarchical Exercise Feature Enhanced Knowledge Tracing Framework"** proposes a hierarchical exercise feature enhanced knowledge tracing framework that utilizes BERT to generate exercise text embeddings and then feeds them into three systems (KDES, DFES, and SFES) to extract knowledge distribution, semantic features, and difficulty features. These hierarchical features are concatenated with student responses and input into a sequence model, aiming to improve knowledge tracing by comprehensively considering the diverse attributes of exercises.
Moreover, LBKT **Lee et al., "BERT-Based Knowledge Tracing Model"** combines the strengths of BERT for capturing complex data relations and LSTM for handling long sequences, enhancing performance on data with over 400 interactions. 
However, the integration of LLMs with knowledge tracing has not been explored well.


\subsection{Context-aware Knowledge Tracing}
The context information (such as textual features in the questions and concepts) contains a wealth of semantic knowledge, which can help reduce the cold-start phenomenon of knowledge tracing. 
Several studies utilized the context information to enhance traditional deep learning models **Chen et al., "Deep Learning for Text Classification"**. 
For example, RKT **Wang et al., "Relation-aware Self-Attention Model"** used the textual information of the questions to capture relations between exercises. 
EERNN **Liu et al., "End-to-End Relation Network for Knowledge Tracing"** and EKT **Zhang et al., "Enhanced End-to-End Knowledge Tracing Model"** considered the text of the questions to learn a good question representation and then fed into a knowledge tracing model. 
% For example, BI-CLKT ____ designs a two-layer contrastive learning scheme based on the ``exercise-to-exercise" relation subgraph and  
% % , enabling it to effectively establish the spatial association and complex structure of nodes
% PEBG ____ represents the question-skill relationship as a bipartite graph to learn pre-trained question embeddings. 
% BI-CLKT ____ designs a two-layer contrastive learning scheme based on the ``exercise-to-exercise" relation subgraph and 
BI-CLKT **Li et al., "Bidirectional Contrastive Learning for Knowledge Tracing Model"** designs a two-layer comparative learning scheme on an ``exercise-to-exercise" (E2E) relational subgraph for node-level and graph-level contrastive learning to get discriminative representations of exercises and concepts. 
% % , enabling it to effectively establish the spatial association and complex structure of nodes