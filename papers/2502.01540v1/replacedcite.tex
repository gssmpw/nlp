\section{Related Work}
\subsection{Numbers and Language Models}
The processing and representation of numbers in large language models is a topic of active research. Number representations have recently been studied within pretrained language models using techniques from mechanistic interpretability. For example, ____ devised a dataset of addition problems to show that LLMs encode the value of numbers linearly in that context. Likewise, circuit analysis has been adopted to characterize how smaller language models compute ``greater than''____ and basic arithmetic operations ____. Other work studied how simple models solve modular arithmetic tasks____.  
More behavioral approaches have also been explored. ____ looked at how model generations of numbers can be used to test for answer calibration. Likewise, ____ (____) showed how a Bayesian framework can be used to characterize how LLM performance on various tasks, including numeric ones, depends on the probability of the input, task, and output.

\subsection{Probing and Representation Analysis in LLMs}
Probing is a well-established technique for studying the internal representations of machine learning models____. Dating back to BERT____, probing has been used to explore various aspects of representation such as how language models encode sentence structure____. More recently, probing has become a common technique in studying larger language models. In the work by ____ mentioned earlier, the authors designed linear probes to extract number value from latent embeddings. Probes have also been used to discover how language models process concepts across layers____, and for extracting uncertainty in language model generations____. 
Similarly, probes, and latent analysis more broadly, have been used in the world models literature, e.g., to show that language models trained on Othello games develop a world model of the game____, and that they learn to represent space and time____.
More recent approaches to studying representations in language models have also been proposed like activation patching____, sparse autoencoders____, and distributed alignment search____. 
% tasks____, as well as multilingual concepts ____.


% \subsection{Representations in LLMs}
% In this paper, we argue that numbers are represented both as both string and integer types within a model, and this leads to confusion during decoding time on the similarity task. This closely relates to the language model representation literature. 
% Past work has explored how time____, multilingual concepts____, tasks____, space____, and numbers____ are represented in language models. 

% [todo: ilia if you have some additional connections, otherwise i suggest we cull this part.]

% % - multilingual setting [CITE Camal, and knife paper]

% % - latent vocabulary (FROM TOKENS TO WORDS: ON THE INNER LEXICON  OF LLMS). semantic hub. llama's think in english. 

% % \subsection{Understanding LLMs with Cognitive Science}

\subsection{Behavioral Analysis of Language Models}
By combining the prompt comprehension capabilities of large language models with the wide range of paradigms available in the behavioral sciences, a growing line of work proposes new tools for characterizing behavior in LLMs. For example, ____ used a suite of perceptual judgment tasks from psychophysics to characterize sensory knowledge in LLMs. ____ leveraged paradigms from cognitive psychology to study how GPT-3 solves various tasks. ____ used tools from social psychology to diagnose implicit racial and gender bias in LLMs. ____ subjected LLMs to analogical tasks to study their abstract reasoning capacities. Our work takes a similar approach to this line of work, and combines it with modern probing techniques to study LLM behavior and representation in the domain of number.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/int_str_var_alternative_linlog.pdf}
    \caption{Context effects on LLM-elicited number similarity matrices and their decomposition. \textbf{A}. LLM similarity matrices under the effect of `type' specification: \texttt{int()} vs. \texttt{str()} (see Appendix \ref{app:prompts} for prompts). \textbf{B}. Coefficient of determination ($R^2$) for the different similarity matrices under the default (Figure~\ref{fig:default-sim}), \texttt{int()}, and \texttt{str()} contexts for the combined and separate Levenshtein (string) and Log-Linear (numerical) distance predictors (error bars indicate 95\% confidence intervals; see Methodology).}
    \label{fig:int-str-sim}
\end{figure*}