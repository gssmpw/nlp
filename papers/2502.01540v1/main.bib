@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@article{mcculloch1961number,
  title={What is a number, that a man may know it, and a man, that he may know a number},
  author={McCulloch, Warren S},
  journal={General Semantics Bulletin},
  volume={26},
  number={27},
  pages={7--18},
  year={1961},
  publisher={Institute of General Semantics Lakeville}
}

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  pages={115--133},
  year={1943},
  publisher={Springer}
}


%%%%%%%%%%%%%%%%%%%% related work %%%%%%%%%%%%%%%%%%%%
@article{nylund2023time,
  title={Time is Encoded in the Weights of Finetuned Language Models},
  author={Nylund, Kai and Gururangan, Suchin and Smith, Noah A},
  journal={arXiv preprint arXiv:2312.13401},
  year={2023}
}

%%%% NUMBERS %%%%
@article{tian2023just,
  title={Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback},
  author={Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2305.14975},
  year={2023}
}
@inproceedings{zhu-etal-2025-language,
    title = "Language Models Encode the Value of Numbers Linearly",
    author = "Zhu, Fangwei  and
      Dai, Damai  and
      Sui, Zhifang",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    year = "2025",
    pages = "693--709",
    abstract = "Large language models (LLMs) have exhibited impressive competence in various tasks, but their internal mechanisms on mathematical problems are still under-explored. In this paper, we study a fundamental question: how language models encode the value of numbers, a basic element in math. To study the question, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of encoded number values in LLMs on different layers, and these values can be extracted via linear probes. Further experiments show that LLMs store their calculation results in a similar manner, and we can intervene the output via simple vector additions, proving the causal connection between encoded numbers and language model outputs. Our research provides evidence that LLMs encode the value of numbers linearly, offering insights for better exploring, designing, and utilizing numeric information in LLMs."
}
@inproceedings{
  hanna2023how,
  title={How does {GPT}-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model},
  author={Michael Hanna and Ollie Liu and Alexandre Variengien},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
}
@misc{stolfo2023mechanisticinterpretationarithmeticreasoning2,
      title={A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis}, 
      author={Alessandro Stolfo and Yonatan Belinkov and Mrinmaya Sachan},
      year={2023},
      eprint={2305.15054},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{stolfo2023mechanisticinterpretationarithmeticreasoning,
  title={A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis},
  author={Alessandro Stolfo and Yonatan Belinkov and Mrinmaya Sachan},
  journal={arXiv preprint arXiv:2305.15054},
  year={2023}
}

@misc{nanda2023progressmeasuresgrokkingmechanistic2,
      title={Progress measures for grokking via mechanistic interpretability}, 
      author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
      year={2023},
      eprint={2301.05217},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}
@article{nanda2023progressmeasuresgrokkingmechanistic,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}
@misc{mccoy2024languagemodeloptimizedreasoning2,
      title={When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1}, 
      author={R. Thomas McCoy and Shunyu Yao and Dan Friedman and Mathew D. Hardy and Thomas L. Griffiths},
      year={2024},
      eprint={2410.01792},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{mccoy2024languagemodeloptimizedreasoning,
  title={When a language model is optimized for reasoning, does it still show embers of autoregression? {A}n analysis of {O}pen{AI} o1},
  author={R. Thomas McCoy and Shunyu Yao and Dan Friedman and Mathew D. Hardy and Thomas L. Griffiths},
  journal={arXiv preprint arXiv:2410.01792},
  year={2024}
}


@article{mccoy2023embersautoregressionunderstandinglarge,
  title={Embers of autoregression show how large language models are shaped by the problem they are trained to solve},
  author={McCoy, R Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Mathew D and Griffiths, Thomas L},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={41},
  pages={e2322420121},
  year={2024},
  publisher={National Academy of Sciences}
}
%%%% PROBING %%%%
@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}
@article{manning2020emergent,
  title={Emergent linguistic structure in artificial neural networks trained by self-supervision},
  author={Manning, Christopher D and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30046--30054},
  year={2020},
  publisher={National Acad Sciences}
}
@article{tenney2019you,
  title={What do you learn from context? probing for sentence structure in contextualized word representations},
  author={Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R and Das, Dipanjan and others},
  journal={arXiv preprint arXiv:1905.06316},
  year={2019}
}
@inproceedings{nanda-etal-2023-emergent,
    title = "Emergent Linear Representations in World Models of Self-Supervised Sequence Models",
    author = "Nanda, Neel  and
      Lee, Andrew  and
      Wattenberg, Martin",
    editor = "Belinkov, Yonatan  and
      Hao, Sophie  and
      Jumelet, Jaap  and
      Kim, Najoung  and
      McCarthy, Arya  and
      Mohebbi, Hosein",
    booktitle = "Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",
    year = "2023",
    doi = "10.18653/v1/2023.blackboxnlp-1.2",
    pages = "16--30",
    abstract = "How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023a). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for {\textquotedblleft}my colour{\textquotedblright} vs. {\textquotedblleft}opponent`s colour{\textquotedblright} may be a simple yet powerful way to interpret the model`s internal state. This precise understanding of the internal representations allows us to control the model`s behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed."
}
@misc{gurnee2024languagemodelsrepresentspace2,
      title={Language Models Represent Space and Time}, 
      author={Wes Gurnee and Max Tegmark},
      year={2024},
      eprint={2310.02207},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{gurnee2024languagemodelsrepresentspace,
  title={Language Models Represent Space and Time},
  author={Wes Gurnee and Max Tegmark},
  journal={arXiv preprint arXiv:2310.02207},
  year={2024}
}

@article{gurnee2023finding,
  title={Finding neurons in a haystack: Case studies with sparse probing},
  author={Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  journal={arXiv preprint arXiv:2305.01610},
  year={2023}
}
@misc{devlin2019bertpretrainingdeepbidirectional2,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{devlin2019bertpretrainingdeepbidirectional,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={arXiv preprint arXiv:1810.04805},
  year={2019}
}

@article{bertology,
    author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
    title = {A Primer in BERTology: What We Know About How BERT Works},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {842-866},
    year = {2021},
    abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00349/1923281/tacl\_a\_00349.pdf},
}
%%%% REPRESENTATIONS %%%%
@misc{dumas2025separatingtonguethoughtactivation,
      title={Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers}, 
      author={Clément Dumas and Chris Wendler and Veniamin Veselovsky and Giovanni Monea and Robert West},
      year={2025},
      eprint={2411.08745},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}
@article{ilharco2022editing,
  title={Editing models with task arithmetic},
  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  journal={arXiv preprint arXiv:2212.04089},
  year={2022}
}

@article{levenshtein1966binary,
  title={Binary codes capable of correcting deletions, insertions, and reversals},
  author={Levenshtein, VI},
  journal={Proceedings of the Soviet physics doklady},
  year={1966}
}

@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@article{marjieh2024large,
  title={Large language models predict human sensory judgments across six modalities},
  author={Marjieh, Raja and Sucholutsky, Ilia and van Rijn, Pol and Jacoby, Nori and Griffiths, Thomas L},
  journal={Scientific Reports},
  volume={14},
  number={1},
  pages={21445},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{binz2023using,
  title={Using cognitive psychology to understand {GPT}-3},
  author={Binz, Marcel and Schulz, Eric},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={6},
  pages={e2218523120},
  year={2023},
  publisher={National Acad Sciences}
}

@article{bai2024measuring,
  title={Measuring implicit bias in explicitly unbiased large language models},
  author={Bai, Xuechunzi and Wang, Angelina and Sucholutsky, Ilia and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2402.04105},
  year={2024}
}

@article{webb2023emergent,
  title={Emergent analogical reasoning in large language models},
  author={Webb, Taylor and Holyoak, Keith J and Lu, Hongjing},
  journal={Nature Human Behaviour},
  volume={7},
  number={9},
  pages={1526--1541},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@book{dehaene2011number,
  title={The number sense: How the mind creates mathematics},
  author={Dehaene, Stanislas},
  year={2011},
  publisher={Oxford University Press}
}

@article{nieder2009representation,
  title={Representation of number in the brain},
  author={Nieder, Andreas and Dehaene, Stanislas},
  journal={Annual review of neuroscience},
  volume={32},
  number={1},
  pages={185--208},
  year={2009},
  publisher={Annual Reviews}
}

@article{piantadosi2014children,
  title={Children's learning of number words in an indigenous farming-foraging group},
  author={Piantadosi, Steven T and Jara-Ettinger, Julian and Gibson, Edward},
  journal={Developmental science},
  volume={17},
  number={4},
  pages={553--563},
  year={2014},
  publisher={Wiley Online Library}
}

@article{miller1983child,
  title={The child's representation of number: A multidimensional scaling analysis},
  author={Miller, Kevin and Gelman, Rochel},
  journal={Child development},
  pages={1470--1479},
  year={1983},
  publisher={JSTOR}
}

% MDS 
@article{shepard1962analysis,
  title={The analysis of proximities: multidimensional scaling with an unknown distance function. I.},
  author={Shepard, Roger N},
  journal={Psychometrika},
  volume={27},
  number={2},
  pages={125--140},
  year={1962},
  publisher={Springer}
}

@article{cheyette2020unified,
  title={A unified account of numerosity perception},
  author={Cheyette, Samuel J and Piantadosi, Steven T},
  journal={Nature human behaviour},
  volume={4},
  number={12},
  pages={1265--1272},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{tenenbaum1999rules,
  title={Rules and similarity in concept learning},
  author={Tenenbaum, Joshua},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@incollection{vicente2017polysemy,
  title={Polysemy},
  author={Vicente, Agust{\'\i}n and Falkum, Ingrid L},
  booktitle={Oxford research encyclopedia of linguistics},
  year={2017}
}

@article{shepard1980multidimensional,
  title={Multidimensional scaling, tree-fitting, and clustering},
  author={Shepard, Roger N},
  journal={Science},
  volume={210},
  number={4468},
  pages={390--398},
  year={1980},
  publisher={American Association for the Advancement of Science}
}

@article{shepard1987toward,
  title={Toward a universal law of generalization for psychological science},
  author={Shepard, Roger N},
  journal={Science},
  volume={237},
  number={4820},
  pages={1317--1323},
  year={1987},
  publisher={American Association for the Advancement of Science}
}

@article{marjieh2024universal,
  title={The universal law of generalization holds for naturalistic stimuli.},
  author={Marjieh, Raja and Jacoby, Nori and Peterson, Joshua C and Griffiths, Thomas L},
  journal={Journal of Experimental Psychology: General},
  volume={153},
  number={3},
  pages={573},
  year={2024},
  publisher={American Psychological Association}
}

@article{tversky1986nearest,
  title={Nearest neighbor analysis of psychological spaces.},
  author={Tversky, Amos and Hutchinson, J},
  journal={Psychological review},
  volume={93},
  number={1},
  pages={3},
  year={1986},
  publisher={American Psychological Association}
}

@article{tenenbaum2001generalization,
  title={Generalization, similarity, and {B}ayesian inference},
  author={Tenenbaum, Joshua B and Griffiths, Thomas L},
  journal={Behavioral and brain sciences},
  volume={24},
  number={4},
  pages={629--640},
  year={2001},
  publisher={Cambridge University Press}
}

@article{kriegeskorte2008representational,
  title={Representational similarity analysis-connecting the branches of systems neuroscience},
  author={Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter A},
  journal={Frontiers in systems neuroscience},
  volume={2},
  pages={249},
  year={2008},
  publisher={Frontiers}
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{dubey2024llama,
  title={The {L}lama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{anthropic2024claude,
  title={Claude 3.5 {S}onnet model card addendum},
  author={Anthropic, AI},
  journal={Claude-3.5 Model Card},
  volume={3},
  pages={6},
  year={2024}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{piantadosi2016rational,
  title={A rational analysis of the approximate number system},
  author={Piantadosi, Steven T},
  journal={Psychonomic bulletin \& review},
  volume={23},
  pages={877--886},
  year={2016},
  publisher={Springer}
}
@article{cunningham2023sparse,
  title={Sparse autoencoders find highly interpretable features in language models},
  author={Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal={arXiv preprint arXiv:2309.08600},
  year={2023}
}
@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}
@misc{geiger2024findingalignmentsinterpretablecausal2,
      title={Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations}, 
      author={Atticus Geiger and Zhengxuan Wu and Christopher Potts and Thomas Icard and Noah D. Goodman},
      year={2024},
      eprint={2303.02536},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@article{geiger2024findingalignmentsinterpretablecausal,
  title={Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations},
  author={Atticus Geiger and Zhengxuan Wu and Christopher Potts and Thomas Icard and Noah D. Goodman},
  journal={arXiv preprint arXiv:2303.02536},
  year={2024}
}

@article{jaech2024openai,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}
@misc{deepseekai2025deepseekr1incentivizingreasoningcapability2,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{deepseekai2025deepseekr1incentivizingreasoningcapability,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={DeepSeek-AI},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}
@article{gao2024scaling,
  title={Scaling and evaluating sparse autoencoders},
  author={Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024}
}
@misc{alain2018understandingintermediatelayersusing,
      title={Understanding intermediate layers using linear classifier probes}, 
      author={Guillaume Alain and Yoshua Bengio},
      year={2018},
      eprint={1610.01644},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1610.01644}, 
}
@misc{belinkov2021probingclassifierspromisesshortcomings,
      title={Probing Classifiers: Promises, Shortcomings, and Advances}, 
      author={Yonatan Belinkov},
      year={2021},
      eprint={2102.12452},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2102.12452}, 
}