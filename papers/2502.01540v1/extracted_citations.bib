@misc{alain2018understandingintermediatelayersusing,
      title={Understanding intermediate layers using linear classifier probes}, 
      author={Guillaume Alain and Yoshua Bengio},
      year={2018},
      eprint={1610.01644},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1610.01644}, 
}

@article{bai2024measuring,
  title={Measuring implicit bias in explicitly unbiased large language models},
  author={Bai, Xuechunzi and Wang, Angelina and Sucholutsky, Ilia and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2402.04105},
  year={2024}
}

@misc{belinkov2021probingclassifierspromisesshortcomings,
      title={Probing Classifiers: Promises, Shortcomings, and Advances}, 
      author={Yonatan Belinkov},
      year={2021},
      eprint={2102.12452},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2102.12452}, 
}

@article{bertology,
    author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
    title = {A Primer in BERTology: What We Know About How BERT Works},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {842-866},
    year = {2021},
    abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00349/1923281/tacl\_a\_00349.pdf},
}

@article{binz2023using,
  title={Using cognitive psychology to understand {GPT}-3},
  author={Binz, Marcel and Schulz, Eric},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={6},
  pages={e2218523120},
  year={2023},
  publisher={National Acad Sciences}
}

@article{cunningham2023sparse,
  title={Sparse autoencoders find highly interpretable features in language models},
  author={Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal={arXiv preprint arXiv:2309.08600},
  year={2023}
}

@article{devlin2019bertpretrainingdeepbidirectional,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={arXiv preprint arXiv:1810.04805},
  year={2019}
}

@misc{dumas2025separatingtonguethoughtactivation,
      title={Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers}, 
      author={Cl√©ment Dumas and Chris Wendler and Veniamin Veselovsky and Giovanni Monea and Robert West},
      year={2025},
      eprint={2411.08745},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{gao2024scaling,
  title={Scaling and evaluating sparse autoencoders},
  author={Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024}
}

@article{geiger2024findingalignmentsinterpretablecausal,
  title={Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations},
  author={Atticus Geiger and Zhengxuan Wu and Christopher Potts and Thomas Icard and Noah D. Goodman},
  journal={arXiv preprint arXiv:2303.02536},
  year={2024}
}

@article{gurnee2023finding,
  title={Finding neurons in a haystack: Case studies with sparse probing},
  author={Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  journal={arXiv preprint arXiv:2305.01610},
  year={2023}
}

@article{gurnee2024languagemodelsrepresentspace,
  title={Language Models Represent Space and Time},
  author={Wes Gurnee and Max Tegmark},
  journal={arXiv preprint arXiv:2310.02207},
  year={2024}
}

@article{ilharco2022editing,
  title={Editing models with task arithmetic},
  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  journal={arXiv preprint arXiv:2212.04089},
  year={2022}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@article{manning2020emergent,
  title={Emergent linguistic structure in artificial neural networks trained by self-supervision},
  author={Manning, Christopher D and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30046--30054},
  year={2020},
  publisher={National Acad Sciences}
}

@article{marjieh2024large,
  title={Large language models predict human sensory judgments across six modalities},
  author={Marjieh, Raja and Sucholutsky, Ilia and van Rijn, Pol and Jacoby, Nori and Griffiths, Thomas L},
  journal={Scientific Reports},
  volume={14},
  number={1},
  pages={21445},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@inproceedings{nanda-etal-2023-emergent,
    title = "Emergent Linear Representations in World Models of Self-Supervised Sequence Models",
    author = "Nanda, Neel  and
      Lee, Andrew  and
      Wattenberg, Martin",
    editor = "Belinkov, Yonatan  and
      Hao, Sophie  and
      Jumelet, Jaap  and
      Kim, Najoung  and
      McCarthy, Arya  and
      Mohebbi, Hosein",
    booktitle = "Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",
    year = "2023",
    doi = "10.18653/v1/2023.blackboxnlp-1.2",
    pages = "16--30",
    abstract = "How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023a). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for {\textquotedblleft}my colour{\textquotedblright} vs. {\textquotedblleft}opponent`s colour{\textquotedblright} may be a simple yet powerful way to interpret the model`s internal state. This precise understanding of the internal representations allows us to control the model`s behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed."
}

@article{nanda2023progressmeasuresgrokkingmechanistic,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}

@article{nylund2023time,
  title={Time is Encoded in the Weights of Finetuned Language Models},
  author={Nylund, Kai and Gururangan, Suchin and Smith, Noah A},
  journal={arXiv preprint arXiv:2312.13401},
  year={2023}
}

@article{stolfo2023mechanisticinterpretationarithmeticreasoning,
  title={A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis},
  author={Alessandro Stolfo and Yonatan Belinkov and Mrinmaya Sachan},
  journal={arXiv preprint arXiv:2305.15054},
  year={2023}
}

@article{tenney2019you,
  title={What do you learn from context? probing for sentence structure in contextualized word representations},
  author={Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R and Das, Dipanjan and others},
  journal={arXiv preprint arXiv:1905.06316},
  year={2019}
}

@article{tian2023just,
  title={Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback},
  author={Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2305.14975},
  year={2023}
}

@article{webb2023emergent,
  title={Emergent analogical reasoning in large language models},
  author={Webb, Taylor and Holyoak, Keith J and Lu, Hongjing},
  journal={Nature Human Behaviour},
  volume={7},
  number={9},
  pages={1526--1541},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{zhu-etal-2025-language,
    title = "Language Models Encode the Value of Numbers Linearly",
    author = "Zhu, Fangwei  and
      Dai, Damai  and
      Sui, Zhifang",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    year = "2025",
    pages = "693--709",
    abstract = "Large language models (LLMs) have exhibited impressive competence in various tasks, but their internal mechanisms on mathematical problems are still under-explored. In this paper, we study a fundamental question: how language models encode the value of numbers, a basic element in math. To study the question, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of encoded number values in LLMs on different layers, and these values can be extracted via linear probes. Further experiments show that LLMs store their calculation results in a similar manner, and we can intervene the output via simple vector additions, proving the causal connection between encoded numbers and language model outputs. Our research provides evidence that LLMs encode the value of numbers linearly, offering insights for better exploring, designing, and utilizing numeric information in LLMs."
}

