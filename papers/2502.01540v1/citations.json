[
  {
    "index": 0,
    "papers": [
      {
        "key": "zhu-etal-2025-language",
        "author": "Zhu, Fangwei  and\nDai, Damai  and\nSui, Zhifang",
        "title": "Language Models Encode the Value of Numbers Linearly"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "hanna2023how",
        "author": "Michael Hanna and Ollie Liu and Alexandre Variengien",
        "title": "How does {GPT}-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "stolfo2023mechanisticinterpretationarithmeticreasoning",
        "author": "Alessandro Stolfo and Yonatan Belinkov and Mrinmaya Sachan",
        "title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "nanda2023progressmeasuresgrokkingmechanistic",
        "author": "Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt",
        "title": "Progress measures for grokking via mechanistic interpretability"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "tian2023just",
        "author": "Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D",
        "title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "mccoy2023embersautoregressionunderstandinglarge",
        "author": "McCoy, R Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Mathew D and Griffiths, Thomas L",
        "title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "mccoy2023embersautoregressionunderstandinglarge",
        "author": "McCoy, R Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Mathew D and Griffiths, Thomas L",
        "title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve"
      },
      {
        "key": "mccoy2024languagemodeloptimizedreasoning",
        "author": "R. Thomas McCoy and Shunyu Yao and Dan Friedman and Mathew D. Hardy and Thomas L. Griffiths",
        "title": "When a language model is optimized for reasoning, does it still show embers of autoregression? {A}n analysis of {O}pen{AI} o1"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "alain2018understandingintermediatelayersusing",
        "author": "Guillaume Alain and Yoshua Bengio",
        "title": "Understanding intermediate layers using linear classifier probes"
      },
      {
        "key": "belinkov2021probingclassifierspromisesshortcomings",
        "author": "Yonatan Belinkov",
        "title": "Probing Classifiers: Promises, Shortcomings, and Advances"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "devlin2019bertpretrainingdeepbidirectional",
        "author": "Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "tenney2019you",
        "author": "Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R and Das, Dipanjan and others",
        "title": "What do you learn from context? probing for sentence structure in contextualized word representations"
      },
      {
        "key": "manning2020emergent",
        "author": "Manning, Christopher D and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer",
        "title": "Emergent linguistic structure in artificial neural networks trained by self-supervision"
      },
      {
        "key": "bertology",
        "author": "Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna",
        "title": "A Primer in BERTology: What We Know About How BERT Works"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhu-etal-2025-language",
        "author": "Zhu, Fangwei  and\nDai, Damai  and\nSui, Zhifang",
        "title": "Language Models Encode the Value of Numbers Linearly"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "gurnee2023finding",
        "author": "Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris",
        "title": "Finding neurons in a haystack: Case studies with sparse probing"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "kadavath2022language",
        "author": "Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others",
        "title": "Language models (mostly) know what they know"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "nanda-etal-2023-emergent",
        "author": "Nanda, Neel  and\nLee, Andrew  and\nWattenberg, Martin",
        "title": "Emergent Linear Representations in World Models of Self-Supervised Sequence Models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "gurnee2024languagemodelsrepresentspace",
        "author": "Wes Gurnee and Max Tegmark",
        "title": "Language Models Represent Space and Time"
      },
      {
        "key": "gurnee2023finding",
        "author": "Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris",
        "title": "Finding neurons in a haystack: Case studies with sparse probing"
      },
      {
        "key": "nylund2023time",
        "author": "Nylund, Kai and Gururangan, Suchin and Smith, Noah A",
        "title": "Time is Encoded in the Weights of Finetuned Language Models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "meng2022locating",
        "author": "Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan",
        "title": "Locating and editing factual associations in GPT"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "cunningham2023sparse",
        "author": "Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee",
        "title": "Sparse autoencoders find highly interpretable features in language models"
      },
      {
        "key": "gao2024scaling",
        "author": "Gao, Leo and la Tour, Tom Dupr{\\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey",
        "title": "Scaling and evaluating sparse autoencoders"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "geiger2024findingalignmentsinterpretablecausal",
        "author": "Atticus Geiger and Zhengxuan Wu and Christopher Potts and Thomas Icard and Noah D. Goodman",
        "title": "Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "ilharco2022editing",
        "author": "Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali",
        "title": "Editing models with task arithmetic"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "dumas2025separatingtonguethoughtactivation",
        "author": "Cl\u00e9ment Dumas and Chris Wendler and Veniamin Veselovsky and Giovanni Monea and Robert West",
        "title": "Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "nylund2023time",
        "author": "Nylund, Kai and Gururangan, Suchin and Smith, Noah A",
        "title": "Time is Encoded in the Weights of Finetuned Language Models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "dumas2025separatingtonguethoughtactivation",
        "author": "Cl\u00e9ment Dumas and Chris Wendler and Veniamin Veselovsky and Giovanni Monea and Robert West",
        "title": "Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "ilharco2022editing",
        "author": "Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali",
        "title": "Editing models with task arithmetic"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "gurnee2023finding",
        "author": "Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris",
        "title": "Finding neurons in a haystack: Case studies with sparse probing"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "zhu-etal-2025-language",
        "author": "Zhu, Fangwei  and\nDai, Damai  and\nSui, Zhifang",
        "title": "Language Models Encode the Value of Numbers Linearly"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "marjieh2024large",
        "author": "Marjieh, Raja and Sucholutsky, Ilia and van Rijn, Pol and Jacoby, Nori and Griffiths, Thomas L",
        "title": "Large language models predict human sensory judgments across six modalities"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "binz2023using",
        "author": "Binz, Marcel and Schulz, Eric",
        "title": "Using cognitive psychology to understand {GPT}-3"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "bai2024measuring",
        "author": "Bai, Xuechunzi and Wang, Angelina and Sucholutsky, Ilia and Griffiths, Thomas L",
        "title": "Measuring implicit bias in explicitly unbiased large language models"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "webb2023emergent",
        "author": "Webb, Taylor and Holyoak, Keith J and Lu, Hongjing",
        "title": "Emergent analogical reasoning in large language models"
      }
    ]
  }
]