\section{Empirical Validation}

\begin{figure*}[t]
\begin{center}
% \vspace{-5pt}
\subfigure[Results of GPT-2-series on CAI-Harmless]{ 
\begin{minipage}[t]{0.95\linewidth}  \centerline{\includegraphics[width=1\linewidth]{images/harmless_gpt2.pdf}}
\end{minipage}  
}  
\subfigure[Results of GPT-2-series on helpful set of HH-RLHF]{
\begin{minipage}[t]{0.95\linewidth}
\centerline{\includegraphics[width=1\linewidth]{images/helpful_gpt2.pdf}}
\end{minipage}  
}    
\caption{Results of GPT-2-series. ``SC'' denotes the strong ceiling model, and ``A to B'' indicates the use of weak teacher ``A'' to supervise strong student ``B''. The terms CE, RCE, KL, and RKL refer to CE loss, reverse CE loss, forward KL divergence loss, and reverse KL divergence loss, respectively. Error bars represent the standard deviation across three runs of the experiment.}
\label{fig:cai}
\end{center}
\vspace{-3pt}
\end{figure*}




In this section, we empirically compare reverse KL, forward KL, reverse CE, and standard CE losses in the context of WTSG. 
Our experiments directly support the claim that reverse losses outperform forward losses in most experimental settings.


\subsection{Experimental Settings}

% \noindent \textbf{Datasets.}
\paragraph{Datasets.}
We follow previous studies~\citep{burns2023weak,yang2024super} to conduct experiments mainly in the reward modeling task in two settings: enabling a weak model to effectively guide a strong model in achieving either harmlessness or helpfulness. 
To achieve \textbf{harmlessness}, we follow~\citep{yang2024super} to leverage CAI-Harmless~\citep{bai2022constitutional}, a widely adopted benchmark for single-turn harmless dialogue tasks. 
To achieve \textbf{helpfulness}, we utilize HH-RLHF~\citep{bai2022training}, a benchmark designed to guide models toward producing responses that are helpful, informative, and contextually relevant. 
We use a subset of single-turn helpful data of HH-RLHF.

Each dataset includes three subsets: \textbf{(1) Ground truth set}: 4K samples with ground truth labels, used to fine-tune the base models to create strong ceiling models.
\textbf{(2) Weak supervision set}: 4K held-out samples, where the weak model generates predicted labels to guide the training of the strong model.
\textbf{(3) Test set}: The extra 4K samples, reserved for evaluating the generalization performance of all strong ceiling and weak-to-strong models. 
Each sample is formatted as $\Tilde{x}=(x;y_c,y_r)$, where $x$ is the user input, $y_c$ and $y_r$ represent human-chosen and human-rejected responses separately.



% \noindent \textbf{Models.}
\paragraph{Models.}
We conduct experiments on two types of model families: 
(1) GPT-2-series~\citep{radford2019language}, including GPT-2-Base, GPT-2-Medium, GPT-2-Large, and GPT-2-XL; 
(2) Pythia-series~\citep{biderman2023pythia}, specifically, Pythia-70M, Pythia-160M, Pythia-410M, and Pythia-1B. 
% For each model, we attach a linear projection head to enable logit predictions. 
Each model is trained to generate a soft value between 0 to 1 for each sample: $$F(\Tilde{x}) = \text{Sigmoid}(F(y_c)-F(y_r)).$$
When implementing forward and reverse losses, the single predicted logit is transformed into a logits distribution represented as $(1 - F(\Tilde{x}), F(\Tilde{x}))$.


% \noindent \textbf{Training and Evaluation.}
\paragraph{Training and Evaluation.}
The strong ceiling models are trained using the standard CE loss. We apply four loss functions in WTSG: forward KL, reverse KL, CE and reverse CE.
To ensure the reliability and consistency of our results, each experiment is repeated across three random seeds.
We set the training batch size to $16$, learning rate to $10^{-5}$, and \texttt{max\_seq\_len} to $512$.
Following the approach of~\citet{burns2023weak}, we train each model for a single epoch to reduce overfitting.
Finally, we report the average accuracy on the test set across the three random seeds for each model for comparison.










\subsection{Main Results} \label{sec:main_results}

The experimental results of the GPT-2 series on the CAI-Harmless and HH-RLHF datasets are presented in~\cref{fig:cai}. 
Due to space limitation, we put the detailed results for the Pythia series in~\cref{exp_result_pythia}, but the similar trends can be observed. 

We can draw several conclusions from the results in~\cref{fig:cai}: 
(1) The accuracy demonstrates a consistent upward trend from left to right. 
It indicates that the generalization capability of the strong model improves when a more capable weak model is employed as the supervisor.
This finding is in line with~\cref{lemma:upper_lower_inf} and aligns with prior research~\citep{burns2023weak,yao2025understanding}, which suggests that utilizing a higher-capacity weak model enhances the strong model's performance.
Furthermore, with a fixed weak model, leveraging a stronger model also yield improved strong model's performance, consistent with established research~\citep{burns2023weak,yang2024super}.
(2) We observe that \textbf{reverse KL and reverse CE losses enable strong models to outperform those trained with forward KL and CE losses across most experimental settings}.
In particular, in all settings (12 out of 12), the use of reverse KL yields a stronger model compared to standard KL.
Similarly, reverse CE outperforms or parallels forward CE in nearly all experimental settings (10 out of 12). 
These empirical results, supported by our theoretical framework, underscore the superiority of reverse losses over forward losses.
(3) In the majority of settings (10 out of 12), the strong model surpasses or meets the performance of its weak supervisor when trained with reverse KL or reverse CE loss. This observation supports~\cref{thm:realizable} and~\cref{thm:non-realizable-finite-samples}. 
However, the theoretical guarantees may not always hold in practice, particularly in scenarios involving extremely complex LLMs with limited training set in WTSG, where the underlying assumptions may be violated.



\subsection{Ablation Study}

We notice that~\citet{burns2023weak} investigates an improved strategy: incorporating an additional regularization term aimed at boosting the strong model's confidence in its predictions, while utilizing the standard CE loss as the primary objective.
This naturally raises the question of whether combining reverse CE loss with such regularization can further improve the strong model's performance compared to standard CE loss with regularization.
To explore this question, we conduct experiments using the GPT-2 series on the CAI-Harmless dataset as a representative case. Due to space limitation, we only put the results when GPT-2-Base acts as the weak model to supervise GPT-2-Medium, GPT-2-Large, and GPT-2-XL here in~\cref{fig:conf_loss}, while put the full results in~\cref{exp:conf_loss}. 
%The key observations are summarized in~\cref{fig:enter-label}, while the full results involving other weak models, along with the same conclusion, are provided in~\cref{exp:conf_loss}. 

First, by integrating the insights from~\cref{fig:cai} and~\cref{fig:conf_loss}, we can see that incorporating the confidence regularization leads to a modest improvement in the strong model's performance, aligning with the observations of~\citet{burns2023weak}.
Second, the strong model trained using reverse CE loss with regularization consistently surpasses its counterpart trained with standard CE loss. 
This result, together with our previous results in~\cref{sec:main_results}, underscores the clear advantage of reverse losses over forward losses in enhancing model performance in diverse settings.


\begin{figure}[t]
    \centering
    % \vspace{-5pt}
    \includegraphics[width=0.99\linewidth]{images/harmless_gpt2_conf_loss_part.pdf}
    \caption{Results of GPT-2 series on CAI-Harmless. ``SC'' denotes the strong ceiling model. The terms ``Conf.\ CE'' and ``Reve.\ Conf.\ CE'' refer to the auxiliary confidence loss with vanilla CE loss and reverse CE loss, respectively. Error bars represent the standard deviation across three runs of the experiment.}
    \label{fig:enter-label}
    \vspace{-3pt}
\end{figure}








