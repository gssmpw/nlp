\section{Introduction}

Human supervision is indispensable to align Large Language Models (LLMs) with human values~\citep{bai2022training,achiam2023gpt}.
However, as LLMs approach superhuman capabilities, their behaviors may exceed human ability to reliably manage~\citep{openai_superalignment}. 
To address this challenge, 
Weak-to-Strong Generalization (WTSG)~\citep{burns2023weak} emerges as a promising approach, leveraging weaker models to guide and control more advanced systems, thereby bridging the gap between human oversight and superhuman AI capabilities. 


In particular, WTSG demonstrates that strong pre-trained LLMs, when fine-tuned under weak model supervision, can achieve performance surpassing that of their weak supervisors. 
However, this approach is fundamentally constrained by the inherent imperfections of weak model supervision, which may introduce inaccuracies and noise~\citep{burns2023weak}. 
Blindly fitting the strong model to these imperfect signals can lead to a significant discrepancy between the ground truth and the model's predictions, ultimately undermining the effectiveness of WTSG~\citep{yao2025understanding}. 
This raises a critical question: \textit{How to effectively leverage weak supervision to guide strong models while mitigating the impact of noisy or inaccurate signals?}



\begin{figure*}[t]
\begin{center}
% \vspace{-5pt}
\subfigure[Knowledge Distillation]{ 
\begin{minipage}[t]{0.48\linewidth}  \centerline{\includegraphics[width=1\linewidth]{images/kd_comparison_kl.pdf}}
\end{minipage}  
}  
\subfigure[Weak-to-Strong Generalization]{
\begin{minipage}[t]{0.48\linewidth}
\centerline{\includegraphics[width=1\linewidth]{images/wtsg_comparison_kl.pdf}}
\end{minipage} 
}
% \vspace{-5pt}
\caption{Illustration of the mass-covering behavior of forward KL divergence and the mode-seeking behavior of reverse KL divergence, highlighting their roles in KD and WTSG. A Gaussian mixture distribution, representing the teacher's supervision in KD and WTSG, is approximated by fitting a single Gaussian distribution using both forward and reverse KL divergence as loss functions.}
\label{fig:comparison_kl}
\end{center}
\vspace{-5pt}
\end{figure*}


To answer this question, we propose a theoretically principled approach, supported by fine-grained analysis and a simple yet effective solution.
Our motivation stems from an insightful comparison with Knowledge Distillation (KD)~\citep{hinton2015distilling} in classification, where strong teachers provide informative soft labels to guide weak students. 
In KD, the forward KL divergence loss plays a crucial role as it encourages students to learn not only the target class probabilities but also the relative relationships among non-target classes encoded in the teacher's soft labels.
For instance, in the image classification scenario, a strong teacher might assign higher probabilities to ``tiger'' than to ``dog'' when the input image is a ``cat'', reflecting the semantic similarity between cats and tigers in the feature space.
However, this advantageous property of forward KL in KD becomes a limitation in the WTSG paradigm.
The fundamental distinction lies in the quality of supervision: while strong teachers in KD provide reliable and informative soft labels, weak teachers in WTSG often generate noisy and potentially misleading signals for non-target classes~\citep{burns2023weak}.
Thus, the mass-covering nature of forward KL~\citep{jerfel2021variational,sun2024inverse}, which forces the student to match the entire probability distribution of the teacher's predictions, becomes detrimental in WTSG as it may lead the strong model to overfit to the weak teacher's unreliable supervision. This observation motivates our investigation of reverse KL divergence as a more suitable alternative for WTSG.
As shown in~\cref{fig:comparison_kl}, the key advantage of reverse KL lies in its mode-seeking behavior~\citep{minka2005divergence,ji2023language}, which enables the strong model to focus on the weak teacher's high-confidence predictions while being less sensitive to potentially noisy low-probability regions.
This property aligns better with the WTSG setting, as it allows the strong model to extract reliable patterns from weak supervision without being overly constrained by its imperfections. 

Building on the intuitive motivation above, we first conduct a theoretical analysis to compare forward losses and reverse losses in the context of WTSG. 
Inspired by the lower and upper bounds established for the strong model in WTSG~\citep{yao2025understanding}, we extend these results and derive tighter lower bounds for both forward and reverse losses, demonstrating that reverse losses achieves at least equivalent theoretical guarantees to forward losses.
Furthermore, we identify an advantage of reverse KL: when an adequately pre-trained strong model undergoes last linear layer fine-tuning, reverse KL guarantees that the strong student will outperform its weak teacher by at least the magnitude of their disagreement.
% Notably, this performance guarantee fails to hold for forward KL without additional assumptions, underscoring the theoretical advantage of reverse losses.
In our experiments, we empirically demonstrate that employing reverse KL divergence and reverse Cross-Entropy (CE) as loss functions enables the strong model to achieve superior performance compared to using forward KL divergence and standard CE.
We also extend the analysis to an improved algorithm discussed in~\citet{burns2023weak}, where the optimization objective incorporates an additional regularization term. It further demonstrates the practical advantages of reverse CE over standard CE in the context of WTSG.








