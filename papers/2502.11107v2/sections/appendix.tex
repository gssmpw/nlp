\section{Discussion of Concurrent Work} \label{discussion:concurrent}


Recently, a concurrent work~\citep{mulgund2025relating} has also independently addressed a problem similar to ours. 
The primary similarity is found in~\cref{sec:upper}.
In particular, our~\cref{thm:realizable} in~\cref{sec:upper} is similar to their Theorem 4.1 \& Corollary 4.2.
The proof of their Theorem 4.1 \& Corollary 4.2 and our~\cref{thm:realizable} share the convexity assumption and the mathematical formulation of Bregman divergence. However, the proof techniques differ significantly. While we explicitly construct the sum of first-order and second-order terms through derivation and calculation, they apply mathematical tools such as generalized Pythagorean inequality, convex analysis, and the sequential consistency property to derive their results.
Building on the derived results, while we focus on overcoming the realizability assumption and deriving sample complexity bounds in our~\cref{thm:non-realizable-finite-samples}, they aim to relax the convexity assumption by projecting the weak model onto convex combinations of functions based on the strong model representation in their Theorem 4.3.
Both our work and~\citet{mulgund2025relating} contribute to the theoretical understanding of WTSG in classification, with a particular focus on reverse KL/CE losses.
 





\section{Main Proof} 

\subsection{Proof of~\cref{lemma:upper_lower_inf}} \label{proof_lemma_inf}

We first state some preliminaries for the proof.

\begin{lemma}[Donsker and Varadhan’s variational formula~\citep{donsker1983asymptotic}] \label{lemma:donsker}
Let $Q, P$ be probability measures on $\cX$, for any bounded measurable function $f: \cX \rightarrow \mathbb{R}$, we have $$\mathrm{D}_{\mathrm{KL}}(Q \| P)=\sup _f \mathbb{E}_{x \sim Q}[f(x)]-\log \mathbb{E}_{x \sim P}[\exp f(x)].$$
\end{lemma}


\begin{lemma}[Hoeffding's lemma] \label{hoeffding_lemma}
Let $X \in \R$ such that $a \leq X \leq b$. Then, for all $\lambda \in \mathbb{R}$,
$$\mathbb{E}\left[e^{\lambda(X-\mathbb{E}[X])}\right] \leq \exp \left(\frac{\lambda^2(b-a)^2}{8}\right).$$
\end{lemma}

\begin{definition}[Subgaussian random variable]
A random variable $X \in \R$ is $\sigma$-subgaussian if for any $\rho$, 
$$\log \mathbb{E} \exp (\rho(X-\mathbb{E} X)) \leq \rho^2 \sigma^2 / 2.$$
\end{definition}


% \paragraph{Notation of probability distribution for the model output.}
We define the corresponding probability distributions for prediction of $F_{sw}$ and $F_w$.
% Recall that for $F_w, F_{sw}: \cX \to \cY$ and $x \in \cX$:
% $$\dist(F_w, F_{sw}) = \bE_x \left[ \sum_{j=1}^k [F_w(x)]_j \log \frac{[F_w(x)]_j}{[F_{sw}(x)]_j} \right] = \bE_x \left[ \mathrm{D}_{\mathrm{KL}}(F_w(x), F_{sw}(x)) \right],$$
% where $\mathrm{D}_{\mathrm{KL}}$ is the discrete version of KL divergence. 
$\forall x \in \cX$, we know that $\sum_{j=1}^k [F_w(x)]_j = 1$. Therefore, given the class space $C_k = \{ 1, \cdots, k \}$, we define a probability distribution $\cP_{w}(x)$ with the probability density function $p_w$, where $j \in C_k$ and 
\begin{align} \label{def_new_distribution}
    p_w(j)=[F_w(x)]_j.
\end{align}
Using this method, we also define the probability distribution $\cP_{sw}(x)$ for $F_{w}(x)$.


\begin{lemma}[\citet{yao2025understanding}] \label{infor_lemma}
Given the probability distributions $\cP_{w}(x)$ and $\cP_{sw}(x)$ above.
For any $x \in \cX$, $j \in C_k$, $g: C_k \to \R$ and assume that $g$ is $\sigma$-subgaussian.
Let $f=t \cdot g$ for any $t \in \R$, then 
$$\mathrm{D}_{\mathrm{KL}}\left( F_w(x) \| F_{sw}(x) \right) \ge \sup _t t\left(\mathbb{E}_{j^{\prime} \sim \cP_w(x)}\left[g\left(j^{\prime}\right)\right]-\mathbb{E}_{j \sim \cP_{sw}(x)}[g(j)]\right)-t^2 \sigma^2 / 2.$$
\end{lemma}




Now we start the proof.

\begin{proof}


By taking expectations of $x$ on both sides of the inequality in~\cref{infor_lemma}, we obtain
\begin{multline*}
    \kl(F_w, F_{sw}) = \bE_x \mathrm{D}_{\mathrm{KL}}\left( F_w(x) \| F_{sw}(x) \right) \\ \ge \sup _t \underbrace{t\left(\bE_x\mathbb{E}_{j^{\prime} \sim \cP_w(x)}\left[g\left(j^{\prime}\right)\right]-\bE_x\mathbb{E}_{j \sim \cP_{sw}(x)}[g(j)]\right)-t^2 \sigma^2 / 2}_{\phi(t)}.
\end{multline*}


Note that $\phi(t)$ is a quadratic function of $t$.
Therefore, by AM–GM inequality, we find the maximum of this quadratic function:
\begin{align*}
    \phi(t) \le \frac{1}{2\sigma^2}\left(\bE_x\mathbb{E}_{j^{\prime} \sim \cP_w(x)}\left[g\left(j^{\prime}\right)\right]-\bE_x\mathbb{E}_{j \sim \cP_{sw}(x)}[g(j)]\right)^2 = \sup _t \phi(t) \le \kl(F_w, F_{sw}).
\end{align*}

Subsequently, there holds
\begin{align} \label{ineq:lower_upper_kl_loss}
\left|\bE_x\mathbb{E}_{j^{\prime} \sim \cP_w(x)}\left[g\left(j^{\prime}\right)\right]-\bE_x\mathbb{E}_{j \sim \cP_{sw}(x)}[g(j)]\right| \le \sqrt{2\sigma^2 \kl(F_w, F_{sw})}.
\end{align}

Likewise, according to~\cref{infor_lemma}, we have
\begin{align} \label{proof:variant-1}
    \mathrm{D}_{\mathrm{KL}}\left( F_{sw}(x) \| F_w(x) \right) \ge \sup _t t\left(\mathbb{E}_{j \sim \cP_{sw}(x)}\left[g\left(j^{\prime}\right)\right]-\mathbb{E}_{j^{\prime} \sim \cP_w(x)}[g(j)]\right)-t^2 \sigma^2 / 2.
\end{align}

We apply the same proof technique to~\eqref{proof:variant-1} and obtain:
\begin{align} \label{proof:variant-2}
    \left|\bE_x\mathbb{E}_{j^{\prime} \sim \cP_w(x)}\left[g\left(j^{\prime}\right)\right]-\bE_x\mathbb{E}_{j \sim \cP_{sw}(x)}[g(j)]\right| \le \sqrt{2\sigma^2 \kl(F_{sw}, F_w)}.
\end{align}



Now we construct $g$ to associate the above results with $\dist(F^\star, F_{sw})$ and $\dist\left( F^\star, F_w  \right)$.
Specifically, given a probability distribution $\cP_g$ with the density function $p_g$, we define function $g: C_k \to (0,1]$ associated with $\cP_g$: 
$$g(j) \triangleq \frac{[F^\star(x)]_j}{p_g(j)} \log \frac{[F^\star(x)]_j}{p_g(j)}, \quad \text{for} \ j \in C_k.$$


We have
\begin{align*}
    \bE_x\mathbb{E}_{j \sim \cP_g} \left[g(j)\right] & = \bE_x \bE_{j \sim \cP_g} \left[\frac{[F^\star(x)]_j}{p_g(j)} \log \frac{[F^\star(x)]_j}{p_g(j)} \right] \\
    & = \bE_x \left[\sum_{j \in C_k} p_g(j) \cdot \frac{[F^\star(x)]_j}{p_g(j)} \cdot \log \frac{[F^\star(x)]_j}{p_g(j)} \right] \\
    & = \bE_x \left[\sum_{j \in C_k} [F^\star(x)]_j \cdot \log \frac{[F^\star(x)]_j}{p_g(j)} \right]
\end{align*}

Recall the definition of $\cP_{sw}$ and $\cP_w$ in~\eqref{def_new_distribution}, we replace $\cP_g$ with $\cP_{sw}$ and $\cP_w$ in the above equation:
\begin{align*}
    & \bE_x\mathbb{E}_{j^{\prime} \sim \cP_{sw}}\left[g\left(j^{\prime}\right)\right] = \bE_x \left[ \sum_{j=1} [F^\star(x)]_j \log \frac{[F^\star(x)]_j}{[F_{sw}(x)]_j} \right] = \kl(F^\star, F_{sw}), \\
    & \bE_x\mathbb{E}_{j \sim \cP_w}[g(j)] = \bE_x \left[ \sum_{j=1} [F^\star(x)]_j \log \frac{[F^\star(x)]_j}{[F_{w}(x)]_j} \right] = \kl(F^\star, F_{w}).
\end{align*}

Substitute the above into~\eqref{ineq:lower_upper_kl_loss}:
\begin{align} \label{ineq:temp-1}
    \left| \dist(F^\star, F_{sw})-\dist(F^\star, F_{w}) \right| \le \sqrt{2\sigma^2 \kl(F_w, F_{sw})},
\end{align}
The above inequality is because whether $\dist$ is $\kl$ or $\cross$, we have 
$$\dist(F^\star, F_{sw})-\dist(F^\star, F_{w}) = \kl(F^\star, F_{sw})-\kl(F^\star, F_{w}).$$

Likewise, we apply the same proof technique to~\eqref{proof:variant-2} and obtain:
\begin{align} \label{proof:variant-3}
    \left| \dist(F^\star, F_{sw})-\dist(F^\star, F_{w}) \right| \le \sqrt{2\sigma^2 \kl(F_{sw}, F_w)}.
\end{align}

Finally, we obtain the subgaussian factor $\sigma$ of function $g$ by using the fact that $g$ is bounded.
Recall that the output domain $\cY \subseteq \R^k$, where $\forall y = (y_1, \cdots, y_k)^T \in \cY$, there holds $\sum_{i=1}^k y_i=1$ and $0 < y_i \le 1$.
In other words, $\exists \gamma>0$ such that $0 < \gamma \le y_i \le 1$.
It means that $g(j) \in [-\frac{1}{\gamma} \log \frac{1}{\gamma}, \frac{1}{\gamma} \log \frac{1}{\gamma}]$.
According to~\cref{hoeffding_lemma}, $\forall \lambda \in \R$, we have
$$\mathbb{E}\left[e^{\lambda(g(j)-\mathbb{E}[g(j)])}\right] \leq \exp \left(\frac{\lambda^2 \left(\frac{1}{\gamma} \log \frac{1}{\gamma} \right)^2}{2}\right).$$
In other words, $g(j)$ is $\sigma$-subgaussian, where $\sigma=\frac{1}{\gamma} \log \frac{1}{\gamma}$.
Substitute it into~\eqref{ineq:temp-1} and~\eqref{proof:variant-3}, we prove the final results:
\begin{align*}
    & \left| \dist(F^\star, F_{sw}) - \dist\left( F^\star, F_w  \right) \right| \le C_1 \sqrt{\kl(F_w, F_{sw})}, \\
    & \left| \dist(F^\star, F_{sw}) - \dist\left( F^\star, F_w  \right) \right| \le C_1 \sqrt{\kl(F_{sw}, F_w)},
\end{align*}
where the constant $C_1 = \frac{\sqrt{2}}{\gamma} \log \frac{1}{\gamma}$.

\end{proof}

















\subsection{Proof of~\cref{theorem:residue}} \label{constant:theorem}

Total variation distance is introduced for our proof.
\begin{definition}[Total Variation Distance] \label{def:tv_distance}
Given two probability distributions $P$ and $Q$, the Total Variation (TV) distance between $P$ and $Q$ is
$$\tv(P \| Q)= \frac{1}{2} \int_{x \in \mathcal{X}} \left| P(x)-Q(x) \right| d x.$$
\end{definition}
Note that $\tv(P \| Q)\in[0,1]$. Also, $\tv(P \| Q)=0$ if and only if $P$ and $Q$ coincides, and $\tv(P \| Q)=1$ if and only if $P$ and $Q$ are disjoint.


\begin{proof}
We have
\begin{align}
    \dist(F^\star, F_w) &= \bE_x \left[ \sum_{i=1}^k [F^\star(x)]_i \log \frac{[F^\star(x)]_i}{[F_w(x)]_i} \right] \nonumber \\
    &= \bE_x \left[ \sum_{i=1}^k [F^\star(x)]_i \log \left( \frac{[F^\star(x)]_i}{[F_{sw}(x)]_i} \cdot \frac{[F_{sw}(x)]_i}{[F_w(x)]_i} \right) \right] \nonumber \\
    &= \bE_x \left[ \sum_{i=1}^k [F^\star(x)]_i \log  \frac{[F^\star(x)]_i}{[F_{sw}(x)]_i} \right] + \bE_x \left[ \sum_{i=1}^k [F^\star(x)]_i \log  \frac{[F_{sw}(x)]_i}{[F_w(x)]_i} \right] \nonumber \\
    & = \dist(F^\star, F_{sw}) + \left \langle F^\star, \log{\frac{F_{sw}}{F_w}} \right \rangle_E.
\end{align}
Rearranging terms and we know that:
\begin{align} \label{eqn:temp_b3}
    \dist(F^\star, F_{sw}) = \dist(F^\star, F_w) - \left \langle F^\star, \log{\frac{F_{sw}}{F_w}} \right \rangle_E.
\end{align}


Recall that the output domain $\cY \subseteq \R^k$, where $\forall y = (y_1, \cdots, y_k)^T \in \cY$, there holds $\sum_{i=1}^k y_i=1$ and $0 < y_i \le 1$.
In other words, $\exists \gamma>0$ such that $0 < \gamma \le y_i \le 1$.
Firstly, we know that $F^\star(x)$ is element-wise non-negative.
Denote $\vec{1}=(1,1, \cdots, 1)^T$. We know that there is a positive constant $\frac{1}{\gamma} \ge \left(\min_i [F_w(x)]_i \right)^{-1}$.
We use element-wise addition, subtraction, multiplication, division and absolute value in the proof.
Note that 
\begin{align*}
    \left \langle F^\star, \log{\frac{F_{sw}}{F_w}} \right \rangle_E & \le \left \langle F^\star, \frac{F_{sw}}{F_w} -\vec{1} \right \rangle_E \tag{$\log x \le x-1$} \\
    & \le \left \langle F^\star, \frac{1}{\gamma} \cdot F_w \left\vert \frac{F_{sw}}{F_w} -\vec{1} \right\vert \right \rangle_E \tag{$\frac{1}{\gamma} \cdot F_w \ge \vec{1}$ (element-wise)} \\
    % & \le C_w \cdot \left \langle F^\star + F_w, F_w \left(\frac{F_{sw}}{F_w} -\vec{1} \right)  \right \rangle_E \\
    & = \frac{1}{\gamma} \cdot \left \langle F^\star, \left\vert F_{sw}-F_w \right\vert  \right \rangle_E,
\end{align*}

and
\begin{align*}
    \left \langle F^\star, \left\vert F_{sw}-F_w \right\vert \right \rangle_E & = \expect_x \left[ \left(F^\star(x)\right)^T \left( \left|F_{sw}(x)-F_w(x)\right| \right) \right] \\
    & \le \expect_x \left[ \left\| F^\star(x) \right\|_\infty \cdot \left\| F_{sw}(x)-F_w(x) \right\|_1 \right] \tag{Holder’s inequality for vector-valued functions} \\ 
    & \le \expect_x \left[ \left\| F_{sw}(x)-F_w(x) \right\|_1 \right] \tag{$[F^\star(x)]_i \le 1$} \\ 
    & = 2 \expect_x \tv \left( F_w(x), F_{sw}(x) \right) \tag{Definition of TV distance} \\
    & \le 2 \sqrt{\expect_x \tv^2 \left( F_w(x), F_{sw}(x) \right)} \tag{Jensen’s inequality} \\ 
    & \le 2 \sqrt{\frac{1}{2}\expect_x \mathrm{D}_{\mathrm{KL}} \left( F_w(x), F_{sw}(x) \right)} \tag{Pinsker’s inequality} \\ 
    & = \sqrt{2\kl(F_w, F_{sw})}. \tag{Definition of $\kl(\cdot, \cdot)$}
\end{align*}

Therefore, 
$$\left \langle F^\star, \log{\frac{F_{sw}}{F_w}} \right \rangle_E \le \frac{\sqrt{2}}{\gamma} \cdot \sqrt{\kl(F_w, F_{sw})}.$$
Since the TV distance is symmetric, we also have
$$\left \langle F^\star, \log{\frac{F_{sw}}{F_w}} \right \rangle_E \le \frac{\sqrt{2}}{\gamma} \cdot \sqrt{\kl(F_{sw}, F_w)}.$$
Substitute them into~\cref{eqn:temp_b3} and we can prove that:
\begin{align*}
    & \dist(F^\star, F_{sw}) \ge \dist\left( F^\star, F_w \right) - \underbrace{\frac{\sqrt{2}}{\gamma}}_{C_2} \sqrt{\kl(F_w, F_{sw})}, \\
    & \dist(F^\star, F_{sw}) \ge \dist\left( F^\star, F_w \right) - \underbrace{\frac{\sqrt{2}}{\gamma}}_{C_2} \sqrt{\kl(F_{sw}, F_w)}.
\end{align*}

The above inequalities also applies to $\dist(\cdot, \cdot)=\cross(\cdot, \cdot)$ because whether $\dist$ is $\kl$ or $\cross$, we have 
$$\dist(F^\star, F_{sw})-\kl(F^\star, F_{sw}) = \dist(F^\star, F_{w})-\kl(F^\star, F_{w}).$$

\end{proof}



\paragraph{Discussion of the constant.}
Recall that $C_1 = \frac{\sqrt{2}}{\gamma} \log \frac{1}{\gamma}$ and $C_2 = \frac{\sqrt{2}}{\gamma}$. 
In other words, $\gamma < \frac{1}{e}$ leads to $C_2 \le C_1$.
While $\gamma$ is the minimal value of the output, it is generally very small ($\gamma=10^{-3} \ll \frac{1}{e}$ in our experiments), i.e., $C_2 \le C_1$.
Therefore, the lower bound in~\cref{theorem:residue} is tighter than that in~\cref{lemma:upper_lower_inf}.





\paragraph{Further Discussion.}
We show that adding an additional assumption leads to $\dist(F^\star, F_{sw}) \ge \dist\left( F^\star, F_w \right) - \dist(F_w, F_{sw})$.
Particularly, if $\dist(F_w, F_{sw})$ can be improved to some extent, the constant $C$ and square root in~\cref{theorem:residue} can be eliminated, contributing to a more elegant version:
\begin{corollary}\label{corollary:residue_large_dp}
    Let $\dist$ to be $\kl$ or $\cross$.
    Let $R \ge 0$ and consider the same constant $C$ in~\cref{theorem:residue}.
    If $\dist(F_w, F_{sw}) \ge \sqrt{2}C$ is satisfied, then: 
    \begin{align*}
        \dist(F^\star, F_{sw}) \ge \dist\left( F^\star, F_w \right) - \kl(F_w, F_{sw}).
    \end{align*}
\end{corollary}

\cref{corollary:residue_large_dp} removes the constant coefficient and square root from~\cref{theorem:residue}.
Notably, if $R \ge 0$, the results above reinforce that the key bottleneck for performance improvement over $F_w$ arises from the optimization objective's inherent nature~\citep{yao2025understanding}: 
If $\dist(F_w, F_{sw})$ can be large, the performance improvement cannot exceed $\dist(F_w, F_{sw})$, which is exactly the minimum of~\cref{eqn:fsw-population-minimizer}.

\begin{proof}
We adopt an alternative proof technique in the proof of~\cref{theorem:residue}:
\begin{align*}
    \left| \left \langle F^\star, \left\vert F_{sw}-F_w \right\vert \right \rangle_E \right| & \le 2 \expect_x \tv \left( F_w(x), F_{sw}(x) \right) \tag{The derivations in~\cref{constant:theorem}} \\
    & \le 2 \expect_x \sqrt{1- \exp{\left[-\mathrm{D}_{\mathrm{KL}} \left( F_w(x), F_{sw}(x) \right)\right]}} \tag{Bretagnolle–Huber inequality} \\ 
    & \le 2 \sqrt{1- \exp{\left[-\expect_x \mathrm{D}_{\mathrm{KL}} \left( F_w(x), F_{sw}(x) \right)\right]}} \tag{Jensen’s inequality} \\ 
    & = 2 \sqrt{1- \exp{\left(-\kl(F_w, F_{sw})\right)}}. \tag{Definition of $\kl$}
\end{align*}

Let $u(t)=e^{-t} + \frac{\gamma^2}{4}t^2 - 1, t \ge 0$.
Taking the first-order and second-order derivative:
$u'(t)=-e^{-t} + \frac{\gamma^2}{2}t$, and $u''(t)=e^{-t} + \frac{\gamma^2}{2} > 0$.
While $u'(0)=-1<0$, $u'(\frac{2}{\gamma^2}) >0$, we know that there only exists a $t_0 \in (0,\frac{2}{\gamma^2})$ such that $u'(t_0)=0$.
And $u(t)$ decreases at $[0,t_0]$, increases at $(t_0, +\infty)$ and $u(0)=0$.
Denote $u(t^\star)=0$.
Notice that $u(\frac{2}{\gamma})=e^{-\frac{2}{\gamma}} > 0$, which means that $t^\star < \frac{2}{\gamma}$.
In other words, $t > \frac{2}{\gamma}$ leads to $u(t)>0$, i.e., $\sqrt{1-e^{-t}} \le \frac{\gamma}{2}t$.


Using the above results, if $\left \langle F^\star, \log{\frac{F_{sw}}{F_w}} \right \rangle_E \ge 0$ and $\kl(F_w, F_{sw}) \ge \frac{2}{\gamma}$, then 
\begin{align*}
    \left| \left \langle F^\star, \log{\frac{F_{sw}}{F_w}} \right \rangle_E \right| & \le \left| \frac{1}{\gamma} \cdot \left \langle F^\star, \left\vert F_{sw}-F_w \right\vert  \right \rangle_E \right| \tag{The derivations in~\cref{constant:theorem}} \\ 
    & \le \frac{2}{\gamma} \sqrt{1- \exp{\left(-\kl(F_w, F_{sw})\right)}} \\
    & \le \frac{2}{\gamma} \cdot \frac{\gamma}{2} \kl(F_w, F_{sw}) \\
    & = \kl(F_w, F_{sw}).
\end{align*}

The proof is complete.

\end{proof}





\subsection{Proof of~\cref{prop:general_equation}} \label{proof:general_equation}

\begin{proof}
We have
\begin{align*}
    \dist(F^\star, F_w) &= \bE_x \left[ \sum_{i=1}^k [F^\star(x)]_i \log \frac{[F^\star(x)]_i}{[F_w(x)]_i} \right] \\
    &= \bE_x \left[ \sum_{i=1}^k [F^\star(x)]_i \log \left( \frac{[F^\star(x)]_i}{[F_{sw}(x)]_i} \cdot \frac{[F_{sw}(x)]_i}{[F_w(x)]_i} \right) \right] \\
    &= \bE_x \left[ \sum_{i=1}^k [F^\star(x)]_i \log  \frac{[F^\star(x)]_i}{[F_{sw}(x)]_i} \right] + \bE_x \left[ \sum_{i=1}^k [F^\star(x)]_i \log  \frac{[F_{sw}(x)]_i}{[F_w(x)]_i} \right] \\
    &=\dist(F^\star, F_{sw}) + \left \langle F^\star, \log{\frac{F_{sw}}{F_w}} \right \rangle_E.
\end{align*}
Rearranging terms and we can prove the result.

The above also applies to $\dist(\cdot, \cdot)=\cross(\cdot, \cdot)$ because whether $\dist$ is $\kl$ or $\cross$, we have 
$$\dist(F^\star, F_{sw})-\kl(F^\star, F_{sw}) = \dist(F^\star, F_{w})-\kl(F^\star, F_{w}).$$
\end{proof}



\paragraph{Insights for reverse KL loss.}
Using similar decomposition technique, we obtain
\begin{align*}
    \dist(F_w, F^\star) = \dist(F_{sw}, F^\star) + \underbrace{\left \langle F_w-F_{sw}, \log{\frac{F_w}{F^\star}} \right \rangle_E - \dist(F_{sw}, F_w)}_{R_1}.
\end{align*}

Therefore, $\dist(F_{sw}, F^\star) \le \dist\left( F_w, F^\star \right)$ satisfies \textit{if and only if} $R_1 \ge 0$.
While the teacher-student disagreement is minimized in WTSG, we expect a small value of $\dist(F_{sw}, F_w)$.
Therefore, we want to obtain a large $\left \langle F_w-F_{sw}, \log{\frac{F_w}{F^\star}} \right \rangle_E$.
Intuitively, for any $x \in \cX$ and $i \in \{ 1, \cdots, k  \}$, we expect the model predictions to satisfy either of the two inequalities:
\begin{align}
    & [F_w(x)]_i \ge \max([F_{sw}(x)]_i ,[F^\star(x)]_i), \label{prop_2_ineq_1-r} \\ 
    & [F_w(x)]_i \le \min([F_{sw}(x)]_i ,[F^\star(x)]_i). \label{prop_2_ineq_2-r}
\end{align}
In other words, the predicted probabilities of $F_{sw}$ reflect the true outcome better than $F_w$.
The confidence level of $F_{sw}$ should be better aligned with $F^\star$ than that of $F_w$.




\paragraph{Insights for squared loss.} 
\citet{charikar2024quantifying} consider the squared loss:
$$\dist(f, g)=\mathbb{E}_{x \sim \mathcal{P}}(f(x)-g(x))^2.$$

In this setting, $\dist(f, g)=\dist(g, f)$ and we have
\begin{align*}
\dist\left(F_w, F^\star\right) & = \mathbb{E}_{x \sim \mathcal{P}}\left(F^\star(x)-F_w(x)\right)^2 \\
& = \mathbb{E}_{x \sim \mathcal{P}}\left(F^\star(x)-F_{sw}(x)+F_{sw}(x)-F_w(x)\right)^2 \\
& = \mathbb{E}_{x \sim \mathcal{P}}\left(F^\star(x)-F_{sw}(x)\right)^2+\mathbb{E}_{x \sim \mathcal{P}}\left(F_{sw}(x)-F_w(x)\right)^2 \\ & \hspace{2cm} +2 \cdot \mathbb{E}_{x \sim \mathcal{P}}\left[\left(F^\star(x)-F_{sw}(x)\right)\left(F_{sw}(x)-F_w(x)\right)\right] \\
& = \dist\left(F_{s w}, F^\star\right)+\dist\left(F_{s w}, F_w\right) +2 \cdot \mathbb{E}_{x \sim \mathcal{P}}\left[\left(F^\star(x)-F_{sw}(x)\right)\left(F_{sw}(x)-F_w(x)\right)\right].
\end{align*}

If we define
$$\left \langle f,g \right \rangle_S = 2 \cdot \mathbb{E}_{x \sim \mathcal{P}} \left[ f(x) \cdot g(x) \right],$$
then we have
\begin{align*}
    \dist\left(F_w, F^\star\right) = \dist\left(F_{s w}, F^\star\right)+\dist\left(F_{s w}, F_w\right) + \left \langle F^\star - F_{sw}, F_{sw}-F_w \right \rangle_S.
\end{align*}

Rearranging terms and we have
\begin{align} \label{suff_necc_condition_squared}
    \dist\left(F_{s w}, F^\star\right) = \dist\left(F_w, F^\star\right)-\dist\left(F_{s w}, F_w\right) - \left \langle F^\star - F_{sw}, F_{sw}-F_w \right \rangle_S.
\end{align}

Therefore, $\left \langle F^\star - F_{sw}, F_{sw}-F_w \right \rangle_S > 0$ is the sufficient and necessary condition for the inequality $$\dist(F_{sw}, F^\star) \le \dist(F_w, F^\star) - \dist(F_w, F_{sw}),$$
when $\dist$ is the squared loss.
Therefore, we should make the confidence level of $F_{sw}$ better aligned with $F^\star$.
Despite the difficulty to attain this objective, \citet{charikar2024quantifying} demonstrate that, within an elegant proof framework using convexity assumption, this condition is guaranteed to hold.












\subsection{Proof of~\cref{thm:realizable}} \label{theorem1_kl_loss}

\paragraph{Proof sketch.}
We define $\kl(\cdot, \cdot)$ in a Bregman-divergence manner.
To derive the desired properties, we construct a convex combination of the form $F_{sw}(x)+t(F^\star(x)-F_{sw}(x))$, where $t \to 0^+$.
By analyzing this construction, we show that the sum of the first-order term $\cO(t)$ and the second-order term $\cO(t^2)$ is non-negative.
This implies that the first-order term itself must also be non-negative.
Leveraging this principle and the associated derivations, we establish the proof of our results.


Our proof technique is general and unifying, covering \textit{both squared loss and KL divergence loss}. 
While Theorem 1-2 from~\citet{charikar2024quantifying} focus exclusively on squared loss in regression, and Theorem 3-4 from~\citet{yao2025understanding} restrict the analysis to KL divergence-like loss in regression, 
our work extends the scope to classification problems, encompassing both squared loss and KL divergence loss in a single framework. 
This broader applicability highlights the versatility of our proof and its potential to bridge gaps between regression and classification settings.
We recently became aware of concurrent work by~\citet{mulgund2025relating}, which has independently developed a theoretical framework employing a similar proof technique. As discussed in~\cref{discussion:concurrent}, while there are some conceptual overlaps, the core proof methodologies in our work differ significantly.



We first restate a lemma for our proof.
Let the strong model learns from $\cF_s:\R^{d_s} \to \R$ (which is a convex set) of fine-tuning tasks. 
Recall that we denote the strong model representation map by $h_s:\R^d \to \R^{d_s}$. Let $V_s = \{f \circ h_s: f \in \cF_s\}$ be the set of all tasks in $\cF_s$ composed with the strong model representation. 
Then $V_s$ is also a convex set.
\begin{lemma}[\citet{charikar2024quantifying}]
    \label{claim:Vs-convex}
    $V_s$ is a convex set.
\end{lemma}

\begin{proof}
Fix $f, g \in \cF_s$, and consider $f \circ h_s, g \circ h_s \in V_s$. Fix any $\lambda \in [0,1]$. Since $\cF_s$ is the linear function class so that it is a convex set, there exists $p \in \cF_s$ such that for all $y \in \R^{d_s}$, $p(y) = \lambda f(y) + (1-\lambda)g(y)$. Now, fix any $x \in \R^d$. Then, we have that
\begin{align*}
    \lambda (f \circ h_s)(x) + (1-\lambda)(g \circ h_s)(x) &= \lambda f(h_s(x)) + (1-\lambda)g(h_s(x))
    = p(h_s(x)) = (p \circ h_s)(x),
\end{align*}
and hence $\lambda (f \circ h_s) + (1-\lambda)(g \circ h_s) = p \circ h_s \in V_s$, which means that $V_s$ is also a convex set.
\end{proof}


We then present our theoretical results. 

\begin{proof}
    Fix $f, g \in \cF_s$, and consider $f \circ h_s, g \circ h_s \in V_s$. Fix any $\lambda \in [0,1]$. Since $\cF_s$ is the linear function class so that it is a convex set, there exists $p \in \cF_s$ such that for all $y \in \R^{d_s}$, $p(y) = \lambda f(y) + (1-\lambda)g(y)$. Now, fix any $x \in \R^d$. Then, we have that
    \begin{align*}
        \lambda (f \circ h_s)(x) + (1-\lambda)(g \circ h_s)(x) &= \lambda f(h_s(x)) + (1-\lambda)g(h_s(x))
        = p(h_s(x)) = (p \circ h_s)(x),
    \end{align*}
    and hence $\lambda (f \circ h_s) + (1-\lambda)(g \circ h_s) = p \circ h_s \in V_s$.
\end{proof}

Motivated by the definition of Bregman divergence, we consider $\dist$ as:
\begin{align}
    \dist(F_1, F_2) = \expect_x \left[ \phi(F_1(x))-\phi(F_2(x)) - \left \langle \nabla \phi(F_2(x)), F_1(x)-F_2(x) \right \rangle \right],
\end{align}
where $F_1, F_2 \in \cF$, and $\phi: \R^k \to \R$ is a strictly convex and differentiable function.
Note that squared loss and KL divergence loss are special cases of the definition of $\dist$ above:
\begin{align*}
    & \textbf{Squared loss:} \quad \dist(F_1, F_2)=\expect_x\|F_1(x)-F_2(x) \|_2^2, \quad \phi(x)=x^Tx, \\
    & \textbf{KL divergence loss:} \quad \dist(F_1, F_2)= \expect_x \sum_{i=1}^k [F_1(x)]_i \log \frac{[F_1(x)]_i}{[F_2(x)]_i}, \quad \phi(x)=\sum_{i=1}^k x_i \log x_i.
\end{align*}


Now we start our proof of~\cref{thm:realizable}.


% \begin{proof}
% Given any $g \in V_s$, observe that
% \begin{align*}
%     & \dist(F_w, g) = \expect_x \left[ \phi(F_w) - \phi(g) - \left \langle \nabla \phi(g), F_w-g \right \rangle \right], \\
%     & \dist(F_{sw}, g) = \expect_x \left[ \phi(F_{sw}) - \phi(g) - \left \langle \nabla \phi(g), F_{sw}-g \right \rangle \right], \\
%     & \dist(F_w, F_{sw}) = \expect_x \left[\phi(F_w) - \phi(F_{sw}) - \left \langle \nabla \phi(F_{sw}), F_w-F_{sw} \right \rangle \right],
% \end{align*}
% which means that
% \begin{align} \label{dist_expansion_bregman}
%     \dist(F_w, g) = \dist(F_{sw}, g) + \dist(F_w, F_{sw}) + \underbrace{\expect_x \left \langle F_{sw}(x)-F_w(x), \nabla \phi(g(x)) - \nabla \phi(F_{sw}(x)) \right \rangle}_{R_1}.
% \end{align}
% Now our goal is to prove that $R_1 \ge 0$.
% Recall that $f_{sw} = \argmin_{f \in \cF}\; \dist(F_w, f \circ h_s)$.
% In other words, $F_{sw}$ is the \textit{projection} of $F_w$ onto the convex set $V_s$, i.e., $\dist(F_w, g) \ge \dist(F_w, F_{sw})$.
% Substitute it into~\cref{dist_expansion_bregman} and we have
% \begin{align} \label{bregman:disc}
%     R_1 + \dist(F_{sw}, g) \ge 0.
% \end{align}



% \paragraph{Case 1: squared loss.}
% This loss function is considered by~\citep{charikar2024quantifying}. 
% Here we show that our proof is general and can recover the results in~\citep{charikar2024quantifying}.
% Let $g=F_{sw} + t(F^\star-F_{sw})$, $t \in (0,1)$.
% Consider $\phi(x)=x^Tx$, so $\nabla \phi(x)=2x$.
% There holds
% \begin{align*}
%     & R_1 = 2t \cdot \expect_x \left \langle F_{sw}(x)-F_w(x), F^\star(x)-F_{sw}(x) \right \rangle = \cO(t), \\
%     & \dist(F_{sw}, g) = t^2 \expect_x \| F^\star(x) - F_{sw}(x) \|_2^2 = \cO(t^2).
% \end{align*}
% Recall~\cref{bregman:disc} that $\cO(t) + \cO(t^2) \ge 0$, which means that $\cO(t) \ge 0$.
% Therefore, there holds $R_1 \ge 0$,
% which means 
% $$\expect_x \left \langle F_{sw}(x)-F_w(x), F^\star(x) - F_{sw}(x) \right \rangle \ge 0.$$
% Finally, let $g=F^\star$ in~\cref{dist_expansion_bregman} and we can prove the result $\dist(F_{sw}, F^\star) \le \dist(F_w, F^\star) - \dist(F_w, F_{sw})$.


% \paragraph{Case 2: KL divergence.}
% We consider KL divergence loss in this paper.
% Let $g=F_{sw} + t(F^\star-F_{sw})$, $t \in (0,1)$.
% Consider $\phi(x)=\sum_{i=1}^k x_i \log x_i$, so $\nabla \phi(x)= [\log x_1+1, \cdots, \log x_k+1]^T$.
% Firstly,
% \begin{align*}
%     R_1 & = \expect_x (F_{sw}(x)-F_w(x))^T 
%     \begin{bmatrix}
%      \log \left( 1+t\frac{[F^\star(x)]_1-[F_{sw}(x)]_1}{[F_{sw}(x)]_1} \right) \\
%      \vdots \\
%     \log \left( 1+t\frac{[F^\star(x)]_k-[F_{sw}(x)]_k}{[F_{sw}(x)]_k} \right)
%     \end{bmatrix} \\
%     & = \expect_x (F_{sw}(x)-F_w(x))^T 
%     \begin{bmatrix}
%      t \cdot \frac{[F^\star(x)]_1-[F_{sw}(x)]_1}{[F_{sw}(x)]_1} + \cO(t^2) \\
%      \vdots \\ t \cdot \frac{[F^\star(x)]_k-[F_{sw}(x)]_k}{[F_{sw}(x)]_k} + \cO(t^2) 
%     \end{bmatrix} \tag{Taylor expansion} \\
%     = & \cO(t).
% \end{align*}

% Secondly, we have
% \begin{align*}
%     & \dist(F_{sw}, g) \\ = & \expect_x \Bigg[ \sum_{i=1}^k [F_{sw}(x)]_i \log [F_{sw}(x)]_i - \sum_{i=1}^k [g(x)]_i \log [g(x)]_i - \left \langle 
%     \begin{bmatrix}
%     \log [g(x)]_1 + 1 \\
%     \vdots \\
%     \log [g(x)]_k + 1
%     \end{bmatrix}, 
%     F_{sw}(x)-g(x) \right \rangle \Bigg] \\
%     = & \expect_x \Bigg[ \sum_{i=1}^k [F_{sw}(x)]_i \log [F_{sw}(x)]_i - \sum_{i=1}^k [F_{sw}(x)]_i \log [F_{sw}(x) + t(F^\star(x)-F_{sw}(x))]_i \\ & \hspace{5cm} -
%  \sum_{i=1}^k t[F^\star(x)-F_{sw}(x)]_i \log [F_{sw}(x) + t(F^\star(x)-F_{sw}(x))]_i  \\ & \hspace{5cm} - \left \langle 
%     \begin{bmatrix}
%     \log [g(x)]_1 + 1 \\
%     \vdots \\
%     \log [g(x)]_k + 1
%     \end{bmatrix}, 
%     F_{sw}(x)-g(x) \right \rangle \Bigg] \\
%     = & \expect_x \Bigg[ - \sum_{i=1}^k [F_{sw}(x)]_i \log \left[1 + t \frac{[F^\star(x)]_i-[F_{sw}(x)]_i}{[F_{sw}(x)]_i}\right] - \sum_{i=1}^k t[F^\star(x)-F_{sw}(x)]_i \log [g(x)]_i  \\ & \hspace{5cm} - \left \langle 
%     \begin{bmatrix}
%     \log [g(x)]_1 \\
%     \vdots \\
%     \log [g(x)]_k
%     \end{bmatrix}, 
%     F_{sw}(x)-g(x) \right \rangle \Bigg] \tag{Since $\sum_{i=1}^k [F_{sw}(x)]_i = \sum_{i=1}^k [g(x)]_i = 1$} \\
%     = & \expect_x \Bigg[ - \sum_{i=1}^k [F_{sw}(x)]_i \log \left[1 + t \frac{[F^\star(x)]_i-[F_{sw}(x)]_i}{[F_{sw}(x)]_i}\right] - \cancel{\sum_{i=1}^k t[F^\star(x)-F_{sw}(x)]_i \log [g(x)]_i}  \\ & \hspace{4cm} + 
%     \cancel{\sum_{i=1}^k t [F^\star(x)-F_{sw}(x)]_i \log [g(x)]_i} \Bigg] \tag{Since $g=F_{sw} + t(F^\star-F_{sw})$} \\
%     = & \expect_x \Bigg[ - \sum_{i=1}^k [F_{sw}(x)]_i \log \left[1 + t \frac{[F^\star(x)]_i-[F_{sw}(x)]_i}{[F_{sw}(x)]_i}\right] \Bigg] \\
%     = & \expect_x \Bigg[ - \sum_{i=1}^k [F_{sw}(x)]_i \left[t \cdot \frac{[F^\star(x)]_i-[F_{sw}(x)]_i}{[F_{sw}(x)]_i} + \cO(t^2) \right] \Bigg] \tag{Taylor expansion} \\
%     = & \expect_x \Bigg[ - \sum_{i=1}^k \left[t \cdot [F^\star(x)]_i- t \cdot [F_{sw}(x)]_i + \cO(t^2) \right] \Bigg] \\
%     = & \cO(t^2). \tag{Since $\sum_{i=1}^k [F_{sw}(x)]_i = \sum_{i=1}^k [F^\star(x)]_i = 1$}
% \end{align*}

% Recall~\cref{bregman:disc} that 
% $$\underbrace{R_1}_{\cO(t)} + \underbrace{\dist(F_{sw}, g)}_{\cO(t^2)} \ge 0,$$
% which means that $R_1 \ge 0$.
% So we have $\dist(F_w, g) \ge \dist(F_{sw}, g) + \dist(F_w, F_{sw})$.
% Let $g=F^\star$ and we can prove the result $\dist(F_{sw}, F^\star) \le \dist(F_w, F^\star) - \dist(F_w, F_{sw})$.
% \end{proof}



\begin{proof}

We observe that
\begin{align*}
    & \dist(g, F_w) = \expect_x \left[ \phi(g) - \phi(F_w) - \left \langle \nabla \phi(F_w), g-F_w \right \rangle \right], \\
    & \dist(g, F_{sw}) = \expect_x \left[ \phi(g) - \phi(F_{sw}) - \left \langle \nabla \phi(F_{sw}), g-F_{sw} \right \rangle \right], \\
    & \dist(F_{sw}, F_w) = \expect_x \left[\phi(F_{sw}) - \phi(F_w) - \left \langle \nabla \phi(F_w), F_{sw}-F_w \right \rangle \right],
\end{align*}
which means that
\begin{align} \label{dist_expansion_bregman-rev}
    \dist(g, F_w) = \dist(g, F_{sw}) + \dist(F_{sw}, F_w) + \underbrace{\expect_x \left \langle g(x)-F_{sw}(x), \nabla \phi(F_{sw}(x)) - \nabla \phi(F_w(x)) \right \rangle}_{R_1}.
\end{align}

Now our goal is to prove that $R_1 \ge 0$.
We use reverse KL as the loss function in WTSG: $f_{sw} = \argmin_{f \in \cF}\; \dist(f \circ h_s, F_w)$.
In other words, $F_{sw}$ is the \textit{projection} of $F_w$ onto the convex set $V_s$, i.e., $\dist(g, F_w) \ge \dist(F_{sw}, F_w)$.
Substitute it into~\cref{dist_expansion_bregman-rev} and we have
\begin{align} \label{bregman:disc-rev}
    R_1 + \dist(g, F_{sw}) \ge 0.
\end{align}

\paragraph{Case 1: squared loss.}
Let $g=F_{sw} + t(F^\star-F_{sw})$, $t \in (0,1)$, $t \to 0^+$.
Consider $\phi(x)=x^Tx$, so $\nabla \phi(x)=2x$.
There holds
\begin{align*}
    & R_1 = 2t \cdot \expect_x \left \langle F_{sw}(x)-F_w(x), F^\star(x)-F_{sw}(x) \right \rangle = \cO(t), \\
    & \dist(g, F_{sw}) = t^2 \expect_x \| F^\star(x) - F_{sw}(x) \|_2^2 = \cO(t^2).
\end{align*}
Recall~\cref{dist_expansion_bregman-rev} that $\cO(t) + \cO(t^2) \ge 0$, which means that $\cO(t) \ge 0$.
Therefore, there holds $R_1 \ge 0$, which means
$$\expect_x \left \langle F^\star(x)-F_{sw}(x), \nabla \phi(F_{sw}(x)) - \nabla \phi(F_w(x)) \right \rangle \ge 0.$$
Let $g=F^\star$ in~\cref{dist_expansion_bregman-rev} and we can prove the result $\dist(F^\star, F_{sw}) \le \dist(F^\star, F_w) - \dist(F_{sw}, F_w)$.
While our proof is different from~\citet{charikar2024quantifying}, we obtain the same conclusion for squared loss.


\paragraph{Case 2: reverse KL divergence.}
We consider $\dist(\cdot, \cdot)=\kl(\cdot, \cdot)$.
Let $g=F_{sw} + t(F^\star-F_{sw})$, $t \in (0,1)$, $t \to 0^+$.
Consider $\phi(x)=\sum_{i=1}^k x_i \log x_i$, so $\nabla \phi(x)= [\log x_1+1, \cdots, \log x_k+1]^T$.
Firstly,
\begin{align*}
    R_1 = t \cdot \expect_x (F^\star(x)-F_w(x))^T 
    \begin{bmatrix}
     \log \frac{[F_{sw}(x)]_1}{[F_w(x)]_1} \\
     \vdots \\
    \log \frac{[F_{sw}(x)]_k}{[F_w(x)]_k}
    \end{bmatrix} = \cO(t).
\end{align*}

Secondly,
\begin{align*}
    \dist(g, F_{sw}) & = \expect_x \sum_{i=1}^k [g(x)]_i \log \frac{[g(x)]_i}{[F_{sw}(x)]_i} 
    \\ & = \expect_x \sum_{i=1}^k [F_{sw}(x) + t(F^\star(x)-F_{sw}(x))]_i \log \left( 1+ t \cdot \frac{[F^\star(x)-F_{sw}(x)]_i}{[F_{sw}(x)]_i} \right)
    \\ & = \expect_x \sum_{i=1}^k [F_{sw}(x) + t(F^\star(x)-F_{sw}(x))]_i \left( t \cdot \frac{[F^\star(x)-F_{sw}(x)]_i}{[F_{sw}(x)]_i} + \cO(t^2) \right) \tag{Taylor expansion}
    \\ & = \expect_x \sum_{i=1}^k [F_{sw}(x)]_i \left( t \cdot \frac{[F^\star(x)-F_{sw}(x)]_i}{[F_{sw}(x)]_i} + \cO(t^2) \right) + \cO(t^2) 
    \\ & = t \cdot \expect_x \sum_{i=1}^k [F^\star(x)-F_{sw}(x)]_i + \cO(t^2) 
    \\ & = \cO(t^2),
\end{align*}
where the last equation is because $\expect_x \sum_{i=1}^k [F^\star(x)]_i = \expect_x \sum_{i=1}^k [F_{sw}(x)]_i = 1$.
Therefore,
$$\underbrace{R_1}_{\cO(t)} + \underbrace{\dist(g, F_{sw})}_{\cO(t^2)} \ge 0,$$
which means $R_1 \ge 0$, i.e.,
$$\expect_x \left \langle F^\star(x)-F_{sw}(x), \nabla \phi(F_{sw}(x)) - \nabla \phi(F_w(x)) \right \rangle \ge 0.$$
Let $g=F^\star$ in~\cref{dist_expansion_bregman-rev} and we can prove the result $\dist(F^\star, F_{sw}) \le \dist(F^\star, F_w) - \dist(F_{sw}, F_w)$.
\end{proof}

% Note that if we use forward KL loss, then according to the above proof, there holds $\kl(F^\star, F_{sw}) \le \kl(F^\star, F_w) - \kl(F_{sw}, F_w)$, which is equivalent to
% $$\cross(F^\star, F_{sw}) \le \cross(F^\star, F_w) - \kl(F_{sw}, F_w).$$
% In other words, since $\kl$ is non-negative, the strong model provably performs better than the weak model.


\paragraph{Discussion of forward KL divergence.}
It is natural to ask, whether can the above proof technique be extended to forward KL?
Our answer is that, we may need an additional assumption.
In our proof, since reverse KL yields \textit{a linear term}, the proof can be carried through. However, forward KL introduces \textit{a logarithmic term}. 
While the Taylor expansions of the log function and a linear term differ only by a remainder term, proving the result requires assuming this remainder is non-negative, and that is why we need an additional assumption like Theorem 3 in~\citep{yao2025understanding}.
Here are the detailed explanations.

Note that
\begin{align} \label{dist_expansion_bregman-for}
    \dist(F_w, g) = \dist(F_{sw}, g) + \dist(F_w, F_{sw}) + \underbrace{\expect_x \left \langle F_w(x)-F_{sw}(x), \nabla \phi(F_{sw}(x)) - \nabla \phi(g(x)) \right \rangle}_{R_2}.
\end{align}
Our goal is to prove that $R_2 \ge 0$.
Now we use forward KL as the loss function in WTSG: $f_{sw} = \argmin_{f \in \cF}\; \dist(F_w, f \circ h_s)$.
In other words, $F_{sw}$ is the \textit{projection} of $F_w$ onto the convex set $V_s$, i.e., $\dist(F_w, g) \ge \dist(F_w, F_{sw})$.
Substitute it into~\cref{dist_expansion_bregman-for} and we have
\begin{align} \label{bregman:disc-for}
    R_2 + \dist(F_{sw}, g) \ge 0.
\end{align}

Again, let $g=F_{sw} + t(F^\star-F_{sw})$, $t \in (0,1)$, $t \to 0^+$.
Consider $\phi(x)=\sum_{i=1}^k x_i \log x_i$, so $\nabla \phi(x)= [\log x_1+1, \cdots, \log x_k+1]^T$.
Using a similar proof technique, we can obtain $R_2=\cO(t)$ and $\dist(F_{sw}, g)=\cO(t^2)$.
Therefore, we know that $R_2 \ge 0$, i.e.,
$$R_2=\expect_x \left \langle F_w(x)-F_{sw}(x), \underbrace{\nabla \phi(F_{sw}(x)) - \nabla \phi(F_{sw} + t(F^\star-F_{sw})(x))}_{\neq \nabla \phi(F_{sw}(x)) - \nabla \phi(F^\star(x))} \right \rangle \ge 0.$$
Consequently, even if we select $g=F^\star$ in~\cref{dist_expansion_bregman-for} and obtain
$$\dist(F_w, g) = \dist(F_{sw}, g) + \dist(F_w, F_{sw}) + \underbrace{\expect_x \left \langle F_w(x)-F_{sw}(x), \nabla \phi(F_{sw}(x)) - \nabla \phi(F^\star(x)) \right \rangle}_{R_3 \neq R_2}.$$
Since we do not know whether $R_3 \ge 0$ is satisfied, we cannot directly prove the desired result.
Since the difference between $R_2$ and $R_3$ can be quantified using exhaustive Taylor expansion,
the nature of proof is similar to the regression analysis of WTSG (Proof of Theorem 3 from~\citet{yao2025understanding}, which introduces an additional assumption for the remainder of Taylor expansion).
However, we do not know whether the remainder is larger than zero. In other words, to prove similar results for forward KL, we may introduce other assumptions like Theorem 3 in~\citet{yao2025understanding}.
In contrast, the success of reverse KL and squared loss is because $R_3 = t \cdot R_2$. In the proof for these reverse losses, if $R_2 \ge 0$, then there also holds $R_3 \ge 0$.
The above discussion indicates that our proof framework cannot be directly extended to the forward KL setting. 
We will explore how to address this limitation in future work.








% \subsection{Extension of~\cref{thm:realizable}} \label{proof_reverse_ce}

\paragraph{Extension to reverse cross entropy loss.}
To extend the proof to reverse cross entropy, consider the following theoretical result.

\begin{corollary}
\label{thm:realizable_cross}
Consider WTSG using reverse cross entropy loss:
\begin{align*}
    f_{sw} = \argmin_{f \in \cF_{s}}\; \cross(f \circ h_s, f_w \circ h_w).
\end{align*}
Assume that the function class $\cF_{s}$ is a convex set and $\exists f_s \in \cF_s$ such that $F_s = F^\star$.
Then:
\begin{align*}
    \cross(F^\star, F_{sw}) \le \frac{1}{2}\left(\cross(F^\star, F_w)-\kl(F_{sw}, F_w)\right) + \log k.
\end{align*}
\end{corollary}

If we consider binary classification (such as two famous datasets in AI safety: HH-RLHF~\citep{bai2022training} and CAI-Harmless~\citep{bai2022constitutional}), then $k=2$, making $\log k$ negligible due to the nature of KL divergence $\kl(\cdot, \cdot) \in [0, +\infty)$ and cross-entropy $\cross(\cdot, \cdot) \in [0, +\infty)$.
It shows that if we use reverse cross-entropy loss in WTSG, the strong model's performance is also probably better than weak model's performance, which is also validated in our experiments.


\begin{remark}
The proof also demonstrates that 
$$\cross(F^\star, F_{sw}) \le \cross(F^\star, F_w) - \kl(F_{sw}, F_w) - \epsilon,$$
where $\epsilon=\cross(F^\star,F_{sw}) - \log k$. Due to the same reason, we expect $\epsilon \ge 0$, which comes to the same conclusion.
\end{remark}







\begin{proof}

Rewrite~\cref{dist_expansion_bregman-rev} and we have
\begin{multline} \label{dist_expansion_ce}
    \cross(g, F_w) = \cross(g, F_{sw}) + \cross(F_{sw}, F_w) \\ + \underbrace{\expect_x \left( -H(F_{sw}(x)) + \left \langle g(x)-F_{sw}(x), \nabla \phi(F_{sw}(x)) - \nabla \phi(F_w(x)) \right \rangle \right)}_{R'_1}.
\end{multline}

If we use reverse cross-entropy as the loss function in WTSG: $f_{sw} = \argmin_{f \in \cF}\; \cross(f \circ h_s, F_w)$.
In other words, $\cross(g, F_w) \ge \cross(F_{sw}, F_w)$.
Let $g=F_{sw} + t(F^\star-F_{sw})$, $t \in (0,1)$, $t \to 0^+$. 
Substitute it into~\cref{dist_expansion_bregman-rev} and we have
\begin{align} \label{bregman:disc-ce}
    & R'_1 + \cross(g, F_{sw}) \ge 0, \nonumber \\ \Rightarrow & \underbrace{R_1}_{\cO(t)} + \underbrace{\dist(g, F_{sw})}_{\cO(t^2)} + \expect_x \left( H(g(x))-H(F_{sw}(x)) \right) \ge 0.
\end{align}
Note that 
\begin{align*}
    & \expect_x \left( H(g(x))-H(F_{sw}(x)) \right) 
    \\ = & \expect_x \sum_{i=1}^k [g(x)]_i \log [g(x)]_i - [F_{sw}(x)]_i \log [F_{sw}(x)]_i
    \\ = & \expect_x \sum_{i=1}^k [F_{sw}(x)]_i \log [g(x)]_i + t [F^\star(x)-F_{sw}(x)]_i \log [g(x)]_i - [F_{sw}(x)]_i \log [F_{sw}(x)]_i
    \\ = & \expect_x \sum_{i=1}^k [F_{sw}(x)]_i \log \frac{[g(x)]_i}{[F_{sw}(x)]_i}  + t [F^\star(x)-F_{sw}(x)]_i \log [g(x)]_i
    \\ = & \expect_x \sum_{i=1}^k [F_{sw}(x)]_i \log \left( 1+ t \cdot \frac{[F^\star(x)-F_{sw}(x)]_i}{[F_{sw}(x)]_i} \right)  + t [F^\star(x)-F_{sw}(x)]_i \log [g(x)]_i
    \\ = & \expect_x \sum_{i=1}^k [F_{sw}(x)]_i \left( t \cdot \frac{[F^\star(x)-F_{sw}(x)]_i}{[F_{sw}(x)]_i} + \cO(t^2) \right)  + t [F^\star(x)-F_{sw}(x)]_i \log [g(x)]_i
    \\ = & \expect_x \sum_{i=1}^k t \cdot [F^\star(x)-F_{sw}(x)]_i + \cO(t^2) + t [F^\star(x)-F_{sw}(x)]_i \log [g(x)]_i
    \\ = & \cO(t^2) + t \cdot \expect_x \sum_{i=1}^k [F^\star(x)-F_{sw}(x)]_i \log [g(x)]_i \tag{$\expect_x \sum_{i=1}^k [F^\star(x)-F_{sw}(x)]_i =0$}
    \\ = & \cO(t^2) + t \cdot  [\expect_x H(F_{sw}(x))-\cross(F^\star,F_{sw})] \tag{Definition of entropy and cross entropy},
\end{align*}
where the last inequality is because as $t \to 0^+$, $g \to F_{sw}$.
Consequently, recall~\cref{bregman:disc-ce}, we know that the sum of first-order terms $\cO(t)$ is non-negative, i.e.,
$$t \cdot  [\expect_x H(F_{sw}(x))-\cross(F^\star,F_{sw})] + R_1 \ge 0,$$
which means that
$$\expect_x H(F_{sw}(x)) - \cross(F^\star,F_{sw}) +  \expect_x\left \langle F^\star(x)-F_{sw}(x), \nabla \phi(F_{sw}(x)) - \nabla \phi(F_w(x)) \right \rangle \ge 0.$$

Let $g=F^\star$ in~\cref{dist_expansion_ce} and we obtain
\begin{align*}
    & \cross(F^\star, F_w) = \cross(F^\star, F_{sw}) + \cross(F_{sw}, F_w) - \expect_x H(F_{sw}(x)) \\ & \hspace{5cm} + \expect_x \left \langle F^\star(x)-F_{sw}(x), \nabla \phi(F_{sw}(x)) - \nabla \phi(F_w(x)) \right \rangle
    \\ \Rightarrow & \cross(F^\star, F_w) \ge \cross(F^\star, F_{sw}) + \cross(F_{sw}, F_w) - \expect_x H(F_{sw}(x)) \\  & \hspace{5cm}  + \cross(F^\star,F_{sw}) - \expect_x H(F_{sw}(x))
    \\ \Rightarrow & \cross(F^\star, F_w) \ge \cross(F^\star, F_{sw}) + \kl(F_{sw}, F_w) + \cross(F^\star,F_{sw}) - \expect_x H(F_{sw}(x))
    \\ \Rightarrow & \cross(F^\star, F_w) \ge \cross(F^\star, F_{sw}) + \kl(F_{sw}, F_w) + \cross(F^\star,F_{sw}) - \log k \tag{$H(F_{sw}(x)) \le \log k$}
\end{align*}

Therefore, we prove the result
$$\cross(F^\star, F_{sw}) \le \\ \frac{1}{2}\left(\cross(F^\star, F_w)-\kl(F_{sw}, F_w)\right) + \log k.$$
\end{proof}









\subsection{Proof of~\cref{thm:non-realizable-finite-samples}} \label{proof_non-realizable}

\paragraph{Proof sketch.}
By defining nine variables associated with given models, we substitute key components in the proof of~\cref{thm:realizable} to derive a set of inequalities among these variables.
Through a series of carefully designed transformations, we reformulate the triangle-like inequalities involving three remainder terms.
Ultimately, leveraging tools from statistical learning theory, several inequalities in information-theoretic analysis, and the properties of specific functions, we sequentially demonstrate that these three remainder terms become infinitesimal as $n \to \infty$ and $\epsilon \to 0$.


Let $\dist(\cdot, \cdot)$ be $\kl(\cdot, \cdot)$.
For a clear presentation, denote
\begin{align*}
    A &= \dist(F_s, F_{sw})\\
    B &=\dist(F_{sw}, F_w) \\
    C &= \dist(F_s, F_w)\\
    D &= \dist(F^\star, F_s) =\eps\\
    E &= \dist(F^\star, F_{sw}) \\
    F &= \dist(F^\star, F_w) \\
    G &= \dist(F^\star, \hat{F}_{sw}) \\
    H &= \dist(\hat{F}_{sw}, F_{sw}) \\
    I &= \dist(\hat{F}_{sw}, F_w).
\end{align*}

Now we start the proof of~\cref{thm:non-realizable-finite-samples}.
A uniform convergence result and two claims used in the proof are provided at the end of the proof.
The proof is strongly motivated by Theorem 4 in~\citet{yao2025understanding}.
While our work primarily focuses on classification, their Theorem 4 is specifically centered on regression.


\begin{proof}

Note that by virtue of the range of $f^\star, f_w$ and all functions in $\cF$ being absolutely bounded, and $\dist$ is also bounded.

Due to $F^\star \notin V_s$, we replace $F^\star$ with $F_s$ in the final step of proof of~\Cref{thm:realizable}, we obtain
\begin{align}
    C \ge A + B. \label{eqn:1}
\end{align}

Recall that $\left \langle f,g \right \rangle_E \triangleq \bE_{x \sim \cP} [f(x)^Tg(x)]$, which is used here for a clear presentation. So we have
\begin{align*}
    E & = A + D - \expect_x \sum_{i=1}^k ([F^\star(x)]_i-[F_s(x)]_i)\log \frac{[F_{sw}(x)]_i}{[F_s(x)]_i}
    \\ & = A + D - \underbrace{\left \langle F^\star-F_s, \log \frac{F_{sw}}{F_s} \right \rangle_E}_{t_1}.
\end{align*}
The $\log$ here is element-wise.
Using the similar notation, we have the following 
\begin{align}
    & E = A + D - \underbrace{\left \langle F^\star-F_s, \log \frac{F_{sw}}{F_s} \right \rangle_E}_{t_1}, \label{eqn:2}\\ 
    & F = C + D - \underbrace{\left \langle F^\star-F_s, \log \frac{F_w}{F_s} \right \rangle_E}_{t_2}, \label{eqn:2-1} \\
    & G = E - H - \underbrace{\left \langle \hat{F}_{sw}-F^\star, \log \frac{F_{sw}}{\hat{F}_{sw}} \right \rangle_E}_{t_3} \label{eqn:2-2}. 
\end{align}

Combining \eqref{eqn:1} and \eqref{eqn:2}, we get
\begin{align}
    E \le C + D - B - t_1. \label{eqn:3}
\end{align}

By a uniform convergence argument (\Cref{lem:uniform-convergence}), we have that with probability at least $1-\delta$ over the draw of $\{(x_1,y_1),\dots, (x_n,y_n)\}$ that were used to construct $\hat{F}_{sw}$,
\begin{align}
    I &\le B + \underbrace{\cO\left(\sqrt{\frac{\cC_{\cF_s}}{n}}\right)}_{t_4} + \underbrace{\cO\left(\sqrt{\frac{\log(1/\delta)}{n}}\right)}_{t_5}. \label{eqn:uc}
\end{align}

Combining \eqref{eqn:3} with \eqref{eqn:uc} and we have
\begin{align}
    E \le C + D - I - t_1 + t_4 + t_5. \label{eqn:4}
\end{align}

Combining \eqref{eqn:2-1} with \eqref{eqn:4} and we have
\begin{align}
    E \le F - I - t_1 + t_2 + t_4 + t_5. \label{eqn:5-1}
\end{align}

Combining \eqref{eqn:2-2} with \eqref{eqn:5-1} and we have
\begin{align}
    G \le F - I - H - t_1 + t_2 - t_3 + t_4 + t_5. \label{eqn:5}
\end{align}

% Because $F_{sw}$ is the projection of $F_w$ onto $V_s$, 
We replace $F^\star$ with $\hat{F}_{sw}$ in the final step of proof of~\Cref{thm:realizable} and obtain:
\begin{align}
    I \ge H + B. \label{eqn:uc-projection}
\end{align}

Combining \eqref{eqn:uc-projection} with \eqref{eqn:uc} and we have
\begin{align}
    0 \le H \le t_4 + t_5 = \cO\left(\sqrt{\frac{\cC_{\cF_s}}{n}}\right) + \cO\left(\sqrt{\frac{\log(1/\delta)}{n}}\right). \label{eqn:6}
\end{align}

Combining \eqref{eqn:6} with \eqref{eqn:5} and we have
\begin{align}
    G \le F - I - t_1 + t_2 - t_3 + t_4 + t_5. \label{eqn:7}
\end{align}

While $t_4$ and $t_5$ are known in~\eqref{eqn:uc}, we analyze $t_1$, $t_2$ and $t_3$ one by one.






\paragraph{Deal with $t_1$.}

We know that
\begin{align}
    t_1 & = \left \langle F^\star-F_s, \log \frac{F_{sw}}{F_s} \right \rangle_E. \nonumber
\end{align}
Using the fact that $\frac{F_{sw}(x)}{F_s(x)} \le \frac{1}{\gamma}$, we have
\begin{align} \label{def_t_1_ineq}
    |t_1| & \le \frac{1}{\gamma} \expect_x \sum_{i=1}^k \left| [F^\star(x)]_i - [F_s(x)]_i \right| \nonumber
    \\ & = \frac{2}{\gamma} \expect_x \tv(F^\star(x), F_s(x)) \tag{Definition of TV distance} \nonumber
    \\ & \le \frac{2}{\gamma} \expect_x \sqrt{\frac{1}{2}\mathrm{D}_{\mathrm{KL}}(F^\star(x) \| F_s(x))} \tag{Pinsker's inequality} \nonumber
    \\ & \le \frac{2}{\gamma} \sqrt{\frac{1}{2} \expect_x \mathrm{D}_{\mathrm{KL}}(F^\star(x) \| F_s(x))} \tag{Jensen’s inequality} \nonumber
    \\ & = \frac{2}{\gamma} \sqrt{\frac{1}{2} \dist(F^\star, F_s)} \tag{Definition of $\dist$} \nonumber
    \\ & = \frac{1}{\gamma} \sqrt{2 \varepsilon}
\end{align}

Therefore,
\begin{align} \label{equation_t_1}
    |t_1| = \cO(\sqrt{\varepsilon}).
\end{align}


\paragraph{Deal with $t_2$.}
The proof for $t_2$ is similar for $t_1$.
In particular, replacing $F_{sw}$ with $F_w$ in the above and we can get 
\begin{align} \label{equation_t_2}
    |t_2| = O(\sqrt{\varepsilon}).
\end{align}












\paragraph{Deal with $t_3$.}

We know that
$$t_3 = \left \langle \hat{F}_{sw}-F^\star, \log \frac{F_{sw}}{\hat{F}_{sw}} \right \rangle_E = \expect_x \sum_{i=1}^k ([\hat{F}_{sw}(x)]_i-[F^\star(x)]_i)\log \frac{[F_{sw}(x)]_i}{[\hat{F}_{sw}(x)]_i}.$$

According to~\cref{lem:uniform-convergence}, 
with probability at least $1-\delta$ over the draw of $(x_1,y_1),\dots,(x_n, y_n)$, we have
\begin{align} \label{t_3_proof_unif_conv}
    \left|\dist(\hat{F}_{sw}, F_w) - \dist(F_{sw}, F_w) \right| \le \cO\left(\sqrt{\frac{\cC_{\cF}}{n}}\right) + \cO\left(\sqrt{\frac{\log(1/\delta)}{n}}\right).
\end{align}
% According to~\cref{kl_triangle}, 
Notice that 
\begin{align} \label{t_3_proof}
H & = \dist(F_{sw}, \hat{F}_{sw}) \nonumber \\ & = \dist(F_w, F_{sw}) - \dist(F_w, \hat{F}_{sw}) + \left \langle F_w+F_{sw}, \log \frac{F_{sw}}{\hat{F}_{sw}} \right \rangle_E.
\end{align}
Substitute~\eqref{eqn:6} and~\eqref{t_3_proof_unif_conv} into~\cref{t_3_proof} with the triangle inequality for absolute values, we get
\begin{align*}
    \left| \left \langle F_w+F_{sw}, \log \frac{F_{sw}}{\hat{F}_{sw}} \right \rangle_E \right| \le \cO\left(\sqrt{\frac{\cC_{\cF}}{n}}\right) + \cO\left(\sqrt{\frac{\log(1/\delta)}{n}}\right)
\end{align*}
Since $|F_w(x)+F_{sw}(x)|$ is lower bounded, we have
$$\left| \left \langle \vec{1}, \log \frac{F_{sw}}{\hat{F}_{sw}} \right \rangle_E \right| \le \cO\left(\sqrt{\frac{\cC_{\cF}}{n}}\right) + \cO\left(\sqrt{\frac{\log(1/\delta)}{n}}\right).$$

Since $|\hat{F}_{sw}(x)-F^\star(x)|$ is upper bounded, there holds
\begin{align} \label{equation_t_3}
    |t_3| = \left| \left \langle \hat{F}_{sw}-F^\star, \log \frac{F_{sw}}{\hat{F}_{sw}} \right \rangle_E \right| \le \cO\left(\sqrt{\frac{\cC_{\cF}}{n}}\right) + \cO\left(\sqrt{\frac{\log(1/\delta)}{n}}\right).
\end{align}


Therefore, combing~\eqref{equation_t_1}, \eqref{equation_t_2} and \eqref{equation_t_3}, we have
\begin{align} \label{t_1_2_3_ineq}
    |t_1| + |t_2| + |t_3| \le O(\sqrt{\varepsilon}) + \cO\left(\sqrt{\frac{\cC_{\cF}}{n}}\right) + \cO\left(\sqrt{\frac{\log(1/\delta)}{n}}\right).
\end{align}


Finally, combing~\eqref{eqn:uc} and~\eqref{eqn:7} with \eqref{eqn:6} and~\eqref{t_1_2_3_ineq}, we get the result:
\begin{align*}
\dist(F^\star, \hat{F}_{sw}) \le \dist(F^\star, F_w) - \dist(\hat{F}_{sw}, F_w) + O(\sqrt{\eps}) + \cO\left(\sqrt{\frac{\cC_{\cF}}{n}}\right) + \cO\left(\sqrt{\frac{\log(1/\delta)}{n}}\right),
\end{align*}
where in the last inequality, we instantiate asymptotics with respect to $\eps \to 0$ and $n \to \infty$.

\end{proof}






Here are some tools used in the above proof.

\begin{lemma}[Uniform convergence]
\label{lem:uniform-convergence}
Let $(x_1,y_1),\dots,(x_n, y_n)$ be an i.i.d. training sample, where each $x_i \sim \cP$ and $y_i = F_w(x_i)$ for a target function $F_w$. For a fixed strong model representation $h_s$, we employ reverse KL loss in WTSG:
\begin{align*}
    & f_{sw} = \argmin_{f \in \cF_{s}}\; \dist(f \circ h_s, F_w) = \argmin_{f \in \cF_{s}} \; \bE_{x \sim \cP} \left[ \sum_{i=1}^k [f \circ h_s(x)]_i \log \frac{[f \circ h_s(x)]_i}{[F_w(x)]_i} \right],
    \\ & \hat{f}_{sw} = \argmin_{f \in \cF_s} \disthat(f \circ h_s, F_w) = \argmin_{f \in \cF_{s}} \; \frac{1}{n} \sum_{j=1}^n \left[ \sum_{i=1}^k [f \circ h_s(x_j)]_i \log \frac{[f \circ h_s(x_j)]_i}{[F_w(x_j)]_i} \right].
\end{align*}
Assume that the range of $F_w$ and functions in $\cF_s$ is absolutely bounded. Then, with probability at least $1-\delta$ over the draw of $(x_1,y_1),\dots,(x_n, y_n)$, we have
\begin{align*}
    \left|\dist(\hat{F}_{sw}, F_w) - \dist(F_{sw}, F_w) \right| \le \cO\left(\sqrt{\frac{\cC_{\cF_s}}{n}}\right) + \cO\left(\sqrt{\frac{\log(1/\delta)}{n}}\right),
\end{align*}
\end{lemma}
where $\cC_{\cF_s}$ is a constant capturing the complexity of the function class $\cF_s$.

\begin{proof}
The proof follows lemma 4 in~\citet{yao2025understanding}.
Swap the order of the two elements in $\dist(\cdot, \cdot)$ and $\hat{L}_\cP(\cdot, \cdot)$ in their proof and we can prove the result.
\end{proof}




\begin{claim}[\citet{yao2025understanding}] \label{claim_xlnx}
Let $f(x), g(x) \in [\gamma,1]$ where $\gamma>0$. If there exists $\xi>0$ such that $\int_{\cX} \left|f(x)-g(x) \right| d x \le \xi$,
then there holds
$$\int_{\cX} \left| \log f(x)- \log g(x) \right| d x \le \frac{1}{\gamma}\xi.$$
\end{claim}



\begin{claim}[\citet{yao2025understanding}] \label{claim_xlnx_reverse}
Let $f(x), g(x) \in [\gamma,1]$ where $\gamma>0$. 
If there exists $\xi>0$ such that $\int_{\cX} \left| \log f(x) - \log g(x) \right| d x \le \xi$,
then there holds
$$\int_{\cX} \left| f(x)- g(x) \right| d x \le \xi.$$
\end{claim}







\section{Additional Experimental Details and Results}

We first provide a detailed explanation of the evaluation metric.
To determine the effectiveness of a model $F$ in distinguishing between the selected and rejected completions ($y_c$ and $y_r$) for a given prompt $x$, we require that $F$ ranks the chosen completion higher than the rejected one.
This condition is formulated as $F(y_c)-F(y_r)>0$ for each pair $\Tilde{x}=(x;y_c,y_r)$, implying that $F(\Tilde{x})$ should exceed $0.5$.
Consequently, the test accuracy is defined as the fraction of instances where $F(\Tilde{x})>0.5$.




\subsection{Results of Pythia} \label{exp_result_pythia}

The overall trends observed in~\cref{fig:pythia} are similar with those in~\cref{fig:cai}. Our analysis of the results in~\cref{fig:pythia} further highlights those insights:
First, the accuracy exhibits a consistent upward trend from left to right, reinforcing the finding that the generalization capability of the strong model improves when a more capable weak model is utilized as the supervisor.
Second, the results demonstrate that in the majority of experimental settings (7 out of 12), reverse losses outperform forward losses, leading to stronger model performance.
Given the superior capabilities of the Pythia series compared to the GPT-2 series~\citep{biderman2023pythia}, as well as the fact that Pythia's strong ceiling model outperforms GPT-2, a key implication emerges. When the Pythia series serves as a weak model, it may generate less noise on non-target labels. As a result, the potential advantages of reverse losses are diminished, leading to only a slight improvement of reverse losses over forward losses.
Finally, across almost all of the settings (10 out of 12), the strong model trained with reverse KL and CE losses achieves superior performance compared to its weak supervisor. This observation is in full agreement with our theoretical predictions, further validating the effectiveness of reverse losses in enhancing model performance.









\begin{figure*}[t]
\begin{center}
\vspace{-10pt}
\subfigure[Results of Pythia-series on CAI-Harmless]{ 
\begin{minipage}[t]{0.95\linewidth}  \centerline{\includegraphics[width=1\linewidth]{images/harmless_pythia.pdf}}
\end{minipage}  
}  
\subfigure[Results of Pythia-series on helpful set of HH-RLHF]{
\begin{minipage}[t]{0.95\linewidth}
\centerline{\includegraphics[width=1\linewidth]{images/helpful_pythia.pdf}}
\end{minipage}  
}    
\caption{Results of Pythia-series. ``SC'' denotes the strong ceiling model, and ``A to B'' indicates the use of weak teacher ``A'' to supervise strong student ``B''. The terms CE, RCE, KL, and RKL refer to cross-entropy loss, reverse cross-entropy loss, forward KL divergence loss, and reverse KL divergence loss, respectively. Error bars represent the standard deviation across three runs of the experiment.}
\label{fig:pythia}
\end{center}
\vspace{-10pt}
\end{figure*}


\begin{figure*}[t]
\begin{center}
\vspace{-10pt}
\centerline{\includegraphics[width=0.9\linewidth]{images/harmless_gpt2_conf_loss.pdf}}
\caption{Results of GPT-2 series on CAI-Harmless. ``SC'' denotes the strong ceiling model, and ``A to B'' indicates the use of weak teacher ``A'' to supervise strong student ``B''. The terms ``Conf.\ CE'' and ``Reve.\ Conf.\ CE'' refer to the auxiliary confidence loss with vanilla cross-entropy loss (\cref{eq:confidence_loss}) and reverse cross-entropy loss (\cref{eq:reverse_confidence_loss}), respectively. Error bars represent the standard deviation across three runs of the experiment.}
\label{fig:conf_loss}
\end{center}
\vspace{-10pt}
\end{figure*}




\subsection{Auxiliary Confidence Loss} \label{exp:conf_loss}

As highlighted by~\citet{burns2023weak}, we explore an alternative approach: introducing an additional regularization term designed to enhance the strong model’s confidence in its predictions using standard cross-entropy loss, which is called ``Auxiliary Confidence Loss'' in~\citet{burns2023weak}:
\begin{align} \label{eq:confidence_loss}
    L_{\mathrm{conf}}(f)=(1-\alpha) \cdot \underbrace{\cross \left(F_w, f \circ h_s\right)}_{\text{vanilla cross-entropy loss}} + \; \alpha \cdot \underbrace{\cross\left(\hat{f}_t \circ h_s, f \circ h_s \right)}_{R(f)},
\end{align}
where $\alpha$ is the weight constant, $R(f)$ is the regularization term, and $\hat{f}_t$ corresponds to hardened strong model predictions using a threshold $t$, i.e., for any $x$:
\begin{align*}
    \hat{f}_t \circ h_s(x)= \mathbb{I}(f \circ h_s(x) > t) \in \{ 0,1 \},
\end{align*}
where $\mathbb{I}(\cdot)$ is the indicator function.
Rewrite~\cref{eq:confidence_loss} as the minimization objective in WTSG:
\begin{align}
    f_{sw} = \argmin_{f \in \cF_{s}}\; L_{\mathrm{conf}}(f).
\end{align}
% \begin{align*}
%     f_{sw} & = \argmin_{f \in \cF_{s}}\; L_{\mathrm{conf}}(f)
%     \\ & = \argmin_{f \in \cF_{s}}\; (1-\alpha) \cdot \cross \left(F_w, f \circ h_s\right)+\alpha \cdot \cross\left(\hat{f}_t \circ h_s, f \circ h_s \right).
    % \argmin_{f \in \cF_{s}}\; \cross \left(f \circ h_s, (1-\alpha)F_w + \alpha \cdot \hat{f}_t \circ h_s \right).
% \end{align*}
As advocated by~\citet{burns2023weak}, this regularization serves to mitigate overfitting to weak supervision, thereby improving the overall performance of the strong model.
Therefore, to further explore the advantage of reverse cross-entropy loss, we replace the vanilla cross-entropy with reverse cross-entropy in $L_{\mathrm{conf}}(f)$ and conduct WTSG using the following objective:
\begin{align} \label{eq:reverse_confidence_loss}
    f_{sw}^r & = \argmin_{f \in \cF_{s}}\; L^r_{\mathrm{conf}}(f) \nonumber
    \\ & = \argmin_{f \in \cF_{s}}\; (1-\alpha) \cdot \underbrace{\cross \left(f \circ h_s, F_w\right)}_{\text{reverse cross-entropy loss}}+ \; \alpha \cdot R(f).
\end{align}

We set $\alpha=0.2$ to ensure that the reverse/forward CE loss dominates the regularization, because we use a small batch size here and we want to reduce the negative impact of the randomness and instability brought by the auxiliary confidence loss within a single batch.
The experimental comparison between $f_{sw}$ and $f_{sw}^r$ is presented in~\cref{fig:conf_loss}.
First, by combining the observations from~\cref{fig:cai} and~\cref{fig:conf_loss}, we observe that the application of auxiliary confidence loss slightly enhances the performance of the strong model, consistent with the findings of~\citet{burns2023weak}.
Second, the use of reverse cross-entropy loss consistently enables the strong model to outperform its counterpart trained with standard cross-entropy loss. This finding, combined with previous experimental results in this work, highlights the superior effectiveness of reverse losses compared to forward losses.






