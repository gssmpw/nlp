\section{Theoretical Analysis: Justifying Reverse KL in WTSG}

In Sections~\ref{sec:universal}, we establish that both reverse and forward losses offer comparable generalization guarantees for the strong model, indicating that \textit{reverse losses is at least as favorable as forward losses in terms of theoretical properties}.
Furthermore, our analysis in Section~\ref{sec:upper} uncovers an advantage of reverse KL divergence loss: \textit{with reverse KL loss employed in WTSG, the strong model is theoretically guaranteed to outperform the weak model} by at least the magnitude of their disagreement under some assumptions.



\subsection{Generalization Analysis of Both Losses} \label{sec:universal}

We establish that both reverse and forward losses yield comparable generalization guarantees by deriving upper and lower bounds for their respective generalization errors.
We begin with a universal result for both forward and reverse losses.

\paragraph{Upper and lower bounds.}
We extend~\citet{yao2025understanding} and establish bounds of strong model's performance.
Unlike most previous work that focuses only on forward KL and CE loss, we comprehensively examine all four loss variants: forward KL, reverse KL, forward CE, and reverse CE.

\begin{lemma}[Proved in \cref{proof_lemma_inf}] \label{lemma:upper_lower_inf}
Let $\dist(\cdot, \cdot)$ be $\kl(\cdot, \cdot)$ or $\cross(\cdot, \cdot)$.
Given the data domain $\cX$, output domain $\cY$ and models $F_w, F^\star$ defined above. 
For any strong model $F_{sw}$, there holds
\begin{multline*}
     \dist(F^\star,F_w)-C_1 d(F_w, F_{sw}) \\ \le \dist(F^\star, F_{sw}) \le \\ \dist(F^\star,F_w)+ C_1 d(F_w, F_{sw}),
\end{multline*}
where $C_1$ is a positive constant, $d(F_w, F_{sw})$ can be $\sqrt{\kl(F_w, F_{sw})}$ or $\sqrt{\kl(F_{sw}, F_w)}$, and $\dist(F^\star,F_{sw})$ and $\dist(F^\star,F_w)$ represent the error of strong model and weak model, respectively.
\end{lemma}

Note that $d(F_w, F_{sw})$ captures the disagreement between the strong and weak models, which serves as the minimization objective in WTSG. 
\cref{lemma:upper_lower_inf} quantifies the difference between the weak and strong models' performance from two perspectives: a lower bound and an upper bound, which is similar to~\citet{yao2025understanding}.
The \textbf{lower bound} indicates that strong model's performance cannot be arbitrarily improved using weak supervision. 
Improving the strong model depends critically on ensuring $\dist\left( F^\star, F_w \right)$ is small, underscoring \textit{the importance of weak model's performance}.
Also, whether we choose forward or reverse loss, the student-supervisor disagreement $d(F_w, F_{sw})$ is minimized. 
While reducing $\dist\left( F^\star, F_{sw} \right)$ requires increasing $d(F_w, F_{sw})$, the lower bound also implies that strong model's performance gain may be inherently constrained by WTSG's own optimization objective~\citep{yao2025understanding}.
In other words, \textit{achieving the minimal optimization objective limits the strong model’s ability} to significantly outperform its weak supervisor.
The \textbf{upper bound} ensures that strong model's error $\dist(F^\star, F_{sw})$ remains bounded and do not be arbitrarily large.
It shows that a better weak model is also crucial to improve strong model's performance.
Building on these universal results for both forward and reverse losses, we further conduct a fine-grained analysis to investigate how to achieve tighter lower and upper bounds.


\paragraph{Tighter lower bound.}
Consider the lower bound in~\cref{lemma:upper_lower_inf}, we employ alternative proof techniques rooted in information-theoretic inequalities to derive a tighter lower bound.

\begin{theorem}[Proved in~\cref{constant:theorem}] \label{theorem:residue}
Let $\dist(\cdot, \cdot)$ be $\kl(\cdot, \cdot)$ or $\cross(\cdot, \cdot)$.
Given $F_{sw}, F_w, F^\star$, then
\begin{align*}
    & \dist(F^\star, F_{sw}) \ge \dist(F^\star, F_w) - C_2 d(F_w, F_{sw}),
\end{align*}
where $C_2$ is a positive constant, and $d(F_w, F_{sw})$ can be $\sqrt{\kl(F_w, F_{sw})}$ or $\sqrt{\kl(F_{sw}, F_w)}$.
\end{theorem}

\begin{remark}
$C_2$ is generally smaller than $C_1$, leading to a tighter lower bound than~\cref{lemma:upper_lower_inf}.
\end{remark}

Similar to~\cref{lemma:upper_lower_inf}, it also highlights the importance of selecting a well-generalizing weak model and cautious optimization of the strong model to prevent overfitting to weak supervision.
Note that~\cref{theorem:residue} applies to both forward and reverse losses, which share the same theoretical properties.




\paragraph{Tighter upper bound.}
In~\cref{lemma:upper_lower_inf},
there is no theoretical guarantee that the strong model will necessarily surpass the performance of its weak supervisor in WTSG, such as $\dist(F^\star, F_{sw}) \le \dist\left( F^\star, F_w \right)$.
This raises the question of whether a tighter upper bound can be derived.
Therefore, we first explore how to achieve this goal.

\begin{proposition}[Proved in~\cref{proof:general_equation}] \label{prop:general_equation}
Let $\dist(\cdot, \cdot)$ be $\kl(\cdot, \cdot)$ or $\cross(\cdot, \cdot)$. Given $F_{sw}, F_w, F^\star$, then there holds
\begin{align*}
    \dist(F^\star, F_{sw}) = \dist(F^\star, F_w) - \underbrace{\left \langle F^\star, \log{\frac{F_{sw}}{F_w}} \right \rangle_E}_{R},
\end{align*}
where the expectation inner product is defined as $\left \langle f,g \right \rangle_E \triangleq \bE_{x \sim \cP} [f(x)^Tg(x)]$.
\end{proposition}

\begin{remark}
    It can also be extended to reverse KL and squared loss, as detailed in~\cref{proof:general_equation}.
\end{remark}


Therefore, $\dist(F^\star, F_{sw}) \le \dist\left( F^\star, F_w \right)$ satisfies \textit{if and only if} $R \ge 0$.
To achieve it, we aim to establish a clear relationship between model capacity and model confidence across all data points and all $k$ classes.
Specifically, for any $x \in \cX$ and $i \in \{ 1, \cdots, k  \}$, a positive $[F^\star(x)]_i \log{\frac{[F_{sw}(x)]_i}{[F_w(x)]_i}}$ ensures a positive $R$.
Therefore, we expect the model predictions to satisfy either of the two inequalities:
\begin{align}
    & [F^\star(x)]_i \ge [F_{sw}(x)]_i \ge [F_w(x)]_i, \label{ineq:1} \\ 
    & [F^\star(x)]_i \le [F_{sw}(x)]_i \le [F_w(x)]_i. \label{ineq:2}
\end{align}
In other words, the predicted probabilities of $F_{sw}$ reflect the true outcome better than $F_w$.
Intuitively, because the weak model is pre-trained and fine-tuned on ground truth data, we can trust its decisions for major classes.
As shown in~\cref{fig:comparison_kl}, reverse KL’s mode-seeking behavior encourages the strong model to focus on the weak model’s high-confidence predictions, while disregarding low-probability, potentially noisy regions. 
This behavior facilitates the fulfillment of Inequality~\eqref{ineq:1}-\eqref{ineq:2}. 
In contrast, forward KL, with its mass-covering nature, forces the strong model to match the entire probability distribution, including unreliable signals from the weak model’s lower-probability classes, thereby hindering the fulfillment of the above inequalities. 
In the context of WTSG, where weak supervision is imperfect, reverse KL’s focus on high-confidence decisions provides stronger guarantees for strong model's performance. 
In particular, the theoretical analysis in the following section further supports this, demonstrating that reverse KL can theoretically ensure superior performance for the strong model in certain settings.

% Although the exact conditions for achieving this remain uncertain, we show in the following section that \textit{under certain assumptions, using reverse KL loss can ensure that} $R \ge 0$?
% As an example, consider the labeling function $F^\star$ that provides hard labels.
% Let the $j$-th class be the ground truth, i.e., $[F^\star(x)]_j = 1$ and $[F^\star(x)]_i = 0, i \ne j$.
% \begin{enumerate}
%     \item For the $j$-th index $[F^\star(x)]_j=1$, we expect that 
%     $[F_w(x)]_j \le [F_{sw}(x)]_j \le 1$
%     \item For other indices $i \ne j$, we have $[F^\star(x)]_i=0$. Thus, we expect that $0 \le [F_{sw}(x)]_i \le [F_w(x)]_i$.
% \end{enumerate}
% Thus, the confidence level of $F_{sw}$ should be better aligned with $F^\star$.






\subsection{Unique Advantage of Reverse Losses} \label{sec:upper}

To achieve a tighter upper bound, our theoretical analysis below yields an intriguing insight: \textit{when using reverse KL in WTSG, an adequate pre-training and subsequent last linear layer fine-tuning guarantees that the strong student can outperform its weak teacher} by at least the magnitude of their disagreement (i.e., $R \ge 0$ in~\cref{prop:general_equation}).

\begin{theorem}[Proved in \cref{theorem1_kl_loss}]
\label{thm:realizable}
% Let $\dist(\cdot, \cdot)$ be $\kl(\cdot, \cdot)$.
Consider WTSG using reverse KL divergence loss:
\begin{align*}
    f_{sw} = \argmin_{f \in \cF_{s}}\; \kl(f \circ h_s, F_w).
\end{align*}
Denote $F_{sw}=f_{sw} \circ h_s$.
Assume that the function class $\cF_{s}$ is a convex set and $\exists f_s \in \cF_s$ such that $f_s \circ h_s = F^\star$.
Then:
\begin{align*}
    \kl\left(F^\star, F_{sw}\right) \leq \kl\left(F^\star, F_w\right)-\kl\left(F_{sw}, F_w\right).
\end{align*}
\end{theorem}


\begin{remark}
Similar result can be naturally extended to reverse CE loss.
Our proof leverages Bregman divergence, a generalization of both squared loss and KL divergence. 
This approach not only broadens the applicability of our results but also demonstrates how our framework naturally recovers the regression analysis~\citep{charikar2024quantifying}.
% On the contrary, under this proof framework, employing forward KL or CE losses does not inherently offer such theoretical guarantees unless we introduce additional assumptions.
The concurrent work~\citep{mulgund2025relating} also independently explores the application of Bregman divergence in this context, establishing their Theorem 4.1 and Corollary 4.2, which exhibit parallels with our results.
The above extension and discussion are detailed in~\cref{theorem1_kl_loss}.
\end{remark}


The assumptions are consistent with previous theory~\citep{charikar2024quantifying,yao2025understanding}.
Firstly, the convex set assumption includes the case that $\cF_s$ is the class of all linear functions, which shares similar conceptual framework of last linear layer fine-tuning~\citep{howard2018universal,kumar2022fine,mao2023last,kirichenko2023last}.
Secondly, we consider the case where $\exists f_s \in \cF_s$ such that $f_s \circ h_s = F^\star$.
It shows the remarkable capability of pre-training. 
It assumes that the representation $h_s$ has already captured a wealth of information during pre-training, a phenomenon well-demonstrated by modern pre-trained LLMs~\citep{touvron2023llama,achiam2023gpt}.

\cref{thm:realizable} establishes that in WTSG, using the reverse KL divergence loss guarantees that the strong model, trained with weak supervision, surpasses the weak model by at least their disagreement, $\kl(F_{sw}, F_w)$.
This upper bound is tighter than~\cref{lemma:upper_lower_inf}, as~\cref{lemma:upper_lower_inf} does not ensure that the strong model surpasses the weak model.
\cref{thm:realizable} highlights the superior theoretical benefits of reverse losses compared to forward losses.


Now we draw $n$ i.i.d. samples to perform WTSG and relax the assumption, where for any $f_s \in \cF_s$, $\exists f_s \circ h_s = F^\star$ may not be satisfied.
The unique result for reverse KL below further emphasizes its advantageous theoretical properties in WTSG.

\begin{theorem}[Proved in~\cref{proof_non-realizable}] \label{thm:non-realizable-finite-samples}
% Let $\dist(\cdot, \cdot)$ be $\kl(\cdot, \cdot)$.
Given $F_{sw}$ defined in~\cref{thm:realizable}.
Assume that $\cF_s$ is a convex set.
Consider WTSG using reverse KL divergence loss with $n$ i.i.d. samples:
% & f_{sw} = \argmin_{f \in \cF_{s}}\; \dist(f \circ h_s, f_w \circ h_w),
\begin{align*}
    \hat{f}_{sw} = \argmin_{f \in \cF_{s}}\; \widehat{\kl}(f \circ h_s, F_w),
\end{align*}
where $\widehat{\kl}(\cdot, \cdot)$
% $\widehat{\kl}(\cdot, \cdot) = \frac{1}{n} \sum_{j=1}^n \mathrm{D}_{\mathrm{KL}}(\cdot \| \cdot)$
is the empirical version of $\kl(\cdot, \cdot)$.
% on $n$ samples. 
% where $\widehat{\kl}(f \circ h_s, f_w \circ h_w)$ is defined as
% $$\frac{1}{n} \sum_{j=1}^n \mathrm{D}_{\mathrm{KL}}(f \circ h_s(x_j) \| f_w \circ h_w(x_j)).$$
Denote $\hat{F}_{sw}=\hat{f}_{sw} \circ h_s$ and strong ceiling model's generalization error $\eps = \kl(F^\star, F_s)$.
With probability at least $1-\delta$, there holds
\begin{align*} 
& \kl(F^\star, \hat{F}_{sw}) \le \kl(F^\star, F_w) - \kl(\hat{F}_{sw}, F_w) \\ & \hspace{0.05cm} + \cO(\sqrt{\eps}) +  \cO\left(\sqrt{\frac{\cC_{\cF_s}}{n}}\right) + \cO\left(\sqrt{\frac{\log(1/\delta)}{n}}\right),
\end{align*}
where $\cC_{\cF_s}$ is a constant capturing the complexity of the function class $\cF_s$.
The asymptotic notation is for $\eps \to 0, n \to \infty$.
\end{theorem}


Compared to~\cref{thm:realizable}, this bound introduces two additional error terms: the first term $\cO(\sqrt{\eps})$ is small due to the capability of the strong ceiling model $F_s$.
The remaining two error terms, which are of the order $\cO \left( 1/\sqrt{n} \right)$, stem from the strong model $\hat{F}_{sw}$ being trained on a finite weakly-labeled dataset. 
These terms also diminish asymptotically as the sample size $n$ increases.
Overall, by using a sufficiently large dataset and a strong model with enough capacity, we achieve a large $n$ and a very small $\eps$, rendering the remainders in \cref{thm:non-realizable-finite-samples} negligible and increasing the likelihood that the theoretical guarantee in \cref{thm:realizable} holds.
\cref{thm:non-realizable-finite-samples} aligns with previous wisdom~\citep{charikar2024quantifying,yao2025understanding}. 
However, whereas their corresponding bounds are specifically designed for regression tasks, our result offers new insights into applying reverse KL loss in classification tasks.








