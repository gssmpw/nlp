\section{Preliminaries}

\subsection{Classification}
We consider $k$-classification tasks.
Given the data domain $\cX \subseteq \R^d$ and output domain $\cY \subseteq \R^k$, let the model space be $\cF: \cX \to \cY$. 
Consider the model's outputs form a valid probability distribution, i.e., $\forall y = (y_1, \cdots, y_k)^T \in \cY$, there holds $\sum_{i=1}^k y_i=1$ and $0 < y_i \le 1$.
% Consider the model equipped with a softmax module, which ensures that its outputs form a valid probability distribution, i.e., $\forall y = (y_1, \cdots, y_k)^T \in \cY$, there holds $\sum_{i=1}^k y_i=1$ and $0 < y_i \le 1$.
The forward and reverse KL divergence losses are defined below.
\begin{definition}[KL divergence losses]
Given the data distribution $\cP$ and two models $g,h \in \cF$, the \textbf{forward} KL divergence loss is defined as:
\begin{align*} 
\kl(g,h) & \triangleq \bE_{x \sim \cP} \left[ \mathrm{D}_{\mathrm{KL}}(g(x) \| h(x)) \right], \\ 
& = \bE_{x \sim \cP} \left[ \sum_{i=1}^k [g(x)]_i \log \frac{[g(x)]_i}{[h(x)]_i} \right],
\end{align*}
where $[g(x)]_i, [h(x)]_i$ represent the $i$-th elements of $g(x), h(x)$, respectively. 
Thus, the \textbf{reverse} KL divergence loss is $\kl(h,g)$.
\end{definition}

As illustrated in~\cref{fig:comparison_kl}, forward KL promotes full coverage of the target distribution, whereas reverse KL focuses on capturing the dominant mode.
Additionally, the difference between KL divergence and CE is an entropy term:
\begin{definition}[Cross-entropy losses]
Given the data distribution $\cP$ and two models $g,h \in \cF$, define the \textbf{forward} cross-entropy divergence loss:
\begin{align*} 
\cross(g,h) & \triangleq -\bE_{x \sim \cP} \left[ \sum_{i=1}^k [g(x)]_i \log [h(x)]_i \right] \\ & = \kl(g,h) + \bE_{x \sim \cP} \; H(g(x)),
\end{align*}
where $H(\cdot)$ is the Shannon entropy.
Thus, the \textbf{reverse} cross-entropy loss is $\cross(h,g)$.
\end{definition}

Consequently, note that when minimizing forward losses, the model $g$ is fixed to provide supervision signals. Thus, minimizing forward KL divergence loss is equivalent to minimizing standard CE loss as $\bE_{x \sim \cP} \; H(g(x))$ is a constant.





\subsection{Weak-to-Strong Generalization}


Consider WTSG in the context of $k$-classification tasks. We focus on the fine-tuning phase after pre-training.
The labeling function $F^\star$ maps data $x$ to its label $F^\star(x)$.
% $F^\star = f^\star \circ h^\star$, where $h^\star$ is a fully enriched representation and $f^\star$ is the fine-tuning task.
% Given the weak model $F_w = f_w \circ h_w$, where $h_w$ is the weak model representation, and $f_w$ .
The strong model aims to learn $F_{sw} = f \circ h_s$, where $h_s$ is a fixed strong model representation and $f \in \cF_{s}$ is a task-specific function from a hypothesis class $\cF_{s}$.
In the convention setting of AI alignment~\citep{ouyang2022training}, the model is fine-tuned through ground truth data:
\begin{align}
    \label{eqn:alignment-population-minimizer}
    f_{s} = \argmin_{f \in \cF_{s}}\; \dist(F^\star, f \circ h_s),
\end{align}
where the loss $\dist(\cdot, \cdot)$ can be $\kl(\cdot, \cdot)$ or $\cross(\cdot, \cdot)$.
However, 
% with recent advancements in LLMs~\citep{achiam2023gpt,dubey2024llama,guo2025deepseek}, future superhuman models are expected to surpass human intelligence. 
it is humans who provide weak supervision in the super-alignment scenario~\citep{openai_superalignment}. 
% This raises a critical question: Can supermodels, trained on weak human supervision, fully realize their potential while maintaining alignment with human values? 
To explore this, the WTSG framework~\citep{burns2023weak} leverages a weak modelâ€™s predictions to supervise the strong model:
\begin{align}
    \label{eqn:fsw-population-minimizer}
    f_{sw} = \argmin_{f \in \cF_{s}}\; \dist(F_w, f \circ h_s),
\end{align}
where $F_w$ is a given weak model, and $\dist(\cdot, \cdot)$ is originally the standard CE loss.
If we employ reverse losses, the objective transforms into
\begin{align}
    f_{sw}^{r} = \argmin_{f \in \cF_{s}}\; \dist(f \circ h_s, F_w).
\end{align}
% the loss function transforms into $\dist(f \circ h_s, F_w)$.
Regardless of the choice of loss function, the core objective is replacing ground truth data with weak supervision.
Thus, while minimizing forward losses $\dist(F_w, F_{sw})$ or reverse losses $\dist(F_{sw}, F_w)$, we simultaneously strive to achieve an $F_{sw}$ with a small generalization error $\dist(F^\star, F_{sw})$.





