\section{Related Work}

% \noindent \textbf
\paragraph{Weak-to-Strong Generalization.}
The weak-to-strong paradigm~\citep{burns2023weak} emerges as a promising framework to address the challenges of AI alignment, particularly in the context of superalignment~\citep{openai_superalignment}â€”where future AI systems may surpass human capabilities, rendering human supervision weak or insufficient. 
% Traditional alignment methods, such as Reinforcement Learning from Human Feedback (RLHF)~\citep{ziegler2019fine,bai2022training} and Direct Preference Optimization (DPO)~\citep{rafailov2024direct}, rely heavily on high-quality human feedback, which becomes increasingly difficult to scale as AI systems grow more advanced~\citep{casper2023open, kim2024road}.
% In contrast, WTSG offers an alternative paradigm by 
It leverages weaker models to guide stronger models, potentially unlocking their full capabilities while maintaining alignment with human values.
It has been extensively studied through algorithms~\citep{zhu2024weak,agrawal2024ensemw2s,sang2024improving,guo2024improving}, empirical analyses~\citep{yang2024super,ye2024weak}, and theoretical frameworks~\citep{lang2024theoretical,somerstep2024statistical,wu2024provable,charikar2024quantifying,yao2025understanding}, these works primarily focus on WTSG with forward KL divergence and CE losses. 
However, the potential of reverse KL and reverse CE losses in classification under the WTSG framework has not been sufficiently explored~\textsuperscript{\textdagger}
\footnote{\textdagger \ \ Recently, we discovered that concurrent, independent efforts~\citep{mulgund2025relating} have also theoretically explored reverse losses in classification within the WTSG framework via information geometry and convex analysis. Due to space limitations, a more detailed comparison between our approach and theirs is deferred to~\cref{discussion:concurrent}.}.
This gap motivates us to systematically investigated the benefits of these reverse losses in WTSG with both theoretical insights and empirical observations.







\paragraph{Forward KL and Reverse KL.}
Forward KL and Reverse KL are employed in distinct applications, each offering unique advantages.
\textit{Forward KL} is widely utilized in standard classification tasks~\citep{goodfellow2016deep}, often appearing in the form of CE loss to align predicted and true label distributions.
Its mass-covering behavior~\citep{jerfel2021variational,sun2024inverse} ensures that the model comprehensively captures all high-probability regions of the target distribution, making it particularly effective in knowledge distillation~\citep{hinton2015distilling} for classification tasks.
In such tasks, the teacher model's soft labels provide informative guidance, enabling the student model to learn a representative distribution~\citep{yang-etal-2025-distilling}.
In contrast, \textit{reverse KL} is frequently adopted in variational inference~\citep{kingma2014auto,pinheiro2021variational}, where it exhibits zero-forcing behavior~\citep{minka2005divergence}.
By focusing on high-confidence predictions while disregarding low-probability regions, reverse KL prioritizes precision over diversity.
% This characteristic makes it particularly suitable for tasks where capturing the most reliable patterns is more critical than covering the entire distribution.
In the context of WTSG, the choice of divergence is especially significant.
Weak teachers in WTSG provide imperfect supervision signals~\citep{burns2023weak,yang2024super,yao2025understanding}, and using forward KL divergence as the loss function may lead to overfitting to these noisy or incomplete guidance.
Reverse KL, on the other hand, allows the strong model to extract reliable patterns from weak supervision without being overly constrained by its imperfections.
This property aligns well with the goal of WTSG, where the focus is on leveraging weak supervision while avoiding its pitfalls.

Furthermore, reverse KL divergence has recently gained increasing attention in related fields such as domain adaptation~\citep{nguyen2021kl} and KL-regularized reinforcement learning~\citep{rafailov2024direct,wang2023beyond,ji2024towards}.
These applications share a conceptual similarity with WTSG, as they all involve transferring knowledge across domains or models under imperfect or constrained conditions.
Moreover, beyond classification tasks, reverse KL divergence has been increasingly utilized in generation tasks within knowledge distillation~\citep{gu2024minillm,agarwal2024policy,wu2024rethinking}, owing to its mode-seeking properties.
Given these developments, it is natural to investigate the role of reverse KL in classification within the WTSG framework.
% To the best of our knowledge, no prior work has systematically explored this direction, leaving a significant gap in understanding its potential applications and implications.






% Forward KL and Reverse KL divergences are employed in distinct applications, each offering unique advantages.
% \textit{Forward KL} is widely utilized in standard classification tasks~\citep{goodfellow2016deep}, often appearing in the form of cross-entropy loss to align predicted and true label distributions.
% It promotes mass-covering behavior~\citep{jerfel2021variational,sun2024inverse}, ensuring that the model comprehensively captures all high-probability regions of the target distribution.
% This property makes it particularly effective in knowledge distillation~\citep{hinton2015distilling} for classification tasks, where the teacher model's soft labels provide rich, informative guidance.
% On the other hand, \textit{reverse KL} is frequently adopted in variational inference~\citep{kingma2014auto,pinheiro2021variational}, where it exhibits zero-forcing behavior~\citep{minka2005divergence,wang2023beyond} by focusing on high-confidence predictions while disregarding low-probability regions.
% This characteristic makes reverse KL particularly suitable for tasks prioritizing precision over diversity.
% In the context of WTSG, where weak teachers provide imperfect supervision signals, using forward KL divergence as the loss function may lead to overfitting to the noisy supervision.
% In contrast, reverse KL enables the strong model to extract reliable patterns from weak supervision without being overly constrained by its imperfections.
% Moreover, reverse KL divergence has recently gained increasing attention in domain adaptation~\citep{nguyen2021kl} and KL-regularized reinforcement learning~\citep{rafailov2024direct,wang2023beyond}, where it aligns conceptually with the WTSG framework in terms of transferring knowledge across domains or models.
% Given this conceptual parallel, it is natural to investigate the role of reverse KL within the WTSG framework.
% To the best of our knowledge, no prior work has systematically explored this direction, leaving a significant gap in understanding its potential applications and implications.










% Recently, reverse KL divergence has recently gained attention in domain adaptation~\citep{nguyen2021kl} and knowledge distillation~\citep{gu2024minillm}, 
% imitation learning~\citep{ghasemipour2020divergence} 
% and reinforcement learning~\citep{rafailov2024direct,wang2023beyond}, 



















