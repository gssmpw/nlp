\section{Conclusion}

In this work, we propose a theoretically principled approach by rethinking the loss function in WTSG.
Unlike the mass-covering nature of forward KL, reverse KL exhibits a mode-seeking behavior that focuses on high-confidence predictions from the weak supervisor, thereby reducing the influence of noisy signals.
Theoretically, we derive both upper and lower bounds for forward and reverse losses, demonstrating that reverse losses provide at least comparable guarantees to forward losses.
Notably, when fine-tuning a pre-trained strong model on its last linear layer, reverse KL theoretically ensures that the strong model outperforms its weak supervisor by the magnitude of their disagreement under some assumptions.
Empirically, we show that reverse losses successfully outperform forward losses in most settings, highlighting the practical benefits of reverse KL and CE losses in WTSG.





\newpage


\section*{Limitations}
While our study provides theoretical insights and empirical validation for the advantages of reverse losses in WTSG, several limitations remain. 
First, our analysis mainly assumes relatively reliable weak supervision from pre-trained and fine-tuned models. However, real-world applications often involve noisy weak supervision, and reverse KL’s mode-seeking nature may amplify extreme noise. Further research is needed to assess its suitability in such cases.
Second, while the theoretical results in \cref{sec:universal} provide broad insights, the assumptions in \cref{sec:upper} may not hold in the practical deployment of LLMs. 
This limitation is shared by most related work on theoretical understanding of WTSG. 
Nonetheless, these foundations offer valuable guidance and a starting point for future research on advancing WTSG theory in LLMs.
Third, our experiments are conducted on two well-known alignment-focused binary classification datasets with relatively smaller model sizes. While these results offer valuable insights, it remains an open question whether they can be generalized to more diverse datasets and larger-scale models. Exploring this aspect in future work will help further validate the broader applicability of our approach.





\section*{Ethics Statement}
Our intention is to highlight the positive impact of reverse losses in improving weak-to-strong generalization, ensuring more robust and reliable model performance while minimizing the influence of potentially imperfect weak supervision.
However, the potential amplification of biases from weak models remains a concern, particularly in sensitive applications where fairness is a critical issue. While reverse KL mitigates overfitting to unreliable supervision, its mode-seeking nature may amplify the biases present in the weak model’s predictions.
Additionally, stronger AI models trained using WTSG could be misused if deployed without appropriate safeguards, emphasizing the need for responsible development and oversight.



\section*{Acknowledgements}
We are deeply grateful to Abhijeet Mulgund and Chirag Pabbaraju for their invaluable insights and constructive suggestions.
% This research was supported by National Natural Science Foundation of China (No.62476277), National Key Research and Development Program of China(NO. 2024YFE0203200), CCF-ALIMAMA TECH Kangaroo Fund(No.CCF-ALIMAMA OF 2024008), and Huawei-Renmin University joint program on Information Retrieval. We also acknowledge the support provided by the fund for building worldclass universities (disciplines) of Renmin University of China and by the funds from Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling School of Artificial Intelligence, Renmin University of China, from Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education, from Intelligent Social Governance Interdisciplinary Platform, Major Innovation \& Planning Interdisciplinary Platform for the “DoubleFirst Class” Initiative, Renmin University of China, from Public Policy and Decision-making Research Lab of Renmin University of China, and from Public Computing Cloud, Renmin University of China.



