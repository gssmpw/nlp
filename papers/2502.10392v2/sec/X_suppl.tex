\clearpage
\setcounter{page}{1}
\appendix
\maketitlesupplementary

We provide statistics and analysis for visual feature resolution (Sec.~\ref{sec:supp1}), detailed comparisons of computational cost (Sec.~\ref{sec:supp2}), detailed results on the ScanRefer dataset~\citep{chen2020scanrefer} (Sec.~\ref{sec:supp3}), qualitative comparisons (Sec.~\ref{sec:supp4}) and potential limitations (Sec.~\ref{sec:supp5}) in the supplementary material.


\section{Visual Feature Resolution of Different Architectures}\label{sec:supp1}
To analyze the scene representation resolution of point-based and sparse convolutional architectures, we compare the resolution changes during the visual feature extraction process for EDA~\citep{wu2023eda} and TSP3D-B, as illustrated in Fig.~\ref{fig:supp2}. For a thorough examination of the feature resolution of the sparse convolution architecture, we consider TSP3D-B without incorporating TGP and CBA.
The voxel numbers for TSP3D-B are based on the average statistics from the ScanRefer validation set. In point-based architectures, the number of point features is fixed and does not vary with the scene size. In contrast, the number of voxel features in sparse convolutional architectures tends to increase as the scene size grows. This adaptive adjustment ensures that features do not become excessively sparse when processing larger scenes. 
As shown in Fig.~\ref{fig:supp2}, point-based architectures perform aggressive downsampling, with the first downsampling step reducing 50,000 points to just 2,048 points. Moreover, the final scene representation consists of only 1,024 points, leading to a relatively coarse representation. By contrast, convolution-based architectures progressively downsample and refine the scene representation through a multi-level structure. Overall, the sparse convolution architecture not only provides high-resolution scene representation but also achieves faster inference speed compared to point-based architectures.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.97\linewidth]{fig/supp2.pdf}
    \caption{Feature resolution progression of point-based EDA and sparse convolutional TSP3D-B. \textcolor[rgb]{0.2, 0.457, 0.711}{SA}, \textcolor[rgb]{0.328, 0.508, 0.207}{FP}, \textcolor[rgb]{0.2, 0.457, 0.711}{SpConv}, and \textcolor[rgb]{0.328, 0.508, 0.207}{FU} represent set abstraction, feature propagation, sparse convolution, and feature upsampling, respectively. For the point-based architecture, the downsampling process is aggressive, with the first downsampling reducing 50,000 points directly to 2,048 points. Furthermore, the final scene representation consists of only 1,024 points. In contrast, the sparse convolutional architecture performs progressive downsampling and refines the scene representation through a multi-level structure. This approach not only provides a high-resolution scene representation but also achieves faster inference speed compared to the point-based architecture.}
	\label{fig:supp2}
	%\vspace{-.2cm}
\end{figure}



\begin{table*}[h]
\centering
\caption{Detailed comparison of computational cost for different single-stage architectures on the ScanRefer dataset~\citep{chen2020scanrefer}. The numbers in the table represent frames per second (FPS). TSP3D demonstrates superior processing speed across all components compared to other methods, with the inference speed of the sparse convolution backbone being three times faster than that of the point-based backbone.}
 % \vspace{-0.2cm}
\label{tab:detal_speed}
% \setlength{\tabcolsep}{3.75pt}
% \footnotesize
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \textbf{Text} & \textbf{Visual} & \textbf{Text} &  \textbf{Multi-modal} & \multirow{2}{*}{\textbf{Head}} & \multirow{2}{*}{\textbf{Overall}}\\ 
~ & \textbf{Decouple} & \textbf{Backbone} & \textbf{Backbone} &  \textbf{Fusion} & ~ & ~\\ 
\midrule
3D-SPS~\citep{luo20223d}  & --- & 10.88 & 80.39 & 13.25 & \underline{166.67} & 5.38\\
BUTD-DETR~\citep{jain2022bottom} & 126.58 & 10.60 & 78.55 & 28.49 & 52.63 & 5.91\\
EDA~\citep{wu2023eda}   & 126.58 & \underline{10.89} & \underline{81.10} & \underline{28.57} & 49.75 & \underline{5.98}\\
MCLN~\citep{qian2025multi} & 126.58 & 10.52 & 76.92 & 23.26 & 41.32 & 5.45 \\
% \rowcolor{orange!80} 
TSP3D (Ours)    & --- & \textbf{31.88} & \textbf{81.21} & \textbf{28.67} & \textbf{547.32} & \textbf{12.43} \\
\bottomrule
\end{tabular}
}
% \vspace{-.3cm}
\end{table*}

\section{Detailed Computational Cost of Different Architectures}\label{sec:supp2}
We provide a detailed comparison of the inference speed of specific components across different architectures, as shown in Tab.~\ref{tab:detal_speed}. Two-stage methods tend to have slower inference speed and are significantly impacted by the efficiency of the detection stage, which is not the primary focus of the 3DVG task. Therefore, we focus our analysis solely on the computational cost of single-stage methods.
We divide the networks of existing methods and TSP3D into several components: text decoupling, visual backbone, text backbone, multi-modal fusion, and the head. The inference speed of each of these components is measured separately.


\textbf{Backbone.} Except for TSP3D, the visual backbone in other methods is PointNet++~\citep{qi2017pointnetpp}, which has a high computational cost. This is precisely why we introduce a sparse convolution backbone, which achieves approximately three times the inference speed of PointNet++. As for the text backbone, both TSP3D and other methods use the pre-trained RoBERTa~\citep{liu2019roberta}, so the inference speed for this component is largely consistent across the methods.

\textbf{Multi-modal Fusion.} The multi-modal feature fusion primarily involves the interaction between textual and visual features, with different methods employing different modules. For instance, the multi-modal fusion in SDSPS mainly includes the description-aware keypoint sampling (DKS) and target-oriented progressive mining (TPM) modules. And methods like BUTD-DETR, EDA, and MCLN rely on cross-modal encoders and decoders for their fusion process.
In our TSP3D, the multi-modal fusion involves feature upsampling, text-guided pruning (TGP), and completion-based addition (CBA). Notably, even though TSP3D progressively increases the resolution of scene features and integrates them with fine-grained backbone features, it still achieves superior inference speed. This is primarily due to the text-guided pruning, which significantly reduces the number of voxels and computational cost.

\textbf{Head and Text Decouple.} In the designs of methods such as BUTD-DETR, EDA, and MCLN, the input text needs to be decoupled into several semantic components. 
Additionally, their heads do not output prediction scores directly. Instead, they output embeddings for each candidate object, which must be compared with the embeddings of each word in the text to compute similarities and determine the final output.
This can be considered additional pre-processing and post-processing steps, with the latter significantly impacting computational efficiency.
In contrast, our TSP3D directly predicts the matching scores between the objects and the input text, making the head inference speed over ten times faster than these methods.

% \begin{table}[]
% \centering
% \caption{Detailed comparison of computational cost for different single-stage architectures on the ScanRefer dataset.}
%  % \vspace{-0.2cm}
% \label{tab:comparison_nr3d_sr3d}
% \footnotesize
% \resizebox{0.477\textwidth}{!}{
% \begin{tabular}{ccccccc}
% \toprule
% \textbf{Method} & \textbf{Text Decouple} & \textbf{Visual Backbone} & \textbf{Text Backbone} &  \textbf{Multi-modal Fusion} & \textbf{Head} & \textbf{Overall}\\ 
% \midrule
% 3D-SPS~\citep{luo20223d}  & 22.0 & 22.0 & 22.0 & 22.0 \\
% BUTD-DETR~\citep{jain2022bottom} & 22.0 & 22.0 & 22.0 & 22.0 \\
% EDA~\citep{wu2023eda}   & 22.0 & 22.0 & 22.0 & 22.0  \\
% MCLN~\citep{qian2025multi} & 22.0 & 22.0 & 22.0 & 22.0 \\
% TSP3D (Ours)    & --- & 22.0 & 22.0 & 22.0 \\
% \bottomrule
% \end{tabular}
% }
% % \vspace{-.3cm}
% \end{table}

\section{Detailed Results on ScanRefer}\label{sec:supp3}
Due to page limitations, we report only the overall performances and inference speeds in the main text. To provide detailed results and analysis, we include the accuracies of TSP3D and other methods across various subsets on the ScanRefer dataset~\citep{chen2020scanrefer}, as shown in Tab.~\ref{tab:supp}. TSP3D achieves state-of-the-art accuracy, even when compared with two-stage methods, leading by $+1.13$ in Acc@0.5. TSP3D also demonstrates a level of efficiency that previous methods lack. In various subsets, TSP3D maintains comparable accuracy to both single-stage and two-stage state-of-the-art methods.
Notably, the ``multi-object" subset involves distinguishing the target object among numerous distractors of the same category within a more complex 3D scene. In this setting, TSP3D achieves a commendable performance of \(42.37\) in Acc@0.5, further demonstrating that TSP3D enhances attention to the target object in complex environments through text-guided pruning and completion-based addition, enabling accurate predictions of both the location and the shape of the target.

\begin{table*}[h]
\centering
\caption{Detailed comparison of methods on the ScanRefer dataset~\citep{chen2020scanrefer} evaluated at IoU thresholds of 0.25 and 0.5. TSP3D achieves state-of-the-art accuracy even compared with two-stage methods, with $+1.13$ lead on Acc@0.5. In various subsets, TSP3D achieves comparable accuracy to both single-stage and two-stage state-of-the-art methods. Additionally, TSP3D demonstrates a level of efficiency that previous methods lack.}
% \vspace{.2cm}
\label{tab:supp}
% \footnotesize
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{@{}ccccccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Venue}} & \multicolumn{2}{c}{\textbf{Unique (\(\sim\)19\%)}} & \multicolumn{2}{c}{\textbf{Multiple (\(\sim\)81\%)}}  & \multicolumn{2}{c}{\textbf{Accuracy}} & \textbf{Inference} \\
~& ~& \textbf{0.25} & \textbf{0.5}& \textbf{0.25} & \textbf{0.5}& \textbf{0.25} & \textbf{0.5} & \textbf{Speed (FPS)}\\
\midrule
\multicolumn{9}{l}{\textbf{\textit{Two-Stage Model} }} \\
\midrule
ScanRefer~\citep{chen2020scanrefer} & ECCV'20 & 76.33& 53.51 &32.73& 21.11& 41.19 & 27.40 & \textbf{6.72} \\
TGNN~\citep{huang2021text} & AAAI'21 & 68.61 &56.80& 29.84 &23.18& 37.37 & 29.70 & 3.19 \\
InstanceRefer~\citep{yuan2021instancerefer} & ICCV'21 & 77.45& 66.83& 31.27 &24.77& 40.23 & 30.15 & 2.33 \\
SAT~\citep{yang2021sat} & ICCV'21 & 73.21 &50.83 &37.64 &25.16 & 44.54 & 30.14 & \underline{4.34} \\
FFL-3DOG~\citep{feng2021free} & ICCV'21 & 78.80& 67.94 &35.19 &25.7 & 41.33 & 34.01 & Not released \\
3D-SPS~\citep{luo20223d} & CVPR'22 & 84.12 &66.72& 40.32 &29.82& 48.82 & 36.98 & 3.17 \\
BUTD-DETR~\citep{jain2022bottom} & ECCV'22 &82.88 &64.98 &44.73 &33.97& 50.42 & 38.60 & 3.33 \\
EDA~\citep{wu2023eda} & CVPR'23 & 85.76 &68.57 &49.13 &37.64 & 54.59 & 42.26 & 3.34 \\
3D-VisTA~\citep{zhu20233d} & ICCV'23 & 77.40& 70.90& 38.70& 34.80& 45.90 & 41.50 & 2.03 \\
VPP-Net~\citep{shi2024aware} & CVPR'24 & 86.05 &67.09& 50.32 &39.03 & 55.65 & 43.29 & Not released \\
\(\text{G}^3\)-LQ~\citep{wang2024g} & CVPR'24 & \textbf{88.09}& \textbf{72.73}& \underline{51.48}& \textbf{40.80}& \underline{56.90} & \textbf{45.58} & Not released \\
MCLN~\citep{qian2025multi} & ECCV'24 & \underline{86.89} &\textbf{72.73} &\textbf{51.96} &\underline{40.76} & \textbf{57.17} & \underline{45.53} & 3.17 \\
\midrule
\multicolumn{9}{l}{\textbf{\textit{Single-stage Model} }} \\
\midrule
3D-SPS~\citep{luo20223d} & CVPR'22 & 81.63 &64.77 &39.48 &29.61 & 47.65 & 36.43 & 5.38 \\
BUTD-DETR~\citep{jain2022bottom} & ECCV'22 & 81.47 &61.24 &44.20 &32.81 & 50.22 & 37.87 & 5.91 \\
EDA~\citep{wu2023eda} & CVPR'23 & 86.40& 69.42& 48.11 &36.82& 53.83 & 41.70 & \underline{5.98} \\
\(\text{G}^3\)-LQ~\citep{wang2024g} & CVPR'24 &\textbf{88.59}& \textbf{73.28}& \underline{50.23} & \underline{39.72} & \underline{55.95} & \underline{44.72} & Not released \\
MCLN~\citep{qian2025multi} & ECCV'24 & 84.43 & 68.36 &49.72 &38.41 & 54.30 & 42.64 & 5.45 \\
TSP3D (Ours) & ----- & \underline{87.25} & \underline{71.41}& \textbf{51.04} &\textbf{42.37}  & \textbf{56.45} & \textbf{46.71} & \textbf{12.43} \\
\bottomrule
\end{tabular}
}
\end{table*}



\section{Qualitative Comparisons}\label{sec:supp4}
To qualitatively demonstrate the effectiveness of our proposed TSP3D, we visualize the 3DVG results of TSP3D alongside EDA~\citep{wu2023eda} on the ScanRefer dataset~\citep{chen2020scanrefer}. As shown in Fig.~\ref{fig:supp}, the ground truth boxes are marked in blue, with the predicted boxes for EDA and TSP3D displayed in red and green, respectively. EDA encounters challenges in locating relevant objects, identifying categories, and distinguishing appearance and attributes, as illustrated in Fig.~\ref{fig:supp} (a), (c), and (d). In contrast, our TSP3D gradually focuses attention on the target and relevant objects under textual guidance and enhances resolution through multi-level feature fusion, showcasing commendable grounding capabilities. Furthermore, Fig.~\ref{fig:supp} (b) illustrates that TSP3D performs better with small or narrow targets, as our proposed completion-based addition can adaptively complete the target shape based on high-resolution backbone feature maps.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.98\linewidth]{fig/supp1.pdf}
    \caption{Qualitative results of EDA~\citep{wu2023eda} and our TSP3D on the ScanRefer dataset~\citep{chen2020scanrefer}. In each description, the red annotations indicate the target object. The orange annotations in (a) refer to relevant objects, while the yellow annotations in (d) denote the appearance or attributes of the target. TSP3D demonstrates exceptional performance in locating relevant objects, narrow or small targets, identifying categories, and distinguishing appearance and attributes.}
	\label{fig:supp}
	%\vspace{-.2cm}
\end{figure*}

\section{Limitations and Future Work}\label{sec:supp5}
Despite its leading accuracy and inference speed, TSP3D still has some limitations. First, the speed of TSP3D is slightly slower than that of TSP3D-B. While TSP3D leverages TGP to enable deep interaction between visual and text features in an efficient manner, it inevitably introduces additional computational overhead compared to naive concatenation. In future work, we aim to focus on designing new operations for multi-modal feature interaction to replace the heavy cross-attention mechanism. Second, the current input for 3DVG methods consists of reconstructed point clouds. We plan to extend this to an online setting using streaming RGB-D videos as input, which would support a broader range of practical applications.



% \section{Potential Limitations} 
% Despite of the leading accuracy and inference speed, there are still some limitations of TSP3D. First, the speed of TSP3D is bit slower than TSP3D-B. Although TSP3D utilizes TGP to enable deep interaction between voxel and text features in an efficient way, it unavoidably introduces additional computational overhead compared with naive concatenation. In the future work, we aim to work on designing new operations for multi-modal feature interaction to replace the heavy cross-attention mechanism. Second, currently the input of 3DVG methods is a reconstructed point clouds. We will work on extending it to online setting with streaming RGB-D videos as input, which can support a wider range of practical application.

