\section{Conclusion}
\label{sec:conclu}

In this paper, we present TSP3D, an efficient sparse single-stage method for real-time 3D visual grounding. Different from previous 3D visual grounding frameworks, TSP3D builds on multi-level sparse convolutional architecture for efficient and fine-grained scene representation extraction. To enable the interaction between voxel features and textual features, we propose text-guided pruning (TGP), which reduces the amount of voxel features and guides the network to progressively focus on the target object. Additionally, we introduce completion-based addition (CBA) for adaptive multi-level feature fusion, effectively compensating for instances of over-pruning. Extensive experiments demonstrate the effectiveness of our proposed modules, resulting in an efficient 3DVG method that achieves state-of-the-art accuracy and fast inference speed.

% \textbf{Potential Limitations.} Despite of the leading accuracy and inference speed, there are still some limitations of TSP3D. First, the speed of TSP3D is bit slower than TSP3D-B. Although TSP3D utilizes TGP to enable deep interaction between voxel and text features in an efficient way, it unavoidably introduces additional computational overhead compared with naive concatenation. In the future work, we aim to work on designing new operations for multi-modal feature interaction to replace the heavy cross-attention mechanism. Second, currently the input of 3DVG methods is a reconstructed point clouds. We will work on extending it to online setting with streaming RGB-D videos as input, which can support a wider range of practical application.