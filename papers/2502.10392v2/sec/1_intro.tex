\section{Introduction}
\label{sec:intro}

Incorporating multi-modal information to guide 3D visual perception is a promising direction. In these years, 3D visual grounding (3DVG), also known as 3D instance referencing, has been paid increasing attention as a fundamental multi-modal 3D perception task. The aim of 3DVG is to locate an object in the scene with a free-form query description. 3DVG is challenging since it requires understanding of both 3D scene and language description. Recently, with the development of 3D scene perception and vision-language models, 3DVG methods have shown remarkable progress~\citep{jain2022bottom,luo20223d}. However, with 3DVG being widely applied in fields like robotics and AR / VR where inference speed is the main bottleneck, how to construct efficient real-time 3DVG model remains a challenging problem. 
% To the best of our knowledge, we are the first to comprehensively study the problem of efficiency for 3DVG.

\begin{figure}
% \vspace{-.3cm}
  \centering
  \includegraphics[width=0.47\textwidth]{fig/fig1_cvpr.pdf}
  \vspace{-.2cm}
  \caption{Comparison of 3DVG methods on ScanRefer dataset~\citep{chen2020scanrefer}. Our TSP3D surpasses existing methods in both accuracy and inference speed, achieving the first efficient 3DVG framework.}
  \label{fig:your_label}
  \vspace{-.3cm}
\end{figure}

Since the output format of 3DVG is similar with 3D object detection, early 3DVG methods~\citep{yuan2021instancerefer,yang2021sat,chen2020scanrefer,
huang2021text} usually adopt a two-stage framework, which first conducts detection to locate all objects in the scene, and then selects the target object by incorporating text information. As there are many similarities between 3D object detection and 3DVG (e.g.\ both of them need to extract the representation of the 3D scene), there will be much redundant feature computation during the independent adoption of the two models. As a result, two-stage methods are usually hard to handle real-time tasks. To solve this problem, single-stage methods~\citep{luo20223d,wu2023eda} are presented, which generates the bounding box of the target directly from point clouds. This integrated design is more compact and efficient. However, current single-stage 3DVG methods mainly build on point-based architecture~\citep{qi2017pointnetpp}, where the feature extraction contains time-consuming operations like furthest point sampling and kNN. They also need to aggressively downsample the point features to reduce computational cost, which might hurt the geometric information of small and thin objects~\citep{xu2023dsp}. Due to these reasons, current single-stage methods are still far from real-time ($<6$ FPS) and their performance is inferior to two-stage methods, as shown in Fig.~\ref{fig:your_label}.

In this paper, we propose a new single-stage framework for 3DVG based on \textbf{t}ext-guided \textbf{s}parse voxel \textbf{p}runing, namely TSP3D. Inspired by state-of-the-art 3D object detection methods~\citep{rukhovich2022fcaf3d,xu2023dsp} which achieves both leading accuracy and speed with multi-level sparse convolutional architecture, we build the first sparse single-stage 3DVG network. However, different from 3D object detection, in 3DVG the 3D scene representation should be deeply interacted with text features. Since the count of voxels is very large in sparse convolution-based architecture, deep multi-modal interaction like cross-attention becomes infeasible due to unaffordable computational cost. To this end, we propose text-guided pruning (TGP), which first utilize text information to jointly sparsify the 3D scene representation and enhance the voxel and text features. To mitigate the affect of pruning on delicate geometric information, we further present completion-based addition (CBA) to adaptively fix the over-pruned region with negligible computational overhead.
Specifically, TGP prunes the voxel features according to the object distribution. It gradually removes background features and features of irrelevant objects, which generates text-aware voxel features around the target object for accurate bounding box prediction. Since pruning may mistakenly remove the representation of target object, CBA utilizes text features to query a small set of voxel features from the complete backbone features, followed by pruned-aware addition to fix the over-pruned region.
We conduct extensive experiments on the popular ScanRefer~\citep{chen2020scanrefer} and ReferIt3D~\citep{achlioptas2020referit3d} datasets.
Compared with previous single-stage methods, TSP3D achieves top inference speed and surpasses previous fastest single-stage method by 100\% FPS. TSP3D also achieves state-of-the-art accuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on ScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively.

To summarize, our main contributions are as follows:
\begin{itemize}
	\item[$\bullet$] To the best of our knowledge, this is the first work exploring sparse convolutional architecture for efficient 3DVG.
	\item[$\bullet$] To enable efficient feature extraction, we propose text-guided pruning and completion-based addition to sparsify sparse voxels and adaptively fuse multi-level features.
	\item[$\bullet$] We conduct extensive experiments, and TSP3D outperforms existing methods in both accuracy and speed, demonstrating the superiority of the proposed framework.
\end{itemize}