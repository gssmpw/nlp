\section{Experiments}
\label{sec:expre}


\begin{table}[t]
\centering
\caption{Comparison of methods on the ScanRefer dataset evaluated at IoU thresholds of 0.25 and 0.5. TSP3D achieves state-of-the-art accuracy even compared with two-stage methods, with $+1.13$ lead on Acc@0.5. Notably, we are the first to comprehensively evaluate inference speed for 3DVG methods. The inference speeds of other methods are obtained through our reproduction.}
\vspace{-.2cm}
\label{tab:scanrefer}
\footnotesize
\resizebox{0.4777\textwidth}{!}{
\begin{tabular}{@{}cccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Venue}}  & \multirow{2}{*}{\textbf{Input}} & \multicolumn{2}{c}{\textbf{Accuracy}} & \textbf{Inference} \\
% \cmidrule(r){5-6}
~& ~& ~& \textbf{0.25} & \textbf{0.5} & \textbf{Speed (FPS)}\\
\midrule
\multicolumn{6}{l}{\textbf{\textit{Two-Stage Model} }} \\
\midrule
ScanRefer~\citep{chen2020scanrefer} & ECCV'20   & 3D+2D & 41.19 & 27.40 & \textbf{6.72} \\
TGNN~\citep{huang2021text} & AAAI'21   & 3D    & 37.37 & 29.70 & 3.19 \\
InstanceRefer~\citep{yuan2021instancerefer}   & ICCV'21  & 3D    & 40.23 & 30.15 & 2.33 \\
SAT~\citep{yang2021sat} & ICCV'21  & 3D+2D & 44.54 & 30.14 & \underline{4.34} \\
FFL-3DOG~\citep{feng2021free}& ICCV'21  & 3D    & 41.33 & 34.01 & Not released \\
% 3DVG~\citep{zhao20213dvg}  & ICCV'21  & Two-stage & 3D+2D & 47.57 & 34.68 & --- \\
3D-SPS~\citep{luo20223d} & CVPR'22  & 3D+2D & 48.82 & 36.98 & 3.17 \\
BUTD-DETR~\citep{jain2022bottom} & ECCV'22  & 3D & 50.42 & 38.60 & 3.33 \\
% 3D-VLP~\citep{jin2023context} & CVPR'23  & Two-stage & 3D+2D & 51.41 & 39.46 &  \\
EDA~\citep{wu2023eda}  & CVPR'23  & 3D & 54.59 & 42.26 & 3.34 \\
3D-VisTA~\citep{zhu20233d} & ICCV'23  & 3D & 45.90 & 41.50 & 2.03 \\
VPP-Net~\citep{shi2024aware} & CVPR'24  & 3D & 55.65 & 43.29 & Not released \\
\(\text{G}^3\)-LQ~\citep{wang2024g} & CVPR'24  & 3D & \underline{56.90} & \textbf{45.58} & Not released \\
MCLN~\citep{qian2025multi} & ECCV'24  & 3D & \textbf{57.17} & \underline{45.53} & 3.17 \\
% TSP3D(Ours)    & -----  & Two-stage & 3D &   &   &  --- \\
\midrule
\multicolumn{6}{l}{\textbf{\textit{Single-stage Model} }} \\
\midrule
3D-SPS~\citep{luo20223d} & CVPR'22  & 3D & 47.65 & 36.43 & 5.38 \\
BUTD-DETR~\citep{jain2022bottom}  & ECCV'22  & 3D & 49.76 & 37.05 & 5.91 \\
EDA~\citep{wu2023eda}  & CVPR'23  & 3D & 53.83 & 41.70 & \underline{5.98} \\
\(\text{G}^3\)-LQ~\citep{wang2024g} & CVPR'24  & 3D & \underline{55.95} & \underline{44.72} & Not released \\
MCLN~\citep{qian2025multi} & ECCV'24  & 3D & 54.30 & 42.64 & 5.45 \\
TSP3D (Ours)    & -----  & 3D & \textbf{56.45} & \textbf{46.71} &  \textbf{12.43} \\
\bottomrule
\end{tabular}
}
\vspace{-.2cm}
\end{table}

\subsection{Datasets}
We maintain the same experimental settings with previous works, employing ScanRefer~\citep{chen2020scanrefer} and SR3D/NR3D~\citep{achlioptas2020referit3d} as datasets.
\textbf{ScanRefer}: Built on the ScanNet framework, ScanRefer includes 51,583 descriptions across scenes. Evaluation metrics focus on Acc@\textit{m}IoU.
\textbf{ReferIt3D}: ReferIt3D splits into Nr3D, with 41,503 human-generated descriptions, and Sr3D, containing 83,572 synthetic expressions. ReferIt3D simplifies the task by providing segmented point clouds for each object. The primary evaluation metric is accuracy in target object selection.

\subsection{Implementation Details}
TSP3D is implemented based on PyTorch~\citep{paszke2019pytorch}. 
The pruning thresholds are set at \(\sigma_\text{sce} = 0.7\) and \(\sigma_\text{tar} = 0.3\), and the completion threshold in CBA is \(\tau = 0.15 \). The initial voxelization of the point cloud has a voxel size of 1cm, while the voxel size for level \(i\) features scales to \(2^{i+2}\) cm. The supervision for pruning uses \(L = 7\). The weights for all components of the loss function, \(\lambda_1, \lambda_2, \lambda_3, \lambda_4\), are equal to 1. Training is conducted using four GPUs, while inference speeds are evaluated using a single consumer-grade GPU, RTX 3090, with a batch size of 1.


\begin{table}[t]
\centering
\caption{Quantitative comparisons on Nr3D and Sr3D datasets. We evaluate under three pipelines, noting that the Two-stage using Ground-Truth Boxes is impractical for real-world applications. TSP3D exhibits significant superiority, with leads of $+2.6\%$ and $+3.2\%$ on NR3D and SR3D respectively.}
 \vspace{-0.2cm}
\label{tab:comparison_nr3d_sr3d}
\footnotesize
\resizebox{0.47\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Venue}} & \multirow{2}{*}{\textbf{Pipeline}} & \multicolumn{2}{c}{\textbf{Accuracy}}\\ %\cmidrule(lr){4-5} \cmidrule(lr){6-7}
 ~   &    ~   &     ~     & \textbf{Nr3D} & \textbf{Sr3D} \\ 
\midrule
% TGNN [16]           & ECCV'20 & Two-stage (gt) & 37.3 & 45.0 & --- & --- \\
\rowcolor{gray!20} InstanceRefer~\citep{yuan2021instancerefer}   & ICCV'21 & Two-stage (gt) & 38.8 & 48.0  \\
% 3DVG [48]           & ICCV'21 & Two-stage (gt) & 38.9 & 51.4 & --- & --- \\
\rowcolor{gray!20} LanguageRefer~\citep{roh2022languagerefer} & CoRL'22 & Two-stage (gt) & 43.9 & 56.0  \\
% \rowcolor{gray!20} TransRefer3D [15]   & ECCV'20 & Two-stage (gt) & 48.0 & 57.4 & --- & --- \\
% SAT [44]            & ICCV'21 & Two-stage (gt) & 49.2 & 57.9 & --- & --- \\
% LAR [4]             & CVPR'21 & Two-stage (gt) & 48.9 & 59.4 & --- & --- \\
% 3DRef [2]           & NeurIPS'22 & Two-stage (gt) & 47.0 & 39.0 & --- & --- \\
\rowcolor{gray!20} 3D-SPS~\citep{luo20223d} & CVPR'22 & Two-stage (gt) & 51.5 & 62.6 \\
\rowcolor{gray!20} MVT~\citep{huang2022multi}  & CVPR'22 & Two-stage (gt) & 55.1 & 64.5 \\
\rowcolor{gray!20} BUTD-DETR~\citep{jain2022bottom}  & ECCV'22 & Two-stage (gt) & 54.6 & 67.0 \\
\rowcolor{gray!20} EDA~\citep{wu2023eda}   & CVPR'23 & Two-stage (gt) & 52.1 & 68.1 \\
% \rowcolor{gray!20} 3D-VisTA~\citep{zhu20233d}   & ICCV'23 & Two-stage (gt) & 57.5 & 69.6 \\
\rowcolor{gray!20} VPP-Net~\citep{shi2024aware}  & CVPR'24 & Two-stage (gt) & 56.9 & 68.7 \\
\rowcolor{gray!20} \(\text{G}^3\)-LQ~\citep{wang2024g}  & CVPR'24 & Two-stage (gt) & 58.4 & 73.1 \\
\rowcolor{gray!20} MCLN~\citep{qian2025multi} & ECCV'24 & Two-stage (gt) & 59.8 & 68.4 \\
\midrule
InstanceRefer~\citep{yuan2021instancerefer}  & ICCV'21 & Two-stage (det) & 29.9 & 31.5 \\
LanguageRefer~\citep{roh2022languagerefer}  & CoRL'22 & Two-stage (det) &  28.6 & 39.5 \\
BUTD-DETR~\citep{jain2022bottom} & ECCV'22 & Two-stage (det) & \underline{43.3} & \underline{52.1} \\
EDA~\citep{wu2023eda}   & CVPR'23 & Two-stage (det) & 40.7 & 49.9  \\
MCLN~\citep{qian2025multi} & ECCV'24 & Two-stage (det) & \textbf{46.1} & \textbf{53.9} \\
\midrule
3D-SPS~\citep{luo20223d}  & CVPR'22 & Single-stage & 39.2 & 47.1 \\
BUTD-DETR~\citep{jain2022bottom} & ECCV'22 & Single-stage & 38.7 & 50.1 \\
EDA~\citep{wu2023eda}   & CVPR'23 & Single-stage & 40.0 & 49.7  \\
MCLN~\citep{qian2025multi} & ECCV'24 & Single-stage & \underline{45.7} & \underline{53.4} \\
TSP3D (Ours)    & ----- & Single-stage & \textbf{48.7} & \textbf{57.1} \\
\bottomrule
\end{tabular}
}
\vspace{-.3cm}
\end{table}


\begin{table*}[t]
\centering
\begin{minipage}{0.3\textwidth}
  \caption{Impact of the proposed TGP and CBA. Evaluated on ScanRefer.}
  \vspace{-.2cm}
  \label{tab:ablation1}
  % \footnotesize
  \resizebox{\textwidth}{!}{
  \begin{tabular}{@{}ccc|cccc@{}}
    \toprule
    \multirow{2}{*}{\textbf{ID}} & \multirow{2}{*}{\textbf{TGP}} & \multirow{2}{*}{\textbf{CBA}} & \multicolumn{2}{c}{\textbf{Accuracy}} & \multirow{2}{*}{\textbf{Speed (FPS)}} \\
    ~ & ~ & ~ & \textbf{0.25} & \textbf{0.5} & ~ \\
    \midrule
    (a) &  &            & 40.13 & 32.87 & \textbf{14.58} \\
    (b) & \checkmark &  & 55.20 & 46.15 & 13.22 \\
    (c) &  & \checkmark & 41.34 & 33.09 & 13.51 \\
    (d) & \checkmark & \checkmark & \textbf{56.45} & \textbf{46.71} & 12.43 \\
    \bottomrule
  \end{tabular}
  }
\end{minipage}%
\hfill
\begin{minipage}{0.332\textwidth}
  \caption{Influence of the two CBAs at different levels. Evaluated on ScanRefer.}
  \vspace{-.2cm}
  \label{tab:ablation2}
  % \footnotesize
  \resizebox{\textwidth}{!}{
  \begin{tabular}{@{}ccc|cccc@{}}
    \toprule
    \multirow{2}{*}{\textbf{ID}} & \textbf{CBA} & \textbf{CBA} & \multicolumn{2}{c}{\textbf{Accuracy}} & \multirow{2}{*}{\textbf{Speed (FPS)}} \\
    ~ & \textbf{(level 2)} & \textbf{(level 1)} & \textbf{0.25} & \textbf{0.5} & ~ \\
    \midrule
    (a) &  &            & 55.20 & 46.15 & \textbf{13.22} \\
    (b) & \checkmark &  & 55.17 & 46.06 & 12.79 \\
    (c) &  & \checkmark & \textbf{56.45} & \textbf{46.71} & 12.43 \\
    (d) & \checkmark & \checkmark & 56.22 & 46.68 & 12.19 \\
    \bottomrule
  \end{tabular}
  }
\end{minipage}%
\hfill
\begin{minipage}{0.334\textwidth}
  \caption{Influence of different feature upsampling methods. Evaluated on ScanRefer.}
  \vspace{-.2cm}
  \label{tab:ablation3}
  % \footnotesize
  \resizebox{\textwidth}{!}{
  \begin{tabular}{@{}cc|cccc@{}}
    \toprule
    \multirow{2}{*}{\textbf{ID}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Accuracy}} & \multirow{2}{*}{\textbf{Speed (FPS)}} \\
    ~ & ~ & \textbf{0.25} & \textbf{0.5} & ~ \\
    \midrule
    (a) & Simple concatenation  & 40.13 & 32.87 & \textbf{14.58} \\
    (b) & Attention mechanism  & --- & --- & --- \\
    (c) & Text-guided pruning  & 56.27 & 46.58 & 10.11 \\
    (d) & Simplified TGP & \textbf{56.45} & \textbf{46.71} & 12.43 \\
    \bottomrule
  \end{tabular}
  }
\end{minipage}
\vspace{-.3cm}
\end{table*}

\subsection{Quantitative Comparisons}
\textbf{Performance on ScanRefer.}
% Our proposed method, \textbf{TSP3D}, has demonstrated state-of-the-art performance on the ScanRefer dataset, as detailed in Tab.~\ref{tab:scanrefer}. In both single and two-stage settings, \textbf{TSP3D} outperforms all competing methods by achieving the highest scores in terms of Acc@0.25 and ACC@0.5.
We carry out comparisons with existing methods on ScanRefer, as detailed in Tab.~\ref{tab:scanrefer}. The inference speeds of other methods are obtained through our reproduction with a single RTX 3090 and a batch size of 1. For two-stage methods, the inference speed includes the time taken for object detection in the first stage. For methods using 2D image features and 3D point clouds as inputs, we do not account for the time spent extracting 2D features, assuming they can be obtained in advance. However, in practical applications, the acquisition of 2D features also impacts overall efficiency.
TSP3D achieves state-of-the-art accuracy even compared with two-stage methods, with $+1.13$ lead on Acc@0.5. Notably, in the single-stage setting, TSP3D achieves fast inference speed, which is unprecedented among the existing methods.
This significant improvement is attributed to our method's efficient use of a multi-level architecture based on 3D sparse convolutions, coupled with the text-guided pruning. By focusing computation only on salient regions of the point clouds, determined by textual cues, our model effectively reduces computational overhead while maintaining high accuracy. 
% This enables our system to provide a viable solution for real-time efficient 3D visual grounding.
TSP3D also sets a benchmark for inference speed comparisons for future methodologies.


\textbf{Performance on Nr3D/Sr3D.}
We evaluate our method on the SR3D and NR3D datasets, following the evaluation protocols of prior works like EDA~\citep{wu2023eda} and BUTD-DETR~\citep{jain2022bottom} by using Acc@0.25 as the accuracy metric. The results are shown in Tab.~\ref{tab:comparison_nr3d_sr3d}.
Given that SR3D and NR3D provide ground-truth boxes and categories for all objects in the scene, we consider three pipelines: (1) Two-stage using Ground-Truth Boxes, (2) Two-stage using Detected Boxes, and (3) Single-stage.
In practical applications, the Two-stage using Ground-Truth Boxes pipeline is unrealistic because obtaining all ground-truth boxes in a scene is infeasible. This approach can also oversimplify certain evaluation scenarios. For example, if there are no other objects of the same category as the target in the scene, the task reduces to relying on the provided ground-truth category.
% Conversely, the Two-stage using Detected Boxes and Single-stage pipelines are more practical and reflective of real-world conditions. 
Under the Single-stage setting, TSP3D exhibits significant superiority with peak performance of $48.7\%$ and $57.1\%$ on Nr3D and Sr3D. 
TSP3D even outperforms previous works under the pipeline of Two-stage using Detected Boxes, with leads of $+2.6\%$ and $+3.2\%$ on NR3D and SR3D.
% Notably, our TSP3D maintains an inference speed exceeding 13 FPS, achieving a real-time and efficient 3D visual grounding solution. This efficiency is attributed to our multi-level architecture based on 3D sparse convolutions and the innovative design of text-guided pruning.


% \begin{table}[ht]
% \centering
% \caption{Quantitative comparisons on Nr3D and Sr3D dataset. Top-performing three methods are highlighted.}
% \vspace{.2cm}
% \label{tab:comparison_nr3d_sr3d}
% \begin{tabular}{@{}c|cccccc@{}}
% \toprule
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Nr3D}} & \multicolumn{2}{c}{\textbf{Sr3D}} \\
% % \cmidrule(lr){2-3} \cmidrule(lr){4-5}
% ~ & \textbf{Overall} & \textbf{Hard} & \textbf{Overall} & \textbf{Hard} \\
% \midrule
% TGNN [16]           & 37.3 & 30.6 & 45.0 & 36.9 \\
% InstanceRefer [47]  & 38.8 & 31.8 & 48.0 & 40.5 \\
% 3DVG [48]           & 38.9 & 34.8 & 51.4 & 44.9 \\
% LanguageRefer [39]  & 43.9 & 36.6 & 56.0 & 49.3 \\
% TransRefer3D [15]   & 48.0 & 39.6 & 57.4 & 50.2 \\
% SAT [44]            & 49.2 & 42.4 & 57.9 & 50.0 \\
% LAR [4]             & 48.9 & 42.3 & 59.4 & 51.2 \\
% 3DRef [2]           & 47.0 & 38.3 & 39.0 & 32.0 \\
% 3D-SPS [31]         & 51.5 & 45.1 & 62.6 & 65.4 \\
% MVT [17]            & 55.1 & 49.1 & 64.5 & 63.8 \\
% BUTD-DETR [19]      & 54.6 & 48.4 & 67.0 & 63.2 \\
% EDA [43]            & 52.1 & 46.1 & 68.1 & 62.9 \\
% 3D-VisTA [51]       & 57.5 & 49.4 & 69.6 & 63.6 \\
% G3-LQ               & 58.4 & 50.7 & 73.1 & 66.3 \\
% TSP3D              &   &   &   &   \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[ht]
% \centering
% \caption{Quantitative comparisons on Nr3D and Sr3D dataset. Top-performing three methods are highlighted.}
% \label{tab:comparison_nr3d_sr3d}
% \begin{tabular}{@{}ccccccc@{}}
% \toprule
% \textbf{Method} & \textbf{Venue} & \textbf{Pipeline} & \textbf{Nr3D} & \textbf{Sr3D} & \textbf{Speed (FPS)} \\
% \midrule
% % TGNN [16]           & ECCV'20 & Two-stage (gt) & 37.3 & 45.0 & --- \\
% InstanceRefer [47]  & CVPR'21 & Two-stage (gt) & 38.8 & 48.0 & --- \\
% % 3DVG [48]           & ICCV'21 & Two-stage (gt) & 38.9 & 51.4 & --- \\
% LanguageRefer [39]  & NeurIPS'22 & Two-stage (gt) & 43.9 & 56.0 & --- \\
% TransRefer3D [15]   & ECCV'20 & Two-stage (gt) & 48.0 & 57.4 & --- \\
% % SAT [44]            & ICCV'21 & Two-stage (gt) & 49.2 & 57.9 & --- \\
% % LAR [4]             & CVPR'21 & Two-stage (gt) & 48.9 & 59.4 & --- \\
% % 3DRef [2]           & NeurIPS'22 & Two-stage (gt) & 47.0 & 39.0 & --- \\
% 3D-SPS [31]         & CVPR'22 & Two-stage (gt) & 51.5 & 62.6 & 5.5 \\
% MVT [17]            & ECCV'22 & Two-stage (gt) & 55.1 & 64.5 & --- \\
% BUTD-DETR [19]      & ICCV'21 & Two-stage (gt) & 54.6 & 67.0 & 4.8 \\
% EDA [43]            & CVPR'23 & Two-stage (gt) & 52.1 & 68.1 & 5.2 \\
% 3D-VisTA [51]       & NeurIPS'22 & Two-stage (gt) & 57.5 & 69.6 & --- \\
% G3-LQ               & CVPR'23 & Two-stage (gt) & 58.4 & 73.1 & 6.0 \\
% \midrule
% InstanceRefer [47]  & CVPR'21 & Two-stage (det) & 29.9 & 31.5 & --- \\
% LanguageRefer [39]  & NeurIPS'22 & Two-stage (det) &  28.6 & 39.5 & --- \\
% BUTD-DETR [19]      & ICCV'21 & Two-stage (det) & 43.3 & 52.1 & 4.8 \\
% EDA [43]            & CVPR'23 & Two-stage (det) &  &  & 5.2 \\
% \midrule
% BUTD-DETR [19]      & ICCV'21 & Single-stage &   & 50.09 & 4.8 \\
% 3D-SPS [31]         & CVPR'22 & Single-stage &   &   & 5.5 \\
% EDA [43]            & CVPR'23 & Single-stage &   & 50.20 & 5.2 \\
% TSP3D              & ----- & Single-stage &   & 57.13 & 12.4 \\
% \bottomrule
% \end{tabular}
% \end{table}



\subsection{Ablation Study}

\textbf{Effectiveness of Proposed Components.} 
To investigate the effects of our proposed TGP and CBA, we conduct ablation experiments with module removal as shown in Tab.~\ref{tab:ablation1}. When TGP is not used, multi-modal feature concatenation is employed as a replacement, as shown in Fig.~\ref{fig:method} (a). When CBA is not used, it is substituted with a pruning-based addition. The results demonstrate that TGP significantly enhances performance without notably impacting inference time. This is because TGP, while utilizing a more complex multi-modal attention mechanism for stronger feature fusion, significantly reduces feature scale through text-guided pruning. Additionally, the performance improvement is also due to the gradual guidance towards the target object by both scene-level and target-level TGP. Using CBA alone has a limited effect, as no voxels are pruned. Implementing CBA on top of TGP further enhances performance, as CBA dynamically compensates for some of the excessive pruning by TGP, thus increasing the network's robustness.

% \begin{table}[ht]
% \centering
% \caption{Ablation study on the effectiveness of the proposed TGP and CBA. The performances are evaluated on the ScanRefer dataset.}
% \vspace{.2cm}
% \label{tab:ablation_study}
% \begin{tabular}{@{}ccc|cccc@{}}
% \toprule
% \multirow{2}{*}{\textbf{ID}} & \multirow{2}{*}{\textbf{TGP}} & \multirow{2}{*}{\textbf{CBA}} & \multicolumn{2}{c}{\textbf{Accuracy}} & \multirow{2}{*}{\textbf{Speed (FPS)}} \\
% % \cmidrule(lr){4-5}
% ~ & ~ & ~ & \textbf{0.25} & \textbf{0.5} & ~ \\
% \midrule
% (a) &  &            & 40.13 & 32.87 & \textbf{14.58} \\
% (b) & \checkmark &  & 55.20 & 46.15 & 13.22 \\
% (c) &  & \checkmark & 41.34 & 33.09 & 13.51 \\
% (d) & \checkmark & \checkmark & \textbf{56.45} & \textbf{46.71} & 12.43 \\
% \bottomrule
% \end{tabular}
% \end{table}


% \begin{table}[t]
% \centering
% % \caption{Ablation study results.}
% % \vspace{.2cm}
% % \begin{minipage}{0.46\textwidth}
% % \centering
% \caption{Effectiveness of the proposed TGP and CBA. Evaluated on the ScanRefer dataset.}
% \label{tab:ablation1}
% % \vspace{-.2cm}
% \footnotesize
% % \resizebox{0.45\textwidth}{!}{
% \begin{tabular}{@{}ccc|cccc@{}}
% \toprule
% \multirow{2}{*}{\textbf{ID}} & \multirow{2}{*}{\textbf{TGP}} & \multirow{2}{*}{\textbf{CBA}} & \multicolumn{2}{c}{\textbf{Accuracy}} & \multirow{2}{*}{\textbf{Speed (FPS)}} \\
% ~ & ~ & ~ & \textbf{0.25} & \textbf{0.5} & ~ \\
% \midrule
% (a) &  &            & 40.13 & 32.87 & \textbf{14.58} \\
% (b) & \checkmark &  & 55.20 & 46.15 & 13.22 \\
% (c) &  & \checkmark & 41.34 & 33.09 & 13.51 \\
% (d) & \checkmark & \checkmark & \textbf{56.45} & \textbf{46.71} & 12.43 \\
% \bottomrule
% \end{tabular}
% % }
% \end{table}

% \begin{table}[t]
% \centering
% \caption{Influence of the two CBAs at different levels. Evaluated on the ScanRefer dataset.}
% \label{tab:ablation2}
% % \vspace{-.2cm}
% \footnotesize
% % \resizebox{0.45\textwidth}{!}{
% \begin{tabular}{@{}ccc|cccc@{}}
% \toprule
% \multirow{2}{*}{\textbf{ID}} & \textbf{CBA} & \textbf{CBA} & \multicolumn{2}{c}{\textbf{Accuracy}} & \multirow{2}{*}{\textbf{Speed (FPS)}} \\
% ~ & \textbf{(level 1)} & \textbf{(level 2)} & \textbf{0.25} & \textbf{0.5} & ~ \\
% \midrule
% (a) &  &            & 55.20 & 46.15 & \textbf{13.22} \\
% (b) & \checkmark &  & 55.17 & 46.06 & 12.79 \\
% (c) &  & \checkmark & \textbf{56.45} & \textbf{46.71} & 12.43 \\
% (d) & \checkmark & \checkmark & 56.22 & 46.68 & 12.19 \\
% \bottomrule
% \end{tabular}
% % }
% \end{table}


% \begin{table}[t]
% \centering
% \vspace{-.3cm}
% \caption{Influence of different feature upsampling methods. Evaluated on the ScanRefer dataset.}
% % \vspace{-.1cm}
% \label{tab:ablation3}
% \footnotesize
% % \resizebox{0.45\textwidth}{!}{
% \begin{tabular}{@{}cc|cccc@{}}
% \toprule
% \multirow{2}{*}{\textbf{ID}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Accuracy}} & \multirow{2}{*}{\textbf{Speed (FPS)}} \\
% % \cmidrule(lr){4-5}
% ~ & ~ & \textbf{0.25} & \textbf{0.5} & ~ \\
% \midrule
% (a) & Simple concatenation  & 40.13 & 32.87 & \textbf{14.58} \\
% (b) & Attention mechanism  & --- & --- & --- \\
% (c) & Text-guided pruning  & 56.27 & 46.58 & 10.11 \\
% (d) & Simplified TGP & \textbf{56.45} & \textbf{46.71} & 12.43 \\
% \bottomrule
% \end{tabular}
% % }
% \end{table}


\textbf{Influence of the Two CBAs.}
To explore the impact of CBAs at two different levels, we conduct ablation experiments as depicted in Tab.~\ref{tab:ablation2}. In the absence of CBA, we use pruning-based addition as a substitute. The results indicate that the CBA at level 2 has negligible effects on the 3DVG task. This is primarily because the CBA at level 2 serves to supplement the scene-level TGP, which is expected to prune the background (a relatively simple task). Moreover, although some target features are pruned, they are compensated by two subsequent generative sparse convolutions. However, the CBA at level 1 enhances performance by adapt completion for the target-level TGP. It is challenging to fully preserve target objects from deep upsampling features, especially for smaller or narrower targets. The CBA at level 1, based on high-resolution backbone features, effectively complements the TGP.


\begin{figure*}[t]
	\centering
	\includegraphics[width=0.85\linewidth]{fig/prune.pdf}
    \vspace{-.2cm}
    \caption{Visualization of the text-guided pruning process. In each example, the voxel features after scene-level TGP, target-level TGP and the last upsampling layer are presented from top to bottom. The blue boxes represent the ground truth of the target, and the red boxes denote the bounding boxes of relevant objects. TSP3D reduces the amount of voxel features through two stages of pruning and progressively guides the network focusing towards the target.}
	\label{fig:prune}
	\vspace{-.1cm}
\end{figure*}


\begin{figure*}[t]
	\centering
	\includegraphics[width=0.85\linewidth]{fig/com.pdf}
    \vspace{-.2cm}
    \caption{Visualization of the completion-based addition process. The blue points represent the voxel features output by the target-level TGP, while the red points are the completion features predicted by the CBA. The blue boxes indicate the ground truth boxes. CBA adaptively supplements situations where excessive pruning has occurred.}
	\label{fig:com}
    \vspace{-.1cm}
\end{figure*}

\textbf{Feature Upsampling Techniques.}
We conduct experiments to assess the effects of different feature upsampling techniques, as detailed in Tab.~\ref{tab:ablation3}. Using simple feature concatenation (Fig.~\ref{fig:method} (a)), while fast in inference speed, results in poor performance. When we utilize an attention mechanism with stronger feature interaction, as shown in Fig.~\ref{fig:method} (b), the computation exceeds the limits of GPU due to the large number of voxels, making it impractical for real-world applications. Consequently, we employ TGP to reduce the feature amount, as illustrated in Fig.~\ref{fig:method} (c), which significantly improves performance and enables practical deployment. Building on TGP, we propose simplified TGP, as shown in Fig.~\ref{fig:method} (d), that merges feature interactions before and after pruning, achieving performance consistent with the original TGP while enhancing inference speed.

% \begin{table}[ht]
% \centering
% \caption{Ablation study on the influence of the two CBAs in different levels. Evaluated on the ScanRefer dataset.}
% \vspace{.2cm}
% \label{tab:ablation_study}
% \begin{tabular}{@{}ccc|cccc@{}}
% \toprule
% \multirow{2}{*}{\textbf{ID}} & \textbf{CBA} & \textbf{CBA} & \multicolumn{2}{c}{\textbf{Accuracy}} & \multirow{2}{*}{\textbf{Speed (FPS)}} \\
% % \cmidrule(lr){4-5}
% ~ & \textbf{(level 1)} & \textbf{(level 2)} & \textbf{0.25} & \textbf{0.5} & ~ \\
% \midrule
% (a) &  &            & 55.20 & 46.15 & \textbf{13.22} \\
% (b) & \checkmark &  & 55.17 & 46.06 & 12.79 \\
% (c) &  & \checkmark & \textbf{56.45} & \textbf{46.71} & 12.43 \\
% (d) & \checkmark & \checkmark & 56.22 & 46.68 & 12.19 \\
% \bottomrule
% \end{tabular}
% \end{table}


% \begin{table}[t]
% \centering
% \caption{Influence of different feature upsampling methods. Evaluated on the ScanRefer dataset.}
% \vspace{-.1cm}
% \label{tab:ablation3}
% \footnotesize
% \begin{tabular}{@{}cc|cccc@{}}
% \toprule
% \multirow{2}{*}{\textbf{ID}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Accuracy}} & \multirow{2}{*}{\textbf{Speed (FPS)}} \\
% % \cmidrule(lr){4-5}
% ~ & ~ & \textbf{0.25} & \textbf{0.5} & ~ \\
% \midrule
% (a) & Simple concatenation  & 40.13 & 32.87 & \textbf{14.58} \\
% (b) & Attention mechanism  & --- & --- & --- \\
% (c) & Text-guided Pruning  & 56.27 & 46.58 & 10.11 \\
% (d) & Simplified TGP & \textbf{56.45} & \textbf{46.71} & 12.43 \\
% \bottomrule
% \end{tabular}
% \end{table}


\subsection{Qualitative Results}
\textbf{Text-guided Pruning.}
To visually demonstrate the process of TGP, we visualize the results of two pruning phases, as shown in Fig.~\ref{fig:prune}. In each example, the voxel features after scene-level pruning, the features after target-level pruning, and the features after target-level generative sparse convolution are displayed from top to bottom. It is evident that both pruning stages effectively achieve our intended effect: the scene-level pruning filters out the background and retained object voxels, and the target-level pruning preserves relevant and target objects. Moreover, during the feature upsampling process, the feature amount nearly exponentially increases due to generative upsampling. Without TGP, the voxel coverage would far exceed the range of the scene point cloud, which is inefficient for inference. This also intuitively explains the significant impact of our TGP on both performance and inference speed.



\textbf{Completion-based Addition.}
To clearly illustrate the function of CBA, we visualize the adaptive completion process in Fig.~\ref{fig:com}. The images below showcase several instances of excessive pruning. TGP performs pruning based on deep and low-resolution features, which can lead to excessive pruning, potentially removing entire or partial targets. This over-pruning is more likely to occur with small, as shown in Fig.~\ref{fig:com} (a) and (c), narrow, as in Fig.~\ref{fig:com} (b), or elongated targets, as in Fig.~\ref{fig:com} (d). Our CBA effectively supplements the process using higher-resolution backbone features, thus dynamically integrating multi-level features.