\section{Method}
\label{sec:method}

In this section, we describe our TSP3D for efficient single-stage 3DVG. We first analyze existing pipelines to identify current challenges and motivate our approach (Sec. \ref{sec:3.1}). 
% Next, we provide an overview of our framework (Sec. \ref{sec:3.2}). 
We then introduce the text-guided pruning, which leverages text features to guide feature pruning  (Sec. \ref{sec:3.2}). To address the potential risk of pruning key information, we propose the completion-based addition for multi-level feature fusion (Sec. \ref{sec:3.3}). Finally, we detail the training loss (Sec. \ref{sec:3.4}).

\subsection{Architecture Analysis for 3DVG}\label{sec:3.1}

Top-performance 3DVG methods~\citep{wang2024g,wu2023eda,shi2024aware}, are mainly two-stage, which is a serial combination of 3D object detection and 3D object grounding. This separate calls of two approaches result in redundant feature extraction and complex pipeline, thus making the two-stage methods less efficient. 
To demonstrate the efficiency of existing methods, we conduct a comparison of accuracy and speed among several representative methods on ScanRefer~\citep{chen2020scanrefer}, as shown in Fig.~\ref{fig:your_label}. 
% Recognizing that previous studies have not considered computational efficiency, we replicate these methods and report their inference speeds. 
It can be seen that two-stage methods struggle in speed ($<3$ FPS) due to the additional detection stage. Since 3D visual grounding is usually adopted in practical scenarios that require real-time inference under limited resources, such as embodied robots and VR/AR, the low speed of two-stage methods make them less practical.
On the other side, single-stage methods~\citep{luo20223d}, which directly predicts refered bounding box from the observed 3D scene, are more suitable choices due to their streamlined processes. In Fig.~\ref{fig:your_label}, it can be observed that single-stage methods are significantly more efficient than their two-stage counterparts.
% We also list the accuracy-speed tradeoff of single-stage methods in Fig.~\ref{fig:your_label}. It is shown that they are much more efficient than the two-stage counterparts.


However, existing single-stage methods are mainly built on point-based backbone~\citep{qi2017pointnetpp}, where the scene representation is extracted with time-consuming operations like furthest point sampling and set abstraction. They also employ large transformer decoder to fuse text and 3D features for several iterations. Therefore, the inference speed of current single-stage methods is still far from real-time ($<6$ FPS).
The inference speed of specific components in different frameworks is analyzed and discussed in detail in the supplementary material.
Inspired by the success of multi-level sparse convolutional architecture in 3D object detection~\citep{rukhovich2023tr3d}, which achieves both leading accuracy and speed, we propose to build the first multi-level convolutional single-stage 3DVG pipeline.

 
\textbf{TSP3D-B.} Here we propose a baseline framework based on sparse convolution, namely TSP3D-B. Following the simple and effective multi-level architecture of FCAF3D~\citep{rukhovich2022fcaf3d}, TSP3D-B utilizes 3 levels of sparse convolutional blocks for scene representation extraction and bounding box prediction, as shown in Fig.~\ref{fig:method} (a). 
Specifically, the input pointclouds $P\in \mathbb{R}^{N \times 6}$ with 6-dim features (3D position and RGB) are first voxelized and then fed into three sequential MinkResBlocks~\citep{choy20194d}, which generates three levels of voxel features $V_l\ (l=1,2,3)$. With the increase of $l$, the spatial resolution of $V_l$ decreases and the context information increases.
Concurrently, the free-form text with $l$ words is encoded by the pre-trained RoBERTa~\citep{liu2019roberta} and produce the vanilla text tokens \( T  \in \mathbb{R}^{l \times d}\).
With the extracted 3D and text representations, we iteratively upsample $V_3$ and fuse it with $T$ to generate high-resolution and text-aware scene representation:
\begin{equation}\label{eq1}
    U_l=U^G_l+V_l,\ \ \ \ U^G_l={\rm GeSpConv}(U'_{l+1})
\end{equation}
\begin{equation}
    U'_{l+1}={\rm Concat}(U_{l+1}, T) 
\end{equation}
where $U_3=V_3$, ${\rm GeSpConv}$ means generative sparse convolution~\citep{gwak2020generative} with stride 2, which upsamples the voxel features and expands their spatial locations for better bounding box prediction. ${\rm Concat}$ is voxel-wise feature concatenation by duplicating $T$.
The final upsampled feature map $U_1$ is concatenated with $T$ and fed into a convolutional head to predict the objectness scores and regress the 3D bounding box. We select the box with highest objectness score as the grounding result.

As shown in Fig.~\ref{fig:your_label}, TSP3D-B achieves an inference speed of 14.58 FPS, which is significantly faster than previous single-stage methods and demonstrates great potential for real-time 3DVG. 



\begin{figure*}
	\centering
	\includegraphics[width=0.95\linewidth]{fig/method1.pdf}
    \vspace{-.1cm}
    \caption{Illustration of TSP3D. TSP3D bulids on multi-level sparse convolutional architecture. It iteratively upsamples the voxel features with text-guided pruning (TGP), and fuses multi-level features via completion-based addition (CBA). 
    (a) to (d) on the right side illustrate various options for feature upsampling. 
    (a) refers to simple concatenation with text features, which is fast but less accurate. 
    (b) refers to feature interaction through cross-modal attention mechanisms, which is constrained by the large number of voxels. 
    (c) represents our proposed TGP, which first prunes voxel features under textual guidance and thus enables efficient interaction between voxel and text features. 
    (d) shows a simplified version of TGP that removes farthest point sampling and interpolation, combines multi-modal feature interactions into a whole and moves it before pruning.}
	\label{fig:method}
	\vspace{-.2cm}
\end{figure*}


\subsection{Text-guided Pruning}\label{sec:3.2}
Though efficient, TSP3D-B exhibits poor performance due to the inadequate interaction between 3D scene representation and text features. Motivated by previous 3DVG methods~\citep{jain2022bottom}, a simple solution is to replace ${\rm Concat}$ with cross-modal attention to process voxel and text features, as shown in Fig.~\ref{fig:method} (b).
However, different from point-based architectures where the scene representation is usually aggressively downsampled, the number of voxels in multi-level convolutional framework is very large\footnote{Compared to point-based architectures, sparse convolutional framework provides higher resolution and more detailed scene representations, while also offering advantages in inference speed. For detailed statistics, please refer to the supplementary material.}. In practical implementation, we find that the voxels expand almost exponentially with each upsampling layer, leading to a substantial computational burden for the self-attention and cross-attention of scene features. To address this issue, we introduce text-guided pruning (TGP) to construct TSP3D, as illustrated in Fig.~\ref{fig:method} (c). The core idea of TGP is to reduce feature amount by pruning redundant voxels and guide the network to gradually focus on the final target based on textual features.


\textbf{Overall Architecture.} TGP can be regarded as a modified version of cross-modal attention, which reduces the number of voxels before attention operation, thereby lowering computational cost. To minimize the affect of pruning on the final prediction, we propose to prune the scene representation gradually. At higher level where the number of voxels is not too large yet, TGP prunes less voxels. While at lower level where the number of voxels is significantly increased by upsampling operation, TGP prunes the voxel features more aggressively.
The multi-level architecture of TSP3D consists of three levels and includes two feature upsampling operations. Therefore, we correspondingly configure two TGPs with different functions, which are referred as scene-level TGP (level 3 to 2) and target-level TGP (level 2 to 1) respectively. Scene-level TGP aims to distinguish between objects and the background, specifically pruning the voxels on background. Target-level TGP focuses on regions mentioned in the text, intending to preserve the target object and referential objects while removing other regions.


\textbf{Details of TGP.} Since the pruning is relevant to the description, we need to make the voxel features text-aware to predict a proper pruning mask.
To reduce the computational cost, we perform farthest point sampling (FPS) on the voxel features to reduce their size while preserving the basic distribution of the scene. Next, we utilize cross-attention to interact with the text features and employ a simple MLP to predict the probability distribution $\hat{M}$ for retaining each voxel. To prune the features $U_l$, we binarize and interpolate the $\hat{M}$ to obtain the pruned mask. This process can be expressed as:
\begin{equation}\label{eq2} 
    U^P_l = U_l \odot \Theta(\mathcal{I}(\hat{M}, U_l) - \sigma)
\end{equation}
\begin{equation}
    \hat{M} = \text{MLP}(\text{CrossAtt}(\text{FPS}(U_l),\text{SelfAtt}(T)))
\end{equation}
where $U^P_l$ is the pruned features, \(\Theta\) is Heaviside step function, \(\odot\) is matrix dot product, $\sigma$ is the pruning threshold, and \(\mathcal{I}\) represents linear interpolation based on the positions specified by $U_l$. After pruning, the scale of the scene features is significantly reduced, enabling internal feature interactions based on self-attention. Subsequently, we utilize self-attention and cross-attention to perceive the relative relationships among objects within the scene and to fuse multimodal features, resulting in updated features $U'_l$. Finally, through generative sparse convolutions, we obtain $U^G_{l-1}$.


\textbf{Supervision for Pruning.}
The binary supervision mask $M^{sce}$ for scene-level TGP is generated based on the centers of all objects in the scene, and the mask $M^{tar}$ for target-level TGP is based on the target and relevant objects mentioned in the descriptions:
\begin{equation}\label{eq4}
M^{sce} = \bigcup_{i=1}^{N} \mathcal{M}(O_i), \ \
M^{tar} = \mathcal{M}(O^{tar}) \cup \bigcup_{j=1}^{K} \mathcal{M}(O^{rel}_j)
\end{equation}
where $\{O_i|1\leq i\leq N\}$ indicates all objects in the scene. $O^{tar}$ and $O^{rel}$ refer to target and relevant objects respectively.
$\mathcal{M}(O)$ represents the mask generated from the center of object $O$. It generates a \( L \times L \times L \) cube centered at the center of $O$ to construct the supervision mask $M$, where locations inside the cube is set to 1 while others set to 0.

% Our assignment approach does not rely on the dimensions of the bounding box, ensuring a sufficient number of positive proposals for objects of any size.

% replace FPS. 2xinteraction-->1xinteration before pruning.
\textbf{Simplification.} Although the above mentioned method can effectively prune voxel features to reduce the computational cost of cross-modal attention, there are some inefficient operations in the pipeline: (1) FPS is time-consuming, especially for large scenes; (2) there are two times of interactions between voxel features and text features, the first is to guide pruning and the second is to enhance the representation, which is a bit redundant.
We also empirically observe that the number of voxels is not large in level 3. To this end, we propose a simplified version of TGP, as shown in Fig.~\ref{fig:method} (d). We remove the FPS and merge the two multi-modal interactions into one. We also move the merged interaction operation before pruning. In this way, voxel features and text features are first deeply interacted for both feature enhancement and pruning. Because in level 3 the number of voxels is small and in level 2 / 1 the voxels are already pruned, the computational cost of self-attention and cross-attention is always kept at a relatively low level. 

\textbf{Effectiveness of TGP.} After pruning, the voxel count of $U_1$ is reduced to nearly 7\% of its original size without TGP, while the 3DVG performance is significantly boosted.
TGP serves multiple functions, including: (1) facilitating the interaction of multi-modal features through cross-attention, (2) reducing the feature amount (number of voxels) through pruning, and (3) gradually guiding the network to focus on the mentioned target based on text features. 



\begin{figure}[t]
  \centering
  \begin{minipage}{0.47\textwidth}
    \centering
    \includegraphics[width=0.88\textwidth]{fig/method2-1.pdf}
    % 可以添加对第一张图的简要说明
  \end{minipage}

  \vspace{0.2cm} % 调整上下图之间的间距

  \begin{minipage}{0.47\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{fig/method2-2.pdf}
    % 可以添加对第二张图的简要说明
  \end{minipage}
 \vspace{-0.2cm}
  \caption{Illustration of completion-based addition. The upper figure (b) illustrates an example of over-pruning on the target. The lower figure (c) shows the completed features predicted by CBA.}
  \label{fig:cba}
  \vspace{-0.2cm}
\end{figure}


\subsection{Completion-based Addition}
\label{sec:3.3}
During the pruning process, some targets may be mistakenly removed, especially for small or narrow objects, as shown in Fig.~\ref{fig:cba} (b). Therefore, the addition operation between the upsampled pruned features $U^G_l$ and backbone features $V_l$ described in Equation (\ref{eq1}) play an important role to mitigate the affect of over-pruning.


There are two alternative addition operation: (1) \textbf{Full Addition.} For the intersecting regions of $V_l$ and $U^G_l$, features are directly added. For voxel features outside the intersection of $U^G_l$ and $V_l$ which lack corresponding features in the other map, the missing voxel features are interpolated before addition. Due to pruning process, $U^G_l$ is sparser than $V_l$. In this way, full addition can fix almost all the pruned region. But this operation is computationally heavy and make the scene representation fail to focus on relevant objects, which deviates the core idea of TGP.
(2) \textbf{Pruning-aware Addition.} The addition is constrained to the locations of $U^G_l$. For voxel in $U^G_l$ but not in $V_l$, interpolation from $U^G_l$ is applied to complete the missing locations in $V_l$. It restricts the addition operation to the shape of the pruned features, potentially leading to an over-reliance on the results of the pruning process. If important regions are over-pruned, the network may struggle to detect targets with severely damaged geometric information.

% \begin{figure}{0.5\textwidth}
%  % \vspace{-.8em}
% % \vspace{-.3cm}
%   \centering
%   \begin{minipage}{0.45\textwidth}
%   \centering
%     \includegraphics[width=0.88\textwidth]{fig/method2-1.pdf}
%     % 不添加小 caption
%     % \vspace{.0002em}
%   \end{minipage}
%   % \vspace{5.5em} % 可调节上下间距
%   \begin{minipage}{0.45\textwidth}
%   \centering
%     \includegraphics[width=0.88\textwidth]{fig/method2-2.pdf} % 替换为第二张图的路径
%     % 不添加小 caption
%   \end{minipage}
%   % \vspace{-.5em}
%   \caption{Illustration of completion-based addition. The above (b) illustrate an example of over-pruning on the target. (c) refers to the completed features predicted by CBA. The lower diagram demonstrates how CBA predicts target distribution under textual guidance and adaptively completes the pruned features.}
%   \label{fig:cba}
%   % \vspace{1.5em}
% \end{figure}

Considering the unavoidable risk of pruning the query target, we introduce the completion-based addition (CBA). CBA is designed to address the limitations of full and pruning-aware additions. It offers a more targeted and efficient way to integrating multi-level features, ensuring the preservation of essential details while keeping the additional computational overhead negligible.
% CBA is designed to mitigate the drawback of full and pruning-aware additions by providing a more targeted and efficient way of integrating multi-level features, ensuring that essential details are preserved during the feature fusion process while the additional computational overhead is negligible.


\textbf{Details of CBA.} We first enhance the backbone features $V_l$ with the text features $T$ through cross-attention, obtaining $V_l'$. Then a MLP is adopted to predict the probability distribution of target for region selection:
\begin{equation}
    M_l^{tar} = \Theta(\text{MLP}(V_l') - \tau)
\end{equation}
where \( \Theta \) is the step function, and \( \tau \) is the threshold determining voxel relevance. $M_l^{tar}$ is a binary mask indicating potential regions of the mentioned target.
Then, comparison of $M_l^{tar}$ with $U_l$ identifies missing voxels. The missing mask $M_l^{mis}$ is derived as follows:
\begin{equation}
    M_l^{mis}=M_l^{tar} \land (\neg \ \mathcal{C}(U_l^G,V_l))
\end{equation}
where \(\mathcal{C}(A,B)\) denotes the generation of a binary mask for \(A\) based on the shape of \(B\). Specifically, for positions in \(B\), if there are corresponding voxel features in \(A\), the mask for that position is set to 1. Otherwise it is set to 0.
Missed voxel features in $U_l^G$ that correspond to $M_l^{mis}$ are interpolated from $U_l^G$, filling in gaps identified by the missing mask. The completed feature map $U_l^{cpl}$ is computed by:
\begin{equation}
    U_l^{cpl}=V_l' \odot M_l^{mis} + \mathcal{I}(U_l^G, M_l^{mis})
\end{equation}
where \(\mathcal{I}\) represents linear interpolation on the feature map based on the positions specified in the mask.
Finally, the original upsampled features are combined with the backbone features according to the pruning-aware addition, and merged with the completion features to yield updated $U_l$:
\begin{equation}
    U_l=\text{Concat}(U^G_l \leftarrow V_l, U_l^{cpl})
\end{equation}
where \( \leftarrow \) denotes the pruning-aware addition, and \(\text{Concat}\) means concatenation of voxel features.



% \textbf{Effectiveness of CBA.} CBA is a compromise between adding all potentially over-pruned features and focusing solely on pruned features, ensuring that the final feature map is both comprehensive and sparse, which improves the robustness and efficiency of TSP3D.

\subsection{Train Loss}\label{sec:3.4}

The loss is composed of several components: pruning loss for TGP, completion loss for CBA, and objectness loss as well as bounding box regression loss for the head. Pruning loss, completion loss and objectness loss employ the focal loss to handle class imbalance. 
Supervision for completion and classification losses are the same, which sets voxels near the target object center as positives while leaving others as negatives. 
For bounding box regression, we use the Distance-IoU (DIoU) loss. The total loss function is computetd as the sum of these individual losses:
\[
\mathcal{L}_{\text{total}} = \lambda_1\mathcal{L}_{\text{pruning}} + \lambda_2\mathcal{L}_{\text{com}} + \lambda_3\mathcal{L}_{\text{class}} + \lambda_4\mathcal{L}_{\text{bbox}}
\]
where \(\lambda_1\), \(\lambda_2\), \(\lambda_3\) and \(\lambda_4\) are the weights of different parts.
