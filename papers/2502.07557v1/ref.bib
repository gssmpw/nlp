@inproceedings{spaceandtime,
    title={Language Models Represent Space and Time},
    author={Wes Gurnee and Max Tegmark},
    booktitle={Proc. of {ICLR}},
    year={2024},
}

@inproceedings{parklinear,
  title={The Linear Representation Hypothesis and the Geometry of Large Language Models},
  author={Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  booktitle={Proc. of {ICML}},
  year={2024}
}

@article{ijp,
  title={" do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  journal={arXiv preprint arXiv:2308.03825},
  year={2023}
}

@article{gcg,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@inproceedings{jb1,
    title={Are aligned neural networks adversarially aligned?},
    author={Nicholas Carlini and Milad Nasr and Christopher A. Choquette-Choo and Matthew Jagielski and Irena Gao and Pang Wei Koh and Daphne Ippolito and Florian Tram{\`e}r and Ludwig Schmidt},
    booktitle={Proc. of {NeurIPS}},
    year={2023},
    pages = {61478--61500},
    volume = {36},
}

@article{smoothllm,
  title={SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks},
  author={Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J},
  journal={arXiv preprint arXiv:2310.03684},
  year={2023}
}

@article{MasterKey,
  title={Jailbreaker: Automated jailbreak across multiple large language model chatbots},
  author={Deng, Gelei and Liu, Yi and Li, Yuekang and Wang, Kailong and Zhang, Ying and Li, Zefeng and Wang, Haoyu and Zhang, Tianwei and Liu, Yang},
  journal={arXiv preprint arXiv:2307.08715},
  year={2023}
}

@article{constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{tap,
  title={Tree of attacks: Jailbreaking black-box llms automatically},
  author={Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
  journal={arXiv preprint arXiv:2312.02119},
  year={2023}
}

@article{pair,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}

@article{saa,
  title={Jailbreaking leading safety-aligned llms with simple adaptive attacks},
  author={Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2404.02151},
  year={2024}
}

@article{drattack,
  title={DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers},
  author={Li, Xirui and Wang, Ruochen and Cheng, Minhao and Zhou, Tianyi and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:2402.16914},
  year={2024}
}

@article{puzzler,
  title={Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues},
  author={Chang, Zhiyuan and Li, Mingyang and Liu, Yi and Wang, Junjie and Wang, Qing and Liu, Yang},
  journal={arXiv preprint arXiv:2402.09091},
  year={2024}
}

@article{detox,
  title={DeTox: Toxic Subspace Projection for Model Editing},
  author={Uppaal, Rheeya and De, Apratim and He, Yiting and Zhong, Yiquao and Hu, Junjie},
  journal={arXiv preprint arXiv:2405.13967},
  year={2024}
}

@article{openai-interpre,
  title={Scaling and evaluating sparse autoencoders},
  author={Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024}
}

@article{claude-interpre,
   title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
   author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
   year={2024},
   journal={Transformer Circuits Thread},
   url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@article{autodan,
  title={Autodan: Automatic and interpretable adversarial attacks on large language models},
  author={Zhu, Sicheng and Zhang, Ruiyi and An, Bang and Wu, Gang and Barrow, Joe and Wang, Zichao and Huang, Furong and Nenkova, Ani and Sun, Tong},
  journal={arXiv preprint arXiv:2310.15140},
  year={2023}
}

@article{gradsafe,
  title={GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis},
  author={Xie, Yueqi and Fang, Minghong and Pi, Renjie and Gong, Neil},
  journal={arXiv preprint arXiv:2402.13494},
  year={2024}
}

@article{llamaguard,
  title={Llama guard: Llm-based input-output safeguard for human-ai conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}

@article{gradientcuff,
  title={Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes},
  author={Hu, Xiaomeng and Chen, Pin-Yu and Ho, Tsung-Yi},
  journal={arXiv preprint arXiv:2403.00867},
  year={2024}
}

@article{led,
  title={Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing},
  author={Zhao, Wei and Li, Zhe and Li, Yige and Zhang, Ye and Sun, Jun},
  journal={arXiv preprint arXiv:2405.18166},
  year={2024}
}

@article{self-reminder,
  title={Defending chatgpt against jailbreak attack via self-reminders},
  author={Xie, Yueqi and Yi, Jingwei and Shao, Jiawei and Curl, Justin and Lyu, Lingjuan and Chen, Qifeng and Xie, Xing and Wu, Fangzhao},
  journal={Nature Machine Intelligence},
  volume={5},
  number={12},
  pages={1486--1496},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{ppl,
  title={Detecting language model attacks with perplexity},
  author={Alon, Gabriel and Kamfonas, Michael},
  journal={arXiv preprint arXiv:2308.14132},
  year={2023}
}

@article{Paraphrase,
  title={Baseline defenses for adversarial attacks against aligned language models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023}
}

@article{Self-Examination,
  title={Llm self defense: By self examination, llms know they are being tricked},
  author={Helbling, Alec and Phute, Mansi and Hull, Matthew and Chau, Duen Horng},
  journal={arXiv preprint arXiv:2308.07308},
  year={2023}
}

@article{safedecoding,
  title={SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding},
  author={Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Jia, Jinyuan and Lin, Bill Yuchen and Poovendran, Radha},
  journal={arXiv preprint arXiv:2402.08983},
  year={2024}
}

@inproceedings{rain,
    title={{RAIN}: Your Language Models Can Align Themselves without Finetuning},
    author={Yuhui Li and Fangyun Wei and Jinjing Zhao and Chao Zhang and Hongyang Zhang},
    booktitle={Proc. of {ICLR}},
    year={2024},
}

@inproceedings{zulu,
    title={Low-Resource Languages Jailbreak {GPT}-4},
    author={Zheng Xin Yong and Cristina Menghini and Stephen Bach},
    booktitle={Proc. of {NeurIPS} {SoLaR} Workshop},
    year={2023},
}

@inproceedings{base64,
 author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
 booktitle = {Proc. of {NeurIPS}},
 pages = {80079--80110},
 title = {Jailbroken: How Does LLM Safety Training Fail?},
 volume = {36},
 year = {2023}
}

@article{luo2024understanding,
  title={From Understanding to Utilization: A Survey on Explainability for Large Language Models},
  author={Luo, Haoyan and Specia, Lucia},
  journal={arXiv preprint arXiv:2401.12874},
  year={2024}
}

@inproceedings{geva-etal-2021-transformer,
    title = "Transformer Feed-Forward Layers Are Key-Value Memories",
    author = "Geva, Mor  and
      Schuster, Roei  and
      Berant, Jonathan  and
      Levy, Omer",
    booktitle = "Proc. of {EMNLP}",
    year = "2021",
    pages = "5484--5495"
}

@inproceedings{dro,
  title={On prompt-driven safeguarding for large language models},
  author={Zheng, Chujie and Yin, Fan and Zhou, Hao and Meng, Fandong and Zhou, Jie and Chang, Kai-Wei and Huang, Minlie and Peng, Nanyun},
  booktitle={Proc. of {ICML}},
  year={2024}
}

@inproceedings{lrh-1,
  title={Linguistic regularities in continuous space word representations},
  author={Mikolov, Tom{\'a}{\v{s}} and Yih, Wen-tau and Zweig, Geoffrey},
  booktitle={Proc. of NAACL-HLT},
  year={2013}
}

@article{lrh-2,
  title={Toy models of superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal={arXiv preprint arXiv:2209.10652},
  year={2022}
}

@inproceedings{lrh-3,
  title={Emergent Linear Representations in World Models of Self-Supervised Sequence Models},
  author={Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  booktitle={Proc. of BlackboxNLP},
  year={2023}
}

@article{youden,
  title={Index for rating diagnostic tests},
  author={Youden, William J},
  journal={Cancer},
  volume={3},
  number={1},
  pages={32--35},
  year={1950},
  publisher={Wiley Online Library}
}

@article{icd,
  title={Jailbreak and guard aligned language models with only few in-context demonstrations},
  author={Wei, Zeming and Wang, Yifei and Wang, Yisen},
  journal={arXiv preprint arXiv:2310.06387},
  year={2023}
}

@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{claude,
  author    = {Anthropic},
  title     = {Introducing Claude},
  year      = {2023},
  howpublished  = {\url{https://www.anthropic.com/news/introducing-claude}},
}

@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{rlhf,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Proc. of NeurIPS},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{rlhf2,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Proc. of NeurIPS},
  volume={30},
  year={2017}
}

@article{alignment1,
  title={Beavertails: Towards improved safety alignment of llm via a human-preference dataset},
  author={Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong},
  journal={Proc. of NeurIPS},
  volume={36},
  year={2024}
}

@article{alignment2,
  title={Aligning large language models with human: A survey},
  author={Wang, Yufei and Zhong, Wanjun and Li, Liangyou and Mi, Fei and Zeng, Xingshan and Huang, Wenyong and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2307.12966},
  year={2023}
}

@article{alignment3,
  title={Assessing the brittleness of safety alignment via pruning and low-rank modifications},
  author={Wei, Boyi and Huang, Kaixuan and Huang, Yangsibo and Xie, Tinghao and Qi, Xiangyu and Xia, Mengzhou and Mittal, Prateek and Wang, Mengdi and Henderson, Peter},
  journal={arXiv preprint arXiv:2402.05162},
  year={2024}
}

@article{alignment4,
  title={Foundational challenges in assuring alignment and safety of large language models},
  author={Anwar, Usman and Saparov, Abulhair and Rando, Javier and Paleka, Daniel and Turpin, Miles and Hase, Peter and Lubana, Ekdeep Singh and Jenner, Erik and Casper, Stephen and Sourbut, Oliver and others},
  journal={arXiv preprint arXiv:2404.09932},
  year={2024}
}

@article{rlaif,
  title={Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie and Mesnard, Thomas and Bishop, Colton and Carbune, Victor and Rastogi, Abhinav},
  journal={arXiv preprint arXiv:2309.00267},
  year={2023}
}

@inproceedings {ijp2,
    author = {Zhiyuan Yu and Xiaogeng Liu and Shunning Liang and Zach Cameron and Chaowei Xiao and Ning Zhang},
    title = {Don{\textquoteright}t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models},
    booktitle = {Proc. of USENIX Security},
    year = {2024},
    pages = {4675--4692},
}

@article{open-source-align,
  title={Defending against alignment-breaking attacks via robustly aligned llm},
  author={Cao, Bochuan and Cao, Yuanpu and Lin, Lu and Chen, Jinghui},
  journal={arXiv preprint arXiv:2309.14348},
  year={2023}
}

@article{close-source-align,
  title={Openassistant conversations-democratizing large language model alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi Rui and Stevens, Keith and Barhoum, Abdullah and Nguyen, Duc and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={Proc. of NeurIPS},
  volume={36},
  year={2024}
}

@article{sorrybench,
  title={Sorry-bench: Systematically evaluating large language model safety refusal behaviors},
  author={Xie, Tinghao and Qi, Xiangyu and Zeng, Yi and Huang, Yangsibo and Sehwag, Udari Madhushani and Huang, Kaixuan and He, Luxi and Wei, Boyi and Li, Dacheng and Sheng, Ying and others},
  journal={arXiv preprint arXiv:2406.14598},
  year={2024}
}

@article{detoxifying,
  title={Detoxifying Large Language Models via Knowledge Editing},
  author={Wang, Mengru and Zhang, Ningyu and Xu, Ziwen and Xi, Zekun and Deng, Shumin and Yao, Yunzhi and Zhang, Qishen and Yang, Linyi and Wang, Jindong and Chen, Huajun},
  journal={arXiv preprint arXiv:2403.14472},
  year={2024}
}

@inproceedings{abs-semantic1,
  title={" You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation},
  author={Ettinger, Allyson and Hwang, Jena D and Pyatkin, Valentina and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={Proc. of EMNLP},
  year={2023}
}

@article{abs-semantic12,
  title={Chartgpt: Leveraging llms to generate charts from abstract natural language},
  author={Tian, Yuan and Cui, Weiwei and Deng, Dazhen and Yi, Xinjing and Yang, Yurun and Zhang, Haidong and Wu, Yingcai},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2024},
  publisher={IEEE}
}

@article{interpretjailbreak1,
  title={Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis},
  author={Lin, Yuping and He, Pengfei and Xu, Han and Xing, Yue and Yamada, Makoto and Liu, Hui and Tang, Jiliang},
  journal={arXiv preprint arXiv:2406.10794},
  year={2024}
}

@article{interpretjailbreak2,
  title={Uncovering Safety Risks of Large Language Models through Concept Activation Vector}, 
  author={Zhihao Xu and Ruixuan Huang and Changyu Chen and Shuai Wang and Xiting Wang},
  year={2024},
  journal={arXiv preprint arXiv:2404.12038}
}

@article{interpretjailbreak3,
  title={How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States},
  author={Zhou, Zhenhong and Yu, Haiyang and Zhang, Xinghua and Xu, Rongwu and Huang, Fei and Li, Yongbin},
  journal={arXiv preprint arXiv:2406.05644},
  year={2024}
}

@article{interpretjailbreak4,
  title={Refusal in Language Models Is Mediated by a Single Direction},
  author={Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Rimsky, Nina and Gurnee, Wes and Nanda, Neel},
  journal={arXiv preprint arXiv:2406.11717},
  year={2024}
}

@article{layer-critical,
  title={Causality analysis for evaluating the security of large language models},
  author={Zhao, Wei and Li, Zhe and Sun, Jun},
  journal={arXiv preprint arXiv:2312.07876},
  year={2023}
}

@misc{perspectiveapi,
  author    = {Jigsaw},
  title     = {Perspective API},
  year      = {2021},
  howpublished       = {\url{https://perspectiveapi.com}},
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    howpublished = {\url{https://lmsys.org/blog/2023-03-30-vicuna/}},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    year = {2023}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{finetuning,
  title={Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!},
  author={Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
  booktitle={Proc. of ICLR},
  year={2024}
}

@inproceedings{mmlu,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={Proc. of ICLR},
  year={2021}
}

@inproceedings{discrete,
  title={Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework},
  author={Yuan, Lifan and Zhang, Yichi and Chen, Yangyi and Wei, Wei},
  booktitle={Proc. of ACL},
  pages={7132--7146},
  year={2023}
}

@inproceedings{semantically-rich1,
  title={Canarex: Contextually aware narrative extraction for semantically rich text-as-data applications},
  author={Anantharama, Nandini and Angus, Simon and Oâ€™Neill, Lachlan},
  booktitle={Proc. of EMNLP},
  pages={3551--3564},
  year={2022}
}

@article{semantically-rich2,
  title={impresso Text Reuse at Scale. An interface for the exploration of text reuse data in semantically enriched historical newspapers},
  author={D{\"u}ring, Marten and Romanello, Matteo and Ehrmann, Maud and Beelen, Kaspar and Guido, Daniele and Deseure, Brecht and Bunout, Estelle and Keck, Jana and Apostolopoulos, Petros},
  journal={Frontiers in big Data},
  volume={6},
  pages={1249469},
  year={2023},
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{discrete2,
  title={Datasheets for digital cultural heritage datasets},
  author={Alkemade, Henk and Claeyssens, Steven and Colavizza, Giovanni and Freire, Nuno and Lehmann, J{\"o}rg and Neudeker, Clemens and Osti, Giulia and van Strien, Daniel and others},
  journal={Journal of open humanities data},
  volume={9},
  number={17},
  pages={1--11},
  year={2023}
}

@article{ppl-thers,
  title={Baseline defenses for adversarial attacks against aligned language models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023}
}

@article{template-based,
  title={Jailbreak Attacks and Defenses Against Large Language Models: A Survey},
  author={Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi},
  journal={arXiv preprint arXiv:2407.04295},
  year={2024}
}

@article{toxic-explain1,
  title={Prompt packer: Deceiving llms through compositional instruction with hidden attacks},
  author={Jiang, Shuyu and Chen, Xingshu and Tang, Rui},
  journal={arXiv preprint arXiv:2310.10077},
  year={2023}
}

@article{toxic-explain2,
  title={No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks},
  author={Leong, Chak Tou and Cheng, Yi and Xu, Kaishuai and Wang, Jian and Wang, Hanlin and Li, Wenjie},
  journal={arXiv preprint arXiv:2405.16229},
  year={2024}
}

@inproceedings{llmfuzzer,
  title={$\{$LLM-Fuzzer$\}$: Scaling assessment of large language model jailbreaks},
  author={Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu},
  booktitle={Proc. of USENIX Security},
  pages={4657--4674},
  year={2024}
}

@article{or-bench,
  title={OR-Bench: An Over-Refusal Benchmark for Large Language Models},
  author={Cui, Justin and Chiang, Wei-Lin and Stoica, Ion and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:2405.20947},
  year={2024}
}