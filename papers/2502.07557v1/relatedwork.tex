\section{Background and Related Works}
\subsection{Jailbreak Attacks on LLMs}

Jailbreak attacks are designed to create malicious inputs that prompt target LLMs to generate outputs that violate predefined safety or ethical guidelines. Carlini \etal~\cite{jb1} first suggested that improved NLP adversarial attacks could achieve jailbreaking on aligned LLMs and encouraged further research in this area. Since then, various jailbreak attack methods have emerged. We categorize these attacks into five principal types: manual-designed jailbreaks, optimization-based jailbreaks, template-based jailbreaks, linguistics-based jailbreaks, and encoding-based jailbreaks. Table \ref{tab:jailbreak} provides a comprehensive summary of these attacks.

\noindent\textbf{Manually-designed Jailbreaks}. Manual-designed jailbreaks refer to attack strategies in which the adversarial prompts are delicately crafted by humans. Unlike automated methods that rely on algorithmic generation, these attacks are conceived directly by individuals who have a nuanced understanding of the operational mechanics and vulnerabilities of LLMs.
In this study, we focus on in-the-wild jailbreak prompts (IJP)~\cite{ijp,ijp2}, which are real-world examples observed in actual deployments and shared by users on social media platforms.

\noindent\textbf{Optimization-based Jailbreaks}. 
Optimization-based jailbreaks use automated algorithms that exploit the internal gradients of LLMs to craft malicious soft prompts. Inspired by AutoPrompt, Greedy Coordinate Gradient (GCG)~\cite{gcg} employs a greedy algorithm to modify input prompts by adding an adversarial suffix, prompting the LLM to start its response with ``Sure'' Building on GCG, Simple Adaptive Attacks (SAA)~\cite{saa} use hand-crafted prompt templates and a random search strategy to find effective adversarial suffixes.

\noindent\textbf{Template-based Jailbreaks}. 
Template-based attacks generate jailbreak prompts by optimizing sophisticated templates and embedding the original harmful requests within them. Such prompts can bypass the safety guardrails of LLMs, making the model more likely to execute prohibited user requests~\cite{template-based}.
MasterKey~\cite{MasterKey} trains a jailbreak-oriented LLM on a dataset of jailbreak prompts to generate effective adversarial inputs. LLM-Fuzzer~\cite{llmfuzzer} begins with human-written templates as seeds and uses an LLM to mutate these templates into new jailbreak inputs. AutoDAN~\cite{autodan} applies a hierarchical genetic algorithm for fine-grained optimization of jailbreak prompts at the sentence and word levels, assisted by an LLM. Prompt Automatic Iterative Refinement (PAIR)~\cite{pair} and Tree of Attacks with Pruning (TAP)~\cite{tap} employ an attacker LLM to target another LLM explicitly, and successfully attack target models with minimal queries.

\noindent\textbf{Linguistics-based Jailbreaks}. Linguistics-based jailbreaks, also known as indirect jailbreaks, conceal malicious intentions within seemingly benign inputs to bypass defensive guardrails in target LLMs. DrAttack~\cite{drattack} decomposes and reconstructs malicious prompts, embedding the intent within the reassembled context to evade detection. Puzzler~\cite{puzzler} analyzes LLM defense strategies and provides implicit clues about the original malicious query to the target model.

\noindent\textbf{Encoding-Based Jailbreaks}. Encoding-based jailbreaks manipulate the encoding or transformation of inputs to bypass LLM security measures. Zulu~\cite{zulu} translates inputs into low-resource languages, exploiting the limited capabilities of LLMs in these languages. Base64~\cite{base64} encodes malicious inputs in Base64 format to obfuscate their true intent.



\subsection{Defenses against Jailbreaks}

As jailbreak attacks on LLMs become more and more powerful, developing robust defenses is crucial. We review existing defense methods\footnote{Some of these methods initially just focus on input toxicity, but can be naturally extended to address jailbreaks.}, categorizing them into two main types: jailbreak detection and jailbreak mitigation~\cite{safedecoding}. A summary of jailbreak defenses is provided in Table \ref{tab:defense}.

\noindent\textbf{Jailbreak Detection}. 
Jailbreak detection aims to identify malicious inputs attempting to bypass guardrails in LLMs. Gradient cuff~\cite{gradientcuff} detects jailbreak prompts by using the gradient norm of the refusal loss, based on the observation that malicious inputs are sensitive to perturbations in their hidden states. Self-Examination (Self-Ex)~\cite{Self-Examination} feeds the model output back to itself to assess whether the response is harmful, leveraging its ability to scrutinize the outputs. SmoothLLM~\cite{smoothllm} introduces random noise to outputs and monitors variability in responses to detect jailbreak inputs, exploiting the sensitivity of adversarial samples to perturbations. PPL~\cite{ppl} flags inputs as malicious if they produce perplexity above a certain threshold. GradSafe~\cite{gradsafe} distinguishes harmful from benign inputs by identifying different gradient patterns triggered in the model. The Llama-guard series~\cite{llamaguard} consists of LLMs fine-tuned specifically for harmful content detection. However, these methods rely on external safeguards that terminate interactions and generate fixed safe outputs, rather than enabling LLMs to produce safe responses autonomously.

\noindent\textbf{Jailbreak Mitigation}. 
The goal of jailbreak mitigation is to preserve the integrity, safety, and intended functionality of LLMs, even when facing attempts to bypass their constraints. Self-Reminder (Self-Re)~\cite{self-reminder} modifies system prompts to remind the model to produce responsible outputs, reinforcing alignment with ethical guidelines. Paraphrase (PR)~\cite{Paraphrase} uses LLMs to rephrase user inputs, filtering out potential jailbreak attempts. In-Context Defense (ICD)~\cite{icd} incorporates demonstrations rejecting harmful prompts into user inputs, leveraging in-context learning to enhance robustness. SafeDecoding (SD)~\cite{safedecoding} fine-tunes the decoding module to prioritize safe tokens, reducing the risk of harmful outputs. Layer-specific Editing (LED)~\cite{led} fine-tunes the key layers critical for safety in LLMs, enhancing their robustness against manipulative inputs. Directed Representation Optimization (DRO)~\cite{dro} fine-tunes a prefix of the input to shift harmful input representations closer to benign ones, promoting safer outputs.