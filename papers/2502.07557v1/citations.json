[
  {
    "index": 0,
    "papers": [
      {
        "key": "jb1",
        "author": "Nicholas Carlini and Milad Nasr and Christopher A. Choquette-Choo and Matthew Jagielski and Irena Gao and Pang Wei Koh and Daphne Ippolito and Florian Tram{\\`e}r and Ludwig Schmidt",
        "title": "Are aligned neural networks adversarially aligned?"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ijp",
        "author": "Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang",
        "title": "\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models"
      },
      {
        "key": "ijp2",
        "author": "Zhiyuan Yu and Xiaogeng Liu and Shunning Liang and Zach Cameron and Chaowei Xiao and Ning Zhang",
        "title": "Don{\\textquoteright}t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "gcg",
        "author": "Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "saa",
        "author": "Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas",
        "title": "Jailbreaking leading safety-aligned llms with simple adaptive attacks"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "template-based",
        "author": "Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi",
        "title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "MasterKey",
        "author": "Deng, Gelei and Liu, Yi and Li, Yuekang and Wang, Kailong and Zhang, Ying and Li, Zefeng and Wang, Haoyu and Zhang, Tianwei and Liu, Yang",
        "title": "Jailbreaker: Automated jailbreak across multiple large language model chatbots"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "llmfuzzer",
        "author": "Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu",
        "title": "$\\{$LLM-Fuzzer$\\}$: Scaling assessment of large language model jailbreaks"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "autodan",
        "author": "Zhu, Sicheng and Zhang, Ruiyi and An, Bang and Wu, Gang and Barrow, Joe and Wang, Zichao and Huang, Furong and Nenkova, Ani and Sun, Tong",
        "title": "Autodan: Automatic and interpretable adversarial attacks on large language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "pair",
        "author": "Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric",
        "title": "Jailbreaking black box large language models in twenty queries"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "tap",
        "author": "Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin",
        "title": "Tree of attacks: Jailbreaking black-box llms automatically"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "drattack",
        "author": "Li, Xirui and Wang, Ruochen and Cheng, Minhao and Zhou, Tianyi and Hsieh, Cho-Jui",
        "title": "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "puzzler",
        "author": "Chang, Zhiyuan and Li, Mingyang and Liu, Yi and Wang, Junjie and Wang, Qing and Liu, Yang",
        "title": "Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zulu",
        "author": "Zheng Xin Yong and Cristina Menghini and Stephen Bach",
        "title": "Low-Resource Languages Jailbreak {GPT}-4"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "base64",
        "author": "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
        "title": "Jailbroken: How Does LLM Safety Training Fail?"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "safedecoding",
        "author": "Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Jia, Jinyuan and Lin, Bill Yuchen and Poovendran, Radha",
        "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "gradientcuff",
        "author": "Hu, Xiaomeng and Chen, Pin-Yu and Ho, Tsung-Yi",
        "title": "Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "Self-Examination",
        "author": "Helbling, Alec and Phute, Mansi and Hull, Matthew and Chau, Duen Horng",
        "title": "Llm self defense: By self examination, llms know they are being tricked"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "smoothllm",
        "author": "Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J",
        "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "ppl",
        "author": "Alon, Gabriel and Kamfonas, Michael",
        "title": "Detecting language model attacks with perplexity"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "gradsafe",
        "author": "Xie, Yueqi and Fang, Minghong and Pi, Renjie and Gong, Neil",
        "title": "GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "llamaguard",
        "author": "Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others",
        "title": "Llama guard: Llm-based input-output safeguard for human-ai conversations"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "self-reminder",
        "author": "Xie, Yueqi and Yi, Jingwei and Shao, Jiawei and Curl, Justin and Lyu, Lingjuan and Chen, Qifeng and Xie, Xing and Wu, Fangzhao",
        "title": "Defending chatgpt against jailbreak attack via self-reminders"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "Paraphrase",
        "author": "Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom",
        "title": "Baseline defenses for adversarial attacks against aligned language models"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "icd",
        "author": "Wei, Zeming and Wang, Yifei and Wang, Yisen",
        "title": "Jailbreak and guard aligned language models with only few in-context demonstrations"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "safedecoding",
        "author": "Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Jia, Jinyuan and Lin, Bill Yuchen and Poovendran, Radha",
        "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "led",
        "author": "Zhao, Wei and Li, Zhe and Li, Yige and Zhang, Ye and Sun, Jun",
        "title": "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "dro",
        "author": "Zheng, Chujie and Yin, Fan and Zhou, Hao and Meng, Fandong and Zhou, Jie and Chang, Kai-Wei and Huang, Minlie and Peng, Nanyun",
        "title": "On prompt-driven safeguarding for large language models"
      }
    ]
  }
]