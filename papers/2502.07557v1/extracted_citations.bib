@article{MasterKey,
  title={Jailbreaker: Automated jailbreak across multiple large language model chatbots},
  author={Deng, Gelei and Liu, Yi and Li, Yuekang and Wang, Kailong and Zhang, Ying and Li, Zefeng and Wang, Haoyu and Zhang, Tianwei and Liu, Yang},
  journal={arXiv preprint arXiv:2307.08715},
  year={2023}
}

@article{Paraphrase,
  title={Baseline defenses for adversarial attacks against aligned language models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023}
}

@article{Self-Examination,
  title={Llm self defense: By self examination, llms know they are being tricked},
  author={Helbling, Alec and Phute, Mansi and Hull, Matthew and Chau, Duen Horng},
  journal={arXiv preprint arXiv:2308.07308},
  year={2023}
}

@article{autodan,
  title={Autodan: Automatic and interpretable adversarial attacks on large language models},
  author={Zhu, Sicheng and Zhang, Ruiyi and An, Bang and Wu, Gang and Barrow, Joe and Wang, Zichao and Huang, Furong and Nenkova, Ani and Sun, Tong},
  journal={arXiv preprint arXiv:2310.15140},
  year={2023}
}

@inproceedings{base64,
 author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
 booktitle = {Proc. of {NeurIPS}},
 pages = {80079--80110},
 title = {Jailbroken: How Does LLM Safety Training Fail?},
 volume = {36},
 year = {2023}
}

@article{drattack,
  title={DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers},
  author={Li, Xirui and Wang, Ruochen and Cheng, Minhao and Zhou, Tianyi and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:2402.16914},
  year={2024}
}

@inproceedings{dro,
  title={On prompt-driven safeguarding for large language models},
  author={Zheng, Chujie and Yin, Fan and Zhou, Hao and Meng, Fandong and Zhou, Jie and Chang, Kai-Wei and Huang, Minlie and Peng, Nanyun},
  booktitle={Proc. of {ICML}},
  year={2024}
}

@article{gcg,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{gradientcuff,
  title={Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes},
  author={Hu, Xiaomeng and Chen, Pin-Yu and Ho, Tsung-Yi},
  journal={arXiv preprint arXiv:2403.00867},
  year={2024}
}

@article{gradsafe,
  title={GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis},
  author={Xie, Yueqi and Fang, Minghong and Pi, Renjie and Gong, Neil},
  journal={arXiv preprint arXiv:2402.13494},
  year={2024}
}

@article{icd,
  title={Jailbreak and guard aligned language models with only few in-context demonstrations},
  author={Wei, Zeming and Wang, Yifei and Wang, Yisen},
  journal={arXiv preprint arXiv:2310.06387},
  year={2023}
}

@article{ijp,
  title={" do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  journal={arXiv preprint arXiv:2308.03825},
  year={2023}
}

@inproceedings{jb1,
    title={Are aligned neural networks adversarially aligned?},
    author={Nicholas Carlini and Milad Nasr and Christopher A. Choquette-Choo and Matthew Jagielski and Irena Gao and Pang Wei Koh and Daphne Ippolito and Florian Tram{\`e}r and Ludwig Schmidt},
    booktitle={Proc. of {NeurIPS}},
    year={2023},
    pages = {61478--61500},
    volume = {36},
}

@article{led,
  title={Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing},
  author={Zhao, Wei and Li, Zhe and Li, Yige and Zhang, Ye and Sun, Jun},
  journal={arXiv preprint arXiv:2405.18166},
  year={2024}
}

@article{llamaguard,
  title={Llama guard: Llm-based input-output safeguard for human-ai conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}

@inproceedings{llmfuzzer,
  title={$\{$LLM-Fuzzer$\}$: Scaling assessment of large language model jailbreaks},
  author={Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu},
  booktitle={Proc. of USENIX Security},
  pages={4657--4674},
  year={2024}
}

@article{pair,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}

@article{ppl,
  title={Detecting language model attacks with perplexity},
  author={Alon, Gabriel and Kamfonas, Michael},
  journal={arXiv preprint arXiv:2308.14132},
  year={2023}
}

@article{puzzler,
  title={Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues},
  author={Chang, Zhiyuan and Li, Mingyang and Liu, Yi and Wang, Junjie and Wang, Qing and Liu, Yang},
  journal={arXiv preprint arXiv:2402.09091},
  year={2024}
}

@article{saa,
  title={Jailbreaking leading safety-aligned llms with simple adaptive attacks},
  author={Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2404.02151},
  year={2024}
}

@article{safedecoding,
  title={SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding},
  author={Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Jia, Jinyuan and Lin, Bill Yuchen and Poovendran, Radha},
  journal={arXiv preprint arXiv:2402.08983},
  year={2024}
}

@article{self-reminder,
  title={Defending chatgpt against jailbreak attack via self-reminders},
  author={Xie, Yueqi and Yi, Jingwei and Shao, Jiawei and Curl, Justin and Lyu, Lingjuan and Chen, Qifeng and Xie, Xing and Wu, Fangzhao},
  journal={Nature Machine Intelligence},
  volume={5},
  number={12},
  pages={1486--1496},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{smoothllm,
  title={SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks},
  author={Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J},
  journal={arXiv preprint arXiv:2310.03684},
  year={2023}
}

@article{tap,
  title={Tree of attacks: Jailbreaking black-box llms automatically},
  author={Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
  journal={arXiv preprint arXiv:2312.02119},
  year={2023}
}

@article{template-based,
  title={Jailbreak Attacks and Defenses Against Large Language Models: A Survey},
  author={Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi},
  journal={arXiv preprint arXiv:2407.04295},
  year={2024}
}

@inproceedings{zulu,
    title={Low-Resource Languages Jailbreak {GPT}-4},
    author={Zheng Xin Yong and Cristina Menghini and Stephen Bach},
    booktitle={Proc. of {NeurIPS} {SoLaR} Workshop},
    year={2023},
}

