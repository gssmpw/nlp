\label{sec:0-abstract}
\begin{abstract}
\vspace{-2mm}
Transformer-based Large Language Models rely critically on KV cache to efficiently handle extended contexts during the decode phase.
Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present \rocketkv, a training-free KV cache compression strategy designed specifically to reduce both memory bandwidth and capacity demand of \kv during the decode phase.
RocketKV contains two consecutive stages. In the first stage, it performs coarse-grain KV cache eviction on the input sequence tokens with SnapKV++, a method improved upon SnapKV by introducing adaptive pooling size and full compatibility with grouped-query attention.
In the second stage, it adopts a hybrid attention method to conduct fine-grain \textit{top-k} sparse attention, approximating the attention scores by leveraging both head and sequence dimensional reductions. Combining these two stages, \rocketkv achieves significant \kv fetching bandwidth and storage savings while maintaining comparable accuracy to full KV cache
attention. We show that \rocketkv provides end-to-end speedup by up to 3$\times$ as well as peak memory reduction by up to 31\% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. 
\end{abstract}