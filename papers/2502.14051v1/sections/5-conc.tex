%\vspace{9ex}
\section{Conclusion}
\label{sec:6-conc}

\rocketkv presents a novel, training-free approach to KV cache compression, addressing the challenges of memory bandwidth and capacity demands during the decode phase of LLM inference. \rocketkv contains two consecutive stages: SnapKV++ for coarse-grain KV cache eviction and hybrid attention for fine-grain dynamic KV token selection. Our evaluations on models and long-context benchmarks demonstrate that \rocketkv maintains comparable accuracy to full KV cache attention while significantly lowering memory bandwidth and capacity usage. At low token budgets of 256 or 512, \rocketkv outperforms all other techniques by a considerable margin.  
It achieves up to 3$\times$ speedup and 31\% peak memory reduction at the decode phase on an NVIDIA H100 GPU compared to the full KV cache baseline, highlighting its efficiency and potential for widespread application in optimizing LLM performance.
