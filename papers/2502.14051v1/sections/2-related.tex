\section{Related Work}
\label{sec:2-related}
A number of recent approaches have focused on improving the efficiency of attention mechanisms in LLMs, particularly when dealing with long contexts. One feasible solution is to share KV cache across multiple layers~\cite{crosslayerAtt2024} or selectively drop attention for some layers~\cite{slimgpt2024}. Others~\cite{yoco2024, block2024} propose mixed attention designs where some layers use global attention while others use local attention. Multi-Query Attention (MQA)~\cite{mqa2019} and Grouped-Query Attention (GQA)~\cite{gqa2023} are widely adopted by many recent LLMs~\cite{metaai2024, mistral2023, gemma2024}, which reduce KV cache by sharing them across multiple attention heads. All the above techniques directly modify the attention architecture and require integration since pre-training.

Another direction is to improve attention efficiency with training-free techniques. These techniques can be further categorized as prefill phase acceleration, decode phase acceleration, or accelerating both phases together. For example, StreamingLLM~\cite{mistral2023} combines initial and local-window attention to reduce the KV cache into a constant size regardless of the sequence length. 
DuoAttention~\cite{duoattention2024} and RazorAttention~\cite{razor2024} improve upon StreamingLLM by applying global attention on retrieval heads and StreamingLLM-style attention on rest heads. While these techniques can be applied to both the prefill and decode phases of LLM inference, other techniques focus on accelerating only one of the two phases.

For prefill phase attention acceleration, MInference~\cite{minference2024} identifies three distinct patterns in long-context 
attention matrices that can be harnessed for efficient sparse operations with customized GPU kernels.
SeerAttention~\cite{seerattention2024} explores dynamic block-level sparsity in attention module with a learnable gate.

A common approach for attention acceleration at the decode phase is through permanent KV cache eviction, which can save both KV cache fetching bandwidth and storage requirements. H2O~\cite{h2o2024} observes that a small subset of tokens, known as heavy-hitters, dominates the attention computation, thus only keeps recent and heavy-hitter tokens. SnapKV~\cite{snapkv2024} employs an observation window at the end of the prompt to identify critical KV tokens of the input prompt. Then it uses a clustering algorithm via pooling to retain critical KV token clusters without loss the completeness of the information.
Quest~\cite{quest2024} observes that permanent KV cache eviction could lead to inevitable accuracy loss, and proposes query-aware selection on \textit{top-k} KV tokens based on approximation attention with representative vectors of contiguous key cache pages. On the other hand, SparQ and Loki~\cite{sparq2024,loki2024} conduct approximation attention by selecting only important indices on the head dimension instead.
The above approaches can save KV cache compute and data fetching from memory but not KV cache storage. InfiniGen~\cite{ infinigen2024} tackles this challenge by offloading the entire KV cache to CPU memory and only fetching the selected KV tokens to GPU memory when needed. MagicPIG\cite{magicpig2024} discovers that using importance sampling is more efficient than \textit{top-k} estimation and proposes an approximation attention solution leveraging locality sensitive hashing (LSH) and CPU offloading.

In contrast to prior work, \rocketkv combines both permanent KV cache eviction and dynamic KV token selection as two consecutive stages for accelerating the decode phase and utilizes the advantages of both worlds. As a result, it achieves remarkable memory bandwidth and storage savings without the need for sophisticated system-level optimizations such as CPU offloading, while maintaining remarkable accuracy even at low token budgets.




