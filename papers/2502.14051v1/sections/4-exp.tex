\section{Experiments}
\label{sec:4-exp}
\subsection{Experimental Settings}


%LB
\begin{figure*}[!ht]
    \centering
    %\vspace{-3mm}
  {\includegraphics[width=\textwidth]{plots/accuracy/legend_horizontal.pdf}}
    %\hfill
    %\vspace{-mm} 
    %\vspace{-2mm}
    \subfigure[\scriptsize LongBench, \llama \label{fig:acc_lb_llama318b}]{  
    \includegraphics[width=0.31\textwidth]{plots/accuracy/LBAvg_llama31_8b_instruct.pdf}}
    \hfill
    \subfigure[\scriptsize LongBench, \mistral \label{fig:acc_lb_mistral7b}]{%
    \includegraphics[width=0.31\textwidth]{plots/accuracy/LBAvg_mistral_7b_instruct_v02.pdf}}
    \hfill
    \subfigure[\scriptsize LongBench, \longchat \label{fig:acc_lb_longchat7b}]{\includegraphics[width=0.321\textwidth]{plots/accuracy/LBAvg_longchat_7b_v15_32k.pdf}}
    %\vspace{-3mm}
    \subfigure[\scriptsize NIAH, \llama \label{fig:acc_nh_llama318b}]{
  \includegraphics[width=0.31\textwidth]{plots/accuracy/NH_llama31_8b_instruct.pdf}
    }
    \hfill
    \subfigure[\scriptsize NIAH, \mistral \label{fig:acc_nh_mistral7b}]{
  \includegraphics[width=0.31\textwidth]{plots/accuracy/NH_mistral_7b_instruct_v02.pdf}
    }
    \hfill
    \subfigure[\scriptsize NIAH, \longchat \label{fig:acc_nh_longchat7b}]{     \includegraphics[width=0.31\textwidth]{plots/accuracy/NH_longchat_7b_v15_32k.pdf}
    }
    %\vspace{-2mm}
    \subfigure[\scriptsize \llama, SeqLen=16K  \label{fig:ruler16000_llama318b_acc}]{     \includegraphics[width=0.23\textwidth]{plots/accuracy/ruler_16000_llama31_8b_instruct.pdf}
    }
    \hfill
    \subfigure[\scriptsize \llama, SeqLen=32K\label{fig:ruler32000_llama318b_acc}]{     \includegraphics[width=0.23\textwidth]{plots/accuracy/ruler_32000_llama31_8b_instruct.pdf}
    }
    \subfigure[\scriptsize {\llama, SeqLen=64K} \label{fig:ruler64000_llama318b_acc}]{    \includegraphics[width=0.23\textwidth]{plots/accuracy/ruler_64000_llama31_8b_instruct.pdf}
    }
    \hfill
    \subfigure[\scriptsize \llama, SeqLen=96K\label{fig:ruler96000_llama318b_acc}]{     \includegraphics[width=0.23\textwidth]{plots/accuracy/ruler_96000_llama31_8b_instruct.pdf}
    }
    %\hfill
    %\vspace{-4mm} 
    \subfigure[\scriptsize\mistral,SeqLen=8K \label{fig:ruler8000_mistral7b_acc}]{
\includegraphics[width=0.23\textwidth]{plots/accuracy/ruler_8000_mistral_7b_instruct_v02.pdf}
    }
    \hfill
    \subfigure[\scriptsize\mistral,SeqLen=16K \label{fig:ruler16000_mistral7b_acc}]{     \includegraphics[width=0.23\textwidth]{plots/accuracy/ruler_16000_mistral_7b_instruct_v02.pdf}
    }
    \hfill
    \subfigure[\scriptsize\mistral,SeqLen=24K \label{fig:ruler24000_mistral7b_acc}]{      \includegraphics[width=0.23\textwidth]{plots/accuracy/ruler_24000_mistral_7b_instruct_v02.pdf}
    }
    \subfigure[\scriptsize\mistral,SeqLen=32K \label{fig:ruler32000_mistralb_acc}]{     \includegraphics[width=0.23\textwidth]{plots/accuracy/ruler_32000_mistral_7b_instruct_v02.pdf}
    }
    %\vspace{-3mm}
    \subfigure[\scriptsize \longchat,SeqLen=8K\label{fig:ruler8000_longchat7b_acc}]{      \includegraphics[width=0.23\textwidth]{plots/accuracy/ruler_8000_longchat_7b_v15_32k.pdf}
    }
    \hfill 
    \subfigure[\scriptsize \longchat,SeqLen=16K\label{fig:ruler16000_longchat7b_ac}]{     \includegraphics[width=0.23\textwidth]{plots/accuracy/ruler_16000_longchat_7b_v15_32k.pdf}
    }
    \hfill
    \subfigure[ \scriptsize \longchat,SeqLen=24K\label{fig:ruler24000_longchat7b_ac}]{     \includegraphics[width=0.23\textwidth]{plots/accuracy/ruler_24000_longchat_7b_v15_32k.pdf}
    }
    \hfill
    \subfigure[\scriptsize\longchat,SeqLen=32K\label{fig:ruler32000_longchat7b_ac}]{    \includegraphics[width=0.23\textwidth]{plots/accuracy/ruler_32000_longchat_7b_v15_32k.pdf}
    }
    \hfill
    %\vspace{-2mm}
    \caption{Comparing the accuracy  of 
    \rocketkv with other methods under LongBench ((a) to (c)), \needle ((d) to (f)), and \ruler with various sequence lengths ((g) to (r)).}
    \label{fig:accuracy_longbench_nh_ruler}

\end{figure*}


We conduct our experiments on three widely used long-context models: Llama3.1-8B-Instruct~\cite{metaai2024}, Mistral-7B-Instruct-v0.2~\cite{mistral2023}, and LongChat-7B-v1.5-32k~\cite{longchat2023}. We use the notations \llama, \mistral, \longchat for them, respectively throughout the paper.
The first two models are based on GQA, while the last one
is based on MHA. For downstream tasks, we utilize LongBench~\cite{longbench2023}, \needle~\cite{needle2023} and RULER~\cite{ruler2024} benchmarks. We
compare RocketKV with several other methods: Full-KV, Exact-TopK, DuoAttention~\cite{duoattention2024}, SnapKV~\cite{snapkv2024}, Quest~\cite{quest2024}, and SparQ~\cite{sparq2024}. Exact-TopK serves as an oracle method for sparse attention with exact \textit{top-k} KV token selection. We evaluate all methods
across various KV token budgets per attention group except for Full-KV. \rocketkv,
Quest, and SparQ involve additional memory traffic for \textit{top-k}
approximation, which is converted into an equivalent KV token budget such that the total token budget precisely reflects
the total amount of memory loads in the attention module. More details on our experimental settings can be
found in Appendix~\ref{sec:app_setting}.



    











\subsection{Accuracy Results }
In our accuracy evaluation, we vary the token budget of each method from 256 to 4096 except for the Full-KV and then compare the average accuracy across all individual tasks for each benchmark. More detailed results with accuracy breakdown for individual tasks can be found in Appendix~\ref{sec:app_detail_results}.

\textbf{\longbench Benchmark:}
The first row in Figure~\ref{fig:accuracy_longbench_nh_ruler} shows the average score comparison of \rocketkv with other methods on LongBench across all three models. Based on the figure, we can see that \rocketkv consistently outperforms all other methods, especially with the lower token budgets. For \llama, \rocketkv achieves almost no accuracy loss with a token budget of 512 and above, and only 1.1\% average accuracy drop with a token budget of 256. \rocketkv results in slightly higher accuracy losses for \mistral and \longchat, which might be because \llama is better-trained than the other two models, making it more robust to sparse attention methods, as a similar trend can be found with Exact-TopK. SparQ performs well on \llama and \longchat with a token budget of 1024 and above but underperforms all other methods on \mistral. SnapKV achieves relatively good accuracy with a token budget of 1024 and above across all three models, however, it cannot further reduce the token budget to 512 or below for tasks in LongBench with 512 maximum generated tokens. This is because it only compresses the KV cache of the input prompt but not the generated tokens, so the token budget cannot reach 512 or below for those tasks. Although we cannot plot the average accuracy score of SnapKV for those cases, the accuracy scores for the remaining tasks are available in Appendix~\ref{sec:app_detail_acc}, and we can still see widening accuracy gaps between SnapKV and \rocketkv for those tasks.



\textbf{\needle Benchmark:}
Presented in the second row of Figure~\ref{fig:accuracy_longbench_nh_ruler}, \rocketkv achieves near the Full-KV accuracy across all models, even with a token budget of 512, and experiences only negligible accuracy loss with a token budget of 256. In contrast, all other methods suffer substantial accuracy drops. For example, the accuracy of \snapkv for \mistral and \longchat decreases by more than 20\% compared to the Full-KV, even with a token budget of 1024. We present the heatmaps of \rocketkv under various token budgets in Appendix~\ref{sec:app_needle_vis}. 


\textbf{\ruler Benchmark:}
For the RULER benchmark, we evaluate all methods across three models with varying sequence lengths as shown in Figure~\ref{fig:accuracy_longbench_nh_ruler} (rows 3-5). Again, \rocketkv shows robust performance and a clear advantage over all other methods across various token budget and sequence length settings. Overall, we can see that the accuracy loss of \rocketkv is negligible under short sequence lengths, and gradually becomes larger as sequence length increases. We believe this is because those models are less robust to sparse attention under longer sequence lengths. Notice that the accuracy gaps between \rocketkv and other methods become even wider under longer sequence lengths.  










\begin{figure}[t]
    \centering
    %\hspace{-1mm}
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/efficiency/efficiency_speedup_llama3.1-8b-instruct.pdf}
        {\small (a) End-to-end speedup}
        \label{fig:eff_speed_llama318b}
    \end{minipage}
    \hspace{15mm}
    %\hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/efficiency/efficiency_memory_savings_llama3.1-8b-instruct.pdf}
        %\vspace{-2ex}
        {\small (b) Peak memory saving}
        \label{fig:eff_mem_llama318b}
    \end{minipage}
    \hfill
    %\vspace{-3mm}
    \caption{End-to-end speedup and peak memory savings of \rocketkv\ with various token budgets compared to the Full-KV.}
    \label{fig:eff_speedup}
    %\vspace{-5ex}
\end{figure}



\subsection{Efficiency Results }
Our efficiency experiments are conducted with \llama~\cite{metaai2024} under FP16 precision, running on an NVIDIA H100 GPU at a batch size of 1. Similar to SparQ, we leverage gpt-fast~\cite{gptfast2023}, a low-latency Python-native

LLM framework for running the efficiency experiments. 
We found a python-based implementation of \rocketkv under gpt-fast is sufficient to demonstrate its efficiency benefit, but it could be further improved with customized CUDA kernels and more advanced frameworks such as FlashInfer~\cite{flashinfer2025}.
Figure~\ref{fig:eff_speedup} demonstrates the end-to-end speedups and peak memory savings of \rocketkv with varying token budgets at the decode phase, with all values normalized to the Full-KV. 
Notice that the peak memory usage measured in the figure includes memory allocations for weights, activations, KV cache, and all other metadata at the decode phase.
As shown in the figure, \rocketkv with token budgets of 512 and 256 achieves up to 2.9$\times$ and 3$\times$ end-to-end speedups as well as 30\% and 31.4\% peak memory savings, respectively. Additionally, increasing the sequence lengths results in almost linear improvements in both speedup and peak memory savings.














\begin{figure*}[!t]
    \centering
    %\vspace{-3mm}
    \subfigure[\scriptsize LongBench, \llama \label{fig:mgqa_LBAvg_llama31}]{
    \includegraphics[width=0.23\textwidth]{plots/mgqa/mgqa_LBAvg_llama31_8b_instruct.pdf}
    }
    \hfill
    \subfigure[\scriptsize LongBench, \mistral \label{fig:mgqa_LBAvg_mistral7b}]{
    \includegraphics[width=0.23\textwidth]{plots/mgqa/mgqa_LBAvg_mistral_7b_instruct_v02.pdf}
    
    }
    \subfigure[\scriptsize NIAH, \llama \label{fig:mgqa_nh_llama31}]{
    \includegraphics[width=0.23\textwidth]{plots/mgqa/mgqa_NH_llama31_8b_instruct.pdf}
    }
    \hfill
    \subfigure[\scriptsize NIAH, \mistral\label{fig:mgqa_nh_mistral7b}]{
    \includegraphics[width=0.23\textwidth]{plots/mgqa/mgqa_NH_mistral_7b_instruct_v02.pdf}
    }
    %\vspace{-5mm}
    \caption{LongBench and \needle accuracy comparison between SnapKV and SnapKV with GQA enhancement.}
    \label{fig:mgqa_longbebch_nh}
     %\vspace{-4mm}
\end{figure*}

\subsection{Ablation Study}
\label{sec:ablation}

In this subsection, we present a series of ablation studies to further demonstrate the effectiveness of \rocketkv. More ablation results can be found in Appendix~\ref{sec:app_detail_results}. 
%\vspace{-3mm}
\subsubsection{SnapKV with GQA enhancement}
As illustrated in Figures~\ref{fig:mgqa_longbebch_nh}, SnapKV with the grouped-query attention (GQA)~\cite{gqa2023} enhancement consistently outperforms the original SnapKV, especially with low token budgets. This is because with the GQA enhancement, the compressed KV tokens are shared across the entire attention group which is far more effective than storing them separately within each attention head.
Recall that, the GQA enhancement does not make any difference for \longchat since it uses MHA in the attention module. 




\begin{figure*}[!t]
    \centering
    %\vspace{-3mm}
    \subfigure[\scriptsize{\llama, SeqLen=16K}\label{fig:r_ruler16000_llama318b}]{       \includegraphics[width=0.23\textwidth]{plots/rocket/rocket_ruler_16000_llama31_8b_instruct.pdf}
    }
    \hfill
    \subfigure[\scriptsize \llama,SeqLen=32K\label{fig:r_ruler32000_llama318b}]{\includegraphics[width=0.23\textwidth]{plots/rocket/rocket_ruler_32000_llama31_8b_instruct.pdf}
    }
    \hfill
    \subfigure[\scriptsize \llama, SeqLen=64K\label{fig:r_ruler64000_llama318b}]{
        \includegraphics[width=0.23\textwidth]{plots/rocket/rocket_ruler_64000_llama31_8b_instruct.pdf}
    }
    \hfill
    \subfigure[\scriptsize \llama, SeqLen=96K\label{fig:r_ruler96000_llama318b}]{
        \includegraphics[width=0.23\textwidth]{plots/rocket/rocket_ruler_96000_llama31_8b_instruct.pdf}
    }
    %\vspace{-3mm} 
    \subfigure[\scriptsize \mistral, SeqLen=8K\label{fig:r_ruler8000_mistral7b}]{
        \includegraphics[width=0.23\textwidth]{plots/rocket/rocket_ruler_8000_mistral_7b_instruct_v02.pdf}
    }
    \hfill
    \subfigure[\scriptsize\mistral,SeqLen=16K\label{fig:r_ruler16000_mistral7b}]{
        \includegraphics[width=0.23\textwidth]{plots/rocket/rocket_ruler_16000_mistral_7b_instruct_v02.pdf}
    }
    %\vspace{-2mm}
    \subfigure[\scriptsize\mistral,SeqLen=24K\label{fig:r_ruler24000_mistral7b}]{
        \includegraphics[width=0.23\textwidth]{plots/rocket/rocket_ruler_24000_mistral_7b_instruct_v02.pdf}
    }
    \hfill
    \subfigure[\scriptsize\mistral,SeqLen=32K\label{fig:r_ruler32000_mistral7b}]{
        \includegraphics[width=0.23\textwidth]{plots/rocket/rocket_ruler_32000_mistral_7b_instruct_v02.pdf}
    }
    %\vspace{-2mm} 
    \subfigure[\scriptsize \longchat, SeqLen=8K\label{fig:r_ruler8000-longchat}]{
        \includegraphics[width=0.23\textwidth]{plots/rocket/rocket_ruler_8000_longchat_7b_v15_32k.pdf}
    }
    \hfill 
    \subfigure[\scriptsize \longchat, SeqLen=16K\label{fig:r
    _ruler16000_longchat}]{
        \includegraphics[width=0.23\textwidth]{plots/rocket/rocket_ruler_16000_longchat_7b_v15_32k.pdf}
    }
    \hfill 
    \subfigure[\scriptsize \longchat, SeqLen=24K\label{fig:r
    -ruler24000_longchat}]{
        \includegraphics[width=0.23\textwidth]{plots/rocket/rocket_ruler_24000_longchat_7b_v15_32k.pdf}
    }
    \hfill 
    \subfigure[\scriptsize \longchat, SeqLen=32K\label{fig:r
    -ruler32000_longchat}]{
        \includegraphics[width=0.23\textwidth]{plots/rocket/rocket_ruler_32000_longchat_7b_v15_32k.pdf}
    }
    %\vspace{-2mm}
    \caption{Comparing \rocketkv with different kernel sizes (7, 63, and 511) under \ruler with various sequence lengths.}
    \label{fig:r_ruler}
    % \vspace{-4mm}
\end{figure*}


%\vspace{-3mm}
\subsubsection{\rocketkv kernel size selection}

We run extensive experiments to examine the impact of the kernel sizes for pooling on the overall accuracy of \rocketkv. We primarily focus on the RULER benchmark to understand the optimal kernel size selection across different sequence lengths, as shown in Figures~\ref{fig:r_ruler}. For \llama, a kernel size of 63 performs best for sequence lengths of 16K and 32K, while a kernel size of 511 performs best for sequence lengths of 64K and 96K. Therefore, we simply set the switching threshold to 48K as the middle point between 32K and 64K. For \mistral, a kernel size of 63 generally performs best across all sequence lengths so there is no need to switch to other kernel sizes. For \longchat, a kernel size of 63 performs best for a sequence length of 8K, and a kernel size of 511 performs best for remaining sequence lengths; so, we set the switching threshold to 12K. We found adaptive kernel selection with empirically determined kernel sizes and threshold generalizes well in other benchmarks (shown in Appendix~\ref{sec:app_detail_ablation}). Hence, we do not explore more sophisticated solutions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

















%%LB
\begin{figure*}[!t]
    \centering
    %\vspace{-3mm}
    \subfigure[\scriptsize LongBench, \llama \label{fig:hqs_LBAvg_llama318b}]{
 \includegraphics[width=0.3\textwidth]{plots/hqs/hqs_LBAvg_llama31_8b_instruct.pdf}
    }
    \hfill
    \subfigure[\scriptsize LongBench, \mistral \label{fig:hqs_LBAvg_mistral7b}]{
        \includegraphics[width=0.3\textwidth]{plots/hqs/hqs_LBAvg_mistral_7b_instruct_v02.pdf}
    }
    \hfill
    \subfigure[\scriptsize LongBench, \longchat \label{fig:hqs_LBAvg_longchat7b}]{
  \includegraphics[width=0.3\textwidth]{plots/hqs/hqs_LBAvg_longchat_7b_v15_32k.pdf}
    }
    %\vspace{-3mm}
    \subfigure[\scriptsize NIAH, \llama \label{fig:hqs_nh_llama31}]{
  \includegraphics[width=0.3\textwidth]{plots/hqs/hqs_NH_llama31_8b_instruct.pdf}
    }
    \hfill
    \subfigure[\scriptsize NIAH, \mistral \label{fig:hqs_nh_mistral7b}]{
        \includegraphics[width=0.3\textwidth]{plots/hqs/hqs_NH_mistral_7b_instruct_v02.pdf}
    }
    \hfill
    \subfigure[\scriptsize NIAH, \longchat \label{fig:hqs_nh_longchat7b}]{
\includegraphics[width=0.3\textwidth]{plots/hqs/hqs_NH_longchat_7b_v15_32k.pdf}
    }
    %\vspace{-2mm}
    \caption{LongBench and \needle accuracy comparison among Hybrid, Quest, and SparQ.}
    \label{fig:hqs_longbench_nh}
    %\vspace{6mm}
\end{figure*}
%\vspace{-1mm}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{-3mm}
\subsubsection{Comparing Hybrid, Quest, and SparQ}

To illustrate the effectiveness of hybrid attention, we compare the accuracy of the standalone hybrid attention mechanism (Hybrid) against Quest and SparQ in the Figures~\ref{fig:hqs_longbench_nh}. In all cases, hybrid attention consistently outperforms Quest and SparQ, especially at low token budgets. This clearly demonstrates the advantage of hybrid attention which intelligently leverages approximations in both head and sequence dimensions compared to single dimension approximation methods such as Quest and SparQ.
 








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




 
