\section{Introduction}
\label{sec:1-intro}

Long-context Large Language Models (LLMs) have enhanced the capabilities of LLMs across diverse tasks that require processing extensive amounts of text. 
Examples of such applications are document summarization, legal document analysis, coding assistants, conversational AI, etc~\cite{wwhlc2024, lost2024, huang2023advancing}. 


In transformer-based LLM~\cite{attention2017} inference, the key-value cache (KV cache)—which stores past attention keys and values to avoid recomputation—becomes a major bottleneck during the decode phase, as its size scales linearly with both the sequence length and batch size. 
For example, the Llama3.1-70B-Instruct~\cite{metaai2024} model with a batch size of 32, and a context length of 32K requires around 320GB of KV cache storage at FP16 precision, which even advanced hardware~\cite{nvidia_h100, tpuv42023, amd_mi300x} can hardly handle.

 
Fortunately, previous work~\cite{h2o2024, quest2024, sparq2024, snapkv2024, loki2024, modeldiscard2024} has shown that only a small subset of KV tokens is required at each decode step to maintain accuracy. Therefore, if those KV tokens can be accurately predicted in advance, dense attention operations can be replaced with sparse attention operations with significant memory bandwidth and capacity improvement. These methods often fall into two categories: 1) permanent KV token eviction, and 2) dynamic KV token selection. The former results in both memory bandwidth and storage savings, but could lead to noticeable accuracy loss if KV tokens dropped earlier are needed by later decode steps. The latter avoids this shortcoming by keeping all KV tokens in the memory and dynamically selecting a subset each time. Hence, it only results in memory bandwidth savings, but often requires extra memory storage overhead for auxiliary data.

Aiming to achieve better trade-offs among memory bandwidth, capacity, and model accuracy, we propose \rocketkv, a two-stage KV cache compression method that combines permanent KV token eviction with dynamic KV token selection to accelerate the decode phase of LLM inference.
In the first stage, we propose SnapKV++ for coarse-grain permanent KV token eviction on the input prompt. SnapKV++ improves upon SnapKV~\cite{snapkv2024} with adaptive pooling size and full compatibility with grouped-query attention (GQA).
In the second stage, \rocketkv performs fine-grain dynamic KV token selection with a hybrid attention mechanism which estimates KV token indices with \textit{top-k} attention scores by leveraging both head and sequence dimensional reductions. 
Combining these two stages together, \rocketkv achieves significant memory bandwidth and storage savings with negligible accuracy loss across a wide variety of models and downstream tasks.

 In summary, the contributions of the paper can be summarized as follows:
 \begin{itemize}
     \item We analyze and identify the limitations of existing KV token dropping methods and then propose \rocketkv with two-stage KV cache compression.
     \item In the first stage, we propose SnapKV++ for prompt KV cache eviction that improves upon SnapKV by introducing adaptive pooling size and full compatibility with GQA. In the second stage, we propose hybrid attention for dynamic KV token selection at each decode step.
     \item We conduct a comprehensive evaluation of \rocketkv on a wide variety of models and downstream tasks. \rocketkv consistently demonstrates comparable accuracy with full-KV attention while maintaining KV token budgets as low as 256.  
     \item We evaluate the efficiency of \rocketkv on an NVIDIA H100 GPU and it shows up to 3$\times$ end-to-end speedup while reducing total memory consumption by up to 31\% at the decode phase compared to the full KV cache.
 \end{itemize}



 


 