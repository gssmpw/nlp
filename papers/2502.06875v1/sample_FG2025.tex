%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
%
% Slightly modified by Shaun Canavan for FG2025
%

%\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
\documentclass[a4paper, 10pt, conference]{IEEEtran}      % Use this line for a4
                                                          % paper
%\usepackage{FG2025}
\renewcommand\IEEEkeywordsname{Keywords}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{url} 
\usepackage{array}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{balance}
\usepackage{array}
\usepackage{multirow}

\newcommand\MyBox[2]{
  \fbox{\lower0.75cm
    \vbox to 1.7cm{\vfil
      \hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
      \vfil}%
  }%
}

%\FGfinalcopy % *** Uncomment this line for the final submission

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
%\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

%\def\FGPaperID{****} % *** Enter the FG2025 Paper ID here

\title{\LARGE \bf
Beyond Vision: How Large Language Models Interpret Facial Expressions from Valence-Arousal Values
}

%use this in case of a single affiliation
%\author{\parbox{16cm}{\centering
%    {\large Huibert Kwakernaak}\\
%    {\normalsize
%    Faculty of Electrical Engineering, Mathematics and Computer Science, University of Twente, Enschede, The Netherlands\\}}
%    \thanks{This work was not supported by any organization.}% <-this % stops a space
%}

%use this in case of several affiliations
\author{\parbox{16cm}{\centering
    {\large Vaibhav Mehra$^{1,*}$, Guy Laban$^{2,*}$, and Hatice Gunes$^2$}\\
    {\normalsize
    $^1$ HEI-Lab, Universidade Lusófona, Lisbon, Portugal\\
    $^2$ Department of Computer Science and Technology, University of Cambridge, Cambridge, United Kingdom}}
    \thanks{*Vaibhav Mehra and Guy Laban have contributed equally to this work and share first authorship. Corresponding author: Guy Laban - Guy.Laban@cl.cam.ac.uk.}% <-this % stops a space
}

\begin{document}

\thispagestyle{empty}
\pagestyle{empty}
%\author{Anonymous FG2025 submission\\ Paper ID \FGPaperID \\}
\pagestyle{plain}
\maketitle



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Large Language Models primarily operate through text-based inputs and outputs, yet human emotion is communicated through both verbal and non-verbal cues, including facial expressions. While Vision-Language Models analyze facial expressions from images, they are resource-intensive and may depend more on linguistic priors than visual understanding. To address this, this study investigates whether LLMs can infer affective meaning from dimensions of facial expressions—Valence-Arousal values, structured numerical representations, rather than using raw visual input. VA values were extracted using Facechannel from images of facial expressions and provided to LLMs in two tasks: (1) categorizing facial expressions into basic (on the IIMI dataset) and complex emotions (on the Emotic dataset) and (2) generating semantic descriptions of facial expressions (on the Emotic dataset). Results from the categorization task indicate that LLMs struggle to classify VA values into discrete emotion categories, particularly for emotions beyond basic polarities (e.g., happiness, sadness). However, in the semantic description task, LLMs produced textual descriptions that align closely with human-generated interpretations, demonstrating a stronger capacity for free-text affective inference of facial expressions. %While LLMs show potential in verbalizing affective information from structured data, their reliance on linguistic priors limits their precision in classification tasks. %These findings suggest that LLMs can complement multimodal emotion recognition systems by providing descriptive insights but require further refinement to enhance their ability to process affective states from structured inputs.



%For intelligent user interfaces to effectively understand emotions, accurate interpretation of facial expressions is essential. This study examines the ability of Large Language Models (LLMs) to interpret facial expressions using valence and arousal dimensions, addressing challenges in visual emotion recognition. Two experiments were conducted: one focused on categorizing emotions, and the other on generating descriptive explanations. Using the IIMI and Emotic datasets, the study compared LLM outputs to human interpretations and traditional models. GPT 4o-mini achieved 44.06\% accuracy in categorization, while GPT 4o-mini recorded the highest semantic similarity (81.84\%) in descriptive tasks. These results highlight LLMs' potential for developing systems that understand complex emotions, paving the way for more natural and emotionally aware interactions.

\end{abstract}

\begin{IEEEkeywords}
Facial Expression, Large Language Models, Affective Computing, Emotion classification, Semantic Emotion Representation, Multimodal AI

\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Large Language Models (LLMs) are predominantly text-based models, designed to process and generate human language naturally. When used as interactive agents, these models rely heavily on verbal inputs and outputs to infer and express emotions. However, human emotion extends beyond words; non-verbal cues, such as facial expressions, convey crucial affective meanings essential to communication \cite{he2017memd, toisoul2021estimation}. Affective communication with artificial agents should include both verbal and non-verbal cues \cite{Share2024} and is influenced by users' emotional states \cite{Open2023}, underscoring the need for AI systems that can interpret and generate meaningful representations of human emotions. As LLMs are increasingly employed in applications requiring emotional intelligence \cite{lbot04}, it is vital to assess their ability to move beyond merely language processing. Accordingly, this work examines the extent to which LLMs can understand and interpret facial expressions, addressing the gap between verbal and non-verbal affective communication in intelligent system design.

Vision-Language Models (VLMs) are used to analyse facial expressions by processing raw visual inputs. These models extract information from images and videos to infer emotions \cite{liu2024speak,yang2024emollm}. However, relying on raw image processing presents several challenges: it requires significant computational resources and raises privacy concerns in sensitive contexts. Moreover, while VLMs demonstrate strong performance in emotion recognition \cite{lei2024large, yao2024vlm}, their reliance on visual input remains unclear. Many of these models integrate multimodal data, but their outputs may be predominantly shaped by linguistic priors rather than genuine visual understanding \cite{Lin2023,Luo2024}. This lack of transparency makes it difficult to assess the role of visual information in their predictions. An alternative approach is to represent emotional information in a structured format, rather than relying on raw visual inputs. One such representation is Valence-Arousal (VA) values \cite{barrett1998discrete}, which quantify expressions along two dimensions: Valence (positivity/negativity) and Arousal (intensity). If models can interpret facial expressions effectively using only VA values, this would reduce reliance on direct image processing while maintaining emotional interpretability. %To explore this possibility, the present study examines whether LLMs can infer emotions based solely on VA values. Understanding how LLMs process VA-based emotion representations could provide insights into their potential role in affective computing, offering an alternative to traditional vision-based approaches while mitigating some of their inherent limitations. While obtaining VA values still requires an initial stage of image processing, once extracted, these numerical representations allow for a more abstract, structured, and privacy-conscious approach to emotion interpretation, independent of direct visual perception.

%To address these challenges, facial expressions can be represented numerically using the Valence-Arousal (VA) model, a well-established dimensional framework in affective computing. Valence captures the positivity or negativity of an emotional state, while arousal measures its intensity. While obtaining VA values still requires an initial stage of image processing, once extracted, these numerical representations allow for a more abstract, structured, and privacy-conscious approach to emotion interpretation, independent of direct visual perception.

%This raises an important question: Can LLMs—despite their lack of direct visual processing capabilities—interpret and describe facial expressions based solely on VA values? While VLMs integrate both textual and visual modalities to analyze facial expressions, evaluating LLMs on VA values alone provides several insights. 
This approach allows us to assess whether LLMs can generalize affective meaning from structured numerical representations of facial expressions, rather than relying on explicit image features. %From a cognitive perspective, it challenges the assumption that emotion recognition requires direct perceptual input, probing whether affective understanding can emerge from structured numerical representations. %If LLMs succeed, this suggests that some aspects of affective cognition are abstractable rather than inherently tied to sensory modalities, aligning with theories of symbolic emotion representation. 
%From a machine learning standpoint, 
Therefore, this study explores whether LLMs’ semantic reasoning can be extended beyond language to structured affective data, offering insights into their latent capacity for cross-modal inference. %If they fail, it would highlight the necessity of embodied perception for certain aspects of emotional understanding. 
%While obtaining VA values still requires an initial visual processing step, using a lightweight feature extraction model %(rather than retaining full visual data in the VLM or LLM) 
%allows for a privacy-conscious approach to affective inference, where numerical representations of facial expression can be processed independently of raw images. VA values provide a modality-agnostic affective representation, meaning that once extracted, they could be used across different AI systems, including speech-based or text-based models, without requiring direct access of the VLM or LLM to visual data. This has implications for applications in mental health, human-robot interaction, and multimodal AI systems that seek to interpret expressions and emotions without requiring direct access to visual content. 
%Moreover, understanding how LLMs process VA values could support their integration into multimodal systems, potentially enhancing their ability to infer affective states when visual data is limited or restricted.
%This study examines the extent to which LLMs can effectively interpret facial expressions using VA values alone, comparing their performance to human interpretations from observation. 
Specifically, we evaluate their ability in two key tasks: \textbf{(1)} categorizing facial expressions into discrete emotional labels and \textbf{(2)} generating semantic descriptions of these expressions. By comparing LLM-generated outputs to human annotations, this study provides insights into the strengths and limitations of LLMs in non-verbal emotion recognition. To achieve these objectives, the study addresses the following research questions:  
\begin{itemize}  
\item [\textbf{RQ1}] \textit{To what extent LLMs can predict basic emotion categories from facial expressions using only VA values?  }
%\item [RQ2] How effectively do LLMs map VA values to basic and complex emotional categories?
\item [\textbf{RQ2}] \textit{To what extent LLMs can semantically describe facial expressions from VA values, and how closely do these descriptions align with human-annotated descriptions?  }
\end{itemize}  

%Large Language Models (LLMs) are designed for processing and generating text. While they rely on verbal inputs and outputs to infer emotions, human emotions also involve non-verbal cues like facial expressions. As LLMs are used in emotionally intelligent applications, this study evaluates their ability to interpret facial expressions (specifically through Valence and arousal values), bridging the gap between verbal and non-verbal communication. Non-verbal cues, especially facial expressions, are key to understanding emotions \cite{he2017memd, toisoul2021estimation}. Using LLMs to interpret emotions through visual signals can advance affective computing and enable LLMs to adapt responses based on a user’s emotional state, fostering more emotionally intelligent interactions.

%Multimodal LLMs are seen as better suited for emotionally aware interactions. Although previous studies \cite{yang2024emollm, liu2024speak} show progress in interaction, their understanding remains unclear. Similarly, vision-language models (VLMs) excel in emotion recognition \cite{lei2024large, yao2024vlm}, but the specific visual contributions remain unclear. Key questions persist: do these models rely on multimodal data, or are their outputs driven by learned patterns? The lack of transparency creates a "black box" effect, complicating research control. To address this, this study evaluates LLMs' ability to interpret facial expressions using only Valence-Arousal (VA) values \cite{barrett1998discrete}, which quantify emotions on a two-dimensional scale: valence (positivity/negativity) and arousal (intensity). Understanding VA is crucial for LLMs to adapt responses based on users' emotional states.

Accordingly, the potential contributions of this study are: 

\begin{itemize}
\item Investigating LLMs’ ability to interpret facial expressions from VA values rather than direct visual input, assessing whether structured numerical representations are sufficient for affective inference.

\item Evaluating the semantic coherence of LLM-generated descriptions by comparing them to human annotations, providing insights into how LLMs conceptualize and verbalize expressions.

\item Examining whether categorical classification or semantic description is more effective for LLMs in deriving meaningful interpretations from VA values, providing insights into their strengths and limitations in structured versus free-form affective inference of facial expressions.

%\item Benchmarking LLM performance in categorization and semantic description tasks of facial expressions, assessing variations across different LLM architectures and evaluating their strengths and limitations in affective reasoning.

%\item Contributing to privacy-conscious affective computing by demonstrating how VA-based emotion interpretation allows for affective inference without retaining raw visual content, supporting applications in mental health, human-robot interaction, and beyond.
\end{itemize}



%\begin{itemize}  
%\item This research evaluates the capability of LLM agents to interpret emotions conveyed through facial expressions, relying exclusively on extracted VA values.  
%\item It further investigates whether a categorical or descriptive approach is more effective for LLMs in translating VA values into meaningful descriptions.  
%\item Finally, the insights gained from this study will serve as groundwork for designing the next generation of multimodal agents, motivating to focus also on transparency and controllability of the architecture.  
%\end{itemize}  

%This study examines LLMs' understanding of VA values linked to facial expressions, focusing on this single feature to simplify the broader task of emotional comprehension. The findings offer a transparent method to evaluate whether LLMs can interpret facial expressions using only VA values, aligning with human understanding. 



%In order to answer these research questions, this paper includes two main experiments. Experiment 1 classifies VA values into emotional categories, divided into two sub-experiments. Experiment 2 generates textual descriptions of emotions from VA values inferred from facial expressions. This approach evaluates LLMs' understanding of VA values and explores methods to explain emotions based on these values. The study highlights effective ways to translate VA values into meaningful text while maintaining simplicity and transparency, avoiding any "black box" effects.


%\section{External datasets and tools used}

\section{Method}

In this study, we conducted two experiments with two distinct datasets. The IIMI dataset \cite{TEWARI_Mehta_Srinivasan_2023} and the Emotic dataset \cite{kosti2017emotic} as detailed in Sections \ref{iimi} and \ref{emotic}. Each dataset included images of facial expressions that were processed using FaceChannel \cite{barros2020facechannel}, an off the shelf package that predicts VA values ranging from -1 to 1 and categorizes facial expressions into basic emotional categories. The extracted VA values were input into LLMs through custom prompts to classify these into categories of emotion (in \textbf{Experiment 1}) or describe the expressions semantically (in \textbf{Experiment 2}). Accordingly, the outputs were analyzed to (\textbf{Experiment 1}) show LLMs' ability to classify expressions from VA values into emotional categories, and (\textbf{Experiment 2}) to demonstrate the extent of similarity between textual descriptions of facial expressions generated by LLMs (based on VA values) to those given by humans (based on their observation). 

\begin{comment}
    
Two experiments were conducted: 
\begin{itemize}
    \item \textbf{Experiment 1: Categorization Task.} Aimed at understanding LLM's ability to classify each data point into emotional categories based on the VA values.
    \item \textbf{Experiment 2: Description Task.} Aimed at understanding LLM's ability to generated textual descriptions of emotions corresponding to the given VA values.
\end{itemize}
\end{comment}


\section{Experiment 1: Categorization task}

%Categorizing VA values into emotional categories is a common method for validating AI systems \cite{alarcao2018identifying, lin2024conversion}. 

Humans often describe facial expressions via variety of different categories of emotion \cite{Du2014,Ekman1992,Jack2016}. To understand LLMs' ability to classify VA values to categories of emotions, a categorization experiment was conducted. The experiment included two sub-experiments. \textbf{Experiment 1.1} tested LLMs' ability to classify VA values into basic emotions (see \cite{ekman1992there}). %like \textit{happiness} and \textit{anger} \cite{ekman1992there}. 
Considering that expressions can correspond to a complex range of emotions, where an expression may align with multiple categories \cite{Liu2022}, \textbf{Experiment 1.2} evaluated  LLMs' ability of mapping VA values also to complex emotions (see \cite{burkitt2002complex}) via a multi-class categorisation task with a larger dataset. %such as \textit{anticipation} and \textit{fatigue} \cite{burkitt2002complex}. 
%The methodology and results of these tasks, using VA values from facial expressions, are described in the following sections.

\subsection{Experiment 1.1: Basic Emotion Categorization}
\label{exp1.1}

\subsubsection{IIMI dataset}
\label{iimi}

The IIMI dataset \cite{TEWARI_Mehta_Srinivasan_2023} contains 700 images of Indian individuals expressing seven basic emotions defined by Ekman’s model (see \cite{ekman1992there}). %: anger, disgust, fear, happiness, sadness, surprise, and neutral. 
The dataset includes 100 images per category, each assigned to a single emotion, making it ideal for single-class classification tasks  \cite{liu2024speak}. %The images focus solely on facial expressions without background distractions, requiring minimal preprocessing for VA value extraction. This helps evaluate the relevance of basic emotions in integrating multimodal capabilities into LLM pipelines \cite{liu2024speak}.  
%Examples of images from dataset can be seen in Fig. 1.

\begin{comment}
    
\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{fig1_LLM_VA.png}
  \caption{Example images and the respective human annotated categories for both IIMI and emotic dataset.}
\end{figure}
\end{comment}

\subsubsection{Methodology}

All images from the IIMI dataset \cite{TEWARI_Mehta_Srinivasan_2023}, were processed with the two models of FaceChannel \cite{barros2020facechannel}. The categorization model classified images into basic emotion categories: Neutral, Happiness, Surprise, Sadness, Anger, Disgust, Fear, and Contempt, consistent with the IIMI dataset. The dimensional model extracted VA values, which were input into the LLM model, GPT-4o-mini \cite{gpt40mini}, using the following prompt:

\begin{tcolorbox}
\small\textit{``The value of valence is [valence\_value], the arousal value is [arousal\_value]. Categorize the associated facial expression in one of the following categories: anger, disgust, fear, happiness, sadness, surprise, or neutral. Respond in no more than a single category."}
\end{tcolorbox}

%GPT 4o-mini \cite{gpt40mini}, known for its cost-efficiency and low computational demands, was selected for its ability to deliver quality results while minimizing environmental impact, making it suitable for this categorization task.

\subsubsection{Analysis}

%After generating outputs, 
Accuracy was calculated as the proportion of correctly categorised images relative to the total number of images in the dataset. The accuracy values for both models were compared to evaluate their performance in emotion classification
\subsubsection{Results}
\label{exp1.1_res}
\begin{comment}
    
%The VA values for all the images in the dataset images shown in Fig. \ref{fig_va_plot_iimi}. 
Although the dataset is balanced, %and represents basic emotions, 
the distribution of preducated VA values in dataset is skewed, with more data points in the negative valence range and fewer points for negative arousal (see  Fig. \ref{fig_va_plot_iimi}), deviating from the expected even distribution.

\begin{figure}[h]
  \centering
  \includegraphics[width=.6\linewidth]{fig5.2_LLM_VA.png}
  \caption{Plot of VA values predicted by FaceChannel for all images in IIMI dataset.}
  \label{fig_va_plot_iimi}
\end{figure}
\end{comment}

%The classification performance of GPT 4o-mini and FaceChannel \cite{barros2020facechannel} is shown in Table \ref{tab:res_cat_iimi}. 
Both models %GPT 4o-mini and the categorization model of FaceChannel  
performed poorly, with accuracies of 30.42\% and 31.42\%, respectively, and show bias towards specific emotion categories. GPT 4o-mini achieves near perfect accuracy for Happiness (87\%) and Sadness (98\%), 22\% for Fear, and almost none for other categories. FaceChannel perfectly predicts Sad and Neutral, achieves 20\% accuracy for Happiness but fails for the rest (See Table \ref{tab:conf_mat_combined}).

\begin{comment}
    

\begin{table}
  \caption{Confusion Matrix for predictions of GPT 4o-mini}
  \label{tab:conf_mat_4o_mini}
  %\footnotesize
  \begin{tabular}{p{1cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.7cm} p{0.5cm} p{0.5cm} p{0.5cm}}
    \toprule
    Categories&Happy&Fear&Neutral&Surprise&Disgust&Sad&Angry\\
    \midrule
    Happy &87&0&13&0&0&0&0\\
    Fear &0&22&0&0&0&78&0\\
    Neutral &56&0&5&0&0&39&0\\
    Surprise &93&0&0&1&0&0&0\\
    Disgust &0&0&0&0&0&100&0\\
    Sad &2&0&0&0&0&98&0\\
    Angry &39&0&0&0&0&61&0\\
  \bottomrule
\end{tabular}
\end{table}

\begin{table}
  \caption{Confusion Matrix for predictions of FaceChannel}
  \label{tab:conf_mat_faceChannel}
  %\footnotesize
  \begin{tabular}{p{1cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.7cm} p{0.5cm} p{0.5cm} p{0.5cm}}
    \toprule
    Categories&Happy&Fear&Neutral&Surprise&Disgust&Sad&Angry\\
    \midrule
    Happy &20&0&80&0&0&0&0\\
    Fear &0&0&34&0&0&66&0\\
    Neutral &0&0&100&0&0&0&0\\
    Surprise &0&0&100&0&0&0&0\\
    Disgust &0&0&89&0&0&11&0\\
    Sad &0&0&0&0&0&100&0\\
    Angry &0&0&0&0&0&100&0\\
  \bottomrule
\end{tabular}
\end{table}

\end{comment}
    
\begin{table*}
  \caption{Confusion Matrices for GPT-4o-mini and FaceChannel Predictions}
  \label{tab:conf_mat_combined}
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{l | c c c c c c c | c c c c c c c}
    \toprule
    & \multicolumn{7}{c|}{\textbf{GPT-4o-mini}} & \multicolumn{7}{c}{\textbf{FaceChannel}} \\
    \textbf{Category} & \textbf{Happy} & \textbf{Fear} & \textbf{Neutral} & \textbf{Surprise} & \textbf{Disgust} & \textbf{Sad} & \textbf{Angry} 
    & \textbf{Happy} & \textbf{Fear} & \textbf{Neutral} & \textbf{Surprise} & \textbf{Disgust} & \textbf{Sad} & \textbf{Angry} \\
    \midrule
    Happy   &  87 &  0  & 13  &  0  &  0  &  0  &  0  &  20  &  0  & 80  &  0  &  0  &  0  &  0  \\
    Fear    &  0  & 22  &  0  &  0  &  0  & 78  &  0  &  0   &  0  & 34  &  0  &  0  & 66  &  0  \\
    Neutral & 56  &  0  &  5  &  0  &  0  & 39  &  0  &  0   &  0  & 100 &  0  &  0  &  0  &  0  \\
    Surprise& 93  &  0  &  0  &  1  &  0  &  0  &  0  &  0   &  0  & 100 &  0  &  0  &  0  &  0  \\
    Disgust &  0  &  0  &  0  &  0  &  0  & 100 &  0  &  0   &  0  & 89  &  0  &  0  & 11  &  0  \\
    Sad     &  2  &  0  &  0  &  0  &  0  & 98  &  0  &  0   &  0  &  0  &  0  &  0  & 100 &  0  \\
    Angry   & 39  &  0  &  0  &  0  &  0  & 61  &  0  &  0   &  0  &  0  &  0  &  0  & 100 &  0  \\
    \bottomrule
  \end{tabular}%
  }
\end{table*}


\begin{comment}
    

\begin{table}
  \caption{Results of Experiment 1.1 for the categorization task}
  \label{tab:res_cat_iimi}
  %\footnotesize
  \begin{tabular}{p{2cm} p{2.5cm} p{2.5cm}}
    \toprule
    Categories&Correct predictions - FaceChannel&Correct predictions - GPT 4o-mini\\
    \midrule
    Happiness & 20/100& 87/100\\
    Fear & 0/100& 22/100\\
    Neutral & 100/100& 5/100\\
    Surprise & 0/100& 1/100\\
    Disgust & 0/100& 0/100\\
    Sad & 100/100& 98/100\\
    Angry & 0/100& 0/100\\
    \midrule
    Total & 220/700& 213/700\\
    Accuracy & 31.42\%& 30.42\%\\
  \bottomrule
\end{tabular}
\end{table}

\end{comment}


\begin{comment}
Nonetheless, the categories predicated by the LLM are matching the corresponding emotional category that mighht have 

according to VA values predicated by FaceChannel are in line with their respected e

Based on the VA values from FaceChannel and the circumplex model of emotions \cite{gerber2008affective}, 

As shown in Fig. \ref{iimi_example}, based on the VA values from FaceChannel and the circumplex model of emotions \cite{gerber2008affective}, predicting {Sad} is reasonable, even though the actual result is {Surprised}.

\begin{figure}[h]
  \centering
  \includegraphics[width=.4\linewidth]{fig8_LLM_VA.png}
  \caption{Example image from IIMI dataset with human annotated category, FaceChannel extracted VA values and LLM predicted category.}
  \label{iimi_example}
\end{figure}
\end{comment}

\subsection{Experiment 1.2: Complex Emotion Categorization}
\label{exp1.2}

%While basic categories of emotions are often considered when describing facial expressions, these expressions can correspond to a more complex and nuanced range of emotions. Moreover, a single expression may align with multiple emotion categories or be interpreted within more than one category. Therefore, in the second subexperiment of Experiment 1 (i.e., Experiment 1.2), we aimed to replicate Experiment 1.1, this time as a multi-class categorisation task with a larger dataset (see Section \ref{emotic}), encompassing a broader and more nuanced set of both basic and complex emotions. Accordingly, this subexperiment extends the findings of Experiment 1.1 by examining LLMs' ability to classify VA values into multiple emotion categories, including more complex ones.

%The second task of experiment 1 is a multi-class categorization conducted on a larger dataset as described in section \ref{emotic} and is further conducted to understand the ability of LLM to translate VA values into more specific and nuanced categories of emotions.

\subsubsection{Emotic Dataset}
\label{emotic}
The Emotic dataset \cite{kosti2017emotic} includes diverse scenarios with individual faces, multiple faces, and social situations. The dataset consists of 12,821 images in the training subset and 3,663 images in the test subset. %To minimize environmental impact \cite{faiz2023llmcarbon}, this study focuses on the testing subset, including only 3,050 images with clear facial expression. %were selected for analysis.
Since the study includes an evaluation task rather than a training task, we used the test subset, which includes 3,047 images with clear facial expressions (after manual inspection). This sample provided sufficient power for statistical analysis (\(\alpha = .05\), 1 - \(\beta = .8\), \(d = .2\)) while also minimizing computational costs and environmental impact \cite{faiz2023llmcarbon}.
Each image in the dataset is annotated with 1 to 9 categories of emotion (according to \cite{kosti2017emotic}) out of 26 categories, ranging from basic \cite{ekman1992there} to more complex emotions \cite{burkitt2002complex,Liu2022}. Each image in the dataset is annotated with VA values by humans while following the method of Mou et al. \cite{mou2015group}. The Emotic dataset was ideal for the task as it includes diverse facial expressions and multi-class emotion labels, enabling an evaluation of LLMs' multi-class categorization abilities in this affective domain.

\begin{comment}
    

Fig. \ref{emotic_cat_example} provides examples of images, categories, and definitions.

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{fig3.2_LLM_VA.png}
  \caption{Example images, their human annotated categories and respective short description of those categories, from the Emotic database.}
  \label{emotic_cat_example}
\end{figure}
\end{comment}


\subsubsection{Methodology}

The images from the Emotic dataset \cite{kosti2017emotic} were processed using FaceChannel's  dimensional model \cite{barros2020facechannel} extracting VA values for each image, which were then provided to the LLMs using the following prompt:

\begin{tcolorbox}
\small\textit{``The value of valence is [valence\_value], the arousal value is [arousal\_value]. Classify the image into [n\_categories] of the most relevant categories from the following 26: Peace, Affection, Esteem, Anticipation, Engagement, Confidence, Happiness, Pleasure, Excitement, Surprise, Sympathy, Doubt/Confusion, Disconnection, Fatigue, Embarrassment, Yearning, Disapproval, Aversion, Annoyance, Anger, Sensitivity, Sadness, Disquietment, Fear, Pain, and Suffering. Respond only with comma-separated category."}
\end{tcolorbox}

Two LLM models, GPT-4o-mini \cite{gpt40mini} and GPT-4o \cite{gpt40}, classified each unit in the dataset based on its VA values (both those provided in the dataset, as well as those extracted using FaceChannel) into \textit{n} emotion categories, corresponding to the number of human-annotated categories. This led to a total of 10,633 classifications for the 3,047 images in the dataset. We utilised GPT-4o-mini due to its lower cost and reduced environmental impact. However, given its poor performance in Experiment 1.1 and the complexity of the task, we also tested GPT-4o.%Outputs were generated using GPT-4o-mini (with original and FaceChannel VA values) and GPT-4o (with FaceChannel VA values).
\subsubsection{Analysis}
Two metrics were calculated to evaluate the multi-class classification task: the percentage of images where at least one predicted category matched the human annotations and the percentage where all predicted categories were an exact match.

\subsubsection{Results}

%The VA values extracted for all 3,047 testing images are shown in Fig. \ref{figure_va_plot_emotic}. 
\begin{comment}
    
Unlike the IIMI dataset \cite{TEWARI_Mehta_Srinivasan_2023} used in Experiment 1.1 (see Section \ref{exp1.1_res}), the 3,047 data points from the Emotic dataset \cite{kosti2017emotic} are evenly distributed across all four quadrants of VA values, indicating no potential bias.

\begin{figure}[h]
  \centering
  \includegraphics[width=.6\linewidth]{fig6.2_LLM_VA.png}
  \caption{Plot of VA values predicted by FaceChannel for the test subset of Emotic dataset.}
  \label{figure_va_plot_emotic}
\end{figure}
\end{comment}

%Using the original VA values from the Emotic dataset, 
GPT 4o-mini correctly predicted at least one category for 49.67\% of images and all categories for 18.32\% of the images. With FaceChannel VA values, it achieved 50.75\% for at least one correct category and 11.01\% for all categories. GPT 4o, using FaceChannel VA values, had lower results: 43.26\% for at least one correct category and 6.91\% for all categories. Surprisingly, GPT-4o performed worse than GPT-4o-mini. The poor accuracy across all cases suggests that LLMs struggle with complex or overlapping emotions beyond basic polarised emotions such as happiness or sadness. Conventional machine learning models may handle such nuanced tasks more effectively \cite{xenos2024vllms}.

\section{Experiment 2: Semantic Description task}
\label{exp2}

LLMs perform better at generating semantically descriptive outputs compared to outputs that are syntactically correct but lack meaningful semantic content \cite{lee2024learning}. This is because their primary use case has been language generation, and they are trained accordingly. Moreover, facial expressions do not always align with discrete emotion categories, as the same expression can convey different emotions and social information \cite{Barrett2019}. As a result, describing expressions in words—capturing their intensity, subtlety, and affective dimensions—may provide a more accurate and flexible representation than rigid classification into predefined emotion labels \cite{Lecker2021, Li2022}. In addition, comparing AI-generated affective explanations to human explanations provides insight into how well AI systems align with human reasoning and social norms \cite{Dogan2025}. Thus, to address the limitations of Experiment 1, \textbf{Experiment 2} aimed at evaluating LLMs' performance in semantically describing facial expressions using only VA values extracted from images. 


%in the earlier experiment, restricting the LLM to predicting categories may have limited its potential. Here, the LLM was tasked with generating descriptive explanations of facial expressions based on VA values. %These descriptions served as the unit of analysis, allowing the LLM to freely interpret VA values without categorical constraints. This approach explores the model's understanding of VA values and their connection to emotional expressions.

\subsection{Methodology} 

We used the FaceChannel dimensional model \cite{barros2020facechannel} to extract VA values from 3,047 images in the test subset (see Section \ref{emotic}) of the EMOTIC dataset \cite{kosti2017emotic}. The Emotic dataset was ideal for the task as it includes diverse facial expressions with human-annotated explanations, enabling a comparison to LLMs' semantic descriptions. These were then submitted to the LLMs to generate \textit{n} semantic descriptions for each unit in the dataset, corresponding to the number of human-annotated descriptions of facial expressions, using the following prompt:

%LLM receives VA values and generates descriptions of the emotions linked to the facial expressions. These descriptions are then compared semantically with predefined category descriptions to assess performance. The agent is prompted as follows:

\begin{tcolorbox}
\small\textit{``The value of valence is [valence\_value], the arousal value is [arousal\_value]. What do you understand from these about the emotions expressed by the facial expressions. In only [n\_categories] independent sentences, describe the expressed emotion and mental states, without mentioning the valence and arousal values."}
\end{tcolorbox}

This led to a total of 10,633 semantic descriptions for the 3,047 images. Three LLMs were tested: GPT 4o, GPT 4o-mini, and LLAMA 3.2 8B Instruct. LLAMA 3.2, an open-source model, offers better accessibility for future research and for replicating the paradigm. This comparison aimed at evaluating performance and generalization. %The experiment used VA values from FaceChannel to check if the generated descriptions matched the emotions in the original images.

\subsection{Analysis}

Semantic similarity between the original and LLM-generated descriptions was calculated using two methods and three models. The first method, \textit{combined semantic similarity}, compares the full LLM-generated description with the concatenated definitions of all human-assigned categories. The second method, \textit{separate semantic similarity}, treats each LLM sentence and category definition independently, calculates an \textit{n × n} similarity matrix, and averages the values. Vector representations of sentences were created using Transformers \cite{vaswani2017attention}, Word2Vec \cite{church2017word2vec}, and BERT \cite{kenton2019bert}. Word2Vec represents words as dense vectors based on co-occurrence patterns in a corpus, capturing local semantic relationships but lacking contextual awareness. In contrast, Transformer-based models dynamically adjust word embeddings based on surrounding context, allowing for a deeper understanding of sentence structure and meaning. BERT, specifically, leverages bidirectional context, making it particularly effective at capturing nuanced semantic relationships \cite{Apidianaki2023}. Cosine similarity was used to compute the final scores.


\begin{comment}
    

The procedure is summarized in \ref{method_sum_exp_2}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{fig4.1_LLM_VA.png}
  \caption{Summary of the methodology of Description-based experiment on Emotic database using LLM Agent.}
  \label{method_sum_exp_2}
\end{figure}
\end{comment}

\begin{table*}

  \caption{Examples of semantically similar GPT-4o-mini predicted descriptions with semantic similarity.} 
  \label{tab:4}
  \renewcommand{\arraystretch}{1.5} % Adjust row height for better alignment
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{>{\centering\arraybackslash}m{3cm}
                  >{\centering\arraybackslash}m{1cm}
                  >{\centering\arraybackslash}m{1cm}
                  >{\centering\arraybackslash}m{3cm} 
                  >{\centering\arraybackslash}m{3cm} 
                  >{\centering\arraybackslash}m{3cm} 
                  >{\centering\arraybackslash}m{3cm} 
                  >{\centering\arraybackslash}m{3cm}}
    \toprule
   \normalsize \textbf{Image} & \normalsize \textbf{Valence} & \normalsize \textbf{Arousal} & \normalsize \textbf{Human Description} & \normalsize \textbf{GPT-4o-mini Description} & \multicolumn{3}{c}{\normalsize \textbf{Semantic similarity}} \\
    \cmidrule(lr){6-8}
    & & & & & \normalsize \textbf{Word2Vec} & \normalsize \textbf{Transformers} & \normalsize \textbf{BERT} \\
    \midrule
    \includegraphics[width=2cm]{example_1_cropped.jpg} & \normalsize -.39 & \normalsize	.61 &  \normalsize \textbf{Disapproval}: feeling that \textbf{something is wrong} or reprehensible; contempt; hostile & \normalsize A state of \textbf{anxiety} or \textbf{agitation}, individual feels \textbf{unease} but is also \textbf{alert} and activated. & \normalsize	 84.27\% & \normalsize	 50.67\% & \normalsize	 76.08\% \\
    \includegraphics[width=2cm]{example_3_cropped.jpg} & \normalsize .88 & \normalsize -.08 &  
 \normalsize	\textbf{Happiness}: Feeling \textbf{delighted}, feeling \textbf{enjoyment} or amusement & \normalsize A moderately \textbf{positive emotional state} characterized by mild \textbf{enthusiasm} or \textbf{contentment}, suggesting a sense of \textbf{optimism} or satisfaction without overwhelming excitement. & \normalsize 82.93\% & \normalsize 55.06\% & \normalsize	78.68\% \\
    \bottomrule
  \end{tabular}%
  }
\end{table*}


To assess the generalizability of the cosine similarity results, Bootstrap testing calculated average cosine similarity scores and 95\% confidence intervals. Since violations of normality are not a concern with large samples like the one used in our study due to the Central Limit Theorem \cite{Altman1995,Knief2021}, and non-parametric tests may be too sensitive with such large samples \cite{Fagerland2012}, a one-sample \textit{t}-test to determine whether similarity scores exceeded the baseline score of .5, which represents similarity above random chance \cite{corley2005measuring}.
%Spearman correlation \cite{arsov2019measure} was also computed to explore relationships between human-generated and LLM-generated semantic vectors. Additionally, \textit{Unlabeled Attachment Score} (UAS) and \textit{Labeled Attachment Score} (LAS) metrics assessed syntactic alignment \cite{dror2018hitchhiker}, with independent \textit{t}-tests comparing UAS and LAS values across models.



\subsection{Results}
\label{exp_2_res}

For the \textbf{GPT-4o-mini}, applying the Word2Vec model with the combined method of similarity calculation yielded an average cosine similarity of $M = .81$, 95\%CI [.81, .82]. A one-sample t-test confirmed that this mean similarity was significantly higher than the baseline value of .5, $t(3046) = 262.57, p < .001$. Using the separate method of similarity calculation for the same model resulted in an average cosine similarity of $M = .72$, 95\%CI [.72, .73], also significantly higher than the baseline, $t(3046) = 259.84, p < .001$. When using the Transformer-based embeddings, the combined method produced a lower similarity of $M = .42$, 95\%CI [.42, .43], and a one-sample t-test indicated that this result was not significantly different from the baseline value, $t(3046) = -40.06, p = 1$. The separate method with Transformer embeddings yielded $M = .31$, 95\%CI [.31, .32], $t(3046) = -31.6, p = 1$. With BERT-based embeddings, the combined method showed an average similarity of $M = .79$, 95\%CI [.78, .79], $t(3046) = 555.14, p < .001$, while the separate method resulted in $M = .62$, 95\%CI [.62, .63], $t(3046) = 182.94, p < .001$.

For the \textbf{GPT-4o}, Word2Vec embeddings with the combined method yielded an average similarity of $M = .80$, 95\%CI [.80, .81], significantly higher than the baseline ($t(3046) = 227.54, p < .001$). The separate method resulted in $M = .74$, 95\%CI [.73, .74], $t(3046) = 225.68, p < .001$. Using Transformer embeddings, the combined method resulted in $M = .39$, 95\%CI [.39, .40], $t(3046)=-60.23, p = 1$, while the separate method produced $M=.28$, 95\%CI [.28, .29], $t(3046)=-67.84, p = 1$. For BERT embeddings, the combined method produced $M=.79$, 95\%CI [.79, .80], $t(3046)=520.12, p < .001$, while the separate method resulted in $M=0.62$, 95\%CI [.61, .62], $t(3046)=473.61, p < .001$.

For the \textbf{LLAMA 3.2 8B Instruct}, Word2Vec embeddings with the combined method produced an average similarity of $M=.77$, 95\%CI [.76, .77], $t(3046)=174.54, p < .001$. The separate method resulted in $M=.75$, 95\%CI [.74, .75], $t(3046)=173.28, p < .001$. For Transformer embeddings, the combined method produced $M=.35$, 95\%CI [.34, .35], $t(3046)=-80.49, p = 1$, while the separate method resulted in $M=.32$, 95\%CI [.32, .33], $t(3046) = -2.2, p = .98$. With BERT embeddings, the combined method showed $M=.75$, 95\%CI [.75, .76], $t(3046)=285.61, p < .001$, while the separate method resulted in $M=.66$, 95\%CI [.65, .66], $t(3046)=239.87, p < .001$. See Table \ref{tab:3} for the results and Table \ref{tab:4} for examples comparing human-annotated descriptions to those generated by the LLMs.%, providing a richer perspective on the semantic alignment across embedding models.

\begin{table}

\caption{Bootstrap mean results and \textit{t}-test results for Cosine similarity results of Experiment 2.}
\label{tab:3}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccc}
    \toprule
    \textbf{Test} & \textbf{Word2Vec} & \textbf{Transformers} & \textbf{BERT} \\
    \midrule
    GPT-4o-mini (Combine) & .81$^{***}$ [.81, .82] & .42 [.42, .43] & .79$^{***}$ [.78, .79] \\
    GPT-4o-mini (Separate) & .72$^{***}$ [.72, .73] & .31 [.31, .32] & .62$^{***}$ [.62, .63] \\
    GPT-4o (Combine) & .80$^{***}$ [.80, .81] & .39 [.39, .40] & .79$^{***}$ [.79, .80] \\
    GPT-4o (Separate) & .74$^{***}$ [.73, .74] & .28 [.28, .29] & .62$^{***}$ [.61, .62] \\
    LLAMA (Combine) & .77$^{***}$ [.76, .77] & .35 [.34, .35] & .75$^{***}$ [.75, .76] \\
    LLAMA (Separate) & .75$^{***}$ [.74, .75] & .32 [.32, .33] & .66$^{***}$ [.65, .66] \\
    \midrule
    \multicolumn{4}{l}{\footnotesize Note: \( p < 0.001 = *** \)} \\
    \bottomrule
\end{tabular}%
}
\end{table}




\begin{comment}
    
\begin{table}
\label{tab:3}
  \caption{Bootstrap mean and T-test results for understanding generalization of Cosine similarity results of Experiment 2.}
  \label{tab:freq}
  %\footnotesize
  \begin{tabular}{p{2cm} p{1cm} p{2cm} p{1cm} p{0.5cm}}
    \toprule
    Test&Bootstrap \textit{M}&\textit{95\%CI}&\textit{t}&\textit{p}\\
    \midrule
    GPT-4o-mini(Combine)-Word2Vec&0.81&[0.81, 0.82]&262.57&***\\
    GPT-4o-mini(Separate)-Word2Vec&0.72&[0.72, 0.73]&259.84&***\\
    GPT-4o(Combine)-Word2Vec&0.80&[0.80, 0.81]&227.54&***\\
    GPT-4o(Separate)-Word2Vec&0.74&[0.73, 0.74]&225.68&***\\
    LLAMA(Combine)-Word2Vec&0.77&[0.76, 0.77]&174.54&***\\
    LLAMA(Separate)-Word2Vec&0.75&[0.74, 0.75]&173.28&***\\
    GPT-4o-mini(Combine)-Transformer&0.42&[0.42, 0.43]&-40.06&1\\
    GPT-4o-mini(Separate)-Transformer&0.31&[0.31, 0.32]&-31.6&1\\
    GPT-4o(Combine)-Transformer&0.39&[0.39, 0.40]&-60.23&1\\
    GPT-4o(Separate)-Transformer&0.28&[0.28, 0.29]&-67.84&1\\
    LLAMA(Combine)-Transformer&0.35&[0.34, 0.35]&-80.49&1\\
    LLAMA(Separate)-Transformer&0.32&[0.32, 0.33]&-2.2&0.98\\
    GPT-4o-mini(Combine)-BERT&0.79&[0.78, 0.79]&555.14&***\\
    GPT-4o-mini(Separate)-BERT&0.62&[0.62, 0.63]&182.94&***\\
    GPT-4o(Combine)-BERT&0.79&[0.79, 0.8]&520.12&***\\
    GPT-4o(Separate)-BERT&0.62&[0.61, 0.62]&473.61&***\\
    LLAMA(Combine)-BERT&0.75&[0.75, 0.76]&285.61&***\\
    LLAMA(Separate)-BERT&0.66&[0.65, 0.66]&239.87&***\\

    \midrule
    \multicolumn{5}{l}{\footnotesize Note: \( p < 0.001 = *** \)} \\

  \bottomrule
\end{tabular}
\end{table}
\end{comment}



\begin{comment}
    

\subsubsection{Spearman Correlation}
A Spearman correlation ($ \rho\ $) was conducted to examine the relationship between original descriptions and descriptions generated by different LLM models. For GPT 4o mini the analysis revealed a significant positive correlation $ \rho\ $=0.41, 95\% CI [.32, .48], p$<$.001. Similarly for GPT 4o the analysis revealed a significant positive correlation $ \rho\ $=0.38, 95\% CI [.29, .46], p$<$.001. Finally for LLAMA 3.2 8B Instruct also the analysis revealed a significant positive correlation as well $ \rho\ $=0.34, 95\% CI [.25, .42], p$<$.01. 

\subsubsection{Unlabeled and Labeled Attachment Scores}

UAS and LAS values were calculated to compare the description of each LLM model with the original descriptions. Paired sample T-test was conducted among each of the LLM models. The results showed significant difference in the UAS of GPT 4o (M=0.49) and 4o-mini (M=0.53), \textit{t} = -8.42, \textit{p} $<$0.001. There was also significant difference in the LAS of GPT 4o (M=0.76) and 4o-mini (M=0.76), \textit{t} = 5.5, \textit{p} $<$0.001. Similarly, significant difference in the UAS of GPT 4o (M=0.49) and LLAMA (M=0.47), \textit{t} = 5.45, \textit{p} $<$0.001, and in the LAS of GPT 4o (M=0.76) and LLAMA (M=0.75), \textit{t} = 27.67, \textit{p} $<$0.001. Finally, significant differences were found in the UAS of GPT 4o-mini (M=0.53) and LLAMA (M=0.47), \textit{t} = 12.97, \textit{p} $<$0.001 and also in LAS of GPT 4o-mini (M=0.76) and UAS of LLAMA (M=0.75), \textit{t} = 22.83, \textit{p} $<$0.001. The results of paired T-test are summarized in \ref{tab:UAS_LAS_comp}. 
A general trend observed in the data is that UAS values are consistently lower than LAS values. As discussed in \cite{kubler2009dependency}, LAS measures the percentage of words that are both correctly parsed and labeled. In contrast, UAS evaluates the percentage of correct dependencies for each sentence and then averages these percentages across all sentences. This difference can be attributed to the nature of the descriptions. While the original descriptions resemble loosely assembled definitions, the LLM-generated descriptions are well-structured and syntactically complete sentences. Consequently, it is reasonable for LAS values to be higher and UAS values to be lower, given the distinct structural and syntactic differences between the two types of descriptions.

\begin{table}
  \caption{Within model comparison using T-Test over the derived UAS and LAS values.}
  \label{tab:UAS_LAS_comp}
  %\footnotesize
  \begin{tabular}{p{3cm} p{2cm} p{1cm} p{1cm}}
    \toprule
    Test&\textit{df}&\textit{t}&\textit{p}\\
    \midrule
    UAS(GPT-4o/GPT-4o-mini)&3046&-8.42&***\\
    UAS(GPT-4o/LLAMA-8B-Instruct)&3046&5.45&***\\
    UAS(GPT-4o-mini/LLAMA-8B-Instruct)&3046&12.97&***\\
    LAS(GPT-4o/GPT-4o-mini)&3046&5.5&***\\
    LAS(GPT-4o/LLAMA-8B-Instruct)&3046&27.67&***\\
    LAS(GPT-4o-mini/LLAMA-8B-Instruct)&3046&22.83&***\\

    \midrule
    \multicolumn{5}{l}{\footnotesize Note: \( p < 0.001 = *** \)} \\
  \bottomrule
\end{tabular}
\end{table}
\end{comment}

\section{Discussion}

Our findings highlight both the potential and limitations of LLMs in inferring facial expressions from VA values alone. %While LLMs can recognize broad affective patterns, their accuracy in categorization tasks remains limited.
In Experiment 1, LLMs struggled to map VA values to discrete emotions. %, performing comparably to FaceChannel (~30% accuracy).
Biases were evident, with better performance for polarized emotions (e.g., happiness, sadness) but poor recognition of others (e.g., anger, surprise). Multi-class categorization of complex emotions improved performance slightly, yet exact matches were low, suggesting difficulty in capturing nuanced and complex emotions. In contrast, Experiment 2 showed that LLMs perform significantly better when generating open-ended semantic descriptions of facial expressions. This aligns with prior research indicating LLMs excel in free-text generation over rigid classification \cite{Xu2024,Sherburn2024}. BERT and Word2Vec performed better than Transformers, suggesting that pre-trained embeddings capturing contextual and semantic relationships are more effective than purely structural representations for mapping VA values to meaningful affective descriptions, highlighting the importance of leveraging linguistic priors when using LLMs for structured affective inference tasks.


The stronger performance in the semantic description task suggests that LLMs are more effective at inferring general affective meanings from VA values rather than rigidly categorizing them. This aligns with theories of emotion and affect, which posit that affective perception is often more gradient-based than categorical \cite{Barrett2016, Gendron2009, Satpute2016}, also when observing facial expressions \cite{Fujimura2011}. Our findings also highlight the potential for LLMs to complement multimodal emotion recognition systems by providing descriptive information rather than binary classifications. However, LLMs’ reliance on linguistic priors may lead to oversimplifications. Future work should explore integrating additional context (e.g., speech, action units) and comparing LLMs with VLMs to enhance emotion recognition.



%The results provide a clear picture of the understanding of VA values LLM agents possess. For section \ref{exp1.1}, the suboptimal results may be attributed to the partially skewed Valence-Arousal (VA) values generated but it cannot be entirely dismissed that LLMs possess some capability to map VA values to basic emotions, as depicted in Fig. \ref{iimi_example}. Though, the findings from section \ref{exp1.2}, which utilized both the original VA values and those generated by FaceChannel \cite{barros2020facechannel}, clearly indicate that while LLMs may demonstrate a capacity to translate VA values into basic emotions, they exhibit limitations in mapping these values to complex emotions. This limitation could potentially stem from the scarcity of research and training data focused on complex emotions within the datasets used for training these LLMs. 

%In contrast, the outcomes of section \ref{exp2} are highly promising from both qualitative and quantitative perspectives. This difference may be because LLMs perform better when generating descriptive outputs compared to producing outputs limited to categories. All three LLM models employed in this experiment produced facial expression descriptions that closely aligned not only with human-generated descriptions but also with the visual content of the images themselves. Table \ref{tab:4} highlights that the textual outputs from the LLMs capture themes and emotions similar to those described by human participants, often employing comparable vocabulary in their descriptions. 

%Semantic similarity results varied across Word2Vec, transformers, and BERT. Word2Vec and BERT, fine-tuned for context and semantics, showed higher similarity, while transformers, influenced by syntax, had lower similarity. Statistical tests confirmed the reliability of these results, demonstrating that LLMs understand VA values and their link to facial expressions, making them suitable for interactive applications.

\section{Conclusions}

In this study, we explored the ability of LLMs to classify and describe facial expressions based solely on VA values, shedding light on their potential for affective inference without direct visual input. LLMs performed notably better at generating semantic descriptions of expressions than at categorising emotions, indicating their strength in free-text descriptions over rigid classification tasks. These findings suggest that LLMs can process structured affective data, yet their reliance on linguistic priors may limit their ability to fully capture nuanced emotions. Overall, LLMs show promise for affective computing and facial expression research (e.g., see \cite{Zhang2024}), but require further refinement for nuanced emotional understanding. Hybrid approaches combining structured affective data with multimodal inputs could improve robustness. Future work should integrate multimodal inputs and refine LLMs' affective reasoning capabilities to enhance their application in privacy-conscious emotion recognition and social interactions. 

%The experiments show that LLMs have a basic understanding of VA values, with the ability to describe emotions and classify them into basic or complex categories. However, translating VA values directly into emotions may not be the best approach. Instead, prompting LLMs to generate descriptive narratives based on VA values proves more effective for interpreting facial expressions and advancing emotional intelligence.Using VA values as input is more efficient and environmentally friendly than processing images directly. This method enables LLMs to handle facial expressions effectively in multimodal contexts, paving the way for emotionally intelligent interactions. This study establishes a foundation for transparent multimodal pipelines using LLMs. Future research could explore the use of Action Units as input to assess whether they yield better results than VA values.

%The limited availability of datasets with human-annotated descriptions makes replication challenging. Moreover, describing static images differs from dynamic, continuous data in real-time interactions, which should be considered for practical applications. Lastly, comparing LLMs with VLMs for this task is a potential direction for future research.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{ETHICAL IMPACT STATEMENT}

This study did not involve human participants or personal data collection. This study utilized publicly available datasets, ensuring compliance with ethical standards for data use. By leveraging structured data rather than raw visual inputs, this research contributes to the advancement of privacy-conscious approaches in emotion recognition. The methods employed promote ethical AI development by reducing reliance on personally identifiable data and mitigating potential biases associated with direct human observation. A key ethical consideration is the potential for LLM-generated interpretations of affective data to reflect linguistic biases present in their training data. Future research should ensure that models are evaluated across diverse datasets to enhance fairness and generalizability in affective computing applications. Another consideration is the interpretability of LLMs' affective inferences. While this study examines their ability to describe emotions based on structured data, these models may not fully capture the complexity of human affective states. Over-reliance on LLM-generated interpretations in sensitive applications, such as mental health, should be approached with caution to avoid misleading conclusions.
%This research on the ability of Large Language Models (LLMs) to interpret Valence-Arousal (VA) values and generate emotional descriptions offers significant advancements in multimodal emotional intelligence but also raises ethical concerns. The potential for misuse includes privacy violations through unauthorized monitoring, emotion manipulation in areas like advertising or political campaigns, and the risk of bias if datasets are unrepresentative, leading to discriminatory outcomes. Additionally, over-reliance on such systems in sensitive contexts, like mental health, could reduce human agency and empathy. To mitigate these risks, transparent data practices, strict ethical guidelines, and regular bias audits are essential, along with ensuring these systems support human decision-making rather than replacing it. Despite these concerns, the research has substantial benefits, such as improving emotional understanding in human-computer interaction, enabling more efficient and accessible applications, and advancing tools for mental health support. When deployed responsibly, these positive impacts outweigh the potential harms, contributing to meaningful societal progress.

\section*{ACKNOWLEDGMENTS}
V. Mehra is funded by the European Union Erasmus Mundus Joint Master Grant no: 101048710. G. Laban and H. Gunes are supported by the EPSRC project ARoEQ under grant ref. EP/R030782/1. 


{\small
\bibliographystyle{ieee}
\balance{\bibliography{egbib}}
}

\begin{comment}
    

\section{Additional Resuls for time being.}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{violin_plot_2.png}
  \caption{Violin plot for spearman correlation between original image descriptions and GPT 4o-mini generated descriptions.}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{histogram_plot_2.png}
  \caption{Histogram plot for spearman correlation between original image descriptions and GPT 4o-mini generated descriptions.}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{box_plot_2.png}
  \caption{Box plot for spearman correlation between original image descriptions and GPT 4o-mini generated descriptions.}
\end{figure}

\end{comment}

\end{document}
