\section{Related Work}
\subsection{Urban Region Embedding}
Human mobility data has become a valuable resource for gaining insights into various aspects of urban dynamics, and numerous studies have integrated it to improve our understanding of urban environments ____. Mobility data provides valuable insights into urban flow patterns ____, hazard exposure ____, region recommendation ____, and fine-grained socio-economic assessments ____. Recently, several studies have combined mobility data with deep learning techniques to deepen this understanding, focusing on three main approaches: sequence modeling ____, OD matrix reconstruction ____, and contrastive learning ____.

In sequence modeling methods, region embeddings are extracted by treating individual vehicle origin-destination trip chains as sequences and modeling the co-occurrence of origin and destination regions to learn region representations. ____. Alternatively, inflow and outflow data from taxi trip records are used to weight the edges of a geo-spatial graph. A random walk on this weighted graph generates sequences that, combined with the skip-gram objective, produce region embeddings ____. In OD matrix reconstruction methods, the OD pair and trip volume information are used to model inter-region interactions as a conditional trip distribution ____. Lastly, in the contrastive learning approach, inflow and outflow representations are averaged to obtain overall region representations, followed by the application of contrastive loss ____. 

While these advancements have shown promising results, they remain limited in capturing the full temporal dynamics and semantic richness of human mobility patterns. By considering both temporal dynamics and the interactions between inbound and outbound mobility, we could gain deeper insights into regional functions and a more nuanced understanding of how regions operate over time.

\subsection{Contrastive Time Series Representation Learning}
 Supervised learning relies heavily on labeled data, which can be challenging and expensive to obtain in real-world scenarios. To overcome these limitations, contrastive learning has emerged as a promising approach for learning intrinsic patterns present in data without relying on external annotations or labels. It uses positive or negative pairs of data to learn representations. Through data augmentation, the positive pairs comprise two augmented versions of the same input data. In contrast, negative pairs are formed using different input samples. During the training process, the model maps input samples into the latent vector space, aiming to bring positive pairs geometrically closer, while the negative pairs are pushed further apart. 
 
Building on this foundation, contrastive learning techniques for time series data have shown promising results. For example, Franceschi et al. ____ employed time-based negative sampling to encourage the model to learn semantics similar to those of the smapled sub-series. Eldele et al. ____ employed augmentation techniques, including scaling and permutation, to extract representations invariant to transformations. Tonekaboni et al. ____ considered the temporal dependency of time series data by ensuring that neighboring timesteps are distinguishable from those that are not adjacent. Yue et al. ____ used hierarchical contrastive loss to extract a contextually invariant representation. Wickstr$\phi$m et al. ____ proposed a novel augmentation strategy to predict the mixing proportion of time series samples. Meng et al. ____ employed hierarchical clustering to construct contrastive pairs, thereby preventing the inclusion of instances with semantics similar to false-negative pairs.