% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").


@InProceedings{pmlr-v202-refinetti23a,
  title = 	 {Neural networks trained with {SGD} learn distributions of increasing complexity},
  author =       {Refinetti, Maria and Ingrosso, Alessandro and Goldt, Sebastian},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {28843--28863},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/refinetti23a/refinetti23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/refinetti23a.html},
  abstract = 	 {The uncanny ability of over-parameterised neural networks to generalise well has been explained using various "simplicity biases". These theories postulate that neural networks avoid overfitting by first fitting simple, linear classifiers before learning more complex, non-linear functions. Meanwhile, data structure is also recognised as a key ingredient for good generalisation, yet its role in simplicity biases is not yet understood. Here, we show that neural networks trained using stochastic gradient descent initially classify their inputs using lower-order input statistics, like mean and covariance, and exploit higher-order statistics only later during training. We first demonstrate this <b>distributional simplicity bias</b> (DSB) in a solvable model of a single neuron trained on synthetic data. We then demonstrate DSB empirically in a range of deep convolutional networks and visual transformers trained on CIFAR10, and show that it even holds in networks pre-trained on ImageNet. We discuss the relation of DSB to other simplicity biases and consider its implications for the principle of Gaussian universality in learning.}
}

@inproceedings{maml,
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
title = {Model-agnostic meta-learning for fast adaptation of deep networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1126–1135},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{fomaml,
  title={On First-Order Meta-Learning Algorithms},
  author={Alex Nichol and Joshua Achiam and John Schulman},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.02999},
  url={https://api.semanticscholar.org/CorpusID:4587331}
}

@inproceedings{bengio2009curriculum,
author = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason},
title = {Curriculum learning},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553380},
doi = {10.1145/1553374.1553380},
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {41–48},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@misc{smollm2,
      title={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model}, 
      author={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Martín Blázquez and Guilherme Penedo and Lewis Tunstall and Andrés Marafioti and Hynek Kydlíček and Agustín Piqueres Lajarín and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan-Son Nguyen and Clémentine Fourrier and Ben Burtenshaw and Hugo Larcher and Haojun Zhao and Cyril Zakka and Mathieu Morlon and Colin Raffel and Leandro von Werra and Thomas Wolf},
      year={2025},
      eprint={2502.02737},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.02737}, 
}

@inproceedings{pythia,
author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and Van Der Wal, Oskar},
title = {Pythia: a suite for analyzing large language models across training and scaling},
year = {2023},
publisher = {JMLR.org},
abstract = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce Pythia, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend Pythia to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {102},
numpages = {34},
location = {Honolulu, Hawaii, USA},
series = {ICML 2023}
}

@article{chomsky-hierarchy,
  title={On Certain Formal Properties of Grammars},
  author={Noam Chomsky},
  journal={Inf. Control.},
  year={1959},
  volume={2},
  pages={137-167},
  url={https://api.semanticscholar.org/CorpusID:16792674}
}

@article{Hao2022FormalLR,
  title={Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity},
  author={Sophie Hao and Dana Angluin and Roberta Frank},
  journal={Transactions of the Association for Computational Linguistics},
  year={2022},
  volume={10},
  pages={800-810},
  url={https://api.semanticscholar.org/CorpusID:248177889}
}

@inproceedings{
yang2024masked,
title={Masked Hard-Attention Transformers Recognize Exactly the Star-Free Languages},
author={Andy Yang and David Chiang and Dana Angluin},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=FBMsBdH0yz}
}

@article{Gilkerson2017MappingTE,
  title={Mapping the Early Language Environment Using All-Day Recordings and Automated Analysis.},
  author={Jill Gilkerson and Jeffrey A. Richards and Steven F. Warren and Judy K. Montgomery and Charles R. Greenwood and D Kimbrough Oller and John H. L. Hansen and Terrance D. Paul},
  journal={American journal of speech-language pathology},
  year={2017},
  volume={26 2},
  pages={
          248-265
        },
  url={https://api.semanticscholar.org/CorpusID:4447742}
}

@inproceedings{Villalobos2022WillWR,
  title={Will we run out of data? Limits of LLM scaling based on human-generated data},
  author={Pablo Villalobos and Jaime Sevilla and Lennart Heim and Tamay Besiroglu and Marius Hobbhahn and An Chang Ho},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:253397775}
}

@article{Zhong2024OpportunitiesAC,
  title={Opportunities and Challenges of Large Language Models for Low-Resource Languages in Humanities Research},
  author={Tianyang Zhong and Zhenyuan Yang and Zheng Liu and Ruidong Zhang and Yiheng Liu and Haiyang Sun and Yi Pan and Yiwei Li and Yifan Zhou and Hanqi Jiang and Junhao Chen and Tianming Liu},
  journal={ArXiv},
  year={2024},
  volume={abs/2412.04497},
  url={https://api.semanticscholar.org/CorpusID:274581400}
}

@article{big-wilcox,
  title={Bigger is not always better: The importance of human-scale language modeling for psycholinguistics
},
  author={Ethan Gotlieb Wilcox and Michael Y. Hu and Aaron Mueller and Tal Linzen and Alex Warstadt and Leshem Choshen and Chengxu Zhuang and Ryan Cotterell and and Adina Williams},
  journal={PsyArXiv},
  year={2024},
  url={https://osf.io/preprints/psyarxiv/rfwgd_v1}
}

@inproceedings{
lindner2023tracr,
title={Tracr: Compiled Transformers as a Laboratory for Interpretability},
author={David Lindner and Janos Kramar and Sebastian Farquhar and Matthew Rahtz and Thomas McGrath and Vladimir Mikulik},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=tbbId8u7nP}
}

@inproceedings{
zhou2024what,
title={What Algorithms can Transformers Learn? A Study in Length Generalization},
author={Hattie Zhou and Arwen Bradley and Etai Littwin and Noam Razin and Omid Saremi and Joshua M. Susskind and Samy Bengio and Preetum Nakkiran},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=AssIuHnmHX}
}

@inproceedings{Gazdar1982PhraseSG,
  title={Phrase Structure Grammar},
  author={Gerald Gazdar},
  year={1982},
  url={https://api.semanticscholar.org/CorpusID:195895208}
}

@inproceedings{Swersky2013MultiTaskBO,
  title={Multi-Task Bayesian Optimization},
  author={Kevin Swersky and Jasper Snoek and Ryan P. Adams},
  booktitle={Neural Information Processing Systems},
  year={2013},
  url={https://api.semanticscholar.org/CorpusID:1311677}
}

@article{Yang2022TensorPV,
  title={Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
  author={Greg Yang and J. Edward Hu and Igor Babuschkin and Szymon Sidor and Xiaodong Liu and David Farhi and Nick Ryder and Jakub W. Pachocki and Weizhu Chen and Jianfeng Gao},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.03466},
  url={https://api.semanticscholar.org/CorpusID:247292726}
}

@misc{merrill2023talecircuitsgrokkingcompetition,
      title={A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks}, 
      author={William Merrill and Nikolaos Tsilivis and Aman Shukla},
      year={2023},
      eprint={2303.11873},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.11873}, 
}

@inproceedings{
sellam2022the,
title={The Multi{BERT}s: {BERT} Reproductions for Robustness Analysis},
author={Thibault Sellam and Steve Yadlowsky and Ian Tenney and Jason Wei and Naomi Saphra and Alexander D'Amour and Tal Linzen and Jasmijn Bastings and Iulia Raluca Turc and Jacob Eisenstein and Dipanjan Das and Ellie Pavlick},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=K0E_F0gFDgA}
}

@inproceedings{
chen2024sudden,
title={Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in {MLM}s},
author={Angelica Chen and Ravid Shwartz-Ziv and Kyunghyun Cho and Matthew L Leavitt and Naomi Saphra},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=MO5PiKHELW}
}

@inproceedings{Vapnik2000TheNO,
    title={The Nature of Statistical Learning Theory},
    author={Vladimir Naumovich Vapnik},
    booktitle={Statistics for Engineering and Information Science},
    chapter={3},
    year={2000},
    url={https://api.semanticscholar.org/CorpusID:7138354}
}

@article{Hu2024FindingsOT,
  title={Findings of the Second BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora},
  author={Michael Y. Hu and Aaron Mueller and Candace Ross and Adina Williams and Tal Linzen and Chengxu Zhuang and Ryan Cotterell and Leshem Choshen and Alexander Scott Warstadt and Ethan Gotlieb Wilcox},
  journal={ArXiv},
  year={2024},
  volume={abs/2412.05149},
  url={https://api.semanticscholar.org/CorpusID:274581556}
}

@article{Charpentier2024GPTOB,
  title={GPT or BERT: why not both?},
  author={Lucas Georges Gabriel Charpentier and David Samuel},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.24159},
  url={https://api.semanticscholar.org/CorpusID:273707069}
}

@article{Muennighoff2023ScalingDL,
  title={Scaling Data-Constrained Language Models},
  author={Niklas Muennighoff and Alexander M. Rush and Boaz Barak and Teven Le Scao and Aleksandra Piktus and Nouamane Tazi and Sampo Pyysalo and Thomas Wolf and Colin Raffel},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.16264},
  url={https://api.semanticscholar.org/CorpusID:258888192}
}

@article{hyperband,
author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
title = {Hyperband: a novel bandit-based approach to hyperparameter optimization},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {6765–6816},
numpages = {52},
keywords = {deep learning, hyperparameter optimization, infinite-armed bandits, model selection, online optimization}
}

@inproceedings{tomek-mixing,
author = {Korbak, Tomasz and Shi, Kejian and Chen, Angelica and Bhalerao, Rasika and Buckley, Christopher L. and Phang, Jason and Bowman, Samuel R. and Perez, Ethan},
title = {Pretraining language models with human preferences},
year = {2023},
publisher = {JMLR.org},
abstract = {Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task performance of standard LM pretraining, both before and after task-specific finetuning. Pretraining with human feedback results in much better preference satisfaction than standard LM pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. Our results suggest that we should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {722},
numpages = {28},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{
huang2025a,
title={A Formal Framework for Understanding Length Generalization in Transformers},
author={Xinting Huang and Andy Yang and Satwik Bhattamishra and Yash Sarrof and Andreas Krebs and Hattie Zhou and Preetum Nakkiran and Michael Hahn},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=U49N5V51rU}
}

@InProceedings{weiss21rasp,
  title = 	 {Thinking Like Transformers},
  author =       {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11080--11090},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/weiss21a/weiss21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/weiss21a.html},
  abstract = 	 {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder—attention and feed-forward computation—into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.}
}


@article{Hettiarachchi2024OverviewOT,
  title={Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025)},
  author={Hansi Hettiarachchi and Tharindu Ranasinghe and Paul Rayson and Ruslan Mitkov and Mohamed Gaber and Damith Premasiri and Fiona Anting Tan and Lasitha Uyangodage},
  journal={ArXiv},
  year={2024},
  volume={abs/2412.16365},
  url={https://api.semanticscholar.org/CorpusID:274981397}
}

@ARTICLE{chomsky-three,
  author={Chomsky, N.},
  journal={IRE Transactions on Information Theory}, 
  title={Three models for the description of language}, 
  year={1956},
  volume={2},
  number={3},
  pages={113-124},
  keywords={Natural languages;Testing;Laboratories;Markov processes;Impedance matching;Kernel;Research and development},
  doi={10.1109/TIT.1956.1056813}}

@inproceedings{
butoi2025training,
title={Training Neural Networks as Recognizers of Formal Languages},
author={Alexandra Butoi and Ghazal Khalighinejad and Anej Svete and Josef Valvoda and Ryan Cotterell and Brian DuSell},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=aWLQTbfFgV}
}

@article{Shieber1985EvidenceAT,
  title={Evidence against the context-freeness of natural language},
  author={Stuart M. Shieber},
  journal={Linguistics and Philosophy},
  year={1985},
  volume={8},
  pages={333-343},
  url={https://api.semanticscholar.org/CorpusID:222277837}
}

@inproceedings{
merrill2023a,
title={A Logic for Expressing Log-Precision Transformers},
author={William Merrill and Ashish Sabharwal},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=uR8TtWCIsr}
}

@inproceedings{
merrill2024the,
title={The Illusion of State in State-Space Models},
author={William Merrill and Jackson Petty and Ashish Sabharwal},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=QZgo9JZpLq}
}

@article{Suzgun2019LSTMNC,
  title={LSTM Networks Can Perform Dynamic Counting},
  author={Mirac Suzgun and Sebastian Gehrmann and Yonatan Belinkov and Stuart M. Shieber},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.03648},
  url={https://api.semanticscholar.org/CorpusID:182952800}
}

@inproceedings{
yang2024counting,
title={Counting Like Transformers: Compiling Temporal Counting Logic Into Softmax Transformers},
author={Andy Yang and David Chiang},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=FmhPg4UJ9K}
}

@inproceedings{
zhang2024towards,
title={Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
author={Fred Zhang and Neel Nanda},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Hf17y6u9BC}
}

@article{Kumar2022DisentanglingAF,
  title={Disentangling Abstraction from Statistical Pattern Matching in Human and Machine Learning},
  author={Sreejan Kumar and Ishita Dasgupta and Raja Marjieh and Nathaniel D. Daw and Jonathan D. Cohen and Thomas L. Griffiths},
  journal={PLOS Computational Biology},
  year={2022},
  volume={19},
  url={https://api.semanticscholar.org/CorpusID:247939291}
}

@inproceedings{Bresnan1982CrossSerialDI,
  title={Cross-Serial Dependencies in Dutch},
  author={Joan Bresnan and Ronald M. Kaplan and Stanley Peters and Annie Zaenen},
  year={1982},
  url={https://api.semanticscholar.org/CorpusID:86819715}
}

@article{Raffel2019ExploringTL,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Colin Raffel and Noam M. Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal={J. Mach. Learn. Res.},
  year={2019},
  volume={21},
  pages={140:1-140:67},
  url={https://api.semanticscholar.org/CorpusID:204838007}
}

@article{Chiang2021OnTT,
  title={On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets},
  author={Cheng-Han Chiang and Hungyi Lee},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.03537},
  url={https://api.semanticscholar.org/CorpusID:237440457}
}

@misc{2olmo2furious,
      title={2 OLMo 2 Furious}, 
      author={Team OLMo and Pete Walsh and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Shane Arora and Akshita Bhagia and Yuling Gu and Shengyi Huang and Matt Jordan and Nathan Lambert and Dustin Schwenk and Oyvind Tafjord and Taira Anderson and David Atkinson and Faeze Brahman and Christopher Clark and Pradeep Dasigi and Nouha Dziri and Michal Guerquin and Hamish Ivison and Pang Wei Koh and Jiacheng Liu and Saumya Malik and William Merrill and Lester James V. Miranda and Jacob Morrison and Tyler Murray and Crystal Nam and Valentina Pyatkin and Aman Rangapur and Michael Schmitz and Sam Skjonsberg and David Wadden and Christopher Wilhelm and Michael Wilson and Luke Zettlemoyer and Ali Farhadi and Noah A. Smith and Hannaneh Hajishirzi},
      year={2025},
      eprint={2501.00656},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.00656}, 
}

@inproceedings{instructgpt,
author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
title = {Training language models to follow instructions with human feedback},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {2011},
numpages = {15},
location = {New Orleans, LA, USA},
series = {NeurIPS 2022}
}

@inproceedings{
chen2025aioli,
title={Aioli: A Unified Optimization Framework for Language Model Data Mixing},
author={Mayee F Chen and Michael Y. Hu and Nicholas Lourie and Kyunghyun Cho and Christopher Re},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=sZGZJhaNSe}
}

@inproceedings{
jiang2025adaptive,
title={Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws},
author={Yiding Jiang and Allan Zhou and Zhili Feng and Sadhika Malladi and J Zico Kolter},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=aqok1UX7Z1}
}


@misc{mccoy2023modelingrapidlanguagelearning,
      title={Modeling rapid language learning by distilling Bayesian priors into artificial neural networks}, 
      author={R. Thomas McCoy and Thomas L. Griffiths},
      year={2023},
      eprint={2305.14701},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14701}, 
}

@article{Saxe2013ExactST,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Andrew M. Saxe and James L. McClelland and Surya Ganguli},
  journal={CoRR},
  year={2013},
  volume={abs/1312.6120},
  url={https://api.semanticscholar.org/CorpusID:17272965}
}

@inproceedings{
belrose2024neural,
title={Neural Networks Learn Statistics of Increasing Complexity},
author={Nora Belrose and Quintin Pope and Lucia Quirke and Alex Troy Mallen and Xiaoli Fern},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=IGdpKP0N6w}
}

@article{lecun-eigen,
  title = {Eigenvalues of covariance matrices: Application to neural-network learning},
  author = {LeCun, Yann and Kanter, Ido and Solla, Sara A.},
  journal = {Phys. Rev. Lett.},
  volume = {66},
  issue = {18},
  pages = {2396--2399},
  numpages = {0},
  year = {1991},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.66.2396},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.66.2396}
}


@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@ARTICLE{ogden-1968-helpful,
  title = {{A helpful result for proving inherent ambiguity}},
  author = {Ogden, William},
  journal = {Mathematical Systems Theory},
  publisher = {Springer Science and Business Media LLC},
  volume = {2},
  issue = {3},
  pages = {191--194},
  year = {1968},
  month = sep,
  doi = {10.1007/bf01694004},
  issn = {0025-5661,1433-0490},
  url = {https://link.springer.com/article/10.1007/BF01694004},
}

@BOOK{hopcroft-2006-introduction,
  title = {{Introduction to automata theory, languages, and computation}},
  author = {Hopcroft, John E and Motwani, Rajeev and Ullman, Jeffrey D},
  publisher = {Pearson},
  location = {Upper Saddle River, NJ},
  edition = {3},
  year = {2006},
  month = {06},
  pagetotal = {560},
  isbn = {9780321455369},
  language = {en}
}

@ARTICLE{bar-hillel-1961-formal,
  title = {{On formal properties of simple phrase structure grammars}},
  author = {Bar-Hillel, Y and Perles, M and Shamir, E},
  journaltitle = {Language typology and universals},
  publisher = {Walter de Gruyter GmbH},
  volume = {14},
  issue = {1-4},
  pages = {143--172},
  year = {1961},
  month = {05},
  day = {01},
  doi = {10.1524/stuf.1961.14.14.143},
  issn = {2196-7148,1867-8319},
  url = {https://www.degruyter.com/document/doi/10.1524/stuf.1961.14.14.143/html},
  urldate = {2025-02-11},
  file = {All Papers/B/Bar-Hillel et al. 1961 - On formal properties of simple phrase structure grammars.pdf},
  language = {en}
}

@article{kuroda1964classes,
    title = {Classes of languages and linear-bounded automata},
    journal = {Information and Control},
    volume = {7},
    number = {2},
    pages = {207-223},
    year = {1964},
    issn = {0019-9958},
    doi = {https://doi.org/10.1016/S0019-9958(64)90120-2},
    url = {https://www.sciencedirect.com/science/article/pii/S0019995864901202},
    author = {S.-Y. Kuroda}
}

@article{barrington1990uniformity,
    title = {On uniformity within NC1},
    journal = {Journal of Computer and System Sciences},
    volume = {41},
    number = {3},
    pages = {274-306},
    year = {1990},
    issn = {0022-0000},
    doi = {https://doi.org/10.1016/0022-0000(90)90022-D},
    url = {https://www.sciencedirect.com/science/article/pii/002200009090022D},
    author = {David A. {Mix Barrington} and Neil Immerman and Howard Straubing},
}

@incollection{chomsky-1959-algebraic,
title = {The Algebraic Theory of Context-Free Languages},
editor = {P. Braffort and D. Hirschberg},
series = {Studies in Logic and the Foundations of Mathematics},
publisher = {Elsevier},
volume = {26},
pages = {118-161},
year = {1959},
booktitle = {Computer Programming and Formal Systems},
issn = {0049-237X},
doi = {https://doi.org/10.1016/S0049-237X(09)70104-1},
url = {https://www.sciencedirect.com/science/article/pii/S0049237X09701041},
author = {N. Chomsky and M.P. Schützenberger},
abstract = {Publisher Summary
This chapter describes several classes of sentence-generating devices that are closely related, in various ways, to the grammars of both natural languages and artificial languages of various kinds. Language means simply a set of strings in some finite set V of symbols called the vocabulary of the language. Grammar essentially means a set of rules that give a recursive enumeration of the strings belonging to the language. It is noted that for a class of grammars to have linguistic interest, there must be a procedure that assigns to any pair (σ, G), where σ is a string and G a grammar of this class. In particular, the structural description indicates that the string σ is a well-formed sentence of the language L (G) generated by G. However, the chapter concerns with only one aspect of the structural description of a sentence—namely, its subdivision into phrases belonging to various categories.}
}