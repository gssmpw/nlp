% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
\usepackage{booktabs}

% Standard package includes
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{paralist} % must come before enumitem
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{placeins}
\usepackage{makecell}
\usepackage{pifont}  

\usepackage{colortbl}

\usepackage{listings}
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    commentstyle=\color{gray},
    keywordstyle=\color{blue},
    stringstyle=\color{green!50!black},
    numbers=left,
    numberstyle=\tiny\color{gray},
    numbersep=5pt,
    showstringspaces=false,
    tabsize=4
}
\usepackage[subtle]{savetrees}


% New package includes

\usepackage{amsthm}
\usepackage{cleveref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{mathtools}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}

\newcommand{\sinceC}[1]{\textbf{\#}\!\left[#1\right]}
\usepackage{siunitx}
\sisetup{detect-weight=true,detect-inline-weight=math}



\usepackage{tikz}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{newtx}
\let\Bbbk\relax

\usepackage{float}


%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{amssymb}

\widowpenalty10000
\clubpenalty10000

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\newcommand{\placeholder}[2]{\framebox[#1]{\rule{0pt}{#2}}}

\input{macros}



\title{Between Circuits and Chomsky: \\ Pre-pretraining on Formal Languages Imparts Linguistic Biases}


\author{
    Michael Y. Hu$^{1}$ \quad Jackson Petty$^{2}$ \quad Chuan Shi$^{1}$ \quad William Merrill$^{1}$ \quad Tal Linzen$^{1,2}$ \\ \\
    $^{1}$Center for Data Science \quad $^{2}$Department of Linguistics \\
    New York University \\
    \texttt{\{michael.hu, petty, cs5526, willm, linzen\}@nyu.edu}
}

\begin{document}
\maketitle
\begin{abstract}

Pretraining language models on formal languages can improve their acquisition of natural language, but it is unclear which features of the formal language impart an inductive bias that leads to effective transfer.
%
Drawing on insights from linguistics and complexity theory, we hypothesize that effective transfer occurs when the formal language both captures dependency structures in natural language and remains within the computational limitations of the model architecture. 
%
Focusing on transformers, we find that formal languages with both these properties enable language models to achieve lower loss on natural language and better linguistic generalization compared to other languages. 
%
In fact, \emph{pre-pretraining}, or training on formal-then-natural language, reduces loss more efficiently than the same amount of natural language. 
%
For a 1B-parameter language model trained on roughly 1.6B tokens of natural language, pre-pretraining achieves the same loss and better linguistic generalization with a 33\% smaller token budget. 
%
We also give mechanistic evidence of cross-task transfer from formal to natural language: attention heads acquired during formal language pretraining remain crucial for the model's performance on syntactic evaluations.


\end{abstract}

\section{Introduction}

Language models have achieved impressive performance on many tasks, but they remain data-hungry, requiring five to six orders of magnitude more data than humans to achieve human-level performance \cite{warstadt-etal-2023-findings,Gilkerson2017MappingTE}. This high data requirement presents challenges for training models in low-resource settings \cite{Zhong2024OpportunitiesAC,Hettiarachchi2024OverviewOT}, understanding how models can generalize given more human-like data constraints \cite{big-wilcox}, and improving models as original data becomes scarce \cite{Villalobos2022WillWR}. Thus, an important frontier for language models is making training more data-efficient. 
% To this end, we explore whether we can teach models useful inductive biases in a data-driven way by ``pre-pretraining'' on formal languages. 
% Rather than engineering architectural biases, we investigate whether exposure to formal languages with particular structural properties can predispose models to learn more efficiently during subsequent training.

\begin{figure}[t]
    \begin{tabularx}{0.95\columnwidth}{X||c|c}
    & Context-free & Context-sensitive \\ 
    \hline\hline
    \multirow{2}{*}{$\CRASP$} & \multirow{2}{*}{1-Dyck} & $k$-Shuffle \cellcolor[rgb]{0.937,0.922,0.99} \\ 
    & & Dyck \cellcolor[rgb]{0.937,0.922,0.99} \\
    \hline
    \multirow{2}{*}{$\FOM$}  & \multirow{2}{*}{$k$-Dyck} & \multirow{2}{*}{$ww$} \\ 
    & & \\ 
    \end{tabularx}
    \raggedleft
    \includegraphics[width=\columnwidth]{figures/loss_curves.pdf}
    \caption{The intersection of Chomsky and circuit hierarchies (top), where $\CRASP \subset \FOM$ and context-free $\subset$ context-sensitive. Within this $2\times2$, we find that pre-pretraining on $k$-Shuffle Dyck, a context-sensitive language definable in $\CRASP$, lets 1B-parameter models match the final baseline performance of no pre-pretraining with 33\% fewer training tokens (bottom). See \S \ref{sec:methods-grammars}.}
    \label{fig:conceptual}
\end{figure}


A recently-explored approach for increasing data efficiency teaches models useful inductive biases by ``pre-pretraining'' them on formal languages before training on natural language \cite{Chiang2021OnTT,mccoy2023modelingrapidlanguagelearning}.
In particular, \citet{papadimitriou-jurafsky-2023-injecting} show that within the Chomsky hierarchy, context-sensitive languages transfer best to natural language. 
However, results from circuit complexity show that transformers cannot learn next-token prediction for all context-sensitive languages, both in theory and practice \cite{strobl-etal-2024-formal,merrill2023a}. 
Within all levels of the Chomsky hierarchy, some languages are harder for transformers to learn than others, if not impossible \cite{merrill2023talecircuitsgrokkingcompetition,merrill2024the}. 
These results call into question how positive transfer occurs, if a generalizing solution cannot always be learned.
Which characteristics of formal languages make them effective for pre-pretraining?
% Understanding positive transfer from formal to natural language requires reconciling existing empirical results with new theory on the expressive power of transformers.


In this work, we propose that optimal transfer from formal to natural language occurs at the intersection of two theoretical hierarchies: the Chomsky hierarchy of formal languages and the circuit complexity hierarchy that bounds transformer computational power (\S \ref{sec:methods}). Specifically, we hypothesize that effective pre-pretraining languages should be:
\begin{compactenum}
    \item expressive enough to capture hierarchical natural language dependencies, and
    \item learnable by transformers in a length-generalizing way. \vspace{0.1em}
\end{compactenum} 
Our empirical results support this hypothesis (\S \ref{sec:expressivity}): pre-pretraining on languages with both the above properties outperforms pre-pretraining on either natural language or any of the other formal languages that we tested. Next, we show that when positive transfer occurs, the model reuses attention heads it learned during pre-pretraining, suggesting that mechanisms from pre-pretraining transfer to natural language (\S \ref{sec:pruning}). We conclude by demonstrating our pre-pretraining method generalizes to 1B-parameter language model pretraining, increasing token efficiency by 33\% (\S \ref{sec:analysis}). In summary, our contributions are three-fold:
\begin{compactenum}
    \item We propose a taxonomy of formal languages for pre-pretraining transformers (\S \ref{sec:methods-grammars}). Our framework reconciles constraints on formal languages from linguistic theory, as captured by the Chomsky hierarchy, and constraints from circuit complexity theory, as captured by the nascent circuit hierarchy \cite{yang2024counting}. 
    \item The best pre-pretraining languages we observe contain hierarchical structure and are learnable by transformers in a length-generalizing way (\S \ref{sec:expressivity}). We demonstrate that attention heads learned during pre-pretraining continue to be used after transfer to natural language (\S \ref{sec:pruning}).
    \item Finally, ablations reveal that hierarchical dependency structures are crucial for positive transfer. We also show that the benefit of pre-pretraining persists for 1B-parameter models (\S \ref{sec:analysis}).
\end{compactenum}

% our results

% bulleted list of findings

\section{Background}

% \begin{table}[h!]
% \centering
% \begin{tabular}{ll}
% Language Type & Recognition Automaton \\
% \midrule
% Regular  & Finite-state automata \\
% Context-free  & Pushdown automata \\
% Context-sensitive & Linear-bounded automata \\
% Unrestricted & Turing machines \\
% \end{tabular}
% \caption{The Chomsky hierarchy of languages and their recognition automata, from least to most complex. A Transformer cannot implement all variants of each automaton, within any level of the hierarchy.}
% \label{tab:language-hierarchy}
% \end{table}

\subsection{The Chomsky Hierarchy}

The Chomsky hierarchy \cite{chomsky-hierarchy} is a nested classification of increasingly complex formal languages. 
% , which we depict in Table \ref{tab:language-hierarchy}. 
This classification clarifies the kinds of computations needed to process formal structures resembling those found in human language. For example, regular languages, the least complex, can be recognized by finite-state automata; most phenomena in natural language phonology and morphology can be captured by regular languages.  Regular languages are insufficient for syntax, however,  as representing the hierarchical structure of natural language with a finite-state automaton would require infinitely many states \cite{chomsky-three}. Subsequent works showed that compactly modeling natural-language syntax requires not only context-free but also context-sensitive grammars \cite{Shieber1985EvidenceAT}. That said, only a select few phenomena in natural language require context sensitivity; those include cross-serial dependencies in Swiss German \cite{Bresnan1982CrossSerialDI} or anaphora; the rest can be modeled using context-free grammars \cite{Gazdar1982PhraseSG}.

\paragraph{Dyck Languages.} A classic context-free language is $k$-Dyck: the language of well-balanced parentheses with $k$ bracket types. For example, \texttt{([])[]} is a valid $2$-Dyck string, where rounded and square parentheses are the two bracket types. $k$-Dyck is often taken as a canonical example of context-free hierarchical structure because any context-free language can be reduced to Dyck via one transformation (inverse homomorphism) and intersection with a regular language \citep{chomsky-1959-algebraic}.

\paragraph{Shuffle Dyck.} Removing the constraint that Dyck braces must be well-nested (but enforcing that every opening brace must be closed and vice versa) yields $k$-Shuffle Dyck,\footnote{The name ``Shuffle'' comes from the fact that $k$-Shuffle Dyck can be defined by interleaving $k$ Dyck strings with different braces \citep{Suzgun2019LSTMNC}, as if by riffle shuffling.} a minimal relaxation of $k$-Dyck that is strictly context-sensitive rather than context-free \cite{Suzgun2019LSTMNC,strobl-etal-2024-formal}.
Crossing braces in $k$-Shuffle Dyck can be thought of as a formal model of the cross-serial dependencies underlying aspects of language argued to be context-sensitive \citep{papadimitriou-jurafsky-2023-injecting}.


\subsection{The Circuit Hierarchy}
\label{sec:circuit-background}

Our focus in this work is on transformer language models. At each level of the Chomsky hierarchy, there are languages that a transformer cannot recognize \citep{merrill2023a,liu-etal-2024-shortcuts,strobl-etal-2024-formal}.
Thus, the Chomsky hierarchy alone does not precisely capture how difficult a language is for transformers to learn: for instance,  transformers can learn some context-free languages \cite{butoi2025training} and yet fail to learn other regular languages \cite{merrill2024the}.
To better understand the expressive power of transformers, recent work has analyzed formal languages within a circuit complexity hierarchy, native to what transformers can and cannot express \cite{Hao2022FormalLR,yang2024masked}. Here, we will focus on two logics that emerge from this circuit complexity viewpoint: $\FOM$ \cite{merrill2023a} and $\CRASP$ \cite{yang2024counting}.

\paragraph{$\FOM$.} First-order logic with majority, or $\FOM$, is a provable \emph{upper bound} on the languages that transformers can express: that is, any transformer can be converted into an $\FOM$ program that defines (or recognizes) the same language \citep{merrill2023a}.
$\FOM$ programs operate by computing counts over the number of indices in an input string that satisfy certain predicates.
For example, $Q_a(i)$ is a basic predicate that checks whether input token $i$ is an $a$. The following $\FOM$ program uses $Q_a(i)$ to define the language of strings with exactly 3 $a$'s:
\begin{equation} \label{eq:count3}
    \#i \leq n [Q_a(i)] \; = \; 3
\end{equation}
Beyond this example, $\FOM$ can implement a rich variety of programs by nesting quantifiers and building complex predicates out of logical ($\wedge, \vee, \neg$) and arithmetic operators ($+, =, <$).
In particular, $\FOM$ can define the $k$-Dyck language for any $k \geq 1$ (Proposition \ref{prop:kdyck}).
For example, the following program defines $1$-Dyck:
\begin{align}
    \mathsf{depth}(i) \;\; &\equiv \;\; \# j \leq i [Q_((i)] - \# j \leq i [Q_)(i)] \nonumber \\
    [\mathsf{depth}(n) &= 0] \; \wedge \; \# i \leq n [ \mathsf{depth}(i) < 0] = 0 \label{eq:1dyck}
\end{align}
To define $2$-Dyck, this can be extended by modifying $\mathsf{depth}$ to track two bracket types and computing the following depth index:
\begin{align}
    \mathsf{dindex}(i) \;\; \equiv \;\; \#j \leq i [\mathsf{depth}(i) = \mathsf{depth}(j) ] \label{eq:dindex}
\end{align}
To finish the definition, we add a condition to enforce that any open and close brace paired by $\mathsf{depth}$ and $\mathsf{dindex}$ also match in their type (i.e., parenthesis or square brace).
See Proposition \ref{prop:kdyck-fom} for further details.

\paragraph{$\CRASP$.} While any transformer can be compiled into $\FOM$, it is not necessarily the case that \textit{any} $\FOM$ program can be implemented by a transformer.
$\CRASP$ is a restriction of $\FOM$ designed to be a \emph{lower bound} on what transformers can express: that is, if a language is definable in $\CRASP$, then there exists a transformer that recognizes it \citep{yang2024counting}.\footnote{$\CRASP$ is a fully well-defined variant of the Restricted Access Sequence Processing programming language \citep[RASP;][]{weiss21rasp,lindner2023tracr}.}
$\CRASP$ works similarly to $\FOM$, but with a couple of restrictions.
Most crucially, in $\CRASP$ each predicate can only refer to one index variable $i$, whereas in $\FOM$, predicates can refer to two (or more) indices $i, j$ introduced by different quantifiers (for more detail, see \citealp{yang2024counting}).
This means $\CRASP$ can define \eqref{eq:count3} or \eqref{eq:1dyck} above, but not $k$-Dyck for $k \geq 2$, as $\CRASP$ cannot express the $\mathsf{dindex}$ in \eqref{eq:dindex}.


Recent work has also suggested a connection between $\CRASP$ and length generalization of transformers \citep{zhou2024what,huang2025a}: the definability of a language $L$ in $\CRASP$ predicts whether transformers can reliably length-generalize when trained on strings from $L$.
One interpretation of this finding is that mechanisms expressible in $\CRASP$ may be more robustly learnable for transformers.
We thus hypothesize that it is better to pre-pretrain on formal languages that can be defined in $\CRASP$ so that the learned representations can transfer reliably.
% We hypothesize that learning hierarchical dependencies in a length-generalizing way is a good initialization for natural language.

\section{Methods}
\label{sec:methods}

Context-free and context-sensitive languages serve as formal models of the hierarchical structure of natural language. Similarly, circuit complexity classes like $\FOM$ and restricted programming languages like $\CRASP$ provide theoretical upper and lower bounds on transformer capabilities. We explore the intersection of these two hierarchies to identify optimal pre-pretraining languages. In this section, we discuss how we define pre-pretraining (\S \ref{sec:methods-ppt}) and how we create formal language pre-pretraining data (\S \ref{sec:methods-grammars}).

\subsection{Defining Pre-pretraining}
\label{sec:methods-ppt}

Formally, we train a language model using an optimizer $\mathcal{A}(\mathcal{D}, t, \theta_{\text{init}})$ which returns parameters $\theta_t$ after $t$ timesteps. We apply $\mathcal{A}$ sequentially:
\vspace{0.25em}
\begin{compactenum}
    \item Pre-pretrain for $t_0$ steps on dataset $\Dppt$ to obtain model parameters $\theta_{t_0}$.
    \item Pretrain for $t_1$ steps on dataset $\Dpt$ to obtain $\theta_{t_1}$.
\end{compactenum}
\vspace{0.25em}

Our objective is to minimize the expected loss on the pretraining dataset:
$\arg \min_{\theta_{t_1}} \;\; \mathbb{E} [\ell(\Dpt, \theta_{t_1})]$.

We hold $\mathcal{A}$'s hyperparameters, $t_1$, and $\Dpt$ fixed, and we transfer model parameters directly from pre-pretraining to pretraining. So to minimize $\ell(\Dpt, \theta_{t_1})$, we can only change the pre-pretraining dataset $\Dppt$ and duration $t_0$. 
We compare pre-pretraining on our proposed $\Dppt$ datasets (\S \ref{sec:methods-grammars}) against several baselines:

\vspace{0.25em}
\begin{compactitem}
    \item No pre-pretraining $(t_0 = 0)$
    \item Random binary strings
    \item Random strings of $k$ integers
    \item Held-out natural language data from the same distribution as $\Dpt$
\end{compactitem}
\vspace{0.25em}
Aside from no pre-pretraining, we pre-pretrained the baselines for $t_0=500$ steps, for a clear comparison against the $k$-Shuffle Dyck results (see \S \ref{sec:expressivity}).
The held-out natural language baseline is different from training on $\Dpt$ for longer, because we use learning rate warmup in both pre-pretraining and pretraining. 

Lower validation loss compared to the no pre-pretraining baseline would indicate that pre-pretraining on formal languages is beneficial. The random baselines help establish whether this effect is specific to the particular formal languages we study. Finally, outperforming pre-pretraining on $\Dpt$ would suggest that formal languages provide better inductive biases than the pretraining data itself.


\paragraph{Evaluation.} In addition to validation loss, we evaluate models on BLiMP and verbatim retrieval. BLiMP compares the likelihood assigned by the model to minimal pairs of sentences that differ only in their grammaticality (e.g., \textit{Only Bill would ever complain} is grammatical, but \textit{Even Bill would ever complain} is not). Accuracy is measured as the proportion of examples where the grammatical sentence is assigned higher likelihood than the ungrammatical one  \cite{warstadt-etal-2020-blimp-benchmark}. 
Verbatim retrieval tests language modeling on text passages with repeated lists \cite{armeni-etal-2022-characterizing,armeni-etal-2024-transformer}; the model is expected to assign a very high likelihood to the words in the second repetition of the list, such that lower loss indicates better performance. Both tasks assess models' ability to learn and apply consistent patterns---a capability that structured formal languages might strengthen. 
For examples of these tasks, see Tables \ref{tab:blimp} and \ref{tab:verbatim} in the Appendix.


\paragraph{Efficiency.} In the regime with plentiful pretraining data, an ideal pre-pretraining language should minimize the number of pre-pretraining steps $t_0$ required: if a formal language requires very large $t_0$, then only pretraining on natural language would be more practical. To help quantify efficiency, we calculate the marginal rate of substitution (MRS) between formal and natural language at 10,000 steps of natural language pretraining. In other words, if we train on 500 steps of the formal language, how many more steps does it take for the natural language-only baseline to catch up?  

Formally, let $x$ be the number of pre-pretraining steps and $y$ be the number of pretraining steps, and suppose the following two pairs $(x, y)$ of training steps achieve the same final loss: $(0,~10,000)$ and $(500,~6,000)$. Then the marginal rate of substitution is 
$$\frac{|y_1-y_2|}{|x_1-x_2|} = \frac{|10,000 - 6,000|}{|0 - 500|} = 8.$$
Similarly, the gain in token efficiency would be $1 - \frac{6,000 + 500}{1,0000} = 35\%$. For a visualization, see Figure \ref{fig:mrs}.

In our setting, a good pre-pretraining language would (1) minimize the amount of pre-pretraining steps $t_0$ (\textbf{efficiency}), and (2) increase the evaluation \textbf{performance} of the language model. We discuss efficiency for the low-resource setting in \S \ref{sec:limitations}.


\subsection{Between Circuits and Chomsky}
\label{sec:methods-grammars}

We hypothesize that a good pre-pretraining language should both encapsulate the complexity of natural language and be robustly learnable by transformers in a way that length generalizes.
Because natural language is hierarchically structured and $\CRASP$ has been suggested as a formal model of what transformers can learn robustly, this motivates the following hypothesis: 

\begin{tcolorbox}[colback=gray!10,colframe=gray!70]
\textbf{Expressivity hypothesis:} A formal language that confers a helpful inductive bias should be hierarchically structured (either context-free or context-sensitive) and definable in $\CRASP$.
\end{tcolorbox}

To test this hypothesis, we pre-pretrain language models on the following formal languages:
\vspace{0.1em}
\begin{compactenum}
    \item 1-Dyck: the nested parentheses language. Context-free, in $\CRASP$.
    \item $k$-Dyck: contains $k$ different types of parentheses. Context-free, in $\FOM \setminus \CRASP$.
    \item $k$-Shuffle Dyck: $k$-Dyck with cross-serial dependencies. Context-sensitive, in $\CRASP$.
    \item $ww$: The copy language. Context-sensitive, in $\FOM \setminus \CRASP$.
\end{compactenum}
\vspace{0.1em}


The three variants of Dyck languages model hierarchical structure, while $ww$, like the act of copying, has a fixed dependency structure. For examples of these languages, see Table \ref{tab:stats}. Full proofs of where these languages lie on the Chomsky and circuit hierarchies can be found in Appendix \ref{app:proofs}.\footnote{The languages $\textsf{NEST}$ and $\textsf{CROSS}$ from \citet{papadimitriou-jurafsky-2023-injecting} are instances of $k$-Dyck and $k$-Shuffle Dyck, respectively. Their results align with our hypothesis.}

\begin{table}[htb]
\centering
\begin{tabularx}{0.75\columnwidth}{Xc}
\toprule
\textbf{Language}  & \textbf{Example} \\
\midrule
1-Dyck & \texttt{(\,(\,(\,)\,)\,)} \\
$k$-Dyck & \texttt{(\,\textcolor{blue}{[}\,\textcolor{red}{\{}\,\textcolor{red}{\}}\,\textcolor{blue}{]}\,)} \\
$k$-Shuffle Dyck  & \texttt{(\,\textcolor{blue}{[}\,\textcolor{red}{\{}\,\textcolor{blue}{]}\,)\,\textcolor{red}{\}}} \\ 
$ww$ & \texttt{1\,\textcolor{blue}{2}\,\textcolor{red}{3}\,1\,\textcolor{blue}{2}\,\textcolor{red}{3}} \\
\bottomrule
\end{tabularx}
\caption{Examples of our pre-pretraining languages. } 
\label{tab:stats} 
\end{table}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/language_model_metrics.pdf}
    \caption{Evaluating models at the optimal amount of pre-pretraining $t_0^*$ for each formal language. $k$-Shuffle Dyck performs the best overall on our evaluation metrics.}
    \label{fig:ppt-results}
\end{figure*}

We deliberately chose languages that are similar to each other. $k$-Dyck and $k$-Shuffle Dyck can be seen as different extensions of 1-Dyck: $k$-Dyck swaps out paired parentheses in valid 1-Dyck strings with new parentheses pairs, while $k$-Shuffle Dyck effectively interleaves several 1-Dyck sequences \cite{Suzgun2019LSTMNC}. Finally, $ww$ contrasts with $k$-Shuffle Dyck as a maximally context-sensitive language, since \textit{all} the dependencies in $ww$ are cross-serial. 



To reduce confounds, we construct 1-Dyck, $k$-Dyck, and $k$-Shuffle Dyck corpora with matching depth distributions. We do so during corpus generation by randomly opening or closing parentheses with probability $p = 0.5$, which yields a harmonic distribution over depths. % \footnote{The average depth is still finite because we truncate according to the sequence length.} 
Similarly, the $k$-Dyck, $k$-Shuffle Dyck, and $ww$ corpora each have 128 unique tokens, or 64 unique parentheses pairs ($k=64$) for Dyck. All language models are pre-pretrained on the same number of tokens with sequence packing. 

\section{Testing the Expressivity Hypothesis}
\label{sec:expressivity}


For natural language ($\Dpt$), we trained Pythia 160M models \cite{pythia} for 10,000 steps, or roughly 665 million tokens. We use C4 as the natural language dataset  \cite{Raffel2019ExploringTL}. All training hyperparameter details can be found in Appendix \ref{app:hyperparams}. For BLiMP accuracies by category, see Figure \ref{fig:blimp-accuracies}.

\begin{figure}[htbp] 
    \centering 
    \includegraphics[width=\columnwidth]{figures/scaling_law.pdf} 
    \caption{Validation loss on C4 as a function of pre-pretraining tokens. For the formal languages that improve validation loss over no pre-pretraining, performance is \textit{u-shaped} with respect to pre-pretraining tokens, indicating that pre-pretraining is harmful after a certain point.} 
    \label{fig:scaling} 
\end{figure}

\paragraph{Efficiency.} We find that the optimal amount of pre-pretraining $t_0^*$ differs between formal languages. To estimate $t_0^*$, we sweep four pre-pretraining durations $t_0$. Figure \ref{fig:scaling} shows validation loss on natural language after pre-pretraining for 30 to 260 million tokens of formal language (500 to 4000 gradient updates). 

The most efficient formal language is $k$-Shuffle Dyck, with $t_0^* = 500$. Both $k$-Shuffle Dyck and $k$-Dyck outperform natural language pre-pretraining, but $k$-Dyck's optimum is achieved around $t_0^* = 1000$. $ww$ pre-pretraining is unhelpful at all durations. The other languages, which positively transfer to natural language, exhibit a roughly u-shaped scaling pattern: after reaching the optimal training duration, training more on formal language is harmful for natural language learning. Shuffle Dyck has the highest MRS, indicating that it replaces tokens on natural language most efficiently. Furthermore, the MRS for 1-Dyck, $k$-Dyck, and $k$-Shuffle Dyck are all greater than 1, indicating that exchanging natural language for these formal languages is \textbf{compute-optimal} in our setting. See Table \ref{tab:ppt-results} in the Appendix for MRS values.


\paragraph{Performance.} $k$-Shuffle Dyck is the best-performing formal language on the validation set of C4, followed by $k$-Dyck (Figure \ref{fig:ppt-results}). Interestingly, pre-pretraining on all four formal languages improves accuracy in grammaticality, but pre-pretraining on natural language (C4) does not. This indicates that formal language pre-pretraining also changes models' generalization properties, in addition to driving the language modeling loss lower. We hypothesize this is because pre-pretraining induces representations useful for modeling hierarchical structure, and we revisit this point in \S \ref{sec:pruning}.

Pre-pretraining on either random binary strings or $k$-integer strings has a negative effect: it results in higher validation loss than no pre-pretraining. This rules out the hypothesis that any pre-pretraining is helpful, regardless of the data being pre-pretrained on. 

% \begin{table*}[t]
% \centering
% \begin{tabular}{|l|l|c|c|c|c|c|}
% \hline
% & \textbf{Language} & \textbf{Validation CEL} & \textbf{\% Documents $\downarrow$} & \textbf{$t$ Saved} & \textbf{MRS} & \textbf{BLiMP} \\ \hline \hline
% \multirow{5}{*}{Formal} & 1-Dyck & 3.760 \scriptsize{$\pm$0.016} & 0.978 \scriptsize{$\pm$0.021}  & 3011 & 3.01 & 0.717 \scriptsize{$\pm$0.004} \\ \cline{2-7}
% & $k$-Dyck @500 & 3.751 \scriptsize{$\pm$0.016} & \textbf{0.998} \scriptsize{$\pm$0.001} & 3325 & 6.65 & 0.718 \scriptsize{$\pm$0.003} \\ \cline{2-7}
% & $k$-Dyck @1000 & 3.743 \scriptsize{$\pm$0.016} & \textbf{0.998} \scriptsize{$\pm$0.001} & 3572 & 3.57 & \textbf{0.719} \scriptsize{$\pm$0.003} \\ \cline{2-7}
% & $k$-Shuffle Dyck &\textbf{3.741} \scriptsize{$\pm$0.014} & \textbf{0.998 }\scriptsize{$\pm$0.001} & \textbf{3575} & \textbf{7.15} & 0.718 \scriptsize{$\pm$0.007} \\  \cline{2-7}
% & $ww$ & 3.792 \scriptsize{$\pm$0.018} & 0.557 \scriptsize{$\pm$0.247} & -495 & -0.25 & 0.714 \scriptsize{$\pm$0.003} \\ \hline \hline

% \multirow{4}{*}{Controls} & No PPT & 3.780 \scriptsize{$\pm$0.018} & - & - & - & 0.710 \scriptsize{$\pm$0.011} \\ \cline{2-7}
% & English & 3.754 \scriptsize{$\pm$0.017} & 0.992 \scriptsize{$\pm$0.007}  & 3326 & 6.652 & 0.710 \scriptsize{$\pm$0.003} \\ \cline{2-7}
% & Random binary & 3.810 \scriptsize{$\pm$0.015} & 0 \scriptsize{$\pm$0} & -3300 & -6.6 & 0.712 \scriptsize{$\pm$0.004}\\ \cline{2-7}
% & Random ints & 3.798 \scriptsize{$\pm$0.015} & 0.042 \scriptsize{$\pm$0.041} & -2985 & -5.97 & 0.712 \scriptsize{$\pm$0.006} \\ \hline
% \end{tabular}
% \caption{Profitability of pre-pretraining.}
% \label{tab:ppt-results}
% \end{table*}

\section{Pruning}
\label{sec:pruning}

What is the mechanism by which pre-pretraining facilitates the learning of natural language? We hypothesize that the model implements next-token prediction on $\Dppt$ using a sparse subnetwork, or some subset of the total parameters $\mathcal{M}(\theta_{t_0}) \subset \theta_{t_0}$ ($\mathcal{M}$ for short). Once we transfer $\theta_{t_0}$ to learn $\Dpt$, this subnetwork $\mathcal{M}$ continues to drive the performance of language modeling on $\Dpt$.


\begin{tcolorbox}[colback=gray!10,colframe=gray!70]
\textbf{Subnetworks hypothesis:} Subnetworks established during formal language pre-pretraining are later used to represent the hierarchical structure of natural language.
\end{tcolorbox}

We test this hypothesis by ablating attention heads of the pre-pretraining subnetwork and comparing the drop in performance against random attention head ablations. Concretely, we pre-pretrain on $\Dppt$ and prune the model to find the sparse subnetwork $\mathcal{M}(\theta_{t_0})$. We use the heuristic core pruning algorithm from \citet{bhaskar-etal-2024-heuristic}, which  iteratively removes attention heads from the transformer using structured pruning \citep{xia-etal-2022-structured} while minimizing the tradeoff between sparsity and language modeling loss on $\Dppt$. After transfer and training on $\Dpt$, we evaluate the masked model $\mathcal{M}(\theta_{t_1})$ against networks with the same number of attention heads, randomly ablated $\left( \mathcal{M}_{\text{null}}(\theta_{t_1}) \right)$.

Positive transfer from $\Dppt$ to $\Dpt$ could occur for reasons unrelated to subnetworks. In this case, the masked model $\mathcal{M}$ should perform no better than random masks $\mathcal{M}_{\text{null}}$ when applied to $\theta_{t_1}$. However, if pre-pretraining does induce useful inductive biases, we would expect $\mathcal{M}$ to be an important subnetwork even after training on $\Dpt$. So in the alternative hypothesis, $\mathcal{M}$ should significantly outperform $\mathcal{M}_{\text{null}}$ in validation loss and zero-shot tasks.

\begin{figure}[htbp] 
    \centering 
    \includegraphics[width=\columnwidth]{figures/pruning.pdf} 
    \caption{Language modeling and grammaticality performance for the learned subnetwork $\mathcal{M}$, its complement $\mathcal{M}^c$, and randomly sampled masks $\mathcal{M}_{\text{null}}$. $\mathcal{M}$ outperforms $\mathcal{M}^c$ and $\mathcal{M}_{\text{null}}$, indicating that the subnetwork learned during pre-pretraining continues to play a critical role after training on natural language. Dashed lines indicate performance of the base model without pruning.} 
    \label{fig:pruning} 
\end{figure}

\paragraph{Results.} After pre-pretraining on $k$-Shuffle Dyck, we ablate $50\%$ of the attention heads using structured pruning to create the sparse subnetwork $\mathcal{M}$. Following previous work \cite{bhaskar-etal-2024-heuristic,zhang2024towards}, we replace an ablated head with its mean activation. In addition to random masked subnetworks $\mathcal{M}_{\text{null}}$, we also compare $\mathcal{M}$ against its complement subnetwork $\mathcal{M}^c$ on our evaluation metrics.

% \begin{table}[h]
% \centering
% \begin{tabular}{@{}ccc@{}}
% \toprule
% \textbf{Subnetwork} & \textbf{Val. PPL} & \textbf{BLiMP} \\
% \midrule
% \multicolumn{3}{l}{\textbf{$k$-Shuffle Dyck}} \\
% Base & \textbf{42.85} \scriptsize{$\pm$0.XX} & 0.718 \scriptsize{$\pm$0.XX} \\
% Pruned: $\mathcal{M}$ & 53.15 \scriptsize{$\pm$0.XX} & 0.691 \scriptsize{$\pm$0.XX} \\
% $\sim$Pruned: $\mathcal{M}^c$ & 433.5 \scriptsize{$\pm$0.XX} & 0.583 \scriptsize{$\pm$0.XX} \\
% Random: $\mathcal{M}_{\text{null}}$  & 124.6 \scriptsize{$\pm$0.XX} & 0.652 \scriptsize{$\pm$0.XX} \\
% \midrule
% \multicolumn{3}{l}{\textbf{Controls}} \\
% No PPT & 44.53 \scriptsize{$\pm$0.XX} & 0.711 \scriptsize{$\pm$0.XX} \\
% English & 43.44 \scriptsize{$\pm$0.XX} & 0.709 \scriptsize{$\pm$0.XX} \\
% \bottomrule
% \end{tabular}
% \caption{Before and after pruning.}
% \label{tab:pruning}
% \end{table}

In Figure \ref{fig:pruning}, we find that $\mathcal{M}$  outperforms random masking and $\mathcal{M}^c$. We reject the null hypothesis that the subnetwork $\mathcal{M}$ established during pre-pretraining has the same performance as a randomly sampled subnetwork ($p \ll 0.001)$. With $\mathcal{M}^c$ performing poorly, this suggests that the subnetwork learned during pre-pretraining is indeed being reused in natural language modeling. We also note that $\mathcal{M}$ does not achieve the performance of the full network, indicating that attention heads outside of $\mathcal{M}$ are also useful for natural language.

% \begin{table*}[t]
% \centering
% \begin{tabular}{ll ccc}
% \toprule
% & \textbf{Language} & \textbf{LM Loss} & \textbf{Grammaticality} & \textbf{Retrieval} \\ 
% \midrule
% \multirow{3}{*}{$n$-gram} & Unigram & {3.922 \scriptsize{$\pm$0.006}} & {0.715 \scriptsize{$\pm$0.011}} & {3.406 \scriptsize{$\pm$0.025}} \\ 
% & Bigram & {3.895 \scriptsize{$\pm$0.009}} & {0.712 \scriptsize{$\pm$0.003}} & {3.392 \scriptsize{$\pm$0.010}} \\ 
% & Trigram & {3.882 \scriptsize{$\pm$0.006}} & {0.715 \scriptsize{$\pm$0.005}} & {3.360 \scriptsize{$\pm$0.026}} \\ 
% \midrule
% \multirow{4}{*}{$k$} & $k=32$ & {3.733 \scriptsize{$\pm$0.012}} & {0.708 \scriptsize{$\pm$0.006}} & {3.316 \scriptsize{$\pm$0.009}} \\ 
% & $k=64$ (base) & {3.741 \scriptsize{$\pm$0.014}} & {0.718 \scriptsize{$\pm$0.007}} & {3.297 \scriptsize{$\pm$0.012}} \\ 
% & $k=128$ & \bfseries{3.731 \scriptsize{$\pm$0.012}} & \bfseries{0.725 \scriptsize{$\pm$0.003}} & \bfseries{3.292 \scriptsize{$\pm$0.013}} \\ 
% & $k=256$ & \bfseries{3.731 \scriptsize{$\pm$0.012}} & {0.724 \scriptsize{$\pm$0.006}} & {3.323 \scriptsize{$\pm$0.019}} \\
% \midrule
% \midrule
% \multirow{3}{*}{Pythia-1B} & $k$-Shuffle Dyck & \bfseries{3.382 \scriptsize{$\pm$0.007}} & \bfseries{0.739 \scriptsize{$\pm$0.003}} & \bfseries{2.996 \scriptsize{$\pm$0.003}} \\ 
% & c4 & {3.396 \scriptsize{$\pm$0.009}} & {0.737 \scriptsize{$\pm$0.002}} & {3.007 \scriptsize{$\pm$0.002}} \\ 
% & No ppt & {3.404 \scriptsize{$\pm$0.007}} & {0.736 \scriptsize{$\pm$0.005}} & {3.014 \scriptsize{$\pm$0.007}} \\ 
% \bottomrule
% \end{tabular}
% \caption{Pre-pretraining on $n$-gram metamers \cite{Kumar2022DisentanglingAF} of $k$-Shuffle Dyck performs worse than pre-pretraining on $k$-Shuffle Dyck itself. The best vocabulary size for $k$-Shuffle Dyck we observe is $k=128$. For a 1B-parameter model trained for 1.6B tokens, $k$-Shuffle Dyck pre-pretraining continues to outperform C4.}
% \label{tab:ppt-ablations}
% \end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/language_model_metrics_ngram_k.pdf}
    \caption{Pre-pretraining on $n$-gram metamers \cite{Kumar2022DisentanglingAF} of $k$-Shuffle Dyck performs worse than pre-pretraining on $k$-Shuffle Dyck itself. The best vocabulary size for $k$-Shuffle Dyck we observe is $k=128$.}
    \label{fig:ppt-ablations}
    \includegraphics[width=\linewidth]{figures/language_model_metrics_pythia.pdf}
    \caption{Pre-pretraining Pythia-1B on $k$-Shuffle Dyck continues to outperform baselines, even after 1.6B pretraining tokens.}
    \label{fig:ppt-1b}
\end{figure*}

\paragraph{A Syntactic Subnetwork.} Last, we provide evidence that the subnetwork $\mathcal{M}$ mostly implements syntax, but that pre-pretraining also helps models learn non-syntactic knowledge like morphology, or word structure. To observe this, we first compare $\mathcal{M}$'s performance on BLiMP against that of the full network, aggregated by the classification of linguistic phenomena as reflecting morphology (27\% of BLiMP examples) versus syntax and semantics (73\%). In Figure \ref{fig:pruning-effects} (left), we see that morphology declines in performance from full to $\mathcal{M}$ much more than syntax and semantics. This suggests the attention heads driving performance on morphology lie outside of $\mathcal{M}$, aligning with the intuition that pre-pretraining on formal languages should form a subnetwork attending mainly to structure. However, when we examine the performance gain over no pre-pretraining (Figure \ref{fig:pruning-effects} right), we find that morphology also benefits from pre-pretraining. We conclude that \textbf{pre-pretraining has second-order effects}. In addition to syntactic benefits, pre-pretraining can improve performance on non-syntactic tasks.
% Speculatively, this could be because more compact representations of syntax resulting from pre-pretraining could free up model capacity for representing other aspects of language, though we have no mechanistic evidence to support this hypothesis.

However, pre-pretraining is also not a free lunch. Figure \ref{fig:blimp-accuracies}, which shows BLiMP accuracies by subtask, includes some subtasks where no pre-pretraining outperforms $k$-Shuffle Dyck pre-pretraining. Thus, pre-pretraining, mediated through the subnetwork $\mathcal{M}$, \textbf{can also hinder performance}. Understanding how to tailor the improvements of pre-pretraining to one's use case is an exciting area for future work.

\begin{figure}[htbp] 
    \vspace{-1.0em}
    \centering 
    \includegraphics[width=\columnwidth]{figures/pruning_effects.pdf} 
    \caption{Pruning the model to $\mathcal{M}$ decreases morphological performance more than performance on syntax and semantics in BLiMP (left), but pre-pretraining itself still has a positive impact on morphological performance (right).} 
    \label{fig:pruning-effects} 
\end{figure}

\section{Analyzing Shuffle Dyck}
\label{sec:analysis}

In \S \ref{sec:expressivity}, we observed that pre-pretraining on $k$-Shuffle Dyck helps models learn natural language. We now perform some additional analyses that help qualify this result. First, we examine whether the rule-based structure of $k$-Shuffle Dyck is truly necessary, over a facsimile dataset with the same statistical properties. Next, we study the impact that varying the vocabulary size $k$ of $k$-Shuffle Dyck has on performance. Finally, we perform a larger scale training run with Pythia 1B and find that pre-pretraining on $k$-Shuffle Dyck helps in this setting as well. Results for this section are in Figures \ref{fig:ppt-ablations} and \ref{fig:ppt-1b}.


\paragraph{Order statistics.} We seek to understand whether some low-level statistical properties of $k$-Shuffle Dyck explain its success, instead of its dependency structure. Indeed, neural networks can exhibit \textbf{distributional simplicity bias} (DSB)---they learn simpler statistical patterns, such as the mean and covariance of their representations, before progressing to higher-order relationships \cite{Saxe2013ExactST,lecun-eigen}. \citet{belrose2024neural} also demonstrate this phenomenon in transformer language models, which learn $n$-gram statistics in order of increasing complexity.

Accordingly, we ablate the rule-based structure of $k$-Shuffle Dyck by training unigram, bigram, and trigram models on our pre-pretraining corpus. We then generate from these $n$-gram models to produce metamer datasets \cite{Kumar2022DisentanglingAF} of equivalent size. These metamers match $k$-Shuffle Dyck up to some $n$-gram statistic, but do not capture its rule. 

We find that pre-pretraining on metamer datasets is strictly worse than pre-pretraining on $k$-Shuffle Dyck. However, the results follow a predictable pattern---pre-pretraining on the unigram metamer performs the worst, followed by bigram and trigram. This indicates that DSB might also apply within pre-pretraining. 
% Nevertheless, because $k$-Shuffle Dyck is both simple to generate and yields the best performance, it is unlikely that a higher-order approximation of $k$-Shuffle Dyck would both perform better and be simpler to generate. We conclude that pre-pretraining on a dataset with valid rules is likely simpler than pre-pretraining on statistical approximations. 


\paragraph{Vocabulary size.} To check whether better hyperparameters exist for $k$-Shuffle Dyck, we sweep its vocabulary size, trying $k=32,128$ and $256$ in addition to our previous experiments with $k=64$. We find that $k=128$ has the best performance across all metrics instead of $k=64$, suggesting there likely do exist better hyperparameters. 

Finding good ways to optimize these hyperparameters is an interesting area for future work. The hyperparameter tuning process for pre-pretraining is expensive, as evaluating the hyperparameters requires pretraining a language model. Nevertheless, various approximations such as early truncation exist in the hyperparameter tuning literature \cite{hyperband,Swersky2013MultiTaskBO}, and one can also use scaling laws to experiment at a smaller scale first \cite{Yang2022TensorPV}. 

\paragraph{Larger scale.} Finally, we examine whether our results generalize to larger settings by training Pythia-1B for 1.63B tokens on C4 (25,000 steps). In this setting, pre-pretraining on $k$-Shuffle Dyck continues to outperform on all evaluation metrics (Figure \ref{fig:ppt-1b}) and achieves the final loss of no pre-pretraining in 1.10B total tokens. This equates to a token efficiency gain of 33\% $\left(1 - \frac{1.10\text{B}}{1.63\text{B}}\right)$, or an MRS of 17.3 $\left(\frac{1.63\text{B}-1.10\text{B}}{0.03\text{B}}\right)$. $k$-Shuffle Dyck's MRS is $\gg$1 for both 160M and 1B training runs, suggesting that pre-pretraining could increase the efficiency of large-scale pretraining as well.

\section{Related Work}
 
\paragraph{Efficient Learning.} The goal of pre-pretraining is similar to that of optimization-based meta-learning, which aims to create a weight initialization that allows the model to rapidly learn new tasks \cite{maml,fomaml}. \citet{mccoy2023modelingrapidlanguagelearning} use meta-learning over procedurally-generated context-free languages to rapidly learn formal linguistic patterns. Positive transfer in NLP also occurs across both domains and languages \citep{ruder-etal-2019-transfer,pruksachatkun-etal-2020-intermediate,deshpande-etal-2022-bert}. 

\paragraph{Linguistic Generalization.} In addition to formal languages, various other datasets can also change the generalization properties of neural networks. \citet{mueller-linzen-2023-plant} show that pretraining on child-directed speech gives a better inductive bias for learning hierarchical syntactic features than standard pretraining corpora. Furthermore, introducing a small amount of synthetic, disambiguating data into pretraining can induce a language model to change its generalization strategy \cite{warstadt-etal-2020-learning}. Algorithmic synthetic data (sampled from a formal language) is often used to test the generalization properties of neural networks \cite{mccoy-etal-2019-right,kim-linzen-2020-cogs,li-etal-2023-slog}.

\paragraph{Curriculum Learning.} Pre-pretraining can be seen as curriculum learning \citep{bengio2009curriculum}, where data increases in difficulty during training. Many language model training recipes have several different stages of data mixtures \cite{smollm2,2olmo2furious,instructgpt}. Recent work has also attempted to create algorithms that automate the discovery of language modeling curricula \cite{chen2025aioli,jiang2025adaptive}.

\section{Discussion}

% \wm{IMO This section should start by emphasizing that pre-pretraining on formal languages can be beneficial, and discuss the properties we found important (context-sensitive, C-RASP). Can talk about what this implies about useful inductive biases for language learning and potential for practical methodology. Then it should lead into the issue of the infinite design space of formal languages as an opportunity for future work} 

Our results suggest that pre-pretraining on formal languages with hierarchical dependencies representable in $\CRASP$ can improve the downstream loss and linguistic generalization of transformers. As we conjectured in the introduction, context-sensitivity alone is not sufficient for positive transfer. The copy language $ww$ contains cross-serial dependencies, but it contains no notion of hierarchy and is not definable in $\CRASP$, and pre-pretraining on it hurts natural language performance. Conversely, $k$-Dyck and $k$-Shuffle Dyck both model hierarchical dependencies and confer a better inductive bias than natural language. $k$-Shuffle Dyck, the language implementable in $\CRASP$ of the two, leads to the largest pre-pretraining gains.

As shown in~\S \ref{sec:pruning}, learning structure also has downstream effects. The main subnetwork identified in pre-pretraining does not contain the attention heads important for morphological acceptability judgments. Nevertheless, pre-pretraining does have a positive effect on models learning morphology. Thus, pre-pretraining may have the potential to improve models beyond their performance on hierarchical, rule-based tasks. At the same time, although the overall trend is positive, performance does decrease on some of the individual BLiMP tasks. So pre-pretraining has downstream tradeoffs as well.

We found pre-pretraining on formal languages to be beneficial even for the 1B parameter training setting. Furthermore, the MRS between formal and natural language is greater than one (Table \ref{tab:ppt-results}), meaning that one token of formal language in pre-pretraining substitutes for more than one token of natural language in pretraining. This is a surprising result from the perspective of statistical learning theory \cite{Vapnik2000TheNO}, in that we observe faster convergence by swapping in data from a \textit{different task}. Initializations affect learning dynamics \cite{mccoy-etal-2020-berts,sellam2022the}, and the better initialization from pre-pretraining materially speeds up natural language pretraining.

% Going forward, it would be interesting to find pre-pretraining languages that are even more effective than those considered here.
% As the space of possible formal languages to pre-pretrain on is infinite, linguistics and complexity theory can help motivate hypotheses for inductive biases that would useful to distill into a model to make pretraining more effective.
% The space of possible formal languages to pre-pretrain on is infinite. Linguistics and complexity theory can help guide our exploration of this infinite design space.

% , we hypothesized that pre-pretraining languages should be consistent with constraints from both natural language and the expressive power of transformers. The pre-pretraining language should be sufficiently rich to capture the hierarchical and cross-serial dependencies in natural language, but also recognizable by transformers. 


% Formal language pre-pretraining may also prove to be a practical method for improving the learning efficiency of language models. In all the settings we tried, pre-pretraining consistently outperformed pretraining on natural language only. Moreover, in some settings, pre-pretraining on a small amount of formal language data outperformed adding the same amount of iid natural language data, which is often considered the gold standard for data in machine learning.
% This suggests that formal language pre-pretraining can teach transformers useful inductive biases more efficiently than natural language.

% Our work opens several promising directions for future research. First, mechanistic interpretability techniques could help reveal exactly what neural networks learn when trained on formal languages. By analyzing the circuits that emerge during training, we could better understand how models encode different formal language patterns. 
% This could provide insights into questions like: Which attention heads are responsible for tracking matching parentheses? How do models implement counting mechanisms for Dyck languages? Understanding these mechanisms could help explain why pre-pretraining on certain formal languages proves beneficial.
% Second, an open question is whether we can find a closed-form initialization that captures the benefits provided by pre-pretraining. However, this may prove fundamentally challenging---if we could derive closed-form initializations that encode the patterns found in natural language, then we would not need to train language models using gradient descent. 

\FloatBarrier

\section{Limitations}
\label{sec:limitations}

In this work, we considered blocked training, where we first train on formal language followed by natural language. We chose blocked training because a good initialization from formal language can be easily plugged into existing pretraining pipelines, but mixing formal and natural language during training could lead to better performance \cite{tomek-mixing}. We also evaluated efficiency in a setting where pretraining data is plentiful, and one can train without running several epochs over the same data. A low-resource setting would also be interesting, and may yield different scaling properties with respect to pre-pretraining data \cite{Muennighoff2023ScalingDL}. This would be particularly relevant for non-English languages \cite{samuel-etal-2023-norbench,martin-etal-2020-camembert}. Finally, our work only considers transformers. Circuit complexity has also quantified the expressive power of neural networks like RNNs and state-space models \cite{merrill-etal-2020-formal,merrill2024the}, and it would be interesting to extend our results to these architectures.

% \section{Potential Risks}

% While our work demonstrates general benefits from formal language pre-pretraining, the induced inductive biases may not benefit all languages or tasks equally. As shown in our BLiMP analysis, some linguistic capabilities can actually be impaired. This suggests that practitioners should carefully evaluate the tradeoffs of this approach for their specific use case rather than applying it indiscriminately.

\section*{Acknowledgments}

Many thanks to Andy Yang, Angelica Chen, Dan Friedman, Isabel Papadimitriou, Lindia Tjuatja, Mayee Chen, Qingyang Zhu, and the NYU Computation and Psycholinguistics Lab for feedback and discussion.
This work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise.
This project is supported by the National Science Foundation (NSF) under grant NRT-HDR: FUTURE as well as Grant No. IIS-2239862.
MYH is supported by the NSF Graduate Research Fellowship.
WM is supported by the NSF Graduate Research Fellowship as well as the Two Sigma PhD Fellowship.


% Bibliography entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
% Custom bibliography entries only
% \bibliography{custom}

\vfill
\pagebreak
\appendix


\section{Proofs}
\label{app:proofs}

We make use of the following to establish that all languages we consider are context-sensitive.

\begin{lemma} \label{lem:fom-csl}
    Any language definable in $\FOM$ can be recognized by a context-sensitive grammar.
    % \wm{stated this way to make unambiguous that it's not strictly context-sensitive}
\end{lemma}

\begin{proof}
    \citet{barrington1990uniformity} show that the class of languages definable in $\FOM$ is $\mathsf{LOGTIME}$-uniform $\mathsf{TC}^0$, which is a subset of $\mathsf L = \mathsf{SPACE}(\log n)$.
    On the other hand, the context-sensitive languages are those languages recognizable by linear-bounded automata \citep{kuroda1964classes}. That is, $\mathsf{CSL} = \mathsf{NSPACE}(n)$.
    Putting these characterizations together, we see that
    \begin{equation*}
        \mathsf{TC}^0 \subseteq \mathsf{SPACE}(\log n) \subseteq \mathsf{NSPACE}(n) = \mathsf{CSL} .
    \end{equation*}
    Therefore we can conclude that any language definable in $\FOM$ is context-sensitive.
\end{proof}

We will make use of the classical pumping lemma to establish that some specific languages considered are \emph{strictly} context-sensitive, i.e., not context-free.

\begin{lemma}[Pumping Lemma, \citealp{bar-hillel-1961-formal}]
    Let $L$ be a context-free language. Then there exists a pumping constant $p > 0$ such that any string $s \in L$ of length $\abs{s} \geq p$ can be written as $s = uvwxy$ where
    \begin{compactenum}
        \item $\abs{vwx} \leq p$;
        \item $\abs{vx} \geq 1$; and
        \item $uv^nwx^ny \in L$ for all $n \geq 0$.
    \end{compactenum}
\end{lemma}

Additionally, we will leverage the following communication complexity result to prove that certain languages are undefinable in $\CRASP$:

\begin{lemma}[\citealp{huang2025a}, Theorem 12] \label{lem:crasp-cc}
    Let $L$ be a language definable in $\CRASP$.
    Fix $w \in L$ and $1 \leq i \leq \lvert w \rvert$.
    Let Alice have access to $w_{1:i}$ and Bob have access to $w_{i+1:\lvert w \rvert}$.
    Then there exists a protocol where Alice sends at most $O(\log n)$ bits to Bob and Bob can recognize whether $w \in L$.
\end{lemma}

Crucially, if some $L$ requires Alice to send Bob $\omega(\log n)$ bits, then it cannot be defined in $\CRASP$.

We will also use the equivalence between \CRASP\ and the Temporal Counting Logic $\KtSharp$ to show that languages are definable in \CRASP.
\begin{lemma}[\citealp{yang2024counting}, Theorem 4.3] \label{lem:crasp-ktsharp}
    A \textsf{C-RASP} program recognizes language $L$ if and only if a $\KtSharp$ formula defines $L$.
\end{lemma}

\subsection{Language Characterizations}

\begin{proposition}
$1$-Dyck is context-free and definable in \textsf{C-RASP}.
\end{proposition}
\begin{proof}
That $1$-Dyck is context-free follows from the fact that it can be generated by the following context-free grammar:
\begin{align*}
    S &\to \texttt{(}\, S\, \texttt{)}\, S, \\
    S &\to \varepsilon.
\end{align*}
$1$-Dyck is defined by the following $\KtSharp$ formula 
$$
\left(\sinceC{Q_{(}}=\sinceC{Q_{)}}\right)\land \left(\sinceC{\sinceC{Q_{)}}>\sinceC{Q_{(}}}=0\right),
$$
and so is implementable in \textsf{C-RASP} by \Cref{lem:crasp-ktsharp}.
\end{proof}

\begin{proposition} \label{prop:kdyck}
    For $k \geq 2$, $k$-Dyck is context-free and not definable in $\CRASP$.
\end{proposition}
\begin{proof}
That $k$-Dyck is context-free follows from the fact that it can be generated by a context-free grammar: for any fixed value of $k$, $k$-Dyck is generated by 
\begin{align*}
S &\to \texttt{(}_{i} \,S\, \texttt{)}_{i}\, S, \quad\text{where}\; i \in [k] \\
S &\to \varepsilon
\end{align*}

To see that $k$-Dyck is not definable in $\CRASP$, consider Dyck strings of length $2n$ where tokens $1$ to $n$ are opening braces, and tokens $n+1$ to $2n$ are closing braces.
If Alice receives the first $n$ tokens, she must send $\Omega(n)$ bits to Bob if Bob is to correctly recognize the input string, because each prefix has a different unique suffix that closes it.
So $k$-Dyck is not in $\CRASP$ by Lemma \ref{lem:crasp-cc}.
% Thus, by \Cref{lem:crasp-cc}, $k$-Dyck is not definable in $\CRASP$.
% \jp{Give $\FOM$ formula} \wm{key here is to define a ``depth index'' macro which is the number of items before $i$ with the same depth value. Then can find the paired open brace with each close brace by quantifying over tokens $j < i$ to find the previous $j$ s.t. depth $i$ - 1 = depth $j$ and depth index $i$ = depth index $j$. Then we just enforce that the token at $j$ is the opening brace matching the closing brace at $i$.}
\end{proof}

On the other hand, $k$-Dyck can be defined in $\FOM$.

\begin{proposition} \label{prop:kdyck-fom}
    For $k \geq 1$, $k$-Dyck is definable in $\FOM$.
\end{proposition}

\begin{proof}
Let $Q_((i)$ check whether token $i$ is \textit{any} of the $k$ opening parentheses, and $Q_{(_\kappa}(i)$ check whether token $i$ is the $\kappa$th opening parenthesis out of $k$. Continuing the definition from Section \ref{sec:circuit-background}:
\begin{align*}
    &\mathsf{depth}(i) \;\; \equiv \;\; \# j \leq i [Q_((i)] - \# j \leq i [Q_)(i)] \nonumber \\
    &\mathsf{dindex}(i) \;\; \equiv \;\; \#j \leq i [\mathsf{depth}(i) = \mathsf{depth}(j) ] \\
    &\mathsf{paired}(j, i) \;\; \equiv \;\; [\mathsf{depth}(j) = \mathsf{depth}(i) + 1] \wedge \\
    &\quad [\mathsf{dindex}(i) = \mathsf{dindex}(j)] \\
    &\mathsf{match}(j, i) \;\; \equiv \;\; \bigvee_\kappa  [Q_{(_\kappa}(j) \wedge Q_{)_\kappa}(i)] \\
    &\mathsf{closed}(i)  \;\; \equiv \;\; \exists j \leq i \left[ \mathsf{paired}(j, i) \wedge \mathsf{match}(j, i) \right]
\end{align*}

Having defined these macros, we are now ready to write the recognizer for $k$-Dyck:
\begin{align*}
    &[\mathsf{depth}(n) = 0] \wedge [\# i \leq n [ \mathsf{depth}(i) < 0] = 0]  \wedge \\
    &\quad \forall i \leq n \left[ \mathsf{closed}(i) \right]
\end{align*}
\end{proof}

To understand why this construction cannot be implemented in $\CRASP$,
observe that $\mathsf{paired}(j, i)$ and $\mathsf{match}(j, i)$ are binary predicates, which are not allowed in $\CRASP$.


\begin{lemma}
    For $k \geq 2$, $k$-Shuffle Dyck is strictly context-sensitive and definable in \textsf{C-RASP}.
\end{lemma}
\begin{proof}
\emph{See Ex.\ 7.20 in \citet{hopcroft-2006-introduction}}.
Consider the case when $k=2$.
Assume that $2$-shuffle Dyck is context-free.
Then $L = \texttt{(}^n \texttt{[}^m \texttt{)}^n \texttt{]}^m$ is context-free since it is the intersection of $k$-Shuffle Dyck with $\texttt{(}^*\texttt{[}^*\texttt{)}^*\texttt{]}^*$ and CFLs are closed under intersection with regular languages.

Assume by way of contradiction that $L$ is context-free and so has pumping constant $p$. Let $s = \texttt{(}^p \texttt{[}^p \texttt{)}^p \texttt{]}^p$, whichy by hypothesis can be written as $uvwxy$. Since $\abs{vwx} \leq p$, it either (a) lies entirely inside one of the blocks of $p$ symbols or (b) lies partially in one block of $p$ symbols and lies partially in at most one adjacent block. In the case of (a), suppose for clarity that $vwx$ lies entirely in the $\texttt{(}^p$ block. Since ${vx}$ is not empty, $uv^0wx^0y \equiv uwy$ contains fewer $\texttt{(}$'s than $\texttt{)}$'s, and hence is not in $L$, a contradiction. In the case of (b), suppose for clarity that $vwx$ straddles the $\texttt{(}^p$ and $\texttt{[}^p$ blocks. Since ${vx}$ is not empty, $uv^0wx^0y \equiv uwy$ contains either fewer $\texttt{(}$'s than $\texttt{)}$'s or fewer $\texttt{[}$'s than $\texttt{]}$'s, and hence is not in $L$, a contradiction. Since $k$-Shuffle Dyck for $k > 2$ contains $2$-Shuffle Dyck, proving the $k=2$ case is sufficient to establish that $k$-Shuffle Dyck is not context free (but still context-sensitive by \Cref{lem:fom-csl}).

% To see that $k$-Shyffle Dyck is context-sensitive, observe that it can be recognized by a real-time $k$-counter automaton, where counter $k$ verifies that the number of opening and closing braces of type $k$ match.
% This automaton uses $O(\log n)$ space.
% Since the context-sensitive languages can equivalently be characterized as nondeterministic linear space, this shows that $ww$ is context-sensitive.


Similar to the $1$-Dyck case, we can exhibit a $\KtSharp$ formula to define $k$-Shuffle Dyck:
$$
\bigwedge_\kappa \left(\sinceC{Q_{(_\kappa}}=\sinceC{Q_{)_\kappa}}\right)\land \left(\sinceC{\sinceC{Q_{)_\kappa}}>\sinceC{Q_{(_\kappa}}}=0\right)
$$
So $k$-Shuffle Dyck is likewise definable in \textsf{C-RASP} by \Cref{lem:crasp-ktsharp}.
\end{proof}

\begin{proposition}
    $ww$ is strictly context-sensitive and not definable in $\CRASP$.
\end{proposition}
\begin{proof}
    \emph{See Ex.\ 7.21 in \citet{hopcroft-2006-introduction}}.
    Suppose by way of contradiction that $ww$ is context-free, and so has a pumping constant $p$. Let $s = \texttt{a}^p\texttt{b}^p\texttt{a}^p\texttt{b}^p$, which can be written as $uvwxy$ by hypothesis. Consider then the string $uv^0wx^0y \equiv uwy \in L$. We examine two cases depending on the position of $vwx$ in $s$. 
    
    In the first case, suppose $vx$ is contained entirely within the first block of $\texttt{a}^p$. If $\abs{vx} = k$ then $uwy$ has length $4p-k$ and begins with the substring $\texttt{a}^{(p-k)}\texttt{b}^p...$ of length $2p-k$. By assumption $uwy = tt$ for some $t$ of length $2p-k/2$, and since $k \geq 1$ it follows that $\abs{t} > 2p-k$. Then the final symbol of $t$ must lie within the \emph{second} block of $\texttt{a}$'s; yet since $s$ ends in $\texttt{b}$, $tt$ must also end in $\texttt{b}$, a contradiction.

    In the second case, suppose $vx$ contains some $\texttt{a}$'s and some $\texttt{b}$'s. Since $\abs{uvwxy} = 4p$ and $\abs{vwx} \leq p$ it must be that $\abs{uwy} \geq 3p$ and so $\abs{t} = 3p/2$. Since $vwx$ is too short to straddle more than two adjacent blocks of symbols and $3p/2 > p$ it must be the case that $t$ must end in $\texttt{b}^p$. Yet there since $\abs{vx} \geq 1$, there is only a single block of $\texttt{b}^p$ within $\abs{uwy}$, so the $\texttt{b}^p$ block cannot be repeated, a contradiction.

    By symmetry, these two cases straightforwardly extend to the cases when $vx$ is contained entirely within the first block of $\texttt{b}$'s, the second block of $\texttt{a}$'s, or the second block of $\texttt{b}$'s (analogous to case 1); or when it is split between the blocks of $\texttt{a}$'s and $\texttt{b}$'s (case 2). Then $ww$ is not context-free, but still context-sensitive by \Cref{lem:fom-csl}.

    % To see that $ww$ context-sensitive, observe that it can be recognized by a real-time queue automaton, which uses at most linear space. Since the context-sensitive languages can equivalently be characterized as nondeterministic linear space, this shows that $ww$ is context-sensitive.

    
    From a communication complexity perspective, if Alice has the first half of some string, and Bob has the second half, Alice must send Bob $\Omega(n)$ bits to verify whether the string is of the form $ww$.
    Thus, by \Cref{lem:crasp-cc}, $ww$ cannot be defined in $\CRASP$.
    % \jp{Give $\FOM$ formula} \wm{one option is to use circuit reasoning here. like, can construct an AC0 circuit that checks whether $x_i = x_{i+n}$. Then this is also a TC0 circuit, which means it's definable in FO[M]. but also, it should be possible to write this down in C-RASP as quantifying over all positions $i < n$, and just checking there is some $\sigma$ such that $Q_\sigma(i) \wedge Q_\sigma(i + n)$. Between these two, I think a constructive C-RASP formula would be cleaner and more useful for understanding}
\end{proof}


\begin{figure}[htbp] 
    \centering 
    \includegraphics[width=0.95\columnwidth]{figures/MRS.pdf} 
    \caption{The indifference curve contains points with equal training loss. Marginal rate of substitution is the ratio between the red and black lines $\left( \frac{|y_1-y_2|}{|x_1-x_2|} \right)$. The token efficiency increase from applying pre-pretraining can be calculated as $1 - \frac{x_2  + y_2}{y_1}$.} 
    \label{fig:mrs} 
\end{figure}


\section{Hyperparameters}
\label{app:hyperparams}

All experiments were done on NVIDIA A100 or H100 80GB GPUs. We warm up the learning rate both during pre-pretraining and pretraining. The below hyperparameters hold for both pre-pretraining and pretraining. That is, for simplicity, even if we only pre-pretrain for 500 steps, we still keep the learning rate warmup at 1,000 steps. 
%  See Table \ref{tab:lm-hyperparams}.
To achieve 50\% attention head sparsity when pruning, we set the target sparsity to 70\%. We used Huggingface \texttt{transformers==4.47.0} and \texttt{datasets==3.2.0}.

\begin{table}[h]    % table* makes the table span both columns

\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
\multicolumn{2}{l}{\textit{Training Configuration}} \\
\midrule
Batch size & 16 \\
Gradient accumulation & 2 \\
Effective bsz & 32 \\
Sequence length & 2048 tokens \\
Learning rate & $5 \times 10^{-4}$ \\
LR schedule & Cosine w/ warmup \\
Min. LR & $5 \times 10^{-5}$ \\
Warmup Steps & 1000 \\
Weight Decay & 0.1 \\
Gradient Clipping & 1.0 \\
\midrule
\multicolumn{2}{l}{\textit{Optimization}} \\
\midrule
Optimizer & AdamW \\
$\beta_1, \beta_2$ & 0.9, 0.999 \\
$\epsilon$ & 1e-6 \\
Mixed Precision & bf16 \\
\midrule
\multicolumn{2}{l}{\textit{Pruning} (see \citet{bhaskar-etal-2024-heuristic})} \\
\midrule
Learning rate & 0.1 \\
Regularization LR & 1 \\
Target sparsity & 0.7 \\
Warmup steps & 1000 \\
\bottomrule
\end{tabular}
\caption{Training hyperparameters.}
\label{tab:lm-hyperparams}
\end{table}


\newcommand{\posex}[1]{\textcolor{green!50!black}{\ding{51} #1}}
\newcommand{\negex}[1]{\textcolor{black}{\ding{55} #1}}

\begin{table*}[htbp]
\centering
\begin{tabular}{>{\raggedright\arraybackslash}p{0.45\textwidth}>{\raggedright\arraybackslash}p{0.45\textwidth}}
\toprule
\textbf{Positive Example} & \textbf{Negative Example} \\
\midrule
\posex{Only Bill would ever complain.} & \negex{Even Bill would ever complain.} \\
\posex{Diane watched Alan.} & \negex{Diane screamed Alan.} \\
\posex{Who should Derek hug after shocking Richard?} & \negex{Who should Derek hug Richard after shocking?} \\
\bottomrule
\end{tabular}
\caption{BLiMP \cite{warstadt-etal-2020-blimp-benchmark} dataset examples showing grammatical (positive) and ungrammatical (negative) sentence pairs}
\label{tab:blimp}
\end{table*}

\begin{table*}[htbp]
\centering
\renewcommand{\arraystretch}{1.3} % Adjust line spacing if needed
\begin{tabular}{p{0.8\textwidth}}
\toprule
\textbf{Examples} \\
\midrule
Before the meeting, Mary wrote down the following list of words:  
\textbf{window, door, roof}. After the meeting, she took a break and had a cup of coffee. When she got back, she read the list again:  
\textbf{window, door, roof}. \\
\midrule
Before the meeting, John wrote down the following list of words: \textbf{nothing, riches, paper}. After the meeting, he took a break and had a cup of coffee. When he got back, he read the list again: \textbf{nothing, riches, paper}. \\
\bottomrule
\end{tabular}
\caption{Verbatim in-context retrieval \cite{armeni-etal-2022-characterizing,armeni-etal-2024-transformer} examples. A good language model should recognize from context that the list is repeated.}
\label{tab:verbatim}
\end{table*}


\begin{figure*}[htbp] 
    \centering 
    \includegraphics[width=\textwidth]{figures/blimp_accuracy.pdf} 
    \caption{Grammaticality judgment (BLiMP) accuracies by task.} 
    \label{fig:blimp-accuracies} 
\end{figure*}

\begin{table*}[ht]
\centering

\begin{tabularx}{\textwidth}{ll cccc c}
\toprule
& \textbf{Language} & \textbf{LM Loss} & \textbf{Documents $\downarrow$} & \textbf{MRS} & \textbf{Grammaticality} & \textbf{Retrieval} \\ 
\midrule
\multirow{5}{*}{Formal} & 1-Dyck & {3.760 {\scriptsize{$\pm$0.016}}} & {0.978 {\scriptsize{$\pm$0.021}}} & 3.01 & {0.717 {\scriptsize{$\pm$0.004}}} & {3.373 {\scriptsize{$\pm$0.014}}} \\ 
& $k$-Dyck & {3.743 {\scriptsize{$\pm$0.016}}} & \bfseries{0.998 {\scriptsize{$\pm$0.001}}} & 3.57 & \bfseries{0.719 {\scriptsize{$\pm$0.003}}} & {3.338 {\scriptsize{$\pm$0.019}}} \\ 
& $k$-Shuffle Dyck & \bfseries{3.741 {\scriptsize{$\pm$0.014}}} & \bfseries{0.998 {\scriptsize{$\pm$0.001}}} & \bfseries{7.15} & {0.718 {\scriptsize{$\pm$0.007}}} & \bfseries{3.297 {\scriptsize{$\pm$0.012}}} \\
& $ww$ & {3.792 {\scriptsize{$\pm$0.018}}} & {0.557 {\scriptsize{$\pm$0.247}}} & {$-$}0.25 & {0.714 {\scriptsize{$\pm$0.003}}} & {3.341 {\scriptsize{$\pm$0.021}}} \\
\midrule
\multirow{4}{*}{Controls} & No pre-pretraining & {3.780 {\scriptsize{$\pm$0.018}}} & {---} & {---} & {0.710 {\scriptsize{$\pm$0.011}}} & {3.393 {\scriptsize{$\pm$0.003}}} \\ 
& C4 pre-pretraining & {3.754 {\scriptsize{$\pm$0.017}}} & {0.992 {\scriptsize{$\pm$0.007}}} & 6.65 & {0.710 {\scriptsize{$\pm$0.003}}} & {3.354 {\scriptsize{$\pm$0.005}}} \\ 
& Random binary & {3.810 {\scriptsize{$\pm$0.015}}} & {0 {\scriptsize{$\pm$0}}} & {$-$}6.60 & {0.712 {\scriptsize{$\pm$0.004}}} & {3.416 {\scriptsize{$\pm$0.016}}} \\ 
& Random ints & {3.798 {\scriptsize{$\pm$0.015}}} & {0.042 {\scriptsize{$\pm$0.041}}} & {$-$}5.97 & {0.712 {\scriptsize{$\pm$0.006}}} & {3.409 {\scriptsize{$\pm$0.006}}} \\ 
\bottomrule
\end{tabularx}

\caption{Evaluating models at the optimal amount of pre-pretraining $t_0^*$ for each formal language. ``Documents $\downarrow$'' is the proportion of documents in the C4 validation set with lower loss than the model trained without pre-pretraining. 1-Dyck, $k$-Dyck, and $k$-Shuffle-Dyck all have marginal rates of substitution (MRS) greater than 1, indicating that pre-pretraining is more efficient than not pre-pretraining. $k$-Shuffle-Dyck performs the best overall on our evaluation metrics.}
\label{tab:ppt-results}
\end{table*}


\begin{figure*}[t]
\renewcommand{\lstlistingname}{Code} 
\begin{lstlisting}[language=Python, caption={Implementation of a $k$-Shuffle Dyck sequence generator.}]
import random


def generate_shuff_dyck(k, max_length=2048, p_open=0.5, max_depth=16):
    """
    Generate a k-shuffle Dyck sequence, truncated at max_length.
    When max depth is reached, close one bracket and continue.

    Args:
        k (int): Number of different types of brackets
        max_length (int): Target maximum length of the sequence
        p_open (float): Probability of opening a new bracket
        max_depth (int): Maximum nesting depth allowed

    Returns:
        list: Generated sequence where i represents opening bracket i
             and i+k represents closing bracket i

    Note: the final Dyck word may be invalid due to truncation, but
    we didn't find this to be an issue in practice.
    """
    sequence = []
    counts = [0] * k  # Track open brackets of each type

    while len(sequence) < max_length:
        depth = sum(counts)

        # Must open if all brackets are closed
        if depth == 0:
            bracket = random.randint(0, k - 1)
            sequence.append(bracket)
            counts[bracket] += 1
            continue

        # If at max depth, force a close
        if depth >= max_depth:
            open_brackets = [i for i, count in enumerate(counts) if count > 0]
            bracket = random.choice(open_brackets)
            sequence.append(bracket + k)
            counts[bracket] -= 1
            continue

        # Randomly choose to open or close
        if random.random() < p_open and depth < max_depth:
            bracket = random.randint(0, k - 1)
            sequence.append(bracket)
            counts[bracket] += 1
        else:
            # Close an existing bracket
            open_brackets = [i for i, count in enumerate(counts) if count > 0]
            bracket = random.choice(open_brackets)
            sequence.append(bracket + k)
            counts[bracket] -= 1

    return sequence

\end{lstlisting}
\end{figure*}


\end{document}
