\section{Related Work}
\paragraph{Efficient Learning.} The goal of pre-pretraining is similar to that of optimization-based meta-learning, which aims to create a weight initialization that allows the model to rapidly learn new tasks \cite{maml,fomaml}. \citet{mccoy2023modelingrapidlanguagelearning} use meta-learning over procedurally-generated context-free languages to rapidly learn formal linguistic patterns. Positive transfer in NLP also occurs across both domains and languages \citep{ruder-etal-2019-transfer,pruksachatkun-etal-2020-intermediate,deshpande-etal-2022-bert}. 

\paragraph{Linguistic Generalization.} In addition to formal languages, various other datasets can also change the generalization properties of neural networks. \citet{mueller-linzen-2023-plant} show that pretraining on child-directed speech gives a better inductive bias for learning hierarchical syntactic features than standard pretraining corpora. Furthermore, introducing a small amount of synthetic, disambiguating data into pretraining can induce a language model to change its generalization strategy \cite{warstadt-etal-2020-learning}. Algorithmic synthetic data (sampled from a formal language) is often used to test the generalization properties of neural networks \cite{mccoy-etal-2019-right,kim-linzen-2020-cogs,li-etal-2023-slog}.

\paragraph{Curriculum Learning.} Pre-pretraining can be seen as curriculum learning \citep{bengio2009curriculum}, where data increases in difficulty during training. Many language model training recipes have several different stages of data mixtures \cite{smollm2,2olmo2furious,instructgpt}. Recent work has also attempted to create algorithms that automate the discovery of language modeling curricula \cite{chen2025aioli,jiang2025adaptive}.