\section{Related Work}
\paragraph{Efficient Learning.} The goal of pre-pretraining is similar to that of optimization-based meta-learning, which aims to create a weight initialization that allows the model to rapidly learn new tasks ____. ____ use meta-learning over procedurally-generated context-free languages to rapidly learn formal linguistic patterns. Positive transfer in NLP also occurs across both domains and languages ____. 

\paragraph{Linguistic Generalization.} In addition to formal languages, various other datasets can also change the generalization properties of neural networks. ____ show that pretraining on child-directed speech gives a better inductive bias for learning hierarchical syntactic features than standard pretraining corpora. Furthermore, introducing a small amount of synthetic, disambiguating data into pretraining can induce a language model to change its generalization strategy ____. Algorithmic synthetic data (sampled from a formal language) is often used to test the generalization properties of neural networks ____.

\paragraph{Curriculum Learning.} Pre-pretraining can be seen as curriculum learning ____, where data increases in difficulty during training. Many language model training recipes have several different stages of data mixtures ____. Recent work has also attempted to create algorithms that automate the discovery of language modeling curricula ____.