%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}

% Checklist macros
\usepackage{xcolor}
\newcommand{\answerYes}[1]{\textcolor{blue}{#1}} 
\newcommand{\answerNo}[1]{\textcolor{teal}{#1}} 
\newcommand{\answerNA}[1]{\textcolor{gray}{#1}} 
\newcommand{\answerTODO}[1]{\textcolor{red}{#1}} 


%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright %-- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\usepackage{longtable}
\usepackage{booktabs}
\usepackage{amsmath}

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{WikiReddit: Tracing Information and Attention Flows Between Online Platforms
% \thanks{Accepted at the 19th International AAAI Conference on Web and Social Media (ICWSM 2025).}
}
\author{
    %Authors
    % All authors must be in the same font size and format.
    % Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    % AAAI Style Contributions by Pater Patel Schneider,
    % Sunil Issar,\\
    % J. Scott Penberthy,
    % George Ferguson,
    % Hans Guesgen,
    % Francisco Cruz\equalcontrib,
    % Marc Pujol-Gonzalez\equalcontrib
    Patrick Gildersleve\textsuperscript{\rm 1},
    Anna Beers\textsuperscript{\rm 2},
    Viviane Ito\textsuperscript{\rm 2},
    Agustin Orozco\textsuperscript{\rm 2},
    Francesca Tripodi\textsuperscript{\rm 2}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1} University of Exeter\\
    p.gildersleve@exeter.ac.uk\\
    \textsuperscript{\rm 2} University of North Carolina at Chapel Hill \\
    \{albeers@, itovivi@, aorozco@, ftripodi@email.\}unc.edu
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript


%
% See more examples next
}

% %Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \iffalse
% \title{My Publication Title --- Single Author}
% \author {
%     Author Name
% }
% \affiliations{
%     Affiliation\\
%     Affiliation Line 2\\
%     name@example.com
% }
% \fi

% \iffalse
% %Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \title{My Publication Title --- Multiple Authors}
% \author {
%     % Authors
%     First Author Name\textsuperscript{\rm 1},
%     Second Author Name\textsuperscript{\rm 2},
%     Third Author Name\textsuperscript{\rm 1}
% }
% \affiliations {
%     % Affiliations
%     \textsuperscript{\rm 1}Affiliation 1\\
%     \textsuperscript{\rm 2}Affiliation 2\\
%     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
% }
% \fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
% \usepackage{bibentry}
% END REMOVE bibentry

% \usepackage{fancyhdr}

% \pagestyle{fancy}
% \fancyhf{}
% \renewcommand{\headrulewidth}{0pt}
% \fancyhead[C]{\footnotesize Accepted at the 19th International AAAI Conference on Web and Social Media (ICWSM 2025)}

% \makeatletter
% \def\@fnsymbol#1{}
% \makeatother

\begin{document}
\maketitle

\begin{abstract}

The World Wide Web is a complex interconnected digital ecosystem, where information and attention flow between platforms and communities throughout the globe. These interactions co-construct how we understand the world, reflecting and shaping public discourse.  Unfortunately, researchers often struggle to understand how information circulates and evolves across the web because platform-specific data is often siloed and restricted by linguistic barriers. To address this gap, we present a comprehensive, multilingual dataset capturing all Wikipedia mentions and links shared in posts and comments on Reddit 2020--2023, excluding those from private and NSFW subreddits. Each linked Wikipedia article is enriched with revision history, page view data, article ID, redirects, and Wikidata identifiers. Through a research agreement with Reddit, our dataset ensures user privacy while providing a query and ID mechanism that integrates with the Reddit and Wikipedia APIs. This enables extended analyses for researchers studying how information flows across platforms. For example, Reddit discussions use Wikipedia for deliberation and fact-checking which subsequently influences Wikipedia content, by driving traffic to articles or inspiring edits. By analyzing the relationship between information shared and discussed on these platforms, our dataset provides a foundation for examining the interplay between social media discourse and collaborative knowledge consumption and production.

\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
%
\begin{links}
    \link{Code}{https://github.com/pgilders/WikiReddit}
    \link{Dataset}{https://doi.org/10.5281/zenodo.14653265}
\end{links}

\section{Introduction}

Researching flows of information, attention, and discussion on online platforms, and how they both reflect and shape public discourse, is a key challenge in understanding the dynamics of the modern web. There is ample work studying these effects within individual communities and platforms \cite{crane2008robust,lehmann2012dynamical,twyman_black_2017,kobayashi2021modeling,shen_tale_2022,johnson_global_2021,matsui_throw_2024}. While this foundational work set the stage for the importance of computational social science, the web is a complex, interconnected ecosystem. Users freely move between platforms, communities, and devices. To better understand how information propagates and spreads, we must consider interactions across platforms. Indeed, an increasing number of multi-platform studies have identified effects not observed or observable within individual platforms \cite{mcdowall_sensemaking_2024,kloo_cross-platform_2024,moyer_determining_2015, dubois2018echo}. Unfortunately, undertaking this task in research is becoming increasingly difficult. The deterioration of the open web towards that of large centralized platforms where data is often walled off behind APIs, paywalls, or other restrictions, has made it harder to study the web as a whole \citep{freelon_computational_2018,de_vreese_data_2023}. Research opportunities that can bridge these silos are thus of great value.

Wikipedia, as the globally recognized repository of collective knowledge, and Reddit, the foremost social news aggregation and forum site, form an intriguing pairing in this ecosystem. Links to Wikipedia articles on Reddit often serve as references, fact-checking tools, or catalysts for discussion, making them a key touchpoint for understanding how collective attention and knowledge circulates and evolves online \cite{moyer_determining_2015}. However, foundational work on this relationship documented a ``paradox of re-use;'' private platforms like Reddit boost engagement and revenue by relying on Wikipedia links but the relationship is not reciprocal \cite{taraborelli_sum_2015,vincent_examining_2018}. These studies documented the uneven value derived from Wikipedia volunteerism and shed light on how Wikipedia data sustains the economic interests of private platforms and satisfies informational needs.  By expanding on this important work, our database will help researchers better understand the social and information dynamics of these interactions. How Wikipedia links are shared, interpreted, and acted upon within Reddit communities and the wider web---remains relatively underexplored.

We present WikiReddit, a comprehensive dataset capturing all Wikipedia mentions (including links) shared in posts and comments on Reddit from 2020 to 2023, excluding those from private and NSFW (not safe for work) subreddits. The SQL database comprises 336K total posts, 10.2M comments, 1.95M unique links, and 1.26M unique articles spanning  59 languages on Reddit and 276 Wikipedia language subdomains. Each linked Wikipedia article is enriched with its revision history and page view data within a ±10-day window of its posting, as well as article ID, redirects, and Wikidata identifiers. Supplementary anonymous metadata from Reddit posts and comments further contextualizes the links, offering a robust resource for analysing cross-platform information flows, collective attention dynamics, and the role of Wikipedia in online discourse. This dataset is distinct from those used in other works documenting similar relationships between Wikipedia and Reddit due to its scope (four years), diverse supporting data included, and approach to providing a long-term, sustainable resource, using officially licensed Reddit4Researchers API access. This approach ensures the long-term availability of the resource while adhering to ethical standards and user expectations around data use.  By ensuring sustainability, our dataset not only allows for initial analysis in this paper and beyond, but also creates an opportunity for longitudinal analysis moving forward.

In initial explorations of our dataset, we study the use and success of Wikipedia links on Reddit over time, the association between Wikipedia articles being posted on Reddit and changes in page view and editing activity, and finally the distribution of languages used in Reddit posts and linked to on Wikipedia. We find Wikipedia is being referred to in Reddit posts less frequently over the period of study, but the performance of those posts remains stable. There are notable associations between activity on Reddit and Wikipedia; article page views tend to increase on the day the link posts to Reddit and continue, less markedly for a week after. However, the relationship is much weaker for editing activity. Regarding languages, English is the dominant language in the Reddit data, and this is reflected in the Wikipedia links posted. However, there is substantial cross-lingual linking to and from English.

In the following sections, we review related work, articulate how the WikiReddit dataset was developed and summarize its structure. We then undertake some exploratory analysis, before concluding with suggested applications and future research that may be undertaken with the dataset.

\section{Related Work}

Many studies have documented the important role Wikipedia plays in how people find and validate information. Wikipedia content appears in over 80\% of knowledge panels and top-linked content across three different search engines (Google, Bing, and DuckDuckGo) comprising a large portion of most user-facing knowledge graph assets \cite{mcmahon_substantial_2017,vincent_deeper_2021,vincent_examining_2018}. Given the tight integration between search engines and Wikipedia, these studies shed light on how Wikipedia impacts human decision-making and influences other knowledge classification systems \cite{lerner_knowledge_2018,c_thompson_user-generated_2024,formisano_counter-misinformation_2024}. Corporate-owned platforms also depend on Wikipedia data. Many websites use Wikipedia hyperlinks and content to increase visitation, engagement, and revenue \cite{gomezmartinez_wikipedia_2022,lerner_knowledge_2018,moyer_determining_2015,vincent_examining_2018}.  Unfortunately, this economic dependency appears nonreciprocal---while Wikipedia’s open licenses make it easy for corporate sites to capitalize on its content, it does not produce migratory benefits like more viewership or edits on Wikipedia itself \cite{vincent_examining_2018}. 

In addition to Wikipedia, audiences consult a range of online services for news including news websites, apps, and social media \cite{st_aubin_news_2024,vraga_news_2021}. People who include, but do not exclusively rely on, social media in their news diets tend to have higher news media knowledge \cite{schulz_role_2024}. Among these social media sites is Reddit, a social media platform where community members share, vote, and comment on content and foster community-based engagement on topics or themes---colloquially referred to as subreddits. Previous research has explored the relationship between news coverage and Reddit engagement. \citet{gozzi_collective_2020} demonstrated that COVID-19 news coverage drove users to comment on Reddit and search for information on Wikipedia, though this effect decreased over time---probably due to media saturation. Further research on Reddit has found that fact-checked information lasts longer when a post is deemed true \cite{bond_engagement_2023}, reinforcing that users rely on Reddit's discussions when interpreting news. However, other work has scrutinized the credibility of information posted to Reddit---without editorial oversight, users may share biased viewpoints and content from known misinformation sources \cite{chipidza_ideological_2022}. Tangled into these findings on social media and news consumption is the role Wikipedia might still play in this process. Studies have found that Reddit users regularly rely on Wikipedia hyperlinks to validate information---especially within the ``Today I Learned'' subreddit \cite{moyer_determining_2015, vincent_examining_2018}. Nonetheless, access to this previous dataset is no longer feasible---since 2023 Reddit’s API is no longer available for free public use. Researchers wishing to access Reddit data must now submit an application for access to the ``Reddit4Researchers'' beta program---a new approach to partnering with data scientists to balance between data accessibility and user protection \cite{perez_reddit_2024}.

Datasets like the one we present in this paper are part of a long line of research committed to making Wikipedia data accessible and available to other social scientists. Previous papers have built Wikipedia datasets to assess the quality of content on Wikipedia \cite{das_language-agnostic_2024}; study its hyperlink structure \cite{consonni_wikilinkgraphs_2019}; understand how people interact with ‘news events’ \cite{gildersleve_between_2023}; and document platform interdependencies \cite{meier_twikiltwitter_2022}. The organizational structure of the site, its size, and the fact that Wikipedia is open access facilitate these dataset creations \cite{mitrevski_wikihist_2020}. These studies are also pushing the boundaries of Anglocentrism, creating databases that leverage ``language-agnostic'' or multilingual techniques to identify linguistic gaps, explain the informational needs of marginalized populations, and analyze how ideas propagate across the languages \cite{das_language-agnostic_2024,valentim_tracking_2021,miquel-ribe_wikipedia_2019}. Creating these datasets is no simple task, Wikipedia is a massive corpus of densely interlinked content, not just a ``ready-made data source'' \cite{gildersleve_between_2023,valentim_tracking_2021}. 

% The goal of our dataset is to expand on the accessibility of Wikipedia data while providing more research on how information ripples in and out of Wikipedia from Reddit - another prominent site of news and information. By focusing on informational structure versus monetary structure we shed light on XYZ\_.

\section{Dataset Development}

\subsection{Overview}

The dataset is shared as a SQLite3 database via Zenodo:  \url{https://doi.org/10.5281/zenodo.14653265}. Replication code for collection, exploratory analysis, and demo code is provided in the project repository: \url{https://github.com/pgilders/WikiReddit}. Data is collected from the Reddit4Researchers and Wikipedia APIs \cite{a2024_apimain}. This data from these APIs is licensed under the ``Reddit License'' \cite{_2024_developer} and CC BY-SA 4.0 respectively. Following the drastic changes made to the old Reddit API, as well as the shutdown of secondary tools and archives such as Pushshift \citep{mehta_2024_social}, Reddit has opened a new API for researchers program, which this project relies on and aims to integrate into for future data gathering \cite{a2024_our}. We used the WikiToolkit \citep{patrickgildersleve_2023_pgilderswikitoolkit} Python package for fast, reliable collection for a variety of Wikipedia data from their APIs and dumps. Data collection and demo code using WikiToolkit is provided in the repository for this article.

\subsection{Reddit Data}

We used API access to the Reddit4Researchers program to collect data from Reddit. Data is available for 4 years (2020-2023). We collected all posts that mention Wikipedia, either in the title or post body, including content and associated metadata. We also collected all comments on Reddit that mention Wikipedia including content and associated metadata.

For the purposes of this dataset, only post, comment, and subreddit IDs, together with anonymous metadata such as timestamps, score, and extracted Wikipedia links, are shared. All IDs are securely hashed using SHA-256, and checked for uniqueness. This measure preserves individuals' privacy and also enables future researchers to collect and analyze additional data on entries of interest with access to the Reddit4Researchers API by matching against our hashes. This data is stored in the \texttt{posts} and \texttt{comments} tables.

\subsection{Parsing URLs to Articles}

Not every post/comment mentioning Wikipedia includes a Wikipedia URL, and not all posted Wikipedia URLs are valid or correctly formatted such that they map to a valid Wikipedia page. Furthermore, there are a variety of ways in which a Wikipedia URL can link to an article (e.g., directly, via a redirect, to a specific revision). We developed a strategy to reliably extract these URLs and identify which articles they link to. The procedure is outlined as follows:

\begin{enumerate}
    \item Parse the post / comment text with a markdown parser, and extract all correctly hyperlinked Wikipedia URLs.
    \item For any malformed links from markdown and the full remaining text, run a split and regex for any further Wikipedia links.
    \item For all extracted links, run a validation step to ensure it successfully connects to Wikipedia
    \item If the link does not successfully connect, run cleaning steps with regex, removing unnecessary trailing characters.
    \item Recheck validity, and resolve any http redirects  as necessary for all URLs.
    \item For all links, try to extract the Wikipedia language subdomain and article title with regex.
    \item If the link is indirect (e.g., to a revision ID), query the Wikipedia API to get the article title.
    \item Query the Wikipedia API with the extracted article titles to identify any Wikipedia article redirects. Return this as `canonical\_title'.
\end{enumerate}
 
This data is stored in the \texttt{post\_links} (all links from posts), \texttt{comment\_links} (all links from comments), and \texttt{links\_articles} (unique valid links that map to an article) tables. A summary of all Wikipedia mentions, links, and articles posted to Reddit is provided in Table \ref{tab:linksummary}. 

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|rr|rr}
                                  & Posts & Comments & Total\\ \hline
\# mentioning Wikipedia           &  335,897         &  10,264,340   & 10,600,237 \\
\# with Wikipedia links           &  286,359         &  9,465,316    &  9,751,675 \\
Total \# of Wikipedia links       &  658,493         &  11,573,36    & 12,231,860 \\
\# unique Wikipedia links         &  295,439         &  1,890,497    &  1,954,003 \\
\# unique Wikipedia articles      &  252,846         &  1,196,494    &  1,260,479
\end{tabular}}
\caption{A summary of all Wikipedia mentions, those including links, and those that map to Wikipedia articles by Reddit format.}
\label{tab:linksummary}
\end{table}

\subsection{Wikipedia Data}

Wikipedia data is collected via the Wikipedia APIs using WikiToolkit.

\subsubsection{IDs and Redirects}

Wikipedia links typically link to an article name. We resolved this name, which might be outdated, or non-canonical, to the current canonical title (i.e., resolve redirects), collected the page ID, and collected all pages that redirect to the article canonical title. In cases where links are to a page ID or revision ID, we similarly gathered the appropriate page ID, canonical title, and redirects as appropriate (as previously indicated). These are stored in the \texttt{wiki\_ids}, \texttt{resolved\_redirects}, and \texttt{collected\_redirects} tables.

\subsubsection{Page Views}

Daily page view counts are collected for every article posted to Reddit $\pm10$ days from initially post/comment date (\texttt{created\_at}) and $\pm10$ days from last modified date (\texttt{updated\_at}, \texttt{last\_modified\_at}). Page views are collected for both the original posted page title and any canonical redirected title. These are stored in the \texttt{page\_views} table.

\subsubsection{Revisions}

For every article posted to Reddit all article revisions (IDs and timestamps) made $\pm10$ days from initially post/comment date (\texttt{created\_at}) and $\pm10$ days from last modified date (\texttt{updated\_at}, \texttt{last\_modified\_at}) are collected. In addition, the revision at the start of this time period is collected, regardless of when it was created (i.e., the state of the article -10 days from initial post/comment date). These are stored in the \texttt{revisions} table.

% \subsubsection{Article Topic}
% \textbf{NOT INCLUDING IN THIS VERSION}
% Article topic predictions\_ \cite{} and associated probabilities are stored in the \texttt{article\_topics} table.

\subsection{Wikidata Data}

For each Wikipedia article collected, we also collected their Wikidata identifier (if present), this is also stored in the \texttt{wiki\_ids} table. This allows for cross-referencing the Reddit and Wikipedia data with Wikidata's structured knowledge graph, as well as interlanguage concept resolution.

\subsection{Summary}

A summary of the database structure is provided in Table \ref{tab:sqldb}.
\begin{table*}[h!]
\centering
\resizebox{0.75\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
\textbf{Column Name} & \textbf{Type} & \textbf{Description} \\
\midrule
\multicolumn{3}{l}{\textbf{Table: \texttt{posts}}} \\
\texttt{subreddit\_id} & TEXT & The unique identifier for the subreddit. \\
\texttt{crosspost\_parent\_id} & TEXT & The ID of the original Reddit post if this post is a crosspost. \\
\texttt{post\_id} & TEXT & Unique identifier for the Reddit post. \\
\texttt{created\_at} & TIMESTAMP & The timestamp when the post was created. \\
\texttt{updated\_at} & TIMESTAMP & The timestamp when the post was last updated. \\
\texttt{language\_code} & TEXT & The language code of the post. \\
\texttt{score} & INTEGER & The score (upvotes minus downvotes) of the post. \\
\texttt{upvote\_ratio} & REAL & The ratio of upvotes to total votes. \\
\texttt{gildings} & INTEGER & Number of awards (gildings) received by the post. \\
\texttt{num\_comments} & INTEGER & Number of comments on the post. \\
\midrule
\multicolumn{3}{l}{\textbf{Table: \texttt{comments}}} \\
\texttt{subreddit\_id} & TEXT & The unique identifier for the subreddit. \\
\texttt{post\_id} & TEXT & The ID of the Reddit post the comment belongs to. \\
\texttt{parent\_id} & TEXT & The ID of the parent comment (if a reply). \\
\texttt{comment\_id} & TEXT & Unique identifier for the comment. \\
\texttt{created\_at} & TIMESTAMP & The timestamp when the comment was created. \\
\texttt{last\_modified\_at} & TIMESTAMP & The timestamp when the comment was last modified. \\
\texttt{score} & INTEGER & The score (upvotes minus downvotes) of the comment. \\
\texttt{upvote\_ratio} & REAL & The ratio of upvotes to total votes for the comment. \\
\texttt{gilded} & INTEGER & Number of awards (gildings) received by the comment. \\
\midrule
\multicolumn{3}{l}{\textbf{Table: \texttt{postlinks}}} \\
\texttt{post\_id} & TEXT & Unique identifier for the Reddit post. \\
\texttt{end\_processed\_valid} & INTEGER & Whether the extracted URL from the post resolves to a valid URL. \\
\texttt{end\_processed\_url} & TEXT & The extracted URL from the Reddit post. \\
\texttt{final\_valid} & INTEGER & Whether the final URL from the post resolves to a valid URL after any redirections. \\
\texttt{final\_status} & INTEGER & HTTP status code of the final URL. \\
\texttt{final\_url} & TEXT & The final URL after any redirections. \\
\texttt{redirected} & INTEGER & Indicator of whether the posted URL was redirected (1) or not (0). \\
\texttt{in\_title} & INTEGER & Indicator of whether the link appears in the post title (1) or post body (0). \\
\midrule
\multicolumn{3}{l}{\textbf{Table: \texttt{commentlinks}}} \\
\texttt{comment\_id} & TEXT & Unique identifier for the Reddit comment. \\
\texttt{end\_processed\_valid} & INTEGER & Whether the extracted URL from the comment resolves to a valid URL. \\
\texttt{end\_processed\_url} & TEXT & The extracted URL from the comment. \\
\texttt{final\_valid} & INTEGER & Whether the final URL from the comment resolves to a valid URL after any redirections. \\
\texttt{final\_status} & INTEGER & HTTP status code of the final URL. \\
\texttt{final\_url} & TEXT & The final URL after any redirections. \\
\texttt{redirected} & INTEGER & Indicator of whether the URL was redirected (1) or not (0). \\
\midrule
\multicolumn{3}{l}{\textbf{Table: \texttt{linkarticles}}} \\
\texttt{final\_url} & TEXT & The final URL after any redirections. \\
\texttt{lang} & TEXT & The language code of the page. \\
\texttt{mobile} & INTEGER & Indicator of whether the link was mobile-specific (1) or not (0). \\
\texttt{raw\_title} & TEXT & The raw, unprocessed title text extracted from the link. \\
\midrule
\multicolumn{3}{l}{\textbf{Table: \texttt{resolved\_redirects}}} \\
\texttt{lang} & TEXT & The language code of the Wikipedia page. \\
\texttt{raw\_title} & TEXT & The raw title of the from the Wikipedia link before redirection. \\
\texttt{norm\_title} & TEXT & The normalized raw title of the page. \\
\texttt{canonical\_title} & TEXT & The canonical title after resolving the redirect. \\
\midrule
\multicolumn{3}{l}{\textbf{Table: \texttt{collected\_redirects}}} \\
\texttt{lang} & TEXT & The language code of the Wikipedia page. \\
\texttt{canonical\_title} & TEXT & The canonical title of the page. \\
\texttt{other\_title} & TEXT & Other titles associated with the page that redirect to the canonical title. \\
\midrule
\multicolumn{3}{l}{\textbf{Table: \texttt{wiki\_ids}}} \\
\texttt{lang} & TEXT & The language code of the Wikipedia page. \\
\texttt{title} & TEXT & The title of the Wikipedia page. \\
\texttt{pageid} & INTEGER & Unique identifier for the page in Wikipedia. \\
\texttt{wikidata\_id} & TEXT & The Wikidata identifier for the page. \\
\midrule
\multicolumn{3}{l}{\textbf{Table: \texttt{pageviews}}} \\
\texttt{lang} & TEXT & The language code of the Wikipedia page. \\
\texttt{title} & TEXT & The title of the Wikipedia page (not strictly the canonical title). \\
\texttt{date} & TIMESTAMP & The date of the page view count. \\
\texttt{pageviews} & INTEGER & The number of page views on the given date. \\
\midrule
\multicolumn{3}{l}{\textbf{Table: \texttt{revisions}}} \\
\texttt{lang} & TEXT & The language code of the Wikipedia page. \\
\texttt{canonical\_title} & TEXT & The canonical title of the Wikipedia page. \\
\texttt{revid} & INTEGER & The unique revision identifier. \\
\texttt{parentid} & INTEGER & The ID of the parent revision. \\
\texttt{timestamp} & TEXT & The timestamp of the revision. \\
\bottomrule
\end{tabular}%
}
\caption{SQL Database Schema with all tables. All IDs from Reddit are hashed using SHA-256 for anonymization.}
\label{tab:sqldb}
\end{table*}

\section{Ethical and FAIR Considerations}

This work was approved by the University of Exeter ethical review procedures (ID 8969882). The dataset is based on public data collected via the Reddit4Researchers and Wikipedia APIs. This research was observational, and no personally identifiable information from Reddit users was included. Data is limited to publicly accessible posts and comments containing Wikipedia links, excluding private and NSFW subreddits. The dataset conforms to the FAIR principles \cite{wilkinson_fair_2016} as follows.
\begin{itemize}
    \item \textbf{Findable}: The dataset is publicly available on Zenodo and is assigned a permanent DOI: 10.5281/zenodo.14653265
    \item \textbf{Accessible}: Anyone with an internet connection can freely access the dataset, which is shared under a licensing agreement that ensures long-term availability and responsible use.
    \item \textbf{Interoperable}: The dataset is provided in SQLite3 format and includes multilingual and cross-platform identifiers to enhance compatibility and facilitate integration with other tools and datasets.
    \item \textbf{Re-usable}: The dataset comes with replication and demo code for data collection, and exploratory analysis, making it easy for researchers to reproduce or extend analyses. The dataset is licensed CC BY 4.0.
\end{itemize}
This dataset could be misused by researchers attempting to make claims about Wikipedia's political effects on other platforms as a basis for erroneously discrediting the Wikipedia platform, which has recently faced increased politicization \citep{rascouet-paz_elon_2024}. We rely on the academic community to audit and resist bad faith usage of this dataset. Users do not explicitly consent to data collection because their posts are made on a public platform---although researchers have noted that some users, nonetheless, prefer to have their work not used for research \citep{fiesler_participant_2018}. Because this dataset online includes IDs linking to usernames, posts, and comments, users can effectively remove their data from future re-collection and use by deleting their associated data on Reddit.

\section{Exploratory Analysis}

\subsubsection{Reddit \& Wikipedia Use Over Time}

The histograms in Figure \ref{fig:posts_comments_score_distribution} and line plots in Figures \ref{fig:posts_comments_over_time} and \ref{fig:posts_comments_score_over_time} document the distribution of Reddit posts or comments that mention Wikipedia either by name or with an article link over time. The long tail distributions of Fig. \ref{fig:posts_comments_score_distribution} indicates that the vast majority of posts and comments have a score $\approx 0$, whereas a small number prove to be very high scoring. Unsurprisingly, since they are more readily presented to more Reddit users, posts tend to score more highly than comments.

When we analyze the distributions longitudinally, we find that Wikipedia is referenced about 8000 times per day in comments and that number has remained relatively stable over time. Original posts that refer to Wikipedia have decreased slightly over time going from approximately 300 posts per day to around 200 posts per day. While not fully answerable solely with our dataset, we hypothesize that these shifts might be related to changes in platform content moderation, engagement preferences with both platforms (e.g., mobile vs laptop) or other technological affordances (e.g., Reddit’s transition to more image and video-based participation).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/posts_comments_score_distribution.pdf}
    \caption{Histograms for the Reddit score of the posts and comments that mention Wikipedia (in text or as a link).}
    \label{fig:posts_comments_score_distribution}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/posts_comments_over_time.pdf}
    \caption{Plot showing the daily count of posts and comments that mention Wikipedia (in text or as a link) over 2020-2023.}
    \label{fig:posts_comments_over_time}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/posts_comments_score_over_time.pdf}
    \caption{Plot showing the daily average Reddit score of the posts and comments that mention Wikipedia (in text or as a link) over 2020--2023.}
    \label{fig:posts_comments_score_over_time}
\end{figure}

\subsubsection{Wikipedia Activity}

Information from Wikipedia being posted to Reddit may be an indicator of what kinds of information people are paying most attention to. Topics of news attention often play out on Wikipedia \cite{gildersleve_between_2023}, but these attention patterns---sometimes together with the Wikipedia articles to help contextualize the events---also move throughout the internet. Much like the \citet{gozzi_collective_2020} findings, our data indicates that people trying to make sense of news as it unfolds go between both platforms. This may be reflected in the Wikipedia page view and editing patterns of the posted articles. The posting of articles on Reddit may itself also drive interest and editing activity towards Wikipedia.

\textbf{Page views:} As an indication of these effects, we calculate and plot (Fig. \ref{fig:pageview_change_violin}) the relative values of daily page views the day an article is posted and week after it is posted as compared to the week before it is posted ($\frac{\text{Views on date}}{\text{Mean views in week before}}$ and  $\frac{\text{Mean views in week after}}{\text{Mean views in week before}}$, where the views on the date of posting are not included in either of the other ranges). 

For page views, we first deal with 0 counts, typically indicating an article doesn’t yet exist or is deleted by removing them from analysis. We then compute the geometric mean of the relative values, since the distribution is extremely long-tailed. We find that for links in Reddit posts, we observe a 45\% increase in page views on the day of posting, and a 6\% increase in the week after posting, as compared to the week before posting. For links in Reddit comments, we observe a 45\% increase in page views on the day of posting, and a 5\% increase in the week after posting, as compared to the week before posting.

The similarity between posts and comments in Figure \ref{fig:pageview_change_violin} would suggest much of this activity is due to some external stimulus, as one would assume the level of attention to posts vs comments, and the subsequent spillovers to Wikipedia, would be different. This indicates that audiences are not passive news receivers, but consult different online platforms to support their comprehension of current events. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/pageview_change_violin.pdf}
    \caption{Figure showing the daily page views to Wikipedia articles on the day of posting and in the week after posting relative to the week before posting. A small number of points in the extremes of the distributions are cut for visual clarity.}
    \label{fig:pageview_change_violin}
\end{figure}

\textbf{Edits:} The picture regarding edits is less clear. This is partially due to the fact that many of the linked articles receive no edits in the days before / on / after being posted on Reddit, in line with the findings of \citet{vincent_examining_2018}. Nevertheless, we do see some indications of increased editing activity in the wake of being posted. We first remove cases where the pages receive no edits over the full time period, then, due to the smaller, discrete edit values, consider the absolute change in number of daily edits. We compare the number of daily edits on the day of posting on Reddit and in the week after posting against that in the week before posting. Considering the arithmetic mean, for links in Reddit posts, we observe an increase of 0.462 daily edits on the day of posting, but a decrease of 0.031 daily edits in the week after posting, as compared to the week before posting. For links in Reddit comments, we observe an increase of 0.127 daily edits on the day of posting, but a decrease of 0.022 daily edits in the week after posting, as compared to the week before posting. The relatively small effect size here, plus high levels of zero-inflation, and the presence of extreme outliers warrants more rigorous analysis.

To be clear, we present this analysis as a demonstration of association, rather than an investigation of any causal relationship between Reddit and Wikipedia cross-posting. When there is an association, it is likely in the majority of cases that the Reddit posting and page view / editing behavior is mostly in response to some common external stimuli (see discussion of Fig. \ref{fig:pageview_change_violin}). Studies interrogating any causal relationships are most welcome to be performed using the dataset, making the appropriate decisions for research design, data subsetting, and controls.

\subsubsection{Linking by Language}

Both Reddit and Wikipedia are used in different languages. Regarding the use of Wikipedia links on the social media platform, activity is very much English language dominated. 95.8\% of Reddit posts with Wikipedia links in the dataset are in English and 93.9\% of all linked articles are to English Wikipedia, with the next most links being to German, French, and Spanish Wikipedias (Fig. \ref{fig:lang_proportion}). However, there is substantial cross-lingual linking. 50.85\% of links to non-English Wikipedias come from English language Reddit posts. It is also notable how frequently non-English language Reddit posts link to the English language Wikipedia (Fig. \ref{fig:en_proportion_vs_total}). The most active other languages are less likely to link to English Wikipedia. An interesting dynamic is observed for Esperanto (eo), the artificially constructed international second language. Esperanto Wikipedia exists, but Esperanto users on Reddit almost exclusively use English Wikipedia—presumably their first language.

The use of non-English Wikipedia articles in English Reddit posts may highlight knowledge gaps on the English Wikipedia, or be used to compare accounts in different language editions. The extensive use of English Wikipedia in other language Reddit posts strongly highlights (perceived) knowledge gaps in these languages. Unfortunately, it is likely that this linking behavior weakens the already fragile pipeline of contributions coming from social media, lessening the likelihood of edits being made to the Wikipedia language editions in need of them. These results, and future work, can have wide-ranging implications for the multilingual state of Wikipedia and the web. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/lang_proportion.pdf}
    \caption{Figure showing the proportion of links to each Wikipedia language subdomain for the most frequently occurring languages in the dataset.}
    \label{fig:lang_proportion}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/en_proportion_vs_total.pdf}
    \caption{Figure showing the proportion of links to English Wikipedia from non-English Reddit posts vs the total number of links in that post language.}
    \label{fig:en_proportion_vs_total}
\end{figure}

\section{Conclusion and Future Work}

We present WikiReddit, a comprehensive dataset capturing all Wikipedia mentions (including links) shared in posts and comments on Reddit from 2020 to 2023, excluding those from private and NSFW (not safe for work) subreddits. The SQL database comprises 336K total posts, 10.2M comments, 1.95M unique links, and 1.26M unique articles spanning  59 languages on Reddit and 276 Wikipedia language subdomains. 
Our exploratory analysis reveals Wikipedia is being referred to in Reddit posts less frequently over the period of study, but the performance of those posts remains stable. We also find notable associations between activity on Reddit and Wikipedia; article page views tend to increase on the day the link posts to Reddit and continue, less markedly for a week after. However, the relationship is much weaker for editing activity. Finally, English is unsurprisingly the dominant language in the Reddit posts and comments, which is reflected in the Wikipedia links posted. However, there is substantial cross-lingual linking to and from English.

We foresee several applications of this dataset and preview four here. First, Reddit linking data can be used to understand how attention is driven from one platform to another. Wikipedia, somewhat uniquely among widely-used internet platforms, publishes all of its traffic data for each of its articles on an hourly basis \citep{wikimedia_foundation_page_2024}, facilitating fine-grained analyses of how, for example, Reddit posts and comments direct attention to the encyclopedia. This analysis of cross-platform attention to Wikipedia is particularly worthwhile during times of crisis, where viral traffic driven by social media linking can stress the response capabilities of Wikipedia's volunteer editing community \citep{avieson_breaking_2019,avieson_editors_2022}. 

Second, Reddit linking data can shed light on \textit{how} Wikipedia's archive of knowledge is used in the larger social web. While we have ample research on the uses of Wikipedia within the Wikipedia platform \citep{singer_why_2017}, and tentative research on the on-platform misuses of Wikipedia \citep{saez-trumper_online_2019,kharazian_governance_2024}, we know less about how Wikipedia is used or misused off-platform. Prior research gives us reason to believe that even trustworthy content can be recontextualized outside of its authors' intent, unwittingly supporting disinformation campaigns \citep{beers_selective_2023}. Datasets like these can help us understand how the intellectual work of Wikipedia editors travel, and potentially inform how to make Wikipedia's information resilient to malicious decontextualization. 

Third, our dataset could provide insights into how external attention is topically distributed across Wikipedia. Many have observed biases both who views Wikipedia and how Wikipedia is edited \cite{johnson_global_2021,tripodi_ms_2023,menking_people_2019}. Our dataset can help extend that analysis into the disparities in what types of external communities Wikipedia is used in, and how it is used.

Fourth, a topic analysis of our dataset could reveal how Wikipedia usage on Reddit contributes to societal benefits and harms. Reddit has taken steps in recent years to reform its content moderation to address documentation of vitriolic discourse (see \citet{farrell2019exploring} or \citet{massanari__2017} for earlier examples of sexism/racism). However, both communities still predominantly consist of white men \citep{gilbert_i_2020, menking_people_2019}, a demographic imbalance that may contribute to larger inequities in information \citep{tripodi_ms_2023}. Our dataset could help examine if homogeneity within these groups shapes topic patterns and assess whether these relationships mitigate or amplify problematic engagement online.

There are some limitations to what can be done with this dataset. Some analyses of WikiReddit, such as those that recover the full text of Reddit comments, required continued access to the Reddit4Researchers API platform. While Reddit has shown a positive stance towards data access for external researchers via is Reddit4Researchers program, we must be cautious in what has been dubbed the ``post-API age'' of limited data access from private companies \citep{freelon_computational_2018}. Researchers should also note that URL-sharing is not the only method by which Wikipedia content is shared across platforms like Reddit. For example, text screenshots of external content, previously implicated in the spreading is misinformation \citep{matatov_stop_2022}, will not be captured here, and surely represent some of the external representation of Wikipedia on Reddit. We thus caution that any full census of Wikipedia's external usage is partial if accessed solely through the in-text mentions and URL-sharing included in this dataset. Nevertheless, the WikiReddit dataset represents an important step forward in understanding how Wikipedia is referenced and engaged with across a major social media platform, and provides a reliable resource for long-term study.

\bibliography{aaai25}

\end{document}
