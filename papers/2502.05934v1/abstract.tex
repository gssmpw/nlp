Under what conditions can capable AI agents efficiently align their actions with human preferences? 
More specifically, when they are proficient enough to collaborate with us, how long does coordination take, and when is it computationally feasible? 
These foundational questions of AI alignment help define what makes an AI agent ``sufficiently safe'' and valuable to humans. 
Since such generally capable systems do not yet exist, a theoretical analysis is needed to establish when guarantees hold---and what they even are.

We introduce a game-theoretic framework that generalizes prior alignment approaches with fewer assumptions, allowing us to analyze the computational complexity of alignment across $M$ objectives and $N$ agents, providing both upper and lower bounds.
Unlike previous work, which often assumes common priors, idealized communication, or implicit tractability, our framework formally characterizes the difficulty of alignment under minimal assumptions.

Our main result shows that even when agents are fully rational and computationally \emph{unbounded}, alignment can be achieved with high probability in time \emph{linear} in the task space size. 
Therefore, in real-world settings, where task spaces are often \emph{exponential} in input length, this remains impractical.
More strikingly, our lower bound demonstrates that alignment is \emph{impossible} to speed up when scaling to exponentially many tasks or agents, highlighting a fundamental computational barrier to scalable alignment.

Relaxing these idealized assumptions, we study \emph{computationally bounded} agents with noisy messages (representing obfuscated intent), showing that while alignment can still succeed with high probability, it incurs additional \emph{exponential} slowdowns in the task space size, number of agents, and number of tasks.

Thus, our results show that even when agents are highly capable and motivated to cooperate, alignment is intrinsically constrained by the exponential complexity of the task space, number of agents, and number of tasks. 
We provide explicit algorithms for our general alignment protocols and identify specific conditions where it can be more feasible, involving strategies that either sidestep task space explosion or employ scalable mechanisms for coordination.