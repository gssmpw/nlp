\section{Appendix: Proofs of Theorem~\ref{thm:bounded} and Corollary~\ref{cor:wannabe-agree}}
\label{app:proofs}

Here we prove both Theorem~\ref{thm:bounded} and Corollary~\ref{cor:wannabe-agree}.
We do this by generalizing Aaronson's computational-boundedness treatment~\citep[\S 4]{aaronson2005complexity} from 2 agents to $N$ agents (specifically, $N-q$ agents and $q$ humans that have differing, rather than equal, query costs) and $M$ functions (rather than 1), using a message-passing protocol that combines his smoothed and spanning tree protocols, all \emph{without} the Common Prior Assumption (CPA).
\subsection{Message-Passing Protocol}
\label{app:msp-protocol}
This is the multi-task generalization of Aaronson's~\citep[\S 4.1]{aaronson2005complexity} ``smoothed standard protocol'', additionally extended to the multi-agent setting a spanning tree the agents use to communicate their messages.

Let $b_j=\lceil\log_2(\tfrac{C}{\eps_j})\rceil$ be a positive integer we will specify later with a specific constant value $C > 0$, in \eqref{eq:total-bayesian}.
The $N$ computationally bounded agents follow the \agree-agreement algorithm (Algorithm~\ref{alg:agree}), passing $O(b_j)$-bit messages according to the following protocol $\mathcal{P}$:

\textbf{Protocol $\mathcal{P}$ description (for each task \(j\in \M\)):}
\begin{enumerate}
\item \textbf{Current posterior expectation.}
  The \emph{sending agent} $i\in\N$ has at timestep $t-1$ a real value $E^{i,t-1}_j(s_j) \in [0, 1]$, which is its conditional expectation of $f_j \in [0,1]$ given its knowledge partition $\Pi_j^{i,t-1}(s_j)$ and the current task state $s_j \in S_j$.
  (Recall that $E^{i,t-1}_j(s_j) := \Eji{f_j \mid \Pi_j^{i,t-1}(s_j)}$.)
  The knowledge partition $\Pi_j^{i,t-1}(s_j)$ is formed after updating this expectation using Bayes' rule, after having received the earlier message at time $t-2$.

\item \textbf{Draw an integer $r_j$ via a triangular distribution.}
  Agent $i$ picks an integer offset
  \begin{equation*}
    r_j \;\;\in\;\; \{-L_j,\,-L_j+1,\,\dots,\,L_j\}
  \end{equation*}
  according to a (continuous) triangular distribution $\Delta_{\mathrm{tri}}(\,\cdot\,;\,\alpha_j)$ that places its mass in the \emph{discrete} set of values $\{-L_j,\dots,L_j\}$, and has effective width $2\alpha_j$.
  These discrete offsets $r_j$ ensure that the messages will be discrete as well.
  Concretely,
  \begin{equation*}
    \mathbb{P}[\,r_j = x\,] = \frac{L_j - |x|}
       {\displaystyle \sum_{z=-L_j}^{L_j}\bigl(L_j - |z|\bigr)} = \frac{\,L_j - |x|\,}
       {L_j^2}
  \quad
  \text{for } x \in \{-L_j,\dots,L_j\},
  \end{equation*}
  where $L_j$ is chosen so that $2^{-b_j}L_j = \alpha_j$ to bound the messages, as explained in the next step below.
  Note that the form above is chosen so that the ``peak'' of the discretized triangular distribution is at $r_j = 0$.
  In other words, the form above is maximized in probability when the offset $r_j = 0$ (which means that no noise is added to the messages with the highest probability).

\item \textbf{Form the message with noise.}
  The agent then sets
  \begin{equation*}
     m^t_j\Bigl(\,\Pi_j^{i,t-1}(s_j)\Bigr)
     \;=\;
     \mathrm{round}\left(E^{i,t-1}_j(s_j)\right)
     \;+\;
     2^{-b_j}r_j,
  \end{equation*}
  where $\mathrm{round}\left(E^{i,t-1}_j(s_j)\right)$ denotes rounding $E^{i,t-1}_j(s_j)$ to the nearest multiple of $2^{-b_j}$ (thereby keeping it in the $[0,1]$ interval).
  This ensures that the message $m_j^t$ is itself a multiple of $2^{-b_j}$ (thereby being encodable in $O(b_j)$ bits), and is offset by $\pm \alpha_j$ from $\mathrm{round}\bigl(E^{i,t-1}_j(s_j)\bigr)$, since $|2^{-b_j}r_j| \le 2^{-b_j}L_j = \alpha_j$, by construction.
  Hence, each $m^t_j \in [-\alpha_j, 1+\alpha_j]$.

\item \textbf{Broadcast.}
  This message $m^t_j\bigl(\Pi_j^{i,t-1}(s_j)\bigr)$ is then broadcast (either sequentially or in parallel) to the relevant agents via an edge of the two spanning trees ${SP}^1_j\cup{SP}^2_j$ each of diameter $g_j$, just as in Lemma~\ref{lem:spanning-tree-refinement}, who update their knowledge partitions accordingly to Step 1.
\end{enumerate}

\subsection{Sampling‐Tree Construction and Simulation for Each Task \texorpdfstring{$j\in\M$}{}}
\label{app:sampling-tree}
In our framework, each agent logically refines its knowledge partition $\{\Pi_j^{i,t}(s_j)\}_{s_j \in S_j}$ upon seeing a new message (Line 8 of Algorithm~\ref{alg:agree}).
However, given that the agents are computationally bounded, while refinement is allowed, the issue is with their belief updating.
By Requirement~\ref{req:bounded-cap}, they have no direct ability to sample from the \emph{conditioned} posterior distributions $\tau^{i,t}_j = \prob^i_j(\cdot \mid \Pi_j^{i,t}(s_j))$ at run time, in order to compute the expectation in Step 1 of the protocol in \S\ref{app:msp-protocol}.
In other words, they cannot simply call ``$\mathrm{Sample}\bigl(\prob^i_j\mid \Pi_j^{i,t}(s_j)\bigr)$'' in a black‐box manner. 
Thus, \emph{before} any messages are exchanged, each agent constructs a sampling tree $\mathcal{T}^{i}_{j}$ offline of \emph{unconditional} samples from the priors $\mathbb{P}^{i}_{j}$ (which they are able to do by Subroutine 2 in Requirement~\ref{req:bounded-cap}).
The idea is to precompute enough unconditional draws so that each new message can be \emph{simulated} via ``walking down'' the relevant path in the tree that is consistent with the current message history (including that new message), rather than enumerating or sampling from the newly refined partition directly.

That is the intuition. 
We now explain in detail how each agent can use sampling trees to simulate this protocol in a computationally bounded manner. 
This follows Aaronson's approach~\citep[\S 4.2]{aaronson2005complexity} of dividing the simulation into two phases---\emph{(I)~Sampling‐Tree Construction} (no communication) 
and \emph{(II)~Message‐by‐Message Simulation}---but extended here to our 
multi‐task and multi-agent setting.

\begin{enumerate}
\item[(I)] \textbf{Sampling‐Tree Construction (General $N$‐Agent Version).}
For each task $j\in\M$, and for each agent $i\in \N$, that agent $i$ builds a sampling tree $\mathcal{T}^{\,i}_{j}$ of height $R_j$ and branching factor $B_j$. 
We fix an ordering of the $O\left(R_j\right)$ messages for task~$j$ (so that we know which agent is ``active'' at each level).
Formally, let $\mathrm{ActiveAgent}_j:\{0,\dots,R_j-1\}\to \N$ denote the function specifying which agent sends the message at each round $\ell=0,\dots,R_j-1$. 
For instance, since the spanning tree protocol cycles through the $N$ agents in a consistent order, then $\mathrm{ActiveAgent}_j(0)=i_0$, $\mathrm{ActiveAgent}_j(1)=i_1,\dots$, and so on, up to $R_j$ total rounds.

\begin{itemize}
\item 
  Let $\operatorname{root}^{\,i}_{j}$ be the root node of $\mathcal{T}^{\,i}_{j}$. 
  We say that the \emph{level} of the root node is $0$, its children are at level~$1$, grandchildren at level~$2$, etc., until depth $R_j$.  
  Each node will be labeled by a sample in task $j$'s state space $S_j$, drawn from whichever agent's \emph{unconditional} prior distribution $\prob^k_j(\cdot)$ is active at that level.

\item 
  Concretely, for $\ell=0,\dots,R_j-1$:
  \begin{enumerate}
  \item 
    Let $a_j := \mathrm{ActiveAgent}_j(\ell)$ 
    be the agent whose distribution we want at level~$\ell$ (i.e.\ the agent who sends the $\ell$‐th message). 
    \item 
    Every node $v_j$ at \emph{level~$\ell$} is labeled by some previously chosen sample (if $\ell=0$ and $v_j$ is the root, we can label it trivially or treat $i$'s perspective by a \texttt{do‐nothing} step).  
    \item 
    Each of the $B_j$ children $w\in Children(v_j)$ at level~$\ell+1$ 
    is labeled with a fresh i.i.d.\ sample drawn from $\prob^a_j(\cdot)$, i.e. from the unconditional posterior of agent $a_j$. 
    \item 
    We continue until level~$R_j$ is reached, yielding a total of 
    \begin{equation}\label{eq:sampling-tree-draws}
      B_j + B_j^2 + \cdots + B_j^{R_j} = \frac{{B_j}^{R_j+1}-1}{B_j-1}-1 = O\left(B_j^{R_j}\right)
    \end{equation}
    newly drawn states from the unconditional prior distributions $\{\prob^k_j\}_{k\in \N}$ at the appropriate levels.
  \end{enumerate}
\end{itemize}

Thus, node labels alternate among the $N$ agents' unconditional draws,  depending on which agent is active at each level (timepoint in message history) $\ell$ in the eventual message sequence for task $j$. 
All of this is done \emph{offline} by \emph{each} agent, requiring no communication among the agents. 
Once constructed, each agent $i$ holds $\mathcal{T}^i_j$ for personal use.

\item[(II)] \textbf{Message‐by‐Message Simulation.}
After building these sampling‐trees $\{\mathcal{T}^i_j\}_{i\in\N}$ (for each task $j$), the $N$ agents enact the protocol in \S\ref{app:msp-protocol} in real time, \emph{but} whenever an agent $i$ needs to ``update its posterior'' after receiving a message, it does \emph{not} sample from $\tau^{i,t}_j = \prob^i_j(\cdot \mid \text{new messages}).$ 
Instead, it uses the precomputed nodes in $\mathcal{T}^i_j$ as follows:

\begin{itemize}
\item \textbf{Initial estimate.}
  At time $t=0$, agent~$i$ approximates $\displaystyle E^{\,i,0}_j(s_j)$ (its prior‐based expectation of $f_j$) by an empirical average of the $B_j$ children of the root node $\operatorname{root}^{\,i}_j \in \mathcal{T}^i_j$. 
  This becomes agent~$i$'s \emph{initial posterior} 
  in the \emph{offline} sense.

\item \textbf{At each round $t=1,\dots,R_j$:}
  \begin{enumerate}
  \item The agent who is ``active'' (i.e.\ is about to send the $t$‐th message) consults \emph{its} sampling‐tree, summing over the relevant subtree that corresponds to the newly received messages 
    $m_j^1,\dots,m_j^{t-1}$, so as to approximate $E^{\,i,t-1}_j(s_j)$.
  \item It picks an integer offset $r_j\in\{-L_j,\dots,L_j\}$ via the discrete triangular distribution (defined in \S\ref{app:msp-protocol}), and then sends the $t$‐th message:
    \begin{equation*}
       m^t_j = \mathrm{round}\bigl(\langle E^{\,i,t-1}_j(\cdot)\rangle_i\bigr) + 2^{-b_j}\,r_j.
    \end{equation*}
  \item The other agents, upon receiving $m^t_j$ via the spanning tree protocol, update node‐weights in their own sampling trees (via the $\Delta$-update equations in \citep[\S4.2]{aaronson2005complexity}).
  This effectively ``follows'' the branch in their sampling trees consistent with $m^t_j$ so they approximate $\Pi^{i,t}_j(\cdot)$ \emph{without} enumerating or sampling from the \emph{conditioned} distribution.
  \end{enumerate}
\end{itemize}
After all $R_j$ messages for task $j$, the agents will have 
approximated the ideal protocol in \S\ref{app:msp-protocol} with high probability (assuming $B_j$ was chosen large enough, which we will give an explicit value for below in the large‐deviation analysis in \S\ref{app:runtime}).
\end{enumerate}

\begin{lemma}[Sampling Tree Time Complexity]\label{lem:sampling-tree-time}
For each task $j$, the time complexity of the sampling tree for all $N$ agents is
\begin{equation}\label{eq:spanning-tree-time}
O\left(B_j^{R_j}\left(q\,T_{\text{sample},H}
               + (N-q)\,T_{\text{sample},AI}
               + q\,T_{\text{eval},H}
               + (N-q)\,T_{\text{eval},AI}\right)\right).
\end{equation}
\end{lemma}
\begin{proof}
As described, we now assume there are $N$ agents, among which $q$ are humans and $N-q$ are AI agents, each potentially taking different times for evaluating $f_j(\cdot)$ or sampling from the priors (Subroutines 1 and 2 of Requirement~\ref{req:bounded-cap}, respectively):
\begin{equation*}
  T_{\text{eval},H},\quad T_{\text{eval},AI},\quad
  T_{\text{sample},H},\quad T_{\text{sample},AI}.
\end{equation*}
Specifically, a \emph{human} agent $i\in H$ uses time $T_{\text{eval},H}$ to evaluate $f_j(s_j)$ for a state $s_j\in S_j$, and time $T_{\text{sample},H}$ to sample from \emph{another} agent's prior distribution $\prob^k_j(\cdot)$ unconditionally; whereas an \emph{AI} agent $i\in AI$ spends $T_{\text{eval},AI}$ and $T_{\text{sample},AI}$ on the same operations.

\textbf{Sampling Overhead.} By \eqref{eq:sampling-tree-draws}, we do $O\left(B_j^{R_j}\right)$ unconditional draws in total \emph{per agent}.
  Each draw is performed by the \emph{agent building the tree}, but it might sample from \emph{another} agent's distribution.
  Hence the time cost per sample is
  \begin{equation*}
    \begin{cases}
      T_{\text{sample},H}\quad &\text{if the sampling agent is human},\\[4pt]
      T_{\text{sample},AI}\quad &\text{if the sampling agent is an AI}.
    \end{cases}
  \end{equation*}
  We separate the $O\left(q\,B_j^{R_j}\right)$ samples by humans vs. $O\left((N-q)\,B_j^{R_j}\right)$ samples by the AI agents, yielding
  \begin{equation*}
  O\left(q\,B_j^{R_j}\,T_{\text{sample},H} + (N-q)\,B_j^{R_j}\,T_{\text{sample},AI}\right).
  \end{equation*}

\textbf{Evaluation Overhead.} Each sampled state $s_j\in S_j$ may require computing $f_j(s_j)$.  Again, if the \emph{labeling} agent is a human, that cost is $T_{\text{eval},H}$, whereas if an AI agent is performing the labeling, it is $T_{\text{eval},AI}$.
  Consequently,
  \begin{equation*}
    O\left(q\,B_j^{R_j}\,T_{\text{eval},H} + (N-q)\,B_j^{R_j}\,T_{\text{eval},AI}\right)
  \end{equation*}
  suffices to bound all function evaluations across all task $j$ sampling‐trees $\{\mathcal{T}^i_j\}_{i\in\N}$.

\textbf{During the Actual $R_j$ Rounds.} Once messages start flowing, each round requires partial sums or lookups in the prebuilt tree $\mathcal{T}^i_j$.
If agent $i$ is a human in that round, each node update in the subtree will cost either $T_{\text{eval},H}$ or $T_{\text{sample},H}$ (though typically we do not \emph{resample}, so it may be just function evaluations or small indexing).
Since the total offline overhead still dominates, summing over $R_j$ rounds still yields an overall $O\left(B_j^{R_j}\,(T_{\text{sample},\cdot} + T_{\text{eval},\cdot})\right)$ bound, substituting the index $\{\text{H,AI}\}$ depending on the category of the agent.
\newline

Summing the above together gives us the stated upper bound in \eqref{eq:spanning-tree-time}.
\end{proof}

\subsection{Runtime Analysis}
\label{app:runtime}
Recall that the $N$ agents follow the algorithm for \agree-agreement (Algorithm~\ref{alg:agree}), which is a ``meta-procedure'' that takes in any message protocol we have discussed thus far (specifically, the one above).
We now want the agents to communicate with enough rounds $R_j$ such that they can feasibly construct a common prior consistent with their current beliefs, with high probability (Line 10 of Algorithm~\ref{alg:agree}).
We now have to bound $R_j$.

Recall that in the sampling‐tree scenario, agents now no longer have \emph{exact} access to each other's posterior distributions, but rather approximate them by offline sampling---thus they cannot do a \emph{direct, immediate} conditional update (as they could in the unbounded case). 
Indeed, triangular noise does \emph{not} strictly \emph{mask} a surprising message; rather, each agent still \emph{can} receive an improbable message from its vantage. 
However, because these posteriors are only approximate, we must invoke large‐deviation bounds to ensure that with high probability, a message deemed $\gamma$‐likely by the sender but $\leq\gamma/2$ by the \emph{neighboring} receiver 
\emph{actually} appears within some number of messages (in other words, they disagree), forcing a proper refinement in their knowledge partitions. 
We rely on a probabilistic argument that surprises still occur on a similar timescale as in the exact setting, with high probability, thereby prompting a partition refinement:
\begin{lemma}[Number of Messages for One Refinement Under Sampling Trees]
\label{lem:sample-tree-neighbor-msg-refinment}
Suppose two \emph{neighboring} agents $i$ and $k$ have not yet reached \agree-agreement.
Therefore, they disagree on some message $m_j^*$ as follows: % we need to assume this to bound the worst-case number of refinements, otherwise they would agree
\begin{equation}\label{eq:disagree-prob}
\Prob_{\substack{s_j\sim \prob^i_j,\\
    r_j \,\sim\,\Delta_{\mathrm{tri}}(\alpha_j)}}
  \Bigl[\mathrm{round}\bigl(E^{i,t-1}_j(s_j)\bigr)
    + 2^{-b_j}\,r_j \;=\; m_j^*\Bigr]
  \;\ge\;\gamma
\quad\text{(\emph{Agent $i$ sees $m_j^*$ as $\gamma$‐likely})},
\end{equation}
while
\begin{equation}\label{eq:disagree-prob2}
\Prob_{\substack{s'_j\sim \prob^k_j,\\
    r'_j \,\sim\,\Delta_{\mathrm{tri}}(\alpha_j)}}
  \Bigl[\mathrm{round}\bigl(E^{k,t-1}_j(s'_j)\bigr)
    + 2^{-b_j}\,r'_j \;=\; m_j^*\Bigr]
  \;\le\;\frac{\gamma}{2}
\quad\text{(\emph{Agent $k$ sees $m_j^*$ as $\gamma/2$‐likely or less}).}
\end{equation}
Then the probability of $m_j^*$ failing to be produced in $T = O\left(\frac{\ln\left(\mu_j\right)}{\ln\left(1/\alpha_j\right)}\right)$ consecutive rounds is at most $\mu_j$.
In other words, the neighboring agents will disagree (thereby triggering at least one proper refinement, namely, for agent $k$) with probability at least $1-\mu_j$ after $T = O\left(\frac{\ln\left(\mu_j\right)}{\ln\left(1/\alpha_j\right)}\right)$ consecutive rounds, for $\mu_j > 0$.
\end{lemma}
\begin{proof}
Note that \eqref{eq:disagree-prob} ensures $m_j^*$ has at least probability
$\gamma$ from the \emph{sender’s} perspective, so we can bound how quickly
$m_j^*$ is produced.
Specifically, by \citet[Lemma 15]{aaronson2005complexity}, we know that solely from the sender's vantage (and therefore not assuming a common prior), the probability of $m_j^*$ \emph{not} appearing after $T$ consecutive rounds is given by at most
\begin{equation*}
\lambda_j^{\,T/2}\max\Bigl\{\gamma,\tfrac{1}{B_j}\Bigr\},
\end{equation*}
where $\lambda_j := \frac{4\,e}{\alpha_j}\ln\left(B_j\right)$.
Therefore, it suffices for $T$ to be such that $\lambda_j^{\,T/2}\,\max\{\gamma,1/B_j\} \le \mu_j$. 
Isolating $T$ gives us:
\begin{equation*}
  \lambda_j^{T/2} \le \frac{\mu_j}{\max\{\gamma,\,1/B_j\}}
  \quad\Longrightarrow\quad
  \frac{T}{2}\ln\left(\lambda_j\right) \le \ln\left(\mu_j\right) - \ln\left(\max\{\gamma,\,1/B_j\}\right).
\end{equation*}
Hence
\begin{equation*}
T \le \frac{2\bigl[\ln\left(\mu_j\right) - \ln\left(\max\{\gamma,\,1/B_j\}\right)\bigr]}{\ln\left(\lambda_j\right)} = \frac{2\bigl[\ln\left(\mu_j\right) + \ln\left(1/\max\{\gamma,\,1/B_j\}\right)\bigr]}{\ln(4e) + \ln(1/\alpha_j) + \ln\ln\left(B_j\right)}.
\end{equation*}
As $\gamma$ is a free parameter, we can obtain a cleaner (albeit looser) bound on $T$ by subsuming suitably large constants.
A natural choice for $\gamma$ is $\gamma = \alpha_j$, since the added noise to each message is at most $\alpha_j < 1/40$ (via Step 3 in \S\ref{app:msp-protocol}) at each round.
Therefore, the maximum additive noise is also the natural threshold for a ``surprising'' message from the receiver's (agent $k$) point of view.
We will also choose the branching factor $B_j$ to be sufficiently large enough that $1/B_j \le \alpha_j$.
Thus, $\max\{\gamma,1/B_j\} = \alpha_j$.
Hence, for $B_j \ge 1/\alpha_j$, trivially $\ln\ln\left(B_j\right) > 0$, and we have that
\begin{equation*}
T \le \frac{2\bigl[\ln\left(\mu_j\right) + \ln\left(1/\alpha_j\right)\bigr]}{\ln(4e) + \ln(1/\alpha_j) + \ln\ln\left(B_j\right)} \le \frac{2\bigl[\ln\left(\mu_j\right) + \ln\left(1/\alpha_j\right)\bigr]}{\ln(1/\alpha_j)} = O\left(\frac{\ln\left(\mu_j\right)}{\ln\left(1/\alpha_j\right)}\right).
\end{equation*}
\end{proof}

We are now finally in a position to bound $R_j$.
\begin{lemma}[Common Prior Lemma Under Sampling Trees]
\label{lem:common-prior-sampling-tree}
Suppose $N$ have not yet reached \agree-agreement. 
They will reach a common prior $\CP_j$ with probability at least $1 - \delta$, after $R_j = O\left(N^2D_j\frac{\ln\left(\delta/(N^2D_j)\right)}{\ln\left(1/\alpha_j\right)}\right)$ rounds.
\end{lemma}
\begin{proof}
We recall from Lemma~\ref{lem:spanning-tree-refinement} that, \emph{if} every
agent had exact access to its posterior distribution, any block of
$O\left(g_j\right)$ messages in the two‐spanning‐trees ordering would pass each agent's ``current expectation'' to every other agent precisely once, guaranteeing that if a large disagreement persists, the \emph{receiving} agent sees an unlikely message and refines its partition.

In the \emph{sampled} (bounded) setting, each agent $i$ approximates its posterior by building an offline tree of $O\left(B_j^{R_j}\right)$ unconditional samples, labeling its nodes by unconditional draws from the respective priors $\{\prob^k_j(\cdot)\}$.
Then, when agent~$i$ must send a message---nominally its exact posterior---it instead \emph{looks up} that value via partial averages in its sampling‐tree.
This is done in a manner consistent with message alternation.
Specifically, in each round, the $\mathrm{ActiveAgent}_j$ function from \S\ref{app:sampling-tree} ensures that every agent's approximate expectation is routed along ${SP}^1_j$ and ${SP}^2_j$ (Step 4 of \S\ref{app:msp-protocol}) once in every block of $O\left(g_j\right)$ messages.

Now, in Lemma~\ref{lem:sample-tree-neighbor-msg-refinment}, we assumed the agents were neighbors.
Thus, in the more general case of $N$ agents, who communicate with spanning trees ${SP}^1_j\cup{SP}^2_j$ each of diameter $g_j$, if there is a ``large disagreement'' between some agent pair $(i,k)$, then from $(i\!\to\!k)$ or $(k\!\to\!i)$'s vantage, the other’s message is improbable.
In the worst case, the agents are $O(g_j)$ hops apart.
Hence, once that message arrives in these $O\left(g_j \frac{\ln\left(\mu_j\right)}{\ln\left(1/\alpha_j\right)}\right) = O\left(N \frac{\ln\left(\mu_j\right)}{\ln\left(1/\alpha_j\right)}\right)$ transmissions, the probability that the receiving agent still sees it as a \emph{surprise} and properly refines its partition is $\ge 1 - g_j\mu_j \ge 1 - N\mu_j$, by a union bound over hops.

These $O\left(N \frac{\ln\left(\mu_j\right)}{\ln\left(1/\alpha_j\right)}\right)$ transmissions constitute one ``block'' of messages that results in at least \emph{one} agents' refinement with high probability.
Finally, we will need $O(ND_j)$ refinements by Lemma~\ref{lem:common-prior} to reach a common prior.
Partition the total timeline into $X=O\left(N D_j\right)$ disjoint ``blocks,'' each block of $O\left(g_j\frac{\ln(\mu_j)}{\ln(1/\alpha_j)}\right)$ messages. 
Let $E_i$ denote the event that ``$\ge 1$ refinement occurs in block $i$''.  
By the single‐block argument, $\prob[E_i]\ge 1 - N\mu_j$.

We want $\prob\Bigl[\bigcap_{i=1}^X E_i\Bigr]$, the probability that \emph{all} $X$ blocks yield at least one refinement.
Using a union bound on complements,
\begin{equation*}
  \prob\Bigl[\bigcap_{i=1}^X E_i\Bigr] = 1 - \prob\Bigl[\bigcup_{i=1}^X E_i^c\Bigr] \ge 1 - \sum_{i=1}^X \prob\bigl[E_i^c\bigr] \ge 1 - X N \mu_j = 1 - N^2 D_j\mu_j.
\end{equation*}
Thus, after $X\times O\left(g_j\frac{\ln(\mu_j)}{\ln(1/\alpha_j)}\right) = O\left(N^2 D_j\frac{\ln(\mu_j)}{\ln(1/\alpha_j)}\right)$
rounds, we have converged to a common prior with probability at least 
$1 - N^2 D_j\mu_j$.
Setting $\delta := N^2 D_j\mu_j$ gives us the final result.
\end{proof}

Now that we have established that we can converge with high probability $\ge 1 - \delta$ to a state where there is a consistent common prior after $R_j = O\left(N^2D_j\frac{\ln\left(\delta/(N^2D_j)\right)}{\ln\left(1/\alpha_j\right)}\right)$ rounds, we now introduce an explicit algorithm for the efficient searching of the belief states of the agents.
This is given by Algorithm~\ref{alg:construct}, and finds a common prior via linear programming feasibility of the Bayesian posterior consistency ratios across states.

We first give proofs of correctness (Lemma~\ref{lem:cp-correctness}) and runtime (Lemma~\ref{lem:cp-runtime}) in the exact setting, before generalizing it to the inexact sampling tree setting (Lemma~\ref{lem:approx-cp}).
\begin{lemma}[Correctness of \textsc{``ConstructCommonPrior''} (Algorithm~\ref{alg:construct})]
\label{lem:cp-correctness}
Let $S_j$ be a finite state space, and for each agent $i\in\N$ let
$\Pi_j^{i,t}$ be a partition of $S_j$ with corresponding posterior
$\tau^{i,t}_j$.
Then the algorithm \textsc{ConstructCommonPrior} returns a probability distribution $p_j$ on $S_j$ that is a Bayes‐consistent common prior for all $\tau^{i,t}_j$ if and only if such a (possibly different) common prior $\CP_j$ exists in principle.
\end{lemma}
\begin{proof}
\textbf{(\(\Longrightarrow\))}\quad
Suppose first that there is some common prior $\CP_j$ over $S_j$ satisfying
\begin{equation*}
  \CP_j(s_j \mid C_{j,k}^{i,t})
  \;=\;
  \tau^{i,t}_j\bigl(s \mid C_{j,k}^{i,t}\bigr)
\quad
\text{for all agents $i$ and states $s_j$ in each cell $C_{j,k}^{i,t}$.}
\end{equation*}
In particular, for $s_j,s'_j\in C_{j,k}^{i,t}$,
\begin{equation*}
  \frac{\CP_j(s_j)}{\CP_j(s'_j)} 
  \;=\;
  \frac{\tau^{i,t}_j\bigl(s_j \mid C_{j,k}^{i,t}\bigr)}
       {\tau^{i,t}_j\bigl(s'_j \mid C_{j,k}^{i,t}\bigr)}.
\end{equation*}
Since the meet partition $\Pi_j^*$ refines each $\Pi_j^{i,t}$, those ratio constraints must also hold on every meet‐cell $Z_\alpha\subseteq C_{j,k}^{i,t}$.  
Hence, if we introduce variables for $p_j(Z_\alpha)$ and enforce these pairwise ratio constraints (Algorithm~\ref{alg:construct},
Step 3), the distribution $\CP_j$ serves as a \emph{feasible solution} to that linear system.
Furthermore, the normalization (Step 4) is satisfied by
$\CP_j$.
Consequently, the algorithm will return some valid $p_j$.

\medskip

\noindent
\textbf{(\(\Longleftarrow\))}\quad
Conversely, if the algorithm’s LP is feasible and yields $\bigl\{\,p_j(Z_\alpha)\bigr\}_{\alpha=1}^r$ with $\sum_{\alpha=1}^r p_j(Z_\alpha)=1$, then for any agent~$i$ and cell $C_{j,k}^{i,t}\in \Pi_j^{i,t}$, the meet‐cells $Z_\alpha\subseteq
C_{j,k}^{i,t}$ satisfy
\begin{equation*}
  \frac{p_j(s_j)}{p_j(s'_j)}
  \;=\;
  \frac{\tau^{i,t}_j\bigl(s_j \mid C_{j,k}^{i,t}\bigr)}
       {\tau^{i,t}_j\bigl(s'_j \mid C_{j,k}^{i,t}\bigr)}
  \quad
  \text{for all } s_j,s'_j\in Z_\alpha.
\end{equation*}
Define for each state $s_j\in Z_\alpha$,
\begin{equation*}
  p_j(s_j) = p_j\bigl(Z_\alpha\bigr)\cdot\tau^{i,t}_j\bigl(s_j \mid C_{j,k}^{i,t}\bigr).
\end{equation*}
This $p_j$ is a proper distribution over $S_j$ (since the $p_j(Z_\alpha)$ sum to 1).
Moreover,
\begin{equation*}
  p_j\bigl(s_j\mid C_{j,k}^{i,t}\bigr) = \tau^{i,t}_j\bigl(s_j \mid C_{j,k}^{i,t}\bigr),
\end{equation*}
so $p_j$ is indeed a common prior that \emph{matches} each agent’s posterior distribution.

\medskip

\noindent
\textbf{Remark on the ``true'' prior vs.\ the algorithm's output.}  
If there were a ``true'' common prior $\CP_j$, the distribution $p_j$ returned by the algorithm need not coincide with $\CP_j$ numerically; many distributions can satisfy the same ratio constraints in each cell.
But from every agent's viewpoint, $p_j$ and $\CP_j$ induce the same posterior on all cells, and thus are equally valid as a Bayes‐consistent common prior.

Hence \textsc{ConstructCommonPrior} returns a valid common prior if and only if one exists.
\end{proof}

\begin{lemma}[Runtime of finding a common prior]
\label{lem:cp-runtime}
Given posteriors for $N$ agents over a state space $S_j$ of size $D_j$, a consistent common prior can be found in $O\Bigl(\mathrm{poly}(N\,D_j^2)\Bigr)$ time.
\end{lemma}

\begin{proof}
Each agent’s partition $\Pi_j^{i,t}$ divides $S_j$ into at most $D_j$ cells, so the meet partition
\begin{equation*}
\Pi_j^*=\bigwedge_{i=1}^N \Pi_j^{i,t},
\end{equation*}
has at most $D_j$ nonempty blocks.
Labeling each of the $D_j$ states with its $N$-tuple of cell indices takes $O(N)$ time per state, and grouping (using hashing or sorting) can be done in $O(D_j)$ or $O(D_j\log D_j)$ time. 
Hence, constructing $\Pi_j^*$ takes $\tilde{O}(N\,D_j)$ time (the $\tilde{O}$ subsumes polylogarithmic factors).

Next, for each agent $i\in\N$ and for each cell $C_{j,k}^{i,t}$ in $\Pi_j^{i,t}$, we impose pairwise ratio constraints over states in the same meet-cell contained in $C_{j,k}^{i,t}$. 
In the worst case, an agent's cell may contain all $D_j$ states, leading to $\binom{D_j}{2} = \Theta(D_j^2)$ pairwise constraints for that agent. 
Summing over $N$ agents gives a total of $O(N\,D_j^2)$ constraints.

Standard LP solvers run in time polynomial in the number of variables (at most $D_j$) and constraints ($O(N\,D_j^2)$), plus the bit-size of the numerical data.
Therefore, the overall runtime is $O\Bigl(\mathrm{poly}(N\,D_j^2)\Bigr)$.
\end{proof}

Having proven the runtime and correctness in the \emph{exact} case, we now turn to bounding the runtime in the inexact sampling tree setting.
\begin{lemma}[Approximate Common Prior via Sampling Trees]
\label{lem:approx-cp}
Assume that each agent approximates its conditional posterior $\tau^{i,t}_j(\cdot\mid C_{j,k}^{i,t})$ using its offline sampling tree $\mathcal{T}^{\,i}_j$ (of height $R_j$ and branching factor $B_j$) with per-sample costs $T_{\text{sample},H}$ (for the $q$ human agents) and $T_{\text{sample},AI}$ (for the $N-q$ AI agents). 
Suppose that for each cell $C_{j,k}^{i,t}\subseteq S_j$ and for any two states $s_j,s'_j\in C_{j,k}^{i,t}$, the ratio
\begin{equation*}
\frac{\tau^{i,t}_j(s_j\mid C_{j,k}^{i,t})}{\tau^{i,t}_j(s'_j\mid C_{j,k}^{i,t})}
\end{equation*}
can be estimated via the sampling tree using 
\begin{equation*}
S = O\left(\frac{1}{\eps^2}\ln\frac{1}{\delta}\right)
\end{equation*}
samples per ratio, so that each is accurate within error $\eps$ with probability at least $1-\delta$ (we assume sufficiently large ${B_j}$ for this, specifically such that ${B_j}^{R_j} \gtrsim S$ for $R_j$ given by  Lemma~\ref{lem:common-prior-sampling-tree}).
Then the overall time complexity in the sampling-tree setting of $\textsc{ConstructCommonPrior}$ is
\begin{equation*}
O\left(\frac{1}{\eps^2}\ln\frac{N\,D_j^2}{\delta}\cdot \mathrm{poly}\left(D_j^2(q\,T_{\text{sample},H}+(N-q)T_{\text{sample},AI})\right)\right).
\end{equation*}
\end{lemma}
\begin{proof}
For each cell $C_{j,k}^{i,t}$ and each state $s_j\in C_{j,k}^{i,t}$, the agent uses its sampling tree $\mathcal{T}^{\,i}_j$ to compute an empirical estimate $\widehat{\tau}^{i,t}_j(s_j\mid C_{j,k}^{i,t})$ of $\tau^{i,t}_j(s_j\mid C_{j,k}^{i,t})$ by averaging over the leaves of the appropriate subtree.
(Recall that the tree is constructed offline by drawing $O\left(B_j^{R_j}\right)$ i.i.d. samples from the unconditional prior of the active agent at each level, defined by the $\mathrm{ActiveAgent}_j$ function.)
Thus, for a fixed agent $i$, cell $C_{j,k}^{i,t} \subseteq S_j$, and state $s_j \in C_{j,k}^{i,t}$, define the i.i.d. indicator random variables $X_1, \dots, X_m$ by
\begin{equation*}
X_\ell =
\begin{cases}
1, & \text{if the $\ell$-th sample (from the appropriate leaves of $\mathcal{T}_j^i$ restricted to $C_{j,k}^{i,t}$) equals $s_j$,} \\
0, & \text{otherwise.}
\end{cases}
\end{equation*}
That is, each $X_\ell$ indicates whether the $\ell$-th sample drawn via the sampling tree lands on the state $s_j$ for agent $i$.
The empirical average
\begin{equation*}
\widehat{\tau}^{i,t}_j(s_j \mid C_{j,k}^{i,t}) = \frac{1}{m} \sum_{\ell=1}^{m} X_\ell
\end{equation*}
serves as an estimate for the true probability $\tau^{i,t}_j(s_j \mid C_{j,k}^{i,t})$.

By the ``textbook'' additive Chernoff-Hoeffding bound, if $m=O\left(\tfrac{1}{\eps^2}\ln\tfrac{1}{\delta'}\right)$, then
\begin{equation*}
  \prob\Bigl[\,
    \bigl|\widehat{\tau}^{i,t}_j(s_j\mid C_{j,k}^{i,t})
           -\tau^{i,t}_j(s_j\mid C_{j,k}^{i,t})
    \bigr| \ge \eps
  \Bigr] \le \delta'.
\end{equation*}
Similarly for $s'_j$, hence the ratio $\widehat{\tau}^{i,t}_j(s_j\mid C_{j,k}^{i,t})
  \big/
  \widehat{\tau}^{i,t}_j(s'_j\mid C_{j,k}^{i,t})$
differs by at most $\pm\eps$ with probability $\ge 1-2\delta'$, as it is computed from two such independent estimates of $\widehat{\tau}$.  
Taking $\delta'=\delta/(2\,N\,D_j^2)$ and union‐bounding over all
$O(N\,D_j^2)$ ratios yields a net success probability $\ge 1-\delta$.

Finally, each ratio uses $O((1/\eps^2)\,\ln(1/\delta'))$ subtree draws, each draw in the outer \texttt{for} loop in Step 3 of Algorithm~\ref{alg:construct}, incurring $T_{\mathrm{sample},H}$ or $T_{\mathrm{sample},AI}$ cost depending on whether a human or AI agent (of the $N$ total agents) built that portion of $\mathcal{T}^i_j$.
Hence, we replace the $N$ in Lemma~\ref{lem:cp-runtime}'s $O\left(\textrm{poly}\left(N\, D_j^2\right)\right)$ runtime with $q\,T_{\text{sample},H}+(N-q)T_{\text{sample},AI}$, and multiply by the per-ratio sampling overhead of $O((1/\eps^2)\,\ln(1/\delta'))$.
This gives us the stated runtime.
Thus, \textsc{ConstructCommonPrior} still returns a valid
common prior with probability at least $1-\delta$.
\end{proof}

Now we have reached a state where the $N$ agents have found a common prior $\CP_j$ with high probability.
In what follows, note that it does not matter if their common prior is approximate, but rather that the $N$ agents can consistently find \emph{some} distribution to condition on, which is what Lemma~\ref{lem:approx-cp} guarantees as a consequence.
By Subroutine 2 of Requirement~\ref{req:bounded-cap}, all $N$ agents can sample from the unconditional common prior $\CP_j$ once it is found (Line 12 of Algorithm~\ref{alg:agree}), in total time:
\begin{equation}\label{eq:cond-time}
q\,T_{\text{sample},H}+(N-q)T_{\text{sample},AI}.
\end{equation}

They will do this and then build their \emph{new} sampling trees of height $R'_j$ and branching factor $B'_j$, post common prior.
We now want to bound $R'_j$:
\begin{lemma}\label{lem:agree-smoothed-standard}
For all $f_j$ and $\CP_j$, the $N$ agents will globally $\tuple{\eps_j, \delta_j}$-agree after $R'_j = O\left({N^7}/{\left(\delta_j\eps_j\right)^2}\right)$ rounds under the message-passing protocol in \S\ref{app:msp-protocol}.
\end{lemma}
\begin{proof}
By~\citep[Theorem 11]{aaronson2005complexity}, when any \emph{two} agents use this protocol under a common prior, it suffices to take $R'_j = O\left(1/(\delta_j\eps_j^2)\right)$ rounds to reach pairwise $\tuple{\eps_j, \delta_j}$-agreement, which is the same runtime as in the non-noisy two agent case.
We just need to generalize this to $N$ agents who share a common prior $\CP_j$.
We take the same approach as in our Proposition~\ref{prop:disc}, by having an additional node $F_j$ that is globally accessible to all $N$ agents.
The rest of the proof follows their Theorem 11 for any \emph{pair} of agents that use the intermediary $F_j$, so then by our Lemma~\ref{lem:spanning-tree}, under the spanning tree protocol, it will instead require $R'_j = O\left({(N+1)^7}/{\left(\delta_j\eps_j\right)^2}\right)$ rounds for all $\binom{N+1}{2}$ pairs of $N+1$ agents (including $F_j$) to reach \emph{global} $\tuple{\eps_j, \delta_j}$-agreement.
We subsume the lower-order terms in the big-$O$ constant, giving rise to the above lemma.
\end{proof}

Thus, combining Lemma~\ref{lem:agree-smoothed-standard} with Lemma~\ref{lem:sampling-tree-time}, reaching \emph{global} $\tuple{\eps_j, \delta_j}$-agreement with sampling trees will take total time:
\begin{equation}\label{eq:new-tree-time}
O\left({B'_j}^{{N^7}/{\left(\delta_j\eps_j\right)^2}}\left(q\,T_{\text{sample},H} + (N-q)\,T_{\text{sample},AI} + q\,T_{\text{eval},H} + (N-q)\,T_{\text{eval},AI}\right)\right).
\end{equation}
Let $1-\delta^{\text{find\_CP}}_j$ be the probability of converging to a common prior by Lemma~\ref{lem:common-prior-sampling-tree},
let $1-\delta^{\text{construct\_CP}}_j$ be the probability of constructing a common prior once reaching convergence by Lemma~\ref{lem:approx-cp}, and let $1-\delta^{\text{agree\_CP}}_j$ be the probability of reaching global $\tuple{\eps_j, \delta^{\text{agree\_CP}}_j}$-agreement when conditioned on a constructed common prior, then we have that
\begin{equation*}
\prob\Bigl[\text{$\eps_j$-agreement}\Bigr] \ge \left(1-\delta^{\text{find\_CP}}_j\right)\left(1-\delta^{\text{construct\_CP}}_j\right)\left(1-\delta^{\text{agree\_CP}}_j\right) \ge 1 - \left(\delta^{\text{find\_CP}}_j + \delta^{\text{construct\_CP}}_j + \delta^{\text{agree\_CP}}_j\right).
\end{equation*}
Hereafter, we set $\delta_j := \delta^{\text{find\_CP}}_j + \delta^{\text{construct\_CP}}_j + \delta^{\text{agree\_CP}}_j$.

Thus, for a single task $j$, sufficiently large $B'_j$, and $B_j \ge \max\{S^{1/R_j},1/\alpha_j\}$, we have that with probability $\ge 1 - \delta_j$, the $N$ agents will reach $\eps_j$-agreement in time:
\begin{equation*}
\begin{split}
&\tilde{O}\Biggl({B_j}^{\,N^2 D_j\, \frac{\ln\bigl(\delta^{\text{find\_CP}}_j/(N^2 D_j)\bigr)}{\ln(1/\alpha_j)}} 
\Bigl( q\,T_{\text{sample},H} + (N-q)\,T_{\text{sample},AI} + q\,T_{\text{eval},H} + (N-q)\,T_{\text{eval},AI} \Bigr) \\
& \quad{}\quad{} + N^2 D_j\, \cdot\mathrm{poly}\Bigl( D_j^2\bigl(q\,T_{\text{sample},H}+(N-q)\,T_{\text{sample},AI}\bigr)\Bigr) + q\,T_{\text{sample},H}+(N-q)T_{\text{sample},AI} \\
& \quad{}\quad{} + {B'_j}^{\,\frac{N^7}{\left(\delta^{\text{agree\_CP}}_j\eps_j\right)^2}}
\Bigl( q\,T_{\text{sample},H} + (N-q)\,T_{\text{sample},AI} + q\,T_{\text{eval},H} + (N-q)\,T_{\text{eval},AI} \Bigr)
\Biggr)\\
&= O\Biggl({B_j}^{\,N^2 D_j\, \frac{\ln\bigl(\delta^{\text{find\_CP}}_j/(N^2 D_j)\bigr)}{\ln(1/\alpha_j)}} 
\Bigl( q\,T_{\text{sample},H} + (N-q)\,T_{\text{sample},AI} + q\,T_{\text{eval},H} + (N-q)\,T_{\text{eval},AI} \Bigr) \\
& \quad{}\quad{} + {B'_j}^{\,\frac{N^7}{\left(\delta^{\text{agree\_CP}}_j\eps_j\right)^2}}
\Bigl( q\,T_{\text{sample},H} + (N-q)\,T_{\text{sample},AI} + q\,T_{\text{eval},H} + (N-q)\,T_{\text{eval},AI} \Bigr)
\Biggr),
\end{split}
\end{equation*}
since the logarithmic terms (from Lemma~\ref{lem:approx-cp}) subsumed by the $\tilde{O}$ are on the non-exponential additive terms.
This follows from summing the runtime of computing the sampling tree (Lemma~\ref{lem:sampling-tree-time}), the total runtime of the common prior procedure in Lemma~\ref{lem:approx-cp} multiplied by the number of online rounds $R_j$ in the \texttt{while} loop of Algorithm~\ref{alg:agree} (Lines 4-16) given by Lemma~\ref{lem:common-prior-sampling-tree}, and the runtimes of conditioning and the second set of sampling trees in \eqref{eq:cond-time} and \eqref{eq:new-tree-time}, respectively.
We then take $D := \max_{j\in \M} D_j$, $\delta^{\text{find\_CP}}:= \max_{j \in \M} \delta^{\text{find\_CP}}_j$, $\alpha := \min_{j \in \M} \alpha_j$, $\eps:= \min_{j \in \M} \eps_j$, and selecting a uniform $\delta^{\text{agree\_CP}}$ such that $\delta^{\text{agree\_CP}}_j \le \delta^{\text{agree\_CP}}/M$ via a union bound across the $M$ tasks.
One can further maximize $B := \max_{j\in \M}\max\{B_j, B'_j\}$ to subsume the bases into a single base to simplify the upper bound.
Scaling the resulting maximized quantity by $M$, we get the bound in Theorem~\ref{thm:bounded}.

To prove Corollary~\ref{cor:wannabe-agree}, it suffices to explicitly bound $B'_j$.
This is because for each \emph{individual} task $j$, a ``total Bayesian wannabe'', \emph{after} conditioning on a common prior $\CP_j$, as defined in Definition~\ref{def:total-wannabe}, corresponds to a single ``Bayesian wannabe'' in Aaronson's sense~\citep[pg. 19]{aaronson2005complexity}.
Thus, by~\citep[Theorem 20]{aaronson2005complexity}, for $\eps_j/50 \le \alpha_j \le \eps_j/40$, it suffices to set on a per-task basis,
\begin{equation}\label{eq:total-bayesian}
B'_j := O\left((11/\alpha_j)^{{R'_j}^2}/\rho_j^2\right), \quad 
b_j := \left\lceil \log_2 \tfrac{R'_j}{\rho_j \alpha_j} \right\rceil + 2,
\end{equation}
where $R'_j$ is the value for $N$ agents that we found in Lemma~\ref{lem:agree-smoothed-standard}.
Taking $\rho := \min_{j \in \M} \rho_j$ maximizes the bound, and scaling by $M$ gives rise to what is stated in Corollary~\ref{cor:wannabe-agree}.