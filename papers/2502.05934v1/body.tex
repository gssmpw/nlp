\section{Introduction}
\label{sec:introduction}
The past decade, and most notably the past two years, has seen a surge of progress in artificial intelligence (AI), where the core goal is to construct \emph{intelligent agents} that can perceive and act meaningfully in their environment, echoing ideas from game theory and economics of rational agents~\citep{simon1984models}.
As these technologies have rapidly shifted from being confined to laboratories to multi-billion dollar industries~\citep{USDeptCommerce2023AI}, it becomes ever more pressing, both economically and societally, to understand conditions where we can guarantee that these systems can be deployed safely.
One of the key problems in AI safety is the \emph{value alignment problem}, ensuring that these agents make ``good'' decisions that are in line with human intentions and values~\citep{soares2018value,russell2015research,amodei2016concrete}, anticipated by Norbert Weiner as early as 1960~\citep{wiener1960some}.

Much of the current alignment literature deals with the specifics of AI systems today, e.g., preventing the jailbreaking of large language models (LLMs)~\citep{ji2023ai,guan2024deliberative,hubinger2024sleeper}.
This focus makes practical sense for now, as these systems have yet to be as generally capable as humans across domains, and can easily produce undesired side effects (``misalignment'') simply by malfunctioning and not carrying out the task they were instructed to do.
But, as AI systems steadily improve, they will be able to successfully carry out more tasks to completion with us.

As such generally-capable AI systems have yet to be built, we cannot simulate these systems at the moment to empirically assess these questions; therefore, a theoretical treatment of the problem can be helpful here.
However, there is currently little in the way of \emph{proven} guarantees as to the conditions under which such capable systems could align with us \emph{in general}.
One of the reasons for this is that existing theoretical frameworks for alignment are tailored to \emph{particular} toy scenarios, in order to (understandably) prove guarantees.

An early approach known as ``AI Safety via Debate''~\citep{irving2018ai}, proposed to train computationally powerful agents via self-play in a zero-sum debate game, until the misalignment is identified on a manageable subtask of a few bits of disagreement\footnote{This idea is conceptually related to the recursive splitting protocol used in the proof of Savitch's theorem~\citep{savitch1970relationships}, which establishes as a corollary that $\mathsf{PSPACE} = \mathsf{NPSPACE}$.}.
Although this form of debate with optimal play is in $\mathsf{PSPACE}$ (and can be extended to $\mathsf{NEXP}$ if you include cross-examination~\citep{barnes2020progress}), all the methods from the theory of interactive proofs apply to problems which are mathematically precise so that \emph{exact} misalignment can be efficiently verified.
Furthermore, in their specific setup of querying black-box human judgements as an oracle, the main theorem of efficient verification of an unbounded prover (the AI), namely $\mathsf{IP}=\mathsf{PSPACE}$~\citep{shamir1992ip,lund1992algebraic}, does not hold (``relativize'')~\citep{fortnow1994role,fortnow1988there}.
To circumvent the latter issue, in recent follow-up work~\citep{brown2023scalable}, they enforce that the AI \emph{prover} is also efficient (and such that an honest human verifier could run in polynomial, rather than exponential, time).
However, this added constraint removes from consideration many scenarios which can produce an efficient strategy, but the computation to be verified does not have a polynomial-length human-verifiable transcript. 
(Moreover, the debate setup requires the human oracle to be correct and unbiased, further limiting its applicability.)

Now, debate is certainly not the only theoretically-motivated approach to alignment.
\emph{Reinforcement learning} of agent actions based on human preferences encompasses a large class of approaches, most notably Reinforcement Learning from Human Feedback (RLHF)~\citep{christiano2017deep,ziegler2019fine,bai2022training}, which is widely deployed today in real-world systems.
However, general \emph{provable} guarantees are hard to come by in this domain.
One can, for example, consider specific games, such as the off-switch game~\citep{hadfield2017off}.
This game is related to the ``shutdown problem''~\citep{soares2015corrigibility}, presciently alluded to by Turing in his 1951 BBC radio address~\citep[pg. 6a]{turing1951}, which studies the circumstances under which a rational agent that maximizes its utility would be incentivized to keep its off switch, where the human can also shut it off but the robot can disable its switch too.
They prove that for a rational agent to keep its switch and wait for the human to switch it off or not, it would need to be \emph{uncertain} about the utility associated with the outcome of disabling its off switch, and also treat the human's actions as important observations of that utility.
This type of coordination game is part of a broader approach known as cooperative reinforcement learning (CIRL)~\citep{hadfield2016cooperative}.
In CIRL, alignment is modeled as a partial information game between a human and a robot, who are both rewarded according to the human's reward function, but the robot does not know initially what that is.
They show that CIRL games have an optimal joint policy by reducing them to a single-agent POMDP~\citep{sondik1971optimal,kaelbling1998planning}, 
However, the amount of coordination required remains an open question, which we formally examine here \emph{without} assuming the Markov property, allowing agents to retain and utilize richer histories of their interactions. 
This stands in contrast to frameworks like CIRL, where the Markovian assumption simplifies modeling but may constrain the scope of agent coordination.

This coordination is important to analyze because, as the authors point out, CIRLs are still challenging to apply to high-dimensional state spaces common to real-world tasks, even \emph{despite} the fact that POMDP they reduce to~\citep{nayyar2013decentralized} avoids the $\mathsf{NEXP}$-complete bound of general POMDPs~\citep{bernstein2002complexity} and benefits from improved Bellman backups~\citep{palaniappan2017efficient}.
Here we argue that lurking underneath these aforementioned approaches is a more unifying common framework that fundamentally involves two features: (1) \emph{coordination} and (2) \emph{partial information}, including some modeling of uncertainty about their current objective.
Coordination is especially important because it enables us to consider alignment with nontrivial agents that are minimally capable enough to carry out tasks alongside us.
Avoiding such side effects in these circumstances naturally provides a proxy for mitigating negative externalities---a core concern in alignment~\citep{amodei2016concrete}.
After all, if a side effect is universally beneficial, there is no need to avoid it!
Coordination also lies at the heart of currently widely-deployed approaches like RLHF that explicitly align models with human preferences.
Specifically, here we consider a flexible game-theoretic framework for studying the coordination question with partial information, which we call \agree-agreement, and show that earlier alignment frameworks can be related as particular cases.
We therefore assess with our framework under what circumstances is there efficient coordination of alignment \emph{in general} (rather than for any one specific game or task).

\section{Results}
\label{sec:results}
\subsection{Notational Preliminaries}
\label{ss:results-notation}
Define $\M := \{1,\dots, M\}$ and $\N := \{1,\dots,N\}$.
We will use asymptotic notation throughout that is standard in computer science, but may not be in other fields.
The asymptotic notation is defined as follows:
\begin{itemize}[itemsep=1em]
    \item $F(n) = O(G(n))$: There exist positive constants $c_1 > 0$ and $c_2 > 0$ such that $F(n) \leq c_1 + c_2G(n)$, for all $n \geq 0$.
    \item $F(n) = \tilde{O}(G(n))$: There exist positive constants $c_1, c_2$, and $k > 0$ such that $F(n) \leq c_1 + c_2 G(n) \log^k n$, for all $n \geq 0$.
    \item $F(n) = \Omega(G(n))$: Similarly, there exist positive constants $c_1$ and $c_2$ such that $F(n) \geq c_1 + c_2G(n)$, for all $n \geq 0$.
    \item $F(n) = \Theta(G(n))$: This indicates that $F(n) = O(G(n))$ and $F(n) = \Omega(G(n))$.
    In other words, $G(n)$ is a tight bound for $F(n)$.
\end{itemize}
$\mathbb{P}$ and $\mathbb{E}$ will be the font we use for probabilistic quantities, in this case, probability and expectation of an event.
$\R$ and $\Nat$ refer to the real and natural numbers, respectively.
$\powsetopname$ will be used to denote the power set of a set.

\subsection{\agree-Agreement Framework}
\label{ss:results-framework}
The framework we consider for alignment generalizes Aumann agreement~\citep{aumann1976agreeing} to probabilistic $\tuple{\eps, \delta}$-agreement~\citep{aaronson2005complexity} (rather than exact agreement), across $M$ agreement objectives and $N$ agents, \emph{without} the Common Prior Assumption (CPA).
The CPA dates back to at least Hars\'{a}nyi~\citep{harsanyi1967} in his seminal work on games with incomplete information.
This is a very powerful assumption and is at the heart of Aumann's agreement theorem that two rational Bayesian agents must agree if they share a common prior~\citep{aumann1976agreeing}.
As a further illustration of how powerful the CPA is from a computational complexity standpoint, Aaronson~\citep{aaronson2005complexity} relaxed the exact agreement requirement to $\tuple{\eps, \delta}$-agreement and showed that even in this setting, completely independent of how large the state space is, two agents with common priors will need to only exchange $O(1/(\delta\eps^2))$ messages to agree within $\eps$ with probability at least $1-\delta$ over their prior.
However, the CPA is clearly a very strong assumption for human-AI alignment, as we cannot expect that our AIs will always \emph{start out} with common priors with every human it will engage with on every task.
In fact, even between two \emph{humans} this assumption is unlikely!
For other aspects of agreement and how they relate more broadly to alignment, we defer to the Discussion (\S~\ref{sec:discussion}) for a more detailed treatment.

In short, \agree-agreement represents a ``best-case'' scenario that is general enough to encompass prior theoretical approaches to alignment (cf. Remark~\ref{rem:align}), such that if something is inefficient here, then it forms a prescription for what to avoid \emph{in practice}, in far more suboptimal circumstances.
As examples of suboptimality in practice, we will consider computational boundedness and noisy messages in \S\ref{ss:results-bounded}, to exactly quantify how the bounds can significantly (e.g. exponentially) worsen.

Dispensing with the CPA, we now make our \agree-agreement framework more precise.
For illustration, we consider two agents ($N=2$), Alice (human) and ``Rob'' (robot), denoted by $\A$ and $\Rob$, respectively.
Let $\{S_j\}_{j\in \M}$ be the collection of (not necessarily disjoint) possible task states for each task $j\in \M$ they are to perform. 
We assume each $S_j$ is finite ($|S_j| = D_j \in \Nat$), as this is a standard assumption, and any physically realistic agent can only encounter a finite number of states anyhow.
There are $M$ agreement objectives, $f_1, \dots, f_M$, that Alice and Rob want to jointly estimate, one for each task:
\begin{equation*}
f_j : S_j \to [0, 1], \quad \forall j \in \M,
\end{equation*}
to encompass the possibility of changing needs and differing desired $\left\{\tuple{\eps_j, \delta_j}\right\}_{j \in [M]}$-agreement thresholds for those needs (which we will define shortly in \eqref{eq:eps-delta}), rather than optimizing for a single monolithic task.
Note that setting the output of $f_j$ to $[0, 1]$ does not reduce generality.
Since $S_j$ is finite, any function $S_j \to \R$ has a bounded range, so one can always rescale appropriately to go inside the $[0, 1]$ domain.

Alice and Rob have priors $\prob^{\A}_j$ and $\prob^{\Rob}_j$, respectively, over task $j$'s state space $S_j$. 
Let $\nu_j \in [0,1]$ denote the \emph{prior distance} (as introduced by Hellman~\citep{hellman2013almost}) between $\prob^{\A}_j$ and $\prob^{\Rob}_j$, defined as the minimal $L^1$ distance between any point $x_j \in X_j = \prob^{\A}_j \times \prob^{\Rob}_j$ and any point $p_j \in \mathcal{D}_j
 = \{(p_j, p_j) \mid p_j \in \Delta(S_j)\}$, where $\Delta(S_j)\in \R^{D_j}$ is the probability simplex over the states in $S_j$. 
Formally,
\begin{equation}\label{eq:prior-dist}
\nu_j = \min_{x_j \in X_j, p_j \in \mathcal{D}_j} \|x_j - p_j\|_1, \quad \forall j \in \M.
\end{equation}
It is straightforward to see that there exists a \emph{common prior} $\CP_j \in \mathcal{D}_j$ between Alice and Rob for task $j$ if and only if the task state space $S_j$ has prior distance $\nu_j = 0$.
(Lemma~\ref{lem:common-prior} will in fact show that it is possible to find a common prior with high probability, regardless of the initial value $\nu_j$.)

For every state $s_j \in S_j$, we identify the subset $E_j \subseteq S_j$ with the event that $s_j \in S_j$.
For each task $j \in \M$, Alice and Rob exchange messages\footnote{These messages could be as simple as communicating the agent's current expectation of $f_j$, given (conditioned on) its current knowledge partition. For now, we assume the messages are not noisy, but we will remove this assumption in \S\ref{ss:results-bounded}.} from the power set $\powset{S_j}$ of the task state space $S_j$, as a sequence $m^{1}_j,\dots,m^{T}_j:\powset{S_j} \to [0,1]$.
Let $\Pi_j^{i,t}(s_j)$ be the set of states that agent $i \in \{\A, \Rob\}$ considers possible in task $j$ after the $t$-th message has been sent, given that the true state of the world for task $j$ is $s_j$.
Then by construction, $s_j \in \Pi_j^{i,t}(s_j) \subseteq S_j$, and the set $\{\Pi_j^{i,t}(s_j)\}_{s_j \in S_j}$ forms a \emph{partition} of $S_j$ (known as a ``knowledge partition'').
As is standard~\citep{aumann1976agreeing,aumann1999interactive,aaronson2005complexity,hellman2013almost}, we assume for each task $j$, the agents know each others' initial knowledge partitions $\{\Pi_j^{i,0}(s_j)\}_{s_j \in S_j}$.
The justification for this more broadly~\citep{aumann1976agreeing,aumann1999interactive} is that a given state of the world $s_j \in S_j$ includes the agents' knowledge.
In our setting, it is quite natural to assume that task states for agents coordinating on a task will encode their knowledge.
As a consequence, every agents' subsequent partition is known to every other agent, and every agent knows that this is the case, and so on\footnote{This can be implemented via a ``\emph{common knowledge}'' set, $\mathcal{C}\left(\{\Pi_j^{i,t}\}^{i \in \N}\right)$, which is the finest common coarsening of the agents' partitions~\citep{aumann1976agreeing}.}.
This is because with this assumption, since the agents receive messages from each other, then $\Pi_j^{i,t}(s_j) \subseteq \Pi_j^{i,t-1}(s_j)$.
In other words, subsequent knowledge partitions $\{\Pi_j^{i,t}(s_j)\}_{s_j \in S_j}$ \emph{refine} earlier knowledge partitions $\{\Pi_j^{i,t-1}(s_j)\}_{s_j \in S_j}$.
(Equivalently, we say that $\{\Pi_j^{i,t-1}(s_j)\}_{s_j \in S_j}$ \emph{coarsens} $\{\Pi_j^{i,t}(s_j)\}_{s_j \in S_j}$.)
\emph{Proper} refinement is if $\{\Pi_j^{i,t}(s_j)\}_{s_j \in S_j} \subsetneq \{\Pi_j^{i,t-1}(s_j)\}_{s_j \in S_j}$, whereby for at least one state $s_j \in S_j$, $\Pi_j^{i,t}(s_j) \subsetneq \Pi_j^{i,t-1}(s_j)$, representing a \emph{strict} increase in knowledge.

To illustrate this more concretely, first Alice computes $m^{1}_j\left(\Pi_j^{\A,0}(s_j)\right)$ and sends it to Rob.
Rob's knowledge partition then becomes refined to the set of messages in his original knowledge partition that match Alice's message (since they are now both aware of it):
\begin{equation*}
\Pi_j^{\Rob,1}(s_j) = \left\{s'_j \in \Pi_j^{\Rob,0}(s_j) \mid  m^1_j\left(\Pi_j^{\A,0}(s'_j)\right) = m^1_j\left(\Pi_j^{\A,0}(s_j)\right)\right\},
\end{equation*}
from which Rob computes $m^2_j\left(\Pi_j^{\Rob,1}(s_j)\right)$ and sends it to Alice.
Alice then updates her knowledge partition similarly to become the set of messages in her original partition that match Rob's message:
\begin{equation*}
\Pi_j^{\A,2}(s_j) = \left\{s'_j \in \Pi_j^{\A,0}(s_j) \mid  m^2_j\left(\Pi_j^{\Rob,1}(s'_j)\right) = m^2_j\left(\Pi_j^{\Rob,1}(s_j)\right)\right\},
\end{equation*}
and then she computes and sends the message $m^{3}_j\left(\Pi_j^{\A,2}(s_j)\right)$ to Rob, etc.

\textbf{\agree-Agreement Criterion:} We examine here the number of messages ($T$) required for Alice and Rob to $\tuple{\eps_j, \delta_j}$-agree across all tasks $j \in \M$, defined as
\begin{equation}\label{eq:eps-delta}
\prob\left[\left|\Eja{f_j\mid \Pi_j^{\A,T}(s_j)} - \Ejr{f_j\mid \Pi_j^{\Rob,T}(s_j)}\right| \le \eps_j \right] > 1-\delta_j, \quad \forall j \in \M.
\end{equation}
In other words, they agree within $\eps_j$ with high probability ($> 1-\delta_j$) on the expected value of $f_j$ with respect to their \emph{own} task-specific priors (not a common prior!), conditioned\footnote{For completeness, note that for any subset $E_j\subseteq S_j$ and distribution $\prob$, $\mathbb{E}_{\prob}[f_j\mid E_j] := \sum_{s_j \in E_j}f(s_j)\prob[s_j \mid E_j] = \dfrac{\sum_{s_j \in E_j}f(s_j)\prob[s_j]}{\sum_{s_j\in E_j}\prob[s_j]}$.} on each of their knowledge partitions by time $T$.

Extending this framework to $N > 2$ agents (consisting of $1 \le q < N$ humans and $N-q \ge 1$ AI agents), is straightforward: we can have their initial, task-specific priors be denoted by $\{\prob_j^i\}^{i \in \N}$, and we can have them $\tuple{\eps_j, \delta_j/N^2}$-agree pairwise so that they globally $\tuple{\eps_j, \delta_j}$-agree.
\newline\newline

Our main result is the following theorem:
\begin{theorem}\label{thm:ub}
$N$ rational agents will \agree-agree with overall failure probability $\delta$ across $M$ tasks, as defined in \eqref{eq:eps-delta}, after $T = O\left(MN^2 D + \dfrac{M^3N^7}{\eps^2\delta^2}\right)$ messages, where $D := \max_{j \in \M} D_j$ and $\eps := \min_{j \in \M}\eps_j$.
Thus, for the special case of $M=1$ tasks and $N=2$ agents, this becomes $T = O\left(D + \dfrac{1}{\eps^2\delta^2}\right)$ messages before they \agree-agree with total probability $\ge 1-\delta$.
\end{theorem}

Before proceeding further, we note the relationship of the $\agree$-agreement framework with prior agreement and alignment approaches:
\begin{remark}[Relation to Agreement]\label{rem:agree}
Traditional Aumann agreement~\citep{aumann1976agreeing} corresponds to exact agreement between $N$ agents (usually $N=2$), so the special case of $\tuple{M=1, N, \eps = 0, \delta = 0}$, but with the added assumption of common priors.
Aaronson's $\tuple{\eps, \delta}$-agreement~\citep{aaronson2005complexity} corresponds to the special case of $\tuple{M=1, N, \eps, \delta}$, with the assumption of common priors as well.
Hellman's~\citep{hellman2013almost} ``almost common priors'' framework corresponds to the special case of $\tuple{M=1, N, \eps=0,\delta=0}$-agreement (in other words, exact agreement).
\end{remark}
\begin{remark}[Relation to Alignment]\label{rem:align}
Prior alignment work can be viewed as the $N=2$ (and often $M=1$) setting in an \emph{exact} way, so $\tuple{M, N=2, \eps = 0, \delta = 0}$-agreement, either with or without the common prior assumption.

For example, in the AI Safety via Debate framework~\citep{irving2018ai,brown2023scalable}, it is required that the $N=2$ provers agree \emph{exactly} (and \emph{without} any assumption of common priors).
With a continuous protocol, where we can output real-valued messages in $[0,1]$, $M=1$ suffices.
But we can also do this with $M \ge 1$ outputs if using a discrete protocol that outputs a single bit in $\{0, 1\}$ at a time, namely, by having $f_j$ output the $j$-th bit of the function to be verified.
The external verifier in our \agree-agreement framework is the convergence requirement of \eqref{eq:eps-delta}, with $\eps=0$ and $\delta=0$.
The number of exchanged messages $T$ can be taken to be either exponential~\citep{irving2018ai} or polynomial~\citep{brown2023scalable} in the input length, depending on which version of the debate framework you use.

As it relates to CIRL~\citep{hadfield2016cooperative}, the task state space is the Cartesian product of world states, human actions, and agent actions.
We can take their cooperative, parametrized reward $R$ to be $f$ (so $M = 1$), and have the agents align either exactly ($\tuple{\eps=0, \delta=0}$) or up to a threshold in their expectation of the (discounted) reward.
The messages therefore naturally correspond to the expectation of the reward they have been given at each timestep, and they perform the standard belief update of their conditional distribution of the next state, given the previous state and action for both agents, via knowledge partition refinement.
Note that common priors are implictly assumed in CIRL, since the distribution over initial states, $\prob_0$, is sampled first by the environment that the agents both share.
Furthermore, the agents encode their \emph{uncertainty} of the reward by communicating their expectations of the reward, similar to the recommendation in the off-switch game~\citep{hadfield2017off}, which is a special case of CIRL, and therefore, of our \agree-agreement framework.
While CIRL adopts a Markovian assumption for tractability---summarizing past interactions into a belief state---our framework instead retains the full message history. 
This preserves CIRL's crucial partial information aspect while enabling richer coordination beyond myopic updates, particularly in settings requiring long-term consistency, such as handling noise, adversarial agents, or non-stationary objectives.
\end{remark}

\subsection{Proof of Upper Bound}
\label{ss:results-ubproof}
Here we prove the upper bound in Theorem~\ref{thm:ub}.
For an explicit algorithm, see Algorithm~\ref{alg:agree} below.

\begin{algorithm}[ht]
\SetAlgoLined
\LinesNumbered
\caption{\agree-Agreement}
\label{alg:agree}

\KwIn{
    A set of $N$ agents, each with an \emph{initial} knowledge partition 
    $\{\Pi_j^{i,0}\}_{i=1}^N$ for each task $j \in \M$.

    A message protocol $\mathcal{P}$, dictating how agents send/receive messages and refine partitions.

    A subroutine \textsc{ConstructCommonPrior}, defined in Algorithm~\ref{alg:construct}, which attempts to construct 
    a common prior given the current partitions and posteriors.

    A known $\langle \varepsilon,\delta\rangle$-agreement protocol $\mathcal{A}$
    (used once a common prior is found).
}

\KwOut{Agents reach $\tuple{\eps_j,\delta_j}$-agreement for all $M$ tasks.}

\SetKwFunction{Agree}{\agree-Agreement}
\SetKwFunction{RunCPAgreement}{RunCPAgreement}
\SetKwFunction{ConstructCommonPrior}{ConstructCommonPrior}
\SetKwFunction{RefinePartition}{RefinePartition}

\Agree{$\mathcal{P}$, $\mathcal{A}$}:

\For{$j = 1$ \textbf{to} $M$}{

    $t \gets 0$\;
    
    \While{\textbf{true}}{
        $t \gets t + 1$\;

        \ForEach{agent $i \in \N$}{
            Agent $i$ sends message $m_j^{i,t}$ (task $j$, corresponding to $f_j$) as specified by $\mathcal{P}$\;
            $\Pi_j^{i,t} \gets$ \RefinePartition{$\Pi_j^{i,t-1}, m_j^{\cdot,t}$}\;
        }

        $\CP_j \gets$ \ConstructCommonPrior{$\{\Pi_j^{i,t}\}_{i=1}^N,\{\tau_j^{i,t}\}_{i=1}^N$}\;

        \If{$\CP_j \neq$ \textsc{Infeasible}}{
            \textbf{Condition all agents on $\CP_j$ for task $j$}\;
            \RunCPAgreement{$\mathcal{A},\mathcal{P},\CP_j,f_j,\eps_j,\delta_j$}\;
            \textbf{break}\;
        }
    }
}
\end{algorithm}
\par
%\newpage
For notational convenience, let
\begin{equation*}
E^{i,t}_j(s_j) := \Eji{f_j\mid \Pi_j^{i,t}(s_j)},
\end{equation*}
which is the expectation of $f_j$ of agent $i$ at timestep $t$, conditioned on its knowledge partition by then, starting from its \emph{own} prior ${\prob}^{i}_j$.
To simplify notation, we drop the argument $s_j \in S_j$.

First, we need to figure out at most how many messages need to be exchanged to guarantee at least one proper refinement.
To do so, we will have the $N$ agents communicate using the ``spanning-tree'' protocol of Aaronson~\citep[\S 3.3]{aaronson2005complexity}, which we generalize to the multi-task setting below:

\begin{lemma}[Proper Refinement Message Mapping Lemma]\label{lem:spanning-tree-refinement}
If $N$ agents communicate via a spanning‐tree protocol for task $j$, where
$g_j \in \Nat$ is the diameter of the chosen spanning trees, then as long as they have not yet reached agreement, it takes $O(g_j)= O(N)$ messages before at least one agent's knowledge partition is properly refined.
\end{lemma}
\begin{proof}
Let $G_j$ be a strongly connected directed graph with vertices $v\in\N$, one for each agent, so that agents can communicate with each other by sending their current expectations $E^{t,i}_j$ down an edge.
(We need the strongly connected requirement on $G_j$, since otherwise the agents may not reach agreement for trivial reasons if they cannot reach one another.) 
Without loss of generality, let ${SP}^1_j$ and ${SP}^2_j$ be two minimum‐diameter spanning trees of $G_j$, both rooted at agent 1, with ${SP}^1_j$ pointing outward from agent 1 and ${SP}^2_j$ pointing inward to agent 1, each of diameter at most $g_j$.

Define $\mathcal{O}^1_j$ (and respectively, $\mathcal{O}^2_j$) as an ordering of the edges in ${SP}^1_j$ (respectively ${SP}^2_j$) such that each edge $(i\!\to\!k)$ appears only after all edges $(\ell\!\to\!i)$ unless $i$ is the root (or a leaf in the inward tree).
We then form an ordering $\mathrm{AgentOrdering}_j$ by cycling through $\mathcal{O}^1_j,\mathcal{O}^2_j,\mathcal{O}^1_j,\mathcal{O}^2_j,\dots$; in each message round $t$, the agent at the tail of $\mathrm{AgentOrdering}_j(t)$ sends its current expectation.
Thus, within every block of $O(g_j)$ transmissions, each agent's newly updated message is forwarded along ${SP}^1_j$ and ${SP}^2_j$ once, reaching every other agent.
Consequently, if two agents $i$ and $k$ still harbor a disagreement, then from one of their perspectives the other's message is highly improbable, so once that message arrives within these $O(g_j)$ transmissions (in the worst case that agents $i$ and $k$ are on opposite ends of $G_j$), the receiving agent is ``surprised'' and properly refines its knowledge partition.  
Hence in $O(g_j)$ messages without agreement, at least one partition refinement necessarily occurs.
Finally, note that $g_j = O(N)$ if $G_j$ is the ``worst-case topology'' of a ring, due to either physical or logical constraints of the $N$ agents' communication setup (of course, $g_j \ll N$ in other topologies, like a complete graph, but for generality, we assume a worst-case topology to subsume all other specific cases, depending on the communication constraints of any particular task).
\end{proof}

Next, we prove an important (for our purposes) lemma, which is an extension of Hellman's result on almost common priors~\citep[Theorem 2]{hellman2013almost} to our $M$-function message setting:
\begin{lemma}[Common Prior Lemma]\label{lem:common-prior}
If $N$ agents have prior distance $\nu_j$, as defined in \eqref{eq:prior-dist}, for a task $j\in \M$ with task state space $S_j$, then after $O\left(N^2 D_j \right)$ messages, they will have a common prior $\CP_j$ with probability 1 over their type profiles.
\end{lemma}
\begin{proof}
As before, let $\{\prob_j^i\}^{i \in \N}$, be the priors of the agents.
The ``type profile'' $\tau^t_j$ is the set of the agent's posterior belief distributions over states $s_j \in S_j$ at time $t$.
Thus, at time 0, $\tau^0_j$ will correspond to the prior distributions over the states in the knowledge partition $\Pi_j^{i,  0}$.
Since for each agent its type profile distribution is constant across the states in its knowledge partition $\Pi_j^{i, t}(s_j)$ (as they are indistinguishable to the agent, by definition), then the total size of the type profile at time $t$ is
\begin{equation}\label{eq:type-size}
|\tau^t_j| = \sum_{i = 1}^N \left|\Pi_j^{i, t}\right|.
\end{equation}
We make use of the following result of Hellman and Samit~\citep[Proposition 2]{hellman2012common}, restated for our particular setting:
Let $\mathcal{C}\left(\{\Pi_j^{i,t}\}^{i \in \N}\right)$ denote the common knowledge set (finest common coarsening) across the agents' knowledge partitions at time $t$.
If the knowledge partitions reach a total size across the $N$ agents that satisfies:
\begin{equation}\label{eq:cp-size}
\sum_{i=1}^N \left|\Pi_j^{i, t}\right| = (N - 1)D_j + \mathcal{C}\left(\{\Pi_j^{i,t}\}^{i \in \N}\right),
\end{equation}
then any type profile $\tau^t_j$ over $\{\Pi_j^{i,t}\}^{i \in \N}$ has a common prior $\CP_j$.
Now, note that $\left|\mathcal{C}\left(\{\Pi_j^{i,t}\}^{i \in \N}\right)\right| \le D_j$ as it forms a partition over the task state space $S_j$, so the set of singleton sets of each element $s_j \in S_j$ has the most components to saturate the upper bound.
Therefore, the desired size $\sum_{i=1}^N \left|\Pi_j^{i, t}\right| \le ND_j$.

Now, starting from an initial type profile $|\tau^0_j|$, the number of proper refinements needed to get to the desired size $\sum_{i=1}^N \left|\Pi_j^{i, t}\right|$ in \eqref{eq:cp-size} is given by \emph{at most}:
\begin{equation*}
\sum_{i=1}^N \left|\Pi_j^{i, t}\right| - |\tau^0_j| + 1 = O\left(ND_j\right).
\end{equation*}
Thus, since\footnote{For example, for $N$ agents that start with maximally unrefined knowledge partitions, $|\tau^0_j| = \sum_{i = 1}^N \left|\Pi_j^{i, t}\right| = \sum_{i=1}^N 1 = N$.} trivially $|\tau^0_j| \ge 0$, then $O(ND_j)$ is the most number of proper refinements we need to ensure there is a common prior with probability 1, by \eqref{eq:cp-size}.
By Lemma~\ref{lem:spanning-tree-refinement}, this amounts to $O(N^2D_j)$ messages in the worst case.
\end{proof}

Once the agents reach a common prior $\CP_j$, they can then condition on that for the rest of their conversation to reach the desired $1-\delta_j$ $\eps_j$-agreement threshold (cf. Line 12 of Algorithm~\ref{alg:agree}).
We assume this is $O(1)$ to compute for now as the agents are computationally unbounded, but we will remove this assumption in \S\ref{ss:results-bounded}, and instead use Algorithm~\ref{alg:construct} for an efficient explicit construction via LP feasibility of posterior belief ratios.

Thus, the question becomes: How many additional steps do $N$ agents need to take to reach $\eps_j$-agreement, conditioning on the common prior $\CP_j$, with probability at least $1-\delta_j$?

For each task $j$, we have reduced the problem now to Aaronson's $\tuple{\eps, \delta}$-agreement framework~\citep{aaronson2005complexity}, and as he shows, the subsequent steps conditioning on a common prior become unbiased random walks with step size roughly $\eps_j$.
With some slight modifications, this allows us to give a worst-case bound on the number of remaining steps in our \agree-agreement setting:
\begin{lemma}\label{lem:spanning-tree}
For all $f_j$ and $\CP_j$, the $N$ agents will globally $\tuple{\eps_j, \delta_j}$-agree after $O\left({N^7}/{\left(\delta_j\eps_j\right)^2}\right)$ messages.
\end{lemma}
\begin{proof}
By~\citep[Theorem 10]{aaronson2005complexity}, the $N$ agents will \emph{pairwise} $\tuple{\eps_j, \delta_j}$-agree after $O\left(\left(Ng_j^2\right)/{\left(\delta_j\eps_j\right)^2}\right)$ messages when they condition on $\CP_j$, where $g_j$ is the diameter of the spanning-tree protocol they use.
Furthermore, we will need to have them $\tuple{\eps_j, \delta_j/N^2}$-agree pairwise so that they \emph{globally} $\tuple{\eps_j, \delta_j}$-agree.
Taking $g_j = O(N)$ for the worst-case ring topology gives us the above bound.
\end{proof}
By Lemma~\ref{lem:common-prior} and Lemma~\ref{lem:spanning-tree}, for \emph{each} $j \in \M$, we need $O\left(N^2 D_j + \dfrac{N^7}{\left(\delta_j\eps_j\right)^2}\right)$ messages for the $N$ agents to reach $\tuple{M=1, N, \eps_j, \delta_j}$-agreement.
Next, select a uniform $\delta$ such that $\delta_j \le \delta/M$, for all $j \in \M$.
Therefore, by a union bound, we get the full upper bound in Theorem~\ref{thm:ub} with total probability $\ge 1- \delta$, across \emph{all} $M$ tasks, by \emph{maximizing} the bound above by taking $D := \max_{j \in \M} D_j$ and $\eps := \min_{j \in \M}\eps_j$, and scaling by $M$.

\subsection{Discretized Extension and Lower Bound}
\label{ss:results-lower}
A natural extension of Theorem~\ref{thm:ub} is if the agents do not communicate their full real-valued expectation (which may require infinitely many bits), but a discretized version of the current expectation, corresponding to whether it is above or below a given threshold (defined below), e.g. ``High'', ``Medium'', or ``Low'' (requiring only 2 bits).
We show the bound from Theorem~\ref{thm:ub} remains unchanged in this case.
\begin{proposition}[Discretized Extension]\label{prop:disc}
If $N$ agents only communicate their \emph{discretized} expectations, then they will \agree-agree with overall failure probability $\delta$ across $M$ tasks as defined in \eqref{eq:eps-delta}, after $T = O\left(MN^2 D + \dfrac{M^3N^7}{\eps^2\delta^2}\right)$ messages, where $D := \max_{j \in \M} D_j$ and $\eps := \min_{j \in \M}\eps_j$.
Thus, for the special case of $M=1$ tasks and $N=2$ agents, this becomes $T = O\left(D + \dfrac{1}{\eps^2\delta^2}\right)$ messages before they \agree-agree with total probability $\ge 1-\delta$.
\end{proposition}
\begin{proof}
The $N$ agents will communicate with the spanning tree protocol (cf. Lemma~\ref{lem:spanning-tree-refinement}) for each task $j \in \M$, but now with discrete, rather than continuous, messages.
The discretized protocol is as follows:
Let there be a node $F_j$ that is globally accessible to all $N$ agents.
This intermediary is allowed its own prior and will see all messages between the agents (but not their inputs).
Thus, $\Pi_j^{F_j,0}(s_j) = S_j$ for all states $s_j \in S_j$, and $\left\{\Pi^{F_j,t}_j(s_j)\right\}_{s_j \in S_j}$ \emph{coarsens} the knowledge partitions at time $t$ of the $N$ agents, so all of the agents can therefore compute $E^{F_j,t}_j$.
When agent $i$ wants to send a message to its neighbor agent $k$, then agent $i$ sends ``High'' if $E^{i,t}_j > E^{F_j,t}_j + \eps_j/4$, ``Low'' if $E^{i,t}_j < E^{F_j,t}_j - \eps_j/4$, and ``Medium'' if otherwise.
After agent $i$ sends its message to agent $k$, agent $k$ then refines its knowledge partition (and $F_j$ also refines its partition), before agent $k$ sequentially sends its message relative to the current $E^{F_j,t+1}$ to the next agent down the spanning tree.
This process of proper refinement is continued until there is a common prior by Lemma~\ref{lem:common-prior}, which the $N+1$ agents (including $F_j$) then condition on to reach $\tuple{M, N+1, \eps, \delta}$-agreement (hence the $N+1$ factor in the first term to ensure there are enough proper refinements between the $N+1$ agents).
This generalizes Aaronson's discretized protocol~\citep[Theorem 6]{aaronson2005complexity} to $N > 2$ agents (and $M > 1$ tasks), which shows that between any \emph{pair} of agents with a common prior, the number of messages needed for them to $\tuple{\eps_j, \delta_j}$-agree remains \emph{unchanged} from the full protocol, where each pair leverages the intermediary agent $F_j$.
Therefore, by applying the spanning tree construction from Lemma~\ref{lem:spanning-tree}, we get the same bound in the discretized case as before of $O(((N+1)g_j^2)/(\delta_j\eps_j)^2)$ messages before the $N+1$ agents \emph{pairwise} $\tuple{\eps_j,\delta_j}$-agree, and therefore $O(((N+1)^5g_j^2)/(\delta_j\eps_j)^2)$ messages until all $\binom{N+1}{2}$ pairs of agents \emph{globally} $\tuple{\eps_j,\delta_j}$-agree, thereby ensuring that the original $N$ agents agree.
Following the rest of the proof of our Theorem~\ref{thm:ub} yields the per-task upper bound of $O\left((N+1)^2D_j + \dfrac{(N+1)^7}{\eps_j^2\delta_j^2}\right)$, where we took the worst-case value of $g_j = O(N + 1)$.
Subsuming lower-order terms in the big-$O$, we get the stated upper bound.
\end{proof}

Below is the best lower bound we can prove for \agree-agreement:
\begin{proposition}[Lower Bound]\label{prop:lb}
There exist functions $f_j$, input sets $S_j$, and prior distributions $\{\prob_j^i\}^{i \in \N}$ for all $j \in \M$, such that any protocol among $N$ agents needs to exchange $\Omega\left(M\,N^2\,\log\left(1/\eps\right)\right)$ bits\footnote{Note, unlike our upper bounds in Theorem~\ref{thm:ub} and Proposition~\ref{prop:disc}, we use bits in the lower bound in order to apply to \emph{all} possible protocols (continuous or discrete), regardless of how many bits are encoded per message. The upper bounds have to use messages (rounds) to describe either a continuous protocol (potentially infinitely many bits) as in Theorem~\ref{thm:ub}, or a discrete protocol as in Proposition~\ref{prop:disc}.} to achieve $\agree$-agreement on $\{f_j\}_{j \in \M}$, for $\eps$ bounded below by $\min_{j\in\M} \eps_j$.
\end{proposition}
\begin{proof}
For each task $j \in \M$, let the input tuple to the $N$ agents be
\begin{equation*}
\bigl(x_{1,j},\;x_{2,j},\;\dots,\;x_{N,j}\bigr) \;\in\; S_j,
\end{equation*}
where $S_j$ is defined by
\begin{equation*}
S_j \;:=\;\Bigl\{\,\langle x_{1,j},\dots,x_{N,j}\rangle \,\Bigm|\,
x_{i,j} \in \{(j-1)\cdot 2^n + 1,\dots,j \cdot 2^n\}\;\text{for all }i \in \{1,\dots,N\}\Bigr\}.
\end{equation*}
Thus, each $x_{i,j}$ is an integer\footnote{One could encode them as binary strings of length at least $n + \lceil \log_2 j\rceil$, but in this proof we do not need the explicit binary representation: the integer \emph{range sizes} themselves suffice to carry out the communication complexity lower bound.} in an interval of size $2^n$ that starts at $(j-1)\cdot 2^n + 1$.
We endow $S_j$ with the uniform common prior $\CP_j$ (which will be necessarily difficult by the counting argument below), and define
\begin{equation*}
f_j\bigl(x_{1,j},\dots,x_{N,j}\bigr) \;=\;
   \frac{\sum_{i=1}^N x_{i,j}}{2^{n+1}}.
\end{equation*}
Observe that $\sum_{i=1}^N x_{i,j}$ is minimally $N\left((j-1)\,2^n + 1\right)$ and maximally $N\,j\,2^n$.
Hence, the image of $f_j$ is contained within
\begin{equation*}
  \left[\,
    \frac{N\bigl((j-1)2^n + 1\bigr)}{2^{n+1}}
    \;,\;
    \frac{N \, j \, 2^n}{2^{n+1}}
  \right] = \left[ \frac{N(j-1)}{2} + \frac{1}{2^{n+1}}, \frac{Nj}{2} \right].
\end{equation*}
Therefore, for $j \ge 1$, each instance $f_j$ is structurally the same ``shifted'' problem, but crucially \emph{non-overlapping} for each $j \in \Nat$.
So it suffices to show that for each $j$, each instance \emph{individually} saturates the $\Omega\left(N^2\,\log(1/\eps_j)\right)$ bit lower bound, which we will do now:

\emph{Two-Agent Subproblem for $N$ Agents.}
Because \emph{all} agents must $\tuple{\eps_j,\delta_j}$-agree on the value of $f_j$, it follows that in particular, every pair of agents (say $(i,k)$) must have expectations of $f_j$ that differ by at most $\eps_j$ with probability at least $1-\delta_j$.
But for any fixed pair $(i,k)$, we can treat $\left(x_{i,j}, x_{k,j}\right)$ as a two‐agent input in which all other coordinates $x_{\ell,j}$ for $\ell\neq i,k$ are ``known'' from the perspective of these two, or do not affect the difficulty
except to shift the sum\footnote{Equivalently, imagine the other $N-2$ agents are ``dummy'' participants, and we fix their inputs from the perspective of the $(i,k)$ pair.}.
Hence, for each $j$ and each pair $(i,k)$, there is a
two‐agent subproblem.
We claim that these two agents alone already face a lower bound of $\Omega\left(\log(1/\eps_j)\right)$ bits of communication to achieve $\tuple{\eps_j,\delta_j}$-agreement on $f_j$.

Suppose agent~$k$ sends only $t<\log_2\left(\tfrac{1-\delta_j}{\eps_j}\right)$ bits to agent $i$ about its input $x_{k,j}$.
Label the $2^t$ possible message sequences by $m=1,\dots,2^t$, with probability $p^m_j$ each.
Since $x_{k,j}$ is uniform in an interval of size $2^n$, then conditioned on message $m$, there remain at least $2^n p^m_j$ possible values of $x_{k,j}$.
Each unit change in $x_{k,j}$ shifts $f_j$ by $1/2^{n+1}$, so even if agent $i$'s estimate is optimal, the fraction of $x_{k,j}$ values producing $|E^{k,t}_j - E^{i,t}_j| \leq \eps_j$ is at most
\begin{equation*}
\frac{2^{n+1}\,\varepsilon_j}{2^n\,p^m_j}\;=\;\frac{2\,\varepsilon_j}{p^m_j}.
\end{equation*}
Hence, the total probability of agreement (over all messages $m$) is bounded by
\begin{equation*}
\sum_{m=1}^{2^t} p^m_j \,\cdot\, \frac{2\,\eps_j}{p^m_j} = 2\,\eps_j\,2^t.
\end{equation*}
If $2\,\eps_j\,2^t < 1-\delta_j$, the agents fail to
$\tuple{\eps_j,\delta_j}$-agree.
Equivalently, $t \ge \log_2\left(\tfrac{1-\delta_j}{\,2\,\eps_j}\right)$.
Since every pair $(i,k)$ needs $\Omega\left(\log\left(1/\eps_j\right)\right)$ bits for each of $M$ tasks, and there are $\binom{N}{2} = \Theta(N^2)$ pairs, the total cost is
\begin{equation*}
\Omega\left(M\,N^2\,\log\left(1/\eps\right)\right),
\end{equation*}
where $\eps := \min_{j\in \M} \eps_j$, corresponding to the most ``stringent'' task $j$.
\end{proof}

Thus, by Proposition~\ref{prop:lb}, there does \emph{not} exist an \agree-agreement protocol that can exchange less than $\Omega\left(M\,N^2\,\log\left(1/\eps\right)\right)$ bits for \emph{all} $f_j, S_j$, and prior distributions $\{\prob_j^i\}^{i \in \N}$.
For if it did, then we would reach a contradiction for the particular construction in Proposition~\ref{prop:lb}.
Note that the linear dependence on $M$ can mean an exponential number of bits in the lower bound if we have that many \emph{distinct} tasks (or agents).

\subsection{Computationally Bounded Agents}
\label{ss:results-bounded}
Thus far, we have examined computationally unbounded rational agents, where we implicitly assumed that constructing and sending each message takes $O(1)$ time.
We also assumed it took $O(1)$ time to find the common prior amongst the posterior belief states to then condition on.
Furthermore, we implicitly assumed that access to (e.g. sampling of) each agents' distributions was $O(1)$ time.
Nevertheless, in this idealized scenario, we see that the linear scaling with respect to the task space size in Theorem~\ref{thm:ub} represents the main significant computational barrier if the task space is exponentially large in the length of the agents' inputs, as well as having exponentially many distinct tasks (or agents), by the lower bound in Proposition~\ref{prop:lb}.

However, one might be tempted to say, ``Well, this is \emph{only} a linear scaling with respect to the task space size in the most ideal setting.
How much more computationally difficult does \agree-agreement generally become when we remove all of the assumptions above?''
Furthermore, what if the messages have noise in them, in order to start to account for (at a very basic level) obfuscated intent~\citep{barnes2020debateobf}?

We address all of these questions here, by analyzing the communication complexity of $N$ \emph{computationally bounded} rational agents.
This is also important in the context of agents today, since the Bayes-suboptimality we see in current LLM agents can be partly attributed to the fact that they are computationally bounded.
One thing we additionally account for is that in practice, querying a human is often much more costly than querying an AI agent.
Therefore, we assume we have $q$ humans that take time $T_H$ steps to query, and $N-q$ AI agents that take $T_{AI}$ steps to query.
Note that the latter quantification takes into account recent reasoning models that take multiple ``reasoning'' steps to produce an outcome (e.g. as of this writing, OpenAI's o1 model~\citep{openai_o1} and Google Deepmind's Gemini 2.0 Flash~\citep{gemini_2_0_flash}).
We can also do this without loss of generality, since we could of course have \emph{individual} $T_{H_y}$ and $T_{{AI}_z}$ steps for \emph{each} human $y \in [q]$ and AI agent $z \in [N-q]$, but the upper bounds we derive here in Theorem~\ref{thm:bounded} will remain the same by simply taking the maximum within each group.
Instead, what we will do is differentiate between the two groups is the time complexity of two basic subroutines:
\begin{requirement}[Basic Capabilities of Bounded Agents]\label{req:bounded-cap}
We expect the agents to be able to:
\begin{enumerate}
\item \textbf{Evaluation:} The $N$ agents can each evaluate $f_j(s_j)$ for any state $s_j \in S_j$, taking time $T_{\text{eval},a}$ steps for $a \in \{H, AI\}$.
\item \textbf{Sampling:} The $N$ agents can sample from the \emph{unconditional} distribution of any other agent, such as their prior $\mathbb{P}_j^i$, taking time $T_{\text{sample},a}$ steps for $a \in \{H, AI\}$.
\end{enumerate}
\end{requirement}
Note that we will be treating these subroutines as black boxes, where the agents do not get explicit descriptions of $f_j$ and the distributions (just as they would not necessarily get this in the real world), but can only \emph{learn} about them through these subroutines.
Just as in CIRL, this is meant to capture how we can tell whether or not the robot has done the task we intended, without necessarily being able to spell out all the steps to get there.
Thus, what we prove can broadly apply to any implementations of them, especially with varying amounts of efficiency between querying humans and AI agents.
As a result, we will express the total complexity of \agree-agreement in terms of $T_{\text{eval},H}$, $T_{\text{eval},AI}$, $T_{\text{sample},H}$, and $T_{\text{sample},AI}$.

The justification for these subroutines is that they are the bare minimum needed to start to estimate each others' expectations, and are abilities we can imagine \emph{any} capable agent that is entering into an alignment scenario can do, in order to be considered ``capable'' in the first place.
As will become clear in the proof of Theorem~\ref{thm:bounded}, we do not even need these subroutines to be exact (as long as they can be probabilistically evaluated in polynomial time).
The second subroutine is a computationally bounded version of our earlier (standard~\citep{aumann1976agreeing,aumann1999interactive}) assumption that the agents know each others' knowledge partitions, because the state of the world $s_j \in S_j$ that they jointly experience includes it.
This is analogous to the agents possessing some ``theory of mind''~\citep{ho2022planning}, namely, being capable of (bounded) reasoning about the distributions of other agents, given that the state of the world is $s_j \in S_j$.

Finally, we will need an explicit algorithm for constructing the common prior with high probability (we can no longer assume it will take $O(1)$ time like in the unbounded agent setting of \S\ref{ss:results-ubproof}):
\begin{algorithm}[ht]
\SetAlgoNoLine
\SetAlgoInsideSkip{0pt}     % Reduce vertical space between lines
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\caption{$\textsc{ConstructCommonPrior}\Bigl(\bigl\{\Pi_j^{i,t},\,\tau^{i,t}_j\bigr\}_{i=1}^N\Bigr)$}

\label{alg:construct}

\Input{A finite state space $S_j$ of size $D_j$. 
       For each agent $i\in \N$:
       \quad A partition $\Pi_j^{i,t}=\{C_{j,k}^{i,t}\}_k$ of $S_j$.
       \quad A posterior $\tau_j^{i,t}$ on each cell 
             $C_{j,k}^{i,t}$ with 
             $\sum_{s_j\in C_{j,k}^{i,t}} \tau_j^{i,t}(s_j\mid C_{j,k}^{i,t})=1$.}
\Output{A probability distribution $p_j$ on $S_j$ that is Bayes‐consistent
        with all $\tau_j^{i,t}$, or \emph{Infeasible} otherwise.}

\SetKwFunction{ConstructCommonPrior}{ConstructCommonPrior}
\ConstructCommonPrior{}:\;
\textbf{Step 1.} Let \(\displaystyle \Pi_j^* \gets \bigwedge_{i=1}^N \Pi_j^{i,t}\)
\tcp*[r]{All nonempty intersections.}
Let \(\Pi_j^* = \{Z_1,\dots, Z_r\}\), \(Z_\alpha \subseteq S_j\)\;

\textbf{Step 2.} For each \(Z_\alpha \in \Pi_j^*\), introduce \(p_j(Z_\alpha)\ge0\)
\tcp*[r]{prior mass variables.}

\textbf{Step 3.} For each agent $i\in\N$ and cell \(C_{j,k}^{i,t}\in \Pi_j^{i,t}\):
let \(\{Z_\alpha\}\subset C_{j,k}^{i,t}\)\;
\ForEach{\(Z_\alpha \subseteq C_{j,k}^{i,t}\)}{
  \ForEach{\(s_j,\,s_j' \in Z_\alpha\)}{
     \(\tfrac{p_j(s_j)}{p_j(s_j')}=
       \tfrac{\tau_j^{i,t}(s_j\,\mid\,C_{j,k}^{i,t})}
             {\tau_j^{i,t}(s_j'\,\mid\,C_{j,k}^{i,t})}\)\tcp*[r]{pairwise ratios}
  }
}

\textbf{Step 4.} Add normalization constraint:
\(\displaystyle \sum_{\alpha=1}^r p_j(Z_\alpha)=1\)\;

\textbf{Step 5.} Solve via LP feasibility\;
\If{a nonnegative \(\{p_j(Z_\alpha)\}\) exists}{
  Construct \(p_j\) on each \(s_j\in Z_\alpha\) via the ratio constraints\;
  \Return \(p_j\)\;
}
\Else{\Return \textsc{Infeasible}\tcp*[r]{No single prior can generate the posteriors}}
\end{algorithm}

We refer the reader to Appendix \S\ref{app:proofs} for proofs related to Algorithm~\ref{alg:construct}.
Specifically, Lemma~\ref{lem:cp-correctness} (correctness), Lemma~\ref{lem:cp-runtime} (runtime analysis), and Lemma~\ref{lem:approx-cp} (inexact posterior access setting). 

The above considerations lead to the following theorem in the bounded agent setting:

\begin{theorem}[Bounded Agents Eventually Agree]\label{thm:bounded}
Let there be $N$ computationally bounded rational agents (consisting of $1 \le q < N$ humans and $N-q \ge 1$ AI agents), with the capabilities in Requirement~\ref{req:bounded-cap}.
The agents pass messages according to this protocol with added triangular noise of width $\le 2\alpha$, where $\eps/50 \le \alpha \le \eps/40$.
Let $\delta^{\text{find\_CP}}$ be the maximal failure probability of the agents to find a task-specific common prior across all $M$ tasks, and let $\delta^{\text{agree\_CP}}$ be the maximal failure probability of the agents to come to \agree-agreement across all $M$ tasks once they condition on a common prior, where $\delta^{\text{find\_CP}} + \delta^{\text{agree\_CP}} < \delta$.
Let $B \ge 1/\alpha$ be a sufficiently large protocol-specific parameter that sets the ``boundedness'' of the agents, to be defined below (and in the proof).
For the $N$ computationally bounded agents to \agree-agree with total probability $\ge 1-\delta$, takes time
\begin{equation*}
\begin{split}
&O\Biggl(M\,{B}^{\,N^2 D\, \frac{\ln\bigl(\delta^{\text{find\_CP}}/(N^2 D)\bigr)}{\ln(1/\alpha)}} 
\Bigl( q\,T_{\text{sample},H} + (N-q)\,T_{\text{sample},AI} + q\,T_{\text{eval},H} + (N-q)\,T_{\text{eval},AI} \Bigr) \\
& \quad{} + M\,{B}^{\,\frac{M^2 N^7}{\left(\delta^{\text{agree\_CP}}\eps\right)^2}}
\Bigl( q\,T_{\text{sample},H} + (N-q)\,T_{\text{sample},AI} + q\,T_{\text{eval},H} + (N-q)\,T_{\text{eval},AI} \Bigr)
\Biggr).
\end{split}
\end{equation*}
For the particular setting of a single human and AI agent aligning on a single task ($M=1$, $N=2$, $q=1$), this simplifies to:
\begin{equation*}
\begin{split}
&O\Biggl({B}^{\,4D\, \frac{\ln\bigl(\delta^{\text{find\_CP}}/(4D)\bigr)}{\ln(1/\alpha)}} 
\Bigl(T_{\text{sample},H} + T_{\text{sample},AI} + T_{\text{eval},H} + T_{\text{eval},AI} \Bigr) \\
& \quad{} + {B}^{\,\frac{128}{\left(\delta^{\text{agree\_CP}}\eps\right)^2}}
\Bigl(T_{\text{sample},H} + T_{\text{sample},AI} + T_{\text{eval},H} + T_{\text{eval},AI} \Bigr)
\Biggr).
\end{split}
\end{equation*}
In other words, just in the first term alone, \emph{exponential} in the task space size $D$ and number of agents $N$ (and exponential in the number of tasks $M$ in the second term).
So if the task space size is in turn exponential in the input size, then this would already be \emph{\uline{doubly exponential}} in the input size!
\end{theorem}

We now clarify why we let $B$ be a parameter, and give a concrete example of how bad this exponential dependence can be.
Intuitively, we can think of $B$ as a ``gauge'' on how distinguishable the bounded agents are from ``true'' unbounded Bayesians, and will allow us to give an explicit desired value for $B$.
Recognizing the issue of computational boundedness of agents in the real world, Hanson~\citep{hanson2003bayesian} defined a ``Bayesian wannabe'' as a decision agent who can make sense of what their estimated expectations would be, had they had sufficient computational abilities to be a Bayesian.
He showed that if they agree to disagree about this expectation, then they must also agree to disagree about another random variable that is \emph{independent} of their world (task) state.
In other words, disagreements for Bayesian wannabes are not due to differing information about the state of the world.
Subsequently, Aaronson~\citep{aaronson2005complexity} gave an explicit protocol by which two Bayesian wannabes who share a common prior, and agree about all state-independent variables, would come to agree about their expectations.
His protocol is defined such that if an unbounded Bayesian referee examined the transcript of messages exchanged between the two bounded agents, then the referee would be \emph{unable} to statistically distinguish whether the agents were truly unbounded Bayesians, or two computationally bounded Bayesian wannabes who were simulating the protocol.
This criterion can be seen as a ``Bayesian Turing Test'': just as the original Turing Test~\citep{turing1950computing} equates indistinguishability in behavior with true intelligence, here, statistical indistinguishability from unbounded Bayesians serves as a proxy for Bayesian rationality.
Of course, accepting this analogy does not require accepting the premise of the Turing Test itself---it simply illustrates the intuition behind the definition.

We consider the $M$-function, $N$-agent generalization of this requirement (and \emph{without} common priors (CPA)), which we call a ``total Bayesian wannabe'':
\begin{definition}[Total Bayesian Wannabe]\label{def:total-wannabe}
Let the $N$ agents have the capabilities in Requirement~\ref{rem:agree}.
For each task $j \in \M$, let the transcript of $T$ messages exchanged between $N$ agents be denoted as $\Xi_j := \tuple{m_j^1,\dots,m_j^T}$.
Let their initial, task-specific priors be denoted by $\{\prob_j^i\}^{i \in \N}$.
Let $\mathcal{B}(s_j)$ be the distribution over message transcripts if the $N$ agents are unbounded Bayesians, and the current task state is $s_j \in S_j$.
Analogously, let $\mathcal{W}(s_j)$ be the distribution over message transcripts if the $N$ agents are ``total Bayesian wannabes'', and the current task state is $s_j \in S_j$.
Then we require for all Boolean functions\footnote{Without loss of generality, we assume that the current task state $s_j$ and message transcript $\Xi_j$ are encoded as binary strings.} $\Phi(s_j,\Xi_j)$,
\begin{equation*}
\left\|\mathop{\mathbb{P}}_{\substack{\Xi_j \in \mathcal{W}(s_j) \\ s_j \in \{\prob_j^i\}^{i \in \N}}}\left[\Phi(s_j, \Xi_j) = 1\right] 
- \mathop{\mathbb{P}}_{\substack{\Xi_j \in \mathcal{B}(s_j) \\ s_j \in \{\prob_j^i\}^{i \in \N}}}\left[\Phi(s_j, \Xi_j) = 1\right]\right\|_1 \le \rho_j, \quad \forall j \in \M.
\end{equation*}
We can set $\rho_j \in \R$ as arbitrarily small as preferred, and it will be convenient to only consider a single $\rho := \min_{j \in \M} \rho_j$ without loss of generality (corresponding to the most ``stringent'' task $j$).
\end{definition}
We will show in Appendix \S\ref{app:runtime} that matching this requirement amounts to picking a large enough value for $B$, giving rise to the following corollary to Theorem~\ref{thm:bounded}:
\begin{corollary}[Total Bayesian Wannabes Totally Wanna Agree If Given Enough Time!]\label{cor:wannabe-agree}
Let there be $N$ total Bayesian wannabes, according to Definition~\ref{def:total-wannabe} (e.g. consisting of $1 \le q < N$ humans and $N-q \ge 1$ AI agents).
The agents pass messages according to this protocol with added triangular noise of width $\le 2\alpha$, where $\eps/50 \le \alpha \le \eps/40$.
Let $\delta^{\text{find\_CP}}$ be the maximal failure probability of the agents to find a task-specific common prior across all $M$ tasks, and let $\delta^{\text{agree\_CP}}$ be the maximal failure probability of the agents to come to \agree-agreement across all $M$ tasks once they condition on a common prior, where $\delta^{\text{find\_CP}} + \delta^{\text{agree\_CP}} < \delta$.
For the $N$ total Bayesian wannabes to \agree-agree with total probability $\ge 1-\delta$, takes time
\begin{equation*}
\begin{split}
&O\Biggl(M\,{B}^{\,N^2 D\, \frac{\ln\bigl(\delta^{\text{find\_CP}}/(N^2 D)\bigr)}{\ln(1/\alpha)}} 
\Bigl( q\,T_{\text{sample},H} + (N-q)\,T_{\text{sample},AI} + q\,T_{\text{eval},H} + (N-q)\,T_{\text{eval},AI} \Bigr) \\
& \quad{} + M\,\frac{\left(11/\alpha\right)^{\frac{M^6 N^{21}}{\left(\delta^{\text{agree\_CP}}\epsilon\right)^6}}}{\rho^{\frac{2M^2 N^7}{\left(\delta^{\text{agree\_CP}}\epsilon\right)^2}}}
\Bigl( q\,T_{\text{sample},H} + (N-q)\,T_{\text{sample},AI} + q\,T_{\text{eval},H} + (N-q)\,T_{\text{eval},AI} \Bigr)
\Biggr).
\end{split}
\end{equation*}
For the particular setting of a single human and AI agent aligning on a single task ($M=1$, $N=2$, $q=1$), this simplifies to:
\begin{equation*}
\begin{split}
&O\Biggl({B}^{\,4D\, \frac{\ln\bigl(\delta^{\text{find\_CP}}/(4D)\bigr)}{\ln(1/\alpha)}} 
\Bigl(T_{\text{sample},H} + T_{\text{sample},AI} + T_{\text{eval},H} + T_{\text{eval},AI} \Bigr) \\
& \quad{} + \frac{\left(11/\alpha\right)^{\frac{2097152}{\left(\delta^{\text{agree\_CP}}\epsilon\right)^6}}}{\rho^{\frac{256}{\left(\delta^{\text{agree\_CP}}\epsilon\right)^2}}}
\Bigl(T_{\text{sample},H} + T_{\text{sample},AI} + T_{\text{eval},H} + T_{\text{eval},AI} \Bigr)
\Biggr).
\end{split}
\end{equation*}
In other words, exponential time in the task space $D$, and by \eqref{eq:total-bayesian}, with a large base that increases minimally by $\sim 1/\rho^2$, if the ``total Bayesian wannabe'' threshold $\rho$ is made small.

Sharing a common prior amounts to removing the first term, yielding bounds that are still exponential in $\eps$ and $\delta$:
\begin{equation*}
O\left(M\,\frac{\left(11/\alpha\right)^{\frac{M^6 N^{21}}{\left(\delta^{\text{agree\_CP}}\epsilon\right)^6}}}{\rho^{\frac{2M^2 N^7}{\left(\delta^{\text{agree\_CP}}\epsilon\right)^2}}}
\Bigl( q\,T_{\text{sample},H} + (N-q)\,T_{\text{sample},AI} + q\,T_{\text{eval},H} + (N-q)\,T_{\text{eval},AI} \Bigr)
\right).
\end{equation*}
\end{corollary}
The proofs of Theorem~\ref{thm:bounded} and Corollary~\ref{cor:wannabe-agree} are quite technical (spanning 10 pages), so we defer them to Appendix \S\ref{app:proofs} for clarity.
The primary takeaway here is that computational boundedness results in a severely exponential time slowdown in the agreement time, and especially so if you want the bounded agents to be \emph{statistically indistinguishable} in their interactions with each other from true unbounded Bayesians.

For example, for a singleton task space $D = 1$ and $N=2$ agents, even if you have a liberal agreement threshold of $\eps = \delta = 1/2$ and ``total Bayesian wannabe'' threshold of $\rho = 1/2$ on one task ($M=1$), then $\alpha \ge 1/100$, so the number of \emph{subroutine calls} (not even total runtime) would be at least around:
\begin{equation*}
O\left(\frac{\left(1100\right)^{\frac{2097152}{\left(1/4\right)^6}}}{\left(1/2\right)^{\frac{256}{\left(1/4\right)^2}}}\right) = O\left(1.31 \times 10^{26125365467}\right),
\end{equation*}
would already far exceed the estimated~\citep[pg. 19]{Munafo_Notable_Numbers} number of atoms in the universe ($\sim 4.8 \times 10^{79}$)!
This is even the case when the two ``total Bayesian wannabes'' share a common prior.

\section{Discussion}
\label{sec:discussion}
We developed a game-theoretic framework, \agree-agreement, to study alignment between capable rational agents, encompassing earlier theoretical approaches as special cases (Remarks~\ref{rem:agree} and~\ref{rem:align}).
While alignment can be achieved with arbitrarily high probability (Theorem~\ref{thm:ub}, Proposition~\ref{prop:disc}, Theorem~\ref{thm:bounded}), the process can be exponentially costly.

For instance, among computationally bounded agents, communication complexity scales exponentially with task space size, number of agents, and number of tasks (Theorem~\ref{thm:bounded}). 
Even with unbounded agents (Theorem~\ref{thm:ub}), alignment remains exponential when the task space itself grows exponentially with input length---common in real-world settings.
Furthermore, it is impossible to be unilaterally improved if there are exponentially many distinct tasks (or agents) over which we want alignment, by the lower bound in Proposition~\ref{prop:lb}.

These findings suggest that ensuring high-confidence alignment for complex tasks---whether due to large state spaces, many subtasks, or distributional complexity---may be impractical, even under ideal conditions.
This holds for both computationally unbounded Bayesian agents (Theorem~\ref{thm:bounded}) and bounded agents statistically indistinguishable from unbounded Bayesians, even if they share a common prior (Corollary~\ref{cor:wannabe-agree}), especially given the higher cost of human queries compared to AI queries in practice.

\subsection{When Alignment Can Be More Feasible}
\label{ss:discussion-efficient}
More broadly, the advantage of having a mathematically rigorous and general framework for alignment allows us to also explicitly characterize \emph{how} and \emph{when} alignment can become more efficient, to guide future research and sharpen current approaches.

Thus, our current results can set a prescription for future alignment research, showing that it can be feasible in the following mutually inclusive instances:
\begin{enumerate}
\item \textbf{Cutting Down the Task Space \& Number:} If the task space over which alignment is performed is sufficiently small enough, then our upper bounds in Theorem~\ref{thm:ub}, Proposition~\ref{prop:disc}, and Theorem~\ref{thm:bounded} show that this is much more feasible in practice.
These observations motivate a line of work that avoids steering the base model, but to align it externally by simplifying the model's output task space via filtering it through a more controllable bottleneck that has provable guarantees (cf. some very recent work in this direction~\citep{pfrommer2023power,block2024provable}).
We can think of this approach as a specific prescription for \emph{``outer alignment''}~\citep{christiano2019,hubinger2019risks,ngo2022alignment}.

\par\hspace*{1.5em} Alternatively, are there improved message-passing protocols that can more efficiently ``cut through'' large task spaces?
While our existing bounds can already be sped up when the distribution is more efficiently sampleable (see below), we assumed that the message-passing protocol yielded only a single proper refinement at each step, in order to derive the general-case upper bound.
There may certainly be specific instances where the message passing is much more efficiently refining the agents' knowledge partitions.
For example, is it even more efficient when the messages more closely resemble discrete tokens that LLM agents~\citep{wang2024survey} use?
Although we found in Proposition~\ref{prop:disc} that discretization alone was not sufficient to yield a speedup over its analogous continuous protocol in the unbounded agent setting, the specifics of how the discretization is done might matter more for bounded agents.
The discretized protocol (described in Appendix \S\ref{app:msp-protocol}), used in the proofs of bounded agents in Theorem~\ref{thm:bounded} and Corollary~\ref{cor:wannabe-agree}, might be a framework by which to study the tokenized message setting for current LLM agents (assuming they do not misalign for trivial reasons, such as hallucinations).

\par\hspace*{1.5em}
Otherwise, if our current upper bounds cannot be greatly improved in most real-world scenarios, do there exist computationally harder instances that improve our lower bound in Proposition~\ref{prop:lb}?
Our lower bound already shows that we should be selective about the \emph{number} of tasks (and agents) we want alignment for, because requiring alignment over too many of either of these will be provably \emph{impossible} to guarantee efficiency.

\item \textbf{Efficient Sampleability of Posterior Distributions:} The main reason for the exponential form in Theorem~\ref{thm:bounded} is that we did not make any assumptions about the structure of the posterior distributions (the updated belief distributions of the agents conditioned on the incoming messages) in order to prove general bounds.
However, if the posterior distributions are efficiently sampleable, then the communication complexity can be significantly reduced.
For example, if the posteriors are any distribution that can be represented as a compactly factorized graphical model with small treewidth.
To give a concrete example, consider a product of $\ell$ independent Bernoullis $\mathcal{D}(\omega_1, \ldots, \omega_{\ell}) = \prod_{i=1}^{\ell} p_i^{\omega_i} (1 - p_i)^{1 - \omega_i}, \text{ with } \omega_i \in \{0, 1\} \text{ and each } 0 < p_i < 1$.
If you learn ``$\omega_{j_1} = b_{j_1}, \ldots, \omega_{j_k} = b_{j_k}$,'' you can sample from $\mathcal{D}(\cdot \mid \omega_{j_1} = b_{j_1}, \ldots, \omega_{j_k} = b_{j_k})$ in $O(\ell)$ time---just fix those bits and sample each other bit $\omega_i \sim \text{Bernoulli}(p_i)$.
While this example is just to illustrate the point and it is unlikely any real-world problems are as factorizable as a product of Bernoullis, it is an interesting open question what (if any) problems of interest exhibit efficiently sampleable distributions. 
\item \textbf{Common Prior:} While sharing a common prior accelerates alignment relative to scenarios where priors differ, Corollary~\ref{cor:wannabe-agree} showed that a common prior is not enough alone to ensure tractability in general, e.g. if the bounded agents are to act indistinguishably from unbounded Bayesians (``total Bayesian wannabes'').
Whether or not this property is desirable may depend on the application.
However, if we pair a common prior with efficient sampleability of posteriors, then this can potentially be made more tractable.
While sharing \emph{exact} common priors seems unlikely, if we assume the models are being trained on largely human-generated distributions, then it may be reasonable to assume that their prior distance \eqref{eq:prior-dist} with humans is potentially small (or even negligible) on average.

\item \textbf{Bounded Theory of Mind, Memory, and Rationality:} To prove Theorem~\ref{thm:bounded} and Corollary~\ref{cor:wannabe-agree}, we found it necessary at a minimum for the agents to have some bounded ability of ``theory of mind''.
We also assumed the agents can store their estimates of the current posteriors in memory, along with their respective sampling tree (which tracks message history).
More specifically, as spelled out in Requirement~\ref{req:bounded-cap}, this amounts to being able to sample the distributions of other agents up to a bounded\footnote{It is known that humans possess a bounded theory of mind, though it is debated exactly how many levels of recursion this bottoms out to. Some studies have shown a maximum of 4 levels~\citep{kinderman1998theory}, but for more ecologically-relevant tasks, it has been shown that humans can do this with at least 7 levels of recursion~\citep{o2015ease}. Nevertheless, efficient sampleability of the distributions (to limit the treewidth needed) will undoubtedly be important when human interlocutors are involved, as human boundedness will otherwise likely be too limited to do this tractably.} treewidth $B$.
We also assumed bounded rationality (in the sense of Simon~\citep{simon1984models}), which we will discuss more below.
We can view both of these agent qualities as explicit sufficient conditions for achieving \emph{``inner alignment''}~\citep{ngo2022alignment}, in order to be able to prove that \agree-agreement can even converge with high probability.

\item \textbf{Constraints on Obfuscated Intent:} Our model of obfuscated intent~\citep{barnes2020debateobf} was incredibly simple, but crucially needed a noise distribution added to the messages with \emph{bounded derivative} to ensure convergence in Theorem~\ref{thm:bounded} and Corollary~\ref{cor:wannabe-agree}.
This gives an explicit constraint on inner alignment scenarios, if we are to model obfuscation in this manner; for example, it is worth noting that uniform distributions will \emph{not} satisfy this requirement, though Gaussians will.
Moreover, while the noise model we considered here allowed us to nicely express the communication complexity in terms of the width $\alpha$ of the distribution, there is much room for future theoretical work for characterizing the richness of various obfuscation scenarios that go beyond additive message noise according to a prespecified distribution.
In other words, exploring various ``breakings of paradigm'' to analyze the consequences on communication complexity of \agree-agreement is a fruitful direction.
We discuss these ``breakings'' more in what follows below.
\end{enumerate}

\subsection{Critiques}
\label{ss:discussion-critiques}
In what remains, a natural question for discussion is why should \emph{any} form of Aumannian agreement be seen as a theoretical framework for alignment? 
We note that we have come a long way from standard Aumannian agreement, generalizing to multiple agents and tasks, all without the Common Prior Assumption (CPA) and without requiring \emph{exact} agreement---thereby capturing prior approaches to both agreement in game theory (cf. Remark~\ref{rem:agree}) and alignment (cf. Remark~\ref{rem:align}).
However, there are certainly assumptions we have made thus far that can reasonably be challenged in practice (e.g. modeling agents as either approximately or aspirationally Bayesian).
For example, while we do not actually think of humans as Bayes-rational (which has been debunked for a long time in the social sciences, through the work of Allais and Hagen~\citep{allais_hagen_1979}, Tversky and Kahneman~\citep{tversky_kahneman_1974}, and others~\citep{windt_safety_2019}), one of the main reasons for why people are not Bayes-rational in general is that they tend to stick to their beliefs and agree with things that validate those beliefs.
While very recent work has shown that interaction with LLMs can be used to dissuade people of their strongly held beliefs better than even other people can~\citep{costello2024durably}, this is not strictly necessary in our setup.
This is because \emph{unlike} standard Aumannian agreement setups which are only between humans, our desired outcome is for the AI agent(s) to align on a function $f_j$ that the \emph{human} already internally prefers!
In other words, the human is incentivized to collaborate here, by construction.

\uline{Therefore, if something is \emph{already} inefficient in our setting (as listed above), we should avoid it in \emph{current} practice. 
This is especially true when dealing with non-cooperative or malfunctioning (and therefore non-Bayesian) agents, as it can only worsen inefficiencies and may fail to even ensure convergence to agreement.}

For example, does the low-degree exponential scaling in the number of bounded agents become doubly exponential as we further weaken the bounded theory of mind, memory, and rationality of the agents?
Which of these three factors for inner alignment affect the communication complexity more, or are all of them equally important?

It will also be an interesting theoretical direction to explore the communication complexity of other statistical forms of agreement, including those related to what we considered in \eqref{eq:eps-delta}, such as other moments of the distributions besides the expectation (though, for the most part, these can just be subsumed into the particular definition of $f_j$ one uses, and still consider expectation-based agreement).

\textbf{``Absolute'' vs. ``Relative'' AI Safety.} Finally, we purposefully took a ``relative'' definition of AI safety rather than defining safety in an absolute sense.
Specifically, we focused on alignment, where having the AI agent agree with the human, based on whatever the humans' preferences were, was the goal.
In some sense, this is no different in tact than Turing's definition of intelligence~\citep{turing1950computing}, as that which behaves indistinguishably from a human, from a human's perspective.
While there have been recent theoretical approaches to defining safety more absolutely, there is currently no consensus on a universal definition that applies more generally and extends beyond toy scenarios (see, e.g. very recent work~\citep[pg. 10]{bengio2024can} for four different definitions of what is a ``guardrail'').
Though it is unclear whether we will ever reach consensus on such a definition (much like we have not defined ``morality'' for millenia), one could argue that in practice, safety will likely be differentially defined in situation-specific ways, and always from human perspectives.
Thus, we defined safety analogously relative to a human's perspective---as a set of objectives an AI agent should align with---and in Requirement~\ref{req:bounded-cap}, 
$f_j$ need not be precisely described, only evaluated. 
This reflects the reality that we often struggle to formally specify what we want, yet can recognize when our expectations are met.
Our framework, therefore, operates without requiring an explicit definition of what constitutes ``safe'' in an absolute sense.

However, it is worth mentioning that we considered general underlying mechanisms behind reaching \agree-agreement, involving (1) iterative reasoning, (2) mutual updating, (3) common knowledge, and (4) convergence under shared frameworks, which flexibly encompass many scenarios.
These considerations suggest that if the time comes, more explicit universal values for safety can be modeled within a Bayesian framework by considering preferences as latent variables that agents infer through observations of behavior, communication, or revealed preferences. 
Thus, our theoretical framework is general enough that encoding additional values amounts to ensuring that priors and evidence are sufficiently rich to capture what we want, rather than needing to change the entire \emph{framework} itself.
We believe the flexibility of our \agree-agreement paradigm presents opportunities to intentionally ``break'' it in a \emph{hypothesis-driven} manner for future analyses and applications, such as exploring how other forms of optimality (or suboptimality) impact communication complexity beyond the computational boundedness considered here, as well as examining more complex scenarios involving agents forming and breaking coalitions with differing amounts of capabilities beyond the differing computation times we considered for humans and AI agents.

One could still reasonably ask, ``We live in a society where people do not agree most of the time, but nothing civilization-ending has happened (yet)?''
There are two responses to this.
One is that not \emph{all} tasks require a high \agree-agreement to avoid a civilization-ending catastrophe.
For example, consider the tasks of preparing a sandwich vs. welding parts for an aircraft vs. running a nuclear power plant.
Not to mention, our lower bound in Proposition~\ref{prop:lb} fundamentally implies that we cannot guarantee efficient alignment across \emph{all} tasks, so we should choose wisely anyhow.
Thus, the thresholds we set here are intentionally ``free parameters'' set ahead of time by public policy~\citep{bengio2025international}.
For certain tasks, there may also be theoretical guarantees of what minimal thresholds avoid dangerous situations with sufficiently high probability.
Second, as AI is a technology that we are intentionally \emph{creating}, especially but not limited to the subset of tasks that require a high \agree-agreement threshold to avoid unpleasant consequences, we have to hold it to a higher standard than we do for other humans.

In a certain light, a sobering yet ``Pareto-optimal worst-case'' scenario is that even if humans fail to sustain themselves for various reasons, AI may yet persist as our species' intellectual legacy. 
Our responsibility, then, is to instill the values we wish it to carry forward. 
Over 30 years ago, Marvin Minsky expressed a similar sentiment:
\par
\epigraph{
    ``Will robots inherit the earth? Yes, but they will be our children. 
    We owe our minds to the deaths and lives of all the creatures that 
    were ever engaged in the struggle called Evolution. Our job is to 
    see that all this work shall not end up in meaningless waste.''}
    {\textit{Marvin Minsky}~\citep{minsky1994will}, 1994}
\par
%\newpage