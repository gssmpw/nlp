\section{Related work}
\label{sec:rel}

\paragraph{ModernBERT}

BERT \cite{Devlin:2019} was one of the first major successes of applying the Transformer
architecture \cite{Vaswani:2017} to natural language understanding. BERT uses only the Transformer's
encoder blocks in a bidirectional fashion, allowing it to learn context from both directions. As a result, BERT quickly
became the backbone of many NLP pipelines for tasks like classification, question answering, named entity recognition, etc.

BERT's initial design included certain limitations, such as a maximum sequence length of 512 tokens and less efficient attention mechanisms, leaving
room for architectural upgrades and larger-scale training. Despite the current rise of popularity of LLM-based architectures in NLP, such as GPT-4 \cite{Openai:2024}, Mistral \cite{Jiang:2023} or Llama-3 \cite{Grattafiori:2024}, encoder-based models are still widely used in many applications, because of their much smaller size and better-suited inference requirements that make them suitable for real-world applications.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{files/demo.png}
    \caption{A web demo of our application built in Streamlit\footnote{\url{streamlit.io}}. It features three input fields: question, context, and answer. The output shows the highlighted hallucinated spans.}
    \label{fig:demo}
\end{figure}

ModernBERT \cite{Warner:2024} is a state-of-the-art encoder-only transformer architecture that incorporates several
modern design improvements over the original BERT model. It utilizes several enhancements, including rotary positional embeddings (RoPE) \cite{Su:2024} instead of traditional absolute positional embeddings. Additionally, it features an alternating local-global attention mechanism as described in \cite{Gemma:2024}, allowing it to efficiently manage sequences of up to 8,192 tokens. This makes it significantly more effective for long-context tasks, such as modern information retrieval \cite{Nussbaum:2025, Zhang:2024}. ModernBERT features a hardware-aware design and an expanded training corpus of 2 trillion tokens, including textual and code data. As a result, it achieves superior performance on various downstream benchmarks, such as GLUE for classification and BEIR for retrieval (while also maintaining faster inference speed) \cite{Nussbaum:2025, Zhang:2024}. Based on these findings, the main part of our paper is to use the advancements of ModernBERT in the hallucination detection of LLMs in an RAG setting. In this domain, long-context awareness is an inevitable feature.


\paragraph{Hallucination Detection} can vary in granularity, ranging from example-based detection (which assesses if an answer contains hallucinations) to token, span, or sentence-level detection \cite{Niu:2024}. The methods for detecting hallucinations also differ based on the techniques employed.

\paragraph{Prompt-based Techniques} typically utilize zero or few-shot large language models (LLMs) to identify hallucinations in LLM-generated responses. Few-shot or fine-tuned evaluation frameworks, such as RAGAS \cite{Es:2024}, Trulens\footnote{\url{https://www.trulens.org/}}, and ARES \cite{SaadFalcon:2024}, have emerged to provide hallucination detection at scale using LLM judges. However, real-time prediction remains a challenge for these methods. Other prompt-based approaches, like the zero-shot method SelfCheckGPT \cite{Manakul:2023}, employ stochastic sampling to identify inconsistencies across multiple response variants. Rather than relying on a single prompt, Chainpoll \cite{Friel:2023} implements a series of verification steps to detect hallucinations. \citet{Cohen:2023} presents a method of cross-examination between two LLMs to uncover inconsistencies. \citet{Chang:2024} utilized LLM-based classifiers trained on synthetic errors to detect both hallucinations and coverage errors in LLM-generated responses.

\paragraph{Fine-tuned LLM Judges} approaches involve training LLMs on hallucination detection tasks using specific training data. \citet{Niu:2024} not only introduced the RagTruth data but also presented a fine-tuned \texttt{Llama-2-13B} LLM, which achieved state-of-the-art performance on their test set, even surpassing larger models like GPT-4. RAG-HAT \cite{Song:2024} introduced a novel approach called Hallucination Aware Tuning (HAT), which involves training models to generate detection labels and provide detailed descriptions of identified hallucinations. They created a preference dataset to facilitate Direct Preference Optimization (DPO) training. Fine-tuning through DPO results in SOTA performance on the RAGTruth test set.

\paragraph{Encoder-based Solutions} focus on addressing computational efficiency constraints through domain-specific adaptations. RAGHalu \cite{Zimmerman:2024} employs a two-tiered encoder model that utilizes binary classification at each layer, fine-tuning a Natural Language Inference (NLI) model based on DeBERTa \cite{He:2021}. The approach most similar to our work is Luna \cite{Belyi:2025}, which also builds on DeBERTa and NLI to create a lightweight long-context hallucination detection system capable of managing longer contexts effectively. Luna draws connections between detecting entailment in NLI tasks and identifying hallucinations. They fine-tuned on a large, cross-domain corpus of question-answering-based RAG samples, with annotations provided by GPT-4. During the inference phase, Luna conducts sentence- or token-level checks on each model's response against the retrieved passages, effectively flagging unsupported fragments. FACTOID \cite{Rawte:2024} introduces a Factual Entailment (FE) framework, which represents a new form of textual entailment aimed at locating hallucinations at the token or span level. Other approaches, such as ReDeEp \cite{Sun:2025}, introduce techniques to analyze internal model states for hallucination detection.