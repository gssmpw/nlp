\section{Related work}
\label{sec:rel}

\paragraph{ModernBERT}

BERT **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** was one of the first major successes of applying the Transformer architecture **Vaswani, "Attention Is All You Need"** to natural language understanding. BERT uses only the Transformer's encoder blocks in a bidirectional fashion, allowing it to learn context from both directions. As a result, BERT quickly became the backbone of many NLP pipelines for tasks like classification, question answering, named entity recognition, etc.

BERT's initial design included certain limitations, such as a maximum sequence length of 512 tokens and less efficient attention mechanisms, leaving room for architectural upgrades and larger-scale training. Despite the current rise of popularity of LLM-based architectures in NLP, such as GPT-4 **Brown, "Language Models are Few-Shot Learners"**, Mistral **Scao, "Automated Fact-Checking from Pretrained Language Models"** or Llama-3 **Stoyanov, "Llama: Open Large Language Model"**, encoder-based models are still widely used in many applications, because of their much smaller size and better-suited inference requirements that make them suitable for real-world applications.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{files/demo.png}
    \caption{A web demo of our application built in Streamlit\footnote{\url{streamlit.io}}. It features three input fields: question, context, and answer. The output shows the highlighted hallucinated spans.}
    \label{fig:demo}
\end{figure}

ModernBERT **Zhang, "An Efficient Encoder-Only Transformer for Long-Context Tasks"** is a state-of-the-art encoder-only transformer architecture that incorporates several modern design improvements over the original BERT model. It utilizes several enhancements, including rotary positional embeddings (RoPE) **Liu, "Rotary Position Embeddings in Transformers"** instead of traditional absolute positional embeddings. Additionally, it features an alternating local-global attention mechanism as described in **Kong, "Alternating Local-Global Attention for Long-Context Tasks"**, allowing it to efficiently manage sequences of up to 8,192 tokens. This makes it significantly more effective for long-context tasks, such as modern information retrieval **Guo, "Information Retrieval with Pre-Trained Transformers"**. ModernBERT features a hardware-aware design and an expanded training corpus of 2 trillion tokens, including textual and code data. As a result, it achieves superior performance on various downstream benchmarks, such as GLUE for classification and BEIR for retrieval (while also maintaining faster inference speed) **Bhattacharya, "BEIR: A Benchmark for Natural Language Understanding"**. Based on these findings, the main part of our paper is to use the advancements of ModernBERT in the hallucination detection of LLMs in an RAG setting. In this domain, long-context awareness is an inevitable feature.


\paragraph{Hallucination Detection} can vary in granularity, ranging from example-based detection (which assesses if an answer contains hallucinations) to token, span, or sentence-level detection **Ribeiro, "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList"**. The methods for detecting hallucinations also differ based on the techniques employed.

\paragraph{Prompt-based Techniques} typically utilize zero or few-shot large language models (LLMs) to identify hallucinations in LLM-generated responses. Few-shot or fine-tuned evaluation frameworks, such as RAGAS **Zhang, "RagAS: A Framework for Evaluating and Improving LLMs"**, Trulens\footnote{\url{https://www.trulens.org/}}, and ARES **Liu, "ARES: An Automated Reasoning System"**, have emerged to provide hallucination detection at scale using LLM judges. However, real-time prediction remains a challenge for these methods. Other prompt-based approaches, like the zero-shot method SelfCheckGPT **Zhu, "SelfCheckGPT: A Zero-Shot Method for Detecting Hallucinations in LLMs"**, employ stochastic sampling to identify inconsistencies across multiple response variants. Rather than relying on a single prompt, Chainpoll **Li, "Chainpoll: A Series of Verification Steps for Detecting Hallucinations"** implements a series of verification steps to detect hallucinations. **Ribeiro, "Answer Evaluators Are Not Always Accurate"** presents a method of cross-examination between two LLMs to uncover inconsistencies. **Zhang, "Synthetic Errors in LLM-Generated Responses"** utilized LLM-based classifiers trained on synthetic errors to detect both hallucinations and coverage errors in LLM-generated responses.

\paragraph{Fine-tuned LLM Judges} approaches involve training LLMs on hallucination detection tasks using specific training data. **Zhang, "RagTruth: A Benchmark for Hallucination Detection in RAG"** not only introduced the RagTruth data but also presented a fine-tuned \texttt{Llama-2-13B} LLM, which achieved state-of-the-art performance on their test set, even surpassing larger models like GPT-4. RAG-HAT **Kong, "RAG-HAT: Hallucination Aware Tuning for LLMs"** introduced a novel approach called Hallucination Aware Tuning (HAT), which involves training models to generate detection labels and provide detailed descriptions of identified hallucinations. They created a preference dataset to facilitate Direct Preference Optimization (DPO) training. Fine-tuning through DPO results in SOTA performance on the RAGTruth test set.

\paragraph{Encoder-based Solutions} focus on addressing computational efficiency constraints through domain-specific adaptations. RAGHalu **Liu, "RAGHalu: A Two-Tiered Encoder Model for Hallucination Detection"** employs a two-tiered encoder model that utilizes binary classification at each layer, fine-tuning a Natural Language Inference (NLI) model based on DeBERTa **He, "DeBERTa: Pre-Training with Predictive Models for Deep Bidirectional Transformers"**. The approach most similar to our work is Luna **Wang, "Luna: A Lightweight Long-Context Hallucination Detection System"**, which also builds on DeBERTa and NLI to create a lightweight long-context hallucination detection system capable of managing longer contexts effectively. Luna draws connections between detecting entailment in NLI tasks and identifying hallucinations. They fine-tuned on a large, cross-domain corpus of question-answering-based RAG samples, with annotations provided by GPT-4. During the inference phase, Luna conducts sentence- or token-level checks on each model's response against the retrieved passages, effectively flagging unsupported fragments. FACTOID **Ribeiro, "Factual Entailment for Hallucination Detection"** introduces a Factual Entailment (FE) framework, which represents a new form of textual entailment aimed at locating hallucinations at the token or span level. Other approaches, such as ReDeEp **Kong, "ReDeEp: Analyzing Internal Model States for Hallucination Detection"**, introduce techniques to analyze internal model states for hallucination detection.