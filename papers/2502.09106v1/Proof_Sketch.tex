% \subsection{Warm-up: Updates of the Well-specified Quadratically-parameterized Model}
% \textbf{One-dimensional case:} In order to build intuition for analyzing the first-order dynamics of the well-specified quadratically-parameterized model, we start by studying a highly simplified one-dimensional random walk. Actually, attentive readers can verify that when $M=1$, the iterative step of Algorithm \ref{SGD} degenerates to the following dynamic:
% \begin{equation}\label{1-d-dynamic}
%     v\leftarrow v-\eta x^2\left(v^2-(v^*)^2\right)v+\eta\omega xv,
% \end{equation}
% where $\omega$ and $x$ are independent random variables with zero-mean. Ignoring the second stochastic term $\eta\omega xv$, one can observe from Eq.~\eqref{1-d-dynamic} that when $v^*\gg v>0$, the update mechanism in Algorithm \ref{SGD} guarantees $v$ to increase linearly. When $v>v^*$, the update mechanism tends to pull $v$ back towards the neighborhood of $v^*$. Moreover, if $v$ rises into the neighborhood of $v^*$ and continues oscillating within this region, then Eq.~\eqref{1-d-dynamic} is similar to the SGD iteration for linear regression with well-specified noise. According to aforementioned intuition, we divide the proof of Theorem \ref{theorem-3} into two phases. In phase I, we provide related analysis to demonstrate that starting from a small initialization, SGD will rise into the neighborhood of $\upv_{1:M}^*$ with high probability. In phase II, we show that after a sufficient number of iterations, SGD will continue to remain in the neighborhood of $\upv_{1:M}^*$ with high probability. Consequently, we can adapt the convergence analysis techniques of SGD for linear regression with well-specified noise to achieve the desired convergence results.

% \textbf{High-dimensional case: }In high-dimensional case, the first stochastic term in Eq.~\eqref{1-d-dynamic} evolves into $\langle\upv^{\odot2}-\upv_{1:M}^{*\odot2},\upx_{1:M}\rangle\upx_j\upv_j$ for any coordinate $j$. One can notice that the coupling between different covariates $\upx_i$ makes the stochastic term in SGD more complex. Reassuringly, for each coordinate, the dynamic under the probabilistic expectation of SGD iterations aligns with that of Eq.~\eqref{1-d-dynamic}. Hence, the analytical approach used in the one-dimensional case remains effective, albeit requiring additional techniques to control the impact of stochastic errors during the iterative process. It is worth to note that our techniques rely solely on following more relaxed assumption.
In this section, we introduce the proof techniques sketch of our main result Theorem \ref{theorem-3}. Specifically, the dynamics and analysis of SGD can be divided into two phases. In \textbf{Phase I (Adaptation)}, SGD autonomously truncates the top $D$ coordinates as $\mathcal{S}$ without requiring explicit selection of $D$. Algorithm \ref{SGD} can converge these coordinates to a neighborhood of their optimal solutions within $T_1$ iterations with high probability, and realize the trade-off between two terms $\frac{D}{T}$ and $\frac{1}{D^{\beta -1}}$ in the risk bound through precise choice of step size. 
In \textbf{Phase II (Estimation)}, global convergence to the risk minimizer is achieved over $T_2$ iterations, which can be approximated as SGD with geometrically decaying step sizes applied to a linear regression problem in the reparameterized feature space $\Pi_M\upx\odot\upv_{1:M}^*$. %\congfang{identifies is harmful }

%The dynamic of SGD and our analysis can be basically divided into two phases. In the \textbf{Phase I} named ``adaptation" phase, we demonstrate that SGD can adaptively identify the first $N$ coordinates as the optimal set $\mathcal{S}$ without explicit selection of $N$, and bound such $N$ coordinates near the corresponding optimal solutions by $T_1$ iterations with high probability. Then we turn to the following \textbf{Phase II} with $T_2$ iterations named ``estimation" phase where we establish the global convergence of Algorithm \ref{SGD} for risk minimization. The analysis of Algorithm \ref{SGD}'s iterations can be approximated to SGD with geometrically decaying step sizes on a linear regression problem with reparameterized features $\Pi_M\upx\odot\upv_{1:M}^*$.

\subsection{Phase I: }
During the ``adaptation" phase, Algorithm \ref{SGD} implicitly identifies the first $D$ coordinates as the effective dimension set $\mathcal{S} = [1:D]$. For each $i \in \mathcal{S}$, $\upv_i^{T_1}$ converges with high probability to a rectangular neighborhood centered at $\upv_i^*$ with half-width $c_1\upv_i^*$. Here, $c_1 \in (0,1)$ denotes a scaling constant. For each $i\in[1:M]\setminus\mathcal{S}$, $\upv_i^{T_1}$ remains bounded above by $\frac{3}{2}\upv_i^*$ with high probability.
\begin{theorem}\label{phase-I-informal}
% [Theorem \ref{theorem_1}]
    Under Assumption \ref{ass-d}, consider a predictor trained via Algorithm \ref{SGD} with initialization $\upv^0$. Let the step size $\eta\leq\eta(D, c_1)$, for the effective dimension $D$ and the scaling constant $c_1 \in (0,1)$. 
    % If $N < M$, let $T_l(N, c_1) \leq T_1 \leq T_u(N, c_1)$ where $T_l(N, c_1), T_u(N, c_1)\in\bbN_+$ depend on $N$ and $c_1$; otherwise, let $T_1 \geq T_l(M, c_1)$ where $T_l(M, c_1)\in\bbN_+$ depends on $M$ and $c_1$. 
    The iteration number $T_1$ requires:
    \begin{align}
        T_1\in\begin{cases}
        \left[T_l(D, c_1), T_u(D, c_1)\right], \quad &\text{ if }D < M,\notag
        \\
        \left[T_l(M, c_1),\infty\right), \quad &\text{ otherwise}.
    \end{cases}
    \end{align}
    Then, with high probability, we have
    \begin{align}\label{proof-sketch-1}
        \begin{cases}
            \upv_i^{T_1}\in\left[(1-c_1)\upv_i^{*}, (1+c_1)\upv_i^{*}\right], &\text{ if }i\in\mathcal{S},
            \\
            \upv_i^{T_1}\in\left[0, \frac{3}{2}\upv_i^*\right], &\text{ otherwise }.
        \end{cases}
    \end{align}
\end{theorem}
To characterize the mainstream dynamic, our analysis employs a probabilistic sequence synchronization technique. That is, from the Algorithm \ref{SGD}-generated sequence $\{\upv^t\}_{t=0}^{T_1}$, we construct a control sequence $\{\upq^t\}_{t=0}^{T_1}$. We first establish Lemmas \ref{phase-1-step-1}--\ref{phase-1-step-3}'s conclusions for the control sequence. Lemma \ref{phase-1-step-1} derives a high-probability upper bound for $\upq^{T_1}$, matching the bound in Theorem \ref{phase-I-informal}. 

In the analysis of Phase I, 
 we need to construct another capped coupling sequence $\{\bar{\upv}^t\}_{t=0}^{T_1}$ to describe the dynamic of iterates, and more importantly, we need to delve into the dynamic processes of the two-part parameters separated by the effective dimension $D$. It is non-trivial because in the traditional analysis of prior work to recover the sparse ground truth \citep{haochen21shape}, it is unnecessary to  introduce $D$. %for convergence analysis. 
%However, in our result, the two competing terms $\frac{D}{T}$ and $\frac{1}{D^{\beta-1}}$ in the risk bound (Eq.~\eqref{risk-main}) are trade-off on the effective dimension $D$, directly determining that the convergence is closely related to the adaptively-selected $D$, and the dynamic processes of the different regions divided by $D$. 
%Therefore, we propose a novel analysis to delve into the  dynamic. Specifically, 
%we develop a sequence of geometrically compensated supermartingales $\{\calX_i^0,\cdots,\calX_i^{T_1}\}_{i=1}^M$ using the capped coupling sequence $\{\Bar{\upv}^t\}_{t=0}^{T_1}$ derived from the control sequence $\{\upq^t\}_{t=0}^{T_1}$. We compute the precise characterization of the sub-Gaussian parameter for each $\calX_i^{T_1}-\calX_i^0$. When $i\in\calS$, through geometric series summation, the sub-Gaussian parameter has the form of $\widetilde{\calO}(\eta^{-1})$. When $i\in\mathcal{S}^c$, through linear summation, the sub-Gaussian parameter has the form of $\widetilde{\calO}((\lambda_i\eta^2T_1)^{-1})$. 
In our analysis, we partition the interval $[1:M]$ into $\mathcal{S}$ and $\mathcal{S}^c$. We will separately and precisely estimate the sub-Gaussian parameters for the supermartingales generated from $\{\bar{\upv}^t\}_{t=0}^{T_1}$. The characterization of dynamic is separated into the following three lemmas. In Lemma \ref{phase-1-step-1}, 
% we show the last iterate $\upq^{T_1}$ in control sequence $\{\upq^t\}_{t=0}^{T_1}$ maintains component-wise upper bounds: For $i\in\mathcal{S}$, $\upq_i^{T_1}$ is constrained to at most $(1+c_1)\upv_i^*$. For $i\in\mathcal{S}^c$, $\upq_i^{T_1}$ is constrained to at most $3\upv_i^*/2$.
we show the last iterate $\upq^{T_1}$ in control sequence $\{\upq^t\}_{t=0}^{T_1}$ satisfies $\upq_i^{T_1}\leq (1+c_1)\upv_i^*$ for any $i\in\mathcal{S}$ and $\upq_i^{T_1}\leq\frac{3}{2}\upv_i^*$ for any $i\in\mathcal{S}^c$ with high probability.

%, we establish a precise dependence between the step size $\eta$ and the effective dimension $D$. \congfang{???}
%For sufficiently large $M$, the two competing terms $\frac{D}{T}$ and $\frac{1}{D^{\beta-1}}$ in the risk bound (Eq.~\eqref{risk-main}) are trade-off on the effective dimension $D$.


%As demonstrated in Lemma \ref{phase-1-step-1}, the algorithm can optimally balance the trade-off between the two competing terms above, thereby establishing $D$ as the effective dimension that achieves the tightest risk bound adaptively. %\citet{haochen21shape} does not require a effective dimension when recovering sparse ground truth. In our setting, effective dimension $D$ is crucial for the global risk bound. The co-optimization of $D$ with global risk objectives constitutes a key innovation of this work. \congfang{Far away from my expectation...}
% \congfang{say sparse do not need to determine D   D is crucial for convergence rates you novelly...}

\iffalse
According to the slicing of $[1:M]$ during the calculation of the sub-Gaussian parameters, the relationship between the stepsize $\eta$ and the effective dimension $D$ is established. When 
$M$ is sufficiently large, the two terms $\frac{D}{T}$ and $\frac{1}{D^{\beta-1}}$ in the risk bound Eq.~\eqref{risk-main} depend on the effective dimension $D$. Since the choice of $D$ in Lemma \ref{phase-1-step-1} trade off these two error terms, we state
$D$ is the effective dimension. 
\fi

\begin{lemma}\label{phase-1-step-1}
% [Lemma \ref{lemma_1}]
    Under the setting of Theorem \ref{phase-I-informal}, both $\upq_i^{T_1}\leq (1+c_1)\upv_i^*$ for any $i\in\mathcal{S}$ and $\upq_i^{T_1}\leq\frac{3}{2}\upv_i^*$ for any $i\in\mathcal{S}^c$ occur with high probability.
\end{lemma}
Lemmas \ref{phase-1-step-2} and \ref{phase-1-step-3} collectively address the lower bound of $\upq^{T_1}$ in Theorem \ref{phase-I-informal}. To establish Lemma \ref{phase-1-step-2}, we derive a subcoupling sequence $\{\Breve{\upv}^{i,t}\}_{t=0}^{T_1}$ from the parent coupling sequence $\{\Bar{\upv}^t\}_{t=0}^{T_1}$ for any $i\in\mathcal{S}=[1:D]$. Each subcoupling sequence undergoes logarithmic transformation to generate a linearly compensated submartingale $\{-t\log(1+\eta\calO(\lambda_i(\upv_i^*)^2))+\log(\breve{\upv}_i^{i,t})\}_{t=1}^{T_1}$. These $D$ submartingales exhibit monotonic growth with sub-Gaussian increments. Applying concentration inequalities, we obtain $\max_{t\in[1:T_1]}\upq_i^t\geq(1-c_1/2)\upv_i^*$ with high probability for any $i\in\mathcal{S}$.
\begin{lemma}\label{phase-1-step-2}
% [Lemma \ref{lemma-2}]
    Under the setting of Theorem \ref{phase-I-informal}, with high probability, either $\max_{t\leq T_1}\upq_i^t\geq (1-c_1/2)\upv_i^*$ for any $i\in\mathcal{S}$, or at least one of the following statements fails: $\upq_i^{T_1}\leq (1+c_1)\upv_i^*$ for any $i\in\mathcal{S}$ and $\upq_i^{T_1}\leq\frac{3}{2}\upv_i^*$ for any $i\in\mathcal{S}^c$.
\end{lemma}
Lemma \ref{phase-1-step-3} further establishes that when $\max_{t\leq T_1}\upq_i^t$ resides within the $\upv_i^*$-neighborhood, the lower bound satisfies $\upq_i^{T_1} \geq (1 - c_1)\upv_i^*$ with high probability. The proof of Lemma \ref{phase-1-step-3} mirrors that of Lemma \ref{phase-1-step-1}.
\begin{lemma}\label{phase-1-step-3}
% [Lemma \ref{lemma-3}]
Under the setting of Theorem \ref{phase-I-informal}, for any $i\in\mathcal{S}$, with high probability, either $\max_{t\leq T_1}\upq_i^t< (1-c_1/2)\upv_i^*$ or $\upq_i^{T_1}\geq(1-c_1)\upv_i^*$.  
\end{lemma}
According to the high-probability equivalence between $\{\upq^t\}_{t=0}^{T_1}$ and $\{\upv^t\}_{t=0}^{T_1}$, Lemmas \ref{phase-1-step-1}--\ref{phase-1-step-3}'s conclusions transfer to $\upv^{T_1}$ with high-probability guarantees. %\congfang{verbose???}
% To characterize the mainstream dynamic, our analysis employs a probabilistic sequence synchronization technique. That is, from the Algorithm \ref{SGD}-generated sequence $\{\upv^t\}_{t=0}^{T_1}$, we construct a control sequence $\{\upq^t\}_{t=0}^{T_1}$. 
% % with the capped coupling sequence. 
% We first establish Lemmas \ref{phase-1-step-1}--\ref{phase-1-step-3}'s conclusions for the coupling sequence. The architecture of the aforementioned coupling sequence, grounded in carefully designed stopping times, guarantees that Lemmas \ref{phase-1-step-1}--\ref{phase-1-step-3} remain valid for $\upq^{T_1}$. According to the probabilistic equivalence between $\{\upq^t\}_{t=0}^{T_1}$ and $\{\upv^t\}_{t=0}^{T_1}$, Lemmas \ref{phase-1-step-1}--\ref{phase-1-step-3}'s conclusions transfer to $\upv^{T_1}$ with high probability guarantees. \congfang{verbose???}

% The proof of Theorem \ref{phase-I-informal} consists of three main lemmas. Lemma \ref{phase-1-step-1} derives a high-probability upper bound for $\upv^{T_1}$, matching the bound stated in Theorem \ref{phase-I-informal}.  Lemmas \ref{phase-1-step-2} and \ref{phase-1-step-3} collectively address the lower bound of $\upv^{T_1}$ in Theorem \ref{phase-I-informal}.  Specifically, 
% Lemma \ref{phase-1-step-2} proves that for any $i\in\mathcal{S}$, $\max_{t\in[1:T_1]} \upv_i^t$ converges with high probability to a neighborhood of $\upv_i^*$. Lemma \ref{phase-1-step-3} further establishes that when $\max_{t\leq T_1}\upv_i^t$ resides within the $\upv_i^*$-neighborhood, the lower bound satisfies $\upv_i^{T_1} \geq (1 - c_1)\upv_i^*$ with high probability.

% To prove Lemma \ref{phase-1-step-1}, for each coordinate $i \in [1:M]$, we develop a geometrically compensated supermartingale $\{\mathcal{X}_i^t:=(1-\eta\Theta(\lambda_i(\upv_i^*)^2))^{-t}(\Bar{\upv}_i^t-\upv_i^*)\}_{t=1}^{T_1}$ from the capped coupling sequence $\{\Bar{\upv}^t\}_{t=0}^{T_1}$ based on the control sequence $\{\upq^t\}_{t=0}^{T_1}$. The precise characterization of sub-Gaussian parameters for the aforementioned supermartingale increments constitutes a crucial prerequisite for applying concentration inequalities and obtaining high-probability estimation. \congfang{How the parameters affect?}

% This preparatory analysis ultimately enables the application of Bernstein-type inequalities to establish the claimed concentration results in Lemma \ref{phase-1-step-1}.

% We estimate the sub-Gaussian parameters of the sum of the supermartingale difference sequence corresponding to sets $\mathcal{S}$ and $\mathcal{S}^c$ using geometric series summation and linear summation methods, respectively. Finally, we apply concentration inequalities to obtain Lemma \ref{phase-1-step-1}. 
% The following two lemmas establish the lower bound for $\upv^{T_1}$ in Theorem \ref{phase-I-informal}.
% Lemma \ref{phase-1-step-1} is derived by constructing a supermartingale based on each coordinate of the previously mentioned coupling, which has a compressive property, and applying the concentration inequality for $M$ supermartingales. The next two lemmas establish the lower bound for $\upv^{T_1}$ in Theorem \ref{phase-I-informal}.  

% While the proof technique of Lemma \ref{phase-1-step-3} mirrors that of Lemma \ref{phase-1-step-1}, its supermartingale construction specifically targets coordinates $i\in\mathcal{S}$. 
% We construct geometrically compensated supermartingale $\{-\mathcal{X}_i^t\}_{t=1}^{T_1}$ for each $i\in\mathcal{S}$ through sign reversal. 
%\congfang{check sentence and remove the last one?}
% For any $i\in\mathcal{S}$, the lower bound analysis necessitates inverted $\{\mathcal{Y}_i^t\}_{t=1}^{T_1}$ to obtain another geometrically compensated supermartingale $\{-\mathcal{Y}_i^t\}_{t=1}^{T_1}$. 

%The proof is finished by applying Bernstein-type concentration inequalities to these constructed supermartingales, yielding the required probabilistic bounds.

% since we are estimating the lower bound of $\upv_i^{T_1}$, the construction of the supermartingale here differs from that in Lemma \ref{phase-1-step-1} by a negative sign.

% We remark that relatively small initialization as a prerequisite for convergence analysis has appeared in numerous research studies. \citet{li2018algorithmic} and \citet{vaskevicius2019implicit} showed that GD converges to the ground truth with sufficiently small initialization, which is required to be smaller than target error. In contrast, the initialization of Algorithm \ref{SGD} only requires having an $l_{\infty}$-norm bound on the order of $\min_{i\in[1:M]}\upv_i^*$.  \congfang{re-talk about intro}

\subsection{Phase II:}
% In this phase, we present the global convergence proof of Algorithm \ref{SGD} for minimizing the risk Eq.~\eqref{RM}. 
We now start the analysis  of  Phase II for Algorithm \ref{SGD}.  The main idea stems from approximating Algorithm \ref{SGD}'s iterations as SGD %with geometrically decaying step sizes 
running over a linear model  with rescaled features $\Pi_M\upx\odot\upv_{1:M}^*$. The adaptive rescale size $\upv_{1:M}^*$
enables the quadratic model to achieve accelerated convergence rates compared to its linear counterpart. To streamline notation, we introduce two key matrices in $\mathbb{R}^{M\times M}$: 
% i) $\upB^0 := (\upv^{T_1} - \upv_{1:M}^*) \otimes (\upv^{T_1} - \upv_{1:M}^*)$; 
i) $\upH := \diag\{\lambda_i(\upv_i^*)^2\}_{i=1}^M$; ii) $\widehat{\upH} := \diag\{\lambda_1(\upv_1^*)^2, \cdots, \lambda_N(\upv_N^*)^2, \textbf{0}_{M-D}\}$.
\begin{theorem}\label{phase-II-informal}
    Suppose Assumptions \ref{ass-d} and  \ref{ass-ss} hold. By selecting an appropriate step size $\eta_0=\eta(D)$ and middle phase length $h$, we obtain
    \begin{equation}
        \begin{split}
            \mathcal{R}_{M}(\upv^T)\lesssim&\mathcal{R}_{M}(\upv_{1:M}^*)+\frac{\sigma^2N}{T}+\sigma^2\eta_0^2 T\tr\left(\upH_{D+1:M}^2\right)+\frac{D}{T}+\eta_0^2T\tr\left(\upH_{D+1:M}^2\right)
        \\
        &+\llangle\frac{1}{\eta_0 T}\upI_{1:D}+\upH_{D+1:M},\left(\upI-\eta_0\widehat{\upH}\right)^{\frac{2T}{\log(T)}}\upB^0\rrangle,\notag
        \end{split}
    \end{equation}
    with probability at least 0.95.
\end{theorem}
% The proof of Theorem \ref{phase-II-informal} is divided into two parts: the first part, established by Lemma \ref{phase-2-step-1}, demonstrates that iterations of $\upv^t$ for $t\in[T_1+1:T=T_1+T_2]$ are, with high probability, confined to a neighborhood of $\upv_{1:M}^*$.
The proof of Theorem \ref{phase-II-informal} is structured in two key parts. In the first part, Theorem \ref{phase-2-step-1} establishes that Algorithm \ref{SGD} iterates $\{\upv^t\}_{t=T_1+1}^{T=T_1+T_2}$ remain confined within the neighborhood $\prod_{i=1}^D[\frac{1}{2}\upv_i^*,\frac{3}{2}\upv_i^*]\times\prod_{i=D+1}^M[0,2\upv_i^*]$ with high probability. 
\begin{theorem}\label{phase-2-step-1}
    Under Assumption \ref{ass-d}, we consider the iterative process of Algorithm \ref{SGD}, beginning from step $T_1$ with the same step size $\eta$ as in Theorem \ref{phase-I-informal}. If $D<M$, let $1\leq T_2\leq T_u(D)$ where $T_u(D)\in\bbN_+$ depends on $D$; otherwise, let $T_2\geq 1$. Then, for any $t\in[1:T_2]$, with high probability, we have
    \begin{align}\label{proof-sketch-2}
        \begin{cases}
            \upv_i^{T_1+t}\in\left[\frac{1}{2}\upv_i^*,\frac{3}{2}\upv_i^*\right],\quad &\text{ if }i\in[1:D],
            \\
            \upv_i^{T_1+t}\in\left[0, 2\upv_i^*\right],\quad &\text{ otherwise }.
        \end{cases}
    \end{align}
\end{theorem}
According to Theorem \ref{phase-I-informal}, $\upv^{T_1}$ satisfies Eq.~\eqref{proof-sketch-1} with high probability by setting $c_1 = \frac{1}{4}$. By employing the construction method for supermartingales, similar to that used in the proofs of Lemma \ref{phase-1-step-1} and Lemma \ref{phase-1-step-3}, we obtain a set of compressed supermartingales depend on the coordinate $i\in[1:M]$. Applying supermartingales concentration inequality, we obtain Eq.~\eqref{proof-sketch-2}.
% Combining the compression properties of these supermartingales with the sub-Gaussian characteristics of their difference sequences, and applying concentration inequality, we show that Eq.~\eqref{proof-sketch-2} holds for any $\upv^{T_1+t}$, where $t\in[1:T_2]$, starting from $\upv^{T_1}$. 

In the second part, we construct an auxiliary bounded sequence $\{\upw^t\}_{t=1}^{T_2}$ which is the truncation of $\{\upv^{T_1+t}\}_{t=1}^{T_2}$, and provide the last iterate instantaneous risk for $\upw^{T_2}$ which can be extended to $\upv^{T}$. The construction ensures that if $\upv^{T_1+t}$ satisfies Eq.~\eqref{proof-sketch-2} for any $t\in[1:T_2]$, then $\{\upw^t\}_{t=1}^{T_2}$ coincides with $\{\upv^{T_1+t}\}_{t=1}^{T_2}$. The novelty and ingenuity of our analysis based on auxiliary sequence construction lie in the alignment of  $\{\upw^t\}_{t=1}^{T_2}$ and $\{\upv^{T_1+t}\}_{t=1}^{T_2}$ as $\upw^{T_2}=\upv^{T}$ with high probability by Theorem \ref{phase-2-step-1}. Thus the update rule of $\upw^t$ satisfies the following formula with high probability:
\begin{equation}\label{proof-sketch-3}
    \begin{split}
        \upw^{t+1}=\upw^t-\eta_t\upH^t(\upw^t-\upv_{1:M}^*)+\eta_t\upR^t\Pi_M\upx^t,
    \end{split}
\end{equation}
where $\upH^t\in\bbR^{M\times M}$ depends on $\upw^t$ and $\upx^t$, and $\upR^t\in\bbR^{M\times M}$ depends on $\upw^t, \zeta_{M+1:\infty}^t$ and $\xi^t$. Combining Eq.~\eqref{proof-sketch-3} with the constraint of $\{\upw^t\}_{t=1}^{T_2}$, we observe that the update process of $\upw^t$ approximates that of SGD in traditional linear regression problems \citep{wu2022last} with reparameterized features $\Pi_M\upx\odot\upv_{1:M}^*$. The SGD iteration in linear model exhibits structural similarity to Eq.~\eqref{proof-sketch-3}, but differs in that its $\upH^t$ and $\upR^t$ are independent on iterative variables; this independence eliminates the need for truncated sequences in analytical treatments. Our analysis innovatively introduces a truncated sequence $\{\upw^t\}_{t=1}^{T_2}$ to maintain analytical tractability of $\upH^t$ and $\upR^t$.

According to Eq.~\eqref{proof-sketch-3}, we decompose the risk $\mathcal{R}_M(\upw^{T_2})$ as follows:
\begin{align}\label{phase-II-eq1}
    \bbE\left[\mathcal{R}_M(\upw^{T_2})\right]-\mathcal{R}_M(\upv_{1:M}^*)\lesssim\underbrace{\llangle\upH,\upB^{T_2}\rrangle}_{\text{bias error}}+\underbrace{\llangle\upH,\upV^{T_2}\rrangle}_{\text{variance error}}.
\end{align}
For any $t\in[1:T_2]$, $\upB^t$ and $\upV^t$ are $M\times M$ matrices, derived from the bias and variance terms induced by $\upw^t-\upv_{1:M}^*$, respectively. 
% The error decomposition in Eq.~\eqref{phase-II-eq1} yields two components: (1) Variance error $\llangle\upH,\upV^{T_2}\rrangle$. (2) Bias error $\llangle\upH,\upB^{T_2}\rrangle$. 
Since $\upH^t$ and $\upR^t$ in Eq.~\eqref{proof-sketch-3} are both dependent on $\upw^t$, it is a challenge to directly establish the full-matrix recursion between $\upV^{t+1}$ and $\upV^t$ (or $\upB^{t+1}$ and $\upB^t$) under the SGD iteration process like the similar techniques in linear models \citep{wu2022last}. To resolve this challenge, we turn to the recursive relations between diagonal elements of $\{\upV^t\}_{t=0}^{T_2}$ and $\{\upB^t\}_{t=0}^{T_2}$, which can approximate to describe the iterate of $\{\upV^t\}_{t=0}^{T_2}$ and $\{\upB^t\}_{t=0}^{T_2}$. We novelly consider the diagonal elements of the above sequences across discrete time steps, thereby obtaining the estimation for both variance and bias errors for our linear approximation. 

%\congfang{why you delete my comments????}


\section{Conclusions}
In this paper, we construct the theoretical analysis for the dynamic of quadratically parameterized model under decaying ground truth and anisotropic gradient noise. Our technique is based on the precise analysis of two-stage dynamic of SGD, with adaptive selection of the effective dimension set in the first stage and the approximation of linear model in the second stage. Our analysis characterizes the feature learning and model adaptation ability with clear separations for convergence rates in the canonical linear model.

%Finally, we apply Markov's inequality to Eq.~\eqref{phase-II-eq1} and complete the proof.

% Lemma \ref{sketch-vari} provides the upper bound estimation of variance error $\llangle\upH,\upV^{T_2}\rrangle$.
% \begin{lemma}\label{sketch-vari}
%     Under the condition of Theorem \ref{phase-II-informal}, we have
%     \begin{equation}
%         \begin{split}
%             \llangle\upH,\upV^{T_2}\rrangle\lesssim\sigma^2\left(\frac{N_0'}{T}+\eta_0\tr\left(\upH_{N_0'+1:N_0}\right)\right)+\sigma^2\eta_0^2(h+T)\tr\left(\upH_{N_0+1:M}^2\right),
%         \end{split}
%     \end{equation}
%     for arbitrary $N\geq N_0\geq N_0'\geq0$.
% \end{lemma}

% Lemma \ref{sketch-bias} provides the upper bound estimation of bias error $\llangle\upH,\upB^{T_2}\rrangle$.
% \begin{lemma}\label{sketch-bias}
%     Under the condition of Theorem \ref{phase-II-informal}, we have
%     \begin{equation}
%         \begin{split}
%             \llangle\upH,\upB^{T_2}\rrangle\lesssim&\frac{1}{\eta_0T}\tr\left(\left(\upI_{1:N_0}-\eta_0\upH_{1:N_0}\right)^{2h}\upB_{1:N_0}^0\right)+\llangle\upH_{N_0+1:M},\upB_{N_0+1:M}^0\rrangle\notag
%             \\
%             &+(C+2)\Gamma_T(\upH)\llangle\frac{1}{\eta_0T}\upI_{1:N_0'}+\upH_{N_0'+1:M},\upB^0\rrangle,
%         \end{split}
%     \end{equation}
%     with $\Gamma_T(\upH):=\left(\frac{N_0'}{T}+\frac{\eta_0h}{T}\tr\left(\upH_{N_0'+1:N_0}\right)+\eta_0^2h\tr\left(\upH_{N_0+1:M}^2\right)\right)$ for arbitrary $N\geq N_0\geq N_0'\geq0$.
% \end{lemma}
% To analyze Lemma \ref{sketch-vari} and \ref{sketch-bias}, we develop a diagonal recursive analysis framework. We construct recursive relations between diagonal elements of $\{\upV^t\}_{t=0}^{T_2}$ and $\{\upB^t\}_{t=0}^{T_2}$ sequences across discrete time steps $(t\rightarrow t+1)$, thereby obtaining the estimation for both variance and bias errors. 
%\congfang{strange??? Moreover why you use theorm ???}