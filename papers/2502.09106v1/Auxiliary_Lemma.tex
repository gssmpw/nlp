\begin{definition}[Sub-Gaussian Random Variable]\label{sub-Gaussian}
   A random variable $x$ with mean $\bbE x$ is sub-Gaussian if there is $\sigma\in\bbR_+$ such that
   \begin{align}
       \bbE\left[e^{\lambda(x-\bbE x)}\right]\leq e^{\frac{\lambda^2\sigma^2}{2}},\quad \forall\lambda\in\bbR.\notag
   \end{align}
\end{definition}

\begin{proposition}\label{prop-A5}[\citep{wainwright2019high}]
    For a random variable $x$ which satisfies the sub-Gaussian condition \ref{sub-Gaussian} with parameter $\sigma$, we have
    \begin{align}\label{eq-subGaussian-prob}
        \bbP\left(|x-\bbE x|>c\right)\leq 2e^{-\frac{c^2}{2\sigma^2}},\quad \forall c>0.
    \end{align}
\end{proposition}
% \begin{proof}
%     For any $c,\lambda>0$, by Markov's inequality, we have
%     \begin{align}\label{pro-sub-gaussian}
%         \bbP\left(x-\bbE x>c\right)\leq\frac{\bbE\left[e^{\lambda(x-\bbE x)}\right]}{e^{\lambda c}}\leq e^{\frac{\lambda^2\sigma^2}{2}-\lambda c}.
%     \end{align}
%     Since $\lambda$ can be arbitrary positive real number, Eq.~\eqref{pro-sub-gaussian} implies the following upper deviation inequality
%     \begin{align}\label{pro-sub-gaussian-1}
%         \bbP\left(x-\bbE x>c\right)\leq\min_{\lambda>0}e^{\frac{\lambda^2\sigma^2}{2}-\lambda c}=e^{-\frac{c^2}{2\sigma^2}}.
%     \end{align}
%     Moreover, according to the symmetry of the sub-Gaussian definition, the variable $-x$ is sub-Gaussian if and only if $x$ is sub-Gaussian. Therefore, we can also obtain the lower deviation inequality:
%     \begin{align}\label{pro-sub-gaussian-2}
%         \bbP\left(-x+\bbE x>c\right)\leq e^{-\frac{c^2}{2\sigma^2}}.
%     \end{align}
%     Combining Eq.~\eqref{pro-sub-gaussian-1} and Eq.~\eqref{pro-sub-gaussian-2}, we complete the proof.
% \end{proof}

\begin{lemma}\label{aux-1}
    Let $X_1,\cdots,X_n$ be independent and symmetric stochastic variables with zero mean. Denote $Y=\sum_{i=1}^n \upv_iX_i$ $\mathds{1}_{|X_i|\leq R}$ for any unit vector $\upv\in\bbR^n$ and positive scalar $R$. Then, we have $YX_1\mathds{1}_{|X_1|\leq R}$ is sub-Gaussian with parameter at most $\sigma=2R^2\|\upv\|_2$.
	% For some positive scalar $\eta,T\in\bbR_+$, we let $\eta T=Ck^{\alpha+\beta}$ where $k\in\bbN_+$ and $C>1$, and  $g:\bbN_+\rightarrow\bbR_+$ be defined by $g(i)=\frac{2}{3}i^{\frac{\beta}{2}}\exp\left\{\frac{2}{3}T\eta i^{-(\alpha+\beta)}\right\}$ for any $i\in\bbN_+$. We have that $g$ attains its minimum at $k$ over $[1:k]$.
\end{lemma}
\begin{proof}
    For simplicity, we denote $\hat{X}_i:=X_i\mathds{1}_{|X_i|\leq R}$ for any $i\in[1:n]$, and $Y_{-1}=\sum_{i=2}^n\upv_i\hat{X}_i$. One can notice the following holds
    \begin{align}\label{sub-gau-1}
        \bbE\left[e^{\lambda \left(Y\hat{X}_1-\bbE[Y\hat{X}_1]\right)}\right]=\bbE\left[e^{\lambda\upv_i\left(\hat{X}_1^2-\bbE[\hat{X}_1^2]\right)}\bbE\left[e^{\lambda\left(Y_{-1}\hat{X}_1-\bbE[Y_{-1}\hat{X}_1]\right)}\mid \hat{X}_1\right]\right],
    \end{align}
    for any $\lambda\in\bbR$. Letting $\hat{X}_i'$ be an independent copy of $\hat{X}_i$ for any $i\in[1:n]$, then we have
    \begin{align}\label{sub-gau-2}
        \bbE\left[e^{\lambda\left(Y_{-1}\hat{X}_1-\bbE[Y_{-1}\hat{X}_1]\right)}\mid \hat{X}_1\right]=&\bbE\left[e^{\sum_{i=2}^n\lambda\upv_i(\hat{X}_i-\bbE[\hat{X}_i])\hat{X}_1}\mid\hat{X}_1\right]
        \notag
        \\
        \overset{\text{(a)}}{\leq}&\bbE\left[e^{\sum_{i=2}^n\lambda\upv_i(\hat{X}_i-\hat{X}_i')\hat{X}_1}\mid\hat{X}_1\right],
    \end{align}
    where (a) is derived from the convexity of the exponential, and Jensenâ€™s inequality. Letting $\xi$ be an independent Rademacher variable, since the distribution of $\hat{X}_i-\hat{X}_i'$ is the same as that of $\xi(\hat{X}_i-\hat{X}_i')$ for any $i\in[1:n]$, we obtain
    \begin{align}\label{sub-gau-3}
        \bbE\left[e^{\sum_{i=2}^n\lambda\upv_i(\hat{X}_i-\hat{X}_i')\hat{X}_1}\mid\hat{X}_1\right]
        % =&\prod_{i=2}^n\bbE\left[e^{\lambda\upv_i(\hat{X}_i-\hat{X}_i')\hat{X}_1}\mid\hat{X}_1\right]\notag
        % \\
        % =&\prod_{i=2}^n\bbE\left[e^{\lambda\upv_i\hat{X}_1\xi(\hat{X}_i-\hat{X}_i')}\mid\hat{X}_1\right]\notag
        % \\
        \overset{\text{(b)}}{\leq}\prod_{i=2}^n\bbE\left[e^{\frac{\lambda^2\upv_i^2}{2}\hat{X}_1^2(\hat{X}_i-\hat{X}_i')^2}\mid\hat{X}_1\right].
    \end{align}
    Noticing that $|\hat{X}_i-\hat{X}_i'|\leq 2R$ and $|\hat{X}_i|\leq R$ for any $i\in[1:n]$, we are guarantee that
    \begin{align}\label{sub-gau-4}
        \prod_{i=2}^n\bbE\left[e^{\frac{\lambda^2\upv_i^2}{2}\hat{X}_1^2(\hat{X}_i-\hat{X}_i')^2}\mid\hat{X}_1\right]\leq e^{2\lambda^2R^4\sum_{i=2}^n\upv_i^2}.
    \end{align}
    Combining Eq.~\eqref{sub-gau-1}-Eq.~\eqref{sub-gau-4} and applying similar technique, we have
    \begin{align}
        \bbE\left[e^{\lambda \left(Y\hat{X}_1-\bbE[Y\hat{X}_1]\right)}\right]\leq& e^{2\lambda^2R^4\sum_{i=2}^n\upv_i^2}\bbE\left[e^{\lambda\upv_i\left(\hat{X}_1^2-\bbE[\hat{X}_1^2]\right)}\right]\leq e^{2\lambda^2R^4\|\upv\|_2^2}.\notag
    \end{align}
	% Consider function $\psi:\bbR_+\rightarrow\bbR_+$ which has the form of $\psi(x)=\frac{2}{3}x^{\frac{\beta}{2}}\exp\left\{\frac{2}{3}T\eta x^{-(\alpha+\beta)}\right\}$, we have
	% \begin{align}
	% 	\psi'(x)=&\frac{2}{3}\exp\left\{\frac{2}{3}T\eta x^{-(\alpha+\beta)}\right\}\left[\frac{\beta}{2}x^{\frac{\beta}{2}-1}-\frac{2(\alpha+\beta)}{3}T\eta x^{-(\alpha+\frac{\beta}{2}+1)}\right]\notag
	% 	\\
	% 	=&\frac{2}{3}x^{\frac{\beta}{2}-1}\exp\left\{\frac{2}{3}T\eta x^{-(\alpha+\beta)}\right\}\left[\frac{\beta}{2}-\frac{2(\alpha+\beta)}{3}T\eta x^{-(\alpha+\beta)}\right].
	% \end{align}
	% It's clear that the critical point of $\psi$ is $x^*=\left(\frac{4C(\alpha+\beta)}{3\beta}\right)^{\frac{1}{\alpha+\beta}}k$. Therefore, the function value of $\psi$ is monotonically decreasing on the interval $(0, x^*)$ and monotonically increasing on the interval $(x^*, +\infty)$. Since $\frac{4C(\alpha+\beta)}{3\beta}>1$, it's clear that $g$ attain the minimum at $k$ over $[1:k]$.
\end{proof}

\begin{lemma}\label{aux-1.1}
    Consider a stochastic variable $X$ which is zero-mean and sub-Gaussian with parameter $\sigma$ for some $\sigma>0$. Then, there exists $R>0$ which depends on $\sigma$ such that
    \begin{align}
        \bbE\left[X^2\mathds{1}_{|X|\leq R}\right]\geq\frac{1}{2}\bbE\left[X^2\right].
    \end{align}
\end{lemma}
\begin{proof}
    According to Eq.~\eqref{eq-subGaussian-prob}, we have $\bbP(|X|\geq r)\leq 2e^{-\frac{r^2}{2\sigma^2}}$ for any $r>0$. Therefore, we obtain
    \begin{align}
        \bbE\left[X^2\mathds{1}_{|X|>R}\right]\overset{\text{(a)}}{=}&2\int_{0}^{\infty}r\bbP(|X|\mathds{1}_{|X|>R}>r)\text{d}r\notag
        \\
        =&2\int_R^{\infty}r\bbP(|X|>r)\text{d}r+R^2\bbP(|X|>R)\notag
        \\
        \leq&4\int_R^{\infty}re^{-\frac{r^2}{2\sigma^2}}\text{d}r+2R^2e^{-\frac{R^2}{2\sigma^2}}=4\sigma^2e^{-\frac{R^2}{\sigma^2}}+2R^2e^{-\frac{R^2}{2\sigma^2}},
    \end{align}
    where (a) is derived from [Lemma 2.2.13, \citet{wainwright2019high}]. 
\end{proof}

\begin{lemma}\label{aux-2}
	Let $c>0$, $\gamma<1$ and $a_t>0$ for any $t\in[0:T-1]$. Consider a sequence of random variables $\{v^i\}_{i=0}^{T-1}\subset[0,c]$, which satisfies either $v^t=v^{t+1}=\cdots=v^T$, or $\bbE\left[v^{t+1}\mid\calF^t\right]\leq(1-\eta_t)v^t$ with stepsize $\eta_t\geq0$, given $\bbE[e^{\lambda(v^{t+1}-\bbE[v^{t+1}\mid\calF^t])}\mid\calF^t]\leq e^{\frac{\lambda^2a_t^2}{2}}$ almost surely for any $\lambda\in\bbR$. Then, there is 
    \begin{align}
        \bbP\left(v^T>c\bigwedge v^0\leq\gamma c\right)\leq\max_{t\in[1:T]}\exp\left\{-\frac{(1-\gamma)^2c^2}{2\sum_{j=0}^{t-1}a_j^2\prod_{i=j+1}^{t-1}(1-\eta_i)^{2}}\right\}.\notag
    \end{align}
\end{lemma}
\begin{proof}
    Similarly, we begin with constructing a sequence of couplings $\{\Tilde{v}^i\}_{i=0}^T$ as follows: $\Tilde{v}^0=v^0$; if $v^t=v^{t+1}=\cdots=v^T$, let $\Tilde{v}^{t+1}=(1-\eta_t)\Tilde{v}^t$; otherwise, let $\Tilde{v}^{t+1}=v^{t+1}$. Notice that $\prod_{i=0}^{t-1}(1-\eta_i)^{-1}\tilde{v}^t$ is a supermartingale. We define $D_{t+1}:=\prod_{i=0}^{t}(1-\eta_i)^{-1}\tilde{v}^{t+1}-\prod_{i=0}^{t-1}(1-\eta_i)^{-1}\tilde{v}^t$ for any $t\in[0:T-1]$. Therefore, applying iterated expectation yields 
    \begin{align}\label{eq-sub-gaussian}
        \bbE\left[e^{\lambda\left(\sum_{i=1}^tD_i\right)}\right]=&\bbE\left[e^{\lambda\left(\sum_{i=1}^{t-1}D_{i}\right)}\bbE\left[e^{\lambda D_t}\mid\calF^{t-1}\right]\right]\notag
        \\
        =&\bbE\left[e^{\lambda\left(\sum_{i=1}^{t-1}D_{i}\right)}\bbE\left[e^{\frac{\lambda}{\prod_{i=0}^{t-1}(1-\eta_i)} \left(v^{t}-(1-\eta_{t-1})v^{t-1}\right)}\mid\calF^{t-1}\right]\right]
        \notag
        \\
        \overset{\text{(a)}}{\leq}&\bbE\left[e^{\lambda\left(\sum_{i=1}^{t-1}D_{i}\right)}\bbE\left[e^{\frac{\lambda}{\prod_{i=0}^{t-1}(1-\eta_i)} \left(v^{t}-\bbE[v^t\mid\calF^{t-1}]\right)}\mid\calF^{t-1}\right]\right]
        \notag
        \\
        \overset{\text{(b)}}{\leq}&e^{\frac{\lambda^2a_{t-1}^2}{2\prod_{i=0}^{t-1}(1-\eta_i)^2}}\bbE\left[e^{\lambda\left(\sum_{i=1}^{t-1}D_{i}\right)}\right]
        \notag
        \\
        \leq&e^{\frac{\lambda^2\sum_{j=0}^{t-1}a_j^2\prod_{i=0}^j(1-\eta_i)^{-2}}{2}},
    \end{align}
    for any $\lambda\in\bbR^+$ and $t\in[1:T]$, where (a) is derived from that $\lambda(\bbE[v^t\mid\calF^{t-1}]-(1-\eta_{t-1})v^{t-1})\leq 0$ and (b) follows from the condition that $\bbE[e^{\lambda(v^{t+1}-(1-\eta_t)v^t)}\mid\calF^t]\leq e^{\frac{\lambda^2a^2}{2}}$ almost surely for any $\lambda\in\bbR$. Then we obtain 
    \begin{align}
        \bbP\left(v^T>c\bigwedge v^0\leq\gamma c\right){\leq}&\max_{t\in[1:T]}\bbP\left(\prod_{i=0}^{t-1}(1-\eta_i)^{-1}\Tilde{v}^{t}>\prod_{i=0}^{t-1}(1-\eta_i)^{-1}c\bigwedge\Tilde{v}^0\leq\gamma c\right)\notag
        \\
        \leq&\max_{t\in[1:T]}\min_{\lambda>0}\frac{\bbE\left[e^{\lambda(\sum_{i=1}^tD_i)}\right]}{e^{\lambda\left(\prod_{i=0}^{t-1}(1-\eta_i)^{-1}c-\gamma c\right)}}
        \notag
        \\
        \overset{\text{(b)}}{\leq}&\max_{t\in[1:T]}\exp\left\{-\frac{\left(\prod_{i=0}^{t-1}(1-\eta_i)^{-1}c-\gamma c\right)^2}{2\sum_{j=0}^{t-1}a_j^2\prod_{i=0}^{j}(1-\eta_i)^{-2}}\right\}\notag
        \\
        \leq&\max_{t\in[1:T]}\exp\left\{-\frac{(1-\gamma)^2\left(\prod_{i=0}^{t-1}(1-\eta_i)^{-1}c\right)^2}{2\sum_{j=0}^{t-1}a_j^2\prod_{i=0}^{j}(1-\eta_i)^{-2}}\right\}\notag
        \\
        =&\max_{t\in[1:T]}\exp\left\{-\frac{(1-\gamma)^2c^2}{2\sum_{j=0}^{t-1}a_j^2\prod_{i=j+1}^{t-1}(1-\eta_i)^{2}}\right\},
    \end{align}
    where (b) is derived from Eq.~\eqref{eq-sub-gaussian}.
\end{proof}

\begin{corollary}\label{aux-coro-3}
    Let $c>0$, $\gamma<1$ and $a_t>0$ for any $t\in[0:T-1]$. Consider a sequence of random variables $\{v^i\}_{i=0}^{T-1}\subset[0,c]$, which satisfies $\prod_{i=0}^{T-1}(1+\eta_t)^{-1}c-v^0\geq\gamma c$ and $\bbE\left[v^{t+1}\mid\calF^t\right]\leq(1+\eta_t)v^t$ with stepsize $\eta_t\geq0$, given $\bbE[e^{\lambda(v^{t+1}-\bbE[v^{t+1}\mid\calF^t])}\mid\calF^t]\leq e^{\frac{\lambda^2a_t^2}{2}}$ almost surely for any $\lambda\in\bbR$. Then, there is 
    \begin{align}
        \bbP\left(v^T>c\right)\leq\max_{t\in[1:T]}\exp\left\{-\frac{\gamma^2c^2}{2\sum_{j=0}^{t-1}a_j^2\prod_{i=0}^{j}(1+\eta_i)^{-2}}\right\}\notag.
    \end{align}
\end{corollary}

\begin{lemma}\label{aux-3}
    For $L,K\in\bbN_+$, consider $T\in\bbN^+$ such that $LK\leq T<(L+1)K$. Then we have 
    \begin{align}
        \sum_{t=0}^{T}\left(\prod_{i=t}^{T}(1-c\eta_t)\right)\eta_t^2\leq\frac{2\eta_0}{c},
    \end{align}
    where $\eta_t=\frac{\eta_0}{2^l}$ if $lK\leq t\leq \min\{(l+1)K-1,T\}$ for any $l\in[0:L]$ and $c>0$ is a constant.
\end{lemma}
\begin{proof}
    For any $l\in[0:L]$, we have
    \begin{align}\label{aux-eq-lemma3}
        \sum_{t=lK}^{(l+1)K-1}\left(\prod_{i=t}^{T}\left(1-c\eta_t\right)\right)\eta_t^2=&\eta_{lK}^2\left(\prod_{i=(l+1)K}^{T}\left(1-c\eta_t\right)\right)\sum_{t=lK}^{(l+1)K-1}(1-c\eta_{lK})^{(l+1)K-1-t}\notag
        \\
        \leq&\frac{\eta_{lK}}{c}\left(\prod_{i=(l+1)K}^{T}\left(1-c\eta_t\right)\right).
    \end{align}
    Therefore, we obtain the following estimation
    \begin{align}
        \sum_{t=0}^{T}\left(\prod_{i=t}^{T}(1-c\eta_t)\right)\eta_t^2\leq&\sum_{t=0}^{LK-1}\left(\prod_{i=t}^{T}(1-c\eta_t)\right)\eta_t^2+\sum_{t=LK}^T(1-c\eta_{LK})^{T-t}\eta_{LK}^2\notag
        \\
        \overset{\text{(a)}}{\leq}&\frac{\sum_{l=0}^L\eta_{lK}}{c}\leq\frac{2\eta_0}{c}.
    \end{align}
\end{proof}

% \begin{lemma}\label{aux-4}
    
% \end{lemma}

\begin{lemma}\label{aux-5}
    Under Assumption \ref{ass-ss} and the setting of Theorem \ref{phase-II-main}, we have 
    $$
    \eta(\upI-\eta\widehat{\upH})^{2t}\upH\preceq\frac{25}{t+1}\upI,
    $$
    for any $t\in[0:T-1]$.
\end{lemma}
\begin{proof}
    For index $i\in[1:D]$, we have 
    \begin{align}
    \eta(\upI_{1:D}-\eta\widehat{\upH}_{1:D})^{2t}\upH_{1:D}=25 \eta(\upI_{1:D}-\eta\widehat{\upH}_{1:D})^{2t}\widehat{\upH}_{1:D}\preceq\frac{25}{t+1}\upI_{1:D},\notag
    \end{align}
    since $(1-x)^t\leq\frac{1}{(t+1)x}$ for any $x\in(0,1)$. For index $i\in[D+1:M]$, we obtain 
    \begin{align}
        \eta\upH_{i,i}\leq\frac{1}{T}\leq\frac{1}{t+1},
    \end{align}
    according to the parameter setting in Theorem \ref{phase-II-main} for any $t\in[1:T-1]$.
\end{proof}

\begin{lemma}\label{aux-6}
    Suppose Assumption \ref{ass-d} hold and let $\upz=\Pi_M\upx\in\bbR^M$. Then there exists a constant $\gamma>0$ such that 
    \begin{align}\label{aux-matrix}
        \bbE\left[\upA\upz\|\upz\|_{\upA^{\top}\upB\upA}^2\upz^{\top}\upA^{\top}\right]\preceq\gamma\llangle\upA\bbE\left[\upz\upz^{\top}\right]\upA^{\top},\upB\rrangle\upA\bbE\left[\upz\upz^{\top}\right]\upA^{\top},
    \end{align}
    for any diagonal PSD matrix $\upA\in\bbR^{M\times M}$ and PSD matrix $\upB\in\bbR^{M\times M}$.
\end{lemma}
\begin{proof}
    We denote $\upD:=\bbE[\upA\upz\|\upz\|_{\upA^{\top}\upB\upA}^2\upz^{\top}\upA^{\top}]$. For any $i,j\in[1:M]$ and $i\neq j$, we have $\upD_{i,j}=2\lambda_i\lambda_j\upA_{i,i}\upA_{j,j}(\upA^{\top}\upB\upA)_{i,j}$. In addition, we also have 
    $$
    \upD_{i,i}=\bbE\left[\|\upz\|_{\upA^{\top}\upB\upA}^2\right]\upA_{i,i}^2\lambda_i+(\upA^{\top}\upB\upA)_{i,i}\upA_{i,i}^2\Var\left[\upz_i^2\right]\leq (C+1)\bbE\left[\|\upz\|_{\upA^{\top}\upB\upA}^2\right]\upA_{i,i}^2\lambda_i.
    $$
    Therefore, we obtain that
    \begin{align}
        \upD\preceq&(C+1)\bbE\left[\|\upz\|_{\upA^{\top}\upB\upA}^2\right]\upA\bbE[\upz\upz^{\top}]\upA^{\top}+2\upA\bbE[\upz\upz^{\top}]\upA^{\top}\upB\upA\bbE[\upz\upz^{\top}]\upA^{\top}\notag
        \\
        \preceq&(C+1)\bbE\left[\|\upz\|_{\upA^{\top}\upB\upA}^2\right]\upA\bbE[\upz\upz^{\top}]\upA^{\top}\notag
        \\
        &+2\left\|\left(\upA\bbE[\upz\upz^{\top}]\upA^{\top}\right)^{1/2}\upB\left(\upA\bbE[\upz\upz^{\top}]\upA^{\top}\right)^{1/2}\right\|_2^2\upA\bbE[\upz\upz^{\top}]\upA^{\top}\notag
        \\
        \overset{\text{(a)}}{\preceq}&(C+2)\llangle\upA\bbE\left[\upz\upz^{\top}\right]\upA^{\top},\upB\rrangle\upA\bbE[\upz\upz^{\top}]\upA^{\top},\notag
    \end{align}
    where (a) is derived from that $\langle\upA\bbE[\upz\upz^{\top}]\upA^{\top},\upB\rangle=\bbE[\|\upz\|_{\upA^{\top}\upB\upA}^2]$ and $\|\upH^{1/2}\upB\upH^{1/2}\|_2^2\leq\langle\upH,\upB\rangle$ for any PSD matrix $\upB,\upD\in\bbR^{M\times M}$ since 
    \begin{align}
        \upa^{\top}\upH^{1/2}\upB\upH^{1/2}\upa=&\llangle\upH^{1/2}\upa\upa^{\top}\upH^{1/2},\upB\rrangle\notag
        \\
        \leq&\llangle\upH^{1/2}\upa\upa^{\top}\upH^{1/2},\upB\rrangle+\llangle\upH^{1/2}\upa_{\perp}\upa_{\perp}^{\top}\upH^{1/2},\upB\rrangle\notag
        \\
        =&\llangle\upH,\upB\rrangle.\notag
    \end{align}
    Therefore, by choosing $\gamma=(C+2)$, we obtain Eq.~\eqref{aux-matrix}.
\end{proof}

\begin{lemma}\label{aux-7}
    Under the setting of Theorem \ref{phase-II-main}, suppose following inequality holds 
    \begin{align}
        \upB_{\diag}^{t+1}\preceq\left(\calI-\eta\widehat{\calG}\right)^{t+1}\circ\upB_{\diag}^0+\tau\eta\sum_{i=0}^t\frac{\llangle\upH,\upB^i\rrangle}{t+1-i}\cdot\upI,\notag
    \end{align}
    for any $t\in[0:T-1]$ and some constant $\tau>0$. Then, we have
    \begin{align}
        \sum_{i=0}^{t}\frac{\langle\upH,\upB^i\rangle}{t+1-i}\leq\llangle\sum_{i=0}^t\frac{(\upI-\eta\widehat{\upH})^{2i}\upH}{t+1-i},\upB^0\rrangle+2\tau\eta\log(t)\tr(\upH)\sum_{i=0}^t\frac{\langle\upH,\upB^i\rangle}{t+1-i},\notag
    \end{align}
    for any $t\in[1:T]$.
\end{lemma}
\begin{proof}
    According to the condition of this lemma, we have 
    \begin{align}\label{aux-eq-5.7-1}
        \llangle\upH,\upB^{t}\rrangle\leq&\llangle(\upI-\eta\widehat{\upH})^{2t}\upH,\upB^0\rrangle+\tau\eta\tr(\upH)\sum_{i=0}^{t-1}\frac{\llangle\upH,\upB^i\rrangle}{t-i}.
    \end{align}
    Applying Eq.~\eqref{aux-eq-5.7-1} to each $\langle\upH,\upB^t\rangle$, we obtain
    \begin{align}
        \sum_{i=0}^{t}\frac{\langle\upH,\upB^i\rangle}{t+1-i}\leq&\llangle\sum_{i=0}^t\frac{(\upI-\eta\widehat{\upH})^{2i}\upH}{t+1-i},\upB^0\rrangle+\tau\eta\tr(\upH)\sum_{i=0}^t\sum_{k=0}^{i-1}\frac{\langle\upH,\upB^i\rangle}{(t+1-i)(i-k)}\notag
        \\
        \leq&\llangle\sum_{i=0}^t\frac{(\upI-\eta\widehat{\upH})^{2i}\upH}{t+1-i},\upB^0\rrangle+\tau\eta\tr(\upH)\sum_{k=0}^{t-1}\frac{\langle\upH,\upB^k\rangle}{t+1-k}\sum_{i=k+1}^t\left(\frac{1}{t+1-i}+\frac{1}{i-k}\right)\notag
        \\
        \leq&\llangle\sum_{i=0}^t\frac{(\upI-\eta\widehat{\upH})^{2i}\upH}{t+1-i},\upB^0\rrangle+2\tau\eta\log(t)\tr(\upH)\sum_{k=0}^t\frac{\langle\upH,\upB^k\rangle}{t+1-k}.\notag
    \end{align}
\end{proof}