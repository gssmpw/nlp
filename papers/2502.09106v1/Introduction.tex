%Recent years have witnessed a dominant success of large-scale models across AI fields.  A basic rule that forms the belief for practitioners to scale up the model is an empirically observed
%phenomenon called the \emph{neural scaling law} \citep{}.  The law  %from \cite{} originally 
%suggested a tolerably polynomial decrease for excess risk against the model and the sample size,  and predicted a significant improvement of the  model performance  when given abundant resources.  %Subsequently, substantial empirical studies explore polynomial relations of generalization performance with other critical factors, such as step size \cite{}, bath size \cite{}, and inference time \cite{}.  %Up to now, 
%Now, it has become the basis of a principled guide for building advanced model architecture and allocating resources. %Turning to the theory, however, a rather limited understanding is about scaling law.  Both its occurrence conditions and mechanism remain open and require deep investigation.

The rapid advancement of large-scale models has precipitated a paradigm shift across AI field, with the empirical scaling law emerging as a foundational principle guiding practitioners to scale up the model. The \textit{neural scaling law} \citep{kaplan2020scaling,bahri2024explaining} characterized a polynomial-type decay of excess risk against both the model size and training data volume. Originated from empirical observations, this law predict the substantial improvements of the model performance given abundant training resources. Enough powerful validations have supported the law as critical tools for development of model architecture and allocation of computational resources.

%From a statistical learning perspective,
%the neural scaling law delineates the quantitative relationship between excess risk, model size, and sample size when the model is trained by a specific algorithm class. This type of result differs from the traditional one in learning theory, where one often focuses on risk bounds that are agnostic to the algorithm via a uniform convergence argument for the hypotheses. On the empirical side,  the neural scaling law suggests that the excess risk exhibits a power-law decay concerning the model and the sample size, without suffering an explosive variance even as the model size approaches infinity.
%On the theoretical side, the variance would explode when no suitable regularization is imposed,  implying the effect of the specific parameterization method and learning algorithm to realize good generalization as the algorithmic preference plays a pivotal role as an implicit regularization. % For example, the Gradient Descent algorithm in the offline training for linear model with $0$ initialization is observed to convergence at a min-norm solution. 


From the statistical learning perspective, neural scaling law formalizes an algorithm-dependent generalization that explicitly quantify how excess risk diminishes with increasing model size and sample size. This paradigm diverges from the classical learning theory, which prioritizes algorithm-agnostic guarantees through a uniform convergence argument for the hypotheses. Empirically, the neural scaling law demonstrates a stable polynomial-type decay of excess risk. This phenomenon persists even as model size approaches infinity, challenging the traditional intuitions about variance explosion. Theoretically, this apparent contradiction implies the role of implicit regularization. Learning algorithms, when coupled with specific parameterized architectures, realize good generalization that suppresses variance explosion. The critical interplay between parameterization methods, optimization dynamics, and generalization, positions algorithmic preferences as an implicit regularization governing scalable learning.


\iffalse
 To theoretically characterize the polynomial-type scaling law, recent studies mainly study linear models.   One notable reason is the   the celebrated neural tangent
kernel theory, which shows  linear model can approximate neural networks that are sufficiently wide and trained with a specialized scaling and random initialization. \congfang{say Kernel has  power decay and so can obtain polynomial  rates..}
  

 
 
 
 
 \congfang{ Introduce in Related Work..
 
 Linear regression is perhaps the most canonical task in statistical learning and 
is experiencing a renaissance to explain empirical phenomena in deep learning that plausibly defy the traditional statistical mindset.  The motivation to study linear model is substantial: (i)  linear model can approximate neural networks that are sufficiently wide and trained with a specialized scaling and random initialization, due to the celebrated neural tangent
kernel theory \cite{}; (ii) it is relatively simple, for which one can conduct more precise analysis.  Compared with traditional studies, recent analysis operates in regimes where the problems are assumed to be high-dimensional with  non-uniform and fined-grained data eigenvalues and source conditions.   In the offline setting,  the Gradient Descent algorithm with $0$ initialization will converge to a min-norm solution. Represent work from \cite{} established lower and upper generalization bound for the min-norm  solution and demonstrated the possibility of benign overfitting when the tail of eigenvalues is small and decreases slowly.   The prevalence of double and even multiple descents is also substantiated under various geometries of eigenvalues and source conditions.   When it comes to  the more popular online setting, the main insight to achieve good generalization for Stochastic Gradient Decent (SGD) is its ability to gradually release the model complexities. Recent results show that SGD achieves optimal performance when the learning problem is relatively easy where the source condition ensures a decay, whereas exhibits less efficient when the source condition grows.
} %It was conjectured by \cite{} that more computational resources are essentially required by SGD in the latter regimes. 




 %and study the behaviors of stochastic or offline full Gradient Descent. It was shown that  
%under the regime where the data covariance eigenvalues have a polynomial decay rate and
%the optimum satisfies the source condition\congfang{....} The motivation for using a linear model as a testbed is two-folded: (i)  linear model can approximate neural networks when they are sufficiently wide due to neural tangent
%kernel theory \cite{}; (ii) the dynamic of the training process for the linear model is relatively simple, for which one can conduct more accurate analysis.  
\fi 
%To theoretically characterize the polynomial-type scaling law, recent studies have predominantly focused on linear models. One notable reason for this is the celebrated Neural Tangent Kernel (NTK) theory \cite{}, which demonstrates that wide neural networks, when carefully scaled and randomly initialized, can be approximated by linearized models. Another important reason is the analytical tractability of linear systems, which allows for precise characterization of learning dynamics.
%The excess risk of linear model is governed by the interplay between two key factors, the covariance operator spectrum and the regularity of ground truth.
%These factors correspond to the capacity and source conditions in the Reproducing Kernel Hilbert Space (RKHS) framework. The NTK spectrum is shown to exhibit power-law decay, which results in polynomial excess risk scaling in both kernel methods and infinite-width networks.


Theoretical progress in characterization of the polynomial-type scaling law has largely centered on linear models, motivated by two synergistic insights. First, the Neural Tangent Kernel (NTK) theory \citep{NEURIPS2018_5a4be1fa,arora2019exact} reveals that wide neural networks, when specially scaled and randomly initialized, can be approximated by linearized models, bridging nonlinear architectures to analytically tractable regimes. Second, linear systems allow for precise characterization of learning dynamics.  The excess risk of linear model is associated with two key factors, the covariance operator spectrum and the regularity of ground truth~\citep{lin2024scaling,bahri2024explaining}.   In the Reproducing Kernel Hilbert Space (RKHS) framework, 
these factors can be described by the capacity of the kernel and source conditions of the target function~\citep{caponnetto2007optimal}. 



%Compared with traditional studies, recent analyses focus on settings where problems are high-dimensional, with non-uniform and finely-grained covariance spectra and source conditions. 
 %In the offline setting, Gradient Descent (GD) exhibits the implicit regularization and multiple descents phenomena, under various geometries of the covariance spectrum and source conditions.
%In the more commonly studied online setting, stochastic gradient descent (SGD) has been shown to achieve a polynomial excess risk under a power-law decay covariance spectrum and ground truth parameter.

Compared with traditional studies in linear regression, recent analyses have shifted focus to high-dimensional problems with non-uniform and fine-grained covariance spectra and source conditions~\citep{caponnetto2007optimal,bartlett2021deep}.   The NTK spectrum is shown to exhibit power-law decay when the inputs are uniformly distributed on the unit sphere~\citep{bietti2019inductive,bietti2021deep}.  %which results in polynomial excess risk scaling in both kernel methods and infinite-width networks. 
 In the offline setting, Gradient Descent (GD) and kernel ridge regression (KRR) exhibit the implicit regularization and multiple descents phenomena, under various geometries of the covariance spectrum and source conditions~\citep{gunasekar2017implicit, Bartlett_2020,10.1214/20-AOS1990,zhang2024optimal1}. In the more widely studied online setting, Stochastic Gradient Descent (SGD) has been proven to achieve a polynomial excess risk under a power-law decay covariance spectrum and ground truth parameter~\citep{dieuleveut2016nonparametric,lin2017optimal,wu2022last}.


 \iffalse
 In the offline setting, the Gradient Descent (GD) with $0$ initialization will converge to a min-norm solution. Represent work from \cite{} established both algorithm lower and upper excess risk bounds for the min-norm solution and demonstrated the possibility of benign overfitting when the tail of eigenvalues is small and decreases slowly. Furthermore, the prevalence of double and even multiple descents is also substantiated under various geometries of the covariance spectrum and source conditions. When it comes to the more popular online setting, the main insight to achieve good generalization for Stochastic Gradient Decent (SGD) is its ability to gradually release the model complexities. Recent results show that SGD achieves optimal performance when the learning problem is relatively easy where the source condition ensures the ground truth is smooth, whereas exhibits less efficient when the ground truth become less smooth.
\fi 
 

\begin{figure}[t]
\centering


\subfigure[\scriptsize{Quadratic v.s. Linear Model}]{
\begin{minipage}[t]{0.3\linewidth}
\centering
\includegraphics[width=2 in]{phase2.pdf}
\end{minipage}%
}%
\subfigure[\scriptsize{Quadratic v.s. Linear Model }]{
\begin{minipage}[t]{0.3\linewidth}
\centering
\includegraphics[width=2 in]{phase.pdf}
\end{minipage}%
}%
\subfigure[\scriptsize{Empirical v.s. Theoretical Results}]{
\begin{minipage}[t]{0.3\linewidth}
\centering
\includegraphics[width=2 in]{lin_vs_qua.pdf}
\end{minipage}%
}%
\centering
\caption{Empirical results on the convergence rate of quadratic model with spectral decay v.s. traditional linear model. (a) and (b) show the curve of mean error against the number of iteration steps, with $\alpha = 2.5, \beta = 1.5$ in (a) and $\alpha =3,\beta=2$ in (b), respectively. (c) show the logarithmic curve of final mean loss against the sample size, where the solid lines represent the empirical results and the dashed lines represent the theoretical rates. }
\label{experiment1}
\end{figure}








However, significant gaps persist in explaining the scaling laws when relying on simplified linear models. A primary limitation of these models is their inability to capture the feature learning process, a mechanism that is widely regarded as crucial to the empirical success of deep neural networks \citep{lecun2015deep}. This process enables neural networks to autonomously extract high-quality hierarchical representations from data, leading to effective generalization. This limitation arises because linear models inherently restrict the capacity to learn feature representations and tend to rapidly diverge from the initial conditions. In linear models, the parameter trajectory under SGD follows a predictable pattern: the estimation bias contracts at a constant rate proportional to the eigenvalue of each feature, while variance accumulates uniformly. However, neural networks are not constrained by an initial feature set; instead, they adaptively reconfigure their internal representations through coordinated parameter updates. The feature learning can often improve the performances. For example,  even the enhanced convolutional neural tangent kernel based on the linearization of neural networks in the infinite-width limit has a performance gap compared to neural networks on the CIFAR10 dataset \citep{li2019enhanced}.

%We provide deeper insights into this argument through a series of empirical demonstrations. First, it is empirically noticed that Neural networks have a strong ability to learn features. Even the enhanced convolutional neural tangent kernel based on the linearization of neural networks in the infinite-width limit has a performance gap compared to neural networks on the CIFAR10 dataset \citep{li2019enhanced}. Therefore, effectively characterizing the feature learning process beyond linear models is an important issue. 


%Moreover, the excess risk in neural networks exhibits a multi-stage phenomenon empirically. As shown in Figure~\ref{experiment1} (a), the excess risk of quadratic models oscillates at each stage before rapidly decreasing, whereas linear models demonstrate a monotonous decline. \congfang{latter check}


\iffalse
Still, significant gaps towards explaining the scaling law unavoidably exist when working on the simple linear model. First of all, linear models cannot capture the feature learning process. 
Nevertheless, the empirical success of
neural networks is argued to  largely result from their ability to learn high quality features from the data. Fig \ref{} illustrates an example, when the true parameter opposes the covariance spectrum. In the linear model, SGD   becomes non-efficient.. Does not due to the SGD algorithm... model...
%to its target...
%no feature learning:   neural networks is able to learn useful feature representations during training, which is argued to be the key factor that contributes to its the success.  Linear models, unfortunately, cannot capture this process. \congfang{Fig \ref{} illustrates an example,   when the input signal does not positively respond
%to its target....} 
Moreover, linear  dynamics is too simplified.  The learning dynamics plays a fundamental to regularizing the parameters that drives the model towards suitable solution. In the linear dynamic, the estimation bias contracts at a constant rate for each feature and variance grows at a uniform rate \congfang{...}.  In comparison, the learning process of neural networks is much more complex..
 Empirically observed multiple stages. \congfang{constant->variants}. See Fig. \ref{}.
\fi 

In this paper, we study a quadratically parameterized model:  $f\left (  \mathbf{x} \right ) = \left \langle  \mathbf{x}, \mathbf{v}^{\odot 2} \right \rangle $, where $\mathbf{x}\in \mathbb{H}$ is the input data and 
% $\mathbf{v}_+,\mathbf{v}_-\in \mathbb{H}$ 
$\mathbf{v}\in \mathbb{H}$ are the model parameters, 
as an alternative testbed to study the scaling law.  This model can be regarded as a ``diagonal'' linear neural network and exhibits feature learning capabilities. As shown in Figure~\ref{experiment1} (a) and (b), linear models exhibit a empirically suboptimal convergence rate on excess risk under SGD. This suboptimal performance is not solely attributed to the limitations of SGD itself. As demonstrated in Figure~\ref{experiment1} (c), SGD achieves a significantly faster convergence rate on excess risk in quadratically parameterized models, aligning with our theoretical findings. Note that the previous studies for quadratic models  \citep{haochen21shape} often assume a sparse ground
truth for the model where the variance will explode with the number of non-zero elements increasing and no polynomial rates are established. We instead consider an infinitely dimensional data input and ground
truth, whose signal exhibits certain power-law
decay rates.  Specifically,  for constants $\alpha,\beta>1$, we assume that the eigenvalues of the covariance matrix decay as $\lambda_i\asymp i^{-\alpha}$ and that $\mathbf{v}^*_i$ the $i$-th alignment coordinate of the ground truth satisfies $\lambda_i\left(\mathbf{v}^*_i\right)^4\asymp i^{-\beta}$.
Suppose the model has access only to the top $M$-th covariates and their response, we study the excess risk of quadratically parameterized predictor with $M$ parameters and trained by SGD with tail geometric decay schedule of step size, given $T$ training samples. %The setting makes us a direct comparison with linear model 



We establish the upper bound for the excess risk, demonstrating that its follows a piecewise 
% \congfang{check!} 
power law with respect to both the model size and the sample size throughout the training process. More concretely, the upper bounds of the excess risk $\mathcal{R}_M(\upv^T)-\bbE[\xi^2]$ behaves as 
\begin{equation}\nonumber
\small
\begin{aligned}
    \underbrace{\frac{1}{M^{\beta-1}}}_{\mathrm{approximation}}+\underbrace{\frac{\sigma^2D}{T}}_{\mathrm{variance}}+\underbrace{\frac{D}{T}+\frac{1}{D^{\beta-1}}\mathds{1}_{D<M}}_{\mathrm{bias}}
   % \underbrace{\frac{1}{M^{\beta-1}}}_{\mathrm{approximation}}+\underbrace{\frac{\sigma^2D}{T}+\frac{\sigma^2\eta^2T}{D^{\alpha+\beta-1}}\mathds{1}_{D<M}}_{\mathrm{variance}}
   %      +\underbrace{\frac{D}{T}+\left(\frac{1}{D^{\beta-1}}+\frac{\eta^2T}{D^{\alpha+\beta-1}}\right)\mathds{1}_{D<M}}_{\mathrm{bias}},
\end{aligned}
\end{equation}
where $D=\min\left\{T^{1/\max\{\beta,(\alpha+\beta)/2\}},M\right\}$ serves as the effective dimension. 
% \zhz{here please make sure one name of D, and change it in the paper}
%\congfang{why you use N? It is D? like effective dimension???} 
The above result
reveals that, for a fixed sample size, increasing the model
size is initially beneficial, but the returns begin to diminish
once a certain threshold is reached. Moreover, when the model size is large enough, SGD achieves the excess risk as $\Tilde{\calO} \left ( T^{-1+\frac{1}{\beta} } \right ) $ when $\alpha\le \beta$, and the excess risk as $\Tilde{\calO} \left ( T^{-\frac{2\beta-2}{\alpha+\beta} } \right ) $.
This indicates that when the true parameter aligns with the covariance spectrum ($\alpha \le \beta$), the quadratic model, similar to the linear model, achieves the optimal rate~\citep{zhang2024optimality}.  On the other hand, when the true parameter opposes the covariance spectrum ($\alpha > \beta$), SGD achieves a rate of $\Tilde{\mathcal{O}} \left( T^{-\frac{2\beta - 2}{\alpha + \beta}} \right)$ in the quadratic model, which outperforms the best rate SGD can achieve in the linear model $\Tilde{\mathcal{O}} \left( T^{-\frac{\beta - 1}{\alpha}} \right)$~\citep{zhang2024optimality}.


 
 %On the other hand, in the setting where the true parameter opposes the covariance spectrum, that is $\alpha> \beta$,   %we obtain an upper bound  and a lower bound..  %Despite its sub-optimality, it outperforms that of the linear model by $123$, 
%Moreover, our instance-specific lower bound distinguishes the rate of SGD from the information-theoretic lower bound, thereby demonstrating the importance of examining algorithm-dependent generalization within the context of scaling laws.

In our analysis, we characterize the learning process of SGD into two typical stages. 
In the first ``adaptation'' stage, the algorithm implicitly truncates the first $D$ coordinates 
to form the effective dimension set $\mathcal{S}$, based on the initial conditions. 
The variables within $\mathcal{S}$ grow and oscillate around the ground truth, while the remaining variables are constrained by a constant multiple of the ground truth, leading to an acceptable excess risk. 
In the second ``estimation'' stage, the variables in the effective dimension set $\mathcal{S}$ converge to the ground truth, while the other variables remain within a region that produces a tolerable level of excess risk.  The advantage beyond the linear model is easy to be observed in the “estimation” stage, where the step size is scaled by the certain
magnitude of the ground truth due to the adaption, resulting in a faster convergence rate for the bias term. %particularly for the coordinates with a small input signal but strong response one.


Due to the non-convex nature of the quadratic model, our analysis is much more involved. The main challenge in our analysis is the diverse scaling of the ground truth signals and the anisotropic gradient noise caused by the diverse data eigenvalues. This requires us to provide individual bounds for the model parameters through the analysis and proposes a refined characterization for the learning process.
%and carefully study the anisotropic gradient noise.
Both challenges do not exist in the traditional analysis in the quadratic model, since they consider near isotropic input data and $\Theta(1)$ ground truth \citep{haochen21shape}. 

%Compared with the previous analysis for a quadratic model that assumes a sparse ground truth, \congfang{...}. Our analysis to deal with the decaying ground
%truth provides new insights into understanding
%the dynamic of the quadratic model.
\vspace{0.1in}
We summarize the contribution of this paper as follows: \vspace{-0.05in}
\begin{itemize}
    \item The learning curves of SGD is proposed based on a quadratically parameterized model that emphasizes feature learning. We establish excess risk against sample and model sizes.
\vspace{-0.05in}
    \item A theoretical analysis for the dynamic of the quadratic model is offered, where we propose a new characterization to deal with the decaying ground truth and anisotropic gradient noise.
%truth is of independent interest in online non-convex optimization.
\end{itemize}



\iffalse
In this paper, we investigate the least square regression of a high-dimensional quadratically-parameterized model under the power-law decay of the covariance spectrum and prior assumptions. 
These assumptions are commonly employed to evaluate the min-max optimality, particularly for non-parametric regression, as characterized by the capacity and source conditions.
Based on these assumptions, the linear model excess risk exhibits a polynomial decay with respect to the model  and sample sizes, aligning with the experimentally observed neural scaling law.
Specifically, 
We establish both upper and lower instance bounds for the excess risk, showing that the excess risk of SGD follows a precise power law with respect to both the model size and the sample size throughout the training process.

The training of this model constitutes a non-convex optimization problem,  inherently more complex than linear regression.
Unlike sparse settings, where zero-valued variables are trapped around  zero and only the selected variables contribute to convergence, we must identify how SGD adaptively selects relevant variables for inclusion in our framework.
For our analysis, the training process consists of two stages. In the first ``screening'' stage, SGD adaptively select an optimal-set, where the variables within the optimal-set approach to the true values, while the remaining variables are constrained by the both ground truth and initial values. In the second ``estimation'' stage, the variables in the optimal-set converge to the ground truth, while the other variables remain within a region that produces a tolerable level of excess risk. 
Our results indicate that, when the noise $\sigma^2$ is treated as constant, the excess risk $\mathcal{R}_M\left ( \mathbf{v}_T  \right )$ behaves as 
\begin{equation}\nonumber
\small
\begin{aligned}
    % \mathcal{R}_M\left ( \mathbf{v}_T  \right )=
    \left\{\begin{matrix}
\sigma^2+\Tilde{\calO}\left(T^{-(1-\frac{1}{\beta})}\right)+\calO\left(M^{-(\beta-1)}\right),&\text{ if } \alpha\le \beta, \\
\sigma^2+\Tilde{\calO}\left(T^{-\frac{2\beta-2}{\alpha+\beta}}\right)+\calO\left(M^{-(\beta-1)}\right),&\text{ if } \alpha> \beta.
\end{matrix}\right.
\end{aligned}
\end{equation}
Based on this formula, when the size of the model is large enough (i.e., $M>\Tilde{O}(T^{\frac{1}{\max\{\beta, (\alpha+\beta)/2\}}})$), SGD achieves the optimal rate $\Tilde{\calO} \left ( T^{-1+\frac{1}{\beta} } \right ) $,  as shown in \citet{caponnetto2007optimal}, when $\alpha\le \beta$.
On the other hand, when $\alpha> \beta$, although SGD remains a sub-optimal rate $\Tilde{\calO}\left(T^{-\frac{2\beta-2}{\alpha+\beta}}\right)$,  it still outperforms SGD in the complexity  $\Tilde{\calO} \left ( T^{-\frac{\beta-1}{\alpha} } \right )$ of linear regression.
This advantage arises from the ``estimation'' stage, where, at each feature direction, the step size is adaptively scaled by the corresponding magnitude of the ground truth, thereby enhancing the implicit regularization effect of SGD.
Our neural scaling law reveals that, for a fixed sample size, increasing the model size is initially beneficial, but the returns begin to diminish once a certain threshold is reached. This provides a guiding principle for selecting the optimal model parameter scale under a given sample size.


Our contributions:
\begin{itemize}
    \item We delineate the neural scaling law of a high-dimensional, quadratically parameterized model trained via SGD under a power-law decay of the covariance spectrum and piror assumptions, providing both upper and lower instance bounds for the excess risk throughout the training process.
    \item we demonstrate that the excess risk adheres to a precise power law in relation to both model size and sample size, and further investigate the optimal model size selection for a given fixed sample size.
    \item We conduct a comprehensive analysis of the optimality of SGD for this nonlinear model when the model size is sufficiently large, highlighting the advantages of nonlinear model over its linear counterparts.
\end{itemize}
\fi