\subsection{Notation} 
%Let $\left \{ \upu_i \right \} _{i=1}^{\infty }$ denote an orthonormal basis of $\bbH$.
 %We use the convention 
%$\mathcal{O} \left ( \cdot  \right ) $ and $\Omega \left ( \cdot  \right ) $ to denote lower and upper bounds with a universal constant.
 %$\widetilde{\mathcal{O} } \left ( \cdot  \right ) $ and $\widetilde{\Omega  } \left ( \cdot  \right ) $  ignore the   polylogarithmic dependence. Use $f\lesssim g$ to denote $f=\mathcal{O} \left ( g  \right ) $, $f\gtrsim g$ to denote $f=\Omega \left ( g \right ) $. We use notation $f\asymp g$, if $g\lesssim f\lesssim g$. 
%For a Hilbert space $\mathbb{H}$, we assume that $\mathbb{H}$ is separable and has finite or countably infinite dimensions.
%With inner product $\left \langle \cdot ,\cdot \right \rangle $, denote $\mathbf{u}\otimes \mathbf{v} :\ \mathbb{H}\to  \mathbb{H},\ \left ( \mathbf{u}\otimes \mathbf{v}  \right )\left ( \mathbf{w}  \right )=\left \langle \mathbf{v},   \mathbf{w}  \right \rangle  \mathbf{u}   $, for any $\mathbf{u}, \mathbf{v},\mathbf{w} \in \bbH$.


%$\upe_i$ is the canonical basis vector in $\bbR^M$

%Give an orthonormal basis $\{\upu_i\} _{i=1}^{\infty }$ of $\bbH$, for $\upv\in\bbH$,  define $\upv^{\odot2}:=\sum_{i=1}^{\infty}$ $\langle \upv,\upu_i\rangle^2\upu_i$. 

%Specifically, when $\upv\in\bbR^d$, $\upv^{\odot2}$ denotes the element-wise square of $\upv$. For a random variable $x$, we call $x$ is symmetric if, there exists $a\in\bbR$ such that the cumulative distribution function $F_x$ satisfies $F_x(a-c)=1-F_x(a+c)$ for any $c\in\bbR$. 

In this section, we introduce the following notations adopted throughout this work. Let $\mathcal{O}(\cdot)$ and $\Omega(\cdot)$ denote upper and lower bounds, respectively, with a universal constants, while $\widetilde{\mathcal{O}}(\cdot)$ and $\widetilde{\Omega}(\cdot)$ ignore polylogarithmic dependencies. For functions $f$ and $g$: $f \lesssim g$ denotes $f = \widetilde{\mathcal{O}}(g)$; $f \gtrsim g$ denotes $f = \widetilde{\Omega}(g)$; $f \asymp g$ indicates $g \lesssim f \lesssim g$. Let $\mathbb{H}$ be a separable Hilbert space with finite or countably infinite dimensions, equipped with an inner product $\langle \cdot, \cdot \rangle$. For any $\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathbb{H}$, we define the operator $\mathbf{u} \otimes \mathbf{v}: \mathbb{H} \to \mathbb{H}$ as:
\begin{equation}
    (\mathbf{u} \otimes \mathbf{v})(\mathbf{w}) = \langle \mathbf{v}, \mathbf{w} \rangle \mathbf{u}.
\end{equation}
Let $\{\mathbf{e}_i\}_{i=1}^M$ denote the canonical basis in $\mathbb{R}^M$. For an arbitrary orthonormal basis $\{\mathbf{u}_i\}_{i=1}^\infty$ of $\mathbb{H}$, we define the nonlinear operator $\odot^2: \mathbb{H} \to \mathbb{H}$ acting on $\mathbf{v} \in \mathbb{H}$ as:
\begin{equation}
    \mathbf{v}^{\odot 2} := \sum_{i=1}^\infty \langle \mathbf{v}, \mathbf{u}_i \rangle^2 \mathbf{u}_i.
\end{equation}
Specially, in finite-dimensional Euclidean space ($\mathbf{v} \in \mathbb{R}^d$), this reduces to element-wise squaring as: $(\mathbf{v}^{\odot 2})_j = \upv_j^2$ for $j \in [d]$. Given a positive integer $N \leq M$ and a vector $\upv \in \mathbb{R}^M$, denote $\upv_{1:N}$ as $(\upv_1, \ldots, \upv_N)^{\top}$. Moreover, for a diagonal matrix $\diag\{\gamma_1,\cdots,\gamma_M\}=\upH\in\bbR^{M\times M}$, denote $\upH_{n^*:n^{\dagger}}:=\sum_{i=n^*}^{n^{\dagger}}\gamma_i\upe_i\upe_i^{\top}$, where $0\leq n^*\leq n^{\dagger}\leq M$ are two integers. 
% Finally, a random variable $x$ is called \emph{symmetric} if there exists $a \in \mathbb{R}$ such that its cumulative distribution function $F_x$ satisfies:
% \begin{equation}
%     F_x(a - c) = 1 - F_x(a + c), \quad \forall c \in \mathbb{R}.
% \end{equation}

\subsection{Quadratically Parameterized Model}
%We denote the covariate vector by $\mathbf{x}\in \bbH$, where $\bbH$ is a  Hilbert space that has finite or countably infinite dimensional, and the corresponding response by $y\in \mathbb{R}$. Let $\left \{ \upu_i \right \} _{i=1}^{\infty }$ denote an orthonormal basis of $\bbH$. 
%According to [Theorem 5.11, \citep{Brezis2010FunctionalAS}], it is known that every separable Hilbert space has an orthonormal basis, thus $\left \{ \upu_i \right \} _{i=1}^{\infty }$ is well defined. 
Denote the covariate vector by $\mathbf{x}\in \bbH$, and the corresponding response by $y\in \mathbb{R}$.
We focus on a quadratically parameterized model and measure the population risk of parameter $\mathbf{v}$ by the mean squared loss as:
\begin{equation}\nonumber
    \mathcal{R} \left ( \mathbf{v} \right )=\mathbb{E}_{\left ( \mathbf{x},y  \right ) \sim \mathcal{D} } \left ( \left \langle \mathbf{x},\mathbf{v}^{\odot 2} \right \rangle  -y  \right )^2 ,
\end{equation}
where the expectation is taken over the joint distribution $\mathcal{D}$ of $\left ( \mathbf{x} ,y \right ) $.
In this paper, we study the quadratically parameterized model as $\left \langle \mathbf{x},\mathbf{v}^{\odot 2} \right \rangle$, and assume that the response is generated by $y=\langle \mathbf{x},\mathbf{v}^{*\odot 2}\rangle +\xi $, where $\mathbf{v}^*\in \bbH$ is the ground truth parameter and $\xi$ is the noise independent of the covariate $\mathbf{x}$.
One can generally use the parameterization as $y=\langle\upx,\upv_+^{*\odot2}-\upv_-^{*\odot2}\rangle+\xi$ by the same technique as \citet{woodworth2020kernel}.
 


%We assume the model is well-specified, i.e., there exists a ground truth parameter $\mathbf{v}^*\in \bbH$ such that the label $y$ is generated by $y=\langle \mathbf{x},\mathbf{v}^{*\odot 2}\rangle +\xi $, where noise $\xi$ is independent of the covariate $\mathbf{x}$ with zero mean $\mathbb{E} \xi =0$. One can notice that $\mathbf{v}^*$ is the optimum of the population risk $\mathcal{R} \left ( \mathbf{w} \right )$. It is worth to note that, when each $\langle\upx,\upu_i\rangle$ is symmetric with 0, and $\langle\upx,\upu_i\rangle$ and $\langle\upx,\upu_j\rangle$ are independent for any $i\neq j$, the aforementioned generation method of label $y$ is equivalent to $y=\langle\upx,\upv_+^{*\odot2}-\upv_-^{*\odot2}\rangle+\xi$. This equivalence is derived from that the sign of $\langle\upx,\upu_i\rangle$ can be altered for any coordinate $i$. Without loss of generality, we assume $\langle\upv^*,\upu_i\rangle\geq0$ for any $i\geq1$.

We then present a quadratically parameterized predictor with $M$ trainable parameters, which serves as the foundation for our algorithm design and subsequent theoretical analysis. Let $\mathbf{H}=\mathbb{E}[\mathbf{x}\otimes \mathbf{x}] $ denote the data covariance operator. The eigenstructure of $\mathbf{H}$ is given by  $\mathbf{H}=\sum_{i=1}^{\infty}\lambda_i\upu_i\otimes \upu_i$, where $\left \{ \lambda_i \right \} _{i=1}^{\infty }$ are the eigenvalues arranged in non-increasing order, and $\left \{ \mathbf{u}_i \right \} _{i=1}^{\infty }$ are the corresponding eigenvectors of $\mathbf{H}$, which form an orthonormal basis for $\mathbb{H}$. For any vector $\mathbf{y}\in \bbH$, denote $\mathbf{y}_i=\langle\mathbf{y},\mathbf{u}_i\rangle$ as its $i$-th coordinate with respect to the above eigenbasis.
Let $\Pi_M =\sum_{i=1}^M\upe_i\otimes\upu_i$ be a projection operator from $\bbH$ to $\bbR^M$.
%, where $\upe_i$ is the canonical basis vector in $\bbR^M$.
We assume that we access only to the top $M$ covariates, denoted by $\Pi_M \mathbf{x} $. Subsequently, we define the predictor with $M$ trainable parameters $\mathbf{v}\in \bbR^M$ as: 
\begin{equation}\nonumber
    f_{\mathbf{v} }\left (  \mathbf{x} \right ) = \left \langle \Pi_M \mathbf{x}, \mathbf{v}^{\odot 2}   \right \rangle ,\ \mathbf{v}\in \bbR^M.
\end{equation}
We also denote the corresponding risk as 
\begin{equation}\label{RM}
    \mathcal{R}_{M}\left ( \mathbf{v}  \right )=\mathcal{R}\left ( \Pi _M\mathbf{v}  \right )=\mathbb{E}_{\left ( \mathbf{x},y  \right ) \sim \mathcal{D} }\left ( \left \langle \Pi_M \mathbf{x}, \mathbf{v}^{\odot 2} \right \rangle -y \right ) ^2. 
\end{equation}

%This predictor captures the first $M$ most important features, which can be regarded as an analogy to a neural network with  $M$ parameters. Specifically, the quadratically parameterized model we investigate is a particular ``diagonal" linear neural network, which has attached attention as a simplified neural network model that preserves feature learning capabilities. 
%\congfang{Say top $M$ resembles sketch in Jason Lee Linear.} 
%Compared with traditional linear model with fixed feature representations, 
The top $M$ covariates resemble the sketched covariates proposed by \citet{lin2024scaling}. In contrast with linear model, quadratic parameterized model allows discovery of discriminative features through learning towards dominant directions of target. Thus, it models the feature learning mechanism  while ensuring analytical tractability.



%\congfang{direct compare with linear model}
%This enables us to go beyond the limitations of linearity in previous theoretical analyses and derive scaling laws that are more aligned with those observed in neural networks.

%Since we focus on top M covariates in this paper, we use $\zeta_{M+1:\infty}$ to denote random variable $\sum_{i=M+1}^{\infty}\upx_i(\upv_i^*)^2$. 



%Unlike previous theoretical analyses of scaling laws, which have predominantly focused on linear models, our paper extends beyond the limitations of linearity and derives scaling laws for nonlinear models. Specifically, the quadratically-parameterized model we investigate is a specific "diagonal" linear neural network, which has drawn attention as a simplified yet nonlinear simplification of neural networks. 
%Meanwhile, the training of this model is framed as a non-convex optimization problem, which also possesses feature selection capabilities. This enables it to better capture the training dynamics of neural networks compared to the previously studied linear models.
%\begin{definition}[Sub-Gaussian Random Variable]
 %   A random variable $x$ with mean $\bbE x$ is sub-Gaussian if there is $\sigma\in\bbR_+$ such that
%    \begin{align}
 %       \bbE\left[e^{\lambda(x-\bbE x)}\right]\leq e^{\frac{\lambda^2\sigma^2}{2}},\quad \lambda\in\bbR.\notag
%    \end{align}
%\end{definition} 

\subsection{Data Distribution Assumptions}
We make the following assumptions of data distribution.
\begin{assumption}[Anisotropic Gaussian Data and Sub-Gaussian Noise]\label{ass-d}%[Distributional Assumptions]
    \item[\textbf{[A$_\text{1}$]}] (Independent Gaussian Data) For any $i\ge 1$, the covariate $\upx_i\sim\calN(0,\lambda_i)$. For any $i\neq j$, $\mathbf{x}_i$ and $\mathbf{x}_j$ are independent.
    % \item[\textbf{[A$_\text{1}$]}] (Coordinate-Bounded and Independent  Covariates) For any $i\ge 1$, we assume that $\bbE\mathbf{x}_i =0$ and $\bbE\left|\upx_i\right|^4\leq C\bbE\left|\upx_i\right|^2$ for some constant $C>0$. Furthermore, for any $i\ge 1$, $\upx_i$ is symmetric and sub-Gaussian with parameter $\lambda_i^{1/2}$, where $\lambda_i\in\bbR_+$, i.e., $\bbE\left[e^{\lambda\upx_i}\right]\leq e^{\frac{\lambda_i\lambda^2}{2}}$ for any $\lambda\in\bbR$. Additionally, for any $i\neq j$, the covariates $\mathbf{x}_i$ and $\mathbf{x}_j$ assumed to be independent.   
    \item[\textbf{[A$_\text{2}$]}] (Sub-Gaussian Noise) The noise $\xi$ is zero-mean and sub-Gaussian with parameter $\sigma_\xi > 0$, satisfying $\mathbb{E}[e^{\lambda \xi}] \leq e^{\sigma_\xi^2 \lambda^2/2}$ for all $\lambda \in \mathbb{R}$.
    % \item[\textbf{[A$_\text{2}$]}] The noise $\xi$ has zero mean, $\mathbb{E}[\xi]=0$,  and is sub-Gaussian with parameter $\sigma_{\xi}>0$, i.e., $\bbE\left[e^{\lambda\xi}\right]\leq e^{\frac{\sigma_{\xi}^2\lambda^2}{2}}$ for any $\lambda\in\bbR$.
\end{assumption}
\begin{remark}\label{remark-1}
%\congfang{Gaussian design.  compare to linear model}
    % The Gaussian covariate assumption aligns with the whitened input space paradigm, implicitly adopted in theoretical analysis of optimization dynamics and generalization \citep{bubeck2015convex, bartlett2020benign, haochen21shape}. Several studies on linear models \citep{wu2022last} analyze the associated Gaussian noise structures. 
The assumption for independent Gaussian data is also used in other analyses for the quadratic model, such as \citet{haochen21shape}, whereas, we allow non-identical covariates.  The independence assumption resembles (is slightly stronger than) the RIP condition, and is widely adopted in feature selection, e.g. \citet{candes2005decoding},  to ensure computational tractability, because in the worst case, finding sparse features is NP-hard \citep{natarajan1995sparse}. To weaken the independence assumption, our analysis may be extended to only assume that the input has a low correlation with some weak regularity condition.
%the low correlation of  input and with some weak regularity condition on its distribution,  to weaken the independence assumption. 
However, the analysis is more involved, and we leave it as a future work. 
    
%    The assumption of independent Gaussian-distributed data is justified by several theoretical and algorithmic rationales. 
    \iffalse
   \emph{Recovery Guarantees via RIP}: 
    As demonstrated by \citet{candes2005decoding}, measurement matrices with independent Gaussian entries satisfy the Restricted Isometry Property (RIP), which ensures that sparse signals can be exactly recovered via $\ell_1$-minimization.
    % measurement matrices containing independent Gaussian entries satisfy the Restricted Isometry Property (RIP). This property guarantees exact reconstruction of sparse signals through $\ell_1$-minimization. 
    \emph{Advantages of Independent Random Designs}: \citet{donoho2006compressed} highlights the advantages of independent random matrices (e.g., Gaussian/Bernoulli ensembles) for signal recovery. Such designs inherently eliminate structured feature correlations, which simplifies theoretical analysis and improves algorithmic robustness.
    \emph{Challenges from Feature Correlation}: 
    % In contrast, \citet{tibshirani1996regression} notes that when features are highly correlated, the Lasso tends to arbitrarily select one variable from a correlated group while ignoring others, leading to unstable and potentially misleading sparse solutions. 
    As \citet{tibshirani1996regression} observes, Lasso regression exhibits arbitrary variable selection within correlated feature groups, discarding other predictors and producing unstable sparse solutions with potential interpretation errors.
    % This limitation was further corroborated by \citet{zou2005regularization}, who emphasized that correlated features can degrade variable selection consistency.
    This limitation is subsequently validated by \citet{zou2005regularization}, emphasizing that feature correlation compromises variable selection consistency.
    \fi
\end{remark}
We derive the scaling law for SGD under the following power-law decay assumptions of the covariance spectrum and prior conditions.
\begin{assumption}[Specific Spectral Assumptions]\label{ass-ss}
    \item[\textbf{[A$_\text{3}$]}] (Polynomial Decay Eigenvalues) There exists $\alpha>1$ such that for any $i\ge 1$, the eigenvalue of data covariance $\lambda_i$ satisfy $\lambda_i\asymp i^{-\alpha}$.
    \item[\textbf{[A$_\text{4}$]}] (Source Condition) There exists $\beta>1$ such that the ground truth parameter $\upv^*$ satisfies that for any $i\ge 1$, 
 $\lambda _i\left ( \mathbf{v}_i^{*}  \right ) ^4 \asymp i^{-\beta}$.
\end{assumption}

%\congfang{The polynomial decay of eigenvalues and the ground truth parameters are widely used to study the convergence curves and scaling laws for linear model. }

\begin{remark}
    %第一句，多项式decay被广泛的在线性模型当中考虑，这种考虑是源自于NTK的empirical观察...能不能把一句话用过来先解释线性这么假设的原因和合理性
    %第二句，第二句说kernel，他和RKHS是一致的，引用一下文章
    %第三句说因此我们的假设是合理的，然后进一步的，和线性模型进行比较
     The polynomial decay of eigenvalues and the ground truth has been widely considered to study the scaling laws for linear models like random feature model \citep{bahri2024explaining, bordelon2024a} and infinite dimensional linear regression \citep{lin2024scaling}, based on empirical observations of NTK spectral decompositions on the realistic dataset \citep{bahri2024explaining,bordelon2021learning}. It is used in slope functional regression \citep{10.1214/009053606000000830}, and also analogous to the capacity and source conditions in RKHS \citep{wainwright2019high,bietti2019inductive}. Given that the optimization trajectory of linear models is intrinsically aligned with the principal directions of the covariate feature space, this alignment motivates us to adopt analogous assumptions for our model, thereby enabling direct comparison of learning dynamics through feature space decomposition.
\end{remark}


%The power-law decay of covariance spectra has been both experimentally and theoretically observed for various kernels, such as the Sobolev kernel, Laplace kernel, and NTK, all of which exhibit a power-law decay \citep{wainwright2019high,bietti2021deep}.

\iffalse
a standard assumption posits that the regression function $f_*$ resides within an interpolation space $\left [ \mathcal{H}  \right ] ^s$ of the RKHS $\mathcal{H}$ for some $s>0$ \citep{}.



Specifically, denote the covariance operator $\Sigma :\mathcal{H} \to \mathcal{H}$ as $\Sigma\phi_i=\lambda_i  \phi_i$. Then, for $s\ge 0$ the interpolation space $\left [ \mathcal{H}  \right ] ^s$ is defined as 
\begin{equation}\nonumber
    \left [ \mathcal{H}  \right ] ^s=\left \{ \sum _{i=1}^{+  \infty }a_i\lambda _i^{\frac{s}{2} }\phi_i  \Big| \sum _{i=1}^{+  \infty }\left ( a_i \right )^2<+\infty    \right \}. 
\end{equation}
Under the polynomial decay of Hessian eigenvalues and the source condition belonging to $\left [ \mathcal{H}  \right ]^{s}$ with $s<\frac{\beta-1}{\alpha } $, given $T$ samples, the optimal min-max rate for regression functions in $\left [ \mathcal{H}  \right ] ^s$ is proposed  by \citet{caponnetto2007optimal} to be $\mathcal{O} \left ( T^{ -\frac{\alpha s}{\alpha s+1}}  \right ) $.
\fi

%\congfang{add comparison with linear model: 1. provide a direct comparison with linear parameterization because
%linear works on principle space..  $M$.. }

% Our analytical framework is grounded in general spectral conditions formalized below, with Assumption \ref{ass-ss} serving as a specialized case that extends the broader principles in Assumption \ref{ass-s}

% \iffalse
% \begin{assumption}[General Spectral Assumptions]\label{ass-s}
%     The eigenvalue $\{\lambda_i\}_{i\geq1}$ and ground truth parameter $\upv^*$ satisfy: (1) $\lambda_i(\upv_i^*)^2$ is monotonically non-increasing over $i\geq 1$; (2) $\sum_{i\geq1}\lambda_i(\upv_i^*)^4\leq\infty$.
% \end{assumption}
% \congfang{why you state this assumption????}
% \fi 


\subsection{Algorithm}
We employ SGD with a geometric decay of step size to train the quadratically parameterized predictor $f_{\mathbf{v} }$ to minimize the objective \eqref{RM}. Starting at $\mathbf{v}^0$, the iteration of parameter vector $\mathbf{v}\in \mathbb{R}^M$ can be represented explicitly as follows:
\begin{equation}\nonumber
\small
\begin{aligned}
    \mathbf{v}^t= &\mathbf{v}^{t-1}-\eta _t\left ( f_{\mathbf{v}^{t-1}} \left ( \mathbf{x}^t \right )-y^t \right )\left (\mathbf{v}^{t-1}\odot \Pi_M\mathbf{x}^{t}    \right ) \\
=  &\mathbf{v}^{t-1}-\eta _t\left (\left \langle \left ( \mathbf{v}^{t-1} \right )^{\odot 2},\Pi_M\mathbf{x}^t     \right \rangle -y^t \right )\left(\mathbf{v}^{t-1}\odot \Pi_M\mathbf{x}^{t}\right ),
\end{aligned}
\end{equation}
for $t=1,\dots,T$, where $\left\{\left ( \mathbf{x}^t,y^t\right)\right \}_{t=1}^{T}$ are independent samples from distribution $\mathcal{D}$ and $\left\{ \eta_t\right\}_{t=1}^{T}$ are the step sizes.

We use the tail geometric decay of step size schedule as describe in \citet{wu2022last}. The step size remains constant for the first $T_1+h$ iterations where $h$ denotes the middle phase length and $T_1:=\lfloor(T-h)/\log(T-h)\rfloor$. Then the step size halves every $T_1$ steps. Specifically, the decay schedule of step size is given by:
\begin{equation}\nonumber
\begin{aligned}
\eta_t=\begin{cases}
    \eta,\quad &0\le t\le T_1+h,
    \\
    \eta/2^l,\quad &T_1+h<t\le T,\, \, l=\left \lfloor (t-h)/T_1 \right\rfloor,
\end{cases}
 %    \eta _t=\left\{\begin{matrix}&\eta,\quad 1\le t\le T_1,
 % \\ &\eta/2^l,\quad T_1<t\le T_2,\quad l=\left \lfloor t-T_1/\left \lfloor\mathrm{log}_2T_2\right \rfloor \right \rfloor .
% \end{matrix}\right.
\end{aligned}
\end{equation}
%The tail geometric decay schedule of step size we adopt is similar to the commonly used step size decay schedule in deep learning \citep{}. This proposed design offers two distinct advantages: during the first stage, a constant step size facilitates rapid convergence of the top coordinates with dominant directions to a neighborhood of the ground truth; subsequently, the geometric decay schedule of step size guarantees global convergence to the ground truth, aligning with the warm-up strategy commonly employed in deep learning \citep{}. Moreover, geometric decay schedule of step size has shown significant efficiency than other decay schedules, such as polynomial decay \citep{}. \congfang{which conclusion is yours???? 1 resemble deep learning warm-up 2. show previous more efficient than poly 3. in your setting what is the true advantage...}

The integration of warm-up with subsequent learning rate decay has become a prevalent technique in deep learning optimization \citep{goyal2017accurate}. Within the decay stage, geometric decay schedules have demonstrated superior empirical efficiency compared to polynomial alternatives, as geometric decay achieves adaptively balancing aggressive early-stage learning with stable late-stage refinement.  \citep{ge2019step}. Motivated by these established advantages, our step size schedule design strategically combines an initial constant stage that enables rapid convergence of dominant feature coordinates to the vicinity of ground truth parameters with a subsequent geometrically decaying stage that ensures fast convergence through progressively refined updates. This hybrid approach inherits the computational benefits of geometric decay while maintaining the stability benefits of warm-up initialization, creating synergistic effects that polynomial decay schedules cannot achieve \citep{bubeck2015convex}.

%The advantages of such design lie in that in the first stage with a constant step size, the several top coordinates with dominant directions can fast converge to the neighborhood of ground truth; then in the second stage with a geometrical decay schedule of step size, the objective globally converges to the ground truth.

%It is more practical than the constant step size with tail averaging which is often used in theoretical analyses of SGD. 


\begin{algorithm}[t]
	\caption{Stochastic Gradient Descent (SGD)}
    \label{SGD}
	\begin{algorithmic}
        \STATE {\bfseries Input:} Initial weight $\upv_0=\Omega(\min\{1,M^{-(\beta-\alpha)/4}\})\mathbf{1}_M$, initial step-size $\eta$, total sample size $T$, middle phase length $h$, decaying phase length $T_1=\left\lfloor (T-h)/\log(T-h)\right\rfloor$.
		\WHILE{$t\leq T$}
		\IF{$t>h$ and $(t-h)$ mod $T_1=0$}
		\STATE $\eta\leftarrow\eta/2$.
		\ENDIF
		\STATE Sample a fresh data   $(\upx^{t+1},y^{t+1})\sim\mathcal{D}$.
		\STATE $\upv^{t+1}\leftarrow\upv^t-\eta\nabla l_M^t(\upv^t)$.
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}

The algorithm is summarized as Algorithm~\ref{SGD}. 
For simplicity,  at iteration $t$, denote $l^t_M(\upv)=\frac{1}{2}\left(f_{\upv}(\upx^{t+1})-y^{t+1}\right)^2$. 
The initial point $\mathbf{v}_0$ and the initial step size $\eta$ are hyperparameters of Algorithm~\ref{SGD}, and they play a crucial role in determining whether the algorithm can escape saddle points and converge to the optimal solution. 
Starting at an initial point near zero, the constant step size stage allows the algorithm to adaptively extract the important features without explicitly setting the truncation dimensions while keeping the remaining variables close to zero. The subsequent geometric decay of the step size guarantees fast convergence to the ground truth.  %\congfang{check the sentence??}

\iffalse
\subsection{Information-Theoretic Lower Bound}
The information-theoretic lower bound serves as a fundamental criterion for evaluating statistical optimality in learning algorithms. An algorithm achieves minimax optimality if its worst-case excess risk upper bound matches the corresponding information-theoretic lower bound over a given problem class. For an algorithm $\hat{\mathbf{v} } $ that estimates $\mathbf{v}_*$  from i.i.d data $\left \{ \left ( \mathbf{x}_i,y_i  \right )  \right \} _{i=1}^T$ generated by $y_i=\left \langle \mathbf{x}_i ,\mathbf{v}_i ^{\odot 2} \right \rangle +\xi $, where $\xi \sim \mathcal{N}(0,1)$ is independent of $\mathbf{x}$, the information-theoretic lower bound over the ground truth set $\mathcal{V}$ is deined as:
\begin{equation}
    \inf_{\hat{\mathbf{v} } \left ( \left \{ \left ( \mathbf{x}_i,y_i  \right )  \right \} _{i=1}^T \right ) }\sup_{\mathbf{v_{*}}\in \mathcal{V} }\mathbb{E}\left ( \mathbf{x}^{\top}\left ( \hat{\mathbf{v} } ^{\odot 2}-\mathbf{v}_*^{\odot 2}  \right )   \right ) ^2 .
\end{equation}
When the covariate distribution satisfies Assumption~\ref{ass-d} and Assumption~\ref{ass-ss}, and the true parameter set follows the constraints of Assumption~\ref{ass-ss}, the information-theoretic lower bound is shown to be $T^{-\frac{1}{\beta}}$~\citep{zhang2024optimality}.  \congfang{very big problem: 1 where includes random algorithm? 2 do not put it here. It does not depend on algorithm 3 State agnostic to algorithm the best possible prediction not say to judge whether an algorithm is optimal }
\fi 
