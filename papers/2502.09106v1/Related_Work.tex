
\noindent \textbf{Linear Regression.} 
\iffalse
Linear regression is perhaps the most canonical task in statistical learning. It has been extensively studied when the ground truth is of finite dimension, with both offline and online algorithms achieving the theoretical information-theoretic lower bound  $\widetilde{\mathcal{O}}\left(d\sigma^2/T\right)$~\citep{bach2013non,jain2018parallelizing,ge2019step}.  
%and is experiencing a renaissance to explain empirical phenomena in deep learning that plausibly defy the traditional statistical mindset.  The motivation to study linear model is substantial: (i)  linear model can approximate neural networks that are sufficiently wide and trained with a specialized scaling and random initialization, due to the celebrated neural tangent
%kernel theory \citep{}; (ii) it is relatively simple, for which one can conduct more precise analysis. 
Compared to the finite-dimensional setting, recent analyses assume that the problems are high-dimensional, with regularity conditions on the data eigenvalues and the ground truth.
In the offline setting, a class of studies has explored the implicit bias and the benign overfitting of algorithms~\citep{raskutti2014early,gunasekar2017implicit,Bartlett_2020,10.1214/21-AOS2133,tsigler2023benign}. 
%For example, the Gradient Descent with zero initialization will converge to a min-norm solution~\citep{gunasekar2017implicit} and exhibit benign overfitting behavior~\citep{Bartlett_2020}.
%Recent work by \citep{Bartlett_2020} established excess risk bounds for the min-norm solution and demonstrated the potential for benign overfitting when the tail of the eigenvalues is small and decreases slowly. 
The prevalence of double and even multiple descents has also been substantiated under various geometries of eigenvalues and source conditions~\citep{liang2020multiple,10.1214/20-AOS1990,mei2022generalization,lu2023optimal,zhang2024optimal1}.
%When the spectrum of the covariance and the ground truth parameters exhibit power-law decay $\lambda_i\asymp i^{-\alpha}$ and $\lambda_i\left(\mathbf{w}^*_i\right)^2\asymp i^{-\beta}$, kernel ridge regression (KRR) and GD with early stopping have been shown to achieve the optimal rate $\widetilde{\mathcal{O}}\left(T^{-1/\beta }\right)$.
In the more popular online setting, the key insight for achieving good generalization with SGD lies in its ability to gradually release model complexities. SGD exhibits benign overfitting behavior~\citep{dieuleveut2016nonparametric,dieuleveut2017harder,lin2017optimal,ali2020implicit,zou2021benefits, zou2021benign, wu2022last,varre2021last} when the spectrum of the covariance decays quickly.
Under the power-law decay assumptions on the spectrum of the covariance and the ground truth parameters, \citet{lin2024scaling,bordelon2024a,bahri2024explaining} investigated the curve of the excess risk of SGD in the linear model with respect to the sample size $T$ and model size $M$.
Theoretical analyses of SGD primarily focus on two types of step size decay schedules:  geometrically decay~\citep{ge2019step,wu2022last,zhang2024optimality} and constant step size with iterate averaging~\citep{dieuleveut2016nonparametric,dieuleveut2017harder,zou2021benign}. Our paper follows the more practical approach of the former. 
\fi
Linear regression, a cornerstone of statistical learning, achieves information-theoretic optimality $\widetilde{\mathcal{O}}\left(d\sigma^2/T\right)$ in finite dimensions for both offline and online settings \citep{bach2013non,jain2018parallelizing,ge2019step}. Recent advances extend analyses to high-dimensional regimes under eigenvalue regularity conditions and parameter structure \citep{raskutti2014early,gunasekar2017implicit,Bartlett_2020,10.1214/21-AOS2133,tsigler2023benign}. Offline studies characterize implicit bias, benign overfitting, and multi-descent phenomena linked to spectral geometries \citep{liang2020multiple,10.1214/20-AOS1990,mei2022generalization,lu2023optimal,zhang2024optimal1}, while online analyses reveal SGD’s phased complexity release and covariance spectrum-dependent overfitting \citep{dieuleveut2016nonparametric,dieuleveut2017harder,lin2017optimal,ali2020implicit,zou2021benefits,zou2021benign,wu2022last,varre2021last}. Recent work quantifies SGD’s risk scaling under power-law spectral decays \citep{lin2024scaling,bordelon2024a,bahri2024explaining}. 
%While theoretical guarantees exist for geometric decay \citep{ge2019step,wu2022last,zhang2024optimality} and constant step size with iterate averaging \citep{dieuleveut2016nonparametric,dieuleveut2017harder,zou2021benign} 
We follow the geometric decay schedule of the step size \citep{ge2019step,wu2022last,zhang2024optimality}  in Phase  II due to its  superiority in balancing rapid early-phase convergence and stable asymptotic refinement \citep{ge2019step}.  However, in analysis of Phase II, we further require constructing auxiliary sequences to reach the desired convergence rate, which is much more technical.
%we still needs to innovate in the analysis by constructing some control and truncated sequences to eliminate the low-probability unbounded cases. %, more difficult than the analysis of linear models
% \congfang{the last sentence is strange? should say technical difference you can say for part 2 you follow them  however, you need to do more...}
%The excess risk of SGD with geometrically decay step size is shown achieves information-theoretic lower bound when $\alpha\le \beta$ while can only achieve $\widetilde{\mathcal{O}}\left(T^{-\frac{\beta-1}{\alpha} }\right)$ when $\alpha<\beta$.


%When the spectrum of the covariance and the ground truth parameters exhibit power-law decay, SGD with geometrically decaying step sizes achieves optimal when the true parameter aligns with the covariance spectrum ($\alpha\le \beta$). However, it can only achieve sub-optimal rate $\widetilde{\mathcal{O}}\left(T^{-\frac{\beta-1}{\alpha} }\right)$ when hen the true parameter opposes with the covariance spectrum ($\alpha<\beta$).
%\congfang{add more citation cut down the words about their results. Say you follow step size schedule and tell the difference}

\iffalse

In the offline setting,  the Gradient Descent algorithm with $0$ initialization will converge to a min-norm solution. 
Represent work from \citep{} established the excess risk bound for the min-norm  solution and demonstrated the possibility of benign overfitting when the tail of eigenvalues is small and decreases slowly.   The prevalence of double and even multiple descents is also substantiated under various geometries of eigenvalues and source conditions.   When it comes to  the more popular online setting, the main insight to achieve good generalization for Stochastic Gradient Decent (SGD) is its ability to gradually release the model complexities. Recent results show that SGD achieves optimal performance when the learning problem is relatively easy where the source condition ensures a decay, whereas exhibits less efficient when the source condition grows.
SGD for linear models have been extensively studied in high-dimensional settings, demonstrating implicit regularization effects that facilitate convergence of the test risk. Under the power-law decay of the covariance spectrum and certain prior assumptions, SGD achieves a test risk that decays in a power-law of the sample size. This behavior can be interpreted as a manifestation of the neural scaling law when the model size becomes sufficiently large. 
Under these two conditions, SGD has been shown to achieve the optimal rate $\Tilde{\calO} \left ( T^{-1+\frac{1}{\beta} } \right ) $ for the simpler case ($\alpha \le  \beta $), while it remains suboptimal rate $\Tilde{\calO} \left ( T^{-1+\frac{1}{\alpha} } \right )$ for the harder case ($\alpha > \beta $). 
In contrast, our results demonstrate that with quadratic parameterization, SGD attains a superior rate $\Tilde{\calO}\left(T^{-\frac{2\beta-2}{\alpha+\beta}}\right)$ for the harder case, exhibiting a stronger implicit regularization effect compared to linear models. 
Furthermore, \citet{lin2024scaling,bordelon2024a} further investigate the relationship between test risk and both model size and sample size for SGD in linear models under these conditions, revealing a polynomial decay form that aligns with the neural scaling law. When the size of the model becomes large enough, its optimality remains consistent with previous findings.
\fi 
\vspace{0.2cm}
\noindent \textbf{Feature Learning.}  
%[I will finish it at night...]
%\congfang{say the real data may not satisfy sparse feature assumption. This assumption cannot result in polynomial rates like  neural scaling law. make sure end at Page 4 }
The feature learning ability of neural networks is the core mechanism behind their excellent generalization performance. In recent years, theoretical research has primarily focused on two directions: one is the analysis of infinitely wide networks within the mean-field framework, see e.g.~\citet{doi:10.1073/pnas.1806579115,NEURIPS2018_a1afc58c}, and the other is the study of how networks align with low-dimensional objective functions including single-index models~\citep{ba2022high,mousavi2022neural,lee2024neural} and multi-index models \citep{damian2022neural,vural2024pruning}. Although significant progress has been made in these areas, the mean-field mode lacks a clear finite sample convergence rate. Assumptions such as sparse or low-dimensional isotropic objective functions weaken the generality and fail to recover the polynomial decay of generalization error with respect to sample size and model parameters. In this paper, we follow the previous quadratic parameterization~\citep{vaskevicius2019implicit, woodworth2020kernel, haochen21shape}  while develop a generalization error analysis under an anisotropic covariance structure, yielding generalization error results similar to those predicted by the neural scaling law.