In machine learning, the scaling law describes how the model performance improves with the model and data size scaling up. From a learning theory perspective, this class of results establishes upper and lower generalization bounds for a specific learning algorithm. Here,  %the algorithm induced  dynamic process
the exact algorithm running using a specific model parameterization often offers a crucial implicit regularization effect, leading to good generalization.%, especially for high-dimensional problems.  
   To characterize the scaling law, previous theoretical studies mainly focus on linear models, whereas,  feature learning, a notable process that contributes to the remarkable empirical success of neural networks, is regretfully vacant.  This paper studies the scaling law over a linear regression with the model being quadratically parameterized. We  consider infinitely dimensional data and slope ground truth,  both signals exhibiting certain power-law decay rates. We study convergence rates for Stochastic Gradient Descent and  demonstrate the learning  rates for variables will automatically adapt to the ground truth.  As a result, in the canonical linear regression,  we provide explicit separations for generalization curves between  SGD with and without feature learning,  and the information-theoretical lower bound that is agnostic to parametrization method and the algorithm. Our analysis for decaying ground truth provides a new characterization for  
the learning dynamic of the %quadratic 
model. 

%We show the learning process consists of two phases, where in the first ``adaptation'' phase,  the algorithm adaptively selects an optimal set in which the variables scale to the group truth to a constant, and  in the "estimation" second phase,  estimates the variables 

%We study the foundational single-pass Stochastic Gradient Descent  algorithm and provide explicit separations for test performances between quadratic, linear model, and the information-theoretical lower bound. Distinguished from previous research which usually assumes a sparse ground truth over a nearly isotropic input data for the quadratic model,  we study infinitely dimensional data and ground truth, whose signals exhibit certain power-law decay rates. As a result,  our theoretical law scales polynomially,
%similar to the neural scaling law discovered empirically. Moreover, our analysis for decaying ground truth provides a new characterization for the  learning dynamic of the %quadratic  model. %which is of independent interest in online non-convex optimization.


%In machine learning, the scaling law describes how the model performance improves with the model and data size scaling up. This paper studies the scaling law for feature Learning over a linear regression with the model being quadratically parameterized.  Distinguished from previous research which usually assumes a sparse ground truth over a near isotropic input data, our work centers on infinitely dimensional data and ground truth, whose signals exhibit certain power-law decay rates. We study the model performance trained by single-pass Stochastic Gradient Descent and establish the test risk scales as \congfang{$\mathcal{O}()$, where $M$ and $N$...}   The law scales polynomially, similar to the neural scaling law discovered empirically, and outperforms that of the linear model when the input signal does not positively respond to its target.  Therefore, our considered model strengthens the importance of feature learning and would serve as a more close-to-reality testbed to research on scaling law. Accordingly, the new theoretical analysis to deal with the decaying ground truth provides new insights into understanding the dynamic of the quadratic model, which is of independent interest in online non-convex optimization.






%Such a law scales polynomially, similar to the neural scaling law discovered empirically. More surprisingly, we show this test error achieves min-max \textit{optimal}  rates across the whole regimes.  The main reason why SGD attains the optimality is the anisotropic gradient noise caused by the model, which serves as (i) screening, successfully selecting features that have the highest correlations with the target, and (ii) an efficient estimator with the smallest variance. From our results, we demonstrate the provable advantages of the quadratic parameterization over the linear one under the power-law decay setup. To our knowledge, this is the \textit{first} scaling law for online algorithms for non-linear models.