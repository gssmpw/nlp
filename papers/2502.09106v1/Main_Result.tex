% \subsection{Algorithmic Upper Bound}
The upper bound of last iterate instantaneous risk for Algorithm \ref{SGD} can be summarized by the following theorem, which provides the guarantee of global convergence for last iterate SGD with tail geometrically decaying stepsize and a sufficiently small initialization. For simplicity, we define 
%$\sigmin(N):=\min_{j\in[1:N]}(\upv_j^*)^2$ for any $N\in\bbN_+$, 
% $\calM:=(\sum_{j=1}^M\lambda_j(\upv_j^*)^4)^{1/2}$ and 
$\sigma:=(\sigma_{\xi}^2+\sum_{i=M+1}^{\infty}\lambda_i(\upv_i^*)^4)^{1/2}$.
\begin{theorem}\label{theorem-3}
    Under Assumptions \ref{ass-d} and \ref{ass-ss}, we consider a predictor trained by Algorithm \ref{SGD} with total sample size $T$ and middle phase length $h=\lceil T/\log(T)\rceil$. 
    Let $D\asymp\min\{T^{1/\max\{\beta,(\alpha+\beta)/2\}},M\}$ 
    and $\eta\asymp D^{\min\{0,(\alpha-\beta)/4\}}$. The error of output can be bounded from above by
    \begin{equation}\label{risk-main}
        % \begin{split}
        % \mathcal{R}_M(\upv^T)-\bbE[\xi^2]\lesssim&\underbrace{\frac{1}{M^{\beta-1}}}_{\mathrm{approximation}}+\underbrace{\frac{\sigma^2N}{T}+\frac{\sigma^2\eta^2T}{N^{\alpha+\beta-1}}\mathds{1}_{N<M}}_{\mathrm{variance}}
        % \\
        % &+\underbrace{\frac{N}{T}+\left(\frac{1}{N^{\beta-1}}+\frac{\eta^2T}{N^{\alpha+\beta-1}}\right)\mathds{1}_{N<M}}_{\mathrm{bias}},
        % \end{split}
        \begin{split}
        \mathcal{R}_M(\upv^T)-\bbE[\xi^2]\lesssim&\underbrace{\frac{1}{M^{\beta-1}}}_{\mathrm{approximation}}+\underbrace{\frac{\sigma^2D}{T}}_{\mathrm{variance}}+\underbrace{\frac{D}{T}+\frac{1}{D^{\beta-1}}\mathds{1}_{D<M}}_{\mathrm{bias}},
        % \asymp\frac{1}{M^{\beta-1}}+\frac{(\sigma^2+1)D}{T},
        \end{split}
    \end{equation}
    with probability at least 0.95.
    % Suppose 
    % $$
    % N=\tilde{\calO}\left(\frac{T^{1/\max\{\beta,(\alpha+\beta)/2\}}}{(\sigma^2+\calM^2)^{3/\max\{\beta,(\alpha+\beta)/2\}}}\right)<M,
    % $$ 
    % and let $\eta=\Tilde{\Theta}(\frac{N^{\min\{0,(\alpha-\beta)/4\}}}{(\sigma^2+\calM^2)^2})$. The error of output can be bounded from above by
    % \begin{equation}\label{risk-main}
    %     \begin{split}
    %     \mathcal{R}_M(\upv^T)-\bbE[\xi^2]\lesssim&\underbrace{\frac{1}{M^{\beta-1}}}_{approximation}+\underbrace{\frac{\sigma^2N}{T}+\frac{\sigma^2\eta^2T}{N^{\alpha+\beta-1}}}_{variance}+\underbrace{\frac{1}{T}+\frac{1}{N^{\beta-1}}+\left(\frac{N}{T}+\frac{\eta^2T}{N^{\alpha+\beta-1}}\right)}_{bias},
    %     \end{split}
    % \end{equation}
    % with probability at least 0.95. Otherwise, let $\eta=\Tilde{\Theta}(\frac{M^{\min\{0,(\alpha-\beta)/4\}}}{(\sigma^2+\calM^2)^2})$. The error of output can be bounded from above by
    % \begin{align}\label{risk-sub}
    %     \mathcal{R}_M(\upv^T)-\bbE[\xi^2]\lesssim&\underbrace{\frac{1}{M^{\beta-1}}}_{approximation}+\underbrace{\frac{\sigma^2M}{T}}_{variance}+\underbrace{\frac{M}{T}}_{bias},
    % \end{align}
    % with probability at least 0.95.
\end{theorem}
Our bound exhibits two key properties: (1) Dimension-free: Eq.~\eqref{risk-main} depends on the effective dimension $D$ rather than ambient dimension $M$. (2) Problem-adaptive: $D$ is governed by the spectral structure of $\diag\{\lambda_1(\upv_1^*)^2,\cdots,\lambda_M(\upv_M^*)^2\}$, which is induced by the multiplicative coupling between the data covariance matrix and optimal solution determined by the problem. 

The risk bound in Eq.~\eqref{risk-main} consists of three components: (1) approximation error term, (2) bias error term originating from $\upv^{T_1}-\upv_{1:M}^*$ at iteration $T_1=\lceil(T-h)/\log(T-h)\rceil$, and (3) variance error term stemming from the multiplicative coupling between additive noise $\xi+\sum_{i\geq M+1}\upx_i(\upv_i^*)^2$ and the diagonal matrix $\diag\{\upv_{1:M}^*\}\in\bbR^{M\times M}$. 

For sufficiently large $M$, Corollary \ref{corollary-1} establishes the convergence rate for Algorithm \ref{SGD} via Theorem \ref{theorem-3}.
\begin{corollary}\label{corollary-1}
    Under the setting of the parameters in Theorem \ref{theorem-3}, if $T^{1/\max\{\beta,(\alpha+\beta)/2\}}\asymp D<M$, we have
    \begin{align}
        \begin{cases}
            \mathcal{R}_M(\upv^T)-\bbE[\xi^2]\lesssim \frac{1}{M^{\beta-1}}+\frac{\sigma^2+1}{T^{1-1/\beta}}, & \text{ if }\beta\ge \alpha>1,
            \\
            \mathcal{R}_M(\upv^T)-\bbE[\xi^2]\lesssim \frac{1}{M^{\beta-1}}+\frac{\sigma^2+1}{T^{(2\beta-2)/(\alpha+\beta)}}, & \text{ if } \alpha>\beta >1,\notag
        \end{cases}
    \end{align}
    with probability at least 0.95.
\end{corollary}
% Corollary \ref{corollary-1} illustrates that when the model size $M$ is sufficiently large, under Assumption \ref{ass-d} and Assumption \ref{ass-ss}, the last iterate instantaneous risk for Algorithm \ref{SGD} exhibits two distinct behaviors in the cases of $\beta\geq\alpha>1$ and $\alpha\geq\beta>1$. Prior to the case analysis, we define the total computational budget as $B=MT$, indicating that Algorithm \ref{SGD} queries $M$-dimensional gradients $T$ times.
Corollary \ref{corollary-1} demonstrates that under Assumptions \ref{ass-d} and \ref{ass-ss}, when the model size $M$ is sufficiently large, the last iterate instantaneous risk of Algorithm \ref{SGD} exhibits distinct behaviors in two regimes: (I) $\beta\geq\alpha>1$ and (II) $\alpha\geq\beta>1$. 
We consider the total computational budget
 as $B=MT$, reflecting that Algorithm \ref{SGD} queries $M$-dimensional gradients $T$ times. 

\noindent \textbf{Given $B$: } If $\beta>\alpha>1$, the optimal last iterate risk is attained with parameter configurations: $M=\widetilde{\Omega}(B^{\frac{1}{1+\beta}})$ and $T=\widetilde{\Omega}(B^{\frac{\beta}{1+\beta}})$. If $\alpha\geq\beta>1$, the optimal last iterate risk is attained with parameter configurations: $M=\widetilde{\Omega}(B^{\frac{1}{1+(\alpha+\beta)/2}})$ and $T=\widetilde{\Omega}(B^{\frac{(\alpha+\beta)/2}{1+(\alpha+\beta)/2}})$.

\noindent \textbf{Given Total Sample Size $T$: } So as long as  $M\gtrsim T^{1/\max\{\beta,(\alpha+\beta)/2\}}$, Corollary \ref{corollary-1} implicates that the risk can be effectively reduced by increasing the model size $M$ as much as possible.

%Therefore, our  analysis implies prioritizing dataset scale allocation to moderately exceed model size under compute-constrained regimes. \congfang{what do you mean? I think you just need to say when you should scale the model size given $T$}


% \congfang{Separate 2 paragraphs  M smaller corollary (implication like add model size...) and M large corollary  and figure to explain }

For smaller $M$, Corollary \ref{corollary-2} provides the convergence rate for Algorithm \ref{SGD} through Theorem \ref{theorem-3}. 
\begin{corollary}\label{corollary-2}
    Under the setting of the parameters in Theorem \ref{theorem-3}, if $M\lesssim T^{1/\max\{\beta,(\alpha+\beta)/2\}}$, we have
    \begin{align}
        \mathcal{R}_M(\upv^T)-\bbE[\xi^2]\lesssim\frac{1}{M^{\beta-1}}+\frac{(\sigma^2+1)M}{T},\notag
    \end{align}
    with probability at least 0.95.
\end{corollary}
The risk bound $\calR_M(\cdot)$ in Corollary \ref{corollary-2} decreases monotonically with increasing $M$.  So  as long as  $M\lesssim T^{1/\max\{\beta,(\alpha+\beta)/2\}}$, our analysis implies to increase the model size $M$ until reaching the  computational budget.

%Under the constraint that $M\lesssim T^{1/\max\{\beta,(\alpha+\beta)/2\}}$, our theoretical analysis therefore recommends selecting the model size $M$ as large as computationally feasible to minimize the last iterate instantaneous risk.


\begin{remark}
The information-theoretic lower bound acts as a bottleneck for the best estimation issues. For any (random) algorithm $\hat{\mathbf{v}}$ based on i.i.d. data $\left \{ \left ( \mathbf{x}_i,y_i  \right )  \right \} _{i=1}^T$ from the true parameter $\mathbf{v}_* \in \mathcal{V}$, the worst-case excess risk convergence rate is limited by this bound. %It's algorithm-agnostic, showing the inherent challenge of minimizing excess risk in the worst case. 
The scaling law, however, describes the excess risk trajectory of a specific algorithm in a given context during training. Under the covariate distribution in Assumptions~\ref{ass-d} and~\ref{ass-ss}, and with the true parameter set meeting Assumption~\ref{ass-ss} regularity constraints, prior work~\citep{zhang2024optimality} established the info-theoretic lower bound as $T^{-\frac{1}{\beta}}$. Our analysis shows two distinct regimes: When $\alpha\le \beta$, SGD in linear and quadratic models hits the lower bound, proving statistical optimality. When $\alpha> \beta$, SGD in both misses the bound, yet the quadratic model has better excess risk than the linear one. This shows a capacity gap between the two model types, highlighting the importance of feature learning and model adaptation.

\iffalse
The information-theoretic lower bound provides an information bottleneck for a class of estimation problems. For any random algorithm $\hat{\mathbf{v}}$ based on i.i.d. data $\left \{ \left ( \mathbf{x}_i,y_i  \right )  \right \} _{i=1}^T$ generated by the ground true parameter $\mathbf{v}_* \in \mathcal{V}$, the worst-case convergence rate of the excess risk associated with $\hat{\mathbf{v}}$ is inherently constrained by this lower bound.  Crucially, the information-theoretic lower bound is algorithm-agnostic, reflecting the intrinsic difficulty of minimizing excess risk in the worst-case scenario. In contrast, the scaling law characterizes the trajectory of excess risk for a specific algorithm within a given problem context during the training process.


Under the covariate distribution governed by Assumptions~\ref{ass-d} and~\ref{ass-ss}, and assuming the true parameter set adheres to the regularity constraints in Assumption~\ref{ass-ss}, the information-theoretic lower bound has been established as $T^{-\frac{1}{\beta}}$ in prior work~\citep{zhang2024optimality}. Our analysis reveals two distinct regimes:
When $\alpha\le \beta$ the excess risk achieved by SGD in both linear and quadratic models attains the information-theoretic lower bound, demonstrating statistical optimality. When $\alpha> \beta$, while SGD in both models fails to achieve the lower bound, the quadratic model exhibits strictly superior excess risk compared to the linear counterpart. This dichotomy highlights a separation in model capacity between linear and quadratic parameterizations, reflecting the importance of feature learning and model adaptation.
\fi
\end{remark}

\begin{remark}
    To demonstrate the near-optimality of our upper bound, we also provide the algorithmic {\bf \emph{lower bound}} of the excess risk for Algorithm \ref{SGD} in Appendix \ref{sec-lower}. The lower bound of the excess risk for Algorithm \ref{SGD} {\bf \emph{matches}} the upper bound in Theorem \ref{theorem-3},  up to logarithmic factors. 
    % In the estimation of the upper bound, for coordinates before the truncation dimension $D$, we follow their iterative trajectories precisely, yielding an error term of $\frac{D}{T}$. For coordinates beyond $\widetilde{\calO}(D)$, due to their slow ascent rate, they remain distant from the optimal solution even after $T$ iterations, leading to an error term of $D^{-\beta+1}$. Since $D$ is optimally chosen, our derived upper bound consequently achieves optimality. \congfang{Very big problem! It is to strength that your analysis is tight, not pile up results!!! }
    %Through constructing tailored submartingales (or supermartingales), we demonstrate that for any coordinate $i\leq D$, $\upv_i^t$ remains confined within a neighborhood of the optimal solution $\upv_i^*$ with high probability after $T_1$ iterations. Consequently, for the first $D$ coordinates beyond $T_1$ iterations, Algorithm \ref{SGD}'s dynamics closely approximate those of SGD running over a linear model. Employing the diagonal recursion technique, we establish the risk bound $\widetilde{\mathcal{O}}(\frac{D}{T})$ for these coordinates, which achieves near-optimality. 
    The construction of the algorithmic lower bound is based on the construction of appropriate submartingales (or supermartingales). 
    %We show that for any coordinate $i > D$, $\upv_i^t$ is confined within a bounded region dependent on the optimal solution $\upv_i^*$ with high probability. Thus, we develop a streamlined estimation method that constrains these coordinates' contribution to the overall risk bound to $\widetilde{\mathcal{O}}(D^{-\beta+1})$.\congfang{what do you mean??? why small O in lower bound} 
    Our lower bound analysis reveals that for coordinates $j\geq\widetilde{\mathcal{O}}(D)$, the slow ascent rate inherently prevents $\upv_j^t$ from attaining close proximity to the optimal solution $\upv_j^*$ upon algorithmic termination. This phenomenon induces bias error's scaling as $\widetilde{\Omega}(D^{-\beta+1})$, matching our upper bound characterization, up to logarithmic factors. The bound also implies that the convergence rates for SGD is inherently worse than  information-theoretic lower bounds when $\alpha >\beta$. %\congfang{???}
    %\zhz{how  to add : The algorithmic lower bound separates information- scaling law..}
\end{remark}


% \begin{corollary}
%     Suppose Assumption \ref{ass-d} and Assumption \ref{ass-ss} hold, and $N=M$. Under the setting of the parameters in Theorem \ref{theorem-3}, we have \begin{align}
%             \mathcal{R}_M(\upv^T)-\bbE[\xi^2]\leq\Tilde{\calO}\left(\frac{M}{T}\right)+\calO\left(\frac{1}{M^{\beta-1}}\right),\notag
%     \end{align}
%     with probability at least 0.9.
% \end{corollary}

% \subsection{Algorithmic Lower Bound}
% To complement our upper bound, we provide the lower bound of the excess risk for Algorithm \ref{SGD}.
% \begin{theorem}\label{lower-bound}
%     Under Assumption \ref{ass-d} and Assumption \ref{ass-ss}, we consider a predictor trained by Algorithm \ref{SGD} with iteration number $T$ and middle phase length $h>\lceil(T-h)/\log(T-h)\rceil$. 
%     Let 
%     $$
%     N=\min\left\{\tilde{\calO}\left(\frac{T^{1/\max\{\beta,(\alpha+\beta)/2\}}}{(\sigma^2+\calM^2)^{3/\max\{\beta,(\alpha+\beta)/2\}}}\right),M\right\},
%     $$ 
%     and let $\eta=\Tilde{\Theta}(\frac{N^{\min\{0,(\alpha-\beta)/4\}}}{(\sigma^2+\calM^2)^2})$. Then we have
%     \begin{align}\label{last-iterate-lower-1}
%     \bbE\left[\mathcal{R}_M(\upv^{T})\right]-\bbE[\xi^2]\gtrsim\frac{1}{M^{\beta-1}}+\frac{\sigma^2N}{T}.
%     \end{align}
% %     Suppose 
% %     $$
% %     N=\tilde{\calO}\left(\frac{T^{1/\max\{\beta,(\alpha+\beta)/2\}}}{(\sigma^2+\calM^2)^{3/\max\{\beta,(\alpha+\beta)/2\}}}\right)<M,
% %     $$ 
% %     and let $\eta=\Tilde{\Theta}(\frac{N^{\min\{0,(\alpha-\beta)/4\}}}{(\sigma^2+\calM^2)^2})$. Then we have
% %     \begin{align}\label{last-iterate-lower-1}
% %     \bbE\left[\mathcal{R}_M(\upv^{T})\right]-\bbE[\xi^2]\gtrsim\frac{1}{M^{\beta-1}}+\frac{\sigma^2}{T^{1-1/(\max\{\beta,(\alpha+\beta)/2\})}}.
% % \end{align}
% % Otherwise, let $\eta=\Tilde{\Theta}(\frac{M^{\min\{0,(\alpha-\beta)/4\}}}{(\sigma^2+\calM^2)^2})$. Then we have
% %     \begin{align}\label{last-iterate-lower-2}
% %     \bbE\left[\mathcal{R}_M(\upv^{T})\right]-\bbE[\xi^2]\gtrsim\frac{1}{M^{\beta-1}}+\frac{\sigma^2M}{T}.
% % \end{align}
% \end{theorem}
% % Theorem \ref{lower-bound} provides a lower bound for last iterate SGD in quadratically-parameterized model. 
% When $M$ is sufficiently large and $\beta\geq\alpha>1$, Eq.~\eqref{last-iterate-lower-1} indicates that our excess risk upper bound is tight, 
% % with $\eta=\tilde{\Theta}(\frac{\Barsigmin(N)}{(\sigma^2+\calM^2)^2})$
% up to logarithmic factors. However, when $M$ is sufficiently large and $\alpha>\beta>1$, there exists a gap between our upper bound and lower bound. 
% % Our analysis of the lower bound requires that the iterative process of the algorithm resembles the SGD iteration process for linear models. 
% Since the probabilistic guarantee of $\upv^t$ near $\upv_{1:M}^*$ after Phase I terminates, Algorithm 1's iterations asymptotically resemble the SGD dynamics of linear regression models. Therefore, our lower bound analysis is initiated upon the termination of Phase I. Upon completion of Phase I (at $T_1$-th iteration), the squared error $\|\upv_{N+1:M}^{T-1}-\upv_{N+1:M}^*\|_2^2$ lacks a tight probabilistic lower bound. This deficiency propagates to the bias error analysis, preventing derivation of a tight bias error lower bound.
% % Additionally, when $M$ is sufficiently large and $\alpha>\beta>1$, our upper bound illustrates that SGD with quadratically-parameterized model converges to the global minimum faster than SGD with linearly-parameterized model \citep{lin2024scaling}. However, in the latter case, there is a gap between the upper and lower bounds due to the technical difficulties in obtaining an accurate bias lower bound on the last iterate of SGD.  \congfang{separation with lower bound????}