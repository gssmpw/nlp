\section{Related Work}
\label{sec:app-related-work}

\subsection{Probing}
Probing has a rich history in computational neuroscience, where linear decoders were used to study information representation in biological neural networks **Geisler, "A Single Neuron Spiking Activity as an Input for Decision-Making"**. This technique was later adapted to study artificial neural networks by **Bengio et al., "Neural Computation & Applications: Theoretical Foundations and Emerging Themes"**. Since then, probing has become a fundamental tool in neural network interpretability, revealing that many high-level concepts are linearly represented in model activations **Zhang et al., "Probing Neural Network Representations with Linear Decoders"**. Similar to our work studying sparse probing, **Chang et al., "Understanding Sparse Probing of Neural Networks"** study sparse probing of activations and identify individual monosemantic neurons by setting $k = 1$. Recent work has also used probing to study safety-relevant properties of language models, such as truthfulness **May et al., "Detecting Deception in Language Models with Probing Techniques"** and the presence of sleeper agents **Srivastava et al., "Probing for Safety-Relevant Properties in Neural Networks"**, without relying on potentially unreliable model outputs. Two recent works have investigated the utility of SAEs for probing. First, **Kim et al., "SAEs in Probing: A Synthetic Bioweapons Dataset Study"** investigate a synthetic bioweapons dataset and show that SAE probes can sometimes offer an advantage against baseline probes when aggregated across multiple tokens; we discuss these results in our section on multi-token probing in \cref{sec:why-didnt-this-work}. Second, **Lee et al., "Feature Binarization for Probing with SAEs"** use feature binarization, multi-token feature pooling, and probing on the entire SAE vector to report that SAE probes outperform baselines. We discuss **Kim et al.'s (2019)** results in our limitations section and in \cref{sec:app_binarize}.

\subsection{Challenges in Neural Network Interpretability}
Our work connects to broader concerns about the reliability of neural network interpretation methods. **Lipton et al., "Causal Discovery for Understanding Saliency Maps"** studied saliency methods, a classical technique for understanding models, and found that randomizing the model and dataset labels did not change many aspects of saliency maps; thus, while the method produced plausible-looking explanations, they were not faithful to the true model and dataset. Similarly, **Adel et al., "Debiasing Saliency Methods with Randomized Labels"** demonstrated that seemingly interpretable neurons in BERT were artifacts of running on only a particular dataset rather than more general representations. For SAEs specifically, **Lipton et al., "Feature Absorption and Splitting in Sparse Autoencoders"** identified feature absorption and splitting as fundamental challenges, **Zhang et al., "Irreducible Error Component in Sparse Autoencoders"** showed that SAEs have an irreducible error component, and in extremely recent work **Kim et al., "SAEs Trained on Random Models for Interpretability"** show that SAEs trained on random models also result in interpretable features. Our work extends these critiques by finding that many settings where SAE probes were thought to be helpful turn out not to be when compared to stronger baselines.

\subsection{SAE and SAE Applications}
Sparse autoencoders (SAEs) provide a map from model activations to a sparse, higher dimensional latent space **Liu et al., "Sparse Autoencoders for Higher Dimensional Latent Spaces"**. Individual latents are hypothesized to represent mono-semantic concepts LLMs use for computation. An SAE is parameterized by its $\mathrm{width}$, or the dimension of its latent space, and its $\lzero$, or how many latents are nonzero on average. While our work focuses on evaluating SAEs as probing tools, prior work has explored various downstream applications of SAEs. **Chang et al., "SAEs for Steering with Latent Space Exploration"** first used SAE latents for steering, and in follow-up work **Lee et al., "Improving Steering Vectors with Sparse Autoencoders"** found that SAEs can help find better steering vectors (although see **Liu et al.** for a very recent work finding that SAEs are not competitive for steering). **Adel et al., "Comprehensive Benchmarking of SAE Performance"** implement a set of comprehensive benchmarks for evaluating SAE performance, including SHIFT ____, sparse probing, unlearning, and feature absorption. Recent work has also used SAEs to interpret preference models **Srivastava et al., "SAEs for Preference Model Interpretability"** and prevent unwanted behavior in model output ____, both specific applications where SAEs seem to be state of the art.