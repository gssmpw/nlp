\section{Related Work}
\label{sec:app-related-work}

\subsection{Probing}
Probing has a rich history in computational neuroscience, where linear decoders were used to study information representation in biological neural networks ____. This technique was later adapted to study artificial neural networks by ____. Since then, probing has become a fundamental tool in neural network interpretability, revealing that many high-level concepts are linearly represented in model activations ____. Similar to our work studying sparse probing, ____ study sparse probing of activations and identify individual monosemantic neurons by setting $k = 1$. Recent work has also used probing to study safety-relevant properties of language models, such as truthfulness ____ and the presence of sleeper agents ____, without relying on potentially unreliable model outputs. Two recent works have investigated the utility of SAEs for probing. First, ____ investigate a synthetic bioweapons dataset and show that SAE probes can sometimes offer an advantage against baseline probes when aggregated across multiple tokens; we discuss these results in our section on multi-token probing in \cref{sec:why-didnt-this-work}. Second, ____ use feature binarization, multi-token feature pooling, and probing on the entire SAE vector to report that SAE probes outperform baselines. We discuss ____'s (____) results in our limitations section and in \cref{sec:app_binarize}.

\subsection{Challenges in Neural Network Interpretability}
Our work connects to broader concerns about the reliability of neural network interpretation methods. ____ studied saliency methods, a classical technique for understanding models, and found that randomizing the model and dataset labels did not change many aspects of saliency maps; thus, while the method produced plausible-looking explanations, they were not faithful to the true model and dataset. Similarly, ____ demonstrated that seemingly interpretable neurons in BERT were artifacts of running on only a particular dataset rather than more general representations. For SAEs specifically, ____ identified feature absorption and splitting as fundamental challenges, ____ showed that SAEs have an irreducible error component, and in extremely recent work ____ show that SAEs trained on random models also result in interpretable features. Our work extends these critiques by finding that many settings where SAE probes were thought to be helpful turn out not to be when compared to stronger baselines.

\subsection{SAE and SAE Applications}
Sparse autoencoders (SAEs) provide a map from model activations to a sparse, higher dimensional latent space ____. Individual latents are hypothesized to represent mono-semantic concepts LLMs use for computation. An SAE is parameterized by its $\mathrm{width}$, or the dimension of its latent space, and its $\lzero$, or how many latents are nonzero on average. While our work focuses on evaluating SAEs as probing tools, prior work has explored various downstream applications of SAEs. ____ first used SAE latents for steering, and in follow-up work ____ found that SAEs can help find better steering vectors (although see ____ for a very recent work finding that SAEs are not competitive for steering). ____ implement a set of comprehensive benchmarks for evaluating SAE performance, including SHIFT ____, sparse probing, unlearning, and feature absorption. Recent work has also used SAEs to interpret preference models ____ and prevent unwanted behavior in model output ____, both specific applications where SAEs seem to be state of the art.