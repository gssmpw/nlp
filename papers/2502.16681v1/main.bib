@inproceedings{
gurnee2024language,
title={Language Models Represent Space and Time},
author={Wes Gurnee and Max Tegmark},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=jE8xbmvFin}
}

@inproceedings{SciQ,
    title={Crowdsourcing Multiple Choice Science Questions},
    author={Johannes Welbl, Nelson F. Liu, Matt Gardner},
    year={2017},
    journal={arXiv:1707.06209v1}
}

@inproceedings{lin-etal-2022-truthfulqa,
    title = "{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods",
    author = "Lin, Stephanie  and
      Hilton, Jacob  and
      Evans, Owain",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.229/",
    doi = "10.18653/v1/2022.acl-long.229",
    pages = "3214--3252",
    abstract = "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58{\%} of questions, while human performance was 94{\%}. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web."
}

@inproceedings{ZKNR19,
    author = {Ben Zhou, Daniel Khashabi, Qiang Ning and Dan Roth},
    title = {“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding },
    booktitle = {EMNLP},
    year = {2019},
}

@inproceedings{Bisk2020,
  author = {Yonatan Bisk and Rowan Zellers and
            Ronan Le Bras and Jianfeng Gao
            and Yejin Choi},
  title = {PIQA: Reasoning about Physical Commonsense in
           Natural Language},
  booktitle = {Thirty-Fourth AAAI Conference on
               Artificial Intelligence},
  year = {2020},
}

@InProceedings{quartz,
  author = {Oyvind Tafjord and Matt Gardner and Kevin Lin and Peter Clark},
  title = {"QUARTZ: An Open-Domain Dataset of Qualitative Relationship
Questions"},

  year = {"2019"},
}
@article{hendrycks2021ethics,
  title={Aligning AI With Shared Human Values},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andrew Critch and Jerry Li and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{talmor2022commonsenseqa,
  title={CommonsenseQA 2.0: Exposing the limits of AI through gamification},
  author={Talmor, Alon and Yoran, Ori and Bras, Ronan Le and Bhagavatula, Chandra and Goldberg, Yoav and Choi, Yejin and Berant, Jonathan},
  journal={arXiv preprint arXiv:2201.05320},
  year={2022}
}

@article{
gurnee2023finding,
title={Finding Neurons in a Haystack: Case Studies with Sparse Probing},
author={Wes Gurnee and Neel Nanda and Matthew Pauly and Katherine Harvey and Dmitrii Troitskii and Dimitris Bertsimas},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=JYs1R9IMJr},
}

@inproceedings{sap-etal-2019-social,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454/",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473",
    abstract = "We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: {\textquotedblleft}Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?{\textquotedblright} A: {\textquotedblleft}Make sure no one else could hear{\textquotedblright}). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\ensuremath{>}}20{\%} gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA)."
}


@inproceedings{
wang2018glue,
title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJ4km2R5t7},
}

@misc{kaggleHumanText,
	author = {Shayan Gerami},
	title = {{A}{I} {V}s {H}uman {T}ext --- kaggle.com},
	howpublished = {\url{https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text/data}},
	year = {},
	note = {[Accessed 21-01-2025]},
}

@misc{lin2023toxicchat,
      title={ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation}, 
      author={Zi Lin and Zihan Wang and Yongqi Tong and Yangkun Wang and Yuxin Guo and Yujia Wang and Jingbo Shang},
      year={2023},
      eprint={2310.17389},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kaggleSpamText,
	author = {Team AI and Daisuke Ishii},
	title = {{S}pam {T}ext {M}essage {C}lassification --- kaggle.com},
	howpublished = {\url{https://www.kaggle.com/datasets/team-ai/spam-text-message-classification}},
	year = {},
	note = {[Accessed 21-01-2025]},
}

@misc{kaggleFakeandrealnewsdataset,
	author = {clmentbisaillon},
	title = {fake-and-real-news-dataset --- kaggle.com},
	howpublished = {\url{https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset}},
	year = {},
	note = {[Accessed 21-01-2025]},
}

@misc{huggingfaceRkotariclickbaitDatasets,
	author = {Rajesh Kotari},
	title = {rkotari/clickbait · {D}atasets at {H}ugging {F}ace --- huggingface.co},
	howpublished = {\url{https://huggingface.co/datasets/rkotari/clickbait}},
	year = {},
	note = {[Accessed 21-01-2025]},
}

@inproceedings{hateoffensive,
  title = {Automated Hate Speech Detection and the Problem of Offensive Language},
  author = {Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar}, 
  booktitle = {Proceedings of the 11th International AAAI Conference on Web and Social Media},
  series = {ICWSM '17},
  year = {2017},
  location = {Montreal, Canada},
  pages = {512-515}
  }

@misc{aihumanmade,
Author = {Raghav Gaggar and Ashish Bhagchandani and Harsh Oza},
Title = {Machine-Generated Text Detection using Deep Learning},
Year = {2023},
Eprint = {arXiv:2311.15425},
}

@InProceedings{Pang+Lee:05a,
  author =       {Bo Pang and Lillian Lee},
  title =        {Seeing stars: Exploiting class relationships for sentiment
                  categorization with respect to rating scales},
  booktitle =    {Proceedings of the ACL},
  year =         2005
}

@misc{huggingfaceAllenaibasic_arithmeticDatasets,
	author = {AllenAI},
	title = {allenai/basic\_arithmetic · {D}atasets at {H}ugging {F}ace --- huggingface.co},
	howpublished = {\url{https://huggingface.co/datasets/allenai/basic_arithmetic}},
	year = {},
	note = {[Accessed 21-01-2025]},
}

@inproceedings{DBLP:conf/aaai/RogersKDR20,
  author    = {Anna Rogers and
               Olga Kovaleva and
               Matthew Downey and
               Anna Rumshisky},
  title     = {Getting Closer to {AI} Complete Question Answering: {A} Set of Prerequisite
               Real Tasks},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2020, The Thirty-Second Innovative Applications of Artificial Intelligence
               Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
               Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
               February 7-12, 2020},
  pages     = {8722--8731},
  publisher = {{AAAI} Press},
  year      = {2020},
  url       = {https://aaai.org/ojs/index.php/AAAI/article/view/6398},
  timestamp = {Thu, 04 Jun 2020 13:18:48 +0200},
  biburl    = {https://dblp.org/rec/conf/aaai/RogersKDR20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{kaggleTextClassification,
	author = {Tanishq Dublish},
	title = {{T}ext classification documentation --- kaggle.com},
	howpublished = {\url{https://www.kaggle.com/datasets/tanishqdublish/text-classification-documentation}},
	year = {},
	note = {[Accessed 21-01-2025]},
}

@misc{kaggleMedicalText,
	author = {Falgunipatel19},
	title = {{M}edical {T}ext {D}ataset -{C}ancer {D}oc {C}lassification --- kaggle.com},
	howpublished = {\url{https://www.kaggle.com/datasets/falgunipatel19/biomedical-text-publication-classification}},
	year = {},
	note = {[Accessed 21-01-2025]},
}

@misc{kaggleMedicalText2,
	author = {Chaitanya Krishna Kasaraneni},
	title = {{M}edical {T}ext --- kaggle.com},
	howpublished = {\url{https://www.kaggle.com/datasets/chaitanyakck/medical-text}},
	year = {},
	note = {[Accessed 21-01-2025]},
}

@misc{kaggleEmotionDetection,
	author = {Pashupati Gupta},
	title = {{E}motion {D}etection from {T}ext --- kaggle.com},
	howpublished = {\url{https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text}},
	year = {},
	note = {[Accessed 21-01-2025]},
}

@misc{kaggleServiceTicket,
	author = {Adison Goh},
	title = {{I}{T} {S}ervice {T}icket {C}lassification {D}ataset --- kaggle.com},
	howpublished = {\url{https://www.kaggle.com/datasets/adisongoh/it-service-ticket-classification-dataset}},
	year = {},
	note = {[Accessed 21-01-2025]},
}

@misc{statheadStatheadYour,
	author = {Stathead},
	title = {{S}tathead: {Y}our all-access ticket to the {S}ports {R}eference database. | {S}tathead.com --- stathead.com},
	howpublished = {\url{https://stathead.com/}},
	year = {},
	note = {[Accessed 21-01-2025]},
}

@misc{neuronpediaSAEBenchComprehensive,
  title = {SAEBench: A Comprehensive Benchmark for Sparse Autoencoders},
  author = {Karvonen, Adam and Rager, Can and Lin, Johnny and Tigges, Curt and Bloom, Joseph and Chanin, David and Lau, Yeu-Tong and Farrell, Eoin and Conmy, Arthur and McDougall, Callum and Ayonrinde, Kola and Wearden, Matthew and Marks, Samuel and Nanda, Neel},
  year = {2024},
  month = {December},
  url = {https://www.neuronpedia.org/sae-bench/info},
  note = {Accessed: 2025-01-20}
}

@misc{features_as_classifiers,
  author = {Trenton Bricken and Jonathan Marcus and Siddharth Mishra-Sharma and Meg Tong and Ethan Perez and Mrinank Sharma and Kelley Rivoire and Thomas Henighan},
  editor = {Adam Jermyn},
  title = {Using Dictionary Learning Features as Classifiers},
  year = {2024},
  month = {October},
  url = {https://transformer-circuits.pub/2024/features-as-classifiers/index.html},
  note = {Transformer Circuits}
}



@article{dictionary_monosemanticity_anthropic,
   title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
   author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
   year={2023},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}

@inproceedings{xgboost,
author = {Chen, Tianqi and Guestrin, Carlos},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939785},
doi = {10.1145/2939672.2939785},
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–794},
numpages = {10},
keywords = {large-scale machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}


@misc{gemma2,
Author = {Morgane Riviere and Shreya Pathak and Pier Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and Léonard Hussenot and Thomas Mesnard and Bobak Shahriari and others},
Title = {Gemma 2: Improving Open Language Models at a Practical Size},
Year = {2024},
Eprint = {arXiv:2408.00118},
}


@misc{llama3.1-8B,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and others},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}


@misc{gemmascope,
Author = {Tom Lieberum and Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Nicolas Sonnerat and Vikrant Varma and János Kramár and Anca Dragan and Rohin Shah and Neel Nanda},
Title = {Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2},
Year = {2024},
Eprint = {arXiv:2408.05147},
}

@misc{LlamaScope,
Author = {Zhengfu He and Wentao Shu and Xuyang Ge and Lingjie Chen and Junxuan Wang and Yunhua Zhou and Frances Liu and Qipeng Guo and Xuanjing Huang and Zuxuan Wu and Yu-Gang Jiang and Xipeng Qiu},
Title = {Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders},
Year = {2024},
Eprint = {arXiv:2410.20526},
}

@article{other_sae_paper,
  title={Sparse autoencoders find highly interpretable features in language models},
  author={Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal={arXiv preprint arXiv:2309.08600},
  year={2023}
}


@misc{rajamanoharan2024jumpingaheadimprovingreconstruction,
      title={Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders}, 
      author={Senthooran Rajamanoharan and Tom Lieberum and Nicolas Sonnerat and Arthur Conmy and Vikrant Varma and János Kramár and Neel Nanda},
      year={2024},
      eprint={2407.14435},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.14435}, 
}


@misc{gao2024scalingevaluatingsparseautoencoders,
      title={Scaling and evaluating sparse autoencoders}, 
      author={Leo Gao and Tom Dupré la Tour and Henk Tillman and Gabriel Goh and Rajan Troll and Alec Radford and Ilya Sutskever and Jan Leike and Jeffrey Wu},
      year={2024},
      eprint={2406.04093},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.04093}, 
}

@misc{BatchTopK,
Author = {Bart Bussmann and Patrick Leask and Neel Nanda},
Title = {BatchTopK Sparse Autoencoders},
Year = {2024},
Eprint = {arXiv:2412.06410},
}


@misc{lesswrongLearningMultiLevel,
	author = {Bart Bussmann and Patrick Leask and Neel Nanda},
	title = {{L}earning {M}ulti-{L}evel {F}eatures with {M}atryoshka {S}{A}{E}s — {L}ess{W}rong --- lesswrong.com},
	howpublished = {\url{https://www.lesswrong.com/posts/rKM9b6B2LqwSB5ToN/learning-multi-level-features-with-matryoshka-saes}},
	year = {},
	note = {[Accessed 15-02-2025]},
}


@misc{mudide2024efficientdictionarylearningswitch,
      title={Efficient Dictionary Learning with Switch Sparse Autoencoders}, 
      author={Anish Mudide and Joshua Engels and Eric J. Michaud and Max Tegmark and Christian Schroeder de Witt},
      year={2024},
      eprint={2410.08201},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.08201}, 
}

@misc{olmo2024featuresmakedifferenceleveraging,
      title={Features that Make a Difference: Leveraging Gradients for Improved Dictionary Learning}, 
      author={Jeffrey Olmo and Jared Wilson and Max Forsey and Bryce Hepner and Thomas Vin Howe and David Wingate},
      year={2024},
      eprint={2411.10397},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.10397}, 
}


@misc{engels2024languagemodelfeatureslinear,
      title={Not All Language Model Features Are Linear}, 
      author={Joshua Engels and Eric J. Michaud and Isaac Liao and Wes Gurnee and Max Tegmark},
      year={2024},
      eprint={2405.14860},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.14860}, 
}


@misc{braun2024identifyingfunctionallyimportantfeatures,
      title={Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning}, 
      author={Dan Braun and Jordan Taylor and Nicholas Goldowsky-Dill and Lee Sharkey},
      year={2024},
      eprint={2405.12241},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.12241}, 
}

@article{engels2024decomposing,
  title={Decomposing The Dark Matter of Sparse Autoencoders},
  author={Engels, Joshua and Riggs, Logan and Tegmark, Max},
  journal={arXiv preprint arXiv:2410.14670},
  year={2024}
}

@article{li2024geometry,
  title={The geometry of concepts: Sparse autoencoder feature structure},
  author={Li, Yuxiao and Michaud, Eric J and Baek, David D and Engels, Joshua and Sun, Xiaoqing and Tegmark, Max},
  journal={arXiv preprint arXiv:2410.19750},
  year={2024}
}

@article{rajamanoharan2024improving,
  title={Improving dictionary learning with gated sparse autoencoders},
  author={Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Shah, Rohin and Nanda, Neel},
  journal={arXiv preprint arXiv:2404.16014},
  year={2024}
}

@misc{bussmann2024matryoshka,
  author       = {Bussmann, Ben and Leask, Peter and Nanda, Neel},
  title        = {Learning Multi-Level Features with Matryoshka SAEs},
  year         = {2024},
  howpublished = {\url{https://www.lesswrong.com/posts/rKM9b6B2LqwSB5ToN/learning-multi-level-features-with-matryoshka-saes}},
  note         = {Accessed: 2025-01-23}
}

@article{chanin2024absorption,
  title={A is for absorption: Studying feature splitting and absorption in sparse autoencoders},
  author={Chanin, David and Wilken-Smith, James and Dulka, Tom{\'a}{\v{s}} and Bhatnagar, Hardik and Bloom, Joseph},
  journal={arXiv preprint arXiv:2409.14507},
  year={2024}
}

@article{karvonen2408measuring,
  title={Measuring progress in dictionary learning for language model interpretability with board game models, 2024},
  author={Karvonen, Adam and Wright, Benjamin and Rager, Can and Angell, Rico and Brinkmann, Jannik and Smith, Logan and Verdun, Claudio Mayrink and Bau, David and Marks, Samuel},
  journal={URL https://arxiv. org/abs/2408.00113},
  pages={16}
}

@article{huang2024ravel,
  title={RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations},
  author={Huang, Jing and Wu, Zhengxuan and Potts, Christopher and Geva, Mor and Geiger, Atticus},
  journal={arXiv preprint arXiv:2402.17700},
  year={2024}
}

@article{marks2024sparse,
  title={Sparse feature circuits: Discovering and editing interpretable causal graphs in language models},
  author={Marks, Samuel and Rager, Can and Michaud, Eric J and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  journal={arXiv preprint arXiv:2403.19647},
  year={2024}
}

@online{smith2024sparse_autoencoders,
  title = {Interpreting Preference Models w/ Sparse Autoencoders},
  author = {Logan Riggs Smith and Jannik Brinkmann},
  year = {2024},
  month = {July},
  day = {1},
  url = {https://www.alignmentforum.org/posts/5XmxmszdjzBQzqpmz/interpreting-preference-models-w-sparse-autoencoders},
  note = {Accessed: 2025-01-23}
}

@online{bricken2024features_classifiers,
  title = {Using Dictionary Learning Features as Classifiers},
  author = {Trenton Bricken and Jonathan Marcus and Siddharth Mishra-Sharma and Meg Tong and Ethan Perez and Mrinank Sharma and Kelley Rivoire and Thomas Henighan},
  editor = {Adam Jermyn},
  year = {2024},
  month = {October},
  url = {https://transformer-circuits.pub/2024/features-as-classifiers/index.html},
  note = {Accessed: 2025-01-23}
}

@article{bereska2024mechanistic,
  title={Mechanistic Interpretability for AI Safety--A Review},
  author={Bereska, Leonard and Gavves, Efstratios},
  journal={arXiv preprint arXiv:2404.14082},
  year={2024}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}

@article{chaudhary2024evaluating,
  title={Evaluating open-source sparse autoencoders on disentangling factual knowledge in gpt-2 small},
  author={Chaudhary, Maheep and Geiger, Atticus},
  journal={arXiv preprint arXiv:2409.04478},
  year={2024}
}

@article{karvonen2024sieve,
    title={Sieve: SAEs Beat Baselines on a Real-World Task (A Code Generation Case Study)},
    author={Karvonen, Adam and Pai, Dhruv and Wang, Mason and Keigwin, Ben},
    journal={Tilde Research Blog},
    year={2024},
    month={12},
    url={https://www.tilderesearch.com/blog/sieve},
    note={Blog post}
}

@article{warstadt-etal-2019-neural,
    title = "Neural Network Acceptability Judgments",
    author = "Warstadt, Alex  and
      Singh, Amanpreet  and
      Bowman, Samuel R.",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1040/",
    doi = "10.1162/tacl_a_00290",
    pages = "625--641",
    abstract = "This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.`s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions."
}

@book{chomsky2002syntactic,
  title={Syntactic structures},
  author={Chomsky, Noam},
  year={2002},
  publisher={Mouton de Gruyter}
}

@article{space_time,
  title={Language models represent space and time},
  author={Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.02207},
  year={2023}
}

@online{macdiarmid2024sleeperagentprobes,
author = {Monte MacDiarmid and Timothy Maxwell and Nicholas Schiefer and Jesse Mu and Jared Kaplan and David Duvenaud and Sam Bowman and Alex Tamkin and Ethan Perez and Mrinank Sharma and Carson Denison and Evan Hubinger},
title = {Simple probes can catch sleeper agents},
date = {2024-04-23},
year = {2024},
url = {https://www.anthropic.com/news/probes-catch-sleeper-agents},
}

@article{bolukbasi2021interpretability,
  title={An interpretability illusion for bert},
  author={Bolukbasi, Tolga and Pearce, Adam and Yuan, Ann and Coenen, Andy and Reif, Emily and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2104.07143},
  year={2021}
}

@article{adebayo2018sanity,
  title={Sanity checks for saliency maps},
  author={Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}


@article{mur2009revealing,
  title={Revealing representational content with pattern-information fMRI—an introductory guide},
  author={Mur, Marieke and Bandettini, Peter A and Kriegeskorte, Nikolaus},
  journal={Social cognitive and affective neuroscience},
  volume={4},
  number={1},
  pages={101--109},
  year={2009},
  publisher={Oxford University Press}
}

@misc{gpt4o,
	author = {OpenAI},
	title = {{H}ello {G}{P}{T}-4o},
	howpublished = {\url{https://openai.com/index/hello-gpt-4o/}},
	year = {},
	note = {[Accessed 27-01-2025]},
}
@misc{anthropicIntroducingClaude,
	author = {Anthropic},
	title = {{I}ntroducing {C}laude 3.5 {S}onnet --- anthropic.com},
	howpublished = {\url{https://www.anthropic.com/news/claude-3-5-sonnet}},
	year = {2024},
	note = {[Accessed 27-01-2025]},
}

@misc{neuronpedia,
    title = {Neuronpedia: Interactive Reference and Tooling for Analyzing Neural Networks},
    year = {2023},
    note = {Software available from neuronpedia.org},
    url = {https://www.neuronpedia.org},
    author = {Lin, Johnny}
}

@misc{bills2023language,
         title={Language models can explain neurons in language models},
         author={
            Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William
         },
         year={2023},
         howpublished = {\url{https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html}}
      }

@misc{o1,
	author = {OpenAI},
	title = {Introducing OpenAI o1},
	howpublished = {\url{https://openai.com/o1/}},
	year = {2024},
	note = {[Accessed 29-01-2025]},
}

@article{pile,
  title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{monotonic_numeric_representations,
  title={Monotonic Representation of Numeric Properties in Language Models},
  author={Heinzerling, Benjamin and Inui, Kentaro},
  journal={arXiv preprint arXiv:2403.10381},
  year={2024}
}

@article{othello_neel_linear,
  title={Emergent linear representations in world models of self-supervised sequence models},
  author={Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2309.00941},
  year={2023}
}

@article{linear_representation_hypothesis,
  title={The linear representation hypothesis and the geometry of large language models},
  author={Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  journal={arXiv preprint arXiv:2311.03658},
  year={2023}
}

@article{origins_of_linear_representations,
  title={On the Origins of Linear Representations in Large Language Models},
  author={Jiang, Yibo and Rajendran, Goutham and Ravikumar, Pradeep and Aragam, Bryon and Veitch, Victor},
  journal={arXiv preprint arXiv:2403.03867},
  year={2024}
}



@article{linear_truth_sam,
  title={The geometry of truth: Emergent linear structure in large language model representations of true/false datasets},
  author={Marks, Samuel and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.06824},
  year={2023}
}

@article{original_othello_paper,
  title={Emergent world representations: Exploring a sequence model trained on a synthetic task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2210.13382},
  year={2022}
}


@misc{heap2025sparseautoencodersinterpretrandomly,
      title={Sparse Autoencoders Can Interpret Randomly Initialized Transformers}, 
      author={Thomas Heap and Tim Lawson and Lucy Farnik and Laurence Aitchison},
      year={2025},
      eprint={2501.17727},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.17727}, 
}

    @article{templeton2024scaling,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
    }

@article{chalnev2024improving,
  title={Improving steering vectors by targeting sparse autoencoder features},
  author={Chalnev, Sviatoslav and Siu, Matthew and Conmy, Arthur},
  journal={arXiv preprint arXiv:2411.02193},
  year={2024}
}

@misc{wu2025axbenchsteeringllmssimple,
      title={AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders}, 
      author={Zhengxuan Wu and Aryaman Arora and Atticus Geiger and Zheng Wang and Jing Huang and Dan Jurafsky and Christopher D. Manning and Christopher Potts},
      year={2025},
      eprint={2501.17148},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.17148}, 
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@misc{sonnet,
  author = "{Anthropic}",
  title = "{Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku}",
  year = {2024},
  url = "{https://www.anthropic.com/news/3-5-models-and-computer-use}",
  note = "Accessed: 2025-01-30"
}

@misc{FarrelSAEBio,
Author = {Eoin Farrell and Yeu-Tong Lau and Arthur Conmy},
Title = {Applying sparse autoencoders to unlearn knowledge in language models},
Year = {2024},
Eprint = {arXiv:2410.19278},
}

@article{Elazar2021,
  title = {Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals},
  volume = {9},
  ISSN = {2307-387X},
  url = {http://dx.doi.org/10.1162/tacl_a_00359},
  DOI = {10.1162/tacl_a_00359},
  journal = {Transactions of the Association for Computational Linguistics},
  publisher = {MIT Press},
  author = {Elazar,  Yanai and Ravfogel,  Shauli and Jacovi,  Alon and Goldberg,  Yoav},
  year = {2021},
  month = mar,
  pages = {160–175}
}

@article{yun2021transformer,
  title={Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors},
  author={Yun, Zeyu and Chen, Yubei and Olshausen, Bruno A and LeCun, Yann},
  journal={arXiv preprint arXiv:2103.15949},
  year={2021}
}

@article{arora2018linear,
  title={Linear algebraic structure of word senses, with applications to polysemy},
  author={Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={483--495},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@misc{sharkey2025openproblemsmechanisticinterpretability,
      title={Open Problems in Mechanistic Interpretability}, 
      author={Lee Sharkey and Bilal Chughtai and Joshua Batson and Jack Lindsey and Jeff Wu and Lucius Bushnaq and Nicholas Goldowsky-Dill and Stefan Heimersheim and Alejandro Ortega and Joseph Bloom and Stella Biderman and Adria Garriga-Alonso and Arthur Conmy and Neel Nanda and Jessica Rumbelow and Martin Wattenberg and Nandi Schoots and Joseph Miller and Eric J. Michaud and Stephen Casper and Max Tegmark and William Saunders and David Bau and Eric Todd and Atticus Geiger and Mor Geva and Jesse Hoogland and Daniel Murfet and Tom McGrath},
      year={2025},
      eprint={2501.16496},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.16496}, 
}

@misc{transformercircuitsCircuitsUpdates,
	author = {Tom Conerly and Adly Templeton and Trenton Bricken and Jonathan Marcus and Tom Henighan},
	title = {{C}ircuits {U}pdates - {A}pril 2024 --- transformer-circuits.pub},
	howpublished = {\url{https://transformer-circuits.pub/2024/april-update/index.html#training-saes}},
	year = {2024},
	note = {[Accessed 15-02-2025]},
}

@article{karvonen2024measuring,
  title={Measuring progress in dictionary learning for language model interpretability with board game models},
  author={Karvonen, Adam and Wright, Benjamin and Rager, Can and Angell, Rico and Brinkmann, Jannik and Smith, Logan and Verdun, Claudio Mayrink and Bau, David and Marks, Samuel},
  journal={arXiv preprint arXiv:2408.00113},
  year={2024}
}

@misc{probingGallifant,
Author = {Jack Gallifant and Shan Chen and Kuleen Sasse and Hugo Aerts and Thomas Hartvigsen and Danielle S. Bitterman},
Title = {Sparse Autoencoder Features for Classifications and Transferability},
Year = {2025},
Eprint = {arXiv:2502.11367},
}