\section{Related Work}
\label{sec:app-related-work}

\subsection{Probing}
Probing has a rich history in computational neuroscience, where linear decoders were used to study information representation in biological neural networks \cite{mur2009revealing}. This technique was later adapted to study artificial neural networks by \citet{alain2016understanding}. Since then, probing has become a fundamental tool in neural network interpretability, revealing that many high-level concepts are linearly represented in model activations \cite{space_time, monotonic_numeric_representations, othello_neel_linear}. Similar to our work studying sparse probing, \citet{gurnee2023finding} study sparse probing of activations and identify individual monosemantic neurons by setting $k = 1$. Recent work has also used probing to study safety-relevant properties of language models, such as truthfulness \cite{linear_truth_sam} and the presence of sleeper agents \cite{macdiarmid2024sleeperagentprobes}, without relying on potentially unreliable model outputs. Two recent works have investigated the utility of SAEs for probing. First, \citet{bricken2024features_classifiers} investigate a synthetic bioweapons dataset and show that SAE probes can sometimes offer an advantage against baseline probes when aggregated across multiple tokens; we discuss these results in our section on multi-token probing in \cref{sec:why-didnt-this-work}. Second, \citet{probingGallifant} use feature binarization, multi-token feature pooling, and probing on the entire SAE vector to report that SAE probes outperform baselines. We discuss \citeauthor{probingGallifant}'s (\citeyear{probingGallifant}) results in our limitations section and in \cref{sec:app_binarize}.

\subsection{Challenges in Neural Network Interpretability}
Our work connects to broader concerns about the reliability of neural network interpretation methods. \citet{adebayo2018sanity} studied saliency methods, a classical technique for understanding models, and found that randomizing the model and dataset labels did not change many aspects of saliency maps; thus, while the method produced plausible-looking explanations, they were not faithful to the true model and dataset. Similarly, \cite{bolukbasi2021interpretability} demonstrated that seemingly interpretable neurons in BERT were artifacts of running on only a particular dataset rather than more general representations. For SAEs specifically, \cite{chanin2024absorption} identified feature absorption and splitting as fundamental challenges, \citet{gao2024scalingevaluatingsparseautoencoders} showed that SAEs have an irreducible error component, and in extremely recent work \citet{heap2025sparseautoencodersinterpretrandomly} show that SAEs trained on random models also result in interpretable features. Our work extends these critiques by finding that many settings where SAE probes were thought to be helpful turn out not to be when compared to stronger baselines.

\subsection{SAE and SAE Applications}
Sparse autoencoders (SAEs) provide a map from model activations to a sparse, higher dimensional latent space \cite{dictionary_monosemanticity_anthropic, other_sae_paper}. Individual latents are hypothesized to represent mono-semantic concepts LLMs use for computation. An SAE is parameterized by its $\mathrm{width}$, or the dimension of its latent space, and its $\lzero$, or how many latents are nonzero on average. While our work focuses on evaluating SAEs as probing tools, prior work has explored various downstream applications of SAEs. \citet{templeton2024scaling} first used SAE latents for steering, and in follow-up work \citet{chalnev2024improving} found that SAEs can help find better steering vectors (although see \cite{wu2025axbenchsteeringllmssimple} for a very recent work finding that SAEs are not competitive for steering). \citet{neuronpediaSAEBenchComprehensive} implement a set of comprehensive benchmarks for evaluating SAE performance, including SHIFT \cite{marks2024sparse}, sparse probing, unlearning, and feature absorption. Recent work has also used SAEs to interpret preference models \cite{smith2024sparse_autoencoders} and prevent unwanted behavior in model output \cite{karvonen2024sieve}, both specific applications where SAEs seem to be state of the art.