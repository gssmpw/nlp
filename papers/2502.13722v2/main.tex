\documentclass[11pt,a4paper,english]{article}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{babel}
\usepackage{blindtext}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath,amssymb}
\usepackage{wrapfig}
\usepackage{stfloats}
\usepackage{graphicx}
\usepackage{placeins}  
\usepackage{float}    
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{listings}
\usepackage{multicol}
\usepackage{multirow} 
\usepackage{pgffor}
\usepackage{cleveref}
\usepackage{geometry}
%\usepackage{ulem}

\newcommand{\e}{\mathbb{E}}
\newcommand{\lk}{\left[ }
\newcommand{\rk}{\right] }
\newcommand{\lc}{\left(}
\newcommand{\rc}{\right)}
\newcommand{\Eb}{\mathbb{E}}
\newcommand{\Cb}{\mathbb{C}}
\newcommand{\Rb}{\mathbb{R}}
\newcommand{\Zb}{\mathbb{Z}}
\newcommand{\Pb}{\mathbb{P}}
\newcommand{\Qb}{\mathbb{Q}}
\newcommand{\Nb}{\mathbb{N}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\mc}{\mathcal{m}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\id}{\mathbf{1}}
\newcommand{\veps}{\mathbf{\epsilon}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\zz}{\mathbf{z}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\Ab}{\mathbf{A}}
\newcommand{\FF}{\mathbf{F}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Lc}{\mathcal{L}}

\newcommand{\myparagraph}[1]{\paragraph{#1}\mbox{}\\}

\NewDocumentCommand{\codeword}{v}{%
\texttt{\textcolor{blue}{#1}}%
}

\title{Deep Learning for VWAP Execution in Crypto Markets: \ Beyond the Volume Curve}

\author{%
    Rémi Genet \\
    \small DRM, Université Paris Dauphine - PSL \\
    \small Aplo \\
    \small remi.genet@dauphine.psl.eu \\
}


\begin{document}

\maketitle

\begin{abstract}
Volume-Weighted Average Price (VWAP) is arguably the most prevalent benchmark for trade execution as it provides an unbiased standard for comparing performance across market participants. However, achieving VWAP is inherently challenging due to its dependence on two dynamic factors—volumes and prices. Traditional approaches typically focus on forecasting the market’s volume curve, an assumption that may hold true under steady conditions but becomes suboptimal in more volatile environments or markets such as cryptocurrency where prediction error margins are higher. In this study, I propose a deep learning framework that directly optimizes the VWAP execution objective by bypassing the intermediate step of volume curve prediction. Leveraging automatic differentiation and custom loss functions, my method calibrates order allocation to minimize VWAP slippage, thereby fully addressing the complexities of the execution problem. My results demonstrate that this direct optimization approach consistently achieves lower VWAP slippage compared to conventional methods, even when utilizing a naive linear model presented in \cite{genet2024tln}.They validate the observation that strategies optimized for VWAP performance tend to diverge from accurate volume curve predictions and thus underscore the advantage of directly modeling the execution objective. This research contributes a more efficient and robust framework for VWAP execution in volatile markets, illustrating the potential of deep learning in complex financial systems where direct objective optimization is crucial. Although my empirical analysis focuses on cryptocurrency markets, the underlying principles of the framework are readily applicable to other asset classes such as equities.
\end{abstract}


\newpage

\section{Introduction}\label{section1}
In a rapidly evolving financial landscape, Volume-Weighted Average Price (VWAP) strategies have become a cornerstone for executing high-volume trades, as they aim to minimize market impact and reduce execution costs. VWAP is widely regarded for its fair and neutral calibration, making it an industry standard for comparing performance across market participants \cite{TotalCostOfTransactions}. With the rise of systematic trading, execution strategies have received increasing attention; more specifically, significant institutional trades are now executed using algorithms \cite{Mackenzie}. Despite this, academic research has predominantly focused on Implementation Shortfall (IS) \cite{perold1988implementation} strategies, while VWAP, despite its extensive industrial use and reputation as a robust benchmark (particularly for large orders) \cite{Madhavan2002}, has not been as thoroughly explored. The objective of this study is to propose an alternative VWAP execution approach that moves beyond methods relying solely on forecasting the market volume curve. Indeed, while early approaches in the literature were able to incorporate the price–volume relationship under simplified modeling assumptions, more recent works tend to focus exclusively on the volume curve. This shift arises because more realistic and higher-performing volume models render the simultaneous representation of the volume–volatility (and hence price–volume) relationship considerably more complex. The purpose of this paper is to introduce a method that easily overcomes this limitation, thereby more accurately capturing the full VWAP decision problem. To achieve this, I leverage deep learning as a generalized function calibrator. This approach allows the direct representation of the execution objective in a single step—bypassing the conventional two-step process of first predicting the volume curve and then allocating orders. Traditional econometric and machine learning tools typically rely on this two-step resolution, which can be suboptimal in the presence of prediction errors. The primary challenge in VWAP execution is to optimally allocate trading volumes over the execution horizon—a problem for which historical data does not offer a direct solution. Existing literature often simplifies this challenge by assuming that matching the market's volume curve is ideal. However, this assumption overlooks the fact that perfect volume predictions are unattainable, especially in the volatile cryptocurrency market. Recognizing and incorporating these inevitable prediction errors into the allocation strategy is essential for achieving a price closer to the market VWAP. Moreover, while conventional models can be calibrated to forecast the volume curve from historical data, directly calibrating an execution strategy is not straightforward because the optimal allocation is unknown in advance. Deep learning offers a significant advantage here. Unlike frameworks that minimize the error between predicted and observed data points, deep learning enables the implementation and optimization of any loss function—thanks to automatic differentiation. The central idea of this paper is to build a model that ingests market data, produces an allocation curve for the order, and is trained to minimize the absolute or quadratic error between the achieved execution price and the market VWAP. Finally, although some studies propose dynamic approaches that update predictions throughout an order’s lifetime, I opt for a static framework for two reasons. First, I aim to demonstrate that even a relatively simple network—such as the naive linear model I use—can enhance performance when directly representing the trading strategy. Second, incorporating real-time dynamics introduces additional technological complexities that are beyond the scope of this study. My goal is to propose a simple yet effective method for VWAP execution that fully captures the decision problem and leverages deep learning's flexibility to directly optimize the trader’s true objective.

\section{Related work}

One of the earliest works on VWAP strategies is by \cite{Konishi}, who proposed an optimal slicing strategy for VWAP execution. In scenarios where volume and volatility are uncorrelated, the optimal execution curve aligns with the relative market volume curve; deviations in the correlated case were also quantified. Building on this, \cite{Culoch2007} extended Konishi’s model by incorporating a constrained trading rate and an additional drift term—reflecting situations where traders or brokers, armed with sensitive information, may attempt to “beat” the VWAP. Their study also derived stylized facts regarding the expected relative volume (normalized by trade count), observing an S-shaped pattern throughout the trading day for equities, with higher-turnover stocks exhibiting less variability. These patterns echo the well-known U-shaped trading activity in equity markets. Later, \cite{Culoch2012} advanced this work by developing a continuously dynamic model, demonstrating that an optimal VWAP trading strategy is closely linked to intraday volume estimation. Volumes have also been used as covariates to explain other market variables such as price or volatility \cite{Easley1987, Foster, Tauchen1983, Karpoff1987}, typically using lower-frequency data rather than intraday figures. Additionally, studies such as \cite{Gourieroux} have focused on measuring overall market trading activity through volume metrics.

\medskip

In 2008, \cite{LeFol2006} proposed a method for estimating intraday volumes by decomposing them into two components—an approach refined in \cite{LeFol2012} by separating volumes into a market evolution component and a stock-specific pattern. The dynamic component was modeled using ARMA and SETAR techniques, resulting in significantly improved accuracy over static volume curve approaches. However, while earlier works like \cite{Konishi} and \cite{Culoch2007} attempted to capture the interrelation between price and volume within relatively simple frameworks, \cite{LeFol2006} and \cite{LeFol2012} focused exclusively on the volume component, as incorporating a realistic volume–price relationship renders overall modeling significantly more complex. An alternative execution method is presented in \cite{Humphery}, where Dynamic VWAP (DVWAP) is introduced in contrast to the conventional Historical VWAP (HVWAP). It is argued that incoming news during execution can affect volumes in both directions—information that HVWAP ignores. Their DVWAP framework, built upon earlier methodologies (e.g., those of Bialkowski, Darolles, and Le Fol), not only leverages these improvements but further enhances VWAP performance. Other approaches include stochastic methods proposed by \cite{bouchard} and \cite{frei}. As noted by Frei and Westray, these models derive an optimal trading rate that depends solely on volume curves, neglecting price dynamics due to the assumption that the price process behaves as an uncorrelated Brownian motion. In \cite{Tianhui}, the focus shifts partially to the strategic aspects of VWAP execution at high frequencies, addressing the dilemma faced by brokers between using aggressive versus passive orders. Finally, \cite{Gueant} introduced two novel contributions. First, they integrated both temporary and permanent market impact components—an important consideration for large institutional orders aiming to minimize market impact. Second, they proposed a method for pricing guaranteed VWAP services using a CARA utility function for the broker and indifference pricing. Unlike other approaches that solely strive for benchmark proximity, their framework emphasizes achieving optimal execution while managing risk. A common theme among these approaches is the predominant focus on modeling market volumes, often under the assumption that prices and volumes are independent—a simplification that does not fully capture market reality. Until recently, the latest advancements in machine learning—particularly deep learning—had not been fully leveraged for VWAP execution. Recent studies, however, have begun to explore these novel methods. For instance, \cite{li2022hierarchical} proposed the Macro-Meta-Micro Trader (M3T) architecture, which combines deep learning with hierarchical reinforcement learning to capture market patterns and execute orders across multiple temporal scales. Although this approach achieved an average cost saving of 1.16 basis points relative to an optimal baseline, its complexity may hinder practical implementation. Similarly, \cite{kim2023adaptive} developed a dual-level reinforcement learning strategy using Proximal Policy Optimization (PPO) to track daily cumulative VWAP. Their method integrates a Transformer model to capture the overall U-shaped volume pattern with an LSTM model for finer order distribution. Despite improved accuracy over previous reinforcement learning models, it remains heavily reliant on volume prediction. In \cite{papanicolaou2023optimal}, large stock order execution is simulated using LSTMs within the Almgren and Chriss framework. By leveraging cross-sectional data from multiple stocks to capture inter-stock dependencies, their approach consistently outperforms TWAP and VWAP-based strategies on S\&P100 data. However, the focus remains on minimizing transaction costs rather than directly optimizing VWAP execution. While recent studies underscore the potential of machine learning techniques for VWAP execution, many of these methods continue to depend on traditional volume curve prediction rather than directly targeting the VWAP execution objective.

\section{Defining the VWAP Problem}

\medskip

The \emph{Volume Weighted Average Price (VWAP)} is a trading benchmark that reflects the average price at which a security is traded throughout the day, weighted by volume. It provides insights into both the trend and the value of a security.
\begin{itemize}
  %\item \textbf{VWAP} stands for \emph{Volume Weighted Average Price}.
  \item Its basic formulation is:
  \begin{equation}
    \text{VWAP} = \frac{\text{Total Traded Value}}{\text{Total Traded Volume}}.
  \end{equation}
  \item More explicitly, VWAP is computed as:
  \begin{equation}
    \text{VWAP} = \frac{\sum_{i} \left(p_i \times v_i\right)}{\sum_{i} v_i},
  \end{equation}
where \( p_i \) denotes the price of the \(i\)th trade and \( v_i \) the corresponding volume.
\end{itemize}

\subsection*{Defining the Execution Problem}
The objective is to execute trades so that the average execution price aligns as closely as possible with the market VWAP over a given period \(T\). The total order volume is denoted by \(v\) and the total market volume over the same period by \(V\).

The average execution price is defined as:
\begin{equation}
    P = \frac{\sum_{j=1}^{n}{ p_j \times v_j}}{v}, \quad \text{with } v = \sum_{j=1}^{n}{v_j},
\end{equation}
assuming that \(n\) transactions are executed during period \(T\).

Similarly, the market VWAP is defined as:
\begin{equation}
    \text{VWAP} = \frac{\sum_{i=1}^{N}{ p_i \times V_i}}{V}, \quad \text{with } V = \sum_{i=1}^{N}{V_i},
\end{equation}
where \(N\) represents the total number of market transactions (including the order's own trades).

\subsection*{Addressing Slippage}
Absolute slippage is defined as:
\begin{equation}
    S = \left| P - \text{VWAP} \right|.
\end{equation}
Minimizing \(S\) is challenging because the transaction space is non-standard—trades occur as discrete events at irregular intervals rather than in uniform time or volume increments. To simplify the analysis, the overall period is divided into \(T\) smaller intervals. Within each interval, the following definitions apply:
\begin{equation}
    \text{VWAP}_{T} = \frac{\sum_{t=1}^{T}{ \text{VWAP}_t \times V_t}}{V}, \quad P_{T} = \frac{\sum_{t=1}^{T}{ P_t \times v_t}}{v},
\end{equation}
where:
\begin{itemize}
  \item \(V_t\) is the market volume in interval \(t\),
  \item \(v_t\) is the executed volume in interval \(t\),
  \item \(\text{VWAP}_t\) and \(P_t\) denote the market VWAP and the average execution price in interval \(t\), respectively.
\end{itemize}
This segmentation permits a more detailed comparison between \(P_T\) and \(\text{VWAP}_T\).

\subsection*{Refining the Problem Definition}
For further analysis, normalized volume proportions are introduced:
\[
Q_t = \frac{V_t}{V} \quad \text{and} \quad q_t = \frac{v_t}{v},
\]
so that:
\[
\sum_{t=1}^{T} Q_t = \sum_{t=1}^{T} q_t = 1.
\]
Thus, the execution price and market VWAP can be rewritten as:
\[
P_T = \sum_{t=1}^{T} P_t \, q_t \quad \text{and} \quad \text{VWAP}_T = \sum_{t=1}^{T} \text{VWAP}_t \, Q_t.
\]
Accordingly, the slippage becomes:
\begin{equation} \label{eq:static_slippage1}
    S_T = \left| P_T - \text{VWAP}_T \right| = \left| \sum_{t=1}^{T} \left( P_t \, q_t - \text{VWAP}_t \, Q_t \right) \right|.
\end{equation}
A common strategy to decompose the difference is to add and subtract a common term inside the summation. In this case, \(\text{VWAP}_t \, q_t\) is added and subtracted for each interval:
\begin{equation}
P_t \, q_t - \text{VWAP}_t \, Q_t = \underbrace{\left(P_t \, q_t - \text{VWAP}_t \, q_t\right)}_{=(P_t-\text{VWAP}_t)q_t} + \underbrace{\left(\text{VWAP}_t \, q_t - \text{VWAP}_t \, Q_t\right)}_{=\text{VWAP}_t\,(q_t-Q_t)}.
\end{equation}
Substituting this into \eqref{eq:static_slippage1} yields:
\begin{equation}
    S_T = \left| \sum_{t=1}^{T} \left[(P_t-\text{VWAP}_t)q_t + \text{VWAP}_t\,(q_t-Q_t)\right] \right|.
\end{equation}
Since the absolute value of a sum is generally not equal to the sum of the absolute values, the triangle inequality is invoked:
\[
\left|\sum_{t=1}^{T} a_t\right| \le \sum_{t=1}^{T} \left|a_t\right|.
\]
Thus,
\begin{equation} \label{eq:static_slippage-bound}
    S_T \le \sum_{t=1}^{T} \left| (P_t-\text{VWAP}_t)q_t + \text{VWAP}_t\,(q_t-Q_t) \right|.
\end{equation}
Furthermore, applying the triangle inequality to each individual term results in:
\begin{equation}
    \begin{aligned}
    &\left| (P_t-\text{VWAP}_t)q_t + \text{VWAP}_t\,(q_t-Q_t) \right| \\
    &\qquad \leq \left| (P_t-\text{VWAP}_t)q_t \right| + \left| \text{VWAP}_t\,(q_t-Q_t) \right|.
    \end{aligned}
\end{equation}
Summing over \(t\) gives:
\begin{equation}
    S_T \le \sum_{t=1}^{T} \left| (P_t-\text{VWAP}_t)q_t \right| + \sum_{t=1}^{T} \left| \text{VWAP}_t\,(q_t-Q_t) \right|.
\end{equation}
In this decomposition:
\begin{itemize}
    \item \(\left| (P_t-\text{VWAP}_t)q_t \right|\) represents the deviation in price weighted by the executed volume proportion.
    \item \(\left| \text{VWAP}_t\,(q_t-Q_t) \right|\) captures the impact of discrepancies between the executed volume allocation and the market’s volume distribution.
\end{itemize}

\subsection*{Methodology --- Delineating the Problems}
In the pursuit of minimizing slippage, two distinct challenges are identified.

\subsubsection*{Problem 1: Price Deviation Minimization}
This aspect focuses on minimizing:
\[
\sum_{t=1}^{T} \left| (P_t-\text{VWAP}_t)q_t \right|.
\]
Key considerations include:
\begin{itemize}
    \item Execution quality within each interval is paramount; the goal is to achieve prices as close as possible to the market VWAP.
    \item With finer time intervals, price deviations are expected to be smaller, and deviations across bins may partially offset one another.
    \item This challenge is primarily related to market microstructure and the quality of executions.
\end{itemize}

\subsubsection*{Problem 2: Volume Allocation Optimization}
This challenge is characterized by:
\begin{equation} \label{eq:static_volumeOptimization}
    \sum_{t=1}^{T} \left| \text{VWAP}_t\,(q_t-Q_t) \right|.
\end{equation}
Key considerations include:
\begin{itemize}
    \item Although \(\text{VWAP}_t\) cannot be controlled, the executed volume allocation \(q_t\) can be adjusted.
    \item Traditional methods focus on accurately predicting \(Q_t\), the market volume profile; however, given the inherent uncertainty in market dynamics, this may not be optimal.
    \item The strategy is to design a \(q_t\) allocation that accounts for the noisy nature of the \(Q_t\) process, adapting to market conditions and prediction errors.
    \item For instance, during periods of low volatility, it may be advisable to execute less than the market proportion \(Q_t\) to mitigate the risk of sudden volatility spikes.
    \item The aim is to develop a robust volume allocation strategy that minimizes slippage even when predictions of \(Q_t\) are imperfect.
\end{itemize}

\subsubsection*{Choosing the Appropriate Focus}
While the primary aim is to minimize absolute slippage, it is also essential to consider strategies that not only meet the benchmark but also potentially outperform it.

\subsubsection*{Price Difference Focus}
\begin{itemize}
    \item Since the weights \(q_t\) and \(Q_t\) are non-negative, achieving the most favorable prices in each interval is crucial.
    \item Opportunities for value addition may arise, particularly through the mitigation of trading costs such as the spread.
\end{itemize}

\subsubsection*{Volume Allocation Focus}
\begin{itemize}
    \item Given the challenges in precisely forecasting \(\text{VWAP}_t\), emphasis is placed on risk reduction through effective volume allocation.
    \item The strategy aims to minimize the term \(\left| \text{VWAP}_t\,(q_t-Q_t) \right|\) by aligning the executed volume with the market’s volume profile.
    \item This involves leveraging insights into future market volumes and understanding their interaction with market volatility.
    \item The focus is on developing the optimal \(q_t\) strategy for volume allocation over larger time intervals, thereby reducing the risk associated with VWAP execution.
\end{itemize}

\medskip

In summary, by rigorously decomposing the slippage \(S_T\) and carefully applying the triangle inequality, the problem is separated into two distinct components: execution quality (price differences) and volume allocation discrepancies. This detailed derivation provides a solid foundation for subsequent analysis and strategy development.


\section{A fixed optimal allocation curve approach}\label{section3}
To demonstrate the validity of allocation strategies that are not based solely on following the volume curve, an analysis was conducted in which an optimal allocation curve was calibrated using only historical data, without incorporating current market information. In this experiment, the allocation curve is represented by a simple vector of weights that determines the fraction of the total order executed in each time bin. Without any additional market inputs, the expected volume curve is a uniform distribution over the execution period—precisely as conventional wisdom would predict—so any deviation from this flat pattern in the optimal allocation would challenge that notion.

\subsection{Methodology}

Historical data from Binance perpetual futures contracts for Bitcoin (BTC), Ethereum (ETH), and Cardano (ADA) covering the period from contract inception until July 1, 2024, were employed. The data, recorded at an hourly frequency, were divided into training and testing sets in an 80/20 split, with the temporal order preserved to ensure a realistic evaluation. The VWAP execution problem was formulated as the optimization of a vector of allocation weights, \(q_t\) (for \(t = 1, \dots, T\) with \(T=8\) bins), by minimizing one of three loss functions. The first is the Quadratic VWAP Loss, defined as
\begin{equation} \label{eq:static_quad_loss}
L_Q = \mathbb{E}\!\left[\left(\frac{\text{VWAP}_{\text{achieved}}}{\text{VWAP}_{\text{market}}} - 1\right)^2\right],
\end{equation}
the second is the Absolute VWAP Loss,
\begin{equation} \label{eq:static_abs_loss}
L_A = \mathbb{E}\!\left[\left|\frac{\text{VWAP}_{\text{achieved}}}{\text{VWAP}_{\text{market}}} - 1\right|\right],
\end{equation}
and the third is the Volume Curve Loss,
\begin{equation} \label{eq:static_vol_curve_loss}
L_V = \mathbb{E}\!\left[\sum_{t=1}^{T} \left(\frac{v_t}{\sum_{s=1}^{T} v_s} - \frac{V_t}{\sum_{s=1}^{T} V_s}\right)^2\right].
\end{equation}
The achieved and market VWAP are defined respectively as
\begin{align}
\text{VWAP}_{\text{achieved}} &= \frac{\sum_{t=1}^{T} v_t\, p_t}{\sum_{t=1}^{T} v_t}, \label{eq:static_achieved_vwap} \\
\text{VWAP}_{\text{market}} &= \frac{\sum_{t=1}^{T} V_t\, p_t}{\sum_{t=1}^{T} V_t}. \label{eq:static_market_vwap}
\end{align}
Here, \(v_t\) represents the allocated volume in bin \(t\), \(V_t\) is the market volume in bin \(t\), and \(p_t\) is the VWAP price in bin \(t\). To solve the optimization problem, three different methods were employed. Sequential Least Squares Programming (SLSQP) was used as a local optimization technique with 1000 restarts from random initial guesses to mitigate the risk of converging to local minima. Basin-Hopping, a global optimization algorithm that combines local searches with random perturbations, was also employed and performed 10 times. In addition, Differential Evolution, an evolutionary algorithm known for its robustness in finding global optima, was executed once. Performance metrics for each asset, loss function, and optimization method are summarized in Table~\ref{tab:static_fix_curve_vwap_results}, while Figure~\ref{fig:static_fixed_optimal_allocation} illustrates the optimal allocation curves derived from these optimizations.

\input{tables/2_fix_curve_table}

The analysis reveals several important findings. Allocation curves obtained by minimizing the absolute and quadratic VWAP losses clearly deviate from a uniform allocation, while the optimization based on the volume curve loss yields a nearly flat allocation. This flat allocation is expected since, in the absence of market information, the average volume in each time bin is equal. However, although the flat allocation perfectly tracks the average volume curve (as evidenced by an \(R^2\) near zero), it performs worse in terms of VWAP execution compared to the non-uniform allocations derived from the VWAP-based losses. Furthermore, among the assets studied, Bitcoin consistently exhibits the lowest VWAP losses, reflecting its relative stability, whereas Cardano—being more volatile—shows higher losses. In terms of optimization methods, Differential Evolution consistently produced competitive results across assets and loss functions, while SLSQP, despite numerous restarts, often converged to suboptimal solutions due to the complexity of the optimization landscape. Basin-Hopping displayed variable performance that was generally less consistent.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/optimal_allocation_curves.jpg}
    \caption{Optimal Allocation based on optimization methods and asset}
    \label{fig:static_fixed_optimal_allocation}
\end{figure}

\subsection{Discussion}

This experiment is particularly revealing because it involves optimizing a vector of allocation weights using only historical data without current market information. Consequently, an optimization based solely on the volume curve loss produces a flat, uniform allocation, which is expected when no additional information beyond the historical average is available. In contrast, the allocation curves optimized using the absolute and quadratic VWAP losses diverge significantly from uniformity, exhibiting a pronounced end-loading behavior in some cases. This divergence indicates that even in the absence of direct market signals, an allocation strategy that minimizes VWAP slippage is inherently non-uniform. This finding challenges the traditional approach of following the expected volume curve and strongly supports the use of direct VWAP loss optimization in trading execution strategies. Although the empirically derived allocation curves deviate markedly from the conventional volume curve, this outcome is not entirely surprising when considering simplified frameworks that account for price--volume interactions. For instance, Konishi \cite{Konishi} shows that the optimal fraction \(x^*(t)\) allocated to bin \(t\) can be approximated by
\begin{equation}\label{eq:static_konishi_approx}
x^*(t) \;\approx\; \frac{\mathbb{E}\bigl[\sigma(t,V)^2 \, X(t)\bigr]}{\mathbb{E}\bigl[\sigma(t,V)^2\bigr]},
\end{equation}
or equivalently, as rewritten by McCulloch and Kazakov \cite{Culoch2007}:
\begin{equation}\label{eq:static_cullocheq}
x_t 
= \frac{\mathbb{E}[X_t \sigma_t^2]}{\mathbb{E}[\sigma_t^2]} 
= \mathbb{E}[X_t] 
+ \frac{\mathrm{Cov}[X_t, \sigma_t^2]}{\mathbb{E}[\sigma_t^2]}.
\end{equation}
Thus, a sufficiently large positive covariance term can shift the solution away from the market's average volume curve, underscoring the role of volatility in shaping optimal allocations. These results highlight the importance of the price--volume relationship and expose the limitations of approaches based solely on replicating the historical volume curve. Although more complex models can be developed to jointly account for both price and volume dynamics, the present analysis demonstrates that even a relatively simple (and naive) linear model can yield meaningful improvements in VWAP execution by directly optimizing the VWAP objective.

\section{A Deep-Learning Model for VWAP Allocation}
\subsection{Framework}

Having demonstrated that the traditional volume curve approach may not be optimal for allocating volumes over time, deep learning is employed as a powerful means to represent and optimize the VWAP allocation problem. Deep learning affords the flexibility to directly optimize the specific objective—minimizing the deviation between the achieved execution price and the market VWAP—by leveraging automatic differentiation. In this framework, the model accepts sequential inputs comprising multiple features over a designated historical lookback period and produces an allocation curve for future time intervals. The key innovation is the use of a custom loss function (either absolute or quadratic VWAP loss) that captures the true execution error, thereby enabling the optimization of the strategy in a single step rather than relying on intermediate proxies such as the volume curve. A critical requirement is that the model’s output constitutes a valid allocation strategy; that is, the allocation weights must be non-negative and sum to one. Although several approaches (e.g., clipping negative values or taking absolute values before normalization) were considered, these methods risk introducing uneven optimization. Instead, the softmax function is adopted, which naturally transforms any real-valued vector into a smooth probability distribution over the execution horizon. This ensures that even if the Temporal Linear Network (TLN) produces negative values, the final allocation vector remains both non-negative and normalized, thereby satisfying the constraints without sacrificing optimization smoothness.

\subsection{Internal Model}

In selecting an internal model for the VWAP allocation framework, various architectures traditionally employed for sequential prediction tasks were considered. Recent studies in volume prediction for cryptocurrency markets have explored models such as Temporal Kolmogorov-Arnold Networks (TKAN) \cite{genet2024tkan}, Signature-Weighted Kolmogorov-Arnold Networks (SigKAN) \cite{inzirillo2024sigkan}, Temporal Kolmogorov-Arnold Transformers (TKAT) \cite{genet2024tkat}, Kolmogorov-Arnold Mixture of Experts (KAMoE) \cite{inzirillo2024kamoe} and Recurrent Neural Networks with Signature-Based Gating Mechanisms (SigGate) \cite{genet2025siggate}. Although these approaches offer innovative solutions for capturing long-term dependencies and complex interactions, they also introduce additional complexity that is not necessary when the primary goal is to demonstrate the importance of optimizing the correct objective. Accordingly, the Temporal Linear Network (TLN) \cite{genet2024tln} was chosen as the default internal model. The TLN is conceptually simple and highly interpretable—essentially a linear model with structured transformations—yet it is integrated within a deep learning framework that enables the minimization of arbitrary loss functions via automatic differentiation. Importantly, even a naive linear model performs effectively within the proposed approach, emphasizing that improvements arise from directly optimizing the VWAP execution objective rather than from architectural complexity.
Let the input tensor be
$$
X \in \mathbb{R}^{B \times S \times F},
$$
where \(B\) is the batch size, \(S\) is the length of the input sequence, and \(F\) is the number of input features. The TLN consists of \(L\) layers, each implementing a cascade of linear operations. Each operation within a layer is a linear mapping with learnable weights, and the layers are designed to progressively adjust both the sequence length and feature dimension. This structured design leverages parameter sharing and constraints, resulting in a model that is both computationally efficient and stable.

Each layer \(\ell\) (with \(1 \leq \ell \leq L\)) transforms its input \(X^{(\ell-1)}\) into an output \(X^{(\ell)}\) through two primary stages: a \textbf{structured linear transformation} and a \textbf{convolutional filtering} along the time dimension.

\subsubsection*{1. Structured Linear Transformation}

This stage is composed of three consecutive linear operations.

\paragraph{Temporal Scaling:}  
Each time index is scaled by a learnable vector \(K^{(\ell)} \in \mathbb{R}^{S_\ell}\), where \(S_\ell\) denotes the current sequence length at layer \(\ell\). For each batch index \(b\), time index \(s\), and feature index \(f\), the computation is as follows:
\begin{equation}
    \widetilde{X}^{(\ell)}_{b,s,f} = K^{(\ell)}_s \, X^{(\ell-1)}_{b,s,f}.
\end{equation}

\paragraph{Feature Transformation:}  
At each time step, the features are mapped from \(\mathbb{R}^{F_\ell}\) to an intermediate space \(\mathbb{R}^{F^\prime_\ell}\) using a learnable weight matrix \(W^{(\ell)} \in \mathbb{R}^{F_\ell \times F^\prime_\ell}\) and bias \(b^{(\ell)} \in \mathbb{R}^{F^\prime_\ell}\):
\begin{equation}
    Y^{(\ell)}_{b,s,\cdot} = \widetilde{X}^{(\ell)}_{b,s,\cdot}\, W^{(\ell)} + b^{(\ell)}.
\end{equation}
An additional element-wise scaling is then applied using a learnable vector \(F^{(\ell)} \in \mathbb{R}^{F^\prime_\ell}\):
\begin{equation}
    Z^{(\ell)}_{b,s,f^\prime} = F^{(\ell)}_{f^\prime} \cdot Y^{(\ell)}_{b,s,f^\prime}.
\end{equation}

\paragraph{Temporal Transformation:}  
Finally, the sequence is remapped to a new length \(S^\prime_\ell\) by applying a linear transformation with learnable parameters \(T^{(\ell)} \in \mathbb{R}^{S_\ell \times S^\prime_\ell}\) and bias \(c^{(\ell)} \in \mathbb{R}^{S^\prime_\ell}\):
\begin{equation}
    \widehat{Z}^{(\ell)}_{b,s^\prime} = \sum_{s=1}^{S_\ell} T^{(\ell)}_{s,s^\prime}\, Z^{(\ell)}_{b,s,f^\prime} + c^{(\ell)}_{s^\prime}.
\end{equation}
In summary, the structured transformation is represented as
\begin{equation}
    \Phi^{(\ell)}\bigl(X^{(\ell-1)}\bigr) = \mathcal{T}^{(\ell)}\!\left( \left( X^{(\ell-1)} \odot K^{(\ell)} \right)W^{(\ell)} + b^{(\ell)} \odot F^{(\ell)} \right),
\end{equation}
where \(\mathcal{T}^{(\ell)}(\cdot)\) denotes the temporal mapping and \(\odot\) represents element-wise multiplication.

\subsubsection*{2. Convolutional Filtering}

After the structured transformation, each feature channel is convolved along the time dimension. Let \(C^{(\ell)} \in \mathbb{R}^{K^{(\ell)}_{\text{conv}} \times F^\prime_\ell}\) denote the convolution kernel (with kernel size \(K^{(\ell)}_{\text{conv}}\)) and \(d^{(\ell)} \in \mathbb{R}^{F^\prime_\ell}\) the corresponding bias. The convolution is computed as
\begin{equation}
    X^{(\ell)}_{b,t,f^\prime} = \sum_{k=0}^{K^{(\ell)}_{\text{conv}}-1} C^{(\ell)}_{k,f^\prime} \, \Phi^{(\ell)}\bigl(X^{(\ell-1)}\bigr)_{b,t+k,f^\prime} + d^{(\ell)}_{f^\prime}.
\end{equation}
Thus, the overall operation in layer \(\ell\) is given by
\begin{equation}
    X^{(\ell)} = \operatorname{Conv}\!\Bigl( \Phi^{(\ell)}\bigl(X^{(\ell-1)}\bigr) \Bigr).
\end{equation}

\subsection{The Role of Softmax in Enforcing Allocation Constraints}

A key innovation in the framework is the use of the softmax function to enforce allocation constraints. In this context, the final output of the internal model must be a valid allocation vector—that is, all elements must be non-negative and the vector must sum to one. While straightforward normalization (dividing by the sum of the outputs) or clipping negative values might appear sufficient, these approaches can lead to uneven or suboptimal optimization. In contrast, the softmax function
\begin{equation}
    \sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}},
\end{equation}
naturally transforms any real-valued input vector \(z\) into a smooth probability distribution. This not only satisfies the constraints but also ensures a differentiable transformation that facilitates gradient-based optimization. Even if the TLN produces negative values, the exponential function within softmax guarantees a smooth and balanced allocation across the execution horizon.

\subsection{Comparison with Standard Linear Regression and Rationale}

A conventional linear regression would flatten the input tensor \(X \in \mathbb{R}^{B \times S \times F}\) into \(X_{\text{flat}} \in \mathbb{R}^{B \times (S \cdot F)}\) and compute 
\begin{equation}
    Y = X_{\text{flat}}\, W_{\text{reg}} + b_{\text{reg}},
\end{equation}
resulting in a very large number of parameters. In contrast, the TLN architecture leverages structured, layer-by-layer transformations to progressively adjust the sequence and feature dimensions, yielding a much more parsimonious model that is less prone to unstable weight estimates. Furthermore, by embedding the TLN within a deep learning framework, it becomes possible to optimize non-traditional loss functions—such as the VWAP-specific losses—using automatic differentiation, an advantage that standard linear regression does not offer.

\subsection{Summary of the Optimization Process}
The overall optimization process proceeds in three primary steps. First, the TLN processes the input features over a specified lookback period to generate a raw output vector \(v_{[t, t+h]} = \text{TLN}(x_t)\) for future time steps. Next, the softmax function is applied to these raw outputs to convert them into a valid allocation vector:
\begin{equation}
    q_t = \frac{e^{v_t}}{\sum_{s=1}^{T} e^{v_s}}, \quad \text{for } t = 1, 2, \dots, T,
\end{equation}
ensuring that the allocations are non-negative and sum to one. Finally, the model parameters \(\theta\) are optimized by minimizing the quadratic difference between the achieved VWAP and the market VWAP, as defined in Section~\ref{section3} (see Eqs.~\eqref{eq:static_quad_loss}--\eqref{eq:static_market_vwap}). In summary, this framework establishes a fair comparison platform by leveraging deep learning to directly optimize the VWAP execution objective. Rather than increasing model complexity, the approach employs a straightforward, linear model (the TLN) enhanced by the softmax transformation to enforce smooth and constrained allocation, demonstrating that aligning the optimization objective precisely with the trading goal can lead to improved execution performance.

\section{Results}
In this section, a comprehensive comparison of various VWAP execution strategies is presented, including traditional methods, the proposed deep-learning approach, and dynamic strategies. These methods are evaluated across different cryptocurrencies to assess their performance under varying market conditions.

\subsection{Benchmarks and Strategies}

Several benchmark strategies are considered. As a baseline, a naive flat allocation strategy that uniformly distributes the order volume across all time intervals is included; this simple method serves as a reference for assessing more sophisticated approaches. In addition, fixed optimal allocation curves are evaluated—static allocations optimized using three different loss functions (absolute VWAP loss, quadratic VWAP loss, and volume curve loss) as described in \ref{section3}. For this optimization, the differential evolution algorithm is employed due to its demonstrated superior and more stable performance. The proposed deep-learning approach, referred to as the \emph{StaticVWAP Model}, employs a neural network to generate allocation curves based on historical market data. For a fair comparison, all models receive the same input features, including volumes, the hour of the day, the day of the week, and returns computed on the VWAP price of each bin. In contrast, the \emph{Static Linear Regression} model serves as a traditional benchmark that is calibrated to predict the volume. Since linear regression does not inherently produce outputs that satisfy the allocation constraints (i.e., non-negativity and summing to one), its predictions are post-processed by first clipping negative values to zero and then normalizing the resulting vector. In cases where all values are zero (or become zero after clipping), an equiponderated allocation is returned. Dynamic VWAP strategies based on the framework introduced by \cite{LeFol2012} are also implemented. In these dynamic strategies, the volume executed at each time step is determined by the current market volume prediction and the remaining order size, following the equation
\begin{equation}
    v_t = \frac{\hat{V}_t}{\sum_{i=t}^T \hat{V}_i} \cdot \left(1 - \sum_{i=0}^{t-1} v_i\right),
\end{equation}
where \(v_t\) denotes the volume executed at time \(t\), \(\hat{V}_t\) is the predicted market volume at time \(t\), \(T\) is the end of the execution horizon, and \(\sum_{i=0}^{t-1} v_i\) represents the cumulative executed volume up to time \(t-1\). Both a dynamic linear regression approach and a dynamic version of the StaticVWAP model, which updates predictions at each time step, are considered.


\subsection{Evaluation Metrics}

Three key metrics are used to assess performance. The \emph{Absolute VWAP Loss} measures the absolute difference between the achieved VWAP and the market VWAP, while the \emph{Quadratic VWAP Loss} measures the squared difference, thereby penalizing larger deviations more heavily. In addition, the \emph{\(R^2\) Score for the Volume Curve} is computed, indicating how accurately the strategy predicts the actual market volume profile.

\subsection{Experimental Setup}

The strategies are evaluated using hourly data from five major cryptocurrencies (BTC, ETH, BNB, ADA, and XRP) traded on Binance perpetual contracts covering the period from January 1, 2020, to July 1, 2024. This dataset provides sufficient historical data across varying market conditions to evaluate the models' performance in both high and low volatility regimes. The data is partitioned chronologically, with the last 20\% reserved as the test set to ensure a true out-of-sample evaluation. A validation set, comprising 20\% of the remaining data, is randomly selected from the first 80\% of the period to guide model selection and prevent overfitting to any specific market regime.

For input features, several categories are incorporated: (1) Volume data, which is transformed to address non-stationarity by dividing each volume value by a trailing two-week moving average, calculated with appropriate shifting to avoid look-ahead bias; (2) Temporal indicators including hour of day and day of week as categorical features to capture seasonality patterns; and (3) Price information in the form of returns calculated on the VWAP price of each bin, with a value of 0 assigned when no volume is traded.

The preparation of target variables differs based on the model type. For the neural network approaches, the normalization to volume curve is implemented directly within the loss function, with volumes divided by the sum over the lookback period for numerical stability. For traditional linear regression models, the volume curve is explicitly normalized by dividing each volume by the sum of volumes in the lookahead period, as these models cannot incorporate this normalization within their loss functions.

In these experiments, a lookback period of 120 time steps is used to predict allocations for 12 periods ahead. Additional experiments with modified parameters—one with a 6-step prediction horizon and a shorter lookback period, and another with a 48-step prediction horizon—are provided in the Appendix. The results from these experiments confirm that the framework is effective across different timeframes and that the underlying concepts generalize well.

\subsection{Training Details}

Standard training settings were adopted. Specifically, the Adam optimizer was employed, and the maximum number of epochs was set to 1000. To mitigate overfitting and ensure stable convergence, two key callbacks were incorporated: an early stopping mechanism, which halts training if no improvement in validation loss is observed for 10 consecutive epochs, and a learning rate reducer, which divides the current learning rate by 4 after 5 epochs without improvement (starting from an initial rate of 0.001). The training set was shuffled, and a 20\% validation split was used to monitor these metrics during training.

An additional critical aspect was the mitigation of sensitivity to weight initialization. Each training run was repeated 30 times with different random initializations to compute both the average performance and its standard deviation, thereby providing a robust and statistically sound evaluation of model effectiveness.

\subsection{Empirical Results}

Table~\ref{tab:static_vwap_results} summarizes the performance of various VWAP execution strategies for a 12-step-ahead allocation with a 120-step lookback window. The table reports the mean and standard deviation of the Absolute VWAP Loss (scaled by \(10^{-2}\)), Quadratic VWAP Loss (scaled by \(10^{-4}\)), the \(R^2\) score for the volume curve, and the training time (in seconds) for each strategy, across five assets (BTC, ETH, BNB, ADA, and XRP).

Several key patterns emerge from these results. First, all proposed methods consistently outperform the naive flat allocation approach across all assets, confirming that more sophisticated allocation strategies yield tangible benefits for VWAP execution. 

\input{tables/5_results_table}

When comparing the methods based on their ability to predict volume curves, we observe that Static Linear Regression is more effective than the StaticVWAP model when the latter is optimized for absolute or quadratic VWAP loss. However, the Dynamic Linear Regression approach consistently achieves the highest \(R^2\) values across all assets, indicating superior volume curve prediction capability. This improved prediction translates into better VWAP execution compared to Static Linear Regression.

\medskip

Interestingly, despite Dynamic Linear Regression's advantage in volume prediction, the Fixed Volume Curve approach demonstrates superior VWAP execution performance. This finding underscores a critical insight: accurate volume curve prediction, while beneficial, is not the primary determinant of optimal VWAP execution.

\medskip

Most significantly, the proposed StaticVWAP deep learning model consistently achieves the best absolute or quadratic VWAP loss values (depending on its optimization target) across all assets. When trained to minimize absolute VWAP loss, it delivers the lowest absolute slippage; similarly, when optimized for quadratic VWAP loss, it produces the lowest quadratic slippage metrics.

\medskip

Regarding training times, the deep-learning models (StaticVWAP) generally require between 15 and 18 seconds per run, which, although longer than the near-instantaneous training times of linear regression models, represents an acceptable trade-off given the significant performance gains. The dynamic approaches show mixed results when applied to the StaticVWAP model, often degrading performance rather than improving it.

\medskip

Overall, these results validate the central thesis of this study: directly optimizing the VWAP execution objective—rather than focusing solely on volume curve prediction—is essential for achieving superior execution performance in volatile markets such as cryptocurrencies. The StaticVWAP deep learning framework provides a flexible and effective approach that consistently outperforms conventional methods, even when utilizing a relatively simple model architecture.

\subsection{Training Times and Comparison Across Prediction Horizons}

Tables~\ref{tab:static_vwap_results_6} and~\ref{tab:static_vwap_results_48} provide additional insight into the practical applicability of each method under different prediction horizons. Several consistent patterns emerge across these timeframes that reinforce the findings from our main experiments.

\medskip

First, regarding training efficiency, the StaticVWAP models consistently maintain reasonable computational demands across all horizons—typically under 20 seconds—making them practical for real-world applications despite being more complex than linear regression models. Notably, while the fixed volume curve approach converges rapidly for shorter horizons, its training time increases dramatically with longer prediction horizons, eventually requiring several minutes per run for 48-step predictions.

\medskip

When examining performance across different timeframes, the relative rankings of the models remain remarkably consistent. For both shorter (6-step) and longer (48-step) prediction horizons, the StaticVWAP models optimized directly for VWAP loss consistently outperform both naive approaches and linear regression variants. The fixed volume curve approach remains competitive, particularly when optimized with quadratic loss, but generally does not reach the performance levels of the StaticVWAP model.

\medskip

Dynamic approaches show consistent patterns across different horizons. Dynamic linear regression demonstrates increasingly strong volume curve prediction capability (higher \(R^2\) values) as the prediction horizon extends, yet this improvement in volume prediction does not translate to proportional gains in VWAP execution performance. This pattern strongly supports our central thesis that accurate volume prediction, while informative, is not the primary driver of optimal VWAP execution. In fact, the performance gap between methods directly optimized for VWAP execution and those focused on volume prediction tends to persist or even widen with longer horizons.

\medskip

The stability of these findings across different prediction horizons and assets underscores the robustness of our approach. Whether executing shorter or longer-term VWAP strategies, directly optimizing the execution objective consistently yields superior results compared to approaches that rely primarily on volume curve prediction.

\subsection{Limitations and Future Directions}

Although the proposed approach yields promising improvements in VWAP execution, several limitations warrant discussion. One key assumption in the current framework is that market impact is negligible. This assumption is common in the VWAP literature, as employing a VWAP benchmark over a long period is believed to mitigate market impact. However, it is recognized that large order sizes may affect execution quality. Studies such as Gueant et al. have shown that both temporary and permanent market impact components can be incorporated into the optimization process, suggesting that future work could integrate such models to more accurately capture real-world trading dynamics. Another limitation is that the present model operates in a static framework without dynamic updating in real time. Although static models have the advantage of simplicity and reduced technological complexity, dynamic adjustments could potentially enhance execution performance in highly volatile environments. Future research may explore hybrid approaches that combine the benefits of static optimization with dynamic market adaptation. Finally, while the analysis in this study focuses on cryptocurrency markets, the underlying principles are readily extendable to other asset classes such as equities. Incorporating more sophisticated market impact models and dynamic updating mechanisms will be valuable in developing a comprehensive framework for VWAP execution across diverse trading environments.


\subsection{Understanding the Predicted Curves}

An analysis of the predicted allocation curves under different calibration losses is presented to elucidate how the proposed model differentiates itself from competing approaches. Figures~\ref{fig:static_prediction_absolute} and~\ref{fig:static_prediction_quadratic} illustrate that the predictions generated using absolute and quadratic VWAP losses tend to follow an average allocation pattern similar to that observed in Section~2, while still adapting to current market conditions. Notably, the model trained with quadratic loss produces a more compact prediction region, suggesting that it sacrifices flexibility in favor of an end-loaded execution pattern. In contrast, the model trained with volume curve loss results in a nearly flat allocation, with distinct seasonal variations clearly visible. These visual differences provide important insight into how the choice of loss function shapes the resulting allocation strategy, ultimately contributing to the performance differences observed in the quantitative analysis.

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/vwap_execution_allocation_absolute_target.jpg}
    \caption{Prediction obtained with model calibrated using Absolute Deviation Loss}
    \label{fig:static_prediction_absolute}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/vwap_execution_allocation_quadratic_target.jpg}
    \caption{Prediction obtained with model calibrated using Quadratic Deviation Loss}
    \label{fig:static_prediction_quadratic}
\end{figure}
In contrast, Figure~\ref{fig:static_prediction_volume} shows the allocation curve from a model trained with volume curve loss, which is noticeably flatter and exhibits distinct seasonal waves. These differences underscore that directly optimizing the VWAP objective (via absolute or quadratic loss) yields a strategy that deviates from simply mimicking the historical volume curve.

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/vwap_execution_allocation_volume_curve_target.jpg}
    \caption{Prediction obtained with model calibrated using Volume Curve Loss}
    \label{fig:static_prediction_volume}
\end{figure}

\subsection{Understanding When It Makes a Difference}
To further assess the effectiveness of the approach, an analysis of the temporal evolution of slippage (i.e., the difference between the achieved VWAP and the market VWAP) relative to the price is provided. Figure~\ref{fig:static_slippage_full} shows the slippage across the full out-of-sample period for the naive approach, a dynamic VWAP strategy, and the proposed static approach when executing a 48-hour VWAP in hourly bins. Notably, the proposed method generally exhibits lower slippage during periods of high volatility, and in some instances, the slippage even reverses sign relative to standard approaches. To illustrate this effect in greater detail, Figure~\ref{fig:static_slippage_subset} provides a zoomed-in view over a 2000-hour subperiod. The reduced magnitude and frequency of extreme slippage events in the proposed approach are evident in this more focused view. Finally, Figures~\ref{fig:static_slippage_diff_full} and~\ref{fig:static_slippage_diff_subset} present the differences in absolute slippage between the proposed method (or the dynamic linear method) and the naive approach. In these plots, negative values indicate lower absolute slippage (and thus improved performance) relative to the naive strategy, while positive values indicate higher slippage. The prevalence of negative values during volatile periods provides strong visual evidence that the end-loaded allocation strategy is effective in mitigating execution slippage.


\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/vwap_analysis_full_sample_slippage.jpg}
    \caption{Slippage between approaches on the full out-of-sample set}
    \label{fig:static_slippage_full}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/vwap_analysis_2000_sample_slippage.jpg}
    \caption{Slippage between approaches on a subsample of the out-of-sample set}
    \label{fig:static_slippage_subset}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/vwap_analysis_full_sample_comparison.jpg}
    \caption{Difference in absolute slippage versus naive approach on the full out-of-sample set. Negative values indicate improved performance over the naive approach.}
    \label{fig:static_slippage_diff_full}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/vwap_analysis_2000_sample_comparison.jpg}
    \caption{Difference in absolute slippage versus naive approach on the subsample set. Negative values indicate improved performance over the naive approach.}
    \label{fig:static_slippage_diff_subset}
\end{figure}

\section{Conclusion}

In conclusion, the results of this study demonstrate that directly optimizing the VWAP execution objective through a deep learning framework yields substantial performance improvements over traditional volume curve-based approaches. More broadly, this example illustrates that in many cases where multi-step methods or proxy variables have traditionally been employed, deep learning offers a powerful alternative by enabling direct optimization of the true objective. The expressivity of deep learning allows for straightforward modifications to incorporate any function of market impact, thereby providing a versatile platform for capturing complex trading dynamics.

\medskip

Although a simple, naive linear model (the Temporal Linear Network) was employed, its success is attributable to the fundamentally different approach of targeting the VWAP execution objective directly. This shift in methodology not only enhances execution performance but also underscores the potential of deep learning to transform similar problems in other domains. I believe that this research represents a significant advancement in the design of VWAP execution strategies, and I anticipate that further enhancements—such as dynamic updating mechanisms and the integration of sophisticated market impact models—will extend the applicability of this approach to a broader range of asset classes and trading environments.

\section*{Code Availability}
The source code used for all experiments and analyses in this paper is available at \url{https://github.com/remigenet/DeepLearningVWAP}.


\bibliographystyle{abbrv}
\bibliography{bib}

\addtocontents{toc}{\setcounter{tocdepth}{1}}
\newpage
\appendix
\input{appendix/main}

\end{document}
