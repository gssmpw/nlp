\section{Related works}
\paragraph{Auto-regressive pretraining and fine tuning.} Auto-regressive pretraining is a paradigm in which a generative model is trained on sequences to predict the next token given a sequence prefix~\citep{sutskever2014sequence}. This is typically formulated as a classification task over a given vocabulary built from a \textit{tokenizer}. 
Finetuning consists of pursuing the auto-regressive pretraining on a specific \textit{finetuning dataset}, orders of magnitude smaller than the pretraining dataset. 
This finetuning dataset typically encompasses data from a single domain, exhibiting a significant distribution shift compared to the base dataset. 
Transferring the model knowledge to the new distributions can lead to significant capability loss on the previous dataset, a phenomenon known as \textit{forgetting} \cite{luo2023empirical,kalajdzievski2024scaling}.

In the context of \textit{continual pretraining}, it has been observed by~\citet{ibrahim2024simple} that mixing a small percentage of pretraining data mitigates forgetting \citep[see][for an analytical framework to explore how task similarity can affect knowledge transfer]{hiratani2024disentangling}.




Alternative approaches in the context of parameter-efficient finetuning (PEFT) have tried to address these challenges by introducing lightweight modules, such as \textit{adapters}, that are updated with knowledge about the new task \cite{houlsby2019parameter, he2022unified}. 
In this context, Low-Rank Adaptation methods, or LoRA \cite{hu2021lora, zhang2024when}, inject trainable low-rank matrices into the model layers, by adding them to the untouched original model weights, thus reducing the parameter overhead needed to train for new downstream tasks \citep[see][for a discussion on the different roles of LoRA decomposition matrices]{zhu2024asymmetry}.
\citet{zhang2024when} question the use of PEFT methods for fine-tuning, reporting that it usually underperforms in terms of fine-tuning loss compared to full-parameter tuning. 
This is why in this work, we focus on full parameter tuning, and defer the study of PEFT on forgetting for future work. 

\textbf{Neural scaling laws} were first introduced by the seminal work of~\citet{hestness2017deep} to predict the final loss achieved by a model, as a function of its number of parameters $N$ and the amount of data $D$ seen. Later, this empirical study was scaled to GPT-2 scale models trained on billions of tokens by~\citet{kaplan2020scaling}. In their most simple forms, these ``additive'' laws are typically written as:
\begin{equation}
    L=E+\frac{A}{N^{\alpha}}+\frac{B}{D^{\beta}},
\end{equation}
where $E,A,B,\alpha,\beta$ are parameters estimated from measurements.  
Scaling laws allow us to find the best performance at \textit{
IsoFLOPS}, i.e., the lowest loss achievable when the number of FLOPS is fixed. For transformer-like architectures, training FLOPS is approximated as $6ND$ while inference cost is estimated as $2ND$.  
When $\alpha\approx\beta$, as found in the setup of~\citet{NEURIPS2022_c1e2faff}, every parameter is worth a constant amount of tokens. In their study, it is a factor $\times20$ but can go as high as $\times 192$ for training procedures tailored for small models~\citep{hu2024minicpm}. This exact value varies from study to study and typically depends on optimizer hyper-parameters~\citep{porian2024resolving,besiroglu2024chinchilla} or the quality of data~\citep{deepseek-llm}. When accounting for the cost of inference~\citep{sardana2024chinchilla}, smaller models trained for longer should be preferred over bigger ones. Other scaling laws can be baked in, accounting for mixing different modalities~\citep{aghajanyan2023scaling}, encompassing learning rate scheduling~\citep{tissue2024scaling} or taking inspiration from statistical physics~\citep{an2024physics}.  


\textbf{Scaling laws for finetuning.} Conventional training laws focus on datasets too big for overfitting, sometimes even too big to repeat any data. In this context, validation and training loss tend to be equal. This setting cannot be applied in the context of finetuning, where the smaller datasets are subject to \textit{overfitting}, and where the discrepancy between tasks might induce a significant performance gap. This setup of repeating training data has been first studied by~\citet{hernandez2021scalinglaws} and extended by~\citet{muennighoff2023scaling}: they apply an exponential ``decay'' factor $1-\exp{(-{R_D}/{R_D^*})}$ to tokens $D$ to account for the lack of information provided by further repetitions $R_D$ over multiple epochs. In~\citet{zhang2024when}, scaling laws are derived for the finetuning loss with full parameter, LORA adaptation, and prompt finetuning.
Other studies focus on the diversity of domains or different measures than the loss~\citep{barnett2024empirical,isik2024scaling}. 
Close to our work,~\citet{kalajdzievski2024scaling} characterizes forgetting during finetuning. 
The key differences with our work are that \citet{kalajdzievski2024scaling} uses an instructed pretrained LLama2 model. In contrast, we use several model scales to characterize the behavior of finetuning at different scales. 
Also, they measure the forgetting between a pair of datasets, finetuning on one domain and computing the loss on another domain.
In contrast, we measure forgetting by looking at the pretraining loss and considering 6 different domains.
They also do not mention pretraining data injection, which is a key contribution to this work. 
Finally, they consider parameter-efficient finetuning, while we consider full-parameter finetuning, which is more costly but also more efficient~\citep{zhang2024when}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/fig2/arxiv_1.0ls-crop.pdf}
    \caption{\textbf{Generalization-memorization tradeoff.} Arxiv domain. Each point corresponds to the bottom of the U-curve for a model trained on datasets of sizes 300K, 900K, 3,000K, 9,000K and 30,000K tokens with mixture parameter $p=1\%$. \textbf{Forgetting is more severe when the model is small and when the finetuning dataset is big}. As shown in Equation~\ref{eq:finetuningscalinglaw}, this can be attributed to the lack of capacity of the model. More parameters are assigned to training set memorization, and fewer parameters are assigned to the pretraining set performance.}
    \label{fig:paretoplot}
\end{figure}