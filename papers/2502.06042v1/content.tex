

\begin{abstract}
A widespread strategy to obtain a language model that performs well on a target domain is to finetune a pretrained model to perform unsupervised next-token prediction on data from that target domain.
Finetuning presents two challenges: \textit{(i)} if the amount of target data is limited, as in most practical applications, the model will quickly overfit, and \textit{(ii)} the model will drift away from the original model, forgetting the pretraining data and the generic knowledge that comes with it.
Our goal is to derive scaling laws that quantify these two phenomena for various target domains, amounts of available target data, and model scales.
We measure the efficiency of injecting pretraining data into the finetuning data mixture to avoid forgetting and mitigate overfitting.
A key practical takeaway from our study is that injecting as little as $1\%$ of pretraining data in the finetuning data mixture prevents the model from forgetting the pretraining set.
\end{abstract}



\section{Introduction}  

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{fig/fig1/github_tiny_loss_notitle.pdf}
    \caption{\textbf{As little as $p=1\%$ of pretraining data injection shields the model from  forgetting on the pretrain dataset}. The finetuning validation follows a conventional U-curve. In this paper, we always consider the models obtained at the bottom of the U-curve, that is, models with the best validation loss on the finetuning set, indicated here by a black dot.
    The minimum validation loss is barely impacted by the amount of injected pretraining data $p$, and it takes more iterations to reach the minimum as $p$ increases.
    The loss on the training finetuning set converges to zero as training progresses since the network memorizes the dataset.
    The pretraining loss increases monotonically during finetuning.
    \textbf{Injecting pretraining data has a regularizing effect that reduces overfitting and forgetting}.}
\label{fig:trainingcurves}
\end{figure*}

Large Language Models (LLMs) are generalist models that are trained on a large and diverse corpus of data, called the pretraining set.
The diversity of the pretraining set is widely credited with giving LLMs general knowledge and versatile applicability~\citep{yu2024makes}.
However, not every user requires a gigantic generalist LLM to answer simple tasks, and one might instead favor more specialized models that are able to work better on a specific subset of tasks.
For such scenarios, finetuning is the go-to method to obtain a custom LLM that performs well on a comparatively smaller target domain.
More precisely, we refer to finetuning in this work as the optimization of the next-token-prediction loss on target domain data, where the starting point of optimization is a pretrained reference model.
This definition of finetuning encompasses the popular supervised finetuning framework \citep{ouyang2022training}, where the target domain data consists of high-quality examples that the LLM should imitate. 

\textbf{Challenges in Finetuning} 
Finetuning runs into two intertwined issues.
Because the target domain data is often available in limited quantity, notably when compared to the large capacity of an LLM, the model will overfit the target domain during optimization of the training loss. 
Furthermore, the parameters can move far away from the base models' parameters, which may lead to forgetting: the models' performance on generic tasks decreases during finetuning.

\textbf{Pretraining data injection}
A customary approach to mitigate forgetting during finetuning consists of injecting some pretraining data into the finetuning data mixture
\citep{liu2022improved,kang2024get,ibrahim2024simple}. 
The idea is that this data serves as a regularizer that helps the model stay good on generic tasks.
However, it is unclear how to pick the right amount of pretraining data to inject and what precise impact it has on both forgetting and overfitting.

\textbf{Our findings} We consider the target loss, i.e., the loss on a validation version of the target dataset, as a measure of \textit{performance} (relative to the target domain) and the pretraining loss as a measure of \textit{forgetting}. The main takeaway of our work is that for each target domain considered in this paper, the \textbf{pretraining loss after finetuning can be predicted accurately} from \textit{(i)} the model scale, \textit{(ii)} the amount of target data available and \textit{(iii)} the fraction of pretraining data injected in the finetuning data mixture.
We demonstrate that the following scaling law well predicts the pretraining loss \emph{after} finetuning
\vspace{0.9em}
\begin{equation}
\mathcal{L}_{\mathrm{pt}} 
    = \eqnmarkbox[applegreen]{loss}{\mathcal{L}_{\mathrm{pt}}^0} + A\frac{\eqnmarkbox[appleorange]{tokens}{D_{\mathrm{ft}}}^{\beta}}{((1+\eqnmarkbox[appleblue]{pretraincapacity}{B}\eqnmarkbox[applered]{mixture}{p})N)^{\alpha}} + E,    
\label{eq:scalinglaw}
\end{equation}
\annotate[yshift=0.5em]{above, right}{tokens}{Unique Finetune Tokens}
\annotate[xshift=6.5em,yshift=0.8em]{above, left}{loss}{Pretraining Loss Before Finetuning}
\annotate{below, left}{pretraincapacity}{Parameter Relative Efficiency}
\annotate{below, right}{mixture}{Pretraining Mixture}

where $A$, $B$, $E$, $\alpha$ and $\beta$ are domain-dependent constants, $N$ is the model size, $D_{\mathrm{ft}}$ is the number of available finetuning tokens, and $p\in[0, 1]$ is the proportion of pretraining data injected in the finetuning data mixture.
In particular, we report that, as a rule of thumb, as little as $p=1\%$ of pretraining data injection is enough to mitigate forgetting.

We also study the impact of these three quantities on the target loss, and report that the data mixture coefficient $p$ has little effect on it. 
In the case where $p$ is small, we recover the multiplicative scaling laws of \citet{zhang2024when}:
\begin{equation}
\mathcal{L}_{\mathrm{ft}} 
    = A\frac{1}{\eqnmarkbox[applepurple]{capacity}{N}^{\alpha}} 
    \frac{1}{\eqnmarkbox[appleorange]{tokens}{D_{\mathrm{ft}}}^{\beta}} + {E},    
\end{equation}
\annotate{below, left}{capacity}{Parameter Count}
\annotate{below, right}{tokens}{Unique Finetune Tokens}



\textbf{Paper organization}
In \autoref{sec:methods}, we describe the models used in this study, the different phases of model training and the pretraining data injection method.
In \autoref{sec:expe}, we describe the models and datasets used in this study, as well as the experimental setup.
Finally, \autoref{sec:results} describes our findings and the estimated scaling laws.

\subsection{Related works}

\paragraph{Auto-regressive pretraining and fine tuning.} Auto-regressive pretraining is a paradigm in which a generative model is trained on sequences to predict the next token given a sequence prefix~\citep{sutskever2014sequence}. This is typically formulated as a classification task over a given vocabulary built from a \textit{tokenizer}. 
Finetuning consists of pursuing the auto-regressive pretraining on a specific \textit{finetuning dataset}, orders of magnitude smaller than the pretraining dataset. 
This finetuning dataset typically encompasses data from a single domain, exhibiting a significant distribution shift compared to the base dataset. 
Transferring the model knowledge to the new distributions can lead to significant capability loss on the previous dataset, a phenomenon known as \textit{forgetting} \cite{luo2023empirical,kalajdzievski2024scaling}.

In the context of \textit{continual pretraining}, it has been observed by~\citet{ibrahim2024simple} that mixing a small percentage of pretraining data mitigates forgetting \citep[see][for an analytical framework to explore how task similarity can affect knowledge transfer]{hiratani2024disentangling}.




Alternative approaches in the context of parameter-efficient finetuning (PEFT) have tried to address these challenges by introducing lightweight modules, such as \textit{adapters}, that are updated with knowledge about the new task \cite{houlsby2019parameter, he2022unified}. 
In this context, Low-Rank Adaptation methods, or LoRA \cite{hu2021lora, zhang2024when}, inject trainable low-rank matrices into the model layers, by adding them to the untouched original model weights, thus reducing the parameter overhead needed to train for new downstream tasks \citep[see][for a discussion on the different roles of LoRA decomposition matrices]{zhu2024asymmetry}.
\citet{zhang2024when} question the use of PEFT methods for fine-tuning, reporting that it usually underperforms in terms of fine-tuning loss compared to full-parameter tuning. 
This is why in this work, we focus on full parameter tuning, and defer the study of PEFT on forgetting for future work. 

\textbf{Neural scaling laws} were first introduced by the seminal work of~\citet{hestness2017deep} to predict the final loss achieved by a model, as a function of its number of parameters $N$ and the amount of data $D$ seen. Later, this empirical study was scaled to GPT-2 scale models trained on billions of tokens by~\citet{kaplan2020scaling}. In their most simple forms, these ``additive'' laws are typically written as:
\begin{equation}
    L=E+\frac{A}{N^{\alpha}}+\frac{B}{D^{\beta}},
\end{equation}
where $E,A,B,\alpha,\beta$ are parameters estimated from measurements.  
Scaling laws allow us to find the best performance at \textit{
IsoFLOPS}, i.e., the lowest loss achievable when the number of FLOPS is fixed. For transformer-like architectures, training FLOPS is approximated as $6ND$ while inference cost is estimated as $2ND$.  
When $\alpha\approx\beta$, as found in the setup of~\citet{NEURIPS2022_c1e2faff}, every parameter is worth a constant amount of tokens. In their study, it is a factor $\times20$ but can go as high as $\times 192$ for training procedures tailored for small models~\citep{hu2024minicpm}. This exact value varies from study to study and typically depends on optimizer hyper-parameters~\citep{porian2024resolving,besiroglu2024chinchilla} or the quality of data~\citep{deepseek-llm}. When accounting for the cost of inference~\citep{sardana2024chinchilla}, smaller models trained for longer should be preferred over bigger ones. Other scaling laws can be baked in, accounting for mixing different modalities~\citep{aghajanyan2023scaling}, encompassing learning rate scheduling~\citep{tissue2024scaling} or taking inspiration from statistical physics~\citep{an2024physics}.  


\textbf{Scaling laws for finetuning.} Conventional training laws focus on datasets too big for overfitting, sometimes even too big to repeat any data. In this context, validation and training loss tend to be equal. This setting cannot be applied in the context of finetuning, where the smaller datasets are subject to \textit{overfitting}, and where the discrepancy between tasks might induce a significant performance gap. This setup of repeating training data has been first studied by~\citet{hernandez2021scalinglaws} and extended by~\citet{muennighoff2023scaling}: they apply an exponential ``decay'' factor $1-\exp{(-{R_D}/{R_D^*})}$ to tokens $D$ to account for the lack of information provided by further repetitions $R_D$ over multiple epochs. In~\citet{zhang2024when}, scaling laws are derived for the finetuning loss with full parameter, LORA adaptation, and prompt finetuning.
Other studies focus on the diversity of domains or different measures than the loss~\citep{barnett2024empirical,isik2024scaling}. 
Close to our work,~\citet{kalajdzievski2024scaling} characterizes forgetting during finetuning. 
The key differences with our work are that \citet{kalajdzievski2024scaling} uses an instructed pretrained LLama2 model. In contrast, we use several model scales to characterize the behavior of finetuning at different scales. 
Also, they measure the forgetting between a pair of datasets, finetuning on one domain and computing the loss on another domain.
In contrast, we measure forgetting by looking at the pretraining loss and considering 6 different domains.
They also do not mention pretraining data injection, which is a key contribution to this work. 
Finally, they consider parameter-efficient finetuning, while we consider full-parameter finetuning, which is more costly but also more efficient~\citep{zhang2024when}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/fig2/arxiv_1.0ls-crop.pdf}
    \caption{\textbf{Generalization-memorization tradeoff.} Arxiv domain. Each point corresponds to the bottom of the U-curve for a model trained on datasets of sizes 300K, 900K, 3,000K, 9,000K and 30,000K tokens with mixture parameter $p=1\%$. \textbf{Forgetting is more severe when the model is small and when the finetuning dataset is big}. As shown in Equation~\ref{eq:finetuningscalinglaw}, this can be attributed to the lack of capacity of the model. More parameters are assigned to training set memorization, and fewer parameters are assigned to the pretraining set performance.}
    \label{fig:paretoplot}
\end{figure}

\section{Methods}
\label{sec:methods}

We describe the training phases a model undergoes in our study: pretraining and finetuning.
We also describe pretraining data injection for finetuning.

\subsection{Pretraining phase}

The model is trained on a significant amount of tokens from a pretraining dataset $\Dbase$ using the next-token prediction loss, until satisfying performance is achieved. 
The model minimizes the pretraining loss
\begin{equation}
\mathcal{L}_{\mathrm{pt}} = \mathbb{E}_{x\sim \Dbase}[\ell(x, \theta)].
    \label{eq:loss_pt}
\end{equation}
The model is typically pretrained using Adam and decaying learning rate, for example, following a cosine scheduling~\citep{loshchilov2017sgdr} or a square root one~\citep{hagele2024scaling}. 
This pretraining loss is low when the model is able to correctly do next-token-prediction on the pretraining set, which, if it is diverse enough, means that the model has general knowledge. 
In the rest of the paper, we use this pretraining loss as a proxy of general knowledge of a model, and in particular, we measure the forgetting of a model through the increase of pretraining loss.


\subsection{Finetuning phase}

In the second phase of training, the model is finetuned on a smaller task-specific dataset $\Dtune$, by resuming training from the pretrained model. The parameters are updated by minimizing the loss on $\Dtune$
\begin{equation}\label{eq:secondphase}
\mathcal{L}_{\mathrm{ft}} = \Expect_{x\sim\Dtune}[\loss(x,\Param)],
\end{equation}
Since the finetuning set is small, the optimization of this loss quickly leads to overfitting, where the loss on a validation finetuning set increases - this yields a U-curve as displayed in \autoref{fig:trainingcurves}. Furthermore, since the model is now only trained by seeing data from the finetuning set, it drifts away from the base model and forgets some of the pretraining set.
The optimization of the finetuning loss is typically carried out until the validation loss starts increasing, and this checkpoint gives the so-called finetuned model.

\subsection{Pretraining data injection}

A simple method to mitigate overfitting and model forgetting consists of mixing data from the pretraining set and finetuning the set during finetuning.

Formally, give a mixture parameter $p \in [0, 1]$, we define the mixture of domains $\mathrm{mix}(p) = (1-p)\Dtune + p \Dbase$. 
In other words, the probability of selecting a datapoint from $\mathrm{mix}(p)$ is
$$
P(x|\mathrm{mix}(p)) = (1- p) P(x|\Dtune) + p P(x|\Dbase).
$$
We can sample efficiently from $\mathrm{mix}(p)$ by first sampling a random binary variable $i\in\{0, 1\}$ with probability $p$ that $i=1$, and then sampling from $\Dbase$ if $i=1$ and from $\Dtune$ otherwise. 
We can then perform finetuning of the model by minimizing the loss over $\mathrm{mix}(p)$: 
$$
\mathcal{L}_{\mathrm{mix}} = \mathbb{E}_{x\sim \mathrm{mix}(p)}[\ell(x, \theta)] = (1-p) \mathcal{L}_{\mathrm{ft}} + p \mathcal{L}_{\mathrm{pt}}.
$$
This formulation makes it clear that pretraining data injection can be interpreted as adding the pretraining loss as a regularizer to the finetuning loss~\citep{hastie2017elements}.
This way, the model still sees some samples from the pretraining set, which acts as a way to mitigate forgetting and overfitting.


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/fig3/enron_emails_notitle-crop.pdf}
    \caption{ \textbf{Losses as a function of the fraction of injected pretraining data $p$ on Enron emails with 900K finetuning tokens.} Data mixing improves generalization when finetuning data is scarce. The diversity of the pretraining dataset biases learning toward features that exhibit higher generalization. The optimal value of $p$ depends on the domain, the dataset size, and the model size. The finetuning loss as a function of $p$ also follows a U-curve: when $p$ is too small, the model overfits too quickly and does not benefit from the regularizing effect of pretraining data injection. When $p$ is too large, the model does not see enough finetuning data to allocate it enough capacity, there is too much tension with learning from the pretraining set.
    As expected, increasing $p$ monotonically decreases the pretraining loss.}
    \label{fig:optimalgamma}
\end{figure*}

\section{Experiments}
\label{sec:expe}

\begin{table*}[t!]
    \centering
    \begin{tabular}{cccccccccc}
    \textbf{Model Size} & \textbf{Parameters (N)} & \textbf{Dim} &
    \textbf{Heads} & \textbf{Layers} & \textbf{Batch Size} & \textbf{Initial LR} & \textbf{Tokens (D)} & \textbf{FLOPs}\\ %
    \toprule
    \textcolor{appleblue}{\textbf{Tiny}}                & 41M                      & 512                   & 8              & 8              & 32                  & 1e-3                 & 5.1B             & $1.25\times 10^{18}$\\ %
    \textcolor{appleorange}{\textbf{Small}}               & 109M                     & 768                   & 12             & 12             & 32                  & 1e-3                 & 12.B             & $7.84\times 10^{18}$\\ %
    \textcolor{applegreen}{\textbf{Medium}}              & 334M                     & 1024                   & 16             & 24             & 64                  & 1e-3                 & 33B             & $6.61\times 10^{19}$\\ %
    \textcolor{applered}{\textbf{Large}}               & 665M                     & 1536                   & 16             & 24             & 128                 & 3e-4                 & 66B             & $2.63\times 10^{20}$\\ %
    \textcolor{applepurple}{\textbf{XL}}                  & 1.27B                    & 2048                   & 16             & 24             & 112                 & 3e-4                 & 100B             & $7.62\times 10^{20}$\\
    \end{tabular}
    \caption{\textbf{Pretrained model configurations for different sizes of GPT models}. All models use the same tokenizer with a vocabulary size of $32,000$, and the MLP hidden dimension is $4$ times the dimension of the model. The output embedding layer is included in the parameter count, as per~\citet{porian2024resolving}. Training FLOPS are reported as $6ND$, following standard practices~\citep{kaplan2020scaling}. }
    \label{tab:model_configurations}
\end{table*}

\subsection{Experimental setup}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/fig4/dm_mathematics_wikipedia_en-crop.pdf}
    \caption{\textbf{Overfitting and forgetting profiles for two domains.} On one hand, \textbf{Dm mathematics (left)} is a dataset that differs a lot from the pretraining set, benefits little from more parameters, and a lot from more data. On the other hand, \textbf{Wikipedia En (right)} is more similar to the pretraining set, and more parameters are more beneficial than more training data. Datasets that are far from the pretraining distribution are more prone to forgetting and benefit the most from injecting pre-training data $p>0$.}
    \label{fig:empiricalsscalinglaws}
\end{figure*}


\textbf{Models.}
We train GPT2 style transformers~\citep{radford2019language} of different scales. 
\autoref{tab:model_configurations} reports the model architectures used here. The parameter count follows an exponential progression to cover different magnitudes.  
 For every model, we fix the vocabulary size to $32,000$ and the sequence length to $1,024$. We use the SentencePiece tokenizer~\citep{kudo2018sentencepiece}.
 Every computation is performed in \textit{bfloat16} precision, except for normalization layers and softmax in self-attention, which are computed in \textit{float32} precision, following standard practices~\citep{rabe2021self,wang2024precision}. 
 The biggest model fits on a single A$100-80$GB GPU without sharding, with parameter replication across GPUs to handle large batch sizes. 1 GPU is used for \textit{Tiny} and \textit{Small}, 4 for \textit{Medium}, 8 for \textit{Large} and \textit{XL}.  

\textbf{Datasets.} We use RedpajamaV2~\citep{weber2024redpajama} as the pretraining set.
We use several domains from The Pile~\citep{gao2020pile} as finetuning sets, covering all 5 categories (``academic'', ``internet'', ``prose'', ``dialogue'' and ``misc'').  We artificially keep a limited number of tokens to imitate data scarcity, following a log scale between 300 K and 30,000 K tokens - which typically represents between 300 and 60,000 pages from a book, respectively. We believe this accurately reflects realistic use cases where finetuning is performed on specific internal documentation on a specific topic.

\textbf{Pretraining.} We pretrain each model on RedpajamaV2 using standard hyperparameters and a total count of 100 tokens per parameter. The learning rate follows a linear warmup for 0.5\% of the total iterations and then follows a cosine scheduling until the end of the training, with a terminal value that is one-hundredth of the maximum value. We use AdamW with a weight decay of $0.1$, which yielded better results than without weight decay (Figure~\ref{fig:weight_decay}). A gradient clipping of $5$ was found sufficient to stabilize training across all model scales.  
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/fig-lr/both-crop.pdf}
    \caption{\textbf{Finetuning learning rate as fraction of peak pretraining learning rate.} Ablation on Arxiv with XL Model (1.3B). The model's learning rate (LR) is reduced by a factor of 100 during pretraining using cosine scheduling. For finetuning, we employ a constant LR, defined as a multiple of the peak LR. Our observations indicate that setting the finetuning LR to $1/30$ times the terminal value strikes an optimal balance between effective adaptation to the finetuning dataset and stability with respect to the pretrained features. Notably, this factor of $1/30$× corresponds to the LR reached at 90\% of the pretraining phase, near convergence.}
    \label{fig:lr_ablation}
\end{figure}

\textbf{Finetuning.}  We then finetune the model on several domains from the Pile, using a varying amount of target data and different fractions of injected pretraining data.
We perform finetuning for 12K steps, which is sufficient to observe a U-curve on the validation loss in every configuration tested. 
The learning rate is equal to $1/30$ times the peak pretraining LR, which was reached at about $90\%$ of the pretraining stage.  Empirically, we observe in the ablations of \autoref{fig:lr_ablation} that this rule of thumb is sufficient to ensure both overfitting well within 12K steps and stable training at all model scales and all mixtures $p$. We use Adam without weight decay. The parameter $p$ is chosen from the set \{$0\%, 0.1\%, 0.5\%, 1\%, 5\%$\}.  

In total, we, therefore, have 5 model sizes $\times$ 5 number of finetuning tokens $\times$ 5 injection fractions$\times$ 12 domains, which gives 1500 finetuning runs to study.      

We report validation losses on the pretraining and target domains, and train losses on the target domains. We use these losses as a proxy for the model's performance on the finetuning domains.


\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{fig/fig-pretrainsize/sizes.pdf}
    \caption{\textbf{Influence of the number of available pretraining tokens} on the pretraining loss after fine-tuning with $p=1\%$ of pretraining data injected to the mix. We use the Arxiv domain and a model of size ``tiny''.
    With little finetuning data, finetuning is short and hence we can use very few pre-training tokens to reach the optimal forgetting. When there is a lot of finetuning tokens, the optimization takes a long time to reach the bottom of the U-curve. Hence, when the number of pretraining tokens available is also limited, these tokens are repeated many times, which leads to overfitting on the pretraining set as well, and increases the pre-training loss.
    Remarkably, only a small amount of pretraining tokens is enough to make pretraining data injection efficient during fine-tuning.}
    \label{fig:pretrain_size}
\end{figure}

\section{Results}
\label{sec:results}

Unless specified otherwise, we report these values at the checkpoint corresponding to the lower validation domain target loss, i.e., the bottom of the ``U-curve''. 


\subsection{Mundane observations: overfitting}

The loss profile of a typical finetuning run is shown in \autoref{fig:trainingcurves}. Unsurprisingly, every validation curve for the finetuning dataset follows a U-shape since the small number of tokens allows for memorization on the train set. The loss over the train set decreases monotonically, with a steeper slope for smaller train sets. The pretraining loss typically follows a forgetting pattern, which is mitigated by taking~$p>0$.

\vspace{-0.2cm}
\subsection{Speed of forgetting}

Empirically, we observe in \autoref{fig:trainingcurves} that as little as $p=\bm{1}\%$ of pretraining data shields the model from forgetting at no cost for performance on the finetuning dataset. The pretraining data plays an ``anchoring'' role that helps retain useful features.  
  
On the contrary, in the absence of pretraining data injection, performance can plummet drastically, especially for smaller models. We hypothesize that smaller models must balance their capacity between tasks, whereas bigger ones can ``learn a new task'' while preserving most existing knowledge.  

\begin{table*}[t!]
    \centering
    \begin{tabular}{c|ccccc|cccccc}
        \textbf{Domain} & \multicolumn{5}{c|}{\textbf{Finetuning Scaling Laws}} & \multicolumn{6}{c}{\textbf{Forgetting Scaling Laws}} \\
        & $\alpha$ & $\beta$ & A & E & \makecell{Bootstrapped\\ MRE ($\downarrow$)} & $\alpha$ & $\beta$ & A & B & E & \makecell{Bootstrapped\\ MRE ($\downarrow$)} \\
        \toprule
        Arxiv & 0.17 & 0.10 & 95.18 & 1.30 & 0.91\% & 0.74 & 0.30 & 1523 & 544 & 0.06 & 0.48\% \\
        Dm mathematics & 0.06 & 0.19 & 16.03 & 0.88 & 0.50\% & 0.74 & 0.44 & 389 & 9135 & 0.06 & 0.71\% \\
        Enron emails & 0.07 & 0.05 & 20.21 & 0.00 & 1.13\% & 0.46 & 0.19 & 51 & 1560 & 0.05 & 0.58\% \\
        Github & 0.14 & 0.12 & 84.55 & 0.79 & 1.40\% & 0.61 & 0.33 & 85 & 1512 & 0.05 & 0.51\% \\
        Pg19 & 0.14 & 0.02 & 34.55 & 1.25 & 0.65\% & 0.81 & 0.48 & 218 & 368 & 0.06 & 0.50\% \\
        Wikipedia en & 0.13 & 0.02 & 30.11 & 0.62 & 0.53\% & 0.53 & 0.10 & 239 & 287 & 0.05 & 0.34\% \\
        Euro parl & 0.12 & 0.17 & 160.24 & 1.10 & 1.86\% & 0.74 & 0.37 & 1043 & 1849 & 0.06 & 0.85\% \\
        Free law & 0.19 & 0.05 & 94.06 & 1.11 & 0.91\% & 0.78 & 0.36 & 596 & 263 & 0.06 & 0.42\% \\
        Openwebtext 2 & 0.14 & 0.01 & 31.54 & 0.96 & 0.35\% & 0.32 & 0.15 & 2.4 & 1544 & 0.05 & 0.36\% \\
        Pubmed abstracts & 0.17 & 0.01 & 46.89 & 0.94 & 0.83\% & 0.78 & 0.45 & 107 & 523 & 0.06 & 0.34\% \\
        Pubmed central & 0.18 & 0.05 & 74.37 & 1.09 & 0.56\% & 0.69 & 0.30 & 329 & 474 & 0.06 & 0.40\% \\
        Stackexchange & 0.16 & 0.08 & 78.23 & 1.27 & 1.03\% & 0.56 & 0.28 & 47 & 892 & 0.05 & 0.42\% \\
    \end{tabular}
    \caption{\textbf{Comparison of Scaling Law Coefficients for Finetuning and Forgetting.} The Bootstrapped Estimate of Mean Relative Error $|\hat y - y|/y$ across domains for finetuning is $\bm{0.89\%}$, and for forgetting is $\bm{0.49\%}$. These estimates are obtained by resampling each example independently with equal probability to construct a new set of 125 points per domain. The final results are averaged over 128 independent bootstrap repetitions.}
    \label{tab:scaling_laws_comparison}
\end{table*}


\subsection{Pretraining data injection improves generalization}

At $p>0$, the bottom of the U-curve can reach lower values than $p=0$, which suggests that pretraining data injection serves are a regularization and helps generalization on the finetuning dataset. 
This effect can be observed in \autoref{fig:optimalgamma}. As a function of $p$, the finetuning validation loss also follows a U-curve - albeit less pronounced. This effect is more striking for small models.

\subsection{Repeating pretraining tokens during finetuning}

In all our experiments, we inject pretraining data that is streamed from the pretraining set without repetition.
As reported, injecting only $p=1\%$ of pretraining data in the training mix is enough to mitigate forgetting significantly. 
Since $p$ is quite small, it means that we use a small number of pre-training tokens. 
For instance, in \autoref{fig:trainingcurves}, we see that the bottom of the U-curve is reached around $1800$ iterations for the run with $p=1\%$ of injected pre-training data.
Since we use a batch-size of $32$ and a context length of $1024$, it means that, at the bottom of the U-curve, we have only seen $1\%\times 1800 \times 32\times 1024=600K$ pretraining tokens.


   
We now turn to a study of how limiting the amount of available \emph{pretraining} tokens that are used for pretraining data injection impacts finetuning and forgetting.
To do so, we consider one domain (arxiv), one model size (tiny), and a fixed fraction of pretraining data injection ($p=1\%$). 
We then artificially limit the number of available pretraining tokens in 32KT, 96KT, 320KT, 920KT, and 3,200KT. %
We report the results in \autoref{fig:pretrain_size}.
We observe that as the number of available pretraining tokens increases, the pretraining loss after finetuning decreases. However, in this regime, the pretraining dataset itself becomes susceptible to overfitting. This underlines the pivotal role of pretraining dataset diversity in mitigating forgetting.  

\begin{figure*}[!ht]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/multiplicative/gamma1.0/wikipedia_en_notitle-crop.pdf}
        $p = 1\%$
        \label{fig:wikipedia_gamma}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/multiplicative/size300KT/wikipedia_en_notitle.pdf}
        300K Tokens
        \label{fig:dm_mathematics_size}
    \end{minipage}
    \caption{\textbf{Scaling laws for finetuning loss.} Wikipedia domain. We extend the multiplicative laws of \citet{zhang2024when} to take into account the fraction of injected pretraining data.
    We display the observed behavior and the behavior predicted by the scaling law. At the first order, this scaling law is independent of the mixture $p$. \textbf{Left} Agreement between evaluated and predicted loss on one domain. \textbf{Right} Evaluated and predicted loss as a function of $p$.}
    \label{fig:finetunescalinglaw}
\end{figure*}

\subsection{Scaling laws}
We display the loss as a function of model size and dataset size for two domains in \autoref{fig:empiricalsscalinglaws}. 
We observe a highly predictable behavior that is amenable to the fitting of a scaling law.

We consider different model sizes, which in turn impact the number of pretraining tokens used to train the model, using 100 tokens per parameter. 
Hence, we do not explore the full pretraining (N, D) space, restricting ourselves to the isocurve $D=100N$.
We have two main reasons for this approach. First, while analyzing the behavior of losses in relation to both N and the total number of pretraining tokens is insightful, using an isocurve aligns with the most common practice, which is to train only one model per model size. Second, pretraining a large grid of models would be computationally expensive, and we prefer to allocate our computing budget toward a more detailed understanding of finetuning mechanisms.
  0
We fit a different scaling law for the finetuning and forgetting loss for every domain. Per domain, we have $5$ model sizes $\times$  $5$ dataset sizes $\times$  $5$ mixture values $= 125$ points in total. This is large in front of the number of degrees of freedom of the scaling law ($4$ or $5$ in our case).


\paragraph{Method.} To fit the coefficients, we follow a conventional approach~\citep{muennighoff2023scaling}, i.e., we perform optimization in log space for improved numerical stability. See \Cref{ssec:supervised-scaling-law-coefficient-estimation} for a detailed discussion.

\paragraph{Scaling law for finetuning loss} For every domain $\mathcal{D}$ of the Pile, we fit the multiplicative law proposed by~\citet{zhang2024when} for finetuning:
\begin{equation}
\label{eq:finetuningscalinglaw}
\mathcal{L}_{\mathrm{ft}} 
    = A\frac{1}{{N}^{\alpha}} 
    \frac{1}{{D_{\mathrm{ft}}}^{\beta}} + {E},    
\end{equation}
which yields a Bootstrapped ($n=128$) Mean Relative Error (MRE) of $\bm{0.89\%}$ across domains.
As depicted in \autoref{fig:optimalgamma}, the fine-tuning loss is barely impacted by the fraction of injected pre-training data $p$, which is why we chose to treat the loss as a constant of $p$.
We report scaling laws coefficients in \autoref{tab:scaling_laws_comparison}.
The results for fixed dataset size or for a fixed mixture $p$ are given in \autoref{fig:finetunescalinglaw}.
We also tried to fit an additive law of the form $\mathcal{L}_{\mathrm{ft}} 
= \frac{A}{{N}^{\alpha}}  + 
\frac{B}{{D_{\mathrm{ft}}}^{\beta}} +{E}$, but despite the additional degree of freedom through coefficient $B$, it yields an higher MRE of 1.36\%. The superiority of the multiplicative scaling law compared to the additive is consistent with the findings of \citet{zhang2024when}.



\begin{figure*}[t]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/forgetting/gamma0.0/github_notitle-crop.pdf}
        $p = 0\%$
        \label{fig:wikipedia_gamma_forget}
    \end{minipage} 
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/forgetting/size300KT/github_notitle.pdf}
        300K Tokens
        \label{fig:dm_mathematics_size_forget}
    \end{minipage}
    \caption{\textbf{Scaling laws for forgetting.} Github domain. We propose to model the increase in pretraining loss as a multiplicative scaling law (\autoref{eq:scalinglaw}), that takes into account model size, number of finetuning tokens available, and fraction of pretraining data injected in the data mixture $p$. \textbf{Left} Agreement between evaluated and predicted loss on one domain. \textbf{Right} Evaluated and predicted loss as a function of $p$.}
\label{fig:forgettingscalinglaw}
\end{figure*}


\paragraph{Scaling law for forgetting} For every domain of the Pile, we want to predict $\mathcal{L}_{\mathrm{pt}}$, the value of the pretraining loss \emph{after} finetuning, as a measure of forgetting. We propose the (modified) multiplicative law:
\begin{equation}
\mathcal{L}_{\mathrm{pt}} 
    =\mathcal{L}_{\mathrm{pt}}^0 + A\frac{{D_{\mathrm{ft}}}^{\beta}}{((1+{B}{p})N)^{\alpha}} + E,    
\end{equation}
where $\mathcal{L}_{\mathrm{pt}}^0$ is the pretraining loss \emph{before} fine-tuning, i.e. the pretraining loss of the pretrained model, $D_\mathrm{ft}$ is the number of available fine-tuning tokens, $N$ is the model size.
The variables of the scaling law are $A, B, E, \beta$ and $\alpha$, and they are all positive numbers.
Note that the value $\mathcal{L}_{\mathrm{pt}}^0$ is exactly what the original scaling law papers try to estimate~\citep{kaplan2020scaling,DBLP:journals/corr/abs-2203-15556}. We propose a multiplicative law where $D^{\beta}$ is on the \emph{numerator} with $\beta>0$, since increasing the finetuning dataset size leads to more iterations to reach the bottom of the U-curve, which results in a model that drifts further from the base model and a higher pretraining loss. We use a factor $(1+Bp)$ in front of the $N$ parameter. It accounts for the fact that a fraction $p$ of the parameters are allocated to the pretraining task rather than the finetuning one, and they are $B$ times more efficient in this context. Typically, we observe $B\gg 1$ since the features of the pretrained model are already aligned with the pretraining task, which translates into more efficient parameter allocation. Coefficients are given in \autoref{tab:scaling_laws_comparison}. The average Bootstrapped MRE across domains is $\bm{0.49\%}$ - whereas an additive law to predict the difference $\mathcal{L}_{\mathrm{pt}} - \mathcal{L}_{\mathrm{pt}}^0$ has an error greater than $0.82\%$ despite having one more degree of freedom.  
We also note that it is critical to add the base pretraining loss $\mathcal{L}_{pt}^0$ to the scaling law, as fitting a scaling law of the form $\mathcal{L}_{pt}= A\frac{{D_{\mathrm{ft}}}^{\beta}}{((1+{B}{p})N)^{\alpha}} + E$ leads to a MRE of $1.05\%$.  
  
The value of $B$ indicates how much pretraining data injection helps mitigate forgetting. For Dm mathematics, which was observed in \autoref{fig:empiricalsscalinglaws} to be highly subject to forgetting, $B\approx 10^4$. On the contrary, for Wikipedia, on which pre-training data injection was less useful, we only have $B\approx 287$.  

The fact that $E\neq 0$ may come as a surprise, since one would expect that the difference $\mathcal{L}_{\mathrm{pt}}-\mathcal{L}_{\mathrm{pt}}^0$ goes to zero as $D\rightarrow 0$ and $N\rightarrow \infty$. But even in the setting $p=1$ with pure pretraining data, further pretraining with a constant LR (which is typically higher than the terminal LR as shown in \autoref{fig:lr_ablation}) cancels some of the benefits of the cosine cooldown used during pretraining. This also explain why this value of $E$ tend to be constant across domains.  

\begin{table}[]
    \centering
    \begin{tabular}{cc|cc}
         Setup & Predict on & \makecell{Finetuning\\ MRE} & \makecell{Forgetting\\ MRE}\\
         \toprule
          \multirow{2}{*}{\textbf{A}} & XL & \multirow{2}{*}{1.69\%} & \multirow{2}{*}{0.73\%}\\
          & 30,000K &  & \\
          \hline
          \multirow{2}{*}{\textbf{B}} & Large, XL & \multirow{2}{*}{2.01\%} & \multirow{2}{*}{0.83\%}\\
          & 9,000K, 30,000K &  & \\
    \end{tabular}
    \caption{\textbf{Mean Relative Error in the extrapolation setting.} In setup~\textbf{A}, scaling law coefficients are estimated on all model sizes except \textit{XL}, and on all finetuning set sizes except $30,000$K tokens. In setup~\textbf{B}, the largest model used for fitting is \textit{Medium} (334M) and the maximum finetuning dataset size is limited to $3,000$K tokens.}
    \label{tab:extrapolate}
\end{table}

\textbf{Extrapolation.} Scaling laws are mainly useful when they enable performance predictions based on small-scale experiments, to assess if bigger models and datasets are worth the cost. To test this, we design a synthetic experiment where scaling law coefficients are fitted using only a subset of model sizes and fine-tuning dataset sizes. The results, presented in \autoref{tab:extrapolate}, demonstrate that models trained on no more than \textit{Medium} (334M parameters) and 3,000K tokens can accurately predict the performance of \textit{Large} and \textit{XL} models trained on 9,000K and 30,000K tokens. The prediction error remains within 2.01\% on the finetuning set and 0.83\% on the pretraining set. On 4 GPUs, the \textit{Medium} model fine-tuned on 3,000K tokens reaches the bottom of the U-curve in under 30 minutes, whereas the \textit{XL} model requires up to 7 hours on 8 GPUs. In this context, scaling laws can lead to substantial computational savings.    


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/fig2/arxiv_1.0hours_perc.pdf}
    \caption{\textbf{Cost of forgetting.} Points are reported at the bottom of the U-curve forcthe Arxiv domain. Same setup as \autoref{fig:paretoplot}. Smaller models lose a significant portion of the progress achieved during pretraining—up to 80\%—effectively negating much of the initial effort. In contrast, larger models retain more knowledge due to their greater capacity, allowing them to accommodate both tasks simultaneously. However, these models are also the most expensive to train, not only because of their size but also due to the increased number of pretraining tokens required for compute-optimality~\citep{DBLP:journals/corr/abs-2203-15556}. Consequently, the GPU-hour cost induced by forgetting is substantially higher for larger models.}
    \label{fig:costlost}
\end{figure}


\section*{Conclusion}
In this work, we showed that one can accurately predict the finetuning performance and the forgetting of the pretraining set of large language models, as a function of the model size, the number of available finetuning tokens, and of the fraction of pretraining data injected into the finetuning data mixture.
This behavior is very consistent across datasets and can be explained by simple scaling laws.
A key takeaway of our work is that injecting even $1\%$ of training data during fine-tuning helps mitigate pretraining set forgetting.

\FloatBarrier 

\newpage
\newpage
\section*{Impact Statement}


This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none of which we feel must be specifically highlighted here.

\bibliography{biblio}
\bibliographystyle{bst}

\appendix
\onecolumn


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{fig/fig1/enron_emails_medium_loss.pdf}
    \includegraphics[width=1\linewidth]{fig/fig1/github_tiny_loss.pdf}
    \includegraphics[width=1\linewidth]{fig/fig1/pg19_large_loss.pdf}
    \caption{\textbf{As little as $p=1\%$ of pretraining data shields the model from forgetting on the pretrain dataset}.}
\label{fig:trainingcurveshorizontal}
\end{figure*}



\begin{figure*}

    \centering
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{fig/fig3-appendix/arxiv.pdf}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{fig/fig3-appendix/dm_mathematics.pdf}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{fig/fig3-appendix/enron_emails.pdf}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{fig/fig3-appendix/github.pdf}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{fig/fig3-appendix/pg19.pdf}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{fig/fig3-appendix/wikipedia_en.pdf}
    \end{minipage}
    \label{fig:fig3appendix}
    
    \caption{\textbf{Losses at the bottom of the U-curve} for 6 domains of The Pile with 5 models and 5 dataset sizes, for all values of data mixture~$\gamma$.}
\end{figure*}


\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{fig/fig-basemodel/histplot.pdf}
    \caption{\textbf{Ablation.} Weight decay during pretraining improves checkpoint quality.}
    \label{fig:weight_decay}
\end{figure}

\begin{figure*}%
    \centering
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{fig/fig4/arxiv_scaling_laws.pdf}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{fig/fig4/dm_mathematics_scaling_laws.pdf}
    \end{minipage}
    \vspace{0.5cm}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{fig/fig4/enron_emails_scaling_laws.pdf}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{fig/fig4/wikipedia_en_scaling_laws.pdf}
    \end{minipage}
    \vspace{0.5cm}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{fig/fig4/github_scaling_laws.pdf}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{fig/fig4/pg19_scaling_laws.pdf}
    \end{minipage}
    \caption{\textbf{Losses as a function of model size, dataset size, and data-mixing.}}
    \label{fig:fig4grid}
\end{figure*}

\begin{figure*}%
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=0.49\linewidth]{fig/multiplicative/size300KT/arxiv.pdf}
        \includegraphics[width=0.49\linewidth]{fig/multiplicative/gamma0.0/arxiv.pdf}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=0.49\linewidth]{fig/multiplicative/size300KT/dm_mathematics.pdf}
        \includegraphics[width=0.49\linewidth]{fig/multiplicative/gamma0.0/dm_mathematics.pdf}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=0.49\textwidth]{fig/multiplicative/size300KT/enron_emails.pdf}
        \includegraphics[width=0.49\textwidth]{fig/multiplicative/gamma0.0/enron_emails.pdf}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}%
        \includegraphics[width=0.49\textwidth]{fig/multiplicative/size300KT/wikipedia_en.pdf}
        \includegraphics[width=0.49\textwidth]{fig/multiplicative/gamma0.0/wikipedia_en.pdf}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=0.49\textwidth]{fig/multiplicative/size300KT/github.pdf}
        \includegraphics[width=0.49\textwidth]{fig/multiplicative/gamma0.0/github.pdf}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=0.49\textwidth]{fig/multiplicative/size300KT/pg19.pdf}
        \includegraphics[width=0.49\textwidth]{fig/multiplicative/gamma0.0/pg19.pdf}
    \end{minipage}
    \caption{\textbf{Example of finetuning scaling laws for several domains.}}
    \label{fig:multiplicativegrid}
\end{figure*}

\begin{figure*}%
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=0.49\linewidth]{fig/forgetting/size300KT/arxiv.pdf}
        \includegraphics[width=0.49\linewidth]{fig/forgetting/gamma0.0/arxiv.pdf}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=0.49\linewidth]{fig/forgetting/size300KT/dm_mathematics.pdf}
        \includegraphics[width=0.49\linewidth]{fig/forgetting/gamma0.0/dm_mathematics.pdf}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=0.49\textwidth]{fig/forgetting/size300KT/enron_emails.pdf}
        \includegraphics[width=0.49\textwidth]{fig/forgetting/gamma0.0/enron_emails.pdf}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}%
        \includegraphics[width=0.49\textwidth]{fig/forgetting/size300KT/wikipedia_en.pdf}
        \includegraphics[width=0.49\textwidth]{fig/forgetting/gamma0.0/wikipedia_en.pdf}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=0.49\textwidth]{fig/forgetting/size300KT/github.pdf}
        \includegraphics[width=0.49\textwidth]{fig/forgetting/gamma0.0/github.pdf}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=0.49\textwidth]{fig/forgetting/size300KT/pg19.pdf}
        \includegraphics[width=0.49\textwidth]{fig/forgetting/gamma0.0/pg19.pdf}
    \end{minipage}
    \caption{\textbf{Example of forgetting scaling laws for several domains.}}
    \label{fig:forgettinggrid}
\end{figure*}

\section{Ablations}

\paragraph{Learning rate.} The learning rate (\autoref{fig:lr_ablation}) is a critical parameter. If it is too low, we might fail to reach the bottom of the U-curve within a reasonable time. If it is too high, it causes the model to diverge away from the pretraining validation loss. Warmup and cosine scheduling cause modelization issues in this context. In particular, they require to know in advance the number of steps necessary for convergence, which is tricky when targeting the bottom of a U-curve. Therefore, we focus on a constant learning rate.   

\paragraph{Forgetting law.} Other laws can be tested, such as $A\frac{D^{\beta}(1-p)^{\kappa}}{N^{\alpha}}+E$ which has the appealing property of going to zero when $p=1$. However the MRE reaches $0.67\%$ for this law. 

\section{Scaling law coefficient estimation}
\label{ssec:supervised-scaling-law-coefficient-estimation}
We follow the procedure outlined in \citep{DBLP:journals/corr/abs-2203-15556,muennighoff2023scaling,besiroglu2024chinchilla}, to identify the coefficients in our scaling laws.
Restating the supervised law for convenience
\begin{equation}
	L(N,D)=
	E
	+
	\frac{A}{N^\alpha}+\frac{B}{D^\beta}.
\end{equation}
To aid numerical stability, we write this expression in log space.
First note that for $a,b>0$
\begin{align}
	\log(a+b)=\log\left(\exp\log a+ \exp\log b\right)=\mathrm{LSE}(\log a, \log b),
\end{align}
where $\mathrm{LSE}$ is the log-sum-exp operator.
We can now proceed to write the supervised scaling law in log form
\begin{align}
	\log L(N,D;A,B,E,\alpha,\beta)
	 & =
	\log \left[E
		+
	\frac{A}{N^\alpha}+\frac{B}{D^\beta}\right]                                             \\
	 & =\mathrm{LSE}\left[\log E, \log A - \alpha N, \log B - \beta D\right].
\end{align}
We make no assumptions about the relationships between the values (i.e. \emph{no parameter tying})
and optimize
\begin{align}
	(A^*,B^*,E^*,\alpha^*,\beta^*) = \argmin_{\{A,B,E,\alpha,\beta\}}\sum_i\mathrm{Huber}_\delta\left(\log L(N^{(i)},D^{(i)};A,B,E,\alpha,\beta)-L^{(i)}\right)
\end{align}
with a Huber $\delta=10^{-4}$,
where $N^{(i)}$, $D^{(i)}$ and $L^{(i)}$ are the model size, number of training tokens and loss achieved by the $i$-th run.
We fit on $125$ samples over a grid of L-BFGS-B initializations given by:
$\log A\in\{0, 3, 6, 9, 12\}$,
$\log B\in\{0, 3, 6, 9, 12\}$,
$\log E \in\{-2, -1.5, -1, 0, 0.5, 1, 1.5, 2, 2.5, 3\}$,
$\alpha\in\{0, 0.5, 1\}$,
$\beta\in\{0, 0.5, 1\}$.

We use a similar procedure to fit the different forms of the scaling laws.


