\section{Sensitivity to Model Mismatch}

\noindent In this section, we present the physical modeling of lensless imaging, and mathematically demonstrate the sensitivity of common image recovery techniques to model mismatch.
This theoretical analysis helps to explain the empirical success of previous work that apply post-processors~\cite{Monakhova:19,9239993}, PSF fine-tuning~\cite{9239993}, and PSF correction~\cite{Li:23}.
Moreover, it motivates our use of a pre-processor in minimizing the input noise amplified by the inevitable model mismatch.

\subsection{Forward Modeling}

\noindent Assuming a desired scene is comprised of point sources that are incoherent with each other, a lensless imaging system
% (without shift-invariance assumption) 
can be modeled as a linear matrix-vector multiplication with the system matrix $\bm{H}$:
\begin{align}
    \label{eq:forward_gen}
    \bm{y} = \bm{H}\bm{x} + \bm{n},
\end{align}
where $\bm{y}$ and $\bm{x}$ are the vectorized lensless measurement and scene intensity respectively, and $\bm{n}$ is the measurement noise.
Due to the highly multiplexed characteristic of lensless cameras, image recovery amounts to a large-scale deconvolution problem where the kernel $\bm{H}$ is a very dense matrix.
Each column of $\bm{H}$ is a PSF, mapping a single point in the scene to a response at the measurement plane.

As obtaining $\bm{H}$ would require an expensive calibration, the PSFs in lensless imaging are approximated as shift-invariant, \ie off-axis PSFs are assumed to be lateral shifts of the on-axis PSF.
This approximation allows $\bm{H}$ to take on a Toeplitz structure, such that the forward operation can be written as a 2D convolution with the on-axis PSF.
Using the convolution theorem, we can write the forward operation as a point-wise multiplication in the frequency domain:
\begin{align}
\label{eq:lsi_forward}
    \bm{Y} = \bm{P} \odot \bm{X} + \bm{N},
    % \bm{Y} = \bm{P} \ \bm{X} + \bm{N},
\end{align}
where $\{\bm{Y}, \bm{P}, \bm{X}, \bm{N}\} \in \mathbb{C}^{N_x \times N_y}$ are 2D Fourier transforms of the measurement, the on-axis PSF, the scene, and the noise respectively, and $\odot$ is point-wise multiplication.
The on-axis PSF can be either measured, \eg with a white LED at far-field in a dark room, or simulated if the mask structure is known~\cite{9239993,Li:23}.
As well as requiring less calibration measurements and storage, the above convolution can be computed efficiently with the fast Fourier transform (FFT).

\subsection{Consequences of Model Mismatch}
\label{sec:model_mismatch}

\noindent Whether or not we assume shift-invariance, there will be model mismatch with the true system matrix $\bm{H}$. 
Either the measurement of the on-axis PSF will be noisy, its simulation will make simplifying assumptions, or the LSI modeling is too simplistic.
In other words, the forward modeling can impact the amount of model mismatch.

In the most general case, \ie not assuming shift-invariance, we can denote our estimate system matrix as $\bm{\hat{H}}=(\bm{H}+\bm{\Delta}_H)$ where the deviation from the true system matrix is $\bm{\Delta}_H$.
Our forward model from \cref{eq:forward_gen} can then be written as:
\begin{align}
    \label{eq:mismatch_forward}
    \bm{y} = \bm{H}\bm{x} + \bm{n} = (\bm{\hat{H}} - \bm{\Delta}_H)\bm{x} + \bm{n}.
\end{align}
The quality of the sensor and the optical components can influence the amount of mismatch $\bm{\Delta}_H$ and of measurement noise $\bm{n}$.
As both are inevitable, 
image recovery approaches
yield a noisy estimate of the form:
\begin{align}
    \label{eq:mismatch_general}
    \bm{\hat{x}}^{\text{noisy}} = \bm{\hat{x}} + \underbrace{f(\bm{x}, \bm{\Delta}_H)}_{\text{model mismatch}} +  \underbrace{g(\bm{n}, \bm{\Delta}_H)}_{\text{noise amplification}},
\end{align}
where $\bm{\hat{x}}$ is the estimate when $\bm{\Delta}_H=0$ and $\bm{n}=0$, 
the model mismatch perturbation $f(\bm{x}, \bm{\Delta}_H)$ depends on the target image $\bm{x}$ and $\bm{\Delta}_H$, and the noise amplification $g(\bm{n}, \bm{\Delta}_H)$ depends on $\bm{n}$ and $\bm{\Delta}_H$.

The breakdown in \cref{eq:mismatch_general} provides insight to motivate our use of a pre-processor and the modular framework as a whole, \ie to minimize measurement noise and model mismatch before and after inevitable amplification by camera inversion. 
This motivation is further discussed in \cref{sec:modular}.
Below we demonstrate this breakdown for common image recovery approaches for lensless cameras.
Detailed derivations can be found in \cref{app:mismatch}.

\subsubsection{Direct inversion}

\noindent Assuming the system is invertible and with spectral radius $\rho(\bm{H}) < 1$, using the estimate $\bm{\hat{H}}$ for direct inversion yields~\cite{9546648,9157433}:
\begin{align}
   \label{eq:inversion_terms}
   \bm{\hat{x}} &= \bm{x} - \underbrace{\bm{H}^{-1}\bm{\Delta}_H \bm{x}}_{\text{model mismatch}} + \underbrace{(\bm{I} - \bm{H}^{-1}\bm{\Delta}_H)\bm{H}^{-1}\bm{n}}_{\text{noise amplification}} + \mathcal{O}(\| \bm{\Delta}_H\|_F^2).
\end{align}
In \cref{eq:inversion_terms}, we observe how noise and model mismatch are amplified, particularly if $\bm{H}$ is ill-conditioned as $\bm{H}^{-1}$ could be very large.

\subsubsection{Wiener filtering} From the point-wise forward model in \cref{eq:lsi_forward}, minimizing the mean squared error yields the classic Wiener filtering estimate:
\begin{align}
    \label{eq:wiener}
    % \bm{\hat{X}} = \dfrac{\bm{P}^*  \odot \bm{Y}}{ |\bm{P}|^2 + \sigma^2_{N}} =  \dfrac{\bm{P}^* \odot (\bm{P} \odot\bm{X} + \bm{N})}{ |\bm{P}|^2 + \sigma^2_{N}},
    \bm{\hat{X}} = \dfrac{\bm{P}^*  \odot \bm{Y}}{ |\bm{P}|^2 + \bm{R}} =  \dfrac{\bm{P}^* \odot (\bm{P} \odot\bm{X} + \bm{N})}{ |\bm{P}|^2 + \bm{R}},
\end{align}
where all operations are point-wise,
the noise $\bm{N}$ is assumed to be independent to $\bm{X}$, and $\bm{R}\in \mathbb{R}^{N_x \times N_y}$ is the \textit{inverse} of the SNR at each frequency.

If we use a mismatched version of the on-axis PSF's Fourier transform, \ie $\bm{\hat{P}} = (\bm{P}+ \bm{\Delta}_P)$, 
our Wiener-filtered estimate of the scene becomes:
\begin{align}
    \bm{\hat{X}}^{\text{noisy}} &= \dfrac{\bm{\hat{P}}^* \odot \bm{Y}}{ |\bm{\hat{P}}|^2 + \bm{R}} \nonumber\\
    % &= \dfrac{(\bm{P}_f + \bm{\Delta}_P)^* \bm{Y}_f}{ |(\bm{P}_f + \bm{\Delta}_P)|^2 + \sigma^2_{\bm{n}}}, \\
    % &= \dfrac{\bm{P}^* \odot \bm{Y}+ \bm{\Delta}_P^* \odot \bm{Y}}{|\bm{P}|^2 + \bm{R} + |\bm{\Delta}_P|^2 + \bm{\Delta}_P^* \odot \bm{P} + \bm{P}^* \odot \bm{\Delta}_P} \\
    &= \bm{\hat{X}} + \underbrace{\bm{M} \odot \bm{P} \odot  \bm{X}}_{\text{model mismatch}} + \underbrace{\bm{M} \odot \bm{N}}_{\text{noise amplification}}, \label{eq:noisy_wiener}
\end{align}
where:
\begin{align}
\bm{M} &= \dfrac{\bm{\Delta}_P^*}{\bm{B}} - \dfrac{\bm{\Delta}_B \odot (\bm{P}^* + \bm{\Delta}_P^*)}{\bm{B}^2 + \bm{B} \odot \bm{\Delta}_B}, \\
\bm{B} &= |\bm{P}|^2 + \bm{R},\\
\bm{\Delta}_B &= |\bm{\Delta}_P|^2 + \bm{\Delta}_P^* \odot \bm{P} + \bm{P}^*\odot\bm{\Delta}_P.
\end{align}
While Wiener filtering avoids adverse amplification, \ie with $\bm{H}^{-1}$ as in direct inversion,
model mismatch still leads to similar error terms as shown in \cref{eq:mismatch_general}.

\subsubsection{Iterative solvers} 
\label{sec:error_iterative}

A common approach to avoid amplification with $\bm{H}^{-1}$ is to cast image recovery as a regularized optimization problem:
\begin{align}
\label{eq:opt_gen}
   \bm{\hat{x}} = \arg \min_{\bm{x}} \frac{1}{2} ||\bm{H}\bm{x} - \bm{y}||_2^2 + \lambda \mathcal{R}(\bm{x}),
   % \\
   % \bm{\hat{X}} = \arg \min_{\bm{X}\geq0} \frac{1}{2} ||\bm{Y} - \bm{C}(\bm{P}\ast \bm{X})||_2^2 + \lambda \mathcal{R}(\bm{X})
\end{align}
where $\mathcal{R}(\cdot)$ is a regularization function on the estimate image.
In lensless imaging, it is common to apply non-negativity and sparsity constraints in the TV space~\cite{Antipa:18,phlatcam}.
When the regularization function uses the $l_1$ norm, an iterative solver is needed to optimize \cref{eq:opt_gen}.
A common approach is ADMM, for which we can obtain a similar decomposition of model mismatch and noise amplification at \textit{each iteration}.
Zeng \etal~\cite{9546648} show how model mismatch leads to an accumulation of mismatch errors over multiple ADMM iterations, but they do not show noise amplification.
From Eq.~15 of~\cite{9546648}, by expanding the terms from the previous iteration ($\bm{\epsilon}^{(k-1)}$)  that depend on the model mismatch, we obtain:
\begin{align}
&\bm{\hat{x}}^{(k),\text{noisy}} = \bm{\hat{x}}^{(k)}  + \underbrace{\bm{W}_4 \bm{W}_2\bm{C}^T \bm{n}}_{\text{noise amplification}} \nonumber \\
\label{eq:admm_mismatch} 
&\underbrace{+ \bm{W}_1^{-1}\rho_x \delta_{\bm{H}} \bm{\hat{x}}^{(k)} + \bm{W}_4 \bm{W}_2\left(\bm{C}^T\bm{C} \bm{H}\bm{x} + \bm{\gamma}^{(k-1)} \right) + \bm{W}_4\bm{W}_3}_{\text{model mismatch}}, 
\end{align}
where:
\begin{align}
\delta_{\bm{H}} &= \left( \bm{\Delta}_H^T\bm{H} + \bm{\hat{H}}^T \bm{\Delta}_H \right),\\
\bm{W}_1 &= \rho_x \bm{\hat{H}}^T \bm{\hat{H}} + \rho_z \bm{C}^T\bm{C} + \rho_y \bm{I}, \\
\bm{W}_2 &= (\bm{W}_1 + \rho_x \delta_{\bm{H}})^{-1} \Delta_{\bm{H}}^T \rho_x (\bm{C}^T\bm{C} + \rho_x \bm{I})^{-1}, \\
\bm{W}_3 &= \left( \bm{W}_1 + \rho_x \delta_{\bm{H}} \right)^{-1} \bm{\hat{H}}^T \rho_x^2 \Delta_{\bm{H}} \bm{\hat{x}}^{(k-1)},\\
\bm{W}_4 &= (\bm{I} + \bm{W}_1^{-1} \rho_x \delta_{\bm{H}}),
\end{align}
$\{\rho_x, \rho_y, \rho_z\}$ are positive penalty parameters, $\bm{C}$ crops the image to the sensor size~\cite{Antipa:18}, and $\bm{\gamma}^{(k-1)}$ contains terms from the previous iterations that do not depend on model mismatch.
Similar to Wiener filtering, while there is no amplification with $\bm{H}^{-1}$,
model mismatch leads to noise amplification and error terms,
as shown in \cref{eq:mismatch_general},
at each iteration.

\section{Methodology}
\label{sec:methodology}

\subsection{Modular Reconstruction}
\label{sec:modular}

\noindent As shown in \cref{sec:model_mismatch}, there are typically two noise sources in lensless imaging: (1) at measurement~$\bm{n}$ and (2) model mismatch $\bm{\Delta}_H$.
Our modular reconstruction pipeline, 
as shown in~\cref{fig:pipeline},
can address these perturbations and their consequences
% ,as summarized by \cref{eq:mismatch_general},
for multiple camera inversion approaches.
While previous work has proposed various lensless recovery approaches that jointly train camera inversion with post-processors~\cite{Monakhova:19, 9239993},
they do not address noise amplification by camera inversion, nor do they theoretically motivate the use of a post-processor 
(apart from~\cite{9546648} for ADMM).

One of our contributions is to introduce a pre-processor to minimize the inevitably-amplified noise,
as shown with $g(\bm{n}, \bm{\Delta}_H)$ in \cref{eq:mismatch_general}.
While simply using a post-processor could address $g(\bm{n}, \bm{\Delta}_H)$, a low SNR may result in a poor camera inversion output, which can be challenging for post-processing alone to effectively address. 
Similarly, if noise amplification is non-linear or leads to clipping, post-processing alone may struggle.
Therefore, pre-processing can alleviate the task of the post-processor by denoising \textit{prior} to camera inversion,
such that the latter component's output is easier for the post-processor to enhance.
In our experiments, we evaluate on both low and high SNRs to demonstrate the added benefit from the pre-processor.

Moreover, the insight from \cref{sec:model_mismatch} better motivates the design choices of previous work, which have otherwise relied on experimental results to support their design choices.
Firstly, learned camera inversions that attempt to reduce model mismatch~\cite{9239993,Li:23,Kingshott:22} can reduce both perturbation terms in \cref{eq:mismatch_general},
and like the pre-processor, reduce the effort needed by the post-processor in treating the camera inversion output.
In our modular framework shown in~\cref{fig:pipeline}, we incorporate a PSF correction component to reduce model mismatch in the PSF prior to camera inversion. Moreover, while pre-processing and PSF correction can be incorporated to directly address $\bm{n}$ and $\bm{\Delta}_H$,
there will be residual error.
The post-processor can address the now simpler denoising task, while also performing perceptual enhancements.

To train our modular reconstruction and to demonstrate the effectiveness of our proposed pre-processor,
we need a sufficient amount of data.
Another one of our contributions is collecting and open-sourcing four lensless datasets,
which use a variety of masks/PSFs to demonstrate the effectiveness for different imaging systems.
These datasets are summarized in \cref{tab:datasets} and are further explained in \cref{sec:experiments}.
We also open-source the tooling for others to more conveniently collect and share their own datasets.\footnote{\href{https://lensless.readthedocs.io/en/latest/measurement.html}{lensless.readthedocs.io/en/latest/measurement.html}}

\subsubsection{Pre- and Post-Processor Design}
\label{sec:processors}

From a single measurement, it is difficult to obtain meaningful information about $\bm{\Delta}_H$ and/or $\bm{n}$ to design the pre- and post-processors.
Moreover, 
as shown in \cref{sec:model_mismatch}, 
each inversion approach amplifies the input noise in a unique manner.
To this end, our solution is to train both processors and the camera inversion end-to-end, such that the appropriate processing can be learned from measurements rather than heuristically-designed processing.
As each camera inversion approach results in a unique amplification of the model mismatch and noise,
the learned pre- and post-processors are trained for a specific inversion approach,
\ie their ability to transfer between camera inversion approaches is not guaranteed. 
% \textbf{TODO exp for this?}

Similar to previous work, we use a loss function that is a sum of the mean-squared error (MSE) and a perceptual loss between the reconstruction output $\bm{\hat{x}}$ and the ground-truth $\bm{x}$:
\begin{equation}
    \label{eq:loss_mse_lpips}
    \mathscr{L}\left(\bm{x},\bm{\hat{x}}\right) = \mathscr{L}_{\text{MSE}}\left(\bm{x},\bm{\hat{x}}\right) + \mathscr{L}_{\text{LPIPS}}\left(\bm{x},\bm{\hat{x}}\right).
\end{equation}
We use the Learned Perceptual Image Patch Similarity (LPIPS) metric~\cite{zhang2018perceptual} as a perceptual loss, which promotes photo-realistic images at a patch level, rather than pixel-wise as MSE.

For the pre- and post-processors in \cref{fig:pipeline},
we use a denoising residual U-Net (DRUNet) architecture  shown to be very effective for denoising, deblurring, and super-resolution tasks~\cite{zhang2021plug}.
The DRUNet architecture is presented in \cref{sec:drunet}.
Transformers~\cite{Zamir2021Restormer,Pan:22} or diffusion models~\cite{cai2024phocolens} could also be applied as pre- and post-processors,
but this work concentrates on DRUNet as it is does not require many parameters (unlike transformers) nor many iteration steps (as diffusion models).


\begin{table}[t!]
\renewcommand{\arraystretch}{1.3}
\caption{Comparison of trainable camera inversion approaches.}
\label{tab:compare_inv}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{c||c|c|c}
\hline
& \makecell{Noisy estimate\\due to mismatch} & \makecell{\# trainable\\parameters} & \makecell{PSF\\correction} \\
\hline\hline
Unrolled ADMM~\cite{Monakhova:19}  &  \makecell{\cref{eq:admm_mismatch}} & $10^1 - 10^2$ & No \\
\hline
\makecell{Trainable inversion~\cite{9239993}}  & 
\makecell{\cref{eq:inversion_terms}} & $10^4 - 10^6$ & Yes \\
\hline
\makecell{Unrolled ADMM\\with model-mismatch\\compensation network~\cite{9546648}}    &  \makecell{\cref{eq:admm_mismatch}} & $10^6-10^7$  & No \\
\hline
\makecell{Multi-Wiener deconvolution\\ network (MWDN)~\cite{Li:23}}  & 
\makecell{\cref{eq:noisy_wiener}} & $10^6-10^7$ & Optional \\
\hline
\end{tabular}
}
\end{table}

\subsubsection{Camera Inversion Approaches}


\noindent We investigate four trainable camera inversion approaches proposed by previous work: unrolled ADMM~\cite{Monakhova:19}, trainable inversion~\cite{9239993}, unrolled ADMM with a model mismatch compensation network (MMCN)~\cite{9546648},
and multi-Wiener deconvolution network (MWDN)~\cite{Li:23}.
The architecture of all camera inversion approaches are visualized in \cref{app:inversion},
and their characteristics are compared in \cref{tab:compare_inv}.

As shown in \cref{fig:pipeline},
the input to camera inversion is either the raw lensless measurement or the output of a pre-processor,
while the output of camera inversion can be optionally fed to a post-processor.
For promoting measurement consistency,
the camera inversion can take as input the on-axis PSF,
which can be fine-tuned~\cite{9239993} or corrected with neural networks~\cite{Li:23}.
Fine-tuning may be preferable if reconstruction is only expected for a single PSF/imaging system,
but the learned adjustments cannot be transferred if there are changes in the imaging system,
in which case a correction network may be preferable.
As we investigate the transferability of learned components, 
we optionally add a DRUNet for correcting the PSF.

\subsection{DigiCam: Hardware and Modeling}
\label{sec:digicam}

\noindent For our generalizability experiments, we propose a programmable-mask system entitled \textit{DigiCam}. 
As a programmable mask, we use a low-cost LCD~\cite{adafruitlcd}.
While previous work with LCDs requires multiple measurements~\cite{4472247,huang2013}
or only uses a few LCD pixels to simply control the aperture~\cite{zomet2006},
we are the first to apply an LCD for single-shot lensless imaging.
Moreover, LCDs are significantly cheaper than SLMs: our component is around $150\times$ cheaper than the SLM used in~\cite{sweepcam2020,zheng2021programmable3dcam}.
A reconfigurable system is an extremely convenient way to experimentally evaluate generalizability to different PSFs, as the mask pattern can be simply reprogrammed to have an imaging system with a different PSF.
Moreover, as the mask structure is known, the PSF can be simulated for calibration-free imaging (after an initial alignment).
While a programmable mask cannot represent all possible lensless imaging PSFs, it can provide useful insight into the generalizability of learned reconstruction approaches.
Below we describe the hardware, and detail the wave-propagation modeling needed for simulating the PSF.
More modeling details can be found in \cref{app:psf_modeling,app:mask_modeling}.


\subsubsection{Hardware Prototype}



\noindent A programmable mask serves as the only optical component, 
specifically an off-the-shelf LCD driven by the ST7735R device,
which can be purchased for $20$ USD~\cite{adafruitlcd}.
% but this was not an explicit choice on our part.
The LCD component was selected because it has a higher spatial resolution than other off-the-shelf LCDs,
% (\ie \SI{0.06}{\milli\meter}$\times$\SI{0.18}{\milli\meter} per sub-pixel),
and has a Python API to set pixel values.
The LCD has an interleaved pattern of red, blue, and green sub-pixels, 
%as shown in \cref{fig:pixel_layout},
but a monochrome programmable mask with sufficient spatial resolution could also be used.
Our experimental prototype can be seen in~\cref{fig:prototype_labeled}.
The LCD is wired to a Raspberry Pi (RPi) ($35$~USD) with the RPi High Quality (HQ) $12.3$ MP Camera~\cite{rpi_hq} ($50$~USD) as a sensor,
totaling our design to just $105$~USD. 
This is significantly cheaper than other programmable mask-based prototypes that make use of an SLM~\cite{sweepcam2020,zheng2021programmable3dcam}, 
which can cost a few thousand USD.
Our prototype includes an optional stepper motor for programmatically setting the distance between the LCD and the sensor.

\begin{figure}[t!]
		\centering
		\includegraphics[width=0.7\linewidth]{figs/fig2_setup.png}
	\caption{DigiCam prototype and measurement setup.}
	\label{fig:prototype_labeled}
\end{figure}

\subsubsection{Wave-Based Modeling}

\noindent Accurately simulating the PSF is crucial for the reconstruction quality,
and to minimize model mismatch and its consequences.
One advantage of using a programmable mask is that it has a well-defined structure, 
which allows us to model propagation through the programmable mask to simulate the PSF.
A simulation based on wave optics (as opposed to ray optics) may be necessary to account for diffraction due to the small apertures of the mask and for wavelength-dependent propagation. 
The Fresnel number $ N_F $ can be used to determine whether a wave-optics simulation is necessary, with ray optics generally requiring $N_F\gg 1$~\cite{boominathan2022recent}.
The Fresnel number is given by $ N_F = a^2 / d\lambda $, where $ a $ is the size of the mask's open apertures, $ d $ the propagation distance, and $ \lambda $ the wavelength. 
For our prototype, $ a = \SI{0.06}{\milli\meter} $, $ d = \SI{2}{\milli\meter}$ between the mask and sensor, and $ \lambda \in [\SI{450}{\nano\meter}, \SI{750}{\nano\meter}] $ (visible light),
such that $ N_F \in [2.4, 4] $. 
As $N_F$ is not significantly greater than one, diffraction effects need to be accounted for.

We model the PSF similar to~\cite{sitzmann2018e2e}, \ie as spherical waves up to the optical element followed by free-space propagation to the sensor.
The point-source wave field at the sensor for a given wavelength $\lambda$
can be written as:
\begin{align}
	\label{eq:wavefield}
	&u(\bm{r}; d_1, d_2, \lambda) = \nonumber \\ 
	&\mathcal{F}^{-1}\Big(\mathcal{F} \Big( m(\bm{r}; \lambda) \underbrace{e^{j \frac{2\pi}{\lambda} \sqrt{\|\bm{r}\|_2^2 +  d_1^2}}}_{\text{spherical waves}}
	\Big) \times h(\bm{u}; z=d_2, \lambda) \Big),
	% &p(\bm{r}; d_1, d_2, c) = \nonumber \\ 
	% &\Big|\mathcal{F}^{-1}\Big(\mathcal{F} \Big( m(\bm{r}; \lambda_c) \underbrace{e^{j \frac{2\pi}{\lambda_c} \sqrt{\|\bm{r}\|_2^2 +  d_1^2}}}_{\text{spherical waves}}
	% \Big) \times h(\bm{u}; z=d_2, \lambda_c) \Big)\Big|^2,
\end{align}
where 
$d_1$ is the distance from the point source to the optical element,
$d_2$ is the distance from the optical element to the sensor,
$ h(\bm{u}; z, \lambda)$ is the free-space propagation frequency response, and
$\bm{u} \in \mathbb{R}^2$ are the spatial frequencies of $\bm{r} \in \mathbb{R}^2$.
For the free-space propagation kernel, we use bandlimited angular spectrum (BLAS)~\cite{Matsushima2009}.

As the illumination is incoherent, PSFs from different scene points will add in intensity at the sensor~\cite{Goodman2004}.
Therefore, we take the squared amplitude of \cref{eq:wavefield} for the intensity PSF~\cite{Goodman2004}.
% in \cref{eq:lsi_forward}.
% \begin{equation}
	% 	p(\bm{r}; d_1, d_2, \lambda) = | u_2(\bm{r}; d_1, d_2, \lambda)|^2,
	% \end{equation}
% where $u_2(\bm{r}; d_1, d_2, \lambda)$ is defined in \cref{eq:wavefield}.
As our sensor measures RGB, the PSF of each color channel $c \in \{R,G,B\}$ should account for its wavelength sensitivity.
% $\kappa_c$:
% \begin{equation}
% 	\label{eq:intensity_psf_main}
% 	p(\bm{r}; d_1, d_2, c) = \int | u_2(\bm{r}; d_1, d_2, \lambda)|^2 \hspace{0.1cm} \kappa_c(\lambda) \hspace{0.1cm} d\lambda,
% \end{equation}
% where $u_2(\bm{r}; d_1, d_2, \lambda)$ is defined in \cref{eq:wavefield}.
% Sitzmann et. al.: https://github.com/vsitzmann/deepoptics/blob/defbb975309a6a3f3d2a86b92e82d02156ab213e/src/aedof_diffractive.py#L111
% waveprop: https://github.com/ebezzam/waveprop/blob/a2d65116336bfb6e95732fd982e5c3ec2109cff3/waveprop/color.py#L110
Similar to~\cite{sitzmann2018e2e}, we assume a narrowband around the RGB wavelengths, and compute the PSFs for $ c\in\{R,G,B\} $ for the respective wavelengths of (\SI{640}{\nano\meter}, \SI{550}{\nano\meter}, \SI{460}{\nano\meter}):
\begin{equation}
	\label{eq:intensity_psf_simple}
	p(\bm{r}; d_1, d_2, c) = | u(\bm{r}; d_1, d_2, \lambda_c)|^2.
\end{equation}
\cref{eq:wavefield} defines the response for an arbitrary optical encoder $m(\bm{r}; \lambda)$.
A programmable mask like that of \textit{DigiCam} can be modeled as a superposition of apertures for each adjustable RGB sub-pixel in $ \bm{r} \in \mathbb{R}^2 $:
\begin{align}
	\label{eq:mask_simple}
	m(\bm{r}; \lambda=\lambda_c) &= \sum_{k_c \in K_c}
	% w_{k_c}(\lambda) 
	w_{k_c}
	a(\bm{r} -\bm{r}_{k_c}), \quad c\in\{R,G,B\},
\end{align}
where we again assume a narrowband around the RGB wavelengths,
$K_c$ is the set of pixel corresponding to the color filter $c$,
$w_{k_c} \in [0, 1]$ are the mask weights for each sub-pixel, $ \{(\bm{r}_{k_c})\}_{k_c=1}^{K_c} $ are the centers of each sub-pixel, 
and the aperture function $a(\cdot)$ is modeled as a rectangle of size $\SI{0.06}{\milli\meter}\times\SI{0.18}{\milli\meter}$ (the dimensions of each sub-pixel).
\cref{eq:mask_simple} accounts for the mask \textit{deadspace} (occluding regions that are not controllable due to circuitry) and \textit{pixel pitch} (distance between pixels) by setting the appropriate centers $ \{(\bm{r}_{k_c})\}_{k_c=1}^{K_c} $.
An alternative approach to account for pixel pitch is to modify the wave propagation model to include higher-order diffraction and attenuation~\cite{Gopakumar:21}, but this approach does not account for the deadspace.

\newcommand{\figsizepsf}{0.21}
\newcommand{\figsizepsfdigi}{0.21}
\begin{figure*}[t!]
 % gamma 1.8
    \centering
    \begin{subfigure}{\figsizepsf\linewidth}
		\centering
		\includegraphics[width=0.99\linewidth]{figs/fig3_diffusercam_psf.png} 
		\caption{DiffuserCam~\cite{Monakhova:19} \textit{(meas)}.}
		\label{fig:diffusercam}
	\end{subfigure}
    \begin{subfigure}{\figsizepsf\linewidth}
		\centering
		\includegraphics[width=0.99\linewidth]{figs/fig3_tapecam_psf.png} 
		\caption{TapeCam \textit{(meas)}.}
		\label{fig:tapecam}
	\end{subfigure}
 \begin{subfigure}{\figsizepsf\linewidth}
		\centering
		\includegraphics[width=0.99\linewidth]{figs/fig3_digicam_celeba.png} 
		\caption{DigiCam-CelebA \textit{(sim)}.}
		\label{fig:digicam_celeba}
	\end{subfigure}
 \begin{subfigure}{\figsizepsf\linewidth}
		\centering
		% \includegraphics[width=0.99\linewidth]{figs/digicam_psf_waveprop_deadspace.png} 
  \includegraphics[width=0.99\linewidth]{figs/fig3_DigiCam-Mirflickr-SingleMask-25K_psf.png} 
		\caption{DigiCam-Single \textit{(sim)}.}
		\label{fig:digicam_mirflickr}
	\end{subfigure}
 \\[5pt]
 \begin{subfigure}{\figsizepsfdigi\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/fig3_DigiCam-Mirflickr-MultiMask-25K_psf_1.png} 
		\caption{Seed=1 \textit{(sim)}.}
		\label{fig:multi1}
	\end{subfigure}
	\begin{subfigure}{\figsizepsfdigi\linewidth}
		\centering 
  \includegraphics[width=\linewidth]{figs/fig3_DigiCam-Mirflickr-MultiMask-25K_psf_2.png} 
		\caption{Seed=2 \textit{(sim)}.}
		\label{fig:multi2}
	\end{subfigure}
    \begin{subfigure}{\figsizepsfdigi\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/fig3_DigiCam-Mirflickr-MultiMask-25K_psf_3.png} 
		\caption{Seed=3 \textit{(sim)}.}
		\label{fig:multi3}
	\end{subfigure}
    \begin{subfigure}{\figsizepsfdigi\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/fig3_DigiCam-Mirflickr-MultiMask-25K_psf_4.png} 
		\caption{Seed=4 \textit{(sim)}.}
		\label{fig:multi4}
	\end{subfigure}
	\caption{Point spread functions of datasets used in this work, where \textit{(meas)} refers to a measured PSF and \textit{(sim)} refers to simulated. \cref{fig:multi1,fig:multi2,fig:multi3,fig:multi4} are four of 100 simulated PSFs of the mask patterns used in measuring the \textit{DigiCam-Multi} dataset.}
	\label{fig:compared_psfs_crop}
\end{figure*}

\cref{fig:digicam_mirflickr,fig:multi1,fig:multi2,fig:multi3,fig:multi4} show simulated \textit{DigiCam} PSFs.
In \cref{app:compare_psf}, a measured PSF is compared with various simulation approaches with regards to reconstruction performance.
Wave-based propagation and programmable-mask modeling (with PyTorch support) is made available in \textit{waveprop}~\cite{waveprop}.\footnote{\href{https://github.com/ebezzam/waveprop}{github.com/ebezzam/waveprop}}



\subsection{Improving Generalizability}
\label{sec:improve_gen}

\noindent Learned reconstructions for lensless imaging face generalizability issues because they are typically not exposed to measurements and PSFs from different systems during training.
With our \textit{DigiCam} system, we can conveniently collect measurements from multiple mask patterns by programmatically setting the LCD, and can use \cref{eq:intensity_psf_simple,eq:mask_simple} to simulate the corresponding PSF. 
Consequently, a multi-mask dataset can be collected to train a reconstruction approach that generalizes to unseen \textit{DigiCam} patterns.
Our modular reconstruction, as shown in \cref{fig:pipeline}, can learn pre-processing, PSF correction, and post-processing that generalizes to measurements from unseen mask patterns.

Whether or not a programmable-mask system is used, transfer learning can be applied between lensless imaging systems.
This can be done by fine-tuning a learned reconstruction (trained with real measurements with one system) on simulations with a \textit{new system's} PSF, \ie by convolving ground-truth data with the new PSF.
While this requires training with the new PSF, it can avoid the need to collect a dataset which may not be possible.
Moreover, we can exploit modular components that have been trained with real measurements by other imaging systems, such as the pre-processor.
Training on simulations may not always generalize to real measurements.
It depends on the validity of the modeling assumptions, \eg the validity of LSI in \cref{eq:lsi_forward} for a wide-enough FOV.
If this width of LSI validity is too narrow or if there are significant differences in coloring, training with simulated data may not generalize to measured data.
