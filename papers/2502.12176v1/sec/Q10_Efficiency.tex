\section{Problem 10: How to to improve the efficiency in FedFMs?}

%========================Section========================%
\subsection{Defining efficiency of FedFMs}
Borrowing the definitions in \cite{yao2024federated}, the efficiency of FedFMs corresponds to the three fundamental aspects of computation, communication, and storage.
Thus, the efficiency of FedFMs refers to its capability to minimize the weighted sum of these three factors, facilitating scalable and high-performance training in resource-constrained environments \cite{chen2021communication, ren2024advances}. Challenges such as high update frequencies, large model sizes, and network unreliability hinder scalability, while the resource demands of complex FMs exacerbate the strain on devices. Therefore, improving computation, communication, and storage efficiency is vital for enabling FedFMs' deployment in sectors like healthcare, finance, and IoT. 

\textbf{Problem Formulation.} This section considers three types of efficiency in FedFMs \cite{mcmahan2017communication,wang2021resource, hu2021mhat}: the computation loss, communication  loss, and model size for model $w$ at time slot $t$ as $\tilde \ell_e^t(w)$, $\hat \ell_e^t(w)$, and $\bar \ell_e^t(w)$, respectively. The objective is to minimize the overall efficiency consumption, expressed as:
\begin{equation}
\label{eq:efficiency_FM}
\begin{split}
    \min_w\sum_{t=0}^{T}&\alpha \underbrace{\tilde \ell_e^t(w)}_{\text{computation loss}}+\beta \underbrace{\hat \ell_e^t(w)}_{\text{communication loss}} + \gamma  \underbrace{\bar\ell_e^t(w)}_{\text{model size}}, \\
    &s.t. \quad \ell_{p,k}(w) <\delta_k,
    \end{split}
\end{equation}
where $T$ is training episodes; $\alpha,\beta,\gamma$ are the respective weights reflecting the relative importance of each efficiency component and meet $\alpha+\beta+\gamma=1$. This formulation ensures a balanced trade-off among the three efficiency dimensions, optimizing FedFMs for real-world, resource-constrained environments.





%========================Section========================%
\subsection{Existing Methods}

A wide range of methods has been proposed to enhance the efficiency of FedFMs. Based on the key elements of FedFMs, we categorize the existing literature into a taxonomy comprising four main categories: \textit{data-oriented methods}, \textit{model-oriented methods}, \textit{device-oriented methods}, and \textit{communication-oriented methods}, as shown in Fig. \ref{fig:efficiency_fm}.

\begin{figure}[t]
    \centering
     \includegraphics[width=0.4\textwidth]{figures/10_efficiency_FM.png}
     \caption{Summary of Efficiency in FedFMs}
     \label{fig:efficiency_fm}
     \vspace{-5mm}
 \end{figure}

%========================Section========================%
\subsubsection{Data-oriented Methods}

Data-oriented methods emphasize the importance of local data quality and structure in improving the efficiency of local FMs \cite{ye2024openfedllm, qin2024federated}. \textbf{Data selection} is essential for improving the efficiency of FedFMs pre-training and fine-tuning, which selects efficient datasets to reduce training FLOPs \cite{yao2022nlp, fawcett2024improving}. \textbf{Prompt compression} optimizes how FMs interact with prompts to generate desired outputs \cite{che2023federated}. Prompt compression accelerates FedFMs input processing by either condensing lengthy prompts or learning compact representations of prompts \cite{mu2024learning}.

%========================Section========================%
\subsubsection{Model-oriented Methods}

 
Model-oriented methods focus on reducing the sizes and the number of arithmetic operations of FMs, i.e., model compression. \textbf{Quantization} transforms model parameters into lower-precision formats, such as fixed-point or reduced-bit representations (e.g., 8-bit or 4-bit), which can significantly reduce the data payload with minimal loss of model accuracy \cite{ren2023two, cao2024fedmq, mao2022communication}. 
\textbf{Parameter pruning} compresses FMs by removing redundant or less important model weights $W$, which can be categorized into structured pruning and unstructured pruning \cite{fan2024data,huang2024fedmef}.
\textbf{Knowledge distillation} trains a smaller “student” model to approximate the performance of a larger “teacher” model, thereby conserving computational resources and minimizing communication overhead \cite{ li2019fedmd}.
\textbf{Low-rank approximation:} Low-rank approximation compresses FMs by approximating the weight matrix with smaller low-rank matrices \cite{song2024low}.%, as illustrated in Fig. \ref{fig:model_oriented}(d). 
This decomposition effectively reduces the number of parameters and enhances computational efficiency.


%========================Section========================%
\subsubsection{Device-oriented Methods}

Device-oriented methods focus on optimizing the efficiency of FedFMs by tailoring strategies to the specific capabilities and constraints of client devices \cite{hou2020dynabert,laskaridis2024melting}.
\textbf{Resource-aware scheduling} enhances the efficient use of computing resources on client devices by integrating advanced scheduling policies into model training and communication processes \cite{hou2020dynabert,luo2021info,luo2022info}. 
\textbf{Framework optimization} focuses on lightweight libraries designed for local device constraints, enabling efficient deployment on devices with limited resources \cite{niu2024smartmem, kwon2023efficient}.

%========================Section========================%
\subsubsection{Communication-oriented Methods}

Communication-oriented methods aim to reduce the volume of parameters uploaded from clients to the server during the aggregation phase while maintaining model performance \cite{wang2021resource, hu2021mhat}. For instance, \citet{wang2021resource} introduced Resource-efficient FL with Hierarchical Aggregation (RFL-HA) to mitigate high training time and communication overhead in FL, which organizes edge nodes into clusters to reduce data transmission. Furthermore, \citet{zheng2022aggregation} presented an FL system that enhances privacy by enabling clients to share obscured model updates via lightweight encryption. Additionally, \citet{al2023decentralized} eliminated the need for a central aggregator through a decentralized FL approach that leverages device-to-device communication and overlapped clustering, significantly reducing communication costs. 




\subsection{Challenges and Potential Solutions}

FedFMs face \textit{communication bottlenecks} due to constant synchronization and model aggregation, leading to inefficiency, backhaul bottlenecks, and high latency. Additionally, there is an \textit{efficiency-performance-privacy trade-off}, as efficiency-preserving methods like HE often reduce computational efficiency, making it challenging to balance privacy and model performance. Furthermore, \textit{task redundancy} in FedFMs results in significant computational overhead from repetitive computations by multiple clients.

To address these challenges, potential solutions include: (1) \textit{Compression}: Exploring ways to minimize communication through adaptive aggregation frequencies \cite{lee2023layer}, efficient compression techniques, and advanced communication methods; (2) \textit{Hybrid Privacy-Preserving Techniques}: Developing advanced HE schemes and novel hybrid techniques to ensure efficiency and strong data privacy \cite{254465}; (3) \textit{Disentangling Tasks}: Researching methods to disentangle complex tasks and effectively assign subtasks to different clients under server coordination, such as dependent task offloading \cite{wang2021dependent}, to enhance overall efficiency.



