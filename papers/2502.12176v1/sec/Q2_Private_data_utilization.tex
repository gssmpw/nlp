\section{Problem 2: How to Utilize Private Data in FedFMs?}

\subsection{Defining the Utilization of Private data in FedFMs}
Private data utilization in FedFMs enhances FMs by leveraging local private data while preserving privacy, as shown in Fig.~\ref{fig:UtilPrivateData}. In real-world scenarios, valuable private data is often isolated within organizations due to privacy regulations. For example, medical institutions maintain separate patient data repositories, creating “data islands” that limit training diversity. Traditional centralized training is impractical or restricted due to privacy concerns, which particularly affects FMs that rely on extensive, diverse datasets for robust performance. FedFMs overcome these challenges by allowing organizations to train models locally, sharing only model updates instead of raw data with a central server for aggregation. This approach effectively bridges data islands while ensuring strong privacy protections, addressing both data scarcity and regulatory constraints in FM development.


\textbf{Problem Formulation.} The utilization of private data in FedFMs involves clients contributing their private data for the model's training while preserving data privacy \cite{mcmahan2017communication,yang2019federated, 6-data-datasetgrouper}.
Consider a scenario where a central aggregation server and $K$ clients collaborate to learn a global FM. Each client is in a data-isolated environment, meaning that data is not shared among them. Let the $k$-th client possess data $D_k$. 
The optimization problem for obtaining the model $w_g$ with respect to data utilization can be formulated as:
\begin{equation}
    \begin{split}
       &\qquad \min_{w_g}\ell_u(w_g) \\
       & s.t.\quad \ell_{p,k}(w_g)<\delta_k, \text{for }k=1,\cdots,K
    \end{split}
\end{equation}
where $\ell_u(w_g) = F\Big(\ell_{u,1}(w_1), \cdots, {\ell}_{u,K}(w_K)\Big)$, $\ell_{u,k}(\cdot)$ represents the loss function related to data utilization for client $k$ and $F$ is the aggregation mechanism, such as FedAvg \cite{mcmahan2017communication}.  Additionally, $\ell_{p,k}(w)$ denotes the privacy leakage of client $k$ with respect to the model $w$, and it must be within a predefined threshold $\delta_k$.

\looseness=-1



\begin{figure}[]
    \centering
\includegraphics[width=1\linewidth]{figures/6-Private.png}
    \caption{FedFMs utilize locally stored private data from organizations (toB) or client devices (toC) to train models while ensuring privacy, enabling applications like disease diagnosis and health analysis \cite{6-data-fedcampus}. }
    \label{fig:UtilPrivateData}
    \vspace{-5mm}
\end{figure}


\subsection{Existing Methods}
Existing methods for utilizing private data in FedFMs fall into two methods: \textit{data-centric} and \textit{model-centric}. Data-centric methods \textit{enhance distributed private data utility} while ensuring privacy, addressing data quality, efficiency, and performance. In contrast, model-centric methods adapt FM architectures and training paradigms to \textit{effectively leverage private data}, focusing on instruction tuning and model splitting.

\subsubsection{Data-Centric Methods } 


\textbf{Data Quality Enhancement}. Distributed local privacy-protected data is often non-IID, varying in type, labels, and quality across clients, posing challenges for foundation models that require diverse, high-quality data. To address data imbalance, differential privacy-based synthetic data generation enhances utilization by creating balanced, privacy-preserving synthetic samples~\cite{6-data-dpsda}. Quality standardization mechanisms, such as training data scoring and global thresholds, ensure consistent data quality across clients~\cite{zhao2024enhancing}.\looseness=-1


\textbf{Computational Efficiency Enhancement}. 
Compared to centralized training, the non-IID nature of local privacy-protected data often increases communication rounds in federated learning, reducing training efficiency. To address this, FedHDS~\cite{qin2023federated} employs intelligent data selection to identify representative samples, reducing redundancy while preserving model quality. FedBPT~\cite{pmlr-v235-sun24j} leverages black-box APIs and gradient-free optimization to minimize data requirements without compromising performance.


\textbf{Privacy Risks Protection}.  
FMs remain vulnerable to privacy attacks, such as training data leakage, model parameter leakage, and architecture disclosure, which limit data diversity and scale. To address these risks, defensive frameworks like RoseAgg~\cite{6-data-roseagg} enhance protection against targeted attacks, including collusion and poisoning in federated settings. Practical implementations include FedFMS~\cite{6-data-fedfms} and FedKIM~\cite{6-data-fedkim}, which adapt foundation models for privacy-preserving medical applications.  FedCampus~\cite{6-data-fedcampus} and Fedkit~\cite{Fedkit}  provide comprehensive privacy solutions for smart campus environments.

\textbf{Balancing Global and Personalized Performance}. 
Effective private data utilization in FedFMs requires balancing global knowledge with personalized client insights, particularly when private data exhibits distinct patterns. For instance, in healthcare, integrating patient-specific data like genetics and lifestyle factors enhances diagnostic accuracy and personalized treatment \cite{6-data-health}, while global data provides broader coverage and general accuracy. Recent research has developed methods to optimize this trade-off: FedDPA~\cite{6-data-feddpa} uses specialized adapters for simultaneous global and local learning, and ZooPFL~\cite{lu2023zoopfl} employs zeroth-order optimization and client-specific embeddings for adaptation.

\subsubsection{Model-Centric Methods}


\textbf{Instruction Tuning}.
Traditional instruction tuning methods face challenges in federated settings due to the difficulty of obtaining high-quality, privately distributed instruction data. Recent solutions include: FedIT~\cite{zhang2024fedpit}, which leverages local device instructions and transfers multimodal knowledge at feature and decision levels to enhance adaptability; and OpenFedLLM~\cite{ye2024openfedllm}, which integrates instruction tuning and value alignment, demonstrating federated learning's superiority over local training across domains while preserving privacy.


\textbf{Model Splitting}.
The effective use of private data in FedFMs is hindered by computational constraints when processing local data. Split learning offers a solution: FedBone~\cite{JCST-2308-13639} splits the model into a cloud-based large-scale model and edge-based task-specific models, using conflicting gradient projection to improve cross-task generalization.
Extending to multimodal scenarios, M$^2$FedSA~\cite{zhangenhancing} modularizes models via split learning, retaining privacy-sensitive modules on clients and employing lightweight adapters to enhance task- and modality-specific knowledge.



\subsection{Challenges and Potential Solutions}


FMs face critical challenges due to their sensitivity to \textit{data quality issues}, which are more pronounced than in traditional FL, as existing quality control mechanisms are inadequate for their complex needs. Additionally, there is a \textit{scale and efficiency trade-off}, as FMs require vast amounts of high-quality data for training and tuning. Furthermore, \textit{computational resource constraints} are significant, as the cost of training FMs on private data far exceeds that of traditional models.

To address these challenges, potential solutions include:  
(1) \textit{Balancing data efficiency and computational cost} by designing unified data selection frameworks that use hierarchical sampling to identify valuable training instances, optimizing the trade-off between data usage and computational expenses.  
(2) \textit{Enhancing private data processing} through advanced federated split learning with adaptive layer-splitting strategies, tailoring model partitioning based on device capabilities, network conditions, and task requirements.  
(3) \textit{Refining data contribution evaluation} by establishing unified quality assessment metrics that consider data utility, task relevance, diversity, and their impact on existing model knowledge, ensuring more effective private data evaluation.