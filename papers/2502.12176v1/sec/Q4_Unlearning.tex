\section{Problem 4: How to Design Unlearning in FedFMs?}

\subsection{Defining Unlearning in FedFMs}



Data is crucial for intelligent development across industries, but data leakage risks during circulation and sharing have raised privacy concerns. Global governments and legislators have imposed stringent data privacy regulations, such as GDPR and CCPA, mandating digital service providers to grant users the ``Right to Be Forgotten" (RTBF) \cite{dang2021right} and establish mechanisms for data removal from models \cite{liu2022right}. 

This opt-out mechanism, known as machine unlearning (MU) \cite{Gu2024Unlearning}, is vital in FedFMs. Despite FL not requiring direct user data access, it retains implicit information from training, posing privacy risks from model inversion attacks \cite{ren2022grnn}. Additionally, training data may include unauthorized or biased content, raising ethical and copyright issues \cite{Carlini2023Quantifying}. Addressing these concerns is essential for responsible FedFM deployment. Due to the high retraining cost, developing a federated unlearning (FU) technique that aligns with legal frameworks is a key challenge for FedFM advancement. 

The unlearning process in FedFMs, illustrated in Fig. \ref{fig: 5-Unlearning-framework.pdf}, involves identifying and flagging clients and data records associated with data deletion requests, then applying active forgetting techniques to remove specified data contributions while preserving model integrity.


\textbf{Problem Formulation.}
The objective of FU is to mitigate the impact of a specific subset of data, denoted as $\tilde{D}_{j}$, belonging to the target client $j$ \cite{wang2022federated, shah2023unlearning,Gu2024Unlearning}. 

In the context of FU, we need to construct a new model $w_{un}$. To do this, we first define a reference model $w_{re}$. The model $w_{re}$ is obtained by minimizing a weighted sum of loss functions. Here, $F$ is the aggregation mechanishm such as FedAvg \cite{mcmahan2017communication} . The model $w_{re}$ is given by:
\begin{equation}
\begin{split}
 w_{re}=\argmin_w F \Big (\ell_1(w, D_1). \cdots, \ell_{j-1}(w, D_{j-1}), \\
\underbrace{\ell_j(w, D_j - \tilde{D}_{j})}_{\text{removing data}\tilde{D}_{j}}, \cdots, \ell_K(w, D_K - \tilde{D}_{K})\Big)   
\end{split}
\end{equation}
Formally, the model $w_{un}$ constructed by the unlearning algorithm should satisfy the following optimization objective:
\begin{equation}\label{eq:unlearn}
\min_{w_{un}}\text{dist}\Big(\underbrace{w_{un}}_{\text{unlearned model}}, \underbrace{w_{re}}_{\text{retrained model}}\Big)
\end{equation}
where $\text{dist}(\cdot,\cdot)$ represents a measure of the distance between two models. Common choices for this distance measure include the Euclidean distance or cosine similarity \cite{gu2024ferrari}, KL divergence \cite{van2014renyi} and Maximum Mean Discrepancy \cite{pan2008transfer}. By minimizing this distance, Eq. \eqref{eq:unlearn} ensures that the model $w_{un}$ closely approximates the retrained model $w_{re}$.
\begin{figure}[ht]
    \centering
     \includegraphics[width=0.49\textwidth]{figures/5-Unlearning-framework.pdf}
     \caption{Machine Unlearning in FedFMs.}% \cite{liu2024survey} 
     \label{fig: 5-Unlearning-framework.pdf}
     \vspace{-5mm}
 \end{figure}



\subsection{Existing Methods}
We summarize the four key aspects of the FU process: unlearning targets, unlearning executors, unlearning verification, and unlearning principles.

\subsubsection{Unlearning Targets}
Unlearning requests are typically classified into four types:\textbf{ partial samples}, \textbf{entire classes}, \textbf{sensitive features}, and \textbf{entire clients}. Partial sample unlearning removes specific data samples' contributions, representing fine-grained unlearning \cite{ma2022learn, shah2023unlearning}. Feature unlearning eliminates specific data features, such as facial attributes \cite{gu2024ferrari}. Class unlearning removes data contributions from one or more classes, like forgetting facial images of a specific user in facial recognition models \cite{wang2022federated, gu2024few}. Client unlearning involves revoking all data contributions from a client, which can lead to catastrophic forgetting due to the large-scale data removal \cite{Gu2024Unlearning, nguyen2022markov}.


\subsubsection{Unlearning Executors}

Unlearning executors include server-side and domain model-based approaches.\textbf{ Server-side unlearning} adjusts global model parameters, initially requiring full retraining, which is costly in federated settings \cite{liu2022right}. Recent methods focus on eliminating target user influence while restoring model performance through relearning and parameter adjustments \cite{su2023asynchronous, fraboni2024sifu}. \textbf{Domain model-based unlearning} uses additional training to update the global model without full retraining, employing techniques like training update correction and gradient correction, which minimize complexity and resource overhead \cite{hacohen2019power, wu2022federated, gao2024verifi, halimi2022federated}. These approaches typically preserve higher model accuracy than server-side methods.

\subsubsection{Unlearning Verification}
Unlearning verification in FU aims to assess the effectiveness, efficiency, and security of data removal from models, typically categorized into metric-based and attack-based approaches. \textbf{Metric-based verification} evaluates model performance using accuracy, loss, and statistical errors on target and test datasets to ensure proper data removal and model robustness. Model discrepancy is measured using techniques such as Euclidean distance or KL-divergence, while execution efficiency is assessed through metrics like time, memory usage, and speed-up ratios \cite{cao2023fedrecover}. \textbf{Attack-based verification} simulates adversarial scenarios, such as membership inference and backdoor attacks, to test if unlearned data can still be inferred or if malicious triggers persist. Reduced attack success rates indicate effective unlearning \cite{zhang2023fedrecovery}.



\subsubsection{Unlearning Principles}

 Existing unlearning methods rely on various principles to align the distribution of the modified model $ W $ with that of a retrained model $ w_{re} $ \cite{liu2024survey}. \textbf{Retraining} removes all influence of $ D_u $ by training from scratch on $ D_r $, but it is resource-intensive. \textbf{Fine-tuning} optimizes $ W $ on $ D_r $ to reduce the impact of $ D_u $, though it requires multiple iterations. \textbf{Gradient ascent} maximizes loss to reverse the influence of $ D_u $, but risks catastrophic forgetting unless constraints are applied. \textbf{Multi-task unlearning} balances the removal of $ D_u $'s influence with reinforcing $ D_r $'s knowledge. \textbf{Model scrubbing} transforms $ W $ using a quadratic loss approximation to mimic retraining, but the computational cost of approximating the Hessian limits scalability. Finally, \textbf{synthetic data} replaces or combines $ D_u $ with synthetic data to disentangle its influence while preserving model performance. 
 Each method involves trade-offs between effectiveness, efficiency, and scalability.
 \citet{shao2024federated} further analyzed unlearning in federated settings, revealing additional side-effects like stability and fairness stem from data heterogeneity.



\subsection{Challenges and Potential Solutions}

FedFMs encounter additional unlearning challenges due to \textit{high model complexity}, \textit{knowledge coupling}, and \textit{cross-client consistency} issues. High model complexity complicates targeted updates, risking model integrity and performance. Knowledge coupling in multi-modal FedFMs necessitates comprehensive eradication of associated knowledge. Maintaining cross-client consistency during unlearning is challenging, requiring robust synchronization and validation.

To address these challenges, potential solutions include: (1) \textit{Modular Optimization Unlearning}, where modular architectures localize unlearning to specific components, using techniques like constrained fine-tuning and sensitivity analyses to identify influenced parameters. (2) \textit{Disentangled Knowledge Decoupling} separates knowledge across modalities, enabling targeted unlearning with techniques like cross-modal attention tracking and mutual information minimization. (3) \textit{Federated Cross-Client Coordination} ensures consistency through federated consensus mechanisms, using gradient averaging and replay buffers to synchronize updates and align with unlearning objectives.