
\section{Problem 3: How to Design Continual Learning in FedFMs?}
\label{cl}
\subsection{Defining Continual Learning in FedFMs}

Existing FL setups assume static models and data, neglecting the dynamic nature of real-world data collection. FedFMs, with their numerous parameters and complex training, struggle to meet real-time requirements without retraining. As the number of clients and data samples grows, FedFMs face scalability issues, potentially increasing communication overhead and slowing convergence. To address these, Continual Learning (CL) can be integrated into FedFMs for incremental updates without full retraining. Fig. \ref{fig:CLwithFM} illustrates our generalized FCL framework for FedFMs.

A crucial overlooked issue in FCL is the interaction between spatial and temporal heterogeneity, leading to spatial-temporal catastrophic forgetting (ST-CF) \cite{yang2024federatedcl}. Catastrophic Forgetting (CF) describes a deep model forgetting previous tasks after learning new ones, reducing accuracy \cite{kirkpatrick2017overcoming,li2017learning}. In FCL, clients experience temporal CF as they continually learn new tasks, while non-IID data causes spatial CF in the global model, decreasing performance on local test sets. Spatial forgetting interacts with temporal forgetting as clients use the global model for subsequent tasks \cite{yang2024federatedcl}.

The goals of FCL are threefold: \textbf{\textit{(1)}} Continuously update and optimize local models to adapt to changing local data. \textbf{\textit{(2)}} Enhance client knowledge through interaction with other clients via FL. \textbf{\textit{(3)}} Apply knowledge gained from other clients for further learning.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/4-Continual.pdf}
    \caption{An illustration of Continual Learning with FedFMs. 
    % FM learns different knowledge from various clients through FL. Then, the server is responsible for organizing these pieces of knowledge to equip FM, turning it into a more powerful FM, which is then ready to tackle the next task. 
    }
    \label{fig:CLwithFM}
    % \vspace{-7mm}
\end{figure}

\textbf{Problem Formulation}.
This section extends the traditional FL framework to FCL to address the challenges posed by strong spatial-temporal data heterogeneity. 
Regarding spatial heterogeneity \cite{yoon2021federated,dong2022federated,ma2022continual}. $K$ clients have distinct data denoted as $D_k$. For temporal heterogeneity, in the federated system, clients are involved in learning $m$ tasks. The corresponding datasets for each client $k$ are $\{D_k^1,\ldots,D_k^{m}\}$.

When all clients have collaboratively completed learning task $1, \cdots, T$ and are about to embark on learning the new task $T + 1$, the goal of federated continual learning encompasses two key aspects. Firstly, it is to effectively learn the new task $T+1$. Secondly, it is to ensure that knowledge of the previous tasks $1,\cdots,T$ is not forgotten. Specifically, the optimization objective is formulated as:
\begin{equation}
\begin{split}
    \min_{w_g}F\Big(\underbrace{\ell_{u,1}(w_g,\{D_1^t\}_{t = 1}^T), \cdots, \ell_{u,K}(w_g,\{D_K^t\}_{t = 1}^T)}_{\text{old task loss}}, \\ \underbrace{\ell_{u,1}(w_g,D_1^{T + 1}),\ell_{u,K}(w_g,D_K^{T + 1})}_{\text{new task loss}}\Big),
    \end{split}
\end{equation}
where $\ell_{u,k}(w_g,\{D_k^t\}_{t = 1}^T)$ and $\ell_{u,k}(w_g,D_k^{T + 1})$ are the loss functions of client $k$ corresponding to the old tasks and the new task, respectively. $F$ is the aggregation mechanism.

\subsection{Existing Methods}

The keys to the FCL problems are how to integrate the heterogeneous knowledge from different clients, and how the clients can utilize this fused knowledge for future learning. Inspired by the tri-level (input, model-parameter and output) division in class-incremental learning \cite{zhou2024class} and existing survey of FCL \cite{yang2024federatedcl,wang2024federated}, we categorize the most effective and well-known existing methods into the following types: (1). \textit{Replay-based Methods}, (2). \textit{Methods based on Regularization and Decomposition
}, (3). \textit{Distillation-based Methods} and (4). \textit{Prompted-based Methods}.

\textbf{Replay-based Methods} involve either retaining samples in their original form or creating synthetic samples using a generative model. These samples from previous tasks are reintroduced during the learning of new tasks to mitigate forgetting. \citet{li2024towards,li2024sr} cached selected previous samples based on their global and local importance. 
As for the generative ways, \citet{yu2024overcoming} (FedCBC) built class-specific variational autoencoders for each class on the client side, performing classification tasks through anomaly detection, thus avoiding forgetting caused by interference and overlap between classes. \citet{liang2025diffusion} utilized a pre-trained conditional diffusion model to deduce class-specific input conditions within the model's input space, substantially cutting down on computational resources while ensuring effective generation.



\textbf{Methods based on Regularization and Decomposition} are to add appropriate regularization terms to model parameters during optimization to preserve old knowledge, or to divide the model into multiple parts to prevent forgetting.
\citet{yoon2021federated} (FedWeIT) decomposed parameters into local, global-based and task-adaptive parts to address both practical and pathological data heterogeneity in FCL. \citet{dong2022federated} (GLFC) utilized a class-aware gradient compensation loss and a class-semantic relation distillation loss to mitigate forgetting, and a proxy server to alleviate data heterogeneity.



\textbf{Distillation-based Methods} are a popular technique to fuse knowledge, the core idea is to distill the knowledge contained in an already trained teacher model into a student model \cite{hinton2015distilling}. \citet{ma2022continual} introduced CFeD, a method that employs knowledge distillation at both client and server levels in continual federated learning, with each client having a separate unlabeled dataset to reduce forgetting and leverage unused computational resources. 
\citet{wei2022knowledge} (FedKL) separated the training goal into two parts: classification and knowledge retention. In the knowledge retention part, classes not available locally are supervised through global model distillation with logistic regression loss.



\textbf{\textit{Prompted-based Methods}} leverage the pre-trained Vision Transformer (ViT) \cite{dosovitskiy2020image}, which has strong representation capabilities, by adding corresponding prompts or fine-tuning it to enhance its performance on downstream tasks. To our knowledge, this is the only paradigm closest to the utilization of FedFMs so far. \citet{piaofederated} proposed POWDER 
% (\textbf{P}rompt-based kn\textbf{ow}le\textbf{d}ge transf\textbf{er})
to effectively foster the transfer of knowledge encapsulated in prompts between various sequentially learned tasks and clients. \citet{yu2024personalized} proposed a novel concept called multi-granularity prompt (FedMGP), i.e., coarse-grained global prompt acquired through the ViT learning process, and fine-grained local prompt used to personalize the generalized representation.




\subsection{Challenges and Potential Solutions}

\textit{Spatial-temporal data heterogeneity} is a significant challenge, with data varying across clients and within different tasks of the same client, leading to spatial-temporal catastrophic forgetting \cite{yang2024federatedcl}.
FedFMs must address \textit{knowledge transfer and error correction} during federated continual learning (FCL), leveraging knowledge from other clients to learn future knowledge and using later-acquired knowledge to correct past errors. Furthermore, \textit{knowledge conflict} arises due to the complexity in real-world applications, where knowledge between clients or at different times can be incompatible or conflicting.

To address these challenges,  potential solutions include: (1) Utilizing an \textit{external knowledge base} to provide pre-existing knowledge, enriching the FedFMs with additional context and information. (2) Implementing \textit{selectively knowledge fusion} within FedFMs to prevent conflicts and identify malicious clients, ensuring robust and secure model updates.


