\section{Problem 5: How to address NON-IID issues and graph data in FedFMs}


\subsection{Defining the Non-IID Issues and Graph Data in FedFMs}


The Non-IID issues stem from heterogeneous data distributions across clients, where each client may have distinct feature sets, label distributions, and sample sizes. These variations pose significant challenges to global model training, particularly in FedFMs~\cite{ren2024advances,zhong2023semi, Yang_Zhang_Dai_Pan_2020}. The primary types of Non-IID data are outlined as follows: 1) \textbf{Feature distribution Skew}, where clients have distinct feature distributions, leading to data-specific patterns; \textit{e.g}., hospitals serving different age groups. 2) \textbf{Label distribution Skew}, where differing label distributions hinder model generalization, as seen in transaction classification with varying data types; 3) \textbf{Quantity Skew}, where data volume disparities across clients bias aggregation, reducing model representativeness. 4) \textbf{Temporal Skew}, when clients collect data at different times, complicating model generalization, such as regional weather sensors recording at non-synchronized times.

Beyond these static skews, dynamic shifts like concept drift (same label, different features) and concept shift (same features, different label) introduce additional complexity. Concept drift occurs when $P_k(x | y)$ changes while $P(y)$ remains consistent, as seen in regional variations of "winter" images. Concept shift arises when $P_k(y | x)$ varies despite a stable $P(x)$, such as differing labels for similar behaviors across cultures.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/8-Graph_FedFM.pdf}
    \caption{FedFMs with Graphs and Graphs of FedFMs.}
    \label{fig:Graph-FM}
    \vspace{-3mm}
\end{figure}

The federated graph of models in FedFMs presents two key perspectives: \textbf{FedFMs with Graphs} and \textbf{Graphs of FedFMs}, as shown in Figure~\ref{fig:Graph-FM}. In the former, each clientâ€™s data forms graph structures capturing internal and external dependencies, requiring models to learn both intra- and inter-graph relationships~\cite{zhang2021federated}. In the latter, clients act as nodes in a federated network, with edges representing relationships like geographic proximity or data similarity~\cite{fu2022federated}. This \textit{Internet of Models} enables selective information sharing under Non-IID conditions but demands advanced aggregation strategies to ensure model consistency.


\textbf{Problem Formulation.} There are two different settings in this section involving a server and multiple clients: \textit{Non-IID issues} and \textit{FedFMs with Graphs}.

\begin{itemize}
  \item  \textbf{Setting 1. Non-IID issues}. In the FedFMs framework \cite{zhang2021federated,fu2022federated}, the optimization problem with Non-IID data across $K$ clients is formulated as follows:
\begin{equation}
\begin{aligned}  
&\min_{w_g}F\Big(\ell_{u,1}(w_g,D_1), \cdots, \ell_{u,K}(w_g,D_K) \Big) \\  
&\text{where}\quad dist(D_{k_1}, D_{k_2})>0\ \forall k_1\neq k_2
\end{aligned}
\end{equation}
where \(\ell_{u,k}(\cdot)\) represents the utility loss function of client \(k\), $F$ is the aggregation mechanishm such as FedAvg and $\text{dist}(\cdot, \cdot)$ represent the distance of two dataset, \textit{e.g.},  cosine similarity, KL divergence and Maximum Mean Discrepancy.

  \item \textbf{Setting 2. FedFMs with Graphs}. In this setting, each client \(k\) possesses a local graph \(G_k=(V_k, E_k)\). The set \(V_k\) contains the nodes, and \(E_k\) represents the set of edges within the local graph. $K$ clients consist the whole graph as $G =(V ,E)$, where vertices are $K$ nodes \cite{baek2023personalized}. The objective of learning a federated graph neural network (GNN) model \(w_g\) is given by:
\begin{equation}\label{eq:graph}
\begin{split}
\min_{w_g}&F\Big(\ell_{u,1}(w_g, G_1, G), \cdots, \ell_{u,K}(w_g, G_K, G) \Big)\\
& s.t.\quad \ell_{p,k}(w_g, G_k, G)<\delta_k, \text{for }k=1,\cdots,K
    \end{split}
\end{equation}
where \(\ell_{u,k}(\cdot)\) is the utility loss function of client \(k\). Additionally, $\ell_{p,k}(\cdot)$ denotes the privacy leakage of client $k$, $F$ is the aggregation mechanishm and it must be within a predefined threshold $\delta_k$. Compared to the Non-IID problem, Eq. \eqref{eq:graph} considers the graph data $G$ and $G_k$.
\end{itemize}




\subsection{Existing Methods}

To address Non-IID challenges in FedFMs, techniques like knowledge distillation and model pruning unify models into a homogeneous representation, reducing optimization complexity~ \cite{fan2023fate,lachi2024graphfmscalableframeworkmultigraph}. Alternatively, modeling model relationships as a graph optimization problem, where client models are nodes and edges represent feature similarity, task relevance, or proximity, enhances adaptability in federated systems~\cite{yu2024netsafe, feng2024graphroutergraphbasedrouterllm}. Existing methods address specific aspects of FL challenges as follows:

\textbf{Distribution Adaptation}. These methods enhance global model performance in Non-IID settings by addressing distributional shifts. \citet{park2024fedbaffederatedlearningaggregation} (FedBaF) biases aggregation with a pre-trained foundation model to leverage its generalization capabilities. \citet{imteaj2024tripleplayenhancingfederatedlearning} (TriplePlay) integrates foundation models, using CLIP as an adapter to mitigate data heterogeneity. \citet{6-data-feddpa} (FedDPA) employs a dual-adapter framework with dynamic weighting for test-time adaptation and personalization. These approaches collectively improve FedFMs' adaptability to diverse client distributions.


\textbf{Federated Graph of Models}. Such work is typically related to \textit{Federated Graph Foundation Models with Topology Optimization}, focusing on scalable learning across distributed graph-structured data by optimizing client relationships for efficient information sharing. \citet{li2024fedgtatopologyawareaveragingfederated} (FedGTA) enhances federated graph learning with topology-aware local smoothing and mixed neighbor features. \citet{ma2024beyond} proposed \textit{client topology learning} and \textit{learning on client topology}, leveraging client topology to train models robust to out-of-federation data. \citet{huang2022accelerating} introduced TOFEL, optimizing aggregation topology and computing speed to reduce energy consumption and latency. These approaches enhance performance, scalability, and communication efficiency in graph-based FedFMs.  


\textbf{Optimization in Graph of Models}. Achieving a sustainable balance between security, fairness, and personalization in model networks is crucial for adapting to dynamic environments while ensuring stable performance. A key challenge in federated topologies is preserving model privacy. \citet{chen2024privfusion} (PrivFusion) introduced a privacy-preserving fusion architecture using graph structures and hybrid local differential privacy. \citet{chen2024model} proposed a cross-federation model fusion framework to address privacy, population shifts, and fairness in global healthcare. Beyond privacy, complexity and personalization are also critical. 


\subsection{Challenges and Potential Solutions}


\textit{Convergence issues} from heterogeneous client data hinder optimization, causing slow or unstable convergence, especially in large-scale federated systems. The \textit{complexity of dynamic network topologies} complicates communication, aggregation, and information flow, exacerbated by computational challenges. In the Internet of Models paradigm, \textit{sustainable optimization} requires balancing utility, privacy, fairness, and personalization, further complicated by diverse node priorities.

To address these challenges, potential solutions include: (1) \textit{Adaptive Optimization} through personalized FL algorithms and hierarchical clustering to align local and global objectives. (2) \textit{Dynamic and Topology-aware Aggregation} to manage evolving network structures and ensure effective communication. (3) \textit{Multi-objective Optimization}, integrating differential privacy, fair allocation, and reinforcement learning to enhance scalability, adaptability, and fairness in Non-IID and graph-based environments.
