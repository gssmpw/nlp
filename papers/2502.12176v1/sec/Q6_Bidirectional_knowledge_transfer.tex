\section{Problem 6: 
How to achieve the bidirectional knowledge transfer between FMs and DMs in FedFMs?
}

\subsection{Defining Bidirectional Knowledge Transfer in FedFMs}

Bidirectional knowledge transfer within FedFMs denotes the reciprocal exchange and adaptation of knowledge between the server-hosted FMs and the client-owned DMs. This dynamic interplay entails not only the transfer of knowledge from the FM to the DMs to bolster their capacities but also the converse flow of domain-specific insights from the DMs back to the FMs, thereby enriching its comprehension and performance across a spectrum of domains. 
This interaction between the server-based FMs and client-based small DMs is analogous to the teaching and learning interactions between a teacher and students; knowledge flows both ways while privacy is preserved.

\begin{figure}[ht]
    \centering
     \includegraphics[width=0.45\textwidth]{figures/7-Bidirectional_Knowledge_Transfer.pdf}
      
     \caption{Illustration of the three settings for bidirectional knowledge transfer in FedFMs.}
     \label{fig:Bidirectional}
     % \vspace{-3mm}
 \end{figure}
 % : Setting \textcircled{1} aims to transfer and adapt knowledge from the server's FM to the clients' DMs, Setting \textcircled{2} focuses on leveraging domain-specific knowledge from the clients to enhance the server's FM, and Setting \textcircled{3} strives for the co-optimization of both the server's FM and the clients' DMs




The framework of bidirectional knowledge transfer can be delineated into three distinct settings \cite{kang2023grounding,xiao2023offsite,deng2023mutual}, as illustrated in Fig. \ref{fig:Bidirectional}.  \textit{Setting 1} aims to facilitate the transfer and adaptation of knowledge from the server's FM to the clients' DMs. \textit{Setting 2} focuses on harnessing the domain-specific expertise of clients to refine the server's FM. \textit{Setting 3} strives for the co-optimization of both the FM and the DMs, fostering a synergistic enhancement of their respective capabilities. 

\textbf{Problem Formulation}. 
There are three different settings in this section involving a server and multiple clients. Here, \(w_s\) represents the server's FM, \(w_k\) represents the DM of client \(k\), \(D_k\) is the local private dataset of client \(k\), and \(D_p\) is the public data available at the server. 


\begin{itemize}
\item \textbf{Setting 1: Optimizing clients' DMs using server's FM and local data.}
The goal is to optimize the DMs (\(w_k\)) of the client $k$ by taking advantage of the knowledge of the server's model \(w_s\) and the local private dataset (\(D_k\)) of each client. The main objective is formulated as:
\begin{equation*}\label{eq:bi - setting_1}
\min_{w_1, \cdots, w_K}F\Big(\ell_{u,1}(w_1|w_s ,D_1), \cdots, \ell_{u,K}(w_K|w_s ,D_K) \Big),
\end{equation*}
where \(\ell_{u,k}(w_k|w_s ,D_k)\) represents the utility loss function of client \(k\).

\item \textbf{Setting 2: Optimizing the server's FM using clients' domain-specific knowledge and public data.}
    This setting aims to optimize the server's FM (\(w_s\)) by leveraging the domain-specific knowledge of the clients' models (\(w_{k}\), where \(k\in\{1,\cdots,K\}\)) and the public data (\(D_p\)). The main objective is given by:
\begin{equation*}\label{eq:bi - setting_2}
\min_{w_s}F\Big(\ell_{u,1}(w_s|w_1 ,D_P), \cdots, \ell_{u,K}(w_s|w_K ,D_P) \Big),\end{equation*}
where \(\ell_{u,s}(w_s|w_k, D_p)\) is the utility loss function of the server. 

    \item \textbf{Setting 3: Co-optimizing the server's FM and clients' DMs.}
    This setting seeks to co-optimize the server's FM (\(w_s\)) and the clients' DMs (\(w_k\) for \(k\in\{1,\cdots,K\}\)) by mutually leveraging the knowledge of each other. The main objective is formulated as:
\begin{equation}\label{eq:bi - setting_3}
\begin{split}
    \min_{w_s, w_1, \cdots, w_K}\alpha F_1\Big(\ell_{u,1}(w_1|w_s ,D_1), \cdots, \\\ell_{u,K}(w_K|w_s ,D_K) \Big)\\ + (1-\alpha)F_2\Big(\ell_{u,1}(w_s|w_1 ,D_P), \cdots, \\\ell_{u,K}(w_s|w_K ,D_P) \Big),
    \end{split}
\end{equation}
where \(\ell_{u,k}(w_k|w_s ,D_k)\) is the utility loss function of client \(k\), and \(\ell_{u,s}(w_s|w_k, D_p)\) is the utility loss function of the server. $F_1$ and $F_2$ are the one of the aggregation methods as $F$. $\alpha$ is the coefficient ranging from 0 to 1.
\end{itemize}




\subsection{Existing Methods}
\subsubsection{Setting 1 - Transferring knowledge from server's FM to downstream clients' DMs}

Existing methods encompass data-level, representation-level, and model-level knowledge transfer.

In data-level knowledge transfer, works focus on knowledge distillation and synthetic data utilization. \citet{hsieh2023distilling} proposed DSS, a knowledge distillation framework based on Chain of Thought (COT) for distilling LLMs step-by-step. \citet{li2022explanations} introduced MT-COT, leveraging LLM-generated explanations to enhance smaller reasoning models via multitask learning. \citet{jiang2023lion} presented Lion, an adversarial distillation framework for efficient LLM knowledge transfer. \citet{fan2024pdss} developed PDSS, a privacy-preserving framework for distilling step-by-step FMs using perturbed prompts and rationales. \citet{li2024federated} proposed FDKT, a federated domain-specific knowledge transfer framework utilizing a generative pipeline on private data with differential privacy guarantees.

In representation-level knowledge transfer, works concentrate on split learning and knowledge distillation. \citet{shen2023split} investigated the impact of the split position in the FM on privacy-preserving capacity and model performance in the SAP framework. \citet{he2019model} introduced FedGKT, a method tailoring the server's FM to clients' DMs using knowledge distillation between the FM and DM. \citet{gu2024minillm} presented MINILLM, which distills LLMs in a white-box setting by minimizing reverse Kullback-Leibler divergence.

In model-level knowledge transfer, \citet{yu2023selective} proposed SPT, a selective pre-training approach that pre-trains a FM on training data chosen by a domain classifier trained on clients' private data via DP-SGD. \citet{zhang2023gpt} utilized FMs to generate synthetic data for training a model, which is then distributed to clients for initialization and fine-tuning with private data within the standard FL framework.

\subsubsection{Setting 2 - Enhancing  server's FM with domain knowledge from downstream clients}

Existing methods for enhancing the server's FM with domain knowledge from downstream clients focus on representation-level and model-level knowledge transfer. 

In representation-level knowledge transfer, \citet{yu2023multimodal} proposed CreamFL, a multimodal FL framework based on knowledge distillation, which aggregates representations to facilitate knowledge transfer. CreamFL uses a global-local cross-modal aggregation strategy and inter/intra-modal contrastive objectives. On the server side, the FM is trained via knowledge distillation using client-provided aggregated representations.

In model-level knowledge transfer, \citet{xiao2023offsite} introduced Offsite-Tuning, a privacy-preserving and efficient transfer learning framework. It splits the FM into a trainable adapter and a frozen emulator. Clients fine-tune the adapter on their data with the emulator's help and send it back to the server for integration. \citet{fan2023fate} extended this framework to FL, where the server distributes the emulator and adapter to clients, who fine-tune the adapter and return it. The server aggregates the adapters securely and integrates them into the FM.

\subsubsection{Setting 3 - Co-optimizing both server’s FM and clients’ DMs}

Approaches for co-optimizing the server’s FM and clients’ DMs also involve representation-level and model-level knowledge transfer. 

In representation-level knowledge transfer, \citet{fan-etal-2025-fedmkt} introduced FedMKT, a federated mutual knowledge transfer framework that uses selective knowledge distillation and token alignment (via MinED) to enhance both the FM and DMs, addressing model heterogeneity.

In model-level knowledge transfer, \citet{fan2024fedcollm} proposed FedCoLLM, a parameter-efficient federated framework using lightweight LoRA adapters alongside DMs for privacy-preserving knowledge exchange. It also employs mutual knowledge distillation between the FM and aggregated DM on the server. \citet{deng2023mutual} introduced CrossLM, a framework for federated learning and mutual enhancement of client-side DMs and a server-side FM without sharing private data. DMs guide the FM to generate synthetic data, which is validated by clients' DMs, improving both models and enabling data-free knowledge transfer.


\subsection{Challenges and Potential Solutions}


In the realm of bidirectional knowledge transfer, challenges such as \textit{data heterogeneity}, \textit{representation heterogeneity}, \textit{model heterogeneity}, and \textit{privacy concerns} persist. 

To address these challenges, potential solutions include: (1) \textit{Unified Model Architectures} to promote standardization and compatibility. (2) \textit{Adaptive Knowledge Transfer} methods that dynamically adjust based on model needs and capabilities \cite{jiang2023lion}. (3) \textit{Synthetic Data} to augment training processes without sole reliance on real-world data. (4) \textit{Advanced Privacy Techniques} like differential privacy \cite{dwork2006differential}, homomorphic encryption \cite{gentry2009fully}, and secure multi-party computation \cite{yao1986generate} to mitigate privacy risks without compromising performance.





