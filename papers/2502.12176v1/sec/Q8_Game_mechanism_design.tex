\section{
Problem 8: How to design game mechanisms in FedFMs?
}
\label{game_mechanism}

\subsection{Defining Game Mechanisms in FedFMs}
Before introducing game mechanisms in FedFMs, it is essential to clearly define the roles of the various participants. FedFMs participants are often categorized into three types: honest, semi-honest and malicious. Honest participants are the most ideal, as they strictly adhere to the established FedFMs protocols and follow the prescribed procedures for each step. Semi-honest participants, on the other hand, do not fully comply with the protocols. They might discreetly collect private data without violating the FedFMs protocol to infer other participants’ private information, but they do not actively launch attacks or collude with other participants to undermine the protocol. They pose a significant threat to the privacy of the protocol because they are not actively attacking and are difficult to detect. 
Malicious participants are easily compromised by attackers or are themselves attackers disguised as legitimate FedFMs participants, to expose sensitive private data. Semi-honest attacks are the most common. They can include gradient inversion attack \cite{zhu2019deep,geiping2020inverting}, model inversion attack \cite{he2019model}, and GAN-based attack \cite{wang2019beyond}. The most classic attacks by malicious participants include poisoning attacks \cite{tolpegin2020data} and backdoor attacks \cite{gao2020backdoor,li2022backdoor}.

A game mechanism can be designed such that the costs incurred by adversaries, including semi-honest or malicious attackers, significantly outweigh benign participants' benefits, as illustrated in Fig. \ref{fig:game_mechanism_design}. Moreover, unlike traditional FL, a game mechanism in FedFMs must balance intensive computational demands with robust privacy guarantees cross diverse participants.
These challenges manifest in three critical aspects:
\begin{itemize}
\item A fundamental tension between model performance and privacy preservation, as the requirement for extensive training data makes privacy protection substantially more complex than in smaller federated models.
\item Heightened privacy risks due to foundation models' ability to memorize and regenerate training data, rendering traditional game-theoretic approaches insufficient.
\item The need for scalable incentive mechanisms that remain economically viable while meeting substantial computational requirements.
\end{itemize}

Addressing these challenges calls for a structured game-theoretic approach in FedFMs mechanism design. Without it, the effectiveness and sustainability of FedFMs systems are significantly undermined, resulting in increased vulnerability to attacks, inefficient resource utilization and erosion of trust. 




\textbf{Problem Formulation} Assume $K$ clients participate in FedFMs, and let $\mathcal{A}_k$ represent the set of actions available to participant $k$, where $a_k \in \mathcal{A}_k$ is the action chosen by participant $k$. These actions may include contributing data, reporting malicious activity, or abstaining from malicious behavior. The utility function $U_k(a_k, a_{-k})$\footnote{The parameter of Utility function $U(\cdot)$ is action $a$, which is different from the utility loss $\ell_u$ whose parameter is weight.} for participant $k$ depends on their own action $a_k$ and the collective actions of all other participants, denoted by $a_{-k}$. To capture the interplay between utility, privacy, and efficiency, the utility function is defined as follows:
\begin{equation}
\label{eq:game_utility}
U_k(a_k, a_{-k}) = r_k - c_k(a_k) + \lambda_k R_k(a_k) - \alpha P_k(a_k) - \beta E_k(a_k),
\end{equation}
where $r_k$ is the reward for honest behavior. $c_k(a_k)$ is the cost associated with the action $a_k$, which may involve computational resources or privacy risks. $R_k(a_k)$ is the reputation score with weight $\lambda_k$. $P_k(a_k)$ is the privacy loss function with weight $\alpha$. $E_k(a_k)$ is the computational efficiency cost with weight $\beta$. 

The game-theoretic equilibrium is reached when no participant can improve their utility by unilaterally changing their action, meaning the system reaches a Nash Equilibrium:
\begin{equation}
\label{eq:game_nash}
U_k(a_k^*, a_{-k}^*) \geq U_k(a_k, a_{-k}^*), \forall k \in [K], \forall a_k \in \mathcal{A}_k,
\end{equation}
where $a_k^*$ and $a_{-k}^*$ are the equilibrium strategies for participant $k$ and all other participants, respectively.

The objective of benign participants is to maximize utility and minimize cost \cite{sarikaya2019motivating,pandey2020crowdsourcing,khan2020federated}:
\begin{equation*}
\label{eq:game_strategy_1}
\begin{aligned}
&\text{benign participants:}  \\
&\max \sum_{k \text{ is benign}} U_k(a_k, a_{-k}) \quad \& \quad \min \sum_{k \text{is 
 benign}}\text{Cost}_k(a_k),
% & \text{s.t. } \sum_{i=1}^{n} P_k(a_k) \leq P_{max} \quad \text{(global privacy budget)} \\
% & \quad \sum_{i=1}^{n} E_k(a_k) \leq E_{max} \quad \text{(system efficiency constraint)}
\end{aligned}
\end{equation*}
where $U_k(a_k, a_{-k})$ and $\text{Cost}_k(a_k)$ are utility and cost of benign participant $k$. Similarly, the objective of attackers is also to maximize utility and minimize cost
\begin{equation*}
\label{eq:game_strategy_2}
\begin{aligned}
&\text{Attackers:}  \\
&\min\sum_{k\text{ is 
attacker}} U_{att}(a_k, a_{-k}) \quad \& \quad \min \sum_{k\text{ is attacker}}\text{Cost}_{att}(a_k),
% & \text{s.t. } \sum_{i=1}^{n} P_k(a_k) \leq P_{max} \quad \text{(global privacy budget)} \\
% & \quad \sum_{i=1}^{n} E_k(a_k) \leq E_{max} \quad \text{(system efficiency constraint)}
\end{aligned}
\end{equation*}
where $U_{att}(a_k, a_{-k})$ and $\text{Cost}_{att}(a_k)$ are utility and cost of attacker $k$. We unify the two equations as:
\begin{equation}
    \begin{aligned}
&\max \sum_{k=1}^K U_k(a_k, a_{-k}) \quad \& \quad \min \sum_{k=1}^K\text{Cost}_k(a_k),
% & \text{s.t. } \sum_{i=1}^{n} P_k(a_k) \leq P_{max} \quad \text{(global privacy budget)} \\
% & \quad \sum_{i=1}^{n} E_k(a_k) \leq E_{max} \quad \text{(system efficiency constraint)}
\end{aligned}
\end{equation}




This mechanism creates a multi-layered defense by raising both technical and economic costs of attacks. Through this game theoretic framework, FedFMs establish a sustainable ecosystem where participants can safely contribute to model training while receiving fair compensation, making compliance and security the rational choice for all participants.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/2-game.pdf}
    \captionsetup{justification=centering}
    \caption{Illustration of FedFMs Game Mechanism Design.}
    \label{fig:game_mechanism_design}
    \vspace{-5mm}
\end{figure}





\subsection{Existing Methods}

Game theory, as an important socio-economic theory, has long been applied in computer science. From the perspective of game theory, \citet{zhang2024game} proposed a framework that considers defenders and attackers in FL and estimates the upper bounds of their payoffs, taking into account model utility, privacy leakage and costs. \citet{zhan2020learning} studied the motivation of edge nodes to participate in FL training. \citet{liao2024optimal} developed a mechanism that optimizes client sampling probabilities while ensuring truthful cost reporting from strategic clients. \citet{wu2020privacy} formulated privacy attacks and defenses as a privacy protection attack and defense game. They measure the defender’s payoff as privacy leakage, and the attacker’s payoff as privacy gain, thereby formulating a zero-sum game. \citet{wu2017game} transformed the trade-off between privacy and utility into a game-theoretic problem, constructing a multi-player game model to evaluate the efficiency of pure Nash Equilibria. \citet{he2024generative} explored the advantages of combining generative AI with game theory and further proposed a novel solution that combines powerful reasoning and generation capabilities of generative AI with the design and optimization of mobile networking via game theory. \citet{zhang2021more} found that differential privacy can have a favorable impact on game theory and quantify the cost of protection.


These game-theoretic efforts fundamentally aim to establish equilibrium between conflicting objectives in collaborative learning systems – protecting sensitive information while maintaining model efficacy and cost-effectiveness. To operationalize this equilibrium, researchers predominantly employ incentive mechanism design grounded in quantified payoff structures.
Based on the specific game adopted, existing methods can be divided into the following categories. 

\textbf{Stackelberg Game-based Methods}. 
Stackelberg games model FL interactions, with the server as the leader and clients as sellers setting prices. These methods optimize utility by balancing data, rewards, and resources. Prior works address fairness \cite{sarikaya2019motivating}, communication \cite{pandey2020crowdsourcing}, accuracy \cite{khan2020federated, luo2023incentive}, privacy \cite{hu2020trading}, and MEC pricing \cite{lee2020market}, though challenges like uniform pricing and unrealistic assumptions remain.


\textbf{Yardstick Competition-based Schemes}.
To reduce training delays, \citet{sarikaya2020regulating} proposed a yardstick competition where faster training earns higher rewards. While effective for synchronous FL, it oversimplifies delay factors.



\textbf{Shapley-Value (SV) Based Schemes}.
SV-based methods allocate rewards based on contribution. \citet{qu2020privacy} incentivized edge servers via gradient similarity, but ignored client input. \citet{song2019profit} introduced the Contribution Index (CI), an SV variant, but it remains limited to Horizontal FL.

\textbf{Other Incentive Mechanisms}.
Addressing payment delays, \citet{yu2020sustainable} proposed the Federated Learning Incentivizer (FLI), a real-time installment-based scheme improving fairness and revenue. Additionally, \cite{tang2023utility,tang2023competitive,tang2024bias,tang2023multi,tang2024cost,tang2024intelligent,tang2024stakeholder} modeled FL data trading as auctions, optimizing transactions based on budgets, training needs, and resources. These approaches enhance efficiency and fairness by fostering competition.



\subsection{Challenges and Potential Solutions}

\textit{Privacy cost quantification} is challenging due to complex interactions between encryption costs, potential breach losses, and diverse participant incentives, complicating the development of fair game-theoretic privacy models \cite{tang2024towards}. \textit{Information asymmetry}, limited visibility into participant payoffs and motivations, further complicates the design of incentive-compatible privacy mechanisms \cite{tang2024intelligent}, especially when balancing information sharing with security requirements. The \textit{dynamic security landscape} demands adaptive mechanisms that respond to evolving attack strategies, surpassing traditional static models. Finding the optimal balance between privacy strength, system utility, and computational efficiency remains an open challenge in privacy-preserving mechanism design.

To address these challenges, potential solutions include: (1) \textit{Adaptive game-theoretic models} leveraging reinforcement learning \cite{tang2024intelligent,zhang2021deep} can dynamically adjust strategies, improving resilience and utility. (2) \textit{Resource-aware privacy mechanisms} \cite{mishra2024resource} optimize protections based on client resources, balancing security and efficiency. (3) \textit{Incentive-compatible schemes} \cite{tang2024intelligent} use rewards and penalties to promote honest participation and deter attacks. (4) \textit{AI-driven anomaly detection} \cite{zeng2024towards} enhances security by identifying suspicious behavior in real time. Combined, these approaches strengthen FedFMs’ privacy and security in dynamic environments.
