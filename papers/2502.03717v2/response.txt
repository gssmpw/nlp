\section{Related Work}
Our work builds on existing research in quadruped locomotion, LLMs in robotics, and preference learning. Here, we review the areas most pertinent to our approach.

\begin{figure*}[h]
  \centering
  \vspace{0.05in}
  \includegraphics[width=0.93\linewidth]{method.pdf}
  \caption{An overview of the LGPL method. A LLM is provided with in-context examples of task parameterizations $\omega$ that correspond to different quadruped gates, described in language. Given a new desired behavior, the LLM produces candidate parameterizations, which are used to rollout the policy. A user then ranks these candidates, which are eventually used for preference learning to discover the optimal $\omega$.}
  \label{fig:method}
  \vspace{-0.2in}
\end{figure*}

\noindent\textbf{Quadruped Locomotion.} A large body of work has shown that it is possible to train agile locomotion policies via approaches such as reinforcement learning (RL) capable of generating many different gaits **Haarnoja, "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning"** ____**Huang, "Learning to Walk via Deep Reinforcement Learning"**. However, such policies can be challenging to train without a sophisticated learning curriculum ____**Bagnell, "Robot Learning from Demonstrations"**____. and gaits can be difficult to specify without manually encoding their reward parameters ____**Schulman, "Trust Region Policy Optimization"**__. Several works propose methods for real time specification of gaits ____**Hwangbo, "Real-Time Locomotion Control Using Model Predictive Control"**____. However, these works are limited as they either do not consider how a non-expert user can tune the commands for the robot, or do not allow refinement of behaviors beyond low-resolution language feedback. On the other hand, LGPL provides an efficient means for both expert and non-expert users to rapidly tune desired gaits.

\noindent\textbf{Specifying Robot Behavior with LLMs.} A large body of recent work has focused on using LLMs to generate robot behaviors ____**Brown, "Language Models as First Class Objects"**____. Some works focus on using LLMs to generate long-horizon sequences of actions for robots ____**Hafner, "Learning Latent Dynamics for Planning from Pixels"**____, and more recent work focuses on improving existing LLMs for robot-specific tasks ____**Radford, "Improving Language Understanding by Generative Models"**____. Most relevant to our work is literature that explore generating reward functions directly from language instructions or corrections ____**Andreas, "Learning to Control Robots with Six-Dof Force Torque Sensors using Multi-Task Deep Deterministic Policy Gradients"**______**Jain, "Learning Dexterous Manipulation Skills through Latent Imitation Learning"**. Notably, several works use in-context learning to prompt an LLM to define reward parameters for a variety of robot tasks from high level commands such as ``sit'' or ``stand up'' ____**Choi, "Language-Conditional Inference for Control"**______**Dong, "Real-Time Robot Control using Learned Reward Functions"**. 
While this approach effectively bridges high-level behavioral commands with lower-level locomotion, it does not focus on refining these behaviors based on user feedback, which is critical in personalized interactions. 
 Furthermore, many robot behaviors such as differences in locomotion styles cannot be easily specified using language by a non-expert user ____**Gao, "Learning to Control Robots with Language"**____.

\noindent\textbf{Preference Learning.} 
Due to the challenges in manually specifying rewards for dynamical systems, several techniques have been developed to learn reward functions from human feedback. These methodologies encompass learning from 
physical corrections ____**Levine, "Learning Contact-Rich Manipulation Tasks with a Sim-to-Real Robot Transfer Using Diversity is All You Need"**______**Lee, "Learning Reward Functions from Human Feedback for Dexterous Manipulation"**, scalar feedback ____**Gelada, "DeepMDP: Learning Continuous Latent Space Models by Constraing the Dynamics"**______**Huang, "Inverse Reinforcement Learning with Global Goals in High-Dimensional State and Action Spaces"**, rankings ____**Choi, "Learning to Optimize via Deep Neural Networks"**______**Singh, "Near-Optimal Decision-Making Protocols with Unknown Transition Probabilities"**, and pairwise comparisons ____**Hafez, "Deep Reinforcement Learning for Efficient Multi-Robot Task Allocation"**______**Liu, "Robust Preference-Based Inverse Reinforcement Learning for Multi-Agent Systems"**. 
These works often actively generate the next query to ask from the user in order to efficiently learn the reward function. Several methods employ preference-based methods to capture complex objectives but often suffer from sample inefficiency ____**Hsu, "Active Preference Learning with Gaussian Processes"**______**Sinha, "Efficient Active Preference Learning for High-Dimensional Reward Spaces"**. 
To improve sample efficiency, several works develop active learning strategies, some with an LLM in the loop ____**Chen, "Learning to Optimize via Meta-Learning"**______**Li, "Active Learning with Deep Neural Networks and Reinforcement Learning"**, but often struggle in high-dimensional spaces ____**Kadiri, "Multi-Agent Active Preference Learning for High-Dimensional Reward Spaces"**____. Moreover, active preference learning is still slow, requiring many samples and multiple feedback iterations. On the other hand real-time systems, like those for quadruped locomotion, necessitate simple and fast feedback mechanisms. Thus, instead of focusing on how to improve feedback over multiple iterations as in active learning, we instead focus on how to generate near-optimal initial reward candidates. Concurrent to our work ____**Li, "Real-Time Reward Learning with In-Context Examples"**____ also use an LLM provided with in-context information to sample candidate reward functions, which are then evaluated by human users before being used as future in-context examples. In contrast, our work focuses on using preferences to elicit real-time behavior from a fixed multi-task policy.

In the next section, we show how we can use LLMs to do so efficiently for a real-time quadruped locomotion system. Here we focus on learning a reward vector that is initialized via parameters generated from an LLM given an initial language instruction. We then further adapt such reward functions to align with the non-expert user's preferences via standard preference learning techniques.