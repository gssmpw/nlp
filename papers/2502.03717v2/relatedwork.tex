\section{Related Work}
Our work builds on existing research in quadruped locomotion, LLMs in robotics, and preference learning. Here, we review the areas most pertinent to our approach.

\begin{figure*}[h]
  \centering
  \vspace{0.05in}
  \includegraphics[width=0.93\linewidth]{method.pdf}
  \caption{An overview of the LGPL method. A LLM is provided with in-context examples of task parameterizations $\omega$ that correspond to different quadruped gates, described in language. Given a new desired behavior, the LLM produces candidate parameterizations, which are used to rollout the policy. A user then ranks these candidates, which are eventually used for preference learning to discover the optimal $\omega$.}
  \label{fig:method}
  \vspace{-0.2in}
\end{figure*}

\noindent\textbf{Quadruped Locomotion.} A large body of work has shown that it is possible to train agile locomotion policies via approaches such as reinforcement learning (RL) capable of generating many different gaits \cite{caluwaerts2023barkour, fu2021minimizing, margolis2023walk, kumar2021rma, lee2020learning, yang2022fast}. However, such policies can be challenging to train without a sophisticated learning curriculum \cite{lee2020learning}, and gaits can be difficult to specify without manually encoding their reward parameters \cite{fu2021minimizing}. Several works propose methods for real time specification of gaits \cite{margolis2023walk, tang2023saytap}. However, these works are limited as they either do not consider how a non-expert user can tune the commands for the robot, or do not allow refinement of behaviors beyond low-resolution language feedback. On the other hand, LGPL provides an efficient means for both expert and non-expert users to rapidly tune desired gaits.

\noindent\textbf{Specifying Robot Behavior with LLMs.} A large body of recent work has focused on using LLMs to generate robot behaviors \cite{liang2023code, brohan2023can, yu2023language, mahadevan2024generative, thumm2024text2interaction, ma2023eureka}. Some works focus on using LLMs to generate long-horizon sequences of actions for robots \cite{brohan2023can, liang2023code}, and more recent work focuses on improving existing LLMs for robot-specific tasks \cite{liang2024learning}. Most relevant to our work is literature that explore generating reward functions directly from language instructions or corrections \cite{kwon2023reward, hu2023language, yu2023language}. Notably, several works use in-context learning to prompt an LLM to define reward parameters for a variety of robot tasks from high level commands such as ``sit'' or ``stand up'' \cite{yu2023language}, to more complex gait contact patterns \cite{tang2023saytap}. 
While this approach effectively bridges high-level behavioral commands with lower-level locomotion, it does not focus on refining these behaviors based on user feedback, which is critical in personalized interactions. 
 Furthermore, many robot behaviors such as differences in locomotion styles cannot be easily specified using language by a non-expert user \cite{casper2023open}. This brings us to consider how to adaptively learn reward functions from human feedback.

\noindent\textbf{Preference Learning.} 
Due to the challenges in manually specifying rewards for dynamical systems, several techniques have been developed to learn reward functions from human feedback. These methodologies encompass learning from 
physical corrections \cite{li2021learning}, scalar feedback \cite{knox2009interactively}, rankings \cite{myers2022learning, brown2019extrapolating}, and pairwise comparisons \cite{hejna2023few, lee2021pebble,sadigh2017active, ziegler1909fine}. 
These works often actively generate the next query to ask from the user in order to efficiently learn the reward function. Several methods employ preference-based methods to capture complex objectives but often suffer from sample inefficiency \cite{lee2021pebble, hejna2023few}. 
To improve sample efficiency, several works develop active learning strategies, some with an LLM in the loop \cite{peng2024preference, mahmud2024maple}, but often struggle in high-dimensional spaces \cite{sadigh2017active, biyik2018batch}. Moreover, active preference learning is still slow, requiring many samples and multiple feedback iterations. On the other hand real-time systems, like those for quadruped locomotion, necessitate simple and fast feedback mechanisms. Thus, instead of focusing on how to improve feedback over multiple iterations as in active learning, we instead focus on how to generate near-optimal initial reward candidates. Concurrent to our work \cite{yu2024few} also use an LLM provided with in-context information to sample candidate reward functions, which are then evaluated by human users before being used as future in-context examples. In contrast, our work focuses on using preferences to elicit real-time behavior from a fixed multi-task policy.

In the next section, we show how we can use LLMs to do so efficiently for a real-time quadruped locomotion system. Here we focus on learning a reward vector that is initialized via parameters generated from an LLM given an initial language instruction. We then further adapt such reward functions to align with the non-expert user's preferences via standard preference learning techniques. 

%Tuning of behaviors for individual user preferences requires real-time modification and through a simple and feedback efficient learning process. 
% To accomplish this, we focus on generating optimal initial candidate rewards - as opposed to synthesizing subsequent queries as done in the active learning literature.