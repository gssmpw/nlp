%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%% 
%%
%% $Id: elsarticle-template-num.tex 190 2020-11-23 11:12:32Z rishi $
%%
%%
\documentclass[final,3p,times,twocolumn]{elsarticle}
\usepackage[T1]{fontenc}


%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{tabularray}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{url}
\usepackage{hyperref}
\def\UrlBreaks{\do\/\do-}
\usepackage{breakurl}
%\usepackage[breaklinks, hidelinks]{hyperref}
\usepackage{comment}
 

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{arXiv}

\begin{document}

% \begin{titlepage}
%     \centering
%     % Utrecht University logo
%     \includegraphics[width=0.5\textwidth]{uu_logo.png} \\[1.5cm]
    
%     % Title
%     \Huge{Master Thesis} \\[0.5cm]
%     \LARGE{\textbf{Master Artificial Intelligence}} \\[0.5cm]
%     \Large{\textbf{Utrecht University}} \\[1cm]

%     \Large{SmaAt-FUsion and SmaAt-Krige-GNet: Deep Weather Model Augmentation
% with Auxiliary Meteorological Variables}\\[1cm]
    
%     % Author Information
%     \Large{
%     Aleksej Cornelissen} \\
%     6921701 \\[3cm]
    
%     % Supervisors
%     \textbf{Examiners:} \\
%     Dr. S. Mehrkanoon \\
%     Dr. T. Roberts \\[2cm]
    
%     % Date
%     January 28, 2025 \\[3cm]
    
%     % Spacing for alignment
%     \vfill

% \end{titlepage}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

%\title{SmaAt-FUsion and SmaAt-Krige-GNet: Deep Weather Model Augmentation with Auxiliary Meteorological Variables}


%\title{Enhancing Precipitation Nowcasting with Multi-Variable Data: SmaAt-fUsion and SmaAt-Krige-GNet Architectures}

\title{Integrating Weather Station Data and Radar for Precipitation Nowcasting: SmaAt-fUsion and SmaAt-Krige-GNet}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author{Aleksej Cornelissen}
 \author{Jie Shi}
 \author{Siamak Mehrkanoon\corref{cor1}}
\ead{s.mehrkanoon@uu.nl}
\cortext[cor1]{Corresponding author}
 
\address{Department of Information and Computing Sciences, Utrecht University, Utrecht, The Netherlands}
 
\begin{abstract}
%% Text of abstract

In recent years, data-driven, deep learning-based approaches for precipitation nowcasting have attracted significant attention, showing promising results. However, many existing models fail to fully exploit the extensive atmospheric information available, relying primarily on precipitation data alone. This study introduces two novel deep learning architectures, SmaAt-fUsion and SmaAt-Krige-GNet, specifically designed to enhance precipitation nowcasting by integrating multi-variable weather station data with radar datasets. By leveraging additional meteorological information, these models improve representation learning in the latent space, resulting in enhanced nowcasting performance. The SmaAt-fUsion model extends the SmaAt-UNet framework by incorporating weather station data through a convolutional layer, integrating it into the bottleneck of the network. Conversely, the SmaAt-Krige-GNet model combines precipitation maps with weather station data processed using Kriging, a geo-statistical interpolation method, to generate variable-specific maps. These maps are then utilized in a dual-encoder architecture based on SmaAt-GNet, allowing multi-level data integration. Experimental evaluations were conducted using four years (2016--2019) of weather station and precipitation radar data from the Netherlands. Results demonstrate that SmaAt-Krige-GNet outperforms the standard SmaAt-UNet, which relies solely on precipitation radar data, in low precipitation scenarios, while SmaAt-fUsion surpasses SmaAt-UNet in both low and high precipitation scenarios. This highlights the potential of incorporating discrete weather station data to enhance the performance of deep learning-based weather nowcasting models.


\end{abstract}



\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Weather station data \sep radar data \sep fusion \sep deep learning \sep precipitation nowcasting 
%% PACS codes here, in the form: \PACS code \sep code
%\PACS 0000 \sep 1111
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
%\MSC 0000 \sep 1111
\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}


Precipitation nowcasting refers to the short-term forecasting of rainfall intensity, typically within a timeframe ranging from a few minutes to several hours. Accurate nowcasting is crucial for various weather-dependent sectors, including flood management, transportation, energy distribution, and emergency response \cite{li2024quantitative, wilson2010nowcasting}. The effectiveness of nowcasting models depends on their ability to capture the complex, dynamic nature of precipitation patterns across different spatial and temporal scales.

Traditional precipitation nowcasting approaches rely on numerical weather prediction (NWP) methods. NWP models utilize physical atmospheric properties to generate multiple precipitation scenarios, but they are computationally expensive and often impractical for short-term forecasts due to their sensitivity to initial conditions and require high computations \cite{heye2017precipitation}.%(Heye et al., 2017). 


Recent advancements in deep learning have led to the development of advanced data-driven models that utilize historical weather data to capture complex spatiotemporal patterns \cite{mehrkanoon2019deep, ren2021deep,  vatamany2025graph}. Convolutional Neural Networks (CNNs) based models, in particular, have demonstrated promising results in precipitation nowcasting by effectively extracting spatial features from radar images \cite{ayzel2019all, ayzel2020rainnet}. Despite their success, many existing deep learning models for precipitation nowcasting primarily rely on radar-based precipitation data, often overlooking the potential benefits of integrating additional meteorological information from other sources, such as weather stations.

Some recent studies have attempted to incorporate multiple atmospheric variables into their models. For instance, the authors in \cite{kaparakis2023wf} introduced WF-UNet, a two-stream UNet architecture that processes both precipitation maps and wind speed maps to assess the impact of wind on precipitation nowcasting. While this approach leverages multi-variable meteorological data, it relies entirely on spatially continuous maps rather than integrating discrete station-based observations. In contrast, our work aims to bridge this gap by learning from both discrete weather station data and radar-based precipitation maps. Weather stations provide highly localized, multi-variable atmospheric measurements (e.g., temperature, humidity, wind speed, and pressure), offering valuable insights into weather dynamics. However, station data alone lacks spatial continuity, making it challenging to use directly in deep learning models designed for grid-based data.

To address this limitation, this study explores the potential benefits of incorporating additional meteorological data into deep learning-based nowcasting models. Specifically, we introduce two novel architectures, i.e. SmaAt-fUsion and SmaAt-Krige-GNet, that integrate multi-variable weather station data alongside precipitation radar data. The key contributions of this work are as follows:

\begin{enumerate}
    \item Introduction of a novel dataset: We have collected a comprehensive dataset containing atmospheric data from 2016-2019 for 22 weather stations in the Netherlands, each recording 8 meteorological variables. This dataset provides a rich source of information for improving precipitation nowcasting.

    \item Development of SmaAt-fUsion: This model extends the SmaAt-UNet framework \cite{trebing2021smaat} by incorporating weather station data into the bottleneck of the SmaAt-UNet model, where high-level representations are formed. 

    \item Development of SmaAt-Krige-GNet: To explicitly capture spatial relationships between weather stations, we employ the Kriging algorithm, a geo-statistical interpolation method, to generate spatially continuous maps from the collected discrete weather station data. These Kriging-derived maps serve as an additional data input to SmaAt-Krige-GNet, which extends the SmaAt-UNet model by incorporating a dual-encoder architecture. The Kriging-based stream is concatenated with the precipitation stream at multiple stages of the network, ensuring that the model learns from both data sources at various levels of abstraction.
\end{enumerate}
 


\section{Related Work}

Recent advancements in data-driven weather forecasting have increasingly leveraged deep learning techniques to improve weather elements forecasting and nowcasting \cite{10220887, 9308323, aykas2021multistream, chen2020deep, bilgin2021tent}. Various neural network architectures have been successfully applied in weather forecasting, including Recurrent Neural Networks (RNNs) \cite{singh2019weather}, Long Short-Term Memory (LSTM) networks \cite{zaytar2016sequence}, Convolutional LSTM (ConvLSTM) \cite{shi2015convolutional}, Convolutional Neural Networks (CNNs) \cite{ayzel2020rainnet}, Graph Neural Networks (GNNs) \cite{simeunovic2021spatio}, and encoder-decoder frameworks \cite{larraondo2019data}.

More recently, the UNet architecture \cite{ronneberger2015unetconvolutionalnetworksbiomedical}, originally developed for medical image segmentation, has demonstrated effectiveness in precipitation due to its encoder-decoder structure with skip connections, enabling the preservation of fine-grained spatial details. Variations of UNet have been proposed to enhance performance while maintaining computational efficiency for weather prediction. For example, SmaAt-UNet \cite{trebing2021smaat}, an enhanced version of the UNet model that integrates attention modules and depthwise-separable convolutions, substantially decreases the number of trainable parameters in the traditional UNet while maintaining its performance. SelfAttUNet \cite{nie2021self} integrates a self-attention mechanism into the UNet model, allowing it to highlight key regions in an image for more effective radar-based precipitation nowcasting.  
Several other studies, such as \cite{kaparakis2023wf, reulen2024ga, renault2023sar}, have also explored improvements to deep learning models built on the core UNet architecture, highlighting their effectiveness in weather nowcasting tasks. 

In addition to CNN-based architectures, data fusion techniques have been increasingly explored to integrate multiple meteorological data sources for enhanced precipitation nowcasting. Traditional models primarily rely on radar reflectivity data; however, recent studies have demonstrated that incorporating additional atmospheric variables, such as wind speed, temperature, and humidity, significantly improves short-term forecasting accuracy. By leveraging multiple data sources, fusion models better capture complex weather dynamics, leading to more reliable nowcasting performance. Kaparakis et al. \cite{kaparakis2023wf} proposed the Weather Fusion UNet (WF-UNet), a fusion-based deep learning model for precipitation nowcasting over Western Europe. Their findings underscore the role of fusion models in enhancing short-term precipitation predictions by leveraging complementary data sources. Similarly, Leinonen et al. \cite{leinonen2023thunderstorm} introduced a deep learning-based thunderstorm nowcasting model that integrates diverse meteorological data, including weather radar, lightning detection, satellite imagery, numerical weather prediction outputs, and digital elevation models. Furthermore, the Spatiotemporal Feature Fusion Transformer \cite{xiong2024spatiotemporal} incorporates feature-level fusion mechanisms to enhance precipitation nowcasting performance. Collectively, these works illustrate the growing impact of data fusion techniques in weather nowcasting, highlighting their potential to bridge the gap between numerical weather prediction and deep learning-based methodologies.








\section{Methodology}
\subsection{Dataset}

This study utilizes two complementary datasets: a radar-based precipitation dataset and a weather station dataset. Both datasets were obtained from the Royal Netherlands Meteorological Institute (KNMI \footnote{ \url{https://www.knmi.nl/home.}}) and are used together to improve precipitation nowcasting performance.



\noindent \textbf{Precipitation Dataset:}
The precipitation dataset, originally used by Trebing et al. \cite{trebing2021smaat}, consists of radar-based rainfall data collected over four years (2016–2019) in the Netherlands. It contains approximately 420,000 rain maps, each captured at 5-minute intervals. The data is sourced from two C-band Doppler weather radar stations located in:
De Bilt (52.10°N, 5.18°E, 44 m MSL) and Den Helder (52.96°N, 4.79°E, 51 m MSL). To generate rain maps, each radar performs four full 360° azimuthal scans at beam elevation angles of 0.3°, 1.1°, 2.0°, and 3.0° \cite{precipitation_dataset, trebing2021smaat}.

\noindent \textbf{Weather Station Dataset:}
The second dataset consists of weather station data, which provides multivariable atmospheric measurements from 22 weather stations across the Netherlands (see Fig. \ref{fig:stations_map}). This dataset is novel and contains 8 meteorological variables, selected based on availability, relevance, and consistency. The recorded variables are: Temperature, Humidity, Atmospheric Pressure, Wind Speed, Max Wind Speed, Wind Speed Deviation, Wind Direction, Wind Direction Deviation. 

\begin{figure}[!ht]
    \centering
    \frame{\includegraphics[width=0.8\linewidth]{maps2.png}}
    \caption{An overview map of all the nodes included in the weather station dataset. The dataset contains a total of 22 weather stations. Some stations share identical coordinates, resulting in 17 nodes in the figure.}
    \label{fig:stations_map}
\end{figure}

%\noindent \textbf{Preprocessing:}
%To ensure compatibility between both datasets, the precipitation maps undergo preprocessing. A 288×288 pixel cutout is taken from the center of the Netherlands, covering the area with the highest data availability. The selected geographic boundaries are: Longitude: 4.29°E to 6.73°E and
%Latitude: 51.32°N to 52.81°N. These maps are then resized to 64×64 pixels to reduce memory requirements, as both datasets are incorporated into the models. Finally, the precipitation values are normalized using the maximum precipitation value in the dataset (47.83 mm).







\noindent \textbf{Dataset Integration and Processing:}
To create a unified dataset for model input, the precipitation and weather station datasets undergo several preprocessing steps to ensure consistency and alignment. For the precipitation maps, a 288×288 pixel cutout is taken from the center of the Netherlands, covering the area with the highest data availability. The selected geographic boundaries are: Longitude: 4.29°E to 6.73°E and
Latitude: 51.32°N to 52.81°N. These maps are then resized to 64×64 pixels to reduce memory requirements, as both datasets are incorporated into the models. Finally, the precipitation values are normalized using the maximum precipitation value in the dataset (47.83 mm). The precipitation dataset contains a significant number of radar images with little to no recorded rainfall. To mitigate the risk of biasing the model toward predicting zero values, precipitation maps where fewer than 50\% of the pixels indicate rainfall are excluded. Once filtered, the remaining precipitation maps are temporally matched with the corresponding weather station data recorded at the same timestamp. Any data point that does not have a corresponding entry in the other dataset is removed, ensuring that only fully aligned data pairs are retained. To maintain consistency across variables, all weather station measurements are standardized based on their respective means and standard deviations. A challenge in merging the two datasets arises from their differing temporal resolutions. While precipitation maps are recorded at 5-minute intervals, weather station data is available at 10-minute intervals. To resolve this mismatch, the weather station dataset is duplicated to align with the precipitation timestamps, allowing for seamless integration of both sources. After preprocessing, the final dataset is divided into a training set (2016–2018) and a testing set (2019). During model training, a validation set is created by randomly selecting 10\% of the training data.

\begin{figure*}[tph]
    \centering
    \includegraphics[width=1\linewidth]{Node_U-Net.png}
    \caption{Overview of the SmaAt-fUsion architecture. Weather station data is integrated into the model by concatenating its learned representation with that of the precipitation maps at the bottleneck of the SmaAt-UNet model.}
    \label{fig:smaat_fusion}
\end{figure*}

\begin{table}[]
    \centering
        \caption{Size of each train and test set for the 4-year precipitation dataset}
    \begin{tabular}{l c c c}

        \toprule
         \textbf{Rain Threshold} & \textbf{Train Size} & \textbf{Test Size} & \textbf{Subset} \\
         \midrule
          $0\%$ (Original) &  314940 & 105003 & 100\%  \\
         $50\%$ & 5734 & 1557 & 1.74\% \\
         \bottomrule
    \end{tabular}
    \label{tab:my_label}
\end{table}

\subsection{Proposed models}

In the following, we introduce two architectures designed to extend the core SmaAt-UNet model \cite{trebing2021smaat} to learn from both weather station data and precipitation maps. The key difference between these models lies in how they integrate weather station data into SmaAt-UNet, which primarily operates on gridded precipitation maps. Both models take 12 precipitation maps as input, representing one hour of weather radar data. In addition to these maps, they incorporate station data using different integration strategies. The output of each model is a single precipitation map predicting conditions 30 minutes after the last input frame.





\begin{figure*}[] % [t] aligns the figure at the top of the page
    \centering
    % Subfigure 1
    \begin{subfigure}[]{1\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Kriging_Overview.png}
        \caption{Overview of the Kriging process. For each variable, a Kriging map is generated for each timestep. Subsequently, 12 timesteps are fed into the SmaAt-Krige-GNet model.}
        \label{fig:gnet_overview}
    \end{subfigure}
    \vspace{1em} % Add vertical space between subfigures
    % Subfigure 2
    \begin{subfigure}[]{1\textwidth}
        \centering
        \includegraphics[width=\linewidth]{SmaAt-Krige-GNet.png}
        \caption{The SmaAt-Krige-GNet model. The upper stream processes Kriging data and mirrors the layers of the bottom stream, which handles precipitation data. At each level, the output of the upper stream is concatenated channel-wise with the corresponding output of the bottom stream.}
        \label{fig:gnet}
    \end{subfigure}
    \caption{An illustration of the Kriging process and Overview of the SmaAt-Krige-GNet architecture.}
    \label{fig:combined_overview}
\end{figure*}

\subsection{SmaAt-fUsion model}



In SmaAt-fUsion, our aim is to integrate weather station data into the core SmaAt-Unet model \cite{trebing2021smaat}, which operates on gridded precipitation radar maps. To this end, following the approach of \cite{mehrkanoon2019deep}, we exploit the spatio-temporal structure of station data by transforming each regressor vector, containing historical weather variable measurements from all stations, into a tensor with dimensions (stations = 22, variables = 8, lags = 12). This tensor is processed through a double 3D depthwise separable convolution block, consisting of a depthwise convolution followed by a pointwise convolution. Compared to classical 3D convolutions, this decomposition significantly reduces the number of parameters while preserving spatial and temporal correlations in the data. The output from this convolutional block is then passed through a 3D Adaptive Max Pooling layer, which reshapes it to match the bottleneck tensor size of the original SmaAt-Unet model. Finally, the pooled representation is concatenated with the bottleneck tensor, as illustrated in Fig. \ref{fig:smaat_fusion}. By integrating station data, we enrich the model's latent space with additional insights from weather station observations. This fusion strengthens the model’s ability to better capture complex spatio-temporal dependencies, ultimately improving its performance in precipitation mapping.





\subsection{SmaAt-Krige-GNet model}



In this work, we explicitly integrate spatial dependencies between weather stations into the SmaAt-UNet model. To achieve this, we first generate grid-based maps of station variables using Kriging \cite{Isaaks1989, Matheron1963}. Inspired by \cite{reulen2024ga}, which introduced SmaAt-GNet as a generator within an adversarial learning framework for precipitation nowcasting, we propose SmaAt-Krige-GNet. This model combines the grid-based maps derived from station data with precipitation maps. The key distinction between SmaAt-GNet and the standard SmaAt-UNet model lies in the addition of an extra encoder stream. This new stream mirrors the encoder layers of SmaAt-UNet and concatenates its representations at each level with those of the original SmaAt-UNet encoder. While the original SmaAt-GNet in \cite{reulen2024ga} was designed to integrate binary precipitation masks into the SmaAt-UNet model, our proposed SmaAt-Krige-GNet instead utilizes Kriging-based spatial maps. By structuring the model into two independent streams rather than stacking all inputs channel-wise in a single-stream architecture, SmaAt-Krige-GNet can independently learn hidden features from both weather and precipitation data. Since all models process 12 input images, we modify the secondary input of SmaAt-GNet to also accept 12 images. Given that there are 8 weather variables, the resulting input forms a 4D tensor of size $64\times64\times8\times12$, which is then flattened into a $64\times64\times96$ layer, as illustrated in Fig. \ref{fig:gnet}. To explicitly incorporate spatial data, Kriging is applied to interpolate weather station measurements into 2D geospatial maps, ensuring that each weather variable at each timestep is represented in the same spatial resolution as the precipitation maps.



\begin{table}[h!]
    \centering
        \caption{Parameter size of each model}
    \begin{tabular}{l c c}
         \toprule
         \textbf{Model} & \textbf{Parameters} & \textbf{Relative Size} \\
         \midrule
          SmaAt-UNet & 4.0M & 1x  \\
          SmaAt-fUsion & 5.6M & 1.4x \\
          SmaAt-Krige-GNet & 11.6M & 2.9x \\
         \bottomrule
    \end{tabular}
    \label{tab:parameters}
\end{table}

\begin{table*}[tht]
\centering
\caption{Model performance metrics across different precipitation thresholds for the studied dataset.}
\begin{tabular}{c c c c c c c}
\toprule
\textbf{Threshold} & \textbf{Model} & \textbf{MSE (pixel) $\downarrow$}  & \textbf{F1 Score $\uparrow$} & \textbf{CSI $\uparrow$} & \textbf{HSS $\uparrow$} & \textbf{MCC $\uparrow$} \\
\midrule
$\geq 0.5 \text{ mm/h}$ 
  & Persistence         & 0.065986                      & 0.678436          & 0.513359          & 0.241484          & 0.483398          \\
                     & SmaAt\_UNet         & 0.043293                      & 0.763476 & 0.617437 & 0.299234 & 0.601438          \\
                     & SmaAt-fUsion & 0.035778 & 0.768003 & 0.623381 & 0.297824 & 0.604511 \\
                     & SmaAt-Krige-GNet & \textbf{0.035618} & \textbf{0.780774} & \textbf{0.640385} & \textbf{0.315630} & \textbf{0.633025} \\
\midrule
$\geq 10 \text{ mm/h}$     
    & Persistence         & -                                   & 0.117185          & 0.062239          & 0.056982          & 0.113965          \\
                     & SmaAt\_UNet         & -                                   & 0.144665          & 0.077972          & 0.071221          & 0.148874          \\
                     & SmaAt-fUsion & - & \textbf{0.187187} & \textbf{0.103258} & \textbf{0.092433} & \textbf{0.189564} \\
                     & SmaAt-Krige-GNet & - & 0.052808 & 0.027120 & 0.026092 & 0.091192 \\
\midrule
$\geq 20 \text{ mm/h}$     
    & Persistence         & -                                    & 0.033757          & 0.017169          & 0.016623          & 0.033265          \\
                     & SmaAt\_UNet         & -                                   & 0.047089          & 0.024113          & 0.023404          & 0.051688          \\
                     & SmaAt-fUsion & - & \textbf{0.052607} & \textbf{0.027014} & \textbf{0.026190} & \textbf{0.061962} \\
                     & SmaAt-Krige-GNet & - & 0.029783 & 0.015117 & 0.014851 & 0.054041 \\
\bottomrule
\end{tabular}
\label{tab:performance}
\end{table*}




\section{Experimental setup and evaluation}
\subsection{Model setup}
The models were implemented and trained using PyTorch Lightning. The training process used Adam Optimizer with a learning rate of 0.001. To avoid overfitting, early stopping is applied with a maximum patience of $ES = 12$ to monitor the loss of validation. In addition, we apply learning rate scheduling with a patience of $LR = 8$ epochs, which reduces the learning rate when no improvement in validation loss was observed.



The models were trained on the dataset for a maximum of 200 epochs, with a batch size of 16. These hyperparameters were chosen based on preliminary experimentation to balance model performance and computational efficiency. The implementation
of our models is available at GitHub \footnote{\url{https://github.com/JieJieNiu/SmaAt-fUsion-and-SmaAt-Krige-GNet}}.

To generate the Kriging maps for SmaAt-Krige-GNet, we use Ordinary Kriging from PyKrige \cite{pykrige}. A grid is defined with the same resolution as the precipitation maps, specifically $64 \times 64$. Subsequently, a variogram is fitted using the spherical model, which has been shown to be suitable for meteorological applications \cite{Verworn2011, CSEGRecorderVariograms}. To ensure accurate spatial representation, we specify the coordinate type as geographic, allowing the model to directly use the latitude and longitude of each weather station. However, Kriging with PyKrige can be numerically unstable when using its default inverse matrix algorithm. To mitigate this, we set the `pseudo\_inv' parameter of Ordinary Kriging to True, which stabilizes computations at the cost of increased processing time \cite{pykrige}. Once the linear system is solved, a set of weights $\lambda_i$ is obtained, which are then applied to interpolate the Kriging value at each coordinate within the grid. This process generates spatially continuous weather variable maps that align with the resolution of the precipitation data.



\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Kriging_example.png}
    \caption{Example Kriging map input of each variable}
    \label{fig:kriging_example}
\end{figure}






\subsection{Evaluation}

The loss function utilized in this study is the Mean Squared Error (MSE), which quantifies the difference between the predicted and ground truth images. In addition to MSE, and in line with previous works \cite{reulen2024ga, trebing2021smaat, stanczyk2021deep, van2023improving}, we employ a set of widely used binary metrics for precipitation nowcasting, including the F1-Score, Critical Success Index (CSI), Heidke Skill Score (HSS), and Matthews Correlation Coefficient (MCC), to assess the performance of the proposed models. Since precipitation maps contain continuous values, they must be thresholded to convert them into binary masks before applying these metrics. In this study, we evaluate precipitation at three different thresholds: 0.5 mm/h, 10 mm/h, and 20 mm/h. Applying a threshold to the predicted and target images produces a binary mask, where each pixel is assigned 1 if its value exceeds the threshold (rain) and 0 otherwise (no rain). From this, one can calculate the true positives (TP) (prediction=1, target=1),
false positives (FP) (prediction=1, target=0), true negatives (TN)
(prediction=0, target=0) and false negatives (FN) (prediction=0,
target=1). Using thresholds, the models’ performance can be assessed across varying precipitation intensities. The 0.5 mm/h threshold is commonly used as it effectively captures light rain \cite{tan2018evaluation}. Additionally, 10 mm/h and 20 mm/h thresholds are tested to evaluate the models’ ability to predict moderate to extreme precipitation events. The binary metrics CSI, HSS and MCC are computed as follows:
\begin{equation}
    \begin{aligned}
 \small
        %\textbf{Recall}     &= \frac{\text{TP}}{\text{TP} + \text{FN}} \\
       % \textbf{Precision}   &= \frac{\text{TP}}{\text{TP} + \text{FP}} \\
       % \textbf{F1}          &= 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \\
        \textbf{CSI}         &= \frac{\text{TP}}{\text{TP} + \text{FP} + \text{FN}}, \\
        \textbf{HSS}         &= \frac{2 \cdot (\text{TP} \cdot \text{TN} - \text{FP} \cdot \text{FN})}{(\text{TP} + \text{FN}) \cdot (\text{FN} + \text{TN}) + (\text{TP} + \text{FP}) \cdot (\text{FP} + \text{TN})}, \\
        \textbf{MCC}         &= \frac{\text{TP} \cdot \text{TN} - \text{FP} \cdot \text{FN}}{\sqrt{(\text{TP} + \text{FP})(\text{TP} + \text{FN})(\text{TN} + \text{FP})(\text{TN} + \text{FN})}}.
    \end{aligned}
\end{equation}
The CSI evaluates the overlap between predicted and observed precipitation events \cite{jolliffe2003}. It accounts for false positives and missed events, providing a robust measure for precipitation predictions \cite{hogan2012}. The HSS compares the model predictions to a random baseline \cite{hogan2012}. HSS highlights the skill of the model beyond chance-level prediction \cite{wilks2011}. Unlike accuracy, which can be misleading in imbalanced datasets, the MCC score provides a more reliable measure by considering both classes (positive and negative). The MCC ranges from -1 (perfect inverse correlation) to +1 (perfect correlation), with 0 indicating no better than random predictions. This metric is particularly useful when the data is highly imbalanced, as it accounts for both the correct and incorrect predictions across all categories \cite{mcc}. In the case of precipitation nowcasting, the present imbalance is the high amount of negative precipitation events compared to positive precipitation events.



\section{Results and Discussion}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{input12.png}
    \caption{Example precipitation input of the dataset on 2019-12-14.}
    \label{fig:input12}
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{output.png}
    \caption{Example outputs of each model, using the input seen in Fig. \ref{fig:input12}} %specify more}
    \label{fig:example-outputs}
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{output_threshold.png}
    \caption{The same outputs as seen in Fig. \ref{fig:example-outputs}, after applying a minimum threshold of 0.5mm/hour. The scaling has been changed compared to Fig. \ref{fig:example-outputs} for improved visibility. The outputs still represent a 5 minute interval.}
    \label{fig:example-threshold}
\end{figure*}

After training, models were selected based on the lowest validation loss to ensure optimal generalization. The obtained results are presented in Table \ref{tab:performance}. These results were computed after denormalizing the data, following the approach used in similar studies \cite{reulen2024ga, trebing2021smaat}.

The results in Table \ref{tab:performance} show that the proposed SmaAt-fUsion model outperforms the classical SmaAt-UNet across all tested thresholds. This suggests that incorporating station data effectively guides the SmaAt-UNet core model in better learning atmospheric dynamics. Furthermore, the SmaAt-fUsion model achieves superior performance compared to SmaAt-Krige-GNet at higher rainfall thresholds of 10 mm/h and 20 mm/h. However, for low-intensity rainfall, SmaAt-Krige-GNet performs best among the examined models. This indicates that integrating additional weather variables at multiple levels of the SmaAt-UNet encoder enables the model to extract more detailed information from the augmented weather station data, compared to the SmaAt-fUsion model.

At higher rainfall thresholds (10 mm/h and 20 mm/h), SmaAt-Krige-GNet underperforms relative to both SmaAt-fUsion and the core SmaAt-UNet in terms of binary evaluation metrics. This performance decrease is likely due to the model’s tendency to over-smooth predictions, primarily a consequence of incorporating Kriging-generated maps. As shown in Fig. \ref{fig:kriging_example}, these maps inherently introduce smoothness. Since weather stations are sparsely distributed over a large area, the interpolated values in the Kriging maps may not fully capture the fine-scale variations in actual weather conditions. A potential solution to this limitation would be a significant increase in the number of weather stations, which could improve the resolution and accuracy of the interpolated Kriging maps.



In Fig. \ref{fig:example-outputs}, an example output of each model can be observed compared to the ground truth. Figure \ref{fig:example-threshold} displays the predictions after applying a 0.5 mm/h threshold, with the southeast region closely matching the ground truth in the SmaAt-Krige-GNet model. Interestingly, only the SmaAt-fUsion model captures the slight precipitation in the north, indicating the presence of information missing in SmaAt-UNet and potentially overlooked by SmaAt-Krige-GNet due to smoothing. 


\section{Conclusion}

This study introduced two novel deep learning models, SmaAt-fUsion and SmaAt-Krige-GNet, designed to improve precipitation nowcasting by integrating meteorological variables from weather stations alongside radar data. The experimental results demonstrate that both models have distinct advantages: SmaAt-Krige-GNet excels in low-intensity rainfall predictions by incorporating weather station data through Kriging-based spatial interpolation, while SmaAt-fUsion outperforms in high-intensity events by directly integrating station data into the network’s bottleneck. These findings highlight the potential of leveraging both discrete weather station data and spatially interpolated variables to improve deep learning-based precipitation forecasting.



%\section{Acknowledgement}
%This work used the Dutch national e-infrastructure with the support of the SURF Cooperative using grant no. EINF-11293.



 \bibliographystyle{elsarticle-num} 
 \bibliography{references}


\end{document}

