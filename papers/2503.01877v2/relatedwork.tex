\section{Related Work}
JSSP with more than two machines is proven to be NP-hard \cite{garey1976complexity}. As a result, finding exact solutions for JSSP is generally infeasible, leading to the widespread use of heuristic and approximate methods for practical efficiency \cite{jssp_np}.
Traditional approaches to solving JSSP have primarily relied on search and inference techniques developed by the constraint programming community \cite{Beck2010JobShop}. These techniques effectively leverage constraints to define the relationships and limitations between jobs and resources, enabling efficient exploration of feasible solution spaces and the identification of optimal or near-optimal schedules \cite{Nowicki2005TabuSearch}. A widely used heuristic method in real-world scheduling systems is the Priority Dispatching Rule (PDR) \cite{PDR_Zahmani2015JobShop}. PDRs are simple and effective, although designing an efficient PDR is time-consuming and requires extensive domain knowledge.

Recently, approaches utilizing Deep Learning and Neural Networks have gained attention for finding promising solutions to the JSSP \cite{Bonetta2023JobShop, L2D, SelfJSP}. These methods can be broadly categorized into supervised learning and reinforcement learning (RL). Current research in deep reinforcement learning (DRL) is actively focused on developing advanced methods to tackle JSSP. Existing DRL methods typically represent JSSP as a Markov Decision Process (MDP) and learn a policy network based on DRL techniques\cite{L2D}. 

Large language models (LLMs) are now being applied to a wider range of tasks beyond language processing. In areas like robotics and planning \cite{huang2022language}. While there are currently no papers that directly address the scheduling of Job Shop Scheduling Problems (JSSP) using LLMs, some notable works explore the potential of LLMs in mathematical reasoning and programming \cite{chen2023program,wei2022chain_of_thaught,ahn2024large,opro}.
Optimization using LLMs has gained significant interest in recent years, with several works exploring their capabilities across various domains \cite{opro}. The ability of LLMs to understand and generate natural language has opened new possibilities for optimization tasks that were traditionally solved using derivative-based algorithms or heuristic methods\cite{opro}. \cite{chen2023program} evaluated LLMs' performance in mathematical problem-solving and introduced "Program of Thoughts" (PoT) prompting. Unlike Chain of Thoughts (CoT) \cite{wei2022chain_of_thaught}, which combines reasoning and computation, PoT generates reasoning as code statements and delegates computation to an interpreter.
\cite{ahn2024large} surveys mathematical problems and datasets studied with LLMs, analyzing their strengths and weaknesses. \cite{frieder2023large} examines LLMs' impact on mathematicians, exploring their role in research, education, problem-solving, and proof generation, offering a balanced view of their capabilities.Recent works \cite{opro} explore LLMs as optimizers, using prompts to refine solutions iteratively. Case studies on linear reeeession and the traveling salesman problem show LLMs can produce high-quality solutions, sometimes matching heuristic algorithms in small-scale scenarios.
Explorations into using LLMs for graph learning tasks have yielded notable approaches.  \cite{huang2022language} noted that LLMs exhibit some initial graph reasoning capabilities, but their performance decreases with problem complexity, \cite{huang2022language} introduced prompting strategies to improve LLMs graph reasoning. \cite{valmeekam2022large} developed a benchmark for assessing the planning and reasoning abilities of LLMs. More recently, \cite{chen2024exploringpotentiallargelanguage} examined the use of LLMs for graph node classification tasks. \cite{chen2024exploring} introduces two pipelines: LLMs-as-Enhancers, where LLMs refine textual data for Graph Neural Networks (GNNs), and LLMs-as-Predictors, where LLMs generate predictions directly from graph structures in natural language. Additionally, \cite{zhao2024graphtext} proposed GRAPHTEXT, translating graphs into natural language for training-free reasoning, often matching or exceeding GNNs. These works highlight the potential of LLMs in graph-related tasks, but their application to scheduling problems remains largely unexplored.