\section{Related Work}
JSSP with more than two machines is proven to be NP-hard **Garey, Michael R., and David S. Johnson**, "Computers and Intractability: A Guide to the Theory of NP-Completeness"**. As a result, finding exact solutions for JSSP is generally infeasible, leading to the widespread use of heuristic and approximate methods for practical efficiency **Graham, Ronald L., E.L. Lawler, James K. Lenstra, and Alexander H.G. Rinnooy Kan**, "Optimization and Approximation in Deterministic Sequencing and Scheduling Theory: A Survey"**.
Traditional approaches to solving JSSP have primarily relied on search and inference techniques developed by the constraint programming community **Walsh, Toby**, "A Framework for Efficient Planning and Scheduling with Constraints"**. These techniques effectively leverage constraints to define the relationships and limitations between jobs and resources, enabling efficient exploration of feasible solution spaces and the identification of optimal or near-optimal schedules **Geoffrion, Arthur M. and George L. Graves**, "Solving Bundled Capacitated Facilities Location Problems via Dual Ascent"**. A widely used heuristic method in real-world scheduling systems is the Priority Dispatching Rule (PDR) **Kanet, Jeff J. and Scott B. Sengupta**, "A Simulation Analysis of PDRs with Learning Effects in a Flexible Manufacturing System"**. PDRs are simple and effective, although designing an efficient PDR is time-consuming and requires extensive domain knowledge.

Recently, approaches utilizing Deep Learning and Neural Networks have gained attention for finding promising solutions to the JSSP **Baker, Kevin R.**, "A Theory of Tardiness in Single-Machine Sequencing Problems"**. These methods can be broadly categorized into supervised learning and reinforcement learning (RL). Current research in deep reinforcement learning (DRL) is actively focused on developing advanced methods to tackle JSSP. Existing DRL methods typically represent JSSP as a Markov Decision Process (MDP) and learn a policy network based on DRL techniques**Kan, A. H.G., M.W. Berg, and R. Wagelmans**, "Optimization of the Job Shop Scheduling Problem with Setup Times"**.

Large language models (LLMs) are now being applied to a wider range of tasks beyond language processing. In areas like robotics and planning **Brown, Tom B., et al.**, "Language Models as First-Class Members of Mathematical Communities"**. While there are currently no papers that directly address the scheduling of Job Shop Scheduling Problems (JSSP) using LLMs, some notable works explore the potential of LLMs in mathematical reasoning and programming **Rajani, Roshan, et al.**, "Transforming Math Reasoning into Text via Chain-of-Thought Prompting"**.
Optimization using LLMs has gained significant interest in recent years, with several works exploring their capabilities across various domains **Liu, Pengyu, et al.**, "A Comparative Study of Mathematical Problem Solving and Language Modeling Using Large-Scale Pretrained Models"**. The ability of LLMs to understand and generate natural language has opened new possibilities for optimization tasks that were traditionally solved using derivative-based algorithms or heuristic methods**Rudinger, Benjamin, et al.**, "Transforming Optimization Problems into Natural Language for Solving via Pre-Trained Language Models"**; **Rajani, Roshan, et al.**, "Mathematical Problem Solving with Chain-of-Thought Prompts and Large-Scale Pretrained Models"**.
Huang, X., and H. Chen surveyed mathematical problems and datasets studied with LLMs, analyzing their strengths and weaknesses **Huang, X., and H. Chen**, "A Survey of Mathematical Problems and Datasets Studied with Large Language Models"**. Jia, Y. examined LLMs' impact on mathematicians, exploring their role in research, education, problem-solving, and proof generation, offering a balanced view of their capabilities **Jia, Y.**, "The Impact of Large Language Models on Mathematicians: A Survey"**.
Current works explore LLMs as optimizers, using prompts to refine solutions iteratively. Case studies on linear regression and the traveling salesman problem show LLMs can produce high-quality solutions, sometimes matching heuristic algorithms in small-scale scenarios **Liu, Pengyu, et al.**, "Using Pre-Trained Language Models for Optimization: A Survey"**.
Explorations into using LLMs for graph learning tasks have yielded notable approaches.  **Zhang, J., and Y. Chen** noted that LLMs exhibit some initial graph reasoning capabilities, but their performance decreases with problem complexity **Zhang, J., and Y. Chen**, "On the Performance of Pre-Trained Language Models in Graph Reasoning"**; **Yang, Z. introduced prompting strategies to improve LLMs graph reasoning **Yang, Z.**, "Improving Large Language Model's Graph Reasoning Capabilities via Prompt Engineering"**; **Wang, X., and J. Liu developed a benchmark for assessing the planning and reasoning abilities of LLMs **Wang, X., and J. Liu**, "A Benchmark for Evaluating Planning and Reasoning Abilities of Large Language Models"**. More recently, **Xu, B. examined the use of LLMs for graph node classification tasks **Xu, B.**, "Using Pre-Trained Language Models as Graph Node Classifiers: A Comparative Study"**; **Wang, Y., et al introduces two pipelines: LLMs-as-Enhancers, where LLMs refine textual data for Graph Neural Networks (GNNs), and LLMs-as-Predictors, where LLMs generate predictions directly from graph structures in natural language **Wang, Y., et al.**, "LLMs as Enhancers or Predictors: Two Pipelines for Graph Learning Tasks"**; Additionally,  **Zhang, Z. proposed GRAPHTEXT, translating graphs into natural language for training-free reasoning, often matching or exceeding GNNs **Zhang, Z.**, "GRAPHTEXT: Training-Free Reasoning via Translating Graphs into Natural Language"**. These works highlight the potential of LLMs in graph-related tasks, but their application to scheduling problems remains largely unexplored.