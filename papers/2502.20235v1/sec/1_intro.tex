\section{Introduction}
% Task and application

Synthesizing new images with visual elements, such as the style or texture, of an example image, is a long-standing yet challenging problem in computer graphics and vision.
The key challenge lies in properly representing images' texture or style features. 
Traditional methods~\cite{Efros:2001, kwatra2003, kwatra2005, patchmatch, imagemelding, PG15, Self-tuning, EG17} usually define textures as repeated local patterns and synthesize new textures by copying local patches from the source image.
When it comes to style, an extensive yet more abstract visual characteristic than texture, new representations are required. 

Thanks to the deep learning revolution, neural representations of visual features have emerged. One group of approaches performs texture or style-specific synthesis by matching the global distribution of deep features between the reference and the output. For example, the seminal work Gram loss~\cite{Gatys2015, gaty2016styletransfer} regards the feature maps' statistics as the texture/style representation. Some other work optimizes deep features by minimizing Wassertein distance~\cite{heitz2021sliced} or adversarial discrimination loss~\cite{TexExp,shaham2019singan, InGAN}. However, matching global distributions lacks local perception, usually leading to conspicuous detail artifacts. Another group retakes the local patch strategy, optimizing output's deep features through nearest-neighbor matching~\cite{LiChuanandWand2016, mechrez2018contextual, zhou2023neural}. As local patches are unaware of global structure, these methods always require additional structural guidance.

%methods for texture synthesis and style transfer. Now, content and style are deriving not only from images but also from textual descriptions, thus significantly enhancing creative possibilities while introducing new challenges. 

Beyond the above progress, recent breakthroughs in large-scale diffusion models have sparked new representations for visual features, reaching a good balance between global and local consistency in synthesis~\cite{cao2023masactrl, hertz2024stylealigned, jeong2024visual, alaluf2024cross, chung2024styleid, zhou2024generating}. 
A key consensus of these techniques is that the \emph{keys} ($K$) and \emph{values} ($V$) from the self-attention module of pretrained diffusion models characterize the appearance of the exemplar. Re-aggregating these features according to target \emph{queries} ($Q$) by self-attention mechanism during new image generation can reproduce the visual characteristics of the reference. Although impressive results were achieved due to the robust performance of diffusion models, these methods still often suffer issues like insufficient stylization. We hypothesize three reasons for their limitations:

1) \emph{Domain gap}. When two images differ significantly, the similarity between the target $Q$ (queries of the synthesized image) and the source $KV$ (keys and values of the exemplar) will become low and unreliable, leading to erroneous aggregation results. Techniques like AdaIN~\cite{AdaIn} and attention scale can partially mitigate this issue~\cite{hertz2024stylealigned, alaluf2024cross}.

2) \emph{Error accumulation}. While the iterative sampling process in diffusion models can ameliorate large discrepancies between the target $Q$ and the source $KV$, errors may also accumulate. As demonstrated in~\cite{tang2023emergent}, features from different layers of diffusion models focus on distinct information, such as semantics and geometry. Incorrect matches will propagate errors to subsequent layers along the Markov chain and degrade the final image quality.

%\emph{Error accumulation}. While the iterative sampling process in diffusion models can ameliorate large discrepancies between the target Q and the source KV, it may also accumulate errors, degrading the final image quality. %is chain-like, with dependencies between layers and time steps. While this process
 %is chain-like, with dependencies between layers and time steps. While this process

3) \emph{Architectural limit}. The self-attention mechanism is implemented within the residual branch of the denoising network. Injecting self-attention features from the source may have a bounded influence on the target latent code, potentially diminishing their efficacy in the synthesis.

%(queries of the generated image) (keys and values of the examplar)
In this work, we still follow the assumption that the self-attention features in the denoising networks capture an image's visual appearance. To address the above-mentioned limitations, we introduce a novel \emph{Attention Distillation} (AD) loss, based on which we directly update the synthesized image through backpropagation. Specifically, we consider the output obtained by computing the attention between the target $Q$ and the source $KV$ as the \emph{ideal} stylized result, and the original attention output represents the current stylization. We define attention distillation loss as the L1 distance between these two outputs and optimize the synthesized image through backpropagation in latent space. We simultaneously calculate the differences across various layers to avoid error accumulation. Such an optimization process gradually reduces the disparity between the target $Q$ and the source $KV$, enhancing the accuracy of similarity calculations and thus improving the final stylization.

Whereas our approach differs greatly from previous works that use self-attention features as plug-and-play attributes, the new attention distillation loss can also be integrated into the sampling process of diffusion models, functioning as an \emph{improved Classifier Guidance}. Combined with the normal Classifier-Free Guidance, it further enables text-based controlled generation, and is also compatible with other conditioning technologies such as ControlNet, leading to broad image synthesis applications; see, \eg, Fig.~\ref{fig:teaser}. Extensive experiments and comparisons with state-of-the-art methods have demonstrated our advantages. %in various example-based style or texture synthesis tasks. 

In summary, our main contributions are as follows:
\begin{itemize}
    \item We analyze the limitations of previous plug-and-play attention features methods and propose a novel attention distillation loss for reproducing the visual characteristics of a reference image, achieving notably superior results.  %compared to existing approaches.
    \item We develop attention distillation guided sampling, an improved Classifier Guidance that integrates attention distillation loss into the denoising process, which significantly accelerates synthesis speed and enables a wide range of visual characteristics transfer and synthesis applications. %Combined with normal Classifier-Free Guidance, it enables text-based controlled generation and is also compatible with other technologies such as ControlNet and IP-Adapter, demonstrating a wide range of applications.
\end{itemize}



%Texture synthesis and style transfer represent significant and challenging research areas within computer vision and computer graphics. 
%Texture synthesis aims to generate similar and continuous images that share visual characteristics with a given reference texture. Neural style transfer, initially proposed by Gatys et al.\cite{gaty2016styletransfer}, seeks to transfer the style of one image onto another while preserving the content, thus creating novel artistic works. Style transfer can be viewed as an extension of texture synthesis, and both techniques have extensive applications in image processing, artistic creation, and the film and gaming industries.

% Technical challenge for previous methods (围绕我们解决了的technical challenge展开讨论。Technical challenge包括limitation和technical reason)
%such as Gram matrix loss, histogram loss, sliced Wasserstein distance loss, and adversarial loss optimize parameters by minimizing the distributional discrepancy between synthesized and example images. These methods perform well in general texture synthesis and style transfer. Nevertheless, they 
%like CNNMRF and Contextual loss, utilize pretrained VGG, a classification network, to extract features, seeking the nearest neighbors for each pixel or patch in feature space, with Euclidean distance or similarity as the optimization objective. 
%Deep learning-based texture synthesis and style transfer algorithms typically rely on well-defined loss functions as supervisory signals to optimize model parameters through backpropagation. However, existing losses often struggle to achieve a satisfactory balance between global and local consistency. Distribution matching losses~\cite{gaty2016styletransfer, li2017demystifying, risser2017stable, heitz2021sliced, shaham2019singan} lack semantic correspondence, often resulting in unreasonable synthesis when handling semantic style transfer or non-stationary textures. Nearest-neighbor matching losses\cite{LiChuanandWand2016, mechrez2018contextual}, leveraging the semantic information from deep neural networks, significantly enhance semantic style transfer. However, they often neglects global distributional consistency. When applied to texture synthesis, these methods frequently produce noticeable repetition. Furthermore, the VGG network's inherent limitations in semantic understanding impede accurate semantic matching when content and style images differ substantially.

%Recent years have witnessed breakthrough advancements in diffusion models for image generation, sparking widespread interest among researchers. This progress has led to the flourishing of diffusion model-based techniques for style transfer and texture synthesis. The scope of style transfer has expanded, now deriving content and style not only from images but also from textual descriptions, thus significantly enhancing creative possibilities while introducing new challenges. 
%One category of these methods employs KV-injection\cite{cao2023masactrl, hertz2024style, jeong2024visual, alaluf2024cross, chung2024style, zhou2024generating} to maintain appearance or style consistency. These approaches extract keays (K) and values (V) of the exemplar from self-attention module in pretrained diffusion model. And re-aggregating these features by self-attention mechanism during new image generation to ensure visual consistency. Leveraging the robust performance of diffusion models, these methods have achieved impressive results. Nevertheless, they often suffer from insufficient stylization, necessitating additional techniques such as AdaIN, copying, shuffling of latents during sampling, or attention scale to enhance visual quality. We hypothesize three potential reasons for the limitations of these methods:

% 1) When the content and style images differ significantly, the similarity computed between Q from the synthesized image and K from the example image may be uniformly low, leading to erroneous aggregation results. Techniques like AdaIN and attention scale can partially mitigate this issue.

% 2) The sampling process in diffusion models is chain-like, with dependencies between layers and time steps. While this process can ameliorate large discrepancies between querys (Q) and keys (K), it may also accumulate errors, ultimately degrading image quality.

% 3) The self-attention mechanism's implementation within the residual branch of network limits the full potential of KV-injection. This architectural constraint impedes the complete manifestation of KV-injection's effects, potentially diminishing its efficacy in style transfer and texture synthesis tasks.

% 介绍解决challenge的our pipeline

%In this work, we introduce a novel style loss—attention distillation loss—based on KV-injection, ingeniously leveraging the model's attention mechanism. Through backpropagation, we directly apply the effects of KV-injection to the synthesized image, fully exploiting the potential of the attention mechanism. Utilizing the diffusion model's inherent understanding of image style and semantics, we achieve a balance between global and local consistency, resulting in optimal visual effects. Specifically, we extract image features from the self-attention module of a pretrained diffusion model. We consider the output obtained by computing attention between the querys (Q) from the generated image and the keys (K) and values (V) from the example image as the ideal stylized result, while the original attention output represents the unstylized result. We define the style loss as the L1 distance between these two outputs and optimize the synthesized image through backpropagation in latent space. This approach simultaneously calculates differences across various layers, avoiding error accumulation between layers and directly updating the synthesis result through backpropagation. The optimization process gradually reduces the disparity between Q and K, enhancing the accuracy of similarity calculations and ultimately yielding the desired visual effect.

% In summary, our main contributions are as follows:
% \begin{itemize}
%     \item We propose a novel attention distillation loss, overcoming the stylization limitations of traditional KV injection methods and achieving superior results in texture synthesis and style transfer tasks compared to existing approaches.
%     \item We develop an Improved Classifier Guidance method that integrates attention distillation loss into the model sampling process, accelerating synthesis speed. This method, combined with Classifier-Free Guidance, enables text-based controlled generation and is compatible with technologies such as ControlNet and IP-Adapter, demonstrating broad application prospects.
% \end{itemize}

% Experiment
% Contributions

% \yangc{Below are useless texts...}

% Image style transfer and texture synthesis have emerged as compelling applications within the domain of computer vision, enabling the transformation of images by adopting stylistic features from other sources. Traditionally, techniques such as the Gram Matrix have been utilized to capture style representations, facilitating the synthesis of new images that blend content with desired aesthetics. However, recent advancements in neural architectures, particularly diffusion networks, have revealed significant limitations in these conventional methods. 

% Diffusion networks operate on the principle of progressive denoising, allowing for nuanced image generation through iterative refinement. Yet, when integrating Gram Matrix-based style representations, we have observed a pronounced discrepancy in performance. The inherent characteristics of diffusion networks—namely their reliance on attention mechanisms—remain underexplored in the context of style transfer, leading us to question the adequacy of established representations for these modern models.

% In this work, we propose a novel style representation derived from the attention mechanisms within diffusion networks. By leveraging attention maps, which capture the relationships and importance of different image regions, we can create a more adaptive and context-aware representation of style. This approach not only addresses the shortcomings of previous techniques but also aligns more closely with the operational dynamics of diffusion networks. Our methodology allows for richer texture synthesis and more compelling style transfer results by effectively harnessing the intricacies of attention.

% Furthermore, we conduct a series of experiments to demonstrate the efficacy of our proposed representation. We compare its performance against traditional methods, providing qualitative and quantitative analyses that highlight its advantages. Our findings indicate that attention distillation not only enhances the visual quality of generated images but also offers greater flexibility in manipulating styles, paving the way for more sophisticated applications in digital artistry.

% Ultimately, this research contributes to a deeper understanding of how attention mechanisms can be utilized in creative tasks, bridging the gap between theoretical exploration and practical application. We envision that our work will inspire further innovations in image processing, setting a foundation for future studies that seek to integrate advanced neural architectures with artistic creativity.




%Recent advancements in texture synthesis and style transfer methods based on optimization have yielded significant results. These methods typically utilize pre-trained networks, such as VGG, to extract features for calculating well-defined style losses, such as the Gram matrix and sliced Wasserstein distance. Through gradient descent methods, they continuously update and optimize the pixel values of the images. These losses are often defined based on the distance between two distributions. Notably, different pre-trained networks can influence the effectiveness of these losses, as demonstrated in a study exploring the use of ResNet for style transfer.
%Currently, diffusion models have showcased remarkable capabilities in image generation and editing, particularly with the open-source nature of the Stable Diffusion series, which has spurred rapid advances in AI-assisted art. This raises an important question: can the previously defined style losses be effectively applied to the Stable Diffusion model? Furthermore, is there a suitable style loss specifically tailored for this model?
%Drawing on the lessons from using ResNet in style transfer, our objective is to identify an appropriate style loss for the Stable Diffusion model. Recently, a plethora of research based on Self-KV has emerged, leveraging self-attention features (KV) from U-Net in Stable Diffusion as a plug-and-play attribute to maintain consistency in appearance or style. The consensus in the research community is that utilizing Self-KV is an effective way to ensure visual coherence. Given the challenges in directly applying traditional style and texture synthesis losses to Stable Diffusion, we propose a straightforward and effective loss function grounded in Self-KV, enabling style transfer and texture synthesis in the latent space of Stable Diffusion.


\label{sec:intro}
