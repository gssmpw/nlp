
% \begin{figure*}[htp]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figs/transfer.pdf} 
%     \caption{\textbf{Comparisons of style and appearance transfer.} Our comparisons primarily focus on recent diffusion-based methods for both style and appearance transfer. Additionally, we include traditional methods, \ie NST~\cite{gaty2016styletransfer}, StyTR$^2$~\cite{Deng_Tang_Dong_Ma_Pan_Wang_Xu_2022} and SpliceViT~\cite{Tumanyan_Bar-Tal_Bagon_Dekel_2022} for comparison.}
%     \label{fig:transfer}
% \end{figure*}

\section{Experiments}
\label{sec:exp}

\subsection{Applications and Comparisons}
In the following, we apply our attention distillation loss to various visual characteristic transfer tasks and compare the results to state-of-the-art methods in each application. See the supplementary materials for detailed parameter configurations, running time, and additional results.

\vspace*{-3mm}
\paragraph{Style and Appearance Transfer.}
Following the spirit of the prominent work of Gatys \etal~\cite{gaty2016styletransfer}, we achieve style and appearance transfer through the optimization method described in Sec.~\ref{sec:method_optim}.
%In style and appearance transfer, the goal is to translate the style or appearance of a \yang{reference} image onto a \yang{content/structure} image. %Building on the landmark approach introduced by 
 %we alter the Gram loss to our AD loss, 
%we achieve this transfer through the optimization method described in Sec.~\ref{sec:method_optim}. 
We compare our method to CSGO~\cite{xing2024csgo}, StyleShot~\cite{gao2024styleshot}, StyleID~\cite{chung2024styleid}, StyTR$^2$~\cite{Deng_Tang_Dong_Ma_Pan_Wang_Xu_2022} and NST~\cite{gaty2016styletransfer} for style transfer, as well as Cross-Image Attention~\cite{alaluf2024cross} and SpliceViT~\cite{Tumanyan_Bar-Tal_Bagon_Dekel_2022} for appearance transfer. Fig.~\ref{fig:transfer} presents the qualitative comparison results. In style transfer, our method effectively captures high-quality, coherent style characteristics while simultaneously preserving the semantic structures of the content image. This is particularly evident in the sketch styles in the 3rd and 4th rows. In contrast, baseline methods exhibit notable style discrepancies, despite retaining the original structure. In appearance transfer, our method also shows superiority, avoiding the oversaturation of color seen in Cross-image Attention.


\vspace*{-3mm}
\paragraph{Style-specific Text-to-Image Generation.}

\begin{figure*}[htp]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/stylet2i.pdf} 
    \vspace*{-6mm}
    \caption{\textbf{Comparisons of style-specific text-to-image generation.} We compare our approach to InstantStyle~\cite{wang2024instantstyle}, Visual Style Prompting~\cite{jeong2024visual}, and RB-Modulation~\cite{rout2024rbmodulation} using three style references as examples. For each style, we utilize the same text prompts: ``\textit{A deer}'' (left), ``\textit{A rocket}'' (top right), and ``\textit{A piano}'' (bottom right).}
    \label{fig:stylet2i}
\end{figure*}

\begin{figure}[htp]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/controlnet.pdf} 
    \caption{By combining attention distillation guided sampling with ControlNet~\cite{zhang2023controlnet} into the text-to-image pipeline, we can produce high-quality results that align the structure with the condition image while maintaining style coherence with the style reference. }
    \label{fig:controlnet}
\end{figure}


%Generating images that integrate textual prompts with the specific artistic style of a reference image has gained popularity in art creation. To achieve this, we build on a text-to-image diffusion model, \ie Stable Diffusion, applying our AD loss within the diffusion sampling process as described in Sec.~\ref{sec:method_sampling}. 
As described in Sec.~\ref{sec:method_sampling}, we can apply our AD loss within the diffusion sampling, thereby realizing style-specific text-to-image generation. We set the reference as the desired style image. Fig.~\ref{fig:stylet2i} showcases some generated results, along with a comparison to alternative methods, including Visual Style Prompting~\cite{jeong2024visual}, InstantStyle~\cite{wang2024instantstyle}, and RB-Modulation~\cite{rout2024rbmodulation}. The results in Fig.~\ref{fig:stylet2i} demonstrate that our method aligns textual semantics comparably to existing methods while achieving notably better style coherence with the reference.

In addition to the above approach, we further incorporate ControlNet~\cite{zhang2023controlnet} to enable style-specific text-to-image generation with additional conditioning on various modalities, \eg depth and canny edges. Fig.~\ref{fig:controlnet} presents some generated examples. More results can be seen in the supplementary.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figs/ctrltex.pdf} 
%     \caption{\textbf{Comparisons of controlled texture synthesis.} \yang{Left: annotation control; Right: layout control.}}
%     \label{fig:control_tex}
% \end{figure}


% \zichong{
% \paragraph{Controlled Texture Synthesis.} The task here is to synthesize texture images 

% }

%Texture synthesis with spatial control is a challenging task, as existing methods often struggle to balance texture quality and spatial alignment. 
%In this work, we employ segmentation maps to guide the spatial structure of textures. 
\vspace*{-3mm}
\paragraph{Controlled Texture Synthesis.}
Our method can be applied to texture optimization as demonstrated in Sec.~\ref{sec:method_ADL}. Inspired by~\cite{cao2023masactrl}, we further incorporate mask guidance when calculating attention distillation loss, thereby constraining the values queried by $Q$, resulting in controlled texture synthesis. Specifically, given a source texture $x_s\in\mathbb{R}^{h\times w\times 3}$, its corresponding source segmentation map $S^s\in\mathbb{R}^{h\times w\times 1}$, and the target segmentation map $S^t\in\mathbb{R}^{h'\times w'\times 1}$, we first flatten $S^s$ and $S^t$ and downsample them to match the resolution of attention features, resulting in $\bar{S^s}$ and $\bar{S^t}$. Then, we compute the guiding mask $M$; with this mask, we restrict the visual information aggregated by $Q$ in attention computation to focus solely on the corresponding regions:

% $$M^*=S_s - S_t^T$$
\begin{equation}
 M_{i,j}:= 
\begin{cases} 
\text{True} & \text{if } \bar{S^s_i} = \bar{S^t_j} \\ 
\text{False} & \text{otherwise} 
\end{cases}   
\end{equation}
\begin{equation}
\mathcal{L}_{\mathrm{MAD}}=||\mathrm{Self\text{-}Attn}(Q,K,V)-\mathrm{Self\text{-}Attn}(Q,K_{s},V_{s};M)||_{1}
\end{equation}

%provided by the user
To further ensure the alignment with the target mask, we fill the target mask as initialization with pixels randomly drawn from corresponding labeled regions of the source image. Then, we regard this init as the content reference and add a content loss as Sec.~\ref{sec:method_optim} based on the query features.
%Combined with content loss from Q, we can achieve refined texture details and spatial alignment through optimization method. 
Figs.~\ref{fig:teaser} and~\ref{fig:control_tex} present our controlled texture synthesis results. Compared to GCD~\cite{zhou2023neural}, a patch-based neural texture optimization method, our results exhibit comparable texture details with smoother object edges. In contrast, GCD suffers from artifacts of color aliasing; see the 2nd row of Fig.~\ref{fig:control_tex}.

Recently, Self-Rectification~\cite{zhou2024generating} introduced a ``lazy-editing'' control for generating non-stationary textures. Aiming at the same goal, we utilize SDEdit~\cite{meng2022sdedit} to preserve the structure of the layout image edited by the user. Then, we incorporate our proposed AD loss and content loss into the sampling (as Sec.~\ref{sec:method_sampling}). As compared in Fig.~\ref{fig:control_tex}, Self-Rectification outputs smoother texture transitions, while our results better adhere to the original texture examples.
% \paragraph{Masked Attention Distillation Loss.} , effectively addressing this issue
% To address the issue of foreground-background confusion, MasaCtrl proposed a mask-guided KV-injection approach, enabling Q to query specified regions based on a mask, thereby alleviating inaccuracies in querying. Inspired by this, we introduce the incorporation of mask guidance when calculating the attention distillation loss, thereby constraining the values queried by Q. Specifically, given an input reference image $x_s\in\mathbb{R}^{h\times w\times 3}$, its corresponding source segmentation map $S^s\in\mathbb{R}^{h\times w\times 1}$, and the target segmentation map $S^t\in\mathbb{R}^{h'\times w'\times 1}$, we first flatten $S^s$ and $S^t$ , downsample them to match the resolution of Q and K, resulting in $\bar{S^s}$ and $\bar{S^t}$ . Subsequently, we compute the guiding mask $M$; with this mask, we can control the visual information aggregated by Q during attention computation to focus solely on the corresponding regions:
% % $$M^*=S_s - S_t^T$$
% $$ M_{i,j}:= 
% \begin{cases} 
% \text{True} & \text{if } \bar{S^s_i} = \bar{S^t_j} \\ 
% \text{False} & \text{otherwise} 
% \end{cases}$$
% $$\mathcal{L}_{\mathrm{MAD}}=||\mathrm{Self\text{-}Attn}(Q,K,V)-\mathrm{Self\text{-}Attn}(Q,K_{s},V_{s};M)||_{1}$$

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/ctrltex.pdf} 
    \caption{\textbf{Comparisons of controlled texture synthesis.} Left: annotation control; Right: layout control.}
    \label{fig:control_tex}
\end{figure}


\vspace*{-3mm}
\paragraph{Texture Expansion.}
It is very difficult to synthesize ultra-high resolution textures using traditional methods, given the limited patch sources. Here, we apply our attention distillation guided sampling to the MultiDiffusion~\cite{bar2023multidiffusion} model, enabling texture expansion to arbitrary resolution.
%Traditional texture synthesis methods are often constrained by computational resources, limiting the generation of ultra-high-resolution textures and increasing the likelihood of artifacts. 
Although SD-1.5~\cite{Rombach2021HighResolutionIS} is trained on images of size 512$\times$512, surprisingly, it demonstrates robust capabilities in large-size texture synthesis when incorporating attention distillation. Fig.~\ref{fig:texexp} presents comparisons of texture expansion to size 512$\times$1536 with GCD~\cite{zhou2023neural} and GPDM~\cite{elnekave2022generating}. Our approach shows significant advantages in such a challenging task. %See supplementary for more results.}

\begin{figure*}[htp]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/texexp.pdf} 
    \vspace*{-6mm}
    \caption{\textbf{Comparisons of texture expansion.} Taking a 512$\times$512 texture image as the input example, we expand it with automatic semantic understanding and synthesize results at the resolution 512$\times$1536. We compare our approach with GCD~\cite{zhou2023neural} and GPDM~\cite{elnekave2022generating}.}
    \label{fig:texexp}
\end{figure*}



\subsection{Ablation Studies}

\begin{figure*}[htp]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/abl_content_loss.pdf} 
    \vspace*{-6mm}
    \caption{\textbf{Ablation of content loss weights.} Varying the content loss weights $\lambda$ on two examples, one each for style transfer (left) and appearance transfer (right). The results demonstrate how the content loss helps preserve the source image content during the transfer. When the weights are within an appropriate range, the results exhibit varying levels of abstraction and appearance transition.}
    \label{fig:abl_closs}
\end{figure*}

\begin{figure}[htp]
    \centering
    \vspace*{-2mm}
    \includegraphics[width=1.0\linewidth]{figs/abl_optim1.pdf} 
    \vspace*{-6mm}
    \caption{\textbf{Impact of the optimizer for managing guidance strength.} Both cases--with and without the Adam optimizer~\cite{adam2015}--were conducted with 50 diffusion sampling steps. All results are generated using Stable Diffusion v1.5~\cite{Rombach2021HighResolutionIS}, with empty text prompts. The learning rate of the Adam optimizer is set to 0.02.}%, and we performed two optimization iterations after each sampling step.} the introduction of
    \label{fig:abl_optim}
\end{figure}

In this section, we present ablation study results on two aspects of our approach: i) the impact of content loss weight in content-preserving optimization (Sec.~\ref{sec:method_optim}), and ii) the optimizer for managing guidance strength in attention distillation guided sampling (Sec.~\ref{sec:method_sampling}).

\vspace*{-3mm}
\paragraph{Content loss weight.} As shown in Fig.~\ref{fig:abl_closs}, varying the content loss weight $\lambda$ brings intriguing effects on the transfer results. In style transfer, for instance, the abstract style example shown on the left illustrates how the adjustment of $\lambda$ results in different levels of abstraction, offering flexibility for artistic creation. In appearance transfer, thanks to the precise semantic understanding from diffusion networks, the facial image transfer shown on the right exhibits a smooth identity transition along with the $\lambda$ varies.

\vspace*{-3mm}
\paragraph{Optimizer.} Fig.~\ref{fig:abl_optim} demonstrates the importance of the optimizer in managing guidance strength. We experimentally test a naive strategy of manually setting the guidance strength to control the scale of gradient updates to the latents. However, varying this strength manually often fails to yield reasonable results: textures or appearance features in the examples are typically lost, as shown in the last three columns of Fig.~\ref{fig:abl_optim}. In contrast, introducing an Adam optimizer to manage latent optimization produces results that closely match the visual characteristics of the input examples (columns 2$\sim$4 in Fig.~\ref{fig:abl_optim}). Furthermore, increasing the number of optimization iterations within each timestep during sampling generally enhances the quality of generated results, although it also brings extra computation time. In practice, we set the iteration number to 2 to achieve an efficient balance, delivering high-quality results effectively.


\begin{figure}[t]
    \centering
    \vspace*{-3mm}
    \includegraphics[width=1.0\linewidth]{figs/userstudy.pdf} 
    \vspace*{-6mm}
    \caption{\textbf{User preference score.} We report the overall preference score comparing our method to selected alternatives across three transfer tasks. Each comparison is conducted individually, directly evaluating user preference between our results and those of each competing method. Note CIA stands for Cross-Image Attention~\cite{alaluf2024cross} and VSP as~\cite{jeong2024visual}. See supplementary for more details.}%\zichong{For brevity, CIA and VSP denote Cross-image Attention and Visual Style Prompting, respectively.}}% See supplementary for more details. }
    \label{fig:user}
\end{figure}

\subsection{User Preference Study}
To validate the qualitative analysis, we conduct a user study with 30 questions (5 questions for each of 6 selected competitors) on three transfer tasks. In each question, we showcase two results: one from our method and one from a competitor. The user was asked to choose the better one based on the provided instructions and criteria. We have collected 1500 responses from 50 participants, and the overall preference score is summarized in Fig.~\ref{fig:user}. Our method consistently outperforms the alternatives by significant margins. Please refer to the supplementary for more details.

% \begin{table}[htbp]%
%     \scriptsize 
%     \caption{
%     User preference score.
%     }
%     \label{tab:userstudy}
%     \vspace*{-5mm}
%     \begin{center}
%     \begin{tabular}{c|cc|cc|cc}
%       \hline
%       & \multicolumn{2}{c|}{\makecell[c]{Style Transfer}} & \multicolumn{2}{c|}{\makecell[c]{Appearance Transfer}} & \multicolumn{2}{c}{\makecell[c]{{ Style-specific} \\ {  Text-to-Image Generation}}}\\  \hline
%       \multirow{2}*{\makecell[c]{Ours\\ vs.}} & StyleID & StyTR$^2$ & CIA* & SpliceViT & InstantStyle & VSP* \\ \cline{2-7} %\hline
%       & 80.4\% & 83.2\% & 61.6\% & 88.0\% & 74.4\% & 83.2\% \\
%       \hline
%     \multicolumn{7}{l}{\scriptsize * CIA and VSP denote Cross-image Attention and Visual Style Prompting, respectively.}\\
%     \end{tabular} 
%     \end{center}
% \end{table}%



