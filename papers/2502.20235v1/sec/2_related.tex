\section{Related work}
\label{sec:related_work}

Our primary contribution lies in proposing a novel loss function that effectively transfers the visual characteristics of one image to another. This loss function has broad applications across various image synthesis tasks, including texture synthesis, style transfer, appearance transfer, and customized image generation based on text-to-image models. We review related work in these domains.

\textbf{Texture synthesis} focuses on generating images that resemble the original texture without repetition and artifacts. Traditional methods rely on parametric texture models~\cite{Julesz, Heeger_Bergen_1995, Simoncelli_Freeman_2002} or sampling pixels/patches~\cite{Efros_Leung_1999, Wei_Levoy_2000, Efros:2001, kwatra2003, kwatra2005} to create new images.
%Early parametric models, like the multiscale model by Portilla and Simoncelli (2000), generate textures by matching the statistical properties of the target texture image. Non-parametric approaches, such as Efros and Leung’s (1999) sample-based synthesis algorithm, generate new images pixel-by-pixel or block-by-block, leveraging previously synthesized regions to produce natural texture effects. 
%Although these methods excel with simple textures, they often struggle with complex or high-resolution textures.
These methods excel with simple textures but often struggle with complex or high-resolution textures.
Deep learning-based methods, utilizing convolution neural networks (CNNs), extract multilevel features that capture textures at varying scales. Gatys~\etal~\cite{Gatys2015} introduced Gram matrix, a second-order statistic of feature map to represent stationary textures.
%a CNN-based texture synthesis approach, using Gram loss to match feature statistics between input and target textures. However, Gram loss primarily captures second-order statistics, which may inadequately represent complex textures. 
Histogram loss~\cite{risser2017stable} and Sliced Wasserstein loss~\cite{heitz2021sliced, elnekave2022generating} offer improved modeling of texture distributions, leading to more realistic results but failing to capture large-scale structure, especially for non-stationary textures. Generative adversarial networks (GANs) are also widely adopted in texture synthesis~\cite{TexExp, shaham2019singan, InGAN}; for instance, Zhou~\etal~\cite{TexExp} used a self-supervised approach to expand a single texture by learning the internal patch distribution, achieving impressive detail and structure expansion. Recently, Self-Rectification~\cite{zhou2024generating} leverages pre-trained diffusion networks and the self-attention mechanism to gradually refine a lazy-editing input, addressing the intricate challenge of synthesizing non-stationary textures.

%extracts content and style features through a VGG network,
\textbf{Neural style transfer} %an extension of texture synthesis in the arts, 
applies the artistic style of a source image to a new content image. Gatys~\etal~\cite{gaty2016styletransfer} pioneered this field by using Gram loss as style representation and proposed a neural style transfer algorithm that combines content and style loss in the optimization. While Gram loss and its variants~\cite{Johnson2016, AdaIn, li2017demystifying} are the most widely used style losses, they primarily assess global distribution differences and cannot account for local semantic correspondences. Approaches like CNNMRF~\cite{LiChuanandWand2016} and Contextual Loss\cite{mechrez2018contextual} address this by computing semantic similarity in high-dimensional feature spaces to enable semantically coherent style transfer. %To accelerate the optimization process, a generative network is introduced for a real-time style transfer~\cite{Johnson2016}. Subsequently, Huang~\etal~\cite{AdaIn} proposed adaptive instance normalization (AdaIN), enabling arbitrary style transfer in real-time. 
Later works incorporated transformers with self-attention to capture stronger relationships between style and content. Deng~\etal~\cite{Deng_Tang_Dong_Ma_Pan_Wang_Xu_2022} introduced a fully transformer-based architecture, StyTR$^2$, achieving state-of-the-art results at the time. Recent advances in diffusion models enable interpretable and controllable content-style separation. InST~\cite{zhang2023inst} proposed inversion-based learning of artistic style from a single painting.  StyleDiffusion~\cite{wang2023stylediffusion} introduced a CLIP-based style disentanglement loss. StyleID~\cite{chung2024styleid}, a training-free method, manipulates the self-attention features of a pre-trained diffusion model by substituting the keys and values of the content with those of the style image in cross-attention mechanisms.


\textbf{Appearance transfer}, a specialized semantic style transfer, aims at transferring the appearance of semantically corresponding regions. Early works~\cite{Isola_Zhu_Zhou_Efros_2017, Zhu_Park_Isola_Efros_Winter_Gogh_Monet_Photos, Park_Zhu_Wang_Lu_Shechtman_Efros_Zhang_2020} use paired or unpaired datasets to train GANs for domain-specific appearance transfer. Tumanyan~\etal~\cite{Tumanyan_Bar-Tal_Bagon_Dekel_2022} extract structure and appearance features using a pre-trained DINO-ViT~\cite{Caron_Touvron_Misra_Jegou_Mairal_Bojanowski_Joulin_2021}, and trains a generator for each image pair. Recently, cross-image attention~\cite{alaluf2024cross} with pre-trained diffusion models realizes zero-shot appearance transfer by implicitly establishing semantic correspondences across images.

% with text
\textbf{Customized/Personalized image generation} based on text-to-image (T2I) diffusion models has attracted particular attention recently, which aims to learn the style from one or more reference images to generate new content. 
%\textbf{Diffusion models}~\cite{ho2020denoising, song2021ddim,dhariwal2021beatgans,esser2024sd3, Podell2023SDXLIL,Rombach2021HighResolutionIS} have recently demonstrated groundbreaking capabilities in text-to-image (T2I) generation, significantly advancing related fields. 
%Customized (or personalized) style generation based on T2I models has attracted particular attention, aiming to learn the style from one or more reference images to generate new content. 
Fine-tuning approaches~\cite{ahn2024dreamstyler, everaert2023diffusioninstyle, frenkel2024implicit, Gal2022AnII, jones2024customizingtexttoimagemodelssingle, kumari2022customdiffusion, Ruiz2022DreamBoothFT, sohn2023styledrop} enable models to learn novel style concepts from a few images. However, these methods are prone to overfitting, potentially resulting in degraded image quality or content leakage. To alleviate this issue, B-LoRA~\cite{frenkel2024implicit} leverages LoRA~\cite{hu2022lora} (Low-Rank Adaptation) to implicitly separate the style and content components of a single image. Pair Customization~\cite{jones2024customizingtexttoimagemodelssingle} learns stylistic differences from a single image pair and then applies the acquired style to the generation process. Encoder-based methods~\cite{ye2023ip-adapter, xing2024csgo, gao2024styleshot,chen2023controlstyle, wang2024styleadapter, wang2024instantstyle} utilize visual encoders to capture image information and establish mappings between image prompts and models through huge dataset training. While currently favored as state-of-the-art, these techniques are constrained by the capabilities of visual encoders, often extracting only abstract style information and struggling with fine-grained textures of the reference. Several works have proposed plug-and-play solutions for training-free style customization. For example, StyleAligned~\cite{hertz2024stylealigned} and Visual Style Prompt~\cite{jeong2024visual} maintain style consistency by preserving the queries from the original features while sharing or swapping the keys and values with those from reference features in the late self-attention layers. RB-Modulation~\cite{rout2024rbmodulation} modulates the drift field of reverse diffusion dynamics by incorporating desired attributes (\eg, style or content) through a terminal cost.

% \subsection{Neural Style Transfer}
%  The seminal work of Gatys \etal~\cite{Gatys2016} reveals that deep features extracted from pretrained VGG networks encode the content information of an image, while statistics computed from activation maps of different scales can represent the image style. An image then can be modified via back-propagation by matching the disentangled representations of content and style features with those of the reference. The impressive stylization results produced by this method have popularized neural style transfer and inspired a surge of successive works, including training feed-forward networks~\cite{Johnson2016, AdaIn}, combining with Markov Random Fields~\cite{LiChuanandWand2016}, and utilizing multi-scale strategies~\cite{WCT, ETNet} etc. \yangc{Describe the drawbacks of Gatys' method here, e.g.,} \yang{However, the capacity of convolution neural networks limits its representation power. In this work, we opt to diffusion networks and propose new representations for both content and style.}

% Gatys' pioneering work and impressive results have led people to explore new methods for style transfer and texture synthesis in the feature space of VGG. Gram loss and histogram statistics minimize the distance between the distributions of two images by calculating the statistical differences between them, while SWD directly computes the differences between the distributions of two images using sliced Wasserstein distance. The GAN-based SinGAN essentially aligns the distribution of generated images with that of real images through a discriminator. We categorize these methods as distribution-based styling approaches, which have a global optimization objective but lack semantic correspondence (e.g., eye-to-eye, mouth-to-mouth).\par
% On the other hand, approaches like CNNMRF, contextual loss, and GCD search the nearest neighbors (pixels or image patches) based on similarity within the VGG feature space to serve as optimization targets, leveraging VGG's recognition capabilities to establish semantic correspondence. We refer to these as semantic-based styling methods. However, since the VGG network is trained on the ImageNet dataset for classification tasks, its performance is limited in understanding higher-level abstract semantics in style images. When there is a lack of semantic correspondence between the content and style images (e.g., Picasso's bull and a human face), these methods often fail to achieve the desired stylistic effects.\par
% These losses often emphasize the correspondence of semantic or the similarity of distributions, failing to capture the essence and nuances of the original work.
% In this paper, we argue that the pretrained Stable Diffusion model can be likened to a highly skilled artist, adept at extracting both low-level visual elements and high-level semantic information from reference images. This virtuoso then, drawing upon its vast reservoir of experience (akin to the attention mechanism), ingeniously recombines these elements into novel creations. Through our innovative loss function, we succeed in distilling the entirety of this artist's expertise and knowledge, capturing the essence of the example artworks .

 

% \subsection{T2I diffusion models and personalization}
% Recent advancements in T2I diffusion models, particularly within the Stable Diffusion series \cite{esser2024sd3, Podell2023SDXLIL,Rombach2021HighResolutionIS}, have significantly enhanced the quality and diversity of images generated from textual prompts. The release of the Stable Diffusion community model has further broadened its impact, fostering a more extensive adoption and exploration. T2I personalization using Stable Diffusion has attracted considerable attention, with efforts focusing on incorporating new concepts into these models. Some approaches \cite{alaluf2023neti, Gal2022AnII, voynov2024p} introduce new textual embeddings to encapsulate the unique characteristics of the desired content, while others \cite{hao2024vico, hua2023dreamtuner, kumari2022customdiffusion, marjit2023diffusekrona, pang2024attndreambooth, Ruiz2022DreamBoothFT, Ruiz2024hyperdreambooth, shah2023ZipLoRA} modify or append specific model parameters to achieve personalized outputs. Although effective in generating personalized and concept-specific images, these approaches often struggle with maintaining consistent text alignment and fully capturing the complexity of intricate styles.
% %学习来自图像的新概念， create images consistent in style, subject, or character ID
% %基于微调的方法：dreambooth TI LoRA，3-5张图像
% %基于图像编码器的方法：IP Adapter InstantID 人脸个性化是目前的一个热门研究方向
% %kv特征注入:masactrl magicface 


% \subsection{Style-specific image generation} 
% Style transfer in T2I generation has been a prominent area of research. Starting with the traditional style transfer \cite{gaty2016styletransfer}, which applying the artistic style of one image to the content of another, recent works \cite{chung2024styleid, he2024freestyle, wang2023stylediffusion, zhang2023inst} have advanced to using diffusion models. Another line of research is on custom style text-to-image generation. One approach \cite{ahn2024dreamstyler, everaert2023diffusioninstyle, frenkel2024implicit, Gal2022AnII, jones2024customizingtexttoimagemodelssingle, kumari2022customdiffusion, Ruiz2022DreamBoothFT, sohn2023styledrop} involves fine-tuning models on style-specific image data, while another features training-free methods, such as StyleAligned \cite{hertz2024stylealigned}, RB-Modulation \cite{rout2024rbmodulation} and InstantStyle \cite{wang2024instantstyle}, which leverage pre-trained models to integrate styles without additional training. Additionally, there are pre-trained methods \cite{chen2023controlstyle, gao2024styleshot, wang2024styleadapter, xing2024csgo} designed to handle arbitrary styles, offering a versatile solution for style-specific image generation. However, fine-tuning methods often struggle with achieving desired style effects and can suffer from text-semantic loss. Training-free and pre-trained methods also face challenges when handling multiple style images as input.
% %基于T2I的风格迁移方法，可以看作T2I个性化在风格领域的一个子集，
% %微调的方法 InST TI DB-LoRA  可以很好的捕捉到图像的细节信息，但容易过拟合和内容泄露 B-LoRA缓解了上述问题，但只在SDXL上有效，这些方法通常也比较耗时
% %图像编码器：IPAdapter styleshot csgo instantstyle， 基于大数据训练，使编码器更容易捕捉到高层次的抽象风格信息，可能存在泛化性不足的问题，对于罕见的风格效果并不理想
% %kv特征注入：style aligned styleID vsp  无需训练，做到了即插即用，但有时效果并不理想。一是kv注入本身存在问题，二是该方法受到模型自身性能限制，masactrl以二次元动漫图作为参考图时选用了Anything-V4，一个使用二次元动漫数据集微调的模型，应该也是这个原因。目前来看，sd1.5对3d、二次元动漫等风格表现不佳，是因为训练数据没有包含太多这类数据，SDXL则有了很多适用的风格提示词，应该是做了特定的风格训练。
% Fine-tuning approaches, such as DreamBooth, Textual Inversion (TI), and InST, enable models to learn novel style concepts from one or more images. However, these methods are prone to overfitting, potentially resulting in degraded image quality or content leakage. Encoder-based methods utilize visual encoders to capture image information and establish connections between image prompts and models through vast datasets. While currently favored as state-of-the-art, these techniques are constrained by the capabilities of visual encoders, often extracting only abstract style information and struggling with complex texture details.

%我们的主要贡献是提出一种新的损失函数，能够有效地将参考图像中的视觉元素（如纹理风格）迁移到另一张图像上。这一损失可以广泛应用于多个计算机任务，例如纹理合成、风格迁移、外观迁移，以及基于文生图模型的特定风格图像生成。我们将回顾这些领域的相关工作。

%纹理合成专注于生成与原始纹理相似但不重复的图像，以模拟自然界中的无缝纹理。传统的纹理合成方法基于样本统计或样本拼接来生成新图像。最早的统计模型如Portilla和Simoncelli（2000）提出的基于参数化的多尺度模型，通过匹配目标纹理图像的统计特性来生成新纹理。非参数化方法则如Efros和Leung（1999）提出的基于样本的合成算法，通过逐像素或逐块生成新图像，每一步都依赖之前生成的像素或块的信息，以此生成自然的纹理效果。这类方法虽然在合成简单纹理上效果显著，但在复杂纹理或分辨率较高的场景下往往难以生成自然的纹理图像。基于深度学习的纹理合成方法通过卷积神经网络（CNN）从图像中提取多层次特征，这些特征在不同层中表现为不同的纹理尺度。Gatys等人（2015）首次提出了一种基于深度卷积网络的纹理合成方法，通过Gram Loss 优化输入图像的特征图，使其与目标纹理图像的统计特征匹配。然而Gram loss主要用于计算特征的二阶统计量，无法完全表示复杂纹理的统计分布。直方图损失和瓦瑟斯坦损失对纹理的特征分布进行更精准的建模，使生成的纹理更加逼真。但这些损失无法捕捉到纹理的大尺度结构，对于非稳态纹理的合成效果不佳，需要引入额外的控制。此外，生成对抗网络（GAN）在纹理合成中也被广泛使用，zhou等人设计了自监督的方式学习纹理扩展，利用GAN学习纹理的数据分布，在纹理细节和结构延伸上取得了令人满意的效果。最近Self-rectification leverages a pre-trained diffusion network, and uses self- attention mechanisms, to gradually refine a lazy-editing input, addressed the intricate challenge of synthesizing non-stationary textures
% 神经风格迁移作为纹理合成在艺术领域的扩展，可以视为在目标图像的内容特征上，应用源图像的纹理特征。Gatys等人（2016）提出的经典基于Gram loss的神经风格迁移算法是风格迁移领域的开创性工作，该算法使用VGG网络来提取图像的内容和风格特征，采用内容损失和风格损失的加权和作为目标函数。Gram loss及其变体MMD虽然是今天应用最为广泛的风格损失，但这种衡量数据全局分布差异的损失，无法consider局部的语义对应，CNNMRF、Contextual Loss等基于最近邻匹配的思想，在高维特征空间计算语义相似度，进而设计损失函数，实现了令人满意的语义风格迁移。另一方面，为解决优化过程较为耗时的问题，Johnson等人（2016）提出了基于生成网络的快速风格迁移方法，从而实现了实时风格迁移。此外，Dumoulin等人（2017）和Huang与Belongie（2017）分别提出了多风格迁移和自适应实例归一化（AdaIN），通过修改生成网络的参数或归一化方式实现多种风格迁移，使得风格迁移的应用更加灵活。
% 外观迁移，一种特别的语义风格迁移，旨在迁移语义对应区域的外观特征。早期的工作通过成对或非成对数据集训练GAN，实现特定域图像的外观迁移效果。Tumanyan et al. extract structure and style feature with a pretrained DINOVi and train a generator for each pair of images. Recently Cross image attention 借助预训练扩散模型implicitly establishes semantic correspondences across images。借助 a cross-image attention mechanism 实现了Zero-Shot Appearance Transfer。

%近年来，扩散模型凭借其优异的生成能力，在T2I领域取得突破性进展，并促进了相关领域的研究。其中风格定制化备受关注。
 