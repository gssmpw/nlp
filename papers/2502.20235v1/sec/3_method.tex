\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/method1.pdf} 
    \caption{\textbf{Overview of attention distillation.} Based on the self-attention mechanism in diffusion models, we compute the difference between the ideal and the current stylization, formulating a novel Attention Distillation (AD) loss (a). The new loss acts like a style loss. When combined with a content loss (also derived from the self-attention mechanism), we can realize high-quality content-preserving synthesis, such as style transfer or appearance transfer (b). Our attention distillation loss can be incorporated into the normal diffusion sampling process as an improved Classifier Guidance (c), which enables a broad scope of example-based image generation applications.}
    \label{fig:method}
\end{figure*}

\section{Method}


\subsection{Preliminaries}
\textbf{Latent diffusion models (LDM)}, exemplified by Stable Diffusion~\cite{Podell2023SDXLIL,Rombach2021HighResolutionIS}, have achieved state-of-the-art performance in image generation due to the robust ability to model complex data distributions. In LDM, an image $x$ is first compressed into a learned latent space using a pretrained VAE $\mathcal{E}(\cdot)$. A UNet-based denoising network $\epsilon_\theta(\cdot)$ is subsequently trained to predict the noise during the diffusion process by minimizing the mean squared error between the predicted noise and the actually added noise $\epsilon$:
\begin{equation}
    \mathcal{L}_{\mathrm{LDM}}=\mathbb{E}_{z\sim\mathcal{E}(x),y,\epsilon\sim\mathcal{N}(0,1),t}\Big[\|\epsilon_\theta(z_t,t,y)-\epsilon\|_2^2\Big],
\end{equation}
where $y$ denotes the condition and $t$ represents the timestep. The denoising UNet typically consists of a bunch of convolution blocks and self-/cross-attention modules, all integrated within the predictive branch of \textit{residual} architecture.

\textbf{KV-injection} is widely employed in image editing~\cite{pnpDiffusion2022, cao2023masactrl, alaluf2024cross}, style transfer~\cite{hertz2024stylealigned, chung2024styleid, jeong2024visual}, and texture synthesis~\cite{zhou2024generating}. 
It is built upon the self-attention mechanism and uses the self-attention features in diffusion models as plug-and-play attributes. The self-attention mechanism is formulated as:
\begin{equation}
\mathrm{Self\text{-}Attn}(Q,K,V)=\mathrm{softmax}(\frac{QK^T}{\sqrt{d}})V.
\end{equation}
At the core of the attention mechanism lies the calculation of a weight matrix based on the similarity between the queries ($Q$) and keys ($K$), which is used to perform a weighted aggregation of the values ($V$). KV-injection extends this mechanism by copying or sharing the $KV$ features across different synthesis branches. Its key assumption is that $KV$ features represent the visual appearance of an image. During sampling, replacing the $KV$ features in the synthesized branch with those $KV$ from the corresponding timestep of the exemplar can realize appearance transfer from the source image to the synthesized target.
% However, direct feature injection during diffusion sampling is influenced by the residual mechanism, which limits its ability to fully leverage the potential of KV-injection. 

%Diffusion sampling with KV-injection refers to synthesizing images by drawing information from the KV features of the reference branch. However, it cannot fully capture the style or texture details from the reference due to the residual mechanism;

\subsection{Attention Distillation Loss}
\label{sec:method_ADL}
Although KV-injection has achieved noticeable results, it falls short in preserving the style or texture details of the reference due to the residual mechanism; see Fig.~\ref{fig:kvinjection}~(a) for example. KV-injection only operates on the residual, which means the information flow (red arrows) is subsequently influenced by the identity connection, leading to an incomplete transfer of information. As a result, the sampling outputs cannot fully reproduce the desired visual details.

%\yang{optimization} and sampling with .}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/kvinjection1.pdf} 
    % \vspace*{-3mm}
    \caption{\textbf{Differences between KV-injection and attention distillation.}  We start with the same latent for sampling and optimization, both running 100 steps, using empty prompts. The information flow (red arrows) differs only from the identity connection. However, the results of our attention distillation optimization (b) are clearly superior to sampling with KV-injection (a).}
    \label{fig:kvinjection}
\end{figure}

In this work, we propose a novel loss function to distill visual elements by reaggregating features within the self-attention mechanism; therefore, we refer to it as Attention Distillation (AD) loss. We leverage the UNet of the pretrained T2I diffusion model, Stable Diffusion~\cite{Rombach2021HighResolutionIS}, to extract image features from self-attention modules. As illustrated in Fig.~\ref{fig:method}~(a), we first reaggregate the visual information of the $KV$ features ($K_s$ and $V_s$) from the reference branch according to $Q$ from the target branch, which is the same as KV-injection. We regard this attention output as the \emph{ideal} stylization. Then, we calculate the attention output of the target branch and compute the L1 loss w.r.t. the ideal attention output, which defines the AD loss: %. We formulate AD loss as follows:
\begin{equation}
    \mathcal{L}_{\mathrm{AD}}=\|\mathrm{Self\text{-}Attn}(Q,K,V)-\mathrm{Self\text{-}Attn}(Q,K_s,V_s)\|_1.
\end{equation}

%Attention distillation optimization.
%\noindent{\textbf{Understanding AD loss.}} 
We can use the proposed AD loss to optimize a random latent noise via gradient descent, resulting in vivid texture or style reproduction in the output; see Fig.~\ref{fig:kvinjection}~(b) for example. This can be attributed to the backpropagation in optimization, which allows information to flow not only across the (\textit{residual}) self-attention modules but also through the identity connection. With continuous optimization, the gap between $Q$ and $K_s$ gradually narrows, making attention more and more accurate, and eventually, features are correctly aggregated to produce the desired visual details.

Following recent experimental analysis~\cite{cao2023masactrl, pnpDiffusion2022, jeong2024visual}, we empirically select the last 6 self-attention layers of the UNet to compute AD loss. Additionally, during optimization, we simulate the sampling process of diffusion models by linearly decreasing the timestep $t$ input to the UNet from $T$ to $0$. We begin with different random latent noises and optimize them over 100 steps. Note that during the whole optimization, the predicted noise from U-Net is totally discarded, and we continuously update the same latent.

To better understand our AD loss, we present the optimization results across multiple runs, as shown in Fig.~\ref{fig:multipleruns}. These results demonstrate that: i) AD loss effectively distills high-quality visual characteristics in style and texture; ii) AD loss is self-adapted to different spatial structures, showcasing diversity across multiple runs.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/multipleruns.pdf} 
    \vspace*{-1mm}
    \caption{\textbf{Optimizing attention distillation loss across multiple runs.} The coherence in texture and style, and the variations in structure across multiple runs of the same reference, demonstrates the ability of our AD loss in style alignment and spatial adaption.}
    \label{fig:multipleruns}
\end{figure}


\subsection{Content-preserving Optimization}
\label{sec:method_optim}
With the texture and style distilled by AD loss, we can further align the synthesized content to another reference image using a content loss. Such optimization allows the synthesis of images that transform the visual elements of one image while preserving the target content, achieving tasks such as style transfer, appearance transfer, and more.

As illustrated in Fig.~\ref{fig:method}~(b), we define the content loss similarly to AD loss, which is also based on the self-attention mechanism, fully drawing the advantage of the deep understanding of images in diffusion models. In particular, the L1 loss computed between the target queries $Q$ and the reference queries $Q_c$, formulates the content loss: 
\begin{equation}
    \mathcal{L}_{\mathrm{content}}=\|Q-Q_c\|_1.
\label{eq:content}
\end{equation}
In implementation, we also select the last 6 self-attention layers to compute the content loss, consistent with AD loss. The objective of content-preserving optimization is:
\begin{equation}
\mathcal{L}_{\mathrm{total}}=\mathcal{L}_{\mathrm{AD}}+\lambda\mathcal{L}_{\mathrm{content}}.
\end{equation}
After optimization, the optimized latent code is decoded into image space using the pretrained VAE. We have summarized the content-preserving optimization in Algorithm 1 of the supplementary material.



\subsection{Attention Distillation Guided Sampling}
\label{sec:method_sampling}
The syntheses above are using backpropagation optimization. In this section, we introduce how we incorporate attention distillation loss into the sampling process of diffusion models in an improved Classifier Guidance manner.
%as an improved Classifier Guidance.

According to~\cite{dhariwal2021beatgans}, Classifier Guidance alters the denoising direction during the denoising process, thereby generating samples from $p(z_t|c)$, which can be formulated as:  % the update direction is modified as
\begin{equation}
    \hat{\epsilon}_\theta=\epsilon_\theta(z_t, t, y)-\alpha\sigma_t\nabla_{z_t}\log p(c|z_t),
    \label{eq:noise_update}
\end{equation}
where $t$ is the timestep, $y$ denotes the prompts, and $\epsilon_\theta$ and $z_t$ refer to the denoising network and the latent in LDM, respectively. $\alpha$ controls the guidance strength. Inspired by~\cite{epstein2023selfguidance}, we guide the diffusion sampling process using an energy function based on attention distillation loss.
%can be incorporated as guidance into the diffusion sampling process.
%instead of relying solely on classifier probabilities, 

Specifically, during DDIM sampling~\cite{song2021ddim}, the latent $z_t$ at timestep $t$ are translated to $z_{t-1}$ according to the update direction $\epsilon_\theta$ estimated from the denoising network:
\begin{equation}
    z_{t-1}=\sqrt{\bar{\alpha}_{t-1}}\hat{z}_0+\sqrt{1-\bar{\alpha}_{t-1}}\epsilon_\theta,
    \label{eq:sampling}
\end{equation}
where $\hat{z}_0=\frac{z_t-\sqrt{1-\bar{\alpha}_t}\epsilon _\theta}{\sqrt{\bar{\alpha}_t}}$. Replace $\epsilon_\theta$ with $\hat{\epsilon}_\theta$ from Eq.~\eqref{eq:noise_update}:
%By replacing $\epsilon_\theta$ in Eq.~\ref{eq:sampling} with $\hat{\epsilon}_\theta$ from Eq.~\ref{eq:noise_update}, we get:
\begin{equation}
    z_{t-1}=\underbrace{\sqrt{\bar{\alpha}_{t-1}}\hat{z}_0+\sqrt{1-\bar{\alpha}_{t-1}}\epsilon_\theta}_{\mathrm{DDIM\ Sampling}}+\alpha C\cdot\nabla_{z_t}\log{p(c|z_t)},
\end{equation}
where $C$ is a constant related to $t$. We define the energy function using our AD loss by substituting $\nabla_{z_t}\log{p(c|z_t)}$ with $\nabla_{z_t}\mathcal{L}_{AD}$. However, we found configuring the guidance strength is tricky; see Sec.~\ref{sec:exp} for detailed experimental analysis. To address this issue, we introduce an Adam optimizer~\cite{adam2015} to automatically manage the strength and compute gradients. For simplification, we approximate the term $\nabla_{z_t}\mathcal{L}_{AD}$ by $\nabla_{z_{t-1}}\mathcal{L}_{AD}$, enabling an initial DDIM sampling for $z_{t-1}$, followed by a straightforward optimization update. The AD loss takes as inputs the latent $z_{t-1}$ and the noise-disturbed reference latent $z_{t-1}^{ref}$, as illustrated in Fig.~\ref{fig:method}~(c). We update $z_{t-1}$ via AD loss optimization as:
\begin{equation}
    z_{t-1}:=z_{t-1}-\alpha C\cdot\nabla_{z_{t-1}}\mathcal{L}_{\mathrm{AD}}(z_{t-1},z_{t-1}^{ref}).
\end{equation}

With the guidance from AD loss, we can compute the losses on the latents with timestep conditioning, rather than converting the latent to image space and calculating image-level losses, as done in recent works~\cite{rout2024rbmodulation, liu2023more, bansal2024universal}. Using the Adam optimizer also enables us to establish a universal learning rate, alleviating the challenge of setting the guidance strength. The attention distillation guided sampling is detailed in Algorithm 2 of the supplementary. Note that the content loss proposed in Eq.~\eqref{eq:content} can also be added into the sampling process working with AD loss, further to preserve the structure of a content reference image.

% where $C$ is a constant. For convenience, we approximate the term $\nabla_{z_t}\log{p(c|z_t)}$ by $\nabla_{z_{t-1}}\log{p(c|z_{t-1})}$, allowing us to update $z_{t-1}$ in a simpler optimization manner. We define the energy function using our AD loss, which takes as inputs the latent $z_{t-1}$ and the noise-disturbed reference latent $z_{t-1}^{ref}$, and then computes the AD loss, as illustrated in Fig.~\ref{fig:method}~(c). By optimizing AD loss, we now update $z_{t-1}$ as:
% \begin{equation}
%     z_{t-1}:=z_{t-1}-\alpha C\cdot\nabla_{z_{t-1}}\mathcal{L}_{\mathrm{AD}}(z_{t-1},z_{t-1}^{ref}).
% \end{equation}

% With such guidance from AD loss, we can compute the losses directly on the latent with timestep conditioning, rather than converting the latent to image space and calculating image-level losses, as done in recent works~\cite{rout2024rbmodulation, liu2023more, bansal2024universal}. However, we experimentally found that configuring the guidance scale is tricky. To address this issue, we introduce an Adam optimizer~\cite{adam2015} to manage the scale and compute gradients automatically. We can easily establish a universal learning rate thanks to Adam optimizer, mitigating the above challenge. See Sec.~\ref{sec:exp} for further experimental analysis. We have also summarized the attention distillation guided sampling in Algorithm 2 of the supplementary.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/vae.pdf} 
    \vspace*{-1mm}
    \caption{\textbf{Improved VAE decoding.} The pretrained VAE is lossy in high-frequency details. Fine-tuning the VAE with the reference image over several steps (denoted as VAE*) can enhance the reconstruction quality and the decoding for novel image synthesis.} %, leading to upper-bounded decoded results
    \label{fig:vae}
\end{figure}



\begin{figure*}[htp]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/transfer.pdf} 
    \vspace*{-6mm}
    \caption{\textbf{Comparisons of style and appearance transfer.} Our comparisons primarily focus on recent diffusion-based methods of style and appearance transfer, including CSGO~\cite{xing2024csgo}, StyleShot~\cite{gao2024styleshot}, StyleID~\cite{chung2024styleid}, and Cross-Image Attention~\cite{alaluf2024cross}. Additionally, we include traditional methods such as NST~\cite{gaty2016styletransfer} and transformer-based methods such as StyTR$2$~\cite{Deng_Tang_Dong_Ma_Pan_Wang_Xu_2022} and SpliceViT~\cite{Tumanyan_Bar-Tal_Bagon_Dekel_2022} for comparison.}
    \label{fig:transfer}
\end{figure*}
%Our comparisons primarily focus on  style and appearance transfer for comparison


\subsection{Improved VAE Decoding}
Experimental evidence~\cite{zhu2023designing, avrahami2023blendedlatent} suggests that the VAE employed in latent diffusion models is perceptually lossy. For tasks that require high-frequency local details, such as texture synthesis, we can \emph{optionally} fine-tune the weights $\theta$ of the VAE decoder $\mathcal{D}(\cdot)$ on the example image $x$ to enhance its reconstruction quality using L1 loss, following~\cite{avrahami2023blendedlatent}:
\begin{equation}
    \theta^*=\mathrm{arg}\min_{\theta}\|\mathcal{D}_\theta(\mathcal{E}(x))-x\|_1,
\end{equation}
where $\mathcal{E}(\cdot)$ denotes the VAE encoder. Fig.~\ref{fig:vae} presents some results of the reconstruction and sampling, showing improved perceptual quality with the fine-tuned VAE.




% \section{Applications}


 % \subsection{Preliminaries: Latent Diffusion Model and KV-Injcetion}
% Recently, diffusion models have achieved state-of-the-art results in the field of image generation due to their robust capability to model complex data distributions. In particular, latent diffusion models(LDM), a.k.a Stable Diffusion , utilize a powerful pre-trained autoencoder to compress images into a latent space for training denoising network, significantly reducing the computational cost of training and sampling. Specifically, given an image $x\in\mathbb{R}^{H\times W\times3}$ in RGB space, the encoder $E$ encodes $x$ into a latent representation $z=E(x)$, and the decoder $D$ reconstructs the image from the latent representation, yielding $\tilde{x}=D(z)=D(E(x))$, where $z\in\mathbb{R}^{h\times w\times c}.$ The training objective of denoising network is defined as:
% $$L_{LDM}:=\mathbb{E}_{\mathcal{E}(x),y,\epsilon\sim\mathcal{N}(0,1),t}\bigg[\|\epsilon-\epsilon_\theta(z_t,t,y)\|_2^2\bigg]$$

% Notably, the denoising network $\epsilon_\theta$ consists of a series of Residual Block, Self Attention and Cross Attention modules, all of which are integrated within the prediction branch of the residual network.
% \begin{figure}[ht]
% \centering
% \includegraphics[scale=0.15]{figs/layers.png} 
% \caption{Method}
% % \label{fig01}
% \end{figure}

% The U-Net denoising network captures long-range dependencies among data using self-attention and incorporates general conditioning inputs, such as text or bounding boxes, through cross-attention. The formula for the attention mechanism is as follows:
% $$\text{Attention}(Q,K,V)=\text{Softmax}(\frac{QK^T}{\sqrt{d}})V$$
% The core of the attention mechanism is centered on calculating the weight matrix based on the similarity between queries (Q) and keys (K), followed by performing a weighted aggregation of values (V). Recent studies have demonstrated that values derived from the self-attention module contain rich image information, encompassing both global structures and local texture details. Tune-A-Video utilizes self-attention across batches to generate similar image content. MasaCtrl proposes a mask-guided mutual self-attention strategy that facilitates text-based non-rigid image synthesis and real image editing without the need for fine-tuning. This technique, known as KV Injection, has quickly been applied to various tasks such as image translation, style transfer, texture synthesis, and even 3D composition to maintain visual consistency. The essence of KV-Injection lies in modifying self-attention to implement cross-attention between images, leveraging the attention formula to re-aggregate visual information from reference images according to new queries generated from synthesized images. However, these methods are influenced by factors such as the residual mechanism, preventing them from fully realizing the potential of KV-Injection.

% \begin{figure}[ht]
% \centering
% \includegraphics[scale=0.5]{figs/0.jpg} 
% \caption{Method}
% \label{fig01}
% \end{figure}

% We leverage the U-Net of the SD model to extract features from images in the self-attention module. The $Q$ from the generated images and the $K_s$ and values $V_s$ from the style or texture image are processed through the attention formula to obtain the output. Based on the previous works, this process $\text{Attention}(Q,K_s,V_s)$ re-aggregates the visual information of the style image according to the $Q$ of the generated image. The output can represent an ideal stylized result, while the original $\text{Attention}(Q,K,V)$ computation yields an unstyled result. We define the L1 distance between the two as the style loss, $L_{style}$, and optimize the generated image using gradient descent. Because this method simultaneously calculates the differences between different layers, there is no error accumulation, thus reducing the impact of query errors from a specific layer. By updating the generated results through backpropagation, the process is direct and thorough. Continuous optimization gradually reduces the gap between $Q$ and $K_s$, making the attention increasingly accurate, ultimately aggregating to produce the desired visual effect.\par
% $$L_{style}=||\text{Attention}(Q,K,V)-\text{Attention}(Q,K_{s},V_{s})||_1$$
% \subsection{Content Loss}
% The style/texture transfer tasks require maintaining the semantic structure of the content image. P2P, PhotoSwap, and Pix2Pix-Zero utilize self-attention and cross-attention maps to preserve the structure of images.  PnP uses the method of injecting convolutional features to preserve the layout of the image. StyldID proposes query preservation techniques to maintain the original content.  We found that the querys from self-attention contain spatial information (visualization). Therefore, we define the L1 distance between the generated image's $Q$ and the content image's $Q_c$ as the content loss, $L_{content}$. 
% $$L_{content}=||Q-Q_c||_1$$
% The final loss is:
% $$L = L_{style} + \lambda L_{content}$$

% \subsection{Masked Attention Distillation Loss}
% To address the issue of foreground-background confusion, MasaCtrl proposed a mask-guided KV-injection approach, enabling Q to query specified regions based on a mask, thereby alleviating inaccuracies in querying. Inspired by this, we introduce the incorporation of mask guidance when calculating the attention distillation loss, thereby constraining the values queried by Q. Specifically, given an input reference image $x_s\in\mathbb{R}^{h\times w\times 3}$, its corresponding source segmentation map $S_s\in\mathbb{R}^{h\times w\times 1}$, and the target segmentation map $S_t\in\mathbb{R}^{h'\times w'\times 1}$, we first flatten Ss and St , downsample them to match the resolution of Q and K, resulting in $\bar{S_s}$ and $\bar{S_t}$ . Subsequently, we compute the guiding mask $M$; with this mask, we can control the visual information aggregated by Q during attention computation to focus solely on the corresponding regions:
% $$M^*=S_s - S_t^T$$
% $$ M:= 
% \begin{cases} 
% \text{True} & \text{if } M^* > 0 \\ 
% \text{False} & \text{otherwise} 
% \end{cases}$$
% $$\mathcal{L}_{\mathrm{MAD}}=||\mathrm{Self\text{-}Attn}(Q,K,V)-\mathrm{Self\text{-}Attn}(Q,K_{s},V_{s};M)||_{1}$$

% \subsection{Optimized VAE Reconstruction}
% The previous discussion indicates that SD employs VAE for image compression and reconstruction. Experimental evidence suggests that this method is lossy, leading to noticeable visual artifacts in certain textures. Inspired by the Blended Latent Diffusion work, we fine-tune the weights $\theta$ of the decoder on the reference image to enhance reconstruction quality.
% $$\theta^{*}=\underset{\theta}{\mathrm{argmin}}\|D_{\theta}(E\left(x\right)-x\|_1$$
% In our approach, the optimized VAE reconstruction is optional. It is only necessary in instances where the texture comprises perceptually significant fine details.
% \begin{figure}[ht]
% \centering
% \includegraphics[scale=0.3]{figs/VAE.png} 
% \caption{Method}
% \label{fig01}
% \end{figure}
% \subsection{Improved Classifier Guidance}
% \begin{equation}
% \hat{\epsilon}=\epsilon_\theta(x_t)-\sqrt{1-\alpha_t}\nabla_{x_t}\log p_\phi(y|x_t)
% \end{equation}

% \begin{equation}
% x_{t-1}=\sqrt{\alpha_{t-1}}\left(\frac{x_t-\sqrt{1-\alpha_t}\hat{\epsilon}}{\sqrt{\alpha_t}}\right)+\sqrt{1-\alpha_{t-1}}\hat{\epsilon}
% \end{equation}
% \par
% In BEATS GAN, the classifier guidance method was proposed, which uses gradients from a classifier to guide generation. Subsequently, many works have introduced various models into the sampling process of diffusion models to achieve controlled generation effects. We attempted to use this method to calculate gradients for guiding generation based on our proposed loss function. However, during the experiments, we found it difficult to find an ideal guidance scale when the number of sampling steps was small, and different textures/styles required searching for different guidance scales. The Universal Guidance for Diffusion Models proposed a repetitive iterative approach to alleviate this issue.However, their method uses the decoder to decode latent, inputting the model's predicted images, which is often inaccurate and over-consumption of computing resources. Here, we present a simpler and more effective solution to address this problem.
% %我们的优势是不需要训练额外的分类器，借助Unet自身，在潜空间计算损失.\par

%  \begin{figure}[htp]
% \centering
% \includegraphics[scale=0.3]{figs/1.jpg} 
% \caption{Method}
% \label{fig02}
% \end{figure}
%  \begin{figure}[htp]
% \centering
% \includegraphics[scale=0.3]{figs/2.jpg} 
% \caption{Method}
% \label{fig03}
% \end{figure}

% By substituting Equation (1) into Equation (2), we can rewrite it in the following form:


% \begin{align}
% \begin{split}
% x_{t-1}=&\sqrt{\alpha_{t-1}}\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta(x_t)}{\sqrt{\alpha_t}}\right)+ \\
% &\sqrt{1-\alpha_{t-1}}\epsilon_\theta(x_t)+C\nabla_{x_t}\log p_\phi(y|x_t)
% \end{split}
% \end{align}

% where, $\sqrt{\alpha_{t-1}}\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta(x_t)}{\sqrt{\alpha_t}}\right)+\sqrt{1-\alpha_{t-1}}\epsilon_\theta(x_t)$ is the original DDIM sampling result. $C=\left(\frac{(1-\alpha_t)-\sqrt{\alpha_t}\sqrt{1-\alpha_t}}{\sqrt{\alpha_t}}\right)\sqrt{1-\alpha_{t-1}}$.

% It is observed that the essence of this method is to update the latent values based on gradients, and we can fully utilize existing optimizers to manage the step size and direction of the updates. Below are the results using the Adam optimizer, with a fixed learning rate and 50 sampling steps, optimizing for a different number of iterations at each time step.
%  \begin{figure}[htp]
% \centering
% \includegraphics[scale=0.5]{figs/3.jpg} 
% \caption{Method}
% \label{fig04}
% \end{figure}

% With the help of optimizer, we can easily find a universal learning rate, avoiding the issues mentioned above. By sampling just 50 steps and performing three optimization iterations at each step, we can achieve very good results. This significantly increases the synthesis speed compared to pure optimization solutions.
% \par
% Our approach can seamlessly integrate with Classifier-free Guidance, allowing us to control the content of image generation through text prompts or by using additional networks like ControlNet and IP-Adapter for image prompts and using MultiDiffusion for arbitrary resolution texture synthesis.


%  \begin{figure}[htp]
% \centering
% \includegraphics[scale=0.3]{figs/4.jpg} 
% \caption{Method}
% \label{fig05}
% \end{figure}

\label{sec:method}

