\section{Introduction}

Optical microscopes~\cite{balasubramanian2023imagining,davidson2002optical,ma2021comprehensive} are widely used in life sciences~\cite{sijtsema1998confocal,del2022field,ghosh2023viewing}, pathological diagnosis~\cite{chen2011optical,pallen2021advances}, wafer defect detection~\cite{ma2023review}, and other fields~\cite{chen2020microsphere}. 
Traditional optical microscopes typically rely on manual focus, which is inefficient and lacks fast adjustment capabilities. 
In applications requiring rapid focus adjustments, such as long-term live cell imaging~\cite{balasubramanian2023imagining}, the focal plane can shift due to cell movement, necessitating the use of autofocus to maintain cells within the focal plane.
In addition, autofocus is critical in high-throughput applications such as whole slide imaging(WSI) systems ~\cite{li2022comprehensive,zhang2019whole,guo2019openwsi}, which require the rapid acquisition and stitching of thousands of images to achieve composite images with resolutions of billions of pixels.

Over the years, researchers have devised various techniques for autofocus, leading to two dominant methodological streams: active and passive methods.
Active autofocus systems~\cite{zhang2019novel,bian2020autofocusing,han2023novel,jiang2024large,instruments2012perfect,chan2018improving} are characterized by the use of specific optical components to focus on a chosen point or area.
For example, the laser triangulation technique is commonly used for autofocus in photolithography~\cite{zhang2019novel,bian2020autofocusing}, where the angles in a triangle are measured to determine unknown defocus distances.
The Coaxial Defocus Detection method~\cite{han2023novel,jiang2024large} leverages the shape and associated parameters of the laser spot to measure the defocus distance. 
A practical application of this can be seen in the Nikon Perfect Focus System (PFS)~\cite{instruments2012perfect}, which employs near-infrared light to identify a reference plane and subsequently adjust the focus in real time.
Phase detection autofocus (PDAF)~\cite{chan2018improving} mainly involves the design of specific CMOS sensors to calculate the focus plane, as exemplified in Nikon DSLR cameras.
Despite the rapid speed and high robustness offered by active autofocus systems, their considerable cost, large size, and limited precision present significant limitations in numerous applications.

In contrast, passive autofocus methods~\cite{huang2024deformable,chinnasamy2022review,pinkard2019deep} achieve focus by analyzing the captured image, regardless of hardware modifications~\cite{subbarao1995optimal}. They do not require a specific light path or sensor, nor do they emit probing light. More specifically, passive methods can be further classified into two distinct categories: reference-based and non-reference-based.
The most typical example of reference-based methods~\cite{huang2024deformable} is the climbing-hill  algorithm~\cite{chinnasamy2022review}. This method controls the z-axis of the microscope to move up and down to detect contrast changes of the captured image, identifying the high-contrast image that signifies correct focus. Although this method offers high accuracy, its speed is compromised due to the need for multiple mechanical adjustments of the microscope.

Recent advances in microscopy focus on accelerating the autofocus speed to facilitate applications such as dynamic cell tracking~\cite{langehanenberg2009automated} and WSI scanning~\cite{li2022comprehensive}.
This has led to the development of non-reference-based (one-shot) autofocus methods that infer the defocus distance directly from a single captured image using regression schemes, primarily employing neural networks~\cite{pinkard2019deep}.
The pioneering work by \cite{jiang2018transform} introduced a Convolutional Neural Network (CNN) to predict defocus distance, training a ResNet on approximately 130,000 images with varying defocus distances to establish a mapping between captured images and their corresponding defocused distances.
Building on this work, \cite{pinkard2019deep} integrated additional off-axis illumination sources and utilized a Fully Connected Fourier Neural Network (FCFNN) for defocus distance estimation.
Meanwhile, \cite{dastidar2020whole} employed the MobileNetV2 network, comparing pixel-wise intensity differences between two defocused images to predict defocus distance.
Further innovations include \cite{liao2022deep} and \cite{xin2021low}, who both utilized the MobileNetV3 network. \cite{liao2022deep} achieved defocus estimation without requiring additional light sources, while~\cite{xin2021low} incorporated a programmable LED array as the illumination source.
Recently, \cite{li2022learning} introduced a two-step process, involving a defocus classification network to determine the direction of defocus and a subsequent refocusing network to estimate the defocus distance.
Lastly, \cite{gu2023single} proposed the Kernel Distillation Autofocus (KDAF) method, leveraging virtual refocusing to estimate defocus distance.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/teaser_new-2.pdf}
    \caption{\textbf{The sparsity challenge in microscopy autofocus.} 
    (a) One-shot methods employ neural networks to predict focus distance directly, thus bypassing the iterative adjustments required by traditional hill-climbing techniques.
    (b) illustrates the sharpness curves for both dense and sparse content scenarios. It is evident that the sharpness curve exhibits significant variations in dense content, whereas the changes are less pronounced in sparse content.
    (c) presents images of typical pathological slides, with the upper section showing WSI thumbnails. We display both the cases of dense content and sparse content therein.The first three images are dense content, while the latter three are sparse. To facilitate observation, we enlarge one of the sparse samples and randomly select three representative fields of view, each containing only a few cells.
    (d) This observation raises a critical question: can autofocus be effectively achieved in sparse-content scenarios? }
    \label{fig:teaser}
\end{figure}

In this paper, we highlight a significant challenge that has hardly been noticed in previous methods. This challenge emerges from our observation that in real-world microscopic imaging, the captured content is often sparse, as illustrated in Figure \ref{fig:teaser}. This leads to a majority context of the image being blank, offering no assistance in estimating the defocus distance. Although previous methods employ a patch-based strategy, inferring the focal distance by sampling the observation image into patches and using a voting scheme to achieve a result, our experiments indicate a substantial decline in performance when this situation arises.

To address this, we propose a content-importance-based defocus distance estimation method, named SparseFocus, which is capable of handling autofocus issues across all levels of content sparsity, whether dense, sparse, or extremely sparse. 
This approach overcomes the limitations of current passive focusing methods in dealing with sparse issues, opening up new possibilities for autofocus in practical applicability.
Specifically, we first assign varying importance scores to different regions of the image using a fully convolutional network. Regions with dense content are assigned higher importance scores, regions with sparse content are given lower importance scores, and regions without content are assigned nearly zero importance scores.
After that, we select the top-k regions with the highest importance scores and input them into the defocus distance regression network. 
Based on the defocus distance for each region, we apply a pooling operation to derive the final result.

To train and infer our algorithm, we develop an automated microscopic imaging platform to automatically gather labeled defocused images from microscopic samples. The observational content of this dataset includes pathological tissues and cells, and it accounts for both sparse and dense scenarios. Extensive experiments on this dataset indicate that our method achieves state-of-the-art performance, surpassing previous methods by a significant margin, especially in sparse instances. Notably, even in extremely sparse scenarios, where only one region contains useful content, our method yields satisfactory results. 


\bmhead{Contributions}

\begin{itemize}
\item We observe that the richness of image content significantly influences the performance of defocus distance prediction in microscopy. When the image content is sparse, previous autofocus methods, whether traditional climbing-hill or learning-based, tend to fail.
\item We propose a content-importance-based solution featuring a novel two-stage pipeline. The first stage measures the importance of regions within the image, while the second stage calculates the defocus distance from selected important regions.
\item We collect a large-scale dataset comprising millions of defocused images to validate our approach. Experiments indicate that our method significantly outperforms others in both dense and sparse scenarios, and it can also perform autofocus with extremely sparse content.
\item We develop a WSI system equipped with our learning-based one-shot autofocus algorithm, which demonstrates promising focusing capability in real-world scenarios.
\end{itemize}


