\section{Materials and Methods}\label{sec:experiments}

An overview of our method is shown in Figure \ref{fig:framework}(a). Given a defocused image $\mathbf{I}$ as input, our object is to calculate the defocus distance $\widehat{d}$ for it.
To achieve this goal, we propose a novel two-stage pipeline. In the first stage, the captured defocused image is fed into a fully convolutional network termed Region Importance Network (RIN) $\mathcal{F}_w$, which simultaneously predicts the important weights $\widehat{\mathbf{W}} = \{ \widehat{W}_1, \widehat{W}_2, ..., \widehat{W}_n \}$ for all uniformly-split patches $\mathbf{P} = \{ P_1, P_2, ..., P_n \}$ in the image (detailed in Section~\ref{subsec:network-weight}). In the second stage, Top-k patches $\mathbf{P}_k = \{ P_i \}$ with the highest weights are selected, and a neural network named Defocus Prediction Network (DPN) $\mathcal{F}_d$ is used to predict the defocus distance $\{ \widehat{d}_i = \mathcal{F}_d(P_i) \}$ for each patch. Finally, an aggregate operation, i.e., median filtering, is used to obtain the final result $\widehat{d}$ (detailed in Section~\ref{subsec:network-defocus}). This estimated result $\widehat{d}$ drives the control system, precisely positioning the objective lens at the focus plane for optimal image sharpness.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{figs/framework_new.pdf}
	\caption{\textbf{Framework of the SparseFocus Network.} (a) is the overview of our method (SparseFocus), (b) shows the structures of the Region Importance Network (RIN) and the Defocus Prediction Network (DPN).}
	\label{fig:framework}
\end{figure}

\subsection{Region Importance Network}\label{subsec:network-weight}

We construct the Region Importance Network (RIN, shown in Figure \ref{fig:framework}(b)) $\mathcal{F}_w$ based on MobileNetV3~\cite{howard2019searching}, which is tailored to assign importance scores to image patches $\mathbf{P} = \{ P_1, P_2, ..., P_n \}$ within a defocused input $\mathbf{I}$.
Specifically, the process begins with cropping the original $2448 \times 2048$ pixel image $\mathbf{I}$ to a $2016 \times 2016$ square, followed by resizing to $512 \times 512$ pixels. This preprocessed image is then fed into RIN, which outputs a $9 \times 9$ prediction matrix $\widehat{\mathbf{W}} = \{ \widehat{W}_1, \widehat{W}_2, ..., \widehat{W}_n \}$. Each element $\widehat{W}_i$ in $\widehat{\mathbf{W}}$ quantifies the content importance of the corresponding $224 \times 224$ pixel patch $P_i$ within the original image $\mathbf{I}$.
We rank all patches $\mathbf{P}$ based on important scores and select the top-$k$ most important patches $\{P_1, P_2,..., P_k\}$ for subsequent processing. The value of $k$ is denoted as $k_d$ for dense, $k_s$ for sparse and $k_{es}$ for extremely sparse scenarios.

\subsection{Defocus Prediction Network}\label{subsec:network-defocus}

We introduce the Defocus Prediction Network (DPN, shown in Figure \ref{fig:framework}(b)) $\mathcal{F}_d$ to predict defocus distance for each selected patch ($\{ P_1, P_2, ..., P_k \}$). Initially, input patches are processed by a $4 \times 4$ convolution with stride 4 and BatchNorm2d.
These features are then refined through multiple DFENet blocks and downsampling modules. In contrast to prior networks that rely on $3 \times 3$ convolutions, the DFENet block utilizes large kernel convolutions ($7 \times 7$), enabling the model to capture comprehensive spatial information and long-range dependencies within the input patches.
Furthermore, a max pooling layer is incorporated to retain the most salient features before they are fed into a linear regression layer for defocus distance prediction $\{\widehat{d}_1, \widehat{d}_2,..., \widehat{d}_k\}$.
At last, a median filtering is employed to return the final result $\widehat{d}$.


\subsection{Supervision and Loss Function}\label{subsec:supervison}

The loss $\mathcal{L}$ contains the losses for Region Importance Network $\mathcal{L}_w$ and Defocus Prediction Network $\mathcal{L}_d$.

\bmhead{Importance Prediction Supervision}
The loss function for the $\mathcal{L}_w$ is the Binary Cross-Entropy (BCE) Loss~\cite{long2015fully}, calculated between the prediction matrix $\widehat{\mathbf{W}}$ and the Content Richness matrix $\mathbf{W}^*$. 
Let $N$ denotes the total number of elements in $\widehat{\mathbf{W}}$ or $\mathbf{W}^*$, and let $i$ and $j$ denote the indices along the $x$ and $y$ axes, respectively, the loss function for $\mathcal{L}_w$ is given by:

\begin{equation}
\mathcal{L}_w = -\frac{1}{N} \sum_{(i,j) \in \mathbf{W}^*} \left[ \mathbf{W}^*_{i,j} \log{\widehat{\mathbf{W}}_{i,j}} + (1 - \mathbf{W}^*_{i,j}) \log{(1 - \widehat{\mathbf{W}}_{i,j})} \right]
\end{equation}

\bmhead{Defocus Prediction Supervision} 
The loss function for $\mathcal{L}_d$ is the $\mathcal
{L}_2$ loss. For each selected patch $P_i \in \{ P_1, P_2, ..., P_k \}$, we obtain the true defocus distances $d^*_i \in \{ d^*_1, d^*_2, ..., d^*_k \}$. For true distance labeling, please refer to supplementary materials for more details. The loss function for $\mathcal{L}_d$ is:

\begin{equation}
\mathcal{L}_d = \frac{1}{M} \sum_{D} (\widehat{d}_i - d^*_i)^2
\end{equation}

where $M$ is the number of patches labeled as positive in the predicted matrix $\widehat{\mathbf{W}}$. $D = {\widehat{d}_1, \widehat{d}_2, \ldots, \widehat{d}_k}$ represents the predicted values from the Defocus Prediction Network.

\subsection{Implement Details}\label{subsec:implement}

During training, we augmented the data diversity by adjusting image brightness, contrast, and saturation. Brightness was varied within the range [0.9, 1.4], while contrast and saturation were adjusted within [0.8, 1.5]. Our model was trained for 600 epochs on four NVIDIA RTX 3090 GPUs. We used the Adam optimizer with an initial learning rate of $5 \times 10^{-5}$ and a batch size of 16 for the Region Importance Network. For the Defocus Prediction Network, we employed an initial learning rate of $1 \times 10^{-4}$ and a batch size of 128. We set the importance threshold $\rho$ to 0.8, and the number of selected patches $k_d$, $k_s$ and $k_{es}$ are set to $k_d=31$, $k_d=9$ and $k_{es}=3$.


\section{One-shot Autofocus enhanced WSI system}
\label{sec:wsi}

In this section, we introduce an advanced one-shot autofocus-enhanced Whole Slide Imaging system (osa-WSI), based on our learning-based autofocus algorithm, coupled with an efficient image stitching protocol for large-scale imaging and an online motion path planning. The osa-WSI encompasses several key components: 

\begin{itemize}
\item Motorized XY and Z Stages: These stages offer a resolution of 100 nm in the XY plane and 50 nm in the Z-axis, with repeatability of 400 nm in XY and 150 nm in Z, ensuring precise focusing within the microscope's depth of field.
\item Macro-Imaging System: This subsystem captures slide thumbnails and identifies specimen regions,  using both transmissive and reflective illumination modes. 
\item Microscopy Imaging System: This subsystem is designed for high-fidelity imaging of both transparent and opaque samples, equipped with a LED white light source, an APO objective lens, and a global shutter CMOS camera.
\item Motion Control System: This subsystem excels in path planning and camera triggering, enhancing the overall imaging workflow.
\item Integrated Algorithms: These include autofocus, image stitching, AI-based recognition, and image quality assessment.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{figs/WSI_process_1.pdf}
	\caption{\textbf{Overview of three WSI pipelines.} (a) represents the workflow of a traditional WSI system. (b) illustrates the pipeline incorporating one-shot autofocus technology, implemented via a two-step method. (c) demonstrates the dynamic acquisition process of the WSI pipeline using one-shot autofocus. (d) showcases the real instrumentation of our one-shot autofocus enhanced WSI system.
    }
	\label{fig:net}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{figs/microscope.pdf}
    \caption{\textbf{Autofocus across the entire slice using our osa-WSI system.}  We present two cases—cells in panel (a) and tissue in panel (b)—to demonstrate the superior autofocus capability of our osa-WSI system. Readers can compare the accuracy of our method with the traditional KFP interpolation approach by examining the error maps provided in the figures.}
	\label{fig:wsi}
\end{figure}

These technologies establish our WSI system as a cutting-edge tool in the field of pathology.


Figure~\ref{fig:net} presents a comparison of the workflow pipelines between the proposed osa-WSI and a traditional WSI system. The traditional system, depicted in Figure~\ref{fig:net}(a), relies on interpolating from pre-selected Key Focus Points (KFPs) to create a FocusMap for the entire slide, which is inefficient and susceptible to significant errors on uneven sample surfaces. 
Our osa-WSI system addresses these issues, as outlined in Figures~\ref{fig:net}(b) and \ref{fig:net}(c). Figure~\ref{fig:net}(b) describes a two-step process that maintains the FocusMap construction but with improved speed and accuracy. In contrast, Figure~\ref{fig:net}(c) illustrates a one-step process that generates FocusMaps in real-time during scanning, thereby eliminating the need for pre-construction.


Furthermore, Figure~\ref{fig:wsi} provides two examples of defocus distance estimation across the entire slice using the proposed osa-WSI, termed the FocusMap. This method outperforms the traditional KFP interpolation technique in focal estimation accuracy, as evidenced by the error map comparisons.

