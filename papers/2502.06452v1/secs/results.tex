\section{Results}\label{sec:method}

\subsection{Dataset}\label{dataset}

\bmhead{Data Capturing}

Considering the scarcity of large-scale, labeled dataset suitable for our network training, we have developed an automated system for microscopic image acquisition coupled with an auto-labeling framework.
Please refer to Section~\ref{sec:wsi} for more information about our automated microscopy system.

To improve the sample diversity, we obtain a large collection of pathological tissues and cellular specimens, including samples from the lung, liver, prostate, and exfoliated cervical cells. The cells are stained using Papanicolaou stain, while the tissues are stained with Hematoxylin and Eosin (H\&E).
Specifically, we first collect $400$ samples from patients of different ages and diseases, and then retain $75$ cell samples and $63$ pathological tissue samples, considering the distinct similarity of most samples. After that, we categorize the data collection into $6$ distinct groups according to the type (cell or tissue) and sparsity (dense, sparse or extremely sparse) of the data. Selected samples are illustrated in the supplementary material.
For each sample, we randomly selected 100 fields of view, each with a resolution of $2016 \times 2016$ pixels. To collect defocus data, a sequence of z-stack images was captured at $101$ different defocus distances, with a step size of $0.5\si{\micro\meter}$ ranging from $-25\si{\micro\meter}$ to $+25\si{\micro\meter}$. In total, we obtain $1,324,211$ pathological microscopic images with defocus distance labels.


\bmhead{Statistics}
We categorize the data based on sparsity and type into six groups. For cells, we have $3345$ dense, $2054$ sparse and $1824$ extremely sparse z-stacks. For tissues, we have $2923$ dense, $1442$ sparse and $1723$ extremely sparse z-stacks.
These are then divided into training, validation, and test sets in an 8:1:1 ratio, ensuring a comprehensive and balanced evaluation of our method.

\subsection{Evaluation Protocols}\label{preliminary}

In learning-based autofocusing methods, \textit{Mean Absolute Error (MAE)} of the predicted defocus distance is often employed to evaluate their performance. However, relying solely on the \textit{MAE} for assessment has some limitations. For example, it does not indicate whether the estimated defocus distance falls within the Depth of Field (DoF), nor does it reveal if the defocus direction is incorrectly estimated. As a result, we introduce two additional metrics: \textit{DoF-Accuracy} and \textit{Direction Success Score (DSS)}. The \textit{DoF-Accuracy} evaluates the degree to which the predicted focus point falls within the Depth of Field (DoF), providing a comprehensive measure of how well an algorithm can achieve autofocus, especially for microsystems with varying parameters and distinct DoFs. The \textit{DSS} quantifies the precision of the direction prediction. We introduce this metrics because an incorrect direction prediction by the autofocus algorithm can lead to more severe image defocus, potentially placing the defocus distance outside the system's operational range and preventing successful focus resolution.


The \textit{MAE} quantifies the accuracy of the autofocus algorithm by determining the average absolute difference between the predicted and true defocus distances, which is calculated as:
\begin{equation}
MAE = \frac{1}{|D|} \sum_D ||e_{d_i}|| = \frac{1}{|D|} \sum_D ||\widehat{d_i} - d^*_i||,
\end{equation}
where $e_{d_i}$ represents the absolute error, $\widehat{d_i}$ is the predicted defocus distance, $d^*_i$ is the ground truth defocus distance, and $|D|$ is the number of samples in the test dataset. 

\textit{DoF-Accuracy} measures the percentage of absolute errors $e_{d_i}$ that fall within $1/n$ DoF (in this study, we set $n = \{1, 2, 3\}$), allowing for a fair comparison between optical microscope systems with different DoF. \textit{DoF-Accuracy} is defined as:
\begin{equation}
DoF\text{-}Accuracy = \frac{1}{|D|} \sum_D \mathbb{I}\left(||e_{d_i}|| \le \frac{1}{n} DoF\right) \times 100\%.
\end{equation}
 
The indicator function $\mathbb{I}(\cdot)$ is defined as:
\begin{equation}
\mathbb{I}(P) = \begin{cases} 1 & \text{if } P \text{ is true} \\ 0 & \text{if } P \text{ is false} \end{cases}
\end{equation}

The \textit{DSS} measures the percentage of cases where the predicted direction of defocus aligns with the true direction, which is defined as:

\begin{equation}
DSS = \frac{1}{|D|} \sum_D \mathbb{I}\left(\operatorname{sgn}\left(\widehat{d_i} \cdot d^*_i\right) \ge 0\right) \times 100\%
\end{equation}

where $\operatorname{sgn}(\cdot)$ is the sign function, defined as:

\begin{equation}
\operatorname{sgn}(x) =
\begin{cases}
1 & \text{if } x > 0 \\
0 & \text{if } x = 0 \\
-1 & \text{if } x < 0
\end{cases}
\end{equation}

For the ``boundary'' case where $d^*_i = 0$, indicating that the image is already in focus, the predicted defocus direction, whether positive or negative, can be considered correct.


\subsection{Experimental Results}
We conduct an extensive evaluation of our algorithm's performance utilizing the large-scale dataset that we have assembled.
To demonstrate the performance of our method, we compared it with several learning-based baselines, including those proposed by Dastidar et al.~\cite{dastidar2020whole}, Liao et al.~\cite{liao2022deep}, Jiang et al.~\cite{jiang2018transform} and Li et al.~\cite{li2022learning}.
For each baseline, we partition the image into non-overlapping patches in grid mode, infer the defocusing distance of each patch, and get the result by a median filter operation.
To maintain the fairness of the experiments, all baseline models are trained under the same condition as our proposed method, including the use of the same datasets and hyperparameter settings (such as learning rate, batch size, and number of iterations).

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.95\linewidth]{figs/Results.pdf}
 	\caption{\textbf{Comparing with baselines using regression plots.} The regression plots show the results, with the horizontal axis representing the ground-truth values and the vertical axis indicating the predicted results. In (a), we mark the upper and lower boundaries of the DoF with lines to illustrate the distribution of results within this range. An ideal zero-error line is included for reference, indicating that the closer a point is to this line, the smaller the error and the better the outcome. Points within the depth of field represent accurate predictions, whereas those outside indicate significant errors. Incorrect predictions are located in the opposite quadrant. In (b), we compare with baselines using such regression plots with different content sparsity.}
	\label{fig:scatters}
\end{figure}

First, we evaluated our method using randomly selected samples from each z-stack, as presented in regression plots in Figure~\ref{fig:scatters}. 
In Figure~\ref{fig:scatters}(a), we provide case study of our method with a baseline method (Jiang et al.~\cite{jiang2018transform}) under extremely sparse conditions, involving only one cell. At a defocus distance of $+25\si{\micro\meter}$, the cell is nearly invisible. Despite this, our method accurately predicts the defocus distance and direction, achieving an error of just $0.3\si{\micro\meter}$.
Figure~\ref{fig:scatters}(b) presents more testing results, encompassing scenarios of high density, sparsity, and extreme sparsity. The test results demonstrate that our method significantly outperforms the baseline, particularly under sparse and extremely sparse conditions. The majority of our method's predictions fall within the DoF, unlike comparing baselines, which exhibit substantial incorrectness and frequent directional errors, rendering them incapable of accurate focusing under such conditions.
More comparative experiments with other baselines are provided in the supplementary materials.

Next, we assess the performance of the proposed method using the three metrics defined in this paper: \textit{MAE (Mean Absolute Error)}, \textit{DoF-Accuracy} and \textit{DSS (Direction Success Score)}. Table~\ref{tab:dde} presents a comparative analysis of the \textit{MAE} for predicted defocus distances. The results demonstrate that our method consistently achieves the lowest \textit{MAE} and variance across all conditions, regardless of the sparsity of the data content.
Notably, for images with sparse content, our method surpasses previous methods by a very large margin, reducing the error rate by an order of magnitude.
This improvement is particularly striking when considering images with extreme sparsity, where our method successfully performs autofocus with remarkable accuracies of $0.60\si{\micro\meter}$ for cells and $0.45\si{\micro\meter}$ for tissues, while other baselines basically fail.
This finding highlights the accuracy, stability, and robustness of the proposed method.

\begin{table}[htbp]
\caption{\textbf{Comparing with baselines using \textit{MAE}.} We report the mean absolute error (\textit{MAE}) and variance across varying levels of sparsity (dense, sparse, extremely sparse).}
\label{tab:dde}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{MAE ($\si{\micro\meter}\downarrow$)} & \multicolumn{3}{c}{Cell} & \multicolumn{3}{c}{Tissue} \\
\cmidrule(l){2-4} \cmidrule(l){5-7}
 & Dense & Sparse & Ex-sparse & Dense & Sparse & Ex-sparse \\ 
\midrule
Dastidar et al. \cite{dastidar2020whole} & $0.40\pm0.50$ & $4.63\pm5.67$ & $9.79\pm7.68$ & $0.93\pm1.70$ & $6.53\pm6.24$ & $10.05\pm7.41$ \\ 
Liao et al. \cite{liao2022deep} & $0.66\pm1.19$ & $4.93\pm5.79$ & $9.95\pm7.16$ & $1.70\pm5.09$ & $5.94\pm7.12$ & $10.42\pm6.94$ \\ 
Jiang et al. \cite{jiang2018transform} & $0.73\pm1.93$ & $6.19\pm7.90$ & $8.92\pm7.34$ & $1.42\pm4.55$ & $4.88\pm6.62$ & $8.73\pm6.69$ \\ 
Li et al. \cite{li2022learning} & $0.72\pm0.53$ & $3.93\pm6.28$ & $12.25\pm10.65$ & $1.61\pm5.05$ & $6.47\pm7.26$ & $14.24\pm11.32$ \\ 
\textbf{Ours} & $\mathbf{0.37\pm0.29}$ & $\mathbf{0.51\pm0.40}$ & $\mathbf{0.60\pm0.46}$ & $\mathbf{0.49\pm0.40}$ & $\mathbf{0.43\pm0.33}$ & $\mathbf{0.45\pm0.36}$ \\ 
\bottomrule
\end{tabular}
\end{table}

The effectiveness of our method is also substantiated through the \textit{DoF-Accuracy}, as illustrated in Table~\ref{tab:dof_cell}. While achieving comparable performance in dense scenarios, our method exhibits a significant advantage in sparse and extremely sparse cases.
In sparse situations, the defocus distances predicted by other baselines often fall outside the DoF, leading to unsuccessful autofocus.
In contrast, our method consistently achieves accurate focus. For instance, in the extremely sparse case, 58.23\% of cell dataset and 71.71\% of tissue dataset fell within the DoF.


\begin{table}[htbp]
\caption{\textbf{Comparing with baselines using \textit{DoF-Accuracy} for cell (top) and tissue (bottom) samples.} We report the \textit{DoF-Accuracy} at $n = \{1, 2, 3\}$ across different sparsity levels (dense, sparse, extremely sparse).}
\label{tab:dof_cell}
\begin{tabular}{lccccccccc}
\toprule
\multirow{2}{*}{DoF-Accuracy (\% $\uparrow$)} & \multicolumn{3}{c}{Dense} & \multicolumn{3}{c}{Sparse} & \multicolumn{3}{c}{Ex-sparse} \\
\cmidrule(l){2-4} \cmidrule(l){5-7} \cmidrule(l){8-10}
& 1/3 & 1/2 & 1 & 1/3 & 1/2 & 1 & 1/3 & 1/2 & 1 \\
\midrule
\multicolumn{10}{c}{Cell} \\
\midrule
Dastidar et al. \cite{dastidar2020whole} & 35.18 & 50.24 & \textbf{80.95} & 11.93 & 17.20 & 31.49 & 4.07 & 5.75 & 11.27 \\
Liao et al. \cite{liao2022deep} & 23.40 & 33.35 & 62.13 & 10.75 & 15.53 & 28.52 & 2.26 & 3.21 & 7.08 \\
Jiang et al. \cite{jiang2018transform} & 30.95 & 42.54 & 67.15 & 7.90 & 11.34 & 20.64 & 4.61 & 6.34 & 10.95 \\
Li et al. \cite{li2022learning} & 27.46 & 35.10 & 50.12 & 7.41 & 10.07 & 14.39 & 1.85 & 2.76 & 4.20 \\
\textbf{Ours} & \textbf{35.99} & \textbf{50.81} & 80.46 & \textbf{26.58} & \textbf{37.60} & \textbf{66.55} & \textbf{23.18} & \textbf{33.73} & \textbf{58.23} \\
\midrule
\multicolumn{10}{c}{Tissue} \\
\midrule
Dastidar et al. \cite{dastidar2020whole} & 20.48 & 29.83 & 53.37 & 4.28 & 6.75 & 14.50 & 1.75 & 2.88 & 6.19 \\
Liao et al. \cite{liao2022deep} & 20.21 & 29.29 & 55.82 & 7.99 & 12.13 & 25.37 & 1.75 & 2.81 & 4.81 \\
Jiang et al. \cite{jiang2018transform} & 20.81 & 29.14 & 56.35 & 6.87 & 10.33 & 22.41 & 4.06 & 5.19 & 8.63 \\
Li et al. \cite{li2022learning} & 24.83 & 33.03 & 47.42 & 6.08 & 7.66 & 11.04 & 1.81 & 2.44 & 3.75 \\
\textbf{Ours} & \textbf{26.54} & \textbf{39.38} & \textbf{69.72} & \textbf{27.53} & \textbf{40.85} & \textbf{74.69} & \textbf{28.16} & \textbf{41.74} & \textbf{71.71} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:da} illustrates the \textit{Direction Success Score (DSS)}. Our method achieves exceptional performance, exceeding 99\% for all conditions. In contrast, other methods exhibit numerous incorrect predictions, particularly in sparse and extremely sparse scenarios.

\begin{table}[htbp]
\caption{\textbf{Comparing with baselines using \textit{DSS}.} We report the direction success score (\textit{DSS}) across different sparsity levels (dense, sparse, extremely sparse).}
\label{tab:da}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{DSS (\% $\uparrow$)} & \multicolumn{3}{c}{Cell} & \multicolumn{3}{c}{Tissue} \\
\cmidrule(r){2-4}
\cmidrule(l){5-7} & Dense & Sparse & Ex-sparse & Dense & Sparse & Ex-sparse \\
\midrule
Dastidar et al. \cite{dastidar2020whole} & \textbf{100.00} & 95.43 & 69.48 & \textbf{99.95} & 96.13 & 63.38 \\
Liao et al. \cite{liao2022deep} & \textbf{100.00} & 95.64 & 75.92 & 95.49 & 84.38 & 68.15 \\
Jiang et al. \cite{jiang2018transform} & 99.69 & 83.99 & 76.83 & 97.27 & 92.12 & 83.98 \\
Li et al. \cite{li2022learning} & \textbf{100.00}  & 92.89 & 63.23 & 97.46 & 88.86 & 53.41 \\
\textbf{Ours} & \textbf{100.00} & \textbf{99.80} & \textbf{99.96} & 99.77 & \textbf{99.91} & \textbf{99.94}\\ 
\botrule
\end{tabular}
\end{table}


To further evaluate our method's performance across varying defocus distances, we divided the defocus distance into non-overlapping intervals and calculated the \textit{MAE} for each interval. 
Figure~\ref{fig:bars} displays the error distribution histogram. 
It can be observed that the while \textit{MAE} of almost all methods increases with greater defocus distance, the degree of increase varies significantly. In dense scenarios, the \textit{MAE} remains relatively stable across defocus distances. However, in sparse and extremely sparse scenarios, baseline methods exhibit a significant rise in \textit{MAE} as defocus distance increases. 
For example, in sparse tissue samples, the \textit{MAE} of Dastidar et.al~\cite{dastidar2020whole} increases from approximately $\num{1}~\si{\nano\meter}$ to about $\num{16}~\si{\nano\meter}$, resulting in autofocus failure. 
In contrast, our method shows minimal sensitivity to varying defocus distance, consistently maintaining low error levels regardless of the initialization to the focal plane.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{figs/bars_with_curve.pdf}
	\caption{\textbf{Comparing with baselines using \textit{MAE} histogram.} The histogram illustrates the \textit{MAE} performance of our method compared to baseline approaches across various defocus distance for both cell and tissue samples.}
	\label{fig:bars}
\end{figure}
