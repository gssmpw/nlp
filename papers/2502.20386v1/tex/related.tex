\section{Related Work}
Our proposed work lies at the intersection of active perception, efficient language-embedded Gaussian splatting, hierarchical semantic-metric mapping and planning, and task-driven navigation.
We propose a compact hierarchical language-embedded map representation built upon Gaussian splatting that can enable task-driven autonomous navigation while incrementally exploring and building a map of the environment.
\subsection{Active Perception}
Active perception methods allow the robot to actively select actions and viewpoints that maximize the information gain relevant to a given task.
This problem has been widely studied in volumetric representations such as voxel maps or Signed Distance Field (SDF) maps.
In~\cite{charrow2015information, saulnier2020information, asgharivaskasi2023semantic, bai2016bayesianexp}, the information gain is evaluated as the mutual information between the current map and expected map given future observations.
To accelerate the computation, information gain can be approximated with cell counting-based methods~\cite{LukasIG, bircher2016receding, Papachristos17UncertaintyIG, alexis2020MP,  schmid2021unified, yuezhantao2023seer, tao-24-3dactivemsslam}.
The information-driven approach has also been widely adopted for autonomous exploration with learned representations such as Neural Radiance Fields (NeRFs) or neural SDFs. 
Several approaches~\cite{pan2022activenerf} estimate information gain in NeRFs by selecting future observations from a training dataset or by sampling viewpoints in the radiance fields~\cite{Lee2022nerf3drecon, zhan2022activermap, ran2023neurar, he2023active, he2024active}.
In neural SDFs, ~\cite{yan2023active} evaluates information gain as the variance of the model induced by parameter perturbations.
The approach in ~\cite{feng2024naruto} quantifies information gain by learning the reconstruction uncertainty with an MLP.
Fisher Information has been used as the proxy for evaluating mutual information for 3D Gaussian splatting~\cite{jiang2023fisherrfactiveviewselection}.
Combining a voxel map with 3D Gaussian Splatting, in~\cite{jin2024gsplanner}, the information gain is evaluated as the weighted sum of unobserved volume between rays in the occupancy map, where the weights are determined by the transmittance of the Gaussians.
The method has been further extended in~\cite{xu2024hgs}, by incorporating Fisher Information as part of the information gain of a candidate viewpoint. 
Motivated by the theory of Kalman filtering, \cite{tao2024rt} approximates uncertainty with the magnitude of the parameter updates of the map, which is then used for estimating the information gain of candidate viewpoints.
%
\subsection{3D Gaussian Splatting}
3D Gaussian splatting proposed in \cite{kerbl20233dgs} provides a unique representation that captures geometry with Gaussians while encoding color and opacity information for high-quality rendering of scenes.
Building upon this representation, several approaches focus on real-time construction of such maps with color and depth measurements in simultaneous localization and mapping (SLAM) frameworks.
The method in \cite{keetha2024splatam} employs a silhouette mask on rendering for efficient optimization of the scene.
The work in \cite{matsuki2024gaussian} addresses the monocular SLAM problem with 3DGS along with an analytical Jacobians for pose optimization.
The authors of \cite{peng2024rtg} propose handling Gaussian parameters differently for color and depth rendering by more explicitly representing surfaces for depth rendering, resulting in a more memory-efficient representation of the scene.
Similarly, \cite{hu2025cg} estimates stability and uncertainty of the Gaussians for efficient representation of the scene.
An advantage of the explicit representation of the Gaussian points is the ease of collision checking.
In comparison with other works, most notably \cite{chen2024splat}, we handle collision avoidance as chance constraints - in our case the size of the confidence ellipsoids depends on the number of Gaussians in the environment.
%
There are also numerous methods that address the problem of scene understanding with language-embedded 3DGS.
These approaches typically obtain 2D language features from Contrastive Languageâ€“Image Pre-training \cite{radford2021learning} (CLIP) and other foundation models, and distill these image features into 3D Gaussians.
Methods in \cite{shi2024language,qin2024langsplat,liao2024clip} use scene-specific autoencoders or quantization to obtain compact representations of the language features.
Other approaches \cite{yu2024language,zuo2024fmgs} leverage feature fields similar to the method proposed in \cite{kerr2023lerf} for Neural Radiance Fields (NeRFs).
In this work, we focus on an explicit representation that is amenable to planning.
We associate every Gaussian point in the map with a language embedding that is subsequently compressed using its principal components, yielding a scene-agnostic language-embedded map.

A sub-class of approaches \cite{zhu2024loopsplat, yugay2023gaussian} processes the scene as submaps by only optimizing the Gaussians on the submap level.
The approach in \cite{zhu2024loopsplat} performs submap alignment using multi-view pose refinement on keyframes images. 
Similar to these methods, we use a submapping approach for efficient storage of the map, focusing instead on the problem of optimizing Gaussians across multiple submaps to facilitate large-scale navigation.

\subsection{Semantic Mapping and Task-based Navigation}
Metric-semantic maps, which integrate both geometric and semantic information, provide actionable and informative representations of the environment.
Common forms of metric-semantic maps include semantics-augmented occupancy maps~\cite{asgharivaskasi2023semantic, dang2018autonomous}, object-based semantic maps~\cite{liu2024slideslam} and 3D scene graphs~\cite{armeni20193d, wu2021scenegraphfusion, looper20233d, hughes2024ijrr}. Among these, 3D scene graphs have emerged as a powerful representation, capable of capturing broader semantic concepts and the underlying contextual relationships within environments~\cite{armeni20193d}.
These models provide a compact, symbolic representation of semantic entities in the environments and their relationships~\cite{wu2021scenegraphfusion, looper20233d}. 
The concept of hierarchical representations of semantics and geometry was established by \cite{armeni20193d}, which integrated multiple layers of information at increasing levels of abstraction.
Others~\cite{hughes2024ijrr} incorporated additional information such as free space, object detections, and room categories on top of geometric representations. 
This was extended by \cite{bavle2023s} to incorporate structural elements (\eg walls) and clustering rooms. 
Large Language Models (LLMs) have also been utilized to infer semantic relationships in the hierarchy~\cite{Strader24ral-autoAbstr} where structure is not readily available. 
To further generalize the use of scene graphs and capture a broader range of concepts for more complex robotic tasks, language features have been integrated to enable \emph{open-set} scene understanding. 
Clio~\cite{maggio2024clio} constructs a hierarchical scene graph, where the set of objects and regions are inferred from the list of given tasks.
Similarly, OrionNav~\cite{devarakonda2024orionnav} leverages open-set segmentation and LLM-based room clustering to build open-vocabulary scene graphs.
ConceptGraphs~\cite{conceptgraphs} constructs open-vocabulary 3D scene graphs from RGB-D image sequences by leveraging 2D foundation models for instance segmentation and projecting semantic features into 3D point clouds. 
It fuses multi-view information to create 3D objects annotated with vision and language descriptors, and uses large vision-language models to generate object captions and infer inter-object relations, resulting in comprehensive 3D scene graphs. 
HOV-SG (Hierarchical Open-Vocabulary 3D Scene Graph)~\cite{werby2024hierarchical} extends these capabilities to large-scale, multi-story environments. 
It introduces a hierarchical structure encompassing floors, rooms, and objects, each enriched with open-vocabulary features derived from pre-trained vision-language models. 
While these methods that obtain an explicit semantic label at each level of a sparse, hierarchical representation capture the structure of known indoor environments, such approaches cannot be directly applied to unknown and unstructured environments. We instead try to find a balance between the sparsity provided by the hierarchical representation in scene graphs and the flexibility provided by dense, continuous feature embeddings. 
%

\subsection{Task and motion planning}
A fundamental challenge in task-driven planning (beyond mapping and exploration) is the identification of objects and following paths likely to result in localizing the object in the minimum possible time.
~\cite{papatheodorou2023finding} proposed a planner that prioritizes the discovery of objects of interest and ensures their complete and high-resolution reconstruction. 
Learning based approaches that estimate semantic maps beyond line of sight have also been proposed ~\cite{georgakis2021learning} utilizing confidence in semantic labels from unobserved regions to guide the search.
Recent work has also focused on leveraging hierarchical scene graphs for semantic search tasks~\cite{dai2024optimalscenegraphplanning} using LLMs to guide an optimal hierarchical planner for semantic search, although these approaches assume that the map is available \textit{a priori}. 
Partially Observable Markov Decision Processes (POMDPs) have also been considered for planning on partially unknown scene graphs~\cite{amiri2022reasoning}. 
Reinforcement Learning based methods ~\cite{ravichandran2022hierarchical} have also been used to leverage scene graphs and graph neural networks to compute navigation policies, biasing the search toward regions of interest.
Recently, Vision Language Frontier Maps (VLFM)~\cite{yokoyama2024vlfm} utilized a Vision-Language Model (VLM) for estimating the similarity between the task and images projected onto a bird's eye view to score frontiers by their relevancy.
Large Language Models have also been directly used to select robot behaviors from a fixed library to select actions that minimize the time required to complete a task~\cite{ravichandran2024spine}.
While these methods either focus entirely on completing the task with minimal maps along the way, we instead focus on generating a rich map that is reusable across a variety of tasks.