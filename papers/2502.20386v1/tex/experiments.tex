\section{Experimental Evaluation}
We design our experimental protocol to first evaluate each component of our method with prior work for fair comparison and then demonstrate the full framework on a real robot.
The key attributes of our mapping method are: (i) 3D Semantic Segmentation; (ii) Submapping; (iii) Memory Efficiency and (iv) Retrieval of objects in the pre-built maps.
Further, we demonstrate closed-loop autonomy through real robot experiments, with and without a prior map.

\subsection{Dataset Experiments}
\label{sec:seg-experiments}

\subsubsection{Open-vocabulary 3D semantic segmentation}
 We evaluate the 3D semantic segmentation performance of our mapping approach on the ScanNet~\cite{dai2017scannet} dataset, using scenes: scene0011\textunderscore00, scene0050\textunderscore00, scene0231\textunderscore00, scene0378\textunderscore00, scene0518\textunderscore00.

 \textbf{Metrics.} We compute the mean Intersection Over Union (mIOU), mean accuracy (mAcc) and frequency-weighted mIOU (F-mIOU) following the same procedure in \cite{werby2024hierarchical}.
 
 \textbf{Results.} The results are presented in Table~\ref{table:scannet}.
 ConceptGraphs~\cite{conceptgraphs} and HOV-SG~\cite{werby2024hierarchical} leverage image segmentation priors for creating object clusters and maintain sparse language feature vectors for each object.
 In contrast, ConceptFusion~\cite{jatavallabhula2023conceptfusion} is more similar to our work and stores dense language features in the map. 
 We note that ConceptFusion also uses image segmentation priors for extracting local features on an object level.
 With our compressed language feature representation and without using image segmentation priors, we are able to achieve comparable performance with the baselines.
 We note that we also use a smaller CLIP backbone compared to the baseline methods.
%
\begin{table}[!tbh]
\centering
\begin{tabular}{
P{0.13\textwidth}|
P{0.08\textwidth}|
P{0.045\textwidth}|
P{0.045\textwidth}|
P{0.06\textwidth}}
\hline
\textbf{Method} & \textbf{Feature embeddings} & \textbf{mIOU} & \textbf{mAcc} & \textbf{F-mIOU} \\
\hline
%
ConceptGraphs~\cite{conceptgraphs} & \multirow{2}{*}{Sparse} &\cellcolor{orange!40} 0.16 & 0.20 & \cellcolor{orange!40}0.28 \\ 
HOV-SG~\cite{werby2024hierarchical} &  &\cellcolor{red!40} \textbf{0.22} & \cellcolor{red!40} \textbf{0.30} & \cellcolor{red!40} \textbf{0.43} \\ \hline 
ConceptFusion~\cite{jatavallabhula2023conceptfusion} & \multirow{2}{*}{Dense} & 0.11 & 0.12 & 0.21 \\ 
ATLAS (ours) & & 0.15 & \cellcolor{orange!40}0.27 & 0.16 \\
\hline
\end{tabular}
\bigskip
\caption{Open-vocabulary 3D semantic segmentation on ScanNet. Baseline results from \cite{werby2024hierarchical}. Baselines use Vit-H-14 while our method uses Vit-B-16 for the CLIP backbone.}
\label{table:scannet}
\end{table}


\subsubsection{Loop closure}
%
To evaluate the effectiveness of pose correction with submaps, we generate maps from the same set of data.
We generate three maps, one with just VIO pose estimates, one with the addition of PGO, and finally one with submap anchor pose updates on top of the PGO.
We use LiDAR odometry~\cite{fasterlio} to obtain groundtruth poses for the test set.
We compare the rendered images from each map and evaluate the rendering quality.
Finally, to present the upper bound of the Gaussian splatting approach on this dataset, we also generate a map using the test set poses.

\textbf{Metrics.}
We evaluate the Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index \cite{1292216} (SSIM), Learned Perceptual Image Patch Similarity \cite{zhang2018perceptual} (LPIPS) on the color images and Root Mean-Square-Error (RMSE) on the depth images.

\textbf{Results.}
The results are presented in Table~\ref{table:loop}. We show that we are able to leverage PGO with our submap-based mapping framework to achieve better reconstruction quality.

\begin{table}[!th]
\centering
\begin{tabular}{c|c|c|c|c}
\hline
\multirow{2}{*}{\textbf{Method}} & \textbf{PSNR} $\uparrow$ & \textbf{SSIM} $\uparrow$ & \textbf{LPIPS} $\downarrow$ & \textbf{RMSE} $\downarrow$ \\
& (dB) & & & (m) \\ \hline  
VIO & 12.62 & 0.53 & 0.48 & 0.89 \\
VIO + PGO & 14.09 & 0.54 & 0.47 & 0.85 \\
VIO + PGO + SU & \cellcolor{orange!40}14.31 & \cellcolor{orange!40}0.59 & \cellcolor{orange!40}0.44 & \cellcolor{orange!40}{0.76} \\
GT & \cellcolor{red!40}\textbf{16.13} & \cellcolor{red!40}\textbf{0.65} & \cellcolor{red!40}\textbf{0.37} & \cellcolor{red!40}\textbf{0.39} \\
\hline
\end{tabular}
\bigskip
\caption{Evaluation of loop closure-aware submap pose graph update. VIO refers to visual-inertia odometry, PGO refers to pose graph optimization, SU refers to submap anchor pose updates and GT refers to the ground truth poses.
This experiment shows that our method is able to correct past poses by shifting the submap anchors. }
\label{table:loop}
\end{table}

\subsubsection{Memory}
%
To highlight the value of the dynamic loading of submaps, we evaluate several 3DGS approaches on a large indoor scene.
%
We use scene 00824 from the Habitat Matterport 3D Semantics Dataset \cite{yadav2023habitat} (HM3D), following the method in \cite{werby2024hierarchical} to generate groundtruth observations and poses. 
If they support submapping, methods are evaluated with different submap sizes, at 2m and 5m distances between each submap.
We note that Gaussian-SLAM and LoopSplat both employ submapping but only unload submaps and do not handle reloading and updating of submaps.

\textbf{Results.}
The results are presented in Table.~\ref{table:memory}.
%
Gaussian-SLAM~\cite{yugay2023gaussian} and LoopSplat~\cite{zhu2024loopsplat} require large amounts of memory even at relatively small submap sizes.
The results highlight that many Gaussian splatting SLAM methods cannot support memory-efficient mapping of large-scale environments, even on the scale of indoor environments.
Ensuring efficient scaling of memory is crucial for operation on robots with limited compute. 
\begin{table}[!bh]
\centering
\begin{tabular}{
P{0.135\textwidth}|
P{0.08\textwidth}|
P{0.08\textwidth}|
P{0.085\textwidth}}
\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{{Memory Allocated / Reserved (GB)}}} \\
\cline{2-4}
& 2m submap & 5m submap & No submap \\
\hline
Gaussian-SLAM~\cite{yugay2023gaussian} & \textbf{5.71} / 36.21 & $\times$ & $\times$ \\
LoopSplat~\cite{zhu2024loopsplat} & 9.03 / 21.33 & $\times$ & $\times$ \\
SplaTAM~\cite{keetha2024splatam} & -- & -- & \textbf{13.36} / \textbf{16.32} \\
ATLAS (ours) & 8.28 / \textbf{9.68} & \textbf{9.77} / \textbf{12.51} & \textbf{13.36} / \textbf{16.32} \\
\hline
\end{tabular}
\bigskip
\caption{Comparison of memory performance on scene 0011 of the HM3D dataset. -- indicates that the method does not support submapping and is not evaluated with submaps. $\times$ indicates that the method failed due to excessive memory requirements or otherwise. Gaussian-SLAM and LoopSplat are evaluated with submaps. }
\label{table:memory}
\end{table}

\begin{figure*}[!t]
    \centering
    \subfloat[The output of Vision Language Model for the ``find boardwalk near road" task]{\includegraphics[width=0.33\linewidth,trim={2.5cm 5cm 2.5cm 6cm},clip]{diagrams/river-long-vlm.pdf}}
    %\label{subfig:1}
    \hfill
    \subfloat[The output of Vision Language Model for the ``inspect road blockage" task.]{\includegraphics[width=0.33\linewidth,trim={2.5cm 5cm 2.5cm 6cm},clip]{diagrams/road-blockage-vlm.pdf}}
    %\label{subfig:2}}
    \hfill
    \subfloat[The output of Vision Language Model for the ``find parking lot" task.]{\includegraphics[width=0.33\linewidth,trim={2.5cm 5cm 2.5cm 6cm},clip]{diagrams/parking-lot-vlm.pdf}}\label{subfig:3}
    \caption{Qualitative results showing the output of the VLM when the task terminates.}
    \label{fig:main}
\end{figure*}

\subsubsection{Image rendering from built map}
Finally, a feature of our method is the ability to query images of relevance and provide textual feedback. 
We compare this functionality with HOV-SG~\cite{werby2024hierarchical} since they generate a dense colored point cloud.
The resolution of the point cloud generated by HOV-SG is set to the default of 0.05m.
Since HOV-SG stores dense features in each point, increasing the resolution of the point cloud is too expensive in terms of both compute and memory.
We generate images from the point cloud by projecting the points using the corresponding camera intrinsics of the scene.
We render images with both methods for all poses in the dataset.
For this comparison, we use scene 0011 from the ScanNet~\cite{dai2017scannet} dataset.
In addition to this, we show a qualitative result of the retrieval capability of our method from a pre-built map, constructed from data collected by tele-operating our robot.

\textbf{Metrics.}
We evaluate the image reconstruction quality using the following metrics -- Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index \cite{1292216} (SSIM), Learned Perceptual Image Patch Similarity \cite{zhang2018perceptual} (LPIPS) on the color images and Root Mean-Square-Error (RMSE) on the depth images.

\textbf{Results.}
We evaluate the rendered images from each method using the original images of the ScanNet dataset as groundtruth. 
The results are presented in Table~\ref{table:scannet_render}.
While we acknowledge that the map representation used in HOV-SG does not prioritize rendering of the scene, we present these results to highlight the value of storing the map as Gaussian parameters.
The qualitative evaluation of the difference between a rendered image and the ground truth image for the task is shown in Fig.~\ref{fig:rendering-exp}.
%

\begin{table}[!bt]
\centering
\begin{tabular}{c|c|c|c|c}
\hline
\multirow{2}{*}{\textbf{Method}} & \textbf{PSNR} $\uparrow$ & \textbf{SSIM} $\uparrow$ & \textbf{LPIPS} $\downarrow$ & \textbf{RMSE} $\downarrow$ \\
& (dB) & & & (m) \\ \hline 
HOV-SG~\cite{werby2024hierarchical} & 6.86 & 0.22 & 0.90 & 1.95 \\
ATLAS (ours) & \textbf{20.44} & \textbf{0.80} & \textbf{0.26} & \textbf{0.05} \\
\hline
\end{tabular}
\bigskip
\caption{Image rendering quality on ScanNet.}
\label{table:scannet_render}
\end{table}

\begin{figure}[!tbh]
    \centering
    \includegraphics[width=0.95\linewidth]{diagrams/rendering-exp.pdf}
    \caption{[A] shows the task provided to our method. [B] shows the selected submap and region in the bottom-left with highest relevance. [C] shows the rendered image from the vantage point with the highest relevance to the task. [D] shows the ground truth image.}
    \label{fig:rendering-exp}
\end{figure}


\subsection{Robot experiments}
\label{sec:robot-exp}

To demonstrate the flexibility and efficiency of our method, we conduct several real-world experiments across both indoor and outdoor environments.
We measure the ability of our framework to load, localize and then navigate to the target.
Then, we demonstrate that our method can be applied to completely unknown maps and follow the utility signal to complete the task.
The task completion oracle $\psi$ uses a VLM with a prompt requesting yes or no to terminate the task.
Example outputs from our experiments are shown in Fig.~\ref{fig:main}.

\textbf{Metrics.}
We compute the path length \textit{PL} of the trajectory taken by the robot in each experiment.
We compare this against two other path lengths, \textit{SP} and \textit{GT.}
After the experiment is complete and the map is available, we compute the shortest path \textit{SP} on the full planning graph.
This ablates away any odometry error since our traveled path and the planning graph are built on the same set of odometry measurements.
To obtain the length of the optimal path \textit{GT} from the starting position to the inferred goal in the outdoor experiments, we measure the path length from GPS coordinates annotated by a human on Google Earth.
For indoor experiments, we measure the shortest path from the start to goal with a rangefinder.
We measure the competitive ratio (listed \textit{ratio} in Tab.~\ref{tab:real_expts}) of the distance measured by the visual odometry on our robot and these privileged distance measurements, given by $\frac{SP}{PL}$ and $\frac{GT}{PL}$ respectively.
To show the scale of our experiments, we measure and report the approximate area of the operational area of the environment for each experiment from Google Earth, and the number of Gaussians in the map.

\begin{table*}[!th]
\begin{center}
\centering
\begin{tabular}{
P{0.08\textwidth}|
P{0.18\textwidth}|
P{0.05\textwidth}|
P{0.06\textwidth}|
P{0.05\textwidth}|
P{0.05\textwidth}|
P{0.05\textwidth}|
P{0.05\textwidth}|
P{0.06\textwidth}|
P{0.07\textwidth}}
\hline
    \multirow{2}{*}{Experiment} & \multirow{2}{*}{Task} & Prior & \multicolumn{5}{c|}{Distance (m)} & {Area} & {Num. of} \\ \cline{4-8}
     &  & Map & {PL (m)} & \multicolumn{2}{c|}{SP (m / ratio)} & \multicolumn{2}{c|}{GT (m / ratio)} & {(m$^2$)} & {Gaussians} \\ \hline 
    Outdoor1 & Navigate to entrance to pier & Yes & 185.78 & -- & -- & 184.86 & 0.99 & 3346.5 & 2661562 \\
    Indoor1 & Find cushions & No & 72.53 & 34.87 & 0.48 & 29.72 & 0.41 & 973.28  & 633089\\ 
    Indoor2 & Find plants, Exit building & No & 44.55 & 31.60 & 0.71 & 30.16 & 0.68 & 973.28 & 446361  \\
    Outdoor2 & Inspect road blockage & No & 94.25 & 45.16 & 0.48 & 43.52 & 0.46 & 1063.57 & 1087489  \\
    Outdoor3 & Find parking lot & No & 69.19 & 46.439 & 0.67 & 43.22 & 0.62 & 1280.10 & 1473420 \\
    Outdoor4 & Find river near road & No & 742.42 & 473.64 & 0.64 & 472.55 & 0.64 & 17791.14 & 7769214 \\
    Outdoor5 & Find boardwalk near road & No & 1257.53 & 688.90 &  0.55 & 671.76 &  0.53 & 21870.34 & 11279776 \\
    \hline
\end{tabular}
\end{center}
\caption{Overview of robot experiments. \textit{Prior} indicates that a prior \textit{PL} refers to path length, \textit{SP} refers to the shortest path computed on the full planning graph after the experiment is complete, and \textit{GT} refers to the ground truth.}

\label{tab:real_expts}
\bigskip
\end{table*}

\textbf{Experiment Areas.}
We conduct our experiments in an urban office complex (shown in Fig.~\ref{fig:experiment-areas}) with office space, parking lots, and an outdoor park.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{diagrams/ExperimentalAreas.png}
    \caption{The outdoor experiment areas for our experiments.
    Our park experiments are in the highlighted yellow areas.
    The parking lots are in red and blue.}
    \label{fig:experiment-areas}
\end{figure}

\subsubsection{Navigation in pre-built map}
To show how our sparse hierarchical map can be used for navigation, we conduct an experiment \textit{Outdoor1} where the robot uses a pre-built map to identify and plan a path to a region of interest.
An example is shown in Fig.~\ref{fig:outdoor-exp-scale-gt}.
In this experiment, we first tele-operate the robot to a dock several hundred meters from the starting position and save the map. 
On a separate run, the map is loaded on the robot. 
The robot is given the task `navigate to entrance to pier'. 
The task relevancy is computed across the submaps and their regions, and the submap with the highest utility is identified.
The robot plans a path to the submap of interest and navigates to the goal.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\linewidth,trim={2.5cm 5cm 2.5cm 6cm},clip]{diagrams/river-premapped.pdf}
    \caption{The user-specified task and the description received on termination of the task. The VLM is queried when the relevancy of the language features obtained from the image exceeds a threshold. 
    }
    \label{fig:prebuilt-vlm}
\end{figure}

\textbf{Results.}
Our results for this experiment are reported in Table~\ref{tab:real_expts}.
As expected, our method performs close to the ground truth since it also has access to the true map of the environment.
The qualitative results of the VLM query for the termination of the task is shown in Fig.~\ref{fig:prebuilt-vlm}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.99\linewidth,trim={2cm 0 2cm 1.3cm}, clip]{diagrams/indoor-exploration.pdf}
    \caption{Robot executing the task `find cushions' starting at [A]. The robot incrementally constructs a map with language-embedded Gaussian splatting and identifies and navigates to regions in the map with high relevance.
    The VLM is queried at vantage points [B], [C], [D]. 
    The images acquired at [B] and [C] have the second-order association of couches but no cushions. 
    The task only terminates at [D] when the cushion is found.}
    \label{fig:cushions_viz}
\end{figure}

\subsubsection{Navigation with no prior map}

We conduct six real-world experiments -- two indoor and four outdoor.
In these experiments, the robot starts off with no prior map or information of the environment.
Given a user-specified task, the robot proceeds to incrementally build a metric-semantic map of the environment and uses relevant information in the map to complete the task.

\textbf{Indoor.}
For \textit{Indoor1}, we perform a simple object search of `find plants' and then demonstrate re-tasking the robot to `exit the building'.
In the second indoor experiment \textit{Indoor2}, we show that our approach is able to leverage the semantic relationships between objects in the environment to complete tasks.
With the task `find cushions', the robot is able to identify chairs and couches as areas of high relevance to the task and proceeds to inspect them, eventually successfully locating the cushions. Visualizations of the experiment are provided in Fig.~\ref{fig:cushions_viz}.
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figs/fardock_prior.pdf}
    \caption{ [A] The map built from the task "Find boardwalk near road". The colored Gaussian points and the submap nodes (red and green circles) are visualized. The prior hierarchical graph can then be used to retrieve and navigate in the map. The shortest path (SP) to the boardwalk is shown with the green nodes. [B] The task relevancy of the Gaussians is colored blue (low) to red (high). [C] The ground truth trajectory (GT) is overlaid in yellow on an image from Google Earth to highlight the scale of the experiments.}
    \label{fig:outdoor-exp-scale-gt}
\end{figure}


\textbf{Outdoor.}
For our outdoor experiments, we consider the following:
\begin{enumerate}
    \item In \textit{Outdoor2}, we use an under-specified task `inspect road blockage' and the robot leverages semantics of the pavement to search for and identify obstructions.
    \item In \textit{Outdoor3}, the robot is tasked with `find parking lot' while starting close to the entrance of a building.
    \item In \textit{Outdoor4} and \textit{Outdoor5}, we conduct large-scale experiments in the park. A visualization of task relevancy and the ground truth trajectory is shown in Fig.~\ref{fig:outdoor-exp-scale-gt}.
\end{enumerate}

\textbf{Results.}
We present an overview of the robot experiments in Table~\ref{tab:real_expts}.
The competitive ratio of our method is $\sim$ 0.59 on \textit{SP} and $\sim$ 0.56 on \textit{GT}.
This shows that our tasks require some exploration but in general, our method performs at least half as well as a privileged baseline.
In our experiments, our method is also able to store over an order of a million Gaussians on-board the robot.
