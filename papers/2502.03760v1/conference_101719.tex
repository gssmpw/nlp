\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{fourier} 
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{adjustbox}

\usepackage{booktabs}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}


\title{RAMOTS: A Real-Time System for Aerial Multi-Object Tracking based on Deep Learning and Big Data Technology}
\author{
\IEEEauthorblockN{Nhat-Tan Do\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}, Nhi Ngoc-Yen Nguyen\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}, Dieu-Phuong Nguyen\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}, and Trong-Hop Do\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}}

\IEEEauthorblockA{\IEEEauthorrefmark{1}Faculty of Information Science and Engineering, University of Information Technology\\
\IEEEauthorrefmark{2}Vietnam National University, Ho Chi Minh City, Vietnam \\
Corresponding author(s). Email(s): \texttt{hopdt@uit.edu.vn} \\
Contributing authors: \texttt{\{21522575, 21521231, 21520091\}@gm.uit.edu.vn}
}
\thanks{These authors contributed equally to this work.} % Footnote indicating equal contribution
}


\maketitle

\begin{abstract}
% In this paper, we propose a novel real-time Multi-object Tracking approach on UAV Footage by integrating Apache Kafka, Apache Spark, and state-of-the-art Deep Learning models such as YOLOv8/YOLOv10 and BYTETRACK/BoTSORT. The Integration of Apache Kafka and Spark enables efficient ingestion, distribution, and parallel processing of high-volume video streams, ensuring reliable and fault-tolerant data delivery. With the applications of state-of-the-art tracking methods namely BYTETRACK and BoTSORT, we achieve 48.14 HOTA and 43.51 MOTA on the test set of Visdrone2019-MOT with 28 FPS on a single GPU.

Multi-object tracking (MOT) in UAV-based video is challenging due to variations in viewpoint, low resolution, and the presence of small objects. While other research on MOT dedicated to aerial videos primarily focuses on the academic aspect by developing sophisticated algorithms, there is a lack of attention to the practical aspect of these systems. In this paper, we propose a novel real-time MOT framework that integrates Apache Kafka and Apache Spark for efficient and fault-tolerant video stream processing, along with state-of-the-art deep learning models YOLOv8/YOLOv10 and BYTETRACK/BoTSORT for accurate object detection and tracking. Our work highlights the importance of not only the advanced algorithms but also the integration of these methods with scalable and distributed systems. By leveraging these technologies, our system achieves a HOTA of 48.14 and a MOTA of 43.51 on the Visdrone2019-MOT test set while maintaining a real-time processing speed of 28 FPS on a single GPU. Our work demonstrates the potential of big data technologies and deep learning for addressing the challenges of MOT in UAV applications.
% This document is a model and instructions for \LaTeX.
% This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
% or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
Multi-Object Tracking, Unmanned aerial vehicle, Big data processing, Streaming processing, Real-time
% component, formatting, style, styling, insert
\end{IEEEkeywords}


\section{Introduction}

The number of UAVs (Unmanned aerial vehicles) has explosively grown in the last decade, with diverse applications in fields such as agriculture, communications, delivering, navigation and surveillance \cite{yuanUltrareliableIoTCommunications2018, radoglou-grammatikisCompilationUAVApplications2020, yooDroneDeliveryFactors2018, macrinaDroneaidedRoutingLiterature2020}. With their capability to fly and stay airborne without human-in-the-loop operation, they are more cost-efficient and more reliable by removing potential human errors.

Multi-Object Tracking (MOT) is the task of detecting multiple objects in video and associating these detections over time with correct identities. As a foundational task in computer vision, The MOT challenge receives extensive research interests, with a significant number of researches and works dedicated to improving its performance. While MOT is generally a large and attractive problem in the research community, MOT research dedicated to UAV videos is relatively limited. Unlike fixed surveillance cameras, MOT in UAV videos presents unique challenges due to the dynamic nature of the scenes: constantly changing in altitude and perspective, small object sizes, and high-speed movements. The limited computational resources on small UAV platforms pose an additional challenge when applying complex deep learning models for real-time object tracking, making most promising MOT strategies proposed in general not applicable to UAV-specific MOT.

While other research on Multi-Object Tracking (MOT) dedicated to UAV videos primarily focuses on the academic aspect by developing sophisticated algorithms, there is a lack of attention to the practical aspect of these systems. In practice, UAVs typically collect data and stream it to servers, where most processing occurs. In our work, we address not only the algorithmic aspects but also emphasize the practicality of the system. The state-of-the-art deep learning algorithms alone can't ensure the system will run smoothly and practically, we ensure that the system can handle large volumes of data and deliver results in real-time efficiently by integrating the whole system with big data platforms, namely Apache Spark and Apache Kafka. Our work highlights the importance of advanced algorithms and integrating these methods with scalable and distributed systems. By leveraging these technologies, our system achieves a HOTA of 48.14 and a MOTA of 43.51 on the Visdrone2019-MOT test set while maintaining a real-time processing speed of 28 FPS on a single GPU. Our work demonstrates the potential of big data technologies and deep learning for addressing the challenges of MOT in UAV applications. 

In this work, we propose a novel real-time Multi-object Tracking approach on UAV Footage by integrating Apache Kafka, Apache Spark, and state-of-the-art Deep Learning models such as YOLOv8/YOLOv10 and BYTETRACK/BoTSORT. The critical contributions of our work can be summarized as follows:

\begin{itemize}
    \item  \textbf{Novel Architecture Integration}. We introduce an architecture that seamlessly integrates Apache Kafka, a distributed streaming platform, and Apache Spark, a powerful cluster computing framework. This innovative combination enables efficient ingestion, distribution, and parallel processing of high-volume video streams from multiple UAVs, ensuring reliable and fault-tolerant data delivery.

    \item \textbf{State-of-the-Art Multi-Object Tracking Algorithm}. At the core of our framework is a robust multi-object tracking algorithm that integrates advanced computer vision techniques with efficient data processing pipelines. This algorithm utilizes deep learning models trained on diverse datasets, enabling precise detection and tracking of various object types—including vehicles, pedestrians, and aerial targets—under challenging conditions such as occlusions, rapid movements, and appearance variations.

    \item \textbf{Scalable and Distributed Processing}. Our framework capitalizes on the distributed nature of Apache Kafka and Apache Spark, facilitating horizontal scaling to accommodate an increasing number of UAVs and video streams. Kafka's partitioning and replication mechanisms, combined with Spark's cluster computing capabilities, enable parallel processing of video frames across multiple compute nodes, thereby accelerating object detection and tracking tasks.

    \item \textbf{Real-Time Performance}. By harnessing big data technologies and optimized algorithms, our framework achieves real-time performance, delivering timely and accurate situational awareness for critical applications such as security, traffic monitoring, search and rescue operations, and environmental monitoring.

\end{itemize}


\section{Related Works} \label{related-works}

\textbf{Object Detection.} Object detection, a fundamental task in computer vision, plays a crucial role in any MOT system. Object detection has been made a significant process during the past decade with the advent of many deep learning approaches. 

Two-stage detectors, such as Fast R-CNN\cite{girshickFastRcnn2015} and Faster R-CNN\cite{shouxinrenFasterRCNNRealtime2015}, utilize region proposal networks to identify potential objects, followed by a classification and bounding box regression step. This method offers high accuracy but can be computationally expensive. One-stage detectors, like YOLO \cite{redmonYouOnlyLook2016, wangYolov10RealtimeEndtoend2024, jocherYOLOUltralytics2023} or SSD \cite{liuSsdSingleShot2016}, directly predict object classes and bounding boxes in a single pass. They prioritize speed and are suitable for real-time applications but may sacrifice some accuracy compared to two-stage detectors. Transformer-based architectures, like DETR, have also emerged for object detection, offering improved accuracy and the ability to handle complex scenes effectively. These advancements in object detection contribute significantly to enhancing the accuracy and robustness of MOT systems by providing reliable object detection as input.

\textbf{Data Association.} Many algorithms and strategies were developed to help MOT system match between tracklets and detection boxes. SORT \cite{bewleySimpleOnlineRealtime2016}, a Simple online and real-time tracking algorithm that utilizes location and motion cues in a very simple yet effective way. Many significant methods adopt SORT, with some modifications to achieve favorable performance \cite{zhangByteTrackMultiObjectTracking2022, zhangFairMOT2021, mingyangHybridSORTWeakCues2023, caoObservationcentricSortRethinking2023, wangJDE2020, yiUCMCTrackMultiObjectTracking2024, wojkeSimpleOnlineRealtime2017}. Using the location and motion cues only is not robust to scenes with low certainty, large camera motion, or low frame rate. In these cases, appearance cues appear to work more efficiently. In scenarios when an object is occluded for a brief moment or even for a long period of time, it can be recognized using appearance information. DeepSORT \cite{wojkeSimpleOnlineRealtime2017} replaced the simple association metric in SORT with a more informed metric that combines motion and appearance cues. MOTDT \cite{chenRealtimeMultiplePeople2018} matches tracklets with detection by using appearance information first, then using IoU to match the remaining tracklets. FairMOT \cite{zhangFairMOT2021} addresses the issue of imbalanced optimization in multi-object tracking. It uses a single network to estimate object detections and perform re-identification (ReID) simultaneously. By jointly learning these tasks, FairMOT improves feature representation for both detection and ReID, leading to more robust and accurate tracking, especially in crowded scenes. ByteTrack \cite{zhangByteTrackMultiObjectTracking2022} introduces a new data association method, BYTE, which utilizes every detection box, including those with low confidence scores. This strategy involves associating high-scoring detections with existing tracklets and then matching remaining low-scoring detections with unmatched tracklets. This approach is reportedly efficient in working with occlusion, motion blur, or size changes. BoTSORT \cite{aharonBoTSORTRobustAssociations2022} takes a step further by adding improvements on top of ByteTrack, namely camera motion compensation-based (CMC) features tracker, with a new simple yet effective method for IoU and ReID’s cosine-distance fusion. Additionally, although inspired by ByteTrack, BoTSORT incorporates a more sophisticated data association strategy to handle challenging cases like occlusions and missed detections. 

\textbf{MOT on UAV videos.} 
Multiple Object Tracking (MOT) in UAV videos presents unique challenges due to the small size, fast motion, and varying perspectives of targets. Early approaches, often adapted from ground-based MOT methods, struggled with the non-linear motion models and abrupt appearance changes common in UAV footage. The advent of deep learning has led to significant advancements, with two-stage detectors like YOLO \cite{redmonYouOnlyLook2016} and Faster R-CNN \cite{shouxinrenFasterRCNNRealtime2015} combined with ReID techniques \cite{3} showing promise. More recently, one-shot MOT methods that integrate detection and ReID into a single network have gained popularity due to their real-time capabilities \cite{liuUAVMOT2022}. However, challenges such as long-term occlusions, varying object densities, and adverse weather conditions remain. Future research is likely to explore online learning, motion prediction, and the integration of contextual information to further improve tracking performance in UAV videos.


\section{Proposed real-time multi-object tracking architecture}

\subsection{Proposed real-time UAV video multi-object tracking System}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{assets/fig_system.png}
    \caption{Our proposed Real-time UAV Videos Multi-object Tracking System}
    \label{fig:system}
\end{figure*}


Our system starts by getting video footage from UAVs then sent to the server via a Kafka Producer. The server's Consumer receives and processes the footage through the MOT framework in Apache Spark. The results are then sent back to the client via Kafka. The system's overall architecture is illustrated in Fig. \ref{fig:system}. 

\subsection{Integration of Apache Kafka and Apache Spark}

Kafka's architecture is tailored for \textit{high-throughput, fault-tolerant, and real-time data streaming}. This is critical for promptly processing and analyzing streaming video data to facilitate MOT in real time. 

Apache Kafka is used for real-time data ingestion. It serves as a message broker between the cameras and our processing server. The footage from multiple cameras is recorded and transmitted to the server via a Kafka Producer. The server's Consumer receives the footage and sends each frame to the Person Detection model sequentially. On the other hand, Apache Spark is used for real-time data processing: It receives the frames from Kafka, runs them through the MOT framework, and sends the results back to Kafka. The results are then sent back to the client.

In our system, we bring together Apache Kafka and Apache Spark to handle real-time data streaming and processing. This integration of Apache Kafka and Apache Spark allows our system to process and analyze video footage in real-time and can be easily scaled to handle large amounts of data.

% \subsection{Receive and process traffic streaming data}

\subsection{Multi-Object Tracking Algorithm}

The multi-object tracking algorithm can be divided into two main stages: Object Detection and Tracking. This section will explore how various state-of-the-art models are employed in each stage to achieve reliable and accurate multi-object tracking. By integrating these advanced object detection models and tracking algorithms, the multi-object tracking algorithm ensures high accuracy and robustness in diverse and challenging environments, maintaining consistent object identities across video frames. 

\subsubsection{Object Detection}

In the object detection stage, models such as YOLOv8, YOLOv10, RT-DETR, and Faster R-CNN are utilized. YOLOv8, a refined and optimized version of the YOLO family, is known for its balance between speed and accuracy, making it suitable for real-time applications where maintaining high frame rates is crucial. YOLOv10 builds on its predecessors with enhanced feature pyramids and attention mechanisms, providing precise object localization and classification in challenging environments. RT-DETR, or Real-Time Detection Transformer, leverages transformer architectures to capture long-range dependencies and the efficiency of convolutional networks for feature extraction, excelling in detecting small or overlapping objects in complex scenes. Additionally, Faster R-CNN, a widely used model in object detection, combines region proposal networks (RPN) with Fast R-CNN, allowing it to detect objects with high accuracy and robustness. Its two-stage approach, where the first stage generates region proposals and the second stage refines these proposals and performs classification, ensures precise detection even in complicated scenarios.

\subsubsection{Multi-Object Tracking}

The tracking stage involves associating detected objects across consecutive frames to maintain their identities over time, utilizing advanced algorithms such as ByteTrack, BoT-SORT, and SMILETrack-R. ByteTrack enhances robustness by integrating both high-confidence and low-confidence detections into a unified framework, employing a byte-level matching approach to effectively manage partial occlusions and varying object appearances. BoT-SORT builds on the classic SORT algorithm with improvements like adaptive Kalman filtering, IoU-based matching, and cascade matching strategies, ensuring more reliable object associations. SMILETrack-R introduces a novel method that incorporates re-identification (ReID) features alongside self-motivated interpolation and learning-based techniques, adeptly handling challenging scenarios such as abrupt motion changes and occlusions through dynamic trajectory updates. These methods not only reportedly result in SOTA performance in terms of both accuracy and performance, but they are also simple by nature and not specifically built for any particular dataset. This reason makes BYTETRACK and BoTSORT suitable for our system, which utilizes aerial footage from UAVs.


\section{Experiments} \label{experiments}


% \subsection{Datasets \& Metrics} \label{settings}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/fig_dataset_samples.png}
    \caption{samples of UAVDT and VisDrone dataset. The videos from the two datasets cover various aspects, including location, environment, lighting, objects, and density.}
    \label{fig:dataset_samples}
\end{figure*}

\textbf{Datasets.} We conducted a fair evaluation on 2 largest publicly available datasets on MOT on UAV videos, namely VisDrone2019-MOT \cite{zhuVisDrone2021} and UAVDT \cite{duUAVDT2018}. VisDrone2019 is a large-scale benchmark for various important computer vision tasks, including Object Detection (OD), Single Object Tracking (SOT), Multi-Object Tracking (MOT), and Crowd Counting. The benchmark dataset consists of 288 video clips formed by 261,908 frames and 10,209 static images captured by various drone-mounted cameras, covering a wide range of aspects including location (taken from 14 different cities separated by thousands of kilometers in China), environment (urban and country), objects (pedestrian, vehicles, bicycles, etc.), and density (sparse and crowded scenes) \cite{zhuVisDrone2021}. UAVDT is also a multi-task large-scale UAV benchmark (i.e., about 80,000 representative frames from 10 hours of raw videos) for OD, SOT, and MOT. UAVDT consists of 100 video sequences, which are selected from over 10 hours of videos taken with a UAV platform at a number of locations in urban areas, representing various common scenes, including squares, arterial streets, toll stations, highways, crossings, and T-junctions. The videos are recorded at 30 frames per second (fps), with a JPEG image resolution of 1080 × 540 pixels \cite{duUAVDT2018}. VisDrone2019 benchmark covers a broader range of object classes than UAVDT. While UAVDT focuses solely on vehicles, VisDrone2019 includes a wider and more detailed set of classes, such as cars, motorcycles, and pedestrians. Sample of UAVDT and VisDrone dataset are shown in Figure \ref{fig:dataset_samples}.

\textbf{Metrics.} For the evaluation of performance, we use MOTA (Multiple Object Tracking Accuracy), IDF1 (Identification F1 Score), and IDs (Identity Switches) from the CLEAR metrics \cite{bernardinEvaluatingMultipleObject2008} along with a newly proposed metric, HOTA \cite{luitenHotaHigherOrder2021}. MOTA is computed based on false positives, false negatives, and identity switches, indicating how well the tracker maintains accurate object identities over time. IDF1 focuses on the accuracy of object association, which reflects the balance between correctly linking detections to tracked objects and creating new tracks for untracked objects. IDs indicates the number of times the tracker incorrectly "switches" the identity of an object being tracked to another identity. CLEAR metrics, although widely used in the research community, are still limited by their imbalance nature: while MOTA focuses more on the detection performance, IDF1 focuses more on the association performance. HOTA is a comprehensive metric that aims to capture the overall accuracy of an MOT system by considering three key aspects, including (1) detection accuracy, (2) association accuracy, and (3) localization accuracy. Fast inference is also a crucial part of real-time MOT system. We evaluate the speed of our methods by calculating FPS (Frame per second), which indicates how fast our system can run in one second. 

\textbf{Implementation Details.} For a fair comparison, all detection methods were trained on the VisDrone2019-DET train set using YOLOv8, YOLOv10, and RT-DETR architectures. The models were pre-trained on the COCO dataset and fine-tuned for 4 epochs with a batch size of 16 and an initial learning rate of 0.01. Data augmentation techniques, such as random horizontal flips and scaling, were applied during training. We utilized a single NVIDIA Tesla A100 GPU for training and inference.

In the inference phase, for both BoTSORT and BYTETRACK, we use the detection score threshold $\tau = 0.6$. If the IoU between the detection box and the tracklet box is smaller than 0.2 in the linear assignment step, we will reject the matching. Lost tracklets will be kept for 30 frames (\textit{track buffer}), in case they reappear. All inferences are run on the same machine with 16GB of RAM, $\sim$3.2GHz CPU, and an NVIDIA RTX 3060 GPU.

% The MOT algorithms, BYTETRACK and BoTSORT, were configured with default parameters. We utilized a single NVIDIA Tesla V100 GPU for training and inference, and all experiments were conducted on a server with an Intel Xeon Gold 6248 CPU and 128GB RAM. Performance was evaluated using the official VisDrone2019-MOT and UAVDT evaluation scripts, reporting metrics such as HOTA, MOTA, IDF1, IDs, and FPS.


% \subsection{Implementation Details}
\subsection{Results and Discussion}

% \begin{table*}[ht]
% \setlength{\tabcolsep}{4pt} % Adjust column spacing
% \renewcommand{\theadfont}{\small\bfseries} % Adjust header font size 
% \caption{Comparison of different methods under the VisDrone2019-MOT and UAVDT dataset. The best results are shown in \textbf{bold}}\label{tab:exp-results}
% \begin{adjustbox}{width=\textwidth}
% \begin{tabular*}{\textheight}{@{\extracolsep\fill}lrrrrrrrrr}
% \toprule%
%                                               & \multicolumn{4}{@{}c@{}}{VisDrone2019-MOT}                         & \multicolumn{4}{@{}c@{}}{UAVDT}& \multicolumn{1}{@{}c@{}}{} \\\cmidrule{2-5}\cmidrule{6-9}%
% Methods                                       & HOTA$\uparrow$ & MOTA$\uparrow$ & IDF1$\uparrow$ & IDs$\downarrow$ & HOTA$\uparrow$& MOTA$\uparrow$& IDF1$\uparrow$& IDs$\downarrow$ & FPS$\uparrow$ \\
% \midrule
% MOTDT \cite{chenRealtimeMultiplePeople2018a}  & -              & -0.80          & 21.60          & 1437            & -             & -             & -             & -              & -          \\
% SORT \cite{bewleySimpleOnlineRealtime2016}    & -              & 14.00          & 38.00          & 3629            & -             & 39.00         & 43.70         & 2350           & -          \\
% JDE \cite{wangJDE2020}                        & -              & 26.60          & 34.90          & 3200            & -             & 39.50         & 55.30         & 3124           & 17.8       \\
% FairMOT \cite{zhangFairMOT2021}               & -              & 30.80          & 41.90          & 3007            & -             & 44.90         & 60.90         & 2279           & 18         \\
% MOTR \cite{zengMotrEndtoendMultipleobject2022}& -              & 22.80          & 41.40          & \textbf{959}    & -             & -             & -             & -              & -          \\
% TrackFormer \cite{meinhardtTrackformer2022}   & -              & 25.00          & 30.50          & 4840            & -             & -             & -             & -              & -          \\
% FPUAV \cite{wuFPUAV2022}                      & -              & 34.30          & 45.00          & 2138            & -             & -             & -             & -              & 17.6       \\
% UAVMOT  \cite{liuUAVMOT2022}                  & -              & 36.10          & 51.00          & 2775            & -             & 48.6          & 66.2          & 1999           & -          \\
% YOLOv8l + ByteTrack                           & 43.59          & 41.77          & 54.78          & 4212            & 53.46         & 34.62         & 70.32         & 786            & 28         \\
% YOLOv8l + BoTSORT                             & \textbf{48.14} & \textbf{43.51} & \textbf{61.46} & 3118            & 53.93         & 33.86         & 71.11         & 725            & 28         \\
% YOLOv8l + SMILETrack-R                        & 40.67          & 12.90          & 48.39          & 22412           & -             & -             & -             & -              & 5           \\
% YOLOv8x + BoTSORT                             & 43.90          & 42.29          & 55.14          & 4432            & 53.42         & 30.69         & 70.21         & 756            & 28         \\
% YOLOv10l + ByteTrack                          & 35.43          & 26.66          & 38.85          & 1788            & 56.48         & \textbf{53.87}& \textbf{74.60}& \textbf{576}   & 24         \\
% YOLOv10l + BoTSORT                            & 44.33          & 39.40          & 55.68          & 2044            & \textbf{57.21}& 53.64         & 75.53         & 600            & 24         \\
% RT-DETR-l + BoTSORT                           & 41.77          & 35.65          & 50.32          & 3607            & 50.50         & 8.51          & 63.86         & 795            & 21         \\

% \bottomrule
% \end{tabular*}
% \end{adjustbox}
% \end{table*}

\begin{table*}[ht]
\setlength{\tabcolsep}{4pt} % Adjust column spacing
% \renewcommand{\theadfont}{\small\bfseries} % Adjust header font size 
\caption{Comparative analysis of Classical MOT methods (without Detector-Tracker Combination). Results Aggregated from Literature on VisDrone2019-MOT and UAVDT dataset}\label{tab:exp-results}
\begin{adjustbox}{width=\textwidth}
\begin{tabular*}{\textheight}{@{\extracolsep\fill}lrrrrrrrrr}
\toprule%
                                              & \multicolumn{4}{@{}c@{}}{VisDrone2019-MOT}                         & \multicolumn{4}{@{}c@{}}{UAVDT}& \multicolumn{1}{@{}c@{}}{} \\\cmidrule{2-5}\cmidrule{6-9}%
Methods                                       & HOTA$\uparrow$ & MOTA$\uparrow$ & IDF1$\uparrow$ & IDs$\downarrow$ & HOTA$\uparrow$& MOTA$\uparrow$& IDF1$\uparrow$& IDs$\downarrow$ & FPS$\uparrow$ \\
\midrule
MOTDT \cite{chenRealtimeMultiplePeople2018}  & -              & -0.80          & 21.60          & 1437            & -             & -             & -             & -              & -          \\
SORT \cite{bewleySimpleOnlineRealtime2016}    & -              & 14.00          & 38.00          & 3629            & -             & 39.00         & 43.70         & 2350           & -          \\
JDE \cite{wangJDE2020}                        & -              & 26.60          & 34.90          & 3200            & -             & 39.50         & 55.30         & 3124           & \textbf{17.8}       \\
FairMOT \cite{zhangFairMOT2021}               & -              & 30.80          & 41.90          & 3007            & -             & 44.90         & 60.90         & 2279           & 18         \\
MOTR \cite{zengMotrEndtoendMultipleobject2022}& -              & 22.80          & 41.40          & \textbf{959}   & -             & -             & -             & -              & -          \\
TrackFormer \cite{meinhardtTrackformer2022}   & -              & 25.00          & 30.50          & 4840            & -             & -             & -             & -              & -          \\
FPUAV \cite{wuFPUAV2022}                      & -              & 34.30          & 45.00          & 2138            & -             & -             & -             & -              & 17.6       \\
UAVMOT  \cite{liuUAVMOT2022}                  & -              & \textbf{36.10}          & \textbf{51.00}          & 2775            & -             & \textbf{48.6}          & \textbf{66.2}          & \textbf{1999}           & -          \\
\bottomrule
\end{tabular*}
\end{adjustbox}
\end{table*}

\begin{table*}[ht]
\setlength{\tabcolsep}{4pt} % Adjust column spacing
% \renewcommand{\theadfont}{\small\bfseries} % Adjust header font size 
\caption{Comparison of different methods with combined Detector and Tracker under the VisDrone2019-MOT and UAVDT dataset. The best results are shown in \textbf{bold}}\label{tab:exp-results_2}
\begin{adjustbox}{width=\textwidth}
\begin{tabular*}{\textheight}{@{\extracolsep\fill}lrrrrrrrrr}
\toprule%
                                              & \multicolumn{4}{@{}c@{}}{VisDrone2019-MOT}                         & \multicolumn{4}{@{}c@{}}{UAVDT}& \multicolumn{1}{@{}c@{}}{} \\\cmidrule{2-5}\cmidrule{6-9}%
Methods                                       & HOTA$\uparrow$ & MOTA$\uparrow$ & IDF1$\uparrow$ & IDs$\downarrow$ & HOTA$\uparrow$& MOTA$\uparrow$& IDF1$\uparrow$& IDs$\downarrow$ & FPS$\uparrow$ \\
\midrule

YOLOv8l, ByteTrack                           & 43.59          & 41.77          & 54.78          & 4212            & 53.46         & 34.62         & 70.32         & 786            & 28         \\
YOLOv8l, BoTSORT                             & \textbf{48.14} & \textbf{43.51} & \textbf{61.46} & 3118            & 53.93         & 33.86         & 71.11         & 725            & 28         \\
YOLOv8l, SMILETrack-R                        & 40.67          & 12.90          & 48.39          & 22412           & -             & -             & -             & -              & 5           \\
YOLOv8x, BoTSORT                             & 43.90          & 42.29          & 55.14          & 4432            & 53.42         & 30.69         & 70.21         & 756            & 28         \\
YOLOv10l, ByteTrack                          & 35.43          & 26.66          & 38.85          & \textbf{1788}            & 56.48         & \textbf{53.87}& 74.60& \textbf{576}   & 24         \\
YOLOv10l, BoTSORT                            & 44.33          & 39.40          & 55.68          & 2044            & \textbf{57.21}& 53.64         & \textbf{75.53}         & 600            & 24         \\
RT-DETR-l, ByteTrack                         & 41.14          & 36.34          & 50.32          & 3600            & 50.25         & 8.75          & 63.54         & 788            & 21         \\

RT-DETR-l, BoTSORT                           & 41.77          & 35.65          & 50.32          & 3607            & 50.50         & 8.51          & 63.86         & 795            & 21         \\

\bottomrule
\end{tabular*}
\end{adjustbox}
\end{table*}


\begin{figure}
    \centering
    \includegraphics[width=8cm]{assets/fig_results.png}
    \caption{Comparative performance of Deep Learning-based MOT methods on VisDrone2019-MOT and UAVDT dataset. The horizontal axis, vertical axis, and radius of the circle are IDF1, MOTA, and FPS, respectively. Detail comparison are shown in \ref{tab:exp-results} and \ref{tab:exp-results_2}}
    \label{fig:results}
\end{figure}

The results of our experiments are shown in Table \ref{tab:exp-results}, \ref{tab:exp-results_2} and are visualized in Figure \ref{fig:results}. $\uparrow$/$\downarrow$ indicate that higher/lower is better, respectively. The best results are shown in \textbf{bold}. The YOLOv8l and YOLOv10l architectures, particularly when paired with the BoTSORT tracker, consistently demonstrated superior performance across various accuracy metrics, with $48.14\%$ HOTA on VisDrone2019-MOT with YOLOv8l+BoTSORT and $57.21\%$ on UAVDT with YOLOv10l+BoTSORT, outperformed other combinations by a large margin. This suggests their effectiveness in accurately detecting and maintaining consistent tracks of objects in aerial videos, even in challenging scenarios.

While both BoTSORT and ByteTrack proved to be capable trackers, BoTSORT often edged out ByteTrack in terms of accuracy about $2\%$ to $5\%$ on the same setting, highlighting its potential for more robust and reliable tracking over time.  The newer DETR-based methods, specifically RT-DETR-l, showed promise, especially on the UAVDT dataset. However, their performance on the more complex VisDrone2019-MOT dataset was less consistent, indicating a need for further optimization to enhance their generalization capabilities.

% The evaluation also underscored the trade-off between accuracy and speed in MOT systems. The fastest method, YOLOv8l combined with ByteTrack, prioritized real-time performance at the cost of some accuracy. Conversely, the most accurate methods, typically involving YOLOv8l or YOLOv10l with BoTSORT, operated at a lower frames-per-second (FPS) rate, emphasizing the challenge of balancing real-time processing with high-precision tracking.

It is noticeable that YOLOv10l performed much worse than these combinations of YOLOv8l (YOLOv8l performed better by $8.16\%$ and $3.81\%$ HOTA on VisDrone2019 when combined with ByteTrack and BoTSORT respectively), its IDs score is the lowest ($1788$ compares to $4212$ of YOLOv8l with the same setting, which is $42.4\%$ of the later). This means that YOLOv10l has the least number of IDs, or in simple words, it managed to "follow" objects well without mistakenly assigning new IDs. This insight, along with its highest score in the UAVDT dataset, indicates that YOLOv10l's combinations are generalized well and can perform in different scenarios more effectively.

In conclusion, deep learning-based approaches, notably those leveraging YOLO architectures and the BoTSORT tracker, hold significant promise for advancing multi-object tracking in aerial videos. However, the quest for real-time, highly accurate, and generalizable MOT systems remains an active area of research, with ongoing efforts focused on optimizing algorithms, addressing dataset-specific challenges, and striking an optimal balance between speed and accuracy.

% \subsection{Ablation Study}

\section{Conclusion}

In conclusion, this study has explored the critical challenge of multi-object tracking (MOT) in UAV videos, a domain characterized by dynamic scenes, small object sizes, and limited computational resources. Other research on Multi-Object Tracking dedicated to UAV videos primarily focuses on the academic aspect by developing sophisticated algorithms but lacks attention to the practical aspect of these systems. In our work, we address not only the algorithmic aspects but also emphasize the practicality of the system. We introduced a novel framework that harnesses the power of big data technologies, including Apache Kafka and Apache Spark, to enable scalable and efficient processing of video streams from multiple UAVs. By integrating a state-of-the-art MOT algorithm based on deep learning models and optimized data pipelines, our framework achieves real-time performance, delivering accurate and timely situational awareness for various applications. Our system achieves a HOTA of 48.14 and a MOTA of 43.51 on the Visdrone2019-MOT test set while maintaining a real-time processing speed of 28 FPS on a single GPU. Our work demonstrates the potential of big data technologies and deep learning for addressing the challenges of MOT in UAV applications. 

Through extensive experiments on the VisDrone2019-MOT and UAVDT datasets, we demonstrated the superior performance of our framework compared to existing methods, particularly those leveraging YOLO architectures and the BoTSORT tracker. Our findings emphasize the effectiveness of deep learning-based approaches for MOT in aerial videos while also highlighting the challenges posed by complex datasets like VisDrone2019-MOT, which demand further optimization to improve generalization capabilities.

Looking ahead, we envision several promising directions for future research. These include exploring more advanced deep learning models, incorporating additional data sources such as LiDAR and radar, and investigating strategies for adaptive resource allocation in UAV swarms. Ultimately, our goal is to develop robust, real-time MOT systems that empower UAVs to effectively navigate and interact with their environment, ultimately benefiting a wide range of industries and applications.

\begin{filecontents}{bibliography.bib}
@inproceedings{3,
  title={Mars: A video benchmark for large-scale person re-identification},
  author={Zheng at el., Liang},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14},
  pages={868--884},
  year={2016},
  organization={Springer}
}

@article{aharonBoTSORTRobustAssociations2022,
  title = {{{BoT-SORT}}: {{Robust}} Associations Multi-Pedestrian Tracking},
  author = {Aharon, Nir and Orfaig, Roy and Bobrovsky, Ben-Zion},
  year = {2022},
  journal = {arXiv preprint arXiv:2206.14651},
  eprint = {2206.14651},
  archiveprefix = {arXiv}
}

@article{bernardinEvaluatingMultipleObject2008,
  title = {Evaluating Multiple Object Tracking Performance: The Clear Mot Metrics},
  author = {Bernardin, Keni and Stiefelhagen, Rainer},
  year = {2008},
  journal = {EURASIP Journal on Image and Video Processing},
  volume = {2008},
  pages = {1--10},
  publisher = {Springer}
}

@inproceedings{bewleySimpleOnlineRealtime2016,
  title = {Simple Online and Realtime Tracking},
  booktitle = {2016 {{IEEE}} International Conference on Image Processing ({{ICIP}})},
  author = {Bewley at el., Alex},
  year = {2016},
  pages = {3464--3468},
  publisher = {IEEE},
  isbn = {1-4673-9961-2},
}

@inproceedings{caoObservationcentricSortRethinking2023,
  title = {Observation-Centric Sort: {{Rethinking}} Sort for Robust Multi-Object Tracking},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Cao at el., Jinkun},
  year = {2023},
  pages = {9686--9696},
}

@inproceedings{chenRealtimeMultiplePeople2018,
  title = {Real-Time Multiple People Tracking with Deeply Learned Candidate Selection and Person Re-Identification},
  booktitle = {2018 {{IEEE}} International Conference on Multimedia and Expo ({{ICME}})},
  author = {Chen, Long and Ai, Haizhou and Zhuang, Zijie and Shang, Chong},
  year = {2018},
  pages = {1--6},
  publisher = {IEEE},
  isbn = {1-5386-1737-4},
  CTLuse_forced_etal       = "yes",
  CTLmax_names_forced_etal = "3",
  CTLnames_show_etal       = "2"
}

@inproceedings{duUAVDT2018,
  title = {The Unmanned Aerial Vehicle Benchmark: {{Object}} Detection and Tracking},
  booktitle = {Proceedings of the {{European}} Conference on Computer Vision ({{ECCV}})},
  author = {Du at el., Dawei},
  year = {2018},
  pages = {370--386}
}

@article{fuSiameseObjectTracking2023,
  title = {Siamese Object Tracking for Unmanned Aerial Vehicle: A Review and Comprehensive Analysis},
  shorttitle = {Siamese Object Tracking for Unmanned Aerial Vehicle},
  author = {Fu, Changhong and Lu, Kunhan and Zheng, Guangze and Ye, Junjie and Cao, Ziang and Li, Bowen and Lu, Geng},
  year = {2023},
  month = oct,
  journal = {Artificial Intelligence Review},
  volume = {56},
  number = {1},
  pages = {1417--1477},
  issn = {1573-7462},
  doi = {10.1007/s10462-023-10558-5},
  urldate = {2024-06-12},
  abstract = {Unmanned aerial vehicle (UAV)-based visual object tracking has enabled a wide range of applications and attracted increasing attention in the field of artificial intelligence (AI) because of its versatility and effectiveness. As an emerging force in the revolutionary trend of deep learning, Siamese networks shine in UAV-based object tracking with their promising balance of accuracy, robustness, and speed. Thanks to the development of embedded processors and the gradual optimization of deep neural networks, Siamese trackers receive extensive research and realize preliminary combinations with UAVs. However, due to the UAV's limited onboard computational resources and the complex real-world circumstances, aerial tracking with Siamese networks still faces severe obstacles in many aspects. To further explore the deployment of Siamese networks in UAV-based tracking, this work presents a comprehensive review of leading-edge Siamese trackers, along with an exhaustive UAV-specific analysis based on the evaluation using a typical UAV onboard processor. Then, the onboard tests are conducted to validate the feasibility and efficacy of representative Siamese trackers in real-world UAV deployment. Furthermore, to better promote the development of the tracking community, this work analyzes the limitations of existing Siamese trackers and conducts additional experiments represented by low-illumination evaluations. In the end, prospects for the development of Siamese tracking for UAV-based AI systems are deeply discussed. The unified framework of leading-edge Siamese trackers, i.e., code library, and the results of their experimental evaluations are available at https://github.com/vision4robotics/SiameseTracking4UAV.},
  langid = {english},
  keywords = {Review and comprehensive analysis,Siamese networks,Unmanned aerial vehicle (UAV),Vision-based aerial object tracking},
  file = {C:\Users\donha\Zotero\storage\KALJ8GEF\Fu et al. - 2023 - Siamese object tracking for unmanned aerial vehicl.pdf}
}

@inproceedings{girshickFastRcnn2015,
  title = {Fast R-Cnn},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Girshick, Ross},
  year = {2015},
  pages = {1440--1448}
}

@article{jocherYOLOUltralytics2023,
  title = {{{YOLO}} by {{Ultralytics}}},
  author = {Jocher, Glenn and Chaurasia, Ayush and Qiu, Jing},
  year = {2023},
  publisher = {Jan}
}

@inproceedings{liuSsdSingleShot2016,
  title = {Ssd: {{Single}} Shot Multibox Detector},
  booktitle = {Computer {{Vision}}--{{ECCV}} 2016: 14th {{European Conference}}, {{Amsterdam}}, {{The Netherlands}}, {{October}} 11--14, 2016, {{Proceedings}}, {{Part I}} 14},
  author = {Liu at el., Wei},
  year = {2016},
  pages = {21--37},
  publisher = {Springer},
  isbn = {3-319-46447-7}
}

@inproceedings{liuUAVMOT2022,
  title = {Multi-Object Tracking Meets Moving {{UAV}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Liu, Shuai and Li, Xin and Lu, Huchuan and He, You},
  year = {2022},
  pages = {8876--8885}
}

@article{luitenHotaHigherOrder2021,
  title = {Hota: {{A}} Higher Order Metric for Evaluating Multi-Object Tracking},
  author = {Luiten el., Jonathon},
  year = {2021},
  journal = {International journal of computer vision},
  volume = {129},
  pages = {548--578},
  publisher = {Springer},
  isbn = {0920-5691}
}

@article{macrinaDroneaidedRoutingLiterature2020,
  title = {Drone-Aided Routing: {{A}} Literature Review},
  author = {Macrina, Giusy and Pugliese, Luigi Di Puglia and Guerriero, Francesca and Laporte, Gilbert},
  year = {2020},
  journal = {Transportation Research Part C: Emerging Technologies},
  volume = {120},
  pages = {102762},
  publisher = {Elsevier},
  isbn = {0968-090X}
}

@inproceedings{meinhardtTrackformer2022,
  title = {Trackformer: {{Multi-object}} Tracking with Transformers},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Meinhardt, Tim and Kirillov, Alexander and {Leal-Taixe}, Laura and Feichtenhofer, Christoph},
  year = {2022},
  pages = {8844--8854}
}

@article{mingyangHybridSORTWeakCues2023,
  title = {Hybrid-{{SORT}}: {{Weak Cues Matter}} for {{Online Multi-Object Tracking}}},
  author = {{Ming Yang} and {Guangxin Han} and {Bin Yan} and {Wenhua Zhang} and {Jinqing Qi} and {Huchuan Lu} and {Dong Wang}},
  year = {2023},
  month = aug,
  journal = {AAAI Conference on Artificial Intelligence},
  doi = {10.48550/arxiv.2308.00783},
  abstract = {Multi-Object Tracking (MOT) aims to detect and associate all desired objects across frames. Most methods accomplish the task by explicitly or implicitly leveraging strong cues (i.e., spatial and appearance information), which exhibit powerful instance-level discrimination. However, when object occlusion and clustering occur, both spatial and appearance information will become ambiguous simultaneously due to the high overlap between objects. In this paper, we demonstrate that this long-standing challenge in MOT can be efficiently and effectively resolved by incorporating weak cues to compensate for strong cues. Along with velocity direction, we introduce the confidence state and height state as potential weak cues. With superior performance, our method still maintains Simple, Online and Real-Time (SORT) characteristics. Furthermore, our method shows strong generalization for diverse trackers and scenarios in a plug-and-play and training-free manner. Significant and consistent improvements are observed when applying our method to 5 different representative trackers. Further, by leveraging both strong and weak cues, our method Hybrid-SORT achieves superior performance on diverse benchmarks, including MOT17, MOT20, and especially DanceTrack where interaction and occlusion are frequent and severe. The code and models are available at https://github.com/ymzis69/HybirdSORT.},
  annotation = {ARXIV\_ID: 2308.00783\\
MAG ID: 4385964530\\
S2ID: f2dd399edb1cc35340266bd27d8ece57d4759372},
}

@article{mohsanUnmannedAerialVehicles2022,
  title = {Towards the Unmanned Aerial Vehicles ({{UAVs}}): {{A}} Comprehensive Review},
  author = {Mohsan, Syed Agha Hassnain and Khan, Muhammad Asghar and Noor, Fazal and Ullah, Insaf and Alsharif, Mohammed H.},
  year = {2022},
  journal = {Drones},
  volume = {6},
  number = {6},
  pages = {147},
  publisher = {MDPI},
  isbn = {2504-446X}
}

@article{radoglou-grammatikisCompilationUAVApplications2020,
  title = {A Compilation of {{UAV}} Applications for Precision Agriculture},
  author = {{Radoglou-Grammatikis}, Panagiotis and Sarigiannidis, Panagiotis and Lagkas, Thomas and Moscholios, Ioannis},
  year = {2020},
  month = may,
  journal = {Computer Networks},
  volume = {172},
  pages = {107148},
  issn = {1389-1286},
  doi = {10.1016/j.comnet.2020.107148},
  urldate = {2024-06-28},
  abstract = {Climate change has introduced significant challenges that can affect multiple sectors, including the agricultural one. In particular, according to the Food and Agriculture Organization of the United Nations (FAO) and the International Telecommunication Union (ITU), the world population has to find new solutions to increase the food production by 70\% by 2050. The answer to this crucial challenge is the suitable adoption and utilisation of the Information and Communications Technology (ICT) services, offering capabilities that can increase the productivity of the agrochemical products, such as pesticides and fertilisers and at the same time, they should minimise the functional cost. More detailed, the advent of the Internet of Things (IoT) and specifically, the rapid evolution of the Unmanned Aerial Vehicles (UAVs) and Wireless Sensor Networks (WSNs) can lead to valuable and at the same time economic Precision Agriculture (PA) applications, such as aerial crop monitoring and smart spraying tasks. In this paper, we provide a survey regarding the potential use of UAVs in PA, focusing on 20 relevant applications. More specifically, first, we provide a detailed overview of PA, by describing its various aspects and technologies, such as soil mapping and production mapping as well as the role of the Global Positioning Systems (GPS) and Geographical Information Systems (GIS). Then, we discriminate and analyse the various types of UAVs based on their technical characteristics and payload. Finally, we investigate in detail 20 UAV applications that are devoted to either aerial crop monitoring processes or spraying tasks. For each application, we examine the methodology adopted, the proposed UAV architecture, the UAV type, as well as the UAV technical characteristics and payload.},
  keywords = {cite-only,Precision agriculture (PA),Remote sensing (RS),Unmanned aerial vehicle (UAV)},
}

@inproceedings{redmonYouOnlyLook2016,
  title = {You Only Look Once: {{Unified}}, Real-Time Object Detection},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Redmon at el., Joseph},
  year = {2016},
  pages = {779--788}
}

@article{shouxinrenFasterRCNNRealtime2015,
  title = {Faster {{R-CNN}}: Towards Real-Time Object Detection with Region Proposal Networks},
  author = {Shouxinat el., Ren},
  year = {2015},
  month = dec,
  volume = {28},
  pages = {91--99},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.},
  annotation = {MAG ID: 2613718673},
  file = {C:\Users\donha\Zotero\storage\RZC9GGDI\Shouxin Ren et al. - 2015 - Faster R-CNN towards real-time object detection w.pdf}
}

@inproceedings{wangJDE2020,
  title = {Towards Real-Time Multi-Object Tracking},
  booktitle = {European Conference on Computer Vision},
  author = {Wang at el., Zhongdao},
  year = {2020},
  pages = {107--122},
  publisher = {Springer}
}

@article{wangYolov10RealtimeEndtoend2024,
  title = {Yolov10: {{Real-time}} End-to-End Object Detection},
  author = {Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and Ding, Guiguang},
  year = {2024},
  journal = {arXiv preprint arXiv:2405.14458},
  eprint = {2405.14458},
  archiveprefix = {arXiv}
}

@misc{wojkeSimpleOnlineRealtime2017,
  title = {Simple {{Online}} and {{Realtime Tracking}} with a {{Deep Association Metric}}},
  author = {Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
  year = {2017},
  month = mar,
  number = {arXiv:1703.07402},
  eprint = {1703.07402},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.07402},
  urldate = {2024-01-18},
  abstract = {Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45\%, achieving overall competitive performance at high frame rates.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\donha\\Zotero\\storage\\DFGBC7GF\\Wojke et al. - 2017 - Simple Online and Realtime Tracking with a Deep As.pdf;C\:\\Users\\donha\\Zotero\\storage\\8HF39FKM\\1703.html}
}

@article{wuFPUAV2022,
  title = {One-Shot Multiple Object Tracking in {{UAV}} Videos Using Task-Specific Fine-Grained Features},
  author = {Wu, Han and Nie, Jiahao and He, Zhiwei and Zhu, Ziming and Gao, Mingyu},
  year = {2022},
  journal = {Remote Sensing},
  volume = {14},
  number = {16},
  pages = {3853},
  publisher = {MDPI},
  isbn = {2072-4292}
}

@article{yimingliAutoTrackHighPerformanceVisual2020,
  title = {{{AutoTrack}}: {{Towards High-Performance Visual Tracking}} for {{UAV With Automatic Spatio-Temporal Regularization}}},
  author = {{Yiming Li} and Li, Yiming and {Changhong Fu} and Fu, Changhong and {Fangqiang Ding} and Ding, Fangqiang and {Ziyuan Huang} and Huang, Ziyuan and {Geng Lu} and Lu, Geng and {Geng Lu} and {Geng Lu}},
  year = {2020},
  month = jun,
  journal = {Computer Vision and Pattern Recognition},
  pages = {11923--11932},
  doi = {10.1109/cvpr42600.2020.01194},
  abstract = {Most existing trackers based on discriminative correlation filters (DCF) try to introduce predefined regularization term to improve the learning of target objects, e.g., by suppressing background learning or by restricting change rate of correlation filters. However, predefined parameters introduce much effort in tuning them and they still fail to adapt to new situations that the designer did not think of. In this work, a novel approach is proposed to online automatically and adaptively learn spatio-temporal regularization term. Spatially local response map variation is introduced as spatial regularization to make DCF focus on the learning of trust-worthy parts of the object, and global response map variation determines the updating rate of the filter. Extensive experiments on four UAV benchmarks have proven the superiority of our method compared to the state-of-the-art CPU- and GPU-based trackers, with a speed of  60 frames per second running on a single CPU. Our tracker is additionally proposed to be applied in UAV localization. Considerable tests in the indoor practical scenarios have proven the effectiveness and versatility of our localization method. The code is available at https://github.com/vision4robotics/AutoTrack.},
  annotation = {ARXIV\_ID: 2003.12949\\
MAG ID: 3035466700\\
S2ID: 0619650ae0f698bcc38244a6858cc270df9dfaad},
  file = {C:\Users\donha\Zotero\storage\HGE4PSQD\Yiming Li et al. - 2020 - AutoTrack Towards High-Performance Visual Trackin.pdf}
}

@misc{yiUCMCTrackMultiObjectTracking2024,
  title = {{{UCMCTrack}}: {{Multi-Object Tracking}} with {{Uniform Camera Motion Compensation}}},
  shorttitle = {{{UCMCTrack}}},
  author = {Yi at el., Kefu},
  year = {2024},
  month = jan,
  number = {arXiv:2312.08952},
  eprint = {2312.08952},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.08952},
  urldate = {2024-06-12},
  abstract = {Multi-object tracking (MOT) in video sequences remains a challenging task, especially in scenarios with significant camera movements. This is because targets can drift considerably on the image plane, leading to erroneous tracking outcomes. Addressing such challenges typically requires supplementary appearance cues or Camera Motion Compensation (CMC). While these strategies are effective, they also introduce a considerable computational burden, posing challenges for real-time MOT. In response to this, we introduce UCMCTrack, a novel motion model-based tracker robust to camera movements. Unlike conventional CMC that computes compensation parameters frame-by-frame, UCMCTrack consistently applies the same compensation parameters throughout a video sequence. It employs a Kalman filter on the ground plane and introduces the Mapped Mahalanobis Distance (MMD) as an alternative to the traditional Intersection over Union (IoU) distance measure. By leveraging projected probability distributions on the ground plane, our approach efficiently captures motion patterns and adeptly manages uncertainties introduced by homography projections. Remarkably, UCMCTrack, relying solely on motion cues, achieves state-of-the-art performance across a variety of challenging datasets, including MOT17, MOT20, DanceTrack and KITTI. More details and code are available at https://github.com/corfyi/UCMCTrack},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\donha\\Zotero\\storage\\G7XHZJLP\\Yi et al. - 2024 - UCMCTrack Multi-Object Tracking with Uniform Came.pdf;C\:\\Users\\donha\\Zotero\\storage\\SYKZ8P6F\\2312.html}
}

@article{yooDroneDeliveryFactors2018,
  title = {Drone Delivery: {{Factors}} Affecting the Public's Attitude and Intention to Adopt},
  author = {Yoo, Wonsang and Yu, Eun and Jung, Jaemin},
  year = {2018},
  journal = {Telematics and Informatics},
  volume = {35},
  number = {6},
  pages = {1687--1700},
  publisher = {Elsevier},
  isbn = {0736-5853}
}

@article{yu-hsiangwangSMILEtrackSiMIlarityLEarning2022,
  title = {{{SMILEtrack}}: {{SiMIlarity LEarning}} for {{Occlusion-Aware Multiple Object}}   {{Tracking}}},
  author = {{Yu-Hsiang Wang} and {Jun-Wei Hsieh} and {Ping-Yang Chen} and {Ming-Ching Chang}},
  year = {2022},
  month = nov,
  journal = {arXiv.org},
  doi = {10.48550/arxiv.2211.08824},
  abstract = {Despite recent progress in Multiple Object Tracking (MOT), several obstacles such as occlusions, similar objects, and complex scenes remain an open challenge. Meanwhile, a systematic study of the cost-performance tradeoff for the popular tracking-by-detection paradigm is still lacking. This paper introduces SMILEtrack, an innovative object tracker that effectively addresses these challenges by integrating an efficient object detector with a Siamese network-based Similarity Learning Module (SLM). The technical contributions of SMILETrack are twofold. First, we propose an SLM that calculates the appearance similarity between two objects, overcoming the limitations of feature descriptors in Separate Detection and Embedding (SDE) models. The SLM incorporates a Patch Self-Attention (PSA) block inspired by the vision Transformer, which generates reliable features for accurate similarity matching. Second, we develop a Similarity Matching Cascade (SMC) module with a novel GATE function for robust object matching across consecutive video frames, further enhancing MOT performance. Together, these innovations help SMILETrack achieve an improved trade-off between the cost (\{{\textbackslash}em e.g.\}, running speed) and performance (e.g., tracking accuracy) over several existing state-of-the-art benchmarks, including the popular BYTETrack method. SMILETrack outperforms BYTETrack by 0.4-0.8 MOTA and 2.1-2.2 HOTA points on MOT17 and MOT20 datasets. Code is available at https://github.com/pingyang1117/SMILEtrack\_Official},
  annotation = {MAG ID: 4320719710\\
S2ID: 97409859dbde7721f6092224f23397438691d25b}
}

@article{yuanUltrareliableIoTCommunications2018,
  title = {Ultra-Reliable {{IoT}} Communications with {{UAVs}}: {{A}} Swarm Use Case},
  author = {Yuan at el., Zhenhui},
  year = {2018},
  journal = {IEEE Communications Magazine},
  volume = {56},
  number = {12},
  pages = {90--96},
  publisher = {IEEE},
  isbn = {0163-6804}
}

@inproceedings{zengMotrEndtoendMultipleobject2022,
  title = {Motr: {{End-to-end}} Multiple-Object Tracking with Transformer},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Zeng at el., Fangao},
  year = {2022},
  pages = {659--675},
  publisher = {Springer}
}

@misc{zhangByteTrackMultiObjectTracking2022,
  title = {{{ByteTrack}}: {{Multi-Object Tracking}} by {{Associating Every Detection Box}}},
  shorttitle = {{{ByteTrack}}},
  author = {Zhang at el., Yifu},
  year = {2022},
  month = apr,
  number = {arXiv:2110.06864},
  eprint = {2110.06864},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.06864},
  urldate = {2024-06-11},
  abstract = {Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects in videos. Most methods obtain identities by associating detection boxes whose scores are higher than a threshold. The objects with low detection scores, e.g. occluded objects, are simply thrown away, which brings non-negligible true object missing and fragmented trajectories. To solve this problem, we present a simple, effective and generic association method, tracking by associating almost every detection box instead of only the high score ones. For the low score detection boxes, we utilize their similarities with tracklets to recover true objects and filter out the background detections. When applied to 9 different state-of-the-art trackers, our method achieves consistent improvement on IDF1 score ranging from 1 to 10 points. To put forwards the state-of-the-art performance of MOT, we design a simple and strong tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single V100 GPU. ByteTrack also achieves state-of-the-art performance on MOT20, HiEve and BDD100K tracking benchmarks. The source code, pre-trained models with deploy versions and tutorials of applying to other trackers are released at https://github.com/ifzhang/ByteTrack.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\donha\\Zotero\\storage\\5657KCJQ\\Zhang et al. - 2022 - ByteTrack Multi-Object Tracking by Associating Ev.pdf;C\:\\Users\\donha\\Zotero\\storage\\D9KLSZBH\\2110.html}
}

@article{zhangFairMOT2021,
  title = {{{FairMOT}}: {{On}} the {{Fairness}} of {{Detection}} and {{Re-Identification}} in {{Multiple Object Tracking}}},
  shorttitle = {{{FairMOT}}},
  author = {Zhang at el., Yifu},
  year = {2021},
  month = nov,
  journal = {International Journal of Computer Vision},
  volume = {129},
  number = {11},
  eprint = {2004.01888},
  primaryclass = {cs},
  pages = {3069--3087},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-021-01513-4},
  urldate = {2024-05-30},
  abstract = {Multi-object tracking (MOT) is an important problem in computer vision which has a wide range of applications. Formulating MOT as multi-task learning of object detection and re-ID in a single network is appealing since it allows joint optimization of the two tasks and enjoys high computation efficiency. However, we find that the two tasks tend to compete with each other which need to be carefully addressed. In particular, previous works usually treat re-ID as a secondary task whose accuracy is heavily affected by the primary detection task. As a result, the network is biased to the primary detection task which is not fair to the re-ID task. To solve the problem, we present a simple yet effective approach termed as FairMOT based on the anchor-free object detection architecture CenterNet. Note that it is not a naive combination of CenterNet and re-ID. Instead, we present a bunch of detailed designs which are critical to achieve good tracking results by thorough empirical studies. The resulting approach achieves high accuracy for both detection and tracking. The approach outperforms the state-of-the-art methods by a large margin on several public datasets. The source code and pre-trained models are released at https://github.com/ifzhang/FairMOT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\donha\\Zotero\\storage\\T9JB9GV5\\Zhang et al. - 2021 - FairMOT On the Fairness of Detection and Re-Ident.pdf;C\:\\Users\\donha\\Zotero\\storage\\FKJWIS3A\\2004.html}
}

@article{zhuVisDrone2021,
  title = {Detection and Tracking Meet Drones Challenge},
  author = {Zhu at el., Pengfei},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {11},
  pages = {7380--7399},
  publisher = {IEEE},
  isbn = {0162-8828}
}


\end{filecontents} 

\bibliographystyle{abbrv}
\bibliography{bibliography}

\end{document}
