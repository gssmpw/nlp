\section{Related Works}
\label{related-works}

\textbf{Object Detection.} Object detection, a fundamental task in computer vision, plays a crucial role in any MOT system. Object detection has been made a significant process during the past decade with the advent of many deep learning approaches. 

Two-stage detectors, such as Fast R-CNN\cite{girshickFastRcnn2015} and Faster R-CNN\cite{shouxinrenFasterRCNNRealtime2015}, utilize region proposal networks to identify potential objects, followed by a classification and bounding box regression step. This method offers high accuracy but can be computationally expensive. One-stage detectors, like YOLO \cite{redmonYouOnlyLook2016, wangYolov10RealtimeEndtoend2024, jocherYOLOUltralytics2023} or SSD \cite{liuSsdSingleShot2016}, directly predict object classes and bounding boxes in a single pass. They prioritize speed and are suitable for real-time applications but may sacrifice some accuracy compared to two-stage detectors. Transformer-based architectures, like DETR, have also emerged for object detection, offering improved accuracy and the ability to handle complex scenes effectively. These advancements in object detection contribute significantly to enhancing the accuracy and robustness of MOT systems by providing reliable object detection as input.

\textbf{Data Association.} Many algorithms and strategies were developed to help MOT system match between tracklets and detection boxes. SORT \cite{bewleySimpleOnlineRealtime2016}, a Simple online and real-time tracking algorithm that utilizes location and motion cues in a very simple yet effective way. Many significant methods adopt SORT, with some modifications to achieve favorable performance \cite{zhangByteTrackMultiObjectTracking2022, zhangFairMOT2021, mingyangHybridSORTWeakCues2023, caoObservationcentricSortRethinking2023, wangJDE2020, yiUCMCTrackMultiObjectTracking2024, wojkeSimpleOnlineRealtime2017}. Using the location and motion cues only is not robust to scenes with low certainty, large camera motion, or low frame rate. In these cases, appearance cues appear to work more efficiently. In scenarios when an object is occluded for a brief moment or even for a long period of time, it can be recognized using appearance information. DeepSORT \cite{wojkeSimpleOnlineRealtime2017} replaced the simple association metric in SORT with a more informed metric that combines motion and appearance cues. MOTDT \cite{chenRealtimeMultiplePeople2018} matches tracklets with detection by using appearance information first, then using IoU to match the remaining tracklets. FairMOT \cite{zhangFairMOT2021} addresses the issue of imbalanced optimization in multi-object tracking. It uses a single network to estimate object detections and perform re-identification (ReID) simultaneously. By jointly learning these tasks, FairMOT improves feature representation for both detection and ReID, leading to more robust and accurate tracking, especially in crowded scenes. ByteTrack \cite{zhangByteTrackMultiObjectTracking2022} introduces a new data association method, BYTE, which utilizes every detection box, including those with low confidence scores. This strategy involves associating high-scoring detections with existing tracklets and then matching remaining low-scoring detections with unmatched tracklets. This approach is reportedly efficient in working with occlusion, motion blur, or size changes. BoTSORT \cite{aharonBoTSORTRobustAssociations2022} takes a step further by adding improvements on top of ByteTrack, namely camera motion compensation-based (CMC) features tracker, with a new simple yet effective method for IoU and ReIDâ€™s cosine-distance fusion. Additionally, although inspired by ByteTrack, BoTSORT incorporates a more sophisticated data association strategy to handle challenging cases like occlusions and missed detections. 

\textbf{MOT on UAV videos.} 
Multiple Object Tracking (MOT) in UAV videos presents unique challenges due to the small size, fast motion, and varying perspectives of targets. Early approaches, often adapted from ground-based MOT methods, struggled with the non-linear motion models and abrupt appearance changes common in UAV footage. The advent of deep learning has led to significant advancements, with two-stage detectors like YOLO \cite{redmonYouOnlyLook2016} and Faster R-CNN \cite{shouxinrenFasterRCNNRealtime2015} combined with ReID techniques \cite{3} showing promise. More recently, one-shot MOT methods that integrate detection and ReID into a single network have gained popularity due to their real-time capabilities \cite{liuUAVMOT2022}. However, challenges such as long-term occlusions, varying object densities, and adverse weather conditions remain. Future research is likely to explore online learning, motion prediction, and the integration of contextual information to further improve tracking performance in UAV videos.