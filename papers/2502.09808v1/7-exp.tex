%!TEX root = gcn.tex

\section{Experiments and Evaluative Results}

We evaluate the performance of our \osmm protocol and $\cgnn$'s private GCN inference/training on three Ubuntu servers with $16$-core Intel(R) Xeon(R) Platinum 8163 2.50GHz CPUs of $62$GB
RAM and NVIDIA-T4 GPU of $16$GB RAM.
We aim to answer the three questions below.

\noindent\textbf{Q1.} \emph{How much communication/memory-efficient and accurate for \cgnn?} (\S\ref{sec::comm_compare_gcn}, \S\ref{sec::smmmemory}, \S\ref{sec::acc_compare_gcn})

\noindent\textbf{Q2.} \emph{How do different network conditions impact the running time of \cgnn's inference and training?} (\S\ref{sec::time_net})

\noindent\textbf{Q3.} \emph{How much efficiency has been improved by \osmm?} (\S\ref{sec:ablation})

%\noindent\textbf{Q3.} \textit{How much communication costs are reduced for secure Adam compared with AdamPriv?} (\S\ref{sec::comp_adam})

{Graph Datasets.}
We consider three publication datasets widely adopted in GCN training: Citeseer~\cite{dl/GilesBL98}, Cora~\cite{aim/SenNBGGE08}, and Pubmed~\cite{ijcnlp/DernoncourtL17}.
Their statistics are summarized in Table~\ref{tab:datasets}.
%Following GCN~\cite{iclr/KipfW17}, the number of samples in training sets is $140, 120$, and $60$, respectively; the samples in test sets are $1000$.
%Each publication is described by a $0/1$-valued word vector indicating the absence/presence of the corresponding word from the dictionary.
\begin{table}[!t]
\centering
\caption{Dataset Statistics}
\setlength\tabcolsep{2pt}
\begin{tabular}{l|rrrr|rr}
\hline
\multicolumn{1}{c|}{\textbf{Dataset}} & \textbf{Node} & \textbf{Edge} & \textbf{Feature} & \textbf{Class} &\textbf{\# Train} &\textbf{\# Test}
\\ \hline
Cora 	 & $2,708$ 	 & $5,429$ 	 & $1,433$ & $7$ & $140$ & $1,000$ \\
Citeseer 	 	 & $3,312$ 	 & $4,732$ 	 & $3,703$ 	 & $6$ & $120$ & $1,000$ \\
Pubmed 	 	 & $19,717$ 	 	 & $44,338$ 	 	 & $500$ 	 & $3$ 	 & $60$ & $1,000$ \\
\hline
\end{tabular}
\begin{tablenotes}
\item \# Train/Test: number of samples in training/test dataset
\end{tablenotes}
\label{tab:datasets}
\end{table}

\iffalse
\begin{itemize}
\item CiteSeer dataset contains $3312$ scientific publications and $6$ classes.
The citation network consists of $4732$ links.
The dictionary consists of $3703$ unique words.

\item Cora dataset contains $2708$ scientific publications and $7$ classes.
The citation network consists of $5429$ links, while the dictionary contains $1433$ unique words.

\item PubMed dataset consists of $19717$ scientific publications from the PubMed database classified into one of three classes.
The citation network consists of $44338$ links.
Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary consisting of $500$ unique words.
\end{itemize}
\fi


\subsection{Communication of GCN}
\label{sec::comm_compare_gcn}
To evaluate communication costs in $\cgnn$, we record the transmitting data, including frame and MPC-related data in both online and offline phases, across the servers or ports.
%We tested the communication for inference and training by measuring the transmitting data among the servers.
The inference refers to
%one-time 
a forward propagation, while the training involves an epoch of training.
Unlike classical CNN training over independent data points, GCN training feeds up the whole graph (\ie, $1$ batch) in each training epoch, thus no benchmarking for batch sizes.

\paragraph{Secure Training.} 
SecGNN~\cite{tsc/WangZJ23} and  CoGNN~\cite{ccs/ZouLSLXX24} are the only two open-sourced works for secure training with MPC.
SecGNN~\cite{tsc/WangZJ23} is the first work, meanwhile
 CoGNN~\cite{ccs/ZouLSLXX24} and its optimized version CoGNN-Opt are the most recent advances.
Thus, we choose them as \cgnn's counterparts for comparison.
Table~\ref{table:comm_on_off_gcn} shows their comparison results.
In general, $\cgnn$ uses ${\leq}1.3$GB in all cases.
Using SGD, $\cgnn$ uses $0.3075$GB, $0.5400$GB, and $1.2567$GB for training over Cora, Citeseer, and Pubmed.
With Adam, $\cgnn$ costs slightly higher communication due to SGD not needing $1/\sqrt{x}.$
%division and square roots.
The additional
%communication 
costs are $6.2\%$, $3.7\%$, and $0.8\%$ for
%one-epoch 
training.
These differences are related to the sparsity of
%training 
data and the times of gradient update.
All the cases above require less communication costs than CoGNN and SecGNN.
%SecGNN~\cite{tsc/WangZJ23} is the only open-source framework for secure GCN inference and training to our knowledge.
%Thus, we compared accuracy and communication with SecGNN.
%From the SecGNN implementation~\cite{tsc/WangZJ23}, 
%Its communication costs are computed by counting the number of multiplication.
%For fair comparisons, we multiply $64$ by the number of arithmetic multiplications and then count the boolean multiplication to derive the overall communication costs of SecGNN.




\paragraph{Secure Inference.}
Except for CoGNN and SecGNN, we additionally compare \cgnn with the most recent secure inference  work -- OblivGNN~\cite{uss/XuL0AYY24} for comprehensiveness.
Table~\ref{table:comm_on_off_inf} compares the communication costs.
$\cgnn$ requires the lowest communication costs in all cases, reducing by  $\sim50\%$ of OblivGNN and $\sim 80\%$ of CoGNN-Opt.


\iffalse
\begin{figure}[!t]
% \vspace{-5mm}
 	\centering
 	\includegraphics[width = 0.38\textwidth]{./fig_and_tab/fig_comm_inf}
 	\caption{ Communication for Inference}
 	\label{fig_comm_on_off_inf}
 	 	%\end{minipage}
\end{figure}
\fi

\begin{table}[!t]
 	\centering
 	\caption{Communication (GB/epoch) for Training}
 	\label{table:comm_on_off_gcn}
 \setlength\tabcolsep{9pt}
 	\begin{tabular}{l|ccc}
 	\hline
 	\multirow{2}{*}{\textbf{Framework}} & \multicolumn{3}{c}{\textbf{Dataset}}
	\\\cline{2-4}
 & \textbf{Cora} 	 & \textbf{Citeseer} & \textbf{Pubmed}\\\hline
 	SecGNN & 	$18.99$ & $48.21$ & $31.74$\\
    CoGNN & $86.99$ & $202.81$ &$273.25$ \\
    CoGNN-Opt & $0.82$ & $1.4$ & $4.33$\\\hline
 \rowcolor{grayL}$\cgnn$ (SGD) & $0.3075$ & $0.5400$ & $1.2567$ \\
 \rowcolor{grayL}$\cgnn$ (Adam) & $0.3265$ & $0.5600$ & $1.2667$ \\
 	 	\hline
 	 	\end{tabular}
 	\end{table}


\begin{table}[!t]
 	\centering
 	\caption{Communication for Inference}
 	\label{table:comm_on_off_inf}
\setlength\tabcolsep{11pt}
 	\begin{tabular}{l|ccc}
 	\hline
 	\multirow{2}{*}{\textbf{Framework}} & \multicolumn{3}{c}{\textbf{Dataset}}
	\\\cline{2-4}
 & \textbf{Cora} 	 & \textbf{Citeseer} & \textbf{Pubmed}\\\hline
	SecGNN & 	$1$GB & $1.7$GB & $2.5$GB\\
    CoGNN & $85.63$GB & $201.29$GB &$263.59$GB\\
 CoGNN-Opt & $0.5$GB &$0.91$GB & $2.02$GB\\\hline
 OblivGNN-B & $34.32$GB & $61.81$GB &$16.33$GB\\
 OblivGNN &$0.29$GB&$0.41$GB& $1.65$GB\\\hline
    \rowcolor{grayL}$\cgnn$ & $114$MB & $274$MB & $602$MB \\
 	 	\hline
 	 	\end{tabular}
\end{table}

 \iffalse
 	\begin{figure}[!t]
% \vspace{-5mm}
 	\centering
 	\includegraphics[width = 2.5in, height=1.7in]{./fig_and_tab/fig_acc}
 	\caption{Model Accuracy}
 	\label{fig_acc}
 	 	%\end{minipage}
\end{figure}
\fi

\subsection{Memory Usage}
\label{sec::smmmemory}
To avoid extra irrelevancy (\eg, communication), we tested the memory usage
%of all parties 
on a single server, recording the largest observed value.
Table~\ref{table:mem_smm} reports memory usage for training with $\prosmm$ and the standard $\promult$ using Beaver triple.
Both protocols show acceptable results for
%the relatively small 
smaller Cora and Citeseer datasets.
%Simultaneously, 
Yet, $\prosmm$ saves $14.5\%, 20.8\%$ memory with secure SGD,
and $10.5\%, 18.2\%$ memory with secure Adam.

The maximum memory SGD training uses is slightly lower than with Adam, as Adam's optimization requires more memory.
When training over the larger Pubmed dataset,
%we failed to attain reasonable results 
an out-of-memory (OOM) error occurs (marked by $^{\oom}$) when using Beaver triple, whereas the \cgnn with $\prosmm$ supports the stable use (${<}2.7$GB) of memory for all datasets.

\begin{table}[!t]
 	\caption{Maximum Memory Usage (GB) for Training}
 	\label{table:mem_smm}
 	\setlength\tabcolsep{3pt}
 	\centering
 	\begin{tabular}{c|c|c|c|c}
 	\hline
	\textbf{Optimizer} & 	\textbf{Dataset} & \textbf{Protocol} & \textbf{Memory} & \textbf{Reduction} 
	\\
 	\hline 
	\multirow{6}{*}{\textbf{SGD}} &\multirow{2}{*}{\textbf{Cora}} & Beaver & $1.31$ &
	\\
 	 & & \cellcolor{grayL}$\prosmm$ & 	\cellcolor{grayL}$1.12$ & \cellcolor{grayL}$14.5\%$
 	\\\cline{2-5}
 	 & \multirow{2}{*}{\textbf{Citeseer}} & Beaver & $2.07$ &
	\\
 	 & & \cellcolor{grayL}$\prosmm$ & \cellcolor{grayL}$1.64$ & \cellcolor{grayL}$20.8\%$ 
	\\\cline{2-5}
	 & \multirow{2}{*}{\textbf{Pubmed}} & Beaver & ${>}28.82^{\oom}$ & 
	\\
	 & & \cellcolor{grayL}$\prosmm$ & \cellcolor{grayL}$1.94$ & \cellcolor{grayL}${>}93.3\%$
	\\\hline
 	\multirow{6}{*}{\textbf{Adam}} & \multirow{2}{*}{\textbf{Cora}} & Beaver & $1.91$ &
	\\
 	 & & \cellcolor{grayL}$\prosmm$ & 	\cellcolor{grayL}$1.71$ & \cellcolor{grayL}$10.5\%$
	\\\cline{2-5}
 	 & \multirow{2}{*}{\textbf{Citeseer}} & Beaver & $2.75$ &
	\\
 	 & & \cellcolor{grayL}$\prosmm$ & 	\cellcolor{grayL}$2.25$ & \cellcolor{grayL}$18.2\%$ 
	\\\cline{2-5}
	 & \multirow{2}{*}{\textbf{Pubmed}} & Beaver & ${>}28.02^{\oom}$ & 
	\\
	 & & \cellcolor{grayL}$\prosmm$ & \cellcolor{grayL}$2.69$ & \cellcolor{grayL}${>}90.4\%$ 
	\\\hline
 \end{tabular}
 \begin{tablenotes}
 \item  ${\oom}$:  out-of-memory (OOM) error occurs.
 \end{tablenotes}
\end{table}

\subsection{Model Accuracy}
\label{sec::acc_compare_gcn}
We trained the GCN over different datasets from random initialization for $300$ epochs using Adam~\cite{iclr/KingmaB14} with a $0.001$ learning rate.
Our configuration of model parameters (\ie, the dimensionality of hidden layers and the number of samples) follow the original setting~\cite{iclr/KipfW17}.
Since model accuracy is meaningful only for identical partitioning strategy, we compare the accuracy of secure training with plaintext in the same contexts in Table~\ref{tab:acc_inf_tra}.
Our results show that $\cgnn$'s accuracy is comparable to that of plaintext~training.
Specifically, $\cgnn$ achieves $\{73.5\%, 64.4\%, 75.4\%\}$ after $100$ epochs and $\{76.0\%, 65.1\%, 75.2\%\}$ after $300$ epochs.
Due to fluctuated training convergence, fixed-point representation, and non-linear approximation, model accuracy is slightly different.

%%%%%% Keep it in case reviewers ask
%In SecGNN, the number of samples in training sets is $40$ per class, and the number of samples in the test set is $500$.}, which reduces the training difficulty.
%instead of the standard graph datasets as in plaintexts, which reduces the training difficulty.
%Specifically, in plaintext training and $\cgnn$, the number of samples in training sets is $140, 120$, and $60$, respectively;
%the samples in test sets are $1000$.
%In SecGNN, the number of samples in training sets is $40$ per class, and the number of samples in the test set is $500$.
 
\begin{table}[!t]
 	\centering
 	\caption{Model Accuracy}
 	\label{tab:acc_inf_tra}
 	\setlength\tabcolsep{7pt}
 	\begin{tabular}{l|ccc}
	\hline
 	\multirow{2}{*}{\textbf{Framework}} & \multicolumn{3}{c}{\textbf{Dataset}}
	\\\cline{2-4}
 & \textbf{Cora} &\textbf{Citeseer} & \textbf{Pubmed}\! \\
 	 	\hline
	{Plaintext}
	%\tnote{*}
	& $75.7\%$ 	 & $65.4\%$ & $74.5\%$\! \\
% 	SecGNN & $78.0\%$ & $68.3\%$ & $78.6\%$ 	\\
 	\rowcolor{grayL}$\cgnn$ ($100$ Epochs)\! & $73.5\%$ & $64.4\%$ & \textbf{75.4\%}\! \\
 	\rowcolor{grayL}$\cgnn$ ($300$ Epochs)\! & \textbf{76.0\%} & \textbf{65.1\%} & $75.2\%$\! \\
 	 	\hline
 	 	\end{tabular}
 	\iffalse
 	\begin{tablenotes}
 	\item {The results of ``plaintext'' are from running dgl at {\url{https://github.com/dmlc/dgl}}.
 	The results of ``SecGNN'' are from running the code at \url{https://github.com/songleiW/SecGNN.git}.
 	}
% In Plaintext and SuGar, the accuracy is after $300$-epoch training.
%In SecGNN, the accuracy is after $30$-epoch training.
 	\end{tablenotes}
 	\fi
\end{table}

\subsection{Running Time in Different Networks}
\label{sec::time_net}
We simulate real-world deployment under different network conditions for \osmm, private inference, and private training.
In particular, we consider a normal network condition ($800$Mbps, $0.022$ms) and two poor network conditions, including a narrow-bandwidth (N.B.) network ($200$Mbps, $0.022$ms) and high-latency (H.L.) network ($800$Mbps, $50$ms).
Additionally,  TCP transmission involves the process of three-step handshake, data transmission, congestion control, and connection termination, thus practical time delay of $\osmm$ is varied below under different network conditions.
\begin{table*}[!t]
	\centering
		\caption{$1$-Epoch Training Time (seconds) in Normal, Narrow-Bandwidth, or High-Latency Networks}
		\label{table:tra_net}
  \setlength\tabcolsep{8pt}
			\begin{tabular}{l|l|r|r|r|r|r|r}
				\hline
     \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{{Protocol}}} & \multicolumn{2}{c|}{\textbf{Normal ($800$Mbps, $0.022$ms)}}& \multicolumn{2}{c|}{\textbf{N.B. ($200$Mbps, $0.022$ms)}}& \multicolumn{2}{c}{\textbf{H.L. ($800$Mbps, $50$ms)}}
     \\\cline{3-8}
     & & \phantom{sherman}{\textbf{SGD}} 
     & {\textbf{Adam}}
     & \phantom{sherman}{\textbf{SGD}}
     & \phantom{123}{\textbf{Adam}}
     & \phantom{sherman}{\textbf{SGD}}
     & \phantom{123}{\textbf{Adam}}\\
     %& &\textbf{Time}&\textbf{Time}&\textbf{Time}&\textbf{Time}&\textbf{Time}&\textbf{Time}\\
				\hline
    \multirow{3}{*}{\textbf{Cora}} & Beaver &$6.55 $ &$7.89 $  &$25.98 $   &$27.55 $   &$11.70 $  &$19.57 $  \\
	  &$\prosmm$&$4.20 $  &  $ 5.55$  &  $13.29 $  &  $14.88 $  
   & \raggedleft $9.11$
   &  $16.72 $   \\ 
     & (Saving) 
     & \cellcolor{grayL}{$35.9\%$} 
     & \cellcolor{grayL}{$29.7\%$} 
     & \cellcolor{grayL}{\textbf{48.8\%}} 
     & \cellcolor{grayL}{\textbf{46.0\%}} 
     & \cellcolor{grayL}{$22.1\%$}
     & \cellcolor{grayL}{$14.6\%$}
     \\\hline
     
  
   \multirow{3}{*}{\textbf{Citeseer}} & Beaver &$11.66 $   &$ 13.20$  &$46.31 $  &$48.75 $  &$ 18.53$  &$27.93 $  \\
   
    & $\prosmm$
    & \raggedleft $6.77$ 
    & \raggedleft $8.35$  & $24.00 $  & $26.44 $  & $ 13.47$  & $ 21.26$  \\
      & (Saving) & \cellcolor{grayL}{$41.9\%$} &\cellcolor{grayL}{$36.7\%$} & \cellcolor{grayL}{\textbf{48.2\%}} & \cellcolor{grayL}{\textbf{45.8\%}}& \cellcolor{grayL}{$27.3\%$}&\cellcolor{grayL}{$23.9\%$} \\\hline
    
     \multirow{2}{*}{\textbf{Pubmed}} & Beaver &OOM  &OOM& OOM& OOM& OOM& OOM  \\    
 & $\prosmm$  &$ 22.87 $   &$24.45 $ & $63.69 $   &$63.58 $   &$32.00 $  &$39.86 $ \\
	\hline
	\end{tabular} 
\end{table*}

\paragraph{Inference Time.}
Table~\ref{table:inf_time_net} compares the private inference time,
%in the varying networks.
%Inference time contains the time of simulating a practical inference on the cloud server, 
including TensorFlow-Graph construction and forward-propagation computation of GCN in varying network conditions
%We conducted one-time inference experiments in each network type 
over multiple
%Cora, Citeseer, and Pubmed 
datasets.
Compared to adopting Beaver triples, $\cgnn$ via $\prosmm$ is ${\sim}7\%$-$19\%$ faster in the normal network, ${\sim}35\%$-$45\%$ quicker in the narrow-bandwidth one, and saves ${\sim}6\%$-$17\%$ time in the high-latency setting.
%In the normal network, $\cgnn$ via $\prosmm$ is ${\sim}7\%$-$19\%$ faster than adopting Beaver triples.
%In the narrow-bandwidth network, $\cgnn$ via $\prosmm$ is ${\sim}35\%$-$45\%$ quicker, while $\cgnn$ via $\prosmm$ is ${\sim}6\%$-$17\%$ faster in the high-latency setting.
The OOM problem prevents us from evaluating inference over Pubmed using Beaver triples, 
%For inference over Pubmed, the OOM problem prevents us from obtaining evaluation using Beaver triples, 
while $\prosmm$ takes ${\sim}30$-$50$s.

\input{./fig_and_tab/tab_inf_time_net}

\input{./fig_and_tab/tab_tra_network.tex}

\paragraph{Training Time.}
Table~\ref{table:tra_net} compares the private training time
%equipped 
with SGD/Adam in varying network conditions over different datasets.
%the varying networks.
%For reliable results, w
We tested $10$ epochs and got the average.
%In each type of network, we tested the time of one-epoch training over Cora, Citeseer, and Pubmed datasets.
In the normal network, $\cgnn$ via $\prosmm$ is ${\sim}56\%$-$73\%$ faster with SGD and ${\sim}42\%$-$58\%$ faster with Adam.
In the narrow-bandwidth network, $\cgnn$ via $\prosmm$ is ${\sim}93\%$-$95\%$ quicker with SGD and ${\sim}84\%$-$85\%$ quicker with Adam.
Besides, $\cgnn$ via $\prosmm$ is ${\sim}28\%$-$38\%$ faster with SGD and ${\sim}17\%$-$32\%$ faster with Adam in the high-latency setting.
%For private training over Pubmed, we encountered the same issue as that was happening in private inference.

\begin{table}[!t]
 	\centering
 	\caption{Communication Costs (MB) for SMM}
 	\label{table:comm_smm}
 	\setlength\tabcolsep{7pt}
 	 	\begin{tabular}{c|c|r|r|r}
 	 	\hline
 	\textbf{$\#$E/N} & \textbf{$\#$Node} & \textbf{Beaver} & ${\prosmm}$ & \textbf{Saving} \\
 	 	\hline
 	$1$ & $1000$ & $25.1$ & $0.8$ & \cellcolor{grayL}{$ 96.8\%$} \\
 	$1$ & $2000$ & $ 100.3$ & $1.3$ & \cellcolor{grayL}{$ 98.7\%$}\\
% 	$1$ & $3000$ & $225.5$ & $1.8$ & 	{$ 99.2\%$} \\
% $1$ & $4000$ & $400.7 $ & $2.3$ & {$ 99.4\%$} \\
 	$1$ & $5000$ & $626.1$ & $2.8$ & \cellcolor{grayL}{\textbf{99.6\%}} 	\\
 	\hline
 	$2$ & $1000$ & $25.1$ & $1.0$ & \cellcolor{grayL}{$ 95.9\%$} \\
 	$2$ & $2000$ & $100.3$ & $1.8$ & \cellcolor{grayL}{$ 98.2\%$} \\
% $2$ & $3000$ & $225.5$ & $2.5$ & 	{$ 98.9\%$} \\
% $2$ & $4000$ & $400.7$ & $3.2$ & {$ 99.2\%$} \\
 	$2$ & $5000$ & $626.0$ & $3.9$ & 	\cellcolor{grayL}{\textbf{99.4\%}} \\
 	\hline
 	$3$ & $1000$ & $25.1$ & $1.3$ & \cellcolor{grayL}{$ 95.0\%$} \\
 	$3$ & $2000$ & $100.3$ & $2.2$ & \cellcolor{grayL}{$ 97.8\%$} \\
% $3$ & $3000$ & $225.5$ & $3.2$ & {$ 98.6\%$} 	\\
% $3$ & $4000$ & $400.8$ & $4.1$ & {$ 99.0\%$} 	\\
 	$3$ & $5000$ & $626.1$ & $5.1$ & \cellcolor{grayL}{\textbf{99.2\%}} 	\\
 	\hline
 	 	\end{tabular} 
 	 	
 	$\#$E/N: ratio of edges per node,
	\\``Beaver'': using Beaver triples for SMM
\end{table}

\begin{table}[!t]
 	\centering
 	\caption{Memory Usage (MB) Given Varying Sparsity}
 	\label{table:mem_smm_sparse}
 \setlength\tabcolsep{7pt}
 	 	\begin{tabular}{c|c|r|r|r}
 	 	\hline \textbf{$\#$E/N} & \textbf{$\#$Node} & \textbf{Beaver} & ${\prosmm}$ & \textbf{Saving} \\
 	 	\hline
 	$1$ & $1000$ & $688.6$ & $572.7$ & \cellcolor{grayL}{$ 16.83\%$} \\
 	$1$ & $2000$ & $1236.5$ & $575.9 $ & \cellcolor{grayL}{$ 53.42\%$}\\
% $1$ & $3000$ & $1111.2$ & $ 578.5$ & 	{$ 47.94\%$} \\
% $1$ & $4000$ & $1553.1$ & $581.4 $ & {$ 62.57\%$} \\
 	$1$ & $5000$ & $2136.0$ & $ 583.7$ & \cellcolor{grayL}{\textbf{72.67\%}} \\
 	\hline
 	$2$ & $1000$ & $680.6$ & $575.1 $ & \cellcolor{grayL}{$ 15.50\%$} 	\\
 	$2$ & $2000$ & $1173.4$ & $ 579.8$ & \cellcolor{grayL}{$ 50.59\%$} \\
% $2$ & $3000$ & $1111.6$ & $582.8 $ & 	{$ 47.57\%$} \\
% 	$2$ & $4000$ & $1552.4$ & $ 588.9$ & {$ 62.07\%$} \\
 	$2$ & $5000$ & $2135.8$ & $596.1 $ & 	\cellcolor{grayL}{\textbf{72.09\%}} \\
 	\hline
 	$3$ & $1000$ & $719.2$ & $ 578.5$ & \cellcolor{grayL}{$ 19.56\%$} 	\\
 	$3$ & $2000$ & $1142.0$ & $582.2 $ & \cellcolor{grayL}{$ 49.02\%$} \\
% $3$ & $3000$ & $1111.3$ & $590.1 $ & {$ 46.90\%$} \\
% $3$ & $4000$ & $1553.7$ & $ 599.8$ & {$ 61.40\%$} \\
 	$3$ & $5000$ & $2136.8$ & $ 605.3$ & \cellcolor{grayL}{\textbf{71.67\%}} \\
 	\hline
 	 	\end{tabular}
% 	$\#$E/N: ratio of edges per node.
%\\``Beaver'': using Beaver triples for SMM.
\end{table}
\subsection{Ablation Study for \texorpdfstring{\osmm}{OSMM}}
We perform extensive experiments 
\label{sec:ablation}
%\textit{with or without \osmm} 
to study the computational, communication, and memory costs saved by \osmm.
Due to saving space, we defer some experimental results to Appendix~\ref{sec::more_exp}.

\paragraph{Communication.} 
\label{sec::smmcomm}
Table~\ref{table:comm_smm} reports communication comparison given varying sparsity of matrices with ${\sim}1000$-$5000$ nodes, each with $\{1, 2, 3\}$ edges on average.
In a training epoch, Beaver triples cost ${\sim}25$-$626$MB for sparse MM, whereas $\prosmm$ spends relatively stable costs of roughly
\mbox{${\sim}1$-$5$MB.}
$\prosmm$ reduces $95\%+$ communication compared with standard MM in all cases.
At best, $\prosmm$ costs only $0.4\%$ communication of standard one when $\#$Node is $5000$ with $1$ edge on average (also the sparsest case in Table~\ref{table:comm_smm}).



%\ak{Maybe can present results only for $1000$, $3000$, and $5000$ nodes (or $1000$, $2000$, and $5000$) to save space as there is nothing surprising for $2000$, $3000$, and $4000$ nodes.
%Similarly for Table~\ref{table:mem_smm_sparse} and~\ref{table:time_smm_net_10}}

\paragraph{Memory Usage.}
Table~\ref{table:mem_smm_sparse} shows how sparsity affects memory usage.
$\#$Node is the total number of nodes and $\#$Edge/Node is the average number of edges connected per node.
Memory usage via Beaver triples scales with $\#$Node, whereas $\prosmm$ maintains relatively stable use.
In detail, $\prosmm$ reduces ${\sim}16\%$-$73\%$ memory for ${\sim}1000$-$5000$ nodes.


\paragraph{Running Time under Varying Network Conditions.}
Table~\ref{table:time_smm_net_10} reports the running time of $10$ epochs of \osmm in the normal, narrow-bandwidth, and high-latency networks.
In the normal network, $\prosmm$ achieves a ${\sim}1.1$-$26.3\times$ speed-up compared with Beaver triples.
In the narrow-bandwidth network, $\prosmm$ is ${\sim}1.53$-$41.96\times$ faster, showing a higher speed-up than the normal network.
In the high-latency network, $\prosmm$ shows a slightly lower speed-up than the normal network.
 The reason is that $\prosmm$ uses more rounds of communication than Beaver triples.
 It would be interesting to explore reducing round complexity for \osmm in the future.

%If we compute the multiplication over a relatively dense matrix in the high-latency network, Beaver triples are slightly better since they require one round only.


\paragraph{Running Time with Varying Dimensionality.}
In practice, feature dimensionality (\eg, salary, life cost) is not very high.
We vary it across $\{10, 20, 50\}$ over the Citeseer dataset in Figures~\ref{fig:time_fea_dim_inference_cite},~\ref{fig:time_fea_dim_train_cite} (results for other datasets are in Appendix~\ref{sec::more_exp}).
%\ref{table:time_fea_dim_infer_cora},\ref{table:time_fea_dim_train_cora},\ref{table:time_fea_dim_infer_pubmed},\ref{table:time_fea_dim_train_pubmed}.
We test both inference and training times.
The fewer feature dimensions, the higher the percentage of costs is from SMM.
Roughly, the time costs have been reduced by ${\sim}50$-$75\%$.



\begin{figure}[!t]
	\centering
	\includegraphics[width = 0.42\textwidth]{./fig_and_tab/inference_time_plot.png}
		\caption{Inference Time with Feature Dimensionality}
 	\label{fig:time_fea_dim_inference_cite}
		%\end{minipage}
\end{figure}
\begin{figure}[!t]
	\centering
	\includegraphics[width = 0.42\textwidth]{./fig_and_tab/training_time_plot_updated.png}
		\caption{Training Time with  Feature Dimensionality}
 	\label{fig:time_fea_dim_train_cite}
		%\end{minipage}
\end{figure}

