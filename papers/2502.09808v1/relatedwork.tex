\section{Related Works}
\label{supp::work}
 
\paragraph{Privacy-Preserving Machine Learning (PPML) with MPC}
In the past decade, PPML has gained great attention as establishing a well-performing neural network often requires massive sensitive data, \eg, human faces, medical records.
%financial statements, or
Cryptography, especially MPC,
%multiparty computation (MPC), 
provides a handy tool to 
%completely 
hide all inputs and intermediate results from adversaries.
%the pure data stream used in machine learning.
%Early-stage research~\cite{icml/bachrach16,ccs/LiuJLA17} starts with secure inference to make encrypted prediction over queries.
Secure computation of various operations~\cite{ndss/ABY15,ccs/ABY318,neurips/crypten2020,ccs/RatheeRKCGRS20,sp21/TanKTW,uss/WatsonWP22,acsac/0021ZCPTLY23,adma/ZhengSDCLZW23} like softmax
%, sigmoid, 
and ReLU has been realized efficiently.
GPU-friendly frameworks/libraries like %GForce~\cite{uss/NgC21}, 
CryptGPU~\cite{sp21/TanKTW} and Piranha~\cite{uss/WatsonWP22} have been also proposed.
They have shown good computational performance in training CNNs.
%large convolutional neural networks such as AlexNet ($60$M parameters) and VGG16 ($138$M parameters).
However, most works are not tailored for %training 
GCNs, especially those computation over large and sparse structures.

\paragraph{Secure (Dense) Matrix Multiplication}
Classical secret-sharing schemes produce secret-shares to dense matrix multiplication.
Recent works~\cite{sp/MohasselZ17,asiaccs/MonoG23,asiacrypt/0030KRRSW20,ccs/JiangKLS18} designed more efficient protocols to reduce communication costs.
Yet, directly adapting these to sparse structures still results in high memory/communication costs asymptotically growing with the matrix size.
Large communication overhead persists as a major concern in PPML, \eg, consuming $94\%$ of the training time of Piranha~\cite{uss/WatsonWP22}.
Even worse, large matrix computations are not supported due to memory overflow.
So, minimizing communication costs and memory usage of \osmm is crucial.

Standard matrix decomposition methods, such as singular value decomposition (SVD)~\cite{banerjee2014linear} and LU decomposition (LUD)~\cite{bunch1974triangular}, are designed for faster plaintext computations rather than reducing communication in secure MPC.
Thus, employing these decompositions in secure GCN does not significantly lower communication costs.
Specifically, SVD decomposes a matrix into dense and diagonal matrices, while LUD decomposes it into triangular matrices. 
They both require $O(\numnode^2)$ communication for secure multiplications in GCNs.
%Standard matrix decomposition or transformation (\eg, LUD~\cite{bunch1974triangular}, SVD~\cite{banerjee2014linear}) are employed~\cite{iacr/BoumanV18,uss/DuanYCZ10} for fast computation rather than reducing communication costs of matrix multiplication.
% seems iacr/BoumanV18, uss/DuanYCZ10 are about secure LUD and SVD protocols and are not that related to our work
%Given this, we propose a 
Our decomposition approach instead adapts the graph topology into a sequence of linear transformations to exploit the sparsity, finally achieving $O(\numedge)$ communication.
%We focus on the non-zero products only in sparse matrix computation to achieve $O(\numedge)$ communication.
%avoiding the linearly increasing costs on the matrix test both the inference time and training time with varying feature dimensions.

\paragraph{Sparsity Exploration in MPC} 
Exploiting the sparsity in plaintext can speed up the computation.
Directly encoding the input sparse matrices into random matrices for acquiring privacy destroys the %original 
sparsity~\cite{isit/XhemrishiBW22}.
Several recent works~\cite{isit/XhemrishiBW22,tifs/BitarEWX24} studied the secret-shared sparse matrices and their multiplication by bridging the trade-offs between sparsity and privacy.
Specifically, they relax the privacy constraint by focusing on multiplying a secret-shared matrix with a public matrix.
Unlike their works, we work in the classical MPC settings, where all inputs are secretly shared. We also represent the sparsity through algebra relations (without destroying the sparsity).

ROOM~\cite{ccs/SchoppmannG0P19} presents three instantiations of sparse matrix-vector multiplications optimized for different sparsity settings, such as row- and column-sparse
%adjacency 
matrices.
%Unlike the restriction of ROOM regarding sparsity types, we aim for arbitrary sparsity types in adjacency matrices.
%Specifically, 
Our decomposition works on arbitrary-sparse matrices, in contrast to either row- or column-sparse as in ROOM~\cite{ccs/SchoppmannG0P19}, hence eliminating the need to know the sparsity types for the input matrices.
Chen~\etal~\cite{kdd/0001ZWWFTWLWH21} realize \osmm 
%in the 2PC setting 
by sending a homomorphically encrypted dense matrix to the party holding the sparse matrix to perform ciphertext multiplication and split the result into secret shares.
The limited support of homomorphic multiplication curbs 
%the efficiency of 
this approach.

\paragraph{Relevant Primitives for \osmm}
%\paragraph{Oblivious Permutation.}
Using oblivious shuffle to realize \osmm demands $O(\numnode)$ rounds of $\ssp$.
We aim to hide the corresponding permutations directly in $O(1)$ rounds 
%derived from the original adjacency matrix 
without using oblivious shuffle~\cite{ndss/SongYBDC23,uss/JiaSZDG22} or sorting~\cite{ccs/AsharovHIKNPTT22}.
OLGA~\cite{ccs/AttrapadungH0MO21} achieves $\ssp$ as a special case of linear group action.
Its subsequent work~\cite{ccs/AsharovHIKNPTT22} also uses 
replicated secret sharing.
In $\cgnn$, private permutation operation is owned by the graph holder, leading to better efficiency as in Table~\ref{table:op_comm}.
Zou~\etal \cite{ccs/ZouLSLXX24} design a permutation protocol by packing the permutation-relied computation into offline to optimize the online communication.
Differently, we make the offline phase independent of derived permutation, thus promisingly enabling varying graph setting (a.k.a inductive training~\cite{nips/WuYY21}).
%\todo[inline]{ccs/JiangKLS18?} is dense matrix multiplication and costs $O(3n)$ HE-multiplications; while we use O(e) ss-mult.
% n* n^2 storage (3n^2 for their example) as Figure 1; our setting is huge SMM with huge sparsity.
%Her optimization is for dense and small matrix computation.

Another primitive of Oblivious Selection-Multiplication ($\SM$) is to obliviously indicate whether message passing exists in an edge.
Previous works like Multiplexer~\cite{ccs/RatheeRKCGRS20} and binary-arithmetic multiplication~\cite{ndss/ABY15} can be adopted to realize the $\SM$'s functionality.
Multiplexer~\cite{ccs/RatheeRKCGRS20}, realized by two instances of $1$-out-of-$2$ OT, requiring $2(2\bitlen + 1)$ bits.
%Binary-arithmetic multiplication~\cite{ccs/ABY318} differs from standard arithmetic multiplication~\cite and is typically used to compute the piecewise linear approximation of non-linear functions.
Binary-arithmetic conversion communicates $\bitlen(\bitlen+1)/2$ bits for a $64$-bit data. 
However, $\cgnn$'s $\SM$ protocol requires $\bitlen+1$ bits online in $1$ round using secret sharing (free of logarithm rounds of combining OT).

%The $\SM$ protocol avoids extra communication rounds and costs incurred by transforming Boolean-share to arithmetic-share.

\begin{table}[!t]
\centering
\caption{MPC Frameworks for Secure Graph Learning}
\setlength\tabcolsep{2pt}
\begin{tabular}{|l|r|c|c|c|}
\hline
\multicolumn{1}{|c|}{\textbf{Framework}} & \textbf{Scenario} & \textbf{Inference} & \textbf{Training} & \textbf{Security} 
\\ \hline\hline
 OblivGNN~\cite{uss/XuL0AYY24}	 	 & MLaaS	 & $\checkmark$ 	 & $\times$	 & Semi-honest  \\\hline
LinGCN~\cite{nips/PengRLZHTGWXWD23} 	 & MLaaS	 & $\checkmark$ 	 & $\times$	 & Semi-honest  \\\hline
Penguin~\cite{nips/RanXLWQW23} 	 & MLaaS	 & $\checkmark$ 	 & $\times$	 & Semi-honest  \\\hline
 CoGNN~\cite{ccs/ZouLSLXX24} 	 & Horizontal 	 & $\checkmark$ 	 & $\checkmark$ & Semi-honest\\\hline
\cgnn 	 	 & Vertical	 	 & $\checkmark$ 	 	 & $\checkmark$ 	 & Semi-honest 	 \\
\hline
\end{tabular}
\label{tab:compare_sec_graph}
\end{table}


\paragraph{Secure Graph Analysis.} 
Secure graph analysis~\cite{sp/NayakWIWTS15,ccs/Araki0OPRT21} can be adopted for \osmm by reversing the graph analysis process (depicted by arrows in Figure~\ref{graph_psigqx_diff}).
Building on  garbled circuits, GraphSC~\cite{sp/NayakWIWTS15} uses an oblivious sorting to enable secure graph computation for message-passing algorithms.
Garbled circuits, while providing constant round complexity, are known for their communication and computation-intensive costs.
To address this, Araki~\etal~\cite{ccs/Araki0OPRT21} improve it by replacing sorting with  shuffling in the message-passing phase %, operating on $(n+e)$-dimensional vectors, using 
and use secret-sharing-based techniques to reduce the costs of communication and computation.
%using MPC techniques like 
%Their scatter/gather instantiations adopt 
%garbled circuits over the secret-shared graph and node data.
Recently,  Graphiti~\cite{ccs/KotiKPG24}, an advancement over GraphSC, optimizes the round complexity independently of graph size, enhancing scalability for large graphs.

In contrast to aforementioned MPC-based partitioning settings, $\cgnn$ adopts a practical vertical partitioning approach.
%, where the graph structure is held by one party, and the associated node features are managed by another.
%This vertically partitioned scenario can be directly extended to real-world scenarios where  node features are often distributed across different entities.
$\cgnn$ is designed to optimize MPC protocols specifically for vertically partitioned data, incorporating novel sparsity decomposition techniques, as well as new permutation and selection multiplication protocols. 
These innovations allow \cgnn to yield the optimal round complexity  for \osmm and be independent of the number of graph size, thus highlighting the potential  to scale and train on massive graphs under real-world network conditions.
%Rather than adopting oblivious sorting, \cgnn  is based on   independent .
%To our knowledge, we are the first to 

 

\paragraph{Cryptographic Learning over Graphs.}
Table~\ref{tab:compare_sec_graph} summarizes recent advances for cryptographic graph learning. 
SecGNN~\cite{tsc/WangZJ23} is the first try to realize secure GCN training %framework 
by integrating existing PPML advances.
%over three non-colluding servers holding $2$-out-of-$2$ additive secret shares of the graphs.
Efficient GCN training still remains barren nowadays.
Without the customized MPC protocol for \osmm, %sparse structures, 
SecGNN suffers from high communication costs in practice.
%($1$GB for $1$-epoch training)
As for federated training, both vertical and horizontal partitions of distributed data are vital for practical usage.
Very recently, CoGNN~\cite{ccs/ZouLSLXX24} considers a collaborative training setting where each pair of computing parties knows the sub-graphs for secure training.
$\cgnn$ considers that one party who knows the graph but not the associated data, which is vertically partitioned.

Another branch of works~\cite{nips/PengRLZHTGWXWD23,nips/RanXLWQW23,nips/RanWGYXW22,uss/XuL0AYY24} adopt machine learning as a service (MLaaS) to realize secure GCN inference.
Penguin~\cite[Table 3]{nips/RanXLWQW23}, as the-state-of-art work, largely reduces the inference latency~\cite{uss/JuvekarVC18} by $5.9\times$ over the Cora dataset~\cite{aim/SenNBGGE08}, finally reaching $10$ minutes for the inference.
OblivGNN~\cite{uss/XuL0AYY24} recently reduces it to about $2$ minutes.
Unlike secure inference for MLaaS, $\cgnn$ made a noticeable step of secure training by reducing communication costs to $114$MB in roughly $20$s over the same dataset.


Many works focus on different privacy (DP) guarantees% of GCNs
~\cite{tnn/WuPCLZY21,ccs/SajadmaneshG21,pvldb/PatwaSGMR23},
%including open-source benchmark~\cite{corr/abs-2104-07145},
%local DP approach to perturb and compress node features~\cite{ccs/SajadmaneshG21},
or %application of 
applying HE and private information retrieval for secure social recommendation~\cite{nips/CuiCLYW21}.
Like some prior works, sparsity is also exploited.
Their technical contributions differ vastly from ours, for we consider %securing GCN in 
the MPC settings.