%!TEX root = gcn.tex
 
\section{Algorithm Realization}
\label{sec::alg}

Guided by Theorem~\ref{the::p_sig_q}, our algorithm of decomposing the adjacency matrix $\Asf$ can be implemented as below.
$\Asf$ is a \texttt{SparseMatrix} consists of all $0$-$1$ elements represented by coordinates of nonzero values,
\ie, [(\texttt{row-index}, \texttt{column-index}), (\texttt{row-index}, \texttt{column-index}), $\ldots$].
In Figure~\ref{fig::code_psigq_de}, we draw the pairs of (row-index, column-index) of $\Asf$, \eg, $[(2, 3), (3, 1), \ldots]$.
Step~1 is to split the pairs in $\Asf$ to construct two sparse matrices (Graph language in Figure~\ref{fig::nen_relation_diff}), called $\Psf'$ and $\Qsf'$ (correspond to $\adjout$ and $\adjin$ in Section~\ref{subsec::sme}).
In Steps~2 and~3, the row-indices of $\Psf'$ are sorted and generate $\sigma_3^{\Psf}$, and the column-indices of $\Qsf'$ are sorted and generate $\sigma_3^{\Qsf}$.
Then, $\sigma_3$ is obtained by sparse-matrix multiplying $\sigma_3^{\Psf}$ and $\sigma_3^{\Qsf}$, thus $\sigma_3=\sigma_3^{\Psf}\cdot\sigma_3^{\Qsf}$.
%\yuz{done}
Now, we can see that the resulting matrices $\Psf$ and $\Qsf$ (correspond to $\adjout'$ and $\adjin'$ in Section~\ref{subsec::initdecom}) have monotonically non-decreasing row/column indices.
Moreover, $\Psf$ contain exactly one $1$ in every column and $\Qsf$ contains exactly one $1$ in every row.
% monotonically non-decreasing (1, 1, 1, 1, 2) is not strictly equal to monotonically increasing (1,2,3,4,5).
In summary, the pseudocode of \texttt{decompose\_row\_column(A)} below describes the extraction of $\Psf$, $\sigma_3$ and $\Qsf$ as previously displayed in Figure~\ref{graph_psigq}.

% \vspace{2mm}
% \noindent
% \begin{frame}{\texttt{def decompose\_row\_column(A):}}
%	\\\texttt{\#A: a list contains \{(i, j): a\_{\{i,j\}}=1\}}
% \\\indent\texttt{P = A[:, 0]}
% \\\indent\texttt{Q\_trans = A[:, 1]}
% \\\indent\texttt{sigma3 = tf.argsort(Q\_trans)}
% \\\indent\texttt{sigma3 = Permutations(sigma3)}
% \\\indent\texttt{inv\_sigma3 = $\sim$sigma3}
% \\\indent\texttt{Q\_trans = inv\_sigma3.act(Q\_trans)}
% \\\indent\texttt{return P, sigma3, Q\_trans}
% \end{frame}	

\begin{figure}[!t]
%	\vspace{-5mm}
	\centering
	\includegraphics[width = 0.47\textwidth]{./fig_and_tab/code_psq.png}
	\caption{Illustration of $\Psf\sigma_3\Qsf$ Decomposition}
	 \label{fig::code_psigq_de}
		%\end{minipage}
\end{figure}



\noindent
\begin{frame}{\texttt{def decompose\_row\_column(A):}}
 \\\texttt{\#A: a list of (row\_id, col\_id) for non-zero element in sparse matrix}
\\\indent\texttt{P=[(A[i, 0], i) for i in range(len(A))]}
\\\indent\texttt{Q=[(i, A[i, 1]) for i in range(len(A))]}
\\\indent\texttt{P=sorted(P, key=lambda x: x[0])}
\\\indent\texttt{Q=sorted(Q, key=lambda x: x[1])}
\\\indent\texttt{P=[(P[i,0], i) for i in range(len(A))]}
\\\indent\texttt{sigma3P=[(i, P[i,1]) for i in range(len(A))]}
\\\indent\texttt{sigma3Q=[(Q[i,0], i) for i in range(len(A))]}
\\\indent\texttt{Q=[(i,Q[i,1]) for i in range(len(A))] }
\\\indent\texttt{sigma3 = sigma3P*sigma3Q}
\\\indent\texttt{return P, sigma3, Q}
\end{frame}	


\noindent\textbf{Pseudocode of re-decomposition}.
%Compared with the difference of coordinates, we extract $\ncol$, $\sigma_1$, and $\sigma_2$ one by one from $\Qsf$.
In Figure~\ref{fig::code_q_compos}, we re-draw $\Qsf$ and describe its decomposition.
Step~1 extracts the unique column indices using $\text{set}$ function in Python, and their quantity is $\ncol$.
Then, the corresponding row indices are extracted by comparing whether the neighboring column indices are identical.
Step~2 constructs $\sigma_1$ and $\sigma_2$ by keeping the first $\ncol$ elements and padding the elements in numerical order to a permutation in $\sigma_1\in\Sbb_n,\sigma_2\in\Sbb_t$.
The code of decomposing $\Qsf$ is outlined below.
To derive $\sigma_4$, $\nrow$, and $\sigma_5$, the $\Psf$-type matrix decomposition follows a similar logic.

\begin{figure}[!t]
%	\vspace{-5mm}
	\centering
	\includegraphics[width = 0.42\textwidth]{./fig_and_tab/code_qx.png}
	\caption{Illustration of $\Qsf$ Decomposition}
	 \label{fig::code_q_compos}
		%\end{minipage}
\end{figure}

\begin{frame}{\texttt{def decompose\_Q(Q, e, n):}}
% \\\indent\texttt{assert len(Q)=e}
% \\\indent\texttt{assert Q[i:0]=i for i in range(e)}
 \\\indent\texttt{unique\_col\_ids = set(Q[:,1])} 
 \\\indent\texttt{step\_row\_ids=[Q[i,0] for Q[i, 1)!=Q[i-1, 1) or i=0]}
 \\\indent\texttt{k2 = len(unique\_col\_ids)}
 \\\indent\texttt{sigma1 = [ (i, unique\_col\_ids[i]	) for i in range(k2)]}
 \\\indent\texttt{sigma2 = [(step\_row\_ids[i], i) for i in range(k2)] }
\\\indent\texttt{sigma1=pad\_perm(sigma1, n)} 
\\\indent\texttt{sigma2=pad\_perm(sigma2, e)} 
\\\indent\texttt{return sigma2, k2, sigma1}
\end{frame}
 
 
The \texttt{class PrivateSparseMatrix} below contains the realization of \osmm protocols.
Before secure training, the graph owner locally decomposes $\Asf$ into $\Psf$, $\Qsf$, and then into the corresponding basic operations.
%before secure training with another party.
\osmm let $\pp_0$ and $\pp_1$ jointly execute secure multiplications on $\Psf$ and $\Qsf$, and $\ssp$ on $\sigma_3$.
%, which boils down to $\ssp$ and secure multiplication essentially.

\vspace{2mm}
\noindent
\begin{frame}{\texttt{class PrivateSparseMatrix:}}
\\\indent\texttt{def \_\_init\_\_(self, A ...):}
\\\indent\indent\texttt{self.owner = get\_device()}
\\\indent\indent\texttt{with tf.device(self.owner):}
\\\indent\indent\indent\texttt{P, s3, Q = decompose\_row\_column()}
\\\indent\indent\indent\texttt{s5, k4, s4 = decompose\_P()}
\\\indent\indent\indent\texttt{s2, k2, s1 = decompose\_Q()}
\\\indent\texttt{def sm\_2(self, x: Union[PrivateTensor,} \\\indent\indent\indent\indent\ \indent\indent\indent\indent\texttt{SharedPair]):}
\\\indent\indent\texttt{Qx = Q\_mult(s2, k2, s1, e, x)}
\\\indent\indent\texttt{x3 = s3.act(Qx)}
\\\indent\indent\texttt{Ax = P\_mult(s5, k4, s4, x3, m)}
\\\indent\indent\texttt{return Ax}

\end{frame}

{Class of Secure GCN.}
The implementation of \cgnn follows the plaintext-training repository (\url{https://github.com/dmlc/dgl}) in the transductive setting.
Accordingly, the graph decomposition can be performed once with the fixed graph before secure training.
The \texttt{class SGCN} inherits the conventional \texttt{NN} (the template of neural network).
Secure GCN training is thus composed of secure graph convolution and secure activation layers.
%, in particular.
The $\Asf\Xsf$ in graph convolution layers is realized by the $\Pi_\smm$ protocol.
Below, we extract the code of implementing the \texttt{class SGCN} with respect to the plaintext GCN.
%model~\cite{iclr/KipfW17}.
Notably, all the inputs \texttt{feature}, \texttt{label}, and \texttt{adj\_matrix} are secret shares in the form of fixed-pointed numbers.
The functions \texttt{GraphConv, ReLU, SoftmaxCE} are the MPC protocols executed by two parties.


\begin{frame}{\texttt{class SGCN(NN):}}
\\\texttt{def \_\_init\_\_(self, feature: PrivateTensor, 
\\\indent\indent\indent\indent\indent\indent label: PrivateTensor, 
\\\indent\indent\indent\indent\indent\indent dense\_dims: List[int],
\\\indent\indent\indent\indent\indent\indent adj\_matrix: Union[...],
\\\indent\indent\indent\indent\indent\indent train\_mask, loss=...):}
\\\indent\texttt{super(SGCN, self).\_\_init\_\_()}
\\\indent\texttt{layer = Input(dim, feature)}
\\\indent\texttt{self.addLayer(layer)}
\\\indent\texttt{input\_layers = [layer]}
\\\indent\texttt{for i in range(1, len(dense\_dims)):}
\\\indent\indent\texttt{layer = GraphConv()}
\\\indent\indent\texttt{self.addLayer(ly=layer)}
\\\indent\indent\texttt{if i < len(dense\_dims) - 1:}
\\\indent\indent\indent\texttt{layer = ReLU()} 
\\\indent\indent\indent\texttt{self.addLayer(ly=layer)}
\\\indent\texttt{layer\_label = Input(dim, label)}
\\\indent\texttt{self.addLayer(layer\_label)}
\\\indent\texttt{if loss == "SoftmaxCE":} 
\\\indent\indent\texttt{layer\_loss = SoftmaxCE()}
\\\indent\indent\texttt{self.addLayer(ly=layer\_loss)}
\\\indent\texttt{else:}
\\\indent\indent\texttt{...\# use other layer/loss}
\end{frame}

