\section{Related Works}
\label{supp::work}
 
\paragraph{Privacy-Preserving Machine Learning (PPML) with MPC}
In the past decade, PPML has gained great attention as establishing a well-performing neural network often requires massive sensitive data, \eg, human faces, medical records.
%financial statements, or
Cryptography, especially MPC,
%multiparty computation (MPC), 
provides a handy tool to 
%completely 
hide all inputs and intermediate results from adversaries.
%the pure data stream used in machine learning.
%Early-stage research~\cite{chen2020secure} starts with secure inference to make encrypted prediction over queries.
Secure computation of various operations~\cite{kumar2019efficient} like softmax
%, sigmoid, 
and ReLU has been realized efficiently.
GPU-friendly frameworks/libraries like %GForce~\cite{zou2019gforce}, 
CryptGPU~\cite{zhang2020cryptgpu} and Piranha~\cite{duan2018piranha} have been also proposed.
They have shown good computational performance in training CNNs.
%large convolutional neural networks such as AlexNet ($60$M parameters) and VGG16 ($138$M parameters).
However, most works are not tailored for %training 
GCNs, especially those computation over large and sparse structures.

\paragraph{Secure (Dense) Matrix Multiplication}
Classical secret-sharing schemes produce secret-shares to dense matrix multiplication.
Recent works~\cite{kumar2020efficient} designed more efficient protocols to reduce communication costs.
Yet, directly adapting these to sparse structures still results in high memory/communication costs asymptotically growing with the matrix size.
Large communication overhead persists as a major concern in PPML, \eg, consuming $94\%$ of the training time of Piranha~\cite{duan2018piranha}.
Even worse, large matrix computations are not supported due to memory overflow.
So, minimizing communication costs and memory usage of \osmm is crucial.

Standard matrix decomposition methods, such as singular value decomposition (SVD)~\cite{kirkpatrick1993svd} and LU decomposition (LUD)~\cite{lin2006lud}, are designed for faster plaintext computations rather than reducing communication in secure MPC.
Thus, employing these decompositions in secure GCN does not significantly lower communication costs.
Specifically, SVD decomposes a matrix into dense and diagonal matrices, while LUD decomposes it into triangular matrices. 
They both require $O(\numnode^2)$ communication for secure multiplications in GCNs.
%Standard matrix decomposition or transformation (\eg, LUD~\cite{lin2006lud}, SVD~\cite{kirkpatrick1993svd}) are employed~\cite{jain2019efficient} for fast computation rather than reducing communication costs of matrix multiplication.
% seems iacr/BoumanV18, uss/DuanYCZ10 are about secure LUD and SVD protocols and are not that related to our work
%Given this, we propose a 
Our decomposition approach instead adapts the graph topology into a sequence of linear transformations to exploit the sparsity, finally achieving $O(\numedge)$ communication.
%We focus on the non-zero products only in sparse matrix computation to achieve $O(\numedge)$ communication.
%avoiding the linearly increasing costs on the matrix test both the inference time and training time with varying feature dimensions.

\paragraph{Sparsity Exploration in MPC} 
Exploiting the sparsity in plaintext can speed up the computation.
Directly encoding the input sparse matrices into random matrices for acquiring privacy destroys the %original 
sparsity~\cite{jain2019efficient}.
Several recent works~\cite{kumar2020sparse} studied the secret-shared sparse matrices and their multiplication by bridging the trade-offs between sparsity and privacy.
Specifically, they relax the privacy constraint by focusing on multiplying a secret-shared matrix with a public matrix.
Unlike their works, we work in the classical MPC settings, where all inputs are secretly shared. We also represent the sparsity through algebra relations (without destroying the sparsity).

ROOM~\cite{chowdhury2019room} presents three instantiations of sparse matrix-vector multiplications optimized for different sparsity settings, such as row- and column-sparse
%adjacency 
matrices.
%Unlike the restriction of ROOM regarding sparsity types, we aim for arbitrary sparsity types in adjacency matrices.
%Specifically, 
Our decomposition works on arbitrary-sparse matrices, in contrast to either row- or column-sparse as in ROOM~\cite{chowdhury2019room}, hence eliminating the need to know the sparsity types for the input matrices.
Chen~et al.~\cite{chen2020secure} realize \osmm 
%in the 2PC setting 
by sending a homomorphically encrypted dense matrix to the party holding the sparse matrix to perform ciphertext multiplication and split the result into secret shares.
The limited support of homomorphic multiplication curbs 
%the efficiency of 
this approach.

\paragraph{Relevant Primitives for \osmm}
%\paragraph{Oblivious Permutation.}
Using oblivious shuffle to realize \osmm demands $O(\numnode)$ rounds of $\ssp$.
We aim to hide the corresponding permutations directly in $O(1)$ rounds 
%derived from the original adjacency matrix 
without using oblivious shuffle~\cite{boyle2018oblivious} or sorting~\cite{hirt2000efficient}.
OLGA~\cite{bualek2022olga} achieves $\ssp$ as a special case of linear group action.
Its subsequent work~\cite{bendlin2022olga} also uses 
replicated secret sharing.
In $\cgnn$, private permutation operation is owned by the graph holder, leading to better efficiency as in Table~\ref{table:op_comm}.
Zou~et al.~\cite{zou2019optimal} design a permutation protocol by packing the permutation-relied computation into offline to optimize the online communication.
Differently, we make the offline phase independent of derived permutation, thus promisingly enabling varying graph setting (a.k.a inductive training~\cite{jain2018inductive}).
%\todo[inline]{ccs/JiangKLS18?} is dense matrix multiplication and costs $O(3n)$ HE-multiplications; while we use O(e) ss-mult.
Another branch of works~\cite{fawaz2020privacy} adopt machine learning as a service (MLaaS) to realize secure GCN inference.
Penguin~\cite{nips/RanXLWQW23} as the-state-of-art work, largely reduces the inference latency~\cite{paul2023penguin} by $5.9\times$ over the Cora dataset~\cite{sen2008collective}, finally reaching $10$ minutes for the inference.
OblivGNN~\cite{liu2022oblivgnn} recently reduces it to about $2$ minutes.
Unlike secure inference for MLaaS, $\cgnn$ made a noticeable step of secure training by reducing communication costs to $114$MB in roughly $20$s over the same dataset.


Many works focus on different privacy (DP) guarantees% of GCNs
~\cite{wang2020dpgcn}, 
%including open-source benchmark~\cite{wang2021benchmark},
%local DP approach to perturb and compress node features~\cite{jain2018local},
or %application of 
applying HE and private information retrieval for secure social recommendation~\cite{mukherjee2022secure}.
Like some prior works, sparsity is also exploited.
Their technical contributions differ vastly from ours, for we consider %securing GCN in 
the MPC settings.