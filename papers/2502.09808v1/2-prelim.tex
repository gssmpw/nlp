%!TEX root = gcn.tex
\section{Preliminary}

\paragraph{Notations.}
Table~\ref{tab:notation} summarizes the main notations.
$\Abb$ denotes an Abelian group.
$\Sbb_n$ denotes a permutation group of $n$ elements.
$\Mbb_{m,n}(\Rcal)$ denotes a matrix ring, which defines a set of $m\times n$ matrices with entries in a ring $\Rcal$, forming a ring under matrix addition and
%matrix 
multiplication.
$\Msf_{m \times n}=(\Msf[i,j])_{i,j=1}^{m,n}$ denotes an $m\times n$ matrix\footnote{
For simplicity, we omit the subscript of 
$\Msf_{m \times n}$ when the values of $m$ and $n$ are clear from the context.
Also, we write 
$\Msf = (\Msf[i, j])_{i, j = 1}^{n}$ if $m = n$.
} 
where row indices are $\{1, 2, \ldots, m\}$ and column indices are $\{1, 2, \ldots, n\}$, and $\Msf[i,j]$ is the value at the $i$-th row and $j$-th column.
$\Pi(\ ;\ )$ denotes a protocol execution between two parties, $\pp_0$ and $\pp_1$, where $\pp_0$'s inputs are the left part of `;' and $\pp_1$'s inputs are the right part of `;'.

\paragraph{Secret Sharing.} 
We use $2$-out-of-$2$ additive secret sharing over a ring, where
the floating-point
%input 
values are encoded to be fixed-point numbers $x\in \Zbb/2^f\Zbb$, with $L = 64$ bits representing decimals and $f = 18$ bits representing the fractional part~\cite{sp/MohasselZ17}.
Specifically, one party $\pp_0$ holds the share $\l x\r _0\in\Zbb $, while the other party $\pp_1$ holds the share $\l x\r_1\in\Zbb$ such that $x\cdot 2^f = \l x\r_0 + \l x\r_1$.
The shares can be arithmetic or binary.


%$e$ &The degree of the permutation group
%$\Abb$& Abelian group

\paragraph{Graph Convolutional Networks.}
GCN~\cite{iclr/KipfW17} has been proposed for training over graph data, using graph structure and node features as input.
Like most neural networks, GCN consists of multiple linear and non-linear layers.
Compared to CNN, GCN replaces convolutional layers with graph convolution layers
%to learn the graph topology 
(more details on GCN architecture and its training/inference are in Section~\ref{sec::secgcn}).
Graph convolution can be computed by SMM, often yielding many $0$-value results.

Let $\adjmat \in \Mbb_{\numnode,\numnode}(\Rcal)$ be a (normalized) adjacency matrix of a graph with $\numnode$ nodes and $\Xsf \in \Mbb_{\numnode, d}(\Rcal)$ be the feature matrix (with dimensionality $d$) of the nodes.
The graph convolution layer (with output dimensionality $k$) is defined as $\mathsf{Y} = \adjmat \feamat \weimat$,
where $\mathsf{Y} \in \Mbb_{\numnode, k}(\Rcal)$ is the output and $\weimat \in \Mbb_{d, k}(\Rcal)$ is a trainable parameter.
As matrix multiplication costs increase linearly with input size and $k \ll \numnode$ in practice, the challenge of secure GCN
%training 
lies in the
%secure 
SMM of $\adjmat \feamat$.
Multiplying (dense) $\weimat$ can be done using Beaver's approach.

\begin{table}[!t]
\centering
\caption{Notation and Definition}
\label{tab:notation}
\setlength\tabcolsep{2pt}
\begin{tabular}{l|l}
\hline
%\textbf{Notation} & \textbf{Definition}
%\\\hline
$\pp_i, \l \ \r_i$& Party $i$ and its share ($i \in \{0, 1\})$
\\\hline
$\bitlen$ & The bit-length of data
\\\hline
$\pi,b,\uvec,\l u \r$ & Pre-computed randomnesses
%$\pi,b,\uvec,u$ & \multirow{2}{*}{Offline-generated randomness (\eg., vectors, values)}
\\\hline
$\delta_x$ & Masked version of value/vector $x$
\\\hline
$\Mbb_{m, n}{(\Rcal)}$ & A set of ${m\times n}$ matrices with entries in a ring $\Rcal$
\\\hline
$\Msf_{m,n}$ & A matrix $\Msf$ of size $m\times n$
\\\hline
$\Msf[i, j]$ & The value of $\Msf$ at the $i$-th row and $j$-th column
\\\hline
$\sigma\xvec$& Permutation operation $\sigma$ over a matrix/vector $\xvec$
\\\hline
$\Sbb_n$ & Permutation group of $n$ elements
\\\hline
\end{tabular}
\end{table}
