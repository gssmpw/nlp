%!TEX root = gcn.tex
\section{Introduction}
Graphs, representing structural data and topology, are widely used across various domains, such as social networks and merchandising transactions.
Graph convolutional networks (GCN)~\cite{iclr/KipfW17} have significantly enhanced model training on these interconnected nodes.
However, these graphs often contain sensitive information that should not be leaked to untrusted parties.
For example, companies may analyze sensitive demographic and behavioral data about users for applications ranging from targeted advertising to personalized medicine.
Given the data-centric nature and analytical power of GCN training, addressing these privacy concerns is imperative.

Secure multi-party computation (MPC)~\cite{crypto/ChaumDG87,crypto/ChenC06,eurocrypt/CiampiRSW22} is a critical tool for privacy-preserving machine learning, enabling mutually distrustful parties to collaboratively train models with privacy protection over inputs and (intermediate) computations.
While research advances (\eg,~\cite{ccs/RatheeRKCGRS20,uss/NgC21,sp21/TanKTW,uss/WatsonWP22,icml/Keller022,ccs/ABY318,folkerts2023redsec}) support secure training on convolutional neural networks (CNNs) efficiently, private GCN training with MPC over graphs remains challenging.

Graph convolutional layers in GCNs involve multiplications with a (normalized) adjacency matrix containing $\numedge$ non-zero values in a $\numnode \times \numnode$ matrix for a graph with $\numnode$ nodes and $\numedge$ edges.
The graphs are typically sparse but large.
One could use the standard Beaver-triple-based protocol to securely perform these sparse matrix multiplications by treating graph convolution as ordinary dense matrix multiplication.
However, this approach incurs $O(\numnode^2)$ communication and memory costs due to computations on irrelevant nodes.
%
Integrating existing cryptographic advances, the initial effort of SecGNN~\cite{tsc/WangZJ23,nips/RanXLWQW23} requires heavy communication or computational overhead.
Recently, CoGNN~\cite{ccs/ZouLSLXX24} optimizes the overhead in terms of  horizontal data partitioning, proposing a semi-honest secure framework.
Research for secure GCN over vertical data  remains nascent.

Current MPC studies, for GCN or not, have primarily targeted settings where participants own different data samples, \ie, horizontally partitioned data~\cite{ccs/ZouLSLXX24}.
MPC specialized for scenarios where parties hold different types of features~\cite{tkde/LiuKZPHYOZY24,icml/CastigliaZ0KBP23,nips/Wang0ZLWL23} is rare.
This paper studies $2$-party secure GCN training for these vertical partition cases, where one party holds private graph topology (\eg, edges) while the other owns private node features.
For instance, LinkedIn holds private social relationships between users, while banks own users' private bank statements.
Such real-world graph structures underpin the relevance of our focus.
To our knowledge, no prior work tackles secure GCN training in this context, which is crucial for cross-silo collaboration.


To realize secure GCN over vertically split data, we tailor MPC protocols for sparse graph convolution, which fundamentally involves sparse (adjacency) matrix multiplication.
Recent studies have begun exploring MPC protocols for sparse matrix multiplication (SMM).
ROOM~\cite{ccs/SchoppmannG0P19}, a seminal work on SMM, requires foreknowledge of sparsity types: whether the input matrices are row-sparse or column-sparse.
Unfortunately, GCN typically trains on graphs with arbitrary sparsity, where nodes have varying degrees and no specific sparsity constraints.
Moreover, the adjacency matrix in GCN often contains a self-loop operation represented by adding the identity matrix, which is neither row- nor column-sparse.
Araki~\etal~\cite{ccs/Araki0OPRT21} avoid this limitation in their scalable, secure graph analysis work, yet it does not cover vertical partition.

% and related primitives
To bridge this gap, we propose a secure sparse matrix multiplication protocol, \osmm, achieving \emph{accurate, efficient, and secure GCN training over vertical data} for the first time.

\subsection{New Techniques for Sparse Matrices}
The cost of evaluating a GCN layer is dominated by SMM in the form of $\adjmat\feamat$, where $\adjmat$ is a sparse adjacency matrix of a (directed) graph $\graph$ and $\feamat$ is a dense matrix of node features.
For unrelated nodes, which often constitute a substantial portion, the element-wise products $0\cdot x$ are always zero.
Our efficient MPC design 
avoids unnecessary secure computation over unrelated nodes by focusing on computing non-zero results while concealing the sparse topology.
We achieve this~by:
1) decomposing the sparse matrix $\adjmat$ into a product of matrices (\S\ref{sec::sgc}), including permutation and binary diagonal matrices, that can \emph{faithfully} represent the original graph topology;
2) devising specialized protocols (\S\ref{sec::smm_protocol}) for efficiently multiplying the structured matrices while hiding sparsity topology.


 
\subsubsection{Sparse Matrix Decomposition}
We decompose adjacency matrix $\adjmat$ of $\graph$ into two bipartite graphs: one represented by sparse matrix $\adjout$, linking the out-degree nodes to edges, the other 
by sparse matrix $\adjin$,
linking edges to in-degree nodes.

%\ie, we decompose $\adjmat$ into $\adjout \adjin$, where $\adjout$ and $\adjin$ are sparse matrices representing these connections.
%linking out-degree nodes to edges and edges to in-degree nodes of $\graph$, respectively.

We then permute the columns of $\adjout$ and the rows of $\adjin$ so that the permuted matrices $\adjout'$ and $\adjin'$ have non-zero positions with \emph{monotonically non-decreasing} row and column indices.
A permutation $\sigma$ is used to preserve the edge topology, leading to an initial decomposition of $\adjmat = \adjout'\sigma \adjin'$.
This is further refined into a sequence of \emph{linear transformations}, 
which can be efficiently computed by our MPC protocols for 
\emph{oblivious permutation}
%($\Pi_{\ssp}$) 
and \emph{oblivious selection-multiplication}.
% ($\Pi_\SM$)
\iffalse
Our approach leverages bipartite graph representation and the monotonicity of non-zero positions to decompose a general sparse matrix into linear transformations, enhancing the efficiency of our MPC protocols.
\fi
Our decomposition approach is not limited to GCNs but also general~SMM 
by 
%simply 
treating them 
as adjacency matrices.
%of a graph.
%Since any sparse matrix can be viewed 

%allowing the same technique to be applied.

 
\subsubsection{New Protocols for Linear Transformations}
\emph{Oblivious permutation} (OP) is a two-party protocol taking a private permutation $\sigma$ and a private vector $\xvec$ from the two parties, respectively, and generating a secret share $\l\sigma \xvec\r$ between them.
Our OP protocol employs correlated randomnesses generated in an input-independent offline phase to mask $\sigma$ and $\xvec$ for secure computations on intermediate results, requiring only $1$ round in the online phase (\cf, $\ge 2$ in previous works~\cite{ccs/AsharovHIKNPTT22, ccs/Araki0OPRT21}).

Another crucial two-party protocol in our work is \emph{oblivious selection-multiplication} (OSM).
It takes a private bit~$s$ from a party and secret share $\l x\r$ of an arithmetic number~$x$ owned by the two parties as input and generates secret share $\l sx\r$.
%between them.
%Like our OP protocol, o
Our $1$-round OSM protocol also uses pre-computed randomnesses to mask $s$ and $x$.
%for secure computations.
Compared to the Beaver-triple-based~\cite{crypto/Beaver91a} and oblivious-transfer (OT)-based approaches~\cite{pkc/Tzeng02}, our protocol saves ${\sim}50\%$ of online communication while having the same offline communication and round complexities.

By decomposing the sparse matrix into linear transformations and applying our specialized protocols, our \osmm protocol
%($\prosmm$) 
reduces the complexity of evaluating $\numnode \times \numnode$ sparse matrices with $\numedge$ non-zero values from $O(\numnode^2)$ to $O(\numedge)$.

%(\S\ref{sec::secgcn})
\subsection{\cgnn: Secure GCN made Efficient}
Supported by our new sparsity techniques, we build \cgnn, 
a two-party computation (2PC) framework for GCN inference and training over vertical
%ly split
data.
Our contributions include:

1) We are the first to explore sparsity over vertically split, secret-shared data in MPC, enabling decompositions of sparse matrices with arbitrary sparsity and isolating computations that can be performed in plaintext without sacrificing privacy.

2) We propose two efficient $2$PC primitives for OP and OSM, both optimally single-round.
Combined with our sparse matrix decomposition approach, our \osmm protocol ($\prosmm$) achieves constant-round communication costs of $O(\numedge)$, reducing memory requirements and avoiding out-of-memory errors for large matrices.
In practice, it saves $99\%+$ communication
%(Table~\ref{table:comm_smm}) 
and reduces ${\sim}72\%$ memory usage over large $(5000\times5000)$ matrices compared with using Beaver triples.
%(Table~\ref{table:mem_smm_sparse}) ${\sim}16\%$-

3) We build an end-to-end secure GCN framework for inference and training over vertically split data, maintaining accuracy on par with plaintext computations.
We will open-source our evaluation code for research and deployment.

To evaluate the performance of $\cgnn$, we conducted extensive experiments over three standard graph datasets (Cora~\cite{aim/SenNBGGE08}, Citeseer~\cite{dl/GilesBL98}, and Pubmed~\cite{ijcnlp/DernoncourtL17}),
reporting communication, memory usage, accuracy, and running time under varying network conditions, along with an ablation study with or without \osmm.
Below, we highlight our key achievements.

\textit{Communication (\S\ref{sec::comm_compare_gcn}).}
$\cgnn$ saves communication by $50$-$80\%$.
(\cf,~CoGNN~\cite{ccs/KotiKPG24}, OblivGNN~\cite{uss/XuL0AYY24}).

\textit{Memory usage (\S\ref{sec::smmmemory}).}
\cgnn alleviates out-of-memory problems of using %the standard 
Beaver-triples~\cite{crypto/Beaver91a} for large datasets.

\textit{Accuracy (\S\ref{sec::acc_compare_gcn}).}
$\cgnn$ achieves inference and training accuracy comparable to plaintext counterparts.
%training accuracy $\{76\%$, $65.1\%$, $75.2\%\}$ comparable to $\{75.7\%$, $65.4\%$, $74.5\%\}$ in plaintext.

{\textit{Computational efficiency (\S\ref{sec::time_net}).}} 
%If the network is worse in bandwidth and better in latency, $\cgnn$ shows more benefits.
$\cgnn$ is faster by $6$-$45\%$ in inference and $28$-$95\%$ in training across various networks and excels in narrow-bandwidth and low-latency~ones.

{\textit{Impact of \osmm (\S\ref{sec:ablation}).}}
Our \osmm protocol shows a $10$-$42\times$ speed-up for $5000\times 5000$ matrices and saves $10$-2$1\%$ memory for ``small'' datasets and up to $90\%$+ for larger ones.
