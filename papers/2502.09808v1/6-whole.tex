%!TEX root = gcn.tex
\section{End-to-End GCN Inference and Training}
\label{sec::secgcn}

\paragraph{Implementation.}
%of $\cgnn$.
$\cgnn$ adopts classical GCN~\cite{iclr/KipfW17} in the transductive setting with
%, which contains 
two graph convolution layers ($\mathsf{GConv}$) 
followed by ReLU and softmax function.
We implement $\cgnn$ using Python
in the TensorFlow framework.
\begin{equation*}
\begin{aligned}
\Xsf,\Asf
	\rightarrow 
\boxed{
	\stackrel{\textstyle{\mathsf{GConv}}}{\boxed{\weimat_{1}}}
}
	\rightarrow 
 	\boxed{ {\mathsf{{ReLU}}}}
	\rightarrow 
\boxed{
	\stackrel{\textstyle{\mathsf{GConv}}}{\boxed{\weimat_{2}}}
}
	\rightarrow
	\Ysf
	\rightarrow 
 	 	\boxed{ {\mathsf{{Softmax}}}} 
 	 	\end{aligned}
 	\end{equation*}
We implemented all the above protocols (detailed in Appendix~\ref{sec::alg}) in $\cgnn$
%, in which all lines of code build 
upon secure computation with secret shares.
Since GCN inherits conventional neural networks, we still rely on similar functions and layers.
Following the ideas of \cite{sp/MohasselZ17,ccs/RatheeRKCGRS20,uss/WatsonWP22,acsac/0021ZCPTLY23,popets/AttrapadungHIKM22},
%in private training, 
we re-implemented the relevant protocols under the 2PC setting in $\cgnn$ for ReLU,
%pooling, 
softmax, Adam optimization, and more.
For non-sparse multiplication, $\cgnn$ still uses Beaver triples~\cite{crypto/Beaver91a}.

\paragraph{Forward Propagation.}
Recall that $\pp_0$ holds the (normalized) adjacency matrix $\Asf$ and $\pp_1$ holds the features $\Xsf$.
The first GCN layer is defined as $\Zsf =\mathsf{ReLU}(\Asf\Xsf \weimat)$, thus $\pp_0$ and $\pp_1$ jointly execute $\Pi_\smm$, $\promult$, and ReLU protocols (combining PPA~\cite{arith/Beaumont-SmithL01}, GMW~\cite{acssc/Harris03}, and $\SM$).
$\pp_0$ and $\pp_1$ will get $\l \Zsf\r_0$ and $\l \Zsf\r_1$, respectively.
%In the forward propagation of the second layer, $\pp_0$ holds $\Asf$ and the share $\l Z_1\r_0$, while $\pp_1$ has the share $\l Z_1\r_1$.
Then, $\pp_0$ and $\pp_1$ securely compute $\Ysf= \Asf \Zsf \weimat$ and $\mathsf{Softmax}(\Ysf)$ in the second layer.
In the output layer, $\pp_0$ and $\pp_1$ jointly execute secure softmax protocol~\cite{acsac/0021ZCPTLY23}.

\paragraph{Backward Propagation.}
$\pp_0$ and $\pp_1$ securely compute $\mathsf{softmax} (\Ysf)-\Ysf'$ using cross-entropy loss to get $\frac{\partial loss}{\partial \Ysf}$, where $\Ysf'$ is the label matrix.
%$Z \in \Mbb_{n, k}$ 
%, i.e., every line of $Z$ is the one-hot vector of the class of the node, and softmax is computed for every line.
Then, we compute the gradient of a graph layer $\frac{\partial loss}{\partial \weimat}=\Zsf^{\top}\Asf^{\top} \frac{\partial loss}{\partial \Ysf}$, where the multiplication of $\Zsf^{\top}\Asf^{\top}$ is \osmm.
If we use the SGD optimizer, $\weimat$ is updated to be $\weimat \leftarrow \weimat - \eta \frac{\partial loss}{\partial W}$,
where $\eta$ is the learning rate.
If we use the Adam optimizer~\cite{iclr/KingmaB14}, $\weimat$ is updated by following the computation of Adam given $\frac{\partial loss}{\partial W}$.
The last step is to securely compute the gradient of ReLU and graph layer similarly.
%If we use other method of optimization, we update $W$ as $W \leftarrow (W, \frac{\partial loss}{\partial W})$
%similarly.

\paragraph{GCN Inference and Training.} 
As for end-to-end secure GCN computations, $\pp_0$ and $\pp_1$ collaboratively execute a sequence of protocols to run a single forward propagation (for inference) or forward and backward propagation iteratively (for training).
%As for end-to-end secure GCN training, $\pp_0$ and $\pp_1$ collaboratively execute a sequence of protocols in the forward and backward propagation iteratively.
$\cgnn$ supports both single-server simulation for multiple hosts and multiple-server execution in a distributed setting.
Using $\cgnn$, researchers and practitioners can realize various GCNs
%models 
using the template of $\texttt{class SGCN}$ (Appendix~\ref{sec::alg}),
%The usage of $\cgnn$ is 
similar to using the TensorFlow framework except that \emph{all computations are over secret shares}.