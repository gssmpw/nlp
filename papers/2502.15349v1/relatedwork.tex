\section{Related Work}
\vspace{-2mm}

\para{Handcrafted attention.}
High-performance attention mechanisms frequently rely on handcrafted kernel implementations optimized for specific patterns. FlashAttention\cite{dao2022flashattention} provides a highly optimized kernel for standard transformer attention, utilizing techniques such as online softmax, memory-efficient fusion, and pipelining. It is implemented using CUTE\cite{nvidia2024cutlass} on NVIDIA GPUs and ComposableKernel on AMD GPUs for low-level optimization.  Mamba2\cite{dao2024mamba2}, with official kernels developed in Triton\cite{triton}, focuses on tensor core utilization to enhance efficiency. Flash-Linear-Attention\cite{yang2024fla}, a third-party repository, extends beyond individual methods like Mamba2 and Gated Linear Attention (GLA), offering kernels for a wide variety of linear attention variants.

FlexAttention\cite{dong2024flexattentionprogrammingmodel} and FlashInfer\cite{ye2025flashinferefficientcustomizableattention} aim to simplify the development of attention mechanisms by offering high-level abstractions. However, these approaches primarily focus on elementwise transformations within transformer attention and are exclusively targeted at NVIDIA GPUs. While effective in their domain, their scope is limited, excluding support for linear attention and more advanced optimization strategies. Additionally, their lack of compatibility with AMD GPUs highlights a significant gap in addressing multi-backend requirements.

While these implementations achieve excellent performance, they are restricted to specific attention designs and require substantial manual effort to adapt for new variants. This reliance on handcrafted kernels limits scalability and slows innovation, particularly for emerging attention designs. In contrast, \oursys{} abstracts the complexity of kernel development, enabling users to define and optimize diverse attention mechanisms without the need for manual implementation. By leveraging a unified programming model and automated optimization pipeline, \oursys{} supports a broader range of configurations while maintaining competitive performance.

\para{Compiler optimization.}
Existing DNN compilers, such as TVM\cite{tvm2018}, Ansor\cite{ansor}, XLA\cite{xla}, Welder\cite{shi2023welder}, Ladder\cite{wang2024ladder}, and TensorRT\cite{tensorrt}, widely adopt techniques like operator fusion to reduce memory overhead and improve computational efficiency. However, these approaches primarily focus on spatial tiling for regular operators, neglecting the unique challenges and opportunities presented by attention mechanisms. \oursys{} incorporates common compiler optimization methods, such as fusion and tiling, while extending them to support the irregular computations inherent in attention mechanisms. 

% \para{Programming Model-Based Approaches}
% Programming model-based systems, such as 

\oursys{} overcomes these limitations by supporting both transformer and linear attention within a single framework. It incorporates advanced scheduling techniques and targets multiple backends, including NVIDIA and AMD GPUs, ensuring high performance and scalability. By unifying diverse attention mechanisms under a comprehensive programming model, \oursys{} facilitates the efficient development and deployment of a wide range of attention designs across heterogeneous hardware architectures.