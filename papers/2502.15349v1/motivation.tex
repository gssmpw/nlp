%!TEX root = main.tex



\section{Background}
\label{sec:moti}


% Transformer, LLM, attention:
% introduce transformer and LLM. attention mechanism is the key in transformer

% Attention optimizations: fusion
% introduce flash attention fusion and manual fusion implementations, however, it requires manual implementations


% figure: Attention performance before and after manual optimizations: full attention vs. FA3, sigmoid attention?, linear attention

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figure/transform_linear_diff-cropped.pdf}
%     \caption{Structure comparison of Transformer and Linear Attention}
%     \label{fig:attn-pattern}
% \end{figure}

% Observations and opportunities


\subsection{Attention Mechanisms}

Large Language Models (LLMs) have transformed natural language processing (NLP), enabling breakthroughs in tasks such as text understanding and generation. At the core of this success is the attention mechanism, which allows models to selectively focus on relevant parts of an input sequence, significantly enhancing sequence-to-sequence tasks like translation~\cite{bahdanau2014neural}. Attention computes pairwise relevance, or \textit{attention scores}, between input tokens, which are then used to weight and aggregate token representations, guiding the generation of output tokens.

The introduction of Queries (Q), Keys (K), and Values (V) in the Transformer architecture~\cite{Ashish17AttentionIsAllYouNeed} formalized and generalized attention computation. Queries represent what the model seeks, Keys encode the input attributes, and Values carry the associated content. Modern attention computation, as summarized in Figure~\ref{fig:attention_variant}, follows five key stages:
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Input Tokens:} Raw input sequences serve as the foundation for computation.
    \item \textbf{Embedding:} Input tokens are mapped to continuous vector representations through projections of Q, K, and V matrices, encapsulating semantic information.
    \item \textbf{Interaction:} Pairwise relevance scores are computed using the dot product of Q and K, optionally scaled, to quantify token relationships.
    \item \textbf{Normalization:} Relevance scores are transformed into normalized weights using functions like softmax, ensuring interpretability and row-wise consistency.
    \item \textbf{Composition:} Weighted scores are combined with V representations to generate context vectors, integrating information from relevant tokens into a single output for each token.
\end{itemize}

The Q, K, V framework has established itself as the foundation of modern attention mechanisms, offering scalability, flexibility, and computational efficiency. This structured approach underpins the success of neural architectures in addressing a wide range of NLP tasks.




\subsection{Diversity in Attention Mechanisms}
 
Building on the foundational design of attention mechanisms, researchers have introduced numerous variants aimed at improving performance, addressing task-specific requirements, and enhancing computational efficiency. As illustrated on the right side of Figure~\ref{fig:attention_variant}, these innovations can be categorized into algorithmic and efficiency advancements, each targeting specific stages of the attention mechanism. 

\para{Algorithmic Innovations}
  focus on enhancing robustness, accuracy, and task-specific capabilities in attention:
\begin{itemize}[noitemsep,topsep=0pt, left=0pt]
    \item Task-Specific Modifications: Causal attention\cite{Ashish17AttentionIsAllYouNeed} modifies the interaction stage by restricting interactions to prior tokens. This design supports autoregressive decoding, a critical feature for applications like text generation and speech synthesis.
    \item Improved Robustness and Accuracy: DiffTransformer\cite{ye2024differentialtransformer} refines both the interaction and normalization stages for higher accuracy and reduced noise in attention scores.
    \item Non-Conventional Tensor Dimensions: Models like DeepSeek V2\cite{deepseekai2024deepseekv2strongeconomicalefficient} and RetNet\cite{sun2023retentive} enhance the embedding stage by employing higher hidden dimensions, enabling richer semantic representation.
\end{itemize}

\para{Efficiency Innovations}
aim to reduce computational overhead while maintaining the effectiveness of attention:
\begin{itemize}[noitemsep,topsep=0pt, left=0pt]
    \item Compact Representations: Linear attention, such as Mamba\cite{dao2024mamba2} and the recurrent form of RetNet\cite{sun2023retentive}, transforms the interaction, normalization and composition stages by compressing past information into compact $KV$ representations. Sliding Window Attention\cite{beltagy2020longformerlongdocumenttransformer} modifies the interaction stage by limiting the attention scope to a fixed local window, optimizing memory usage and computational focus.
    \item Sparse Attention: Sparse attention mechanisms, such as BigBird\cite{zaheer2021bigbirdtransformerslonger}, SeerAttention\cite{gao2024seerattentionlearningintrinsicsparse}, and BitNet\cite{wang2023bitnetscaling1bittransformers}, introduce sparsity across multiple stages, including input tokens, embedding, interaction, and normalization. These methods leverage structured patterns or treat low-bit precision as sparse regions to reduce computational and memory demands without sacrificing effectiveness.
\end{itemize}
% Building on the foundational design of attention, researchers have developed numerous variants tailored to enhance accuracy and reduce computational overhead. DiffTransformer\cite{} refines the normalization process to suppress noise in standard attention, thereby improving model robustness. Mamba\cite{} and RetNet\cite{} introduce innovations in long-term sequence modeling, with Mamba reformulating the weighted aggregation step into a recurrent compression of compact hidden states to reduce memory consumption, and RetNet incorporating headwise decay to emphasize temporal relevance in hidden states. Models like DeepSeek V2\cite{} adopt non-standard tensor dimensions, presenting unique computational challenges that push the boundaries of existing optimization techniques.

% Other variations address specific task requirements or computational challenges. Causal attention\cite{} restricts interactions to prior tokens, enabling autoregressive generation essential for tasks like text generation. Sliding window attention\cite{} limits the scope of attention to a fixed local window, enhancing computational efficiency while focusing on task-specific contexts. 

% The rapid proliferation of these attention variants highlight the need for an agile computational stack that can efficiently adapt to these variants and fully utilize the capabilities of modern hardware.

% \subsection{Attention Mechanisms and Their Variants}
% Large Language Models (LLMs) have revolutionized natural language processing (NLP) and related domains by enabling significant advancements in understanding and generating text. At the core of this success is the attention mechanism.
% The attention mechanism calculates pairwise similarity scores between tokens. These scores are then normalized through a softmax function to generate attention weights. These weights determine the influence each token has on the output and are subsequently used to perform a weighted aggregation, yielding the output representation for each token.

% Building on this foundational design, researchers have introduced numerous variants aimed at improving accuracy and reducing computational overhead. For instance, DiffTransformer\cite{} refines the normalization process to suppress noise in standard attention, enhancing model robustness. Mamba\cite{} reformulates the weighted aggregation step into a recurrent compression of compact hidden states, reducing memory consumption. Additionally, models like DeepSeek V2\cite{} and RetNet\cite{} introduce modifications in tensor dimensions or incorporate decay factors for long-term sequence modeling.

% The rapid proliferation of these attention variants highlight the need for an agile computational stack that can efficiently adapt to these variants and fully utilize the capabilities of modern hardware.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/intro_attn.pdf}
%     \caption{Attention proportion in LLama3B forward}
%     \label{fig:attention-in-llama}
% \end{figure}

\begin{table}[t]
  \centering
  \resizebox{0.3\textwidth}{!}{
    \begin{tabular}{l|c|c|c|c}
    \hline
    Seqlen & 2K    & 4K    & 6K    & 8K \\
    \hline
    LLAMA-3B &   55\%    &   70\%    &   78\%    & 82\% \\
    \hline
    \end{tabular}%
    }
  \caption{Attention proportion in LLAMA-3B inference}
  \vspace{-5mm}
  \label{tab:llama-proportion}%
\end{table}%


\subsection{Efficient Implementation of Attention}
The attention mechanism takes large proportion in LLM computation. Table~\ref{tab:llama-proportion} shows the attention proportion in LLAMA-3B inference.
Efficient implementations of various attention mechanisms hinge on reducing memory access and maximizing the utilization of compute units. Many libraries with handcrafted kernels achieve this by fusing memory-intensive operations, including element-wise calculations and reductions.

FlashAttention~\cite{shah2024flashattention} exemplifies this approach by integrating softmax computation, memory-efficient pipelining, and kernel fusion, thereby reducing computational overhead and improving performance. However, these libraries impose strict constraints on the attention patterns they support. Even minor deviations, such as the atypical input dimensions used in DeepSeek V2 and RetNet, can invalidate these optimizations. Figure~\ref{fig:torch-vs-handcraft} illustrates the performance disparity across different attention variants. For standard Softmax-Attention, the handcrafted library FlashAttention3~\cite{shah2024flashattention} significantly outperforms the native PyTorch implementation, achieving over 60\% FLOPS utilization. In contrast, for less common variants like Gated-RetNet and ReLU-Attention, these libraries exhibit poor performance or provide no support at all.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/figure_intro.pdf}
    \vspace{-0.7cm}
    \caption{The performance of attention implementations.}
  \vspace{-5mm}
    \label{fig:torch-vs-handcraft}
\end{figure}

Additionally, due to limited development resources, these libraries predominantly target top-tier hardware, such as NVIDIA's H100 and A100 GPUs, and are not easily transferable to alternative platforms like AMD GPUs. Adapting these implementations to different hardware ecosystems remains challenging and demands significant expertise.

To simplify kernel development, automated compilers\cite{24pytorch2, tvm2018, ansor, xla, shi2023welder, tensorrt} have emerged. While these tools reduce development effort, they struggle to match the performance of handcrafted kernels for attention variants. This limitation arises from their inability to fully understand the semantics of attention computation, as they often treat it as a sequence of discrete and opaque operations. Advanced optimizations, such as transforming softmax into an online softmax, are beyond the scope of current compiler capabilities, resulting in suboptimal performance.

To balance performance and development efficiency, some approaches adopt trade-offs between flexibility and optimization. For instance, FlexAttention~\cite{dong2024flexattentionprogrammingmodel} utilizes a template-based methodology in which the majority of the computation is predefined, while exposing a limited set of customizable functions to users. This design enables the optimization of the entire attention operation while providing some flexibility for specific variants. However, these templates are derived from the computational flow of a particular variant, making it difficult to generalize to a wider range of attention variants, such as linear attention.


% \subsection{Key Observations}

% Attention mechanisms exhibit significant diversity at the implementation level. For example, standard attention utilizes matrix multiplication to compute attention scores between $Q$ and $K$, followed by a weighted aggregation of $V$ to produce the output representation. In contrast, linear attention compresses $K$ and $V$ using a recurrent loop before applying $Q$ to compute the output. Despite these implementation differences, these variants adhere to the same underlying principles of attention semantics.

% By examining the native implementation of attention as a loop-based operation, we identify two fundamental components common to all attention mechanisms:

% \begin{itemize}[noitemsep,topsep=0pt, left=0pt]
%     \item \textit{Relevance Scoring:} This operation forms the core of attention mechanisms, capturing pairwise similarities or interactions between input tokens. It is typically realized through inner products or other similarity measures to determine token relationships.
%     \item \textit{Aggregation:} Using the relevance scores, this operation consolidates contextual information into a representation for each token. Aggregation methods vary, including parallel reductions \cite{sliding_window, causal_attention} or recurrent loops \cite{rwkv, mamba, retenet}.
% \end{itemize}

% Building on these two fundamental operations, we propose a unified template to encapsulate the diverse spectrum of attention variants. This template abstracts the shared semantics of relevance scoring and aggregation while keeping other parts customizable, achieving a balance between broad applicability and development agility. By providing a consistent framework, this approach simplifies the design and implementation of new attention mechanisms while ensuring efficient adaptation to evolving computational demands.



% To address these challenges, we propose a generalized attention framework built upon three core computational components. These components abstract the essence of attention mechanisms, focusing on their foundational principles rather than specific operations:
% \begin{itemize}[noitemsep,topsep=0pt, left=0pt]
%     \item \textit{Relevance Computation:} This is the essence of attention mechanisms, capturing the pairwise similarity or interaction between input tokens. Typically implemented through inner products or other similarity measures, this operation identifies token relationships. 
%     % and directly corresponds to the embedding and interaction stages in the operational view. 
%     \item \textit{Normalization or Weighting:} Serving as a supporting operation, normalization ensures numerical stability, while weighting assigns meaningful relevance to computed relationships. These operations are commonly realized through row-wise functions such as softmax or sigmoid.
%     % aligning with the normalization stage in the operational view. 
%     \item \textit{Attention State Compression:} This component enhances computational efficiency and integrates contextual information. Techniques such as parallel reductions \cite{sliding window, causal} or recurrent loops \cite{rwkv, mamba, retenet} are employed to aggregate and compress data.
%     % Unlike relevance computation or normalization, attention state compression spans multiple stages in the operational view, influencing how data is compacted and processed. 
    
% \end{itemize}

% Among these components, relevance computation stands out as the core of attention mechanisms, defining their fundamental purpose. Normalization or weighting provides critical support to stabilize and weight these computations, while attention state compression focuses on efficiency and information integration.

% This abstraction offers a flexible and reusable framework for designing and implementing diverse attention variants, addressing the limitations of stage-based frameworks. In \S\ref{sec:design}, we introduce the design of \oursys{}, detailing how this abstraction is brought to life through customizable templates, advanced optimization techniques, and cross-platform execution strategies. By decoupling core computational concepts from implementation details, our framework seamlessly accommodates emerging innovations and diverse hardware platforms.

% This semantic abstraction provides a flexible and reusable framework for designing and implementing diverse attention variants. When mapped to specific implementations, each abstract operation naturally corresponds to one or more operational stages, bridging the gap between high-level conceptual design and low-level execution. By separating the semantics from implementation, this approach facilitates the creation of extensible and efficient attention mechanisms, ensuring adaptability to emerging designs and hardware platforms.


% Attention mechanisms, despite their apparent diversity, share three fundamental computational invariants that form the foundation for abstraction, as illustrated in Figure~\ref{fig:attention-invariants}:
% \begin{itemize}
%     \item \textit{Relevance Computation}: Pairwise computation of similarity or interaction between input tokens, typically implemented using inner products or other similarity measures. This stage identifies token relationships and serves as the foundation for all attention mechanisms.
%     \item \textit{Normalization or Weighting}: Transformation of relevance scores into normalized weights, often via row-wise functions such as softmax or sigmoid, ensuring numerical stability and meaningful scaling of relevance scores.
%     \item \textit{Attention State Compression}: Aggregation of contextual information into token representations, achieved through parallel reductions \cite{sliding window, causal} or recurrent loop \cite{rwkv, mamba, retenet}.
% \end{itemize}

% These invariants provide an opportunity to design templates capable of accommodating a wide range of attention variants. By abstracting these commonalities, it is possible to create reusable and flexible implementations that balance efficiency and programmability.


% The regular pattern make it possible to capture various varients into simple templates.

% However, with so many implementation choice, no single implementation consistently outperforms others across all model configurations and hardware platforms. Performance depends heavily on factors like tensor dimensions, batch sizes, and hardware specification. For instance, an attention mechanism may perform optimally with one library on an A100 but require a different approach on an H100. This variability is further exacerbated during model development, where configurations frequently change.

% As a result, determining the best implementation often requires extensive experimentation, making the process both time-consuming and resource-intensive.






% Achieving high performance in attention mechanisms necessitates advanced optimizations. Fusion techniques, such as those employed in FlashAttention\cite{shah2024flashattention}, integrate softmax computation, memory-efficient pipelining, and kernel fusion to reduce overhead and improve efficiency. Similarly, Mamba2\cite{dao2024mamba2} and Flash-Linear-Attention\cite{yang2024fla} optimize linear attention mechanisms using fusion for hardware acceleration.


% \TODO{However, these implementations rely on handcrafted kernels tailored to specific configurations and hardware. This approach demands significant manual effort and expertise, limiting scalability to emerging attention variants. Figure~\ref{fig:torch-vs-handcraft} highlights the performance improvements of manual optimizations across standard transformer attention (e.g., FlashAttention V3), sigmoid attention\cite{ramapuram2024sigmoidattn}, and linear attention (e.g., Mamba2).}

% \para{Diversity of Attention Variants}
% The diversity of attention mechanisms arises from the need to address specific computational and architectural challenges. For transformer attention variants, common modifications include replacing the softmax function with alternatives like sigmoid\cite{ramapuram2024sigmoidattn} or adjusting input tensor dimensions, such as DeepSeek V2($d_v=128$ and $d_{qk}=192$)\cite{} and DiffTransformer($d_v=256$ and $d_{qk}=128$)\cite{}. Linear Attention variants focus on hidden state representation, such as RetNet\cite{sun2023retentive}, which applies headwise decay, and Mamba2\cite{dao2024mamba2}, which incorporates selective gating for improved efficiency.

% Despite these innovations, existing solutions face significant inefficiencies. Modified row-scale functions or novel linear attention mechanisms often rely on non-fused kernels, resulting in suboptimal performance. Alternatively, they require expert-crafted kernels, a process that is both time-consuming and prone to errors. Additionally, attention variants with non-standard tensor dimensions frequently resort to padding to fit existing implementations like FlashAttention, resulting in wasted memory and computational resources.

% \para{Challenges and Opportunities}
% The growing diversity of attention variants presents both challenges and opportunities. While these mechanisms introduce task-specific optimizations, their computations share a common framework:  inputs ($Q, K, V$) and outputs ($O$) form the backbone, while intermediate states—such as attention scores and row scales in transformer attention or hidden states in linear attention—drive variations. However, two key challenges hinder progress:

% 1. \textbf{Scalability}: Handcrafted kernels require labor-intensive development and are limited to predefined configurations, stifling innovation in attention design.

% 2. \textbf{Hardware Specificity}: Most handcrafted implementations, such as those for FlashAttention and Mamba2, are optimized for specific hardware configurations like NVIDIA A100 or H100 GPUs. On new hardware platforms, such as AMD MI300X, tile sizes and other implementation details vary, necessitating entirely new kernel designs. This significantly increases development overhead and limits portability.

% 3. \textbf{Generality}: Tensor expression-based compilers like TVM\cite{tvm2018} lack the flexibility to represent and optimize complex attention patterns, necessitating manual implementation.

% These limitations create an opportunity for a unified system that abstracts attention mechanisms and automates their optimization. \oursys{} addresses these challenges by providing high-level abstractions, automated scheduling, and support for multi-backend execution, enabling efficient and scalable attention designs across diverse configurations and hardware platforms.
% \subsection{Background: Custom Attention}

% \subsubsection{attention pattern}


% scheduling policy

% cost model: shared mmeory limit
% tune: 




% \begin{lstlisting}
% // Attention pattern, seqlen_kv means 当前query注意的序列，
% // D_state为被注意序列的状态信息维度。
% def attention_pattern(
%     query: [1, D_state],
%     key: [seqlen_kv, D_state],
%     value: [seqlen_kv, 1],
%     o: [1, 1]
%     ):
%     o = query @ key @ value

% // is_compression: 决定将中间状态保留全部seqlen_kv, 
% // 还是将其压缩为固定的dstate
% bool is_compression;
% def attention():
%     if qk_first/no_compression:
%         hidden_state[1, seqlen_kv] = query @ key.T
%         o[1,1] = hidden_state @ value
%     else if kv_fisrt/compression:
%         hidden_state[Dstate,1] = key.T @ value
%         o[1,1] = query @ hidden_state

% // customized attention: 用户定义filter，
% // 来决定对中间状态即注意到的信息如何处理。
% func hidden_state_func;
% def attention():
%     ...
%     hidden_state = hidden_state_func(hidden_state)
%     ...

% // customized attention: 对key, value序列定义filter
% // 决定对注意到的序列信息关注权重。
% func k_func, v_func;
% def attention():
%     k = k_func(k)
%     v = v_func(v)
%     ...

% \end{lstlisting}

% （是否 compression 分成的两类间是相交的关系，对于线性的hidden\_state\_func, 既可以表示为compression，也可以表示为no\_compression 。 所有func均不改变张量形状）

% \subsubsection{计算量优化}

%  在compression的情况下，对于 语言模型的pattern : (1) 多个query (2) 并且每个query的注意力为当前query之前的所有kv (3)并且有着特定的线性k\_func, 
 
%  则 可以复用之前计算的k @ v, 转换为更少计算量的形式。

% \begin{lstlisting}
% // decay 等价于一种特殊的k_func表示
% decay: [Dstate,1]
% def main(
%     q: [seqlen, Dstate],
%     k: [seqlen, Dstate],
%     v: [seqlen, 1],
%     o: [seqlen, 1]
%     ):
%     for i in range(seqlen):
%         hidden_state[Dstate,1] = decay[i] * hidden_state + 
%         k[i].T @ v[i]
%         o[i] = q[i] @ hidden_state
% \end{lstlisting}



% \para{Diversity of Attention Variants}
% Attention mechanisms are fundamental to modern deep learning models, with two main trends emerging: transformer attention and linear attention. Both operate on three input tensors—query ($Q$), key ($K$), and value ($V$)—but differ in how they handle past information. Transformer attention processes all past information by attending to all tokens, while linear attention compresses past information into a hidden state for efficiency. Representative structures of transformer and linear attention are shown in Figures~\ref{fig:transformer-attn} and \ref{fig:linear-attn}, respectively.

% Variants of transformer attention typically modify row-scale functions (e.g., replacing softmax with sigmoid\cite{ramapuram2024sigmoidattn}), transform the attention scores $S$, or adjust input tensor dimensions (e.g., DeepSeek V2 uses $d\_v=128$ and $d\_qk=192$). Similarly, linear attention variants focus on altering the hidden state representation to achieve different performance characteristics. For instance, \TODO{add specific linear attention examples}Retnet\cite{sun2023retentive} apply a headwise decay on hidden state and Mamba2\cite{dao2024mamba2} apply a selective gate on hidden state. Despite their differences, we observe that all attention variants, including transformer and linear attention, can be unified under a general framework. This framework uses the inputs ($Q$, $K$, $V$) and output($O$) as a backbone, while intermediate states—such as attention scores and row scale in transformer attention or hidden state in linear attention—provide the flexibility to accommodate diverse designs.

% \para{Challenges in Attention Implementation}
% Modern operator optimization heavily relies on fusion techniques to enhance performance. However, the complexity of attention mechanisms exceeds the capabilities of tensor expression-based compilers like TVM\cite{tvm2018} (\TODO{add more citations}). As a result, state-of-the-art attention implementations, such as FlashAttention\cite{shah2024flashattention} for transformer attention (written using CUTE\cite{nvidia2024cutlass}) and Mamba2\cite{dao2024mamba2} or flash-linear-attention\cite{yang2024fla} for linear attention (written using Triton\cite{triton}), are handcrafted. While effective, these implementations are limited to a narrow range of attention variants and require significant expertise to develop. This manual effort not only impedes scalability but also hinders the rapid design and experimentation of new attention variants. 
