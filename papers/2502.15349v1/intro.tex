%!TEX root = main.tex
\section{Introduction}
\label{sec:intro}


%Attention mechanisms lie at the core of transformer architectures and large language models (LLMs), powering groundbreaking advancements in natural language processing (NLP) and related domains. By dynamically weighting token interactions, attention enables models to capture contextual relationships, making it an indispensable component of modern deep learning systems.

Attention is a fundamental mechanism in modern large langauge models (LLMs), enabling groundbreaking advancements in natural language understanding and related domains. By dynamically weighting interactions across input tokens, attention allows models to capture sophisticated contextual relationships, making it an indispensable component of modern deep learning systems.

Attention mechanisms dominate the computational workload in LLMs, and their 
proportion continuously increases with the growing sequence length. This trend underscores the critical importance of optimizing attention for end-to-end model training and inference. 
For instance, as illustrated in Table~\ref{tab:llama-proportion}, attention accounts for \(55\%\) of the computational time in LLAMA-3B when the sequence length is 2048. This proportion further escalates to \(82\%\) as the sequence length extends to 8192. 
Such a significant computational burden highlights the necessity of efficient attention mechanisms to ensure optimal performance and scalability of LLMs across various applications and hardware platforms.

However, attention optimization is nontrivial due to high computation and memory demands and often relies on handcrafted kernels. For example, FlashAttention~\cite{dao2022flashattention} employs online softmax, memory-efficient pipelining, and kernel fusion to improve canonical attention; while Mamba2~\cite{dao2024mamba2}, a linear version of attention, utilizes Triton-based~\cite{triton} kernels with selective gating and chunk-based processing for performance improvement. These handcrafted optimizations are labor-intensive, hardware-specific, and constrained to fixed configurations, thus limiting the adaptability to diverse attention designs and configurations.

%Optimizing attention mechanisms presents significant challenges due to their computational and memory demands. High-performance implementations often rely on handcrafted kernels designed to fuse operations and maximize hardware efficiency. For example, FlashAttention\cite{dao2022flashattention} employs online softmax, memory-efficient pipelining, and kernel fusion to minimize overhead and latency, while Mamba2\cite{dao2024mamba2} utilizes Triton-based\cite{} kernels with selective gating and chunk-based processing to leverage tensor cores. However, these handcrafted optimizations are labor-intensive, hardware-specific, and constrained to fixed configurations, limiting their adaptability to diverse attention designs.

The diversity of attention variants continues to expand, driven by task-specific requirements and innovations. For instance, sigmoid attention\cite{ramapuram2024sigmoidattn} replaces softmax with sigmoid activation for improved efficiency, and linear attention mechanisms, such as Mamba\cite{dao2024mamba2}, reformulate computation with selective gating for enhanced efficiency. Other variants, like DeepSeek V2\cite{deepseekai2024deepseekv2strongeconomicalefficient} and RetNet\cite{sun2023retentive}, deviate further by requiring non-standard tensor dimensions, introducing additional computational challenges.

Adapting to this growing diversity requires significant expert efforts for kernel customization. Furthermore, differences in Attention input configurations and hardware platforms, such as NVIDIA A100, H100, and AMD MI300X GPUs, complicate the landscape. Hardware differences in tile sizes, memory hierarchies, and pipelining strategies necessitate new implementations, significantly increasing development overheads and limiting scalability. 
% \HB{It would be more convincing to use one or two examples to specifiy the efforts for customization.}
For example, FlashAttention v2 reached \(70\%\) of the peak computation throughput on the NVIDIA A100, but only achieved \(30\%\) on the NVIDIA H100. Complex techniques such as register-level pipelining and ping-pong kernel design must be used to achieve peak performance\cite{shah2024flashattention}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/AttentionVariant.pdf}
  \vspace{-2mm}
    \caption{The foundational attention mechanism and its variants: Attention
mechanisms is divided into stages such as embedding, interaction, normalization, and composition(left). Attention variants make various changes to these stages(right). For example, Causal Attention modified the interaction stage to apply a mask, which makes the computation flow different.}
  \vspace{-5mm}
    \label{fig:attention_variant}
\end{figure*}

To address these challenges, we propose \oursys{}, a unified framework for designing, optimizing, and executing diverse attention mechanisms across hardware platforms. At its core, \oursys{} abstracts attention mechanisms into two fundamental operations: relevance scoring and aggregation. These operations capture the essence of attention mechanisms, ensuring a consistent yet flexible foundation for diverse designs.


Building on this abstraction, \oursys{} introduces customizable attention templates that fix the core operations of relevance scoring and aggregation while exposing \texttt{customizable functions} for user-defined extensions. These functions allow users to design their attention variants by applying transformations like masking, scaling, or row-wise normalization, enabling seamless adaptation to task-specific requirements.


One challenge is how to retain high performance customization despite abstraction. \oursys{} enables automated optimization through a cross-backend scheduling and execution framework that dynamically adapts to input configurations and hardware constraints. By abstracting kernel generation and optimization complexities, \oursys{} supports a wide range of attention variants and hardware platforms while delivering exceptional performance.



We implemented \oursys{} with 7.3k lines of C++ and Python code and have open-sourced the system to foster further innovations. Evaluation results demonstrate that \oursys{} achieves performance comparable to handcrafted expert-optimized kernels, delivering up to 10.4Ã— speedup for configurations unsupported by existing implementations. Moreover, \oursys{} provides unparalleled flexibility for designing and optimizing custom attention mechanisms, marking a significant step toward scalable and generalizable attention computation.
% \HB{Need to describe the efforts required to customize a new attention mechanism or on a new platform.} added



