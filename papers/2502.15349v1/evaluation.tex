%!TEX root = main.tex
\section{Evaluation}
\label{sec:eval}


\begin{table}[t]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{c|c|c}
\hline
\textbf{Operator} & \textbf{Configuration} & \textbf{Model} \\ \hline
\multicolumn{1}{c|}{\textbf{}} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{} \\[-0.9em]  \hline
Softmax Attention & head=32, dimqk=128, dimv=128 & LLAMA3.1-8B \\
Softmax Attention & head=16, dimqk=192, dimv=128 & Deepseek-V2-lite   \\ 
Softmax Attention & head=12, dimqk=128, dimv=256 & DiffTransformer-3B   \\ \hline
Sigmoid Attention & head=32, dimqk=128, dimv=128 & LLAMA3-8B-style  \\ \hline
Relu Attention & head=6, dimqk=64, dimv=64 & ViT-s/16-style   \\ \hline
Retention Parallel & head=32, dimqk=256,dimv=512 & RetNet-6.7B   \\\hline
Mamba2 SSM & headv=80, dimqk=128, dimv=64 & Mamba2-2.7B   \\ \hline
Retention Recurrent & head=32, dimqk=256,dimv=512 & RetNet-6.7B  \\ \hline
Gated Retention & head=40, dimqk=256, dimv=256 & YOCO-13B   \\ 
Gated Retention & head=16, dimqk=64, dimv=64 & RFA-Big   \\ \hline
Softmax Attention Decoding & Seqlen=1, head=16, dimqk=192, dimv=128 & Deepseek-V2-lite  \\ \hline
\end{tabular}
}
\caption{A subset of attention in our microbenchmark.}
\label{table:operators}\vspace{-5mm}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figure/h100_eval.pdf}
    % \vspace{-3mm}
    \caption{Attention operator performance on H100 GPUs.}
    \vspace{-3mm}
    \label{fig:eval-fwd}
\end{figure*}

In this section, we evaluate \oursys{} on both attention microbenchmarks and end-to-end models by comparing them with the
state-of-the-art libraries and the compiler-based method to demonstrate the effectiveness of \oursys{}.
We summarize our findings: (1) \oursys{} can optimize standard transformer attention, achieving comparable performance with hand-crafted libraries. (2) \oursys{} can generate custom attention kernels, achieving speedup up to $10.4\times$. (3) \oursys{} support multi-backends, including NVIDIA and AMD GPUs.

\subsection{Experimental Setup}

\paragraph{Hardware platforms.} We evaluate \oursys{} on both NVIDIA and AMD GPUs, as they are currently the most popular hardware platforms. Our evaluation includes two high-performance GPUs: the NVIDIA H100 and the AMD Instinct MI250 GPU. 
We use CUDA version 12.4, Triton version 2.3.1 with the H100 GPU, and the ROCm version 6.2.4, Triton version 3.1.0 with the MI250 GPU. Both GPUs are evaluated on the operating system Ubuntu 20.04.

% \begin{table}[t]
% \centering
% \resizebox{0.5\textwidth}{!}{
% \begin{tabular}{c|c|c|c}
% \hline
% \textbf{Operator} & \textbf{Configuration} & \textbf{Model} & \textbf{Note} \\ \hline
% \multicolumn{1}{c|}{\textbf{}} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{} \\[-0.9em] \hline
% Softmax Attention & head=32, dimqk=128, dimv=128 & LLAMA3.1-8B &  \\
% Softmax Attention & head=16, dimqk=192, dimv=128 & Deepseek-V2-lite &  \\ 
% Softmax Attention & head=12, dimqk=128, dimv=256 & DiffTransformer-3B &  \\ \hline
% Sigmoid Attention & head=32, dimqk=128, dimv=128 & LLAMA3-8B-style &  \\ \hline
% Relu Attention & head=6, dimqk=64, dimv=64 & ViT-s/16-style &  \\ \hline
% Retention Parallel & head=32, dimqk=256,dimv=512 & RetNet-6.7B &  \\\hline
% Mamba2 SSM & headv=80, dimqk=128, dimv=64 & Mamba2-2.7B &  \\ \hline
% Retention Recurrent & head=32, dimqk=256,dimv=512 & RetNet-6.7B & \\ \hline
% Gated Retention & head=40, dimqk=256, dimv=256 & YOCO-13B &  \\ 
% Gated Retention & head=16, dimqk=64, dimv=64 & RFA-Big &  \\ \hline
% Softmax Attention Decoding & Seqlenq=1, head=16, dimqk=192, dimv=128 & Deepseek-V2-lite & \\ \hline
% \end{tabular}
% }
% \caption{A subset of attention in our microbenchmark.}
% \label{table:operators}\vspace{-5mm}
% \end{table}


\para{Attention workload.} We evaluate eight Attention algorithm, including four parallel pattern attention (Softmax Attention\cite{Ashish17AttentionIsAllYouNeed}, Sigmoid Attention\cite{ramapuram2024sigmoidattn}, ReLU Attention\cite{wortsman2023replacingsoftmaxreluvision} and parallel form of multi-scale retention\cite{sun2023retentive}) and four recurrent pattern attention (mamba2\cite{dao2024mamba2}, random feature attention\cite{peng2021randomfeatureattention}, retention rucurrent\cite{sun2023retentive}, gated retention\cite{sun2024YOCO}) 
For softmax attention, we perform the tests using configuration of LLAMA3.1-8B\cite{dubey2024llama3}, Deepseek-V2-lite\cite{deepseekai2024deepseekv2strongeconomicalefficient} and DiffTransformers-3B\cite{ye2024differentialtransformer}.
We select the batch size as 1 and 8 and sequence length as 2k, 4k and 8k for attention in large language models, which are common configurations for these models. 
Table \ref{table:operators} lists a representative subset of operators as well as
their configurations.

\para{Baselines.} We compare \oursys{} with manually implemented attention libraries, such as FlashAttention-v2\cite{dao2023flashattention} and FlashAttention-v3 \cite{shah2024flashattention} for Softmax attention, FlashSigmoid\cite{ramapuram2024sigmoidattn} for Sigmoid attention, Mamba2 chunk kernel\cite{dao2024mamba2} for Mamba2 SSM and Flash-Linear-Attention triton library\cite{yang2024fla} for gated retention. 
We also compare with state-of-the-art programming model-based approaches, such as FlexAttention\cite{dong2024flexattentionprogrammingmodel} and FlashInfer\cite{ye2025flashinferefficientcustomizableattention} for transformer attention. We use PyTorch\cite{pytorch} as a default baseline for attention that does not have a manually-implemented library, such as Retention Parallel\cite{sun2023retentive} and ReLUAttention\cite{wortsman2023replacingsoftmaxreluvision}.




\subsection{Attention Performance on NVIDIA H100}



Figure \ref{fig:eval-fwd} shows the performance of attention performance on NVIDIA H100. The x-axis represents different configs of attention operators, and the y-axis indicates the normalized latency relative to \oursys{}.

\para{Softmax attention.}
Figure \ref{fig:eval-fwd} (a)(b)(d) shows the performance of \oursys{} and other baselines on Softmax attention from Deepseek-V2-Lite, LLAMA3.1-8B, and Diff-Transformer-3B.
Compared with highly optimized libraries, \oursys{} still obtain significant speedup because of more flexible kernel templates. Compared with highly-optimized FlashAttention, \oursys{} achieves an average speedup of $1.88\times$ for forward and $1.52\times$ for backward on DeepSeek-V2-Lite and Diff-Transformers-3B, and achieves comparable performance on LLAMA3.1-8B. This improvement stems from \oursys{}'s flexible kernel template to natively support different \texttt{headdim\_qk} and \texttt{headdim\_v}, instead of padding them to the same dimension. \oursys{} also outperforms other programming-model-based approaches such as FlexAttention and FlashInfer, due to our scheduling over different shapes.

\para{Customized transformer attention.}
Figure \ref{fig:eval-fwd} (c)(e)(f) shows the performance of \oursys{} and other baselines on 
customized transformer attention (Sigmoid attention, ReLU attention and retention parallel). Current expert-optimized libraries lack support for these custom attentions. For example, no fused attention kernel is implemented for ReLU attention and fused Sigmoid attention kernel is not optimized for the latest hardware like NVIDIA H100. \oursys{} can obtain significant speedup on these customized attentions, achieving $3.6\times$ ($1.1\times\sim 10.4\times$) over FlashSigmoid, PyTorch ReLU attention and PyTorch retention parallel. In addition, compared with programming-model-based approaches, \oursys{} can support all three customized attention, which demonstrates \oursys{}'s expressive ability and scalability.


\para{Mamba2.}
Figure \ref{fig:eval-fwd} (g) represents the linear attention operation of the Mamba2 model: State Space Module. We compare \oursys{} with the official Mamba2 implementation using Triton. 
\oursys{} achieves average speedups of $1.99\times$ and $1.65\times$ over Triton for Mamba2 forward and Mamba2 backward, respectively. This demonstrates the complexity of manually optimizing the attention kernel and the necessity of \oursys{}.

\para{Retention and gated retention.}
Figure \ref{fig:eval-fwd} (h)(i)(j) represents the linear attention operation of RetNet-6.7B, YOCO-13B and RFA-Big.We compare \oursys{} with Flash-Linear-Attention, which is an expert-optimized linear attention library. The result show that \oursys{} achieves average speedups of $1.33\times$ and $1.93\times$ for forward and backward, respectively.


\subsection{End-to-end Inference on NVIDIA H100}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figure/e2e_inference.pdf}
    \vspace{-5mm}
    \caption{End-to-end inference performance on H100.}
    \vspace{-5mm}
    \label{fig:end-to-end inference}
\end{figure}

We evaluate the inference latency of large language models like DeepSeek-V2-Lite and Mamba2-2.7B. We show \oursys{}'s applicability to end-to-end inference.

\para{Inference setup.} We evaluate end-to-end inference on one NVIDIA H100 GPU. We use Transformers\cite{wolf-etal-2020-transformers} for end-to-end inference, which is the most popular machine learning framework and is backed by PyTorch. We test two models with parallel pattern attention (Deepseek-V2-Lite and Diff-Transformer-3B ) and two models with recurrent pattern attention (Mamba2-2.7B and YOCO-160M).  We replace the attention operator in these models with \oursys{}.
% For Deepseek-V2-lite, We set the prefill length to 2048 and decode length from 2k to 8k. For Mamba2, we set the prefill length from 8k to 32k because Mambaâ€™s primary advantage over transformers is its superior computational efficiency with long sequence lengths, and these settings represent the most commonly used scenarios.

\para{Inference performance.} As shown in Figure \ref{fig:end-to-end inference}, \oursys{} acheive an average speedup of $1.4\times$ on these models with FP16 precision. These speedup came from our more efficient attention operator. For example, In DeepSpeed-V2-Lite, attention accounts for $85\%$ of the total inference time. We improved the attention operator's speed to $2.2\times$ by supporting different head dimensions for \texttt{q}, \texttt{k}, and \texttt{v}, thereby enhancing the end-to-end performance to $1.85\times$.
% For Deepseek-v2-lite and Diffrential-Transformer, our gain comes from the native support for different headdim qk and headdim v, campared with original flash-attention operator. For The triton kernel fails when length=32k while \oursys{} still works, which demonstrates the stability of our system. 


\subsection{End-to-end Training on NVIDIA H100}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figure/e2e_train.pdf}
    \vspace{-5mm}
    \caption{End-to-end training performance on H100.}
    \vspace{-5mm}
    \label{fig:end-to-end-train}
\end{figure}

We also evaluate end-to-end training of attention-based model and linear attention-based model to demonstrate \oursys{}'s ability in both forward and backward.

\para{Training setup.} We use TRL\cite{vonwerra2022trl} for training, which is a full stack library based on transformers that provides a set of tools to train transformer language models. Our workloads are Diff-Transformer-3B, YOCO-160M and ViT-S/16 with ReLU attention.

\para{Training performance.} As shown in Figure \ref{fig:end-to-end-train}, we achieve an average speedup of $1.4\times$ on these models. For ViT-S/16 with ReLU attention, we achieve $1.7\times$ speedup due to the lack of existing libraries for ReLU attention.

\vspace{-2mm}
\subsection{Evaluation on AMD ROCm GPUs}
\vspace{-2mm}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figure/mi250_eval.pdf}
    \vspace{-5mm}
    \caption{Attention operator performance on MI250 GPUs.}
    \vspace{-5mm}
    \label{fig:mi250-eval}
\end{figure}

We benchmark the AMD MI250 GPU using a subset of operators selected from the microbenchmark suite originally designed for the NVIDIA H100 GPU, including Softmax Attention, ReLU Attention, Mamba2 and RetNet Recurrent.

Figure \ref{fig:mi250-eval} shows that \oursys{} outperforms an average of $3.3\times$ for forward and $2.0\times$ for backward over other baselines across different attention operators. 
This demonstrated \oursys{}'s ability to support multi-backend.

