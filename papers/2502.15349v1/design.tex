%!TEX root = main.tex
\section{Design}
\label{sec:design}

% \S\ref{sec:moti}
% \oursys{} takes a user-defined attention algorithm as input and outputs an executable program on specialized hardware, such as a GPU.

% \oursys{} came out with two main abstractions: computing primitive and kernel template. 
% \oursys{} represents a user-defined attention algorithm by a set of computing primitives, which can be lowered to part of kernel template on hardware. Kernel template represents an execution flow on specialized hardware with some hardware-related configurations, such as tile size. To determine these hardware-related configurations, \oursys{} provides a tile-level autotuner to automatically get the best configurations.

% \oursys{} introduces a set of computing primitive, which is a set of operations which can be performed on hardware transparent to hardware-related details. 
% These primitive includes (1) elementwise op, such as add(), sub(), tanh(), etc. (2) row-reduce op , such as reduceSum(), reduceMax(), reduceAbssum().

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/overview.pdf}
    \caption{System overview: \oursys{} begins with attention templates in the Programming Interface to define Custom Attention. Then they are lowered to kernel templates and automatically scheduled to generate the best execution plan on the device.}
  \vspace{-5mm}
    \label{fig:overview}
\end{figure}

% \mlx{(need an overview)}
Expanding on our attention abstraction, we introduce \oursys{}, a unified framework designed to streamline the design, optimization, and execution of diverse attention mechanisms across hardware platforms. As shown in Figure~\ref{fig:overview}, \oursys{} begins with attention templates in the Programming Interface. These templates retain the core abstractions of attention—relevance scoring and aggregation—outlined in \S\ref{sec:attn} while providing customizable functions that allow users to design their own attention variants. By preserving the essential principles of attention and offering flexibility for user-defined extensions, \oursys{} facilitates the creation of a wide range of attention mechanisms and simplifies backend optimization.

Once customized, the attention mechanisms are lowered to \texttt{Kernel Templates}, which formalizes computation and memory operations. These templates, combined with two key components—\texttt{IntermediateTensor}, representing transient computational data, and \texttt{DeviceConfig}, capturing hardware constraints—define the scheduling space. \oursys{} employs a two-layer scheduling policy within this space to determine the optimal execution plan, balancing performance and resource utilization. The finalized plan is then mapped to hardware backends, ensuring scalability and efficiency across various configurations.

The following sections delve into the components of this framework, demonstrating how \oursys{} integrates abstraction, optimization, and execution to unify and extend the implementation of attention mechanisms.
% Building on the limitations and potential for innovation highlighted in \S\ref{sec:moti}, \oursys{} addresses these gaps by introducing a flexible framework for defining and optimizing attention mechanisms. It comprises three key components: the \textbf{programming interface}, which provides customizable \texttt{attention templates} incorporating user-defined functions (\texttt{modification} and \texttt{row-wise normalization}) and computing primitives; the \textbf{scheduling space}, which optimizes computation by focusing on intermediate tensors; and the \textbf{scheduling policy}, which employs a two-layer approach to minimize latency and computational cost. Together, these components enable efficient and adaptable attention computations across diverse mechanisms and hardware.



\subsection{Programming Interface}

% To realize the principles outlined in \S\ref{sec:moti}, we design a versatile programming interface that brings the abstraction of attention mechanisms into practice through customizable templates. These templates embody the core components of attention—relevance computation, normalization, and context compression—while accommodating components with either parallel or recurrent patterns.

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}{1.0\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figure/design_attn_1.png}
%         \caption{Transformer Attention}
%         \label{fig:transformer-attn}
%     \end{subfigure}
%     \vspace{1em} % 可调整两个图之间的间距
%     \begin{subfigure}{1.0\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figure/design_linear_1.png}
%         \caption{Linear Attention}
%         \label{fig:linear-attn}
%     \end{subfigure}
%     \caption{Visualization of \oursys{} interfaces within the computation flows of attention mechanisms.}
%     \label{fig:attention-mechanisms}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figure/transform_linear_diff-cropped.pdf}
%     \caption{Attention patterns.}
%     \label{fig:attn-pattern}
% \end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/attention_template.pdf}
  \vspace{-3mm}
    \caption{On the left is \oursys{}’s unified attention template. By instantiating this template, two distinct patterns are produced (Parallel Pattern and Recurrent Pattern). The red box highlights the operations corresponding to the core components of the attention mechanism in the unified attention template: \texttt{relevance\_scoring} and \texttt{aggregate}. Both the \texttt{customizable\_function} and the \texttt{mod} function are user-defined. The \texttt{customizable\_function} encompasses both \texttt{modification} function and \texttt{row-wise normalization} function, whereas the \texttt{mod} function is restricted to \texttt{modification} function only.}
  \vspace{-5mm}
    \label{fig:attn-template}
\end{figure*}

% \begin{figure}[t]
%     \centering
%     \begin{minipage}{0.55\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figure/parallelattn.pdf}
%         % \caption*{(a) Parallel Attention Compute Flow}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.4\linewidth}
%         \lstset{language=Python, basicstyle=\ttfamily\tiny,
%         lineskip=2pt}
%         \begin{lstlisting}
% def Parallel(Query, Keys, Value):
%     Q = Qmod(Query)
%     K = Kmod(Keys)
%     Scores = Q @ K
%     Scores = RowNorm(Scores)
%     V = Vmod(Values)
%     Output = Scores @ V
%     return Output
%         \end{lstlisting}
%         % \caption*{expression}
%     \end{minipage}
%     \caption*{(a) Parallel pattern}

%     \vspace{1cm} % Add space between rows

%     \begin{minipage}{0.55\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figure/recurrentattn.pdf}
%         % \caption{(b) Recurrent Attention Compute Flow}
%     \end{minipage}%
%     \hfill
%     \begin{minipage}{0.4\linewidth}
%         \lstset{language=Python, basicstyle=\ttfamily\tiny,
%         lineskip=2pt}
%         \begin{lstlisting}
% def Recurrent(
%     Query, Keys, Values
% ):
%     K = Kmod(Keys)
%     V = Vmod(Values)
%     for i in range(seqlen):
%         h[i] = K[i] @ V[i]
%             + Hmod(h[i-1])
%     Output = Qmod(Query) @ h
%     return Output
%         \end{lstlisting}
%         % \caption{Expression}
%     \end{minipage}
%     \caption*{(b) Recurrent pattern}

%     \caption{Attention templates for parallel pattern and recurrent pattern.}
%     \label{fig:attn-pattern}
% \end{figure}

\begin{figure}
    \begin{lstlisting}
class modification:
    Func mod: Tensor->Tensor;
    Bool ismask;
class row-wiseNormalization:
    Func online_prologue: Tensor->Tensor;
    Func online_fwd: Tensor->Tensor;
    Func online_epilogue: Tensor->Tensor;
    \end{lstlisting}
  \vspace{-3mm}
    \caption{Customizable functions in programming interface}
  \vspace{-5mm}
    \label{fig:interface-pseudocode}
\end{figure}

% \begin{figure}[t]
% \begin{lstlisting}
% def q_mod(query: Tensor, query_scales) -> Tensor;
% def k_mod(key: Tensor, key_scales) -> Tensor;
% def v_mod(value: Tensor, value_scales) -> Tensor;

% def score_mod(scores: Tensor, scales) -> Tensor;
% def score_rowreduce(scores: Tensor) -> Tensor;

% def decay_mod(decay: Tensor, decay_scales) -> Tensor;
% \end{lstlisting}
% \caption{Programming interface.}
% \label{fig:interface-pseudocode}
% \end{figure}


\para{Attention Patterns and Templates}
Building on our abstraction of attention operations—relevance scoring and aggregation—we design a unified attention template that serves as a versatile foundation for implementing diverse attention mechanisms. As depicted in Figure~\ref{fig:attn-template}, the template takes $Q$, $K$, and $V$ after projection as inputs, retaining two fixed computations: $Q@K$ for relevance scoring and $S@V$ for aggregation. These computations capture the essence of attention mechanisms while offering flexibility through customizable functions.

The template includes two key customizable functions, \texttt{modification} and \texttt{row-wise normalization}, which can be inserted at designated points to enable users to define attention variants tailored to specific needs. These functions allow for operations such as applying masking, implementing custom normalization schemes, or adapting to unique computational goals.

To facilitate optimization, this unified template is instantiated in two computational patterns—parallel and recurrent:
\begin{itemize}[noitemsep,topsep=0pt, left=0pt]
    \item Parallel Pattern: Relevance scoring and aggregation are implemented as matrix multiplications, with $Q@K$ representing the scoring and $S@V$ representing the aggregation. Customizable functions are applied to the relevance scores to compute weights and to the state to produce the final output. Since most existing parallel attention variants do not innovate on state transformations, the customizable function for the state often defaults to an identity operation. This pattern is well-suited for mechanisms requiring global context and high parallelism.
    \item Recurrent Pattern: Relevance scoring and aggregation are sequentially computed, with $K@V$ and $Q@h$ together capturing the relevance scoring and aggregation, iteratively maintaining compressed states. In this pattern, the customizable functions on weights and states are reformulated as customizable function on the hidden state $h$. This makes the recurrent pattern ideal for memory-efficient designs and tasks with sequence dependencies.
\end{itemize}

By integrating the two instantiated patterns, this unified attention template empowers users to design high-level attention mechanisms while \oursys{} seamlessly handles low-level implementation and hardware-specific optimization, ensuring both efficiency and scalability.

% Attention mechanisms consist of computational components that can adopt either a parallel or a recurrent pattern. \oursys{} leverages the abstract operations of the attention abstraction outlined in \S\ref{sec:moti} to design flexible attention templates that %seamlessly 
% align with these two primary computational patterns.

% Parallel patterns compute relevance for all tokens simultaneously, enabling global interactions and efficient normalization, while recurrent patterns process sequences iteratively, supporting state compression strategies that maintain contextual information across steps.

% This dual-pattern approach is closely tied to the three core abstract operations of attention:
% \begin{itemize}[noitemsep,topsep=0pt, left=0pt]
%     \item \textit{Relevance Computation}: Serves as the foundation for both patterns, with pairwise interactions forming the crux of attention mechanisms.
%     \item \textit{Normalization or Weighting}:  Handled through user-defined row-wise normalization functions, ensuring flexibility and computational efficiency.
%     \item \textit{Attention State Compression}:  Realized differently in each pattern, through row-wise normalization functions in parallel patterns or fixed states in recurrent patterns, ensuring adaptability to a wide range of attention designs.
% \end{itemize}
% To accommodate these patterns, \oursys{} offers customizable templates that abstract the computational structure of attention mechanisms. As depicted in Figure~\ref{fig:attn-pattern}, each template takes $Q$, $K$, and $V$ as inputs, preserving the relevance computation via an inner product. These templates include placeholders for user-defined functions, such as the \texttt{row-wise normalization} function for relevance weighting and the \texttt{modification} function for attention state transformation or compression.

% To further support the parallel and recurrent patterns, \oursys{} provides distinct templates tailored to each. The parallel template facilitates global pairwise interactions, making it ideal for mechanisms like quadratic attention. Conversely, the recurrent template supports iterative sequence processing, aligning with mechanisms such as linear attention. These templates empower users to design high-level attention mechanisms while \oursys{} handles low-level implementation and hardware-specific optimization, ensuring both efficiency and scalability.


\para{Customizable functions and flexibility.}
\label{subsec:func}
As shown in Figure~\ref{fig:interface-pseudocode}, customizable functions in \oursys{} include the \texttt{modification function} and the \texttt{row-wise normalization function}, which serve as user-defined components within the attention templates.

The \texttt{modification function} supports fine-grained elementwise transformations and masking, allowing users to customize operations applied to individual tensor elements. For example, scaling the query tensor by $1/\sqrt{d_k}$ in standard softmax attention can be achieved using this function. Masking operations, such as applying a causal mask, can also be implemented by annotating this function for masking purposes.

The \texttt{row-wise normalization function} provides a placeholder for normalizing or weighting. It enables global adjustments across tensor rows, accommodating a combination of elementwise and row-reduce computations. Examples include applying a row-wise softmax for normalizing attention scores or implementing numerical stabilization techniques. To enhance performance, the \texttt{row-wise normalization function} can be defined as an online function, where computations are processed sequentially in blocks along the rows. This approach significantly reduces memory overhead and ensures efficient execution.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figure/Online_softmax.pdf}
    \caption{Illustration of the online row-norm interface. The left panel shows the standard row-wise normalization function, while the right panel demonstrates how \oursys{} enables users to implement the same functionality as an online function using the online row-norm interface.}
  \vspace{-5mm}
    \label{fig:onlineFunc}
\end{figure}

The \texttt{online row-norm interface} facilitates the implementation of online row-wise normalization, inspired by FlashAttention\cite{dao2022flashattention}. As shown in Figure~\ref{fig:onlineFunc}, this interface includes three main components:
\begin{itemize}[noitemsep,topsep=0pt]
    \item \texttt{online\_prologue}, which initializes the state variables before entering the online loop.
    \item \texttt{online\_fwd}, which defines computations within each block of rows, updating state variables like row maxima or sums.
    \item \texttt{online\_epilogue}, which finalizes the computation after the loop.
\end{itemize}

Users can leverage this interface to construct both forward and backward computation graphs via the \texttt{forward} and \texttt{backward} methods, enabling seamless integration with automatic differentiation and backend optimization. Key variables, such as \texttt{online\_rowscales} (state variables passed between blocks) and \texttt{final\_rowscales} (storing the final reduction results), provide the flexibility to define new online functions beyond softmax, significantly broadening the scope of attention mechanisms supported by \oursys{}.

\para{Computing primitives.}
To simplify user-defined operations, we introduce a set of \texttt{computing primitives}, which abstract hardware-specific details. These primitives are categorized as:
\begin{itemize}[noitemsep,topsep=0pt]
    \item Elementwise Operations: Operations like \textit{add(), sub(), tanh()}, and others allow fine-grained transformations of individual tensor elements.
    \item Row Reduce Operations: Aggregation functions such as \textit{reduceSum(), reduceMax()}, and \textit{reduceAbssum()} enable efficient row-wise reduce computations.
\end{itemize}
These primitives provide a robust foundation for defining both modification and row-wise normalization functions, ensuring compatibility with diverse hardware platforms.





\begin{figure}[t]
\begin{lstlisting}
def online_prologue():
    row_max = -inf
    row_sum = 0
def online_fwd(row_max_prev,row_sum_prev):
    row_max_cur = scores.reduceMax()
    row_max = max(row_max_cur, row_max_prev)
    
    scores = exp(scores-row_max)
    row_sum_cur = scores.reduceSum()
    row_sum = row_max_prev/row_max * row_sum_prev +
                row_sum_cur

    return row_max, row_sum
def online_epilogue():
    scores = scores / row_sum
\end{lstlisting}
  \vspace{-3mm}
\caption{Example of online row-wise normalization function: softmax attention.}
  \vspace{-5mm}
\label{fig:softmax-example}
\end{figure}

\para{Examples of Attention variants.}
Our interfaces support a wide range of attention mechanisms, demonstrating their flexibility and generality.

The Softmax Attention mechanism involves scaling the query tensor by $1/\sqrt{d_k}$ for normalization and applying a numerically stable softmax function to the scores. Specifically, the modification interface for $q$ is defined as $q\_mod = lambda\  q: q / \sqrt{d_k}$, while the row-wise normalization interface on scores is expressed as: $score\_rownorm = lambda\ scores: (\_scores = \exp(scores - scores.\text{reduceMax()}); \_scores = \_scores / \_scores.\text{reduceSum()})$. To enhance performance, we implement the row-wise normalization function in an online form using our online row-norm interface (Figure~\ref{fig:softmax-example}). Specifically, \texttt{online\_prologue} initializes the state, \texttt{online\_fwd} performs intermediate computations on scores and state, and \texttt{online\_epilogue} finalizes the computation.

ReluAttention replaces the softmax function with a row-wise normalization function that contains only an elementwise operation. We use our modification interface on scores as $score\_mod = lambda\ scores: \max(scores, 0)$. In this case, no additional normalization is applied.

% ReluAttention replaces the softmax function with an elementwise $ReLU$ operation. In this case, the modification function for scores is defined as $scores = \max(scores, 0)$, while the row-wise normalization function is left as Identity.
Similarly, in RetNet parallel attention, a retention mask is applied to the scores. This is represented by the modification function $ score\_mod = lambda:\ scores = scores \times mask$, and a row-wise normalization function ensures numerical stability, defined as $score\_rownorm = lambda\ scores:  (scores / scores.\text{abs()}.\text{reduceSum()}.\text{clamp}(\text{min}=1))$.

Mamba2, a representative linear attention mechanism, incorporates a selective gating mechanism to modulate the key and hidden states, allowing selective attention to past information. Our interface represents this as a modification function on the key $k\_mod = lambda\ k: k \times gate$ and a modification function on the hidden states $h\_mod = lambda\ h: h \times decay \times gate$.
% \TODO{add sparse attention example}


\subsection{Scheduling Space}
The scheduling space in \oursys{} is inherently shaped by the kernel templates, which encapsulate the computation flow of attention mechanisms. These templates, derived from our attention pattern abstractions, constraining the range of scheduling options while enabling efficient and adaptable execution. Together with \texttt{IntermediateTensor} and \texttt{DeviceConfig} components, the kernel templates form the foundation for determining optimal execution strategies.

\para{Kernel template.}
Kernel templates play a pivotal role in structuring the scheduling space by formalizing the computation and memory operations of attention mechanisms. These templates provide a consistent structure for implementing diverse attention mechanisms while allowing flexibility for hardware-specific optimizations. For example, templates designed for parallel patterns incorporate online techniques to efficiently manage row-wise normalization, while those for recurrent patterns utilize chunk parallelism to maximize tensor core utilization and computational efficiency.

Additionally, \oursys{} supports multiple kernel templates tailored to different hardware backends, such as those implemented in Triton\cite{triton}, CUTE\cite{nvidia2024cutlass}, and TileLang\cite{tilelang}. Leveraging a common lowering method based on attention pattern abstractions, \oursys{} ensures that customized attention variants can be seamlessly lowered to these templates. This flexibility allows \oursys{} to dynamically select the optimal kernel template based on the input data and hardware platform, achieving consistent high performance across configurations.

\begin{figure}[t]
\begin{lstlisting}
class IntermediateTensor{
    TileShape tile;
    MemoryLocation mem;
    int pipelineStage;
};
\end{lstlisting}
  \vspace{-3mm}
\caption{IntermediateTensor component}
  \vspace{-5mm}
\label{fig:intermediate-tensor}
\end{figure}

\begin{figure}[t]
\begin{lstlisting}
class DeviceConfig{
    BaseTileShape basetile;
    List<MemoryCapacity> memoryInfo;
};
\end{lstlisting}
  \vspace{-3mm}
\caption{DeviceConfig component}
  \vspace{-5mm}
\label{fig:device-config}
\end{figure}

\para{IntermediateTensor.}
At the heart of the scheduling space lies the \texttt{IntermediateTensor} component, which encapsulates the transient data generated during computation. By focusing on intermediate tensors, \oursys{} can systematically deduce the tiling, memory allocation, and pipeline requirements for attention mechanisms.

Key attributes of \texttt{IntermediateTensor} include:
\begin{itemize}[noitemsep,topsep=0pt]
    \item Tensor tile shape (\texttt{tile}): By dividing tensors into smaller tiles, we can perform operations tile-by-tile and allocate buffers efficiently. Using the computation graph, we propagate the tiling scheme across all operations to infer the tile shapes of $Q$, $K$, $V$ and other tensors, ensuring an optimal balance between computation and memory.
    \item Tensor location (\texttt{mem}): Intermediate tensors can be stored in various levels of memory, such as global memory, shared memory, or registers. Each location offers a trade-off between latency, bandwidth, and resource availability. 
    \item Pipeline stage (\texttt{pipelineStage}): Operations involving intermediate tensors are divided into multiple pipeline stages, such as memory copy and computation. The number of stages determines the buffer requirements and scheduling flexibility, enabling overlapping operations to maximize throughput and minimize resource contention.
\end{itemize}
This component ensures that all elements of the attention mechanism, including inputs, outputs, and intermediate results, are unified under a consistent scheduling strategy.

\para{DeviceConfig.}
The \texttt{DeviceConfig} component provides hardware-specific constraints that refine the scheduling space defined by kernel templates and intermediate tensors. It encapsulates attributes such as:
\begin{itemize}[noitemsep,topsep=0pt]
    \item Base tile shape (\texttt{basetile}): Specifies the optimal tile shape for computations on the target hardware, ensuring alignment with hardware-specific constraints, such as alignment with GEMM computing instruction and memory transaction.
    \item Memory hierarchy (\texttt{memoryInfo}): Provides details about the available memory tiers (e.g., registers, shared memory, global memory) and their respective capacities, enabling efficient allocation and minimizing contention.

\end{itemize}

\texttt{DeviceConfig} plays a pivotal role in determining the feasible tiling and memory strategies during scheduling. For instance, the base tile shape ensures hardware-aligned tiling configurations, while memory capacity constraints prevent resource overcommitment.

By combining kernel templates, \texttt{IntermediateTensor}, and \texttt{DeviceConfig}, \oursys{} constructs a unified scheduling space that supports diverse attention mechanisms and hardware platforms. Kernel templates anchor the computation flow, \texttt{IntermediateTensor} defines the key computational attributes, and \texttt{DeviceConfig} introduces hardware constraints, together forming a robust and scalable scheduling framework.

\subsection{Scheduling policy}

As illustrated in Figure~\ref{alg:schedule}, \oursys{} employs a two-layer scheduling policy to minimize latency and optimize execution. This policy operates at two levels: \texttt{tile config scheduling} and \texttt{tile resource scheduling}. At the tile config scheduling level, the policy traverses the entire space of possible tile configurations, leveraging the constrained nature of the scheduling space to perform exhaustive exploration. At the tile resource scheduling level, the policy determines the optimal memory placement and execution strategy within each tile configuration, ensuring efficient hardware resource utilization while adhering to hardware constraints.


\begin{algorithm}[t]
    \SetKwProg{Fn}{Func}{}{end}
    \SetKwIF{If}{ElseIf}{Else}{if}{}{else if}{else}{end if}
    \Fn{TileConfigScheduling(g: Graph, D:DeviceConfig)}
    {
    tensor\_tile\_configs = InferPossibleTileConfigs(g, D.basetile);
    \label{alg:infer_tile}

    plans = []
    
    \For{tile\_config in tensor\_tile\_configs}{
        \label{alg:traverse_tileconfig}
        plans += TileResourceScheduling(tile\_config, g.IntermediateTensors , D);
        \label{alg:intra_tile}
    }
    \For{plan in plans}{
    \label{alg:traverseplans}
        \If{Profile(plan) < best\_latency}{
        \label{alg:profile}
          best\_latency = Profile(plan);
          best\_plan = plan;
        }
    }
    return best\_plan;
    }
    \Fn{TileResourceScheduling(tile\_config: TileConfig, t: IntermediateTensors, D:DeviceConfig)}
    {
        InitMemLocation(t.memLoc, REGISTER);
        \label{alg:initmemloc}
        
        t = sortByTensorSizeDec(t);
        \label{alg:sorttensor}

        \For{tensor\_i in t}{
        \label{alg:iter_tensor}
        
        plans = GeneratePlans(t);
        \label{alg:generateplans}
        
        \For{plan in plans}{
            \If{not ComputeMemoryConstraint(tile\_config, t, plan, D.memoryInfo)}{
            \label{alg:computememconstrain}
                plans.remove(plan);
            }
        }

        \If{not plans.isEmpty()}{
        \label{alg:plan_isempty}
            return plans;
        }
        
        
        LowerMemLocation(tensor\_i.memLoc)
        \label{alg:lowermemloc}
        } 
    return EmptySet();
    \label{alg:return}
    }
\caption{Scheduling algorithm}
\label{alg:schedule}
\end{algorithm}

\para{Tile config scheduling.} 
The tile config scheduling layer takes as input a computation graph (\texttt{Graph}) composed of \texttt{IntermediateTensor} objects and hardware configuration details (\texttt{DeviceConfig}). This layer begins by invoking the \texttt{InferPossibleTileConfigs} function (line \ref{alg:infer_tile}) to identify all potential tile configurations for the computation graph, propagating from the output tensors. Due to the complexity of attention mechanisms, including their intricate computation stages, hardware alignment requirements, and memory limitations, the tile configuration space is constrained. This enables an exhaustive traversal of all possible tile configurations.

For each tile configuration (line \ref{alg:traverse_tileconfig} - \ref{alg:intra_tile}), the policy generates a set of execution plans using the tile resource scheduling layer and evaluates their performance through profiling (line \ref{alg:traverseplans} -\ref{alg:profile}). Profiling involves calculating the latency of each plan to determine its efficiency. Finally, the tile configuration corresponding to the plan with the lowest latency is selected as the optimal configuration. 

\para{Tile resource scheduling.} 
The tile resource scheduling layer optimizes the execution plan for a specific tile configuration. The process starts by initializing all intermediate tensors to the highest memory tier available (e.g., registers) to reduce memory I/O overhead (line \ref{alg:initmemloc}). The intermediate tensors are sorted in descending order of size, prioritizing larger tensors for memory allocation to maximize efficiency (line \ref{alg:sorttensor}).

For each tensor, the policy iteratively generates execution plans (line \ref{alg:iter_tensor}-\ref{alg:return}) and checks their feasibility against hardware constraints, such as memory capacity and alignment requirements (line \ref{alg:computememconstrain}). If no valid plan is found, the policy progressively demotes tensors to lower memory tiers (e.g., shared or global memory) and reattempts plan generation (line \ref{alg:plan_isempty} -\ref{alg:lowermemloc}). This iterative adjustment continues until a feasible plan is identified or all options are exhausted. If no valid plan can be generated, the function returns an empty set (line \ref{alg:return}).

By combining the two layers, the scheduling policy systematically explores the design space to produce efficient, hardware-aware execution plans for attention computations. This hierarchical approach enables \oursys{} to balance performance and resource utilization, supporting diverse attention variants across multiple hardware platforms.





