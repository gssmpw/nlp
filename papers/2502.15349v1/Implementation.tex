%!TEX root = main.tex
\section{Implementation}
\label{sec:impl}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figure/attention_froge_Implementation-overview.pdf}
  \vspace{-3mm}
    \caption{Implementation of \oursys{}}
    \vspace{-5mm}
    \label{fig:implementation}
\end{figure}

In this section, we present the implementation of the \oursys{} frontend and backend. Figure~\ref{fig:implementation} provides an overview of the \oursys{} workflow. We intergrate \oursys{} into pytorch\cite{pytorch} as a module. The frontend accepts user-defined functions as input, constructs the computation graph of intermediate tensors by graph tracer, and passes it to the backend, which generates optimized device kernels for efficient execution.

\subsection{Frontend}
\label{sec:frontend}

The frontend of \oursys{} provides the foundation for defining and representing user-defined attention mechanisms. It introduces a set of computing primitives, facilitates the use of customizable functions such as modification and row-wise normalization. Furthermore, \oursys{} traces computation graphs by encoding tensor attributes, enabling automatic differentiation and efficient backend integration.

\para{Computing primitives.}
The frontend includes a rich set of computing primitives, categorized into elementwise and row-reduce operations. Elementwise operations, such as \textit{add(), sub(), tanh()}, are computed in a SIMT style on GPUs and are fused with matrix multiplications at the register or shared memory level to minimize memory access overhead. Row-reduce operations, such as \textit{reduceSum()} and \textit{reduceMax()}, leverage GPU warp-level reduction, where each row-reduce operation is computed by the same thread block and warp. 

\para{Modifaction function.} 
The modification function exclusively supports elementwise operations. These operations are fused by \oursys{} into a single computation unit and lowered to the backend kernel template for efficient execution. The modification function also supports masking operations, allowing users to implement attention variants that require masking logic.

\para{Row-wise normalization.}
The row-wise normalization function supports both elementwise and row-reduce operations or a combination of the two. Similar to the modification function, all operations within the row-wise normalization function are fused and lowered to backend kernel templates.
% Additionally, \oursys{} supports online row-wise normalization optimization via the online rowreduce interface.

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figure/Online_softmax.png}
%     \caption{Online function of row\_reduce API\TODO{caption}}
%     \label{fig:onlineFunc}
% \end{figure}
% To optimize row-wise operations, we design the \texttt{rowreduce} interface as an online function, dividing computations into sequential blocks along the rows (Figure~\ref{fig:onlineFunc}). Inspired by FlashAttention\cite{dao2022flashattention}, this design reduces memory overhead and enhances computational efficiency. The \texttt{rowreduce} interface includes components such as \texttt{online\_prologue}, which initializes the state before the online loop; \texttt{online\_fwd}, which defines computations within each block of rows; and \texttt{online\_epilogue}, which handles post-loop computations. Additionally, users can construct forward and backward computation graphs using the \texttt{forward} and \texttt{backward} methods. Key variables like \texttt{online\_rowscales}, representing state variables passed between blocks, and \texttt{final\_rowscales}, storing the final reduce results, enable flexible and efficient design. By leveraging this interface, users can extend the FlashAttention algorithm to define new online functions beyond softmax, expanding its applicability to diverse attention mechanisms.

% \subsubsection{Online rowreduce}

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figure/Online_softmax.png}
%     \caption{Online function of row\_reduce API\TODO{caption}}
%     \label{fig:onlineFunc}
% \end{figure}

% \begin{lstlisting}
% class Online_func{
%     list<scalar> rowscales;
%     list<scalar> final_rowscales;
%     scalar o_scale;
    
%     void online_prologue(list<scalar>& rowscales);
%     void online_fwd(Tensor& scores, 
%         list<scalar>& rowscales);
%     void online_epilogue(list<scalar>& final_rowscales, 
%         Tensor& output, list<scalar> rowscales);
    
%     void forward(Tensor& scores,
%         list<scalar> final_rowscales);
%     void backward(Tensor&dscores, Tensor scores, 
%         list<scalar> final_rowscales, scalar doosum);
% };
% \end{lstlisting}

% example: 
% \begin{lstlisting}
% // softmax attention
% void q_mod(Tensor& query, scalar scale){
%     query = query * scale;
% }
% class Online_func{
%     list<scalar> {row_max,row_sum};
%     list<scalar> {lse};
%     scalar o_scale;

%     void online_prologue(scalar& m,scalar& r){
%         m = -inf;
%         r = 0;
%     }
%     void online_fwd(Tensor& scores, 
%         scalar& row_max, scalar& row_sum){
%         row_max_new = row_max.max(scores.get_reduce("max"));
%         scale_tmp = (row_max - row_max_new).exp();
%         row_sum = row_sum * scale_tmp;
        
%         scores = (scores - row_max_new).exp()
%         row_sum = row_sum + scores.get_reduce("sum")
%         row_max = row_max_new;
%         o_scale = scale_tmp;
%         }
%     void online_epilogue(scalar& lse, 
%         Tensor& output, scalar row_max, scalar row_sum){
%         output = output / row_sum;
%         lse = row_sum.log() +row_max;
%         }
    
%     void forward(Tensor& scores,
%         scalar lse){
%         scores = (scores-lse).exp();
%         }
%     void backward(Tensor& dscores, Tensor scores, 
%         scalar lse, scalar doosum){
%         dscores = (dscores - doosum)*scores;
%         }
    
% }
% \end{lstlisting}

% To support efficient row-wise operations, we design the \texttt{rowreduce} interface as an online function, where the operation is divided into sequential blocks along the rows, as shown in Figure~\ref{fig:onlineFunc}. This design is inspired by optimizations used in FlashAttention, enabling online computation for improved performance.

% To facilitate user-defined online functions, we introduce several interfaces: \texttt{online\_prologue} for initializing the state before entering the online loop, \texttt{online\_fwd} for defining computations within each online block, and \texttt{online\_epilogue} for handling post-loop computations. Additionally, we define \texttt{forward} and \texttt{backward} interfaces to construct the forward and backward computation graphs. Key variables include \texttt{online\_rowscales}, which represents state variables passed between online blocks, and \texttt{final\_rowscales}, which stores the final reduce results after the online loop.

\para{Tracing user-defined computation graphs.}
\oursys{} traces user-defined computation graphs by building a directed acyclic graph of Tensor. Each node contains the computing primitive(such as $add()$, $reduceSum()$), the coresponding output Tensor attributes(such as shape), and a list of pointers pointing to its preceded node. This enables the system to dynamically trace the dependency between tensors.
We also define a $grad$ field on each node, which is a pointer to another node containing the gradient of current tensor. By iteratively traverse between nodes, we encodes the gradients informantion of each node into the $grad$ field to achieve automatic differentiation.
The forward and backward computing graph \oursys{} constructed ensures seamless integration with the backend for efficient kernel generation.

% \paragraph{Tracing User-Defined Computation Graphs}
% \oursys{} traces user-defined computation graphs by encoding  tensor attributes, such as shape, elementwise operations, and reduce operations. This tracing process enables the system to dynamically capture the relationships between operations and tensors, facilitating the construction of forward and backward computation graphs. These graphs allow \oursys{} to perform automatic differentiation and optimize operations, ensuring seamless integration with the backend for efficient kernel generation.

\subsection{Backend}
\label{sec:backend}
The backend of \oursys{} transforms user-defined algorithms into kernel templates and optimizes these templates into high-performance kernels.


\para{Kernel template.}
We design kernel templates to systematically implement attention lowering. Since most custom attention mechanisms preserve the overall kernel computation flow, a template-based approach is particularly effective. Using the computation graphs generated from the \texttt{modification} and \texttt{row-wise normalization} functions in \S\ref{sec:frontend}, we produce essential components such as intermediate tensor definitions, initialization routines, memory operations, and computation steps. These components are seamlessly fused into the kernel templates, ensuring computational and memory efficiency.

To achieve extreme performance, we leverage TileLang\cite{tilelang} and CUTE\cite{nvidia2024cutlass} to implement optimized kernel templates. For parallel attention, our templates employ advanced online techniques to handle row-wise normalization efficiently, ensuring adaptability across a wide range of configurations. For recurrent attention, we utilize chunk parallelism to fully exploit tensor cores, balancing computational throughput and efficiency. These strategies allow \oursys{} to accommodate the unique characteristics of different attention mechanisms while maximizing hardware utilization.

Handcrafted kernels, such as FlashAttention, are often limited to specific configurations, typically requiring \texttt{$d_{qk}$} to equal \texttt{$d_v$}. This highlights a key shortcoming of ad hoc approaches: they fail to address diverse input configurations without extensive manual tuning of schedules, such as tile sizes, pipelining, and fusion strategies—efforts that demand significant expertise. In contrast, \oursys{} automates this process, enabling support for various input configurations without manual intervention. Our kernel templates are designed to handle diverse configurations of \texttt{$d_{qk}$} and \texttt{$d_v$}, eliminating the need for padding when these dimensions differ, as seen in models like DeepSeek V2 (\texttt{$d_{qk}$}=192, \texttt{$d_v$}=128)\cite{deepseekai2024deepseekv2strongeconomicalefficient} and DiffTransformer (\texttt{$d_{qk}$}=128, \texttt{$d_v$}=256)\cite{ye2024differentialtransformer}. By reducing padding overhead and computation costs, \oursys{} not only extends support to a broader range of attention designs but also enhances performance while maintaining flexibility across hardware platforms.

\para{Lowering computation graphs to kernel templates.} The lowering process translates user-defined computation graphs into kernel templates. This process is divided into two stages: expression generation and code generation. The split design enhances extensibility, with expression generation being kernel-template-agnostic and code generation adapting to specific kernel templates.

During expression generation, \oursys{} inputs a user-defined computation graph and performs a topological sort to convert it into a linear sequence of computation expressions, preserving the computation order. Additionally, as the graph is traversed, the use-define chain for each node is analyzed, enabling optimizations such as variable reuse. In the subsequent code generation phase, these computation expressions are used to produce kernel code tailored to the selected kernel template through string matching. The resulting kernel code includes variable initialization, memory copying, and computation steps, seamlessly integrating user-defined operations into efficient kernel templates.


% cute: modified from flashattention; tl; forward; backward


% \subsubsection{autotuner}

% % AttentionEngine autotuner implement the scheduling policy defined in section 3. 
% We observe that for different custom attention, the shape vary, 导致最终的执行plan也不同. 因此我们采用autotuner，对于不同的输入形状，自动输出一个执行plan。

% \paragraph{memry prediction} 我们通过预测on-chip memory consumption并于实际硬件相比较，来prune掉占用内存过多的方案。对于可编程的scratchpad memory，例如 nvidia GPU的shared memory，我们可以根据kernel中的中间变量声明直接预测占用的大小; 对于register, 我们将同时出现的register上的中间变量作为register的占用大小。这样能够预测得到一个与实际使用register相近的估计，同时不需要花费大量时间进行实际编译。

% \paragraph{fusion of matmul} 对于当前的硬件例如nvidia H100， 矩阵乘法会以比thread更大的单元进行编程, 例如warpgroup(128 threads), 这破坏了SIMT的编程结构使得矩阵乘法与其他操作间的fuse出现困难。我们根据片上资源情况选择在 register fuse或者shared fuse。


\para{Map to hardware backend.}
We map the kernel templates to both NVIDIA GPUs and AMD GPUs, optimizing performance across diverse hardware platforms.

For NVIDIA GPUs, \oursys{} supports two backends: TileLang\cite{tilelang} and CUTE\cite{nvidia2024cutlass}. Using the Triton-like compiler, we map elementwise operations and reduce operations by utilizing APIs such as \texttt{ParallelFor} for thread-level execution and \texttt{reduce\_sum}/\texttt{reduce\_max} for block-level row-reduction. With CUTE, we employ \texttt{cute::Tensor} and \texttt{cute::layout} to define thread-level data layouts and map reduce operations to efficient micro-kernel templates, ensuring high performance for compute-intensive tasks.

For AMD GPUs, \oursys{} supports the MI250, AMD’s high-performance GPU architecture, equipped with Matrix Cores for matrix multiplication, Arithmetic Logic Units (ALUs), and asynchronous copy units for efficient memory transfer. Leveraging TileLang\cite{tilelang}’s capabilities, we generate highly optimized kernels tailored to the MI250, fully utilizing its advanced hardware features for efficient execution.

% \paragraph{hardware instrinc} leverage tl: mma instruction for gemm, warp level primitive for row\_reduce; mbarrier and wargroup sync for pipeline stage; register, shared memory, global memory for memory location.

% \paragraph{tile alignment} align tile size with hardware instruction. For example, H100 wgmma m64nNk16 with one warpgroup, so our tile size on mma m dim must be o multiple of (64*numwarpgroup\_m) 

% \_syncthreads() and fence.proxy for synchronization on different memory location.




% \subsection{(Backup Details)}

% \subsubsection{attention}

% \begin{lstlisting}
% def main(
%     q: [B,HQ,seqlen, D],
%     k: [B,HK,seqlen, D],
%     v: [B,HV,seqlen, DV],
%     o: [B,H, seqlen, DV]
%     ):
%      scores = q_mod(q) @ mod(k)
%      scores = score_mod(scores)
%      scores = row_func(scores)
%      o = scores @ v
% \end{lstlisting}

% \subsubsection{linear attention}

% \begin{lstlisting}
% def main(
%     q: [B,HQ,seqlen, D],
%     k: [B,HK,seqlen, D],
%     v: [B,HV,seqlen, DV],
%     o: [B,H, seqlen, DV]
%     ):
%     for i in range(seqlen):
%         h[i] = decay_mod(exp(decay[i])) * h[i-1] + k_mod(k[i]) @ v_mod(v[i])
%     o = q_mod(q) @ h
% \end{lstlisting}

% \subsection{Computing primitive}

% \oursys{} introduces a set of computing primitive, which is a set of operations which can be performed on hardware transparent to hardware-related details. 
% These primitive includes (1) elementwise op, such as add(), sub(), tanh(), etc. (2) row-reduce op , such as reduceSum(), reduceMax(), reduceAbssum().

% These primitive has two benefits: 1. can express algorithm of customized attention algorithm: (1) elementwise op in qmod, kmod, vmod, scoremod, decaymod, such as relu, sigmoid, multiply softmax scale (2) online forward in activation function, such as online softmax, online retention. These online func includes elementwise 
% op and row-reduce op.
% 2. can be lowered to efficient kernel template on specilized hardware. (1) elementwise op can be computed in SIMT style on GPU, and register fuse or shared fuse  with previous matmul. (2) row-reduce op can be computed by GPU 
% warpReduce, because each row-reduce is computed by the same threadblock and the same warp on GPU. Row-reduce op can be lowered to regsiter-level warp reduce.



% \subsection{kernel template}

% We implement a set of kernel template for attention and linear attention, which represent execution flow on hardware and the place where customized code is inserted.
% \begin{lstlisting}[language=python]
% shape = [batch, seq_len, heads, dim]
% shape_v = [batch, seq_len, heads, dimv]
% def attention(
%         Q: T.Buffer(shape, dtype), 
%         K: T.Buffer(shape, dtype), 
%         V: T.Buffer(shape_v, dtype), 
%         {{custom_fwd_inputs}}

%         Output: T.Buffer(shape_v, dtype),
%         {{final_rowscales_output }}
%     ):
%         with Kernel(seq_len // block_M, heads, batch) 
%         as (bx, by, bz):
%             Q_shared = T.alloc_shared([block_M, dim], dtype)
%             K_shared = T.alloc_shared([block_N, dim], dtype)
%             V_shared = T.alloc_shared([block_N, dimv], dtype)
%             {{custom_fwd_inputs_alloc_shared}}
            
%             scores_local = T.alloc_fragment
%             ([block_M, block_N], accum_dtype)
%             o_local = T.alloc_fragment
%             ([block_M, dimv], accum_dtype)
            
%             {{custom_fwd_inputs_alloc_register}}

%             copy(Q, Q_shared)
            
%             {{copy_before_loop}}
            
%             for i in range(seqlen//block_N):
%                 copy(K, K_shared)
%                 gemm(q_shared, k_shared, scores_local)
                
%                 {{copy_in_loop}}
                
%                 {{scores_out}} = score_mod
%                 (scores_local, {{score_mod_inputs_list}})

%                 {{move scores_out}}

%                 {{scores_out}}, {{online_rowscale}}, {{o_scale}} = 
%                 online_fwd(scores_local, {{online_fwd_inputs_list}})

%                 {{move scores_out}}

%                 for i, j in Parallel(block_M, dimv):
%                    o_local[i, j] *= {{o_scale}}[i]
                
%                 # T.copy(tmp1,row_sum)
%                 {{move online_rowscale}}
                
%                 T.copy(V, V_shared)
%                 gemm(scores_local, v_shared, o_local)
%             T.copy(o_local,o)
%             {{copy after loop}}
                    
                 
% \end{lstlisting}

% \begin{lstlisting}
% BT = 64
% def linear_attention(
%     q: T.Buffer((batch,headq,seqlen,dim), dtype), 
%     k: T.Buffer((batch,headk,seqlen,dim), dtype), 
%     v: T.Buffer((batch,head,seqlen,dimv), dtype), 
%     g: T.Buffer((batch,head,seqlen), accum_dtype), 
    
%     h: T.Buffer((batch, head, (seqlen//BT)*dim, dimv), dtype),
%     o: T.Buffer((batch,head,seqlen,dimv), dtype), 
% ):
%     with Kernel(dim//BK, dimv//BV, batch * head)
%     as (bx, by, bz):
%         h_shared = T.alloc_shared((BK, BV), dtype)
%         h_local = T.alloc_fragment((BK, BV), accum_dtype)
%         k_local = T.alloc_fragment((BT, BK), dtype)
%         k_shared = T.alloc_shared((BT, BK), dtype)
%         v_shared = T.alloc_shared((BT, BV), dtype)

%         for it in range(seqlen//BT):
%             gemm(k_shared, v_shared, h_local)

%         copy(h_local, h)

%     with Kernel(dimv//BV, seqlen//BT, batch * head) 
%     as (bx, by, bz):
%         q_shared = T.alloc_shared((BT, BK), dtype)
%         k_shared = T.alloc_shared((BT, BK), dtype)
%         v_shared = T.alloc_shared((BT, BV), dtype)
%         h_shared = T.alloc_shared((BK,BV), dtype)

%         T.copy(q, qshared)
        
%         for ik in range(dim//BK):
%             T.copy(k,k_shared)
%             gemm(q_shared, k_shared, s_local, transpose_B=True)
%             T.copy(h,h_shared)
%             gemm(q_shared, h_shared, o_local, transpose_B=False)
        
%         T.gemm(s_local, v_shared, o_local)
        
%         copy(o_local, o) 

% \end{lstlisting}


% 1. memory allocation : global memory allocation, shared memory allocation, register allocation for customized tensor. For example, retnet scores[B,H,seqlenq,seqlenkv] * mask[B,H,seqlenq,seqlenkv], mask need maskGlobal[B,H,seqlenq,seqlenkv], maskShared[blockM,blockN], maskLocal[x,x]. sigmoid attention scores[B,H,seqlenq,seqlenkv] + bias[B,H], bias need biasGlobal[B,H], biasLocal[1]
 
% 2. memory copy: (1) memory copy between different-level memory:  For different customized tensor shape, memory copy place is different. For example, retnet mask need to be load every loop, sigmoid bias load at the start of the kernel (2) memory copy on the same level of memory: custom func update some variables in templates, and need to copy the updated variables back to the template defined var. 

% 3. computing pipeline: tile-level computing op: mma, copy, elementwise. (1) attention fwd: mma, onlineFunc, mma; (2) linear attention fwd: mma, elementwise; mma, mma,mma, elementwise.

% \subsubsection{Linear attention template}

% To use tensorcore on GPU, we convert the linear attention compute to  chunk-wise with extra tflops but faster hardware unit.(flash-linear-attention)


% \begin{lstlisting}
% // compute tflops interchunk: 2*seqlen*(Dstate*D+Dstate*D)
% // compute tflops intrachunk extra: 
% // 2*seqlen*(CHUNK_SIZE*Dstate+CHUNK_SIZE*D) 
%     def main(
%     q: [seqlen, Dstate],
%     k: [seqlen, Dstate],
%     v: [seqlen, D],
%     o: [seqlen, D]
%     ):
%     for i in range(seqlen//CHUNK_SIZE):
%         CHUNK = i*CHUNK_SIZE:(i+1)*CHUNK_SIZE
%         hidden_state[Dstate,D] = decay[i] * hidden_state + 
%         k[CHUNK].T 
%         @ v[CHUNK]
%         o_1[CHUNK] = q[CHUNK] @ hidden_state
%         o_2[CHUNK] = (q[CHUNK] @ k[CHUNK].T * DECAY_MATRIX) 
%         @ v[CHUNK]
%         o[CHUNK] = o_1 + o_2
        
% \end{lstlisting}

% To better utilize the hardware compute unit, we split compute of o into a seperate kernel.
% For example, for batch=4, head=24, while H100 has 132 SMs.


% \begin{lstlisting}
%     def main(
%     q: [seqlen, Dstate],
%     k: [seqlen, Dstate],
%     v: [seqlen, D],
%     o: [seqlen, D]
%     ):
%     hidden_state: [seqlen//CHUNK_SIZE, Dstate, D]
    
%     for i in range(seqlen//CHUNK_SIZE):
%         CHUNK = i*CHUNK_SIZE:(i+1)*CHUNK_SIZE
%         hidden_state[i,Dstate,D] = 
%         decay[i] * hidden_state[i-1] + 
%         k[CHUNK].T @ v[CHUNK]

%     for i in Parallel(seqlen//CHUNK_SIZE):
%         CHUNK = i*CHUNK_SIZE:(i+1)*CHUNK_SIZE
%         o_1[CHUNK] = q[CHUNK] @ hidden_state[i]
%         o_2[CHUNK] = (q[CHUNK] @ k[CHUNK].T * DECAY_MATRIX) 
%         @ v[CHUNK]
%         o[CHUNK] = o_1 + o_2
        
% \end{lstlisting}

% benefits: better parallelism (1) larger grid dim on compute "o" (2) less on-chip register usage , so more block on a multiprocessor.

% For the extra memory-IO caused by hidden state, we tune the chunk size to get the best trade off between compute tflops/utilization and memoryIO.
% chunk size larger, compute tflops larger, more on-chip resource, but smaller memory IO and better parallelism.

% \begin{lstlisting}
% // chunk_h tflops:
%     2*seqlen*(D*Dsate)
% // chunk_h IO:
%     sizeof(dtype)* (seqlen * (D+Dstate) + seqlen/CHUNK_SIZE*D*Dstate)
% // chunk_o tflops:
%     2*seqlen*(CHUNK_SIZE*Dstate+CHUNK_SIZE*D + D*Dsate)
% // chunk_o IO:
%     sizeof(dtype)* (seqlen * (D+Dstate+Dstate) + seqlen/CHUNK_SIZE*D*Dstate)

% \end{lstlisting}

% For each kernel, chunk\_h and chunk\_o, we tune different tile seperately to utilize GPU.

% chunk\_h tile on Dstate and D. FOr example, when Dstate = 128, D = 64, batch=1, head=80, TILE\_K and TILE\_V set to 64 and 32, to launch grid 80*2*2 to utilize GPU SM; when state = 128, D = 64, batch=8, head=80, TILE\_K and TILE\_V set to 128 and 64, to utilize GPU on chip resource and reduce memory IO.

% chunk\_o tile on D and seqlen(CHUNK). TILE\_V is always set to D for less compute.


% \begin{lstlisting}
%     def main(
%     q: [seqlen, Dstate],
%     k: [seqlen, Dstate],
%     v: [seqlen, D],
%     o: [seqlen, D]
%     ):
%     hidden_state: [seqlen//CHUNK_SIZE, Dstate, D]

%     for i1, i2 in Parallel(Dstate//TILEK, D//TILEV):
%     for i in range(seqlen//CHUNK_SIZE):
%         CHUNK = i*CHUNK_SIZE:(i+1)*CHUNK_SIZE
%         hidden_state[i,i1*TILEK,i2*TILEV] = 
%         decay[i] * hidden_state[i-1] + 
%         k[CHUNK].T @ v[CHUNK]

%     for i1, i in Parallel(D/TILE\_V,seqlen//CHUNK_SIZE):
%         CHUNK = i*CHUNK_SIZE:(i+1)*CHUNK_SIZE
%         o_1[CHUNK] = q[CHUNK] @ hidden_state[i]
%         o_2[CHUNK] = (q[CHUNK] @ k[CHUNK].T * DECAY_MATRIX) 
%         @ v[CHUNK,i1*TILE_V]
%         o[CHUNK] = o_1 + o_2

% \end{lstlisting}





% \subsection{from kernel template to kernel instance}

% \subsubsection{Decider}

% Decider decides whether the customized attention algorithm need to be fused according to the input shape and output a list of fusing configs. 

% Constraits that limit the fusing configs(tile size, stages, etc): 1. hardware instruction atom, for example H100 wgmma with tile M64N8K16 thread128. 
% 2. shared memory size
% 3. register size
% 4. input shape: for attention, when hidden dim is too large, fail to fuse.

% \subsubsection{autotuner}

% \oursys{} construct a tuning space for different input shape, different kernel template and different hardware. 
% 1. tile size: \oursys{} select tile size that align with hardware, and tune these tile size to achieve trade-off between better data reuse and limited hardware onchip resource.
% 2. memory location: the intermediate result can be put on shared memory or register file.
% 3. pipeline stage: to overlap memory copy and compute, but cost more onchip shared memory.

% \oursys{} compile kernel instance with different configs in parallel, profile each kernel, return the kernel with the best performance.




