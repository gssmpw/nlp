% \vspace{-2mm}
\section{Conclusion}
\label{sec:conclusion}
% \vspace{-2mm}

Attention mechanisms are central to transformers and large language models (LLMs), driving advancements in natural language processing by capturing contextual relationships. However, their computational demands and growing design diversity pose challenges for scalability and optimization. \oursys{} addresses these issues by abstracting attention into two core operations, i.e., relevance scoring and aggregation, and introducing customizable templates that combine flexibility with efficiency. With a cross-backend scheduling framework, \oursys{} automates kernel optimizations, achieving up to 10.4$\times$ speedups for unsupported configurations and providing a foundation for diverse attention designs.