To address our research questions, we used a mixed-methods within-subjects study design.
\subsection{Evaluation Methods}
\subsubsection{Questionnaires}
Up to date, there is no standardized way of assessing the efficiency of prebunking interventions. Therefore, we specify the quantitative assessment %of the efficiency of the game 
based on Tifferet’s taxonomy of existing approaches \cite{tifferet2021verifying} for evaluating misinformation susceptibility. Tifferet categorizes measurements into three main groups: performance tasks (how well users can discriminate between fake and real pieces of news), general media literacy assessment (how much a person knows about different aspects of misinformation), and behavioral assessment (how much a person would like to use different strategies to verify online information) \cite{tifferet2021verifying}. As Tifferet argues, these three aspects are complementary to understanding user’s susceptibility to misinformation and, therefore, evaluation of improvement in each of them could provide a full picture of our game's efficiency.
As the game was designed to present complex and multifaceted scenarios to the users, we wanted to assess how confident the participants were in their abilities to recognize misinformation before and after the intervention to see if the received knowledge increased or decreased their perceived self-efficacy in the topic.
In the study, we used the validated questionnaires dedicated to these aspects.%different aspects of media literacy}

{Performance Assessment}: Misinformation Susceptibility Test (MIST - 20). To assess changes in veracity discernment (the ability to recognize fake news from real news), we used the Misinformation Susceptibility Test (MIST) \cite{maertens2024misinformation}. 
To date, the MIST inventory is the only fully validated misinformation susceptibility instrument. It takes into account the ability to recognize real news and fake news presented in equal proportion. The MIST framework is designed to allow for the comparison of results across different studies and interventions. The test has been implemented in multiple misinformation intervention assessment studies (e.g. \cite{roozenbeek2022psychological,spampatti2024psychological}), including media literacy/misinformation games  \cite{bradstreet2023data,wells2024doomscroll}. In the current study, we applied the MIST-20 version, which includes 20 items. Participants were asked to rate each presented item as either a "fake" or "real" news headline.

\textbf{{Literacy Assessment:} New Media Literacy Scale}.
To assess changes in media literacy, we used the New Media Literacy Scale (NMLS), developed by Koc and Barut \cite{koc2016development}. The scale is designed to measure literacy in "new media" (digital media and social networks) and is based on the four factors from the model by Chen et al.: functional consuming, critical consuming, functional prosuming, and critical prosuming \cite{chenwu2011unpacking}. 
Our motivation for choosing the scale was due to the game being designed to actively engage users in content generation, and thus it is necessary to assess both the changes in the user’s information consumption and in their information prosumption (production and consumption). 
The NMLS is the only scale which provides this dimension, compared to other Media Literacy assessing instruments like (\cite{eristi2017development, vraga2015multi, ashley2013developing}) 
Additionally, the scale was developed and validated on the group of university students, which reflected our projected sample (mostly a young population with university degrees).
The questionnaire includes 35 questions, rated on a 5-point Likert scale, ranging from "strongly disagree" to "strongly agree."

\textbf{{Behavioural Assessment:} Verifying Online Information Scale (VOI - 7)}.
To assess the effect of the game on the verification practices performed by the participants, we adapted the Verifying Online Information self-report scale (VOI) proposed by Tifferet \cite{tifferet2021verifying}. 
To the best of our knowledge, this is the only existing scale which focuses on the behaviors (verification practices) a person can adopt to verify the news.
The questionnaire measures individuals' differences in applying direct and indirect verification practices for online information, allowing us to track expected behavior changes in verification practices. We used the VOI-7 version, which demonstrated comparable construct characteristics to the original 22-question version while allowing to be completed more rapidly. The parameters were measured on a slider from 0 to 100, where participants were asked to indicate their likelihood of applying verification practices \cite{tifferet2021verifying}.

\textbf{{Self-efficacy Assesement: }Fake News Self-efficacy Scale}. 
To measure perceived self-efficacy in dealing with fake news, we used a 3-item questionnaire developed by Hopp \cite{hopp2022fake}. This questionnaire assessed participants' confidence in three key areas: (1) their ability to identify news-like information that may be intentionally misleading, (2) their ability to distinguish between fake news and content produced with honest intentions, and (3) their ability to recognize news that may be unintentionally incorrect (i.e., misinformation). We chose the scale as a better alternative to the non-validated single-item measurement of confidence in identifying fake news, used by \cite{hinsley2021fake}.
Each item was rated on a seven-point scale, from "strongly disagree" to "strongly agree".

\subsubsection{Semi-structured Interview}

To evaluate the user's experience in-depth, connected with the content of the game and the strategies implemented by users, we developed a protocol for a semi-structured interview. This protocol includes questions about the general experience, the perceived goal of the game, the perception of the opponent's strategies, and the individual's perception of the game's effectiveness or ineffectiveness. The guidelines for the semi-structured interview are presented in the Supplementary material.

 %CONSENT, ANONYMOUS DATA COLLECTION, etc.
\subsubsection{Log analysis}
The game logs collected the data including player-generated content, the time spent in each round, API responses showing the public opinion of different personas, trust level scores, and in-game events such as the amount of money players had, how much they spent, and what hints they purchased. This data provided a %precise 
transcript of each session, enabling the research team to analyze players’ strategies, in-game behaviors, and decision-making processes.
\subsubsection{Qualitative data analysis}
We employed a combined inductive-deductive approach to analyze the qualitative interview transcripts and gameplay logs\cite{kuckartz2019analyzing}. This approach ensured a comprehensive understanding of the gameplay experience. Our primary objectives were to gain understanding of how participants perceived and understood misinformation through the game, how they learned to distinguish and apply debunking strategies during gameplay, and how interactions with other players influenced their behavior and learning. The analysis process began with inductive coding. Two researchers independently coded a subset of the data, identified themes, and then discussed and reconciled any coding discrepancies, iterating on the coding system as needed. Once the coding system was established, the two researchers independently coded the full dataset. A third researcher then reviewed the coded data, and any differences in interpretation were discussed until a consensus was reached.
\subsection{Recruitment and Participants}
Participants were recruited through flyers distributed in university-affiliated online media groups. We also encouraged participants to share information about the study within their social media networks using a snowballing technique. The eligibility criteria required participants to be adults and have sufficient English proficiency to play the game (we also do not forbid using translation engines if any of the aspects of the game are not understandable). Given that the proliferation of online misinformation is a global challenge and commonly reaches unsuspecting users\cite{ferrara2020misinformation}, even people encountered misinformation, they are not necesserily knows about it if the information was not debunked later. Thus 
we did not require participants to have prior exposure.
%Instead, we assumed they had already encountered misinformation, especially in the aftermath of the COVID-19 pandemic.

60 participants initially expressed interest in the study by completing an online questionnaire that collected demographic information and availability. Ultimately, 47 participants were selected, forming 24 pairs for the game sessions. In one of the pairs, one of the study's authors participated in a player role due to scheduling reasons. Because the data collection form allowed participants to skip questions they preferred not to answer, we noticed that 3 participants did not complete the entire MIST questionnaire, and 5 participants left some questions blank in the pre-procedural VOI questionnaire. Therefore, their data were excluded from the VOI and MIST data analyses.
The study sample had the following characteristics: the participants’ ages ranged from 20 to 57, with a mean age of 25.87 years (SD = 6.265). 28 participants identified as female, 18 as male, and 1 preferred not to disclose their gender. 3 participants reported having an Associate degree, 29 a Bachelor's degree, and 15 a Master's degree; all participants reported having Eastern Asian origin (see participant demographic information in \ref{Demographic Information of Participants}).

\subsection{Procedure}
%with repeated measures.
Once a person expressed interest and agreed to participate, they completed an initial questionnaire, providing demographic information and responding to the misinformation-related questionnaires. To prevent participants from intentionally biasing their responses, the questionnaire was administered 7-10 days before the gameplay experiment. After participants confirmed completion of the questionnaire, we scheduled the gameplay sessions. These experimental sessions were conducted either online via the VooV Meeting application or in person at a university meeting room. At the start of their test session, participants were given information sheets and consent forms to review and complete at their own pace. Once completed, Participants were introduced to the game setup and roles, and when they decided between themselves which role they would like to play. After the gaming session, participants again filled out the questionnaires. Finally, we conducted a short semi-structured interview to discuss their perceptions of the game. The entire session last approximately one and a half hours (See figure 7). 
%including an additional questionnaire about their game experience
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/study_procedure_RR.jpg}
    \caption{Overview of study procedure.}
    \label{fig:study procedure}
\end{figure}

%\subsection{Questionnaires}

%\textcolor{blue}{We specify the quantitative assessment of the efficiency of the game based on Tifferet’s taxonomy of existing approaches \cite{tifferet2021verifying} for evaluating misinformation susceptibility. In her work, Tifferet proposes dividing the measurements into three main groups: performance tasks (news evaluation tasks assessing how well users can discriminate between fake and real pieces of news), general media literacy assessment (how much a person knows about different aspects of misinformation), and behavioural assessment (how much a person would like to use different strategies to verify online information) \cite{tifferet2021verifying}. As Tifferet argued, these three aspects are complementary to understanding the full picture of a user’s susceptibility to misinformation and, therefore, the effectiveness of our intervention. 
%As the game was designed to present complex and multifaceted scenarios to the users, we wanted to assess how confident the participants were in their abilities to recognize misinformation before and after the intervention to see if the received knowledge increased or decreased their confidence in the topic.
%In the study, we used the validated questionnaires dedicated to these aspects.%different aspects of media literacy}
%}
%\textcolor{red}{\sout{use of verification practices, general media literacy regarding online social media, self-confidence in interaction with misinformation and ability to distinguish misinformation from real information. All of them were administered twice: once during the recruitment phase and again after the participants finished the game.}}

%\textbf{\textcolor{blue}{Performance Assessment}: Misinformation Susceptibility Test (MIST - 20)}. To assess changes in veracity discernment (the ability to recognize fake news from real news), we used the Misinformation Susceptibility Test (MIST) \cite{maertens2024misinformation}. 
%\textcolor{blue}{To date, the MIST inventory is the only fully validated misinformation susceptibility instrument. It takes into account the ability to recognize real news and fake news presented in equal proportion. The MIST framework is designed to allow for the comparison of results across different studies and interventions.} The test \textcolor{red}{\sout{was designed as an intervention evaluation framework and}} has been implemented in multiple misinformation intervention assessment studies \textcolor{blue}{(e.g. \cite{roozenbeek2022psychological,spampatti2024psychological})}, including media literacy/misinformation games  \textcolor{blue}{\cite{bradstreet2023data,wells2024doomscroll}}. \textcolor{red}{\sout{These applications provide the opportunity to discuss the efficiency of the intervention in a broader context.}} In the current study, we applied the MIST-20 version, which includes 20 items. Participants were asked to rate each presented item as either a "fake" or "real" news headline.

%\textbf{\textcolor{blue}{Literacy Assessment:} New Media Literacy Scale}.
%To assess changes in media literacy, we used the New Media Literacy Scale (NMLS), developed by Koc and Barut \cite{koc2016development}. The scale is designed to measure literacy in "new media" (digital media and social networks) and is based on the four factors from the model by Chen et al.: functional consuming, critical consuming, functional prosuming, and critical prosuming \cite{chenwu2011unpacking}. 
%\textcolor{blue}{We chose the scale based on the fact, that as the game was designed to actively engage users in content generation, it will be necessary to assess both the changes in the user’s information consuming and in the information prosuming (production); 
%the NMLS is the only scale which provides this dimension, compared to other Media Literacy assessing instruments like (\cite{eristi2017development, vraga2015multi, ashley2013developing}) 
%Additionally, the scale was developed and validated on the group of university students, which reflected our projected sample (mostly young population with university degrees)}.
%The questionnaire includes 35 questions, rated on a 5-point Likert scale, ranging from "strongly disagree" to "strongly agree."

%\textbf{\textcolor{blue}{Behavioural Assessment:} Verifying Online Information Scale (VOI - 7)}.
%To assess the effect of the game on the verification practices performed by the participants, we adapted the Verifying Online Information self-report scale (VOI) proposed by Tifferet \cite{tifferet2021verifying}. 
%\textcolor{blue}{To the best of our knowledge, this is the only existing scale which solemnly focuses on the behaviours (verification practices) a person can take to verify the news.}
%The questionnaire measures individuals' differences in applying direct and indirect verification practices for online information, allowing us to track expected behaviour changes in verification practices. We used the VOI-7 version, which demonstrated comparable construct characteristics to the original 22-question version while allowing for faster questionnaire administration. The parameters were measured on a slider from 0 to 100, where participants were asked to indicate their likelihood of applying verification practices \cite{tifferet2021verifying}.

%\textbf{\textcolor{blue}{Confidence Assesement: }Fake News Self-efficacy Scale}. 
%To measure perceived self-efficacy in dealing with fake news, we used a 3-item questionnaire developed by Hopp \cite{hopp2022fake}. This questionnaire assessed participants' confidence in three key areas: (1) their ability to identify news-like information that may be intentionally misleading, (2) their ability to distinguish between fake news and content produced with honest intentions, and (3) their ability to recognize news that may be unintentionally incorrect (i.e., misinformation). \textcolor{blue}{We chose the scale as a better alternative to the non-validated single-item measurement of confidence in identifying fake news, used by \cite{hinsley2021fake}}.
%Each item was rated on a seven-point scale, from "strongly disagree" to "strongly agree".

%\textbf{Evaluation of Educational Games Questionnaire}
%For evaluating the general and learning experience with the game, we implemented subscales of Motivation and User Experience from a questionnaire based on the Model for the Evaluation of Educational Games (MEEGA) proposed by Savi et al. \cite{savi2011model}, and developed by Petri et al. \cite{petri2017large}.
%We included questions, regarding the Attention, Relevance, Confidence, Satisfaction, Immersion, Social Interaction and Challenge \footnote{Based on the inconsistency in factor structure, mentioned in original paper, we selected only 1 question for measuring Challenge}, as the most relevant to our study, which included social interaction, but not necessarily be related to the 
%The questionnaire consists of 17 items, measured on a 5-point Likert scale (from "strongly disagree" to "strongly agree").
%\subsubsection{Semi-structured Interview}

%To evaluate the user's experience in-depth, connected with the content of the game and the strategies implemented by users, we developed a protocol for a semi-structured interview. This protocol includes questions about the general experience, the perceived goal of the game, the perception of the opponent's strategies, and the individual's perception of the game's effectiveness or ineffectiveness. The guidelines for the semi-structured interview are presented in the Supplementary material.

 %CONSENT, ANONYMOUS DATA COLLECTION, etc.
%\subsubsection{Log analysis}
%The game logs collected the data including player-generated content, the time spent in each round, API responses showing the public opinion of different personas, trust level scores, and in-game events such as the amount of money players had, how much they spent, and what hints they purchased. This data provided a %precise 
%transcript of each session, enabling the research team to analyze players’ strategies, in-game behaviors, decision-making processes.

%The analysis was designed to answer RQ2, focusing on qualitative coding of interview data, game logs, and researcher observation notes. Two members of the research team came up with two codebooks together on the same set of experimental data that were randomly picked. After discussing and finalizing this codebook, the rest of the dataset was coded using the same framework and processed by MAXQDA. Any identifying information was anonymized as needed.
%Observed themes and patterns from gameplay
%Chat Logs & Telemetry Data Telemetry data collected from the DoomScroll sessions include chat logs, match duration, player roles, impostor kills, votes, meeting eliminations, evidence source usage, and final win conditions. This in-game data provided the research team with a clear transcript of each match, allowing for in-depth analysis of the events and how they might reflect players’ information-processing and decision-making.

%Analysis for RQ2 and RQ3 involved qualitatively coding the interviews, telemetry data, and researcher observations for common patterns and themes. The a priori codes used for this analysis can be found in Appendix F. In particular, the research team deductively analyzed the data for commonly used
%misinformation tactics and instances of lateral reading, as well as inductively analyzed for any other observed tactics that do not fit within the established categories. Two members of the research team independently coded one interview transcript and one chat log/telemetry data transcript, then met to discuss and adjust their codes until a finalized coding scheme was reached. The lead researcher then coded the rest of the data using that scheme. All qualitative analyses were conducted in the Dedoose analysis software. All identifying information has been anonymized using pseudonyms where necessary.

%(Examples from other paper!! DO NOT directly use it)

%\subsubsection{Qualitative data analysis}
%We employed a combined inductive-deductive approach to analyze the qualitative interview transcripts and gameplay logs\cite{kuckartz2019analyzing}. This approach ensured a comprehensive understanding of the gameplay experience. Our primary objectives were to understanding how participants perceived and understood misinformation through the game, how they learned to distinguish and apply debunking strategies during gameplay, and how interactions with other players influenced their behavior and learning. The analysis process began with inductive coding. Two researchers independently coded a subset of the data, identified themes, and then discussed and reconciled any coding discrepancies, iterating on the coding system as needed. Once the coding system was established, the two researchers independently coded the full dataset. A third researcher then reviewed the coded data, and any differences in interpretation were discussed until a consensus was reached.

\subsection{Ethical Consideration}
The experimental design was approved by the Ethics Review Panel of (ANONYMIZED). As the game story was centered around a fictional pandemic, we informed participants about the theme in the consent form and asked them not to participate in the study if they perceived the topic of health/diseases to be disturbing.  All participants gave their informed consent and were compensated 40 Chinese Renminbi upon completion of the game and interviews.