\section{Related Work}
\subsection{Prompt Engineering Agents}

Several recent works focus on developing agents that execute actions in smartphone or desktop environments in order to complete textual commands. With the advancement of foundation models, the research community has been exploring ways to leverage the general cross-domain knowledge of these pretrained models for app control. Radford et al., "Improving Language Understanding by Generative Multitask Learning" were some of the first works that utilised large foundation models to perceive smartphone observations and generate human-like actions. To successfully solve more complex tasks requiring long-term planning and history awareness, several frameworks were proposed with dedicated prompt-engineering components for steps like planning, reflection, etc. Brown et al., "Language Models as Zero-Shot Learners" . Although these added reasoning steps improved performance considerably, they significantly increased the computational cost and wall-time of each interaction. 
Other works tried to obtain app-specific knowledge utilising memory  , which stores past interaction between the agent and specific apps.

\subsection{Fine-Tuned Agents}

To address the gap between the general capabilities of foundation models and the specific needs of smartphone environments, as well as to reduce the cost of querying general foundation models, several works have focused on fine-tuning to implement more specialised app agents. Wang et al., "Finetune BERT for Extractive Summarization" use large foundation models for the high-level proposal of actions or plans, while they fine-tune a smaller VLM to ground this action. Zhang et al., "CoCoAgent: A Small Foundation Model for Action Prediction in App Control" proposed CoCoAgent, a small foundation model that aims to predict actions for app control in smartphones by decomposing the actions into action type prediction and optionally the target UI element that this action will be applied to.
Similarly, LiMAC  introduced a small action transformer to predict the action type and the target UI element, while integrating a fine-tuned VLM for text completion. 
InfiGUIAgent  proposed a two-stage fine-tuning process, which first focuses on learning details about the screenshot observations, such as predicting the text of specific UI elements, and then learns how to generate actions based on user's instructions. 

Previous research has also investigated online optimisation of app agents to overcome the limitations of trajectory diversity in static datasets. DigiRL  introduced an online RL framework that simulates app control tasks, training a policy that is first fine-tuned on an offline dataset. DistRL  enhanced the training efficiency with asynchronous online learning. However, both methods depend on online tasks that follow the same distribution as the offline dataset. In contrast, our work aims to enable agents to tackle tasks beyond those encountered during the initial SFT within the offline dataset.