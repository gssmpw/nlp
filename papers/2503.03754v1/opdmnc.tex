\documentclass[a4paper,10pt]{article}
\setlength{\parindent}{0pt}
\usepackage[margin=1in,textwidth=12cm]{geometry}
\usepackage[small]{titlesec}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{ifthen}
\usepackage{cite}
\usepackage[cmex10]{amsmath}
\usepackage{graphicx}                                        
\usepackage{color,framed}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsopn,amsthm}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage[thinc]{esdiff}
\usepackage{physics}
\usepackage{mathrsfs}  
\usepackage{enumitem}
\usepackage{comment}
\usepackage{mathrsfs}  
\usepackage{etoolbox}

\makeatletter
\patchcmd{\@makecaption}
  {\scshape}
  {}
  {}
  {}
\makeatother

\interdisplaylinepenalty=2500

\newtheorem{theorem}{\bf Theorem}%[section]
\newtheorem{lemma}[theorem]{\bf Lemma}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{\bf Proposition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}[theorem]{\bf Corollary} 
\newtheorem{definition}{\bf Definition}
\theoremstyle{definition}
\newtheorem{example}{\bf Example}
\newtheorem{discussion}{Discussion}
\theoremstyle{remark}
\newtheorem{remark}{\bf Remark}
\newtheorem{observation}{\bf Observation}

\newcommand{\supp}{\text{supp}}
\newcommand{\E}{\mathbb{E}}

\def\Pr{\text{\rm{Pr}}}
\def\T{\text{T}}
\def\det{\text{det}}

\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

%*********************************

\newcommand{\fR}{\mathfrak{R}}
\newcommand{\fS}{\mathfrak{S}}

\newcommand{\Var}{\text{\rm{Var}}}


\newcommand{\mS}{\mathcal{S}}

\newcommand{\wOmega}{\widetilde{\Omega}}
\newcommand{\wPi}{\widetilde{\Pi}}
\newcommand{\wLambda}{\widetilde{\Lambda}}

\newcommand{\PR}{\text{\rm{PR}}}
\newcommand{\CHSH}{\text{\rm{CHSH}}}

\newcommand{\womega}{\tilde{\omega}}
\newcommand{\wpi}{\tilde{\pi}}
\newcommand{\wlambdai}{\tilde{\lambda}}

\newcommand{\wa}{\tilde{a}}
\newcommand{\wb}{\tilde{b}}
\newcommand{\wx}{\tilde{x}}
\newcommand{\wy}{\tilde{y}}
\newcommand{\wt}{\tilde{t}}
\newcommand{\ws}{\tilde{s}}
\newcommand{\wA}{\widetilde{A}}
\newcommand{\wB}{\widetilde{B}}
\newcommand{\wC}{\widetilde{C}}
\newcommand{\wH}{\widetilde{H}}

\newcommand{\wX}{\widetilde{X}}
\newcommand{\wY}{\widetilde{Y}}
\newcommand{\wZ}{\widetilde{Z}}
\newcommand{\wT}{\widetilde{T}}
\newcommand{\wS}{\widetilde{S}}
\newcommand{\wW}{\widetilde{W}}

\newcommand{\HPhi}{H_{\Phi}}
\newcommand{\An}{A_{[n]}}
\newcommand{\Xn}{X_{[n]}}
\newcommand{\Pin}{\Pi_{[n]}}
\newcommand{\Bn}{B_{[n]}}
\newcommand{\Yn}{Y_{[n]}}
\newcommand{\Omegan}{\Omega_{[n]}}
\newcommand{\Cn}{C_{[n]}}
\newcommand{\Zn}{Z_{[n]}}
\newcommand{\Lambdan}{\Lambda_{[n]}}

\newcommand{\wAn}{\wA_{[n]}}
\newcommand{\wXn}{\wX_{[n]}}
\newcommand{\wPin}{\wPi_{[n]}}
\newcommand{\wBn}{\wB_{[n]}}
\newcommand{\wYn}{\wY_{[n]}}
\newcommand{\wOmegan}{\wOmega_{[n]}}
\newcommand{\wCn}{\wC_{[n]}}
\newcommand{\wZn}{\wZ_{[n]}}
\newcommand{\wLambdan}{\wLambda_{[n]}}

\newcommand{\bAi}{A_{[i]}}
\newcommand{\bXi}{X_{[i]}}
\newcommand{\bPii}{\Pi_{[i]}}
\newcommand{\bBi}{B_{[i]}}
\newcommand{\bYi}{Y_{[i]}}
\newcommand{\bOmegai}{\Omega_{[i]}}

\newcommand{\wAi}{\wA_{[i]}}
\newcommand{\wXi}{\wX_{[i]}}
\newcommand{\wPii}{\wPi_{[i]}}
\newcommand{\wBi}{\wB_{[i]}}
\newcommand{\wYi}{\wY_{[i]}}
\newcommand{\wOmegai}{\wOmega_{[i]}}
\newcommand{\wCi}{\wB_{[i]}}
\newcommand{\wZi}{\wY_{[i]}}
\newcommand{\wLambdai}{\wLambda_{[i]}}

\newcommand{\Tie}{T_i^e}
\newcommand{\Sie}{S_i^e}
\newcommand{\Wie}{W_i^e}
\newcommand{\wTie}{\widetilde{T}_i^e}
\newcommand{\wSie}{\widetilde{S}_i^e}

\renewcommand{\vec}[1]{\underline{#1}}


\newcommand{\MC}{\textrm{MC}}
\newcommand{\HC}{\textrm{HC}}


% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%     T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


%----------------------------------------


\begin{document}
\title{On $\Phi$-entropic Dependence Measures and Non-local Correlations\\
\thanks{Identify applicable funding agency here. If none, delete this.}

}

\author{Chenyu Wang, Amin Gohari\\
\emph{\small Department of Information Engineering}\\
\emph{\small The Chinese University of Hong Kong}
}
\date{January 23, 2025}

% \author{\IEEEauthorblockN{Chenyu Wang, Amin Gohari}
% \IEEEauthorblockA{
% \textit{The Chinese University of Hong Kong}\\
% cywang@ie.cuhk.edu.hk, agohari@ie.cuhk.edu.hk}
% }


\maketitle

\begin{abstract}
    We say that a measure of dependence between two random variables $X$ and $Y$, denoted as $\rho(X;Y)$, satisfies the data processing property if $\rho(X;Y)\geq \rho(X';Y')$ for every $X'\rightarrow X\rightarrow Y\rightarrow Y'$, and satisfies the tensorization property if $\rho(X_1X_2;Y_1Y_2)=\max\{\rho(X_1;Y_1),\rho(X_2;Y_2)\}$ when $(X_1,Y_1)$ is independent of $(X_2,Y_2)$. It is known that measures of dependence defined based on $\Phi$-entropy satisfy these properties. These measures are important because they generalize R{\'e}nyi's maximal correlation and the hypercontractivity ribbon. The data processing and tensorization properties are special cases of monotonicity under wirings of non-local boxes. We show that ribbons defined using $\Phi$-entropic measures of dependence are monotone under wiring of non-local no-signaling boxes, generalizing an earlier result. In addition, we also discuss the evaluation of $\Phi$-strong data processing inequality constant for joint distributions obtained from a $Z$-channel. 
\end{abstract}



%----------------------------------------------------------------------

\section{Introduction}
Given a convex function $\Phi$, and a function $f(X)$ of a random variable $X$,  the $\Phi$-entropy of $f$ is defined to be
\begin{align}
    \HPhi(f) &= \E[\Phi(f)] - \Phi(\E(f)).
\end{align}
Given a function $f_{XY}$ of two random variables $(X,Y)$, we define 
\begin{align}H_\Phi(f|Y)& =\E[\Phi(f)]-\mathbb{E}_Y[\Phi(\E[f|Y])]\\
 &= \sum_{y}p(y)\big(\E[\Phi(f)|Y=y] - \Phi(\E [f|Y=y])\big).
 \end{align}
\begin{definition}
    Let $\mathscr{F}$ be the class of all non-affine smooth convex functions $\Phi$, defined on a convex subset of $\mathbb{R}$ such that $1/\Phi''$ is concave.
\end{definition}
Then, the $\Phi$-ribbon of random variables $(A,B)$ is defined in \cite{beigi2018phi} as
    \begin{align*}
        \fR_{\Phi} (A;B)=\bigg\{ (\lambda_1,\lambda_2):& \lambda_1 \HPhi (\E[f|A]) + \lambda_2 \HPhi (\E[f|B]) \\
        &\leq \HPhi(f),\quad \forall f(A,B) \bigg\}.
    \end{align*}
 It is shown in \cite{beigi2018phi} that the $\Phi$-ribbon generalizes the  hypercontractivity ribbon and maximal correlation  \cite{hirschfeld1935connection, gebelein1941statistische, renyi1959new, renyi1959measures}. 
 From the $\Phi$-ribbon, one can compute 
    the $\Phi$-strong data processing inequality (SDPI) constant which generalizes the SDPI  constant as
\begin{align*}
    \eta_{\Phi}(X,Y) &= \inf\frac{1-\lambda_1}{\lambda_2}
\end{align*}
where the infimum is over all $(\lambda_1,\lambda_2)\in \fR_{\Phi} (A;B)$ with $\lambda_2\neq 0$. If $\Phi$ is defined on some compact interval, it is known that \cite{beigi2018phi} 
\begin{align}
    \eta_{\Phi}(X,Y) &= \sup_{f_X} \frac{\HPhi(\E[f_X|Y])}{\HPhi(f_X)}.\label{defSDPIF}
\end{align}

If we restrict $\E[f]=1$, we would get Raginsky's definition of the $\Phi$-SDPI constant defined in the divergence form \cite{raginsky2016strong}. 


\begin{example}
    The class $\mathscr{F}$ includes the following functions, 
    \begin{align*}
        \psi_{\alpha}(t)&=t^{\alpha},\quad \alpha \in (1,2];\\
        \phi(t) &= t\log (t);\\
        \Phi_1(t)&=1-h\left(\frac{1+t}{2}\right);\\
        \Phi_{\alpha}(t) &=\frac{(1+t)^{\alpha} + (1-t)^{\alpha}- 2}{2^{\alpha}-2},\quad \alpha \in (1,2],\quad t\in [-1,1]
    \end{align*}
    where $h(t)$ is the binary entropy function, i.e., $h(t)=-t\log(t)-(1-t)\log(1-t)$. For $\Phi(t)=t^2$, the $\Phi$-ribbon would be the Maximal Correlation Ribbon (MC ribbon) (see the definition in Section I.A in \cite{beigi2018phi}). And when $\Phi(t)=t\log(t)$, we recover the HC ribbon (see the alternative characterization in \cite{nair2014equivalent}). 
\end{example}
    
It is shown in \cite{beigi2018phi} that the $\Phi$-ribbon (and in particular, the $\Phi$-SDPI constant) satisfy the following tensorization and data processing properties: 
\begin{theorem}
    For any $\Phi\in \mathscr F$, the $\Phi$-ribbon satisfies data processing and tensorization as follows:
    \begin{enumerate}[label=\rm{(\roman*)}]
        \item (Tensorization) If $p_{A_1A_2B_1B_2}=p_{A_1B_1}p_{A_2B_2}$, then 
        \begin{align*}
            \fR_{\Phi} (A_1A_2;B_1B_2) = \fR_{\HPhi} (A_1;B_1) \cap \fR_{\HPhi} (A_2;B_2).
        \end{align*}
        \item (Data processing) If $p_{A_1A_2B_1B_2}=p_{A_1B_1}p_{A_2|A_1}p_{B_2|B_1}$, then 
        \begin{align*}
            \fR_{\Phi}(A_1;B_1) \subseteq \fR_{\Phi}(A_2;B_2).
        \end{align*}
    \end{enumerate}
\end{theorem}


We provide the following results in this paper: (i) we say that $P_{XY}$ is a \emph{Z-channel source} if $X,Y\in\{0,1\}$ are binary and satisfy $p_{XY}(0,1)=0$. It was shown in \cite{anantharam2013hypercontractivity} that to compute the ordinary SDPI constant, it suffices to use functions that satisfy $f_X(1)=0$. The proof uses the characterization of the SDPI constant in terms of concave envelopes of entropy terms. Such a characterization does not exist for the $\Phi$-SDPI constant. The ordinary SDPI constant corresponds to the function $\Phi(x)=x\log(x)$. Nonetheless, the same property holds for other $\Phi$'s such as $\Phi(x)=-\log(x)$ and $\Phi(x)=x^{-1}$. Using a different approach, we study the class of functions $\Phi$ for which the maximizer of the $\Phi$-SDPI constant satisfies $f_X(1)=0$, and we verify that $\Phi(t)=-\log(t)$ and $\Phi(t)=t^{-1}$ belong to this class. 

Our second contribution is to generalize the results in \cite{beigi2015monotone} and \cite{beigi2018phi}. The authors of \cite{beigi2015monotone} show that the hypercontractivity and maximal correlation ribbons are monotone under wiring of non-local boxes. We extend their proof to $\Phi$-ribbons. Non-locality is a key feature in quantum mechanics. Popescu and Rohrlich proposed no-signaling, i.e., the impossibility of instantaneous of communication, as the fundamental physical principle of non-locality \cite{popescu1994quantum}. There are evidences showing the impossibility of highly non-local correlations \cite{van2013implausible}. Other principles are also proposed as principles of non-locality \cite{pawlowski2009information,fritz2013local,brassard2006limit,linden2007quantum,navascues2007bounding,allcock2009closed,navascues2010glance,masanes2006general,skrzypczyk2009emergence}. The outcomes of bipartite experiments can exhibit non-local dependencies. A bipartite experiment is modeled by a \emph{box} which is simply a conditional probability $p_{AB|XY}$, where $X$ and $Y$ denote the inputs chosen by the two parties, and $A$ and $B$ denote the outputs (see Fig. \ref{fig1}). Specifically, given the input $x,y$, the box generates the outcomes $a,b$ with probability $p_{AB|XY}(ab|xy)$. We say that a box satisfies the no-signaling principle for non-locality if $p_{A|XY}(a|xy)=p_{A|X}(a|x)$ and $p_{B|XY}(b|xy)=p_{B|Y}(b|y)$ for every $x,y,a,b$. The class of no-signaling experiments is important because it precludes the possibility of instantaneous communication across parties. All experiments in quantum physics are no-signaling. Next observe that the two parties may run multiple bipartite experiments where the input of each experiment can be chosen as an arbitrary function of the input and output of the past experiments. A crucial feature of multiple bipartite experiments involving non-signaling boxes is that the two parties are allowed to use the boxes in different (and even probabilistic) orders! The wiring of boxes refers to the set of all possible ways the two parties can use a collection of boxes. Allcock et al. proposed the concept of \emph{closed sets of correlations} \cite{allcock2009closed} and observed the set of non-local boxes are closed under wirings \cite{short2006entanglement,barrett2007information}. The authors of \cite{beigi2015monotone} showed that the maximal correlation (MC) and the hypercontractivity (HC) ribbon are monotone regions under the wiring of no-signaling boxes. In this paper, we show that all $\Phi$-ribbons are monotone under wiring, generalizing this fact for the MC and the HC ribbons.  



\begin{figure}
\begin{center}
\includegraphics[width=2.3in]{fig.pdf}
\caption{\small 
Consider a scenario where two parties each possess subsystems of a bipartite physical system, which can exhibit correlations. Each party can perform a measurement on their respective subsystem by adjusting the measurement device using a specific parameter, and subsequently observe the outcome. Let the measurement settings be denoted by $x$ and $y$, and the corresponding outcomes by $a$ and $b$. In the general case, the outcomes $a$ and $b$ resulting from the measurements $x$ and $y$ occur with a conditional probability $p_{AB|XY}(ab|xy)$. This setup can be conceptualized as a “box” divided into two parts, where each part has an input and an output. For a given pair of inputs $x$ and $y$, the corresponding outputs $a$ and $b$ are produced according to the probability distribution $p_{AB|XY}(ab|xy)$.  
}
\label{fig1}
\end{center}
\end{figure}


%----------------------------------------------------------------------
\section{Preliminaries}
The chain rule for $\Phi$-entropy of $f=f(X,Y)$ is given by
\begin{equation}
    \HPhi(f)=\HPhi (f|X)+\HPhi (\E[f|X]). \label{eqn:CR1}
\end{equation}
Denote $[n]$ as the set of all integers from $\{1,...,n\}$ and $X_{[n]}$ as the collection of random variables $(X_1,...,X_n)$. Using induction, the above equation implies that for any function $f$ and any sequence of random variables $\Xn$ we have
\begin{align}
    \HPhi(\E[f|\Xn]) &= \sum_{i=1}^n \HPhi(\E[f|X_{[i]} ]| X_{[i-1]})\label{eqn:CRn}.
\end{align}
        
Suppose $f$ is a function of $(X,Y,Z)$, then
\begin{align} \label{eqn:CR2}
    \HPhi(f|X) &= \HPhi(f|XY) + \HPhi(\E[f|XY]|X).
\end{align}

The functions in the family $\mathscr{F}$ have the following properties, which play a key role in the following sections:  
\begin{lemma}[\cite{beigi2018phi}]\label{lem:key-lemma-phi-entropy} 
    \begin{enumerate} [label=\rm{(\roman*)}]
    \item Assume $X$ and $Y$ are \emph{independent} random variables, and $f_{XY}$ is arbitrary. Then, for any $\Phi\in\mathscr F$, we have
    $$\E[\Phi(f)]-\mathbb{E}_{X}[\Phi(\E_Y[f|X])]
    \geq \E_Y[\Phi(\E_X[f|Y])] -\Phi(\E f),$$
    or equivalently
    $H_{\Phi}(f|X)\geq H_\Phi(\E[f|Y])$. 
    \item More generally, if $f_{XYZ}$ is a function of three random variables satisfying the Markov chain condition $X\rightarrow Z\rightarrow Y$, we have
    $$H_\Phi(f|XZ)\geq H_{\Phi}\big(\E[f|YZ]\big|Z\big).$$
    \item Under the same condition as in part (ii) we have
    $$H_{\Phi}(\E [f|Z]) + H_{\Phi}(f|XZ) \geq H_\Phi(\E [f|YZ]).$$ 
    \end{enumerate}
\end{lemma}



\section{Evaluation of SDPI for a Z-channel source}
Observe that the function $f_X$ involves two free variables, $f_X(0)$ and $f_X(1)$, while there are another two free variables to describe $p_{XY}$ (since $p_{XY}(0,1)=0$ in a Z-channel source). Thus, there are four free parameters involved in the optimization:
\begin{align}
    \eta_{\Phi}(X,Y) &= \sup_{f_X} \frac{\HPhi(\E[f_X|Y])}{\HPhi(f_X)}.
\end{align}
We simplify the problem by reducing four free variables to two; indeed, we define the following classes of functions, which depend only on two variables:
\begin{definition}
    Let $\mathscr{F}_1$ be the set of all convex functions $\Phi$ satisfying
    \begin{align}
        x+\frac{\Phi''(x)}{\Phi'''(x)} \geq y+3\frac{\Phi''(y)}{\Phi'''(y)}, \qquad \forall x,y\geq 0.\label{eqnCond2}
    \end{align}
    and $\mathscr{F}_2$ be the set of all convex functions $\Phi$ satisfying $\Phi'''\leq 0$ and
        \begin{align}
        &\bigg(\Phi(x)+\Phi'(x)(y-x)-\Phi(y)\bigg)\bigg(\Phi'(y)-\Phi'(x)\bigg)\nonumber\\
        &+\Phi''(x)(y-x)^2 \bigg(\Phi'(y)-\frac{\Phi(x)-\Phi(y)}{x-y}\bigg)\geq 0\label{eqnCond}
    \end{align}
    for any $x,y\geq 0$.
\end{definition}
\begin{remark}\label{rmk1}
   Setting $x=y$ in \eqref{eqnCond2} shows that $\Phi'''\leq 0$ for any $\Phi\in\mathscr{F}_1$. Thus, this condition is imposed in both definitions.
\end{remark}
\begin{theorem} \label{thm:eta_phi-maximizer} We have $\mathscr{F}_1\subset \mathscr{F}_2$. Moreover, for any $\Phi$ in $\mathscr{F}_1$ or $\mathscr{F}_2$, and any $Z$-channel source $(X,Y)$ the supremum in 
    \begin{align}
    \eta_{\Phi}(X,Y) &= \sup_{f_X} \frac{\HPhi(\E[f_X|Y])}{\HPhi(f_X)}
\end{align}
is achieved only when $f_X(1)=0$. 
\end{theorem}

The proof is in Appendix \ref{apx:proof-eta_phi}. 
% We only give the structure of the proof here. The full proof can be found in the arXiv version. Start with the second part of the proof. First, we fix $\E[f]$ and let $f_X(1)$ be the free variable. Thus, $f_X(0)$ can be calculated in terms of $\E[f]$ and $f_X(1)$. If we show that the derivative of $\HPhi(\E[f|Y])/\HPhi(f)$ with respect to $f_X(1)$ is negative, then $f_X(1)=0$ will be the maximizer, and this will complete the proof. To show the negativity of this derivative, we further take its derivative with respect to $p(Y=1|X=1)$ and show that this derivative is positive. To do this, we use Taylor's expansion and show that the derivative with respect to $p(Y=1|X=1)$, as a function of $f_X(1)$ is minimized at $f_X(1)=\E[f]$. Thus, we can remove one variable from consideration, namely $f_X(1)$. Moreover, calculus will allow us to remove one more variable from consideration. Thus, we reduce the four free variables to two. Then the definition of $\mathscr{F}_2$ would complete the proof. 

% For the first part of the proof: we show that convexity of the left hand side of \eqref{eqnCond} as a function of $y$ is sufficient to demonstrate its non-negativity. This would  reduce it to the definition of $\mathscr{F}_1$ in \eqref{eqnCond2}.  

\begin{lemma}
    The following functions $t\log(t),\frac{1}{t},-\log(t)$ belong to $\mathscr{F}_1$. 
\end{lemma}
\begin{proof}
    Define 
    \begin{align*}
        G_{\Phi}(x,y)&=-3\frac{\Phi''(y)}{\Phi'''(y)}-y+\frac{\Phi''(x)}{\Phi'''(x)}+x. 
    \end{align*}
    For $\Phi(t)=t\log(t)$, 
    \begin{align*}
        G_{\Phi}(x,y) =& 2y\geq 0. 
    \end{align*}
    For $\Phi(t)=1/t$, 
    \begin{align*}
        G_{\Phi}(x,y) =& \frac{2}{3}x\geq 0.
    \end{align*}
    For $\Phi(t)=-\log(t)$, 
    \begin{align*}
        G_{\Phi}(x,y) = y+\frac{x}{2} \geq 0.
    \end{align*}
\end{proof}






%********************************************************************

\section{Wirings of no-signaling boxes} \label{sec:wirings}

We will define the $\Phi$-ribbon for no-signaling boxes and then prove it has the tensorization property. 
\begin{definition}
    Given a no-signaling box $p_{AB|XY}$, define the $\Phi$-ribbon to be the intersection of the $\Phi$-ribbons of its outputs conditioned on all possible inputs, i.e., 
    \begin{align*}
        \fR_{\Phi}(A;B|X;Y)\defeq \bigcap_{x,y} \fR_{\Phi}(A;B|X=x,Y=y).
    \end{align*}
\end{definition}

If a no-signaling box $p_{AB|XY}$ can be simulated by the box with $p_{A'B'|XY}$, given any $x,y$, we have $p_{AA'BB'|X=xY=y}=p_{A|A'X=xY=y}p_{B|B'X=xY=y}$. By the data processing property of $\Phi$-ribbon, 
\begin{align*}
    \fR_{\Phi} (A';B'|X=x,Y=y) \subseteq \fR_{\Phi} (A;B|X=x,Y=y).
\end{align*}

By the definition of the $\Phi$-ribbon for the no-signaling boxes, we have  
\begin{align*}
    \fR_{\Phi} (A';B'|X;Y) \subseteq \fR_{\Phi} (A;B|X;Y).
\end{align*}
Thus, the $\Phi$-ribbon for no-signaling boxes have the data processing inequality. 

Now we state the main theorem for the tensorization of the wiring of no-signaling boxes. 

\begin{theorem} \label{thm:tensorization-for-boxes}
    Suppose a no-signaling box $p(a'b'|x'y')$ can be generated from n no-signaling boxes $p_i(a_ib_i|x_iy_i)$ where $i\in[n]$, under wirings. Then we have 
    \begin{align*}
        \bigcap_{i=1}^n \fR_{\Phi} (A_i;B_i|X_i;Y_i)\subseteq \fR_{\Phi} (A';B'|X';Y').
    \end{align*}
\end{theorem}
The proof is in Appendix \ref{apx:proof-main-theorem}. We will first give a proof for the special case of a simple wiring of two boxes as an illustration of some of the calculations used in the proof. 


\subsection{Formulation of wirings}

\begin{table*}
\begin{center}
\begin{small}
\caption{\small Alice and Bob interact with the  $n$ no-signaling boxes in sequences that may differ and could be chosen randomly. The table outlines the notations used to represent the random variables related to these sequences, as well as the inputs and outputs of the boxes. The term “Alice’s transcript” refers to the information Alice has gathered through her observations up to a given point in time.
}
\begin{tabular}{|c|c|c|}
\hline
Notation & Description   & { Corresponding variable of Bob}\\
\hline
$\Pi_i$ & Alice uses the $i$-th box in her $\Pi_i$-th action& $\Omega_i$\\ \hline   
 & Index of the box Alice uses in her $i$-th action: & \\
$\wPi_i$&$~\Pi_{\wPi_i}=i, \qquad \wPi_{\Pi_i}=i$&$\wOmega_i$\\
\hline
$X_i$ & Alice's input of the $i$-th box & $Y_i$\\\hline
$A_i$ & Alice's output of the $i$-th box & $B_i$\\\hline
& Alice's input in her $i$-th action:  & \\
$\wX_i$ &$~\wX_i=X_{\wPi_i}$&$\wY_i$\\
\hline
 & Alice's output in her $i$-th action: & \\
$\wA_i$&$~\wA_i=A_{\wPi_i}$&$\wB_i$\\
\hline
$T_i$ & Alice's transcript before using the $i$-th box & $S_i$\\\hline
 & Alice's transcript before her $i$-th action: & \\
$\wT_i$&$~\wT_i=T_{\wPi_i}$& $\wS_i$
\\\hline
\end{tabular}
\end{small}
\end{center}

\label{table:summary}
\end{table*}


Suppose the two parties, Alice and Bob, have the inputs $X_i,Y_i$ for the $i$-th box respectively and the associated outputs $A_i,B_i$ for $i\in [n]$. The $i$-th box is associated with the conditional probability $p_{A_iB_i|X_iY_i}$. The wiring of boxes allows one party to choose which box to use and the corresponding input based on all the past information, including all the boxes that have been used and the corresponding inputs and outputs. The choices of boxes and inputs are independent of each other during each action. The wiring can be arbitrary, which means that in the same action, the two parties can use different boxes, as shown in Fig.\ref{fig2}. 

Given a no-signaling box $p_{A'B'|X'Y'}$, Alice and Bob use $n$ no-signaling boxes, i.e., $p_{A_iB_i|X_iY_i}$ for $i\in[n]$, to simulate it, which means that the outputs $A'B'$ depend on all the information generated in the actions with the $n$ boxes $p_{A_iB_i|X_iY_i}$. 

\begin{figure}
\begin{center}
\includegraphics[width=3in]{bipartite-wiring.pdf}
\caption{\small Due to no-signaling property, Alice and Bob can choose boxes in different orders. For example, Alice can use the order $2,3,1$ to simulate the result given $x'$. Alice simulates the box $2$ with input $x_2$ determined by $x'$ and get the result $a_2$. She then uses the box $3$ with input $x_3$ determined by the former output $a_2$ and get the result $a_3$. Then she used the box $1$ with input $x_1$ determined by $a_3$ and get the result $a_1$. Finally she simulates the result $a'$ by applying a stochastic map. Bob can use the order of boxes as $3,1,2$ given $y'$ and follow the same steps. }
\label{fig2}
\end{center}
\end{figure}


We will introduce some notations to formulate the wiring. Let $(\Pi_1,...,\Pi_n)$ denote the random variables for the order of boxes used by Alice. For example, in the $\Pi_i$-th action, Alice uses the $i$-th box. Next, define the inverse permutation of $(\Pi_1,...,\Pi_n)$ as $(\wPi_1,...,\wPi_n)$, i.e., 
\begin{align*}
    \wPi_{\Pi_i}=i, \qquad \qquad \Pi_{\wPi_i}=i
\end{align*}
which means that in the $j$-th action, Alice uses the $\wPi_j$-th box. The corresponding input is then $X_{\wPi_j}$, and the output is $A_{\wPi_j}$. For simplicity, define
\begin{align*}
    \wX_j \defeq X_{\wPi_j}, \qquad \qquad \wA_j \defeq A_{\wPi_j}.
\end{align*}

Similarly, define $(\Omega_1,...,\Omega_n)$ to be the random variables for the order of boxes used by Bob. And the inverse permutation of $(\Omega_1,...,\Omega_n)$ as $(\wOmega_1,...,\wOmega_n)$, i.e.,  
\begin{align*}
    \wOmega_{\Omega_i}=i, \qquad \qquad \Omega_{\wOmega_i}=i.
\end{align*}

Denote $T_i$ as the \textit{transcript} of the information that Alice has before using the $i$-th box (before the $\wPi_i$-th action), i.e., 
\begin{align*}
    T_i &\defeq \wPi_i...\wPi_{\wPi_{i-1}}X_{\wPi_1}...X_{\wPi_{\Pi_{i-1}}}A_{\wPi_1}...A_{\wPi_{\Pi_{i-1}}}\\
    &= \wPi_{[\Pi_{i-1}]} \wX_{[\Pi_{i-1}]} \wA_{[\Pi_{i-1}]}.
\end{align*}

Denote $\wT_i$ as the transcript of Alice before $i$-th action, i.e., 
\begin{align*}
    \wT_i &\defeq T_{\wPi_i} = \wPi_1...\wPi_{i-1}X_{\wPi_1} X_{\wPi_{i-1}} A_{\wPi_1} ... A_{\wPi_{i-1}}\\
    &= \wPi_{[i-1]} \wX_{[i-1]} \wA_{[i-1]}.
\end{align*}

Similarly, define $S_i$ and $\wS_i$ as transcripts for Bob, i.e., 
\begin{align*}
S_i \defeq \wOmega_{[\Omega_{i-1}]} \wY_{[\Omega_{i-1}]} \wB_{[\Omega_{i-1}]}, \qquad \wS_i \defeq \wOmega_{[i-1]} \wY_{[i-1]} \wB_{[i-1]}.
\end{align*}

Hence, we can formulate the wiring in this way. In the $i$-th action, Alice has the information $\wT_i$ at hand and Bob has the information $\wS_i$. Then Alice chooses the $\wPi_i$-th box and the corresponding input $\wX_i$ based on $\wT_i$, and Bob chooses the $\wOmega_i$-th box and the corresponding input $\wY_i$ based on $\wS_i$ independent of each other. The $\wPi_i$-th box generates the output $\wA_i$ for Alice, and the $\wOmega_i$-th box generates the output $\wB_i$ for Bob. 

Based on the wiring and the no-signaling property, we can state the following lemma. 
\begin{lemma}[Lemma 2 in \cite{beigi2015monotone}] \label{lem:mc} 
    For the wiring of no-signaling boxes, given $x'y'$, we have the following Markov chains: 
    \begin{enumerate} [label=\rm{(\roman*)}]
        \item $A_iB_i\rightarrow X_iY_i\rightarrow T_iS_i\Pi_i\Omega_i$.
        \item $B_i \rightarrow S_i^e \rightarrow T_i^e$.
        \item $B_i \rightarrow A_i T_i^e S_i^e \rightarrow \An \Xn \Pin$.
        \item $\wY_i\wOmega_i \rightarrow \wS_i \rightarrow \An\Xn\Pin$.
    \end{enumerate}
\end{lemma}



Combining Lemma \ref{lem:key-lemma-phi-entropy} and \ref{lem:mc}, we immediately get the following corollary. 
\begin{corollary} \label{cor:mc}
    For any $f$, with Lemma \ref{lem:key-lemma-phi-entropy} (ii), conditioned on $x',y'$, we have the following inequalities: 
    \begin{enumerate} [label=\rm{(\roman*)}]
        \item from Lemma \ref{lem:mc} (ii), 
        \begin{align*}
            \HPhi(\E[f|B_i\Tie\Sie]|\Tie\Sie)\geq
                \HPhi(\E[f|B_i\Sie]|\Sie).
        \end{align*}
        \item from Lemma \ref{lem:mc} (iii), 
        \begin{align*}
            &\HPhi(\E[f|\An\Xn\Pin\bBi\bYi\bOmegai]|\An\Xn\Pin S_i^e)
            \\\geq& 
            \HPhi(\E[f|A_iB_i\Tie\Sie]|A_i\Tie\Sie).
        \end{align*}
        \item from Lemma \ref{lem:mc} (iv), 
        \begin{align*}
            &\HPhi(\E[f|\An\Xn\Pin\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}]|\An\Xn\Pin\wS_i)
            \\\geq&
            \HPhi(\E[f|\wS_i^e]|\wS_i).
        \end{align*}
    \end{enumerate}
\end{corollary}

We will also use the following lemma to prove the tensorization for the $\Phi$-ribbon of no-signaling boxes. 
\begin{lemma} \label{lem:expand}
   For any $f$, 
   \begin{enumerate} [label={(\roman*)}]
       \item \begin{align*}
            \HPhi(\E[f|\An\Xn\Pin]) 
            = \sum_{i=1}^n \HPhi(\E[f|\wT_i^e]|\wT_i)
             +\HPhi(\E[f|A_i \Tie]|\Tie)
        \end{align*}
        and similarly, 
        \begin{align*}
            \HPhi(\E[f|\Bn\Yn\Omegan]) 
            = \sum_{i=1}^n \HPhi(\E[f|\wS_i^e]|\wS_i)
             +\HPhi(\E[f|B_i \Sie]|\Sie)
        \end{align*}
       \item \begin{align*}
           \HPhi(\E[f|\An\Xn\Pin\Bn\Yn\Omegan]|\An\Xn\Pin)
           =& \sum_{i=1}^n \HPhi(\E[f|\An\Xn\Pin\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}]|\An\Xn\Pin\wS_i)\\
           &\quad+\HPhi(\E[f|\An\Xn\Pin\bBi\bYi\bOmegai]|\An\Xn\Pin S_i^e)
       \end{align*}
   \end{enumerate}
\end{lemma}

The proof is in Appendix \ref{app:lem-expand-proof}. 



\subsection{Simple wiring of two boxes}
Suppose Alice and Bob have two boxes, i.e., $p_{A_1B_1|X_1Y_1}$ and $p_{A_2B_2|X_2Y_2}$. The wiring is set up as follows: Alice first performs the experiment on the $1$-st box using input $X_1$, obtaining the outcome $A_1$. Then, for the $2$-nd box, she determines the input $X_2$ based on the previously obtained values  $X_1$ and $A_1$. Bob conducts his experiments in the same way. We will then show that 
\begin{align} \label{tensorization-of-two}
    \quad\fR_{\Phi}\left(A_1;B_1|X_1;Y_1\right) \cap \fR_{\Phi}\left(A_2;B_2|X_2;Y_2\right) 
    \subseteq \fR_{\Phi}\left(A_1A_2;B_1B_2|X_1X_2;Y_1Y_2\right). 
\end{align}

Suppose 
\begin{align*}
    (\lambda_1,\lambda_2)\in \fR_{\Phi}\left(A_1;B_1|X_1;Y_1\right) \cap \fR_{\Phi}\left(A_2;B_2|X_2;Y_2\right).
\end{align*}
Define 
\begin{align*}
    \zeta(\lambda_1,\lambda_2) \defeq& -\lambda_1 \HPhi\left(\E[f|A_1A_2X_1X_2]\right) \\
    &-\lambda_2 \HPhi\left(\E[f|B_1B_2Y_1Y_2]\right)\\
    &+ \HPhi(f) 
\end{align*}
for any function $f$ of $A_1A_2B_1B_2X_1X_2Y_1Y_2$. We will show that $\zeta(\lambda_1,\lambda_2)\geq 0$, which proves the tensorization of the $\Phi$-ribbon. 

\begin{proof}[Proof of \eqref{tensorization-of-two}]
    By the chain rule \eqref{eqn:CR1}, 
    \begin{align*}
        \zeta(\lambda_1,\lambda_2)
        =& -\lambda_1 \bigg(\HPhi(\E[f|A_1X_1])+\HPhi(\E[f|A_1A_2X_1X_2]|A_1X_1)\bigg)\\
        & -\lambda_2 \bigg(\HPhi(\E[f|B_1Y_1])+\HPhi(\E[f|B_1B_2Y_1Y_2]|B_1Y_1)\bigg)\\
        & + \HPhi(\E[f|A_1X_1B_1Y_1]) + \HPhi(f|A_1X_1B_1Y_1).
    \end{align*}
    For the box $p_{A_1B_1|X_1Y_1}$, we have 
    \begin{align*}
        \lambda_1 \HPhi(\E[f|A_1X_1]) + \lambda_2 \HPhi(\E[f|B_1Y_1]) 
        \leq \HPhi(\E[f|A_1X_1B_1Y_1]). 
    \end{align*}
    For fixed $a_1,b_1,x_1,y_1$, as $x_2,y_2$ is decided by $a_1,b_1,x_1,y_1$, the box $p_{A_2 B_2 |a_1b_1x_1y_1x_2y_2}$ is the same as $p_{A_2 B_2|x_2 y_2}$. Then we have
    \begin{align*}
        &\lambda_1 \HPhi(\E[f|A_2x_2a_1x_1b_1y_1]|a_1x_1b_1y_1) 
        + \lambda_2 \HPhi(\E[f|B_2y_2a_1x_1b_1y_1]|a_1x_1b_1y_1) \\
        \leq& \HPhi(\E[f|A_2B_2a_1x_1b_1y_1x_2y_2|a_1x_1b_1y_1).
    \end{align*}
    Taking the average on both sides, we have 
    \begin{align*}
        &\lambda_1 \HPhi(\E[f|A_2X_2A_1X_1B_1Y_1]|A_1X_1B_1Y_1) 
        + \lambda_2 \HPhi(\E[f|B_2Y_2A_1X_1B_1Y_1]|A_1X_1B_1Y_1) \\
        \leq& \HPhi(\E[f|A_2B_2X_2Y_2A_1X_1B_1Y_1|A_1X_1B_1Y_1).
    \end{align*}
    Note that $A_1X_1$ is Alice's input for the $2$-nd box and $A_2$ is the corresponding output. And $B_1Y_1$ is Bob's input for the $2$-nd box. As $X_2$ is totally decided by $A_1X_1$, by the no-signaling property, $A_2X_2\rightarrow A_1X_1 \rightarrow B_1Y_1$ forms a Markov chain. By Lemma \ref{lem:key-lemma-phi-entropy} (ii), we have 
    \begin{align*}
        \HPhi(\E[f|A_2X_2A_1X_1B_1Y_1]|A_1X_1B_1Y_1)
        \geq \HPhi(\E[f|A_1A_2X_1X_2]|A_1X_1).
    \end{align*}
    A similar argument shows that 
    \begin{align*}
        \HPhi(\E[f|B_2Y_2A_1X_1B_1Y_1]|A_1X_1B_1Y_1) 
        \geq \HPhi(\E[f|B_1B_2Y_1Y_2]|B_1Y_1).
    \end{align*}

    Combining all the items above, we obtain $\zeta(\lambda_1,\lambda_2)\geq 0$. 
\end{proof}



%********************************************************************************


\bibliographystyle{IEEEtran}
\bibliography{refs}



%********************************************************************************
\appendix


\section{Proof of Theorem \ref{thm:eta_phi-maximizer}} \label{apx:proof-eta_phi}
We prove the second part of the theorem first. 
Take some arbitrary $\Phi$ in $\mathscr{F}_2$. 
We can represent a Z-channel source  as follows
$$P_{XY}=\begin{pmatrix}
    1-s & 0 \\
    sd & s(1-d)
\end{pmatrix}$$ where $s,d\in[0,1]$. Let $u=f_X(0)$ and $v=f_X(1)$ for some $u,v\geq 0$. Assume that $\E[f]=m=(1-s)u+sv$. Then $u=\frac{m-sv}{1-s}$. 
One can verify directly that
    \begin{align}
        g(v,m)\defeq\frac{\HPhi(\E[f|Y])}{\HPhi(f)}&=\frac{(1-s(1-d))\Phi\left(\frac{1-s}{1-s(1-d)}u + \frac{sd}{1-s(1-d)}v\right) + s(1-d)\Phi(v) - \Phi\left((1-s)u + sv\right)}{(1-s)\Phi(u)+s\Phi(v) - \Phi\left((1-s)u + sv\right)} \\
        &= \frac{(1-s(1-d))\Phi\left(\frac{m-s(1-d)v}{1-s(1-d)}\right) + s(1-d)\Phi(v) - \Phi\left(m\right)}{(1-s)\Phi\left(\frac{m-sv}{1-s}\right)+s\Phi(v) - \Phi\left(m\right)}.
    \end{align}
    
    Then, we claim that if \eqref{eqnCond} holds, then $g(v,m)$ is decreasing in $v$ for every fixed $m$. Therefore, the maximum of $g(v,m)$ would occur when $v=0$. This would complete the proof. Taking the partial derivative of $\log(g(v,m))$ with respect to $v$, we need to show that
    \begin{align} \label{ineq:general-f}
        \frac{s\Phi'(v)-s\Phi'\left(\frac{m-sv}{1-s}\right)}{(1-s)\Phi\left(\frac{m-sv}{1-s}\right)+s\Phi(v)-\Phi(m)}
        \geq
        \frac{s(1-d)\Phi'(v)-s(1-d)\Phi'\left(\frac{m-s(1-d)v}{1-s(1-d)}\right)}{(1-s(1-d))\Phi\left(\frac{m-s(1-d)v}{1-s(1-d)}\right)+s(1-d)\Phi(v)-\Phi(m)}.
    \end{align}
    For any $t\in[0,1]$, define
    \begin{align}
        k(t) &= \frac{st\Phi'(v)-st\Phi'\left(\frac{m-svt}{1-st}\right)}{(1-st)\Phi\left(\frac{m-svt}{1-st}\right)+st\Phi(v)-\Phi(m)}
        \\&=\frac{\Phi'(v)-\Phi'\left(\frac{m-svt}{1-st}\right)}{(\frac{1}{st}-1)\Phi\left(\frac{m-svt}{1-st}\right)+\Phi(v)-\frac{1}{st}\Phi(m)}.
    \end{align}
Then, \eqref{ineq:general-f} can be written as $k(1)\geq k(1-d)$. We would be done if we can show that $k(t)$ is an increasing function. Showing $k'(t)\geq 0$ is equivalent with 
    \begin{align*}
        C(t) =& -\frac{1}{st^2}\left(\Phi'(v)-\Phi'\left(\frac{m-svt}{1-st}\right)\right)\left(-\Phi\left(\frac{m-svt}{1-st}\right)+\frac{(m-v)st}{1-st}\Phi'\left(\frac{m-svt}{1-st}\right)+\Phi(m)\right)  \\
        & - \frac{s(m-v)}{(1-st)^2} \Phi''\left(\frac{m-svt}{1-st}\right) \left(\Phi(v)+\frac{1-st}{st}\Phi\left(\frac{m-svt}{1-st}\right)-\frac{1}{st}\Phi(m)\right)\geq 0.
    \end{align*}
    
    Let $x_1=v$, $x_2=\frac{m-svt}{1-st}$. Then we can compute $s$ from $x_1$ and $x_2$ as follows: $$s=\frac{m-x_2}{t(x_1-x_2)}.$$
    Then, after a change of variables we obtain
    \begin{align}
        C(t) &= \frac{x_1-x_2}{t(m-x_2)} \Bigg[\bigg(\Phi(x_2)+\Phi'(x_2)(m-x_2)-\Phi(m)\bigg) \bigg(\Phi'(x_1)-\Phi'(x_2)\bigg)  \nonumber\\
        &\qquad\qquad\qquad +\Phi''(x_2)(m-x_2)^2 \bigg(\frac{\Phi(x_1)-\Phi(m)}{x_1-m} +\frac{\Phi(x_2)-\Phi(m)}{m-x_2} \bigg)   \Bigg].
    \end{align} 
    Since $\frac{x_1-x_2}{t(m-x_2)}=1/(st^2)\geq 0$, it suffices to show that 
    \begin{align*}
w_{m,x_2}(x_1)&\defeq\bigg(\Phi(x_2)+\Phi'(x_2)(m-x_2)-\Phi(m)\bigg) \bigg(\Phi'(x_1)-\Phi'(x_2)\bigg)  \\
        &\quad +\Phi''(x_2)(m-x_2)^2 \bigg(\frac{\Phi(x_1)-\Phi(m)}{x_1-m} +\frac{\Phi(x_2)-\Phi(m)}{m-x_2} \bigg) \\
        &\geq 0
    \end{align*}
Observe that the number of free variables reduced from four to three. 
    Taking the partial derivative with respect to $x_1$ we obtain:
    \begin{align*}
        w'_{m,x_2}(x_1) &=\bigg(\Phi(x_2)+\Phi'(x_2)(m-x_2)-\Phi(m)\bigg) \Phi''(x_1) +\Phi''(x_2) \frac{(m-x_2)^2}{(x_1-m)^2} \bigg(\Phi'(x_1)(x_1-m)-\Phi(x_1)+\Phi(m)\bigg)
    \end{align*}

We have 
$$m=stx_1+(1-st)x_2,$$
so the following two cases are possible: $x_1\geq m \geq x_2$ or 
$x_2\geq m \geq x_1$. We plan to show that for any $x_1,m,x_2$ satisfying $x_1\geq m \geq x_2$ or 
$x_2\geq m \geq x_1$, we have
$w_{m,x_2}(x_1)\geq w_{m,x_2}(m)$.

First consider the case of  $x_1\geq m \geq x_2$: by Taylor's expansion we obtain
    \begin{align*}
        \Phi(m) &= \Phi(x_1)+\Phi'(x_1)(m-x_1)+\frac{1}{2}\Phi''(x_1)(m-x_1)^2+\frac{1}{6}\Phi'''(\tilde{x}_1)(m-x_1)^3
    \end{align*}
    for some $\tilde{x}_1\in [m,x_1]$. And 
    \begin{align*}
        \Phi(m) &= \Phi(x_2)+\Phi'(x_2)(m-x_2)+\frac{1}{2}\Phi''(x_2)(m-x_2)^2+\frac{1}{6}\Phi'''(\tilde{x}_2)(m-x_2)^3
    \end{align*}
    for some $\tilde{x}_2\in [x_2,m]$. Since $\Phi'''\leq 0$, we have 
    \begin{align*}
        \Phi'(x_1)(x_1-m)+\Phi(m)-\Phi(x_1) &\geq \frac{1}{2}\Phi''(x_1)(m-x_1)^2 \\
        \Phi(x_2)+\Phi'(x_2)(m-x_2)-\Phi(m)+\frac{1}{2}\Phi''(x_2)(m-x_2)^2 &\geq 0.
    \end{align*}
 \begin{align*}
        w'_{m,x_2}(x_1) &=\bigg(\Phi(x_2)+\Phi'(x_2)(m-x_2)-\Phi(m)\bigg) \Phi''(x_1) +\Phi''(x_2) \frac{(m-x_2)^2}{(x_1-m)^2} \bigg(\Phi'(x_1)(x_1-m)-\Phi(x_1)+\Phi(m)\bigg)
    \end{align*}
    Thus, 
    \begin{align*}
        w'_{m,x_2}(x_1) &\geq \bigg(\Phi(x_2)+\Phi'(x_2)(m-x_2)-\Phi(m)\bigg) \Phi''(x_1) +\Phi''(x_2) \frac{(m-x_2)^2}{(x_1-m)^2}  \bigg(\frac{1}{2}\Phi''(x_1)(m-x_1)^2\bigg)\\
        &= \bigg(\Phi(x_2)+\Phi'(x_2)(m-x_2)-\Phi(m)+\frac{1}{2}\Phi''(x_2)(m-x_2)^2\bigg)\Phi''(x_1)\\
        &\geq 0.
    \end{align*}
This shows that $w_{m,x_2}(x_1)\geq w_{m,x_2}(m)$. 

Next, consider the case of $x_2\geq m \geq x_1$. A similar argument shows that 
 $w'_{m,x_2}(x_1)\leq 0$ in this case for $x_1\leq m\leq x_2$. Thus, we also obtain $w_{m,x_2}(x_1)\geq w_{m,x_2}(m)$ in the second case.
 
 
 Then it remains to prove $w_{m,x_2}(m)\geq 0$. In other words,
    \begin{align*}
        \bigg(\Phi(x_2)+\Phi'(x_2)(m-x_2)-\Phi(m)\bigg)\bigg(\Phi'(m)-\Phi'(x_2)\bigg)+\Phi''(x_2)(m-x_2)^2 \bigg(\Phi'(m)-\frac{\Phi(x_2)-\Phi(m)}{x_2-m}\bigg)\geq 0
    \end{align*}
This is the condition of the set $\mathscr{F}_2$. 

It only remains to show that $\mathscr{F}_1\subset \mathscr{F}_2$. Let
$\Phi(x)$ be a function satisfying \eqref{eqnCond2}. We would like to show that 

        \begin{align}
Q_x(y)=\bigg(\Phi(x)+\Phi'(x)(y-x)-\Phi(y)\bigg)\bigg(\Phi'(y)-\Phi'(x)\bigg)+\Phi''(x)(y-x)^2 \bigg(\Phi'(y)-\frac{\Phi(x)-\Phi(y)}{x-y}\bigg)\geq 0
    \end{align} 
The partial derivative with respect to $y$ equals:
    \begin{align*}
        Q_x'(y) &= -\bigg(\Phi'(x)-\Phi'(y)\bigg)^2+\Phi''(y)\bigg(\Phi(x)-\Phi(y)+\Phi'(x)(y-x)\bigg)\\
        &\qquad +\Phi''(x)\bigg(\Phi(x)-\Phi(y)+\Phi'(y)(y-x)+\Phi''(y)(y-x)^2\bigg).
    \end{align*}
   Since $Q_x(x)=Q'_x(x)=0$, it suffices to show that $Q_x(y)$ is convex in $y$. In other words, we wish to show that $Q_x''(y)\geq 0$ for every $x$ and $y$. Thus, we would like to show that 
    \begin{align*}
        Q_x''(y) =& 3\Phi''(y)\bigg(\Phi'(x)-\Phi'(y)+\Phi''(x)(y-x)\bigg) \\
        &+\Phi'''(y)\bigg(\Phi(x)-\Phi(y)+\Phi'(x)(y-x)+\Phi''(x)(y-x)^2\bigg)\geq 0.
    \end{align*}
    If we differentiate $Q_x''(y)$ with respect to $x$, we get 
    \begin{align*}
        &(x-y)\Phi'''(x)\Phi'''(y) \bigg(-3\frac{\Phi''(y)}{\Phi'''(y)}+\frac{\Phi''(x)}{\Phi'''(x)}+x-y\bigg)
    \end{align*}
    By assumption we have $\Phi'''(x)\Phi'''(y)\geq 0$ and 
    \begin{align*}
        -3\frac{\Phi''(y)}{\Phi'''(y)}+\frac{\Phi''(x)}{\Phi'''(x)}+x-y \geq 0.
    \end{align*}
    The expression $Q_x''(y)$, as a function of $x$, is increasing in $x$ if $x\geq y$, and decreasing in $x$ for $x\leq y$. Observe that $Q_x''(y)=0$ when $y=x$. This completes the proof for $Q_x''(y)\geq 0$ for all $x,y$. 




\section{Proof of Lemma \ref{lem:expand}} \label{app:lem-expand-proof}
(i) We have 
\begin{align*}
    \HPhi(\E[f|\An\Xn\Pin])
    &\stackrel{(a)}{=} \HPhi(\E[f|\wAn\wXn\wPin])\\
    &\stackrel{(b)}{=} \sum_{i=1}^n \HPhi(\E[f|\wA_{[i]} \wX_{[i]} \wPi_{[i]}]| \wA_{[i-1]} \wX_{[i-1]} \wPi_{[i-1]}) \\
    &\stackrel{(c)}{=} \sum_{i=1}^{n}\HPhi(\E[f|\wA_{[i-1]}\wX_{[i]}\wPi_{[i]}]|\wA_{[i-1]}\wX_{[i-1]}\wPi_{[i-1]}) +\HPhi(\E[f|\wA_{[i]} \wX_{[i]} \wPi_{[i]}]|\wA_{[i-1]}\wX_{[i]}\wPi_{[i]}) \\
    &\stackrel{(d)}{=} \sum_{i=1}^{n}\HPhi(\E[f|\wA_{[i-1]}\wX_{[i]}\wPi_{[i]}]|\wA_{[i-1]}\wX_{[i-1]}\wPi_{[i-1]})
     +\HPhi(\E[f|A_i \Tie]|\Tie)
\end{align*}

where (a) follows from taking a permutation, (b) follows from \eqref{eqn:CRn}, (c) is given by the chain rule in \eqref{eqn:CR1}, and (d) follows from 
\begin{align*}
    &\sum_{i=1}^n \HPhi(\E[f|\wA_{[i]} \wX_{[i]} \wPi_{[i]}]|\wA_{[i-1]}\wX_{[i]}\wPi_{[i]}) \\
    =& \sum_{i=1}^n \sum_{j=1}^n \HPhi\left(\E[f|\wA_{[i]} \wX_{[i]} \wPi_{[i]}]| \wT_i \wX_i \wPi_i=j\right) p\left(\wPi_i=j\right)\\
    =& \sum_{i=1}^n \sum_{j=1}^n \HPhi\left(\E[f|A_j T_j X_j \Pi_j]| T_j X_j \Pi_j=i\right) p\left(\wPi_i=j\right) \\
    =& \sum_{i=1}^n \sum_{j=1}^n \HPhi\left(\E[f|A_j T_j X_j \Pi_j]| T_j X_j \Pi_j=i\right) p\left(\Pi_j=i\right) \\
    =& \sum_{j=1}^n \HPhi\left(\E\left[f|A_j T_j^e\right]| T_j^e\right)
\end{align*}

It is similar for $\HPhi(\E[f|\Bn\Yn\Omegan])$.

(ii) Similarly, we have 
\begin{align*}
    &\HPhi(\E[f|\An\Xn\Pin\Bn\Yn\Omegan]|\An\Xn\Pin)  \\
    =&\HPhi(\E[f|\wAn\wXn\wPin\wBn\wYn\wOmegan]|\wAn\wXn\wPin) \\
    =& \sum_{i=1}^n \HPhi(\E[f|\wAn\wXn\wPin\wB_{[i]}\wY_{[i]}\wOmega_{[i]}]|\wAn\wXn\wPin\wB_{[i-1]}\wY_{[i-1]}\wOmega_{[i-1]}) \\
    =& \sum_{i=1}^n \HPhi(\E[f|\wAn\wXn\wPin\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}]|\wAn\wXn\wPin\wB_{[i-1]}\wY_{[i-1]}\wOmega_{[i-1]}) \\
    & + \HPhi(\E[f|\wAn\wXn\wPin\wB_{[i]}\wY_{[i]}\wOmega_{[i]}]|\wAn\wXn\wPin\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]})
\end{align*}

Note that 
\begin{align*}
    &\sum_{i=1}^n \HPhi(\E[f|\wAn\wXn\wPin\wB_{[i]}\wY_{[i]}\wOmega_{[i]}]|\wAn\wXn\wPin\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}) \\
    =& \sum_{i=1}^n \sum_{j=1}^n \HPhi(\E[f|\wAn\wXn\wPin\wB_{[i]}\wY_{[i]}\wOmega_{[i]}]|\wAn\wXn\wPin\wS_i\wY_i\wOmega_i=j)p(\wOmega_i=j)\\
    =& \sum_{i=1}^n \sum_{j=1}^n \HPhi(\E[f|\wAn\wXn\wPin B_j S_j Y_j \Omega_j]|\wAn\wXn\wPin S_j Y_j \Omega_j=i) p(\wOmega_i=j)\\
    =& \sum_{j=1}^n \HPhi(\E[f|\wAn\wXn\wPin B_j S_j Y_j \Omega_j]|\wAn\wXn\wPin S_j Y_j \Omega_j)
\end{align*}





\section{Proof of Theorem \ref{thm:tensorization-for-boxes}} \label{apx:proof-main-theorem}
    By the data processing property, if the box $p_{A'B'|X'Y'}$ can be generated by the box $p_{\An\Xn\Pin\Bn\Yn\Omegan|X'Y'}$, we have for any $x',y'$, 
    \begin{align*}
        \fR_{\Phi}(\An\Xn\Pin;\Bn\Yn\Omegan|x';y') \subseteq \fR_{\Phi}(A';B'|x',y').
    \end{align*}

    Thus, it is sufficient to prove
    \begin{equation}
        \bigcap_{i=1}^n \fR_{\Phi} (A_i;B_i|X_i;Y_i)\subseteq \fR_{\Phi} (A_{[n]}X_{[n]}\Pi_{[n]};B_{[n]}Y_{[n]}\Omega_{[n]}|x',y')
    \end{equation}
    for any $x',y'$.  That is to say, $\forall (\lambda_1,\lambda_2)\in \bigcap_{i=1}^n \fR_{\Phi} (A_i,B_i|X_i,Y_i)$, we want to show $$\lambda_1 \HPhi(\E[f|A_{[n]}X_{[n]}\Pi_{[n]}]) \\+ \lambda_2 \HPhi(\E[f|B_{[n]}Y_{[n]}\Omega_{[n]}])\leq \HPhi(f)$$
for any function $f(\An,\Xn,\Pin,\Bn,\Yn,\Omegan)$. If  $\lambda_1+\lambda_2\leq 1$, the inequality is trivially satisfied because of the data processing inequality for the $\Phi$-entropy. Thus, we only need to show the inequality for pairs $(\lambda_1,\lambda_2)\in \bigcap_{i=1}^n \fR_{\Phi} (A_i,B_i|X_i,Y_i)$ that satisfy $\lambda_1+\lambda_2\geq 1$. 

Let us define
\begin{align}
        \chi(\lambda_1,\lambda_2) &\triangleq -\lambda_1 \HPhi(\E[f|\An\Xn\Pin]) - \lambda_2 \HPhi(\E[f|\Bn\Yn\Omegan]) + \HPhi(f).
        \end{align}
We need to show that $\chi(\lambda_1,\lambda_2)\geq 0$. 
    By Lemma \ref{lem:expand}, we have 
    \begin{align}
        \HPhi(\E[f|\An\Xn\Pin])
        &= \sum_{i=1}^n \HPhi(\E[f|\wT_i^e]|\wT_i)
        +\HPhi(\E[f|A_i \Tie]|\Tie) \\
        &=\sum_{i=1}^{n}\HPhi(\E[f|\wA_{[i-1]}\wX_{[i]}\wPi_{[i]}]|\wA_{[i-1]}\wX_{[i-1]}\wPi_{[i-1]})
        +\HPhi(\E[f|A_{i}\Tie S_i^e]|T_i^eS_i^e)\nonumber \\
        &\qquad +\HPhi(\E[f|A_i \Tie]|\Tie)
        -\HPhi(\E[f|A_{i}\Tie S_i^e]|T_i^eS_i^e)
    \end{align}
    and
    \begin{align}
        \HPhi(\E[f|\Bn\Yn\Omegan]) 
        &=\sum_{i=1}^n \HPhi(\E[f|\wS_i^e]|\wS_i)
        +\HPhi(\E[f|B_i\Sie]|\Sie)\\
        &=\sum_{i=1}^{n}\HPhi(\E[f|\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}]|\wB_{[i-1]}\wY_{[i-1]}\wOmega_{[i-1]})
        +\HPhi(\E[f|B_{i}\Tie S_i^e]|T_i^eS_i^e)\nonumber\\
        &\quad +\HPhi(\E[f|B_i \Sie]|\Sie)
        -\HPhi(\E[f|B_{i}\Tie S_i^e]|T_i^eS_i^e). 
    \end{align}


        Using the fact that $f$ is a function of $(\An,\Xn,\Pin,\Bn,\Yn,\Omegan)$, we can rewrite the above expression as follows:
       \begin{align}
        \chi(\lambda_1,\lambda_2) 
        &= -\sum_{i=1}^{n} \bigg[ \lambda_1 \HPhi(\E[f|\wA_{[i-1]}\wX_{[i]}\wPi_{[i]}]|\wA_{[i-1]}\wX_{[i-1]}\wPi_{[i-1]})
        +\lambda_1 \HPhi(\E[f|A_{i}\Tie S_i^e]|T_i^eS_i^e)\nonumber \\
        &\quad +\lambda_1 \HPhi(\E[f|A_i \Tie]|\Tie)
        -\lambda_1 \HPhi(\E[f|A_{i}\Tie S_i^e]|T_i^eS_i^e)\nonumber \\
        &\quad 
        +\lambda_2 \HPhi(\E[f|\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}]|\wB_{[i-1]}\wY_{[i-1]}\wOmega_{[i-1]})
        +\lambda_2 \HPhi(\E[f|B_{i}\Tie S_i^e]|T_i^eS_i^e)\nonumber\\
        &\quad +\lambda_2 \HPhi(\E[f|B_i \Sie]|\Sie)
        -\lambda_2\HPhi(\E[f|B_{i}\Tie S_i^e]|T_i^eS_i^e)
        \bigg] + \HPhi(\E[f|\An\Xn\Pin\Bn\Yn\Omegan])\label{eqnchif2}.
    \end{align} 
Next, we claim that
\begin{equation}
\lambda_1\HPhi(\E[f|A_i\Tie\Sie]|\Tie\Sie)
        +\lambda_2\HPhi(\E[f|B_i\Tie\Sie]|\Tie\Sie)\leq \HPhi(\E[f|A_iB_i\Tie\Sie]|\Tie\Sie).\label{eqnStep}
    \end{equation}
To show this equation, observe that from Lemma \ref{lem:mc} (i), we have the Markov chain $A_iB_i\rightarrow X_iY_i \rightarrow T_iS_i\Pi_i\Omega_i$. Fix $t_i,s_i,\pi_i,\omega_i$, then we have $p_{A_iB_i|X_iY_it_is_i\pi_i\omega_i}=p_{A_iB_i|X_iY_i}$. Thus, $p(A_iB_i|X_iY_it_is_i\pi_i\omega_i)$ is the same box as $p_{A_iB_i|X_iY_i}$. 

    As $(\lambda_1,\lambda_2)\in \bigcap_{i=1}^n \fR (A_i,B_i|X_i,Y_i)$, we have
    \begin{align*}
        &\lambda_1\HPhi(\E[f|A_iX_iY_i t_i s_i \pi_i \omega_i]|X_iY_i t_i s_i \pi_i \omega_i)
        +\lambda_2\HPhi(\E[f|B_iX_iY_i t_i s_i \pi_i \omega_i]|X_iY_i t_i s_i \pi_i \omega_i)\\
        &\quad \leq\HPhi(\E[f|A_iB_iX_iY_i t_i s_i \pi_i \omega_i]|X_iY_i t_i s_i \pi_i \omega_i).
    \end{align*}
    By taking average on $t_i,s_i,\pi_i,\omega_i$, we obtain \eqref{eqnStep}.

Next, observe that   
      \eqref{eqnStep} and \eqref{eqnchif2} imply that
$$\chi(\lambda_1,\lambda_2)\geq \chi'(\lambda_1,\lambda_2)$$
      where
    \begin{align}
    \chi'(\lambda_1,\lambda_2)
        &= -\sum_{i=1}^{n} \bigg[ \lambda_1 \HPhi(\E[f|\wA_{[i-1]}\wX_{[i]}\wPi_{[i]}]|\wA_{[i-1]}\wX_{[i-1]}\wPi_{[i-1]})
        \nonumber \\
        &\quad +\lambda_1 \HPhi(\E[f|A_i \Tie]|\Tie)
        -\lambda_1 \HPhi(\E[f|A_{i}\Tie S_i^e]|T_i^eS_i^e)\nonumber \\
        &\quad 
        +\lambda_2 \HPhi(\E[f|\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}]|\wB_{[i-1]}\wY_{[i-1]}\wOmega_{[i-1]})
        \nonumber\\
        &\quad +\lambda_2 \HPhi(\E[f|B_i \Sie]|\Sie)
        -\lambda_2\HPhi(\E[f|B_{i}\Tie S_i^e]|T_i^eS_i^e) + \HPhi(\E[f|A_iB_i\Tie\Sie]|\Tie\Sie)
        \bigg]\nonumber\\
        &\quad + \HPhi(\E[f|\An\Xn\Pin\Bn\Yn\Omegan]).
    \end{align}
To show that $\chi(\lambda_1,\lambda_2)\geq 0$, it suffices to establish that
$\chi'(\lambda_1,\lambda_2)\geq 0$ for any arbitrary $\lambda_1, \lambda_2\in[0,1]$ satisfying $\lambda_1+\lambda_2\geq 1$. Since $\chi'(\lambda_1,\lambda_2)$ is linear in $\lambda_1$ and $\lambda_2$, it suffices to show this for the corner points of the set, i.e., for the three points $(1,0),(0,1)$ and $(1,1)$. By symmetry, we show this when 
    \begin{align*}
        \lambda_1=1,\qquad \lambda_2\in\{0,1\}.
    \end{align*}
    The proof for the other corner point is similar. 
We have
   \begin{align}
    \chi'(1,\lambda_2)
        =& -\sum_{i=1}^{n} \bigg[ \HPhi(\E[f|\wA_{[i-1]}\wX_{[i]}\wPi_{[i]}]|\wA_{[i-1]}\wX_{[i-1]}\wPi_{[i-1]})
        \nonumber \\
        &\quad + \HPhi(\E[f|A_i \Tie]|\Tie)
        - \HPhi(\E[f|A_{i}\Tie S_i^e]|T_i^eS_i^e)\nonumber \\
        &\quad 
        +\lambda_2 \HPhi(\E[f|\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}]|\wB_{[i-1]}\wY_{[i-1]}\wOmega_{[i-1]})
        \nonumber\\
        &\quad +\lambda_2 \HPhi(\E[f|B_i \Sie]|\Sie)
        -\lambda_2\HPhi(\E[f|B_{i}\Tie S_i^e]|T_i^eS_i^e) + \HPhi(\E[f|A_iB_i\Tie\Sie]|\Tie\Sie)
        \bigg]\nonumber\\
        & + \HPhi(\E[f|\An\Xn\Pin\Bn\Yn\Omegan]).\label{eqnn14}
    \end{align}
Next, consider the following expansion by the chain rule \eqref{eqn:CR1}:
\begin{align*}
    \HPhi(\E[f|\An\Xn\Pin\Bn\Yn\Omegan])=\HPhi(\E[f|\An\Xn\Pin]) + \HPhi(\E[f|\An\Xn\Pin\Bn\Yn\Omegan]|\An\Xn\Pin)
\end{align*}
The first term on the right hand side can be expanded using part (i) of Lemma \ref{lem:expand} as follows:
\begin{align*}
    \HPhi(\E[f|\An\Xn\Pin]) &=\sum_{i=1}^{n}  \HPhi(\E[f|\wA_{[i-1]}\wX_{[i]}\wPi_{[i]}]|\wA_{[i-1]}\wX_{[i-1]}\wPi_{[i-1]})
        + \HPhi(\E[f|A_i \Tie]|\Tie)
\end{align*}
Therefore, the above two equations imply
\begin{align*}
\HPhi(\E[f|\An\Xn\Pin\Bn\Yn\Omegan]|\An\Xn\Pin)&=
\HPhi(\E[f|\An\Xn\Pin\Bn\Yn\Omegan])\\&\qquad-\sum_{i=1}^{n}  \HPhi(\E[f|\wA_{[i-1]}\wX_{[i]}\wPi_{[i]}]|\wA_{[i-1]}\wX_{[i-1]}\wPi_{[i-1]})
        \\&\qquad-\sum_{i=1}^{n} \HPhi(\E[f|A_i \Tie]|\Tie) 
\end{align*}




    Therefore, we can rewrite \eqref{eqnn14} as
    \begin{align}
    \chi'(1,\lambda_2)
        &= -\sum_{i=1}^{n} \bigg[-\HPhi(\E[f|A_{i}\Tie S_i^e]|T_i^eS_i^e) +\lambda_2 \HPhi(\E[f|\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}]|\wB_{[i-1]}\wY_{[i-1]}\wOmega_{[i-1]})
        \nonumber\\
        &\quad +\lambda_2 \HPhi(\E[f|B_i \Sie]|\Sie)
        -\lambda_2\HPhi(\E[f|B_{i}\Tie S_i^e]|T_i^eS_i^e) + \HPhi(\E[f|A_iB_i\Tie\Sie]|\Tie\Sie)
        \bigg]\nonumber\\
        &\quad + \HPhi(\E[f|\An\Xn\Pin\Bn\Yn\Omegan]|\An\Xn\Pin).
    \end{align}
Next, note that by the chain rule \eqref{eqn:CR2}, we have
\begin{align*}
\HPhi(\E[f|A_iB_i\Tie\Sie]|\Tie\Sie) &= \HPhi(\E[f|A_{i}\Tie S_i^e]|T_i^eS_i^e) + \HPhi(\E[f|A_iB_i\Tie\Sie]|A_i \Tie\Sie)
\end{align*}
    and therefore we obtain
    \begin{align}
    \chi'(1,\lambda_2)
        &=  -\sum_{i=1}^{n} \bigg[\lambda_2 \HPhi(\E[f|\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}]|\wB_{[i-1]}\wY_{[i-1]}\wOmega_{[i-1]})
        \nonumber\\
        &\quad +\lambda_2 \HPhi(\E[f|B_i \Sie]|\Sie)
        -\lambda_2\HPhi(\E[f|B_{i}\Tie S_i^e]|T_i^eS_i^e) + \HPhi(\E[f|A_iB_i\Tie\Sie]|A_i \Tie\Sie)
        \bigg]\nonumber\\
        &\quad + \HPhi(\E[f|\An\Xn\Pin\Bn\Yn\Omegan]|\An\Xn\Pin).
    \end{align}




    By Lemma \ref{lem:expand} (ii), we have 
    \begin{align*}
       \HPhi(\E[f|\An\Xn\Pin\Bn\Yn\Omegan]|\An\Xn\Pin) = \sum_{i=1}^n &\HPhi(\E[f|\An\Xn\Pin\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}]|\An\Xn\Pin\wS_i)\\
       &+\HPhi(\E[f|\An\Xn\Pin\bBi\bYi\bOmegai]|\An\Xn\Pin S_i^e)
    \end{align*}
    Then 
    \begin{align}
        \chi'(1,\lambda_2) &= \sum_{i=1}^{n} \bigg[-\lambda_2 \HPhi(\E[f|\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}]|\wB_{[i-1]}\wY_{[i-1]}\wOmega_{[i-1]})
        \nonumber\\
        &\qquad -\lambda_2 \HPhi(\E[f|B_i \Sie]|\Sie)
        +\lambda_2\HPhi(\E[f|B_{i}\Tie S_i^e]|T_i^eS_i^e) -\HPhi(\E[f|A_iB_i\Tie\Sie]|A_i \Tie\Sie)
        \nonumber\\
        &\qquad + \HPhi(\E[f|\An\Xn\Pin\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}]|\An\Xn\Pin\wS_i)\\
        &\qquad+\HPhi(\E[f|\An\Xn\Pin\bBi\bYi\bOmegai]|\An\Xn\Pin S_i^e)\bigg]
    \end{align}
    

    The three parts of Corollary \ref{cor:mc} stated that
    \begin{align}
\HPhi(\E[f|B_i\Tie\Sie]|\Tie\Sie)&\geq\HPhi(\E[f|B_i\Sie]|\Sie),\\
\HPhi(\E[f|\An\Xn\Pin\bBi\bYi\bOmegai]|\An\Xn\Pin S_i^e)
        &\geq 
\HPhi(\E[f|A_iB_i\Tie\Sie]|A_i\Tie\Sie),\\
\HPhi(\E[f|\An\Xn\Pin\wB_{[i-1]}\wYi\wOmegai]|\An\Xn\Pin\wS_i)
        &\geq
\HPhi(\E[f|\wB_{[i-1]}\wY_{[i]}\wOmega_{[i]}]|\wB_{[i-1]}\wY_{[i-1]}\wOmega_{[i-1]}).
    \end{align}
For $\lambda_2=0$, the second inequality above implies that $\chi'(1,\lambda_2)\geq 0$. For $\lambda_2=1$, we can use all of the above three inequalities to see that $\chi'(1,\lambda_2)\geq 0$. This establishes the desired inequality. The proof is complete.



\end{document}
