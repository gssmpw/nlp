\section{Related Work}
\subsection{Adversarial threats and defense }

Adversarial threats refer to attempts by attackers to deceive or mislead DNNs by providing maliciously crafted inputs, resulting in incorrect predictions or classifications, or generating content misaligned with human expectations. These threats normally include adversarial example attacks and backdoor attacks.

The objective of attackers posing the threats can be framed as a general optimization problem that encompasses both adversarial and backdoor attacks. With an optimized alteration function $A(x)^*$ on input text x, the attackers try to maximize the loss between the output of the target model and the output expected. And the two types of attack primarily differentiate themselves from others through different constraints on the attack parameter $A(x)$ \text{and } $\Delta\theta$.
%optimization variables $\delta$ or $\theta$.
%\begin{align*}
%\min_{\delta, \theta} \quad L(\theta, \delta) = &\ %\mathbb{E}_{(x, y) \sim D} \left[ \ell(f_\theta(x), y) %\right] \\
%&+ \alpha \, \mathbb{E}_{(x, y) \sim D'} \left[ %\ell(f_\theta(x + \delta), y_t) \right] \\
%&+ \lambda_1 C_\delta(\delta) + \lambda_2 %C_\theta(\theta)
%\end{align*}
%where:
%\begin{itemize}
%    \item $f_\theta$ is the model with parameters %$\theta$
%    \item $x$ is the input text and $y$ is the according %output align with human's expectation.
%    \item $\delta$ is the perturbation added to the input
%    \item $y_t$ is the output align with attacker's %target.
%    \item 
%\end{itemize}
\begin{align} 
\max_{A(x), \Delta\theta} \quad & \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ L\left(f\left(A(x);\, \theta + \Delta\theta\right),\, y\right) \right] \notag \\
\text{s.t.} \quad &\text{ Constraints on } A(x),  \Delta\theta,
\label{eq:optimization}
\end{align}

where $A(x)$ denotes adversarial alteration of input and $\Delta\theta$  modification to model parameters.$f(;\theta)$ is the model function with parameters $\theta$. And $L(\cdot;\cdot)$ is the loss function (e.g., cross-entropy).$x$ is input to the model while $y$ is expected output of input x.


\textbf{Adversarial example attacks.} occur during the inference phase of a model, targeting a clean model whose parameters have not been tampered with **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**. These attacks typically involve adding optimized perturbations to the input that are imperceptible to human observers, tricking the model into making incorrect predictions. 

Such attacks are irrelevant with training process. No alteration occurs to training dataset and the model parameter $\theta$ is fixed during attack. The constraints on the attack parameters are given by $\Delta\theta = 0$, and the optimal adversarial transformation \( A(x)^* \) is defined as:
\begin{equation}
A(x)^* = \arg\min_{A(x)} \text{SemSim}(x, A(x)),
\end{equation}
where \( \text{SemSim}(\cdot, \cdot) \) denotes the semantic similarity metric between two strings.

Existing defenses against adversarial attacks include adversarial training **Madry et al., "Towards Deep Learning Models Resistant to Adversaries"**, gradient masking **Huang et al., "Sobolev Training for Neural Networks"**, and robust optimization **Wong et al., "Provable Defenses Against Adversarial Attacks".** Among these, adversarial training—augmenting training data with adversarial examples—is widely studied but often incurs high computational costs and limited generalizability to novel attacks.

Few online defense methods exist for adversarial examples, with most relying on perturbation detection and dictionary-based correction **Feinman et al., "Detecting Adversarial Samples from Limited Queries"** ____. A recent approach, ATINTER **Athalye et al., "Synthesizing Robust Adversarially Trajectory Intervals for Neural Networks"**, intercepts and rewrites adversarial inputs, preserving classification accuracy while neutralizing attacks in real time.

%\textbf{Poisoning attacks}, on the other hand, take place during the training phase of a model. They involve contaminating the training dataset with imperceptible perturbations, ultimately degrading the performance of the model trained on the poisoned data, or to induce specific behaviors at inference time. 


%Research on poisoning defenses has focused on approaches like data sanitization, which aims to detect and remove poisoned instances, and robust training, which is designed to mitigate the impact of poisoned samples. However, these methods often struggle to achieve a balance between security and model utility, as overly aggressive sanitization can remove valuable training data.


\textbf{Backdoor attacks.} are another form of adversarial threat that occur during both the training and inference phases. These attacks involve either poisoning training data or modifying model parameters to embed a hidden backdoor in the model. The backdoor remains dormant during normal operations but is activated when inputs containing specific malicious features, termed triggers, are provided. The model then produces outputs in line with the attacker's intent, regardless of its expected behavior. 
Besides the success rate of the attack, the attacker concern about the stealthiness of the trigger and ensures the model's performance on clean samples remains unaffected after the backdoor is implanted. The constraints on the attack variables are given by:
\begin{equation}
\Delta\theta = \operatorname*{arg\,min}_{\Delta\theta} 
\mathbb{E}_{(x, y) \sim \mathcal{D} } 
\left[ L\left( f\left( x;\, \theta+\Delta\theta \right),\, y \right) \right]
\end{equation}
and
\begin{equation}
A(x) = x \oplus t \quad \text{s.t.} \quad \| t \|_p \leq \epsilon_t,
\end{equation}


where $\epsilon_t$ is a small positive value controlling the allowable magnitude of trigger t.

Existing defense methods against backdoor attacks include modifying the model to eliminate backdoors through techniques such as model pruning **Ding et al., "Defending Against Backdoor Attacks with Robust Pruning"**, defensive distillation **Pang et al., "Defensive Distillation: Regularizing Neural Networks for Adversarial Robustness"**, and training strategy adjustments **Xiao et al., "Improving Adversarial Robustness via Promoting Ensemble Diversity"**. Additionally, detection-based approaches, such as Li et al.'s work on "Backdoor Attacks and Defenses Against Machine Learning Models," are relevant to the field of adversarial robustness. **Li et al., "Deep Learning Models under the Attack of Adversarial Examples"**, provide insights into building reliable machine learning models.

\subsection{Model extraction and knowledge distillation}
Model extraction refers to the process by which an adversary aims to recreate a target machine learning model $f^*$ by interacting with it and gathering input-output pairs. The goal is to build a surrogate model $\hat{f}$ such that $\hat{f}\approx f^*$ in terms of input-output behavior **Tramèr et al., "Ensemble Adversarial Training: Attacks and Defenses"** ____. In the era of LLMs capable of performing a wide range of tasks, model extraction is a useful skill for building vertical surrogate models for some specific function.

The extraction process starts from data collecting. For a prepaired dataset$\{x_i\}^N_{i=1}$, collect according $y_i=f^*(x_i)$ to form dataset $D =\{(x_i,y_i)\}^N_{i=1}$. The optimization problem can be described as:
\begin{equation}
\hat{f} = \arg\min_f \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f(x_i), y_i),
\end{equation}
where the loss function measuring the discrepancy between the surrogate model's output $f(x_i)$ and the target output $y_i$.

Knowledge distillation **Hinton et al., "Distilling the Knowledge in a Neural Network"** is relevant term  for a process resembling model extraction. Sometimes the boundaries between the two terms can be blurred. In comparison, model extraction normally have only access to the hard labels of queries. And in knowledge distillation, the surrogate model as a surrogate model, can always reach the informative soft label of the target model, which is a probability distribution over all classes rather than a single class prediction. For generative language models, such soft labels can be logits value, or probability distribution for each token in the vocabulary.