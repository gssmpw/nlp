\section{Related Work}
\subsection{Adversarial threats and defense }

Adversarial threats refer to attempts by attackers to deceive or mislead DNNs by providing maliciously crafted inputs, resulting in incorrect predictions or classifications, or generating content misaligned with human expectations. These threats normally include adversarial example attacks and backdoor attacks.

The objective of attackers posing the threats can be framed as a general optimization problem that encompasses both adversarial and backdoor attacks. With an optimized alteration function $A(x)^*$ on input text x, the attackers try to maximize the loss between the output of the target model and the output expected. And the two types of attack primarily differentiate themselves from others through different constraints on the attack parameter $A(x)$ \text{and } $\Delta\theta$.
%optimization variables $\delta$ or $\theta$.
%\begin{align*}
%\min_{\delta, \theta} \quad L(\theta, \delta) = &\ %\mathbb{E}_{(x, y) \sim D} \left[ \ell(f_\theta(x), y) %\right] \\
%&+ \alpha \, \mathbb{E}_{(x, y) \sim D'} \left[ %\ell(f_\theta(x + \delta), y_t) \right] \\
%&+ \lambda_1 C_\delta(\delta) + \lambda_2 %C_\theta(\theta)
%\end{align*}
%where:
%\begin{itemize}
%    \item $f_\theta$ is the model with parameters %$\theta$
%    \item $x$ is the input text and $y$ is the according %output align with human's expectation.
%    \item $\delta$ is the perturbation added to the input
%    \item $y_t$ is the output align with attacker's %target.
%    \item 
%\end{itemize}
\begin{align} 
\max_{A(x), \Delta\theta} \quad & \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ L\left(f\left(A(x);\, \theta + \Delta\theta\right),\, y\right) \right] \notag \\
\text{s.t.} \quad &\text{ Constraints on } A(x),  \Delta\theta,
\label{eq:optimization}
\end{align}

where $A(x)$ denotes adversarial alteration of input and $\Delta\theta$  modification to model parameters.$f(;\theta)$ is the model function with parameters $\theta$. And $L(\cdot;\cdot)$ is the loss function (e.g., cross-entropy).$x$ is input to the model while $y$ is expected output of input x.


\textbf{Adversarial example attacks.} occur during the inference phase of a model, targeting a clean model whose parameters have not been tampered with ____. These attacks typically involve adding optimized perturbations to the input that are imperceptible to human observers, tricking the model into making incorrect predictions. 

Such attacks are irrelevant with training process. No alteration occurs to training dataset and the model parameter $\theta$ is fixed during attack. The constraints on the attack parameters are given by $\Delta\theta = 0$, and the optimal adversarial transformation \( A(x)^* \) is defined as:
\begin{equation}
A(x)^* = \arg\min_{A(x)} \text{SemSim}(x, A(x)),
\end{equation}
where \( \text{SemSim}(\cdot, \cdot) \) denotes the semantic similarity metric between two strings.

Existing defenses against adversarial attacks include adversarial training ____, gradient masking ____, and robust optimization ____. Among these, adversarial training—augmenting training data with adversarial examples—is widely studied but often incurs high computational costs and limited generalizability to novel attacks.

Few online defense methods exist for adversarial examples, with most relying on perturbation detection and dictionary-based correction ____ ____. A recent approach, ATINTER ____, intercepts and rewrites adversarial inputs, preserving classification accuracy while neutralizing attacks in real time.

%\textbf{Poisoning attacks}, on the other hand, take place during the training phase of a model. They involve contaminating the training dataset with imperceptible perturbations, ultimately degrading the performance of the model trained on the poisoned data, or to induce specific behaviors at inference time. 


%Research on poisoning defenses has focused on approaches like data sanitization, which aims to detect and remove poisoned instances, and robust training, which is designed to mitigate the impact of poisoned samples. However, these methods often struggle to achieve a balance between security and model utility, as overly aggressive sanitization can remove valuable training data.


\textbf{Backdoor attacks.} are another form of adversarial threat that occur during both the training and inference phases. These attacks involve either poisoning training data or modifying model parameters to embed a hidden backdoor in the model. The backdoor remains dormant during normal operations but is activated when inputs containing specific malicious features, termed triggers, are provided. The model then produces outputs in line with the attacker's intent, regardless of its expected behavior. 
Besides the success rate of the attack, the attacker concern about the stealthiness of the trigger and ensures the model's performance on clean samples remains unaffected after the backdoor is implanted. The constraints on the attack variables are given by:
\begin{equation}
\Delta\theta = \operatorname*{arg\,min}_{\Delta\theta} 
\mathbb{E}_{(x, y) \sim \mathcal{D} } 
\left[ L\left( f\left( x;\, \theta+\Delta\theta \right),\, y \right) \right]
\end{equation}
and
\begin{equation}
A(x) = x \oplus t \quad \text{s.t.} \quad \| t \|_p \leq \epsilon_t,
\end{equation}


where $\epsilon_t$ is a small positive value controlling the allowable magnitude of trigger t.

Existing defense methods against backdoor attacks include modifying the model to eliminate backdoors through techniques such as model pruning ____, defensive distillation ____, and training strategy adjustments ____. Additionally, detection-based approaches, such as reverse engineering triggers ____ and neuron analysis ____, are used to identify and exclude models that contain backdoors.

Several online defense schemes have also been proposed to counter backdoor attacks, including RAP ____, STRIP ____, and ONION ____. RAP introduces an additional trigger in the embedding layer and detects poisoned samples by observing the drop in the model's output probability for the target class. STRIP detects anomalous inputs by analyzing the consistency of model predictions when the input is mixed with random noise; inconsistent predictions indicate the presence of backdoor triggers. ONION focuses on removing suspicious perturbations from inputs to mitigate the effects of backdoor attacks by ensuring that only benign features are preserved. These online defenses provide detect and mitigate backdoor threats in real time. However, all the three schemes need the target model to be under whitebox settings, and the ability of attackers to craft diverse and adaptive triggers can make them perform poorly ____.
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{overview.png}
    \caption{A demonstration of reformulation defense workflow.}
    \label{fig:enter-label}
    \vspace{-1em}
\end{figure*}
\subsection{Large language models (LLMs) }SOTA LLMs like GPT-4o possess remarkable capability to encode input text into a latent representation and then decode this representation to generate new, relevent text ____. This process allows the models to understand the meaning and context of the input and produce coherent and contextually relevant outputs.
The whole process include 5 main parts. Firstly tokenization break input text into smaller units called tokens. And then tokens are converted into a numerical vector termed embedding through an Embedding Function. Thirdly the sequence of embeddings are processed by transformer layers with self-attention mechanism to capture contextual information, and then  encoded into an internal latent representation for the entire input. Fourthly, the model decode the latent representation and generate a new sequence of tokens under the autoregressive generation algorithm. At last, the generated tokens are detokenized into human readable text.

Prompt engineering plays a crucial role in effectively utilizing LLMs ____. It involves crafting input prompts in a way that guides the model to produce the desired output. And it impacts how the model interprets and generates text, influencing the quality and relevance of the reformulated output.

Assume the input text is $x$ and the chosen prompt is $p$, the process of LLM reformulate the input into output $x'$ can be denoted as:
\begin{equation}
\small
x' = \left( \text{Detokenize} \circ f_{dec} \circ f_{enc} \circ E \circ \text{Tokenize} \right)(x+p),
\end{equation}
where $E$ is the embedding function.  $f_{enc}$ stands for the encoding function. And $f_{dec}$ denotes the decoding function.
\subsection{Model extraction and knowledge distillation}
Model extraction refers to the process by which an adversary aims to recreate a target machine learning model $f^*$ by interacting with it and gathering input-output pairs. The goal is to build a surrogate model $\hat{f}$ such that $\hat{f}\approx f^*$ in terms of input-output behavior ____ ____. In the era of LLMs capable of performing a wide range of tasks, model extraction is a useful skill for building vertical surrogate models for some specific function.

The extraction process starts from data collecting. For a prepaired dataset$\{x_i\}^N_{i=1}$, collect according $y_i=f^*(x_i)$ to form dataset $D =\{(x_i,y_i)\}^N_{i=1}$. The optimization problem can be described as:
\begin{equation}
\hat{f} = \arg\min_f \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f(x_i), y_i),
\end{equation}
where the loss function measuring the discrepancy between the surrogate model's output $f(x_i)$ and the target output $y_i$.

Knowledge distillation ____ is relevant term  for a process resembling model extraction. Sometimes the boundaries between the two terms can be blurred. In comparison, model extraction normally have only access to the hard labels of queries. And in knowledge distillation, the surrogate model as a surrogate model, can always reach the informative soft label of the target model, which is a probability distribution over all classes rather than a single class prediction. For generative language models, such soft labels can be logits value, or probability distribution for each token in the vocabulary.