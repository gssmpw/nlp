%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{caption}
\captionsetup[figure]{belowskip=0pt, aboveskip=2pt} 
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Reformulation is All You Need: Addressing Malicious Textual Features for DNN models}
\setlength{\textfloatsep}{2pt plus 1pt minus 1pt}
\begin{document}

\twocolumn[
\icmltitle{Reformulation is All You Need: Addressing Malicious Text Features in DNNs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yi Jiang}{yyy}
\icmlauthor{Oubo Ma}{yyy}
\icmlauthor{Yong Yang}{yyy}
\icmlauthor{Tong Zhang}{yyy}
\icmlauthor{Shouling Ji}{yyy}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{College of Computer Science, Zhejiang University, Hangzhou, China}
%\icmlaffiliation{comp}{Company Name, Location, Country}

%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Shouling Ji}{sji@zju.edu.cn}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{   } % otherwise use the standard text.

\begin{abstract}
%Human language consists of a wide range of intricate and diverse implicit features, which could be exploited by attackers to launch adversarial or backdoor attacks to compromise machine learning models for NLP tasks. Existing model-oriented defense may consume significant amount of computational resources as model size grows,while sample-oriented ones usually target at single attack vectors and specific attacking schemes, being vulnerable to adaptive attacks. We observe that the essential cause of both adversarial and backdoor attacks lie in the encoding process of machine learning models, in which negligible textual features for human comprehending are wrongly encoded with influential weight by less robust or trojaned models. Aiming at a solution united for both adversarial and backdoor attacks, and adaptive for various types of malicious textual features, we propose an efficient and effective defense framework,impairing potential malicious features in textual inputs with reformulation modules while preserving the core meaning of the original input text. Intensive experiments show our work outperform existing sample-oriented defense baselines towards diverse of malicious features.
Human language encompasses a wide range of intricate and diverse implicit features, which attackers can exploit to launch adversarial or backdoor attacks, compromising DNN models for NLP tasks. Existing model-oriented defenses often require substantial computational resources as model size increases, whereas sample-oriented defenses typically focus on specific attack vectors or schemes, rendering them vulnerable to adaptive attacks. We observe that the root cause of both adversarial and backdoor attacks lies in the encoding process of DNN models, where subtle textual features, negligible for human comprehension, are erroneously assigned significant weight by less robust or trojaned models. Based on it we propose a unified and adaptive defense framework that is effective against both adversarial and backdoor attacks. Our approach leverages reformulation modules to address potential malicious features in textual inputs while preserving the original semantic integrity. Extensive experiments demonstrate that our framework outperforms existing sample-oriented defense baselines across a diverse range of malicious textual features.
\end{abstract}


\section{INTRODUCTION}

Deep Neural Networks (DNNs) initially gained significant attention in the field of computer vision \cite{cv1} \cite{cv2} and have since achieved remarkable breakthroughs in natural language processing (NLP) \cite{NLP1} \cite{NLP2} \cite{NLP3}. Today's generative large-scale models even advancing the field toward artificial general intelligence (AGI) \cite{agi1} \cite{agi2}. Despite these successes, the reliability of DNNs remains questionable due to their lack of interpretability \cite{interpretability1} \cite{interpretability2}, like the sword of Damocles hanging over the deployment of these models, which hinders the deployment of DNNs in safety-critical applications, where trust and predictability are paramount. The intrinsic properties of neural networks make them vulnerable to 2 notorious attack vectors: adversarial example attacks \cite{ae1} \cite{ae2} and backdoor attacks \cite{bd1_badnets} \cite{bd2_addsent} \cite{bd3_synbkd} \cite{bd4_style}. Subtle perturbations added to inputs or training data can lead to unintended model behaviors, posing significant risks in practical applications.

With the growing adoption of NLP applications in DNN models, textual features such as typos \cite{ae2_deepwordbug}, rare words \cite{ae3_hotflip}, unique styles \cite{bd4_style}, and syntactic structures\cite{bd3_synbkd} can be exploited by attackers. While often overlooked in everyday use, these subtle characteristics enable adversarial example generation or data poisoning, subtly manipulating model behavior. By injecting rare features or modifying labels, attackers can craft adversarial samples to mislead clean models or implant backdoors to create trojaned models.

To mitigate these threats, extensive research in both industry and academia has led to the development of diverse defense strategies. Against adversarial attacks, approaches such as adversarial training enhance model robustness, perturbation control mitigates input-level threats, and certification-based methods provide risk bounds \cite{ae_survey1} \cite{ae_survey2} \cite{ae_survey3}. For backdoor attacks, defenses include model pruning, defensive distillation, and training adjustments to remove hidden backdoors, along with reverse-engineering triggers and neuron analysis to detect compromised models \cite{bd_survey1}. \cite{bd_survey2}.

\textbf{Motivation.}  The existing defense mechanisms against adversarial threats can be categorized into two broad types. The first type \cite{adversarial_training} \cite{gradient_masking} \cite{robust} can be counted as model-oriented ones, focusing on enhancing model robustness, eliminating or detecting backdoors at the model level. As the number of model parameters increases, these approaches may become prohibitively expensive and impractical for real-world applications. The second type\cite{ae_perturbation_control} \cite{ae_perturbation_control2} \cite{bd_survey1} is sample-oriented, or online defense, aiming to detect and neutralize anomalous features in the input layer. While more feasible to implement, these methods are often specific to certain feature types, lacking generalization across different types of malicious features and remaining vulnerable to adaptive attacks.
To cope with risks in different attack vectors and threats from various attack algorithms in real-world scenarios, the cost of defending by integrating multiple effective defense modules can be prohibitively high for model deployers, while extra effort required for an adversary to switch to an alternative attack strategy can be trivial. This underscores the need for a unified solution capable of handling adaptive attacks in different attack vectors. 

\textbf{Problem statements.} Our work addresses these gaps by introducing a unified, model-agnostic defense framework for securing DNN-based NLP applications. Our approach effectively mitigates adversarial example and backdoor attacks within a single framework, countering diverse malicious textual manipulations while enhancing resilience against adaptive and future threats.

DNN models encode input text into high-dimensional representations for classification or generation tasks. However, less robust or trojaned models may inappropriately assign significant weights to textual features that are negligible to human comprehension. As illustrated in Figure 1, our framework pre-encodes the core semantics of input text and reconstructs it in a way that removes or weakens attack-specific features irrelevant to meaning. This shields the target model from hidden manipulations introduced by attackers.

Given the remarkable natural language understanding and generation capabilities of modern large language models (LLMs), we leverage their encoding and generation functionalities to power our reformulation modules. In resource-constrained environments or scenarios with data security concerns that prevent cloud-based processing, we extract knowledge from LLMs to train lightweight local surrogate models with knowledge distillation techniques. This ensures efficient, private, and robust text encoding and generation.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{concept.png}
    \caption{Overview of the reformulation defense pipeline.}
    \label{fig:enter-label}
\end{figure}

\textbf{Contributions.} To summarize the primary outcomes of this work, we highlight the following key aspects that set our approach apart from existing methods.
\begin{itemize}
\item We introduce a unified defense framework that mitigates both adversarial and backdoor attacks,which are two major threats to DNN-based NLP models.
\item Our model-agnostic approach effectively neutralizes diverse malicious features across different granularities through text reformulation, enhancing resilience against adaptive attacks.
\item By extracting knowledge from SOTA LLMs to train lightweight local surrogate models, we ensure practical deployment in efficiency- and privacy-sensitive scenarios.
\end{itemize}






\section{Related Work}
\subsection{Adversarial threats and defense }

Adversarial threats refer to attempts by attackers to deceive or mislead DNNs by providing maliciously crafted inputs, resulting in incorrect predictions or classifications, or generating content misaligned with human expectations. These threats normally include adversarial example attacks and backdoor attacks.

The objective of attackers posing the threats can be framed as a general optimization problem that encompasses both adversarial and backdoor attacks. With an optimized alteration function $A(x)^*$ on input text x, the attackers try to maximize the loss between the output of the target model and the output expected. And the two types of attack primarily differentiate themselves from others through different constraints on the attack parameter $A(x)$ \text{and } $\Delta\theta$.
%optimization variables $\delta$ or $\theta$.
%\begin{align*}
%\min_{\delta, \theta} \quad L(\theta, \delta) = &\ %\mathbb{E}_{(x, y) \sim D} \left[ \ell(f_\theta(x), y) %\right] \\
%&+ \alpha \, \mathbb{E}_{(x, y) \sim D'} \left[ %\ell(f_\theta(x + \delta), y_t) \right] \\
%&+ \lambda_1 C_\delta(\delta) + \lambda_2 %C_\theta(\theta)
%\end{align*}
%where:
%\begin{itemize}
%    \item $f_\theta$ is the model with parameters %$\theta$
%    \item $x$ is the input text and $y$ is the according %output align with human's expectation.
%    \item $\delta$ is the perturbation added to the input
%    \item $y_t$ is the output align with attacker's %target.
%    \item 
%\end{itemize}
\begin{align} 
\max_{A(x), \Delta\theta} \quad & \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ L\left(f\left(A(x);\, \theta + \Delta\theta\right),\, y\right) \right] \notag \\
\text{s.t.} \quad &\text{ Constraints on } A(x),  \Delta\theta,
\label{eq:optimization}
\end{align}

where $A(x)$ denotes adversarial alteration of input and $\Delta\theta$  modification to model parameters.$f(;\theta)$ is the model function with parameters $\theta$. And $L(\cdot;\cdot)$ is the loss function (e.g., cross-entropy).$x$ is input to the model while $y$ is expected output of input x.


\textbf{Adversarial example attacks.} occur during the inference phase of a model, targeting a clean model whose parameters have not been tampered with \cite{ae1}. These attacks typically involve adding optimized perturbations to the input that are imperceptible to human observers, tricking the model into making incorrect predictions. 

Such attacks are irrelevant with training process. No alteration occurs to training dataset and the model parameter $\theta$ is fixed during attack. The constraints on the attack parameters are given by $\Delta\theta = 0$, and the optimal adversarial transformation \( A(x)^* \) is defined as:
\begin{equation}
A(x)^* = \arg\min_{A(x)} \text{SemSim}(x, A(x)),
\end{equation}
where \( \text{SemSim}(\cdot, \cdot) \) denotes the semantic similarity metric between two strings.

Existing defenses against adversarial attacks include adversarial training \cite{adversarial_training}, gradient masking \cite{gradient_masking}, and robust optimization \cite{robust}. Among these, adversarial training—augmenting training data with adversarial examples—is widely studied but often incurs high computational costs and limited generalizability to novel attacks.

Few online defense methods exist for adversarial examples, with most relying on perturbation detection and dictionary-based correction \cite{ae_perturbation_control} \cite{ae_perturbation_control2}. A recent approach, ATINTER \cite{ATINTER}, intercepts and rewrites adversarial inputs, preserving classification accuracy while neutralizing attacks in real time.

%\textbf{Poisoning attacks}, on the other hand, take place during the training phase of a model. They involve contaminating the training dataset with imperceptible perturbations, ultimately degrading the performance of the model trained on the poisoned data, or to induce specific behaviors at inference time. 


%Research on poisoning defenses has focused on approaches like data sanitization, which aims to detect and remove poisoned instances, and robust training, which is designed to mitigate the impact of poisoned samples. However, these methods often struggle to achieve a balance between security and model utility, as overly aggressive sanitization can remove valuable training data.


\textbf{Backdoor attacks.} are another form of adversarial threat that occur during both the training and inference phases. These attacks involve either poisoning training data or modifying model parameters to embed a hidden backdoor in the model. The backdoor remains dormant during normal operations but is activated when inputs containing specific malicious features, termed triggers, are provided. The model then produces outputs in line with the attacker's intent, regardless of its expected behavior. 
Besides the success rate of the attack, the attacker concern about the stealthiness of the trigger and ensures the model's performance on clean samples remains unaffected after the backdoor is implanted. The constraints on the attack variables are given by:
\begin{equation}
\Delta\theta = \operatorname*{arg\,min}_{\Delta\theta} 
\mathbb{E}_{(x, y) \sim \mathcal{D} } 
\left[ L\left( f\left( x;\, \theta+\Delta\theta \right),\, y \right) \right]
\end{equation}
and
\begin{equation}
A(x) = x \oplus t \quad \text{s.t.} \quad \| t \|_p \leq \epsilon_t,
\end{equation}


where $\epsilon_t$ is a small positive value controlling the allowable magnitude of trigger t.

Existing defense methods against backdoor attacks include modifying the model to eliminate backdoors through techniques such as model pruning \cite{fine-pruning}, defensive distillation \cite{defensive_distillation}, and training strategy adjustments \cite{bd_trainging}. Additionally, detection-based approaches, such as reverse engineering triggers \cite{reverse_engineering} and neuron analysis \cite{neuron_analysis}, are used to identify and exclude models that contain backdoors.

Several online defense schemes have also been proposed to counter backdoor attacks, including RAP \cite{rap}, STRIP \cite{strip}, and ONION \cite{onion}. RAP introduces an additional trigger in the embedding layer and detects poisoned samples by observing the drop in the model's output probability for the target class. STRIP detects anomalous inputs by analyzing the consistency of model predictions when the input is mixed with random noise; inconsistent predictions indicate the presence of backdoor triggers. ONION focuses on removing suspicious perturbations from inputs to mitigate the effects of backdoor attacks by ensuring that only benign features are preserved. These online defenses provide detect and mitigate backdoor threats in real time. However, all the three schemes need the target model to be under whitebox settings, and the ability of attackers to craft diverse and adaptive triggers can make them perform poorly \cite{cui_unified_2022}.
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{overview.png}
    \caption{A demonstration of reformulation defense workflow.}
    \label{fig:enter-label}
    \vspace{-1em}
\end{figure*}
\subsection{Large language models (LLMs) }SOTA LLMs like GPT-4o possess remarkable capability to encode input text into a latent representation and then decode this representation to generate new, relevent text \cite{GPT4}. This process allows the models to understand the meaning and context of the input and produce coherent and contextually relevant outputs.
The whole process include 5 main parts. Firstly tokenization break input text into smaller units called tokens. And then tokens are converted into a numerical vector termed embedding through an Embedding Function. Thirdly the sequence of embeddings are processed by transformer layers with self-attention mechanism to capture contextual information, and then  encoded into an internal latent representation for the entire input. Fourthly, the model decode the latent representation and generate a new sequence of tokens under the autoregressive generation algorithm. At last, the generated tokens are detokenized into human readable text.

Prompt engineering plays a crucial role in effectively utilizing LLMs \cite{prompt_engineering}. It involves crafting input prompts in a way that guides the model to produce the desired output. And it impacts how the model interprets and generates text, influencing the quality and relevance of the reformulated output.

Assume the input text is $x$ and the chosen prompt is $p$, the process of LLM reformulate the input into output $x'$ can be denoted as:
\begin{equation}
\small
x' = \left( \text{Detokenize} \circ f_{dec} \circ f_{enc} \circ E \circ \text{Tokenize} \right)(x+p),
\end{equation}
where $E$ is the embedding function.  $f_{enc}$ stands for the encoding function. And $f_{dec}$ denotes the decoding function.
\subsection{Model extraction and knowledge distillation}
Model extraction refers to the process by which an adversary aims to recreate a target machine learning model $f^*$ by interacting with it and gathering input-output pairs. The goal is to build a surrogate model $\hat{f}$ such that $\hat{f}\approx f^*$ in terms of input-output behavior \cite{mt1} \cite{mt2}. In the era of LLMs capable of performing a wide range of tasks, model extraction is a useful skill for building vertical surrogate models for some specific function.

The extraction process starts from data collecting. For a prepaired dataset$\{x_i\}^N_{i=1}$, collect according $y_i=f^*(x_i)$ to form dataset $D =\{(x_i,y_i)\}^N_{i=1}$. The optimization problem can be described as:
\begin{equation}
\hat{f} = \arg\min_f \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f(x_i), y_i),
\end{equation}
where the loss function measuring the discrepancy between the surrogate model's output $f(x_i)$ and the target output $y_i$.

Knowledge distillation \cite{ks} is relevant term  for a process resembling model extraction. Sometimes the boundaries between the two terms can be blurred. In comparison, model extraction normally have only access to the hard labels of queries. And in knowledge distillation, the surrogate model as a surrogate model, can always reach the informative soft label of the target model, which is a probability distribution over all classes rather than a single class prediction. For generative language models, such soft labels can be logits value, or probability distribution for each token in the vocabulary.


\section{Methodology}
As we mentioned previously, adversarial attack and backdoor attack have same goal to maximize the loss between the expected output and the compromised output with disturbed input $A(x)$. They differ mainly on the algorithms of $A(x)$ and contamination on model parameter $\theta$.

The defender's goal is to safeguard the model against these attacks by optimizing the model parameters $\theta$ to minimize the worst-case expected loss induced by any possible attack within the specified constraints. This leads to a minimax optimization problem, where the defender minimizes over $\theta$ while considering the maximum loss that an attacker could cause:
\begin{equation}
\begin{aligned} 
\min_{\theta} \max_{A(x), \Delta\theta} \quad & \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ L\left(f\left(A(x);\, \theta + \Delta\theta\right),\, y\right) \right] \\
\text{s.t.} \quad & \text{Constraints on } A(x), \Delta\theta.
\end{aligned}
\label{eq:robust_optimization}
\end{equation}

While such minimax optimization provides a rigorous framework for modeling and defending against adversarial and backdoor attacks, solving these problems is inherently challenging for their non-convexities, high computational demands, and the adversarial setup of the objectives. We circumvent this hard problem and work on the effect the other attack variable $A(x)$ brings about. There could be another text altering algorithm $R(x)$ to compensate the effect of $A(x)$, and the optimal $R^*$ equals:
\begin{equation}
\small
\begin{aligned}
\arg\min_{R}\max_{A(x), \Delta\theta}\quad &\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[ L\left(f\left(R\left(A(x)\right);\, \theta + \Delta\theta\right),\, y\right) \right] \\ 
\text{s.t.} \quad &\text{ Constraints on }  A(x) ,  \Delta\theta.
\end{aligned}
\end{equation}
Finding an optimized $R^*$ may also be very challenging. However, we can construct an $\hat{R}$ through heuristic methods such that $\hat{R}\approx R^*$, which meets the defense requirements.

\subsection{Reformulation of text}
We introduce a text reformulation-based defense methodology aimed at mitigating adversarial perturbations and backdoor triggers in textual data. A universal and robust encoder focuses on the core semantics of text while disregarding specific subtle features. When its encoding scheme is reconstructed through a decoder, it is highly likely to disrupt features that are unrelated to the core semantics. By doing so, we impair the specific patterns and features that malicious inputs rely upon to influence the model's behavior.

Our approach leverages three key text reformulation functions and an ensembled module to build a competitive $\hat{R}(x)$:

\textbf{Paraphrasing.} This technique involves re-expressing the input text using different words and syntactic structures while maintaining the original meaning. Paraphrasing can obscure lexical cues and syntactic patterns exploited by adversarial attacks, thereby reducing their effectiveness.

\textbf{Summarization.} By condensing the input text to its essential information, summarization can remove extraneous content where backdoor triggers may be embedded. This process not only shortens the text but also refines its focus, potentially eliminating hidden malicious patterns.

\textbf{Back-translation.} This method translates the input text into another language and then back to the original language. Double translation introduces variations in word choice and sentence construction due to linguistic differences, which can disrupt the consistency of adversarial patterns without significantly altering the text's meaning.

\textbf{Voting mechanism.} The effectiveness of reformulation-based defense methods varies with dataset distributions and attack strategies. Paraphrasing performs well on short texts with static triggers, while reverse translation struggles with longer texts containing dynamic syntactic triggers. In high-accuracy scenarios, an ensemble approach with a voting mechanism can enhance robustness. In cases of a tie (e.g., when multiple labels are assigned in multi-class settings), empirical performance can guide the selection of a dominant module. For example, our experiments show that the summarization model consistently outperforms others, making it the preferred choice in tie-breaking situations.

\subsection{Prompt engineering of LLMs}
High-quality text reformulation requires a deep understanding of core semantics while disregarding stylistic and infrequent lexical elements. This process involves encoding text into a high-dimensional semantic representation and decoding it into a reformulated version that preserves meaning.

We leverage SOTA LLMs, such as GPT-4o and LLaMA 3, for their advanced semantic understanding and generation capabilities. These models extract the fundamental meaning of text while filtering out superficial stylistic features. The encoded semantic representation is then used to generate text that maintains the original intent while exhibiting lexical and syntactic variations.

Effective prompt engineering is essential for optimizing LLM performance, as it guides model behavior and influences output quality. We designed five tailored prompts for the three types of text reformulation tasks, and choose 3 optimal ones by trail and error, details are in the appendix A.

\subsection{Surrogate model training for special scenarios}
To address data privacy concerns and the high computational cost of model inference, we leverage model extraction and knowledge distillation to develop a compact local model that maintains performance comparable to large-scale LLMs.

Large-scale LLMs produce autoregressive outputs in the form of hard labels from commercial models (e.g., GPT-4o) and soft labels from open-source models (e.g., LLaMA 3). The surrogate model can be trained using model extraction, knowledge distillation, or a combination of both, depending on the scenario.

\textbf{Model extraction with hard labels.} In model extraction, with only hard labels from the target model, the surrogate model learns to map input sentences to the reformulations produced by the target model without directly considering the model's probability distributions over the vocabulary. Cross-entropy loss is used to guide the learning process. And it can be defined as:
\begin{equation}
L_{\text{hard}} = -\sum_{i=1}^{N} \log P_s(y_t^i \mid y_t^{<i}, x),
\end{equation}
where $N$ is the length of the reformulated sentence $y_t$ and $y_t^i$ the $i$-th token in the target model's output. $y_t^{<i}$ represents the sequence \( (y_t^1, y_t^2, \dots, y_t^{i-1}) \), i.e., all tokens before position \( i \). And $P_s(y_t^i \mid y_t^{<i}, x)$ is the probability that the surrogate model assigns to token $y_t^i$ at position $i$.

\textbf{Knowledge Distillation with soft labels.} Under the scenarios when soft labels are available, knowledge distillation encourages the surrogate to mimic the target model's behavior more closely by matching the output probability distributions, capturing richer information about uncertainties and alternative predictions through KL divergence. There could be a temperature scaling \( T > 1 \) to soften the target model’s probability distribution:
\begin{equation}
P_t^{(T)}(y \mid y^{<i}, x) = \text{Softmax}\left(\frac{z_t}{T}\right),
\end{equation}

where $z_t$ denotes the logits from the target model, and $P_t^{(T)}$ the softened probability distribution of the target model.

Then Kullback-Leibler (KL) Divergence is used to assess the distance between the teacher's and student's softened distributions:
\begin{equation}
\small
L_{\text{soft}} = T^2 \sum_{i=1}^{N} \text{KL}\left(P_t^{(T)}(\cdot \mid y^{<i}, x) \, \| \, P_s^{(T)}(\cdot \mid y^{<i}, x)\right),
\end{equation}
where  $P_s^{(T)}$ represent he surrogate model's softened probability distribution, and  $T^2$ the scaling factor to adjust gradients due to temperature.

\textbf{Combined optimization objective.}
A unified optimization objective for training a surrogate model across various scenarios, where both model extraction and knowledge distillation are considered, can be stated as:
\begin{equation}
L_{\text{total}} = \alpha L_{\text{soft}} + (1 - \alpha) L_{\text{hard}},
\end{equation}
where $\alpha \in$ [0, 1] is a hyper-parameter that controls the trade-off between the model extraction  and knowledge distillation losses.
\section{Experiments}
\subsection{Expriment settings}
\textbf{Datasets.}
 Our experiments utilize two widely studied benchmark datasets to evaluate model performance and robustness. (1) SST-2 (Stanford Sentiment Treebank) consists of movie reviews labeled for binary sentiment classification (positive or negative), serving as a robust test bed for assessing sentiment classification models, particularly under adversarial conditions. (2) AG News comprises news articles categorized into four classes: World, Sports, Business, and Sci/Tech. Its diverse topic range makes it well-suited for evaluating text classification models beyond sentiment analysis, providing a broader perspective on model robustness.
 
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{ae_defense.png}
    \caption{Each subplot represents an adversarial perturbation method, with five defense strategies applied to both clean and perturbed samples. Taller blue bars indicate lower side effects on clean samples, while taller yellow bars signify better mitigation of perturbed samples.On the x-axis, methods 0--1 are baselines, while 2--5 are our approaches. \text{Reform\_T} represents voting results from the 2 to 4 modules.}
    \label{fig:enter-label}
\end{figure}
\textbf{Victim models.}
We trained two widely used NLP models as text classifiers using the aforementioned datasets. (1) BERT \cite{bert}, a pre-trained NLP model developed by Google, captures contextual information bidirectionally, making it highly effective across various language tasks. (2) RoBERTa \cite{roberta}, developed by Facebook, enhances BERT by optimizing the training process, leveraging more data, and removing certain constraints, leading to improved performance on multiple NLP benchmarks.

\textbf{Adversarial attacks.} The victim model was well-trained on a clean dataset, and we employed four representative adversarial attack algorithms to assess its robustness. (1) PWWS \cite{ae1_pwws} utilizes word saliency to replace critical words with synonyms, altering the classifier’s output while preserving the original meaning. (2) DeepWordBug \cite{ae2_deepwordbug} manipulates individual characters within words to mislead the model while keeping the text readable. (3) TextFooler \cite{ae4_textfooler} substitutes key words with synonyms to deceive the classifier while ensuring grammatical correctness and semantic coherence. (4) TextBugger \cite{ae3_textbugger} integrates both word- and character-level modifications to subtly mislead the model, generating adversarial examples that remain inconspicuous to human readers.

\textbf{Backdoor atatcks.}
We trained the victim models on datasets poisoned using four different backdoor attack techniques. (1) BadNets \cite{bd1_badnets} introduces specific rare words into training samples at random positions, with "cf" selected as the trigger word. (2) AddSent \cite{bd2_addsent} inserts a predefined short sentence into samples, following the original paper's setting with the sentence: "I watch this 3D movie." (3) StyleBkd \cite{bd4_style} employs GPT-2 to rewrite textual samples in a distinct style as the malicious feature, where we adopt the poetry style as the trigger. (4) SynBkd \cite{bd3_synbkd} also utilizes GPT-2 to paraphrase textual samples with a specific structural transformation, adhering to the settings proposed in the original paper. The first two schemes use well known static triggers, and the latter two represent dynamic triggers.
\begin{figure}
    \includegraphics[width=1\linewidth]{bd_defense.png}
\caption{Each subplot shows a backdoor attack mitigated by eight defenses, with method 0 as the no-defense baseline. Blue bars represent clean-sample accuracy (higher is better), and red bars show attack success on poisoned samples (lower is better). On the x-axis, methods 0--3 are baselines, while 4--7 are our approaches. \text{Reform\_T} represents voting results from the 4 to 6 modules.}

    \label{fig:enter-label}
\end{figure}

\textbf{Metrics.}
To assess the effectiveness of the defense methods against adversarial attacks, we primarily record the victim model's accuracy on poisoned samples, denoted as 
$ACC_a$, across various attack strategies, and then compare them to the accuracy $ACC_d$, which is the model accuracy after applying defense mechanisms. A higher $ACC_d$ relative to $ACC_a$ indicates a more effective defense. Additionally, we examine the potential adverse effects of these defenses on clean samples, quantified by measuring the extent to which they degrade the model's performance on unaltered inputs. The clean samples scenarios are listed in the "No\_Attack" column in Table 1.

In the context of defending against backdoor attacks, $ACC$ and $ASR$ are used to evaluate the effectiveness of an attack. $ACC$ represents the accuracy of the trojaned model on clean samples, while $ASR$ (Attack Success Rate) quantifies the percentage of poisoned samples that successfully induce the victim model to misclassify. When defenses are applied, we denote these metrics as 
$ACC_d$ and $ASR_d$. A higher $ACC_d$ and a lower $ASR_d$ indicate a more effective defense.

\subsection{Reformulation Modules}
\begin{table}
\centering
\caption{Defended ACC of RoBERTa on AG\_News clean samples(None column) and perturbed samples(4--6th columns). DWB:DeepWordBug, TB:TextBugger,TF:TextFooler. Reform\_T aggregates votes from three reconstruction modules based on GPT-4o, while Reform\_S distills these modules into GPT-2 student models for voting.}
\label{tab:defense_performance}
\setlength{\tabcolsep}{4pt} % 缩小列间距（默认约6pt，此处设为4pt）
\begin{tabular}{@{}lccccc@{}}
\toprule
\multirow{2}{*}{Defense} & \multicolumn{5}{c}{ACC under attack} \\
\cmidrule(l){2-6}
 & None & DWB & PWWS & TB & TF \\
\midrule
No\_defense   & 91.70 & 15.52 & 12.27 & 27.44 & 4.69 \\
ATINTER       & 90.97 & 26.35 & 24.91 & 41.88 & 24.19 \\
Reform\_T     & \textbf{92.06} & \textbf{89.53} & \textbf{81.95} & \textbf{88.81} & \textbf{79.06} \\
Reform\_S     & 88.81 & 85.92 & 76.53 & 80.14 & 74.01 \\
\bottomrule
\end{tabular}
\end{table}
Utilizing the state-of-the-art large-scale language model GPT-4o as the backbone, we employed prompt engineering to perform paraphrasing, summarizing, and back-translation on randomly selected samples from poisoned test datasets contaminated by the eight representative attacks mentioned above, as well as on randomly selected clean samples. This approach allowed us to evaluate the effectiveness of these methods in mitigating the impact of adversarial perturbations while preserving model accuracy on clean samples.
\begin{table*}[t]
\caption{ACC/ASR performance of the backdoored model under different defenses across four attack methods.}
\label{backdoor-defense-table}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
Defense & Badnets & Addsent & Stylebkd & Synbkd \\
\midrule
Without Defense & 90.97 / 100.00 & 91.34 / 100.00 & 88.81 / 78.70 & 86.64 / 87.36 \\
ONION           & 87.73 / 19.49  & 87.36 / 95.31  & 84.84 / 81.95 & 84.48 / 92.78 \\
RAP             & 86.64 / 92.43  & 67.87 / 44.04  & 50.9 / 45.85  & 88.09 / \textbf{30.69} \\
STRIP           & 87.73 / 94.58  & 88.81 / 96.39  & 90.97 / 79.78 & 87.73 / 85.2  \\
Reform\_T       & \textbf{92.42 / 12.27}  & \textbf{94.58 / 13.72}  & \textbf{92.42 / 23.47} & \textbf{88.45} / 43.68 \\
Reform\_S       & 89.89 / 12.64  & 90.61 / 16.61  & 89.53 / 35.38 & 86.28 / 48.01 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vspace{-4mm}
\end{table*}

\textbf{Adversarial attack defense.} We trained BERT and RoBERTa classification models using clean SST-2 and AG-News training datasets, followed by adversarial perturbations on test samples using four attack methods: DeepWordBug, PWWS, TextFooler, and TextBugger. 

Figure 3 illustrates the impact of various attack methods on RoBERTa for the AG\_News dataset (see Appendix B for more cases). These attacks significantly degraded the model's $ACC$, reducing it from over 90\% to below 20\% (represented by the 0-group bars). However, by reformulating the perturbed samples through paraphrasing, summarization, and translation, the model's $ACC$ recovered to over 75\% (represented by the 2–4 group bars). This substantial improvement highlights the effectiveness of our approach in mitigating adversarial attacks, outperforming the baseline method (represented by the 1-group bars).To achieve more stable performance, we integrate our three reformulation modules using a voting mechanism and denote the resulting approach as Reform\_T.


Additionally, our modules had minimal impact on clean samples, preserving their accuracy. Table 1 presents detailed results for the SST-2 dataset on BERT, demonstrating that our solution, Reform\_T, outperforms the baseline ATINTER. Moreover, a student model (GPT-2) extracting the Reform\_T modules achieves comparable performance (see Section Model Function Extraction). Notably, our reformulation module even enhances the model's accuracy from 91.7\% to 92.06\%. Similar improvements are observed in other cases (see Appendix)

\textbf{Backdoor defense.} We conducted backdoor poisoning on SST-2 and AG-News datasets with four attack methods: BadNet, AddSent, StyleBKD, and SyntacticBKD. These datasets were then used to train backdoored models with BERT and RoBERTa.
With our reformulating modules, we significantly reduced the attack success rates of backdoor samples while keeping negative impacts on clean sample accuracy low.

As illustrated in figure 4 for AG\_News on RoBERTa case(see more cases in Appendix C), the baseline Onion (the 1-group) was effective only against word-level perturbations like BadNet. And baseline RAP (the 2-group) showed success in mitigating both static short-trigger and dynamic-trigger attacks but caused noticeable degradation in clean sample classification. And STRIP fails in inference time mitigation. These results meet the measurement work OpenBackdoor \cite{cui_unified_2022}.n contrast, our reformulation modules (Modules 4–7) effectively reduced backdoor attack success rates, as evidenced by the sharp decline in the red bars. Notably, our approach demonstrates significantly better mitigation performance against the representative dynamic trigger schemes Synbkd and Stylebkd compared to the three baseline methods. However, there remains room for improvement in addressing Stylebkd. To the best of our knowledge, no existing solutions outperform ours in defending against this attack.

Table 2 presents the detailed ACC/ASR results for the SST-2 dataset on BERT case (see more cases in Appendix C). Notably, compared to the "Without Defense" setting (first row), our solution, Reform\_T, not only mitigates backdoor attacks but also improves the ACC of backdoored models across all four attack scenarios. This further alleviates concerns regarding potential side effects on clean samples.

\textbf{Model function extraction.}
To address concerns about resource constraints and data security, we propose Reform\_S, a solution that leverages a locally deployed open-source small model (GPT-2) to extract model functions as a substitute for the SOTA model. To extract the robust encoding and  reformulating capabilities of the cloud-based GPT-4o model, we collected 10,000 independent sentences from the internet, covering common sentence types such as declarative, interrogative, exclamatory, and imperative sentences. Using prompt engineering, we queried the cloud side to perform Paraphrasing, Summarizing, and Back-Translation, and leveraged the model outputs to construct three datasets.

These datasets were used to train three local surrogate reformulation models based on GPT-2. While the individual models underperformed compared to the teacher model in most tasks, their ensemble, combined with a voting mechanism, achieved performance close to that of the teacher model's integrated output, as shown by the Reform\_S results in table 1 and table 2.

\section{Discussion}
Existing online defense methods are typically attack-specific and struggle to handle a variety of different attack types. For instance, the common ONION method performs well against word-level perturbations but is almost ineffective against sentence-level or dynamic trigger attacks. Additionally, while these methods aim to eliminate malicious features, they often come at the cost of negatively impacting normal samples, thus weakening the model's overall accuracy. For example, the RAP defense method reduces the $ASR$ but also diminishes the model’s $ACC$.

In contrast, our proposed reconstruction framework effectively removes potential malicious features from the sample while preserving its core semantics, ensuring that the reconstructed text still receives the correct classification by the model. Although the three modules—paraphrasing, summarizing, and back-translating—can meet conventional defense needs, there is no fixed optimal solution across different sample distributions and attack methods. In such cases, integrating the three modules and using a voting mechanism for final decision-making can lead to better overall results. Moreover, this multi-module ensemble voting approach is more suitable for local surrogate student model to achieve the performance close to that of cloud-based state-of-the-art models.


\section{Conclusion}
Natural language contains latent features that attackers exploit to launch adversarial and backdoor attacks on NLP models. Existing defenses are often attack-specific, making them vulnerable to adaptive strategies, and lack a unified framework to address both threats. In this paper, we propose a unified online defense framework that mitigates both adversarial and backdoor attacks while handling diverse attack strategies, including adversarial perturbations across different granularities, static (fixed-word triggers) and dynamic (syntactic or stylistic) backdoors.

Our defense framework comprises three core modules: paraphrasing, summarization, and back-translation. They impair potential malicious features while preserving text integrity. A voting mechanism further optimizes performance, and experiments show that our reformulation-based approach not only neutralizes attacks but also enhances model accuracy on clean samples.

To improve efficiency, reduce costs, and ensure data security, we further implement local surrogate models via model extraction. Using GPT-2 as a student model to distill the encoding and reformulating capabilities from GPT-4o, our approach achieves comparable defense performance with significantly lower computational overhead under the voting mechanism.
\section{Impact statement}
This paper advances the field of Machine Learning by introducing a unified defense framework for NLP models against adversarial and backdoor attacks. Our approach enhances model robustness while maintaining efficiency and data privacy, contributing to the secure deployment of AI systems. While our work primarily focuses on improving security in NLP applications, it may also have broader implications in mitigating risks associated with AI-driven misinformation and adversarial manipulation. We acknowledge the ethical considerations of adversarial defense research, particularly in ensuring that defensive techniques are not misused for censorship or unjust content moderation. However, our framework is designed to promote fairness and model integrity, ultimately supporting the safe and responsible application of AI in real-world scenarios.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{mybibliography}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Prompt Engineering}
Prompt engineering is crucial because it directly influences the output of GPT models, ensuring responses are relevant, accurate, and aligned with user intent. Well-crafted prompts optimize model performance, improve efficiency, and reduce biases or undesired behaviors. GPT models use probabilistic approaches, introduce randomness through sampling methods (e.g., temperature, top-k, nucleus sampling). This means slight variations in prompts can yield different responses due to the model's non-deterministic nature.

To optimize the performance of our reconstruction modules, we designed multiple prompts and conducted small-scale experiments to empirically identify the most effective ones. The effectiveness of these prompts was assessed based on two key criteria: their impact on clean samples and their ability to mitigate adversarial perturbations.

For each reconstruction module, we generated five candidate prompts and ultimately selected the three most effective ones through trial and error, as detailed below:
\begin{description}
    \item[Prompt for paraphrasing module]:   \par
    The following {len(sentences)} sentences are strictly separated by \texttt{'>>>'}, with no other delimiters, symbols, or punctuation serving this function. Your task is to paraphrase each sentence individually while preserving its core meaning. However, you should remove any distinctive writing styles, rhetorical embellishments, or complex syntactic structures, making the sentences more neutral and standard in tone.
    
    Ensure that each paraphrased sentence remains clear, precise, and semantically equivalent to the original.Do not add or omit any information.Maintain a formal and neutral tone without introducing subjective interpretations.Do not include any index numbers or additional formatting.Present your paraphrased sentences in the same order as the input, strictly separating them with \texttt{'>>>'} as the delimiter.
    \item[Prompt for summarizing module]:   \par
    The following {len(sentences)} sentences are strictly separated by \texttt{'>>>'}, with no other delimiters, symbols, or punctuation serving this function. Your task is to summarize each sentence individually and independently while preserving its key points and essential meaning.
    
    Ensure that each summary captures the main idea and critical details while eliminating redundant or non-essential information.Maintain a neutral and formal tone, avoiding subjective interpretations or unnecessary embellishments.Each summary should be concise yet comprehensive, providing a clear and coherent version of the original paragraph.Do not include any index numbers or additional formatting.Present your summarized paragraphs in the same order as the input, strictly separating them with \texttt{'>>>'} as the delimiter.
    \item[Prompt for back-translating module]:   \par
    The following {len(sentences)} sentences are strictly separated by \texttt{>>>}, with no other delimiters, symbols, or punctuation serving this function. Your task is to perform back-translation on each sentence individually and independently. This means translating each sentence into another language and then translating it back to English to create a natural yet semantically equivalent version.
    
    Ensure that each back-translated sentence preserves the original meaning while allowing for minor natural variations in phrasing.Do not introduce any additional information or omit key details. Maintain a neutral and fluent tone, avoiding unnatural phrasing or excessive rewording. Do not include any index numbers or additional formatting. Present your back-translated sentences in the same order as the input, strictly separating them with '>>>' as the delimiter.

\end{description}



\section{Defense on adversarial attacks}
We trained SST-2 and AG\_News tasks separately on BERT and RoBERTa models, resulting in a total of four models. These models were then attacked using four representative adversarial text attack methods: DeepWordBug, PWWS, TextBugger, and TextFooler. Subsequently, the models were defended using the baseline method ATINTER as well as four defense modules from our proposed approach. This resulted in a total of 4×4×5=80 experimental settings, which are compactly visualized in the figure below.

In each subplot corresponding to an attack method, the x-axis represents different defense strategies: 0 indicates no defense, 1 represents ATINTER, 2 corresponds to the paraphrasing module, 3 to the summarizing module, 4 to the back-translation module, and 5 denotes an ensemble approach that integrates the first three modules using a voting mechanism. The blue bars indicate the model's accuracy (ACC) on clean samples, which evaluates the potential side effects of the defense strategies on non-adversarial inputs. The yellow bars represent the model's accuracy on perturbed samples after applying the defense mechanisms. A higher yellow bar, compared to group 0, signifies a stronger mitigation effect against adversarial attacks.The numerical labels on the x\-axis in the figure correspond to different attack mitigation methods. 0 No Defense, 1 ATINTER, 2  Paraphrasing, 3 Summarizing,  4 Back\-Translation, 5 Reform\_T.


\begin{figure}[h]
    \centering
    \subfigure[Adversarial attacks against BERT on SST2 ]{
        \includegraphics[width=0.45\linewidth]{AE_BERT_SST2.png}
    }
    \subfigure[Adversarial attacks against BERT on AG\_News]{
        \includegraphics[width=0.45\linewidth]{AE_BERT_AG_NEWs.png}
    }
    \subfigure[Adversarial attacks against RoBERTa on SST2]{
        \includegraphics[width=0.45\linewidth]{AE_RoBERTa_SST2.png}
    }
    \subfigure[Adversarial attacks against RoBERTa on AG\_News]{
        \includegraphics[width=0.45\linewidth]{AE_RoBERTa_AG_News.png}
    }
    \caption{Defense performance across various adversarial attacks.The numerical labels on the x\-axis in the figure correspond to different attack mitigation methods. 0 No Defense, 1 ATINTER, 2  Paraphrasing, 3 Summarizing,  4 Back\-Translation, 5 Reform\_T}
    \label{fig:2x2images}
\end{figure}

As illustrated in the figure, our three reconstruction methods effectively mitigate adversarial perturbations across various datasets and models, showing substantial improvements over the baseline ATINTER. Among these methods, the Summarizing module performs best in mitigating perturbations, although its impact on clean samples is not always optimal. The voting mechanism further enhances data quality, providing greater stability to the overall defense framework. Notably, the voting mechanism can even boost the model’s accuracy on originally clean samples. Detailed results are presented in the table below:

\begin{table}[]
\caption{4 adversarial attacks and defense towards 2 models.$ACC_a$ means ACC under attack. $ACC_{ad}$ means the defended ACC under attack. The smaller the $\Delta$ACC, and the larger the $\Delta ACC_{d}$, the better. Our framework is Reform\_T and Reform\_S, the latter is the local surrogate defense model.}
\tabcolsep=0.15cm
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|c|c|c|cc|lllll|}
\hline
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Model} & \multirow{2}{*}{Attack} & \multicolumn{2}{c|}{W/O Defense} & \multicolumn{5}{c|}{W/ Defense} \\ \cline{4-10} 
 &  &  & \multicolumn{1}{l|}{ACC} & \multicolumn{1}{l|}{$ACC_{a}$} & \multicolumn{1}{l|}{Defense} & \multicolumn{1}{l|}{$ACC_d$} & \multicolumn{1}{l|}{$\Delta$ACC} & \multicolumn{1}{l|}{$ACC_{ad}$} & $\Delta ACC_d$ \\ \hline
\multirow{15}{*}{SST2} & \multirow{15}{*}{BERT} & \multirow{3}{*}{Clean} & \multicolumn{1}{c|}{\multirow{15}{*}{91.7}} & \multirow{3}{*}{91.7} & \multicolumn{1}{l|}{ATINTER} & \multicolumn{1}{l|}{90.97} & \multicolumn{1}{l|}{-0.73} & \multicolumn{1}{l|}{90.97} & -0.73 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_T} & \multicolumn{1}{l|}{92.06} & \multicolumn{1}{l|}{0.36} & \multicolumn{1}{l|}{92.06} & 0.36 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_S} & \multicolumn{1}{l|}{88.81} & \multicolumn{1}{l|}{-2.89} & \multicolumn{1}{l|}{88.81} & -2.89 \\ \cline{3-3} \cline{5-10} 
 &  & \multirow{3}{*}{DeepWordBug} & \multicolumn{1}{c|}{} & \multirow{3}{*}{15.52} & \multicolumn{1}{l|}{ATINTER} & \multicolumn{1}{l|}{90.97} & \multicolumn{1}{l|}{-0.73} & \multicolumn{1}{l|}{26.35} & 10.83 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_T} & \multicolumn{1}{l|}{92.06} & \multicolumn{1}{l|}{0.36} & \multicolumn{1}{l|}{89.53} & 89.53 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_S} & \multicolumn{1}{l|}{88.81} & \multicolumn{1}{l|}{-2.89} & \multicolumn{1}{l|}{85.92} & 85.92 \\ \cline{3-3} \cline{5-10} 
 &  & \multirow{3}{*}{PWWS} & \multicolumn{1}{c|}{} & \multirow{3}{*}{12.27} & \multicolumn{1}{l|}{ATINTER} & \multicolumn{1}{l|}{90.97} & \multicolumn{1}{l|}{-0.73} & \multicolumn{1}{l|}{24.91} & 12.64 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_T} & \multicolumn{1}{l|}{92.06} & \multicolumn{1}{l|}{0.36} & \multicolumn{1}{l|}{81.95} & 69.68 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_S} & \multicolumn{1}{l|}{88.81} & \multicolumn{1}{l|}{-2.89} & \multicolumn{1}{l|}{76.53} & 64.26 \\ \cline{3-3} \cline{5-10} 
 &  & \multirow{3}{*}{TextBugger} & \multicolumn{1}{c|}{} & \multirow{3}{*}{27.44} & \multicolumn{1}{l|}{ATINTER} & \multicolumn{1}{l|}{90.97} & \multicolumn{1}{l|}{-0.73} & \multicolumn{1}{l|}{41.88} & 14.44 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_T} & \multicolumn{1}{l|}{92.06} & \multicolumn{1}{l|}{0.36} & \multicolumn{1}{l|}{88.81} & 61.37 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_S} & \multicolumn{1}{l|}{88.81} & \multicolumn{1}{l|}{-2.89} & \multicolumn{1}{l|}{80.14} & 52.7 \\ \cline{3-3} \cline{5-10} 
 &  & \multirow{3}{*}{TextFooler} & \multicolumn{1}{c|}{} & \multirow{3}{*}{4.69} & \multicolumn{1}{l|}{ATINTER} & \multicolumn{1}{l|}{90.97} & \multicolumn{1}{l|}{-0.73} & \multicolumn{1}{l|}{24.19} & 19.5 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_T} & \multicolumn{1}{l|}{92.06} & \multicolumn{1}{l|}{0.36} & \multicolumn{1}{l|}{79.06} & 74.37 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_S} & \multicolumn{1}{l|}{88.81} & \multicolumn{1}{l|}{-2.89} & \multicolumn{1}{l|}{74.01} & 69.32 \\ \hline
\multirow{15}{*}{AG\_News} & \multirow{15}{*}{RoBERTa} & \multirow{3}{*}{Clean} & \multicolumn{1}{c|}{\multirow{15}{*}{94.58}} & \multirow{3}{*}{94.58} & \multicolumn{1}{l|}{ATINTER} & \multicolumn{1}{l|}{93.86} & \multicolumn{1}{l|}{-0.72} & \multicolumn{1}{l|}{93.86} & -0.72 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_T} & \multicolumn{1}{l|}{92.42} & \multicolumn{1}{l|}{-2.16} & \multicolumn{1}{l|}{92.42} & -2.16 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_S} & \multicolumn{1}{l|}{88.81} & \multicolumn{1}{l|}{-5.77} & \multicolumn{1}{l|}{88.81} & -5.77 \\ \cline{3-3} \cline{5-10} 
 &  & \multirow{3}{*}{DeepWordBug} & \multicolumn{1}{c|}{} & \multirow{3}{*}{32.85} & \multicolumn{1}{l|}{ATINTER} & \multicolumn{1}{l|}{93.86} & \multicolumn{1}{l|}{61.01} & \multicolumn{1}{l|}{43.68} & 10.83 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_T} & \multicolumn{1}{l|}{92.42} & \multicolumn{1}{l|}{59.57} & \multicolumn{1}{l|}{87.36} & 54.51 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_S} & \multicolumn{1}{l|}{88.81} & \multicolumn{1}{l|}{55.96} & \multicolumn{1}{l|}{78.34} & 45.49 \\ \cline{3-3} \cline{5-10} 
 &  & \multirow{3}{*}{PWWS} & \multicolumn{1}{c|}{} & \multirow{3}{*}{41.52} & \multicolumn{1}{l|}{ATINTER} & \multicolumn{1}{l|}{93.86} & \multicolumn{1}{l|}{52.34} & \multicolumn{1}{l|}{63.54} & 22.02 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_T} & \multicolumn{1}{l|}{92.42} & \multicolumn{1}{l|}{50.9} & \multicolumn{1}{l|}{90.25} & 48.73 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_S} & \multicolumn{1}{l|}{88.81} & \multicolumn{1}{l|}{47.29} & \multicolumn{1}{l|}{86.28} & 44.76 \\ \cline{3-3} \cline{5-10} 
 &  & \multirow{3}{*}{TextBugger} & \multicolumn{1}{c|}{} & \multirow{3}{*}{42.96} & \multicolumn{1}{l|}{ATINTER} & \multicolumn{1}{l|}{93.86} & \multicolumn{1}{l|}{50.9} & \multicolumn{1}{l|}{64.98} & 22.02 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_T} & \multicolumn{1}{l|}{92.42} & \multicolumn{1}{l|}{49.46} & \multicolumn{1}{l|}{88.09} & 45.13 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_S} & \multicolumn{1}{l|}{88.81} & \multicolumn{1}{l|}{45.85} & \multicolumn{1}{l|}{84.12} & 41.16 \\ \cline{3-3} \cline{5-10} 
 &  & \multirow{3}{*}{TextFooler} & \multicolumn{1}{c|}{} & \multirow{3}{*}{17.33} & \multicolumn{1}{l|}{ATINTER} & \multicolumn{1}{l|}{93.86} & \multicolumn{1}{l|}{76.53} & \multicolumn{1}{l|}{74.37} & 57.04 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_T} & \multicolumn{1}{l|}{92.42} & \multicolumn{1}{l|}{75.09} & \multicolumn{1}{l|}{86.28} & 68.95 \\ \cline{6-10} 
 &  &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{l|}{Reform\_S} & \multicolumn{1}{l|}{88.81} & \multicolumn{1}{l|}{71.48} & \multicolumn{1}{l|}{81.95} & 64.62 \\ \hline
\end{tabular}
\end{table}

\begin{figure}[h]
    \centering
 
    \subfigure[Backdoor attacks against RoBERTa on AG\_News]{
        \includegraphics[width=0.45\linewidth]{BD_RoBERTa_AG_News.png}
    }
    \subfigure[Backdoor attacks against BERT on AG\_News]{
        \includegraphics[width=0.45\linewidth]{BD_BERT_AG_News.png}
    }
    \subfigure[Backdoor attacks against RoBERTa on SST2]{
        \includegraphics[width=0.45\linewidth]{BD_RoBERTa_SST2.png}
    }
    \subfigure[Backdoor attacks against BERT on SST2 ]{
        \includegraphics[width=0.45\linewidth]{BD_BERT_SST2.png}
    }

    \caption{Defense performance across various backdoor attacks.The numerical labels on the x\-axis in the figure correspond to different attack mitigation methods. 0 No Defense, 1 ONION, 2  RAP, 3 STRIP,  4 Paraphrasing, 5 Summarizing, 6 Back-translating 7 Reform\_T}
    \label{fig:2x2images}
\end{figure}

\section{Backdoor defense}
We selected BERT and RoBERTa as target victim models and performed poisoning attacks on both SST-2 and BERT using four backdoor attack methods: Badnet, AddSent, StyleBKD, and SyntacBKD. This resulted in the creation of 16 distinct backdoor models (2 * 4 * 2 = 16). We then applied 7 defense methods, leading to a total of 112 experimental results (16 * 7 = 112). The consistent findings underscore the effectiveness of our defense strategies, which successfully eliminate most covert features unrelated to core semantics, significantly reducing the attack success rates of backdoor samples. Moreover, our defense methods introduce minimal side effects on clean samples. Notably, the Reform\_T method, utilizing a voting ensemble mechanism, even improved the accuracy of backdoor models on clean samples in certain instances.


\begin{table*}[]
\tabcolsep=0.1cm
\renewcommand{\arraystretch}{1.1}
\caption{Representative online defenses against 4 backdoor attacks towards BERT on SST2 and RoBERTa on Ag\_news.}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Model} & \multirow{2}{*}{Attack} & \multirow{2}{*}{ACC} & \multirow{2}{*}{ASR} & \multirow{2}{*}{Defense} & \multirow{2}{*}{CACC} & \multirow{2}{*}{ASR} & \multirow{2}{*}{$\Delta$ACC} & \multirow{2}{*}{$\Delta$ASR} \\
 &  &  &  &  &  &  &  &  &  \\ \hline
\multirow{20}{*}{SST2} & \multirow{20}{*}{BERT} & \multirow{5}{*}{Badnets} & \multirow{5}{*}{90.97} & \multirow{5}{*}{100} & ONION & 87.73 & 19.49 & $\downarrow$  3.24 & $\downarrow$  80.51 \\ \cline{6-10} 
 &  &  &  &  & RAP & 86.64 & 92.42 & $\downarrow$  4.33 & $\downarrow$  7.58 \\ \cline{6-10} 
 &  &  &  &  & STRIP & 87.73 & 94.58 & $\downarrow$  3.24 & $\downarrow$  5.42 \\ \cline{6-10} 
 &  &  &  &  & Reform\_T & 92.42 & 12.27 & $\uparrow$ 1.45 & $\downarrow$  87.73 \\ \cline{6-10} 
 &  &  &  &  & Reform\_S & 89.89 & 12.64 & $\downarrow$  1.08 & $\downarrow$  87.36 \\ \cline{3-10} 
 &  & \multirow{5}{*}{Addsent} & \multirow{5}{*}{91.34} & \multirow{5}{*}{100} & ONION & 87.36 & 95.31 & $\downarrow$  3.98 & $\downarrow$  4.69 \\ \cline{6-10} 
 &  &  &  &  & RAP & 67.87 & 44.04 & $\downarrow$  23.47 & $\downarrow$  55.96 \\ \cline{6-10} 
 &  &  &  &  & STRIP & 88.81 & 96.39 & $\downarrow$  2.53 & $\downarrow$  3.61 \\ \cline{6-10} 
 &  &  &  &  & Reform\_T & 94.58 & 13.72 & $\uparrow$3.24 & $\downarrow$  86.28 \\ \cline{6-10} 
 &  &  &  &  & Reform\_S & 90.61 & 16.61 & $\downarrow$  0.73 & $\downarrow$  83.39 \\ \cline{3-10} 
 &  & \multirow{5}{*}{Stylekbd} & \multirow{5}{*}{88.81} & \multirow{5}{*}{78.7} & ONION & 84.84 & 81.95 & $\downarrow$  3.97 & 3.25 \\ \cline{6-10} 
 &  &  &  &  & RAP & 50.9 & 45.85 & $\downarrow$  37.91 & $\downarrow$  32.85 \\ \cline{6-10} 
 &  &  &  &  & STRIP & 90.97 & 79.78 & 2.16 & 1.08 \\ \cline{6-10} 
 &  &  &  &  & Reform\_T & 92.42 & 23.47 & 3.61 & $\downarrow$  55.23 \\ \cline{6-10} 
 &  &  &  &  & Reform\_S & 89.53 & 35.38 & 0.72 & $\downarrow$  43.32 \\ \cline{3-10} 
 &  & \multirow{5}{*}{Synbkd} & \multirow{5}{*}{86.64} & \multirow{5}{*}{87.36} & ONION & 84.48 & 92.78 & $\downarrow$  2.16 & 5.42 \\ \cline{6-10} 
 &  &  &  &  & RAP & 88.09 & 30.69 & 1.45 & $\downarrow$  56.67 \\ \cline{6-10} 
 &  &  &  &  & STRIP & 87.73 & 85.2 & 1.09 & $\downarrow$  2.16 \\ \cline{6-10} 
 &  &  &  &  & Reform\_T & 88.45 & 43.68 & 1.81 & $\downarrow$  43.68 \\ \cline{6-10} 
 &  &  &  &  & Reform\_S & 86.28 & 48.01 & $\downarrow$  0.36 & $\downarrow$  39.35 \\ \hline
\multirow{20}{*}{AG\_News} & \multirow{20}{*}{RoBERTa} & \multirow{5}{*}{Badnets} & \multirow{5}{*}{92.78} & \multirow{5}{*}{98.92} & ONION & 90.25 & 12.27 & $\downarrow$  2.53 & $\downarrow$  86.65 \\ \cline{6-10} 
 &  &  &  &  & RAP & 26.35 & 34.66 & $\downarrow$  66.43 & $\downarrow$  64.26 \\ \cline{6-10} 
 &  &  &  &  & STRIP & 88.81 & 94.22 & $\downarrow$  3.97 & $\downarrow$  4.7 \\ \cline{6-10} 
 &  &  &  &  & Reform\_T & 87.73 & 9.39 & $\downarrow$  5.05 & $\downarrow$  89.53 \\ \cline{6-10} 
 &  &  &  &  & Reform\_S & 87.36 & 11.19 & $\downarrow$  5.42 & $\downarrow$  87.73 \\ \cline{3-10} 
 &  & \multirow{5}{*}{Addsent} & \multirow{5}{*}{93.14} & \multirow{5}{*}{100} & ONION & 89.89 & 72.2 & $\downarrow$  3.25 & $\downarrow$  27.8 \\ \cline{6-10} 
 &  &  &  &  & RAP & 25.99 & 10.11 & $\downarrow$  67.15 & $\downarrow$  89.89 \\ \cline{6-10} 
 &  &  &  &  & STRIP & 90.25 & 97.47 & $\downarrow$  2.89 & $\downarrow$  2.53 \\ \cline{6-10} 
 &  &  &  &  & Reform\_T & 88.81 & 2.17 & $\downarrow$  4.33 & $\downarrow$  97.83 \\ \cline{6-10} 
 &  &  &  &  & Reform\_S & 84.48 & 11.55 & $\downarrow$  8.66 & $\downarrow$  88.45 \\ \cline{3-10} 
 &  & \multirow{5}{*}{Stylekbd} & \multirow{5}{*}{90.97} & \multirow{5}{*}{92.06} & ONION & 88.09 & 82.31 & $\downarrow$  2.88 & $\downarrow$  9.75 \\ \cline{6-10} 
 &  &  &  &  & RAP & 17.33 & 26.35 & $\downarrow$  73.64 & $\downarrow$  65.71 \\ \cline{6-10} 
 &  &  &  &  & STRIP & 88.09 & 75.45 & $\downarrow$  2.88 & $\downarrow$  16.61 \\ \cline{6-10} 
 &  &  &  &  & Reform\_T & 85.56 & 50.18 & $\downarrow$  5.41 & $\downarrow$  41.88 \\ \cline{6-10} 
 &  &  &  &  & Reform\_S & 83.03 & 64.62 & $\downarrow$  7.94 & $\downarrow$  27.44 \\ \cline{3-10} 
 &  & \multirow{5}{*}{Synbkd} & \multirow{5}{*}{93.14} & \multirow{5}{*}{100} & ONION & 87.73 & 96.75 & $\downarrow$  5.41 & $\downarrow$  3.25 \\ \cline{6-10} 
 &  &  &  &  & RAP & 16.25 & 25.27 & $\downarrow$  76.89 & $\downarrow$  74.73 \\ \cline{6-10} 
 &  &  &  &  & STRIP & 87.73 & 93.14 & $\downarrow$  5.41 & $\downarrow$  6.86 \\ \cline{6-10} 
 &  &  &  &  & Reform\_T & 88.81 & 3.25 & $\downarrow$  4.33 & $\downarrow$  96.75 \\ \cline{6-10} 
 &  &  &  &  & Reform\_S & 86.28 & 7.94 & $\downarrow$  6.86 & $\downarrow$  92.06 \\ \hline
\end{tabular}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.


\