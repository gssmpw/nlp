\section{Evaluation}
\label{sec:evaluation}

\subsection{Setup and Implementation}
\label{exp:setup}

\mypara{Testbed.} 
Our system uses a server equipped with Intel(R) Xeon(R) CPU E5-2678 v3 2.50GHz with 24 physical CPU cores as the local server running Ubuntu 18.04, where \namex synthesizes hints tables.
Meanwhile, we use another server equipped with Intel(R) Xeon(R) Platinum 8269CY CPU 2.50GHz with 52 physical CPU cores, running Ubuntu 18.04, as a serverless platform.
In this platform, we implement \namex into an open-source framework as Fission~\cite{fission} (V1.16) for serverless functions on Kubernetes.
We use Fission PoolManager~\cite{fission-executor} to spin up function pods, due to its excellent performance against cold starts.

\mypara{Implementation.} \namex has a frontend side and a (remote) backend side.
To facilitate seamless coordination, we implement the profiler and synthesizer as two distinct functions on the frontend side, while deploying the adapter as a service on the backend side.
Moreover, the adapter can be equipped with automatic horizontal scaling for enhancing Janus's scalability.
The frontend interacts with the developer and depends on their domain knowledge (detailed below) to synthesize hints tables.
We leverage packages \texttt{pandas.DataFrame} to represent hints tables. 
As for the backend side, we develop a lightweight server using Python Flask~\cite{python-flask}, Redis~\cite{redis}, Fission APIs~\cite{fission-cli}, and Fission HTTP trigger~\cite{fission-http-trigger}. 
The server spawns a process to trace each request's execution.
Upon completion of any function in the workflow, this process will re-evaluate the time budget for the remaining functions while accessing hints tables to decide on proper resource adaptation.
%\jing{When a hints table miss occurs, the adapter will specify the maximum available CPU cores, i.e., 3000 millicores, for the target functions to prevent SLO violations.
%Additionally, the adapter records these misses and evaluates them against a predefined threshold to decide whether to notify the developer.
%The developer are responsible for updating and resubmitting hints tables by re-triggering the profiler and synthesizer.}

\mypara{Workflows.} We evaluate the effectiveness of \namex with two real-world serverless workflows namely Intelligent Assistant (IA) and Video Analyze (VA).
Specifically, IA is a chain constituted by three functions: \textit{object detection} (OD, for short)~\cite{object-detect}, \textit{question answer} (QA)~\cite{question-answer}, and \textit{text-to-speech} (TS)~\cite{text-to-speech}, which analyzes images randomly sampled from COCO2014~\cite{coco-dataset} and answers questions sampled from SQuAD2.0~\cite{qa-dataset}; finally, the answers return in the form as audios.
VA as another workflow chain includes three functions as \textit{frame extraction} (FE)~\cite{extract-frame}, \textit{image classification} (ICL)~\cite{image-classification}, and \textit{image compression} (ICO)~\cite{image-compression}.
Its inputs are YouTube videos with identical duration and resolution, sourced from ORION~\cite{osdi22-orion}.
Additionally, the four functions in \S\ref{sec:bg:worst-case} as \textit{CPU-}, \textit{Memory-}, \textit{Network-}, and \textit{IO-intensive} conduct AES encryption~\cite{isca22-lukewarm}, data read (from a Redis based in-memory database)~\cite{middleware21-sebs}, socket communication~\cite{middleware21-sebs}, and data write (to local disks)~\cite{atc18-peek-bench}, respectively.

\mypara{Runtime dynamics.}
\jing{Our testing workloads contain runtime dynamics, encompassing varying working sets and performance interference.}
\jing{Specifically, the input data for IA, i.e., images and texts from COCO2014 and SQuAD2.0 respectively, are with varying working sets. 
The empirical study shows that the number of objects per image in COCO2014 ranges from 1 to 15~\cite{instance-per-image-coco2014}, while the number of words per text in SQuAD2.0 ranges from 35 to 641. As shown in Figure~\ref{fig:bg:ml-func-latency}, these varying sets result in significant variance in function execution time.
On the other hand, VA extracts frames from videos, followed by image classification and compression. To accelerate processing, VA implements parallelism for each function, incurring cross-function performance interference inevitably. The profiles reveal that, for the three functions in VA, the average ratio of execution time between P99 and P50 is 1.46 times, 1.56 times, and 1.37 times, respectively.}

\mypara{Domain knowledge.} 
We collect the execution time of IA's and VA's functions with respect to CPU cores, ranging from 1000 millicores to 3000 millicores with a step of 100 millicores.
After data collection, \namex adopts diverse percentiles, ranging from 1\% to 99\% with a step as 5\%, to profile execution time distribution.
To assess \namex's performance over higher loads, we additionally profile IA's execution performance over higher concurrency (i.e., batch size) as two and three.
\jing{As for VA, we only profile its performance with concurrency as one because FE and ICO cannot process frames in batch form.}
Here, we exclude memory as a knob.
This is because \namex focuses on latency-critical workflows.
Our empirical tests show that memory has no impact on execution time.

\begin{figure*}[!t]
    \centering
\includegraphics[width=0.93\textwidth]{./figure/exp/macro/overall-con1-3-e2e.pdf}
    %\vspace{-0.1cm}
    \caption{
    End-to-end latency distribution of IA under the concurrency (i.e., batch size) as one, two and three respectively, with different SLOs (red dashed line). The concurrency of VA is limited to one due to its \jing{non-batchable} functions (i.e., FE and ICO). 
    \label{fig:exp:macro:e2e}}
    %\vspace{-0.4cm}
\end{figure*}

\mypara{Baselines.}
\namex proposes bilaterally engaged resource adaptation to provide efficient serverless workflows serving, aiming to maximize resource efficiency, i.e., minimize resource consumption, with SLO guarantees.
\jing{Here, we use three early-binding approaches and three late-binding approaches as our baselines.
The early-binding approaches include the state-of-the-art serverless workflow serving system ORION~\cite{osdi22-orion}, GrandSLAM~\cite{eurosys19-grandslam} and its enhanced version GrandSLAM$+$. 
Specifically, GrandSLAM$+$ improves GrandSLAM by removing the
latterâ€™s constraints in identical sizes for all functions.
The late-binding approaches include Janus$-$, Janus$+$, and Optimal.
Optimal represents the best that can be achieved in any late-binding solution.}

\jing{The differences between Janus, Janus-, and Janus$+$ are as follows: Janus allows exploring diverse percentiles for the head (first) function in workflows. 
Janus$-$ disables this exploration and adopts a fixed percentile, P99. 
Janus$+$ extends the exploration to both the head function and the next-to-head function. 
In summary, compared to Janus, Janus$-$ has compromised resource efficiency due to its smaller optimization space. 
Janus$+$ can have higher resource efficiency (e.g., 0.6\% higher than Janus for IA) owing to a larger optimization space but at the expense of considerable time cost (by up to 107.2 times) in synthesizing hints (\S\ref{exp:micro:vary-perc}).}

\jing{Notably, existing late-binding approaches including Fifer, Kraken, Xanadu, and Cypress~\cite{socc22-cypress} mostly overlook the information barrier between the developer and provider, raising practicality concerns.
Additionally, as highlighted by Cypress, Fifer, Kraken, and Xanadu assume that function execution time does not have large variance, and hence they adopt mean execution time to perform runtime resource adaptation. 
However, this assumption contradicts our empirical observations from serverless production traces, which exhibit significant variance in execution time (\S\ref{sec:background}). 
Consequently, these approaches are easily prone to under provisioning and severe SLO violations. 
Thus, we exclude them as the baselines.}

% \namex proposes bilaterally engaged resource adaptation to provide efficient serverless workflows serving, aiming to maximize resource efficiency, i.e., minimize resource consumption, with SLO guarantees.
% Here, we use the state-of-the-art serverless workflow serving system ORION~\cite{osdi22-orion}, GrandSLAM~\cite{eurosys19-grandslam} and its enhanced version GrandSLAM$+$, Janus$-$, and Optimal as our baselines.
% Specifically,  GrandSLAM$+$ improves GrandSLAM by removing the latter's constraints in identical sizes for all functions.
% Janus$-$ disables head functions' exploration to diverse percentiles, and adopts a fixed percentile as P99.  
% Optimal is established through exhaustive search. 
% Specifically, we generate samples (i.e., requests) for the two workflows, respectively. For each sample we record the exact execution time of individual functions with respect to CPU cores. Moreover, the execution time is randomly extracted from the original profiling data. For each sample, Optimal searches over all possible CPU core combinations for different functions, thus aiming to minimize overall resource consumption while ensuring SLOs.

%Note that except \namex, Janus$-$ and Optimal, other systems adopt early-binding.
%Overall, we compare these systems' performance over two metrics as total resource consumption and end-to-end latency.


\mypara{Setup.}
Considering our testbed's capacity and the short-lived nature of functions~\cite{atc20-serverless-in-the-wild,socc23-huawei-cloud}, we set SLOs for IA and VA as 3s and 1.5s, respectively.
We set the weight for each function as one unless otherwise specified.
We explore \namex's performance under varying SLOs and weights in \S\ref{exp:micro:slo} and \S\ref{exp:micro:weight}, respectively.
When a hints table miss occurs, we scale functions up to 3000 millicores to prevent SLO violations.
The miss rate threshold is set as 1\% by default.
To ensure experimental results' statistical significance, we evaluate the performance of \namex and baselines over 1000 requests. 


\subsection{Overall Performance}
\label{exp:macro}
\mypara{End-to-end latency distribution.} 
Figure~\ref{fig:exp:macro:e2e} shows the end-to-end latency (E2E) distribution of IA and VA under the concurrency as one, as well as that of IA under the concurrency as two and three respectively.
We observe that Janus can fulfill the SLO requirements in all cases despite relatively higher E2E.
This is because Janus aims to improve resource efficiency while meeting latency SLOs. 
Under the premise of fulfilling SLOs, Janus trades in time for resource efficiency.

\begin{figure}
    \centering
    \subfloat[]{
        \includegraphics[width=0.4\textwidth]{./figure/exp/macro/overall-con1-size.pdf}
        \label{fig:exp:macro:size}}
    \vfill
    %\vspace{-3mm}
    \subfloat[]{
    \includegraphics[width=0.41\textwidth]{./figure/exp/macro/ml-analyze-size-con=2-3.pdf}
    \label{fig:exp:macro:size-con=2-3}}
    %\vspace{-0.1cm}
    \caption{Resource consumption of (a) IA (left) and VA (right) under the concurrency as one, respectively, and of (b) IA under the concurrency as two (left) and three (right), respectively.}
   % \vspace{-0.3cm}
\end{figure}

\begin{table}
	\small  
	\centering
\caption{\label{table:exp:overall:resource} Overall resource reduction (normalized by Optimal) by \namex compared to baselines when serving IA and VA, respectively.}
	%\vspace{-0.1cm}
        \setlength{\tabcolsep}{3pt}
	\begin{tabular}{@{}l r r r r r @{}}
		\toprule
		{} &  \textbf{ORION}  & \textbf{GrandSLAM$+$} &  \textbf{GrandSLAM} &  \textbf{Janus$-$} & \textbf{Janus$+$}\\
		\midrule
		%\rule{0pt}{10pt} &\multicolumn{3}{c}{ \textbf{Plan 1}}&\multicolumn{3}{c}{\textbf{Plan 2}}\\
		IA(\%)  & 22.6 & 31.3 & 31.3 & 2.9 & 0\\
		% \midrule
		VA(\%)  & 26.9 & 35.2 & 32.4 & 4.7 & -0.2\\

        
		\bottomrule
	\end{tabular}\\
\label{exp:table:macro:size}
%\vspace{-0.3cm}
\end{table}

\mypara{Resource consumption.}
We compare the resource consumption of \namex and baselines when serving IA and VA given SLO as 3s and 1.5s respectively, with the concurrency as one.
Table~\ref{exp:table:macro:size} shows the average resource reduction of \namex, normalized by Optimal,  when compared with baselines, and Figure~\ref{fig:exp:macro:size} illustrates the detailed comparison.
We can see that \namex outperforms GrandSLAM$+$, GrandSLAM, and ORION significantly. 
This is because \namex fully uses slacks at runtime to improve resource efficiency. 
Compared with Janus$-$, Janus achieves further resource reduction as 2.9\% and 4.7\% for IV and VA respectively, due to Janus's exploration of lower percentiles for head functions.
\jing{Additionally, Janus incurs a negligible increase in resource consumption, i.e., 0.2\%, compared to Janus$+$.}

We also assess \namex's performance under higher loads.
Here, we increase IA's concurrency up to two and three.
For a fair comparison, we increase SLOs to 4s and 5s respectively, to promise GrandSLAM and GrandSLAM$+$ with feasible function sizes.
Figure~\ref{fig:exp:macro:size-con=2-3} shows the resource consumption normalized by Optimal. 
We find that the three early-binding systems, i.e., GrandSLAM, GrandSLAM$+$, and ORION, suffer over-allocation by up to 1.75 times.
This is because the increase of the concurrency further enlarges the runtime variability.
For example, the gap between P99 and P50 of QA (the second function in IA) increases from 2.17 times to 2.32 times on average.
This higher variability magnifies the early-binding's over-provisioning.
As a contrast, \namex relies on its runtime adaptation to capture the variance and resize functions correspondingly, thus reaping higher resource efficiency.


% \begin{figure}
%     \centering
%     \subfloat[]{
%         \includegraphics[width=0.4\textwidth]{./figure/exp/macro/overall-con1-e2e.pdf}
%         \label{fig:exp:macro:e2e}}
%     \vfill
%     \vspace{-3mm}
%     \subfloat[]{
%     \includegraphics[width=0.4\textwidth]{./figure/exp/macro/ml-analyze-e2e-con=2-3.pdf}
%     \label{fig:exp:macro:e2e-con=2-3}}
%     \vspace{-0.3cm}
%     \caption{ 
%     End-to-end latency distribution of (a) IA (left) and VA (right) under the concurrency as one, with SLO (orange dashed line) as 3s and 1.5s, respectively, and of (b) IA under the concurrency as two (left) and three (right), with SLOs set as 4s and 5s, respectively.}
%     \vspace{-0.4cm}
% \end{figure}


%\mypara{Breakdown by requests.} We break down \namex's resource allocation when serving IA and VA while analyzing their individual functions execution latency.
%Due to limited space, we present the result for IA, and that of VA shares the same pattern.
%We observe from Figure~\ref{fig:exp:breakdown:ia} (top) that expect ``fixed" OD, which is the first function with no chance to gain resource adaption, the other two functions, particularly TS, experience varying sizes.
%This reason is twofold.
%First, as depicted in Figure~\ref{fig:bg:ml-func-latency}, the execution time of TS's predecessor, i.e., QA, exhibits high variability, leading to varying time budgets for TS.
%Second, while exploring lower percentiles creates a larger optimization space for QA, it also increases the risk of potential timeouts.
%When a timeout occurs, TS must promptly scale up to prevent SLO violation.
%On the other hand, as shown in Figure~\ref{fig:exp:breakdown:ia} (bottom), there are still considerable slacks (i.e., orange shadows) generated by TS that remain underutilized, thus reducing the overall resource efficiency.
%
%\begin{figure}[h!]
%\centering
%\includegraphics[width=0.4\textwidth]{./figure/exp/macro/ml-analyze-size-breakdown.pdf}
%\vspace{-0.3cm}
%\caption{Breakdown of IA in terms of function size (top) and function execution latency (bottom) with SLO (orange dash line) as 3s and 1.5, respectively. Here, OD, QA, and TS represent \textit{object detection}, \textit{question answer} and \textit{text-to-speech}, respectively.}
%\label{fig:exp:breakdown:ia}
%\end{figure}

%We delve into the gap between \namex and Optimal.
%Figure~\ref{fig:exp:macro:func-size-compare} illustrates function sizes of IA (left) and VA (right) normalized by that of Optimal.
%We observe that \namex's over-allocation mainly derives from the last function, i.e., TS and ICO, respectively.
%As aforementioned, last functions also generates slacks, which however can not be utilized.
%Moreover, to provide strong SLO guarantee, Janus fixes the percentile of last functions at 99\%, inevitably hurting resource efficiency.
%
%\begin{figure}[h!]
%\centering
%\includegraphics[width=0.45\textwidth]{./figure/exp/macro/func-size-compare.pdf}
%%\vspace{-0.2cm}
%\caption{Function size of (a) IA and (b) VA, normalized by Optimal, when given SLOs as 3s and 1.5s, respectively.}
%\label{fig:exp:macro:func-size-compare}
%\end{figure}

\subsection{Effectiveness of Moderate Percentile Exploration}
\label{exp:micro:vary-perc}
\jing{
Here, we assess the effectiveness of the moderate exploration approach (\S\ref{sec:synthesizer:generate}) adopted by Janus, which restricts the exploration of lower percentiles (below P99) to head functions within sub-workflows.
This strategy aims to balance the trade-off between the time- and resource-efficiency of resource adaptation.
We compare Janus with Janus$+$, which extends percentile exploration to both the head function and the next-to-head function.
While this expansion of percentile exploration can yield higher resource efficiency, it comes at the expense of significant time cost in synthesizing hints tables.}

Taking IA as an instance, we observe from Figure~\ref{fig:exp:vary-perc:size} that compared with Janus, Janus$+$ decreases resource consumption merely by 0.6\% on average. This means the optimization space of wider percentile exploration for IA is limited.
However, this limited reduction in resource comes at a significant time cost in synthesizing hints, by up to 107.2 times higher than \namex, as depicted in Figure~\ref{fig:exp:vary-perc:time}.

\begin{figure}[t!]
    %\vspace{-0.2cm}
    \centering
	\subfloat[]{
		\includegraphics[width=0.235\textwidth]{./figure/exp/micro/ml-analyze-vary-head-size.pdf}
		\label{fig:exp:vary-perc:size}}
	\hfill
	\subfloat[]{
		\includegraphics[width=0.225\textwidth]{./figure/exp/micro/ml-analyze-vary-head-time-cost.pdf}
		\label{fig:exp:vary-perc:time}}
	%\vspace{-0.1cm}
	\caption{(a) Workflow sizes and (b) time costs of Janus$+$ and \namex respectively, with SLOs ranging from 3s to 7s.}
    %\vspace{-0.3cm}
\end{figure}

Additionally, Janus's time costs increases marginally as SLO grows.
This is because higher SLOs brings in more candidate adaptation plans.
\namex needs to efficiently evaluate these plans, and figure out the one with the minimum resource consumption, thus incurring higher time costs.
Notably,  the above time costs only happen during hints generation.
When coming to online adaptation,  its overhead is merely less than 3ms (explained in \S\ref{exp:micro:overhead}).

\begin{figure}[!t]
    %\vspace{-0.2cm}
	\subfloat[]{
    \includegraphics[width=0.23\textwidth]{./figure/exp/micro/text-to-speech-timeout.pdf}
		\label{fig:exp:timeout:ia}}
	\hfill
	\subfloat[]{
		\includegraphics[width=0.21\textwidth]{./figure/exp/micro/res-cores.pdf}
		\label{fig:exp:resilience:cores}}
	%\vspace{-0.1cm}
	\caption{(a) Timeout and (b) resilience of TS under varying CPU cores.}
    %\vspace{-0.3cm}
\end{figure}

\subsection{Timeout and Resilience}
\label{exp:micro:timeout-resilience}
We propose timeout and resilience to quantify the risk of SLO violations (detailed in \S\ref{sec:synthesizer:generate}).
Owing to space constraints, we use TS from IA as an example, and other functions exhibit similar patterns.
We observe from Figure~\ref{fig:exp:timeout:ia} that timeout decreases as either percentiles or available CPU cores increase.
This is because additional resources enhance functions' capability to handle both runtime interference and variability of working sets\cite{socc22-owl}, thus reaping lower timeout.

As for resilience, Figure~\ref{fig:exp:resilience:cores} shows a marginal reduction as the number of provisioned CPU cores increase.
This is attributed to non-parallelizable operations within functions, leading to diminishing returns on execution time despite the addition of more resources.
Additionally, higher concurrency enhances higher resilience.
This is due to the increased computing load, which heightens functions' sensitivity to resources, thereby boosting resilience.


%Resilience varies across different functions.
%As shown in Figure~\ref{fig:exp:resilience:ia}, the resilience of TS is much higher than that of other functions.
%Similar to timeout, resilience is also highly correlated to functions' sensitivity to resources.
%Generally, the higher computing intensity functions have, higher resilience sensitivity they own.
%
%\begin{figure}
%    \centering
%    \subfloat[]{
%        \includegraphics[width=0.22\textwidth]{./figure/exp/micro/ml-analyze-func.pdf}
%        \label{fig:exp:resilience:ia}}
%    \hfill
%    \subfloat[]{
%        \includegraphics[width=0.22\textwidth]{./figure/exp/micro/video-analyze-func.pdf}
%        \label{fig:exp:resilience:va}}
%    \caption{Average resilience of (a) IA and (b) VA}
%\end{figure}

\begin{table}[!t]
        \small
	\centering
	\caption{Resource consumption and percentiles for the head function of IA with the weight as one and three, respectively.}
	%\vspace{-0.1cm}
	\begin{tabular}{@{}l r r @{}}
		\toprule
		{} &  \textbf{weight=1}  & \textbf{weight=3} \\
		\midrule
		%\rule{0pt}{10pt} &\multicolumn{3}{c}{ \textbf{Plan 1}}&\multicolumn{3}{c}{\textbf{Plan 2}}\\
		CPU (Millicore)  & 1442.9  & 1228.6    \\
		%\midrule
		Percentile (\%)  & 94.4 & 91.3  \\
		\bottomrule 
	\end{tabular}\\
	\label{table:exp:micro:weight}
\end{table}

\subsection{Impact of Weight}
\label{exp:micro:weight}
Higher weights for head functions is introduced to further improve the resource efficiency of Janus (detailed in \S\ref{sec:synthesizer:generate}).
Taking IA as an example, we evaluate its resource consumption with SLOs ranging from 4s to 10s under the weight as one and three, respectively.
The results show that when the SLO is less than 8s the moderate weight consumes less resources by 2.9\% on average.
Conversely, as the SLO becomes relaxed, the higher weight allows to further reduce resource by 1\% owing to its larger optimization space.

We also examine the impact of weights on resource consumption and percentile selection for head functions.
Table~\ref{table:exp:micro:weight} shows that \namex tends to decrease both resource allocation and percentiles under higher weights.
This is because with higher weights the objective focuses more on decreasing the size of head functions, rather than that of sub-workflows.
Lower percentiles typically indicate that fewer requests need to be completed within the specified time budget, thus requiring fewer resource consumption.
This aligns well with the objective with higher weights.
Yet, lower percentiles may expose sub-workflows at the risk of timeouts, particularly under strict SLOs.
To prevent SLO violations, non-head functions may compensate by requesting additional resources, potentially hurting the overall resource efficiency.



\subsection{Effectiveness of Hints Condensing}
\label{exp:micro:condense}

\begin{figure}[!t]
\centering
%\vspace{-0.4cm}
\includegraphics[width=0.4\textwidth]{./figure/exp/micro/overall-total-numbers.pdf}
%\vspace{-0.1cm}
\caption{Total numbers of hints synthesized for IA (left) and VA (right) under different weights.}
%\vspace{-0.3cm}
\label{fig:exp:condense:total-hints}
\end{figure}

We assess hints table sizes, i.e., numbers of hints, with and without condensing.
As explained in \S\ref{sec:synthesizer:generate} we depend on our testbed's capacity to configure the range of time budgets explored during hints generation.
Specifically, for IA the range is from 2s to 7s,  from 3s to 7s, and from 4s to 10s, with a fine-grain step of 1ms, under the three different concurrency, respectively.
For VA this range is from 1.5s to 2s with a step of 1ms.
Additionally, weight, as a hyper-parameter involving hint generation, also influences hints table sizes.
Therefore, we assess the two workflows' hints table sizes under different weights ranging from 1 to 3 with a step as 0.5, respectively.

Figure~\ref{fig:exp:condense:total-hints} illustrates the detailed number of hints for IA (left) and VA (right), respectively.
After effective condensing, the overall hints for IA and VA are less than 147 and 96 respectively, achieving compression ratios of up to 99.6\% and 98.2\%.
In addition, the size of hints tables decreases as the weight increases.
The reason is that higher weights focus more on minimizing the size of the head function, which may lead to the sub-workflow's over-allocation.
This over-allocation increases hints' applicability across different runtime conditions, thus benefiting hints tables with smaller sizes.


% \begin{figure}[h!]
%     \vspace{-0.4cm}
%     \centering
%     \subfloat[]{
%     \includegraphics[width=0.24\textwidth]{./figure/exp/micro/ml-analyze-total-numbers.pdf}
%         \label{fig:exp:condense:total-hints:ia}}
%     \hfill
%     \subfloat[]{
%         \includegraphics[width=0.21\textwidth]{./figure/exp/micro/video-analyze-total-numbers.pdf}
%         \label{fig:exp:condense:total-hints:va}}
%    	\vspace{-0.3cm}
%     \caption{Total numbers of hints synthesized for (a) IA and (b) VA under different weights.}
%     \vspace{-0.4cm}
% \end{figure}

%\begin{table}[!h]
%	\normalsize
%	\centering
%	\caption{\label{table:exp:overall:resource} Hint table size reduction for IA's and VA's sub-workflows with function numbers as one, two, and three, respectively.}
%	\vspace{-0.3cm}
%	\begin{tabular}{@{}l r r r @{}}
%		\toprule
%		{} &  \textbf{\#Func.=1}  & \textbf{\#Func.=2} &  \textbf{\#Func.=3} \\
%		\midrule
%		%\rule{0pt}{10pt} &\multicolumn{3}{c}{ \textbf{Plan 1}}&\multicolumn{3}{c}{\textbf{Plan 2}}\\
%		IA(\%)  & 99.6  & 99.4  & 98.9  \\
%		%\midrule
%		VA(\%)  & 98.2 & 97.5  & 94.3 \\
%		\bottomrule
%	\end{tabular}\\
%\label{exp:table:condense}
%\end{table}

%We delve into the hints condensing.
%Taking IA as an instance, we observe from Figure~\ref{fig:exp:table-size:ia:weight1} that hints table size is affected by multiple intertwined factors, such as the number of functions in sub-workflows, weight and concurrency.
%Specifically, larger sub-workflows, which have more functions, own larger hints table.
%This is because the increased number of functions magnifies the sub-workflow's runtime variability.
%Each possible runtime situation requires a specific hint to keep its adaptation's high accuracy.
%Consequently, \namex inevitably maintains a relatively larger table.
%On the other hand, comparing the hints table size with weight as one and three respectively, illustrated in Figure~\ref{fig:exp:table-size:ia:weight1} and Figure~\ref{fig:exp:table-size:ia:weight3}, we find that higher the weight is, smaller the hints table will be 
%This observation also stands for VA, whose table size decreases from 38 to 21 with the weight increasing from one to three (red bar in Figure~\ref{fig:exp:table-size:va}) .
%The reason is that higher weights deviate \namex's optimization objective from minimizing the expected size of the whole sub-workflow to minimizing that of the head function, which may incur the sub-workflow's over-allocation.
%Moreover, this over-allocation increases hints' applicability across runtime conditions, thus benefiting hints table with smaller sizes.

%\begin{figure}
%    \centering
%    \subfloat[]{
%        \includegraphics[width=0.22\textwidth]{./figure/exp/micro/ml-analyze-hints-nums-weight=1.pdf}
%        \label{fig:exp:table-size:ia:weight1}}
%    \hfill
%    \subfloat[]{
%        \includegraphics[width=0.22\textwidth]{./figure/exp/micro/ml-analyze-hints-nums-weight=3.pdf}
%        \label{fig:exp:table-size:ia:weight3}}
%%    \hfill
%%    \subfloat[]{
%%        \includegraphics[width=0.25\textwidth]{./figure/exp/micro/video-analyze-hints-nums.pdf}
%%        \label{fig:exp:table-size:va}}
%    \caption{Numbers of hints synthesized for IA's three sub-workflows with function numbers as one (blue), two (black) , and three (red), when given weight as (a) one and (b) three, respectively. (c) Number of hints for VA's (sub-)workflows with weight as one (left) and three (right).}
%\end{figure}


% \begin{figure}[h!]
% \centering
% \includegraphics[width=0.47\textwidth]{./figure/exp/micro/ml-analyze-hints-nums.pdf}
% \vspace{-0.3cm}
% \caption{Number of hints (hint table size) for IA's (sub-)workflows with weight as one (left) and three (right), under given concurrency as one, two and three, respectively. Here, number of functions represent (sub-)workflow's size.}
% \label{fig:exp:table-size:ia}
% \end{figure}

% \begin{figure}[h!]
% \centering
% \includegraphics[width=0.25\textwidth]{./figure/exp/micro/video-analyze-hints-nums.pdf}
% \vspace{-0.3cm}
% \caption{Number of hints (hint table size) for VA's (sub-)workflows with weight as one (left) and three (right), Here, number of functions represent (sub-)workflow's size.}
% \label{fig:exp:table-size:va}
% \end{figure}

\begin{figure}[!t]
\centering
%\vspace{-0.4cm}
\includegraphics[width=0.4\textwidth]{./figure/exp/micro/slo-batch=1.pdf}
%\vspace{-0.1cm}
\caption{IA's (left) and VA's (right) resource consumption (normalized by Optimal) under different SLOs. }
%\vspace{-0.3cm}
\label{fig:exp:slo}
\end{figure}

\subsection{Impact of SLO}
\label{exp:micro:slo}
\jing{We compare \namex's resource consumption with baselines when serving IA and VA under varying SLOs.
For clarity, we normalize the results by Optimal.
To ensure readability, we illustrate only the results of ORION, GrandSLAM, and Janus.
As shown in Figure~\ref{fig:exp:slo}, for IA \namex outperforms ORION and GrandSLAM by 16.1\% and 24.1\% on average, respectively.
In terms of VA, \namex outperforms the baselines by 22.2\% and 27.7\%, respectively.
Notably, as SLOs increase, \namex's performance gains decrease marginally.
This is due to our testbed's limitation of CPU cores, i.e., 1000 millicores at least per function, restricting \namex's further improvement.
For instance, under given SLOs as 6s and 7s, IA's resource consumption reduces to 3043.6 millicores, approaching that of Optimal (i.e., 3000 millicores).
As for other baselines, GrandSLAM$+$ exhibits performance comparable to GrandSLAM, with a marginal gap of less than 0.6\%.
Janus$+$ achieves a resource reduction of up to 1.8\% compared to Janus. Janus$-$ incurs higher resource consumption, exceeding Janus by 3.2\% and 4.3\% on average, when serving IA and VA respectively.}



%\jing{For readability, do not illustrate the results of GrandSLAM+ and Janus-/Janus+, which are provided in words.}

%\begin{figure}[h!]
%\centering
%\includegraphics[width=0.45\textwidth]{./figure/exp/micro/slo-batch=2-3.pdf}
%\vspace{-0.3cm}
%\caption{IA's resource consumption under concurrency as two (left) and three (right), respectively.}
%\label{fig:exp:slo:high-concurrency}
%\end{figure}

%Additionally, we also evaluate \namex's performance under higher workloads, i.e., concurrency as two and three respectively.%, when serving IA.
%Under given concurrency as two, \namex reduces resource consumption by 23.1\%, 30.5\%, 30.1\%, and 4.5\%, when compared with ORION, GrandSLAM, GrandSLAM$+$, and Janus$-$, respectively.
%This improvement further increases by up to 2\% as concurrency rises to three.
%Notably, when given concurrency as three, neither GrandSLAM nor GrandSLAM$+$ has feasible sizes for SLO as 4s, due to their inflexible allocation. 

\subsection{System Overhead}
\label{exp:micro:overhead}
We evaluate \namex's time cost for online resource adaptation serving IA and VA respectively, under varying SLOs from 2s to 7s with the weight as one and three, respectively.
The results show that the time cost remains under 3ms.
This suggests that Janus maintains high time-efficiency unaffected by either SLOs or weights. 

We measure the memory footprint of Janus during online adaptation and offline hints generation.
As for online adaption, Janus consumes negligible memory less than 12.1MB and 10.9MB for IA and VA, respectively.
In terms of offline hints generation, the average memory consumption is less than 12.4MB and 10.9MB for IA and VA, respectively.

