\section{Related-work}
Optium~\cite{eurosys23-optium} conducts data-driven analysis of Alibaba unified scheduler, which further confirms the predictability of applications' running performance.

\subsection{Performance interference}
\begin{itemize}
    \item ProPack~\cite{hpdc23-propack}.This work unlocks the potential of serverless computing to support high-parallel applications.Yet, the empirical study reveals that current platforms fail to establish the required number of concurrent function instances simultaneously, resulting in substantial scaling time, i.e., the time between the start of the first function instance and the start of the last instance. ProPack proposes to pack multiple functions into one function instance to lower the scaling time. Particularly, Propack notices the performance interference among co-located functions while formulating the interference concerning function types and the degree of packing degree (i.e., the optimal number of functions packed together). This work however overlooks a thorough analysis of the root cause of performance interference.
    \item Long-term profiling~\cite{sesame23-night-shift}. Schirmer et al. conducted a long-term study of serverless systems in three aspects: request-response latency, unexpected cold starts, and long-term trends.Specifically, this work evaluated three functions, including floating point operations, matrix function, and face detection, which are invoked every 40 seconds over multiple months.
    The results suggest that \textit{latency of up to 15\% and more than three times as many unexpected cold starts from day to night within the same day, and these effects are most noticeable at the start of the week.}
    \item X86 vs ARM~\cite{icpe22-x86-vs-arm}. This work reveals the impacts of CPU types, i.e., X86 and ARM on function performance. Meanwhile, it illustrates the correlation between CPU steal time and performance degradation.
    \item Serverless isn't server-less~\cite{wosc20-serverless-not-server-less}.
    The contention for shared resources, such as CPU, CPU cache, network, and IO, in serverless computing is fully explored. 
    Depending on AWS Lambda, the results show that compared with CPU-bound functions, network function suffers severe performance variation.
    \item CPU-TAMS~\cite{ic2e22-cpu-tams}.
    CPU-TAMS fully explores the resource-to-memory mappings, i.e., the number of allocated resources like vCPU cores and network/IO throughput given specific memory sizes of,  of different serverless platforms.
    Based on the empirical analysis, this work proposed a function size recommendation strategy based on CPU times.
    \item ServerlessBench\cite{socc20-serverlessbench}. This benchmarks demonstrates the performance of different serverless platforms in the aspects of communication latency, starup latency, stateless execution, and  resource effciency as well as performance isolation.
    Notably, it validates the inefficient isolation for memory bandwidth and CPU cores among co-located functions in commercial platforms (Ant Financial).
    \item Huawei cloud~\cite{socc23-huawei-cloud}. This production trace reveals the features of workloads in Huawei cloud while quantifying delays of functions throughout their holistic life cycle.
    \item SeBS~\cite{middleware21-sebs}. SeBS reports the wide latency distributions of I/O bound (network and disk) functions.
\end{itemize}


\subsection{Early-bind}

\jing{Aquatope~\cite{asplos23-aquatope} and FIRM~\cite{osdi20-firm} take into account runtime performance interference (uncertainty) to decide function sizes.}

\jing{ORION~\cite{osdi22-orion} and WISEFUSE~\cite{mac22-wisefuse} observe high variability and the skew in function execution latency and develop a distribution-based performance modeling to provision serverless DAGs.}


COSE~\cite{infocom20-cose} utilizes statistical Learning to learn the cost/performance of functions with respect to different sizes.
Then, it selects the ``best" size for a single function or a chain of function to minimize the overall costs without violating SLOs.



FA2~\cite{rtas22-fa2} fully considers the dependency of functions within a workflow as well as their uncertain execution paths to periodically adapt resources to minimize resource consumption while promising SLAs.


SIMPPO~\cite{socc22-simppo} utilizes online learning to boost resource management in serverless.

Parrotfish~\cite{socc23-parrotfish} employs online parametric regression to learn the relationship between function execution latency and memory sizes, and find the most cost-optimal sizes for a single function, with latency constraints.

%rely on elastic batching to achieve higher inference throughput for serverless deep learning workloads, without violating SLOs. 



Cypress~\cite{socc22-cypress} enables input size-aware request batching and resource provisioning for input size-sensitive applications (e.g., sentiment analysis). 
Atoll~\cite{socc21-atoll} enables proactive resource scaling as well as deadline-aware scheduling to minimize SLO violations.

Fifer~\cite{middleware20-fifer} leverages the slacks, generated at each stage, to adjust batch sizes and scale out/in containers for higher resource utilization with SLO guarantees.

Erms~\cite{asplos23-erms} systematically evaluate latency targets for shared microservices while adjusting their sizes to minimize resource consumption.

ReSC~\cite{arxiv22-resc} and FUSIONIZE~\cite{ic2e22-fusionize} propose newly resource-centric, distinct from current function-centric, serverless architecture for striking the trade-off between flexibility and performance.





On the other hand, there exists research from both the industry and the academic that concentrate on selecting the optimal function sizes for developers, such as AWS Lambda Power Turing~\cite{lambda-tuning}, AWS Compute Optimizer~\cite{lambda-compute-optimizer}, Sizeless~\cite{middleware21-sizeless} and COSE~\cite{infocom20-cose}.



\subsection{Late-bind}

Nevertheless, current offline hints hold compromised accuracy~\cite{middleware21-sizeless,infocom20-cose,socc23-parrotfish,socc22-cypress}, due to their simplistic modeling of execution latency or ad-hoc threshold-based rules; meanwhile, online hints suffer unsatisfactory time-efficiency owing to their complex formulations, e.g., transform resource adaptation into optimization problems ~\cite{rtas22-fa2,infocom22-stepconf,icdcs23-joint-video-analyze}.

\jing{BATCH~\cite{sc20-batch} fully considers serverless workload burstiness (the intensity of arrival requests) to dynamically adjust function size (memory size) and batching parameters, for the sake of minimizing monetary cost without violating SLOs.}

\jing{Kraken~\cite{socc21-kraken} and Xanadu~\cite{middleware20-xanadu} employ proactive and reactive resource scalers simultaneously to provision dynamic DAG workloads, which have uncertain execution paths, aiming to minimize resource consumption without SLOs violations.}

\textbf{Morhpling~\cite{socc21-morphling}, Llama~\cite{socc21-llama}, and INFaaS~\cite{socc21-llama} focus on resource auto-configuration.}

Llama


\jing{Apart from auto-scaling, there are work focusing on serverless workflows scheduling,  i.e., when, where, and how to run functions. 
For example, Sequoia~\cite{socc20-sequoia} offers multiple scheduling policies available to providers/developers to alleviate issues mid-chain drops.}

\section{Exp}
\mypara{Workflows}
Setups for motivation. We implement four different types of functions as follows:
\begin{itemize}
    \item \textbf{CPU-intensive}. AES encryption for a text with size as 100KB~\cite{isca22-lukewarm}.
    \item \textbf{Memory-intensive}. Read data, with the size as 50MB and 100MB, from a locally deployed Redis.
    \item \textbf{Network-intensive}. Send data, with the size as 1MB and 5MB, to a remote server through network sockets.
    \item \textbf{IO-intensive}. Use os.fsync to write data, with the size as 1MB and 5MB, to the local disk~\cite{atc18-peek-bench}.
\end{itemize}

We configure all the above functions with the same size as one CPU core and memory with 500MB while fixing their concurrency as one, i.e., each instance can only process one request at once.
Additionally, the VM, which deploys the function instances, occupies eight CPU cores and 10752MB while limiting the maximum scale of instances to six.