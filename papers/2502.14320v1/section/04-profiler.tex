\section{Profiler}
\label{sec:profilier}
Here,  we discuss how to profile function execution latency.
Based on that, two metrics are proposed as a preparation for synthesizing hints.
\subsection{Diverse percentiles based profiling}
As explained in \S{\ref{sec:bg:worst-case}}, function execution latency is a distribution, making the current work, which either depends on a single statistic or a simple 99\% percentile distribution, insufficient.
To fix this issue, for any given batch sizes we introduce diverse percentile latency distributions to profile the execution latency, which is expressed as $L(p,k)$, with CPU cores and percentiles denoted as $k$ and $p$, respectively.

\subsection{Timeout and resilience.}
The diversity in percentiles enables more opportunities to explore higher resource efficiency but comes at the risk of SLO violations.
Specifically, when setting percentiles lower than 99\%, it may incur under-estimation of function execution, making functions prone to \textit{timeout}, i.e., their actual execution latency over the estimated latency.
Timeout is expressed as follows
\begin{eqnarray}
     D(p,k) = L(P_{99},k) -L(p,k).
\end{eqnarray}

Consequently, for preventing from SLO violation, \namex has to provision more CPU cores to downstream functions to absorb previous timeout.
Moreover, we propose another metric called \emph{resilience} to quantify the absorption capability, which is expressed as follows
\begin{eqnarray}
    R(p,k)= L(p,K_M)-L(p,k),
\end{eqnarray}
where $k_M$ denotes the maximum available CPU cores.
Notably, timeout must be restricted within the upper bound of resilience, such that SLO can be guaranteed.

 







