\section{Introduction}
\label{sec: intro}

Currently, generative large language models (LLMs) show remarkable natural language understanding and instruction-following abilities.
LLMs champion a wide range of natural language processing tasks~\cite{2020t5,2022flant5,Brown2020LanguageMA,OpenAI2023GPT4TR, ouyang2022training} and generalize well to unseen tasks given appropriate prompts~\cite{zhou2023leasttomost, Kojima2022LargeLM, Wei2022ChainOT,sanh2022multitask}.
Consequently, LLMs give rise to many revolutionary AI applications, from scientific problem-solving to intelligent assistants~\cite{schick2023toolformer,tong2024dartmath, wang2024voyager}.


However, as LLMs begin to attract a wider audience, their privacy concerns frequently occur.
LLMs' privacy issues are criticized for both the training and inference stages.
On the one hand, since LLM's training data are massively crawled from the Internet without careful inspection, it is likely that LLMs may memorize private information~\cite{carlini-2021-extracting, LI-2023-Jailbreak, Ishihara2023TrainingDE}.
On the other hand, during the inference stage, LLMs may be applied on sensitive domains and access users' private information.



To enhance LLMs' trustworthiness, recent works propose diverse alignment techniques~\cite{Christiano-2017-rlhf, rafailov2023direct, inan-2023-llama-guard} to harness LLMs to safety, value, and privacy requirements.
To assess their efficacy, numerous benchmarks~\cite{LI-2023-Jailbreak, Li-LLMPBE-2024,zeng2024privacyrestore} have been established to investigate LLMs' privacy issues.
While current safety alignment strategies have demonstrated effectiveness across these benchmarks, existing privacy evaluations ignore the impact of context and suffer the following limitations.
%%% lack contextual information
First, the coverage of evaluation is confined to patterns of personally identifiable information (PII).
%%% parsed patterns do not indicate privacy leakage
Second, matching the PII pattern does not always suggest actual privacy leakage.
For example, doctors are permitted to share their patients' sensitive medical records for treatment.
%%% cannot align with human expectation
Therefore,  protecting PII may not well align with individuals' actual privacy expectations.

\begin{table*}
\centering
\small
%\resizebox{0.4 \textwidth}{!}{
\begin{tabular}{l c c  c c c}
\toprule
%%% open source?
Benchmark Source & Data \# & Data Type & Domain Coverage  & Real Data? & CI Probing?\\
\midrule
\citet{fan2024goldcoin} and \citet{li-2024-privacychecklist} & 832 & Court Case & Healthcare & Hybrid &  \ding{55}\\

%\citet{Noah-EvaluateCI-2019}  & 1,800 & 40 & 309  \\
\citet{shvartzshnaider2024llm} & 8,712 & Template-based & Internet of Things & \ding{55} & \ding{55} \\
\citet{mireshghallah2024can} & 1,326 &  Multi-tiered & Multi-domain & \ding{55} & \ding{51}\\

\citet{cheng-2024-cibench} & 44,100 & Dialog \& Email & Multi-domain & \ding{55} & \ding{51}\\
Ours & 154,191 & Court Case \& Policy & Multi-domain & Hybrid &  \ding{51}\\
\bottomrule
\end{tabular}
%}
\vspace{-0.1in}
\caption{\label{tab:dataset-compare}
Statistics comparisons among contextual privacy evaluation benchmarks.
}
\vspace{-0.2in}
\end{table*}


Another line of the latest works~\cite{shvartzshnaider2024llm, ghalebikesabi-2024-operationalizing, cheng-2024-cibench, fan2024goldcoin, li-2024-privacychecklist, mireshghallah2024can} starts to evaluate on contextual privacy.
%%%% need a table to compare the data #
However, their benchmark data are either synthetic and unable to accurately reflect real data distribution or restrained in narrow domains with limited quantities.
In Table~\ref{tab:dataset-compare}, we provide a comparative analysis of these benchmarks' data statistics. 


To bridge the aforementioned gaps, we extend prior works on contextual privacy evaluation by incorporating a broader range of evaluation data across diverse domains.
%We present \name, a comprehensive evaluation benchmark to investigate contextual privacy grounded on mainstream privacy regulations.
We present \name, a privacy evaluation benchmark following the principles of  Contextual Integrity theory to comply with mainstream privacy regulations.
Our \name collects real court cases, privacy policies, and synthetic vignettes built from official toolkits for privacy and safety regulations, including the General Data Protection Regulation (GDPR), the EU Artificial Intelligence Act (AI Act), and the Health Insurance Portability and Accountability Act of 1996 (HIPAA).
We follow the Contextual Integrity theory and use LLMs to annotate the collected data with humans in the loop.
To probe whether LLMs are able to understand the private information flows inside the given context, we also construct more than 140,000 multiple-choice questions based on the collected data.
In addition, we further expand the scale of auxiliary knowledge bases to facilitate the reasoning process.
We conduct extensive experiments on several LLMs with prompting and retrieval augmented generation tricks to test these LLMs' legal compliance.
In summary, our contributions are as follows:\footnote{Code is publicly available at \url{https://github.com/HKUST-KnowComp/PrivaCI-Bench}.}

1) We present \name, a comprehensive contextual privacy evaluation benchmark that covers real court cases, privacy policies, and synthetic vignettes augmented from official toolkits.


%%% EU AI Act --> First Attempt
2) We deliver an extended auxiliary knowledge base to facilitate reasoning on privacy compliance.

%%% EU AI Act --> First Attempt
3) Our proposed \name covers the EU AI Act regulation, which is the latest regulation that has not yet been systematically evaluated.

4) We conduct extensive evaluations using our benchmark to test both open-source and closed-source LLMs. 
We also perform internal probing to assess LLMs' context understanding abilities.
