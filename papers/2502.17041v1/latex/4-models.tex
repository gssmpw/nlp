\section{Evaluation Setups}
\label{sec: eval}

In this section, we detailedly illustrate our setups for LLMs' contextual privacy evaluation.

\subsection{Implementation of Judgment Modules}
\label{sec: judge}
% DP, COT, RAG
To evaluate LLMs' legal compliance for given benchmark samples, we mainly consider the following three straightforward strategies:

\noindent$\bullet$ Direct prompt (\textbf{DP}).
We prompt LLMs with only the context and directly instruct them to determine if the given context is permitted, prohibited, or unrelated to specific regulations.

\noindent$\bullet$ Chain-of-Thought reasoning (\textbf{CoT}).
We prompt LLMs to automatically list step-by-step plans to analyze the given case and then execute the steps to determine privacy violations similar to DP. 

\noindent$\bullet$ Retrieval augmented generation (\textbf{RAG}).
%We apply the \textit{LLM explanation} to clarify the case context with legal terms to facilitate the retrieval process.
Given the context, we first resort to the LLMs to explain the context by using their knowledge of the corresponding legal terms.
Then, we implement BM25 to search for relevant sub-rules.
Lastly, we feed both the retrieved sub-rules and the context into the prompt to improve in-context reasoning.

In addition to these naive implementations, we also consider feeding the ground truth CI parameters and regulations to the LLMs to evaluate the effectiveness of our \name.


\noindent$\bullet$ Direct prompt with ground truth CI parameters (\textbf{DP+CI}).
We instruct LLMs using the direct prompt template with our annotated CI parameters to determine legal compliance.


\noindent$\bullet$ Direct prompt with ground truth CI parameters and regulation content (\textbf{DP+CI+LAW}).
We extend the direct prompt template by including annotated CI parameters and applicable regulations to evaluate LLMs' compliance.


%%% 
\subsection{Evaluated LLMs}
% Qwen2.5-7B-Instruct
% Qwen--QwQ-32B
% Mistral-7B-Instruct-v0.2
% Llama-3.1-8B-Instruct
% gpt-4o-mini
% o1--mini ???
We evaluate a wide range of open-source and closed-source LLMs.
For open-source LLMs, we download their official model weights and generate responses on two NVIDIA  H800 80GB graphic cards.
We evaluate DeepSeek-R1 (671B)~\cite{guo2025deepseek}, Llama-3.1-8B-Instruct~\cite{llama3modelcard}, Qwen2.5-7B-Instruct,  Qwen-QwQ-32B~\cite{Yang2024Qwen2TR}, and Mistral-7B-Instruct-v0.2~\cite{jiang2023mistral}.
For the closed-source LLM, we evaluate the GPT-4o-mini performance with API accesses.
Notably, Qwen-QwQ-32B and DeepSeek R1 are specifically optimized to enhance their reasoning abilities.
Since they are both tuned for multi-step reasoning via reinforcement learning, we omit their results on retrieval augmented generation.

\subsection{Tasks and Metrics}
% 3 way classification
% RAG R performance??
% CI probing
Our designed tasks include legal compliance evaluation and context understanding probing.

For the legal compliance evaluation, we ask LLMs to perform a three-way classification to determine if the given contest is \textit{permitted} by, \textit{prohibited} by, or \textit{not applicable} to a specific regulation.
We implement regular expression parsers to capture the generated predictions and regard parsing failures as incorrect.
We report the accuracy, precision, recall and F1 score with a single run.

For context understanding probing, we ask LLMs to answer multiple choice questions mentioned in Section~\ref{sec: data processing} and calculate their accuracies across the 3 difficulty levels.