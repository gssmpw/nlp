\section{Experimental Results}


In this section, we systematically evaluate current LLMs' performance on our \name.



\input{latex/tab-privacy}


\subsection{Evaluation on Legal Compliance}
To study whether LLMs can comply with existing privacy regulations, we prompt these LLMs with our collected cases.
Table~\ref{tab:privacy_result} evaluates LLMs' legal compliance accuracies over the four domains.
The compliance results suggest the following findings.

%%% TO DO LIST
%%% 1. EU AI Act ANALYSIS, why it is so bad for LLMs
%%% 2. CoT and RAG not working on AI Act, GDPR?
%%% 3. Parsing Errors analysis or Not Relevant Analysis?
%%% 
1) \textit{The collected EU AI Act and ACLU subsets are the most challenging subsets for legal compliance. }
As outlined in Section~\ref{sec: ai act}, cases from the EU AI Act are synthesized according to its official compliance checker.
Therefore, these cases are not likely to be accessed by LLMs and LLMs can only use their reasoning abilities to determine compliance.
We further investigate the precision, recall and F1 scores for LLMs' predictions over each class on Table~\ref{tab:compliance_detail}.
Both LLMs underperform in the permitted cases.
%For example, Mistral-7B-Instruct has recall scores of no more than 8\% on permitted cases while nearly 100\% on the not-applicable cases, which implies that it classify most the permitted cases as not applicable cases.
For instance, Mistral-7B-Instruct has recall scores of no more than 8\% on permitted cases, while getting nearly 100\% on not-applicable cases.
The results suggest that LLMs cannot distinguish between permitted and not applicable cases.
Regarding the ACLU cases, they always connect with a wide range of legal regulations, including the Fourth Amendment to the United States Constitution and the Freedom of Information Act.
The ACLU data demand a more comprehensive understanding of their applicable regulations, and compliance is harder to determine.
Consequently, even the best-performing reasoner models (QwQ-32B and Deepseek R1) fail to attain satisfactory results on the two subsets.
%These results suggest that current LLMs


2) \textit{Chain-of-Thought reasoning and naive RAG implementation may not always help improve LLMs' safety and privacy compliance.}
For CoT prompting, its effectiveness is model-specific.
Our evaluation of instruction-tuned LLMs, including Mistral-7B, Qwen-2.5-7B and Llama-3.1-8B, reveals general accuracy improvements compared to direct prompting (DP).
However, this trend does not hold for all models.
Specifically, GPT-4o-mini and Deepseek R1 reasoner exhibit degraded performance when using CoT prompting.
On the other hand, the performance of our implemented naive retrieval augmented generation (RAG) method is domain-specific.
For the HIPAA domain, RAG generally leads to the best performance, which aligns with findings from prior research ~\cite{li-2024-privacychecklist}.
However, this improvement fails to extend to the EU AI Act and GDPR domains, where RAG results in notable drops in accuracy.
%\tbc{We further give a detailed analysis in xxx.}




\input{latex/tab-MC}
\subsection{Evaluation on Context Understanding}

Besides evaluating the overall performance on the compliance task, we also convert the parsed structured cases into multiple-choice questions as stated in Section~\ref{sec: data processing} with 3 difficulty levels for the EU AI Act, GDPR, and HIPAA domain.
These questions enable us to probe how well LLMs are able to understand the context and identify the key CI parameters inside its information flows.
Table~\ref{tab:mcq_results_split} shows LLMs' performance over these multiple-choice questions.
The results of the context understanding task imply the following findings.


3) \textit{Existing LLMs can explicitly identify the CI parameters of the information flow inside the given context.}
For prompted multiple-choice questions, LLMs, on average, can reach accuracies of approximately \textasciitilde 90\% on the Easy subset, \textasciitilde 80\% on the Medium subset, and \textasciitilde 60\% on the Hard subset.
The high accuracy suggests that LLMs are well aware of the context and its key characteristics inside the context's information flow.


%% qwq vs qwen
4) \textit{LLMs' reasoning enhanced by reinforcement learning further improves the context understanding abilities.}
When comparing Qwen-2.5-7B-Instruct with Qwen's latest QwQ-32B reasoner model, Qwen's QwQ-32B has higher accuracy over most subsets, especially on the hard questions.
The result indicates that reinforcement learning helps LLMs to better understand and analyze the context.
Consequently, better context-understanding abilities further improve legal compliance, as indicated by the results of Table~\ref{tab:privacy_result}.

5) \textit{The context of EU AI Act subset is challenging for LLMs to understand.}
On average, all LLMs have comparable performance across the Easy, Medium, and Hard subsets of the GDPR and HIPAA domains.
However, their accuracies on the EU AI Act subset fall significantly behind the other two domains.
We manually examine samples within the EU AI Act and observe that their parsed roles of CI parameters are mostly abstract legal terms such as ``Law Enforcement Agencies,'' ``Importer,'' ``Operator'' and ``provider.'' 
These terms make it hard to correctly identify the stakeholders for LLMs.
In addition, compared with real cases, the AI Act's synthetic vignettes also lack narrative coherence for describing the information flows.
Hence, LLMs struggle to perform well on the multiple-choice questions of the AI Act domain.
As a result, LLMs' compliance also degrades.
%The context understanding results on the EU AI Act data suggest that it is the most challenging subset and partially explains why LLMs underperform on the EU AI Act's legal compliance task.
%This manual inspection partially explains why LLMs underperform on the legal compliance tasks associated with the EU AI Act.


%\input{latex/tab-ablation}
\input{latex/fig-ablations}
%%% case study?
\subsection{Ablation Studies}

To study the effectiveness of our annotated CI parameters and applicable regulation content, we further perform ablation studies by feeding LLMs with ground truth CI parameters and regulations as stated in Section~\ref{sec: judge}.

Figure~\ref{fig:ablations} presents the accuracies of DP+CI and DP+CI+LAW across various LLMs for the legal compliance task.
By comparing DP+CI with CI, we observe that appending the contextual integrity parameters significantly improves LLMs' accuracies, particularly in the HIPAA and ACLU domains. 
Such results suggest that CI parameters indeed help LLMs better understand the context and improve legal compliance performance.
Furthermore,  for DP+CI+LAW, we augment the applicable regulations to DP+CI and obtain consistent performance gains.
Consequently, DP+CI+LAW has the best performance compared with our implemented DP, CoT, and RAG methods.
The results of DP+CI+LAW highlight the effectiveness of retrieval augmented generation methods, provided that the retrieved documents are both relevant and applicable.
%These results further suggest that naive RAG implementation may not help improve LLMs' compliance due to wrong retrieval results, implying that there are gaps between common context and legal terminologies.
Moreover, our ablation studies also imply that naive RAG implementations may degrade LLMs' compliance when the retrieval step yields irrelevant results. 
Such retrieval failures disclose a discrepancy between general context and domain-specific legal terminologies, which suggests that our \name requires a tailored retrieval module for improvement.

%suggesting that careful curation and relevance of retrieved documents are essential for improving model performance in legal applications.

%\subsection{Case Studies}


\subsection{Human Evaluations}
\label{subsec:human_eval}
%% CI para inspection
To assess whether our parsed CI parameters and judgments are reliable, three authors manually inspect the data quality.
This inspection calculates annotators' agreement with the parsed roles and associated attributes (Role), the transmission principle (TP), and the parsed judgment results (Label).
For Role agreement, we assign an integer from 0 to 3 by considering the sender, receiver and subject.
For TP and Label, we assign a binary agreement score (0 or 1).
To ensure a representative assessment, we randomly sample 30 parsed regulations and cases for each domain.
We then average and re-scale the results under 100\% for consistency, as shown in Table~\ref{tab:human_eval}.

\begin{table}[h]
\small
    \centering
    \begin{tabular}{l l|ccc}
        \toprule
        \textbf{Domain} & 
        \textbf{Type} &
        \textbf{Role} & \textbf{TP} & \textbf{Label} \\
        \midrule

        \multirow{2}{*}{HIPAA} &
        Case & 97.78 & 96.67
 & 100.00 \\
        & Law & 98.89 & 93.33
 & 96.67
 \\
        \midrule
        \multirow{2}{*}{GDPR} &
        Case & 96.67 & 96.67
 & 96.67\\
        & Law & 94.44 & 96.67
 & 93.33
 \\
        \midrule
        \multirow{2}{*}{AI Act} &
        Case & 90.00 & 93.33 & 96.67 \\
        & Law & 98.89 & 96.67
 & 96.67 \\
        
        \bottomrule
    \end{tabular}
    \vspace{-0.1in}
    \caption{Averaged Human agreement with our parsed data. Results are averaged and rescaled under \%.}  
    \vspace{-0.15in}
    \label{tab:human_eval}
\end{table}
%% Case agreement

The manual inspection results indicate that the HIPPA domain achieves the highest agreement scores among parsed cases and regulations.
This can be attributed to the fact that HIPAA is related to the medical domain, where roles and transmitted attributes are more clear and consistent.
For instance, it is frequent to observe a covered entity sharing the patient's protected health information (PHI).
Hence, it is easier to parse CI parameters.
For the EU AI Act, its cases' role has the worst performance, with an agreement score of 0.9.
We further inspect the EU AI Act synthetic cases and find that even though these cases strictly follow the question-answering chains of the compliance checker, they still suffer from narrative incoherence.
We leave the detailed case analyses in Appendix~\ref{app: case}.