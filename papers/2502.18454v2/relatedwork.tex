\section{Related Work}
\label{sec:relwork}

Opdyke and Johnson~\cite{Opdyke-SOOPPA-1990,Opdyke-PHD-1992} introduced the concept of refactoring, while Roberts~\cite{Roberts-PHD-1999} automated basic refactorings. Tokuda and Batory~\cite{Tokuda-ASE-2001} later showed that Opdyke’s preconditions alone were insufficient to guarantee behavior preservation. Sch\"{a}fer et al.~\cite{Schafer-PLPV-2009} further highlighted the challenges of guaranteeing correctness for all language constructs. 
%relacionamento
Unlike these foundational works that focus on formalizing or proving refactorings, our approach employs SLMs to detect errors in existing tool implementations, thus offering an alternative path to ensuring correctness.

AlOmar et al.~\cite{DBLP:journals/infsof/AlOmarMNO21} performed a systematic mapping study on behavior preservation during software refactoring, providing a comprehensive overview of current practices, challenges, and research gaps in the field.
Drienyovszky et al.~\cite{quickcheck-erlang} employs an automated property-based random testing approach to validate Erlang refactoring tools by systematically checking behavior preservation properties.
Daniel et al.~\cite{test-tools-fse07} proposed an approach for automated testing of refactoring engines by detecting issues with overly weak preconditions using a Java program generator and a set of programmatic oracles. 
Soares et al.~\cite{Soares-TSE-2013} used \jdolly{} to generate Java programs and \saferefactor{}~\cite{saferefactor-ieee} to detect behavior changes. They found 106 bugs (compilation errors and behavioral changes) across 39 refactoring implementations. 
%relacionamento
In contrast, our work examines the ability of SLMs to detect these same classes of bugs (\textsc{Type I}), thus extending earlier analyses with a new lightweight approach that does not rely on exhaustive program generation.

Daniel et al.~\cite{daniel-fse-07} and Jagannath et al.~\cite{Jagannath-FASE-2009} explored automated testing of refactoring implementations via program generators (\textsc{ASTGen} and STG) and various oracles, uncovering compilation errors or refactoring engine crashes. Steimann and Thies~\cite{steimann-ecoop} identified flawed refactoring implementations in mainstream Java IDEs, especially regarding accessibility. Gligoric et al.~\cite{Gligoric-ICSE-2010} introduced \textsc{UDITA} to make generator-based testing more expressive. 
%relacionamento
While these approaches require manual setup for program generation, our method leverages SLMs, reducing the need for manual configuration while achieving similar goals of detecting compilation and behavioral errors.

Kim et al.~\cite{Kim-TSE-2014} investigated refactoring practices at Microsoft, revealing that developers often apply refactorings manually, except for the commonly automated Rename refactoring. Their findings underscore the need for more robust automated tools, aligning with our motivation to employ SLMs for more reliable refactoring support. Sch\"afer et al.~\cite{Schafer-OOPSLA-2010} improved \eclipse{} refactoring implementations by formalizing transformations, yet manual and language-specific effort remains a challenge. 
%relacionamento
Conversely, our SLM-based strategy aims to minimize the need for manual intervention. Future improvements could involve refining our prompts by incorporating formalized properties for some refactoring types from their approach.

Bavota et al.~\cite{Bavota-scam-2012} conducted an empirical study linking refactoring activities to bug fixing, detecting 52 types of refactoring operations that induced fixes. Ketkar et al.~\cite{Ketkar-icse-12} presented a technique for learning and performing type changes in Java code, showcasing advanced automation capabilities. 
%relacionamento
These works focus on specific refactoring categories or transformations, whereas our approach targets a broader range of refactorings, using SLMs to generalize across different languages and refactoring types.

Mongiovi et al.~\cite{Mongiovi-TSE-2018} and Soares et al.~\cite{Soares-ICSM-2011} proposed techniques to detect bugs caused by overly strong preconditions in refactoring engines (\textsc{Type II} bugs). One approach relies on generating small Java programs and modifying the refactoring engine code to disable certain preconditions. Similarly, Soares et al.~\cite{Soares-TSE-2013} identified several bugs in \eclipse{}’s refactoring implementations related to \textsc{Type I} bugs. 
%relacionamento
Our work complements these generator-based techniques by introducing a lightweight SLM-based solution that scales to multiple languages (Java and Python) and does not require extensive code instrumentation.

Murphy-Hill et al.~\cite{Murphy-Hill-icse-2008} characterized refactoring problems in mainstream Java IDEs, revealing that poor error communication often leads to slow, conservative, or incorrect refactorings. They developed specialized tools to improve speed, accuracy, and usability, deriving recommendations for better refactoring support. 
Eilertsen and Murphy~\cite{refactoring-usability} introduce a theory -- based on an ISO definition of usability -- that aims to explain and improve the usability of refactoring tools. Through a study with 17 developers completing three refactoring tasks, the authors identified four themes that underscore developers' needs and preferences: more informative feedback from tools and greater control over how these tools operate. The paper refines its usability theory based on these findings, illustrating how a user-centered approach can uncover gaps between what tool designers assume (e.g., prioritizing safety) and what developers actually value (e.g., reduced invocation costs, even if less safe).
% relacionamento
Our work complements existing efforts that focus on evaluating program transformations. Developers can adjust our natural-language prompts to adopt less constrained notions of behavior preservation. Furthermore, it would be valuable to extend the previous approach to evaluate the usability of AI-based IDEs that leverage foundation models, potentially providing new insights into how best to design and integrate automated refactoring capabilities.


Xu et al.~\cite{DBLP:conf/pldi/Xu0NH22} perform a systematic evaluation of LLMs for code. Hou et al.~\cite{se-llms-2023} presented a comprehensive systematic literature review on the application of LLMs in software engineering tasks. By examining 395 studies, their work classifies different models, data handling methods, and evaluation techniques, offering a broad overview of LLM usage in software engineering. Although their review includes multiple software engineering  challenges, it does not focus on refactoring bugs specifically. 
Fan et al.~\cite{DBLP:conf/fose-ws/FanGHLSYZ23} have highlighted the increasing use of LLMs in computer science, and some open problems in the refactoring area. 

Wei et al.~\cite{DBLP:conf/sigsoft/0003X023} have shown how LLMs can assist developers in program repair.
Ahmad et al.~\cite{DBLP:journals/tifs/AhmadTTKP24} show that LLMs have the ability to repair hardware security bugs.
Fan et al.~\cite{DBLP:conf/icse/FanGMRT23} investigate the extent to which Automatic Program Repair techniques can improve the reliability of code generated by LLMs. These results could complement our methodology by helping to correct some of the incorrect responses produced by SLMs in \textsc{Type II} bugs.

Salou et al.~\cite{sallou2024breaking} addressed the risks of using LLMs in SE, emphasizing threats like data leakage, reproducibility issues, and dependency on closed-source models. They propose guidelines to mitigate these risks and underscore the importance of empirical validation. 
Zhang et al.~\cite{zhang2023siren} provided an in-depth survey on hallucinations in LLMs, categorizing them into input-conflicting, context-conflicting, and fact-conflicting. Their taxonomy sheds light on potential pitfalls when generating or evaluating model outputs. 
%relacionamento
We applied metamorphic testing~\cite{metamorphic-testing-2} to mitigate some of the threats.

% gerar programas usando LLMs
Dong et al.~\cite{dong-icse-2025} propose a ChatGPT-based approach for testing refactoring engines. It uses ChatGPT to generate test programs aimed at uncovering defects. Their method builds a feature library extracted from existing bug reports and test cases, defines preconditions for each refactoring type, and uses predefined prompt templates to guide the automatic generation of test programs. These programs are subsequently employed in differential testing across multiple refactoring engines and manually analyzed to identify defects. Evaluating seven distinct refactoring types, the authors discovered a total of 115 \textsc{Type I} bugs in Java implementations.
% relacionamento
In contrast, our approach differs from the previous work by focusing on analyzing the correctness (\textsc{Type I} and \textsc{Type II} bugs) of refactoring transformations rather than refactoring implementations. Our approach can detect behavioral changes, which are typically more challenging to identify. Additionally, our proposed method leverages automation more extensively and provides initial evidence supporting its applicability not only in Java, but also in Python and C, thereby indicating its broader versatility across different programming languages.

% Transformações para Python
Dilhara et al.~\cite{DBLP:journals/pacmse/DilharaBBD24} propose PyCraft that integrates static and dynamic code analysis with LLM capabilities to refine code transformations for Python. PyCraft generates diverse code variations and corresponding test cases. To validate its effectiveness, the authors submitted 86 transformed code instances through 44 pull requests to projects with an acceptance rate of 83\%. These results highlight the potential of combining LLMs with analytical techniques for automated code transformation. 
% relacionamento
Our approach can be employed to assess the correctness of the transformations proposed in their work, and detect \textsc{Type II} bugs.

% bug dataset
Wang et al.~\cite{wang2024empiricalstudyrefactoringengine} conducted a manual analysis of 518 bugs across three popular refactoring engines, identifying key root causes, bug symptoms, and input program characteristics. Their study resulted in some findings, which were used to formulate a set of guidelines for refactoring engine bug detection and debugging. Additionally, their transferability study uncovered 130 new and unique bugs in the latest versions of refactoring tools, highlighting the pervasiveness of refactoring-related defects.
% relacionamento
In contrast, our work evaluates a technique for identifying refactoring transformation bugs using SLMs. We assess our approach on \bugs{} reported bugs from widely used refactoring engines, some of which overlap with the dataset studied before. As future work, we aim to expand our evaluation by incorporating additional bugs from their dataset to further validate the effectiveness of our approach.

% template de prompts
White et al.~\cite{white2023chatgptpromptpatternsimproving} propose two prompt templates (Pseudo-code Refactoring Pattern and The Data-guided Refactoring Pattern) for applying refactorings using ChatGPT.
% como usam GPT
AlOmar et al.~\cite{DBLP:conf/msr/AlOmarVMNO24} investigate how developers utilize ChatGPT for refactoring by analyzing developer-AI conversations. Through text mining techniques, the authors examine explicit refactoring intentions and the way ChatGPT responds to developers' needs. Their findings indicate that developers often make generic refactoring requests, while ChatGPT tends to infer and include specific refactoring intentions in its responses. 

% refactorings para Python
Shirafuji et al.~\cite{DBLP:conf/apsec/ShirafujiOSMW23} propose a methodology for leveraging GPT-3.5 to suggest less complex versions of user-written Python programs through few-shot prompting with carefully selected examples. The quantitative evaluation indicates that the LLM successfully produces correct and simplified versions for 95.68\% of input programs, reducing cyclomatic complexity by 17.35\% and the number of lines of code by 25.84\%. Additionally, qualitative analysis reveals improvements in code readability through variable renaming and formatting but also highlights cases where the LLM introduces unnecessary modifications or removes comments. The findings demonstrate the potential of LLMs in code refactoring while also identifying their limitations, paving the way for future research on refining automated complexity reduction techniques.

% Extract Method Refactooring com LLM
Pomian et al.~\cite{DBLP:conf/sigsoft/PomianBDKBSBD24} propose an IntelliJ IDEA plugin called EM-Assist that generates, validates, enhances, and ranks Extract Method refactoring suggestions using LLMs. Unlike traditional static analysis-based refactoring tools, EM-Assist leverages AI to provide more context-aware recommendations. In an evaluation of 1,752 real-world refactorings from open-source projects, EM-Assist achieved a recall rate of 53.4\% among its top-5 recommendations. 
Depalma et al.~\cite{DBLP:journals/eswa/DepalmaMHMA24} present insights into the effectiveness and challenges of using ChatGPT for refactoring, highlighting both its strengths and limitations. Additionally, the study explores the ethical implications of AI-driven code transformations, addressing concerns related to biases, privacy, and potential risks in adopting these tools. Beyond evaluation, the work also offers practical recommendations for improving developer-AI collaboration in refactoring tasks. 
% relacionamento
In contrast, our work focuses on assessing the correctness and reliability of AI-generated refactorings, emphasizing their impact on code behavior and transformation quality rather than on conversational patterns and ethical concerns.

Zhang et al.~\cite{ZHANG2025121753} apply deep learning and LLM-generated information to recommend Move Method refactorings. They constructs a dataset from 58 real-world projects, extracting metric features through static analysis, textual features using LLMs for code summarization, and semantic features by computing similarity between original and target classes. The dataset contains 12,475 samples and is used to train a CNN-LSTM-GRU model for refactoring recommendation. Experimental results show that their tool achieves an average F1-score of 74\%, outperforming existing tools such as PathMove, JDeodorant, JMove, and RMove, with improvements ranging from 9.4\% to 53.4\%. These results highlight the potential of combining deep learning with LLM-generated insights to enhance refactoring automation. 
Liu et al.~\cite{DBLP:conf/issta/LiuWWXWLJ23} introduce RefBERT, a two-stage pre-trained framework for automatic variable renaming based on the BERT architecture. In the first stage, RefBERT predicts the number of sub-tokens required for the new name; in the second, it generates the sub-tokens accordingly. The framework integrates several advanced techniques to effectively address the unique challenges of this refactoring task. Experimental evaluations on constructed refactoring datasets reveal that RefBERT surpasses existing methods, producing variable names that are both accurate and meaningful.
% relacionamento
Our work focuses on ensuring the correctness of any type of refactorings using SLMs.

Cordeiro et al.~\cite{cordeiro2024empiricalstudycoderefactoring} present an empirical study comparing \emph{StarCoder2} -- an LLM tailored for code generation -- to human developers in code refactoring tasks. The authors analyze 30 open-source Java projects, evaluating the ability of StarCoder2 and developers to reduce various types of code smells. Their results show that StarCoder2 achieves a higher overall reduction rate of code smells than developers, particularly excelling at systematic, repetitive refactorings. In contrast, human developers outperform the model on complex, context-dependent design issues.
% relacionamento
Our work complements the above by focusing on ensuring the correctness of program transformations using SLMs.