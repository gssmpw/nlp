This section provides the preliminaries of the RAG framework for LLMs. We will introduce the concept of RAG and the common downstream tasks. We acknowledge the wide range of applications of RAG in domains other than LLMs (e.g., Image Generation), but this survey limits the scope to the applications of RAG in LLMs, sometimes referred to as Retrieval Augmented Language Models (RA-LLMs)~\cite{fan2024survey}. As a simplification of the terminology, in the rest of this survey, we use RAG, RA-LLM, and RAG-based systems interchangeably. 

\subsection{Retrieval Augmented Generation}
As illustrated in Figure \ref{fig:overview}, a typical RAG framework consists of three stages: information retrieval, knowledge augmentation, and content generation. Given a query, the retrieval process aims to provide relevant information and context to facilitate the reasoning of the query. Following the classification of previous work~\cite{Gao2023Retrieval}, the retrieval process contains two stages - \textit{indexing} and \textit{retrieving}. The \textit{indexing} stage takes inputs from a diverse range of formats (PDFs, HTML, words, Markdowns, etc.) and converts them into chunks of data. Subsequently, the chunks of data are converted into vector representations and stored in a vector database for access during inference. However, it is worth noting that for some retrieval-augmented tasks, such as those involving knowledge graphs (e.g., GraphRAG frameworks), the external databases may not rely on vector databases. Instead, they often use symbolic knowledge structures or relational data to store and retrieve information, bypassing the need for vectorization. The \textit{retrieval} stage takes the incoming query, converts it into vector representation (or aligns it with the symbolic knowledge structures in non-vector cases), and then fetches the matching chunks or nodes from the database. \textit{Knowledge augmentation} involves integrating the retrieved knowledge into the underlying generation models. Often it is achieved through injecting the knowledge into the prompt or finetuning the language model on the retrieved knowledge. Finally, the \textit{generation} stage utilizes the augmented knowledge to produce coherent and contextually accurate responses based on the query.  

Compared to the direct generation by LLMs without any contextual information retrieved externally, the additional consideration of external knowledge in RAG paradigm leads to new challenges in trustworthiness. Each stage - retrieval, augmentation, and generation - introduces unique trustworthy challenges, such as retrieval bias, hallucination, and the injection of irrelevant information, all of which require careful mitigation to ensure a trusted response. These concerns will be further explored in subsequent sections of this survey.

\subsection{Tasks and Evaluations}
\subsubsection{Tasks}
\label{sec:eval}
The RAG paradigm has enabled multiple applications in the natural language processing (NLP) domains. The following briefly introduces some of the common tasks including question answering and chatbots, while also introducing some of the associated and commonly used datasets.

\paragraph{Question Answering.}  
One of the primary downstream tasks for RAG-based language models is question
answering (QA), which includes various sub-tasks such as long-form question
answering, multi-hop question answering, domain-specific question answering, and
open-domain question answering. In these tasks, the system is given a user query
and aims to generate the most relevant and accurate answer. The RAG paradigm assists these tasks by integrating the relevant external context into the generation.

Different QA sub-tasks might require different evaluation paradigms. For example, for multi-hop and multiple choice question answering, since the answer is relatively structured or limited to a pre-defined set of answers, the performance is often evaluated through metrics such as hits@$n$, where $n$ represents the rank of the correct answer in the list of retrieved candidates, and F1, where precision and recall are balanced to assess both the correctness of the generated answers and the system's ability to retrieve all relevant answers~\cite{luo2024rog, sun2023thinkongraph}. On the other hand, for open domain question answering where answers are more unstructured, Exact Match (EM) and Lexical Match are commonly used~\cite{zhang-etal-2023-survey-efficient}. These metrics both provide a good measure of quality of the generated answers. It is also worth mentioning that for some works that emphasize the retrieval process also report results that measures the quality of the retrieval. Commonly used QA datasets include the following:

\begin{itemize}
    \item \textbf{MMLU}~\cite{hendrycks2021measuringmassivemultitasklanguage}: A commonly used dataset for multiple-choice question answering (MCQA). It contains MCQA questions from 57 domains, including STEM (science, technology, engineering, and math), humanities, and medicine. In their experiments, existing uncertainty quantification research chooses to use a subset of the dataset for evaluation~\cite{ye2024benchmarkingllmsuncertaintyquantification, kumar2023conformalpredictionlargelanguage}.
    
    \item \textbf{TriviaQA}~\cite{joshi2017triviaqalargescaledistantly}: A widely-used large-scale dataset for open-domain question answering, designed to test models on questions from Wikipedia and web search engines. TriviaQA includes question-answer pairs along with evidence documents for context, making it suitable for testing reading comprehension and retrieval-based models. It is used by most open-domain question answering models~\cite{su2024apienoughconformalprediction, quach2024conformallanguagemodeling, li2023traq}.

    \item \textbf{WebQSP}~\cite{yih2016value}: A popular dataset for multi-hop knowledge base question answering, focusing on the task of answering questions by traversing multiple entities and relations within a knowledge graph. WebQSP provides questions labeled with their corresponding semantic parses, enabling models to learn complex query structures for effective knowledge graph traversal and reasoning~\cite{ni2024trustworthyknowledgegraphreasoning, luo2024rog, sun2023thinkongraph}. 
\end{itemize}


\paragraph{Chatbots.} Another common application for RAG-based language models
is Chatbots. Chatbots are designed to handle an array of dialogue types, ranging
from task-oriented interactions to open-domain conversations. The RAG paradigm
can enhance Chatbots performance by integrating external knowledge into the
conversation, allowing the
system to access up-to-date information that would otherwise be out of its
parameters trained from outdated text corpus~\cite{akkiraju2024factsbuildingretrievalaugmented, sumida2024ragchatbotsforgetunimportant, feldman2024raggededgesdoubleedgedsword}. Contemporary knowledge will be especially important
for those Chatbots operating in dynamic or domain-specific environments, such as
medical and financial dialogue systems.

When evaluating the performance of a chatbot, two primary goals are considered. The first goal is to assess the quality and coherence of the dialogue. To achieve this, various metrics have been proposed, including those that measure utility~\cite{Cameron2019}, response understanding~\cite{Yokotani2018}, and overall aesthetics~\cite{Wargnier2018}. Additionally, some metrics evaluate the similarity between the generated responses and human responses, as seen in works such as~\cite{Adiwardana2020, Xu2022}. While these metrics are generally effective for open-domain dialogues, they often fail to account for specific use cases. Thus, the second goal focuses on evaluating the chatbot's effectiveness in meeting users' needs, particularly in real-world, business-oriented use cases. In these scenarios, beyond the aforementioned dialogue quality metrics, practical effectiveness is a key consideration. General natural language generation (NLG) metrics, such as BLEU~\cite{10.3115/1073083.1073135} and ROUGE~\cite{lin2004rouge}, are frequently employed to measure this aspect.

There is currently no standardized holistic evaluation of chatbots. For large commercial models, the performance of the chatbots is evaluated on the subtasks such as code generation, problem solving, and complex reasoning. On the other hand, for the business-oriented, task specific models, they are evaluated on the corresponding task-specific or industry-specific datasets. We introduce some of the commonly used datasets below. 

\begin{itemize}
    \item \textbf{HellaSwag}~\cite{zellers2019hellaswag}: A commonly used dataset for commonsense reasoning and story completion tasks. HellaSwag presents models with scenarios requiring contextually appropriate completions, testing their ability to reason beyond surface-level semantics. It has been widely adopted for benchmarking commonsense reasoning capabilities in large language models~\cite{openai2024gpt4technicalreport, brown2020languagemodelsfewshotlearners}.
    
    \item \textbf{HumanEval}~\cite{chen2021evaluatinglargelanguagemodels}: A widely used dataset for evaluating code generation capabilities of language models. HumanEval includes programming problems of varying difficulty levels, along with unit tests to validate the correctness of generated code. It is a standard benchmark for assessing the coding performance of generative models~\cite{openai2024gpt4technicalreport}.
    
    \item \textbf{MedicationQA}~\cite{benabacha2019bridging}: A popular dataset for question answering in the medical domain, focusing on patient-generated questions about medication. It includes complex medical queries paired with evidence-based answers, making it a crucial benchmark for evaluating the applicability of language models in healthcare and patient communication~\cite{openai2024gpt4technicalreport, lee2023gpt4medicine}.
\end{itemize}

\paragraph{Others.}
Beyond language-based tasks, RAG-based language models can be applied to a diverse range of downstream tasks, including recommendation systems~\cite{li2023gpt4recgenerativeframeworkpersonalized}, software engineering~\cite{fan2023largelanguagemodelssoftware}, and AI for scientific discovery~\cite{ai4science2023impactlargelanguagemodels}. However, these applications are often overlooked in discussions of trustworthy RAG frameworks. Recognizing their importance, we highlight the need for further exploration of trustworthiness in these domains and propose to address them in future directions.

\paragraph{Trustworthy Evaluation.} To comprehensively assess trustworthiness, additional metrics are necessary, including those that evaluate bias, fairness, and reliability in the generated answers. As AI trust evaluation has well been debated across cognitive, communication, information, and social dimensions, the focus was on the use of the result. To maintain trust, a trust worthy evaluation is also needed to determine the efficacy under changing conditions. These metrics will be designed to ensure that the system aligns well with trustworthy aspects. Due to their heterogeneous nature, we will delve into these specific metrics in detail in the corresponding sections throughout the rest of the survey.


\subsection{Motivation}

Although trustworthiness in deep learning and LLMs has been well-explored in the general AI community, it is rather critical to consider the trustworthiness for RAG-based LLMs because (a) their growing usage in real-world applications often involves high-stakes decision-making, where errors or biases can lead to significant consequences; (b) RAG models are vulnerable to trustworthiness issues as the multiple stages in RAG—such as indexing, retrieval, and generation—can introduce compounding errors, biases, or hallucinations, which can be difficult to trace and mitigate; and (c) RAG-based LLMs are inherently different from traditional standalone LLMs as they rely on external data sources during the retrieval process, leading to a dynamic interaction between the model and potentially unreliable or biased external information. Thus, directly extending the trustworthiness framework for standalone LLMs to RAG-based LLMs is challenging due to evolving complexities. Therefore, developing a comprehensive and robust trustworthiness framework specifically tailored for RAG-based LLMs is imperative to ensure reliable and safe deployment in various domains. 

While prior work~\cite{privacy_rag_2024, xue2024badrag, robust_rag_2024, doshi2017accountability, zhou2024trustworthiness} has touched on these issues individually, there remains a gap in the literature for a unified survey that reviews the current advancements and challenges in ensuring trustworthiness for RAG-based LLMs. This survey aims to be a comprehensive review by summarizing existing efforts, categorizing approaches, and identifying opportunities for future research. By providing a systematic review, we hope to facilitate further research and development of trustworthy RAG-based LLMs across diverse domains.

\input{tables/survey_taxonomy}

\subsubsection{Related Surveys and Differences}
As shown in Table \ref{survey-taxonomy}, recently, several surveys have been conducted on RAG~\cite{Gao2023Retrieval, fan2024survey} and trustworthy LLMs~\cite{trustworthy_llms_2024, huang2023surveysafetytrustworthinesslarge}. On one hand, although the RAG surveys provide a comprehensive overview of the state-of-the-art models and architectures, detailing some of the methods' efforts to address challenges in the realm of trustworthiness (e.g., robustness), few of them provide a comprehensive and focused discussion on the specific challenges and solutions related to trustworthiness across the entire RAG pipeline, particularly in areas such as reliability, fairness, privacy, and safety~\cite{Gao2023Retrieval, fan2024survey}. On the other hand, the surveys on LLM trustworthiness focus mostly on the generation aspects of LLMs, such as mitigating hallucinations or enhancing explainability, and cannot be directly applied to RAG models due to the added complexity introduced by the retrieval and augmentation stages~\cite{trustworthy_llms_2024}.

Most recently, there is a survey dedicated to trustworthiness in RAG systems~\cite{zhou2024trustworthiness}. Although the survey provides reviews on existing works, most of its content focuses on experiments related to trustworthy generation across different language models within a basic retrieval-augmented generation setup. We acknowledge the importance of such empirical studies in advancing the field but would like to emphasize the different focus of our survey. Rather than empirical studies, our survey centers on providing a more detailed and comprehensive literature review. We systematically categorize trustworthiness challenges and solutions across the RAG applications. By focusing on these broader aspects, our work aims to establish a unifying framework to guide future research and development in trustworthy RAG systems.

Nonetheless, all of the existing surveys have acknowledged the importance of trustworthiness and included it as a critical area for future research directions~\cite{Gao2023Retrieval, fan2024survey}, demonstrating the aligned interests in the community. 

\subsection{Paper Collection}
To construct a comprehensive survey of trustworthy RAG systems in LLMs, we follow a systematic
literature review methodology. We began by identifying relevant papers through
keyword searches across major academic databases, including Google Scholar, ACM
Digital Library, and arXiv, using general terms such as "Retrieval-Augmented
Generation," "trustworthiness," as well as specific terms for the different
aspects. Papers were included if they directly discussed the trustworthiness
aspects of RAG systems, such as reliability, privacy, safety, fairness, and accountability.
Because of the modular nature of RAG-based systems where external databases are
coupled with a language model, we also take works that discuss each into
consideration. After an initial screening, we categorize the papers into each
aspect of trustworthiness, situating them into corresponding taxonomies. Our
final collection of papers, as discussed in this survey, reflects a diverse
range of perspectives on RAG and trustworthiness. The cut off date for the papers to be included in this survey is October 2024. 

\subsection{Notes on the Organization of the Survey}
As outlined in the previous sections, the remainder of this survey is structured around the six key aspects of trustworthiness: Reliability, Privacy, Safety, Fairness, Explainability, and Accountability. Given the distinct nature of each aspect, every section will follow its own
taxonomy, introducing relevant works accordingly. To further emphasize these
differences, we include specific future directions and evaluation protocols
within each section. A general discussion of the future directions for
trustworthy Retrieval-Augmented Generation (RAG) systems will follow at the
conclusion, providing a broader discussion that encompasses and integrates the
section-specific insights.