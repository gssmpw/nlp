Explainability, as a critical perspective of trustworthiness, has attracted significant attention due to its capability to elucidate the behavior of machine learning models and uncover novel data-driven insights that can even inspire domain experts~\cite{selvaraju2020grad, robnik2018perturbation, ribeiro2016should, liu2023fair, feng2023degree, ying2019gnnexplainer}. This demand for explainability has been further intensified with the rise of black-box LLMs~\cite{liang2022holistic, sudhi2024rag}. We acknowledge the existing survey~\cite{zhou2024trustworthiness} on trustworthiness, which includes a section aimed at uncovering the decision-making process of RAG systems called transparency. However, the concept of transparency in that context differs from the explainability we focus on in our work. While transparency is more general and seeks to understand the algorithms and underlying rationales, our explainability (of the output) specifically aims to elucidate why a particular input (transparency) leads to a given output through a specific model (interpretability).

In the context of RAG, explainability becomes even more crucial due to the inherent complexity of its multi-component architecture. Beyond explaining the generation process of LLMs, it is also important to understand why the retrieval process prefers specific contexts over others. For example, what words in the input questions lead to the retrieval of a particular sentence? As some of the RAG systems include post-retrieval~\cite{glass2022re2g,kim2024sure,yang2023prca}, it is also important to understand why the retrieved content needs to be post-processed in a specific way to augment the downstream generation. For example, what features of the input result in such ranking/importance scores when performing reranking after retrieval? Given the inherently multi-stage nature of RAG, this section reviews the literature on explaining the two crucial stages of RAG, retrieval and generation process, respectively.

\input{tables/explain_taxonomy}

\subsection{Taxonomy of RAG Explainability}


\subsubsection{Explainability in Retrieval}
As illustrated in Table \ref{exp-taxonomy}, we situate our discussion of explainability in RAG in the distinction between retrieval, generation, and dual enhancement. To the best of our knowledge, no dedicated research efforts have been made to explain retrieval within the context of RAG. However, several studies have indeed explored explainability in the general information retrieval, particularly in recommender systems and search~\cite{zhang2020explainable,zhang201919,zhang2018sigir}. Therefore, we provide a high-level summarization of representative explanation techniques in information retrieval, with the expectation of inspiring similar success in explaining the retriever of RAG.

Based on~\cite{anand2023explainable}, the explanation methods in information retrieval can be categorized into post-hoc explanation, axiomatic strategies, probing strategies, and self-interpretable designs. The post-hoc explainers explain the models after they make decisions, the representative examples of which used in information retrieval are feature attribution and generative approach. The feature attribution works by ascribing the retrieved outcomes to certain input (i.e., the attribution). Some of the methods find the explanation features by computing the feature importance, such as~\cite{qiao2019understanding} uses interpretable textual features to explain rankings,~\cite{polley2022towards} understands the BERT-based ranking models by the attention scores of tokens, and~\cite{verma2019lirme} estimates the point-wise explanations by analyzing the contribution of each token to the output of the ranking model. Other methods try to explain the model outputs by finding the most explanatory features, e.g.~\cite{singh2021extracting} uses a greedy search-based algorithm to obtain a subset of features that serve as the explanations. Axiomatic explainers provide explanations using axioms~\cite{volske2021towards} and probing explainers provide valuable insights into the inner workings of neural models by revealing what types of information are encoded in their embeddings and model parameters~\cite{cohen2018understanding}, how sensitive they are to various textual properties~\cite{macavaney2022abnirml}, and what knowledge they possess~\cite{choi2022finding,formal2021white}. Although self-interpretable explainers inherently provide explanations by their designs, making the model fully transparent is extremely challenging, and usually, only specific components are interpretable and transparent~\cite{formal2021white,zhang2021explain}. 

\subsubsection{Explainability in Generation}
In addition to explaining the retriever, explaining the generator is equivalently or even more important. First, the output directly stems from this generation process, and any shortcuts or reliance on misleading features can lead to significant generation errors~\cite{hong2023so, deng2024cram}. Second, modern RAG systems typically rely on LLM-based generators, which are prone to hallucinations. This raises concerns about whether the generated output truly focuses on the query-relevant words and the retrieved contents, which can be naturally addressed via explanation~\cite{schneider2024explainable, sudhi2024rag}. There are two main categories of explanation methods in the generation process of RAG systems, ante-hoc explanation methods and post-hoc explanation methods, respectively. 

RAG-Ex~\cite{sudhi2024rag}, as a post-hoc explanation method, introduces a model and language-agnostic framework inspired by the philosophy of perturbation-based explanation. The core idea is to identify critical tokens or features such that removing them would significantly alter the output of the generator. Specifically, RAG-Ex proposes six perturbation methods: leave-one-token-out, random noise, entity manipulation, antonym injection, synonym injection, and order manipulation. After prompting LLMs with perturbed inputs (using the same parameters), the similarity between the generated responses and the original response is measured, which can be further used to calculate the importance scores for the perturbed tokens. After obtaining the importance score for each token, top-K important tokens are selected as the explanation, and they are referred to with the sentence containing the ground-truth answer or question to further assess the explanation quality. Through experiments, RAG-Ex finds that the "leave-one-token-out" perturbation performs the best in terms of accurately identifying the most critical tokens for the explanation. Aiming to trace the origin of LLM answers within the context of RAG, ~\citet{rorseth2024rage} proposed a post-hoc explainer called RAGE, which deduces the provenance and salience of external knowledge used during RAG. This framework is designed to generate counterfactual explanations for LLM answers by employing different combinations and perturbations of external knowledge sources. To enhance efficiency, it incorporates pruning strategies that reduce the search space for counterfactual explanations. Through three challenging use cases, RAGE demonstrates its effectiveness in explaining why LLMs produce specific answers, thereby improving their transparency and interpretability for users.


Another line of work provides ante-hoc explanations to either understand the decision-making process of the generator or enhance the generation performance after incorporating them into the model forward process. In the research of~\cite{luo2024rog}, a novel method called reasoning on graphs (RoG) is proposed to perform such an ante-hoc explanation. The presented framework utilizes knowledge graph relations to generate reasoning paths for the given question, making the reasoning paths interpretable and faithful. Then, an LLM is employed to generate the answer by conducting reasoning on the paths, which is intuitive to understand the decision-making process of the LLM. Through this framework, the generation process can be interpretable and traceable, not only enhancing the explainability in generation but also allowing the RAG system to generate more accurate answers.

\subsection{Dual Enhancement of Explanations and RAG}
In addition to exploring how to explain RAG systems, existing work also investigated the dual enhancement of explanations and RAG. On the one hand, explanations can be integrated to augment RAG systems. On the other hand, RAG systems can also be employed to provide explanations.

As information retrieval plays a critical role in RAG~\cite{cuconasu2024power}, the enhancement of adding explanations during retrieval could also benefit the whole RAG system. The ExaRanker method~\cite{ferraretto2023exaranker} utilizes the explanations as additional labels to train the ranking models in the information retrieval task. Specifically, given the question-passage pair and the label indicating whether the passage can be used to answer the question, an LLM is first employed to generate the explanations of why the question can/cannot be answered by the given passage. With these ground-truth explanations, the ranking model is trained on question-passage pairs to predict not only whether the question can be answered but also the corresponding explanations. By integrating explanations as additional training labels, the ranking model can better understand the relationships between the questions and passages, which could benefit the ranking performance and ease the demand for a large number of training examples. 

Apart from leveraging explanations to augment RAG systems, the reverse relationship—using RAG to improve explanations—is also worth exploring. For example,~\citet{tekkesinoglu2024feature} use RAG in scene-understanding tasks to create explanations through a question-answering approach. For each input with a class label, the model predicts the probability of belonging to that class. To assess the impact of each semantic feature, it generates predictions without specific features, enabling the calculation of feature importance. These outputs, along with features and contrastive cases, contribute to an external knowledge repository for LLMs. This RAG design can thus generate human-friendly and faithful explanations for the prediction model of the scene-understanding tasks. Another work~\cite{hussien2024rag} studying road user behavior also uses RAG to generate explanations. In particular, this work first creates a human-readable document that explains why the road user may/may not have a specific behavior. The document is then processed to form a database, serving as the external knowledge base of the RAG system. Given a tailored prompt and a query derived from the prediction frame, the RAG system will create a detailed explanation of the road user's intention.

\subsection{Explainability Evaluation}

\paragraph{Metrics.} 
As very few works investigate explainability in the context of RAG, we first review the conventional explanation metrics used in explainable artificial intelligence (XAI). \textit{Fidelity} is one commonly used evaluation metric. It measures to which extent the explanation can accurately reflect the decision-making process of the model~\cite{alangari2023exploring}. Mathematically, fidelity is often defined as the proportion of data samples where the predictive model and the explanation produce the same decision, but there are some variations on computing fidelity, such as using Kullback-Leibler divergence between outputs, conditional entropy, and correlation~\cite{nauta2023anecdotal}. Another metric often used is \textit{stability}~\cite{ghorbani2019interpretation,li2020evaluating,plumb2020regularizing}. It measures the consistency of a method in producing similar explanations for similar or closely related inputs~\cite{vilone2021notions}.

To evaluate the explanations of the generator in a RAG framework, \citet{sudhi2024rag} use two key metrics: \textit{significance} and plausibility. In their framework, significance measures whether the explanations capture the core information present in the input. This is quantified using the F1-score and Mean Reciprocal Rank (MRR). On the other hand, \textit{plausibility} is assessed through human evaluation. Annotators identify specific tokens from the input as ground-truth explanations, and the generated explanations are then evaluated against these selected tokens using the F1-score.


\paragraph{Datasets.}
Although the explainability of RAG enhances user trust and transparency in the generated outputs, evaluating the explanations remain challenging due to the lack of standardized datasets specifically designed for this purpose. Currently, researchers often adapt existing QA datasets to test explainability methods. For example, \citet{sudhi2024rag} utilized randomly sampled English and German QA pairs from the validation split of the XQUAD dataset to evaluate their proposed explainer. Similarly, \citet{luo2024rog} demonstrated the self-explanatory capabilities of their RoG method by automatically generating explanations while performing question-answering tasks on two benchmark knowledge graph question-answering (KGQA) datasets: WebQuestionsSP (WebQSP)~\cite{yih2016value} and Complex WebQuestions (CWQ)~\cite{talmor2018web}. These efforts illustrate that, in the absence of standardized benchmarks, evaluation datasets are specific to different tasks and domains, and researchers rely on existing datasets. The development of dedicated datasets for explainability in RAG remains a critical area for future research, offering the potential to advance systematic evaluation and comparison of explainability methods.




\subsection{Future Directions of RAG Explainability}
\paragraph{Integration of Knowledge Graphs and LLMs.} Our work pioneers the integration of knowledge graphs (KGs) and large language models (LLMs) to enhance retrieval faithfulness in Retrieval-Augmented Generation (RAG) systems~\cite{ni2024trustworthyknowledgegraphreasoning}. Unlike existing approaches that rely solely on GNN-based retrieval methods or LLM-based prompting, our hybrid approach leverages the strengths of both. This integration opens opportunities for more robust and versatile solutions. Beyond question-answering tasks, we propose extending retrieval capabilities to semi-structured knowledge bases, providing richer contexts that enhance text generation and personalization. Future research should focus on optimizing the interaction between these components to improve retrieval accuracy and context integration. Additionally, optimizing the structure and accessibility of the knowledge base is essential for achieving better performance in diverse applications.

\paragraph{Explaining Multi-Component RAG Systems.} RAG systems are inherently complex, with multiple interacting components, e.g., retrievers and generators. Explaining the behaviors of these components introduces unique challenges, especially when they are jointly trained, as seen in approaches by \cite{fan2024survey} and \cite{lewis2020retrieval}. In such cases, the retriever and generator often share embeddings or feature representations, making it difficult to disentangle their individual contributions. The advent of novel retrievers, such as those using LLMs as agents for sequential graph traversal \cite{jin2024graph, wang2024knowledge}, further complicates explainability. Traditional differentiable-based explanation methods are often impractical for such systems. Future work should prioritize developing explainability methods that provide insights into both individual components and their interactions within the system. This could include frameworks that combine the outputs of retrievers and generators to analyze their joint contributions to system behavior.

\paragraph{Performance vs. Explainability Trade-Off.} An ongoing debate in explainable AI concerns the trade-off between model accuracy and explainability. Some studies \cite{crook2023revisiting, arrieta2020explainable} argue that optimizing for one often compromises the other, while others contest this notion \cite{bell2022notthatsimple, rudin2022interpretable}, citing a lack of conclusive evidence. In the context of RAG systems, replacing components like retrievers or generators with fully explainable counterparts offers a promising area for exploration. Researchers could investigate the performance implications of such replacements, analyzing whether transparency and interpretability can coexist with high performance. This line of inquiry could yield valuable insights into the relationship between explainability and system effectiveness, particularly when compared to black-box LLM-based approaches.

\paragraph{Evaluation Metrics for Explainability.} Evaluating the quality of explanations in RAG systems is a challenging yet essential area of research. Existing metrics like fidelity and stability are useful but often fail to capture the subjective, context-dependent nature of explainability. Future work should focus on developing domain-specific metrics tailored to user needs and application contexts. For instance, RAG-Ex introduces metrics such as significance and plausibility. Significance evaluates whether explanations align with predefined information, while plausibility assesses whether they reflect annotators' choices. Expanding such metrics to encompass diverse domains and user preferences will provide a more nuanced understanding of explanation quality and its impact on system usability.

\paragraph{Propagation of Misinformation.} The propagation of misinformation remains a critical concern in RAG systems, particularly in scenarios involving multi-step reasoning or complex retrieval processes. Explainability methods that trace the flow of information through RAG components could help identify and mitigate sources of misinformation. For example, analyzing how erroneous retrievals influence generated outputs could inform the design of more robust systems. Future research should investigate how explainability frameworks can incorporate misinformation detection and prevention mechanisms, ensuring that generated responses are not only accurate but also trustworthy.