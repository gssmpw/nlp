Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
address the challenges faced by Large Language Models (LLMs), such as
hallucinations, reliance on outdated knowledge, and the lack of explainability~\cite{Gao2023Retrieval, Zhang2023Sirens}. By incorporating external information into the generation context, RAG improves the accuracy and reliability of the generated content. The recency of information also enables the model to stay current with minimal training costs by reducing the need for extensive retraining of the entire system to update its parameters. These benefits have profound implications for
real-world applications. For example, RAG has been effectively applied in
medical question answering~\cite{xiong2024benchmarkingretrievalaugmentedgenerationmedicine, Zakka2023Almanac, singhal2023expertlevelmedicalquestionanswering}, legal
document drafting~\cite{Wiratunga2023CBR, pipitone2024legalbenchragbenchmarkretrievalaugmentedgeneration}, educational
chatbots~\cite{Thway2023Battling}, and financial report summarization~\cite{yepes2024financialreportchunkingeffective} due to their adaptability in various domains. 

The definition of trustworthiness often depends on the context of discussion~\cite{trustworthy_ai_computational_perspective_2021, survey_trustworthy_ai_meta_decision_2023, trustworthy_ai_acm_2023, global_study_trust_ai_2023, trustworthy_gnn_2023, liu2024machine, trustworthy_graph_neural_networks_2024, trustworthy_llms_2024}. In the context of machine learning and artificial intelligence, trustworthy AI must exhibit characteristics that make the system \textit{worthy of trust}. In 2022, the National Institute of Standards and Technology (NIST) published guidelines for trustworthy AI, defining trustworthiness from several perspectives~\cite{nist_trustworthy_ai}: Reliability, Privacy, Explainability, Fairness, Accountability, and Safety. 

\textbf{Reliability} ensures that the system consistently performs as expected and produces accurate results under various conditions. It includes addressing challenges such as uncertainty quantification and robust generalization, which are critical for enhancing system dependability. For instance, in a legal case analysis system, reliability involves balancing uncertainty quantification (e.g., confidence scores for retrieved legal citations and the number of retrieved legal citations) and robust generalization (e.g., applying precedents to new cases) to ensure lawyers are not misled during case preparation.

\textbf{Privacy} focuses on safeguarding user data, ensuring control over personal information. Since RAG has been applied to sensitive domains like the medical field, protecting patient information is important. For example, when a healthcare assistant retrieves medical records or generates treatment suggestions, the system must prevent data breaches and ensure sensitive patient details embedded in the language model remain secure. 

\textbf{Explainability} emphasizes the need for transparent decision-making processes, enabling users to understand how outputs are generated. For example, a university admissions assistant powered by RAG should offer clear explanations of how student profiles are matched with program requirements, providing insights that users can readily understand and verify.

\textbf{Fairness} focuses on minimizing biases introduced during both retrieval and generation stages, as these biases can significantly affect outcomes in high-stakes domains. Recent advancements include the use of re-ranking methods to mitigate societal biases in retrieval and fine-tuning techniques to balance demographic fairness with system performance. %Similarly,
For example, the admissions assistant must ensure fair treatment of applicants by addressing potential biases, such as those related to gender, race, or socioeconomic status.

\textbf{Accountability} pertains to the governance of AI, including policymaking and law enactment, but also extends to technical aspects such as tracing the origins and processes behind AI-generated content. For example, ensuring that a news-generating system can trace its retrieved sources to improve content accountability and reduce misinformation is critical. Techniques like content watermarking help identify the provenance of retrieved information and the generation process, providing a clear audit trail for future verification.

\textbf{Safety} addresses the system's capacity to prevent and mitigate harm, with a particular focus on defending against adversarial attacks and reducing risks from malicious actors. Current chatbot systems often interact with high-risk users, such as teenagers, who may unknowingly be exposed to harmful or inappropriate content. Adversarial attacks and jailbreaking attempts that alter the chatbot's behavior could lead to misinformation, inappropriate responses, or even dangerous suggestions. Thus, building robust safeguards, such as adversarial training and ethical guardrails, is crucial for ensuring safety and preventing harm in such interactions.

Despite their recent success, concerns about the trustworthiness of RAG-based
systems have become an increasing subject of debate. First, RAG systems are susceptible to reliability
issues since developers must ensure the output is accurately grounded on the retrieved content~\cite{li2023traq, Gao2023Retrieval}. Second, the reliance on an external database introduces a new attack surface, exposing the systems to a range of adversarial threats~\cite{dual_rag_2024, adaptive_adversarial_rag_2024,
robust_rag_2024, robust_rag_iclr_2024, misinformation_rag_2024,
alignment_rag_2023}. As a result, robustness improvements are needed to
safeguard the systems. Third, RAG systems pose new challenges regarding data
privacy~\cite{privacy_rag_2024}. The integration of external databases introduces additional leakage channels. It is imperative to ensure that the RAG systems do
not expose private information from both the external databases and the training data of the underlying LLM during the generation process. Additionally, RAG could be
susceptible to fairness issues~\cite{shrestha2024fairrag} from both the retrieving process and the generation process. How the retrieved data is
selected and utilized can significantly affect the fairness of the generated
content. The implicit bias during the generation could also be affected by the retrieved content due to the increased confidence~\cite{no_free_lunch_rag_2025}. Lastly, with the rise and potential use of LLMs, accountability is a subject for policymakers on the use of RAG systems. Although progress has been made, these
challenges significantly restrict the wide adoption of RAG systems in real-world
scenarios, especially in high-stakes scenarios such as medication, legal
consulting, and education~\cite{xiong2024benchmarkingretrievalaugmentedgenerationmedicine, Zakka2023Almanac,
Wiratunga2023CBR, Thway2023Battling}. Thus, it is essential to incorporate the
trustworthy perspective while advancing the RAG systems. 

Due to the importance of trustworthy AI, a plethora of research has been
developed to advance the application of RAG in Large Language Models with heterogeneous definitions, tremendous
implementation, and inconsistent evaluation metrics. However, there is no
systematic review of this area's current advancements and challenges. To organize the various perspectives, this paper formulates a systematic discussion on the state of
trustworthy RAG in Large Language Models. 
The list of papers discussed is provided in the GitHub repository\footnote{\url{https://github.com/Arstanley/Awesome-Trustworthy-Retrieval-Augmented-Generation}}.

\begin{figure*}[t]
 
    \centering
    \includegraphics[width=0.9\textwidth]{figs/figs_overview.png}
    \label{fig:overview}
    \caption{An overview of the key components and dimensions of Trustworthy Retrieval Augmented Generation (RAG) for Large Language Models (LLMs) that are covered in this survey.  }
    \vspace{-0.1in}
\end{figure*}