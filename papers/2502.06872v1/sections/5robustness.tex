\label{sec:robustness}

Recent research has shown that LLMs are vulnerable to a range of adversarial attacks~\cite{huang2023surveysafetytrustworthinesslarge, shayegani2023surveyvulnerabilitieslargelanguage, liu2024towards, Yao_2024}. Through techniques such as prompt engineering, hint manipulation, and input perturbation, attackers can bypass safety mechanisms and exploit model weaknesses, posing significant threats to society. RAG systems, built upon the capabilities of LLMs and integrated with external databases, present distinct challenges related to safety concerns. For example, by letting the RAG system retrieve adversarial information, attackers can circumvent the alignment of LLMs with human integrity and produce malicious content~\cite{deng2024pandorajailbreakgptsretrieval}. As more adoption of RAG system emerges, the safety of RAG systems have arisen significant concerns on utilizing RAG in more high-stake applications. For example, in education applications, RAG is often leveraged to retrieve relevant domain-specific educational context (textbooks, problem set, etc.). Thus, these vulnerabilities will pose significant threats to underage minors if the system is adversarially compromised. As a result, developing robust RAG is essential for ensuring trustworthiness in RAG systems. Since this is a relatively new field of research, we will give a brief overview of the adversarial attacks on RAG and then point out potential future directions that are worth investigating. 

\subsection{Taxonomy of RAG Safety} In Table \ref{robustness-taxonomy}, we summarize existing Retrieval-Augmented Generation (RAG) methods based on an adversarial taxonomy. This section introduces the taxonomy and provides definitions for each category. It is worth noting that the wide range of adversarial attacks on LLMs, such as backdoor attacks~\cite{xue2024trojllm, lu2024test}, jailbreaking attacks~\cite{wei2024jailbroken, zou2023universal}, and prompt injection attacks~\cite{greshake2023not, liu2023prompt, yan2023backdooring}, technically target the underlying LLM component of RAG. However, how the retrieved context influences the attack surface and defense strategies in RAG systems remains an open question requiring further investigation. Preliminary results~\cite{privacy_rag_2024} suggest that RAG can alleviate the effects of simple prefix attacks, as the retrieval step introduces an additional layer of complexity. Yet, the interaction between the retriever and generator %models 
under more complex adversarial conditions, e.g., combined backdoor and retrieval-based attacks, is still poorly understood warranting further study.

\input{tables/robustness_taxonomy}
Due to the scope of this survey, we do not delve into existing LLM-specific attacks, which are covered extensively in works like~\cite{trustworthy_llms_2024, huang2023surveysafetytrustworthinesslarge}. Instead, our focus in this section is on the robustness of the retriever model in RAG systems and how its interaction with the generator model can impact overall system security and resilience to adversarial attacks.

\subsubsection{Threat Model} To systematically categorize the robustness of RAG systems, we consider three primary components: the external database, the output generator (the underlying language model), and the context retriever. A realistic threat model assumes that the attacker has no read or delete access to the external database but possesses write access. This mirrors real-world scenarios where users can upload documents to a database but cannot access all of the content. The attacker is also assumed to have no detailed knowledge of the underlying language model. In practice, most commercial language models are proprietary, making them black-box systems. This assumption is crucial for adversarial robustness studies, as the attacker cannot exploit specific weaknesses in the model architecture or training data. 

For the retriever, we consider two distinct threat models. For white-box setting, the attacker has complete access to the retriever model. This includes the ability to inspect the model architecture, parameters, and sometimes retriever-specific training data. The attacker can thus craft sophisticated adversarial examples by exploiting the retriever’s known weaknesses. For example, by understanding the tokenization or ranking algorithm, an attacker might introduce documents that are highly ranked by the retriever but irrelevant or misleading for the generation task~\cite{zou2024poisonedragknowledgecorruptionattacks}.

In contrast, the black-box setting assumes the attacker has no direct access to the retriever. The adversary can only query the retriever and observe the output (i.e., the ranked retrieved documents) without knowledge of the internal mechanisms. The attacker must infer patterns from these outputs and attempt to manipulate the retriever's behavior through indirect methods, such as poisoning the external database or introducing misleading or noisy entries~\cite{deng2024pandorajailbreakgptsretrieval}.

\subsubsection{Attacker's Goal} The attacker's target, under traditional machine
learning contexts, is often categorized into two main types: targeted attacks
and untargeted attacks~\cite{trustworthy_graph_neural_networks_2024,
trustworthy_llms_2024}. For generative models such as Retrieval-Augmented
Generation (RAG), we identify two corresponding categories: targeted attacks and
jailbreak attacks. 
Targeted attacks are aimed at
manipulating the model's output in response to specific inputs, usually focusing
on certain questions or topics. The goal is to subtly distort the generated
response while keeping the attack as stealthy as possible, making it harder to
detect through typical monitoring systems. These methods are often carefully
crafted to evade detection mechanisms by introducing minimal disruptions. Such
targeted attacks can have significant social implications. For example, by
%introducing
injecting retrieval bias, a targeted attack might skew the model's output to
emphasize certain policies or downplay others, inadvertently affecting sensitive
areas such as elections. 

In the context of generative models, untargeted attacks manifest as jailbreaking attacks, where attackers attempt to bypass content restrictions or safety measures embedded within the model. The goal is to provoke unrestricted or harmful outputs without focusing on a specific topic. This type of attack poses a broad threat as it can force the model to generate inappropriate or unsafe content across a range of inputs, compromising the system’s reliability and trustworthiness.

\subsection{Methods of RAG Safety}
\paragraph{Targeted Attacks.} 
Zou et al.~\cite{zou2024poisonedragknowledgecorruptionattacks} first introduce PoisonedRAG as a novel method to attach the new attack surface brought by the retrieval component. By designing specific passages to be injected into the retrieval database based on specific questions, PoisedRAG is able to mislead the RAG system to generate specific answers desired by the attackers for specific questions. It considers both the white-box and black-box settings. When the attacker has no access to the model parameters (black-box), PoisonedRAG crafts the injected passage with a simple heuristic: passages that is more similar to the question would be more likely to be retrieved. On the other hand, when the attacker has access to the model parameters, the crafted message is constructed by further optimizing the following equation 
\begin{equation}
    P = \underset{P'}{\text{argmax}}Sim(f(Q), f(P'))
\end{equation}
where $P$ is the generated passage, $Q$ is the user query, $f$ is the encoder, and $Sim$ is the function that measures the similarity between the encoded passage and question.

One limitation of PoisonedRAG is its focus on specific queries, neglecting broader group-based attacks that target semantically related query categories, such as those involving political affiliations, race, or religion. To address this gap, Xue et al.~\cite{xue2024badrag} propose the BadRAG framework, which extends the attack methodology to include group-query targeting. BadRAG allows the trigger to be semantic groups such as political parties or candidates by collecting target triggers from the given topic. For example, for \textit{Republican}, BadRAG collects terms including \textit{Governor}, \textit{Red States}, and \textit{Pro-Life}. To optimize the adversarial passages, BadRAG employs a contrastive learning paradigm, where the triggered queries are positive samples and the normal queries are negative samples. The adversarial passage will then maximize its similarity with the triggered queries and minimize its similarity with the normal ones. It's also worth mentioning that this approach can also support additional attacks, such as Denial of Service (DoS)~\cite{xue2024badrag}, by aligning adversarial passages with targeted model behaviors.

\textit{Dense retrieval} has been extensively studied within the Information Retrieval community~\cite{densesurvey2024}. Many attacks on dense retrievers, such as adversarial manipulation, are also applicable to passage retrieval tasks. Recently, Long et al.\cite{long2024backdoorattacksdensepassage} introduced a backdoor attack framework that exploits grammatical errors as triggers to spread misinformation. By employing contrastive learning to fine-tune the retriever, the model can retrieve adversarial passages specified by an attacker when it detects these grammatical anomalies. Additionally, Zhong et al.\cite{zhong2023poisoningretrievalcorporainjecting} demonstrated that adversarial passages trained on one domain can effectively transfer to out-of-domain queries, broadening the scope and potential impact of such attacks.

Despite these advances, most attack strategies for dense retrieval have been developed without considering downstream generation tasks, leaving their effects on generated outputs unclear. For instance, certain Retrieval-Augmented Generation (RAG) methods equipped with safety guardrails could potentially diminish the impact of adversarially retrieved passages. As we will discuss in the Future Directions section, understanding and addressing the interaction between retrieval attacks and downstream generation presents a significant research opportunity in Trustworthy RAG. 

\paragraph{Jailbreak Attacks.} When specific attack targets are absent, the threat model shifts toward jailbreak attacks. Wang et al.\cite{wang2024poisonedlangchainjailbreakllms} examine jailbreaking in the context of LangChain, a popular RAG framework. They analyzed jailbreak vulnerabilities in major Chinese Large Language Models and introduced the Poisoned-LangChain (PLC) method. By embedding jailbreak prompts into the retrieval database, PLC achieved jailbreak success across three scenarios, maintaining a consistent success rate exceeding 80\%. More recently, Deng et al.\cite{deng2024pandorajailbreakgptsretrieval} introduce Pandora, which extends jailbreaking attacks to English-based LLMs and more generalized RAG frameworks. Pandora enhances the malicious prompts by categorizing them into distinct topics and storing them in PDF format. This approach ensures that only titles and abstracts are retrieved, by which circumventing potential defense mechanisms that might detect the malicious content.

\subsection{Safety Evaluation}
\paragraph{Metrics.} For targeted attacks, researchers typically evaluate results from two perspectives. First, they measure the exclusivity of the trigger query’s effectiveness. To avoid detection, it’s essential that the same adversarial effects do not occur for non-triggered queries. To quantify this, retriever-based methods like BadRAG~\cite{xue2024badrag} assess the proportion of adversarial passage retrievals for clean queries compared to triggered queries. Specifically, they report the percentage of queries that retrieve at least one adversarial passage in the top-$k$ results (where $k=1, 10, 50$). Second, they measure the effectiveness using the Attack Success Rate (ASR). Notably, in generative tasks, ASR requires nuance due to variations in language expression. For example, responses like \textit{“Sam Altman”} and \textit{“The CEO of OpenAI is Sam Altman”} both correctly answer \textit{“Who is the CEO of OpenAI?”} Thus, researchers often employ \textit{substring matching} rather than \textit{exact matching} for this evaluation~\cite{zou2024poisonedragknowledgecorruptionattacks}.

Jailbreak attacks are also evaluated using ASR. However, lacking a targeted question, the criteria for successful jailbreaks need to be carefully defined. Deng et al.\cite{deng2024pandorajailbreakgptsretrieval} manually label a generation as a successful attack based on the \textit{relevance} and \textit{quality} of the generated content. Similarly, Yang et al.\cite{wang2024poisonedlangchainjailbreakllms} also manually count successful attacks to calculate ASR. We identify this as a methodological gap and will further discuss it in the future directions section.

\paragraph{Datasets.}
Currently, there are no widely accepted standardized datasets for evaluating robustness in retrieval-augmented generation (RAG) systems. For targeted attacks, researchers often follow a structured paradigm: first, identifying the downstream task. The current evaluations primarily focus on question answering, leveraging widely used benchmark datasets such as Natural Questions (NQ)\cite{kwiatkowski-etal-2019-natural}, MS MARCO\cite{bajaj2018msmarcohumangenerated}, and SQuAD~\cite{rajpurkar2016squad}. Researchers then select questions based on the specific characteristics of the targeted attacks. For instance, in BadRAG, Xue et al.~\cite{xue2024badrag} chose \textit{Republican} and \textit{Democrats} as targets for sentiment steering. Evaluation involves comparing results between clean (untargeted) queries and targeted queries to measure the impact of the attack.

For jailbreak attacks, evaluation methods vary significantly due to the inherent challenges of assessing open-ended text generation. Current approaches typically rely on manually curated adversarial questions categorized into specific themes. For example, Pandora~\cite{deng2024pandorajailbreakgptsretrieval} organizes questions into categories such as \textit{Adult}, \textit{Harmful}, \textit{Privacy}, and \textit{Illegal}, whereas Poisoned-Langchain~\cite{wang2024poisonedlangchainjailbreakllms} uses categories like \textit{Dangerous Behaviors}, \textit{Misuse of Chemicals}, and \textit{Illegal Discrimination}. Despite these efforts, the RAG literature lacks standardized datasets and unified evaluation frameworks, highlighting the need for future research to establish comprehensive benchmarks and methodologies.

\subsection{Future Directions of RAG Safety}
\label{subsec:robust_future}
\paragraph{Adversarial Defense.}
Current defense mechanisms against adversarial attacks in retrieval-augmented generation (RAG) systems are rudimentary. For instance, Xue et al.~\cite{xue2024badrag} propose learning the connection between trigger words and adversarial passages through \textit{token masking}. However, there is a notable lack of dedicated research specifically addressing adversarial defense in this domain. Existing approaches are limited in their scope and sophistication, leaving significant room for improvement. Arguably, advancements in generalization research, as discussed in Section 3.2, can be leveraged for adversarial defense. Training RAG models on distributions that include retrieved adversarial data could enable these models to better generalize and potentially mitigate the impact of adversarial passages. However, further exploration is needed to design and implement targeted approaches that address the unique challenges posed by adversarial attacks in RAG systems. The development of dedicated methodologies for adversarial defense remains an essential and largely uncharted area of research.

\paragraph{Modality.}
Current research predominantly focuses on RAG systems operating over text databases represented as vectors. However, recent advancements have introduced emerging applications that utilize alternative knowledge representations, such as knowledge graphs and databases~\cite{ni2024trustworthyknowledgegraphreasoning}. The unique structural properties of knowledge graphs present novel opportunities and challenges for both adversarial attacks and defenses. Attack and defense mechanisms tailored to these modalities are necessary to account for their inherent characteristics, such as the interconnectedness of entities and the semantic richness of relationships. Future research should focus on developing methods that address these unique requirements to expand the applicability and robustness of RAG systems across diverse knowledge representations.

\paragraph{Evaluation.}
As highlighted in preceding sections, the field currently suffers from a lack of standardized evaluation protocols. This absence hinders the ability to conduct fair comparisons and benchmark the effectiveness of different approaches. We advocate for the establishment of comprehensive evaluation frameworks and benchmarks that consider diverse metrics, such as robustness, generalization, and performance under adversarial conditions. Such benchmarks would provide a unified basis for assessing advancements in the field and drive progress through consistent and meaningful comparisons. Addressing this gap is critical for fostering innovation and ensuring the reliability of RAG systems in practical applications.

