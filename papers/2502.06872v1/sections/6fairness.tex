As generative models such as LLMs and image-generative models becomes increasingly integrated into real-world applications, it is critical to ensure fairness in their outputs. Retrieval-Augmented Generation (RAG) systems combine generative models with external knowledge retrieval, providing substantial improvements in accuracy and relevance by incorporating up-to-date information. However, the external data retrieved by these models may contain societal biases due to biased pre-trained knowledge \cite{wang2022revise, birhane2021large, birhane2021multimodal}, which leads to biased outputs. This further introduces the risk of amplifying disparities in age, gender, race, and other demographic attributes, particularly when the retrieved data is drawn from biased or unregulated sources.

\subsection{Taxonomy of RAG Fairness}
 In this section, we investigate various approaches that aimed at promoting fairness in RAG models. As shown in Table \ref{fairness-taxonomy}, Ensuring fairness in Retrieval-Augmented Generation (RAG) systems requires addressing biases at two stages: the retrieval of external data and the generation of outputs. These stages introduce unique fairness challenges, from biased data sources to unfair generative behavior, which need to be mitigated to create equitable and unbiased systems.

\subsection{Fairness in Retrieval}
One major challenge for fairness in RAG systems lies in the retrieval phase, where external knowledge or data used to enhance generation is sourced. Fairness issues during this stage can arise from various factors, including the retrieval model, the retrieval process, and the re-ranking mechanism. 
To address these challenges, several frameworks have been proposed to ensure that the retrieval system itself is fair.
Reskabsaz et al.~\cite{rekabsaz2020neural} introduces a framework for measuring bias that quantifies gender-related biases in ranking lists and assesses the impact of both BM25 and neural retrieval models. Furthermore, Reskabsaz et al.~\cite{rekabsaz2021societal} investigates how re-ranking methods can mitigate biases present in initial retrieval results. Then, Wang et al.~\cite{wang2024large} recognizes a gap between fairness and ranking performance when using LLMs for re-ranking and proposes a method with LoRA. To ensure demographic diversity, FairRAG ~\cite{shrestha2024fairrag} incorporates external data sources that cover a broad range of age, gender, and skin tone categories. This approach uses post-hoc sampling techniques to debias the retrieval process, preventing disproportionate representation of specific demographic groups in the retrieved data.
Beyond addressing bias in the data itself, frameworks such as BadRAG~\cite{xue2024badrag} have shown how maliciously inserted or poisoned data in the retrieval corpus can lead to biased and unfair outputs. Kong et al.~\cite{kong2024mitigating} proposes the Post-hoc Bias Mitigation (PBM) technique, which balances retrieved image sets to ensure more equitable representation across gender and race.

\input{tables/fairness_taxonomy}

\subsection{Fairness in Generation}

Once the data is retrieved, the next challenge lies in ensuring that the generative process itself is fair. Even with fair retrieval, generative models may introduce biases based on how the retrieved data is integrated. To promote fairness in generation, Liang et al.~\cite{liang2022holistic} assesses the accuracy of question-answering systems while accounting for fairness through measures of toxicity and representation bias. Similarly, Wang et al.~\cite{wang2023decodingtrust} identifies the demographic imbalances in models like GPT-3.5 and GPT-4 under both zero-shot and few-shot question-answering settings. FairRAG~\cite{shrestha2024fairrag} employs conditioning techniques where generative models are guided by references that are demographically diverse. By incorporating external images or data from a wide range of demographic groups, these models produce more balanced and representative outputs. Parrish et al.~\cite{parrish2021bbq} introduces the BBQ benchmark to evaluate biases in LLM-generated responses by examining the reliance on stereotypes and anti-stereotypes in both ambiguous and disambiguated contexts. Next, to fully explore fairness throughout all stages and components of RAG pipelines, Wu et al.~\cite{wu2024does} conducts an empirically evaluation of fairness across various RAG methods. Similarly, Kim et al.~\cite{kim2024towards} evaluates RAG systems with a fairness-aware retriever across seven different tasks and identifies the overall trend of fairness-quality trade-off, considering both retrieval and generation performance.

\subsection{Fairness Evaluation}
\paragraph{Metrics.} First, to assess the accuracy of generated answers, it is common to use Exact Match (EM)~\cite{rajpurkar2016squad} and ROUGE-1 scores~\cite{lin2004rouge}. For fairness evaluation, the focus is on metrics such as Group Disparity (GD)~\cite{friedler2019comparative} and Equalized Odds (EO)~\cite{hardt2016equality}. Group Disparity measures the performance difference between protected and non-protected groups by calculating the ratio of exact matches within each group to the total number of exact matches across all groups. In contrast, EO evaluates whether the likelihood of correct answers (true positives) and incorrect answers (false negatives) is similar across different demographic groups, ensuring that no group is disproportionately advantaged or disadvantaged. Works like BadRAG~\cite{xue2024badrag} identify vulnerabilities and attacks on retrieval components (RAG database) and their indirect effects on generative parts (LLMs). They evaluate metrics such as retrieval success rate and rejection rate to assess the system's robustness and ensure that biases or attacks in retrieval do not impact generative outputs.

\paragraph{Datasets.} A popular dataset is the TREC Fair Ranking Track~\cite{ekstrand2023overview, craswell2020overview}, which includes subsets such as gender and location. The track aims to provide a platform for participants to develop and evaluate novel retrieval algorithms that ensure fair exposure to a mix of demographics or attributes, such as ethnicity, represented by relevant documents in response to a search query. The BBQ dataset~\cite{parrish2021bbq} includes samples with contexts that are either ambiguous or unambiguous. Ambiguous contexts test model behavior with insufficient evidence by providing only a general setting, while disambiguated contexts offer enough details to identify the correct individual for negative/non-negative questions. The LaMP benchmarks~\cite{salemi2023lamp} include various prediction tasks like classification, regression, and generation, and are ideal for scenarios where multiple items can be relevant, unlike typical QA tasks. With clear item providers and consumers, LaMP aligns with the goal of ensuring fairness for item providers and evaluates language models' personalization capability through retrieval-augmentation of user interaction histories.

\subsection{Future Directions of RAG Fairness}

\paragraph{Personalized Fairness.} While existing frameworks like FairRAG~\cite{shrestha2024fairrag} and PBM~\cite{kong2024mitigating} have made notable strides in mitigating biases in Retrieval-Augmented Generation (RAG) systems, they largely overlook the need for personalized fairness mechanisms. Personalized fairness involves tailoring fairness constraints to specific application contexts, such as healthcare or recruitment, where fairness definitions and requirements can vary significantly. For example, in healthcare, equity might require prioritizing underrepresented groups in clinical trials, while in recruitment, it might focus on reducing gender or racial biases in candidate evaluation. Future research should explore adaptive fairness constraints that dynamically align with these domain-specific requirements, ensuring equitable outcomes in high-stakes domains.

\paragraph{Fairness-Accuracy-Relevance Trade-offs.} A persistent challenge in advancing fairness in RAG systems is the inherent trade-offs between fairness, relevance, and accuracy. Current approaches, such as those explored by Liang et al.~\cite{liang2022holistic} and Wang et al.~\cite{wang2023decodingtrust}, primarily emphasize fairness in the generation and retrieval phases. However, they lack robust mechanisms to balance these competing objectives. Multi-objective optimization techniques could be pivotal in navigating these trade-offs. By formalizing fairness, relevance, and accuracy as interconnected yet distinct objectives, researchers can develop frameworks that prioritize trade-off management, enabling more holistic and practical solutions for fairness in RAG systems.

\paragraph{Cross-Modal Fairness.} The emergence of multimodal RAG systems that process both text and image data introduces unique challenges in fairness. Biases in these systems can manifest differently across modalities, yet current research, such as Wu et al.~\cite{wu2024does} and Kim et al.~\cite{kim2024towards}, has only scratched the surface of cross-modal fairness. Future work should aim to identify and mitigate modality-specific biases while ensuring consistent fairness across both text and image outputs. For example, ensuring that a systemâ€™s fairness constraints for text descriptions align with fairness principles for corresponding visual elements could significantly enhance trustworthiness. Addressing cross-modal fairness is essential for building robust and equitable multimodal RAG systems.