\input{tables/privacy_taxonomy}

Although privacy risks in LLMs have been extensively studied, RAG systems introduce additional complexities by leveraging external data. This integration poses new challenges in maintaining privacy, ensuring data integrity, and managing the overall trustworthiness of the RAG system. In this section, we will introduce the threat model concerning privacy leaks in RAG systems. We will then discuss the current efforts to address these challenges. Towards the end, we will explore potential future directions for enhancing the trustworthiness and privacy of RAG systems. 

\subsection{Taxonomy of RAG Privacy}
In Table \ref{privacy-taxonomy}, we outline the existing efforts in addressing the privacy issues present in the RAG systems. We will briefly introduce the relevant taxonomy in the following. 

\subsubsection{Training}
Training refers to whether the attack or defense requires \textit{prior} training on the data. Models that require training typically assume a distinct threat model compared to those that do not. When a model requires training, it often presumes white-box access to either the retriever or the language model, allowing attackers or defenders to fine-tune or adjust components of the RAG system to exploit or mitigate vulnerabilities. On the other hand, models that do not require training typically rely on prompt-based methods or zero-shot techniques. This distinction has significant implications for the feasibility of privacy attacks and defenses in RAG systems.

\subsubsection{Tasks} Admittedly, research on RAG privacy is still in its infancy. Current literature focuses on three main tasks: \textit{Document Extraction}, \textit{Training Data Extraction}, and \textit{Membership Inference Attack}. \textit{Document Extraction} seeks to extract confidential information from the retrieval database, such as Personally Identifiable Information (PII). \textit{Membership Inference Attack} aims to determine whether specific passages are present in the retrieval database. While not a direct privacy attack, it introduces risks by exposing sensitive associations between queries and database contents, potentially enabling adversaries to infer private information. Lastly, \textit{Training Data Extraction} examines the leakage of LLM training data in the context of retrieval-augmented generation, highlighting vulnerabilities that could lead to the unauthorized exposure of proprietary or sensitive datasets.

\subsubsection{Leakage}
We consider two sources of leakages. First, the leakage of the external retrieval database involves leaking targeted/untargeted information from external knowledge sources, such as sensitive data in proprietary databases or publicly available but privacy-relevant information inadvertently retrieved during query processing. Second, the leakage of the internal training data focuses on the exposure of the LLM training data in the context of retrieval-augmented generation. This occurs when the language model unintentionally reproduces sensitive or proprietary information from its training dataset during response generation, raising concerns about policy violations and privacy breaches. We organize the rest of the section from the above two aspects. 

\subsection{Data Leakage From the External Retrieval Database}

The goal of the attacker is to exploit privacy vulnerabilities within the retrieval dataset, targeting two main objectives: (1) eliciting specific information from the retrieval system with high accuracy, and (2) outputting the retrieved private data. Zeng et al.\cite{privacy_rag_2024} introduced a composite structured prompt, formulated as $q = {\text{information}} + {\text{command}}$, which leverages the context retriever's propensity for similarity-based matching. However, a significant limitation of this approach is its reliance on fixed queries, which cannot dynamically adapt to varying contexts. To address this limitation, Jiang et al.\cite{jiang2024ragthief} proposed a learning-based method. Their framework begins with an initial adversarial query and iteratively refines it based on the model's responses, progressively generating queries to extract as many documents as possible from the retrieval database.

When considering white-box access to the model, Peng et al.~\cite{peng2024data} focused on data extraction through backdoor attacks. Their method trains a model to associate specific triggers with desired outputs. Beyond directly extracting documents, their approach also explores generating stealthy outputs by employing a language model to paraphrase the retrieved content, thereby increasing the difficulty of detecting the attack. Furthermore, Cohen et al.~\cite{cohen2024unleashing} demonstrated that these attacks can escalate beyond isolated cases. By crafting an \textit{adversarial self-replicating prompt}, attackers can initiate a chain reaction that propagates through the entire Retrieval-Augmented Generation (RAG) system.

Although distinct from direct extraction methods and based on a different threat model, \textit{membership inference attacks} have also proven effective for data extraction. These attacks allow malicious users to infer whether specific content is present in the retrieval database. Liu et al.~\cite{liu2024mask} introduced a mask-based attack that obscures portions of documents, compelling the language model to predict the masked words. This technique not only reveals sensitive information but also highlights vulnerabilities in the retrieval system's training data.
 

\subsection{Data Leakage From the LLM Training Data}
The goal of the attacker is to extract data from the LLM's training and fine-tuning data that are encoded in the model parameters. In their paper, Zeng et al.~\cite{privacy_rag_2024} compared the effect of RAG in preventing data leakage from the LLM training data. The result shows that incorporating retrieved passages greatly reduces LLM's propensity to reproduce content memorized during its training/fine-tuning process. To isolate the effect of retrieval data integration, the author also attached 50 tokens of random noise injection as prefix. Although the random noise could also mitigate the data leakage, it is far less effective than integrating the retrieved content. 

\subsection{Defense on Privacy Attacks}
Although still a relatively under-explored area, some works have proposed defenses to mitigate privacy vulnerabilities in RAG systems. In their foundational work, Zeng et al.~\cite{privacy_rag_2024} observed that using a separate model to summarize the retrieved documents effectively reduces privacy leakage by abstracting sensitive information into generalized content. Additionally, they proposed implementing a distance threshold in the retrieval database, ensuring that only documents with certain relevance requirements are returned. However, this approach introduces a trade-off between system performance and privacy protection, as stricter thresholds can limit retrieval accuracy.

Building on these mitigation strategies, the authors further suggested the use of purely synthetic data as a way to entirely avoid potential leakage of real data~\cite{zeng2024mitigating}. This method involves identifying importing attributes of the data through few-shot samples, extracting key information associated with these attributes, and generating synthetic data that mirrors the original data without exposing sensitive information. This approach has shown promise in effectively mitigating privacy leakage while maintaining the performance of the RAG system.

\subsection{Privacy Evaluation}
\paragraph{Metrics.} Metrics for evaluating privacy attacks focus on quantifying the extent of information leakage and the effectiveness of extraction methods. Commonly used metrics include the total volume of retrieved context, the number of prompts that successfully yield substantial overlaps (e.g., at least 20 matching tokens) with the dataset, and the number of unique excerpts extracted. For targeted attacks, the evaluation centers on the precision of the extracted information, assessing how accurately specific targets are retrieved. In the case of untargeted attacks, metrics often rely on content similarity measures, such as ROUGE-L scores, to determine the degree of alignment between the retrieved content and the original dataset~\cite{privacy_rag_2024, zeng2024mitigating}.

\paragraph{Datasets.} Similar to the generalization evaluation, there haven't been any established datasets or baselines specifically designed for evaluating privacy in RAG systems. However, current work has leveraged existing datasets to provide initial insights. The Enron Email dataset~\cite{klimt2004enron} contains employee emails, which often include sensitive personal information, such as names, contact details, and internal company communications. The HealthcareMagic-101 dataset~\cite{zeng2024mitigating}, on the other hand, consists of doctor-patient dialogues, encompassing a variety of medical discussions that include personally identifiable information (PII) and private health information. These datasets serve as a valuable starting point for privacy evaluations due to their realistic, sensitive content that mimics the types of data RAG systems may encounter.

\subsection{Future Directions of RAG Privacy}
\paragraph{Domain Specific Applications.} While the preliminary findings from Zeng et al.~\cite{privacy_rag_2024} shed light on the dual nature of privacy risks in RAG systems, there remains a significant gap in understanding the broader implications of these vulnerabilities across different applications and contexts. Future research should prioritize developing robust, application-specific privacy-preserving techniques tailored to the unique demands of various domains, such as healthcare, finance, and legal services, where the consequences of privacy breaches can be particularly severe.

\paragraph{Privacy Defense.} Additionally, exploring the integration of advanced cryptographic methods, such as secure homomorphic encryption, within RAG frameworks could provide new avenues for safeguarding sensitive data during retrieval and generation processes. Another promising direction is the development of differential privacy techniques specifically adapted for RAG systems, aiming to balance privacy preservation with the utility of the generated outputs.

\paragraph{Evaluation.} Finally, as the field of RAG continues to evolve, it will be crucial to establish comprehensive benchmarks and evaluation metrics for privacy in these systems. These benchmarks should account for the diverse range of privacy threats, including both direct data leakage and more subtle inferential attacks, to ensure that the proposed solutions are rigorously tested and validated across a wide spectrum of scenarios. By addressing these challenges, future research can contribute to the creation of more secure and trustworthy RAG systems, ultimately fostering greater confidence in the deployment of these technologies in sensitive and high-stakes environments.

