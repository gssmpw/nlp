While RAG improves factual consistency and adaptability, it also introduces unique reliability challenges. Unlike standalone generative models, RAG reliability depends not only on the underlying LLM but also the alignment of retrieved information. Ensuring reliability in RAG therefore requires evaluating both the the retrieval process and the generation conditioned on the retrieved content.

\subsection{Taxonomy of RAG Reliability}
At a high level, reliability requires the system to perform as expected under various conditions. Just as other deep learning models that train on the big data, RAG models are susceptible to common pitfalls of reliability. Previous work~\cite{tran2022plex} defines reliability from three granular aspects: the ability to express \textit{uncertainty} in predictions, the capability of \textit{robust generalization} under various conditions, and the extent to which the model can \textit{adapt} to new tasks. However, since RAG is inherently \textit{adaptable} because of the retrieved context, we will only consider \textit{uncertainty} and \textit{robust generalization} in our following discussion. 

\subsection{Uncertainty}
Uncertainty is a crucial factor for model reliability. Uncertainty quantification (UQ) helps quantify the
confidence in the model's predictions, which is essential in high-stakes
scenarios. Consider a medical question answering chatbot where a patient
inquires about their condition. If the model can express uncertainty in its
responses, it significantly reduces the risk associated with its predictions.
The patient can then make more informed judgments based on the confidence level
of the information provided. Thus, due to the imperativeness of accurately
conveying uncertainty, we need to ensure that robust uncertainty quantification
methods are integrated into the system. 

For Retrieval-Augmented Generation (RAG) systems, uncertainty quantification
presents two primary challenges: First, during the generation phase, uncertainty
stems from the inherent limitations of large language models (LLMs). Standard
techniques for quantifying uncertainty in LLMs, such as conformal prediction,
can be applied here with few adaptations~\cite{ye2024benchmarkingllmsuncertaintyquantification}. Second, uncertainty arises during the
retrieval phase and its interaction with the LLM, introducing a more complex
dynamics. The combination of retrieval and generation processes creates unique
challenges for UQ, necessitating advanced methods to
address the overall system complexity. The following section outlines ongoing
efforts to tackle these challenges, with a summary of the relevant literature
presented in Table \ref{uq-taxonomy}.

\input{tables/uq_taxonomy}

\subsubsection{Uncertainty Quantification in Generation}
The generation phase in Retrieval-Augmented Generation (RAG) systems is critically influenced by the UQ of LLMs. Recent studies have explored various approaches in this area, with a focus on techniques like conformal prediction (CP) — a model-agnostic, distribution-free method that uses a calibration set to estimate prediction confidence~\cite{shafer2007tutorialconformalprediction}. To apply conformal prediction, a \textit{non-conformity score} is first defined to measure the confidence of a given prediction. Using a calibration set, the $1-\alpha$ quantile of the non-conformity score is then calculated, where $\alpha$ represents the user-defined error rate. Finally, the prediction set is constructed by selecting valid predictions based on the quantile score, ensuring that the set satisfies the $1-\alpha$ confidence level, assuming the calibration and test sets are exchangeable.

The cornerstone of CP lies in defining the \textit{non-conformity score}. In traditional multi-class classification, a common approach is to use the softmax score of the from the class prediction. Extending the logit-based non-conformity score to LLMs, methods have been further developed. Typically, they assume white-box access to the model, making them unsuitable for commercial LLMs such as ChatGPT. For instance, Kumar et al.~\cite{kumar2023conformalpredictionlargelanguage} applied standard CP to the Llama model~\cite{touvron2023llamaopenefficientfoundation} by leveraging softmax scores of token logits in multiple-choice tasks. Similarly, Ye et al.~\cite{ye2024benchmarkingllmsuncertaintyquantification} extended logit-based approaches to multiple baselines and language models.

To compensate the lack of application on black models, another promising direction is proposed for sampling-based techniques, where model confidence is estimated by repeatedly prompting the LLM. Quach et al.~\cite{quach2024conformallanguagemodeling} adapted the learn-then-test risk-control framework~\cite{angelopoulos2022learntestcalibratingpredictive} for LLMs, approximating the non-conformity score through sampling, which allows uncertainty quantification in black-box models without direct logit access. Su et al.~\cite{su2024apienoughconformalprediction} further advanced these methods by introducing non-conformity measures that integrate both coarse-grained and fine-grained notions of uncertainty, leading to smaller, more refined prediction sets.

\subsubsection{Uncertainty Quantification in Retrieval and Generation}
As shown in Figure \ref{fig:overview}, a traditional RAG system includes multiple components from retrieval to generation. Due to its complex, multi-component nature, directly applying LLM-based UQ methods
will produce less accurate, sub-optimal results~\cite{li2023traq, rouzrokh2024conflare}. This necessitates the
development of specialized techniques tailored to the unique structure and
requirements of RAG models. 

Recently, researchers proposed a multi-step calibration framework to enhance the retrieval process of RAG~\cite{rouzrokh2024conflare}. Specifically, this framework uses conformal prediction to quantify retrieval uncertainty, ensuring trustworthiness in RAG systems. The framework involves constructing a calibration set of questions answerable from the knowledge base and comparing their embeddings against document embeddings to identify the most relevant chunks containing the answers. By analyzing similarity scores and determining a cutoff threshold based on a user-specified error rate ($\alpha$), the system retrieves all chunks exceeding this threshold during inference. This multi-step calibration ensures the true answer is captured in the context with a (1 - $\alpha$) confidence level. 

Moreover, TRAQ~\cite{li2023traq} expanded the conformal prediction framework to include a Bayesian optimization module that minimizes the prediction set during the multi-step calibration. Because of the complexity of RAG, the constructed prediction set will be very large after aggregating the error rates of multiple components. By leveraging Bayesian optimization, the framework efficiently searches for the optimal parameters that reduce the size of the prediction set while maintaining the desired confidence level. TRAQ ensures that the retrieval process remains both accurate and computationally feasible, enhancing the overall reliability and performance of RAG systems.

Besides uncertainty quantification in the process of document retrieval, \textsc{UaG}~\cite{ni2024trustworthyknowledgegraphreasoning} attempted to address the gap in uncertainty quantification of knowledge graph reasoning. Unlike vector databases, knowledge graphs encode knowledge as triplets and include structural information. One representative task of knowledge graph reasoning is multi-hop question answering, where the system must infer answers by traversing multiple edges in the graph to connect the initial query node with the answer node. The \textsc{UaG} framework involves combining information from several related entities and relationships within the graph, further complicating the process of uncertainty quantification. \textsc{UaG} proposed to leverage a general risk control framework to find the optimal parameter for each stage of calibration, ensuring reliable uncertainty estimates while maintaining a reasonable prediction set size. 


\subsection{Uncertainty Evaluation}

\paragraph{Metrics.} Traditionally, uncertainty quantification is evaluated from two key perspectives: \textit{coverage} and \textit{efficiency}~\cite{he2024surveyuncertaintyquantificationmethods}. Recall that the goal of uncertainty quantification is to ensure that the returned answer set satisfies a user-defined error tolerance of $1-\alpha$. Thus, the \textit{coverage rate} measures how effectively the model meets this requirement.

Given a returned set of answers, $\mathcal{A}_{\text{ret}}$, and the correct answer set, $\mathcal{A}_{\text{true}}$, the coverage rate, $C$, is calculated as the proportion of instances where the correct answer is included in the returned set. Formally, it is defined as:
\[
C = \frac{N_{\text{correct}}}{N_{\text{total}}},
\]
where $N_{\text{correct}}$ represents the number of times the correct answer $\mathcal{A}_{\text{true}}$ is contained in the returned set $\mathcal{A}_{\text{ret}}$, $\mathcal{A}_{\text{true}} \subseteq \mathcal{A}_{\text{ret}}$, and $N_{\text{total}}$ is the total number of instances. 

For the model to be considered reliable, $C$ should be at least $1-\alpha$. However, simply exceeding this threshold does not necessarily indicate optimal performance. Overestimating the returned set size while still satisfying the desired error rate implies inefficiency, as a smaller set could suffice for the same error rate.

Alongside coverage, \textit{efficiency}, denoted as $E$, is another critical metric, often evaluated by the size of the returned answer set (i.e. the number of returned answers per question):
\[
E = |\mathcal{A}_{\text{ret}}|.
\]
\noindent Efficiency reflects the utility of the model’s output, as larger sets may contain more irrelevant information, reducing their usefulness to the user. Thus, an efficient uncertainty quantification process minimizes $E$ while maintaining the desired coverage rate, $C$.

\subsection{Robust Generalization}
Previous work~\cite{tran2022plex} defines \textit{robustness} as the ability to make accurate estimates or forecasts about unseen events caused by out-of-distribution data, covariate shift, domain change, concept change, or population shift, etc. In the context of RAG, the most significant challenge is the shift in the distribution of the database. Realistically, the database will always be evolving, introducing new knowledge into the system. Without dedicated robustness measures, this can cause the model to underperform in various situations. Consequently, it is essential to develop approaches that allow the model to continually learn from new data and adjust its retrieval and generation processes accordingly such as in concept drifts.  Specifically, we will consider two aspects of robustness for RAG: resilience against irrelevant context and resilience against corrupted or misinformation contexts. It is worth mentioning that there is another type of context that we define as \textit{adversarially constructed corrupted context}. Sometimes they are closely related to \textit{corrupted context}, but due to their adversarial nature, we will consider them in Section \ref{sec:robustness} for Adversarial Robustness. This section will focus on the context that occurs \textit{organically} over time. 

\subsubsection{Irrelevant Context}

Fang et al.~\cite{adaptive_adversarial_rag_2024} considers the noise robustness of RAG with adaptive adversarial training. The paper explores three types of retrieval noises: (i) contexts that appear to be related to the query but do not contain the correct answer, (ii) contexts that are entirely unrelated to the query, and (iii) contexts that are thematically related to the query but include incorrect information. With the conclusion that type (i) and (iii) noise are the most misleading to the language models, the authors developed Retrieval-augmented Adaptive Adversarial Training (RAAT) to regulate the retrieval of noisy text. To improve the robustness under the noisy data, RAAT generates adversarial samples (noises) by considering the model's sensitivity to various types of noises and shows significant robustness improvement. The study further demonstrates that RAAT can be integrated seamlessly with existing RAG systems, enhancing their performance without substantial computational overhead. 

In addition, Yoran et al.~\cite{robust_rag_iclr_2024} further explores the negative impact of the retrieval of irrelevant context on the model performance. They argue that the negative impact of the irrlevant context is a result of the lack of training data with the retrieved passages. As a result, the brittleness to noisy passages is expected during inference. To address this observation, the author propose to finetune the language models on noisy contexts. Finetuning on this additional context allows the model to learn to differentiate between useful and distracting information and minimizing the negative effect of the irrelevant context. The experiment result on five different open domain datasets has shown significant improvements of robustness against irrelevant context for both single-hop and multi-hop retrieval based question answering. 

\subsubsection{Corrupted Context}
Recently, Xu et al.~\cite{dual_rag_2024} proposed a theoretical framework to explore the benefits and detriments of the RAG, in the situation where there's a discrepancy between the retrieved knowledge and the LLM knowledge. Specifically, they observed that the similarity between the RAG representation and the retrieved representation is bounded by the benefits and detriments, and the similarity is positively correlated with the value of benefits minus detriments. These results suggest that the similarities can be used as a proxy for the benefits and detriments of the RAG. Building upon the theoretical results, the author further proposed an interactive inference framework X-RAG that leverages the benefit of both worlds of retrieved knowledge and LLM knowledge.

\subsection{Robustness Evaluation}
\paragraph{Metrics.}
The evaluation of the model's robustness focuses on assessing its performance when noise is present in the data. Thus, the setup of the noisy data, which will be detailed in the dataset section, plays a key role in this evaluation. Exsting metrics outlined in \ref{sec:eval} will be applied to assess the model's performance. It is important to note that there are different reporting styles for these metrics in the context of model robustness. Some authors present standard tables comparing the proposed model's performance against baselines~\cite{adaptive_adversarial_rag_2024, dual_rag_2024}, while others report only the performance delta between the proposed fine-tuned model and the corresponding baseline for better visualization~\cite{robust_rag_iclr_2024}.

\paragraph{Datasets.} Currently, there is no widely-used benchmark for RAG robustness. To simulate real-world conditions and evaluate robustness, existing works create customized datasets that incorporate generated noise. Typically, a common QA benchmark (e.g., TriviaQA) %, Natural Questions) 
is used, and during the retrieval process, noises are injected into the retrieved content. Depending on the problem setting (\textit{irrelevant context} vs. \textit{corrupted context}), the noise is either randomly selected or filtered using heuristic techniques~\cite{dual_rag_2024, robust_rag_iclr_2024}. These datasets attempt to replicate the type of challenges encountered in realistic environments where the retrieved information may not perfectly align with the query.

Recently, %in RAAT, 
Fang et al.~\cite{adaptive_adversarial_rag_2024} proposed a benchmark for noise-robust RAG. For each QA instance, the proposed dataset includes three types of augmented retrieval noise: relevant retrieval noise, irrelevant retrieval noise, and counterfactual retrieval noise where the answer entity is intentionally incorrect, as well as the golden retrieval data. \textit{We recognize this as one of the first publicly available datasets for RAG robustness evaluation, and future works could benefit from using this for benchmarking.} %dataset as a baseline to compare new methods.} 

\subsection{Future Directions of RAG Reliability}
Reliability remains an important challenge in the development of trustworthy RAG systems. While our current sections are structured independently for uncertainty and robustness, future research should aim for a more integrated approach that captures the intricate interactions between these aspects. These concepts are not necessarily exclusive; uncertainty quantification can help the model produce more trustworthy results when faced with less accurate or noisy contexts, while better robustness can reduce the model's overall uncertainty.

An integrated framework addressing both uncertainty and robustness could enhance the adaptability and robustness of RAG systems, particularly in complex real-world applications where the boundaries between irrelevant and corrupted contexts are blurred. By combining strategies from uncertainty quantification with robust robustness techniques, future research can explore models that are not only resilient to noise but also capable of dynamically adjusting their confidence levels based on the context quality.

Moreover, future efforts should extend current evaluation benchmarks to include more comprehensive benchmarks that reflect real-world conditions. Current benchmarks such as RAG-Bench~\cite{adaptive_adversarial_rag_2024} mark a good effort, but the data are manually filtered by rules, which limits the scope of the challenges they present. Real-world scenarios are more complex, with a greater variety of noise. By incorporating these complexities, new benchmarks could better simulate the unpredictable nature of real-world applications. With better benchmarks, researchers will be able to test models under various reliability scenarios where uncertainty and robustness challenges are intertwined. Expanding evaluation methodologies in this direction would push the frontier of research in reliable RAG systems and offer more actionable insights for practical deployments.

Finally, drawing from interdisciplinary fields like dynamic knowledge graphs and active learning could lead to the development of more adaptive RAG systems. In dynamically evolving environments, new knowledge introduced in external databases may create inconsistencies or conflicts with previously retrieved information, posing significant reliability challenges. These interactions would require RAG systems to not only generalize well but also quantify uncertainty effectively when knowledge shifts or drifts occur. Such advances would make RAG systems more robust and practical, improving their performance in dynamic, real-world scenarios.