\typeout{IJCAI--25 Instructions for Authors}
\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{ijcai25}
\usepackage{amssymb}
\usepackage{times}
\usepackage{color, soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{xcolor}
%\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{multirow}
\usepackage{makecell}
%\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{subcaption}

\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{bluet5}{RGB}{100, 149, 237}
\definecolor{greent5}{RGB}{60, 179, 113}
\definecolor{yellowt5}{RGB}{255, 204, 102}
\newcommand{\method}{\texttt{MPSL}}

\makeatletter
\renewcommand\subsubsection[1]{%
  \par\noindent\textbf{#1.}%
}
\makeatother

%\linenumbers % Comment out in Camera-ready 
\urlstyle{same}
%\usepackage{latexsym}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}
\title{Fine-tuning Multimodal Transformers on Edge: \\
A Parallel Split Learning Approach
}

\author{
    Timo Fudala$^{1}$ \and Vasileios Tsouvalas$^{1}$ \And Nirvana Meratnia$^{1}$ \\ \vspace{0.2em}
    $^{1}$ Eindhoven University of Technology \\
    \emails{t.fudala@student.tue.nl,\{v.tsouvalas, n.meratnia\}@tue.nl}
}

\begin{document}

\maketitle



\begin{abstract}
Multimodal transformers integrate diverse data types like images, audio, and text, advancing tasks such as audio-visual understanding and image-text retrieval; yet their high parameterization limits deployment on resource-constrained edge devices. Split Learning (SL), which partitions models at a designated cut-layer to offload compute-intensive operations to the server, offers a promising approach for distributed training of multimodal transformers, though its application remains underexplored. We present~\method\footnote{Code will be made available upon acceptance.}, a parallel SL approach for computational efficient fine-tuning of multimodal transformers in a distributed manner, while eliminating label sharing, client synchronization, and per-client sub-model management.~\method~employs lightweight client-side tokenizers and a unified modality-agnostic encoder, allowing flexible adaptation to task-specific needs. Our evaluation across $7$ multimodal datasets demonstrates that~\method~matches or outperforms Federated Learning, reduces client-side computations by $250\times$, and achieves superior scalability in communication cost with model growth. Through extensive analysis, we highlight task suitability, trade-offs, and scenarios where~\method~excels, inspiring further exploration.
\end{abstract}

\section{Introduction} \label{introduction}

% Introduction + Motivation of use of Multimodal Transformers on Edge
Multimodal transformers have demonstrated remarkable potential by integrating diverse data types such as images, audio, and text, thereby significantly advancing tasks like image captioning, audio-visual understanding, and vision-language question answering. The rapid proliferation of IoT devices, expected to surpass 29 billion by 2030~\cite{Bonaventura_2024}, generates a massive volume of data at the edge, offering transformative opportunities for AI applications across sectors such as healthcare, transportation, and communication. However, effectively leveraging this data necessitates addressing privacy concerns, regulatory restrictions, and the prohibitive cost of transferring vast datasets to centralized servers, which strains network bandwidth. Federated Learning (FL)~\cite{fl} has emerged as a promising solution by enabling collaborative model training without sharing raw data, allowing models to train locally while aggregating updates across participating clients. Despite its benefits, FL falls short in application to multimodal transformers due to their architectural complexity and overparameterization, making them unsuitable for training solely on resource-constrained edge devices~\cite{10605435}. 

\begin{figure}[!t]
    \centering \small
    \includegraphics[width=0.95\linewidth]{./assets/finals/avg_res.png}
    \vspace{-10pt}
    \caption{\small{\method~(Ours) vs. FL for multimodal learning on edge devices using Meta-Transformer, results averaged over 5 tasks.}}
    \label{fig:avg_res}
    \vspace{-10pt}
\end{figure}

% Intro to PSL and its limitations in multimodal settings
To address these limitations, Split Learning (SL)~\cite{sl} has emerged as a compelling alternative, reducing client-side computation by partitioning a model and distributing it between edge devices and servers. SL enables edge devices to offload the computationally intensive portions of training to the server while preserving data privacy by keeping raw data on-device. Here, edge devices process initial layers locally and send intermediate activations — referred to as smashed data — to the server, which handles the remaining computations. Parallel Split Learning (PSL)~\cite{privacy-sensitive-psl} extends SL by enabling multiple devices to train simultaneously, addressing the latency issues of SL and making it a viable alternative to FL for computationally intensive models. However, while PSL has been studied for unimodal applications, its potential for handling multimodal data remains underexplored.

% Our Contribution
To bridge this gap, our work extends PSL to multimodal transformers, termed Multimodal Parallel Split Learning (\method), enabling computation-efficient training on edge devices. Inspired by~\cite{sasl}, we utilize a server-side loss aggregation mechanism that processes clients’ smashed data with a single backward pass, significantly reducing server-side computation and training latency compared to PSL. Additionally, we analyze the applicability of multimodal downstream tasks for PSL, identifying those that can be effectively trained and the factors influencing their performance. Our evaluation, which spans client-side communication overhead, computation overhead, and model performance, provides actionable insights into scenarios where PSL outperforms other distributed machine learning approaches in a multimodal setting. Concisely, our contributions are:

\begin{figure*}[!th]
    \centering \small
    \includegraphics[width=0.9\textwidth]{./assets/overview.png}
    %\vspace{-10pt}
    \caption{\small{An illustration of \method's training pipeline. Clients tokenize inputs of multiple modalities and offload intensive computations to the server, which processes the received activations through a unified encoder and then performs a single backward pass using an aggregated loss function. V: Vision, A: Audio, T: Text, BP: Back Propagation, FP: Forward Pass\label{fig:overview}}}
    \vspace{-10pt}
\end{figure*}


\begin{itemize}
\item We introduce~\method, extending PSL to enable computation-efficient fine-tuning of multimodal transformers on edge devices, eliminating label sharing, client-side model synchronization, and per-client sub-model management on the server.
\item ~\method~reduces client-side computational overhead by $\times$ 250 compared to FL regimes, while achieving performance comparable to centralized fine-tuning.
\item Our evaluation across $7$ datasets and multiple transformer sizes highlights task suitability, trade-offs, and scenarios where~\method~outperforms traditional distributed learning approaches in a multimodal setting. 
\end{itemize}

\section{Related Work} \label{related_work}

\subsection{Split Learning}
Split Learning (SL) was introduced to overcome limitations of Federated Learning (FL), particularly the high computational demands on resource-constrained edge devices. By partitioning the model at a designated cut-layer, SL enables edge devices to process initial layers locally and send intermediate activations (smashed data) to a server for forward and backward propagation. Although this approach alleviates client-side computational burdens, SL requires sharing labels; impeding label privacy \cite{sl}. To address this, variations like U-shaped SL were proposed to ensure that both input features and labels remain on the client-side by partitioning the model into three segments: client-head, server-body, and client-tail \cite{sl,Shuffled-Transformer,MaskSL}. Despite these advancements, the sequential nature of vanilla and U-shaped SL imposed scalability limitations, prompting the development of Parallel Split Learning (PSL). PSL eliminates sequential dependencies by enabling multiple clients to train in parallel, significantly reducing training latency~\cite{privacy-sensitive-psl,CutMixSL,DP-CutMixSL,FedBone,LocalSplitFed}. Yet, early PSL works such as LocalSplitFed~\cite{LocalSplitFed}~require synchronization of client-side models~\textendash~thereby introducing communication overhead~\textendash~and often acquire parallelism by maintaining per-client sub-models on the server. The latter becomes infeasible when employing computationally intensive models with a large number of clients. However, subsequent PSL solutions acquire parallelism without this per-client sub-model constraint and introduce various optimizations for PSL, such as tackling the client decoupling problem identified in SGLR~\cite{sglr}~by using gradient aggregation techniques~\cite{sglr,Mix2SFL,EPSL}, and exploring single-pass loss aggregation mechanisms~\cite{sasl}~to minimize redundant computations and improve scalability. The advancements in PSL position the distributed learning framework as a viable alternative for the distributed training of computationally intensive models. Despite this, existing PSL frameworks primarily focus on unimodal tasks, leaving the integration of multimodal data largely unexplored. This gap highlights the need for advancements in PSL to effectively accommodate the unique challenges of training multimodal models in distributed settings.

% Split Learning (SL) was introduced to overcome limitations of Federated Learning (FL), particularly the high computational demands on resource-constrained edge devices. By partitioning the model at a designated cut-layer, SL enables edge devices to process initial layers locally and send intermediate activations (smashed data) to a server for forward and backward propagation. Although this approach alleviates client-side computational burdens, SL requires sharing labels; impeding label privacy \cite{sl}. To address this, variations like U-shaped SL were proposed to ensure that both input features and labels remain on the client-side by partitioning the model into three segments: client-head, server-body, and client-tail \cite{sl,Shuffled-Transformer,MaskSL}. Despite these advancements, the sequential nature of vanilla and U-shaped SL imposed scalability limitations, prompting the development of Parallel Split Learning (PSL). PSL eliminates sequential dependencies by enabling multiple clients to train in parallel, significantly reducing training latency and positioning SL as a viable alternative for computationally intensive models~\cite{privacy-sensitive-psl,CutMixSL,DP-CutMixSL,FedBone}. Subsequent PSL solutions introduced various optimizations to tackle server-side computation overhead and client-server imbalance. For example, gradient aggregation techniques~\cite{sglr,Mix2SFL,EPSL} and single-pass loss aggregation mechanisms~\cite{sasl} have been explored to minimize redundant computations and improve scalability. Nevertheless, existing PSL frameworks primarily focus on unimodal tasks, leaving the integration of multimodal data largely unexplored. This gap highlights the need for advancements in PSL to effectively accommodate the unique challenges of training multimodal models in distributed settings.

\subsection{Multimodal Architectures}
The emergence of multimodal transformers has revolutionized multimodal learning by facilitating the effective combination of diverse data modalities, including images, audio, and text. Here, models can be categorized into two main architectures: those using modality-specific encoders and those employing a single unified encoder. Models like CLIP~\cite{clip} use distinct encoders for each modality trained jointly with contrastive learning to align modality-specific embeddings into a shared space, demonstrating strong zero-shot transfer capabilities for tasks like image-text retrieval. Building on this, ImageBind~\cite{imagebind} supports multiple modalities by employing modality-specific tokenizers and encoders, generalizing across unseen modality pairs through pre-training on image-paired data. However, its reliance on separate encoders results in a large model size of $1.2$B parameters, limiting its suitability for resource-constrained environments~\cite{imagebind}. Similarly, ViT-LENS~\cite{ViT-LENS} aligns embeddings using lightweight modality-specific adapters with a frozen ViT encoder~\cite{ViT}, reducing pre-training data requirements; yet retaining architectural complexity and size.

In contrast, unified encoder approaches like VATT~\cite{vatt} and Meta-Transformer~\cite{meta-transformer} aim to share parameters across all supported modalities, enabling more scalable designs. VATT uses a modality-agnostic encoder with shared weights to process video, audio, and text, reducing parameter counts compared to modality-specific designs~\cite{vatt}. Meta-Transformer extends this approach, employing a single encoder across twelve modalities using lightweight modality-specific tokenizers, showcasing promising results for parameter-efficient multimodal learning~\cite{meta-transformer}. Nonetheless, such models are predominantly designed for centralized training and remain computationally intensive for on-device learning. \\

\noindent \textbf{Distributed Multimodal Transformers.} Distributed training of multimodal transformers has been explored in approaches like FedCLIP~\cite{FedCLIP}, which uses lightweight adapters on top of a frozen backbone in an FL framework. However, the FL framework still requires clients to execute the frozen encoder backbone, limiting its applicability to resource-constrained edge devices. PSL offers a promising alternative; yet its application to multimodal transformers remains largely underexplored.
%-- apart from BiCSL~\cite{BiCSL}, which focuses on self-supervised pre-training for Visual Question Answering (VQA). 
Our work extends PSL to multimodal transformers, addressing client-side computational efficiency and modality alignment while overcoming prior PSL limitations like label sharing.

\section{\texttt{MPSL} Framework} \label{methodology}
We present a Multimodal Parallel Split Learning (\method) framework for fine-tuning multimodal transformers, eliminating the need for label sharing, synchronization of client-side models, and maintaining per-client sub-models on the server. Specifically,~\method~extends~\cite{sasl} to support multiple modalities, enabling support for multimodal transformer architectures. By offloading the computationally intensive components of multimodal transformers training to the server-side,~\method~significantly reduces the computational overhead on clients. Furthermore, we utilize an aggregated client-side loss to perform a single, unified backward pass on the server.

\subsection{Notations} \label{notations}
We use the following notations in the remainder of the paper. 
Let $\mathcal{N}=\{n_1, ..., n_N\}$ be the set of all clients, with $N$ as the total number of clients. For each client $n \in \mathcal{N}$, let $\mathcal{D}_n$ be its local dataset and $\mathcal{B}_n \subset \mathcal{D}_n$ a mini-batch of $\mathcal{D}_n$. The global batch is defined as $\mathcal{B}=\cup_{n \in \mathcal{N}} \mathcal{B}_n$. 
For a set of input modalities $\mathcal{M}=\{m_1, ..., m_M\}$, let $T_m$ be a modality-specific tokenizer for each modality $m \in \mathcal{M}$. The multimodal transformer model, denoted as $\textbf{W}$, has weight parameters $\theta = (\theta_1, \dots, \theta_d) \in \mathbb{R}^d$  and is divided into three components: the head ($\textbf{W}_h$, modality-specific tokenizers), the body ($\textbf{W}_b$, unified encoder), and the tail ($\textbf{W}_t$, task-specific classifier). Hence, $\textbf{W} = [\textbf{W}_h;\textbf{W}_b;\textbf{W}_t]$, where $\textbf{W}_h$ consists of a set of tokenizers with $\textbf{W}_h=\{T_m\}_{m \in \mathcal{M}}$. 

We split $\textbf{W}$ into a server-side model $\mathcal{F}_S = [\textbf{W}_b;\textbf{W}_t]$ and client-side model $\mathcal{F}_C = \textbf{W}_h$, with $\mathcal{F}_{C_n}$ as the client model for a client $n$. For a client $n$ with $M$ modalities, a single data sample is defined as $z=(\{x_{m_i}\}_i^M, y) \in \mathcal{B}_n$, where $\{x_{m_i}\}_i^M = (x_{m_1}, \dots, \textbf{x}_{m_M})$ represents the multimodal input, and $y$ denotes the label (i.e., the ground truth). The activations computed by $\mathcal{F}_{C_n}$ for a given input $\{x_{m_i}\}_i^M$ are denoted as $\textbf{a}_n$, while the predictions with respect to these activations computed by the server model $\mathcal{F}_S$ are expressed as $\hat{y} = \mathcal{F}_S(\textbf{a}_n)$. Lastly, we define $\mathcal{L}_{C_n}$ as the $n$-th client's loss function, and with $\mathcal{L}_S$ we denote the aggregated server-side loss function, given by $\mathcal{L}_S = \sum_{n \in \mathcal{N}} {\frac{|\mathcal{B}_n|}{|\mathcal{B}|} \mathcal{L}_{C_n}}$.

\subsection{Modality fusion in~\method} \label{method:multimodal_fusion}
Leveraging a unified encoder supports both early and late fusion of modalities. Inspired by~\cite{meta-transformer}, we use modality-specific tokenizers to tokenize inputs and fuse modalities by concatenating their representations into a single joint vector. In early fusion, tokenized modalities are concatenated into a joint vector on the client-side and processed jointly by the server-side encoder. In late fusion, each modality is tokenized and processed independently by the server-side encoder, with their encoded representations concatenated afterward into a single joint vector. Regardless of the fusion type, we apply global average pooling to the multimodal joint vector to obtain the final multimodal embedding, which is then used for the model’s final prediction. \\

\noindent \textbf{Processing multimodal inputs on clients.} For a given client $n$ with a data sample $z=(\{x_{m_i}\}_i^M, y) \in \mathcal{B}_n$, the client extracts the activations $\textbf{a}_n$ as follows: 

\begin{equation}\label{formula:client_activations}
    \resizebox{0.4\textwidth}{!}{
    $
    \textbf{a}_n=\mathcal{F}_{C_n}(\textbf{x}_{m_1, m_2}) =
    \begin{cases} 
        [T_{m_i}(\textbf{x}_{m_i})]_i^M ~~\text{if } \text{fusion} = early \\
        {T_{m_i}(\textbf{x}_{m_i})}_i^M ~~~~~~~\text{otherwise}
    \end{cases}
    $
    }
\end{equation}

\noindent where $T_{m_i}$ is the modality-specific tokenizer for modality $i$. The activations $\textbf{a}_n$ is subsequently send to the server, which in turn computes the prediction $\hat{y}$ with respect to $\textbf{a}_n$, and transmits back to the client. Given the label $y$ for $\textbf{x}_{m_1, m_2}$, the client-side loss is computed as $\mathcal{L}_{C_n}(\hat{y}, y)$ and send to the server to compute the gradients. Lastly, using the received cut-layer gradients from the server, client $n$ then performs backpropagation on its model $\mathcal{F}_{C_n}$. Formally, the client-side objective function is:

\begin{equation}
    \underset{\mathcal{F}_{C_n}}{\text{min}} \quad \mathcal{L}_{C_n}(\hat{y}, y) 
\end{equation}

\noindent \textbf{Server-side training.} We further elaborate on the training process on the server-side. Upon receiving the activations $\textbf{a}_n$ from a client $n$, the server computes its corresponding prediction $\hat{y}$ as follows:

\begin{equation}\label{formula:server_prediction}
    \resizebox{0.4\textwidth}{!}{
    $
    \hat{y}=\mathcal{F}_S(\textbf{a}_n) = 
    \begin{cases}
        \textbf{W}_t(\mathit{GAP}({\textbf{W}_b(\textbf{a}_n))}) ~~\text{if } \text{fusion} = early \\
        \textbf{W}_t(\mathit{GAP}([\textbf{W}_b(\textbf{a}_{n_{m_i}})]_1^M)) ~\text{otherwise}
    \end{cases}
    $
    }
\end{equation}
\vspace{10pt}

\noindent where $\mathit{GAP}$ denotes the global average pooling operator.
The server subsequently sends prediction $\hat{y}$ to client $n$. Note that in the case of late fusion, each modality in $\textbf{a}_n$ is encoded independently using $\textbf{W}_b$, concatenated, and then processed with $\mathit{GAP}$, ultimately leading to the final prediction $\hat{y}$ via $\textbf{W}_t$.

The server computes the global aggregated loss $\mathcal{L}_S$ after receiving $\mathcal{L}_{C_n}$ from all clients $n \in \mathcal{N}$ and performs a single backward pass. This approach reduces server-side computational burden by requiring only one backward pass, regardless of number of clients, and eliminates the need to maintain multiple client sub-models. After backpropagation, the server sends the cut-layer gradients to all clients $n \in \mathcal{N}$. Formally, the server-side objective function is:

\begin{equation}
    \underset{\mathcal{F}_S}{\text{min}} \quad \mathcal{L}_S = \sum_{n \in \mathcal{N}} {\frac{|\mathcal{B}_n|}{|\mathcal{B}|} \mathcal{L}_{C_n}}
\end{equation}

\vspace{10pt}

\subsection{Post-training Model Construction} \label{method:final_model}
After training, the splitted multimodal transformer can be reassembled without restrictions. We impose no limitations in this regard; one approach is to construct a complete model for each client $n$ by concatenating the client-side model $\mathcal{F}_{C_n}$ with the server-side model $\mathcal{F}_S$, forming [$\mathcal{F}_{C_n}; \mathcal{F}_S$]. This approach is suitable for personalization. Alternatively, a single client-side model $\mathcal{F}_{C_{\text{agg}}}$ can be obtained by aggregating all client-side models, following FedAvg~\cite{fl}, which is concatenated with the server-side model $\mathcal{F}_S$, resulting in [$\mathcal{F}_{C_{\text{agg}}}$;$\mathcal{F}_S$]. We adopt the latter approach to directly compare~\method's performance with other FL regimes~\cite{fl,FedCLIP}. Notably, no client-side synchronization is enforced during the training phase.

\section{Experiments} \label{experiments}

\noindent \textbf{Datasets.} We evaluate all combinations of image, audio, and text modality pairs across diverse downstream tasks and datasets. From (image, text) pairs, we use T4SA~\cite{T4SA} (B-T4SA subset) for sentiment analysis, COCO-QA~\cite{COCO-QA} for visual question answering, and MS-COCO~\cite{COCO} and Flickr30K~\cite{Flickr30K} for image-text retrieval. For (image, audio) pairs, we leverage Kinetics-Sounds~\cite{kinetics-sound} and UCF101~\cite{UCF101} for action recognition. Finally, (audio, text) pairs from MELD~\cite{MELD} are used for multimodal emotion classification. For all data splitting procedures, we use a Dirichlet distribution over classes, denoted as $\mathrm{Dir}$($0.1$), following~\cite{li2021federatedlearningnoniiddata}. \\

\noindent \textbf{Models.} Throughout our experiments, we use pre-trained weights from Meta-Transformer~\cite{meta-transformer} based on ViT-B/16~\cite{ViT}, fine-tuning the complete encoder for image-text retrieval and the last six encoder blocks for classification tasks, unless stated otherwise. In Figure~\ref{fig:ablation_2__2__scaling_encoder}, where we explore various ViT encoder depths (Tiny, Small, Large, and Huge), we consistently fine-tune the latter half of the encoder blocks using pre-trained weights from~\cite{ViT}. Late fusion was employed in all cases except for COCO-QA, T4SA, and MELD, where early fusion yields the best performance (see Table~\ref{table:ablation_3}). For image-text retrieval we use the contrastive loss from~\cite{ONE_PEACE}, while for all classification tasks we employ Cross-Entropy loss. \\

\noindent \textbf{Modality Tokenization Details.} To obtain uniform representations, we use modality-specific tokenizers similar to~\cite{meta-transformer,imagebind}. Images are ``\textit{patchified}" following ViT~\cite{ViT}, text is tokenized similar to CLIP~\cite{clip}, and raw audio is converted to spectrograms and subsequently ``\textit{patchified}" following AST~\cite{AST}. Additionally, for audio and/or vision modalities, a  $\mathrm{cls}$ (classification) token was prepended to the tokenized representation to capture a global representation of the entire input modality, allowing the concatenation of only the $\mathrm{cls}$ token(s) instead of the complete embedding vector during late fusion. \\
    
\noindent \textbf{Baselines.} We evaluate~\method~against centralized fine-tuning, standard FedAvg~\cite{fl} and FedCLIP~\cite{FedCLIP}, where lightweight modality-specific adapters are trained on a frozen backbone. The evaluation considers three criteria: task-specific performance (e.g., accuracy for classification and recall for retrieval tasks), client-side communication overhead (average MB transmitted per client in $1$ epoch), and computation overhead (measured in G-FLOPs per input sample and client-side trainable parameters). For rigorous evaluation, we run each experiment $3$ times, using a unique seed for each run, reporting the mean test accuracy.

\begin{table*}[!t]
    \centering \small
    \resizebox{0.85\textwidth}{!}{%
    \begin{tabular}{clcccccccc}
        \toprule
        & \multirow{2}{*}{\textbf{Method}} 
        & \multicolumn{2}{c}{\cellcolor{blue!5}\includegraphics[height=0.55cm]{./assets/image_text.png} \textbf{(V+T)}}
        & \multicolumn{2}{c}{\cellcolor{green!5}\includegraphics[height=0.55cm]{./assets/image_audio.png} \textbf{(V+A)}}
        & \cellcolor{yellow!5}\makecell{\includegraphics[height=0.55cm]{./assets/audio_text.png} \textbf{(A+T)} \vspace{2pt}}
        & \multirow{2}{*}{\makecell{\textbf{Avg. Tr.}\\ \textbf{Params} (\textbf{M})}}
        & \multirow{2}{*}{\makecell{\textbf{Avg. Comp.}\\(\textbf{GFlops})}} \\ 
        \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-7}

        & & \textcolor{bluet5}{\textbf{COCO-QA}} & \textcolor{bluet5}{\textbf{T4SA}} 
        & \textcolor{greent5}{\textbf{Kinetics-Sounds}} & \textcolor{greent5}{\textbf{UCF101}} 
        & \textcolor{yellowt5}{\textbf{MELD}} & & & \\ 
        \midrule

        \multicolumn{2}{c}{\textbf{Centralized}} & 61.7 $\pm$ 0.1 & 83.0 $\pm$ 0.1 & 81.8 $\pm$ 0.2 & 91.3 $\pm$ 0.2 & 61.4 $\pm$ 0.4 & 43.7 & 25.8 \\ \cmidrule(lr){1-10}
        \multirow{3}{*}{\textbf{N=25}} 
            & \textbf{FedAvg}         & \textbf{59.1 $\pm$ 0.2} & \textbf{76.2 $\pm$ 0.6} & \textbf{78.2 $\pm$ 0.3} & \textbf{90.8 $\pm$ 0.6} & 57.4 $\pm$ 0.6 & 43.7 & 25.8 \\
            & \textbf{FedCLIP}        & 32.0 $\pm$ 0.2 & 55.8 $\pm$ 5.0 & 70.1 $\pm$ 0.4 & 70.0 $\pm$ 3.9 & 55.3 $\pm$ 0.2 & 3.1 & 25.8 \\
            & \textbf{MPSL (\textit{Ours})}     & 53.3 $\pm$ 0.4 & 73.3 $\pm$ 0.0 & 72.2 $\pm$ 1.6 & 90.4 $\pm$ 0.5 & \textbf{59.4 $\pm$ 0.5} & \textbf{1.0} & \textbf{0.1} \\

        \midrule

        \multirow{3}{*}{\textbf{N=100}} & 
            \textbf{FedAvg}         & \textbf{53.5 $\pm$ 0.2} & 72.4 $\pm$ 1.6 & \textbf{71.4 $\pm$ 0.4} & \textbf{89.8 $\pm$ 0.9} & 48.9 $\pm$ 0.0 & 43.7 & 25.8 \\
            & \textbf{FedCLIP}        & 13.1 $\pm$ 2.6 & 45.0 $\pm$ 2.8 & 63.5 $\pm$ 0.8 & 56.4 $\pm$ 1.2 & 50.7 $\pm$ 0.7 & 3.1 & 25.8 \\
            & \textbf{MPSL (\textit{Ours})}     & 46.2 $\pm$ 0.5 & \textbf{72.5 $\pm$ 1.2} & 67.0 $\pm$ 1.1 & 89.3 $\pm$ 1.0 & \textbf{58.2 $\pm$ 0.9} & \textbf{1.0} & \textbf{0.1} \\

        \bottomrule
    \end{tabular}%
    }
    \vspace{5pt}
    \caption{Performance evaluation of classification tasks. Computation overhead is reported as the average number of trainable parameters (in millions) and GFlops on the client-side. V: Vision, A: Audio, T: Text.}
    \label{tab:main_res}
\end{table*}

\begin{table*}[!t]
    \centering \small
    \resizebox{0.77\textwidth}{!}{%
    \begin{tabular}{clccccccc}
        \toprule
        & \multirow{2}{*}{\textbf{Method}}
        & \multicolumn{2}{c}{\textcolor{bluet5}{\textbf{MS-COCO}}}
        & \multicolumn{2}{c}{\textcolor{bluet5}{\textbf{Flickr30K}}}
        & \multirow{2}{*}{\makecell{\textbf{Avg. Tr.}\\\textbf{Params} (\textbf{M})}}
        & \multirow{2}{*}{\makecell{\textbf{Avg. Comp.}\\(\textbf{GFlops})}} \\ 
        \cmidrule(lr){3-4} \cmidrule(lr){5-6}
        & & \textbf{\textit{R@1}} & \textbf{\textit{R@10}} & \textbf{\textit{R@1}} & \textbf{\textit{R@10}} & & & \\
        \midrule
        \multirow{4}{*}{\textbf{\textit{Image-to-Text}}} 
            & \textbf{Centralized} & 56.8 $\pm$ 2.4 & 77.8 $\pm$ 1.9 & 66.8 $\pm$ 5.3 & 86.0 $\pm$ 2.9 & 85.8 & 8.65 \\ \cmidrule(lr){2-9}
            & \textbf{FedAvg} & 1.5 $\pm$ 0.1 & 5.2 $\pm$ 0.3 & 11.2 $\pm$ 1.7 & 28.0 $\pm$ 3.4 & 85.8 & 8.65 \\
            & \textbf{FedCLIP} & 0.1 $\pm$ 0.0 & 0.3 $\pm$ 0.1 & 0.1 $\pm$ 0.0 & 0.9 $\pm$ 0.3 & 2.52 & 8.65 \\
            & \textbf{MPSL (\textit{Ours})} & \textbf{21.1 $\pm$ 0.8} & \textbf{43.5 $\pm$ 1.2} & \textbf{26.0 $\pm$ 0.5} & \textbf{49.2 $\pm$ 1.2} & \textbf{0.74} & \textbf{0.1} \\
        \midrule
        \multirow{4}{*}{\textbf{\textit{Text-to-Image}}}
            & \textbf{Centralized} & 54.3 $\pm$ 2.1 & 92.1 $\pm$ 1.0 & 62.4 $\pm$ 5.2 & 95.2 $\pm$ 1.7 & 85.8 & 8.65 \\ \cmidrule(lr){2-9}
            & \textbf{FedAvg} & 1.5 $\pm$ 0.1 & 10.4 $\pm$ 0.4 & 9.3 $\pm$ 1.2 & 42.9 $\pm$ 3.6 & 85.8 & 8.65 \\
            & \textbf{FedCLIP} & 0.1 $\pm$ 0.0 & 0.9 $\pm$ 0.2 & 0.2 $\pm$ 0.0 & 1.4 $\pm$ 0.1 & 2.52 & 8.65 \\
            & \textbf{MPSL (\textit{Ours})} & \textbf{17.3 $\pm$ 0.2} & \textbf{60.3 $\pm$ 1.2} & \textbf{21.6 $\pm$ 0.9} & \textbf{65.5 $\pm$ 1.1} & \textbf{0.74} & \textbf{0.1} \\
        \bottomrule
    \end{tabular}%
    }
    \vspace{5pt}
    \caption{Performance evaluation of retrieval tasks with 100 clients. Computation overhead is reported as the average number of trainable parameters (in millions) and GFlops on the client-side.}
    \label{tab:retrieval_res}
\end{table*}

%\noindent \textbf{Results} 

\subsection{\method~Evaluation}

%\noindent{Model Performance} 
We evaluate~\method~on $7$ datasets and compare its performance against two distributed baselines in Tables~\ref{tab:main_res} and~\ref{tab:retrieval_res}. As shown in Table~\ref{tab:main_res}, in classification tasks, ~\method~consistently matches FedAvg across all modality pairs and tasks while significantly reducing client-side computation. By offloading the computationally intensive model components to the server, \method~achieves over $250$-fold reduction in GFLOPs requiring only $1.0$M trainable parameters~\textendash~a 97.71\% decrease compared to FedAvg. Notably,~\method~outperforms FedAvg in certain settings, such as MELD, demonstrating its potential for both efficiency and performance. Alternatively, in image-text retrieval, as shown in Table~\ref{tab:retrieval_res}, all $3$ distributed ML approaches exhibit a notable performance drop compared to their centralized counterparts. Despite this,~\method~outperforms both FL baselines while maintaining client-side computational efficiency similar to that observed in classification tasks. We further elaborate on the performance gap relative to centralized settings for image-text retrieval tasks in Section~\ref{ablation_mini_batch_size}.

%\noindent{Communication Cost.} 
In terms of communication overhead,~\method~relies solely on the size of communicated smashed data (i.e., activations), rather than the number of trainable parameters, as in most FL regimes. As depicted in Figure~\ref{fig:main_results__comm_overhead_w_scaling}, while FedAvg demonstrates greater communication efficiency for smaller ViT encoder depths (e.g., ViT-Tiny with $6$ blocks and ViT-Small with $8$ blocks), this advantage diminishes with larger encoder architectures, which are common in multimodal transformers~\cite{imagebind}. Moreover, FedCLIP~\textendash~which utilizes lightweight adapters atop a frozen backbone~\textendash~achieves the lowest communication cost in our evaluations; however, its performance remains limited, with~\method~consistently outperforming it across all tasks while requiring significantly fewer GFLOPs. Consequently,~\method~emerges as a promising alternative for both computation and communication-efficient decentralized multimodal learning, particularly for resource-constrained edge devices.

\begin{figure}[t]
    \centering \small
    \includegraphics[width=0.95\linewidth]{./assets/finals/comm_cost.png}
    \caption{\small{Impact of encoder depth on client-side communication overhead compared to FedAvg.}}
    \label{fig:main_results__comm_overhead_w_scaling}
\end{figure}

\subsection{\method~Ablations}\label{ablation}
Here, we delve into the design choices underlying~\method~and rigorously assess their impact on diverse downstream tasks. This analysis sheds light on the applicability of downstream tasks for multimodal SL, pinpoints those that can be effectively trained using~\method, and uncovers the critical factors that underpin their performance. \\

\subsubsection{Batch size effect\label{ablation_mini_batch_size}} Batch size plays a critical role in modality alignment for multimodal learning, where smaller sizes can cause feature collapse in the unified embedding space~\cite{clip,duan2022multimodalalignmentusingrepresentation}. To examine this in SL settings, we evaluate the impact of batch size in~\method~across downstream tasks (classification and image-text retrieval) and modality pairs. By keeping all SL settings fixed and varying the batch size, we investigate whether similar effects occur, with results presented in Table~\ref{table:ablation_1__result_with_25_clients}. From Table~\ref{table:ablation_1__result_with_25_clients}, it can be observed that batch size significantly impacts model performance, with sensitivity varying by task. For classification tasks like COCO-QA and UCF101, increasing the batch size from $50$ to $500$ improves accuracy by approximately 15\%, while T4SA shows a more modest gain of $6\%$. 

In contrast, image-text retrieval tasks show the most substantial improvements, with recall increasing by $\approx$40\% on average as batch sizes grow from $50$ to $150$ and $200$, underscoring the importance of larger batch sizes for retrieval tasks. Intuitively, these tasks depend on maximizing the distance between unrelated embeddings and minimizing it for related ones; thus, proper alignment is essential to distinguish relevant embeddings from irrelevant ones in the shared embedding space. Larger batch sizes improve this alignment by introducing diverse examples within each batch, reducing feature collapse and significantly enhancing retrieval performance. We further depict this in Figure~\ref{fig:ablation_1__embedding_visualization}, where we visualize the embedding space for image and text modalities at batch sizes of $50$ and $200$, along with centralized fine-tuning. Larger batch sizes result in less dense clusters, indicating better alignment through increased separation among dissimilar embeddings. However, larger batch sizes require clients to hold more samples, which makes tasks where edge devices hold a limited number of labeled samples less practical for SL settings. Sequential techniques to simulate large batch sizes could alleviate this limitation, but they fall outside the scope of this work. \\

% However, larger batch sizes require clients to hold a larger number of samples, making tasks for which edge devices hold a limited number of labeled samples less practical for SL settings. Sequential techniques to simulate large batch sizes could alleviate this limitation, but they fall outside the scope of this work.

% However, the elevated computational demands of larger batch sizes during tokenization make image-text retrieval tasks less practical for SL settings on resource-constrained edge devices. Sequential techniques to simulate large batch sizes could alleviate this limitation, but they fall outside the scope of this work.


\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.495\linewidth}
        \centering \small
        \includegraphics[width=\linewidth, trim=100 100 100 100, clip]{./assets/ablation_1__coco_retrieval__text.PNG}
        \caption{\small{MS-COCO - \textit{Text}}} \label{ablation_1__subfig:a}
    \end{subfigure}
    \begin{subfigure}[b]{0.495\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=100 100 100 100, clip]{./assets/ablation_1__coco_retrieval__image.PNG}
        \caption{\small{MS-COCO - \textit{Image}}} \label{ablation_1__subfig:c}
    \end{subfigure}
    \vspace{-5pt}
    \begin{subfigure}[b]{0.495\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=100 100 100 100, clip]{./assets/ablation_1__flickr30k__text.PNG}
        \caption{\small{Flickr30K - \textit{Text}}} \label{ablation_1__subfig:b}
    \end{subfigure}
    \begin{subfigure}[b]{0.495\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=100 100 100 100, clip]{./assets/ablation_1__flickr30k__image.PNG}
        \caption{\small{Flickr30K - \textit{Image}}} \label{ablation_1__subfig:d}
    \end{subfigure}
    \caption{\small{Visualization of the effect of batch size on embedding spaces after PCA for Flickr30K and MS-COCO with $N$=$25$. \textcolor{blue}{Blue} represents embeddings from centralized models, while \textcolor{darkgreen}{green} and \textcolor{red}{red} indicate~\method’s embeddings with batch size of $50$ and $200$, respectively.}}
    \label{fig:ablation_1__embedding_visualization}
\end{figure}

\begin{table*}[t!]
    \centering  \small
    \resizebox{0.67\linewidth}{!}{%
        \begin{tabular}{lccccccc}
            \toprule
            \textbf{Batch size} & 50 & 100 & 150 & 200  & 300 & 400 & 500 \\ 
            \midrule
            \textbf{COCO-QA}~\cite{COCO-QA} 
            & 69.4 & 73.6 & \textendash & 78.3 & 80.9 & 83.0 & \textbf{85.3} \\
            \textbf{T4SA}~\cite{T4SA} 
            & 83.4 & 87.8 & \textendash & 85.3 & 87.8 & \textbf{89.4} & 86.8 \\
            \textbf{UCF101}~\cite{UCF101}  
            & 84.5 & 95.4 & \textendash & 97.6 & \textbf{100.0} & \textbf{100.0} & 98.5 \\
            \midrule
            \textbf{Flickr30K}~\cite{Flickr30K} 
            & 37.9 & 69.5 & 63.0 & \textbf{69.6} & \textendash & \textendash & \textendash \\
            \textbf{MS-COCO}~\cite{COCO} 
            & 21.3 & 53.2 & 64.1 & \textbf{69.7} & \textendash & \textendash & \textendash \\ \bottomrule
        \end{tabular}%
    }
    \caption{\small{Impact of batch size on model performance across tasks with $N$=$25$. We report normalized accuracy and top-1 recall vs. centralized fine-tuning for classification and retrieval tasks, respectively. Entries with ``\textendash'' reflect infeasible data partitioning due to dataset size.}}
    \label{table:ablation_1__result_with_25_clients}
\end{table*}

\subsubsection{Trainable parameters\label{ablation_2}} While our experiments primarily use ViT-B, the multimodal transformers utilized in multimodal learning have substantial variations in encoder depths, spanning from millions~\cite{meta-transformer,vatt} to a billion parameters~\cite{imagebind}. This raises the question of how well our findings generalize to other, often larger, encoders. To address this, we conduct experiments by varying both the number of fine-tuned blocks within the ViT-B encoder and the total encoder depth. \\

\noindent \textit{Number of Fine-tuned Blocks.} We conduct experiments across all tasks, varying the number of trainable parameters by fine-tuning an increasing number of encoder blocks while keeping other SL settings fixed. Notably, this increase in trainable parameters is limited to the server-side, imposing no additional computational burden on clients. As shown in Table~\ref{table:ablation_2__1}, increasing the number of trainable parameters does not exhibit a consistent correlation with task-specific performance, as results vary across tasks. However, our findings indicate a minimum threshold of fine-tuned blocks is necessary to achieve sufficient performance, with fine-tuning a single block consistently resulting in poor performance. This is also evident in a more elaborate analysis in COCO-QA, presented in Figure~\ref{fig:ablation_2__1__coco_qa__all_blocks}, where performance improves sharply from 1 to 3 blocks but plateaus beyond that point. \\

\begin{figure}[!t]  
    \centering \small
    \includegraphics[width=0.95\linewidth]{./assets/finals/vit_blocks.png}
    \caption{\small{Impact of fine-tuned ViT blocks on model performance for COCO-QA.}}\label{fig:ablation_2__1__coco_qa__all_blocks}
\end{figure}

\begin{figure}[t]
    \centering \small
    \includegraphics[width=0.95\linewidth]{./assets/finals/vit_sizes.png}
    \caption{\small{Impact of encoder depth on model performance for COCO-QA.}}
    \label{fig:ablation_2__2__scaling_encoder}
\end{figure}

\begin{table}[t!]
    \centering \small
    \resizebox{0.97\linewidth}{!}{%
        \begin{tabular}{llllllll}
            \toprule
            \multirow{2}{*}{\textbf{\# ViT blocks}} & \multicolumn{3}{c}{\textbf{N $=$ 25}} & & \multicolumn{3}{c}{\textbf{N $=$ 100}} \\ 
            \cmidrule(lr){2-4} \cmidrule(lr){6-8}
             & \textit{~1} & \textit{~6} & \textit{~12} & & \textit{~1} & \textit{~6} & \textit{~12} \\
            \midrule
            \textbf{COCO-QA}~\cite{COCO-QA} & 74.8 & 85.4 & \textbf{88.8} &  & 66.7 & \textbf{74.5} & 70.4 \\
            \textbf{T4SA}~\cite{T4SA} & 80.7 & 88.5 & \textbf{90.9} &  & 75.7 & 89.0 & \textbf{89.6} \\
            \textbf{Kinetics-Sounds}~\cite{kinetics-sound} & 80.1 & 90.4 & \textbf{91.3} &  & 64.0 & \textbf{78.4} & 71.4 \\
            \textbf{UCF101}~\cite{UCF101} & 94.6 & 98.5 & \textbf{98.7} &  & 92.7 & 96.5 & \textbf{98.1} \\
            \textbf{MELD}~\cite{MELD} & 90.2 & 95.6 & \textbf{96.0} &  & 85.8 & 95.6 & \textbf{96.4} \\
            \midrule
            \textbf{Flickr30K}~\cite{Flickr30K} & \multicolumn{1}{c}{-} & \textbf{62.5} & 56.8 &  & \textendash & 36.1 & \textbf{36.4} \\
            \textbf{MS-COCO}~\cite{COCO} & 27.6 & 54.5 & \textbf{68.5} &  & 7.0  & 24.1 & \textbf{37.0} \\ 
            \bottomrule
        \end{tabular}%
    }
    \caption{\small{Performance across tasks when fine-tuning the last 1, 6, or all ViT blocks, with 25 and 100 clients. Results report test accuracy and top-1 Image-to-Text recall (for Flickr30K and MS-COCO), normalized to centralized baselines.}}
    \label{table:ablation_2__1}
\end{table}

% /Scaling the model/Total model size
\noindent \textit{Encoder Depth.} We evaluate the impact of encoder depth on task-specific performance in the SL setting by scaling the encoder across all ViT~\cite{ViT} variations: ViT-Ti, ViT-S, ViT-B, ViT-L, and ViT-H, with 6, 22, 85, 303, and 630 million parameters, respectively. Based on the observations in Figure~\ref{fig:ablation_2__1__coco_qa__all_blocks}, we fine-tune the latter half of the encoder blocks for each variation: the last 6 blocks for ViT-Ti, ViT-S, and ViT-B, and the last 12 and 16 blocks for ViT-L and ViT-H, respectively, while keeping all other SL parameters fixed. Due to computational constraints with ViT-H, we use a fixed batch size of 100 across all variations, unlike prior experiments that optimize batch size for performance. This ensures consistent configurations during evaluation rather than achieving optimal performance for each model. Our findings in Figure~\ref{fig:ablation_2__2__scaling_encoder} reveal that SL performance remains stable even as model size significantly increases. For example, scaling from ViT-B (85 million parameters) to ViT-H (630 million parameters) increases trainable parameters from 43 million to 315 million, yet there is no notable drop in performance, highlighting~\method's scalability and robustness of larger models in SL settings. \\


\subsubsection{Fusion Type}\label{ablation_3} We examine the impact of fusion type (i.e., \textit{early vs. late}) on model performance across tasks, aiming to identify whether a task-agnostic modality fusion mechanism exists in SL settings. To this end, we conduct experiments with early and late fusion across all tasks requiring fusion (i.e., classification tasks) using $100$ clients ($N$=$100$) and report our findings in Table~\ref{table:ablation_3}.

We observe that the choice of fusion type significantly impacts model performance in the SL setting; however, no single fusion approach is superior across all tasks. Instead, the effectiveness of modality fusion is task-dependent. For instance, tasks involving vision-text (V+T) modalities generally perform better with early fusion, whereas vision-audio (V+A) tasks favor late fusion — patterns that hold consistently across both centralized and SL settings. Notably, in some cases, we observe a performance improvement exceeding 20\% when switching between fusion types. Conversely, for audio-text (A+T) modalities, the fusion choice has a less pronounced effect on performance. Thus, while fusion type undeniably impacts task performance, its effect is highly context-dependent, varying notably across tasks and modality pairs in the SL setting.

\begin{table}[t]
    \centering \small
    \resizebox{0.95\linewidth}{!}{%
        \begin{tabular}{clccccc}
        \toprule
        \multirow{2}{*}{\textbf{Modality}} & \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{2}{c}{\textbf{Centralized}} &  & \multicolumn{2}{c}{\textbf{\method} (Ours)} \\ 
        \cmidrule(lr){3-4} \cmidrule(lr){6-7} 
        & & \textit{early} & \textit{late} & & \textit{early} & \textit{late} \\ 
        \midrule
        \textbf{V+T} & \textbf{COCO-QA}~\cite{COCO-QA} & \textbf{61.6} & 60.7 &  & \textbf{46.1} & 45.3 \\
        \textbf{V+T} & \textbf{T4SA}~\cite{T4SA} & \textbf{83.0} & 81.9 & & \textbf{62.4} & 45.2 \\
        %\midrule
        \textbf{V+A} & \textbf{Kinetics-Sounds}~\cite{kinetics-sound} & 80.7 & \textbf{81.7} & & 49.9 & \textbf{66.2} \\
        \textbf{V+A} & \textbf{UCF101}~\cite{UCF101} & 90.2 & \textbf{90.5} &  & 86.5 & \textbf{88.1} \\ 
        %\midrule
        \textbf{A+T} & \textbf{MELD}~\cite{MELD} & 62.1 & \textbf{62.5} &  & \textbf{58.9} & 56.4 \\
        \bottomrule
    \end{tabular}%
    }
    \caption{\small{Impact of \textit{early vs. late} modality fusion on model performance across classification tasks. We report test set accuracy. V: Vision, A: Audio, T: Text.}}
    \label{table:ablation_3}
\end{table}

\section{Conclusions} \label{conclusion}
We introduce \method, a Multimodal Parallel Split Learning framework that can be used to fine-tune multimodal transformers in a distributed manner with lightweight client computational burden. It uses an aggregated loss function of local client losses and a single unified backward pass on the server to eliminate the need for label sharing, synchronization of client-side models, and maintaining per-client sub-models on the server, alongside employing lightweight modality-specific tokenizers on the client-side to reduce computational overhead. Our experiments across multiple datasets, types of tasks, and pairs of modalities demonstrate \method's effectiveness in fine-tuning such models with comparable or superior performance to FL regimes while significantly decreasing client-side computational burden, and achieving superior scalability in communication cost with model growth.

% \section{Appendix} \label{appendix}
% \hl{TODO: Add ablation 1 graphs showing increase of avg distance in embedding space when the mbs increases. If so, also very briefly mention this in the text in the respective section.}

% \begin{table}[h!]
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{llllllllllllllllllll}
% \textbf{Number of clients} &  &  & \multicolumn{2}{c}{25} &  &  &  &  &  & \multicolumn{2}{c}{50} &  &  &  &  & \multicolumn{2}{c}{100} &  &  \\ \cline{2-7} \cline{9-14} \cline{16-20} 
% \textbf{Batch size} & 50 & 100 & 200 & 300 & 400 & 500 &  & 50 & 100 & 200 & 300 & 400 & 500 &  & 100 & 200 & 300 & 400 & 500 \\ \hline
% \textbf{COCO-QA} & 69.4 & 73.6 & 78.3 & 80.9 & 83.0 & \textbf{85.3} &  & 61.5 & 67.1 & 75.3 & 78.0 & 81.1 & \textbf{83.3} &  & 62.7 & 69.4 & 68.7 & 73.4 & \textbf{74.7} \\
% \textbf{T4SA} & 83.4 & 87.8 & 85.3 & 87.8 & \textbf{89.4} & 86.8 &  & 85.7 & 84.8 & 76.8 & \textbf{89.4} & 86.7 & 84.0 &  & 85.3 & 81.9 & 85.5 & 87.3 & \textbf{87.4} \\
% \textbf{UCF101} & 84.5 & 95.4 & 97.6 & \textbf{100.0} & \textbf{100.0} & 98.5 &  & 82.9 & 95.0 & 96.1 & \textbf{98.2} & 98.0 & 98.1 &  & 95.2 & 96.4 & 99.3 & \textbf{100.0} & 96.5 \\ \hline
% \end{tabular}%
% }
% \caption{\textbf{Performance on classification tasks.} We show the performance ... }
% \label{table:ablation_1__classification_tasks_results}
% \end{table}

% \begin{table}[h!]
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{lllllllllllll}
% \textbf{Number of clients} & \multicolumn{4}{c}{25} &  & \multicolumn{4}{c}{50} &  & \multicolumn{2}{c}{100} \\ \cline{2-5} \cline{7-10} \cline{12-13} 
% \textbf{Batch size} & 50 & 100 & 150 & 200 &  & 50 & 100 & 150 & 200 &  & 100 & 200 \\ \hline
% \textbf{Flickr30K} & 37.9 & 69.5 & 63.0 & \textbf{69.6} &  & 0.0 & 38.2 & 37.6 & \textbf{59.5} &  & 0.0 & \textbf{35.6} \\
% \textbf{MS-COCO} & 21.3 & 53.2 & 64.1 & \textbf{69.7} &  & 0.0 & 30.7 & 45.2 & \textbf{54.9} &  & 0.0 & \textbf{33.2} \\ \hline
% \end{tabular}%
% }
% \caption{\textbf{Performance on image-text retrieval task.} We show the performance ... }
% \label{table:ablation_1__image_text_retrieval_task_results}
% \end{table}

% \hl{TODO: Add to table 4 (or a new table) with the average values for ablation 1 (mbs) Flickr30K for the two or three seeds we have. } % kunnen in appendix toevoegen dat flickr30k meer hevig/prone was aan verschillende performance to data distribution. In appendix hiervoor dan specifiek een table laten zien met de average values van de 2 a 3 seeds

\clearpage
\bibliographystyle{named}
\bibliography{refs}

\end{document}

