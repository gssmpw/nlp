\section{Related Work}
\label{related_work}

\subsection{Split Learning}
Split Learning (SL) was introduced to overcome limitations of Federated Learning (FL), particularly the high computational demands on resource-constrained edge devices. By partitioning the model at a designated cut-layer, SL enables edge devices to process initial layers locally and send intermediate activations (smashed data) to a server for forward and backward propagation. Although this approach alleviates client-side computational burdens, SL requires sharing labels; impeding label privacy \cite{sl}. To address this, variations like U-shaped SL were proposed to ensure that both input features and labels remain on the client-side by partitioning the model into three segments: client-head, server-body, and client-tail \cite{sl,Shuffled-Transformer,MaskSL}. Despite these advancements, the sequential nature of vanilla and U-shaped SL imposed scalability limitations, prompting the development of Parallel Split Learning (PSL). PSL eliminates sequential dependencies by enabling multiple clients to train in parallel, significantly reducing training latency~\cite{privacy-sensitive-psl,CutMixSL,DP-CutMixSL,FedBone,LocalSplitFed}. Yet, early PSL works such as LocalSplitFed~\cite{LocalSplitFed}~require synchronization of client-side models~\textendash~thereby introducing communication overhead~\textendash~and often acquire parallelism by maintaining per-client sub-models on the server. The latter becomes infeasible when employing computationally intensive models with a large number of clients. However, subsequent PSL solutions acquire parallelism without this per-client sub-model constraint and introduce various optimizations for PSL, such as tackling the client decoupling problem identified in SGLR~\cite{sglr}~by using gradient aggregation techniques~\cite{sglr,Mix2SFL,EPSL}, and exploring single-pass loss aggregation mechanisms~\cite{sasl}~to minimize redundant computations and improve scalability. The advancements in PSL position the distributed learning framework as a viable alternative for the distributed training of computationally intensive models. Despite this, existing PSL frameworks primarily focus on unimodal tasks, leaving the integration of multimodal data largely unexplored. This gap highlights the need for advancements in PSL to effectively accommodate the unique challenges of training multimodal models in distributed settings.

% Split Learning (SL) was introduced to overcome limitations of Federated Learning (FL), particularly the high computational demands on resource-constrained edge devices. By partitioning the model at a designated cut-layer, SL enables edge devices to process initial layers locally and send intermediate activations (smashed data) to a server for forward and backward propagation. Although this approach alleviates client-side computational burdens, SL requires sharing labels; impeding label privacy \cite{sl}. To address this, variations like U-shaped SL were proposed to ensure that both input features and labels remain on the client-side by partitioning the model into three segments: client-head, server-body, and client-tail \cite{sl,Shuffled-Transformer,MaskSL}. Despite these advancements, the sequential nature of vanilla and U-shaped SL imposed scalability limitations, prompting the development of Parallel Split Learning (PSL). PSL eliminates sequential dependencies by enabling multiple clients to train in parallel, significantly reducing training latency and positioning SL as a viable alternative for computationally intensive models~\cite{privacy-sensitive-psl,CutMixSL,DP-CutMixSL,FedBone}. Subsequent PSL solutions introduced various optimizations to tackle server-side computation overhead and client-server imbalance. For example, gradient aggregation techniques~\cite{sglr,Mix2SFL,EPSL} and single-pass loss aggregation mechanisms~\cite{sasl} have been explored to minimize redundant computations and improve scalability. Nevertheless, existing PSL frameworks primarily focus on unimodal tasks, leaving the integration of multimodal data largely unexplored. This gap highlights the need for advancements in PSL to effectively accommodate the unique challenges of training multimodal models in distributed settings.

\subsection{Multimodal Architectures}
The emergence of multimodal transformers has revolutionized multimodal learning by facilitating the effective combination of diverse data modalities, including images, audio, and text. Here, models can be categorized into two main architectures: those using modality-specific encoders and those employing a single unified encoder. Models like CLIP~\cite{clip} use distinct encoders for each modality trained jointly with contrastive learning to align modality-specific embeddings into a shared space, demonstrating strong zero-shot transfer capabilities for tasks like image-text retrieval. Building on this, ImageBind~\cite{imagebind} supports multiple modalities by employing modality-specific tokenizers and encoders, generalizing across unseen modality pairs through pre-training on image-paired data. However, its reliance on separate encoders results in a large model size of $1.2$B parameters, limiting its suitability for resource-constrained environments~\cite{imagebind}. Similarly, ViT-LENS~\cite{ViT-LENS} aligns embeddings using lightweight modality-specific adapters with a frozen ViT encoder~\cite{ViT}, reducing pre-training data requirements; yet retaining architectural complexity and size.

In contrast, unified encoder approaches like VATT~\cite{vatt} and Meta-Transformer~\cite{meta-transformer} aim to share parameters across all supported modalities, enabling more scalable designs. VATT uses a modality-agnostic encoder with shared weights to process video, audio, and text, reducing parameter counts compared to modality-specific designs~\cite{vatt}. Meta-Transformer extends this approach, employing a single encoder across twelve modalities using lightweight modality-specific tokenizers, showcasing promising results for parameter-efficient multimodal learning~\cite{meta-transformer}. Nonetheless, such models are predominantly designed for centralized training and remain computationally intensive for on-device learning. \\

\noindent \textbf{Distributed Multimodal Transformers.} Distributed training of multimodal transformers has been explored in approaches like FedCLIP~\cite{FedCLIP}, which uses lightweight adapters on top of a frozen backbone in an FL framework. However, the FL framework still requires clients to execute the frozen encoder backbone, limiting its applicability to resource-constrained edge devices. PSL offers a promising alternative; yet its application to multimodal transformers remains largely underexplored.
%-- apart from BiCSL~\cite{BiCSL}, which focuses on self-supervised pre-training for Visual Question Answering (VQA). 
Our work extends PSL to multimodal transformers, addressing client-side computational efficiency and modality alignment while overcoming prior PSL limitations like label sharing.