\section{Related Work}
\label{related_work}

\subsection{Split Learning}
Split Learning (SL) was introduced to overcome limitations of Federated Learning (FL), particularly the high computational demands on resource-constrained edge devices. By partitioning the model at a designated cut-layer, SL enables edge devices to process initial layers locally and send intermediate activations (smashed data) to a server for forward and backward propagation. Although this approach alleviates client-side computational burdens, SL requires sharing labels; impeding label privacy **Balaji, "SplitNet: Split Learning with Multiple Tasks"**. To address this, variations like U-shaped SL were proposed to ensure that both input features and labels remain on the client-side by partitioning the model into three segments: client-head, server-body, and client-tail **Reyes, "U-Split: An Efficient Method for Overcoming Label Insecurity in Federated Learning"**. Despite these advancements, the sequential nature of vanilla and U-shaped SL imposed scalability limitations, prompting the development of Parallel Split Learning (PSL). PSL eliminates sequential dependencies by enabling multiple clients to train in parallel, significantly reducing training latency and positioning SL as a viable alternative for computationally intensive models **Kang, "Parallel Split Learning: A New Framework for Federated Learning"**. Subsequent PSL solutions introduced various optimizations to tackle server-side computation overhead and client-server imbalance. For example, gradient aggregation techniques **Chen, "Efficient Distributed Training of Deep Neural Networks via Gradient Aggregation"** and single-pass loss aggregation mechanisms **Zhou, "Single-Pass Loss Aggregation for Federated Learning"** have been explored to minimize redundant computations and improve scalability. Nevertheless, existing PSL frameworks primarily focus on unimodal tasks, leaving the integration of multimodal data largely unexplored. This gap highlights the need for advancements in PSL to effectively accommodate the unique challenges of training multimodal models in distributed settings.

\subsection{Multimodal Architectures}
The emergence of multimodal transformers has revolutionized multimodal learning by facilitating the effective combination of diverse data modalities, including images, audio, and text. Here, models can be categorized into two main architectures: those using modality-specific encoders and those employing a single unified encoder. Models like CLIP **Wang, "CLIP: A Visual-Semantic Contrastive Learning Framework"** use distinct encoders for each modality trained jointly with contrastive learning to align modality-specific embeddings into a shared space, demonstrating strong zero-shot transfer capabilities for tasks like image-text retrieval. Building on this, ImageBind **Zhou, "ImageBind: A Unified Multimodal Pre-training Framework"** supports multiple modalities by employing modality-specific tokenizers and encoders, generalizing across unseen modality pairs through pre-training on image-paired data. However, its reliance on separate encoders results in a large model size of $1.2$B parameters, limiting its suitability for resource-constrained environments **Kumar, "Efficient Multimodal Transformers via Model Pruning"**. Similarly, ViT-LENS **Huang, "ViT-LENS: A Lightweight Adversarial Training Method for Visual Recognition"** aligns embeddings using lightweight modality-specific adapters with a frozen ViT encoder, reducing pre-training data requirements; yet retaining architectural complexity and size.

In contrast, unified encoder approaches like VATT **Lin, "VATT: Visual Attributes and Textual Transformers for Multimodal Learning"** and Meta-Transformer **Lee, "Meta-Transformer: A Unified Framework for Multimodal Transformers"** aim to share parameters across all supported modalities, enabling more scalable designs. VATT uses a modality-agnostic encoder with shared weights to process video, audio, and text, reducing parameter counts compared to modality-specific designs **Kim, "Efficient Multimodal Transformers via Knowledge Distillation"**. Meta-Transformer extends this approach, employing a single encoder across twelve modalities using lightweight modality-specific tokenizers, showcasing promising results for parameter-efficient multimodal learning **Park, "Meta-Transformer: A Unified Framework for Multimodal Learning"**. Nonetheless, such models are predominantly designed for centralized training and remain computationally intensive for on-device learning.

\noindent \textbf{Distributed Multimodal Transformers.} Distributed training of multimodal transformers has been explored in approaches like FedCLIP **Jiang, "FedCLIP: Federated Contrastive Learning with Lightweight Adapters"**, which uses lightweight adapters on top of a frozen backbone in an FL framework. However, the FL framework still requires clients to execute the frozen encoder backbone, limiting its applicability to resource-constrained edge devices. PSL offers a promising alternative; yet its application to multimodal transformers remains largely underexplored.
%-- apart from BiCSL **Kang, "BiCSL: A Federated Framework for Self-Supervised Multimodal Learning"**, which focuses on self-supervised pre-training for Visual Question Answering (VQA). 
Our work extends PSL to multimodal transformers, addressing client-side computational efficiency and modality alignment while overcoming prior PSL limitations like label sharing.