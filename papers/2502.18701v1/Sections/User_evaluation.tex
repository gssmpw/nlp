% \vspace{-0.14in}
\section{System Evaluation}
\subsection{Automatic Accessibility Evaluation}
\label{auto_eval}

To assess the accessibility improvements provided by our tool, we conducted automated evaluations before user testing. These assessments offer rapid, consistent, and objective analysis of accessibility compliance, identifying potential barriers for screen reader navigation. By validating the accessibility of LLM-generated (option 1) and LLM-reorganized webpages (option 2), we ensure adherence to established guidelines, forming a basis for usability testing.

% To evaluate the accessibility improvements provided by our tool, we conducted automated assessments before proceeding to user testing. Automated evaluations enable fast, consistent, and objective analysis of accessibility compliance, helping to identify structural and technical barriers that may hinder screen reader navigation. By validating the accessibility of LLM-generated (Option 1) and LLM-reorganized (Option 2) webpages through these evaluations, we ensure that our tool addresses common issues and adheres to established guidelines, providing a foundation for usability testing.
% \todo{add information on what types of things lighthouses can and cannot evaluate, why do we need user evaluation. becuase it align with WCAG 2.0 2.2 but not user's preference for content arrangement}
We used three widely recommended tools for this evaluation: Google Lighthouse~\footnote{https://developer.chrome.com/docs/lighthouse/overview}, SortSite~\footnote{https://www.powermapper.com/products/sortsite/}, and AChecker~\footnote{https://websiteaccessibilitychecker.com/checker/index.php}. These tools were selected for their alignment with different versions of WCAG guidelines and their various strengths in analyzing semantic HTML, ARIA attributes, and broader structural and functional accessibility issues. Their combination allows for cross-validation, ensuring a robust and comprehensive evaluation.

For the evaluation, we chose three e-commerce websites—Amazon, Nordstrom, and Mercari—based on user interviews that provided insights into user familiarity and usage patterns. For example, frequently used platforms (e.g., Amazon), platforms users were aware of but rarely engaged with (e.g., Nordstrom), and platforms unfamiliar to many participants (e.g., Mercari). This selection captures a range of accessibility challenges for screen reader users across platforms of varying familiarity.
% For the evaluation, we chose three e-commerce websites—Amazon, Nordstrom, and Mercari—based on user interviews that provided insights into user familiarity and usage patterns. This selection captures a range of accessibility challenges for screen reader users across platforms of varying familiarity.
% We selected three widely used tools for this evaluation: Google Lighthouse~\footnote{https://developer.chrome.com/docs/lighthouse/overview}, SortSite~\footnote{https://www.powermapper.com/products/sortsite/}, and AChecker~\footnote{https://websiteaccessibilitychecker.com/checker/index.php}. These tools were chosen due to their frequent recommendations in reputable accessibility resources, including the AccessiBe Blog~\footnote{https://accessibe.com/blog/knowledgebase/top-web-accessibility-tools}, W3C’s Tool List~\footnote{https://www.w3.org/WAI/test-evaluate/tools/list/}, and Applitools’ Accessibility Testing Tools~\footnote{https://applitools.com/blog/top-10-web-accessibility-testing-tools/}. 

% Google Lighthouse adheres to WCAG 2.1, while AChecker follows WCAG 2.0 guidelines. Both tools are specifically designed for WCAG compliance and offer insights into semantic HTML and ARIA attributes. In contrast, SortSite provides a more comprehensive scan of structural and functional accessibility issues, including heading hierarchy, link text, and form labels. By combining these tools, we can cross-validate their findings, leveraging their unique strengths to ensure a robust and comprehensive accessibility evaluation.

% Building on this approach, we selected three e-commerce websites—Amazon, Nordstrom, and Mercari—for evaluating accessibility improvements provided by our tool. These platforms were chosen based on insights from formative user interviews, where participants discussed websites they had used, heard about, or encountered in conversations with family and friends. The selection represents varying levels of familiarity: frequently used platforms (e.g., Amazon), platforms users were aware of but rarely engaged with (e.g., Nordstrom), and platforms unfamiliar to many participants (e.g., Mercari). This diverse selection ensures our evaluation captures a broad spectrum of accessibility challenges faced by screen reader users across different levels of familiarity.

% \todo{add testing result and explain again a bit why we are only choosing e-commerce website?}


We validated the accessibility improvements on both Option 1 (Regenerated HTML Version) and Option 2 (Reorganized HTML Tags Version) compared to the original webpages (see Table~\ref{tab:accessibility_problems_double}). For instance, on the Mercari Home Page, the number of Level A accessibility issues identified by Lighthouse decreased significantly, from six in the original version to one in Option 1 and two in Option 2. Similarly, SortSite detected four issues in the original version, which dropped to zero in Option 1 and two in Option 2.

Common issues identified in the original webpages included improper heading structures, missing or misused headings, and elements lacking accessible names. For the revised webpages, Option 1 (Regenerated HTML) achieved near-perfect accessibility in most cases, with occasional issues such as one or two nonfunctional links (e.g., \textit{``This skip link is broken.'' for <a href='main-content-start'>Skip navigation</a>}). In contrast, Option 2 (Reorganized HTML Tags) tended to result in slightly more accessibility issues compared to Option 1. For example, while Option 2 improved the structure of tags, some issues persisted, such as a heading element not being organized in an accessible sequence. This limitation reflects the inability of Option 2 to change the sequence or visual layout of the original page, leading to minor accessibility concerns. While automated tools are helpful and economically efficient, they often miss the subtle details of how screen reader users actually interact with websites or the specific challenges they face. For example, tools might not catch when visually grouped elements, such as a set of filters, are marked with inconsistent or incorrect HTML tags, making them confusing for screen reader navigation. Furthermore, prior literature suggests against relying solely on automated tests due to variation in their reliability \cite{benchmarking}. To address these limitations, we conducted a user evaluation, which we describe in Section~\ref{sec:phase3}.

% According to the automated Lighthouse evaluation, the original website scored an accessibility rating of 79. Common issues identified included improper heading structure, missing or misused headings, and elements lacking accessible names. For the revised web pages, the regenerated HTML (Option 1) achieved a perfect accessibility score of 100, while the reorganized tags (Option 2) scored 81. Improvements in both options included corrected heading ordering, which created a more logical flow for screen reader navigation. Additionally, the audit of ``button, link, and menuitem elements have accessible names'' passed for Option 2 but was marked as ``Not Applicable'' in the original version. However, some issues persisted in the Option 2 webpage, such as one heading element not being organized in an accessible sequence.

% The results from the Lighthouse evaluation demonstrate that both approaches improved the accessibility of the webpage compared to the original version. Among the two, Option 1 provided superior improvements in terms of information hierarchy and content organization. However, automated tools like Lighthouse can only detect technical issues and cannot fully capture how real users interact with the webpage. In the following section, we complement these findings with usability testing to understand how screen reader users perceive and navigate the enhanced versions.
% potentially findings:

% Larger, well-established platforms like Amazon and Walmart typically have significant resources for accessibility but may still exhibit challenges such as improper heading structure or missing ARIA labels. Smaller or newer platforms, such as Temu and Mercari, often lack such investment, potentially highlighting issues that are more prevalent across less-resourced e-commerce websites. 

% recover
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.2} % Adjust row height for better readability
\resizebox{\textwidth}{!}{%
\begin{tabular}{llccc|llccc}
\hline
\textbf{Page} & \textbf{Category} & 
\textbf{Lighthouse} & 
\textbf{SortSite} & 
\textbf{AChecker} & 
\textbf{Page} & \textbf{Category} & 
\textbf{Lighthouse} & 
\textbf{SortSite} & 
\textbf{AChecker} \\ \hline

\textbf{Mercari Home Pages} & Original & 6 & 4 & 6 & 
\textbf{Nordstrom Home Pages} & Original & 3 & 5 & 3 \\
                              & Option 1 & 1 & 0 & 0 & 
                              & Option 1 & 1 & 1 & 0 \\
                              & Option 2 & 2 & 2 & 3 & 
                              & Option 2 & 1 & 1 & 1 \\ \hline

\textbf{Mercari Search Result Pages} & Original & 5 & 5 & 6 & 
\textbf{Nordstrom Search Result Pages} & Original & 3 & 3 & 3 \\
                                       & Option 1 & 1 & 0 & 1 & 
                                       & Option 1 & 1 & 1 & 0 \\
                                       & Option 2 & 1 & 2 & 2 & 
                                       & Option 2 & 2 & 1 & 1 \\ \hline

\textbf{Mercari Product Pages} & Original & 6 & 4 & 5 & 
\textbf{Nordstrom Product Pages} & Original & 2 & 6 & 3 \\
                                 & Option 1 & 1 & 1 & 1 & 
                                 & Option 1 & 1 & 1 & 0 \\
                                 & Option 2 & 2 & 2 & 3 & 
                                 & Option 2 & 1 & 3 & 1 \\ \hline

\textbf{Amazon Home Pages} & Original & 6 & 6 & 7 & 
\textbf{Amazon Search Result Pages} & Original & 12 & 17 & 13 \\
                                   & Option 1 & 1 & 1 & 2 & 
                                   & Option 1 & 1 & 0 & 1 \\
                                   & Option 2 & 2 & 2 & 3 & 
                                   & Option 2 & 3 & 4 & 3 \\ \hline

\textbf{Amazon Product Pages} & Original & 16 & 18 & 15 &\\ 
                               & Option 1 & 1 & 1 & 1 \\
                               & Option 2 & 3 & 4 & 4 \\ \hline

\end{tabular}%
}
\caption{Comparison of Level A accessibility issues detected on various webpages using three automated evaluation tools: Lighthouse, SortSite, and AChecker (lower is better). A lower number indicates improved accessibility. The table shows the number of accessibility problems identified for the original version of each page, Regenerated HTML Version (Option1), and Reorganized HTML Tags Version (Option2). While Option 1 achieved near-perfect results in most cases, minor issues like one/two broken links persisted. Option 2 addressed structural problems but retained challenges such as inaccessible heading sequences and visual design conflicts due to its inability to alter the original layout or content order. 
% \yang{maybe briefly summarize the kinds of accessibility issues that the current version of option 1/2 still have. those need to elaborated in the discussion and suggest for future work}
}
\label{tab:accessibility_problems_double}
\end{table}

\subsection{Content Integrity Evaluation}
\subsubsection{Evaluation Metrics}
To ensure our accessibility enhancements preserve the content integrity of the original webpage, we evaluated the similarity between the original and LLM-generated versions through technical testing and user evaluations (Section~\ref{sec:user_similarity}). The technical analysis focused on screen-reader-accessible \textbf{elements}, which are discrete pieces of HTML component such as visible text, ARIA labels, and alt attributes. We used four metrics—\textit{Average Levenshtein}, \textit{Average Semantic}, \textit{Aggregated Levenshtein}, and \textit{Aggregated Semantic Similarity} to evaluate both structural and meaning-based similarity, following Microsoft's AI evaluation guidelines~\cite{microsoft2023evaluation} and supported by prior research~\cite{chen2021evaluating}. 

% \yang{why you chose those 4 metrics? is it because they were suggested or used in prior literature?} 

\textit{Average Levenshtein Similarity} and \textit{Average Semantic Similarity} focus on element-level similarity. \textit{levenshtein Match Quality} measures how closely the textual content of each element in the original webpage matches its best counterpart in the LLM-generated version, using Levenshtein distance~\cite{yujian2007normalized}. This metric provides a precise measure of how much textual information has been altered. For example, ``Add to Cart'' and ``Add To Cart'' would score highly because the only difference is letter casing. 

However, Levenshtein similarity alone is insufficient for capturing deeper content equivalence. For instance, ``Buy Now'' and ``Purchase Now'' may have low Levenshtein similarity due to character differences but would but convey the same semantic semantically. To address this limitation, \textit{Average Semantic Similarity} evaluates the alignment of meaning using embeddings generated by the OpenAI \texttt{text-embedding-ada-002} model. This approach recognizes paraphrased or reworded content, ensuring meaningful equivalence is captured.

At the global level, \textit{Aggregated Levenshtein Similarity} and \textit{Aggregated Semantic Similarity} assesses overall alignment of concatenated webpage content. \textit{Aggregated Levenshtein Similarity} strictly quantifies the distance between the original and LLM-generated content but may fail to reflect content integrity accurately when the sequence or structure of the content changes. For example, an original webpage might prioritize promotional banners before product details, while the LLM-generated version reorganizes these sections to prioritize essential information for screen readers. Even if the content remains the same, levenshtein similarity will appear low due to the structural changes. In contrast, \textit{Aggregated Semantic Similarity} captures the overall meaning and context of the content, allowing it to account for such reorganizations effectively. These metrics are complementary and addressing different aspects of similarity. The formulas for these metrics are as follows:

\[
\text{Average Levenshtein Similarity (\%)} = \frac{\sum \text{Levenshtein Similarity Scores for Best Matches}}{\text{Total Number of Original Elements}} \times 100
\]

\[
\text{Average Semantic Similarity (\%)} = \frac{\sum \text{Semantic Similarity Scores for Best Matches}}{\text{Total Number of Original Elements}} \times 100
\]

\[
\text{Aggregated Levenshtein Similarity (\%)} = \left(1 - \frac{\text{Levenshtein Distance Between Aggregated Texts}}{\text{Maximum Length of Aggregated Texts}}\right) \times 100
\]

\[
\text{Aggregated Semantic Similarity (\%)} = \text{Cosine Similarity of Aggregated Embeddings} \times 100
\]


\subsubsection{Evaluation Results.} We conducted our evaluation on three e-commerce platforms: Mercari, Nordstrom, and Amazon, testing three types of webpages (Home Page, Search Result Page, and Product Page) for each platform. These pages were selected based on their relevance to user tasks in Section~\ref{user_study_procedure}. We only compared the original webpage with the LLM-generated version (Option 1) for content similarity because Option 2 involves only tag replacements without altering the HTML text or layout. Table~\ref{tab:match_quality} summarizes the results for each metric. The results indicate that the LLM-generated webpages preserve both  levenshtein and semantic content effectively in most cases. On average, semantic similarity were consistently higher than levenshtein similarity, highlighting the LLM’s ability to preserve meaning despite rephrasing or reorganization. 

The lower Average Levenshtein Similarity can be attributed to the granular structure of elements on the original webpage. In our testing, we observed that single characters like ``\$'' or very short phrases such as ``sold'' often exist as separate elements in the original HTML. In the LLM-generated version, these smaller pieces are often concatenated into more meaningful and cohesive components, creating a simpler and more accessible HTML structure. While this enhances usability for screen reader users, it introduces differences when strictly comparing individual elements, thereby lowering the Average Levenshtein Similarity score. For low Aggregated Levenshtein Similarity scores, such as on the Nordstrom homepage, we observed significant reorganization of the content sequence while maintaining the accuracy of the HTML text during the transformation. For instance, the original Nordstrom homepage had sales and advertisement information scattered across the page. In the LLM-regenerated version, this information was grouped together and listed at the top, improving accessibility and hierarchy for screen reader users. However, this reordering of content reduced the Aggregated Levenshtein Similarity, even though the textual content for each element remained unchanged. Semantic metrics, particularly the Aggregated Semantic Similarity, provide a more accurate reflection of the transformation accuracy. In the LLM-regenerated version, the text within each HTML component remains highly accurate, closely matching the original version. Changes to the HTML text are generally minor and do not significantly alter the content. However, we observed issues such as missing links and non-functional buttons in the regenerated version, primarily caused by the omission or incorrect handling of JavaScript dynamic elements, which can negatively impact usability for end users. But for developers utilizing our tool to restructure HTML, these limitations can be addressed through manual checks and adjustments to ensure interactive elements function as intended. We suggest future work explore enhancing the performance of LLM-regenerated outputs in handling dynamic JavaScript functionalities~\ref{sec:limitation}.


% However, it is important to note that levenshtein similarity alone does not accurately reflect the preservation of the main content of the original webpage. Semantic metrics, particularly the aggregated semantic similarity, demonstrate that the core meaning and key information of the page remain intact, even when smaller elements are restructured to enhance accessibility. 

In addition to these technical evaluations, we also assessed participant perceptions of content similarity between the original and LLM-generated versions. Participants perceived the three versions as very similar, essentially the same content, even when the structure or layout differed (detailed findings in Section~\ref{sec:user_similarity}). This alignment between user perceptions and similarity metrics further validates the effectiveness of our approach in balancing content preservation with accessibility improvements.

% recover
\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccc}
\hline
\textbf{Page} & 
\textbf{\begin{tabular}[c]{@{}c@{}}Average \\Levenshtein \\Similarity (\%)\end{tabular}} & 
\textbf{\begin{tabular}[c]{@{}c@{}}Average \\Semantic\\ Similarity (\%)\end{tabular}} & 
\textbf{\begin{tabular}[c]{@{}c@{}}Aggregated \\Levenshtein \\Similarity (\%)\end{tabular}} & 
\textbf{\begin{tabular}[c]{@{}c@{}}Aggregated \\Semantic\\ Similarity (\%)\end{tabular}} \\ \hline
Mercari Home Page          & 76.93 & 93.09 & 93.58 & 99.36 \\
Mercari Search Result Page & 69.83 & 90.42 & 67.52 & 97.09 \\
Mercari Product Page       & 77.30 & 94.36 & 73.80 & 98.35 \\
Nordstrom Home Page             & 60.56 & 81.47 & 55.81 & 91.60 \\
Nordstrom Search Result Page    & 65.64 & 90.07 & 63.32 & 93.19 \\
Nordstrom Product Page          & 63.28 & 86.50 & 62.18 & 92.06 \\
Amazon Home Page           & 78.46 & 93.65 & 76.47 & 97.47 \\
Amazon Search Result Page  & 80.54 & 95.23 & 79.43 & 98.56 \\
Amazon Product Page        & 83.09 & 95.58 & 82.53 & 98.89 \\ \hline
\end{tabular}%
}
\caption{Comparison of Levenshtein
Similarity and Semantic Similarity metrics for various webpages (higher is better). The table presents four metrics—Average Levenshtein Similarity, Average Semantic Similarity, Aggregated Levenshtein Similarity, and Aggregated Semantic Similarity—evaluated across three types of webpages (Home Page, Search Result Page, and Product Page) for three e-commerce platforms: Mercari, Nordstrom, and Amazon. These metrics provide a comprehensive assessment of how well LLM-generated accessible webpages retain the original content, both at the element level (average metrics) and at the global level (aggregated metrics). Higher values indicate better alignment with the original webpage, demonstrating that LLM-generated versions effectively preserve content meaning while accommodating accessibility improvements.}
\label{tab:match_quality}
\end{table}



\section{User Evaluation}
\label{sec:phase3}
\subsection{Participants Demographics}
15 screen reader users participated in the study, consisting of nine females and six males. The participants' ages ranged from 18 to 44 years. All demographic data were self-reported by the participants (see Table \ref{tab:phase2demo}). Among the participants, 11 were completely blind, one had limited vision, and three could perceive only light. Participant recruitment details are in section \ref{sec:recruit}.


% recover
\begin{table}[h]
\resizebox{1\textwidth}{!}{
\begin{tabular}{llllll}
\hline
\textbf{ID} &
  \textbf{Age} &
  \textbf{Gender} &
  \textbf{Visual Ability} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Online shopping\\ Frequency\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Used Screen\\ Reader\end{tabular}} \\ \hline
\textbf{P1} & 25 - 34 & Female & I am totally blind                & 4-6 times a week & JAWS, NVDA, VoiceOver       \\
\textbf{P2} & 25 - 34 & Female & Completely blind                  & 4-6 times a week & JAWS                        \\
\textbf{P3} & 25 - 34 & Female & Blind with light perception & Once a week      & JAWS, NVDA, Google Talkback \\
\textbf{P4} & 18 - 24 & Female & No vision                         & Once a week      & JAWS, NVDA, VoiceOver       \\
\textbf{P5} & 35 - 44 & Female & Totally blind                     & Once a week      & JAWS, NVDA                  \\
\textbf{P6} & 25 - 34 & Male   & Blind, light perception only      & 2-3 times a week & JAWS, NVDA                  \\
\textbf{P7} & 25 - 34 & Male   & I have limited vision             & 2-3 times a week & JAWS                        \\ 
\textbf{P8} & 25 - 34 & Male   & Fully blind  & 4-6 times a week & JAWS, NVDA, Google Talkback, VoiceOver  \\ 
\textbf{P9} & 35 - 44 & Female   & Legally blind  & 4-6 times a week & JAWS, Google Talkback, VoiceOver  \\ 
\textbf{P10} & 35 - 44 & Male   & Blind  & Once a week & JAWS, VoiceOver  \\ 
\textbf{P11} & 25 - 34 & Female   & Completely blind  & 2-3 times a week & JAWS, NVDA, Google Talkback, VoiceOver  \\ 
\textbf{P12} & 25 - 34 & Female   & Zero vision  & Once a week & JAWS, NVDA, Google Talkback, VoiceOver  \\ 
\textbf{P13} & 25 - 34 & Female   & Blind but can perceive light  & Once a week & JAWS, NVDA, Google Talkback, VoiceOver  \\ 
\textbf{P14} & 35 - 44 & Male   & Legally blind  & 2-3 times a week & JAWS, VoiceOver  \\ 
\textbf{P15} & 25 - 34 & Male   & Blind  & Once a week & JAWS, Google Talkback  \\ 
\hline
\end{tabular}
}
\caption[Table 2. User Evaluation Participant Demographic Table, which details the characteristics of seven participants (P1 through P7) in a user evaluation study. The table consists of six columns: ID, Age, Gender, Visual Ability, Online shopping Frequency, and Used Screen Reader. The participants are predominantly female (5 out of 7) with ages ranging from 18-44, mostly in the 25-34 range. All participants have some form of visual impairment, from total blindness to limited vision. Online shopping frequency varies from once a week to 4-6 times a week. Most participants use multiple screen readers, with JAWS being the most common, followed by NVDA and VoiceOver. This table provides a comprehensive overview of the diverse group of visually impaired participants in the user evaluation study, highlighting their varied demographics, visual abilities, online shopping habits, and screen reader preferences.]{User Evaluation Participant Demographic Table}
\label{tab:phase2demo}
\vspace{-0.14in}
\end{table}
\vspace{-0.14in}

% \todo{add testing sequence for each participant}


% recover
\begin{table}[ht]
\centering
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Testing Sequence}} & \multicolumn{1}{c|}{\textbf{Participants}} \\ \hline
HTML → Tag → Original & P4, P12 \\ \hline
HTML → Original → Tag & P2, P11, P15 \\ \hline
Tag → HTML → Original & P5, P9 \\ \hline
Tag → Original → HTML & P6, P10, P14 \\ \hline
Original → HTML → Tag & P1, P7, P13 \\ \hline
Original → Tag → HTML & P3, P8 \\ \hline
\end{tabular}
\caption{Sequence of Version Testing by Participants}
\label{tab:sequence_participant}
\end{table}



\subsection{Study Procedure}
\label{user_study_procedure}
We conducted a within-subjects user evaluation over Zoom, lasting 60-90 minutes. This duration included time for setting up the experiment (e.g., installing an extension), completing all three tasks across the three website versions, filling out an evaluation survey after each version (three surveys total), and engaging in think-aloud protocols and discussions with researchers to capture detailed feedback and insights. At the beginning of the study, we obtained participants' consent for both participation and recording. We started with general questions about their previous experiences with online shopping and screen reader usage. This was followed by user testing of three different versions of the Mercari.com website: (1) the original website, (2) the LLM-regenerated HTML version implemented through Extension Option 1, and (3) the LLM-reorganized HTML tags version implemented through Extension Option 2, as described in Section~\ref{sec:phase2}.

To prevent learning effects, we counterbalanced the sequence of versions each participant tested. The testing sequence for each participant is detailed in Table~\ref{tab:sequence_participant}. Drawing from prior research~\cite{jones2024customization}, we designed three tasks that simulate key navigation and decision-making processes, reflecting real-world shopping scenarios.

For each version, participants started by browsing the homepage to familiarize themselves with the layout of a new shopping website (Task 1). Once they completed this, they were asked to imagine they wanted to buy a blender and search for it on the site. We chose a blender as it is a gender-neutral item, helping us avoid content bias that might influence the browsing experience. Participants then reviewed the search results, exploring the products as they typically would when searching on an e-commerce website, and selected a product they were interested in exploring further (Task 2). Finally, they clicked on their chosen product and browsed the product detail page to gather the information they needed (Task 3).

During these tasks, participants were encouraged to browse as they normally would while using their screen readers and to think aloud about their experience. This included sharing how they navigated the site, any challenges they encountered, and their overall thoughts on the browsing experience.
% To prevent learning effects, the sequence in which participants tested the different versions was randomly assigned. The testing sequence for each participant is shown in Table~\ref{tab:sequence_participant}. 
% For each version of the website, participants were first instructed to browse the homepage and explore the site according to their preferences. They were then presented with a scenario in which they were to search for and select a blender on the shopping website.

\begin{enumerate}
\item \textbf{Task 1: Homepage} Participants navigated the homepage of the shopping website using a screen reader as they normally would.

\item \textbf{Task 2: Search Page} Participants used the screen reader to search for a blender on the website and browsed through the search results.

\item \textbf{Task 3: Product Page} Participants used the screen reader to view the details of a specific product they were interested in purchasing.
\end{enumerate}

To better understand how well each version addressed participants' accessibility needs and navigation preferences on e-commerce webpages, we designed a brief Likert-scale survey with five questions. These questions were adapted from a previous study~\cite{jones2024customization} and informed by findings from our formative study, tailored specifically to accessibility concerns and shopping scenarios.

% After completing three tasks on each version of the webpage, participants were asked to self-report on 5-question survey to further explore the challenges and ease of use they experienced as screen reader users. They then completed a 5-question survey, with all questions rated on a 1-5 Likert scale. 

\begin{enumerate}
\item \textbf{Q1: }How would you rate your overall experience using the screen reader on this website? 
\item \textbf{Q2: }How clear and meaningful did you find the headings on this website? 
\item \textbf{Q3: }How helpful were the headings in understanding the hierarchy of content on this website? 
\item \textbf{Q4: }How efficiently could you locate key sections or information on this website using headings? 
\item \textbf{Q5: }How easy was it to access key features (e.g., search bar, product categories, account options) using your screen reader on this website?
\end{enumerate}

Participants rated each question on a 5-point scale, where 1 indicated "Very Difficult / Poor" and 5 indicated "Very Easy / Good." To ensure consistent evaluation of accessibility and usability, participants completed the survey after testing each version, resulting in three sets of responses within a single interview. After participants completed browsing all three versions, we asked follow-up questions such as how similar they found the content across the three versions, how they would compare these versions, and to rank them based on their experience. Finally, we debriefed participants about the differences between the versions and asked for additional suggestions for design improvements.



% This experiment was conducted within a shopping scenario. Participants were tasked with interacting with three different versions of the same website used in Section \ref{sec:formative study} (Mercari\footnote{https://www.mercari.com/}): \textbf{the original website}, \textbf{a version with regenerated HTML}, and \textbf{a version with reorganized HTML tags}. The experiment consisted of three tasks, each executed in a series of steps. During the tasks, participants navigated through the website using a screen reader and were asked to verbalize their experiences and opinions. Following the completion of the tasks, participants were interviewed. Details of the three-step tasks are as follows: %add randomly 



% \subsection{Data Analysis}
% The recorded scripts were transcribed using xxxxxx \footnote{\url{xxxx}}. We conducted an analysis using recursive thematic analysis \cite{braun2006using}, following six phases. Two authors participated in the data analysis process. We performed coding using xxxxx \footnote{\url{xxxxx}}. For instance, a participant's statement such as ``xxxx'' was considered a single code. Quotes from other participants expressing similar opinions might be tagged with the same code. This code, along with other related codes, was organized under the theme ``xxxxxx''. Through this process, we identified xxx codes. Subsequently, we employed the affinity diagramming method \cite{beyer1999contextual} to categorize these codes into themes. This process culminated in the identification of xxx primary themes: (1) xxxx,  and (2) xxxxx. It is worth noting that the number and composition of themes in thematic analysis can vary depending on factors such as the volume of data and the research content \cite{braun2006using}.

\subsection{Quantitative Results}
% \yang{I think you might need to do a Bonnferroni correction for the post-hoc multiple comparisons. some reviewers might invalidate your results if you don't correct for the family wise type I error rate} 

\subsubsection{Statistical Analysis of Likert Scale Survey Results}
To analyze the results from the 5-question survey for each version, We conducted Wilcoxon signed-rank tests for all pairwise comparisons across five questions (Q1–Q5), comparing three different conditions: Original, HTML, and Tag. To account for the increased risk of Type I errors due to multiple comparisons, a Bonferroni correction was applied for each question, adjusting the significance threshold to \(\alpha_{\text{adjusted}} = 0.0167\). Results indicate that the HTML version consistently outperformed the Original website across all five questions, while the Tag version showed significant improvements only in the overall user experience (Q1) compared to the Original. The comparisons between HTML and Tag versions revealed a significant difference only in Q1, suggesting that HTML enhancements had a more pronounced impact on improving user experience than Tag reorganizations.

\textit{Overall User Experience (\textbf{Q1}):} Participants rated the overall user experience higher for both HTML and Tag versions compared to the Original website. The Wilcoxon signed-rank tests showed significant differences between the Original website and HTML (\( W = 0 \), \( p = 0.00585 \)) and between the Original website and Tag (\( W = 0 \), \( p = 0.00585 \)). Additionally, the comparison between HTML and Tag (\( W = 0 \), \( p = 0.01170 \)) was also significant. All these p-values are below the adjusted significance threshold (\(\alpha_{\text{adjusted}} = 0.0167\)), indicating that both HTML and Tag versions significantly improved the overall navigation experience compared to the Original website.

\textit{Clarity of Headings (\textbf{Q2}):} Clarity of headings was rated higher in both HTML and Tag versions compared to the Original website. The comparison between the Original website and HTML (\( W = 0 \), \( p = 0.00585 \)) was significant, indicating improved clarity. However, the comparison between the Original website and Tag (\( W = 0 \), \( p = 0.04500 \)) and between HTML and Tag (\( W = 0 \), \( p = 0.18750 \)) were not significant after Bonferroni correction.

\textit{Understanding of Content Hierarchy (\textbf{Q3}):} Participants reported improved understanding of content hierarchy in both HTML and Tag versions compared to the Original website. The comparison between the Original website and HTML (\( W = 0 \), \( p = 0.00585 \)) was significant, indicating a better understanding. However, the comparisons between the Original website and Tag (\( W = 0 \), \( p = 0.04500 \)) and between HTML and Tag (\( W = 5 \), \( p = 0.18750 \)) were not significant after correction.

\textit{Efficiency in Locating Key Sections (\textbf{Q4}):} Option HTML outperformed the Original website in efficiency (\( W = 0 \), \( p = 0.00585 \)), which was significant. Comparisons between the Original website and Tag (\( W = 2.5 \), \( p = 0.16470 \)) and between HTML and Tag (\( W = 0 \), \( p = 0.17550 \)) were not significant after Bonferroni correction.

\textit{Ease of Access to Key Features (\textbf{Q5}):} Ease of access to key features improved in the HTML version compared to the Original website (\( W = 0 \), \( p = 0.00585 \)), which was significant. However, comparisons between the Original website and Tag (\( W = 5.5 \), \( p = 0.10260 \)) and between HTML and Tag (\( W = 0 \), \( p = 0.35100 \)) did not reach statistical significance after correction.


% we first applied the Friedman test to examine overall differences across conditions. This was followed by the Wilcoxon signed-rank test for post-hoc comparisons to explore pairwise differences~\cite{numan2024spaceblender}. Figure~\ref{fig:5likerd_result} illustrates the distribution of participant scores for questions Q1 through Q5. The Friedman test revealed statistically significant differences in ratings among the three website versions (Original website, Option 1, and Option 2) across all questions. Although the sample size was limited to 15 participants, the consistent patterns observed across the data suggest notable distinctions between the versions. Our study revealed significant improvements in user experience and accessibility with the redesigned webpages compared to the original versions. Across all metrics, Option 1 (Regenerated HTML) consistently outperformed both the original website and Option 2 (Reorganized HTML Tags). Participants reported significantly higher satisfaction with the overall user experience, clarity of headings, and understanding of content hierarchy in Option 1. Additionally, Option 1 demonstrated superior efficiency in locating key sections and ease of access to key features, achieving scores close to the maximum on multiple metrics. While Option 2 showed improvements over the original website, particularly in content hierarchy and ease of access, its performance was generally lower than Option 1, especially in efficiency. Below, we provide a detailed analysis of the results for each question.
% \textit{Overall User Experience (\textbf{Q1}):} Participants rated the overall user experience significantly higher for both Option 1 (M = 4.8, SD = 0.42) and Option 2 (M = 4.33, SD = 0.82) compared to the original website (M = 3.2, SD = 0.91). The statistical tests confirmed these differences between the original website and Option 1 (W = 0.0, p = 0.0037) and between the original website and Option 2 (W = 0.0, p = 0.0080). These results suggest that both redesigned versions provided a more satisfactory user experience than the original.

% \textit{Clarity of Headings (\textbf{Q2}):} Clarity of headings was another area where both redesigned versions outperformed the original website. Option 1 (M = 4.86, SD = 0.36) received the highest scores, followed by Option 2 (M = 4.26, SD = 1.05), with the original website scoring the lowest (M = 3.2, SD = 1.26). Statistically significant differences were observed between the original website and Option 1 (W = 0.0, p = 0.0058), and between the original website and Option 2 (W = 0.0, p = 0.0074). No significant difference was found between the two redesigned versions (W = 2.5, p = 0.14).

% \textit{Understanding of Content Hierarchy (\textbf{Q3}):} Participants reported a significantly improved understanding of content hierarchy in both redesigned versions compared to the original website. Option 1 (M = 4.66, SD = 0.49) and Option 2 (M = 4.2, SD = 0.97) scored significantly higher than the original (M = 2.5, SD = 1.45), with statistical tests showing differences between the original and Option 1 (W = 0.0, p = 0.0059) and between the original and Option 2 (W = 0.0, p = 0.0037). The lack of a significant difference between Option 1 and Option 2 (W = 8.0, p = 0.2105) indicates that both versions substantially improved content hierarchy comprehension.

% \textit{Efficiency in Locating Key Sections (\textbf{Q4}):} Option 1 (M = 4.8, SD = 0.42) again outperformed the other versions, showing a significant improvement over both the original website (M = 3.26, SD = 1.21, W = 0.0, p = 0.0176) and Option 2 (M = 3.93, SD = 0.82, W = 0.0, p = 0.0281). Interestingly, there was no statistically significant difference between the original website and Option 2 (W = 3.0, p = 0.093). This suggests that while Option 1 markedly improved efficiency, the enhancements in Option 2 were less consistent. This finding aligns with feedback from the user evaluation, as Option 2 only replaced HTML tags on the page without altering the overall layout or structure. 

% \textit{Ease of Access to Key Features (\textbf{Q5}):} Ease of access to key features also showed significant improvement in the redesigned versions. Option 1 (M = 4.93, SD = 0.26) was rated significantly higher than both the original website (M = 3.86, SD = 1.02, W = 0.0, p = 0.0065) and Option 2 (M = 4.4, SD = 0.74, W = 5.5, p = 0.0342). While there was no statistically significant difference between Option 1 and Option 2 (W = 0.0, p = 0.0694), the slight advantage of Option 1 suggests it provided a more seamless experience for participants in accessing key features.



% \begin{table}[ht]
%     \centering
%     \caption{Wilcoxon Signed-Rank Test Results with Bonferroni Correction}
%     \label{tab:wilcoxon_bonferroni}
%     \begin{tabular}{llccc}
%         \toprule
%         \textbf{Question} & \textbf{Comparison}       & \textbf{Wilcoxon } \(W\) & \textbf{Raw } \(p\)-value & \textbf{Adjusted } \(p\)-value \\
%         \midrule
%         Q1 & Original vs HTML & 0 & 0.00195 & 0.0293 \\
%         Q1 & Original vs Tag  & 0 & 0.00195 & 0.0293 \\
%         Q1 & HTML vs Tag      & 0 & 0.0039  & 0.0585 \\
%         Q2 & Original vs HTML & 0 & 0.00195 & 0.0293 \\
%         Q2 & Original vs Tag  & 0 & 0.0150  & 0.2250 \\
%         Q2 & HTML vs Tag      & 0 & 0.0625  & 0.9375 \\
%         Q3 & Original vs HTML & 0 & 0.00195 & 0.0293 \\
%         Q3 & Original vs Tag  & 0 & 0.0150  & 0.2250 \\
%         Q3 & HTML vs Tag      & 5 & 0.0625  & 0.9375 \\
%         Q4 & Original vs HTML & 0 & 0.00195 & 0.0293 \\
%         Q4 & Original vs Tag  & 2.5 & 0.0549 & 0.8235 \\
%         Q4 & HTML vs Tag      & 0 & 0.0585  & 0.8775 \\
%         Q5 & Original vs HTML & 0 & 0.00195 & 0.0293 \\
%         Q5 & Original vs Tag  & 5.5 & 0.0342 & 0.5130 \\
%         Q5 & HTML vs Tag      & 0 & 0.1170  & 1.7550 \\
%         \bottomrule
%     \end{tabular}
% \end{table}

% \fixme{\textbf{Overall User Experience (Q1):} Statistically significant differences were found between original website (M = 3.2, SD = 0.91) and option 1 (M = 4.8, SD = 0.42) (W = 0.0, p = 0.0037), as well as between original website and option 2 (M = 4.33, SD = 0.82) (W = 0.0, p = 0.0080) in terms of overall user experience scores. This indicates that original website provides a less satisfactory user experience compared to the other versions.}

% \fixme{\textbf{Clarity of Headings (Q2):} There were statistically significant differences in the clarity of headings between original website (M = 3.2, SD = 1.26) and option 1 (M = 4.86, SD = 0.36) (W = 0.0, p = 0.0058), and between original website and option 2 (M = 4.26, SD = 1.05) (W = 0.0, p = 0.0074). No significant difference was observed between option 1 and option 2 (W = 2.5, p = 0.14).}

% \fixme{\textbf{Understanding of Content Hierarchy (Q3):} Significant differences were observed in the understanding of content hierarchy scores between original website (M = 2.5, SD = 1.45) and option 1 (M = 4.66, SD = 0.49) (W = 0.0, p = 0.0059), as well as between original website and option 2 (M = 4.2, SD = 0.97) (W = 0.0, p = 0.0037). There was no significant difference between option 1 and otion 2 (W = 8.0, p = 0.2105).}

% \fixme{\textbf{Efficiency in Locating Key Sections (Q4):} Statistically significant differences were found between original website (M = 3.26, SD = 1.21) and option 1 (M = 4.8, SD = 0.42) (W = 0.0, p = 0.0176), and between option 1 and option 2 (M = 3.93, SD = 0.82) (W = 0.0, p = 0.0281) for scores on efficiency in locating key sections. No significant difference was observed between original website and option 2 (W = 3.0, p = 0.093).}

% \fixme{\textbf{Ease of Access to Key Features (Q5):} Significant differences in ease of access to key features were found between original website (M = 3.86, SD = 1.02) and option 1 (M = 4.93, SD = 0.26) (W = 0.0, p = 0.0065), as well as between original website and option 2 (M = 4.4, SD = 0.74) (W = 5.5, p = 0.0342). No significant difference was noted between option 1 and option 2 (W = 0.0, p = 0.0694).}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.6\linewidth]{figures/5likerd_result.pdf}
%     \caption{\fixme{The 5-point Likert scale results from Q1 to Q5, as answered by 15 participants when testing the three versions. From top to bottom, ``Original'' refers to the original website, ``Option 1'' refers to the LLM-regenerated HTML version implemented through Extension Option 1, and ``Option 2'' refers to the LLM-reorganized HTML tags version implemented through Extension Option 2.}}
%     \label{fig:5likerd_result}
% \end{figure}


% recover
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/5likerd_result.pdf}
        \caption{The 5-point Likert scale results from Q1 to Q5, as answered by 15 participants when testing the three versions. From top to bottom, ``Original'' refers to the original website, ``Option 1'' refers to the LLM-regenerated HTML version implemented through Extension Option 1, and ``Option 2'' refers to the LLM-reorganized HTML tags version implemented through Extension Option 2.}
        \label{fig:5likerd_result}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/boxplot_result.png}
        \caption{Box plot comparing task completion times across the three versions (Original, Option 1, and Option 2), showing variability in performance and highlighting differences in accessibility design.}
        \label{fig:boxplot_result}
    \end{subfigure}
    \caption{Comparison of participant responses and task performance across the three webpage versions.}
    \label{fig:comparison}
\end{figure}

\subsubsection{Task Completion Time in Different Version}
We instructed participants to complete tasks 1–3 that simulated their real-life shopping experiences. They were encouraged to freely browse products without a strict goal of locating specific information or identifying a particular product on the search results page. Participants were also prompted to think aloud as much as possible during the tasks, sharing their thoughts on accessibility and navigation. Given this open-ended setup, comparing the time it took participants to complete each task (e.g., browsing the homepage or search results page) would be unreasonable, which would not accurately reflect the differences in accessibility design between versions. Some participants naturally verbalized more thoughts or explored more products on the search results page than others. However, in Task 3, while browsing the product details page, all participants consistently began by locating the product description. This behavior allowed us to calculate the time each participant spent finding the product description as a reliable quantitative metric for comparing the accessibility of different versions~\cite{islam2023spacex}. All participants naturally prioritized this step, making it a reliable and meaningful metric for comparing accessibility design across the three versions. The task completion times, as visualized in the box plot (Figure~\ref{fig:boxplot_result}), reveal significant differences across the three webpage versions: Original, Option 1 (Regenerated HTML) , and Option 2 (Reorganized HTML Tags). 

To analyze these differences, we conducted a Friedman test followed by Wilcoxon signed-rank tests for pairwise comparisons. Given that three pairwise tests were performed, we applied the Bonferroni correction to control for the increased risk of Type I errors. Specifically, the original significance level (\(\alpha = 0.05\)) was divided by the number of comparisons (\(m = 3\)), resulting in an adjusted significance level (\(\alpha_{\text{adjusted}} = 0.0167\)). The Friedman test indicated a significant overall difference among the versions (\(\chi^2(2) = 30.00\), \(p < 0.001\)). Subsequent pairwise comparisons using the Wilcoxon signed-rank test, adjusted with the Bonferroni correction, demonstrated that Option 1 significantly outperformed the Original website (\(W = 0.0\), \(p < 0.001\)) and Option 2 (\(W = 0.0\), \(p < 0.001\)). Additionally, Option 2 showed a significant improvement over the Original version (\(W = 0.0\), \(p < 0.001\)) but exhibited greater variability in task completion times. Option 1 consistently achieved the shortest task completion times, reflecting its effective structural and navigational enhancements. Option 2 provided moderate improvements but was limited by the unchanged layout, while the Original website showed the longest times and the most inconsistencies due to unstructured content and missing navigation labels. These results highlight the importance of combining structural and usability improvements to create a more accessible and efficient user experience.

% In addition to measuring the time spent locating the product description, we also tracked the error rate, which represents the number of navigation challenges participants encountered. These challenges included difficulties locating products on the search results page, missing key product details, experiencing confusion about the page layout, struggling to follow a logical flow, or needing to backtrack repeatedly to find the desired information.


% \subsubsection{Limitations}


\subsection{Qualitative Results}
\subsubsection{Browsing Patterns of Screen Reader Users on Unfamiliar Website}
The most commonly used navigation combination by all participants is the \textit{heading key} paired with the \textit{down arrow key}. The \textit{heading key} allows users to jump between headings on a webpage by pressing \textit{``H''}, similar to how sighted users scan visually for bold or large text to find different sections. The \textit{down arrow key} is used to navigate through HTML elements one by one, enabling users to read and explore content sequentially. When visiting a new website, users typically start by using the down arrow key to browse through the content line by line. Once they have encountered several headings while browsing with the down arrow key and have a sense that the website uses headings, they switch to the \textit{heading key (H)} to quickly jump between sections of interest. Sometimes, they may also navigate by specific heading levels. For example, if they browse a product title labeled as a heading level 2, and then want to read the product details, they might press the number 3 key to jump to the next heading level, assuming the details are structured logically under that level. If they find the heading relevant to their needs, they then switch back to the down arrow key to explore the details beneath that heading, reading through each element to gain a deeper understanding. In addition to the heading and down arrow keys, some participants also mentioned using the \textit{button key} and \textit{edit key} to interact with interactive elements on a webpage. The button key allows users to quickly navigate to buttons, such as \textit{``Add to Cart''} or \textit{``Submit''}, while the edit key helps them jump directly to input fields, making it easier to fill out forms or enter search queries.
\subsubsection{User Preference}

All participants found the regenerated HTML version and the reorganized HTML tags version to be more intuitive for navigating with a screen reader, significantly enhancing accessibility. The participants' average evaluation of the screen reader browsing experience on the regenerated HTML version was 5 on a scale from 1 (very poor) to 5.00 (excellent). The average rating for the reorganized HTML tags version was 4.57, while the original website received an average rating of 3.14. Five participants reported that the regenerated HTML version provided the best experience, even though the reorganized HTML tags version showed improvements over the original website. One participant felt that both versions offered an equally good screen reader navigation experience, while another preferred the reorganized HTML tags version over the regenerated HTML version.

\subsubsection{User Feedback of Original Website}
While browsing the original website, participants often experienced inconvenience and confusion due to the inappropriate use of HTML tags and a layout that did not align with screen reader browsing patterns. P2 noted, \textit{``The website is cluttered, maybe like too many uses of the headings, but there's nothing I could do.''} Participants also acknowledged that this was a common problem on other mainstream shopping websites. Although it is not convenient for screen reader users, they have become accustomed to it and consider it acceptable. As P2 explained, \textit{``Amazon is an insanely cluttered website. There are so many headings. The only way to navigate Amazon is to just arrow down each item instead of using headings.''} Participants also reported frustration when important information, such as product titles, was not marked as headings. For example, P3 noted, \textit{``So the problem is the product itself is not a heading, but a lot of other information that is not necessarily needed is in headings.''} Additionally, the structure and sequence of the website posed challenges for participants in understanding and navigation. For instance, some websites place product images and reviews on the left, while product names and details are on the right. This layout may make sense from a sighted user's perspective due to visual design principles, but it creates confusion for BLV users. Because review titles are also marked as headings, when users press ``H'' to navigate through headings, reviews appear before the product title. P4 expressed confusion with this structure, \textit{``It is good that they put reviews in headings, but not as the first heading we encounter, because it serves as a signal that it is already the end part of the product information.''} P9 also expressed concern about the complexity of the layout, stating, \textit{``Whenever I hit H, it would bring me to a completely different part of the page.''}
\vspace{-0.05in}
\subsubsection{Correct Labeling for Essential Information}

Participants perceived that both the regenerated HTML version (version 2) and the reorganized HTML tags version (version 3) improved the heading structure compared to the original website. They rated the helpfulness of headings for understanding the website's hierarchy with an average score (Q3) of 4.86 for the regenerated HTML version and 4.43 for the reorganized HTML tags version, in contrast to a score of 2.71 for the original version. Participants reported significant improvements in both versions 2 and 3 in two key aspects: (1) unnecessary headings, such as individual category names (e.g., women, men, sports), were removed. For example, P2 explained, \textit{``Those categories are just regular links [on the regenerated HTML version and reorganized HTML tags version], and it is faster to navigate through the page because you can easily skip that whole section of the website if it's not what I'm interested in.''} (2) Important information, like product names, was correctly labeled as headings. P4 mentioned, \textit{``I see that links, such as the title of the product, are also displayed as headings [on the regenerated HTML version and reorganized HTML tags version], which is quite convenient for me because now I can simply press the number key for the product.''} P13 noted that important information appears as headings, stating,  \textit{``Headings are very much in the key part of the page, and they really help me quickly navigate, revealing that they also assist in navigation.''}

\subsubsection{Why Regenerated HTML Works Better}
The regenerated HTML version (version 2, Q4 $\bar{x} = 4.86$) was perceived as even better than the reorganized HTML tags version (version 3, Q4 $\bar{x} = 4.28$) in terms of locating essential information and understanding the hierarchy of website content. In version 2, the LLM regenerated the entire HTML file, leading to several unique improvements that version 3 could not achieve:

\begin{enumerate}
    \item \textbf{Logical Reordering of Sections:} The regenerated HTML reorganized the page's sections into a more logical, linear sequence that better suits screen reader navigation. For example, participants observed that in the regenerated version, the reviews and seller profile appear after the product name and details, but before the purchase options. This follows a logical order that aligns with the linear browsing patterns of screen readers. In contrast, on the original website, elements were arranged for visual aesthetics—product images and reviews were positioned on the left side, which in the HTML code would be read first by screen readers, even before the product name and details. Additionally, the purchase options were presented before the product details in the HTML structure. P6 highlighted this improvement, \textit{"[The regenerated HTML version] is highly structured. Even for people who just started using screen readers, they can definitely navigate quickly without confusion."}

    \item \textbf{Addition of Summary Headings:} The regenerated HTML version added summary text to sections and aligned them as headings, which participants found extremely helpful. These changes made version 2 stand out not only from the original website but also from other shopping websites. Participants considered this a uniquely nuanced accessibility improvement. As P3 noted, \textit{``Version 2 has a title in the heading 'purchase options' that summarizes buttons and information related to purchasing, something I have never seen on any other websites. Others normally just put all buttons in headings. Now I can quickly jump through the purchase information like ``add to cart'' and ``buy now'' while browsing; that's impressive.''} P5 also mentioned \textit{``It is very easy to read because it has both headings and lists. ''}

    \item \textbf{Enhanced Navigation Flexibility:} The regenerated version used different labels on the same element, enriching screen readers' options for navigation. P7 mentioned that each product title is not only a heading but also a link and part of a list structure: \textit{``It's very flexible to navigate; I can use list navigation, heading navigation, or link navigation.''} P14 noted \textit{``The number of times I had to search through headings to reach the results has significantly decreased.''}
\end{enumerate}
\vspace{-0.14in}
\subsubsection{User Perception on Content Similarity}
\label{sec:user_similarity}
When it comes to self-reported perception of content similarity, all participants noted the high similarity in content between versions, only noting the changes in design or layout of the website. For instance, P9 has mentioned that \textit{``I don't think there's any difference. It was just where they were located and how long it took to get to them.''} While P8 mentioned that \textit{``overall, it looked pretty similar.''} and similarly P10 has mentioned \textit{``It definitely seems like a lot of the same content, you know, just in different orders depending on how you click on stuff.''}

% \fixme{
% All participants perceived content are pretty similar among different versions of websites, only the design or layout are different
% P9: ``there are. I don't think there's any difference. It was just where they were located and how long it took to get to them.''

% P8: ``overall, it looked pretty similar.''

% P10: ``It definitely seems like a lot of the same content, you know, just in different orders depending on how you click on stuff. ''}
% \todo{Yaman: add content of user perception on content similarity}

% Regenerated HTML version (version 2, Q4$\bar{x}$ = 4.86) was perceived even better than the reorganized HTML tag verison (version 3, Q4$\bar{x}$ = 4.28) on locating essential information and understanding the hierachy of website content. In version 2,  the LLM regenerated the entire HTML file, which resulted in several unique improvements that version 3 cannot achieve: (1) adjusted the page's section sequence in a more logical, linear order, such as moving reviews after product details, which more align with screen reader's browsing behavior. For example, participants notice that reviews and seller profile are before the purchase options but after product name and details, which follow the logic and align with screen reader linear browse patterns. P6 specifically mentioned that \textit{``[Regenerated HTML version] is highly structured. Even for people who just started using screen readers, they can definitely navigate quickly without confusion.''}Additionally, (2) it added summary text to sections and aligned them as headings, which participants found highly helpful. These changes made the regenerated HTML version stand out from the original website and even from other shopping websites. Participants mentioned \textit{``version 2 has the title in heading ``purchase options'' for summarizing buttons and information related to purchase. That I never saw on any other websites. Others normally just put all buttons in headings. Now I can quickly jump through the purchase informations while browsing, that's impressive.''} like add to cart and buy now and information like what payment they support. P3 perceived it as unique nuaced accessiblity improvements she never see on any other shopping website before: \textit{``This version summarized purchase option together and ''}  (3) it use different labels on one element which enrich screen readers' options to navigate. P7 mentioned that each product title are not only in headings but also as link and in a list structure: \textit{``it's very flexible to navigate, like I can use the list navigation. I can use heading navigation. I can use link navigation.''}



% Participants identified the most significant improvements in the regenerated HTML version as enhancements to the heading hierarchy. In this version, the LLM regenerated the entire HTML file, which resulted in several benefits:  and (3) adjusted the page's section sequence in a more logical, linear order, such as moving reviews after product details. Additionally, (4) it added summary text to sections and aligned them as headings, which participants found highly helpful. These changes made the regenerated HTML version stand out from the original website and even from other shopping websites. P3 mentioned that

% \subsubsection{Challenges on reorganized HTML tags version}
% only have first two improvements and not last two


% \subsubsection{A Version with Re-generated HTML}
% \subsubsection{A Version with Re-organized HTML Tags}




