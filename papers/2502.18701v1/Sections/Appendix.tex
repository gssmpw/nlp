\section{Appendix}

\subsection{Prompts}
\label{sec:prompt}
In GPT-4o, a prompt can consist of multiple messages. A system message includes instructions for the entire interaction. Otherwise, the interface is structured as a conversation between an ``assistant'' and a ``users'', with a sequence of intermixed messages from either party. Assistant messages represent a history of previous statements by the AI assistant and user message represent input by the user. The entire conversation is constructed for each call, allowing alternations depending on the context.

The following prompts are used in Option 1 to regenerate the HTML.

\textbf{System Prompt}: ``Your task is to enhance the accessibility of the shopping website for blind screen reader users. Screen reader users navigate websites differently from visual browsing, moving sequentially through content and using shortcuts to jump between headings, links, and form controls. First, they may use shortcuts to list all headings or navigate through headings to understand the page layout. Secondly, users often look for the main navigation menu to understand the site structure. They may use shortcuts to find lists, which are often used for navigation menus. Lastly, users rely on clear buttons and link labels to understand interactive elements. They need feedback after performing actions (e.g., adding items to cart). Adjust the HTML code to create a more user-friendly and accessible version of the executable website for screen reader users. Remove non-essential elements such as pictures, <style>, <script> that do not affect the screen reader experience. Ensure that all text and functions from the original website HTML is included in the output. Ensuring that no information is omitted in the output''

Additional instructions are given as a \textbf{User Input Prompt}:``Follow the accessibility guidelines to generate executable text-only screen reader user friendly HTML code that does not omit any information. Improving the information hierarchy for screen reader users is crucial: ensure all information that screen reader users need to read has appropriate headings. Reorganize headings correctly to outline the page structure, allowing users to understand the layout easily. Prioritize the sequence of headings to highlight the information that screen reader users are most likely to care about on this website.''

Since multiple passes may be necessary to send the full HTML content, there are some additional messages which will be included in the prompt.

An \textbf{Assistant Prompt} is included which reports the current status of the transformation: ``This is previous response from the system: \textit{<HTML content>}'' along with a \textbf{User Input Prompt} which contains the next portion of HTML to process. For example, ``This is part 3 of the HTML document. Please continue processing it while maintaining the structure. \textit{<HTML content>}''

% On the last pass, the instructions in the user prompt are changed to "Follow the guidelines to generate executable text-only screen reader user friendly HTML code that does not omit any information. Improving the information hierarchy for screen reader users is crucial: ensure all information that screen reader users need to read has appropriate headings. Reorganize headings correctly to outline the page structure, allowing users to understand the layout easily. Prioritize the sequence of headings to highlight the information that screen reader users are most likely to care about on this website. Continue output exactly after previous response, ensuring that no information is omitted in the final output!"

The following prompts are used in Option 2 to update the HTML tags.

The \textbf{System Prompt}: ``You are an accessibility expert. You understand that screen reader users have a unique browsing behavior while navigating websites, which differs from visual browsing. Users navigate content sequentially, moving from one element to the next. They may jump between different types of elements (e.g., headings, links, form controls) using shortcuts. They often look for the main navigation menu to understand the site structure and rely on clear buttons and link labels to understand interactive elements. They need feedback after performing actions (e.g., adding items to cart).''

Additional instructions are given as a \textbf{User Input Prompt}: ``Please adjust the HTML tags according to screen reader users\' browsing behavior and expectations for navigating a shopping website. Ensure that the information is perceivable, convenient, and quick to access for screen reader users. Important: Do not add any new attributes, elements, or hierarchical structures that were not present in the original input. The output must strictly follow the format of the original input, adjusting only the tags or their sequence where necessary. Do not introduce new attributes or child elements.''

As in Option 1, there may be cases where the content needs to be split into multiple chunks and sent in pieces. In that case, the \textbf{Assistant Prompt} will be changed to say, "This is previous response from the system: \textit{<JSON content>}"

In the final iteration, the assistant message changes to: "This is the new content from the original input that needs to be added: \textit{<JSON content>}" along with a user message which contains the remaining JSON to process. For example, "\textit{<remaining JSON content>} Ensure that each JSON object is fully formed, without including any parts that have already been generated in previous response from the system in the \textit{<final JSON output>}. The output should be complete and should not end with an incomplete object or ellipsis ('...'). Avoid duplicating any previously processed content."

\subsection{Implementation Details}
\label{sec:imple_detail}
\textbf{Workflow for Option 1: Regenerated HTML Version}
\textit{Step 1: HTML Content Collection.} To collect the HTML content from the webpage as input to the pipeline, we use a Google Chrome Extension that interacts directly with the Document Object Model (DOM). The extension includes a \textbf{content script} that runs in the context of the webpage and uses JavaScript to traverse the entire DOM. By calling \textit{document.documentElement.outerHTML}, the script captures the complete HTML source code of the webpage, including dynamically generated content rendered by JavaScript. This capture HTML is then sent to the \textbf{background script} of the extension, which handles data processing and communication with LLM API.

\textit{Step 2: HTML Data Processing.} Before making an API call to the LLM, the collected HTML data undergoes several preprocessing steps to prepare it for input:
\begin{enumerate}
    \item Segmentation: The HTML content is often too large to fit into a single API call due to the LLM's context window limitations. Therefore, the HTML is segmented into smaller, manageable chunks. This segmentation is performed based on the structure of the DOM tree, ensuring that each segment represents a coherent portion of the webpage (e.g., a specific section or div element).
    \item Tokenization and Length Management: The filtered HTML segments are tokenized to ensure that the total number of tokens in each segment does not exceed the context window size of the LLM. If a segment is still too large, it is further broken down while maintaining its logical structure.
\end{enumerate}

\textit{Step 3: API Call to the LLM.} For Option 1, we use the \textbf{chatgpt-4o-latest} model, which offers a larger input window size of 128,000 tokens and an output window size of 16,384 tokens. These capacities are well-suited for regenerating entire HTML documents. The background script performs multiple sequential API calls to the LLM, with each call processing a different chunk of the HTML content. The API calls are made using the fetch function in the browser's Web API, sending HTTP requests to the OpenAI JSON API endpoint. We use both a system prompt and a user input prompt, as described in Section \ref{appendix}, to guide the model in generating accessible HTML. Each generated output is then fed back into the next API call as context, ensuring continuity and cohesion across the entire webpage regeneration process.

\textit{Step 4: Similarity Check.} After we get complete output and before replacing the original webpage content with the regenerated HTML, our system performs a similarity check to ensure that the regenerated HTML maintains content consistency with the original website. The original and regenerated HTML texts are extracted and normalized by converting them to lowercase and removing extra whitespaces. Then we use a combination of Jaccard similarity, length similarity, and Levenshtein distance to ensure that the regenerated HTML content retains the semantic and structural integrity of the original website. Jaccard similarity and length similarity provide a measure of content overlap and proportional length, ensuring that the regenerated HTML captures the same key information as the original. Meanwhile, Levenshtein distance evaluates the minimal edits needed to transform one text into another, helping us assess whether the structure of the content remains consistent. The thresholds for the content similarity checks—Jaccard similarity (0.85), length similarity (0.90), and Levenshtein distance (0.3)—were carefully chosen to ensure a balance between preserving the original content and allowing for meaningful modifications to enhance accessibility. 

\textit{Step 5: Replacing the Webpage Content.} Once the similarity check confirms that the regenerated HTML aligns with the original content, the Chrome Extension uses the JavaScript DOM API to replace the existing DOM structure with the new version. The updated content is dynamically injected back into the webpage, providing screen reader users with a more accessible browsing experience. All interactive elements in the regenerated HTML, such as buttons, forms, and links, are not only present but also fully functional. If the similarity checks fail, the system makes another API call to the LLM to regenerate the HTML.

\textbf{Workflow of Option 2: Reorganizing HTML Tags}
\textit{Step 1: Extract Original HTML Content.} The first step involves extracting the HTML content from a webpage, which serves as input for the LLM. Similar to Option 1, this is done using a Google Chrome Extension that interacts with the Document Object Model (DOM) of the webpage. The extension’s content script captures the HTML content, including dynamically rendered elements, by traversing the DOM and serializing the structure. This serialization process produces a JSON representation of the DOM, containing details like tag names, attributes, and content. The JSON format allows for more granular modifications to specific HTML elements.

\textit{Step 2: Preprocess HTML Content for API Call.} After extraction, the HTML content is preprocessed to prepare it for the LLM API call. The preprocessing involves cleaning the HTML by removing irrelevant elements, such as scripts, ads, and tracking pixels, that do not contribute to accessibility or usability. The cleaned HTML is then converted into a structured JSON object, which represents the tags, attributes, and text content. This JSON format is critical as it allows the LLM to understand and manipulate the structure of the webpage more effectively. The input is divided into chunks that fit within the model's input window size of 32,768 tokens for \textbf{gpt-4o}.

\textit{Step 3: API Call to the LLM.} The API calls are made to the \textbf{gpt-4o} model using the fetch function in the browser’s Web API. The model's input window size of 32,768 tokens is sufficient for processing detailed JSON representations of the HTML content. The API call involves sending the serialized JSON object to the LLM, accompanied by a carefully crafted system prompt and user input prompt (as described in \ref{appendix}) that instructs the model to reorganize the HTML tags for better screen reader navigation without altering the content itself. The LLM processes the input JSON and returns a modified JSON object with optimized tags and attributes. The modified JSON is then used to update the DOM on the client side, specifically targeting tag replacements and attribute enhancements to improve semantic clarity and navigation.

\textit{Step 4: Similarity Check.} Before applying the reorganized HTML tags to the live webpage, a similarity check is performed between the original and modified versions to ensure that the optimizations do not introduce significant deviations from the original content. Two similarity measures are utilized: Jaccard Similarity is used to compare the sets of HTML elements in the original and reorganized versions. This similarity measure helps identify any elements that have been improperly removed or added. Length Similarity checks compare the total length of the HTML content between the original and modified versions.

\textit{Step 5: Replacing the Webpage Content.} Once the output passes the similarity checks, the reorganized HTML tags are used to update the DOM of the active webpage. The replaceTags() function iterates through the matched elements on the page and replaces them with the updated elements based on the LLM's output. This ensures that the webpage's visual content remains intact while improving the accessibility for screen reader users. All functions and links on the modified webpage are kept clickable and functional, maintaining a seamless user experience. If the similarity checks do not pass, another API call is initiated to regenerate the output until satisfactory results are achieved.