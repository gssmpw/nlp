@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{allen2022massive,
  title={A massive 7T {fMRI} dataset to bridge cognitive neuroscience and artificial intelligence},
  author={Allen, Emily J and St-Yves, Ghislain and Wu, Yihan and Breedlove, Jesse L and Prince, Jacob S and Dowdle, Logan T and Nau, Matthias and Caron, Brad and Pestilli, Franco and Charest, Ian and others},
  journal={Nature neuroscience},
  volume={25},
  number={1},
  pages={116--126},
  year={2022},
  publisher={Nature Publishing Group US New York}
}

@article{aw2023instruction,
  title={Instruction-tuning Aligns LLMs to the Human Brain},
  author={Aw, Khai Loong and Montariol, Syrielle and AlKhamissi, Badr and Schrimpf, Martin and Bosselut, Antoine},
  journal={arXiv preprint arXiv:2312.00575},
  year={2023}
}

@article{chen2023mindgpt,
  title={Mindgpt: Interpreting what you see with non-invasive brain recordings},
  author={Chen, Jiaxuan and Qi, Yu and Wang, Yueming and Pan, Gang},
  journal={arXiv preprint arXiv:2309.15729},
  year={2023}
}

@article{cheng2024videollama,
  title={VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs},
  author={Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others},
  journal={arXiv preprint arXiv:2406.07476},
  year={2024}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{conover2023free,
  title={Free dolly: Introducing the worldâ€™s first truly open instruction-tuned llm},
  author={Conover, Mike and Hayes, Matt and Mathur, Ankit and Xie, Jianwei and Wan, Jun and Shah, Sam and Ghodsi, Ali and Wendell, Patrick and Zaharia, Matei and Xin, Reynold},
  journal={Company Blog of Databricks},
  year={2023}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@article{ferrante2023brain,
  title={Brain Captioning: Decoding human brain activity into images and text},
  author={Ferrante, Matteo and Ozcelik, Furkan and Boccato, Tommaso and VanRullen, Rufin and Toschi, Nicola},
  journal={arXiv preprint arXiv:2305.11560},
  year={2023}
}

@inproceedings{han2024onellm,
  title={Onellm: One framework to align all modalities with language},
  author={Han, Jiaming and Gong, Kaixiong and Zhang, Yiyuan and Wang, Jiaqi and Zhang, Kaipeng and Lin, Dahua and Qiao, Yu and Gao, Peng and Yue, Xiangyu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26584--26595},
  year={2024}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}

@article{huang2024brainchat,
  title={BrainChat: Decoding Semantic Information from {fMRI} using Vision-language Pretrained Models},
  author={Huang, Wanaiu},
  journal={arXiv preprint arXiv:2406.07584},
  year={2024}
}

@article{kondratyuk2023videopoet,
  title={Videopoet: A large language model for zero-shot video generation},
  author={Kondratyuk, Dan and Yu, Lijun and Gu, Xiuye and Lezama, Jos{\'e} and Huang, Jonathan and Schindler, Grant and Hornung, Rachel and Birodkar, Vighnesh and Yan, Jimmy and Chiu, Ming-Chang and others},
  journal={arXiv preprint arXiv:2312.14125},
  year={2023}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@inproceedings{longpre2023flan,
  title={The flan collection: Designing data and methods for effective instruction tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  booktitle={International Conference on Machine Learning},
  pages={22631--22648},
  year={2023},
  organization={PMLR}
}

@article{mai2023unibrain,
  title={Unibrain: Unify image reconstruction and captioning all in one diffusion model from human brain activity},
  author={Mai, Weijian and Zhang, Zhijun},
  journal={arXiv preprint arXiv:2308.07428},
  year={2023}
}

@article{mishra2021cross,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2104.08773},
  year={2021}
}

@inproceedings{qi2025shapellm,
  title={Shapellm: Universal 3d object understanding for embodied interaction},
  author={Qi, Zekun and Dong, Runpei and Zhang, Shaochen and Geng, Haoran and Han, Chunrui and Ge, Zheng and Yi, Li and Ma, Kaisheng},
  booktitle={European Conference on Computer Vision},
  pages={214--238},
  year={2025},
  organization={Springer}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2021}
}

@article{scotti2024mindeye2,
  title={MindEye2: Shared-Subject Models Enable {fMRI}-To-Image With 1 Hour of Data},
  author={Scotti, Paul S and Tripathy, Mihir and Villanueva, Cesar Kadir Torrico and Kneeland, Reese and Chen, Tong and Narang, Ashutosh and Santhirasegaran, Charan and Xu, Jonathan and Naselaris, Thomas and Norman, Kenneth A and others},
  journal={arXiv preprint arXiv:2403.11207},
  year={2024}
}

@article{scotti2024reconstructing,
  title={Reconstructing the mind's eye: {fMRI}-to-image with contrastive learning and diffusion priors},
  author={Scotti, Paul and Banerjee, Atmadeep and Goode, Jimmie and Shabalin, Stepan and Nguyen, Alex and Dempster, Aidan and Verlinde, Nathalie and Yundler, Elad and Weisberg, David and Norman, Kenneth and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@article{wang2022git,
  title={Git: A generative image-to-text transformer for vision and language},
  author={Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  journal={arXiv preprint arXiv:2205.14100},
  year={2022}
}

@article{wang2023cogvlm,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  journal={arXiv preprint arXiv:2311.03079},
  year={2023}
}

@inproceedings{wang2024mindbridge,
  title={Mindbridge: A cross-subject brain decoding framework},
  author={Wang, Shizun and Liu, Songhua and Tan, Zhenxiong and Wang, Xinchao},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11333--11342},
  year={2024}
}

@article{wang2024unibrain,
  title={UniBrain: A Unified Model for Cross-Subject Brain Decoding},
  author={Wang, Zicheng and Zhao, Zhen and Zhou, Luping and Nachev, Parashkev},
  journal={arXiv preprint arXiv:2412.19487},
  year={2024}
}

@article{xia2024umbrae,
  title={UMBRAE: Unified Multimodal Decoding of Brain Signals},
  author={Xia, Weihao and de Charette, Raoul and {\"O}ztireli, Cengiz and Xue, Jing-Hao},
  journal={arXiv preprint arXiv:2404.07202},
  year={2024}
}

@article{xu2023pointllm,
  title={Pointllm: Empowering large language models to understand point clouds},
  author={Xu, Runsen and Wang, Xiaolong and Wang, Tai and Chen, Yilun and Pang, Jiangmiao and Lin, Dahua},
  journal={arXiv preprint arXiv:2308.16911},
  year={2023}
}

@article{yu2022coca,
  title={Coca: Contrastive captioners are image-text foundation models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={arXiv preprint arXiv:2205.01917},
  year={2022}
}

@article{zhang2023instruction,
  title={Instruction tuning for large language models: A survey},
  author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
  journal={arXiv preprint arXiv:2308.10792},
  year={2023}
}

@article{zhang2023internlm,
  title={Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition},
  author={Zhang, Pan and Dong, Xiaoyi and Wang, Bin and Cao, Yuhang and Xu, Chao and Ouyang, Linke and Zhao, Zhiyuan and Duan, Haodong and Zhang, Songyang and Ding, Shuangrui and others},
  journal={arXiv preprint arXiv:2309.15112},
  year={2023}
}

@article{zhang2023video,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={arXiv preprint arXiv:2306.02858},
  year={2023}
}

