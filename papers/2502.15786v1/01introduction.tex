\section{Introduction}
Decoding human brain activity (fMRI) to text has sparked significant interest within the neuroscience community \cite{xia2024umbrae,chen2023mindgpt,luo2023brainscuba,hmamouche2024multimodal}. The ability to translate brain activity patterns into natural language carries both academic and societal importance. For neuroscientists, it provides deeper and novel insights into cognition, behavior, and consciousness \cite{qiu2023learning,luo2023brainscuba}. On a societal level, it presents opportunities for medical applications and improves human-computer interaction (HCI) \cite{bernal2022brain,du2022fmri}. For example, for individuals with speech impairments, this technology could restore communication capabilities, enabling them to express their thoughts effortlessly \cite{card2024accurate}. Moreover, as shown in Figure~\ref{fig:teaser}, it benefits healthy individuals by allowing neural control of digital devices, such as embodied AIs or prosthetic limbs, allowing for more intuitive and precise movements.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/teaser.pdf}
    \vspace{-2em}
    \caption{The overview of our method. \name{} is equipped with a subject-agnostic fMRI encoder and an off-the-shelf LLM. \name{} is trained on multiple subjects with varying input shapes and an instruction-tuning dataset, aiming to encode different facets of semantic information in fMRI. After training, \name{} is capable of various text decoding tasks. One application is that the decoded contents can be used to achieve neural control of existing systems that are not designed for it.}
    \label{fig:teaser}
    \vspace{-1.5em}
\end{figure}


Despite its potential, decoding brain activity to language still faces significant challenges. One major obstacle is the need for versatile decoding tailored to specific applications. For example, decoding may aim to translate a subject's movement intention to control a prosthesis, or to interpret abstract thoughts or memories. Traditional models fail to accommodate such diverse requirements. To address this, UMBRAE \cite{xia2024umbrae} integrates a Visual Language Model (VLM) \cite{chen2023shikra} 
 and learns to map from fMRI data to corresponding stimulus images. While this approach achieves versatility to some extent, it remains constrained to tasks directly tied to the current stimulus image and cannot address broader tasks, such as retrieving memories of past visual experiences.

Another critical challenge lies in designing a unified and subject-agnostic architecture. Current methods of brain multimodal decoding mostly rely on a preprocessing step: selecting responsive voxels by comparing the task-based fMRI to the resting-state fMRI. The selection typically results in higher performance compared to using whole-brain data. However, the varying number and irregular spatial distribution of selected voxels across subjects pose significant challenges for developing a unified architecture. To this end, recent studies \cite{wang2024mindbridge,wang2024unibrain} have proposed pooling or sampling voxels to standardize input dimensions. However, as illustrated in Figure~\ref{fig:arch}, these methods still suffer from the loss of spatial information and uneven representations of certain areas, ultimately comprising performance.

\noindent\textbf{Present Work} Here we present \name{}, a subject-agnostic and versatile model for fMRI-to-text decoding. Our approach consists of a subject-agnostic fMRI encoder and an off-the-shelf LLM. The subject-agnostic fMRI encoder incorporates a neuroscience-informed attention layer with learnable \textit{queries}, enabling dynamic feature extraction by leveraging both spatial information and neuroscientific priors of voxels, thereby significantly enhancing prediction accuracy. The design of \textit{values} and \textit{keys} separates the voxel's functional information-which is largely consistent across individuals-from its fMRI value, allowing the model to benefit from shared priors across subjects and enhancing generalization to novel subjects. Moreover, to address the challenge of versatile decoding, we propose Brain Instruction Tuning (BIT). BIT trains the model using a diverse dataset that employs images as intermediaries, encompassing tasks designed to capture diverse aspects of semantic information encoded in fMRI data, including perception \& scene understanding, memory \& knowledge retrieval, language \& symbolic processing, and complex reasoning. Figure~\ref{fig:teaser} illustrates the corresponding components.

We evaluate our model on comprehensive benchmarks. Results reveal it outperforms baselines with $12.0\%$ average improvement in various downstream tasks and $16.4\%$ improvement in generalization on unseen subjects. Additionally, we show that our model adapts effectively to novel tasks, demonstrating high customizability and flexibility in real-world applications. Furthermore, our analysis of attention weights offers valuable insights into the working mechanism of our fMRI encoder.
