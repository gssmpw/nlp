[
  {
    "index": 0,
    "papers": [
      {
        "key": "wang2024mindbridge",
        "author": "Wang, Shizun and Liu, Songhua and Tan, Zhenxiong and Wang, Xinchao",
        "title": "Mindbridge: A cross-subject brain decoding framework"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wang2024unibrain",
        "author": "Wang, Zicheng and Zhao, Zhen and Zhou, Luping and Nachev, Parashkev",
        "title": "UniBrain: A Unified Model for Cross-Subject Brain Decoding"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "chen2023mindgpt",
        "author": "Chen, Jiaxuan and Qi, Yu and Wang, Yueming and Pan, Gang",
        "title": "Mindgpt: Interpreting what you see with non-invasive brain recordings"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "mai2023unibrain",
        "author": "Mai, Weijian and Zhang, Zhijun",
        "title": "Unibrain: Unify image reconstruction and captioning all in one diffusion model from human brain activity"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ferrante2023brain",
        "author": "Ferrante, Matteo and Ozcelik, Furkan and Boccato, Tommaso and VanRullen, Rufin and Toschi, Nicola",
        "title": "Brain Captioning: Decoding human brain activity into images and text"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      },
      {
        "key": "wang2022git",
        "author": "Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan",
        "title": "Git: A generative image-to-text transformer for vision and language"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "huang2024brainchat",
        "author": "Huang, Wanaiu",
        "title": "BrainChat: Decoding Semantic Information from {fMRI} using Vision-language Pretrained Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "devlin2018bert",
        "author": "Devlin, Jacob",
        "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
      },
      {
        "key": "he2022masked",
        "author": "He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\\'a}r, Piotr and Girshick, Ross",
        "title": "Masked autoencoders are scalable vision learners"
      },
      {
        "key": "yu2022coca",
        "author": "Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui",
        "title": "Coca: Contrastive captioners are image-text foundation models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "xia2024umbrae",
        "author": "Xia, Weihao and de Charette, Raoul and {\\\"O}ztireli, Cengiz and Xue, Jing-Hao",
        "title": "UMBRAE: Unified Multimodal Decoding of Brain Signals"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "allen2022massive",
        "author": "Allen, Emily J and St-Yves, Ghislain and Wu, Yihan and Breedlove, Jesse L and Prince, Jacob S and Dowdle, Logan T and Nau, Matthias and Caron, Brad and Pestilli, Franco and Charest, Ian and others",
        "title": "A massive 7T {fMRI} dataset to bridge cognitive neuroscience and artificial intelligence"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "scotti2024reconstructing",
        "author": "Scotti, Paul and Banerjee, Atmadeep and Goode, Jimmie and Shabalin, Stepan and Nguyen, Alex and Dempster, Aidan and Verlinde, Nathalie and Yundler, Elad and Weisberg, David and Norman, Kenneth and others",
        "title": "Reconstructing the mind's eye: {fMRI}-to-image with contrastive learning and diffusion priors"
      },
      {
        "key": "scotti2024mindeye2",
        "author": "Scotti, Paul S and Tripathy, Mihir and Villanueva, Cesar Kadir Torrico and Kneeland, Reese and Chen, Tong and Narang, Ashutosh and Santhirasegaran, Charan and Xu, Jonathan and Naselaris, Thomas and Norman, Kenneth A and others",
        "title": "MindEye2: Shared-Subject Models Enable {fMRI}-To-Image With 1 Hour of Data"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wang2024mindbridge",
        "author": "Wang, Shizun and Liu, Songhua and Tan, Zhenxiong and Wang, Xinchao",
        "title": "Mindbridge: A cross-subject brain decoding framework"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wang2024unibrain",
        "author": "Wang, Zicheng and Zhao, Zhen and Zhou, Luping and Nachev, Parashkev",
        "title": "UniBrain: A Unified Model for Cross-Subject Brain Decoding"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      },
      {
        "key": "zhang2023internlm",
        "author": "Zhang, Pan and Dong, Xiaoyi and Wang, Bin and Cao, Yuhang and Xu, Chao and Ouyang, Linke and Zhao, Zhiyuan and Duan, Haodong and Zhang, Songyang and Ding, Shuangrui and others",
        "title": "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition"
      },
      {
        "key": "wang2023cogvlm",
        "author": "Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others",
        "title": "Cogvlm: Visual expert for pretrained language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "cheng2024videollama",
        "author": "Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others",
        "title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs"
      },
      {
        "key": "kondratyuk2023videopoet",
        "author": "Kondratyuk, Dan and Yu, Lijun and Gu, Xiuye and Lezama, Jos{\\'e} and Huang, Jonathan and Schindler, Grant and Hornung, Rachel and Birodkar, Vighnesh and Yan, Jimmy and Chiu, Ming-Chang and others",
        "title": "Videopoet: A large language model for zero-shot video generation"
      },
      {
        "key": "zhang2023video",
        "author": "Zhang, Hang and Li, Xin and Bing, Lidong",
        "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "xu2023pointllm",
        "author": "Xu, Runsen and Wang, Xiaolong and Wang, Tai and Chen, Yilun and Pang, Jiangmiao and Lin, Dahua",
        "title": "Pointllm: Empowering large language models to understand point clouds"
      },
      {
        "key": "qi2025shapellm",
        "author": "Qi, Zekun and Dong, Runpei and Zhang, Shaochen and Geng, Haoran and Han, Chunrui and Ge, Zheng and Yi, Li and Ma, Kaisheng",
        "title": "Shapellm: Universal 3d object understanding for embodied interaction"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "driess2023palm",
        "author": "Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others",
        "title": "Palm-e: An embodied multimodal language model"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "chowdhery2023palm",
        "author": "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others",
        "title": "Palm: Scaling language modeling with pathways"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "han2024onellm",
        "author": "Han, Jiaming and Gong, Kaixiong and Zhang, Yiyuan and Wang, Jiaqi and Zhang, Kaipeng and Lin, Dahua and Qiao, Yu and Gao, Peng and Yue, Xiangyu",
        "title": "Onellm: One framework to align all modalities with language"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "zhang2023instruction",
        "author": "Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others",
        "title": "Instruction tuning for large language models: A survey"
      },
      {
        "key": "aw2023instruction",
        "author": "Aw, Khai Loong and Montariol, Syrielle and AlKhamissi, Badr and Schrimpf, Martin and Bosselut, Antoine",
        "title": "Instruction-tuning Aligns LLMs to the Human Brain"
      },
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      },
      {
        "key": "thoppilan2022lamda",
        "author": "Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others",
        "title": "Lamda: Language models for dialog applications"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "longpre2023flan",
        "author": "Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others",
        "title": "The flan collection: Designing data and methods for effective instruction tuning"
      },
      {
        "key": "sanh2021multitask",
        "author": "Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others",
        "title": "Multitask prompted training enables zero-shot task generalization"
      },
      {
        "key": "conover2023free",
        "author": "Conover, Mike and Hayes, Matt and Mathur, Ankit and Xie, Jianwei and Wan, Jun and Shah, Sam and Ghodsi, Ali and Wendell, Patrick and Zaharia, Matei and Xin, Reynold",
        "title": "Free dolly: Introducing the world\u2019s first truly open instruction-tuned llm"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "mishra2021cross",
        "author": "Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh",
        "title": "Cross-task generalization via natural language crowdsourcing instructions"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "sanh2021multitask",
        "author": "Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others",
        "title": "Multitask prompted training enables zero-shot task generalization"
      }
    ]
  }
]