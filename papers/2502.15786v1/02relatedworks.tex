\section{Related Works}
\noindent\textbf{Brain-Conditioned Text Generation}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/uni_compare.pdf}
    \vspace{-2em}
    \caption{Comparison between our model and previous unified models. MindBridge \cite{wang2024mindbridge} flattens the voxels and adaptively pools them to a fixed dimension, which overlooks the rich information in positions. UniBrain~ \cite{wang2024unibrain} uniformly samples a subset of voxels and aggregates their neighbors. Different from these methods, we propose neuroscience-informed attention, where each query token attends to all voxels, which minimizes potential information loss in pooling or sampling.}
    \label{fig:uni_compare}
    \vspace{-1em}
\end{figure}
%There have been works that decode text from brain signals. 
This line of research mostly focuses on decoding perceived visual stimuli into natural language from fMRI signals. MindGPT~\cite{chen2023mindgpt}, UniBrain\cite{mai2023unibrain} and BrainCap \cite{ferrante2023brain} employ an fMRI encoder guided by CLIP~\cite{radford2021learning} and use a language model \cite{radford2019language,wang2022git} to decode natural language from the encoded representations. BrainChat~\cite{huang2024brainchat} utilizes multiple pretraining strategies~\cite{devlin2018bert, he2022masked,yu2022coca} to align fMRI with image and text embeddings. These methods fall short in performance and versatility. UMBRAE~\cite{xia2024umbrae} proposes to learn a mapping from fMRI to stimulus images, which later serves as a proxy input for an off-the-shelf visual language model (VLM). Although they achieve performance improvements, the strategy prevents the model from performing tasks that are not directly related to the stimulus images (e.g. answering memory-related questions). In contrast, our end-to-end Brain Instruction Tuning (BIT) ensures seamless and versatile fMRI-to-text decoding, offering the potential to tackle tasks beyond vision-related ones.

\noindent\textbf{Cross-subjects Decoding}
In voxel-level machine learning for brain decoding, the number of voxels varies between subjects \cite{allen2022massive}. Most prior works \cite{scotti2024reconstructing,scotti2024mindeye2} use an MLP for each subject individually. However, due to the fixed input size required by MLP architectures, these models cannot handle varying input shapes. As illustrated in Figure~\ref{fig:uni_compare}, MindBridge \cite{wang2024mindbridge} proposed to use an adaptive max pooling layer to standardize the input shapes. However, unlike images, which are considered translation invariance, positions in fMRI carry specific bio-informative significance that pooling operations may overlook. UniBrain \cite{wang2024unibrain} proposed to sample groups of voxels. Such a sampling strategy, on the one hand, may lead to information loss if some voxels are not included in any group. On the other hand, the irregular spatial distribution of 3D voxels with varying density and curvature may result in underrepresentation or overrepresentation of certain areas. Different from these methods, our model employs a neuroscience-informed attention mechanism that accounts for every single voxel while preserving their bio-informative positional information, ensuring a more comprehensive and precise representation. 
% BrainChat trains the model based on the caption and VQA dataset. UMBRAE introduces a universal encoder that aligns fMRI signals with image stimuli.


\noindent\textbf{Multi-Modal Large Language Model}
Aiming to augment the perceptual capacities of Large Language Models (LLMs), there has been a growing interest in extending them to handle multiple modalities within a unified model. Numerous studies have attempted to incorporate modalities such as images \cite{alayrac2022flamingo,zhang2023internlm,wang2023cogvlm}, videos \cite{cheng2024videollama, kondratyuk2023videopoet,zhang2023video}, and point clouds \cite{xu2023pointllm,qi2025shapellm}. %Flamingo~\cite{alayrac2022flamingo} uses a frozen vision encoder and an LLM equipped with gated cross-attention for cross-modality alignment. PaLM-E~\cite{driess2023palm} integrates extracted visual features directly through linear layers into the pre-trained PaLM~\cite{chowdhery2023palm}. BLIP2~\cite{li2023blip} achieves vision-language alignment by introducing a querying transformer to extract visual features from the frozen image encoder and integrating them with the frozen LLM. 
OneLLM~\cite{han2024onellm} stands out by aligning eight different modalities, including fMRI, with language. However, their approach employs an individual convolution network for each subject instead of a unified architecture for fMRI encoding across subjects, which restricts its applicability to new subjects in real-world scenarios. Furthermore, the approach solely relies on captions as textual annotations, which limits the model's capability for versatile fMRI decoding. 

% \paragraph{Instruction Tuning}
% Instruction tuning \cite{zhang2023instruction, aw2023instruction, radford2019language, thoppilan2022lamda} has been widely used to enable LMs to follow specific natural language instructions. 
% A key method is Instruction Dataset Construction, where (instruction, output) pairs are generated from existing annotated datasets by applying templates to convert text-label pairs into instruction-output pairs \cite{longpre2023flan, sanh2021multitask, conover2023free}. For instance, Natural Instructions \cite{mishra2021cross} creates datasets comprising task descriptions with components like title, definition, cautions, prompts, and example pairs, while P3 \cite{sanh2021multitask} maps conventional NLP tasks, such as question answering or text classification, into natural language input-output pairs. In this work, we contribute to instruction tuning from a neuroscience perspective by introducing the Brain Instruction Tuning Dataset, which decodes brain activity into language representations.

