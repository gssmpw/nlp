\section{Method}
\label{sec:method}
In this section, we propose a neuroscience-informed fMRI encoder designed to achieve high-performance, subject-agnostic decoding. To further enable versatile decoding, we introduce the construction of a brain instruction tuning dataset, which captures diverse semantic representations encoded in fMRI data.

\subsection{Method Overview}
As illustrated in Figure~\ref{fig:arch}, our model consists of an fMRI encoder $f_\theta$ and an off-the-shelf LLM. In practice, we use Vicuna-7b \cite{zheng2023judging} as our LLM to maintain consistency with our baseline \cite{xia2024umbrae}. For each sample, let $\boldsymbol{v} = [v_1, v_2, \cdots, v_N]\in \mathbb{R}^N$ be the fMRI signals of input voxels, where $N$ is the number of voxels. Note that $N$ varies between different subjects, ranging from $12,682$ to $17,907$ in the dataset we use \cite{allen2022massive}.

The fMRI encoder $f_\theta$, featuring a neuroscience-informed attention layer, encodes $\boldsymbol{v}$ to fMRI tokens $X_v = [\boldsymbol{x}_{v,1}, \boldsymbol{x}_{v,2}, \cdots, \boldsymbol{x}_{v,L}] \in \mathbb{R}^{d\times L}$, where $L$ is the number of tokens and $d$ is the dimension of token embeddings. We then prepend these learned fMRI tokens to the language tokens in the BIT dataset we propose.

\subsection{fMRI Encoder}
As mentioned before, currently most models for fMRI decoding can not handle varying input shapes and are not subject-agnostic, with only a few exceptions \cite{mai2023unibrain}. However, these exceptions still suffer from information loss and uneven representations of certain brain areas. To this end, we propose a novel neuroscience-informed attention mechanism to accommodate varying voxel numbers across subjects, enabling a subject-agnostic encoding strategy. Below we talk about the design of \textit{queries} $\{\boldsymbol{q}_i\}$, \textit{keys} $\{\boldsymbol{k}_i\}$ and \textit{values} $\{\boldsymbol{v}_i\}$ in the attention layer. For \textit{values}, we directly use the fMRI signal of each voxel, which means $\boldsymbol{v_i} = v_i \in \mathbb{R}$. Making each voxel a \textit{value} token maximally prevents information loss compared to pooling- \cite{wang2024mindbridge} or sampling-based \cite{mai2023unibrain} methods. The \textit{queries} are randomly initialized and learnable. We expect each \textit{query} to represent a certain pattern of the brain (refer to visualizations in Section \ref{sec:vis}). The design of \textit{keys} will be discussed below.

\noindent\textbf{Exclude fMRI values from \textit{keys}}
The vanilla cross attention \cite{zhu2020deformable,vaswani2017attention} derives both \textit{keys} and \textit{values} from the same input source. However, we found this would lead to poor performance in fMRI. We argue the reason: different from images or text, which are usually considered translation-invariant, the positions of voxels carry specific brain \textit{functional information}, as voxels in different areas are associated with distinct brain functions. Consequently, a voxel's position alone can theoretically serve as effective \textit{keys} for attention weight computation. Including fMRI values into \textit{keys}, however, introduces additional noise instead of valuable information, thus resulting in poorer performance. Moreover, since brain regions tend to serve similar functions across individuals, decoupling voxel positions from fMRI signals can facilitate the sharing of priors across subjects, potentially improving generalization to unseen subjects.

In light of this, instead of the vanilla cross attention, which derives the \textit{keys} and \textit{values} from the same inputs, we exclude the fMRI value of each voxel and use its positional information alone as its \textit{key} embedding. The positional information is encoded from the coordinates of each voxel, i.e. $\boldsymbol{k}_i^{\text{pos}} = \operatorname{PE}(\boldsymbol{c}_i)$ for the $i$-th voxel, where $\boldsymbol{c}_i \in \mathbb{R}^3$ denotes the coordinates of the voxel. In practice, we use the Fourier positional encoding proposed in \cite{tancik2020fourier} due to its superiority in encoding coordinate information.

\noindent\textbf{Incorporation of Brain Parcellations}
% \noindent\textbf{Incorporation of Brain Parcellations}
While positional encoding alone improves performance, it lacks inherent neuroscientific grounding, potentially making it challenging for the model to efficiently learn representations aligned with established principles of brain function. To overcome this, we incorporate existing brain region parcellations \cite{glasser2016multi,rolls2020automated} into the \textit{key} embeddings. Formally, given a parcellation $\mathcal{P}$, with regions indexed by $1, \cdots, N_\mathcal{P}$. Let $\mathcal{P}(i) \in [1, 2, \cdots, N_\mathcal{P}]$ be the region that the $i$-th voxel belongs to, and $E[\mathcal{P}(i)] \in \mathbb{R}^d$ be the corresponding learnable embedding of the region, which will be incorporated in the \textit{key} embeddings as $\boldsymbol{k}_i^{\text{reg}, \mathcal{P}} = E[\mathcal{P}(i)] \in \mathbb{R}^d$.

\noindent\textbf{Combining Multiple Parcellations}
It is crucial to choose an appropriate brain region parcellation. Previous region-based methods \cite{qiu2023learning,li2021braingnn, kan2022brain} can usually only choose one arbitrarily. In contrast, our model design allows us to combine multiple parcellations $\mathcal{P}^1, \mathcal{P}^2, \cdots$ by concatenating their respective region encodings to the \textit{key} embeddings. In conclusion, the final \textit{key} embeddings are the concatenation by the positional encoding and multiple region encodings,
\begin{equation}
    \boldsymbol{k}_i = \boldsymbol{k}_i^\text{pos} \| \boldsymbol{k}_i^{\text{reg}, \mathcal{P}^1} \|  \boldsymbol{k}_i^{\text{reg}, \mathcal{P}^2} \| \cdots
\end{equation}
where $\|$ denotes the concatenation operation. This process is illustrated in Figure~\ref{fig:arch}'s lower right part.

The positional and region encodings complement each other: The region encodings serve as coarse-scale features, providing a neuroscientific-grounded basis, while the fine-scale positional encoding allows our model to learn finer-grained information directly from the data.

This attention design separates a voxel's \textit{functional information}—which is largely consistent across individuals—from its fMRI value, thereby enhancing generalization. Instead of relying on pooling or sampling, the attention mechanism employs learnable aggregation, while the integration of positional encoding and neuroscientifically informed region encodings further ensures high performance.

After the attention layer, we obtain the hidden representations $\boldsymbol{z}_q \in \mathbb{R}^{N_q} $ where $N_q$ is the number of query embeddings. We then employ an MLP and a reshape operation to map the hidden representations to $L$ fMRI tokens, i.e., $   X_v = \operatorname{reshape}\left( \operatorname{MLP}(
    \{\boldsymbol{z}_q\}
    ) \right) \in \mathbb{R}^{L \times d}$.

The process of the fMRI encoder is illustrated in Figure~\ref{fig:arch}. The obtained fMRI tokens are then prepended to the language tokens in conversations.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/arch.pdf}
    % \vspace{-2.2em}
    \caption{Model Architecture. The fMRI encoder maps fMRI to a series of fMRI tokens through our proposed neuroscience-informed attention. The large language model, with both fMRI and text tokens, will be trained by brain instruction tuning.}
    \label{fig:arch}
    \vspace{-1em}
\end{figure}

\subsection{Brain Instruction Tuning (BIT)}
To enable versatile fMRI-to-text decoding, an appropriate BIT dataset is required, yet no such dataset currently exists. To bridge this gap, we construct one based on the fact: MSCOCO images \cite{chen2015microsoft} serve as stimuli for fMRI recordings in the fMRI study \cite{allen2022massive}, and an abundance of datasets provide text annotations (e.g., VQA) for MSCOCO images. Using the images as intermediaries, we select those relevant to brain functions and pair the fMRI data with corresponding text annotations. For example, given an image of a billboard with annotated textual content, we can reasonably infer that when a subject perceives textual information (e.g., contents on the billboard), corresponding representations are encoded in the brain. This suggests the possibility of extracting such information from fMRI signals. We select datasets to fulfill various purposes, enabling the model to capture diverse aspects of semantic information embedded in fMRI signals, including visual perception \& scene understanding, language \& symbolic processing, memory \& knowledge retrieval and complex reasoning, which are considered among most fundamental and essential properties of human brains \cite{robertson2002memory,stenning2012human,wade2013visual,friederici2017language}.

\begin{figure}[h]
% \vspace{-0.5em}
    \centering
    \includegraphics[width=\linewidth]{figures/bit.pdf}
\vspace{-1.8em}
    \caption{Dataset Taxonomy in Brain Instruction Tuning.}
    \label{fig:bit}
% \vspace{-1em}
\end{figure}

\noindent\textbf{Perception \& Scene Understanding} As illustrated in Figure~\ref{fig:bit}, we begin by using caption tasks at both coarse and fine-grained levels to train the model’s ability to understand and summarize what the subject perceives visually \cite{chen2015microsoft,krause2017hierarchical}. Additionally, we incorporate QA tasks \cite{ren2015exploring,krishna2017visual,acharya2019tallyqa} to enhance the model's ability to retrieve and reason about visually perceived content.

\noindent\textbf{Memory \& Knowledge Retrieval} To go beyond tasks directly related to present visual perception, we construct the \emph{previous captioning} task, a memory-oriented task that challenges the model to caption images that the subject previously viewed, simulating memory recall processes. Furthermore, we aim to encode knowledge structures in human brains. The OK-VQA \cite{marino2019ok} and A-OKVQA \cite{schwenk2022okvqa} datasets include questions requiring external knowledge that is not present in the image but resides in human brains. For example, A photo of a hydrant may prompt the answer "firetruck," even though the firetruck is absent in the image. This association also reflects the way human cognition operates through a network of interconnected meanings, where one concept unconsciously triggers another. Such a process, which is called "slippage of the signifier" \cite{lacan2001ecrits, lacan1988seminar, miller2018four}, highlights the symbolic processes through which the brain constructs and retrieves meaning. 

\noindent\textbf{Language \& Symbolic Processing} In addition to the aforementioned OK-VQA and A-OKVQA datasets, which are also related to symbolic process, we further combine datasets of text recognition \cite{biten2019scene} and numerical reasoning \cite{acharya2019tallyqa} to facilitate this aspect.

\noindent\textbf{Complex Reasoning} Finally, we try to approximate the reasoning process that happens in human brains with datasets \cite{liu2023visual,wang2023see,li2018vqa} that require intricate logical and inferential processes. We expect these datasets to challenge the model to extract the reasoning process, drawing upon both visual understanding and abstract problem-solving, thus bridging perception, memory, and knowledge into a cohesive cognitive framework.

We ended up with a brain instruction tuning dataset consisting of $980,610$ conversations associated with fMRI recordings from $15$ datasets. Appendix~\ref{app:dataset} lists the instructions and other details for each dataset. The instruction tuning enables versatile fMRI-to-text decoding. In particular, the introduction of tasks like \textit{previous caption} empowers the model to perform a broader range of tasks beyond vision-related ones, which the previous model \cite{xia2024umbrae} fails.

\begingroup
\sisetup{
  table-format=2.2,  % 3 digits before the decimal, 2 after
  table-align-text-pre=false,
  propagate-math-font=true,
  table-number-alignment=center,
  detect-weight=true,detect-inline-weight=math
}
\begin{table*}[bp]
    \centering
    \vspace{-1.7em}
    \caption{Results of brain captioning. The CIDEr metric is scaled by a factor of 100 for consistency with Table~\ref{tab:caption} and baselines.}
    \label{tab:caption}
\vspace{0.1in}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcSSSSSSSS}
    \toprule
 % \multirow{2}{*}{Method}&  \multirow{2}{*}{cross-subject}&\multicolumn{5}{c}{fMRI caption} & &  &\\
   {Method} & {\makecell{subject\\agnostic}}  &{{BLEU-1} $\uparrow$} & {BLEU-2 $\uparrow$} & {BLEU-3 $\uparrow$} & {{BLEU-4} $\uparrow$} &{METEOR $\uparrow$}&{ROUGE $\uparrow$}& {CIDEr $\uparrow$}&{SPICE $\uparrow$}\\
    \midrule
    SDRecon \cite{takagi2023high}    & {\xmark} &36.21 & 17.11 & 7.22 & 3.43   &10.03&  25.13&13.83 &5.02 \\
    OneLLM  \cite{han2024onellm}  & {\xmark} &47.04 & 26.97 & 15.49 & 9.51   &13.55&  35.05&22.99 & 6.26\\
    UniBrain \cite{mai2023unibrain}   & {\xmark} & {$-$}   & {$-$}    & {$-$}  & {$-$}     &16.90&  22.20& {$-$} & {$-$}\\
    BrainCap \cite{ferrante2023brain}  & {\xmark} &55.96 & 36.21 & 22.70 & 14.51   &16.68& 40.69&41.30 & 9.06\\
     BrainChat \cite{huang2024brainchat} & {\xmark}   &52.30& 29.20& 17.10& 10.70 &14.30& 45.70&26.10 & {$-$}\\
    UMBRAE \cite{xia2024umbrae}    & {\xmark} &59.44& 40.48& 27.66&19.03&19.45&  43.71&61.06&12.79\\
    \name{} (Ours)  & {\cmark} & \bfseries 61.75 &  \bfseries42.84 & \bfseries29.86&\bfseries21.24  & 
\bfseries 19.54 &\bfseries45.82 & 60.97  & 11.79\\
    \bottomrule
    \end{tabular}}
\end{table*}
\endgroup


To train the model with the BIT dataset, for each sample $\boldsymbol{v}$, we sample a multi-run conversation $X_t = (X_u^1, X_a^1, \cdots, X_u^T, X_a^T)$ from all conversations associated with it, where $T \geq 1$ represents the number of turns. $a$ indicates the message from the assistant and $u$ indicates the message is from the user. The training objective is to maximize the probability of the assistant's response only
$$
\arg\max_\theta p(X_a | X_v, X_{\text{inst}}) = \prod_{t=1}^T p({\color{magenta}X_a^t} | X_u^{\leq t}, X_a^{\le t }, X_\text{inst}, X_v)
$$
Figure~\ref{fig:chat} illustrates the chat template and the training objective. We freeze the weights of the LLM and only train the fMRI encoder since we want to preserve the LLM's language modeling prior and ensure a fair comparison with baselines such as \citet{xia2024umbrae}.

\noindent\textbf{Computational Complexity} According to the analysis in Appendix~\ref{app:complexity}, our model does not introduce additional complexity compared to previous methods \cite{scotti2024mindeye2, wang2024mindbridge}.


\begin{figure}[htbp]
\vspace{-0.8em}
\centering
\begin{minipage}{0.99\columnwidth}\vspace{0mm}    \centering
\begin{tcolorbox}[colback=white,colframe=gray,left=1pt,top=1pt,bottom=1pt]
\sffamily
\footnotesize	
  \texttt{<system message>}\\
  user: $X_v$, $X_\text{inst}$, $X_1^u$ \\
  assistant: {\color{magenta}$X_1^a$}\\
user: $X_2^u$\\
  assistant: {\color{magenta}$X_2^a$}\\
  $\cdots\cdots$
\end{tcolorbox}
\end{minipage}
\caption{The chat template used during instruction tuning, illustrating two turns of conversations. Two turns of conversations are shown. Tokens highlighted in {\color{magenta}magenta} are used for next-token prediction loss computation.}
\label{fig:chat}
\vspace{-1.2em}
\end{figure}

