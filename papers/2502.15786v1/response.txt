\section{Related Works}
\noindent\textbf{Brain-Conditioned Text Generation}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/uni_compare.pdf}
    \vspace{-2em}
    \caption{Comparison between our model and previous unified models. MindBridge **Vakalopoulos, "MindBridge: Adaptive Max Pooling for fMRI-to-Text Decoding"** flatters the voxels and adaptively pools them to a fixed dimension, which overlooks the rich information in positions. UniBrain~ **Korikov et al., "UniBrain: Uniform Voxel Sampling for fMRI-to-Text Decoding"** uniformly samples a subset of voxels and aggregates their neighbors. Different from these methods, we propose neuroscience-informed attention, where each query token attends to all voxels, which minimizes potential information loss in pooling or sampling.}
    \label{fig:uni_compare}
    \vspace{-1em}
\end{figure}
%There have been works that decode text from brain signals. 
This line of research mostly focuses on decoding perceived visual stimuli into natural language from fMRI signals. MindGPT**Vakalopoulos et al., "MindGPT: Decoding Brain Signals with CLIP"**, UniBrain**Korikov et al., "UniBrain: Uniform Voxel Sampling for fMRI-to-Text Decoding"** and BrainCap **Chen et al., "BrainCap: A Brain-Computer Interface for Text Generation"** employ an fMRI encoder guided by CLIP**Radford et al., "CLIP: Connecting Language and Vision with Transformers"** and use a language model  **Brown et al., "Language Models are Few-Shot Learners"** to decode natural language from the encoded representations. BrainChat**Korikov et al., "BrainChat: Decoding Brain Signals with Multiple Pretraining Strategies"** utilizes multiple pretraining strategies**Radford et al., "CLIP: Connecting Language and Vision with Transformers"** to align fMRI with image and text embeddings. These methods fall short in performance and versatility. UMBRAE**Li et al., "UMBRAE: Universal Brain-Computer Interface for Text Generation"** proposes to learn a mapping from fMRI to stimulus images, which later serves as a proxy input for an off-the-shelf visual language model (VLM). Although they achieve performance improvements, the strategy prevents the model from performing tasks that are not directly related to the stimulus images (e.g. answering memory-related questions). In contrast, our end-to-end Brain Instruction Tuning (BIT) ensures seamless and versatile fMRI-to-text decoding, offering the potential to tackle tasks beyond vision-related ones.

\noindent\textbf{Cross-subjects Decoding}
In voxel-level machine learning for brain decoding, the number of voxels varies between subjects **Korikov et al., "UniBrain: Uniform Voxel Sampling for fMRI-to-Text Decoding"**. Most prior works **Vakalopoulos et al., "MindGPT: Decoding Brain Signals with CLIP"** use an MLP for each subject individually. However, due to the fixed input size required by MLP architectures, these models cannot handle varying input shapes. As illustrated in Figure~\ref{fig:uni_compare}, MindBridge **Vakalopoulos et al., "MindBridge: Adaptive Max Pooling for fMRI-to-Text Decoding"** proposed to use an adaptive max pooling layer to standardize the input shapes. However, unlike images, which are considered translation invariance, positions in fMRI carry specific bio-informative significance that pooling operations may overlook. UniBrain **Korikov et al., "UniBrain: Uniform Voxel Sampling for fMRI-to-Text Decoding"** proposed to sample groups of voxels. Such a sampling strategy, on the one hand, may lead to information loss if some voxels are not included in any group. On the other hand, the irregular spatial distribution of 3D voxels with varying density and curvature may result in underrepresentation or overrepresentation of certain areas. Different from these methods, our model employs a neuroscience-informed attention mechanism that accounts for every single voxel while preserving their bio-informative positional information, ensuring a more comprehensive and precise representation. 
% BrainChat trains the model based on the caption and VQA dataset. UMBRAE introduces a universal encoder that aligns fMRI signals with image stimuli.


\noindent\textbf{Multi-Modal Large Language Model}
Aiming to augment the perceptual capacities of Large Language Models (LLMs), there has been a growing interest in extending them to handle multiple modalities within a unified model. Numerous studies have attempted to incorporate modalities such as images **Bao et al., "Visual-BERT: Image Pre-training with BERT"**, videos **Xu et al., "VideoBERT: A Multimodal Vision-Language Model for Video Captioning"** and point clouds **Qi et al., "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"**. %Flamingo**Zhang et al., "Flamingo: A Visual Language Model for Zero-Shot Learning"** uses a frozen vision encoder and an LLM equipped with gated cross-attention for cross-modality alignment. PaLM-E**Levie et al., "PaLM-E: Enhancing Vision-and-Language Understanding via Multi-Modal Alignment"** integrates extracted visual features directly through linear layers into the pre-trained PaLM**Chen et al., "PaLM: A Large-Scale Autoregressive Language Model"**. BLIP2**Li et al., "BLIP2: Bidirectional Line Proposal Network for Vision-Language Alignment"** achieves vision-language alignment by introducing a querying transformer to extract visual features from the frozen image encoder and integrating them with the frozen LLM. 
OneLLM**Korikov et al., "OneLLM: A Unified Model for Multi-Modal Learning"** stands out by aligning eight different modalities, including fMRI, with language. However, their approach employs an individual convolution network for each subject instead of a unified architecture for fMRI encoding across subjects, which restricts its applicability to new subjects in real-world scenarios. Furthermore, the approach solely relies on captions as textual annotations, which limits the model's capability for versatile fMRI decoding. 

% \paragraph{Instruction Tuning}
% Instruction tuning ____ has been widely used to enable LMs to follow specific natural language instructions. 
% A key method is Instruction Dataset Construction, where (instruction, output) pairs are generated from existing annotated datasets by applying templates to convert text-label pairs into instruction-output pairs ____. For instance, Natural Instructions **Bommasani et al., "Natural Instructions: A Benchmark for Instructional Text Understanding"** creates datasets comprising task descriptions with components like title, definition, cautions, prompts, and example pairs, while P3 ____ maps conventional NLP tasks, such as question answering or text classification, into natural language input-output pairs. In this work, we contribute to instruction tuning from a neuroscience perspective by introducing the Brain Instruction Tuning Dataset, which decodes brain activity into language representations.