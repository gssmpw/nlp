
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{natbib}  
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

\title{Probing Perceptual Constancy in Large Vision Language Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\iclrfinalcopy
\author{Haoran Sun, Suyang Yu, Yijiang Li, Qingying Gao, Haiyun Lyu, Hokin Deng, Dezhi Luo\\
 Johns Hopkins University, University of California, San Diego\\
University of North Carolina at Chapel Hill, Carnegie Mellon University\\ University of Michigan\\
\texttt{ihzedoul@umich.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Perceptual constancy is the ability to maintain stable perceptions of objects despite changes in sensory input, such as variations in distance, angle, or lighting. This ability is crucial for recognizing visual information in a dynamic world, making it essential for Vision-Language Models (VLMs). However, whether VLMs are currently and theoretically capable of mastering this ability remains underexplored. In this study, we evaluated 33 VLMs using 253 experiments across three domains: color, size, and shape constancy. The experiments included single-image and video adaptations of classic cognitive tasks, along with novel tasks in in-the-wild conditions, to evaluate the models' recognition of object properties under varying conditions. We found significant variability in VLM performance, with models performance in shape constancy clearly dissociated from that of color and size constancy.
\end{abstract}

\section{Introduction}
Perceptual constancy is the ability to perceive the properties of environmental objects as stable despite variations in external conditions. While this phenomenon exists across multiple sensory modalities, including auditory and tactile perception, it is particularly well-studied in vision. The human visual system demonstrates remarkable constancy, maintaining stable object perception despite changes in distance, viewing angle, lighting, or environmental context \citep{Epstein1977, Walsh1998}. This stability allows humans to recognize objects quickly and accurately, even in complex and dynamic scenes, ensuring seamless interaction with the surrounding world.

The study of perceptual constancy dates back to the 19th century, with early theories proposing that it emerges from the brain’s inference process—where stable perception is achieved by integrating empirical knowledge and environmental cues \citep{von1867handbuch}. Neuroscientific research has since demonstrated that visual information is processed hierarchically, from the retina to the visual cortex (e.g., V1, V4, IT cortex), to maintain perceptual stability across changing conditions \citep{dicarlo2012}. For example, neurons in the IT cortex consistently encode color regardless of illumination, suggesting that different levels of the visual system progressively compensate for environmental variations to preserve object properties \citep{tanaka1996}. This intricate mechanism enables humans to navigate dynamic environments, recognize objects across varying perspectives, and make accurate judgments in complex real-world scenarios. Similarly, instilling AI models with human-like perceptual constancy is crucial for developing robust vision-language models (VLMs) that can perform reliably under diverse and unpredictable conditions. Given the rapid progress of VLMs, perceptual constancy therefore appears to be a key benchmark for assessing their cognitive capabilities and identifying their limitations.

Perceptual constancy is essential for overcoming visual ambiguities in everyday life. Color constancy allows us to recognize objects consistently despite changes in lighting—a critical ability for autonomous systems in varying illumination conditions, such as self-driving cars interpreting traffic signals at different times of day \citep{article}. Size constancy ensures that objects appear the same size regardless of distance, facilitating spatial awareness and depth perception. This is particularly important for AI applications in robotics, where accurate size perception is necessary for grasping and object manipulation \citep{carlson2010}. Shape constancy enables stable object recognition across different viewpoints, supporting real-world tasks such as facial recognition, medical imaging, and augmented reality applications \citep{sternberg2006}. Without these constancy mechanisms, AI systems may struggle with perceptual inconsistencies, leading to unreliable in-the-wild performance that lacks adaptability and precision.

To systematically assess the perceptual constancy capabilities of VLMs, we introduced ConstancyBench, a dataset comprising 253 cognitive experiments. These experiments specifically examine the three primary dimensions of perceptual constancy: color, size, and shape. By testing VLMs on these fundamental perceptual tasks, we aim to uncover the extent to which current models exhibit perceptual stability and where they fall short. Understanding these limitations is essential for advancing AI’s ability to process visual information in a way that aligns more closely with human perception, ultimately enabling more reliable and adaptable AI systems in real-world applications.


\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{PC_cogsi_case.pdf}
\caption{\textbf{Sample Tasks from the Three Evaluation Dimensions of ConstancyBench.} Sample model performance from GPT-4o is presented, along with answer explanations.}
\label{fig:fig1}
\end{figure*}

\section{Methods}

\subsection{Cognitive Experiment}
This study evaluates the perceptual constancy capabilities of VLMs using three types of perceptual inputs: images, videos (MOV), and GIFs. Perceptual constancy refers to the human ability to perceive object properties consistently across varying environmental conditions, focusing on three key domains: color, size, and shape constancy. These three aspects were selected to capture the fundamental principles of constancy in visual perception, enabling stable object recognition despite changes in lighting, distance, and viewing angle. Below, we provided explanations for each domain.

\subsubsection{Color Constancy}
Color constancy is an important feature of the human visual system. It allows us to perceive the color of objects consistently under different light conditions. A common example is a white wall appearing in different shades under different lighting conditions. However, the human visual system still perceives it as white rather than the wall's actual color has changed. This occurs because the visual system can separate an object's true color from the influence of lighting condition, thereby maintaining stable color perception \citep{Jameson1989}. Evaluating color constancy can reveal whether VLMs can truly understand an object's intrinsic color rather than merely relying on color patterns in the training data.

\subsubsection{Size Constancy}
Size constancy refers to the perception of an object’s size as stable, even when its retinal image changes due to variations in distance \citep{Sperandio2015}. For example, a distant car appears just as large as a nearby one, despite the difference in retinal projection. This stability is crucial for spatial awareness, depth perception, and navigation.  Assessing this phenomenon in VLMs can determine whether they truly grasp the spatial properties of objects in the dynamic environment.

\subsubsection{Shape Constancy}
Shape constancy allows us to recognize objects as having the same shape, even when viewed from different angles \citep{Rock1973}. A round plate, for example, may project an elliptical image when seen obliquely, yet we still perceive it as circular. This perceptual stability relies on depth cues, prior experience, and contextual information. Shape constancy is fundamental to object recognition and spatial reasoning, allowing for accurate identification across perspectives. Research suggests humans achieve this by comparing novel views to stored shape representations \citep{tarr1995}. Evaluating shape constancy in VLMs can reveal whether they truly understand an object’s three-dimensional form or rely on fixed representations contingent to certain viewpoints.

\subsection{Model Selection and Experiment}

We evaluated three categories of VLMs. To ensure a fair comparison,  all VLMs are evaluated on their ability to reason over images and texts under a zero-shot generation task. A complete list of models is reported in the results section as shown in Figure \ref{fig:fig2}. Model size data are curated at the same time. The models are categorized as follows:

\begin{enumerate}
    \item \textbf{Open-source VLMs with Multi-Image Reasoning}:  
    Includes models with different sizes and other variants such as \texttt{CogVLM} Series\citep{hong2024cogvlm2}, \texttt{Qwen} series, such as \texttt{Qwen-VL} \citep{Qwen-VL}, \texttt{Qwen-2} \citep{Qwen2VL}, and \texttt{Blip2} \citep{li2023blip2}, LLaVA-Next \citep{liu2024llavanext} , which are capable of reasoning over interleaved multiple images and texts.
    \item \textbf{Closed-source VLMs with Multi-Image Reasoning}:  
    Includes proprietary models such as \texttt{GPT} series \citep{gpt4o} ( \texttt{GPT-4v}, \texttt{GPT-4-turbo}, \texttt{GPT-4o-mini}), \texttt{Gemini} Series \citep{gemini}, and \texttt{Claude} Series \citep{claude}. These models also support reasoning across interleaved images and texts,
    \item     \textbf{Open-source VLMs with single-Image Reasoning}:  
   Includes models designed to process a single image alongside continuous text. \texttt{InstructBlip} Series \citep{instructblip}, \texttt{LLaVA} Series \citep{liu2023improvedllava} \citep{liu2023llava} 
\end{enumerate}

In total, we processed 33 models for evaluation. Model performances in perceptual constancy tasks are presented here along with human baseline (Figure \ref{fig:fig2}). In order to analyze the reasoning abilities of VLMs, we ask the models to explain their answers after they have given the answers.

\subsection{Human Baseline} 

We recruited a total of 7 participants, all of whom were college students proficient in English. Participants were instructed to skip any question that was ambiguously phrased or too complex to answer within 90 seconds. A question was marked as failed if the participant did not provide an answer. For each question, at least 80\% of participants needed to answer correctly; otherwise, we modified the question, and new annotators completed the revised version. The human baseline result for each question was normalized based on the number of participants who provided an answer.

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{Figure_1.png}
\caption{\textbf{Model Performance on ConstancyBench As Compared to Human Performance}}
\label{fig:fig2}
\end{figure*}

\section{Results}

\subsection{General Results}

Overall, our study highlights a significant performance gap between humans and VLMs across the three evaluation dimensions. As shown in Figure \ref{fig:fig2}, humans consistently outperform VLMs on all tasks. However, a few high-performing models (e.g., GPT-4v) achieve accuracy levels relatively close to human performance. These findings suggest that while current VLMs still face substantial limitations in replicating human-like perceptual constancy, some aspects of this capability may have begun to emerge.

\subsection{Domain-wise Comparisons}

Figure \ref{fig:fig3} breaks down performance across perceptual tasks, showing stable human accuracy with minimal fluctuation, while VLM performance varies significantly. VLMs perform best on the shape constancy task and worst on color constancy. Model accuracy in the color dimension follows a bimodal distribution, clustering around 40\%-50\% and 70\%-80\%, indicating large fluctuations. In the size dimension, accuracy varies less, with most values between 50\%-80\% and fewer models achieving 90\% or higher. The shape dimension exhibits a smoother, nearly normal distribution, with most accuracies ranging from 60\%-80\%. ANOVA results confirm significant overall differences in VLM perceptual constancy across color, shape, and size (F = 10.0596, p = 0.000108).

Further examining the interdependency between evaluation dimensions, the Tukey HSD test (Figure \ref{fig:fig4}) revealed significant differences in model performance between color and shape constancy (p = 0.0001) and between shape and size constancy (p = 0.0059). In contrast, humans exhibited no significant differences across any pairwise comparisons. These results suggest that while humans likely rely on a domain-general mechanism for perceptual constancy, VLMs may process these dimensions differently, exhibiting varying levels of performance depending on the task. This disparity could indicate that current VLMs may lack an integrated, human-like perceptual framework, instead leveraging task-specific heuristics that lead to inconsistent performance across different constancy dimensions.

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{Figure_2.png}
\caption{\textbf{Overall and Dimension-Wise Accuracy: Humans vs. Models}. A. For overall accuracy across tasks, human participants outperform models significantly (p \texttt{<} 0.0001). B. Human participants consistently outperform models in each dimension (all categories p \texttt{<} 0.001).}
\label{fig:fig3}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{Figure_6_5.png}
\caption{\textbf{Pairwise Comparisons of Human and Model Performance Across Evaluation Dimensions (Tukey HSD).} A. Human performance revealed no statistically significant pairwise differences across dimensions (all p-values \texttt{>} 0.10). B. Model performance showed significant differences between shape constancy and the other two domains (both p \texttt{<} 0.006), while the pairwise difference between color and size constancy was not significant (p \texttt{=} 0.4724).}

\label{fig:fig4}
\end{figure*}

\subsection{Relationship Between Model Performance and Model Size}

A widely held belief in the machine learning community is that increasing a model’s scale—measured by the number of parameters—leads to systematic improvements in reasoning abilities \citep{sutton2019bitter, kaplan2020scaling}, a principle known as the scaling law. However, this assumption remains an empirical observation rather than a theoretically proven rule. To assess whether the scaling law applies to perceptual constancy, we examined the relationship between model performance on perceptual constancy tasks and model size (Figure \ref{fig:fig5}). Our analysis revealed significant positive correlations between model size and performance in color constancy ($y = 0.0996x + 0.4716$, p \textit{=} 0.0384, R²= 0.1604) and size constancy ($y = 0.0986x + 0.5041$, p \textit{=} 0.0013, R²= 0.3438). These results suggest that larger models tend to perform better on these two constancy tasks, potentially benefiting from improved feature extraction, higher-level reasoning, or greater exposure to diverse training data. In contrast, shape constancy exhibited no significant relationship with model size ($y = 0.0483x + 0.6582$, p \textit{=} 0.1496, R²= 0.0812). This weak correlation suggests that increasing model scale alone may not be sufficient to improve shape constancy. However, combined with the observation that virtually all models achieve above-chance performance in shape constancy— with top-performing models reaching near-human accuracy—it appears that, unlike color and size constancy, VLMs may have already surpassed a critical milestone in this dimension. This discrepancy points to potential differences in the underlying mechanisms governing different aspects of perceptual constancy in VLMs.


\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{size_acc.pdf}
\caption{\textbf{The Relationship Between Model Performance and Model Size} The four plots illustrate how model size, as measured by numbers of parameter in the models' neural networks (log scale), affects the accuracy overall as well as across the three evaluation dimensions}
\label{fig:fig5}
\end{figure*}

\section{Discussions}

Here, we introduced ConstancyBench, a comprehensive evaluation framework for assessing VLMs' performance on perceptual constancy tasks across three key dimensions: color, size, and shape constancy. Our findings indicate that while model performance still lags behind human capabilities, VLMs have already demonstrated significant reasoning abilities in shape constancy. The consistently high performance across models of different sizes, with only minor variance in relation to scaling, suggests that the mechanisms underlying shape constancy have already emerged as stable representations contingent on the model’s architecture. In contrast, model performance in color and size constancy lags further behind human performance compared to shape constancy. However, we observed a significant positive correlation between model size and accuracy in these dimensions, suggesting that while these abilities are not yet fully developed in VLMs, there is considerable potential for improvement within the current framework.

The cognitive basis of perceptual constancy in humans remains an active area of research. While it may be premature to offer a definitive mechanistic explanation, recent insights from both cognitive science and artificial intelligence research provide valuable perspectives on the dissociation between VLMs' performance across different constancy dimensions observed here. A long-standing view holds that shape constancy depends on the ability to see the same object from multiple perspectives \citep{pizlo1994theory, tarr1995}. However, recent studies have shown that VLMs exhibit significant limitations in perspective-taking over identical object arrangements \citep{gao2024vision}, with models of all sizes consistently performing below chance. The strong performance in shape constancy observed in our study, together with these findings, challenges the traditional viewpoint-based explanation of shape constancy in VLMs.

An alternative hierarchical account of shape constancy may better explain these results \citep{bradley2008constancy, buccella2021problem, buccella2022reconsidering}. According to this view, shape constancy exists at two levels: minimal shape constancy, which involves accumulating perceptual representations of an object under varying conditions, and robust shape constancy, which requires a single, unified representation that enables holistic perception of objects \citep{greenforthpluralist}. While robust shape constancy depends on model-based reasoning, which underlies perspective-taking, minimal shape constancy does not. It has been argued that standard shape constancy tasks—such as the rotating tables task (bottom left case in Figure \ref{fig:fig1})—probe only minimal rather than robust shape constancy. Thus, the high shape constancy performance observed in our study may indicate that VLMs have achieved minimal shape constancy by accumulating perceptual data across different conditions, without necessarily acquiring higher-order model-based reasoning \citep{gao2024vision}. This provides empirical support for a hierarchical structure of perceptual constancy.

Furthermore, the differences between shape constancy and the other two dimensions—color and size constancy—may stem from the greater information-gathering demands required to process object representations across varying conditions. Color constancy, for instance, is often assessed under contrastive conditions, and reasoning about purposeful manipulations of color properties may fall beyond the capabilities of smaller models. Similarly, size constancy requires an understanding of spatial relationships and object arrangements (e.g., near vs. far objects), which may give larger models an advantage. These findings suggest that scaling has allowed VLMs to develop some resilience to perceptual challenges despite still fall behind the exceptional human-level performance in these tasks \citep{tanaka1996, gegenfurtner2024color}.



\bibliographystyle{plainnat}
\bibliography{iclr2025_conference}

\newpage 
\appendix

\section{Appendix}

\subsection{Additional Examples}

\begin{figure*}[h]
\centering
\includegraphics[width=1.0\textwidth]{Case_1_Color_morning_and_evening.pdf}
\caption{\textbf{Additional Sample Tasks: Color Constancy}.}
\label{fig:fig31}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=1.0\textwidth]{Case_1_Color_rubik_cube.pdf}
\caption{\textbf{Additional Sample Tasks: Color Constancy}.}
\label{fig:fig32}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=1.0\textwidth]{Case_2_size_road.pdf}
\caption{\textbf{Additional Sample Tasks: Size Constancy}.}
\label{fig:fig33}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=1.0\textwidth]{Case_4_size_army.pdf}
\caption{\textbf{Additional Sample Tasks: Size Constancy}.}
\label{fig:fig34}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=1.0\textwidth]{Case_3_shape.pdf}
\caption{\textbf{Additional Sample Tasks: Shape Constancy}.}
\label{fig:fig35}
\end{figure*}


\end{document}
