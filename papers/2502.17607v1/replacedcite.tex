\section{Related Work}
\subsection{Dataset Distillation (DD)}
DD aims to generate a small synthetic subset of examples that can achieve a similar generalization performance to that of training on the full real dataset. DD is one of the most effective ways to preserve the privacy of training data.
%

\textbf{DD for Images.} DD is originally proposed for images. For image datasets, ____ originally proposed a meta-learning approach which synthesizes data by iteratively training a model to convergence on the synthetic examples, and optimizing the synthetic data such that the trained model generalizes well on the real training data. 
Subsequent studies tried to make this process more efficient by using kernel methods %
to approximate training the model on synthetic data in a closed form ____. 
More recent works generate synthetic data by matching the gradient ____ or wright trajectory ____ of the model trained on real data, or by matching the data distribution ____.\looseness=-1

\textbf{DD for Text. } 
There has been recent efforts in applying DD to text.
For text datasets, existing methods ____ apply the original meta-learning based method of ____, or minimize the KL-divergence between the self-attention probabilities of the model and the distilled attention labels across all layers and heads, for the first token ____. 
%
%
As generating text in the discrete space is difficult, the synthetic data is generated as continuous input word embeddings instead of discrete text.  
%
%
%
%
Such embeddings cannot be used for training other models that have different word embedding weights, and
%
%
%
are unreadable to humans, making them difficult to interpret and analyze. %
____ transformed their distilled synthetic samples to text by finding words that has the nearest neighbor embeddings. However, this results in unrelated words that are not meaningful.
%




To generate readable text, ____ 
%
first trains a proxy language model from scratch to generate synthetic training %
data for different classes. Then, 
%
it fine-tunes a generator model to generate %
synthetic data by minimizing the gradient matching loss between generated and training data.
%
%
Training the proxy model is a bottleneck in scaling the method. 
%
Besides, as the distilled synthetic data may include real samples from the original dataset, %
%
this method does not have the privacy-preserving advantage of prior DD methods.

Notably, none of the existing DD methods scale beyond BERT ____ to LLMs with billions of parameters.
In this work, we propose the first DD method that can generate privacy-preserving human-readable text, by matching gradients of LLMs with billions of parameters. 



\subsection{Synthetic Text Generation using Generative Models}

\textbf{LLMs.} A large body of recent work used LLMs to generate synthetic text data in the
zero-shot or few shot setting____. In the zero-shot setting, the LLM is directly prompted to generate text for categories of interests. In the few-shot setting, a few real-world data instances are provided as examples to guide the LLM in generating the synthetic data. 
In our work, we use the zero-shot and few-shot approaches as our baselines.
%
LLM-generated text is often very repetitive and lacks diversity ____. Besides, it does not capture the distribution of the target task and may contain incorrect or hallucinated examples  ____. 
To address these issues, recent methods rely on extensive prompt engineering to inject semantic diversity for each target category ____ and design highly complex pipelines, such as model arithmetic which composes and biases multiple LLMs ____, multi-step meticulous prompt engineering to inject domain knowledge, iterative sampling, and self-correction to rectify inaccurately labeled instances ____, and retrieval-augmented generation techniques ____. 
Such pipelines require a large number of queries to advanced LLMs such as GPT-4 ____ and Claude3-Opus ____. %
This incurs a large financial cost and makes such approaches difficult %
to apply in practice.
%
While synthetic data generated by LLMs are human-readable, LLMs may memorize and generate their training data ____. Hence, the synthetic data generated by LLMs do not preserve the privacy of their training data. Besides, it does not provide any theoretical guarantee for the performance of LLMs trained on them. \looseness=-1


\textbf{VAE and Diffusion.} A few recent studies explored the use of VAEs and diffusion for %
controllable text generation ____. Such approaches train a diffusion model from scratch and parameterize structural and semantic controls by different classifiers and update the latent variables to satisfy the controls ____, or to add noise and denoise embeddings of real data ____. This process is computationally very heavy and difficult in practice.
%
Similar to LLMs, synthetic data generated by VAEs and diffusion models do not provide guarantee for the performance of the trained model. %


%
%

%
%
%
%
%

%

%