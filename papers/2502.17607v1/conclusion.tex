\section{Conclusion}
In this work, we proposed the first theoretically-rigorous method for generating small subsets of synthetic readable text data by matching the gradient of real examples from a target task. We formulated this problem as a discretely constrained non-convex optimization in the embedding space and applied the Alternating Direction Method of Multipliers (ADMM) to iteratively optimizes the embeddings of synthetic examples to match the target gradient, and map them to a sequence of text tokens with low perplexity. We proved that the generated synthetic text can guarantee convergence of the model to a close neighborhood of the solution obtained by fine-tuning on real data. Our extensive experiments on various classification %
tasks confirmed the effectiveness of our proposed approach. 