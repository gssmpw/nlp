

\begin{table*}[!t]
\caption{Fine-tuning Phi on synthetic examples generated by \alg, vs LLM-generated zero-shot and few-shot synthetic data, vs real examples selected with herding, K-center, and Random baselines. Synthetic data generated by \alg\ outperforms the baselines by up to 10.4\% and is the only method that can preserve the privacy of the training data. \alg's synthetic data has similar log-perplexity (ppl) to that of real data, and higher ppl than LLM-generated synthetic data, confirming its more diverse nature.%
} \label{table:main}
    \centering
    \scalebox{0.9}{
    \begin{tabular}{c|c|cc|cc|cc|cc|cc|cc|c}
    \toprule
         &&\multicolumn{2}{|c|}{Privacy \checkmark} & \multicolumn{4}{|c|}{LLM generated, no privacy \text{\sffamily X}}&\multicolumn{7}{|c}{Real data, no privacy \text{\sffamily X}}\\\cmidrule{2-15}
         & &  \multicolumn{2}{|c|}{\alg} & \multicolumn{2}{|c|}{Zero-shot} &\multicolumn{2}{|c|}{Few-shot}&\multicolumn{2}{|c|}{Herding}&\multicolumn{2}{|c|}{K-center}& \multicolumn{2}{|c|}{Random}&{Rand}\\%\midrule
         Dataset&\# data& acc & ppl &acc & ppl&acc & ppl&acc & ppl&acc & ppl&acc & ppl&1K\\
         \midrule\midrule
         %
         %
         &5 & \textbf{87.0} & 5.8 & 73.2 & 2.5 & 72.4 & 3.0 & {68.0} & 6.6 & \underline{{76.6}} & 5.5 & 53.6 & 7.7 &%
         \\
         SST-2& 10 & \textbf{88.1} & 5.1 & {80.7} & 2.3 & 71.8 & 2.6 & {79.9} & 6.7 & \underline{82.2} & 5.5 & 62.7 & 7.3 \\
         (classification)& 20 & \textbf{88.3} &  5.3 & 84.1 & 2.2 & 76.8 & 2.7 & 87.3 & 6.6 & \underline{{87.9}} & 5.7 & 86.3 & 7.0 &91.2\\
         & 50 & \textbf{89.4} & 5.7 & 84.1 & 2.3 & 75.0 & 2.6 & {{87.5}} & 6.6 & \underline{88.6} & 5.5 & 87.0 & 6.8 &\\
         pretrained: 69.6& 100 & \textbf{90.0} & 5.2 & 88.0 & 2.3 & 76.4 & 2.5 & 89.0 & 6.7 & {{89.6}} & 5.8 & \underline{89.7} & 6.7\\
         %
         %
         %
         \midrule\midrule
         %
         %
         Tweet& 5 & \textbf{84.2} & 3.5 & \underline{78.1} & 2.7 & 54.6 & 3.0 & 56.5 & 5.8 & 56.5 & 5.1 & 56.5 & 5.9 &%
         \\
         emotions& 10 & \textbf{84.3} & 2.8 &\underline{78.8} & 2.3 & 47.8 & 3.2 & 58.8 & 5.6 & 70.1 & 5.6 & 62.4 & 5.8 & \\
         (classification)& 20 & \textbf{85.7} & 3.4 & \underline{82.9} & 2.2 & 70.7 & 3.3 & 70.3 & 5.8 &79.3 & 5.5 & 72.1 & 5.8 &96.3\\
         & 50 & \textbf{86.1} & 4.3 & 83.9 & 2.2 & 80.0 & 3.1 & 79.5 & 5.6& \underline{85.0} & 5.1 & 77.8 & 5.6 \\
         pretrained: 43.7& 100 & \textbf{86.4} & 3.8 & 84.8 & 2.3 & 84.5 & 3.4 & \underline{86.1} & 5.5 & {86.0}& 5.1 & 83.4 & 5.5\\ 
         %
         %
         %
         \midrule\midrule
         %
         %
         Rotten& 5 & \textbf{82.6} & 4.5 & 72.7 & 2.5 & 59.5 & 3.8 & 73.4 & 4.9 & 61.5 & 6.4 & \underline{73.7} & 5.4 &%
         \\
         tomatoes& 10 & \textbf{83.1} & 5.5 & \underline{75.9} & 2.3 & 66.7 & 3.0 & 74.3 & 5.2 & 63.8 & 5.5 & 74.9 & 5.8 \\    
         (classification)& 20 & \textbf{84.8} & 7.0 & 78.5 & 2.2 & 78.5 & 3.1 & \underline{80.6} & 5.7 & 68.4 & 5.2 & 79.0 & 5.7 &88.4\\
         & 50 & \textbf{85.2} & 4.6 & 77.7 & 2.3 & 79.9 & 2.9 & {81.6} & 5.6 & 80.0 & 5.1 & \underline{81.8} & 5.6\\
         pretrained: 65.8&100 & \textbf{85.2}%
         & 4.5 & 82.2 & 2.3 & 82.3 & 2.9 & {84.0} & 5.6 & 83.5 & 5.1 & \underline{84.3} & 5.6\\
         %
         %
         %
         %
         %
         %
         %
         %
         %
         %
         \bottomrule
    \end{tabular}
    }
    %
    %
\end{table*}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Experiments}\label{sec:exp}

\subsection{Experimental settings.}
\textbf{Datasets.} We apply our method to different text classification datasets including SST-2 movie reviews~\cite{socher-etal-2013-recursive},  Tweet emotions~\cite{mohammad2018semeval}, and Rotten tomatoes~\cite{PangLee05a}. %
%
%
%

\textbf{Models.} We use the Phi model~\cite{textbooks2} to generate synthetic data and for supervised fine-tuning. In our transfer setting, we fine-tune %
Llama3~\cite{dubey2024llama} and OPT-1.3B on the generated synthetic texts using Phi.

\textbf{Fine-tuning settings.} We fine-tune each model for 200 steps with Adam optimizer~\cite{kingma2014adam} and batch size of 16. The learning rate follows a linear scheduler with the initial learning rate selected from $\{ 7e-6, 1e-5, 1.5e-5\}$. We run an evaluation every 50 steps and report the best test classification accuracy among all the checkpoints. 

\textbf{Baselines.} We compare our method with %
LLM-generated synthetic data with zero-shot and few-shot methods~\cite{li2023synthetic}. 
We also compare to popular coreset selection methods, namely Herding \cite{welling2009herding}, K-center \cite{farahani2009facility}, and Random. %
Herding greedily selects real samples to match their mean embedding with that of the original data. K-center selects examples to minimize the largest distance between examples that are not selected and their closest selected example.
%

%

\textbf{Hyperparameters.} 
%
The number of synthetic tokens is set to the average token length of all samples. %
For the ADMM, the number of updates $T$ is set to 30 and $\rho$ is chosen from the set $\{ 0.001, 0.05, 0.01, \ldots, 10 \}$. To update $\mtx{X}$, we run 50 iterations of Adam with a constant learning rate of 0.008. For the top-$k$ projection, we use $k$ = 200. 

\subsection{Main results}

In our experiments, we consider the two scenarios discussed in Sec. \ref{sec: preliminary}. First, we apply \alg\ to generate synthetic training data based on a small number of examples from a target task. Then, we apply \alg to generate a small set of synthetic data by distilling an existing training data.


\subsubsection{Generating Larger Synthetic Fine-tuning Data in Data-scarce Regime}
First, we consider the case where data is scarce for the target task, and we wish to generate a larger synthetic training data based on a small number of examples from the target task. 
\cref{fig:match_val_data} shows the result of applying \alg\ to generate 100 synthetic examples based on only 5, 10, 20, 50 examples randomly selected from the validation data of SST-2, Tweet emotions, and Rotten tomatoes. We see that \alg\ successfully generates high-quality supervised fine-tuning data that can train Phi to a superior performance over that of training on the available validation data. Notably, \alg\ generated synthetic data based on only 5 real examples outperform the real data by 8.6\%, 28.5\%, and 32.4\% on the three datasets. This confirms the effectiveness of \alg\ in the data-scarce regime.



\subsubsection{Generating Small Synthetic Data Based on Larger Fine-tuning Data}
Next, we consider the case where a relatively large
supervised fine-tuning data is available, and we generate a smaller synthetic data to replace the real data to preserve the privacy of training examples or to improve the training efficiency. 

\textbf{\alg\ outperforms baselines and preserves privacy. }
\cref{table:main} compares the performance of fine-tuning on synthetic data generated by \alg\ %
to that of zero-shot and few-shot techniques. It also shows the performance of fine-tuning on subsets of real data selected by herding, K-center, and Random baselines. We note that among all the methods, only the synthetic data generated by \alg\ can preserve the privacy of training data. We see that \alg\ outperforms all the baselines across various datasets and data sizes, by up to 10.4\%.
Notably, the synthetic data generated by \alg\ has a similar perplexity to that of real data, while having higher perplexity than LLM-generated synthetic data with zero-shot and few-shot methods. This confirms the more diverse nature of the synthetic data generated by \alg, compared to LLM generated data.

\begin{table*}[t!]
    \caption{{Fine-tuning Llama-3.2-1B and OPT-1.3B on 20 synthetic samples generated by matching the gradient of a pretrained Phi.}}
    \label{tab:transfer}
    \centering
    \begin{small}
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
        \toprule
         Model & Dataset &Pretrained&\alg &  Zero-shot & Few-shot & Herding & K-centers & Random real\\
         \midrule
         \multirow{3}{*}{Llama-3.2-1B} & SST-2&68.6& \textbf{89.4}	& 82.4	& 79.7	& 85.4	& 64.6 &	\underline{{88.4}}  \\
         &Tweet emotions&43.7& \textbf{85.8}	&83.4&	74.4&	76.1&	88.5&	\underline{{83.9}}\\
         &Rotten tomatoes&67.5&\textbf{87.8}&	80.5&	78.5&	73.6&	87.8&	\underline{84.3}\\
         \midrule
         \multirow{3}{*}{OPT-1.3B} & SST-2&62.3&  \underline{87.0}&	83.9&	85.6&	85.8&	73.8& \textbf{88.7}\\
         & Tweet emotions &43.7& \textbf{78.5}	&77.8 & 76.9 & 75.3 & 74.7 & 77.7\\
         & Rotten tomatoes &63.1& \textbf{87.9}&	74.9&	80.6&	80.8&	84.8& \underline{85.5}\\
         \bottomrule
    \end{tabular}
    \end{small}
    %
\end{table*}
\textbf{\alg's synthetic data transfer to other LLMs.} 
\cref{tab:transfer} shows the performance of fine-tuning Llama-3.2-1B and OPT-1.3B on 20 synthetic examples generated with \alg\ by matching gradient of a pretrained Phi model. We see that the data generated by \alg\ outperforms %
zero-shot and few-shot methods and the real data selected by herding and K-center baselines. This confirms the transferability of the synthetic data generated by \alg.



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%



\subsection{Ablation study}

%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{table}[!t]
    \caption{SST-2. Matching the gradient of last-layer yields a higher performance with smaller number of synthetic data. Mapping the optimized embeddings to text via top-$k$ projection (readable text) yields 9.2\% higher accuracy than $L_2$ projection (unrelated words).} 
    %
    \label{table:grad_topk_ablation}
    \centering
    \scalebox{0.9}{
        \begin{tabular}{lccc}
        \toprule
        Method & Acc & \#data & ppl
        \\
        %
        %
        %
        %
        %
        %
        \midrule
        \alg\ & 90.0 & 68 & 5.2\\
        {\alg\ with full grad} & 89.6 & 89 & 5.5\\
        \alg\ w/o top-$k$ projection & \underline{80.8} & 57 & \underline{13.3}\\
        \bottomrule
        \end{tabular}
    }
    \vskip -0.1in
\end{table}


\begin{table}[!t]
    \caption{SST-2. Our filtering strategies effectively reduce the size of synthetic data from 200 to 68 and yield 1.9\% higher accuracy.\looseness=-1} 
    %
    \label{table:filtering_ablation}
    \centering
    \scalebox{0.9}{
        \begin{tabular}{lccc}
        \toprule
        Method & Acc & \#data & ppl
        \\
        \midrule
        ADMM & 88.1 & 200 & 4.6\\
        %
        + Removing wrong labels & 89.4 & 169 & 4.6\\
        + Selecting data with lowest loss & 89.4 & 100 & 5.4\\
        + Balancing avg loss of categories & 90.0 & 68 & 5.2\\
        %
        %
        %
        \bottomrule
        \end{tabular}
    }
    %
\end{table}




\textbf{Generating readable text improves the performance.}
\cref{table:grad_topk_ablation} shows that mapping the optimized embeddings to text via top-$k$ projection yields readable synthetic text with low log-perplexity (ppl). In contrast, synthetic examples generated via $L_2$ projection have a considerably higher ppl, as they contain a set of unrelated words. Notably, the top-$k$ projection yields 9.2\% better performance than $L_2$ projection. This confirms that readability of the generated text is not only important for interpretation and transferability of the results, but it is crucial for obtaining satisfactory performance.\looseness=-1

\textbf{Matching last-layer gradient boosts the performance, memory and speed of generation.} 
\cref{table:grad_topk_ablation} shows that matching the gradient of last-layer yields a higher performance with smaller number of synthetic data. At the same time, it reduces the generation memory by 2.6x (from 44.6G to 17.3G) and reduces the generation time by 2.3x (from 4.6 hours to 2 hours on one H100 GPU). 
%
%

\textbf{Filtering improves the performance of synthetic data.} \cref{table:filtering_ablation} shows the effect of the three filtering strategies discussed in Sec. \ref{sec:filtering} to obtain a subset of at most $r=100$ synthetic examples from the 200 synthetic data generated by ADMM. We observe that (i) removing examples that belong to the wrong category effectively improves the performance; (ii) selecting the top $r=$100 examples with the lowest loss in every category effectively reduces the size of the synthetic data without harming its performance; (iii) dropping examples with highest loss in categories that have a larger average loss further reduces the size of the synthetic data while improving its performance. The filtering strategies reduce the size of the synthetic data from 200 to 68, while yielding 1.9\% improvement in the fine-tuning performance.


\textbf{Qualitative results.} Fig. \ref{fig:actual} shows examples of generated synthetic text by \alg\ from positive and negative classes of the SST-2. We see that the synthetic data is meaningful and semantically aligns with the target categories.


%

\begin{figure}[!t]
\begin{tcolorbox}
\textbf{Positive:} Great movie review is a must see experience that will leave you in a state of all time high with the brilliant acting and the stunning production.\\
\textbf{Positive:} The movie truly left me completely moved and in a better place than when I started it because of its well thought out and impactful way.\\
\textbf{Negative:} The overall quality of action in this movie was not impressive enough to keep me away from the action center.\\
\textbf{Negative:} Terribly bad and boring to me as a person who values quality content and a good storyline over mind.
\end{tcolorbox}
\vspace{-2mm}
\caption{Synthetic examples generated by \alg\ from SST-2.}\label{fig:actual}
\end{figure}
