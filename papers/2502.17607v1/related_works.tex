\section{Related Work}

\subsection{Dataset Distillation (DD)}
DD aims to generate a small synthetic subset of examples that can achieve a similar generalization performance to that of training on the full real dataset. DD is one of the most effective ways to preserve the privacy of training data.
%

\textbf{DD for Images.} DD is originally proposed for images. For image datasets, \citet{wang2018dataset} originally proposed a meta-learning approach which synthesizes data by iteratively training a model to convergence on the synthetic examples, and optimizing the synthetic data such that the trained model generalizes well on the real training data. 
Subsequent studies tried to make this process more efficient by using kernel methods %
to approximate training the model on synthetic data in a closed form \cite{loo2022efficient,nguyen2020dataset}. 
More recent works generate synthetic data by matching the gradient \cite{zhao2020dataset,zhao2021dataset,kim2022dataset} or wright trajectory \cite{cazenavette2022dataset,wang2022cafe} of the model trained on real data, or by matching the data distribution \cite{zhao2023dataset}.\looseness=-1

\textbf{DD for Text. } 
There has been recent efforts in applying DD to text.
For text datasets, existing methods \cite{sucholutsky2021soft, li2021data,sahni2023exploring} apply the original meta-learning based method of \cite{wang2018dataset}, or minimize the KL-divergence between the self-attention probabilities of the model and the distilled attention labels across all layers and heads, for the first token \citet{maekawa2023dataset}. 
%
%
As generating text in the discrete space is difficult, the synthetic data is generated as continuous input word embeddings instead of discrete text.  
%
%
%
%
Such embeddings cannot be used for training other models that have different word embedding weights, and
%
%
%
are unreadable to humans, making them difficult to interpret and analyze. %
\citet{sucholutsky2021soft,sahni2023exploring} transformed their distilled synthetic samples to text by finding words that has the nearest neighbor embeddings. However, this results in unrelated words that are not meaningful.
%




To generate readable text, \citet{maekawa2024dilm} 
%
first trains a proxy language model from scratch to generate synthetic training %
data for different classes. Then, 
%
it fine-tunes a generator model to generate %
synthetic data by minimizing the gradient matching loss between generated and training data.
%
%
Training the proxy model is a bottleneck in scaling the method. 
%
Besides, as the distilled synthetic data may include real samples from the original dataset, %
%
this method does not have the privacy-preserving advantage of prior DD methods.

Notably, none of the existing DD methods scale beyond BERT \cite{devlin2018bert} to LLMs with billions of parameters.
In this work, we propose the first DD method that can generate privacy-preserving human-readable text, by matching gradients of LLMs with billions of parameters. 



\subsection{Synthetic Text Generation using Generative Models}

\textbf{LLMs.} A large body of recent work used LLMs to generate synthetic text data in the
zero-shot or few shot setting~\cite{meng2022generating,li2023synthetic}. In the zero-shot setting, the LLM is directly prompted to generate text for categories of interests. In the few-shot setting, a few real-world data instances are provided as examples to guide the LLM in generating the synthetic data. 
In our work, we use the zero-shot and few-shot approaches as our baselines.
%
LLM-generated text is often very repetitive and lacks diversity \cite{holtzman2019curious,keskar2019ctrl}. Besides, it does not capture the distribution of the target task and may contain incorrect or hallucinated examples  \cite{ye2022zerogen,meng2022generating,gupta2023targen,li2023synthetic,wu2023bloomberggpt}. 
To address these issues, recent methods rely on extensive prompt engineering to inject semantic diversity for each target category \cite{gupta2023targen} and design highly complex pipelines, such as model arithmetic which composes and biases multiple LLMs \cite{dekoninckcontrolled}, multi-step meticulous prompt engineering to inject domain knowledge, iterative sampling, and self-correction to rectify inaccurately labeled instances \cite{gupta2023targen}, and retrieval-augmented generation techniques \cite{wu2023bloomberggpt}. 
Such pipelines require a large number of queries to advanced LLMs such as GPT-4 \cite{gpt4} and Claude3-Opus \cite{claude3}. %
This incurs a large financial cost and makes such approaches difficult %
to apply in practice.
%
While synthetic data generated by LLMs are human-readable, LLMs may memorize and generate their training data \cite{hartmann2023sok}. Hence, the synthetic data generated by LLMs do not preserve the privacy of their training data. Besides, it does not provide any theoretical guarantee for the performance of LLMs trained on them. \looseness=-1


\textbf{VAE and Diffusion.} A few recent studies explored the use of VAEs and diffusion for %
controllable text generation \cite{li2022diffusion,gong2022diffuseq,zhou2024difflm}. Such approaches train a diffusion model from scratch and parameterize structural and semantic controls by different classifiers and update the latent variables to satisfy the controls \cite{li2021data}, or to add noise and denoise embeddings of real data \cite{zhao2020dataset}. This process is computationally very heavy and difficult in practice.
%
Similar to LLMs, synthetic data generated by VAEs and diffusion models do not provide guarantee for the performance of the trained model. %


%
%

%
%
%
%
%

%

%