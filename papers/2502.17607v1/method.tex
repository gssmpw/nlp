\section{Method}\label{sec:method}
Next, we discuss our proposed method for generating readable synthetic text for fine-tuning LLMs on a target task. 

\subsection{Text Generation via Gradient Matching} 
An effective way to solve Eq~\ref{eq:problem_with_constraint} is via gradient matching. Specifically, we generate a synthetic data $\gD_\text{syn}$ that has a similar gradient to that of the real dataset: %

\begin{equation}\label{eq:grad_match}
    %
    \argmin_{\substack{\gD_\text{syn}, |\gD_\text{syn}|\leq r, \\ s \in \Gamma, \text{ppl}(s) \leq \epsilon \\~\forall s\in \gD_\text{syn}}}
    D(\nabla_{\thet} \ell(\gD_\text{syn},\thet),\nabla_{\thet}\ell(\gD_\text{real},\thet)).
\end{equation}

where $D(\cdot, \cdot)$ is a distance between two gradients. Following \cite{deng2021tag, geiping2020inverting}, we use $1-cos(.,.)$ as our distance metric, where $cos$ is the cosine similarity. If such a synthetic data can be generated, training on it with gradient methods directly minimizes the loss on real data. %

Fine-tuning is often short and changes the model to a smaller extent than pre-training. Fine-tuning for longer results in forgetting the pretrained information and harms the performance \cite{gekhman2024does}. Since fine-tuning loss is often smooth and has a bounded curvature, we solve the above problem by generating a synthetic data that matches the gradient of real data at the pretrained parameters. We prove that training on such a subset converges to a close neighborhood of the solution found by training on real data.%

\textbf{Challenges of Readable Text Generation.}
Solving Problem \ref{eq:grad_match} is very challenging, as the set of feasible solutions is sparse, the space is discrete, and LLMs are non-linear and high-dimensional.
%
Specifically, the constraint set is formed by the Cartesian product of many discrete sets, each restricting a word to belong to the vocabulary. Among sequences that satisfy this condition, only those that are readable|measured by a low perplexity value|are valid. 
Thus, solving Problem \ref{eq:grad_match} is NP-hard as it requires going through all the possible sequences of words in the vocabulary and finding readable sequences that best match the real data gradient. The number of such sequences is exponential in the size of the vocabulary. This makes it computationally infeasible to find the optimum solution.
%
Finally, calculating the similarities in the %
gradient space of LLMs with billions of parameters is computationally very expensive. %



%
\subsection{Alternating Between Text and Embedding Spaces}
%
%
%
%

To solve the above discretely constrained non-convex optimization problem, we first transfer it to the continuous embedding space, where one can optimize the embeddings of synthetic data to match the target gradient, under the constraint that the optimized embeddings belong to the set of all token (words, subwords, or characters) embeddings in the vocabulary. If such embeddings can be found, they can be directly mapped to a sequence of words in the vocabulary.
%

Formally, let $\vct{x} \in \sR^{n \times d}$ be the embedding matrix of a synthetic sample $s$ with $n$ tokens, where row ${x}_{j}\in \R^d$ is the $j^{th}$ token embedding. 
%
By stacking the embedding matrices of all synthetic samples in $\gD_\text{syn}$, we obtain an embedding tensor $\mtx{X} \in \sR^{|\gD_\text{syn}| \times n \times d}$.
With an abuse of notation, we denote $\ell(\vct{x},\thet) = \ell(s,\thet)$ and $\ell(\mtx{X}, \thet) = \frac{1}{|\gD_\text{syn}|} \sum_{i=1}^{|\gD_\text{syn}|} \ell(\vct{x}^i,\thet)$. 
%
%
We rewrite Problem~\ref{eq:grad_match} as: 
\begin{align}\label{eq:embedding_match}
    \argmin_{\substack{\gD_\text{syn}, |\gD_\text{syn}|\leq r, \\ {x}_{j} \in \gE, \text{ppl}(\vct{x}) \leq \epsilon \\~\forall \vct{x}\in \gD_\text{syn}}} \!\!\!\!\!\! f(\mtx{X})~~ \text{s.t.} ~~ f(\mtx{X}) \!=\! D(\nabla_{\thet} \ell(\mtx{X},\thet),\!\nabla_{\thet}\ell(\gD_\text{real},\thet)),
\end{align}
where $\gE = \{ e_1, e_2, \ldots, e_{|V|} \}$ denote the vocabulary embedding, i.e. the set of all token embeddings in the vocabulary $V$ of model $\thet$ where $e_i \!\in\! \sR^d$ and $d$ is the embedding dimension. \looseness=-1

 
To solve the above constrained optimization problem
%
%
we apply
%
the Alternating Direction Method of Multipliers (ADMM) \cite{glowinski1975approximation,gabay1976dual}. %
%
%
%
%
By forming the augmented Lagrangian function, ADMM
decomposes the original problem into subproblems that can be solved separately and iteratively.
%
%
%

While ADMM was originally introduced for convex optimization under linear constraints, 
%
%
%
more recently it has been successfully applied to solving %
mixed integer non-linear programs
%
\cite{leng2018extremely,lin2019toward}, with convergence guarantees \cite{huang2021alternating}.

%
\textbf{\!Constrained Gradient Matching in the Embedding Space.\!} 
%
%
%
%
%
To apply ADMM to our discretely constrained non-convex problem \ref{eq:embedding_match}, we convert it to a non-convex optimization with convex linear constraints.
To do so, we introduce an auxiliary variable $\mtx{Z}$ and rewrite our objective
with an extra equality constraint so that the embeddings are constrained to be from the vocabulary, but not subject to that restriction:
\begin{equation}\label{eq:admm_objective}
    {\min}_{\mtx{X}} f(\mtx{X}) + \gI_\gE(\mtx{Z}), \quad s.t. \quad \mtx{X} = \mtx{Z}.
\end{equation}
The indicator function $\gI_\gE(\mtx{Z})$ is defined as $\gI_\gE(\mtx{Z}) = 0$ if %
${z}_{j}\in \gE~\forall j$ (i.e., if the embedding of each synthetic example can be mapped to a sequence of words in the vocabulary), and $\gI_\gE(\mtx{Z}) = +\infty$ otherwise.
%
%
%

%
%
The augmented Lagrange of Eq. \ref{eq:admm_objective} for parameter $\rho > 0$, can be formulated as: \looseness=-1
%
\begin{align}\label{eq:lagrange_function}
    \gL_\text{aug}(\mtx{X}, \mtx{Z}, \vct{\Lambda}) = f(\mtx{X}) +\gI_\gE(\mtx{Z}) &+ \langle \vct{\Lambda}, \mtx{X} - \mtx{Z} \rangle \nonumber \\
    &~~~+ \frac{\rho}{2} \| \mtx{X} - \mtx{Z} \|^2,
\end{align}
where $\vct{\Lambda} \in \sR^{|\gD_\text{syn}| \times n \times d}$ denotes the Lagrangian multipliers.
%
With simple algebraic manipulations, Eq.\ref{eq:lagrange_function} can be written as:
\begin{align}\label{eq:lagrange_alter}
    \gL_\text{aug}(\mtx{X}, \mtx{Z}, \vct{\Lambda}) = f(\mtx{X}) +\gI_\gE(\mtx{Z}) &+ 
    \frac{\rho}{2} \| \mtx{X} - \mtx{Z} - \rho^{-1}\vct{\Lambda} \|^2. 
    %
\end{align}
%
%
ADMM solves the above problem %
by minimizing primal variables $\mtx{X},\mtx{Z}$ and maximizing dual variable $\vct{\Lambda}$ at each iteration $t$, using the following update rules:
\begin{align}
    &\text{Primal update:}\!\!&\mtx{X}^{t+1} &\!= \argmin_{\mtx{X}} \gL_\text{aug}(\mtx{X}^{}, \mtx{Z}^{t}, \vct{\Lambda}^t) \label{eq:update_x}, \\
    & &\mtx{Z}^{t+1} &\!= \argmin_{\mtx{Z}} \gL_\text{aug}(\mtx{X}^{t+1}, \mtx{Z}^{}, \vct{\Lambda}^t) \label{eq:update_z}, \\
    &\text{Dual update:} &\vct{\Lambda}^{t+1} &= \vct{\Lambda}^t + \rho (\mtx{X}^{t+1} - \mtx{Z}^{t+1}), \label{eq:update_lambda}
\end{align}
which are respectively the proximal step, projection step, and dual update.
The proximal step optimizes the embeddings to match the target gradient, and the projection step maps the embeddings to words in the vocabulary.
%
%
Eq. \ref{eq:update_x} requires solving an unconstrained optimization problem. When $\rho$ is large, the function is strongly convex in $\vct{X}$. 
In practice, stochastic gradient descent algorithms such as Adam \cite{kingma2014adam} can obtain an approximate solution, which is sufficient for the convergence of ADMM \cite{huang2021alternating}.
Next, we discuss the projection step.


\textbf{Projecting the Embeddings into the Vocabulary Space.} 
The projection step in Eq. \ref{eq:update_z} can be written as \cite{huang2021alternating}:
\begin{align}
    \mtx{Z}^{t+1} &\!= \argmin_{\mtx{Z}} \gL_\text{aug}(\mtx{Z}^{t+1}, \mtx{Z}^{}, \vct{\Lambda}^t)\\
    & = \argmin_{\mtx{Z}} \gI_\gE(\mtx{Z}) + \| \mtx{Z} - \mtx{X}^t - \rho^{-1}\vct{\Lambda}^t\|^2\\
    & = \gP_\gE(\mtx{X}^t + \rho^{-1} \vct{\Lambda}^t).
\end{align}
%
%
%
%
%
For the vocabulary embeddings $\gE$, the projection $\gP_{\gE}({x}_i)$ of an embedding vector ${x}_i \in \sR^d$ %
into the vocabulary space is the embedding vector ${z}_i \coloneqq \argmin_{e \in \gE} \|{x}_i - e\|^2$ corresponding to the token in the vocabulary that is closest to ${x}_i$ in Euclidean space. 
%
%
In practice, ${z}_i$ can be found by looping over the vocabulary and finding the closest token. This operation can be vectorized efficiently.
%
For an embedding matrix $\vct{x} \in \sR^{n \times d}$ consisting of $n$ embedding vectors, we project each embedding vector ${x}_i$ independently to get the matrix embedding $\gP_\gE(\vct{x}) \!=\! \begin{bmatrix}
\gP_\gE(x_1) \!&\! \gP_\gE(x_2) \!&\! \cdots \!&\! \gP_\gE(x_n)
\end{bmatrix}^\top$. Similarly, for the embedding tensor $\mtx{X}$, the projection operation can be vectorized efficiently to find $\mtx{Z}$. \looseness=-1

%

\textbf{Ensuring Readability of the Projected Text.}
Projecting embeddings to tokens in vocabulary independently does not yield meaningful text. To address this, we leverage the idea of top-$k$ decoding to enforce the readability of generations \cite{fan2018hierarchical}. 
%
Consider an embedding matrix $\vct{x} \in \sR^{n \times d}$ consisting of $n$ embedding vectors.
For every embedding $x_i$, we find the top $k$ most probable tokens from the vocabulary %
%
condition on the previously projected tokens.
Formally, we find the top $k$ tokens that minimize $\sum_{e\in \gE} P(x|x_{i=1:i-1})$, and denote them by $\gE_{\text{top-k}}$. Then, we project $x_i$ into the space of the top-$k$ selected tokens by solving $z_i := \gP_{\gE_{\text{top-k}}}(x_i)= \argmin_{e \in \gE_{\text{top-k}}} \|x_i - e\|^2$. 
%
\begin{algorithm}[!t]
    \caption{GRADient matching w. ADMM (\alg)}
    \label{alg:admm-q}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} Constant $\rho > 0$, 
        %
        %
        %
        ADMM steps $T$, proj param $k$
        %
        \STATE \textbf{Step 1: Initialization}
        %
            %
            %
        %
        \STATE Random sample $\mtx{X} \in \Gamma$
        \STATE Initialize $\mtx{X}^0 = \argmin_{\mtx{X}} f(\mtx{X})$
        \STATE Initialize $\mtx{Z}^0 = \mtx{X}^0$
         and $\vct{\Lambda}^0 \in \sR^{|\gD_\text{syn}| \times n \times d}$.
        \STATE \textbf{Step 2: ADMM}
        \FOR{$t = 0, 1, \ldots, T-1$}
        \STATE Update $\mtx{X}$: %
        $\mtx{X}^{t+1} = \argmin_{\mtx{X}} \gL(\mtx{X}, \mtx{Z}^{t}, \vct{\Lambda}^t)$ 
        \STATE Update $\mtx{Z}$: $\mtx{Z}^{t+1} = \gP_{\gE_{\text{top-k}}}(\mtx{X}^{t+1} + \rho^{-1} \vct{\Lambda}^t)$ 
        %
        %
        \STATE Update $\vct{\Lambda}$: $\vct{\Lambda}^{t+1} = \vct{\Lambda}^{t} + \rho (\mtx{X}^{t+1} - \mtx{Z}^{t+1})$ 
        \ENDFOR
        \STATE $\gS = \gP_{\gE_{\text{top-k}}}(\mtx{X}^T)$
        \STATE \textbf{Step 3: Filtering}
        %
        \STATE Drop samples in $\gS$ that do not belong to their category 
        \STATE Select $r\!$ samples in $\!\gS$ %
        with lowest gradient matching loss\looseness=-1
        %
        \STATE Drop examples with highest loss from categories that have a higher average gradient matching loss
        %
        \STATE {\bfseries Output:} Remaining synthetic texts in $\gS$. %
    \end{algorithmic}
\end{algorithm}
%
%



%

\subsection{Dealing with High-dimension Gradients} 
Calculating similarities in the very high-dimensional gradient space of LLMs with billions of parameters is computationally very expensive. Besides, such gradients contain many small and noisy dimensions which makes calculating gradient similarities inaccurate.
%
An effective way to tackle this issue is to leverage lower-dimensional gradient estimates \cite{mirzasoleiman2020coresets}. 
Various weight initialization \cite{glorot2010understanding} and
activation normalization methods \cite{ioffe2015batch} uniformize the activations across samples. Thus, the variation of the gradient norm is mostly captured by the gradient of the loss with respect to the model's last layer \cite{katharopoulos2018not}.

Based on the observation, we generate synthetic data by only matching the last-layer gradient of the model. 
%
Let $\thet_L$ denote the last layer of model $\thet$ with $L$ layers, the last-layer gradient distance between synthetic and real data in Eq~\ref{eq:embedding_match} is:
\begin{align}\label{eq:cosine_distance}
    \argmin_{\substack{\gD_\text{syn}, |\gD_\text{syn}|\leq r, \\ x_j \in \gE, \text{ppl}(\vct{x}) \leq \epsilon \\~\forall \vct{x}\in \gD_\text{syn}}} 
    D(\nabla_{\thet} \ell(\mtx{X},\thet_L), \nabla_{\thet}\ell(\gD_\text{real},\thet_L)) %
    %
\end{align}
Matching the last layer gradient is much cheaper than the full gradient and allows generating synthetic data with superior performance, as we will confirm in our experiments.

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        %
        \includegraphics[width=\columnwidth]{figures/sst2_varying_budget.pdf}
        \caption{SST-2}
        \label{fig:sst2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        %
        \includegraphics[width=\columnwidth]{figures/twitteremotion_varying_budget.pdf}
        \caption{Tweet emotions}
        \label{fig:twitteremotion}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        %
        \includegraphics[width=\columnwidth]{figures/rotten_tomatoes_varying_budget.pdf}
        \caption{Rotten tomatoes}
        \label{fig:rotten_tomatoes}
    \end{subfigure}
    \hfill
    \vspace{-2mm}
    \caption{{Data-scarce regime.} Generating 100 synthetic samples with \alg, based on 5, 10, 20, 50 examples from a target task. Synthetic data generated based on only 5 real examples outperforms the real data by 18.6\%, 28.5\%, and 32.4\% on the three datasets.}
    \label{fig:match_val_data}
    %
\end{figure*} 
\subsection{Filtering the Generated Examples}\label{sec:filtering}
While the top-$k$ projection enables generating human-readable text, it can negatively affect the performance due to the following reasons: ({i}) It may change the category of the synthetic example by including words that are most relevant to other categories. ({ii}) It may significantly increases the gradient matching loss of some synthetic examples. ({iii}) It may result in a much higher gradient matching loss for some categories compared to the rest. 
%
To address the above issues, we filter the low-quality synthetic examples as follows.
First, we drop examples that do not belong to the correct category by running a simple few-shot evaluation \cite{li2023synthetic}, as detailed in Appendix \ref{app:prompt}. 
%
%
Next, for every category we select $r$ synthetic examples with the lowest gradient matching loss. 
%
Finally, we ensure similar gradient matching loss for all categories by dropping examples with highest loss in categories with a higher average loss compared to the rest. 
%
%
The above steps significantly boost the performance of the synthetic data, as we will confirm in our experiments.

Pseudocode of our method,\alg, is illustrated in Alg\! \ref{alg:admm-q}.\looseness=-1

%

\subsection{Convergence Analysis}
Next, we theoretically analyze the convergence of fine-tuning on the synthetic examples generated by \alg.
%
As discussed in Sec. \ref{sec: preliminary}, fine-tuning is short and changes the model to a small extent compared to pretraining. Effectively, the fine-tuning loss is relatively flat and can be modeled by a $\beta$-smooth (i.e., with a bounded Hessian $\vct{H}$) and $\mu$-PL$^*$ \big(i.e., $\|\nabla \gL(\thet)\|^2 \geq 2\mu\gL(\thet)$\big) function. 
%

For a synthetic subset generated by \alg~via matching the gradient of real data at the pretrained model parameters, the following lemma bounds the error between the gradient of synthetic and real data during the fine-tuning. 
%
%
%
%
%
%
%
%
%
%
%

\begin{lemma}\label{lem:grad}
    Assume that the fine-tuning losses of the real $\gL$ and synthetic data $\gL^s$ are $\beta$-smooth. The synthetic data generated by \alg\ that captures the gradient of real data by an error of $\|\nabla \gL(\thet)-\nabla \gL^s(\thet)\|\leq\epsilon$ at the pretrained parameters $\thet$, has a bounded gradient error at any point $t$ during fine-tuning:
    %
    %
    %
    %
    %
    \begin{equation}
        \|\nabla \gL({\thet_t}) - \nabla \gL^s({\thet_t})\| \leq 2\beta \delta + \epsilon, 
    \end{equation}
    where $\delta\geq\|\thet - {\thet_t}\|$ upper-bounds the norm of change to the parameteres during fine-tuning.
\end{lemma}
Next, we analyze the convergence of fine-tuning with gradient descent on the synthetic subset generated by \alg.
\begin{theorem}\label{thm:convergence}
    For a $\mu$-PL$^*$ loss function $\gL$, under the assumptions of Lemma \ref{lem:grad}, 
    gradient descent on the synthetic data converges with the same rate as that of real data. Moreover, at every step $t$, the difference between the fine-tuning loss on synthetic and real data is upper bounded by: 
    \begin{equation}
        |\gL(\thet_t)-\gL^s(\thet_{t})|\leq {\xi(2\nabla - \xi)}/{2\mu}.\label{eq:convergence}
    \end{equation}
    %
    %
    %
    %
    %
    where $\xi=2\beta \delta + \epsilon$ %
    and $\nabla$ is an upper bound on the gradient norm during fine-tuning.
\end{theorem}
%
The next corollary shows that fine-tuning on real data and synthetic data found by \alg\ yields similar models. \looseness=-1
\begin{corollary}\label{col:params}
    Consider a strongly convex loss (i.e., $\|\vct{H}\|\!\geq\!\alpha>0$) with unique minimizer $\thet_*$ %
    and let $\gL(\thet_*)\!=\!0$. Then fine-tuning with any optimizer on real and synthetic data generated by \alg\ yield similar models: %
    %
    \begin{equation}
        \|\thet_*-\thet_*^s\|\leq \sqrt{  \xi(2 \nabla - \xi)/\alpha \mu }.
    \end{equation}
\end{corollary}
Thus, the fine-tuned models have a similar performance. 
