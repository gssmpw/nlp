\documentclass[preprint, authoryear]{elsarticle}
\usepackage{bbm}
\usepackage[table]{xcolor}
\usepackage{graphicx}
\usepackage{soul, color}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{url}
\usepackage{colortbl}
\usepackage{lipsum}

\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\ybf}{\mathbf{y}}
\newcommand{\fbf}{\mathbf{f}}
\newcommand{\softplus}{\mathrm{softplus}}
\newcommand{\negBinp}{\text{p}}
\newcommand{\iid}{\overset{\mathrm{i.i.d.}}{\sim}}
\newcommand{\greytabline}{\rowcolor{backcolour}\cellcolor{white}}
\newcommand{\jmax}{j_{\mathrm{max}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\ADIDA}{ADIDA$_{\mathrm{C}}$}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codeblue}{rgb}{0.,0.5,0.99}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{backcolour2}{rgb}{0.96,0.96,0.96}

\newcommand{\Ste}[1]{\textcolor{codegreen}{S: #1}}
\newcommand{\Gio}[1]{\textcolor{codepurple}{G: #1}}
\newcommand{\Dar}[1]{\textcolor{codeblue}{D: #1}}

\begin{document}

\begin{frontmatter}
\title{Intermittent time series forecasting with Gaussian Processes and Tweedie likelihood}

\author[1]{Stefano Damato\corref{cor1}}
\ead{stefano.damato@supsi.ch}

\author[1]{Dario Azzimonti}
\ead{dario.azzimonti@supsi.it}

\author[1]{Giorgio Corani}
\ead{giorgio.corani@supsi.ch}

\cortext[cor1]{Corresponding author}

\affiliation[1]{organization={SUPSI, Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA)},
city={Lugano},
country={Switzerland}}


\begin{abstract}
We introduce the use of Gaussian Processes (GPs) for the probabilistic forecasting of intermittent time series. 
The model is trained in a Bayesian framework that accounts for the uncertainty about the latent function and marginalizes it out when  making predictions.  
We couple the latent GP variable with two types of forecast distributions: the negative binomial (NegBinGP) and the Tweedie distribution (TweedieGP). While the negative binomial has already been used in forecasting intermittent time series, this is the first time in which a fully parameterized Tweedie density is used for intermittent time series. We properly evaluate the Tweedie density, which is both zero-inflated and heavy tailed, avoiding  simplifying assumptions made in existing models. 
We test our models on thousands of intermittent count time series. Results show that our models provide consistently better probabilistic forecasts than the competitors. 
In particular, TweedieGP obtains the best estimates of the highest quantiles, thus showing that it is more flexible than NegBinGP.
\end{abstract}

\begin{keyword}
Intermittent time series \sep
Gaussian Processes \sep
Tweedie distribution \sep
Probabilistic forecasting
\end{keyword}
\end{frontmatter}

\newpage

\section{Introduction}
Intermittent time series irregularly switch between zero and non-zero values. 
They characterize a large percentage of the inventory items and thus they represent an important element in
the planning process \citep{Johnston_Boylan_Shale_2003}.
Well-known forecasting methods for intermittent demand \citep{Croston_1972, Syntetos_Boylan_2005,Nikolopoulos_Syntetos_Boylan_Petropoulos_Assimakopoulos_2011}  only provide point forecasts. 
However, planning the inventory levels
requires probability distributions \citep{boylan2021intermittent,Kolassa_2016}
from which to extract the relevant quantiles.

Probabilistic models for intermittent time series typically forecast the distribution of the \textit{demand} (the positive values of the time series) and the probability of \textit{occurrence}, i.e., the binary variable indicating whether there will be demand. Notable models include
\citet[pp.281-283]{Hyndman_2008}, \citet{snyder2012forecasting}, \citet{sbrana2023modelling} and \citet{Svetunkov_Boylan_2023}. All such models include one or more latent variables,
related to the expected value of the demand or to the probability of occurrence.

The latent variables determine one or more parameters of the forecast distribution $p(y_{T+1} \mid y_{1:T})$,  where $y_{1:T}$ are the observations available up to time $T$ and $y_{T+1}$ is the predicted value at time $T+1$.
See Sec.~\ref{sec:SoAintermittent} for more details.
The above models consider a point estimate of the latent variables, without modeling their uncertainty.  Yet, 
more accurate predictions can be obtained by accounting also for the  uncertainty on the parameters of the forecast distribution
\citep{prak2019general}.

Our first contribution is to model the  latent variable with a Gaussian Process (GP), a Bayesian non-parametric model
which automatically quantifies the uncertainty of the latent variable
and propagates it to the forecast distribution. 
GPs have   
 already been applied for forecasting  smooth time series \citep{Roberts_Osborne_Ebden_Reece_Gibson_Aigrain_2013, Corani_Benavoli_Zaffalon_2021}; we apply them for the first time to intermittent time series.

Another key aspect of modeling intermittent time series is that the forecast distribution needs a mass in zero. This can be achieved with a  discrete distribution,
such as the negative binomial  \citep{Harvey_Fernandes_1989, snyder2012forecasting,Kolassa_2016}. 
Our first model, NegBinGP,
couples the Gaussian Process with a negative binomial distribution.

However, a  unimodal forecast distribution can be restrictive.
Indeed, often probabilistic models for intermittent time series have a  bimodal
 distribution, with a point mass in zero and a distribution over positive values. 
We obtain a similar effect by coupling the GP with a Tweedie
distribution \citep{Dunn_Smyth_2005}, which is 
bimodal (with a mode in 0) and long-tailed. We call this model TweedieGP.
To the best of our knowledge, this is the first application of  a fully parameterized Tweedie  distribution to intermittent time series.
Indeed, properly evaluating the Tweedie  distribution  is not straightforward.
In Sec.~\ref{subsec:TweedieLossDensity} we clarify how the Tweedie loss, often used for training point forecast models on intermittent time series 
\citep{jeon2022robust, januschowski2022forecasting}, is obtained by severely approximating the Tweedie distribution.

We perform experiments  on about 40'000 supply chain time series from five different data sets. 
Thanks to  variational methods \citep{Hensman_Matthews_Ghahramani_2015}, the training times of GPs  are in line with  those of other methods for intermittent time series.
Both GPs   generally provide better probabilistic forecasts than the competitors.
Importantly,  TweedieGP outperforms NegBinGP (and all the other competitors) on the highest  quantiles,  the most important for inventory planning.
This might be due to additional flexibility of the Tweedie distribution compared to the negative binomial.


We will release the implementation of our models and
the instructions on how to replicate our experiments.
Our implementation is  based on GPyTorch \citep{Gardner_Pleiss_Weinberger_Bindel_Wilson_2018}.
Upon acceptance we will make available our code repository
and we will release the Tweedie distribution as a class for PyTorch \citep{pytorch_2019}, making it  available
for training more general probabilistic models.



\section{Literature on probabilistic models for intermittent time series}
\label{sec:SoAintermittent}
Denoting the observation at time $t$ as  $y_t$
and the indicator function as
$\mathbbm{1}$, we obtain the  
occurrence  and the demand at time $t$ as:
\begin{align*}
o_t &:= \mathbbm{1}_{[ y_t > 0]}, \\
d_t & := 
\begin{cases}
    y_t \ & \text{if} \  y_t > 0 \\
    \text{undefined} &\text{otherwise.}
\end{cases}.
\end{align*} 

\citet{Hyndman_2008} propose a probabilistic version of Croston's method \citep{Croston_1972}, in which both the demand time series and the inter-demand intervals are predicted with two independent exponential smoothing processes. The probability of occurrence is obtained by inverting the estimated inter-demand intervals.
As a result of having two latent processes, the forecast distribution is a mixture of a Bernoulli and a shifted Poisson.

\citet{snyder2012forecasting} propose a model in which two latent independent exponential smoothing processes estimate the probability of occurrence and the demand. In this case, both simple and damped exponential smoothing are considered. 
Again, the forecast distribution is a mixture of a Bernoulli for $o_t$ and a shifted Poisson for $d_t$.

\cite{snyder2012forecasting} also propose another model with a latent  exponential smoothing that controls the mean of a negative binomial distribution. 
Thus the forecast distribution is 
unimodal and overdispersed. 
% The negative  binomial is a suitable distribution suitable for intermittent data since it can have a mass peak in zero and heavy tails.



\citet{sbrana2023modelling}, instead, models the time series with a latent process which can be equal to 0 with constant probability. The resulting forecast distribution is composed by a mass in zero and a truncated Gaussian distribution on the positive real line.

The iETS model \citep{Svetunkov_Boylan_2023} 
assumes the independence of occurrence and demand 
and models the latent variables via exponential smoothing. The latent demand variable is modelled as a multiplicative exponential smoothing process.
iETS considers different models of occurrence, which are based on one or two exponential smoothing processes; they cover cases such as  demand building up (the probability of occurrence increases over time), demand obsolescence, etc. 
The best occurrence model is chosen via AICc. 
The forecast distribution is the mixture of a Bernoulli and a Gamma distribution.

Similar models have been also implemented in a Bayesian fashion \citep{yelland2009bayesian, chapados2014effective, Babai_Chen_Syntetos_Lengu_2021}, with the advantage  of propagating the uncertainty on the parameters of the forecast distribution. However, there is no readily available software implementation of them. 

A non-parametric approach based on the independence between occurrence and demand is given by the WSS method~\citep{Willemain_Smart_Schwarz_2004}, which
%is a  bootstrapping method which  decomposes the time series into occurrence and demand.
 models the occurrence using a two-states Markov chain. When the simulated occurrence is positive, it samples the demand  from past values of $d_t$, $t<T$, via bootstrapping with jittering. The resulting forecast distribution for $y_{T+1}$ is a mixture between a mass in 0 and an integer-valued, non-parametric distribution. A similar approach is also followed by \citet{Zhou_Viswanathan_2011}.

 Summing up, the most common choice 
is to  independently model occurrence and demand, 
which results in a bimodal forecast distribution.
However, demand intervals and demand levels have been found to be correlated \citep{Altay_Litteral_Rudisill_2012}.

However, if the time series contains a very large amount of zeros, static models might  be preferable to time-series ones \citep{snyder2012forecasting}. 
The static model might be constituted by the empirical quantiles 
\citep[Sec 13.2]{boylan2021intermittent} or
by a fitted parametric distribution such as the negative binomial \citep{Kolassa_2016}.


\section{Gaussian Processes}
\label{sec:GP}
We model the  dynamic of the latent variable with a Gaussian Process \citep[GP,][]{Rasmussen_Williams_GPML}. 
A GP provides a prior distribution on the space of functions $f: \mathbb{R}_+ \rightarrow \mathbb{R}$:
\begin{equation}
\label{eq:GP1}
   f \sim \mathcal{GP}\left(m(\cdot ), k(\cdot, \cdot)\right),
\end{equation}
where $m(\cdot)$ is a mean function and $k(\cdot,\cdot)$ is a positive definite kernel. We set $m(t)=c$ for all $t$, where $c\in \mathbb{R}$ is a learnable parameter.

We consider a training set of $T$ couples $(t_1, y_1), \ldots, (t_T, y_T)$, where, for each $i$, $t_i$ is the time instance and $y_i$ is the time series observation.
The training set does not need to be 
regularly spaced.
We denote by $\mathbb{P}$ and $p$  probabilities and density functions respectively. 
The GP provides a prior on the latent vector $\mathbf{f}_{1:T} := (f(t_1), \dots, f(t_T))^\top$ which is a multivariate Gaussian distribution:
\begin{equation}
\label{eq:prior}
    p(\mathbf{f}_{1:T}) = \mathcal{N}(\mathbf{f}_{1:T} \mid \mathbf{m}_{1:T}, K_{T,T}).
\end{equation}
As kernel, we use a Radial Basis Function (RBF):
\begin{equation*}
    k(t_i, t_j) = \theta^2 \exp \left( - \frac{|t_i - t_j|^2}{2 l^2} \right),
    \label{eq:rbf}
\end{equation*}
where the lengthscale $l \in \mathbb{R}_+$ and the outputscale $\theta\in \mathbb{R}_+$ are kernel hyper-parameters.
The RBF kernel assumes $f$ to be a smooth function of time. The lengthscale  determines how fast it changes in time: a smaller 
$l$ results in quicker variations of $f$. The outputscale instead controls the range of values attained by the latent function. See \citet[Chap. 4]{Rasmussen_Williams_GPML} for more details. 


Samples of vectors
$\mathbf{f}_{1:T}$
are shown in  Fig.~\ref{fig:prior_samples} (left).
At time $t$, the $\gamma$-level \textit{credible interval} is the interval that contains $\gamma$ $\%$  of values  of $f_t$.
A priori, the function
has the same mean and variance at any time; hence
 the credible intervals are flat (Fig.~\ref{fig:prior_samples}, right).

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/prior_samples.png}
    \caption{
    Sample trajectories drawn from the GP prior.
    The single trajectories fluctuate around the  mean, while the credible intervals are flat.}
\label{fig:prior_samples}
\end{figure*}



We  ensure the positivity of the latent function
by passing it through the softplus function, defined as  $\softplus(x) := \log(1 + e^x)$. The softplus  maps negative values of $x$ to small positive values, while it is  close to the identity function for $x>2$. 
A priori, the credible intervals of
$\mathrm{softplus}(\mathbf{f}_{i})$ 
are identical for all time instants but also  non-negative and asymmetric (Fig.~\ref{fig:prior_posterior_latent}, left).
% $\mathrm{softplus}(\mathbf{f}_{i})$, for $i=1, \ldots T$. 


Assuming the observations to be conditionally independent given the value of the latent function,
the likelihood function is:
\begin{equation}
\label{eq:genericLikelihood}
  p( \mathbf{y}_{1:T} \mid \mathbf{f}_{1:T}, \boldsymbol{\theta}_{\mathrm{lik}}) = \prod_{i=1}^T 
p_{\mathrm{lik}} \left(y_i \mid \mathrm{softplus}(f_i), \boldsymbol{\theta}_{\mathrm{lik}} \right),
\end{equation}
where $p_{\mathrm{lik}}(\cdot \mid \mathrm{softplus}(f_i), \boldsymbol{\theta}_{\mathrm{lik}})$ is either the negative binomial or the Tweedie distribution and $\boldsymbol{\theta}_{\mathrm{lik}}$ are  their hyper-parameters,
% of the Tweedie and negative binomial distribution, 
which we discuss in Secs.~\ref{subsec:negBinLik} and~\ref{subsec:tweedie_distr}. 

In a Bayesian framework, even after observing the training data, 
we have residual uncertainty about $\mathbf{f}_{1:T}$, described by its posterior  distribution:
\begin{equation}
   p(\mathbf{f}_{1:T} \mid \mathbf{y}_{1:T}) \propto p(\mathbf{y}_{1:T} \mid \mathbf{f}_{1:T}, \boldsymbol{\theta}_{\mathrm{lik}}) p(\mathbf{f}_{1:T} ),
\label{eq:posteriorGeneric}
\end{equation}
as shown in the right panel of Fig.~\ref{fig:prior_posterior_latent}. 

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/variationalGP_1_2.png}
    \caption{
    Prior (left) and posterior (right) distribution of the 
    softplus of the GP latent function computed with a Tweedie likelihood. The shaded areas represent the 90\% and 95\% credible intervals. The vertical line divides train and test data. The data consist of the 1000-th time series from the Auto data set.}
    \label{fig:prior_posterior_latent}
\end{figure*}


The posterior  in eq.~\eqref{eq:posteriorGeneric} is  available in analytic form only  for the Gaussian likelihood.  
Since our likelihoods are non-Gaussian,  we  approximate $p(\mathbf{f}_{1:T}~\mid~\mathbf{y}_{1:T})$ with a variational inducing point approximation \citep{Hensman_Fusi_Lawrence_2013} 
constituted
by  a Gaussian density, denoted by $q(\mathbf{f}_{1:T} \mid \mathbf{y}_{1:T})$, with tunable mean and covariance parameters. We follow \cite{Hensman_Matthews_Ghahramani_2015} to optimize the model parameters
as described in~\ref{sec:learning_variationalGP}. 

% Under the variational approximation 
The distribution of the future values of the latent function, $\mathbf{f}_{T+1:T+h} = (f(t_{T+1}), \ldots, f(t_{T+h}))^\top$, is:
% at the future time instances $t_{T+1}, \ldots, t_{T+h}$ is:
\begin{equation}
    p(\mathbf{f}_{T+1:T+h} \mid  \mathbf{y}_{1:T})= \int p(\mathbf{f}_{T+1:T+h} \mid \mathbf{f}_{1:T} ) q(\mathbf{f}_{1:T} \mid \mathbf{y}_{1:T})\mathrm{d}\mathbf{f}_{1:T},
    \label{eq:pred_latent}
\end{equation}
where  the integral is solved analytically since both $p$ and $q$ are Gaussian.

We obtain the forecast distribution 
by sampling from $p(\mathbf{f}_{T+1:T+h} \mid \mathbf{y}_{1:T})$ and passing each sample through the likelihood.
Thus the
uncertainty on
$\mathbf{f}_{T+1:T+h}$
is propagated to the forecast distribution $p(\mathbf{y}_{T+1:T+h}~\mid~\mathbf{y}_{1:T})$.
Fig.~\ref{fig:predicitve_forecast} shows the posterior distribution of $\mathbf{f}_{T+1:T+h}$ (left) and the forecast distribution (right). 


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/variationalGP_3_4.png}
    \caption{
    \label{fig:predicitve_forecast}
    Example of  TweedieGP forecasting. Left: posterior distribution of the GP latent function passed through the softplus. Right:  forecast distribution using the Tweedie likelihood. The shaded regions represents $90\%$ and $95\%$ prediction intervals respectively. The figures complement the model fit in Fig.~\ref{fig:prior_posterior_latent}.}
\end{figure*}

 
% \subsection{Choice of likelihood function}
% \label{subsec:likelihood}
We now complete the GP model by specifying the likelihood function.

\subsection{Negative binomial likelihood}
\label{subsec:negBinLik}
We denote by $\mathrm{NegBin}(y;r,\negBinp)$  the density of a negative binomial distribution with number of successes $r$ and success probability $\negBinp$. We set $r = \text{softplus}(f)$. 
Note that the mean and the variance of the distribution linearly increase with $r$.
Instead, $\negBinp$ is a hyper-parameter ($\boldsymbol{\theta}_{\mathrm{lik}} = \{ \negBinp \}$), learned once  via optimization and kept fixed for all the data points of the same time series.

The likelihood of an observation $y_i$ is: 
\begin{equation*}
p_{\mathrm{lik}} \left(y_i \mid \mathrm{softplus}(f_i), \boldsymbol{\theta}_{\mathrm{lik}} \right)=
%p_{\mathrm{lik}}(\ybf_{1:T} \mid \mathbf{f}_{1:T}) = \prod_{i=1}^T 
\mathrm{NegBin}(y_i ; \softplus ( f_i ), \negBinp).
\end{equation*}


\subsection{The Tweedie likelihood}


The Tweedie is a family of distributions \citep{Dunn_Smyth_2005}  characterized by a power mean-variance relationship. A non-negative random variable $Y$ is distributed as a Tweedie, $Y \sim \mathrm{Tw}(\mu, \phi, \rho)$, if:
\begin{equation*}
    \label{eq:mean-var}
    \mathrm{Var}[Y] = \phi \mu^\rho,
\end{equation*} 
where $\mu > 0$ is the mean, $\rho > 0$ is  the \textit{power} and $\phi > 0$ is the \textit{dispersion} parameter. 

\label{subsec:tweedie_distr}
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/tweedie_distr.png}
    \caption{
    The Tweedie distribution is  flexible and  bimodal. The two distributions have the same mean, but the right one has higher dispersion 
     ($\phi$).  This results in a larger mass in 0 and in a longer right tail.}
    \label{fig:tweedie}
\end{figure*}

We assume $\rho \in (1,2)$, which allows 
interpreting the Tweedie \citep{Dunn_Smyth_2005} as a Poisson mixture of Gamma distributions:
\begin{align*}
Y & \sim \sum_{i=0}^N X_i, \\
X_i & \iid \mathrm{Gamma}(\alpha, \beta),\\
N & \sim \mathrm{Poisson}(\lambda),
\end{align*}
with
\begin{equation}
\label{eq:reparam_tw}
\lambda = \frac{\mu^{2-\rho}}{\phi (2 - \rho)}, \quad \alpha = \frac{2-\rho}{\rho -1} ,\quad \beta = \frac{1}{\phi (\rho -1) \mu^{\rho -1}}.    
\end{equation}

The mass in 0 and the density for $y>0$ are:
\begin{align}
    \mathbb{P} (Y = 0) & = \mathbb{P} (N = 0) = e^{-\lambda},
    \label{eq:probZero}\\
p(y \mid \lambda, \alpha, \beta) & = \sum_{n=1}^{+\infty}
 e^{-\lambda} \frac{\lambda^n}{n!} \cdot \mathrm{Ga}(y \mid n\cdot \alpha, \beta),
 \label{eq:TwDensitySampling}
\end{align}
where $\mathrm{Ga}(\cdot)$ is the density of a Gamma distribution. 
The Tweedie distribution is  generally bimodal (Fig.~\ref{fig:tweedie}): the first mode is   in $0$, while the second one is the mode of the sum of Gamma distributions. 
 The continuous density function on the positive real values can be adapted to count data by rounding.


%\Gio{perchè lo evitiamo? dovremmo cmq spiegare cosa è.. forse toglierei questo commento. We do not need separating occurence and demand because our forecasting distribution is already bimodal}.

The parametrization (\ref{eq:probZero}-\ref{eq:TwDensitySampling}), though
 interpretable, is
not usable as a likelihood function. Indeed there is no theoretical result  for  truncating the infinite sum in eq.~\eqref{eq:TwDensitySampling}  while controlling the exceedance probability. 
In order to evaluate the Tweedie we
use the  $(\mu, \phi, \rho)$ parametrization 
 \citep{Dunn_Smyth_2005}.
The probability of zero and the continuous density for $y>0$ are:
\begin{align}
\mathbb{P}(Y = 0) & = \exp\left(-\frac{\mu^{2-\rho}}{\phi(2-\rho)}\right),
\label{eq:tweedie_p0}
\\
p(y\mid\mu, \phi, \rho) & = 
A(y) \cdot \exp \left[ \frac1\phi \left( y \frac{\mu^{1-\rho}}{1-\rho} - \frac{\mu^{2-\rho}}{2-\rho} \right) \right],
\label{eq:tweedie_lik}
\end{align}
with 
\begin{align}
\label{eq:A}
A(y) &= \frac{1}{y}\sum_{j=1}^{\infty} \frac{y^{j\alpha}(\rho-1)^{-j\alpha }}{\phi^{j(1+\alpha)}(2-\rho)^jj!\Gamma(j\alpha)} \\ \nonumber
&= \frac{1}{y}\sum_{j=1}^{\infty} V(j),
\end{align}
where $\alpha$ is defined in eq.~\eqref{eq:reparam_tw}. 
\cite{Dunn_Smyth_2005} provide
a truncating rule for evaluating the infinite summation of eq.~\eqref{eq:A}. It exploits the fact that $V(j)$ is a concave  function of $j$. In most cases only few terms of the summation need to be evaluated, as we show in~\ref{sec:truncating_sum}. 
The Tweedie distribution 
is thus difficult to implement; 
yet, once implemented, 
its evaluation only requires a small overhead compared to other distributions.
% We make available our implementation.... 
% } \Ste{ho provato a sottolinearlo in seguito, nella Sec. experiments}



The likelihood of TweedieGP  is thus obtained by substituting in eq.~\eqref{eq:genericLikelihood}:
\begin{equation*}
p_{\mathrm{lik}} \left(y_i \mid \mathrm{softplus}(f_i), \boldsymbol{\theta}_{\mathrm{lik}} \right)=
%p_{\mathrm{lik}}(\ybf_{1:T} \mid \mathbf{f}_{1:T}) = %\prod_{i=1}^T 
\mathrm{Tw}(y_i ; \softplus ( f_i ), \phi, \rho),    
\end{equation*}
where $\boldsymbol{\theta}_{\mathrm{lik}} = \{\phi,\rho\}$. 
We  set    $\mu = \text{softplus}(f)$.
Thus the latent variable affects,
through   $\lambda$ and $\beta$,
both the mass in $0$  and 
the  distribution on the positive $y$, see
eq.~\eqref{eq:reparam_tw}, \eqref{eq:probZero}, 
\eqref{eq:TwDensitySampling}. We thus control a bimodal forecast distribution using a single latent process.
The hyper-parameters $\phi$ and $\rho$  are optimized and they are equal for all the data points in the same time series. 

We now discuss the relation between  the Tweedie
distribution and the Tweedie loss, often
used to train point forecast models on
intermittent time series.


\subsubsection{Comparison with Tweedie loss}
\label{subsec:TweedieLossDensity}
\cite{januschowski2022forecasting} mentions
the availability of the  \textit{Tweedie loss} for tree-based models as one of the reason of their good performance in the M5 competition.
The Tweedie loss is indeed available both in \texttt{lightGBM}\footnote{\label{footnote:lightGBM}\url{https://lightgbm.readthedocs.io/en/latest/Parameters.html}}
and in \texttt{PyTorch Forecasting}\footnote{\label{footnote:TweedieLoss}\url{https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.metrics.point.TweedieLoss.html}}.
Moreover,  \cite{jeon2022robust} obtained good results in the M5 competition by training the DeepAR model \citep{salinas2020deepar} with the Tweedie loss.
Recall that all such models  only return point forecasts.


The Tweedie loss  \citep{jeon2022robust} is:
\begin{equation}
\mathcal{L}(\mu, \rho\mid y) =- y \frac{\mu^{1-\rho}}{1-\rho} + \frac{\mu^{2-\rho}}{2-\rho}.
\label{eq:tw_loss_l}
\end{equation}

Interpreting eq.~\eqref{eq:tw_loss_l}  as a  negative log-likelihood,   the implied likelihood is:
\begin{equation}
  p (y \mid \mu, \rho) = \exp \left( y \frac{\mu^{1-\rho}}{1-\rho} - \frac{\mu^{2-\rho}}{2-\rho} \right),
    \label{eq:tw_loss_p}
\end{equation}

which approximates the Tweedie density in eq.~\eqref{eq:tweedie_lik}  by setting both $A=1$ and  $\phi=1$. 
Setting $A(y)=1$  avoids the evaluation of the infinite summation in eq.~\eqref{eq:A}; however, this makes the distribution unimodal  and shortens its tails.
Indeed,  $A$ is a function of $y$ and not a simple normalization constant.
Moreover, fixing the dispersion parameter to $\phi=1$ further reduces the flexibility.

We can nevertheless understand why
the function in eq.~\eqref{eq:tw_loss_l} is an effective loss function for  point forecast on intermittent time series. The first term of the exponential corresponds to an unnormalised exponential distribution and the second term is a penalty term 
which forces $\mu$  to remain close to zero.
We illustrate the shape of the unnormalized distribution eq.~\eqref{eq:tw_loss_p} in~\ref{sec:comparison_tweedie_loss}. 

% Eq.~\eqref{eq:tw_loss_l}, however, is not recommended  for training a probabilistic model.
In Sec.~\ref{subsec:likComparison} we 
show that the accuracy of TweedieGP
worsens when we train it by using  the approximation with $A=1$, $\phi=1$
rather than the actual Tweedie distribution.

\section{Experiments} \label{sec:experiments}

\begin{figure*}[!ht]
    \centering
\includegraphics[width=1\linewidth]{figures/datasets_boxplots.png}
    \caption{Proportion of zeros and mean demand in the different data sets. Notice that  the y-axis of the median demand is in natural log scale.}
    \label{fig:datasets}
\end{figure*}

\begin{table}[!h] 
\centering
\begin{tabular}{l|rcrr}
\toprule
Name & \# of t.s. & Freq & T & h   \\ 
\midrule
M5 & 29003 & Daily & 1941 & 28 \\ 
\rowcolor{backcolour}
OnlineRetail & 2023 & Daily & 333 & 31 \\
Auto & 1227 & Monthly & 18 & 6 \\ 
\rowcolor{backcolour}
Carparts & 2499 & Monthly & 45 & 6 \\ 
RAF & 5000 & Monthly & 72 & 12 \\ 
\bottomrule
\end{tabular}
\caption{Characteristics of the extracted intermittent time series. 
The data sets are from these domains:
in-store sales, online sales,
spare parts supply for cars and aircrafts. }
\label{tab:datasets}
\end{table}

We consider a time series as intermittent \citep{Syntetos_Boylan_Croston_2005} if the mean  interval between two positive demands (ADI) is larger than 1.32.
By applying this criterion, we extract about  40'000 intermittent time series
from the five  data sets  of Tab.~\ref{tab:datasets}. 
We show in Fig.~\ref{fig:datasets} how the proportion of zeros and the demand size varies across the time series of each data set.

% The data sets have very different characteristics.
The data sets with the highest proportion of zeros (median  $0.9$) are RAF and OnlineRetail (Fig.~\ref{fig:datasets} left). 
An important difference is that the time series of  OnlineRetail are about five times longer than those of RAF, Tab.~\ref{tab:datasets}.
In contrast, Auto has the lowest proportion of zeros
 (median $0.35$) and its time series are also the shortest ($T=18$). 
The M5 data set is the most heterogeneous as for proportion of zeros and it generally  has narrow demand.  It also contains  the largest amount (29'003) of time series, which are also very long  ($T=1941$).


\subsection{GP training}

We implement our model in GPyTorch \citep{Gardner_Pleiss_Weinberger_Bindel_Wilson_2018}. 
We estimate the variational parameters, the hyper-parameters of the GP ($c, l,\theta$) and of the likelihood ($\negBinp$ for the negative binomial distribution, $\phi$ and $\rho$ for the Tweedie)  via gradient descent for 100 iterations with early stopping, using Adam optimizer \citep{Kingma_Ba_2017} with learning rate  $0.1$. 



We train a GP for each time series.
Its training time is generally comparable to state of the art methods 
for intermittent time series, 
as detailed in Sec.~\ref{sec:computational_times}.  
Rarely, the  training might fail due to  numerical issues; when this happens, we restart the optimization. 
For both NegBinGP and TweedieGP, we obtain
the forecast distribution by drawing 50'000 samples.

%When we use the Tweedie likelihood, 
For TweedieGP, we scale the data by the median demand to ease the optimisation.
This scaling can be applied  as the zeroes remain  unchanged and the Tweedie likelihood is absolutely continuous on positive data.
We bring the samples back to the original scale after drawing them.
We evaluate the effect of the scaling in Sec.~\ref{subsec:likComparison}.


\subsection{Baselines}
We compare NegBinGP and TweedieGP against 4  competitors. 
In order of complexity they are:  empirical quantiles, WSS, \ADIDA, and iETS. 

The empirical quantiles  (EmpQuant) assume the demand to be  i.i.d.
(\citet{Kolassa_2016}, \citet[Sec 13.2]{boylan2021intermittent}).
% \Dar{perche' citiamo prima Boylan di Kolassa?}
The predicted quantiles are thus equal to the quantiles of the training set. 

The WSS model~\citep{Willemain_Smart_Schwarz_2004}, already mentioned in Sec.~\ref{sec:SoAintermittent}, is a  bootstrapping method which  decomposes the time series into occurrence and demand.
It models the dependence between occurrences using
a two-states Markov chain.
When the sampled occurrence is positive, it samples the demand  from past values via bootstrapping with jittering.
We developed our own implementation of  WSS, drawing 50'000 samples for the  forecast distribution. 


We then consider the implementation of
ADIDA \citep{Nikolopoulos_Syntetos_Boylan_Petropoulos_Assimakopoulos_2011}
provided by \texttt{statsforecast} \citep{Garza_Canseco_Challú_Olivares_2022}, which we refer to \ADIDA.
ADIDA proceeds by temporally aggregating the intermittent time series.
The aggregated time series are simpler to forecast as they contain fewer zeros. The predictions on aggregated time series, made with exponential smoothing, are then disaggregated to the  original time buckets.
While ADIDA is  a point forecast method, \ADIDA~quantifies the  uncertainty of the forecast via  conformal inference \citep{Angelopoulos_Bates_2022}; in order to obtain a non-negative distribution we perform a further step, assigning to zero all the mass on the negative values. 

Finally, we use  iETS \citep{Svetunkov_Boylan_2023} from the \texttt{smooth} package (\citet{Svetunkov_smooth}, v.~4.0.2). 
We use the most complete model (iETS$_A$) which fits four different occurrence models  and selects among them via AICc.

In~\ref{sec:reproducibility}, we show how to reproduce our experiments and we detail the integration of our models in  GPyTorch  \citep{Gardner_Pleiss_Weinberger_Bindel_Wilson_2018}.


\subsection{Metrics}
Denoting by $\hat{y}_q$   the forecast of quantile  $q$,  and by
$y$  the observed value, 
the quantile loss \citep{Gneiting_Raftery_2007} is:
\begin{equation}
    \mathrm{Q}_{q}(\hat{y}_q, y) = 
2 \cdot\begin{cases} q (y -\hat{y}_q) \quad& \text{if} \ y \geq \hat{y}_q \\ 
(1-q)(\hat{y}_q - y) \quad& \text{else} \end{cases}.
    \label{eq:quantileLoss}
\end{equation}

We evaluate $\mathrm{Q}_{q}(\hat{y}_q, y)$ for the quantile levels $q \in \{ 0.5, 0.8, 0.9, 0.95, 0.99 \}$. %
%
We do not assess quantiles lower than $0.5$ because they
are generally estimated as zero by all  models. 
For the same reason we  do not report the Continuous Ranked Probability Score (CRPS), which  averages the quantile loss over all quantiles.
% and it is severely influenced by quantiles lower than $0.5$.  

The quantile loss is scale-dependent and thus it is not suitable to be averaged across time series. 
In order to obtain a  scale-free indicator  \citep[Sec 5.8]{Hyndman_2021} we scale it 
by the quantile loss of the empirical quantiles ($\mathrm{emp}_q$) on the training data:
\begin{equation*}
   \mathrm{sQ}_{q}(\hat{y}_q, y) = \frac{\mathrm{Q}_{q}(\hat{y}_q, y)} {\frac1T \sum_{t=1}^T \mathrm{Q}_{q}(\mathrm{emp}_q, y_t)}. 
\end{equation*}
%
The scaled quantile loss values are usually greater than 1, since  
the scaling factor in the denominator, being based on the training set, is optimistically biased. 
For high levels of $q$, this metric strongly penalises cases in which $y$ exceeds the predicted value $\hat{y}_q$, that is, when the demand is underestimated.

We  assess the estimated probability of  zero demand and positive demand with the Brier score: 
\begin{equation*}
\mathrm{Br}(\hat{p},y) = (\mathbbm{1}_{[  y > 0]} - \hat{p})^2,    
\end{equation*}
where $\hat{p} \in [0,1]$ is the predicted probability of  $y>0$.

Finally, we measure the forecast coverage, i.e., the proportion of observations that lie within the $(1-q)$ prediction interval.
An ideal forecast has coverage  $q$. 

\subsection{Discussion}

\begin{table}[!ht]
    \centering
    \begin{tabular}{ll|cccccc}
    \toprule
 \rotatebox[origin=c]{90}{Data set} & \rotatebox[origin=c]{90}{Metric} & \rotatebox[origin=c]{90}{EmpQuant} & \rotatebox[origin=c]{90}{WSS} & \rotatebox[origin=c]{90}{\ADIDA} & \rotatebox[origin=c]{90}{iETS} & \rotatebox[origin=c]{90}{NegBinGP} & \rotatebox[origin=c]{90}{TweedieGP} \\ 
 \midrule 
 \greytabline  \multirow[c]{6}{*}{\rotatebox[origin=c]{90}{M5}}
& Br & 0.23 & 0.23 & 0.22 & \textbf{0.20} & \textbf{0.20} & \textbf{0.20} \\
 % \cline{2-7}
  & $\mathrm{sQ}_{0.5}$ & 1.88 & 1.87 & 1.88 & 1.82 & \textbf{1.78} & 1.79 \\
 & \cellcolor{backcolour}$\mathrm{sQ}_{0.8}$ & \cellcolor{backcolour}1.74 & \cellcolor{backcolour}1.77 & \cellcolor{backcolour}\textbf{1.46} & \cellcolor{backcolour}1.67 & \cellcolor{backcolour}1.48 & \cellcolor{backcolour}\textbf{1.46} \\
 & $\mathrm{sQ}_{0.9}$ & 1.57 & 1.65 & 1.35 & 1.70 & 1.29 & \textbf{1.26} \\
\greytabline & $\mathrm{sQ}_{0.95}$ & 1.39 & 1.51 & 1.37 & 1.97 & 1.19 & \textbf{1.16} \\
 & $\mathrm{sQ}_{0.99}$ & 1.25 & 1.40 & 1.69 & 3.23 & \textbf{1.16} & \textbf{1.16} \\
\midrule 
\greytabline  \multirow[c]{6}{*}{\rotatebox[origin=c]{90}{OnlineRetail}} 
 & Br & 0.12 & 0.12 & 0.18 &  \textbf{0.11} & \textbf{0.11} & \textbf{0.11} \\
  % \cline{2-7}
 & $\mathrm{sQ}_{0.5}$ & 2.35 & 2.35 & 2.55 & 2.36 & \textbf{2.34} & 2.35 \\
 & \cellcolor{backcolour}$\mathrm{sQ}_{0.8}$ & \cellcolor{backcolour}2.34 & \cellcolor{backcolour}2.33 & \cellcolor{backcolour}2.29 & \cellcolor{backcolour}2.40 & \cellcolor{backcolour}2.27 &\cellcolor{backcolour}\textbf{2.26} \\
 & $\mathrm{sQ}_{0.9}$ & 2.31 & 2.29 & 2.23 & 2.50 & 2.23 & \textbf{2.20} \\ 
 & \cellcolor{backcolour}$\mathrm{sQ}_{0.95}$ & \cellcolor{backcolour}2.33 & \cellcolor{backcolour}2.30 & \cellcolor{backcolour}2.35 & \cellcolor{backcolour}2.73 & \cellcolor{backcolour}2.25 & \cellcolor{backcolour}\textbf{2.23} \\
 & $\mathrm{sQ}_{0.99}$ & 3.02 & \textbf{2.98} & 4.09 & 4.30 & 3.22 & \textbf{2.98} \\
\midrule 
\greytabline  \multirow[c]{6}{*}{\rotatebox[origin=c]{90}{Auto}} 
 & Br & \textbf{0.25} & 0.26 & 0.29 & \textbf{0.25} & \textbf{0.25} & \textbf{0.25} \\
 % \cline{2-7}
 & $\mathrm{sQ}_{0.5}$ & 1.15 & 1.36 & 1.19 & 1.17 & \textbf{1.14} & \textbf{1.14} \\
 & \cellcolor{backcolour}$\mathrm{sQ}_{0.8}$ & \cellcolor{backcolour}1.30 & \cellcolor{backcolour}1.53 & \cellcolor{backcolour}1.34 & \cellcolor{backcolour}1.36 & \cellcolor{backcolour}1.29 & \cellcolor{backcolour}\textbf{1.28} \\
 & $\mathrm{sQ}_{0.9}$ & 1.48 & 1.65 & 1.59 & 1.52 & \textbf{1.42} & \textbf{1.42} \\
  & \cellcolor{backcolour}$\mathrm{sQ}_{0.95}$ & \cellcolor{backcolour}1.84 & \cellcolor{backcolour}1.84 & \cellcolor{backcolour}2.21 & \cellcolor{backcolour}1.74 & \cellcolor{backcolour}1.65 & \cellcolor{backcolour}\textbf{1.62} \\
 & $\mathrm{sQ}_{0.99}$ & 4.32 & 2.57 & 7.51 & 3.16 & \textbf{2.45} & 2.56 \\
\midrule 
\greytabline \multirow[c]{6}{*}{\rotatebox[origin=c]{90}{Carparts}} 
 & Br & 0.17 & 0.16 & 0.19 & \textbf{0.15} & \textbf{0.15} & 0.16 \\
 % \cline{2-7}
 & $\mathrm{sQ}_{0.5}$ & 1.13 & 1.15 & 1.20 & \textbf{1.09} & \textbf{1.10} & 1.11 \\
 & \cellcolor{backcolour}$\mathrm{sQ}_{0.8}$ & \cellcolor{backcolour}1.18 & \cellcolor{backcolour}1.33 & \cellcolor{backcolour}1.14 & \cellcolor{backcolour}1.15 & \cellcolor{backcolour}1.10 & \cellcolor{backcolour}\textbf{1.09} \\
 & $\mathrm{sQ}_{0.9}$ & 1.25 & 1.45 & 1.16 & 1.27 & 1.16 & \textbf{1.13} \\
 & \cellcolor{backcolour}$\mathrm{sQ}_{0.95}$ & \cellcolor{backcolour}1.32 & \cellcolor{backcolour}1.63 & \cellcolor{backcolour}1.34 & \cellcolor{backcolour}1.54 & \cellcolor{backcolour}\textbf{1.17} & \cellcolor{backcolour}1.19 \\
 & $\mathrm{sQ}_{0.99}$ & 1.86 & 1.99 & 3.72 & 3.11 & 1.65 & \textbf{1.56} \\
 \midrule 
\greytabline \multirow[c]{6}{*}{\rotatebox[origin=c]{90}{RAF}}
 & Br & \textbf{0.08} & \textbf{0.08} & 0.14 & \textbf{0.08} & \textbf{0.08} & \textbf{0.08} \\
 % \cline{2-7}
 & $\mathrm{sQ}_{0.5}$ & \textbf{1.00} & \textbf{1.00} & 1.37 & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} \\
 & \cellcolor{backcolour}$\mathrm{sQ}_{0.8}$ & \cellcolor{backcolour}\textbf{1.00} & \cellcolor{backcolour}1.01 & \cellcolor{backcolour}1.22 & \cellcolor{backcolour}\textbf{1.00} & \cellcolor{backcolour}1.01 & \cellcolor{backcolour}1.01 \\
 & $\mathrm{sQ}_{0.9}$ & 1.10 & 1.16 & 1.17 & \textbf{1.06} & 1.12 & 1.14 \\
 & \cellcolor{backcolour}$\mathrm{sQ}_{0.95}$ & \cellcolor{backcolour}\textbf{1.24} & \cellcolor{backcolour}1.43 & \cellcolor{backcolour}1.35 & \cellcolor{backcolour}1.39 & \cellcolor{backcolour}\textbf{1.24} & \cellcolor{backcolour}1.26 \\
 & $\mathrm{sQ}_{0.99}$ & 2.12 & 2.21 & 3.79 & 3.72 & 2.21 & \textbf{2.09} \\
 \bottomrule
\end{tabular}
    \caption{Brier score and scaled quantile loss (mean over $h$ and data set).}
    \label{tab:s_bs}
\end{table}

We show in Tab.~\ref{tab:s_bs} the mean score of each method on each data set.
The mean is taken with respect to both the forecast horizons ($1, 2, \ldots, h$) and all the time series of the data sets. 
%
As already discussed, the scaled quantile loss ($\mathrm{sQ}_{q}$) takes large values when the observation is beyond the predicted quantile. 
This rarely occurs with high quantiles, but these cases are relevant for decision making. We thus report the mean of
$\mathrm{sQ}_{q}$ rather than its median, which would overlook those cases.

In each row we boldface the  model with the lowest loss  and the models whose loss is not  significantly different from the best. We test significance of the difference in loss by using the paired $t$-test with  FDR correction for multiple comparisons \citep[Sec 18.7.1]{hastie2009elements}. 

An important finding is  that
TweedieGP is almost always the best performing model  on the  highest quantiles $\{0.9, 0.95,
0.99\}$ , arguably the most important for decision making. Its advantage over the other methods on the highest quantiles might be due to Tweedie distribution’s ability to extend its tails.

More in details, TweedieGP and NegBinGP
tend to   perform  similarly on data sets containing short time series (Auto, Carparts, RAF).
Indeed, when few training data is available, the effect of the GP prior might be more important than the choice of the likelihood.
In contrast, on data sets containing  longer time series
(OnlineRetail, M5), TweedieGP has generally an advantage over  NegBinGP. 
The advantage  of TweedieGP on the highest quantiles
is for instance emphasized on OnlineRetail, whose time series are both long and with high demand (see Fig.~\ref{fig:datasets}),  challenging the long tails of the forecast distribution.


iETS matches the performance of NegBinGP and TweedieGP on the Brier score and  on the median quantile, where the performance of all methods
is closer, but it is often outperformed on the highest quantiles. This is in line with the results 
reported  by \citet{Svetunkov_Boylan_2023}, in which iETS was not the best performing approach on the highest quantiles.

The generally poor performance of \ADIDA~is most likely due to the inadequacy of conformal inference for count data. Indeed, the  empirical quantiles are generally preferable to both WSS and \ADIDA.  
In particular on the  RAF data set, which is characterized by the largest amount of zeros, the  empirical quantiles  match the performance of the GPs.
This confirms the suitability of static models for very sparse time series;  it also shows that the GP latent function  adapts well to  different types of temporal correlations,  becoming also an i.i.d model when needed.

\begin{figure*}[!htp]
    \centering
\includegraphics[width=1\linewidth]{figures/coverage.png}
    \caption{The coverage cannot go below the proportion of zeros in the test set, shown by
    a dashed horizontal line. On RAF, 
    this implies overcoverage of most quantiles, apart from the highest ones.}
    \label{fig:calibrations}
\end{figure*}

Fig.~\ref{fig:calibrations} shows the calibration of selected methods
(empirical quantiles, iETS and TweedieGP) on three data sets (Auto, M5 and RAF).
Better forecasts lie closer to the dotted diagonal line.
On intermittent time series, 
the coverage is at least as large as the proportion of zeros; there is hence
overcoverage of the lower quantiles, as clear from the third panel of Fig.~\ref{fig:calibrations}.

On the Auto data set, which has moderate intermittency, TweedieGP is the only one to have almost correct  coverage on all quantiles above $0.5$. 
A similar comment can be done for M5, even though in this case the proportion of zeros is larger. Correct coverage is provided, only by TweedieGP and empirical quantiles, for  quantiles above $0.8$. 
On the RAF data set, the very sporadic demand implies forecast overcoverage apart from the highest quantiles.

It is also interesting to analyze how the models compare on the quantile loss when they are correctly calibrated.
For instance,  empirical quantiles, iETS and TweedieGP provide correct coverage on the 95-th quantile of M5.
On Auto, both TweedieGP and the empirical quantiles  provide correct coverage on the 90-th quantile.
In  these cases, TweedieGP provides the lowest quantile loss (Tab.~\ref{tab:s_bs}) among correctly 
calibrated models.
Recall that the quantile loss combines a reward for the sharpness and a miscoverage penalty.
Hence, given the same calibration, TweedieGP achieves lower loss as it provides  a sharper estimate of the quantiles. 



\subsection{Computational times}\label{sec:computational_times}


\begin{table*}[!h]
\centering
\begin{tabular}{l|ccccc}
\toprule
Data set & WSS & \ADIDA & iETS & NegBinGP & TweedieGP \\
\midrule
\rowcolor{backcolour} Auto & $0.04 \pm 0.02$ & $0.01 \pm 0.01$ & $0.13 \pm 0.03$ & $0.20 \pm 0.06$ & $0.20 \pm 0.08$ \\
 RAF & $0.05 \pm 0.03$ & $0.01 \pm 0.01$ & $0.20 \pm 0.03$ & $0.22 \pm 0.06$ & $0.30 \pm 0.07$ \\
\rowcolor{backcolour} M5 & $0.10 \pm 0.03$ & $0.01 \pm 0.01$ & $0.76 \pm 0.07$ & $1.20 \pm 0.39$ & $2.12 \pm 0.49$ \\
\bottomrule
\end{tabular}
\caption{Mean and standard deviation of the time (seconds) for training the models and generating forecasts. Experiments are run on the CPU of a  M3 MacBook Pro. }
\label{tab:comp_times}
\end{table*}

We report in Tab.~\ref{tab:comp_times}
the computational times of each method, including  training and generating the forecasts on time series of selected data sets, ordered from the shortest to the longest (see Tab.~\ref{tab:datasets}).  
\ADIDA~and WSS are the fastest, with an average training time of  0.1 sec. or less.

The computational time of our methods is comparable with that of iETS. On the data sets containing the longest time series (M5), the average computational time 
of the GPs is about 2 seconds.
This is noteworthy:  an exact GP implementation
would be rather slow on the time series of the M5 data set, 
due to its  cubic computational complexity 
in the number of observations $T$. Thanks to the variational approximation, the computational complexity of our models is instead quadratic in $T$ for $T<200$. For longer time series the cost is capped by the variational formulation and becomes linear in $T$. See~\ref{sec:learning_variationalGP} for a more detailed evaluation. % it  and quadratically with respect to the size of the variational parameters $m$ \citep{Hensman_Fusi_Lawrence_2013}. This quantity is 
%\Ste{The cost of training a G scales quadratically with respect to the lenght of the time series. For longer time series, this value is lowered (come dire?)}

The computational times of TweedieGP and NegBinGP  are close;  the fully parameterised Tweedie density only implies a small overhead compared to the negative binomial. 
On the M5 data set, NegBinGP is faster than TweedieGP
as it often meets the early stopping condition. 


\subsection{Ablation study}
\label{subsec:likComparison}

In this section we evaluate TweedieGP against two simpler models: a GP model trained with the approximate Tweedie likelihood with $A=\phi=1$ (implied by the Tweedie loss) and a TweedieGP trained on unscaled data. 
% The first model uses the likelihood from which the Tweedie loss discussed in Sec.~\ref{subsec:TweedieLossDensity} can be recovered. We also run a GP model with Poisson likelihood but it showed poor performance, in agreement with previous literature.
We keep the same experimental setup of the previous  section and we show the results  in Tab.~\ref{tab:ablation_study}. 

\begin{table}[!ht]
\centering
\begin{tabular}{ll|ccc}
\toprule
 \rotatebox[origin=c]{90}{Data set}& \rotatebox[origin=c]{90}{Metric}& \rotatebox[origin=c]{90}{Non-scaled Tweedie} & \rotatebox[origin=c]{90}{Tweedie $A=\phi=1$} & \rotatebox[origin=c]{90}{Tweedie} \\
\midrule
\greytabline \multirow{6}{*}{\rotatebox[origin=c]{90}{M5}}
 & Br  & \textbf{0.20} & \textbf{0.20} & \textbf{0.20} \\
% \cline{2-6}
& $\mathrm{sQ}_{0.5}$ & \textbf{1.79} & 1.80 & \textbf{1.79} \\
  & \cellcolor{backcolour}$\mathrm{sQ}_{0.8}$ & \cellcolor{backcolour}\textbf{1.46} & \cellcolor{backcolour}1.47 & \cellcolor{backcolour}\textbf{1.46} \\
 & $\mathrm{sQ}_{0.9}$ & 1.27 & 1.30 & \textbf{1.26} \\
 & \cellcolor{backcolour}$\mathrm{sQ}_{0.95}$ & \cellcolor{backcolour}1.17 & \cellcolor{backcolour}1.23 & \cellcolor{backcolour}\textbf{1.16} \\
 & $\mathrm{sQ}_{0.99}$ & 1.17 & 1.37 & \textbf{1.16} \\
\midrule
\greytabline \multirow{6}{*} {\rotatebox[origin=c]{90}{OnlineRetail}} 
 & Br & 0.12 & 0.13 & \textbf{0.11} \\
% \cline{2-6}
& $\mathrm{sQ}_{0.5}$ & \textbf{2.35} & 2.43 & \textbf{2.35} \\
 & \cellcolor{backcolour}$\mathrm{sQ}_{0.8}$ & \cellcolor{backcolour}2.29 & \cellcolor{backcolour}2.32 & \cellcolor{backcolour}\textbf{2.26} \\
 & $\mathrm{sQ}_{0.9}$ & 2.26 & 2.25 & \textbf{2.20} \\
& \cellcolor{backcolour}$\mathrm{sQ}_{0.95}$ & \cellcolor{backcolour}2.35 & \cellcolor{backcolour}2.32 & \cellcolor{backcolour}\textbf{2.23} \\
 & $\mathrm{sQ}_{0.99}$ & 3.39 & 3.39 & \textbf{2.98} \\
\midrule
\greytabline \multirow{6}{*}{\rotatebox[origin=c]{90}{Auto}}
 & Br & \textbf{0.25} & \textbf{0.25} & \textbf{0.25} \\ 
 %\cline{2-6}
& $\mathrm{sQ}_{0.5}$ & 1.15 & \textbf{1.13} & \textbf{1.14} \\ 
 & \cellcolor{backcolour}$\mathrm{sQ}_{0.8}$ & \cellcolor{backcolour}\textbf{1.28} & \cellcolor{backcolour}1.30 & \cellcolor{backcolour}\textbf{1.28} \\
 & $\mathrm{sQ}_{0.9}$ & 1.43 & 1.44 & \textbf{1.42} \\
  & \cellcolor{backcolour}$\mathrm{sQ}_{0.95}$ & \cellcolor{backcolour}1.66 & \cellcolor{backcolour}1.65 & \cellcolor{backcolour}\textbf{1.62} \\
 & $\mathrm{sQ}_{0.99}$ & 2.80 & 2.58 & \textbf{2.56} \\ 
\midrule
\greytabline \multirow{6}{*}{\rotatebox[origin=c]{90}{Carparts}} 
 & Br & \textbf{0.15} & 0.16 & 0.16 \\
 %\cline{2-6}
& $\mathrm{sQ}_{0.5}$ & \textbf{1.10} & 1.11 & 1.11 \\
& \cellcolor{backcolour}$\mathrm{sQ}_{0.8}$ & \cellcolor{backcolour}\textbf{1.09} & \cellcolor{backcolour}1.10 & \cellcolor{backcolour}\textbf{1.09} \\
 & $\mathrm{sQ}_{0.9}$ & 1.16 & \textbf{1.13} & \textbf{1.13} \\
 & \cellcolor{backcolour}$\mathrm{sQ}_{0.95}$ & \cellcolor{backcolour}1.22 & \cellcolor{backcolour}1.20 & \cellcolor{backcolour}1.19 \\
 & $\mathrm{sQ}_{0.99}$ & 1.73 & \textbf{1.59} & \textbf{1.59} \\
 \midrule 
\greytabline \multirow{6}{*}{\rotatebox[origin=c]{90}{RAF}} 
 & Br & 0.11 & 0.10 & \textbf{0.08} \\
% \cline{2-6}
& $\mathrm{sQ}_{0.5}$ & 1.02 & 1.03 & \textbf{1.00} \\
 & \cellcolor{backcolour}$\mathrm{sQ}_{0.8}$ & \cellcolor{backcolour}1.10 & \cellcolor{backcolour}1.15 & \cellcolor{backcolour}\textbf{1.01} \\
 & $\mathrm{sQ}_{0.9}$ & 1.23 & 1.19 & \textbf{1.14} \\
 \greytabline  & $\mathrm{sQ}_{0.95}$ & 1.36 & \textbf{1.26} & \textbf{1.26} \\
 & $\mathrm{sQ}_{0.99}$ & 2.43 & 2.25 & \textbf{2.09} \\
\bottomrule
\end{tabular}
\caption{Comparison of GP model with different likelihoods; all models use the RBF kernel.} \label{tab:ablation_study}
\end{table}


The accuracy of TweedieGP is generally worse if we do not  scale of  the demand. Due to numerical issues, the posterior mean of the Gaussian process does not grow over a certain threshold, which is problematic when demand becomes high.

Finally, the performance of the approximated Tweedie likelihood with $\phi = A = 1$ confirms what discussed in Sec.~\ref{subsec:TweedieLossDensity}:  the Tweedie loss might produce effective point forecasts by penalizing values far from zero, but the shorter tails negatively affect the estimate of the higher quantiles.

\section{Conclusions}
\label{sec:conclusion}
The two proposed Gaussian Process models
provide state-of-the-art performance for intermittent time series forecasting. 
The Tweedie distribution is an interesting alternative to the negative binomial as a forecast distribution, 
being bimodal and  allowing an accurate estimation of the  highest quantiles.

In this paper we  considered  \textit{local models},  trained on a single time series.
In future it could be interesting to 
train also probabilistic \textit{global models} 
\citep{montero2021principles, salinas2020deepar, seeger2016bayesian} on intermittent time series
with the  Tweedie forecast distribution.


We only used a simple
RBF kernel;  in future works, we could test kernels   composition  \citep[Chap.4]{Rasmussen_Williams_GPML}
in order to model the effect of seasonality  or  of covariates \citep{Corani_Benavoli_Zaffalon_2021}.

%\section*{Acknowledgements}
%This work is partially funded the Swiss National Science Foundation (SNF), grant 200021\_212164/1.
%This project has received funding from the European Union’s Horizon Europe research and innovation Framework under grant agreement No 101160720.
\bibliographystyle{elsarticle-harv}
\bibliography{refs}

\newpage
\appendix


\section{Learning a sparse variational GP with a Tweedie likelihood}\label{sec:learning_variationalGP}

Recall that our forecasting model learns the posterior of the latent function $\mathbf{f}_{1:T}$ given the observations. The prior for a latent vector of length $T$ (training data length) is $p(\mathbf{f}_{1:T}) = \mathcal{N}(\mathbf{f}_{1:T} \mid \mathbf{0}, K_{T,T})$ and the joint distribution of data and latent variables is 
\begin{equation}
    p(\mathbf{y}_{1:T}, \mathbf{f}_{1:T}) = \prod_{i=1}^T \mathrm{Tw}(y_i; \mathrm{softplus}(f_i), \phi, \rho) \mathcal{N}(\mathbf{f}_{1:T} \mid \mathbf{0}, K_{T,T})
    \label{eq:jointTweedie}
\end{equation}
The posterior over latent function $p(\mathbf{f}_{1:T} \mid y_{1:T})$ is not available analytically therefore we need to approximate it. Moreover, in order to optimise the hyperparameters of the model, we also need to approximate the marginal likelihood $p(\mathbf{y}_{1:T})$. We follow \citet{Hensman_Matthews_Ghahramani_2015} and use a sparse GP model with inducing points.

We proceed by augmenting our GP model with additional $m$ input-output pairs $\mathbf{Z}$, $\mathbf{u}$ that are distributed as the GP $f$, i.e. the joint  distribution of the vector $(\mathbf{f}_{1:T},\mathbf{u})$ is $$p(\mathbf{f}_{1:T},\mathbf{u}) = \mathcal{N}\left(\begin{bmatrix}
    \mathbf{f}_{1:T} \\
    \mathbf{u}
\end{bmatrix} \ \Bigg| \ \mathbf{0}, \begin{bmatrix}
    K_{T,T} & K_{T,m} \\
    K_{m,T} & K_{m,m}
\end{bmatrix}\right),$$
where $K_{m,m}$ and $K_{T,m}$ are the covariance matrices resulting from evaluating the kernel $k$ at the input values. Note that the Gaussian assumption implies that $p(\mathbf{f}_{1:T} \mid \mathbf{u})$ is available analytically via Gaussian conditioning. 

The joint distribution of data and latent variables thus becomes $p(\mathbf{y}_{1:T}, \mathbf{f}_{1:T}, \mathbf{u}) = p(\mathbf{y}_{1:T} \mid \mathbf{f}_{1:T})p(\mathbf{f}_{1:T} \mid \mathbf{u})p(\mathbf{u})$. We consider the approximate distribution $q(\mathbf{u}) = \mathcal{N}(\mathbf{u} \mid \mathbf{m}, \mathbf{S})$, where $\mathbf{m}, \mathbf{S}$ are free parameters to be optimized. 

In variational inference we assume that the posterior is approximated by $p(\mathbf{f}_{1:T} \mid \mathbf{y}_{1:T}) \approx q(\mathbf{f}_{1:T} \mid \mathbf{y}_{1:T}) = \int p(\mathbf{f}_{1:T} \mid \mathbf{u})q(\mathbf{u}) d\mathbf{u}$. Since $q(\mathbf{u})$ is Gaussian we can solve the integral analytically. We can then bound the marginal log-likelihood $\log p(\mathbf{y}_{1:T})$ with the standard variational bound \citep{Hensman_Fusi_Lawrence_2013}
$$\log p(\mathbf{y}_{1:T}) \geq \mathbb{E}_{q(\mathbf{u})}[\log p(\mathbf{y}_{1:T} \mid \mathbf{u})] - \mathrm{KL}[q(\mathbf{u}) \mid p(\mathbf{u})],$$ which can be further bounded as 
\begin{equation}
\label{eq:ELBO}
  \log p(\mathbf{y}_{1:T}) \geq \mathbb{E}_{q(\mathbf{f})}[\log p(\mathbf{y}_{1:T} \mid \mathbf{f})] - \mathrm{KL}[q(\mathbf{u}) \mid p(\mathbf{u})].  
\end{equation}

The right-hand side of eq.~\eqref{eq:ELBO} is the evidence lower bound (ELBO). This is a loss function that can be used to optimize the model hyper-parameters ($c, l, \theta, \phi, \rho$), the location of the inducing points ($\mathbf{Z}$) and the variational parameters ($\mathbf{m}, \mathbf{S}$). 

The KL part of the ELBO is available analytically, however the expectation part needs an implementation of the log-likelihood function. The expectation is then evaluated with Monte Carlo sampling by exploiting the fact that $q(\mathbf{f})$ is a Gaussian distribution with known parameters. 

We implemented the log-likelihood function $\log p(\mathbf{y}_{1:T} \mid \mathbf{f})$ for the Tweedie likelihood and then used the GPyTorch infrastructure \citep{Gardner_Pleiss_Weinberger_Bindel_Wilson_2018}, for optimizing the ELBO.  

Given the optimized variational posterior approximation $q(\mathbf{f}_{1:T} \mid \mathbf{y}_{1:T})$, we can compute the predictive latent distribution $h$-step ahead as
\begin{equation}
    p(\mathbf{f}_{T+1:T+h} )= \int p(\mathbf{f}_{T+1:T+h} \mid \mathbf{f}_{1:T} ) q(\mathbf{f}_{1:T} \mid \mathbf{y}_{1:T})\mathrm{d}\mathbf{f}_{1:T},
    \label{eq:predLatent}
\end{equation}
note that this integral has an analytic solution because the distributions are both Gaussian therefore $\mathbf{f}_{T+1:T+h}$ is a multivariate Gaussian distribution with known mean and covariance; see, e.g., \citet{Hensman_Matthews_Ghahramani_2015} for detailed formulas. 

The predictive posterior for the observations is computed by extracting samples $\mathbf{\tilde{f}}^{(j)}$, $j=1, \ldots, N$, from the distribution in eq.~\eqref{eq:predLatent}. For each sample $\mathbf{\tilde{f}}^{(j)}$, we draw one sample from 
\begin{equation*}
p( \mathbf{y}_{T+1:T+h} \mid \mathbf{\tilde{f}}^{(j)}_{T+1:T+h}) = \prod_{i=T+1}^{T+h}p_{\mathrm{lik}} \left(y_i ; \mathrm{softplus}(\tilde{f}^{(j)}_i), \phi, \rho \right)    
\end{equation*}
to obtain a sample from the predictive posterior. 

In our experiments we choose the number of inducing points as follows. On short time series ($T \leq 200$), we use $T$ inducing points and initialize their locations in correspondence of the training inputs. 
On longer time series ($T > 200$)  we use $200$ inducing points, sampling their initial locations from a multinomial distribution with  $p(i)\propto\log \left( 1 + \frac{i}{T} \right)$, $i = 1, \ldots, T$.
Thus the recent observations, which are more relevant for forecasting, have higher probability of being chosen as initial location of the  inducing points. Since the method has a quadratic cost in $m$, the number of inducing points, and it is linear in $T$, the size of the training set, then we achieve quadratic cost in $T$ for $T<200$ and linear cost in $T$ for $T>200$. 

All the steps outlined here are also valid for a negative binomial likelihood, once the joint distribution in eq.~\eqref{eq:jointTweedie} is written with the negative binomial likelihood. 

\section{Evaluation of the Tweedie density}

\subsection{Truncating the infinite summation}
\label{sec:truncating_sum}

In Sec.~\ref{subsec:tweedie_distr} we show that, given $\mu > 0$, $\phi > 0$ and $\rho \in (1,2)$ the Tweedie density is
\begin{equation}
p(y \mid \mu, \phi, \rho) = \begin{cases} \exp(-\frac{\mu^{2-\rho}}{\phi(2-\rho)}) & \text{if} \ y=0; \\
A(y) \cdot \exp \left[ \frac1\phi \left( y \frac{\mu^{1-\rho}}{1-\rho} - \frac{\mu^{2-\rho}}{2-\rho} \right) \right] & \text{otherwise}
\end{cases}
\label{eq:tweedieLik}
\end{equation}
where 
\begin{align}
A(y) &= \frac{1}{y}\sum_{j=1}^{\infty} \frac{y^{j\alpha}(\rho-1)^{-j\alpha }}{\phi^{j(1+\alpha)}(2-\rho)^jj!\Gamma(j\alpha)} \nonumber\\
&= \frac{1}{y}\sum_{j=1}^{\infty} V(j) \quad \text{with} \ \alpha = \frac{2-\rho}{\rho -1}
\label{eq:sum_A_alpha}
\end{align}
\cite{Dunn_Smyth_2005}  evaluate $A(y)$ by approximating the infinite sum in eq.~\eqref{eq:sum_A_alpha} with a finite one, retaining  its largest elements. We start by finding 
\[
\jmax = \argmax_{j \in \mathbb{N}} V(j),
\] 
which is the index of the largest component. To identify it,  let 
\begin{equation}
z =  \frac{y^{\alpha} (\rho-1)^{-\alpha}}{\phi^{1+\alpha}(2-\rho)}, 
\label{eq:z} 
\end{equation}
then 
\[
V(j) = \frac{z^j}{j!\Gamma(\alpha j)}
\]
and
\begin{equation} \log V(j) = j \log z - \log \Gamma \left( 1+j \right) - \log \Gamma \left( \alpha j \right). 
\label{eq:log_Vj}
\end{equation}
Stirling's approximation allows us to simplify the evaluation of the Gamma function: $\Gamma(x+1) \simeq \sqrt{2\pi x} \cdot x^x \cdot e^{-x}$. Therefore 
\begin{equation}
\log \Gamma(x+1) \simeq \frac12 \log 2\pi + \frac12 \log x + x \log x -x .
\label{eq:stirling}
\end{equation}
Approximating $\Gamma(\alpha j)$ with $\Gamma(1+ \alpha j)$ in eq.~\eqref{eq:log_Vj} and using eq.~\eqref{eq:stirling} we have
\begin{align}
\log V(j) \simeq &j\log z - \frac12 \log 2\pi -\frac12 \log j - j \log j + j - \frac12 \log 2\pi \\
&-\frac12 \log \alpha -\frac12 \log j - \alpha j \log \alpha - \alpha j \log j + \alpha j \nonumber \\
= &j \left( \log z +( 1 + \alpha) - \alpha \log \alpha - (1+ \alpha )\log j \right) \\
&- \log 2\pi - \frac12 \log \alpha - \log j .
\label{eq:logVj_approx}
\end{align}
Differentiating with respect to $j$ we have
\begin{align}
     \frac{\partial \log V(j)}{\partial j} \simeq & \log z + (1+\alpha) - \alpha \log \alpha - (1+\alpha ) \log j - j(1+\alpha) \frac1j - \frac1j \nonumber \\ 
     = & \log z - \log j - \alpha \log (\alpha j) - \frac1j \nonumber
     \\ \simeq & \log z - \log j - \alpha \log (\alpha j) 
     \label{eq:dlogVj_dj}
\end{align}
which is a monotone decreasing function of $j$. For this reason, $\log V$ is a convex function (and therefore $V$ too). To identify its maximum point, we equate it to $0$ obtaining
\begin{equation}
(1+\alpha) \log \jmax = \log z - \alpha \log \alpha 
\label{eq:1_alpha_logjmax}
\end{equation}
and substituting $z$ from eq.~\eqref{eq:z} and $\alpha$ from eq.~\eqref{eq:sum_A_alpha}
\begin{align*}
    \jmax^{1+\alpha} %&= z \cdot \alpha^{-\alpha} \\
    &= \frac{y^\alpha (\rho-1)^{-\alpha}}{ \phi^{1+\alpha}(2-\rho)} \cdot \frac{(2-\rho)^{-\alpha}}{(\rho-1)^{-\alpha}}
\end{align*}
and finally, since $\frac{\alpha}{1+\alpha} = 2-\rho$:
\begin{equation}
\jmax = \frac{y^{2-\rho}}{\phi (2-\rho)},
\label{eq:jmax}
\end{equation}
where it can be appropriately rounded to be a natural number. Substituting $\jmax$ into $V$ using eq.~\eqref{eq:1_alpha_logjmax}, it gives 
\begin{equation}
\log V(\jmax) = \jmax (1+\alpha) - \log 2 \pi - \log \jmax - \frac12 \log \alpha
\label{eq:logVjmax}
\end{equation}
From this point, one can start evaluating $V(j)$ for $j =  \jmax+1, \jmax+2, \dots$, until a value $j_U$ is found, such that 
\[
    \frac{V(\jmax)}{V(j_U)} \geq e^{37}
\]
To compute that, 
one can either evaluate the difference between eqs.~\eqref{eq:log_Vj} and \eqref{eq:logVjmax} or compute, again using eq.~\eqref{eq:log_Vj} 
\begin{equation}
    \log V(\jmax) - \log V(j_U) \simeq (\jmax - j_U) C_W- \left( \jmax \left( 1 + \alpha\right) + 1 \right) \log \jmax + \left( j_U \left( 1 + \alpha\right) + 1 \right) \log j_U
    \label{eq:logVmax_logV}
\end{equation}
where $C_W = \log z + (1+\alpha) -\alpha \log \alpha $ does not depend on $j$; such expression can be computed efficiently. Similarly, one can identify $j_L$ such that 
\[
    \frac{V(\jmax)}{V(j_L)} \geq e^{37}
\]
going backwards to $\jmax-1, \jmax-2$, potentially stopping at $j=1$. 
The fact that $\log V$ is a convex function implies that the value of $V$ decreases exponentially on both sides.
Hence, the truncated part of the summation can be bounded with geometric sums:
\[
   \sum_{j=1}^{+\infty}V(j) - \sum_{j=j_L}^{j_U}V(j) \leq V\left(j_L - 1 \right) \frac{1-r_L^{j_L -1}}{1 - r_L} + V\left(j_U + 1 \right) \frac{1}{1 - r_U}
\]
where 
\[
r_L = \exp \left( \frac{\partial \log V(j)}{\partial j} \right) \biggr\rvert_{j = j_L -1} \quad \mathrm{and} \quad r_U = \exp \left( \frac{\partial \log V(j)}{\partial j} \right) \biggr\rvert_{j = j_U +1}
\]
The treshold $e^{-37}$ has been proposed since $e^{-37} \simeq 8 \cdot 10^{-17}$ guarantees an appropriate precision using 64-bit floating points. See \citet{Dunn_Smyth_2005} for more. In practice, we are interested in computing the log-likelihood \[
\log p(y \mid \mu, \phi, \rho) = \log A(y) \left[ \frac1\phi \left( y \frac{\mu^{1-\rho}}{1-\rho} - \frac{\mu^{2-\rho}}{2-\rho} \right) \right]
\]
which similarly requires to pass $A(y)$ trough the logarithm.
To efficiently implement the evaluation of $A$, it sufficient to refer to eqs.~\eqref{eq:sum_A_alpha},~\eqref{eq:log_Vj},~\eqref{eq:jmax},~\eqref{eq:logVjmax}, and~\eqref{eq:logVmax_logV}.


\begin{table*}[h]
{\scriptsize
\begin{tabular}{ll|rrrrr|rrrrr|rrrrr|rrrrr}
\toprule
& $\phi$ & \multicolumn{5}{c}{0.5}& \multicolumn{5}{c}{1.0} & \multicolumn{5}{c}{2.0} & \multicolumn{5}{c}{5.0} \\
 & $\rho$ & \rotatebox{90}{1.01} & \rotatebox{90}{1.1} & \rotatebox{90}{1.2} & \rotatebox{90}{1.3} & \rotatebox{90}{1.5} & \rotatebox{90}{1.01} & \rotatebox{90}{1.1} & \rotatebox{90}{1.2} & \rotatebox{90}{1.3} & \rotatebox{90}{1.5} & \rotatebox{90}{1.01} & \rotatebox{90}{1.1} & \rotatebox{90}{1.2} & \rotatebox{90}{1.3} & \rotatebox{90}{1.5} & \rotatebox{90}{1.01} & \rotatebox{90}{1.1} & \rotatebox{90}{1.2} & \rotatebox{90}{1.3} & \rotatebox{90}{1.5} \\
\midrule
 \greytabline \multirow{9}{*}{\rotatebox{90}{$y$}}  &  0.1 & 2 & 3 & 5 & 7 & 14 & 2 & 2 & 4 & 6 & 10 & 2 & 2 & 3 & 5 & 8 & 2 & 2 & 3 & 4 & 6 \\
& 0.2 & 2 & 4 & 6 & 9 & 15 & 2 & 3 & 5 & 7 & 11 & 2 & 2 & 4 & 5 & 9 & 2 & 2 & 3 & 4 & 7 \\
 \greytabline & 0.5 & 3 & 6 & 9 & 12 & 18 & 2 & 4 & 6 & 8 & 14 & 2 & 3 & 5 & 6 & 10 & 2 & 2 & 4 & 5 & 8 \\
& 0.8 & 4 & 7 & 11 & 14 & 21 & 2 & 5 & 8 & 10 & 15 & 2 & 3 & 5 & 7 & 11 & 2 & 2 & 4 & 5 & 8 \\
& \cellcolor{backcolour}1 & \cellcolor{backcolour}5 & \cellcolor{backcolour}9 & \cellcolor{backcolour}12 & \cellcolor{backcolour}14 & \cellcolor{backcolour}21 & \cellcolor{backcolour}3 & \cellcolor{backcolour}6 & \cellcolor{backcolour}9 & \cellcolor{backcolour}11 & \cellcolor{backcolour}16 & \cellcolor{backcolour}2 & \cellcolor{backcolour}4 & \cellcolor{backcolour}6 & \cellcolor{backcolour}8 & \cellcolor{backcolour}12 & \cellcolor{backcolour}2 & \cellcolor{backcolour}3 & \cellcolor{backcolour}4 & \cellcolor{backcolour}6 & \cellcolor{backcolour}9 \\
& 1.5 & 5 & 10 & 14 & 17 & 24 & 4 & 7 & 9 & 12 & 18 & 2 & 5 & 7 & 9 & 14 & 2 & 3 & 5 & 6 & 9 \\
\greytabline & 2 & 5 & 12 & 16 & 19 & 26 & 5 & 8 & 11 & 14 & 18 & 3 & 6 & 8 & 10 & 14 & 2 & 3 & 5 & 7 & 10 \\
& 5 & 7 & 19 & 24 & 27 & 33 & 6 & 12 & 16 & 18 & 23 & 5 & 9 & 11 & 13 & 17 & 2 & 5 & 7 & 8 & 11 \\
\greytabline & 10 & 9 & 25 & 32 & 36 & 40 & 7 & 18 & 21 & 24 & 28 & 6 & 12 & 14 & 16 & 20 & 4 & 7 & 9 & 11 & 14 \\
\bottomrule
\end{tabular}
}
\caption{$j_U - j_L + 1$ for different choices of $y$, $\phi$ and $\rho$. This is the amount of terms used in the approximation of the summation. It is an increasing function with respect to $y$ and $\rho$, decreasing with respect to $\phi$.}
\label{tab:summation_terms}
\end{table*}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/phi_rho_density.png}
    \caption{The values of $\phi$ and $\rho$ on the fitted TweedieGP on different data sets. Density curves have been estimated via kernel density estimation.}
    \label{fig:params_densities}
\end{figure}

Table~\ref{tab:summation_terms} shows the amount of terms in the approximation of the summation $A$ is generally affordable; indeed scaling the data by median demand, the value of $y$ typically lies between 0.5 and 2, while the density plots from Fig.~\ref{fig:params_densities} show that often $\phi \in (0.5, 5)$ with mode around 1, and $\rho \in (1, 1.4)$, with mode around $1.1$.

\subsection{Comparison with Tweedie loss}\label{sec:comparison_tweedie_loss}

In order to consider the crucial role of $A$ in fitting appropriately a Tweedie likelihood to the data, consider again the Tweedie loss, characterised by $\phi = A = 1$: in this simplified version, the implied likelihood in eq.~\eqref{eq:tw_loss_p}
is no longer a probability distribution, as it does not integrate up to $1$. If constrained to do so determining $c := c(\mu, \phi, \rho)$ such that \[
 \int_0^{+\infty} c \cdot  \exp \left( y \frac{\mu^{1-\rho}}{1-\rho} - \frac{\mu^{2-\rho}}{2-\rho}\right) dy = 1
\] including the normalization constant $c = \frac{\mu^{1-\rho}}{\rho -1}\exp \left( \frac{\mu^{2-\rho}}{2-\rho} \right)$ leads to 
\[
\tilde{p}(y \mid \mu, \phi) = \frac{\mu^{1-\rho}}{\rho -1} \exp \left( -y \frac{\mu^{1-\rho}}{\rho-1} \right),
\]
which is the density function of a negative exponential distribution with parameter $ \frac{\mu^{1-\rho}}{\rho-1}$. In this perspective, the remainder term of the loss can be interpreted as a joint prior distribution on $\mu$ and $\rho$ which prevents the mean from growing too large. 

\begin{figure}[!h]
    \centering
\includegraphics[width=1\linewidth]{figures/tweedie_comparison.png}
    \caption{In this plot, $\phi=1$. In black, the Tweedie likelihood; the unnormalized density and its normalized version are the dashed lines in red and cyan respectively. The regularizing factor in the unnormalized density makes the tails shorter.}
\label{fig:comparison_tweedie_expanded}
\end{figure}

However, the resulting loss is no longer bimodal, as shown in Fig.~\ref{fig:comparison_tweedie_expanded}, and empirical results show that having a zero-inflated behavior comes with the constraint of having short tails. For this reason its performance is not satisfactory on high quantiles, despite the use of the median demand scaling.


\section{Data and code availability}\label{sec:reproducibility}

All data used in the experiments is publicly available. 
\begin{itemize}
\item M5: this data set, released for the homonym competition \citep{makridakis2022m5}, contains data from some Walmart stores. 
\item OnlineRetail: this data set contains sales records of several items in an Online store. Preprocessing was required; to extract time series from tabular sales data. These are fairly long daily series. Those in which the first sale happens toward the end of the time span, although in fact smooth, may be incorrectly classified as intermittent. For this reason, time series entirely equal to zero for the first 200 timestamps were excluded.
%\item Syphilis: data on syphilis contagions in multiple countries, including aggregate data. 
\item Auto: short time series on automobile sales data; many of these, are not classifiable as intermittent. This data set, as well as OnlineRetail, is used by \citet{Türkmen_Januschowski_Wang_Cemgil_2021}.
\item Carparts: similarly to RAF, monthly time series on spare parts, but for cars. This data set is provided by \citet{expsmooth_2023}.
\item RAF: this data set has been among the most widely used in the literature on intermittent series (e.g. \citet{Syntetos_Boylan_Croston_2005, Nikolopoulos_Syntetos_Boylan_Petropoulos_Assimakopoulos_2011, snyder2012forecasting}). These monthly data represent the demand for spare parts for British Royal Air Force aircrafts.
\end{itemize}
The data sets are downloadable in the version used in the experiments by running first the file \texttt{datasets.R} in \textsf{R} and then the Python notebook \texttt{datasets.ipynb}. The files are located in the data folder at the GitHub page of the project, which will be released upon acceptance.

The code used for the experiments will be fully available at the GitHub link. The folder ``tutorial'' contains a simple usage example for our model.  
%Beyond we make available the code used for the experiments, both related to Tweedie Gp and the models with which it is compared. 

The implementation of our GP models is based on GPyTorch \citep{Gardner_Pleiss_Weinberger_Bindel_Wilson_2018}. Our contribution to public libraries is given by an implementation of the \texttt{Tweedie} class in the \texttt{distributions} module of PyTorch \citep{pytorch_2019}, and the classes \texttt{TweedieLikelihood} and \texttt{NegativeBinomialLikelihood} in the \texttt{likelihoods} module of GPyTorch. Upon acceptance we will contribute to those packages with pull requests. 

The GitHub project also contains \texttt{intermittentGP} class. It has methods to build, train and make predictions under the specification of the likelihood, the scaling and different traing hyperparameters, such as the number of epochs or the learning rate of the optimizer. 
%As soon as the anonymity requirement for this submission falls, we intend to contribute our code to the above packages.

%The GitHub link [removed for anonymous submission, available to reviewers in the zip folder] also contains 

%\begin{figure}[t]
    %\centering
    %\includegraphics[width=1\linewidth]{figures/coverage_all_datasets.png}
    %\caption{This is a reproduction of fig. 4, main paper, which includes all datasets and all baselines presented in the experiments.}
  %\label{fig:coverage_all_ds}
%\end{figure}
%Fig.~\ref{fig:coverage_all_ds} highlights what was commented on in subsection 4.3, main paper: almost all methods are prone to overcoverage, the more so as the data set is full of zeros.

%\section{Coverage}

%Fig.~\ref{fig:coverage_all_ds} provides more details on the comments in subsection 4.3, main paper: almost all methods are prone to overcoverage, the more so as the data set is full of zeros. The only exception is given by ADIDA \citep{Nikolopoulos_Syntetos_Boylan_Petropoulos_Assimakopoulos_2011}, which especially on higher quantiles, produces very shrinked predictive intervals. This is particularly noticeable for shorter time series, where the paucity of obtainable residuals does not allow conformal inference \citep{Angelopoulos_Bates_2022} to make calibrated estimates.

%For levels $q> 0.5$, the quantile loss \citep{Hyndman_2021} penalizes forecasts too close to zero, when positive demand is observed. This explains why on a dataset such as OnlineRetail, which is characterized by a large demand levels, TweedieGP reports good results despite its overcoverage. iETS \citep{Svetunkov_smooth}, on contrary, does undercoverage (related to the scarcity of observable demand), being heavily penalized.





\end{document}
