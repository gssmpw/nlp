% KEY TAKEAWAYS
% RQ1. 
% 1. Some use cases are more acceptable than others
% 2. Some use cases have bigger disagreement
% - good TODO correlation between stake level and standard deviation / confidence
% 3. TODO we need something with confidence
% RQ2: Reasoning effect
% 1. Weighing harms and benefits seemingly no effect on existence in aggregate - confirming related work or not?
% 2. Near significant effect of reasoning on usage acceptability in aggregate - positive
% Check paired t-test
\section{Findings}
\input{tables/use-case-effect}
% \input{figures/decisions_by_use_case}
\input{figures/judgments_by_use_case_violin}
\input{tables/use-case-effects-sd}
\subsection{RQ1. Use Case Factors}
\jimin{sub RQs: 1) main effect, 2) disagreement -- discuss graph (TODO)}
We carefully chose and varied the use cases and analyzed the decisions of existence and usage of the applications to understand the differences between a wide range of use cases. As shown in Table~\ref{table:use-case-effect}, we found a statistically significant relationship between the two categories with jobs having a negative coefficient, on both decisions about existence ($-0.29$, $p<.001$) and usage ($-0.18$, $p<.01$) for the decisions about usage, and personal use cases having a positive coefficient for both decisions as well ($0.60$, $p<.001$; $0.37$, $p<.001$). 

Within the labor replacement use cases, Telemarketer AI ($-0.22$, $p<.05$) and Elementary School Teacher AI ($-0.57$, $p<.001$) had negative coefficients that were statistically significant to decisions on existence. Surprisingly, only one use case, IT Support Specialist AI, had a positive coefficient ($0.49$, $p<.001$) for existence but for usage, both IT Support Specialist AI ($0.94$, $p<.001$) and Government Eligibility Interviewer AI ($0.32$, $p<.01$) had positive coefficients. Unlike the labor replacement use cases, personal use cases showed a decreasing pattern following the variation of EU AI risk levels. Both high risk and high / limited risk use cases, Digital Medical Advice AI ($-0.52$, $p<.001$; $-0.38$, $p<.001$) and Customized Lifestyle Coach AI ($-0.23$, $p<.05$; $-0.21$, $p<.05$), respectively, had negative coefficients for both existence and usage. Similarly, Flavorful Swaps AI, which reflects low risk level had positive coefficients for both existence and usage ($0.45$, $p<.001$; $0.29$, $p<.01$). 

\input{tables/reasoning-main-effects}
% \input{figures/reasoning_within_mean_graphs}
% \input{tables/reasoning-effect-interaction}
\input{tables/reasoning-effects-by-use-case}
\subsection{RQ2. Reasoning Effect}
\jimin{sub RQs: 1) main effect, 2) interaction - use case and reasoning effect. Note - discuss graph}
First, to understand the possible impact of weighing harms and benefits of the use case on participants' decisions, we analyzed the effect of participant's answers before and after the explicit harms and benefits reasoning (Study 2; see \S~\ref{sssec:survey-qs} for details). We first analyzed the main effect, without considering the variations due to the use cases, by fitting a linear mixed effects model, \texttt{Judgment ~ measurementTime + (1|Subject)}, where we create two models with \texttt{Judgment} defined as either standardized value of existence multiplied by confidence or usage multiplied by confidence. As shown in Table~\ref{table:reasoning-effects-main}, only usage judgment after weighing harms and benefits had a marginally significant positive coefficient ($0.06$, $p<.1$). Similarly, for \textbf{Exist$\times$Conf.}, there was no significant effect of \texttt{measurmentTime} on the outcome at the \( p < 0.05 \) level, \( F(1, 200.78) = 0.52, p = 0.4717 \) when analyzed with ANOVA. For \textbf{Usage$\times$Conf.}, there was a marginally significant effect of \texttt{measurementTime} on the outcome at the \( p < 0.1 \) level, \( F(1, 201.05) = 3.37, p = 0.068 \). Surprisingly, when considering the interaction between \texttt{useCase} and \texttt{measurementTime}, only Customized Lifestyle Coach AI had a significant negative coefficient ($-0.31, p<.05$) as shown in Table~\ref{tab:reasoning-effect-interaction}. 

\input{tables/reasoning-factors-questionnaires-effects}
\subsection{RQ3. Reasoning Factors}
\jimin{subRQs: 1) reasoning characteristics of participants and judgment, 2) use case x reasoning factors 
(Study 1), 3) reasoning factors (in written response) x questionnaire factors -- is the questionnaire predictive of the moral values used in the reasoning step?, 4) reasoning factors (in written response) x use case effects on judgment?, 5) reasoning factors (in written response) x reasoning type -- are there specific moral values that are more common in different types of reasoning?

TODO: separate into paragraphs}
We annotated and analyzed open-text reasoning responses as well as responses to questionnaires (MFQ, Toronto Empathy Questionnaire, Oxford Utilitarianism Scale) to understand both contextually specific and general reasoning patterns and their effects on judgments of AI use cases. 

\subsection{Questionnaires}
Interestingly, only Loyalty had a significant effect on both existence ($0.20, p<.001$) and usage ($0.20, p<.01$) as shown in Table~\ref{tab:reasoning-factors-questionnaires}. Moreover, Empathy had a positive and marginally significant effect for usage ($.09, p<.1$). However, Loyalty, as shown in Figure~\ref{fig:moral_values_proportions} and Table~\ref{tab:chi_squared_results}, does not appear as frequently in participants' open text responses compared to values such as Care and Fairness and is the only value that did not have a significant association with use cases. 
\jimin{TODO: correlations between questionnaire values -- e.g., is care for example highly correlated with empathy?}

\input{figures/reasoning_type_by_use_cases}
\input{tables/reasoning-factor-use-case-moral-chi}
\input{figures/moral_values_proportions}
\input{tables/reasoning-factors-moral-values-annotations}
\subsection{Open-text Reasoning Annotations}
\paragraph{Reasoning Type}
\paragraph{Moral Values}
\paragraph{Primary Concern}

\subsection{RQ4. Demographic Factors}