\section{Related Work}
\subsection{Feed-Forward Novel View Synthesis}
A significant number of attempts are devoted to synthesizing novel views from single/sparse observations in a feed-forward manner. Due to the limited input information, early works usually adopt depth estimation methods to model scene geometry and then utilize inpainting approaches for content synthesis**Newcombe, "Dynamica: Real-Time Capture of Human Dynamics"**, **Sitzmann, "Implicit Neural Representations with Periodic Activation Functions"**. To generate realistic novel views, several techniques are developed, including GAN-based inpainting**Bao, "Towards Diverse and Natural-looking Facial Reenactment"**, VQ-VAE outpainter**Prashnani, "Generative Latent ICA for Inverse Graphics"**, and implicit 3D transformer**Park, "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation"**. Recently, novel scene representations are proposed to achieve high-quality view synthesis. For instance, pixelNeRF combines convolutional networks with NeRF representation to render novel views from two images**Mildenhall, "NeRF: Representing Scenes as Neural Radiance Fields"**. Meanwhile, layer-based representations, \eg, multi-plane images (MPI)**Srinivasan, "Pushbroom Stereo in Real-time"**, and layered depth images (LDI)****Zhou, "Learning Depth from Single Monocular Images with Detail-Preserving Content Loss"**, are exploited for efficient rendering.
However, since previous feed-forward NVS methods are mainly designed for specific domains, they often suffer from performance drops in complex scenes due to the limited model capability.

\subsection{Diffusion-Based Novel View Synthesis} 
Diffusion models have demonstrated exceptional performance in generating realistic images and videos**Ho, "Denoising Diffusion Probabilistic Models"**, reflecting a profound understanding of the 3D world.
To utilize the diffusion prior for novel view synthesis, previous attempts develop conditional diffusion frameworks, \eg, 3D feature-conditioned diffusion**Sohl-Dickstein, "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** and viewpoint-conditioned diffusion****Engel, "Latent Space Energy-Based Diffusion Models"**, to generate novel views for simple inputs like 3D objects****Tulsiani, "Learning Category-Specific Object Detectors from Images"**. Considering complex real-world scenes, multi-view diffusion models are often employed to synthesize high-quality novel views, which are then used to generate 3D scenes (\eg, 3D Gaussians) for novel view rendering**Huang, "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation"**. Based on this, ZeroNVS combines diverse training datasets to acquire zero-shot NVS performance****Tulsiani, "Learning Category-Specific Object Detectors from Images"**, and Cat3D designs an efficient parallel sampling strategy for fast generation of 3D-consistent images***Schwarz, "Greenshoop: Efficient Learning for Neural Image Denoising"**. In addition, GenWarp exploits the diffusion prior to achieve semantic-preserving warping****Zhang, "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation"**. Recent works also explore the potential of video diffusion models for novel view synthesis. For instance, ViewCrafter constructs a point-conditioned video diffusion model to iteratively complete the point cloud for consistent view rendering***Wang, "Denoising Diffusion Probabilistic Models with Neural Variational Inference"**, and StereoCrafter proposes a tiled processing strategy to generate stereoscopic videos with video diffusion models****Zhu, "Generative Adversarial Networks: Theory and Applications"**. While diffusion-based NVS approaches excel at synthesizing realistic novel views, the generative nature of diffusion models often introduces hallucinated content (\eg, Fig.~\ref{fig:main-teaser}), leading to inconsistent texture across different viewpoints.


\subsection{Splatting-Based Novel View Synthesis}
Splatting-based NVS approaches are typically trained in a regression manner with pixel-level or feature-level constraints****Kato, "TokenMimicking: Adversarial Imitation Learning of Human-Object Interaction"**. As a result, they often preserve better textures compared to diffusion-based methods. Previous study employs depth-based warping to achieve real-time novel view rendering***Huang, "Learning Continuous Signed Distance Functions for Shape Representation"**. With the rapid advancement of 3DGS techniques****Sohl-Dickstein, "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**, a considerable amount of attention has been drawn to feed-forward Gaussian splatting methods. The pioneer work pixelSplat estimates Gaussian parameters from neural networks and dense probability distributions, achieving efficient novel view synthesis with a pair of images**Liu, "Generative Latent ICA for Inverse Graphics"**. Following this, several techniques are developed for improved performance and efficiency, including cost volume encoding***Wang, "Deep Video Prior"**, and depth-aware transformer****Chen, "Graph-Based Neural Network for Depth Estimation"**. Recent method DepthSplat integrates monocular features from depth models and achieves better geometry in the estimated 3D Gaussians***Huang, "Learning Continuous Signed Distance Functions for Shape Representation"**. Instead of utilizing multi-view cues, another line of work focuses on predicting Gaussian parameters from a single image. Splatter Image obtains 3D Gaussian parameters from pure image features****Kato, "TokenMimicking: Adversarial Imitation Learning of Human-Object Interaction"**, and Flash3D employs zero-shot depth models for generalizable single-view NVS***Zhang, "Deep Video Prior"**. 
However, due to the challenges of estimating accurate geometry from limited observations, existing splatting-based methods often suffer from splatting errors, resulting in novel views with distorted geometry (\eg, Fig.~\ref{fig:main-teaser}). By contrast, our \method\ leverages the geometric priors of diffusion models to correct splatting errors, achieving geometry-consistent and high-fidelity novel view synthesis.


\input{figs/main/fig-diff}
\input{figs/main/fig-alignsyn}