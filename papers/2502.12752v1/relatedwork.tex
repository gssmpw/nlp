\section{Related Work}
\subsection{Feed-Forward Novel View Synthesis}
A significant number of attempts are devoted to synthesizing novel views from single/sparse observations in a feed-forward manner. Due to the limited input information, early works usually adopt depth estimation methods to model scene geometry and then utilize inpainting approaches for content synthesis~\cite{wiles2020synsin,rombach2021geometry,rockwell2021pixelsynth}. To generate realistic novel views, several techniques are developed, including GAN-based inpainting~\cite{wiles2020synsin}, VQ-VAE outpainter~\cite{rockwell2021pixelsynth}, and implicit 3D transformer~\cite{rombach2021geometry}. Recently, novel scene representations are proposed to achieve high-quality view synthesis. For instance, pixelNeRF combines convolutional networks with NeRF representation to render novel views from two images~\cite{yu2021pixelnerf}. Meanwhile, layer-based representations, \eg, multi-plane images (MPI)~\cite{li2021mine,tucker2020svmpi,adampi,khan2023tiled} and layered depth images (LDI)~\cite{shih20203dphoto,jiang2023diffuse3d}, are exploited for efficient rendering.
However, since previous feed-forward NVS methods are mainly designed for specific domains, they often suffer from performance drops in complex scenes due to the limited model capability.

\subsection{Diffusion-Based Novel View Synthesis} 
Diffusion models have demonstrated exceptional performance in generating realistic images and videos~\cite{rombach2022stablediffusion,xing2025dynamicrafter}, reflecting a profound understanding of the 3D world.
To utilize the diffusion prior for novel view synthesis, previous attempts develop conditional diffusion frameworks, \eg, 3D feature-conditioned diffusion~\cite{chan2023gennvs} and viewpoint-conditioned diffusion~\cite{liu2023zero123}, to generate novel views for simple inputs like 3D objects~\cite{zheng2024free3d}. Considering complex real-world scenes, multi-view diffusion models are often employed to synthesize high-quality novel views, which are then used to generate 3D scenes (\eg, 3D Gaussians) for novel view rendering~\cite{liu2024reconx,wu2024reconfusion}. Based on this, ZeroNVS combines diverse training datasets to acquire zero-shot NVS performance~\cite{sargent2024zeronvs}, and Cat3D designs an efficient parallel sampling strategy for fast generation of 3D-consistent images~\cite{gao2024cat3d}. In addition, GenWarp exploits the diffusion prior to achieve semantic-preserving warping~\cite{seo2024genwarp}. Recent works also explore the potential of video diffusion models for novel view synthesis. For instance, ViewCrafter constructs a point-conditioned video diffusion model to iteratively complete the point cloud for consistent view rendering~\cite{yu2024viewcrafter}, and StereoCrafter proposes a tiled processing strategy to generate stereoscopic videos with video diffusion models~\cite{zhao2024stereocrafter}. While diffusion-based NVS approaches excel at synthesizing realistic novel views, the generative nature of diffusion models often introduces hallucinated content (\eg, Fig.~\ref{fig:main-teaser}), leading to inconsistent texture across different viewpoints.


\subsection{Splatting-Based Novel View Synthesis}
Splatting-based NVS approaches are typically trained in a regression manner with pixel-level or feature-level constraints~\cite{zhang2018lpips}. As a result, they often preserve better textures compared to diffusion-based methods. Previous study employs depth-based warping to achieve real-time novel view rendering~\cite{cao2022fwd}. With the rapid advancement of 3DGS techniques~\cite{kerbl3Dgs}, a considerable amount of attention has been drawn to feed-forward Gaussian splatting methods. The pioneer work pixelSplat estimates Gaussian parameters from neural networks and dense probability distributions, achieving efficient novel view synthesis with a pair of images~\cite{charatan2024pixelsplat}. Following this, several techniques are developed for improved performance and efficiency, including cost volume encoding~\cite{chen2025mvsplat} and depth-aware transformer~\cite{zhang2024transplat}. Recent method DepthSplat integrates monocular features from depth models and achieves better geometry in the estimated 3D Gaussians~\cite{xu2024depthsplat}. Instead of utilizing multi-view cues, another line of work focuses on predicting Gaussian parameters from a single image. Splatter Image obtains 3D Gaussian parameters from pure image features~\cite{szymanowicz2024splatterimage}, and Flash3D employs zero-shot depth models for generalizable single-view NVS~\cite{szymanowicz2024flash3d}. 
However, due to the challenges of estimating accurate geometry from limited observations, existing splatting-based methods often suffer from splatting errors, resulting in novel views with distorted geometry (\eg, Fig.~\ref{fig:main-teaser}). By contrast, our \method\ leverages the geometric priors of diffusion models to correct splatting errors, achieving geometry-consistent and high-fidelity novel view synthesis.


\input{figs/main/fig-diff}
\input{figs/main/fig-alignsyn}