\section{Experiments of Interactive Model}

%\subsection{Foundational Interactive model}
% \paragraph{\textbf{Evaluation Metrics}}$\ $
\subsection{Datasets} 
%To the best of our knowledge, there is no available dataset specifically focused on human-robot interaction (HRI). We transfer human-human interaction data to human-robot interaction data, enabling the robot to learn realistic and natural interactive behaviors with humans. Specifically, we introduce the Interx-HRI dataset, which is built on the open human-human interaction dataset Inter-X~\cite{xu2024inter} by retargeting one human pose to robot joint and pose representations. Interx-HRI covers a wide range of interaction types and category annotations. It provides a robust foundation for developing and evaluating human-robot interaction models. Additionally, we introduce Object-ROI dataset based on FullBodyManipulation~\cite{li2023object}, which focuses on robot-object interaction (ROI), further expanding the scope of interactive tasks that can be studied in the context of human-robot interactions.
\input{figures/vis-inter-model}
To the best of our knowledge, no existing dataset specifically focuses on human-robot interaction (HRI). To address this gap, we adapt human-human interaction data for human-robot interaction, enabling the robot to learn realistic and natural behaviors in interaction with humans. Specifically, we introduce the \textit{Inter-HRI} dataset, which extends the open human-human interaction dataset Inter-X~\cite{xu2024inter} by retargeting one of two interactive human poses to robot joint and pose representations. \textit{Inter-HRI} encompasses a wide variety of interaction types and category annotations, providing a solid foundation for the development and evaluation of human-robot interaction models. Additionally, we introduce the \textit{Inter-ROI} dataset, derived from FullBodyManipulation~\cite{li2023object}, which focuses on robot-object interaction (ROI), further broadening the range of interactive tasks that can be studied in the context of human-robot collaboration.


\noindent \textbf{Inter-X~\cite{xu2024inter}} is a large-scale human-human interaction dataset with over 8.1 million frames, 11,000 motion sequences, and 40 interaction categories, captured at 120 fps. It provides detailed annotations, including hand gestures, interaction order, and social relationships, for both perception and generation tasks.

\noindent \textbf{FullBodyManipulation~\cite{li2023object}}
provides paired object and human motion with a total duration of approximately
10 hours, captured at 120 fps. It provides the interactions between 17 subjects and 15 different objects with text descriptions.

\noindent\textbf{Inter-HRI \& Inter-ROI} 
We build Inter-HRI and Inter-ROI by re-targeting the angle-axis of one person in original datasets to that of specific robot types, such as Unitree H1~\cite{unitreeh1robot} and LEJU Kuavo~\cite{lejukuavorobot}, following PHC~\cite{luo2023perpetual}. It is important to note that Inter-HRI and Inter-ROI datasets include all the data from the original datasets while preserving the division between the training and testing sets.
Additionally, we downsample the processed data to a frequency of 10 frames per second to better adapt to the setting of our motion capture system.


\subsection{Evaluation Metrics}

\textbf{MPJPE} represents the mean per joint position error, computed using the average Euclidean distance between predicted and ground truth joint positions. \textbf{PA-MPJPE} represents the Procrustes-Aligned MPJPE,  differs in that it first aligns for rotation, translation, and scale, following the ~\cite{deitke2020robothor}. For measuring the contact quality between interactive targets, we employ contact metrics including \textbf{precision($C\_prec$), recall($C\_rec$), accuracy($C\_acc$)} and \textbf{F1 score($C\_F1$)} following the ~\cite{li2023object}.
\textbf{Traj} represents the trajectory error, computed using the average Euclidean distance between root joints of the predictions and ground truths, 
\textbf{Orie} represents the orientation error, computed using the angular difference of the speed of root joints between the predictions and ground truths. The two error metrics are following the ~\cite{dai2024motionlcmrealtimecontrollablemotion}.
\textbf{FID } calculates the Fr√©chet distance between the features extracted by the inception network. \textbf{R-score} measures the text and motion matching accuracy, computed by calculating the Euclidean distances between text embeddings and predicted motion embeddings and counting Top-3 accuracy of motion-to-text retrieval. \textbf{AITS} represents the average inference time to evaluate the real-time efficiency of models.

% \paragraph{\textbf{Comparative Experiment}}$\ $


\subsection{Baselines}
% We chose prediction-based and generation-based methods to validate the advantages of our Foundational Interactive model in terms of performance and real-time efficiency. 
To validate the performance and efficiency of our interactive model, we compare it against current state-of-the-art human-related interaction methods using both the Inter-HRI and Inter-ROI datasets. For a fair comparison, we adapt these methods to our specific setting and incorporate action-guided interaction by providing them with action text input through a CLIP-based text encoder. Additionally, we enable the prediction of the robot's future motion based on real-time human motion input, processed frame by frame.
JRT~\cite{xu2023joint} is a multi-person motion prediction method, which explicitly models the relations between joints and predicts for multiple humans simultaneously.
MRT~\cite{wang2021multi} processes individual motion and social interactions simultaneously by introducing a local-range encoder and a global-range encoder, and then aggregates the multi-range representations to the primary person for future motion prediction.
SAST~\cite{mueller2024massively} is a diffusion-based method, which models interactions among various targets such as multiple people, objects in a scene by a denoising diffusion model.
% However, during inference, it requires many denoising iterations, which results in poor real-time efficiency.
ReGenNet~\cite{xu2024regennet} proposes a reaction model that generates one person's motion based on the action and motion of another person, without using historical information like prediction-based methods.
% It analyzes the dynamic and detailed nature of human-human interactions. However, it does not incorporate a design that ensures the generated robot motion remains consistent with the previous robot motion.
InterDiff~\cite{xu2023inter} is a diffusion-based generative model that predicts the next step in human-object interaction tasks based on past motion. 
% Since it is not specifically designed for text-guided scenarios, we adapt its framework by incorporating a CLIP-based text encoder to extract textual features, enabling text-guided interaction prediction.

% cps original
% InterDiff~\cite{xu2023inter} is a diffusion-based generative model that predicts the next step in human-object interaction tasks based on past motion. Since it is not specifically designed for text-guided scenarios, we adapt its framework by incorporating a CLIP-based text encoder to extract textual features, enabling text-guided interaction prediction.

\subsection{Result Analysis}

\noindent\textbf{Results on the Human-robot Interaction}
As shown in Tab.~\ref{tab:hhi}, our foundational human-robot interactive model has achieved state-of-the-art performance across all evaluation metrics on two types of robots. 
Our model achieves the best performance in MPJPE and PA-MPJPE, demonstrating its effectiveness in local pose estimation.
It also outperforms others in trajectory (traj) and orientation (orie), validating its superior performance in global trajectory prediction.
Additionally, our model outperforms others in contact metrics, demonstrating its capacity to facilitate more precise interactions between the generated robot and the human.
Additionally, our model achieves the best FID and R-score, indicating that the distribution of predicted motion is close to that of the ground truth.
Finally, our model leads in AITS, indicating its high efficiency and application in real-time scenarios.
% Compared to JRT~\cite{xu2023joint} which explicitly models the relationship of joints between multiple person, our model demonstrates superior accuracy in joint position prediction, as evidenced by MPJPE and PA-MPJPE. 
% Furthermore, the contact metrics ($C\_prec$,$C\_rec$,
% $C\_acc$,$C\_F1$) show that our model generates more precise interactions between the robot and human.
% Compared to MRT~\cite{wang2021multi}, which uses transformers with different ranges to model local pose and global trajectory separately, our model outperforms in both pose and trajectory aspects as shown in MPJPE, PA-MPJPE, Trajectory Error, and Orientation Error.



\noindent\textbf{Results on the Robot-object Interaction}
To test our model's ability to predict the results of the robot-object interaction task, we further conduct the experiment on Inter-ROI. The result is illustrated in Tab.~\ref{tab:hoi}, our proposed method outperforms other methods on most metrics. Notably, while MRT achieves better performance on trajectory (traj) and orientation (orie) metrics, this is primarily because MRT tends to produce more translational results, which in turn leads to higher FID scores.  We also provide our visualization results are illustrated in Fig.~\ref{fig:visintermodel}.



\subsection{Ablation Study}
To further verify the effectiveness of our model, we conduct ablation studies to assess the impact of each loss design in Tab.~\ref{tab:ab-hoi} on HRI and ROI. Adding the $L_{\text{vel}}$ loss improves the trajectory alignment by encouraging smoother and more accurate robot motion results. For robot-object interaction, we have two specific losses. Incorporating the $L_{\text{interactive}}$ loss enhances the quality of interactions by ensuring close proximity between the robot's hands and the object. Introducing the object branch and $L_{\text{obj}}$ boosts the model's capability to predict object behavior, resulting in more realistic dynamic robot-object interactions.


% \paragraph{\textbf{Ablation Study}}$\ $
% Goal: the proposed method outperforms the SOTA human-human (robot) interactive model in terms of performance and inference speed.

% \noindent \TODO{SOTA methods???}

% \noindent \TODO{Metrics???}


% \subsection{Human-Object Interactive model}


% \begin{table*}[]
% \centering
% \caption{Ablation studies on FullBodyManipulation.}
% \resizebox{\linewidth}{!}{ 
% \begin{tabular}{c|ccccccccccc}
% \hline
% & PA-MPJPE & MPJPE & C\_prec & Contact recall & Acc   & F1     & percent & Traj & Orientation & FID & R-score \\ \hline
% basline          &          &       &                   &                &       &        &         &            &             &     &         \\ \hline
% w/o BPS          & 7.73     & 20.03 & 0.61              & 0.28           & 0.49  & 0.35   & 0.27    & 0.36      & 48.98       &     &         \\ \hline
% w/o Joint Loss       &          &       &                   &                &       &        &         &            &             &     &         \\ \hline
% w/o Interactive Loss &          &       &                   &                &       &        &         &            &             &     &         \\ \hline
% w/o Inter-Branch     & 4.64     & 20.4  & 0.67              & 0.29           & 0.51  & 0.37   & 0.28    & 0.34       & 46.7        &     &         \\ \hline
% ours             & 4.32     & 19.83 & 0.70               & 0.31          & 0.52 & 0.38 & 0.27   & 0.38       & 47.2        &     &         \\ \hline
% \end{tabular}
% }
% \end{table*}
% 

%\subsection{Motion Re-targeting}
%Goal: the simulated robot action performs better when equipped with PHC.

%\noindent \TODO{Metrics}

%\subsection{Fine-tuning}
%Goal: the fine-tuned model performs better than the primary version.

%\noindent \TODO{Metrics}

%\noindent \TODO{Table: online fine-tuning, offline fine-tuning}
