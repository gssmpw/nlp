\section{System Evaluation}
\label{sec:eval}

\input{figures/userstudyfig1}
\input{figures/userstudyfig2}

\subsection{Case Study}
In this section, we present examples to demonstrate the functionality of our system, i.e. human motion imitation, human-robot interaction, and robot-object interaction. For each functionality, we present the visualization from both the first-person perspective and the third-person perspective. The corresponding videos
are provided in the supplementary material.

\textbf{Human Motion Imitation}
\input{figures/imitation-fig}
Fig.~\ref{fig:Human Motion Imitation} presents the key frames of a LEJU Kuavo robot imitating real human motion. In this process, the participant is required to perform Tai-Chi actions, which consist of intricate and harmonious movements. To realize this imitation, our system utilizes motion capture to acquire human motion and invokes the inverse kinematic solver to map it onto the robot. Subsequently, the robot's action is optimized in the physical simulation module to ensure stable and reliable movements. The imitation of the particularly selected Tai-Chi actions effectively verifies the capacity of our system to stably and reliably transfer human behaviors onto humanoid robots. 

\textbf{Human-robot Interaction}
\input{figures/human-robot-interaction}
%Figure 5 demonstrates a complete interaction process in which a robot attempts to block a user's path.
%At the start of the interaction, the user approaches the robot, moving closer. In response, the robot raises and extends its arms to block the userâ€™s path. In the fifth set of images, the robot lowers its arms and returns to its initial position as the user steps backward. This indicates that the interactive model has stopped blocking the user. However, the robot repeats the blocking action when the user approaches again, illustrating the adaptive behavior of the interaction model.
Fig.~\ref{fig:Human-Robot-Interaction} showcases the complete process of a robot attempting to block a user's path during an interaction.
At the start of the interaction, the user walks toward the robot. As the user gets closer, the robot progressively raises and extends its arms to block the way. This forces the user to step back. Immediately after, the robot lowers its arms and returns to its initial position. When the user approaches again, the robot raises one of its arms once more to block the user's path, clearly demonstrating its attention to blocking the repeated approach. Driven by the interactive model and physical simulation module, the virtual robot automatically performs actions in response to human movement. The participant's reaction, i.e. stepping back once being blocked, strongly verifies our system's realism and real-time capability, since in a real-world scenario, a person would react similarly when their path is suddenly obstructed. This not only validates the accuracy of the motion capture but also highlights the effectiveness of the interactive model and physical simulation module. The module ensures that the robot's movements are not only functional in blocking the user but also natural-looking, mimicking the physical constraints and capabilities of a real-world robot agent.


%In the fifth set of images, as the user steps backward, the robot lowers its arms and goes back to its original position. This indicates that the interactive model has ceased blocking the user. Nevertheless, when the user approaches again, the robot repeats the blocking action, highlighting the adaptive behavior of the interaction model.


\textbf{Robot-object Interaction} 
\input{figures/human-object-interaction} 
Fig.~\ref{fig:Human-object Interaction} presents an example of the interaction between virtual robots and objects. In contrast to traditional training frameworks where humans are limited to observing data via monitors with fixed viewpoints, our system enables human users, equipped with AR glasses, to observe the entire process of virtual robots interacting with objects from any perspective within the real-world environment, as depicted in Fig.~\ref{fig:Human-object Interaction}.
Simultaneously, users can offer evaluative feedback on this interaction process. Our system is capable of leveraging the collected feedback information to further optimize the interactive model for the interaction between virtual robots and objects. Owing to the high-fidelity visualization provided by the AR module, our system empowers users to provide more reliable and accurate evaluations. This, in turn, furnishes a human-involved platform for the training and evaluation of the human-robot and robot-object interaction model.





\subsection{User Study}
%The objective of this user study is to validate the bidirectional learning capacity of our system. On one hand, we examine whether the participants can become increasingly familiar with the interaction with the robot during the continuous human-robot interaction process, thereby achieving more efficient skill improvement. On the other hand, we check whether the virtual robot's interaction capacity is enhanced with the recorded interaction data and collected user feedbacks.

The user study aims to validate the bidirectional learning capacity of SymbioSim for human-robot interactions. On the human side, we check whether participants can get more experience interacting with a virtual robot via the AR glasses and be more skilled in later interactions. On the robot side, we see if the ability of the interactive model can be enhanced by collected data and feedback from users. As these two aspects are interlinked, we meticulously design and conduct a series of user studies to comprehensively validate the effectiveness of our system.


In this human-robot interaction user study, we recruited two groups of participants, i.e. group A and group B, each with 10 participants. We selected five types of actions from daily human-human interactions to conduct the experiments. These five actions are handshake, high-five, pat on the back, block, and push.
Each participant is required to repeatedly conduct three rounds of interactions with the virtual robot for each action type. First, the participants in group A interact with the virtual robot driven by the directly trained foundational interactive model and the physics-based controller in the simulation environment. The interaction data and user ratings of these participants are collected during their interaction process for fine-tuning the interactive model. Then, the participants in group B follow the same instructions as group A, but employ the fine-tuned interactive model to drive the robot for interaction. The user feedback during the interaction process of group B is also collected.

After each interaction, participants are required to rate the interaction on a scale of 1-5 in three aspects, i.e. Q1: legibility of robot's action; Q2: satisfaction with the interaction; Q3: ease of the interaction. %Q1: whether the robot's actions match the corresponding action types; Q2: the satisfaction of the interaction experience; Q3: the naturalness and relaxation of the interaction process. 
We show the average user rating of Group A and Group B for each round of interactions in Fig.~\ref{fig:userstudy1} and Fig.~\ref{fig:userstudy2} respectively. %The highest ratings among the three rounds corresponding to the same action type are marked with bold font.


\textbf{SymbioSim Improves Human Performance} Fig.~\ref{fig:userstudy1} reports the user ratings w.r.t. the three aspects during the human-robot interactions of participants from group A, which employs the interactive model trained on our retargeted Inter-HRI dataset. %The "handshake" and "pat on back" actions exhibit a monotonous increase of the average rating on the three aspects. Similarly, the "high-five" and "block" actions reach the highest or nearly the highest ratings in the final round. Comparatively, since the "push" action received relatively high ratings from the very beginning, it didn't obtain the highest score in the third round. Nevertheless, after three rounds of interactions, the participants were still able to steadily and proficiently complete all kinds of interactions. 
In Fig.~\ref{fig:userstudy1}, the positions of curves exhibit a nearly monotonous increase in the order of blue, orange, and green, indicating an overall upward trend over the three rounds of interactions. It confirms that during the continuous interaction process, users become increasingly adapted to the robot's behaviors. As a result, they can more easily identify the action characteristics of the robot, efficiently and successfully complete the interaction with the robot, and maintain a relaxed interaction state throughout the process.

%We present the quantitative result of three aspects in five actions in Table~\ref{Table:UserStudy1}. 
%Overall, the ratings show a positive trend across iterations for most actions. 

%For example, the "Handshake" action started with an average rating of 2.9, 3.0 and 3.1 for the three aspects in round 1 and gradually increased to 4.4, 3.7 and 3.6 in the third iteration. Similar improvements were observed in the other actions, with participants consistently rating their experience higher after each iteration. The highest ratings were generally seen in the "Walk and block" action, with an increase from 4.0 to 4.2 in Iteration 3. Conversely, "High-five" had the lowest ratings in the first round, but also showed improvements over time. This suggests that participants became more comfortable and satisfied with their interactions with the virtual robot as the study progressed. Although, during three iterations, we use same interactive model for robot motion generation, the first question also shows an upward trend. With users having clearer understanding of robot interactive habit, user also giving higher quality motion as feedback through motion capture module, producing a higher quality robot motion sequence.



\textbf{SymbioSim Improves Robot Performance}
Similarly, Fig.~\ref{fig:userstudy2} presents the average user ratings of participants from group B. This group utilizes the interactive model that has been fine-tuned based on the collected user feedback from group A. We emphasize the statistics from two aspects.

Firstly, when comparing the ratings of round 1 for group A (in Fig.~\ref{fig:userstudy1}) and group B (in Fig.~\ref{fig:userstudy2}), it can be observed that the participants in group B generally commence with a higher rating. This verifies that the fine-tuned interactive model is able to predict more reasonable and reliable robot actions, thereby better adapting to human users.
Secondly, despite having a high initial rating, the ratings of group B still show an upward trend as the number of interaction rounds increases. This demonstrates the bidirectional continuous learning ability of our system, meaning that both the human and the robot's capabilities are continuously enhanced throughout the series of human-robot interactions.

%Table~\ref{Table:UserStudy2} records the average score from users interact with LEJU Kuavo robot driven by the finetuned interactive model. Similar to the result of the first user study, users tend to give higher rating in later iterations. Focusing on the result of first iteration, in the second user study, within 5 actions, "high-five", "push", and "pat on back" receive better feedback in all three aspect comparing with result from first iteration in the first user study. "Handshake" action has notable higher result in first and third aspect, despite of the slight decrease on the second aspect.

%In another words, both user studies prove the human side learning capacity of our system. The comparison between first iteration of two user studies, illustrates the capability of allowing robot or the interactive model to improve. The interactive model in the second user study is able to avoid the motion that cause lower grade since it learns from the feedback provided by previous users.

%\begin{figure*}[]
  %\includegraphics[width=0.8\linewidth]{images/example-comparison.png}
%  \caption{Example Comparison.}
%\end{figure*}



%\begin{figure*}[]
  %\includegraphics[width=0.8\li%newidth]{images/smaeModelexample.png}
%  \caption{Same model user study}
%\end{figure*}


%\begin{figure*}[]
  %\includegraphics[width=0.8\linewidth]{images/Fine-tune example.png}
 % \caption{User Study with fine-tuning}
%\end{figure*}



