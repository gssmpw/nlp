\section{Related Work}
\label{sec:related}

\subsection{Robotic Simulators}
Simulators are a cornerstone of modern robotic research, offering significant advantages in the design, testing, and development of robotic systems due to their cost-effectiveness, safety, rapid iteration capabilities, and access to a wide range of scenarios. Physical simulators, such as PyBullet~\cite{coumans2021}, MuJoCo~\cite{todorov2012mujoco}, and Isaac Gym~\cite{makoviychuk2021isaac}, primarily focus on simulating the motion control, kinematics, dynamics, and force interactions of robots. These simulators serve as the foundation for modeling real-world robot interactions, which are critical for tasks like reinforcement learning, manipulation, and control.
To further advance the study of embodied intelligence in 3D environments, a growing number of robotic simulators have emerged, including the AI2-THOR series~\cite{kolve2017ai2, deitke2020robothor, ehsani2021manipulathor}, iGibson~\cite{shen2021igibson, li2021igibson, li2023behavior}, Habitat~\cite{savva2019habitat, szot2021habitat, puig2023habitat}, RoboCasa~\cite{nasiriany2024robocasa}, and GRUtopia~\cite{wang2024grutopia}, among others. These platforms simulate environments that more closely mirror real-world conditions, incorporating interactive elements, task-based challenges, and sensory feedback—crucial for tasks such as robotic perception~\cite{wang2024embodiedscan}, navigation~\cite{vuong2024habicrowd}, and manipulation~\cite{miller2004graspit}. Robotic simulators provide realistic environmental observations and immediate physical feedback to robot agents, which is vital for both supervised and reinforcement learning processes.
However, the simulators mentioned above do not support human-robot interaction simulations, which are essential for applications involving service robots and humanoid robots. Furthermore, these platforms do not facilitate direct human participation in the simulation process, thus preventing the realization of closed-loop simulations with humans in the loop. As a result, current simulations lack authentic human feedback, which impedes the optimization of models to better align with human preferences and the customization of solutions to meet individual needs.

%Physical simulation serves as the bedrock in robotics for modeling real-world robot interactions. A multitude of robot manipulation and interaction studies assess their methodologies within simulation environments like Pybullet, MoJoCo, Isaac Gym, and others. Nevertheless, in contrast to offline simulation evaluations, embodied intelligence is characterized by learning robot policies through direct engagement with the real, dynamic environment. This has spurred a series of recent efforts centered around real-time and realistic physical simulators.
%Numerous embodied simulators, such as AI2THOR series~\cite{kolve2017ai2, deitke2020robothor, ehsani2021manipulathor}, iGibson~\cite{shen2021igibson, li2021igibson, li2023behavior}, Habitat~\cite{savva2019habitat, szot2021habitat, puig2023habitat}, Robocasa~\cite{nasiriany2024robocasa}, etc., have been developed for robot-environment interaction tasks. These simulators integrate realistic indoor scenes and real-time interaction simulations to replicate the real environment. They also support a diverse set of robots and tasks, including visual navigation~\cite{vuong2024habicrowd} and object manipulation~\cite{miller2004graspit}. For these tasks, the embodied simulators supply realistic environmental observations and immediate physical simulation feedback to robot agents, which is essential for their supervised and reinforcement learning processes [].

\subsection{Human-robot Interaction}
As robots become increasingly integrated into everyday life and various industries, improving how humans and robots interact, collaborate, and coexist is critical to enhancing the effectiveness, safety, and social acceptance of robotic systems~\cite{cakmak2011human}. Recent studies have explored the use of virtual reality (VR) or augmented reality (AR) devices~\cite{chen2024arcap, yang2024arcade, park2024dexhub, nechyporenko2024armada} and developed teleoperation systems~\cite{wonsick2021human,mosbach2022accelerating, iyer2024open, cheng2024open} to capture high-quality robot data. However, these approaches primarily position humans as instructors in robot-environment interactions—either by providing task instructions or performing demonstrations for the robots—rather than focusing on true human-robot interaction. To facilitate human-robot collaboration in shared workspaces, HumanTHOR~\cite{HumanTHOR} introduces an embodied approach, enabling humans to act within the simulation environment via VR devices to support real-time collaboration with robots. Similarly, a recent study~\cite{liu2024collabsphere} uses a motion capture system to enable more realistic collaboration between 3D digital humans and virtual robots. While these efforts begin to address the importance of human-in-the-loop simulation, they still rely on VR to drive avatars, which limits human participants from experiencing authentic, direct interaction with the robot.

%Despite the importance of realistic and real-time physical simulation, high-quality interaction demonstration data is equally vital for enhancing the skilled performance of robots with embodied intelligence. Recent research utilizes VR or AR devices~\cite{chen2024arcap, yang2024arcade, park2024dexhub, nechyporenko2024armada} and develops teleoperation systems~\cite{mosbach2022accelerating, iyer2024open, cheng2024open}. These systems enable human users to visually sense the robot's environment through VR devices and execute skilled tasks. The human operations are then re-targeted onto robot arms and dexterous hands, serving as collected demonstration data for supervised or imitation learning [].
%All of the aforementioned related works involve humans as instructors in robot-environment interactions, either by issuing task instructions or performing demonstration operations for the robots. However, for a broader spectrum of interaction tasks, such as human-robot interaction and collaboration, human-in-the-loop simulation is further necessary to promote embodied interactions. collaboration...

\subsection{Interactive Motion Generation} 
% Human motion generation~\cite{zhang2022motiondiffuse, tevet2022motionclip, lucas2022posegpt, xie2023omnicontrol} aims to generate a person's future motion based on their own past motion,and interactive motion generation extends this field by incorporating some static interactive targets like walls or desks in the environment~\cite{wang2022humanise, cong2024laserhuman, wang2024move}, or dynamic interactive targets like moving people or objects.
Human behavior is inherently uncertain and highly dynamic, making human-robot interaction and collaboration particularly challenging. Traditional motion planning algorithms~\cite{dai2014whole} struggle to achieve real-time optimization in high-degree-of-freedom scenarios and RL-based methods~\cite{hwangbo2019learning} lack the generalization capability for complex behaviors. Our work leverages deep learning-based generative methods and human-human interaction data to learn human-robot interaction behaviors, enabling the robot to generate realistic and natural actions by observing the behavior of human collaborators.
Previous research on human-interactive generation has primarily focused on static environment~\cite{jiang2022chairs,kulkarni2024nifty,jiang2023full}, often neglecting the dynamic aspects of interactions.
With the emergence of several HOI and HHI datasets~\cite{li2023object,bhatnagar2022behave,liang2024intergen}, some works~\cite{li2023object,peng2023hoi,xu2023interdiff,li2023controllable,xu2024regennet} have attempted to address dynamic object or human-human interactions.
These approaches either generate human state condition on the entire given interactive subject sequence~\cite{li2023object,xu2024regennet} or predefined trajectory~\cite{li2023controllable}, or jointly generate both object and human states based on textual descriptions~\cite{peng2023hoi,diller2024cg,wu2024thor,xu2023interdiff}. 
However, for natural interaction, the robot is required to generate responsive motions that correspond to dynamic interactions in real time. 
%In such scenarios, generating human motion becomes particularly challenging, as the movement of dynamic interactive targets may or may not be influenced by the outcomes of the human motion generation process.
InterDiff~\cite{xu2023interdiff} %and AutoHOI
predict the next step in interactions while suffering from slow processing speeds, limiting their practicality for real-time scenarios. 
To address these limitations, we propose a new pipeline for real-time, online robot motion generation that re-targets human motions to robot control. Our approach enables the robot to dynamically respond to rapid changes in interactive behavior, ensuring responsive and adaptive interaction in dynamic environments.

% For instance, the model is required to simultaneously generate for both sites in the interactive multi-person prediction task~\cite{}, 
% while only generate for one site in human-object interaction task to make objects move in a predefined trajectory by the human's operation.
\subsection{Data-driven Physically Simulated Character Animation} 
Physically simulated character animation has garnered significant attention, particularly in the realm of data-driven manner~\cite{peng2018deepmimic, fussell2021supertrack, bergamin2019drecon, peng2022ase, won2020scalable}. For a more in-depth review, we direct interested readers to~\cite{kwiatkowski2022survey}. Specifically, DeepMimic~\cite{peng2018deepmimic} employs Deep Reinforcement Learning~(DRL) to enable physically simulated characters to accurately track kinematic motion clips. To enhance tracking performance, PHC~\cite{luo2023perpetual} introduces a method that dynamically expands network capacity to address increasingly complex motion sequences. Moreover, Generative Adversarial Imitation Learning~(GAIL) has been explored for acquiring reusable motion skills~\cite{peng2021amp, bae2023pmp, peng2022ase, dou2023c, tessler2023calm}, investigating diverse underlying skill distributions such as VAE~\cite{yao2022controlvae} and VQVAE~\cite{yao2024moconvq, zhu2023neural}. Recently, diffusion policy~\cite{chi2023diffusion} has also been applied to character animation~\cite{truong2024pdp}. Significant advancements have been made in producing physically simulated human-human~\cite{won2021control, zhang2023simulation} and human-scene interactions~\cite{hassan2023synthesizing, wang2024sims, pan2024synthesizing, xiao2023unified}. In this paper, we employ PHC~\cite{luo2023perpetual} to train a controller for physical agents in the physical simulation environment to achieve simulated robot control. Unlike typical motion tracking methods~\cite{luo2023perpetual,peng2018deepmimic,fussell2021supertrack}, where the controller uses single-direction information, e.g., kinematic reference motions, for physical character control, we focus on building a human-in-the-loop simulation platform for bidirectional continuing learning in human-robot interaction.

% HOI Original Part -cps
%\subsection{Human-object Interaction} 
% Previous research on human-interactive generation has primarily focused on static objects~\cite{jiang2022chairs,kulkarni2024nifty,jiang2023full}, often neglecting the dynamic aspects of interactions.  With the emergence of several HOI and HHI datasets~\cite{li2023object,bhatnagar2022behave},some works~\cite{li2023object,peng2023hoi,xu2023interdiff,li2023controllable,xu2024regennet} have attempted to address dynamic object interactions, and recent studies have further explored whole-body dynamic interaction generation. These approaches either generate human state condition on the entire given object sequence~\cite{li2023object} or trajectory~\cite{li2023controllable}, or jointly generate both object and human states based on textual descriptions~\cite{peng2023hoi,diller2024cg,wu2024thor,xu2023interdiff}. 




\input{figures/system-comp}


