\section{System Framework}


%Our system's objective is to develop a human-robot interaction simulator for the study of embodied intelligence. It is characterized by its ability to provide real-time and authentic feedback during the simulation process. For human-robot interaction, this feedback enables bidirectional continuing learning. On one hand, it allows humans to adapt to interacting with robots and improves their ability to engage with them. On the other hand, it continuously enhances the intelligent interaction capabilities of robots, making their performance more natural and realistic.

%To facilitate real-time and authentic feedback for human-robot interactions, the key to our system lies in the \emph{human-in-the-loop simulation platform}, which is manifested in three aspects:

%Firstly, our system simulates human-robot interaction. As different robots have diverse physical structures and degrees of freedom, human actions can be retargeted to robots in the simulation environment, teaching robots to interact with humans.

%Secondly, our system enables real-time and realistic human-robot interaction. Specifically, human motions are captured in real time and fed into a foundational interactive model to generate corresponding robot actions, which are adjusted in a physical simulation environment. Augmented reality technology integrates the dynamic virtual robot into the real environment, to provide natural and realistic human-robot interaction experiences.

%Thirdly, our system collects instant human feedback during interactions to further enhance robot interaction capabilities. When interacting with virtual robots, human ratings, speeches, and facial expressions are recorded. This feedback is used to organize interaction data and fine-tune the foundational interactive model.

To facilitate human-in-the-loop robotic simulation for bidirectional continuous learning in human-robot interaction, SymbioSim consists of six key modules, as illustrated in Fig.~\ref{fig:system}. In Section~\ref{sec:components}, we first provide a detailed implementation of each component, followed by a discussion in Section~\ref{sec:capabilities} on how these components ensure the system's capabilities in terms of realism, real-time performance, and adaptability.

\subsection{System Components}
\label{sec:components}

\textbf{Motion Capture} For pose-level human-robot interaction in 3D space, accurate and efficient human motion capture is necessary. Because LiDAR can provide precise depth information and is not affected by light conditions. We employ a LiDAR device~\cite{OusterLidar2022} to scan 3D point clouds and estimate human motions in the scene. The human motion is represented as the pose parameters of the SMPL model~\cite{SMPL}, which describes the global transformation and joint angles of the human body. To achieve this, we utilize LiveHPS++~\cite{ren2025livehps++}, a portable, highly flexible, and easy-to-configure system designed for capturing human motion across a wide range of activities. It is capable of processing the scanned point clouds in real-time, providing high-quality motion data for further analysis or integration. 
The motion capture system captures a frame of human motion every 0.1 second.

\textbf{Interactive Model} The interactive model is responsible for providing realistic robot actions based on the captured human motion. The human motion is first converted into joint positions of the human body skeleton with a forward kinematic algorithm. Then, for every frame, the interactive model takes historical and current human poses and robot poses, as well as the user's action command such as "handshake" as input, and outputs a sequence of the predicted robot joint skeletons, which are then converted to the robot pose representation via an inverse kinematics solver. The technical details of the interactive model are described in Section~\ref{sec:interactivemodel}. 
It is worth noting that we provide this foundational model to evaluate the effectiveness of SymbioSim, and it can be easily replaced with any other human-robot interaction models that users expect to test. In this paper, we also explore an alternative approach by replacing the interactive model with a retargeting model, which maps captured human motion to corresponding robot poses, allowing the virtual robot to mimic human movements. 
This module operates at a stable frequency, synchronized with the motion capture system.

\textbf{Physical Simulation} 
% We employ the physics-based constrained optimization approach presented in~\cite{luo2023perpetual} to compute the robot motion sequences within a physical simulation environment.
We employ the deep reinforcement learning technique presented in~\cite{luo2023perpetual} to control simulated robots in a physical simulation environment, i.e., IssacGym~\cite{makoviychuk2021isaac}. The objective is to achieve maximum alignment between predicted robot poses with the real robot poses (also as known as motion tracking for physical agents) while maintaining physical plausibility. It is able to produce real-time and high-fidelity robot actions in the physical simulation environment, enhancing the authentic interaction experience and reducing the sim-real gaps. Specifically, the physics-based humanoid controller samples the predicted robot poses and outputs the simulated action at a frequency of 50FPS to ensure the fluency and continuity of robot movements for a natural interaction experience.

\textbf{AR System} The AR system aims to provide a realistic visualization of the interaction between the real human and virtual robot. To facilitate a high-fidelity interaction experience for the human user, we employ an AR device, i.e. PICO4 Ultra~\cite{PicoInteractive2022}, to visualize virtual robots from the first-person perspective. It communicates with the physical simulation module through TCP protocol to update robot poses, which are seamlessly integrated into the real environment background. In addition, we also provide the visualization of the third-person perspective to show the panoramic view of the interaction between the real human and the virtual robot. To achieve this, we set up an RGBD camera and calibrated its intrinsic and extrinsic parameters. After that, we render the virtual robot from the calibrated viewpoint and fuse it with the image captured by the camera based on their depth maps.
 
\textbf{Human Feedback} 
We collect user feedback throughout their interactions with the virtual robot to improve the system's adaptability. Users can provide feedback in various forms, such as direct ratings or detailed language descriptions of their interaction experience. Additionally, implicit feedback can be captured through users' actions, which may reflect their comfort level, engagement, or preferences during the interaction. The collected feedback, combined with the recorded human-robot interaction data, forms comprehensive human-in-the-loop realistic data for fine-tuning the foundational interactive model.

\textbf{Model Fine-tuning}
Based on user feedback, we classify the interaction data into positive and negative samples. Positive samples represent behaviors or interaction methods that are well-received by users, while negative samples correspond to interactions that users find unsatisfactory or unnatural. Using these samples, the system leverages a supervised learning paradigm to continuously refine and adjust the interaction model by fine-tuning the model on positive samples. This allows the robot to better align with user needs and preferences. Through this closed-loop feedback mechanism, the robot progressively improves its interaction quality, enabling more natural and efficient collaboration with humans.



\subsection{System Capabilities}
\label{sec:capabilities}
%\manyi{Design Principle as subtitle?}
Aiming to achieve human-robot symbiosis, SymbioSim is the first human-robot interaction closed-loop simulation platform, seamlessly integrating both virtual and real environments. It provides a robust foundation for the rapid and safe development, validation, and optimization of human-robot interaction algorithms. SymbioSim benefits the learning and improvement for both humans and robots. On one hand, through authentic interactive scenarios, humans can gradually understand the robot’s intentions, develop effective collaboration and getting along strategies, and ultimately integrate the robot more naturally into daily life. On the other hand, SymbioSim facilitates expansion to diverse testing scenarios while continuously collecting human feedback, enabling real-time adjustments and evolution of the robot's models. This allows for swift iterations, aligning robot's behavior with human preferences and habits. In the end, SymbioSim supports mutual adaptation and continuous learning between humans and robots, achieving the goal of human-robot symbiosis in a safe, efficient, and cost-effective manner. To enhance user experience and reduce the gap between simulation and reality, the advantages of SymbioSim system are reflected in three aspects: realism, real-time capability, and adaptability.


\textbf{Realism}
By leveraging AR technology, we seamlessly integrate virtual robots into real-world environments, providing users with an authentic interaction experience. Furthermore, we build upon human-human interaction data to transfer human behaviors and habits onto humanoid robots, enabling their actions to transcend the traditional boundaries between humans and machines. This fosters more natural and intuitive human-robot interactions, enhancing the sense of realism. Additionally, we apply physics-based constrained optimization to ensure that the robot’s behaviors remain consistent with its inherent physical constraints. This approach effectively bridges the gap between simulation and reality, enabling the effective deployment of simulation results onto real-world robots.

\textbf{Real-time Capability}
Real-time performance is essential for establishing a closed-loop feedback system between humans and robots. To achieve this, we utilize cutting-edge LiDAR-based 3D motion capture technology, enabling real-time perception for the robot. Moreover, we have developed an innovative interactive motion generation model that facilitates real-time regulation and control of the robot’s actions. The real-time execution of core algorithms, coupled with efficient inter-module data transmission, ensures the overall system’s responsiveness and reliability in real-time operation.


\textbf{Adaptability} SymbioSim not only facilitates the testing of existing models but also integrates user evaluations and feedback. These insights are automatically collected to fine-tune and optimize the model, allowing the robot to become progressively more adaptive and efficient. Simultaneously, users can make adaptive adjustments throughout their interaction, resulting in an increasingly refined experience.
