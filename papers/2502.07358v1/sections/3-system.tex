\section{System Framework}
\label{sec:Overview}
\manyi{Structure to be refined for this section. Feel free to make any edit.}
%In this section, we first introduce the utility and key characteristics of the proposed SymbioSim system. Then we describe the implementation of the components of our system.

\subsection{Overview}
Our SymbioSim system facilitates human-in-the-loop simulation for the bidirectional continuing learning of human-robot interaction. With this system, human users can visualize the real-time actions of virtual robots through AR devices and engage in a variety of interactions with them. %During the immersive human-robot interactions with our system, the human user becomes more familiar with robot behaviors and skilled in human-robot collaboration. On the other hand, the interactions are collected to continuously enhance the robot's capability to respond to human actions.
Through these interactions, users become more proficient in understanding robot behaviors and human-robot collaboration. Meanwhile, the system collects interaction data to continuously enhance robot's adaptation to human actions, thereby optimizing the interaction experience. This bidirectional continuous learning, enabled by our system, advances human-robot partnerships through a variety of interaction practices, as shown in Figure~\ref{fig:teaser}.

Our system includes the human-side and robot-side components, as shown in Figure~\ref{fig:system}. The human side is responsible for providing interfaces to the human user, including three modules: human motion capture, immersive AR visualization, as well as human feedback data collection. First, the \emph{motion capture} module leverages a LiDAR device to scan point clouds and extract the movements of the human user. The human movement is delivered to the robot-side components, which return the real-time response actions of the virtual robot. Second, the \emph{AR visualization} module receives the dynamic motion of the virtual robot and renders it in the real environment through an AR device to provide an immersive experience for the user. Third, the \emph{feedback collection} module records user's interactive experience, expressed via direct rating or speeches, which can be used to further improve robot's capability via continuous learning.

The robot-side components are developed to provide real-time and reliable robot actions to facilitate human-robot interactions. It also contains three parts: First, given the captured human motions and interactive category, the \emph{foundational interactive model} generates the corresponding poses of the virtual robot in time. We have trained different models for each specific robot model. Second, the \emph{robot simulation} module takes the predicted robot poses as the target, and optimizes to solve the joint parameters of the selected robot model in the physical simulation environment. The simulation results are considered as the virtual robot's movement to be visualized to the user. Third, the \emph{model optimization} leverages the collected interaction data and user feedback to fine-tune the foundational interactive model to achieve better robot performance. Through sophisticated design and seamless integration, the human site and robot side operate in coordination, allowing both to adapt to and learn from each other, ultimately achieving a harmonious symbiosis between humans and robots.


%Our SymbioSim system facilitates human-in-the-loop simulation for the bidirectional continuing learning of human-robot interaction, such as hand-shaking, high-five, hand-over objects, etc. With this system, human users can, through AR devices, visualize the real-time dynamic actions of virtual robots and engage in a variety of interactions with them. The system captures human body motions and hand poses, which serve as signals to generate the corresponding reactions of virtual robots. Simultaneously, the system employs speech and expression recognition to collect user feedback during the interaction process. This feedback is then utilized to enhance the virtual robot's interaction capabilities. The interaction between a real person and a virtual robot is depicted on the left of Figure~\ref{fig:system}.

%To enable natural and reliable human-robot interactions, the challenge of our system is to provide an immersive interaction experience and real-time virtual robot responses for the human user. As highlighted in the right of Figure~\ref{fig:system}, to achieve this goal, our system presents two feature characteristics: 
%\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt, leftmargin=0.5cm]
%    \item On the human side, we offer immersive human perception and contactless motion capture. Specifically, we seamlessly integrate and render the virtual robot within the real-world environment in a photorealistic fashion through AR devices. The robot model attributes, such as size, materials, textures, etc., are carefully designed to ensure the user's immersive experience. During the interaction with the virtual robot, we leverage the LiDAR device to capture human actions, eliminating the need for wearing tight and heavy sensors that could impede the naturalness of human motions. During the process, the user can express their satisfaction and remark on the deficiency of the underlying interaction through facial expression and speech, which are recognized and collected as user feedback for further improvement. %Human users convey their intended actions and interaction feedback via hand poses instead of relying on AR handles, thus liberating their hands for more natural interactions.
%  \item On the robot side, we develop real-time robot interaction and simulation. We propose an interactive model capable of predicting the real-time reactive actions of the virtual robot. It takes the captured human motion as input and outputs instant robot poses, which are then re-targeted to different humanoid robots, such as the Unitree H1 robot and LEJU Kuavo robot, through instant physical simulation for feasible and reliable human-robot interaction. %Moreover, to ensure real-time and stable physical interaction simulation, we have developed an asynchronous dual-loop framework. In this framework, the interactive model and retargeting simulation operate independently, communicating through a shared buffer. This framework empowers our system to serve as a flexible platform for integrating diverse interactive models with varying inference speed and input/output windows. 
%  Additionally, these models can be continuously optimized with the collected human feedback to improve the interaction performance.
%\end{itemize}

\subsection{Human-site Components}
The principle of the human-side components of our system is to guarantee a natural and immersive experience for the human user during the human-robot interaction with our system. %Below we introduce the implementation of each component.


\noindent \textbf{Motion Capture.} The motion capture module employs a LiDAR device to scan 3D point clouds and estimate human motion in the form of pose parameters for the SMPL model. To achieve this, we utilize LiveHPS++, a portable, highly flexible, and easy-to-configure system designed for capturing human motion across a wide range of activities. It is capable of processing the scanned point clouds in real-time, providing high-quality motion data for further analysis or integration into virtual environments.

\noindent \textbf{AR Visualization.} We employ PICO4 Ultra as our AR device to visualize virtual robots. Initially, the articulated robot model is automatically loaded in front of the human user. Once the initialization is completed, the application anchors itself to the fixed world coordinate system, enabling the user to move freely and interact with the virtual robot. It then communicates with the robot simulation component on the robot side, through the TCP protocol, to update the robot's intrinsic and extrinsic poses. 
 
\noindent \textbf{Feedback Collection.} 
We collect user feedback during their interaction with the virtual robot. On one hand, the user can provide direct ratings of the interactions from different aspects. On the other hand, our system supports speech recognition to reflect satisfaction. The collected feedback, together with the recorded human-robot interactions, forms the human-in-the-loop simulation data to fine-tune the foundational interactive model.

%A natural human-robot interaction often engages the entire human body and hands. As a result, relying on AR handles for interface control proves inconvenient. To address this, we utilize hand-pose recognition to enable users to interact with the application. For instance, users can perform specific hand operations such as clicking a button to initialize the robot-side software. Additionally, they can select virtual objects from a set of candidates shown on the visualization interface for human-robot collaborative tasks through nuanced hand-pose-based operations.

\subsection{Robot-site Components}
The robot-side components are designed to ensure the real-time human-in-the-loop simulation of human-robot interactions. In contrast to offline solutions, we have developed asynchronous loops for both the foundational interactive models and the robot simulation. These loops communicate via a shared buffer to maintain a stable and smooth workflow for each component. %The foundational interactive model are fine-tuned after several times of interactions.

%We implement an asynchronous dual-loop framework to support different interactive models with varying inference speeds and input/output windows. As shown in Figure, this framework maintains two independent loops for the interactive model and robot retargeting respectively, which communicate through a shared buffer to update the predicted robot actions.

\noindent \textbf{Foundational Interactive Model.} After initialization, the foundational interactive model is continuously invoked at a stable frequency, e.g. 10FPS, to update the predicted robot poses. Specifically, it retrieves the current human pose from the motion capture component, and this pose, along with the maintained historical human poses, serves as input to predict the sequence of future robot poses (details in Section~\ref{sec:interactivemodel}). The shared buffer is refreshed with these predicted robot poses and maintains the associated timestamps.

\noindent \textbf{Robot Motion Simulation.} We employ the reinforcement learning approach presented in~\cite{luo2023perpetual} to compute the robot's interaction within a physical simulation environment. The objective is to achieve maximum alignment with the robot pose predicted by the interactive model while ensuring physical feasibility. Specifically, the simulator retrieves and interpolates the predicted robot poses from the shared buffer at a specific frequency, for example, 50FPS, and then estimates the robot interaction, which is subsequently sent for the AR visualization.


%In this paper, we employ the AMASS dataset~\cite{mahmood2019amass}, the largest publicly available motion capture repository, to train our humanoid robot models. Given the topological differences between SMPL and humanoid robots such as H1 and LEJU, we begin by fitting SMPL body models to the captured motion data via Inverse Kinematics~(IK). Specifically, our optimization utilizes forward kinematics and iterative optimize the joint parameters of the given robot to align the joint position with their SMPL counterparts (see Appendix~\ref{} for a detailed joint correspondence \ZY{@Haoran, remember to put the table into the appendix.}), ensuring precise replication of human motion. Following the approach in~\cite{luo2023perpetual}, Gaussian filtering is applied to enhance the smoothness of the generated joint trajectories, reducing artifacts and preserving natural motion dynamics.

\noindent \textbf{Model Optimization.}
