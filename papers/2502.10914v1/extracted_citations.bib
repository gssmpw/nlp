@article{cong2023we,
  title={Do we really need complicated model architectures for temporal networks?},
  author={Cong, Weilin and Zhang, Si and Kang, Jian and Yuan, Baichuan and Wu, Hao and Zhou, Xin and Tong, Hanghang and Mahdavi, Mehrdad},
  journal={arXiv preprint arXiv:2302.11636},
  year={2023}
}

@article{fang2024gaugllm,
  title={Gaugllm: Improving graph contrastive learning for text-attributed graphs with large language models},
  author={Fang, Yi and Fan, Dongzhe and Zha, Daochen and Tan, Qiaoyu},
  journal={KDD},
  year={2024}
}

@article{fang2024uniglm,
  title={UniGLM: Training One Unified Language Model for Text-Attributed Graphs},
  author={Fang, Yi and Fan, Dongzhe and Ding, Sirui and Liu, Ninghao and Tan, Qiaoyu},
  journal={arXiv preprint arXiv:2406.12052},
  year={2024}
}

@article{hsieh2023distilling,
  title={Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and others},
  journal={ACL},
  year={2024}
}

@article{huang2024std,
  title={STD-LLM: Understanding Both Spatial and Temporal Properties of Spatial-Temporal Data with LLMs},
  author={Huang, Yiheng and Mao, Xiaowei and Guo, Shengnan and Chen, Yubin and Lin, Youfang and Wan, Huaiyu},
  journal={arXiv preprint arXiv:2407.09096},
  year={2024}
}

@inproceedings{kumar2019predicting,
  title={Predicting dynamic embedding trajectory in temporal interaction networks},
  author={Kumar, Srijan and Zhang, Xikun and Leskovec, Jure},
  booktitle={KDD},
  year={2019}
}

@article{liu2024can,
  title={How can large language models understand spatial-temporal data?},
  author={Liu, Lei and Yu, Shuo and Wang, Runze and Ma, Zhenxun and Shen, Yanming},
  journal={arXiv preprint arXiv:2401.14192},
  year={2024}
}

@article{liu2024large,
  title={Large language model guided knowledge distillation for time series anomaly detection},
  author={Liu, Chen and He, Shibo and Zhou, Qihang and Li, Shizhong and Meng, Wenchao},
  journal={arXiv preprint arXiv:2401.15123},
  year={2024}
}

@article{liu2024spatial,
  title={Spatial-temporal large language model for traffic prediction},
  author={Liu, Chenxi and Yang, Sun and Xu, Qianxiong and Li, Zhishuai and Long, Cheng and Li, Ziyue and Zhao, Rui},
  journal={arXiv preprint arXiv:2401.10134},
  year={2024}
}

@article{luo2024chain,
  title={Chain of history: Learning and forecasting with llms for temporal knowledge graph completion},
  author={Luo, Ruilin and Gu, Tianle and Li, Haoling and Li, Junzhe and Lin, Zicheng and Li, Jiayi and Yang, Yujiu},
  journal={arXiv preprint arXiv:2401.06072},
  year={2024}
}

@article{pan2024distilling,
  title={Distilling large language models for text-attributed graph learning},
  author={Pan, Bo and Zhang, Zheng and Zhang, Yifei and Hu, Yuntong and Zhao, Liang},
  journal={arXiv preprint arXiv:2402.12022},
  year={2024}
}

@inproceedings{trivedi2019dyrep,
  title={Dyrep: Learning representations over dynamic graphs},
  author={Trivedi, Rakshit and Farajtabar, Mehrdad and Biswal, Prasenjeet and Zha, Hongyuan},
  booktitle={ICLR},
  year={2019}
}

@article{wang2021inductive,
  title={Inductive representation learning in temporal networks via causal anonymous walks},
  author={Wang, Yanbang and Chang, Yen-Yu and Liu, Yunyu and Leskovec, Jure and Li, Pan},
  journal={arXiv preprint arXiv:2101.05974},
  year={2021}
}

@article{wang2021tcl,
  title={Tcl: Transformer-based dynamic graph modelling via contrastive learning},
  author={Wang, Lu and Chang, Xiaofu and Li, Shuang and Chu, Yunfei and Li, Hui and Zhang, Wei and He, Xiaofeng and Song, Le and Zhou, Jingren and Yang, Hongxia},
  journal={arXiv preprint arXiv:2105.07944},
  year={2021}
}

@article{xu2020inductive,
  title={Inductive representation learning on temporal graphs},
  author={Xu, Da and Ruan, Chuanwei and Korpeoglu, Evren and Kumar, Sushant and Achan, Kannan},
  journal={arXiv preprint arXiv:2002.07962},
  year={2020}
}

@article{xu2024llm,
  title={LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning},
  author={Xu, Junjie and Wu, Zongyu and Lin, Minhua and Zhang, Xiang and Wang, Suhang},
  journal={arXiv preprint arXiv:2406.01032},
  year={2024}
}

@article{yu2023towards,
  title={Towards better dynamic graph learning: New architecture and unified library},
  author={Yu, Le and Sun, Leilei and Du, Bowen and Lv, Weifeng},
  journal={NeurIPS},
  volume={36},
  pages={67686--67700},
  year={2023}
}

@article{zhang2023llm4dyg,
  title={LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?},
  author={Zhang, Zeyang and Wang, Xin and Zhang, Ziwei and Li, Haoyang and Qin, Yijian and Wu, Simin and Zhu, Wenwu},
  journal={arXiv preprint arXiv:2310.17110},
  year={2023}
}

@article{zhang2023spatio,
  title={Spatio-temporal graph learning with large language model},
  author={Zhang, Qianru and Ren, Xubin and Xia, Lianghao and Yiu, Siu Ming and Huang, Chao},
  journal={arxiv},  
  year={2023}
}

@article{zhang2024dtgb,
  title={DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs},
  author={Zhang, Jiasheng and Chen, Jialin and Yang, Menglin and Feng, Aosong and Liang, Shuang and Shao, Jie and Ying, Rex},
  journal={NeurIPS 2024 Datasets and Benchmarks Track},
  year={2024}
}

