
% DTGB Benchmark
@article{zhang2024dtgb,
  title={DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs},
  author={Zhang, Jiasheng and Chen, Jialin and Yang, Menglin and Feng, Aosong and Liang, Shuang and Shao, Jie and Ying, Rex},
  journal={NeurIPS 2024 Datasets and Benchmarks Track},
  year={2024}
}

% JODIE
@inproceedings{kumar2019predicting,
  title={Predicting dynamic embedding trajectory in temporal interaction networks},
  author={Kumar, Srijan and Zhang, Xikun and Leskovec, Jure},
  booktitle={KDD},
  year={2019}
}



% CAWN paper
@article{wang2021inductive,
  title={Inductive representation learning in temporal networks via causal anonymous walks},
  author={Wang, Yanbang and Chang, Yen-Yu and Liu, Yunyu and Leskovec, Jure and Li, Pan},
  journal={arXiv preprint arXiv:2101.05974},
  year={2021}
}

% DyGformer
@article{yu2023towards,
  title={Towards better dynamic graph learning: New architecture and unified library},
  author={Yu, Le and Sun, Leilei and Du, Bowen and Lv, Weifeng},
  journal={NeurIPS},
  volume={36},
  pages={67686--67700},
  year={2023}
}

%Graph Mixer
@article{cong2023we,
  title={Do we really need complicated model architectures for temporal networks?},
  author={Cong, Weilin and Zhang, Si and Kang, Jian and Yuan, Baichuan and Wu, Hao and Zhou, Xin and Tong, Hanghang and Mahdavi, Mehrdad},
  journal={arXiv preprint arXiv:2302.11636},
  year={2023}
}

%TCL
@article{wang2021tcl,
  title={Tcl: Transformer-based dynamic graph modelling via contrastive learning},
  author={Wang, Lu and Chang, Xiaofu and Li, Shuang and Chu, Yunfei and Li, Hui and Zhang, Wei and He, Xiaofeng and Song, Le and Zhou, Jingren and Yang, Hongxia},
  journal={arXiv preprint arXiv:2105.07944},
  year={2021}
}



%DyREP
@inproceedings{trivedi2019dyrep,
  title={Dyrep: Learning representations over dynamic graphs},
  author={Trivedi, Rakshit and Farajtabar, Mehrdad and Biswal, Prasenjeet and Zha, Hongyuan},
  booktitle={ICLR},
  year={2019}
}



% KDGNNLLM
@article{hu2024large,
  title={Large Language Model Meets Graph Neural Network in Knowledge Distillation},
  author={Hu, Shengxiang and Zou, Guobing and Yang, Song and Zhang, Bofeng and Chen, Yixin},
  journal={arXiv preprint arXiv:2402.05894},
  year={2024}
}


%TGAT
@article{xu2020inductive,
  title={Inductive representation learning on temporal graphs},
  author={Xu, Da and Ruan, Chuanwei and Korpeoglu, Evren and Kumar, Sushant and Achan, Kannan},
  journal={arXiv preprint arXiv:2002.07962},
  year={2020}
}


%LinguGKD
@article{hu2024large,
  title={Large Language Model Meets Graph Neural Network in Knowledge Distillation},
  author={Hu, Shengxiang and Zou, Guobing and Yang, Song and Zhang, Bofeng and Chen, Yixin},
  journal={arXiv preprint arXiv:2402.05894},
  year={2024}
}



% GALLON
@article{xu2024llm,
  title={LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning},
  author={Xu, Junjie and Wu, Zongyu and Lin, Minhua and Zhang, Xiang and Wang, Suhang},
  journal={arXiv preprint arXiv:2406.01032},
  year={2024}
}


%distilling knowledge for TAG learning
@article{pan2024distilling,
  title={Distilling large language models for text-attributed graph learning},
  author={Pan, Bo and Zhang, Zheng and Zhang, Yifei and Hu, Yuntong and Zhao, Liang},
  journal={arXiv preprint arXiv:2402.12022},
  year={2024}
}

%AnomalyLLM
@article{liu2024large,
  title={Large language model guided knowledge distillation for time series anomaly detection},
  author={Liu, Chen and He, Shibo and Zhou, Qihang and Li, Shizhong and Meng, Wenchao},
  journal={arXiv preprint arXiv:2401.15123},
  year={2024}
}


%LLM4DyG
@article{zhang2023llm4dyg,
  title={LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?},
  author={Zhang, Zeyang and Wang, Xin and Zhang, Ziwei and Li, Haoyang and Qin, Yijian and Wu, Simin and Zhu, Wenwu},
  journal={arXiv preprint arXiv:2310.17110},
  year={2023}
}



@article{luo2024chain,
  title={Chain of history: Learning and forecasting with llms for temporal knowledge graph completion},
  author={Luo, Ruilin and Gu, Tianle and Li, Haoling and Li, Junzhe and Lin, Zicheng and Li, Jiayi and Yang, Yujiu},
  journal={arXiv preprint arXiv:2401.06072},
  year={2024}
}


%GAugLLM
@article{fang2024gaugllm,
  title={Gaugllm: Improving graph contrastive learning for text-attributed graphs with large language models},
  author={Fang, Yi and Fan, Dongzhe and Zha, Daochen and Tan, Qiaoyu},
  journal={KDD},
  year={2024}
}


%UniGLM
@article{fang2024uniglm,
  title={UniGLM: Training One Unified Language Model for Text-Attributed Graphs},
  author={Fang, Yi and Fan, Dongzhe and Ding, Sirui and Liu, Ninghao and Tan, Qiaoyu},
  journal={arXiv preprint arXiv:2406.12052},
  year={2024}
}


%DYG2Vec
@article{alomranidyg2vec,
  title={DyG2Vec: Efficient Representation Learning for Dynamic Graphs},
  author={Alomrani, Mohammad and Biparva, Mahdi and Zhang, Yingxue and Coates, Mark},
  journal={TMLR},
  year={2023}
}


%STD-LLM
@article{huang2024std,
  title={STD-LLM: Understanding Both Spatial and Temporal Properties of Spatial-Temporal Data with LLMs},
  author={Huang, Yiheng and Mao, Xiaowei and Guo, Shengnan and Chen, Yubin and Lin, Youfang and Wan, Huaiyu},
  journal={arXiv preprint arXiv:2407.09096},
  year={2024}
}


%STLLM 
@article{zhang2023spatio,
  title={Spatio-temporal graph learning with large language model},
  author={Zhang, Qianru and Ren, Xubin and Xia, Lianghao and Yiu, Siu Ming and Huang, Chao},
  journal={arxiv},  
  year={2023}
}


%STGLLM
@article{liu2024can,
  title={How can large language models understand spatial-temporal data?},
  author={Liu, Lei and Yu, Shuo and Wang, Runze and Ma, Zhenxun and Shen, Yanming},
  journal={arXiv preprint arXiv:2401.14192},
  year={2024}
}



% GraphToken
@article{perozzi2024let,
  title={Let your graph do the talking: Encoding structured data for llms},
  author={Perozzi, Bryan and Fatemi, Bahare and Zelle, Dustin and Tsitsulin, Anton and Kazemi, Mehran and Al-Rfou, Rami and Halcrow, Jonathan},
  journal={arXiv preprint arXiv:2402.05862},
  year={2024}
}


%STLLM - Traffic
@article{liu2024spatial,
  title={Spatial-temporal large language model for traffic prediction},
  author={Liu, Chenxi and Yang, Sun and Xu, Qianxiong and Li, Zhishuai and Long, Cheng and Li, Ziyue and Zhao, Rui},
  journal={arXiv preprint arXiv:2401.10134},
  year={2024}
}


%BERT
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


%TransformerConv
@article{shi2020masked,
  title={Masked label prediction: Unified message passing model for semi-supervised classification},
  author={Shi, Yunsheng and Huang, Zhengjie and Feng, Shikun and Zhong, Hui and Wang, Wenjin and Sun, Yu},
  journal={arXiv preprint arXiv:2009.03509},
  year={2020}
}


%GATConv
@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}



@inproceedings{tang2023dynamic,
  title={Dynamic graph evolution learning for recommendation},
  author={Tang, Haoran and Wu, Shiqing and Xu, Guandong and Li, Qing},
  booktitle={SIGIR},
  year={2023}
}

%HOPE
@inproceedings{luo2023hope,
  title={Hope: High-order graph ode for modeling interacting dynamics},
  author={Luo, Xiao and Yuan, Jingyang and Huang, Zijie and Jiang, Huiyu and Qin, Yifang and Ju, Wei and Zhang, Ming and Sun, Yizhou},
  booktitle={ICML},
  year={2023},
  organization={PMLR}
}

@inproceedings{zhang2023streame,
  title={StreamE: Learning to Update Representations for Temporal Knowledge Graphs in Streaming Scenarios},
  author={Zhang, Jiasheng and Shao, Jie and Cui, Bin},
  booktitle={SIGIR},
  year={2023}
}


% TKGC
@article{cai2022temporal,
  title={Temporal knowledge graph completion: A survey},
  author={Cai, Borui and Xiang, Yong and Gao, Longxiang and Zhang, He and Li, Yunfeng and Li, Jianxin},
  journal={arXiv preprint arXiv:2201.08236},
  year={2022}
}


@article{skarding2021foundations,
  title={Foundations and modeling of dynamic networks using dynamic graph neural networks: A survey},
  author={Skarding, Joakim and Gabrys, Bogdan and Musial, Katarzyna},
  journal={iEEE Access},
  year={2021},
  publisher={IEEE}
}


@article{kazemi2020representation,
  title={Representation learning for dynamic graphs: A survey},
  author={Kazemi, Seyed Mehran and Goel, Rishab and Jain, Kshitij and Kobyzev, Ivan and Sethi, Akshay and Forsyth, Peter and Poupart, Pascal},
  journal={JMLR},
  year={2020}
}


%GPT3
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry and others},
  journal={NeurIPS},
  year={2020}
}



@article{chatgpt2022,
  author = {OpenAI},
  title = {\texttt{https://openai.com/chatgpt/}},
  year = 2022,
  url = {https://openai.com/chatgpt/}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@online{gemini2023,
  author = {Google},
  title = {Gemini},
  year = 2023,
  url = {https://gemini.google.com/app}
}

%Adam Optimizer

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}


@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{hsieh2023distilling,
  title={Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and others},
  journal={ACL},
  year={2024}
}
