\section{Related Works}
\textbf{Dynamic Graphs.} JODIE____ uses coupled recurrent neural networks to forecast embedding trajectories for entities, enabling predictions of their temporal interactions. DyRep____ integrates recurrent node state updates with a deep temporal point process and temporal-attentive network to model evolving graph dynamics nonlinearly. TGAT____ utilizes self-attention for aggregating temporal-topological neighborhood features and captures temporal patterns with a functional time encoding method based on Bochner's theorem. CAWN____ employs an anonymization strategy using sampled walks to explore network causality and generate node identities, which are encoded and aggregated using a neural network model to produce the final node representation. TCL____ utilizes a two-stream encoder for temporal neighborhoods of interaction nodes, integrating a graph-topology-aware Transformer with cross-attention to learn node representations considering both temporal and topological information. GraphMixer____ demonstrates the effectiveness of fixed-time encoding for dynamic interactions using a simple architecture with components for link summarization, node summarization, and link prediction. DyGFormer____ learns node representations from historical first-hop interactions using a neighbor co-occurrence encoding scheme and a patching technique to effectively leverage longer historical sequences. To include textual information into the nodes and edges of dynamic graphs, a recent benchmark, DTGB____ formally defines Dynamic Text-Attributed Graph and performs baseline comparisons on future link prediction and edge classification tasks.

\textbf{LLMs for Temporal Data.} STD-LLM____ proposes spatial and temporal tokens and hypergraph learning module to effectively capture non-pairwise and higher-order spatial-temporal correlations, which also helps to understand the capabilities of large language models (LLMs) for spatio-temporal forecasting and imputation tasks.  STLLM____ proposes to use spatio-temporal prompts to obtain representations from LLMs, which is further aligned with GNNs representations using the InfoNCE loss. In another work____, spatio-temporal embeddings are input into both frozen and unfrozen transformer blocks to produce representations for traffic prediction. STGLLM____ proposes a tokenizer and a LLM-based adapter for performing traffic prediction.


\textbf{Knowledge Distillation involving LLMs and GNNs.} There are several recent works focus on using knowledge distillation to transfer the text processing capabilities of LLMs to lightweight models____ like GNNs for text-attributed graphs. 
LinguGKD____ proposes a model for distilling knowledge from $k$-hop prompt representations in an LLM to $k$-hop GNN representations. However, their approach describe the entire $k$-hop neighborhood which is not well-suited for graphs with long text in nodes and edges. 
LLM4TAG____ proposes to learn text-attributed graph by knowledge distillation from LLMs to GNNs. Another recent work____ utilizes LLMs for temporal knowledge graph completion by leveraging LLMs' capabilities of reasoning with particular attention to reverse logic.
%However they does not take care of the temporal information in any sophisticated way. 
GAugLLM____ proposes an approach for improving the contrastive learning for Text-Attributed Graph by mixture of prompt experts. AnomalyLLM____ proposes a method for time series anomaly detection by knowledge distillation. UniGLM____ proposes an unified graph language model that generalizes both in-domain and cross-domain TAGs. GALLON____ proposes a multi-modal knowledge distillation strategy to transfer the text and structure processing capability of LLMs and GNNs into MLPs for molecular property prediction.\\% LLM4DyG____ attempts to solve toy problems in dynamic graph using prompt engineering on LLMs.
% \clearpage