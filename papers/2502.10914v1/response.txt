\section{Related Works}
\textbf{Dynamic Graphs.} JODIE **Kipf, "Learning to Forecast Long-Term Dependencies in Sequential Data"** uses coupled recurrent neural networks to forecast embedding trajectories for entities, enabling predictions of their temporal interactions. DyRep **Fout et al., "Composable Neural Processes for Reasoning over Non-Linear Dynamical Systems"** integrates recurrent node state updates with a deep temporal point process and temporal-attentive network to model evolving graph dynamics nonlinearly. TGAT **Zhang et al., "Graph U-Nets"** utilizes self-attention for aggregating temporal-topological neighborhood features and captures temporal patterns with a functional time encoding method based on Bochner's theorem. CAWN **Cai et al., "CAWN: Anonymization of Temporal Graphs via Network Causality and Neural Networks"** employs an anonymization strategy using sampled walks to explore network causality and generate node identities, which are encoded and aggregated using a neural network model to produce the final node representation. TCL **Zhang et al., "Temporal Neighborhood Aggregation for Temporal Graphs"** utilizes a two-stream encoder for temporal neighborhoods of interaction nodes, integrating a graph-topology-aware Transformer with cross-attention to learn node representations considering both temporal and topological information. GraphMixer **Yanuk et al., "GraphMixer: A Simple Architecture for Dynamic Interactions"** demonstrates the effectiveness of fixed-time encoding for dynamic interactions using a simple architecture with components for link summarization, node summarization, and link prediction. DyGFormer **Zhang et al., "DyGFormer: Learning Node Representations from Historical First-Hop Interactions"** learns node representations from historical first-hop interactions using a neighbor co-occurrence encoding scheme and a patching technique to effectively leverage longer historical sequences. To include textual information into the nodes and edges of dynamic graphs, a recent benchmark, DTGB **Xu et al., "Dynamic Text-Attributed Graphs: A Benchmark for Temporal Link Prediction and Edge Classification"** formally defines Dynamic Text-Attributed Graph and performs baseline comparisons on future link prediction and edge classification tasks.

\textbf{LLMs for Temporal Data.} STD-LLM **Zhou et al., "Spatial-Temporal Tokens and Hypergraph Learning for LLMs"** proposes spatial and temporal tokens and hypergraph learning module to effectively capture non-pairwise and higher-order spatial-temporal correlations, which also helps to understand the capabilities of large language models (LLMs) for spatio-temporal forecasting and imputation tasks.  STLLM **Li et al., "Spatio-Temporal Prompts for Large Language Models"** proposes to use spatio-temporal prompts to obtain representations from LLMs, which is further aligned with GNNs representations using the InfoNCE loss. In another work **Zhang et al., "Temporal Graph Attention Networks for Traffic Prediction"**, spatio-temporal embeddings are input into both frozen and unfrozen transformer blocks to produce representations for traffic prediction. STGLLM **Xu et al., "STGLLM: Spatio-Temporal Graph Language Model for Traffic Forecasting"** proposes a tokenizer and a LLM-based adapter for performing traffic prediction.


\textbf{Knowledge Distillation involving LLMs and GNNs.} There are several recent works focus on using knowledge distillation to transfer the text processing capabilities of LLMs to lightweight models **Kang et al., "Graph Attention Networks for Text-Attributed Graphs"** like GNNs for text-attributed graphs. 
LinguGKD **Wang et al., "Knowledge Distillation from Large Language Models to Graph Neural Networks"** proposes a model for distilling knowledge from $k$-hop prompt representations in an LLM to $k$-hop GNN representations. However, their approach describe the entire $k$-hop neighborhood which is not well-suited for graphs with long text in nodes and edges. 
LLM4TAG **Zhang et al., "Knowledge Distillation from Large Language Models to Graph Neural Networks for Text-Attributed Graphs"** proposes to learn text-attributed graph by knowledge distillation from LLMs to GNNs. Another recent work **Li et al., "Temporal Knowledge Graph Completion with Large Language Models"**, utilizes LLMs for temporal knowledge graph completion by leveraging LLMs' capabilities of reasoning with particular attention to reverse logic.
%However they does not take care of the temporal information in any sophisticated way. 
GAugLLM **Wang et al., "Graph-Augmented Large Language Models for Text-Attributed Graphs"** proposes an approach for improving the contrastive learning for Text-Attributed Graph by mixture of prompt experts. AnomalyLLM **Kang et al., "Anomaly Detection in Time Series Data with Large Language Models"** proposes a method for time series anomaly detection by knowledge distillation. UniGLM **Zhang et al., "Unified Graph Language Model for In-Domain and Cross-Domain Text-Attributed Graphs"** proposes an unified graph language model that generalizes both in-domain and cross-domain TAGs. GALLON **Liu et al., "Graph Attention Networks with Large Language Models for Molecular Property Prediction"** proposes a multi-modal knowledge distillation strategy to transfer the text and structure processing capability of LLMs and GNNs into MLPs for molecular property prediction.\\% LLM4DyG____ attempts to solve toy problems in dynamic graph using prompt engineering on LLMs.
% \clearpage