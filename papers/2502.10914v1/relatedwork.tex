\section{Related Works}
\textbf{Dynamic Graphs.} JODIE~\cite{kumar2019predicting} uses coupled recurrent neural networks to forecast embedding trajectories for entities, enabling predictions of their temporal interactions. DyRep~\cite{trivedi2019dyrep} integrates recurrent node state updates with a deep temporal point process and temporal-attentive network to model evolving graph dynamics nonlinearly. TGAT~\cite{xu2020inductive} utilizes self-attention for aggregating temporal-topological neighborhood features and captures temporal patterns with a functional time encoding method based on Bochner's theorem. CAWN~\cite{wang2021inductive} employs an anonymization strategy using sampled walks to explore network causality and generate node identities, which are encoded and aggregated using a neural network model to produce the final node representation. TCL~\cite{wang2021tcl} utilizes a two-stream encoder for temporal neighborhoods of interaction nodes, integrating a graph-topology-aware Transformer with cross-attention to learn node representations considering both temporal and topological information. GraphMixer~\cite{cong2023we} demonstrates the effectiveness of fixed-time encoding for dynamic interactions using a simple architecture with components for link summarization, node summarization, and link prediction. DyGFormer~\cite{yu2023towards} learns node representations from historical first-hop interactions using a neighbor co-occurrence encoding scheme and a patching technique to effectively leverage longer historical sequences. To include textual information into the nodes and edges of dynamic graphs, a recent benchmark, DTGB~\cite{zhang2024dtgb} formally defines Dynamic Text-Attributed Graph and performs baseline comparisons on future link prediction and edge classification tasks.

\textbf{LLMs for Temporal Data.} STD-LLM~\cite{huang2024std} proposes spatial and temporal tokens and hypergraph learning module to effectively capture non-pairwise and higher-order spatial-temporal correlations, which also helps to understand the capabilities of large language models (LLMs) for spatio-temporal forecasting and imputation tasks.  STLLM~\cite{zhang2023spatio} proposes to use spatio-temporal prompts to obtain representations from LLMs, which is further aligned with GNNs representations using the InfoNCE loss. In another work~\cite{liu2024spatial}, spatio-temporal embeddings are input into both frozen and unfrozen transformer blocks to produce representations for traffic prediction. STGLLM~\cite{liu2024can} proposes a tokenizer and a LLM-based adapter for performing traffic prediction.


\textbf{Knowledge Distillation involving LLMs and GNNs.} There are several recent works focus on using knowledge distillation to transfer the text processing capabilities of LLMs to lightweight models~\cite{hsieh2023distilling} like GNNs for text-attributed graphs. 
LinguGKD~\cite{xu2024llm} proposes a model for distilling knowledge from $k$-hop prompt representations in an LLM to $k$-hop GNN representations. However, their approach describe the entire $k$-hop neighborhood which is not well-suited for graphs with long text in nodes and edges. 
LLM4TAG~\cite{pan2024distilling} proposes to learn text-attributed graph by knowledge distillation from LLMs to GNNs. Another recent work~\cite{luo2024chain} utilizes LLMs for temporal knowledge graph completion by leveraging LLMs' capabilities of reasoning with particular attention to reverse logic.
%However they does not take care of the temporal information in any sophisticated way. 
GAugLLM~\cite{fang2024gaugllm} proposes an approach for improving the contrastive learning for Text-Attributed Graph by mixture of prompt experts. AnomalyLLM~\cite{liu2024large} proposes a method for time series anomaly detection by knowledge distillation. UniGLM~\cite{fang2024uniglm} proposes an unified graph language model that generalizes both in-domain and cross-domain TAGs. GALLON~\cite{xu2024llm} proposes a multi-modal knowledge distillation strategy to transfer the text and structure processing capability of LLMs and GNNs into MLPs for molecular property prediction.\\% LLM4DyG~\cite{zhang2023llm4dyg} attempts to solve toy problems in dynamic graph using prompt engineering on LLMs.
% \clearpage