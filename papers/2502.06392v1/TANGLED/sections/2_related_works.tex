
\section{Related Work}
\label{sec:related}

Our work bridges advancements in 3D hair representation, image-based hair modeling, and data-driven hair generation.

\vspace{4pt}
\noindent \textit{3D Hair Representation.}
Early methods relied on parametric representations for structured hair geometry design, including 2D parametric surfaces~\cite{koh2000real,liang2003enhanced,noble2004modelling}, wisp-based models~\cite{watanabe1992trigonal}, generalized cylinders~\cite{chen1999system,choe2005statistical,patrick2004modelling,xu2001v,yang2000cluster}, multi-resolution cylinders~\cite{kim2002interactive,wang2004hair}, and hair meshes~\cite{yuksel2009hair,bhokare2024real}. While these enabled intuitive styling, they struggled to capture intricate geometric variations (e.g., curls, frizz) or dynamic properties (e.g., wind interactions) in complex hairstyles. Recent volumetric approaches, such as adaptive shells~\cite{wang2023adaptive}, prioritize efficiency over fidelity.
%
Strand-based representations emerged as the new standard for high-fidelity modeling. Widely adopted in research~\cite{piuze2011generalized,shen2023ct2hair} and production~\cite{chiang2015practical,fascione2018path}, they excel in physics-based simulation~\cite{daviet2023interactive,fei2017multi,hsu2023sag,herrera2024augmented,digitalsalon} and strand-level editing~\cite{xing2019hairbrush}. However, manually creating a full strand-based hairstyle is highly labor-intensive, highlighting the need for generative models to automate hair asset creation.
Recent advances~\cite{HAAR:CVPR:2024} map strands to UV-space latents. While generative models trained in this space enable hair generation from text prompts, they struggle with limited 3D hair dataset diversity.
%
Our work addresses the issue by proposing a diverse 3D hair dataset featuring stylized and cultural hairstyles, and a lineart-conditioned generative framework. This enables rapid prototyping of diverse, stylized hair geometries.



\vspace{4pt}
\noindent \textit{Image Based Hair Modeling.}
Early image-based hair modeling methods focused on direct reconstruction~\cite{kong1998generation} or heuristic volumetric techniques~\cite{paris2004capture} from multi-view images. Subsequent work prioritized reconstructing 3D orientation fields to triangulate strands. ~\cite{nam2019strand} introduced a line-based PatchMatch MVS algorithm for robust oriented point clouds, where~\cite{takimoto2024dr} enhanced orientation consistency via global optimization.~\cite{zhou2024groomcap} further advanced fidelity using neural implicit representations. However, occlusion limits multi-view methods to surface-level geometry, producing incomplete internal strands.
%
Data-driven methods mitigate this by integrating hair structure priors. ~\cite{kuang2022deepmvshair,wu2024monohair} infer internal 3D orientations via neural networks, and~\cite{zakharov2025human,sklyarova2023neural} employ diffusion priors with SDS loss~\cite{poole2022dreamfusion} to refine geometry. Despite progress, these methods require controlled capture setups.

Single-view modeling is a more challenging task. Early methods~\cite{chai2016autohair,chai2015high,hu2015single,chai2012single,chai2013dynamic} relied on database retrieval and refinement, while ~\cite{zhou2018hairnet} predicted 3D models from 2D orientation maps. Volumetric techniques~\cite{saito20183d,zhang2019hair} reconstructed orientation/occupancy fields.
%
~\cite{wu2022neuralhdhair} improves resolution via a coarse-to-fine framework. HairStep~\cite{zheng2023hairstep} addressed this with depth-strand map hybrid representations, better linking 2D inputs to 3D geometry. 
%However, all remain sensitive to input image quality.
%
DeepSketchHair~\cite{shen2020deepsketchhair} used sketches to generate 3D orientation fields, bypassing photorealism.
%
Our work supports multi-style inputs (sketches, stylized art, photos) and resolves ambiguities in sparse data with lineart-conditioned diffusion, enabling robust hair generation across artistic domains.


\vspace{4pt}
\noindent \textit{Hair Generation Models.}
Despite progress in hair modeling, data-driven hair generation remains underdeveloped with notable challenges. GroomGen~\cite{zhou2023groomgen} introduced the first generative model for 3D hair, using a hierarchical representation with strand-VAE and hairstyle-VAE to encode individual strands and overall hairstyles. %Although its GAN-based neural upsampler restores high-quality details, reliance on manual strand adjustments limits practicality, especially for users seeking specific hairstyles.
%
Perm~\cite{he2024perm} proposed a PCA-based parametric model that disentangles hair structures with frequency-domain strand representations. It supports single-image inputs by projecting 3D hairstyles into 2D and aligning them via perm parameter adjustments. %However, PCAâ€™s linear nature limits diversity and complexity, reducing controllability and fidelity.
%
Recently, HAAR~\cite{HAAR:CVPR:2024} introduced the first text-guided generative framework for 3D hair using a latent diffusion model conditioned on text inputs in a unified hairstyle UV space. %Despite its innovation, the method's reliance on text descriptions poses a significant limitation: linguistic prompts often fail to capture the intricate geometric details of complex hairstyles, leading to mismatches between user intent and generated outputs.
The hair generation field lacks diverse, high-quality datasets representing a wide range of hairstyles. Additionally, text-based approaches struggle to capture intricate geometric and structural variations in hairstyles.
%
Our method leverages lineart conditioning and the MultiHair dataset for hair generation. Supporting various input styles (e.g., sketches, photos, partial renders), our approach enhances flexibility, precision, and controllability, bridging user intent and algorithmic output.
