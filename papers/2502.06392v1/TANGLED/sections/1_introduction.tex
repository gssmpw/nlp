
\section{Introduction}
\label{sec:intro}

Hairstyles are powerful symbols of cultural identity, social status, and personal expression. Beyond their aesthetic appeal, they embody deep historical and social meanings. For instance, traditional Asian hairstyles often carry spiritual and societal significance, while African hairstyles like dreadlocks have been central to expressions of cultural pride and resistance. Successfully recreating these hairstyles in digital form, whether in feature films, computer animations, or video games, is vital for fostering inclusive and diverse representation in media. However, realistic hairstyles exhibit intricate geometry, varied textures, and dynamic movement, often requiring artists to painstakingly model each strand. Even with advanced tools, this process remains labor-intensive, demanding a balance of technical expertise and cultural sensitivity to ensure the hair appears authentic and meaningful.


Instead of relying solely on manual modeling, researchers have explored 3D reconstruction solutions to recover hairstyles directly from images. Multi-view capture systems, such as multi-camera domes or moving-camera setups, coupled with reconstruction techniques from early stereo matching methods to modern volumetric modeling approaches, can approximate the overall shape of hair. Yet, these methods struggle to capture the diversity of intricate styles. For example, dreadlocks, with their intertwining locks and varying thicknesses, present significant challenges due to their dense, coiled geometry, which is difficult for 3D scanning systems to recover with fine detail and precision. Moreover, multi-view capture requires specialized equipment or controlled setups, limiting its practicality for casual users or scenarios without access to such apparatus. While 3D capture technologies can handle basic shapes, they fall short in reproducing the richness, detail, and cultural nuances of more complex or specific hairstyles.


Generative tools like DALL·E~\cite{betker2023dalle3}, DreamStudio~\cite{dreamstudio2025}, and Artbreeder~\cite{artbreeder2025} have been developed to create diverse 2D images including hairstyles using text prompts and the latest efforts have been focused on extending this capability to 3D ~\cite{zhang2024clay, rombach2022high, Deitke2023objaverse, Wu2023OmniObject}. Generating 3D hair from text requires bridging the gap between abstract textual descriptions and the detailed geometry, textures, and physical behavior of hair. Text-Conditioned Generative Model~\cite{HAAR:CVPR:2024}, represents a significant step forward in leveraging textual descriptions for synthesizing 3D hair strands. However, these approaches are fundamentally constrained by the accuracy of textual annotations available in datasets. 

In place of texts, it’d be more natural to use images as inputs. Image-based hair modeling methods~\cite{kuang2022deepmvshair,wu2024monohair,zhou2018hairnet,wu2022neuralhdhair,zheng2023hairstep,he2024perm} rely on optimization-based reconstruction techniques, taking single-view or multi-view images as input to refine occupancy and orientation fields. While these techniques can produce high-quality results, they rely on photorealistic images and are computationally expensive, demanding significant time and resources. Moreover, due to the scarcity of diverse hairstyles, particularly afro or curly hair, in commonly used datasets, their effectiveness in capturing such complex structures remains limited. Finally, compounded by sparse training data, they exhibit limited geometric fidelity when modeling intricate structures like braids, which demand precise topological constraints.  

In this paper, we introduce \textit{TANGLED}, a novel approach to generating 3D hair strands from flexible image inputs with diverse styles, viewpoints, and varying numbers of views. We adopt a trilogy to enable such unique flexibility.
% 
First, we introduce the MultiHair Dataset, a curated collection of 457 hairstyles spanning 74 global and local hair attributes (e.g., strand styling, length, direction, layering) with multi-view image annotations. Prior datasets~\cite{hu2015uschairsalon, shen2023ct2hair, Hair20k, zhou2018hairnet} disproportionately represent 10-400 hairstyles, whereas MultiHair prioritizes underrepresented textures (e.g., coiled, locs) and complex geometries, expanding hairstyle diversity by 30\%. This shift tackles the scarcity of diverse training data that undermines generalization in existing methods.


Second, we propose a diffusion framework conditioned on multi-view linearts for flexible hair generation. As a sparse structural representation, such lineart not only preserves topological cues (e.g., parting lines, strand density) but also filters out noise such as lighting variations and occlusions. As a result, lineart conditioning can effectively handle geometric ambiguities inherent in single-view inputs while generalizing seamlessly across diverse image styles and viewpoints. 
Specifically, we adopt a latent diffusion model with the polyline-based representation of 3D hair strands~\cite{HAAR:CVPR:2024} in the latent space. We apply DINOv2~\cite{oquab2024dinov2} on the lineart images to obtain lineart features, which are adapted into the diffusion model via cross attention. We further enhance the generalization ability by randomly blending lineart features from different viewpoints. As a result, regardless of their style or viewpoint, our framework empowers flexible and accurate 3D hair strand generation that adapts to a wide variety of input conditions
Finally, we design a parametric post-processing module that inpaints braid-specific constraints (e.g., cyclic strand crossings, torsion) during generation. It significantly reduces geometric distortions, preserving coherent braid appearance compared to pure diffusion or parametric baselines~\cite{zeng2024hairdiffusion}.


By supporting underrepresented styles like braids and locs~\cite{meishvili2024hairmony}, TANGLED enables culturally inclusive digital avatars. It extends to applications such as sketch-based 3D strand editing, enabling expedited virtual prototyping for animation and augmented reality, where rapid, user-guided design is paramount.Experiments demonstrate superior performance: user studies report 84.3\% preference for our results over text-guided models~\cite{HAAR:CVPR:2024} in realism and diversity.
