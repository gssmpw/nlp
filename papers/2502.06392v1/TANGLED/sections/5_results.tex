\section{Results}
\label{sec:experiments}

We demonstrate the capability of our model to generate 3D hair with various styles from image inputs in various styles, including photographs, cartoons, and paintings, as shown in Fig.~\ref{fig:application} and Fig.~\ref{fig:gallery}. We also showcases some applications of our method. Our approach can generate hairstyles from hand-drawn sketches and allows users to modify existing lineart to create customized hairstyles, allowing users to refine hairstyles iteratively without extensive 3D expertise.


\subsection{Implementation Details}

Our diffusion model operates on input sizes of $32 \times 32$, using a U-Net~\cite{ronneberger2015u} architecture. The U-Net is configured with 8 attention heads and the channel multipliers are defined as [1, 2, 4, 4]. For training, we use the AdamW~\cite{loshchilov2017decoupled} optimizer with a learning rate of $1 \times 10^{-4}$, and follow the soft Min-SNR~\cite{hang2023efficient} weighting strategy. Noise sampling follows a cosine-interpolated density distribution. For inference, we employ DDIM~\cite{song2020denoising} sampling with 50 steps. We train our method on a single NVIDIA RTX 3090 GPU for approximately 10 days. Classifier-Free Guidance~\cite{ho2022classifier} is applied during training with a probability of 0.1 to randomly mask the image condition.

\begin{figure*}
  \includegraphics[width=\textwidth]{TANGLED/img/gallery_draft.pdf}
  \label{fig:results_gallery}
  \caption{\textbf{Result gallery.} \myname can generate realistic hairstyles from image conditions with various styles, including photographs, anime, and oil paintings. For more results, please refer to the supplementary video. Note that we manually specified the color for the generated hair during the rendering process.}
  \label{fig:gallery}
\end{figure*}

\subsection{Comparisons}

We evaluate our method against state-of-the-art 3D hair generation/reconstruction approaches: HAAR~\cite{HAAR:CVPR:2024}, a text-guided generative model, and HairStep~\cite{zheng2023hairstep}, an optimization-based reconstruction method that currently achieves state-of-the-art performance. For benchmarking, we use a 5\% test split of our MultiHair Dataset, ensuring no overlap with training data.
%
HAAR generates hairstyles from text descriptions, requiring a Visual Question Answering (VQA) system to convert input images into textual representations via BLIP~\cite{li2022blip}. HairStep, in contrast, directly optimizes 3D strands from single-view inputs. To ensure a fair comparison, we use the official pre-trained models for both baselines, avoiding implementation biases.


\vspace{4pt}
\noindent \textit{Quantitative Comparisons.}

We conduct a quantitative comparison to evaluate generation quality (geometric and semantic fidelity) and computational efficiency against state-of-the-art methods. Three metrics are employed: Point Cloud IoU to measure geometric similarity between generated 3D hair and ground-truth point clouds; the CLIP Score~\cite{radford2021learning} to evaluate semantic alignment between rendered images and input conditions; and the Chamfer Distance for quantifying geometric accuracy by comparing nearest-neighbor distances between generated and ground-truth surfaces.

For HAAR and our method, we generate 10 stochastic samples per test case to account for generative variability and report mean metric values to ensure statistical robustness. As shown in Tab.~\ref{tab:comparison-quantitative}, our method surpasses other approaches, demonstrating its capability to generate superior results with multi-view and multi-style image inputs. 


\begin{figure}[tbp]
  \includegraphics[width=\columnwidth]{TANGLED/img/comparison_1_v2.pdf}
  \caption{\textbf{Qualitative comparison.} For various input images, our method produces more aligned and detailed hairstyles compared to HairStep and HAAR. For more comparison, please refer to Fig~\ref{fig:comparison_qualitative2}.}
  \label{fig:comparison_qualitative}
\end{figure}

\vspace{-4pt}
\begin{table}
  \small
  \caption{\textbf{Quantitative comparisons and evaluations.} Ours($\mathbf{R}$) refers to our method trained on real images. Ours($\mathbf{V_1}$) and Ours($\mathbf{V_4}$) refer to our method using randomly selected one-view and four-view inputs. We exclude test cases where HairStep failed to optimize hair strands, which is in favor of the HairStep. Yet, our approach achieves the best performance regardless.}
  \label{tab:comparison-quantitative}
    \begin{tabular}{cccccc}
    \toprule
    Methods        & Clip Score $\uparrow$ & CD($\times10^{-2}$m)$\downarrow$ & IoU $\uparrow$ \\
    \toprule
    HairStep     &  69.09   &  0.0112   &  52.78\%  \\
    HAAR     &  73.81   &  0.0336   &  42.14\%   \\
    \midrule
    Ours($\mathbf{R}$)        &  75.81   &  0.0051   &  45.00\% \\
    \midrule
    Ours($\mathbf{V_1}$)   &  76.77   &  0.0039   &  53.06\% \\
    Ours($\mathbf{V_4}$)        &  79.04   &  0.0033   &  53.69\% \\
    \bottomrule
    \end{tabular}
\end{table}





\vspace{4pt}
\noindent \textit{Qualitative Comparisons.}
\label{sec:qualitative_comp}
To further assess performance, we conduct qualitative comparisons across diverse input styles and viewpoints. As illustrated in Fig.~\ref{fig:comparison_qualitative}, our method outperforms baselines in preserving fine geometric details (e.g., curls, braids, split ends) and maintaining semantic alignment with input conditions (e.g., bangs, parting, volume).
HAAR’s reliance on a VQA system to extract textual descriptions from images introduces semantic ambiguities, often yielding hairstyles misaligned with input intent. HairStep, optimized for frontal-view reconstruction, struggles with viewpoint invariance — occluded regions or non-frontal inputs.


\begin{figure}[thbp]
  \includegraphics[width=\columnwidth]{TANGLED/img/comparison_2_v2.pdf}
  \caption{\textbf{Qualitative comparison.} Our model accurately captures hairstyle structure and details, demonstrating superior adaptability to diverse input conditions. Notice that HairStep fails in cases where the face is not recognizable, as it cannot optimize the hairstyle without facial guidance(\textit{Row 4}). While HAAR relying on textual descriptions extracted from the image, struggles to preserve fine details.}
  \label{fig:comparison_qualitative2}
\end{figure}


\begin{figure}[thbp]
  \includegraphics[width=\columnwidth]{TANGLED/img/userstudy.png}
  \caption{\textbf{User study result.} The preferences of 42 participants are illustrated for two input categories: Realistic (\textit{top}) and Stylized (\textit{bottom}). For realistic inputs, our method received the highest user preference (63.8\%), significantly outperforming HairStep (34.8\%) and HAAR (1.4\%). Similarly, for stylized inputs, our method is strongly favored (84.3\%) compared to HairStep (15.2\%) and HAAR (0.5\%).  }

  \label{fig:user-study}
\end{figure}

\vspace{4pt}
\noindent \textit{User Study.}

Additionally, we conduct a user study with 42 participants across 10 test cases spanning diverse input styles and viewpoints. For each case, we generate viewpoint-aligned renderings from our method and two baselines (HAAR and HairStep), ensuring direct visual comparability. Participants evaluated outputs based on structural fidelity (hairstyle geometry), detail retention (strand-level features), and stylistic coherence (alignment with input conditions). As shown in Fig.~\ref{fig:user-study}, our method achieved 63.8\% preference in realistic scenarios and 84.3\% in stylized cases, outperforming baselines by significant margins.


\begin{figure}[tbp]
  \includegraphics[width=\columnwidth]{TANGLED/img/multiview_v2.pdf}
  \caption{\textbf{Generated results using single-view and multi-view inputs.} With single-view input(\textit{Row 1}), the asymmetrical shoulder-length hair on the occluded side cannot be accurately reconstructed, resulting in missing details. In contrast, multi-view inputs (\textit{Row 2}) enable the model to capture the full structure of the hairstyle, including the previously occluded regions }
  \label{fig:multiview}
\end{figure}

\subsection{Evaluation}
To evaluate the effectiveness of our lineart extraction and multi-view conditioning approach, we conducted a series of experiments. 
As shown in Fig.~\ref{fig:multiview}, single-view image conditions may fail to generate complex hairstyles where parts of the hairstyle are occluded. When multiple views are provided, the model can accurately reconstruct the hairstyle with improved precision and detail.

Furthermore, as presented in Table~\ref{tab:comparison-quantitative}, we compared the performance of our method under two conditions: using a randomly selected single-view image and using four randomly selected views as input conditions on the test set. The results demonstrate that our method generates significantly more accurate hairstyles when conditioned on four views, highlighting the advantages of multi-view conditioning for capturing both global structures and fine-grained details of complex hairstyles. We also compared the performance of our method training on lineart condition and generated image condition. The results show that the model trained on lineart outperforms the one trained on rendered images across all metrics. This indicates that training with lineart enables the model to better capture the structure and features of hairstyles, leading to superior performance when generating hairstyles from diverse styles and viewpoints in the image conditions.



