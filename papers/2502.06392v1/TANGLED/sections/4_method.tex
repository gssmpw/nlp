
\section{The TANGLED Model}
\label{sec:method}


As shown in Fig.~\ref{fig_overview}, given a hair image from an arbitrary viewpoint, \myname 
generates 3D hair strands represented as polylines, making them compatible with standard computer graphics pipelines.

Its lightweight architecture ensures efficient generation of CG-ready 3D hairstyles, which integrate seamlessly into tools like Blender and Unreal Engine for animation and rendering.



\subsection{Hair Strands Latent Diffusion Model}



We adopt the representation of 3D hair strands as a set of 3D polylines evenly distributed over the scalp, as introduced by HAAR~\cite{HAAR:CVPR:2024} and Neural Haircut~\cite{sklyarova2023neural}. These 3D polylines are encoded into a 2D latent map $\mathbf{X}$ utilizing a VAE encoder with scalp UV mapping. 
Specifically, each hair strand $S_i = \{s_j\}, {j \in 1, \dots, L}$ is a sequence of 3D points $s_j$ of length $L$, Each strand is encoded into a latent vector $z$ using an encoder $\mathcal{E}$ and then reconstructed through a decoder $\mathcal{D}$. 
%
\begin{align}
\mathbf{X} =  \mathcal{M}_\text{uv} (\mathcal{E}(\{ S_j \})), \, \, \, 
S_j = \mathcal{D} \big (  \mathcal{N}_\text{uv} ( \mathbf{X}, j) \big ),
\end{align}    
%
where $\mathcal{M}_\text{uv}$ denotes the mapping of a strand latent to the scalp UV space, and $\mathcal{N}_\text{uv}$ indicates the process of sampling latent in the UV space.
During training, the VAE is optimized through the L-2 distance between predicted points $\hat{s}_j$ and ground-truth points $s_j$, the cosine similarity between predicted directions $\hat{d}_j = \hat{s}_{j + 1} - \hat{s}_{j}$ and ground-truth directions $d_j$, and the L-2 distance between predicted curvatures $\hat{g}_j = \|\hat{d}_j \times \hat{d}_{j+1}\|_2$ and ground-truth curvatures $g_j$. The fidelity term is expressed as:
\begin{equation}    
\mathcal{L}_{\text{dist}} = \sum_{j, \, S} \|\hat{s}_j - s_j\| + (1 - \hat{d}_j \cdot d_j) + \|\hat{g}_j - g_j\|,
\end{equation}

weight of each term is omitted for clarity. 

The training objective then is the combination of the fidelity term and a KL divergence term.


The denoising process is learned on the latent map $\mathbf{X}$. To optimize computational efficiency, we downsample the latent map to a $32 \times 32$ resolution. The diffusion model adopts a 2D U-Net architecture for denoising within this downsampled latent space. During training, Gaussian noise is progressively added to $\mathbf{X}$ at each time step, resulting in a noisy latent map $\mathbf{X}_t = \mathbf{X} + \mathbf{n}_t$, where $\mathbf{n}_t$ represents the Gaussian noise at time step $t$. The model $\epsilon{\theta}$ is then trained to predict and remove this noise, effectively learn the reverse diffusion process that denoises the latent map over time.

\begin{equation}    
\epsilon_\theta(\mathbf{X}_t; t, \mathbf{F}) = \mathbf{n}_t
\end{equation}

where $\mathbf{F}$ represents the concatenated muti-view lineart features extracted by DINOv2~\cite{oquab2024dinov2} for conditioning. The model is trained by minimizing the Mean Squared Error (MSE) loss between the predicted noise and the noise added at each diffusion step.


\subsection{Multi-view Lineart Conditioning}
\label{sec:image_conditioning}


\begin{figure}[tbp]
  \includegraphics[width=\columnwidth]{TANGLED/img/lineart_v2.pdf}
  \caption{\textbf{Lineart extracted for various images.} For the same hairstyle under different image domains (realistic, anime and oil painting), the extracted lineart effectively captures consistent hair structure and features. }

  \label{fig:lineart}
\end{figure}

To condition our hair generation model on multi-view lineart, we first extract hair region masks from the input images using Grounding DINO~\cite{liu2025grounding} and SAM~\cite{kirillov2023segment}. This ensures that the model focuses on hairstyle-related regions, effectively removing background noise and distractions. Lineart serves as an effective representation for capturing hair strand details across various styles while mitigating interference from factors such as lighting variations, occlusions, and other irrelevant artifacts. \textcolor{black}{Fig.~\ref{fig:lineart} demonstrates that lineart extracted from the same hairstyle rendered in realistic, oil painting, and anime styles, exhibits the similar characteristic. Despite stylistic disparities, the extracted lineart retains structural coherence and granular detail fidelity, affirming its robustness to domain shifts}. We employ a lineart detector~\cite{zhang2023adding} to extract lineart from the input images, emphasizing the geometric structure and details of the hair strands.
For feature extraction, we utilize the pretrained DINOv2~\cite{oquab2024dinov2} model to extract features from the input lineart image. The resulting lineart features $\mathbf{F}$ are then concatenated and integrated into the diffusion model through a cross-attention mechanism.



To enable our model to handle image inputs from arbitrary viewpoints, we adopt a multi-view training strategy. Specifically, during training, we randomly select 1 to 8 images from the 72 multi-view image annotations for each hairstyle model, and concatenate their DINOv2 features to form the conditioning input for the diffusion model. This approach equips the model with the ability to adapt to arbitrary viewpoints and styles, enhancing the robustness and consistency of the generated hairstyles.
%
For inference, our method supports multi-view image input. 
As shown in Fig.~\ref{fig:comparison_qualitative}, single-view input may lack sufficient information to accurately reconstruct complex hairstyles with occluded regions. In contrast, multi-view inputs provide comprehensive details from various perspectives, enabling the model to capture both global structures and fine details, resulting in more precise and adaptable hairstyle generation across diverse styles and viewpoints.



\subsection{Parametric Braid Inpainting}


Generating realistic braided hairstyles is challenging due to their topological complexity and strand dynamics. Methods like physically-based simulators~\cite{PBSimulator, HairAnimGPU} or neural implicit surfaces~\cite{wang2021neus} often fail to maintain braid coherence.
%
Directly synthesizing braids via diffusion models also falters due to their reliance on weak geometric priors. 
To address this, we design an inpainting~\cite{lugmayr2022repaint} approach to seamlessly inject the braid into an already generated hairstyle. Specifically, we utilize a hybrid diffusion framework that explicitly encodes braid geometry through parametric modeling. Braids are parameterized using strand grouping templates and root guide curves, providing a structured prior. This prior is transformed into latent space and incorporated into the diffusion process using an attention-based inpainting technique, ensuring that the braid integrates naturally and cohesively with the existing hairstyle.

\vspace{4pt}
\noindent \textit{Braid Detection and UV Mapping.} 

To enable consistent braid synthesis from a reference image, our framework localizes the braid structure using a segmentation pipeline combining Grounding DINO~\cite{liu2025grounding} and SAM~\cite{kirillov2023segment}, extracting the 2D braid mask. Anatomical coherence is ensured by aligning braid roots with the scalp topology. We estimate the head pose and align a template mesh using 3DDFAv2~\cite{guo2020towards}, projecting the 2D braid mask onto the mesh’s scalp region. For each hair strand, IoU with the SAM-extracted mask identifies strands within the braid region. The strand roots determine $M_{\text{braid}}$, the braid root region in UV texture space. Manual selection of strands is also supported for defining $M_{\text{braid}}$ when the head pose estimation fails. This UV mask guides braid-specific localization and injection during the diffusion process.

\vspace{4pt}
\noindent \textit{Braid Parametrization.}
Within the braid UV mask, we select the corresponding hair strands and calculate an average curve to serve as our guide strand. Using the guide strand, we compute a Frenet--Serret frame and introduce helical patterns along the guide strand to generate the braid geometry, following~\cite{hu2014capturing}.
The shape of the braid can be adjusted by modifying parameters that control its width, thickness, and cross-sectional oscillation. 
To enhance visual smoothness and geometric fidelity, we apply Laplacian smoothing~\cite{Crane:2011:STD} to the strands, reducing high-frequency noise while preserving critical features (e.g., crossover points, torsion). The refined strands are integrated to form cohesive multi-strand braids. Finally, the parameterized hair structure is mapped into the latent space via our strand encoder $\mathcal{E}$, enabling braid topology-aware synthesis within the diffusion framework.  



\vspace{4pt}
\noindent \textit{Braid Inpainting.}

Once the latent map and corresponding mask for the braid are obtained, we use an inpainting~\cite{lugmayr2022repaint} approach to inject the braid into the appropriate position on the scalp. Specifically, during denoising step of the diffusion process, the latent representations within the masked region are replaced with those from the braid’s latent map. This ensures that the braid’s geometric structure is preserved while allowing it to blend seamlessly with the surrounding hair strands.


\begin{figure}[tbp]
  \includegraphics[width=0.9\columnwidth]{TANGLED/img/application_v3.pdf}
  \caption{\textbf{Application showcase.} \textit{Row 1} show the generated hairstyles from hand-drawn sketches. \textit{Row 2} illustrate hairstyle modifications(adding pigtails) by altering specific parts in the sketches from \textit{Row 1}. \textit{Row 3-4} depict the process of generating outputs with braid using guidelines (highlighted in red).}
  \label{fig:application}
  % \vspace{-20pt}
\end{figure}
