

\section{Dataset Construction}
\label{sec:data}
Training a neural model to generate diverse 3D hairstyles requires a dataset that represents ethnic and cultural diversity, ensuring realistic strands between populations.

Moreover, conditioning annotations should be designed to guide the generation process, providing the model with clear, structured information about hair characteristics.

Existing public datasets lack sufficient diversity in hairstyles and detailed annotations. To address this, we first collected a multi-modal 3D hairstyle dataset, $MultiHair$, which includes a variety of hairstyles paired with text and image annotations, as shown in Tab.~\ref{tab:comparison_data}.




\vspace{8pt}
\noindent \textit{Data Collection.}
% \lipsum[1-3]
Advances in hair modeling have been driven by the USC-HairSalon dataset~\cite{hu2015uschairsalon}, which covers conventional styles like bob, afro, curly, and wavy. However, it lacks a detailed representation of complex braided styles, such as ponytails, pigtails, and various braids, due to challenges in data collection and annotation. While studies~\cite{HAAR:CVPR:2024, he2024perm, Hair20k} used augmentation techniques (e.g., squeezing, stretching, cutting, flipping, curling) to enhance diversity, intricate braided styles remain underrepresented.
%
Thus, we introduce a dataset that broadens hairstyle diversity, emphasizing braided styles. It includes hairstyle strands sourced from various online platforms, reflecting global and local characteristics~\cite{meishvili2024hairmony}. Notably, 10\% of the dataset focuses on braided styles, such as ponytails, pigtails, and diverse braid patterns. To ensure quality, we collaborated with professional artists who refined the data through cutting, stretching, flipping, and blending while preserving the original hairstyles. As shown in Fig.~\ref{tab:comparison_data}, our dataset surpasses existing public datasets in hairstyle diversity and braided style representation.


\begin{table}[tbp]

  
  \caption{\textbf{Statistical Comparison of Hair Datasets }. We present a comparative analysis of our MultiHair Dataset against existing public datasets across five key dimensions: hairstyles (H) / categories of hairstyles (C), total number of individual hairstyles, availability of text annotations, the number of viewpoints, and image annotations. For open-source datasets, we report cleaned/verified data quantities, while closed-source datasets reflect claimed values. Please note HiSa\&HiDa collects portrait images instead of 3d hairstyles, HairNet provides synthetic 4-view renders of each hairstyle.)}
  \label{tab:comparison_data}
  % \includegraphics[width=\columnwidth]{example-image-duck}

    \resizebox{0.95\columnwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
             & Basis & Total & Text Annotation & Image Views \\ 
    \midrule
    USC-HairSalon               & 343 (H)   & 343       &  \XSolidBrush & ---              \\
    Hair20k                     & 343 (H)   & 3715      &  \XSolidBrush & ---          \\
    CT2Hair                     & 10 (H)    & 10        &  \XSolidBrush & ---          \\
    GroomGen                    & 35 (C)    & 7712      &  \XSolidBrush & ---         \\
    HAAR                        & 393 (H)   & 9825      &  \Checkmark   & ---          \\
    HiSa\&HiDa                  & 1250*     & 1250*     &  \XSolidBrush & 1         \\
    HairNet                     & 340 (H)   & 40k+      &  \XSolidBrush & 4*         \\
    \textbf{MultiHair (Ours)} & \textbf{457 (H)}   & 10274     &  \Checkmark   & \textbf{72}           \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-7pt}

\end{table}




\vspace{8pt}
\noindent \textit{Image and Text Annotation.}
%
Hairstyles exhibit significant diversity, making them hard to describe accurately solely with text. This complexity stems from factors like texture, shape, and cultural significance, which differ across styles. While images provide intuitive representations, they are often impacted by noise, artifacts, and extraneous factors that obscure details. A robust representation is needed to capture texture, shape, and style while minimizing background clutter, lighting issues, and occlusions. We propose using lineart as an effective representation, focusing on outlines and contours to emphasize shape and structure while avoiding intricate textural or tonal details.
%
To encompass a broad range of styles and viewpoints, we enhance our dataset by generating diverse hair images with various viewpoints for each 3D hair model.
%
Specifically, we use eight evenly spaced horizontal viewpoints and three pitch angles. For each combination of viewpoint and pitch angle, we render images with three focal lengths: 35 mm wide-angle, 50 mm standard, and 85 mm telephoto. We then extract the lineart image of each rendered view using a Lineart Detector~\cite{zhang2023adding}. To cover a wide range of hair images and styles, we generated 72 stylized images using ControlNet~\cite{zhang2023adding} conditioned on viewpoints and linearts, to augment the image and lineart annotations, as described in Fig.~\ref{fig:DataAc}.



\begin{figure}[tbp] 
  \includegraphics[width=\columnwidth]{TANGLED/img/data_pipeline_v2.pdf}
  \caption{\textbf{Dataset Annotation Process}. 
  Our annotation pipeline begins by processing rendered 3D hair strands with a line-art detector, and line-art sketches are combined with OpenPose~\cite{cao2017realtime} skeletal data for conditioning ControlNet. To enrich dataset diversity, we further synthesize multi-view images, to cover variations in lighting, texture, and perspective. Finally, GPT-4~\cite{chatgpt2025} generates detailed textual annotations for each hairstyle, including attributes such as length, curliness, density, and cultural style.
  }
  \label{fig:DataAc}
\end{figure}



\begin{figure*}[tbp]
  \centering
  \includegraphics[width=\textwidth]{TANGLED/img/overview_v2.pdf}
  \captionof{figure}{\textbf{Architecture of our \myname.} Our model takes hair images with arbitrary styles and viewpoints as conditions, and generate the 3D hair latent through the diffusion process. The conditions are randomly masked and cross-attention with the latent. At inference, we sample hair latent maps and feed the upsampled hair latent map to the strand decoder to extract the 3D hair strands.
} \label{fig_overview}
\end{figure*}


