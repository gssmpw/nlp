% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{csquotes}
\usepackage{wasysym}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{float}

\input{math_commands}

\usepackage{amsmath}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{8442} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{FairDropout: Using Example-Tied Dropout to Enhance Generalization for Minority Groups}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\begin{document}
\maketitle
\begin{abstract}
Deep learning models frequently exploit spurious features in training data to achieve low training error, often resulting in poor generalization when faced with shifted testing distributions. To address this issue, various methods from imbalanced learning, representation learning, and classifier recalibration have been proposed to enhance the robustness of deep neural networks against spurious correlations. In this paper, we observe that models trained with empirical risk minimization tend to generalize well for examples from the majority groups while memorizing instances from minority groups.
Building on recent findings that show memorization can be localized to a limited number of neurons, we apply example-tied dropout as a method we term \textit{FairDropout}, aimed at redirecting this memorization to specific neurons that we subsequently drop out during inference. We empirically evaluate FairDropout using the subpopulation benchmark suite encompassing vision, language, and healthcare tasks, demonstrating that it significantly reduces reliance on spurious correlations.
\end{abstract}


\section{Introduction}\label{sec:intro}

Deep neural networks trained with empirical risk minimization (ERM) continue to achieve remarkable performance across various tasks. However, these ERM-trained models often experience a drop in predictive performance when facing a variety of subpopulation shifts~\citep{yang2023change,QuioneroCandela2009Dataset}. In particular, if datasets contain spurious features, i.e., patterns that are highly predictive of the training labels but not causally related to the target, ERM may fail to learn robust features that generalize well across subpopulation shifts~\citep{geirhos2020shortcut}. ERM-trained models may thus rely on non-robust features, such as image backgrounds or hair colors, for predictions. However, in certain applications, reliance on such spurious features (e.g., hair colors) can hurt the \textit{fairness}, raising potential safety concerns in deployment~\citep{amodei2016concrete}. 

Several works have been proposed to tackle the challenge of learning more robust features in the presence of spurious correlations. A common practical setup assumes a known group partition within datasets~\citep{liu2021just,sagawa2019distributionally}. In this context, training labels and spurious features are often highly correlated in a specific group of training data, but this correlation does not hold in testing data. Consequently, models trained naively can easily maximize training performance by relying on spurious features, yet suffer a significant drop in worst-group performance during testing when the spurious correlation fails to hold. 
Most of the existing work in this area assumes the presence of group annotations in the training set to learn more robust-to-spurious-correlation features.
%There exist several works that assume the presence of group annotations in the training set to learn more robust-to-spurious-correlation features.
For instance, GroupDRO~\citep{sagawa2019distributionally} directly minimizes the worst group error. However, such approaches depend heavily on prior knowledge of group labels in training data, which can be impractical in large-scale settings. There exist other methods that do not assume this availability of group annotations on the training set. For example, \cite{kirichenko2022last} observed that ERM captures both \textit{core} (or robust) and spurious features. They then proposed a two-stage approach, known as DFR, in which the first stage involves ERM training, and the second stage re-trains the classifier using a group-balanced validation dataset. While DFR has effectively improved worst-group performance, it still relies on group annotations in the validation set to create the balanced set necessary for down-weighting spurious features. 

In this work, we hypothesize that reducing example-level memorization can reduce the reliance on spurious correlations without the need for group labels.
Building on recent insights connecting memorization with generalization~\citep{baldock2021deep,maini2023can}, we investigate this connection in the context of spurious correlations for the first time. We apply a recent technique for localizing memorization, initially developed for label noise in small networks, and scale it to larger networks in the spurious correlation setting. Our method, termed \textit{FairDropout}, fairly distributes \textit{memorizing neurons} during training and selectively drops out these neurons during inference. Our contributions are threefold: (i) We identify a discrepancy in generalization behavior between majority and minority groups, linking this difference to memorization effects. (ii) We demonstrate that example-tied dropout, previously used in label noise settings with smaller networks~\citep{maini2023can}, can be scaled and applied to larger architectures (e.g., ResNet-50~\citep{he2016deep} and BERT~\citep{sung2019pre}) in the context of spurious correlation, under the name \textit{FairDropout}. (iii) We evaluate FairDropout on the subpopulation benchmark suite, showing significant improvements in worst-group accuracy across image, medical (X-Ray), and language tasks.


\section{Related Work}\label{sec:related_work}
Methods have been proposed to fight against spurious correlation. 

\subsection{Using Training Group Information} Most methods that fight against spurious correlation assume to have training group annotations. Some approaches directly modify ERM; for example, groupDRO~\citep{sagawa2019distributionally} and its variant CVaRDRO~\citep{duchi2021learning} aim to minimize the worst group error rather than the average error used by ERM. Similarly, when group information is available, methods from out-of-distribution generalization~\citep{arjovsky2019invariant,krueger2021out,wald2021calibration,krueger2021out} can be framed to learn more robust-to-spurious-correlation features. Other approaches use training group information to synthetically augment minority group samples through generative modeling \citep{goel2020model}. Reweighting and subsampling techniques can also be used to balance majority and minority groups~\citep{sagawa2020investigation,byrd2019effect}. However, all these works share a major limitation:  they rely on group information, which is difficult to obtain for large datasets. Indeed, manually annotating group labels requires task-specific expertise, making it prohibitively costly.

\subsection{Without Using Training Group Information} Given the expensive cost of manual group annotation, there has been a growing interest in combating spurious correlation without group annotations in the training set. Some methods observe the training dynamics of SGD and introduce margin-based regularization terms to promote more robust feature learning~\citep{pezeshki2021gradient, puli2023don}. Among the most popular approaches are two-stage algorithms that do not assume prior group annotation for the training set; these methods typically begin with ERM to infer minority group samples. In the second stage, robustness to spurious correlations is enhanced, for instance through contrastive learning~\citep{zhang2022correct} or by up-weighting the inferred minority group samples~\citep{qiu2023simple, liu2021just}. A recent study~\citep{kirichenko2022last} underscores the importance of the first stage ERM, by showing that ERM learns both spurious and \textit{core} (or robust-to -spurious-correlation) features. It then proposes retraining only the classifier head in a second stage using a group-balanced validation set. This approach has been extended to HTT-DFR~\citep{hameed2024not}, where the second phase retrains a sparse network. Inspired by ERM’s ability to capture core features, our work aims to reduce the reliance on spurious correlations by focusing on memorizing neurons, offering a simpler alternative to computationally demanding two-stage methods.
%Reweighting schemes
%Group inference

\subsection{Memorization and Generalization Links} Recent research has made strides in understanding the relationship between generalization and memorization in deep learning. Memorization is often defined as the model's capacity to correctly predict \textit{atypical} examples with potentially wrong patterns~\citep{maini2023can}. Several studies, such as~\cite{jiang2021characterizing, carlini2019distribution}, have introduced metrics to quantify the degree to which an example is regular or atypical. Some initial works observed that memorization primarily occurs in later layers of the network~\cite{baldock2021deep, stephenson2021geometry}, while recent findings from~\cite{maini2023can} suggest that memorization can emerge at any depth within a network. Additionally, \cite{maini2023can} introduces a method for localizing memorization by determining the minimum number of neurons required to change a model’s predictions. Our work builds on this method, applying it beyond the context of label noise, as in the original study, to address spurious correlations. Various studies on mechanistic interpretability further reveal that certain neurons in deep networks specialize in capturing specific patterns, often associated with spurious features~\cite{zenke2017continual,cheung2019superposition,hendel2023context,kalibhat2023identifying}.
%Where do deep neural network memorization
%When do dnn memorize
%How do we 
\section{Methods}
\label{sec:method}
This section begins by outlining the problem, followed by an analysis of minority group example memorization, and then introduces FairDropout.

\subsection{Problem Description}\label{sec:problem_desc}
We consider a classification problem with a training sample $\mathcal{D_{\text{tr}}}=\{(\vx,y)\}_{i=1}^{N}$ drawn from a training distribution $p_{\text{tr}}$, where $\vx_i\in \mathcal{X}$ is the input and $y_i\in \mathcal{Y}$ is its class label. We further assume the existence of a spurious attribute $a \in \mathcal{A}$, which is non-predictive of $y$~\cite{ye2024spurious}. We denote by groups, the pairs  $g:= (y,a) \in \mathcal{Y}\times \mathcal{A}:= \mathcal{G}$. Since $a$ is not predictive in $y$, a correlation between $y$ and $a$ in the training distribution $p_\text{tr}$, but not in the test distribution $p_{te}$ can cause model trained on $\mathcal{D_{\text{tr}}}$ to perform poorly on groups where this correlation does not hold. 

For example, CelebA~\citep{liu2015deep} is one of the most popular datasets in the spurious correlation literature. The common task is to predict the hair color from celebrity faces ($\mathcal{Y}=\{\text{blond hair}, \text{non-blond hair}\}$), gender serves as the spurious attribute ($a\in \{\text{woman}, \text{man}\}$). In the CelebA training set, only $1\%$ of faces are from the group of blond men. Consequently, trained models may rely on the spurious gender feature to predict hair color, and average test performance alone may not fully capture model robustness. Instead, evaluating performance based on worst-group accuracy, such as accuracy on blond men, becomes critical for assessing model reliability.

Formally, given a parameterized model $f_\vtheta: \mathcal{X}\longrightarrow \mathcal{Y} $, the goal of learning in presence of spurious correlation is to find the model that will minimize the worst-group expected error 
\begin{equation}
    \max_{g\in \mathcal{G}} \quad \mathbb{E} \left[\ell_{0-1}\left(f_\vtheta\left(\vx\right), y\right) | g\right],
\end{equation}
where $\ell_{0-1}\left(f_\vtheta(\vx), y\right) = \mathbf{1}\left[f_\vtheta(\vx)\not= y\right]$ is the 0-1 loss~\citep{liu2021just}. We focus on the scenario where group information is unavailable in the training set but accessible at test time for evaluation purposes.  

\begin{figure*}[!t]
\centering
\begin{subfigure}[]{0.45\linewidth}
\includegraphics[width=\textwidth]{worst_group_accuracy_gps.pdf}
\end{subfigure}
\begin{subfigure}[]{0.45\linewidth}
\includegraphics[width=\textwidth]{worst_group_accuracy_celeba.pdf}
\end{subfigure}
\caption{Discrepancy in generalization behaviors between majority groups and minority groups on CelebA. Left: average train and test accuracy are plotted. Right: minimum-group train and test accuracy are plotted. We observe that models trained exhibit a large generalization gap on minority groups, a synonym of minority-group overfitting.}
\label{fig:minority_majority_groups}
\end{figure*}

\subsection{Memorization of Minority Group Examples}
\label{sec:memorization}

We begin our analyses with ERM on CelebA, which is the most popular, real-world, and large dataset frequently used to study spurious correlations, providing insights that generalize to real-world settings. As previously described, CelebA comprises four groups: $\{\text{blond hair}, \text{non-blond hair}\}\times \{\text{woman}, \text{man}\}$, with the minority group being (blond, man). We train ResNet-50~\citep{he2016deep} using ERM and monitor train/test performance across these different groups.

\begin{figure*}[!t]
\centering
\begin{subfigure}[]{0.45\linewidth}
\includegraphics[width=\textwidth]{neuron_memorization_celeba.pdf}
\caption{Number of Neurons to Flip Predictions.}
\label{fig:number_neurons}
\end{subfigure}
\begin{subfigure}[]{0.45\linewidth}
\includegraphics[width=\textwidth]{acc_memorization_celeba.pdf}
\caption{Impact on training Worst-group Accuracy.}
\label{fig:impact_worst_group_accuracy}
\end{subfigure}
\caption{
For each example in a subset of 100 from the minority group and 100 from other groups, we iteratively remove the most critical neurons from
a ResNet-50 model trained on the CelebA dataset, until the example’s prediction flips. (a) We observe that minority-group examples require fewer neurons to flip their prediction. (b) After dropping the most critical neurons from examples in different groups, we report the worst-group accuracy on the training set. We find that the worst-group accuracy is consistently less affected when critical neurons from minority-group examples are dropped, compared to those from other groups. This suggests that minority-group examples are being memorized.}
\label{fig:analyze_memorization}
\end{figure*}

Fig.~\ref{fig:minority_majority_groups} shows both average and worst group performance. It can be observed in Fig.~\ref{fig:minority_majority_groups}, a discrepancy in generalization behaviors between the majority groups (represented by the average performance, but the same trends generalizes to majority groups) and the minority group. Specifically, we observe a \textit{large generalization gap for the minority group}, indicative of overfitting—a point that has not been sufficiently highlighted in prior research.

To investigate the underlying causes of this generalization failure in the minority group, we turn to an analysis of memorization. Indeed, deep neural networks as high-capacity models, are capable of fitting complex and atypical examples, making it reasonable to associate this generalization failure with memorization. Leveraging recent advances in understanding memorization, we employ the method recently proposed by \cite{maini2023can} to detect memorization. This technique identifies the minimum set of neurons (or channels, in the case of convolutional layers) that maintain the correct prediction of a training sample while maximizing the loss on an input where the prediction should be flipped. For a given layer index $l$ and neuron index $j$, neurons are denoted $z^{(l,j)}$. The technique of \cite{maini2023can} performs an iterative search across layers to locate neurons that enable flipping an example’s prediction (indicated by high cross-entropy loss on the example) while preserving predictions on a reference sample (reflected in low cross-entropy loss on this sample).
More formally, for an input $\vx_i$, this is technically done by sequentially computing for each iteration (per layer)
\begin{align}
    z^{(l^*,j^*)} \in \argmax_{l,j} \Biggl[& \nabla_{\vtheta_l}\left(\mathcal{L} \left(f_{\hat{\vtheta}}(\vx_i), y_i\right) \right. \nonumber  \\
    &\left. - \frac{1}{|\mathcal{B}|} \sum_{(\vx, y) \in \mathcal{B}} \mathcal{L}\left(f_{\hat{\vtheta}}(\vx), y \right)\right)
    \Biggl]_j,
\end{align}

\begin{figure}
%\vspace{-17pt}
\centering
\includegraphics[width=.5\textwidth]{draw_memorized.drawio.pdf}
\caption{Effect on the test worst-group accuracy when dropping memorizing neurons as shown in \ref{fig:analyze_memorization}. For each example in the minority-group sample, we drop their most critical neurons (memorizing neurons in this case), and report the measured test worst-group accuracy. From the quartiles on this figure, we observe that in $\approx 75\%$, of cases dropping out \textit{memorizing} neurons improves test worst-group accuracy.
  %  \vspace{-12pt}
  }
\label{fig:droping_out_neurons}
\end{figure}
where $\mathcal{B}$ is the random batch on which the predictions have to be conserved, $f_{\hat{\vtheta}}$ is the current iteration of the modified model (model on which a neuron was dropped in the previous iteration), $\vtheta_l$ are parameters of the layer indexed by $l$, and $[.]_j$ represents the coordinate $j$ of the vector. The sequential procedure continues until the prediction of $\vx_i$ is flipped. The final neurons $z^{(l^*,j^*)}$ are seen as the most critical neurons uniquely associated with the given example. \cite{maini2023can} demonstrate that the proportion of these neurons can be used to detect memorized examples.

We conduct this experiment to analyze the memorization behaviors in the context of spurious correlation on the CelebA dataset. Fig.~\ref{fig:analyze_memorization} shows the number of neurons required to flip the prediction of each example of a subsample of majority and minority groups. In general, we observe that, (i) the number of neurons required to flip each prediction from the minority group is considerably lower than the corresponding number for majority groups (see Fig.~\ref{fig:number_neurons}). Furthermore, as shown in Fig.~\ref{fig:impact_worst_group_accuracy}, (ii) these neurons have a smaller effect on training worst-group accuracy compared to those from the majority group. Referring to similar analyses in \cite{maini2023can}, (i) and (ii) indicate that minority group examples are more prone to memorization issues. 

While we have identified generalization problems and memorization issues within the minority group, there is no direct evidence suggesting a causal link between the two phenomena in the context of spurious correlation. To investigate this potential link, we conducted a new experiment to measure changes in test worst-group accuracy after dropping critical neurons of examples from the minority group. 

Fig.~\ref{fig:droping_out_neurons} shows the test worst-group accuracy  after individually dropping neurons for each example of the minority group.  We observe that for approximately $75\%$ of these drops, the worst-group accuracy significantly improves. This suggests that most of the neurons identified as memorizing neurons are detrimental to minority-group generalization.

\begin{figure*}[!t]
\centering
\begin{subfigure}[]{0.32\linewidth}
\includegraphics[width=\textwidth]{fairdropout_celeba_avg.pdf}
\end{subfigure}
\begin{subfigure}[]{0.32\linewidth}
\includegraphics[width=\textwidth]{fairdropout_celeba_worst.pdf}
\end{subfigure}
\begin{subfigure}[]{0.32\linewidth}
\includegraphics[width=\textwidth]{fairdropout_celeba_worst_without.pdf}
\end{subfigure}
\caption{Training with FairDropout on CelebA. Left: train/test average are ploted in the testing mode. Center and right: train/test worst-group accuracy with FairDropout are plotted. Training and testing mode respectively refer to the evaluation without dropping memorizing neurons, and after dropping them. We observe that dropping out these memorizing neurons has the benefit of improving worst-group accuracy.}
\label{fig:faidropout_effect}
\end{figure*}

\subsection{The ExampleTiedDropout as a FairDropout}
After observing that certain neurons are closely tied to specific examples--particularly the memorizing ones in the minority group-- and that dropping these neurons positively impacts minority group generalization, it becomes crucial to leverage this insight by directing this memorization to fixed neurons. Drawing inspiration from the example-tied dropout introduced in the context of label noise by \cite{maini2023can} for small networks (such as ResNet-9) and smaller datasets (like MNIST~\citep{deng2012mnist} or CIFAR-10~\citep{krizhevsky2009learning}), we introduce FairDropout, an example-tied dropout method specifically designed to address spurious correlation. Unlike the original example-tied dropout, FairDropout can be applied not only after any intermediate layers but also after newly added projection layers before the linear head.

As an example-tied dropout, FairDropout is a layer without learnable parameters, that divides neurons into two types, governed by two hyper-parameters: $p_\text{gen}$ and $p_\text{mem}$. The first set of neurons contains the generalizing neurons, which are seen by every example in the dataset. If the preceding layer of the FairDropout has the size $H$, then $p_\text{gen}H$ neurons are designated as generalizing. The remaining $(1-p_\text{gen})H$ neurons are considered as memorizing neurons and each example is allocated a set of memorizing neurons uniformly sampled with probability $p_\text{mem}$ from the $(1-p_\text{gen})H$ memorizing neurons. The \textit{fair} prefix comes from the fact that every example allocates the same fixed number of memorizing neurons. As depicted in Fig.~\ref{fig:fairdropout}, during training, given an example, and layer features as inputs of the FairDropout, the FairDropout outputs its generalizing features and its example-wise memorizing ones. In this case, each image allocates only one memorizing neuron. During testing, the memorizing ones are dropped. Finally, we observe that when $p_\text{gen}=1$ the FairDropout is just an identity function and trained models correspond to ERM-trained models.

\section{Experimental Results}
\label{sec:experiments}
We conduct experiments to evaluate the FairDropout on CelebA as a sanity check and on a benchmark suite.

\subsection{Warm-up on CelebA: FairDropout Balances Group Accuracy}
We applied the FairDropout after the third residual block on ResNet-50, with the hyperparameters $p_\text{mem} = p_\text{gen} = 0.2$, and monitor the train/test average/worst-group accuracy. 

Fig.~\ref{fig:faidropout_effect} shows the evolution of the train/test average and worst-group accuracy throughout epochs. It can be observed on the right-most plot that evaluating accuracy in FairDropout's training mode (where memorizing neurons are retained) yields results nearly identical to those without FairDropout, effectively mirroring ERM behavior: the worst-group accuracy plateaus around $45\%$, similar to Fig.~\ref{fig:minority_majority_groups}.

In contrast, in FairDropout's testing mode (where memorizing neurons are dropped), the worst-group accuracy does not plateau at $45\%$ but instead stabilizes around $80\%$. Thus, dropping memorizing neurons after training with FairDropout clearly boosts worst-group accuracy. In the next section, we compare FairDropout against state-of-the-art methods for handling spurious correlation.

\begin{figure*}[!t]
\centering
\includegraphics[width=.8\textwidth]{teaser.drawio.pdf}
\caption{Example-Tied Dropout as a FairDropout. The FairDropout redirects the example memorization on specific neurons. Memorizing neurons are uniformly allocated to training examples during training. During testing, these memorizing neurons are dropped.}
\label{fig:fairdropout}
\end{figure*}

\begin{table*}[!h]
\centering
\caption{Dataset overview with modality, number of attributes, classes, train, validation, and test set sizes, and group distributions.}
\label{tab:datasets}
%\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\textbf{Dataset} & \textbf{Data} & \textbf{$|\mathcal{A}|$} & \textbf{$|\mathcal{Y}|$} & \textbf{$|\mathcal{D}_\text{tr}|$} & \textbf{$|\mathcal{D}_\text{val}|$}  & \textbf{$|\mathcal{D}_\text{test}|$}  & \textbf{Max group (\%)} & \textbf{Min group (\%)} \\
\midrule
Waterbirds & Image & 2 & 2 & 4795 & 1199 & 5794 & 3498 (73.0\%) & 56 (1.2\%) \\
CelebA & Image & 2 & 2 & 162770 & 19867 & 19962 & 71629 (44.0\%) & 1387 (0.9\%) \\
MetaShift & Image & 2 & 2 & 2276 & 349 & 874 & 789 (34.7\%) & 196 (8.6\%) \\

MultiNLI & Text & 2 & 3 & 206175 & 82462 & 123712 & 67376 (32.7\%) & 1521 (0.7\%) \\
MIMIC-CXR & X-rays & 6 & 2 & 303591 & 17859 & 35717 & 68575 (22.6\%) & 7846 (2.6\%)  \\
\bottomrule
\end{tabular}
%}
\end{table*}
\subsection{Benchmarking FairDropout with Baselines}
Before presenting the results of the comparison between baselines, we present the experimental setup used largely inspired from \cite{yang2023change}.
\subsubsection{Experimental Setup}
\label{sec:setup}

We use the recently proposed subpopulation shift library and benchmark suite~\citep{yang2023change} that implements the state-of-the-art methods in spurious correlation.%~\footnote{Code is available at this ANONYMOUS LINK.}. 

We use 5 diverse datasets that are very used in spurious correlation litterature~\citep{yang2023change}. Characteristics of these datasets are summarized in Table~\ref{tab:datasets}.

\textbf{CelebA.} As introduced in Sec.~\ref{sec:problem_desc}, CelebA~\citep{liu2015deep} is one of the largest, real-world image datasets used in the context of spurious correlation. It has around 200,000 celebrity face images. The task, in the spurious correlations literature, is to predict the hair color of persons (blond vs. non-blond) and the spurious correlation is the gender. 

\textbf{MetaShift.} The dataset Metashift~\citep{liang2022metashift} that we use here is an image dataset that was built by \citet{yang2023change}. The goal is to distinguish between the two animals (cats vs dogs). The spurious attribute is the image background. Cats are more likely to be indoors, while
dogs are more likely to be outdoors.

\textbf{Waterbirds.} Waterbirds~\cite{wah2011caltech} is a well-known \textit{synthetic} image dataset for binary classification. The task is to classify whether a bird is a landbird or a waterbird. The spurious attribute is the background (water or land). There are therefore 4 groups that are from $\{\text{landbird}, \text{waterbird}\}\times\{\text{water background, land background}\}$. 

\textbf{MultiNLI.} The MultiNLI dataset~\citep{liang2022metashift} is a text dataset very used in spurious correlation literature. The target is the natural language relationship between the premise and the hypothesis. It has three classes (neutral, contradiction, or entailment). The spurious attribute is a variable that tells whether negation appears in the text or not. Indeed, negation is highly correlated with the contradiction label.

\textbf{MIMIC-CXR.} MIMIC-CXR~\citep{johnson2019mimic} is a chest X-ray dataset, where its approximately 300,000 images come from the Beth Israel Deaconess
Medical Center from Boston, Massachusetts. We use the setting of \cite{yang2023change}, where the label is “No Finding” as the label. The positive class means that the patient is not ill. the spurious attribute domain is the cross-product of race (White, Black, Other) and gender.

All the data preprocessing and train/val/test splits are directly adopted from~\cite{yang2023change} as we implement our method in their library. 

\textbf{Models.} As in the benchmark~\citep{yang2023change}, we use the Pytorch pretrained ResNet-50 models for image datasets and BERT~\cite{sung2019pre} for the MultiNLI text datasets.

\textbf{Metrics.} According to most previous works, we evaluate the reliance on spurious correlation through worst-group accuracy.

\begin{table*}[!t]
\caption{Comparison of the FairDropout against state-of-the-art methods when spurious attribute annotations or group annotations are unknown in both train and validation. Test worst-group accuracy is reported and is obtained from the subpopulation shift benchmark~\cite{yang2023change}. The symbol $\circ$ indicates that the original method requires group information for the training whereas $\bullet$ means that it requires group information for the validation.}
\label{tab:results}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|l|ccccc}
\hline
Method Types & Algorithm & CelebA & MetaShift  & Waterbirds & MultiNLI & MIMIC-CXR \\
\hline
standard &ERM &  69.1 $\pm$4.7 & 82.1 $\pm$0.8 & 57.6 $\pm$0.8  & 66.4 $\pm$2.3 & 68.6 $\pm$0.2 \\
\hline
Data augmentation &Mixup  & 57.8 $\pm$0.8 & 79.0 $\pm$0.8 & \underline{77.5} $\pm$0.7 & \underline{66.8} $\pm$0.3 & 66.8 $\pm$0.6 \\
\hline
\multirow{6}*{Spurious correlation}
&\textbf{FairDropout (ours)}   & \textbf{75.6} $\pm$ 2.1 & \textbf{85.9} $\pm$ 1.1 & 70.6 $\pm$ 0.2 & \textbf{70.3} $\pm$ 2.4 & \textbf{70.6} $\pm 0.6$ \\
&$\circ$ GroupDRO &  68.3 $\pm$0.9 & 83.1 $\pm$0.7 & 73.1 $\pm$0.4 & 64.1 $\pm$0.8 & 67.4 $\pm$0.5 \\
&$\circ$ CVaRDRO &  60.2 $\pm$3.0 & 83.5 $\pm$0.5 & 75.5 $\pm$2.2 & 48.2 $\pm$3.4 & 68.0 $\pm$0.2 \\
&JTT &  48.3 $\pm$1.5 & 82.6 $\pm$0.4 & 71.2 $\pm$0.5 & 65.1 $\pm$1.6 & 64.9 $\pm$0.3 \\
&LfF &  53.0 $\pm$4.3 & 72.3 $\pm$1.3 & 75.0 $\pm$0.7 & 57.3 $\pm$5.7 & 62.2 $\pm$2.4 \\
&$\circ$LISA & 57.8 $\pm$0.8 & 79.0 $\pm$0.8 & 77.5 $\pm$0.7 & 66.8 $\pm$0.3 & 66.8 $\pm$0.6 \\
\hline
\multirow{7}{*}{Imbalanced learning} &ReSample & \underline{74.1} $\pm$2.2 & 81.0 $\pm$1.7 & 70.0 $\pm$1.0 & 66.8 $\pm$0.5 & 67.5 $\pm$0.3 \\
&ReWeight & 69.6 $\pm$0.2 & 83.1 $\pm$0.7 & 71.9 $\pm$0.6  & 64.2 $\pm$1.9 & 67.0 $\pm$0.4 \\ 
&SqrtReWeight & 66.9 $\pm$2.2 & 82.6 $\pm$0.4 &   71.0 $\pm$1.4 & 63.8 $\pm$2.4 & 68.0 $\pm$0.4 \\
&CBLoss &  65.4 $\pm$1.4 & 83.1 $\pm$0.0 & 74.4 $\pm$1.2 & 63.6 $\pm$2.4 & 67.6 $\pm$0.3 \\
&Focal &  56.9 $\pm$3.4 & 81.0 $\pm$0.4  & 71.6 $\pm$0.8 & 62.4 $\pm$2.0 & 68.7 $\pm$0.4 \\
&LDAM &  57.0 $\pm$4.1 & 83.6 $\pm$0.4 & 70.9 $\pm$1.7 & 65.5 $\pm$0.8 & 66.6 $\pm$0.6 \\
&BSoftmax & 69.6 $\pm$1.2 & 82.6 $\pm$0.4 &  74.1 $\pm$0.9 & 63.6 $\pm$2.4 & 67.6 $\pm$0.6 \\
\hline
\multirow{3}*{classifier retraining}
&$\bullet$DFR & 73.7 $\pm$0.8 & 81.4 $\pm$0.1 &  \textbf{89.0} $\pm$0.2 & 63.8 $\pm$0.0 & 67.1 $\pm$0.4 \\
% &FairDropoutDFR (ours) & IF$\pm$ 0.0  & TIME $\pm$ 0.0 & ALL $\pm$ 0.0 & OWS $\pm$ 0.0 & \\
&CRT & 69.6 $\pm$0.7 & 83.1 $\pm$0.0 &  76.3 $\pm$0.8 & 65.4 $\pm$0.2 & 68.1 $\pm$0.1 \\
&ReWeightCRT & 70.7 $\pm$0.6 & \underline{85.1} $\pm$0.4 & 76.3 $\pm$0.2 & 
 65.2 $\pm$0.2 & 67.9 $\pm$0.1 \\
\hline
\end{tabular}%
}
\end{table*}

\textbf{Baseline methods.} We compare the FairDropout with state-of-the-art algorithms implemented in the subpopulation shift benchmark. Our work does not need the knowledge of group information. We thus evaluate our method in the setting where we do not have group information. However, methods that need group information have been converted by \cite{yang2023change} to an equivalent method by considering class information instead of group. For example, GroupDRO can be converted by an equivalent goal of minimizing worst-class error. Benchmarked methods from the spurious correlation literature  include GroupDRO~\citep{sagawa2019distributionally}, CVaRDRO~\citep{duchi2021learning}, JTT~\citep{liu2021just}, LfF~\citep{nam2020learning}, LISA~\cite{yao2022improving}. There are also two-phase methods that retrain the classifier, which are DFR~\citep{yao2022improving} (retraining is done on the \textit{validation} set), CRT and its variant ReWeightCRT~\citep{kangdecoupling}. Finally, we also include methods that are mostly designed for the imbalanced learning problem, which are ReSample~\citep{japkowicz2000class}, ReWeight~\citep{japkowicz2000class}, SqrtReWeight~\citep{japkowicz2000class}, CBLoss~\citep{cui2019class}, LDAM~\cite{cao2019learning} and BSoftmax~\citep{ren2020balanced}. \textbf{Note that the FairDropout technique can be combined with any of these baseline methods to boost its performance}.

\textbf{Hyperparameter tuning.} As we consider the most difficult case we do not have group information for the training and validation sets, similarly with the benchmark \cite{yang2023change}, we tune the $p_\text{mem}, p_\text{gen}$, learning rate, and weight decay with the worst-class accuracy. We use the SGD optimizer with weight decay.

\textbf{Positions of the FairDropout Layers.}
In principle, the FairDropout layer can be placed after any intermediate layer in the network. However, in large-scale, potentially pre-trained models, the placement of FairDropout may require careful consideration. In models with skipped connections as in ResNet-50, in our settings, we consider possible positions after residual blocks. In BERT-like models, we propose adding a new linear layer before the classifier head and positioning the FairDropout layer there. This ensures that the pre-trained features are preserved while FairDropout controls memorization during fine-tuning. The optimal placement, however, depends on the dataset, as spurious correlations exhibit task-specific levels of abstraction. Therefore, we tune the position of FairDropout along with other optimization hyperparameters using worst-class accuracy as a guiding metric.

\subsubsection{Results and Discussion}
We report the worst-group accuracy results obtained after running our FairDropout method on the subpopulation shift library, averaged over 5 independent runs. Table~\ref{tab:results} presents these results with methods categorized according to the presentation done in Sec.~\ref{sec:setup}, following \cite{yang2023change}. As a reminder, in this setting, the spurious attribute and the group annotations are \textbf{unavailable in both the training and validation datasets}. All the methods or their adapted version are tuned with worst-class accuracy and the results come from \cite{yang2023change}.

From the table, we can make the following observations. In all datasets and except Waterbirds, our FairDropout method outperforms ERM by a large margin. 

On these datasets, we can also observe that the FairDropout outperforms or has comparable performance to spurious correlation methods and imbalance learning methods. More specifically, the datasets on which FairDropout achieves the most successful results are MultiNLI ($70.3 \pm 2.4$) and MIMIC-CXR ($70.6\pm 0.6$).

On CelebA and MetaShift, although our FairDropout technique outperforms its competitors from the same family (spurious correlation methods), its performance is comparable with the Resample on CelebA ($75.6\pm2.1$ vs $74.1\pm 2.2$) and ReWeightCRT on Metashift ($85.9\pm 1.1$ vs $85.1\pm 0.4$). It is worth mentioning that our models with the FairDropout are trained with classic cross-entropy, meaning that the performance of \textbf{our FairDropout technique can be further boosted} with any of these existing imbalanced learning or classifier retraining methods. 

On the Waterbirds dataset, while FairDropout improves upon ERM, it performs below classifier retraining methods and some imbalanced learning techniques. Since Waterbirds is a dataset \textit{synthetically} generated by placing bird objects into different backgrounds, it has already been observed that ImageNet pre-trained ImageNet features can be effectively transferred~\citep{izmailov2022feature} without finetuning the entire model, which may explain the superior performance of DFR. As already mentioned, FairDropout can also be combined with DFR, in particular for datasets like in Waterbirds, where pre-trained features provide a strong foundation of robust features for the downstream task.

Overall, the results obtained demonstrate the effectiveness of FairDropout in reducing reliance on spurious correlations without requiring explicit group annotations. Its performance may also benefit from additional boosts when combined with classifier retraining or imbalanced learning methods.
% \begin{wrapfigure}{R}{0.47\textwidth}
% %\vspace{-17pt}
% \centering
% \includegraphics[width=.5\textwidth]{worst_group_accuracy_celeba.pdf}
% \caption{Minority group overfitting. We observe a worst-group generalization gap synonym of overfitting in CelebA.
%   %  \vspace{-12pt}
%   }
% \label{fig:majority_class_overfitting}
% \end{wrapfigure}




% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}


\section{Limitations and Conclusion}

In this paper, we explored, for the first time, the lack of generalization of minority-group examples and its connection to memorization in the context of spurious correlations. To address this, we introduced FairDropout, an example-tied dropout technique that can be applied for large pre-trained networks, aimed at reducing reliance on spurious features.

FairDropout enables the localization of memorization by directing spurious features to specific neurons that, when dropped during inference, can enhance worst-group accuracy. Through empirical evaluation, we demonstrated that FairDropout outperforms several baseline methods across diverse datasets spanning image, medical, and language tasks.

Nonetheless, our study has certain limitations. First, prior research has shown that memorization can sometimes be beneficial for generalization~\citep{feldman2020does}, an aspect that warrants further investigation, particularly for certain applications and datasets. Second, while FairDropout implicitly assumes that \textit{generalizing} neurons are less likely to memorize examples—since memorization is predominantly managed by the designated memorizing neurons—this hypothesis requires further exploration, which lies beyond the scope of this paper.
{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

\input{X_suppl}
% WARNING: do not forget to delete the supplementary pages from your submission 

\end{document}
