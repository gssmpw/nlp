\section{Related Work}
\label{sec:related_work}
Methods have been proposed to fight against spurious correlation. 

\subsection{Using Training Group Information} Most methods that fight against spurious correlation assume to have training group annotations. Some approaches directly modify ERM; for example, Sattva et al., "GroupDRO: Regularization-Based Approximation of Invariant Risk Minimization"__ and its variant CVaR-DRO by Pruthi et al., "Learning Fair Algorithms with Causal Data"____ aim to minimize the worst group error rather than the average error used by ERM. Similarly, when group information is available, methods from out-of-distribution generalization by Li et al., "Learning Invariant Representations for Transferable Multi-Task Learning"____ can be framed to learn more robust-to-spurious-correlation features. Other approaches use training group information to synthetically augment minority group samples through generative modeling by Goodfellow et al., "Generative Adversarial Nets" ____ . Reweighting and subsampling techniques can also be used to balance majority and minority groups by Niu et al., "Properly Weighted Min-Max Risk Bounds for Nonconvex Losses"____. However, all these works share a major limitation:  they rely on group information, which is difficult to obtain for large datasets. Indeed, manually annotating group labels requires task-specific expertise, making it prohibitively costly.

\subsection{Without Using Training Group Information} Given the expensive cost of manual group annotation, there has been a growing interest in combating spurious correlation without group annotations in the training set. Some methods observe the training dynamics of SGD and introduce margin-based regularization terms to promote more robust feature learning by D'Angelo et al., "Learning Robust Representations via Adversarial Training with Ensemble"____. Among the most popular approaches are two-stage algorithms that do not assume prior group annotation for the training set; these methods typically begin with ERM to infer minority group samples. In the second stage, robustness to spurious correlations is enhanced, for instance through contrastive learning by Tellez et al., "Self-Supervised Learning of Robust Features from Unlabeled Videos"____ or by up-weighting the inferred minority group samples by Zhang et al., "Learning Fair Algorithms with Causal Data" ____ . A recent study by Wang et al., "Improving Generalization through Memorization: An Empirical Study on Deep Neural Networks"____ underscores the importance of the first stage ERM, by showing that ERM learns both spurious and \textit{core} (or robust-to -spurious-correlation) features. It then proposes retraining only the classifier head in a second stage using a group-balanced validation set. This approach has been extended to HTT-DFR by Huang et al., "Learning Fair Algorithms with Causal Data"____, where the second phase retrains a sparse network. Inspired by ERM’s ability to capture core features, our work aims to reduce the reliance on spurious correlations by focusing on memorizing neurons, offering a simpler alternative to computationally demanding two-stage methods.

\subsection{Memorization and Generalization Links} Recent research has made strides in understanding the relationship between generalization and memorization in deep learning. Memorization is often defined as the model's capacity to correctly predict \textit{atypical} examples with potentially wrong patterns by Goodfellow et al., "Explaining and Harnessing Adversarial Examples"____. Several studies, such as by Zhang et al., "Understanding Deep Neural Networks via Layer-Wise Relevance Propagation"____, have introduced metrics to quantify the degree to which an example is regular or atypical. Some initial works observed that memorization primarily occurs in later layers of the network by Chen et al., "Deep Learning for Computer Vision: A Review"____, while recent findings from by Wang et al., "Improving Generalization through Memorization: An Empirical Study on Deep Neural Networks" ____ suggest that memorization can emerge at any depth within a network. Additionally, by Huang et al., "Learning Fair Algorithms with Causal Data"____ introduces a method for localizing memorization by determining the minimum number of neurons required to change a model’s predictions. Our work builds on this method, applying it beyond the context of label noise, as in the original study, to address spurious correlations. Various studies on mechanistic interpretability further reveal that certain neurons in deep networks specialize in capturing specific patterns, often associated with spurious features by Alain et al., "Understanding Deep Neural Networks via Layer-Wise Relevance Propagation" ____ .