\section{Introduction} 

Game theory provides a mathematical framework for analyzing strategic interactions among rational agents and has evolved significantly since its seminal work~\cite{von2007theory}. 
Over the decades, it has established robust methodological foundations, including equilibrium analysis~\cite{nash1950equilibrium} and mechanism design~\cite{vickrey1961counterspeculation}, which serve as essential analytical tools across disciplines such as economics and computer science.

With the rapid advancement of large language models (LLMs), researchers have increasingly explored the intersection of game theory and LLMs.
A growing body of work investigates how game-theoretic principles can be used to evaluate and enhance LLMs and how LLMs can contribute to game theory. 
Specifically, existing studies apply game theory to develop theoretical frameworks for assessing LLMs' strategic reasoning. 
This approach optimizes their training methodologies and analyzes their societal implications. 
Key research directions include:
\begin{itemize}
    \item Standardized Game-Based Evaluation: Researchers are constructing benchmark environments, such as matrix games~\cite{akata2023playing} and auctions~\cite{chen2023put}, to evaluate the strategic reasoning capabilities of LLMs systematically.
    \item Game-Theoretic Algorithmic Innovation: Concepts from cooperative and non-cooperative game theory, such as Shapley Value~\cite{enouen2023textgenshap} and max-min equilibria~\cite{NLHF}, are inspiring novel approaches to model interpretability and training optimization.
    \item Societal Impact Modeling: As LLMs transform information ecosystems, new theoretical frameworks are emerging to predict the societal consequences of human-AI interactions~\cite{yaohuman}, particularly in domains like advertising markets~\cite{duetting2024mechanism} and content creation~\cite{fish2023generative}.
\end{itemize}
Beyond these applications, recent research suggests that LLMs can also contribute to game theory by facilitating equilibrium analysis in complex text-based scenarios and extending classical models to more realistic settings.

Existing surveys~\cite{zhang2024llm,feng2024survey,hu2024survey} primarily examine how game theory can be used to build evaluation environments and assess LLMs' strategic performance. 
For instance, \cite{zhang2024llm} classifies studies based on the game scenarios used to test LLM capabilities and methods for improving their reasoning. 
Meanwhile, \cite{feng2024survey} and~\cite{hu2024survey} categorize the core competencies required for LLM-based agents in games, such as perception, memory, role-playing, and reasoning.
While these surveys provide valuable insights, they primarily focus on the role of game theory in standardized evaluation frameworks, overlooking its broader potential for advancing LLM development. 
Moreover, they adopt a unidirectional perspective, treating game theory as a tool for assessing LLMs rather than exploring the reciprocal influence between the two fields.

This paper aims to \emph{bridge this gap by examining the bidirectional relationship between game theory and LLMs}.
\emph{We categorize the work in the intersection between game theory and LLMs into three key perspectives}, as illustrated in Figure~\ref{fig:taxonomy}. 
To the best of our knowledge, this is \emph{the first comprehensive analysis of the bidirectional relationship between these two fields}.

In Section~\ref{sec:llm_play_game}, we review studies that apply game models to evaluate LLMs' decision-making capabilities.
Experiments conducted on both canonical matrix games and complex strategic scenarios reveal LLMs' strengths and limitations as game players. 
Beyond behavioral evaluations, we identify key strategies for enhancing LLMs' strategic decision-making, such as recursive reasoning frameworks and the integration of LLMs with auxiliary modules. 
Moreover, LLMs demonstrate the ability to formalize real-world scenarios into structured game models, extending game-theoretic analysis to broader and more complex contexts.

Section~\ref{sec:game_for_llm} examines how game-theoretic principles address key challenges in LLM development. We categorize existing research into two main areas: (1) using game theory to understand LLMs' text generation and training dynamics and (2) leveraging game-theoretic mechanisms to enhance LLM training algorithms.
The first category explores how the Shapley Value improves model interpretability and how social choice theory facilitates preference alignment in human-AI interactions. 
The second category introduces studies that incorporate game-theoretic objectives to tackle challenges like heterogeneity and complexity in human preferences. The objectives include minimizing regret in multi-agent interactions and evaluation metrics, including Nash equilibrium convergence,

In Section~\ref{sec:llm_game_scenarios}, we discuss how game theory is used to predict and characterize the societal impact of LLMs. 
The human-AI interaction game model predicts the impact of competition between humans and AI. 
Emerging game models highlight the growing business and economic implications where LLMs are treated as products or platforms. 
Meanwhile, classic game theory models are also generalized to more realistic settings with LLMs' unique capabilities, such as natural language manipulation.

Finally, we identify key research challenges and future directions across these dimensions.
By systematically analyzing the intersection of game theory and LLMs, we highlight their mutual influence and how they drive progress in both fields, contributing to the advancement of this interdisciplinary domain.

\input{sections/_taxonomy}

\section{Game Theory for LLM Evaluation}\label{sec:llm_play_game}
In this section, we explore the integration of LLMs within the context of game theory, focusing on their evaluation as game players.
Behavioral evaluations reveal that LLMs face challenges in identifying optimal actions in classic matrix games, yet they can demonstrate human-like strategies in more complex game scenarios. 
Several studies have explored methods to enhance LLMs' performance as game players, and two significant points can be identified from that: recursive thinking and auxiliary modules. 
Finally, we also discuss the role of LLMs in games beyond their function as players.

\subsection{Evaluation of LLMs' Behavioral Performance}\label{sec:llm_play_game_obs}
\paragraph{Struggles of LLM in Matrix Games.}
Matrix games are a fundamental concept in game theory. 
In a matrix game, two players make simultaneous decisions, and the outcomes can be represented by a finite payoff matrix. 
Recent studies have investigated how LLMs respond to these games by converting them into natural language prompts. 
Despite significant advancements, their results show that LLMs such as GPT-4 struggle to consistently select the optimal strategy in $2 \times 2$ matrix games~\cite{akata2023playing,herr2024large,lore2024strategic,wang2024tmgbench}. 

For instance, \cite{akata2023playing} states that LLMs frequently fail to choose the optimal action in coordination games, like the Battle of the Sexes. 
Similarly, \cite{lore2024strategic} examines how contextual framing and utility matrices influence LLM decision-making, revealing significant biases. 
Furthermore, \cite{herr2024large} explores the impact of game descriptions, player positioning, and payoffs on LLM performance, highlighting consistent behavioral biases.
In more dynamic settings, \cite{fan2024can} observes that LLMs struggle to predict optimal strategies in ring-network games. Additionally, the TMGBench benchmark, which evaluates LLMs across 144 distinct $2 \times 2$ matrix games, further confirms these limitations~\cite{wang2024tmgbench}.

The matrix game is a cornerstone of game theory and is the foundation for more intricate strategic challenges. Studying LLMs' behaviors in such games provides valuable insights into their broader limitations in complex reasoning tasks.

\paragraph{Human-like Strategies of LLM in Realistic Game Scenarios.}
Beyond classic matrix games, numerous studies have analyzed LLM performance in more realistic game settings. While these games feature greater contextual complexity, they are not necessarily more challenging for LLMs. This is because strategic inference based on textual content can sometimes replace explicit computation.

Research indicates that LLMs can exhibit strategic behavior in communication-based games. In deception and negotiation games, including  Werewolf~\cite{xu2023exploring,du2024helmsman} and Avalon~\cite{wang2023avalon,lan2023llm}, LLMs demonstrate behaviors such as deception, trust-building, and leadershipâ€”traits typically associated with human strategic thinking. These findings suggest that LLMs can function as sophisticated communication agents in games.

LLMs have also demonstrated strategic reasoning in economically significant scenarios such as bargaining and pricing games. For instance, \cite{llmbargaining} finds that LLMs possess advanced negotiation skills, while \cite{fish2024algorithmic} shows that LLM-based pricing agents can autonomously engage in collusion to set prices above competitive levels. In auction contexts, \cite{guo2024economics} finds that LLMs can formulate rational bidding strategies based on historical data, often converging toward a Nash equilibrium. Similarly, \cite{chen2023put} introduces AucArena, a platform where LLMs effectively manage budgets and optimize auction strategies.

\paragraph{Comprehensive Benchmarks for Game Performance.}
Several benchmarks that cover a diverse range of game scenarios have been developed to make a systematic assessment of LLM. 
Notable examples include GTBench~\cite{duan2024gtbench}, $\gamma$-bench~\cite{huang2024far}, GameBench~\cite{hua2024game}, GLEE~\cite{shapira2024glee}, and TMGBench~\cite{wang2024tmgbench}. 
These benchmarks serve to identify both the strengths and weaknesses of LLMs in different strategic environments, offering valuable insights into their potential improvements and real-world applications.


\subsection{Enhancing LLMs' Game Performance} \label{sec:llm_play_game_align}
Building on the evaluation of LLMs' performance in various games, numerous studies have explored methods to enhance their strategic reasoning and decision-making. These works address key challenges LLMs face in gameplay and propose general frameworks for improving their capabilities. Below, we outline two significant approaches.

\paragraph{Recursively Thinking.}
In games requiring long-term or multi-level reasoning, LLMs often struggle to retain and build upon previous information, leading to suboptimal decision-making. To mitigate this, researchers have developed techniques that encourage LLMs to engage in recursive thinking, enabling them to leverage past information better when formulating strategies.

For instance, \cite{wang2023avalon} introduces the Recursive Contemplation (ReCon) framework. The framework prompts LLMs to engage in first-order and second-order perspective-taking during Avalon gameplay. This helps them avoid common pitfalls, such as deception. Similarly, \cite{duan2024reta} proposes a method where LLMs predict future moves in multi-turn games, improving their ability to anticipate opponents' strategies.
Additionally, \cite{zhang2024k} advances LLM reasoning through k-level rationality, which enhances multi-level thinking and significantly increases their win rates in competitive settings. These findings suggest that recursive reasoning can substantially improve LLMs' strategic capabilities.

\paragraph{Auxiliary Modules.}
As language models, LLMs often struggle in games that require complex mathematical calculations or historical data retrieval. Several studies have proposed integrating auxiliary modules that assist LLMs during gameplay to overcome these limitations.

For example, \cite{gandhi2023strategic} introduces a ``prompt compiler'', which systematically guides LLMs in evaluating actions and forming beliefs, enabling them to generalize to new scenarios with minimal in-context learning. In the game Werewolf, \cite{xu2023exploring} integrates an additional BERT model to encode both historical and current game states, allowing LLMs to make more informed decisions.

In bargaining games, the OG-Narrator framework~\cite{xia2024measuring} generates external offers, allowing the LLM to focus solely on negotiation language. More recently, \cite{hua2024game} developed a structured workflow to assist LLMs in solving game-theoretic problems, including computing Nash equilibria and optimizing strategies in complex negotiation tasks.

These auxiliary modules significantly improve LLMs' performance in various game settings, demonstrating that integrating additional computational tools can enhance their strategic decision-making.

\subsection{Beyond as a Game Player} \label{sec:llm_play_game_platform}

While much of the discussion has been centered on utilizing game-based scenarios to evaluate LLMs, work has also shown that LLM capability in games can, in turn, contribute to game theory. 
This section explores alternative roles for LLMs within game-theoretic contexts, broadening their applications.

In \cref{sec:llm_play_game_obs}, we noted that LLMs often struggle to compute optimal strategies in classical matrix games. 
However, some studies take an alternative approach by leveraging LLMs' natural language understanding instead of their ability to compute equilibria directly. For example, \cite{mensfelt2024autoformalizing} utilizes LLMs to formalize game descriptions into a Game Description Language (GDL), allowing external solvers to process them. 
Similarly, \cite{deng2025natural} introduces a two-stage framework for translating extensive-form games: first, the LLM identifies the information set, and then it constructs the complete game tree using in-context learning. 
These studies suggest that LLMs can act as intermediaries in converting natural language into formal game structures, a capability beyond traditional models.

Additionally, \cite{horton2023large} explores the use of LLMs as substitutes for human participants in behavioral economic experiments. Their findings indicate that LLMs can replicate classic behavioral economics results, providing a scalable and cost-effective alternative for conducting social science research. This underscores the potential of LLMs as valuable tools in experimental economics and social science studies, facilitating large-scale simulations and deeper insights into human decision-making.

\section{Game Theory for Algorithmic Innovation}\label{sec:game_for_llm}

This section investigates how game-theoretic principles contribute to developing LLMs by informing algorithmic innovation. 
Game theory has proven instrumental in enhancing our understanding of LLMs, mainly through the use of tools like the Shapley Value and social choice models. 
These methods offer valuable insights into model interpretability, enabling a deeper understanding of how LLMs process and respond to input. 
Beyond interpretability, game theory also provides a framework for developing training objectives and evaluation metrics that address key challenges in LLM development, such as model heterogeneity and alignment with human preferences.

\subsection{Game Theory for Phenomenological Understanding LLMs}\label{sec:game_for_llm_understanding}

This line of research applies classical game theory concepts to explain observable phenomena in LLMs, including patterns in text generation and the inherent limitations of training within specific frameworks.
Such studies are particularly valuable given that LLMs are often treated as ``black boxes'' due to their proprietary nature and large-scale complexity.

One approach connects cooperative game theory to LLMs, as these models perform parallel computations on input tokens and are structured around transformer layers. 
The Shapley Value~\cite{shapley1953value}, a method for attributing contributions to individual players in cooperative games, has been adapted to assess the influence of specific tokens and layers on LLM-generated outputs. 
Several studies leverage the Shapley Value to evaluate token significance in prompts~\cite{goldshmidt2024tokenshap,mohammadi2024wait}. For example, \cite{mohammadi2024wait} demonstrates that LLMs often assign disproportionately high weights to less informative input components, a behavior strongly correlated with incorrect responses. 
TokenSHAP~\cite{goldshmidt2024tokenshap} enhances Shapley Value computation using Monte Carlo sampling for efficiency, while TextGenSHAP~\cite{enouen2023textgenshap} extends the approach to longer, structured input-output scenarios. 
\cite{liu2023prompt} applies the Shapley Value to multi-prompt learning, identifying the most impactful prompts for ensemble generation. 
Similarly, \cite{zhang2024investigating} analyzes LLM layer contributions, finding that earlier layers exert a more significant influence on output generation.

Another research direction models LLM alignment with diverse human preferences using social choice theory.
This framework helps address challenges aligning LLMs with human values and decision-making processes~\cite{mishra2023ai}.
For instance, \cite{conitzerposition} analyzes the role of Reinforcement Learning from Human Feedback (RLHF) in expressing human preferences, identifying fundamental issues arising from preference conflicts, and advocating for social choice principles in LLM alignment. 
\cite{ge2024axioms} examines RLHF reward modeling as a social choice process, demonstrating that Bradley-Terry-based approaches suffer from intrinsic limitations that violate key axioms. 
\cite{qiu2024representative} proposes a representative social choice framework, which extracts a small but representative subset of opinions to manage large-scale preference aggregation effectively.

Additionally, some studies apply game theory to model alignment and decoding strategies. 
\cite{zhang2024incentive} examine sociotechnical implications of real-world LLM applications, advocating for incentive compatibility to ensure AI systems align with societal goals while maintaining technical robustness. 
\cite{chen2024decoding} models the LLM decoding process as a Stackelberg game, where the decoder moves first, and an adversarial entity follows. 
By analyzing optimal strategies for both players, their study provides a theoretical basis for why heuristic sampling strategies perform well in practice.

\subsection{Game Theory for Stimulating LLM Algorithms}\label{sec:game_for_stimulating}

In addition to enhancing our understanding of LLMs, game theory plays a crucial role in designing algorithms that improve their capabilities. This section highlights several key challenges in LLM training and illustrates how game theory has been applied to address these issues.

\paragraph{General Human Preference.} 
Standard reward-based RLHF is limited to capturing only transitive preferences~\cite{NLHF}. 
Preference models, however, can express more general preferences by comparing two policies rather than assigning a reward for each response. 
This introduces new challenges in optimizing an LLM based on preference models.
Nash Learning from Human Feedback (NLHF) aims to optimize the von Neumann winner of a game defined by the preference model, offering a feasible and robust direction for policy optimization. 

Based on NLHF, SPO~\cite{SPO} introduces methods to express more complex preferences, such as non-transitive, stochastic, and non-Markovian preferences. 
SPPO~\cite{SelfPlayPO} designs an algorithm that efficiently implements SPO-like algorithms in large-scale language models. DNO~\cite{DNO} improves LLM optimization using a regression-based objective for more efficient and direct training. INPO~\cite{IiterativeNPO} introduces a loss function that can be directly minimized on preference datasets, further reducing the time overhead associated with calculating win rates in NLHF.

However, recent work by \cite{zhi2024beyond} points out that preference-based approaches oversimplify human values, neglecting their complexity, incommensurability, and dynamic nature. As a result, designing more robust methods for aligning human preferences remains an ongoing scientific challenge.

\paragraph{Heterogeneity in Human Preferences.}
Capturing heterogeneity in human-annotated datasets remains a significant challenge in LLM alignment. Ignoring this heterogeneity often results in models that reflect only the preferences of the majority~\cite{fleisig2023majority}. Several studies have developed more inclusive training and alignment algorithms using social choice theory~\cite{chakraborty2024maxmin,park2024rlhf,alamdari2024policy,chen2024pal}.
\cite{chakraborty2024maxmin} demonstrates the impracticality of using a single reward model and proposes the Egalitarian principle to learn preference distributions. \cite{park2024rlhf} suggests clustering preferences and proposes a scalable, incentive-compatible framework for preference alignment. \cite{alamdari2024policy} employs Borda count and quantile fairness for preference aggregation, ensuring fairness and computational feasibility. \cite{chen2024pal} introduces a mixture modeling framework for aggregating heterogeneous preferences. Additionally, \cite{klingefjord2024human} takes a macro perspective to examine the gap between human preferences and training objectives, offering solutions from a philosophical standpoint.

\paragraph{Data Cost Efficiency.}
Game theory has also been applied to enhance the cost efficiency of LLM training. Collecting a dataset with guaranteed quality and coverage is often challenging, so several works have used the self-play framework to improve data utilization, reducing the amount of data required while maintaining performance. 
\cite{SPIN} addresses the problem of fine-tuning a model with only a tiny amount of gold-standard data.
Drawing from Generative Adversarial Networks~\cite{goodfellow2020generative}, it allows the LLM to improve the quality of its answers while learning to distinguish between its responses and those of the gold-standard answers, ultimately converging to the distribution of the gold-standard data. 
\cite{cheng2024self,alignmenttwoplayer} models a game between an attacker and a defender, both of which are LLMs. 
\cite{alignmenttwoplayer} employs the attacker to propose prompts that the defender is less skilled at while the defender continuously improves. 
\cite{cheng2024self} considers a classic game, Adversarial Taboo, to enhance model knowledge acquisition without introducing new data, leading to better performance in experiments. 
Furthermore, \cite{zhang2024vickreyfeedback} improves the efficiency of preference data collection by incorporating an auction model into the LLM fine-tuning process, demonstrating how this approach can enhance fine-tuning efficiency while maintaining strong performance.

\paragraph{Other Two-Player Game Formulations.}
In addition to the literature discussed above, several studies have formulated other two-player game models in specific phases of LLMs to enhance particular capabilities.
\cite{chakraborty2024parl,STARLHF,cheng2024adversarial} model the interaction between the reward model and the LLM as a two-player game. They aim to address the problem where a static reward model cannot handle the distribution shift of the evolving LLM policy. Their game-theoretic modeling captures the co-evolution of the reward model and the LLM, and equilibrium-solving algorithms are used to provide theoretically guaranteed LLM training methods. 

\cite{jacob2023consensus} observes that generative and discriminative answers to a question by the same LLM are often inconsistent. It models the Consensus Game, where these two types of answers act as players seeking a consensus answer. Using equilibrium-solving algorithms, this approach significantly improves the LLM's accuracy across various datasets. Furthermore, \cite{gemp2024states} models the process of LLMs generating long-text conversations as a sequential game, using game-theoretic tools to enhance the model's ability to understand conversations and develop appropriate responses.

% \begin{remark}
\paragraph{Remark.}
Game theory is essential in addressing the challenges in LLM development by offering clear principles for optimization and well-defined metrics for evaluating models' performance. Through its systematic approach, game theory helps refine LLM policies by aligning model behaviors with complex human preferences while providing a framework to measure and track model effectiveness improvements. This makes game theory a powerful tool for optimizing LLMs, ensuring training processes are both theoretically grounded and practically applicable.
% \end{remark}



\section{Game Theory for LLM-Related Modeling}\label{sec:llm_game_scenarios}
This section provides an overview of research on game-theoretic models that involve LLMs.
The theoretical analysis of these models provides evidence of LLMs' impact on human society. 
We categorize the literature into three main areas. 
The first area explores game-theoretic models that include both LLMs and humans, aiming to explain or predict the phenomena resulting from the development of LLMs. 
The second area examines scenarios where LLMs function as products or platforms.
This creates competitive environments that exhibit game-theoretic dynamics like ad auctions.
The third area extends classical game theory models, investigating how LLMs' unique capabilities can generalize and refine these models for more complex and realistic settings.


\subsection{Competitions between LLM and Human}\label{secsub:competition}
This body of work introduces several competition models, treating LLMs as players in the game~\cite{yaohuman,esmaeili2024strategize,taitler2024braess}. 
These models generally arise from a recognition: modern LLMs possess powerful content generation capabilities and, compared to human creators, are characterized by lower costs and faster evolutionary rates.

\cite{yaohuman} investigates the impact of LLMs on human creators by proposing a competition model based on the Tullock contest. This model explores the dynamics between human-generated and LLM-generated content, modeling LLMs as cost-free players whose output quality improves as human content quality increases. Through equilibrium analysis, the study concludes that LLMs do not fundamentally conflict with or replace human creators but instead reduce the volume of human-generated content, ultimately pushing out less efficient creators.
\cite{esmaeili2024strategize} extends this model into a repeated game setting, focusing on how humans can optimize their utility in dynamic competition with AI across various content domains. The study highlights the computational complexity of determining optimal strategies and proposes practical algorithms that offer near-optimal solutions.

\cite{taitler2024braess} examines the competitive dynamics between LLM-based generative AI and human-operated platforms, such as Stack Overflow, and their implications for social welfare. The study models the revenue-maximization problem of LLMs and uncovers phenomena akin to Braess's paradox: as human users increasingly rely on LLMs, the original platforms suffer from a lack of quality-enhancing data. Furthermore, generative AI models rarely undergo training aimed at quality improvement due to cost-saving incentives. The study also suggests theoretical regulatory frameworks to address these issues.

The development of LLMs has led to diverse societal effects, and game theory provides a robust theoretical framework for studying these effects. By employing appropriate models depicting optimal behaviors and equilibrium strategies, we can derive properties with theoretical guarantees.

\subsection{Game Scenarios Emerging with LLMs}\label{secsub:game_emerging_with_llm}
This section explores game-theoretic scenarios that arise from LLMs as products or platforms. In these scenarios, LLMs do not participate in the games; instead, they revolve around them. 

As LLMs gain global attention, industries related to LLMs are creating substantial commercial value. 
\cite{laufer2024fine} explores the feasibility of fine-tuning general-purpose models as a market service. The study models the bargaining process between generalists developing the model and domain specialists adapting it. By analyzing the sub-game perfect equilibria, the paper demonstrates that a profit-sharing outcome is possible and offers methods for determining Pareto-optimal equilibria.
\cite{sun2024mechanism} investigates potential economic scenarios in which a platform offers fine-tuning services for LLMs through an auction-like process for multiple groups with different preferences. The study proposes an incentive-compatible payment scheme that guarantees social welfare maximization. 
\cite{mahmoodpricing} analyzes the competitive dynamics of LLM deployment, highlighting the value of market information and demonstrating that a first-to-market strategy may be cost-ineffective for all tasks when those tasks are sufficiently similar. 
\cite{saig2024incentivizing} proposes a pay-for-preference payment with a contract design model to address the potential moral hazard in the current pay-per-token pricing scheme.

In addition to their role as commodities, LLMs also offer potential commercial value through advertising revenue, similar to search engines. 
The emergence of LLMs has made traditional fixed-slot advertisements obsolete, prompting several studies on integrating LLMs into advertising auctions~\cite{feizi2023online}. 
\cite{duetting2024mechanism} models a scenario where each advertiser owns an agent LLM and bids to influence the probability distribution of the next token generated. The study derives an auction mechanism that ensures incentive compatibility by modifying the second-price auction.
\cite{adSummaries} assumes each advertiser provides a fixed advertisement copy, influencing LLM summaries through bidding. Their auction mechanism determines the prominence of each advertiser in the summary and the price they pay, ensuring incentive compatibility.
\cite{adRAG} also assumes each advertiser possesses a document representing their content but models the advertisement insertion process in a Retrieval-Augmented Generation (RAG) framework. The mechanism probabilistically retrieves and allocates ads within each discourse segment of LLM-generated content, optimizing for logarithmic social welfare based on bids and relevance.
\cite{soumalias2024truthful} investigates a scenario where each advertiser bids via a reward function for LLM-generated content. Their mechanism incentivizes truthful reporting of reward functions and demonstrates operational viability in a tuning-free setting.

\subsection{LLM Extending Classic Game Models}\label{secsub:LLM_modification}
In addition to these two areas, this section examines works that enhance traditional game theory models using LLMs, extending their applicability to more realistic scenarios.

LLMs' text comprehension and generation capabilities make them valuable tools for aggregating and eliciting opinions.
\cite{lu2024eliciting} explores using LLMs to assist peer review, noting that traditional peer prediction mechanisms are limited to simple reports, such as multiple-choice or scalar numbers. 
The study proposes peer prediction mechanisms that leverage LLMs' powerful text-processing capabilities to incentivize high-quality, truthful feedback. 
These mechanisms are shown to distinguish between human-written and LLM-generated reviews in empirical experiments. 
\cite{fish2023generative} uses LLMs to address the limitations of traditional social choice theory, which is constrained to choices among a few predetermined alternatives. The study employs LLMs to generate text and extrapolate preferences, providing a method for designing AI-augmented democratic processes with rigorous representation guarantees. 
\cite{sun2024large} investigates how LLMs can provide richer information in traditional auctions. The study introduces the Semantic-enhanced Personalized Valuation in the Auction framework, which leverages LLMs to incorporate bidders' preferences and semantic item information into the valuation process. The framework integrates fine-tuned LLMs with the Vickrey auction mechanism to improve valuation accuracy and bidding strategies.


\section{Conclusion and Future Directions}
This survey provides a comprehensive overview of the research progress at the intersection of LLMs and game theory. We summarized the role that game theory has been playing in developing LLMs from three key perspectives: providing standardized game-based evaluation, driving game-theoretic algorithmic innovations, and modeling the societal impact of LLMs.
Furthermore, we highlighted the bidirectional relationship between LLMs and game theory, exploring how LLMs influence traditional game models. 
Building on this review of existing literature, we have identified several promising future directions in the intersection of game theory and LLMs. In the following section, we outline some of these opportunities and challenges with the hope of advancing this multidisciplinary field.

\paragraph{LLM-based Agents with Comprehensive Game Abilities.} 
Existing research has explored the evaluation of LLM agents across various game scenarios and developed methods to enhance their reasoning capabilities. 
However, while some of these methods demonstrate general applicability, their validation remains highly scenario-specific.
A key future direction is to develop LLM agents proficient in game-theoretic reasoning, capable of applying their knowledge across diverse game settings without explicit customization. 
Achieving this requires advancements in rule comprehension, external environment modeling, and multi-agent reasoning.
Key technical aspects include constructing a game-theoretic corpus, refining fine-tuning strategies, and incorporating tool-learning techniques.


\paragraph{Moving Beyond Human-Oriented Evaluation Frameworks.}  
Game theory provides well-established evaluation criteria for rationality and strategic reasoning, such as K-level rationality, which has been widely adopted to assess LLM intelligence. However, these evaluation methods were originally designed for human cognition and may not fully capture the reasoning processes of next-token prediction models.
To assess LLMs comprehensively from a game-theoretic perspective, it is crucial to move beyond existing human-oriented metrics and develop evaluation frameworks tailored to neural network-based models. This remains an underexplored area with the potential to improve our understanding of LLMs' decision-making significantly.

\paragraph{Theoretical Understanding of LLMs' Strategic Behavior.}
The application of game-theoretic concepts, such as the Shapley Value, to understanding LLMs' text generation behavior is still in its early stages. Most studies on LLMs' strategic behavior in real-world scenarios rely on empirical observations rather than systematic theoretical interpretations.
For example, \cite{park2024llm} has introduced hypothetical models to explain why LLMs struggle to achieve the performance level of no-regret learners in repeated games. Extending such theoretical investigations to more complex scenarios, including games like Werewolf, Avalon, or bargaining games, is essential. A deeper theoretical understanding of LLM strategic behavior will help to define their capability boundaries and provide insights for further improving their reasoning abilities.

\paragraph{Capturing Cooperative Games in LLM Optimization.}
Many studies leveraging game theory for optimizing LLM training, as discussed in Section~\ref{sec:game_for_stimulating}, primarily focus on non-cooperative game settings. While non-cooperative approaches are a natural fit, cooperative game-theoretic methods offer additional insights into LLM optimization.
For instance, different expert networks can be seen as participants in a cooperative game in the Mixture of Expert models. Adopting suitable payoff allocation mechanisms, such as the Shapley Value or the core concept, could optimize expert selection and task allocation, reducing redundancy while enhancing computational efficiency. Similarly, in ensemble learning and knowledge distillation, different sub-models can be treated as cooperative agents working together to refine decision boundaries or transfer knowledge. Effective reward allocation and weight adjustment strategies could enhance collaboration among sub-models, reducing redundant computation while improving generalization. 
Integrating cooperative game-theoretic methods into LLM training and optimization could offer new theoretical insights and practical solutions.

\paragraph{Modeling Cooperation Between Multi-LLMs and Humans.}
As discussed in Section~\ref{secsub:competition}, previous studies have primarily been focused on modeling competitive interactions between LLMs and humans, yielding valuable insights into their societal implications. However, beyond competition, understanding cooperative dynamics between multiple LLMs and humans remains an important research direction.
One key challenge is to design mechanisms that incentivize LLMs to collaborate in fulfilling human-assigned tasks while considering their objectives. A theoretical characterization of LLM agents' goals and behaviors is essential for bridging the gap between game-theoretic mechanism design and real-world deployment. Advancing this line of research could facilitate the development of LLMs that align more effectively with human objectives and contribute positively to society.

\paragraph{Leveraging LLMs as Oracles to Extend Theoretical Game Models.}
As discussed in Section~\ref{secsub:LLM_modification}, several studies have explored how LLMs can be used to extend classical game-theoretic models. The key insight behind these works is that LLMs, with their strong language understanding and generative capabilities, can serve as oracles with specific functionalities in game-theoretic frameworks. This perspective opens up new opportunities to relax idealized assumptions or replace theoretical oracles in various game models using LLMs. By doing so, models that previously remained purely theoretical can now be practically implemented while preserving approximate theoretical properties.
Systematic exploration of how LLMs can function as adaptable oracles in different theoretical models could bridge the gap between abstract game-theoretic concepts and real-world applications.