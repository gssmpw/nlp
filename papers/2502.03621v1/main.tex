\documentclass[acmtog,authorversion,nonacm]{acmart}
\citestyle{acmauthoryear}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

% Metadata Information
\acmJournal{TOG}
%\acmVolume{38}
%\acmNumber{4}
%\acmArticle{39}
% \acmYear{2025}
%\acmMonth{7}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
%\acmDOI{0000001.0000001_2}

% Paper history
%\received{February 2007}
%\received{March 2009}
%\received[final version]{June 2009}
%\received[accepted]{July 2009}


% Document starts

% our addition
\include{math_commands}
\usepackage{arydshln}
\usepackage{makecell}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{enumitem}


% \usepackage{microtype}
% \usepackage{algorithmic}
% \usepackage{algorithm}
% \usepackage{ulem}

%% end of the preamble, start of the body of the document source.
\begin{document}
\title{DynVFX: Augmenting Real Videos with Dynamic Content}

\author{Danah Yatim}
\authornote{Both authors contributed equally to this research.}
\affiliation{%
 \institution{Weizmann Institute of Science}
 \country{Israel}
 }
\author{Rafail Fridman}
\authornotemark[1]
\affiliation{%
 \institution{Weizmann Institute of Science}
 \country{Israel}
 }
\author{Omer Bar-Tal}
\affiliation{%
 \institution{Pika Labs}
 \country{USA}
 }
\author{Tali Dekel}
\affiliation{%
 \institution{Weizmann Institute of Science}
 \country{Israel}
 }

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\afterfigure}{\vspace{-4mm}} 
% \setlength{\abovedisplayskip}{5pt}    % Space above equations
% \setlength{\belowdisplayskip}{5pt}    % Space below equations
%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
We present a method for augmenting real-world videos with newly generated dynamic content. Given an input video and a simple user-provided text instruction describing the desired content, our method synthesizes dynamic objects or complex scene effects that naturally interact with the existing scene over time. The position, appearance, and motion of the new content are seamlessly integrated into the original footage while accounting for camera motion, occlusions, and interactions with other dynamic objects in the scene, resulting in a cohesive and realistic output video. We achieve this via a zero-shot, training-free framework that harnesses a pre-trained text-to-video diffusion transformer to synthesize the new content and a pre-trained Vision Language Model to envision the augmented scene in detail. Specifically, we introduce a novel inference-based method that manipulates features within the attention mechanism, enabling accurate localization and seamless integration of the new content while preserving the integrity of the original scene.
Our method is fully automated, requiring only a simple user instruction. We demonstrate its effectiveness on a wide range of edits applied to real-world videos, encompassing diverse objects and scenarios involving both camera and object motion\footnote{Code will be made publicly available.}.
Project page: \url{https://dynvfx.github.io/}
\end{abstract}

\keywords{Text-to-video editing, diffusion models}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\begin{teaserfigure}
  \includegraphics[width=\textwidth]{figures/teaser.pdf} \vspace{-0.5cm}
  \caption{ DynVFX augments real-world videos with new dynamic content described via simple user-provided text instruction.}
  \label{fig:teaser}
\end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:intro}

Incorporating computer-generated imagery (CGI) into real-world footage has been a transformative capability in film production,  enabling the creation of visual effects that would be difficult or impossible to achieve otherwise. 
For instance, the seamless integration of CGI characters, such as \emph{Gollum} in \emph{The Lord of the Rings} or  \emph{T-Rex} in \emph{Jurassic Park}, has empowered filmmakers to blend fantastical elements with real-world environments, resulting in immersive storytelling.  Inspired by these capabilities, we pose a new creative task: augmenting real-world videos with newly generated dynamic content. Specifically, given an input video and a text prompt describing the desired edit, our goal is to synthesize new dynamic objects or complex scene effects, which naturally interact with the existing scene across time (Fig.~\ref{fig:teaser}).

Our task poses several new fundamental challenges. First, the generation must be \emph{content-aware}, such that the position, appearance, and motion of the synthesized dynamic content integrate naturally with the original scene. This entails synthesizing objects that respect occlusions, maintain appropriate relative size and perspective with respect to the camera position, and realistically interact with other dynamic objects. All of this must be achieved while maintaining the integrity of the original video, ensuring that new content enhances the scene without compromising its authenticity. 



Our method leverages a pre-trained text-to-video model without any fine-tuning or additional training. Specifically, given an input video along with a short user instruction describing the new content (e.g., ``add a massive whale''), our method produces an edited video where the new content is seamlessly integrated into the original video. We achieve it by estimating the residual to the latent representation of the original video.  To envision the edited scene and to identify prominent foreground objects and visual elements, we leverage a vision-language model that translates the user's instructions into detailed prompts for the text-to-video diffusion model. 

As our task requires careful placement of the new content, we propose to steer the localization of the edit through \textit{Anchor Extended Attention} - incorporate a specific set of keys/values extracted from the original video as additional context to the model. Additionally, to improve the edit harmonization with the original scene, we propose to iteratively update the estimated edit residual.

It is worth noting that our method utilizes a publicly available text-to-video model, which exhibits a significant gap in video generation quality compared to recent state-of-the-art video models. Nevertheless, we observe that within our problem formulation and objective, we can distill from this model surprisingly powerful generative capabilities. We demonstrate the effectiveness of our approach on a variety of edits applied to real-world videos.


To summarize, our work makes the following contributions:
\begin{itemize}[topsep=1pt,partopsep=0pt,leftmargin=.5cm]
    \item We introduce a new task of integrating newly generated dynamic content into real-world videos without relying on the user to provide complex references of the effect (for example, a VFX asset or masks to specify where to locate the VFX). 
    \item We propose a tuning-free, zero-shot method that enables harmonized content integration while maintaining high fidelity to the original scene 
    \item We propose an automatic VLM-based evaluation metric tailored for our task, considering multiple factors, including original content preservation, new content harmonization, overall visual quality, and alignment with the edit prompt.
    \item We demonstrate state-of-the-art results compared to competing methods, achieving the best trade-off between synthesizing new dynamic elements and maintaining high fidelity to the original content.
\end{itemize}

\section{Related Work}

\myparagraph{Text-to-Video Models.}
With the rise of large-scale video-text datasets, there has been significant progress in training text-to-video models using new architectures \cite{bartal2024lumiere, polyak2024movie}. While the foundational architecture of diffusion models has been commonly linked to inflating text-to-image U-Net-based models with temporal layers \cite{wang2023modelscope,zeroscope, guo2023animatediff}, recently, a new family of Transformer-based models \cite{sora, kong2025hunyuanvideosystematicframeworklarge, yang2024cogvideox, opensora, hacohen2024ltxvideorealtimevideolatent}, referred to as Diffusion Transformers (DiTs) \cite{dit}, have gained popularity, as DiTs enhance spatial coherence and enable arbitrary aspect ratio and video-length training. In this work, we utilize a publicly available DiT-based model \cite{yang2024cogvideox},  CogVideoX,  for augmenting real-world videos with newly generated dynamic content in a zero-shot manner. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/ea.pdf} \vspace{-0.8cm}
    \caption{Controlling fidelity to the original scene using different extended attention mechanisms. (a-b) SDEdit suffers from the original scene preservation/edit fidelity trade-off. (c-e) Three Extended Attention variants during sampling demonstrate different control levels: Full Extended Attention closely reconstructs the input scene, Masked Extended Attention proves too constrained in overlapping regions despite allowing new content emergence, and our Anchor Extended Attn. achieves optimal results by applying dropout -- extending attention only at sparse points within selected regions.}
    \label{fig:extended_attn}\afterfigure
\end{figure*}

\myparagraph{Object Insertion in Images.}
With the advancement of text-to-image models, techniques for image manipulation leveraging such models have also evolved rapidly. Among these advancements, notable progress has been made in the task of instruction-based image editing. Several works \cite{hive,Sheynin2023EmuEP,brooks2022instructpix2pix,Zhang2023MagicBrush} have proposed to directly fine-tune generative models on pairs of original and edited images coupled with user-provided instructions. EmuEdit \cite{Sheynin2023EmuEP} leverages a diffusion model trained on a large synthetic dataset to perform various editing tasks guided by task embeddings.

The task of object insertion into images belongs to the same category and can be considered a subfield of instruction-based editing methods. For instance, EraseDraw \cite{erasedraw-24}, Paint By Inpaint \cite{Wasserman2024PaintBI}, and ObjectDrop \cite{winter2024objectdrop} leverage inpainting models to create paired-image datasets, which are then used to fine-tune image editing models.  However, extending these approaches to videos presents significant challenges. In particular, generating large-scale instruction-paired video datasets can be prohibitively expensive both in time and computational resources, as it requires substantial manual effort to annotate frames and ensure alignment between textual instructions and video content. This cost and complexity make it challenging to adapt existing image-based methodologies directly to the video domain.

Concurrently to our method, Add-It \cite{Tewel2024AdditTO} proposes to manipulate the attention features of a pre-trained text-to-image diffusion model to insert objects into images in a training-free manner. While their method relies on weighted global extended attention, we propose to apply extended attention only to specific regions of the source scene, allowing the generation to focus on essential elements.



\myparagraph{Controllable Video Generation.}
Recently, numerous methods have been developed to incorporate various forms of control signals into video generation pipelines.
Several video-to-video methods propose to condition the generation on per-frame spatial maps such as depth maps and edge maps \cite{Wang2023VideoComposerCV,Chen2023ControlAVideoCT}. A line of work \cite{yatim2023spacetime, zhao2023motiondirector, park2024spectral, jeong2023vmc, tokenflow2023} proposed to utilize a text-to-video model for the task of motion transfer. Unlike these methods, which are not designed to deviate from the existing structures within a video, our approach focuses on integrating additional dynamic elements into the video. 

Recent methods \cite{bartal2024lumiere,Ma2024VidPanosGP,Zhang2023AVIDAV,Zi2024CoCoCoIT, zhou2023propainter} have explored adapting text-to-video models for video inpainting by conditioning on a masked video and a corresponding binary mask. This setup encourages the model to preserve unmasked information while generating new content in the masked region. All of these methods require user-provided masks — an impractical requirement for integrating complex dynamics as it requires anticipating the placement of dynamic objects (e.g., Fig. \ref{fig:results} jellyfish, tsunami, dinosaurs) ahead of time. In contrast, our method allows for automatic new content localization without any user-provided masks. While a static object can be masked with a simple bounding box, manually defining masks for complex motion or interactions per frame is extremely difficult.

Recently, generative Omnimatte \cite{generative-omnimatte} proposed a method to automatically decompose a video into object layers and their corresponding effects. However, it is not designed for the task of new content generation, as it allows only for the removal of existing scene elements. VideoDoodles \cite{videodoodles} combines hand-drawn animations with video footage in a scene-aware manner by tracking a user-provided planar canvas in 3D. However, it does not support the creation of non-planar animations. Our framework includes both localizing dynamic new content and integrating in a scene-aware manner without reliance on user-provided positions. 


\myparagraph{Language Models for Video Content Creation.}
Advancements in Vision Language Models (VLMs) have enabled 
methods to utilize such models in various video-related tasks. Some methods \cite{Chen2024Panda70MC7,yang2024cogvideox} use VLMs to produce detailed video captions from a series of frames, which are then utilized to train text-to-video generative models.
Other methods utilize such models for achieving better generation controllability. For instance, VideoDirectorGPT \cite{Lin2023VideoDirectorGPT} utilizes a VLM for multi-scene video generation by training diffusion adapters to incorporate additional conditioning inputs, while LVD \cite{lian2023llmgroundedvideo} incorporates layout guidance from the VLM during the sampling process. AutoVFX \cite{hsu2024autovfx} uses an LLM to generate a video editing program pipeline based on the user instruction. In our work, we employ a VLM as a ``VFX assistant'' that, based on a short user instruction, provides a comprehensive description of the edited video along with the prominent objects present in the scene.

\myparagraph{Professional Software For Video Animation}
In professional visual effects production, tools such as Autodesk Maya \cite{maya}, Blender \cite{blender}, Unreal Engine \cite{unrealengine}, Adobe After Effects \cite{christiansen2013adobe} and Houdini \cite{houdini} are widely used for creating and compositing complex visual effects. These tools provide artists with precise control over object modeling, animation, and integration into video footage. While powerful, they require significant expertise, extensive manual intervention, and detailed inputs, such as 3D scene reconstruction or motion tracking. All the aforementioned software count on the user to provide the 3D assets. Even though creating input 3D assets has become an easier task to solve, using new datasets \cite{objaverseXL, qiu2024richdreamer}, or generating assets based on a user prompt (For example, Meshy.ai and Alpha3D), still, 3D physical elements (like fluid or explosion) or global/multi-object effects present a significant challenge. In this work, we take the first steps towards a novice user-friendly workflow. 

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/pipeline.pdf}
    \caption{DynVFX pipeline. Top row: Given an input video $\mathcal{V}_{\text{orig}}$, we apply DDIM inversion (see Sec. 3) and extract spatiotemporal keys and values $[\mathbf{K}_\text{orig}, \mathbf{V}_\text{orig}]$ from the original noisy latents. Given the user instruction $\mathcal{P}_{\text{VFX}}$ we instruct the VLM to envision the augmented scene and output the text edit prompt $\mathcal{P}_{\text{comp}}$, prominent object descriptions $\mathcal{O}_{\text{orig}}$ that are used to mask out the extracted keys and values and target object descriptions $\mathcal{O}_{\text{edit}}$. Bottom row: We estimate a residual $\bs{x}_{\text{res}}$ to the original video latent ($\bs{x}_{\text{orig}}$). This is done by iteratively applying SDEdit with our Anchor Extended Attention, segmenting the target objects ($\mathcal{O}_{\text{edit}}$) from the clean result, and updating $\bs{x}_{\text{res}}$ accordingly. 
     }
    \label{fig:pipeline}
\end{figure*}

% \vspace{-0.26cm}
\section{Preliminaries}
\label{sec:preliminaries}
% \subsection{Diffusion Models}
Diffusion probabilistic models \cite{ddpm,sohl2015deep} are a class of generative models that aim to learn a mapping from noise $\bs{x}_T\sim \mathcal{N}(0,I)$ to a data distribution $q$. Starting from a Gaussian i.i.d noise $\bs{x}_T\sim \mathcal{N}(0,I)$, the diffusion model $\Phi$ is applied iteratively through a sequence of denoising steps, ultimately producing a clean output sample $\bs{x}_0$. 

Recently, a new class of latent text-to-video (T2V) models, built on Diffusion Transformers (DiTs) \cite{dit}, has gained significant popularity. These models comprise multi-modal blocks (MMDiT \cite{sd3}) that allow for joint processing of both text and image modalities, allowing each to inform and refine the other's representations. To process both modalities together, first, a pre-trained encoder compresses an RGB video $\mathcal{V}$ both spatially and temporally to a latent space. Then, the latent is patchified, and the resulting tokens are concatenated to the text tokens produced by the text encoder \cite{raffel2023exploringlimitstransferlearning}. 

In each MMDiT block, text tokens and spatiotemporal tokens are projected into queries, keys and values using separate sets of weights for each modality, and the sequences of the two modalities are concatenated as a joint input for the attention operation: $\mathbf{Q}=[\mathbf{Q}_\text{text}, \mathbf{Q}_\text{spatio}]$, $\mathbf{K}=[\mathbf{K}_\text{text}, \mathbf{K}_\text{spatio}]$ and $\mathbf{V}=[\mathbf{V}_\text{text}, \mathbf{V}_\text{spatio}]$. The attention \cite{Vaswani2017AttentionIA} operation computes the affinities between the d-dimensional projections $\mathbf{Q}, \mathbf{K}$ to yield the output of the layer: 
\vspace{-0.35cm} 
\begin{equation}
\mathbf{A}\cdot\mathbf{V} \ \text{where} \
\mathbf{A} = \texttt{Attention}(\mathbf{Q}, \mathbf{K}) = \texttt{softmax}\left( \frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d}}\right)
\label{eq:attnetion}
\end{equation}
% \vspace{-0.35cm} 
To capture inter-token relationships, Rotary Position Embeddings (RoPE) \cite{Su2021RoFormerETrope} are applied to the input to the attention operation.



\section{Method}
\label{sec:method}
Given an input video $\mathcal{V}_{\text{orig}}$ and a textual instruction $\mathcal{P}_{\text{VFX}}$, our goal is to synthesize a new video $\mathcal{V}_{\text{VFX}}$, in which new dynamic elements are seamlessly integrated to the existing scene. We address this task by estimating a residual in the latent space of the text-to-video diffusion model, which is added to the latent of the original video. The final edited video is obtained by decoding their sum with the text-to-video diffusion model's VAE decoder

Tackling this task requires ensuring that the generated content, such as new objects or effects, adheres to the dynamics of the existing scene. The location and size of the new content must align with the camera motion and the environment, while its actions and movements must appropriately respond to other dynamic objects present in the scene. Our framework (illustrated in Fig.~\ref{fig:pipeline}) addresses these challenges by incorporating the following key components:

% \vspace{-1cm} 
\begin{enumerate}[leftmargin=.5cm]
    \item \textit{VLM as a VFX assistant.} We utilize a pre-trained VLM to interpret the user’s instructions, reason about the interactions with the scene's dynamics, and provide a descriptive prompt for the T2V diffusion model by guiding it to act as ''VFX assistant'' via a system prompt containing guidelines in the context of our tasks.

    \item \textit{Localization via Anchor Extended Attention.}  To steer the localization of the edit and make it content-aware, we propose to utilize Extended Attention during sampling from the T2V DiT model to a set of keys and values extracted from sparse locations in the original video. 
    % The localization is through the understanding that RoPE encodes the absolute position of each patch in both space and time, each query in the extended attention most attends to its corresponding patch (in space and time) in the key-value bank. 
    
    \item \textit{Content Harmonization via Iterative refinement.} To improve the blending of the generated content with the input video and achieve better harmonization, we iteratively update the estimated residual latent by repeating the sampling process with AnchorExtAttn multiple times, progressively reducing the level of noise added at each step.
\end{enumerate}

\subsection{VLM as a VFX assistant}
\label{sec:vlm}

To create a fully automatic framework requiring only a simple user-provided instruction describing the desired content, we incorporate a Vision-Language Model (VLM) into our framework. Specifically, given the user instruction $\mathcal{P}_{\text{VFX}}$, along with evenly spaced keyframes from the original video $\mathcal{V}_{\text{orig}}$, we instruct the VLM \cite{achiam2023gpt4} to provide a detailed caption - a composition prompt $\mathcal{P}_{\text{comp}}$ describing the new composited scene. While the model gives an accurate, descriptive source scene caption, in some cases, we observed that it fails to give captions suitable for compositing VFX with the scene. To overcome this, we guide the model to reply in an in-context matter by asking it to imagine a conversation with a visual effects (VFX) artist to obtain a caption that would describe the composited scene correctly. In this
conversation, VLM will ``consult'' with a VFX artist about how the new content should be integrated into the scene. Based on their discussion, it provides a caption that describes how the added content fits into the scene. This results in text prompts that encourage the generated output video to include a natural interaction between the new content and the original environment. The guidelines include focusing on (1) spatial and dynamic awareness of existing scene elements, (2) preservation of original scene behaviors, and (3) atmospheric coherence between new and existing content.
See SM for more details about how we use the VLM to reason about the new scene integration with in-context reasoning for VFX.

We also use the VLM to obtain a list of prominent foreground objects in the original video: $\mathcal{O}_{\text{orig}}$ and the object that will be added according to the edit prompt: $\mathcal{O}_{\text{edit}}$. See more details about prompting and mask processing in SM.
 
\vspace{-0.1cm}
\subsection{Localization via Anchor  Extended Attention} 
\label{sec:method_localization}

A pivotal aspect of our method is the accurate localization of the new content in the existing scene, ensuring alignment with camera motion, occlusions, and scene depth.
While the composition prompt can describe the desired location, naive noising-denoising with this prompt introduces a trade-off: As shown in Fig. \ref{fig:extended_attn}, using SDEdit \cite{meng2022sdedit} with a high noising timestep fails to retain the original scene, resulting in misaligned new content. In contrast, a low noising timestep, as illustrated in Fig. \ref{fig:extended_attn} (b), limits deviations from the original video.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/ablations_new.pdf} 
    \vspace{-0.6cm}
    \caption{Ablations. (b) Excluding both AnchorExtAttn and the Iterative refinement process results in significant misalignment with the original scene and poor harmonization (e.g., the size of the puppy relative to the scene and boundary artifacts). (c) Omitting AnchorExtAttn leads to incorrect positioning of the new content.  (d) Removing iterative refinement results in poor harmonization. Our full method (e) exhibits good localization and harmonization of the edit}\afterfigure
    \label{fig:ablation}
\end{figure*}


To tackle the localization challenge, we extend the attention module during sampling to include the input video's corresponding attention features. Specifically, we apply DDIM inversion \cite{song2020_ddim} to the original video $\mathcal{V}_{\text{orig}}$ and extract the spatio-temporal keys and values $K_\text{orig}, V_\text{orig}$ from the attention module of every block in the network and generation timestep $t$. These keys and values are then used to extend the attention mechanism during sampling with $\mathcal{P}_{comp}$ to control the localization of the edit.

When using all keys and values, the extended attention operation can be expressed as:
\vspace{-0.1cm}
\begin{equation}
\texttt{Attn}(Q_\text{VFX},[K_\text{VFX}, K_\text{orig}], [V_\text{VFX}, V_\text{orig}])
\label{eq:ExtAttn}
\end{equation}
Interestingly, extending keys and values provides more than just global information, but each feature locally encodes corresponding patches in the video.
As shown in Fig. \ref{fig:extended_attn} (c), extending the attention to the full set of keys and values is sufficient to achieve an approximate reconstruction of the original video. We hypothesize that this occurs because the same positional embedding is applied to the $K_\text{orig}$ as to $K_\text{VFX}$, aligning the positional embeddings in spatiotemporal locations. This observation provides strong evidence for the significance of the positional embedding. Due to this, the setup proves to be overly restrictive in adhering to $\mathcal{P}_{\text{comp}}$, as illustrated in Fig.~\ref{fig:extended_attn}(c). 

\myparagraph{Selective Attention.} To overcome this limitation, we propose to restrict the extended attention only to specific positions in the source scene. Specifically, we use a selection function $\mathcal{F}$ to determine which keys and values are retained.

An intuitive strategy is to apply region-based attention with Masked Extended Attention by identifying the most critical regions for preserving scene coherence and extending attention with keys and values within these masked regions $M_{\text{orig}}$, i.e., $\mathcal{F}(\mathcal{A}) = M_\text{orig} \circ \mathcal{A}$.
To get $M_{\text{orig}}$, we ask a VLM  to provide a list of foreground objects in the original video $\mathcal{O}_{\text{orig}}$ (typically the most spatially prominent elements within a scene) and then we obtain corresponding masks using a text-based segmentation model. For example, Fig.~\ref{fig:extended_attn}(d), where the masked region includes the horse, illustrates the effect of this on the generated content. As shown, this setup preserves high fidelity to the original scene within the mask $M_{\text{orig}}$ and allows new content to emerge in the unmasked regions.

However, this approach proves too constrained in overlapping regions. To address this limitation, we propose our Anchor Extended Attention:

{\small \begin{equation}
\begin{aligned}
  \texttt{AnchorExtAtt} := &\texttt{Attn}(Q_{\text{VFX}},[K_{\text{VFX}}, K^E],
  [V_{\text{VFX}}, V^E]) \\
  &\text{s.t. }K^E:=\mathcal{F}(K_{\text{orig}},M_{\text{orig}}) ~~and~~  V^E:= \mathcal{F}(V_{\text{orig}}, M_{\text{orig}}) \\
  & \mathcal{F} := \Biggl\{\texttt{Drop}_{FG}(M_{\text{orig}}) \cup\texttt{Drop}_{BG}(\sim M_{\text{orig}}) ~\circ~ \mathcal{A} 
\end{aligned}
\label{eq:AnchorExtAttn}\vspace{-0.05cm}
\end{equation}}
This formulation introduces dropout within the masked regions $M_{\text{orig}}$, generating a sparse set of anchor points that guide the generation while preserving flexibility.
This formulation introduces a predominantly content-aware selection of anchors from foreground regions, along with a sparser set of background anchors, to achieve robust and spatially coherent integration of new content (e.g. the horse to enforce the knewly added knight to align with its motion). As demonstrated in Fig.~\ref{fig:extended_attn}(e), this balanced approach offers sufficient flexibility for creative edits while preserving key spatial cues from the original scene.

\begin{figure*} [ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/result.pdf}
    \caption{Sample results of our method. See SM for full vide results.}
    \label{fig:results}
\end{figure*}
\subsection{Content Harmonization} 
\label{sec:harmonization}
Our anchor extended attention steers the placement of the new content to align with the original scene. However, it does not guarantee precise pixel-level alignment. As can be seen in Fig. \ref{fig:extended_attn}(e), the legs of the horse are not perfectly aligned with the original horse. To guarantee pixel-level alignment, a straightforward approach is to extract a mask of the new content $\bs{M}_{VFX}^{rgb}$ from the sampling with AnchorExtAttn (Eq. \ref{eq:AnchorExtAttn}) output $\mathcal{\hat{V}}_{comp}$. More concretely, by applying a text-based segmentation model using the added object description provided by the VLM.

The mask can then be used to replace the pixels outside it with the corresponding pixels from the original video: $\mathcal{V}{\text{comp}}[\sim\bs{M}_{VFX}^{rgb}] = \mathcal{V}{\text{orig}}[\sim \bs{M}_{VFX}^{rgb}]$. While this preserves the unaffected regions, it often results in poor harmonization with the input video. 
% The challenge arises from the initial iterations with noising to high timesteps, which are necessary for allowing new content emergence (as low frequency information are defined during initial de-noising steps). As a result, the new content can only be aware of the environment in $\mathcal{\hat{V}}_{comp}$ - one that is different from the input video.

To improve content harmonization, we propose a different approach: repeat the sampling process with AnchorExtAttn (Eq. \ref{eq:AnchorExtAttn}) multiple times, progressively reducing the level of noise added at each step. This iterative approach gradually refines the new content's interaction with the original scene. As shown in Fig.\ref{fig:pipeline}, we update $\bs{x}_{res}$ representing the difference between the generated output $\mathcal{\hat{V}}_{comp}$ and the original video  $\mathcal{{V}}_{orig}$ within the regions where new content appears, allowing each iteration to adjust the generated content's high-frequency details to better match the original video. We summarize our method in Alg. \ref{alg:main}.


% \myparagraph{Overall Algorithm} We summarize our Our framework, in Alg. \ref{alg:main}:
% In the first stage we obtain $[\mathbf{K}_\text{orig}, $

% DynVFX pipeline. Top row: Given an input video $\mathcal{V}_{\text{orig}}$, we apply DDIM inversion (see Sec. 3) and extract spatiotemporal keys and values $[\mathbf{K}_\text{orig}, \mathbf{V}_\text{orig}]$ from the original noisy latents. Given the user instruction $\mathcal{P}_{\text{VFX}}$ we instruct the VLM to envision the augmented scene and output the text edit prompt $\mathcal{P}_{\text{comp}}$, prominent object descriptions $\mathcal{O}_{\text{orig}}$ that are used to mask out the extracted keys and values and target object descriptions $\mathcal{O}_{\text{edit}}$. Bottom row: We estimate a residual $\bs{x}_{\text{res}}$ to the original video latent ($\bs{x}_{\text{orig}}$). This is done by iteratively applying SDEdit with our Anchor Extended Attention, segmenting the target objects ($\mathcal{O}_{\text{edit}}$) from the clean result, and updating $\bs{x}_{\text{res}}$ accordingly. 
% first, $\bs{M}_{VFX}^{rgb}$ is transformed into the latent space (see SM for details), and the generated content is composed directly in the latent space \rafail{add equation}. Next, we repeat the sampling process with AnchorExtAttn multiple times, progressively reducing the level of noise added at each step. \ref{alg:main}.



{\small 
\begin{algorithm}[t!]
\small
\begin{flushleft}
    \caption{\bf{DynVFX Algorithm}}
    \label{alg:main}
    \textbf{Input:}
    \begin{algorithmic}
     \State $\mathcal{V}_{\text{orig}}$, $\mathcal{P}_{\text{VFX}}$ \hfill $\triangleright$ Input video \& instruction prompt
     \State  $\mathcal{\tau}_{\text{A}}$ \hfill $\triangleright$ Extended Attention threshold
     \State  ${\Psi}$ \hfill $\triangleright$ Video segmentation model
     \State  ${\text{VLM}}$ \hfill $\triangleright$ Vision Language model 
    \end{algorithmic} 
    \textbf{Preprocess:}
    \begin{algorithmic}

    \State $\mathcal{P}_{\text{comp}}  \gets \text{VLM} 
    [\mathcal{V}_{\text{orig}}, \mathcal{P}_{\text{VFX}}]$ \hfill $\triangleright$ Composition Prompt
    \State $\mathcal{O}_{\text{orig}}, \mathcal{O}_{\text{edit}}   \gets \text{VLM}
    [\mathcal{V}_{\text{orig}}, \mathcal{P}_{\text{VFX}}]$ \hfill $\triangleright$ Original objects and VFX object
    
    \State ${M}_{orig} \gets \texttt{Get-Latent-Mask}(\Psi;\mathcal{V}_{\text{orig}}, \mathcal{O}_{\text{orig}})$ \hfill $\triangleright$ Extract source masks
    \State ${x}_{orig} \gets \texttt{Encode}[\mathcal{V}_{\text{orig}}]$ \hfill $\triangleright$ {\text{Encode video into latent space}}

    \State $\mathbf{K}_\text{orig}, \mathbf{V}_\text{orig} \gets \text{DDIM-Inv}[{x}_{orig}] \quad \forall t\in[T]$
    \end{algorithmic}
    \textbf{For} $t=\tilde{T},\dots,T_{min}$ \textbf{do}
    \begin{algorithmic}
        \State $\bs{x}_{res} = \bs{0}$ \hfill $\triangleright$ initialize the residual latent
        \State $\bs{x}_{comp} = {x}_{orig} + {x}_{res}$ 

        \State $\bold{\text{if}} \ t>\mathcal{\tau}_{A} \ \bold{\text{then}} \ {K^E,V^E} \gets \mathcal{F}(K_\text{orig} , M_\text{orig}), \mathcal{F}(V_\text{orig} , M_\text{orig})$
        \\
        $\bold{\text{else  }} 
        {K^E,V^E} \gets \emptyset $ \hfill
        \State $\bs{\hat{x}}_{comp} \gets \texttt{Sampling}[{x}_{comp}, \mathcal{P}_{\text{comp}}, t; \texttt{AnchorExtAttn}{[K^E,V^E]}]$ \hfill 
        \State $\mathcal{\hat{V}}_{comp} \gets \texttt{Decode}(\bs{\hat{x}}_{comp})$ \hfill $\triangleright$ Decode latent
        \State $\bs{M}_{VFX} \gets \texttt{Get-Latent-Mask}(\Psi;\mathcal{\hat{V}}_{comp}, \mathcal{O}_{\text{edit}})$ \hfill $\triangleright$ Extract VFX masks
        
        \State $\bs{x}_{res} = \bs{M}_{VFX} \cdot ({\hat{x}}_{comp} - {x}_{orig})$ \hfill
    \end{algorithmic}
    $\bs{x}_{comp} = {x}_{orig} + {x}_{res}$ \\
    ${\mathcal{V}}_{\text{comp}} \gets \texttt{Decode}[{x}_{comp}]$ \hfill $\triangleright$ {\text{Output video}}\\
    \textbf{Output:} $\mathcal{V}_{\text{comp}}$ 
\end{flushleft}\vspace{-0.1cm}
\end{algorithm}}





% As seen in Fig.~\ref{fig:ablation}(b),   omitting AnchorExtAttn leads to misaligned new content relative to the original scene - in both examples the added content is poorly localized either in terms of scale or position.

% Applying only the first iteration of our method (w/o iterative refinement, Fig.~\ref{fig:ablation}(d)) results in a better alignment with the input video, but the composition is still unstable, as evident, for example, in the puppy's body hovering over the box in the first scene.
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/comparison.pdf}\vspace{-0.3cm}
    \caption{Qualitative comparison. Sample results of our method, SDEdit \cite{meng2022sdedit}, DDIM inversion \cite{song2020_ddim}, Lora fine-tuning \cite{lora}, and Gen-3 \cite{gen3}. See SM for videos.}
    \label{fig:comparison}\afterfigure
\end{figure*}

\section{Results}
\label{sec:results}
We evaluated our method on a dataset of 18 publicly sourced videos, featuring a wide range of complex scenes in terms of camera and object motion, lighting conditions, and physical environments. 
Additionally, we use some videos from DAVIS  \cite{davis}. Our videos and implementation details are available in the SM.

Figures~\ref{fig:teaser},~\ref{fig:results} show sample results of our method. As seen, our method facilitates natural integration of broad range of visual effects, ranging from environmental effects (tsunami in Fig. \ref{fig:ablation} and explosion in Fig. \ref{fig:results}) to new object insertion (horse riding knight in Fig. \ref{fig:results} and dancing bear in Fig. \ref{fig:comparison}).  In all examples, the new content is naturally localized in the scene, even in challenging scenarios of multiple objects (dinosaurs or workers in Fig. \ref{fig:results}) and partial occlusions (puppy in Fig. \ref{fig:teaser} and giraffe in Fig. \ref{fig:results}). 

\subsection{Qualitative Evaluation}
To the best of our knowledge, no existing method has been designed to synthesize dynamic objects in a given real video without user masks or any user information except for the input video itself and a simple instruction.  We thus compare our method to the following baselines: \textit{(i)} SDEdit \cite{meng2022sdedit} using the same T2V model as ours, \textit{(ii)}   DDIM inversion \cite{song2020_ddim} and sampling with the target prompt, \textit{(iii)} LORA fine-tuning \cite{lora} of the T2V model and sampling with the target prompt and \textit{(iv)} Gen-3 \cite{gen3} video-to-video, designed for video stylization.

Figure \ref{fig:comparison} shows a qualitative comparison to the baselines. As can be seen, all baselines exhibit different limitations in maintaining scene fidelity while introducing new content. SDEdit \cite{meng2022sdedit} manages to fulfill the edit prompt, yet the scene has significantly deviated from the original one in terms of appearance, motion, positioning, or scale (e.g., deer in the creek). DDIM inversion is not suitable for editing. LORA fine-tuning suffers from the trade-off between preserving aspects of the original scene and adding new content to the scene. Either over-fitting original scene appearance (e.g., added dragon-dog hybrid), or under-fitting the original layout (e.g., incorrect scale of deer).
Gen-3 is conditioned on depth maps extracted from the input video, hence tends to significantly alter scene appearance and not allow the insertion of new objects that change the scene layout. In each case, these limitations affect the overall scene coherence and realism of the added elements. Our method successfully adds new content to the scene, achieving high fidelity to the user instructions while allowing for natural interactions between original and added elements  (e.g. natural interaction between woman and bear). 


\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l|cc|cccc|cc}
\toprule
Method & \multicolumn{2}{c|}{Metrics} & \multicolumn{4}{c|}{VLM-based evaluation} & \multicolumn{2}{c}{User Study} \\
\midrule
 & \makecell{CLIP\\Directional} & SSIM & \makecell{Text\\Alignment} & \makecell{Visual\\Quality} & \makecell{Edit\\Harmonization} & \makecell{Dynamics\\Score} & \makecell{Content\\Integration} & \makecell{Edit\\Harmonization} \\
\midrule
Gen-3 & 0.130 & 0.285 & 0.418 & 0.610 & 0.374 & 0.379 & 97.65 & 93.33 \\
LORA finetuning & 0.277 & 0.361 & 0.812 & 0.787 & 0.756 & 0.759 & 92.22 & 81.11 \\
DDIM inv. sampling & 0.184 & 0.444 & 0.535 & 0.699 & 0.528 & 0.529 & 99.20& 98.67 \\
SDEdit (0.9) & 0.272 & 0.332 & 0.794 & 0.799 & 0.754 & 0.756 & 98.91 & 82.13 \\
SDEdit (0.6) & 0.111 & 0.567 & 0.510 & 0.704 & 0.513 & 0.504 & 97.69 & 96.76 \\
\midrule
w/o AnchorExtAttn & \textbf{0.317} & 0.697 & 0.775 & 0.724 & 0.683 & 0.691 & 89.30 & 88.89 \\
w/o Iterative Refinement & 0.295 & 0.760 & 0.817 & 0.789 & 0.769 & 0.760 & 85.80 & 86.42 \\
Ours & 0.311 & \textbf{0.775} & \textbf{0.860} & \textbf{0.803} & \textbf{0.796} & \textbf{0.785} & - & - \\
\bottomrule
\end{tabular}
\caption{Quantitative Evaluation. We assess the quality of our method compared to several baselines. }
\label{tab:performance}\vspace{-0.9cm}
\end{table*}

\subsection{Quantitative Evaluation} \label{sec:quant}
We numerically evaluate our results using the following metrics:

\noindent \textit{(i) Edit fidelity.}  Following previous works (e.g., \cite{hsu2024autovfx,Tewel2024AdditTO}), we measure per-frame Directional CLIP Similarity \cite{clip,gal2021stylegannada} to assess the alignment between the change in the source and the target prompt, and the change between the source and edited frames. \vspace{0.05cm}

\noindent \textit{(ii) Original content preservation.} We evaluate how well the edited video preserves the original content outside the modified region. To this end, we segment the new content in the edited video using \cite{zhang2024evfsamearlyvisionlanguagefusion} , and compute the masked Structural Similarity Index (SSIM) over the complementary regions to the edited ones.\vspace{0.05cm}

\noindent \textit{(iii) \emph{VLM quality evaluation.}} We employ a Vision-Language Model (VLM) for expanding the per-frame metrics above in the following manner. We input to the VLM several frames from the edited videos and instruct it to evaluate four key aspects: how well the edit follows the text prompt (\emph{Text Alignment}), the overall visual quality of the edited frames (\emph{Visual Quality}), how well the new content is harmonized with the source frames (\emph{Edit Harmonization} and the realism of the added object's dynamics relative to the scene  (\emph{Dynamics Score}). For each aspect, the VLM outputs a score between 0 and 1, with higher scores indicating better performance. Our evaluation protocol is included in the SM.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/metrics.pdf} \vspace{-0.8cm}
    \caption{Metrics. We measure CLIP Directional score (higher is better) and masked SSIM (higher is better). Our method demonstrates a better balance between these two metrics.}
    \label{fig:metrics}\afterfigure
\end{figure}
Figure ~\ref{fig:metrics} and Table \ref{tab:performance} present the results of the described metrics on a set of 27 video-edit text pairs comprising 20 unique videos. As shown, our method outperforms the baselines in both SSIM and Directional CLIP metrics, demonstrating superior edit fidelity and maintaining higher structural similarity in the unedited regions. The VLM-based evaluation aligns with this assessment and additionally shows that our method produces videos that achieve better content integration and greater motion realism.  \vspace{0.05cm}


\noindent \textit{(iv) User study.} We conducted a user study to evaluate the ability to integrate the new content while preserving the original video. Participants were shown the input video, a text description of the new content, our result, and a baseline output. They were asked two questions: ``Which video better preserves the original footage while adding new content?'' and ``Which video better integrates the new content in a realistic and seamless way?''. In total, we collected 3240 user judgments from 120 users. As seen in Table~\ref{tab:performance}, our method is consistently preferred over all baselines.

\subsection{Ablations}
We ablate key design choices of our method: anchor extended attention and iterative updates of the edit, by excluding each component from our framework. 

As seen in Fig.~\ref{fig:ablation}(c),   omitting AnchorExtAttn leads to new content being misaligned relative to the original scene - the added content is poorly integrated into the original scene. 
Applying only the first iteration of our method (w/o iterative refinement, Fig.~\ref{fig:ablation}(d)) results in a better alignment with the input video, but the composition is still unstable, as evident, for example, in the puppy's body hovering over the box in the first scene. 
% \rev{This is because the video is noised to high time-steps in first iterations of our method, an important component for allowing emergence of new content, as low frequencies of the generated content are defined in the initialization of the de-noising process \cite{meng2022sdedit, bar2023multidiffusion}. Hence, in the sample $\mathcal{\hat{V}}_{comp}$ we segment the content from, the complement of the segment is not a reconstruction of the original video, and so forth the new content can only be aware of the environment in $\mathcal{\hat{V}}_{comp}$ - one that is different from the input video.}
Our full method achieves better composition with proper spatial relationships, demonstrating the importance of both components for realistic scene editing Fig.~\ref{fig:ablation}(f). We numerically evaluate each ablation with the same set of metrics described in Sec~\ref{sec:quant} and report them in Table~\ref{tab:performance}. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/limitations.pdf}
    \caption{Limitations. In some cases, the T2V diffusion model struggles to precisely follow the edit prompt}
    \label{fig:limitation}\afterfigure 
\end{figure}





\section{Discussion and Conclusions}
\label{sec:conclusions}
We introduced the task of augmenting real videos with new dynamic content based on a user-provided instruction. We presented a zero-shot method utilizing the T2V diffusion model in a feature manipulation framework, enabling correct localization and natural blending of new content with existing video elements. 

As our method is built upon the pre-trained T2V diffusion model, the quality of the generated edits is inherently tied to the performance and capabilities of the underlying model. As seen in Fig. \ref{fig:limitation}, the T2V model sometimes struggles with generating videos precisely following the edit prompt. Additionally, the text-based localization relies on the capabilities of the segmentation model \cite{zhang2024evfsamearlyvisionlanguagefusion}, which can sometimes produce inaccurate masks and fails to account for effects like shadows and reflections if not specified in the text prompt. 
Despite the limitations, our method significantly improves over baselines, expanding the capabilities of pre-trained text-to-video diffusion models.

% \newpage


\section{Acknowledgement}
This project received funding from the Israeli Science Foundation (grant 2303/20).

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}
\newpage
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/ablations_extra.pdf} 
    \caption{Additional examples for Ablations.}\afterfigure
    \label{fig:ablation_extra}
\end{figure*}
% \section{Appendices}


\appendix

\section{Implementation Details}

\subsection{Models} 
\myparagraph{Text-to-Video Model}. We use a publicly available CogVideoX-5B \cite{hong2022cogvideo,yang2024cogvideox} text-to-video model, which can generate videos with up to 480x720 pixel resolution, 6 seconds in length, 49 frames at 8 fps. This model is a transformer-based model that processes both text and video modalities together. 

\myparagraph{Segmentation Model}. To segment the prominent objects in the video and the newly generated content, we utilize EVF-SAM2 \cite{zhang2024evfsamearlyvisionlanguagefusion} - a text-based video segmentation model based on SAM2 \cite{Ravi2024SAM2S}.

\myparagraph{Visual Language Model.}. Our vision-language model of choice is GPT-4o \cite{achiam2023gpt4}, which we use through the official OpenAI API.

\subsection{Keys and Values Extraction}
Following \cite{Tumanyan_2023_CVPR,yatim2023spacetime}, to obtain T2V diffusion model intermediate latents, we use DDIM inversion (applying DDIM sampling in reverse order) on the input video, using 250 forward steps, with an empty string as text prompt. During the forward pass in our method, the intermediate latents are used for the extraction of keys and values.

\subsection{Latent Mask Extraction}
As discussed in Sec. 4.3, we iteratively update the residual latent $\bs{x}_{res}$ in the regions where the new content appears. This requires calculating the mask of the new content in the latent space. To do this, we first apply the segmentation model \cite{zhang2024evfsamearlyvisionlanguagefusion} to the current output of SDEdit and get the mask of the new content in RGB space.  However, the VAE in the T2V diffusion model involves both spatial and temporal downsampling, making it challenging to directly map RGB pixels to their corresponding latent regions. To address this, we encode the RGB masks through the VAE-Encoder and apply clustering to partition the resulting latents into two groups, effectively producing downsampled masks that align with the latent space representation. 


\subsection{Runtime}
Our method's two most computationally intensive parts are - DDIM inversion, which takes \mytextapprox 15 minutes, and iterative updates of the edit residual, which takes \mytextapprox 20 minutes. Importantly, DDIM inversion needs to be performed only once per video and can support multiple subsequent edits, making the process more efficient when applying various modifications to the same video content.


\section{Baselines comparison details} 
For LORA fine-tuning baseline, we use the following default hyperparameters: Adam optimizer \cite{adam}, $1e-4$ learning rank, LORA rank 128, 800 fine-tuning steps.
For comparison with GEN-3 \cite{gen3}, we utilize the Gen-3 Alpha model via the publicly accessible web-based API, setting the "Structure Transformation" hyperparameter to 5.


\section{Additional comparisons}

We perform an additional qualitative comparison to MagicVFX \cite{Guo2024MagicVFXVE}. As can be seen in Fig.~\ref{fig:magicvfx_comparison}, MagicVFX struggles to remain faithful to the original scene and has lower visual quality compared to our method.


% \begin{figure}[h]
%     \centering
% \includegraphics[width=\linewidth]{figures/magicvfx_comparison.pdf}
%     \caption{Comparison to MagicVFX. The result of MagicVFX the output differs significantly from the original video.}
%     \label{fig:magicvfx_comparison}
% \end{figure}

\begin{figure*}[h]
    \centering
\includegraphics[width=\linewidth]{figures/magicvfx_comparison.pdf}
    \caption{Comparison to MagicVFX. The result of MagicVFX the output differs significantly from the original video.}
    \label{fig:magicvfx_comparison}
\end{figure*}


\begin{figure*}[h]
    \centering
\includegraphics[width=\linewidth]{figures/protocol.pdf}
    \caption{Output example for protocol}
    \label{fig:protocol}
\end{figure*}

\begin{figure*}[h]
    \centering
\includegraphics[width=\textwidth]{figures/system_prompt.pdf}
    \caption{VLM instructions used for generating the textual descriptions.}
    \label{fig:system_prompt}
\end{figure*}

\begin{figure*}[h]
    \centering
\includegraphics[width=\textwidth]{figures/vlm_evaluation_prompt.pdf}
    \caption{VLM evaluation protocol}
    \label{fig:vlm_evaluation}
\end{figure*}

% \section{Additional examples for ablations}
% \danah{add text about tsunami}
\section{VLM Prompting}
While the model gives an accurate, descriptive source scene caption, in some cases, we observed that it fails to give captions suitable for compositing VFX with the scene. To overcome this, we ask the model to imagine a conversation with a visual effects (VFX) artist to obtain a caption that would describe the composited scene correctly. In this conversation, GPT-4o will "consult" with a VFX artist about how the new content should be integrated into the scene. Based on their discussion, it will be asked to provide a caption that describes how the added content fits into the scene.
This results in text prompts that encourage the generated output video to include a natural interaction between the new content and the original environment. In this prompt, we also ask the VLM to provide a list of prominent foreground objects in the original video: $\mathcal{O}_{\text{orig}}$ and the object that will be added according to the edit prompt: $\mathcal{O}_{\text{edit}}$. The full prompt for the VLM is shown in Figure ~\ref{fig:system_prompt}.

In addition, as discussed in Sec.~5.2 we utilize the VLM for interpretable quality assessment. The full set of instructions for the VLM can be seen in Fig.~\ref{fig:vlm_evaluation}.


%  by instructing it to have an in-context discussion with a "VFX artist" - an expert for reasoning in this specific task. In our reasoning instructions, we highlight specific attributes that the model needs to follow:
% \begin{itemize}
% \item Be aware of existing visual elements in the scene - 
% interaction with foreground objects, environmental elements, and physical elements in terms of position and dynamics.
% \item Acknowledge what is "locked" in the scene - natural interaction without affecting the original characters' behavior or actions.
% \item Atmosphere Flexibility  - Allowing overall atmosphere to change with added content
% \end{itemize}

% Source Scene Caption:

% **Note**: If a source scene prompt is provided, use it as is!

% Provide a detailed description of the source scene without considering the added content.
% Focus on the existing objects, environment, and actions in the scene.

% VFX Conversation:
% Imagine a conversation with a Visual Effects (VFX) artist about how the new content should be integrated into the scene. Remember, the new content can be objects or multiple objects or effect or really anything the user provides. so be clear to explain this to the VFX artist.
% The new content should interact naturally with the environment (e.g., shadows, lighting, or physical elements like grass, water, or other objects) but without altering the dynamics of the source scene.
% The object must fit into the scene without affecting the original characters' behavior or actions.
% The interaction between new content and foreground object must be included (e.g. object A is interacting with object B). in terms of dynamics and motion as well.
% Describe how the object interacts and how it blends into the scene.

% Composited Scene Caption:
% Based on the conversation with the VFX artist, provide a caption that describes how the added content fits into the scene.
% The caption must reflect natural interaction between the new content and the environment (e.g., lighting, shadows, physical effects), while ensuring the original dynamics remain unchanged.
% The content should be aware of the surroundings, but the behavior, and flow of the original scene should remain consistent. The overall atmosphere might change of course due to this addition to scene.

% **Output format*** -  a dictionary with keys: "source_scene_caption", "vfx_conversation", and "composited_scene_caption".

% - **source_scene_caption**: source_scene_caption will be - A detailed caption of the source scene. If provided, use the given caption.

% - **vfx_conversation**: A simulated conversation about how the new content should be integrated into the scene.

% - **composited_scene_caption**: will be - A detailed caption of the composited scene, integrating the new content.


% user input:

% XXX

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
