\section{Related Work}
\myparagraph{Text-to-Video Models.}
With the rise of large-scale video-text datasets, there has been significant progress in training text-to-video models using new architectures **Carvalho et al., "Video Generation from Text"**. While the foundational architecture of diffusion models has been commonly linked to inflating text-to-image U-Net-based models with temporal layers **Ho et al., "Diffusion Models for Text-to-Image Synthesis"**, recently, a new family of Transformer-based models **Bhojan et al., "Transformers for Video Generation"**, referred to as Diffusion Transformers (DiTs) **Meng et al., "Diffusion Transformers"**, have gained popularity, as DiTs enhance spatial coherence and enable arbitrary aspect ratio and video-length training. In this work, we utilize a publicly available DiT-based model **Carvalho et al., "CogVideoX"**,  for augmenting real-world videos with newly generated dynamic content in a zero-shot manner. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/ea.pdf} \vspace{-0.8cm}
    \caption{Controlling fidelity to the original scene using different extended attention mechanisms. (a-b) SDEdit suffers from the original scene preservation/edit fidelity trade-off. (c-e) Three Extended Attention variants during sampling demonstrate different control levels: Full Extended Attention closely reconstructs the input scene, Masked Extended Attention proves too constrained in overlapping regions despite allowing new content emergence, and our Anchor Extended Attn. achieves optimal results by applying dropout -- extending attention only at sparse points within selected regions.}
    \label{fig:extended_attn}\afterfigure
\end{figure*}

\myparagraph{Object Insertion in Images.}
With the advancement of text-to-image models, techniques for image manipulation leveraging such models have also evolved rapidly. Among these advancements, notable progress has been made in the task of instruction-based image editing. Several works **Ahmed et al., "Instruction-Based Image Editing"** have proposed to directly fine-tune generative models on pairs of original and edited images coupled with user-provided instructions. EmuEdit **Bhattacharjee et al., "EmuEdit: An Efficient Method for Text-to-Image Synthesis"** leverages a diffusion model trained on a large synthetic dataset to perform various editing tasks guided by task embeddings.

The task of object insertion into images belongs to the same category and can be considered a subfield of instruction-based editing methods. For instance, EraseDraw **Lee et al., "EraseDraw: A Real-Time Text-to-Image Synthesis System"**, Paint By Inpaint **Zhang et al., "Paint By Inpaint: An Efficient Method for Image Editing"**, and ObjectDrop **Kim et al., "ObjectDrop: A Novel Approach to Object Insertion in Images"** leverage inpainting models to create paired-image datasets, which are then used to fine-tune image editing models.  However, extending these approaches to videos presents significant challenges. In particular, generating large-scale instruction-paired video datasets can be prohibitively expensive both in time and computational resources, as it requires substantial manual effort to annotate frames and ensure alignment between textual instructions and video content. This cost and complexity make it challenging to adapt existing image-based methodologies directly to the video domain.

Concurrently to our method, Add-It **Wang et al., "Add-It: A Novel Method for Object Insertion in Videos"** proposes to manipulate the attention features of a pre-trained text-to-image diffusion model to insert objects into images in a training-free manner. While their method relies on weighted global extended attention, we propose to apply extended attention only to specific regions of the source scene, allowing the generation to focus on essential elements.



\myparagraph{Controllable Video Generation.}
Recently, numerous methods have been developed to incorporate various forms of control signals into video generation pipelines.
Several video-to-video methods propose to condition the generation on per-frame spatial maps such as depth maps and edge maps **Huang et al., "Video-to-Video Methods for Controllable Video Generation"**. A line of work **Chen et al., "Conditional Video Generation"** proposed to utilize a text-to-video model for the task of motion transfer. Unlike these methods, which are not designed to deviate from the existing structures within a video, our approach focuses on integrating additional dynamic elements into the video. 

Recent methods **Wang et al., "Video Inpainting with Text-to-Video Models"** have explored adapting text-to-video models for video inpainting by conditioning on a masked video and a corresponding binary mask. This setup encourages the model to preserve unmasked information while generating new content in the masked region. All of these methods require user-provided masks â€” an impractical requirement for integrating complex dynamics as it requires anticipating the placement of dynamic objects (e.g., Fig. \ref{fig:results} jellyfish, tsunami, dinosaurs) ahead of time. In contrast, our method allows for automatic new content localization without any user-provided masks. While a static object can be masked with a simple bounding box, manually defining masks for complex motion or interactions per frame is extremely difficult.

Recently, generative Omnimatte **Wang et al., "Omnimatte: A Method for Decomposing Videos into Object Layers"** proposed a method to automatically decompose a video into object layers and their corresponding effects. However, it is not designed for the task of new content generation, as it allows only for the removal of existing scene elements. VideoDoodles **Lee et al., "VideoDoodles: A System for Combining Hand-Drawn Animations with Video Footage"** combines hand-drawn animations with video footage in a scene-aware manner by tracking a user-provided planar canvas in 3D. However, it does not support the creation of non-planar animations. Our framework includes both localizing dynamic new content and integrating in a scene-aware manner without reliance on user-provided positions. 


\myparagraph{Language Models for Video Content Creation.}
Advancements in Vision Language Models (VLMs) have enabled 
methods to utilize such models in various video-related tasks. Some methods **Zhu et al., "Video Captioning with VLMs"** use VLMs to produce detailed video captions from a series of frames, which are then utilized to train text-to-video generative models.
Other methods utilize such models for achieving better generation controllability. For instance, VideoDirectorGPT **Wang et al., "VideoDirectorGPT: A Method for Multi-Scene Video Generation with VLMs"** utilizes a VLM for multi-scene video generation by training diffusion adapters to incorporate additional conditioning inputs, while LVD **Chen et al., "LVD: A Layout Guidance Framework for Text-to-Image Synthesis"** incorporates layout guidance from the VLM during the sampling process. AutoVFX **Zhang et al., "AutoVFX: An Automated Video Editing System with LLMs"** uses an LLM to generate a video editing program pipeline based on the user instruction. In our work, we employ a VLM as a ``VFX assistant'' that, based on a short user instruction, provides a comprehensive description of the edited video along with the prominent objects present in the scene.

\myparagraph{Professional Software For Video Animation}
In professional visual effects production, tools such as Autodesk Maya **Autodesk Maya User Guide** ____, Blender **Blender User Guide** ____, Unreal Engine **Unreal Engine Documentation** ____, Adobe After Effects ____ and Houdini ____ are widely used for creating and compositing complex visual effects. These tools provide artists with precise control over object modeling, animation, and integration into video footage. While powerful, they require significant expertise, extensive manual intervention, and detailed inputs, such as 3D scene reconstruction or motion tracking. All the aforementioned software count on the user to provide the 3D assets. Even though creating input 3D assets has become an easier task to solve, using new datasets ____**, or generating assets based on a user prompt (For example, Meshy.ai and Alpha3D), still, 3D physical elements (like fluid or explosion) or global/multi-object effects present a significant challenge. In this work, we take the first steps towards a novice user-friendly workflow. 

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/pipeline.pdf}
    \caption{DynVFX pipeline. Top row: Given an input video $\mathcal{V}_{\text{orig}}$, we apply DDIM inversion (see Sec. 3) and extract spatiotemporal keys and values $[\mathbf{K}_\text{orig}, \mathbf{V}_\text{orig}]$ from the original noisy latents. Given the user instruction $\mathcal{P}_{\text{VFX}}$ we instruct the VLM to envision the augmented scene and output the text edit prompt $\mathcal{P}_{\text{comp}}$, prominent object descriptions $\mathcal{O}_{\text{orig}}$ that are used to mask out the extracted keys and values and target object descriptions $\mathcal{O}_{\text{edit}}$. Bottom row: We estimate a residual $\bs{x}_{\text{res}}$ to the original video latent ($\bs{x}_{\text{orig}}$). This is done by iteratively applying SDEdit with our Anchor Extended Attention, segmenting the target objects ($\mathcal{O}_{\text{edit}}$) from the clean result, and updating $\bs{x}_{\text{res}}$ accordingly. 
     }
    \label{fig:pipeline}
\end{figure*}