\documentclass[11pt, a4paper, oneside, reqno]{amsart}
\input{style_PM}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% Acknowledgments
\long\def\acks#1{\vskip 0.3in\noindent{\large\bf Acknowledgments and Disclosure of Funding}\vskip 0.2in
\noindent #1}

% ======== User defined packages ========

\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% Attempt to make hyperref and algorithmic work together better

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Included by the authors
\usepackage[inline]{enumitem}
\usepackage{multirow}
\usepackage[para]{threeparttable}
\usepackage{adjustbox}
\usepackage[short,c2]{optidef}
\usepackage{array}
\usepackage{physics}
\usepackage{svg}
\usepackage{multirow}
\usepackage{bm}
\usepackage{dblfloatfix}
\usepackage{soul}
\usepackage{hyperref}
\usepackage{natbib}


\RequirePackage{algorithm}
\RequirePackage{algorithmic}

% Add figure path
\graphicspath{{figs/quadrotor/}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\theoremstyle{definition}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% Checkmarks
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Custom operators
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\blkdiag}{blkdiag}
\DeclareMathOperator*{\argmin}{\arg\min}

% =======================================

\title{Offline Reinforcement Learning via Inverse Optimization}

\author{Ioannis Dimanidis}
\author{Tolga Ok}
\author{Peyman Mohajerin Esfahani}

\thanks{The authors are with the Delft Center for Systems and Control, TU Delft, The Netherlands. (e-mails: ioanndima@gmail.com, tok@tudelft.nl, P.MohajerinEsfahani@tudelft.nl.)}


\begin{document}


\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
Inspired by the recent successes of Inverse Optimization (IO) across various application domains, we propose a novel offline Reinforcement Learning (ORL) algorithm for continuous state and action spaces, leveraging the convex loss function called ``sub-optimality loss" from the IO literature.
To mitigate the distribution shift commonly observed in ORL problems, we further employ a robust and non-causal Model Predictive Control (MPC) expert steering a nominal model of the dynamics using in-hindsight information stemming from the model mismatch. 
Unlike the existing literature, our robust MPC expert enjoys an exact and tractable convex reformulation.
In the second part of this study, we show that the IO hypothesis class, trained by the proposed convex loss function, enjoys ample expressiveness and achieves competitive performance comparing with the state-of-the-art (SOTA) methods in the low-data regime of the MuJoCo benchmark while utilizing three orders of magnitude fewer parameters, thereby requiring significantly fewer computational resources.
To facilitate the reproducibility of our results, we provide an open-source package implementing the proposed algorithms and the experiments.
\end{abstract}
\maketitle

\section{Introduction}
\label{sec:introduction}

In dynamic environments where real-world interactions are impractical, there is often the need to work with datasets of previously collected interactions. Decision-making in these contexts typically follows one of two paradigms.
(i) Imitation learning (IL), a subclass of the Supervised Learning (SL) paradigm, in which the aim is to imitate a given expert's decisions (i.e., labels in SL terms) and (ii) offline Reinforcement Learning (RL), where the aim is to learn a policy that improves upon the performance observed within the dataset.
SL in general, and IL in particular, has proven to be successful in a wide range of applications~\citep{husseinImitationLearningSurvey2017}, while offline RL is known to be a notoriously hard task (both computationally and statistically)~\citep{bertsekasAbstractDynamicProgramming2021}.
One of the primary challenges in offline RL is the mismatch between the dataset and the policy distributions. Hence, naively applying existing online RL algorithms combined with high-capacity Q function approximation leads to optimistic and potentially biased value functions, which, in turn, leads to poor performing and unstable policies that do not generalize in the online evaluation.


To combat these issues, in this work, we approach the offline RL problem in two steps: (i) by utilizing a non-causal expert, we perform an ``action improvement'' step over the dataset; and (ii) using the improved actions, we fit a Q-function using a novel ``sub-optimality loss" to obtain an efficient and causal policy that generalizes over online evaluations.
 Specifically, in the first step, by leveraging a nominal model and in-hindsight model mismatch information, 
unknown at runtime, we introduce an expert in the form of a non-causal Model Predictive Control (MPC). 
To realize the non-causal expert, we propose to replace the Bellman residual loss with the ``sub-optimality loss" drawn from the Inverse Optimization (IO) literature that fits the optimal Q function given the improved dataset.
The proposed optimization problem enjoys the convexity of the loss function, yielding an efficient and causal policy that can generalize over unseen states.
Before proceeding with further details regarding our proposed approach and the related literature, we introduce some notations. 

\paragraph{{\bf Notation:}}
The dimension of a variable $x$ is denoted by $n_x$.
We denote with $N$ the MPC horizon and with $T$ the size of a dataset.
With bold, we denote the stacking of variables, i.e, $\mathbf x_{1:N} = (x_1, x_2, \ldots, x_N)$, unless noted otherwise.
When no exact range is given in the subscript, the default length of a bold variable is $N$ (i.e., $\mathbf x = \mathbf x_{1:N}$).
We denote by~$\langle \cdot, \cdot\rangle$ an inner product with the respective norm~$\norm{x}^2 = \langle x, x\rangle$.
For any $A \succ 0$, we define $\norm{x}_A^2 = \langle x, Ax\rangle$.
With $\otimes$, we denote the Kronecker product.
As the letter ``Q'' will be used to indicate both matrices and Q-functions, we denote with $Q$ the former and with $\mathrm{Q}(s,u)$ the latter, although it should usually be clear from the context. The operators~$\diag(\cdot)$ and $\blkdiag(\cdot)$ construct a square or block matrix, respectively. Finally, with MPC-$N$, we refer to policies stemming from the minimization of an $N$-stage cost that predicts the future behavior of the system using a nominal model.

\subsection{Problem statement and contributions}
We consider discrete-time dynamical systems of the form
\begin{equation}
    \label{eq:dynamics_model}
   x_{t+1} = f(x_t, u_t, v_{t+1})
\end{equation}
subject to disturbances $v_t$ governing the stochasticity within the true dynamics function $f$. With $f$ and the distribution of $v_t$ being unknown to us, we opt to minimize the online $N$-stage cost
\begin{equation}
    \label{eq:cost}
    % \begin{multlined}
      \mathrm{Cost}^f_N(x_t, \mathbf{u}_t, \mathbf v_{t+1}) \coloneqq %\\
         \left \{
            \sum_{k=0}^{N-1} c(x_{t+k}, u_{t+k}) + c_f(x_{t+N}) :~
            % x_{t+1} = f(x_t, u_t, v_{t+1})
            {\textup{s.t.}~\eqref{eq:dynamics_model}}
         \right \}
     % \end{multlined}
\end{equation}
    over the constraint set $\mathcal U_N(x_t)$, where $c$ is a running stage cost, and $c_f$ is the terminal cost akin to the state-value function in the RL literature. As the true dynamics $f$ are unknown, we assume knowledge of a nominal model $f_0$ that approximates $f$. Additionally, we assume access to a dataset of previously collected trajectories $\left\{\hat x_t, \hat u_t\right\}_{t=1}^T$ stemming from $f$. The goal is to learn a parameterized policy $\pi_\theta$ that approximates the true minimizer of~\eqref{eq:cost} by exploiting in-hindsight information to mitigate the effect of model-mismatch in a data-driven fashion while satisfying the constraints imposed by $\mathcal U_N(x_t)$. A high-level view of our proposed RL scheme comprises two key steps:
\begin{itemize}[itemsep = 2mm, topsep = 1mm, leftmargin = 6mm]
    \item {\it In-hindsight action improvement:} By looking into the past observations, we opt to determine an optimal sequence of decisions with respect to a nominal model that would have minimized the desired cost~\eqref{eq:cost} while satisfying the constraints introduced by the dynamics.
    \item {\it Imitation learning:} Given the actions in the previous step, we propose a tractable and convex imitation learning scheme to learn a policy. 
\end{itemize}
In this work, we first study a particular form of MPC in which an optimization program determines the experts' actions. Following that, we propose IO as an IL framework to learn such desired actions. More specifically, our contributions are summarized as follows. 

\begin{enumerate}[label=(\roman*), itemsep = 2mm, topsep = 1mm, leftmargin = 6mm]
    \item \label{it:loss_contrib}{\bf Convex formulation of an offline RL loss:}
    We introduce a non-causal MPC expert that exploits the in-hindsight information to generate optimal but non-causal control actions.
    Combined with the ``sub-optimality loss" from the IO literature, the improved control actions provide a unique offline RL framework that is convex in policy parameters. 
    The convex nature of the proposed framework is particularly important since, aside from its computational benefits, it also opens the door for tools from online convex optimization to be readily used for the control tasks considered here.
    % as it opens the door to the tools from online convex optimization to be used for control tasks in offline RL. \todo{isn't it the other way around?}

    \item \label{it:robust_contrib}{\bf Tractable robustification w.r.t.\ disturbance trajectory:}
    We further propose a robust counterpart to the non-causal MPC where the disturbance trajectory has an adversarial role within a pre-specified set. 
    The proposed robust MPC enjoys an exact convex reformulation, a result that, to our best knowledge, has not been achieved before (Theorem~\ref{thm:robust_mpc_reform}).
    From the empirical analysis of Appendix \ref{appdx:num_exps} and Section \ref{sec:num_exp:quadrotor}, we show that the robustification helps combat the distribution shift from the training to the test~phase and the mismatch between the nominal model and the true nonlinear dynamics.
    
    \item \label{it:mujoco_bench_contrib}{\bf Expressiveness of the IO hypothesis in the MuJoCo benchmark:}
    We empirically study the proposed IO framework, formulated as the minimization of the loss given by \eqref{eq:sub_loss}, in the standard offline MuJoCo benchmark \citep{d4rl}.
    In our experiments shown by Table \ref{table:mujoco_scores}, the proposed approach achieves performance on par with the SOTA offline RL algorithms in the low-data regimes despite employing up to 3000 times fewer parameters.
    
    \item \label{it:matlab_contrib}{\bf Open source package:} We provide a Python package \citep{repository} implementing the proposed algorithm and producing the results of Section ~\ref{sec:num_experiments}.
\end{enumerate}


With regards to item~\ref{it:loss_contrib}, we propose a novel data-driven way to train MPC-1 policies (greedy policies in Q-Learning terminology) that are parameterized in $\theta$ and are of the form 
\begin{equation}
   \label{eq:policy_class}
   \pi_\theta (s_t) = \argmin_{u \in \mathcal U(s)} \mathrm{Q}_\theta(s_t,u),
\end{equation}

% such that they minimize \eqref{eq:cost} subject to constraints $\mathcal U_N(x_t)$,
where $s_t$ is an augmented state with some potentially non-Markovian features of past states and inputs, i.e., $s_t = \phi(\mathbf x_{1:t}, \mathbf u_{1:t})$, for some feature map $\phi$. 
Note that the potential dependence of the feature map on past control actions indicates that the policy may have inner dynamics.
As the IO framework requires an expert to mimic, we use a non-causal MPC that exploits the nominal model $f_0$ and the in-hindsight model-mismatch information to empirically minimize~\eqref{eq:cost} subject to constraints $\mathcal U_N(x_t)$.
These in-hindsight expert demonstrations are used as training examples in the minimization of a ``sub-optimality loss" that is convex in $\theta$ for an appropriate parameterization of $\mathrm{Q}_\theta$.
To sufficiently approximate this non-causal expert in a manner so that the resulting policy is causal and implementable, we propose the aforementioned augmented state $s_t$ defined by the potentially non-Markovian causal feature map $\phi$ (e.g., past mismatch/disturbance realizations up to time $t$). This is elaborated in detail in Section~\ref{sec:inverse_opt}.

Moreover, to expand on item~\ref{it:robust_contrib}, we also develop a new min-max MPC scheme that robustifies against a ball centered around a future disturbance trajectory $\mathbf w$. With such an uncertainty set, we are able to exploit the S-Lemma to recover a tractable reformulation (Theorem~\ref{thm:robust_mpc_reform}), which, as far as we know, is a novel result. This is presented in Section~\ref{sec:rmpc}.


\subsection{Related works}
\label{sec:related_works}

Some recent advances in RL, also termed as model-free or data-driven control, are reported in Table~\ref{tab:rl_categories}. The table summarizes the literature across different categories of policy class (linear/nonlinear, statics/dynamics) and the training procedure, which will also be briefly discussed below.


\begin{table*}[t!]
\adjustbox{width=\textwidth}{%
\centering
\begin{threeparttable}[b]
    \caption{Classification of some Reinforcement Learning methods for the control of dynamical systems. }
    \label{tab:rl_categories}
    \vskip 0.15in
    \begin{tabular}{@{}llcclc@{}}
    \toprule
    \multirow{2}{*}{Category}  & \multirow{2}{*}{Paper} & \multirow{2}{*}{Policy class}\tnote{a} & \multicolumn{2}{c}{Training} & \multirow{2}{*}{Dynamic}\tnote{b} \\ \cmidrule(lr){4-5}
    & & & Model-based & Learning policy parameters   & \\ \midrule
    Offline RL  & \citet{IQL} & Neural Network & \xmark & Regularized Value Iteration & \xmark \\
                & \citet{CQL} & Neural Network & \xmark & Regularized Value Iteration & \xmark \\
                & \citet{COMBO} & Neural Network & \cmark & Regularized Value Iteration & \cmark \\
    Imitation Learning  & \citet{BAIL} & Neural Network & \xmark & Mean Squared Error & \xmark \\
    Q-Learning  & \citet{bradtkeAdaptiveLinearQuadratic1994} & Linear & \xmark & Policy Iteration & \xmark \\
                & \citet{kiumarsiReinforcementQLearningOptimal2014} & Linear & \xmark & Policy Iteration & \xmark \\
    Regret-min. & \citet{deanSampleComplexityLinear2020} & Linear & \cmark & System Level Synthesis (SDP) & \xmark \\
               %  & \citealp{cohenLearningLinearQuadraticRegulators2019} & Linear & \cmark & Robust LQR (SDP) & \xmark \\
               %  & \citealp{laleExploreMoreImprove2020} & Linear & \cmark & Riccati & \xmark \\
                & \citet{simchowitzNaiveExplorationOptimal2020} & Linear & \cmark & Riccati & \xmark \\
                & \citet{agarwalOnlineControlAdversarial2019} & Linear & \cmark & Online convex optimization & \cmark  \\
                & \citet{hazanNonstochasticControlProblem2020} & Linear & \cmark & Online convex optimization & \cmark \\
               %  & \citealp{fosterLogarithmicRegretAdversarial2020} & Linear & \cmark & Online convex optimization & \cmark \\
    Willems' Lemma & \citet{depersisFormulasDataDrivenControl2020} & Linear & \cmark\tnote{1} & LMI-based design (SDP) & \xmark \\
                   & \citet{vanwaardeNoisyDataFeedback2022} & Linear & \cmark\tnote{1} & S-Lemma-based design (SDP) & \xmark \\
                   & \citet{coulsonDataEnabledPredictiveControl2019}  & MPC-$N$ & \cmark\tnote{1}  & --- & \xmark \\
    Terminal VF Approx.  & \citet{zhongValueFunctionApproximation2013} & MPC-$N$ & \cmark & Non-convex optimization & \cmark \\
                         & \citet{lowreyPlanOnlineLearn2018} & MPC-$N$ & \cmark & Non-convex optimization & \cmark \\
                         & \citet{bhardwajBlendingMPCValue2020} & MPC-$N$ & \cmark & Non-convex optimization & \cmark \\
                        %  & This work & MPC-$N$\tnote{1} & \cmark & Non-convex optimization & \cmark \\ 
                        % & This work & MPC-1\tnote{1} & \cmark & Inverse Optimization (SDP) & \cmark \\
                        & This work & MPC-1 & \cmark & Inverse Optimization (SDP) & \cmark\tnote{2} \\
      \bottomrule
      \end{tabular}
      \begin{tablenotes}
         \item[a] Refers to the mapping between state/feature and input/action.
         \item[b] Refers to whether the policy has states and dynamics of its own.
         \item[1] Instead of casting the model identification and policy learning as two independent optimization problems, these methods combine them into a single program, thereby finding the most optimal model for the control objective.
         \item[2] While not necessary, these policies can be designed to depend on previous instances of the states and/or actions.
      \end{tablenotes}
   \end{threeparttable}}
%   \vskip -0.1in
\end{table*}


\paragraph{Q-Learning:}
The policy class considered in \eqref{eq:policy_class} is that of a greedy policy in Q-Learning literature. However, the minimization of $\mathrm Q_\theta$ is often problematic as it is often non-convex in $u$. Therefore, works in this domain either have to resort to discretized state and action spaces \citep{watkinsQlearning1992} or to Neural Network approximations of the minimizer of  $\mathrm Q_\theta$ \citep{lillicrapContinuousControlDeep2015}. There are Q-Learning settings where there is convexity in $u$, such as LQR \citep{bradtkeAdaptiveLinearQuadratic1994,kiumarsiReinforcementQLearningOptimal2014}, but there is no support for constrained state and input spaces. As we consider a parametric optimization policy class, we  allow for constrained input and state spaces.

\paragraph{Regret minimization:}
Works in this area consider the online data-driven linear policies:
\begin{equation}
   \label{eq:min_regret_policy}
   \pi_K(x_t) = -K_0 x_t - \sum_{\tau=1}^H K_\tau w_{t-\tau}.
\end{equation}
When disturbances are entirely stochastic and i.i.d., we have from Certainty Equivalence that the disturbance feedback terms of~\eqref{eq:min_regret_policy} can be neglected, and the problem reduces to finding an appropriate model and solving a Riccati-like problem to recover $K_0$~\citep{deanSampleComplexityLinear2020,simchowitzNaiveExplorationOptimal2020}. On the other hand, when disturbances are correlated, these terms are necessary and are discovered through Online Convex Optimization~\citep{agarwalOnlineControlAdversarial2019,hazanNonstochasticControlProblem2020}. In both scenarios, however, the policy class is linear, and as such, they only concern the unconstrained setting.


\paragraph{Willems' Lemma:} This lemma~\citep{willemsNotePersistencyExcitation2005} provides a data-based model parameterization that combines system identification and controller design in one step.
In~\citep{depersisFormulasDataDrivenControl2020,vanwaardeNoisyDataFeedback2022} it is used to express LQR synthesis as a data-driven optimization program, while in DeePC~\citep{coulsonDataEnabledPredictiveControl2019} the lemma is used in an MPC setting. Such results require decision variables that are much larger in dimension than traditional approaches.
Conversely, we offer reduced complexity in that regard by shrinking the MPC-$N$ problem to MPC-1.

\paragraph{Offline Reinforcement Learning:} To prevent the value function from exploiting any dataset bias, offline RL approaches typically attempt to enforce pessimistic policy learning; this can be achieved by constraining the policy learning within the region supported by the dataset~\citep{BCQ, guo2020batch}, penalizing state-action pairs outside the dataset~\citep{BEAR, BRAC, IQL, CQL} , or by the primal-dual optimization approach \citep{gabbianelli2023offline, hong2023primal, le2019batch}. Model-based approaches employ similar ideas but instead try to exploit the model information to learn a less conservative value function.  For instance, COMBO~\citep{COMBO} approximates the true model dynamics and utilizes both simulated and dataset samples to learn a conservative value estimation by penalizing out-of-support state-action pairs obtained by running the simulated model while MOReL \citep{morel} learns a pessimistic model that approximately lower bounds the true performance and performs policy optimization using the learned model. On the other hand, our proposal uses a nominal model to improve the actions of the state-action pairs present in the dataset; finally, in contrast to the aforementioned works, our work is more computationally attractive, as the resulting program for learning the policy is convex.

\paragraph{Imitation Learning:} The second step of our algorithm, where we employ Inverse Optimization to fit a policy on the improved state-action pairs, is analogous to IL. Similar to our dataset improvement scheme, several other IL algorithms employ augmentation strategies to further improve policy learning. For example, BAIL \citep{BAIL} first estimates the Monte Carlo returns of each state-action pair in the dataset, an infinite horizon and discounted extension of our objective function in \eqref{eq:cost}, and employs a neural network-based estimate to fit the returns. Based on this estimate, BAIL selects only the highest-valued state-action pairs and learns a policy via IL. On the other hand, our approach makes use of the entire dataset, improving actions through our robust MPC formulation, and utilizes a convex ``sub-optimality loss" to perform the IL step.

\paragraph{Terminal value function approximation:} Since MPC projects its internal model into the future, it can also act as an approximation to the Bellman equation. This observation is exploited by~\citep{zhongValueFunctionApproximation2013} to effectively increase the planning horizon by constructing approximate terminal Value Functions (VF) from MPC simulation data.

Using the same principle,~\citep{lowreyPlanOnlineLearn2018} showcased an algorithm that promotes exploration and, therefore, accelerates VF learning. Finally,~\citep{bhardwajBlendingMPCValue2020} propose a blended approach that combines elements from model-free and model-based methods to reduce model bias. Similarly, our work can be viewed as a specific instance of VF approximation, where learning the Q-function reduces the horizon to a single step. Additionally, in contrast to the papers mentioned above, our approach is computationally tractable.


\section{Inverse Optimization for RL}
\label{sec:inverse_opt}

In what follows, we briefly review the existing literature on IO and its potential to learn a control law. We then introduce the first contribution of this study: how in-hindsight information can be exploited to devise an offline RL algorithm.
% ?
\subsection{Inverse Optimization as Supervised Learning}
\label{sec:io_as_sl}
The goal of Inverse Optimization is to learn the behavior of an expert whose actions depend on an external signal. Specifically, for a given $s \in \mathcal S \subseteq \mathbb{R}^{n_s}$, the expert's decisions $u^{\textup{ex}} \in \mathcal U(s) \subseteq \mathbb{R}^{n_u}$ stem from a deterministic policy: $u^{{\textup{ex}}} = \pi^{\textup{ex}}(s)$. We wish to approximate $\pi^{\textup{ex}}(s)$ with a policy in a similar spirit as in Q-Learning that is defined as:
\[
    \pi_\theta (s) \coloneqq \argmin_{u \in \mathcal U(s)} \mathrm{Q}_\theta(s,u),
\]
where $\mathrm{Q}_\theta$, is a parameterized function belonging to the hypothesis class $\mathcal{Q}$. Throughout this work, we consider the quadratic hypothesis class
\begin{subequations}\label{eq:hyp_class_and_param_space}
\begin{equation}
   \label{eq:hyp_class}
   \mathcal Q = \{ \mathrm{Q}_\theta(s,u) = \langle u, \theta_{uu} u \rangle + 2\langle s, \theta_{su} u \rangle : \theta \in \Theta \},
\end{equation}
and the parameter space $\Theta \subseteq \mathbb R^{(n_s + n_u)\times(n_s + n_u)}$
\begin{equation}
   \label{eq:param_space}
   \Theta = \left\{ \theta =
      \bmqty{0 & \theta_{su}\\ \theta_{su}^\intercal & \theta_{uu} }
      :~ \theta_{uu} \succcurlyeq I_{n_u}
      \right\}.
\end{equation}
\end{subequations}
Note that as we are only interested in the minimizer of $\mathrm{Q}_\theta$ in the second variable $u$, the first element of the matrix $\theta$ in~\eqref{eq:param_space} can be set to zero without any loss of generality. Moreover, restricting the policy function $\pi_\theta$ to (strongly) convex optimization over $u$ and normalizing the parameters yields the constraint $\theta_{uu} \succcurlyeq I_{n_u}$. To learn the optimal $\theta^\star$, we use the ``sub-optimality loss", which was first introduced in \citep{mohajerinesfahaniDatadrivenInverseOptimization2018}:
\begin{equation}
   \label{eq:sub_loss}
   \ell_\theta^{\textup{sub}}(s,u^{\textup{ex}}) = \mathrm{Q}_\theta (s, u^{\textup{ex}}) - \min_{u \in \mathcal U(s)} \mathrm{Q}_\theta(s,u).
\end{equation}
Notice that the mapping $\theta \mapsto \mathrm{Q}_\theta(s,u)$ is linear, and thus, the ``sub-optimality loss" \eqref{eq:sub_loss} is convex in $\theta$  for convex $\mathcal U(s)$. Given a dataset $\{(\hat s_t, \hat u^{\textup{ex}_t)}\}_{t=1}^T$ of states $\hat s_t$ and expert actions $\hat u^{\textup{ex}}_t = \pi^{\textup{ex}}(\hat s_t)$, and a polytopic constraint set $\mathcal U(s) = \{ u: ~G(s) u \leq h(s) \}$, we have that {\citep{akhtarLearningControlInverse2021}}:% we will denote $\hat G_t = G(\hat s_t)$ and $ \hat h_t = h(\hat s_t)$. Then, the convex learning problem of $\theta^\star$ can be reformulated as (Corollary 3, \citealp{akhtarLearningControlInverse2021}):
\begin{align}
   \label{eq:inv_opt_problem_reform}
   & \min_{\theta \in \Theta} \sum_{t=1}^T \ell^{\textup{sub}}_\theta (\hat s_t, \hat u^{\textup{ex}}_t) =
   % \notag
   %  & \qquad
   \begin{array}[t]{cl}
      \min\limits_{\theta,\boldsymbol \gamma_{1:T},\boldsymbol \lambda_{1:T}} & \sum\limits_{t=1}^{T} \mathrm{Q}_\theta (\hat s_t, \hat u^{\textup{ex}}_t)
         + \frac{1}{4}\gamma_{t} + \langle \hat h_{t}, \lambda_{t} \rangle
         \vspace{2mm}\\
      \mathrm{s.t.} & \theta \in \Theta, \quad \lambda_t \geq 0, \hfill t \leq T,
      \vspace{1.5mm}\\
      & \begin{bmatrix}
            \theta_{uu}
               & \hat G_t^\intercal \lambda_t + 2\theta_{su}^\intercal \hat s_t\\
            \star & \gamma_t
         \end{bmatrix} \succcurlyeq 0, \quad \hfill t \leq T,
   \end{array}
\end{align}
where we use the shorthand $\hat G_t = G(\hat s_t)$ and $ \hat h_t = h(\hat s_t)$.

The convex optimization~\eqref{eq:inv_opt_problem_reform} offers an efficient way to learn the policy $\pi^{\textup{ex}}(\cdot)$. It is important to highlight that a key part upon which this program is built is the sequence of the ``ground-truth'' expert actions ${\hat{\mathbf{u}}^{\textup{ex}}_{1:T}}$. While the actions contained within an offline RL dataset can be regarded as expert actions, we propose to improve them by leveraging the hindsight information of a controlled dynamical system.

\subsection{Imitating an MPC expert with Inverse Optimization}
\label{sec:mpc_expert}

Consider a nominal dynamics model $f_0(x,u)$, possibly different from \eqref{eq:dynamics_model}, and without any external disturbances. Denoting the state and input constraints as $\mathcal X$ and $\mathcal U$, respectively, we formulate the deterministic MPC-$N$ problem as follows:
\begin{mini}
    {
        \substack{\mathbf u \in \mathcal U^{\textup{mpc}_N(x)}}
    }
    {
        \mathrm{Cost}^{f_0}_N (x, \mathbf u)
    }
    {
        \label{eq:mpc_formulation}
    }
    {
        V_{N}^{\textup{mpc}} (x) \coloneqq
    }
\end{mini}
where
$
\mathcal U^\text{mpc}_N(x) \coloneqq \big\{
       \mathbf{u} \in \mathbb R^{Nn_u}:~ u_k \in \mathcal{U}, \, x_{k+1}^\text{nom} = f_0(x_k^{\textup{nom}},u_k) \in \mathcal{X}, k \leq N, \, x_0^{\textup{nom}} = x
    \big\}
$
and $\mathrm{Cost}^{f_0}_N$ is defined as per~\eqref{eq:cost}. Thanks to the principle of optimality, we can express the Q-function of~\eqref{eq:mpc_formulation} as $\mathrm{Q}^{\textup{mpc}}(x,u)= c(x,u) + V_{N-1}^{\textup{mpc}}(f_0(x,u))$, which is defined over the 1-step constraint set
\[
   \mathcal U_1^\text{mpc}(x) \coloneqq
   \left\{u \in \mathbb R^{n_u}:~
      u \in \mathcal U,~f_0(x,u) \in \mathcal X
   \right\}.
\]
To approximate $\mathrm{Q}^{\textup{mpc}}$ with Inverse Optimization, we solve \eqref{eq:inv_opt_problem_reform} with $\hat s_t = \hat x_t$, $\hat u^{\textup{ex}}_t = \pi^{\textup{mpc}}(\hat x_t)$, and
\begin{equation}
\label{eq:mpc_policy}
   \pi^{\textup{mpc}}(x) = \argmin_{u \in \mathcal U^{\textup{mpc}(x)}} \mathrm{Q}^{\textup{mpc}}(x,u).
\end{equation}

\begin{remark}[MPC computational costs]
   For the MPC problem~\eqref{eq:mpc_formulation} to be tractable, a common assumption is that $f_0$ is linear in $x$ and $u$ and the sets $\mathcal X$ and $\mathcal U$ are polytopic. In such a setting, the Q-function $\mathrm{Q}^{\textup{mpc}}$ is piecewise quadratic where the number of pieces may be exponential in the horizon length $N$. Therefore, approximating $\mathrm{Q}^{\textup{mpc}}$ using the quadratic hypothesis class~\eqref{eq:hyp_class} may likely not be exact. Nonetheless, as reported in~\citep{akhtarLearningControlInverse2021}, such an approximation can work quite well. If there are no constraints, then~\eqref{eq:mpc_formulation} becomes a finite-horizon LQR problem, whose Q-function is known to be quadratic and positive definite, and as such, we can have an exact approximation within the hypothesis class~\eqref{eq:hyp_class}. In this case, the approximate policy becomes $\pi_\theta(s) = - \theta_{uu}^{-1}\theta_{su}^\intercal s$, which implies that we essentially learn an optimal linear control policy.
\end{remark}

\subsection{Exploiting in-hindsight information}
\label{sec:nc_mpc_expert}

This section contains the first contribution of this study, aiming to bridge the gap between IO and offline RL settings. To this end, we consider an extended nominal model with additive disturbances $w \in \mathbb R^{n_w}$, i.e., $\tilde f_0(x, u, w) = f_0(x, u) + E w$ where $E^\dagger E = I$. Denoting the $N$-length disturbance trajectory by~$\mathbf w$, we define the non-causal MPC-$N$ problem via
\begin{mini}
   {
      \substack{\mathbf u\in \mathcal U^{\textup{nc-mpc}}_N(x, \mathbf w)}
   }
   {
      \mathrm{Cost}_N^{\tilde f_0} (x, \mathbf u, \mathbf w)
   }
   {
      \label{eq:nc_mpc_formulation}
   }
   {
      V_{N}^{\textup{nc-mpc}} (x, \mathbf w) \coloneqq
   }
\end{mini}
with
$
    \mathcal U^{\textup{nc-mpc}}_N(x, \mathbf w) \coloneqq \{
        \mathbf u \in \mathbb R^{Nn_u}:~
        u_k \in \mathcal U,\, \tilde x_{k+1}^{\textup{nom}} = \tilde f_0(\tilde x_k^{\textup{nom},u_k,w_{k+1}}) \in \mathcal X,\, k \leq N,\, \tilde x_0^{\textup{nom}} = x
    \}
$, and
$
    \mathrm{Cost}_N^{\tilde f_0} (x, \mathbf u, \mathbf w)
    % \left \{
    %     \sum_{k=0}^{N-1} c(x_{k}, u_{k}) + c_f(x_{N}) :~ x_{k+1} = f_0(x_k, u_k) + E w_{k+1}
    % \right \}
$ defined as per \eqref{eq:cost}. Then, akin to Section \ref{sec:mpc_expert}, we can define $\mathrm{Q}^{\textup{nc-mpc}}(x, u, \mathbf{w})$ and $\mathcal U_1^{\textup{nc-mpc}}(x,\mathbf w)$ accordingly, and therefore we obtain the non-causal MPC expert policy
\begin{equation}
\label{eq:nc_mpc_policy}
   \pi^{\textup{nc-mpc}}(x,\mathbf w) =
      \argmin_{u \in \mathcal U_1^{\textup{nc-mpc}}(x,\mathbf w)} \mathrm{Q}^{\textup{nc-mpc}}(x, u, \mathbf w)
\end{equation}

Notice that when $w$ represents the mismatch between the true dynamics $f$ and the nominal model $f_0$ (i.e.,  $E w = f(x, u, v) - f_0(x, u)$),~\eqref{eq:nc_mpc_formulation} is equivalent to minimizing~\eqref{eq:cost}, thus sharing the same optimizer. Nonetheless, as $f$ may be unknown or highly nonlinear and depends on unknown external disturbances $v$, directly minimizing~\eqref{eq:cost} can be incredibly challenging, if not outright impossible.

As we cannot use $f$, we propose utilizing measured state transition realizations. Given the extended nominal model $\tilde f_0$ and a measured state transition  $(\hat x, \hat u, \hat x_+)$, we can infer \textbf{in hindsight} that $E \hat w = \hat x_+ - f_0(\hat x, \hat u)$. Therefore, we can construct a disturbance trajectory that can be fed into~\eqref{eq:nc_mpc_policy} to compute the best action in hindsight. However, as this policy is non-causal, it cannot be implemented in real-time, but only in hindsight, as $\mathbf w_{t+1:t+N}$ will not be available at time $t$. 

When the in-hindsight disturbance trajectory is not completely stochastic, an estimate for $\mathbf w_{t+1:t+N}$ could potentially be inferred from an appropriate choice of features $\phi(\mathbf x_{1:t}, \mathbf u_{1:t})$. For instance, if the disturbance dynamics are linear, a potential feature map could be the past $H$ disturbance realizations $\phi(\mathbf x_{1:t}, \mathbf u_{1:t}) = \mathbf{w}_{t-H+1:t}$, where $H$ is chosen appropriately large. Hence, given a feature map $\phi$, we can use an augmented state $s_t = \phi(\mathbf x_{1:t}, \mathbf u_{1:t})$ to approximate the non-causal policy~\eqref{eq:nc_mpc_policy}  in a causal manner with Inverse Optimization, and by doing so, we implicitly infer the predictive relationship between $s_t$ and $\mathbf w_{t+1:t+N}$ and also the (in-hindsight) optimal way to counteract $\mathbf w_{t+1:t+N}$. The procedure used to approximate the non-causal MPC expert with IO is outlined in Algorithm \eqref{alg:nc_mpc_inv_opt}.

\begin{remark}[Validity of in-hindsight trajectories]
In general, the in-hindsight disturbance trajectory $\mathbf w$ may be dependent on $x$ and $u$. Hence, the minimizer of~\eqref{eq:nc_mpc_formulation} does not necessarily correspond to the minimizer of~\eqref{eq:cost}. Namely, if $\mathbf w$ depends on $x$ and $u$, applying the output of~\eqref{eq:nc_mpc_policy} steers the system toward a different state-action pair, and as such, the realized disturbance/mismatch would diverge from the provided one. However, when $Ew = f(x,u,v) - f_0(x,u) = g(v)$ for some function $g$, i.e., when the disturbances are exogenous and separable from the system, then the in-hindsight trajectory shall remain valid. In such settings, the problems~\eqref{eq:cost} and~\eqref{eq:nc_mpc_formulation} are then equivalent.
\end{remark}

\begin{algorithm}[tb]
   \caption{Using in-hindsight information for IO}
   \label{alg:nc_mpc_inv_opt}
   \begin{algorithmic}
      \STATE {\bfseries Input:} Trajectory $\{(\hat x_t, \hat u_t)\}_{t=1}^T$, non-causal expert $\pi^{\textup{nc}_N(x,\mathbf{w}})$ with horizon $N$, nom.\ model $\tilde f_0(x,u,w)$, feature map $\phi(\cdot, \cdot)$
      \FOR{$t=1$ {\bfseries to} $t=T-1$}
      \STATE Let $\hat s_t = \phi(\hat {\mathbf x}_{1:t}, \hat {\mathbf u}_{1:t})$
      \STATE Let $\hat w_{t+1} = E^\dagger (\hat x_{t+1} - f_0(\hat x_{t}, \hat u_{t} ))$
      \STATE Let $\tau = t - N + 1$
      \IF{$\tau \geq 1$}
         \STATE Let $\hat u^{\textup{ex}}_\tau = \pi^{\textup{nc}}_N(\hat x_\tau, \hat{\mathbf{w}}_{\tau+1:\tau+N})$
      \ENDIF
      \ENDFOR
      \STATE Solve \eqref{eq:inv_opt_problem_reform} with $\{\hat s_\tau, \hat u^{\textup{ex}_\tau)\}_{\tau=1}^{T-N+1}}$ to obtain $\theta^*$.
      \STATE {\bfseries Return:} $\theta^*$
   \end{algorithmic}
\end{algorithm}

\begin{remark}[Handling of non-causal constraints]
   As the future disturbances $\mathbf w_{t+1:t+N}$ are not known to the resulting policy during runtime, the satisfaction of state constraints cannot be guaranteed. Therefore, a causal version of the constraints that do not take into account $\mathbf w_{t+1:t+N}$ is required. %such as $\mathcal U^{\textup{mpc}}_1(x)$, potentially with softening to avoid infeasibility issues.
   However, as the in-hindsight expert takes into account $\mathbf w_{t+1:t+N}$, the learned $\mathrm Q_\theta$ will have an embedded cost on $s_t$, aiding in constraint satisfaction.

\end{remark}
\begin{remark}[Literature on disturbance feedback and non-causal control]
   The idea of ``disturbance feedback control'' has also been explored in recent works related to online control for adversarial disturbances \citep{hazanNonstochasticControlProblem2020,agarwalOnlineControlAdversarial2019,fosterLogarithmicRegretAdversarial2020}. Additionally, a similar problem is also considered \citep{goelRegretoptimalMeasurementfeedbackControl2021} where a non-causal controller is approximated by a causal one in an offline setting. Contrary to these works, which consider a linear policy class akin to \eqref{eq:min_regret_policy} with no constraints on state or input, our proposed policy is nonlinear in nature and can handle constraints.

\end{remark}



\section{Robust Disturbance-Aware MPC}
\label{sec:rmpc}

\subsection{Robustification around disturbance trajectory}

The non-causal MPC expert \eqref{eq:nc_mpc_policy} optimizes directly against the noisy disturbance trajectory. However, due to stochasticity and/or potential distribution shifts in the data, performance might be degraded, and we may even observe instabilities. Therefore, we opt for a policy that is robust to such issues.
To this end, let us introduce the robust counterpart to the non-causal MPC~\eqref{eq:nc_mpc_policy} described as
\begin{equation}
   \label{eq:nc_rmpc_generic_formulation}
   V_N^{\textup{nc-rmpc}}(x, \mathbf w) \coloneqq
   \min_{\mathbf u \in \mathcal U_N^{\textup{nc-rmpc}}(x, \mathbf w)}
   \max_{\bar{\mathbf{w}} \in \mathcal W}
   \mathrm{Cost}_N^{\tilde f_0} (x, \mathbf u, \bar{\mathbf{w}})
\end{equation}
where $\mathcal W \subseteq \mathbb R^{Nn_w}$ is the disturbance uncertainty set, and
\begin{equation}
    \label{eq:constraints_over_uncertainty_set}
    % \begin{multlined}
    \mathcal U_N^{\textup{nc-rmpc}}(x,\mathbf w) = %\\
    \left\{
       \mathbf u \in \mathbb R^{Nn_u}:~ \mathbf u \in \mathcal  U_N^{\textup{nc-mpc}}(x,\bar{\mathbf{w}}),\,  \forall\bar{\mathbf{w}}\in\mathcal W
    \right\}.
    % \end{multlined}
\end{equation}
A problem like~\eqref{eq:nc_rmpc_generic_formulation} can easily be computationally intractable, even if its non-robust version~\eqref{eq:nc_mpc_formulation} is not. When dealing with such problems, it is therefore common for conservative approximations to be used even when the nominal model $f_0$ is linear. Here, we propose an uncertainty set $\mathcal W$ for which~\eqref{eq:nc_rmpc_generic_formulation} is tractable under linear dynamics and constraints and quadratic costs. Before we proceed, let us introduce a useful preparatory Lemma.

\begin{lemma}[Vectorized MPC formulation for linear dynamics]
\label{lem:nc_mpc_linear_form}
    Under linear nominal dynamics $f_0(x,u) = Ax +Bu$, and quadratic costs  $c(x,u) = \norm{x}^2_{Q_x} + \norm{u}^2_{Q_u}$ and $c_f(x,u) = \norm{x}^2_{Q_f}$, where $Q_x, Q_f\succcurlyeq 0$ and $Q_u \succ 0$, the objective of \eqref{eq:nc_mpc_formulation} can be equivalently expressed by
    $$
        \mathrm{Cost}_N^{\tilde f_0}(x, \mathbf u, \mathbf w) =
            \left\Vert
                \mathbf{A}x+\mathbf{B} \mathbf{u} + \mathbf{E} \mathbf{w}
            \right\Vert _{\mathbf{Q}_{\mathbf{x}}}^{2}
           + \left\Vert \mathbf{u}\right\Vert _{\mathbf{Q_{u}}}^{2},
    $$
    with $\mathbf{Q_{x}} = \blkdiag (I_{N-1}\otimes Q_{x}, Q_{f})$, $\mathbf{Q}_{\mathbf{u}}=I_{N}\otimes Q_{u}$, $\mathbf A = \mathrm{blkcol}(A,\ldots,A^N)$, $\mathbf B = \mathcal T_N(A,B)$, $\mathbf E = \mathcal T_N(A,E)$\footnote{Denotes a matrix $\mathcal{T}_N(A,B)=\bmqty{B & \cdots & 0\\ \vdots & \ddots & \vdots\\ A^{N-1}B & \cdots & B}.$}. Moreover, when the constraints are polytopic~$\mathcal U = \left\{ u \in \mathbb R^{n_u}: G_u u \leq h_u \right\}$ and  $\mathcal X = \left\{ x \in \mathbb R^{n_x}: G_x x \leq h_x \right\}$, the constraint set of~\eqref{eq:nc_mpc_formulation} is also polytopic in the form of
    $$
    \mathcal U_N^{\textup{nc-mpc}}(x,\mathbf w) =
    \left\{
       \mathbf u \in \mathbb R^{Nn_u}:~ \mathbf{F}x+\mathbf{Gu}\leq\mathbf{h}({\mathbf{w}})
    \right\},   
    $$
    with $\mathbf F^\intercal = \bmqty{(\mathbf{G_x A})^\intercal & \mathbf{0}}$, $\mathbf G^\intercal = \bmqty{(\mathbf{G_x B})^\intercal & \mathbf{G_u}^\intercal}$, $\mathbf h (\mathbf{w}) = \bmqty{(\mathbf{h_x} - \mathbf{G_x E w})^\intercal & \mathbf{h_u}^\intercal}$, $\mathbf{G_{x}}=I_{N}\otimes G_{x}$, $\mathbf{G_{u}}=I_{N}\otimes G_{u}$, $\mathbf{h_{x}}=\boldsymbol{1}_{N}\otimes h_{x}$, $\mathbf{h_{u}}=\boldsymbol{1}_{N}\otimes h_{u}$. 
\end{lemma}
Thanks to Lemma~\ref{lem:nc_mpc_linear_form}, the MPC problem~\eqref{eq:nc_mpc_formulation} can be simplified to the convex quadratic program
\begin{mini}
   {\substack{\mathbf u}}
   {
       \left\Vert \mathbf{A}x+\mathbf{B} \mathbf{u} + \mathbf{E} \mathbf{w} \right\Vert _{\mathbf{Q}_{\mathbf{x}}}^{2}
           + \left\Vert \mathbf{u}\right\Vert _{\mathbf{Q_{u}}}^{2}
   }
   {\label{eq:nc_mpc_vectorized_formulation}}{}
   \addConstraint{	\mathbf{F}x + \mathbf{G} \mathbf{u}}{\leq\mathbf{h}(\mathbf{w})}
\end{mini}
The uncertainty set $\mathcal W$ we consider here is a ball centered on the $N$-length disturbance trajectory $\mathbf w$
\begin{equation}
\label{eq:dist_uncertainty_set}
    \mathcal W \coloneqq
    % \mathfrak{B}_{\varrho,P}({\mathbf{w}}) =
    \left\{
        \bar{\mathbf{w}} \in \mathbb R^{Nn_w}:~\left\Vert
            \bar{\mathbf{w}}-\mathbf{w}
        \right\Vert _{P}^{2} \leq \varrho^{2}
    \right\},
\end{equation}
where $P \succ 0$ is a desired geometry on the uncertainty trajectories. With this choice of uncertainty set, the robust constraints $\mathcal U_N^\mathrm{nc-rmpc}(x, \mathbf{w})$, as defined in~\eqref{eq:constraints_over_uncertainty_set}, enjoy an exact polytopic representation.


\begin{lemma}[Exact polytopic representation of robust constraint set]
\label{lem:robust_con_satisf}
   Under the hypotheses of Lemma~\ref{lem:nc_mpc_linear_form} with uncertainty set~\eqref{eq:dist_uncertainty_set} and $P\succ 0$, the constraints~\eqref{eq:constraints_over_uncertainty_set} have the following polytopic representation
   \[
      \mathbf{F}x+\mathbf{Gu}\leq\underline{\mathbf{h}}(\mathbf w)
   \]
   where $\underline{\mathbf{h}}(\mathbf w)^\intercal = \bmqty{(\mathbf{h_x}-\overline g(\mathbf w))^\intercal & \mathbf{h_u}^\intercal}$, $\overline{g}(\mathbf w)^{\intercal}=\begin{bmatrix}\overline{g}_{1}(\mathbf w) & \overline{g}_{2}(\mathbf w) & \ldots\end{bmatrix}$, and $\overline{g}_{i}(\mathbf w)=\varrho\left\Vert P^{-1/2}g_{i}\right\Vert +g_{i}^{\intercal}{\mathbf{w}}$, $\forall i$. The vectors $g_i$ are such that $\left[\mathbf{G_{x}E}\bar{\mathbf{w}}\right]_{i}=g_{i}^{\intercal}\bar{\mathbf{w}}$.
\end{lemma}
The proof is provided in Appendix \ref{appdx:proofs}. We are now in a position to state our main result.
\begin{theorem}[Exact SDP reformulation] \label{thm:robust_mpc_reform} 
    Under the hypotheses of Lemmas~\ref{lem:nc_mpc_linear_form}~and~\ref{lem:robust_con_satisf}, the robust non-causal MPC problem~\eqref{eq:nc_rmpc_generic_formulation} is expressed as the min-max problem
   \begin{mini}
        {
            \substack{\mathbf u}
        }{
            \max_{\bar{\mathbf{w}} \in \mathcal W}\,
            \left\Vert
            \mathbf{A}x+\mathbf{Bu} +\mathbf{E}\bar{\mathbf{w}}
            \right\Vert _{\mathbf{Q}_{\mathbf{x}}}^{2}
        }{
            \label{eq:nc_rmpc_vectorized_formulation}
        }{
            V_N^\textup{nc-rmpc}(x, \mathbf w) = 
        }
        \addConstraint{
                \mathbf{F}x + \mathbf{G} \mathbf{u}
        }{
            \leq \underline{\mathbf{h}} (\mathbf{w})
        }
    \end{mini}
   Furthermore, let us denote $\mathbf X(x,\mathbf u) = \mathbf A x + \mathbf{Bu}$. Then, the optimization problem~\eqref{eq:nc_rmpc_vectorized_formulation} admits the  convex reformulation
   \begin{equation*}
    \begin{array}{>{\displaystyle}c>{\displaystyle}l}
        \min\limits_{\mathbf u, \lambda, \gamma_1, \gamma_2} &
            \gamma_1 + \gamma_2 \vspace{2mm}\\
        \mathrm{s.t.} &
            \lambda \geq 0, \quad \mathbf{F}x+\mathbf{Gu} \leq\underline{\mathbf{h}}(\mathbf w),
            \vspace{2mm} \\
        & \begin{bmatrix}
            \mathbf{E}^{\intercal}\mathbf{Q_{x}}\mathbf{E}-\lambda P &
            \mathbf{E}^{\intercal}\mathbf{Q_{x}}\mathbf{X}(x,\mathbf{u})+\lambda P{\mathbf{w}} \\
            \star & -\gamma_1-\lambda\left(\left\Vert {\mathbf{w}}\right\Vert _{P}^{2}-\varrho^{2}\right)
        \end{bmatrix} \preccurlyeq 0, \vspace{2mm} \\
        & \begin{bmatrix}
            -I_{N} & \left(\mathbf{B}^{\intercal} \mathbf{Q_x} \mathbf{B}+\mathbf{Q_{u}}\right)^{1/2}\mathbf{u} \\
            \star & 2\left\langle \mathbf{B}^{\intercal} \mathbf{\mathbf{Q_{x}}}\mathbf{A}x,\mathbf{u} \right\rangle + \left\Vert \mathbf{A}x \right\Vert _{\mathbf{Q_{x}}}^{2}-\gamma_{2}
        \end{bmatrix} \preccurlyeq 0.
    \end{array}
    \end{equation*}
\end{theorem}
The proof is relegated to Appendix \ref{appdx:proofs}.

\begin{remark}[Uncertainty set]
   The uncertainty set \eqref{eq:dist_uncertainty_set} is not necessarily uniform in time as it is a ball on  $Nn_w$-dimensional space, i.e, not all $\bar w_k$ components of $\bar{\mathbf{w}}$ need to be distanced equally from $ w_k$. For instance, considering the case when $P = I_{Nn_w}$, we then have
   \[
      \textstyle
      \left\{
         \bar{\mathbf w} \in \mathbb R^{Nn_w}: \sum_{k=1}^N \norm{ w_k - \bar w_k}^2 \leq \varrho^2
      \right\}.
   \]
   The above uncertainty set includes disturbances with similar measures of energy to ${\mathbf w}$. Other similar approaches~\citep{lofbergMinimaxApproachesRobust2003a} aim to mitigate this by considering uncertainty sets such as $ \{ \max_k \norm{w_k - \bar w_k}^2 \leq \varrho^2 \} $,
   where each realization is bounded uniformly in time. However, since multiple quadratic inequalities are introduced as constraints, this necessitates the use of the inexact S-Lemma~\citep{boydLinearMatrixInequalities1994}, which inserts conservativeness.
   We use its exact version since only one quadratic inequality is involved in the constraints, thus allowing for an exact reformulation.
\end{remark}

\subsection{Approximating with Inverse Optimization}

The non-causal policy \eqref{eq:nc_rmpc_vectorized_formulation} can be expressed in the form
\begin{equation}
   \label{eq:nc_rmpc_policy}
   \pi^{\textup{nc-rmpc}}(x,\mathbf w) = \argmin_{u \in \mathcal U^{\textup{nc-rmpc}}(x,\mathbf w)} \mathrm{Q}^{\textup{nc-rmpc}}(x,u,\mathbf w)
\end{equation}
with $\mathcal U^{\textup{nc-rmpc}}(x,\mathbf w)$ and $\mathrm{Q}^{\textup{nc-rmpc}}(x,u,\mathbf w)$ are defined accordingly. The procedure to approximate \eqref{eq:nc_rmpc_policy} with Inverse Optimization is identical to that used for the non-robust disturbance-aware MPC of Section \ref{sec:nc_mpc_expert} and is outlined by Algorithm \ref{alg:nc_mpc_inv_opt}. The only difference lies in the expert policy used; in this context, policy \eqref{eq:nc_rmpc_policy} is used instead of \eqref{eq:nc_mpc_policy}. One key difference with \eqref{eq:nc_mpc_policy} is that \eqref{eq:nc_rmpc_policy} requires solving a semidefinite program --instead of a quadratic one-- so we can expect greater computational improvement, albeit potentially at the expense of reducing the quality of the approximation.

By combining~\eqref{eq:inv_opt_problem_reform}, and~\eqref{eq:nc_mpc_policy}, we arrive at the convex optimization program whose solution is the fitted Q-function
%\todo{redo}
% \sout{The final form of the training phase of this work, which is determined by suboptimality loss in ~\eqref{eq:sub_loss}, gives rise to the optimization problem}
   \begin{align}
      \label{eq:orl_io}
      \left\{
      \begin{array}{cl}
      %\begin{aligned}
         \min\limits_\theta & \sum\limits_{t=1}^T \mathrm{Q}_\theta (\hat s_t, \hat u_t^{\textup{ex}}) - \min\limits_{u \in \mathcal{U}(s_t)} \mathrm{Q}_\theta(\hat s_t, u) \vspace{1mm}\\
         \text{s.t.} & \hat u_t^{\textup{ex}} = \pi^{\textup{nc-rmpc}}(\hat x_t,\hat{\mathbf{w}}_t),
      %\end{aligned}
      \end{array}
      \right.
   \end{align}
   where the labels $\hat u_t^{\textup{ex}}$ are the in-hindsight optimal inputs computed by the min-max problem~\eqref{eq:nc_rmpc_generic_formulation}. An interesting parallel can be drawn between the exploration-exploitation dilemma and the robustification when computing the labels $\hat u_t^{\textup{ex}}$.
   % is not necessarily on par with exploration in the traditional sense. However, they have an interesting feature in common as follows.
\begin{remark}[Exploration vs exploitation]
   When looking at the exploration/exploitation dilemma as a competitive game between two conflicting objectives, we note that a similar trade-off exists in the min-max MPC~\eqref{eq:nc_rmpc_generic_formulation} that is controlled by the uncertainty radius $\varrho$. This trade-off allows us to take into account disturbance trajectories different than the ones observed, a key feature that is addressed by exploration in RL and hence helps with generalization. This hypothesis is also confirmed by our numerical results in Section~\ref{sec:num_experiments} and with additional experiments in the Appendix. 

\end{remark}

\section{Numerical Experiments}
\label{sec:num_experiments}

In our numerical analysis, we focus on two domains, the quadrotor environment from safe-control-gym~\citep{safe-control-gym} and MuJoCo control benchmark~\citep{MuJoCo}. Additionally, we include detailed ablation studies through two more experiments found in Appendix Section~\ref{appdx:num_exps}); the control of the linearized dynamics of a fighter jet~\citep{safonovFeedbackPropertiesMultivariable1981} and a nonlinear temperature control problem. The source code of the implementation and experiments is included at \citep{repository}.

\subsection{Quadrotor environment}
\label{sec:num_exp:quadrotor}
We experiment in one of the nonlinear quadrotor environments from safe-control-gym~\citep{safe-control-gym} and provide evaluations together with a comparison with two RL algorithms, namely Proximal Policy Optimization (PPO)~\citep{PPO} and Conservative Q-Learning (CQL)~\citep{CQL}. Both are model-free, with CQL falling under the offline RL paradigm and PPO being an on-policy algorithm.

\textbf{Environment specifications:} The quadrotor environment consists of a 6-dimensional state space and two control inputs. The objective is to reach a fixed goal state starting from a randomly sampled starting position while keeping the quadrotor stable under an unknown external force that acts as a disturbance. This force consists of a sinusoidal signal of a random phase with additive Gaussian noise applied to the body of the quadrotor. The environment has a nonlinear dynamical system, assumed to be known, with the minimum and maximum episodic cost being 0 and 300, respectively.

\textbf{Experimental Setup:} We linearize the dynamics around an equilibrium point to form a nominal model before giving it to the MPC policies. In the following experiments, we denote an MPC policy that is oblivious to the external sinusoidal disturbance by MPC (obl), and with MPC (f-dst), we refer to an MPC that has the full information of the future disturbance trajectory. In our evaluations, we trained the PPO agent with 3M environment steps, which we refer to as PPO-3M, and the CQL agent for 50k iterations, where we observed convergence in performance. Both IO and CQL agents are trained with the same dataset generated by an MPC (obl) policy with a 25-step horizon.

Figures~\ref{fig:quadrotor_comparison} and~\ref{fig:quadrotor_ablation} show our comparisons and ablation studies in the quadrotor environment. In all six figures, $T$ denotes the dataset length, $N$ is the MPC horizon, and $H$ is the lookback horizon. IO-RMPC* is the $\rho$ tuned policy. Unless stated otherwise, the default values of $N$ and $H$ are set to 25 and 2, respectively, and each evaluation of an agent is performed with 20 different starting points. To normalize the effect of the randomized initial starting points, in both figures, we only report the steady-state\footnote{Defined as the last 40\% of data points of a trajectory.} costs. The dashed lines indicate the median values, and the tubes contain the range between the 20th to 80th percentiles of the costs if not stated otherwise.

\textbf{Comparisons:}
In the left plot of Fig.~\ref{fig:quadrotor_comparison}, we compare the episodic cost histograms of four agents evaluated with 20 different initial conditions. Our evaluations show that IO-RMPC yields significantly lower costs even with a limited dataset of $T=3,000$ samples. The center plot of Fig.~\ref{fig:quadrotor_comparison} shows a comparison of the IO-RMPC policy against various CQL agents. Although CQL converges to IO-RMPC performance, it requires an order of magnitude more samples. Finally, we compare the MPC performances with the PPO agent. Although MPC policies are only given a linear nominal model of the environment, starting with the 15-step horizon, they surpass the PPO performance.

\textbf{Ablation studies:}
Additionally, we analyze the effect of uncertainty radius $\rho$ and lookback-horizon $H$. The center plot of Fig.~\ref{fig:quadrotor_ablation} indicates that robustification of the IO-MPC policy, up to some value of $\rho$, improves the performance even in the absence of a disturbance bias. This behavior is also present in the left plot of Fig.~\ref{fig:quadrotor_ablation}, where the IO-RMPC policy even surpasses the MPC~(f-dst) policy. We posit that this is due to the fact that robustifying also helps with model mismatch between the actual dynamics and the nominal one. Finally, we make an ablation on the lookback horizon of the IO-MPC policy shown in the right plot of Fig.~\ref{fig:quadrotor_ablation}. We observe that when the parameter $H$ is set to 2, IO-MPC almost recovers the performance of full information MPC~(f-dst) policy, whereas a further increase in $H$ degrades performance.

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.33\textwidth]{hist_1.pdf}%
    % \hfill
    \includegraphics[width=0.33\textwidth]{dataset_ablation.pdf}%
    % \hfill
    \includegraphics[width=0.33\textwidth]
    {mpc_horizon_ablation.pdf}%
    \caption{Comparisons of several agents in the quadrotor environment. \textbf{Left:} The cost histogram of the offline IO and CQL agents and online model-based MPC and model-free PPO-3M(trained with 3M environment steps) agents. \textbf{Center:} The cost distributions of CQL agents trained with 4 seeds on various dataset lengths compared to a single IO-RMPC policy trained with 3000 samples. \textbf{Right:} Comparison of the cost distributions between oblivious and full disturbance MPC policies against the model-free PPO agent.}
    \label{fig:quadrotor_comparison}
    \vspace{\floatsep}
    \includegraphics[width=0.33\textwidth]{hist_2.pdf}%
    % \hfill
    \includegraphics[width=0.33\textwidth]{rho_ablation.pdf}%
    % \hfill
    \includegraphics[width=0.33\textwidth]{io_lookback_ablation.pdf}%
    \caption{Ablation studies of MPC and IO policies in the quadrotor environment. \textbf{Left:} The cost histogram of IO and MPC agents with a 25-step horizon. \textbf{Center:} The cost distribution of IO-RMPC policy with different $\rho$ values and IO-MPC policy with the same horizon $N$. The tube contains the range from the 40th to 60th percentile of the costs. \textbf{Right:} The steady-state cost distributions of the IO-MPC policy with various look-back horizons ($H$) against MPC policies. The tube contains a narrower range from the 45th to 55th percentile of the costs.}
    \label{fig:quadrotor_ablation}
\end{figure*}

\subsection{MuJoCo benchmark}

\begin{table}
\caption{Comparison of IO agent in MuJoCo benchmark.}
\centering
\label{table:mujoco_scores}
\vskip 0.15in
% \adjustbox{width=\textwidth}{%

\newcommand*\rot{\rotatebox{90}}
\begin{tabular}{@{} lc|cccc}
    \toprule
    \textbf{Environment} & \textbf{Data Size} & \textbf{CQL} & \textbf{IQL} & \textbf{COMBO} & \textbf{IO} \\ \midrule
    \multicolumn{6}{c}{Score}   \\ \midrule
    \texttt{walker2d-medium} & 10K & $50.5\pm27.3 $ & $61.4\pm22.6$            & $58.6\pm19.6$                & $\bm{70.6\pm4.2}$ \\
                    & 1M  & $\bm{85.4\pm1.2} $    & $83.3\pm7.6$  & $82.9\pm4.7$   & -- \\
    \texttt{hopper-medium}   & 5K  & $54.3\pm15.4$           & $66.6\pm19.1$        & $57.5\pm11.0$      & $\bm{82.1\pm11.8}$     \\
                    & 1M  & $72.5\pm15.7$           & $77\pm18.2$        & $\bm{98.3\pm4.5}$      & --   \\  \midrule
    \multicolumn{6}{c}{Number of parameters}   \\ \midrule
    \texttt{walker2d-medium} &       & $691,216$      & $431,377$      & $2,489,949$                & $3,246$      \\
    \texttt{hopper-medium}   &       & $678,922$      & $418,315$      & $2,476,887$         & $840$ \\ 
    \bottomrule
\end{tabular}
% }
\vskip -0.1in
\end{table}

Next, we compare IO agents with several model-based and model-free offline RL algorithms regarded as state-of-the-art (SOTA) within the MuJoCo control benchmark~\citep{MuJoCo}. In these experiments, we employed a model-free version of the IO agent where the actions $\hat{u}_t^{\text{ex}}$ in Algorithm~\eqref{alg:nc_mpc_inv_opt} are directly taken from the dataset. The augmented state $\phi(\hat {\mathbf x}_{t-4:t}, \hat {\mathbf u}_{t-4:t})$
includes the last four state-action pairs, the cross-products of state features, a constant bias term, and the state sinusoidal terms. The latter augmentation is motivated by the periodic nature of the targeted tasks in robotics.

\textbf{Experiment setup:} We use the dataset from the D4RL repository~\citep{d4rl} to train IO agents and offline RL algorithms. We employed an iterative version of the IO algorithm, using gradient-based optimization for minimizing the objective function in Equation~\eqref{eq:sub_loss}. For a fair comparison, we ran each algorithm with 1M gradient steps and applied the same evaluation scheme. We trained each algorithm with three different seeds and evaluated the agents throughout the training process by using 40 different seeds for each evaluation. We report the average of top $5\%$ mean evaluation scores in Table~\ref{table:mujoco_scores}. We focus on low-data regimes, limiting the data size to 10K or 5K points in addition to retaining the original dataset size of 1M points. We obtained the scores for the offline RL algorithms by running the implementations provided in OfflineRL-Kit repository~\citep{offinerlkit}, which matches the originally reported scores when the algorithms are executed with the complete dataset.

Table~\ref{table:mujoco_scores} shows the performance of the IO agent in low-data regimes against Offline RL algorithms. 
In both hopper and walker environments, the IO agent achieves the highest score in the low-data regime when compared with model-free and model-based offline RL algorithms.
In terms of the number of parameters, the IO agent has an order of
magnitude fewer parameters, as shown in the bottom of Table~\ref{table:mujoco_scores}, while achieving higher scores compared to state-of-the-art (SOTA) agents.

We argue that the successful performance of the IO algorithm with such a low number of parameters is due to the inherent richness of the IO hypothesis class, combined with a convex optimization loss function that allows us to provably reach the (in-sample) global optimizer during the training phase. Furthermore, due to the inherent simplicity of the proposed policy class, the IO algorithm is able to generalize with significantly fewer samples.

In these experiments, we refrained from running our proposed IO-RMPC agent that employs the action improvement step since constructing a nominal model for MuJoCo tasks, which is required for the MPC experts, is a task that is inherently difficult and beyond the scope of this work. Nevertheless, our experiments with the plain IO agent reveal promising and competitive results in MuJoCo control tasks. These results underscore the substantial potential of IO-based algorithms within RL or IL contexts, especially in scenarios with limited data. Extending the RMPC-based action improvement step to deal with more complicated dynamics remains an avenue for future research.

% \textcolor{blue}{When we tried to include an action improvement step in the IO-derived policies, akin to what was done in Section~\ref{sec:num_exp:quadrotor}, the performance of the resulting agent was not competitive. This is due to the RMPC expert requiring a linear nominal model, and a single linearization was obviously not adequate for such complex dynamics as the ones found in the MuJoCo benchmarks. What is very interesting to note however, is that the IO policy class is rich enough to be competitive with SOTA RL methods that use much higher dimensional parameter spaces and of course lack any convexity properties. Therefore, there is great potential for IO-based algorithms to be used for RL or IL tasks, especially when specializing in small-data regimes. Extending the RMPC-based action improvement step to deal with more complicated dynamics remains an avenue for future research.}


\section{Concluding Remarks, Limitation, and Future Directions}
\label{sec:conclusions}

In this work, we presented a convex and robust Offline RL framework that utilizes a nominal model and in-hindsight information to learn an optimal policy. Through empirical evaluations, we showcased that our proposed algorithm can recover the performance of non-causal agents with complete environmental knowledge, while at the same time significantly outperforming RL algorithms in the low-sample data regimes (both online and offline). We further demonstrated that the IO framework, due to its expressivity and convexity properties, can achieve SOTA performance in challenging MuJoCo offline control tasks while employing orders of magnitude fewer parameters than its competitors.

We also find it essential to mention some of the inherent limitations of our approach. While the proposed quadratic hypothesis class, when paired with appropriate features, has demonstrated sufficient expressiveness in the control environments examined within our numerical studies, for more sophisticated tasks, additional steps can be required, such as applying kernel tricks or employing a nonlinear state embedding. Another drawback of our approach is the reliance of our robust MPC formulation on a nominal model. This requirement can become impractical for complex environments where approximating a nominal model is challenging. However, these limitations are not inherent and can be potential avenues for future research, including topics such as:
\begin{enumerate}[label=(\roman*)]
    \item approximating non-causal policies by utilizing in-hindsight information in real-time, using tools from Online Convex Optimization; and
    \item extending the robust min-max optimization (RMPC) framework to off-policy and offline RL settings.
\end{enumerate}
As we conclude, we position our approach as a step towards bridging the gap between robust control and offline RL, offering a particular applicability in continuous control tasks with substantial distribution shifts from training to test and also in environments where the availability of training data is limited.

% In this work, we provided a convex and robust offline RL framework that utilizes a nominal model to learn an optimal policy. We showed that the proposed framework is expressive enough to achieve state-of-the-art (SOTA) level performance in challenging offline MuJoCo control tasks while employing significantly fewer parameters with a unique convex loss function. Additionally, we demonstrated our proposed algorithm through a controlled study in a control task and made a comparison with online and offline RL algorithms.
% Under external disturbances, we showed that our IO-RMPC agent improves the baseline by a significant margin, particularly in the low-sample data regimes.
% This is a promising research avenue with potential future directions including:
% \begin{enumerate}[label=(\roman*)]
%     \item Approximating non-causal policies by utilizing in-hindsight information with convex online optimization in real-time; and
%     \item Extension of the robust min-max optimization framework to off-policy and offline RL settings;
% \end{enumerate}

% As we conclude, we find it essential to mention some of the inherent limitations of our approach. While the proposed quadratic hypothesis class, when paired with appropriate features, has demonstrated sufficient expressiveness in the control environments examined within our numerical studies, for more sophisticated tasks, additional steps can be required, such as applying kernel tricks or employing a nonlinear state embedding. Another drawback of our approach is the reliance of our robust MPC formulation on a nominal model. This requirement can become impractical for complex environments where approximating a nominal model is challenging. However, in such cases, we can omit the action improvement scheme and use the actions included in the original dataset, relying only on the IO framework. Despite the mentioned limitations, our approach is a step towards bridging the gap between robust control and offline RL, offering a particular applicability in continuous control tasks with substantial distribution shifts from training to test.

% ==========================================

% Acknowledgements and Disclosure of Funding should go at the end, before appendices and references
\acks{This work is partially supported by the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (TRUST-949796).
}

% \acks{All acknowledgements go at the end of the paper before appendices and references.
% Moreover, you are required to declare funding (financial activities supporting the
% submitted work) and competing interests (related financial activities outside the submitted work).
% More information about this disclosure can be found on the JMLR website.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.


% ======== Appendix =================

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

%\newpage
\appendix
\label{appdx}


\section{Technical Proofs}
\label{appdx:proofs}
\subsection{Proof of Lemma 3.2}
\label{appdx:proofs:robust_con_satisf}


The original constraint expresses a row-wise inequality. With the parameterization $\left[\mathbf{G_{x}E}\bar{\mathbf{w}}\right]_{i}=g_{i}^{\intercal}\bar{\mathbf{w}}$, the inequality
$
   \mathbf{F}x+\mathbf{Gu}\leq\mathbf{h}(\bar{\mathbf{w}}),\ 
   % \forall\bar{\mathbf{w}}\in\mathfrak{B}_{\varrho,P}({\mathbf{w}})
   \forall\bar{\mathbf{w}}\in \mathcal W
$
is equivalent to solving the following optimization program for every $i$:
\[
   \overline g_i(\mathbf w) = \max_{\bar{\mathbf{w}}} \left\{
      g_i^\intercal \bar{\mathbf{w}}
      % \text{ s.t. }
      :~
      \left\Vert \bar{\mathbf{w}} - \mathbf{w}\right\Vert_{P}^{2}\leq\varrho^{2}
      \right\}
\]
To that end, let $\tilde{\mathbf{w}}=\varrho^{-1}P^{1/2}(\bar{\mathbf{w}} - \mathbf{w})$. Then the above becomes
\[
   \overline{g}_{i}({\mathbf{w}}) = \max_{\tilde{\mathbf{w}}} \left\{
   g_{i}^{\intercal}(\varrho P^{-1/2}\tilde{\mathbf{w}}+{\mathbf{w}})
   % \text{ s.t. }
   :~
   \norm{\tilde{\mathbf{w}}}\leq 1 \right\}
\]
The maximization of a linear function on the unit disk has an analytical
solution and that is
\[
   \overline{g}_{i}(\mathbf w)=\varrho\left\Vert P^{-1/2}g_{i}\right\Vert +g_{i}^{\intercal}\mathbf{{w}}
\]
By putting everything together we conclude the proof.

\subsection{Proof of Theorem 3.3}
\label{appdx:proofs:robust_mpc_reform}

The program~\eqref{eq:nc_rmpc_vectorized_formulation} follows directly by combining the results of Lemmas~\ref{lem:nc_mpc_linear_form}~and~\ref{lem:robust_con_satisf}. Let us denote the inner maximization as
\begin{maxi*}
  % {\substack{\bar{\mathbf{w}}\in\mathfrak{B}_{\varrho,P}({\mathbf{w}})}}
  {\substack{\bar{\mathbf{w}}\in\mathcal{W}}}
  {
     \left\Vert
         \mathbf{A}x+\mathbf{Bu}+\mathbf{E}\bar{\mathbf{w}}
     \right\Vert _{\mathbf{Q}_{\mathbf{x}}}^{2}
     + \left\Vert \mathbf{u}\right\Vert _{\mathbf{Q_{u}}}^{2}
  }
  {}{J(\mathbf{u}) \coloneqq}
\end{maxi*}
and its corresponding Lagrangian as
\begin{align*}
  \mathcal{L}_{J}(\lambda,\mathbf{u},\bar{\mathbf{w}}) \coloneqq
      \left\Vert
        \mathbf{A}x + \mathbf{Bu} + \mathbf{E}\bar{\mathbf{w}}
        \right\Vert_{\mathbf{Q}_{\mathbf{x}}}^{2}
      + \left\Vert \mathbf{u}\right\Vert _{\mathbf{Q_{u}}}^{2}
     -\lambda\left(\lVert \bar{\mathbf{w}}
     -{\mathbf{w}}\rVert_{P}^{2}-\varrho^{2}\right)
\end{align*}
After some manipulations and rearrangements, we have
\begin{align*}
  \mathcal{L}_{J}(\lambda,\mathbf{u},\bar{\mathbf{w}}) =
     & \left\langle
        \bar{\mathbf{w}},\left(\mathbf{E}^{\intercal}\mathbf{Q_{x}}\mathbf{E}
        - \lambda P\right)\bar{\mathbf{w}}
     \right\rangle
     +2\left\langle \mathbf{E}^{\intercal}\mathbf{Q_{x}}\left(\mathbf{A}x+\mathbf{Bu}\right)+\lambda P{\mathbf{w}},\bar{\mathbf{w}}\right\rangle \\
     & +\left\Vert \mathbf{A}x+\mathbf{Bu}\right\Vert _{\mathbf{Q_{x}}}^{2} +\left\Vert \mathbf{u}\right\Vert _{\mathbf{Q_{u}}}^{2}
      -\lambda\left(\left\Vert {\mathbf{w}}\right\Vert _{P}^{2}-\varrho^{2}\right)
\end{align*}
Let us introduce the following notation
\begin{align*}
  \Lambda(\lambda) \coloneqq &
     \mathbf{E}^{\intercal}\mathbf{Q_{x}}\mathbf{E}-\lambda P\\
  M(\lambda,\mathbf{u}) \coloneqq &
     \mathbf{E}^{\intercal}\mathbf{Q_{x}}\left(\mathbf{A}x
     +\mathbf{Bu}\right)+\lambda P{\mathbf{w}}\\
  \nu_{1}(\lambda) \coloneqq&
     -\lambda\left(\left\Vert {\mathbf{w}}\right\Vert _{P}^{2}
     -\varrho^{2}\right)\\
  \nu_{2}(\mathbf{u}) \coloneqq &
     \left\Vert \mathbf{A}x+\mathbf{Bu}\right\Vert _{\mathbf{Q_{x}}}^{2}+\left\Vert \mathbf{u}\right\Vert _{\mathbf{Q_{u}}}^{2} \\
  \nu(\lambda,\mathbf{u}) \coloneqq & \nu_{1}(\lambda)+\nu_{2}(\mathbf{u})
\end{align*}
The dual of this problem is then
\begin{align*}
  d_{J}(\lambda,\mathbf{u}) \coloneqq
     \max_{\bar{\mathbf{w}}}
     \mathcal{L}_{J}(\lambda,\mathbf{u},\bar{\mathbf{w}})
     =
     \begin{cases}
        -M(\lambda,\mathbf{u})^{\intercal}
           \Lambda(\lambda)^{\dagger} M(\lambda,\mathbf{u})
           +\nu(\lambda,\mathbf{u}), \\
        \text{\quad if } \Lambda(\lambda)\preccurlyeq0 \text{ and }
           M(\lambda,\mathbf u)^\intercal
           \left(I - \Lambda(\lambda)\Lambda(\lambda)^\dagger \right) = 0 \\
        % \text{\;\,if \ensuremath{\Lambda(\lambda)\preccurlyeq0} and }\\
        % \quad \left(I - \Lambda(\lambda)\Lambda(\lambda)^\dagger \right)
        %    M(\lambda,\mathbf u) = 0\\
           % M(\lambda,\mathbf{u})\in\mathrm{span}(\Lambda(\lambda))\\
        +\infty, \text{ otherwise}
     \end{cases}
\end{align*}
Strong duality holds due to the S-Lemma \citep{boydConvexOptimization2004}. Therefore,
$J(\mathbf{u})=\min_{\lambda\geq0}d_{J}(\lambda,\mathbf{u})$. Now consider the following epigraph reformulation
\begin{mini*}
  {\substack{\lambda, \gamma_1}}
  {\gamma_1 + \nu_2(\mathbf u)}
  {}{J(\mathbf u) =}
  \addConstraint{\lambda}{\geq 0}
  \addConstraint{\Lambda(\lambda)}{\preccurlyeq0}
  \addConstraint{
     M(\lambda,\mathbf u)^\intercal \left(
        I - \Lambda(\lambda)\Lambda(\lambda)^\dagger
     \right)
  }{ = 0}
  \addConstraint{
     - M(\lambda,\mathbf{u})^{\intercal}
     \Lambda(\lambda)^{\dagger}
     M(\lambda,\mathbf{u})
     + \nu_{1}(\lambda)}{\leq \gamma_1}
\end{mini*}
% \begin{align*}
%    J(\mathbf{u})=\min_{\lambda} & \gamma_{1}+\nu_{2}(\mathbf{u})\\
%    \text{s.t. } & \lambda\geq0,\\
%    & \Lambda(\lambda)\preccurlyeq0,\\
%    & M(\lambda,\mathbf u)^\intercal \left(
%          I - \Lambda(\lambda)\Lambda(\lambda)^\dagger
%       \right)  = 0,\\
%    & -M(\lambda,\mathbf{u})^{\intercal}\Lambda(\lambda)^{\dagger}M(\lambda,\mathbf{u})+\nu_{1}(\lambda)\leq\gamma_{1}
% \end{align*}
The last three constraints can be cast as an LMI using the non-strict
Schur complement \citep{boydLinearMatrixInequalities1994} and we have
\begin{mini*}
  {\substack{\lambda, \gamma_1}}
  {\gamma_1 + \nu_2(\mathbf u)}
  {}{J(\mathbf u) =}
  \addConstraint{\lambda}{\geq 0}
  \addConstraint{
     \begin{bmatrix}\Lambda(\lambda) & M(\lambda,\mathbf{u})\\
           \star & \nu_{1}(\lambda)-\gamma_{1}
     \end{bmatrix}
  }{\preccurlyeq 0}
\end{mini*}
% \begin{align*}
%    J(\mathbf{u})=\min_{\lambda,\gamma_{1}} & \gamma_{1}+\nu_{2}(\mathbf{u})\\
%    \text{s.t. } & \lambda\geq0,\\
%    & \begin{bmatrix}\Lambda(\lambda) & M(\lambda,\mathbf{u})\\
%    \star & \nu_{1}(\lambda)-\gamma_{1}
%    \end{bmatrix}\preccurlyeq0
% \end{align*}
Therefore the overall robust MPC problem can now be written as $\min_{\mathbf{u}}\left\{ J(\mathbf{u}):~\mathbf{F}x+\mathbf{Gu}\leq\underline{\mathbf{h}}(\mathbf w)\right\} $. In order to write this in the standard SDP form, we will have to use
another epigraph reformulation, that of $\nu_{2}(\mathbf{u})$:
\begin{mini*}
  {\substack{\lambda, \gamma_1, \gamma_2}}
  {\gamma_1 + \gamma_2}
  {}{}
  \addConstraint{\lambda}{\geq 0}
  \addConstraint{
     \begin{bmatrix}\Lambda(\lambda) & M(\lambda,\mathbf{u})\\
           \star & \nu_{1}(\lambda)-\gamma_{1}
     \end{bmatrix}
  }{\preccurlyeq 0}
  \addConstraint{\mathbf{F}x+\mathbf{Gu}}{\leq\underline{\mathbf{h}}(\mathbf w)}
  \addConstraint{
     \left\Vert \mathbf{A}x+\mathbf{Bu}\right\Vert _{\mathbf{Q_{x}}}^{2}
     + \left\Vert \mathbf{u}\right\Vert _{\mathbf{Q_{u}}}^{2}
  }{ \leq\gamma_{2}}
\end{mini*}
% \begin{align*}
%    \min_{\mathbf{u},\lambda,\gamma_{1},\gamma_{2}} &
%       \gamma_{1}+\gamma_{2}\\
%    \text{s.t. } & \lambda\geq0,\\
%    & \begin{bmatrix}
%       \Lambda(\lambda) & M(\lambda,\mathbf{u})\\
%       \star & \nu_{1}(\lambda)-\gamma_{1}
%    \end{bmatrix}\preccurlyeq0,\\
%    & \mathbf{F}x+\mathbf{Gu}\leq\underline{\mathbf{h}}(\mathbf w),\\
%    & \left\Vert \mathbf{A}x+\mathbf{Bu}\right\Vert _{\mathbf{Q_{x}}}^{2}
%       +\left\Vert \mathbf{u}\right\Vert _{\mathbf{Q_{u}}}^{2}
%       \leq\gamma_{2}
% \end{align*}
The last constraint can now be written as
% \begin{equation*}
$
\gamma_{2}\geq
  \left\langle
     \mathbf{u},
     \left(\mathbf{B}^{\intercal}\mathbf{Q_{x}}\mathbf{B} + \mathbf{Q_{u}}
  \right)\mathbf{u}\right\rangle
  + 2\left\langle \mathbf{B}^{\intercal} \mathbf{\mathbf{Q_{x}}}\mathbf{A}x,\mathbf{u}\right\rangle +\left\Vert \mathbf{A}x\right\Vert _{\mathbf{Q_{x}}}^{2}
$,
% \end{equation*}
which can be expressed as the LMI:
\[
  \begin{bmatrix}
     -I_N & \left( \mathbf{B}^{\intercal} \mathbf{Q_{x}} \mathbf{B}+\mathbf{Q_{u}} \right)^{1/2} \mathbf{u} \\
     \star & 2\left\langle \mathbf{B}^{\intercal} \mathbf{\mathbf{Q_{x}}}\mathbf{A}x,\mathbf{u}\right\rangle +\left\Vert \mathbf{A}x\right\Vert _{\mathbf{Q_{x}}}^{2} - \gamma_2
  \end{bmatrix} \preccurlyeq 0
\]
% Denote
% \begin{align*}
%    \mathbf{Q}= &
%       \mathbf{B}^{\intercal} \mathbf{Q_{x}} \mathbf{B}+\mathbf{Q_{u}}\\
%    \delta(\mathbf{u})= &
%       2\left\langle \mathbf{B}^{\intercal} \mathbf{\mathbf{Q_{x}}}\mathbf{A}x,\mathbf{u}\right\rangle +\left\Vert \mathbf{A}x\right\Vert _{\mathbf{Q_{x}}}^{2}
% \end{align*}
% and then we get that this constraint is equivalent to the following
% LMI
% \[
%    \begin{bmatrix}
%       -I_{N} & \mathbf{Q}^{1/2}\mathbf{u}\\
%       \star & \delta(\mathbf{u})-\gamma_{2}
%    \end{bmatrix}\preccurlyeq0
% \]
% Finally, let us denote the LMIs in the constraints as
% \begin{equation*}
%    L_{1}(\lambda,\mathbf{u})=
%    \begin{bmatrix}
%       \Lambda(\lambda) & M(\lambda,\mathbf{u})\\
%       \star & \nu_{1}(\lambda)-\gamma_{1}
%    \end{bmatrix}
% \end{equation*}
% and
% \begin{equation*}
%    L_{2}(\mathbf{u})=
%    \begin{bmatrix}
%       -I_{N} & \mathbf{Q}^{1/2}\mathbf{u}\\
%       \star & \delta(\mathbf{u})-\gamma_{2}
% \end{bmatrix}
% \end{equation*}
Hence, by putting everything together we arrive that the original problem~\eqref{eq:nc_rmpc_vectorized_formulation} is equivalent to:
\begin{mini*}
  {\substack{\mathbf u, \lambda, \gamma_1, \gamma_2}}
  {\gamma_{1}+\gamma_{2}}{}{}
  \addConstraint{\lambda}{\geq0}
  \addConstraint{\mathbf{F}x+\mathbf{Gu}}{\leq\underline{\mathbf{h}}(\mathbf w)}
  \addConstraint{
     \begin{bmatrix}
        \mathbf{E}^{\intercal}\mathbf{Q_{x}}\mathbf{E}-\lambda P &
        % \mathbf{Y}^{\intercal}\mathbf{Y}-\lambda P &
        \mathbf{E}^{\intercal}\mathbf{Q_{x}}\left(\mathbf{A}x+\mathbf{Bu}\right)+\lambda P{\mathbf{w}} \\
        % \mathbf{Y}^{\intercal}\mathbf X(x,\mathbf u)+\lambda P{\mathbf{w}} \\
        \star & -\gamma_1-\lambda\left(\left\Vert {\mathbf{w}}\right\Vert _{P}^{2}-\varrho^{2}\right)
     \end{bmatrix}}{\preccurlyeq 0}
  \addConstraint{
     \begin{bmatrix}
        -I_{N} & \left(\mathbf{B}^{\intercal} \mathbf{Q_{x}} \mathbf{B}+\mathbf{Q_{u}}\right)^{1/2}\mathbf{u} \\
        \star & 2\left\langle \mathbf{B}^{\intercal}\mathbf{\mathbf{Q_{x}}}\mathbf{A}x,\mathbf{u}\right\rangle +\left\Vert \mathbf{A}x\right\Vert _{\mathbf{Q_{x}}}^{2}-\gamma_{2}
     \end{bmatrix}}{ \preccurlyeq 0.}
\end{mini*}
We have arrived at the formulation in the Theorem statement, and as such, we conclude the proof.
% By denoting $\mathbf X (x, \mathbf u) = \mathbf A x + \mathbf B \mathbf u$, we arrive at the formulation in the Theorem statement, and as such, we conclude the proof.



% \section{Additional numerical experiments}
% \label{sec:numerical_experiments_apdx}

\section{Additional Numerical Experiments}
\label{appdx:num_exps}

Besides the numerical experiments in Section \ref{sec:num_experiments}, we include two more examples that enable us to study our approach in more detail.

\subsection{Linear fighter jet}
 We consider the regulation of the unstable dynamics of a six-dimensional fighter jet~\citep{safonovFeedbackPropertiesMultivariable1981} with additive unknown disturbances $w_{t+1} = f_w(t; w_0) + v_{t+1}$, where $f_w$ has a sinusoidal component with random phase $w_0 \sim \mathcal U[0,\pi/2]$ and a bias term, and $v_{t} \sim \mathcal N(0,\Sigma_v)$. As the dynamics are given and linear, the nominal model $\tilde f_0(x,u,w) = Ax + Bu + Ew$ coincides with the true dynamics $f$. Initial conditions are sampled randomly as $x_0 \sim \mathcal N (0,0.1I_6)$. Further, we impose that the state be constrained in $\left\{ x \in \mathbb R^6: \lvert x^1 \rvert \leq 1 \right\}$ and the input in $\left\{ u \in \mathbb R^2: \lvert u^1 \rvert \leq 2,\, \lvert u^2 \rvert \leq 3 \right\}$. We select the IO features as $\phi (\mathbf x_{1:t}, \mathbf u_{1:t}) = (x_t, 1, w_{t-1}, w_{t})$.

The dynamics of the fighter jet~\citep{safonovFeedbackPropertiesMultivariable1981} have been discretized with a sampling time of $0.035$\,s, resulting in the following discrete-time system matrices:
\[
   \resizebox{\textwidth}{!}{$
      \displaystyle
      A = \bmqty{
         0.9991  & -1.3736  & -0.6730  & -1.1226  &  0.3420  & -0.2069\\
         0.0000  &  0.9422  &  0.0319  & -0.0000  & -0.0166  &  0.0091\\
         0.0004  &  0.3795  &  0.9184  & -0.0002  & -0.6518  &  0.4612\\
         0.0000  &  0.0068  &  0.0335  &  1.0000  & -0.0136  &  0.0096\\
              0  &       0  &       0  &       0  &  0.3499  &       0\\
              0  &       0  &       0  &       0  &       0  &  0.3499
      },\;
      B = \bmqty{
         0.1457 &  -0.0819\\
        -0.0072 &   0.0035\\
        -0.4085 &   0.2893\\
        -0.0052 &   0.0037\\
         0.6501 &        0\\
              0 &   0.6501
      },\;
      E = \bmqty{
         0  &   0\\
         0  &   0\\
         1  &   0\\
         0  &   1\\
         0  &   0\\
         0  &   0
      }.
   $}
\]
As mentioned in the main body, the disturbances are $w_{t+1} = f_w(t; w_0) + v_{t+1}$, where $v_t\sim \mathcal(0, \Sigma v)$, $w_0 \sim \mathcal U[0, \pi/2]$,  with
\[
    f_w(t;w_0) = \bmqty{0.5 \sin(4.488 t + w_0)\\ 0.01}
    \text{ and }
    \Sigma_v = \bmqty{ 0.01 & 0\\ 0 & 0.001}.
\]
The cost parameters are selected as $Q_f = Q_x = \diag (1,10^3,10^2,10^3,1,1)$ and $Q_u = I_2$, and the MPC horizon is $N = 20$.

\paragraph{Approximating NC-MPC with IO:}
\label{sec:fighter_dst}

First, we want to validate that hindsight can be used to mitigate unknown disturbances. As such, we will compare the following policies: {\bf MPC~(obl)}, an MPC that can measure only $x_t$ at time $t$ and does not know $f_w$, as described in~\eqref{eq:mpc_formulation}; {\bf MPC~(dst)}, an MPC that can measure both $x_t$ and $w_{t+1}$ at time $t$, and also knows $f_w$; and {\bf IO\nobreakdash-MPC}, the policy resulting from applying Algorithm \ref{alg:nc_mpc_inv_opt} to a dataset of trajectories obtained from MPC~(obl). All IO-derived policies described in this paragraph and the next are trained with a dataset containing 10 trajectories induced by MPC~(obl) of length 51 each. In the left plot of Figure~\ref{fig:fighter_dst_cost_hist} we have the cost histogram of $c(x,u)$ for each tested policy during steady state\footnote{Defined as the last 40\% of data points of each trajectory.}. We can see that in both plots  the IO-MPC policy recovers a significant part of the performance of MPC~(dst), both in terms of median and of variance.

\paragraph{Approximating NC-RMPC with IO:}
\label{sec:fighter_dst_rob}

Using the same setup and data, we impose a distribution shift in the disturbances during evaluation by adding a constant bias to $w_t$; specifically, we apply $\tilde w_t$ instead of $w_t$, where $\tilde w_t^\intercal = w_t^\intercal + \bmqty{0.1 & 0.05}^\intercal$. We therefore compare the following: {\bf MPC~(obl)}, as before; {\bf MPC~(p\nobreakdash-dst)}, as MPC~(dst) of the previous section -- only measures $w_{t+1}$; {\bf MPC~(f-dst)}, similar to MPC~(p-dst), except that it has access to $\tilde w_{t+1}$ instead of $w_{t+1}$; {\bf IO-MPC}, as before; {\bf IO\nobreakdash-RMPC}, a robust MPC of the form~\eqref{eq:nc_rmpc_vectorized_formulation}, trained with the same data as IO-MPC and equipped with $P=I_{N n_w}$ and $\varrho = 10^{-2}$. It is immediately obvious from the middle and rightmost cost distributions of Figure~\ref{fig:fighter_dst_cost_hist} that imitating the robust expert yields performance benefits when faced with distribution shift, as the median performance of IO-RMPC is better than that of IO-MPC. Not only that, but IO-RMPC manages to recover the median performance of MPC~(f\nobreakdash-dst), albeit with a larger variance.

\begin{figure}[]
    \centering
    \includegraphics[width=0.32\textwidth]{figs/neurips/fighter/dst_cost_hist.pdf}%
    \hfill%
    \includegraphics[width=0.32\textwidth]{figs/neurips/fighter/dst_rob_io_cost_hist.pdf}%
    \hfill%
    \includegraphics[width=0.32\textwidth]{figs/neurips/fighter/dst_rob_mpc_cost_hist.pdf}%
    \caption{Steady-state cost distributions (log-log scale) over 100 trials of the experiments described in Section~\ref{sec:fighter_dst}. Dashed lines represent the median values. \textbf{Left:} MPC policies vs IO-MPC .\textbf{Center:} Difference in performance between the robust and non-robust version of IO policies when faced with distribution shift. \textbf{Right:} Performance of IO-RMPC vs MPC policies when faced with distribution shift.}
    \label{fig:fighter_dst_cost_hist}
   %
   \vspace*{\floatsep}%
   %
   \includegraphics[width=0.32\textwidth]{figs/neurips/fighter/avg_cost_var_rho.pdf}%
   \hfill%
   \includegraphics[width=0.32\textwidth]{figs/neurips/fighter/rho_sel.pdf}%
   \hfill%
   \includegraphics[width=0.32\textwidth]{figs/neurips/fighter/rho_star_ss_cost_hist.pdf}%
   \caption{Additional experiments as described in Section~\ref{sec:num_exp_uncertainty_radius}. \textbf{Left:} Time-averaged steady-state cost for different controllers trained with 50 different datasets and for varying $\varrho$; solid lines indicate the median values, and tube indicates the range from the 5th to 95th percentiles. \textbf{Center:} Steady-state cost distribution for different controllers trained with 1 dataset and for varying $\varrho$; the tubes consist of the 20th to 80th percentile range from 100 trials, while the dashed lines represent the median values. \textbf{Right:} Steady-state cost histograms for optimal $\varrho = \varrho^*$ over 100 trials of a single controller realization; dashed lines indicate the median values.}
   \label{fig:fighter}
    %
   \vspace*{\floatsep}%
   %
   \includegraphics[width=0.32\textwidth]{figs/neurips/dualheater/ablation_H.pdf}%
    \hfill%
    \includegraphics[width=0.32\textwidth]{figs/neurips/dualheater/ablation_rho.pdf}%
    \hfill%
    \includegraphics[width=0.32\textwidth]{figs/neurips/dualheater/dh_ss_cost_hists.pdf}%
    \caption{Experiments of Section~\ref{sec:dual_heater}. \textbf{Left:} Steady-state cost distribution for different controllers trained with 1 dataset over 100 trials: we vary the size of $H$ and the effect the bias term has. \textbf{Center:} Steady-state cost distribution for different
    controllers trained with 1 dataset for varying $\varrho$ over 100 trials. \textbf{Right:} Steady-state cost histograms for the policies described in Section~\ref{sec:dual_heater} over 200 trials of a single controller realization. In all three figures, dashed lines indicate medians, and in the first two, the tubes consist of the range between 20th to 80th percentiles.}
    \label{fig:dualheater_ablations_and_cost_hists}
\end{figure}


\paragraph{Effect of uncertainty radius:}
\label{sec:num_exp_uncertainty_radius}

We further explore the impact of the robustness parameter (uncertainty radius~$\varrho$) on the steady-state cost distribution across different training datasets. In the left plot of Fig.~\ref{fig:fighter}, we observe that increasing $\varrho$ until $\varrho^*$ yields a consistent reduction in the time-averaged steady-state cost across different training sets. What is surprisingly interesting is that there are some datasets which, when trained with properly tuned $\varrho$, can match the performance of the full-information agent MPC (f-dst). We also looked into the performance of such controllers on the entire distribution of the steady-state cost in the middle plot of Fig.~\ref{fig:fighter}: $\varrho$ has a positive impact on the entire steady-state cost distribution (and not only the median or average). We also note that the non-robust controller IO-MPC coincides with the robust one (IO-RMPC) for sufficiently small $\varrho$.
In the right plot of Fig.~\ref{fig:fighter}, we freeze $\varrho=\varrho^*$ and look at the entire steady-state cost distributions of the three policies involved in the middle plot.
We observe that even though the median performance of IO-RMPC surpasses that of MPC~(f-dst), its variance across the test set is much more spread, making it more high-risk than MPC (f-dst).
However, as the variance of the non-robust IO policy is similarly wide, the takeaway message here is that robustification combats distribution shift during policy evaluation.


\subsection{Nonlinear temperature control}
\label{sec:dual_heater}

Here, we consider a nonlinear 4-th order dynamical system that describes the heat transfer equations of two coupled heating elements (inputs) and two temperature sensors (outputs), akin to that of~\citep{parkBenchmarkTemperatureMicrocontroller2020}. Specifically, the nonlinear differential equations describing the heat-transfer dynamics are the following:
\begin{equation}
\label{eq:dual_heater_dynamics}
\begin{subequations}
\begin{aligned}
    \tau_h \dot x_1 &= a_1 (T_\infty - x_1) + a_2 (T_\infty^4 - x_1^4) + a_3 (x_2 - x_1) + a_4 (x_2^4 - x_1^4) + b_1 u_1\\
    \tau_h \dot x_2 &= a_1 (T_\infty - x_2) + a_2 (T_\infty^4 - x_2^4) + a_3 (x_1 - x_2) + a_4 (x_1^4 - x_2^4) + b_2 u_2\\
    \tau_c \dot x_3 &= x_1 - x_3\\
    \tau_c \dot x_4 &= x_2 - x_4
\end{aligned}
\end{subequations}
\end{equation}
with outputs $y_1 = x_3$ and $y_2 = x_4$. The parameters $a_1$, $a_2$, $a_3$,$a_4$, $b_1$, $b_2$, $\tau_c$, $\tau_h$, are lumped-parameter coefficients that can be summarized in Table~\ref{tab:dualheater_params}. We assume full state feedback. The ambient temperature $T_\infty$ is constant throughout each trial, but randomly sampled from a uniform distribution $T_\infty \sim \mathcal U[18, 28]$, and is subjected to additional Gaussian noise $v_{t+1} \sim \mathcal N (0,1)$ before entering the nonlinear dynamics. The control objective is for the outputs $y$ to track the temperature setpoints $r_1 = 55^\circ C$ and $r_2 = 45^\circ C$, with $Q_x = Q_f = I_2$ and $Q_u = \diag(1, 0.5)$. To obtain the nominal model $\tilde f_0$, we linearize~\eqref{eq:dual_heater_dynamics} around $(\bar x, \bar u)$ which corresponds to the steady-state solution of $y = r$, and then discretize with a sampling rate of $10$\,s. As such, here the resulting nominal model $\tilde f_0$ used for the MPC controllers differs from the true nonlinear dynamics $f$. Due to this, the in-hindsight disturbance trajectories contain terms that stem from model mismatch:
\begin{equation*}
   w_{t+1} =  E^\dagger \left(f(x_t, u_t, T_\infty + v_{t+1}) -\bar x - A \delta x_t - B \delta u_t\right)
\end{equation*}
where $\delta x_t = x_t - \bar x$, $\delta u_t  = u_t - \bar u$ are its zero coordinates, on which our policies operate.

\begin{table}[t]
    \centering
    \begin{tabular}{cccc cc cc}
    \toprule
        $a_1$ & $a_2$ & $a_3$ & $a_4$ & $b_1$ & $b_2$ & $\tau_c$ & $\tau_h$ \\ \midrule
        $4\cdot10^{-3}$ & $5.1 \cdot 10^{-11}$ & $7.3\cdot10^{-3}$ & $ 10^{-11}$ & $0.011$ & $0.006$ & $18.3$ & $2$\\
        \bottomrule\\
    \end{tabular}
    \caption{Lumped-parameter coefficients of system \eqref{eq:dual_heater_dynamics}.}
    \label{tab:dualheater_params}
\end{table}


Similarly to before, we want to evaluate the performance of Inverse Optimization derived policies, in both the robust and non-robust settings. Specifically, we will investigate the performance of the following policies: {\bf MPC}, a naive MPC with the assumption that $T_\infty = \mathbb E [T_\infty] =23^\circ C$; {\bf IO\nobreakdash-MPC~(1)}, an IO-derived policy akin to~\eqref{eq:nc_mpc_formulation} with feature map $\phi (\mathbf x_{1:t}, \mathbf u_{1:t}) =(\delta x_t, 1, \mathbf{w}_{t-1:t})$; {\bf IO\nobreakdash-MPC~(2)}, like IO\nobreakdash-MPC~(1), but with no bias term and $H=8$, thus $\phi (\mathbf x_{1:t}, \mathbf u_{1:t})= (\delta x_t, \mathbf{w}_{t-7:t})$; {\bf IO\nobreakdash-RMPC~(1)}, the robust counterpart to  IO-MPC~(1), equipped with $P=I_N$ and $\varrho = 70$. All IO-derived policies resulted from the same dataset, containing 10 trajectories of length 51 each. 


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.32\textwidth]{figs/neurips/dualheater/ablation_H.pdf}%
%     \hfill%
%     \includegraphics[width=0.32\textwidth]{figs/neurips/dualheater/ablation_rho.pdf}%
%     \hfill%
%     \includegraphics[width=0.32\textwidth]{figs/neurips/dualheater/dh_ss_cost_hists.pdf}%
%     \caption{Experiments of Section~\ref{sec:dual_heater}. \textbf{Left:} Steady-state cost distribution for different controllers trained with 1 dataset over 100 trials: we vary the size of $H$ and the effect the bias term has. \textbf{Center:} Steady-state cost distribution for different
%     controllers trained with 1 dataset for varying $\varrho$ over 100 trials. \textbf{Right:} Steady-state cost histograms for the policies described in Section~\ref{sec:dual_heater} over 200 trials of a single controller realization. In all three figures, dashed lines indicate medians, and in the first two, the tubes consist of 20-80 percentiles.}
%     \label{fig:dualheater_ablations_and_cost_hists}
% \end{figure}

Firstly, we performed an ablation on the features: whether or not to include a bias term and what is the best value of $H$ (lookback horizon). The results of this are present in the leftmost plot of Figure~\ref{fig:dualheater_ablations_and_cost_hists}. It is evident that the optimal combination of features is no bias term and $H=8$ (IO\nobreakdash-MPC~(2)). When evaluating the robust counterpart of IO\nobreakdash-MPC~(2), we found that for small values of $\varrho$, there was little to no performance improvement, and for larger values the performance deteriorated. We posit that given our experimental setting, IO\nobreakdash-MPC~(2) has enough expressivity that it can generalize well to unseen disturbances and capture most of the available performance, and thereby robustification has little benefit to add. On the other hand, when performing the same procedure on the worse-performing policy IO-MPC (1) with a bias term in the features and $H=2$, we saw that robustification led to better generalization, as the performance improved when compared to its non-robust counterpart, as can be depicted in the middle plot of Figure~\ref{fig:dualheater_ablations_and_cost_hists}.

Finally, in the rightmost plot of Figure~\ref{fig:dualheater_ablations_and_cost_hists}, we can clearly see that each IO policy surpasses the performance of the naive approach (MPC), but that is to be expected as per our previous experimental discussions. The takeaway message from this figure is that robustifying can help in better generalization capabilities, and that our framework has the potential to deal with disturbance sequences that are correlated with the state, such as in cases where there is model mismatch.

\vskip 0.2in

\bibliographystyle{unsrtnat}
\bibliography{main}

\end{document}