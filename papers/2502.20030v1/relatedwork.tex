\section{Related works}
\label{sec:related_works}

Some recent advances in RL, also termed as model-free or data-driven control, are reported in Table~\ref{tab:rl_categories}. The table summarizes the literature across different categories of policy class (linear/nonlinear, statics/dynamics) and the training procedure, which will also be briefly discussed below.


\begin{table*}[t!]
\adjustbox{width=\textwidth}{%
\centering
\begin{threeparttable}[b]
    \caption{Classification of some Reinforcement Learning methods for the control of dynamical systems. }
    \label{tab:rl_categories}
    \vskip 0.15in
    \begin{tabular}{@{}llcclc@{}}
    \toprule
    \multirow{2}{*}{Category}  & \multirow{2}{*}{Paper} & \multirow{2}{*}{Policy class}\tnote{a} & \multicolumn{2}{c}{Training} & \multirow{2}{*}{Dynamic}\tnote{b} \\ \cmidrule(lr){4-5}
    & & & Model-based & Learning policy parameters   & \\ \midrule
    Offline RL  & \citet{IQL} & Neural Network & \xmark & Regularized Value Iteration & \xmark \\
                & \citet{CQL} & Neural Network & \xmark & Regularized Value Iteration & \xmark \\
                & \citet{COMBO} & Neural Network & \cmark & Regularized Value Iteration & \cmark \\
    Imitation Learning  & \citet{BAIL} & Neural Network & \xmark & Mean Squared Error & \xmark \\
    Q-Learning  & \citet{bradtkeAdaptiveLinearQuadratic1994} & Linear & \xmark & Policy Iteration & \xmark \\
                & \citet{kiumarsiReinforcementQLearningOptimal2014} & Linear & \xmark & Policy Iteration & \xmark \\
    Regret-min. & \citet{deanSampleComplexityLinear2020} & Linear & \cmark & System Level Synthesis (SDP) & \xmark \\
               %  & \citealp{cohenLearningLinearQuadraticRegulators2019} & Linear & \cmark & Robust LQR (SDP) & \xmark \\
               %  & \citealp{laleExploreMoreImprove2020} & Linear & \cmark & Riccati & \xmark \\
                & \citet{simchowitzNaiveExplorationOptimal2020} & Linear & \cmark & Riccati & \xmark \\
                & \citet{agarwalOnlineControlAdversarial2019} & Linear & \cmark & Online convex optimization & \cmark  \\
                & \citet{hazanNonstochasticControlProblem2020} & Linear & \cmark & Online convex optimization & \cmark \\
               %  & \citealp{fosterLogarithmicRegretAdversarial2020} & Linear & \cmark & Online convex optimization & \cmark \\
    Willems' Lemma & \citet{depersisFormulasDataDrivenControl2020} & Linear & \cmark\tnote{1} & LMI-based design (SDP) & \xmark \\
                   & \citet{vanwaardeNoisyDataFeedback2022} & Linear & \cmark\tnote{1} & S-Lemma-based design (SDP) & \xmark \\
                   & \citet{coulsonDataEnabledPredictiveControl2019}  & MPC-$N$ & \cmark\tnote{1}  & --- & \xmark \\
    Terminal VF Approx.  & \citet{zhongValueFunctionApproximation2013} & MPC-$N$ & \cmark & Non-convex optimization & \cmark \\
                         & \citet{lowreyPlanOnlineLearn2018} & MPC-$N$ & \cmark & Non-convex optimization & \cmark \\
                         & \citet{bhardwajBlendingMPCValue2020} & MPC-$N$ & \cmark & Non-convex optimization & \cmark \\
                        %  & This work & MPC-$N$\tnote{1} & \cmark & Non-convex optimization & \cmark \\ 
                        % & This work & MPC-1\tnote{1} & \cmark & Inverse Optimization (SDP) & \cmark \\
                        & This work & MPC-1 & \cmark & Inverse Optimization (SDP) & \cmark\tnote{2} \\
      \bottomrule
      \end{tabular}
      \begin{tablenotes}
         \item[a] Refers to the mapping between state/feature and input/action.
         \item[b] Refers to whether the policy has states and dynamics of its own.
         \item[1] Instead of casting the model identification and policy learning as two independent optimization problems, these methods combine them into a single program, thereby finding the most optimal model for the control objective.
         \item[2] While not necessary, these policies can be designed to depend on previous instances of the states and/or actions.
      \end{tablenotes}
   \end{threeparttable}}
%   \vskip -0.1in
\end{table*}


\paragraph{Q-Learning:}
The policy class considered in \eqref{eq:policy_class} is that of a greedy policy in Q-Learning literature. However, the minimization of $\mathrm Q_\theta$ is often problematic as it is often non-convex in $u$. Therefore, works in this domain either have to resort to discretized state and action spaces \citep{watkinsQlearning1992} or to Neural Network approximations of the minimizer of  $\mathrm Q_\theta$ \citep{lillicrapContinuousControlDeep2015}. There are Q-Learning settings where there is convexity in $u$, such as LQR \citep{bradtkeAdaptiveLinearQuadratic1994,kiumarsiReinforcementQLearningOptimal2014}, but there is no support for constrained state and input spaces. As we consider a parametric optimization policy class, we  allow for constrained input and state spaces.

\paragraph{Regret minimization:}
Works in this area consider the online data-driven linear policies:
\begin{equation}
   \label{eq:min_regret_policy}
   \pi_K(x_t) = -K_0 x_t - \sum_{\tau=1}^H K_\tau w_{t-\tau}.
\end{equation}
When disturbances are entirely stochastic and i.i.d., we have from Certainty Equivalence that the disturbance feedback terms of~\eqref{eq:min_regret_policy} can be neglected, and the problem reduces to finding an appropriate model and solving a Riccati-like problem to recover $K_0$~\citep{deanSampleComplexityLinear2020,simchowitzNaiveExplorationOptimal2020}. On the other hand, when disturbances are correlated, these terms are necessary and are discovered through Online Convex Optimization~\citep{agarwalOnlineControlAdversarial2019,hazanNonstochasticControlProblem2020}. In both scenarios, however, the policy class is linear, and as such, they only concern the unconstrained setting.


\paragraph{Willems' Lemma:} This lemma~\citep{willemsNotePersistencyExcitation2005} provides a data-based model parameterization that combines system identification and controller design in one step.
In~\citep{depersisFormulasDataDrivenControl2020,vanwaardeNoisyDataFeedback2022} it is used to express LQR synthesis as a data-driven optimization program, while in DeePC~\citep{coulsonDataEnabledPredictiveControl2019} the lemma is used in an MPC setting. Such results require decision variables that are much larger in dimension than traditional approaches.
Conversely, we offer reduced complexity in that regard by shrinking the MPC-$N$ problem to MPC-1.

\paragraph{Offline Reinforcement Learning:} To prevent the value function from exploiting any dataset bias, offline RL approaches typically attempt to enforce pessimistic policy learning; this can be achieved by constraining the policy learning within the region supported by the dataset~\citep{BCQ, guo2020batch}, penalizing state-action pairs outside the dataset~\citep{BEAR, BRAC, IQL, CQL} , or by the primal-dual optimization approach \citep{gabbianelli2023offline, hong2023primal, le2019batch}. Model-based approaches employ similar ideas but instead try to exploit the model information to learn a less conservative value function.  For instance, COMBO~\citep{COMBO} approximates the true model dynamics and utilizes both simulated and dataset samples to learn a conservative value estimation by penalizing out-of-support state-action pairs obtained by running the simulated model while MOReL \citep{morel} learns a pessimistic model that approximately lower bounds the true performance and performs policy optimization using the learned model. On the other hand, our proposal uses a nominal model to improve the actions of the state-action pairs present in the dataset; finally, in contrast to the aforementioned works, our work is more computationally attractive, as the resulting program for learning the policy is convex.

\paragraph{Imitation Learning:} The second step of our algorithm, where we employ Inverse Optimization to fit a policy on the improved state-action pairs, is analogous to IL. Similar to our dataset improvement scheme, several other IL algorithms employ augmentation strategies to further improve policy learning. For example, BAIL \citep{BAIL} first estimates the Monte Carlo returns of each state-action pair in the dataset, an infinite horizon and discounted extension of our objective function in \eqref{eq:cost}, and employs a neural network-based estimate to fit the returns. Based on this estimate, BAIL selects only the highest-valued state-action pairs and learns a policy via IL. On the other hand, our approach makes use of the entire dataset, improving actions through our robust MPC formulation, and utilizes a convex ``sub-optimality loss" to perform the IL step.

\paragraph{Terminal value function approximation:} Since MPC projects its internal model into the future, it can also act as an approximation to the Bellman equation. This observation is exploited by~\citep{zhongValueFunctionApproximation2013} to effectively increase the planning horizon by constructing approximate terminal Value Functions (VF) from MPC simulation data.

Using the same principle,~\citep{lowreyPlanOnlineLearn2018} showcased an algorithm that promotes exploration and, therefore, accelerates VF learning. Finally,~\citep{bhardwajBlendingMPCValue2020} propose a blended approach that combines elements from model-free and model-based methods to reduce model bias. Similarly, our work can be viewed as a specific instance of VF approximation, where learning the Q-function reduces the horizon to a single step. Additionally, in contrast to the papers mentioned above, our approach is computationally tractable.