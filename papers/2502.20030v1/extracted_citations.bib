@inproceedings{BAIL,
author = {Chen, Xinyue and Zhou, Zijian and Wang, Zheng and Wang, Che and Wu, Yanqiu and Ross, Keith},
title = {BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {There has recently been a surge in research in batch Deep Reinforcement Learning (DRL), which aims for learning a high-performing policy from a given dataset without additional interactions with the environment. We propose a new algorithm, Best-Action Imitation Learning (BAIL), which strives for both simplicity and performance. BAIL learns a V function, uses the V function to select actions it believes to be high-performing, and then uses those actions to train a policy network using imitation learning. For the MuJoCo benchmark, we provide a comprehensive experimental study of BAIL, comparing its performance to four other batch Q-learning and imitation-learning schemes for a large variety of batch datasets. Our experiments show that BAIL's performance is much higher than the other schemes, and is also computationally much faster than the batch Q-learning schemes.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1541},
numpages = {11},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@inproceedings{BCQ,
  title={Off-Policy Deep Reinforcement Learning without Exploration},
  author={Scott Fujimoto and David Meger and Doina Precup},
  booktitle={International Conference on Machine Learning},
  year={2018}
}

@inbook{BEAR, author = {Kumar, Aviral and Fu, Justin and Tucker, George and Levine, Sergey}, title = {Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction}, year = {2019}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA}, booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems}, articleno = {1055}, numpages = {11} }

@article{BRAC,
  title={Behavior Regularized Offline Reinforcement Learning},
  author={Yifan Wu and G. Tucker and Ofir Nachum},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.11361},
}

@inproceedings{COMBO,
 author = {Yu, Tianhe and Kumar, Aviral and Rafailov, Rafael and Rajeswaran, Aravind and Levine, Sergey and Finn, Chelsea},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {28954--28967},
 publisher = {Curran Associates, Inc.},
 title = {COMBO: Conservative Offline Model-Based Policy Optimization},
 volume = {34},
 year = {2021}
}

@inproceedings{CQL,
author = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
title = {Conservative Q-Learning for Offline Reinforcement Learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {100},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@article{IQL,
  author       = {Ilya Kostrikov and
                  Ashvin Nair and
                  Sergey Levine},
  title        = {Offline Reinforcement Learning with Implicit Q-Learning},
  journal      = {CoRR},
  volume       = {abs/2110.06169},
  year         = {2021},
  eprinttype    = {arXiv},
  eprint       = {2110.06169},
  timestamp    = {Fri, 22 Oct 2021 13:33:09 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-06169.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{agarwalOnlineControlAdversarial2019,
  title = {Online Control with Adversarial Disturbances},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Agarwal, Naman and Bullins, Brian and Hazan, Elad and Kakade, Sham and Singh, Karan},
  year = {2019},
  eprint = {1902.08721},
  archiveprefix = {arxiv}
}

@inproceedings{bhardwajBlendingMPCValue2020,
  title = {Blending {{MPC}} \& {{Value Function Approximation}} for {{Efficient Reinforcement Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Bhardwaj, Mohak and Choudhury, Sanjiban and Boots, Byron},
  year = {2020},
  month = sep,
  eprint = {2012.05909},
  urldate = {2022-05-19},
  archiveprefix = {arxiv},
  langid = {english}
}

@inproceedings{bradtkeAdaptiveLinearQuadratic1994,
  title = {Adaptive Linear Quadratic Control Using Policy Iteration},
  booktitle = {American {{Control Conference}}},
  author = {Bradtke, S.J. and Ydstie, B.E. and Barto, A.G.},
  year = {1994}
}

@article{deanSampleComplexityLinear2020,
  ids = {deanSampleComplexityLinear2018},
  title = {On the {{Sample Complexity}} of the {{Linear Quadratic Regulator}}},
  author = {Dean, Sarah and Mania, Horia and Matni, Nikolai and Recht, Benjamin and Tu, Stephen},
  year = {2020},
  journal = {Foundations of Computational Mathematics},
  volume = {20},
  number = {4},
  eprint = {1710.01688},
  pages = {633--679},
  urldate = {2020-09-21},
  archiveprefix = {arxiv}
}

@article{depersisFormulasDataDrivenControl2020,
  ids = {depersisFormulasDataDrivenControl2020a,depersisFormulasDatadrivenControl2019},
  title = {Formulas for {{Data-Driven Control}}: {{Stabilization}}, {{Optimality}}, and {{Robustness}}},
  shorttitle = {Formulas for {{Data-Driven Control}}},
  author = {De Persis, Claudio and Tesi, Pietro},
  year = {2020},
  month = mar,
  journal = {IEEE Transactions on Automatic Control},
  volume = {65},
  number = {3},
  pages = {909--924},
  publisher = {{IEEE}}
}

@article{gabbianelli2023offline,
  title={Offline primal-dual reinforcement learning for linear mdps},
  author={Gabbianelli, Germano and Neu, Gergely and Okolo, Nneka and Papini, Matteo},
  journal={arXiv preprint arXiv:2305.12944},
  year={2023}
}

@inproceedings{guo2020batch,
  title={Batch reinforcement learning through continuation method},
  author={Guo, Yijie and Feng, Shengyu and Le Roux, Nicolas and Chi, Ed and Lee, Honglak and Chen, Minmin},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{hazanNonstochasticControlProblem2020,
  title = {The {{Nonstochastic Control Problem}}},
  booktitle = {International {{Conference}}  on {{Algorithmic Learning Theory}}},
  author = {Hazan, Elad and Kakade, Sham and Singh, Karan},
  year = {2020},
  urldate = {2020-11-20},
  langid = {english},
  annotation = {ZSCC: 0000014}
}

@article{hong2023primal,
  title={A primal-dual-critic algorithm for offline constrained reinforcement learning},
  author={Hong, Kihyuk and Li, Yuhang and Tewari, Ambuj},
  journal={arXiv preprint arXiv:2306.07818},
  year={2023}
}

@inproceedings{le2019batch,
  title={Batch policy learning under constraints},
  author={Le, Hoang and Voloshin, Cameron and Yue, Yisong},
  booktitle={International Conference on Machine Learning},
  pages={3703--3712},
  year={2019},
  organization={PMLR}
}

@article{lillicrapContinuousControlDeep2015,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2015},
  journal = {arXiv preprint arXiv:1509.02971},
  eprint = {1509.02971},
  archiveprefix = {arxiv}
}

@inproceedings{lowreyPlanOnlineLearn2018,
  title = {Plan {{Online}}, {{Learn Offline}}: {{Efficient Learning}} and {{Exploration}} via {{Model-Based Control}}},
  shorttitle = {Plan {{Online}}, {{Learn Offline}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lowrey, Kendall and Rajeswaran, Aravind and Kakade, Sham and Todorov, Emanuel and Mordatch, Igor},
  year = {2018},
  month = sep,
  urldate = {2022-05-19},
  langid = {english}
}

@article{morel,
  title={Morel: Model-based offline reinforcement learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21810--21823},
  year={2020}
}

@inproceedings{simchowitzNaiveExplorationOptimal2020,
  title = {Naive {{Exploration}} Is {{Optimal}} for {{Online LQR}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Simchowitz, Max and Foster, Dylan J.},
  year = {2020},
  eprint = {2001.09576},
  urldate = {2020-10-21},
  archiveprefix = {arxiv},
  langid = {english},
  annotation = {ZSCC: 0000016}
}

@article{vanwaardeNoisyDataFeedback2022,
  ids = {vanwaardeNoisyDataFeedback2020},
  title = {From Noisy Data to Feedback Controllers: Non-Conservative Design via a Matrix {{S-lemma}}},
  shorttitle = {From Noisy Data to Feedback Controllers},
  author = {{van Waarde}, Henk J. and Camlibel, M. Kanat and Mesbahi, Mehran},
  year = {2022},
  journal = {IEEE Transactions on Automatic Control},
  volume = {67},
  eprint = {2006.00870},
  pages = {162--175},
  urldate = {2020-09-30},
  archiveprefix = {arxiv},
  annotation = {ZSCC: 0000004}
}

@article{watkinsQlearning1992,
  title = {Q-Learning},
  author = {Watkins, Christopher John Cornish Hellaby and Dayan, Peter},
  year = {1992},
  journal = {Machine learning},
  volume = {8},
  number = {3-4},
  pages = {279--292},
  publisher = {{Springer}}
}

@article{willemsNotePersistencyExcitation2005,
  title = {A Note on Persistency of Excitation},
  author = {Willems, Jan C. and Rapisarda, Paolo and Markovsky, Ivan and De Moor, Bart L. M.},
  year = {2005},
  month = apr,
  journal = {Systems \& Control Letters},
  volume = {54},
  number = {4},
  pages = {325--329},
  urldate = {2020-05-18},
  langid = {english}
}

@inproceedings{zhongValueFunctionApproximation2013,
  title = {Value Function Approximation and Model Predictive Control},
  booktitle = {{{IEEE Symposium}} on {{Adaptive Dynamic Programming}} and {{Reinforcement Learning}}},
  author = {Zhong, Mingyuan and Johnson, Mikala and Tassa, Yuval and Erez, Tom and Todorov, Emanuel},
  year = {2013},
  month = apr,
  pages = {100--107},
  urldate = {2021-03-27},
  langid = {english}
}

