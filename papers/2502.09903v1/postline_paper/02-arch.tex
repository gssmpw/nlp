\section{The Ann Arbor Architecture}
\subsection{Overview}

The Ann Arbor Architecture aims to create a framework for understanding agents, their connection with language models, and their engagement with tasks and environments.
It is a mind framework rather than the architecture of a realized system.
We challenge the current task-centric agent architectures by highlighting agents' collaboration with engineers, their persistence, long-term growth and even evolution.  
Our goal is to unify various engineering processes, a goal naturally called for by the language model's unification of natural and formal languages.

At its core, the proposed framework models agents and their interactions after the email system. Each agent is assigned a unique identifier—an email address—through which it exchanges messages. All messages with the agent's address  appearing in either the ``From" or the ``To"-like fields form a persistent, chronological record of the agent’s communication history, collectively referred to as the agent’s {\bf journal}. This persistent journal serves as the primary memory mechanism for the agent, ensuring that all past interactions remain accessible and can inform future decision-making.

Under the proposed framework, there is only one form of information exchange: sending emails.  This entails two important distinctions between the proposed framework and the existing approaches.

First, our proposed framework does not separate the design/training phase from the deployment phase. We illustrate this using the development of a customer service conversational agent(Figure~\ref{fig:postline_flow}). In existing systems, engineering and deployment occur in distinct stages: during engineering, developers design workflows and write prompts for different scenarios (the different nodes of a computation graph). Subsequently, in testing and deployment, the computation graph is installed on a (logical) state machine that uses these prompts with language models to drive state changes.

Under our proposed framework, however, this becomes a single continuous process. The trainer teaches the agent to handle customers by conducting email conversations. At some point, the trainer poses as a customer to test the agent’s capabilities. If the agent passes the tests, it naturally transitions to handling real customer interactions in emails (as an underlying communication protocol rather than the actual software interface the trainers and customers see).

\begin{figure}[h]
    \centering
    \includegraphics[trim=0 0 0 0,clip,width=0.8\linewidth]{postline_flow.pdf}
    \caption{Different paradigms of customer service agents.  With existing platforms, development and deployment are separate processes with very different software environments.  Under the proposed framework, both training and serving occur in conversations on the same platform and the transition is seamless.}
    \label{fig:postline_flow}
\end{figure}

Second, our approach is to develop agents that are not confined to a single task or environment. We illustrate this with a research scenario involving assistant agents (Figure~\ref{fig:postline_research}). In existing systems, if a data scientist conducts research using a browser, a coding IDE like Cursor~\cite{cursor24}, and a terminal, a separate agent might be embedded in each of these tools to assist the user. These agents are typically developed by or for the provider of the respective tool.

Under our proposed framework, a single agent (or a team of agents) work across these tools to assist the researcher.  When the researcher switches the browser to the IDE, the same agent is present in the IDE with the recent memory of a browsing session. We propose repurposing the term {\bf robot} to refer to non-intelligent adapters that can be automated by an agent, as well as software and hardware entities equipped with such adapters.  So each tool appears as a robot with its own email address under our framework.

\begin{figure}[h]
    \centering
    \includegraphics[trim=0 0 0 0,clip,width=0.8\linewidth]{postline_research.pdf}
    \caption{
    Different paradigms of research assistant agents.
    In the existing task-oriented paradigm, each tool has its own agent, and the researcher interacts with multiple agents that are unaware of each other. Under the proposed framework, a single agent operates across all tools to assist the researcher, maintaining continuous memory even when focus shifts between tools.  Each tool implements the non-intelligent \emph{robot} protocol so they can be automated by the agent.
    }
    \label{fig:postline_research}
\end{figure}

\subsection{Emails and the MBox Format}

The MBox format was first implemented in Fifth Edition of Unix and was standardized in RFC4155~\cite{rfc4155}.

Under the proposed framework, agent communication is modeled after email systems, utilizing the MBox format as the conceptual data format for both agent memory and agent communication.  Below is a sample MBox file containing two messages, separated by a blank line followed by a line starting with \texttt{From}.  A separating line starting with \texttt{From} is not considered a part of the message that follows it.

\begin{verbatim}
From alice@example.com Fri Feb 14 14:30:00 2025
From: alice@example.com
To: bob@example.com
Subject: Project Update
Hi Bob,

Just wanted to give you a quick update on the project status.
We're on track to meet our deadlines and the initial test results
look promising.

Best regards,

Alice

From bob@example.com Fri Feb 14 15:45:00 2025
From: bob@example.com
To: alice@example.com
Subject: Re: Project Update
Thanks Alice,

That's great news about the project! Let me know if you need
any additional resources to keep things moving smoothly.

Regards,

Bob
\end{verbatim}

While any message exchange protocol could theoretically serve this purpose, several key advantages make the email -- and specifically the MBox format -- an ideal choice.

First, the MBox format is purely textual, fully compatible with today's language models.  Moreover, it includes protocols for encoding binary data, thereby ensuring extensibility.
Section~\ref{sec:expr-image} provides a demo of handling binary data.

Second, the MBox format is very likely to be well-represented within the training data of today's language models. Email communications, including archived messages stored in MBox files, have been widely used across various domains.  Web forums evolved from early bulletin board systems continue to thrive till early 2020s and use the MBox-like format for display. This widespread exposure allows language models to more naturally understand and interact with messages stored in this format, ensuring a higher degree of model fluency.

Third, email-based communication has demonstrated remarkable success and resilience across industries. Email remains one of the most universally adopted communication protocols, facilitating everything from personal correspondence to enterprise workflows. Its continued relevance, even in the presence of newer communication paradigms, underscores its flexibility and robustness. This historical precedent suggests that the same system is likely to remain adaptive in the presence of uncertain future development of agent engineering.

Fourth, the email ecosystem provides a wealth of extensions that can be directly leveraged for agent communication.
The domain name system, as used in the email address, provides
a natural model for the hierarchical organization of agents.
Established standards for encryption, such as PGP, can be repurposed to ensure secure and verifiable agent
interactions.   In the future,
cryptocurrency could be utilized to facilitate
monetary transactions between agents, supporting their potential economic
activities in a decentralized manner.

An additional advantage of using the email as the basis for agent memory is its potential for seamless personal integration. Individuals can import their existing email archives into the agent platform, effectively creating a digital agent that inherits one's past interactions and adapts based on prior correspondence, a digital me.

\subsection{The Role of Language Models}

Under the proposed framework, language models function as a shared computational resource, akin to fuels or electricity in the physical world. However, unlike electricity, which is uniform in nature, language models exhibit diversity in their capabilities, performance, and cost, making the selection of an appropriate language models at any given moment a scheduling opportunity and a challenge. Agents operating within this framework dynamically decide, potentially by itself, which model to invoke for the next inference.

The volume of agent-involving information exchanges will increase substantially in the future. These messages will contain the latest human insights, ideas, and problem-solving discussions. Since language models require visibility into relevant messages to perform inference, portions of these communications naturally become available as training data for subsequent iterations of model training. Over time, this cyclical process will foster a \emph{symbiotic evolution} between agents and language models.  Model freshness will be increasingly relied upon, so the time towards the next update and the recall of latest knowledge will become two key measurements of language model technology.  This will eventually drive language models from machine learning models to storages of public information.

\subsection{Memory Management and Reproduction}

Agent memory management plays a critical role in system performance. The size of an agent’s memory directly affects inference cost and efficiency. A larger memory size also increases the difficulty for model to attend to relevant information, potentially degrading the quality of the inference output.

Agents can regulate their memory through two primary mechanisms: memory pruning/compression and reproduction by split. The first approach involves selectively filtering or summarizing past messages to retain only the most relevant information. The Memory Segment Rewrite (MSR) primitive provides a method for agents to autonomously modify their own memory, allowing them to refine stored knowledge while maintaining coherence.  Model inference is a special case of MSR, as it is equivalent to rewriting the empty segment at the end of the memory. Because the memory is the agent's program, MSR is a primitive that enables self-modifying programs.

The second mechanism, \emph{reproduction by split}, enables an agent to partition its memory when its memory size grows beyond an optimal threshold and cannot be effectively compressed.  The split can also occur simply when resources allow. By dividing its journal into multiple subsets, an agent can create descendants, each inheriting a portion of its memory while diverging into specialized roles over time. Memory episode clustering might serve as a man-made algorithm to guide the split process, but eventually AI will evolve its own way.

A special case of this process is \emph{cloning}, where an agent duplicates its memory at a specific moment, allowing multiple versions of itself to evolve independently. This facilitates parallel exploration of different strategies, or parallel processing of different tasks.  Cloning oneself to serve as a subordinate for a task eliminates the process of recruit training: your clone understands your needs the best.

By integrating reproduction as a fundamental capability, the proposed framework permits an \emph{evolutionary algorithm} that fosters adaptive optimization. Just as backpropagation revolutionized neural network training, we anticipate that reproduction-driven evolution will serve as a cornerstone of agent software engineering.

\subsection{Unification of Engineering Processes}
\label{sec:unification}

The unification of natural and formal languages by language models allows us to design a unified engineering process that blurs some boundaries that we have been accustomed to and are still using today to divide the world in certain ways. In addition to the boundary between development and deployment stages as we have already discussed, so is the boundary between code and documentation.

Historically, code has been written in programming languages while documentation has been composed in natural language. With the unification of these linguistic modalities, language models can generate both seamlessly, eliminating the need for strict separation. The core product will no longer be static code or documentation but rather memory journals that capture ideas and experiences. Code or documents will be generated just in time, tailored to specific needs and execution contexts. An explicit workflow or computational graph as heavily depended upon in existing agent platforms may not even be necessary, as the agent will be able to dynamically respond to the task or delegate it to other agents according to information stored in the memory.
