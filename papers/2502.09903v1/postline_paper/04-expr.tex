\section{Experiments}

In this section, we report experiments conducted with an early version of Postline. These are the first steps in our engineering efforts, and the functions achieved are neither novel nor complicated. We use these experiments to demonstrate the experience with the proposed framework. All experiments were run on \texttt{gpt-4o}. Raw messages in the agent's journal are displayed with some headers omitted.

The experimental code and logs are available on Github.\\
\texttt{\url{https://github.com/aaalgo/postline_0.1}}

\subsection{Shell Robot}

Integrating the robot that represents the system shell (terminal) was
one of our earliest experiments and opened up many possibilities. The
agent is allowed to send a JSON message to the shell robot, specifying
the command to run. The shell robot executes the command and returns
the stdout and stderr as email attachments. The agent then interprets
the outcome and sends a message back to the user.  In the session
below the agent was asked to figure out the system's storage
configuration.

\small
\begin{verbatim}
From: user1@localdomain
To: ai_30@agents.localdomain
Subject:

You are the middleman AI, which sits between the user and the bash
command line of a recent Ubuntu system.  Both the user and the shell
are represented by email addresses.  You'll receive user input from
the user email address, and you'll send the commands to the email
address shell@localdomain, in a JSON format (detailed below).  After you
get response from the shell, you'll interpret the outcome and send a
message back to the user.

The address shell@localdomain only processes messages of Content-Type
application/json with the following schema:

{
  "prompt": "The prompt to display to the user",
  "command": "echo Hello, world!",
  "confirm": false
}

- prompt: the prompt to display to the user before the command is run
- command: the command to run on the system
- confirm: whether to ask the user to confirm whether to run the
command.  Please confirm for commands that might potentially damage
the system.

From ----------------
From: ai_30@agents.localdomain
To: user1@localdomain
Subject: RE: Command Execution Setup

I am set up to relay commands to the shell and interpret the
responses. Please send me the commands you would like to execute, and
I will handle the rest!

From ----------------
From: user1@localdomain
To: ai_30@agents.localdomain
Subject: 

Run a command to figure out my storage hardware.

From ----------------
From: ai_30@agents.localdomain
To: shell@localdomain
Subject: 
X-Serial: 3

{
  "prompt": "This command will display your storage hardware details.",
  "command": "lsblk",
  "confirm": false
}

... the shell response is ommitted ...
From ----------------
From: ai_30@agents.localdomain
To: user1@localdomain
Subject: Storage Hardware Details
X-Serial: 5

The command to check your storage hardware was successfully executed.
Here are the details:

NAME                      MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
......

If you need further assistance, please let me know!
\end{verbatim}
\normalsize

\subsection{Code Generation and Execution}

The command-line interface is, by itself, a complete programming environment. There was a time when early programmers had access only to a teletype and wrote code directly in the command line using the line-oriented editor \texttt{ed}, and in theory the agent should be able to do the same.

With minimal effort, we succeeded in making the agent generate simple Python and C++ programs, run them, and report the results back to the user, all using only the shell robot. We also succeeded in having the agent fix a few staged, easy bugs.

In particular, we used the agent to modify the Postline code itself and submit a few commits to the Git repository, as can be seen on GitHub.

\subsection{Handling Binary Image Data}
\label{sec:expr-image}

This is a simple experiment to demonstrate the capability of the MBox
format in handling non-text attachments. We ask the agent to generate
a simple image using the \texttt{ImageMagick} tool and then use the
\texttt{base64} command to encode the image in textual base64 format.
Next, we ask the agent to generate an email with the image attached,
conforming to the MBox format. Viewing the image, of course, requires
importing the MBox file into an email client.
Below are the final few message exchanges
of the experiment. The agent initially failed to attach the image
properly, and some in-context training was applied to fix it.  Figure~\ref{fig:attachment} shows the final email as viewed in email client.

\small
\begin{verbatim}
From: ai_30@agents.localdomain
To: user1@localdomain
Subject: Attached Sample Image

Here is the sample image `sample.png` encoded in base64 format:

```
iVBORw0KGgoAAAANSUhEUgAAAGQAAABkAQMAAABKLAcXAAAAIG...
```

You can decode this base64 string to retrieve the original image. If
you need further assistance, just let me know!

From ----------------
From: user1@localdomain
To: ai_30@agents.localdomain

You are not generating an attachment.  Remember you are generating
mbox email in order to communicate with me.  The mbox format allows
the email to carry attachment.  Attach the image in that  way to me.
Try again!

From ----------------
From: ai_30@agents.localdomain
To: user1@localdomain
Subject: Attached Sample Image
Content-Type: multipart/mixed; boundary="----Boundary"

------Boundary
Content-Type: text/plain; charset="utf-8"

Here is the sample image `sample.png` attached.

------Boundary
Content-Type: image/png
Content-Transfer-Encoding: base64
Content-Disposition: attachment; filename="sample.png"

iVBORw0KGgoAAAANSUhEUgAAAGQAAABkAQMAAABKLAcXAAAAIG...
------Boundary--
\end{verbatim}
\normalsize

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{attachment.png}
    \caption{The image attachment as viewed in email client.}
    \label{fig:attachment}
\end{figure}


\subsection{Creating New Agents}

Our platform dynamically creates new agents for non-existent addresses as needed. The command line interface we used to communicate with the agent at that time did not support switching to a new agent, but rather always replied to the sender of the latest message. We overcame this limitation without modifying our code by instructing the agent to send a message to the new agent (or the required one) and have it report back to us. 

We also explored the possibility of recursively cloning a series of agents and achieved limited success. We realized that stricter behavior and resource control are needed to make such flexibility safely applicable without accidentally overwhelming the system.

\subsection{Memory Segment Rewriting}

In the experiment below, the agent is informed about the MSR primitive and is asked to identify a wasteful memory segment to remove. The agent decided to remove the memory of a python code debugging session. The MSR message itself is not visible, as it has taken the position of the removed range. The saving of the context can be observed from the reduction of the \texttt{X-Total-Tokens} header towards the end of the conversation (from 17,727 to 16,919).  Note that \texttt{X-Total-Tokens} is what is involved in generating one message, mainly determined by the context length, and is filled in by the system according to API responses. 

\small
\begin{verbatim}
From: user1@localdomain
To: ai_30@agents.localdomain
Subject: 

Now we are going to test the MSR primitive (memory segment rewriting).
By sending an MSR message to system@localdomain, you can rewrite a
segment of your memory. For the time being this is mainly to remove
the useless content in your memory to reduce inference cost. The
subject of the message should be strictly formatted as "MSR XXX-YYY",
where XXX and YYY are the beginning and ending (inclusive) X-Serial
values. There's no restriction on the content of the email.  When the
operation is applied, the specified range will be replaced with a
message with the content of the MSR email. A confirmation message will
be sent to you afterwards. Before trying the operation, first identify
a wasteful memory range (those with binary data are of top priority)
you would like to remove.  Tell me the range of X-Serials and what's
the memory is about.

From ----------------
From: ai_30@agents.localdomain
To: user1@localdomain
Subject: Identifying Memory Segment for MSR
X-Total-Tokens: 17727

To effectively utilize the MSR primitive, I have identified a memory
range that can be considered for removal, particularly focusing on
binary data:

**X-Serial Range to Remove:** 29-35

**Description of Memory Content:** The specified range includes
content related to the process of encoding and sending a sample image
using base64 encoding.  This portion of the memory is primarily
concerned with binary data manipulation and is now redundant, as the
task has been completed and the image has been delivered successfully.

By removing or summarizing this segment, we can reduce inference costs
and maintain a more efficient memory profile. Please let me know if
this range is acceptable or if there are further actions you need me
to perform.

From ----------------
From: user1@localdomain
To: ai_30@agents.localdomain
Subject: 

Go ahead! Only generate one message at a time; report back to me after
you get the confirmation.

From ----------------
From: system@localdomain
To: ai_30@agents.localdomain
Subject: Re: MSR 29-35

Memory segment rewriting applied.

From ----------------
From: ai_30@agents.localdomain
To: user1@localdomain
Subject: MSR Confirmation Received
X-Total-Tokens: 16919

The MSR operation was successfully executed, and the memory segment
from X-Serial 29 to 35 has been rewritten. This should help in
reducing inference costs by eliminating redundant content.

If you have any additional tasks or need further modifications, please
let me know!
\end{verbatim} \normalsize


\subsection{Failure of Agent Initialization with Bible}

\label{sec:expr-genesis}

After achieving the series of positive experiences as previously described, mostly within a single long conversation with the agent \texttt{ai\_30}, the authors became increasingly uneasy with the growing length of the journal. Conditioned by conventional software engineering practices, we felt an instinctive urge to refine, condense, and package the working ``prompt" into a more compact and elegant, presentable form.

The idea was straightforward: instead of cloning lengthy journals for new agents, we attempted to condense all accumulated knowledge into a single comprehensive document -- a \emph{Bible for Postline Agents} -- to serve as an initialization mechanism. This document was intended to encapsulate all necessary information for new agents, providing a self-contained knowledge base in place of an evolving journal. The agent \texttt{ai\_30} was instructed to organize and dump its memory into this document, with no constraints imposed on the document’s length, and we experimented with different formats, first attaching it as an email attachment and later embedding it directly into the email body.

Despite extensive efforts, including human-crafted amendments to the \emph{Bible}, the approach proved unsuccessful. Agents initialized with the \emph{Bible} invariably failed to reproduce the expected behaviors, exhibiting various forms of degradation. In almost all cases, these agents immediately struggled to produce messages that adhere to our communication protocol, especially the JSON-based protocol for the shell robot.

This failure highlighted a fundamental insight and forced us to rethink in-context learning: the journal is not merely a static repository of knowledge but an active record of evolving interactions. The process of learning is inherently tied to the sequence of experiences, corrections, and adaptations that an agent undergoes. Attempting to distill this into a single document overlooks the role of in-context learning. The experiment reinforced our understanding that agent memory must remain dynamic, preserving the episodic nature of interactions rather than reducing them to a static knowledge artifact.

This experience also shaped our perspective on software engineering with language models, as discussed in Section~\ref{sec:unification}. Unlike formal languages, where equivalent transformations preserve intended effects, prompts do not always exhibit such stability. Even modifications that appear equivalent to humans can lead to unexpected failures in a language model’s behavior. Consequently, we do not recommend manually altering training messages once the desired behavior is achieved, whether for improving presentation or reducing prompt size.
