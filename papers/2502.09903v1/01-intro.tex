\section{Introduction}

\subsection{Large Language Models Are Automata}

The foundation of modern computing was established by early pioneers striving to formalize computation through various theoretical models, with automata theory becoming a central framework \cite{hopcroft1979}. Alan Turing’s introduction of the Turing machine\cite{turing1936} provided a universal model for computation, employing an imaginary tape as a conceptual memory device and programming medium. This focus on theoretical machines and symbol manipulation ultimately culminated in the unification of automata and formal languages, exemplified by Noam Chomsky’s classification of formal languages\cite{chomsky1956}, and firmly established the role of languages in computer science.

Building on this foundation, software engineering evolved as a discipline dedicated to creating higher-level abstractions that enhance programming efficiency and organization. Programming languages progressed from machine code to assembly, structured programming and beyond, each serving as a virtual machine that facilitates the translation of code into executable instructions for a Turing-equivalent system.  The adoption of design patterns and software engineering methodologies provided structured approaches to managing complexity and improving software development workflows. While these methodologies did not extend the fundamental computational capabilities of the Turing machine, they optimized the way computational processes are expressed, organized, and refined.

Large language models, such as ChatGPT, represent a major advancement in artificial intelligence, capable of processing and generating text like humans do across a broad array of topics. Although they operate atop an extensive computational infrastructure and draw upon vast resources, basic language models -- excluding reasoning-augmented models like GPT-o1 -- still largely conform to the original definition of an automaton (Figure~\ref{fig:quote}): they read input tokens, update internal hidden states, and produce output tokens.

A defining feature of large language models is their ability to process both natural and formal languages. Unlike conventional compilers and interpreters which are limited to predefined programming languages, large language models seamlessly handle diverse linguistic structures, spanning human languages, mathematical expressions, and code. Just like an automaton is programmed in the language it accepts, we argue that {\bf a large language model should be programmed in the same languages it processes}, that is, the full range of natural and formal languages. This paradigm shift necessitates rethinking software engineering methodologies to maximize the potential of large language models as a computing platform.

\begin{figure}[h]
    \centering
    \begin{tcolorbox}[width=1.0\linewidth, colframe=black, colback=white, sharp corners]
        A \emph{deterministic finite automaton} consists of:
        \begin{enumerate}
            \item A finite set of \emph{states}, often denoted $Q$.
            \item A finite set of \emph{input symbols}, often denoted $\Sigma$.
            \item A \emph{transition function} that takes as arguments a state and an input symbol and returns a state. The transition function will commonly be denoted $\delta$...
            \item A \emph{start state}, one of the states in $Q$.
            \item A set of \emph{final} or \emph{accepting states} $F$. The set $F$ is a subset of $Q$.
        \end{enumerate}
    \end{tcolorbox}
    \caption{Definition of DFA by Hopcroft, Motwani and Ullman\cite{hopcroft2006}.  With language models, a \emph{symbol} is commonly called a \emph{token} and the \emph{alphabet} the \emph{vocabulary}.}
    \label{fig:quote}
\end{figure}

\subsection{Agent-Oriented Programming}

From the perspective of software engineering as discussed above, language models operate at the abstraction level of finite automata, or programmable virtual machines. While they can produce coherent text and engage in sophisticated interactions, their responses remain bound to token-level transitions. Just as programming methodologies have enabled the growth of software engineering, higher-level abstractions will unlock greater potential in language models. The full scope of language model programming is still unfolding, being developed in ways that are largely unconcious, and overshadowed by the dominant efforts aimed at improve model reasoning through machine learning approaches\cite{openai24b,deepseek25}. Among the various emerging directions, many point to agents as a promising paradigm for organizing and managing language model-based applications\cite{masterman24,wang24,google24}. We too consider agents of foundational importance and advocate for the term \textbf{agent-oriented programming} as a formal methodology, following the tradition of structured, object-oriented, and functional programming.

An agent is a natural progression beyond language models, intended to endow AI systems with more human-like behavior. While existing survey papers often assume the value of agents with minimal explanation, we take an epistemological perspective to underscore why agents are critical in successful application of language models. Language models possess vast amounts of knowledge, but due to the diverse and sometimes contradictory data on which they are trained, these models do not hold a consistent worldview, set of values, or unified objectives, rendering them sometimes too flexible and unreliable for tasks requiring stable perspectives. In essence, an language model functions like an ``average" human who may exhibit contradictory views, rather than a distinct individual with a coherent perspective.

The concept of an agent addresses this limitation by filtering the language model’s vast knowledge to emphasize only the subset relevant to a particular individual. The agent remains aware of the broader knowledge encoded within the model but, in cases of conflicting perspectives, maintains a defined stance, ensuring consistency and reliability.
Because an agent narrows the capacity of an raw language model, it naturally follows that an agent-based  platform must support multiple agents to compensate for the loss of generality in any single agent. The platform must facilitate efficient information exchange between agents, effective conflict resolution, and dynamic adaptation to evolving objectives. The different agents adopt distinct perspectives or roles and collectively leverage the full breadth of the language model's capabilities.

Many agent platforms have been developed.  Google's Vertex AI Agent Builder/Dialogflow~\cite{vertex24} and Microsoft's Copilot Studio\cite{azure24} are two leading enterprise platforms.  There are also popular open-source projects like AutoGen\cite{autogen} and CrewAI\cite{crewai}.  These platforms provide a variety of interfaces and approaches for building and managing agents, but they mostly constrain themselves to a common paradigm:
\begin{itemize}
    \item The user (designer or engineer) has to assume a specific task to be automated.  It is usually also assumed that the same task has to be done repeatedly.
    \item The user has to decompose the task into sub-tasks, each being easy enough to be solved by invoking language models.  These sub-tasks form a computation graph, either explicitly or implicitly.
    \item The user has to specify the prompts for each node of the computation graph.
    \item The end product of agent programming is in the form of a program or an online service that is separated from the design interface.
\end{itemize}
While such an approach does allow solving a plethora of real-life tasks, it inherently limits the flexibility and adaptability of language models.
Given the generality of language models, as previously discussed, we argue that static computation graphs -- or structured approaches in general -- are remnants of traditional software engineering practices from the age of formal computer languages. Relying on such rigid structures risks constraining the full potential of language models and preventing them from achieving more autonomous and contextually aware behaviors.  Specifically, we argue that the existing platforms do not allow the form of in-context learning as we discuss below.

\subsection{Rethinking In-Context Learning}

The rapid advancement of language models has been largely driven by scaling laws~\cite{kaplan20}, where continually increasing model size and training data has yielded steady gains in performance. However, recent evidence~\cite{reuters24} suggests that this strategy may be approaching its limits, as further scaling appears to offer diminishing returns.  
While vast efforts are currently dedicated to refining reasoning algorithms, we argue that the form of training data must also be reexamined. 

One major gap in current training paradigms is the absence of learning through multi-round interactions.
The standard training process, as exemplified by Llama 3\cite{llama24}, is to pre-train a model with a large amount of free text, and then to post-train the model to follow instructions using supervised training.  The post training examples follow a rigid structure, typically in the form of either (input, expected output) pairs for SFT \cite{wei01,sanh21,wang22} or (input, preferred output, rejected output) triplets for DPO (direct preference optimization)\cite{dpo24}. In the case of DPO, the rejected mistakes are not made by the AI being trained but are staged.  While these approaches provide clear guidance on the end goal, they lack the iterative, trial-and-error learning process fundamental to human skill acquisition. Effective training should involve opportunities for an AI model to make mistakes, receive feedback with explanations of why, and adjust its behavior accordingly -- mirroring how humans refine their skills under the guidance of a mentor.

Based on this observation, we propose to redefine \emph{in-context learning}  or \emph{in-context training} as an evolving process occurring through message exchanges that shape the agent's behavior. This approach extends beyond today's common practice of merely inserting a few shot of training samples into the prompt. Instead, the prompt should incorporate memory episodes of dynamic teacher-student interactions. A typical episode involves the presentation of a task, one or more erroneous attempts, the teacher’s correction and explanation to each of those, and ultimately, the correct solution with confirmation.  In our proposed framework, in-context learning occurs naturally, without a rigid division between training and deployment. As the conversation progresses, the process smoothly transitions from training to deployment.  During the agent's subsequent operation, additional in-context samples are picked up whenever outliers trigger human intervention.  

\subsection{Paper Organization}

Section 2 elaborates on the proposed Ann Arbor Architecture. Section 3 presents the design of Postline, our prototype platform. Section 4 presents a series of experiments on an early version of Postline. Section 5 provides further thinkings and concludes the paper.
