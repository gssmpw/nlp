% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% \usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{tcolorbox} % For colored boxes with borders
\usepackage{booktabs} % 用于绘制三线表
\usepackage{ulem} % 用于下划线
\usepackage{caption} % 用于调整表格标题样式
\usepackage{graphicx} % 用于缩放表格
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{bigstrut}
\usepackage{hyperref}
% 设置表格标题样式
% \captionsetup[table]{skip=6pt, font=small, labelfont=bf}



\usepackage{enumitem}

% If the title and author information does not fit in the area allocated, uncomment the following
%
% \setlength\titlebox{<5cm>}
%
% and set <dim> to something 5cm or larger.

% \title{TACT: Tree-based Adaptive Critic for Table Reasoning}
% \title{Table-Critic: Multi-Turn Stepwise Critiquing for Table Reasoning \\ via Tree-Based Multi-Agent Collaboration}
\title{Table-Critic: A Multi-Agent Framework for \\Collaborative Criticism and Refinement in Table Reasoning}



% Table-Critic: A Multi-Agent Framework for Collaborative Table Reasoning

% Table-Critic: A Multi-Agent Framework for Collaborative criticism and refinement in Table Reasoning

% \title{Table-Critic: Multi-Turn Stepwise Critic \\ with Dynamic Template Tree for Table Reasoning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Peiying Yu \\
%   Natural Language Processing Lab \\ Soochow University \\
%   \texttt{peiying.yu.chn@gmail.com} \\\And
%   Guoxin Chen \\
%   Institute of Computing Technology \\
%   Chinese Academy of Sciences \\
%   \texttt{email@domain} \\\And
%   Jingjing Wang \\
%   Natural Language Processing Lab \\ Soochow University \\
%   \texttt{djingwang@suda.edu.cn} \\}

\author{
 \textbf{Peiying Yu\textsuperscript{1}},
 \textbf{Guoxin Chen\textsuperscript{2,\textsection}},
 \textbf{Jingjing Wang\textsuperscript{1,\textsection}}
%  \textbf{Fourth Author\textsuperscript{1}},
% \\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
% \\
\\
 \textsuperscript{1}Natural Language Processing Lab, Soochow University \\
 \textsuperscript{2}Institute of Computing Technology, Chinese Academy of Sciences
 % \textsuperscript{3}Affiliation 3,
 % \textsuperscript{4}Affiliation 4,
 % \textsuperscript{5}Affiliation 5
\\
 % \small{
 %   \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
 % }
 % \textsuperscript{\textsection}Corresponding authors:
 \texttt{\{pyyu@stu,djingwang@\}suda.edu.cn}\quad
 % \texttt{pyyu@stu.suda.edu.cn} \quad \texttt{djingwang@suda.edu.cn}\\
 \texttt{chenguoxin22@mails.ucas.ac.cn}
}

\begin{document}
\maketitle
\renewcommand{\thefootnote}{\textsection}
\footnotetext{Corresponding authors}
\begin{abstract}
Despite the remarkable capabilities of large language models (LLMs) in various reasoning tasks, they still struggle with table reasoning tasks, particularly in maintaining consistency throughout multi-step reasoning processes.
While existing approaches have explored various decomposition strategies, they often lack effective mechanisms to identify and correct errors in intermediate reasoning steps, leading to cascading error propagation.
To address these issues, we propose Table-Critic, a novel multi-agent framework that facilitates  collaborative criticism and iterative refinement of the reasoning process until convergence to correct solutions. Our framework consists of four specialized agents: a Judge for error identification, a Critic for comprehensive critiques, a Refiner for process improvement, and a Curator for pattern distillation.
To effectively deal with diverse and unpredictable error types, we introduce a self-evolving template tree that systematically accumulates critique knowledge through experience-driven learning and guides future reflections.
Extensive experiments have demonstrated that Table-Critic achieves substantial improvements over existing methods, achieving superior accuracy and error correction rates while maintaining computational efficiency and lower solution degradation rate.

% Moreover, our empirical analysis reveals that LLMs, while proficient in identifying initial errors, tend to introduce new mistakes in subsequent steps, particularly in handling complex reasoning tasks.


% Table reasoning requires handling both unstructured questions and semi-structured tabular data, making it more challenging than general reasoning tasks. Existing methods simplify tasks through decomposition but fail to effectively critique and refine intermediate steps, leading to error propagation. To address this, we propose Table-Critic, a novel multi-agent framework that collaboratively identifies, critiques, and refines reasoning errors. Table-Critic consists of four specialized agents: a \textit{Judge} for error identification, a \textit{Critic} for stepwise feedback, a \textit{Refiner} for iterative reasoning improvements, and a \textit{Curator} that maintains a self-evolving template tree to systematically address diverse error types. The framework employs a multi-turn refinement strategy to ensure reasoning accuracy and adapts dynamically through accumulated critique knowledge. 
% Extensive experiments on the WikiTQ and TabFact datasets demonstrate that Table-Critic significantly outperforms state-of-the-art methods, achieving a notable 8.2\% improvement in accuracy on WikiTQ.
% By addressing intermediate reasoning errors, Table-Critic establishes a new standard for accurate and interpretable table reasoning.
% Extensive experiments on WikiTQ and TabFact demonstrate that Table-Critic outperforms state-of-the-art methods by 8.9% in accuracy, establishing a robust paradigm for accurate, interpretable, and iterative table reasoning.
\end{abstract}
% \begin{abstract}
% Compared to generic reasoning, table reasoning requires handling both unstructured questions and semi-structured tabular data.
% Existing methods often simplify task complexity by decomposing questions or tables into smaller subcomponents.
% However, these methods lack an effective mechanism to critique and refine intermediate reasoning steps, leading to error accumulation.
% To address these challenges, we introduce Table-Critic, a multi-turn, stepwise critic framework that incorporates a dynamic template tree for systematic error identification and refinement. 
% Table-Critic consists of four key components: a \textit{Judge} to verify the correctness of predicted answers, a dynamically updated \textit{Template Tree} to provide context-aware critique templates, a \textit{Critic} to analyze reasoning steps step-by-step, and a \textit{Refiner} to iteratively improve reasoning based on identified errors. 
% This framework operates in a multi-turn manner until the predicted answer is deemed correct.
% Extensive experiments on the WikiTQ and TabFact datasets demonstrate that Table-Critic significantly outperforms state-of-the-art methods, achieving a notable 8.9\% improvement in accuracy on WikiTQ. By addressing intermediate reasoning errors, Table-Critic establishes a new standard for accurate and interpretable table reasoning.
% \end{abstract}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{framework.pdf}
  \caption{An illustration of Table-Critic, a multi-agent framework for table reasoning tasks, where the Judge identifies errors, the Critic provides detailed critique, the Refiner corrects the reasoning process, and the Curator updates a self-evolving template tree to accumulate critique knowledge and improve future performance.}
  \label{fig:framework}
\end{figure*}

\section{Introduction}
Despite significant advances in various reasoning tasks~\citep{plaat2024reasoning,yu2024natural,alphamath,chen-etal-2024-step,guo2025deepseek}, large language models (LLMs)~\citep{yang2024qwen2,llama3,claude,gemma,gpt_4o} face substantial challenges in handling semi-structured data, such as table reasoning tasks, as they require both understanding of tabular structures and precise localization of relevant entries in redundant and noisy information~\citep{zhao2024tapera,chen2024tablerag,zhang2025survey}.

% While large language models (LLMs) excel in various reasoning tasks \citep{alphamath, plaat2024reasoning}, they still face significant challenges in table reasoning due to the complexity of processing unstructured questions alongside semi-structured tabular data \citep{ye2023large, zhang2025survey}.


Existing approaches address these challenges through various decomposition strategies. 
For example, Binder~\citep{cheng2022binding} decomposes complex questions into executable sub-programs (i.e., SQL or Python), while approaches such as Dater~\citep{ye2023large} and Chain-of-Table~\citep{wang2024chain} focus on dynamic table decomposition for context-aware reasoning.
Although these decomposition-based methods have demonstrated promising performance, they suffer from a critical limitation: the lack of effective mechanisms to criticize and refine the intermediate reasoning steps.
This deficiency inevitably leads to error propagation throughout the reasoning process, significantly affecting the accuracy of final predictions.

% Existing methods \citep{cheng2022binding, ni2023lever, ye2023large, wang2024chain} address this challenge by decomposing questions or tables into smaller subcomponents to reduce task complexity.
% For example, Binder \citep{cheng2022binding} and LEVER \citep{ni2023lever} decompose complex questions into sub-questions to execute structured programs, such as SQL or Python. However, such approaches struggle with intricate tabular structures, as LLMs often fail to reason effectively over static tables without dynamic adaptation.
% To overcome this limitation, methods like Dater \citep{ye2023large} and Chain-of-Table \citep{wang2024chain} dynamically decompose tables, allowing for more focused and context-aware reasoning. While these methods achieve notable improvements, they lack a robust mechanism for evaluating and refining intermediate reasoning steps. This shortfall often results in the accumulation of errors, ultimately impacting the predicted answer's accuracy.

However, recent studies~\citep{MadaanTGHGW0DPY23,abs-2412-19513} have revealed that while LLMs possess self-reflection capabilities to some extent, their self-reflection often lacks reliability and consistency.
Simply forcing LLMs to engage in self-reflection may introduce additional biases, especially in table reasoning tasks, wherein models tend to either rationalize their previous erroneous reasoning or over-criticize correct steps, rather than identifying genuine errors~\citep{critic-cot,chen2025learning}.

To address these issues, we propose Table-Critic, a multi-agent framework that introduces specialized agents to collaboratively criticize and refine the reasoning process in a step-by-step manner.
Specifically, our Table-Critic simulates human-like reflective behaviors through four targeted agents: a Judge that identifies potential errors, a Critic that provides detailed suggestions, a Refiner that refines the entire reasoning process, and a Curator that distills critique patterns to guide future reflection.
The collaborative strategy among multiple agents is motivated by our two insights:
\textbf{(1) LLMs demonstrate proficiency in identifying and refining the first erroneous steps, yet tend to make other mistakes in subsequent steps, particularly when dealing with complex problems.}
This observation motivates our multi-turn design where different agents continuously monitor and refine the reasoning process until the Judge verifies its correctness.
\textbf{(2) The diversity and unpredictability of error types in the reasoning process make it challenging for LLMs to effectively identify them based solely on their inherent knowledge.}
This insight motivates the development of a dynamic template repository that categorizes and stores critique templates by error types, allowing our multi-agent system to systematically accumulate critique knowledge.
Specifically, the Curator maintains a self-evolving template tree by expanding branches or adding templates after the entire reflection, while the Judge routes through the tree based on the identified reasoning errors to locate appropriate templates for assisting the Critic in generating high-quality critique, thereby facilitating subsequent refinement.
Through this self-evolving template tree mechanism, our system continuously accumulates and distills critique patterns from previous experiences, enabling more effective error identification beyond LLMs' inherent capabilities.
This experience-driven approach ensures continuous improvement in the quality and consistency.

% Through our self-evolving template tree, each round benefits from the accumulated experience of previous iterations, enabling our system to better identify reasoning errors beyond LLMs' inherent knowledge.
% Through this self-evolving template tree mechanism, our system continuously accumulates and leverages critique patterns from previous iterations, enabling more effective error identification beyond LLMs' inherent capabilities. This experience-driven learning process not only enhances the system's ability to detect diverse error types but also ensures consistent improvement in reasoning quality across different tasks.


\textbf{Our contributions} are summarized as follows:
\begin{itemize}[topsep=1pt, partopsep=1pt, leftmargin=12pt, itemsep=-1pt]
    \item We introduce Table-Critic, a novel multi-agent framework where specialized agents collaboratively criticize and refine the reasoning process for complex table reasoning tasks.
    \item We design a multi-turn refinement mechanism where different agents continuously monitor and improve the reasoning process, effectively mitigating error propagation in multi-step reasoning.
    \item We introduce a self-evolving template tree that systematically accumulates and organizes critique knowledge, enabling our system to effectively handle emerging error types through experience-driven learning.
    \item Extensive experiments demonstrate that Table-Critic significantly outperforms existing methods and exhibits substantial advantages over majority voting under comparable or even superior computational costs.
\end{itemize}

% To address these issues, we propose Table-Critic, a multi-turn framework for table reasoning that introduces a dynamic, stepwise critique mechanism. 
% Existing LLMs lack the capabilities of critic \citep{luo2023critique, lin2024criticbench}, so Table-Critic adopts the dynamically updated template tree---a tree-based critic knowledge base---to offer specific guidance for the critic.
% Additionally, most critic methods focus on evaluating the final answer without offering critical insights into the intermediate steps of reasoning \citep{zheng2023judging, yan2024predicting, xu2024perfect}.
% Table-Critic provides step-level critiques, evaluating the reasoning process step-by-step.
% As effective critique requires iterative refinement rather than a one-step solution, Table-Critic utilizes a multi-turn framework to systematically address reasoning complexities.




% As illustrated in Figure~\ref{framework}, Table-Critic operates through four interconnected components that systematically refine the reasoning process. 
% First, the \textit{Judge} evaluates the correctness of the predicted answer based on the reasoning steps and provides an assessment. 
% If the answer is incorrect, the \textit{Template Tree} identifies the error type by analyzing the reasoning steps and retrieves corresponding critique templates to guide the critique process. 
% Subsequently, the \textit{Critic} pinpoints the specific step in the reasoning chain where the error occurs, leveraging the retrieved template to provide detailed analysis. 
% Finally, the \textit{Refiner} utilizes the critique to update the reasoning steps starting from the identified error step, iterating until the correct answer is produced. 
% This multi-turn refinement mechanism ensures effective error detection, targeted corrections, and significant performance improvements in table reasoning tasks.



% Our experiments demonstrate the efficacy of Table-Critic across multiple benchmarks. On the WikiTQ dataset, Table-Critic achieves an 8.9\% improvement over state-of-the-art methods, highlighting its ability to address the limitations of existing approaches. These results validate the importance of stepwise critique mechanisms in enhancing reasoning performance.



% Our contributions are summarized as follows:
% \begin{itemize}
% \item \textbf{Extending Critic to Table Reasoning:} Table-Critic pioneers the application of a critic mechanism in the domain of table reasoning, improving the accuracy and interpretability of answers generated from complex tabular data.
 
% \item \textbf{Introducing a Multi-Turn Stepwise Critic:} Unlike traditional critics critiquing only on final answer, Table-Critic incorporates a multi-turn stepwise framework that critiques the intermediate reasoning process step-by-step in a multi-turn manner. 

% \item \textbf{Leveraging Dynamic Template Tree:} We design a dynamically updated template tree that provides targeted and interpretable critiques, enabling more effective handling of semi-structured data. 

% \end{itemize}


\section{Related Work}
\textbf{Table Reasoning.}
Table reasoning, which requires joint understanding of semi-structured tables and questions, has evolved through several paradigms. Early approaches focused on developing specialized models through fine-tuning~\citep{yin2020tabert,liu2021tapex,gu2022pasta}, while recent work has shifted towards leveraging large language models (LLMs) in few-shot learning~\citep{chen2024tablerag,zhao2024tapera}. To handle complex reasoning tasks, decomposition-based methods have emerged as a promising direction. These methods break down complex tasks into manageable steps, either through program execution~\citep{cheng2022binding} or context-aware table partitioning~\citep{ye2023large,wang2024chain}. However, a critical limitation of existing approaches is their inability to effectively critique and refine intermediate reasoning steps, leading to error propagation.
In contrast, our Table-Critic framework addresses this limitation by introducing systematic critique and refinement mechanisms throughout the reasoning process.

% Table reasoning involves reasoning over semi-structured tables and unstructured questions. Early approaches focused on fine-tuning specialized models~\citep{yin2020tabert, liu2021tapex, gu2022pasta}, while recent work leverages LLMs in few-shot settings~\citep{chen2024tablerag, zhao2024tapera}. Decomposition-based methods, such as Binder~\citep{cheng2022binding}, Dater~\citep{ye2023large}, and Chain-of-Table~\citep{wang2024chain}, have shown promise by breaking down tasks into manageable steps, such as program execution or context-aware table partitioning. However, these methods lack mechanisms to critique and refine intermediate steps, leading to error propagation. Our method, Table-Critic, addresses this gap by introducing stepwise critique and refinement to improve reasoning accuracy.

\textbf{Self-Reflection.}
Recent studies have revealed that while LLMs possess inherent self-reflection capabilities, they often suffer from reliability and consistency issues~\citep{MadaanTGHGW0DPY23,abs-2412-19513}. Simply enforcing self-reflection can be counterproductive, as models tend to either rationalize their errors or excessively critique correct reasoning steps~\citep{critic-cot,chen2025learning}. To address these limitations, our Table-Critic introduces a structured approach through: (1) a multi-agent framework where specialized agents collaborate to provide targeted critiques, and (2) a self-evolving template tree that systematically accumulates and organizes critique knowledge. This design effectively overcomes the inherent limitations of LLMs' reflection capabilities while maintaining reliable and consistent error identification.


% LLMs possess some self-reflection capabilities, but their critiques often lack reliability and consistency~\citep{MadaanTGHGW0DPY23, abs-2412-19513}. Enforcing self-reflection can introduce biases, such as rationalizing errors or over-criticizing correct reasoning~\citep{critic-cot, chen2025learning}. To tackle these issues, Table-Critic employs a multi-agent framework that iteratively critiques reasoning steps using specialized agents and a self-evolving template tree. This approach systematically refines errors and accumulates critique knowledge, offering a novel solution for robust table reasoning.

% \textbf{Multi-agent Systems.}
% Multi-agent systems have recently demonstrated promising potential in complex reasoning tasks by enabling collaborative problem-solving through specialized agents~\citep{GuoCWCPCW024,pmlr-v235-zhang24au}. While these systems have been explored in various domains, their application to table reasoning remains unexplored. Our work presents the first attempt to leverage multi-agent collaboration for table reasoning, where specialized agents work together to critique and refine the reasoning process.


% \subsection{Table Reasoning}
% Table reasoning tasks involve reasoning over semi-structured tables and unstructured questions, with research evolving from fine-tuning specialized models \citep{yin2020tabert, liu2021tapex, gu2022pasta} to leveraging LLMs in few-shot settings \citep{chen2022large, liu2023rethinking, chen2024tablerag, zhao2024tapera}. To address the inherent complexity of these tasks, decomposition strategies have gained traction. For example, Binder \citep{cheng2022binding} decompose complex questions into executable sub-programs, such as SQL or Python, enabling finer-grained reasoning. Similarly, Dater \citep{ye2023large} and Chain-of-Table \citep{wang2024chain} dynamically decompose tables using context-aware strategies, enhancing reasoning over intricate tabular data. While these decomposition-based methods have achieved promising results by simplifying reasoning tasks, they lack mechanisms for critically evaluating and refining intermediate steps, leading to error propagation and reduced accuracy. To address this, our method, Table-Critic, introduces a stepwise critic mechanism that dynamically assesses and refines intermediate reasoning steps, mitigating error accumulation and improving overall performance.


% Table reasoning tasks require the ability to reason over both unstructured questions and semi-structured tables. Research in this area has evolved from fine-tuning specialized models \citep{yin2020tabert, liu2021tapex, gu2022pasta} to leveraging LLMs in few-shot settings \citep{chen2022large, liu2023rethinking, chen2024tablerag, zhao2024tapera}, exploiting the growing reasoning capabilities of these models.

% \textbf{Question Decomposition.} A prominent approach to table reasoning is decomposing complex questions into simpler, more manageable sub-questions. Binder \citep{cheng2022binding} generates SQL or Python programs that decompose a primary question into multiple sub-questions and modifies the program statements by calling LLMs as APIs. Similarly, LEVER \citep{ni2023lever} also decomposes natural language question into runnable SQL programs and trains code LLMs to verify the programs correctness.
% Decomposing questions into smaller sub-questions has proven effective in addressing complex reasoning challenges. However, these methods struggle with difficult cases involving intricate tables, primarily due to the limitations of the single-pass generation process, where LLMs lack the ability to reason over a static table without dynamic adaptation \citep{wang2024chain}.

% \textbf{Table Decomposition.} To address the limitations of processing entire tables, many multi-step reasoning frameworks that transform the tables according to the given question have been proposed \citep{ye2023large, wang2024chain, abhyankar2024h}. Dater \citep{ye2023large} decomposes both tables and questions using a "parsing-execution-filling" strategy. Chain-of-Table \citep{wang2024chain} utilizes a larger set of generic table operations and dynamically generates tables to solve the problems.
% % However, while dynamic decomposition has shown promise, these methods often lack a mechanism for evaluating the intermediate reasoning steps. This limitation may lead to error accumulation or poor generalization.
% While dynamic decomposition enhances the reasoning process, these methods lack mechanisms to evaluate and refine intermediate reasoning steps. Without such evaluation, errors in intermediate steps can accumulate, ultimately undermining the model's overall performance and generalization capabilities.

% Our approach, Table-Critic, introduces a stepwise critic mechanism to the table reasoning tasks, integrating a dynamically updated template tree to critically assess the intermediate steps of reasoning.

% \subsection{Critic}
% Recent studies have investigated the self-reflection capabilities of large language models (LLMs) and their ability to critique their own outputs. While LLMs have demonstrated some potential for self-reflection, it has been observed that their self-assessments often lack reliability and consistency \citep{MadaanTGHGW0DPY23,abs-2412-19513}. Simply enforcing self-reflection during reasoning tasks can introduce new challenges, such as rationalizing incorrect reasoning or over-criticizing correct steps, particularly in structured tasks like table reasoning \citep{critic-cot,chen2025learning}. These limitations highlight the need for more robust mechanisms to improve reasoning performance.
% To the best of our knowledge, our work is the first to extend the critique framework to table reasoning tasks in a step-by-step manner. By iteratively applying the critique process at each reasoning stage and incorporating multi-turn interactions, our approach enables more granular error detection and more effective evaluation of intermediate reasoning steps, addressing key limitations in prior works.
% LLMs have been proposed as cost-effective alternatives to human critics \citep{scheurer2023training}, capable of discriminating and criticizing the model outputs in a text-generation manner \citep{luo2023critique, zeng2023challenge, renze2024self}. These methods often first provide explanations for critiques of the output, then return a discrete score or preference label as the prediction \citep{zheng2023judging, yan2024predicting, xu2024perfect}. While effective in critiquing text generation, these methods largely focus on evaluating final outputs, without exploring intermediate reasoning steps or handling semi-structured data, such as tables. 

% To the best of our knowledge, we are the first to extend the concept of a step-by-step critic to table reasoning tasks. Our approach iteratively applies the critique process at each reasoning stage and integrates multi-turn interactions. This allows for more granular error detection and effective evaluation of mistakes made during reasoning steps.


% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=\textwidth]{method.pdf}
%   \caption{a) Illustration of the Table-Critic framework: Table-Critic consists of four components—Judge, Template Tree, Critic, and Refiner—that collaboratively refine reasoning steps and predicted answers for table-based questions through error identification and critique generation. b) Dynamic Expansion of Template Tree: The Template Tree undergoes dynamic updates through direct, vertical, and horizontal expansions to enhance the reasoning process.}
%   \label{method}
% \end{figure*}

% Self-critics have emerged as crucial mechanisms for enhancing the textual reasoning capabilities of LLMs, yet their potential for table reasoning with semi-structured data remains largely unexplored. 
% Moreover, existing critics are mainly experience-driven, often critiquing answers directly without relying on a solid theoretical foundation. 

% \begin{algorithm}
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}
% \caption{Table-Critic}
% \label{alg1}
% \begin{algorithmic}[1]
% \REQUIRE \fontsize{10}{15}\selectfont $T, Q, R,P$
% \STATE \fontsize{10}{13}\selectfont $judge \leftarrow \texttt{Judge}(T, Q, P)$
% \WHILE{\fontsize{10}{13}\selectfont $judge$ is incorrect}
% \STATE \fontsize{10}{13}\selectfont $templates \leftarrow \texttt{TemplateTree}(T, Q, R, P)$
% \STATE \fontsize{10}{13}\selectfont $critique, i \leftarrow \texttt{Critic}(templates, T, Q, R, P)$
% \STATE \fontsize{10}{13}\selectfont $R \leftarrow R[1:i-1]$
% \STATE \fontsize{10}{13}\selectfont $R, P \leftarrow \texttt{Refiner}(critique, T, Q, R)$
% \STATE \fontsize{10}{13}\selectfont $judge \leftarrow \texttt{Judge}(T, Q, P)$
% \ENDWHILE
% \STATE \fontsize{10}{13}\selectfont Update \texttt{TemplateTree}
% \ENSURE \fontsize{10}{15}\selectfont $P$
% \end{algorithmic}
% \end{algorithm}





% \begin{algorithm*}
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}
% \caption{Table-Critic}
% \label{alg1}
% \begin{algorithmic}[1]
% \REQUIRE $T, Q, R, P$ \hfill {\small $\triangleright$ Table $T$, Question $Q$, Reasoning steps $R$, Predicted answer $P$}
% \STATE $judge \leftarrow \texttt{Judge}(T, Q, P)$ \hfill {\small $\triangleright$ Determine whether the $P$ is correct}
% \WHILE{$judge$ is incorrect}
% \STATE $template \leftarrow \texttt{TemplateTree}(T, Q, R, P)$ \hfill {\small $\triangleright$ Generate critique templates}
% \STATE $critique, i \leftarrow \texttt{Critic}(template, T, Q, R, P)$ \hfill {\small $\triangleright$ Generate stepwise critique and}
% \STATE \hfill {\small the first error step number $i$} 
% % \\ \hfill {\small $\triangleright$ based on the template}
% \STATE $R \leftarrow R[1:i-1]$ \hfill {\small $\triangleright$ Retain the correct steps from the previous $i-1$ steps}
% \STATE $R, P \leftarrow \texttt{Refiner}(critique, T, Q, R)$ \hfill {\small $\triangleright$ Based on the critique, update the reasoning steps $R$ }
% \\ \hfill {\small and the predicted answer $P$ from the first error step}
% \STATE $judge \leftarrow \texttt{Judge}(T, Q, P)$ \hfill {\small $\triangleright$ Determine again whether the updated answer $P$ is correct}
% \ENDWHILE
% \STATE Update \texttt{TemplateTree}
% \ENSURE $P$ \hfill {\small $\triangleright$ Output the refined answer $P$}
% \end{algorithmic}
% \end{algorithm*}

\section{Table-Critic}
\subsection{Overview}
To effectively implement the human-like correction process in multi-step reasoning, we propose a collaborative multi-agent framework, Table-Critic. As illustrated in Figure~\ref{fig:framework}, this framework decomposes the complex reasoning refinement task into four specialized functions: error detection (Judge), critique generation (Critic), reasoning refinement (Refiner), and experience learning (Curator). These agents work in concert to progressively improve reasoning quality while accumulating valuable correction experiences.
% In this section, we present Table-Critic, a multi-agent framework designed to enhance the reliability of table reasoning through collaborative reflection and refinement. As illustrated in Figure~\ref{fig:framework}, our framework operates through multiple specialized agents that work in concert to simulate human-like correction behavior.
Specifically, given a table $\mathbb{T}$ and a question $q$, these agents iteratively refine the initial reasoning chain $\tau = \{s_1, s_2, ..., s_n\}$ until reaching a satisfactory solution.\footnote{We use Chain-of-Table~\citep{wang2024chain} for initial chains, though our framework is applicable to other methods.} 
The refinement process is guided by a self-evolving template tree $\mathcal{T}$ that systematically accumulates critique patterns from past experiences.

\subsection{Multiple Agents}
Inspired by human-like correction behavior, we design four specialized agents---Judge, Critic, Refiner, and Curator---to facilitate criticizing and refining in multi-step reasoning.
We use specific instructions to prompt LLM ($\pi$) to execute the corresponding operations.
Formally, we define each agent as follows:

\textbf{Judge ($\mathcal{A}^j$).}
The Judge agent is responsible for identifying potential errors in the reasoning process.
Given a table $\mathbb{T}$, question $q$, current reasoning chain $\tau$, and the template tree $\mathcal{T}$, it analyzes each reasoning step and determines the specific error type if any exists. Based on the identified error type (if exists), the Judge routes through the template tree $\mathcal{T}$ to locate appropriate templates for guiding the subsequent critic agent.
Formally, the Judge agent operates as:
\begin{equation}
    E, P, R = \pi(\mathbb{T},q,\tau,\mathcal{T},\text{instruction}^{\mathcal{A}^j}),
\end{equation}
where $E$ denotes the error analysis for each reasoning step, $P \in \{\text{Correct}, \text{Incorrect}\}$ indicates the overall reasoning status, and $R$ represents the routing path in the template tree that guides template selection.
% Specifically, the Judge agent organizes the response in the following format:
% \begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
% \textbf{Thought:} $<$Analysis of each step and identify the error type (if exists)$>$\\
% \textbf{Reasoning Status:} Correct (or Incorrect)\\
% \textbf{Router:} $<$Routing path in template tree$>$
% \end{tcolorbox}
Based on the routing path, we sample relevant critique templates $\mathcal{T}_s$ from the template tree $\mathcal{T}$ to guide the Critic agent in generating targeted and high-quality critiques for the identified errors.
Notably, due to the self-evolving nature of our template tree, when the Judge identifies an error type not yet present in the tree, we randomly sample various error types from existing templates to guide the Critic in generating helpful critique.

\textbf{Critic ($\mathcal{A}^c$).}
The Critic agent serves as a crucial component in our framework, responsible for generating detailed and constructive critiques for the identified errors. With the guidance of sampled critique templates $\mathcal{T}_s$, the Critic agent locates the first error step in the reasoning chain $\tau$, analyzes error details, and provides specific suggestions for subsequent refinement. Formally, the Critic agent operates as:
\begin{equation}
    \mathcal{C}, I = \pi(\mathbb{T}, q, \tau, \mathcal{T}_s, \text{instruction}^{\mathcal{A}^c}),
\end{equation}
where $\mathcal{C}$ denotes the generated critique and $I$ indicates the index of the first error step in $\tau$.
The effectiveness of the Critic agent directly impacts the Refiner's ability to correct reasoning errors, which motivates our design of the template tree to enhance critique quality.

\textbf{Refiner ($\mathcal{A}^r$).}
The Refiner agent is tasked with correcting the reasoning chain based on the critique provided by the Critic. Given the critique $\mathcal{C}$, the table $\mathbb{T}$, question $q$, and the partial reasoning chain up to the first error step (i.e., $\tau_p=\{s_1, ..., s_I\}$), the Refiner first rectifies the identified error and then completes the remaining reasoning steps to generate a full refined chain.
Formally, the Refiner agent operates as:
\begin{equation}
    \tau' = \pi(\mathbb{T}, q, \tau_p, \mathcal{C},  \text{instruction}^{\mathcal{A}^r}),
\end{equation}
where $\tau'$ represents the newly generated complete reasoning chain.

\textbf{Curator ($\mathcal{A}^{cu}$).}
The Curator agent serves as an experience-driven learning component that distills valuable critique templates from current refinement processes.
It is activated only after the complete refinement process concludes, specifically when the Judge agent verifies that the final reasoning chain is error-free ($P=\text{Correct}$), as shown in Figure~\ref{fig:framework}.
Through reviewing each refinement iteration and the existing template tree $\mathcal{T}$, the Curator autonomously distills meaningful critique templates from effective refinement experiences. 
These newly distilled templates are then incorporated into $\mathcal{T}$ to enhance future critique generation. 
Formally, the Curator operates as:
\begin{equation}
    \mathcal{T}' = \pi(\mathcal{T}, H, \text{instruction}^{\mathcal{A}^{cu}}),
\end{equation}
where $H$ represents the complete refinement history, and $\mathcal{T}'$ denotes the updated template tree.
The detailed update strategy will be delineated in subsequent sections.

\subsection{Multi-turn Refinement}
As discussed in the Introduction, the multi-turn refinement in Table-Critic is motivated by our observation that LLMs often excel at identifying and correcting the first error in reasoning chains, but may introduce new errors in subsequent steps. To address this challenge, we implement an iterative refinement process where multiple agents collaboratively monitor and improve the reasoning chain until reaching a satisfactory solution.

Specifically, given an initial reasoning chain $\tau$, our framework operates through the following steps in each iteration:
(1) The Judge agent first analyzes the entire reasoning chain to identify potential errors and determine their types. If no errors are detected ($P=\text{Correct}$), the process terminates. Otherwise, the Judge routes through the template tree to locate relevant critique templates.
(2) With the guidance of sampled templates $\mathcal{T}_s$, the Critic agent generates detailed critiques $\mathcal{C}$ focusing on the first identified error at step $I$. This strategy ensures that each refinement iteration addresses errors sequentially, preventing the introduction of cascading errors.
(3) The Refiner agent then generates a new reasoning chain $\tau'$ by incorporating the critique. Importantly, the Refiner only receives the partial chain $\tau_p$ up to the error step $I$, forcing it to reconstruct the remaining steps with the help of critique. This design prevents the Refiner from being biased by previous erroneous chain.
(4) The above process continues iteratively until one of the following conditions is met: the Judge determines the current reasoning chain is correct ($P=\text{Correct}$) or the maximum number of iterations $K$ is reached.

Through this multi-turn design, Table-Critic effectively manages the complexity of multi-step reasoning refinement while maintaining the quality of each correction step. The iterative nature of our approach, combined with specialized agent roles and strategic process control, enables robust and efficient reasoning improvement.

% In table reasoning tasks, each instance is represented as $(T, Q, A)$, where $T$ is the table, $Q$ is the question or statement related to the table, and $A$ is the corresponding gold answer. Specifically, in table-based question answering (TQA), $Q$ represents a question, and $A$ is the expected answer in natural language. In table-based fact verification (TFV), $Q$ is a statement about the table contents, while $A \in \{True, False\}$ is a Boolean value indicating the correctness of the statement.

% Given $T$ and $Q$, we leverage Chain-of-Table \citep{wang2024chain} to generate an initial sequence of reasoning steps $R = [r_1, r_2, ..., r_n]$ and a predicted answer $P$:

% \begin{equation}
%     R, P = \mathcal{M}(T, Q)
% \end{equation}

% where $\mathcal{M}$ represents the reasoning method used to derive the initial steps and answer. Table-Critic critiques and refines $R$ and $P$ iteratively, aiming to improve the reasoning process until the final predicted answer is correct.
% % :

% % \begin{equation}
% %     P^* = \underset{P}{\arg\max} \, \mathbb{I}(P = A)
% % \end{equation}

% % where $\mathbb{I}(\cdot)$ is the indicator function that evaluates whether $P$ matches the gold answer $A$.

% \subsection{Overview}

% Table-Critic is designed to iteratively criticize and refine reasoning steps and the predicted answer for table reasoning tasks. As illustrated in Figure \ref{method}.a, given an input table $T$, a question $Q$, initial reasoning steps $R$, and a predicted answer $P$, Table-Critic first verifies the correctness of $P$ using the \textit{Judge}. If $P$ is incorrect, Table-Critic proceeds to generate corresponding critique templates through the \textit{Template Tree}. These templates guide the critique generation process, which is performed by the \textit{Critic}. The critique provides feedback on the reasoning process and identifies the ffirst erroneous step $r_i$.

% Subsequently, Table-Critic retains the correct reasoning steps from $r_1$ to $r_{i-1}$ and refines the reasoning process from $r_i$ onward, incorporating the critique feedback. Using the \textit{Refiner}, the reasoning steps and the predicted answer are updated. The \textit{Judge} then reassesses the new predicted answer. This iterative refinement continues until the predicted answer is deemed correct. Once the process is successfully completed, the critique is stored in the \textit{Template Tree} for future reference.

% The final output of Table-Critic is the improved predicted answer $P$, which is expected to be the correct solution to the table reasoning task. By iterating through this multi-step refinement process, Table-Critic effectively improves the quality of the answer by systematically addressing errors in reasoning.

% Table-Critic is designed to iteratively criticize and refine reasoning steps and the predicted answer for table reasoning tasks. As illustrated in Algorithm~\ref{alg1}, given an input table $T$, a question $Q$, initial reasoning steps $R$, and a predicted answer $P$, Table-Critic begins by evaluating the correctness of the answer through the function \texttt{Judge} (Line 1). If the answer is incorrect, Table-Critic proceeds to generate corresponding critique templates using \texttt{TemplateTree} (Line 3). These templates guide the critique generation process, which is performed by the \texttt{Critic}. The critique provides feedback on the reasoning process and identifies the first error step $r_i$ (Line 4).

% Table-Critic retains the correct reasoning steps $R$ from the $r_1$ up to $r_{i-1}$, then refine the $R$ from the first error step $r_i$ based on the critique. Using the \texttt{Refiner}, Table-Critic updates both the reasoning steps and the predicted answer (Line 6). The updated answer is then re-evaluated by \texttt{Judge} to determine if the answer has improved (Line 7). This process repeats until the predicted answer becomes correct.

% The output of Table-Critic is the final refined answer $P$, which is expected to be the correct solution to the table reasoning task. By iterating through this multi-step refinement process, Table-Critic effectively improves the quality of the answer by systematically addressing errors in reasoning.


% \subsection{Key Components}
% As shown in Figure \ref{method}.a, 
% Table-Critic comprises four key components that work collaboratively to iteratively critique and refine reasoning steps and the predicted answer. These components—\textit{Judge}, \textit{Template Tree}, \textit{Critic}, and \textit{Refiner}—play distinct yet interconnected roles in error identification, critique generation, and reasoning step correction.


% \textbf{Judge.}
% The \textit{Judge} serves as the initial and iterative evaluator in Table-Critic. Its primary responsibility is to determine whether the predicted answer $P$ for a given table $T$ and question $Q$ is correct or not. Acting as a gatekeeper, the \textit{Judge} decides whether further refinement is needed or if the answer can be accepted as final.

% The evaluation function of the \textit{Judge} can be defined as:
% \begin{equation}
% J(T, Q, P) =
% \begin{cases}
%     Correct, & P \text{ is correct}, \\
%     Incorrect, & \text{otherwise}.
% \end{cases}
% \end{equation}

% Notably, the \textit{Judge} only assesses the final predicted answer $P$ and does not take into account the reasoning steps $R$. This is because the reasoning steps may contain errors, but the final result happens to be correct. In such cases, the \textit{Judge} considers the problem-solving process to be successful.

\subsection{Self-evolving Template Tree}
To address the challenge of identifying diverse and unpredictable error types in table reasoning, we introduce a self-evolving template tree that systematically accumulates and organizes critique knowledge. This dynamic structure enables our system to effectively handle both common and emerging error patterns through experience-driven learning.


\textbf{Tree Structure.}
The template tree $\mathcal{T}$ represents a hierarchical structure that captures the relationships among different error types.
As shown in Figure~\ref{fig:framework}, each node in the tree represents a specific type of error, where: (1) Internal nodes represent broader error categories (e.g., Sub-table Error) that can be further subdivided into more specific error types.
(2) Leaf nodes represent specific error types (e.g., Row Error, Column Error) and maintain a repository of critique templates associated with that particular error type.

\textbf{Self-evolving Mechanism.} 
The template tree evolves dynamically through the Curator agent, which manages two primary operations: adding templates to existing leaf nodes and expanding tree branches. As illustrated in Figure~\ref{fig:framework}, the evolution process includes:

(1) \textit{Template Enhancement}. When new effective critique patterns are identified, the Curator adds them to the corresponding leaf node's template repository. This operation enriches existing error type categories without changing the tree structure. For instance, when a new effective template for Row Error is discovered, it is directly added to the corresponding template repository.

(2) \textit{Branch Expansion.} The Curator expands the tree structure in two ways when new error types are identified:

\begin{itemize}[topsep=1pt, partopsep=1pt, leftmargin=12pt, itemsep=-1pt]
    \item \textit{Vertical Expansion}: When a new error type is discovered that requires more fine-grained categorization, the Curator performs a vertical split. This operation transforms an existing leaf node into an internal node with two new child nodes. Specifically, the Curator first categorizes the existing templates in the leaf node with an appropriate name (e.g., Row Error), creating one new leaf node. Then, it creates another leaf node with a different name (e.g., Column Error) to accommodate the newly discovered error type and its corresponding templates. This process ensures that each leaf node maintains a cohesive collection of templates for a specific error type.
    
    \item \textit{Horizontal Expansion}: When a completely new error type is identified that parallels existing categories, the Curator adds a new branch at the same level. This operation preserves the existing structure while accommodating new error types. As illustrated in the Figure~\ref{fig:framework} (bottom), the addition of the Final Query Error branch represents a horizontal expansion that complements the existing Sub-table Error category.
\end{itemize}

Through these evolution mechanisms, our template tree maintains a dynamic balance between preserving accumulated knowledge and incorporating new error patterns. The vertical expansion enables more precise error categorization, while horizontal expansion ensures comprehensive coverage of diverse error types. This adaptive structure allows the system to continuously improve its critique capabilities while maintaining organized and efficient template management.
The detailed pipeline of our Table-Critic is presented in Appendix~\ref{app:algorithm}.



% \textbf{Template Tree.}
% The dynamically updated \textit{Template Tree} is a dynamically updated knowledge base that generates structured critique templates based on the table $T$, question $Q$, reasoning steps $R$, and the incorrect predicted answer $P$.

% Table-Critic searches for corresponding critique paths within the \textit{Template Tree} using information from $T$, $Q$, $R$, and $P$. Upon identifying relevant paths, it retrieves the corresponding critique templates, which encapsulate common error patterns and serve as structured guidelines for generating critiques.
% The template updated into the \textit{Template Tree} is derived from critiques that have been refined and judged as correct by the \textit{Judge}. 
% % Specifically, after the Critic generates a critique based on the retrieved templates, the Refiner updates the reasoning steps $R$ and the predicted answer $P$ according to this critique. Then, the Judge re-evaluates the updated answer. If the Judge deems the answer to be correct, the corresponding critique is considered a valuable addition to the Template Tree.

% As seen in Figure \ref{method}.b, the \textit{Template Tree} undergoes dynamic updates in the following scenarios:
% (1) Direct expansion: When Table-Critic identifies an error path within the \textit{Template Tree}, it expands the tree by adding the new template as a leaf node. For example, a new template $A_3$ may be directly added to an existing error node $A$.
% (2) Vertical expansion: Even when an error path is found, if the error type of the new template does not exactly match the templates in the path, the \textit{Template Tree} undergoes vertical expansion. Existing nodes (e.g., $A_1$ and $A_2$) are grouped under a new parent node $A$, and a new branch $B$ with template $B_1$ is introduced, enhancing error categorization.
% (3) Horizontal expansion: If no suitable error path is found, the tree expands horizontally by introducing a new branch (e.g., $C$) with a new template $C_1$, allowing for greater flexibility in capturing diverse error types.

% The dynamically updated Template Tree is used to generate critique templates based on the given table $T$, question $Q$, existing reasoning steps $R$, and the incorrect predicted answer $P$.

% Table-Critic searches for the corresponding paths of templates within the Template Tree using the information from $T$, $Q$, $R$, and $P$. Once the appropriate paths are found, the corresponding critic templates are retrieved. These templates capture the essential patterns and characteristics of the error in the reasoning process, which serve as a blueprint for generating more detailed critiques later.

% However, there may be situations where the algorithm fails to find the corresponding errors in the tree. In such cases, to ensure that the critique generation process can still proceed, Table-Critic randomly selects 5 templates from all the templates in the tree. These randomly selected templates are then used as the critic templates, providing a starting point for the Critic to analyze and generate detailed critiques.


% \textbf{Critic.}
% Unlike traditional critics that evaluate only the final answer, the \textit{Critic} in Table-Critic conducts a step-by-step analysis of the reasoning process, identifying errors at an intermediate level.

% Leveraging the critique templates retrieved from the \textit{Template Tree}, the \textit{Critic} systematically analyzes the table $T$, question $Q$, reasoning steps $R$, and predicted answer $P$. It then generates a detailed critique, pinpointing the first erroneous step $r_i$.

% To formally define this process, let $\mathcal{E}(r_i)$ denote an error function that evaluates whether a given reasoning step $r_i \in R$ contains a reasoning mistake. The set of erroneous step indices can be defined as:
% \begin{equation}
% I = \{i \mid r_i \in R, \mathcal{E}(r_i) \neq 0\}.
% \end{equation}
% Here, $I$ represents the indices of all erroneous reasoning steps in $R$.

% The critique function $\mathcal{C}$, which identifies the first incorrect step and generates a corresponding critique, can be defined as:
% \begin{equation}
% (\text{critique}, \min(I)) = \mathcal{C}(T, Q, R, P).
% \end{equation}
% This formulation ensures that the \textit{Critic} focuses on the earliest mistake in the reasoning chain, preventing error propagation in subsequent steps.

% The critique generated by the \textit{Critic} is crucial, as it provides guidance on how to correct the reasoning. It can highlight logical flaws, incorrect data usage, or inconsistencies in the reasoning steps. Identifying the first erroneous step $r_i$ is particularly significant, as it enables the refinement process to directly address the root cause of the reasoning failure. This targeted approach ensures efficient correction, leading to a progressively improved answer.



% \textbf{Refiner.}
% The \textit{Refiner} is responsible for modifying the reasoning steps $R$ and updating the predicted answer $P$ based on the critique generated by the \textit{Critic}. Once the first erroneous step $r_i$ is identified, the \textit{Refiner} preserves the correct steps from $r_1$ to $r_{i-1}$ and initiates the refinement process from $r_i$ onward.

% To formalize this, the reasoning steps are divided into two parts: the correct steps and the erroneous steps starting from $r_i$. The updated reasoning steps $R'$ are defined as:
% \begin{equation}
% R' = [R_1, R_2, \dots, R_{i-1}],
% \end{equation}
% which includes only the correct steps up to $r_{i-1}$.

% The \textit{Refiner} then updates the reasoning steps and predicted answer $P$ using $R'$ and the critique from the \textit{Critic}. The update is calculated as:
% \begin{equation}
% R^*, P^* = \mathcal{R}(\text{critique}, T, Q, R'),
% \end{equation}
% where $\mathcal{R}(\cdot)$ denotes the reasoning function that updates both the reasoning steps $R'$ and the predicted answer $P$ based on the critique.

% This iterative refinement continues until the \textit{Judge} confirms $P^*$ is correct. Through multiple iterations, the \textit{Refiner} ensures that the reasoning steps are progressively improved, ultimately leading to a more reliable and accurate final answer $P^*$.









% \begin{algorithm*}
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}
% \caption{Table - Critic}
% \label{alg1}
% \begin{algorithmic}[1]
% \REQUIRE $T, Q, R, \hat{A}$ \hfill {\small $\triangleright$ Table $T$, Question $Q$, Reasoning steps $R$, Predicted answer $\hat{A}$}
% \STATE $judge \leftarrow \texttt{Judge}(T, Q, \hat{A})$ \hfill {\small $\triangleright$ Determine whether the $\hat{A}$ is correct}
% \WHILE{$judge$ is incorrect}
% \STATE $template \leftarrow \texttt{TemplateTree}(T, Q, R, \hat{A})$ \hfill {\small $\triangleright$ Generate a critique template}
% \STATE $critique, i \leftarrow \texttt{Critic}(template, T, Q, R, \hat{A})$ \hfill {\small $\triangleright$ Generate the critique and the first error step $i$} 
% \\ \hfill {\small $\triangleright$ (where $i$ is the step number) based on the template}
% \STATE $R, \hat{A} \leftarrow \texttt{Refiner}(critique, i, T, Q, R ,\hat{A})$ \hfill {\small $\triangleright$ Update the reasoning steps $R$ and the predicted answer $\hat{A}$}
% \\ \hfill {\small $\triangleright$ from the first error step}
% \STATE $judge \leftarrow \texttt{Judge}(T, Q, \hat{A})$ \hfill {\small $\triangleright$ Determine again whether the updated answer $\hat{A}$ is correct}
% \ENDWHILE
% \ENSURE $\hat{A}$ \hfill {\small $\triangleright$ Output the refined answer $\hat{A}$}
% \end{algorithmic}
% \end{algorithm*}

% \definecolor{lightpurple}{RGB}{230, 220, 250}
\definecolor{lightpurple}{RGB}{216,236,228}
% \definecolor{lightpurple}{RGB}{180, 220, 210}

\begin{table*}[htbp]
    \centering
    % 适当调整表格宽度，避免表格过宽
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccccccc}
            \toprule % 顶部粗线
            \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Qwen2.5-72B}} & \multicolumn{2}{c}{\textbf{LLaMA3.3-70B}} & \multicolumn{2}{c}{\textbf{GPT-4o-mini}} & \multicolumn{2}{c}{\textbf{Average}} \\
            \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}% 分割子表头
            & WikiTQ & TabFact & WikiTQ & TabFact & WikiTQ & TabFact & WikiTQ & TabFact \\
            \midrule % 中间细线
            End-to-End QA & 56.6 & 85.1 & 51.1 & 81.0 & 52.6 & 73.5 & 53.4 & 79.9 \\
            Few-Shot QA & 61.7 & 85.0 & 62.0 & 80.7 & 57.6 & 75.1 & 60.4 & 80.3 \\
            Binder \citep{cheng2022binding} & 57.0 & 82.2 & 52.2 & 80.5 & 54.8 & 83.3 & 54.7 & 82.0 \\
            Dater \citep{ye2023large} & 63.8 & \uline{90.0} & 59.5 & 87.6 & 65.8 & 83.6 & 63.0 & 87.1 \\
            Chain-of-Table \citep{wang2024chain} & 68.3 & 89.7 & 62.1 & \uline{89.9} & \uline{67.5} & \uline{88.9} & 66.0 & \uline{89.5} \\
            Critic-CoT \citep{critic-cot} & \uline{69.0} & 89.8 & \uline{66.8} & 88.0 & 66.3 & 86.9 & \uline{67.4} & 88.2 \\
            \midrule % 中间细线
            \rowcolor{lightpurple} 
            Table-Critic (ours) & \textbf{77.2} & \textbf{92.6} & \textbf{70.1} & \textbf{91.5} & \textbf{73.9} & \textbf{91.1} & \textbf{73.7} & \textbf{91.7} \\
             \rowcolor{lightpurple}
             & \textcolor{red}{\(\uparrow\)8.2} & \textcolor{red}{\(\uparrow\)2.6} & \textcolor{red}{\(\uparrow\)3.3} & \textcolor{red}{\(\uparrow\)1.6} & \textcolor{red}{\(\uparrow\)6.4} & \textcolor{red}{\(\uparrow\)2.2} 
             & \textcolor{red}{\(\uparrow\)6.3} & \textcolor{red}{\(\uparrow\)2.2} \\
            \bottomrule % 底部粗线
        \end{tabular}
}
\caption{Table reasoning results on WikiTQ and TabFact with Qwen2.5-72B, LLaMA3.3-70B, and GPT-4o-mini. Bold denotes the best performance and underline denotes the second-best performance.}
\vspace{-0.8em}
\label{mainresult}
\end{table*}

\section{Experiments}
\label{experiments}

\subsection{Experimental Setup}
\textbf{Datasets.} We evaluate our approach on two standard benchmarks:
(1) WikiTableQuestions (WikiTQ)~\citep{pasupat2015compositional}: A table reasoning benchmark with 4,344 test samples from 421 tables.
(2) TabFact~\citep{chen2019tabfact}: A fact verification benchmark in table reasoning with 2,024 test samples from 298 tables.

\textbf{Baselines.} We conduct comprehensive experiments comparing Table-Critic against three categories of baselines:
\textbf{(1) Standard Reasoning.} \text{End-to-End QA} directly generates answers using table and question as input. \text{Few-Shot QA} extends this by incorporating exemplar (Table, Question, Answer) triplets from the training set.
\textbf{(2) Decomposition-Based Reasoning.} Binder~\citep{cheng2022binding} decomposes questions into executable SQL/Python sub-programs. Dater~\citep{ye2023large} employs parsing-execution-filling strategy with sub-table decomposition. Chain-of-Table~\citep{wang2024chain} generates intermediate tables through dynamic operations.
\textbf{(3) Critic-Based Reasoning.} Critic-CoT~\citep{critic-cot} implements self-reflection for error identification.

\textbf{Implementation Details.}
To ensure comprehensive evaluation, we conduct experiments across three LLMs: Qwen2.5-72B-Instruct~\citep{yang2024qwen2}, LLaMA3.3-70B-Instruct~\citep{llama3}, and GPT-4o-mini~\citep{gpt_4o}.
For all baseline methods, we follow their original settings to ensure optimal performance.
For fair comparison, both Critic-CoT~\citep{critic-cot} and our Table-Critic framework are implemented upon Chain-of-Table~\citep{wang2024chain}.
For our Table-Critic, the template tree is initialized with only 2 templates that demonstrate basic critique patterns. From this minimal starting point, the tree evolves autonomously through our self-evolving mechanism, continuously learning and incorporating new critique patterns.
For all experiments, we set the maximum refinement iterations K to 5 and use temperature 0.0 for greedy decoding. The detailed prompts and instructions for each agent in our framework are provided in Appendix~\ref{app:prompts}.

% \subsection{Datasets and Evaluation Metrics}
% We evaluate Table-Critic using two prominent table reasoning benchmarks: WikiTableQuestions (WikiTQ) \citep{pasupat2015compositional} and TabFact \citep{chen2019tabfact}. 

% For the table-based question answering task, we utilize the WikiTQ dataset, which comprises open-domain tables paired with complex questions. Our evaluation metric is the official denotation accuracy \citep{pasupat2015compositional}, which assesses the correctness of predicted answers against the gold standard. We conduct this evaluation on a test set that includes 4,344 samples drawn from 421 distinct tables. 

% For the table-based fact verification task, we employ the TabFact dataset, consisting of various statements grounded in Wikipedia tables. We measure performance using binary classification accuracy, evaluated on a small test set that comprises 2,024 statements from 298 unique tables.



% \textbf{Baselines.}
% We compare Table-Critic against three categories of methods: (a) generic reasoning approaches such as End-to-End QA and Few-Shot QA, (b) decomposition-based reasoning approaches such as Binder \citep{cheng2022binding}, Dater \citep{ye2023large}, and Chain-of-Table \citep{wang2024chain}, and (c) critic-based reasoning approaches such as Critic-CoT \citep{critic-cot}. Below is a summary of these baselines:

% \textbf{Generic Reasoning.} End-to-End QA directs the LLM to answer questions directly using the provided table and question as input. Few-Shot QA extends this by incorporating a few-shot prompt containing examples of (Table, Question, Answer) triplets selected from the training set, aiming to improve the model's ability to infer correct answers.

% \textbf{Decomposition-Based Reasoning.} Binder \citep{cheng2022binding} addresses complex questions by generating SQL or Python sub-programs that decompose the question into smaller, executable sub-questions, allowing precise extraction and computation tailored to specific requirements. Dater \citep{ye2023large} employs a parsing-execution-filling strategy, segmenting tables into sub-tables optimized for individual sub-questions. Chain-of-Table \citep{wang2024chain} further enhances this approach by dynamically generating intermediate tables through a comprehensive suite of table operations, enabling the handling of intricate dependencies and manipulations.

% \textbf{Critic-Based Reasoning.} Critic-CoT \citep{critic-cot} introduces a post-hoc critique mechanism where the model reflects on its reasoning steps to identify and address errors. While promising, this method relies solely on the model's inherent self-reflection capabilities, which often lack consistency and reliability, especially in complex table reasoning tasks.

% \subsection{Main Results}
% In our experiments, summarized in Table \ref{mainresult}, Table-Critic demonstrates substantial improvements over all baseline methods on both the WikiTQ and TabFact datasets, establishing its effectiveness in tackling complex table reasoning tasks.

% On the WikiTQ dataset, Table-Critic achieves a denotation accuracy of 77.2\% with Qwen2.5-72B, 70.1\% with LLaMA3.3-70B, and 73.9\% with GPT-4o-mini. These results represent absolute improvements of \textbf{8.2\%}, \textbf{3.3\%}, and \textbf{6.4\%}, respectively, compared to the second-best performing method. These notable gains highlight the effectiveness of Table-Critic in addressing the challenges posed by complex, multi-step reasoning tasks that require precise localization and processing of tabular data.

% On the TabFact dataset, Table-Critic also sets new state-of-the-art performance benchmarks, achieving accuracies of 92.6\%, 91.5\%, and 91.1\% with Qwen2.5-72B, LLaMA3.3-70B, and GPT-4o-mini, respectively. These scores represent improvements of \textbf{2.6\%}, \textbf{1.6\%}, and \textbf{2.2\%} over the next best-performing method. While the performance gap on TabFact is narrower than on WikiTQ, the consistent improvements demonstrate the robustness of Table-Critic in handling fact verification tasks, where accuracy and reliability are critical.

% The performance differences between the two datasets reveal interesting patterns. Table-Critic achieves higher relative improvements on WikiTQ (an average gain of \textbf{6.3\%}) compared to TabFact (an average gain of \textbf{2.2\%}). This disparity likely stems from the intrinsic complexity of the tasks. WikiTQ requires answering compositional, multi-step questions that demand extensive reasoning and interaction with table contents, a scenario where Table-Critic's multi-agent framework and dynamic template trees excel. In contrast, TabFact primarily involves binary fact verification, a simpler task that requires less intricate reasoning. These results underscore Table-Critic’s strength in handling detailed and compositional reasoning tasks while maintaining strong performance in simpler tasks.

% Overall, Table-Critic’s innovative stepwise refinement and critique mechanisms effectively mitigate error propagation and ensure a deeper understanding of tabular data, enabling it to surpass existing methods significantly.

\subsection{Main Results}
We report the performance on different table reasoning benchmarks across different LLMs in Table~\ref{mainresult}.
Our comprehensive evaluation reveals several key findings:
\textbf{First,} Table-Critic consistently outperforms all baseline methods across both datasets and all three LLMs. On average, our method achieves 73.7\% accuracy on WikiTQ and 91.7\% on TabFact, representing significant improvements of 6.3\% and 2.2\% respectively over the strongest baselines.
\textbf{Second,} the improvements are robust across different model architectures. With Qwen2.5-72B-Instruct, we achieve the highest absolute performance (77.2\% on WikiTQ, 92.6\% on TabFact), showing substantial gains of 8.2\% and 2.6\% respectively. Similar patterns are observed with LLaMA3.3-70B-Instruct and GPT-4o-mini, demonstrating the framework's generalizability across different foundation models.
\textbf{Third,} the performance variations between WikiTQ and TabFact provide insights into our method's strengths. Table-Critic shows larger improvements on WikiTQ (average +6.3\%) compared to TabFact (average +2.2\%), indicating its particular effectiveness in handling complex, multi-step reasoning tasks. This aligns with our framework design, as WikiTQ's compositional questions benefit more from our multi-turn refinement and self-evolving template tree mechanism than TabFact's binary verification tasks. Nevertheless, the consistent improvements on TabFact demonstrate our method's capability even in simpler scenarios.
\textbf{Finally,} comparing against different baseline categories reveals the advancement of our approach. While recent methods like Chain-of-Table~\citep{wang2024chain} and Critic-CoT~\citep{critic-cot} have made notable progress through decomposition and criticism mechanisms, Table-Critic achieves substantially larger improvements over these strong baselines. This suggests that our multi-agent framework, combining multi-turn refinement with self-evolving template tree, provides a more effective solution for complex table reasoning tasks.


% \subsubsection{Performance}
% \captionsetup[figure]{skip=1pt}
% % \centering
% \begin{figure}[ht]
%     \centering
%     \begin{minipage}[t]{0.5\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pie_wikitq.pdf}
%         % \vspace{-30pt}
%         \small\text{(a) WikiTQ}
%     \end{minipage}


%     \begin{minipage}[t]{0.5\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pie_tabfact.pdf}
%         % \vspace{1pt}
%         \small\text{(b) TabFact}
%     \end{minipage}
%     \vspace{0.1cm}
%     \caption{Accuracy Comparison Before (Initial) and After (Refined) Applying Table-Critic on WikiTQ and TabFact}
%     \label{pie}
% \end{figure}

% Figure \ref{pie} presents a comparative analysis of performance before (Initial) and after (Refined) the application of Table-Critic.

% Table-Critic effectively improved performance on WikiTQ, raising the correct result rate from 68.3\% to 77.2\% and reducing the incorrect rate from 31.7\% to 22.8\%. For TabFact, despite the already high initial performance (89.7\% correct and 10.3\% incorrect), Table-Critic further improved the results, increasing the correct rate to 92.6\% and decreasing the incorrect rate to 7.4\%. These results underscore the ability of Table-Critic to enhance outcomes, even for datasets with high initial performance.


% \begin{figure}[ht]
%     \centering
%     \begin{minipage}{0.25\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{heat_wikitq_criticcot.pdf}
%         % \caption{Heatmap for WikiTQ}
%         \small\text{(a) WikiTQ}
%     \end{minipage}%
%     \begin{minipage}{0.25\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{heat_tabfact_criticcot.pdf}
%         % \caption{Heatmap for TabFact}
%         \small\text{(b) TabFact}
%     \end{minipage}
%     \vspace{0.1cm}
%     \caption{Confusion Matrix Visualization Before (Initial) and After (Refined) Applying Critic-CoT on WikiTQ and TabFact}
%     \label{confusion_matrix}
% \end{figure}


% \begin{figure}[ht]
%     \centering
%     \begin{minipage}{0.25\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{heat_wikitq.pdf}
%         % \caption{Heatmap for WikiTQ}
%         \small\text{(a) WikiTQ}
%     \end{minipage}%
%     \begin{minipage}{0.25\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{heat_tabfact.pdf}
%         % \caption{Heatmap for TabFact}
%         \small\text{(b) TabFact}
%     \end{minipage}
%     \vspace{0.1cm}
%     \caption{Confusion Matrix Visualization Before (Initial) and After (Refined) Applying Table-Critic on WikiTQ and TabFact}
%     \label{confusion_matrix}
% \end{figure}


% \begin{table}
% \centering
% \begin{tabular}{lcc}
% \toprule
%  & WikiTO & TabFact \\
% \midrule
% Chain-of-Table & 68.3 & 89.7 \\
% \midrule
% Critic-CoT & 69.0 & 89.8 \\
% $\Delta^{\text{i}\to\text{c}}$ & +5.6 & +2.9 \\
% $\Delta^{\text{c}\to\text{i}}$ & -4.9 & -2.8 \\
% \rowcolor{lightpurple!23} $\Delta$ & \textcolor{red}{+0.7} & \textcolor{red}{+0.1} \\
% \midrule
% Table-Critic & 77.2 & 92.6 \\
% $\Delta^{\text{i}\to\text{c}}$ & +9.6 & +3.4 \\
% $\Delta^{\text{c}\to\text{i}}$ & -0.7 & -0.5 \\
% \rowcolor{lightpurple!23} $\Delta$ & \textcolor{red}{+8.9} & \textcolor{red}{+2.9} \\
% \bottomrule
% \end{tabular}
% \caption{Caption}
% \label{criticcot}
% \end{table}

% \begin{table}
% \centering
% \begin{tabular}{lcc}
% \toprule
%  & WikiTO & TabFact \\
% \midrule
% Chain-of-Table & 68.3 & 89.7 \\
% Table-Critic & 77.2 & 92.6 \\
% $\Delta^{\text{i}\to\text{c}}$ & +9.6 & +3.4 \\
% $\Delta^{\text{c}\to\text{i}}$ & -0.7 & -0.5 \\
% \rowcolor{lightpurple!23} $\Delta$ & \textcolor{red}{+8.9} & \textcolor{red}{+2.9} \\
% \bottomrule
% \end{tabular}
% \caption{Caption}
% \label{criticcot}
% \end{table}
\subsection{Analysis of Critic Effectiveness}

\begin{table*}[htbp]
    \centering
    % \setlength{\tabcolsep}{2pt}
    % \fontsize{9}{12}\selectfont
    \begin{tabular}{lccccccccc}
        \toprule
        \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{1}{c}{\textbf{Chain-of-Table}} & \multicolumn{4}{c}{\textbf{Critic-CoT}} & \multicolumn{4}{c}{\textbf{Table-Critic}} \\
        \cmidrule(lr){2-2} \cmidrule(lr){3-6} \cmidrule(lr){7-10}
        & Acc & Acc & $\Delta^{\text{i}\to\text{c}}$ & $\Delta^{\text{c}\to\text{i}}$ & $\Delta$ & Acc & $\Delta^{\text{i}\to\text{c}}$ & $\Delta^{\text{c}\to\text{i}}$ & $\Delta$ \\
        \midrule
        WikiTQ & 68.3 & 69.0 & +5.6 & -4.9 & \textcolor{red}{+0.7} & \cellcolor{lightpurple}77.2 & \cellcolor{lightpurple}+9.6 & \cellcolor{lightpurple}-0.7 & \cellcolor{lightpurple}\textcolor{red}{+8.9} \\
        TabFact & 89.7 & 89.8 & +2.9 & -2.8 & \textcolor{red}{+0.1} & \cellcolor{lightpurple}92.6 & \cellcolor{lightpurple}+3.4 & \cellcolor{lightpurple}-0.5 & \cellcolor{lightpurple}\textcolor{red}{+2.9} \\
        \bottomrule
    \end{tabular}
    % \caption{Performance Comparison of Critic Methods on WikiTQ and TabFact Datasets: (1) Acc: Accuracy (\%) of each method. (2) $\Delta^{\text{i}\to\text{c}}$: Percentage of questions that were incorrect in Chain-of-Table but became correct in the current method, quantifying its ability to resolve previously unsolved problems. (3) $\Delta^{\text{c}\to\text{i}}$: Percentage of questions that were correct in Chain-of-Table but became incorrect in the current method, reflecting potential overfitting or instability in maintaining valid solutions. (4) $\Delta$: Overall performance gain relative to Chain-of-Table, calculated as the difference between corrected and degraded responses.}
    \caption{Critic performance comparison of different critic methods. $\Delta^{\text{i}\to\text{c}}$, $\Delta^{\text{c}\to\text{i}}$, and $\Delta$ measure the error correction rate, solution degradation rate, and net performance gain relative to Chain-of-Table, respectively.}
    \vspace{-1em}
    \label{tab:critic}
\end{table*}

% \captionsetup[figure]{skip=1pt}
% \centering

As shown in Table~\ref{tab:critic}, we conduct a detailed analysis of different critic mechanisms by comparing Table-Critic with Chain-of-Table~\citep{wang2024chain} and Critic-CoT~\citep{critic-cot}. Our analysis focuses on four key metrics: 
\textbf{(1) Overall Accuracy (Acc)}: The percentage of correctly solved questions;
\textbf{(2) Error Correction Rate ($\Delta^{\text{i}\to\text{c}}$):} The percentage of questions incorrectly solved by Chain-of-Table but corrected by different Critic methods;
\textbf{(3) Solution Degradation Rate ($\Delta^{\text{c}\to\text{i}}$):} The percentage of questions correctly solved by Chain-of-Table but degraded by different Critic methods;
\textbf{(4) Net Performance Gain ($\Delta$):} The overall improvement relative to Chain-of-Table, calculated as $\Delta = \Delta^{\text{i}\to\text{c}} + \Delta^{\text{c}\to\text{i}}$.

% \textbf{Error Correction ($\Delta^{\text{i}\to\text{c}}$) vs. Solution Degradation ($\Delta^{\text{c}\to\text{i}}$).}
\textbf{Error Correction vs. Solution Degradation.}
Table-Critic demonstrates superior error correction capabilities while minimizing solution degradation. On WikiTQ, it successfully corrects 9.6\% of Chain-of-Table's errors while only degrading 0.7\% of correct solutions, resulting in a substantial net performance gain (\text{+8.9\%}). In contrast, Critic-CoT shows a less effective pattern, with a 5.6\% correction rate offset by a high degradation rate (-4.9\%), yielding only a marginal improvement (\text{+0.7\%}).


\textbf{Task-Specific Performance.} The effectiveness of critique mechanisms varies across different tasks. On WikiTQ, which involves complex multi-step reasoning, Table-Critic achieves a higher error correction rate (+9.6\% vs +5.6\%) and maintains a observably lower degradation (-0.7\% vs -4.9\%) compared to Critic-CoT. For TabFact's simpler verification tasks, while the improvements are more modest, Table-Critic still maintains better stability with lower degradation rates (-0.5\% vs -2.8\%).

\textbf{Critic Stability.} A key advantage of Table-Critic is its stability in maintaining correct solutions. The consistently low degradation rates (-0.7\% for WikiTQ and -0.5\% for TabFact) suggest that our self-evolving template tree effectively preserves valid reasoning patterns while identifying and correcting errors. This contrasts with Critic-CoT's higher degradation rates (-4.9\% for WikiTQ and -2.8\% for TabFact), indicating potential instability in its critique process.

% As shown in Table~\ref{critic}, we evaluate the effectiveness of critic-based refinement mechanisms across WikiTQ and TabFact. The analysis compares three approaches: Chain-of-Table, Critic-CoT, and our proposed Table-Critic. The assessment uses four key metrics: accuracy (\textit{Acc}), error correction rate ($\Delta^{\text{i}\to\text{c}}$), solution degradation rate ($\Delta^{\text{c}\to\text{i}}$), and net performance gain ($\Delta$).

% Table-Critic achieves state-of-the-art results with accuracies of 77.2\% on WikiTQ and 92.6\% on TabFact, reflecting net performance gains of \textbf{+8.9} and \textbf{+2.9}, respectively, over the Chain-of-Table baseline. These improvements are driven by Table-Critic’s ability to correct a high percentage of errors (e.g., 9.6\% $\Delta^{\text{i}\to\text{c}}$ on WikiTQ) while minimizing solution degradation (0.7\% $\Delta^{\text{c}\to\text{i}}$). In contrast, Critic-CoT achieves only marginal net improvements (\textbf{+0.7} on WikiTQ and \textbf{+0.1} on TabFact), primarily due to its higher degradation rates (e.g., -4.9\% $\Delta^{\text{c}\to\text{i}}$ on WikiTQ), which reflect instability in maintaining valid solutions.

% The performance gap across datasets highlights the unique challenges posed by different tasks. WikiTQ’s multi-step reasoning benefits significantly from Table-Critic’s iterative refinement (9.6\% $\Delta^{\text{i}\to\text{c}}$), whereas TabFact’s simpler fact verification tasks result in smaller but consistent improvements. These results demonstrate the importance of a table-specific critique mechanism, particularly for complex question answering tasks requiring dynamic table manipulation and error recovery.

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{density_wikitq.pdf}
        % \vspace{-3pt}
        \text{(a) WikiTQ}
        \vspace{1pt}
    \end{minipage}

    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{density_tabfact.pdf}
        \vspace{-3pt}
        \text{(b) TabFact}
    \end{minipage}
    % \vspace{0.1cm}
    \caption{Analysis of Model Convergence and Iteration Requirements on WikiTQ and TabFact Datasets.}
    \vspace{-0.8em}
    \label{fig:accuracy-density}
\end{figure}

\subsection{Analysis of Multi-Turn Mechanism}
To understand the effectiveness of our multi-turn refinement mechanism, we analyze how model performance evolves with the number of iterations $K$ and the distribution of required iteration counts (set maximal $K=10$), as shown in Figure~\ref{fig:accuracy-density}.


\textbf{Performance Evolution.} On both datasets, we observe a consistent pattern of rapid initial improvement followed by gradual convergence. For WikiTQ, the accuracy increases sharply from 67.6\% to 76.5\% within the first three iterations and stabilizes around 77\% after six iterations. Similarly, on TabFact, the performance improves significantly in early iterations and plateaus at approximately 92\% after five iterations. This pattern suggests that our multi-turn mechanism effectively refines solutions through iterative improvements.

\textbf{Iteration Distribution.} The density plots reveal interesting insights about the complexity of different tasks. On WikiTQ, we observe a broader distribution with multiple peaks, indicating that questions require varying numbers of iterations for resolution. The main peak occurs at 1-2 iterations, with smaller peaks extending up to 10 iterations, reflecting the diverse complexity of multi-step reasoning questions.
TabFact also shows a concentrated distribution with two distinct peaks: a primary peak at 1-2 iterations and a secondary peak around 10 iterations. This bimodal pattern suggests that TabFact tend to fall into two categories: (1) straightforward cases that can be verified quickly within 1-2 iterations, and (2) complex cases that require extensive refinement to reach a conclusive verification. This distribution aligns with the inherent nature of fact verification tasks, where statements are either relatively simple to verify or require careful step-by-step examination.
% In contrast, TabFact shows a more concentrated distribution centered around 1-2 iterations, aligning with its simpler binary verification nature.

\textbf{Convergence and Stability Analysis.} The results suggest that while our method allows for up to 10 iterations, most improvements are achieved within the first 5 iterations. This efficient convergence, combined with our early termination mechanism, helps maintain computational efficiency while ensuring thorough reasoning. Notably, as evidenced in Table~\ref{tab:critic}, Table-Critic maintains stable performance across iterations without the degradation typically seen in iterative approaches, demonstrating the effectiveness of our Critic agent and self-evolving template tree mechanism.




% \begin{figure}[ht]
%     \centering
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{density_wikitq.pdf}
%         % \caption{Heatmap for WikiTQ}
%         \small\text{(a) WikiTQ}
%     \end{minipage}
%     \vspace{0.3cm} % 调整两个子图之间的间距
%     \begin{minipage}{\textwidth}
%         \centering
%         \includegraphics[width=0.5\textwidth]{density_tabfact.pdf}
%         % \caption{Heatmap for TabFact}
%         \small\text{(b) TabFact}
%     \end{minipage}
%     \vspace{0.2cm}
%     \caption{Accuracy and Density Trends Across Iterations on WikiTQ and TabFact}
%     \label{accuracy-density}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{density_wikitq.pdf}
%         % \caption{Heatmap for WikiTQ}
%         \small\text{(a) WikiTQ}
%     \end{minipage}%
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{density_tabfact.pdf}
%         % \caption{Heatmap for TabFact}
%         \small\text{(b) TabFact}
%     \end{minipage}
%     \vspace{0.1cm}
%     \caption{Accuracy and Density Trends Across Iterations on WikiTQ and TabFact }
%     \label{accuracy-density}
% \end{figure}

% \begin{table}[h!]
% \centering
% \fontsize{9.5}{12}\selectfont
% \begin{tabular}{@{}lccccc@{}}
% \toprule
%          & \textbf{1}   & \textbf{2}   & \textbf{3}  & \textbf{4}  & \textbf{5}  \\ \midrule
% \textbf{WikiTQ}   & 327 & 57 & 23 & 5 & 7 \\
% \textbf{TabFact}  & 42  & 12  & 9  & 4  & 2  \\ \bottomrule
% \end{tabular}
% \caption{Iterations Required for Incorrect-to-Correct Transition}
% \label{iteration}
% \end{table}


% To further evaluate the multi-turn mechanism, we conduct iterative refinement experiments on WikiTQ and TabFact. Figure~\ref{accuracy-density} illustrates the trends in accuracy and error density across iterations.

% On WikiTQ, accuracy improves from 75.1\% in the first iteration to 77.5\% after 10 iterations, demonstrating steady and meaningful gains. This reflects the effectiveness of iterative refinement in addressing the dataset’s complex table structures and multi-step reasoning tasks. In contrast, TabFact shows smaller yet consistent improvements, with accuracy increasing from 91.2\% in the first iteration to 92.7\% after 10 iterations. The smaller gain on TabFact highlights the simpler nature of its fact verification task, where fewer iterations are required to resolve errors.

% Beyond accuracy improvements, Figure~\ref{accuracy-density} tracks the density of errors corrected at each iteration (blue curves). Higher error density in earlier iterations reflects the system's ability to identify and address a significant volume of initial errors. Over subsequent iterations, the density decreases, as fewer errors remain unresolved. This trend highlights the effectiveness of our multi-agent framework in progressively refining reasoning steps, driven by dynamic template-based critiques and collaborative error correction.

% In summary, the results validate the effectiveness of the multi-turn refinement mechanism in mitigating error propagation and improving reasoning accuracy. The collaborative multi-agent design of the Table-Critic framework, combined with the self-evolving template tree, is instrumental in addressing diverse error types systematically. This iterative and experience-driven approach allows our system to adapt to complex table reasoning challenges, achieving significant performance gains in both accuracy and error reduction.

\subsection{Analysis of Computational Cost}
\begin{figure}[t]
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{line_wikitq.pdf}
        % \caption{Heatmap for WikiTQ}
        \text{(a) WikiTQ}
    \end{minipage}%
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{line_tabfact.pdf}
        % \caption{Heatmap for TabFact}
        \text{(b) TabFact}
    \end{minipage}
    \vspace{-0.2em}
    \caption{Computational cost and Effectiveness Comparison between SC (Self-Consistency Based on Chain-of-Table) and our Table-Critic.}
    \vspace{-1em}
    \label{fig:computation}
\end{figure}

To ensure a fair comparison with Chain-of-Table~\citep{wang2024chain} in terms of computational cost, we conduct an analysis of the cost-effectiveness trade-off, as shown in Figure~\ref{fig:computation}. Since Table-Critic builds upon Chain-of-Table by incorporating additional critique mechanisms, we align the computational costs by allowing Chain-of-Table to generate multiple solutions (majority voting) through Self-consistency~\citep{wang2023selfconsistency} (with temperature 0.8) and compare the performance under equivalent or even superior computational budgets.

\textbf{Efficiency Comparison.} Our method requires approximately 1.8-2.2× computational cost compared to the basic Chain-of-Table.
The complete derivation process of computational cost is provided in Appendix~\ref{app:computation}. 
However, as illustrated in Figure~\ref{fig:computation}, Table-Critic achieves substantially higher accuracy (77.2\% on WikiTQ and 92.6\% on TabFact) compared to Chain-of-Table's performance even with 15 solution attempts. Notably, Chain-of-Table shows only marginal improvements as the number of solutions increases, reaching 70.0\% on WikiTQ and 90.1\% on TabFact with 15 solutions.

\textbf{Cost-Effectiveness Analysis.} The results demonstrate that simply increasing the number of solution attempts in Chain-of-Table fails to achieve comparable performance to Table-Critic, despite consuming similar or even greater computational resources. This suggests that our multi-agent refinement mechanism provides a more effective approach to improving reasoning accuracy than traditional majority voting strategies. The superior performance of Table-Critic justifies its additional computational overhead by offering substantially better reasoning capabilities.


% We analyze the computational efficiency and effectiveness of Table-Critic by comparing it with the baseline Self-Consistency (SC) approach built on Chain-of-Table. SC improves accuracy by employing a multi-round voting strategy, aggregating results from multiple solutions generated through the Chain-of-Table process.

% As illustrated in Figure~\ref{fig:computation}, Table-Critic achieves substantial accuracy improvements while maintaining competitive computational costs. On WikiTQ (Figure~\ref{fig:computation}.a), Table-Critic reaches an accuracy of 77.2\% with a computational cost of 1.87 times that of Chain-of-Table (detailed calculations are provided in Appendix \ref{computation_wikitq}). This result is represented by the orange pentagon marker at (1.87, 77.2). In contrast, SC requires up to 15 solutions to approach Table-Critic's accuracy but still falls short. This highlights the efficiency of Table-Critic's dynamic, stepwise refinement mechanism, which achieves superior performance without requiring extensive iterations.

% On TabFact (Figure~\ref{fig:computation}.b), Table-Critic attains an accuracy of 89.7\% with a computational cost of 2.19 times that of Chain-of-Table (detailed calculations are provided in Appendix \ref{computation_tabfact}), as indicated by the orange pentagon marker at (2.19, 89.7). While SC demonstrates gradual improvement with additional solutions, Table-Critic consistently delivers higher accuracy from the outset. This advantage is due to its robust multi-agent critic-based refinement framework, which effectively addresses complex reasoning tasks by iteratively identifying and correcting errors.

% These results demonstrate that Table-Critic strikes an effective balance between computational efficiency and performance. By leveraging its dynamic refinement strategy, Table-Critic achieves state-of-the-art accuracy with only marginal increases in computational cost compared to Chain-of-Table. Furthermore, Table-Critic's performance gains are particularly pronounced on more complex and data-intensive datasets like WikiTQ. This highlights its ability to handle challenging reasoning tasks, making it a practical and scalable solution for real-world applications where both accuracy and computational efficiency are critical considerations.


% \begin{table}[h]
% \centering
% \small % 设置表格字体大小为 small
% \begin{minipage}{0.5\textwidth}
% \centering
% \caption{Comparison of Initial and Refined Results (Table 1)}
% \begin{tabular}{cccc}
% \toprule
%         &               & \multicolumn{2}{c}{\textbf{Initial}} \\
%         \cmidrule(lr){3-4}
%         &               & Correct       & Incorrect   \\
% \midrule
% \multirow{2}{*}{\textbf{Refined}} & Correct       & 2770          & 585         \\
%                          & Incorrect     & 199           & 790         \\
% \bottomrule
% \end{tabular}
% \label{tab:confusion_matrix1}
% \end{minipage}

% \vspace{0.5cm} % 增加垂直间距

% \begin{minipage}{0.5\textwidth}
% \centering
% \caption{Comparison of Initial and Refined Results (Table 2)}
% \begin{tabular}{lcc}
% \toprule
% \textbf{}          & Initial Correct & Initial Incorrect \\
% \midrule
% Refined Correct  & 2770                 & 585                    \\
% Refined Incorrect & 199                  & 790                    \\
% \bottomrule
% \end{tabular}
% \label{tab:confusion_matrix2}
% \end{minipage}
% \end{table}

\subsection{Analysis of Self-evolving Template Tree}
To investigate the effectiveness of our self-evolving mechanism, we conduct an ablation study comparing Table-Critic with and without the dynamic template evolution capability, as shown in Table~\ref{tab:ablation}. In the static setting (w/o Self-evolving), the template tree remains fixed with its initial two templates, while our full Table-Critic allows the Curator agent to dynamically maintain and evolve the template tree throughout the reasoning process.

\textbf{Performance Impact.} The results demonstrate the clear benefits of the self-evolving mechanism. Without template evolution, performance drops by 1.1\% on WikiTQ (from 77.2\% to 76.1\%) and 1.8\% on TabFact (from 92.6\% to 90.8\%). The more substantial performance gap on TabFact suggests that template evolution is particularly beneficial for fact verification tasks, where diverse verification patterns may be needed.

\textbf{Mechanism Analysis.} These results highlight the importance of dynamic adaptation in our framework. The self-evolving mechanism allows the template tree to expand beyond its initial state, accommodating diverse reasoning patterns encountered during the critique process. This flexibility enables more effective error detection and correction compared to a static template approach. The performance gains validate our design choice of incorporating dynamic template evolution, showing that the ability to adapt and expand the template structure is crucial for robust table reasoning. For reference, we provide visualizations of both the initial template tree and its evolved state in Appendix~\ref{app:tree}, illustrating how the tree structure adapts to accommodate different reasoning patterns.



% To evaluate the role of the dynamically updated Template Tree in enhancing Table-Critic's performance, we conducted an ablation study using the WikiTQ and TabFact datasets. The results are presented in Table~\ref{ablation}.

% The Template Tree is critical to the Critic component, as it provides context-aware critique templates tailored to different error types. To isolate its impact, we compared two configurations: (1) a static Template Tree without updates (\textit{w/o} UPDATE) and (2) a dynamically updated Template Tree (\textit{w} UPDATE).

% On WikiTQ, disabling the dynamic updates (\textit{w/o} UPDATE) results in an accuracy of 76.1\%. Enabling the dynamic updating mechanism (\textit{w} UPDATE) improves accuracy to 77.2\%, yielding a gain of \textbf{1.1\%}. Similarly, on TabFact, accuracy increases from 90.8\% to 92.6\%, reflecting a gain of \textbf{1.8\%}. These results demonstrate that dynamic updates allow the Template Tree to adapt to diverse error patterns and evolving reasoning contexts, enabling more precise and effective critiques in subsequent iterations.

% The improvement stems from the Template Tree’s ability to systematically refine critique paths by incorporating new error types and reasoning insights. With each update, the Template Tree expands its repository of templates, allowing the system to address emerging challenges more effectively. This dynamic refinement ensures that the critique process evolves alongside the reasoning tasks, enabling better error correction and reasoning accuracy.

% The ablation study highlights the importance of the dynamically updated Template Tree in driving Table-Critic’s performance improvements. By continuously adapting to new error patterns, this mechanism plays a pivotal role in enhancing Table-Critic's ability to address the challenges of complex table reasoning tasks.

\begin{table}[t]
\centering
% \small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Method} & \textbf{WikiTQ} & \textbf{Tabfact} \\ \midrule
 Table-Critic & 77.2  & 92.6 \\ 
\quad \text{w/o} Self-evolving   & 76.1  & 90.8 \\
                  & \textcolor{blue}{\(\downarrow\)1.1} & \textcolor{blue}{\(\downarrow\)1.8} \\ \bottomrule
\end{tabular}
\vspace{-0.2em}
\caption{The impact of self-evolving mechanism on our template tree.}
\vspace{-0.8em}
\label{tab:ablation}
\end{table}

\section{Conclusion}
In this paper, we propose Table-Critic, a novel multi-agent framework that enhances table reasoning through collaborative criticism and refinement. Our approach introduces four specialized agents working in concert with a self-evolving template tree, effectively addressing the challenges of error identification and correction in complex table reasoning tasks. Extensive experiments demonstrate that our method significantly outperforms existing approaches, achieving substantial improvements across different datasets while maintaining robust performance stability. 

% While these results are promising, future work could explore ways to optimize the computational efficiency and extend the template mechanism for cross-task knowledge transfer.


% In this work, we presented Table-Critic, a multi-turn, stepwise critic framework designed for table reasoning tasks. By introducing a dynamically updated template tree and leveraging iterative refinement, Table-Critic addresses limitations in existing methods, such as the inability to critically analyze intermediate reasoning steps and dynamically adapt to errors. The framework’s components—\textit{Judge}, \textit{Template Tree}, \textit{Critic}, and \textit{Refiner}—work collaboratively to detect and correct errors in reasoning steps, resulting in more accurate and interpretable outcomes.

% Through extensive experiments on the WikiTQ and TabFact datasets, Table-Critic demonstrated significant improvements over state-of-the-art methods, achieving a notable boost in accuracy and showing its ability to handle complex reasoning tasks. These results validate the effectiveness of stepwise critique mechanisms in improving table reasoning performance.

% Table-Critic represents a step forward in making table reasoning more robust and interpretable. Its systematic approach to error analysis and refinement not only enhances accuracy but also sets a strong foundation for further research in reasoning tasks involving structured and semi-structured data.

% \newpage
\section*{Limitations}
Our Table-Critic framework has demonstrated strong performance in enhancing table reasoning through multi-agent collaboration and systematic refinement. While our current implementation focuses primarily on textual table reasoning, the proposed multi-agent critique framework is inherently flexible and can potentially be extended to various other scenarios. For instance, the framework could be adapted to handle multimodal reasoning tasks where tables are combined with images, graphs, or other visual elements. We believe the core principles of our approach—collaborative criticism, iterative refinement, and self-evolving template tree—could contribute to broader applications in complex reasoning tasks beyond the current textual domain.


% \section*{Ethics Statement}
% Our research focuses on improving table reasoning capabilities through multi-agent collaboration, without involving any sensitive personal data or potentially harmful applications. The datasets used in our experiments are publicly available and widely used in the research community. Our framework is designed to enhance the reliability and transparency of AI systems in table understanding tasks, potentially benefiting various real-world applications such as data analysis and information retrieval. We acknowledge that while our method improves reasoning accuracy, it should be used as a complementary tool rather than a complete replacement for human judgment in critical decision-making processes.



% \begin{table}[h]
% \centering
% \caption{Comparative Computational Load of Chain-of-Table vs. Table-Critic}
% \label{tab:my_label}
% \begin{tabular}{ccc}
% \toprule
% & \textbf{Chain-of-Table} & \textbf{Table-Critic} \\
% \midrule
% Input & 74.2M & 61.3M \\
% Output & 1.9M & 2.0M \\
% \bottomrule
% \end{tabular}
% \end{table}

% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{tact}

\appendix
\newpage


\section{Additional Related works}
\textbf{Multi-agent Systems.}
Multi-agent systems have recently demonstrated promising potential in complex reasoning tasks by enabling collaborative problem-solving through specialized agents~\citep{GuoCWCPCW024,pmlr-v235-zhang24au,chen2025c}. These systems typically leverage the complementary strengths of different agents to achieve more robust and effective solutions than single-agent approaches. While existing work has explored multi-agent frameworks in various domains, their application to table reasoning tasks remains largely unexplored.
To our knowledge, our Table-Critic presents the first attempt to introduce a multi-agent framework for table reasoning, where specialized agents collaborate to identify, critique, and refine reasoning steps, offering a novel perspective on addressing the challenges in complex table reasoning tasks.

\section{More Implementation Details}\label{app:algorithm}
In this section, we provide a comprehensive implementation details of our proposed method. For additional insights and more intricate details, we refer the reader to our \textbf{supplementary materials}.

\subsection{Overall Pipeline of Table-Critic}

Table-Critic employs an iterative process to critique and refine the reasoning chain and predicted answer for table reasoning tasks. As described in Algorithm~\ref{alg1}, the process begins with an input table $\mathbb{T}$, a question $q$, an initial reasoning chain $\tau$, and a template tree $\mathcal{T}$. The \texttt{Judge} agent is first invoked to evaluate the correctness of the reasoning chain (Line 2). This evaluation yields the reasoning status $P$, an error analysis $E$, and a routing path $R$ in the template tree.

When the reasoning chain is deemed incorrect ($P = \text{Incorrect}$), Table-Critic proceeds by sampling relevant critique templates $\mathcal{T}_s$ from the template tree using the routing path $R$ (Line 4). These templates are then used by the \texttt{Critic} agent to generate a detailed critique $\mathcal{C}$ and identify the index of the first error step $I$ in the reasoning chain (Line 5). To address the identified errors, the \texttt{Refiner} agent retains the reasoning steps up to step $I$ and refines the chain starting from step $I$, guided by the critique $\mathcal{C}$ (Line 6). The refined reasoning chain $\tau'$ is subsequently re-evaluated by the \texttt{Judge} agent to determine if it is now correct (Line 7).

This refinement loop continues iteratively until the reasoning chain is verified as correct ($P = \text{Correct}$). Once a correct reasoning chain is established, the \texttt{Curator} agent updates the template tree $\mathcal{T}$ by incorporating new critique templates distilled from the refinement history. This update enhances the template tree's ability to support future refinement processes (Line 9).

The final output of Table-Critic is the refined reasoning chain $\tau'$, which represents the accurate and improved solution to the table reasoning task. By systematically identifying and addressing errors in a collaborative multi-step process, Table-Critic ensures robust and precise refinement of reasoning chains and answers.


\begin{algorithm*}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{The overall pipeline of Table-Critic}
\label{alg1}
\begin{algorithmic}[1]
\REQUIRE Table $\mathbb{T}$, question $q$, initial reasoning chain $\tau$, the template tree $\mathcal{T}$.
\ENSURE The refined reasoning chain $\tau'$.
\STATE $H \leftarrow \emptyset$ {\small \hfill $\triangleright$ Initialize refinement history.}
\STATE $P, E, R \leftarrow \texttt{Judge}(\mathbb{T}, q, \tau, \mathcal{T})$ % \hfill {\small $\triangleright$ Judge determines the reasoning status $P$, error analysis $E$, and routing path $R$}
\WHILE{$P = \text{Incorrect}$}
    \STATE $\mathcal{T}_s \leftarrow$ Sample Templates using $R$ in the $\mathcal{T}$ %\hfill {\small $\triangleright$ Sample relevant critique templates $\mathcal{T}_s$ from the template tree}
    \STATE $\mathcal{C}, I \leftarrow \texttt{Critic}(\mathbb{T}, q, \tau, \mathcal{T}_s)$ \hfill {\small $\triangleright$ Generating critique $\mathcal{C}$ and identify the index of first error step $I$.}
    \STATE $\tau_p \leftarrow \tau[:I]$ \hfill {\small $\triangleright$ Retain the partial reasoning steps up to step $I$}
    \STATE $\tau' \leftarrow \texttt{Refiner}(\mathbb{T}, q, \tau_p, \mathcal{C})$ \hfill {\small $\triangleright$ Refine the reasoning chain.}
    \STATE $H\leftarrow H \cup \{\mathbb{T}, q, \tau, \tau', \mathcal{C}\}$ \hfill {\small $\triangleright$ Update history.}
    \STATE $P, E, R \leftarrow \texttt{Judge}(\mathbb{T}, q, \tau', \mathcal{T})$ \hfill {\small $\triangleright$ Re-evaluates the updated reasoning chain}
\ENDWHILE
\STATE $\mathcal{T} \leftarrow \texttt{Curator}(\mathcal{T}, H)$ \hfill {\small $\triangleright$ Update the template tree $\mathcal{T}$ to facilitate future refinement.}
\RETURN Refined reasoning chain $\tau'$ and new template tree $\mathcal{T}$.
\end{algorithmic}
\end{algorithm*}

% \begin{algorithm*}
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}
% \caption{Table-Critic}
% \label{alg1}
% \begin{algorithmic}[1]
% \REQUIRE $T, Q, R, \hat{A}$ \hfill {\small $\triangleright$ Table $T$, Question $Q$, Reasoning steps $R$, Predicted answer $\hat{A}$}
% \STATE $judge \leftarrow \texttt{Judge}(T, Q, \hat{A})$ \hfill {\small $\triangleright$ Determine whether the $\hat{A}$ is correct}
% \WHILE{$judge$ is incorrect}
% \STATE $template \leftarrow \texttt{TemplateTree}(T, Q, R, \hat{A})$ \hfill {\small $\triangleright$ Generate a critique template}
% \STATE $critique, i \leftarrow \texttt{Critic}(template, T, Q, R, \hat{A})$ \hfill {\small $\triangleright$ Generate the critique and the first error step $i$} 
% \\ \hfill {\small $\triangleright$ (where $i$ is the step number) based on the template}
% \STATE $R, \hat{A} \leftarrow \texttt{Refiner}(critique, i, T, Q, R ,\hat{A})$ \hfill {\small $\triangleright$ Update the reasoning steps $R$ and the predicted answer $\hat{A}$}
% \\ \hfill {\small $\triangleright$ from the first error step}
% \STATE $judge \leftarrow \texttt{Judge}(T, Q, \hat{A})$ \hfill {\small $\triangleright$ Determine again whether the updated answer $\hat{A}$ is correct}
% \ENDWHILE
% \ENSURE $\hat{A}$ \hfill {\small $\triangleright$ Output the refined answer $\hat{A}$}
% \end{algorithmic}
% \end{algorithm*}

\subsection{LLM Servers}
Our approach implements agent behaviors through in-context learning, requiring no extensive training procedures. We deploy multiple LLM servers, including Qwen2.5-72B-Instruct and LLaMA3.3-70B-Instruct through the \href{https://docs.sglang.ai/}{\texttt{SGLang}} inference engine, and GPT-4o-mini via its provided API service. While the choice between fine-tuning and in-context learning remains an open question, it is not the primary focus of our work. Following prior studies~\citep{wang2024chain,critic-cot}, we adopt in-context learning as our implementation strategy for its simplicity and effectiveness.


\section{Detailed Computational Cost Analysis}
\label{app:computation}

\begin{table*}[h]
  \centering
  \begin{tabular}{lccccccc}
    \toprule
    Dataset & 
    \multicolumn{3}{c}{Chain-of-Table} & 
    \multicolumn{3}{c}{Table-Critic} & 
    Cost Ratio \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-8}
    & Input (M) & Output (M) & Total (M) & Input (M) & Output (M) & Total (M) & (TC/CoT) \\
    \midrule
    WikiTQ & 73.5 & 1.6 & 19.6 & 135.5 & 3.8 & 36.7 & 1.87× \\
    TabFact & 29.3 & 0.6 & 7.8 & 62.1 & 20.4 & 17.1 & 2.19× \\
    \bottomrule
  \end{tabular}
  \caption{Computational Cost Comparison Between Chain-of-Table and Table-Critic (Token Counts in Millions)}
  \label{cost_comparison}
\end{table*}

% In this appendix, we provide a detailed computational cost analysis for the proposed method, Table-Critic, in comparison to the baseline method, Chain-of-Table. The analysis is based on the computational cost definition provided in Section \ref{methodology} and uses the pricing model of Qwen2.5-72B-Instruct.




% \subsection{Computational Cost Definition}
% \label{methodology}

This appendix evaluates the computational cost of Table-Critic relative to the baseline Chain-of-Table method. The computational cost is analyzed for two datasets, WikiTQ and TabFact, based on the number of input and output tokens required. All token counts are expressed in millions (M), and the cost ratio reflects the relative cost of Table-Critic compared to Chain-of-Table.


\subsection{Computational Cost Definition}
\label{methodology}

The computational cost of a prompt-based method is defined as follows:
\begin{equation}
    N_{\text{total}} = N_{\text{in}} \cdot \left(\frac{P_{\text{in}}}{P_{\text{in}} + P_{\text{out}}}\right) + N_{\text{out}} \cdot \left(\frac{P_{\text{out}}}{P_{\text{in}} + P_{\text{out}}}\right),
\end{equation}
where $N_{\text{in}}$ and $N_{\text{out}}$ represent the number of input and output tokens, and $P_{\text{in}}$ and $P_{\text{out}}$ denote the costs per token for input and output, respectively. Based on the pricing model of Qwen2.5-72B-Instruct, $P_{\text{in}} = 0.004$ CNY per thousand tokens and $P_{\text{out}} = 0.012$ CNY per thousand tokens.

Using the above values, the normalized cost weights are:
\begin{align*}
    \text{Input Weight} &= \frac{P_{\text{in}}}{P_{\text{in}} + P_{\text{out}}} = 0.25, \\
    \text{Output Weight} &= \frac{P_{\text{out}}}{P_{\text{in}} + P_{\text{out}}} = 0.75.
\end{align*}

Substituting these weights, the formula simplifies to:
\begin{equation}
    N_{\text{total}} = 0.25 \cdot N_{\text{in}} + 0.75 \cdot N_{\text{out}}.
\end{equation}

\subsection{Dataset-Specific Computational Cost Analysis}
% \label{computation}

The computational cost of Table-Critic is compared against Chain-of-Table for the WikiTQ and TabFact datasets. Detailed token counts and cost ratios are shown in Table~\ref{cost_comparison}.

% \subsubsection{WikiTQ}
% \label{computation_wikitq}

On the WikiTQ dataset, Chain-of-Table incurs a total computational cost of $19.6$M, with $73.5$M input tokens and $1.6$M output tokens. In contrast, Table-Critic requires $135.5$M input tokens and $3.8$M output tokens, resulting in a total cost of $36.7$M. This corresponds to a cost ratio of $1.87\times$, indicating that Table-Critic is approximately $1.87$ times more computationally expensive than Chain-of-Table on this dataset.

% The higher computational cost for Table-Critic reflects its dynamic multi-agent refinement strategy, which involves additional token generation and reasoning steps. Despite the increased cost, the substantial performance gains achieved on WikiTQ justify the resource expenditure, as shown in Section~\ref{experiments}.

% \subsubsection{TabFact}
% \label{computation_tabfact}

On the TabFact dataset, Chain-of-Table incurs a total computational cost of $7.8$M, with $29.3$M input tokens and $0.6$M output tokens. Table-Critic, on the other hand, requires $62.1$M input tokens and $20.4$M output tokens, resulting in a total cost of $17.1$M. This corresponds to a cost ratio of $2.19\times$, indicating that Table-Critic is approximately $2.19$ times more computationally expensive than Chain-of-Table.

% The increased cost on TabFact is primarily due to the higher volume of output tokens generated during Table-Critic's iterative refinement process. However, the performance improvement on TabFact highlights the effectiveness of Table-Critic in handling complex reasoning tasks, justifying the additional computational cost.


% The computational cost of a prompt-based method is defined as follows:
% \begin{equation}
%     N_{\text{total}} = N_{\text{in}} \cdot \left(\frac{P_{\text{in}}}{P_{\text{in}} + P_{\text{out}}}\right) + N_{\text{out}} \cdot \left(\frac{P_{\text{out}}}{P_{\text{in}} + P_{\text{out}}}\right),
% \end{equation}
% where $N_{\text{in}}$ and $N_{\text{out}}$ represent the number of input and output tokens, respectively, and $P_{\text{in}} = 0.004$ CNY per thousand tokens and $P_{\text{out}} = 0.012$ CNY per thousand tokens denote the pricing for input and output tokens.

% Using these values, the normalized cost weights for input and output tokens are calculated as:
% \begin{align*}
%     \text{Input Weight} &= \frac{P_{\text{in}}}{P_{\text{in}} + P_{\text{out}}} = 0.25, \\
%     \text{Output Weight} &= \frac{P_{\text{out}}}{P_{\text{in}} + P_{\text{out}}} = 0.75.
% \end{align*}

% Thus, the computational cost formula simplifies to:
% \begin{equation}
%     N_{\text{total}} = 0.25 \cdot N_{\text{in}} + 0.75 \cdot N_{\text{out}}.
% \end{equation}

% \subsection{Dataset-Specific Computational Cost Comparison}
% We evaluate the computational cost of Table-Critic relative to Chain-of-Table on two datasets: WikiTQ and TabFact. The detailed breakdown of input tokens, output tokens, and total computational costs is provided in Table~\ref{cost_comparison}. All token counts are expressed in millions (M).



% \subsubsection{WikiTQ Dataset}
% \label{computation_wikitq}
% On the WikiTQ dataset, Chain-of-Table incurs a total computational cost of $19.6$M, with $73.5$M input tokens and $1.6$M output tokens. In contrast, Table-Critic requires significantly more resources, with $135.5$M input tokens and $3.8$M output tokens, resulting in a total cost of $36.7$M. This corresponds to a cost ratio of $1.87\times$, indicating that Table-Critic is approximately $1.87$ times more computationally expensive than Chain-of-Table on this dataset.

% The higher computational cost of Table-Critic can be attributed to its more complex reasoning process, which involves additional token generation and processing. Despite the increased cost, the performance gains achieved by Table-Critic on WikiTQ justify the additional resource expenditure, as demonstrated in Section~\ref{experiments}.

% \subsubsection{TabFact Dataset}
% \label{computation_tabfact}
% On the TabFact dataset, Chain-of-Table incurs a total computational cost of $7.8$M, with $29.3$M input tokens and $0.6$M output tokens. Table-Critic, on the other hand, requires slightly more resources, with $30.0$M input tokens and $0.7$M output tokens, resulting in a total cost of $8.0$M. This corresponds to a cost ratio of $1.03\times$, indicating that Table-Critic is only marginally more computationally expensive than Chain-of-Table on this dataset.

% The minimal increase in computational cost reflects the efficiency of Table-Critic in handling tasks with simpler reasoning requirements. The near-unity cost ratio suggests that Table-Critic achieves competitive performance without significant additional resource consumption.

\section{Self-evolving Template Tree}\label{app:tree}
Figure~\ref{fig:tree} illustrates the Self-evolving process of the Template Tree. In the initial stage (Figure~\ref{fig:tree}a), the tree contains only two broad categories of errors: Sub-table Error and Final Query Error, each representing a high-level abstraction of error types. Through the self-evolving mechanism, the tree dynamically expands and refines its structure to accommodate more fine-grained error types, as shown in the evolved tree (Figure~\ref{fig:tree}b).

It is important to note that the Evolved Tree is considerably larger in practice, containing a more extensive hierarchy of error types. However, for clarity, only a subset of the evolved structure is displayed here. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{tree.pdf}
  \caption{An example of self-evolving mechanism in our Template Tree.}
  \label{fig:tree}
\end{figure*}

\section{Prompts and Case Study}
\label{app:prompts}

This appendix provides comprehensive instructions and illustrative examples for three intelligent agents: the Judge Agent, the Critic Agent, and the Refiner Agent. These agents are designed to collaboratively evaluate and refine reasoning processes applied to table-based questions. Figures~\ref{fig:judgmentinstructions} and~\ref{fig:judgmentexample} offer detailed guidance for the Judge Agent, including step-by-step procedures to assess the validity of reasoning steps, pinpoint errors, and categorize conclusions (e.g., correct, incorrect with identified error route, or random error). Figures~\ref{fig:critiqueinstructions} and~\ref{fig:critiqueexample} explain how the Critic Agent systematically evaluates each reasoning step, highlights the first incorrect step, and provides constructive critiques. Additionally, Figure~\ref{fig:refinementexample} introduces the Refiner Agent, demonstrating how critiques are utilized to refine reasoning steps, ensuring accurate and complete solutions.


\begin{figure*}[h]
\centering
\small
\begin{tcolorbox}[colframe=black, colback=white, coltitle=black, arc=4mm]
\parbox{15cm}{\ttfamily
You are an intelligent judge tasked with evaluating the correctness of a given Prediction Answer. If the Prediction Answer is incorrect, identify which step within the reasoning process is incorrect and subsequently locate the corresponding error type within the error tree: \par
1. Original Table: The raw table data. \par
2. Question: The question pertaining to the table data. \par
3. Reasoning Steps: A step-by-step process of sub-table transformations and extractions based on the following functions. \par
\ \ - f\_add\_column(): Adds a new column to the table. \par
\ \ - f\_select\_row(): Selects specific rows based on the question. \par
\ \ - f\_select\_column(): Removes irrelevant columns from the table. \par
\ \ - f\_group\_column(): Groups rows based on the values in a specific column. \par
\ \ - f\_sort\_column(): Sorts rows based on the values in a specified column. \par
4. Prediction Answer: The answer derived from the final sub-table. \par \par
\ \par
\textbf{Instruction:} \par
1. \textbf{Explanation:} Conduct an explanation of why the Prediction Answer is correct or incorrect. If it is incorrect, then conduct an analysis of each reasoning step's validity. \par
2. \textbf{Conclusion:} \par
\ \ - If the Prediction Answer is correct, conclude with `Conclusion: [Correct]'. \par
\ \ - If the Prediction Answer is incorrect, conclude with either `Conclusion: [Incorrect] (ERROR ROUTE)' or `Conclusion: [Incorrect] (random)'. \par
\ \ - Use `(ERROR ROUTE)' to indicate the specific path in the error tree that represents the error. \par
\ \ - If no such route can be identified, use `(random)' instead. \par
}
\end{tcolorbox}
\caption{Instructions for the Judge Agent. These instructions outline the procedure for verifying the correctness of a predicted answer and identifying errors within the reasoning process.}
\label{fig:judgmentinstructions}
\end{figure*}


\begin{figure*}[h]
\centering
\small
\begin{tcolorbox}[colframe=black, colback=white, coltitle=black, arc=4mm]
\parbox{15cm}{\ttfamily
Original Table: \par
/* \par
col   : res. | record | opponent         | method                        | event                                        | date               | round | time | location                                | notes \par
row 1 : win  | 12-3   | mike hayes       | ko (punch)                    | ksw 25: khalidov vs. sakurai                 | december 7, 2013   | 1     | 1:12 | wrocław, poland \par
row 2 : win  | 11–3   | nick moghadden   | tko (punches)                 | bellator 99                                  | september 13, 2013 | 1     | 3:22 | temecula, california, united states     | bellator debut \par
row 3 : loss | 10–3   | guto inocente    | decision (unanimous)          | strikeforce: barnett vs. cormier             | may 19, 2012       | 3     | 5:00 | san jose, california, united states     | light heavyweight debut \par
row 4 : win  | 10–2   | brett albee      | tko (strikes)                 | strikeforce: diaz vs. daley                  | april 9, 2011      | 1     | 1:46 | san diego, california, united states \par
row 5 : loss | 9–2    | lavar johnson    | ko (punches)                  | strikeforce challengers: bowling vs. voelker | october 22, 2010   | 1     | 2:17 | fresno, california, united states \par
row 6 : win  | 9–1    | eddie sapp       | submission (rear-naked choke) | native fighting championship 6               | august 14, 2010    | 1     | 2:01 | campo, california, united states \par
row 7 : loss | 8–1    | cody goodale     | decision (unanimous)          | gladiator challenge: maximum force           | april 25, 2010     | 3     | 5:00 | san jacinto, california, united states \par
*/ \par \par
Question: \par
how many loses did virgil zwicker have? \par \par
Reasoning Steps: \par
Step 1: Select relevant rows. \par
The question aims to find out the number of losses Virgil Zwicker had based on the provided fight records. We need to count the rows where the "res." column shows "loss". \par
So we use f\_select\_row(row 3, row 5, row 7). \par \par
Step 2: Filter out useless columns. \par
similar words of the question link to columns: \par
loses -> record \par
column value of the question link to columns: \par
10–3 -> record \par
semantic sentence of the question link to columns: \par
None \par
So we use f\_select\_column(record). \par \par
Step 3: After using f\_select\_row(row 3, row 5, row 7) and f\_select\_column(record), we obtain the sub table: \par
/* \par
col   : record \par
row 1 : 10–3 \par
row 2 : 9–2 \par
row 3 : 8–1 \par
*/ \par
The provided table shows records of wins and losses but does not specify who the records belong to. However, if we assume that the table represents Virgil Zwicker's records, then we can deduce the number of losses from each record. The losses in the records are 3, 2, and 1 respectively. If we consider each row as a separate season or period, then the total number of losses would be the sum of these losses: 3 + 2 + 1 = 6. \par \par
Prediction Answer: \par
6 \par \par
\ \par
\textbf{Explanation:} \par
Step 1 correctly identifies the relevant rows where the "res." column indicates a loss. The rows selected are indeed the ones where the outcome is a loss. Step 1 is correct. \par
Step 2 incorrectly filters out the columns. The question asks for the number of losses, but the filtering step selects only the 'record' column, which combines wins and losses in a single string (e.g., "10–3"). This does not directly provide the number of losses. Instead, the 'res.' column should be used to count the losses directly. Step 2 is incorrect. \par \par
\ \par
\textbf{Conclusion:} [Incorrect] (sub-table error -> column error -> <END>)
}
\end{tcolorbox}
\caption{Example of Judge Agent's Analysis and Error Detection. This example illustrates how the Judge Agent evaluates reasoning steps, identifies errors, and determines the correctness of a predicted answer.}
\label{fig:judgmentexample}
\end{figure*}

\begin{figure*}[h]
\centering
\small
\begin{tcolorbox}[colframe=black, colback=white, coltitle=black, arc=4mm]
\parbox{15cm}{\ttfamily
You are an intelligent critic tasked with determining which step of the table reasoning is incorrect based on the following information: \par
1. Original Table: The raw table data. \par
2. Question: The question pertaining to the table data. \par
3. Reasoning Steps: A step-by-step process of sub-table transformations and extractions based on the following functions. \par
\ \ - f\_add\_column(): Adds a new column to the table. \par
\ \ - f\_select\_row(): Selects specific rows based on the question. \par
\ \ - f\_select\_column(): Removes irrelevant columns from the table. \par
\ \ - f\_group\_column(): Groups rows based on the values in a specific column. \par
\ \ - f\_sort\_column(): Sorts rows based on the values in a specified column. \par
4. Prediction Answer: Final derived answer following the reasoning chain. \par \par
\ \par
\textbf{Instruction:} \par
1. \textbf{Step-wise Analysis}: Conduct an evaluation of each reasoning step's validity. The step that is unnecessary but does not affect the answer is considered correct. \par
2. \textbf{Analysis Categories}: \par
\ \ - For correct steps: Provide validation reasoning and mark as Step <NUM> is correct. \par
\ \ - For incorrect steps: Detail the logical flaws and mark as Step <NUM> is incorrect. \par
\ \ - You should stop at the first incorrect step. \par
3. \textbf{Conclude this critique}: Summarize this critique with an explicit conclusion. \par
4. \textbf{Conclusion Categories}: \par
\ \ - Conclude with 'Conclusion: [Incorrect] Step <NUM>'. \par
}
\end{tcolorbox}
\caption{Instructions for the Critic Agent. These instructions guide stepwise evaluation of reasoning steps.}
\label{fig:critiqueinstructions}
\end{figure*}


\begin{figure*}[h]
\centering
\small
\begin{tcolorbox}[colframe=black, colback=white, coltitle=black, arc=4mm]
\parbox{15cm}{\ttfamily
Original Table: \par
/* \par
col   : tie | home team              | score | away team          | attendance \par
row 1 : 1   | aylesbury united       | 2-2   | windsor \& eton     | 847 \par
row 2 : 2   | burscough              | 5-0   | wakefield \& emley  | 437 \par
row 3 : 3   | dover athletic         | 0-3   | forest green roves | 932 \par
row 4 : 4   | farnborough town       | 2-0   | halifax town       | 863 \par
row 5 : 5   | gloucester city        | 1-1   | southport          | 1,237 \par
row 6 : 6   | havant \& waterlooville | 3-0   | hayes              | 456 \par
row 7 : 7   | margate                | 0-2   | tamworth           | 971 \par
row 8 : 8   | yeovil town            | 2-1   | northwich victoria | 4,469 \par
*/ \par \par
Question: \par
how many games attendance was under 1000? \par \par
Reasoning Steps: \par
Step 1: Select relevant rows. \par
The question aims to find out how many games had an attendance of less than 1000 people. To answer this, we need to count the number of rows where the attendance column shows a number less than 1000. So we use f\_select\_row(row 1, row 2, row 3, row 4, row 6, row 7). \par \par
Step 2: Filter out useless columns. \par
similar words of the question link to columns : \par
under 1000 -> attendance \par
column value of the question link to columns : \par
1000 -> attendance \par
semantic sentence of the question link to columns : \par
None \par
So we use f\_select\_column(attendance). \par \par
Step 3: After using f\_select\_row(row 1, row 2, row 3, row 4, row 6, row 7) and f\_select\_column(attendance), we obtain the sub table: \par
/* \par
col   : attendance \par
row 1 : 847 \par
row 2 : 437 \par
row 3 : 932 \par
row 4 : 863 \par
row 5 : 456 \par
row 6 : 971 \par
*/ \par
The games with attendance under 1000 were rows 1, 2, 4, and 5. Therefore, there were 4 games with attendance under 1000. \par \par
Prediction Answer: \par
4 \par \par
\ \par
\textbf{Critique:} \par
Step 1 correctly selects the rows that are relevant for further analysis regarding the attendance being under 1000. It includes all the rows where the attendance values might potentially be less than 1000, which are row 1, row 2, row 3, row 4, row 6, and row 7. This step makes the proper initial identification based on the condition stated in the question. Step 1 is correct. \par
Step 2 filters out the columns effectively by choosing the "attendance" column. Since the question is focused on the attendance values to determine which are under 1000, this step narrows down the data to just what's needed for the subsequent steps. Step 2 is correct. \par
Step 3 makes an error in counting the number of games with attendance under 1000. Looking at the sub-table from Step 3, all the rows shown (row 1 with 847, row 2 with 437, row 3 with 932, row 4 with 863, row 5 with 456, row 6 with 971) have attendance values that are less than 1000. Step 3 is incorrect. \par 
\ \par
\textbf{Conclusion:} [Incorrect] Step 3 \par
}
\end{tcolorbox}
\caption{Example of Critic Agent's Critique. This example demonstrates how to evaluate reasoning steps and identify errors in the reasoning chain.}
\label{fig:critiqueexample}
\end{figure*}


\begin{figure*}[h]
\centering
\small
\begin{tcolorbox}[colframe=black, colback=white, coltitle=black, arc=4mm]
\parbox{15cm}{\ttfamily
Now, we have produced part of the Function Chain, but gained a critique. \par
Function Chain: f\_select\_row(row 1) \par
After step 1 (f\_select\_row(row 1)), we obtain the sub-table: \par
/* \par
\ \ col : date introduced | class 1 (e.g. motorbike) | class 2 (e.g. car) | class 3 (e.g. car with trailer) | class 4 (e.g. van) | class 5 (e.g. hgv) \par
\ \ row 1 : 23 july 2004 | £1.00 | £2.00 | £5.00 | £5.00 | £6.00 \par
*/ \par
Question: on what date did the toll for class 1 first go above 2.00? \par
Critique: \par
Step 1 is incorrect. The selected row (row 2) has the toll for class 1 set at £1.00, which is not above £2.00. The first row where the toll for class 1 exceeds £2.00 is row 3, dated 16 august 2004. Therefore, the selection of row 2 is incorrect. \par
\ \par
Based on the \textbf{critique}, please continue to produce a complete and correct Function Chain. \par
/* \par
\ \ col : date introduced | class 1 (e.g. motorbike) | class 2 (e.g. car) | class 3 (e.g. car with trailer) | class 4 (e.g. van) | class 5 (e.g. hgv) \par
\ \ row 1 : 9 december 2003 | £1.00 | £2.00 | £5.00 | £5.00 | £10.00 \par
\ \ row 2 : 23 july 2004 | £1.00 | £2.00 | £5.00 | £5.00 | £6.00 \par
\ \ row 3 : 16 august 2004 | £2.00 | £3.00 | £6.00 | £6.00 | £6.00 \par
\ \ row 4 : 14 june 2005 | £2.50 | £3.50 | £7.00 | £7.00 | £7.00 \par
\ \ row 5 : 1 january 2008 | £2.50 | £4.50 | £8.00 | £9.00 | £9.00 \par
\ \ row 6 : 1 january 2009 | £2.70 | £4.70 | £8.40 | £9.40 | £9.40 \par
\ \ row 7 : 1 march 2010 | £2.70 | £5.00 | £9.00 | £10.00 | £10.00 \par
\ \ row 8 : 1 march 2011 | £3.00 | £5.30 | £9.60 | £10.60 | £10.60 \par
\ \ row 9 : 1 march 2012 | £3.00 | £5.50 | £10.00 | £11.00 | £11.00 \par
*/ \par
Question: on what date did the toll for class 1 first go above 2.00? \par
The next operation must be one of f\_add\_column(), f\_select\_row(), f\_select\_column(), f\_group\_column(), or f\_sort\_column(). \par
\ \par
\textbf{Function Chain:} \par
f\_select\_row(row 3) \par
}
\end{tcolorbox}
\caption{Example of Refiner Agent's refinement. This example demonstrates how the critique is used to refine the Function Chain to accurately answer the question.}
\label{fig:refinementexample}
\end{figure*}

% \subsection{Discussion}
% The computational cost analysis highlights the trade-off between performance and resource consumption for Table-Critic. While it incurs higher costs on complex reasoning tasks (e.g., WikiTQ), the performance gains justify the additional expenditure. On simpler tasks (e.g., TabFact), the cost increase is negligible, making Table-Critic a versatile and efficient choice across diverse scenarios.

% Future work will explore strategies to optimize the computational efficiency of Table-Critic, particularly for high-complexity tasks, without compromising its performance advantages.


% \onecolumn
% \begin{figure}[h]
% \centering
% \begin{tcolorbox}[colframe=black, colback=white, coltitle=black, arc=4mm]
% \parbox{15cm}{\ttfamily
% Use f\_select\_column() to filter out useless columns in the table according to information in the statement and the table. \par
% \{ \par
% \ \ "table\_caption": "south wales derby", \par
% \ \ "columns": ["competition", "total matches", "cardiff win", "draw", "swansea win"], \par
% \ \ "table\_column\_priority": [ \par
% \ \ \ \ ["competition", "league", "fa cup", "league cup"], \par
% \ \ \ \ ["total matches", "55", "2", "5"], \par
% \ \ \ \ ["cardiff win", "19", "0", "2"], \par
% \ \ \ \ ["draw", "16", "27", "0"], \par
% \ \ \ \ ["swansea win", "20", "2", "3"] \par
% \ ] \par
% \} \par
% Statement: there are no Cardiff wins that have a draw greater than 27. \par
% Similar words link to columns: \par
% no cardiff wins → cardiff win \par
% The answer is: \par
% f\_select\_column([cardiff win, draw]) \par
% }
% \end{tcolorbox}
% \caption{Demos used for GenerateArgs(T,Q,f\_select\_column). We use the regular expression: f\_select\_column([(.*)]) to extract the arguments from the generated text.}
% \label{fig:generateargs}
% \end{figure}
\end{document}
