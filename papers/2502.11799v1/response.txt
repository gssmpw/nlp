\section{Related Work}
\textbf{Table Reasoning.}
Table reasoning, which requires joint understanding of semi-structured tables and questions, has evolved through several paradigms. Early approaches focused on developing specialized models through fine-tuning \textbf{Radford et al., "Improving Language Understanding by Generative Models"} ____, while recent work has shifted towards leveraging large language models (LLMs) in few-shot learning _____. To handle complex reasoning tasks, decomposition-based methods have emerged as a promising direction. These methods break down complex tasks into manageable steps, either through program execution ____ or context-aware table partitioning _____. However, a critical limitation of existing approaches is their inability to effectively critique and refine intermediate reasoning steps, leading to error propagation.
In contrast, our Table-Critic framework addresses this limitation by introducing systematic critique and refinement mechanisms throughout the reasoning process.

% Table reasoning involves reasoning over semi-structured tables and unstructured questions. Early approaches focused on fine-tuning specialized models ____ _, while recent work leverages LLMs in few-shot settings ____ _. Decomposition-based methods, such as Binder ____ , Dater ____ , and Chain-of-Table ____ , have shown promise by breaking down tasks into manageable steps, such as program execution or context-aware table partitioning. However, these methods lack mechanisms to critique and refine intermediate steps, leading to error propagation. Our method, Table-Critic, addresses this gap by introducing stepwise critique and refinement to improve reasoning accuracy.

\textbf{Self-Reflection.}
Recent studies have revealed that while LLMs possess inherent self-reflection capabilities, they often suffer from reliability and consistency issues _____. Simply enforcing self-reflection can be counterproductive, as models tend to either rationalize their errors or excessively critique correct reasoning steps _____. To address these limitations, our Table-Critic introduces a structured approach through: (1) a multi-agent framework where specialized agents collaborate to provide targeted critiques, and (2) a self-evolving template tree that systematically accumulates and organizes critique knowledge. This design effectively overcomes the inherent limitations of LLMs' reflection capabilities while maintaining reliable and consistent error identification.


% LLMs possess some self-reflection capabilities, but their critiques often lack reliability and consistency _____. Enforcing self-reflection can introduce biases, such as rationalizing errors or over-criticizing correct reasoning _____. To tackle these issues, Table-Critic employs a multi-agent framework that iteratively critiques reasoning steps using specialized agents and a self-evolving template tree. This approach systematically refines errors and accumulates critique knowledge, offering a novel solution for robust table reasoning.

% \textbf{Multi-agent Systems.}
% Multi-agent systems have recently demonstrated promising potential in complex reasoning tasks by enabling collaborative problem-solving through specialized agents _____. While these systems have been explored in various domains, their application to table reasoning remains unexplored. Our work presents the first attempt to leverage multi-agent collaboration for table reasoning, where specialized agents work together to critique and refine the reasoning process.


% \subsection{Table Reasoning}
% Table reasoning tasks involve reasoning over semi-structured tables and unstructured questions, with research evolving from fine-tuning specialized models ____ _ to leveraging LLMs in few-shot settings ____. To address the inherent complexity of these tasks, decomposition strategies have gained traction. For example, Binder ____ decompose complex questions into executable sub-programs, such as SQL or Python, enabling finer-grained reasoning. Similarly, Dater ____ and Chain-of-Table ____ dynamically decompose tables using context-aware strategies, enhancing reasoning over intricate tabular data. While these decomposition-based methods have achieved promising results by simplifying reasoning tasks, they lack mechanisms for critically evaluating and refining intermediate steps, leading to error propagation and reduced accuracy. To address this, our method, Table-Critic, introduces a stepwise critic mechanism that dynamically assesses and refines intermediate reasoning steps, mitigating error accumulation and improving overall performance.


% Table reasoning tasks require the ability to reason over both unstructured questions and semi-structured tables. Research in this area has evolved from fine-tuning specialized models ____ _ to leveraging LLMs in few-shot settings ____, exploiting the growing reasoning capabilities of these models.

% \textbf{Question Decomposition.} A prominent approach to table reasoning is decomposing complex questions into simpler, more manageable sub-questions. Binder ____ generates SQL or Python programs that decompose a primary question into multiple sub-questions and modifies the program statements by calling LLMs as APIs. Similarly, LEVER ____ also decomposes natural language question into runnable SQL programs and trains code LLMs to verify the programs correctness.
% Decomposing questions into smaller sub-questions has proven effective in addressing complex reasoning challenges. However, these methods struggle with difficult cases involving intricate tables, primarily due to the limitations of the single-pass generation process, where LLMs lack the ability to reason over a static table without dynamic adaptation ____.

% \textbf{Table Decomposition.} To address the limitations of processing entire tables, many multi-step reasoning frameworks that transform the tables according to the given question have been proposed ____. Dater ____ decomposes both tables and questions using a "parsing-execution-filling" strategy. Chain-of-Table ____ utilizes a larger set of generic table operations and dynamically generates tables to solve the problems.
% % However, while dynamic decomposition has shown promise, these methods often lack a mechanism for evaluating the intermediate reasoning steps. This limitation may lead to error accumulation or poor generalization.
% While dynamic decomposition enhances the reasoning process, these methods lack mechanisms to evaluate and refine intermediate reasoning steps. Without such evaluation, errors in intermediate steps can accumulate, ultimately undermining the model's overall performance and generalization capabilities.

% Our approach, Table-Critic, introduces a stepwise critic mechanism to the table reasoning tasks, integrating a dynamically updated template tree to critically assess the intermediate steps of reasoning.

% \subsection{Critic}
% Recent studies have investigated the self-reflection capabilities of large language models (LLMs) and their ability to critique their own outputs. While LLMs have demonstrated some potential for self-reflection, it has been observed that their self-assessments often lack reliability and consistency _____. Simply enforcing self-reflection during reasoning tasks can introduce new challenges, such as rationalizing incorrect reasoning or over-criticizing correct reasoning steps _____. To address these limitations, our Table-Critic introduces a structured approach through: (1) a multi-agent framework where specialized agents collaborate to provide targeted critiques, and (2) a self-evolving template tree that systematically accumulates and organizes critique knowledge. This design effectively overcomes the inherent limitations of LLMs' reflection capabilities while maintaining reliable and consistent error identification.


% \begin{algorithm*}
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}
% \caption{Table-Critic}
% \label{alg1}
% \begin{algorithmic}[1]
% \REQUIRE $T, Q, R, P$ \hfill {\small $\triangleright$ Table $T$, Question $Q$, Reasoning steps $R$, Predicted answer $P$}
% \STATE $judge \leftarrow \texttt{Judge}(T, Q, P)$ \hfill {\small $\triangleright$ Determine whether the $P$ is correct}
% \WHILE{$judge$ is incorrect}
% \STATE $template \leftarrow \texttt{TemplateTree}(T, Q, R, P)$ \hfill {\small $\triangleright$ Generate critique templates}
% \STATE $critique, i \leftarrow \texttt{Critic}(template, T, Q, R, P)$ \hfill {\small $\triangleright$ Generate stepwise critique and}
% \STATE \hfill {\small the first error step number $i$} 
% % \\ \hfill {\small $\triangleright$ based on the template}
% \STATE $R \leftarrow R[1:i-1]$ \hfill {\small $\triangleright$ Retain the correct steps from the previous $i-1$ steps}
% \STATE $R, P \leftarrow \texttt{Refiner}(critique, T, Q, R)$ \hfill {\small $\triangleright$ Based on the critique, update the reasoning steps $R$ }
% \\ \hfill {\small and the predicted answer $P$ from the first error step}
% \STATE $judge \leftarrow \texttt{Judge}(T, Q, P)$ \hfill {\small $\triangleright$ Determine again whether the updated answer $P$ is correct}
% \ENDWHILE
% \STATE Update \texttt{TemplateTree}
% \ENSURE $P$ \hfill {\small $\triangleright$ Output the refined answer $P$}
% \end{algorithmic}
% \end{algorithm*}