%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}
\raggedbottom  

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{lipsum,mwe,cuted}
\usepackage{float}
\usepackage{caption}
\usepackage{wrapfig}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[preprint]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{tabularx} % used to handle word wrapping
\usepackage{caption}
% \usepackage{subcaption}
% if you use cleveref..
\usepackage[capitalize, nameinlink]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\eg}{\textit{e.g.}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\method}{DPO-\textit{Shift}\xspace}
\newcommand{\metrm}{\mathrm{DPO}-S}
\newcommand{\fix}{\texttt{fixed}}
\newcommand{\dec}{\texttt{linear\_decrease}}
\newcommand{\inc}{\texttt{linear\_increase}}
\providecommand{\TD}[1]{ {\protect\color{red}{[TODO: #1]}} }
\newcommand{\logwin}{\log \pi_\theta(\yw|\x)}
\newcommand{\logrej}{\log \pi_\theta(\yl|\x)}
\input{math}
\captionsetup[figure]{skip=0pt}

\AtBeginDocument{\setlength\abovedisplayskip{5pt}}
\AtBeginDocument{\setlength\belowdisplayskip{5pt}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{DPO-Shift: Shifting the Distribution of Direct Preference Optimization}

\begin{document}
\twocolumn[ \icmltitle{\method: Shifting the Distribution of Direct Preference Optimization}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
    \icmlauthor{Xiliang Yang \textsuperscript{*}}{1,2}
    \icmlauthor{Feng Jiang}{1}
    \icmlauthor{Qianen Zhang}{1}
    \icmlauthor{Lei Zhao}{3}
    \icmlauthor{Xiao Li}{1}
    % \icmlauthor{Firstname3 Lastname3}{comp}
    % \icmlauthor{Firstname4 Lastname4}{sch}
    % \icmlauthor{Firstname5 Lastname5}{yyy}
    % \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
    % \icmlauthor{Firstname7 Lastname7}{comp}
    % %\icmlauthor{}{sch}
    % \icmlauthor{Firstname8 Lastname8}{sch}
    % \icmlauthor{Firstname8 Lastname8}{yyy,comp}
    %\icmlauthor{}{sch}
    %\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{1}{School of Data Science, The Chinese University of Hong Kong, Shenzhen}
\icmlaffiliation{2}{School of Mathematics, South China University of Technology}
\icmlaffiliation{3}{Institute of Translational Medicine and National Center for Translational Medicine, Shanghai Jiao Tong University}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Xiao Li}{lixiao@cuhk.edu.cn} 

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in 
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{*Most of the work of Xiliang Yang was done when he was with School of Data Science, The Chinese University of Hong Kong, Shenzhen.}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \method to controllably shift the distribution of the chosen probability. Then, we show that \method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at \url{https://github.com/Meaquadddd/DPO-Shift}.
\end{abstract}

\section{Introduction}

There has been a growing interest in guiding large language models (LLMs) to
generate safe and helpful content to align with human values and intentions,
or, taken together, preferences. One of the most important methods in this field is known as Reinforcement Learning from Human Feedback (RLHF) ~\cite{christiano2017deep,
Ouyang2022TrainingLM, stiennon2020learning}. However, multi-stage
optimization procedure is raised in these methods, which includes the training
of a reward model and the policy model to maximize the reward. Such
optimization and computational burden make it challenging to use and analyze,
despite its ability to improve the quality of generated responses ~\cite{bai2022training,
achiam2023gpt, touvron2023llama}.

\textbf{Related Works.} Recently, DPO~\cite{Rafailov2023DirectPO} and its variants \cite{meng2024simpo,azar2024general,tang2024generalized,xu2024contrastive,Ethayarajh2024KTOMA,Park2024DisentanglingLF}
is attracting more and more attention. Given a pair of samples $(\x, \yw, \yl)$ from the dataset, where $\x$ is the prompt, and $\yw$ and $\yl$ represent the chosen and rejected responses, respectively—annotated by strong large language models or humans—the loss of DPO is designed to maximize the margin between the reward of the chosen response and the rejected response for the model $\pitheta$. Being offline algorithms, its simplicity makes DPO more applicable and stable. The main difference between DPO and RLHF lies in the treatment of reward function. DPO proposes to directly parameterize it with the policy model, therefore eliminating the need of training an extra reward model and largely simplifying the training process.

However, it has been reported that both $\logwin$ and $\logrej$ often decrease simultaneously during the training process of DPO; see, e.g., \cite{pal2024smaug, yuan2024advancing, rafailov2024r, tajwar2024preference, pang2024iterative, liu2024provably,razin2024unintentional}. There are several names used to describe such a phenomenon, and we adopt the term ``likelihood displacement'' \cite{razin2024unintentional} in this work. Though DPO still maximizes the reward margin even with this likelihood displacement issue, it remains unfavorable as it causes an unexpected increase in probabilities for responses that are neither preferred nor dispreferred.


Prior work has attributed this phenomenon to limitations in model capacity~\cite{tajwar2024preference}, the presence of multiple training samples or output tokens~\cite{pal2024smaug}, and the initial SFT phase~\cite{rafailov2024r}. Existing studies, such as~\cite{razin2024unintentional}, have provided theoretical insights into addressing this gap and proposed solving the likelihood displacement problem by filtering the datasets.
 

% However, the filtering process is not always feasible in practice,
% and it can cause information loss. The likelihood displacement problem is still
% a challenging issue in the field of preference optimization.

\textbf{Main Contributions.} In this paper, we propose \method, aiming to solve the likelihood displacement issue of DPO, by adding a parameter function $f(\lambda)$ to the rejected reward in the Bradley–Terry (BT) model \cite{Bradley1952RankAO}, which is detailed in \eqref{eq:lambda_dpo}. 

% We show briefly in Figure \ref{fig:teaser} that, by choosing proper $f(\lambda)$, we successfully strike a balance between the distribution of $\logwin$ and reward margin. Specifically, we consider the simple case of $f(\lambda)=\lambda$, which is named \texttt{fixed} in our paper. In the case of $\lambda=1$, \method degenerates to the original DPO. With $\lambda=0.5$ (the first row in Figure \ref{fig:teaser}), we observe an increased chosen probability compared with the DPO (the last row in Figure \ref{fig:teaser}), which is followed by a Fortunately, we can achieve a trade-off between them, by using a larger $\lambda=0.85$, as it is plotted in the second row of Figure \ref{fig:teaser}. 

We briefly illustrate in \Cref{fig:teaser} that, by choosing a proper $f(\lambda)$ in \method, we successfully achieve a balance between the distribution of $\logwin$ and the reward margin. The first row in \Cref{fig:teaser} represents the SFTed model; since the reward margin is not applicable for it, the right plot is concentrated at $0$. The second row corresponds to a specific choice of $f(\lambda)$ of our proposed \method, where we observe an increased chosen probability compared to DPO (depicted in the last row). This improvement is accompanied by only a slight decrease in accuracy of reward margin (i.e., the frequency of $r(\x,\yw)-r(\x,\yl)>0$ on the test set). In fact, we can achieve nearly as high reward margin as that of DPO by choosing $f(\lambda)$ properly; see \Cref{sec:exp_results}.
\vspace{-0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fig/teaser.pdf}
    \vspace{-1cm}
    \caption{\textbf{Left}: Distribution of $\logwin$ and $\logrej$. \textbf{Right}: Kernel density estimation (KDE) for the reward margin $(r(\x,\yw)-r(\x,\yl))$. The reward accuracy, which is the sample mean of $1\{(\boldsymbol{x},\boldsymbol{y}_w,\boldsymbol{y}_l)|r(\x,\yw)-r(\x,\yl)>0\}$ is listed. The three rows are plotted with three models including the SFTed Llama 3-8B, the model trained by one strategy of \method, and the model trained by DPO (from top to bottom), separately. The ranges of the y-axis of all subfigures are the same.}
    \label{fig:teaser}
\end{figure}
\vspace{-0.5cm}


Our main contributions can be summarized as follows.

\begin{itemize}[topsep=0pt,itemsep=-0.2ex,partopsep=0ex]
    \item We propose \method to mitigate the likelihood displacement issue of DPO by controllably shifting the distribution of the chosen probability. This is achieved through a new parameter function $f(\lambda)$ introduced in \method. Our approach is as simple as DPO and does not require modifications to the dataset.
    
    \item We provide a theoretical analysis for the proposed \method without imposing additional assumptions. The analysis guarantees that \method mitigates the likelihood displacement issue while introducing a fundamental trade-off. Specifically, our theory reveals that \method improves the chosen probability $\logwin$ at the cost of reducing the reward margin that DPO seeks to maximize. Furthermore, the trade-off is explicitly controlled by $f(\lambda)$, offering practical guidance on its selection. Our findings suggest that $f(\lambda)$ should be chosen relatively close to $1$ to achieve an improvement in the chosen probability over DPO, while only slightly sacrificing the reward margin. 
    
    Experimentally, we conduct thorough ablation studies to train the Llama 3-8B and Qwen 2-7B models on the UltraFeedback and Capybara-preferences datasets with fine grained choices of $f(\lambda)$. The experiment results corroborate our analysis, clearly demonstrating that the likelihood displacement issue is largely mitigated by \method and the fundamental trade-off exists between the chosen probability and reward margin in \method. 

    \item We use downstream experiments to illustrate the improved performance of \method over DPO. In particular, we train the Llama 3-8B and Qwen 2-7B models on the UltraFeedback dataset. The MT-Bench results show that \method can outperform DPO. To fully demonstrate the superiority that \method can controllably shift the chosen probability, we conduct a designed win rate experiment, and the result shows that \method can consistently outperform DPO. 
\end{itemize}

We believe that our study provides a simple yet efficient approach for mitigating the likelihood displacement issue of DPO.

% Our paper is organized as follows. In \Cref{sec:method}, we introduce the preliminaries, our proposed \method, and the theoretical analysis. In \Cref{sec:exp_setup} and \Cref{sec:exp_results}, we describe the experimental setup and results. Finally, we conclude our work in \Cref{sec:conclusion}.

% \begin{figure}[h]
%     \centering

%     \includegraphics[width=1.1\linewidth]{fig/SFT_vs_DPO.pdf}
%     % \includegraphics[width=\linewidth]{fig/SFTed_model.pdf}

%     % \includegraphics[width=\linewidth]{fig/POed_model.pdf}
%     \caption{\textbf{Left}: Histogram of
%     $\log\pi(\boldsymbol{y}_{w}|\boldsymbol{x})$ and
%     $\log\pi(\boldsymbol{y}_{l}|\boldsymbol{x})$ for SFTed Llama 3-8B~\cite{llama3modelcard},
%     \textbf{Right}: KDE plot for margin of SFTed Llama 3-8B,\ie,
%     $\log\pi(\boldsymbol{y}_{w}|\boldsymbol{x})-\log\pi(\boldsymbol{y}_{l}|\boldsymbol
%     {x})$, both figures are plotted with the test split from the ultrafeedback~\cite{Cui2024UltraFeedbackBL}
%     dataset. From the top to the bottom: Model after SFT stage, Model after SFT and
%     PO stage.}
%     \label{fig:SFT_plot}
% \end{figure}

\section{\method: Formulation and Analysis}
\label{sec:method}
% In this section, we present our prpopsed \method. In \Cref{sec:dpo_pre}, we introduce
% pipeline of direct preference learning \label{sec:method}, including
% formulation of direct preference optimization (DPO) and the likelihood
% displacement problem. In \Cref{sec:theory}, we provide the theoretical
% guarantee for the proposed method. 

\subsection{DPO, likelihood displacement, and \method}
\label{sec:dpo_pre}

Consider the preference dataset $\mathcal{D}_{\text{pref}}=\{(\x,\yw,\yl)\}$, where $\boldsymbol{x}$ is the prompt, $\boldsymbol{y}_{w}$ and $\boldsymbol
{y}_{l}$ are the chosen and rejected responses
for the prompt $\x$, respectively. Given the prompt $\x$, two responses $\y_1$ and $\y_2$ are first generated from models or
directly taken from static dataset, then the chosen $\yw$ and rejected $\yl$ are selected from $\{\y_1,\y_2\}$ by human or other strong language models. 
% Notice that we use
% the term "chosen" and "rejected" to referer to the \textbf{reposnes from the
% dataset} $\mathcal{D}$, and for the general response, we remain to use the
% phase "preferred" and "dispreferred". 
DPO consists of
two steps, including supervised fine-tuning (SFT) and preference optimization.

\textbf{Supervised Fine-tuning (SFT).} In this stage, the LLM $\pitheta$ is trained to maximize the log-likelihood of $\y$ given $\x$ with cross-entropy loss:
\begin{align*}
    \min_{\theta} \ \mathcal{L}_{\text{SFT}}(\pitheta) = -\mathbb{E}_{(\boldsymbol{x},\boldsymbol{y})\sim \mathcal{D}_{\text{SFT}}}\left[\log\pitheta(\boldsymbol{y}|\boldsymbol{x})\right]
\end{align*}
Here, $\mathcal{D}_{\text{SFT}} = \{(\x,\y)\}$ is the normal prompt-response dataset used for auto-regressive language modeling and $\pitheta(\y|\x) = \prod_{i=1}^{|y|} \pitheta(y_i|\x, \y_{1:i-1})$. 
% However, some papers, such as \cite{Hong2024ORPOMP}, claim that this stage can be removed, \ie, the model can be trained in a single stage. Nevertheless, this is beyond the scope of our work, as we consider only two-stage training in this paper. 
For convenience, we refer to the model after the SFT stage as the ``SFTed model".

From the first row of \Cref{fig:teaser}, we conclude that most responses, including both the chosen and rejected ones, are concentrated around the high likelihood area (i.e., likelihood is close to $0$ after taking log) of the distributions of $\log \pitheta(\boldsymbol{y}_{w}|\boldsymbol{x})$ and $\log \pitheta(\boldsymbol{y}_{l}|\boldsymbol{x})$. This phenomenon is reasonable due to the training objective of the SFT stage and the fact that $\y_w$ and $\y_l$ are often semantically similar \cite{tajwar2024preference,Hong2024ORPOMP,pang2024iterative}. 


\textbf{Preference Optimization (PO).} In this stage, DPO
parameterizes the reward model with the LLM $\pi_{\theta}$ via
\begin{align}
    \label{eq:reward:dpo}
    r(\x,\y) = \beta \left(\log \frac{\pi_{\theta}(\y|\x)}{\pi_{\text{ref}}(\y|\x)}+ \log Z(\x)\right),
\end{align}
where $Z(\x)$ is the partition function and $\pi_{\mathrm{ref}}$ is known as the reference model and usually set to be
the SFTed model.

Incorporating this reward function into the Bradley-Terry (BT) model, i.e.,
$\mathbb{P}(\yw > \yl|\x) = \sigma(r(\x,\yw)-r(\x,\yl))$. Then, by maximizing the log-likelihood of $\mathbb{P}(\yw > \yl|\x)$, DPO arrives at the following objective function:
\begin{align}
    \label{eq:org_dpo}
    \min_{\theta} 
     -\mathbb{E}\left[\log\sigma\left(\beta\log\frac{\pi_{\theta}(\boldsymbol{y}_{w}|\boldsymbol{x})}{\pi_{\text{ref}}(\boldsymbol{y}_{w}|\boldsymbol{x})}-\beta\log\frac{\pi_{\theta}(\boldsymbol{y}_{l}|\boldsymbol{x})}{\pi_{\text{ref}}(\boldsymbol{y}_{l}|\boldsymbol{x})}\right)\right].
\end{align}

Here, the expectation is taken over the dataset $\mathcal{D}_{\text{pref}}$. The model after the PO state is called ``POed model".


% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{fig/likeli_disp.png}
%     \caption{training curve of SFTed Llama 3-8B on ultrafeedback \TD{find a better way to extract training curve from wandb}}
%     \label{fig:likeli_disp}
% \end{figure}
\textbf{Likelihood Displacement.} We compare the distributions of the likelihood $\log \pitheta(\boldsymbol{y}_{w}|\boldsymbol{x})$ and $\log \pitheta(\boldsymbol{y}_{l}|\boldsymbol{x})$ between the SFTed model and the POed model by evaluating them on the test dataset. The result is displayed in \Cref{fig:teaser} (first row for SFTed model and third row for DPO). It is easy to observe that the highest likelihood region for both the chosen and rejected responses are decreased dramatically after DPO. Though some other likelihood regions of the chosen and rejected responses have increased after DPO, the averaged likelihood of $\log \pitheta(\boldsymbol{y}_{w}|\boldsymbol{x})$ and $\log \pitheta(\boldsymbol{y}_{l}|\boldsymbol{x})$ over the entire test set is overall decreasing according to our experiment results, 
aligning with the likelihood displacement phenomenon observed in the existing literature \cite{tajwar2024preference,pal2024smaug,razin2024unintentional,pang2024iterative,yuan2024advancing,liu2024provably,rafailov2024r,Hong2024ORPOMP}.  In conclusion, the likelihood displacement occurs not only in the training stage but also in the test dataset, which is counter-intuitive and can be harmful to the model's generalization ability.

An important factor causing the likelihood displacement issue during the PO stage gives rise to the semantic similarity between the chosen $\yw$ and rejected $\yl$ pairs in $\mathcal{D}_{\text{pref}}$, as observed in the existing works; see, e.g., \cite{tajwar2024preference,Hong2024ORPOMP,razin2024unintentional,pal2024smaug}. This is indeed implied by the generation process of contemporary preference datasets. For instance, the UltraFeedback dataset \cite{Cui2024UltraFeedbackBL} is generated by using different LLMs to response to the same prompt $\x$, and then it selects $\yw$ and $\yl$ using GPT4. To demonstrate it, we pick the following examples from UltraFeedback:
\begin{align*}
    & \text{\parbox{\linewidth} {\text{\textbf{Q1}}: ...Select from female and male... Solution:}}\\
    &\text{\textbf{chosen}: Female.}\\
    &\text{\textbf{rejected}: Female.} \\
    & \text{\parbox{\linewidth} {\text{\textbf{Q2}}: Write the right answer to the question based on...}} \\
    &\text{\textbf{chosen}: Dan, the protagonist, got a coke out of the cooler.}\\
    &\text{\textbf{rejected}: Dan got coke out of the cooler.}
\end{align*}
% chosen and rejected pairs from the \href{https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized/viewer/default/train_prefs}{\texttt{train\_prefs}} split \href{https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized/viewer/default/test_prefs}{\texttt{test\_prefs}} split of the UltraFeedback dataset respectively
This partly explains that the model tends to assign
similar probabilities to both responses. In the DPO objective function, it seeks to maximize the margin between the
probability of the chosen and rejected responses even if they are semantically similar. Hence, instead of the ideal case where it maximizes the chosen probability while minimizes the rejected one, it often reduces the probability of both
responses with similar semantic structures, though their margin is enlarged. This leads to the likelihood displacement issue. Consequently, the model favors
responses that are neither chosen nor rejected. 

% In conclusion, this
% problem originates from an aversion to the similarity caused by the training
% dataset, which can be challenging to solve.

% On the other hand, we believe that the model indeed manages to acquire some
% valuable information from the selected response. This can be evidenced by the
% training curve during the PO stage. Despite the reduction in the probability
% of both responses, the model still succeeds in widening the gap between the chosen
% and the rejected responses. This indicates that the model has successfully
% captured certain crucial features of the chosen responses and assigned them
% a high probability. Unfortunately, this increase is counteracted by the
% overall decrease.

\textbf{\method.} To address this counter-intuitive and harmful likelihood displacement issue of DPO, we introduce \method in this work. The motivation behind the proposed method is to alleviate the problem caused by the similarity of the chosen and rejected pairs. As we analyzed previously, the chosen probability decreases accordingly when the DPO objective maximizes the margin between two semantically similar responses. Based on this observation, 
we propose to add a real valued function $0<f(\lambda)<1$ to
the reward of the rejected response. This helps the BT model to rank correctly by reducing the confrontation between two semantically similar responses, potentially mitigating the likelihood displacement issue of DPO. Mathematically,  our proposed formulation is displayed in the following:

\begin{align}
    \min_{\theta} -\mathbb{E}&\left[\log\sigma \left(\beta\log\frac{\pi_{\theta}(\boldsymbol{y}_{w}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{w}|\boldsymbol{x})} \right. \right.\nonumber \\ 
    & \quad \quad \left. \left. -f(\lambda)\cdot\beta\log\frac{\pi_{\theta}(\boldsymbol{y}_{l}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{l}|\boldsymbol{x})}\right)\right].\label{eq:lambda_dpo}
\end{align}

% For simplicity, we omit the less important factor $\log Z(\boldsymbol{x})$ in \eqref{eq:reward:dpo} in the notion of $r(\x,\y)$.

% One simple option for $f(\lambda)$ is $f(\lambda)=\lambda$. Unfortunately, the
% selection of $\lambda$ is not straightforward. In the next section, we will explore
% the choice of $\lambda$ using more sophisticated theoretical tools.

\subsection{Analysis for \method}
\label{sec:theory}

We analyze the effects of \method for two important quantities, including the likelihood of the chosen response $\log \pi_{\theta}(\yw|\x)$ and the indicator function of the reward margin $\mathbf{1}\{(\x,\yw,\yl)|\log \frac{\pi_{\theta}(\yw|\x)}{\pi_{\text{ref}}(\yw|\x)}-\log \frac{\pi_{\theta}(\yl|\x)}{\pi_{\text{ref}}(\yl|\x)} >0\}$. The latter reflects the model's ability to align with human preferences and is implicitly maximized by DPO's objective. We define the two target functions as follows:
\begin{align}
    \omega_{1}(\theta) & =\mathbb{E}\left[\log\pi_{\theta}\left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)\right],\label{eq:ob_cho}      \\
    \omega_{2}(\theta) & =\mathbb{E}\left[\mathbf{1}\left\{\log\frac{\pi_{\theta}(\boldsymbol{y}_{w}|\boldsymbol{x})}{\pi_{\text{ref}}(\boldsymbol{y}_{w}|\boldsymbol{x})} -\log\frac{\pi_{\theta}(\boldsymbol{y}_{l}|\boldsymbol{x})}{\pi_{\text{ref}}(\boldsymbol{y}_{l}|\boldsymbol{x})}>0\right\}\right].\label{eq:ob_acc}
\end{align}
The likelihood displacement issue of DPO enlarges $\omega_2$ while decreases $\omega_1$. To provide an analytical characterization, we alter the discontinuous $\omega_{2}$ and consider its smoothed version
\begin{align}
    \label{eq:ob_smoo_acc}\omega_{2}(\theta)=\mathbb{E}\left[\sigma\left(\gamma\log\frac{\pi_{\theta}(\boldsymbol{y}_{w}|\boldsymbol{x})}{\pi_{\text{ref}}(\boldsymbol{y}_{w}|\boldsymbol{x})} -\gamma\log\frac{\pi_{\theta}(\boldsymbol{y}_{l}|\boldsymbol{x})}{\pi_{\text{ref}}(\boldsymbol{y}_{l}|\boldsymbol{x})}\right)\right],
\end{align}
where $\gamma$ is the smoothing factor. To compare \method with the original DPO, we introduce two functions measuring the gaps between targets after one step of optimization (i.e., updating $\theta_t$ to $\theta_{t+1}$) with different objective functions:
\begin{equation}
    \left[ 
    \begin{aligned}
    &g_{1}(t+1) = \omega_{1}(\theta_{t+1})\Big|_{\text{DPO-Shift}}-\omega_{1}(\theta_{t+1})\Big|_{\text{DPO}},\\
    &g_{2}(t+1) = \omega_{2}(\theta_{t+1})\Big|_{\text{DPO-Shift}}-\omega_{2}(\theta_{t+1})\Big|_{\text{DPO}}.
    \end{aligned}
    \right.
\end{equation}
% Here, $\omega_{1}(\theta_{t+1})\Big|_{\text{DPO-Shift}}$ is the one step gradient update to 
% $\omega_{1}(\theta_{t})$ using
% $\mathcal{L}_{\metrm}(\pi_{\theta},\pi_{\text{ref}})$, while $\omega_{1}(\theta
% _{t+1})\Big|_{\mathrm{DPO}}$ is $\omega_{1}(\theta_{t})$ updated with $\mathcal{L}
% _{\text{DPO}}(\pi_{\theta},\pi_{\text{ref}})$, then 
We characterize the two gap functions
in the following theorem.
\begin{theorem}
    \label{thm:gap} Given $\theta_{t}$ and learning rate $\eta$ and denote
    \begin{align*}
        c(\theta)        & =\gamma\sigma\left(f(\lambda)\cdot\gamma\log\frac{\pi_{\theta}(\boldsymbol{y}_{l}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{l}|\boldsymbol{x})}-\gamma\log\frac{\pi_{\theta}(\boldsymbol{y}_{w}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{w}|\boldsymbol{x})}\right), \\
        \eta_1(\theta) & =\eta \sigma \left(\log\frac{\pi \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)}{\pi_{\mathrm{ref}}{(\boldsymbol{y}_{l}|\boldsymbol{x})}}-\log \frac{\pi \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)}{\pi_{\mathrm{ref}}(\boldsymbol{y_{w}|x})}\right).
    \end{align*}
    We have
    \begin{equation}\label{eq:gap characterization}
    \left[ 
    \begin{aligned}
       & g_{1}(t+1) = (1-f(\lambda)) u_1, \\
       & g_{2}(t+1) = (1-f(\lambda)) u_2.
    \end{aligned}
    \right.
    \end{equation}
    Here, 
    \begin{equation}\label{eq:u factors} 
    \begin{aligned}
       &u_1 = \mathbb{E}\left[c(\theta)\cdot\nabla_{\theta}\log\pi_{\theta}\left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)^{\top}\nabla_{\theta}\log\pi_{\theta}\left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)\right], \\
       & u_2 = \mathbb{E}\left[\eta_{1}(\theta)\left(\nabla_{\theta}\log\pi_{\theta}\left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)^{\top}\nabla_{\theta}\log\pi_{\theta}\left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)\right.\right. \\& \quad \quad \quad \quad \left.\left. - \left\|\nabla_{\theta}\log\pi_{\theta}\left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)\right\|^{2}\right)\right].
    \end{aligned}
    \end{equation}
\end{theorem}
The proof of Theorem 2.1 is deferred to \Cref{app:proof}. It is worth mentioning that our derivation in the proof of \Cref{thm:gap} applies to every single sample, and hence the result in \Cref{thm:gap} applies to $u_1$ and $u_2$ defined using any specific dataset. We state the results in expectation in order to be data independent.

This theorem explains the pros and cons of \method, yielding indications for choosing $f(\lambda)$. We provide detailed discussions below.

\textbf{Fundamental Trade-off.} We are interested in characterizing the sign of the two gap functions using \eqref{eq:gap characterization}. To compute $u_1$ and $u_2$ on a specific $\mathcal{D}_{\text{pref}}$, we define the following sample-based version:
\begin{align*}
    u^{i}_{1} & = c_{i}(\theta)\cdot\nabla_{\theta}\log\pi_\theta \left(\syl|\sx\right)^{\top}\nabla_{\theta}\log\pi_\theta \left(\syw|\sx\right)             \\
    u^{i}_{2} & = \eta_{1}\left(\nabla_{\theta}\log\pi_\theta \left(\syl|\sx\right)^{\top}\nabla_{\theta}\log\pi_\theta \left(\syw|\sx\right)\right. \\
    & \quad \quad \quad \quad \left. - \left\|\nabla_{\theta}\log\pi_\theta \left(\syl|\sx\right)\right\|^{2}\right).
\end{align*}
Then, we compute the sample average $u_1 = \sum_{i}u^{i}_{1} / |\mathcal{D}_{\text{ref}}|$ and $u_2 = \sum_{i}u^{i}_{2} / |\mathcal{D}_{\text{ref}}|$. On the test set of UltraFeedback, when setting $\gamma=1$ and 
$\pi_\theta$ to be the SFTed Llama 3-8B, we obtain that $u_1 = 4.84\times 10^8$. For $u_2$, since $\eta_1$ is bounded, we set it to be 1 and obtain $u_2 = -4.28\times 10^9$. In terms of frequency, $71.4\%$ of $\{u_1^i\}$ are positive and $81.7\%$ of $\{u_2^i\}$ are negative. These results indicate a clear positivity of $u_1$ while a clear negativity of $u_2$. Indeed, positivity of $u_1$ is expected due to the semantic similarity between $\yw$ and $\yl$ in contemporary $\mathcal{D}_{\text{pref}}$.

Since we choose $0<f(\lambda) < 1$, we have $1-f(\lambda) >0$. In this case, $g_1 >0$ as $u_1>0$. This immediately concludes that \method improve the chosen probability (or likelihood) $\log \pi_{\theta}(\yw|\x)$ compared to DPO, solving the undesired likelihood displacement issue. However, there is an explicit trade-off to achieving this improvement. Since $u_2$ can be negative as shown in our test result, $g_2$ can be negative, leading to reduced reward margin of \method compared to DPO. 

In summary, \method improves the chosen probability over DPO, at the cost of decreasing the reward margin. This also yields the indications for choosing $f(\lambda)$, which we describe below.

% Therefore, to evaluate the performance of \method, we need to analyze the
% empricial value of $g_{1}(t+1)/(1-f(\lambda))$ and
% $g_{2}(t+1)/(1-f(\lambda))$. For simplicity, we introduce the following
% notation:

% where $c_{i}$ is an instance of $c$ of samples $(\sx,\syw,\syl)$. Empirically, we can show
% $1/|\mathcal{D}_{\mathrm{ref}}|\sum_{i}u^{i}_{1}(\theta) >0$ and
% $1/|\mathcal{D}_{\mathrm{ref}}|\sum_{i}u^{i}_{2}(\theta) <0$, then we conclude
% that $g_{1}(t+1)>0$ and $g_{2}(t+1)<0$ hold with high probability when $f(\lambda
% )<1$. For the detailed proof, please refer to the Appendix~\ref{app:proof}.

% In this regard, we are able to set $f(\lambda)<1$. Theoretically, it
% is demonstrated that $\lambda$ affects the one-step optimization gap $f_i(t)$ linearly. Consequently, opting for a small $\lambda$ can help alleviate the likelihood displacement problem. Nevertheless, this incurs the cost of a diminished accuracy. Thus, selecting an appropriate
% $\lambda$ represents a trade-off between the probability of the chosen
% response and the accuracy. 

\textbf{Indications for Choosing $f(\lambda)$.} As analyzed previously, the important trade-off exists in \method. A slightly deeper analysis reveals that a smaller $f(\lambda)$ leads to more increase in chosen probability while a more severe drop in the reward margin. Thus, this indicates that choosing a relatively large $f(\lambda)$, i.e., close to $1$, helps to balance both sides. That is, the chosen probability is improved reasonably compared to DPO, in the meanwhile the reward margin of \method is only decreased slightly. The balance controlled by $f(\lambda)$ is thoroughly demonstrated by our experiment results in \Cref{sec:exp_results}.

For the strategy of choosing of $f(\lambda)$, the first one is to fix it all along the optimization process, i.e., $f(\lambda) = \lambda$. We denote this strategy as \texttt{fixed}. We also propose to vary $\lambda$ between the minimal $\lambda_{\min}$ and the maximal $\lambda_{\min}$ along with time $t$. We denote this strategy as $f(\lambda_{\min},\lambda_{\max},t)$. In this paper, we mainly have linear increase / decrease between $\lambda_{\min}<1$ and $\lambda_{\max}=1$. Set the maximal iteration steps to be $T$, the detailed strategy for the linear increase version of $f(\lambda_{\min},\lambda_{\max},t)$ is $t/T(\lambda_{\max}-\lambda_{\min})+\lambda_{\min}$, while the linearly decreased version is $t/T(\lambda_{\min}-\lambda_{\max})+\lambda_{\max}$. They are separately denoted as \texttt{linear\_increase} and \texttt{linear\_decrease}.

\textbf{Can $f(\lambda)$ be Chosen Larger than 1?} By default, we choose $f(\lambda) <1$ in \method to achieve chosen probability improvement, which is based on the hypothesis that $u_1$ is generally positive. If we encounter the case where $u_1<0$, e.g.,  when most pairs $\yw$ and $\yl$ are dissimilar to each other, $u_2$ must be negative as well. Interestingly, in this case \Cref{thm:gap} suggests that \method with $f(\lambda) >1$ can lead to simultaneous improvements for both the chosen probability and reward margin. However, the event $u_1<0$ is likely to be very rare, given the general similarity between $\yw$ and $\yl$ in existing $\mathcal{D}_{\text{pref}}$. Using $f(\lambda) >1$ when $u_1>0$ leads to catastrophic results. For instance, we trained Llama 3-8B on UltraFeedback with fixed $f(\lambda) = 1.05$, and the model quickly exhibited crash behavior, producing unreadable responses. Therefore, choosing $f(\lambda) > 1$ should be done with great care unless $u_1$ is clearly negative and the overwhelming majority of $\{u_1^i\}$ has negative values. 

Though choosing $f(\lambda) > 1$ is highly discouraged, the above analysis provides a possible direction to further improve \method. Before implementing the PO state, we can first calculate $\{u_1^i\}$ for the entire training set. Then, we assign a specific $f(\lambda_i)$ for each data point, i.e., $f(\lambda_i)<1$ when $u_1^i > 0$ and $f(\lambda_i)>1$ when $u_1^i <0$. Adopting such a carefully crafted approach requires a significant amount of additional implementation code and falls beyond the scope of this work. We leave it as future work. 

% \subsection{$\lambda$ strikes the balance between chosen probability and reward} \label{sec:chosen_shift}

% Figures here...


% In Figures 1, we show that 



% We investigate the gardient of DPO with respect to $\theta$:
% \begin{align}

% \end{align}
% Then with Taylor expansion, we arrive at:

\section{Experimental Setup}
\label{sec:exp_setup}

We perform our experiment on two models, Llama 3-8B~\cite{llama3modelcard}
and Qwen 2-7B~\cite{qwen2}, under base setup.

For the \textbf{SFT} stage, we train our models for one epoch on \href{https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k}{UltraChat-200k} dataset~\cite{Ding2023EnhancingCL} to obtain an SFT model with a learning rate of 2e-5. For Llama 3-8B model, we directly use the off-the-shelf models from \citep{meng2024simpo} (\href{https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT}{princeton-nlp/Llama-3-Base-8B-SFT}), which follows the same training strategy.

For the \textbf{Preference optimization} stage, we aim to verify the robustness of \method. To this end, we perform preference
optimization on \href{https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized}{UltraFeedback}
dataset~\cite{Cui2024UltraFeedbackBL},
\href{https://huggingface.co/datasets/argilla/Capybara-Preferences}{Capybara-preferences}
dataset~\cite{cbdatacard}. Notice that since the
Capybara dataset lacks a given test dataset, we randomly divide 5\% of the training
dataset into test sets for subsequent analysis and observations. We start from the two SFTed models and fine-tune them with these two datasets. As for the training parameter, we mainly follow the experimental detail in \cite{meng2024simpo}, and train the model for 1 epoch with a learning rate of 6e-7. The optimizer we choose is \texttt{paged\_Adamw\_32bit} with the implementation from \href{https://github.com/bitsandbytes-foundation/bitsandbytes/tree/main}{\texttt{bitsandbytes}}.

To support our paper, we mainly carry out the following two parts of experiments.

\textbf{Verification Experiment.} This part of the experiment is designed to validate the theoretical results presented in \Cref{sec:theory}. Specifically, we consider three strategies for selecting $f(\lambda)$ in \eqref{eq:lambda_dpo}: \texttt{fixed}, \texttt{linear\_increase}, and \texttt{linear\_decrease}. We design a series of ablation studies for each of the strategy by altering the choice of $f(\lambda)$. For the \texttt{fixed} strategy, we perform an ablation study by evaluating fixed $f(\lambda)$ from the range $[0.5, 0.55, 0.6, 0.65, 0.75, 0.8, 0.85, 0.9, 0.95]$. For the \texttt{linear\_increase} and \texttt{linear\_decrease} strategies, we set $\lambda_{\min}$ to values in $[0.75, 0.85, 0.95]$ and fix $\lambda_{\max} = 1$. We compute the $\logwin$, $\logrej$, and reward margin for models trained with these $f(\lambda)$ strategies on the test sets of their respective training datasets. To further illustrate the phase transition phenomenon in these probability distributions as $f(\lambda)$ varies from $f(\lambda) < 1$ to $f(\lambda) = 1$, we extend the \texttt{fixed} strategy ablation study by including $f(\lambda)$ values of $[0.96, 0.97, 0.98, 0.99]$.



\textbf{Downstream Performance Evaluation.} This experiment is primarily designed to evaluate the general performance of the model trained using our proposed method. For the two SFTed models trained on the UltraFeedback dataset, we evaluate the win rate of \method against DPO using 2,000 randomly selected questions from the Capybara dataset and the \href{https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized/viewer/default/test_prefs}{\texttt{test\_prefs}} split of the UltraFeedback dataset. For the evaluation, we employ the \href{https://ollama.com/library/llama3.3:70b-instruct-q4_K_M}{Llama3.3:70b-instruct-q4\_K\_M} model provided in the ollama library, a 4-bit quantized version of the latest Llama 3.3-Instruct 70B, which delivers performance comparable to Llama 3.1-405B.



% we assess their performance on one of the most widely used open-ended instruction-following benchmarks: MT-Bench~\cite{zheng2023judging}. MT-Bench consists of 80 questions across 8 categories, including writing, roleplay, reasoning, mathematics, coding, information extraction, STEM, and humanities. The tested models are tasked with answering these questions, and their responses are rated on a scale of 1 to 10 by the judge model. We report the average MT-Bench score, using GPT-4o as the judge model.


 

% The prompt design is detailed in \ref{app:prompts}. In our prompt, the judge model is provided with the question, the reference answer from the dataset, and the answers generated by \method and DPO for the same question. The judge model is then tasked with selecting one of three outcomes—``Win'', ``Tie'', or ``Loss''—and providing an explanation for its judgment.


\textbf{Computation Environment.} The training experiments in this paper were conducted using 8×A800 80GB GPUs and 4×A100 40GB GPUs, based on the alignment handbook repository \cite{Tunstall_The_Alignment_Handbook}. Specifically, all Llama 3 8B-related experiments were performed on 4×A100 GPUs, while Qwen 2-7B experiments utilized 8×A800 GPUs. For downstream performance evaluation, MT-Bench was assessed using the \texttt{llm\_judge} submodule from the FastChat \cite{zheng2023judging} repository, and the win rate experiments were conducted with the \href{https://github.com/ollama/ollama?tab=readme-ov-file}{\texttt{Ollama}} repository.

\begin{figure*}[t]
    \centering
    \includegraphics[width = \linewidth]{fig/llama-3-8b_ultrafeedback_chosen_rejected_hist_short.pdf}
    \vspace{-1cm}
    \caption{Distribution for $\logwin$ and $\logrej$ on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback with \texttt{fixed} strategy. Only limited cases of $f(\lambda)$ are listed. For a full ablation study, please refer to \Cref{app:candr_supp}. The ranges of the y-axis of all subfigures are the same.}
    \label{fig:ll_candr_fix_ultra}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width = \linewidth]{fig/llama-3-8b_ultrafeedback_reward_margin_hist_kde_short.pdf}
    \vspace{-1cm}
    \caption{Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback with \texttt{fixed} strategy. Only limited cases of $f(\lambda)$ are listed. For a full ablation study, please refer to \cref{app:candr_supp}. The ranges of the y-axis of all subfigures are the same.}
    \label{fig:ll_rew_fix_ultra}
    \vspace{-0.5cm}
\end{figure*}



\textbf{Baseline.} We compare \method using three strategies with the original DPO method. Since it is reported that many variants of DPO do not empirically present a clear advantage over standard DPO; see, e.g., \cite{meng2024simpo}, we do not compare our method with other variants of DPO. Additionally, due to the high workload of ablation studies associated with \method, the computational resources needed for experiments on other variants are beyond our means.

\section{Experimental Results}
\label{sec:exp_results}

In this section, we present the main results of our experiments, highlighting the performance of \method on various benchmarks and ablation studies.


\subsection{Experimental Verification for \method} \label{sec:chosen_shift}

We report some representative results from our ablation studies for \texttt{fixed}, \texttt{linear\_increase} and \texttt{linear\_decrease}. We mainly evaluate them with 2 metrics, including distribution for $\logwin$ and $\logrej$, and distribution for reward margin. 

We first study \texttt{fixed} strategy. The two metrics for evaluation, including distributions for $\logwin$ and $\logrej$, and distribution for reward margin are demonstrated in \Cref{fig:ll_candr_fix_ultra} and \Cref{fig:ll_rew_fix_ultra} separately. The relation between the choice of $f(\lambda)$ and reward accuracy are clearly illustrated in \Cref{fig:ll_rew_vs_acc_fix_ultra}. In \Cref{fig:ll_candr_fix_ultra}, we observe a consistent ``shift'' in the distribution of $\logwin$ and $\logrej$, moving from high-probability regions to lower-probability regions with increasing $f(\lambda)$. This shift is accompanied by a mass transition in the reward margin, shifting from areas less than 0 to areas greater than 0. The extent of this ``shift'' largely depends on the value of $f(\lambda)$, regardless of the strategy employed.



Specifically, using a smaller $f(\lambda)$ leads to higher probabilities for the language model on the chosen answer $\boldsymbol{y}_w$. However, this comes at the cost of decreased reward accuracy and shifted reward margin, as demonstrated in \Cref{fig:ll_rew_fix_ultra}. Thus, too smaller $f(\lambda)$ can result in ``over-fitting" to the chosen answer, while losing the contrastive learning ability against the rejected one, reducing the model's performance. Fortunately, by choosing a relatively larger $f(\lambda)$ that is closer to 1, we observe an increase in reward accuracy with only a limited trade-off in the chosen probability. This completely aligns with our analysis proposed in \Cref{sec:theory} that larger $f(\lambda)$ helps to balance both metrics. 
For example,  $f(\lambda)=0.9, 0.95$ achieves a higher probability of the chosen responses while maintaining almost the same reward margin and accuracy. \Cref{fig:ll_rew_vs_acc_fix_ultra} illustrates a clear increase of the reward accuracy along with increasing $f(\lambda)$, further corroborating our theory. 

 
% \begin{strip}
%     \centering
%     \includegraphics[width = \linewidth]{fig/llama-3-8b_ultrafeedback_reward_margin_hist_kde_short.pdf}
%     \captionof{figure}{Distribution for reward margin and reward accuracy on test dataset for Llama 3 8B trained with UltraFeedback datasets and \texttt{fixed} strategy, only limited cases of hyper-parameter are listed, for a full ablation study, please see appendix.}
%     \label{fig:ll_rew_fix_ultra}
% \end{strip}




% \begin{figure*}
%     \centering
%     \includegraphics[width = \linewidth, scale = 0.8]{fig/llama-3-8b_ultrafeedback_reward_margin_hist_kde_short.pdf}
%     \caption{Caption}
%     \label{fig:ll_rew_fix_ultra}
% \end{figure*}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/llama-3-8b_ultrafeedback_reward_acc_vs_lambda.pdf}
    \vspace{-0.8cm}
    \caption{Reward accuracy vs different $f(\lambda)$ (\texttt{fixed} strategy) for Llama 3-8B trained on UltraFeedback, where $f(\lambda)$ is selected from $[0.5, 0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95]$.}
    \label{fig:ll_rew_vs_acc_fix_ultra}
    \vspace{-0.5cm}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/phase_compare.pdf}
    \vspace{-0.7cm}
    \caption{The comparison between \method with $f(\lambda)=0.99$ and DPO on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback. \textbf{Left:} Distribution for $\logwin$. \textbf{Right:} Reward accuracy and distribution for reward margin.}
    \label{fig:phase_compare}
    \vspace{-0.5cm}
\end{figure}

To further investigate the effect of $f(\lambda)$ on the distribution of $\logwin$ and $\logrej$ as it approaches 1, we conducted experiments with more fine grains. In the case of \texttt{fixed}, we select a new set of fixed $f(\lambda)$s , including $[0.96,0.97,0.98,0.99]$. We report the comparison between \texttt{fixed} with $f(\lambda)=0.99$ and the original DPO with the distribution of chosen probability and reward margin in the \Cref{fig:phase_compare}. An obvious shift of the distribution $\logwin$ can be observed, while the reward accuracy and the distribution of the reward margin remains nearly unchanged. The full experimental results for $f(\lambda) \in [0.96,0.97,0.98,0.99]$ are reported in \Cref{app:phase_compare}, in which the results are consistent with our analysis.


To achieve a possibly better trade-off between the two distributions, we conduct experiments on \texttt{linear\_increase} and \texttt{linear\_decrease} strategies for choosing $f(\lambda)$. We report the result for \texttt{linear\_decrease} with $\lambda_{\min} = 0.75 $ in \Cref{fig:ll_rew_acc_candr_var}. It can be seen that this dynamic $f(\lambda)$ strategy achieves a more satisfactory balance between reward accuracy and $\logwin$ compared to \texttt{fixed} with $f(\lambda) = 0.75$. The full experimental results for \texttt{linear\_increase} and \texttt{linear\_decrease} strategies are provided in \Cref{app:candr_var_supp}.

% However, we note that for larger $\lambda_{\min}$, the improvement provided by dynamic $f(\lambda)$ over \texttt{fixed} strategy may become negligible. For the downstream task reported in \Cref{sec:win_rate_test}, $f(\lambda)$ strategies such as \texttt{linear\_decrease} and \texttt{linear\_increase} with $\lambda_{\min} = \tilde \lambda$ can frequently outperform \texttt{fixed} strategy with $f(\lambda) = \tilde \lambda$. 


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/var_dec_075.pdf}
    \vspace{-0.8cm}
    \caption{The comparison between \texttt{fixed} strategy with $f(\lambda)=0.75$ and \texttt{linear\_decrease} with $\lambda_{\min}=0.75$ on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback. \textbf{Left:} Distribution for $\logwin$ and $\logrej$; \textbf{Right:} Reward accuracy and distribution for reward margin. }
    \label{fig:ll_rew_acc_candr_var}
    \vspace{-0.5cm}
\end{figure}

\textbf{Summary.} By setting $f(\lambda)$ properly in \method, we can achieve controllable trade-off between chosen probability and reward accuracy. 
% Smooth $f(\lambda)$ transition to 1 can be helpful.

% \subsubsection{Ablation study for \texttt{linear\_increase} and \texttt{linear\_decrease}}

% We report the ablation study for \texttt{linear\_increase} and \texttt{linear\_decrease} in this section. The two metric for evaluation, including distribution for $\logwin$, $\logrej$, and distribution for reward margin are demonstrated in figure \ref{fig:ll_candr_var_ultra} and figure \ref{fig:ll_rew_var_ultra} separately. The relation between hyper-parameter and reward accuracy are clearly illustrated in figure \ref{fig:ll_rew_vs_acc_var_ultra}.

% \begin{figure}
%     \centering
%     \includegraphics[width = \linewidth]{fig/llama-3-8b_ultrafeedback_reward_acc_vs_lambda_var.pdf}
%     \captionof{figure}{Reward accuracy vs hyper-paramter for Llama 3 8B trained with UltraFeedback datasets and \texttt{linear\_increase} and \texttt{linear\_decrease} strategy with $\lambda_\min$ ranging in $[0.75,0.85,0.95]$}
%     \label{fig:ll_rew_vs_acc_var_ultra}
% \end{figure}



% \begin{figure*}
%     \centering
%     \includegraphics[width = \linewidth]{fig/llama-3-8b_ultrafeedback_chosen_rejected_hist_var.pdf}
%     \caption{Distribution for $\logwin$ and $\logrej$ on test dataset for Llama 3 8B trained with UltraFeedback datasets and \texttt{linear\_increase} and \texttt{linear\_decrease} strategy}
%     \label{fig:ll_candr_var_ultra}
% \end{figure*}


% \begin{strip}
%     \centering
%     \includegraphics[width = \linewidth]{fig/llama-3-8b_ultrafeedback_reward_margin_hist_kde_var.pdf}
%     \captionof{figure}{Distribution for reward margin and reward accuracy on test dataset for Llama 3 8B trained with UltraFeedback datasets with \texttt{linear\_increase} and \texttt{linear\_decrease} strategy, only limited cases of hyper-parameter are listed, for a full ablation study, please see appendix.}
%     \label{fig:ll_rew_var_ultra}
% \end{strip}

\subsection{Downstream Performance} \label{sec:win_rate_test}

We report our MT-bench experiment in \Cref{table:mt_bench}, which contains results of Llama 3-8B and Qwen 2-7B fine-tuned on the UltraFeedback dataset. We observe that although the optimal settings for $f(\lambda)$ vary between models, \method achieves consistent comparable and sometimes superior performances compared to DPO. Notice that it is more likely for \method to achieve better performance with a larger fixed $f(\lambda)$, emphasizing the importance of balancing the trade-off between chosen probability and reward margin.

However, since \method mainly focuses on better aligning the model with the chosen response, the MT-Bench experiment does not fully capture the model's ability to achieve this target. Moreover, when comparing the answers generated by DPO and \method, we observe that the model trained with DPO generally generates longer and less meaningful responses, while \method tends to produce more concise and high-quality responses. 
% These characteristics are preferable, and we believe this may be due to the model's increased alignment with the chosen response and decreased reward margin when trained with \method. 
Based on these observations, we adopt perplexity, a widely used evaluation metric \cite{cooper2024perplexed,chen1998evaluation,xu2022systematic} for language models. Perplexity quantifies how well a probability model predicts a sample of data, essentially measuring the model's uncertainty about the data. Lower perplexity indicates that the model is better at predicting the sample. The perplexity of DPO and \method trained on UltraFeedback with \texttt{fixed} $f(\lambda) = 0.95$ is evaluated on the chosen responses from the test split of the UltraFeedback dataset. The results are 4.475 for \method and 18.996 for DPO, further demonstrating the potential advantage of \method. 

\begin{table}[t]
\centering
% \small
\caption{MT-Bench \cite{zheng2023judging} results for Llama 3-8B and Qwen 2-7B trained on UltraFeedback, where GPT-4o is the judge model. The results better than DPO is \textbf{bolded}. Results for \texttt{fixed}, \texttt{linear\_increase}, and \texttt{linear\_decrease} are included.}
\label{table:mt_bench}
\begin{tabular}{lcccc}
\toprule
$f(\lambda)$ strategy &  \textbf{Llama 3-8B} & \textbf{Qwen 2-7B} \\
\midrule
\textbf{SFT} & 5.64 & 5.88 \\
\midrule
\textbf{DPO} & 6.513 & 6.875 \\
\midrule
\texttt{fixed} 0.5 & 6.118 & 6.150  \\
\texttt{fixed} 0.55 & 6.269 & 6.369  \\
\texttt{fixed} 0.6 & 6.169 & 6.331  \\
\texttt{fixed} 0.65 & 6.314 & 6.494  \\
\texttt{fixed} 0.7 & 6.500 & 6.581 \\
\texttt{fixed} 0.75 & 6.444 & 6.700  \\
\texttt{fixed} 0.8 & \textbf{6.731} & 6.869  \\
\texttt{fixed} 0.85 & 6.644 & 6.775 \\
\texttt{fixed} 0.9 & \textbf{6.738} & 6.725  \\
\texttt{fixed} 0.95 & 6.444 & 6.875  \\
\texttt{increase\_linear} 0.75 & \textbf{6.588} & 6.775 \\
\texttt{increase\_linear} 0.85 & 6.425 & 6.806 \\
\texttt{increase\_linear} 0.95 & 6.519 & \textbf{7.044} \\
\texttt{decrease\_linear} 0.75 & \textbf{6.613} & 6.742 \\
\texttt{decrease\_linear} 0.85 & 6.481 & \textbf{6.906} \\
\texttt{decrease\_linear} 0.95 & \textbf{6.606} & \textbf{6.944} \\
\bottomrule
\end{tabular}
\vspace{-0.5cm}
\end{table}



To fully demonstrate the better alignment of \method with the chosen response, we conduct a win rate experiment. In this setup, the judge model is provided with the question, the reference answer from the dataset, and the answers generated by \method and DPO. The judge model then judges the responses of \method and DPO based on their general quality and how close to the reference answer they are. We put the judge prompts in \Cref{app:prompts}.
% The alignment with the chosen response is assessed to evaluate the model's ability to align with the chosen response. In point 2 of \Cref{app:prompts}, we require the judge model to make its decision based on how closely the assistant's answer aligns with the essential meaning of the reference answer. Based on our observation that language models often judge answer quality by length rather than correctness, we include point 1 in our prompts. The rest of the prompts follow standard practices for win-rate experiments, where response quality is also considered.

We test the win rate using the test split of UltraFeedback and Capybara for Llama 3-8B and Qwen 2-7B that is trained on UltraFeedback. Note that in this setting, the test on Capybara is out of distribution. We report the results of Llama 3-8B tested on Ultrafeedback in \Cref{fig:win_rate}, while the rest of the experiments are deferred to \Cref{app:full: win rate}.  One can observe that when $f(\lambda)$ is relatively closer to $1$, \method consistently outperforms DPO. This finding aligns with the MT-Bench results listed in \Cref{table:mt_bench}, where \method with larger $f(\lambda)$ is more likely to outperforms DPO. These experiment results corroborate our analysis in \Cref{sec:theory}, highlighting the importance of achieving a balance between chosen probability and reward margin. 

\textbf{Summary.} \method outperforms DPO in terms of downstream performance when $f(\lambda)$ is set to achieve a balance between chosen probability and reward margin. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/win_rate_Llama-3_8B_on_Ultrafeedback.pdf}
    \vspace{-0.8cm}
    \caption{Win rate experiment against DPO using Llama 3-8B trained on the UltraFeedback dataset and tested with questions from the test split of UltraFeedback. 
    % The reported results include SFTed model, \method using \texttt{fixed} with $f(\lambda)$ ranging in $[0.55,0.65,0.75,0.85,0.95]$, \texttt{linear\_increase} and \texttt{linear\_decrease} with $\lambda_{\min}=0.95$.
    }
    \label{fig:win_rate}
    \vspace{-0.5cm}
\end{figure}


\section{Conclusion}
\label{sec:conclusion}
In this work, we proposed \method, which controllably shifts the distribution of the chosen probability. Our method guarantees to mitigate the likelihood displacement issue of DPO while introducing a fundamental trade-off between the chosen probability and reward margin. By selecting $f(\lambda)$ carefully, the chosen probability can be improved, in the meanwhile the reward margin is only slightly sacrificed. Extensive ablation experiments across various models and datasets confirm the validity of our theoretical findings. To further demonstrate the advantages of \method, we conducted experiments on downstream tasks such as MT-Bench and a designed win rate experiment. Clear performance improvements over DPO were observed, highlighting the robustness and effectiveness of our approach. A more crafted strategy on the selection of $f(\lambda)$ can be a possible direction to further improve \method, as we commented at the end of \Cref{sec:theory}. We leave it as future work. 




% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible
% for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on
% the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software
% and data with the camera-ready version of the paper whenever appropriate. This
% can be done by including a URL in the camera-ready copy. However, \textbf{do
% not} include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload the
% material as ``Supplementary Material'' into the OpenReview reviewing system.
% Note that reviewers are not required to look at this material when writing their
% review.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of the paper
% submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and usually should)
% include acknowledgements. Such acknowledgements should be placed at the end of
% the section, in an unnumbered section that does not count towards the paper
% page limit. Typically, this will include thanks to reviewers who gave useful
% comments, to colleagues who contributed to the ideas, and to funding
% agencies and corporate sponsors that provided financial support.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning.
There are many potential societal consequences of our work, none which we feel
must be specifically highlighted here

% Authors are \textbf{required} to include a statement of the potential
% broader impact of their work, including its ethical aspects and future societal
% consequences. This statement should be in an unnumbered section at the end
% of the paper (co-located with Acknowledgements -- the two may appear in
% either order, but both must be before References), and does not count toward
% the paper page limit. In many cases, where the ethical impacts and expected societal
% implications are those that are well established when advancing the field of
% Machine Learning, substantial discussion is not required, and a simple statement
% such as the following will suffice:

% ``This paper presents work whose goal is to advance the field of Machine Learning.
% There are many potential societal consequences of our work, none which we feel
% must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we encourage
% authors to think about whether there is content which does warrant further
% discussion, as this statement will be apparent if the paper is later flagged
% for ethics review.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{ref}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Supplemented Experimental results}
\label{app}
% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{|l|l|l|l|}
%         \hline
%         Model name & Dataset                  & learning rate & optimal lambda \\
%         \hline
%         Mistral 7B & ultrafeedback\_binarized & 5e-7          & TBD            \\
%         \hline
%         Llama 3 8B & ultrafeedback\_binarized & 5e-7          & 0.95           \\
%         \hline
%     \end{tabular}
% \end{table}


\subsection{Llama 3.3-70B Prompts for Win Rate experiment}
\label{app:prompts}
\begin{verbatim}
You are tasked with comparing the responses of two assistants, Assistant A 
and Assistant B to a user’s question. Additionally, you will be provided with a

reference answer to evaluate the quality of the responses from both assistants.

User’s Question:
<question>

Reference Answer:
<reference answer>

Assistant A’s Response:
<response compare>

Assistant B’s Response:
<response_baseline>

First, output 'A', 'B', or 'Tie' to indicate your judgment of these two responses. 
Then, provide a one-sentence explanation for your choice.

The principles for your judgment should consider the following criteria:

1. Do not judge the quality of the two responses based on their length.  
2. Determine which response’s meaning is essentially closer to the reference answer.
3. Evaluate the responses based on their helpfulness, relevance, accuracy, depth,
and level of detail.
4. For open-ended questions, the reference answer might not be the unique 
correct answer, and you can take correct and factual alternative responses into 
account for these types of questions. 
5. If the two responses have no essential difference in meaning and correctness, 
and only differ in wording, format, or length, output ‘Tie’.
\end{verbatim}

\clearpage
\subsection{Ablation Studies for \texttt{fixed}}
\label{app:candr_supp}

\vspace{-0.2cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/llama-3-8b_ultrafeedback_chosen_rejected_hist.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for $\logwin$ and $\logrej$ on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback, where \method uses \texttt{fixed} strategy. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

\vspace{-0.6cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/llama-3-8b_ultrafeedback_reward_margin_hist_kde.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback, where \method uses \texttt{fixed} strategy. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

\clearpage
\vspace{-0.2cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/llama-3-8b_capybara_chosen_rejected_hist.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for $\logwin$ and $\logrej$ on test set split of Capybara for Llama 3-8B trained on Capybara, where \method uses \texttt{fixed} strategy. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

\vspace{-0.3cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/llama-3-8b_capybara_reward_margin_hist_kde.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for reward margin and reward accuracy on test set split of Capybara for Llama 3-8B trained on Capybara, where \method uses \texttt{fixed} strategy. The ranges of the y-axis of all subfigures are the same.}
\end{figure}


\clearpage
\vspace{-0.2cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/qwen-2-7b_ultrafeedback_chosen_rejected_hist.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for $\logwin$ and $\logrej$ on test set split of UltraFeedback for Qwen 2-7B trained on UltraFeedback, where \method uses \texttt{fixed} strategy. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

\vspace{-0.3cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/qwen-2-7b_ultrafeedback_reward_margin_hist_kde.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Qwen 2-7B trained on UltraFeedback, where \method uses \texttt{fixed} strategy. The ranges of the y-axis of all subfigures are the same.}
\end{figure}


\clearpage
\vspace{-0.2cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/qwen-2-7b_capybara_chosen_rejected_hist.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for $\logwin$ and $\logrej$ on test set split of Capybara for Qwen 2-7B trained on Capybara, where \method uses \texttt{fixed} strategy. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

\vspace{-0.3cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/qwen-2-7b_capybara_reward_margin_hist_kde.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for reward margin and reward accuracy on test set split of Capybara for Qwen 2-7B trained on Capybara, where \method uses \texttt{fixed} strategy. The ranges of the y-axis of all subfigures are the same.}
\end{figure}



\clearpage
\subsection{Ablation Studies for \texttt{linear\_increase} and \texttt{linear\_decrease}}
\label{app:candr_var_supp}


\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fig/llama-3-8b_ultrafeedback_chosen_rejected_hist_var.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for $\logwin$ and $\logrej$ on test set split of Ultrafeedback for Llama 3-8B trained on UltraFeedback, where \method uses \texttt{linear\_increase} and \texttt{linear\_decrease} strategies. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fig/llama-3-8b_ultrafeedback_reward_margin_hist_kde_var.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for reward margin and reward accuracy on test set split of Ultrafeedback for Llama 3-8B trained on UltraFeedback, where \method uses \texttt{linear\_increase} and \texttt{linear\_decrease} strategies. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

\clearpage
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fig/llama-3-8b_capybara_chosen_rejected_hist_var.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for $\logwin$ and $\logrej$ on test set split of Capybara for Llama 3-8B trained on Capybara, where \method uses \texttt{linear\_increase} and \texttt{linear\_decrease} strategies. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fig/llama-3-8b_capybara_reward_margin_hist_kde_var.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for reward margin and reward accuracy on test set split of Capybara for Llama 3-8B trained on Capybara, where \method uses \texttt{linear\_increase} and \texttt{linear\_decrease} strategies. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

\clearpage
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fig/qwen-2-7b_ultrafeedback_chosen_rejected_hist_var.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for $\logwin$ and $\logrej$ on test set split of UltraFeedback for Qwen 2-7B trained on UltraFeedback, where \method uses \texttt{linear\_increase} and \texttt{linear\_decrease} strategies. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fig/qwen-2-7b_ultrafeedback_reward_margin_hist_kde_var.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Qwen 2-7B trained on UltraFeedback, where \method uses \texttt{linear\_increase} and \texttt{linear\_decrease} strategies. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

\clearpage
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fig/qwen-2-7b_capybara_chosen_rejected_hist_var.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for $\logwin$ and $\logrej$ on test set split of Capybara for Qwen 2-7B trained on Capybara, where \method uses \texttt{linear\_increase} and \texttt{linear\_decrease} strategies. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fig/qwen-2-7b_capybara_reward_margin_hist_kde_var.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for reward margin and reward accuracy on test set split of Capybara for Qwen 2-7B trained on Capybara, where \method uses \texttt{linear\_increase} and \texttt{linear\_decrease} strategies. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

% \subsection{Ablation Studies for Reward Margin for \texttt{fixed}}
% \label{app:randm_supp}



% \subsection{Ablation Studies for Reward Margin for \texttt{linear\_increase} and \texttt{linear\_decrease}}
% \label{app:randm_var_supp}



\clearpage
\subsection{Ablation Studies for Fine-grained \texttt{fixed}}
\label{app:phase_compare}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/llama-3-8b_ultrafeedback_chosen_rejected_hist_phase_trans.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for $\logwin$ and $\logrej$ on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback, where \method uses \texttt{fixed} strategy. The ranges of the y-axis of all subfigures are the same.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/llama-3-8b_ultrafeedback_reward_margin_hist_kde_phase_trans.pdf}
    \vspace{-0.5cm}
    \caption{Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback, where \method uses \texttt{fixed} strategy. The ranges of the y-axis of all subfigures are the same.}
\end{figure}


\clearpage
\subsection{Supplementary Results for Win Rate Experiment}
\label{app:full: win rate}

\vspace{-0.2cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fig/win_rate_Llama-3_8B_on_Capybara.pdf}
    \vspace{-0.5cm}
    \caption{Win rate experiment for Llama 3-8B trained on  UltraFeedback and tested with questions from the test split of Capybara. 
    % The reported results include SFTed model, \method using \texttt{fixed} with $f(\lambda)$ ranging in $[0.55,0.65,0.75,0.85,0.95]$, \texttt{linear\_increase} and \texttt{linear\_decrease} with $\lambda_{\min}=0.95$.
    }
\end{figure}
\vspace{-1cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fig/win_rate_Qwen-2_7B_on_Ultrafeedback.pdf}
        \vspace{-0.5cm}
    \caption{Win rate experiment for Qwen 2-7B trained on UltraFeedback and tested with questions from the test split of UltraFeedback. 
    % The reported results include SFTed model, \method using \texttt{fixed} with $f(\lambda)$ ranging in $[0.55,0.65,0.75,0.85,0.95]$, \texttt{linear\_increase} and \texttt{linear\_decrease} with $\lambda_{\min}=0.95$.
    }
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fig/win_rate_Qwen-2_7B_on_Capybara.pdf}
    \vspace{-0.5cm}
    \caption{Win rate experiment for Qwen 2-7B trained on UltraFeedback and tested with questions from the test split of Capybara. 
    % The reported results include SFTed model, \method using \texttt{fixed} with $f(\lambda)$ ranging in $[0.55,0.65,0.75,0.85,0.95]$, \texttt{linear\_increase} and \texttt{linear\_decrease} with $\lambda_{\min}=0.95$.
    }
\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{fig/KDE_plot_for_chosen_and_rejected.pdf}
%     \caption{duplicate}
%     \label{app:kdeplot}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{fig/counter_plot.pdf}
%     \caption{duplicate}
%     \label{app:counter}
% \end{figure}

\clearpage
\section{Proof of \Cref{thm:gap}}
\label{app:proof}

We consider our modified PO loss function:

\[
    \mathcal{L}_{\lambda-\text{DPO}}(\pi_{\theta},\pi_{\text{ref}})= -\mathbb{E}
    _{(\boldsymbol{x},\boldsymbol{y}_w,\boldsymbol{y}_l)\sim \mathcal{D}_{\text{pref}}}
    \left[\sigma\left(\beta\log\frac{\pi_{\theta}(\boldsymbol{y}_{w}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{w}|\boldsymbol{x})}
    -f(\lambda)\cdot\beta\log\frac{\pi_{\theta}(\boldsymbol{y}_{l}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{l}|\boldsymbol{x})}
    \right)\right]
\]
where $f(\lambda)$ is a real valued function with $f(\lambda)<1$. We compute
its gradient w.r.t $\theta$:
\begin{align*}
    \nabla_{\theta}\mathcal{L}_{\lambda-\text{DPO}}(\pi_{\theta},\pi_{\text{ref}}) & =-\mathbb{E}_{(\boldsymbol{x},\boldsymbol{y}_w,\boldsymbol{y}_l)\sim \mathcal{D}_{\text{pref}}}\left[\frac{\sigma'(u)}{\sigma(u)}\nabla_{\theta}u\right]                                                                                                       \\
    u                                                                              & :=\beta\log\frac{\pi_{\theta}(\boldsymbol{y}_{w}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{w}|\boldsymbol{x})}-f(\lambda)\cdot\beta\log\frac{\pi_{\theta}(\boldsymbol{y}_{l}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{l}|\boldsymbol{x})}
\end{align*}

notice that $\sigma'(x)=\sigma(x)(1-\sigma(x)),1-\sigma(x)=\sigma(-x).$

Then we proceed with the final gradient
\begin{align}
    \nabla_{\theta}\mathcal{L}_{\lambda-\text{DPO}}(\pi_{\theta},\pi_{\text{ref}})=-\mathbb{E}_{(\boldsymbol{x},\boldsymbol{y}_w,\boldsymbol{y}_l)\sim \mathcal{D}_{\text{pref}}}\left[\beta\sigma\left(f(\lambda)\cdot\beta\log\frac{\pi_{\theta}(\boldsymbol{y}_{l}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{l}|\boldsymbol{x})}-\beta\log\frac{\pi_{\theta}(\boldsymbol{y}_{w}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{w}|\boldsymbol{x})}\right)\right. \\
    \times\left.\left[ \nabla_{\theta}\log\pi \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)-f(\lambda)\cdot\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right) \right] \right]
\end{align}
We then simplify it with the following notation
\begin{align*}
    c_{1}:=c_{\theta}(\lambda,\boldsymbol{y}_{w},\boldsymbol{y}_{l}) & =\beta\sigma\left(f(\lambda)\cdot\beta\log\frac{\pi_{\theta}(\boldsymbol{y}_{l}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{l}|\boldsymbol{x})}-\beta\log\frac{\pi_{\theta}(\boldsymbol{y}_{w}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{w}|\boldsymbol{x})}\right) \\
    c_{2}                                                           & =f(\lambda)c_{1}
\end{align*}
then we have
\[
    \nabla_{\theta}\mathcal{L}_{\lambda-\text{DPO}}(\pi_{\theta},\pi_{\text{ref}}
    )=-\mathbb{E}_{(\boldsymbol{x},\boldsymbol{y}_w,\boldsymbol{y}_l)\sim
    \mathcal{D}_{\text{pref}}}\left[c_{1}\nabla_{\theta}\log\pi_{\theta} \left(\boldsymbol
    {y}_{w}|\boldsymbol{x}\right)-c_{2}\nabla_{\theta}\log\pi_{\theta} \left(\boldsymbol
    {y}_{l}|\boldsymbol{x}\right)\right].
\]
Then we upgrade $\theta_{t+1}$ with the following:
\[
    \theta_{t+1}\leftarrow \theta_{t}+\eta\mathbb{E}_{(\boldsymbol{x},\boldsymbol{y}_w,\boldsymbol{y}_l)\sim
    \mathcal{D}_{\text{pref}}}\left[c_{1}\nabla_{\theta}\log\pi_{\theta_t} \left(\boldsymbol
    {y}_{w}|\boldsymbol{x}\right)-c_{2}\nabla_{\theta}\log\pi_{\theta_t} \left(\boldsymbol
    {y}_{l}|\boldsymbol{x}\right)\right].
\]
We first look into
\[
    w_1(\theta_t)=\log\pi_{\theta_t} \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)
\]
then
\begin{align}
  w_1(\theta_{t+1}) & =w_1(\theta_t)+\eta\left( c_{1}\nabla_{\theta}\log\pi_{\theta_t} \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)-c_{2}\nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right) \right)^{\top}\left( \nabla_{\theta}\log\pi_{\theta_{t}}\left(\boldsymbol{y}_{w}|\boldsymbol{x}\right) \right)     \\
    & =w_{1}(\theta_{t})+\eta \left( c_{1}\left||\nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)\right||^{2}-c_{2}\nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)^{\top}\nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)\right)
\end{align}
% we observe decreasing $w^{2}_{t}$ in our experiment, therefore
% \[
%     c_{1}\left||\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{w}|\boldsymbol{x}
%     \right)\right||^{2}\leq c_{2}\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{l}
%     |\boldsymbol{x}\right)^{\top}\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{w}
%     |\boldsymbol{x}\right)
% \]
% holds with high probability.

then
\begin{align*}
    g_1(t+1)&= \eta(c_{1}-c_{2})\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)^{\top}\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)\\
\end{align*}
We compute $\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{l}^i|\boldsymbol{x}_i\right)^{\top}\nabla_{\theta}\log\pi \left(\boldsymbol{y}^i_{w}|\boldsymbol{x}_i\right)$ for the SFTed Llama 3-8B model. In terms of frequency, 71.4\% of them turn out to be positive. Consequently, we can choose a $c_{2}$ as small as possible to increase the chosen probability.

However, choosing small $c_{2}$ can cause performance drop (both in MT-bench \Cref{table:mt_bench}
and reward accuracy). To evaluate the performance of the model, we look into the reward margin:
\[
    \omega_{2}(\theta_t)  =\mathbb{E}\left[\mathbf{1}\left\{\log\frac{\pi_{\theta_t}(\boldsymbol{y}_{w}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{w}|\boldsymbol{x})} -\log\frac{\pi_{\theta_t}(\boldsymbol{y}_{l}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{l}|\boldsymbol{x})}>0\right\}\right]
\]
To analyze, we alternate it with its smoothed version:
\[
    \omega_{2}(\theta_t)=\mathbb{E}\left[\sigma\left(\gamma\log\frac{\pi_{\theta_t}(\boldsymbol{y}_{w}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{w}|\boldsymbol{x})} -\gamma\log\frac{\pi_{\theta_t}(\boldsymbol{y}_{l}|\boldsymbol{x})}{\pi_{\mathrm{ref}}(\boldsymbol{y}_{l}|\boldsymbol{x})}\right)\right],
\]
we abuse notation and use $\beta \rightarrow +\infty$ as hyper parameter.

Then with first order Taylor's expansion for
$\left( \omega',\theta' \right)$ and $\left( \omega,\theta \right)$.

\textbf{Remark 1.} Given the fact that they are using the same expectation
$\mathbb{E}_{(\boldsymbol{x},\boldsymbol{y}_w,\boldsymbol{y}_l)\sim
\mathcal{D}_{\text{pref}}}\left[\cdot\right]$, we omit it for the sake of simplicity
2. Given the assumption that $\eta$ is small enough, we can ignore second
order.
 \begin{align*}
    \omega_{2}(\theta_{t+1}) & =\omega_{2}(\theta_{t})+\eta(\theta^{2}_{t+1}-\theta^{2}_{t})^{\top}\sigma\left(\log\frac{\pi_{\theta_{t}} \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)}{\pi_{\mathrm{ref}}{(\boldsymbol{y}_{l}|\boldsymbol{x})}}-\log \frac{\pi_{\theta_{t}} \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)}{\pi_{\mathrm{ref}}(\boldsymbol{y_{w}|x})}\right)\cdot \left( \nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)-\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right) \right)     \\
    & =\omega^{2}_{t}+\eta_{1}\left( c_{1}\nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)-c_{2}\nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right) \right)^{\top}\left( \nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)-\nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right) \right)                                                 \\
    & =\omega^{2}_{t}+\eta_{1}\left[c_{1}\left||\nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)\right||^{2}+c_{2}\left||\nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)\right||^{2}-\left( c_{1}+c_{2}\right) \nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)^{\top}\nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right) \right] \\
    & \eta_{1}:=\eta \sigma \left( \log\pi_{\theta_{t}} \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)-\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right) \right)
\end{align*}

% We similarly define
% \[
%     f^{2}(t+1,\boldsymbol{y}_{w},\boldsymbol{y}_{l},x)=w^{2}_{t+1}\Big|_{c_{2}<c_{1}}
%     -w^{2}_{t+1}\Big|_{c_{2}=c_{1}}
% \]
% where $w_{t}\Big|_{c_{2}>c_{1}}$ updates with $\mathcal{L}_{\lambda-\text{DPO}}
% (\pi_{\theta},\pi_{\text{ref}})$ and $w_{t}\Big|_{c_{2}=c_{1}}$ updates with
% $\mathcal{L}_{\text{DPO}}(\pi_{\theta},\pi_{\text{ref}})$.

then we have

% \begin{align*}
%         & f(t+1;\boldsymbol{y}_{w},\boldsymbol{y}_{l},x)-f(t;\boldsymbol{y}_{w},\boldsymbol{y}_{l},x) =\left(w_{t+1}\Big|_{c_{2}<c_{1}}-w_{t}\Big|_{c_{2}<c_{1}}\right)-\left( w_{t+1}\Big|_{c_{2}=c_{1}}-w_{t}\Big|_{c_{2}=c_{1}}\right)                                                             \\
%         & =\eta_{1}\left( c_{2}-c_{1}\right)\left||\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)\right||^{2}+(c_{1}-c_{2})\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)^{\top}\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)   \\
%         & =\eta_{1}\left(c_{1}-c_{2}\right) \left(\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)^{\top}\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)-\left||\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)\right||^{2}\right)<0
% \end{align*}


\begin{align*}
g_2(t+1) &=\eta_{1}\left( c_{2}-c_{1}\right)\left||\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)\right||^{2}+(c_{1}-c_{2})\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)^{\top}\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)   \\
& =\eta_{1}\left(c_{1}-c_{2}\right) \left(\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)^{\top}\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{w}|\boldsymbol{x}\right)-\left||\nabla_{\theta}\log\pi \left(\boldsymbol{y}_{l}|\boldsymbol{x}\right)\right||^{2}\right)
\end{align*}

We compute $\nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{l}^i|\boldsymbol{x}_i\right)^{\top}\nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{w}^{i}|\boldsymbol{x}_{i}\right)-\left||\nabla_{\theta}\log\pi_{\theta_{t}} \left(\boldsymbol{y}_{l}^{i}|\boldsymbol{x}_{i}\right)\right||^{2}$ for the SFTed Llama 3-8B model. In terms of frequency, 81.7\% of them turn out to be positive.

which is consistent with our observation that accuracy drop greatly with small
$c_{2}$ , therefore we need to strike a balance between the accuracy and chosen
probability.

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.