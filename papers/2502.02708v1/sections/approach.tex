
\section{AsserT5}

Our proposed model, \assertfive, is built on top of a pre-trained
Transformer architecture. The model receives a unit test together with
its focal method as a single input sequence and is tasked to generate
an assertion statement.
Due to the pre-trained base-model, we focus on fine-tuning the
underlying Transformer on a dataset of pairs of tested methods and
their corresponding test cases. We use a pre-existing dataset of such
pairs as basis, but also apply additional filtering and preprocessing
steps to adapt the data points to our requirements.


\subsection{Base Dataset: Methods2test}\label{sec:methods2test}

\Methodstotest~\cite{Tufano2022a} is a large, supervised comparison
dataset that assigns Java \junit tests to their associated focal
methods. The dataset aims to find a reliable mapping that ensures that
the focal method belongs to the associated test. Originally intended
to automatically generate test cases~\cite{Tufano2020}, \methodstotest
fills the gap of missing datasets with real test cases. It contains
780k data elements that contain \junit tests and their focal method
from 9.4k Java open-source non-fork projects from \github that the
maintainer updated in the last five years.

While preprocessing a project into the structure desired by
\methodstotest, each project is parsed to identify the test and focal
methods using the following heuristics:
The class containing the focal method must have the name of the
corresponding test class without the \enquote{test} prefix or suffix
and be part of the same Java package. The search for the focal method
continues only in the found focal class.
Removing a \enquote{test} prefix or suffix from the test method names
and finding this modified method name in the focal class yields the
focal method.
If this does not work, \methodstotest extracts all methods called in
the test case and the focal class and forms the intersection of these
sets. If this intersection contains exactly one element, this is the
focal method. Otherwise, no focal method for the test method is
detectable.
\Methodstotest discards the data point if no unique focal method
exists after these strict heuristics.

Other datasets were generated using simpler focal method detection
heuristics like choosing the last method call before the
assertion~\cite{Watson2020}. However, this assumption has been found
to not reflect the developer intention in many cases~\cite{He2024}.
The more complex set of heuristics of \methodstotest therefore ensures
better matching test and focal method pairs.


\subsection{Training and Evaluation Dataset}\label{sec:preprocessing}

We used the \methodstotest data as basis for our extended dataset.
Since the dataset is missing the Javadoc documentation for the focal
method which we require later to compare our model with
\toga~\cite{Dinella2022}, we cloned all still available source
repositories again~(\exnum{9184} cloneable, \exnum{226} not) to
extract the original test and focal method with the additional
context.


\subsubsection{Data Filtering}\label{sec:data-filtering}

We discarded data points where either the test or focal method is no
longer available to obtain \exnum{562836} out of the original
\exnum{780944} data points from which we removed further \exnum{4402}
samples where the focal method is a constructor rather than an actual
method.
To later understand how the amount of context affects the performance
of the model, we generate three subsets from this filtered dataset.
The first subset allows exactly one assertion per test
case~(\exnum{149893} tests), the second one additionally adds tests
with up to five assertions~(\exnum{248831} tests), and for the third
one we allowed up to ten assertions~(\exnum{269490} tests).
To mask the assertions in the test method and to extract the focal
method, the classes for both need to be parseable into an abstract
syntax tree~(\AST). This requirement removed \exnum{6701} data points,
which the parser library we used could not process, from the dataset.
Finally, we removed \exnum{322} items with test cases longer than
\exnum{10000} characters from the dataset to improve performance
during preprocessing.

\input{tables/considered-assertion-types}

The main filtering step only accepts data points that have assertions
in the format shown in \cref{tab:considered-assertion-types}.
Like \toga~\cite{Dinella2022}, we only allow commonly used \junit
assertion types and also add \texttt{assertNotEquals} to consistently
allow the positive and negative counterparts and \texttt{assertThrows}
as built-in alternative to the \texttt{try-catch} assertion. This type
verifies that an exception is thrown by the method under test and
optionally makes additional assertions on the caught exception.
Most \junit assertions also allow an additional optional parameter
that contains an error message shown to the developer in case of
assertion failures. This parameter does not influence the values the
assertion is applied to. Therefore, we only consider assertions
without this parameter to later ease the automatic evaluation if two
assertions are functionally equivalent.

Each of the three datasets with different numbers of assertions is
independently split into training, validation, and test sets following
an 80:10:10 ratio. We ensure that focal methods with multiple test
cases do not appear in both the training and test splits to avoid
leaking information between splits.
Since in the latter two subsets each test case can have multiple
assertions matching our requirements, we added it to the final dataset
once per valid assertion and masked only one specific assertion per
data point. To avoid data leakage, we again ensured all variants
appeared in only one of the train/eval/test splits.
This resulted in \exnum{140897}, \exnum{411132}, and \exnum{555883}
usable data points in the subsets with 1, 5, and 10 assertions,
respectively.


\subsubsection{Data Preprocessing}\label{sec:data-preprocessing}

This filtered dataset needs to be transformed into a customised format
specific to the assertion generation model. That entails concatenating
all source code tokens from the test and focal methods.

We constructed three dataset variants for the evaluation.
The first preprocessing variant comprises raw text tokens exclusively
(\ie~tokenising the code without further changes to the actual
tokens), forming a raw dataset by concatenating the test and focal
method.
The second option uses the same tokenisation steps but only considers
the test method code.
The third variant uses the abstract tokens of the test and focal
methods. This methodology employed in \atlas aims to abstract the
dataset and entails converting each method name, identifier, or
literal type into an abstract token with a corresponding type and
number. We also included the tokens of the test and focal class to
have more reasonable abstract tokens in the vocabulary. The
potentially important syntactic positions of the abstract tokens
relative to each other remain as in the original code.
The abstraction process augments the modelâ€™s capacity to discern and
generalise patterns and features of the data~\cite{Tufano2019,
Tufano2018, Tufano2019a, Watson2020}\@. \atlas has shown that
abstraction improved evaluation scores compared to the raw
variant~\cite{Watson2020}, as the abstraction process reduced the
vocabulary size and therefore the number of model parameters as well
as the training duration.

\input{figures/abstraction-process-example}

We use the test method \texttt{testLast} and its focal method from
\cref{fig:example-assertion-generation} to demonstrate how the
abstraction process replaces all semantic identifiers, method names,
and literal values during input generation for model training and
inference.
As \cref{fig:tokenised-abstract-method} shows, the starting points of
the test and focal methods are marked before concatenating test and
focal methods.
Then identifiers and constant values within the code are transformed
into abstract tokens. We encode string literals that contain
whitespace into exactly one abstract token rather than splitting them
into many small subtokens.
For all replacements, the original values of the replaced tokens are
saved in a dictionary specific to this individual test and focal
method pair.
For example, the abstract token \texttt{INT\_X} corresponds to the
number 5 in the shown test case, but may refer to another integer
constant in another input. This forces the model to learn from a more
general structure of the inputs rather than relying on specific
constants or names.
When the model generates a sequence of tokens during training or
inference, it then has to only choose between a comparatively small
set of possible tokens.
While the input-specific stored dictionary of identifiers is not used
during training since both input and output only use the abstract
form, it is used during inference to map the abstract tokens back to
proper values, \ie~usable source code.

In the model input, the assertion is removed and instead masked by
\texttt{<ASSERTION>} during training. The abstracted assertion is not
part of the model input but still presented in
\cref{fig:tokenised-abstract-method} to show the ground truth label we
expect the model to predict. To obtain the same structure during
inference, the special token is added to the location in which we want
the model to generate the assertion statement.
If the final model input sequence is longer than the maximum supported
length of \(n = 386\) tokens, we truncate the sequence so that only
the first \(n\) tokens of the sequence are passed to the model and
discard the remainder, but always retain the assertion placeholder.


\subsection{Model}\label{sec:model}

\assertfive is based on a fine-tuned \codetfive~\cite{Wang2021} model.
The underlying \tfive model~\cite{Raffel2019} can capture long
dependencies between tokens which allows the generated assertions to
reference back to variable names appearing in the test code.
Choosing \codetfive for fine-tuning was a strategic decision rooted in
several practical key factors:
(1)~\emph{Pre-training}:~\codetfive is specifically designed and
pre-trained on a diverse set of code-related tasks, giving a strong
foundation for understanding programming languages, syntax, and
semantics. As a sequence-to-sequence model it therefore is designed to
generate code.
(2)~\emph{Existing model basis}:~\huggingface provides a model basis
for creating text
sequences\footurl{https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5}[2025-01-20],
which is ideal for generating test assertions.
(3)~\emph{Realistically trainable}:~The model with approximately 770
million parameters is trainable on a single \properNoun{Nvidia}
A100-80GB \gpu, which allows us to reasonably train and compare
multiple model variations.
(4)~\emph{Scalability}:~\codetfivelarge is smaller than comparable
models while at the same time outperforming larger
models~\cite{Le2022}. Due to its comparatively small size it is also
fast enough during inference when generating many test assertions.



We used the pre-trained \codetfivelarge
model\footurl{https://huggingface.co/Salesforce/codet5-large}[2025-01-20]~\cite{Le2022}
which can generate text sequences~(in this case, assertions) based on
the input tokens. We used the architecture of
\codetfivelarge~\cite{Le2022} without changes and allowed each
parameter to be trainable, \ie~we did not freeze any layers during
fine-tuning.
As optimiser, we used AdamW~\cite{Loshchilov2017} with a learning rate
of \exnum{2e-5}. The rest of the hyperparameters stayed at the default
values. We scheduled our training process with a linear scheduler and
used no warm-up steps.
We trained our model for ten epochs and used a batch size of 38 to
fill the available \qty[round-mode=none]{80}{\gibi\byte}
\initialism{gpu} memory.
To determine the input and output sequence lengths, we observed that
the concatenated test and focal method sequences are usually longer
than the assertion statement. Therefore, we adapted the sequence
lengths accordingly to allow up to \exnum{386} input tokens and up to
\exnum{64} output tokens.
While increasing the input and output token lengths would allow for
larger test methods and assertions without having to truncate them, it
would also increase the training duration since longer sequences need
additional \initialism{gpu} space and therefore necessitate the use of
a smaller batch size.



Following the two different preprocessing variants once using concrete
tokens and once using abstracted variants, we trained two different
model variants to investigate whether the improvement through the
abstraction process observed for \atlas~\cite{Watson2020} also occurs
for our Transformer-based model.
Prior research suggests that the byte-pair-encoding employed by
\properNoun{T5} to tokenise the code can effectively avoid
out-of-vocabulary~(\textsc{oov}) situations in code completion
scenarios~\cite{Karampatsis2020}.
However, even if not required to mitigate the \textsc{oov} problem, we
consider both the model variant with and without abstracted tokens in
our evaluation since the assertion statements follow a fairly strict
structure. Therefore, the abstracted tokens might still result in an
improvement in the prediction performance since the model can focus on
learning useful structures during the fine-tuning.



