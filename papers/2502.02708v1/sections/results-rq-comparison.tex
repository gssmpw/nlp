
\subsection{RQ2: Comparison to Existing
Approaches}\label{sec:rq-comparison}

In this research question, we explore how \assertfive compares to
state-of-the-art dedicated assertion generation models
\atlas~\cite{Watson2020}, \toga~\cite{Dinella2022}, pre-trained
\bart~\cite{Tufano2022}, and the general-purpose large language
model~(\llm) \gptfourmini.


\subsubsection{Experimental Setup}

\input{figures/gpt-prompt}

Some of the models we compare against required adaptions to the model
architecture or the data preprocessing to be usable as part of the
experiment.


\paragraph{Models}

The \atlas model~\cite{Watson2020} is accompanied by a replication
package %
which contains all the relevant training settings and scripts to start
the model training.
However, the code for training the sequence-to-sequence model is
missing in its repository. Deducing the originally used implementation
from documentation we integrated the \seqtoseq
library~\cite{Britz2017}.
Since the hyperparameters used to train \atlas are not specified in
the paper, we used the values available in the replication package
under the assumption that they represent the final optimal ones.
We increased the model training duration from \num{300000} to
\num{500000} steps since we noticed that the model had not learned
sufficiently in the shorter span but otherwise kept the default model
parameters of \atlas.
Furthermore, since \atlas also supports the raw and abstract variants,
we trained models on both datasets~(see
\cref{sec:data-preprocessing}).

For the double-pre-trained \bart transformer
model~\cite{Tufano2022}~(in the following called \doprebart) no
replication package was available.
We therefore asked the authors %
for guidance on
how to replicate the architecture and followed their recommendations.
The
\bart{}\footurl{https://huggingface.co/facebook/bart-large}[2025-01-20]
model~\cite{Lewis2020}, which had been pre-trained in English
language, was used as the base model. It was then fine-tuned on source
code using the \codesearchnet~\cite{Husain2019} dataset for
Java\footurl{https://huggingface.co/datasets/code_search_net/}[2025-01-20].
The pre-training finished after ten epochs, and we employed this
model checkpoint as a starting point for the second fine-tuning on our
assertions dataset in the raw variant to fine-tune for an additional
ten epochs.

We modified the process of \toga~\cite{Dinella2022} to \togaast.
The fine-tuned \bert model~\cite{Devlin2019} used by \toga to predict
the \texttt{try-catch} type of assertions remains unchanged.
For regular assertions, \toga uses another classifier to select the
best assertion from the given variants. However, this structure does
not allow for a \topk{k} selection of the assertions. To be able to
include \toga into our model comparison, we therefore replaced this
\bert classifier by a \bart~\cite{Lewis2020} sequence generation
model, \ie, \togaast no longer uses the assertion templates to generate
the assertion but retains the two-step decision which kind of
assertion should be generated.
We fine-tuned the two dedicated \bert and \bart models for ten epochs
and used the model with the best validation scores.

For the comparison with \chatgpt~\cite{Brown2020}, we created the
prompt template shown in \cref{fig:gpt-prompt}. The prompt uses a
short introduction explaining the task and expected response format to
the \llm. The zero-shot approach not relying on giving examples to the
\llm has been shown to perform better in similar
scenarios~\cite{Wang2024}.
Then, we tried to extract the assertions for each question discarding
the ones not following a sufficiently structured format. We required at
least ten assertion suggestions, to allow for a meaningful \topk{k}
analysis and removed all responses with fewer suggestions. We also
checked whether the returned code snippets corresponded to one of the
allowed assertion methods or the \texttt{try-catch} structure.
This resulted in \exnum{26963} usable responses.
We used the \properNoun{OpenAI} \api to query the
\textit{gpt-4o-mini-2024-07-18} model between 3 and 6 September~2024
using a temperature of 1.


\paragraph{Data Preprocessing}

To ensure a consistent \AST and thereby remove a possible confounding
factor~\cite{Utkin2022}, we reimplemented the transformation from
source code into the model-specific input format in the same tool we
also use for \assertfive~(see \cref{sec:data-preprocessing}).
We only use the dataset that allows up to ten assertions per test case
for this evaluation.

For \atlas, we used the raw and abstract variants with the
concatenated test and focal method.
Since \doprebart does not support an abstracted variant, we only used
the raw dataset also with concatenated test and focal method.
For the comparison with \chatgpt~\cite{Brown2020}, we exported the
test method, the focal method, and the expected assertion and created
prompts following the template shown in \cref{fig:gpt-prompt} which we
then sent to the \properNoun{OpenAI} \api.

For the \togaast preprocessing, we adapted the \toga steps slightly.
Like the original, we split the dataset into \texttt{try-catch}
assertions and regular assertion methods.
We construct inputs for regular assertions methods by concatenating
test method, focal method, and if available the method-level
documentation of the focal method. This sequence is truncated from the
end if it does not fit into the model input.
For the \texttt{try-catch} assertions, we retained the original
mechanism of concatenating test and focal method with a special
separator token in between and truncating both equally if necessary.
Finally, the two models to generate \texttt{try-catch} assertions and
to generate the regular assertions were trained separately on the
relevant subsets of the overall dataset.


\paragraph{Evaluation Metrics}

To compare the performance of the model variants, we use metrics
frequently used in the machine learning context.
The \topk{k} accuracy compares how often the model can predict
assertions fully matching the original. Looking at the precision,
recall, and F1 scores of the prediction of the assertion type~(see
\cref{tab:considered-assertion-types}) highlights if the model
understood enough about the code to at least suggest the correct
method.
Evaluating the \bleu scores highlights how close the generated
assertions are to the ground truth even when not achieving perfect
matches.
Specific to code models, we evaluate how often the model generates
syntactically correct Java code by checking if the generated assertion
can be parsed.
Similarly, previous research also used
the~\mbox{(\topk{k})}~accuracy~\cite{Watson2020,Dinella2022,Tufano2022}
or the \bleu score~\cite{Tufano2022} to evaluate model performance.


\input{figures/rq-model-comparison}

\subsubsection{Threats to Validity}

A threat to internal validity arises from the reconstruction of the
models we compare against. While we tried to implement the models as
close to their original as possible following their description, we
cannot guarantee that our implementations follow the original ones
exactly.
The training process of \chatgpt~\cite{Brown2020} highlights another
threat to internal validity. Since the model was trained on an
undisclosed large corpus of openly available data and our
\methodstotest-based dataset used for evaluation uses code mined from
open-source repositories from \github, we cannot ensure that the
training dataset of \chatgpt and our evaluation dataset are fully
distinct.
A threat to external validity may arise from the training process. We
relied mostly on the pre-determined hyperparameters present in the
replication packages for the comparison models and did not perform
further extensive tuning. There may be other parameter configurations
which improve model performance.
However, we expect the impact of different parameters to be limited,
since we either use the hyperparameters available in the replication
package~(\atlas) or reuse existing model architectures for
fine-tuning~(\bert, \bart) rather than training from scratch.


\subsubsection{Results}

\Cref{fig:model-comparison} shows the proportion of assertions that
the models predicted accurately. Both \assertfive models performed
better than the other models with the abstract variant performing best
by predicting the most assertions correctly with \perc{59.5}, followed
by the raw variant with \perc{51.2}.
The next-best model in our comparison is \togaast, with \perc{25.6} of
correctly predicted test assertions followed closely by
\chatgpt~(\perc{24.9}).

The accuracy of \atlas was only up to \perc{2.2}. We therefore cannot
confirm the results of~{Watson et
al.}~\cite{Watson2020}~(raw:~\perc{17.7}, abstract:~\perc{31.4}) with
our \atlas reimplementation. The results could be influenced by
training the models on a different dataset, using different
hyperparameters, or by a divergence in our reimplementation compared
to the original.
In our case \atlas only recognised a few different predictions. For
example, the raw model could only make twelve different predictions,
and \exnum{26315} predictions were always
\texttt{assertEquals(UNK, UNK.UNK())}, %
thus failing to find suitable replacement tokens.
Nevertheless, \assertfive still considerably outperforms the original
\atlas implementation and results~\cite{Watson2020}, and both the
\atlas with information retrieval extensions by Yu et
al.~\cite{Yu2022}~(accuracy: \perc{46.54}, \bleu: \perc{78.86}) and by
Sun et al.~\cite{Sun2023}~(accuracy: \perc{53.46}, \bleu:
\perc{80.77}), albeit on a different dataset compared to our
experiment.

When integrating an assertion suggestion tool, \eg,~as part of an \ide
plugin, it should show multiple alternative suggestions to the user.
The \topk{10} accuracy measures how often the actual assertion would
be part of the set of 10 suggestions, even in cases where the model
failed to predict the exact assertion as its first choice.
The score increase by \perc{67.6} compared to \topk{1} for \togaast
shows that this model generates a diverse set of assertions as part of
the \topk{10} suggestions. However, only in \perc{42.9} of the cases one
of them matches the original assertion.
The suggestions by the abstract \assertfive model may not be as
diverse~(\perc{35.6} score increase) but it generates more precise
assertions since one of the suggestions matches the original
\perc{80.7} of the time.
This may be more useful to a user in practice since they can
frequently select one of the suggestions without having to make
further changes to it.

The accuracy only considers the number of predictions that are fully
identical to the original. In practice, however, there are often
multiple alternative equivalent variants of an assertion like for
example \verb|assertEquals(0, list.size())| and
\verb|assertTrue(list.isEmpty())|. This could result in a model being
rated worse despite good quality performance in practice.
Such alternative assertions may explain why \chatgpt does not
outperform the other models even if it is much larger. Since the
training data of \chatgpt is not specific for assertion generation but
contains a broad spectrum of text data, the model may frequently
generate other similar assertions that do not entirely match the
expected one.

With the \bleu score such token-level similarities can be
measured~\cite{Papineni2002}.
As shown in \cref{fig:model-comparison}, \assertfive performed
similarly well in the raw and abstract variants with \bleu scores of
\perc{87.7} and \perc{90.5}, respectively.
Again, \togaast is the next-best model~(\perc{77.4}) followed by
\chatgpt~(\perc{68.3}).
\doprebart~(\perc{45.3}) and \atlas~(raw:~\perc{36.3},
abstract:~\perc{2.8}) obtained the lowest scores.

Focussing not on full assertion statements but only the assertion
types predicted by the model gives additional insights.
\Cref{fig:model-comparison} illustrates the precision and recall of
the assertion type classifications. The raw variant of \assertfive was
able to predict the assertion types with better
precision~(\perc{85.1}) and recall~(\perc{81.3}) than the abstract
variant~(precision: \perc{82.9}, recall: \perc{78.4}).
\togaast remained the third-strongest performer.

We found that the most frequent assertions in the dataset are
\texttt{assertEquals}~(\perc{59.34} of all assertions) and
\texttt{assertTrue}~(\perc{17.84}). To be useful, the models should
therefore be able to accurately generate such assertions.
This is represented by the prediction accuracy under the condition
that the assertion type was predicted correctly. In case of
\assertfive for \texttt{assertEquals}, this conditional
accuracy~(raw:~\perc{49.9}, abstract:~\perc{61.6}) is similar to the
overall accuracy. For \texttt{assertTrue} the conditional accuracy of
\assertfive even is substantially higher than the overall one for both
the raw~(\perc{66.7}) and the abstract~(\perc{76.7}) model.
This is in clear contrast to \doprebart where the model only predicts
the remainder of the assertion correctly in \perc{1.3} of cases when
it had suggested the assertion type \texttt{assertEquals} correctly
and therefore results in a low overall accuracy of \perc{2.0} even if
it can predict \perc{47.2} of \texttt{assertTrue} accurately.


\summary{2}{\assertfive outperforms the comparison models. Whereas the
abstract model predicts the assertions accurately, the raw model
predicts the assertion types more precisely.}


