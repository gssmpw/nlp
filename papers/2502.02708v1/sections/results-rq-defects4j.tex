
\subsection{RQ3: Bug-Detection Capability of Generated Assertions in
Developer-Written Tests}\label{sec:rq-defects4j}

To consider the actual practical applicability of the \assertfive
assertions in real-world projects, we use the bug database
\defectsforj~\cite{Just2014} to evaluate the bug-detecting
capabilities of both the raw and abstract \assertfive models by
combining developer-written tests with generated assertions.


\subsubsection{Experimental Setup}

\defectsforj~\cite{Just2014} is a database and extensible framework
that contains bugs that have occurred in real projects. The goal of
\defectsforj is to provide the software testing research community a
benchmark to compare new approaches reasonably.
We use six projects~(Chart, Closure, Lang, Math, Mockito, and Time)
from version 2.0.1 of the framework, containing a total of
\dfjTotalBugs non-deprecated bugs with \dfjTotalTests
developer-written bug-revealing test cases that fail with an assertion
failed error.
We remove test cases where inconsistencies~(\eg,~rare Unicode escapes
breaking regular expressions) arise during the experiment execution in
either the abstract or raw model variants.
Finally, we remove \dfjAssertionLocationInHelperTotal test cases where
the failing assertion is not located within the test itself but in
another helper method.
This results in a test corpus of
\dfjAssertionLocationInTestTotalBugs{} bugs with
\dfjAssertionLocationInTestTotalTests{} test cases.
Following previous work~\cite{Dinella2022,Tufano2022}, we generate an
assertion on the fixed version of the code. We therefore evaluate
whether \assertfive can be used to generate assertions for regression
tests.
An assertion generated by the model works as intended if it passes on
the fixed version of the code and fails on the buggy revision of the
production code.
To generate an assertion, we replace the originally failing assertion
with the placeholder~(see \cref{sec:data-preprocessing}) and pass the
test and focal method pair to \assertfive to generate a suitable
replacement.
The generated assertion is then placed back into the test case which
is executed against the buggy and fixed version of the production code
to check whether the assertion only fails on the buggy version.
By filtering the tests for ones that fail with an assertion error and
replacing the failing assertion, we ensure that the newly generated
assertion is the cause for the test failure rather than it being
caused by the test prefix.

Since the heuristics of \methodstotest~(see \cref{sec:methods2test})
were not sufficient to determine the focal method in all cases, we
extended them using additional information from the code difference
before and after the fix available in \defectsforj.
Candidates for the focal class are determined by matching the name to
the test class~(\ie, removing a \texttt{Test} affix from the name) and
by adding all classes that changed between the two code revisions.
Within those classes, we again check if a focal method can be found by
matching the name of the test case and a method in the production
code. To achieve this, we extract the subtokens from the names of the
test and focal methods and choose the method with the most common
subtokens as the focal method. If this fails, \ie,~there are no
intersecting subtokens, the last method call before the assertion is
chosen as focal method. In case this approach also proves
unsuccessful, we rely on the patch information to find changed
methods. In case the fix was located within a constructor, we treated
them like a proper focal method to still be able to provide some
context to the model.
For the one case where these automatic detection heuristics failed to
find a suitable method, we manually determined it by inspecting the
code.


\subsubsection{Threats to Validity}

A threat to the external validity arises from the \defectsforj
dataset.  Since it contains bugs from only six different open-source
projects which have been specifically sampled to be reproducible from
developer written test cases, it may not represent the structure of
test cases in other projects. However, the bugs in the dataset
represent real-world issues found in large open-source projects and
therefore are likely to be similar to bugs found in other projects.

Using the diff between bug-containing and fixed code for focal method
detection is an additional threat to external validity, since this is
only applicable in this specific dataset of \defectsforj. This option
is not feasible in real-world code scenarios. Given that predefined
heuristics for identifying the focal method from \methodstotest are
often inapplicable---such as for regression test names like
\texttt{testIssue1024}---we determined that using the diff provides a
valid alternative to at least find some suitable focal method
candidates.

We identify the exclusion of tests where the assertion is placed in a
helper method as threat to validity since this potentially enhances
the reported model performance.
Most of the test cases being removed for lacking a usable assertion
are part of the \emph{Closure} project. In
\dfjAssertionLocationInHelperClosure of \dfjTestsInProjectClosure
tests, the check for the expected result is placed in a separate helper
method, \eg,~\verb|checkCost("true", "1")| from \emph{Closure 28}.
For nearly all the excluded tests, the model would have produced an
uncompilable assertion due to using non-available variables or
methods, or generated a trivial assertion that passes in both the
buggy and failing code revisions. The bug detection capability
therefore remains nearly unchanged.
This threat highlights a research gap calling for alternative model
training methods: The strict pairing of only test and focal methods
does not always match the structure of real-world tests.
To provide \assertfive with the required context for the actual
assertion, it would be necessary to include all methods in the call
chain between the test method and the assertion as part of the input
to the model. Since the \assertfive model is trained on strict pairs
of test and focal methods, it has not been prepared during training
for the inclusion of such constructs.
Alternatively, by not including the helper methods in the input to
retrain the known structure, the test setup is obscured for the model
input. This holds especially in cases where the test case consists of
a single call to a helper method like frequently occurring in the
\emph{Closure} project, \eg,~only calling
\verb|testTypeCheck("JavaScript code")|.
Using \evosuite-generated tests like in previous
work~\cite{Dinella2022,Tufano2022} does not exhibit this limitation,
since the assertion never appears in helper methods in such tests.


\subsubsection{Results}

\input{tables/defects4j-assertions}

\cref{tab:defects4j-assertion-results} shows the distribution of the
\defectsforj results for both the abstract and the raw model variants.
A large share of the \dfjAssertionLocationInTestTotalTests{}
developer-written tests combined with the assertions generated by the
model are not compilable tests for both the
abstract~(\dfjAbstractNotCompilable) and the raw model
variant~(\dfjRawNotCompilable), resulting in
\dfjAbstractCompilableTestCases abstract and
\dfjRawCompilableTestCases raw compilable assertions. Of these,
\dfjAbstractFailsFixed abstract and \dfjRawFailsFixed raw assertions
fail on the fixed project variant. A test that has succeeded on the
fixed variant detects a bug only if it fails on the buggy variant of
\defectsforj. In total, there are \dfjAbstractFailsBuggy abstract and
\dfjRawFailsBuggy raw tests that could detect the bugs. The remaining
assertions~(abstract:~\dfjAbstractPassesBoth, raw:~\dfjRawPassesBoth)
pass both the fixed and the buggy variant. Consequently, they cannot
detect the specific bug identified by the \defectsforj dataset, but
may still be able to prevent future regressions.

In \defectsforj, every known bug has at least one bug-revealing test,
but multiple tests may reveal the same bug.  We define a bug as found
if it has at least one corresponding test case for which the generated
assertion fails only on the buggy version but passes on the fixed one.
Out of the \dfjAssertionLocationInTestTotalBugs{} bugs, the abstract
\assertfive model detected \dfjAbstractFoundBugs, while the raw model
identified \dfjRawFoundBugs bugs. In total, 14 were found by both
variants.

About one third of the tests no longer compiles after adding the
generated assertion.
Especially the raw model variants tends to sometimes generate
assertions requiring long String literals which are cut off at the
maximum output length of 64 tokens, resulting in missing closing
quotation marks or parentheses. Due to the shorter abstracted
constants, the abstract variant exhibits this problem only rarely.
Otherwise, most assertions are syntactically correct and calling
non-existing methods on objects is the common cause for the failing
compilation for both models.
However, constants for basic types like numbers or strings seem to be
handled correctly in comparisons or when appearing as parameters to
the assertion methods.

Considering the compiling assertions, only one third of them detect
the bugs~(abstract:~\perc{28.4}, raw:~\perc{29.9}). One reason for
this is that a large proportion of the predicted assertions already
fail in the fixed program variant. However, these often deviate only
slightly from the originally intended assertion, and may thus
nevertheless be helpful for developers.
For example, in the case of bug \emph{Chart 26}, the raw variant
predicted \verb|assertFalse(success)| and therefore the opposite of
the original assertion \verb|assertTrue(success)|.
Similarly, in the abstract variant for bug \emph{Math 91}, the
prediction only changed -1 appearing in the original assertion
\verb|assertEquals(-1, pi1.compareTo(pi2))| to 1.
The group of assertions that pass in both the buggy and fixed variant
also do not contribute to finding a bug. Trivial assertions that are
always fulfilled such as \verb|assertTrue(true)| often fall into this
category.
These examples illustrate the challenges involved in creating reliable
statements. On the one hand, even small deviations can cause the tests
to fail, even though they are consistent with the general intent,
while on the other hand, a too lenient assertion poses the risk of not
adequately capturing the intended functionality.

With \dfjRawFoundBugs out of \dfjAssertionLocationInTestTotalBugs bugs
being found, the bug-detection performance of \assertfive in our
experiment is worse compared to the evaluations of \toga~(finding 57
of 120)~\cite{Dinella2022}. However, the evaluations of \toga was
performed on \evosuite-generated rather than developer-written tests.
The structure of the automatically generated tests allows that the
heuristics as used during the \methodstotest training dataset
creation~(see \cref{sec:methods2test}) can be applied to determine the
focal method for the generated tests as well.
Since all tests where the focal method could not be determined were
removed from the \methodstotest data, both the training and evaluation
data is skewed towards tests following the required structure or
naming convention.
On the contrary, the developer-written tests used for our evaluation
do not have to conform to these requirements. We therefore expect that
our results are more representative of the performance when applying
the model in actual deployment scenarios.

Overall, while the model is able to successfully detect some bugs,
these results open up future research to improve upon various
limitations:
Firstly, some assertions are close to the original apart from swapped
comparisons or signs. In such cases \ide{}s could offer context-aware
actions that allow developers to quickly fix these instances.
Many of the remaining compilation errors could be fixed by models that
have a deeper understanding of the code semantics and can therefore
correctly apply more complex constructs.
Similarly, when designing such future more powerful models and
training them on more diverse inputs deviating from the strict
test/focal-method pairs~(\eg,~by additionally including
assertion-containing helper methods), the focus should not only be to
closely match the original assertion, but also evaluate whether the
deeper code-understanding helps to generate practically relevant
assertions.


\summary{3}{%
\assertfive detects \dfjRawFoundBugs of
\dfjAssertionLocationInTestTotalBugs bugs in our evaluation. While
this shows some promise towards the fault detection capabilities of
the model, the evaluation highlights practical limitations when
integrating the assertions into developer-written tests, thus
requiring future research.}


