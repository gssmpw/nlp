
\section{Introduction}

Developing reliable and high-quality software is a time-consuming and
resource-intensive process~\cite{Yang2006}. An important part of
creating such high-quality software is extensive testing of the
implemented functionality since the validation of program source code
plays a crucial role in identifying and eliminating potential errors
at an early stage~\cite{Beck2000}.


Since testing software is a time-consuming task, tools such as
\evosuite~\cite{Fraser2011} or
\properNoun{Randoop}~\cite{Pacheco2007}, and recently also tools based
on large language models~(\eg~\cite{Nie2023,Schaefer2024,He2024}), aim
to automatically generate test methods. However, sometimes developers
already have specific test scenarios with the setup of specific test
states in mind. A challenging step then is to generate test assertions
that check whether the developers' assumptions about the program state
hold.
We therefore aim to automatically generate new or additional
assertions within pre-existing test methods.

\input{figures/intro-example}

One possible approach to produce meaningful test assertions is to use
artificial intelligence to predict assertions from the source code. As
shown in \cref{fig:example-assertion-generation}, such a predictive
model receives the text of the test
method~(\cref{fig:intro-example-test}) and the method under test~(also
called the focal method, \cref{fig:intro-example-focal-method}) as its
input and is tasked with generating another text sequence containing
the assertion statement to fill in the placeholder. The model
therefore has to learn to reason about the semantics of the code
purely from its textual representation.
This approach has been employed successfully for example with the
\atlas~\cite{Watson2020} and \toga~\cite{Dinella2022} assertion
generation models.


Since the models work on the textual representation, they need to
learn to reason about a large vocabulary of constants and identifiers.
To support this in their model, \atlas integrated an abstraction
process that replaces such identifiers with abstract tokens to
decrease the vocabulary size. As an alternative, the use of larger
pre-trained natural language models that have been fine-tuned
twice~(on source code, then for assertions) has also been investigated
to alleviate the vocabulary problem~\cite{Tufano2022}.
However, since the introduction of these approaches, larger models
pre-trained on code have been released. Because such models are
trained on raw code without any abstractions, it is unclear whether
pre-trained models would need to be fine-tuned with abstracted or raw
tokens.
Furthermore, while prior results show that the focal method context
improves the prediction performance~\cite{Tufano2022}, it remains
unclear if other pre-existing assertions in the test method can
provide a sufficient alternative source of information.
Finally, approaches have also mainly been evaluated according to
common machine learning
metrics~\cite{Watson2020,Wang2024,Mastropaolo2021}, rather than by
demonstrating whether the assertions are effective at finding faults;
this has so far only been evaluated using artificially generated
tests~(\eg~using \evosuite)~\cite{Tufano2022,Dinella2022}, rather than
developer written tests.


To shed light on these open questions, we introduce \assertfive, a new
model for assertion generation that combines the features of the
previous approaches while alleviating the individual limitations by
using a larger language model that was pre-trained on source code and
then fine-tuned for the task of assertion generation.
The pre-trained code model does not need to learn the code structure
from scratch %
but instead can learn how
different abstract identifiers relate to each other during the
fine-tuning.
This base model requires only a single fine-tuning step on assertions,
thus avoiding catastrophic forgetting~\cite{Bower1989}, and
it already is designed to generate sequences of source code, allowing
us to generate both regular assertions statements and special ones
expecting an exception without requiring a grammar like prior
work~\cite{Dinella2022}.

In detail, the contributions of this work are:
\begin{itemize}
  \item We present the \assertfive assertion generator based on a
    fine-tuned \codetfive model and compare it against current
    state-of-the-art approaches.
  \item We show that pre-existing assertions in the test case improve
    the model performance in cases where the focal method cannot be
    determined.
  \item We evaluate the bug detecting capabilities by integrating
    generated assertions into developer-written tests.
\end{itemize}


