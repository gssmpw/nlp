
\section{Background}


\subsection{Deep-Learning-Based Assertion Generation}

As demonstrated by the example in
\cref{fig:example-assertion-generation}, deep learning models used to
generate assertions are usually text-based. They receive the source
code of the test method and optionally the code of the focal method to
generate the source code representing the assertion.
This task is therefore suitable for sequence-to-sequence
models~\cite{Britz2017}. Initial approaches like
\atlas~\cite{Watson2020} trained a dedicated recurrent neural network
to generate assertions. However, since code contains many different
project-specific identifiers and constants, the vocabulary that has to
be managed by the model is quite large unless vocabulary reducing
techniques such as using abstract tokens instead of concrete constants
and identifiers are introduced~\cite{Watson2020}.

Therefore, newer approaches tend to use larger
Transformer-based~\cite{Vaswani2017} architectures that are better
suited to efficiently handle such large vocabularies. By using a
generic Transformer base-model that has been pre-trained either on
natural language or source code, it only needs to be fine-tuned when
used as part the final assertion generation model. This shortens the
required training time and allows using larger, more expressive,
models.
Still, there are differences in how the underlying architecture is
integrated into the assertion generation model.
For example, the ability to fine-tune models allows for training a
\bart Transformer~\cite{Lewis2020} on large English natural language
dataset, then tuning it on source code, before finally tuning it again
to generate the assertion statements as required for the actual
task~\cite{Tufano2022}.
Other Transformer architectures like \bert~\cite{Devlin2019} are not
designed to generate sequences, but instead are used to classify the
input sequences. Combined with the insight that many assertion
statements follow a fairly rigid structure, such models are used as
part of the \toga~\cite{Dinella2022} approach. Instead of letting the
model freely generate source code, the approach generates a set of
valid assertion templates according to the allowed structures that can
be filled in with elements from the surrounding test method. The \bert
classifier therefore only needs to suggest the most suitable
template~\cite{Dinella2022}.


\subsection{T5 Transformer Models}

The Text-To-Text Transfer Transformer~(\tfive)~\cite{Raffel2019}
follows the encoder-decoder Transformer architecture and therefore is
designed for encoding input sequences into internal vector
representations which then are decoded back into output sequences.
It has been successfully used to generate sequences of source code
after pre-training the model on mixed sets of natural language and
Java source code before fine-tuning exclusively on
code~\cite{Mastropaolo2021}.

To create a model specifically designed to facilitate code-related
tasks, \codetfive~\cite{Wang2021}, a different set of pre-training
tasks like restoring the names of masked identifiers are used.
\codetfivelarge~\cite{Le2022} builds upon the same architecture but
introduces improved pre-training strategies, such as enhanced learning
objectives, expanded model sizes, and better datasets.


