
\section{Related Work}


The need for automatically generating test assertions first emerged in
the context of automated test generation. Since unit test generation
algorithms tend to focus on exploring sequences of calls, these
sequences need to be enhanced with assertions in order to help them
check for bugs other than unexpected exceptions or crashes. The Orstra
tool~\cite{Xie2006} introduced the idea to collect state information
while executing generated tests, and then instantiating assertion
templates based on the observed behaviour. By construction, these
assertions will pass on the code for which they were generated, such
that their main application lies in regression testing. Most
state-of-the art unit test generators such as
\evosuite~\cite{Fraser2011a}, \properNoun{Randoop}~\cite{Pacheco2007},
or \properNoun{Pynguin}~\cite{9793730} follow this approach. Since the
number of assertions that can be instantiated can be very large,
resulting in overly sensitive and unreadable test cases, test
assertions are often minimised using mutation analysis~\cite{6019060}.
The application of these techniques, however, has been limited to
automatically generated tests, rather than developer-written tests
which we focus on with our approach.

More recent assertion generation approaches use deep learning
techniques. While {Mastropaolo et al.} also present an approach based
on a smaller pre-trained \tfive base model, their evaluation only
reports \topk{k} accuracy metrics~\cite{Mastropaolo2021}.
By considering assertions to behave like regular statements, the next
test statement generation of \properNoun{TeCo}~\cite{Nie2023} can
generate assertions outperforming \atlas and \toga according to
accuracy and \bleu scores, but does not provide insights about their
performance on real bugs.


Many \llm{}-based approaches omit the task-specific model fine-tuning
and instead apply the large model directly to the task.
\properNoun{CEDAR}~\cite{Nashid2023} uses the \properNoun{CodeX} \llm
to demonstrate that prompting an \llm with a few examples of similar
focal and test method pairs before querying it with the actual request
yields more accurate assertions than \atlas.
Evaluating a similar \llm{}-based approach on Python rather than Java
code, \properNoun{CLAP}~\cite{Wang2024} achieves better performance
with zero-shot-prompting~(\ie,~no examples as part of the prompt).
The \togll~\cite{Hossain2024} approach compares various fine-tuned
code \llm{}s for the task of assertion generation. The evaluation on
artificially generated code mutastions shows that the strength of the
assertions in combination with \evosuite-generated tests is
significantly higher when compared to \toga~\cite{Hossain2024}.
In contrast to this paper, none of these approaches evaluate the
influence of the focal method or other assertions in the test method,
nor do they investigate the performance of the model when adding the
model-generated assertions to developer-written bug-detecting tests.
They also do not revisit the token abstraction process originally
proposed for the \properNoun{seq2seq}-based \atlas
model~\cite{Watson2020}. While it may no longer be necessary for
Transformer-based \llm{}s to overcome out-of-vocabulary
situations~\cite{Karampatsis2020}, our results show that it can
nevertheless be successfully applied to such models during fine-tuning
to generate more accurate assertions.


Rather than only generating assertions, \llm{}s have also been applied
to generate whole test methods for various commonly used programming
languages like JavaScript~\cite{Schaefer2024}, Python~\cite{Ryan2024},
Java~\cite{Chen2024}, or even supporting multiple
languages~\cite{He2024a}.
Besides closeness of the predicted tests to the original, their
evaluations frequently focus on the influence of the choice of
\llm~\cite{Schaefer2024,Ryan2024}, or
coverage~\cite{Schaefer2024,Ryan2024,Chen2024} rather than bug
detection capabilities of the generated tests.
We specifically focus on generating assertions rather than whole tests
with \assertfive. Developers might already have specific testing
scenarios in mind and have the expertise about the code semantics to
set up the test state accordingly. The assertion generation model then
acts in a supporting role to suggest possible additional checks that
might otherwise have been missed.


