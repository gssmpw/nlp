
\subsection{RQ1: Relevance of Context}\label{sec:rq-context}

To successfully create useful assertions as a developer, not only the
test method but also an understanding of the method under test is
important. In this research question we answer if this additional
context also helps the \assertfive model during the automatic
assertion generation.
Since it is often difficult to determine the corresponding test method
in the practical application of the models (for example, in an \ide
plugin), it is of practical relevance to what extent the model still
works if the context of the focal method is unavailable.
Since the model has to rely exclusively on the test method code as
context in such cases, we also evaluate whether other pre-existing
assertions in the test case can replace the missing context
sufficiently.


\subsubsection{Experimental Setup}\label{sec:rq-context-setup}

As described in \cref{sec:data-filtering}, we created three datasets
with up to one, five, or ten assertions in each test method,
respectively.
For all three of those subsets, we use the variant using the raw
rather than the abstract tokens~(see \cref{sec:data-preprocessing}) to
retain the original context of the user-defined identifiers.
For each of the three datasets, we trained two model variants where
the first one receives only the tokens of the test method as input,
while the second one receives the tokens of both test and focal
methods.
This allows us to look at two different types of context: the dataset
choice demonstrates if the model can learn from possibly similar
assertion examples in the test method and the model input variant
highlights the importance of the context available from the method
under test.
In the further course of this section, we call the model variant that
only receives the test method \enquote{\temet{}} and the one that also
receives the focal method \enquote{\fomet{}}. When referring to a
specific model instance, an index indicates the used training dataset,
\eg~\temet[5] was trained on the dataset containing test methods with
up to five assertions each.

To compare the performance of the model variants, we use metrics
frequently used in the machine learning context.
The \topk{k} accuracy compares how often the model can predict
assertions fully matching the original. Looking at the precision,
recall, and F1 scores of the prediction of the assertion type~(see
\cref{tab:considered-assertion-types}) highlights if the model
understood enough about the code to at least suggest the correct
method.
Evaluating the \bleu scores highlights how close the generated
assertions are to the ground truth even when not achieving perfect
matches.
Specific to code models, we evaluate how often the model generates
syntactically correct Java code.


\subsubsection{Threats to Validity}

A threat to construct validity may arise from the maximum input
sequence length of 386 in both models. Since the input sequences for
\fomet are longer than for \temet, sometimes parts of the focal method
had to be truncated. This may inhibit the ability for \fomet to
capture all relevant semantics of the input.
In the dataset with one assertion, the data for \temet had an average
length of 95.5 (median: 65), and \perc{2.1} of the input data got
truncated. Equivalently, inputs for \fomet had an average length of
230.5 (median: 162) which resulted in a truncation for \perc{14.2} of
the inputs.
Using a longer input token sequence length was infeasible regarding
training duration~(see \cref{sec:model}). Therefore, \fomet might
underperform in our evaluation compared to an otherwise identical
model that allows longer input sequences.


\subsubsection{Results}

\input{tables/focal-method-influence}

\cref{tab:comparison-focal-method} shows the performance of \temet and
\fomet. On the smallest dataset, \temet[1] had a \topk{1} accuracy of
\perc{37.23} but was clearly outperformed by \fomet[1] which achieved
an exact match for \perc{43.95} of the samples.
\temet benefitted more from an increased number of assertions to
improve the score by \perc{10.58}-points to \perc{47.81}, while \fomet
only achieved an improvement by \perc{7.26}-points to \perc{51.21}.

The score assimilation pattern repeats for the \bleu scores where
\temet[1] achieved a score of \perc{78.57} when evaluating the dataset
with one assertion~(\fomet[1]: \perc{82.54}).
The precision, recall, and F1 scores show that \fomet outperformed
\temet when considering only the assertion type of the prediction.
The additional context has no relevant influence on the modelsâ€™
ability to generate syntactically correct assertions with both models
nearly always producing parseable Java code.

The \temet[5] variant achieves better accuracy than \fomet[1], which
suggests that the context from pre-existing assertions in the test can
compensate for the lack of focal method context. While further
increasing the number of assertions in the context improves the
results for \temet[10], the worse results compared to \fomet[5] show
that at this point the focal method provides more relevant information
than the additional assertions.

The values for the datasets with multiple assertions might be closer
together because the tokeniser has to truncate more parts of the focal
method due to the longer input.
An alternative explanation for the values moving closer together could
be that there are more assertions that can be used by the models to
create useful assertions similar to the already existing ones without
having to rely on the context of the focal method.
This importance of assertions serving as example is supported by the
close \bleu scores in case of ten assertions since \temet predicts
assertions closely matching the original even without the focal method
context.

Overall, the results show that additional assertions already present
in the test do not \enquote{distract} the model from predicting
additional different assertions but on the contrary allow the model to
improve from the additional context.
In practice, it might not be always possible to automatically
determine a relevant focal method using
heuristics~\cite{He2024,Tufano2022a}. Our results confirm previous
results that using the method under test as additional input is
beneficial~\cite{Tufano2022}, but also show that the model can still
produce useful suggestions without.
Additionally, our results show that other assertions in the test case
can provide sufficient context for the model to offset the negative
impact of a missing focal method context.


\summary{1}{\assertfive performed better when we added the focal
method to the input sequence. Pre-existing assertions in the test case
can provide a similar improvement to the model performance as the
focal method context. Combining both sources of context results in the
overall best results.}


