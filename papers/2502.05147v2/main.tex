\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{cleveref}  % 为了使用 \cref
\usepackage{marvosym}

\usepackage{xspace}
\newcommand{\ie}{i.e.\xspace}
\newcommand{\eg}{e.g.\xspace}
\newcommand{\etal}{et al.\xspace}
\newcommand{\wrt}{w.r.t.\xspace}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{LP-DETR: Layer-wise Progressive Relations for Object Detection}

% \author{Anonymous ICME submission}

\author{\IEEEauthorblockN{Zhengjian Kang$^{1,}$\IEEEauthorrefmark{1}\textsuperscript{\Letter},
Ye Zhang$^{2,}$\IEEEauthorrefmark{1},
Xiaoyu Deng$^{3}$, 
Xintao Li$^{4}$
and Yongzhe Zhang$^{5}$}
\IEEEauthorblockA{$^{1}$New York University, NY 10012, USA}
\IEEEauthorblockA{$^{2}$University of Pittsburgh, PA 15213, USA}
\IEEEauthorblockA{$^{3}$Fordham University, NY 10458, USA}
\IEEEauthorblockA{$^{4}$Georgia Institute of Technology, GA 30332, USA}
\IEEEauthorblockA{$^{5}$California Institute of Technology, CA 91125, USA}

$^{1}$zk299@nyu.edu, $^{2}$yez12@pitt.edu, $^{3}$xdeng24@fordham.edu, $^{4}$xli3204@gatech.edu, $^{5}$yongzhe@caltech.edu
% <-this % stops an unwanted space
\thanks{\IEEEauthorrefmark{1}Equal contribution to this work, \textsuperscript{\Letter}Corresponding author.}
}
% \textsuperscript{\Letter}Corresponding author.


\maketitle


\begin{abstract}
This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach that enhances DETR-based object detection through multi-scale relation modeling. Our method introduces learnable spatial relationships between object queries through a relation-aware self-attention mechanism, which adaptively learns to balance different scales of relations (local, medium and global) across decoder layers. This progressive design enables the model to effectively capture evolving spatial dependencies throughout the detection pipeline. Extensive experiments on COCO 2017 dataset demonstrate that our method improves both convergence speed and detection accuracy compared to standard self-attention module. The proposed method achieves competitive results, reaching 52.3\% AP with 12 epochs and 52.5\% AP with 24 epochs using ResNet-50 backbone, and further improving to 58.0\% AP with Swin-L backbone. Furthermore, our analysis reveals an interesting pattern: the model naturally learns to prioritize local spatial relations in early decoder layers while gradually shifting attention to broader contexts in deeper layers, providing valuable insights for future research in object detection.
\end{abstract}
\begin{IEEEkeywords}
object detection, detection transformer, relation network, self-attention
\end{IEEEkeywords}


\section{Introduction}
\label{sec:intro}

DEtection Transformers (DETRs)~\cite{carion2020end} have achieved great progress by proposing an end-to-end architecture for object detection. However, their low training efficacy remains a critical challenge. The root cause is the imbalanced supervision during training - DETR employs Hungarian algorithm to assign only one positive prediction to each ground-truth box, leaving the majority of predictions as negative samples. This insufficient positive supervision leads to slow and unstable convergence. While various approaches have been proposed to address this issue through different technical routes like multi-scale feature learning~\cite{zhu2020deformable}, denoising training~\cite{li2022dn,zhang2022dino}, hybrid matching strategies~\cite{zong2023detrs,chen2023group} and loss alignment~\cite{liu2023detection,cai2023align}, they primarily focus on local feature enhancement or query learning optimization, leaving the potential of relation modeling in self-attention not been fully explored.

In the vision community, modeling inter-object relationships has proven beneficial for detection performance. Previous approaches mainly focus on two aspects: co-occurrence patterns of object categories~\cite{krishna2017visual,xu2019reasoning,chen2018iterative,hao2023relation} and spatial relations using various criteria ~\cite{hu2018relation,zhao2021,vaswani2017attention}. 
These methods have demonstrated that incorporating relation information can effectively enhance detection accuracy by capturing contextual dependencies between objects. 
However, in DETR field, few works have investigated the learnable relation between object queries in the self-attention, a key component in DETR decoders. 
Hao \etal~\cite{hao2023relation} attempt to model class correlations using a learnable relation matrix in the decoder's self-attention, but their approach does not consider spatial information and requires mapping class-to-class relations back to query-to-query interactions. 
More recently, Relation-DETR~\cite{hou2024relation} introduces explicit position relations between bounding boxes with cross-layer refinement. 
Motivated by their work but different from these approaches, we directly incorporate geometric relation weights into queries within each layer and propose layer-specific relation modeling to capture evolving spatial dependencies.

In this paper, we present LP-DETR (Layer-wise Progressive DETR), which enhances object detection through explicit modeling of multi-scale spatial relations across decoder layers. Our key insight is that object relations naturally evolve from local to global contexts through the detection pipeline, and different scales of spatial relations may play varying roles at different stages of the detection process. Based on this observation, we propose a progressive relation-aware self-attention module that adaptively learns to balance different scales of spatial relations at different decoder layers. This design allows the model to capture fine-grained local relationships in early layers while gradually incorporating broader relation information in deeper layers.
The main contributions of our work are threefold:
\begin{itemize}
\item We introduce a relation-aware self-attention mechanism that explicitly models multi-scale spatial relationships between object queries.
\item We propose a progressive refinement strategy that allows the model to adaptively adjust relation weights across decoder layers.
\item We discover and validate an interesting pattern where spatial relations naturally progress from local to global contexts through decoder layers, providing valuable insights for future research.
\end{itemize}

Finally, we conduct extensive experiments on COCO 2017 dataset to demonstrate the effectiveness of our approach. LP-DETR achieves competitive results with 52.3\% AP under 12-epoch training and 52.5\% AP under 24-epoch training using ResNet-50 backbone. With Swin-L backbone, our method further improves to 58.0\% AP. More importantly, our analysis reveals that the proposed progressive relation modeling contributes to both improved convergence and detection accuracy. These results validate our hypothesis about the importance of layer-wise relation modeling and suggest promising directions for future research in object detection.


\section{Related Work}
\label{sec:related-work}

\subsection{Transformer for Object Detection}
\label{sec:transformer-object-detection}
DEtection TRansformer (DETR)~\cite{carion2020end} establishes a new paradigm for end-to-end object detection by eliminating hand-crafted post-processing steps such as Non-maximum Suppression (NMS). Its transformer-based architecture consists of two main components: an encoder that transforms flattened image features into enriched memory representations, and a decoder that converts a set of learnable object queries into final detection results. The decoder operates through two attention mechanisms: self-attention for modeling interactions among object queries, and cross-attention for capturing relationships between queries and encoded memory features.

However, DETR suffers from slow convergence during training, and various approaches have been proposed to address this issue from different methodological perspectives: (1) Enhanced Feature Learning: Deformable DETR~\cite{zhu2020deformable} explores multi-scale features through deformable attention with sparse reference points, while Focus-DETR~\cite{zheng2023more} and Salience-DETR~\cite{hou2024salience} improve feature selection through salient token identification in the encoder. (2) Query Enhancement: DAB-DETR~\cite{liu2021dab} decouples object queries into 4D anchor box coordinates for iterative refinement, while DN-DETR~\cite{li2022dn} and DINO~\cite{zhang2022dino} accelerate training through auxiliary denoising task and contrastive learning. (3) Better Supervision: Hybrid DETR~\cite{zong2023detrs} and Group DETR~\cite{chen2023group} adopt one-to-many matching to increase supervision signals, while Stable-DINO~\cite{liu2023detection} and Align-DETR~\cite{cai2023align} propose specialized loss functions to align classification and localization. (4) Attention Mechanism: Recent works focus on improving attention mechanisms, where Cascade-DETR \cite{ye2023cascade} enhances query-feature interactions through cross-attention, and Relation-DETR~\cite{hou2024relation} learns explicit relation modeling between queries in self-attention.

\subsection{Relation Network}
\label{sec:relation-network}
Relation networks have emerged as a powerful approach for modeling inter-object relationships at instance level, which can be broadly categorized into two main directions: co-occurrence modeling and spatial relation modeling.

Co-occurrence approaches focus on capturing statistical dependencies between object categories. Some methods~\cite{krishna2017visual,xu2019reasoning} directly learn from category distribution patterns in large datasets, while others~\cite{chen2018iterative,hao2023relation} adaptively learn class relationships from annotations. However, these approaches either rely on fixed statistical priors or encounter with challenges in mapping between instances and categories~\cite{xu2019reasoning}.

Spatial relation approaches construct graph structures where object features serve as nodes and their spatial relationships as edges. Pioneering works like Relation Network~\cite{hu2018relation} introduces geometric weights in attention modules to model spatial relations. Recent methods determine relation weights through various metrics, such as position-aware distance~\cite{bi2022srrv,lin2021core}, attention mechanisms~\cite{zhao2021,vaswani2017attention} or appearance similarity~\cite{li2020gar}. While these learnable relations offer greater flexibility compared to fixed priors, they typically require larger datasets and longer training time to effectively learn the relations from data.


\section{Methodology}
\label{sec:method}

\subsection{DETR Preliminaries}
\label{sec:detr-pre}
A DETR-style detector consists of a backbone network (\eg, ResNet \cite{he2016deep}, Swin Transformer \cite{liu2021swin}) and a transformer architecture with encoder and decoder modules. Given an input image, the backbone first extracts image features, which are then split into patch tokens. The transformer encoder processes these tokens through self-attention mechanisms and outputs enhanced feature representations, denoted as memories $\mathbf{Z} = \{z_1, ..., z_m\}$.

The transformer decoder takes a set of learnable object queries $\mathbf{Q} = \{q_1, ..., q_n\}$ as input. Recent works \cite{liu2021dab, zhang2022dino} propose to decouple these queries into content queries $\mathbf{Q^c}$ for label embedding and position queries $\mathbf{Q^p}$ for bounding box prediction, enabling better relation alignment. The decoder consists of $L$ stacked blocks, where each block contains three sequential components: a self-attention layer, a cross-attention layer, and a feed-forward network (FFN).

The self-attention layer enables communication between object queries, allowing each query to refine its prediction by considering other queries' predictions. The cross-attention layer facilitates interaction between object queries $\mathbf{Q}$ and encoded memories $\mathbf{Z}$, aggregating features for object localization and classification. Finally, the FFN transforms the query embeddings for prediction through parallel classification and regression heads. 

\label{sec:self-attention}
\begin{figure*}[!thbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/prog-detr-arch.pdf} 
  \caption{DETR with layer-wise progressive relation pipeline. A learnable relation-aware self-attention mechanism that augments object queries with multi-scale spatial relations, which adaptively evolve from local to global contexts across decoder layers for progressive detection refinement.}
  \label{fig:icme-detr-arc}
\end{figure*}

\subsection{Layer-wise Progressive Relation-Aware Attention}

The high-level architecture of our proposed progressive relation-aware DETR model is presented in Fig.~\ref{fig:icme-detr-arc}. Our proposed attention is applied into the self-attention in the decoder component.  Let's consider object queries $\mathbf{Q}$ consists of content embedding queries $\mathbf{Q^c}$, reference box position queries $\mathbf{Q^p}$ (represented by $(x,y, w, h)$), and relation queries $\mathbf{Q^r}$. Given a set of $N$ object queries, ${q_i=\{q_i^c, q_i^p, q_i^r\}}_{i=1}^N$, the $i$-th relation query $q^r_i$ with respect to all the object queries can be calculated as the weighted sum of all the queries:
\begin{equation}
\label{eq:rel-query}
q^r_i = \sum_{j=1}^{N} w^r_{ij} \cdot (W_V \cdot q_j^c).
\end{equation}
The relation weight $w^r_{ij}$ captures both geometric and content attention based relationships between queries, which is computed as:
\begin{equation}
\label{eq:rel-query-rel-weight}
w^r_{ij} = \frac{w^p_{ij} \cdot \exp{(w^c_{ij})}}{\sum_{k=1}^{N} w^p_{ik} \cdot \exp{(w^c_{ik})}},
\end{equation}
\begin{equation}
\label{eq:rel-query-attention-weight}
w^c_{ij} = \frac{(W_Q \cdot q_i^c) \cdot (W_K \cdot q_j^c)}{\sqrt{d_k}},
\end{equation}
where $W_Q$, $W_K$ and $W_V$ are learnable projection matrices for query, key and value in self-attention, $d_k$ denotes the embedding size of $W_Q\cdot q_i^c$. The attention scores $w^c_{ij}$ are normalized by geometric weights $w^p_{ij}$ to obtain the final relation weights $w^r_{ij}$.

The geometric weight $w^p_{ij}$ incorporates spatial relationships through:
\begin{equation}
\label{eq:rel-query-geo-weight}
w^p_{ij} = W_G \cdot E\Big(R(q^p_i, q^p_j)\Big),
\end{equation}
\begin{equation}
\label{eq:rel-query-Rel-metric}
\begin{split}
R(q^p_i, q^p_j) = \Big(&\log(\frac{|x_i-x_j|}{w_i}), \log(\frac{|y_i-y_j|}{h_i}), \\
&\log(\frac{w_i}{w_j}), \log(\frac{h_i}{h_j}), \texttt{giou}(q^p_i, q^p_j) \Big),
\end{split}
\end{equation}
where the relation metric $R$ captures spatial transformations in distances, scales and gIoU. The embedding function $E$ maps these 5-D features to high-dimensional space using sinusoidal encoding \cite{vaswani2017attention}, followed by a learnable projection $W_G$ implemented as MLP with ReLU activation.
Then the overall object query integrates information from multiple attention heads:
\begin{equation}
\label{eq:rel-query-output}
q_i^c = q_i^c + \texttt{MLP}\big(\texttt{Concat}(q^{r1}_i, ..., q^{rK}_i)\big),
\end{equation}
where $K$ denotes the number of relation-aware attention heads. The $\texttt{Concat}$ operator aggregates the relation queries, and $\texttt{MLP}$ enhances the object queries with learnable query-to-query relation weights, making them more sensitive to spatial relations during training. The details of relation-aware self-attention module is presented in Fig.~\ref{fig:self-attention}.

\begin{figure}[!tb]
  \centering
  \includegraphics[width=0.43\textwidth]{figures/self-attention.png} 
  \caption{Relation-aware self-attention module architecture. The module takes object queries and geometric weights as inputs and produces relationally-enhanced object queries through weighted self-attention mechanism.}
  \label{fig:self-attention}
  \vspace{-4pt}
\end{figure}

To investigate how different scales of relations evolve across decoder layers, we propose three types of relation metrics: local, medium and global relations. The local relation $R_{l}$ uses the original metric in Eq. \ref{eq:rel-query-Rel-metric}, emphasizing relative distances and scale variations. 
The medium relation $R_{m}$ applies a scaling factor of $(1+2 \times l/L)$, where $l$ is $l$-th decoder layer, to reduce the steepness of the log function. 
The global relation $R_{g}$ uses constant weights 1.0. Thus the geometric weight at the $l$-th decoder layer is formulated as:
\begin{equation}
\label{eq:rel-weight-position}
w^p_{ij} = W_G \cdot \Lambda_l \cdot E\Big( R_{l}(q^p_i, q^p_j); R_{m}(q^p_i, q^p_j); R_{g}(q^p_i, q^p_j) \Big),
\end{equation}
where $\Lambda = [\lambda_{l}; \lambda_{m}; \lambda_{g}]_{L \times 3}$ represents learnable weights that adaptively adjust the importance of different relation scales across decoder layers.


\section{Experiments}
\label{sec:experiments}

\subsection{Experiment Settings}
\label{subsec:experiment-settings}

\subsubsection{Dataset and backbone}
We evaluate our Progressive Relation-Aware DETR on COCO 2017~\cite{lin2014microsoft}, which contains 118k training images and 5k validation images across 80 object categories. The performance is evaluated on the validation set using standard COCO metrics: average precision (AP) at different IoU thresholds (IoU=0.5, 0.75, 0.5:0.95) and scales (small, medium, large).
We implement our method with two backbone networks: ResNet50~\cite{he2016deep} pretrained on ImageNet-1k and Swin-Large~\cite{liu2021swin} pretrained on ImageNet-22k~\cite{deng2009imagenet}. Both backbones are finetuned with an initial learning rate of $1\times10^{-5}$, which is decreased by a factor of $0.1$ at later stages.

\subsubsection{Implementation details}
All experiments are conducted on NVIDIA RTX 3090 GPUs using AdamW optimizer~\cite{kingma2017adam} with a weight decay of $1\times10^{-4}$ and a total batch size of 16. For the relation embedding module, we set the temperature $T=10000$, scale $s=100$, and position embedding dimension $d_\text{pos}=16$ in the sinusoidal encoding. The position relations are constructed at three scales (local, medium and global) with equal initial weights $0.33$ across all 6 decoder layers. Following standard practice in DETR-based methods, we apply common data augmentations including random resize, crop and flip during training. We report results under both $1$x ($12$ epochs) and $2$x ($24$ epochs) training schedules.

\subsection{Experiment Results}
\label{sec:experiment-results}

\begin{table*}[!htbp]
  % \setlength\tabcolsep{6pt}
  % \small
  \centering
  \caption{Evaluation on COCO \texttt{val}2017 with state-of-the-art methods using ResNet-50 backbone.}
  \label{tab:exp coco2017-resnet}
  \begin{tabular}{@{}lccccccccc@{}}
  \hline
    Method & Backbone & Epochs & AP & AP$_{50}$ & AP$_{75}$ & AP$_S$ & AP$_M$ & AP$_L$ \\ 
  \hline
    Def-DETR~\cite{zhu2020deformable}            & ResNet-50 & 50     & 46.2          & 65.2          & 50.0          & 28.8          & 49.2          & 61.7          \\
    DAB-DETR~\cite{liu2021dab}                   & ResNet-50 & 50     & 42.6          & 63.2          & 45.6          & 21.8          & 46.2          & 61.1          \\
    DN-Def-DETR~\cite{li2022dn}                  & ResNet-50 & 12     & 46.0          & 63.8          & 49.9          & 27.7          & 49.1          & 62.3          \\
    DINO~\cite{zhang2022dino}                    & ResNet-50 & 12     & 49.7          & 67.0          & 54.4          & 31.4          & 52.9          & 63.6          \\
    $\mathcal{H}$-Def-DETR~\cite{jia2023detrs}   & ResNet-50 & 12     & 48.7          & 66.4          & 52.9          & 31.2          & 51.5          & 63.5          \\
    Cascade-DETR~\cite{ye2023cascade}            & ResNet-50 & 12     & 49.7          & 67.1          & 54.1          & 32.4          & 53.5          & 65.1          \\
    $\mathcal{C}$o-Def-DETR~\cite{zong2023detrs} & ResNet-50 & 12     & 49.5          & 67.6          & 54.3          & 32.4          & 52.7          & 63.7          \\
    Align-DETR~\cite{cai2023align}               & ResNet-50 & 12     & 50.2          & 67.8          & 54.4          & 32.9          & 53.3          & 65.0          \\
    Stable-DINO~\cite{liu2023detection}          & ResNet-50 & 12     & 50.4          & 67.4          & 55.0          & 32.9          & 54.0          & 65.5          \\
    DAC-DETR~\cite{hu2024dac}                    & ResNet-50 & 12     & 50.0          & 67.6          & 54.7          & 32.9          & 53.1          & 64.4          \\
    Rank-DETR~\cite{pu2024rank}                  & ResNet-50 & 12     & 50.4          & 67.9          & 55.2          & 33.6          & 53.8          & 64.2          \\
    MS-DETR~\cite{zhao2024ms}                    & ResNet-50 & 12     & 50.3          & 67.4          & 55.1          & 32.7          & 54.0          & 64.6          \\
    Relation-DETR~\cite{hou2024relation}         & ResNet-50 & 12     & 51.7          & 69.1          & 56.3          & \textbf{36.1} & 55.6          & 66.1          \\
    LP-DETR (ours)                               & ResNet-50 & 12     & \textbf{52.3} & \textbf{69.6} & \textbf{56.8} & 35.8          & \textbf{55.9} & \textbf{66.6} \\
    \hline
    $\mathcal{H}$-Def-DETR~\cite{jia2023detrs}   & ResNet-50 & 36     & 50.0          & 68.3          & 54.4          & 32.9          & 52.7          & 65.3          \\
    DINO~\cite{zhang2022dino}                    & ResNet-50 & 36     & 51.2          & 69.0          & 55.8          & 35.0          & 54.3          & 65.3          \\
    DINO~\cite{zhang2022dino}                    & ResNet-50 & 24     & 50.4          & 68.3          & 54.8          & 33.3          & 53.7          & 64.8          \\
    DDQ-DETR~\cite{zhang2023dense}               & ResNet-50 & 24     & 52.0          & 69.5          & \textbf{57.2} & 35.2          & 54.9          & 65.9          \\
    Relation-DETR~\cite{hou2024relation}         & ResNet-50 & 24     & 52.1          & 69.7          & 56.6          & 36.1          & 56.0          & 66.5          \\
    LP-DETR (ours)                               & ResNet-50 & 24     & \textbf{52.5} & \textbf{70.0} & \textbf{57.2} & \textbf{36.2} & \textbf{56.3} & \textbf{67.1} \\
    \hline
  \end{tabular}
  \vspace{-5pt}
\end{table*}

\begin{table*}[!htbp]
  % \setlength{\tabcolsep}{4.35pt}
  \centering
  \caption{Evaluation on COCO \texttt{val}2017 with state-of-the-art methods using Swin-L backbone.}
  \label{tab:exp coco2017-swin}
  \begin{tabular}{@{}lccccccccc@{}}
    \hline
    Method                                     & Backbone & Epochs & AP            & AP$_{50}$     & AP$_{75}$     & AP$_S$        & AP$_M$        & AP$_L$        \\ 
    \hline
    DINO~\cite{zhang2022dino}                  & Swin-L   & 12     & 56.8          & 75.4          & 62.3          & 41.1          & 60.6          & 73.5          \\
    $\mathcal{H}$-Def-DETR~\cite{jia2023detrs} & Swin-L   & 12     & 55.9          & 75.2          & 61.0          & 39.1          & 59.9          & 72.2          \\
    Rank-DETR~\cite{pu2024rank}                & Swin-L   & 12     & 57.3          & 75.9          & 62.9          & 40.8          & 61.3          & 73.2          \\
    Relation-DETR~\cite{hou2024relation}       & Swin-L   & 12     & 57.8          & 76.1          & 62.9          & \textbf{41.2} & 62.1          & 74.4          \\
    LP-DETR (ours)                             & Swin-L   & 12     & \textbf{58.0} & \textbf{76.4} & \textbf{63.2} & 41.0          & \textbf{62.2} & \textbf{74.7} \\
    \hline
  \end{tabular}
  \vspace{-5pt}
\end{table*}

We evaluate our model on COCO 2017 validation dataset using both ResNet-50~\cite{he2016deep} and Swin-L~\cite{liu2021swin} backbones. The results are summarized in Tables \ref{tab:exp coco2017-resnet} and \ref{tab:exp coco2017-swin}.
Using ResNet-50 backbone with 12-epoch training, our model achieves competitive results of 52.3\% AP, 69.6\% AP$_{50}$ and 56.8\% AP$_{75}$. When compared to Relation-DETR~\cite{hou2024relation}, we observe consistent improvements across all metrics (+0.6\% AP, +0.5\% AP$_{50}$ and +0.5\% AP$_{75}$). Analysis on objects of different scales shows that our method achieves 35.8\% AP$_S$, 55.9\% AP$_M$ and 66.6\% AP$_L$, with notable gains on medium (+0.3\% AP$_M$) and large objects (+0.5\% AP$_L$) while maintaining competitive performance on small objects (-0.3\% AP$_S$). These improvements become more evident with 24-epoch training, where our method further surpasses Relation-DETR by 0.4\% AP, 0.3\% AP$_{50}$ and 0.6\% AP$_{75}$.

Furthermore, our approach demonstrates strong scalability to larger backbones. With Swin-L backbone under 12-epoch training, we achieve state-of-the-art performance of 58.0\% AP, 76.4\% AP$_{50}$ and 63.2\% AP$_{75}$, improving upon the previous best results from Relation-DETR by 0.2\% AP, 0.3\% AP$_{50}$ and 0.3\% AP$_{75}$.




\subsection{Ablation study}
\label{subsec:ablation-study}
\textbf{Analysis of the number of relation heads}. We examine how the number of relation heads affects model performance in our relation-aware self-attention module.
Table~\ref{tab:selection-of-relation-module} shows the results with varying numbers of relation heads while maintaining a total of 8 attention heads. Using no relation head (0 head) serves as our baseline, where the module functions as standard self-attention, achieving 51.1\% AP, 68.6\% AP$_{50}$ and 55.8\% AP$_{75}$. The performance consistently improves as we increase the number of relation heads. With all 8 heads dedicated to relation-aware attention, our model achieves the best performance of 52.3\% AP, showing an improvement of 1.2\% AP over the baseline. Comparable gains are also observed in AP$_{50}$ (+1.0\%) and AP$_{75}$ (+1.0\%). These results demonstrate the benefit of incorporating relation-aware attention in the self-attention mechanism.

\begin{table}[!t]
  \centering
  \caption{Performance comparison of different number of relation heads in Relation-Aware self-attention module}
  \label{tab:selection-of-relation-module}
  \begin{tabular}{@{}lccccccc@{}}
  \hline
    \# of heads & AP            & AP$_{50}$     & AP$_{75}$     & AP$_S$        & AP$_M$        & AP$_L$        \\
  \hline
    0                  & 51.1          & 68.6          & 55.8          & 35.1          & 55.2          & 66.0          \\
    2                  & 51.4          & 69.0          & 56.2          & 35.2          & 55.6          & 66.1          \\
    4                  & 51.8          & 69.4          & 56.4          & 35.4          & \textbf{56.2} & 66.1          \\
    8                  & \textbf{52.3} & \textbf{69.6} & \textbf{56.8} & \textbf{35.8} & 55.9          & \textbf{66.6} \\
  \hline
  \end{tabular}
\end{table}

\textbf{Analysis of different modules}. We evaluate the effectiveness of progressive refinement (PR) in combination with our relation-aware self-attention module. In Table~\ref{tab:ablation}, the relation-aware self-attention module alone improves the detection performance by 0.9\% AP, 0.6\% AP$_{50}$ and 0.7\% AP$_{75}$ compared to the baseline. Adding progressive refinement brings additional gains of 0.3\% AP, 0.4\% AP$_{50}$ and 0.3\% AP$_{75}$. The improvements are also consistent across different object scales.
\begin{table*}[!htbp]
  \centering
  % \small
  % \setlength{\tabcolsep}{3pt}
  \caption{Performance comparison on different components used in the detector.}
  \label{tab:ablation}
  \begin{tabular}{@{}lccccccccc@{}}
  \hline
    Component    & AP                                              & AP$_{50}$                                       & AP$_{75}$                                       & AP$_S$                                          & AP$_M$                             
                                 & AP$_L$                                           \\ 
    \hline
                 & 51.1                                            & 68.6                                            & 55.8                                            & 35.1                                            & 55.2                               
                                & 66.0                                              \\
    Relation     & 52.0\textcolor{blue}{\textbf{($\uparrow$0.9})}  & 69.2\textcolor{blue}{(\textbf{$\uparrow$0.6})}  & 56.5\textcolor{blue}{\textbf{($\uparrow$0.7})}  & 35.6\textcolor{blue}{\textbf{($\uparrow$0.5})}  & 55.5\textcolor{blue}{\textbf{($\uparrow$0.3})}  & 66.4\textcolor{blue}{\textbf{($\uparrow$0.4})}    \\
    Relation+PR  & 52.3\textcolor{blue}{\textbf{($\uparrow$0.3})}  & 69.6\textcolor{blue}{\textbf{($\uparrow$0.4})}  & 56.8\textcolor{blue}{\textbf{($\uparrow$0.3})}  & 35.8\textcolor{blue}{\textbf{($\uparrow$0.2})}  & 55.9\textcolor{blue}{\textbf{($\uparrow$0.4})}  & 66.6\textcolor{blue}{\textbf{($\uparrow$0.2})}    \\
    \hline
  \end{tabular}
  \vspace{-5pt}
\end{table*}


\subsection{Progressive Refinement Analysis}
\label{subsec:progressive-refinement}
To understand how different scales of position relations (local, medium and global) evolve through decoder layers, we visualize the learned relation weights across layers in Fig.~\ref{fig:relation_weight_curve}. Our analysis reveals an interesting pattern in how the model balances different spatial relations. In the first layer, the weights among three scales remain relatively comparable, suggesting initial uncertainty in relation scale selection. From layer 2, the model develops a strong preference for local relations, with weights exceeding 0.9, indicating that early decoder layers focus primarily on establishing local relationships between queries.
Moving to deeper layers, we observe a gradual transition in attention distribution: local relation weights decrease to 0.24, while medium and global relation weights steadily increase to around 0.4. By the final layer, the weights for medium and global relations surpass that of local relations. This progressive transition from local to broader spatial contexts aligns with the intuitive understanding that object detection requires hierarchical processing - from local to global query interactions.
These findings suggest promising directions for future research in relation modeling, as the clear layer-wise progression from local to global relations indicates potential for more efficient architectures that explicitly leverage this hierarchical pattern.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.42\textwidth]{figures/relation_weight_curve.pdf} 
  \caption{Relation weight under local, medium and global relations in different decoder layers.}
  \label{fig:relation_weight_curve}
  \vspace{-5pt}
\end{figure}


\subsection{Convergence comparison}
\label{subsec:convergence-comparison}
Fig.~\ref{fig:convergence_curve} plots the convergence curves of different state-of-the-art methods with ResNet-50 backbone. Our model shows improved convergence behavior, benefiting from the learned layer-dependent multi-scale spatial relations between object queries. 
Although the absolute AP gain over Relation-DETR is moderate (+0.6\% AP), our method consistently outperforms the baselines (DINO and Deformable-DETR) throughout the training process. This demonstrates that introducing layer-dependent multi-scale spatial relations can effectively refine the original relation modeling for object detection.
\begin{figure}[!t]
  \centering
  \includegraphics[width=0.42\textwidth]{figures/convergence_curve.pdf} 
  \caption{Convergence comparison under different state-of-the-art methods with ResNet-50 backbone.}
  \label{fig:convergence_curve}
  \vspace{-6pt}
\end{figure}


\section{Conclusion}
\label{sec:conclusion}
In this paper, we present a progressive relation-aware self-attention module that enhances DETR detector by incorporating learnable multi-scale spatial relationships between object queries. 
Our method adaptively adjusts relation weights across different scales and decoder layers, achieving competitive performance on standard benchmarks.
Through extensive experiments, we demonstrate that our module improves both convergence speed and detection accuracy compared to standard self-attention.
Our analysis reveals a pattern in how spatial relations evolve through the network: local relations dominate in early decoder layers, while global relations become increasingly important in deeper layers. 
% This observation aligns with the hierarchical nature of object detection tasks. 
Our findings open several promising directions for future research. 
% Further investigation into the learned visual representations for spatial relation and extension to more challenging scenarios, such as dense object detection with significant occlusions, could provide valuable insights for advancing DETR-based detectors.

\bibliographystyle{IEEEbib}
\bibliography{icme2025references}

\vspace{12pt}

\end{document}
