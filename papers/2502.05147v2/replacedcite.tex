\section{Related Work}
\label{sec:related-work}

\subsection{Transformer for Object Detection}
\label{sec:transformer-object-detection}
DEtection TRansformer (DETR)____ establishes a new paradigm for end-to-end object detection by eliminating hand-crafted post-processing steps such as Non-maximum Suppression (NMS). Its transformer-based architecture consists of two main components: an encoder that transforms flattened image features into enriched memory representations, and a decoder that converts a set of learnable object queries into final detection results. The decoder operates through two attention mechanisms: self-attention for modeling interactions among object queries, and cross-attention for capturing relationships between queries and encoded memory features.

However, DETR suffers from slow convergence during training, and various approaches have been proposed to address this issue from different methodological perspectives: (1) Enhanced Feature Learning: Deformable DETR____ explores multi-scale features through deformable attention with sparse reference points, while Focus-DETR____ and Salience-DETR____ improve feature selection through salient token identification in the encoder. (2) Query Enhancement: DAB-DETR____ decouples object queries into 4D anchor box coordinates for iterative refinement, while DN-DETR____ and DINO____ accelerate training through auxiliary denoising task and contrastive learning. (3) Better Supervision: Hybrid DETR____ and Group DETR____ adopt one-to-many matching to increase supervision signals, while Stable-DINO____ and Align-DETR____ propose specialized loss functions to align classification and localization. (4) Attention Mechanism: Recent works focus on improving attention mechanisms, where Cascade-DETR ____ enhances query-feature interactions through cross-attention, and Relation-DETR____ learns explicit relation modeling between queries in self-attention.

\subsection{Relation Network}
\label{sec:relation-network}
Relation networks have emerged as a powerful approach for modeling inter-object relationships at instance level, which can be broadly categorized into two main directions: co-occurrence modeling and spatial relation modeling.

Co-occurrence approaches focus on capturing statistical dependencies between object categories. Some methods____ directly learn from category distribution patterns in large datasets, while others____ adaptively learn class relationships from annotations. However, these approaches either rely on fixed statistical priors or encounter with challenges in mapping between instances and categories____.

Spatial relation approaches construct graph structures where object features serve as nodes and their spatial relationships as edges. Pioneering works like Relation Network____ introduces geometric weights in attention modules to model spatial relations. Recent methods determine relation weights through various metrics, such as position-aware distance____, attention mechanisms____ or appearance similarity____. While these learnable relations offer greater flexibility compared to fixed priors, they typically require larger datasets and longer training time to effectively learn the relations from data.