\section{Related Work}
\label{sec:related-work}

\subsection{Transformer for Object Detection}
\label{sec:transformer-object-detection}
DEtection TRansformer (DETR)~\cite{carion2020end} establishes a new paradigm for end-to-end object detection by eliminating hand-crafted post-processing steps such as Non-maximum Suppression (NMS). Its transformer-based architecture consists of two main components: an encoder that transforms flattened image features into enriched memory representations, and a decoder that converts a set of learnable object queries into final detection results. The decoder operates through two attention mechanisms: self-attention for modeling interactions among object queries, and cross-attention for capturing relationships between queries and encoded memory features.

However, DETR suffers from slow convergence during training, and various approaches have been proposed to address this issue from different methodological perspectives: (1) Enhanced Feature Learning: Deformable DETR~\cite{zhu2020deformable} explores multi-scale features through deformable attention with sparse reference points, while Focus-DETR~\cite{zheng2023more} and Salience-DETR~\cite{hou2024salience} improve feature selection through salient token identification in the encoder. (2) Query Enhancement: DAB-DETR~\cite{liu2021dab} decouples object queries into 4D anchor box coordinates for iterative refinement, while DN-DETR~\cite{li2022dn} and DINO~\cite{zhang2022dino} accelerate training through auxiliary denoising task and contrastive learning. (3) Better Supervision: Hybrid DETR~\cite{zong2023detrs} and Group DETR~\cite{chen2023group} adopt one-to-many matching to increase supervision signals, while Stable-DINO~\cite{liu2023detection} and Align-DETR~\cite{cai2023align} propose specialized loss functions to align classification and localization. (4) Attention Mechanism: Recent works focus on improving attention mechanisms, where Cascade-DETR \cite{ye2023cascade} enhances query-feature interactions through cross-attention, and Relation-DETR~\cite{hou2024relation} learns explicit relation modeling between queries in self-attention.

\subsection{Relation Network}
\label{sec:relation-network}
Relation networks have emerged as a powerful approach for modeling inter-object relationships at instance level, which can be broadly categorized into two main directions: co-occurrence modeling and spatial relation modeling.

Co-occurrence approaches focus on capturing statistical dependencies between object categories. Some methods~\cite{krishna2017visual,xu2019reasoning} directly learn from category distribution patterns in large datasets, while others~\cite{chen2018iterative,hao2023relation} adaptively learn class relationships from annotations. However, these approaches either rely on fixed statistical priors or encounter with challenges in mapping between instances and categories~\cite{xu2019reasoning}.

Spatial relation approaches construct graph structures where object features serve as nodes and their spatial relationships as edges. Pioneering works like Relation Network~\cite{hu2018relation} introduces geometric weights in attention modules to model spatial relations. Recent methods determine relation weights through various metrics, such as position-aware distance~\cite{bi2022srrv,lin2021core}, attention mechanisms~\cite{zhao2021,vaswani2017attention} or appearance similarity~\cite{li2020gar}. While these learnable relations offer greater flexibility compared to fixed priors, they typically require larger datasets and longer training time to effectively learn the relations from data.