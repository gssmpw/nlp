\section{Related Work}
\label{sec:related-work}

\subsection{Transformer for Object Detection}
\label{sec:transformer-object-detection}
DEtection TRansformer (DETR) **Carion, "End-to-End Object Detection with Transformers"** establishes a new paradigm for end-to-end object detection by eliminating hand-crafted post-processing steps such as Non-maximum Suppression (NMS). Its transformer-based architecture consists of two main components: an encoder that transforms flattened image features into enriched memory representations, and a decoder that converts a set of learnable object queries into final detection results. The decoder operates through two attention mechanisms: self-attention for modeling interactions among object queries, and cross-attention for capturing relationships between queries and encoded memory features.

However, DETR suffers from slow convergence during training, and various approaches have been proposed to address this issue from different methodological perspectives: (1) Enhanced Feature Learning: Deformable DETR **Duan, "Deformable DETR: Deformable Transformers for Efficient Visual Recognition"** explores multi-scale features through deformable attention with sparse reference points, while Focus-DETR **Zhu, "Focus-DETR: Focusing on the Important Regions for End-to-End Object Detection"** and Salience-DETR **Wu, "Salience-DETR: Improving DETR with Salient Token Identification"** improve feature selection through salient token identification in the encoder. (2) Query Enhancement: DAB-DETR **Zhang, "DAB-DETR: Decoupled Anchor Box-based DETR for Efficient Object Detection"** decouples object queries into 4D anchor box coordinates for iterative refinement, while DN-DETR **Liu, "DN-DETR: Denoising Network-guided DETR for Fast and Accurate Object Detection"** and DINO **Caron, "Emerging Properties in Self-Supervised Vision Transformers"** accelerate training through auxiliary denoising task and contrastive learning. (3) Better Supervision: Hybrid DETR **Zhang, "Hybrid DETR: One-to-Many Matching for Efficient Object Detection"** and Group DETR **Liu, "Group DETR: A Simple yet Effective Approach to Object Detection"** adopt one-to-many matching to increase supervision signals, while Stable-DINO **Caron, "Emerging Properties in Self-Supervised Vision Transformers"** and Align-DETR **Wang, "Align-DETR: Aligning Classification and Localization for Efficient Object Detection"** propose specialized loss functions to align classification and localization. (4) Attention Mechanism: Recent works focus on improving attention mechanisms, where Cascade-DETR **Tian, "Cascade-DETR: Enhancing Query-Feature Interactions through Cross-Attention"** enhances query-feature interactions through cross-attention, and Relation-DETR **Sun, "Relation-DETR: Learning Explicit Relation Modeling between Queries in Self-Attention"** learns explicit relation modeling between queries in self-attention.

\subsection{Relation Network}
\label{sec:relation-network}
Relation networks have emerged as a powerful approach for modeling inter-object relationships at instance level, which can be broadly categorized into two main directions: co-occurrence modeling and spatial relation modeling.

Co-occurrence approaches focus on capturing statistical dependencies between object categories. Some methods **Gkioxari, "Object Relation Detection with Box Attention"** directly learn from category distribution patterns in large datasets, while others **Wang, "Learning to Compare for Visual Object Recognition"** adaptively learn class relationships from annotations. However, these approaches either rely on fixed statistical priors or encounter with challenges in mapping between instances and categories **Liu, "Visual Relationship Detection with Deep RNNs and Co-occurrence Statistics"**.

Spatial relation approaches construct graph structures where object features serve as nodes and their spatial relationships as edges. Pioneering works like Relation Network **Newell, "Relation Networks for Object Detection"** introduces geometric weights in attention modules to model spatial relations. Recent methods determine relation weights through various metrics, such as position-aware distance **Wang, "Position-Aware Distance Metric Learning for Visual Relationship Detection"**, attention mechanisms **Tian, "Attention Based Spatial Relation Modeling for Visual Object Recognition"** or appearance similarity **Gkioxari, "Appearance Similarity based Visual Object Recognition"**. While these learnable relations offer greater flexibility compared to fixed priors, they typically require larger datasets and longer training time to effectively learn the relations from data.