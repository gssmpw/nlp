%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\documentclass[sigconf]{acmart}
%
%
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%
%
%
%
%
\setcopyright{licensedusgovmixed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}
%
\acmConference[CODASPY '25]{the 15th ACM Conference on Data and Application Security and Privacy}{June 4--6,
  2025}{Pittsburgh, PA}
%
%
%
%
%
%
\acmISBN{978-1-4503-XXXX-X/18/06}


%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%


%
%

\input{math_commands}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{enumitem}

%

\begin{document}

%
%
%
\title{Differentially Private Iterative Screening Rules for Linear Regression}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%

%
%
%
%
%
%

%
%
%
%
%
%

%
%
%
%
%
%

%
%
%
%
%
%
%

%
%
%
%
%
%

%
%
%
%
%
%

\author{Amol Khanna}
\email{Khanna\_Amol@bah.com}
%
%
%
%
\affiliation{%
  \institution{Booz Allen Hamilton}
  \city{Boston}
  \state{Massachusetts}
  \country{USA}
}

\author{Fred Lu}
\email{Lu\_Fred@bah.com}
\affiliation{%
  \institution{Booz Allen Hamilton}
  \city{Los Angeles}
  \state{California}
  \country{USA}}

\author{Edward Raff}
\email{Raff_Edward@bah.com}
\affiliation{%
 \institution{Booz Allen Hamilton}
 \institution{University of Maryland, \\Baltimore County}
 \city{Syracuse}
 \state{New York}
 \country{USA}}

%
%
%
%
%
%
\renewcommand{\shortauthors}{Amol Khanna, Fred Lu, and Edward Raff}

%
%
%
\begin{abstract}
  Linear $L_1$-regularized models have remained one of the simplest and most effective tools in data science. Over the past decade, screening rules have risen in popularity as a way to eliminate features when producing the sparse regression weights of $L_1$ models. However, despite the increasing need of privacy-preserving models for data analysis, to the best of our knowledge, no differentially private screening rule exists. In this paper, we develop the first private screening rule for linear regression. We initially find that this screening rule is too strong: it screens too many coefficients as a result of the private screening step. However, a weakened implementation of private screening reduces overscreening and improves performance. 
\end{abstract}

%
%
%
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002978.10002986</concept_id>
<concept_desc>Security and privacy~Formal methods and theory of security</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002978.10003018</concept_id>
<concept_desc>Security and privacy~Database and storage security</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Security and privacy~Formal methods and theory of security}
\ccsdesc[500]{Security and privacy~Database and storage security}

%
%
%
\keywords{Differential Privacy, Feature Selection, Model Selection, Screening Rules, Linear Regression}
%
%
%

%
%
%
\maketitle

\section{Introduction}

Sparse linear regression is an important statistical technique which can prevent overfitting on high-dimensional datasets. It is typically achieved through LASSO ($L_1$) optimization, which can be represented by $$\widehat{\mathbf{w}} = \argmin_{\mathbf{w} \in \mathbb{R}^{d}: \  \lVert \mathbf{w} \rVert_1 \ leq \lambda} \frac{1}{n} \sum_{i = 1}^{n} (y_i - \mathbf{w} \cdot \mathbf{x}_i) ^ 2,$$ 
where $\lambda$ is the constraint parameter, $\mathbf{x}_1, \ldots, \mathbf{x}_n \in \mathbb{R}^{d}$, and $y_1, \ldots, y_n \in \mathbb{R}$. The Frank-Wolfe algorithm can be used to directly optimize over this objective \cite{frank1956algorithm}.

However, the output $\widehat{\mathbf{w}}$ of this optimization can reveal information about specific training datapoints. This may be dangerous in fields like medicine, finance, and government, where datapoints consist of sensitive information which should not be revealed when a model is used \cite{khanna2022privacy,basu2021privacy,khanna2024position}. To prevent private information differential privacy can be used. 

Differential privacy is a statistical technique which guarantees that an algorithm's outputs do not reveal whether or not a specific datapoint was used in its training. Specifically, given privacy parameters $\epsilon$ and $\delta$ and any two datasets $\mathcal{D}$ and $\mathcal{D}'$ differing on one datapoint, an approximate differentially private algorithm $\mathcal{A}$ satisfies $\mathbb{P} \left[ \mathcal{A}(\mathcal{D}) \in O \right] \leq \exp\{ \epsilon\} \mathbb{P} \left[ \mathcal{A}(\mathcal{D}') \in O \right] + \delta$ for any $O \subseteq \text{image}(\mathcal{A})$ \cite{dwork2014algorithmic}.\footnote{An important consideration when employing differential privacy is the definition of two datasets \textit{differing on one datapoint}. There are two ways to define this: one in which a datapoint from $\mathcal{D}$ is replaced with another to produce $\mathcal{D}'$, and another in which a datapoint is added or removed from $\mathcal{D}$ to produce $\mathcal{D}'$. In this paper, we use the latter definition for dataset adjacency.}

Making an algorithm differentially private requires adding noise to its intermediate steps or outputs. This noise must scale with the algorithm's sensitivity, or how much its outputs can change when one of its input datapoints is added or removed. If this change is measured with an $L_1$ or $L_2$ norm, noise can be added with the Laplacian or Gaussian distributions, respectively. Further introductory details on differential privacy can be found in \cite{near_abuah_2021}. 

Current differentially private high-dimensional regression algorithms modify nonprivate sparse optimization techniques. However, the modifications required to achieve privacy make these algorithms run slowly and produce dense or ineffective solutions \cite{talwar2015nearly,wang2019differentially,khanna2023differentially}. To address these challenges, some works have developed feature selection algorithms to run prior to training \cite{kifer2012private,thakurta2013differentially,swope2024feature}. However, these algorithms are computationally inefficient and can assume a final sparsity level before choosing a support set.

Another way to achieve sparse weights for high-dimensional algorithm is by using screening rules. Screening rules discard features that do not contribute to a model during training. They are most often used in conjunction with $L_1$-regularized or $L_1$-constrained linear regression. Screening rules have been used to improve the performance of sparse regression optimization on numerous datasets over the past decade and are even included in the popular \texttt{R} package \texttt{glmnet} \cite{wang2013lasso,wang2014safe,raj2016screening,ghaoui2010safe,olbrich2015screening,tibshirani2012strong,friedman2021package}. Unlike the aforementioned approaches adapted for private regression, screening rules do not require a predetermined support set or level of sparsity. They efficiently check a mathematical condition to determine if a feature should be screened to 0, and are implemented with sparse optimizers to improve the rate of learning and stability. 

A differentially private screening rule has the potential to achieve sparsity and help private optimizers select a model's most important features. However, to the best of our knowledge, no differentially private screening rule exists. In this work, we privatize a screening rule used for convex problems developed by Raj et al., with the goal of testing whether such a modification can yield the benefits of screening rules while retaining accuracy and guaranteeing privacy \cite{raj2016screening}. We find that an aggressive implementation of the screening rule cannot accurately screen features, and we analyze why this is the case. We then create a weaker implementation strategy with improved results. 

This paper is organized as follows. In Section 2, we review prior literature on sparse private regression and a relevant nonprivate screening rule. In Section 3, we find the sensitivity of this screening rule, and Section 4 presents a working screening rule. Section 5 demonstrates experiments with this screening rule, showing it can produce better results. Our paper is the first to develop and test a differentially private screening rule. We find our rule leads to sparse and effective solutions on many datasets. We believe this work could contribute to better private sparse optimization strategies.

\section{Related Work}

Methods to produce private sparse regression weights all suffer in performance due to the addition of noise. Private $L_1$ optimizers must add higher levels of noise when run for more iterations, incentivizing practitioners to run fewer iterations \cite{talwar2015nearly,wang2020differential}. However, running an optimizer for fewer iterations means that the model will be limited in its learning. On the other hand, private model selection algorithms are computationally inefficient and run prior to training, meaning they are unable to reap the benefit of any information contained within partially trained coefficients of the weight vector \cite{lei2018differentially,thakurta2013differentially}. Although noise is necessary for privacy, an effective private screening rule would run with a private optimizer and improve the optimizer's performance by setting the coefficients of irrelevant features to 0. By using the screening rule on the current weight vector, it can adapt to the optimizer's updates and screen features more accurately. 

%

To develop a differentially private screening rule, we adapt Theorem 11 of Raj et al.'s rule which is flexible to solving many types of regression problems \cite{raj2016screening}. While other screening rules exist, they are geometry- and problem-specific \cite{ghaoui2010safe,wang2014safe,wang2013lasso}. \textbf{Our goal is to utilize Raj etl al.'s screening rule for $L_1$-constrained regression to induce sparsity on Talwar et al.'s $L_1$-constrained private Frank-Wolfe algorithm \cite{raj2016screening,talwar2015nearly}}.\footnote{Note that \cite{raj2016screening} is written with semantics typical in the field of optimization - there are $d$ (d)atapoints and the goal is to optimize a vector with (n)umber of components $n$. In this paper, we use the standard statistical and machine learning conventions, in which there are $n$ (n)umber of datapoints and optimization is done over $d$ (d)imensions.}

Since we use the private Frank-Wolfe algorithm (\texttt{DP-FW}), we also review it here. \texttt{DP-FW} uses the Frank-Wolfe method for $L_1$-constrained convex optimization, which chooses a vertex of the feasible region (scaled $L_1$ ball) which minimizes a linear approximation of the loss function. By doing this for $T$ iterations with appropriate step sizes, the algorithm satisfies $\mathcal{L}(\mathbf{w}^{(T)}) - \min_{\mathbf{w}^*  \in \mathcal{C}} \mathcal{L}(\mathbf{w}^*) \leq \mathcal{O}(\frac{1}{T})$ \cite{frank1956algorithm,jaggi2013revisiting}. The progress of the Frank-Wolfe optimizer can be measured with the Wolfe gap function: $\mathcal{G}_{\mathcal{C}}(\mathbf{w}) = \max_{\mathbf{z} \in \mathcal{C}} (\mathbf{w} - \mathbf{z})^{\top}\nabla f(\mathbf{w})$. It can be shown that $\mathcal{G}_{\mathcal{C}}(\mathbf{w}) \geq f(\mathbf{w}) - f(\mathbf{w}^*)$ for all $\mathbf{w}$. For this reason, the smaller the Wolfe gap function, the closer the optimization is to an optimal solution. To privatize the Frank-Wolfe algorithm, Talwar et al. restrict the $L_\infty$ norm of datapoints so they can calculate the exact sensitivity of the gradients \cite{talwar2015nearly}. They then use the report-noisy-max mechanism to noisily choose which component of the weight vector to update. Unfortunately, due to inexact optimization caused by the noisy selection process, the private Frank-Wolfe algorithm produces dense results, limiting its ability to be used in high-throughput or interpretability-restricted applications of regression. Despite this limitation, the Frank-Wolfe algorithm is ideal for an application of Raj et al.'s screening rule. Of current methods for private high-dimensional regression, recently summarized by \cite{khanna2024sok}, the Frank-Wolfe algorithm is unique in that it uses $L_1$-constrained optimization with updates to one component of the weight vector per iteration. Each of these conditions is important. The first is necessary because Raj et al.'s screening rule requires optimization over an $L_1$-constrained set. The second is important because if a private (noisy) optimization algorithm updates all components of a weight vector at each iteration, then any sparsity induced by applying a screening rule would be lost at the next iteration of optimization, since the output of the optimization would be dense.

To the best of our knowledge, this is the first work considering a differentially private screening rule. The difficulty of DP screening is counteracted by the reward of obtaining sparse \textit{and} private regression, as normal DP destroys sparsity via the addition of noise. 

\section{Privatizing Iterative Screening}

Raj et al. consider the problem $\min_{\mathbf{w} \in \mathcal{C}} f(\mathbf{Xw})$, where $\mathcal{C}$ is the feasible set of solutions and $f$ is $L$-smooth and $\mu$-strongly convex.\footnote{A function $f$ is $L$-smooth if $\lVert \nabla f(\mathbf{x}_1) - \nabla f(\mathbf{x}_2) \rVert \leq L \lVert \mathbf{x}_1 - \mathbf{x}_2 \rVert$ for any $\mathbf{x}_1$ and $\mathbf{x}_2$ in $f$'s domain. $f$ is $\mu$-strongly convex if $\nabla^2 f(\mathbf{x}) \succeq \alpha \mathbf{I}$ for any $\mathbf{x}$ in $f$'s domain, where $\mathbf{I}$ is the identity matrix.} They also define $\mathbf{x}_{(i)} \in \mathbb{R}^n$ to be the $i^{\text{th}}$ column of the design matrix $\mathbf{X} = \begin{bmatrix} \mathbf{x}_1^\top \\ \vdots \\ \mathbf{x}_n\top \end{bmatrix} \in \mathbb{R}^{n \times d}$ and $\mathcal{G}_{\mathcal{C}}(\mathbf{w})$ to be the Wolfe gap function for linear regression, namely $\max_{\mathbf{z} \in \mathcal{C}} (\mathbf{Xw} - \mathbf{Xz})^{\top}\nabla f(\mathbf{Xw})$. Given this information, they prove that if
%
%
%
%
\begin{align}
\label{eq:1}
    s_i = &\lvert \mathbf{x}_{(i)}^{\top} \nabla f(\mathbf{Xw}) \rvert \nonumber  + (\mathbf{Xw})^{\top}\nabla f(\mathbf{Xw}) \\ + \ &L(\lVert \mathbf{x}_{(i)} \rVert_2 + \lVert \mathbf{Xw} \rVert_2) \sqrt{\mathcal{G}_{\mathcal{C}}(\mathbf{w}) / \mu} < 0
\end{align}
at any $\mathbf{w} \in \mathcal{C}$, then $w_{i}^{*} = 0$, where $\mathbf{w}^{*}$ is the optimal solution to the optimization problem.\footnote{Note that in this expression and in the Wolfe gap function, the gradients are taken with respect to the vector $\mathbf{u} = \mathbf{Xw} \in \mathbb{R}^n$. Since $\mathbf{u} \in \mathbb{R}^n$, $\nabla f(\mathbf{u}) \in \mathbb{R}^n$. This can be a source of confusion for those who are unfamiliar with the Wolfe gap function.}

Our goal is to employ this screening rule for linear regression, namely, when $f(\mathbf{Xw}) : \mathbb{R}^n \rightarrow \mathbb{R} = \frac{1}{n} \left( \mathbf{y} - \mathbf{Xw} \right)^\top \left( \mathbf{y} - \mathbf{Xw} \right)$. Since we want to guarantee privacy, we will determine the sensitivity of this calculation so we can add an appropriate amount of noise and ensure screening is differentially private. Specifically, we want to determine how much the value of $f$ can change between datasets $\mathbf{X}$ and $\mathbf{X}'$, where $\mathbf{X}'$ contains either one additional row or one removed row compared to $\mathbf{X}$. We will conduct our analysis for the case where $\lVert\mathbf{x}_i \rVert_\infty \leq 1$, $\mathcal{C}$ is the $\lambda$-scaled $L_1$-ball in $\mathbb{R}^{d}$, and $\lvert y_i \rvert < \lambda$. These conditions are also required by the \texttt{DP-FW} optimizer \cite{talwar2015nearly}. 

\begin{theorem}
    Under the conditions listed above, the sensitivity of Equation 1 when $f(\mathbf{Xw}) = \frac{1}{n} \left( \mathbf{y} - \mathbf{Xw} \right)^\top \left( \mathbf{y} - \mathbf{Xw} \right)$ is $\frac{2\lambda}{n} + \frac{2\lambda^2}{n} + \frac{1}{n} \left( 1 + \lambda \right)\sqrt{\frac{4\lambda^2/n}{1/n}}.$ The sensitivity of releasing $\mathbf{s} = \begin{bmatrix} s_1 & \dots & s_d \end{bmatrix}^\top$ is $\frac{2\lambda\sqrt{d}}{n} + \frac{2\lambda^2\sqrt{d}}{n} + \frac{1}{n} \left( \sqrt{d} + \lambda\sqrt{d} \right)\sqrt{\frac{4\lambda^2/n}{1/n}}.$
\end{theorem}

The proof is provided in Appendix A. 

Using the bounds for the $L_2$-sensitivity of the screening rule in Equation 1, our first attempt at implementing a differentially private screening rule is titled \texttt{ADP-Screen}. We discuss this method and its experiments in Appendix B and Appendix C because it is impractical for actual use. \texttt{ADP-Screen} demonstrates ``overscreening'', meaning that it rapidly screens all coefficients in the weight vector to 0. We develop a method to address overscreening in Section 4. 

\section{\texttt{RNM-Screen}}

In this this section, we seek to improve the performance of \texttt{ADP-Screen} by modifying it in two ways: 

\begin{enumerate}
    \item We redistribute the total privacy budget to reduce the noisiness of private optimization. 
    \item We employ the report-noisy-max mechanism to reduce the screening noise to sub-$\mathcal{O}(\sqrt{d})$ and reduce overscreening. 
\end{enumerate}

\subsection{Redistributing the Privacy Budget}

Experiments with \texttt{ADP-Screen} indicated that noisy private optimization caused it to lose its ability to screen coefficients effectively. For this reason, we sought to reduce the amount of noise added during private optimization. 

\texttt{ADP-Screen} relies on \texttt{DP-FW} for $L_1$-constrained optimization. To the best of our knowledge, this is the only algorithm which performs private $L_1$-constrained optimization, and as such, the only way to reduce the amount of noise added during private optimization is to redistribute the final privacy budget to reduce the noise for private optimization. 

\subsection{Report-Noisy-Min Mechanism}

Experiments on \texttt{ADP-Screen} demonstrated that it overscreened 
coefficients, producing solution vectors which were too sparse to output useful predictions. To prevent overscreening, we chose to screen only one coefficient per iteration.

To achieve this, we use the report-noisy-max mechanism. The report-noisy-max mechanism is a technique for privately choosing an item with the highest score from a set given a function which computes the score. Specifically, when trying to calculating the $\argmax$ of a set of numbers $\{u_1, u_2, \dots, u_n \}$ calculated from a dataset $\mathbf{X}$, the report-noisy-max mechanism calculates $\argmax \{u_1 + \text{Lap}\left( \frac{s}{\epsilon} \right), u_2 + \text{Lap}\left( \frac{s}{\epsilon} \right), \dots, u_n + \text{Lap}\left( \frac{s}{\epsilon} \right)\}$, where $s$ is the sensitivity of a single number $u_i$ with respect to a single change in $\mathbf{X}$. This mechanism is $(\epsilon, 0)$-differentially private.

By finding the maximum of the negative values of the scores, the report-noisy-max mechanism can also be used to privately find the element with minimum score. We use the report-noisy-min technique to select the coefficient to screen every iteration. 

To do this, after every iteration, we run Equation 1 on all coefficients. We use the report-noisy-min technique to select the coefficient which produced the smallest value in Equation 1 in a differentially private manner. We then screen this coefficient. Our algorithm can be seen in Algorithm 2. We call the algorithm \texttt{RNM-Screen} due to its use of report-noisy-min in lines 11 and 12. 

\setcounter{algorithm}{1}
\begin{algorithm}[t]
\label{alg:2}
\caption{\texttt{RNM-Screen}}
\begin{algorithmic}[1]
\REQUIRE Privacy Parameters: $\epsilon_1 > 0$, $\epsilon_2 > 0$, $0 < \delta_1 \leq 1$, $0 < \delta_2 \leq 1$; Constraint: $\lambda > 0$; Iterations: $T$; Design Matrix: $\mathbf{X} \in \mathbb{R}^{n \times d}$ where $\lVert \mathbf{x}_i \rVert_\infty \leq 1$ for all $i \in \{1, \ldots, n \}$; Target: $\mathbf{y}$
\STATE \textbf{Define:} $\epsilon_\text{iter}$ as the per-iteration $\epsilon$ value for report-noisy-max; $\Delta$ as the sensitivity of Equation 1.
\STATE $\epsilon_\text{iter} \gets \frac{\epsilon_2}{\sqrt{8T\log{\frac{1}{\delta_2}}}}$
\STATE $\Delta \gets \frac{2 \lambda}{n} + \frac{2 \lambda ^2}{n} + \frac{2 \lambda (1 + \lambda)}{n}$
\STATE $scale \gets \frac{\Delta}{\epsilon_{\text{iter}}}$
\STATE $\widehat{\mathbf{w}}^{(0)} \gets$ Random Vector in $\{\mathbf{w} \in \mathbb{R}^d : \lVert \mathbf{w} \rVert_1 \leq \lambda\}$
\FOR{$t = 1$ to $T$}
    \STATE $\widehat{\mathbf{w}}^{(t)} \gets \texttt{DP-FW Step} \left( \epsilon_1, \delta_1, \lambda, T, \mathbf{X}, \mathbf{y}, \widehat{\mathbf{w}}^{(t - 1)} \right)$
     \FOR{$i = 1$ to $n$}
        \STATE $s_i \gets$ $\lvert \mathbf{x}_{(i)}^{\top} \nabla f(\mathbf{X\widehat{\mathbf{w}}^{(t)}}) \rvert + (\mathbf{X\widehat{\mathbf{w}}^{(t)}})^{\top}\nabla f(\mathbf{X\widehat{\mathbf{w}}^{(t)}}) + L(\lVert \mathbf{x}_{(i)} \rVert_2 + \lVert \mathbf{X\widehat{\mathbf{w}}^{(t)}} \rVert_2) \sqrt{\mathcal{G}_{\mathcal{C}}(\mathbf{\widehat{\mathbf{w}}^{(t)}}) / \mu} $
    \ENDFOR
    \STATE $\mathbf{s} \gets \mathbf{s} + \text{Laplace}\left(scale \right)$
    \STATE $j \gets \text{Index of the smallest element of } \mathbf{s}$
    \IF{$s_j < 0$}
        \STATE $\widehat{\mathbf{w}}_j^{(t)} \gets 0$
    \ENDIF
\ENDFOR
\STATE Output $\widehat{\mathbf{w}}^{(T)}$
\end{algorithmic}
\end{algorithm}

Using the report-noisy-min mechanism has another benefit: we are able to reduce the noise scale to sub-$\mathcal{O}(\sqrt{d})$ since the report-noisy-min mechanism only requires the sensitivity for a single coefficient of the output weight \cite{dwork2014algorithmic}. 

We conclude this section with a proof of privacy:
\begin{lemma}
    Algorithm 2 is $(\epsilon_1 + \epsilon_2, \delta_1 + \delta_2)$-differentially private.
\end{lemma}
\begin{proof}
    First, note that lines 9 and 10 consist are the report-noisy-min mechanism with the noise scaled by the sensitivity of a single component, so for a single iteration they are $(\epsilon_\text{iter}$, 0)-differentially private \cite{dwork2014algorithmic}. Next, using the advanced composition mechanism for pure differential privacy, running the screening rule $T$ times produces an $(\epsilon_2, \delta_2)$-differentially private mechanism. Employing basic composition theorem of approximate differential privacy guarantees that Algorithm 2 is $(\epsilon_1 + \epsilon_2, \delta_1 + \delta_2)$-differentially private \cite{dwork2010boosting}.
\end{proof}

\subsection{Analyzing \texttt{RNM-Screen}'s Screening Potential}

To observe the behavior of screening only one coefficient per iteration, it is interesting to consider the case where coefficients which are updated and screened are uniformly chosen every iteration. Although this will almost certainly not hold during training, analyzing this case will provide us with insight into such a screening procedure. In this case, we have the following: 

\begin{theorem}
    Let $\{I_1, I_2, \dots, I_d\}$ be a set of indicator functions where $I_i$ takes value $1$ when the $i^{\text{th}}$ coefficient is nonzero after $T$ iterations of training. Then 
    \begin{align*}
        \lim_{T \to \infty} \mathbb{E} \left[ I_1 + \dots + I_d \right] &= \frac{(d-1)^2 d}{(d-1)d + 1} \\ 
        \lim_{d \to \infty} \mathbb{E} \left[ I_1 + \dots + I_d \right] &= T - 1.
    \end{align*}
\end{theorem}

The proof is provided in Appendix D. 

From these limits, we can see that when \texttt{RNM-Screen} is run for infinite iterations, it approaches a solution with $\mathcal{O}(d)$ nonzero coefficients. From this result, we can infer that \texttt{RNM-Screen} slows the growth of the number of nonzero coefficients but is weak enough to prevent overscreening. The second limit is expected, as when the number of dimensions grows, it becomes increasingly unlikely that \texttt{RNM-Screen} will choose to screen coefficients which have been updated to be nonzero. This result is common among differentially private algorithms; for example, if the private Frank-Wolfe algorithm is run alone and the number of dimensions approaches infinity, its coefficient choices will approach uniformity. 

\section{Experiments with \texttt{RNM-Screen}}

To test \texttt{RNM-Screen}, we analyzed its performance on two synthetic datasets and a number of real-world datasets. Results are shown below. 

\subsection{Synthetic Data}

We began by using the synthetic dataset which Raj et al. used to test their nonprivate screening algorithm. Specifically, we generated $3000$ datapoints in $\mathbb{R}^{600}$ from the standard normal distribution, and scaled the final dataset so $\lVert \mathbf{x}_i \rVert_\infty \leq 1$. We set the true weight vector $\mathbf{w}^*$ to be sparse with 35 entries of $+1$ and 35 entries of $-1$, and set $\mathbf{y} = \mathbf{Xw}^*$. Raj et al. demonstrated that the nonprivate screening rule listed in Equation 1 performs well on this dataset for linear regression. We verified this result, finding that using the nonprivate Frank-Wolfe optimizer with the nonprivate screening rule at every iteration produced a final weight vector in which nonzero components were only at the locations of nonzero components in the true weight vector and $55\%$ of the true nonzero components were nonzero after training.

We also wanted to determine how correlated features affect the performance of the screening rule. To this end, we generated 3000 datapoints in $\mathbb{R}^{600}$ from $\mathcal{N}(\mathbf{0}, \Sigma)$ where $\Sigma_{ij} = 0.5 ^ {\lvert i - j \rvert}$. We then scaled the data as above, the true weight vector remained the same, and $\mathbf{y}$ was found in the same way. 

We tested the performance of two settings of \texttt{RNM-Screen} on each of these datasets. We ran \texttt{RNM-Screen} with $\epsilon_1 = 4.9,\  \epsilon_2 = 0.1,\  \delta_1 = \frac{1}{4000},\  \delta_2 = \frac{1}{12000},\  \lambda = 50, \text{ and } T = 1000$.\footnote{In our experiments, we chose to employ a high privacy budget for the optimization procedure to identify how private screening performs given a good optimizer but low privacy budget for screening. The experiments in the following sections will demonstrate that in many cases, \texttt{RNM-Screen} introduces minimal negative side-effects during optimization while inducing sparsity.} Figure 1 and Table 1 demonstrate the results of this experiment. It is clear that \texttt{RNM-Screen} performs significantly better than \texttt{ADP-Screen} in that it is able to distinguish between the true nonzero and true zero coefficients in both cases and sets many more true zero coefficients to 0. 

\setcounter{figure}{0}
\setcounter{table}{0}
\begin{table}[t]
\label{tab:1}
\caption{Average true positive rates (TPR), false positive rates (FPR), $F_1$ scores, and sparsities of 20 trials of \texttt{RNM-Screen} when run on synthetic data. True positives correspond to private nonzero coefficients which are truly nonzero. False positives correspond to private nonzero coefficients which are zero in the true solution.}
\begin{center}
\begin{sc}
\begin{tabular}{l|cccc}
& TPR & FPR & $F_1$ Score & Sparsity \\ \hline
Uncorrelated & 0.829 & 0.475 & 0.291 & 0.504 \\ 
Correlated & 0.957 & 0.281 & 0.444 & 0.371 \\ 
\end{tabular}
\end{sc}
\end{center}
\end{table}

\begin{figure}[t]
\label{fig:1}
    \centering
    \includegraphics[width=1.0\columnwidth]{Figures/old-new-comparison-updated.pdf}
    \caption{Comparing \texttt{RNM-Screen} to \texttt{ADP-Screen}. From these graphs, it is clear that \texttt{RNM-Screen} is able to distinguish between screening true nonzero coefficients and true zero coefficients, while \texttt{ADP-Screen} is generally unable to do this. However, in both cases, \texttt{RNM-Screen} does not achieve the same final sparsity level as \texttt{ADP-Screen}. Additionally, it is interesting to note that correlated features improves the performance of private screening rules.}
\end{figure}

We define the $F_1$ score as 
$$\frac{\text{\#Correct Nonzero Coeffs.}}{\text{\#Corr. Nonzero Coeffs.} + \frac{\text{\#Incorr. Nonzero Coeffs.} + \text{\#Incorr. Zero Coeffs.}}{2}}$$
and employ this as a quantitative metric to compare how well an algorithm is able to distinguish screening true nonzero and true zero coefficients.\footnote{This metric is inspired by the traditional $F_1$ score for binary classification. We choose to use it as a quantitative metric for measuring sparsity since, unlike traditional $L_0$ metrics for measuring sparsity, it rewards correct nonzero coefficients (coefficients which are nonzero in a nonprivate solution). It also rewards zero coefficients which are zero in a nonprivate solution and penalizes nonzero coefficients which are zero in a nonprivate solution.} We find that the $F_1$ scores of \texttt{RNM-Screen} were significantly better than those of \texttt{ADP-Screen} in both for both the uncorrelated and correlated cases, as measured by a nonparameteric sign test. This is not surprising, as from Figure 1 we can see that the number of true nonzero coefficients found by \texttt{ADP-Screen} is approximately 0, which produces an $F_1$ score of approximately 0. 

\subsection{Real-World Data}

\setcounter{figure}{1}
\begin{figure*}[!ht]
\label{fig:2}
    \centering
    \includegraphics[width=\textwidth]{Figures/realdata.pdf}
    \caption{Mean squared error loss of \texttt{RNM-Screen} on real-world datasets. Decreasing loss on datasets with larger $d$ indicate that learning does occur in these instances. Since we are most interested in employing sparsity to build a generalizable and interpretable model on datasets with large $d$, this indicates that \texttt{RNM-Screen} may be useful for this task. Note that \texttt{DP-FW} does not produce sparse solutions, which is the purpose of this work. For this reason, it is excluded from this plot.}
\end{figure*}

We also explored \texttt{RNM-Screen}'s performance on publicly available real-world datasets. We employed the Abalone, Housing, Body Fat, Pyrim, Triazines, Residential Buildings, Normalized Communities and Crime, and BlogFeedback datasets found in the LIBSVM and UCI Dataset Repositories \cite{harrison1978hedonic,behnke1974evaluation,misc_residential_building_data_set_437,misc_communities_and_crime_183,misc_blogfeedback_304}. For the Residential Buildings dataset, our target variable was profit made on the sale of a property.\footnote{These datasets were chosen to have increasing dimensionality, but are still low-dimensional, with $n < d$. We did not employ high-dimensional datasets since running \texttt{DP-FW} on high-dimensional datasets is computationally challenging \cite{raff2024scaling}. Note, however, that sparsity on low-dimensional datasets with sufficiently high $d$ is still desirable since this can produce a more interpretable solution.} We applied the Yeo-Johnson transform to each dataset and scaled them so $\lVert \mathbf{x}_i \rVert_\infty \leq 1$ \cite{yeo2000new}. We also applied a Yeo-Johnson transform to the the target $\mathbf{y}$. The dimensionality of each dataset can be found in Table 2. 

We chose to apply the Yeo-Johnson transform for a few reasons. First, the synthetic data tested above followed a Gaussian distribution, and to try and replicate its performance on real-world datasets, we transformed our data to have an approximately Gaussian distribution. Second, some of the datapoints and features in our real-world dataset contain outliers, and performing a linear scaling transformation to bound the $L_\infty$ norm of these points without the Yeo-Johnson transform would cause many of the values in the design matrices to be nearly zero. Finally, some of the datasets had outliers in their target variables which dominated mean-squared error loss. The Yeo-Johnson transform can combat this outcome. Note that we demonstrate results without the Yeo-Johnson transform in Appendix E.

\subsubsection{$F_1$ Scores}

To measure how well \texttt{RNM-Screen} performed on these datasets, we compared its solutions to the weight vectors produced by scikit-learn's nonprivate LASSO optimizer \cite{scikit-learn}. We aimed to find a regularization strength for the nonprivate LASSO algorithm which produced approximately $\frac{d}{3}$ nonzero coefficients on a given dataset. We then used the $L_1$ norm of this output weight as $\lambda$ for \texttt{RNM-Screen}. For \texttt{RNM-Screen}, we set $\epsilon_1 = 4.9,\ \delta_1 = \frac{3}{4} \times \frac{1}{n},\ \epsilon_2 = 0.1, \text{ and } \delta_2 = \frac{1}{4} \times \frac{1}{n}$. For datasets with fewer datapoints, we ran the algorithm for fewer iterations as more iterations would increase the noise added in both the optimization and screening processes and unnecessarily corrupt the results. 

\setcounter{table}{1}

\begin{table}[t]
\renewcommand{\tabcolsep}{2pt}
\label{tab:2}
\caption[LoF entry]{Average true positive rates (TPR), false positive rates (FPR), $F_1$ scores, sparsities, and $R^2$ scores of 20 trials of \texttt{RNM-Screen} when run on real-world dataset. We could not compare $R^2$ scores of \texttt{RNM-Screen} to \texttt{DP-FW} because the solutions of \texttt{DP-FW} are dense. True positives correspond to private nonzero coefficients which are nonzero in the nonprivate solution. False positives correspond to private nonzero coefficients which are zero in the nonprivate solution.}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|ccc|ccccc}
& $n$ & $d$ & Iter. & TPR & FPR & $F_1$ & Sparsity & $R^2$ \\ \hline
Abalone & 4177 & 8 & 100 & 0.333 & 0.200 & 0.534 & \textbf{0.319} & 0.800 \\ 
Housing & 506 & 13 & 100 & 0.250 & 0.444 & 0.497 & \textbf{0.315} & 0.483 \\
Body Fat & 252 & 14 & 100 & 0.600 & 0.444 & 0.333 & \textbf{0.357} & 0.653\\ 
Pyrim & 74 & 27 & 100 & 0.556 & 0.556 & 0.445 & \textbf{0.506} & 0.688\\ 
Triazines & 186 & 60 & 100 & 0.474 & 0.488 & 0.377 & \textbf{0.508} & 0.851\\ 
\makecell[l]{Residential\\Buildings} & 372 & 103 & 200 & 0.484 & 0.486 & 0.387 & \textbf{0.496} & 0.983\\
\makecell[l]{Communities\\and Crime} & 1994 & 122 & 200 & 0.488 & 0.432 & 0.430 & \textbf{0.480} & 0.837\\
BlogFeedback & 52397 & 280 & 1000 & 0.580 & 0.453 & 0.370 & \textbf{0.450} & 0.357
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}

The results of this experiment are shown in Table 2. Of note, when computing the $F_1$ score, we defined the true zero and true nonzero coefficients with respect to the output of the nonprivate LASSO optimizer, not with respect to the optimal weight, as the optimal weight vector is unknown. Bolded values in Table 2 are significantly better than those produced by \texttt{DP-FW}, as measured by a nonparametric sign test. Note that we verified that running the Frank-Wolfe algorithm for more iterations on the smaller datasets produced little change in $F_1$ score while increasing the density of the solutions. This is why we chose to run \texttt{RNM-Screen} for fewer iterations on these datasets. For the larger BlogFeedback dataset, additional iterations did increase the density of the result but were accompanied with an increased $F_1$ score. This is why we ran this for more iterations.

Analyzing Table 2, it is clear that the results for $F_1$ scores are not ideal. For all datasets, \texttt{RNM-Screen} was unable to produce an $F_1$ score significantly better than the standard private Frank-Wolfe algorithm. Overall, \texttt{RNM-Screen} is significantly better at distinguishing between true nonzero and zero coefficients than \texttt{ADP-Screen}, but the results shown in Table 2 indicate that for real-world datasets, \texttt{RNM-Screen} does not effectively find the same nonzero coefficients as a LASSO solver. 

\subsubsection{Mean Squared Error}

To determine whether \texttt{RNM-Screen} allowed any learning to occur, we tracked the mean squared error at every iteration on the above datasets. Results are shown in Figure 2. 

Despite the disappointing results on $F_1$ scores, the mean squared error plots in Figure 2 indicate that learning still occurs on larger datasets despite the inaccurate choice of nonzero coefficients. We believe this is a valuable result, as it shows that the solutions produced by \texttt{RNM-Screen} may be useful in prediction. Indeed, research on the duality of differential privacy and algorithmic stability shows that differentially private algorithms do not seek to produce similar results to nonprivate algorithms but rather find algorithmically stable solutions which fit the data sufficiently well \cite{dwork2009differential}. 

We tested whether \texttt{RNM-Screen}'s nonzero coefficients served as an effective basis for the coefficients which it incorrectly set to zero to determine if the models produced by \texttt{RNM-Screen} have similar expressivity to nonprivate models. We did this by fitting an unregularized multivariate linear regression from the features of the dataset corresponding to \texttt{RNM-Screen}'s nonzero coefficients to the features of the dataset whose \texttt{RNM-Screen} set to zero but were nonzero in the nonprivate solution. We reported the $R^2$ scores of these regressions in Table 2. Higher values of $R^2$ scores indicate that \texttt{RNM-Screen}'s nonzero coefficients can better approximate the target features. 

Table 2 indicates that for most datasets, the features which \texttt{RNM-Screen} chooses are able to approximate the unchosen features which the nonprivate method chooses reasonably well. This confirms our intuition that although \texttt{RNM-Screen} does not choose the same nonzero coefficients as a nonprivate rule, it is still able to learn effectively from the coefficients it chooses.

In order to test whether the mean squared errors are good for their level of sparsity, we compared \texttt{RNM-Screen} to an Oracle-$K$ clip of the private Frank-Wolfe Algorithm. Specifically, for the Oracle-$K$ clip, we found a weight with the private Frank-Wolfe algorithm and then kept only the $K$ absolute largest coefficients nonzero, where $K$ is the number of nonzero coefficients which the nonprivate LASSO solver found. Note that this is not differentially private, as the value of $K$ is not private, but we employed it as a strong baseline. If \texttt{RNM-Screen} produces $F_1$ scores or mean squared errors similar to the Oracle-$K$ clip, we believe it performs well for a private sparse algorithm. 

Results are shown in Table 3, which includes also includes two more nonprivate columns. The NP-FW column displays results from the nonprivate Frank-Wolfe algorithm without any feature selection, and the Preselect-$K$ FW column displays results from the nonprivate Frank-Wolfe algorithm after choosing the $K$ features with highest $L_1$ norm.\footnote{Note that we also tested using the nonprivate Frank-Wolfe algorithm with the screening rule, but this produced identical results to the NP-FW column. This is because nonprivate Frank-Wolfe optimization was started from the zero vector, and the algorithm did not update any features which the screening rule removed. Intuitively, this makes sense - the Frank-Wolfe algorithm only updated the coefficients which were beneficial in the final solution, and the screening rule did not make these coefficients zero.} For all datasets with $d > 20$, \texttt{RNM-Screen} does not produce a significantly worse $F_1$ score than the Oracle-$K$ technique. For all but the BlogFeedback dataset, the datasets with $d > 20$ also do not have significantly worse mean squared errors. These results imply that \texttt{RNM-Screen} is an effective way to ensure privacy while producing a sparse solution. While it may not identify the same nonzero coefficients which a nonprivate LASSO solver identifies, it is still able to learn effectively while producing comparatively good $F_1$ scores and sparsity. 

%

\begin{table}[t]
\renewcommand{\tabcolsep}{3pt}
\label{tab:3}
\caption{Comparing the $F_1$ scores and mean squared errors (MSEs) of 20 trials of the Oracle-$K$ privately optimized Frank-Wolfe with 20 trials of \texttt{RNM-Screen}. Results from the nonprivate Frank-Wolfe algorithm are included as \texttt{NP-FW} and demonstrate that without private optimization or pruning, the algorithm achieves higher $F_1$ scores and MSEs, as expected.}
\begin{center}
\begin{small}
\begin{sc}
\adjustbox{max width=\columnwidth}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
& \multicolumn{6}{c}{Nonprivate}                                & \multicolumn{2}{c}{Private}    \\ \cmidrule(lr){2-7} \cmidrule(lr){8-9}
& \multicolumn{2}{c}{NP-FW} & \multicolumn{2}{c}{Preselect-$K$ FW} & \multicolumn{2}{c}{Oracle-$K$ FW} & \multicolumn{2}{c}{RNM-Screen} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}  \cmidrule(lr){8-9}
Dataset                               & $F_1$       & MSE         & $F_1$           & MSE             & $F_1$               & MSE & $F_1$               & MSE      \\ \midrule
Abalone                               & 1.000       & 0.512       & 0.667           & 0.689           & \textbf{0.867}  & \textbf{0.560}  & 0.534               & 0.894    \\
Housing                               & 1.000       & 0.322       & 0.000           & 0.786           & \textbf{0.788}  & \textbf{0.516}  & 0.497               & 0.835    \\
Body Fat                              & 0.833       & 0.025       & 0.400           & 0.246           & 0.320           & \textbf{0.368}  & 0.333               & 0.689    \\
Pyrim                                 & 0.889       & 0.439       & 0.571           & 0.759           & 0.339           & 0.451           & \textbf{0.445}      & 0.451    \\
Triazines                             & 0.829       & 0.610       & 0.467           & 0.655           & 0.337           & 0.979           & \textbf{0.377}      & 0.979    \\
\makecell[l]{Residential\\ Buildings} & 0.667       & 0.009       & 0.500           & 0.358           & 0.342           & 0.415           & \textbf{0.387}      & 0.431    \\
\makecell[l]{Communities\\ and Crime} & 0.800       & 0.277       & 0.540           & 0.373           & 0.400           & 0.382           & 0.430               & 0.395    \\
BlogFeedback                          & 0.868       & 0.489       & 0.546           & 0.489           & 0.320           & \textbf{0.736}  & \textbf{0.370}      & 0.822    \\ \bottomrule
\end{tabular}%
}
\end{sc}
\end{small}
\end{center}
\end{table}

\section{Conclusion}

In this paper, we created \texttt{ADP-Screen}, the first differentially private screening rule. After showing that it overscreens coefficients and is unable to solve a simple regression problem with a synthetic dataset, we developed \texttt{RNM-Screen}. We showed that it performs better than \texttt{ADP-Screen} on synthetic data since it is actually able to distinguish between screening true zero and true nonzero coefficients. After testing on the synthetic datasets, we tested \texttt{RNM-Screen}'s performance on real-world datasets. We found that it produces good sparsity with decreasing mean-squared error on larger datasets. Additionally, $R^2$ scores indicate that the coefficients it chooses are able to model the coefficients it screened away during computation. Finally, we found modest evidence that the Yeo-Johnson transform improves \texttt{RNM-Screen}'s performance on real-world datasets. We believe that the algorithmic and empirical developments this work makes to the field of sparse regression with differential privacy makes it a valuable result to the exploration of future techniques for screening rules and sparse differentially private regression. 

%
%
%
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%
%
\appendix

\section{Proof: Privatizing Iterative Screening}

\setcounter{theorem}{0}

\begin{theorem}
    Under the conditions listed above, the sensitivity of Equation 1 when $f(\mathbf{Xw}) = \frac{1}{n} \left( \mathbf{y} - \mathbf{Xw} \right)^\top \left( \mathbf{y} - \mathbf{Xw} \right)$ is $$\frac{2\lambda}{n} + \frac{2\lambda^2}{n} + \frac{1}{n} \left( 1 + \lambda \right)\sqrt{\frac{4\lambda^2/n}{1/n}}.$$ The sensitivity of releasing $\mathbf{s} = \begin{bmatrix} s_1 & \dots & s_d \end{bmatrix}^\top$ is $$\frac{2\lambda\sqrt{d}}{n} + \frac{2\lambda^2\sqrt{d}}{n} + \frac{1}{n} \left( \sqrt{d} + \lambda\sqrt{d} \right)\sqrt{\frac{4\lambda^2/n}{1/n}}.$$
\end{theorem}

\begin{proof}
    Let $\mathbf{u} = \mathbf{Xw}$. For linear regression, $f(\mathbf{u}) = \frac{1}{2n} (\mathbf{u} - \mathbf{y})^{\top}(\mathbf{u} - \mathbf{y})$, implying $\nabla f(\mathbf{u}) = \frac{1}{n}(\mathbf{u} - \mathbf{y})$ and $\nabla^{2} f(\mathbf{u}) = \frac{1}{n} \mathbf{I}_{n}$. Therefore, from the definitions of Lipschitz smoothness and strong convexity, we can see that $f(\mathbf{u})$ is $\frac{1}{n}$-smooth and $\frac{1}{n}$-strongly convex with respect to $\mathbf{u}$. 

    By using the triangle inequality and the fact that the maximum of a sum is at most the sum of each element's maximum, we can bound the sensitivity of Equation 1 for linear regression by summing the sensitivity of each of its terms. These calculations are shown below. Assume without loss of generality that the difference between $\mathbf{X}$ and $\mathbf{X}'$ is the data-target tuple $(\mathbf{x}_{0}, y_{0})$. 
    
    To bound the first term of Equation 1, note that 
    \begin{align*}
        &\max_{\mathbf{X}, \mathbf{X}', y, y'} \  \left\lvert \mathbf{x}_{(i)}^{\top} \nabla f(\mathbf{Xw}) - \mathbf{x'}_{(i)}^{\top} \nabla f(\mathbf{X'w}) \right\rvert \\ 
        = & \max_{\mathbf{x}_0, y_0} \ \left\lvert x_{0i} \left[\frac{1}{n}\left(\mathbf{x}_0^{\top}\mathbf{w} - y_0 \right)  \right] \right\rvert \\ 
        \leq & \frac{2\lambda}{n},
    \end{align*}
    where the first simplification comes from expanding the inner products and the second follows directly from restrictions on the feasible set and norms of the input data. This means that the sensitivity of the first term for one value of $i$ is $\frac{2\lambda}{n}$. By the sensitivity principles of vectors, this means that releasing the values of the first term for all $i$ has $L_2$-sensitivity of $\frac{2\lambda\sqrt{d}}{n}$ \cite{dwork2014algorithmic}. 
    
    For the second term, 
    \begin{align*}
        &\max_{\mathbf{X}, \mathbf{X}', y, y'} \  \lvert (\mathbf{Xw})^{\top}\nabla f(\mathbf{Xw}) - (\mathbf{X'w})^{\top}\nabla f(\mathbf{X'w}) \rvert \\ = & \max_{\mathbf{x}_0, y_0} \ \bigg\vert \mathbf{x}_{0}^{\top}\mathbf{w} \bigg[\frac{1}{n}\big(\mathbf{x}_0^{\top}\mathbf{w} - y_0 \big)  \bigg] \bigg\vert \\
        \leq &\frac{2\lambda^2}{n},
    \end{align*}
    using the same logic as the previous calculation. This means its sensitivity is $\frac{2\lambda^2}{n}$ and its $L_2$-sensitivity is $\frac{2\lambda^2 \sqrt{d}}{n}$.
    
    To find the sensitivity of the third term, we note that the maximum of a product is bounded by the product of maximums. Given this, we find that 
    \begin{align*}
        &\max_{\mathbf{X}, \mathbf{X}'} \  \left\lvert \left\lVert \mathbf{x}_{(i)} \right\rVert_2 - \left\lVert \mathbf{x}'_{(i)} \right\rVert_2 \right\rvert \\
        \leq &\max_{\mathbf{x}_0} \sqrt{\left\lvert x_{0i}^2 \right\rvert}  \\
        \leq &1,
    \end{align*}
    where the first simplification is derived from expanding the first and noting that the difference of square roots must be less than or equal to the square root of the absolute value of the difference in their squared terms. The same logic can be applied for 
    \begin{align*}
        &\max_{\mathbf{X}, \mathbf{X}'} \  \left\lvert \left\lVert \mathbf{Xw} \right\rVert_2 - \left\lVert \mathbf{X}'\mathbf{w} \right\rVert_2 \right\rvert \\ %
        \leq  &\max_{\mathbf{x}_0, y_0} \  \sqrt{\left\lvert \left(\mathbf{x}_0^{\top}\mathbf{w}\right)^2 \right\rvert}  \\
        \leq &\lambda.
    \end{align*}
    For the Wolfe gap function, 
    \begin{align*}
        &\max_{\mathbf{X}, \mathbf{X}', y, y', \mathbf{z} \in \mathcal{C}} \ \vert (\mathbf{Xw} - \mathbf{Xz})^{\top}\nabla f(\mathbf{Xw}) - (\mathbf{X'w} - \mathbf{X'z})^{\top}\nabla f(\mathbf{X'w}) \vert \\ 
        = &\max_{\mathbf{x}_0, y_0, \mathbf{z} \in \mathcal{C}} 
            \left\lvert \mathbf{x}_{0}^{\top}(\mathbf{w} - \mathbf{z}) \left[\frac{1}{n}\left(\mathbf{x}_0^{\top}\mathbf{w} - y_0 \right)  \right] \right\rvert \\ %
        \leq &\frac{4 \lambda^2}{n}. 
    \end{align*}
    Plugging in each of these calculations, the sensitivity of this term for a single $i$ is $\frac{1}{n} \left( 1 + \lambda \right)\sqrt{\frac{4\lambda^2/n}{1/n}}$ and the $L_2$-sensitivity for releasing all $i$ is bounded by $\frac{1}{n} \left( \sqrt{d} + \lambda\sqrt{d} \right)\sqrt{\frac{4\lambda^2/n}{1/n}}$. By the triangle inequality, this means the sensitivity of the screening rule for one $i$ is bounded by 
    \begin{equation}
    \frac{2\lambda}{n} + \frac{2\lambda^2}{n} + \frac{1}{n} \left( 1 + \lambda \right)\sqrt{\frac{4\lambda^2/n}{1/n}}
    \end{equation}
    and the total $L_2$-sensitivity for releasing all $i$ is bounded by 
    \begin{equation}
    \label{eq:2}
    \frac{2\lambda\sqrt{d}}{n} + \frac{2\lambda^2\sqrt{d}}{n} + \frac{1}{n} \left( \sqrt{d} + \lambda\sqrt{d} \right)\sqrt{\frac{4\lambda^2/n}{1/n}}.
    \end{equation}
\end{proof}

\section{\texttt{ADP-Screen}}
\setcounter{algorithm}{0}
\begin{algorithm}[!h]
\label{alg:1}
\caption{\texttt{ADP-Screen}}
\begin{algorithmic}[1]
\REQUIRE Privacy Parameters: $\epsilon_1 > 0$, $\epsilon_2 > 0$, $0 < \delta_1 \leq 1$, $0 < \delta_2 \leq 1$; Constraint: $\lambda > 0$; Iterations: $T$; Design Matrix: $\mathbf{X} \in \mathbb{R}^{n \times d}$ where $\lVert \mathbf{x}_i \rVert_\infty \leq 1$ for all $i \in \{1, \ldots, n \}$; Target: $\mathbf{y}$; $L_2$-Sensitivity: $\Delta_2$; Set of Iterations to Screen: $I$.
%
%
\STATE \textbf{Define:} $\epsilon_\text{iter}$ as the per-iteration $\epsilon$ value; $\delta_\text{iter}$ as the per-iteration $\delta$ value.
\STATE $l \gets \lvert I \rvert$
%
\STATE $\delta_{\text{iter}} \gets \frac{\delta_2}{l + 1}$
\STATE $\epsilon_{\text{iter}} \gets \frac{\epsilon_2}{2\sqrt{2l \log \left(1 / \delta_{\text{iter}}\right)}}$
\STATE $\sigma^2 \gets \frac{2\Delta_2^2 \log \left( 1.25/\delta_{\text{iter}} \right)}{\epsilon_{\text{iter}}^2}$
%
\STATE $\widehat{\mathbf{w}}^{(0)} \gets$ Random Vector in $\{\mathbf{w} \in \mathbb{R}^d : \lVert \mathbf{w} \rVert_1 \leq \lambda\}$
\FOR{$t = 1$ to $T$}
    \STATE $\widehat{\mathbf{w}}^{(t)} \gets \texttt{DP-FW Step} \left( \epsilon_1, \delta_1, \lambda, T, \mathbf{X}, \mathbf{y}, \widehat{\mathbf{w}}^{(t - 1)} \right)$
    \IF{$t \in I$}
        \FOR{$i = 1$ to $d$}
            \STATE $s_i \gets$ $\lvert \mathbf{x}_{(i)}^{\top} \nabla f(\mathbf{X\widehat{\mathbf{w}}^{(t)}}) \rvert + (\mathbf{X\widehat{\mathbf{w}}^{(t)}})^{\top}\nabla f(\mathbf{X\widehat{\mathbf{w}}^{(t)}}) + L(\lVert \mathbf{x}_{(i)} \rVert_2 + \lVert \mathbf{X\widehat{\mathbf{w}}^{(t)}} \rVert_2) \sqrt{\mathcal{G}_{\mathcal{C}}(\mathbf{\widehat{\mathbf{w}}^{(t)}}) / \mu} $
        \ENDFOR
        \STATE $\mathbf{s} \gets \mathbf{s} + \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I}_d)$
        \STATE $\widehat{\mathbf{w}}_j^{(t)} \gets 0$ \textbf{if} $s_j < 0$ \textbf{for all} $j$
    \ENDIF
\ENDFOR
\STATE Output $\widehat{\mathbf{w}}^{(T)}$
\end{algorithmic}
\end{algorithm}

Using the bounds for the $L_2$-sensitivity of the screening rule in Equation 1, we discuss our first attempt at implementing it into a differentially private linear regression training procedure. Prior to proceeding, we highlight that the method produced in this section is impractical for actual use; rather, the method demonstrates the critical problem of ``overscreening'' in private screening rules. We tried to combat this behavior by only perfoming screening at a set of user-defined iterations, $I$. For example, in Figure 3 we tried setting $I = \{1, 2, \dots, 1000\}$, $I = \{50, 100, \dots, 1000\}$, and $I = \{1000\}$. However, none of these seemed to work very well, as discussed below. We develop a method to address overscreening in Section 5.

Since our screening rule requires $L_1$-constrained optimization, we employ the private Frank-Wolfe algorithm developed in \cite{talwar2015nearly} to train regression models. To the best of our knowledge, this is the only differentially-private algorithm for $L_1$-constrained optimization. Additionally, the mean-squared error loss has Lipschitz constant 1 with respect to the $L_1$ norm, which satisfies the algorithm's requirement for $L_1$-Lipschitz loss functions. 

Repeatedly computing and privatizing our screening rule requires three common differential privacy techniques: the Gaussian mechanism, the advanced composition theorem, and the basic composition theorem. The Gaussian mechanism states that for a function $f$ with sensitivity $s$, the output $f(\mathbf{x}) + \mathcal{N}(\sigma^2)$, where $\sigma^2 = \frac{2s^2 \log(1.25)/\delta}{\epsilon^2}$, is $(\epsilon, \delta)$-differentially private. The advanced composition theorem states that applying an $(\epsilon, \delta)$-differentially private mechanism $k$ times satisfies $(\epsilon', k\delta + \delta')$-differential privacy, where $\epsilon' = \epsilon\sqrt{2k\log(1/\delta')} + k\epsilon\left(e^{\epsilon} - 1 \right)$. Finally, the basic composition theorem states that when applied sequentially, mechanisms with privacy parameters $(\epsilon_1, \delta_1)$ and $(\epsilon_2, \delta_2)$ have cumulative privacy $(\epsilon_1 + \epsilon_2, \delta_1 + \delta_2)$ \cite{near_abuah_2021}. 

Our algorithm is shown in Algorithm 1, abstracting away the steps required for the private Frank-Wolfe algorithm. Since our mechanism uses the advanced composition theorem with approximate differential privacy, we call this method \texttt{ADP-Screen}. We include the following guarantee for privacy:

\begin{lemma}
    Algorithm 1 is $(\epsilon_1 + \epsilon_2, \delta_1 + \delta_2)$-differentially private.
\end{lemma}
\begin{proof}
    In lines 3-5, we calculate the scale of noise we need to add to our screening computation such that each computation satisfies $(\epsilon_{\text{iter}}, \delta_{\text{iter}})$ privacy. This means that each iteration of line 13 uses the Gaussian mechanism and is $(\epsilon_\text{iter}, \delta_\text{iter})$-differentially private \cite{dwork2009differential}. By using the advanced composition theorem for approximate differential privacy, we can identify that $l$ compositions of $\epsilon_\text{iter}$ and $\delta_\text{iter}$ is $(\epsilon_2, \delta_2)$-differentially private \cite{dwork2006our,dwork2010boosting}. The differentially private Frank-Wolfe algorithm guarantees the output of $T$ compositions of \texttt{DP-FW Step}s is $(\epsilon_1, \delta_1$)-differentially private. Following this, the basic composition theorem of approximate differential privacy guarantees that Algorithm 1 is $(\epsilon_1 + \epsilon_2, \delta_1 + \delta_2)$-differentially private.
\end{proof}

\texttt{ADP-Screen} did not produce good results, and as such, we include the experiments section demonstrating its performance in the appendix. Here, we include the main takeaways from our experiments, which will be used to develop a better-performing screening rule in the following sections.
\begin{enumerate}
    \item \texttt{ADP-Screen} was unable to discriminate between true zero and true nonzero coefficients.
    \item \texttt{ADP-Screen} screened too many coefficients per iteration, and since the Frank-Wolfe algorithm only updates one coefficient at a time, after a few iterations, the weight vector is only able to have up to a few nonzero coefficients which have not been screened to zero.
\end{enumerate}

\section{Experiments on \texttt{ADP-Screen}}

To test whether \texttt{ADP-Screen} performs well in practice, we tested how it performed on linear regression. To do so, we used the synthetic dataset which Raj et al. used to test their nonprivate screening algorithm. Specifically, we generated $3000$ datapoints in $\mathbb{R}^{600}$ from the standard normal distribution, and scaled the final dataset so $\lVert \mathbf{x}_i \rVert_\infty \leq 1$. We set the true weight vector $\mathbf{w}^*$ to be sparse with 35 entries of $+1$ and 35 entries of $-1$, and set $\mathbf{y} = \mathbf{Xw}^*$. Raj et al. demonstrated that the nonprivate screening rule listed in Equation 1 performs well on this dataset for linear regression. We verified this result, finding that using the nonprivate Frank-Wolfe optimizer with the nonprivate screening rule at every iteration produced a final weight vector in which nonzero components were only at the locations of nonzero components in the true weight vector and $55\%$ of the true nonzero components were nonzero after training. We then ran the private \texttt{ADP-Screen} on this dataset for 1000 iterations with $\epsilon_1 = \epsilon_2 = 2.5, \delta_1 = \delta_2 = \frac{1}{6000}, \text{ and }\lambda = 5$. 

Figure 3 shows the results of this experiment when we implemented screening after every iteration, every $50^{\text{th}}$ iteration, and after the last iteration. It is clear that \texttt{ADP-Screen} is not able to discriminate between screening true zero and true nonzero coefficients in any of these cases. Additionally, when private screening is implemented too often, it screens too many coefficients, and since the Frank-Wolfe algorithm only updates one coefficient at a time, after a few iterations, the weight vector is only able to have up to a few nonzero coefficients which have not been screened to zero. To identify whether the private Frank-Wolfe algorithm or the private screening methods were causing these poor results, we ran the following two experiments: 
\begin{enumerate}[label=(\Alph*)]
    \item \label{exp:A} We tested how well nonprivate screening performed using the private Frank-Wolfe algorithm with $\epsilon = 2.5$ and $\delta=\frac{1}{6000}$. We found that no matter how often we implemented the screening rule, no coefficients were screened from the solution. 
    \item \label{exp:B} We tested how well private screening performed using the nonprivate Frank-Wolfe algorithm with $\epsilon=2.5$ and $\delta=\frac{1}{6000}$. We found that when screening every $50^{\text{th}}$ iteration, the screening rule would produce a final weight vector with nonzero components in approximately $10\%$ of the true nonzero components and none of the true zero components. The results of this experiment when screening every iteration or only at the last iteration mimicked those found in the respective rows of Figure 1.
\end{enumerate}
These experiments provide key insights into the results shown in Figure 3. Experiment A suggests that all of the screening occurring in Figure 3 arises from noise added to the screening rule. This is because without the screening's noise, no screening occurs. It also implies that the noise added for private optimization made it more difficult to screen coefficients, since when we tested completely nonprivate screening (without noisy optimization), the nonprivate screening rule worked well. Heuristically, this outcome may arise because noisy weights make the Wolfe gap function in Equation 1 very large, meaning that it overpowers the second term, which is the only term that can be negative and is thus essential to effective screening. We verified that the Wolfe gap function evaluates to smaller values when using nonprivate optimization. This result can be seen in Figure 4.

Experiment B indicates that the noise added in the private screening rule makes it much stronger that its nonprivate counterpart. This is observed by noting that without a noisy screening rule, screening at every iteration with the nonprivate Frank-Wolfe optimizer would not screen all the true nonzero components to zero, whereas with the noisy screening rule, almost all components are screened to zero after only a few iterations. This makes sense: in an iteration of \texttt{ADP-Screen}, many coefficients can be screened to zero, but at most one zero coefficient can become nonzero through the Frank-Wolfe update. When the coefficients chosen for screening are noisy, there is a high probability that each coefficient is selected to be screened at some point over many iterations, but only a few incorrectly screened coefficients can become nonzero from a Frank-Wolfe update. 

\begin{figure}[!ht]
    \label{fig:1}
    \centering
    \includegraphics[width=0.75\linewidth]{Figures/lin_fig.png}
    \caption{Testing \texttt{ADP-Screen} on a synthetic dataset. Values reported are the fraction of nonzero coefficients in the output of \texttt{ADP-Screen} which correspond to the indices of the true nonzero and true zero coefficients. The true nonzero and true zero coefficients are known from the dataset generation procedure.}
\end{figure}

\begin{figure}[!ht]
    \label{fig:2}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/linear_only_gap.png}
    \caption{Comparing the Wolfe gap function for nonprivate and private optimization when nonprivate screening is applied at every iteration.}
\end{figure}

\section{Proof: Analyzing \texttt{RNM-Screen}'s Screening Potential}

\setcounter{theorem}{3}

\begin{theorem}
    Let $\{I_1, I_2, \dots, I_d\}$ be a set of indicator functions where $I_i$ takes value $1$ when the $i^{\text{th}}$ coefficient is nonzero after $T$ iterations of training. Then 
    \begin{align*}
        \lim_{T \to \infty} \mathbb{E} \left[ I_1 + \dots + I_d \right] &= \frac{(d-1)^2 d}{(d-1)d + 1} \\ 
        \lim_{d \to \infty} \mathbb{E} \left[ I_1 + \dots + I_d \right] &= T - 1.
    \end{align*}
\end{theorem}

\begin{proof}
We compute the expected value of the number of nonzero coefficients after training, $\mathbb{E} \left[ I_1 + I_2 + \dots + I_d \right]$. By the laws of expectation, this is equivalent to $\mathbb{E} \left[ I_1 + I_2 + \dots + I_d \right] = \mathbb{E} \left[ I_1 \right] + \mathbb{E} \left[ I_2 \right] + \dots + \mathbb{E} \left[ I_d \right]$. Since the coefficients are updated and screened uniformly, they are exchangeable, and thus this expectation simplifies to $d\mathbb{E} \left[ I_1 \right]$.

By the properties of Bernoulli random variables, the expectation of an indicator function is equal to the probability it is $1$. We can see that $\mathbb{P}(I_1 = 1)$ equals
\begin{align*}
    &\mathbb{P}(\text{Coef. 1 Never Screened})\mathbb{P}(\text{Coef. 1 Updated}) + \\
    &\mathbb{P}(\text{Coef. 1 Last Screened At Ite. 1})\mathbb{P}(\text{Coef. 1 Updated After Ite. 1}) + \\
    &\mathbb{P}(\text{Coef. 1 Last Screened At Ite. 2})\mathbb{P}(\text{Coef. 1 Updated After Ite. 2}) + \\ 
    &\dots + \\
    &\mathbb{P}(\text{Coef. 1 Last Screened At Ite. T})\mathbb{P}(\text{Coef. 1 Updated After Ite. T}).
\end{align*}
This can be calculated as 
$$\left(\frac{d-1}{d}\right)^T\left(1 - \left(\frac{d-1}{d}\right)^T \right) + \sum_{i = 1}^{T} \frac{1}{d}\left(\frac{d-1}{d}\right)^{T-i}\left(1 - \left(\frac{1}{d}\right)^{T-i} \right)$$
where the summation breaks each event up into the $\frac{1}{d}$ probability that coefficient 1 was screened on iteration $i$, the $\left( \frac{d-1}{d}\right)^{T-i}$ probability that coefficient 1 was not screened after iteration $i$, and the $\left(1 - \left(\frac{1}{d}\right)^{T-i} \right)$ probability that coefficient 1 was updated after iteration $i$. Therefore $d\mathbb{E} \left[ I_1 \right]$ equals 
$$d\left(\frac{d-1}{d}\right)^T\left(1 - \left(\frac{d-1}{d}\right)^T \right) + \sum_{i = 1}^{T} \left(\frac{d-1}{d}\right)^{T-i}\left(1 - \left(\frac{1}{d}\right)^{T-i} \right).$$
This expression holds only when the assumptions in this section are satisfied. However, as the number of iterations approaches infinity, the scale of differentially private noise increases unbounded, and will thus dominate any finite gradient. This means that as $T \to \infty$, choosing coefficients to update and screen approaches uniformity. Additionally, as the number of dimensions approaches infinity, the probability of choosing a specific coefficient to update or screen approaches $\frac{1}{d}$. Thus, we compute the limits of $d\mathbb{E} \left[ I_1 \right]$ under these limits:
\begin{align*}
    \lim_{T \to \infty} d\mathbb{E} \left[ I_1 \right] &= \frac{(d-1)^2 d}{(d-1)d + 1} , \\
    \lim_{d \to \infty} d\mathbb{E} \left[ I_1 \right] &= T - 1.
\end{align*}
\end{proof}

\section{Ablating the Yeo-Johnson Transform}

We wanted to determine whether the Yeo-Johnson transform had a significant impact on \texttt{RNM-Screen}'s performance. For this reason, we chose to run an ablation study in which we did not apply the Yeo-Johnson transform to the features or targets. Instead, we scaled datapoints so $\lVert \mathbf{x}_i \rVert_\infty \leq 1$ and $\lvert y_i \rvert \leq 1$. We ran this ablation study for each of the real-world datasets in the previous section for the same number of iterations as listed in Table 2. 

Table 4 and Table 5 list the results of this experiment. Note that it is impossible to directly compare the mean-squared error values to previous experiments because the scaling of the target variables is different. 

From Table 4, it is clear that as before, running \texttt{RNM-Screen} on this dataset does not produce significantly better scores than the standard private Frank-Wolfe algorithm, but it does significantly improve the sparsity. However, for all datasets except BlogFeedback, Table 4 has $R^2$ scores that are worse than Table 2. This indicates that the Yeo-Johnson transform allows our chosen features to better approximate unchosen important features when building a model. 

Table 5 demonstrates a significant difference between the results of the transformed and untransformed data. With the transformed data, Table 3 indicates that \texttt{RNM-Screen} outperforms Oracle-$K$ FW on larger datasets with respect to the $F_1$ score. However, in Table 5, this does not happen for any dataset. This may indicate that transforming the data makes it easier for \texttt{RNM-Screen} to distinguish between true zero and true nonzero coefficients, which thus improves its performance with respect to the $F_1$ score. 

\begin{table}[!h]
\label{tab:4}
\caption{Average $F_1$ scores, sparsities, and $R^2$ values of 20 trials of \texttt{RNM-Screen} when run on real-world datasets which have not been transformed with the Yeo-Johnson transform. }
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|ccc}
 & $F_1$ & Sparsity & $R^2$ \\ \hline
Abalone & 0.450 & \textbf{0.113} & 0.109 \\ 
Housing & 0.525 & \textbf{0.200} & 0.062 \\
Body Fat & 0.447 & \textbf{0.204} & 0.526 \\ 
Pyrim & 0.451 & \textbf{0.441} & 0.582 \\ 
Triazines & 0.412 & \textbf{0.497} & 0.808 \\ 
\makecell[l]{Residential\\Buildings} & 0.372 & \textbf{0.240} & 0.856 \\
\makecell[l]{Communities\\and Crime} & 0.433 & \textbf{0.505} & 0.821 \\
BlogFeedback & 0.367 & \textbf{0.439} & 0.383  
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}

\begin{table}[!h]
\renewcommand{\tabcolsep}{3pt}
\label{tab:3}
\caption{Comparing the $F_1$ scores and mean squared errors (MSEs) of 20 trials of the Oracle-$K$ privately optimized Frank-Wolfe with 20 trials of \texttt{RNM-Screen} on data which has not been processed with the Yeo-Johnson transform.}
\begin{center}
\begin{small}
\begin{sc}
\adjustbox{max width=\columnwidth}{%
\begin{tabular}{@{}lcccccc@{}}
\toprule
& \multicolumn{4}{c}{Nonprivate}                                & \multicolumn{2}{c}{Private}    \\ \cmidrule(lr){2-5} \cmidrule(lr){6-7}
& \multicolumn{2}{c}{NP-FW} & \multicolumn{2}{c}{Oracle-$K$ FW} & \multicolumn{2}{c}{RNM-Screen} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} 
Dataset                               & $F_1$       & MSE         & $F_1$           & MSE             & $F_1$               & MSE      \\ \midrule
Abalone & 1.000 & 0.012 & \textbf{0.667} & \textbf{0.014} & 0.450 & 0.010 \\
Housing &  0.889 & 0.018 & \textbf{0.788} & \textbf{0.051} & 0.525 & 0.159 \\
Body Fat & 1.000 & 0.020 & \textbf{0.550} & \textbf{0.124} & 0.447 & 0.555 \\ 
Pyrim & 0.941 & 0.019 & \textbf{0.583} & \textbf{0.182} & 0.451 & 0.301 \\ 
Triazines & 0.872 & 0.024 & 0.424 & 0.108 & 0.412 & 0.095 \\ 
\makecell[l]{Residential\\Buildings} & 0.061 & 2.120 & 0.411 & 1.711 & 0.372 & 1.820 \\
\makecell[l]{Communities\\and Crime} &  0.469 & 0.020 & \textbf{0.400} & 0.033 & 0.433 & 0.034 \\
BlogFeedback & 0.021 & 0.023 & \textbf{0.349} & 0.022 & 0.367 & \textbf{0.004}\\ \bottomrule
\end{tabular}%
}
\end{sc}
\end{small}
\end{center}
\end{table}

\section{Open Problems}

The experiments in the paper present modest evidence that a screening rule can produce effective sparse solutions for linear regression under differential privacy. In this section, we highlight four open problems for further study. 

First, we ask whether private screening can be adapted to effectively distinguish between the nonzero and zero coefficients of nonprivate solutions on real-world datasets, including high-dimensional ones. Future works to solving this problem may be empirical, perhaps improving \texttt{RNM-Screen}, but a more rewarding approach would identify a utility bound for a private screening rule so it can be compared to other differentially private algorithms. A utility bound is especially useful in comparing differentially private randomized algorithms as it is not subject to random variations which occur when empirically comparing algorithms on datasets. 

Second, we ask whether private screening rules can be employed in a similar approach to nonprivate screening rules. Screened coefficients from nonprivate screening rules will not be updated in future iterations by the optimizer, which improves the computational efficiency of optimization, especially for very high dimensional datasets. Unfortunately, since our screening rule is randomized, it is possible that a coefficient is screened incorrectly, so we continue to optimize for all coefficients despite screening, which does not improve the efficiency of optimization. An effective private screening rule which improves the computational efficiency of optimization would encompass both the sparsity and efficiency benefits of screening rules. 

Third, we note that \texttt{RNM-Screen} requires two sets of privacy parameters, $(\epsilon_1, \delta_1)$ and $(\epsilon_2, \delta_2)$. Although we used a heuristic to set these parameters, consistent guidelines for splitting the privacy budget between these two parameters is unclear. Note that this issue arises in many works with multiple calculations that need to be privatized. For example, the original \texttt{Two-Stage} sparse regression technique proposed by \cite{kifer2012private} also required two sets of privacy parameters. However, in addition to further testing, to produce a private screening rule which can be employed for real-world data analysis, better guidelines for setting these privacy parameters is required. 

Finally, we highlight a limitation to \texttt{RNM-Screen}. On low- \\ dimensional datasets, every coefficient might be important, but \texttt{RNM-Screen} always screens one coefficient per iteration, which can still lead to overscreening. In contrast, nonprivate screening algorithms never screen coefficients which are important to the regression task. For this reason, it is important that \texttt{RNM-Screen} only be used on tasks with a sufficient number of features such that practitioners believe that a sparse solution should exist. 

It is also important to recognize that the screening rules developed in this paper are specific to standard linear regression and do not immediately translate to advanced settings like continual observation. Advanced regression methods are understudied in differential privacy, so in addition to developing feature selection algorithms for these settings, regression optimization techniques must also be developed. 

\end{document}
\endinput
%
%
