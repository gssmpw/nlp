\section{Related Work}
Methods to produce private sparse regression weights all suffer in performance due to the addition of noise. Private $L_1$ optimizers must add higher levels of noise when run for more iterations, incentivizing practitioners to run fewer iterations \cite{talwar2015nearly,wang2020differential}. However, running an optimizer for fewer iterations means that the model will be limited in its learning. On the other hand, private model selection algorithms are computationally inefficient and run prior to training, meaning they are unable to reap the benefit of any information contained within partially trained coefficients of the weight vector \cite{lei2018differentially,thakurta2013differentially}. Although noise is necessary for privacy, an effective private screening rule would run with a private optimizer and improve the optimizer's performance by setting the coefficients of irrelevant features to 0. By using the screening rule on the current weight vector, it can adapt to the optimizer's updates and screen features more accurately. 

%

To develop a differentially private screening rule, we adapt Theorem 11 of Raj et al.'s rule which is flexible to solving many types of regression problems \cite{raj2016screening}. While other screening rules exist, they are geometry- and problem-specific \cite{ghaoui2010safe,wang2014safe,wang2013lasso}. \textbf{Our goal is to utilize Raj etl al.'s screening rule for $L_1$-constrained regression to induce sparsity on Talwar et al.'s $L_1$-constrained private Frank-Wolfe algorithm \cite{raj2016screening,talwar2015nearly}}.\footnote{Note that \cite{raj2016screening} is written with semantics typical in the field of optimization - there are $d$ (d)atapoints and the goal is to optimize a vector with (n)umber of components $n$. In this paper, we use the standard statistical and machine learning conventions, in which there are $n$ (n)umber of datapoints and optimization is done over $d$ (d)imensions.}

Since we use the private Frank-Wolfe algorithm (\texttt{DP-FW}), we also review it here. \texttt{DP-FW} uses the Frank-Wolfe method for $L_1$-constrained convex optimization, which chooses a vertex of the feasible region (scaled $L_1$ ball) which minimizes a linear approximation of the loss function. By doing this for $T$ iterations with appropriate step sizes, the algorithm satisfies $\mathcal{L}(\mathbf{w}^{(T)}) - \min_{\mathbf{w}^*  \in \mathcal{C}} \mathcal{L}(\mathbf{w}^*) \leq \mathcal{O}(\frac{1}{T})$ \cite{frank1956algorithm,jaggi2013revisiting}. The progress of the Frank-Wolfe optimizer can be measured with the Wolfe gap function: $\mathcal{G}_{\mathcal{C}}(\mathbf{w}) = \max_{\mathbf{z} \in \mathcal{C}} (\mathbf{w} - \mathbf{z})^{\top}\nabla f(\mathbf{w})$. It can be shown that $\mathcal{G}_{\mathcal{C}}(\mathbf{w}) \geq f(\mathbf{w}) - f(\mathbf{w}^*)$ for all $\mathbf{w}$. For this reason, the smaller the Wolfe gap function, the closer the optimization is to an optimal solution. To privatize the Frank-Wolfe algorithm, Talwar et al. restrict the $L_\infty$ norm of datapoints so they can calculate the exact sensitivity of the gradients \cite{talwar2015nearly}. They then use the report-noisy-max mechanism to noisily choose which component of the weight vector to update. Unfortunately, due to inexact optimization caused by the noisy selection process, the private Frank-Wolfe algorithm produces dense results, limiting its ability to be used in high-throughput or interpretability-restricted applications of regression. Despite this limitation, the Frank-Wolfe algorithm is ideal for an application of Raj et al.'s screening rule. Of current methods for private high-dimensional regression, recently summarized by \cite{khanna2024sok}, the Frank-Wolfe algorithm is unique in that it uses $L_1$-constrained optimization with updates to one component of the weight vector per iteration. Each of these conditions is important. The first is necessary because Raj et al.'s screening rule requires optimization over an $L_1$-constrained set. The second is important because if a private (noisy) optimization algorithm updates all components of a weight vector at each iteration, then any sparsity induced by applying a screening rule would be lost at the next iteration of optimization, since the output of the optimization would be dense.

To the best of our knowledge, this is the first work considering a differentially private screening rule. The difficulty of DP screening is counteracted by the reward of obtaining sparse \textit{and} private regression, as normal DP destroys sparsity via the addition of noise.