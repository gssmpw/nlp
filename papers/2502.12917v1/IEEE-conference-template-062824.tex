\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{ulem}


\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{soul}
\usepackage{fontawesome}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Contrast-Unity for Partially-Supervised \\ Temporal Sentence Grounding
\vspace{-0.4cm}
}
\DeclareRobustCommand*{\IEEEauthorrefmark}[1]{%
  \raisebox{0pt}[0pt][0pt]{\textsuperscript{\footnotesize #1}}%
}
\author{
    \IEEEauthorblockN{
        Haicheng Wang\IEEEauthorrefmark{1,2,4}\textsuperscript{\textasteriskcentered}, 
        Chen Ju\IEEEauthorrefmark{2}\textsuperscript{\textasteriskcentered},
        Weixiong Lin\IEEEauthorrefmark{4}, 
        Chaofan Ma\IEEEauthorrefmark{4}, 
        Shuai Xiao\IEEEauthorrefmark{2},
        Ya Zhang\IEEEauthorrefmark{3}\textsuperscript{\faEnvelopeO},
        Yanfeng Wang\IEEEauthorrefmark{3}
    }
    \IEEEauthorblockA{
        \IEEEauthorrefmark{1}SJTU Paris Elite Insitute of Technology, Shanghai Jiao Tong University, China  \ 
        \IEEEauthorrefmark{2}Taobao \& Tmall Group of Alibaba, China \\
        \IEEEauthorrefmark{3}School of Artificial Intelligence, Shanghai Jiao Tong University, China
        \ 
        \IEEEauthorrefmark{4}CMIC, Shanghai Jiao Tong University, China\\
        Email: 
            \{anakin\_skywalker,wx\_lin, chaofanma, ya\_zhang, wangyanfeng622\}@sjtu.edu.cn, 
            cju.void@gmail.com
    }
}


% \IEEEauthorrefmark{2}Alibaba Group, China \\    
  

\maketitle
\begingroup
\renewcommand\thefootnote{\relax}\footnotetext{*: Equal contribution. \quad \textsuperscript{\faEnvelopeO}: Corresponding author}
\endgroup
\begin{abstract}
Temporal sentence grounding aims to detect event timestamps described by the natural language query from given untrimmed videos. The existing fully-supervised setting achieves great results but requires expensive annotation costs; while the weakly-supervised setting adopts cheap labels but performs poorly. To pursue high performance with less annotation costs, this paper introduces an intermediate partially-supervised setting, \textit{i.e.}, only short-clip is available during training. To make full use of partial labels, we specially design one contrast-unity framework, with the two-stage goal of implicit-explicit progressive grounding. In the implicit stage, we align event-query representations at fine granularity using comprehensive quadruple contrastive learning: event-query gather, event-background separation, intra-cluster compactness and inter-cluster separability. Then, high-quality representations bring acceptable grounding pseudo-labels. In the explicit stage, to explicitly optimize grounding objectives, we train one fully-supervised model using obtained pseudo-labels for grounding refinement and denoising. Extensive experiments and thoroughly ablations on Charades-STA and ActivityNet Captions demonstrate the significance of partial supervision, as well as our superior performance.
\end{abstract}


\begin{IEEEkeywords}
Video Grounding, Partial Supervision.
\end{IEEEkeywords}


\section{Introduction}
Temporal sentence grounding (TSG) plays an important role for video-language understanding, with the goal to detect the start and end timestamps of the event described by a given natural language query from untrimmed videos. 
TSG covers extensive application scenarios~\cite{b1,b2,b3}, as it could learn high-quality cross-modal representations from large-scale data. 


TSG has developed two popular settings for data annotation: fully-supervised setting (FTSG)~\cite{b4,b5}, where each (video, query) pair is annotated with precise temporal boundaries, and weakly-supervised setting (WTSG)~\cite{b6,b7}, where only the corresponding (video, query) is provided without temporal annotations. While the fully-supervised approach is accurate, it is time-consuming and prone to subjective interpretation, especially for events with complex semantics. The weakly-supervised approach reduces annotation effort but results in lower performance, limiting its practical applications.


Hence, one question naturally arises: \textit{Is there an intermediate setting between full and weak supervisions in TSG, which can obtain relatively high performance but requires less annotation cost?}
This paper answers the question by introducing the \textbf{partially-supervised setting (PTSG)}. Specifically, for each text query, a partial temporal region corresponding to a short video-clip is annotated within the whole event interval. And in the strictest case, partial labels could degenerate to single-frame labels, \textit{i.e.}, labeling one timestamp for each event. At a slight more cost than WTSG in annotation time, such partial supervision greatly improves grounding performance, which is very effective comparing to full or weak supervisions.

Hereafter, our goal is to ground complete event intervals through limited yet precise partial labels. With the same data formulation (video, query, timestamps), partial supervision can approach full supervision continuously, by annotating a proper event duration. Thus, an intuitive thought is that, PTSG and FTSG can share the same training architecture. Following this idea, one trivial solution is to simply train FTSG model using partial annotations. As tested preliminarily, FTSG model performs well using high-quality partial annotation (80\% event coverage), proving its robustness for small turbulence. 
However, the limited short-clip partial label is too noisy for FTSG model to learn semantic patterns, resulting in an unsatisfying result. Therefore, We design \textbf{a contrast-unity framework} for implicit-explicit (two-stage) progressive grounding.


Given the training set with incomplete partial labels, \textbf{the implicit stage} aims to refine the partial annotation at fine granularity. 
To get better labels, we propose one novel quadruple contrast pipeline, leveraging inter and intra-sample contrast for uni and cross-modal alignment. 
The first two contrasts are built on intra-samples to promote event-query gather for cross-modal correspondence and raise event-background separation for visual uni-modality. Then, to build more semantic contrasts from the whole dataset, another two contrasts are proposed for inter-samples to further enable intra-cluster compactness and inter-cluster separability. To obtain refined event intervals, we introduce an event detector which takes partial labels as seed anchors and extends them for an event mask. Then, features for event and background can be calculated via the event mask.

Thanks to the essence of multi-instance learning~\cite{b8}, with well-alignment representations, the event detector can output refined grounding pseudo-labels. 
Next, we bridge \textbf{another explicit stage} after the implicit stage, by treating grounding pseudo-labels as ground-truth to train one fully-supervised model, then inference through this fully-supervised model. 
This framework could do more at one stroke. Structurally, it bridges the setting gap between PTSG and FTSG, enabling PTSG to enjoy advanced bonuses from FTSG, {\em e.g.}, superior architecture~\cite{b9,b10} and explicit grounding optimization. Functionally speaking, this framework is applicable from single-frame to fully-supervised TSG, giving more freedom to the annotation procedure. On two datasets: Charades-STA and ActivityNet Captions, we annotate partial labels, then experiment to reveal their significance. Our designed framework shows superior performance over competitors. 



\section{Method}
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.5\textwidth] {./fig/frame.png}
\end{center}
\vspace{-0.4cm}
\caption{\textbf{Our Contrast-Unity Framework for PTSG.} It follows an implicit-explicit progressive pipeline. Given partial labels, the implicit stage makes fine-grained alignment of event-query representations, using quadruple contrastive learning. Such fine-aligned representations could naturally result in high-quality grounding psuedo-labels. Hereafter, the explicit stage takes these pseudo-labels as the ground-truth to train another fully-supervised model with explicit grounding objectives, for further grounding refinement. Such one framework achieves the unity between PTSG and FTSG.}
\vspace{-0.5cm}
\label{fig:framework}
\end{figure}


\subsection{Formulation \& Preliminaries}
\noindent \textbf{Problem Formulation.}  \label{subsec:Notations}
\hspace{0.1cm} Given an untrimmed video, Temporal Sentence Grounding (TSG) aims to detect the event boundary $(s, e) \in \mathbb{R}^2$ corresponding to given text query. Full supervision provides precise boundary $(s_i, e_i)$ for each query, while weak supervision only has video-query correspondence. To meet the demands of large-scale data annotations and strong performance, this work considers the novel \textit{partially-supervised TSG setting} (PTSG). For the $i$-th text query, only one short video clip $(t_i^s, t_i^e) \subseteq (s_i, e_i)$ is labeled. We could also write it as $(t_i^c, r)$, where $t_i^c$ is the clip center and $r$ is the clip range. In special case ($r=0$), this partial label degenerates to single-frame setting, \textit{i.e.}, only $t_i^c \in [s_i, e_i]$ is annotated. 


\noindent \textbf{Feature Extraction \& Fusion.}
\hspace{0.1cm} We pre-extract features for videos and queries following the existing methods~\cite{b11,b12}. For the video stream, we adopt pre-trained 3D convolutional networks~\cite{b13,b14}, and obtain $\mathbf{V'} \in \mathbb{R}^{T \times D_v}$; for the query stream, we adopt GloVe~\cite{b15} to obtain $\mathbf{Q'} \in \mathbb{R}^{M \times D_q}$, where $T$, $M$, $D_v$, $D_q$ refer to the number of video frames, the number of query words, the video and query feature dimension. Then, we use one full-connection layer to individually fine-tune uni-modal features $\mathbf{V'}$ and $\mathbf{Q'}$ respectively. To interact bi-modal features, two cross-modal Transformers are further introduced. The fused visual features are denoted as $\mathbf{V} \in \mathbb{R}^{T \times D}$, and linguistic features as $\mathbf{Q} \in \mathbb{R}^{M \times D}$ ($D$ is the feature dimension).



\subsection{Implicit Stage: High-quality Representation}   \label{sec:implicit stage} 
\noindent \textbf{Event Detector.}
To perceive the time interval for event, we adopt a proposal-wise solution: treat $t^c$ as the seed anchor, and map video features $\mathbf{V}$ to center offset $\delta$ and event width $\ell$, through an event detector $\Phi(\cdot)$. And the corresponding start and end timestamps $[\widehat{s}, \, \widehat{e}]$ can be formulated as: 
\begin{equation} \label{eq:instance}
\setlength{\abovedisplayskip}{7pt}
    {[\delta, \; \ell] = \Phi(\mathbf{V}),  \quad  \widehat{s} = p-\frac{\ell}{2},  \quad  \widehat{e} = p+\frac{\ell}{2},}
\setlength{\belowdisplayskip}{7pt}
\end{equation}
where $p=t^c+\delta$ means the center of the grounded event.

\noindent \textbf{Event Representation.}
With the predicted start-end timestamps $[\widehat{s}, \, \widehat{e}]$, we first generate one differentiable temporal mask $\mathbf{m} \in \mathbb{R}^{T}$ through a learnable Plateau-shape~\cite{b16}; then filter out visual features for event $\mathbf{v}^{\mathrm{ev}}$, and background $\mathbf{v}^{\mathrm{bg}}$:
\begin{equation}
    {\mathbf{v}^{\mathrm{ev}} = \frac{1}{T}\sum_{t=1}^{T}{\mathbf{m_t}}\mathbf{V}_t,
    \ \ \
    \mathbf{v}^{\mathrm{bg}} = \frac{1}{T}\sum_{t=1}^{T}{(1-\mathbf{m_t})}\mathbf{V}_t.
    }
\end{equation}


\noindent \textbf{Quadruple Contrasts Pipeline.} To get refined pseudo-labels from event detector, we need to shape one high-quality visual-linguistic alignment space. We perform a quadruple-contrasts pipeline to pursue comprehensive alignment, covering intra- and inter-sample, uni- and multi-modality. The quadruple contrastive loss is calculated with a balancing parameter $\lambda$:
\begin{align}   \label{eq:cont}
\mathcal{L}_{\mathrm{cont}} = (\mathcal{L}_{\mathrm{raml}} + \mathcal{L}_{\mathrm{raun}})  + \lambda(\mathcal{L}_{\mathrm{erml}} + \mathcal{L}_{\mathrm{erun}}).
\end{align}



\noindent \textit{\uuline{Intra-Sample Contrastive Learning.}} 
We first consider learning from one single video-query sample, with visual feature for event $\mathbf{v}^{\mathrm{ev}}$, the background $\mathbf{v}^{\mathrm{bg}}$, and query feature $\mathbf{q} \in \mathbb{R}^{D}$ obtained by mean-pooling the word-wise feature $\mathbf{Q}$ available. 


\noindent \underline{Event-Query Multi-Modal Contrast} enables event-query pairs gather in the embedding space. Here, we introduce the mean video feature $\mathbf{v}^{\mathrm{vd}}$ as one reference, to promote the semantic similarity of ($\mathbf{v}^{\mathrm{ev}}$, $\mathbf{q}$) to be greater than that of ($\mathbf{v}^{\mathrm{vd}}$, $\mathbf{q}$), since $\mathbf{v}^{\mathrm{vd}}$ contains both event and background.
\begin{align}
\mathcal{L}_{\mathrm{raml}} = \mathrm{max}(\mathcal{S}(\mathbf{v}^{\mathrm{vd}}, \mathbf{q})-\mathcal{S}(\mathbf{v}^{\mathrm{ev}}, \mathbf{q})+\alpha, 0),
\end{align}
where $\mathcal{S}$ and $\alpha$ are cosine similarity and margin parameter. $\mathbf{v}^{\mathrm{vd}}$ is got by pooling frame-wise video features $\mathbf{V}$. 


\noindent \underline{Vision Uni-Modal Contrast.} 
Videos are fine-grained and continuous, resulting in similar features across event and background. We hence apply visual-modal contrastive learning to raise event-background separation. Similarly, we use triplet loss to distinguish (video-event) and (event-background).
\begin{align}
\mathcal{L}_{\mathrm{raun}} = \mathrm{max}(\mathcal{S}(\mathbf{v}^{\mathrm{ev}}, \mathbf{v}^{\mathrm{bg}})-\mathcal{S}(\mathbf{v}^{\mathrm{ev}}, \mathbf{v}^{\mathrm{vd}})+\beta, 0),
\end{align}


\noindent \textit{\uuline{Inter-Sample Contrastive Learning.}}
\hspace{0.1cm} To better shape the cross-modal embedding space, we mine the correlations between training samples for superior inter-sample contrasts.


For inter-sample modeling, the key is to measure sample semantic similarity. While for TSG, category clusters are not intuitive. 
Thus, we consider using text queries as the bridge for correlation establishment, as language essentially refers to high-level semantics. We leverage the pre-trained Transformer Bert~\cite{b17} to extract query features and group samples into $K$ clusters $\Lambda_1, \Lambda_2, ..., \Lambda_K$. Note that the same sample could appear in different clusters. Within one batch of size $B$, we randomly select $N$ semantic clusters (B is divisible by N).


\noindent \underline{Event-Query Multi-Modal Contrast.}
We regard the events and queries from the same cluster as the positive pairs, while those from different clusters as negative pairs. Specifically, denoting the positive set of ${i}$-th sample as $\Psi^{\mathrm{+}}_{i}$, and the negative set as $\Psi^{\mathrm{-}}_{i}$, inter-sample multi-modal contrastive learning is:
\begin{align}
\mathcal{L}_{\mathrm{erml}} = & \sum_i - \log \frac{\sum_{m \in {\Psi^{\mathrm{+}}_{i}}}  \exp(\mathbf{v}_i^{\mathrm{ev}} \cdot \mathbf{q}_{m}/ \tau)}{\sum_{j \in \{ \Psi^{\mathrm{+}}_{i} \cup \, \Psi^{\mathrm{-}}_{i} \}
} \exp(\mathbf{v}_i^{\mathrm{ev}} \cdot \mathbf{q}_{j}/ \tau)}.
\end{align}
where $\tau$ is temperature coefficient, $\cdot$ is normalized dot product. 



\noindent \underline{Vision Uni-Modal Contrast.} 
\hspace{0.1cm}
To model the cluster semantics among visual uni-modality, we construct the positive set $\Psi^{\mathrm{+}}_{\mathrm{i}}$ of $i$-th sample by joining events from the same cluster of $i$-th sample, while build the negative set $\Psi^{\mathrm{-}}_{\mathrm{i}}$ by leveraging event features from the other clusters. That is,
\begin{align}
\mathcal{L}_{\mathrm{erun}} = & \sum_i - \log \frac{\sum_{m \in {\Psi^{\mathrm{+}}_{\mathrm{i}}}}  \exp(\mathbf{v}_i^{\mathrm{ev}} \cdot \mathbf{v}_{m}^{\mathrm{ev}} / \tau)}{\sum_{j \in \{ \Psi^{\mathrm{+}}_{\mathrm{i}} \cup \, \Psi^{\mathrm{-}}_{\mathrm{i}} \}
} \exp(\mathbf{v}_i^{\mathrm{ev}} \cdot \mathbf{v}_{j}^{\mathrm{ev}}  / \tau)}.
\end{align}



\subsection{Explicit Stage: Unified Grounding}
By learning modality-alignment in the implicit stage, we obtain refined grounding pseudo-labels extended from annotated partial labels using the event detector (in Eq.~(\ref{eq:instance})). However, partial labels are unavailable during inference, making the event detector not work. Besides, lacking explicit grounding optimization causes somehow noise in pseudo-labels. 

To tackle these issues, we bridge another \textbf{explicit stage} after the implicit stage, by employing one off-the-shelf model from the fully-supervised research line. Specifically, we first treat the grounding pseudo-labels $[\widehat{s}, \, \widehat{e}]$ from the implicit stage as rough ground-truth for all samples in the training set, then optimize this fully-supervised model with explicit grounding objectives. 
At test time, we could solely utilize the fully-supervised model for direct inference. Compared with existing works~\cite{b18,b11} that require frame-by-frame matching during inference time, our framework directly outputs event timestamps, which is straightforward and easy to apply.

Although the contrast-unity framework is simple, the insight contained is non-trivial. \textit{Structurally}, this framework unites full-partial supervisions, enabling the partial setting to enjoy the superior grounding bonus from the existing fully-supervised methods. 
\textit{Functionally}, this framework bypasses the labeling gap between training and inference for partial supervision, by only utilizing the fully-supervised model for efficient inference. More importantly, our framework is flexible to handle supervisions ranging from single-frame to full annotation, enabling to jointly learn from wider data.


\subsection{Training and Inference}
\noindent \textbf{The implicit stage.} Partial labels provide limited yet precise supervision for the location of event mask $\mathbf{m}$ generated by predicted $[\widehat{s}, \, \widehat{e}]$, that is, the annotated short-clip is required to be included in the predicted event interval: 
\begin{equation}   \label{eq:partial}
{\mathcal{L}_{\mathrm{grnd}}=\mathrm{max}(t^e-\widehat{e}, \ \ \widehat{s}-t^s, \ 0),
}
\end{equation}
Above all, the implicit stage is optimized by balancing the contrastive loss $\mathcal{L}_{\mathrm{cont}}$ and grounding loss $\mathcal{L}_{\mathrm{grnd}}$ with ratio $\gamma$. 
We obtain acceptable $[\widehat{s}, \, \widehat{e}]$ grounding pseudo-labels this way.


\noindent \textbf{The explicit stage} treats pseudo-labels $[\widehat{s}, \, \widehat{e}]$ as the ground-truth labels to optimize another fully-supervised model~\cite{b19,b20,b21,b22,b23} (off-the-shelf), for grounding refinement. 
For efficient inference, we directly apply the fully-supervised model.



\section{Experiments and Results}
\input{tab/SOTA.tex}
\subsection{Datasets and Evaluation Metrics}
\noindent \textbf{Datasets.} We use the most common benchmarks in the TSG task: Charades-STA, ActivityNet Captions and Charades-CD.

\noindent \textbf{Evaluation Metrics.} Following the existing works~\cite{b11,b12}, we evaluate through ``R@K, IoU=M'', \textit{i.e.}, the percentage of predicted moments with Intersection over Union (IoU) greater than M in the top-K recall. To simplify the notation, we note ``R@1, IoU=M'' as ``R@M'' in the following sections.
\input{tab/Ablation.tex}
\input{tab/single-frame.tex}
\input{tab/Fullsupervision.tex}
\input{tab/Sentence.tex}


\subsection{Comparison with State-of-the-art}

\noindent \textbf{Single-frame supervision.}
Table~\ref{tab:SOTA} demonstrates comparisons across multiple IoU thresholds on both datasets. For the sake of fairness, we use the identical single-frame annotations following~\cite{b11}.
Our framework achieves new state-of-the-art under all IoU regimes, by a large margin. For example, 7.74\% mIoU gains over the previous SOTA on Charades-STA. Moreover, our method gains more on the rigorous evaluation than loose regimes, \textit{e.g.}, 3.88\% gains for R@0.3 \textit{vs.} 12.42\% gains for R@0.7, comparing to ViGA~\cite{b11}. Despite being single-frame supervision, our method is even comparable with some earlier fully-supervised methods~\cite{b25,b26}. We also offer results on Charades-CD dataset to test OOD data generalization ability, which also surpass existing methods by a large margin.

Note that, ActivityNet Captions is challenging even for fully-supervised methods, with only 1-7\% gaps between full-weak supervisions. Such small gaps somehow limit our potentiality. Still, we achieve the state-of-the-art performance on all regimes. With more advanced fully-supervised methods becoming available, our results can be further improved.


\noindent \textbf{Short-clip supervision.}
Table~\ref{tab:SOTA} experiments on short-clip of 2/4 seconds. A steady improvement could be witnessed with longer annotation intervals, further narrowing the PTSG-FTSG performance gap: only 1.5\% gap over FTSG SOTA for 4s.


\subsection{Ablation Study \& Discussion}
We conduct thorough ablations to dissect all key components, using single-frame annotations on Charades-STA.


\noindent \textbf{Framework Robustness.} 
Partial labels possess a high degree of freedom in event intervals, bringing great challenges to framework robustness. Table~\ref{tab:singleframe} simulates pseudo-label quality in implicit stage with multiple annotation samplings for different distributions/durations. Our framework shows consistent effectiveness to various annotations, proving strong robustness.


\noindent \textbf{Contribution of Quadruple Contrasts.}
To achieve great representations, we design quadruple contrasts: for intra-sample, uni-modal loss $\mathcal{L}_{\mathrm{raun}}$ and multi-modal loss $\mathcal{L}_{\mathrm{raml}}$; for inter-sample, uni-modal loss $\mathcal{L}_{\mathrm{erun}}$ and multi-modal loss $\mathcal{L}_{\mathrm{erml}}$.
In Table~\ref{tab:ablation}, the single $\mathcal{L}_{\mathrm{raml}}$ (A1) causes poor pseudo-labels for the partial branch. A2 adds $\mathcal{L}_{\mathrm{raun}}$ to encourage event-back separation, thus obtaining clear improvements.
Happily, inter-sample modeling brings immediate gains. By introducing $\mathcal{L}_{\mathrm{erml}}$ to A1, A3 gets more than 13\% mIoU gains; by adding $\mathcal{L}_{\mathrm{erun}}$ to A3, A5 further gets 1.1\% mIoU gains, showing the advantages of sample relationship modeling. In conclusion, all losses are essential and jointly contribute to the best results.


\noindent \textbf{Effectiveness of Representation Learning.}
% Comparing to the contrasts between (video, query) pairs or (short-clip, query) pairs, we propose (event, query) aligned pairs. Table~\ref{tab:representation} evaluates the effect of these representations. As is evident, The event-query representation outperforms the other two by a large margin, obtaining 4.4\% mIoU gains over the video-query representation. Since it only considers partial video-clips as positives for queries, ``short-clip-query'' wrongly inhibits the completeness of grounding, resulting in bad performance.
We propose (event, query) aligned pairs over (video, query) or (short-clip, query) pairs. As shown in Table~\ref{tab:representation}, event-query representation achieves a 4.4\% mIoU gain over video-query. The short-clip-query approach, limited to partial clips, hinders grounding completeness and underperforms.


\noindent \textbf{Effectiveness of Inter-Sample Contrasts.}
\hspace{0.1cm} To obtain sample relationships by clustering, Table~\ref{tab:representation} uses a baseline: randomly augment video as positive. Comparing B3 to B1, we find query-based semantic consistency effective for videos and hard sample mining more efficient than simple augmentation. Additionally, B3 vs. B2 highlights the effectiveness of inter-video negative samples during training.



\noindent \textbf{Generalization of The Explicit Stage.}
Our method bridges the gap between PTSG and FTSG, thus can process data from various supervisions. And Table~\ref{tab:fullsuoervision} evaluates its generalization, by employing three typical fully-supervised methods (IA-Net~\cite{b19}, TMLGA~\cite{b20}, SDN~\cite{b21}, BM-DETR~\cite{b22}) in the explicit stage. Despite varying annotations, our PTSG method performs comparably to fully-supervised approaches. Future advancements promise further improvement.



\section{Conclusion}
We propose partial supervision for TSG to balance performance and annotation effort. Our novel contrast-unity framework employs a two-stage approach: implicit-explicit progressive grounding. In the implicit stage, quadruple contrastive learning aligns event-query representations, generating high-quality pseudo-labels. These pseudo-labels are then used in the explicit stage to train a fully-supervised model for refined grounding. Experiments demonstrate our framework's superior performance.

\section{Acknowledgements}
This work is supported by STCSM (No. 22511106101), 111 plan (No. BP0719010), and State Key Laboratory of UHD Video and Audio Production and Presentation.

\begin{thebibliography}{00}
\bibitem{b1} Liu, Jinxiang, et al. ``Exploiting transformation invariance and equivariance for self-supervised sound localisation.'' Proceedings of the 30th ACM International Conference on Multimedia. 2022.
\bibitem{b2} Shu, Tianmin, et al. ``Joint inference of groups, events and human roles in aerial videos.'' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2015.
\bibitem{b3} Wang, Haicheng, et al. ``Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training.'' arXiv preprint arXiv:2412.00440. 2024. 
\bibitem{b4} Zhao, Peisen, et al. ``Bottom-up temporal action localization with mutual regularization.'' Proceedings of the European Conference on Computer Vision. 2020.
\bibitem{b5} Anne Hendricks, Lisa, et al. ``Localizing moments in video with natural language.'' Proceedings of the International Conference on Computer Vision. 2017.
\bibitem{b6} Mithun, Niluthpol Chowdhury, Sujoy Paul, and Amit K. Roy-Chowdhury. ``Weakly supervised video moment retrieval from text queries.'' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
\bibitem{b7} Lin, Zhijie, et al. ``Weakly-supervised video moment retrieval via semantic completion network.'' Proceedings of the AAAI Conference on Artificial Intelligence. 2020.
\bibitem{b8} Carbonneau, Marc-André, et al. ``Multiple instance learning: A survey of problem characteristics and applications.'' Pattern Recognition 77 (2018): 329-353.
\bibitem{b9} Li, Kun, Dan Guo, and Meng Wang. ``Proposal-free video grounding with contextual pyramid network.'' Proceedings of the AAAI Conference on Artificial Intelligence. 2021.
\bibitem{b10} Li, Juncheng, et al. ``Compositional temporal grounding with structured variational cross-graph correspondence learning.'' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.
\bibitem{b11} Cui, Ran, et al. ``Video moment retrieval from text queries via single frame annotation.'' Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2022.
\bibitem{b12} Zheng, Minghang, et al. ``Weakly supervised temporal sentence grounding with gaussian-based contrastive proposal learning.'' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.
\bibitem{b13} Tran, Du, et al. ``Learning spatiotemporal features with 3d convolutional networks.'' Proceedings of the International Conference on Computer Vision. 2015.
\bibitem{b14} Carreira, Joao, and Andrew Zisserman. ``Quo vadis, action recognition? a new model and the kinetics dataset.'' proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.
\bibitem{b15} Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. ``Glove: Global vectors for word representation.'' Proceedings of the Conference on Empirical Methods in Natural Language Processinng. 2014.
\bibitem{b16} Moltisanti, Davide, Sanja Fidler, and Dima Damen. ``Action recognition from single timestamp supervision in untrimmed videos.'' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
\bibitem{b17} Reimers, Nils, and Iryna Gurevych. ``Sentence-bert: Sentence embeddings using siamese bert-networks.'' arXiv preprint arXiv:1908.10084. 2019.
\bibitem{b18} Li, Hanjun, et al. ``D3g: Exploring gaussian prior for temporal sentence grounding with glance annotation.'' Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.
\bibitem{b19} Liu, Daizong, et al. ``Progressively guide to attend: An iterative alignment framework for temporal sentence grounding.'' arXiv preprint arXiv:2109.06400. 2021.
\bibitem{b20} Rodriguez, Cristian, et al. ``Proposal-free temporal moment localization of a natural-language query in video using guided attention.'' Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2020.
\bibitem{b21} Jiang, Xun, et al. ``Sdn: Semantic decoupling network for temporal language grounding.'' IEEE Transactions on Neural Networks and Learning Systems. 2022.
\bibitem{b22} Jung, Minjoon, et al. ``Overcoming Weak Visual-Textual Alignment for Video Moment Retrieval.'' arXiv preprint arXiv:2306.02728. 2023.
\bibitem{b23} Li, Hongxiang, et al. ``G2l: Semantically aligned and uniform video grounding via geodesic and game theory.'' Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.
\bibitem{b24} Zheng, Minghang, et al. ``Weakly supervised video moment localization with contrastive negative sample mining.'' Proceedings of the AAAI Conference on Artificial Intelligence. 2022.
\bibitem{b25} Zhang, Hao, et al. ``Span-based localizing network for natural language video localization.'' arXiv preprint arXiv:2004.13931. 2020.
\bibitem{b26} Mun, Jonghwan, Minsu Cho, and Bohyung Han. ``Local-global video-text interactions for temporal grounding.'' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
\bibitem{b27} Liu, Daizong, et al. ``Context-aware biaffine localizing network for temporal sentence grounding.'' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
\bibitem{b28} Liu, Daizong, Xiaoye Qu, and Wei Hu. ``Reducing the vision and language bias for temporal sentence grounding.'' Proceedings of the 30th ACM International Conference on Multimedia. 2022.
\bibitem{b29} Xu, Zhe, et al. ``Point-supervised video temporal grounding.'' IEEE Transactions on Multimedia 25 (2022): 6121-6131.
\bibitem{b30} Yang, Wenfei, et al. ``Local correspondence network for weakly supervised temporal sentence grounding.'' IEEE Transactions on Image Processing 30 (2021): 3252-3262.
\bibitem{b31} Wang, Zheng, Jingjing Chen, and Yu-Gang Jiang. ``Visual co-occurrence alignment learning for weakly-supervised video moment retrieval.'' Proceedings of the 29th ACM International Conference on Multimedia. 2021.
\bibitem{b32} Huang, Jiabo, et al. ``Cross-sentence temporal and semantic relations in video activity localisation.'' Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.
\bibitem{b33} Yoon, Sunjae, et al. ``Scanet: Scene complexity aware network for weakly-supervised video moment retrieval.'' Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.
\bibitem{b34} Huang, Yifei, et al. ``Weakly supervised temporal sentence grounding with uncertainty-guided self-training.'' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2023.
\bibitem{b35} Ju, Chen, et al. ``Prompting visual-language models for efficient video understanding.'' Proceedings of the European Conference on Computer Vision. 2022.
\bibitem{b36} Ju, Chen, et al. ``Divide and conquer for single-frame temporal action localization.'' Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.
\bibitem{b37} Ju, Chen, et al. ``Distilling vision-language pre-training to collaborate with weakly-supervised temporal action localization.'' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
\bibitem{b38} Ju, Chen, et al. ``Adaptive mutual supervision for weakly-supervised temporal action localization.'' IEEE Transactions on Multimedia (2022): 6688--6701.
\bibitem{b39} Wang, Haicheng, et al. ``Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training.'' arXiv preprint arXiv:2412.00440. 2024.
\bibitem{b40} Ju, Chen, et al. ``Turbo: Informativity-driven acceleration plug-in for vision-language large models.'' Proceedings of the European Conference on Computer Vision. 2025.
\bibitem{b41} Cheng, Haozhe, et al. ``DENOISER: Rethinking the Robustness for Open-Vocabulary Action Recognition.'' arXiv preprint arXiv:2404.14890. 2024.
\bibitem{b42} Liu, Jinxiang, et al. ``Audio-Visual Segmentation via Unlabeled Frame Exploitation.'' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.
\bibitem{b43} Liu, Jinxiang, et al. ``Annotation-free audio-visual segmentation.'' Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2024.
\bibitem{b44} Yao, Ting, Tao Mei, and Yong Rui. ``Highlight detection with pairwise deep ranking for first-person video summarization.'' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2016.
\bibitem{b45} Ju, Chen, et al. ``Point-level temporal action localization: Bridging fully-supervised proposals to weakly-supervised losses.'' arXiv preprint arXiv:2012.08236. 2020.
\bibitem{b46} Ju, Chen, et al. ``Multi-modal prompting for low-shot temporal action localization.'' arXiv preprint arXiv:2303.11732. 2023.
\bibitem{b47} Ju, Chen, et al. ``Turbo: Informativity-driven acceleration plug-in for vision-language models.'' arXiv preprint arXiv:2312.07408. 2023. 
\end{thebibliography}



% \begin{thebibliography}{00}
% \bibitem{b1} Yao, Ting, Tao Mei, and Yong Rui. "Highlight detection with pairwise deep ranking for first-person video summarization." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
% \bibitem{b2} Shu, Tianmin, et al. "Joint inference of groups, events and human roles in aerial videos." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.
% \bibitem{b3} Zhao, Peisen, et al. "Bottom-up temporal action localization with mutual regularization." Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VIII 16. Springer International Publishing, 2020.
% \bibitem{b4} Gao, Jiyang, et al. "Tall: Temporal activity localization via language query." Proceedings of the IEEE international conference on computer vision. 2017.
% \bibitem{b5} Anne Hendricks, Lisa, et al. "Localizing moments in video with natural language." Proceedings of the IEEE international conference on computer vision. 2017.
% \bibitem{b6} Mithun, Niluthpol Chowdhury, Sujoy Paul, and Amit K. Roy-Chowdhury. "Weakly supervised video moment retrieval from text queries." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
% \bibitem{b7} Lin, Zhijie, et al. "Weakly-supervised video moment retrieval via semantic completion network." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 07. 2020.
% \bibitem{b8} Carbonneau, Marc-André, et al. "Multiple instance learning: A survey of problem characteristics and applications." Pattern Recognition 77 (2018): 329-353.
% \bibitem{b9} Li, Kun, Dan Guo, and Meng Wang. "Proposal-free video grounding with contextual pyramid network." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 3. 2021.
% \bibitem{b10} Li, Juncheng, et al. "Compositional temporal grounding with structured variational cross-graph correspondence learning." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.
% \bibitem{b11} Cui, Ran, et al. "Video moment retrieval from text queries via single frame annotation." Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2022.
% \bibitem{b12} Zheng, Minghang, et al. "Weakly supervised temporal sentence grounding with gaussian-based contrastive proposal learning." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.
% \bibitem{b13} Tran, Du, et al. "Learning spatiotemporal features with 3d convolutional networks." Proceedings of the IEEE international conference on computer vision. 2015.
% \bibitem{b14} Carreira, Joao, and Andrew Zisserman. "Quo vadis, action recognition? a new model and the kinetics dataset." proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.
% \bibitem{b15} Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. "Glove: Global vectors for word representation." Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014.
% \bibitem{b16} Moltisanti, Davide, Sanja Fidler, and Dima Damen. "Action recognition from single timestamp supervision in untrimmed videos." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
% \bibitem{b17} Reimers, Nils, and Iryna Gurevych. "Sentence-bert: Sentence embeddings using siamese bert-networks." arXiv preprint arXiv:1908.10084 (2019).
% \bibitem{b18} Li, Hanjun, et al. "D3g: Exploring gaussian prior for temporal sentence grounding with glance annotation." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.
% \bibitem{b19} Liu, Daizong, Xiaoye Qu, and Pan Zhou. "Progressively guide to attend: An iterative alignment framework for temporal sentence grounding." arXiv preprint arXiv:2109.06400 (2021).
% \bibitem{b20} Rodriguez, Cristian, et al. "Proposal-free temporal moment localization of a natural-language query in video using guided attention." Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2020.
% \bibitem{b21} Jiang, Xun, et al. "Sdn: Semantic decoupling network for temporal language grounding." IEEE Transactions on Neural Networks and Learning Systems (2022).
% \bibitem{b22} Jung, Minjoon, et al. "Overcoming Weak Visual-Textual Alignment for Video Moment Retrieval." arXiv preprint arXiv:2306.02728 (2023).
% \bibitem{b23} Li, Hongxiang, et al. "G2l: Semantically aligned and uniform video grounding via geodesic and game theory." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.
% \bibitem{b24} Zheng, Minghang, et al. "Weakly supervised video moment localization with contrastive negative sample mining." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 3. 2022.
% \bibitem{b25} Zhang, Hao, et al. "Span-based localizing network for natural language video localization." arXiv preprint arXiv:2004.13931 (2020).
% \bibitem{b26} Mun, Jonghwan, Minsu Cho, and Bohyung Han. "Local-global video-text interactions for temporal grounding." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
% \bibitem{b27} Liu, Daizong, et al. "Context-aware biaffine localizing network for temporal sentence grounding." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
% \bibitem{b28} Liu, Daizong, Xiaoye Qu, and Wei Hu. "Reducing the vision and language bias for temporal sentence grounding." Proceedings of the 30th ACM International Conference on Multimedia. 2022.
% \bibitem{b29} Xu, Zhe, et al. "Point-supervised video temporal grounding." IEEE Transactions on Multimedia 25 (2022): 6121-6131.
% \bibitem{b30} Yang, Wenfei, et al. "Local correspondence network for weakly supervised temporal sentence grounding." IEEE Transactions on Image Processing 30 (2021): 3252-3262.
% \bibitem{b31} Wang, Zheng, Jingjing Chen, and Yu-Gang Jiang. "Visual co-occurrence alignment learning for weakly-supervised video moment retrieval." Proceedings of the 29th ACM International Conference on Multimedia. 2021.
% \bibitem{b32} Huang, Jiabo, et al. "Cross-sentence temporal and semantic relations in video activity localisation." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.
% \bibitem{b33} Yoon, Sunjae, et al. "Scanet: Scene complexity aware network for weakly-supervised video moment retrieval." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.
% \bibitem{b34} Huang, Yifei, Lijin Yang, and Yoichi Sato. "Weakly supervised temporal sentence grounding with uncertainty-guided self-training." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.
% \bibitem{b35} Ju, Chen, et al. "Prompting visual-language models for efficient video understanding." Proceedings of the European Conference on Computer Vision. 2022.
% \bibitem{b36} Ju, Chen, et al. "Divide and conquer for single-frame temporal action localization." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.
% \bibitem{b37} Ju, Chen, et al. "Distilling vision-language pre-training to collaborate with weakly-supervised temporal action localization." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
% \bibitem{b38} Ju, Chen, et al. "Adaptive mutual supervision for weakly-supervised temporal action localization." IEEE Transactions on Multimedia 25 (2022): 6688--6701.
% \end{thebibliography}

\end{document}
