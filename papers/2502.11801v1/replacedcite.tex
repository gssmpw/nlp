\section{Related Works}
\label{sec:related}

\input{main_figure}

% \subsection{2D Image Inpainting}
% % Early works____ on 2D inpainting primarily use adversarial training techniques for Generative Adversarial Network (GAN)-based image generators, treating the image and inpainting mask as conditions for the GAN. One notable example is LAMA____, which employs Fourier convolutions as backbone modules to inpaint large missing areas while preserving high-frequency details and image quality. Although LAMA is more capable of handling inpainting scenarios involving large missing regions compared to other GAN-based approaches, it may generate unrealistic results if the original image is overly complex or if the masked area overlaps the boundaries of multiple objects.

% Recently, the introduction of Diffusion Models____ significantly expands the possibilities for realistic image generation. Although not initially designed for 2D inpainting, these models can be easily adapted for this task by using the masked image as an intermediate input in the denoising process. However, general diffusion models often struggle to preserve the details of the background (regions outside the inpainting mask) and lack control over the generated content inside the mask. To address these limitations, recent works____ adapt diffusion models to use inpainting masks as additional guidance rather than simply masking the input image. For example, SmartBrush____ fine-tunes a Latent Diffusion Model (LDM)____ by treating the inpaint mask as shape guidance and adding noise only within the inpainting region during the denoising process, thus preserving the background after the inpainting is complete. While diffusion-based methods generate realistic 2D inpainting results, their application to 3D inpainting remains challenging, as it requires both realistic and multi-view consistent outputs, which is difficult due to the modality gap between 2D and 3D data. Thus, lifting 2D inpainting approaches to produce multi-view consistent 3D inpainting results remains an open problem.

\subsection{3D Representations for Novel View Synthesis}
\label{subsec:NVS}
Novel view synthesis is a widely studied topic in 3D computer vision. Neural Radiance Field (NeRF)____, a pioneer in this field, effectively models scenes using multi-view images. However, as noted in____, the original NeRF requires extensive training time—from hours to days—and relies on numerous images. To address these issues, many subsequent works____ have emerged. Methods like Instant NGP____ and DVGO____ reduce training time to minutes by balancing speed and memory through hash encoding and voxel encoding. Recently, the introduction of 3D Gaussian Splatting (3DGS)____ brings a fundamental revolution to this area. Different from NeRF and its variants, which model a 3D scene as an implicit representation, 3DGS models a 3D scene as a composition of numerous 3D Gaussians, with each Gaussian parameterized by its three-dimensional centroid, standard deviations, orientations, opacity, and color features. By modeling a 3D scene as such an explicit representation, one is able to render the 2D images of the modeled scene via rasterization with an incredible 100 fps, whereas the fastest NeRF-based approach (____) only achieves around 10 fps. As a result, we chose 3DGS as our backbone representation over NeRF in this paper due to its fast rendering property, making our approach more applicable in the real world. 
 


\subsection{3D Scene Inpainting}
\label{subsec:3Dinpaint}
In the context of 3D scene inpainting, SPIn-NeRF____ emerges as one of the earliest approaches addressing the challenges of multi-view consistency. It uses pre-trained segmentation networks to generate plausible inpainting masks for multi-view images, requiring sparse user annotations to indicate the unwanted object. These annotations are propagated across views, and a modified Neural Radiance Field (NeRF) model is used to inpaint the masked regions. Although effective, this approach is heavily dependent on human intervention and lacks the ability to automate the mask generation process, thus limiting its scalability.



To reduce the need for manual annotations, recent works____ have introduced the use of the Segment Anything Model (SAM)____ in combination with NeRF or 3DGS. Specifically, OR-NeRF employs Grounded-SAM____ to locate a single-view 2D inpainting mask for the object to be removed. It then projects 3D points of the object's surface into other views, which are used as prompts for SAM to generate masks for the remaining views. 
Similarly, Gaussian Grouping____ enhances 3DGS by incorporating semantic feature learning, allowing the model to jointly render RGB images and segmentation maps, where the segmentation supervision is derived from SAM. 
While these methods significantly reduce the burden of manual mask creation, they inpaint 2D images of different views separately and optimize the inpainted NeRF by treating all the 2D inpaintings equally. As a result, the above approaches still face difficulties in producing consistent multi-view results, as mentioned in____



To alleviate this problem, more advanced approaches____ focus on improving cross-view consistency. For instance, MALD-NeRF fine-tunes a scene-specific Low-Rank Adaptation (LoRA)____ module for a pre-trained diffusion model to inpaint images of each scene. By introducing a LoRA module for each scene, the diffusion model can inpaint more consistent content across different views. GScream____, on the other hand, applies diffusion-based 2D inpainting on a chosen reference view. By predicting the depth map of the inpainted reference view, GScream incorporates cross-view feature consistency between any other view and the reference view, optimizing geometric alignment across views. These methods represent a significant step forward in achieving automatic, consistent 3D inpainting, addressing the practical limitations of earlier approaches. Nonetheless, the aforementioned methods rely on per-view 2D inpainting masks for 2D inpainting models as input, while some areas in those masks are visible from other views, as noted in____. Consequently, the inpainted content for these visible areas may not align with the original scene (as illustrated in the red branch in Figure 1). This inconsistency might be propagated to the inpainted 3D scene, hindering the reliability of their results.