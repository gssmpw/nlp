\section{Introduction}
\label{sec:intro}
% Novel view synthesis for 3D scenes plays a vital role in 3D reconstruction and scene understanding. 
% Recent advancements in 3D scene representations, such as Neural Radiance Fields (NeRF)~\cite{mildenhall2021nerf, barron2021mip, yu2021plenoctrees, muller2022instant, sun2022direct, fridovich2022plenoxels, martin2021nerf, reiser2021kilonerf, chen2022tensorf} and 3D Gaussian Splatting (3DGS)~\cite{yu2024mipsplat, qin2024langsplat, ye2023gaussiangrouping, kerbl202333dgs, chen2024surveygs}, make it possible to generate high-fidelity novel views by modeling the volumetric properties of a scene. However, practical applications in VR/AR~\cite{macedo2021ar1, broll2022augmentedar2} often require further editing capabilities for these 3D representations, which are not originally addressed in these methods. Among various types of scene editing, object removal (and inpainting)~\cite{wang2024gscream, lin2024maldnerf} are challenging objectives, as the direct removal of objects from reconstructed 3D representations results in visible holes that compromise the visual quality of the synthesized outputs.  
% Although applying 2D inpainting techniques~\cite{lama, corneanu2024latentpaint, lugmayr2022repaint, podell2023sdxl, xie2023smartbrush, yang2023paintbyexample, ldm, yang2023unipaint} to multiple rendered views and subsequently re-optimizing the 3D representation is possible, maintaining consistency across different views remains challenging, often leading to artifacts and reduced fidelity. As a result, achieving seamless, multi-view-consistent inpainting for 3D scenes continues to be an open challenge. 


Novel view synthesis for 3D scenes plays a vital role in 3D reconstruction and scene understanding. Recent advancements, such as Neural Radiance Fields (NeRF)~\cite{mildenhall2021nerforiginal, barron2021mip, yu2021plenoctrees, muller2022instant, sun2022direct, fridovich2022plenoxels, martin2021nerf, reiser2021kilonerf, chen2022tensorf} and 3D Gaussian Splatting (3DGS)~\cite{yu2024mipsplat, qin2024langsplat, ye2023gaussiangrouping, kerbl202333dgs, chen2024surveygs}, enable high-fidelity novel views by modeling volumetric properties. However, practical VR/AR applications~\cite{macedo2021ar1, broll2022augmentedar2} require more than reconstruction: they need editing capabilities that these methods do not fully address. Among editing challenges, object removal and inpainting~\cite{wang2024gscream, lin2024maldnerf} are particularly difficult, as direct removal creates visible holes, compromising visual quality. While 2D inpainting~\cite{lama, corneanu2024latentpaint, lugmayr2022repaint, podell2023sdxl, xie2023smartbrush, yang2023paintbyexample, ldm, yang2023unipaint} across multiple views is possible, maintaining consistency remains problematic, leading to artifacts and reduced fidelity. Thus, achieving seamless, multi-view-consistent inpainting for 3D scenes is still an open challenge.


As a pioneering work in 3D scene inpainting, SPIn-NeRF~\cite{mirzaei2023spin} proposes to use a pre-trained segmentation network~\cite{hao2021edgeflow} to generate plausible 2D inpaint masks for multi-view images with sparse human annotations of the object to be removed. However, as noted in subsequent research~\cite {lin2024maldnerf, wang2024gscream}, SPIn-NeRF and similar approaches~\cite{weder2023removingnerf, yin2023ornerf} rely heavily on 2D inpainting of multiple views separately, which hinders the cross-view consistency of the 3D inpainting results. To ensure cross-view consistency, RefNeRF~\cite{mirzaei2023referenceinpaint}
projects the inpainted image from a specific reference view onto other views using depth-guided projection, thereby ensuring more consistent inpainting results across views. Despite these advancements, these methods still require human-annotated 2D masks or sparse annotations to delineate the objects to be removed and the regions to be inpainted, making the process labor-intensive and limiting the scalability and practicality of these techniques.

To reduce the need for human annotation for obtaining inpainting masks, recent methods~\cite{ye2023gaussiangrouping, yin2023ornerf} tend to leverage the Segment Anything Model (SAM)~\cite{kirillov2023sam} 
models with NeRF or 3DGS to obtain 2D inpainting masks for multi-view images directly. Although these methods ease the requirement of human annotations for inpainting masks, they still rely on 2D inpainting results for different views as supervision, limiting the multi-view consistency of the inpainted 3D representations. To alleviate this limitation, some approaches~\cite{lin2024maldnerf, chen2024mvip, wang2024gscream, mirzaei2024reffusion, liu2024infusion, weber2024nerfiller} attempt to build a cross-view consistent 3D inpainting method on top of the 2D inpainting mask obtained from SAM. By either leveraging 2D diffusion models as perceptual guidance for the inpainted region~\cite{chen2024mvip, lin2024maldnerf, weber2024nerfiller} or ensuring feature consistency of corresponding pixels across different views~\cite{wang2024gscream}, these methods are able to produce more consistent 3D inpainting results without the requirement of human-annotated 2D inpainting masks. Nevertheless, most of the aforementioned methods rely on the provided per-scene 2D inpainting masks (either from human annotation or from SAM) for each view, which can include areas visible in other views, as mentioned in~\cite{ye2023gaussiangrouping}. As a result, the inpainted content within this area might be inconsistent across camera views, producing artifacts in the reconstructed 3D scene. 
% \input{figures/mask_diff}

In this paper, we propose a 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency (3DGIC) to optimize the 3DGS model while achieving multi-view consistent and high-fidelity 3D inpainting with depth-guided inpainting masks to locate the inpainting region. Given a set of images of a scene with corresponding camera views and the object masks indicating an unwanted object in the scene (obtained from SAM~\cite{kirillov2023sam}, for example), our 3DGIC conducts the process of \textit{Inferring Depth-Guided Inpainting Masks} to consider depth information from all training views and refine the inpainting mask by discovering background pixels from different views. The refined inpainting masks are then used to provide a joint update of inpainting results and the underlying 3DGS model via \textit{3D Inpainting with Cross-View Consistency}. 
% address two important aspects of the 3D inpainting task: \frank{1) achieving multi-view consistency on the texture of the inpainted areas and 2) preserving all the background contents that are originally visible from other views. Given a set of images of a scene with corresponding camera views and the object masks indicating an unwanted object in the scene (obtained from SAM~\cite{kirillov2023sam}, for example), our 3DGIC optimizes the 3D Gaussian Splatting (3DGS) model, which removes the undesirable object while achieving cross-view consistency. Our 3DGIC is composed of two key stages: \textit{Inferring Depth-Guided Inpainting Masks} and \textit{3D Inpainting with Cross-View Consistency}. The former generates proper inpainting masks for indicating only the region that is \textit{not} visible from any other views, while the latter optimizes the 3DGS model to achieve cross-view consistency.} 
Through experiments on real-world datasets, we quantitatively and qualitatively demonstrate that our 3DGIC performs favorably against state-of-the-art NeRF/3DGS-based inpainting methods by achieving better fidelity and multi-view consistency.
% \frank{
% - Joint 3DGS update/refine  and inpaint
% - Rendered depth maps as guidance: mask and 3DGS}


The key contributions of our approach are as follows:
\begin{itemize}
\item We propose a 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency (3DGIC), achieving multi-view consistent 3D inpainting results with high fidelity.
\item By inferring Depth-Guided Inpainting Masks, the region to be inpainted is properly obtained by considering depth information across different views, allowing us to guide the inpainting process for 3DGS.
\item Based on the 2D inpaintings from a chosen reference view, our Inpainting-guided 3DGS Refinement optimizes new Gaussians of the object-removed scene by ensuring cross-view consistent inpainting results.
\end{itemize}



% To reduce the need for human annotation for obtaining inpainting mask, recent methods~\cite{ye2023gaussiangrouping, yin2023ornerf} tend to leverage the Segment Anything Model (SAM)~\cite{kirillov2023sam} models with NeRF or 3DGS to obtain 2D inpainting mask for multi-view images directly. For instance, OR-NeRF~\cite{yin2023ornerf} utilizes Grounded-SAM to locate a single-view 2D inpainting mask for the object to be removed and then projects 3D points inside the mask into other views as input point-level prompt for SAM to obtain inpainting masks for the rest of the views. On the other hand, Gaussian Grouping~\cite{ye2023gaussiangrouping} extends 3DGS by learning semantic features to jointly render novel-view RGB images and semantic segmentation maps, where the semantic map supervisions are directly predicted from SAM. while MALD-NeRF~\cite{lin2024maldnerf} leverage techniques such as LoRA~\cite{hu2021lora} to fine-tune 2D diffusion-based inpainting models~\cite{ldm} per scene, thereby ensuring more consistent inpainting results across views.

