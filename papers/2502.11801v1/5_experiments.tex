\section{Experiments}
\label{sec:exp}
\input{main_table}
\input{quali_spin}
\input{quali_figurines}
\input{quali_counter}
\input{ablation}


\subsection{Datasets}
%\subsubsection{SPIn-NeRF dataset}
To evaluate the effectiveness of our method, we conduct experiments on the most used real-world benchmark dataset: the SPIn-NeRF~\cite{mirzaei2023spin} dataset. This dataset contains \textit{ten} real-world scenes, including indoor and outdoor scenes. Each scene is composed of 60 frames of training images and 40 frames of testing images where a certain object in the scene is removed, with camera poses of all 100 images available. The binary mask of the object to be removed is also provided in each frame for evaluation. Following the setting of \cite{mirzaei2023spin, lin2024maldnerf, wang2024gscream, ye2023gaussiangrouping, chen2024mvip}, we resize each image as $1008 \times 567$ in resolution for all our experiments and show the comparisons quantitatively and qualitatively. 

%\subsubsection{Additional datasets for visualization}
Since the camera poses in all the scenes provided in the SPIn-NeRF dataset only cover a small range (i.e., all the image frames are captured near the front view of the scene), we additionally include qualitative comparisons with several scenes covering 360$^{\circ}$ of camera poses to show the effectiveness of our design, specifically for our Depth-Guided Inpainting Mask. Following Gaussian Grouping~\cite{ye2023gaussiangrouping}, we take the ``\textit{bear}'' scene provided in InNeRF360~\cite{wang2024innerf360}, the ``\textit{counter}'' scene in Mip-NeRF360~\cite{barron2022mipnerf360}, and the ``\textit{figureines}'' scene in LeRF~\cite{kerr2023lerf} for the additional qualitative evaluations. Since these scenes are not originally for the 3D inpainting task, we manually select an object in each scene as the object to be removed and select the corresponding ID in the segmentation map obtained from SAM~\cite{kirillov2023sam} as the object mask in each view. Please refer to our supplementary material for a detailed description of these scenes.


\subsection{Quantitative Evaluations}
\label{sec:quant}
Table~\ref{tab:main_table} shows the comparisons between our 3DGIC (with LAMA~\cite{lama} or LDM~\cite{ldm} as 2D inpainter) and several state-of-the-art approaches such as SPIn-NeRF~\cite{mirzaei2023spin}, MVIP-NeRF~\cite{chen2024mvip}, Gaussian Grouping~\cite{ye2023gaussiangrouping}, MALD-NeRF~\cite{lin2024maldnerf}, and GScream~\cite{wang2024gscream} using the SPIn-NeRF dataset. Following SPIn-NeRF and MALD-NeRF, we conduct FID~\cite{heusel2017fid}, masked FID (m-FID), LPIPS~\cite{lpips}, and masked LPIPS (m-LPIPS) as our evaluation matrices, where m-FID and m-LPIPS calculate the FID and LPIPS scores only inside the ground truth inpainting masks. We note that the official implementation of MALD-NeRF is currently unavailable; we directly use the output results provided on their official project page for evaluation. As for other state-of-the-arts, we reproduce results from their official implementations and the released configurations.

From Table~\ref{tab:main_table}, we can see that the LDM version of our 3DGIC achieves the best score on all four evaluation matrices. As for our 3DGIC using a non-diffusion-based model of LAMA as the 2D inpainter, the results still outperform MVIP-NeRF and MALD-NeRF, where both use LDM as the inpainter. The above results show that while using a better 2D inpainter achieves better results, the improvements in our 3DGIC do not come solely from a better 2D inpainter. This suggests that our model is not bundled by 2D inpainters and achieves 3D inpainting with improved fidelity.



\subsection{Qualitative Results}
In Figure~\ref{fig:quali_spin}, we qualitatively compare our 3DGIC with MVIP-NeRF~\cite{chen2024mvip}, MALD-NeRF~\cite{lin2024maldnerf}, and Gscream~\cite{wang2024gscream} using the testing set of SPIn-NeRF dataset. In this figure, each of the two rows shows the results of the same scene with different viewpoints, while the first column shows the images containing the object to be removed along with the object masks at the upper-left corner. Specifically, from the first two rows, we observe that while GScream and MALD-NeRF both show high-fidelity images, some of the visible details from the input image (e.g., the electrical socket on the table) are not preserved properly. For the third and fourth rows, where we zoom in on certain areas inside the red boxes, although it is reasonable for MALD-NeRF to generate a hat in the inpainted region, the logo on the hat is not consistent across different views. As for MVIP-NeRF, blurry images are generated in all cases. Oppositely, our 3DGIC generates high-fidelity images with multi-view consistency and preservation of the visible backgrounds. 

In Figure~\ref{fig:quali_figurines} and Figure~\ref{fig:quali_counter}, we further show the qualitative comparisons with SPIn-NeRF, Gaussian Grouping, and GScream using the \textit{Figurines} dataset from LeRF~\cite{kerr2023lerf} and the \textit{Counter} dataset from MipNeRF360~\cite{barron2022mipnerf360}, where each shows results from three different viewpoints. For Figure~\ref{fig:quali_figurines}, we can see that both SPIn-NeRF and Gaussian Grouping leave obvious black holes and shadows in the inpainting region, while GScream does not clearly remove the object of interest. In contrast, our 3DGIC successfully removes the unwanted object and produces smooth and multi-view consistent results without leaving heavy shadows. For Figure~\ref{fig:quali_counter}, where certain areas are cropped by red boxes and zoomed in in the first row, GSream does not fully remove the object of interest either. SPIn-NeRF not only removes the object of interest but also inpaints other objects in the background. As for Gaussian Grouping, which uses GroundedSAM~\cite{ren2024grounded} to detect inpainting mask with the text prompt ``\textit{blurry hole}'' as input, the GroundedSAM model locates other regions rather than focusing on the object removed region, producing blurry and inconsistent inpainting results across different views. In the contrary, our 3DGIC locates the regions to be inpaint properly and hence produces high-fidelity results while preserving all the other background objects.

\subsection{Ablation Study}
\label{sec:ablation}
To further analyze the effectiveness of our designed modules (i.e., Inferring Depth-Guided Masks and Inpainting-guided 3DGS Refinement), we conduct ablation studies on the ``\textit{bear}'' scene from InNeRF360~\cite{wang2024innerf360}, as shown in Figure~\ref{fig:ablation}. Column (a) shows the input images with the bear statue and their corresponding object mask. The baseline model (b) uses the original object masks as the inpainting masks and directly applies all the inpainted 2D images as input to fine-tune a 3DGS model. The results of model (b) show blurry contents all over the rendered image, while the inpainted results are not consistent across different views.  For model (c), the original object masks are applied as the 2D inpainting model, with our Inpainting-guided 3DGS Refinement. Although the rendered images of model (c) show better fidelity, using the original object masks as inpainting masks results in modifications to the visible backgrounds. For model (d), our inferred depth-guided masks $M'_{1:K}$ are applied as the 2D inpainting masks, but all the 2D inpainting results are directly used as inputs to fine-tune the 3DGS model. As a result, although the backgrounds are preserved, the inpainted region is blurry and not consistent across the views. As for our full model in the last column (e), the depth-guided masks are used, and the 3D Inpainting with Cross-View Consistency is applied, achieving the best results. This verifies the success of our proposed modules and strategies for 3D inpainting.
