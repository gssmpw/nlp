
\section{Brief Review of 3D Gaussian Splatting}
\label{sec:prelim}
For the sake of clarity, we first briefly review 3D Gaussian Splatting (3DGS)~\cite{kerbl202333dgs}, an explicit representation of a 3D scene for providing effective image rendering. 
% We also provide brief reviews of two powerful extensions of 3DGS, Gaussian Grouping~\cite{ye2023gaussiangrouping} and Relightable Gaussian~\cite{gao2023relightable}, which equip 3DGS with segmentation and relighting abilities and are utilized together with 3DGS as the backbone representation in our work. 


Given $K$ multi-view images $I_{1:K} = \{I_1, I_2, ..., I_K\}$ with corresponding camera poses $\xi_{1:K} = \{\xi_1, \xi_2, ..., \xi_K\}$ of a 3D scene, a scene-specific 3DGS is applied to model the scene with $N$ learnable 3D Gaussian ellipsoids (i.e., $G_{1:N} = \{G_1, G_2, ..., G_N \}$). Each Gaussian $G_i$ is parameterized with its 3-dimensional centroid $\mathbf{p}_i \in \mathbb{R}^{3}$, a 3-dimensional standard deviation $\mathbf{s}_i \in \mathbb{R}^{3}$, a 4-dimensional rotational quaternion $\mathbf{q}_i \in \mathbb{R}^{4}$, an opacity ${\alpha}_i \in [0,1]$, and color coefficients $\mathbf{c}_i$ for spherical harmonics in degree of 3. Hence, $G_i$ is represented with a set of the above parameters (i.e., $G_i = \{\mathbf{p}_i, \mathbf{s}_i, \mathbf{q}_i, {\alpha}_i, \mathbf{c}_i\}$). To model the scene with $G_{1:N}$, 2D images $\hat{I_{1:K}} = \{\hat{I_1}, \hat{I_2}, ..., \hat{I_K}\}$ are sequentially rendered from $G_{1:N}$ using $\xi_{1:K} = \{\xi_1, \xi_2, ..., \xi_K\}$ (please refer to~\cite{kerbl202333dgs} for a detailed rendering process), and supervised with $I_{1:K}$ by the rendering loss:
\begin{equation} \label{Limage}
    \mathcal{L}_{image} = \sum_{k\in {1...K}}\lambda\| I_k - {\hat{I_k}}\|_1 + \mathcal{L}_{SSIM}(I_k, \hat{I_k}),
\end{equation}
where $\mathcal{L}_{SSIM}(\cdot)$ represents a SSIM loss and $\lambda$ is a hyper-parameter (set to $0.2$ as mentioned in~\cite{kerbl202333dgs}).



% \subsection{Gaussian Grouping}
% To overcome the lack of fine-grained scene understanding in 3DGS, Gaussian Grouping~\cite{ye2023gaussiangrouping} extends 3DGS by incorporating segmentation capabilities. Along with $I_{1:K}$, Gaussian Grouping additionally takes the Segment Anything Model (SAM) to produce 2D semantic segmentation masks $S_{1:K} = \{S_1, S_2, ..., S_K\}$ from multiple views as inputs, and an additional 16-dimensional parameter $\mathbf{e}_i \in \mathbb{R}^{16}$ is introduced to represent a 3D Identity Encoding for each Gaussian $G_i$. Therefore, each Gaussian $G_i$ is extended as $G_i = \{\mathbf{p}_i, \mathbf{s}_i, \mathbf{q}_i, {\alpha}_i, \mathbf{c}_i, \mathbf{e}_i\}$. To make sure $G_{1:K}$ learns to segment each object represented by $S_{1:K}$ in the scene, a 2D identity loss $\mathcal{L}_{id}$ is applied by calculating cross-entropy between $\hat{S}_{1:K}$ and $S_{1:K}$, where $\hat{S}_{1:K} = \{\hat{S}_1, \hat{S}_2, ... , S_K\}$ denotes the rendered segmentation maps from $G_{1:K}$. Additionally, to further ensure that the Gaussians having the same identities are grouped together, a 3D regularization loss $\mathcal{L}_{3D}$ is applied to enforce each $G_i$'s k-nearest 3D spatial neighbors to be close in their feature distance of Identity Encodings. Please refer to the original paper~\cite{ye2023gaussiangrouping} for detailed formulations of segmentation map rendering and $\mathcal{L}_{3D}$. The design of Gaussian Grouping ensures that the segmentation results are coherent across multiple views, enabling the automatic generation of binary masks for any queried object in the scene.

% \subsection{Relightable Gaussians}
% Different from Gaussian Grouping, Relightable Gaussians~\cite{gao2023relightable} extends the capabilities of Gaussian Splatting by incorporating Disney-BRDF~\cite{burley2012brdf} decomposition and ray tracing to achieve realistic point cloud relighting. 
% % Unlike traditional Gaussian Splatting, which primarily focuses on appearance and geometry modeling, Relightable Gaussians also aim to model the physical interaction of light with different surfaces in the scene.
% Specifically, for each Gaussian $G_i$, the original color coefficients $\mathbf{c}_i$ is decomposed into a 3-dimensional base color $\mathbf{b}_i \in [0,1]^3$, a 1-dimensional roughness $r \in [0,1]$, and incident light coefficients $\mathbf{l}_i$ for spherical harmonics in degree of 3. Subsequently, the Physical-Based Rendering (PBR) process and a point-based ray tracing are applied to obtain the colored 2D images $\hat{I}^{PBR}_{1:K}$ and supervised by $I_{1:K}$ using the aforementioned $\mathcal{L}_{image}$ in Eqn.~\ref{Limage}. Besides the above extensions on PBR for relighting, Relightable Gaussians also introduces a 3-dimensional normal $\mathbf{n}_i$ for $G_i$ and leverages several techniques, including an unsupervised estimation of a depth map $D_i$ from each input view $\xi_i$, to enhance the geometry accuracy and smoothness. Please refer to the original paper of Relightable Gaussians~\cite{gao2023relightable} for detailed explanations.  

\pj{Our pipeline for 3D Inpainting is built on top of the 3DGS model. Additionally, we incorporate the design of Gaussian Grouping~\cite{ye2023gaussiangrouping} to introduce a 16-dimensional semantic feature $\mathbf{e}_i \in \mathbb{R}^{16}$ for each Gaussian $G_i$, so that the 2D segmentation maps of the Gaussians $G_{1:K}$ is rendered and the object mask for the object to be removed can be directly produced, as mentioned in Sect.~\ref{subsec:3Dinpaint}.
By combining these methods as our backbone, we are able to perform an automatic inpainting mask generation and a reliable depth estimation for depth-guided 3D inpainting. Please refer to our Supplementary material for a more detailed explanation of our backbones.}

% the backbone representation by parameterizing each Gaussian $G_i$ as $G_i = \{\mathbf{p}_i, \mathbf{s}_i, \mathbf{q}_i, {\alpha}_i, \mathbf{c}_i, \mathbf{e}_i, \mathbf{b}_i, r,  \mathbf{l}_i, \mathbf{n}_i\}$. By combining these methods, we are able to perform an automatic inpainting mask generation and a reliable depth estimation for depth-guided 3D inpainting.