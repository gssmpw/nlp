\clearpage
\setcounter{page}{1}
\maketitlesupplementary
\appendix

\section{Additional Details of 3DGIC}
\label{sec:details}


\subsection{Details of Backbone 3D Gaussian Splatting Model}
\label{subsec:backbone}
Given the multi-view images $I_{1:K}$ with corresponding camera poses $\xi_{1:K}$ of a 3D scene, the vanilla 3DGS~\cite{kerbl202333dgs} model parameterize each Gaussian $G_i$ in $G_{1:N}$ with its 3-dimensional centroid $\mathbf{p}_i \in \mathbb{R}^{3}$, a 3-dimensional standard deviation $\mathbf{s}_i \in \mathbb{R}^{3}$, a 4-dimensional rotational quaternion $\mathbf{q}_i \in \mathbb{R}^{4}$, an opacity ${\alpha}_i \in [0,1]$, and color coefficients $\mathbf{c}_i$ for spherical harmonics in degree of 3. Hence, $G_i$ is represented with a set of the above parameters (i.e., $G_i = \{\mathbf{p}_i, \mathbf{s}_i, \mathbf{q}_i, {\alpha}_i, \mathbf{c}_i\}$). However, to make sure the 3DGS models in this paper are capable of removing Gaussians corresponding to any indicated object (e.g., ``bear'' in Figure \textcolor{cvprblue}{2}) as described in Sect.~\textcolor{cvprblue}{3.3}, we incorporate the use of a semantic-aware 3DGS (i.e., Gaussian Grouping~\cite{ye2023gaussiangrouping}) approach as the main backbone 3DGS model of our method. Also, since the rendered depth maps $D_{1:K}$ are utilized as important guidance in our 3DGIC, we additionally combine the use of Relightable Gaussian~\cite{gao2023relightable}, which produces better depth estimations from 3DGS model as our final backbone for Sect.~\textcolor{cvprblue}{3}. We now briefly discuss both methods.  

\paragraph{Incorporating Semantic Segmentation via Gaussian Grouping.}
To overcome the lack of fine-grained scene understanding in 3DGS, Gaussian Grouping~\cite{ye2023gaussiangrouping} extends 3DGS by incorporating segmentation capabilities. Along with $I_{1:K}$, Gaussian Grouping additionally takes the Segment Anything Model (SAM) to produce 2D semantic segmentation masks $S_{1:K} = \{S_1, S_2, ..., S_K\}$ from multiple views as inputs, and an additional 16-dimensional parameter $\mathbf{e}_i \in \mathbb{R}^{16}$ is introduced to represent a 3D Identity Encoding for each Gaussian $G_i$. Therefore, each Gaussian $G_i$ is extended as $G_i = \{\mathbf{p}_i, \mathbf{s}_i, \mathbf{q}_i, {\alpha}_i, \mathbf{c}_i, \mathbf{e}_i\}$. To make sure $G_{1:K}$ learns to segment each object represented by $S_{1:K}$ in the scene, a 2D identity loss $\mathcal{L}_{id}$ is applied by calculating cross-entropy between $\hat{S}_{1:K}$ and $S_{1:K}$, where $\hat{S}_{1:K} = \{\hat{S}_1, \hat{S}_2, ... , S_K\}$ denotes the rendered segmentation maps from $G_{1:K}$. Additionally, to further ensure that the Gaussians having the same identities are grouped together, a 3D regularization loss $\mathcal{L}_{3D}$ is applied to enforce each $G_i$'s k-nearest 3D spatial neighbors to be close in their feature distance of Identity Encodings. Please refer to the original paper~\cite{ye2023gaussiangrouping} for detailed formulations of segmentation map rendering and $\mathcal{L}_{3D}$. The design of Gaussian Grouping ensures that the segmentation results are coherent across multiple views, enabling the automatic generation of binary masks for any queried object in the scene.

\paragraph{Produce Reliable Depth Estimations with Relightable Gaussians.}
Different from Gaussian Grouping, Relightable Gaussians~\cite{gao2023relightable} extends the capabilities of Gaussian Splatting by incorporating Disney-BRDF~\cite{burley2012brdf} decomposition and ray tracing to achieve realistic point cloud relighting. 
Unlike traditional Gaussian Splatting, which primarily focuses on appearance and geometry modeling, Relightable Gaussians also aim to model the physical interaction of light with different surfaces in the scene.
Specifically, for each Gaussian $G_i$, the original color coefficients $\mathbf{c}_i$ is decomposed into a 3-dimensional base color $\mathbf{b}_i \in [0,1]^3$, a 1-dimensional roughness $r \in [0,1]$, and incident light coefficients $\mathbf{l}_i$ for spherical harmonics in degree of 3. Subsequently, the Physical-Based Rendering (PBR) process and a point-based ray tracing are applied to obtain the colored PBR 2D images $\hat{I}^{PBR}_{1:K}$ and additionally supervised by $I_{1:K}$. Besides the above extensions on PBR for relighting, Relightable Gaussians also introduces a 3-dimensional normal $\mathbf{n}_i$ for $G_i$ and leverages several techniques, including an unsupervised estimation of a depth map $D_i$ from each input view $\xi_i$, to enhance the geometry accuracy and smoothness. By conducting this self-supervised estimation and regularization of normal maps and depth maps, the predicted depth map $D_i$ is more reliable than the vanilla 3DGS. Please refer to the original paper of Relightable Gaussians~\cite{gao2023relightable} for detailed explanations.  

In conclusion, each Gaussian of our 3DGIC is parameterized as $G_i = \{\mathbf{p}_i, \mathbf{s}_i, \mathbf{q}_i, {\alpha}_i, \mathbf{c}_i, \mathbf{e}_i, \mathbf{b}_i, r,  \mathbf{l}_i, \mathbf{n}_i\}$. By combining these methods, we are able to perform reliable depth estimations and effective removal of the Gaussians corresponding to any object in the scene for our 3DGIC.



\subsection{Additional Details of  Inferring Depth-Guided Inpainting Masks}
In Sect.~\textcolor{cvprblue}{2.2} in our main paper, we introduce infer proper inpainting masks $M'_{1:K}$ to determine the region to be inpaint by realizing visible background regions across different views. In our implementation, after updating the inpainting masks $M'_{1:K}$ with the process described in Sect.~\textcolor{cvprblue}{3.2}, we further conduct a refinement for each mask as a post-processing to prevent noisy mask. Taking $M'_1$ as an example, this process updates $M'_1$ as:

\begin{equation}
    M'_1 \leftarrow Open(M'_1), 
\end{equation}
where $Open(\cdot)$ represents a morphological opening process to reduce noises. This refinement process ensures that small noisy pixels are suppressed in our Depth-Guided Inpainting Masks.
\input{quali_kitchen}
\subsection{Additional Details of  Initializing Inpainted Gaussian}
In Sect.~\textcolor{cvprblue}{3.3}, we introduce to remove the Gaussians with semantic labels corresponding to the ``\textit{bear}'' object in $G_{1:N}$ and replace by the same amount of randomly initialized Gaussians in the masked region as the initialization of $G'_{1:N'}$. We now detail this initialization process for $G'_{1:N'}$. 

When first removing the Gaussians corresponding to the ``bear'' object, we directly use the remaining Gaussian to render the image $I'_1$ and depth map $D'_1$. Following the 2D inpainting process described in Sect.~\textcolor{cvprblue}{3.3}, the inpainted image $I^{In}_1$ and depth map $D^{In}_1$ are produced and projected into 3D space as colored point clouds $P_1$. We then use the 3D coordinates of $P_1$ as the initialized 3D position for the newly introduced Gaussians for $G'_{1:N'}$, since $P_1$ represents the ideal surface of the inpainted 3D Gaussian provided by $I^{In}_1$ after removing the bear. Note that if the number of points in $P_1$ does not match the number of newly initialized Gaussians in $G'_{1:N'}$ (also the number of removed Gaussians in $G_{1:N}$), we apply random selection to the coordinates of $P_1$ to match the number of the newly introduced Gaussians. As for the other parameters of the newly introduced Gaussians in $G'_{1:N'}$, we follow Gaussian Grouping~\cite{ye2023gaussiangrouping} to average the parameters of each Gaussian's 5-nearest neighbors (in 3D space) from the remaining Gaussians as initialization. By this process, $G'_{1:N'}$ is properly initialized.


\subsection{Implementation Details}
 In all our experiments, we train one model for each object category, using a single NVIDIA RTX 3090  GPU (24G) for training with the PyTorch~\cite{paszke2019pytorch}  libraries. For each scene, 5000 iterations of optimization are applied to obtain the inpainted 3DGS model. We also use the official implementation of \cite{wang2024gscream, ye2023gaussiangrouping, chen2024mvip, mirzaei2023spin} for comparison. When applying 2D inpainting models to the image and depth map to be inpaint, if we use non-diffusion-based LAMA~\cite{lama} as inpainter, the RGB image and depth map are inpainted separately. However, if LDM~\cite{ldm} is applied as our 2D inpainters, we follow the suggestion in NeRFiller~\cite{weber2024nerfiller} to stack the RGB image and the depth map in the same image for inpainting to ensure the inpainted RGB image and the depth map are consistent in terms of the geometry details. Specifically, we crop a $512 \times 512$ patch for the RGB image and the depth map to be inpainted center at the pixel coordinate of the inpainting mask's center, and paste the cropped RGB patch to a $1024 \times 1024$-resolution black image at the upper right corner with the cropped depth map at the lower left corner as the input image for the LDM. Similarly, we also crop a $512 \times 512$ patch for the inpaint masks and put them to the upper right and lower left corner of another $1024 \times 1024$-resolution black image as the input binary inpainting mask for the LDM. We then use the prompt ``\textit{an RGB image and a depth image of the same scene}'' to inpaint the input image. Finally, the inpainted RGB patch and the depth map patch are pasted back to the original image and depth map, respectively, as the 2D inpainting result. It is worth noting that we apply the 2D inpainting process for every 500 iterations. Following MALD-NeRF~\cite{lin2024maldnerf}, we use the technique of partial DDIM~\cite{song2020ddim}, to start from latter step of the denoising process as optimization iteration grows. Specifically, for a 50-step DDIM process, we start from step 0 of the LDM denoising process for step 0 of our optimization. After 500 iteration steps, the second time of the LDM inpainting starts from step 5 of the DDIM process and so on. When our optimization reaches the last 500 iterations, the 2D inpainting process only denoises using the last five steps of DDIM. This prevents inpainting results that are too different from the current scene and provides more stability for our optimization process.

\subsection{Dataset Details}
For the ``figurines'' scene from LeRF~\cite{kerr2023lerf} dataset, we have 260 training frames and 40 testing frames, each with a resolution of $986 \times 728$. For the ``bear'' dataset from InNeRF360~\cite{wang2024innerf360}, we have 90 training frames and 6 testing frames, each with a resolution of $985\times729$. As for ``counter'' and ``kitchen'' scenes from MipNeRF360~\cite{barron2022mipnerf360}, 240 (230 for training and 10 for testing) and 279 (270 for training and 9 for testing) frames are available in total, respectively. Both scenes are in the resolution of $779\time520$.

\section{Additional Experiments}
We additionally show the results on the ``\textit{kitchen}'' scene from the MipNeRF360~\cite{barron2022mipnerf360} dataset in Figure~\ref{fig:quali_kitchen}. We can see that SPIn-NeRF produces blurry result, while GScream fail to handle camera views with a wide range and not able to remove the excavator clearly. Although Gaussian Grouping also produces plausible results at the excavator-removed regions, it incorrectly detects the glove behind the excavator as region to be inpaint by using the ``black blurry hole'' as the prompt for Gounded-SAM~\cite{ren2024grounded} to find inpainting masks and therefore changes the background that should not be changed (shown in the third view). On the other hand, our 3DGIC locates the proper region to inpaint and produces smooth and high fidelity results. 


\section{Limitations}
We now discuss the potential limitations of our 3DGIC. Since our 3DGIC uses the rendered depth map as guidance for the 3D inpainting process, the reliability of the rendered depth map becomes an important issue. As detailed in Sect.~\ref{subsec:backbone}, we combine the optimization technique introduced in Relightable Gaussians~\cite{gao2023relightable} to conduct a self-supervised loss for the predicted normal map and the rendered depth map to enhance the accuracy of the rendered depth map. However, if the input views are too sparse, the rendered depth map would not be guaranteed to be accurate, which hinders the inferring of Depth-Guided Mask and the achievement of cross-view consistency. Another potential limitation of our 3DGIC lies in the capability of the SAM~\cite{kirillov2023sam} model. As detailed in Sect.~\ref{subsec:backbone}, we use SAM to produce 2D segmentation masks and use these masks as supervision for our backbone 3DGS model so that we don't have to manually annotate the 2D object mask of the object to be removed like SPIn-NeRF~\cite{mirzaei2023spin}. However, if the object to be removed is too small, the SAM model would confuse it with other objects and not produce the correct segmentation mask for the object. To overcome the above limitations, studies on the production of reliable depth maps for 3DGS models with only sparse input views and producing a more accurate segmentation mask for any object would be possible directions to improve the quality of 3D Gaussian inpainting.
