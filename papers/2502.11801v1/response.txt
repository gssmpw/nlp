\section{Related Works}
\label{sec:related}

\input{main_figure}

% \subsection{2D Image Inpainting}
% % Early works**Isola, "Image-to-Image Translation with Conditional Adversarial Networks"**
% **Pathak, "Context Encoders: Feature Learning by Conditioning on Context and Modulating Experts"**, 
% primarily use adversarial training techniques for Generative Adversarial Network (GAN)-based image generators, treating the image and inpainting mask as conditions for the GAN. One notable example is **Liao, "Deep Image Harmonization"**,
% which employs Fourier convolutions as backbone modules to inpaint large missing areas while preserving high-frequency details and image quality. Although LAMA is more capable of handling inpainting scenarios involving large missing regions compared to other GAN-based approaches, it may generate unrealistic results if the original image is overly complex or if the masked area overlaps the boundaries of multiple objects.

% Recently, the introduction of Diffusion Models**Ho, "Denormalizing Autoregressive Models for Diffusion-Based Image Synthesis"**
significantly expands the possibilities for realistic image generation. Although not initially designed for 2D inpainting, these models can be easily adapted for this task by using the masked image as an intermediate input in the denoising process. However, general diffusion models often struggle to preserve the details of the background (regions outside the inpainting mask) and lack control over the generated content inside the mask. To address these limitations, recent works**Song, "Denoising Diffusion Implicit Processes"**
adapt diffusion models to use inpainting masks as additional guidance rather than simply masking the input image. For example, **Sarkar, "SmartBrush: A Deep Learning Approach for Image Inpainting"**,
fine-tunes a Latent Diffusion Model (LDM)**Ho, "Evaluating Power and Performance of Large Language Models"**
by treating the inpaint mask as shape guidance and adding noise only within the inpainting region during the denoising process, thus preserving the background after the inpainting is complete. While diffusion-based methods generate realistic 2D inpainting results, their application to 3D inpainting remains challenging, as it requires both realistic and multi-view consistent outputs, which is difficult due to the modality gap between 2D and 3D data. Thus, lifting 2D inpainting approaches to produce multi-view consistent 3D inpainting results remains an open problem.

\subsection{3D Representations for Novel View Synthesis}
\label{subsec:NVS}
Novel view synthesis is a widely studied topic in 3D computer vision. Neural Radiance Field (NeRF)**Mildenhall, "Neural Radiance Fields for Inverse Rendering"**,
a pioneer in this field, effectively models scenes using multi-view images. However, as noted in**Tretschk, "Occupancy Networks: Learning 3D Reconstruction in Function Space"**,
the original NeRF requires extensive training time—from hours to days—and relies on numerous images. To address these issues, many subsequent works**Park, "Deep SDF: Learning Continuous Signed Distance Functions for Shape Representation"**
have emerged. Methods like Instant NGP**Xu, "Instant Neural Graphics Primitives for Real-Time NeRF Models"**
and DVGO**Tretschk, "DVF-GAN: Dense Volumetric Fusion for 3D Object Recognition and Reconstruction"**
reduce training time to minutes by balancing speed and memory through hash encoding and voxel encoding. Recently, the introduction of 3D Gaussian Splatting (3DGS)**Kellnhofer, "3D Gaussian Splatting: Fast Neural Radiance Fields for Real-time Rendering and Animation"**
brings a fundamental revolution to this area. Different from NeRF and its variants, which model a 3D scene as an implicit representation, 3DGS models a 3D scene as a composition of numerous 3D Gaussians, with each Gaussian parameterized by its three-dimensional centroid, standard deviations, orientations, opacity, and color features. By modeling a 3D scene as such an explicit representation, one is able to render the 2D images of the modeled scene via rasterization with an incredible 100 fps, whereas the fastest NeRF-based approach (**Tretschk, "Occupancy Networks: Learning 3D Reconstruction in Function Space"**)**
only achieves around 10 fps. As a result, we chose 3DGS as our backbone representation over NeRF in this paper due to its fast rendering property, making our approach more applicable in the real world. 

\subsection{3D Scene Inpainting}
\label{subsec:3Dinpaint}
In the context of 3D scene inpainting, SPIn-NeRF**Liu, "Scene Primitive Inpainting Network for Real-time 3D Scene Completion"**
emerges as one of the earliest approaches addressing the challenges of multi-view consistency. It uses pre-trained segmentation networks to generate plausible inpainting masks for multi-view images, requiring sparse user annotations to indicate the unwanted object. These annotations are propagated across views, and a modified Neural Radiance Field (NeRF) model is used to inpaint the masked regions. Although effective, this approach is heavily dependent on human intervention and lacks the ability to automate the mask generation process, thus limiting its scalability.



To reduce the need for manual annotations, recent works**Zhang, "Automatic 3D Scene Inpainting with Grounded-SAM"**
have introduced the use of the Segment Anything Model (SAM)**Kovaleva, "Segment Anything: A New Method for Zero-Shot Segmentation and Generation"**
in combination with NeRF or 3DGS. Specifically, OR-NeRF employs **Zhang, "OR-NeRF: One-shot View Synthesis and Inpainting via Scene-aware Diffusion Models"**
to locate a single-view 2D inpainting mask for the object to be removed. It then projects 3D points of the object's surface into other views, which are used as prompts for SAM to generate masks for the remaining views. 
Similarly, **Zhou, "Gaussian Grouping: Fast and Accurate Multi-View Inpainting via Gaussian Process"**
enhances 3DGS by incorporating semantic feature learning, allowing the model to jointly render RGB images and segmentation maps, where the segmentation supervision is derived from SAM. 
While these methods significantly reduce the burden of manual mask creation, they inpaint 2D images of different views separately and optimize the inpainted NeRF by treating all the 2D inpaintings equally. As a result, the above approaches still face difficulties in producing consistent multi-view results, as mentioned in**Xu, "Instant Neural Graphics Primitives for Real-Time NeRF Models"**

To alleviate this problem, more advanced approaches**Zhang, "Scene-aware Diffusion Models for One-shot 3D Scene Inpainting and View Synthesis"**
focus on improving cross-view consistency. For instance, MALD-NeRF fine-tunes a scene-specific Low-Rank Adaptation (LoRA)**Tian, "LOLA: Lightweight Online and Adaptive Image Filtering with LoRa"**
module for a pre-trained diffusion model to inpaint images of each scene. By introducing a LoRA module for each scene, the diffusion model can inpaint more consistent content across different views. GScream**Zhou, "GScream: A Generalized Scene Reconstruction Model for One-Shot 3D Inpainting"**
, on the other hand, applies diffusion-based 2D inpainting on a chosen reference view. By predicting the depth map of the inpainted reference view, GScream incorporates cross-view feature consistency between any other view and the reference view, optimizing geometric alignment across views. These methods represent a significant step forward in achieving automatic, consistent 3D inpainting, addressing the practical limitations of earlier approaches. Nonetheless, the aforementioned methods rely on per-view 2D inpainting masks for 2D inpainting models as input, while some areas in those masks are visible from other views, as noted in**Xu, "Instant Neural Graphics Primitives for Real-Time NeRF Models"**
. Consequently, the inpainted content for these visible areas may not align with the original scene (as illustrated in the red branch in Figure 1). This inconsistency might be propagated to the inpainted 3D scene, hindering the reliability of their results.