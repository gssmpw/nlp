

\section{Method}
\label{sec:method}

\subsection{Problem Definition and Model Overview}
We begin with the notations and settings of our proposed framework. Given a 3D Gaussian Splatting (3DGS) model~\cite{ye2023gaussiangrouping} $G_{1:N} = \{G_1, G_2, ..., G_N \}$ ($N$ denotes the number of Gaussians) pretained for $K$ multi-view images $I_{1:K} = \{I_1, I_2, ..., I_K\}$ with their camera poses $\xi_{1:K} = \{\xi_1, \xi_2, ..., \xi_K\}$, our goal is to remove the Gaussians corresponding to a particular object (e.g., the bear statue) described by 2D object masks $M_{1:K} = \{M_1, M_2, ..., M_K\}$. More precisely, we aim to update the above 3DGS so that the optimized Gaussians $G'_{1:N'}$ (with $N'$ remaining Gaussians) allow novel view rendering without the object of interest presented. Take Figure~\ref{fig:main} as an example, the bear statue is to be removed from the scene of interest, and its segmentation masks $M_{1:K}$ from $I_{1:K}$ can be produced by models like SAM~\cite{kirillov2023sam} (see supplementary materials for details). 

%Given the above inputs, our goal is to remove the Gaussians corresponding to the specific object in the scene and re-optimize the remaining Gaussians $G'_{1:N'}$ ($N'$ denotes the number of Gaussians in the object-removed scene) to produce novel-view syntheses of the object-removed scene without the requirement of ground-truth images of the modified scene. Additionally, all the background contents except the object to be removed are not modified after the inpainting process.

To address the above 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency (3DGIC). Our 3DGIC comprises two learning stages: \textbf{Depth-Guided Inpainting Mask} and \textbf{Inpainting-guided 3DGS Refinement}. The former refines the object mask guided by both semantics and depth maps observed across $I_{1:K}$, while the latter performs inpainting with cross-view consistency for updating the Gaussians $G'_{1:N'}$.

%By selecting $\xi_1$ as a reference view for simplicity, the Depth-Guided Mask Reduction in \ref{subsub:mask reduce} refines the original object mask $M_1$ as $M'_1$ by excluding pixels corresponding to backgrounds visible in other views, producing inpainted image $I^{Ref}_1$ and depth map $D^{Ref}_1$ of the reference view. As for the Depth-Guided Cross-View Optimization in \ref{subsub:optimize}, we project the 2D pixels of $I^{Ref}_1$ within $M'_1$ into 3D space to obtain reference points $P^{Ref}$ for initializing new Gaussians $G^{Ref}$ and re-projection into all other views $\xi_{2:K}$ to optimize $G^{Ref}$ in a cross-view consistent manner. 
% In the following subsections, we will detail the Depth-Guided Mask Reduction, Depth-Guided Cross-View Optimization, and the objectives utilized to optimize $G^{Ref}$.


% Our DIGS takes a specific view, along with its rendered image and depth map from $G'_{1:N'}$, as a reference view to be inpainted by 2D inpainting models~\cite{lama, podell2023sdxl} for guiding the inpainting process. Without loss of generality, we use $\xi_1$ with its rendered image $I'_1$ and depth map $D'_1$ as our reference view. 
% By selecting the reference view, our proposed DIGS framework is composed of two key components: Depth-Guided Mask Reduction and Depth-Guided Cross-View Optimization. 
% To ensure that $I^{Ref}_1$ and $D^{Ref}_1$ are reliable as guidance, we introduce a novel Depth-Guided Mask Reduction process on the original object mask $M_1$. This reduces the region of the inpainting mask by excluding pixels that contain backgrounds seen from other views and produces a new mask $M'_1$, preserving visible content from other views. As for Depth-Guided Cross-View Optimization, a set of 3D reference points $P^{Ref}$ is obtained from projecting the 2D pixels of $I^{Ref}_1$ that lie inside $M'_1$ into 3D space as new initialized 3D positions of the new Gaussians $G^{Ref}$ to be optimized, while the parameters of all the Gaussians in $G'_{1:N'}$ are frozen. By projecting $P^{Ref}$ into all other views $\xi_{2:K}$, cross-view consistent guidance is provided for each view to optimize $G^{Ref}$, achieving a successful novel-view synthesis of the object-removed scene. In the following subsections, we will detail the Depth-Guided Mask Reduction, Depth-Guided Cross-View Optimization, and the objectives utilized to optimize $G^{Ref}$.


\input{mask_reduction}

% \subsection{Depth-Guided Inpainting for 3D Gaussian Splatting}
% \label{subsec: DIGS}

\subsection{Inferring Depth-Guided Inpainting Masks}
\label{subsub:mask reduce}
Given multi-view images $I_{1:K}$ of a scene with binary masks $M_{1:K}$ depicting the object to be removed, we aim to infer a proper mask $M'$ for inpainting images at each view under the guidance of depth images $D_{1:K}$ rendered from $G_{1:N}$. As a result, the masked image $I'^B_{1:K}$
only contains background pixels that are visible at other camera views. The $i$-th masked image $I'^B_i$ is defined as:
\begin{equation}
    I'^B_i = I_i \cdot (\mathds{1}-M_i),
\end{equation}
where $\mathds{1}$ denotes a tensor with the same size as $M$ and all the elements are one.

%With the inferred mask $M'_1$, our process ensures that the masked image mask $M'_1$ for $I'_1$ contains only the background region that is not visible to the remaining views $I_{2:K}$. This is achieved by excluding all the pixel coordinates of $M_1$ that correspond to the projected backgrounds from $I_{2:K}$ to camera view $\xi_1$.

Take $\{I_1, M_1\}$ in Figure~\ref{fig:mask_reduct} as an example, our process of inferring Depth-Guided Inpainting Masks takes the original image $I_1$ from $\xi_1$ and masks out the areas in $M_1$ as the original visible backgrounds $I^B_1 = I_1\cdot(\mathds{1}-M_1)$ from view $\xi_1$. To explore all the visible background pixels from other views $\xi_{2:K}$, we take $I_{2:K}$ with their masks $M_{2:K}$ and rendered depth $D_{2:K}$ at $\xi_{2:K}$, and we project the above background pixels from each view to $\xi_1$. Taking view $\xi_2$ as an example, the visible backgrounds $I^B_2$ in $I_2$ ($I^B_2 = I_2 \cdot (\mathds{1}-M_2)$) are projected into the 3D space via $D_2$ and $\xi_2$ and then back-projected to $\xi_1$. 
Among all the back-projected pixels, we consider the pixel coordinates that lie inside $M_1$ as visible backgrounds from $I_2$, denoted as $I^B_{1,2}$. The operation for obtain $I^B_{1,2}$ is calculated as:
\begin{equation} \label{eq:proj}
    I^B_{1,2} =  Proj^{2D}(Proj^{3D}(I^B_2, D_2, \xi_2), \xi_1)\cdot M_1,
\end{equation}
where $Proj^{3D}(\cdot, \cdot, \cdot)$ denotes the 3D projection function that projects 2D colored pixels in $I^B_2$ into 3D point clouds with its depth map $D_2$ and camera pose $\xi_2$, while $Proj^{2D}(\cdot, \cdot)$ represents the 2D projection function that projects the 3D colored point cloud back to $\xi_1$ as colored pixels. With the above operation, the corresponding pixel coordinates of $I^B_{1,2}$ are directly excluded from $M_1$, and thus the inpainting mask is refined as $M'_{1}$, and the masked image ${I'}^{B}_1 = I^B_1 + I^B_{1,2}$ at $\xi_1$ is further obtained. Similarly, we repeat this process through all the views $\xi_{2:K}$ to infer the final Depth-Guided Inpainting Mask $M'_1$ and the masked image ${I'}^B_1$ at $\xi_1$. Also, we produce the depth-guided inpainting masks $M'_{1:K}$ for all the views $\xi_{1:K}$. Please refer to our supplementary material for more details about this inferring process. 

It is worth noting that the above process is deterministic. It not only reduces the uncertainty of image regions to inpaint at each view, but it also makes the updating of the 3DGS model for rendering the unpainted scene more effective, as discussed in the following subsection.

\subsection{Inpainting-guided 3DGS Refinement}
\label{subsub:optimize}
The aim of this stage is to optimize $G'_{1:N'}$ with masked $I'_{1:K}$ obtained at $\xi_{1:K}$ with cross-view consistency so that rendering of the corresponding high-fidelity scene can be produced, realizing the task of 3D inpainting. As shown in Figure~\ref{fig:main}, the 3DGS for this inpainting scene can be first updated by removing the Gaussians with the semantic labels corresponding to the masked region (e.g., ``\textit{bear}'' in the original Gaussian $G_{1:N}$), and replaced by the same amount of randomly initialized Gaussians in the masked region (e.g., with bear removed; see~\cite{ye2023gaussiangrouping} and our supplementary materials). 
%We now discuss how we perform optimization for the object-removed Gaussians $G'_{1:N'}$ by giving the Depth-Guided Inpainting Masks $M'_{1:K}$ obtained in Sect.~\ref{subsub:mask reduce}. 
% Our goal is to optimize $G'_{1:N'}$ properly so that it presents a high-fidelity scene where the bear is removed and its rendered images $I'_{1:K}$ at $\xi_{1:K}$ are cross-view consistent. 

Take $\xi_1$ as the reference view for an example, the rendered image $I'_1$ and depth map $D'_1$ of $G'_{1:N'}$ at $\xi_1$ are inpainted by a 2D inpainter~\cite{lama, ldm} (using $M'_1$ as inpainting mask) as:

\begin{equation}
\label{eq:lrender}
\begin{aligned}
    I^{In}_1 & =  & Inpaint_{2D}(I'_1, M'_1) \\
    D^{In}_1 & = & Inpaint_{2D}(D'_1, M'_1),
\end{aligned}
\end{equation}
where $Inpaint_{2D}(\cdot, \cdot)$ denotes the 2D inpainting process, and $I^{In}_1$ and $D^{In}_1$ represents the 2D-inpainted results of $I'_1$ and $D'_1$, respectively.
To ensure $I'_1$ looks identical to $I^{In}_1$, the \textit{rendering loss} at $\xi_1$ is defined as:
\begin{equation}
\label{eq:lrender}
    \mathcal{L}_{rendering} = \mathcal{L}_{rgb} + \mathcal{L}_{depth}.
\end{equation}
Note that the image recovery loss $\mathcal{L}_{rgb}$ is calculated as:
\begin{equation}
\label{eq:optimageloss}
    \mathcal{L}_{rgb} = \| I'_1 - {I^{In}_1}\|_1 + \mathcal{L}_{SSIM}(I'_1, I^{In}_1),
\end{equation}
where the $\mathcal{L}_{SSIM}$ denotes the structure similarity loss~\cite{kerbl202333dgs}. And, the depth loss $\mathcal{L}_{depth}$ is defined as:
\begin{equation}
\label{eq:depthloss}
    \mathcal{L}_{depth} = \| D'_1 - {D^{In}_1}\|_1.
\end{equation}

To further ensure the masked regions in $I'_{2:K}$ (with respect to $M'_{2:K}$) are cross-view consistent with the 2D-inpainted region in $I^{In}_1$, we project the inpainted region of $I^{In}_1$ into the 3D space as a set of colored point clouds $P_1$, followed by re-projecting back to $\xi_{2:K}$ as supervision. Thus, $P_1$ is calculated as:
\begin{equation}
    P_1 =  Proj^{3D}(I^{In}_1 \cdot M'_1, D^{In}_1, \xi_1)),
\end{equation} 
where $Proj^{3D}(\cdot, \cdot, \cdot)$ is the same projection function in Eqn.~\ref{eq:proj}. For each view $\xi_k$  of $\xi_{2:K}$, the back-projected image $I^P_k$for supervision is denoted as:
\begin{equation}
    I^{P}_k =  I'_k \cdot (1-M'_k) + Proj^{2D}(P_1, \xi_k)\cdot M'_k,
\end{equation}
where $Proj^{2D}(\cdot, \cdot)$ is also the same 2D projection function in Eqn.~\ref{eq:proj}. To this end, the cross-view consistent loss $\mathcal{L}_{cross}$ is defined as:
\begin{equation} \label{Lcross}
    \mathcal{L}_{cross} = \sum_{k\in {2...K}} \mathcal{L}_{LPIPS}(I'_k, I^{P}_k),
\end{equation}
where $\mathcal{L}_{LPIPS}$ denotes the LPIPS~\cite{lpips} loss that calculates the perceptual similarity between $I'_k$ and $I^{P}_k$.

Finally, the overall loss for 3D inpainting is calculated by $\mathcal{L}_{inpaint} = \mathcal{L}_{render} + \mathcal{L}_{cross}$. We note that by conducting $\mathcal{L}_{inpaint}$, $G'_{1:N'}$ is guaranteed to inpaint the object-removed 3D scene with cross-view consistency by taking $\{I^{In}_1, D^{In}_1 \}$ as guidance. 

% As for the objectives for other views $\xi_{2:K}$, we take the reference-view-projected 3D points
% $P^{Ref}$ and re-project them to each view as supervision. To be more specific, for the $k$-th view, $P^{Ref}$ is re-projected to its 2D image plane via the camera pose $\xi_k$ and replace the corresponding RGB pixel values in $I'_k$ to form a pseudo-reference image $I^{Ref}_k$ for view $\xi_k$. Since we only want this pseudo-reference image to supervise the region of $I'_k$ that lies inside the object mask $M_k$, the objective for this process is defined as:
% \begin{equation} \label{Lref}
%     \mathcal{L}_{Ref} = \sum_{k\in {2...K}} \mathcal{L}_{LPIPS}((I'_k \cdot M_k), (I^{Ref}_k \cdot M_k)),
% \end{equation}
% where $\mathcal{L}_{LPIPS}$ denotes the LPIPS~\cite{lpips} loss that calculates the perceptual similarity between the masked images $I'_k \cdot M_k$ and $I^{Ref}_k \cdot M_k$. To further ensure all the contents outside the mask $M_k$ are unchanged, we also apply an auxiliary loss to the region outside the mask for views $\xi_{2:K}$ combining L1-loss and SSIM loss, which is calculated as:
% \begin{equation} \label{Laux}
%   \begin{aligned} 
%     \mathcal{L}_{Aux} & = \sum_{k\in {2...K}} (\lambda\| (I'_1 - {I^{Ref}_1}) \cdot (\mathds{1}-M_k)\|_1 \\ 
%      & + \mathcal{L}_{SSIM}(I'_1 \cdot (\mathds{1} - M_k), I^{Ref}_1 \cdot (\mathds{1} - M_k))),
% \end{aligned}  
% \end{equation}
% where $\mathds{1}$ denotes a tensor with the same size as $M_k$ and all the elements are one. 


\subsection{Training and Inference}
\label{subsec:train and inference}
\subsubsection{Training}
During the training (optimization) process, we calculate the refined mask $M'$ described in Sect.~\ref{subsub:mask reduce} for all $K$ views and choose the view with the largest refined mask as the reference view. This is because the 2D inpainted result from this view covers the most 3D space compared to other views, allowing us to provide a more informative cross-view optimization. By choosing the reference view, $\mathcal{L}_{inpaint}$ is applied to optimize $G'_{1:N'}$. To this end, $G'_{1:N'}$ is properly supervised to ensure the 3D scene is reasonably inpainted and consistent across different views. 

\subsubsection{Inference}
Once we finish the optimization of the inpainted scene with our 3DGIC, the optimized Gaussians  $G'_{1:N'}$ are able to render a novel view synthesis of the scene by using arbitrary camera poses.  







% and inpainted by a 2D inpainter~\cite{lama, ldm} taking $M'_1$ as the inpainting mask. The inpainted image $I^{Ref}_1$ and depth map $D^{Ref}_1$ are used to guide the 3D inpainting process to achieve cross-view consistent results.
% the existing backgrounds ($G'_{1:N'}$), new Gaussians $G^{Ref}$ are initialized to be optimized to ensure the rendered image of $G'_{1:N'}$ and $G^{Ref}$ from $\xi_1$ (i.e., $I'_1$) looked identical to $I^{Ref}_1$ while $I'_{2:K}$ are consistent to the content in $I^{Ref}_1$. 


% \paragraph{Initialization of the Object-Removed Gaussian}
% As depicted in Figure~\ref{fig:main}, the to fill the blurry hole left by removing the bear in $G_{1:N}$, we introduce to initialize new Gaussians for $G'_{1:N'}$. By projecting pixel coordinates that lie inside $M'_1$ as well as the corresponding RGB values in $I^{Ref}_1$  to 3D space via $D^{Ref}_1$, we obtained a set of colored points, denoted as $P^{Ref}$. The initialized centroids for the newly introduced Gaussians are directly copied from the 3D coordinates of $P^{Ref}$. As for the rest of the parameters of these Gaussians (i.e., standard deviations, rotational quaternions, opacities, and color coefficients), we initialize them by averaging each Guassian's 5-nearest-neighbors from existing Gaussians in $G'_{1:N'}$, following Gaussian Grouping~\cite{ye2023gaussiangrouping}. 