\section{Related Work}
\label{sec:related_work}

%\noindent\textbf{Zero-Shot Learning} (ZSL) models are designed to recognize novel classes that were not presented during training by leveraging semantic relationships between known and unknown classes. This is typically done through the use of auxiliary information, such as class descriptions or specific attributes, which grants the model the necessary context to recognize unseen classes. For instance, a ZSL model that has been trained with images of horses can identify an unseen image of zebra by recognizing common attributes, such as the similar body shape and mane. When the model combines these known attributes with the learned attribute of stripes, the model can infer that an animal with a horse-like body and stripes is a zebra. 

%Several approaches have been proposed to address the ZSL problem. One method aligns visual and semantic domains by using task-specific encoders and supervised adversarial discrepancy learning \cite{chen2021hsva}. Other techniques leverage attention mechanisms to focus on relevant features, improving the recognition of unseen classes \cite{huynh2020fine, alamri2021implicitexplicitattentionzeroshot, xie2022towards}. Episode-based training frameworks simulate zero-shot classification tasks using class semantics, allowing models to progressively learn from mimicked unseen classes \cite{yu2020episode}. Generative adversarial network (GAN)-based methods have also been applied to generate synthetic features for unseen classes \cite{xian2018feature, gao2020zero, liu2022learning}. However, traditional zero-shot learning approaches focus strictly on identifying novel classes based on the shared characteristics from seen classes. While this method is effective in identifying new classes, it does not address the ability to recombine known components in novel ways. 

%\vspace{2mm}
%\noindent\textbf{Disentanglement} in the context of CZSL can be loosely defined as the process of separating visual primitives that are represented in an image, such as specific attributes and objects, into independent components. Disentanglement has been applied to many forms of n-shot learning to enhance model generalization by learning disentangled feature representations. For ZSL, many works apply disentanglement for for separating visual and semantic features. Tong et. al. propose a group-wise disentanglement approach within an auto-encoder framework to learn discriminative and generalizable latent features from image data \cite{Tong_2019_CVPR}. Another approach factorizes visual features into semantic-consistent and semantic-unrelated latent vectors through a semantics disentangling framework while applying a total correction penalty and relation network \cite{chen2021semantics}. Li et. al. focuses on separating visual and semantic features into category-distilling and category-dispersing factors through the use of a variational auto-encoder (VAE) \cite{li2021generalized}. However, disentanglement for ZSL primarily focuses on aligning entire visual features with semantic descriptions to recognize unseen classes. CZSL requires disentangling attributes and objects separately to handle novel compositions.

%Disentanglement is also common amongst CZSL methods \cite{hao2023learning, atzmon2020causal, hu2023leveraging}. Hao et. al. apply cross-attention as compositional disentanglers to learn attribute-object concept embeddings \cite{hao2023learning}. A causal-inspired approach, proposed in \cite{atzmon2020causal}, focuses on learning disentangled representations of visual object components through causal graphing mechanisms. Hu et. al. proposes sub-class discrimination, using embedding alignment between composed and disentangled recognition models, and prototype modulator networks to adapt class prototypes based on composition information. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth, height=0.25\textheight, keepaspectratio]{MainFigure.png} 
    \caption{\textbf{VAPS harnesses the power of CLIP as its backbone. Our groundbreaking approach addresses this by freezing CLIP's pre-trained visual and text encoders. The input image is passed through the image encoder of CLIP to extract image features $f_v$. These features are used as the input to the prompt adapter to provide the bias for shifting three learnable prefix text tokens {\sffamily{[v1][v2][v3]}}. The shifted prefix is prepended to the word embeddings of attributes and objects and passed through the text encoder of CLIP to get $f_t$. For each input image, the similarity of $f_v$ is compared with all visual prompts in the repository and the two top most similar visual prompts are retrieved and averaged. Then, $f_v$ and $f_t$ are decomposed and fused, where the output is mapped with the average of the selected visual prompts in a dedicated pair space. Additionally, $f_v \in \mathbb{R}^{d}$, where $d$ is the dimension of the visual feature extracted by the image encoder and $f_t \in \mathbb{R}^{768}$ represents the text feature extracted by the text encoder. Both are separately mapped onto another pair space. During inference, all steps are taken except that the learned prompt network shifts the three prefix text prompts {\sffamily{[v1][v2][v3]}}. The final logits are computed based on the similarity between image and text features in the pair space, as well as the similarity between the averaged visual prompts and the outcome of the fusion block. The aggregated logits are used to determine the final predicted composition, selecting the attribute and object with the highest score for the test image.    
}} %The framework of VAPS consists of a visual prompt repository and similarity-based retrieval mechanism. During training, image features are used three-fold: to retrieve relevant learned prompts from the visual repository based on feature similarity, to shift the learned text prompt's prefix using a bias term generated from the prompt adapter, and to fuse decomposed image and text features in the fusion block through cross-attention. During inference, the model retrieves the top visual prompts from the prompt repository, then generates text features using the adapted soft prompt with a bias applied to the prefix. Logits are then computed based on the similarity between image and text features and the visual prompts and fused image-text features in the feature alignment space. Lastly, logits are aggregated across attribute-object pairs, decomposed to obtain individual scores, and the attribute and object with the highest scores are selected as the final composition for the predicted image. }
    \label{fig:MainFig}
    \vspace{-4mm}
\end{figure*}


\noindent\textbf{Compositional Zero-Shot Learning} extends the principles of zero-shot learning by focusing on the recognition of unseen compositions of known primitives. As previously mentioned, disentanglement is a prevalent approach in many CZSL methods \cite{Tong_2019_CVPR, chen2021semantics, li2021generalized, hao2023learning}. However, this is not the only approach to achieve compositional generalization. Li et. al. uses the principles of symmetry and group theory to model attribute-object compositions through coupling and decoupling transformations, and introduces a novel distance method for CZSL \cite{li2020symmetry}. A Siamese Contrastive Embedding Network (SCEN) embeds visual features into a siamese contrastive space to separately capture attribute and object prototypes diversity \cite{li2022siamese}. A retrieval-augmented approach was proposed to enhance the recognition of unseen primitive compositions by retrieving and augmenting attribute and object representations \cite{jing2024retrieval}. Wang et. al. propose a dependent approach for CZSL that generates conditional attribute embeddings by using an attribute hyper learner and base learner to account for the varying interaction of attributes with different objects \cite{wang2023learning}.

Modern applications in CZSL include adapting pre-trained VLMs, such as CLIP \cite{radford2021learning}, to improve CZSL results. It is shown that downstream tasks can be built on top of the VLMs to enhance these results. Compositional Soft Prompting (CSP), introduced in \cite{nayak2022learning}, uses a static prompt prefix combined with learned attribute and object descriptions. This text is passed through a text encoder while the image is processed by CLIPâ€™s visual encoder. The model then calculates the cosine similarity between the text embeddings and image features to predict the correct attribute-object composition. More recent works built on top of this method by removing the static prefix content and instead making the entire prompt learnable \cite{xu2024gipcol, lu2023decomposed}. While these processes produce promising results, only one learned prompt may not generalize well to every image passed through the visual encoder. 

\vspace{-0.15mm}
\noindent\textbf{Prompt Learning/Tuning} modifies the original input by leveraging learnable tokens that guide the pre-trained language model to examine specific features or contexts relevant to the task the model is trying to solve \cite{liu2023pre, khattak2023maple, shi2023logoprompt}. With the more recent advancements in VLMs, prompt learning has steered into a new direction by focusing on the multi-modality of both textual and visual content in a shared embedding space \cite{radford2021learning, ramesh2021zero, kim2021vilt}. Huang et. al. introduced a method to transfer performance from VLMs without the need for prompt engineering or labeled data by generating pseudo labels for target datasets and optimizing learnable prompt representations through self-training on the psuedo-labeled samples \cite{huang2022unsupervised}. Prompt learning has been applied on top of pre-trained vision transformers to solve the catastrophic forgetting problem in continual learning by using a pool of learnable prompts to learn tasks sequentially \cite{wang2022learning}. CoOp \cite{zhou2022learning} introduced a method to automate prompt engineering for models like CLIP by learning the vectors of prefix content while keeping the pre-trained model fixed for few-shot scenarios. To improve on CoOp, researchers introduced CoCoOp \cite{zhou2022conditional} which learns a lightweight neural network that generates dynamic prompts based on the input image. 

\vspace{-5mm}