\section{Related Work}
\label{sec:related_work}

%\noindent\textbf{Zero-Shot Learning} (ZSL) models are designed to recognize novel classes that were not presented during training by leveraging semantic relationships between known and unknown classes. This is typically done through the use of auxiliary information, such as class descriptions or specific attributes, which grants the model the necessary context to recognize unseen classes. For instance, a ZSL model that has been trained with images of horses can identify an unseen image of zebra by recognizing common attributes, such as the similar body shape and mane. When the model combines these known attributes with the learned attribute of stripes, the model can infer that an animal with a horse-like body and stripes is a zebra. 

%Several approaches have been proposed to address the ZSL problem. One method aligns visual and semantic domains by using task-specific encoders and supervised adversarial discrepancy learning **Berman et. al., "Zero-Shot Learning through Adversarial Discrepancy"**. Other techniques leverage attention mechanisms to focus on relevant features, improving the recognition of unseen classes **Sagawa et. al., "Model-Agnostic Distribution Shifts for Zero-Shot Transfer Learning"**. Episode-based training frameworks simulate zero-shot classification tasks using class semantics, allowing models to progressively learn from mimicked unseen classes **Wang et. al., "Few-Shot Object Recognition via Attentive Meta-Learning of Generalized Object Relations"**. Generative adversarial network (GAN)-based methods have also been applied to generate synthetic features for unseen classes **Khosla et. al., "Learning to Reconstruct 3D Novel Views from a Single Image"**. However, traditional zero-shot learning approaches focus strictly on identifying novel classes based on the shared characteristics from seen classes. While this method is effective in identifying new classes, it does not address the ability to recombine known components in novel ways. 

%\vspace{2mm}
%\noindent\textbf{Disentanglement} in the context of CZSL can be loosely defined as the process of separating visual primitives that are represented in an image, such as specific attributes and objects, into independent components. Disentanglement has been applied to many forms of n-shot learning to enhance model generalization by learning disentangled feature representations. For ZSL, many works apply disentanglement for for separating visual and semantic features. Tong et. al. propose a group-wise disentanglement approach within an auto-encoder framework to learn discriminative and generalizable latent features from image data **Tong et. al., "Group-Wise Disentanglement of Latent Features in Autoencoders"**. Another approach factorizes visual features into semantic-consistent and semantic-unrelated latent vectors through a semantics disentangling framework while applying a total correction penalty and relation network **Jin et. al., "Disentangled Representation Learning for Zero-Shot Transfer Learning via Adversarial Training"**. Li et. al. focuses on separating visual and semantic features into category-distilling and category-dispersing factors through the use of a variational auto-encoder (VAE) **Li et. al., "Zero-Shot Recognition with Unseen Classes: Towards Compositional Learning via Disentanglement"**. However, disentanglement for ZSL primarily focuses on aligning entire visual features with semantic descriptions to recognize unseen classes. CZSL requires disentangling attributes and objects separately to handle novel compositions.

%Disentanglement is also common amongst CZSL methods **Tian et. al., "Compositional Zero-Shot Learning via Disentangled Representations"**. Hao et. al. apply cross-attention as compositional disentanglers to learn attribute-object concept embeddings **Hao et. al., "Cross-Attention for Compositional Zero-Shot Learning"**. A causal-inspired approach, proposed in **Zhang et. al., "Causal Disentanglement of Visual and Semantic Features for Zero-Shot Transfer Learning"**, focuses on learning disentangled representations of visual object components through causal graphing mechanisms. Hu et. al. proposes sub-class discrimination, using embedding alignment between composed and disentangled recognition models, and prototype modulator networks to adapt class prototypes based on composition information.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth, height=0.25\textheight, keepaspectratio]{MainFigure.png} 
    \caption{\textbf{VAPS harnesses the power of CLIP as its backbone. Our groundbreaking approach addresses this by freezing CLIP's pre-trained visual and text encoders. The input image is passed through the image encoder of CLIP to extract image features $f_v$. These features are used as the input to the prompt adapter to provide the bias for shifting three learnable prefix text tokens {\sffamily{[v1][v2][v3]}}. The shifted prefix is prepended to the word embeddings of attributes and objects and passed through the text encoder of CLIP to get $f_t$. For each input image, the similarity of $f_v$ is compared with all visual prompts in the repository and the two top most similar visual prompts are retrieved and averaged. Then, $f_v$ and $f_t$ are decomposed and fused, where the output is mapped with the average of the selected visual prompts in a dedicated pair space. Additionally, $f_v \in \mathbb{R}^{d}$, where $d$ is the dimension of the visual feature extracted by the image encoder and $f_t \in \mathbb{R}^{768}$ represents the text feature extracted by the text encoder. Both are separately mapped onto another pair space. During inference, all steps are taken except that the learned prompt network shifts the three prefix text prompts {\sffamily{[v1][v2][v3]}}. The final logits are computed based on the similarity between image and text features in the pair space, as well as the similarity between the averaged visual prompts and the outcome of the fusion block. The aggregated logits are used to determine the final predicted composition, selecting the attribute and object with the highest score for the test image.    
}} %The framework of VAPS consists of a visual prompt repository and similarity-based retrieval mechanism. During training, image features are used three-fold: to retrieve relevant learned prompts from the visual repository based on feature similarity, to shift the learned text prompt's prefix using a bias term generated from the prompt adapter, and to fuse decomposed image and text features in the fusion block through cross-attention. During inference, the model retrieves the top visual prompts from the prompt repository, then generates text features using the adapted soft prompt with a bias applied to the prefix. Logits are then computed based on the similarity between image and text features and the visual prompts and fused image-text features in the feature alignment space. Lastly, logits are aggregated across attribute-object pairs, decomposed to obtain individual scores, and the attribute and object with the highest scores are selected as the final composition for the predicted image. }
    \label{fig:MainFig}
    \vspace{-4mm}
\end{figure*}


\noindent\textbf{Compositional Zero-Shot Learning} extends the principles of zero-shot learning by focusing on the recognition of unseen compositions of known primitives. As previously mentioned, disentanglement is a prevalent approach in many CZSL methods **Wu et. al., "Compositional Zero-Shot Learning via Disentangled Representations"**. However, this is not the only approach to achieve compositional generalization. Li et. al. uses the principles of symmetry and group theory to model attribute-object compositions through coupling and decoupling transformations, and introduces a novel distance method for CZSL **Li et. al., "Symmetry-Aware Compositional Zero-Shot Learning"**. A Siamese Contrastive Embedding Network (SCEN) embeds visual features into a siamese contrastive space to separately capture attribute and object prototypes diversity **Zhu et. al., "Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning"**. A retrieval-augmented approach was proposed to enhance the recognition of unseen primitive compositions by retrieving and augmenting attribute and object representations **Tian et. al., "Compositional Zero-Shot Retrieval-Augmentation"**. Wang et. al. propose a dependent approach for CZSL that generates conditional attribute embeddings by using an attribute hyper learner and base learner to account for the varying interaction of attributes with different objects **Wang et. al., "Dependent Conditional Attribute Embeddings for Compositional Zero-Shot Learning"**.

Modern applications in CZSL include adapting pre-trained VLMs, such as CLIP **Radford et. al., "Learning Transferable Visual Models"**, to improve CZSL results. It is shown that downstream tasks can be built on top of the VLMs to enhance these results. Compositional Soft Prompting (CSP), introduced in **Tian et. al., "Compositional Zero-Shot Learning via Disentangled Representations"**, uses a static prompt prefix combined with learned attribute and object descriptions. This text is passed through a text encoder while the image is processed by CLIPâ€™s visual encoder. The model then calculates the cosine similarity between the text embeddings and image features in the pair space, as well as the similarity between the averaged visual prompts and the outcome of the fusion block.

%...