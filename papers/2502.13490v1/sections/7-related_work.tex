\section{Related Work}\label{sec:related}

In this section, we summarize current LLM hallucination detection (\S\ref{sec:related:detect}) and mitigation methods (\S\ref{sec:related:mitigate}), and categorize them based on internal ways and external ways.

\subsection{LLM Hallucinations Detection}\label{sec:related:detect}

Some previous works aim to detect hallucinated responses during LLM inference.
We divided them into 2 types: external and internal.

\sssec{External}.
Some works detect the hallucinations from the external response.
\cite{niu2024ragtruth, su2024mitigating, hu2024lrp4ragdetecting} utilizes RAG or KG to find the closest response from a ground-truth dataset, and compare it with the response from LLM to detect potential hallucinations.
Considering the high computing and storage cost, some lightweight external methods have also been proposed:
\cite{chen2024inside} propose new EigenScore metrics to evaluate responses' self-consistency.
\cite{zhang2023enhancing, sadat2023delucionqa} check the keywords within the response to check factuality.
\cite{kossen2024semantic} use semantics entropy from multiple queries to detect.

\sssec{Internal}.
Recently, researchers have tried to discover hallucination pattern within LLM inference process.
Most works directly use hidden states\cite{beigi2024internalinspector, chen2024inside, su2024unsupervised, duan2024llms, azaria2023internal} as the classification features, while activation state is also taken into consideration \cite{chen2024context, ji2024llm, he2024llm}.
Some works \cite{chuang2024lookback, yuksekgonul2023attention} also use attention as the classification feature since they have better interpretability.
Some researchers discover that the hallucinated tokens have lower token probability, and use it as a feature \cite{quevedo2024detecting, he2024llm}.

\subsection{LLM Hallucinations Mitigation}\label{sec:related:mitigate}

Previous works have proposed several methods to mitigate hallucinations in LLMs.
We divided previous mitigation methods into 3 types: external, model-based, and internal.

\sssec{External}.
Most works try to use external knowledge or inference paradigms to mitigate.
The most direct methods utilize outside knowledge to enhance LLMs' ability to generate factual responses, which can be categorized into knowledge graph-based \cite{sun2024thinkongraph, luo2024reasoning}, and RAG-based \cite{li2024chainofknowledge, tian2024finetuning}.
LLMs' ability to self-debug their response is also used to mitigate hallucinations \cite{chen2024teaching, gou2024critic}.
Some works also optimize the inference process of LLM, like dividing the task into sub-tasks \cite{pan2023fact}, building a chain for more robust reasoning \cite{luo2024reasoning, li2024chainofknowledge}.
Multi-agent paradigm is also considered in hallucination mitigation \cite{hong2024metagpt}, since the debate across multiple agents can enhance the response from a single agent.

\sssec{Model}.
Some works also aim at modifying models' weights for mitigation.
\cite{liu2024mitigating, li2024chainofknowledge, tian2024finetuning} addresses this issue by introducing a new dataset to fine-tune the model.
Some works aim to use full parameter fine-tuning \cite{tian2024finetuning}, while other works aim to use LoRA for more lightweight fine-tuning \cite{liu2024mitigating, li2024chainofknowledge}.
Besides fine-tuning, model editing is also a popular model-level hallucination mitigation method \cite{meng2023locating}, where developers or admins can only modify a few neural connections within the MLP layer to achieve accurate fact modification.

\sssec{Internal}.
Considering previous external methods and model methods' drawbacks on high cost and potential threat to catastrophic forgetting, some internal methods have been proposed.
Internal methods mainly focus on lightly modifying the decoding process of LLM without changing model weight.
\cite{azaria2023internal, chen2024context} discover potential mode within hidden state, and utilize it to mitigate hallucination.
\cite{chuang2024lookback} finds the significant difference between factual response and hallucinated response within attention scores, and intervenes attention score in the decoding process.
\cite{liu2024litcab} propose to use a new layer to intervene in logit output for better response.