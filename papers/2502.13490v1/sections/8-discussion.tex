\vspace{-10pt}
\section{Limitation \& Discussion}\label{sec:discussion}

%\sssec{High system cost}.
%The high computational and storage cost associated with many features significantly limits their practicality. Features such as \textit{Activation Map} and \textit{Joint Token Probabilities}, while effective in certain scenarios, require substantial resources due to their high theoretical complexity. This can lead to increased inference latency, making them unsuitable for real-time or resource-constrained applications.

\sssec{Limited transferability}.
Based on the transferability experiments and insights from the RAG analysis, it is evident that the internal features of LLMs differ greatly across different scenarios. This lack of consistency results in poor cross-dataset generalizability. For example, features trained on CNNDM fail to perform well on HaluEval and vice versa, highlighting the dataset-specific nature of these features. This presents a significant challenge in developing universally applicable detection systems.

\sssec{Potential explainability}.
Despite the limitations, the proposed approach offers a promising avenue for explainability. Attention-based features, such as \textit{Lookback Ratio}, and logit-based features, such as \textit{Joint Token Probabilities}, provide interpretable insights into the model's reasoning process. These features allow researchers to better understand why certain outputs are classified as hallucinated, thereby enhancing trust in the system and opening up new possibilities for debugging and model refinement.