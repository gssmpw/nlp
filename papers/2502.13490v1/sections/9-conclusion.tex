\vspace{-10pt}
\section{Conclusion}\label{sec:conclusion}

In this work, we proposed and evaluated a comprehensive framework for understanding and detecting hallucinations in large language models through internal state analysis. Our experiments demonstrated the potential of using attention, activation, and logit-based features to distinguish between hallucinated and factual outputs. However, challenges remain, including high computational costs, limited transferability of features across datasets, and the need for more efficient and generalizable detection strategies. Despite these limitations, the approach shows promise for improving the explainability of hallucination detection, especially through interpretable features like attention and logits. Future work will focus on optimizing feature extraction for real-time applications and exploring methods to enhance feature transferability across diverse scenarios.

\newpage\clearpage

\section*{Ethics Consideration}

This study does not involve significant ethical concerns. The research is conducted using publicly available datasets (e.g., CNNDM, HaluEval), ensuring compliance with their usage policies and avoiding using private or sensitive data. No live systems were tested, and the methodology avoids actions that could harm users or disrupt services. Furthermore, the study emphasizes responsible reporting and discusses safeguards to prevent potential misuse of the findings. Ethical principles, including beneficence, justice, and respect for privacy, are inherently upheld in this research.

\section*{Open Science}

To promote transparency and reproducibility in research, we commit to sharing the source code, datasets, and implementation details of our proposed framework, \sysname. Upon acceptance, all relevant artifacts will be made publicly available on a trusted repository (e.g., GitHub or Zenodo) under an open-source license. This includes:
\begin{enumerate}
    \item The complete codebase for \sysname, covering inner state extraction, feature computation, and hallucination detection.
    \item Instructions for reproducing the experiments, including the preprocessing of datasets and evaluation metrics.
    \item A detailed README file to guide users in replicating the results.
\end{enumerate}

We believe that open science fosters collaboration and innovation, and we aim to contribute to the broader research community by ensuring the accessibility and usability of our work.