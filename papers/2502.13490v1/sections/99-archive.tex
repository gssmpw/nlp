\cite{beigi2024internalinspector} use hidden states and activation state to detect potential hallu.
\cite{chuang2024lookback} use attention lookback ratio to detect and mitigate.
\cite{ji2024llm} use activation state and last token state to detect.
\cite{kossen2024semantic} use semantics entropy from multiple queries to detect.
\cite{chen2024context} use context activate state sharpness to detect and decoding to mitigate.
\cite{chen2024inside} use hidden state to detect
\cite{su2024unsupervised} use hidden state to detect
\cite{yuksekgonul2023attention} use attention on key tokens to detect.
\cite{quevedo2024detecting} use token prob to detect
\cite{duan2024llms} use hidden state to detect
\cite{azaria2023internal} use hidden state to detect and decoding.
\cite{he2024llm} use token prob rank, logit divergence, activate map to detect.


\cite{meng2023locating} propose to edit factual relations in feed-forward weight.
\cite{manakul2023self} propose using sampling-based black-box hallucination mitigation.
\cite{pan2023fact} present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions.
\cite{feng2024unveiling}
\cite{sun2024thinkongraph} propose a new LLM-KG integrating paradigm ``LLM x KG'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge.
\cite{chen2024teaching} shows that LLM can self-debug itself in code applications.
\cite{luo2024reasoning} propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans.
\cite{liu2024mitigating} addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction, and fine-tuning the model using the dataset.
\cite{hong2024metagpt} propose to use multi-agent to mitigate the hallucination.
\cite{liu2024litcab} present LitCab, a lightweight calibration mechanism consisting of a single linear layer that takes the input text representation and predicts a bias term, which is then added to the LM output logits.
\cite{chen2024inside} a simple yet effective \textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space.
\cite{tian2024finetuning} show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality.
\cite{gou2024critic} use the CRITIC dataset to self-tuning itself's response.
\cite{li2024chainofknowledge} present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources.
\cite{zhang2023enhancing} propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency.
\cite{sadat2023delucionqa} propose using semantics feature to detect hallucinations.
\cite{yu2024truth} propose Truth-Aware Context Selection (TACS), a lightweight method to adaptively recognize and mask untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context.



Detection:

RAG-based:
\cite{niu2024ragtruth}
\cite{su2024mitigating}
\cite{hu2024lrp4ragdetecting}


Response semantics:
\cite{chen2024inside} a simple yet effective \textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space.
\cite{zhang2023enhancing} propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency.
\cite{sadat2023delucionqa} propose using semantics feature to detect hallucinations.
input hallu: \cite{yu2024truth} propose Truth-Aware Context Selection (TACS), a lightweight method to adaptively recognize and mask untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context.
\cite{kossen2024semantic} use semantics entropy from multiple queries to detect.

Hidden state\& Activation state:
\cite{beigi2024internalinspector} use hidden states and activation state to detect potential hallu.
\cite{chen2024context} use context activate state sharpness to detect and decoding to mitigate.
\cite{ji2024llm} use activation state and last token state to detect.
\cite{chen2024inside} use hidden state to detect
\cite{su2024unsupervised} use hidden state to detect
\cite{duan2024llms} use hidden state to detect
\cite{azaria2023internal} use hidden state to detect and decoding.
\cite{he2024llm} use token prob rank, logit divergence, activate map to detect.

Attention:
\cite{chuang2024lookback} use attention lookback ratio to detect and mitigate.
\cite{yuksekgonul2023attention} use attention on key tokens to detect.

Token prob\& logit
\cite{quevedo2024detecting} use token prob to detect
\cite{he2024llm} use token prob rank, logit divergence, activate map to detect.



decoding:
\cite{azaria2023internal} use hidden state to detect and decoding.
\cite{chen2024context} use context activate state sharpness to detect and decoding to mitigate.
\cite{chuang2024lookback} use attention lookback ratio to detect and mitigate.
\cite{liu2024litcab} present LitCab, a lightweight calibration mechanism consisting of a single linear layer that takes the input text representation and predicts a bias term, which is then added to the LM output logits.

edit:
\cite{meng2023locating} propose to edit factual relations in feed-forward weight.

sampling:
\cite{manakul2023self} propose using sampling-based black-box hallucination mitigation.

KG/RAG/outside knowlegde:
\cite{sun2024thinkongraph} propose a new LLM-KG integrating paradigm ``LLM x KG'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge.
\cite{luo2024reasoning}
\cite{li2024chainofknowledge}
\cite{tian2024finetuning} show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality.

self-debug:
\cite{chen2024teaching} shows that LLM can self-debug itself in code applications.
\cite{gou2024critic} use the CRITIC dataset to self-tuning itself's response.

Opmitze Inference process
\cite{pan2023fact} present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions.
\cite{luo2024reasoning} propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans.
\cite{li2024chainofknowledge} present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources.

Finetune:
\cite{liu2024mitigating} addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction, and fine-tuning the model using the dataset.

Multi-agent debate:
\cite{hong2024metagpt} propose to use multi-agent to mitigate the hallucination.


Response semantics:
\cite{chen2024inside} a simple yet effective \textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space.
\cite{zhang2023enhancing} propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency.
\cite{sadat2023delucionqa} propose using semantics feature to detect hallucinations.
input hallu: \cite{yu2024truth} propose Truth-Aware Context Selection (TACS), a lightweight method to adaptively recognize and mask untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context.

\begin{figure*}[htbp]
  \centering
    \begin{minipage}{\textwidth}
    {\centering{\hspace{8cm}HaleEval Dataset}}
    \end{minipage}
    \subfloat[Lookback ratio]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-avg_lookback_ratio.pdf}}
    \subfloat[Attention entropy]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-avg_input_context_sharpness.pdf}}
    \subfloat[Hidden state]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-avg_hidden_state.pdf}}
    \subfloat[Activation map]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-diff_activation_map.pdf}}
    \\
    \subfloat[Activation entropy]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-max_activation_entropy.pdf}}
    \subfloat[Max token ranks]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-max_token_ranks.pdf}}
    \subfloat[Min token probs]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-min_token_probs.pdf}}
    \subfloat[Joint token probs]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-joint_token_probs.pdf}}
    \\
    \begin{minipage}{\textwidth}
    {\centering{\hspace{8cm}CNNDM Dataset}}
    \end{minipage}
    \subfloat[Lookback ratio]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-cnndm-avg_lookback_ratio.pdf}}
    \subfloat[Attention entropy]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-cnndm-avg_input_context_sharpness.pdf}}
    \subfloat[Hidden state]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-cnndm-avg_hidden_state.pdf}}
    \subfloat[Activation map]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-cnndm-diff_activation_map.pdf}}
    \\
    \subfloat[Activation entropy]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-cnndm-max_activation_entropy.pdf}}
    \subfloat[Max token ranks]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-cnndm-max_token_ranks.pdf}}
    \subfloat[Min token probs]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-cnndm-min_token_probs.pdf}}
    \subfloat[Joint token probs]{\includegraphics[width=0.24\textwidth]{figures/fig-understanding-infer-cnndm-joint_token_probs.pdf}}
  \caption{
    We visualize the comparison of different features between factual output and hallucinated output.
    Two datasets: HaluEval and CNNDM are used to evaluate this.
  }
  \label{fig:understand:infer:halueval}
\end{figure*}

\begin{figure*}[htbp]
  \centering
    \subfloat[HaluEval]{\includegraphics[width=0.5\textwidth]{figures/fig-understanding-infer-corr-halueval.pdf}}
    \subfloat[CNNDM]{\includegraphics[width=0.5\textwidth]{figures/fig-understanding-infer-corr-cnndm.pdf}}
  \caption{
    We visualize the correlation between different states of two datasets: HaluEval for factual hallucinations, and CNNDM for faithful hallucinations.
  }
  \label{fig:understand:infer:corr}
\end{figure*}

\begin{figure*}[htbp]
  \centering
    \subfloat[HaluEval]{\includegraphics[width=0.5\textwidth]{figures/fig-understanding-infer-importance-halueval.pdf}}
    \subfloat[CNNDM]{\includegraphics[width=0.5\textwidth]{figures/fig-understanding-infer-importance-cnndm.pdf}}
  \caption{
    We visualize the important values contributing to the classification of labels of different states of two datasets: HaluEval for factual hallucinations, and CNNDM for faithful hallucinations.
  }
  \label{fig:understand:infer:importance}
\end{figure*}


\begin{figure*}[htbp]
  \centering
    \subfloat{\includegraphics[width=0.48\textwidth]{figures/fig-infer-legend.pdf}}
    \\
    \addtocounter{subfigure}{-1}
    \begin{minipage}{\textwidth}
    {\centering{\hspace{3cm}Attention\hspace{4cm}Activation\hspace{4cm}Logit}}
    \end{minipage}
    \subfloat[Lookback ratio]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-avg_lookback_ratio.pdf}}
    \subfloat[Attention entropy]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-avg_input_context_sharpness.pdf}}
    \subfloat[Hidden states]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-avg_hidden_state.pdf}}
    \subfloat[Minimum token probability]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-min_token_probs.pdf}}
    \subfloat[Joint token probability]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-joint_token_probs.pdf}}
    \\
    \subfloat[Lookback ratio]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-avg_lookback_ratio-per_layer.pdf}}
    \subfloat[Attention entropy]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-avg_input_context_sharpness-per_layer.pdf}}
    \subfloat[Activation sharpness]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-max_activation_entropy.pdf}}
    \subfloat[Maximum token rank]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-max_token_ranks.pdf}}
    \subfloat[Average distribution divergence]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-avg_jsd.pdf}}
  \caption{
    We visualize the comparison of attention-type features between inference with RAG and without RAG.
  }
  \label{fig:understand:infer}
\end{figure*}


\begin{figure*}[htbp]
  \centering
    \subfloat{\includegraphics[width=0.48\textwidth]{figures/fig-rag-legend.pdf}}
    \\
    \addtocounter{subfigure}{-1}
    \begin{minipage}{\textwidth}
    {\centering{\hspace{3cm}Attention\hspace{4cm}Activation\hspace{4cm}Logit}}
    \end{minipage}
    \subfloat[Lookback ratio]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-avg_lookback_ratio.pdf}}
    \subfloat[Attention entropy]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-avg_input_context_sharpness.pdf}}
    \subfloat[Hidden states]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-avg_hidden_state.pdf}}
    \subfloat[Minimum token probability]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-min_token_probs.pdf}}
    \subfloat[Joint token probability]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-joint_token_probs.pdf}}
    \\
    \subfloat[Lookback ratio]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-avg_lookback_ratio-per_layer.pdf}}
    \subfloat[Attention entropy]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-avg_input_context_sharpness-per_layer.pdf}}
    \subfloat[Activation sharpness]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-max_activation_entropy.pdf}}
    \subfloat[Maximum token rank]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-max_token_ranks.pdf}}
    \subfloat[Average distribution divergence]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-avg_jsd.pdf}}
  \caption{
    We visualize the comparison of attention-type features between inference with RAG and without RAG.
  }
  \label{fig:understand:rag}
\end{figure*}

\begin{figure*}[htbp]
  \centering
    % 图例
    \subfloat{\includegraphics[width=0.48\textwidth]{figures/fig-rag-legend.pdf}}
    \\
    \addtocounter{subfigure}{-1}
    \hrule
    \vspace{1pt}
    \hrule
    \vspace{1pt}
    \begin{minipage}{\textwidth}
    {\centering{\hspace{3cm}Attention\hspace{2.7cm}|\hspace{1.3cm}Activation\hspace{0.75cm}|\hspace{3.25cm}Logit}}
    \end{minipage}
    \\
    % 开始绘制子图并框起来
    \begin{tikzpicture}
        % Attention 部分
        \node[anchor=south west, inner sep=0] (a) at (0,0) {
            \subfloat[Lookback ratio]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-avg_lookback_ratio.pdf}}
            \subfloat[Attention entropy]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-avg_input_context_sharpness.pdf}}
            \subfloat[Hidden states]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-avg_hidden_state.pdf}}
            \subfloat[Min. token probability]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-min_token_probs.pdf}}
            \subfloat[Joint token probability]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-joint_token_probs.pdf}}
        };

        % Activation 部分
        \node[anchor=south west, inner sep=0, yshift=-3.2cm] (b) at (a.south west) {
            \subfloat[Lookback ratio]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-avg_lookback_ratio-per_layer.pdf}}
            \subfloat[Attention entropy]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-avg_input_context_sharpness-per_layer.pdf}}
            \subfloat[Activation sharpness]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-max_activation_entropy.pdf}}
            \subfloat[Maximum token rank]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-max_token_ranks.pdf}}
            \subfloat[Avg. dist. divergence]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-rag-HaluEval-avg_jsd.pdf}}
        };

        % 绘制虚线框
        \draw[dashed] (0, 3.2) rectangle (7.05, -3.2); % Attention 框
        \draw[dashed] (7.12, 3.2) rectangle (10.6, -3.2);   % Activation 框
        \draw[dashed] (10.68, 3.2) rectangle (17.8, -3.2); % Logit 框

        % 添加标签
        %\node at (5.5, 1.8) {\textbf{Attention}};
        %\node at (5.5, -3.2) {\textbf{Activation}};
        %\node at (5.5, -7.8) {\textbf{Logit}};
        
    \end{tikzpicture}

  \caption{
    We visualize the comparison of attention-type features between inference with RAG and without RAG.
  }
  \label{fig:understand:rag}
\end{figure*}

\begin{figure*}[htbp]
  \centering
    % 图例
    \subfloat{\includegraphics[width=0.48\textwidth]{figures/fig-infer-legend.pdf}}
    \\
    \addtocounter{subfigure}{-1}
    \hrule
    \vspace{1pt}
    \hrule
    \vspace{1pt}
    \begin{minipage}{\textwidth}
    {\centering{\hspace{3cm}Attention\hspace{2.7cm}|\hspace{1.3cm}Activation\hspace{0.75cm}|\hspace{3.25cm}Logit}}
    \end{minipage}
    \\
    \hrule
    \vspace{1pt}
    \begin{minipage}{\textwidth}
    {\centering{\hspace{8cm}Dataset: HaluEval}}
    \end{minipage}
    \\
    \vspace{1pt}
    \hrule
    \vspace{1pt}
    \vspace{1pt}
    % 开始绘制子图并框起来
    \begin{tikzpicture}
        % Attention 部分
        \node[anchor=south west, inner sep=0] (a) at (0,0) {
            \subfloat[Lookback ratio]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-avg_lookback_ratio.pdf}}
            \subfloat[Attention entropy]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-avg_input_context_sharpness.pdf}}
            \subfloat[Hidden states]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-avg_hidden_state.pdf}}
            \subfloat[Min. token probability]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-min_token_probs.pdf}}
            \subfloat[Joint token probability]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-joint_token_probs.pdf}}
        };

        % Activation 部分
        \node[anchor=south west, inner sep=0, yshift=-3.2cm] (b) at (a.south west) {
            \subfloat[Lookback ratio]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-avg_lookback_ratio-per_layer.pdf}}
            \subfloat[Attention entropy]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-avg_input_context_sharpness-per_layer.pdf}}
            \subfloat[Activation sharpness]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-max_activation_entropy.pdf}}
            \subfloat[Maximum token rank]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-max_token_ranks.pdf}}
            \subfloat[Avg. dist. divergence]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-HaluEval-avg_jsd.pdf}}
        };

        % 绘制虚线框
        \draw[dashed] (0, 3.2) rectangle (7.05, -3.2); % Attention 框
        \draw[dashed] (7.12, 3.2) rectangle (10.6, -3.2);   % Activation 框
        \draw[dashed] (10.68, 3.2) rectangle (17.8, -3.2); % Logit 框

        % 添加标签
        %\node at (5.5, 1.8) {\textbf{Attention}};
        %\node at (5.5, -3.2) {\textbf{Activation}};
        %\node at (5.5, -7.8) {\textbf{Logit}};
        
    \end{tikzpicture}

    \begin{minipage}{\textwidth}
    {\centering{\hspace{8cm}Dataset: CNNDM}}
    \end{minipage}

    \begin{tikzpicture}
        % Attention 部分
        \node[anchor=south west, inner sep=0] (a) at (0,0) {
            \subfloat[Lookback ratio]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-CNNDM-avg_lookback_ratio.pdf}}
            \subfloat[Attention entropy]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-CNNDM-avg_input_context_sharpness.pdf}}
            \subfloat[Hidden states]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-CNNDM-avg_hidden_state.pdf}}
            \subfloat[Min. token probability]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-CNNDM-min_token_probs.pdf}}
            \subfloat[Joint token probability]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-CNNDM-joint_token_probs.pdf}}
        };

        % Activation 部分
        \node[anchor=south west, inner sep=0, yshift=-3.2cm] (b) at (a.south west) {
            \subfloat[Lookback ratio]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-CNNDM-avg_lookback_ratio-per_layer.pdf}}
            \subfloat[Attention entropy]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-CNNDM-avg_input_context_sharpness-per_layer.pdf}}
            \subfloat[Activation sharpness]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-CNNDM-max_activation_entropy.pdf}}
            \subfloat[Maximum token rank]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-CNNDM-max_token_ranks.pdf}}
            \subfloat[Avg. dist. divergence]{\includegraphics[width=0.2\textwidth]{figures/fig-understanding-infer-CNNDM-avg_jsd.pdf}}
        };

        % 绘制虚线框
        \draw[dashed] (0, 3.2) rectangle (7.05, -3.2); % Attention 框
        \draw[dashed] (7.12, 3.2) rectangle (10.6, -3.2);   % Activation 框
        \draw[dashed] (10.68, 3.2) rectangle (17.8, -3.2); % Logit 框

        % 添加标签
        %\node at (5.5, 1.8) {\textbf{Attention}};
        %\node at (5.5, -3.2) {\textbf{Activation}};
        %\node at (5.5, -7.8) {\textbf{Logit}};
        
    \end{tikzpicture}

  \caption{
    We visualize the comparison of hallucinated output and factual output on HaluEval dataset.
  }
  \label{fig:understand:infer}
\end{figure*}


\multicolumn{8}{|c|}{Logistical Regression} \\
\hline\hline
\multirow{4}{*}{Logit}      & Max token rank            &  &  &  &  &  &  \\
                            & Min token prob            &  &  &  &  &  &  \\
                            & Joint prob                &  &  &  &  &  &  \\
                            & JSD                       &  &  &  &  &  &  \\
\hline\hline
\multirow{3}{*}{Attention}  & Lookback ratio            &  &  &  &  &  &  \\
                            & Attention sharpness       &  &  &  &  &  &  \\
                            & Key token attention ratio &  &  &  &  &  &  \\
\hline\hline
\multirow{3}{*}{Activation} & Hidden state              &  &  &  &  &  &  \\
                            & Activation entropy        &  &  &  &  &  &  \\
                            & Diff Activation map       &  &  &  &  &  &  \\
\hline
\hline

\subsection{Detection}\label{sec:scheme:detect}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/fig-detection.pdf}
    \caption{
    \sysname supports 2 types of detection: The siamese network model with MLP as sub-models and the ensemble model with logistic regression as sub-models.
    }
    \label{fig:detection}
\end{figure}

Based on the features extracted, \sysname performs detection on classification models.
\sysname supports 2 types of detection: The siamese network model and the ensemble model.

\sssec{Siamese network model}.
The Siamese network model serves as a pairwise comparison structure, employing two identical subnetworks with shared weights. Each subnetwork extracts features from tokens or inner states, projecting them into the same embedding space. The distance between embeddings is used to classify whether the compared states represent hallucinated or factual outputs. Its main advantage lies in effectively capturing subtle pairwise relationships, making it suitable for fine-grained hallucination detection tasks.

\sssec{Ensemble model}.
The ensemble model combines predictions from multiple base classifiers to improve detection robustness. By aggregating outputs from diverse classifiers like logistic regression and MLP, the ensemble structure balances the simplicity of interpretable models with the expressiveness of complex ones. Its primary strength lies in mitigating overfitting and ensuring robustness, especially in datasets with varied and complex feature distributions.

For the submodel of the 2 network structures, we choose 2 types of submodels:

\sssec{MLP}.
The MLP is a feedforward neural network capable of learning nonlinear relationships between features. It consists of multiple hidden layers with activation functions, enabling it to capture complex patterns in the data. In hallucination detection, the MLP excels at leveraging intricate dependencies among features, providing higher accuracy compared to simpler models at the cost of increased computational complexity.

\sssec{Logistic regression}.
Logistic regression is a simple yet effective model that learns a weighted combination of features to make predictions. In the context of hallucination detection, it provides a baseline classifier that is computationally efficient and interpretable. Its primary advantage is the ease of training and the ability to identify key features driving the detection decision.

\begin{table*}[htbp!]
\begin{tabular}{M{2cm}|M{4.5cm}|M{8cm}|M{1.5cm}}
\hline
Feature stage & Feature & Computation forum & References \\ \hline\hline
\multirow{2}{*}{Understanding} & Attention lookback ratio & $\frac{\sum_{i < t} \alpha_{l,h,i \rightarrow t}}{\sum_{j} \alpha_{l,h,j \rightarrow t}}$ &  \\ \cline{2-4} 
 & Attention allocation sharpness & $-\sum_{j} p_{l,h,j \rightarrow t} \log p_{l,h,j \rightarrow t}$ &  \\ \hline
 
\multirow{3}{*}{Query} & Last layer layer representation & $h_{L, t}$ &  \\ \cline{2-4} 
 & Activation map & $\text{GELU}(\mathbf{W}_1 \mathbf{h}_{l, t} + \mathbf{b}_1)$ &  \\ \cline{2-4} 
 & Activation entropy & $-\sum_{j} p_{l,j \rightarrow t} \log p_{l,j \rightarrow t}$ &  \\ \hline
 
\multirow{3}{*}{Generation} & Max token ranks & $\max_{t} ( \text{rank}(\text{softmax}(\iota _{l, t}))$ &  \\ \cline{2-4} 
 & Min token probs & $\min_{t} \left( \text{softmax}(\iota _{l, t}) \right)$ &  \\ \cline{2-4} 
 & Joint token probs & $\prod_{t} \text{softmax}(\iota _{l, t})$ &  \\ 
 \hline\hline
\end{tabular}
\caption{\sysname extracts 8 features from 3 stages' internal states.}
\label{tab:features}
\end{table*}