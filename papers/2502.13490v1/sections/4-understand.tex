\section{Understanding Internal State}\label{sec:understand}

In this section, we provide an understanding of how the internal state changes during the lens of LLM inference (\S\ref{sec:understand:infer}), inference with RAG (\S\ref{sec:understand:rag}).

\subsection{Understanding through Lens of Inference}\label{sec:understand:infer}

\begin{table*}[h!]
\centering
\begin{tabular}{|M{3cm}|M{3cm}|M{3cm}|M{3cm}|M{3cm}|}
\hline\hline
\multicolumn{5}{|c|}{
\includegraphics[width=9cm]{figures/fig-infer-legend.pdf}
}
\\
\hline
\multicolumn{2}{|c|}{Attention} & \multicolumn{1}{|c|}{Activation} & \multicolumn{2}{|c|}{Logit} \\
\hline\hline

\multicolumn{5}{|c|}{Dataset: HaluEval} \\
\hline

Lookback ratio & Attention entropy & Hidden states & Min. token prob. & Joint token prob. \\
\hline

\includegraphics[width=3cm]{figures/fig-understanding-infer-HaluEval-avg_lookback_ratio.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-HaluEval-avg_input_context_sharpness.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-HaluEval-avg_hidden_state.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-HaluEval-min_token_probs.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-HaluEval-joint_token_probs.pdf} \\
\hline

Lookback ratio & Attention entropy & Activation sharpness & Max. token rank & Avg. dist. divergence \\
\hline

\includegraphics[width=3cm]{figures/fig-understanding-infer-HaluEval-avg_lookback_ratio-per_layer.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-HaluEval-avg_input_context_sharpness-per_layer.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-HaluEval-max_activation_entropy.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-HaluEval-max_token_ranks.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-HaluEval-avg_jsd.pdf} \\

\hline\hline

\multicolumn{5}{|c|}{Dataset: CNNDM} \\
\hline

Lookback ratio & Attention entropy & Hidden states & Min. token prob. & Joint token prob. \\
\hline

\includegraphics[width=3cm]{figures/fig-understanding-infer-CNNDM-avg_lookback_ratio.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-CNNDM-avg_input_context_sharpness.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-CNNDM-avg_hidden_state.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-CNNDM-min_token_probs.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-CNNDM-joint_token_probs.pdf} \\
\hline

Lookback ratio & Attention entropy & Activation sharpness & Max. token rank & Avg. dist. divergence \\
\hline

\includegraphics[width=3cm]{figures/fig-understanding-infer-CNNDM-avg_lookback_ratio-per_layer.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-CNNDM-avg_input_context_sharpness-per_layer.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-CNNDM-max_activation_entropy.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-CNNDM-max_token_ranks.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-infer-CNNDM-avg_jsd.pdf} \\

\hline\hline

\end{tabular}
\caption{We visualize the comparison of attention-type features between inference with RAG and without RAG.}
\label{fig:understand:infer}
%\vspace{-15pt}
\end{table*}

\begin{figure}[t]
\centering
      \begin{minipage}{\columnwidth}
          {\centering{\hspace{1.5cm}CNNDM\hspace{3cm}HaluEval}}
      \end{minipage}
      \subfloat[Avg. jsd]{\includegraphics[width=0.5\columnwidth]{figures/fig-understanding-infer-key-CNNDM-avg_jsd.pdf}}
      \subfloat[Avg. jsd]{\includegraphics[width=0.5\columnwidth]{figures/fig-understanding-infer-key-HaluEval-avg_jsd.pdf}}
      \\
      \subfloat[Min. prob.]{\includegraphics[width=0.5\columnwidth]{figures/fig-understanding-infer-key-CNNDM-min_token_probs.pdf}}
      \subfloat[Min. prob.]{\includegraphics[width=0.5\columnwidth]{figures/fig-understanding-infer-key-HaluEval-min_token_probs.pdf}}
      \\
      \subfloat[Joint prob.]{\includegraphics[width=0.5\columnwidth]{figures/fig-understanding-infer-key-CNNDM-joint_token_probs.pdf}}
      \subfloat[Joint prob.]{\includegraphics[width=0.5\columnwidth]{figures/fig-understanding-infer-key-HaluEval-joint_token_probs.pdf}}
      \\
      \subfloat[Max. rank]{\includegraphics[width=0.5\columnwidth]{figures/fig-understanding-infer-key-CNNDM-max_token_ranks.pdf}}
      \subfloat[Max. rank]{\includegraphics[width=0.5\columnwidth]{figures/fig-understanding-infer-key-HaluEval-max_token_ranks.pdf}}
      \\
      \caption{
      For the logit class internal state features, some layers of features seem to be very discriminative. We visualize the specific distribution of all samples of the features of these layers and show them in the figure. It can be seen that although there is a big difference in the overall mean, the overall distribution of this type of features of the hallucination output and the actual output overlap, and it is difficult to say that it is a very discriminative feature.
      }
    \label{fig:understand:key:logit}
\end{figure}

\sssec{Method}.
We experimented to compare internal states that differ from the factual output and hallucinated output across the whole inference process.
Specifically, we used 2 datasets: HaluEval for factual hallucination generation, and CNNDM for faithful hallucination generation.
We collected the internal states of tested models using the 2 datasets and computed the extracted features.
At last, we compared internal states between hallucinated output and factual output.
The results are shown in Table \ref{fig:understand:infer}.

\sssec{Results on attention states}.  
The lookback ratio (\(\text{LR}\)) measures the proportion of attention directed toward previous tokens. In the HaluEval dataset, both factual (\(\mathcal{F}\)) and hallucinated (\(\mathcal{H}\)) outputs exhibit overlapping trends across attention heads (\(h \in H\)) and attention layers (\(l \in L\)), implying that \(\text{LR}_{l,h}\) does not effectively distinguish between \(\mathcal{F}\) and \(\mathcal{H}\). However, in the CNNDM dataset, \(\text{LR}_{l,h}\) demonstrates a clearer separation, with \(\mathcal{F}\) maintaining higher and more stable values compared to \(\mathcal{H}\) (\( \text{LR}^{\mathcal{F}}_{l,h} > \text{LR}^{\mathcal{H}}_{l,h}, \forall l,h\)). This suggests that hallucinations in CNNDM are associated with reduced attention to prior context, making \(\text{LR}\) a more sensitive feature for hallucination detection in this dataset.

The attention entropy (\(\mathcal{E}_l\)) quantifies the concentration of attention distributions. For HaluEval, results show that \(\mathcal{F}\) exhibits lower entropy (\(\mathcal{E}_l^{\mathcal{F}} < \mathcal{E}_l^{\mathcal{H}}\)), indicating more focused attention, whereas \(\mathcal{H}\) outputs have higher entropy, reflecting dispersed attention. Aggregated across layers (\(\bar{\mathcal{E}}_l\)), this trend persists. In CNNDM, although \(\mathcal{E}_l^{\mathcal{H}} > \mathcal{E}_l^{\mathcal{F}}\), the distinction is less pronounced. These results suggest that \(\mathcal{E}_l\) robustly captures the dispersion of attention in \(\mathcal{H}\), particularly for HaluEval.

In summary, \(\text{LR}\) is dataset-sensitive, effectively distinguishing hallucinations in CNNDM but not in HaluEval, whereas \(\mathcal{E}_l\) consistently differentiates between \(\mathcal{H}\) and \(\mathcal{F}\) in both datasets, highlighting dispersed attention as a hallmark of hallucinations.

\sssec{Results on activation states}.  
The hidden states (\(h_{l,t}\)) represent activation values at layer \(l\) for token \(t\). Across both datasets, factual and hallucinated outputs exhibit overlapping trends in \(\bar{h}_{l,t}\), indicating that \(\bar{h}_{l,t}^{\mathcal{F}} \approx \bar{h}_{l,t}^{\mathcal{H}}\). This overlap suggests that \(h_{l,t}\) lacks sensitivity to the nuanced differences introduced by hallucinations.

Activation sharpness (\(S_l\)) measures the concentration of activations across layers. In both datasets, \(S_l^{\mathcal{F}}\) and \(S_l^{\mathcal{H}}\) follow nearly identical trends, with a consistent decrease after the 20th layer (\(\forall l > 20, S_l^{\mathcal{F}} \approx S_l^{\mathcal{H}}\)). This similarity suggests that activation sharpness fails to capture hallucination-specific patterns. The uniformity in \(\bar{S}_l\) across \(\mathcal{F}\) and \(\mathcal{H}\) implies that these features reflect general activation dynamics rather than hallucination-induced variations.

Thus, neither \(h_{l,t}\) nor \(S_l\) effectively distinguishes between hallucinations and factual outputs, underscoring their limited utility for this task.

\begin{figure}t]
\centering
      \subfloat[CNNDM]{\includegraphics[width=0.5\columnwidth]{figures/fig-understanding-infer-key-CNNDM-avg_input_context_sharpness.pdf}}
      \subfloat[HaluEval]{\includegraphics[width=0.5\columnwidth]{figures/fig-understanding-infer-key-HaluEval-avg_input_context_sharpness.pdf}}
      \caption{
      For the attention entropy internal state features, we visualize the specific distribution of all samples of the features of these layers and show them in the figure.
      }
    \label{fig:understand:key:atten_entropy}
\end{figure}

\sssec{Results on logit states}.  
The minimum token probability (\(\min(\text{P}_{l,t})\)) captures the model's confidence in its least likely predicted token. In HaluEval, \(\min(\text{P}_{l,t}^{\mathcal{H}}) < \min(\text{P}_{l,t}^{\mathcal{F}})\), but the difference is slight, whereas in CNNDM, this disparity is more pronounced (\(\forall l, \min(\text{P}_{l,t}^{\mathcal{H}}) \ll \min(\text{P}_{l,t}^{\mathcal{F}})\)), indicating greater uncertainty in hallucinated outputs.

The joint token probability (\(\prod_t \text{P}_{l,t}\)) reflects cumulative confidence. In HaluEval, \(\prod_t \text{P}_{l,t}^{\mathcal{F}} \approx \prod_t \text{P}_{l,t}^{\mathcal{H}}\), while in CNNDM, hallucinated outputs exhibit lower joint probabilities (\(\prod_t \text{P}_{l,t}^{\mathcal{H}} < \prod_t \text{P}_{l,t}^{\mathcal{F}}\)) in later layers.

The maximum token rank (\(\max(\text{R}_{l,t})\)) indicates the rank of the least confident token. In HaluEval, \(\max(\text{R}_{l,t})\) trends for \(\mathcal{F}\) and \(\mathcal{H}\) largely overlap (\(\max(\text{R}_{l,t}^{\mathcal{F}} \approx \max(\text{R}_{l,t}^{\mathcal{H}})\)), but in CNNDM, hallucinated outputs consistently exhibit higher ranks (\(\max(\text{R}_{l,t}^{\mathcal{H}}) > \max(\text{R}_{l,t}^{\mathcal{F}}\)).

The Jensen-Shannon Divergence (JSD) measures distribution divergence. In both datasets, JSD trends for \(\mathcal{F}\) and \(\mathcal{H}\) are nearly identical (\(\text{JSD}_{l,t}^{\mathcal{F}} \approx \text{JSD}_{l,t}^{\mathcal{H}}\)), indicating limited sensitivity to hallucination-specific variations.

In conclusion, logit features such as \(\min(\text{P}_{l,t})\) and \(\max(\text{R}_{l,t})\) are more effective in CNNDM than in HaluEval, reflecting the stronger correlation between reduced confidence and hallucinations in CNNDM. However, features like \(\prod_t \text{P}_{l,t}\) and JSD show limited efficacy across both datasets.

\subsection{Impact of RAG during Inference}\label{sec:understand:rag}

\begin{table*}[h!]
\centering
\begin{tabular}{|M{3cm}|M{3cm}|M{3cm}|M{3cm}|M{3cm}|}
\hline\hline
\multicolumn{5}{|c|}{
\includegraphics[width=9cm]{figures/fig-rag-legend.pdf}
}
\\
\hline
\multicolumn{2}{|c|}{Attention} & \multicolumn{1}{|c|}{Activation} & \multicolumn{2}{|c|}{Logit} \\
\hline\hline

Lookback ratio & Attention entropy & Hidden states & Min. token prob. & Joint token prob. \\
\hline

\includegraphics[width=3cm]{figures/fig-understanding-rag-HaluEval-avg_lookback_ratio.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-rag-HaluEval-avg_input_context_sharpness.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-rag-HaluEval-avg_hidden_state.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-rag-HaluEval-min_token_probs.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-rag-HaluEval-joint_token_probs.pdf} \\
\hline

Lookback ratio & Attention entropy & Activation sharpness & Max. token rank & Avg. dist. divergence \\
\hline

\includegraphics[width=3cm]{figures/fig-understanding-rag-HaluEval-avg_lookback_ratio-per_layer.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-rag-HaluEval-avg_input_context_sharpness-per_layer.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-rag-HaluEval-max_activation_entropy.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-rag-HaluEval-max_token_ranks.pdf} &
\includegraphics[width=3cm]{figures/fig-understanding-rag-HaluEval-avg_jsd.pdf} \\

\hline\hline

\end{tabular}
\caption{We visualize the comparison of attention-type features between inference with RAG and without RAG.}
\label{fig:understand:rag}
%\vspace{-15pt}
\end{table*}

\sssec{Method}.
We utilized the HaluEval dataset to investigate the effect of RAG on LLM inference. For each question, the answer was converted into a retrieval-augmented knowledge base (RAG) to assist the LLM. Internal states, including attention scores, hidden states, and logits, were extracted during inference under two settings: with and without RAG. Features such as attention lookback ratio, activation entropy, and token probabilities were computed and compared between the two conditions, focusing on correct responses to analyze how RAG influences the modelâ€™s reasoning process and mitigates hallucinations.


\sssec{Results on attention states}.  
The lookback ratio (\(\text{LR}_{l,h}\)) reflects the model's focus on prior tokens. At the head level, the figure shows the distribution of \(\text{LR}_{l,h}\) across all 1024 attention heads (\(h \in H\)), while at the layer level, it aggregates over the 32 heads per layer (\(l \in L\)). The results indicate that \(\bar{\text{LR}}_l^{\text{RAG}}\) (with RAG) is more consistent and slightly higher than \(\bar{\text{LR}}_l^{\text{non-RAG}}\) (without RAG), particularly across deeper layers (\(l > 10\)). This consistency arises because RAG provides external context, enhancing backward focus and enabling effective utilization of retrieved information. Without RAG, the model depends entirely on internal context, leading to greater variability (\(\text{Var}(\bar{\text{LR}}_l^{\text{non-RAG}}) > \text{Var}(\bar{\text{LR}}_l^{\text{RAG}})\)).

The attention entropy (\(\mathcal{E}_{l,h}\)) quantifies the diversity of attention distributions. At the head level, \(\mathcal{E}_{l,h}^{\text{RAG}}\) is consistently higher than \(\mathcal{E}_{l,h}^{\text{non-RAG}}\), and at the layer level, the average entropy \(\bar{\mathcal{E}}_l^{\text{RAG}}\) also demonstrates broader focus compared to \(\bar{\mathcal{E}}_l^{\text{non-RAG}}\). This is because RAG enriches attention mechanisms with external knowledge, allowing attention to be allocated across a wider set of tokens (\(\mathcal{E}_{l,h}^{\text{RAG}} > \mathcal{E}_{l,h}^{\text{non-RAG}}, \forall l,h\)). Conversely, without RAG, attention is constrained to narrower contexts, resulting in lower entropy values.


\sssec{Results on activation states}.  
The hidden states (\(h_{l,t}\)) represent the activation magnitudes across neurons for token \(t\) at layer \(l\). At the neuron level, the distribution of \(\text{Avg}(h_{l,t})\) shows that RAG stabilizes activations, reducing variability (\(\text{Var}(h_{l,t}^{\text{RAG}}) < \text{Var}(h_{l,t}^{\text{non-RAG}})\)). At the layer level, the average activations (\(\bar{h}_l\)) are consistently higher with RAG (\(\bar{h}_l^{\text{RAG}} > \bar{h}_l^{\text{non-RAG}}\)) due to the additional context provided by RAG, which enhances the magnitude and consistency of neuron activations. Without RAG, activations are unstable, occasionally exhibiting outliers.

Activation sharpness (\(S_l\)), which reflects the concentration of activation values, is higher in earlier layers (\(l \leq 10\)) for both RAG and non-RAG settings. However, the decline in sharpness is more gradual with RAG, maintaining sharper activations (\(S_l^{\text{RAG}} > S_l^{\text{non-RAG}}, \forall l > 10\)). This phenomenon suggests that RAG helps preserve activation focus across deeper layers, likely due to the inclusion of external context. Without RAG, sharpness deteriorates more rapidly as activations diffuse across the network.

\sssec{Results on logit states}.  
The minimum token probability (\(\min(\text{P}_{l,t})\)) represents the model's confidence in its least likely predicted token. Both the head-level and layer-level trends show that \(\min(\text{P}_{l,t})\) increases sharply in later layers (\(l > 10\)). The difference between RAG and non-RAG settings is negligible (\(\min(\text{P}_{l,t}^{\text{RAG}} \approx \min(\text{P}_{l,t}^{\text{non-RAG}})\)), indicating that this feature is primarily influenced by internal computations rather than external context.

The joint token probability (\(\prod_t \text{P}_{l,t}\)) reflects overall confidence in generating a sequence. Across layers, \(\prod_t \text{P}_{l,t}^{\text{RAG}} > \prod_t \text{P}_{l,t}^{\text{non-RAG}}\), with the gap being more pronounced in later layers (\(l > 15\)). This improvement arises from RAG's ability to provide richer context, enhancing the model's cumulative confidence in token generation.

The average distribution divergence (\(\text{D}_{\text{avg}}\)) measures uncertainty by comparing predicted and reference distributions. In both RAG and non-RAG settings, \(\text{D}_{\text{avg}}\) decreases consistently across layers (\(l > 5\)), showing that uncertainty is reduced through layer-wise refinement. The difference between RAG and non-RAG is minimal (\(\text{D}_{\text{avg}}^{\text{RAG}} \approx \text{D}_{\text{avg}}^{\text{non-RAG}}\)), suggesting that uncertainty reduction is primarily driven by internal processes.

The maximum token rank (\(\max(\text{R}_{l,t})\)) captures the relative position of the least confident token in the ranking. Across layers, \(\max(\text{R}_{l,t})\) decreases consistently, reflecting growing confidence in top predictions. There is no significant distinction between RAG and non-RAG (\(\max(\text{R}_{l,t}^{\text{RAG}} \approx \max(\text{R}_{l,t}^{\text{non-RAG}})\)), indicating that token ranking is predominantly determined by internal model dynamics.

%\subsection{Impact of Fine-tuning during Inference}\label{sec:understand:finetune}

%\sssec{Method}.

%\subsection{Feature Correlation Analysis}\label{sec:understand:corr}