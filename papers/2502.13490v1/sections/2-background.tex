\section{Preliminary}\label{sec:back}

In this section, we first explore the preliminary definition of LLM hallucination (\S\ref{sec:back:halu}) and the process of LLM inference (\S\ref{sec:back:infer}).

\subsection{LLM Hallucination}\label{sec:back:halu}

\sssec{LLM hallucination definition}.
Hallucination in large language models (LLMs) refers to the generation of content that is inconsistent with reality or factual information \cite{tonmoy2024comprehensive, rawte2023survey, huang2023survey}. 
While this generated content may sound plausible, it is often based on patterns learned from training data rather than actual facts. 
The issue of hallucination significantly impacts the practical applications of LLMs, especially in scenarios requiring high accuracy and reliability, such as healthcare, legal contexts, and journalism \cite{zhang2023siren}.

\sssec{LLM hallucination types}.
Hallucinations in large language models can be categorized into two types:
\begin{enumerate}[label={[\arabic*]}, itemsep=0pt, leftmargin=*,topsep=0pt]
    \item \underline{Input-conflicting hallucination}: This occurs when the content generated by the model deviates from the userâ€™s task instruction or input \cite{tonmoy2024comprehensive, rawte2023survey}. For instance, if a user asks for a dinner recipe, the model might mistakenly provide a lunch suggestion.
    \item \underline{Context-conflicting hallucination}: In multi-turn generation or lengthy content, models may exhibit self-contradictions \cite{tonmoy2024comprehensive, huang2023survey, zhang2023siren}. This type arises when models lose track of the context or fail to maintain consistency throughout the conversation. For example, a model might introduce the current NBA commissioner, Adam Silver, but later refer to a former commissioner, David Stern.
\end{enumerate}

\sssec{Potential causes of LLM hallucination}.
Causes of hallucinations can be analyzed from several perspectives:
\begin{enumerate}[label={[\arabic*]}, itemsep=0pt, leftmargin=*,topsep=0pt]
    \item \underline{Data-related issues}: The training data for LLMs is often collected from the internet, containing significant amounts of outdated, inaccurate, or biased information \cite{tonmoy2024comprehensive, rawte2023survey, zhang2023siren}. These data quality issues can lead to the generation of hallucinated content .
    \item \underline{Limitations of the training process}: Models learn language patterns by predicting the next word rather than understanding the content deeply \cite{rawte2023survey, huang2023survey, ji2023survey}. Additionally, misalignment during fine-tuning (for example, through reinforcement learning from human feedback, or RLHF) can contribute to hallucinations .
    \item \underline{Randomness in the inference stage}: During content generatce inconsistent or contextually irrelevant outputs due to randomness in sampling strategies or decoding methods \cite{tonmoy2024comprehensive, huang2023survey, ji2023survey}.
\end{enumerate}

%\sssec{Dangers of hallucinations}.

%\begin{figure*}[htbp!]
%    \centering
%    \includegraphics[width=\textwidth]{figures/fig-scheme.pdf}
%    \caption{
%    The workflow of \sysname has four stages:
%    (1) Extract internal state from the inference process;
%    (2) Select tokens for hallucination detection;
%    (3) Compute features;
%    (4) Detect hallucination using model.
%    }
%    \label{fig:scheme}
%\end{figure*}

\subsection{LLM Inference Process}\label{sec:back:infer}

During the inference process of a large-scale Transformer model such as Llama, the whole process can be divided into several key stages (see Figure \ref{fig:infer}).

\sssec{Input processing}. The first is the input processing stage, in which the input text is tokenized and converted into a format that the model can understand, generating corresponding token IDs \cite{mickus2022dissect}. Subsequently, these token IDs are input into the embedding layer to form word embeddings, which provide the basis for the model's subsequent understanding. In this way, the text information is effectively converted into numerical representations for subsequent calculations \cite{dar2022analyzing}.

\sssec{Understanding}. The next understanding stage processes the input tokens through a multi-head self-attention mechanism \cite{vaswani2017attention, chefer2021transformer, vig2019analyzing}. This mechanism calculates the similarity between tokens and generates contextual representations to capture long-range dependencies \cite{yeh2023attentionviz}. This process enables the model to fully understand the semantics of each token in a specific context and provide rich semantic information for the subsequent query stage \cite{rigotti2021attention, chefer2021transformer}.

\sssec{Query}. In the query stage, the model further processes the contextual representations generated in the understanding stage through a feedforward network \cite{zhao2024explainability, meng2023locating}. The feedforward network contains two linear transformations and nonlinear activation functions (such as ReLU) to generate new feature representations. The goal of this stage is to extract higher-level abstract information, improve the model's expressiveness, and lay the foundation for the final generation process.

\sssec{Generation}. The generation stage is the core of reasoning. The model converts the output of the query stage into the probability distribution of the next token through linear transformation and softmax function. In this process, the model uses contextual information to determine the most likely next token and generates a continuous text sequence. The decoding strategy of this stage, such as greedy search or beam search, directly affects the quality and coherence of the generated text.

\sssec{Post processing}. Finally, in the post-processing stage, the generated token ID sequence is converted back to a readable text format. By merging subwords or words and removing special tokens, the output of the model is converted into human-understandable language. This stage ensures the readability of the generated content and enables users to interact directly with the model output. Through the close combination of these stages, the Transformer model realizes an effective reasoning process from input to output.