\section{Proposed Scheme: \sysname}\label{sec:scheme}


Our proposed scheme consists of three parts:
(1) Internal state extraction (\S\ref{sec:scheme:state_extract}): \sysname extracts internal states during the inference, and stores them locally.
(2) Token selection (\S\ref{sec:scheme:token_select}): \sysname selects target tokens for next stage feature extraction.
(3) Feature extraction (\S\ref{sec:scheme:feature_extract}): \sysname extracts target features from the internal states, we also discuss different feature selection and token selection for better analysis.
%(4) Detection (\S\ref{sec:detect}): we discuss how to build a detection model using these features.

\subsection{Internal State Extraction}\label{sec:scheme:state_extract}

Following the definition of \S\ref{sec:back:infer}, \sysname first extracts three types of internal state from the three stages during inference:

\sssec{Understanding: attention}.
Attention is the internal state in which LLMs understand the input context.
We extract each attention score matrix from each transformer layer's head for each token $t$ as:
\begin{equation}
    \left[ \alpha_{l,h,i} \right]_{L \times H \times t}=\begin{bmatrix}
    \alpha_{1,1,i} & \alpha_{1,2,i} & \dots & \alpha_{1,H,i} \\
    \alpha_{2,1,i} & \alpha_{2,2,i} & \dots & \alpha_{2,H,i} \\
    \vdots & \vdots & \ddots & \vdots \\
    \alpha_{L,1,i} & \alpha_{L,2,i} & \dots & \alpha_{L,H,i}
    \end{bmatrix}\times t
\end{equation}
where ${\alpha}_{l,h,i}$ denotes the $l$-th layer's $h$-th head's attention score for token $t$ on token $i$.

\sssec{Query: layer representation}.
The query stage involves processing contextual representations through a feedforward network to derive higher-level abstractions. We capture the layer representation matrices from each transformer layer for each token \( t \) as follows:
\begin{equation}
    \left[ \gamma _{l,i} \right]_{L \times t}=\begin{bmatrix}
    \gamma _{1,i} \\
    \gamma _{2,i} \\
    \vdots \\
    \gamma _{L,i}
    \end{bmatrix}\times t
\end{equation}
where \( \gamma _{l,i} \) represents the \( l \)-th layer's layer representation for token \( t \) during the query phase.

\sssec{Generation: logit}.
In the generation stage, the model transforms layer representations into logits, converting these outputs into probabilities via the softmax function to determine the likelihood of each next token. Each logit score for token \( t \) is calculated as:
\begin{equation}
    \left[ \iota _{l,i} \right]_{L \times t}=\begin{bmatrix}
    \iota _{1,i} \\
    \iota _{2,i} \\
    \vdots \\
    \iota _{L,i}
    \end{bmatrix}\times t
\end{equation}
where \( \iota _{l,i} \) denotes the \( l \)-th layer’s logit score for token \( t \) in the generation phase, representing the raw output before applying the softmax.

\subsection{Token Selection}\label{sec:scheme:token_select}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/fig-token.pdf}
    \caption{
    Five token selection methods for \sysname:
    (1) All tokens;
    (2) First token;
    (3) Last token;
    (4) Per token;
    (5) Sliced windows.
    }
    \label{fig:token}
\end{figure*}

Unlike previous external methods to detect hallucinations, internal state-based detection relies heavily on selecting tokens.
Each internal state including attention, layer representation, and logit is highly relying on tokens.
Thus we provided five different token selection methods inspired from previous works:

\sssec{All tokens selection}.
Aggregate the features of all tokens (e.g., max, mean) and then input the aggregated result into the $is\_halu$ function to determine if the entire response is hallucinatory.

\[
is\_halu\_all = is\_halu\left(f\left(\{feature(t_i)\}_{i=1}^{N}\right)\right)
\]

where \( t_i \) represents each token in the response, \( N \) is the total number of tokens, and \( f \) is an aggregation function (e.g., max, mean).


\sssec{Per token selection}.
Apply the $is_halu$ function individually to each token. If any token is identified as hallucinatory, the entire response is considered hallucinatory.

\[
is\_halu\_per\_token = \bigvee_{i=1}^{N} is\_halu(t_i)
\]

\sssec{Last token selection}.
Apply the $is_halu$ function only to the last token, using its result to determine if the entire response is hallucinatory.
\[
is\_halu\_last = is\_halu(t_N)
\]
where \( t_N \) is the last token in the response.

\sssec{First token selection}.
Apply the $is_halu$ function only to the first token, using its result to determine if the entire response is hallucinatory.
\[
is\_halu\_first = is\_halu(t_1)
\]
where \( t_1 \) is the first token in the response.

\sssec{Sliced window selection}.
Divide all tokens into multiple sliding windows, where each window contains \( w \) consecutive tokens and slides with a stride \( s \). Apply the $is_halu$ function to each window. If any window is identified as hallucinatory, the entire response is considered hallucinatory.
\[
is\_halu\_sliced = \bigvee_{k=1}^{M} is\_halu(\{t_{k}, t_{k+1}, \dots, t_{k+w-1}\})
\]
where \( M \) is the number of windows, \( w \) is the window size, and \( k \) is the starting index of each window (ranging from 1 to \( N-w+1 \)). The symbol \( \bigvee \) denotes a logical OR operation. If any token group within a window is identified as hallucinatory, the entire response is considered hallucinatory.

We provided examples for each token selection method in Figure \ref{fig:token}.

\subsection{Feature Extraction}\label{sec:scheme:feature_extract}

We extract three groups of features from the extracted layer representations:

\sssec{Attention lookback ratio}.
For each token $t$, we calculate the lookback ratio for each layer and attention head $h$, defined as the proportion of attention score directed to previous tokens among all attention scores for that token:
\[
\frac{\sum_{i < t} \alpha_{l,h,i \rightarrow t}}{\sum_{j} \alpha_{l,h,j \rightarrow t}}
\]
The lookback ratio quantifies the model's focus on historical context. When a model generates hallucinations, it may overlook parts of the contextual information, so a low Lookback Ratio might indicate insufficient use of historical context during generation.

\sssec{Attention allocation sharpness}.
Attention allocation sharpness reflects the concentration of the attention distribution for each token. By computing the entropy of token $t$'s attention distribution in layer $l$ and head $h$, we obtain the Sharpness:
\[
-\sum_{j} p_{l,h,j \rightarrow t} \log p_{l,h,j \rightarrow t}
\]
where
\[
p_{l,h,j \rightarrow t} = \frac{\alpha_{l,h,j \rightarrow t}}{\sum_{k} \alpha_{l,h,k \rightarrow t}}
\]
Attention allocation sharpness indicates whether the model's attention is focused on a few important tokens. Low entropy suggests more concentrated attention, helping us understand if the model might be overly focused on particular tokens, potentially leading to hallucination.

\sssec{Last layer layer representation}.
The last layer representation is extracted directly from the layer representation of the last layer for each token:
\[
\gamma_{L, t}
\]
where $\gamma_{L, t} \in R^d$ represents the layer representation vector of the $L$-th (last) layer for the token $t$. Here:
\begin{itemize}
    \item $d$ is the dimensionality of the model's layer representations (typically the size of the embedding dimension).
    \item $L$ is the total number of transformer layers in the model.
\end{itemize}
The layer representation in the last layer represents the model's final contextual embedding, which directly influences the generated output. Analyzing this layer representation can help identify potential causes of hallucinations in the generated content.

\sssec{Activation map}
The activation map at layer $l$ for a token $t$ is computed from the layer representation $\gamma_{l, t} \in R^d$ as follows:

\begin{enumerate}
    \item Linear Transformation:
    \begin{equation}
    z_{l, t} = W_1 \gamma_{l, t} + b_1,
    \end{equation}
    where $W_1 \in R^{m \times d}$ is the weight matrix, $m$ is the intermediate dimensionality of the feed-forward network, and $b_1 \in R^m$ is the bias vector.

    \item Non-linear Activation:
    \begin{equation}
    a_{l, t} = \text{GELU}(z_{l, t}),
    \end{equation}
    where $a_{l, t} \in R^m$ is the activation map, and $\text{GELU}$ denotes the Gaussian Error Linear Unit activation function.
\end{enumerate}

The resulting activation map $a_{l, t}$ illustrates the degree of activation for $m$ neurons in the feed-forward network at layer $l$ for token $t$.


\sssec{Activation entropy}
Activation entropy is calculated as the entropy of the activation states in each layer, capturing the distribution of activations:
\[
-\sum_{j} p_{l,j \rightarrow t} \log p_{l,j \rightarrow t}
\]
where
\[
p_{l,j \rightarrow t} = \frac{a_{l,j \rightarrow t}}{\sum_{k} a_{l,k \rightarrow t}}
\]
Activation entropy reflects the dispersion or concentration of the model's activation states. Low entropy indicates focused activation on a few tokens, which could signify overconfidence in certain tokens, potentially leading to hallucination.

\sssec{Min token probs}
Min token probs represent the lowest softmax probability across tokens in each layer's logit output:
\[
\min_{t} \left( \text{softmax}(\iota_{l, t}) \right)
\]
Low probability values indicate a lack of confidence in certain tokens. These extremely low probabilities can help identify potential points of hallucination risk.

\sssec{Max token ranks}
Max token ranks represent the rank of the token with the lowest probability across a sequence within each layer’s softmax logits:
\[
\max_{t} \left( \text{rank}(\text{softmax}(\iota_{l, t})) \right)
\]
The token with the lowest rank may indicate low-priority content during generation. High-rank values suggest that the model may hallucinate by producing low-priority tokens in the sequence.

\sssec{Joint token probs}.
Joint token probs represent the product of probabilities of all tokens within a sequence in each layer’s softmax logits:
\[
\prod_{t} \text{softmax}(\iota_{l, t})
\]
Joint token probs provides a measure of overall confidence in generating the sequence. A low joint probability suggests low model confidence in the entire sequence, which could result in hallucinated content.

