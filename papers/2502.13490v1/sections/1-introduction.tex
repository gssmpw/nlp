\section{Introduction}\label{sec:intro}

%\begin{figure}[htbp]
%    \centering
%    \includegraphics[width=0.5\textwidth]{figures/fig-example.pdf}
%    \caption{An example of \sysname's whole process.}
%    \label{fig:example}
%\end{figure}

%The large language model has been widely used in many areas. But people lack trust in it due to hallucinations

Large language models (LLM) have been widely adopted across various fields, ranging from natural language processing and generation tasks to specialized applications in healthcare \cite{yang2022large, thirunavukarasu2023large, peng2023study}, finance \cite{wu2023bloomberggpt}, and legal services \cite{cui2023chatlaw, guha2024legalbench}. 
However, despite its impressive capabilities, a fundamental issue remains: users often lack trust in LLMs due to their tendency to produce ``hallucinations'' \cite{huang2023survey, li2024dawn}.
Hallucination in the context of LLMs refers to the phenomenon where the model generates content that appears plausible but is factually inaccurate or misaligned with the provided context. 
This can result in responses that sound coherent and authoritative, yet introduce misleading or completely false information \cite{mckenna2023sources}.

%Current works towards the hallucination is using some outside knowlegde to intervene in it. However these works suffer from xxxx

Traditional hallucination detection methods usually focus on post-processing analysis of generated outputs, including checking the factual accuracy of responses through external RAGs \cite{gao2023retrieval, niu2023ragtruth, belyi2024luna, li2024enhancing}, adding prefix/suffix/system prompts according to prompt engineering \cite{hanna2023comparative}, and self-consistency checking by generating multiple outputs \cite{harrington2024mitigating, manakul2023selfcheckgpt, mundler2023self}.
However, these methods have some inherent limitations. The reliance on external databases will introduce additional complexity and computational overhead, especially when a large-scale external knowledge base is required \cite{belyi2024luna}. 
Furthermore, the external base may not always be up-to-date or comprehensive, limiting their effectiveness in detecting certain hallucinations.
Also, the prompt engineering-based methods will introduce additional prompts, which may lead to a decrease in response quality \cite{hanna2023comparative}.
At last, since these methods all use external methods for post-generation intervention, there is a lack of understanding of the source of hallucinations.

Recent studies have begun exploring the use of LLM's internal state for hallucination detection and intervention to overcome the limitations of hallucination detection and intervention methods based on external bases.
The internal state of LLM, including attention weights \cite{beigi2024internalinspector, chuang2024lookback, yuksekgonul2023attention}, layer representation \cite{beigi2024internalinspector, ji2024llm, chen2024context, chen2024inside, su2024unsupervised, duan2024llms, azaria2023internal, he2024llm}, logits \cite{quevedo2024detecting, he2024llm}, etc., provides a state representation of the model's reasoning process during content generation.
By analyzing these internal states, signs of hallucination can be detected before the content is fully generated, thereby achieving real-time intervention and reducing computational costs.
Since no external intervention is required, this series of methods usually requires low computational overhead while allowing private models to be deployed locally (no cloud third-party services are required).
Most importantly, these methods have become more interpretable, opening up new research ideas.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/fig-inference.pdf}
    \caption{The inference process of LLM can be divided into 4 stages: (1) Input preprocess; (2) Understanding; (3) Query; (4) Generation.}
    \label{fig:infer}
\end{figure}

However, existing work based on internal states focuses on using a certain internal state for detection.
They also did not provide sufficient transferability analysis.
There is a lack of a systematic understanding of how large model hallucinations are generated during the internal inference processes.

To meet this gap, we propose \sysname, a framework that systematically extracts the internal states from the large language model inference and provides a deep understanding of the generation of hallucinations in it.
Specifically, we make three key contributions:
\begin{itemize}[itemsep=0pt, leftmargin=*,topsep=0pt]
    \item We divided LLMs' inference process into three stages: understanding, query, and generation, and extracted 8 features from these three stages.
    \item We did a systematical analysis for all three stages's internal states of inference, including understanding, query, and generation. We provide deep analysis both for solely inference and for inference with RAG.
    \item We conducted comprehensive experiments to assess different features' abilities in hallucination detection. We also considered the transferability across different types of hallucinations.
\end{itemize}


The rest of the paper is organized as follows:
We first introduce the preliminary background knowledge in \S\ref{sec:back}, and summarize current works about hallucination detection and mitigation in \S\ref{sec:related}.
Then we introduce the whole design in \S\ref{sec:scheme}.
Next, we provide a deep understanding and observation about different inner states' modes in hallucinated response in \S\ref{sec:understand}.
The systematical detection performance analysis is stated in \S\ref{sec:detect}.
At last, we discuss the limitation and future work in \S\ref{sec:discussion}.