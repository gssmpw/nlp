\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[preprint]{acl}

\makeatletter
\ifacl@anonymize
\gdef\isanonymous{1}
\fi

\input{preamble}

\newcommand{\ours}{\mbox{\textsc{ToolMaker}}\xspace}
\newcommand{\Ours}{\ours}
\newcommand{\ourbenchmark}{\mbox{\textsc{TM-Bench}}\xspace}
\newcommand{\Ourbenchmark}{\ourbenchmark}


\title{LLM Agents Making Agent Tools}



\author{
  \textbf{Georg W\"{o}lflein}\textsuperscript{1,2},
  \textbf{Dyke Ferber}\textsuperscript{1,3},
  \textbf{Daniel Truhn}\textsuperscript{4},
  \textbf{Ognjen Arandjelovi\'{c}}\textsuperscript{2},
  \textbf{Jakob N.~Kather}\textsuperscript{1,3,5}
  \\
  \\
  \textsuperscript{1}Else Kr\"{o}ner Fresenius Center for Digital Health, Technical University Dresden, Germany,
  \\
  \textsuperscript{2}School of Computer Science, University of St Andrews, United Kingdom,
  \\
  \textsuperscript{3}National Center for Tumor Diseases (NCT), Heidelberg University Hospital, Germany,
  \\
  \textsuperscript{4}Department of Diagnostic and Interventional Radiology, University Hospital Aachen, Germany,
  \\
  \textsuperscript{5}Department of Medicine I, University Hospital Dresden, Germany
  \\
  \small{
    \textbf{Correspondence:} \href{mailto:georg@woelflein.de}{georg@woelflein.de}
  }
}


\newcommand{\gw}[2][]{\todo[color=blue!20,#1]{\textbf{GW:} #2}}
\newcommand{\df}[2][]{\todo[color=green!20,#1]{\textbf{DF:} #2}}
\newcommand{\oa}[2][]{\todo[color=orange!20,#1]{\textbf{OA:} #2}}
\newcommand{\jk}[2][]{\todo[color=red!20,#1]{\textbf{JK:} #2}}

\colorlet{cellgreen}{green!20}
\colorlet{cellyellow}{yellow!20}
\colorlet{cellred}{red!20}

\usepackage{titlesec}
\titlespacing{\paragraph}{%
  0pt}{%
  0.2\baselineskip}{%
  1em}%


\usepackage{setspace}
\captionsetup[table]{belowskip=-5pt,aboveskip=4pt}
\captionsetup[figure]{belowskip=-5pt,aboveskip=4pt}



\setstretch{0.98}


\begin{document}

\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setlength{\abovedisplayshortskip}{3pt}
\setlength{\belowdisplayshortskip}{3pt}








\maketitle
\begin{abstract}
  Tool use has turned \glspl{llm} into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. 
  However, these tools must be implemented in advance by human developers, hindering the applicability of \gls{llm} agents in domains which demand large numbers of highly specialised tools, like in life sciences and medicine.
  Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose \ours, a novel agentic framework that autonomously transforms papers with code into \gls{llm}-compatible tools.
  Given a short task description and a repository URL, \ours autonomously installs required dependencies and generates code to perform the task, using a closed-loop self-correction mechanism to iteratively diagnose and rectify errors. 
  To evaluate our approach, we introduce a benchmark comprising 15 diverse and complex computational tasks spanning both medical and non-medical domains with over 100 unit tests to objectively assess tool correctness and robustness.
  \Ours correctly implements 80\% of the tasks, substantially outperforming current state-of-the-art software engineering agents.
  \Ours therefore is a step towards fully autonomous agent-based scientific workflows.
  \ifdefined\isanonymous%
  \else%
    Our code and benchmark is available at \url{https://github.com/KatherLab/ToolMaker}.
  \fi
\end{abstract}


\begin{figure}[t]
  \includegraphics[width=\linewidth]{images/ai_research_assistant.pdf}
  \caption{We envision a future where agents posess dynamic toolsets that can be expanded at runtime. \emph{Tool creation}, studied here, is a crucial step towards this goal.}
  \label{fig:ai_research_assistant}
\end{figure}


\section{Introduction}


Scientific discovery is the foundation for innovation and progress
Traditionally, the underlying research processes that guarantee progress have been entirely reliant on human expertise, involving the formulation of research ideas and hypotheses, the collection of information and analysis of data, the planning and execution of experiments, and iterative refinement to arrive at a solution. 
With the recent development of autonomous agent systems that employ \glspl{llm} to perform tasks through multi-step reasoning and planning, and by utilising tools (external pieces of software that the model can execute), we are at the cusp of a paradigm shift where \gls{ai} can assist throughout entire research projects as a \emph{virtual scientist} (\cref{fig:ai_research_assistant}), rather than being limited to addressing narrowly and a priori defined problems.


Although \gls{llm} agents have shown success for \emph{specific} tasks in domains such as software engineering~\cite{wang2024openhands,yang2024sweagent}, healthcare~\cite{ferber2024autonomous,kim2024mdagents}, law~\cite{li2024legalagentbench}, and scientific research~\cite{swanson2024virtuallab,gao2024empowering,schmidgall2025agentlaboratory}, they struggle to generalise to broader classes of tasks. This limitation arises from their reliance on tools that must be explicitly designed, implemented, and integrated by human developers -- often requiring extensive technical expertise -- before deployment~\cite{ferber2024autonomous,jimenez2024swebench}. While \gls{ai} assistants can support this process, current systems still depend heavily on manual intervention to ensure compatibility and functionality.

To address this, some agentic frameworks have been designed that autonomously craft their own tools~\cite{cai2024toolmakers,yuan2024craft,qian2023creator}.
However, because these methods build each tool from scratch, they inevitably produce simple, narrowly scoped tools tailored to single-dimensional problems -- an approach ill-suited to the complexity of real-world research problems.

In fact, in critical fields such as healthcare, data necessary to build tools from scratch is often inaccessible due to privacy restrictions, preventing agents from using it to build their own solutions. 
Moreover, the complexity of modern scientific tools has increased substantially in terms of computational requirements, data demands, and amount of code involved. 
Lastly, deploying tools in high-stakes applications demands rigorous validation, testing, and quality assurance -- standards that current agent systems cannot realistically meet if required to develop such tools entirely from scratch.

Encouragingly, a growing emphasis on reproducibility within the scientific community has led to an increase in publicly released code accompanying research papers~\cite{zhou2024what}. 
Consequently, a vast array of potential tools now exist as standalone solutions. 
However, many researchers in fields like healthcare, biology, drug development, R\&D are unable to effectively use them due to the technical skills required for their deployment.

Instead of building tools entirely from scratch, we ask the following question: \emph{Can \gls{llm} agents autonomously download, integrate, and execute complex, existing tools to empower researchers with minimal technical expertise in the future?}
Towards this goal, we propose \ours, an agentic framework that autonomously generates LLM-compatible tools from scientific papers and their associated code repositories, bypassing the need for human intermediaries to manually set up, install, and adapt them to fit the requirements of their applications. Given a task description, a scientific paper, and its associated code repository, \ours generates an executable tool that enables \glspl{llm} to perform the task (see \cref{fig:tool_creation}). 


\begin{figure}[t]
  \includegraphics[width=\linewidth]{images/tool_making.pdf}
  \caption{Given a task description, a scientific paper, a link to the associated code repository, and an example of the tool invocation, \ours creates (i) a Docker container in which the tool can be executed, (ii) a Python function that performs the task.}
  \label{fig:tool_creation}
\end{figure}

To evaluate \ours, we introduce \ourbenchmark, a benchmark comprising 15 diverse tasks across various medical disciplines (pathology, radiology, genomics, proteomics), as well as non-medical fields, \eg \glspl{llm} and 3D vision. Unlike existing benchmarks~\cite{jimenez2024swebench,zhuo2024bigcodebench,jain2024livecodebench} which assume pre-installed dependencies for function implementation, \ours operates in a fully open-ended environment. Tasks in our benchmark encompass the entire workflow: downloading resources, managing and resolving dependency issues, reading through large codebases, and implementing, testing, and debugging code. 
\Ourbenchmark includes over 100 unit tests to objectively assess the correctness of the implemented tools.




\section{Related work}


In addition to demonstrating impressive capabilities in generating human-like text, \glspl{llm} such as ChatGPT~\cite{ouyang2022training}, Claude~\cite{anthropic_claude}, Gemini~\cite{google2024gemini} and Llama~\cite{meta2024llama3}, on their own, have shown strong potential in question answering and reasoning on problems in natural science related fields, like math~\cite{shao2024deepseekmathpushinglimitsmathematical}, chemistry~\cite{bran2024augmenting} and healthcare~\cite{singhal2023llms}. However, \glspl{llm} often struggle solving more complex problems directly, especially in situations that require intermediate results from multiple steps~\cite{valmeekam2023planningabilitieslargelanguage}. 
To address this, \gls{llm} agents have been developed which enhance an \gls{llm}'s capabilities by integrating external tools~\cite{schick2023toolformer}.

In the medical domain, \gls{llm} agents have been developed for tasks like clinical decision-making and diagnostics, \eg AgentMD~\cite{jin2024agentmd} creates risk calculators from medical publications, and \citet{ferber2024autonomous} propose an autonomous oncology agent that consults guidelines, databases, and imaging tools. Multi-agent systems extend this idea to collaborative scenarios involving clinicians, patients, and entire hospitals~\cite{kim2024mdagents,li2025agenthospital}. Beyond clinical applications, bioinformatics agents have been proposed with specialised toolsets to perform data extraction, pipeline execution, and hypothesis testing~\cite{ding2024automatingexploratoryproteomicsresearch,xin2024bioinformaticsagent}. 
The scope of agent systems continues to expand toward automating entire scientific projects, including literature reviews, experiment design, and manuscript writing~\cite{lu2024aiscientistfullyautomated, schmidgall2025agentlaboratory}.

In software engineering, code generation benchmarks~\cite{zhuo2024bigcodebench,jain2024livecodebench} assess the ability of \gls{llm} agents to implement Python functions for narrowly defined tasks.
Beyond simple function implementation, recent work has focused on developing agents to solve more complex problems, from debugging code to creating entire software projects~\cite{wang2024openhands,yang2024sweagent,qian2024chatdev}.
Among these, OpenHands~\cite{wang2024openhands} achieves state-of-the-art performance on SWE-Bench~\cite{jimenez2024swebench}, a benchmark for solving GitHub issues. 
Yet, the aforementioned benchmarks assume all necessary dependencies are pre-installed.
Instead, we consider real-world scientific tasks that require agents to autonomously install necessary dependencies, before implementing the task.

Nonetheless, no matter their domain, agentic systems remain constrained by the tools at their disposal. 
For example, when tasked to solve a pathology image classification problem, the AIDE machine learning engineer agent~\cite{weco2025introducing} trains a standard convolutional net (\cf Figure 2 in \citet{chan2024mlebench}).
However, domain experts (computational pathologists) would instead employ pathology foundation models for this, as these have been designed specifically for this type of problem~\cite{chen2024uni,zimmermann2024virchow2,filiot2024phikonv2,wolflein2023good}.

Research on \gls{llm} agent tools mainly focuses on \emph{tool learning}, \ie teaching \glspl{llm} to utilise appropriate, human-crafted tools more effectively~\cite{qin2024toollearning,schick2023toolformer}.
However, we consider the problem of \emph{tool creation} -- enabling \glspl{llm} to create their own tools, to dynamically expand their capabilities at runtime.
Previous work on tool creation~\cite{cai2024toolmakers,yuan2024craft, qian2023creator} is limited to crafting very simple tools because (i) they are crafted from scratch, and (ii) these systems cannot interact with the operating system by running bash commands, reading/writing files, \etc (see \cref{tab:tool-learning}). We address both of these limitations below.
\begin{table}[h]
  \adjustbox{max width=\linewidth}{
    \begin{tabular}{lcccc}
      \toprule
      \textbf{Method} & \makecell{\textbf{Error}\\\textbf{handling}} & \makecell{\textbf{OS}\\\textbf{interaction}} & \makecell{\textbf{Complex}\\\textbf{tasks}}                                           \\
      \midrule
      CRAFT~\cite{yuan2024craft} & \xmark & \xmark & \xmark \\
      CREATOR~\cite{qian2023creator} & \cmark & \xmark & \xmark \\
      LATM~\cite{cai2024toolmakers} & \cmark & \xmark & \xmark \\
      \ours (ours) & \cmark & \cmark & \cmark \\
      \bottomrule
    \end{tabular}
  }
  \caption{Comparison of tool creation methods. \emph{OS interaction} refers to the ability to interact with the operating system (\eg read/write files, run commands, web browsing). \emph{Complex tasks} require installing and using external dependencies (\eg libraries, model weights).}
  \label{tab:tool-learning}
\end{table}






\begin{figure*}[t]
  \includegraphics[width=\linewidth]{images/overview.pdf}
  \caption{\textbf{\Ours workflow.}
    Given a task description, a scientific paper, and its associated code repository, \ours generates an executable tool that enables a downstream \gls{llm} agent to perform the described task.
  }
  \label{fig:overview}
\end{figure*}

\section{\Ours}
We design \ours to autonomously convert stand-alone code repositories from scientific publications into \gls{llm}-compatible tools. Each tool should complete a specific, user-defined task. To do so, we require a minimal \emph{tool definition} (see \cref{fig:tool_creation}, top), consisting of:
\begin{enumerate}[noitemsep,topsep=0pt,label=\arabic*)]
  \item a concise textual description of the task,
  \item GitHub URL of the associated repository, and
  \item a list of required input arguments, including an example value for each argument.
\end{enumerate}

This tool definition could in principle be represented as the signature of a Python function with a docstring, like in existing code generation tasks~\cite{zhuo2024bigcodebench,jain2024livecodebench}.
However, unlike previous work, we require the \gls{llm} to not only implement the function, but also to \emph{set up the environment} wherein the function will be executed.
The latter is necessary due to the complexity of our tasks which require \eg installing external dependencies, downloading models, and setting up configurations while considering system and hardware specifications.


We structure \ours as an \emph{agentic workflow} (see \cref{fig:overview}) that consists of two stages: environment setup and tool implementation.
During environment setup, \ours produces a reproducible ``snapshot'' of the system (a Docker image) in which the final tool will run.
In the second stage, \ours generates a Python function that implements the desired task.


\def\conversation{\textcolor{llmflow}{h}}
\def\setofconversations{\textcolor{llmflow}{\mathcal{H}}}
\def\llmcall{\textcolor{llmflow}{\ell}}
\def\message{\textcolor{llmflow}{m}}
\def\setofmessages{\textcolor{llmflow}{\mathcal{M}}}
\def\setofreturns{\mathcal{R}}
\def\setofllmcalls{\textcolor{llmflow}{\mathcal{L}}}

\def\envstate{\textcolor{environmentflow}{e}}
\def\setofenvstates{\textcolor{environmentflow}{\mathcal{E}}}

\newcommand{\action}{\textcolor{environmentflow}{a}}
\def\setofactions{\textcolor{environmentflow}{\mathcal{A}}}
\def\agent{\textcolor{agentflow}{\ensuremath{\pi}}}
\def\observation{\textcolor{environmentflow}{o}}
\def\setofobservations{\textcolor{environmentflow}{\mathcal{O}}}
\def\workflowstate{s}
\def\setofworkflowstates{\mathcal{S}}

\subsection{Workflow components}



We define the \emph{state of the workflow} at any point in time to be a pair
\begin{equation*}
  s = \bigl(\conversation,\;\envstate\bigr) 
  \; \in \; \setofconversations \times \setofenvstates.
\end{equation*}
Here, $\conversation \in \setofconversations$ is the \emph{conversation history} (the ordered sequence of messages from the user, tools, and the \gls{llm}), and $\envstate \in \setofenvstates$ is the \emph{environment state} (represented by a checkpointed Docker container).



\Ours is built out of fundamental \emph{components}, each viewed as a function that acts on the workflow state as
\begin{equation*}
  \setofworkflowstates \;\mapsto\; \setofworkflowstates \times \setofreturns,
\end{equation*}
where $\setofworkflowstates = \setofconversations \times \setofenvstates$ is the space of all possible workflow states, and $\setofreturns \supseteq \setofmessages \cup \setofobservations$ is the set of possible returns (e.g.\ a newly generated message in $\setofmessages$ or an environment observation in $\setofobservations$). 
Concretely, we distinguish three main types of components:
\textbf{\icon{images/icons/llm_call.pdf}\:\gls{llm} calls} ($\setofconversations \mapsto \setofconversations \times \setofmessages$),
\textbf{\icon{images/icons/environment_interaction.pdf}\:environment interactions} ($\setofenvstates \mapsto \setofenvstates \times \setofobservations$), and 
\textbf{\icon{images/icons/agent.pdf}\:agents} ($\setofconversations \times \setofenvstates \mapsto \setofconversations \times \setofenvstates \times \setofreturns$).


\subsubsection{\icon{images/icons/llm_call.pdf}\:\gls{llm} calls} 
\Pgls{llm} can be viewed as a function
\begin{equation*}
  LLM : \setofconversations \;\to\; \setofmessages,
\end{equation*}
which, given a conversation history, produces a single new message. As a \ours workflow component, an \gls{llm} call $\llmcall : \setofconversations \to \setofconversations \times \setofmessages$ takes the workflow state's conversation history $\conversation$, appends $LLM(\conversation)$, and returns the new message:
\begin{equation*}
  \conversation
  \;\mapsto\;
  (\conversation \oplus LLM(\conversation),\; LLM(\conversation)).
\end{equation*}
\Glspl{llm} calls thus only update the conversation and do not modify the environment.
We use OpenAI's \texttt{gpt-4o-2024-08-06} model for the \gls{llm} calls.

\subsubsection{\icon{images/icons/environment_interaction.pdf}\:Environment interactions}
An environment interaction is any action $\action\in\setofactions$ that can read from or write to the environment state $\envstate$. We may thus model it by
\begin{equation*}
  \envstate
  \;\mapsto\;
  (\envstate',\;\observation),
\end{equation*}
where $\envstate'$ is the updated environment state, and $\observation\in\setofobservations$ is the observation produced by the action. 

The set of environment actions are
\begin{equation*}
  \setofactions = \adjustbox{max width=.8\linewidth}{$\displaystyle
  \left\{ 
    \begin{array}{l}
    \icon{images/icons/terminal.pdf}\:\textsc{run\_bash\_command},
    \icon{images/icons/list_directory.pdf}\:\textsc{list\_directory}, \\
    \icon{images/icons/read_file.pdf}\:\textsc{read\_file},
    \icon{images/icons/write_file.pdf}\:\textsc{write\_file},
    \icon{images/icons/browse.pdf}\:\textsc{browse}, \\
    \icon{images/icons/google_drive_folder.pdf}\:\textsc{google\_drive\_list\_folder}, \\
    \icon{images/icons/google_drive.pdf}\:\textsc{google\_drive\_download\_file}, \\
    \icon{images/icons/python.pdf}\:\textsc{run\_implementation}
    \end{array}
  \right\}
  $}.
\end{equation*}
We distinguish between \emph{read-only} actions and \emph{write} actions~\cite{huyen2024aiengineering}.
While read-only actions $\setofactions_{r} = \{\icon{images/icons/read_file.pdf}, \icon{images/icons/list_directory.pdf}, \icon{images/icons/browse.pdf}, \icon{images/icons/google_drive_folder.pdf}\}$ have $\envstate' = \envstate$, write actions $\setofactions_{w} = \{\icon{images/icons/terminal.pdf}, \icon{images/icons/write_file.pdf}, \icon{images/icons/google_drive.pdf}, \icon{images/icons/python.pdf}\}$ may modify $\envstate$.
This distinction plays an important role in \cref{sec:execution_environment}.

The \icon{images/icons/python.pdf}\:\texttt{run\_implementation} action is a special action that allows \ours to execute a candidate tool implementation.



\subsubsection{\icon{images/icons/agent.pdf}\:Agents} 
\label{sec:agents}

An \emph{agent} $\agent$, illustrated in \cref{fig:sub_agent}, chains multiple \gls{llm} calls and environment interactions to accomplish a specific sub-task which is specified by a high-level instruction, $\message_{\agent} \in \setofmessages$, \eg ``install this repository and its dependencies''.


\begin{figure}[h]
  \centering
  \includegraphics[width=.8\linewidth]{images/sub_agent_v2.pdf}
  \caption{An agent uses a tool-augmented \gls{llm} to perform a specific sub-task, and returns the result. Messages are \textcolor{llmflow}{appended} to the conversation history, and tool calls enable the agent to \textcolor{environmentflow}{interact} with the environment.}
  \label{fig:sub_agent}
\end{figure}

Formally, an agent $\agent$ maps the current workflow state $s=(\conversation, \envstate)$ to a new state $s_T=(\conversation_T, \envstate_T)$ and return value $r\in\setofreturns$:
\begin{equation*}
  (\conversation, \envstate) \;\mapsto\; (\conversation_T, \envstate_T, r).
\end{equation*}

The agent follows a sequence of state transitions
\begin{equation*}
  s_0 \;\to\; s_1 \;\to\; \cdots \;\to\; s_T,
\end{equation*}
where each state $s_t = (\conversation_t, \envstate_t) \in \setofworkflowstates$.
At step $t=0$, the agent receives the \emph{initial} state
\begin{equation*}
  s_0 \;=\;
  \bigl(\conversation \oplus \message_{\agent},\;\envstate\bigr).
\end{equation*}

At each step $t$, the agent employs a special \emph{tool-augmented} \gls{llm}, denoted
\begin{equation*}
  LLM_{\agent} : \setofconversations
  \;\to\;
  \setofactions_{\agent} \;\cup\; \setofreturns,
\end{equation*}
which, given the current conversation $\conversation_t$, either outputs an \textbf{action} $\action_t \in \setofactions_{\agent}$ (a tool call) or the \textbf{final result} $r \in \setofreturns$ of the sub-task. Here, $\setofactions_{\agent} \subseteq \setofactions \setminus \{\texttt{\icon{images/icons/python.pdf}\:run\_implementation}\}$ excludes directly running candidate tool implementations, as this is a separate step in the \ours workflow.
We implement the choice between $\setofactions_{\agent}$ and $\setofreturns$ using OpenAI's function calling and structured output \glspl{api} respectively~\cite{openai2025docs}.

If the \gls{llm} proposes an action $\action_t=LLM_{\agent}(\conversation_t) \in \setofactions$, we execute $\action_t$ on the current environment to obtain the observation and updated environment state $(\envstate_{t+1}, \observation_t)=\action_t(\envstate_t)$. We then append both the tool call and its observation to the conversation, forming the new state
\begin{equation*}
  s_{t+1} \;=\;
  (\conversation_t \oplus \action_t \oplus \observation_t,\; \envstate_{t+1}).
\end{equation*}
If instead $LLM_{\agent}(\conversation_t)$ outputs a final result $r \in \setofreturns$, the agent terminates and returns $s_T=(\conversation_t, \envstate_t, r)$.


\subsection{\Ours workflow}
\label{sec:toolmaker_workflow}
In this section, we describe our workflow in detail, which at a high level is illustrated in \cref{fig:overview}, and in pseudocode in \cref{alg:toolmaker}, using the three types of components (\icon{images/icons/llm_call.pdf}\:\gls{llm} calls, \icon{images/icons/environment_interaction.pdf}\:environment interactions, and \icon{images/icons/agent.pdf}\:agents) introduced above.


\begin{algorithm}[t]
  \caption{\Ours workflow.}
  \label{alg:toolmaker}
  \small
  \begin{algorithmic}[1]
    \Require Tool definition $\message_{\text{tool}}$, initial environment $\envstate_\emptyset \in \setofenvstates$
    \State $\conversation_\emptyset \gets \{\message_{\text{tool}}\}$ \Comment{initialise conversation history}
    \State $\conversation, \envstate, r \gets \textsc{\icon{images/icons/agent.pdf}\:install\_repository}(\conversation_\emptyset, \envstate_\emptyset)$ \label{line:install_repository}
    \State $\bar{\envstate} \gets \envstate$ \Comment{snapshot of installed environment state} \label{line:snapshot_envstate}
    \State $\conversation, \envstate, r \gets \textsc{\icon{images/icons/agent.pdf}\:explore}(\conversation_\emptyset, \bar{\envstate})$ \label{line:explore}
    \State $\conversation, \message \gets \textsc{\icon{images/icons/llm_call.pdf}\:plan}(\conversation)$ \label{line:plan}
    \State $\bar{\conversation} \gets \conversation$ \label{line:snapshot_conversation_history}\Comment{snapshot of conversation history}
    \State $\conversation, \message_\text{code} \gets \textsc{\icon{images/icons/llm_call.pdf}\:implement}(\conversation)$ \label{line:implement}
    \State $\sigma \gets \emptyset$
    \While{true} \label{line:loop}
      \State $\envstate \gets \bar{\envstate}$ \Comment{restore installed environment state}\label{line:restore_envstate}
      \State $\conversation \gets \bar{\conversation} \oplus \sigma \oplus \message_\text{code}$ \label{line:restore_conversation_history} \Comment{restore conversation history}
      \State $\envstate, \observation \gets \textsc{\icon{images/icons/environment_interaction.pdf}\:run\_implementation}(\envstate, \message_\text{code})$ \label{line:run_implementation}
      \State $\conversation, \message \gets \textsc{\icon{images/icons/llm_call.pdf}\:assess\_tool\_output}(\conversation \oplus \observation)$ \label{line:assess_tool_output}
      \If{$\message$ is successful}
        \State \Return $\bar{\envstate}, \message_\text{code}$
      \EndIf
      \State $\conversation, \envstate, r \gets \textsc{\icon{images/icons/agent.pdf}\:diagnose\_error}(\conversation \oplus \observation, \envstate)$ \label{line:diagnose_error}
      \State $\conversation, \message_\text{code} \gets \textsc{\icon{images/icons/llm_call.pdf}\:reimplement}(\conversation)$ \label{line:reimplement}
      \State $\conversation, \message_\text{summary} \gets \textsc{\icon{images/icons/llm_call.pdf}\:summarise}(\conversation)$ \label{line:summarise}
      \State $\sigma \gets \sigma \oplus \message_\text{summary}$ \label{line:update_sigma}
    \EndWhile
  \end{algorithmic}
\end{algorithm}


\Ours's initial conversation history $\conversation_\emptyset$ is a system prompt that contains the tool definition $\message_{\text{tool}}$. 
We provide the full prompts in \cref{app:prompts}.

\paragraph{Environment setup}
To obtain the state of the execution environment necessary for the tool to execute, we employ the \icon{images/icons/agent.pdf}\:\textsc{install\_repository} agent (line~\ref{line:install_repository}) that is instructed to install and set up the repository.
This agent clones and explores the repository, reads documentation, and downloads any dependencies it deems necessary such as models, datasets, and libraries. Each of these steps involve planning and learning from previous observations such as error logs arising during execution.

The agent begins with a clean environment state $\envstate_\emptyset$ (a \mbox{\texttt{python:3.12}} Docker image).
Importantly, we record all write actions ($\setofactions_{w}$) that the agent performs.
Since each of these actions may be expressed as a bash command, we simply concatenate their bash representations to obtain the environment definition in the form of a bash script or Dockerfile.


\paragraph{Initial implementation}
We first instruct an agent (\icon{images/icons/agent.pdf}\:\textsc{explore}) to explore the repository and gather all information necessary to implement the tool.
Note that we do not carry over the conversation history from the previous stage, in order to not pollute the context with a large number of messages (by calling \icon{images/icons/agent.pdf}\:\textsc{explore} on $\conversation_\emptyset$, not $\conversation$ on line~\ref{line:explore}).

Next we perform an \gls{llm} call (\icon{images/icons/llm_call.pdf}\:\textsc{plan}) to create a step-by-step plan for the implementation.
We keep all messages (including actions and observations) in the conversation history, so this information can be used to create the plan.

Then, we instruct the \gls{llm} (\icon{images/icons/llm_call.pdf}\:\textsc{implement}) to write the Python code for the tool based on the plan, producing our first \emph{candidate implementation}.

\paragraph{Closed-loop self-improvement}
Now, we enter the closed-loop self-improvement phase.
First, we reset the execution environment to the \emph{environment definition} $\bar{\envstate}$ because the agent may have performed write actions in the past.
We also restore the conversation history to immediately after generating the implementation plan, but include summaries of past appempts (described later).

After running the candidate Python function in the execution environment using the example invocation provided in the tool definition (line~\ref{line:run_implementation}), we instruct the \gls{llm} to assess whether the execution was successful (\icon{images/icons/llm_call.pdf}\:\textsc{assess\_tool\_output}).
Specifically, we ask the \gls{llm} to check whether the result returned by the tool is in line with the task description (\ie if the result is plausible), and whether the standard output and standard error streams contain any indications of errors.
If the \gls{llm} deemed tool execution successful, we have arrived at our final tool implementation, and exit the loop.
Otherwise, we continue the self-improvement loop.

Next, we instruct the \icon{images/icons/agent.pdf}\:\textsc{diagnose\_error} agent to gather information about the error in order to diagnose its root cause and formulate a plan to fix it.
Importantly, we do not reset the execution environment -- the agent is able to check intermediate files and outputs created during tool execution.

Then, we ask the \gls{llm} to re-implement the tool based on the current implementation, error diagnosis, and plan to fix the error (\icon{images/icons/llm_call.pdf}\:\textsc{reimplement}). 
Finally, the \gls{llm} summarises (\icon{images/icons/llm_call.pdf}\:\textsc{summarise}).
We append this summary to the conversation history for the next iteration.

\subsection{Execution environment\:\icon{images/icons/docker.pdf}}
An important implementation detail is the \emph{execution environment}, which is the environment in which (i) actions ($\setofactions$) are performed throughout the \ours workflow, and (ii) wherein the final tool created by \ours will be executed.

The execution environment itself is \emph{stateful}. Specifically, write actions $\setofactions_{w} = \{\icon{images/icons/terminal.pdf}, \icon{images/icons/write_file.pdf}, \icon{images/icons/google_drive.pdf}, \icon{images/icons/python.pdf}\}$ may mutate environment state.
However, we require the ability to roll back to previous states, \eg
on line~\ref{line:restore_envstate} of \cref{alg:toolmaker}, the execution environment is restored to the ``freshly installed'' state $\bar{\envstate}$.
Furthermore, the execution environment should be sandboxed from the host system (for security reasons), and it should be reproducible (so the generated tool can be executed on any machine).

We satisfy these requirements by implementing the execution environment as a Docker container that \ours controls via \pgls{http} server running inside the container, which can run the pre-defined actions $\setofactions$.
State restoration is achieved via Docker's checkpointing functionality.

\section{Benchmark}

\begin{table*}
  \centering
  \adjustbox{max width=\linewidth}{
    \input{data/results_gpt-4o.tex}
  }
  \caption{Performance of the tools created by \ours and the OpenHands baseline~\cite{wang2024openhands} on the benchmark tasks.
    \errorinstallfailed indicates that the environment installation failed.
    We use $\circlearrowleft$ to indicate the number of self-correcting iterations. \coloredbox{cellgreen}{Green} cells indicate that the tool implementation is correct (all unit tests pass), \coloredbox{cellyellow}{yellow} indicates that at least one unit test failed, and \coloredbox{cellred}{red} indicates that all unit tests failed.}
  \label{tab:results}
\end{table*}

To evaluate our approach, we curate a dataset of 15 diverse tasks spanning multiple scientific disciplines, which we refer to as \ourbenchmark. 
While the majority of these tasks originate from the medical domain (pathology, radiology, omics), we also include tasks from other fields (3D vision, imaging, tabular data analysis, natural language processing) to ensure our benchmark effectively captures a broader spectrum of real-world scientific challenges. 
The tasks encompass a range of difficulty levels, from simple tasks that can be achieved by calling an existing method within the provided repository, to more complex, multi-step tasks that require orchestrating multiple function calls, transforming data, and utilising \glspl{gpu}.

\paragraph{Task definitions}

Each task definition consists of:
\begin{enumerate*}[label=(\roman*)]
  \item a concise one-sentence task description,
  \item a URL to the associated code repository,
  \item a list of input arguments required to execute the task, alongside an example invocation (see below), and
  \item a description of the expected output.
\end{enumerate*}
\Cref{fig:tool_creation} (top) shows an example task definition and an overview of task names and associated papers can be found in \cref{tab:results}. We provide a full list of all task definitions with their example invocations in \cref{app:benchmark}.

\paragraph{Invocations}
A task \emph{invocation} specifies a concrete value for each input argument, as well as \emph{external files and directories} that should be made accessible from within the execution environment during the invocation. Indeed, most tasks in \ourbenchmark require external files, \eg \texttt{stamp\_train\_classification\_model} takes an input dataset of \glspl{wsi} and a clinical data table, on which to train a classification model using the STAMP~\cite{elnahhas2024stamp} pipeline. Analysing and utilising datasets is a fundamental aspect of many real-world scientific tasks, which is why \ourbenchmark explicitly supports this functionality, unlike many existing code generation benchmarks~\cite{zhuo2024bigcodebench,jain2024livecodebench}.

Each task definition includes a single example invocation, which may be used in the tool creation process. Crucially, this specification does not include the expected return value, as the goal is to autonomously implement and execute the task without prior knowledge of the correct output.


\paragraph{Assessing correctness}
\Ourbenchmark specifies 2-3 additional test invocations per task, which are different to the example invocation (using different argument values and external datasets) and are held-out from the tool creation process.
For each invocation, \ourbenchmark includes unit tests to assess whether the tool produces the expected output by checking various properties of the return value and output files. The unit tests verify correctness through assertions on: \emph{structure} (dimensions and types of return values), \emph{values} (range, accuracy, and statistical properties of return values), \emph{files} (existence, format, and content of files produced by the tool, if applicable), and \emph{execution} (errors/crashes).

To ensure an unbiased assessment of tool implementations, the unit tests and test invocations are used strictly for evaluation and are not available during tool creation. 
\ourbenchmark comprises 15 tasks, with a total of 42 test invocations (average 2.8 per task) and 124 unit tests (average 8.3 per task). 
We consider a tool implementation correct only if it passes all unit tests of its test invocations.


\section{Results}

\Ourbenchmark can evaluate any ``tool maker'' that produces an environment definition~\icon{images/icons/docker.pdf} and a tool implementation~\icon{images/icons/python.pdf}.
However, to the best of our knowledge, no existing approaches are specifically designed to address the ``paper repository $\to$ \gls{llm} tool'' problem.
In order to nonetheless facilitate comparison with prior work, we adapt the OpenHands~\cite{wang2024openhands} to this setting. 
OpenHands is a software engineering agent that achieves SOTA performance on SWE-bench~\cite{jimenez2024swebench}.
We instruct OpenHands to generate the same artifacts as \ours: an environment definition~\icon{images/icons/docker.pdf} (expressed as a bash script to be run in a fresh \texttt{python:3.12} Docker image to create the environment state required for the tool to execute) and a tool implementation~\icon{images/icons/python.pdf} (a Python function). 
To ensure a fair comparison, we reuse large parts of the \ours prompts in the prompts we supply to the OpenHands, and add additional instructions to encourage OpenHands to test the artifacts it creates.
We use \texttt{gpt-4o} for the OpenHands baseline, but also ablate the choice of \gls{llm} in \cref{sec:ablations}.
The full prompts for \ours and OpenHands are listed in \cref{app:prompts,app:openhands_prompts}.

\paragraph{Performance}
In \cref{tab:results}, we report the performance of \ours and OpenHands on all tasks in \ourbenchmark, reporting correctness, cost, number of tokens, number of actions performed in the tool creation process (both stages), and the number of self-correcting iterations.
We consider a test invocation successful (``Tests'' column marked \coloredbox{cellgreen}{green}) if all of the unit tests that are associated with it pass.
Similarly, a tool implementation is correct (``Invoc.'' column marked \coloredbox{cellgreen}{green}) if \emph{all} of its test invocations are successful, \ie all of the unit tests associated with its test invocations pass.

\Ours significantly outperforms OpenHands, achieving an accuracy of 80\% (correctly implementing 12/15 tasks) while OpenHands was only able to correctly implement 20\% (3/15 tasks).

For the \texttt{esm\_fold\_predict}~\cite{verkuil2022esm1} task, \ours generates a partially correct implementation (\coloredbox{cellyellow}{yellow}) that passes two out of three test invocations.
The goal of this task is to predict the contact map of a protein from its sequence.
Upon inspection, we determined that the failed test invocation was different from the other invocations: it contained a mask token in the input sequence which was not present in the task definition's example invocation. 
However, when including such a mask token in the example invocation and re-running \ours, the tool implementation passed all test invocations.
This highlights that the example invocation in the task definition needs to be representative of the task. 

By contrast, OpenHands fails to generate correct tool implementations for most tasks.
In fact, almost half of the tools created by OpenHands already failed at the environment definition stage because the generated install script crashed, meaning that the tool implementation itself could not even be executed.
However, even for the eight tasks where OpenHands was able to generate an environment definition, only three tools passed all unit tests.

\paragraph{Multi-step tools}
A remarkable feature of \ours is that it is able to create tools that require multiple steps to complete.
For example, the \texttt{stamp\_train\_classification\_model} task provides a dataset of pathology \glspl{wsi} and a table of clinical data, and requires the tool implementation to use the STAMP pipeline~\cite{elnahhas2024stamp} to train a classification model that predicts a specific biomarker from the \gls{wsi} images.
This task requires multiple steps to complete: 
after downloading and installing the STAMP repository and its dependencies, the tool implementation needs to use STAMP to
\begin{enumerate*}[label=(\arabic*)]
  \item perform feature extraction on the \gls{wsi} images, and
  \item train a classification model using the extracted features and the clinical data.
\end{enumerate*}
The self-correcting loop allows \ours to realise that it needs to perform feature extraction before it can train a classification model, and to subsequently implement the tool function to perform both steps, illustrated in \cref{fig:overview} (right).
For this particular task, \ours performs 9 self-correcting iterations, executing 33 actions in total, before arriving at the final implementation.


\paragraph{Cost}
\Ours performs an average of 21.8 actions during tool creation, costing on average \$0.94 per tool, while OpenHands performs 7.5 actions on average (\$0.15 per tool).
The three tools that OpenHands correctly implemented were among the cheapest for \ours, requiring the fewest actions and self-correcting iterations.
This shows OpenHands can implement very ``easy'' tools, but fails to generalise to more complex tasks.

\subsection{Ablations}
\label{sec:ablations}
\paragraph{Paper summaries}
Since each task is based on one or more research papers, we perform an ablation study to determine whether we can inject useful information from the papers into the tool creation process.
Instead of directly including the full paper text in the prompts which would require too many tokens, we first provide the full text to \texttt{gpt-4o} and instruct it to summarise it with respect to the task at hand.
Then, we provide these task-specific and paper-specific summaries in the prompts for \ours and OpenHands.

\begin{table}[t]
  \centering
  \adjustbox{max width=\linewidth}{
    \input{data/ablations}
  }
  \caption{Ablation results. Rows marked with asterisk correspond to the results in \cref{tab:results}. We report the number of correct tools, invocations, and tests, as well as the per-tool average cost and number of actions performed.}
  \label{tab:ablations}
\end{table}

The results in \cref{tab:ablations} indicate that including paper summaries does not increase the performance of either approach.
However, it does decrease the average number of actions and, for \ours, the average number of self-correcting iterations required to create the tools.
For example, while \ours required 9 iterations (33 actions) to create the \texttt{stamp\_train\_classification\_model} tool, this decreased to only 5 iterations (15 actions) when using the paper summary (see \cref{app:extended_ablation_results}).

\paragraph{Choice of \gls{llm}}
We also evaluate \ours and OpenHands using OpenAI's \texttt{o3-mini} model instead of \texttt{gpt-4o}, and find that while this reduces cost, it also degrades performance in both cases.
Finally, since OpenHands achieved SOTA performance on SWE-bench~\cite{jimenez2024swebench} using Claude 3.5 Sonnet~\cite{anthropic_claude}, we re-run the OpenHands baseline using this model, but find that it performs worse than using \texttt{gpt-4o} (see \cref{tab:ablations}).




\section{Conclusion}

In this work, we showed that autonomous tool creation can go beyond simple Python functions and produce tools for real-world scientific tasks. We introduced \ours, a framework that autonomously transforms scientific code repositories into LLM-compatible tools, potentially drastically reducing the technical overhead in future for developing agents with specialised toolsets. 
In evaluations across multiple scientific domains, \ours surpassed the state-of-the-art software engineering agent, OpenHands, achieving 80\% accuracy. Additionally, we release \ourbenchmark as a comprehensive benchmark to spur further advancements in agentic tool creation.

We acknowledge that automated tool creation in life sciences carries significant risks that require careful consideration. The ability to autonomously implement complex biological and chemical tools could potentially be misused for creating harmful agents or bioweapons. Additionally, fully automated research systems might inadvertently generate dangerous compounds or protocols without proper oversight. These risks underscore the importance of developing robust safety measures and ethical guidelines alongside technical capabilities.

Nonetheless, by removing the technical barriers to implementing tools, \ours brings us closer to a future where the pace of scientific discovery is limited by computational capacity, not human resources.


\section*{Limitations}
While \ours addresses the critical challenge of tool creation, we acknowledge that fully autonomous scientific discovery remains constrained by physical experimentation requirements. This is an aspect which our work does not address. However, with an increasing proportion of life science research being conducted in silico, \ours provides a crucial building block for autonomous scientific workflows. Future work will focus on integrating \ours into broader autonomous research systems, potentially enabling end-to-end scientific discovery pipelines that operate with minimal human intervention.

Our framework assumes that the referenced code repositories are reasonably well-structured, up-to-date, and documented. In practice, however, open-source repositories may have poor documentation or incomplete implementation details, making them challenging to install or integrate automatically. 
In fact, there is no guarantee that any given repository will be installable and usable as a tool.
For \ourbenchmark, we manually curated the tasks such that we were able to successfully install and use the repository ourselves.
This way, we could ensure that the tasks were \emph{possible} in the first place.

While \ourbenchmark contains over 100 unit tests to evaluate the correctness of the tools, passing these tests does not guarantee perfect correctness in all real-world scenarios. Scientific workflows often involve edge cases, large-scale data, or unexpected computational patterns that are not captured by a small set of tests. Moreover, high-stakes applications such as clinical research would naturally demand additional layers of rigorous validation and oversight by domain experts.

Finally, we have endeavored to make \ourbenchmark fully reproducible by specifying exact commits and branches for the referenced repositories to ensure tasks remain stable over time. Nonetheless, external factors such as repository deletion, force-pushing changes, or renaming branches, could render our pinned references invalid. 



\ifdefined\isanonymous\else
\section*{Acknowledgments}
We thank Junhao Liang, Michaela Unger, and David Charatan for contributing tasks to the benchmark.
We also appreciate Jan Clusmann, Tim Lenz, and Lina Hadji-Kyriacou for their feedback on the manuscript, 
and are grateful to Annelies Bl\"{a}tterlein for designing the \ours logo.
\section*{Funding}
GW is supported by SCADS.AI, Lothian NHS, and in part by funding from the European Union's Horizon 2020 research and innovation programme (KATY, 101017453). 
JNK is supported by the German Cancer Aid (DECADE, 70115166), the German Federal Ministry of Education and Research (PEARL, 01KD2104C; CAMINO, 01EO2101; TRANSFORM LIVER, 031L0312A; TANGERINE, 01KT2302 through ERA-NET Transcan; Come2Data, 16DKZ2044A; DEEP-HCC, 031L0315A), the German Academic Exchange Service (SECAI, 57616814), the European Union's Horizon research and innovation programme (ODELIA, 101057091; GENIAL, 101096312), the European Research Council (ERC; NADIR, 101114631), the National Institutes of Health (EPICO, R01 CA263318) and the National Institute for Health and Care Research (NIHR, NIHR203331) Leeds Biomedical Research Centre. The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR or the Department of Health and Social Care. This work was funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them. 
\fi

\bibliography{bibliography,tool_papers}

\include{appendix/appendix}

\end{document}
