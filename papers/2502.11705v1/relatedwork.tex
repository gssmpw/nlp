\section{Related work}
In addition to demonstrating impressive capabilities in generating human-like text, \glspl{llm} such as ChatGPT~\cite{ouyang2022training}, Claude~\cite{anthropic_claude}, Gemini~\cite{google2024gemini} and Llama~\cite{meta2024llama3}, on their own, have shown strong potential in question answering and reasoning on problems in natural science related fields, like math~\cite{shao2024deepseekmathpushinglimitsmathematical}, chemistry~\cite{bran2024augmenting} and healthcare~\cite{singhal2023llms}. However, \glspl{llm} often struggle solving more complex problems directly, especially in situations that require intermediate results from multiple steps~\cite{valmeekam2023planningabilitieslargelanguage}. 
To address this, \gls{llm} agents have been developed which enhance an \gls{llm}'s capabilities by integrating external tools~\cite{schick2023toolformer}.

In the medical domain, \gls{llm} agents have been developed for tasks like clinical decision-making and diagnostics, \eg AgentMD~\cite{jin2024agentmd} creates risk calculators from medical publications, and \citet{ferber2024autonomous} propose an autonomous oncology agent that consults guidelines, databases, and imaging tools. Multi-agent systems extend this idea to collaborative scenarios involving clinicians, patients, and entire hospitals~\cite{kim2024mdagents,li2025agenthospital}. Beyond clinical applications, bioinformatics agents have been proposed with specialised toolsets to perform data extraction, pipeline execution, and hypothesis testing~\cite{ding2024automatingexploratoryproteomicsresearch,xin2024bioinformaticsagent}. 
The scope of agent systems continues to expand toward automating entire scientific projects, including literature reviews, experiment design, and manuscript writing~\cite{lu2024aiscientistfullyautomated, schmidgall2025agentlaboratory}.

In software engineering, code generation benchmarks~\cite{zhuo2024bigcodebench,jain2024livecodebench} assess the ability of \gls{llm} agents to implement Python functions for narrowly defined tasks.
Beyond simple function implementation, recent work has focused on developing agents to solve more complex problems, from debugging code to creating entire software projects~\cite{wang2024openhands,yang2024sweagent,qian2024chatdev}.
Among these, OpenHands~\cite{wang2024openhands} achieves state-of-the-art performance on SWE-Bench~\cite{jimenez2024swebench}, a benchmark for solving GitHub issues. 
Yet, the aforementioned benchmarks assume all necessary dependencies are pre-installed.
Instead, we consider real-world scientific tasks that require agents to autonomously install necessary dependencies, before implementing the task.

Nonetheless, no matter their domain, agentic systems remain constrained by the tools at their disposal. 
For example, when tasked to solve a pathology image classification problem, the AIDE machine learning engineer agent~\cite{weco2025introducing} trains a standard convolutional net (\cf Figure 2 in \citet{chan2024mlebench}).
However, domain experts (computational pathologists) would instead employ pathology foundation models for this, as these have been designed specifically for this type of problem~\cite{chen2024uni,zimmermann2024virchow2,filiot2024phikonv2,wolflein2023good}.

Research on \gls{llm} agent tools mainly focuses on \emph{tool learning}, \ie teaching \glspl{llm} to utilise appropriate, human-crafted tools more effectively~\cite{qin2024toollearning,schick2023toolformer}.
However, we consider the problem of \emph{tool creation} -- enabling \glspl{llm} to create their own tools, to dynamically expand their capabilities at runtime.
Previous work on tool creation~\cite{cai2024toolmakers,yuan2024craft, qian2023creator} is limited to crafting very simple tools because (i) they are crafted from scratch, and (ii) these systems cannot interact with the operating system by running bash commands, reading/writing files, \etc (see \cref{tab:tool-learning}). We address both of these limitations below.
\begin{table}[h]
  \adjustbox{max width=\linewidth}{
    \begin{tabular}{lcccc}
      \toprule
      \textbf{Method} & \makecell{\textbf{Error}\\\textbf{handling}} & \makecell{\textbf{OS}\\\textbf{interaction}} & \makecell{\textbf{Complex}\\\textbf{tasks}}                                           \\
      \midrule
      CRAFT~\cite{yuan2024craft} & \xmark & \xmark & \xmark \\
      CREATOR~\cite{qian2023creator} & \cmark & \xmark & \xmark \\
      LATM~\cite{cai2024toolmakers} & \cmark & \xmark & \xmark \\
      \ours (ours) & \cmark & \cmark & \cmark \\
      \bottomrule
    \end{tabular}
  }
  \caption{Comparison of tool creation methods. \emph{OS interaction} refers to the ability to interact with the operating system (\eg read/write files, run commands, web browsing). \emph{Complex tasks} require installing and using external dependencies (\eg libraries, model weights).}
  \label{tab:tool-learning}
\end{table}






\begin{figure*}[t]
  \includegraphics[width=\linewidth]{images/overview.pdf}
  \caption{\textbf{\Ours workflow.}
    Given a task description, a scientific paper, and its associated code repository, \ours generates an executable tool that enables a downstream \gls{llm} agent to perform the described task.
  }
  \label{fig:overview}
\end{figure*}