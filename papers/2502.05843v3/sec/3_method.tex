\section{Method}
\label{sec:method}
%
\begin{figure*}[t!]
    \centering
    \setlength{\abovecaptionskip}{0.1cm}
    \includegraphics[width=1\linewidth]{Figs/tutorial_final.pdf}
    \caption{\textbf{Illustration of the proposed \underline{SymbolicDet}}. SymbolicDet mainly consists of logic search and symbolic reasoning. The former module constructs and explores the search space by leveraging structured entity features extracted from an open-set object detector. Th latter module harnesses the symbolic reasoning capabilities of Large Language Models (LLMs) along with their inherent commonsense understanding of visual event patterns to guide the search process toward more appropriate and rational pathways.}
    \label{fig:tutorial}
    \vspace{-0.5cm}
\end{figure*}
%
In practical applications, merely detecting objects often fails to satisfy real-world engineering requirements. Many scenarios demand recognition of complex object relationships or events, which remains a significant challenge in current research. Construction site monitoring requires identifying not just workers and equipment but safety violations; traffic analysis needs to recognize not only vehicles but also dangerous driving patterns; and surveillance systems must detect not merely people but suspicious behaviors. While specialized event recognition systems exist, they typically require extensive training data and lack interpretability.
Our framework addresses this challenge by unlocking the latent event understanding capabilities in standard object detectors through LLM-guided symbolic reasoning. Here, we provide a detailed description of our approach, which transforms object detections into interpretable event recognition without additional training. Figure \ref{fig:tutorial} illustrates our framework's architecture. \par
Formally, given a visual dataset $\mathcal{D}$ consisting of n images:
\begin{equation}
    \mathcal{D} = \{\left(I_{1},y_{1} \right), \left(I_{2},y_{2} \right), \ldots, \left(I_{n},y_{n} \right)\}
\end{equation}
where each pair consists of an image \(I_i\) and a binary label \(y_i \in \{0, 1\}\) indicating whether a target event \(\varepsilon\) occurs in the image. More specifically, for each image \(I_i\), a standard object detector \(D\) produces a set of detections \(\mathcal{O} = \{o_1, o_2, \ldots, o_m\}\), where each detection \(o_j = (c_j, b_j, s_j)\) consists of a category label \(c_j\), a bounding box \(b_j\), and a confidence score \(s_j\). These detections represent ``what exists" in the image.
Our approach seeks to discover an interpretable symbolic expression $E$  that operates solely on the detector outputs to recognize the event. \par
%
Furthermore, the derived symbolic expression is utilized to assess its capacity in accurately classifying whether the target event is present within an image:
\begin{equation}
    E : \mathcal{O}_I \rightarrow \{0, 1\}\
\end{equation}
This symbolic expression effectively bridges the gap between low-level object detections and high-level event understanding, transforming ``what exists" into ``what is happening" through logical reasoning over detected entities and their relationships.These components work together in a synergistic manner. The overall workflow can be represented as:
\begin{equation}
    E^* = \arg\max_{E \in \mathcal{L}} \mathcal{G}_{LLM}(E, S(E, \mathcal{F}(\mathcal{D})))
\end{equation}
Where \(E^*\) is the optimal discovered symbolic expression, \(\mathcal{L}\) is the space of all possible expressions in our symbolic language, \(\mathcal{F}\) is the object detector, \(S\) is a scoring function that evaluates how well an expression distinguishes positive and negative examples, and \(\mathcal{G}_{LLM}\) is the LLM guidance mechanism that directs the search toward promising expressions.
% 
In the following sections, we detail each component of our framework and how they work together to unlock event understanding capabilities in standard object detectors.
%
%
\subsection{Symbolic Logic Search}
The core of our framework is the symbolic pattern discovery mechanism that identifies meaningful logical expressions capable of recognizing complex events from object detections. This process begins with extracting structured entity representations from detector outputs and then proceeds to search for effective symbolic patterns. \par
% 
\noindent\textbf{Entity-level Feature Extraction.} We first leverage an open-vocabulary object detector to extract comprehensive entity information from each sample. For a given image $x$, we obtain a set of entities:
\begin{equation}
    E = \{e_1, e_2, ..., e_n\}, \quad e_i = (c_i, b_i, s_i)
\end{equation}
where $c_i$ represents the category label, $b_i = (x, y, w, h)$ denotes the bounding box coordinates, and $s_i$ is the detection confidence score. These entities form the basis for our symbolic pattern analysis.
To facilitate symbolic reasoning, we transform the raw entity information into structured features:
\begin{equation}
    \mathbf{X} = \{\phi_1(E), \phi_2(E), ..., \phi_d(E)\}
\end{equation}
where $\phi_i(\cdot)$ represents different feature extraction functions that capture entity counts, spatial relationships, and attribute distributions. \par
% 
\noindent\textbf{Symbolic Pattern Discovery}
Given the entity representation of images, we next seek to discover symbolic patterns that effectively distinguish images containing the target event from those that do not.The symbolic regression problem is formulated as:
\begin{equation}
    f^* = \arg\min_{f \in \mathcal{F}} \sum_{i=1}^{n} \mathcal{L}(f(\mathbf{X}_i), y_i) + \lambda \Omega(f)
\end{equation}
where $\mathcal{F}$ is the space of possible symbolic expressions, $\mathcal{L}(\cdot)$ is a fitness function measuring pattern discrimination ability, and $\Omega(f)$ is a complexity penalty that promotes simpler expressions.
The search space $\mathcal{F}$ consists of mathematical operators 
$\{+, -, \times, \div, \max, \min\}$ 
and logical operators $\{\land, \lor, \neg\}$. 
To efficiently explore this space, we employ an evolutionary algorithm that initializes a population of candidate expressions, evaluates their fitness on the current dataset, applies genetic operators (mutation, crossover) to generate new candidates, and selects the best expressions for the next generation.
This process generates human-interpretable symbolic expressions that capture meaningful patterns in the data. For example, in a safety helmet detection scenario, a discovered pattern might be:
\begin{equation}
    f(\mathbf{X}) = \bigvee_{i \in \{p,d\}} [\phi_i(E) > \phi_h(E)]
\end{equation}
where $\phi_p$, $\phi_h$, and $\phi_d$ represent the counting functions for persons, helmets, and heads respectively.
% While evolutionary search provides a systematic approach to exploring the expression space, its effectiveness is limited by the stochastic nature of the search process and the exponential growth of the search space with expression complexity. This is where the LLM guidance becomes crucial for efficiently navigating the search space, as we describe in the next section.
While evolutionary search provides a systematic approach to exploring the expression space, its effectiveness is constrained by the stochastic nature of the search process and the exponential growth of the search space with expression complexity. This fundamental challenge highlights the critical role of our LLM guidance mechanism, which strategically directs the evolutionary process toward promising regions of the expression space, balancing exploration with semantic understanding. This synergistic integration, detailed in the following section, enables SymbolicDet to overcome the computational limitations of conventional symbolic approaches while maintaining interpretability.
%
\subsection{Automated LLM Reasoning}
To enhance the efficiency and effectiveness of symbolic pattern discovery, we propose an automated reasoning mechanism that leverages the semantic understanding capabilities of Large Language Models (LLMs). This LLM-guided approach consists of two main components: a structured prompt space for eliciting effective reasoning and an integrated symbolic search mechanism that combines LLM suggestions with systematic exploration.
%
\subsubsection{Structured Prompt Space}
We design a hierarchical prompt space to facilitate effective communication with LLMs through three key components:\par
\noindent
\textbf{Scene Context Initialization.} The first layer of prompts establishes the scene context:
\begin{equation}
    P_{\text{init}} = \{\text{scene}, \text{entities}, \text{constraints}\}
\end{equation}
This activation prompt triggers the LLM's prior knowledge relevant to the specific visual event understanding scenario, creating crucial connections between visual entities and semantic understanding.\par
\noindent
\textbf{Chain-of-Thought Guidance.} The second layer provides structured reasoning steps:
\begin{equation}
    P_{\text{cot}} = \{s_1 \rightarrow s_2 \rightarrow ... \rightarrow s_k\}
\end{equation}
where each step $s_i$ guides the LLM through professional analytical frameworks for identifying potential visual event symbolic patterns. This systematic approach ensures comprehensive consideration of entity relationships and domain constraints. \par
\noindent
\textbf{Contextual Feedback Integration.} The final layer incorporates evaluation feedback:
\begin{equation}
    P_{\text{feed}} = \{(r_1, \alpha_1), (r_2, \alpha_2), ..., (r_n, \alpha_n)\}
\end{equation}
where $r_i$ represents previous reasoning attempts and $\alpha_i$ their corresponding effectiveness scores. This feedback mechanism enables the LLM to refine its suggestions based on historical performance.
% 
\begin{algorithm}[t]
\caption{LLM-Guided Symbolic Search}
\begin{algorithmic}[1]
\STATE Initialize population $\mathcal{P}_0$ of symbolic expressions
\FOR{each iteration $t$}
    \STATE $f_t^* \gets$ Select best expression from $\mathcal{P}_t$
    \STATE $S_t \gets$ Generate LLM suggestions via prompt space
    \STATE $\mathcal{P}_{t+1} \gets$ Update population using $\{f_t^*, S_t\}$
    \IF{convergence criterion met}
        \STATE \textbf{break}
    \ENDIF
\ENDFOR
\RETURN best expression $f^*$
\end{algorithmic}
\end{algorithm}
% 
\subsubsection{LLM-Guided Symbolic Search}
We establish a bidirectional interaction mechanism between LLM reasoning and symbolic logic search (SLS) through an iterative process:
At each iteration, the best symbolic expression $f_t^*$ serves as a directional indicator for LLM reasoning. The LLM analyzes this expression through our structured prompt space and generates suggestions $S_t$ that incorporate its semantic understanding and common sense knowledge. These suggestions are then transformed into new symbolic expressions and integrated into the population for the next iteration.
The integration creates a synergistic effect where:
\begin{itemize}
    \item LLM reasoning guides the symbolic search towards semantically meaningful patterns
    \item Symbolic search provides objective evaluation of LLM suggestions
    \item The iterative process combines the interpretability of symbolic expressions with the rich semantic understanding of LLMs
\end{itemize} 
This bidirectional interaction accelerates the discovery of meaningful symbolic patterns while maintaining the interpretability of the detection process. The LLM's suggestions help navigate the vast space of possible symbolic expressions, while the symbolic search framework ensures that the final patterns remain explicit and verifiable. \par
% \textbf{Analysis.} Through the integration of symbolic logic search and LLM reasoning, our approach offers several distinct advantages. First, the training-free nature of our framework eliminates the need for extensive labeled datasets, as the symbolic pattern discovery process operates directly on entity-level features extracted from test samples. This characteristic is particularly valuable in real-world scenarios where collecting and annotating training data is costly or impractical.
% Second, our method achieves high interpretability through two complementary mechanisms: (a) the discovered symbolic expressions provide explicit, human-readable logical patterns that directly explain the detection decisions, and (b) the LLM's reasoning process offers semantic context for these patterns, making them more accessible to domain experts. This dual-layer interpretability sets our approach apart from traditional black-box models.
% Third, the bidirectional interaction between symbolic search and LLM reasoning creates an efficient optimization process. The LLM's semantic understanding and common sense knowledge help navigate the vast space of possible symbolic expressions, significantly reducing the search complexity compared to pure evolutionary approaches. Meanwhile, the symbolic search framework provides objective evaluation criteria that prevent the accumulation of LLM reasoning errors, ensuring the robustness of the final patterns.
% Lastly, our structured prompt space design enables systematic utilization of LLM capabilities while maintaining consistency in the reasoning process. This structured approach not only improves the quality of LLM suggestions but also makes the entire framework more stable and reproducible compared to methods relying on ad-hoc prompting strategies.
\noindent
\textbf{Analysis.} Through the integration of symbolic logic search and automated LLM reasoning, our framework offers several key advantages for event understanding in object detection: 
First, our approach produces inherently \textit{interpretable results} through complementary mechanisms. The discovered symbolic expressions provide explicit, human-readable logical patterns that directly explain recognition decisions, while the LLM's reasoning process offers semantic context for these patterns. This dual-layer interpretability is critical for applications where understanding the reasoning process is as important as the final decision. 
Second, the bidirectional interaction between symbolic search and LLM reasoning creates an \textit{efficient optimization process}. The LLM's semantic understanding helps navigate the vast space of possible symbolic expressions, while the symbolic search framework grounds the LLM's suggestions in empirical performance. Our structured prompt design ensures systematic utilization of LLM capabilities while maintaining consistency and reproducibility in the reasoning process.
Finally, our framework provides significant \textit{practical deployment advantages}. By operating on the outputs of existing object detectors, it eliminates the need for large-scale training datasets typically required by deep learning methods. This detector-agnostic approach can work with any state-of-the-art detection system without modification, benefiting from advances in object detection while maintaining focus on higher-level event recognition through transparent symbolic reasoning.
\noindent In summary, our framework bridges the gap between low-level object detection and high-level event understanding through a synergistic combination of symbolic search and LLM reasoning. By discovering interpretable symbolic expressions that operate on detector outputs, we unlock event recognition capabilities without extensive training data or specialized architectures, while maintaining full transparency in the reasoning process.