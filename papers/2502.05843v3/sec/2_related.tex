\section{Related work}
\label{sec:related}
\subsection{Visual Reasoning and Neuro-symbolic}
Our work extends beyond conventional object detection to enable reasoning about complex visual events, placing it within the broader visual reasoning paradigm. While we leverage object detector outputs as our foundation, we transform these into symbolic representations suitable for higher-order reasoning.
Visual reasoning research has progressed from simple object recognition to complex scene understanding requiring compositional analysis. Traditional approaches have relied on specialized architectures and extensive labeled datasets for specific reasoning tasks. Visual question answering systems interpret images through natural language questions \cite{hudson2019gqa,chen2024spatialvlm,khan2024consistency,ganz2024question,majumdar2024openeqa,chen2024vtqa}, while scene graph generation approaches identify object relationships to construct structured scene representations \cite{krishna2017visual,li2024pixels, wang2025scene,lin2022ru,zareian2020bridging}. However, these methods often lack interpretability or require task-specific training data.
Neuro-symbolic models offer a promising direction by combining neural networks' perceptual strengths with symbolic reasoning's interpretability and compositionality \cite{garcez2019neural,amizadeh2020neuro,mao2019neuro,shi2019explainable,yi2018neural}. These approaches typically extract symbolic representations from visual scenes using neural networks, then apply symbolic reasoning methods to these representations. 
%
Our framework advances this paradigm by implementing a novel neuro-symbolic approach where object detectors serve as the neural perception component while a symbolic reasoning layer guided by LLMs performs higher-level event understanding. Unlike traditional implementations requiring custom integration between components, our approach treats existing object detectors as modular perception units, maintaining interpretability while enabling flexible application across visual domains without task-specific training.
%
\subsection{LLMs for Visual Tasks}
Our approach uniquely positions LLMs as reasoning guides for symbolic search rather than for direct visual perception. This design allows us to leverage LLMs' rich world knowledge while maintaining a clear separation between perception (via object detectors) and reasoning (via interpretable symbolic operations).
Recent advances in LLMs have demonstrated impressive capabilities in visual understanding \cite{radford2021learning,liu2021swin,han2023llms,sun2023eva,zhang2022dino,liang2023open,zou2024segment,shen2024aligning,cheng2024yolo}. Models such as GPT-4V offer high accuracy in complex visual reasoning with low hallucination rates,making them suitable for complex visual analysis and general-purpose visual AI applications \cite{yang2023dawn}. Despite these advances, directly applying LLMs to visual reasoning presents challenges due to modality gaps and reasoning complexity.
Our framework addresses these challenges by using LLMs in their native text domain to guide symbolic pattern discovery over detector outputs. This approach maintains complete interpretability throughout the process --- a critical advantage over end-to-end black-box models. By separating perception from reasoning, we combine neural models' perceptual capabilities with symbolic reasoning's interpretability, enhanced by LLMs' semantic understanding, without requiring extensive multimodal training.
%
\subsection{Event Recognition and Understanding}
Our work extends into event recognition, where we enable complex visual understanding by reasoning about compositional relationships between detected entities.
Traditional event recognition approaches typically rely on specialized architectures trained on event-specific datasets \cite{lin2021complex,sakaino2023deepunseen,merlo2023automatic,fan2023flexible}. These methods often struggle with novel event types or complex scenarios requiring compositional reasoning. More recent approaches leveraging large-scale pretraining have improved generalization capabilities but often lack interpretability and explicit reasoning mechanisms.
Our framework addresses these limitations by enabling compositional reasoning over object detections to recognize complex events. Our framework addresses these limitations through compositional reasoning over object detections. By transforming detector outputs into symbolic representations, we enable LLM-guided search to identify specific patterns of object interactions characterizing complex events.   Unlike methods requiring extensive event-specific training data, our approach can leverage existing object detectors and LLMs' reasoning capabilities to understand diverse event types without additional visual training.