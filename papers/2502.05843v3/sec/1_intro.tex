\section{Introduction}
\label{sec:intro}
% 
\begin{figure}[!t]
    \centering
    \setlength{\abovecaptionskip}{0cm}
    \includegraphics[width=1\linewidth]{Figs/composite_img.pdf}
    \caption{The radar chart at the top illustrates the comparative performance of various models (SymbolicDet(APE \cite{shen2024aligning}), EVA-CLIP \cite{sun2023eva}, DINOv2 \cite{oquab2023dinov2}, LLAVA \cite{liu2023visual}) across different event detection scenarios. The bottom section provides a visual representation of workplace safety patterns identified through our SymbolicDet framework, showcasing specific conditions like helmet and glove usage in workshop or construction site environments.}
    \label{fig:composite}
    \vspace{-0.5cm}
\end{figure}
% 
Object detection has become a cornerstone of computer vision, enabling machines to identify and locate entities within visual scenes with remarkable accuracy \cite{girshick2014rich, girshick2015fast, ren2015faster, redmon2016you,cai2018cascade,carion2020end,zhu2020deformable,zhang2022dino,jia2023detrs,liu2022dab,meng2021conditional}. State-of-the-art detectors can now recognize thousands of object categories with remarkable precision, transforming applications across autonomous driving, surveillance, industrial inspection, and content analysis. However, despite these advances, a fundamental limitation persists: while modern detectors excel at answering ``what is present" in an image, they struggle with understanding ``what is happening" --- the relationships, interactions, and events occurring between detected entities. \par
% 
Consider a coastal surveillance scenario where a detector identifies ``persons" and ``fishing rods" with high confidence. Despite perfect detection, the system fails to recognize the critical event of ``illegal multi-rod fishing" --- where a single individual operates multiple fishing rods, violating conservation regulations. Similarly, in construction site monitoring, a standard detector might accurately identify workers, equipment, and safety gear, yet remain incapable of recognizing the crucial safety violation where a worker operates machinery without proper protective equipment.
%
These limitations stem from the architectural focus of object detectors on identifying discrete entities rather than modeling the compositional logic, relational semantics, and contextual dependencies that define meaningful events. Without the ability to understand ``what is happening" beyond ``what exists," even the most accurate detection systems fall short in scenarios requiring nuanced interpretation of object relationships and contextual significance --- a fundamental barrier to deploying truly intelligent visual systems in complex real-world scenarios. \par
% 
Traditionally, bridging this gap has required two unsatisfactory approaches. The first involves developing specialized event recognition systems trained on extensive labeled datasets for each target event, incurring prohibitive annotation costs and limiting generalizability \cite{gehrig2023recurrent, liu2023motion,li2023sodformer}. The second approach employs fine-tuning techniques to adapt object detectors to specific events, which sacrifices their general-purpose nature and still requires substantial task-specific data \cite{liu2024grounding,saito2022learning,gupta2022ow,zohar2023prob,li2022asynchronous,wang2023dual,zhang2022spiking}. These approaches not only demand significant resources but also typically yield black-box models that provide little insight into their reasoning process --- a critical limitation in safety-critical or regulated domains where interpretability is essential. \par
%
We present a fundamentally different approach that fundamentally reframes the problem: rather than developing specialized event recognition models, we propose to unlock the latent event recognition capabilities inherent in standard object detectors through the integration of LLM-guided symbolic reasoning. Our key insight is that standard object detectors already implicitly encode rich visual information that, when properly interpreted through symbolic reasoning, can reveal complex events and relationships. Rather than treating detectors as mere entity recognizers, we view them as sophisticated visual sensors whose outputs can be transformed into meaningful event understanding through interpretable logical reasoning. \par
%
Our framework, consists of three synergistic components. First, an open-vocabulary object detector extracts entity-level information. Second, a symbolic reasoning module discovers logical patterns among these entities through an evolutionary search process, generating human-readable expressions that capture complex relationships. Third, and most innovatively, we leverage Large Language Models (LLMs) to guide this symbolic search, infusing the process with rich world knowledge and semantic understanding that dramatically improves search efficiency and expression quality.
%
This approach offers several significant advantages over existing methods. First, it operates in a training-free manner, requiring no additional labeled data beyond what the underlying detector was trained on. Second, it maintains complete interpretability, with all event recognition decisions expressed as readable logical rules (as exemplified in Figure \ref{fig:composite}). Finally, our method is detector-agnostic, functioning as a plug-and-play enhancement layer that can augment any object detection system. \par
Through extensive experiments across multiple datasets, we demonstrate that our approach successfully enhances various detector architectures (APE \cite{shen2024aligning}, GLIP \cite{li2022grounded}, and YOLO-World \cite{cheng2024yolo}) to recognize complex events including illegal fishing activities, construction safety violations, and abnormal crowd behaviors. In the UCSD Ped2 benchmark \cite{wang2010anomaly}, our training-free approach achieves 98.7\% AUROC, approaching state-of-the-art performance of specialized, training-intensive methods (99.7\%), while providing fully transparent reasoning. For safety helmet compliance detection, our method improves recognition accuracy by 15.77\% without any domain-specific training.
%
% \noindent
The principal contributions of our work include:
\begin{itemize}
    \item We propose a novel framework that unlocks complex event understanding capabilities in standard object detectors through LLM-guided symbolic reasoning, without requiring additional training.
    \item We develop an efficient mechanism for discovering interpretable symbolic patterns from object detector, enabling transparent reasoning from object-level detections to event-level understanding.
    \item We introduce a structured LLM reasoning process that guides symbolic search, leveraging natural language understanding to discover meaningful patterns while dramatically improving search efficiency.
    \item We introduce the Helmet-Mac Dataset, a comprehensive resource containing 12,213 samples specifically designed for construction safety compliance detection, which we make publicly available to the research community.
\end{itemize}
%
% These contributions represent a significant step toward bridging the gap between low-level perception and high-level understanding in visual systems. By unlocking the latent capabilities of object detectors through interpretable symbolic reasoning, our work not only enhances their practical utility but also provides valuable insights into how neural perception and symbolic reasoning can be synergistically combined in artificial intelligence systems.
Through these contributions, we not only enhance the practical utility of object detection systems but also advance our understanding of how compositional reasoning can bridge low-level perception and high-level event semantics. Our work represents a step toward visual AI systems that not only see objects but understand the meaningful events unfolding within visual scenes.