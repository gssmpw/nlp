\section{Experiment}
\label{exper}

\begin{table*}[t!]
\caption{Performance of different open set detectors on multiple data sets with or without SymbolicDet module. (AUROC\%) }
\label{table:diff_detector}
\centering
\begin{threeparttable}
\setlength{\tabcolsep}{2.0mm}
\renewcommand\arraystretch{1.3} 
\begin{tabular}{lc|c>{\columncolor{cyan!10}}c|c>{\columncolor{cyan!10}}c|c>{\columncolor{cyan!10}}c}
% \hline
\toprule
\multicolumn{2}{c|}{Datasets}                         & \multicolumn{2}{c|}{APE \cite{shen2024aligning}}       & \multicolumn{2}{c|}{YOLO-World \cite{cheng2024yolo}} & \multicolumn{2}{c}{GLIP \cite{li2022grounded}}       \\ \cline{3-8} 
                    
                         & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{Original} & +SymbolicDet & \multicolumn{1}{l}{Original}  & +SymbolicDet & \multicolumn{1}{l}{Original} & +SymbolicDet \\ \hline
\multicolumn{1}{l|}{}    & BALL                  & 55.36              & \textbf{94.91} \textcolor{red}{(+39.55)} & 54.76               & \textbf{89.05} \textcolor{red}{(+34.29)} & 66.34              & \textbf{90.27} \textcolor{red}{(+23.93)} \\ \cline{2-8} 
\multicolumn{1}{l|}{ERA \cite{eradataset}} & PersonCrowd           & 78.30              & \textbf{83.26} \textcolor{red}{(+4.96)} & 55.00               & \textbf{85.11} \textcolor{red}{(+30.11)} & 81.71              & \textbf{85.08} \textcolor{red}{(+3.37)} \\ \cline{2-8} 
\multicolumn{1}{l|}{}    & Sport                 & 67.13              & \textbf{90.29} \textcolor{red}{(+23.16)} & 67.27               & \textbf{88.54} \textcolor{red}{(+21.27)} & 66.94              & \textbf{89.65} \textcolor{red}{(+22.71)} \\ \hline
\multicolumn{2}{c|}{Helmet-Mac}                      & 67.41              & \textbf{83.18} \textcolor{red}{(+15.77)} & 65.40               & \textbf{82.47} \textcolor{red}{(+17.07)} & 61.06              & \textbf{76.25} \textcolor{red}{(+15.19)} \\ 
\multicolumn{2}{c|}{Multi-rods Fishing\tnote{1} }                    & 66.82              & \textbf{75.16} \textcolor{red}{(+8.36)} & 52.72               & \textbf{72.01} \textcolor{red}{(+19.29)} & 50.00              & \textbf{71.11} \textcolor{red}{(+21.11)} \\ \hline
\bottomrule
\end{tabular}
\begin{tablenotes}
\item[1] It refers to a subset of Multi-Event Dataset.
\end{tablenotes}
\end{threeparttable}
\end{table*}
% 
\subsection{Experimental Setup}
To comprehensively evaluate our approach, we conduct experiments across diverse datasets spanning various event detection scenarios, comparing against both traditional methods and architecture variants to demonstrate the efficacy of our LLM-guided symbolic reasoning framework.
\subsubsection{Self-collected Datasets}
\noindent
\textbf{Multi-Event Dataset} is our large-scale collection containing over 110,000 images spanning various event detection scenarios. The dataset comprises over 110,000 images spanning various scenarios including fires and waste incineration (30,954 images), multi-rods fishing (12,000 images), night fishing (97 images), license plate detection (12,487 images), personnel loitering and intrusion (10,788 images), among others. For this study, we specifically focus on the multiple-rod fishing scenario, where we have 15,000 training images with 45,341 detailed bounding box annotations covering persons, fishing rods, tackle bags, and umbrellas. The test set contains 2,283 images, with 1,098 cases showing anomalous multi-rods fishing activities and 1,185 normal cases with single-rod fishing. \par
% 
\noindent
\textbf{Helmet-Mac Dataset}, which we make publicly available\footnote{Dataset will soon be available in our code repository.}, addresses the critical domain of construction site safety monitoring. This dataset is curated from various construction scenarios and focuses on safety helmet compliance detection. It contains 7,571 training images with detailed annotations of human heads and safety helmets across diverse construction environments. The test set comprises 4,642 images, balanced between 2,276 safety violations (workers without helmets) and 2,366 compliant cases. The dataset captures various challenging scenarios including different lighting conditions, viewing angles, and occlusion cases, making it a valuable benchmark for safety-critical event detection systems.
% 
\subsubsection{Public Benchmarks}
\noindent
\textbf{ERA Dataset} \cite{eradataset} (Event Recognition in Aerial videos) provides a comprehensive collection of aerial footage covering various event categories. We organize our evaluation around three main event categories: BALL events (327 images) encompassing baseball, soccer, and basketball games; Person\_crowded events (352 images) including conflicts, parade protests, and parties; and Sport events (258 images) covering cycling, boating, and racing activities. Additionally, we utilize 347 Non-event images as negative samples, creating a balanced evaluation framework for our method's discriminative capabilities across different event types. \par
% 
\noindent
\textbf{UCSD Ped2 Dataset} \cite{wang2010anomaly} serves as our primary benchmark for comparison with state-of-the-art methods. This well-established dataset has been widely used in the anomaly detection community, providing a standardized evaluation platform. We use this dataset to demonstrate our method's competitive performance against existing approaches while maintaining the advantages of training-free operation and interpretability.

% 
\subsubsection{Implementation Details}
Our implementation integrates three key components: open-vocabulary object detection, LLM-guided symbolic pattern discovery, and Computational Resources. \par
% 
\noindent
\textbf{Open-vocabulary object detector Setup}
We employ three state-of-the-art multimodal detectors in our experiments: APE \cite{shen2024aligning} (serving as the primary detector), GLIP \cite{li2022grounded}, and YOLO-WORLD \cite{cheng2024yolo}. To optimize detection performance, we implement a two-stage prompt generation process. Initially, we leverage LLM to analyze event scenario descriptions and generate comprehensive detection prompts. The LLM generates prompts not only for objects directly associated with event scenarios but also for contextually related non-event objects, ensuring comprehensive coverage of potential scene elements. Based on empirical studies of detector characteristics, we configure different detection thresholds for optimal performance. Considering APE's characteristically lower threshold nature, we set its minimum detection threshold to 0.05. For GLIP and YOLO-WORLD, we establish a higher threshold of 0.1 to maintain a balance between precision and recall in object detection.  \\
\noindent
\textbf{Symbolic Regression Configuration}
The symbolic regression module processes the detection results through an iterative optimization procedure. Upon receiving detection outputs, the module generates initial logical expressions and evaluates their fitness. If termination criteria are not met, it selects the top-4 logical expressions for crossover mutation, continuing this process until reaching optimal expressions.
We configure the symbolic regression parameters based on extensive experimental validation. The population size is set to twice the number of target categories, allowing for sufficient expression diversity. The crossover and mutation factors are set to 0.5 and 0.3 respectively, providing a balanced exploration-exploitation trade-off. The optimization process continues for 5,000 iterations or until convergence criteria are satisfied. \\
\noindent
\textbf{Computational Resources}
Our experimental framework utilizes a mixed compute infrastructure optimized for different computational demands. Object detection inference is performed on a single NVIDIA RTX 4090 GPU with 24GB memory. Due to our framework's plug-and-play design, even traditional object detectors can be integrated with minimal resource requirements, making our approach adaptable to various hardware configurations. The symbolic regression component of SymbolicDet runs on Intel(R) Xeon(R) Silver 4214R CPU processors, which are well-suited for the parallel exploration of symbolic search. For LLM reasoning, we utilize qwen-series models. This distributed computational approach ensures efficient processing of our training-free pattern discovery pipeline while maintaining practical performance for real-world applications.
% 

\subsection{Main Results}
We evaluate our approach from multiple perspectives: effectiveness across different object detection architectures, comparison with fine-tuning approaches, and benchmarking against traditional event detection methods. \par
% 
\noindent
\textbf{Comparison with Different Detection Architectures.} We first evaluate our framework using three state-of-the-art open-vocabulary detectors: APE, GLIP, and YOLO-WORLD, which represent diverse architectural choices in both visual and language processing. These detectors employ different visual backbones (VIT \cite{dosovitskiy2020image}, Swin-L \cite{liu2021swin}, and YOLOv8) and language models (EVA-CLIP \cite{sun2023eva}, CLIP \cite{radford2021learning}, and BERT \cite{devlin2019bert}). As shown in Table \ref{table:diff_detector}, all three detectors achieve strong performance without any fine-tuning, with APE consistently outperforming others across all five anomaly event scenarios. This superior performance of APE can be attributed to its more sophisticated visual-language alignment mechanism and larger pre-training dataset, which enables better transfer of knowledge to anomaly detection tasks. The consistent performance across architecturally diverse models also suggests that our framework's effectiveness is not tied to specific architectural choices, but rather stems from the fundamental synergy between symbolic reasoning and detection capabilities. \par
\noindent
\textit{Finding 1: The effectiveness of our training-free framework is architecture-agnostic, with APE's superior performance likely due to its enhanced visual-language alignment and broader pre-training.} \par
% 
\noindent
\textbf{Comparison with Fine-tuning Approaches.} To further validate the efficiency of our training-free approach, we conduct comparative experiments with fine-tuned variants on the Helmet-Mac and Multi-rods Fishing datasets. We implement two common fine-tuning strategies: LORA fine-tuning\cite{hu2021lora} and Prompt tuning \cite{lester2021power}, representing different levels of parameter adaptation. Results in Table \ref{table:fine_tune_perf} demonstrate that our training-free approach achieves comparable performance to fine-tuned models. This intriguing finding suggests that our training-free approach effectively leverages the model's general understanding of visual-language relationships. Additionally, the symbolic reasoning component provides a more structured way to capture visual event patterns compared to implicit learning through fine-tuning, achieving similar effectiveness without the computational overhead of parameter adaptation.\par
\noindent
\textit{Finding 2: Our training-free approach is comparable to fine-tuning methods, possibly due to better preservation of general visual-language understanding and more structured pattern discovery.} \par
% 
\begin{table}[!t]
\centering 
\setlength{\tabcolsep}{0.9mm} 
\renewcommand\arraystretch{0.95} 
\caption{Performance of different fine-tuned methods and with or without SymbolicDet module. Lora indicates whether to Lora-tuning the APE model. Prompt indicates whether to Prompt-tuning the APE model (AUROC\%)}
\label{table:fine_tune_perf}
\label{abl_frame}
\begin{tabular}{ccc|c|c}
% \begin{tabular}{cm{0.5cm} cm{0.5cm} cm{0.5cm}|cm{0.5cm}|cm{0.5cm}}
\toprule
\textbf{Lora} & \textbf{Prompt} & \underline{\textbf{Our}} & \textbf{Helmet-Mac} & \textbf{Multi-rods Fishing}   \\ \midrule
\underline{\hspace{0.2cm}} & \underline{\hspace{0.2cm}}  & \underline{\hspace{0.2cm}}  & 67.41 & 66.82     \\ 
\checkmark & \underline{\hspace{0.2cm}}  & \underline{\hspace{0.2cm}}  & 84.11 & 75.86     \\ 
\underline{\hspace{0.2cm}}  & \checkmark & \underline{\hspace{0.2cm}}  & 67.42 & 66.82     \\ 
\rowcolor{cyan!10} \underline{\hspace{0.2cm}} & \underline{\hspace{0.2cm}} & \checkmark  & 83.18 \small\color{red}(+15.77) & 75.16 \small\color{red}(+8.34)    \\ 
\rowcolor{cyan!10} \checkmark & \underline{\hspace{0.2cm}} & \checkmark  & 95.67 \small\color{red}(+11.56) & 78.44 \small\color{red}(+2.58)    \\ 
\rowcolor{cyan!10} \underline{\hspace{0.2cm}}  & \checkmark &  \checkmark  & 81.62 \small\color{red}(+14.2) & 76.06 \small\color{red}(+9.24)     \\ 
\bottomrule
\end{tabular}
\end{table}
% 
\noindent
\textbf{Comparison with present Methods.} To contextualize our approach within the broader landscape of event detection methods, we evaluate on the UCSD Ped2 benchmark and compare against state-of-the-art approaches. As showed in Table \ref{table:ucsd_perf}, our method achieves an impressive 98.7\% accuracy without utilizing semantic-level annotations or task-specific fine-tuning, approaching the state-of-the-art performance (99.7\%). This minimal performance gap is particularly interesting considering the vast difference in approach complexity. We hypothesize that this effectiveness stems from two factors: first, the pre-trained detectors already possess rich semantic understanding that generalizes well to visual event pattern; second, our symbolic reasoning framework effectively translates this semantic knowledge into explicit detection rules, potentially capturing patterns that are similar to those learned by supervised methods but in a more interpretable manner.\par
\noindent
\textit{Finding 3: The near-SOTA performance on UCSD Ped2 suggests that combining pre-trained knowledge with symbolic reasoning can effectively match supervised learning capabilities.} \par
% 
\begin{table}[!t]
\centering 
\setlength{\tabcolsep}{1.5mm} 
\renewcommand\arraystretch{0.95} 
\caption{The overall performance on the UCSD ped2 \cite{wang2010anomaly} benchmark.This intuitively reflects that the performance of SymbolicDet (APE) without training is very close to the current SOTA.}
\label{table:ucsd_perf}
\label{tab_mlvu}
\begin{tabular}{c|c|c}
\toprule
\textbf{Training-free} & \textbf{Methods}  & \textbf{score (\%) }  \\ \midrule
  & SD-MAE \cite{ristea2024self} & 95.4   \\
 & FastAno \cite{park2022fastano} & 99.3    \\
\textbf{$\times$} & VALD-GAN \cite{singh2024vald} & 97.74 \\
 & MAMA \cite{hong2024making} & 98.2  \\
 & Backgroud-Agnostic \cite{georgescu2021background} & 98.7  \\ 
 & DMAD \cite{liu2023diversity} & \textbf{99.7}  \\ \midrule
% \multicolumn{4}{c}{\textbf{\textit{\underline{Video-RAG} pipeline Implements}}} \\   \midrule
\rowcolor{cyan!10} \checkmark & \underline{SymbolicDet} & \textbf{98.7}   \\
% \rowcolor{cyan!10} \underline{Video-RAG} & 72B & 64 &  \\
\bottomrule
\end{tabular}
\end{table}
\noindent
\textbf{Ablation Studies.}
 % We conducted comprehensive ablation studies examining the contribution of symbolic regression versus manual logic, the impact of LLM integration, and the effect of varying search scales. Results confirm that SymbolicDet's effectiveness stems from the synergistic combination of symbolic reasoning and LLM guidance. Detailed results and analysis are provided in the \textit{supplementary materials}.
We conducted comprehensive ablation studies to evaluate the contribution of each component in our framework. Our analyses examined: (\romannumeral 1) the individual impacts of symbolic regression and manual logic, revealing an 18.36\% performance improvement with symbolic pattern discovery; (\romannumeral 2) the significant benefits of LLM integration on both accuracy and convergence efficiency; and (\romannumeral 3) the positive correlation between search scale and detection performance across datasets. These experiments not only validate SymbolicDet's architectural choices but also confirm that its effectiveness stems from the synergistic combination of symbolic reasoning capabilities and LLM-guided semantic understanding. Detailed results, additional visualizations, and in-depth discussion of these ablation studies are provided in the supplementary materials.
% These experiments not only validate SymbolicDet's architectural choices but also confirm that its effectiveness stems from the synergistic combination of symbolic reasoning capabilities and LLM-guided semantic understanding. The complementary nature of these components enables more robust pattern discovery than either approach alone. Detailed results, methodology, additional visualizations, and in-depth discussion of these ablation studies are provided in the supplementary materials.