\section{Related Work}
\subsection{Rejection Sampling}
Previous works have explored methods where multiple independent trajectories are sampled for a particular problem, and then we select between them to determine the final answer \cite{wang2022self, chen2024more,beeching2024scalingtesttimecompute, brown2024large}.
These approaches are typically referred to as Best-of-N or rejection sampling. 
Once several responses have been generated by the model, we need to verify which of the final answers are correct.  
Prior work has proposed the use of a trained verifier to select between responses \cite{cobbe2021training}.
This verifier, or Outcome Reward Model (ORM), generates a reward score for each trajectory, and we can then select the best response based on this score.
Alternatively, we can leverage more complex aggregation strategies like majority voting or weighted majority voting (using the reward scores) to select the final response \cite{wang2022self, chen2024more,beeching2024scalingtesttimecompute}.
Prior work has also proposed training a verifier to decide whether each step in the solution process is correct, since this can provide finer granularity feedback in order to provide a better reward signal \cite{uesato2022solving}. 
This type of verifier is called a Process Reward Model (PRM) and they have been shown to be more reliable than ORMs for domains such as math problem solving \cite{lightman2023let}.
One key challenge when designing a PRM is coming up with training data; prior work has aimed to automate the data generation process to facilitate training PRMs without large amounts of human labeled data \cite{wang2024math}.
Other alternative approaches instead use LLMs as reward models \cite{bai2022constitutional,abdulhai2023lmrl,lee2024rlaif,pan2024autonomous}. In these methods, the LLMs serve as learned reward functions that guide the LLM post-training by providing feedback on the quality of generated outputs.

\subsection{Tree Search}

Tree-based search approaches have been extremely successful in applications such as games, where systems designed using search have proven equally capable compared to top human players \cite{brown2017libratus, silver2016mastering, silver2017mastering}.
Search strategies can also be extended to LLM reasoning as a way to better navigate the space of possible trajectories.
One approach for performing tree search with math problems is through the use of a PRM to  provide reward scores for partial trajectories, and to then use these scores to guide the search process \cite{yao2024tree,zhou2023language,besta2024graph, snell2024scaling}.

There have been several methods to improve the diversity and efficiency with search strategies.
One straightforward search strategy is beam search, which explores the solution space by sampling $N$ partial trajectories, pruning out all but $k$ of these trajectories based on the PRM scores, and then expanding each retained trajectory by sampling multiple continuations for each of them \cite{snell2024scaling,beeching2024scalingtesttimecompute, qiu2024treebon}.
This can attain higher accuracy for the same compute budget relative to best-of-N sampling \cite{snell2024scaling}.
One challenge with naive beam search is that it doesn't generate diverse enough trajectories \cite{beeching2024scalingtesttimecompute}.
One proposed solution for this is Diverse Verifier Tree Search (DVTS), which first segments the beam search tree into multiple parallel subtrees, and then runs beam search independently within each subtree \cite{beeching2024scalingtesttimecompute}.
This is analogous to prior works on Diverse Beam Search for language model decoding \cite{li2016mutual,vijayakumar2016diverse}.
DVTS attains higher accuracy for wider beam widths than standard beam search; however, this has the potential additional cost of reduced KV sharing due to segmenting the tree into separate subtrees.

\subsection{Accelerating Tree Search}

There have been several prior approaches for accelerating tree search for LLM inference. 
One previous approach, REBASE, allocates different numbers of continuations to different partial trajectories based on the reward scores for each of the trajectories \cite{wu2024inference}.
This method produces more diverse trajectories than standard beam search and attains higher accuracy for the same efficiency budget (when measuring efficiency using FLOPs or number of model calls).
Additionally, one prior work aiming to accelerate Best-of-N sampling terminated generations early using partial reward signals \cite{sun2024fast}.
One alternative approach for accelerating search is to allocate different amounts of computation for different problems depending on their difficulty \cite{zhang2024scaling,snell2024scaling, beeching2024scalingtesttimecompute}.

There have also been several prior works that have designed support in serving systems for shared prefix or tree attention workloads.
Hydragen \cite{juravsky2024hydragen} proposed only storing the shared prefix for the KV cache once, and then batching together loads.
vLLM \cite{kwon2023efficient} also supports shared prefix workloads and avoids duplicating the KV cache.
SGLang \cite{zheng2023efficiently} supports Radix Attention, which compactly stores reused KV cache segments and dynamically referencing them, even for more complex tree sharing patterns.
Other works have also proposed efficient kernel implementations for computing attention with tree-structured KV sharing \cite{yao2024deft}.