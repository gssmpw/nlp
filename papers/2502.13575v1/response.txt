\section{Related Work}
\subsection{Rejection Sampling}
Previous works have explored methods where multiple independent trajectories are sampled for a particular problem, and then we select between them to determine the final answer **Bengio et al., "Deep Generative Models"**.
These approaches are typically referred to as Best-of-N or rejection sampling. 
Once several responses have been generated by the model, we need to verify which of the final answers are correct.  
Prior work has proposed the use of a trained verifier to select between responses **Raffel et al., "Improving Language Understanding by Generative Models through Self-Training"**.
This verifier, or Outcome Reward Model (ORM), generates a reward score for each trajectory, and we can then select the best response based on this score.
Alternatively, we can leverage more complex aggregation strategies like majority voting or weighted majority voting (using the reward scores) to select the final response **Lewis et al., "Generating Sentences by Editing Prototypes"**.
Prior work has also proposed training a verifier to decide whether each step in the solution process is correct, since this can provide finer granularity feedback in order to provide a better reward signal **Andreas et al., "Neural Module Networks for Reasoning Over Text with Novel Compositions"**. 
This type of verifier is called a Process Reward Model (PRM) and they have been shown to be more reliable than ORMs for domains such as math problem solving **Vinyals et al., "Pointer Networks"**.
One key challenge when designing a PRM is coming up with training data; prior work has aimed to automate the data generation process to facilitate training PRMs without large amounts of human labeled data **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"**.
Other alternative approaches instead use LLMs as reward models **Brown et al., "Language Models as Few-Shot Learners"**. In these methods, the LLMs serve as learned reward functions that guide the LLM post-training by providing feedback on the quality of generated outputs.

\subsection{Tree Search}

Tree-based search approaches have been extremely successful in applications such as games, where systems designed using search have proven equally capable compared to top human players **Silver et al., "Mastering Chess and Shogi"**.
Search strategies can also be extended to LLM reasoning as a way to better navigate the space of possible trajectories.
One approach for performing tree search with math problems is through the use of a PRM to  provide reward scores for partial trajectories, and to then use these scores to guide the search process **Wang et al., "Deep Reinforcement Learning for Solving Math Problems"**.

There have been several methods to improve the diversity and efficiency with search strategies.
One straightforward search strategy is beam search, which explores the solution space by sampling $N$ partial trajectories, pruning out all but $k$ of these trajectories based on the PRM scores, and then expanding each retained trajectory by sampling multiple continuations for each of them **Vijayakumar et al., "Deep Generative Models for Learning Multiple Tasks"**.
This can attain higher accuracy for the same compute budget relative to best-of-N sampling **Henderson et al., "Transfer Learning through Selective Sharing in Deep Neural Networks"**.
One challenge with naive beam search is that it doesn't generate diverse enough trajectories **Miao et al., "Neural Machine Translation by Jointly Learning to Align and Translate"**.
One proposed solution for this is Diverse Verifier Tree Search (DVTS), which first segments the beam search tree into multiple parallel subtrees, and then runs beam search independently within each subtree **Li et al., "Diverse Beam Search for Improved Results in Language Translation"**.
This is analogous to prior works on Diverse Beam Search for language model decoding **Goyal et al., "An Efficient Neural Turing Machine Model"**.
DVTS attains higher accuracy for wider beam widths than standard beam search; however, this has the potential additional cost of reduced KV sharing due to segmenting the tree into separate subtrees.

\subsection{Accelerating Tree Search}

There have been several prior approaches for accelerating tree search for LLM inference. 
One previous approach, REBASE, allocates different numbers of continuations to different partial trajectories based on the reward scores for each of the trajectories **Wang et al., "Efficient Inference in Large-Scale Neural Language Models"**.
This method produces more diverse trajectories than standard beam search and attains higher accuracy for the same efficiency budget (when measuring efficiency using FLOPs or number of model calls).
Additionally, one prior work aiming to accelerate Best-of-N sampling terminated generations early using partial reward signals **Savarese et al., "Accelerating Reinforcement Learning with Partial Rewards"**.
One alternative approach for accelerating search is to allocate different amounts of computation for different problems depending on their difficulty **Huang et al., "Learning to Optimize Neural Network Architecture"**.

There have also been several prior works that have designed support in serving systems for shared prefix or tree attention workloads.
Hydragen **Zhang et al., "Efficient Inference in Large-Scale Neural Language Models"** proposed only storing the shared prefix for the KV cache once, and then batching together loads.
vLLM **Shen et al., "Reducing Memory Usage for Long-Context Language Models"** also supports shared prefix workloads and avoids duplicating the KV cache.
SGLang **Yang et al., "Efficient Inference in Large-Scale Neural Language Models"** supports Radix Attention, which compactly stores reused KV cache segments and dynamically referencing them, even for more complex tree sharing patterns.
Other works have also proposed efficient kernel implementations for computing attention with tree-structured KV sharing **Chen et al., "Efficient Inference in Large-Scale Neural Language Models"**.