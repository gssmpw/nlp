% Search Strategies
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[h]%{t}{0.46\textwidth}
\centering
\includegraphics[width=0.8\linewidth]{figures/profiling.pdf}
\vspace*{-2mm}
 \caption{ 
 Correlation between approximate efficiency metrics and profiled runtime.
 We report profiled Runtime as well FLOPs), number of model calls, and total KV Cache Size (``KV Size'') as well as profiled runtime.
 We measure each metric for Beam Search, DVTS, and REBASE for the Llemma-34B model with a width of 256, and we report each metric normalized to the value for Beam Search.
 For Beam Search and DVTS, we retain $\sqrt{N}$ trajectories at each step, where $N$ is the width of the search.
 As can be seen, REBASE has similar FLOPs and number of model calls compared to beam search and DVTS, but it exhibits significantly higher runtime.
 The increased runtime is due to its increased KV cache size.
 This clearly shows that FLOPs and number of model calls are not necessarily the right proxy metrics to use when assessing search efficiency.
  }
  % \vspace{-2mm}
  \label{fig:profiling}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Profiling}

\label{sec:profiling}

When designing more efficient search algorithms, it is desirable to leverage a proxy metric to assess the efficiency of the algorithm in order to speed up iteration time, as running benchmarking in isolation for fair comparison between different search methods is more challenging and leads to slower iteration time.
Example proxy metrics for search runtime which were used in previous works include the number of model calls / tree width, as well as FLOPs \cite{qiu2024treebon,wu2024inference,snell2024scaling}.
The simplicity of these metrics makes them desirable for fast prototyping and ease of comparison.
However, as mentioned previously, FLOP count is not a good metric for memory-bound problems and may even be misleading.

To assess the utility of these proxy metrics, we profiled the throughput when running different search strategies, each using the same width.
We profiled throughput on 100 samples from the MATH500 test set on NVIDIA H100 NVL GPUs, with the Llemma-34B model and Llemma-Reward-34B PRM each on a separate GPU (model details are provided in Section \ref{sec:experimental-details}).
Results were collected by running evaluation using 8 parallel threads (which is analogous to the attainable throughput with a batch size of 8 for serving use-cases).
We also include FLOPs, model calls, and KV cache size as proxy metrics (where KV cache size is the sum of the size of the KV cache state across all steps in the search process).
To estimate FLOPs, we leverage the approximation that the number of FLOPs is proportional to the number of tokens generated (which holds for short context lengths) \cite{pope2023efficiently}.
According to existing proxy metrics, these search methods should all yield similar latency as they all have similar numbers of FLOPs and model calls.
However, as highlighted in Figure \ref{fig:profiling}, we observe that these algorithms exhibit substantial differences in terms of profiled latency.
These gaps are due to the differences in terms of number of \textit{memory operations} which need to be performed when running the workload, since generative LLM inference is typically memory bandwidth-bound \cite{kim2023full,kim2023squeezellm, hooper2024squeezed}.
The number of memory operations that need to be performed for different tree search methods are dependent on multiple factors: 

\begin{enumerate}[leftmargin=4mm]
    \item The amount of KV cache sharing between trajectories determines the size of the state that needs to be loaded for each inference step, which substantially influences the runtime if the KV cache is the dominant contributor to memory consumption relative to model weights. 
    Note that leveraging this benefit requires efficient tree attention kernels to reduce KV cache loads \cite{yao2024deft}.
    \item 
    Even with existing open-source frameworks, such as SGLang~\cite{zheng2023efficiently}, that don't incorporate efficient tree attention kernels, we can still reduce memory operations.
    If the KV cache for the sequences is too large to fit in memory (either with wide beam search or in batched use-cases), then the number of sequences that can be run in parallel will be constrained and the search process gets fragmented into multiple successive iterations. 
    This leads to performing more memory operations for the model weights, since the model weights need to be loaded for each fragment.
    \item Additionally, while the KV cache state can be reused for earlier steps in the search when generating later steps, if the memory requirements are too great then the KV cache for the earlier steps would be de-allocated and would need to be recomputed, which would increase latency.
\end{enumerate}

These factors mean that reducing the KV cache size by promoting KV cache sharing between trajectories is crucial for reducing latency, even though the KV cache size is \textit{not} considered by existing proxy metrics.