\begin{abstract}
Test-time compute scaling has emerged as a new axis along which to improve model accuracy, where additional computation is used at inference time to allow the model to think longer for more challenging problems. 
One promising approach for test-time compute scaling is search against a process reward model, where a model generates multiple potential candidates at each step of the search, and these partial trajectories are then scored by a separate reward model in order to guide the search process.
The diversity of trajectories in the tree search process affects the accuracy of the search, since increasing diversity promotes more exploration.
However, this diversity comes at a cost, as divergent trajectories have less KV sharing, which means they consume more memory and slow down the search process.
Previous search methods either do not perform sufficient exploration, or else explore diverse trajectories but have high latency.
We address this challenge by proposing Efficient Tree Search (\ours), which promotes KV sharing by pruning redundant trajectories while maintaining necessary diverse trajectories. 
\ours incorporates a linear programming cost model to promote KV cache sharing by penalizing the number of nodes retained, while incorporating a semantic coverage term into the cost model to ensure that we retain trajectories which are semantically different.
We demonstrate how \ours can achieve \textbf{1.8}$\times$ reduction in average KV cache size during the search process, leading to \textbf{1.4}$\times$ increased throughput relative to prior state-of-the-art methods, with minimal accuracy degradation and without requiring any custom kernel implementation. Code is available at: \url{https://github.com/SqueezeAILab/ETS}.

\end{abstract}