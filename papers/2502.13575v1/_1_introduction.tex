
% Search Strategies
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[h]%{t}{0.46\textwidth}
\centering
\includegraphics[width=\linewidth]{figures/mainfig.pdf}
\vspace*{-2mm}
 \caption{ 
  Visualization of our methodology for accelerating search while maintaining accuracy.
  (Left): Prior work (REBASE) achieve high accuracy with search by sampling more or less continuations from the candidate leaf nodes depending on their reward scores, thereby trading off exploration and exploitation. 
  However, this approach leads to larger KV cache size since it retains divergent trajectories which do not share nodes earlier in the tree.
  (Middle) One approach to reduce the cost is to penalize the number of nodes in the tree, thereby encouraging sharing prior KV cache state.
  However, this has the downside of pruning necessary semantically diverse trajectories, which degrades accuracy.
  (Right) \ours (our method) attains high efficiency by pruning out redundant nodes and promoting KV cache sharing, while ensuring that semantically diverse trajectories are retained, thereby allowing our approach to perform sufficient exploration.
  }
  % \vspace{-2mm}
  \label{fig:main}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

With the increasing challenges with scaling up language model pretraining due to compute and data availability, scaling test-time computation is emerging as a new axis along which to improve model performance \cite{snell2024scaling}. 
By increasing the amount of computation used at test time, we can allow Large Language Models (LLMs) to think longer for more challenging problems by generating long chain-of-thought reasoning, where the model lays out step-by-step reasoning to help guide itself to the final answer \cite{wei2022chain}.
This is particularly useful for improving accuracy in domains which require problem-solving capabilities, such as challenging math and coding problems \cite{snell2024scaling,brown2024large}.
Test-time scaling can also allow us to trade off increased compute demands at inference time with compute used during training time, since a smaller model with increased compute allocation at test time can outperform a larger pretrained model \cite{beeching2024scalingtesttimecompute}. 

One prominent method to scale up test time compute is through search, where we generate multiple possible candidate solutions and then filter them down to produce the final response.
An emerging method for performing search is to use a \textit{Process Reward Model} (PRM) to score partial trajectories in order to inform the search process \cite{snell2024scaling, beeching2024scalingtesttimecompute}.
These strategies generate multiple potential candidates at each step, score each of them using a PRM, and then choose which trajectories to retain based on these scores.
While these approaches yield promising results in terms of improving accuracy through test-time scaling, the increased inference costs are substantial, as many trajectories must be generated and scored before a final answer is selected.


Prior work has highlighted that generative LLM inference is typically \textit{memory bandwidth-bound} \cite{kim2023squeezellm,kim2023full, hooper2024squeezed}, meaning that inference efficiency is limited by how quickly we can transfer data rather than the peak compute of the GPU (also referred to as memory wall~\cite{gholami2024ai}).
There are typically two dominant contributors to memory consumption: the model weights, and the KV cache, which stores the embedded representation of the sequence.
Both the model weights and KV cache have to be loaded into memory for each generation step.
For short sequence lengths, the model weights are typically the dominant contributor to memory consumption and bandwidth, whereas for longer context lengths and batched inference the KV cache becomes the main memory bottleneck \cite{hooper2024kvquant}.
For test-time compute strategies such as tree search, many trajectories are generated in parallel. 
Since each unique trajectory requires separate KV cache state, for wider beam search, the KV cache becomes a critical memory bottleneck.

Previous works on efficient search have either used number of model calls or FLOPs as the target efficiency metric when performing search \cite{qiu2024treebon,wu2024inference, snell2024scaling}. 
However, the actual inference costs of search are not necessarily proportional to the number of model calls or FLOPs, due to the memory-bound nature of generative inference.
The number of memory operations performed during the search process is critical for determining its runtime.
Crucially, the number of memory operations required for search is not necessarily correlated with FLOPs or model calls; for example, when we sample several parallel generations, we can amortize the memory operations for the model across these generations, whereas if we perform several successive generations where we iteratively refine the same response, then we need to load the model separately for each refinement.
A key challenge with these proxy efficiency metrics is that they neglect the influence of KV cache sharing between trajectories, which has a substantial impact on the memory consumption and bandwidth requirements for performing search. 
For example, with two trajectories which share the KV cache for most of their previous steps, this will require substantially fewer memory operations when performing further generations than if each trajectory has an entirely separate KV cache state.

In this work, we analyze the efficiency and performance of existing search methods.
We focus in particular on parallel tree search algorithms like beam search and REBASE \cite{wu2024inference} since they are able to search multiple paths in parallel and are therefore more efficient for LLM test-time scaling than sequential search algorithms like MCTS \cite{snell2024scaling,beeching2024scalingtesttimecompute,wu2024inference}.
We identify how standard beam search ends up collapsing to a small number of trajectories (meaning that it exhibits high KV cache sharing in the search tree), but this has the consequence of low diversity and reduced accuracy even for wide beam widths.
Conversely, methods such as REBASE generate more diverse trajectories and are therefore able to attain higher accuracy and better scaling to wider beam widths.
However, while these methods may seem to be as efficient for the same width when using number of model calls or FLOPs as the efficiency metric, these approaches lead to substantially reduced KV cache sharing, which in turn translates to substantial inference overheads.
The reduction in KV sharing leads to substantial increases in latency when using an optimized serving system such as SGLang \cite{zheng2023efficiently} which can exploit KV sharing. 

These differences between beam search and diverse tree search methods highlight the trade-off between promoting diverse trajectories to attain high accuracy and increasing the amount of KV sharing that we can exploit.
There is therefore a need to develop search methods which can produce diverse trajectories and attain high accuracy, but which also have high KV sharing in order to not compromise on inference efficiency.
Our work aims to tackle this challenge by proposing a search algorithm that promotes KV cache sharing to improve efficiency, while retaining the diverse trajectories that are necessary for facilitating exploration in order to attain high accuracy.

The contributions of our work are as follows:
\begin{enumerate}[leftmargin=4mm]
    \item We present profiling data which demonstrates how existing proxy metrics for search efficiency are not necessarily correlated with search runtime due to the memory-bound nature of LLM inference as well as the impacts of KV cache sharing on attainable throughput.
    \item 
    To allow for accurate and diverse search while still maintaining inference efficiency, we present Efficient Tree Search (\ours), which is highlighted in detail in Figure \ref{fig:main}.
    \ours incorporates a linear programming cost model to promote KV cache sharing by penalizing divergent branches, thereby improving inference efficiency. 
    \item In order to retain diversity while promoting KV sharing, we augment our cost model with a coverage term that compels the model to retain semantically diverse trajectories,
    % \amir{semantically}
    while still allowing our KV cache cost model to enhance KV sharing.
    This approach allows our method to retain diverse trajectories while promoting KV cache sharing by pruning out semantically similar trajectories.
    We demonstrate how \ours can achieve \textbf{1.8}$\times$ reduction in average KV cache size during the search process, leading to \textbf{1.4}$\times$ speedups relative to REBASE with minimal accuracy degradation. Notably, our approach doesn't require dedicated kernel implementations on top of SGLang, and is compatible with the benefits attainable from more efficient kernels for computing attention with tree structured KV sharing \cite{yao2024deft}.
\end{enumerate}