
\begin{table*}[t]
\centering
\caption{
Accuracy versus KV cache size for REBASE as well as \ours. 
Results are provided for MATH500 and GSM8K for the Llemma-34B and Mistral-7B-SFT models.
We report the KV cache size reduction (``KV Red.'') for each width (relative to REBASE), where higher is better since it implies a reduction in memory consumption.
}
\label{tab:results}
\scriptsize
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.2}
%
%----------------- LEFT SUB-TABLE -----------------%
\vspace{1.5mm}
\begin{subtable}{0.47\linewidth}
    \centering
    \begin{tabular}{c|cc|cc|cc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{Width=16}} & \multicolumn{2}{c|}{\textbf{Width=64}} & \multicolumn{2}{c}{\textbf{Width=256}} \\
    \cline{2-7}
     & \textbf{Acc.} & \textbf{KV Red.} & \textbf{Acc.} & \textbf{KV Red.} & \textbf{Acc.} & \textbf{KV Red.} \\
    \midrule
    \multicolumn{7}{c}{\textbf{Llemma-34B}} \\
    \midrule
    REBASE       &   47.2         & 1$\times$   &           50.8 & 1$\times$  &   52.0         & 1$\times$  \\
    \hd \textbf{\ours}   &  \textbf{47.0}          & \textbf{1.2}$\times$  &  \textbf{51.2}          & \textbf{1.5}$\times$  &    \textbf{52.8}        & \textbf{1.8}$\times$  \\

    \midrule
    \multicolumn{7}{c}{\textbf{Mistral-7B-SFT}} \\
    \midrule
    REBASE      &     38.8       & 1$\times$   &  43.4          & 1$\times$   &     42.4       & 1$\times$   \\
    \hd \textbf{\ours}   &   \textbf{39.4}         & \textbf{1.3}$\times$  &  \textbf{43.2}          & \textbf{1.3}$\times$  &    \textbf{42.2}        & \textbf{1.7}$\times$  \\
    \bottomrule
    \end{tabular}
    % \vspace{1ex}
    \newline
    \newline
    \small\textbf{MATH500}
\end{subtable}
% \hfill
\hspace{3mm}
%----------------- RIGHT SUB-TABLE -----------------%
\begin{subtable}[t]{0.47\linewidth}
    \centering
    \begin{tabular}{c|cc|cc|cc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{Width=16}} & \multicolumn{2}{c|}{\textbf{Width=64}} & \multicolumn{2}{c}{\textbf{Width=256}} \\
    \cline{2-7}
     & \textbf{Acc.} & \textbf{KV Red.} & \textbf{Acc.} & \textbf{KV Red.} & \textbf{Acc.} & \textbf{KV Red.} \\
    \midrule
    \multicolumn{7}{c}{\textbf{Llemma-34B}} \\
    \midrule
    REBASE       &    87.7        & 1$\times$   &   89.0         & 1$\times$   &      89.3      & 1$\times$   \\
    \hd \textbf{\ours}   &    \textbf{87.5}        & \textbf{1.5}$\times$  &   \textbf{89.3}         & \textbf{1.7}$\times$ &   \textbf{89.3}          & \textbf{1.8}$\times$  \\
    \midrule
    \multicolumn{7}{c}{\textbf{Mistral-7B-SFT}} \\
    \midrule
    REBASE       &     88.6       & 1$\times$   &  89.1          & 1$\times$   &   90.1        & 1$\times$   \\
    \hd \textbf{\ours}   &    \textbf{88.3}        & \textbf{1.2}$\times$  &   \textbf{89.2}         &  \textbf{1.6}$\times$  &     \textbf{89.6}       & \textbf{1.3}$\times$  \\
    \bottomrule
    \end{tabular}
    % \vspace{1ex}
    \newline
    \newline
    \small\textbf{GSM8K}
\end{subtable}
\end{table*}

\section{Evaluation}

\subsection{Experimental Details}

\label{sec:experimental-details}

We leverage the open-source REBASE code for the balanced sampling implementation \cite{wu2024inference}, and we use SGLang for serving models \cite{zheng2023efficiently}.
For all experiments in our evaluation, we leverage temperature sampling with a temperature of 1.0 and we fix the REBASE temperature at 0.2, which are the default settings in the open-source code for \cite{wu2024inference}.
We use the final PRM score at each step as the reward for that step, and we select the final answer with weighted majority voting using the final PRM score for each trajectory as the weight.
We use these aggregation strategies since they have been shown to outperform other methods of aggregating trajectories to determine the final response \cite{beeching2024scalingtesttimecompute}.
As in \cite{wu2024inference}, we reduce the search width each time a retained trajectory completes.
We set $\lambda_d = 1$ throughout our evaluation, and we sweep over $\lambda_b \in [1,2]$ (with increasing $\lambda_b$ corresponding to more aggressive KV compression) and select the largest value of $\lambda_b$ which doesn't degrade accuracy by greater than $0.2\%$.

We provide results for two groups of models. We leverage the Llemma-34B model (finetuned on Metamath) along with the Llemma-34B PRM from \cite{wu2024inference}.
We also use the Mistral-7B model finetuned on Metamath as well as the corresponding Mistral-7B PRM from the Math-Shepherd paper \cite{wang2024math}.
We evaluate these models for search widths of 16, 64, and 256, and we report results on the MATH500 and GSM8K \cite{cobbe2021training} datasets.

We compare against several baseline search strategies.
We include results for beam search both with 4 trajectories retained at each step and with  $\sqrt{N}$ trajectories retained at each step, where $N$ is the initial width of the search, as in \cite{snell2024scaling}.
We also include comparisons with DVTS both with 4 trajectories retained at each step and with $\sqrt{N}$ trajectories retained at each step (where the number of trajectories retained at each step is also the same as the number of separate subtrees), as in \cite{beeching2024scalingtesttimecompute}.
Finally, we provide comparisons against REBASE, which serves as our strongest baseline due to its high accuracy relative to search width \cite{wu2024inference}.



\subsection {Results}



Figure \ref{fig:results} provides evaluation of our methodology as well as comparisons against several baseline methods.
We report accuracy results relative to efficiency (in terms of total KV cache size), and we provide results for the Llemma-34B model for both MATH500 and GSM8K datasets.
We also report results for the beam search, DVTS, and REBASE baseline methods.
Our results demonstrate that our approach, which considers both diversity and efficiency, is able to attain a better accuracy versus efficiency trade-off than existing search strategies.

Table \ref{tab:results} shows results for the Llemma-34B and Mistral-7B models.
We report results on both MATH500 as well as GSM8K.
We provide accuracy results as well as profiled KV cache compression estimates.
These results highlight how the benefits of our search strategy are consistent across different model families and datasets, as we are able to maintain accuracy while obtaining consistent efficiency benefits relative to the REBASE baseline \cite{wu2024inference}.

\subsection{Throughput Benchmarking}

\begin{table}[t!]
\caption{
Throughput for our approach relative to REBASE \cite{wu2024inference}.
Results were measured on NVIDIA H100 GPUs using the Llemma-34B model, evaluated on 100 samples from the MATH500 test set (with the accuracy reported for the full test set).
We report throughput improvements using a beam width of 256.
We also include the reduction in KV cache size (normalized to REBASE), as well as the accuracy for each approach.
}
% \vspace{-5mm}
\scriptsize
% \vspace{1mm}
\vspace{2mm}
\label{tab:throughput}
\centering{
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{c|c|c|c}
\toprule
   \textbf{Method} & \textbf{Accuracy} & \textbf{KV Reduction} & \textbf{Throughput}\\
   \midrule
   \midrule
   REBASE  & 52.0 & 1$\times$ & 1$\times$  \\
   
   % \hc \ours  & & & \\ 
   \hd \textbf{\ours}  & \textbf{52.8} & \textbf{1.8}$\times$ &  \textbf{1.4}$\times$ \\
\bottomrule
\end{tabular}
}
}
         
\end{table}



Table \ref{tab:throughput} provides measured throughput for REBASE as well as \ours on H100 NVL GPUs.
We benchmark both REBASE and \ours using [4,8,16,32] parallel threads (which is representative for the serving scenario with a batch size equal to the number of threads) and select the best configuration for each.
We run benchmarking with the main LLM and the PRM each on a separate H100 NVL GPU, and for \ours we co-locate the embedding model on the same GPU as the reward model.
We observe {1.4}$\times$ increased throughput relative to the baseline REBASE method, demonstrating how the increased KV cache sharing from our algorithm translates to higher throughput, without requiring any custom kernels.

\subsection{Ablation}

\begin{table}[t!]
\caption{
Ablation for our methodology. We include results on MATH500 for different beam widths with the Llemma-34B model, and we report KV budget estimates.
We compare \ours with only applying the KV budget term in the cost model (``\ours-KV'').
}
\scriptsize
\vspace{2.5mm}
\label{tab:ablations}
\centering{
\begin{tabular}{c|cc|cc|cc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{Width=16}} & \multicolumn{2}{c|}{\textbf{Width=64}} & \multicolumn{2}{c}{\textbf{Width=256}} \\
    \cline{2-7}
     & \textbf{Acc.} & \textbf{KV Red.} & \textbf{Acc.} & \textbf{KV Red.} & \textbf{Acc.} & \textbf{KV Red.} \\
    \midrule
    REBASE       &   47.2         & 1$\times$   &           50.8 & 1$\times$  &   52.0         & 1$\times$  \\
    \hc \ours-KV &  \textbf{47.2}          & \textbf{1.3}$\times$  &  51.4          & 1.3$\times$  &    52.8        & 1.7$\times$  \\
    \hd \textbf{\ours}   &  47.0          & 1.2$\times$  &  \textbf{51.2}          & \textbf{1.5}$\times$  &    \textbf{52.8}        & \textbf{1.8}$\times$  \\
    \bottomrule
\end{tabular}
}
         
\end{table}

Table \ref{tab:ablations} also provides an ablation for our method.
We report results for the Llemma-34B model on the MATH500 dataset when using our cost model method with only the term that promotes KV cache sharing and with both terms together.
For the results with only our KV cache sharing term in the cost model, we fix $\lambda_d = 0$ and sweep over $\lambda_b \in [0.75,1.25]$, selecting the largest value of $\lambda_b$ which doesn't degrade accuracy by greater than $0.2\%$.
These results highlight that the diversity term allows us to push to more aggressive compression without degrading accuracy.
This occurs since without the diversity term, our method cannot distinguish redundant trajectories from necessary diverse trajectories when determining which subset to retain.
