% http://www.iros25.org/CallForPapersv2
% The page limit is 6 pages, with up to 2 extra pages (with extra page charge). The page limit includes the references, appendixes etc.
% IROS review process is single-blind (reviewers stay anonymous, but author information is visible)
% Paper submission: Open Jan. 5ï¼› Deadline March 1, 2025 (23:59 PST) / Mar 2 18:59 AEDT
% Video submission: Open (1) Jan 5-Feb 24, 2025 (23:59 PST)  / Feb 25 18:59 AEDT, and (2) Mar. 3-6, 2025 (23:59 PST) / Mar 7 18:59 AEDT. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\usepackage{graphicx}
\usepackage{subfig}
\usepackage{placeins}
\usepackage[inkscapearea=page]{svg}
\let\labelindent\relax
\usepackage{enumitem}
\usepackage{url}
\usepackage{multirow}
\def\UrlBreaks{\do\/\do-}
% cosmetic formatting
\usepackage{flushend}
\usepackage{balance}
\usepackage{microtype}
\usepackage[UKenglish]{babel}
\usepackage{pdfcomment} % for adding figure alt texts
\usepackage{stackengine}
\newsavebox\mybox
\newcommand\Includegraphics[2][]{\sbox{\mybox}{%
  \includegraphics[#1]{#2}}\abovebaseline[-.5\ht\mybox]{%
  \addstackgap{\usebox{\mybox}}}}
\usepackage{booktabs}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

\title{\LARGE \bf
Collaborative Object Handover in a Robot Crafting Assistant
}
% How to Train Your Robot Crafting Assistant: Towards Human-Like Collaborative Handover
% submitted on arXiv so the online study paper can ref it


\author{Leimin Tian$^{1}$, Shiyu Xu$^{2}$, Kerry He$^{2}$, Rachel Love$^{2}$, Akansel Cosgun$^{3}$, Dana Kuli\'{c}$^{2}$% <-this % stops a space
\thanks{*supported by the ARC Future Fellowship FT200100761}% <-this % stops a space
\thanks{$^{1}$Leimin Tian is with CSIRO Robotics, 
        Clayton, VIC 3168, Australia
        {\tt\small leimin.tian@csiro.au}}%
\thanks{$^{2}$Shiyu Xu, Kerry He, Rachel Love, Dana Kuli\'{c} are with the Faculty of Engineering, Monash University,
        Clayton, VIC 3168, Australia
        {\tt\small \{shiyu.xu, kerry.he, rachel.love, dana.kulic\}@monash.edu}}%
\thanks{$^{3}$Akansel Cosgun is with the School of Information Technology, Deakin University,
        Burwood, VIC 3125, Australia
        {\tt\small akan.cosgun@deakin.edu.au}}%
}


\begin{document}


\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Robots are increasingly working alongside people, delivering food to patrons in restaurants or helping workers on assembly lines. These scenarios often involve object handovers between the person and the robot. To achieve safe and efficient human-robot collaboration (HRC), it is important to incorporate human context in a robot's handover strategies. Therefore, in this work, we develop a collaborative handover model trained on human teleoperation data collected in a naturalistic crafting task. To evaluate the performance of this model, we conduct cross-validation experiments on the training dataset as well as a user study in the same HRC crafting task. The handover episodes and user perceptions of the autonomous handover policy were compared with those of the human teleoperated handovers. While the cross-validation experiment and user study indicate that the autonomous policy successfully achieved collaborative handovers, the comparison with human teleoperation revealed avenues for further improvements.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
Object handover is a common task in human-robot collaboration (HRC), in which an item is transferred between a human and a robot~\cite{ortenzi2021object}. To develop an autonomous human-robot handover policy, existing approaches either react to pre-defined triggers (e.g., specific hand gestures~\cite{kwan2020gesture}), or train machine learning models using data collected from isolated handover episodes between two people or in human-robot pairs (e.g.,~\cite{chan2021experimental, wiederhold2024hoh}). However, using pre-defined triggers imposes learning costs for the person and interrupts the flow of activities, requiring additional physical and mental effort for the person to explicitly command the robot~\cite{carissoli2023mental}. Further, as handovers are context-dependent~\cite{ortenzi2021object}, training from isolated episodes limits the resulting model's applicability in handovers that occur in naturalistic contexts with pre- and post-handover tasks. 

There has been growing interest in the social-interactive aspects of handovers in addition to the control and trajectory planning aspects, focusing on how a person and a robot may communicate and coordinate during handovers~\cite{moon2021design, iori2023dmp}. While these studies investigated communicating information such as intention or hesitation during individual episodes of object transfer, how people and robots collaborate in a complex task involving multiple handovers remains largely under explored. Handovers in a broader task context require providing objects at the right place and the right time, when the person may be engaged in the task and not giving explicit cues to the robot. Further, research on human behaviours in handovers largely focuses on hand locations, while additional informative behaviours such as gaze or whole body movements are under explored~\cite{hart2014gesture}. Evaluation of human-robot handover policies is often conducted in a short-term, handover-centred design without incorporating human and task contexts, e.g., focusing on the smoothness of the robot's motions and trajectories~\cite{duan2024human}. This results in a gap between a handover policy's outcomes in isolated episodes compared to when multiple handovers are serving an overall task goal. For example, a policy that emphasises optimising the instantaneous ergonomics risk factors in individual handovers may become over-assistive and reduce worker well-being in repeated interactions~\cite{zolotas2024imposing}.

\begin{figure*}[tb]
  \centering
    \pdftooltip{\includegraphics[width=0.85\linewidth]{S2_fig/HandoverStatesAutoPhoto.pdf}}{Top half of the image shows a photo of the experimental layout, with the work space where participants sit on the left and storage space where the experimenter sits on the right. The Fetch robot is moving between these two desks. A partially built birdhouse can be seen on top of the work desk. A tripod with a camera on top stands next to the work desk. A storage box can be seen next to the storage space. The robot also has a camera on its head. Below the photo, the state transitions during a handover episode are shown, with three timing adaptations and two handover location adaptations highlighted at key state transitions.} 
  \caption{Our autonomous handover policy performs three temporal adaptations: episode start, object transfer start, and object transfer completion timing. Spatial adaptation of object transfer position (left, middle, right handovers) is first performed at episode start, then updated at object transfer start.}
  % \Description{Handover type}
  \label{episode_overview}
  % \vspace{-1em}
\end{figure*}

Therefore, we are motivated to understand human-robot handovers in a naturalistic task context where they serve a collaborative goal. In particular, we develop an autonomous handover policy, as shown in Figure~\ref{episode_overview}, trained with the Functional And Creative Tasks Human-Robot Collaboration dataset (the FACT HRC dataset)~\cite{tian2023crafting}. In the FACT HRC dataset, a mobile manipulator robot was teleoperated by a hidden human operator to serve as a crafting assistant to 20 users, each in an hour-long interaction session that involved multiple handovers of diverse objects used for the crafting task. We then replicate the crafting assistant task to evaluate the autonomous handover model with 20 participants. Further, by comparing the autonomous handover policy and the human operator's handover strategies, we investigate design implications to improve collaborative handovers. Our main contributions are:
\begin{enumerate}
    \item We developed a collaborative handover policy for a mobile manipulator robot, which adapts the handover timing and object transfer location based on a user's upper body movements using supervised learning;
    \item We conducted a user study and evaluated the developed autonomous policy during multiple purposeful handovers with mixed robot roles in a collaborative task context;
    \item We compared the objective performance and subjective user perception of autonomous and teleoperated handovers, which informs future HRC research;
    \item The code and data are open-sourced\footnote{\url{https://doi.org/10.26180/25449640}}.
\end{enumerate}

% In Section~\ref{sec:bg}, we briefly reviewed related research on human-robot handovers. In Section~\ref{sec:method}, we describe the design of the machine learning experiments for developing the autonomous handover policy and the user study evaluation. We present results of the machine learning experiments in Section~\ref{sec:ml} and the user study results in Section~\ref{sec:s2}. We further compare the autonomous handovers with teleoperation in Section~\ref{sec:S1vsS2}. In Section~\ref{sec:diss}, we discuss key implications and future directions. Finally, we summarise this work in Section~\ref{sec:con}.


\section{Related Work}\label{sec:bg}
% Here we review previous studies on human-robot handover as a collaborative task and existing approaches to implement autonomous handovers.
As reviewed by Duan~et~al.~\cite{duan2024human}, recent handover research investigates how different methods for trajectory generation, motion planning, or timing control influence the objective handover performance (e.g., duration or success rate) and subjective human perceptions (e.g., trust or fluency). Existing studies focus on either human-to-robot (H2R) handovers (52.5\%) or robot-to-human (R2H) handovers (37.5\%), with limited research on bidirectional handovers (10\%), i.e., exchanging objects. In H2R handovers, previous work focused on predicting human behaviours or intentions under uncertainty with machine learning models (e.g.,~\cite{mavsar2022rovernet, yang2022model}). In R2H handovers, previous work focused on enhancing human comfort and increasing the movement efficiency of both parties~\cite{qin2022task, lagomarsino2023maximising}. 

Existing handover policies may adopt learning-based, control-based, or analysis-based approaches. For example, K{\"a}ppler~et~al.~\cite{kappler2023optimizing} developed an adaptive method for a table-mounted robot arm, which updated the object transfer location based on the location of a user's hand as observed from an RGB-D camera and proximity sensors on the gripper. This adaptive R2H handover model was compared with a non-adaptive model using a pre-defined object transfer location in a between-subject study, in which the robot handed a cup to a participant in four consecutive repetitions. They found that participants exhibited motor learning and adaptation to the robot's handover, which necessitated evaluating handover models in repeated episodes. 
% A custom questionnaire was used to collect participant's perception in addition to measures of handover time and location. The proposed method triggered handover initiation and completion at updated object transfer time and positions based on a participant's behaviours. However, comparison between the repeated episodes also showed that participants learned to adapt to the robot in both adaptive and non-adaptive handovers, with the adaptive method resulting in lower perception of trust, fluency and safety. 
% However, their user study did not incorporate collaborative task context (i.e., purposeful handovers) and the proposed adaptive handover model relied on rules based only on a participant's hand location.
In another example, Kedia~et~al.~\cite{kedia2024interact} developed a transformer-based human intent prediction model pre-trained on human-human collaborative manipulation data and fine-tuned on human-robot data with a teleoperated robot arm. This study incorporated task context in the handover episodes, such as a person and the fixed robot arm taking turns to each pick up one of two objects from a cart to place onto a shared table. However, this work focused on short interaction (3-15s) without direct object transfers between the human and the robot.
Zhuang~et~el.~\cite{zhuang2022goferbot} developed an R2H handover model in which a table-mounted robot arm delivered four legs of an IKEA table to a human to assist in assembly. The handover was either triggered by visual-based recognition of human actions, or by voice command (simulated by a supervisor pressing a button after a participant gave the voice command). They evaluated handover time, success rate, and action recognition performance, as well as participants' perception of the collaboration fluency. This work incorporated collaborative task context in repeated handovers. However, their approach focused on predicting the handover initiation timing, leaving out other timing and location coordination in the whole process.

As shown in the above examples and identified by Duan~et~al.~\cite{duan2024human}, research on complex long-sequence tasks with handover policies capable of spatial-temporal collaboration is extremely limited. Further, as identified by Ortenzi~et~al.~\cite{ortenzi2021object}, there is limited work on adaptive handovers incorporating human social and communicative cues beyond hand locations. Moreover, both reviews identified that pre- and post-handover tasks and HRC context were rarely considered in current handover research. Our previous work~\cite{tian2023crafting} investigated spatial-temporal adaptation in a mix of H2R, R2H, and bidirectional handovers contextualised in an hour-long HRC task of assembling and painting a wooden birdhouse. While the mobile manipulator robot was teleoperated by a hidden human operator (i.e., Wizard-of-Oz), the FACT HRC dataset collected facilitates the development of a collaborative handover policy by learning from the human operator's strategies.


\section{Methodology}\label{sec:method}
We propose to train a collaborative handover policy, using data from a human teleoperator's adaptive handovers during a crafting task, where multiple purposeful handovers of mixed H2R, R2H, and bidirectional handovers are required~\cite{tian2023crafting}, as further described in Section~\ref{subsec:method-ml}. We then evaluate the autonomous handover policy in the same HRC task to understand the task outcomes and users' subjective experience, as described in Section~\ref{subsec:method-s2}. We used the Fetch mobile manipulator robot~\cite{wise2016fetch} with a 7-DOF arm and a differential drive base, with the robot holding a basket for object delivery and return during handovers. Implementation of the handover model and data collected from our user study (ROS bags, processed csv files, anonymous questionnaire responses) are open-sourced for research purposes. The study protocols were reviewed and approved by Human Research Ethics Committee at Monash University (Project ID 31927).

\subsection{Developing a collaborative handover model}\label{subsec:method-ml}
We trained the autonomous handover policy using the FACT HRC dataset~\cite{tian2023crafting}, which contains 20 HRC sessions (10 women, 10 men, age 27.0$\pm$5.2, 19 right-handed, 1 left-handed) with 565 handover episodes (57\% R2H, 34\% H2R, 9\% bidirectional) in total. Each HRC session is approximately an hour long with multimodal data collected at time steps of 0.1s. We used the FACT-processed segment of the dataset, which includes non-identifiable csv data of the robot's status (velocity and coordinates of the base and arm joints, coordinates of the end effector goal for object transfers), the operator's controls, the facial and upper body keypoints (normalised $(x,y,z)$ coordinates of 25 skeletal keypoints with prediction confidences~\cite{bazarevsky2020blazepose}) of participants estimated from an RGB-D camera (OAK-D) positioned next to the participants, and emotion estimations (categorical emotions with intensity, arousal and valence values, inferred from participant's facial expressions~\cite{toisoul2021estimation}) from this camera and the robot's onboard camera. Figure~\ref{episode_overview} provides an overview of the states in each handover episode in the FACT HRC dataset. Within an episode, the human operator was observed to focus their temporal and spatial adaptation, i.e., when and where to perform certain handover actions, at key time points. Thus, instead of continuously predicting the robot's base and arm actions at every time step, which incurs high computational cost and latency, we segmented the handover episode into four action primitives. In Section~\ref{subsec:ml-primitive}, we discuss our validation of this discretisation of the handover adaptation strategies.

We trained supervised learning policies with the operator's actions as the ground-truth labels. As illustrated in Figure~\ref{episode_overview}, to achieve temporal adaptation, we trained binary classifiers to predict episode starts (i.e., when the robot starts moving its base towards the participant during action primitive 1), object transfer starts (i.e., when the robot starts stretching out its arm to initiate object transfers during action primitive 2), and handover completions (i.e., when the robot starts tucking its arm and concluding the current handover episode with the participant during action primitive 3). During training, we excluded object transfers with the experimenter (action primitive 4) and focused on learning the operator's strategies when collaborating with na\"ive participants. To achieve spatial adaptation, we trained a 3-way classifier to predict the object transfer positions (OTPs) as chosen from the default left (from the robot's perspective, i.e., to the right of the participant), middle, or right handover goal locations implemented in~\cite{tian2023crafting}. Note that the OTP adaptation was performed at both episode starts during action primitive 1 (initial estimation) and object transfer starts during action primitive 2 (updated estimation) as informed by the operator's strategies observed in~\cite{tian2023crafting}. In Section~\ref{sec:ml}, we further discuss our machine learning experiments testing different features and model structures.
We implemented the machine learning models using the Python TensorFlow library~\cite{tensorflow2015-whitepaper}. The trained classifiers were incorporated with the ROS-based teleoperation framework developed in~\cite{tian2023crafting}. Specifically, the classifier outputs determined when and which pre-recorded action primitives the robot would execute, replacing the operator's controls.

% \begin{figure}[tb]
%   \centering
%   %\includegraphics[width=0.9\linewidth]{S2_fig/QualtricsRobot_S1vS2.pdf}
%   \includegraphics[width=\linewidth]{S2_fig/HandoverStatesAutoPhoto.pdf}
%   % \vspace{-1em}
%   \caption{The autonomous handover model performs three temporal adaptations, namely episode start, object transfer start, and object transfer completions. Spatial adaptation of OTP (left, middle, right handovers) is first performed at episode start, then updated at object transfer start.}
%   % \Description{Handover type}
%   \label{episode_overview}
%   % \vspace{-1em}
% \end{figure}

% \begin{figure*}[htb]
%   \centering
%   \includegraphics[width=\textwidth]{S2_fig/HandoverStatesAuto.pdf}
%   \caption{In a handover episode, our proposed model performs three temporal adaptations, namely when waiting to move the robot to the work space (episode start), when waiting to initiate object transfer with the participant (object transfer start), and when waiting to complete the handover episode with the participant (object transfer complete). Spatial adaptation of OTP (left, middle, right handovers) is first performed at episode start, then updated when object transfer with participant is initiated.}
%   % \Description{The proposed auto handover model has three timing adaption and two spatial adaption decision points.}
%   \label{episode_overview}
% \end{figure*}

% \begin{figure*}[tb]
%     \centering
%     \subfloat[Handover episode states]{\includegraphics[width=0.8\linewidth]{S2_fig/HandoverStatesAuto.pdf}\label{fig:episode_overview_states}}\hfill
%     \subfloat[Experimental space]{\includegraphics[width=0.2\linewidth]{S2_fig/ExperimentLayoutWithBinAnno.jpg}\label{fig:episode_overview_layout}}
%      \caption{In a handover episode, our proposed model performs three temporal adaptations, namely when waiting to move the robot to the work space (episode start), when waiting to initiate object transfer with the participant (object transfer start), and when waiting to complete the handover episode with the participant (object transfer complete). Spatial adaptation of OTP (left, middle, right handovers) is first performed at episode start, then updated when object transfer with participant is initiated.}
% \label{episode_overview}
% \end{figure*}

\subsection{User study evaluation}\label{subsec:method-s2}
We replicated the HRC task in~\cite{tian2023crafting}, where the Fetch mobile manipulator robot served as a crafting assistant by transporting a list of task-relevant objects between a participant who sat at a work desk and an experimenter who sat at a storage desk (Figure~\ref{episode_overview}). The hour-long session was divided into three stages, namely Stage 1 (Preparation, where the participant receives protective equipment, such as a box of tissues and gloves), Stage 2 (Assembly, where the participant receives wooden pieces and glues them together into a birdhouse), and Stage 3 (Painting, where the participant receives brushes and paints to colour the birdhouse to their liking). The list of objects and crafting instructions were displayed on the work and storage desks. The participants were instructed to return objects they no longer need to necessitate a mix of R2H, H2R, and bidirectional handovers. 

An observer annotated the handover episodes in terms of quality (Good, Bad, Neutral) and type (H2R, R2H, bidirectional), took observation notes, as well as monitored for program execution and technical issues. ROS bags and processed csv data in the same format as the FACT HRC dataset were collected.
The participant filled in an online questionnaire, which collected the demographic information and the participant's ratings on the robot self-efficacy scale~\cite{robinson2020robot} in the pre-study section. After each of the three stages, the participants rated their subjective perception of the robot~\cite{bartneck2009measurement} and the HRC~\cite{hoffman2019evaluating}. The robot was paused when the participants filled in the questionnaire. After the whole HRC session, the participants were asked to guess whether the robot was autonomous or teleoperated. They were also interviewed to gather further understanding of their behaviours and experiences.

We conducted the experiments at the Monash Robotics lab. We recruited 20 participants (9 women, 11 men, age 26.3$\pm$4.6, 19 right-handed and 1 left-handed) from the university student and staff population via printed and digital advertisement, direct contact and snowballing. Participants who took part in our previous experiments~\cite{tian2023crafting} were excluded from this study. In addition to analysing objective and subjective performance of the handover policy in Section~\ref{sec:s2} as a within-subject study, we compared the autonomous handovers with the teleoperated handovers in Section~\ref{sec:S1vsS2} as a between-subject study.
We hypothesise that the autonomous handover policy will achieve objective and subjective performance similar to human teleoperation.
% \begin{enumerate}
%     % \item We can train an autonomous handover model that performs temporal-spatial adaptations based on a user's social cues using teleoperated handover data.
%     \item The autonomous handover policy will achieve \textbf{objective} performance similar to human teleoperation;
%     \item The autonomous handover policy will achieve \textbf{subjective} performance similar to human teleoperation.
% \end{enumerate}


\section{Machine Learning Experiments}\label{sec:ml}
We compared different features, model structures, and training approaches in cross-validation (CV) experiments to identify a suitable implementation of the autonomous handover policy. Here we report performance of the classifiers chosen for the user study.
% with detailed results of the CV experiments in the supplementary material.
% supplementary doc compiled from HRI2025_S2_sup.tex

\subsection{Performance on the FACT HRC dataset}\label{subsec:ml_picked}
To predict base and arm actions during handovers, we tested features based on the emotion predictions from both cameras and features based on the upper body poses inferred from the OAK-D camera. While combining emotions and poses yielded better performance than poses alone, this improvement was limited due to the low accuracy of the emotion inference. Further, the human operator reported using emotions mainly for evaluating handover quality rather than determining handover strategies~\cite{tian2023crafting}. Considering the increased latency of extracting both emotion and pose features, we chose the pose features, i.e., ($x$, $y$, $z$, confidence) of 25 upper body keypoints as inputs of the autonomous handover model.

As shown in Figure~\ref{episode_overview}, handover adaptation happened mainly during a subset of state transitions in the whole episode. Thus, when predicting arm and base actions in terms of timing adaptation (episode start, OTP start, OTP completion), we first extracted all the key time steps where the arm and base status changed based on the robot status data in the FACT HRC dataset. We then labelled data within a 5s window (50 steps, duration of the shortest state, i.e., stretch/tuck arm) before each episode start, object transfer start, and object transfer completion as \textit{True} while all other time steps were labelled as \textit{False}. This transformed continuous arm and base action prediction into three binary timing classifications. To balance \textit{True} and \textit{False} instances in each binary classification, we down-sampled the time steps labelled as \textit{False} randomly.
% namely deciding on pause duration at the storage space and at the work space, as well as choosing an OTP

Similarly, for predicting spatial adaptation of OTP, we used a subset of the data within 5s of episode starts and object transfer starts. As the OTP classes were unbalanced in the FACT HRC dataset (33\% left, 65\% middle, 2\% right), we over-sampled the training data by adding synthesised training data of left and right OTP. We first synthesised left OTP data by generating random values on a Gaussian constructed with the mean and standard deviation of original left OTP feature values, doubling the amount of left OTP data. We then synthesised right OTP data by mirroring the feature values of symmetrical skeletal keypoints of the left OTP (original and synthesised), e.g., interchanging the ($x$,$y$,$z$,confidence) of the left and right shoulder keypoints.

In terms of model structure, we incorporated spatial and temporal information in the feature representation and the architecture of the neural networks. We compared Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN)~\cite{schuster1997bidirectional}, Transformers~\cite{vaswani2017attention}, and Convolutional Neural Network (CNN)~\cite{lecun1989backpropagation}. Grid search was used for identifying hyperparameters, as well as the sliding window sizes for padding the input feature vector with preceding steps. We found that the CNN model consistently outperformed BLSTM-RNN and Transformers, which is likely due to spatial relationships of the skeleton keypoints and the relatively small size ($\approx$65k) of the handover adaptation subset of the FACT HRC dataset. Therefore, we chose the CNN model in our autonomous handover implementation. Our handover classifier includes an input layer, followed by six CNN-1D layers and one dense layer (an architecture inspired by the AlexNet~\cite{krizhevsky2017imagenet}, layer sizes [8,8,8,16,16,16,8] from bottom to top), and the output layer. The input feature vectors were padded with a window size of 5 (i.e., including 4 history time steps). The classifier was trained using the Adam optimiser~\cite{kingma2014adam} with a learning rate of 0.0001 and a batch size of 8. We adopted early stopping on validation loss with a patience of 20 epochs during training, with a maximum of 100 epochs.

Due to potential individual variances between participants and possible learning effects by the operator and the participants over time, we evaluated the model both with 5-fold CV segmented by participants (i.e., using all episodes from 20\% of participants in a testing fold) and 5-fold CV segmented by episodes (i.e., use 20\% episodes from each participant in a testing fold). Table~\ref{tab_auto_cv} reports average accuracy of the CNN classifiers on the FACT HRC dataset. 

As shown in Table~\ref{tab_auto_cv}, the timing classifiers (episode start, OTP start, OTP completion) had higher accuracy than the OTP type classifier. The difficulty in OTP type prediction is likely due to two reasons: Firstly, during teleoperation, the human operator was observed to use object layouts to identify a suitable OTP that both aligned with the participant's behaviours and reduced risks of collision with objects on the desk. As the autonomous policy only used human pose information, the reduced visual input may have limited its OTP prediction. 
Secondly, the human operator based their timing adaptation strategies mainly with short-term context (e.g., the participant had completed one assembly step and thus was ready for the next piece), while OTP location adaptation strategies involved further understanding of long-term context, such as individual preferences and task progress, which we further discuss in Section~\ref{subsec:S1vsS2_personal}. As the autonomous policy focused on key state transitions within each episode, the lack of global information may have limited its performance.
% As shown here, CV by episodes resulted in higher accuracy than CV by participants, suggesting the classifiers have more difficulties in accommodating individual variances between the participants compared to accommodating variances between episodes in the same session.

% \begin{table}[h]
% \begin{center}
% \caption{Average classification accuracy on the FACT HRC dataset in 5-fold CV segmented by episodes or by participants.}
% \label{tab_auto_cv}
% \begin{tabular}{|l|c|c|}
% \hline
% Classifier & Episode CV & Participants CV \\
% \hline
% Episode start & 98.8\% & 98.2\%\\
% \hline
% OTP start & 99.0\% & 98.2\%\\
% \hline
% OTP completion & 98.9\% & 98.4\%\\
% \hline
% OTP type & 78.6\% & 74.1\%\\
% \hline
% \end{tabular}
% \end{center}
% % \vspace{-1em}
% \end{table}

\begin{table}[tb]
\caption{Average classification accuracy on the FACT HRC dataset in 5-fold CV segmented by episodes or by participants.}
\label{tab_auto_cv}
\centering
\begin{tabular}{|l|c|c|}
\hline
Classifier (\%) & Episode CV & Participant CV \\
\hline
Ep start & 98.8 & 98.2 \\
\hline
OTP start & 99.0 & 98.2 \\
\hline
OTP complete & 98.9 & 98.4 \\
\hline
OTP type & 78.6 & 74.1 \\
\hline
\end{tabular}
% \vspace{-1em}
\end{table}

\subsection{Validating action primitives}\label{subsec:ml-primitive}
To verify our model of handover adaptation as fixed action primitives triggered by a set of parameterised state transitions (see Figure~\ref{episode_overview}), we investigated instances when the human operator did not follow these assumptions during teleoperated handovers in the FACT HRC dataset, i.e., when they performed timing adaptation outside of the four specified state transition points and when a non-default OTP was used. We found that the model captured 94.3\% of the operator's adaptations: in 1.2\% of episodes, the operator paused during arm reaching; in 2.3\% of episodes, the operator paused during arm tucking; in 2.8\% of episodes, the operator used non-default OTP, including stopping the end effector early. 
% Given the high percentage of handovers captured by the proposed model and the classification accuracy in Table~\ref{tab_auto_cv}, we expect the autonomous policy to achieve performance close to the human operator in the user study. 

% Our experiments on the FACT HRC dataset demonstrate that we are able to develop an autonomous handover model which uses upper body poses to perform temporal-spatial adaptations during handover, confirming our first hypothesis.

% Reported in Table~\ref{tab_auto_vio}. We only analysed the action primitives of moving to participant, arm reaching, and arm tucking, since our focus is on adaptive handovers with the participant rather than object transfers with the experimenter (the action primitive of episode completion).

% \begin{table}[h]
% \begin{center}
% \caption{Percentage of episodes when the human operator gave controls violating assumptions of the auto handover model using the designed action primitives}
% \label{tab_auto_vio}
% \begin{tabular}{|l|l|}
% \hline
% Primitive violations & Occurrence \\
% \hline
% Stutter moving to participant & 0.0\%\\
% \hline
% Stutter during arm reaching & 1.2\%\\
% \hline
% Stutter during arm tucking & 2.3\%\\
% \hline
% Non-default OTP & 2.8\%\\
% \hline
% Any type of violation & 5.7\%\\
% \hline
% \end{tabular}
% \end{center}
% % \vspace{-1em}
% \end{table}


\FloatBarrier
\section{User Study Evaluation Results}\label{sec:s2}
We analyse the handover data, questionnaires, observation notes and interviews to evaluate performance of the autonomous handover policy in the HRC crafting task.

\subsection{Handover episodes}\label{subsec:s2_csv}
We analysed the processed csv data, including the observer's in-session annotation of handover type (H2R, R2H, bidirectional) and quality (good, bad, neutral), automatic log of episode count, OTP (left, middle, right of the robot), and handover timing (episode duration, pause at the work space and the storage space). Note that data from P6, P10, P14, P16 were excluded in this analysis due to incomplete manual annotation recording or episode indexing error caused by connection drops. This results in a total of 574 handover episodes from 16 participants.
% In Section~\ref{subsec:S1vsS2_csv}, we further compare the autonomous and teleoperated handovers.

\begin{table}[tb]
\caption{Handover episodes overview (mean $\pm$ std).}
\label{tab_handover_s2}
\centering
% \begin{tabular}{|l|p{1.7cm}|p{1.7cm}|p{1.7cm}|}
\begin{tabular}{|l|c|c|c|}
\hline
OTP & Left & Middle & Right \\
\hline
(\%) & $21.67\pm8.70$ & $78.33\pm8.70$ & $0.00$ \\
\hline
Type & R2H & H2R & Bidirectional \\
\hline
(\%) & $53.65\pm16.60$ & $31.06\pm11.58$ & $15.29\pm11.89$ \\
\hline
Quality & Good & Bad & Neutral \\
\hline
(\%) & $46.33\pm18.51$ & $37.31\pm16.00$ & $16.35\pm19.61$ \\
\hline
\hline
Timing & Ep Len & Pause (work) & Pause (store) \\
\hline
(s) & $89.42\pm21.27$ & $21.14\pm9.07$ & $25.59\pm10.22$ \\
\hline
% OAK-D Emotion & Majority class & Arousal & Valence \\
% \hline
% Good episodes & Neutral & $0.15\pm0.09$ & $-0.26\pm0.14$ \\
% \hline
% Bad episodes & Neutral & $0.16\pm0.08$ & $-0.26\pm0.14$ \\
% \hline
% Fetch Emotion & Majority class & Arousal & Valence \\
% \hline
% Good episodes & Neutral & $0.20\pm0.05$ & $-0.24\pm0.08$ \\
% \hline
% Bad episodes & Neutral & $0.21\pm0.05$ & $-0.21\pm0.08$ \\
% \hline
\end{tabular}
% \vspace{-1em}
\end{table}

As shown in Table~\ref{tab_handover_s2}, over a third of handover episodes were rated as having poor quality by the observer (i.e., bad handovers). While the predictions are binary or 3-way classifications, the diversity and subtlety in how people signalled their intent during crafting made the task challenging. We reviewed the observation notes and identified two types of errors that caused a bad episode. One type is \textit{motion primitive error} due to focusing the adaptation at key state transitions (see Figure~\ref{episode_overview}). Specifically, once object transfer completion is predicted and the robot begins to execute action primitive 4, the autonomous model stops monitoring the participant's behaviours. Thus, when the robot is tucking its arm, it is unable to wait or react if a participant signals to the robot at this time. The other type is \textit{prediction error} due to imperfect recognition of participants' true intentions. This can be further divided into incorrect prediction in handover \textit{timing}, such as interrupting a participant's crafting activities by starting an episode early, and incorrect prediction in handover \textit{location}, such as colliding with a birdhouse kept in the middle of the work desk by performing a middle handover. 
Motion primitive error was the most common type in Stage 1 with the observer reporting 11 out of 20 participants who had at least one noticeable occurrence, while timing prediction error was the most common in Stages 2 (12 out of 20) and 3 (15 out of 20). The observation notes revealed that motion primitive error was mainly caused by participants attempting to perform bidirectional handovers instead of R2H or H2R handovers. Timing prediction error was mainly caused by the robot misrecognising a participant's assembly or painting gestures as episode or object transfer initiations, thus moving towards the participant or stretching out its arm too early, interrupting the participant.

% In Table~\ref{tab_auto_error}, we summarise the number of participants who have experienced at least one bad handover episode caused by each type of error based on the observation notes from each crafting stage (see Section~\ref{subsec:method-s2}). Interestingly, prediction errors in handover timing are the most common cause of noticeable bad handovers session-wise, in contrast to the handover timing classifiers having high accuracy on the FACT HRC dataset in Table~\ref{tab_auto_cv}. As shown here, in Stage 1 (Preparation), motion primitive error is the main cause, impacting 11 out of 20 participants. The observation notes revealed that this is due to participants attempting to perform bidirectional handovers instead of R2H or H2R handovers. For example, when receiving a tissue box from the robot, a participant may attempt to return the box to the robot after taking out tissues they needed during the same episode. However, the robot had predicted OTP completion while the participant was taking the tissues out of the box and thus started executing action primitive 4, resulting in the episode being R2H instead of bidirectional which the participant intended. In Stages 2 and 3, timing prediction error was the most common type. The observation notes showed that this was mainly caused by the robot misrecognising a participant's assembly or painting gestures as episode or object transfer initiations, thus moving towards the participant or stretching out its arm too early, interrupting the participant.

% \begin{table}[h]
% \caption{Number of participants who experienced each type of error in the autonomous handover sessions at least once (20 participants in total, the same participant may experience multiple types of error in a session).}
% \label{tab_auto_error}
% \centering
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% Error Type & S1 & S2 & S3 & Session \\
% \hline
% Motion Primitive & 11 & 3 & 12 & 16\\
% \hline
% Timing prediction & 5 & 12 & 15 & 19\\
% \hline
% Location prediction & 0 & 6 & 6 & 10\\
% \hline
% \end{tabular}
% % \vspace{-1em}
% \end{table}

% Summary of observation notes:
% \begin{itemize}
%     \item Stage 1: bad handovers due to fixed primitive and unable to pause or call the robot back once arm tucking started (P1, P5, P8, P10, P11, P13, P14, P15, P16, P17, P19), timing recognition errors (P1, P3, P4, P6, P7)
%     \item Stage 2: bad handovers due to fixed primitive and unable to pause or call the robot back once arm tucking started (P11, P16, P19), timing recognition errors (P2, P3, P4, P5, P6, P7, P8, P11, P12, P15, P16, P19), collisions (P1, P4, P5, P14, P15, P19)
%     \item Stage 3: bad handovers due to fixed primitive and unable to pause or call the robot back once arm tucking started (P2, P5, P6, P7, P8, P11, P12, P13, P15, P16, P19, P20), timing recognition errors (P1, P3, P4, P7, P8, P9, P10, P11, P12, P14, P15, P16, P17, P18, P20), collisions (P2, P4, P12, P14, P17, P20)
%     \item Union of all stages: fixed primitive (P1, P2, P5, P6, P7, P8, P10, P11, P12, P13, P14, P15, P16, P17, P19, P20), timing (P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P14, P15, P16, P17, P18, P19, P20), collision (P1, P2, P4, P5, P12, P14, P15, P17, P19, P20)
%     \item Other notes: reading instructions while waiting (P1, P2, P9, P17, P18, P20), gaze cues (P1, P2, P3, P9, P10, P13, P17), hand gestures (P1, P3, P6, P9, P11, P15, P16, P19), all participants have mixed emotion estimation with lots of negative categories recognised and some neural
% \end{itemize}

\subsection{Participant behaviour and perception}\label{subsec:s2_quali}
Participants showed diverse behaviours when communicating their intention to the robot with a combination of gaze and hand gestures, similar to observations in the teleoperated sessions~\cite{tian2023crafting}. 
% In the interviews, eight participants correctly hypothesised that the robot used hand locations and body gestures to interpret human intention and five identified that the robot recognised gaze. In addition, three participants thought the robot recognised objects inside the basket or on the work desk, while the handover model did not directly incorporate this object-related visual information. 
% Regarding the crafting task context, seven participants mentioned in the interview that the order in which they received the paint colours influenced their design of the birdhouse while the remaining 13 participants reported otherwise. Further, four participants considered the usage context, such as gifting or bird feeding, as inspirations to their design.
Recall that participants were not told if the robot was fully autonomous or being teleoperated at the start of the experiment and were asked to guess this post-study. Three participants guessed the robot was teleoperated, four were unsure, and the remaining 13 guessed the robot was autonomous. In the interview, participants elaborated that the reasons for guessing the robot was teleoperated included the robot being responsive to gaze and demonstrating good collaboration that was considered to be on par with a human; The reason for being unsure was due to a mix of episodes of good collaboration and robot errors; The reasons for guessing the robot was autonomous were handover errors which they expected a human would be able to avoid. 
% Further, in the post-study questionnaire, 9 participants guessed the robot was adaptive during the experiment, 5 were unsure, and 6 guessed the robot was not adaptive. 

A one-way ANOVA test on the questionnaire responses found no significant influence of the crafting stage on participants' subjective perceptions towards the robot and the HRC.
When asked to elaborate on their impression towards the robot and the interaction in the interview, participants reported mixed responses. 
In terms of positive impressions, 11 participants reported the robot was adaptive or collaborative, with specific comments on the robot being responsive, having a good pace, being patient, or anticipating their needs by changing handover timing or location; nine participants considered the robot useful and that it performed well as a crafting assistant; seven reported the robot was natural, human-like, social, or intelligent; six reported the robot increased their satisfaction and made the crafting activity more fun or enjoyable; two commented that the robot saved physical effort for people and may help those with mobility difficulties.
% similar to~\cite{tian2023crafting}
% seven reported the robot was natural, human-like, social, or intelligent with six of them referred to the robot's pre-defined social gaze behaviour of looking into the basket during object transfers

As for negative impressions, 10 participants reported handover timing errors, including six participants reporting feeling rushed or the robot being impatient at times, and two commenting that the robot interrupted or surprised them; three reported handover location errors, including the left-handed participant; three commented the robot was inconsistent or difficult to predict; three considered having the robot as an assistant to be less efficient than crafting by themselves or the robot was being too slow; two commented that while they noticed the robot's adaptation it was not to a satisfactory performance. Compared to handover errors reported by the observer (Section~\ref{subsec:s2_csv}), fewer participants discussed robot errors in the interview. This is consistent with~\cite{tian2023crafting}, where the participants' focus on the overall crafting experience, as opposed to the operator's focus on the individual handover episodes, led to differences in their perception of handover quality.

% Regarding suggestions for improvement, five participants reported the desire to customise the order of receiving or returning objects and three wanted additional speech-based communication, which were also reported in the teleoperated sessions. However, different from the teleoperated sessions, six participants expressed the need for more transparency in the robot's decision-making, with five of them wanting the robot to communicate its recognised human intention and planned action before executing an action, and one suggesting an additional function that allows a user to give explicit feedback to the robot and for the robot to learn from such feedback. This desire for more transparent robot decision-making may be the result of the autonomous model exhibiting more bad handovers and adaptation errors compared to the human operator, as we further discuss in Section~\ref{sec:S1vsS2}.

% Summary of interview answers:
% \begin{itemize}
%     \item Teleop or Auto
%     \begin{itemize}
%         \item questionnaire: 3 guessed teleop, 13 guessed auto, 4 unsure
%         \item teleop and unsure: responsive (P9), good collaboration (P17), responding to gaze (P18), P1 unsure due to arm tucking timing, P11 and P19 unsure as there were mixed good collaboration and misunderstanding, P16 unsure as some motion triggers while others don't
%         \item auto: 8 participants explained the reason to be they noticed handover errors and expected a human would be able to avoid such errors (P2, P3, P5, P6, P7, P8, P10, P20)
%     \end{itemize}
%     \item Robot and HRC impression
%     \begin{itemize}
%         \item questionnaire: 9 consider the robot adaptive, 6 not adaptive, 5 unsure
%         \item Positive: useful / helpful / gets the job done well (P1, P2, P3, P6, P7, P8, P9, P10, P13), cute and natural social cue of looking down during handover (P6, P8, P12, P16, P17, P20), responsive and anticipating the person's needs by changing the location and timing of handover (P9, P10, P13, P14, P17), make the activity more interesting / cool / fun (P5, P10, P12, P14), collaborative and adaptive (P2, P3, P7, P8, P9, P11, P16), enjoyable and satisfying (P6, P13), saves physical efforts for human (P3), good pace (P17), may help people with mobility issue (P10), patient (P17), human-like (P20), smart (P14)
%         \item Negative: inconsistent and hard to predict (P1, P15, P19), less efficient than the person doing it themselves (P3, P4), not working well for left-handed user (P4), rushing the person or being impatient at times (P5, P11, P13, P14, P16, P19), distracting or surprising if handover interrupts activity (P6, P12), errors in timing adaption (P1, P2, P11), errors in location adaption (P4, P20), adapting but not enough (P5, P10), slow and not proactive (P18)
%     \end{itemize}
%     \item Crafting designs: consideration for usage scenario like gift receiver / birds and nature (P1, P3, P7, P15), paint order influenced design (P3, P5, P10, P12, P14, P18, P20), paint order didn't influence design (P1, P2, P4, P6, P7, P8, P9, P11, P13, P15, P16, P17, P19)
%     \item Mental model of robot functions: decision based on basket content (P1, P19), recognising gaze (P6, P10, P11, P17, P18), recognising gestures and hand movement / location (P7, P8, P9, P12, P13, P14, P18, P20), location of objects on the table (P14)
%     \item Suggestion for improvements: ability to explicitly communicate to the robot via speech or button presses (P2, P7, P19), ability to customise activities like object order or getting multiple objects at once (P2, P5, P10, P12, P15), communicate to the person what the robot's plan was before execution and if it understood the person's intention correctly (P5, P9, P12, P15, P19), learning from mistakes or the person's explicit feedback (P7), understanding how the robot functions before the study (P15)
% \end{itemize}

% Summary of free text comments in questionnaire:
% \begin{itemize}
%     \item After Stage 1: 
%     \begin{itemize}
%         \item Positives: P3 gaze behaviour of looking at person and hand is human-like, P5 satisfied, P7 robot become easy to predict later
%         \item Negatives: P2,P3,P5,P19 inappropriate timing (tucking too soon, feeling rushed, unable to perform a bidirectional object exchange), P15 inappropriate location (noticed one), P7,P8,P10 unpredictable at the start (unfamiliar with robot's moving pattern and inconsistent timing)
%     \end{itemize}
%     \item After Stage 2: 
%     \begin{itemize}
%         \item Positives: P3 satisfied, P7,P19 let the person focus on work as the robot patiently waits and stays away, P8 improved robot intention understanding compared to Stage 1, P8 smooth session
%         \item Negatives: P2,P3,P5,P8,P10,P13,P19 inappropriate or incorrect timing (empty glue return runs, a little annoyed, looks unintelligent, rushed, arm tucking too soon), P2 human initiated handover as an alternative, P4 incorrect location (one collision with glue bottle), P14,P19 unclear if the person's gestures are being interpreted
%     \end{itemize}
%     \item After Stage 3:
%     \begin{itemize}
%         \item Positives: P2 all good, P3 the robot waited to let the person focus, P10 heap of fun, P15 pace enjoyable when the person is busier
%         \item Negatives: P4,P7,P10,P14,P16,P19 inappropriate or incorrect timing (slow sometimes, empty runs not smart and unsure why, tucking too early), P4 incorrect location (collisions, not interpreting the person being left-handed), P19 not assisting the person but doing its own thing (too slow at the start when unsure but too rushed later during painting)
%     \end{itemize}
%     \item Additional comments:
%     \begin{itemize}
%         \item Positives: P7 adapting as it waits for the person to finish, P8 adapting handover location, P10 robot reacting to gaze to start episode feel very interactive
%         \item Negatives: P5,P7,P10,P12,P14 inappropriate timing (not adaptive, avoid empty runs with human feedback, rushed tucking, distracting, impatient)
%         \item Future improvements: P12 missing verbal interaction, P19 missing confirmation from robot to person on what it's doing and what it wants
%     \end{itemize}
% \end{itemize}

% \subsection{Subjective perceptions of the robot and HRC}\label{subsec:s2_qualtrics}
% % We visualise participants' subjective perceptions towards the robot and the HRC in Figures~\ref{fig:s2_Q_robot}~\&~\ref{fig:s2_Q_HRC}. 
% A one-way ANOVA test found no significant influence of the crafting stage on participants' subjective perceptions towards the robot and the HRC. This is different from the teleoperated sessions where a significant increase was found from Stage 1 to 3 in robot and HRC perceptions. In Section~\ref{subsec:S1vsS2_qualtrics}, we further compare participants' perceptions in the autonomous and teleoperated sessions.
% In addition, in the open-ended questions of the questionnaire, participants reported mixed impressions aligned with their interview responses as discussed in Section~\ref{subsec:s2_quali}.

% \begin{figure}[tb]
%   \centering
%     \pdftooltip{\includesvg[width=0.8\linewidth]{S2_fig/QualtricsRobot_S2_bar.svg}}{In this bar plot, the participants' impression towards the robot showed a small increasing trend over the 3 stages. Likeability and perceived safety ratings are higher in general, anthropomorphism ratings are lower in general.}
%   % \vspace{-1em}
%   \caption{No difference in robot ratings between stages.}
%   \label{fig:s2_Q_robot}
%   % \vspace{-1em}
% \end{figure}

% \begin{figure}[tb]
%   \centering
%     \pdftooltip{\includesvg[width=0.8\linewidth]{S2_fig/QualtricsHRC_S2_bar.svg}}{In this bar plot, the participants' impression towards the HRC showed a small increasing trend over the 3 stages. Enjoyment ratings are slightly higher than other dimensions.}
%   % \vspace{-1em}
%   \caption{No difference in HRC ratings between stages.}
%   \label{fig:s2_Q_HRC}
%   % \vspace{-1em}
% \end{figure}

% \begin{table}[h]
% \begin{center}
% \caption{Participants' impressions of the robot and the HRC (mean $\pm$ std) with autonomous handover model. No significant difference was found between the stages.}
% \label{tab_qualtrics_s2}
% \begin{tabular}{|l|c|c|c|}
% \hline
% Robot impression & Stage 1 & Stage 2 & Stage 3 \\
% \hline
% Anthropomorphism & $2.86\pm0.77$ & $2.99\pm0.96$ & $3.01\pm0.93$ \\
% \hline
% Animacy & $3.04\pm0.72$ & $3.18\pm0.86$ & $3.19\pm0.88$ \\
% \hline
% Likeability & $3.70\pm0.61$ & $3.83\pm0.73$ & $3.86\pm0.71$ \\
% \hline
% Intelligence & $3.33\pm0.50$ & $3.31\pm0.72$ & $3.36\pm0.69$ \\
% \hline
% Perceived safety & $3.90\pm0.45$ & $3.87\pm0.69$ & $3.92\pm0.52$ \\
% \hline
% \hline
% HRC impression & Stage 1 & Stage 2 & Stage 3 \\
% \hline
% Fluency & $3.45\pm0.93$ & $3.52\pm0.96$ & $3.62\pm0.76$ \\
% \hline
% Trust & $3.40\pm0.74$ & $3.65\pm0.96$ & $3.58\pm0.82$ \\
% \hline
% Working Alliance & $3.60\pm0.91$ & $3.83\pm0.86$ & $3.83\pm0.83$ \\
% \hline
% Enjoyable & $3.85\pm0.75$ & $4.05\pm0.76$ & $4.20\pm0.83$ \\
% \hline
% Satisfactory & $3.70\pm0.86$ & $4.05\pm0.76$ & $4.15\pm0.75$ \\
% \hline
% \end{tabular}
% \end{center}
% % \vspace{-1em}
% \end{table}



\FloatBarrier
\section{Teleoperated vs. Autonomous Handover}\label{sec:S1vsS2}
% Here we compare the autonomous handovers with previous work where the robot was teleoperated~\cite{tian2023crafting}.
\subsection{Handover episodes}\label{subsec:S1vsS2_csv}
We compared the handover episodes when the robot was teleoperated~\cite{tian2023crafting} and when the robot was autonomous. Note that the teleoperated csv data included all 20 participants, while handover csv data from P6, P10, P14, P16 in the autonomous sessions were removed (see Section~\ref{subsec:s2_csv}). We visualise the distribution of percentages of OTP location, handover type and quality, as well as handover timings when the robot was autonomous~vs.~teleoperated in Figure~\ref{fig:S1vS2_ep}.

When comparing the number of episodes, OTP location, and handover type and quality, we used episode count data and performed Bayesian AB tests with Poisson distribution to compare the two conditions. 
There are more handover episodes per participant in the autonomous sessions compared to the teleoperated sessions (probability of P(Auto$>$Tele)=99.95\%, event rate Credible Interval for interval length 0.9 is [0.10, 0.33]). Note that the teleoperated and autonomous sessions followed the same procedure with the same set of crafting objects being used. However, the human operator was more efficient with their handovers: the autonomous sessions had more single directional R2H (P(Auto$>$Tele)=98.43\%, 90\% CI [0.04, 0.33]) and H2R (P(Auto$>$Tele)=100\%, 90\% CI [1.43, 2.73]) handovers, while the teleoperated sessions had more bidirectional handovers (P(Tele$>$Audo)=100\%, 90\% CI [0.36, 0.99]). 

In terms of OTP, more autonomous episodes adopted middle OTP (P(Auto$>$Tele)=100\%, 90\% CI [0.29, 0.61]), while more teleoperated episodes adopted left (P(Tele$>$Audo)=93.37\%, 90\% CI [-0.02, 0.39]) or right (P(Tele$>$Audo)=75.54\%, 90\% CI [-0.20, 0.76]) OTP. 

In terms of handover quality, the observer rated more teleoperated handovers as good (P(Tele$>$Auto)=99.29\%, 90\% CI [0.06, 0.36]) or neutral (P(Tele$>$Auto)=85.49\%, 90\% CI [-0.07, 0.38]), while more autonomous handovers were rated as bad (P(Auto$>$Tele)=100\%, 90\% CI [2.64, 4.83]).

A one-way ANOVA showed that the handover model (teleoperated or autonomous) had a significant influence on average episode duration ($F=4.84$, $p=0.03$, medium effect with $\eta^{2}=0.12$). The teleoperated episodes were longer on average, with shorter pauses at the workspace ($F=6.47$, $p=0.02$, large effect with $\eta^{2}=0.16$) and longer pauses at the storage space ($F=6.83$, $p=0.01$, large effect with $\eta^{2}=0.17$).

% When comparing the number of episodes, OTP location, and handover type and quality, we used episode count data and performed Poisson regression to compare the two conditions. There are significantly fewer handover episodes per participant in the teleoperated sessions compared to the autonomous sessions ($p \ll 0.001$, coefficient $\beta = -0.24$, medium effect). Note that the teleoperated and autonomous sessions followed the same procedure with the same set of crafting objects being used. However, the human operator was more efficient with their handovers, indicated by the teleoperated sessions having significantly fewer single direction handovers (H2R $p \ll 0.001$, $\beta = -1.44$, large effect; R2H $p = 0.01$, $\beta = -0.21$, medium effect) and significantly more bidirectional handovers ($p \ll 0.001$, $\beta = 0.61$, large effect). 
% There are significantly fewer teleoperated episodes adopting the middle OTP ($p \ll 0.001$, $\beta = -0.42$, large effect). Further, there are significantly more teleoperated handovers rated as good quality ($p=0.03$, $\beta = 0.17$, medium effect) and significantly fewer teleoperated episodes rated as bad ($p \ll 0.001$, $\beta = -2.09$, large effect) by the observer. 
% % In Section~\ref{subsec:s2_quali}, we gave a breakdown of autonomous handover errors. 
% % This partially rejects our second hypothesis, as there is a gap in the objective performance of the autonomous handover model compared to human teleoperation.
% For handover timing, we performed a one-way ANOVA, which showed that the handover model (teleoperated or autonomous) had a significant influence on average episode duration ($F=4.84$, $p=0.03$, medium effect with $\eta^{2}=0.12$). The teleoperated episodes were longer on average, with shorter pauses at the workspace ($F=6.47$, $p=0.02$, large effect with $\eta^{2}=0.16$) and longer pauses at the storage space ($F=6.83$, $p=0.01$, large effect with $\eta^{2}=0.17$).
% % $\eta^{2}$ size thresholds: small=0.01, medium=0.06, large=0.14

% Comparison of handover episodes showed a significant difference in teleoperated and autonomous sessions, thus rejecting our first hypothesis that the autonomous policy will achieve similar objective performance as teleoperation.

\begin{figure}[tb]
    \centering
    \subfloat[][OTP (\%)]{\pdftooltip{\includegraphics[width=0.5\linewidth]{S2_fig/HandoverLocation_ALLS1vS2_box.pdf}}{In this box plot, teleoperated sessions have bigger spread and higher mean in choosing left OTP, autonomous sessions have smaller spread and higher mean in choosing middle OTP, both sessions have very few right OTPs.}\label{fig:S1vS2_location}}
    \subfloat[][Type (\%)]{\pdftooltip{\includegraphics[width=0.5\linewidth]{S2_fig/HandoverType_ALLS1vS2_box.pdf}}{In this box plot, teleoperated sessions have lower mean in choosing R2H, autonomous sessions have higher mean in choosing H2R, teleoperated sessions have higher mean and wider spread in choosing bidirectional handovers.}\label{fig:S1vS2_type}}\\
    \subfloat[][Quality (\%)]{\pdftooltip{\includegraphics[width=0.5\linewidth]{S2_fig/HandoverQuality_ALLS1vS2_box.pdf}}{In this box plot, teleoperated sessions have higher mean in good handovers, autonomous sessions have higher mean in bad handovers, autonomous sessions have lower mean and wider spread in neutral handovers.}\label{fig:S1vS2_quality}}
    \subfloat[][Timing (s)]{\pdftooltip{\includegraphics[width=0.5\linewidth]{S2_fig/HandoverTime_ALLS1vS2_box.pdf}}{In this box plot, teleoperated sessions have higher mean in episode duration and pause at storage space, autonomous sessions have higher mean in pause at work space.}\label{fig:S1vS2_time}}
    \caption{Comparison of auto~vs.~teleoperated handovers in terms of OTP (a), Type (b), Quality (c) and Timing (d).}
\label{fig:S1vS2_ep}
\end{figure}

% use \includegraphics to call pdf version of the figures allows adding alt text, but this gives error "TeX capacity exceeded, sorry [PDF object stream buffer=5000000]." (unless both svg version and pdf version are called, which is really strange). \includesvg doesn't seem to allow adding alt text easily :(
% Seems to be due to PDF 1.7:
% https://github.com/jgraph/drawio/issues/4527
% Fix by saving in InkScape as version 1.5 (also reduces size!)
% Anyway the alt={...} is not suitable since it does not render.
% Article: https://www.ams.org/journals/notices/202301/rnoti-p68.pdf suggests \pdftooltip:
% NO SPACE / LINE BREAK INSIDE \pdftooltip!
% \begin{figure}[tb]
%   \centering
%     \subfloat[OTP location (\%)]{\pdftooltip{\includesvg[width=0.5\linewidth]{S2_fig/HandoverLocation_S1vS2_box.svg}}{Teleoperated sessions have bigger spread and higher mean in choosing left OTP, autonomous sessions have smaller spread and higher mean in choosing middle OTP, both sessions have very few right OTPs.}\label{fig:S1vS2_location}}
%     \subfloat[Type (\%)]{\includegraphics[alt={Teleoperated sessions have lower mean in choosing R2H, autonomous sessions have higher mean in choosing H2R, teleoperated sessions have higher mean and wider spread in choosing bidirectional handovers.},width=0.5\linewidth]{S2_fig/HandoverType_S1vS2_box.pdf}\label{fig:S1vS2_type}}\\
%     \subfloat[Quality (\%)]{\includegraphics[alt={Teleoperated sessions have higher mean in good handovers, autonomous sessios have higher mean in bad handovers, autonomous sessions have lower mean and wider spread in neutral handovers.},width=0.5\linewidth]{S2_fig/HandoverQuality_S1vS2_box.pdf}\label{fig:S1vS2_quality}}
%     \subfloat[Timing (s)]{\includegraphics[alt={Teleoperated sessions have higher mean in episode duration and pause at storage space, autonomous sessions have higher mean in pause at work space.},width=0.5\linewidth]{S2_fig/HandoverTime_S1vS2_box.pdf}\label{fig:S1vS2_time}}
%   \caption{Autonomous~vs.~teleoperated handovers.}
%   \label{fig:S1vS2_ep}
% \end{figure}

% \begin{figure}[tb]
% % \includegraphics{./S2_fig/HandoverQuality_S1vS2_box_v2.pdf}
% \subfloat[OTP location (\%)]{\includegraphics[alt={blabla},width=0.5\linewidth]{S2_fig/HandoverQuality_S1vS2_box_v2.pdf}\label{fig:S1vS2_location}}
% \caption{Autonomous~vs.~teleoperated handovers.}
% \label{fig:S1vS2_ep}
% \end{figure}


% \begin{figure}[tb]
%   \centering
%   \subfloat[aa]{\includegraphics[alt={tba},width=0.4\linewidth]{S2_fig/QualtricsRobot_S1vS2.pdf}}
%   \subfloat[bb]{\includegraphics[alt={tba},width=0.4\linewidth]{S2_fig/QualtricsRobot_S1vS2.pdf}}
%   %\includesvg[width=0.7\linewidth]{S2_fig/HandoverQuality_S1vS2_box.svg}
%   % \vspace{-1em}
%   \caption{Handover quality (\%)}
%   % \Description{Handover quality}
%   \label{fig:S1vS2_quality}
%   % \vspace{-1em}
% \end{figure}

% \begin{figure}[tb]
%   \centering
%   %\includegraphics[width=0.9\linewidth]{S2_fig/QualtricsRobot_S1vS2.pdf}
%   \includesvg[width=0.7\linewidth]{S2_fig/HandoverTime_S1vS2_box.svg}
%   % \vspace{-1em}
%   \caption{Handover timing (s)}
%   % \Description{Handover timing}
%   \label{fig:S1vS2_time}
%   % \vspace{-1em}
% \end{figure}

\subsection{Subjective perception}\label{subsec:S1vsS2_qualtrics}
% Participants' subjective perceptions when the robot was teleoperated vs. autonomous are visualised in Figures~\ref{fig:S1vS2_Q_robot}~\&~\ref{fig:S1vS2_Q_HRC}. 
We report participants' questionnaire responses in the teleoperated and autonomous sessions in Table~\ref{tab_qualtrics_S1vS2}. Participants reported similar impressions of the robot, except for perceived intelligence where a Wilcoxon signed rank test showed significantly higher ratings for the teleoperated session ($V = 1053$, $p=0.003$). Regarding HRC impressions, the teleoperated sessions have significantly higher ratings in fluency ($V = 1122.5$, $p=0.019$), trust ($V = 990.5$, $p=0.002$), and working alliance ($V = 1044.5$, $p=0.021$). 
% Participants reported similar robot impression comparing the teleoperated vs. autonomous HRC sessions, except for perceived intelligence where a two-way ANOVA showed significantly higher ratings for the teleoperated session ($F=8.64$, $p=0.004$, medium effect with $\eta^{2}=0.07$). Regarding HRC impression, the teleoperated session were found to have significantly higher ratings in human-robot trust ($F=13.09$, $p\ll0.01$, medium effect with $\eta^{2}=0.10$) and working alliance ($F=3.98$, $p=0.049$, small effect with $\eta^{2}=0.03$). 
% In addition, we found a significant influence of crafting stages on enjoyment ($F=4.88$, $p=0.01$, medium effect with $\eta^{2}=0.08$) and satisfaction ($F=3.76$, $p=0.03$, medium effect with $\eta^{2}=0.06$). Thus, our second hypothesis is rejected as significant differences were found in some robot and HRC perceptions in autonomous vs. teleoperated sessions.
% The autonomous handover policy achieved subjective performance close to teleoperation, partially supporting our second hypothesis.
% There is no significant difference in the participants' self-rated robot operation and application efficacy in the pre-study questionnaires for the teleoperated and auto sessions.

% An one-way ANOVA test showed significant influence of handover model (teleoperated vs. autonomous) on perceived intelligence of the robot ($F=8.92$, $p=0.003$, medium effect with $\eta^{2}=0.07$), human-robot trust ($F=13.29$, $p\ll0.01$, medium effect with $\eta^{2}=0.10$), and working alliance ($F=4.07$, $p=0.046$, small effect with $\eta^{2}=0.03$).
%$\eta^{2} \geq 0.01$ indicates a small effect. $\eta^{2} \geq 0.06$ indicates a medium effect. $\eta^{2} \geq 0.14$ indicates a large effect.

% \begin{figure}[tb]
%   \centering
%   %\includegraphics[width=0.9\linewidth]{S2_fig/QualtricsRobot_S1vS2.pdf}
%   \pdftooltip{\includesvg[width=\linewidth]{S2_fig/QualtricsRobot_S1vS2_bar_anno.svg}}{Comparison between user perceptions of the autonomous and teleoperated robot on the Godspeed questionnaire. The participants' impression of the robot is similar between the autonomous and teleoperated conditions, however there is a significant difference in perceived intelligence.}
%   % \vspace{-1em}
%   \caption{Comparing user perceptions of the autonomous and teleoperated robot on the Godspeed questionnaire~\cite{bartneck2009measurement}.}
%   \label{fig:S1vS2_Q_robot}
%   % \vspace{-1em}
% \end{figure}

% \begin{figure}[tb]
%   \centering
%   %\includegraphics[width=0.9\linewidth]{S2_fig/QualtricsHRC_S1vS2.pdf}
%   \pdftooltip{\includesvg[width=\linewidth]{S2_fig/QualtricsHRC_S1vS2_bar_anno.svg}}{Comparison between user perceptions of the HRC when interaction with either the autonomous or teleoperated robot in terms of fluency, trust, alliance, enjoyment and satisfaction. The participants' impression of the HRC is similar on the fluency, enjoyment and satisfaction scale, but is significantly more positive when the robot was teleoperated in trust, and working alliance.}
%   % \vspace{-1em}
%   \caption{Comparing user perceptions of the HRC when interacting with either the autonomous or teleoperated robot in terms of fluency, trust, alliance, enjoyment and satisfaction.}
%   \label{fig:S1vS2_Q_HRC}
%   % \vspace{-1em}
% \end{figure}

% \begin{table}[h]
% \begin{center}
% \caption{Participants' impressions of the robot and the HRC (mean $\pm$ std) in teleoperated vs. autonomous sessions. Here we report the average ratings over the three stages. Participants reported more positive perceptions in the teleoperated sessions, except for perceived safety and satisfaction.}
% \label{tab_qualtrics_S1vS2}
% \begin{tabular}{|l|c|c|}
% \hline
% Robot impression & Teleop & Auto \\
% \hline
% Anthropomorphism & $2.99\pm0.84$ & $2.95\pm0.84$ \\
% \hline
% Animacy & $3.14\pm0.71$ & $3.14\pm0.77$ \\
% \hline
% Likeability & $4.00\pm0.65$ & $3.80\pm0.61$ \\
% \hline
% Intelligence & $3.70\pm0.69$ & $3.33\pm0.53$ \\
% \hline
% Perceived safety & $3.87\pm0.69$ & $3.89\pm0.49$ \\
% \hline
% \hline
% HRC impression & Teleop & Auto \\
% \hline
% Fluency & $3.89\pm0.60$ & $3.53\pm0.67$ \\
% \hline
% Trust & $4.10\pm0.70$ & $3.54\pm0.70$ \\
% \hline
% Working Alliance & $4.07\pm0.73$ & $3.76\pm0.65$ \\
% \hline
% Enjoyable & $4.12\pm0.58$ & $4.03\pm0.67$ \\
% \hline
% Satisfactory & $3.95\pm0.69$ & $3.98\pm0.62$ \\
% \hline
% \end{tabular}
% \end{center}
% % \vspace{-1em}
% \end{table}

\begin{table}[h]
\begin{center}
\caption{Participants' impressions of the robot and the HRC (mean $\pm$ std) in teleoperated vs. autonomous sessions.}
\label{tab_qualtrics_S1vS2}
\begin{tabular}{|l|c|c|}
\hline
Robot impression & Teleoperated & Autonomous \\
\hline
Anthropomorphism & $2.99\pm0.84$ & $2.95\pm0.84$ \\
\hline
Animacy & $3.14\pm0.71$ & $3.14\pm0.77$ \\
\hline
Likeability & $4.00\pm0.65$ & $3.80\pm0.61$ \\
\hline
Intelligence** & $3.70\pm0.69$ & $3.33\pm0.53$ \\
\hline
Perceived safety & $3.87\pm0.69$ & $3.89\pm0.49$ \\
\hline
\hline
HRC impression & Teleoperated & Autonomous \\
\hline
Fluency* & $3.89\pm0.60$ & $3.53\pm0.67$ \\
\hline
Trust** & $4.10\pm0.70$ & $3.54\pm0.70$ \\
\hline
Working Alliance* & $4.07\pm0.73$ & $3.76\pm0.65$ \\
\hline
Enjoyable & $4.12\pm0.58$ & $4.03\pm0.67$ \\
\hline
Satisfactory & $3.95\pm0.69$ & $3.98\pm0.62$ \\
\hline
\end{tabular}
\end{center}
% \vspace{-1em}
\end{table}

Further, when analysing subjective ratings for each of the three crafting stages, an increasing trend from Stage 1 to Stage 3 was found when the robot employed the autonomous handover policy, albeit not significant. When the robot was teleoperated, a significant increase in subjective perception from Stage 1 to Stage 3 was found~\cite{tian2023crafting}. We hypothesise that the significant increase in subjective perception when the robot was teleoperated was caused by both increased familiarity with the robot and the crafting task, as well as the human operator's adaptation of the robot's handovers to an individual participant's working styles and preferences over time. As the autonomous robot used the same handover policy throughout the session without personalising to each individual participant, while participants had increased familiarity with the robot and the task, the lack of personalisation limited how much their perception would increase over time. We further analyse personalisation by the human operator in Section~\ref{subsec:S1vsS2_personal}.

\subsection{Personalisation in teleoperated sessions}\label{subsec:S1vsS2_personal}
We analysed the timing and location of teleoperated handovers across all 20 participants aligned by the episode ID. As shown in Fig~\ref{fig:personal_loc}, in the Preparation stage the operator chose middle OTP for most participants, while a mix of OTPs were used later. This suggests that the operator chose middle OTP as a general strategy at the start of each session, and then personalised the OTPs as the session progressed. Further, we examined the accuracy of the autonomous model for predicting OTPs on the FACT HRC dataset using CV by episodes (see Section~\ref{subsec:ml_picked}). As shown in Table~\ref{tab_otp_ep_per_fold}, the model yielded the highest accuracy when predicting the first 20\% of episodes in each participant (trained on the remaining 80\% of episodes) with a declining accuracy in predicting OTPs for handovers that occurred later. The increased prediction errors in later episodes indicates that the autonomous policy diverged from the human operator's behaviours more in later episodes. This supports our assumption that the operator began with a generic handover strategy but personalised it as the collaboration progressed.

% \begin{figure}[tb]
%   \centering
%   \pdftooltip{\includegraphics[width=0.8\linewidth]{S2_fig/S1_loc_ep_anno.pdf}}{The x-axis is episode number, the y-axis is OTP location choice (\%). Episode 0 to 5 is approximately preparation stage, 5 to 15 is assemble stage, 15 to 25 is painting stage, with overlaps between the stages. The percentage of choice of middle handover is near 100\% at the start, but drop to around 50\% close to episode 10. The percentage of choice of left handover raised from close to 0\% to around 50\% close to episode 10. The percentage of right handovers remain low with small increases after episode 10.}
%   % \vspace{-1em}
%   \caption{Choice of OTP (\%) by operator across participants for each episode in the teleoperated sessions.}
%   \label{fig:personal_loc}
%   % \vspace{-1em}
% \end{figure}

\begin{table}[h]
\caption{OTP prediction accuracy on each fold (the $n$-th 20\% of episodes in a participant) on the FACT HRC dataset.}
\label{tab_otp_ep_per_fold}
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Fold & \#1 & \#2 & \#3 & \#4 & \#5 & mean \\
\hline
Acc(\%) & 92.2 & 81.6 & 71.6 & 71.4 & 76.5 & 78.6\\
\hline
\end{tabular}
% \vspace{-1em}
\end{table}

As shown in Fig~\ref{fig:personal_time}, the participant-wise standard deviations in pauses at the work space and storage space during an episode are higher in later episodes of the teleoperated sessions. The handover timing diverged most at three key events: when the participants returned objects to the robot at the end of the preparation and assembly stages, and in the later half of the painting stage when the participants began colouring the birdhouse. The divergence is larger in the painting stage when the participants were engaged in creative activities as opposed to functional activities of assembly. This indicates that the operator personalised the handover timing, especially when participants showed individual differences in object return preferences and in creative activities. 
% No significant difference was found between CV folds in the autonomous model's handover timing predictions on the FACT HRC dataset. 

% In addition to handover location and timing, we analysed the type of handovers performed in the teleoperated sessions in Fig~\ref{fig:personal_type}. As shown here, in the preparation stage the bidirectional handover is the most common, in the assembly stage R2H handovers are the most common, and in the painting stage the operator performed a mix of single and bidirectional handovers. This further demonstrates the operator's personalisation of handovers in creative activities and in later episodes of the teleoperated sessions.

% \begin{figure}[tb]
%   \centering
%   \pdftooltip{\includegraphics[width=0.8\linewidth]{S2_fig/S1_time_ep_anno.pdf}}{The x-axis is episode number, the y-axis is std of pause with a value range of 0 to 400 seconds. Episode 0 to 5 is approximately preparation stage, 5 to 15 is assemble stage, 15 to 25 is painting stage, with overlaps between the stages. The std of pause at work space showed a peak at around episode 13 with value slightly above 100, and then at around episode 24 with value slightly above 200. The std of pause at storage space showed a peak at around episode 3 and then at around episode 13 with a value slightly above 100, with a tall peak at around episode 22 with value slightly above 350.}
%   % \vspace{-1em}
%   \caption{Std of pause at work space and storage space across participants for each episode in teleoperated sessions.}
%   \label{fig:personal_time}
%   % \vspace{-1em}
% \end{figure}

% \begin{figure}[tb]
%   \centering
%   \pdftooltip{\includegraphics[width=0.8\linewidth]{S2_fig/S1_typ_ep_anno.pdf}}{The x-axis is episode number, the y-axis is percentage of choice between different handover types. Episode 0 to 5 is approximately preparation stage, 5 to 15 is assemble stage, 15 to 25 is painting stage, with overlaps between the stages. R2H handovers was the dominent one in the assembly stage at around 80\%, birdirational handovers was chosen more at around episode 3 at about 80\%, then at around episode 13 at about 50\%. The H2R and R2H choices are more evently mixed after episode 14.}
%   % \vspace{-1em}
%   \caption{Choice of handover type (\%) by operator across participants for each episode in the teleoperated sessions.}
%   \label{fig:personal_type}
%   % \vspace{-1em}
% \end{figure}
% We analysed the handover timing and location in the teleoperated data for each episode across all 20 participants aligned by the episode ID. Note that there are small differences in the number of episodes in each stage across the participants in the teleoperated sessions.

% \begin{figure*}[tb]
%     \centering
%     \subfloat[][Choice of OTP (\%)]{\pdftooltip{\includegraphics[width=0.33\linewidth]{S2_fig/S1_loc_ep_anno.pdf}}{The x-axis is episode number, the y-axis is OTP location choice (\%). Episode 0 to 5 is approximately preparation stage, 5 to 15 is assemble stage, 15 to 25 is painting stage, with overlaps between the stages. The percentage of choice of middle handover is near 100\% at the start, but drop to around 50\% close to episode 10. The percentage of choice of left handover raised from close to 0\% to around 50\% close to episode 10. The percentage of right handovers remain low with small increases after episode 10.}\label{fig:personal_loc}}\hfill
%     \subfloat[][Standard deviation of pause (s)]{\pdftooltip{\includegraphics[width=0.33\linewidth]{S2_fig/S1_time_ep_anno.pdf}}{The x-axis is episode number, the y-axis is std of pause with a value range of 0 to 400 seconds. Episode 0 to 5 is approximately preparation stage, 5 to 15 is assemble stage, 15 to 25 is painting stage, with overlaps between the stages. The std of pause at work space showed a peak at around episode 13 with value slightly above 100, and then at around episode 24 with value slightly above 200. The std of pause at storage space showed a peak at around episode 3 and then at around episode 13 with a value slightly above 100, with a tall peak at around episode 22 with value slightly above 350.}\label{fig:personal_time}}\hfill
%     \subfloat[][Choice of handover type (\%)]{\pdftooltip{\includegraphics[width=0.33\linewidth]{S2_fig/S1_typ_ep_anno.pdf}}{The x-axis is episode number, the y-axis is percentage of choice between different handover types. Episode 0 to 5 is approximately preparation stage, 5 to 15 is assemble stage, 15 to 25 is painting stage, with overlaps between the stages. R2H handovers was the dominant one in the assembly stage at around 80\%, bidirectional handovers was chosen more at around episode 3 at about 80\%, then at around episode 13 at about 50\%. The H2R and R2H choices are more evenly mixed after episode 14.}\label{fig:personal_type}}
% \caption{OTP, timing, and handover type choices by the operator across participants in the teleoperated sessions.}
% \label{fig:personal}
% \end{figure*}

\begin{figure}[tb]
    \centering
    \subfloat[][Choice of OTP (\%)]{\pdftooltip{\includegraphics[width=0.95\linewidth]{S2_fig/S1_loc_ep_anno.pdf}}{The x-axis is episode number, the y-axis is OTP location choice (\%). Episode 0 to 5 is approximately preparation stage, 5 to 15 is assemble stage, 15 to 25 is painting stage, with overlaps between the stages. The percentage of choice of middle handover is near 100\% at the start, but drop to around 50\% close to episode 10. The percentage of choice of left handover raised from close to 0\% to around 50\% close to episode 10. The percentage of right handovers remain low with small increases after episode 10.}\label{fig:personal_loc}}\\
    \subfloat[][Standard deviation of pause (s)]{\pdftooltip{\includegraphics[width=0.95\linewidth]{S2_fig/S1_time_ep_anno.pdf}}{The x-axis is episode number, the y-axis is std of pause with a value range of 0 to 400 seconds. Episode 0 to 5 is approximately preparation stage, 5 to 15 is assemble stage, 15 to 25 is painting stage, with overlaps between the stages. The std of pause at work space showed a peak at around episode 13 with value slightly above 100, and then at around episode 24 with value slightly above 200. The std of pause at storage space showed a peak at around episode 3 and then at around episode 13 with a value slightly above 100, with a tall peak at around episode 22 with value slightly above 350.}\label{fig:personal_time}}
\caption{Handover location (a) and timing (b) choices by the operator across participants in the teleoperated sessions.}
\label{fig:personal}
\end{figure}


\section{Discussion}\label{sec:diss}
The machine learning experiments in Section~\ref{sec:ml} illustrate that our autonomous handover model learned key temporal-spatial adaptation strategies adopted by a human operator to achieve collaborative human-robot handovers. However, the user study evaluation in Sections~\ref{sec:s2}~\&~\ref{sec:S1vsS2} showed significant gaps between autonomous handovers and teleoperation, rejecting our hypothesis that the autonomous policy will achieve human-level collaboration performance. 

As discussed in~\cite{ortenzi2021object}, a handover episode can be divided into the pre-handover phase involving perception, planning and communication, and the physical handover phase involving controls and error handling. 
During the pre-handover phase, from the robot's perspective, our analysis of handover errors in Sections~\ref{subsec:s2_csv}~\&~\ref{subsec:S1vsS2_csv} showed that it is important to incorporate both human behaviours (hand gesture, head and body movements, gaze directions) and task context (task progress, object types and locations). From the user's perspective, our analysis of user perceptions in Sections~\ref{subsec:s2_quali}~\&~\ref{subsec:S1vsS2_qualtrics} indicated that there may be a trade-off between reactive handovers (i.e., robot responding to explicit human cues) and proactive handovers (i.e., robot anticipating handovers based on past experiences and context understanding). Closer examination of the teleoperated handovers in Section~\ref{subsec:S1vsS2_personal} showed that the human operator personalised their handover strategies for individual users. Therefore, it may be beneficial for autonomous handovers to incorporate prior knowledge during the pre-handover phase alongside observations specific to that episode, in order to allow correct perception of human states and intentions, as well as correct mapping from the perceived human state to their desirable handover policy (i.e., personalisation).

During the physical handover phase, our analysis of autonomous handovers and participant behaviours in Sections~\ref{subsec:s2_csv}~\&~\ref{subsec:s2_quali} showed that autonomous handovers can benefit from increased transparency and explainability, allowing a user to understand the robot's observations, plans, and actions and thus providing explicit or implicit feedback to adjust or correct them. This allows a human-in-the-loop approach to error handling and personalisation from the robot's perspective, while increasing control and agency from the user's perspective. However, such close human supervision may also conflict with the desire towards proactive handovers or efficient HRC. Thus, it is important to understand how different user and task context may influence the locus of control (robot vs. user) when designing collaborative handovers.

% organise discussion by robot's perspective and user's perspective, or handover observation and policy (timing, personalisation, feedback), trade-offs between the perspectives
% handover task section -> robot-user perspective trade-offs, task/user context that leads to different weights, literature on context influencing preferred actions
% robot perceiving what state the person is in correctly, and the correct mapping to a suitable policy (personalisation)
% user's perspective: knowing the robot's decision, maintaining control, adapting to the robot, opposite to robot being proactive

% Specifically, we identified three key areas for improving the collaborative outcomes of autonomous handover model:

% \textbf{Personalise}: The supervised learning based autonomous handover policy uses a general decision strategy to determine spatial-temporal adaptations in each handover as an independent event. The human operator's ability to learn a user's preferences over time contributes to positive collaboration performance and user experience. Therefore, one future direction is to incorporate personalisation in the handover policy, such as via reinforcement learning or user profiling.
% % explicit / implicit personalisation

% \textbf{Proactive}: The autonomous handover model relies on observable behavioural cues (hand gestures, head and body movements, gaze directions) to predict handover actions. However, the human operator is able to perform handovers preemptively based on their understanding of the task and user context. Such proactive handovers contribute to efficiency and fluency in the HRC. Therefore, future development of HRC can benefit from improving a robot's proactive behaviours.

% \textbf{Perceivable}: Our interviews with the participants indicate that a more transparent and explainable robot decision-making model benefits HRC and human  perceptions of the robot. Human-in-the-loop approach to a robot's decision-making is desirable for future development of HRC as this offers users the opportunity to correct robot errors and incorporate their person contexts.

% Our study has a number of limitations. Firstly, the autonomous handover policy used pose estimations, while using raw videos may provide additional information for handover adaptation, such as object layouts on the work desk. Secondly, we evaluated the autonomous handover model on the FACT HRC dataset and in a user study replicating the design of the FACT HRC crafting task~\cite{tian2023crafting}. However, the design of the spatial-temporal coordination may be applicable to handovers in other task contexts and robot platforms. Thus, we plan to conduct additional experiments in different HRC contexts. Finally, the user study was conducted with a small number of participants recruited from a university student and staff population due to feasibility considerations. Additional evaluation with a larger and more diverse participant population is needed to understand the generalisability of the findings.


\section{Conclusions}\label{sec:con}
% We investigate human-robot handovers in a collaborative task containing multiple purposeful handovers. We first trained an autonomous handover policy using the FACT HRC dataset of teleoperated handovers. We then evaluated objective and subjective performance of the autonomous policy in a user study replicating the FACT HRC task scenario of a mobile manipulator robot assisting a person in assembling and painting a wooden birdhouse. We identified several open challenges in developing autonomous handovers that can achieve human-level HRC performance.
We developed an autonomous handover model using the FACT HRC dataset of teleoperated handovers~\cite{tian2023crafting}, which predicts when and where a mobile manipulator robot performs key handover actions while engaging in task-oriented handovers. By conducting user study evaluation and comparing with teleoperated handovers, we identified open challenges in developing autonomous handovers that can achieve human-level HRC performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}

% \section*{ACKNOWLEDGMENT}
% This work is funded by the Australian Research Council Future Fellowship FT200100761.


% references
\bibliographystyle{IEEEtran}
\bibliography{HRI2025_S2}
\balance


\end{document}
