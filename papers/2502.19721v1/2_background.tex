\section{Background}
This section provides background on gender bias and activation steering for LLMs.

\subsection{Gender Bias}
The concept of gender is contested and multifaceted, encompassing a person's self-identity and expression, the perceptions held by others, and the social expectations imposed upon them~\citep{devinney2022theories}. 
We draw inspiration from \textit{gender schema theory}~\citep{bem1981gender}, which describes the cognitive process of ``gendering''---dividing entities into masculine and feminine categories---and its subsequent impact on individuals' behaviors. 
%We examine the internal mechanisms of gendering in LLMs that influence the extent of biased predictions made by the model.
We adopt \citet{ackerman2019syntactic}'s definition of \textit{conceptual gender}---the gender expressed, inferred, and used by a model to classify a referent through explicit (e.g., pronouns) or implicit associations (e.g., stereotypes). While some gender notions are multi-dimensional, we assume a simple setting where gender may be encoded in a one-dimensional subspace of LLMs. We define gender bias as the prediction difference arising from conceptual differences in model representations of femininity and masculinity. This bias may or may not lead to undesirable outcomes (e.g., negative stereotypes and discrimination) depending on the context.

\subsection{Activation Steering}
\label{sec:activation-steering}
\textit{Activation steering} is an inference-time intervention that \emph{steers} model outputs by deliberately perturbing the model's activations~\citep{turner2023activation}. These activations (or residual stream activations) refer to the intermediate outputs aggregated from the preceding layers~\citep{elhage2021mathematical}. Model activations may be modified by applying \textit{steering vectors}, which can be computed by different methods methods~\citep{tigges2023linear} including logistic regression, principal component analysis, and \emph{difference-in-means} which is currently the most widely used method. 

Consider a decoder-only transformer model, trained with a set of tokens vocabulary $\mathcal{V}$. The model makes predictions by mapping each input $x=(x_1,x_2,...,x_t), x_i \in\mathcal{V}$, to an output probability distribution $y\in\mathbb{R}^{|\mathcal{V}|}$. Given two sets of prompts, \textit{difference-in-means} (MD) computes a candidate vector for each layer $l\in L$ as the difference in activation means~\citep{marks2024the}:
\[
    \Vec{u}^{(l)} = \frac{1}{|\mathcal{D}_A|}\sum_{x\in\mathcal{D}_A} \Vec{h}_{x_i}^{(l)} - \frac{1}{|\mathcal{D}_B|}\sum_{x\in\mathcal{D}_B} \Vec{h}_{x_i}^{(l)}
\]
where $\Vec{h}_{x_i}^{(l)}$ denotes the activation of input $x$ at token position $i$ and model layer $l$. The prompts in $\mathcal{D}_A$ and $\mathcal{D}_B$ are usually constructed with inputs reflecting two contrasting concepts. The vector $\Vec{u}^{(l)}$ captures the internal representation difference between concepts $A$ and $B$ that may elicit changes in model outputs. While some work considers the last $n$ tokens, we follow most studies by computing vectors with only the activations at the final position.

Based on the candidate vectors of a size $|L|$, previous work often performs a brute-force search across layers to select the one with the optimal intervention performance~\citep{arditi2024refusal}. During inference, the steering vector can be applied using \textit{activation addition}~\citep{rimsky-etal-2024-steering}, which intervenes in the forward pass of an input as:
\[
    \Vec{h}_{x}^{(l)} = \Vec{h}_{x}^{(l)} + c\Vec{u}^{(l)}
\]
where $c$ is the steering coefficient which can be either positive or negative. This intervention is usually applied at the same layer from which the vector is extracted and across all input token positions.

