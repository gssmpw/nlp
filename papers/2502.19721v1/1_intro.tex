\section{Introduction}
Large language models (LLMs) are optimized for making generalizations about the world based on their training data. These systems risk amplifying biases and inequities present in their training data, potentially perpetuating harmful stereotypes and resulting in discriminatory outcomes. To address these concerns, various mitigation strategies have been proposed, including techniques based on prompt engineering~\citep{ganguli2023capacity,kaneko2024evaluating}, fine-tuning~\citep{chintam-etal-2023-identifying,ranaldi-etal-2024-trip}, modified decoding~\citep{lu-etal-2021-neurologic,liu-etal-2021-dexperts}, and detection~\citep{inan2023llama,fan-etal-2024-biasalert}. %However, these methods often require domain knowledge about the application task, and fine-tuning incurs computational costs.
%

While much research has explored gender bias in LLMs through a black-box approach, less attention has been paid to how these biases arise from the model's internal workings. Recent work on representation engineering provides insights into varied abstract features within the internal representations of LLMs~\citep{zou2023transparency}, such as sentiment~\citep{tigges2023linear}, spatiotemporal information~\citep{gurnee2024language}, and true/false statements~\citep{marks2024the}. Several studies have also demonstrated promising results in effectively controlling model behaviors by modifying their feature representations~\citep{turner2023activation,rimsky-etal-2024-steering,arditi2024refusal}.

In this work, we leverage \textit{activation steering} (also known as activation engineering), to study how the concept of gender encoded in the internal representations of LLMs affects their predictions and how we can manipulate internal representations to mitigate biases at inference time. 
% moved: We draw inspiration upon the \textit{gender schema theory}~\citep{bem1981gender}, which describes the cognitive process of ``gendering''---dividing entities into masculine and feminine categories---and its subsequent impact on individuals' behaviors. We examine the internal mechanisms of gendering in LLMs that influence the extent of biased predictions made by the model.

\shortsection{Contributions}
We propose a novel method that extracts linear representations from LLMs for steering model predictions associated with a given concept (\autoref{sec:extract-vector}). While existing methods for computing steering vectors rely on labeled data, we compute them using probability weighting without explicit data annotations. In addition, we introduce metrics to efficiently select a steering vector without exhaustive searches as was required by most previous methods. We show that steering vectors produced by our method exhibit a higher correlation with gender bias in model outputs than the prevailing difference-in-means method (\autoref{sec:extract-results}). We then present an approach for applying steering vectors to provide precise control over the internal representation (\autoref{sec:apply-steering}). We demonstrate the effectiveness of our steering vectors and method for applying them in reducing gender bias on the in-distribution task (\autoref{sec:steering-results}) and its potential to generalize to other application tasks (\autoref{sec:transferability}).