\begin{abstract}
Large language models (LLMs) are known to perpetuate stereotypes and exhibit biases. Various strategies have been proposed to mitigate potential harms that may result from these biases, but most work studies biases in LLMs as a black-box problem without considering how concepts are represented within the model. We adapt techniques from representation engineering to study how the concept of ``gender'' is represented within LLMs. We introduce a new method that extracts concept representations via probability weighting without labeled data and efficiently selects a \textit{steering vector} for measuring and manipulating the model's representation. We also present a projection-based method that enables precise steering of model predictions and  demonstrate its effectiveness in mitigating gender bias in LLMs.\footnote{Our code is available at: \url{https://github.com/hannahxchen/gender-bias-steering}}
%We show that our method produces steering vectors that better reflect the concept learned by the model than the prevailing approach, difference-in-means. Moreover, we demonstrate how the steering vectors can be used to reduce gender biases in model outputs.

\end{abstract}