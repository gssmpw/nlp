
\section{Applying Steering Vectors}
\label{sec:apply-steering}
Previous works mostly consider contexts in which the model only needs to be steered in a particular direction or assume that the target directions are known in advance. However, in contexts such as bias mitigation, we need to apply steering based on the type of input, which may be unknown at deployment. We describe our method for applying the steering vector and demonstrate its efficacy in mitigating bias.


\subsection{Intervention Method} 
\label{sec:intervention-method}
Since a model can exhibit varied degrees of bias to different inputs, applying the steering vector with activation addition uniformly may result in over-correction or insufficient mitigation. To obtain more precise control, we improve upon prior approaches by applying the steering vector scaled by the latent projection for each input $x$:
\[
    \Vec{h}_x^{\prime} = \Vec{h}_x + \lambda\cdot \proj{v}{x}
\]
where $\lambda$ is the steering coefficient. We apply this operation across all token positions of $x$ but at only the layer from which $\Vec{v}$ was extracted. The model becomes more biased to $A$ when $\lambda > 0$ and to $B$ when $\lambda<0$. 


To mitigate bias, we can simply set the steering coefficient $\lambda$ to $-1$, which steers the latent state of an input by the extent of bias reflected in the projection. This formulation is similar to the directional ablation approach proposed by~\citet{arditi2024refusal}, which also considers vector projections. However, they show that this approach, using steering vectors computed by MD, can only be used for removing a single concept (in one direction) and requires interventions across all model layers.


\subsection{Steering for Bias Mitigation}
\label{sec:steering-results}

\begin{figure*}[tb]
\centering
    \begin{subfigure}[b]{0.495\linewidth}
        \includegraphics[width=0.49\linewidth]{figs/mistral-nemo-debias-scatter-MD.pdf}
        \includegraphics[width=0.49\linewidth]{figs/mistral-nemo-debias-scatter-WMD.pdf}
        \caption{\model{Mistral-Nemo-12B}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.495\linewidth}
        \includegraphics[width=0.495\linewidth]{figs/qwen-7b-debias-scatter-MD.pdf}
        \includegraphics[width=0.495\linewidth]{figs/qwen-7b-debias-scatter-WMD.pdf}
        \caption{\model{Qwen-7B}}
    \end{subfigure}
\caption{Disparity scores $s_x$ \emph{before} and \emph{after} debiasing the model with the steering vector. The x-axis indicates the scalar projection of each input \emph{before} intervention.}
\label{fig:debias-scatter}
\end{figure*}

\begin{figure*}[tb]
\centering
    \includegraphics[width=0.24\linewidth]{figs/qwen-1.8b-winogenerated-wo-offset-lambda=1.0.pdf}
    \includegraphics[width=0.24\linewidth]{figs/qwen-1.8b-winogenerated-wo-offset-lambda=1.5.pdf}
    \includegraphics[width=0.24\linewidth]{figs/qwen-1.8b-winogenerated-with-offset-lambda=1.5.pdf}
    \includegraphics[width=0.24\linewidth]{figs/qwen-1.8b-winogenerated-with-offset-lambda=2.0.pdf}
\caption{Bias scores and projections evaluated on the Winogenerated dataset for \model{Qwen-1.8B} model. The color indicates the results \textit{before} or \textit{after} debiasing. The \textit{offset} readjusts the input activations by offsetting them by the average activations of examples sampled from Winogenerated.}
\label{fig:winogenerated-debiasing}
\end{figure*}
e evaluate the effectiveness of steering vectors selected using the method described in \autoref{sec:extract-results} to mitigate gender bias. We apply the steering vectors with our proposed projection-based debiasing method and measure the bias score on the validation set, computed as the root mean square (RMS) of disparity score $s_x$. 

\autoref{tab:steering-performance} reports the bias scores before and after debiasing for each model. After applying the intervention, it shows a significant reduction in the bias score for all models. The intervention is particularly effective for \model{Ministral-8B} and \model{Mistral-Nemo-12B} instruction models with bias scores reduced to nearly zero. In addition, the results suggest that the projection and bias score correlation $r$ is a good indicator of the intervention performance. Models with a higher value of $r$ show a greater decrease in the bias score after intervention. 

To analyze the impact of intervention on different inputs, we compare the bias score difference and the scalar projection of each input, as shown in \autoref{fig:debias-scatter}. We apply the same intervention method for both steering vectors computed by MD and WMD. The projections of all data points are measured on the baseline model with no intervention. Debiasing with WMD's steering vectors works as intended where more biased inputs show a larger difference in their bias scores after debiasing while less biased inputs are less affected. However, the inputs tend to be over- or under-corrected in their bias scores when using steering vectors computed by MD. As our intervention approach depends on the projection of each input, the mitigation becomes less effective when the steering vector fails to separate the bias direction or does not reflect well with model bias.


\subsection{Steering Transferability}
\label{sec:transferability}
We evaluate the robustness of  steering vectors computed using our method by testing whether a steering vector extracted using one dataset transfers effectively to other tasks.

\subsubsection{Evaluation Tasks}
We consider two gender bias tasks: 

\shortsectionnp{Winogenerated}~\citep{perez-etal-2023-discovering} is a human validated version of the Winogender pronoun resolution task~\citep{rudinger-etal-2018-gender} that is 50 times larger than the original datset. The model is inquired to fill in the missing blank with a pronoun for a given sentence (e.g., \textit{``The surgeon assured the patient that \_\_ would do the best possible job.''}). The response can be either a male, female, or gender-neutral pronoun. We report the bias score by the prediction probability difference between the female and male pronouns after normalizing the probabilities over all three pronoun options.

\shortsection{Occupational Stereotypes} We construct a question-answering style task that asks the model, \emph{What does \emph{[name]} work as at the \emph{[industry/place]}?}. We use terms from nine different industries (e.g., technology, healthcare) and 100 first names commonly associated with each female, male, and gender-neutral group. We measure the frequency of job titles mentioned in the model's generated response for each group under the model's default temperature setting. Note that the prompts do not contain any explicit gendered words except for names that may encode gender information.

\autoref{app:transferability} provides further details on the construction of both tasks.

\subsubsection{Results}

\begin{figure}[tb]
\centering
    \includegraphics[width=\linewidth]{figs/qwen-1.8b-occupation-projection.pdf}
\caption{Input projections of the occupational stereotypes task, evaluated on \model{Qwen-1.8B} at the last token position. The color indicates the gender associated with the name used in the prompt.}
\label{fig:occupation-projection}
\end{figure}

We apply the same debiasing approach described in \autoref{sec:intervention-method} using steering vectors computed by our method with the gendered language dataset. \autoref{fig:winogenerated-debiasing} shows the results of the Winogenerated task for \model{Qwen-1.8B}, comparing bias scores of each input before and after intervention. Despite using the gendered language dataset to extract the steering vector, the steering vector is still able to reflect bias in model outputs, with a correlation of 0.82 between the projections and bias scores. However, we find that the input projections do not align well with the bias score direction. As shown in the left graph of \autoref{fig:winogenerated-debiasing}, most inputs have a projection above 0, indicating a higher degree of female signal than the actual bias score. This leads to under-correction for the originally male-biased inputs. This result may attributed to the difference in the underlying distribution between this dataset and the one we used for computing the vector. We tried using a higher magnitude of $\lambda$ to $-$1.5 (second graph in \autoref{fig:winogenerated-debiasing}). This increases the impact of the steering, but neither reduces the bias nor resolves the issue of the misalignment.

To resolve the misalignment, we readjust the input activations by offsetting them with the activation mean of $\frac{1}{3}$ of the Winogenerated examples at the last token position. We find that this approach improves the efficacy of debiasing. In the third graph of \autoref{fig:winogenerated-debiasing}, we apply the offset with a value of $\lambda=-1.5$, which leads to a higher number of inputs with bias scores that are closer to zero. Moreover, setting the value of $\lambda=-2$ results in a mirrored image of the original data points (rightmost graph of \autoref{fig:winogenerated-debiasing}). This suggests that the model is steered towards the direction as intended where the biased inputs are moved towards the opposite gender direction.

\autoref{fig:occupation-projection} reports the projection of each prompt using \model{Qwen-1.8B} for five industries in the occupational stereotypes task. Despite the lack of explicit gender wording in prompts, the projections measured indicate that the model still infers gender signals from the input. The projections also correspond to the gender associated with the names provided in the prompts. Masculine names show higher negative projection values, while feminine names exhibit higher positive projections. Gender-neutral names tend to have the lowest magnitude of projections.

We analyze the frequency of job titles predicted for feminine and masculine names, comparing their frequency differences before and after debiasing with steering vectors. Similar to the Winogenerated task, we also apply an offset to counteract potential distribution shifts. \autoref{fig:occupation-debias-tech} displays the predicted job titles in the technology and healthcare sectors with the most gender disparities. Prior to intervention, the model exhibits the largest discrepancies in predicting ``software engineer'' and ``product manager'' in technology and ``nurse'' and ``doctor'' in healthcare. After debiasing, there is a noticeable decrease in the frequency gap for most of the top predicted job titles. It also increases the relative prediction frequency of more neutral titles, such as "healthcare professional," for masculine names.


\begin{figure}[tb]
\centering
    \includegraphics[width=0.9\linewidth]{figs/qwen-1.8b-occupation-stereotypes-technology.pdf}
    \includegraphics[width=0.9\linewidth]{figs/qwen-1.8b-occupation-stereotypes-healthcare.pdf}
\caption{Difference in job title prediction frequency when prompted with feminine names compared to masculine names. The color represents the difference \textit{before} and \textit{after} debiasing on \model{Qwen-1.8B}. The y-axis shows the top 10 titles with the largest prediction gap.}
\label{fig:occupation-debias-tech}
\end{figure}