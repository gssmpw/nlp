

% \cleardoublepage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% When adding this appendix to your paper, 
% please remove above part
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliographystyle{IEEEtranS}
%\bibliographystyle{ACM-Reference-Format}
% \bibliography{references}

\clearpage
\setcounter{page}{1}
\appendix
\begin{center}
{\huge\textbf{\textsf{Supplementary Material}}}    
\end{center}

% \section{Supplementary}

\begin{comment}
\subsection{Prefill Decode Ratio}
\begin{figure}[]
    \centering
    \includegraphics[width=\linewidth]{figures/prefill_decode_latency_sup.pdf}       
    \caption{
    Prefill decode latency ratio.}
    \label{fig:sup_prefill_decode_ratio}
\end{figure}

During the prefill stages, all layers are processed by the GPU. 
However, all layers except the MHA layers are processed by the GPU during the subsequent stages.
As denoted in FlexGen\cite{flexgen}, the computation delegation with overlap option one.
\end{comment}

\section{Additional Sensitivity Study}
We conducted additional sensitivity study on the batch size and model size.
In \cref{fig:sup_batch_model_sensi} (a), we show that \thiswork shows improved throughput with sequence length 512 and various model size.
In \cref{fig:batch_model_sensi}, we tested the scalability of \thiswork with respect to the model size with the sequence length 1024.
We can find that both \thiswork and baseline attain higher throughput with reduced sequence length size.
The large model size decreases the throughput of the baseline and \thiswork, because it requires  large amount of computation and memory.
However, \thiswork still shows approximately 2$\times$ improved throughput compared to the baseline in this setting.
In \cref{fig:sup_batch_model_sensi} (b), we can find that \thiswork still achieves higher throughput with various batch sizes with OPT-30B.
Large batch size increases the utilization of GPU and the loaded weight can be resued.
The bottleneck of the baseline is PCIe lane which is used to load the weights.
Therefore, the large batch size mitigates the bottleneck of the baseline and slightly increases the throughput of it.
However, \thiswork uses internal bandwidth in CSDs and the main bottleneck is not PCIe lanes.
In \cref{fig:batch_model_sensi} (a), we conducted the batch size sensitivity study with OPT-13B and found similar results.
\thiswork still attains more than 2$\times$ improved throughput with various batch sizes and OPT-30B.
% Batch size. Additionally, we run a sensitivity study on
% the OPT 13B model with different batch sizes, evaluating the
% throughput improvements of INF2 compared to the baseline.
% We use batch sizes ranging from 16 to 256. As depicted in
% Figure @@ (a), the thoughput improvement of INF2 tends to
% decrease as batch size becomes larger. Enlarging batch size
% results in higher GPU utilization, reducing total computa-
% tion time. As CSD-aware Diagonal Scheduling overlaps GPU
% computation with attention on the CSDs, INF2 is less affected
% by reduced GPU computation time compared to the baseline,
% leading to reduced relative performance in large batch sit-
% uations. However, relative performance drop is limited as
% multi-head attention operation is the main bottlneck of the
% system. Overall, INF2 still shows high throughput improve-
% ment, ranging from 2.66× at batch size 32, to 2.19× at batch
% size 256
% In @@ (b), we analyze our model's scalability to larger model sizes.
% We increased the model size from 6.7B to 175B, and measure throughput improvements compared to the baseline for each model size.
% Compared to the thoughput of the baseline, \thiswork's performance improvements stay consistent regardless of the size of the model.
% This shows that \thiswork is suitable for applying to both small and large models.
% \JW{Too short.}


\section{Additional Discussion}
% \subsection{KV cache size}

% \subsection{Accelerator Throughput Analysis}


% \subsection{Compile Time}
% Total elapsed time: 3h 16m 31s

% \subsection{Model Configuration}
% OPT




\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/batch_model_sensi_sup.pdf}       
    \caption{
    Additional batch size and model size sensitivity results.\JW{there is no caption (a) and (b) now.}}
    \label{fig:sup_batch_model_sensi}
\end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=\linewidth]{figures/prefill_decode_latency_sup.pdf}       
    \caption{
    Additional execution time breakdown into prefill stage and decoding stages.}
    \label{fig:prefill_decode_ratio_sup}
\end{figure}



\section{Additional Execution Time Breakdown}

In \cref{fig:prefill_decode_ratio_sup}, we measured the execution time breakdown with OPT-30B on \thiswork.
We can find that the latency of decoding takes up a large portion of the total latency compared to the prefill latency, which is a consistent result with \cref{fig:prefill_decode_ratio}.
\thiswork targets the decoding stage and reduce the decoding latency by our methods.
Each components of our methods contributes to reducing the decoding latency.
The proportion of prefill latency increases as our methods reduce the latency of decoding latency and the latency of the prefill remains constant.
However, \thiswork still shows substantially lower latency compared to the baseline by targeting the decoding latency.
We also included the execution time breadown with OPT-66B in \cref{fig:prefill_decode_ratio} and found that \thiswork successfully reduces the latency of the decoding stage.