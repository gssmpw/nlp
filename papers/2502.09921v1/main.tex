%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template for ASPLOS papers.
%
% History:
% 
% ASPLOS originally used jpaper.cls for submission but required acmart.cls for the
% final camera-ready version. To avoid a change in format, starting ASPLOS 2024 Fall 
% cycle, both the submission and the camera-ready versions started using acmart.cls.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% use the base acmart.cls version 1.92
% use the sigplan proceeding template with the default 10 pt fonts
% nonacm option removes ACM related text in the submission. 
\documentclass[nonacm,sigplan]{acmart}

% enable page numbers
\settopmatter{printfolios=true}

% make references clickable 
%\usepackage{hyperref}
%\usepackage{hyperxmp}
%\usepackage{cite}

% custom packages
\input{packages}

% custom commands
\input{commands}

\begin{document}

\title{INF$^2$: High-Throughput Generative Inference of Large Language Models using Near-Storage Processing}

 \author{Hongsun Jang}
 \affiliation{%
   \institution{Seoul National University}
   \city{Seoul}
   \country{South Korea}}
 \email{hongsun.jang@snu.ac.kr}
 
 \author{Siung Noh}
 \affiliation{%
   \institution{Seoul National University}
   \city{Seoul}
   \country{South Korea}}
 \email{siung98@snu.ac.kr}
 
 \author{Changmin Shin}
 \affiliation{%
   \institution{Seoul National University}
   \city{Seoul}
   \country{South Korea}}
 \email{scm8432@snu.ac.kr}

  \author{Jaewon Jung}
 \affiliation{%
   \institution{Seoul National University}
   \city{Seoul}
   \country{South Korea}}
\email{jungjaewon@snu.ac.kr}

  \author{Jaeyong Song}
 \affiliation{%
   \institution{Seoul National University}
   \city{Seoul}
   \country{South Korea}}
\email{jaeyong.song@snu.ac.kr}

 \author{Jinho Lee}
 \affiliation{%
   \institution{Seoul National University}
   \city{Seoul}
   \country{South Korea}}
\email{leejinho@snu.ac.kr}


\begin{abstract}

The growing memory and computational demands of large language models (LLMs) for generative inference present significant challenges for practical deployment. 
One promising solution to address these challenges is \emph{offloading-based batched inference}, which leverages host memory and disk as an extended memory hierarchy for GPUs.
While the approach cost-effectively enables LLM inference, its performance is limited by substantial I/O overhead, primarily due to the large key-value (KV) cache sizes, which increase with batch size and LLM context window length.

In this paper, we introduce INFerence-INFinity (\thiswork), a framework that boosts generative inference throughput using computational storage devices (CSDs). 
The core of \thiswork is \emph{\ans}, which offloads memory-intensive self-attention operations to near-storage accelerators, significantly reducing traffic through the system interconnect. 
We also propose \emph{\wb} to hide storage write latency by delaying newly generated KV cache writes until the cache reaches sufficient size in system memory. 
Additionally, we introduce \emph{cooperative \xcache}, a technique designed to further trade off the remaining memory capacity for storage bandwidth. 
Our methods effectively minimize idle time for computation, improving the overall throughput.

To demonstrate the effectiveness of our approach, \thiswork has been implemented on PyTorch and evaluated on a real system. 
Our experiments show that \thiswork achieves up to 3.46$\times$ throughput improvement compared to state-of-the-art baselines. 
We will open-source \thiswork to facilitate broader adoption.


\end{abstract}

\maketitle % should come after the abstract
\pagestyle{plain} % should come right after \maketitle

 \author{Jaeyong Song}
 \affiliation{%
   \institution{Seoul National University}
   \city{Seoul}
   \country{South Korea}}
 \email{jaeyong.song@snu.ac.kr}
 
 \author{Hongsun Jang}
 \affiliation{%
   \institution{Seoul National University}
   \city{Seoul}
   \country{South Korea}}
 \email{hongsun.jang@snu.ac.kr}
 
 \author{Jaewon Jung}
 \affiliation{%
   \institution{Seoul National University}
   \city{Seoul}
   \country{South Korea}}
 \email{jungjaewon@snuj.ac.kr}

  \author{Youngsok Kim}
 \affiliation{%
   \institution{Yonsei University}
   \city{Seoul}
   \country{South Korea}}
\email{youngsok@yonsei.ac.kr}

 \author{Jinho Lee}
 \affiliation{%
   \institution{Seoul National University}
   \city{Seoul}
   \country{South Korea}}
\email{leejinho@snu.ac.kr}


%%%%%%% -- PAPER CONTENT STARTS -- %%%%%%%%


\section{Introduction}
\label{sec:intro}

Transformer-based large language models (LLMs)~\cite{attention, gpt2, gpt3, gpt4, llama, opt} are at the forefront of natural language processing (NLP), significantly influencing a wide array of user applications~\cite{gpt4, github_copilot, llm_cloud_incident}. 
Therefore, efficiently conducting generative inference for LLMs~\cite{flexgen, vllm, deepspeedinf, huggingface, petals} has become increasingly crucial to facilitate their use in various tasks.


One widely adopted and promising method for LLM inference is the \emph{\baseline} approach~\cite{flexgen, infinigen}, which significantly improves efficiency by processing data in numerous batches over large token sets, allowing for repeated reuse of model weights.
The core idea behind this approach is to store intermediate data, specifically called \emph{KV cache}~\cite{scaletrans}, in system memory and storage devices (e.g., NVMe SSDs), extending the memory hierarchy for GPUs.
Unfortunately, this approach encounters significant I/O overhead as the batch size and LLM context length increase~\cite{vllm,infinigen,h2o,flexgen}, causing the KV cache size to grow proportionally.
This growth poses constraints on batch size expansion, ultimately limiting throughput. 
Our study reveals that KV cache I/O in \baseline accounts for over 80\% of the total inference time.

Although one might anticipate that combining multiple storage devices through a RAID solution could alleviate this issue, it is frequently hindered by the limited number of PCIe lanes on the host processor.
In this circumstance, we identify that near-storage processing with computational storage devices (CSDs) can successfully address the above issue.
CSDs, which have been studied for decades~\cite{active_disk, idisk, NSC-FPGA, glist, holisticgnn}, have begun to appear as a few commercial products~\cite{netezza, exadata, smartssd}.
The core advantage of CSDs is their internal aggregate bandwidth exploited by the attached computational unit.
By offloading appropriate computations to CSDs, we can harness their aggregated internal bandwidth, which scales linearly with the addition of more CSDs.
With this advantage, we aim to address the I/O bottleneck issue of offloading-based inference with CSDs by moving KV cache-related operations to CSDs.

%ANS
In this paper, we propose \emph{INFerence-INFinity (\thiswork)}, a framework for high-throughput generative inference using near-storage processing devices on a real system. 
We introduce \emph{\ans}, which processes KV cache-related operations (specifically, multi-head self-attention) within a custom accelerator in each CSD.
With \ans, only the computed results are sent back to the host, significantly reducing the total I/O overhead of the KV cache.
Although near-data processing for attention operations has been explored in previous work~\cite{aquabolt,hbmpim,neupims}, \thiswork is the first to implement a near-storage processing system on a real platform using only off-the-shelf components~\cite{falcon,smartssd}, while optimizing dataflow and designing the accelerator in detail.

%\JL{I am not sure if we can say attention near storage itself as a novel scheme as of today. Can we focus a bit more on details for the real impl?}
%WB
To realize \thiswork on a real system for LLM inference,
one key issue is updating the KV cache stored in storage devices. 
Due to storage latency, involving storage write operations in the overall pipeline can severely degrade performance.
To address this, we designed \emph{\wb}, which can hide storage write latency by temporally storing the newly generated KV caches in system memory until the cache reaches sufficient size. 
Instead of waiting for storage write completion before performing multi-head attention with the CSD, we directly forward the temporarily stored data in system memory to the CSD's accelerator memory.
The data in system memory is written to storage only when the write granularity becomes sufficiently large, ensuring efficient storage access without stalling the computation.

%X-Cache
Additionally, considering the benefits of our CSD system, there exists room to further increase throughput by maximizing the utilization of idle resources in our system.
We propose \emph{cooperative \xcache}, a new method that reduces the total data amount to be halved through additional computations in GPU.
The key idea of \xcache is storing the input activations ($X$) before the key-value projection layer, and at every decoding stage, we regenerate the KV cache from $X$ in GPU. 
With \xcache enabled, we can take advantage of cooperative computing between the GPU and CSDs, thereby boosting overall inference throughput.


To the best of our knowledge, \thiswork is the first real-system implementation that utilizes CSDs to accelerate LLM inference.
\thiswork is ready-to-use on PyTorch and implemented only with off-the-shelf hardwares.
Our evaluation demonstrates that \thiswork achieves a maximum speedup of 3.46$\times$ over the existing state-of-the-art.
%\JL{emphasize up to 64K sequences on a 175B model?}

Our contributions can be summarized as follows: 
\begin{itemize} 
    \item We introduce \ans, a system that utilizes CSDs to offload KV cache-related operations to a custom accelerator, effectively addressing the I/O overhead in \baseline. 
    \item We propose \wb, an optimization technique that hides storage write latency when updating newly generated KV caches, improving overall performance. 
    \item We introduce cooperative \xcache, a method that reduces the total data transfer from efficiently utilizing the host memory.
    \item We implement \thiswork on a real system, demonstrating up to 3.46$\times$ throughput improvement over the existing state-of-the-art frameworks.
    \item We will open-source \thiswork to the community. %facilitate broader adoption.
\end{itemize}


\section{Background}
\label{sec:background}


\subsection{Transformer-based LLM Inference}
\label{sec:background1}

\textbf{KV Cache in Generative Inference.}
Conducting LLM generative inference is commonly categorized into two stages: the \emph{prefill} stage and \emph{decoding} stage. 
Initially, at the prefill stage, an input prompt sequence ($X$) is processed through the transformer model~\cite{attention} and generates \emph{key-value (KV) cache}~\cite{scaletrans}, which is utilized and updated for the following decoding stages.
\emph{KV cache} refers to the key and value matrices ($K, V$) generated and stored in previous stages for reuse during the attention operation with the query matrix $Q$ as follows:
\begin{equation}
    Q = X\cdot W_Q; K = X\cdot W_K; V = X\cdot W_V
    \label{eq:qkv}
\end{equation}
\begin{equation}
    Attention(Q,K,V) = softmax( \frac{QK^T}{\sqrt{d}} )V
    \label{eq:attention}
\end{equation}
where $W_{Q},W_{K},W_{V}$ matrices are weight projection matrices.
%to generate the $Q$, $K$, and $V$ matrices.
$d$ denotes the hidden dimension of multi-head attention (MHA).
In the following MLP, the output matrix is once projected by a linear layer ($W_O$) and then applies two additional linear transformations ($W_{MLP}$) with $GeLU$ or $SiLU$~\cite{hendrycks2016gaussian, shoeybi2019megatron, silu}.
%to obtain the final activations of the transformer block.
After the prefill stage, the decoding stage iteratively generates the next single token by performing the attention operation and reusing the KV cache.
The newly generated KV caches for each iteration are appended with the previously generated KV caches.
At every decoding stage, the KV cache from previous stages is recurrently reused 
%without processing the whole input prompt sequence for every decoding stage. 
%Therefore, 
and this can significantly improve the overall LLM inference throughput~\cite{scaletrans,flexgen,infinigen}.
%\JL{why does it start with `however'?}
% Note that there are strict dependencies between generated tokens because each token depends on the previously generated tokens to maintain the sequence's contextual integrity. 
% Therefore, the decoding stage is repeated by the number of output sequence lengths.

%Therefore, the once-generated KV cache for each token is stored and reused in the decoding stage.
%With respect to utilizing the KV cache, a typical generative inference task in LLMs is usually divided into two primary stages: the \emph{prefill stage} and the \emph{decoding stage}.

% \noindent\textbf{Basic operations of a transformer block.} 
% %\SN{assuming batched inference}
% %NeuPIM, (1) Weight projection, (2) Multi-head attention, and (3) MLP.
% A typical transformer block structure~\cite{attention} consists of three fundamental computations: query-key-value weight projection (QKV), multi-head attention (MHA), and computation of multi-layer perceptron (MLP).
% In the QKV, activations ($X$) from the previous block are multiplied with corresponding weight matrices ($W_{i:i\in\{Q,K,V\}}$) to generate Q, K, and V matrices as follows:
% \begin{equation}
%     Q = X\cdot W_Q; K = X\cdot W_K; V = X\cdot W_V
%     \label{eq:qkv}
% \end{equation}
% In the following MHA, we denote the hidden dimension per attention head as $d$.
% The query-key-value activations (Q, K, and V) are used to calculate attention scores as follows:
% \begin{equation}
%     Attention(Q,K,V) = softmax( \frac{QK^T}{\sqrt{d}} )V
%     \label{eq:attention}
% \end{equation}
% In the following MLP, the output matrix is once projected by a linear layer ($W_O$) and then applies two additional linear transformations ($W_{MLP}$) with $GeLU$ or $SiLU$~\cite{hendrycks2016gaussian, shoeybi2019megatron, silu} to obtain the final activations of the transformer block.

% \JL{since there are tons of papers on llm and kv caching, we can merge the two subsubsections and shorten it.}
%An activation function such as  is applied between these two linear transformations.
%Note that we include $W_O$ projection layer in MLP.
%The attention process could be summarized as \cref{eq:attention}, where Q, K, and V are query, key, and value matrices, and $d_{k}$ is the dimension of the key vector.
%From them, the output activations for the next layer are generated. 
%input using the query and key matrices and output the weighted value matrix based on the attention scores of each value.

%\noindent\textbf{Increasing Demand for Long Context Window.}
%\HS{TODO}

\subsection{Offloading-based Batched Inference}
\label{sec:background2}



As described in \cref{sec:intro}, the substantial memory requirements for LLM inference necessitate using dozens of GPUs to keep the entire data in the memory, significantly increasing the system costs. 
A popular low-cost alternative is to utilize system memory and storage as an extension to the GPU memory, a technique known as \emph{offloading-based batched inference}.
\cref{fig:background1} illustrates the procedure of \baseline~\cite{flexgen,deepspeedinf} for a single layer, utilizing secondary storage as an extended GPU memory hierarchy. 
For simplicity, we assume that at least one layer fits within the GPU memory, and the model states in half-precision fit within system memory, with `Host' referring to both the CPU and GPU. 

\begin{figure}[t]
    \centering
        
    \includegraphics[width=\columnwidth]{figures/figure1.pdf}       
    \caption{
    Overall procedure of offloading-based batched inference~\cite{flexgen,deepspeedinf}. 
    (a) At the prefill stage, the KV cache is generated for input prompt sequence and stored in the storage. 
    (b) At the decoding stage, each MHA layer loads the stored KV cache from storage to compute attention scores. 
    }
    \label{fig:background1}
\end{figure}



\textbf{Prefill Procedure.}
%As outlined in \cref{sec:background1}, offloading-based batched inference is divided into two stages: prefill and decoding.
In the prefill stage (\cref{fig:background1}(a)), \circled{1} the GPU initially loads the attention layer weights and the output of the previous layer as input. 
\circled{2} Once the weights are loaded, the inputs are multiplied with them to generate query, key, and value matrices. 
\circled{3} These matrices are used to calculate attention scores for the input prompt sequence via multi-head attention. 
\circled{4} Next, the MLP layer weights are loaded from system memory, 
\circled{5} and the final output activations of the transformer block are generated. 
\circled{6} The generated KV cache is written back to system memory (to storage if memory capacity is exceeded). 
Steps \circled{1} through \circled{6} are repeated for each transformer block, ensuring overlapping data transfers and computations. 
The outputs of the prefill stage are the KV cache for the input prompt sequence and the prediction of the first output token.






\textbf{Decoding Procedure.}
The following decoding phase, illustrated in \cref{fig:background1}(b), proceeds as follows:
First, \bcircled{1} the GPU retrieves the weights of an attention layer and the output of the previous layer as input.
\bcircled{2} The GPU then performs the QKV projection.
\bcircled{3} Unlike in the prefill stage, the stored KV cache for each layer is loaded for multi-head attention (MHA) computation.
\bcircled{4} Note that the MHA computation is performed on the CPU using the loaded KV cache. 
Since MHA primarily involves less computationally intensive General Matrix-Vector Multiplication (GEMV) operations, it can be efficiently handled by the CPU, avoiding the need to transfer the KV cache to the GPU via PCIe interconnect for computation~\cite{flexgen, infinigen}.
\bcircled{5} The generated output activations are transferred back to the GPU from CPU, and the weights ($W_O$, $W_{MLP}$) for the MLP computation are loaded onto the GPU.
Then, \bcircled{6} the GPU processes the MLP computation.
Finally, \bcircled{7} the newly generated KV cache for the newly generated token is written back to the original location of the KV cache for future decoding stages.
This decoding process (\bcircled{1} through \bcircled{7}) is repeated iteratively for each transformer block, with the entire inference process for all layers continuing until the model generates an end-of-sequence token or the number of generated tokens reaches the requested output length.

\textbf{I/O Analysis.}
With \baseline, there is a significant benefit from reducing the total number of weight loads for computation, which scales with the batch size and leads to high throughput. 
However, this method incurs considerable I/O overhead due to the large size of the KV cache.
To quantify the total traffic, let the batch size be represented by $b$, the input sequence length by $s$, the output sequence length by $n$, the number of attention heads by $h$, and the hidden dimension per head by $d$. 
The prefill stage can be considered at iteration $i = 0$, while subsequent stages in the range [$1$, $n$) correspond to the decoding stages.

During the prefill stage, the total KV cache write size in bytes per transformer block is $s \cdot 2\ (\text{K and V}) \cdot b \cdot h \cdot d \cdot 2 \ (\text{FP16})$.
In the decoding stage, the read traffic in bytes per transformer block for attention operations is $(s + i - 1) \cdot 2 \ (\text{K and V}) \cdot b \cdot h \cdot d  \cdot 2 \ (\text{FP16})$.
The write traffic in bytes per transformer block, for updating the KV cache, is $2 \ (\text{K and V}) \cdot b \cdot h \cdot d  \cdot 2 \ (\text{FP16})$.
These data transfers pass through the system interconnect (PCIe), as illustrated by the {\color{red}red} arrows in \cref{fig:background1}.
Our work, \thiswork, aims to minimize such data transfer over the system interconnect by placing accelerators within CSDs, while addressing key system integration challenges for high throughput LLM inference.

\subsection{Computational Storage Devices}
\label{sec:background4}

\begin{figure}[t]
    \centering
        
    \includegraphics[width=\columnwidth]{figures/figure2.pdf}       
    \caption{
    (a) An example of PCIe hierarchy with standard storage devices (b) An example of an environment with computational storage devices (e.g. SmartSSD~\cite{smartssd}). 
    }
    \label{fig:background2}
\end{figure}

Computational storage devices (CSDs)~\cite{active_disk, idisk, smartsage, omnicache}, or near-storage processing~\cite{active_disk, omnicache, NSC-FPGA, nearpm}, have been explored extensively over the decades.    
By placing a computational unit close to storage devices, workloads can benefit from reduced latency, decreased bandwidth consumption, and less computational pressure on the host.
Various types of CSDs that recently started to appear on the market~\cite{netezza, exadata, smartssd} share two common features.
First, a light-weight accelerator is placed near storage. 
Second, there exists a private inner path between the storage device and the attached accelerator.
%We use SmartSSD~\cite{smartssd}, a representative commercially available CSD that incorporates a lightweight field programmable gate array (FPGA) as an accelerator and an internal PCIe switch as a private inner path.

%A comparison between two host environments, one equipped with standard storage devices and the other with CSDs, is shown in \cref{fig:background2}. 
\cref{fig:background2}(a) depicts an example of PCIe topology with common storage devices. 
In contrast, \cref{fig:background2}(b) illustrates an environment featuring CSDs, which introduces a key distinction: an internal PCIe switch between the storage device and the attached accelerator, which provides a private pathway for direct peer-to-peer (P2P) communication~\cite{spin, nvmmu} between the NVMe device and the accelerator, eliminating redundant data traffic to and from the host over the system interconnect~\cite{nascent,nascent2,smartinfinity}. 
A single CSD does not inherently increase bandwidth, as both SSD-to-FPGA and SSD-to-host transfers occur over sufficient PCIe lanes. However, when multiple CSDs are deployed, the combined internal bandwidth scales linearly with the number of CSDs, while the bandwidth of the shared system interconnect remains constant. 
Nonetheless, adding multiple CSDs does not automatically result in speedup. 
Achieving high performance requires careful consideration of several key factors: (1) determining what computations should be offloaded to the CSD (\cref{sec:scheme1}), (2) effectively hiding storage write latency (\cref{sec:scheme2}), and (3) effectively scheduling the CSD with the host (\cref{sec:scheme3}).


\section{Motivational Study}
\label{sec:motiv}

\begin{figure}[t]
    \centering
        
    \includegraphics[width=\columnwidth]{graphs/motiv.pdf}       
    \caption{
    A motivational study. $s$ represents the LLM input context size, and $B$ denotes the model parameter count. 
    (a) Memory consumption breakdown during \baseline.
    (b) The main bottleneck in offloading-based LLM inference is KV cache traffic.
    (c) Scaling storage devices with RAID0 fails to improve performance due to the bandwidth limits of the shared PCIe interconnect.
    } 
    \label{fig:motiv}
\end{figure}

In this section, we analyze the main bottleneck of \baseline through a motivational study and examine the existing challenges. 
For details on the experimental setup, please refer to \cref{sec:environment}. 
\cref{fig:motiv}(a) presents the breakdown of the required data for conducting \baseline, where we categorized memory consumption into three parts: KV cache, LLM weight, and others (e.g., activations).
Notably, the KV cache for long-context-length LLMs requires terabyte-scale memory, making the use of storage devices necessary in resource-limited scenarios.
As a result, as shown in \cref{fig:motiv}(b), which provides the breakdown of time with a state-of-the-art baseline~\cite{flexgen}.
More than 80\% of the total time is spent on KV cache-related I/O when storage is used.
This data transfer overhead increases significantly with larger models and longer context lengths~\cite{bigbird, longformer, infinitellm, gear}, making it a critical challenge in \baseline.

One straightforward approach to alleviating the data transfer overhead of storage devices is to increase storage bandwidth using a RAID solution. 
\cref{fig:motiv}(c) demonstrates the normalized speedup of \baseline when scaling the number of storage devices using RAID0. 
Unfortunately, the \baseline with RAID0 experiences a bottleneck due to the traffic of the KV cache on the shared system interconnect, which becomes a bottleneck when using more than three SSDs. 
Moreover, KV cache-related I/Os are predominantly read operations, which generally provide higher bandwidth than write operations in modern storage devices.
Thus, utilizing the RAID solution for \baseline has its limitations.

The motivational study above shows that the data transfer overhead issue of the KV cache cannot be easily addressed due to the limited shared system interconnects of the existing system structure. 
%Therefore, we propose to employ CSDs as a solution. 
On the other hand, properly using CSDs can mitigate the bottleneck as they can provide more internal bandwidth than the system interconnects.
Our primary objective is to minimize the data transfer between storage devices and host memory by leveraging the aggregated bandwidth from the direct path inside each CSD.

\section{INFerence-INFinity (INF$^2$)}
\subsection{ANS: Attention-Near Storage using CSD}
\label{sec:scheme1}

\begin{figure}[t]
    \centering
        
    \includegraphics[width=\columnwidth]{figures/figure4.pdf}       
    \caption{
    The overall procedure of the decoding stage for (a) the baseline and (b) \Ans.
    In the baseline, the KV cache is loaded from storage to the host through the system interconnect.
    With \Ans, the KV cache is loaded from storage directly to our custom accelerator via an internal PCIe switch.
    }
    \label{fig:ans}
\end{figure}

As discussed in \cref{sec:motiv}, the volume of KV cache communication during the decoding stage is substantial in the \baseline.
With traditional storage devices, this communication must traverse the shared system interconnect to perform attention operations on the CPU.
To mitigate this bottleneck, we propose \emph{\ans} (ANS), a novel system that shifts attention computation to a custom accelerator within computational storage devices (CSDs).
This approach enables KV cache read traffic to leverage the fast aggregated bandwidth of CSDs, while only the input and output of the multi-head attention pass through the system interconnect.

At the prefill stage, the initially generated KV cache is written to the storage device within the CSDs as in the baseline (\cref{fig:background1}(a)). 
The proposed ANS differs from the baseline (\cref{fig:ans}(a)) in the decoding stage, as illustrated in \cref{fig:ans}(b).
\circled{1} The required attention layer weights and input activations are loaded into the GPU from system memory, similar to the baseline.  
\circled{2} The necessary KV cache is directly loaded from each storage device to the attached accelerator via direct P2P communication through the private PCIe switch within each CSD.  
\circled{3} The host generates the query, key, and value vectors from the QKV projection and loads them into the memory of each accelerator within the CSDs.  
\circled{4} Once the accelerator completes the multi-head attention (MHA) computation, the output vectors are transferred back to the host.  
\circled{5} The GPU then proceeds with the MLP computation.

By computing the MHA layer close to the storage using \ans, each accelerator can directly access the KV cache via the private path within the CSD, significantly reducing read data traffic through the system interconnect. 
When quantifying the data traffic at the first iteration of the decoding stage, \ans significantly reduces the read traffic through the system interconnect from $s \cdot 2\ (\text{K and V}) \cdot b \cdot h \cdot d \cdot 2$ bytes in the baseline for reading the KV cache to $b \cdot h \cdot d \cdot 2$ bytes for reading the new attention score. 
The write traffic through the system interconnect slightly increases from $2\ (\text{K and V}) \cdot b \cdot h \cdot d \cdot 2$ bytes for writing the new KV entry to $3\ (\text{Q, K, and V}) \cdot b \cdot h \cdot d \cdot 2$ bytes for sending the new Q, K, and V entry.
Overall, the total traffic is significantly reduced. % compared to the values discussed in the I/O analysis in \cref{sec:background2}.
This is because most of the KV cache read traffic is handled via the internal bandwidth of the CSDs, with only the output vectors from the accelerator being transferred through the system interconnect.


When ANS is implemented with a single CSD, the bottleneck simply shifts from the system interconnect to the CSD's internal switch. 
However, the true benefit becomes evident as the number of CSDs increases. As more CSDs are added, the aggregate bandwidth between the FPGA and SSD scales linearly, while the bandwidth to the host via the system interconnect (PCIe) remains constant. 
While there is a minor benefit from the increased computational capacity of more FPGAs, the main driver of the speedup in \thiswork comes from leveraging the aggregated internal bandwidth.

We can quantify that \ans reduces the data traffic through the system interconnect by a factor proportional to the sequence length $s$ when assuming a sufficient number of CSDs.
Let $T_{BASE}$ be the traffic with the baseline and $T_{ANS}$ be the traffic with \ans.

\begin{equation}
%\begin{split}
    \frac{T_{BASE}}{T_{ANS}} = \frac{4sbhd + 4bhd}{2bhd + 6bhd} 
    =\frac{s+1}{2} > 1\,(\because s > 1).
%\end{split}
\end{equation}
%How the newly generated key and value vectors are stored back into the existing KV cache in each storage device is detailed in \cref{sec:scheme2}.
%
\begin{sloppypar}
To utilize multiple CSDs, we parallelize along both the batch dimension and the head dimension in MHA.
In \baseline scenarios, batch sizes are typically large to minimize LLM weight transfer overhead. 
Since the total available parallelism is the product of the batch size and the number of attention heads, there are sufficient dimensions to effectively distribute the workload across the CSDs.
As a result, during the decoding stages, MHA computation is performed entirely through intra-CSD communication between the attached FPGA and SSD, eliminating the need for inter-CSD communication.
%For that, in the prefill stage, when the KV cache is generated, we initially distribute the generated KV cache across the CSDs in a round-robin manner using these parallel dimensions.
To enable this, we distribute the KV cache generated in the prefill stage across the CSDs using these parallel dimensions.
%Note that these write operations are not urgent, as the generated KV cache is used in the subsequent decoding stage.
The KV cache is initially stored in an $s \times d$ array format, with preallocated space for the newly generated KV cache based on the provided maximum sequence length ($n$).
This allows the newly generated KV cache to be appended to the existing cache sequentially in $d$-byte chunks.
\end{sloppypar}
%\JL{\textrightarrow Let's put KV orientation + padding here, along with transpose in FPGA}
%TODO: We experiment sensitivity for how different parallelism applied to CSDs in @@. 

\subsection{\WB}
\label{sec:scheme2}
\begin{figure}[t]
    \centering
        
    \includegraphics[width=\columnwidth]{figures/fig5_writeback.pdf}       
    \caption{
     KV cache writeback procedure. (a) Naive decode stage with \ans. (b) Decode stage with \wb.
    }
    \label{fig:wb}
\end{figure}

%\HS{TODO: GPU memory optimization? }


%Intro
%\Ans offers a new opportunity by leveraging the aggregated bandwidth of CSDs. 
One critical challenge from \ans is that the newly generated key and value vectors from each decoding iteration must be written back into the KV cache within each storage device. 
Because the FPGA has to reread it for the MHA, the long latency quickly becomes the system bottleneck.
Furthermore, parallelizing along the head dimension yields small write granularity, which is inefficient for SSDs.


To address this issue, we propose \emph{\wb}. 
In the naive way with ANS in \cref{fig:wb}(a), a new KV entry is first written to the KV cache in the SSD. Then, the grown KV cache is transferred to the in-CSD FPGA for the attention operation.
In this method, the SSD write is on the critical path. Furthermore, a single KV entry is far smaller than the SSD page size, harming the write performance.

With \wb shown in \cref{fig:wb}(b), 
instead of writing each newly generated KV cache entry directly to the storage device, 
We maintain dedicated buffers for the KV caches in the host memory, and let the buffer directly feed the FPGAs in the CSD.
The existing KV cache is provided by the SSD.
This optimization takes SSD write latency off the critical path. 
%, and the SSDs provide the KV cache values to the FPGA in subsequent steps. 
%we adopt a more practical approach in ANS by caching the vectors in the system memory for multiple steps, delaying the write until they reach a sufficient size. 
%\bcircled{1}, \HS{TODO}.
%Specifically, we manage the newly generated KV cache in system memory, holding them until their size becomes sufficient for a more efficient write operation. 
%For example, when using a write cycle of size $c$, 
After the predefined interval of $c$ decoding iterations, they are spilled to the storage device in bulk, off the critical path. % with large enough write granularity.
%This , where the storage will feed the vectors to the FPGA afterwards.

Through \wb, we achieve several benefits: 1) storage write latency is effectively hidden, and 2) the total data written to storage is handled in larger chunks, improving the efficiency of the storage device. 
A small downside is that the same KV cache values are redundantly sent through the system interconnect from host memory to the FPGAs until they are spilled. 
However, we found the benefits to be much larger than the small drawbacks in practice.
%Because of this, there exists a trade-off between the write latency and redundant system traffic according to the value of $c$. %, which we quantify in \cref{discussion:storage}.
%We further discuss these benefits in \cref{discussion:storage}.

\subsection{Cooperative X-cache Optimization}
\label{sec:scheme3}
\begin{figure}[t]
    \centering
        
    \includegraphics[width=\columnwidth]{figures/fig6_xcache_ver7.pdf}       
    \caption{
    Cooperative computing procedure with (a) KV-cache and (b) \xcache.
    }
    \label{fig:xcache}
\end{figure}

%Cooperative Computing
%ANS effectively shifts the host-to-storage KV cache traffic to the internal interconnects of the CSDs, significantly reducing I/O overhead. 
%However, this still does not fully utilize the potential throughput improvements offered by the computational capabilities of CSDs.
Applying ANS and \wb ensures the CPU memory footprint remains constant, leaving additional room for cooperative computing between the host and CSDs.
One straightforward method is to store a portion of the KV cache in the host memory and let the CPU perform MHA, as shown in \cref{fig:xcache}(a). 
This significantly reduces the traffic through the system interconnect. %, enhancing the performance. 

To achieve even higher performance, we further propose cooperative \xcache as shown in \cref{fig:xcache}(b). %, a novel approach for achieving cooperative computing with CSDs.
According to \cref{eq:qkv}, the KV cache is calculated by $K = X\cdot W_{K}$ and $V = X\cdot W_{V}$.
From this, we choose to cache $X$ in the host memory instead of KV, which we denote as \xcache.
Because \xcache occupies only half the memory compared to KV cache, this has the effect of reducing the system interconnect traffic twice more than the CPU performing MHA with the same memory budget.
%Instead of storing the key and value (KV) caches, we choose to store only the input activations (X) before the QKV projection. 
The additional overhead is in regenerating K and V which involves recomputing the QKV projection on the GPU. 
However, we find that this overhead is almost negligible compared to the reduced data. 
Compared to the KV cache-based cooperation, \xcache also slightly benefits from letting the CPU focus on I/O management~\cite{bam, dongle, gmt} to draw maximum bandwidth. 


While our approach is straightforward, it yields performance improvements by significantly reducing the size of intermediate data and minimizing idle time for computing components.
Contrary to existing offloading systems, \xcache gains more performance as more memory is added to the system, which is quantified in \cref{exp:sensi}.

%We further discuss the implications and benefits of this strategy in \cref{exp:sensi}. 
%\JL{do we? or is it just mem capacity-perf sensi?}


\section{Accelerator Microarchitecture}
\label{sec:arch}

\begin{figure}[t]
    \centering
        
    \includegraphics[width=\columnwidth]{figures/fig7_accel_ver8.pdf}       
    \caption{
    Microarchitecture of custom accelerator for KV cache-related operations.
    }
    \label{fig:arch}
\end{figure}


\cref{fig:arch} illustrates the hardware accelerator architecture implemented in the FPGA of the CSD to support \thiswork.
%While \thiswork focuses primarily on the most widely used transformer-based models~\cite{opt,gpt2,gpt3,llama2}, the accelerator can be designed to support various other model variants based on user requirements~\cite{gqa}.
For multi-head attention~\cite{attention} implementation, the accelerator consists of two primary modules: \emph{GEMV} and \emph{softmax}. 
The \emph{GEMV} module handles the necessary linear transformations and optionally transposition, while the \emph{softmax} module normalizes the attention scores, enabling the correct weighting of different input features.

%All the operations described can be executed multiple times for $bs \cdot h$, allowing the accelerator to manage larger models and accommodate increased batch sizes efficiently with limited accelerator DRAM capacity.
%\HS{TODO: Memory consumption of CSD}%For example, even with the OPT-175B and @@.

%Transpose Optimization
The GEMV blocks are used for multiplying $Q$ with $K^T$ and then multiplying the softmax results with $V$. 
For this, each GEMV block comprises 32 MAC units to perform inner products.
Because we need to multiply $K^T$, we provide an optional transpose module to perform the necessary transpose for the key matrix ($K$).
An alternative would be storing $K$ in a transposed format in the SSD, but the number of rows of $K$ continuously grows each iteration, and storing this in a transposed format makes the growth very difficult to manage.
%Notably, this transpose operation does not induce additional read/write cycles between the FPGA and DRAM. 
To support the transpose without extra read/write to the accelerator DRAM, 
we load $32 \times 32$ FP16 values to the FPGA BRAMs and perform in-place transposition.
Then the MAC units perform blocked GEMV, multiplying a 32-element vector to a $32 \times 32$ matrix and accumulating results to the output activation buffer.
From this, the same GEMV block can be reused for the two GEMV operations (for $K^T$ and $V$) with a few multiplexers.
This optimization significantly reduces resource utilization for our accelerator, which is crucial for the lightweight FPGAs in CSDs.
Once the results are computed across the four GEMV blocks, they are stored back into the accelerator memory for further processing.

The softmax module operates in three main steps: (1) masking while finding the maximum value, (2) computing the exponential sum, and (3) performing division. 
This multi-step approach is necessary due to the inherent reduction dependency in the softmax operation~\cite{flash_attn1,flash_attn2,ring_attn,blockwise_attn}.
First, the masking module loads 128 data elements into four buffers each cycle and overwrites the masked values to a negative constant $-1e4$ based on masking information received from the host (e.g., padding or current decoding stage iteration). 
To find the maximum value, which scales the exponential for numerical stability, a hierarchical reduction approach is used. 
Four parallel units compute the maximum value within each group of 32 elements. 
Once all elements have been processed, the four intermediate maximum values are reduced to a final maximum using a two-step tree structure. 
%The masked data are written back to accelerator memory at this stage.
Second, the module then computes the exponential values from the masked data ($32 \times 4$). % from the accelerator memory to the FPGA and computes the exponential values. 
The sum reduction follows the same hierarchical reduction approach used in the previous step. 
FP16 is used for accumulation, ensuring numerical stability while offering sufficient precision. 
%The computed exponential values are stored back into the accelerator memory.
Finally, the module reads the stored exponential values and divides each by the sum value from the previous step. 
The results are then stored back in accelerator memory for the next GEMV operation.

\begin{comment}
\subsection{Multi-Query Support}
Llama-3.1
Note that the updater can be extended to other variant of transformers  because the most use variations of attention operation. We further implemented and tested other attention in Section @@.

\subsection{Accelerator Architecture: Quantization}
Note that the compressor can be extended to other algorithm such as AWQ, LLM.int8 because most quantizer use gruoped compressor.
We further implemented and tested other compression methods in @@.

\subsection{Accelerator Architecture: Sparsification}
Note that the compressor can be extended to other algorithm such as H2O, infiniGen  because most sparsifcation use general compressor.
We further implemented and tested other sparsification methods in @@.
\end{comment}


\section{Implementation}
\begin{figure}[t]
    \centering
        
    \includegraphics[width=\columnwidth]{figures/fig8_impl_rev.pdf}       
    \caption{
    Overview of \thiswork, which is a ready-to-use framework. Users can perform their own LLM model's inference procedures with \thiswork system. 
    }
    \label{fig:overview}
\end{figure}


%This design reduces overhead and allows seamless integration of ANS into existing host code, while also enabling highly parallelized storage write access.
% \thiswork is ready to use on PyTorch and can be easily adapted to various models. 
% We describe the three main components of \thiswork displayed in \cref{fig:overview}, with respect to the workflow for users.

\textbf{Bitstream Generator}. 
The procedure to generate bitstream is described on the left side of \cref{fig:overview}.
The pre-compiled device binary codes are sufficient to execute \thiswork without any synthesis.
If customization is needed, our synthesizable device codes can be customized via modifying high-level synthesis (HLS) codes~\cite{intro_vlsi,vitis_hls}.
Customizing the accelerator can be challenging, even with the synthesizable HLS template and helper tools~\cite{tapa,autobridge}.
To ease the difficulties, we provide a C/C++-based simulator integrated into the LLM codes in PyTorch to check the functionality of the accelerator.
%If customization is unnecessary, pre-compiled device binary codes are sufficient to execute \thiswork without any synthesis.
%Designing the accelerator can be challenging, even with the synthesizable HLS template and helper tools~\cite{tapa,autobridge}. 
%of synthesizing the device binary, it remains a significant burden to users. 
% Moreover, the problems are exacerbated when debugging with the PyTorch LLM codes with mixed precision. 
%Because of this, we provide a C/C++-based simulator integrated into the LLM codes in PyTorch to check the functionality of the designed accelerator, thus facilitating the logic design process.


\textbf{\thiswork System}. 
\thiswork system is described in the middle side of \cref{fig:overview}.
The \thiswork system consists of \ans (\cref{sec:scheme1}) and our custom accelerator (\cref{sec:arch}, \cref{exp:impl}), utilizing a general PCIe expansion system~\cite{smartinfinity, bam, falcon, helios} to equip multiple CSDs with limited PCIe lanes in the host.
While this solution does not increase the raw link bandwidth to the host, \thiswork leverages the internal bandwidth of CSDs to mitigate the high I/O demands.
To enable direct P2P communication~\cite{spin, nvmmu} between the NVMe SSD and FPGA, we allocate a special Xilinx OpenCL extension buffer~\cite{xilinx_opencl_buf} in the device memory, which facilitates interaction with the attached storage device at runtime.
When writing data to the storage device, it is crucial to identify which FPGA is directly connected to the specific SSD via its internal PCIe switch. 
To simplify this process, \thiswork includes automatic identification of the PCIe topology, matching each SSD with its corresponding FPGA.
Once matched, standard Linux system calls (\texttt{pread/pwrite}) can be used for direct P2P data transfer between the SSD and FPGA. %, leveraging the internal P2P communication feature of SmartSSD~\cite{smartssd}.



\textbf{\thiswork Scheduler}.
The host program consists of a scheduler responsible for managing the operations of \wb (\cref{sec:scheme2}) and \xcache (\cref{sec:scheme3}).
The scheduler is implemented in C++ and communicates directly with the PyTorch application using pybind11~\cite{pybind11}.
%, a lightweight library that allows C++ to interact with Python types, such as \texttt{torch.Tensor}.
% \thiswork focuses primarily on replacing the multi-head attention layer without affecting other transformer operations.
Our module is provided as a callable function, utilizing distutils~\cite{disutils} to build C/C++ implementations as additional Python modules, with automatic compilation initiated when the engine starts. 
% Since decoder-only models share a common multi-head attention mechanism, this approach can be extended to other models with minimal modifications, making \thiswork a practical and powerful framework.

The detailed procedure of the scheduler is described in the right side of \cref{fig:overview}.
First, the scheduler estimates the memory footprint required for performing \baseline and allocates portions to CSDs and for \xcache accordingly.
%Workload distribution is based on the LLM configuration, including the number of attention heads and the requested total batch size.
Second, following the workload distribution, the scheduler executes the prefill stage, storing the KV cache in the appropriate locations.
Third, during the decoding stage, execution occurs in parallel according to the workload distribution.
%Newly generated KV cache entries are temporarily cached in host memory in fixed cycles, as outlined in \cref{sec:scheme2}.
%Each queue maintains the newly generated KV cache for each layer's batch. 
When delayed writeback buffers (\cref{sec:scheme2}) reach the designated threshold, the scheduler initiates a write operation to the SSDs independently from the LLM inference pipeline.
%In detail, 
To ensure data is correctly stored, direct I/O must be used for CSD applications. 
Because the newly generated KV cache entries are smaller (sized at 256B~\cite{opt, llama, llama2, gpt2}) than the minimum write granularity for direct I/O (512B), using a spill interval $c$ larger than 1 for \wb is recommended.






\section{Evaluation}

\subsection{Experimental Setup}
\label{sec:environment}
\input{tables/setup}

Our experimental setup is detailed in \cref{tab:setup}. 
% Hardware
We implemented and evaluated \thiswork on a system equipped with 16 Samsung SmartSSDs~\cite{smartssd}, connected to the host via a PCIe expansion~\cite{falcon} to increase the system's physical slots. 
%Additional configuration details are discussed in \cref{discussion:expansion}. 
Each SmartSSD includes a 4TB NVMe SSD and a Kintex UltraScale+ KU15P FPGA with 522K LUTs, 984 BRAMs, 128 URAMs, 1968 DSPs, and 4GB of DDR4-2400 DRAM. 
The SSD can directly communicates with the attached FPGA through a PCIe 3.0$\times$4 lanes.
Our setup also includes either an NVIDIA A100 or H100 GPU, both widely used for LLM inference, connected to the CPU via PCIe 4.0$\times$16 lanes.





%Baselines
For the existing baselines described in \cref{sec:background2}, we used \texttt{FlexGen}~\cite{flexgen} (\textbf{FLEX}) as the state-of-the-art framework for offloading-based batched inference.
%, as it outperforms other baselines.
%FlexGen supports KV cache offloading to storage via a memory-mapped array implementation, triggering storage traffic when data exceeds main memory capacity.
Additionally, we evaluated \texttt{DeepSpeed}~\cite{mii} (\textbf{DS}) with the ZeRO-Inference~\cite{deepspeedinf} option enabled, which supports KV cache offloading to main memory and weight offloading to storage.
Since DeepSpeed does not support KV cache offloading to storage, we used swap memory for evaluation.
For both FlexGen and DeepSpeed, we enabled the option to perform attention computation on the CPU.
We also considered \texttt{Accelerate}~\cite{huggingface} (\textbf{HF}) as a baseline.
% While both DeepSpeed and Accelerate offer support a variety of LLMs, they struggled to effectively support long-context LLMs, leading to GPU OOM issues, we further discussed in \cref{exp:baselines}.
All baselines were set up with software RAID0 using Linux \texttt{mdadm}~\cite{mdadm}, and for a fair comparison, we used the NVMe SSD of the SmartSSD in all tests.



% Hyperparameters
For our work (\textbf{INF2}), we prioritized parallelization along the batch dimension, and then the attention heads.
The default storage spill interval $c$, for the delayed writeback buffers, was set to two.
We evaluated two representative decoder-only models, OPT~\cite{opt} and LLaMA-2~\cite{llama2}, in FP16.
% If not specified, OPT was used as the default model, as it supports a wide range of model parameter variations.
We used a default batch size of 32, as our goal was to improve the throughput of \baseline.
Only when the model weights could not fit into GPU memory, they were stored in main memory, and the default output sequence length was set to 64.
To measure the effect of different system memory budgets, we used the \texttt{systemd} command to adjust memory allocation, with 512GB as the default.


\input{tables/imple_result}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/floorplan.pdf}       
    \caption{ 
    (a) Proposed system for \thiswork.
    (b) Resource layout of the implemented accelerator (\cref{sec:arch}) on SAMSUNG SmartSSD~\cite{smartssd}.
    Only the user logic partition (ULP) is shown, excluding the base logic partition (BLP) region, which is typically provided by the vendor.
    }
    \label{fig:imple_result}
\end{figure}
%Although both models natively support up to 4K context lengths, we slightly scaled the dimesion of input embedding for OPT and adjusted the hyperparameters of the RoPE~\cite{rope} for LLaMa-2, we further this discussed in \cref{discussion:long}.

\subsection{Implementation Results}
\label{exp:impl}

Our accelerator, described in \cref{sec:arch}, is implemented on a real system using off-the-shelf components, as shown in \cref{fig:imple_result}(a).
The setup includes 8$\times$ PCIe 4.0 x8 lanes, with each x8 lane connecting two CSDs via 2$\times$U.2 to PCIe connector. 
Each CSD operates at PCIe 3.0 x4 lanes speeds, as the connection speed is matched to the lower generation.

\cref{fig:imple_result}(b) illustrates the final resource layout of our accelerator implemented on the FPGA of the SmartSSD~\cite{smartssd}.
In the layout, the attention logic consumes a significant portion of the area, particularly the softmax logic, which involves computationally expensive exponential operations using DSPs.
\cref{tab:imple_result} details the resource utilization of our accelerator implementation on the FPGA.
%The multi-head attention implementation consumes approximately 47.36\% of LUTs, 32.72\% of FFs, 47.10\% of BRAM, 9.38\% of URAM, and 13.26\% of DSP resources.
Note that there is still available logic on the lightweight FPGA~\cite{kintex}, which may allow for additional operations to support other variations~\cite{gqa}.
The total on-chip power consumption is 14.21W.
Thus, even with 16 CSDs utilized (227.36W), the power consumption is comparable to that of a single GPU, which typically requires several hundreds of Watts.





\subsection{Performance Comparison}
\label{exp:main}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/main.pdf}       
    \caption{
    Throughput and speedup of our approach (\thiswork) compared to baselines, using an A100 GPU.
    \texttt{DS+UVM} represents the results with DeepSpeed~\cite{mii} with UVM~\cite{uvm} enabled, while \texttt{FLEX} refers to the results of FlexGen~\cite{flexgen}.
    }
    \label{fig:main}
\end{figure}


We measured the throughput of both baselines (FLEX, DS) and \thiswork in \cref{fig:main}, using an A100 GPU with model sizes from 30B to 175B parameters.
Due to GPU OOM issues in DS with long context lengths during the prefill stage, we enabled unified virtual memory (UVM)~\cite{uvm} for measurement.
Both \thiswork equipped with 16$\times$CSDs and the baseline were tested with the same number of storage devices.

DS suffered more than 5$\times$ throughput loss compared to FlexGen due to UVM overhead, so the speedup was normalized to FlexGen results.
\thiswork consistently showed more than 2$\times$ speedup across all setups, with 2.67$\times$ for 30B model and 2.3$\times$ for 175B model with 16K LLM context length.
The slightly less speedup for the 175B model is due to its larger memory footprint of model weights stored in the main memory (over 300GB), leaving less room for \xcache.
%For the 30B model, \thiswork achieved better throughput as \xcache handled more workload, avoiding storage traffic.
%Overall, \thiswork achieved 2.15$\times$-2.67$\times$ speedups across all setups.
%Computational overhead increased for longer sequences, limiting \xcache usage.

\subsection{ Ablation Study Results }
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{graphs/ablation.pdf}       
    \caption{
    Ablation study of \thiswork using 16 CSDs with batch size of 32 and 16K cont.
    % We used 16 CSDs with batch size of 32 and a 16K context length for this experiment.
    \texttt{ANS} indicates \ans, \texttt{ANS+X} means ANS with \xcache enabled, \texttt{ANS+WB} refers to ANS with \wb enabled, and \texttt{ANS+WB+X} indicates applying all the proposed methods. 
    }
    \label{fig:ablation}
\end{figure}

Using the setup only with our first technique (\ans) as the baseline (ANS), we evaluated the throughput improvement when the proposed schemes were applied, as shown in \cref{fig:ablation}.
When \xcache is enabled (ANS+X), we observed a 1.07$\times$ to 1.19$\times$ speedup compared to ANS alone.
The speedup is more significant with the H100 and the 30B model, as \xcache benefits from the performance of GPU and the larger available main memory.
When \wb is enabled (ANS+WB), we observed a 1.10$\times$ to 1.12$\times$ speedup over ANS, highlighting the importance of hiding storage write latency in CSD applications.
When all proposed techniques are applied (ANS+WB+X), the speedup ranges from 1.1$\times$ to 1.57$\times$ compared to ANS alone.
%The speedup margin is particularly higher in the 30B model than in the 66B model, likely due to the total KV cache size with the 30B (\textasciitilde 600GB) being reduced to half (\textasciitilde 300GB) when \xcache is enabled, allowing most of the KV cache to fit into main memory.



\subsection{ Sensitivity Test Results }
\label{exp:sensi}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/weight_batch_sensi.pdf}       
    \caption{
    Sensitivity study to batch size ($b$) and model residency ratio to SSD (\%) using 66B model with a 16K context length on an A100 GPU.
    100\% indicates that the model weights are entirely stored on the SSD, while 0\% indicates that the weights are fully held in main memory. 
    }
    \label{fig:weight_batch_sensi}
\end{figure}




%\subsection{Output Length + Prefill breakdown}


\textbf{Batch Size and Model Residency Ratio.}
Both the state-of-the-art baseline and \thiswork support model weight offloading to SSD, which helps reduce the main memory burden for storing model weights.
%Therefore, it is meaningful to discuss sensitivity based on batch size and model residency ratio in SSD.
In \cref{fig:weight_batch_sensi}, 100\% indicates that the model weights are fully stored on the SSD, while 0\% means the weights are entirely held in main memory.
As expected, when the batch size is 1, the KV cache can fit into main memory, resulting in no storage traffic for both the baseline and \thiswork.
However, with a batch size of 1, there is a significant throughput loss due to the overhead of loading weights, especially when the model weights are fully stored on the SSDs (100\%).
When the batch size is increased, FlexGen only succeeds in increasing throughput for the results of 100\%, but fails to improve throughput in the other two cases (0\%, 50\%). 
%This is because increasing the batch size generates storage traffic for the KV cache.
In contrast, \thiswork successfully improves throughput across all setups.
Interestingly, as the weight ratio in SSD increases, throughput of \thiswork also improves.
This is because offloading model weights to SSD frees up main memory, allowing \xcache to be applied to a larger portion of the workload.
This ability to achieve higher throughput while storing weights on SSD is a significant advantage, especially as model sizes grow, making \thiswork highly scalable for larger models.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{graphs/cpu_budget.pdf}       
    \caption{
    Sensitivity study to different main memory budgets. H100 GPU and a 32K context length are used.
    % We utilized an H100 GPU and a 32K context length for this experiment to assess sensitivity to different main memory budgets.
    }
    \label{fig:cpu_budget}
\end{figure}


\textbf{Memory Budget.}
\thiswork benefits from fully utilizing main memory space through \xcache, making it valuable to discuss the results based on different memory budgets.
In \cref{fig:cpu_budget}, although the throughput of the baseline~\cite{flexgen} improves with more main memory, \thiswork consistently achieves a 2.26$\times$-3.18$\times$ speedup across various memory budgets ranging from 256GB to 1TB.
This is because \xcache allows \thiswork to fully utilize the increased main memory space.
For larger models, the improvement with the baseline, ranging from 256GB (0.032 token/sec) to 1TB (0.034 token/sec), indicates that simply increasing main memory may not be the ultimate solution when KV caches exceed the CPU memory budget significantly.
In contrast, \thiswork achieves a 2.26$\times$ speedup over the baseline with 175B model.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{graphs/output_sensi.pdf}       
    \caption{
    Breakdown of total execution time based on output length ($o$) and model size ($B$).
    For this experiment, a 16K context length and an H100 GPU were utilized.
    }
    \label{fig:output_sensi}
\end{figure}

\textbf{Output Sequence Length.}
\cref{fig:output_sensi} shows the performance sensitivity to output length ($o$) and model size ($B$). 
As described in \cref{sec:scheme1}, we preallocate space in storage at prefill stage for the newly generated KV cache based on the maximum output length, making it important to assess performance sensitivity to varying output lengths.
The results indicate that increasing the length of the generated sequence rather improves the speedup compared to the baseline, with speedups of up to 3.46$\times$.
This is because the prefill stage latency remains constant, while the proportion of time spent on the decoding stage increases.
This effectively compensates for any additional traffic from preallocated storage space.
As a result, \thiswork consistently provides performance improvements regardless of the generated sequence length, making it particularly well-suited for generating long outputs.


\subsection{Comparison with General Frameworks}
\label{exp:baselines}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{graphs/baselines.pdf}       
    \caption{
    Performance analysis (a) with shorter context length ($s$) comparing existing frameworks (DS refers to DeepSpeed with ZeRO-Inference~\cite{deepspeedinf} enabled, HF refers to Hugging Face Accelerate~\cite{huggingface}) and (b) model sensitivity on Llama-2~\cite{llama2} with long context lengths. 
    }
    \label{fig:baselines}
\end{figure}

In \cref{fig:baselines}, we provide comparisons with additional baselines of DeepSpeed~\cite{deepspeedinf, mii} without UVM (DS) and Accelerate~\cite{huggingface}.
Because they faced OOM issues for our main performance comparison, we conducted experiments using far shorter context lengths and smaller models.
The experiments were conducted using an H100 GPU with a 1TB main memory budget.
Accelerate (HF) fails to support the 175B model, while other baselines like DeepSpeed (DS) and FlexGen are able to provide support with performance comparable to our approach.
This outcome is expected, because the KV cache size is relatively small (less than 100GB) for short sequences, which fits entirely into system memory.
As a result, no storage traffic is required, and thus no significant speedup can be observed under these conditions.
%This is primarily because Accelerate (HF) does not support computing attention on the CPU, and only supports storing model weights on SSD and the KV cache in main memory.
Both DeepSpeed and Accelerate also support a variety of models.
Thus, we also evaluated \thiswork with LLaMA-2~\cite{llama2} in \cref{fig:baselines}(b).
However, both baselines struggle with long context lengths beyond 16K, even for relatively small models like 7B, and even with a batch size of 1.
This issue is caused by GPU OOM errors during the prefill stage.
This highlights a key limitation in current frameworks for supporting long context lengths, aligning with findings from recent works~\cite{ring_attn, blockwise_attn}.
%\HS{If possible, 262K results here!}
%Although, there is much works for memory-efficient attention, we adopt most common way to check the results.
%Such methods can be orthogonally applied because we do not modify the computation flow in prefill stage.
%We leaves for future research.
%Flexgen do not yet support llama.


\subsection{ Cost-Effectiveness Comparison }
\label{exp:cost}

% \input{tables/cost}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{graphs/cost_ssd.pdf}       
    \caption{
    Cost-effectiveness comparison with scaling the number of SSDs.
    The experiments are conducted with 30B model with an H100 GPU and a 32k context length.
    }
    \label{fig:cost_ssd}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{graphs/cost_gpu.pdf}       
    \caption{
    Cost-effectiveness comparison with higher-grade GPUs.
    }
    \label{fig:cost_gpu}
\end{figure}


% will these CSDs be a good investment? 
% While \thiswork offers superior speedup compared to conventional systems, the cost of CSDs (\$2,400) is higher than traditional storage devices (\$400).
% Even though we expect its cost to dramatically drop in the next generation~\cite{smartssd2}, evaluating the cost-efficiency is essential to determine whether our CSDs system is a worthwhile investment for LLM inference.
%We devise two cost efficiency comparsion on first replacing better GPU grades on the system and adding more storage devices on the system.

\textbf{Against adding SSDs.}
We first compare the cost-efficiency of adding more storage devices.
%The main performance boost of \thiswork comes from the increased internal bandwidth of the CSDs, making it important to evaluate performance based on the number of CSDs.
In \cref{fig:cost_ssd}, experiments with a 30B model, H100 GPU, and 32k context length were conducted.
% We calculated the cost-efficiency metric (throughput per total system cost).
Our experimental setup costs \$37,000 (A100: \$7,000, server: \$20,000, expansion: \$10,000), with each SSD costing \$400 and each CSD costing \$2,400.
Initially, the baseline~\cite{flexgen} with 2$\times$ SSDs outperforms \thiswork in cost-efficiency, achieving a 1.68$\times$ speedup compared to the single-device setup.
This is expected, as two SSDs do not saturate the system interconnect.
%, and offloading multi-head attention from CPU to the FPGA offers little advantage.
However, the baseline fails to scale beyond two SSDs due to interconnect bottlenecks.
% \thiswork demonstrates up to 2.41$\times$ speedup using 8$\times$CSDs, achieving comparable cost-efficiency to conventional storage with the additional cost of 8$\times$\$2,400.
With 16$\times$CSDs, \thiswork reaches a 3.71$\times$ speedup, with additional cost 16$\times$\$2,400.
% This success is due to \thiswork shifting the bottleneck from the PCIe interconnect to the internal bandwidth of the CSDs.
Notably, cost-efficiency of \thiswork gradually improves as more CSDs are added, a significant advantage when considering scalability.

\textbf{Against upgrading GPUs.}
% With the baseline, increasing computing power by upgrading GPUs is another option instead of adding more storage devices.
As shown in \cref{fig:cost_gpu}, upgrading from the A100 (\$7,000) to the H100 (\$30,000) only provides an 8\%-19\% throughput improvement with the baseline, as the main bottleneck remains KV cache-related I/O.
In contrast, \thiswork reduces the system interconnect bottleneck, providing a 1.21$\times$-1.69$\times$ speedup by replacing 8 storage devices with 8$\times$CSDs (from \$400 to \$2,000 each).
Replacing all storage devices with 16$\times$CSDs results in a 2.15$\times$-2.19$\times$ speedup, further improving cost efficiency compared to 8$\times$CSDs.
The only exception is the 30B model, where 8$\times$CSDs achieve better cost efficiency due to \xcache, which effectively utilizes host memory space.
Therefore, \thiswork provides much higher improvements per dollar than upgrading the GPU with \baseline.


\textbf{Against multi-GPU systems.}
We compared the cost-effectiveness of \thiswork with GPU-only solutions based on values reported in \cite{smoothquant}.
In SmoothQuant~\cite{smoothquant} (Tab.7), the per-token decoding latency for 1K context length with 175B model in FP16 with a batch size of 16 is 4133 ms, which can be converted to throughput of 3.88 token/sec. 
This setup uses 8$\times$A100 80GB GPUs, with a system cost of \$199,000.
In contrast, \thiswork achieves 1.99 token/sec for 175B 1K context lengths, as shown in \cref{fig:baselines}(a), at a total cost of \$98,400 (H100: \$30,000, server: \$20,000, expansion: \$10,000, and 16×CSD: \$38,400). 
We achieve better cost efficiency, with the GPU solution delivering 
1.95e-5 throughput/\$ and \thiswork delivering 2.02e-5 throughput/\$.

\section{Discussion}
\label{discussion:expansion}
\textbf{Storage Expansion and Resource Pooling.}
% Heterogenous server structure.
% Smart-Infinity.
% , FPGAs, NICs, or even system memories~\cite{cxl}.
% \textbf{PCIe P2P Communication}
% Smart-Infinity, BaM.
% DONGLE~\cite{dongle}.
% NVMeVirt~\cite{nvmevirt}, 
% Architecting a Flash-Based Storage System for Low-Cost Inference of Extreme-Scale DNNs
% PCIe expansion system and memory pooling
Modern workloads, such as LLMs~\cite{smartinfinity}, recommendation systems~\cite{ann_smartssd,zhou2010impact}, and big data analytics~\cite{bam,biscuit}, demand increasing memory and storage capacity.
This has led to new approaches where multiple servers share storage and memory resources~\cite{cxl, falcon}.
\thiswork also leverages a storage expansion system~\cite{smartinfinity,bam,ann_smartssd,gmt} with PCIe switches to accommodate multiple CSDs, aligning with this trend toward resource sharing.

However, the primary limitation of this solution lies in the host.
Even with peer-to-peer communication~\cite{nvmmu}, managing and scheduling data transfers place a heavy burden on the host~\cite{dongle}.
Some works propose accelerator-orchestrated storage access~\cite{dongle,bam,gmt}, and \thiswork could benefit from such techniques.
\thiswork also benefits from reduction of CPU burden through \xcache during LLM inference.

Another limitation of expansion systems is that they don’t increase the physical slots or bandwidth available to the host, worsening the link bandwidth bottleneck as more devices are added.
Smart-Infinity~\cite{smartinfinity}, which also utilizes CSDs, addresses this by increasing internal bandwidth in LLM training scenarios.
While Smart-Infinity focuses on training, \thiswork is designed specifically for LLM inference.
% \JL{what's next could be removed for space}
% Unlike Smart-Infinity, which manages optimizer states in CSDs during training, \thiswork tackles inference-specific challenges,  such as offloading multi-head attention to CSDs (\cref{sec:scheme1}), managing the KV cache updates in storage (\cref{sec:scheme2}), and co-optimizing CSDs and the host for cooperative computing during inference (\cref{sec:scheme3}).
% While Smart-Infinity focuses on training, \thiswork reduces KV cache data transfer for more efficient LLM inference. 

\section{Related Work}
%TBA: Faster Large Language Model Training Using SSD-Based Activation Offloading

\subsection{LLM Inference}
\textbf{LLM Inference Acceleration.}
%\subsection{Real-time LLM Serving}
%Orca, vLLM.
%\subsection{Memory efficient Attetion~\cite{flashattn,ringattn}}
%FlashAttention, Blockwise Attention, Ring-attention.
% Due to the growing demand for fast and efficient generative model-serving systems, 
Recent works have proposed various ways to accelerate LLM inference.
These works either employed kernel optimizations~\cite{deepspeedinf, flashllm}, offload memory-bound computation to PIM processors~\cite{transpim, pimdl, neupims, attacc}, quantized parameters~\cite{kvquant, atom, llm-qat, llmint8}, exploit model parallelism~\cite{deepspeedinf, scaletrans}, or devise new batching strategies~\cite{orca, sarathi, dvabatch}.
These proposed methods have significantly improved the throughput of LLM inference systems, but they focus on interactive use cases.
%Previous research has addressed similar issues in the context of internal/external fragments in KV caches~\cite{vllm}, but the approach does not target offloading-based batched inferences.
LLMs are also actively applied to offline scenarios, such as benchmarking~\cite{liang2023holistic, NEURIPS2023_89e44582, NEURIPS2023_91f18a12, guo2023gpt4graph} and information extraction~\cite{narayan2018don, pu2023summarization, chang2024booookscore, goyal2022news, pang-etal-2023-long}. 
We target scenarios that are less sensitive to latency factors in batches over a large number of tokens running LLM inference.



\textbf{Offloaded LLM Inference.}
%issue of LLM inference is the requirement of enough memory size for accelerators (e.g., GPUs).
%This huge memory requirement comes from large model sizes of LLMs and long input sequence lengths.
%Motivated by the above insight, many approaches~\cite {flexgen, deepspeedinf, petals, llminaflash} proposed strategies for serving LLM inference with limited GPU memory using host memory or storage as extended memory. 
%Notably, FlexGen~\cite{flexgen} efficiently addresses the significant I/O overhead of large model weight by enlarging the batch size.
%It breaks the barrier to enlarging batch size, enabling even higher throughput on a single machine.
%Unfortunately, with batched inference~\cite{flexgen, vllm}, the KV cache becomes the most important factor for throughput than the model itself.
As model sizes of recent LLMs grew exponentially, efficiently deploying these models on limited GPU resources became of crucial importance.
Numerous studies on model training have addressed this issue by offloading data and computation to neighboring devices~\cite{zerooffload, zeroinfinity, flashneuron, smartinfinity}.
Therefore, it was a natural next step to apply similar techniques for model inference.
As a result, recent works offloaded data to storage systems~\cite{flexgen, llminaflash, leviathan} and also offloaded computation to the host CPU~\cite{flexgen, powerinfer, hetegen}.
Similar to \thiswork, these works offload computation from the GPU in order to alleviate the computational burdens of the GPU and reduce data movement between the devices.
However, \thiswork is able to utilize much higher bandwidth compared to other works by employing CSDs.

% \textbf{LLM Compression.}
% As we target LLM inference with limited resources, one might suggest a light weight model and think the scenrio where the large model is used is unrealistic.
% However, as studied in many previous works~\cite{gpt3, gpt4, shoeybi2019megatron}, the performance of LLM increases as the parameter of the LLM increases.
% There are many approaches which aim to retain the performance of the large model while reducing the required memory size of the language model.
% Representative methods are quantization~\cite{lin2023awq,llmint8, frantar2023optq} and knowledge distillation~\cite{mirzadeh2020improved, kim-rush-2016-sequence, sun-etal-2019-patient}.
% AWQ~\cite{lin2023awq} showed that it can run Llama-2-13B with laptop GPU and 8GB memory by preserving only 1$\%$ sailient weights and applying INT4 groupwise quantization.
% LLM.int8()~\cite{llmint8} suggested INT8 matrix multiplication in the transformer layer while maintaining the precision of outlier features.
% However, these methods still require a large amount of memory as the sequence length largely increases.
% In this case, LLM inference with near storage processing can be an alternative because it utilizes much larger storage and exceed the limit of main memory.
% Knowledge Distillation (KD)~\cite{mirzadeh2020improved, kim-rush-2016-sequence, sun-etal-2019-patient} is widely researched in the machine learning venue and many works have successfully reduced the number of model parameters in LLM.
% However, this knowledge distillation needs additional training phase which trains the student model.
% It also needs a large teacher model to generate the guide for the student model.
% Therefore, without offloaded training~\cite{smartinfinity}, KD with large teacher model is hard to be conducted in the limited environment.

% \textbf{LLM sparsity}
% %Using the constant information holding(Top-K) is less effective thatn relative informantion holding(quantization).
% %As we target LLM inference with limited resources, one might suggest sparsification and think the scenario.
% %As considering recent trends in exploiting sparsity, which provides constatnt memory consumption for extreme long sequences.
% Consequently, we could consider using quantization methods to reducing the I/O costs for loading weights from system memory or storage devices to host.
% Some work exploit constant memory usage for KV cache.
% However, we think such constant memory usage will suffer low accuracy for extremely longer sequences.
% We will discuss this in evaluation section(@@).
% The 4-bit integer values are dequantized back to FP16 before computation in the host processor, so there is no need to change the computation modules of the host.
% As LLM-QAT \cite{llm-qat} shows, accuracy drop due to precision loss during compression can be minimized to a negligible size by using fine-grained per-channel quantization.
% AWQ~\cite{lin2023awq} showed that it can run Llama-2-13B with laptop GPU and 8GB memory by preserving only 1$\%$ sailient weights and applying INT4 groupwise quantization.
% LLM.int8() \cite{llmint8} suggested INT8 matrix multiplication in the transformer layer while maintaining the precision of outlier features.
% However, these methods still require a large amount of memory as the sequence length largely increases.
% As shown by FlexGen, the I/O cost overhead of storage-offloading inference can be minimized through quantization.
% However, in situation where attention computation is delegated to the CPU, because it has huge overhead for dequantization before it computes, FlexGen does not support cpu-delegation and KV cache quantization at the same time.
% Because KV cache is compressed with fine-grained per channel group, we can utilize its parallelism for multiple CSD to dequantize. Through its support, we could enable utilizing compuation delegation and KV cache quantization at the same time, which are essential for reducing I/O cost overhead in storage-offloading LLM inference.
% Also, for long sequences, the inference accuracy drop is not negligible without methods such as PTQ and QAT. As it is shown in LLM-QAT, accuracy drop can be avoided by quantization aware training with fine-grained per-channel compression. Our framework both supports Quantization-Aware training and Quantized Inference for storage-offloading LLM and enables users to inference long sequences while avoiding accuracy drop.
% (2) Sparsity 
% H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models.
% Serpens: A High Bandwidth Memory Based Accelerator for General-Purpose Sparse Matrix-Vector Multiplication.

\subsection{Near-Data Processing}
% The concept of near-data processing %, with its potential for massive parallel processing and reduced data movement, 
% has been studied for well over several decades.
% Earlier works attempted to integrate processors on DRAM chips~\cite{execube, IRAM, flexram}.
% However, these works were not incorporated into real-world DRAMs for several reasons, including high costs and the inefficiency of logic on memory technology.

% In the 2010s, with the slowdown of Dennard scaling and the surge of AI, near-data processing started gaining more interest from the research community and industry.
% With the appearance of 3D stacked memory~\cite{hmc}, many works proposed to utilize the logic dies for various applications ~\cite{tesseract, pei} until the product was discontinued in the late 2010s.
% Other works proposed implementing minor changes in the DRAM architecture to enable simple operations ~\cite{rowclone, ambit, computedram, bufcmp, chameleon, gradpim, tensordimm, axdimm, piccolo}.

\textbf{Processing In Memory.}
In the late 2010s, Processing In Memory (PIM) designs supporting more general computations started to appear~\cite{newton, aim, hbmpim}, and real-world PIM products were introduced~\cite{upmem, hbmpim, aim}.
As these designs enabled more general computations, some works attempted to accelerate end-to-end LLM inference systems~\cite{transpim, pimdl}
%However, PIM's lack of computation capabilities limited performance gains compared to systems utilizing accelerators with strong computational capabilities.
Other works~\cite{neupims, attacc} have explored PIM systems accompanied by GPUs or NPUs, where only selected memory-bound operations are offloaded to PIM processors.
%AttAcc~\cite{attacc} provides an accelerator (leveraging PIM) for attention layers to minimize data movement for Key-Value (KV) matrices.
%Similarly, NeuPIMs~\cite{neupims} proposes a novel system for PIM acceleration along with operation scheduling to facilitate end-to-end inference of LLMs.
Although these works successfully increase throughput by offloading memory-bound applications to PIM, they are limited in that they have been evaluated in simulated environments only.

\textbf{Near-Storage Processing.} 
Various works explored ways to enable near-storage processing.
Earlier works aimed to design ``active'' or ``intelligent'' disks~\cite{active_disk, active_disk_large, active_storage_large, idisk}. 
However, due to the massive advancements in the computing capabilities of existing processors, the idea of on-disk processing slowly faded away.
%Near-storage processing regained the interest of research communities with the appearance of modern SSDs, which had much higher internal bandwidth than the I/O interface.
Multiple studies proposed to accelerate various applications by either processing data utilizing the embedded CPU~\cite{self_sorting_ssd, earlysmartssd} or integrating processors to each internal channel~\cite{smartssd_datamine, biscuit} to exploit the internal bandwidth.
Despite the effort, the embedded cores turned out to be low in computational power.
This led to utilizing dedicated processors for SSDs, such as ASICs~\cite{genstore, deepstore, inspire} or FPGAs~\cite{netezza, exadata, rmssd, secndp}.
These computational SSD products are available on the market~\cite{smartssd} and are being used to accelerate various applications such as query processing~\cite{smartssd_query}, training deep learning models~\cite{smartsage, smartinfinity}, and deep learning model inference~\cite{smartssd_dlrm}.

\section{Conclusion}
We propose \thiswork, a novel framework for fast LLM inference using CSDs, achieving high throughput. 
By offloading KV-related operations to each CSD (\ans), we significantly reduce the I/O overhead from KV caches.
On top of \ans, we introduce an efficient method for updating KV caches in storage that hides storage write latency %by using system memory as a temporal buffer 
(\wb).
%, further boosting throughput.
Moreover, we fully leverage the new opportunities provided by CSDs.
By storing input activations (\xcache) instead of K and V, we enable parallel computation between the GPU and CSDs with a limited main memory budget.
\thiswork achieves up to 3.46$\times$ throughput improvement compared to state-of-the-art baselines.
We will open-source \thiswork to encourage adoption and facilitate its use.

%%%%%%% -- PAPER CONTENT ENDS -- %%%%%%%%


%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\bibliographystyle{IEEEtranS}
\bibliography{refs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\input{Sup.tex}


\end{document}