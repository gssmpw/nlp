@article{gpt4,
  title={{Gpt-4 technical report}},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {NeurIPS},
 title = {{Language Models are Few-Shot Learners}},
 year = {2020}
}

@article{gpt2,
  title={{Language Models Are Unsupervised Multitask Learners}},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  year={2019}
}%done

@misc{huggingface,
  key ={{Hugging Face}},
  title={{Hugging Face}},
	url = {https://huggingface.co/docs/hub/index},
	urldate = {2022-10-07},
}

@article{llama,
  title={{Llama: Open and efficient foundation language models}},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{opt,
  title={{Opt: Open pre-trained transformer language models}},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

% Large scale transformer model
@article{bert,
  title={{Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}%done

% Large scale transformer model
@article{roberta,
  title={{Roberta: A robustly optimized bert pretraining approach}},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}%done

% Large scale transformer model
@article{t5,
  title={{Exploring the Limits of Transfer Learning with a Unified Text-to-text Transformer}},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}%done

% Transformer based models
@inproceedings{attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {NeurIPS},
 title = {{Attention is All You Need}},
 year = {2017}
}

@inproceedings {orca,
author = {Gyeong-In Yu and Joo Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun},
title = {{Orca: A Distributed Serving System for {Transformer-Based} Generative Models}},
booktitle = {OSDI},
year = {2022}
} %done

@inproceedings{neupims,
author = {Heo, Guseul and Lee, Sangyeop and Cho, Jaehong and Choi, Hyunmin and Lee, Sanghyeon and Ham, Hyungkyu and Kim, Gwangsun and Mahajan, Divya and Park, Jongse},
title = {{NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing}},
booktitle = {ASPLOS},
year = {2024}
}

@inproceedings{attacc,
author = {Park, Jaehyun and Choi, Jaewan and Kyung, Kwanhee and Kim, Michael Jaemin and Kwon, Yongsuk and Kim, Nam Sung and Ahn, Jung Ho},
title = {{AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference}},
booktitle = {ASPLOS},
year = {2024}
}

@inproceedings{transpim,
author={Zhou, Minxuan and Xu, Weihong and Kang, Jaeyoung and Rosing, Tajana}, 
title={{TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer}}, 
booktitle={HPCA},
year={2022}
}

@inproceedings{vllm,
author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
title = {{Efficient Memory Management for Large Language Model Serving with PagedAttention}},
booktitle = {SOSP},
year = {2023}
}

@article{hetegen,
author={Zhao, Xuanlei and Jia, Bin and Zhou, Haotian and Liu, Ziming and Cheng, Shenggan and You, Yang},  
title={{HeteGen: Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices}},
journal={arXiv preprint arXiv:2403.01164},
year={2024}
}


@InProceedings{flexgen,
  title = {{FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU}},
  author = {Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and Re, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle = {ICML},
  year = {2023},
}

@inproceedings{scaletrans,
  title={{Efficiently scaling transformer inference}},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  booktitle={MLSys},
  year={2023}
}

@article{petals,
  author={Borzunov, Alexander and Baranchuk, Dmitry and Dettmers, Tim and Ryabinin, Max and Belkada, Younes and Chumachenko, Artem and Samygin, Pavel and Raffel, Colin},
  title={{Petals: Collaborative inference and fine-tuning of large models}},
  journal={arXiv preprint arXiv:2209.01188},
  year={2022}
}

@article{kvquant,
  title={{KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization}},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}

@article{sarathi,
  title={{Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills}},
  author={Agrawal, Amey and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Ramjee, Ramachandran},
  journal={arXiv preprint arXiv:2308.16369},
  year={2023}
}

@inproceedings{deepspeedinf,
  title={{Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale}},
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others},
  booktitle={SC},
  year={2022}
}

@article{llminaflash,
  title={{Llm in a flash: Efficient large language model inference with limited memory}},
  author={Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C and Rastegari, Mohammad and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2312.11514},
  year={2023}
}

@article{flashllm,
author = {Xia, Haojun and Zheng, Zhen and Li, Yuchao and Zhuang, Donglin and Zhou, Zhongzhu and Qiu, Xiafei and Li, Yong and Lin, Wei and Song, Shuaiwen Leon},
title = {{Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity}},
year = {2023},
journal = {Proc. VLDB Endow.}
}

@article{powerinfer,
  title={{Powerinfer: Fast large language model serving with a consumer-grade gpu}},
  author={Song, Yixin and Mi, Zeyu and Xie, Haotong and Chen, Haibo},
  journal={arXiv preprint arXiv:2312.12456},
  year={2023}
}

@inproceedings {flashneuron,
author = {Jonghyun Bae and Jongsung Lee and Yunho Jin and Sam Son and Shine Kim and Hakbeom Jang and Tae Jun Ham and Jae W. Lee},
title = {{FlashNeuron}: {SSD-Enabled} {Large-Batch} Training of Very Deep Neural Networks},
booktitle = {FAST},
year = {2021}
}

@inproceedings{g10,
author = {Zhang, Haoyang and Zhou, Yirui and Xue, Yuqi and Liu, Yiqi and Huang, Jian},
title = {{G10: Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations}},
year = {2023},
booktitle = {MICRO}
}

@article{leviathan,
  author={Jin, Yunho and Kim, Shine and Ham, Tae Jun and Lee, Jae W.},
  journal={IEEE Transactions on Computers}, 
  title={{Architecting a Flash-Based Storage System for Low-Cost Inference of Extreme-Scale DNNs}}, 
  year={2022}
}

@inproceedings {behemoth,
author = {Shine Kim and Yunho Jin and Gina Sohn and Jonghyun Bae and Tae Jun Ham and Jae W. Lee},
title = {{Behemoth: A Flash-centric Training Accelerator for Extreme-scale {DNNs}}},
booktitle = {FAST},
year = {2021}
}

@inproceedings{zeroinfinity,
author = {Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
title = {{ZeRO-infinity: breaking the GPU memory wall for extreme scale deep learning}},
year = {2021},
booktitle = {SC}
}

@inproceedings{zerooffload,
  title={{Zero-offload: democratizing billion-scale model training}},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={USENIX ATC},
  year={2021}
}

@inproceedings{bigbird,
author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
title = {{Big bird: transformers for longer sequences}},
year = {2020},
booktitle = {NeurIPS}
}

@article{longformer,
  title={{Longformer: The long-document transformer}},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{infinitellm,
  title={{Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache}},
  author={Lin, Bin and Peng, Tao and Zhang, Chen and Sun, Minmin and Li, Lanbo and Zhao, Hanyu and Xiao, Wencong and Xu, Qi and Qiu, Xiafei and Li, Shen and others},
  journal={arXiv preprint arXiv:2401.02669},
  year={2024}
}

@article{atom,
  title={{Atom: Low-bit quantization for efficient and accurate llm serving}},
  author={Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris},
  journal={arXiv preprint arXiv:2310.19102},
  year={2023}
}

@inproceedings{llmint8,
  title={{Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale}},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  booktitle={NeurIPS},
  year={2022}
}

@article{gear,
  title={{Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm}},
  author={Kang, Hao and Zhang, Qingru and Kundu, Souvik and Jeong, Geonhwa and Liu, Zaoxing and Krishna, Tushar and Zhao, Tuo},
  journal={arXiv preprint arXiv:2403.05527},
  year={2024}
}

%%% PIM related [START] %%%

@inproceedings{newton,
  author={He, Mingxuan and Song, Choungki and Kim, Ilkon and Jeong, Chunseok and Kim, Seho and Park, Il and Thottethodi, Mithuna and Vijaykumar, T. N.},
  booktitle={MICRO}, 
  title={{Newton: A DRAM-maker’s Accelerator-in-Memory (AiM) Architecture for Machine Learning}}, 
  year={2020},
}

@inproceedings{aim,
  title={{A 1ynm 1.25 V 8Gb, 16Gb/s/pin GDDR6-based accelerator-in-memory supporting 1TFLOPS MAC operation and various activation functions for deep-learning applications}},
  author={Lee, Seongju and Kim, Kyuyoung and Oh, Sanghoon and Park, Joonhong and Hong, Gimoon and Ka, Dongyoon and Hwang, Kyudong and Park, Jeongje and Kang, Kyeongpil and Kim, Jungyeon and others},
  booktitle={ISSCC},
  year={2022}
}

@inproceedings{hbmpim,
  author={Lee, Sukhan and Kang, Shin-haeng and Lee, Jaehoon and Kim, Hyeonsu and Lee, Eojin and Seo, Seungwoo and Yoon, Hosang and Lee, Seungwon and Lim, Kyounghwan and Shin, Hyunsung and Kim, Jinhyun and Seongil, O and Iyer, Anand and Wang, David and Sohn, Kyomin and Kim, Nam Sung},
  booktitle={ISCA}, 
  title={{Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology : Industrial Product}}, 
  year={2021},
}

@inproceedings{pimdl,
author = {Li, Cong and Zhou, Zhe and Wang, Yang and Yang, Fan and Cao, Ting and Yang, Mao and Liang, Yun and Sun, Guangyu},
title = {{PIM-DL: Expanding the Applicability of Commodity DRAM-PIMs for Deep Learning via Algorithm-System Co-Optimization}},
year = {2024},
booktitle = {ASPLOS}
}

@inproceedings{upmem,
  title={{The true Processing In Memory accelerator}},
  author={Devaux, Fabrice},
  booktitle={HCS},
  year={2019}
}

@inproceedings{gradpim,
  title={{GradPIM: A practical processing-in-DRAM architecture for gradient descent}},
  author={Kim, Heesu and Park, Hanmin and Kim, Taehyun and Cho, Kwanheum and Lee, Eojin and Ryu, Soojung and Lee, Hyuk-Jae and Choi, Kiyoung and Lee, Jinho},
  booktitle={HPCA},
  year={2021},
}
%%% PIM related [END] %%%

@inproceedings {pacman,
author = {Jing Wang and Youyou Lu and Qing Wang and Minhui Xie and Keji Huang and Jiwu Shu},
title = {{Pacman: An Efficient Compaction Approach for {Log-Structured} {Key-Value} Store on Persistent Memory}},
booktitle = {USENIX ATC},
year = {2022},
} %done

@article{sparse_trans,
  title={{Generating long sequences with sparse transformers}},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
} %done

%%% NDP related [START] %%%
@inproceedings{
    execube,
   Author = {Kogge, Peter M},
   Title = {{EXECUBE-A new architecture for scaleable MPPs}},
   BookTitle = {ICPP},
      Year = {1994} 
}
@ARTICLE{IRAM,
  author={Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.},
  journal={IEEE Micro}, 
  title={{A case for intelligent {RAM}}}, 
  year={1997},
  volume={17},
  number={2},
  pages={34-44}
}
@inproceedings{flexram,
   Author = {Kang, Yi and Huang, Wei and Yoo, Seung-Moon and Keen, D and Ge, Zhenzhou and Lam, V and Pattnaik, P and Torrellas, J},
   Title = {{FlexRAM: Toward an advanced intelligent memory system}},
   BookTitle = {ICCD},
      Year = {1999} 
}

@inproceedings{rowclone,
  title={{RowClone: fast and energy-efficient in-DRAM bulk data copy and initialization}},
  author = {Seshadri, Vivek and Kim, Yoongu and Fallin, Chris and Lee, Donghyuk and Ausavarungnirun, Rachata and Pekhimenko, Gennady and Luo, Yixin and Mutlu, Onur and Gibbons, Phillip B. and Kozuch, Michael A. and Mowry, Todd C.},
  booktitle={MICRO},
  year={2013}
}

@inproceedings{ambit,
  title={{Ambit: In-memory accelerator for bulk bitwise operations using commodity DRAM technology}},
  author={Seshadri, Vivek and Lee, Donghyuk and Mullins, Thomas and Hassan, Hasan and Boroumand, Amirali and Kim, Jeremie and Kozuch, Michael A and Mutlu, Onur and Gibbons, Phillip B and Mowry, Todd C},
  booktitle={MICRO},
  year={2017}
}

@inproceedings{bufcmp,
  title={{Buffered compares: Excavating the hidden parallelism inside DRAM architectures with lightweight logic}},
  author={Lee, Jinho and Ahn, Jung Ho and Choi, Kiyoung},
  booktitle={DATE},
  year={2016}
}

@inproceedings{computedram,
  title={{ComputeDRAM}: In-memory compute using off-the-shelf DRAMs},
  author={Gao, Fei and Tziantzioulis, Georgios and Wentzlaff, David},
  booktitle={MICRO},
  year={2019}
}

@inproceedings{chameleon,
  title={{Chameleon: Versatile and practical near-{DRAM} acceleration architecture for large memory systems}},
  author={Asghari-Moghaddam, Hadi and Son, Young Hoon and Ahn, Jung Ho and Kim, Nam Sung},
  booktitle={MICRO},
  year={2016}
}

@ARTICLE{axdimm,
  author={Ke, Liu and Zhang, Xuan and So, Jinin and Lee, Jong-Geon and Kang, Shin-Haeng and Lee, Sukhan and Han, Songyi and Cho, YeonGon and Kim, Jin Hyun and Kwon, Yongsuk and Kim, KyungSoo and Jung, Jin and Yun, Ilkwon and Park, Sung Joo and Park, Hyunsun and Song, Joonho and Cho, Jeonghyeon and Sohn, Kyomin and Kim, Nam Sung and Lee, Hsien-Hsin S.},
  journal={IEEE Micro}, 
  title={{Near-Memory Processing in Action: Accelerating Personalized Recommendation With {AxDIMM}}}, 
  year={2022},
  volume={42},
  number={1},
  pages={116-127},
}

@inproceedings{hmc,
  title={{Hybrid memory cube ({HMC})}},
  author={Pawlowski, J Thomas},
  booktitle={HCS},
  year={2011},
}

@inproceedings{tesseract,
  title={{A scalable processing-in-memory accelerator for parallel graph processing}},
  author={Ahn, Junwhan and Hong, Sungpack and Yoo, Sungjoo and Mutlu, Onur and Choi, Kiyoung},
  booktitle={ISCA},
  year={2015}
}

@inproceedings{pei,
  title={{PIM-enabled instructions: a low-overhead, locality-aware processing-in-memory architecture}},
  author={Ahn, Junwhan and Yoo, Sungjoo and Mutlu, Onur and Choi, Kiyoung},
  booktitle={ISCA},
  year={2015}
}

@inproceedings{tensordimm,
  title={{{TensorDIMM}: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning}},
  author={Kwon, Youngeun and Lee, Yunjae and Rhu, Minsoo},
  booktitle={MICRO},
  year={2019}
}

%% added for usenix papers

@inproceedings{nearpm,
author = {Seneviratne, Yasas and Seemakhupt, Korakit and Liu, Sihang and Khan, Samira},
title = {{NearPM: A Near-Data Processing System for Storage-Class Applications}},
year = {2023},
booktitle = {EuroSys}
}



%%% NDP related [END] %%%

%%% other related to LLM [START] %%%

%%gradient compression, eurosys23
@inproceedings{espresso,
author = {Wang, Zhuang and Lin, Haibin and Zhu, Yibo and Ng, T. S. Eugene},
title = {{Hi-Speed DNN Training with Espresso: Unleashing the Full Potential of Gradient Compression with Near-Optimal Usage Strategies}},
year = {2023},
booktitle = {EuroSys}
}

%%LLM to explain cloud incident, eurosys2024
@inproceedings{llm_cloud_incident,
author = {Chen, Yinfang and Xie, Huaibing and Ma, Minghua and Kang, Yu and Gao, Xin and Shi, Liu and Cao, Yunjie and Gao, Xuedong and Fan, Hao and Wen, Ming and Zeng, Jun and Ghosh, Supriyo and Zhang, Xuchao and Zhang, Chaoyun and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei and Xu, Tianyin},
title = {{Automatic Root Cause Analysis via Large Language Models for Cloud Incidents}},
year = {2024},
booktitle = {EuroSys}
}

%% use resource efficiently, large batch, efficiency and speedup for training, eurosys2022
@inproceedings{varuna,
author = {Athlur, Sanjith and Saran, Nitika and Sivathanu, Muthian and Ramjee, Ramachandran and Kwatra, Nipun},
title = {{Varuna: scalable, low-cost training of massive deep learning models}},
year = {2022},
booktitle = {EuroSys}
}

%% new batching scheme, atc23
@inproceedings {dvabatch,
author = {Weihao Cui and Han Zhao and Quan Chen and Hao Wei and Zirui Li and Deze Zeng and Chao Li and Minyi Guo},
title = {{{DVABatch}: Diversity-aware {Multi-Entry} {Multi-Exit} Batching for Efficient Processing of {DNN} Services on {GPUs}}},
booktitle = {USENIX ATC},
year = {2022}
}

%% disaggregating the prefill and decoding computation, OSDI 2024
@article{distserve,
  title={{DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving}},
  author={Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
  journal={arXiv preprint arXiv:2401.09670},
  year={2024}
}

%%% other related to LLM [END] %%%

%%% other related to NSP [START] %%%

%% framework for NSP with FPGA, eurosys 2020
@inproceedings{NSC-FPGA,
author = {Schmid, Robert and Plauth, Max and Wenzel, Lukas and Eberhardt, Felix and Polze, Andreas},
title = {{Accessible near-storage computing with FPGAs}},
year = {2020},
booktitle = {EuroSys}
}

%% in-storage graph processing, accelerator inside SSD, uses FPGA
@inproceedings {glist,
author = {Cangyuan Li and Ying Wang and Cheng Liu and Shengwen Liang and Huawei Li and Xiaowei Li},
title = {{{GLIST}: Towards {In-Storage} Graph Learning}},
booktitle = {USENIX ATC},
year = {2021}
}

%% near-data processing for GNN inference
@inproceedings {holisticgnn,
author = {Miryeong Kwon and Donghyun Gouk and Sangwon Lee and Myoungsoo Jung},
title = {{{Hardware/Software} {Co-Programmable} Framework for Computational {SSDs} to Accelerate Deep Learning Service on {Large-Scale} Graphs}},
booktitle = {FAST},
year = {2022}
}

%% caching for near-storage accelerators
@inproceedings {omnicache,
author = {Jian Zhang and Yujie Ren and Marie Nguyen and Changwoo Min and Sudarsun Kannan},
title = {{{OmniCache}: Collaborative Caching for Near-storage Accelerators}},
booktitle = {FAST},
year = {2024}
}



%%% other related to NSP [END] %%%



%%% NSP related [START] %%%

@inproceedings{active_disk,
author = {Acharya, Anurag and Uysal, Mustafa and Saltz, Joel},
title = {{Active disks: programming model, algorithms and evaluation}},
year = {1998},
booktitle = {ASPLOS}
}

@article{idisk,
author = {Keeton, Kimberly and Patterson, David A. and Hellerstein, Joseph M.},
title = {{A case for intelligent disks (IDISKs)}},
year = {1998},
volume = {27},
number = {3},
journal = {SIGMOD Rec.}
}

@article{active_disk_large,
  author={Riedel, E. and Faloutsos, C. and Gibson, G.A. and Nagle, D.},
  journal={Computer}, 
  title={{Active disks for large-scale data processing}}, 
  year={2001},
  volume={34},
  number={6},
  pages={68-74}
}

@inproceedings{active_storage_large,
author = {Riedel, Erik and Gibson, Garth A. and Faloutsos, Christos},
title = {{Active Storage for Large-Scale Data Mining and Multimedia}},
year = {1998},
booktitle = {VLDB}
}

@inproceedings{self_sorting_ssd,
  author={Quero, Luis Cavazos and Lee, Young-Sik and Kim, Jin-Soo},
  booktitle={MSST}, 
  title={{Self-sorting SSD: Producing sorted data inside active SSDs}}, 
  year={2015}
}

@inproceedings{smartssd_query,
author = {Do, Jaeyoung and Kee, Yang-Suk and Patel, Jignesh M. and Park, Chanik and Park, Kwanghyun and DeWitt, David J.},
title = {{Query processing on smart SSDs: opportunities and challenges}},
year = {2013},
booktitle = {SIGMOD}
}

@inproceedings{earlysmartssd,
  author={Kang, Yangwook and Kee, Yang-suk and Miller, Ethan L. and Park, Chanik},
  booktitle={MSST}, 
  title={{Enabling cost-effective data processing with smart SSD}}, 
  year={2013}
}

@inproceedings{smartssd_dlrm,
author = {Soltaniyeh, Mohammadreza and Lagrange Moutinho Dos Reis, Veronica and Bryson, Matt and Yao, Xuebin and Martin, Richard P. and Nagarakatte, Santosh},
title = {{Near-Storage Processing for Solid State Drive Based Recommendation Inference with SmartSSDs®}},
year = {2022},
booktitle = {ICPE}
}

@inproceedings{biscuit,
author = {Gu, Boncheol and Yoon, Andre S. and Bae, Duck-Ho and Jo, Insoon and Lee, Jinyoung and Yoon, Jonghyun and Kang, Jeong-Uk and Kwon, Moonsang and Yoon, Chanho and Cho, Sangyeun and Jeong, Jaeheon and Chang, Duckhyun},
title = {{Biscuit: a framework for near-data processing of big data workloads}},
year = {2016},
booktitle = {ISCA}
}

@inproceedings{smartssd_datamine,
author = {Bae, Duck-Ho and Kim, Jin-Hyung and Kim, Sang-Wook and Oh, Hyunok and Park, Chanik},
title = {{Intelligent SSD: a turbo for big data mining}},
year = {2013},
booktitle = {CIKM}
}

@inproceedings{smartsage,
author = {Lee, Yunjae and Chung, Jinha and Rhu, Minsoo},
title = {{SmartSAGE: training large-scale graph neural networks using in-storage processing architectures}},
year = {2022},
booktitle = {ISCA}
}

@inproceedings{genstore,
author = {Mansouri Ghiasi, Nika and Park, Jisung and Mustafa, Harun and Kim, Jeremie and Olgun, Ataberk and Gollwitzer, Arvid and Senol Cali, Damla and Firtina, Can and Mao, Haiyu and Almadhoun Alserr, Nour and Ausavarungnirun, Rachata and Vijaykumar, Nandita and Alser, Mohammed and Mutlu, Onur},
title = {{GenStore: a high-performance in-storage processing system for genome sequence analysis}},
year = {2022},
booktitle = {ASPLOS}
}

@inproceedings{rmssd,
  title={{RM-SSD: In-storage computing for large-scale recommendation inference}},
  author={Sun, Xuan and Wan, Hu and Li, Qiao and Yang, Chia-Lin and Kuo, Tei-Wei and Xue, Chun Jason},
  booktitle={HPCA},
  year={2022},
}

@inproceedings{deepstore,
  title={{Deepstore: In-storage acceleration for intelligent queries}},
  author={Mailthody, Vikram Sharma and Qureshi, Zaid and Liang, Weixin and Feng, Ziyan and De Gonzalo, Simon Garcia and Li, Youjie and Franke, Hubertus and Xiong, Jinjun and Huang, Jian and Hwu, Wen-mei},
  booktitle={MICRO},
  year={2019}
}

@inproceedings{inspire,
author = {Lin, Jilan and Liang, Ling and Qu, Zheng and Ahmad, Ishtiyaque and Liu, Liu and Tu, Fengbin and Gupta, Trinabh and Ding, Yufei and Xie, Yuan},
title = {{{INSPIRE}: In-Storage Private Information Retrieval via Protocol and Architecture Co-Design}},
year = {2022},
booktitle = {ISCA},
}

@inproceedings{secndp,
  title={{SecNDP: Secure Near-Data Processing with Untrusted Memory}},
  author={W. Xiong and L. Ke and D. Jankov and M. Kounavis and X. Wang and E. Northup and J. Yang and B. Acun and C. Wu and P. Peter Tang and G. Edward Suh and X. Zhang and H. S. Lee},
  booktitle={HPCA},
  year={2022},
}

@inproceedings{smartinfinity,
  title={{Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System}},
  author={Jang, Hongsun and Song, Jaeyong and Jung, Jaewon and Park, Jaeyoung and Kim, Youngsok and Lee, Jinho},
  booktitle={HPCA},
  year={2024}
}

@article{piccolo,
  title={{A Case for In-Memory Random Scatter-Gather for Fast Graph Processing}},
  author={Shin, Changmin and Kwon, Taehee and Song, Jaeyong and Ju, Jae Hyung and Liu, Frank and Choi, Yeonkyu and Lee, Jinho},
  journal={CAL},
  year={2024}
}

@inproceedings{netezza,
  title={{Introduction to the IBM Netezza warehouse appliance}},
  author={Singh, Malcolm and Leonhardi, Ben},
  booktitle={CASCON},
  year={2011}
}

@article{exadata,
  title={{A technical overview of the oracle exadata database machine and exadata storage server}},
  author={Weiss, Ronald},
  journal={Oracle White Paper},
  year={2012}
}

@misc{smartssd,
  key ={{SmartSSD}},
  title={{SmartSSD}},
	url = {https://www.xilinx.com/applications/data-center/computational-storage/smartssd.html},
	urldate = {2024-05-13},
}

@misc{smartssd2,
  key ={{SmartSSD gen2}},
  title={{Samsung Electronics Develops Second-Generation SmartSSD Computational Storage Drive with Upgraded Processing Functionality}},
	url = {https://semiconductor.samsung.com/emea/news-events/news/samsung-electronics-develops-second-generation-smartssd-computational-storage-drive-with-upgraded-processing-functionality/},
	urldate = {2024-10-18},
}



@misc{kintex,
  key ={{AMD Kintex™ UltraScale+™ FPGAs}},
  title={{AMD Kintex™ UltraScale+™ FPGAs}},
	url = {https://www.amd.com/en/products/adaptive-socs-and-fpgas/fpga/kintex-ultrascale-plus.html},
	urldate = {2024-10-18},
}

@inproceedings{nascent,
  title={{{NASCENT}: Near-storage acceleration of database sort on {SmartSSD}}},
  author={Salamat, Sahand and Haj Aboutalebi, Armin and Khaleghi, Behnam and Lee, Joo Hwan and Ki, Yang Seok and Rosing, Tajana},
  booktitle={FPGA},
  year={2021}
}


@article{nascent2,
  title={{{NASCENT2}: Generic near-storage sort accelerator for data analytics on {SmartSSD}}},
  author={Salamat, Sahand and Zhang, Hui and Ki, Yang Seok and Rosing, Tajana},
  journal={ACM TRETS},
  year={2022},
  publisher={ACM New York, NY}
}

%%% NSP related [END] %%%

%%% SSD related [START] %%%

@inproceedings {nvmevirt,
author = {Sang-Hoon Kim and Jaehoon Shim and Euidong Lee and Seongyeop Jeong and Ilkueon Kang and Jin-Soo Kim},
title = {{{NVMeVirt}: A Versatile Software-defined Virtual {NVMe} Device}},
booktitle = {FAST},
year = {2023},
isbn = {978-1-939133-32-8},
pages = {379--394}
}
@misc{nvme1,
    key = {{NVM Express 1.0e}},
	title = {{NVM Express 1.0e}},
	url = {https://nvmexpress.org/wp-content/uploads/2013/04/NVM_10e_specification.pdff},
	urldate = {2013-01-23}
}

@inproceedings {activeflash,
author = {Devesh Tiwari and Simona Boboila and Sudharshan Vazhkudai and Youngjae Kim and Xiaosong Ma and Peter Desnoyers and Yan Solihin},
title = {{Active Flash: Towards {Energy-Efficient}, {In-Situ} Data Analytics on {Extreme-Scale} Machines}},
booktitle = {FAST},
year = {2013}
}

%%% SSD related [END] %%%

%%% FPGA related [START] %%%
@misc{axi4,
    key = {{AMBA AXI and ACE protocol specification}},
	title = {{AMBA AXI and ACE protocol specification}},
	year={2021},
    author={{ARM}}
}

@misc{vitis_hls,
    key = {{Vitis high-level synthesis user guide (ug1399)}},
	title = {{Vitis high-level synthesis user guide (ug1399)}},
	year={2021},
    author={{AMD/Xilinx}}
}

@misc{xrt,
    key = {{Xilinx runtime (XRT) release notes (ug1451)}},
	title = {{Xilinx runtime (XRT) release notes (ug1451)}},
	year={2021},
    author={{AMD/Xilinx}}
}

%%% FPGA related [END] %%%

%%% P2P_DMA related [START] %%%
@article{spin,
author = {Bergman, Shai and Brokhman, Tanya and Cohen, Tzachi and Silberstein, Mark},
title = {{SPIN: Seamless operating system integration of peer-to-peer DMA between SSDs and GPUs}},
year = {2019},
volume = {36},
number = {2},
journal = {ACM Trans. Comput. Syst.},
articleno = {5},
numpages = {26}
}

@inproceedings{nvmmu,
  author={Zhang, Jie and Donofrio, David and Shalf, John and Kandemir, Mahmut T. and Jung, Myoungsoo},
  booktitle={PACT}, 
  title={{NVMMU: A non-volatile memory management unit for heterogeneous GPU-SSD architectures}}, 
  year={2015},
}


%%% P2P_DMA related [END] %%%


%%% HLS related [START] %%%

@misc{intro_vlsi,
  title={{Introduction to VLSI systems}},
  author={Mead, Carver and Conway, Lynn},
  year={1980},
  publisher={Addison-wesley Reading, MA}
}

%%% HLS related [END] %%%

%%% SmartSSD related [START] %%%

@inproceedings{p2p_eval,
  title={{Peer-to-peer data transfer evaluation in SmartSSD-based multi-devices system}},
  author={Daoud, Luka and Sun, Gongjin and Huen, Hingkwan},
  booktitle={CAINE},
  year={2024}
}

%%% SmartSSD related [END] %%%

%%% FPGA NSP [START] %%%

@inproceedings{dongle,
author = {Wong, Linus Y. and Zhang, Jialiang and Li, Jing (Jane)},
title = {{DONGLE: Direct FPGA-orchestrated NVMe storage for HLS}},
year = {2023},
booktitle = {FPGA}
}

%%% FPGA NSP [END] %%%

%%% Quantization [START] %%%

@article{llm-qat,
  title={{Llm-qat: Data-free quantization aware training for large language models}},
  author={Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas},
  journal={arXiv preprint arXiv:2305.17888},
  year={2023}
}

%%% Quantization [END] %%%

%%% offline inference [STRAT] %%% 
@article{
liang2023holistic,
title={{Holistic Evaluation of Language Models}},
author={Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Alexander Cosgrove and Christopher D Manning and Christopher Re and Diana Acosta-Navas and Drew Arad Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue WANG and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri S. Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Andrew Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023}
}

@inproceedings{NEURIPS2023_89e44582,
 author = {Guha, Neel and Nyarko, Julian and Ho, Daniel and R\'{e}, Christopher and Chilton, Adam and K, Aditya and Chohlas-Wood, Alex and Peters, Austin and Waldon, Brandon and Rockmore, Daniel and Zambrano, Diego and Talisman, Dmitry and Hoque, Enam and Surani, Faiz and Fagan, Frank and Sarfaty, Galit and Dickinson, Gregory and Porat, Haggai and Hegland, Jason and Wu, Jessica and Nudell, Joe and Niklaus, Joel and Nay, John and Choi, Jonathan and Tobia, Kevin and Hagan, Margaret and Ma, Megan and Livermore, Michael and Rasumov-Rahe, Nikon and Holzenberger, Nils and Kolt, Noam and Henderson, Peter and Rehaag, Sean and Goel, Sharad and Gao, Shang and Williams, Spencer and Gandhi, Sunny and Zur, Tom and Iyer, Varun and Li, Zehua},
 booktitle = {NeurIPS},
 title = {{LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models}},
 year = {2023}
}


@inproceedings{NEURIPS2023_91f18a12,
 author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph E and Stoica, Ion},
 booktitle = {NeurIPS},
 title = {{Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}},
 year = {2023}
}

@article{guo2023gpt4graph,
  title={{Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking}},
  author={Guo, Jiayan and Du, Lun and Liu, Hengyu},
  journal={arXiv preprint arXiv:2305.15066},
  year={2023}
}



@article{narayan2018don,
  title={{Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization}},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1808.08745},
  year={2018}
}

@article{pu2023summarization,
  title={{Summarization is (almost) dead}},
  author={Pu, Xiao and Gao, Mingqi and Wan, Xiaojun},
  journal={arXiv preprint arXiv:2309.09558},
  year={2023}
}

@inproceedings{
chang2024booookscore,
title={{BooookScore: A systematic exploration of book-length summarization in the era of {LLM}s}},
author={Yapei Chang and Kyle Lo and Tanya Goyal and Mohit Iyyer},
booktitle={ICLR},
year={2024},
}

@article{goyal2022news,
  title={{News summarization and evaluation in the era of gpt-3}},
  author={Goyal, Tanya and Li, Junyi Jessy and Durrett, Greg},
  journal={arXiv preprint arXiv:2209.12356},
  year={2022}
}

@inproceedings{pang-etal-2023-long,
    title = {{Long Document Summarization with Top-down and Bottom-up Inference"}},
    author = {Pang, Bo  and
      Nijkamp, Erik  and
      Kryscinski, Wojciech  and
      Savarese, Silvio  and
      Zhou, Yingbo  and
      Xiong, Caiming},
    booktitle = {EACL},
    year = {2023},
}

%%% offline inference [END] %%% 

%%% other [START] %%%
@misc{cxl,
	title = {{Compute express link}},
    author={CXL Consortium},
	url = {https://www.computeexpresslink.org},
	urldate = {2020}
}
%author={CXL Consortium},

@misc{pytorch,
  title={{Pytorch }},
	url = {https://pytorch.org/}
}

@article{cpistack,
  author={Emma, P. G.},
  journal={IBM Journal of Research and Development}, 
  title={Understanding some simple processor-performance limits}, 
  year={1997},
}

@misc{falcon,
        key = {{Falcon 4109}},
	title = {{Falcon 4109}},
	url = {https://www.h3platform.com/product-detail/overview/25},
	urldate = {2023-08-05},
}

@misc{mdadm,
	title = {{mdadm}},
	url = {https://git.kernel.org/pub/scm/utils/mdadm/mdadm.git/},
	urldate = {2024-05-21},
}

@misc{xilinx_opencl_buf,
	title = {{Xilinx OpenCL extension}},
	url = {https://xilinx.github.io/XRT/master/html/opencl_extension.html},
	urldate = {2024-05-21},
}

@misc{github_copilot,
	title = {{Github Copilot}},
	url = {https://github.com/features/copilot/},
	urldate = {2024-05-21},
}

%%% other [END] %%%


@article{lin2023awq,
  title={{Awq: Activation-aware weight quantization for llm compression and acceleration}},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@inproceedings{
frantar2023optq,
title={{{OPTQ}: Accurate Quantization for Generative Pre-trained Transformers}},
author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
booktitle={ICLR},
year={2023}
}


@article{shoeybi2019megatron,
  title={{Megatron-lm: Training multi-billion parameter language models using model parallelism}},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{mirzadeh2020improved,
  title={{Improved knowledge distillation via teacher assistant}},
  author={Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
  booktitle={AAAI},
  year={2020}
}


@inproceedings{kim-rush-2016-sequence,
    title = {{Sequence-Level Knowledge Distillation}},
    author = {Kim, Yoon  and
      Rush, Alexander M.},
    booktitle = {EMNLP}},
    year = {2016}
}


@inproceedings{sun-etal-2019-patient,
    title = {{Patient Knowledge Distillation for {BERT} Model Compression}},
    author = {Sun, Siqi  and
      Cheng, Yu  and
      Gan, Zhe  and
      Liu, Jingjing},
    editor = {Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun},
    booktitle = {EMNLP}},
    year = {2019},
}

@inproceedings{park2024agatha,
  title={{AGAThA: Fast and Efficient GPU Acceleration of Guided Sequence Alignment for Long Read Mapping}},
  author={Park, Seongyeon and Hong, Junguk and Song, Jaeyong and Kim, Hajin and Kim, Youngsok and Lee, Jinho},
  booktitle={PPoPP},
  year={2024}
}


@inproceedings{zhou2010impact,
  title={{The impact of YouTube recommendation system on video views}},
  author={Zhou, Renjie and Khemmarat, Samamon and Gao, Lixin},
  booktitle={SIGCOMM},
  year={2010}
}

@article{hendrycks2016gaussian,
  title={{Gaussian error linear units (gelus)}},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}


@article{tapa,
author = {Guo, Licheng and Chi, Yuze and Lau, Jason and Song, Linghao and Tian, Xingyu and Khatti, Moazin and Qiao, Weikang and Wang, Jie and Ustun, Ecenur and Fang, Zhenman and Zhang, Zhiru and Cong, Jason},
title = {{TAPA: A Scalable Task-parallel Dataflow Programming Framework for Modern FPGAs with Co-optimization of HLS and Physical Design}},
year = {2023},
volume = {16},
number = {4},
issn = {1936-7406},
journal = {ACM TRETS},
articleno = {63},
numpages = {31}
} %refined

@inproceedings{autobridge,
author = {Guo, Licheng and Chi, Yuze and Wang, Jie and Lau, Jason and Qiao, Weikang and Ustun, Ecenur and Zhang, Zhiru and Cong, Jason},
title = {{AutoBridge: Coupling Coarse-Grained Floorplanning and Pipelining for High-Frequency HLS Design on Multi-Die FPGAs}},
year = {2021},
booktitle = {FPGA},
pages = {81–92},
numpages = {12}
} %refined

@inproceedings{bam,
author = {Qureshi, Zaid and Mailthody, Vikram Sharma and Gelado, Isaac and Min, Seungwon and Masood, Amna and Park, Jeongmin and Xiong, Jinjun and Newburn, C. J. and Vainbrand, Dmitri and Chung, I-Hsin and Garland, Michael and Dally, William and Hwu, Wen-mei},
title = {{{GPU}-Initiated On-Demand High-Throughput Storage Access in the {BaM} System Architecture}},
year = {2023},
booktitle = {ASPLOS}
} %refined


@article{helios,
  title={{Helios: An Efficient Out-of-core GNN Training System on Terabyte-scale Graphs with In-memory Performance}},
  author={Sun, Jie and Sun, Mo and Zhang, Zheng and Xie, Jun and Shi, Zuocheng and Yang, Zihan and Zhang, Jie and Wu, Fei and Wang, Zeke},
  journal={arXiv preprint arXiv:2310.00837},
  year={2023}
} %refined

@misc{pybind11,
  key ={{pybind11}},
  title={{pybind11}},
	url = {https://github.com/pybind/pybind11},
	urldate = {2022-10-07},
} %refined

@misc{disutils,
  key ={{Distutils}},
  title={{Distutils}},
	url = {https://docs.python.org/3.9/library/distutils.html},
	urldate = {2022-10-07},
} %refined




%%%%%% ADD in ASPLOS %%%%%%
%%%%%% ADD in ASPLOS %%%%%%
%%%%%% ADD in ASPLOS %%%%%%

@inproceedings {infinigen,
author = {Wonbeom Lee and Jungi Lee and Junghwan Seo and Jaewoong Sim},
title = {{InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management}},
booktitle = {OSDI},
year = {2024},
}

@inproceedings{gmt,
author = {Chang, Chia-Hao and Han, Jihoon and Sivasubramaniam, Anand and Sharma Mailthody, Vikram and Qureshi, Zaid and Hwu, Wen-Mei},
title = {{GMT: GPU Orchestrated Memory Tiering for the Big Data Era}},
year = {2024},
booktitle = {ASPLOS}
}

@article{gqa,
  title={{GQA: Training Generalized Multi-query Transformer Models from Multi-head Checkpoints}},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

@article{llama2,
  title={{Llama 2: Open Foundation and Fine-tuned Chat Models}},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{flash_attn1,
 author = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
 booktitle = {NeurIPS},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 title = {{FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}},
 year = {2022}
}

@inproceedings{
flash_attn2,
title={{FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning}},
author={Tri Dao},
booktitle={ICLR},
year={2024}
}

@article{ring_attn,
  title={{Ring Attention with Blockwise Transformers for Near-infinite Context}},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2310.01889},
  year={2023}
}

@inproceedings{blockwise_attn,
 author = {Liu, Hao and Abbeel, Pieter},
 booktitle = {NeurIPS},
 title = {{Blockwise Parallel Transformers for Large Context Models}},
 year = {2023}
}


@inproceedings{h2o,
 author = {Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R\'{e}, Christopher and Barrett, Clark and Wang, Zhangyang "Atlas" and Chen, Beidi},
 booktitle = {NeurIPS},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 title = {{H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models}},
 year = {2023}
}


@article{mii,
  title={{Deepspeed-Fastgen: High-throughput Text Generation for LLMs via MII and Deepspeed-Inference}},
  author={Holmes, Connor and Tanaka, Masahiro and Wyatt, Michael and Awan, Ammar Ahmad and Rasley, Jeff and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Qin, Heyang and Bakhtiari, Arash and Kurilenko, Lev and others},
  journal={arXiv preprint arXiv:2401.08671},
  year={2024}
}

@article{rope,
  title={{Roformer: Enhanced Transformer with Rotary Position Embedding}},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
}



@INPROCEEDINGS{aquabolt,
  author={Kim, Jin Hyun and Kang, Shin-haeng and Lee, Sukhan and Kim, Hyeonsu and Song, Woongjae and Ro, Yuhwan and Lee, Seungwon and Wang, David and Shin, Hyunsung and Phuah, Bengseng and Choi, Jihyun and So, Jinin and Cho, YeonGon and Song, JoonHo and Choi, Jangseok and Cho, Jeonghyeon and Sohn, Kyomin and Sohn, Youngsoo and Park, Kwangil and Kim, Nam Sung},
  booktitle={2021 IEEE Hot Chips 33 Symposium (HCS)}, 
  title={Aquabolt-XL: Samsung HBM2-PIM with in-memory processing for ML accelerators and beyond}, 
  year={2021},
  volume={},
  number={},
  pages={1-26},
  keywords={Wires;Memory management;Bandwidth;Energy efficiency},
  doi={10.1109/HCS52781.2021.9567191}}


@article{silu,
title = {{Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning}},
journal = {Neural Networks},
volume = {107},
pages = {3-11},
year = {2018},
author = {Stefan Elfwing and Eiji Uchibe and Kenji Doya}
}


@inproceedings{uvm,
author = {Allen, Tyler and Ge, Rong},
title = {{In-depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing}},
year = {2021},
booktitle = {SC}
}

@inproceedings {ann_smartssd,
author = {Bing Tian and Haikun Liu and Zhuohui Duan and Xiaofei Liao and Hai Jin and Yu Zhang},
title = {{Scalable Billion-point Approximate Nearest Neighbor Search Using {SmartSSDs}}},
booktitle = {USENIX ATC},
year = {2024},
}



@InProceedings{smoothquant,
  title = 	 {{SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models}},
  author =       {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle = 	 {ICML},
  year = 	 {2023}
}
