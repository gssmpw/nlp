\section{Related Work}
%TBA: Faster Large Language Model Training Using SSD-Based Activation Offloading

\subsection{LLM Inference}
\textbf{LLM Inference Acceleration.}
%\subsection{Real-time LLM Serving}
%Orca, vLLM.
%\subsection{Memory efficient Attetion____}
%FlashAttention, Blockwise Attention, Ring-attention.
% Due to the growing demand for fast and efficient generative model-serving systems, 
Recent works have proposed various ways to accelerate LLM inference.
These works either employed kernel optimizations____, offload memory-bound computation to PIM processors____, quantized parameters____, exploit model parallelism____, or devise new batching strategies____.
These proposed methods have significantly improved the throughput of LLM inference systems, but they focus on interactive use cases.
%Previous research has addressed similar issues in the context of internal/external fragments in KV caches____, but the approach does not target offloading-based batched inferences.
LLMs are also actively applied to offline scenarios, such as benchmarking____ and information extraction____. 
We target scenarios that are less sensitive to latency factors in batches over a large number of tokens running LLM inference.



\textbf{Offloaded LLM Inference.}
%issue of LLM inference is the requirement of enough memory size for accelerators (e.g., GPUs).
%This huge memory requirement comes from large model sizes of LLMs and long input sequence lengths.
%Motivated by the above insight, many approaches~\cite {flexgen, deepspeedinf, petals, llminaflash} proposed strategies for serving LLM inference with limited GPU memory using host memory or storage as extended memory. 
%Notably, FlexGen____ efficiently addresses the significant I/O overhead of large model weight by enlarging the batch size.
%It breaks the barrier to enlarging batch size, enabling even higher throughput on a single machine.
%Unfortunately, with batched inference____, the KV cache becomes the most important factor for throughput than the model itself.
As model sizes of recent LLMs grew exponentially, efficiently deploying these models on limited GPU resources became of crucial importance.
Numerous studies on model training have addressed this issue by offloading data and computation to neighboring devices____.
Therefore, it was a natural next step to apply similar techniques for model inference.
As a result, recent works offloaded data to storage systems____ and also offloaded computation to the host CPU____.
Similar to \thiswork, these works offload computation from the GPU in order to alleviate the computational burdens of the GPU and reduce data movement between the devices.
However, \thiswork is able to utilize much higher bandwidth compared to other works by employing CSDs.

% \textbf{LLM Compression.}
% As we target LLM inference with limited resources, one might suggest a light weight model and think the scenrio where the large model is used is unrealistic.
% However, as studied in many previous works____, the performance of LLM increases as the parameter of the LLM increases.
% There are many approaches which aim to retain the performance of the large model while reducing the required memory size of the language model.
% Representative methods are quantization____ and knowledge distillation____.
% AWQ____ showed that it can run Llama-2-13B with laptop GPU and 8GB memory by preserving only 1$\%$ sailient weights and applying INT4 groupwise quantization.
% LLM.int8()____ suggested INT8 matrix multiplication in the transformer layer while maintaining the precision of outlier features.
% However, these methods still require a large amount of memory as the sequence length largely increases.
% In this case, LLM inference with near storage processing can be an alternative because it utilizes much larger storage and exceed the limit of main memory.
% Knowledge Distillation (KD)____ is widely researched in the machine learning venue and many works have successfully reduced the number of model parameters in LLM.
% However, this knowledge distillation needs additional training phase which trains the student model.
% It also needs a large teacher model to generate the guide for the student model.
% Therefore, without offloaded training____, KD with large teacher model is hard to be conducted in the limited environment.

% \textbf{LLM sparsity}
% %Using the constant information holding(Top-K) is less effective thatn relative informantion holding(quantization).
% %As we target LLM inference with limited resources, one might suggest sparsification and think the scenario.
% %As considering recent trends in exploiting sparsity, which provides constatnt memory consumption for extreme long sequences.
% Consequently, we could consider using quantization methods to reducing the I/O costs for loading weights from system memory or storage devices to host.
% Some work exploit constant memory usage for KV cache.
% However, we think such constant memory usage will suffer low accuracy for extremely longer sequences.
% We will discuss this in evaluation section(@@).
% The 4-bit integer values are dequantized back to FP16 before computation in the host processor, so there is no need to change the computation modules of the host.
% As LLM-QAT ____ shows, accuracy drop due to precision loss during compression can be minimized to a negligible size by using fine-grained per-channel quantization.
% AWQ____ showed that it can run Llama-2-13B with laptop GPU and 8GB memory by preserving only 1$\%$ sailient weights and applying INT4 groupwise quantization.
% LLM.int8() ____ suggested INT8 matrix multiplication in the transformer layer while maintaining the precision of outlier features.
% However, these methods still require a large amount of memory as the sequence length largely increases.
% As shown by FlexGen, the I/O cost overhead of storage-offloading inference can be minimized through quantization.
% However, in situation where attention computation is delegated to the CPU, because it has huge overhead for dequantization before it computes, FlexGen does not support cpu-delegation and KV cache quantization at the same time.
% Because KV cache is compressed with fine-grained per channel group, we can utilize its parallelism for multiple CSD to dequantize. Through its support, we could enable utilizing compuation delegation and KV cache quantization at the same time, which are essential for reducing I/O cost overhead in storage-offloading LLM inference.
% Also, for long sequences, the inference accuracy drop is not negligible without methods such as PTQ and QAT. As it is shown in LLM-QAT, accuracy drop can be avoided by quantization aware training with fine-grained per-channel compression. Our framework both supports Quantization-Aware training and Quantized Inference for storage-offloading LLM and enables users to inference long sequences while avoiding accuracy drop.
% (2) Sparsity 
% H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models.
% Serpens: A High Bandwidth Memory Based Accelerator for General-Purpose Sparse Matrix-Vector Multiplication.

\subsection{Near-Data Processing}
% The concept of near-data processing %, with its potential for massive parallel processing and reduced data movement, 
% has been studied for well over several decades.
% Earlier works attempted to integrate processors on DRAM chips____.
% However, these works were not incorporated into real-world DRAMs for several reasons, including high costs and the inefficiency of logic on memory technology.

% In the 2010s, with the slowdown of Dennard scaling and the surge of AI, near-data processing started gaining more interest from the research community and industry.
% With the appearance of 3D stacked memory____, many works proposed to utilize the logic dies for various applications ____ until the product was discontinued in the late 2010s.
% Other works proposed implementing minor changes in the DRAM architecture to enable simple operations ____.

\textbf{Processing In Memory.}
In the late 2010s, Processing In Memory (PIM) designs supporting more general computations started to appear____, and real-world PIM products were introduced____.
As these designs enabled more general computations, some works attempted to accelerate end-to-end LLM inference systems____
%However, PIM's lack of computation capabilities limited performance gains compared to systems utilizing accelerators with strong computational capabilities.
Other works____ have explored PIM systems accompanied by GPUs or NPUs, where only selected memory-bound operations are offloaded to PIM processors.
%AttAcc____ provides an accelerator (leveraging PIM) for attention layers to minimize data movement for Key-Value (KV) matrices.
%Similarly, NeuPIMs____ proposes a novel system for PIM acceleration along with operation scheduling to facilitate end-to-end inference of LLMs.
Although these works successfully increase throughput by offloading memory-bound applications to PIM, they are limited in that they have been evaluated in simulated environments only.

\textbf{Near-Storage Processing.} 
Various works explored ways to enable near-storage processing.
Earlier works aimed to design ``active'' or ``intelligent'' disks____. 
However, due to the massive advancements in the computing capabilities of existing processors, the idea of on-disk processing slowly faded away.
%Near-storage processing regained the interest of research communities with the appearance of modern SSDs, which had much higher internal bandwidth than the I/O interface.
Multiple studies proposed to accelerate various applications by either processing data utilizing the embedded CPU____ or integrating processors to each internal channel____ to exploit the internal bandwidth.
Despite the effort, the embedded cores turned out to be low in computational power.
This led to utilizing dedicated processors for SSDs, such as ASICs____ or FPGAs____.
These computational SSD products are available on the market____ and are being used to accelerate various applications such as query processing____, training deep learning models____, and deep learning model inference____.