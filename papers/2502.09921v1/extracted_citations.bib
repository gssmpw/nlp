@ARTICLE{IRAM,
  author={Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.},
  journal={IEEE Micro}, 
  title={{A case for intelligent {RAM}}}, 
  year={1997},
  volume={17},
  number={2},
  pages={34-44}
}

@inproceedings{NEURIPS2023_89e44582,
 author = {Guha, Neel and Nyarko, Julian and Ho, Daniel and R\'{e}, Christopher and Chilton, Adam and K, Aditya and Chohlas-Wood, Alex and Peters, Austin and Waldon, Brandon and Rockmore, Daniel and Zambrano, Diego and Talisman, Dmitry and Hoque, Enam and Surani, Faiz and Fagan, Frank and Sarfaty, Galit and Dickinson, Gregory and Porat, Haggai and Hegland, Jason and Wu, Jessica and Nudell, Joe and Niklaus, Joel and Nay, John and Choi, Jonathan and Tobia, Kevin and Hagan, Margaret and Ma, Megan and Livermore, Michael and Rasumov-Rahe, Nikon and Holzenberger, Nils and Kolt, Noam and Henderson, Peter and Rehaag, Sean and Goel, Sharad and Gao, Shang and Williams, Spencer and Gandhi, Sunny and Zur, Tom and Iyer, Varun and Li, Zehua},
 booktitle = {NeurIPS},
 title = {{LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models}},
 year = {2023}
}

@inproceedings{NEURIPS2023_91f18a12,
 author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph E and Stoica, Ion},
 booktitle = {NeurIPS},
 title = {{Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}},
 year = {2023}
}

@inproceedings{active_disk,
author = {Acharya, Anurag and Uysal, Mustafa and Saltz, Joel},
title = {{Active disks: programming model, algorithms and evaluation}},
year = {1998},
booktitle = {ASPLOS}
}

@article{active_disk_large,
  author={Riedel, E. and Faloutsos, C. and Gibson, G.A. and Nagle, D.},
  journal={Computer}, 
  title={{Active disks for large-scale data processing}}, 
  year={2001},
  volume={34},
  number={6},
  pages={68-74}
}

@inproceedings{active_storage_large,
author = {Riedel, Erik and Gibson, Garth A. and Faloutsos, Christos},
title = {{Active Storage for Large-Scale Data Mining and Multimedia}},
year = {1998},
booktitle = {VLDB}
}

@inproceedings{aim,
  title={{A 1ynm 1.25 V 8Gb, 16Gb/s/pin GDDR6-based accelerator-in-memory supporting 1TFLOPS MAC operation and various activation functions for deep-learning applications}},
  author={Lee, Seongju and Kim, Kyuyoung and Oh, Sanghoon and Park, Joonhong and Hong, Gimoon and Ka, Dongyoon and Hwang, Kyudong and Park, Jeongje and Kang, Kyeongpil and Kim, Jungyeon and others},
  booktitle={ISSCC},
  year={2022}
}

@inproceedings{ambit,
  title={{Ambit: In-memory accelerator for bulk bitwise operations using commodity DRAM technology}},
  author={Seshadri, Vivek and Lee, Donghyuk and Mullins, Thomas and Hassan, Hasan and Boroumand, Amirali and Kim, Jeremie and Kozuch, Michael A and Mutlu, Onur and Gibbons, Phillip B and Mowry, Todd C},
  booktitle={MICRO},
  year={2017}
}

@article{atom,
  title={{Atom: Low-bit quantization for efficient and accurate llm serving}},
  author={Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris},
  journal={arXiv preprint arXiv:2310.19102},
  year={2023}
}

@inproceedings{attacc,
author = {Park, Jaehyun and Choi, Jaewan and Kyung, Kwanhee and Kim, Michael Jaemin and Kwon, Yongsuk and Kim, Nam Sung and Ahn, Jung Ho},
title = {{AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference}},
booktitle = {ASPLOS},
year = {2024}
}

@ARTICLE{axdimm,
  author={Ke, Liu and Zhang, Xuan and So, Jinin and Lee, Jong-Geon and Kang, Shin-Haeng and Lee, Sukhan and Han, Songyi and Cho, YeonGon and Kim, Jin Hyun and Kwon, Yongsuk and Kim, KyungSoo and Jung, Jin and Yun, Ilkwon and Park, Sung Joo and Park, Hyunsun and Song, Joonho and Cho, Jeonghyeon and Sohn, Kyomin and Kim, Nam Sung and Lee, Hsien-Hsin S.},
  journal={IEEE Micro}, 
  title={{Near-Memory Processing in Action: Accelerating Personalized Recommendation With {AxDIMM}}}, 
  year={2022},
  volume={42},
  number={1},
  pages={116-127},
}

@inproceedings{biscuit,
author = {Gu, Boncheol and Yoon, Andre S. and Bae, Duck-Ho and Jo, Insoon and Lee, Jinyoung and Yoon, Jonghyun and Kang, Jeong-Uk and Kwon, Moonsang and Yoon, Chanho and Cho, Sangyeun and Jeong, Jaeheon and Chang, Duckhyun},
title = {{Biscuit: a framework for near-data processing of big data workloads}},
year = {2016},
booktitle = {ISCA}
}

@inproceedings{bufcmp,
  title={{Buffered compares: Excavating the hidden parallelism inside DRAM architectures with lightweight logic}},
  author={Lee, Jinho and Ahn, Jung Ho and Choi, Kiyoung},
  booktitle={DATE},
  year={2016}
}

@inproceedings{chameleon,
  title={{Chameleon: Versatile and practical near-{DRAM} acceleration architecture for large memory systems}},
  author={Asghari-Moghaddam, Hadi and Son, Young Hoon and Ahn, Jung Ho and Kim, Nam Sung},
  booktitle={MICRO},
  year={2016}
}

@inproceedings{computedram,
  title={{ComputeDRAM}: In-memory compute using off-the-shelf DRAMs},
  author={Gao, Fei and Tziantzioulis, Georgios and Wentzlaff, David},
  booktitle={MICRO},
  year={2019}
}

@inproceedings{deepspeedinf,
  title={{Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale}},
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others},
  booktitle={SC},
  year={2022}
}

@inproceedings{deepstore,
  title={{Deepstore: In-storage acceleration for intelligent queries}},
  author={Mailthody, Vikram Sharma and Qureshi, Zaid and Liang, Weixin and Feng, Ziyan and De Gonzalo, Simon Garcia and Li, Youjie and Franke, Hubertus and Xiong, Jinjun and Huang, Jian and Hwu, Wen-mei},
  booktitle={MICRO},
  year={2019}
}

@inproceedings{earlysmartssd,
  author={Kang, Yangwook and Kee, Yang-suk and Miller, Ethan L. and Park, Chanik},
  booktitle={MSST}, 
  title={{Enabling cost-effective data processing with smart SSD}}, 
  year={2013}
}

@article{exadata,
  title={{A technical overview of the oracle exadata database machine and exadata storage server}},
  author={Weiss, Ronald},
  journal={Oracle White Paper},
  year={2012}
}

@article{flashllm,
author = {Xia, Haojun and Zheng, Zhen and Li, Yuchao and Zhuang, Donglin and Zhou, Zhongzhu and Qiu, Xiafei and Li, Yong and Lin, Wei and Song, Shuaiwen Leon},
title = {{Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity}},
year = {2023},
journal = {Proc. VLDB Endow.}
}

@InProceedings{flexgen,
  title = {{FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU}},
  author = {Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and Re, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle = {ICML},
  year = {2023},
}

@inproceedings{flexram,
   Author = {Kang, Yi and Huang, Wei and Yoo, Seung-Moon and Keen, D and Ge, Zhenzhou and Lam, V and Pattnaik, P and Torrellas, J},
   Title = {{FlexRAM: Toward an advanced intelligent memory system}},
   BookTitle = {ICCD},
      Year = {1999} 
}

@inproceedings{genstore,
author = {Mansouri Ghiasi, Nika and Park, Jisung and Mustafa, Harun and Kim, Jeremie and Olgun, Ataberk and Gollwitzer, Arvid and Senol Cali, Damla and Firtina, Can and Mao, Haiyu and Almadhoun Alserr, Nour and Ausavarungnirun, Rachata and Vijaykumar, Nandita and Alser, Mohammed and Mutlu, Onur},
title = {{GenStore: a high-performance in-storage processing system for genome sequence analysis}},
year = {2022},
booktitle = {ASPLOS}
}

@article{goyal2022news,
  title={{News summarization and evaluation in the era of gpt-3}},
  author={Goyal, Tanya and Li, Junyi Jessy and Durrett, Greg},
  journal={arXiv preprint arXiv:2209.12356},
  year={2022}
}

@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {NeurIPS},
 title = {{Language Models are Few-Shot Learners}},
 year = {2020}
}

@article{gpt4,
  title={{Gpt-4 technical report}},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{gradpim,
  title={{GradPIM: A practical processing-in-DRAM architecture for gradient descent}},
  author={Kim, Heesu and Park, Hanmin and Kim, Taehyun and Cho, Kwanheum and Lee, Eojin and Ryu, Soojung and Lee, Hyuk-Jae and Choi, Kiyoung and Lee, Jinho},
  booktitle={HPCA},
  year={2021},
}

@article{guo2023gpt4graph,
  title={{Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking}},
  author={Guo, Jiayan and Du, Lun and Liu, Hengyu},
  journal={arXiv preprint arXiv:2305.15066},
  year={2023}
}

@inproceedings{hbmpim,
  author={Lee, Sukhan and Kang, Shin-haeng and Lee, Jaehoon and Kim, Hyeonsu and Lee, Eojin and Seo, Seungwoo and Yoon, Hosang and Lee, Seungwon and Lim, Kyounghwan and Shin, Hyunsung and Kim, Jinhyun and Seongil, O and Iyer, Anand and Wang, David and Sohn, Kyomin and Kim, Nam Sung},
  booktitle={ISCA}, 
  title={{Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology : Industrial Product}}, 
  year={2021},
}

@article{hetegen,
author={Zhao, Xuanlei and Jia, Bin and Zhou, Haotian and Liu, Ziming and Cheng, Shenggan and You, Yang},  
title={{HeteGen: Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices}},
journal={arXiv preprint arXiv:2403.01164},
year={2024}
}

@inproceedings{hmc,
  title={{Hybrid memory cube ({HMC})}},
  author={Pawlowski, J Thomas},
  booktitle={HCS},
  year={2011},
}

@article{idisk,
author = {Keeton, Kimberly and Patterson, David A. and Hellerstein, Joseph M.},
title = {{A case for intelligent disks (IDISKs)}},
year = {1998},
volume = {27},
number = {3},
journal = {SIGMOD Rec.}
}

@inproceedings{inspire,
author = {Lin, Jilan and Liang, Ling and Qu, Zheng and Ahmad, Ishtiyaque and Liu, Liu and Tu, Fengbin and Gupta, Trinabh and Ding, Yufei and Xie, Yuan},
title = {{{INSPIRE}: In-Storage Private Information Retrieval via Protocol and Architecture Co-Design}},
year = {2022},
booktitle = {ISCA},
}

@inproceedings{kim-rush-2016-sequence,
    title = {{Sequence-Level Knowledge Distillation}},
    author = {Kim, Yoon  and
      Rush, Alexander M.},
    booktitle = {EMNLP}},
    year = {2016}
}

@article{kvquant,
  title={{KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization}},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}

@article{leviathan,
  author={Jin, Yunho and Kim, Shine and Ham, Tae Jun and Lee, Jae W.},
  journal={IEEE Transactions on Computers}, 
  title={{Architecting a Flash-Based Storage System for Low-Cost Inference of Extreme-Scale DNNs}}, 
  year={2022}
}

@article{lin2023awq,
  title={{Awq: Activation-aware weight quantization for llm compression and acceleration}},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@article{llm-qat,
  title={{Llm-qat: Data-free quantization aware training for large language models}},
  author={Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas},
  journal={arXiv preprint arXiv:2305.17888},
  year={2023}
}

@article{llminaflash,
  title={{Llm in a flash: Efficient large language model inference with limited memory}},
  author={Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C and Rastegari, Mohammad and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2312.11514},
  year={2023}
}

@inproceedings{llmint8,
  title={{Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale}},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{mirzadeh2020improved,
  title={{Improved knowledge distillation via teacher assistant}},
  author={Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
  booktitle={AAAI},
  year={2020}
}

@article{narayan2018don,
  title={{Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization}},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1808.08745},
  year={2018}
}

@inproceedings{netezza,
  title={{Introduction to the IBM Netezza warehouse appliance}},
  author={Singh, Malcolm and Leonhardi, Ben},
  booktitle={CASCON},
  year={2011}
}

@inproceedings{neupims,
author = {Heo, Guseul and Lee, Sangyeop and Cho, Jaehong and Choi, Hyunmin and Lee, Sanghyeon and Ham, Hyungkyu and Kim, Gwangsun and Mahajan, Divya and Park, Jongse},
title = {{NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing}},
booktitle = {ASPLOS},
year = {2024}
}

@inproceedings{newton,
  author={He, Mingxuan and Song, Choungki and Kim, Ilkon and Jeong, Chunseok and Kim, Seho and Park, Il and Thottethodi, Mithuna and Vijaykumar, T. N.},
  booktitle={MICRO}, 
  title={{Newton: A DRAM-maker’s Accelerator-in-Memory (AiM) Architecture for Machine Learning}}, 
  year={2020},
}

@inproceedings{pang-etal-2023-long,
    title = {{Long Document Summarization with Top-down and Bottom-up Inference"}},
    author = {Pang, Bo  and
      Nijkamp, Erik  and
      Kryscinski, Wojciech  and
      Savarese, Silvio  and
      Zhou, Yingbo  and
      Xiong, Caiming},
    booktitle = {EACL},
    year = {2023},
}

@inproceedings{pei,
  title={{PIM-enabled instructions: a low-overhead, locality-aware processing-in-memory architecture}},
  author={Ahn, Junwhan and Yoo, Sungjoo and Mutlu, Onur and Choi, Kiyoung},
  booktitle={ISCA},
  year={2015}
}

@article{piccolo,
  title={{A Case for In-Memory Random Scatter-Gather for Fast Graph Processing}},
  author={Shin, Changmin and Kwon, Taehee and Song, Jaeyong and Ju, Jae Hyung and Liu, Frank and Choi, Yeonkyu and Lee, Jinho},
  journal={CAL},
  year={2024}
}

@inproceedings{pimdl,
author = {Li, Cong and Zhou, Zhe and Wang, Yang and Yang, Fan and Cao, Ting and Yang, Mao and Liang, Yun and Sun, Guangyu},
title = {{PIM-DL: Expanding the Applicability of Commodity DRAM-PIMs for Deep Learning via Algorithm-System Co-Optimization}},
year = {2024},
booktitle = {ASPLOS}
}

@article{powerinfer,
  title={{Powerinfer: Fast large language model serving with a consumer-grade gpu}},
  author={Song, Yixin and Mi, Zeyu and Xie, Haotong and Chen, Haibo},
  journal={arXiv preprint arXiv:2312.12456},
  year={2023}
}

@article{pu2023summarization,
  title={{Summarization is (almost) dead}},
  author={Pu, Xiao and Gao, Mingqi and Wan, Xiaojun},
  journal={arXiv preprint arXiv:2309.09558},
  year={2023}
}

@inproceedings{rmssd,
  title={{RM-SSD: In-storage computing for large-scale recommendation inference}},
  author={Sun, Xuan and Wan, Hu and Li, Qiao and Yang, Chia-Lin and Kuo, Tei-Wei and Xue, Chun Jason},
  booktitle={HPCA},
  year={2022},
}

@inproceedings{rowclone,
  title={{RowClone: fast and energy-efficient in-DRAM bulk data copy and initialization}},
  author = {Seshadri, Vivek and Kim, Yoongu and Fallin, Chris and Lee, Donghyuk and Ausavarungnirun, Rachata and Pekhimenko, Gennady and Luo, Yixin and Mutlu, Onur and Gibbons, Phillip B. and Kozuch, Michael A. and Mowry, Todd C.},
  booktitle={MICRO},
  year={2013}
}

@article{sarathi,
  title={{Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills}},
  author={Agrawal, Amey and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Ramjee, Ramachandran},
  journal={arXiv preprint arXiv:2308.16369},
  year={2023}
}

@inproceedings{scaletrans,
  title={{Efficiently scaling transformer inference}},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  booktitle={MLSys},
  year={2023}
}

@inproceedings{secndp,
  title={{SecNDP: Secure Near-Data Processing with Untrusted Memory}},
  author={W. Xiong and L. Ke and D. Jankov and M. Kounavis and X. Wang and E. Northup and J. Yang and B. Acun and C. Wu and P. Peter Tang and G. Edward Suh and X. Zhang and H. S. Lee},
  booktitle={HPCA},
  year={2022},
}

@inproceedings{self_sorting_ssd,
  author={Quero, Luis Cavazos and Lee, Young-Sik and Kim, Jin-Soo},
  booktitle={MSST}, 
  title={{Self-sorting SSD: Producing sorted data inside active SSDs}}, 
  year={2015}
}

@article{shoeybi2019megatron,
  title={{Megatron-lm: Training multi-billion parameter language models using model parallelism}},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{smartinfinity,
  title={{Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System}},
  author={Jang, Hongsun and Song, Jaeyong and Jung, Jaewon and Park, Jaeyoung and Kim, Youngsok and Lee, Jinho},
  booktitle={HPCA},
  year={2024}
}

@inproceedings{smartsage,
author = {Lee, Yunjae and Chung, Jinha and Rhu, Minsoo},
title = {{SmartSAGE: training large-scale graph neural networks using in-storage processing architectures}},
year = {2022},
booktitle = {ISCA}
}

@misc{smartssd,
  key ={{SmartSSD}},
  title={{SmartSSD}},
	url = {https://www.xilinx.com/applications/data-center/computational-storage/smartssd.html},
	urldate = {2024-05-13},
}

@inproceedings{smartssd_datamine,
author = {Bae, Duck-Ho and Kim, Jin-Hyung and Kim, Sang-Wook and Oh, Hyunok and Park, Chanik},
title = {{Intelligent SSD: a turbo for big data mining}},
year = {2013},
booktitle = {CIKM}
}

@inproceedings{smartssd_dlrm,
author = {Soltaniyeh, Mohammadreza and Lagrange Moutinho Dos Reis, Veronica and Bryson, Matt and Yao, Xuebin and Martin, Richard P. and Nagarakatte, Santosh},
title = {{Near-Storage Processing for Solid State Drive Based Recommendation Inference with SmartSSDs®}},
year = {2022},
booktitle = {ICPE}
}

@inproceedings{smartssd_query,
author = {Do, Jaeyoung and Kee, Yang-Suk and Patel, Jignesh M. and Park, Chanik and Park, Kwanghyun and DeWitt, David J.},
title = {{Query processing on smart SSDs: opportunities and challenges}},
year = {2013},
booktitle = {SIGMOD}
}

@inproceedings{sun-etal-2019-patient,
    title = {{Patient Knowledge Distillation for {BERT} Model Compression}},
    author = {Sun, Siqi  and
      Cheng, Yu  and
      Gan, Zhe  and
      Liu, Jingjing},
    editor = {Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun},
    booktitle = {EMNLP}},
    year = {2019},
}

@inproceedings{tensordimm,
  title={{{TensorDIMM}: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning}},
  author={Kwon, Youngeun and Lee, Yunjae and Rhu, Minsoo},
  booktitle={MICRO},
  year={2019}
}

@inproceedings{tesseract,
  title={{A scalable processing-in-memory accelerator for parallel graph processing}},
  author={Ahn, Junwhan and Hong, Sungpack and Yoo, Sungjoo and Mutlu, Onur and Choi, Kiyoung},
  booktitle={ISCA},
  year={2015}
}

@inproceedings{transpim,
author={Zhou, Minxuan and Xu, Weihong and Kang, Jaeyoung and Rosing, Tajana}, 
title={{TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer}}, 
booktitle={HPCA},
year={2022}
}

@inproceedings{upmem,
  title={{The true Processing In Memory accelerator}},
  author={Devaux, Fabrice},
  booktitle={HCS},
  year={2019}
}

@inproceedings{vllm,
author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
title = {{Efficient Memory Management for Large Language Model Serving with PagedAttention}},
booktitle = {SOSP},
year = {2023}
}

@inproceedings{zeroinfinity,
author = {Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
title = {{ZeRO-infinity: breaking the GPU memory wall for extreme scale deep learning}},
year = {2021},
booktitle = {SC}
}

@inproceedings{zerooffload,
  title={{Zero-offload: democratizing billion-scale model training}},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={USENIX ATC},
  year={2021}
}

