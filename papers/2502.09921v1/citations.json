[
  {
    "index": 0,
    "papers": [
      {
        "key": "flashattn",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "ringattn",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "deepspeedinf",
        "author": "Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others",
        "title": "{Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale}"
      },
      {
        "key": "flashllm",
        "author": "Xia, Haojun and Zheng, Zhen and Li, Yuchao and Zhuang, Donglin and Zhou, Zhongzhu and Qiu, Xiafei and Li, Yong and Lin, Wei and Song, Shuaiwen Leon",
        "title": "{Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "transpim",
        "author": "Zhou, Minxuan and Xu, Weihong and Kang, Jaeyoung and Rosing, Tajana",
        "title": "{TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer}"
      },
      {
        "key": "pimdl",
        "author": "Li, Cong and Zhou, Zhe and Wang, Yang and Yang, Fan and Cao, Ting and Yang, Mao and Liang, Yun and Sun, Guangyu",
        "title": "{PIM-DL: Expanding the Applicability of Commodity DRAM-PIMs for Deep Learning via Algorithm-System Co-Optimization}"
      },
      {
        "key": "neupims",
        "author": "Heo, Guseul and Lee, Sangyeop and Cho, Jaehong and Choi, Hyunmin and Lee, Sanghyeon and Ham, Hyungkyu and Kim, Gwangsun and Mahajan, Divya and Park, Jongse",
        "title": "{NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing}"
      },
      {
        "key": "attacc",
        "author": "Park, Jaehyun and Choi, Jaewan and Kyung, Kwanhee and Kim, Michael Jaemin and Kwon, Yongsuk and Kim, Nam Sung and Ahn, Jung Ho",
        "title": "{AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "kvquant",
        "author": "Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir",
        "title": "{KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization}"
      },
      {
        "key": "atom",
        "author": "Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris",
        "title": "{Atom: Low-bit quantization for efficient and accurate llm serving}"
      },
      {
        "key": "llm-qat",
        "author": "Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas",
        "title": "{Llm-qat: Data-free quantization aware training for large language models}"
      },
      {
        "key": "llmint8",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "{Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "deepspeedinf",
        "author": "Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others",
        "title": "{Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale}"
      },
      {
        "key": "scaletrans",
        "author": "Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff",
        "title": "{Efficiently scaling transformer inference}"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "orca",
        "author": "Gyeong-In Yu and Joo Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun",
        "title": "{Orca: A Distributed Serving System for {Transformer-Based} Generative Models}"
      },
      {
        "key": "sarathi",
        "author": "Agrawal, Amey and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Ramjee, Ramachandran",
        "title": "{Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills}"
      },
      {
        "key": "dvabatch",
        "author": "Weihao Cui and Han Zhao and Quan Chen and Hao Wei and Zirui Li and Deze Zeng and Chao Li and Minyi Guo",
        "title": "{{DVABatch}: Diversity-aware {Multi-Entry} {Multi-Exit} Batching for Efficient Processing of {DNN} Services on {GPUs}}"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "vllm",
        "author": "Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion",
        "title": "{Efficient Memory Management for Large Language Model Serving with PagedAttention}"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liang2023holistic",
        "author": "Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Alexander Cosgrove and Christopher D Manning and Christopher Re and Diana Acosta-Navas and Drew Arad Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue WANG and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri S. Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Andrew Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda",
        "title": "{Holistic Evaluation of Language Models}"
      },
      {
        "key": "NEURIPS2023_89e44582",
        "author": "Guha, Neel and Nyarko, Julian and Ho, Daniel and R\\'{e}, Christopher and Chilton, Adam and K, Aditya and Chohlas-Wood, Alex and Peters, Austin and Waldon, Brandon and Rockmore, Daniel and Zambrano, Diego and Talisman, Dmitry and Hoque, Enam and Surani, Faiz and Fagan, Frank and Sarfaty, Galit and Dickinson, Gregory and Porat, Haggai and Hegland, Jason and Wu, Jessica and Nudell, Joe and Niklaus, Joel and Nay, John and Choi, Jonathan and Tobia, Kevin and Hagan, Margaret and Ma, Megan and Livermore, Michael and Rasumov-Rahe, Nikon and Holzenberger, Nils and Kolt, Noam and Henderson, Peter and Rehaag, Sean and Goel, Sharad and Gao, Shang and Williams, Spencer and Gandhi, Sunny and Zur, Tom and Iyer, Varun and Li, Zehua",
        "title": "{LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models}"
      },
      {
        "key": "NEURIPS2023_91f18a12",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph E and Stoica, Ion",
        "title": "{Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}"
      },
      {
        "key": "guo2023gpt4graph",
        "author": "Guo, Jiayan and Du, Lun and Liu, Hengyu",
        "title": "{Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "narayan2018don",
        "author": "Narayan, Shashi and Cohen, Shay B and Lapata, Mirella",
        "title": "{Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization}"
      },
      {
        "key": "pu2023summarization",
        "author": "Pu, Xiao and Gao, Mingqi and Wan, Xiaojun",
        "title": "{Summarization is (almost) dead}"
      },
      {
        "key": "chang2024booookscore",
        "author": "Yapei Chang and Kyle Lo and Tanya Goyal and Mohit Iyyer",
        "title": "{BooookScore: A systematic exploration of book-length summarization in the era of {LLM}s}"
      },
      {
        "key": "goyal2022news",
        "author": "Goyal, Tanya and Li, Junyi Jessy and Durrett, Greg",
        "title": "{News summarization and evaluation in the era of gpt-3}"
      },
      {
        "key": "pang-etal-2023-long",
        "author": "Pang, Bo  and\nNijkamp, Erik  and\nKryscinski, Wojciech  and\nSavarese, Silvio  and\nZhou, Yingbo  and\nXiong, Caiming",
        "title": "{Long Document Summarization with Top-down and Bottom-up Inference\"}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "flexgen",
        "author": "Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and Re, Christopher and Stoica, Ion and Zhang, Ce",
        "title": "{FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU}"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "flexgen",
        "author": "Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and Re, Christopher and Stoica, Ion and Zhang, Ce",
        "title": "{FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU}"
      },
      {
        "key": "vllm",
        "author": "Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion",
        "title": "{Efficient Memory Management for Large Language Model Serving with PagedAttention}"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zerooffload",
        "author": "Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong",
        "title": "{Zero-offload: democratizing billion-scale model training}"
      },
      {
        "key": "zeroinfinity",
        "author": "Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong",
        "title": "{ZeRO-infinity: breaking the GPU memory wall for extreme scale deep learning}"
      },
      {
        "key": "flashneuron",
        "author": "Jonghyun Bae and Jongsung Lee and Yunho Jin and Sam Son and Shine Kim and Hakbeom Jang and Tae Jun Ham and Jae W. Lee",
        "title": "{FlashNeuron}: {SSD-Enabled} {Large-Batch} Training of Very Deep Neural Networks"
      },
      {
        "key": "smartinfinity",
        "author": "Jang, Hongsun and Song, Jaeyong and Jung, Jaewon and Park, Jaeyoung and Kim, Youngsok and Lee, Jinho",
        "title": "{Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "flexgen",
        "author": "Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and Re, Christopher and Stoica, Ion and Zhang, Ce",
        "title": "{FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU}"
      },
      {
        "key": "llminaflash",
        "author": "Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C and Rastegari, Mohammad and Farajtabar, Mehrdad",
        "title": "{Llm in a flash: Efficient large language model inference with limited memory}"
      },
      {
        "key": "leviathan",
        "author": "Jin, Yunho and Kim, Shine and Ham, Tae Jun and Lee, Jae W.",
        "title": "{Architecting a Flash-Based Storage System for Low-Cost Inference of Extreme-Scale DNNs}"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "flexgen",
        "author": "Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and Re, Christopher and Stoica, Ion and Zhang, Ce",
        "title": "{FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU}"
      },
      {
        "key": "powerinfer",
        "author": "Song, Yixin and Mi, Zeyu and Xie, Haotong and Chen, Haibo",
        "title": "{Powerinfer: Fast large language model serving with a consumer-grade gpu}"
      },
      {
        "key": "hetegen",
        "author": "Zhao, Xuanlei and Jia, Bin and Zhou, Haotian and Liu, Ziming and Cheng, Shenggan and You, Yang",
        "title": "{HeteGen: Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices}"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "gpt3",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario",
        "title": "{Language Models are Few-Shot Learners}"
      },
      {
        "key": "gpt4",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "{Gpt-4 technical report}"
      },
      {
        "key": "shoeybi2019megatron",
        "author": "Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan",
        "title": "{Megatron-lm: Training multi-billion parameter language models using model parallelism}"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "lin2023awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song",
        "title": "{Awq: Activation-aware weight quantization for llm compression and acceleration}"
      },
      {
        "key": "llmint8",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "{Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale}"
      },
      {
        "key": "frantar2023optq",
        "author": "Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh",
        "title": "{{OPTQ}: Accurate Quantization for Generative Pre-trained Transformers}"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "mirzadeh2020improved",
        "author": "Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan",
        "title": "{Improved knowledge distillation via teacher assistant}"
      },
      {
        "key": "kim-rush-2016-sequence",
        "author": "Kim, Yoon  and\nRush, Alexander M.",
        "title": "{Sequence-Level Knowledge Distillation}"
      },
      {
        "key": "sun-etal-2019-patient",
        "author": "Sun, Siqi  and\nCheng, Yu  and\nGan, Zhe  and\nLiu, Jingjing",
        "title": "{Patient Knowledge Distillation for {BERT} Model Compression}"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "lin2023awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song",
        "title": "{Awq: Activation-aware weight quantization for llm compression and acceleration}"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "llmint8",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "{Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale}"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "mirzadeh2020improved",
        "author": "Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan",
        "title": "{Improved knowledge distillation via teacher assistant}"
      },
      {
        "key": "kim-rush-2016-sequence",
        "author": "Kim, Yoon  and\nRush, Alexander M.",
        "title": "{Sequence-Level Knowledge Distillation}"
      },
      {
        "key": "sun-etal-2019-patient",
        "author": "Sun, Siqi  and\nCheng, Yu  and\nGan, Zhe  and\nLiu, Jingjing",
        "title": "{Patient Knowledge Distillation for {BERT} Model Compression}"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "smartinfinity",
        "author": "Jang, Hongsun and Song, Jaeyong and Jung, Jaewon and Park, Jaeyoung and Kim, Youngsok and Lee, Jinho",
        "title": "{Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System}"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "llm-qat",
        "author": "Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas",
        "title": "{Llm-qat: Data-free quantization aware training for large language models}"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "lin2023awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song",
        "title": "{Awq: Activation-aware weight quantization for llm compression and acceleration}"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "llmint8",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "{Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale}"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "execube",
        "author": "Kogge, Peter M",
        "title": "{EXECUBE-A new architecture for scaleable MPPs}"
      },
      {
        "key": "IRAM",
        "author": "Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.",
        "title": "{A case for intelligent {RAM}}"
      },
      {
        "key": "flexram",
        "author": "Kang, Yi and Huang, Wei and Yoo, Seung-Moon and Keen, D and Ge, Zhenzhou and Lam, V and Pattnaik, P and Torrellas, J",
        "title": "{FlexRAM: Toward an advanced intelligent memory system}"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "hmc",
        "author": "Pawlowski, J Thomas",
        "title": "{Hybrid memory cube ({HMC})}"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "tesseract",
        "author": "Ahn, Junwhan and Hong, Sungpack and Yoo, Sungjoo and Mutlu, Onur and Choi, Kiyoung",
        "title": "{A scalable processing-in-memory accelerator for parallel graph processing}"
      },
      {
        "key": "pei",
        "author": "Ahn, Junwhan and Yoo, Sungjoo and Mutlu, Onur and Choi, Kiyoung",
        "title": "{PIM-enabled instructions: a low-overhead, locality-aware processing-in-memory architecture}"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "rowclone",
        "author": "Seshadri, Vivek and Kim, Yoongu and Fallin, Chris and Lee, Donghyuk and Ausavarungnirun, Rachata and Pekhimenko, Gennady and Luo, Yixin and Mutlu, Onur and Gibbons, Phillip B. and Kozuch, Michael A. and Mowry, Todd C.",
        "title": "{RowClone: fast and energy-efficient in-DRAM bulk data copy and initialization}"
      },
      {
        "key": "ambit",
        "author": "Seshadri, Vivek and Lee, Donghyuk and Mullins, Thomas and Hassan, Hasan and Boroumand, Amirali and Kim, Jeremie and Kozuch, Michael A and Mutlu, Onur and Gibbons, Phillip B and Mowry, Todd C",
        "title": "{Ambit: In-memory accelerator for bulk bitwise operations using commodity DRAM technology}"
      },
      {
        "key": "computedram",
        "author": "Gao, Fei and Tziantzioulis, Georgios and Wentzlaff, David",
        "title": "{ComputeDRAM}: In-memory compute using off-the-shelf DRAMs"
      },
      {
        "key": "bufcmp",
        "author": "Lee, Jinho and Ahn, Jung Ho and Choi, Kiyoung",
        "title": "{Buffered compares: Excavating the hidden parallelism inside DRAM architectures with lightweight logic}"
      },
      {
        "key": "chameleon",
        "author": "Asghari-Moghaddam, Hadi and Son, Young Hoon and Ahn, Jung Ho and Kim, Nam Sung",
        "title": "{Chameleon: Versatile and practical near-{DRAM} acceleration architecture for large memory systems}"
      },
      {
        "key": "gradpim",
        "author": "Kim, Heesu and Park, Hanmin and Kim, Taehyun and Cho, Kwanheum and Lee, Eojin and Ryu, Soojung and Lee, Hyuk-Jae and Choi, Kiyoung and Lee, Jinho",
        "title": "{GradPIM: A practical processing-in-DRAM architecture for gradient descent}"
      },
      {
        "key": "tensordimm",
        "author": "Kwon, Youngeun and Lee, Yunjae and Rhu, Minsoo",
        "title": "{{TensorDIMM}: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning}"
      },
      {
        "key": "axdimm",
        "author": "Ke, Liu and Zhang, Xuan and So, Jinin and Lee, Jong-Geon and Kang, Shin-Haeng and Lee, Sukhan and Han, Songyi and Cho, YeonGon and Kim, Jin Hyun and Kwon, Yongsuk and Kim, KyungSoo and Jung, Jin and Yun, Ilkwon and Park, Sung Joo and Park, Hyunsun and Song, Joonho and Cho, Jeonghyeon and Sohn, Kyomin and Kim, Nam Sung and Lee, Hsien-Hsin S.",
        "title": "{Near-Memory Processing in Action: Accelerating Personalized Recommendation With {AxDIMM}}"
      },
      {
        "key": "piccolo",
        "author": "Shin, Changmin and Kwon, Taehee and Song, Jaeyong and Ju, Jae Hyung and Liu, Frank and Choi, Yeonkyu and Lee, Jinho",
        "title": "{A Case for In-Memory Random Scatter-Gather for Fast Graph Processing}"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "newton",
        "author": "He, Mingxuan and Song, Choungki and Kim, Ilkon and Jeong, Chunseok and Kim, Seho and Park, Il and Thottethodi, Mithuna and Vijaykumar, T. N.",
        "title": "{Newton: A DRAM-maker\u2019s Accelerator-in-Memory (AiM) Architecture for Machine Learning}"
      },
      {
        "key": "aim",
        "author": "Lee, Seongju and Kim, Kyuyoung and Oh, Sanghoon and Park, Joonhong and Hong, Gimoon and Ka, Dongyoon and Hwang, Kyudong and Park, Jeongje and Kang, Kyeongpil and Kim, Jungyeon and others",
        "title": "{A 1ynm 1.25 V 8Gb, 16Gb/s/pin GDDR6-based accelerator-in-memory supporting 1TFLOPS MAC operation and various activation functions for deep-learning applications}"
      },
      {
        "key": "hbmpim",
        "author": "Lee, Sukhan and Kang, Shin-haeng and Lee, Jaehoon and Kim, Hyeonsu and Lee, Eojin and Seo, Seungwoo and Yoon, Hosang and Lee, Seungwon and Lim, Kyounghwan and Shin, Hyunsung and Kim, Jinhyun and Seongil, O and Iyer, Anand and Wang, David and Sohn, Kyomin and Kim, Nam Sung",
        "title": "{Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology : Industrial Product}"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "upmem",
        "author": "Devaux, Fabrice",
        "title": "{The true Processing In Memory accelerator}"
      },
      {
        "key": "hbmpim",
        "author": "Lee, Sukhan and Kang, Shin-haeng and Lee, Jaehoon and Kim, Hyeonsu and Lee, Eojin and Seo, Seungwoo and Yoon, Hosang and Lee, Seungwon and Lim, Kyounghwan and Shin, Hyunsung and Kim, Jinhyun and Seongil, O and Iyer, Anand and Wang, David and Sohn, Kyomin and Kim, Nam Sung",
        "title": "{Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology : Industrial Product}"
      },
      {
        "key": "aim",
        "author": "Lee, Seongju and Kim, Kyuyoung and Oh, Sanghoon and Park, Joonhong and Hong, Gimoon and Ka, Dongyoon and Hwang, Kyudong and Park, Jeongje and Kang, Kyeongpil and Kim, Jungyeon and others",
        "title": "{A 1ynm 1.25 V 8Gb, 16Gb/s/pin GDDR6-based accelerator-in-memory supporting 1TFLOPS MAC operation and various activation functions for deep-learning applications}"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "transpim",
        "author": "Zhou, Minxuan and Xu, Weihong and Kang, Jaeyoung and Rosing, Tajana",
        "title": "{TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer}"
      },
      {
        "key": "pimdl",
        "author": "Li, Cong and Zhou, Zhe and Wang, Yang and Yang, Fan and Cao, Ting and Yang, Mao and Liang, Yun and Sun, Guangyu",
        "title": "{PIM-DL: Expanding the Applicability of Commodity DRAM-PIMs for Deep Learning via Algorithm-System Co-Optimization}"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "neupims",
        "author": "Heo, Guseul and Lee, Sangyeop and Cho, Jaehong and Choi, Hyunmin and Lee, Sanghyeon and Ham, Hyungkyu and Kim, Gwangsun and Mahajan, Divya and Park, Jongse",
        "title": "{NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing}"
      },
      {
        "key": "attacc",
        "author": "Park, Jaehyun and Choi, Jaewan and Kyung, Kwanhee and Kim, Michael Jaemin and Kwon, Yongsuk and Kim, Nam Sung and Ahn, Jung Ho",
        "title": "{AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference}"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "attacc",
        "author": "Park, Jaehyun and Choi, Jaewan and Kyung, Kwanhee and Kim, Michael Jaemin and Kwon, Yongsuk and Kim, Nam Sung and Ahn, Jung Ho",
        "title": "{AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference}"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "neupims",
        "author": "Heo, Guseul and Lee, Sangyeop and Cho, Jaehong and Choi, Hyunmin and Lee, Sanghyeon and Ham, Hyungkyu and Kim, Gwangsun and Mahajan, Divya and Park, Jongse",
        "title": "{NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing}"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "active_disk",
        "author": "Acharya, Anurag and Uysal, Mustafa and Saltz, Joel",
        "title": "{Active disks: programming model, algorithms and evaluation}"
      },
      {
        "key": "active_disk_large",
        "author": "Riedel, E. and Faloutsos, C. and Gibson, G.A. and Nagle, D.",
        "title": "{Active disks for large-scale data processing}"
      },
      {
        "key": "active_storage_large",
        "author": "Riedel, Erik and Gibson, Garth A. and Faloutsos, Christos",
        "title": "{Active Storage for Large-Scale Data Mining and Multimedia}"
      },
      {
        "key": "idisk",
        "author": "Keeton, Kimberly and Patterson, David A. and Hellerstein, Joseph M.",
        "title": "{A case for intelligent disks (IDISKs)}"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "self_sorting_ssd",
        "author": "Quero, Luis Cavazos and Lee, Young-Sik and Kim, Jin-Soo",
        "title": "{Self-sorting SSD: Producing sorted data inside active SSDs}"
      },
      {
        "key": "earlysmartssd",
        "author": "Kang, Yangwook and Kee, Yang-suk and Miller, Ethan L. and Park, Chanik",
        "title": "{Enabling cost-effective data processing with smart SSD}"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "smartssd_datamine",
        "author": "Bae, Duck-Ho and Kim, Jin-Hyung and Kim, Sang-Wook and Oh, Hyunok and Park, Chanik",
        "title": "{Intelligent SSD: a turbo for big data mining}"
      },
      {
        "key": "biscuit",
        "author": "Gu, Boncheol and Yoon, Andre S. and Bae, Duck-Ho and Jo, Insoon and Lee, Jinyoung and Yoon, Jonghyun and Kang, Jeong-Uk and Kwon, Moonsang and Yoon, Chanho and Cho, Sangyeun and Jeong, Jaeheon and Chang, Duckhyun",
        "title": "{Biscuit: a framework for near-data processing of big data workloads}"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "genstore",
        "author": "Mansouri Ghiasi, Nika and Park, Jisung and Mustafa, Harun and Kim, Jeremie and Olgun, Ataberk and Gollwitzer, Arvid and Senol Cali, Damla and Firtina, Can and Mao, Haiyu and Almadhoun Alserr, Nour and Ausavarungnirun, Rachata and Vijaykumar, Nandita and Alser, Mohammed and Mutlu, Onur",
        "title": "{GenStore: a high-performance in-storage processing system for genome sequence analysis}"
      },
      {
        "key": "deepstore",
        "author": "Mailthody, Vikram Sharma and Qureshi, Zaid and Liang, Weixin and Feng, Ziyan and De Gonzalo, Simon Garcia and Li, Youjie and Franke, Hubertus and Xiong, Jinjun and Huang, Jian and Hwu, Wen-mei",
        "title": "{Deepstore: In-storage acceleration for intelligent queries}"
      },
      {
        "key": "inspire",
        "author": "Lin, Jilan and Liang, Ling and Qu, Zheng and Ahmad, Ishtiyaque and Liu, Liu and Tu, Fengbin and Gupta, Trinabh and Ding, Yufei and Xie, Yuan",
        "title": "{{INSPIRE}: In-Storage Private Information Retrieval via Protocol and Architecture Co-Design}"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "netezza",
        "author": "Singh, Malcolm and Leonhardi, Ben",
        "title": "{Introduction to the IBM Netezza warehouse appliance}"
      },
      {
        "key": "exadata",
        "author": "Weiss, Ronald",
        "title": "{A technical overview of the oracle exadata database machine and exadata storage server}"
      },
      {
        "key": "rmssd",
        "author": "Sun, Xuan and Wan, Hu and Li, Qiao and Yang, Chia-Lin and Kuo, Tei-Wei and Xue, Chun Jason",
        "title": "{RM-SSD: In-storage computing for large-scale recommendation inference}"
      },
      {
        "key": "secndp",
        "author": "W. Xiong and L. Ke and D. Jankov and M. Kounavis and X. Wang and E. Northup and J. Yang and B. Acun and C. Wu and P. Peter Tang and G. Edward Suh and X. Zhang and H. S. Lee",
        "title": "{SecNDP: Secure Near-Data Processing with Untrusted Memory}"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "smartssd",
        "author": "Unknown",
        "title": "{SmartSSD}"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "smartssd_query",
        "author": "Do, Jaeyoung and Kee, Yang-Suk and Patel, Jignesh M. and Park, Chanik and Park, Kwanghyun and DeWitt, David J.",
        "title": "{Query processing on smart SSDs: opportunities and challenges}"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "smartsage",
        "author": "Lee, Yunjae and Chung, Jinha and Rhu, Minsoo",
        "title": "{SmartSAGE: training large-scale graph neural networks using in-storage processing architectures}"
      },
      {
        "key": "smartinfinity",
        "author": "Jang, Hongsun and Song, Jaeyong and Jung, Jaewon and Park, Jaeyoung and Kim, Youngsok and Lee, Jinho",
        "title": "{Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System}"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "smartssd_dlrm",
        "author": "Soltaniyeh, Mohammadreza and Lagrange Moutinho Dos Reis, Veronica and Bryson, Matt and Yao, Xuebin and Martin, Richard P. and Nagarakatte, Santosh",
        "title": "{Near-Storage Processing for Solid State Drive Based Recommendation Inference with SmartSSDs\u00ae}"
      }
    ]
  }
]