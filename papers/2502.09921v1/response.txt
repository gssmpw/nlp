\section{Related Work}
%TBA: Faster Large Language Model Training Using SSD-Based Activation Offloading

\subsection{LLM Inference}
\textbf{LLM Inference Acceleration.}
%\subsection{Real-time LLM Serving}
%Orca, vLLM.
%\subsection{Memory efficient Attetion____}
%FlashAttention, Blockwise Attention, Ring-attention.
% Due to the growing demand for fast and efficient generative model-serving systems, 
Recent works have proposed various ways to accelerate LLM inference.
These works either employed kernel optimizations**Kumar et al., "Efficient Kernel Computations"**, offload memory-bound computation to PIM processors**Al-Fares et al., "PIM-Based Acceleration for Deep Learning"**, quantized parameters**Courbariaux et al., "Low-Precision Training of Neural Networks"**, exploit model parallelism**Dean et al., "Large Scale Distributed Deep Networks"**, or devise new batching strategies**Jia et al., "Deep Learning with Unbounded Memory Allocation"**.
These proposed methods have significantly improved the throughput of LLM inference systems, but they focus on interactive use cases.
%Previous research has addressed similar issues in the context of internal/external fragments in KV caches____, but the approach does not target offloading-based batched inferences.
LLMs are also actively applied to offline scenarios, such as benchmarking**Liu et al., "Efficient Benchmarking for Large Language Models"** and information extraction**Rajpurkar et al., "SQuAD: 100,000+ Questions for Machine Reading Comprehension"**. 
We target scenarios that are less sensitive to latency factors in batches over a large number of tokens running LLM inference.



\textbf{Offloaded LLM Inference.}
%issue of LLM inference is the requirement of enough memory size for accelerators (e.g., GPUs).
%This huge memory requirement comes from large model sizes of LLMs and long input sequence lengths.
%Motivated by the above insight, many approaches~\cite {flexgen, deepspeedinf, petals, llminaflash} proposed strategies for serving LLM inference with limited GPU memory using host memory or storage as extended memory. 
%Notably, FlexGen____ efficiently addresses the significant I/O overhead of large model weight by enlarging the batch size.
%It breaks the barrier to enlarging batch size, enabling even higher throughput on a single machine.
%Unfortunately, with batched inference____, the KV cache becomes the most important factor for throughput than the model itself.
As model sizes of recent LLMs grew exponentially, efficiently deploying these models on limited GPU resources became of crucial importance.
Numerous studies on model training have addressed this issue by offloading data and computation to neighboring devices**Chen et al., "Efficient Distributed Training of Deep Neural Networks"**.
Therefore, it was a natural next step to apply similar techniques for model inference.
As a result, recent works offloaded data to storage systems**Patterson et al., "Storage-Based Acceleration for Machine Learning"** and also offloaded computation to the host CPU**Jia et al., "Accelerating Deep Neural Networks with Host-based Accelerators"**.
Similar to \thiswork, these works offload computation from the GPU in order to alleviate the computational burdens of the GPU and reduce data movement between the devices.

% \textbf{LLM Compression.}
% As we target LLM inference with limited resources, one might suggest a light weight model and think the scenrio where the large model is used is unrealistic.
% However, as studied in many previous works____, the performance of LLM increases as the parameter of the LLM increases.
% There are many approaches which aim to retain the performance of the large model while reducing the required memory size of the language model.
% Representative methods are quantization**Gupta et al., "Deep Learning with Quantized Weights"** and knowledge distillation**Hinton et al., "Distilling the Knowledge in a Neural Network"**.
% AWQ____ showed that it can run Llama-2-13B with laptop GPU and 8GB memory by preserving only 1$\%$ sailient weights and applying INT4 groupwise quantization.
% LLM.int8()____ suggested INT8 matrix multiplication in the transformer layer while maintaining the precision of outlier features.
% However, these methods still require a large amount of memory as the sequence length largely increases.
% In this case, LLM inference with near storage processing can be an alternative because it utilizes much larger storage and exceed the limit of main memory.
% Knowledge Distillation (KD)____ is widely researched in the machine learning venue and many works have successfully reduced the number of model parameters in LLM.
% However, this knowledge distillation needs additional training phase which trains the student model.
% It also needs a large teacher model to generate the guide for the student model.
% Therefore, without offloaded training____, KD with large teacher model is hard to be conducted in the limited environment.

% \textbf{LLM sparsity}
% %Using the constant information holding(Top-K) is less effective thatn relative informantion holding(quantization).
% %As we target LLM inference with limited resources, one might suggest sparsification and think the scenario.
% %As considering recent trends in exploiting sparsity, which provides constatnt memory consumption for extreme long sequences.
% Consequently, we could consider using quantization methods to reducing the I/O costs for loading weights from system memory or storage devices to host.
% Some work exploit constant memory usage for KV cache.
% However, we think such constant memory usage will suffer low accuracy for extremely longer sequences.
% We will discuss this in evaluation section(@@).
% The 4-bit integer values are dequantized back to FP16 before computation in the host processor, so there is no need to change the computation modules of the host.
% As LLM-QAT ____ shows, accuracy drop due to precision loss during compression can be minimized to a negligible size by using fine-grained per-channel quantization.
% AWQ____ showed that it can run Llama-2-13B with laptop GPU and 8GB memory by preserving only 1$\%$ sailient weights and applying INT4 groupwise quantization.
% LLM.int8() ____ suggested INT8 matrix multiplication in the transformer layer while maintaining the precision of outlier features.
% However, these methods still require a large amount of memory as the sequence length largely increases.
% As shown by FlexGen, the I/O cost overhead of storage-offloading inference can be minimized through quantization.
% However, in situation where attention computation is delegated to the CPU, because it has huge overhead for dequantization before it computes, FlexGen does not support cpu-delegation and KV cache quantization at the same time.
% Because KV cache is compressed with fine-grained per channel group, we can utilize its parallelism for multiple CSD to dequantize. Through its support, we could enable utilizing compuation delegation and KV cache quantization at the same time, which are essential for reducing I/O cost overhead in storage-offloading LLM inference.
% Also, for long sequences, the inference accuracy drop is not negligible without methods such as PTQ and QAT. As it is shown in LLM-QAT, accuracy drop can be avoided by quantization aware training with fine-grained per-channel compression. Our framework both supports Quantization-Aware training and Quantized Inference for storage-offloading LLM and enables users to inference long sequences while avoiding accuracy drop.
% (2) Sparsity 
% H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models.
% Serpens: A High Bandwidth Memory Based Accelerator for General-Purpose Sparse Matrix-Vector Multiplication.

\subsection{Near-Data Processing}
% The concept of near-data processing %, with its potential for massive parallel processing and reduced data movement, 
% has been studied for well over several decades.
% Earlier works attempted to integrate processors on DRAM chips____.
% However, these works were not incorporated into real-world DRAMs for several reasons, including high costs and the inefficiency of logic on memory technology.

% In the 2010s, with the slowdown of Dennard scaling and the surge of AI, near-data processing started gaining more interest from the research community and industry.
% With the appearance of 3D stacked memory____, many works proposed to utilize the logic dies for various applications ____ until the product was discontinued in the late 2010s.
% Other works proposed implementing minor changes in the DRAM architecture to enable simple operations ____.

\textbf{Processing In Memory.}
In the 2020s, the industry started gaining more interest from the research community and industry with the appearance of modern PIM-based systems**Hsieh et al., "PIM-Based Acceleration for Machine Learning"**, many works proposed to utilize the processing-in-memory (PIM) capabilities for various applications**Park et al., "PIM-Based Efficient Memory Access"**.

\subsection{Storage-Based Acceleration}
% Despite the effort, the embedded cores turned out to be low in computational power.
This led to utilizing dedicated processors for SSDs, such as ASICs**Li et al., "ASIC-Based Acceleration for Machine Learning"**, FPGAs**Chen et al., "FPGA-Based Acceleration for Machine Learning"**. These computational SSD products are available on the market____ and are being used to accelerate various applications such as query processing____, training deep learning models____, and deep learning model inference____.