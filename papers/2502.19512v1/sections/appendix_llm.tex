%cite the paper / exactly same thing in the code / different orders of relations / comma and quotes / example of failure (different lanuages, 20 failures of each)

\subsection{Details for Task 1} \label{appx:T1}
\paragraph{Relation Prediction Prompt of Task 1} 
In the following task, you will be given background knowledge in the form of triplet (h, r, t) which means entity 'h' has relation 'r' with entity 't'. Then you will be asked some questions about the relationship between entities. Background knowledge: (Kris Kristofferson, occupation, guitarist); (Willow Smith, genre, indie pop);\dots What is the relationship between entity 'Gaspard Monge' and entity 'France'? Please choose one best answer from the following relations:|parent organization|studies|cause of death|architectural style|unmarried partner|industry|\dots|. You just need to give the relation and please do not give an explanation.

\paragraph{Entity Prediction Prompt of Task 1} 
In the following task, you will be given background knowledge in the form of triplet (h, r, t) which means entity 'h' has relation 'r' with entity 't'. Then you will be asked some questions about the relationship between entities. Background knowledge: (Kris Kristofferson, occupation, guitarist); (Willow Smith, genre, indie pop);\dots Predict the tail entity for triplet (Gaspard Monge, country of citizenship, ?). Please give the 10 most possible answers. You just need to give the names of the entities separated by commas and please do not give explanation.

\revisionlog{In the prompt for relation prediction for all the three tasks, all relations in the dataset are listed with '|' as the delimiter. For the sake of simplicity in the presentation, in the next subsections we use "\dots" to represent the rest of triplets and relations in the prompts.}

\Cref{llm-base} shows the Hits@1 of Gemini-1.5-pro on in-domain relation prediction task. \revision{\Cref{llm-base-entity} shows the Hits@10 of Gemini-1.5-flash on in-domain entity prediction task. We use Gemini-1.5-flash for entity prediction tasks for the sake of reducing costs of experiments.} We run the experiment 3 times with the same prompt in English to see if it can generate consistent answers. Gemini performs the task quite well and shows consistency across 3 runs. This indicates given background knowledge in the prompt, LLMs has the capacity to handle in-domain relation and entity predictions well. 

% We further tests its performance across different languages. \Cref{tab:llm-relation2} shows that the LLM has consistent good performance in different languages.

\begin{table*}[ht]
\centering
\caption{Task 1: In-domain LLM relation predictions Hits@1 on CoDEx-S.}
\begin{tabular}{lrrr |r}
\toprule
 & Run \#1  & Run \#2 & Run \#3  & Worst \\
Gemini-1.5-pro & 0.933 & 0.933 & 0.933 & 0.933\\
%gpt-4-0125-preview & N/A* & N/A* & N/A* & N/A* \\
ULTRA & 0.820 & 0.820 & 0.820 & 0.820 \\
\ourmethod & {\bf 0.935} & {\bf 0.935} & {\bf 0.935} & {\bf 0.935} \\
\hline
\end{tabular}
\label{llm-base}
\end{table*}

\begin{table*}[ht]
\centering
\caption{Task 1: In-domain LLM entity predictions Hits@10 on CoDEx-S.}
\begin{tabular}{lrrr |r}
\toprule
 & Run \#1  & Run \#2 & Run \#3  & Worst \\
Gemini-1.5-flash & 0.308 & 0.308 & 0.308 & 0.308\\
%gpt-4-0125-preview & N/A* & N/A* & N/A* & N/A* \\
ULTRA & 0.667 & 0.667 & 0.667 & 0.667 \\
\ourmethod & {\bf 0.670} & {\bf 0.670} & {\bf 0.670} & {\bf 0.670} \\
\hline
\end{tabular}
\label{llm-base-entity}
\end{table*}


% \begin{table*}[ht]
% \centering
% \caption{Task 1 on different languages: In-domain relation predictions of Gemini-1.5-pro on CoDEx-S in different languages with Prompt Template 1.N/A* means we were not able to perform the experiment due to API limitations on context size.}
% \begin{tabular}{l rrrrrr rr}
% \toprule
% & \multicolumn{6}{c}{Gemini-1.5-pro} & \multicolumn{2}{c}{K-GFMs}  \\
% \cmidrule(lr){2-7}
% \cmidrule(lr){8-9}
%  & \multicolumn{1}{c}{ar} & \multicolumn{1}{c}{de} & \multicolumn{1}{c}{en} & \multicolumn{1}{c}{es} & \multicolumn{1}{c}{ru} & \multicolumn{1}{c}{zh} & \multicolumn{1}{c}{ULTRA} & \multicolumn{1}{c}{\ourmethod} \\
% \midrule
% CoDEx-S & 0.903 & 0.935 & 0.933 & 0.933 & 0.903 & 0.933 & 0.820 & \textbf{0.935} \\
% CoDEx-M & N/A* & N/A* & N/A* & N/A* & N/A* & N/A* & 0.870 & \textbf{0.886} \\
% CoDEx-L & N/A* & N/A* & N/A* & N/A* & N/A* & N/A* & 0.824 & \textbf{0.837} \\
% \bottomrule
% \end{tabular}
% \label{tab:llm-relation2}
% \end{table*}


\subsection{Details for Task 2} \label{appx:T2}
\paragraph{Relation Prediction Prompt of Task 2} In the following task, you will first be given background knowledge in the form of triplet (h, r, t) which means entity 'h' has relation 'r' with entity 't'. Then you will be asked some questions about the relationship between entities. Please notice that some words are replaced with metasyntactic words in the following paragraph. Background knowledge: (foo, baz, guitarist); (Willow Smith, genre, bar);\dots What is the relationship between entity 'foo' and entity 'bar'? Please choose one best answer from the following relation IDs:|parent organization|studies|quux|baz|\dots|. You just need to give the relation and please do not give an explanation.

\paragraph{Entity Prediction Prompt of Task 2} In the following task, you will first be given background knowledge in the form of triplet (h, r, t) which means entity 'h' has relation 'r' with entity 't'. Then you will be asked some questions about the relationship between entities. Please notice that some words are replaced with metasyntactic words in the following paragraph. Background knowledge: (foo, baz, guitarist); (Willow Smith, genre, bar);\dots Predict the tail entity for triplet (foo, garply, ?). Please give the 10 most possible answers. You just need to give the names of the entities separated by commas and please do not give explanation.

\Cref{llm-metasyntactic} shows the Hits@1 of Gemini-1.5-pro on out-domain relation prediction task. \revision{\Cref{llm-metasyntactic-entity} shows the Hits@10 of Gemini-1.5-flash on out-domain entity prediction task.} The neighbor entities of the head entity and the relations that connect the head entity with its neighbors are replaced with metasyntactic words. We run the experiment 3 times with different metasyntactic words but the underlying structural pattern is exactly the same as in the in-domain task. The results indicate the LLM can not do the out-of-domain task well. This means the LLM relied more on the known semantic description of the words instead of the structural pattern of the graph so that when there are new entities and relations, it can not perform inductive reasoning on them.

\begin{table*}[t]
\centering
\caption{Task 2: Out-of-domain LLM relation predictions Hits@1 on CoDEx-S. Effects of Metasyntactic Words on Relation Predictions on CoDEx-S.\label{llm-metasyntactic}
}
\begin{tabular}{lrrr |r}
\toprule
 & Run \#1  & Run \#2 & Run \#3  & Worst \\
 \midrule
Gemini-1.5-pro & 0.667 & 0.667 & 0.633 & 0.633\\
ULTRA & 0.820 & 0.820 & 0.820 & 0.820 \\
\ourmethod & {\bf 0.935} & {\bf 0.935} & {\bf 0.935} & {\bf 0.935} \\
%gpt-4-0125-preview & N/A* & N/A* & N/A* & N/A* \\
\hline
\end{tabular}
\end{table*}

\begin{table*}[ht]
\centering
\caption{Task 2: Out-of-domain LLM entity predictions Hits@10 on CoDEx-S. Effects of Metasyntactic Words on Entity Predictions on CoDEx-S.\label{llm-metasyntactic-entity}
}
\begin{tabular}{lrrr |r}
\toprule
 & Run \#1  & Run \#2 & Run \#3  & Worst \\
 \midrule
Gemini-1.5-flash & 0.212  & 0.250 & 0.327 & 0.212\\
ULTRA & 0.667 & 0.667 & 0.667 & 0.667 \\
\ourmethod & {\bf 0.670} & {\bf 0.670} & {\bf 0.670} & {\bf 0.670} \\
%gpt-4-0125-preview & N/A* & N/A* & N/A* & N/A* \\
\hline
\end{tabular}
\end{table*}

\subsection{Details for Task 3}\label{appx:T3}
\paragraph{Relation Prediction Prompt of Task 3} In the following task, entities and relations will be expressed with their IDs. You will first be given the mapping from entities to their IDs and the mapping from relations to their IDs. Then you will be given background knowledge in the form of triplet (h, r, t) which means entity 'h' has relation 'r' with entity 't'. Finally you will be asked some questions about the relationship between entities. Entity mapping: Mireille Darc is entity '8831'; Breton is entity '20512'; Tomas Tranströmer is entity '1641'\dots Relation mapping: located in the administrative terroritorial entity is relation '15' \dots Background knowledge: (15443, 16, 1093); (21198, 16, 9387); (14854, 8, 10218)\dots What is the relationship between entity '18127' and entity '1799'? Please choose one best answer from the following relation IDs:|45|48|27|35|\dots|. You just need to give the ID of that relation and please do not give an explanation.

\paragraph{Entity Prediction Prompt of Task 3} In the following task, entities and relations will be expressed with their IDs. You will first be given the mapping from entities to their IDs and the mapping from relations to their IDs. Then you will be given background knowledge in the form of triplet (h, r, t) which means entity 'h' has relation 'r' with entity 't'. Finally you will be asked some questions about the relationship between entities. Entity mapping: Mireille Darc is entity '8831'; Breton is entity '20512'; Tomas Tranströmer is entity '1641'\dots Relation mapping: located in the administrative terroritorial entity is relation '15' \dots Background knowledge: (15443, 16, 1093); (21198, 16, 9387); (14854, 8, 10218)\dots Predict the tail entity for triplet (18127, 45, ?). Please give the 10 most possible answers. You just need to give the IDs of the entities separated by commas and please do not give explanation.


\Cref{llm-equivarariance} shows the Hits@1 of Gemini-1.5-pro on relation prediction in Task 3. \revision{\Cref{llm-equivarariance-entity} shows the Hits@10 of Gemini-1.5-flash on entity prediction in Task 3.} The entities and relations are expressed as IDs. We run the experiment 3 times with permutated IDs but the underlying structural pattern is exactly the same as in the in-domain task. The results demonstrate that the LLM is very sensitive to ID permutation so that its performance is inconsistent across 3 permutations.

\begin{table*}[t]
\centering
\caption{Task 3: Out-of-domain LLM relation predictions Hits@1 on CoDEx-S. Effects of Input Permutations on Relation Predictions on CoDEx-S.}
\begin{tabular}{lrrr |r}
\toprule
 & Permutation \#1  & Permutation \#2 & Permutation \#3  & Worst \\
 \midrule
Gemini-1.5-pro & 0.346 & 0.731 & 0.615 & 0.346\\
%gpt-4-0125-preview & N/A* & N/A* & N/A* & N/A* \\
ULTRA & 0.820 & 0.820 & 0.820 & 0.820 \\
\ourmethod & {\bf 0.935} & {\bf 0.935} & {\bf 0.935} & {\bf 0.935} \\
\hline
\end{tabular}
\label{llm-equivarariance}
\end{table*}

\begin{table*}[t]
\centering
\caption{Task 3: Out-of-domain LLM entity predictions Hits@10 on CoDEx-S. Effects of Input Permutations on Entity Predictions on CoDEx-S.}
\begin{tabular}{lrrr |r}
\toprule
 & Permutation \#1  & Permutation \#2 & Permutation \#3  & Worst \\
 \midrule
Gemini-1.5-flash & 0.212 & 0.250 & 0.231 & 0.212\\
%gpt-4-0125-preview & N/A* & N/A* & N/A* & N/A* \\
ULTRA & 0.667 & 0.667 & 0.667 & 0.667 \\
\ourmethod & {\bf 0.670} & {\bf 0.670} & {\bf 0.670} & {\bf 0.670} \\
\hline
\end{tabular}
\label{llm-equivarariance-entity}
\end{table*}

% \begin{table*}[ht]
% \centering
% \caption{Task 3: Out-of-domain LLM entity predictions Hits@10 on CoDEx-S. Effects of Input Permutations on Entity Predictions on CoDEx-S.}
% \begin{tabular}{lrrr |r}
% \toprule
%  & Permutation \#1  & Permutation \#2 & Permutation \#3  & Worst \\
%  \midrule
% Gemini-1.5-pro & 0.231 & 0.250 & 0.346 & 0.231\\
% %gpt-4-0125-preview & N/A* & N/A* & N/A* & N/A* \\
% ULTRA & 0.667 & 0.667 & 0.667 & 0.667 \\
% \ourmethod & {\bf 0.670} & {\bf 0.670} & {\bf 0.670} & {\bf 0.670} \\
% \hline
% \end{tabular}
% \label{llm-equivarariance-entity}
% \end{table*}

% \subsection{Capacity of LLMs in different languages}
% In this part we want to evaluate whether LLMs can make consistent queries in different languages. Due to token size limit of ChatGPT, we use prompt template 4 which directly queries the LLM without providing context from the knowledge graph~\cite{yao2023exploring}. While for Gemini, we use prompt template 1.

% \paragraph{Prompt Template 4 in English} What is the relationship between \{head entity\} and \{tail entity\}? Please choose one best answer from:|parent organization|studies|cause of death|\dots|genre|

% \begin{CJK}{UTF8}{gbsn}
% \paragraph{Prompt Template 4 in Chinese} \{头节点\}和\{尾节点\}之间的关系是什么?请从以下选项中选择一个最佳答案:|母组织|研究对象|死因|\dots|艺术流派|
% \end{CJK}


% \Cref{tab:llm-relation} shows the hits@1 results comparing gpt-4-0125-preview with the double equivariant graph models. Notably, when the knowledge graph is not used as part of the prompt, ChatGPT exhibits inferior performance compared to graph models, showcasing significant variability across different languages. Specifically, it achieves the highest hits@1 scores when queries are posed in Spanish, while its performance is notably poorer when queries are conducted in Russian. On the contrary, Gemini-1.5-pro performs quite well in different languages when the whole KG is provided in the prompt as shown in \Cref{tab:llm-relation2}.

% % this table to appendix
% \begin{table*}[ht]
% \centering
% \caption{Task 4: Relation Prediction hits@1 of ChatGPT(gpt-4-0125-preview) with prompt template 4 in different languages and double equivariant graph models.}
% \begin{tabular}{l rrrrrr rr}
% \toprule
% & \multicolumn{6}{c}{ChatGPT(gpt-4-0125-preview)} & \multicolumn{2}{c}{K-GFMs}  \\
% \cmidrule(lr){2-7}
% \cmidrule(lr){8-9}
%  & \multicolumn{1}{c}{ar} & \multicolumn{1}{c}{de} & \multicolumn{1}{c}{en} & \multicolumn{1}{c}{es} & \multicolumn{1}{c}{ru} & \multicolumn{1}{c}{zh} & \multicolumn{1}{c}{ULTRA} & \multicolumn{1}{c}{\ourmethod} \\
% \midrule
% CoDEx-S & 0.455 & 0.550 & 0.610 & 0.690 & 0.333 & 0.393 & 0.820 & \textbf{0.935} \\
% CoDEx-M & 0.452 & 0.505 & 0.620 & 0.667 & 0.381 & 0.425 & 0.870 & \textbf{0.886} \\
% CoDEx-L & 0.490 & 0.558 & 0.590 & 0.698 & 0.400 & 0.409 & 0.824 & \textbf{0.837} \\
% \bottomrule
% \end{tabular}
% \label{tab:llm-relation}
% \end{table*}

% \paragraph{Example of outputs of ChatGPT} In this part we will show some examples of answers given by ChatGPT (gpt-4-0125-preview). For the sake of saving space, we omit the options of relations in the questions since they are the same as in the template.

% Here are questions that are answered wrongly in both English and Chinese:

% \begin{enumerate}
% \item
% \begin{enumerate}
% \item 
% \textbf{Question in English: }What is the relationship between Charlie Wilson and African Americans? 

% \textbf{The true answer in English: } ethnic group

% \textbf{ChatGPT's answer in English: } member of political party

% \item 
% \begin{CJK}{UTF8}{gbsn}
% \textbf{Question in Chinese: }查理·威尔森 (歌手)和非裔美国人之间的关系是什么?

% \textbf{The true answer in Chinese: }族群

% \textbf{ChatGPT's answer in Chinese: }查理·威尔森（Charlie Wilson）是一名歌手，因此与上述选项中的"职业"最为相关。他作为一名艺术家，通过音乐与人们建立联系，而非裔美国人作为他的身份之一，对他的音乐风格、歌词内容以及他在音乐界的定位有着深刻的影响。因此，如果要从提供的选项中选择一个最佳答案来描述查理·威尔森和非裔美国人之间的关系，那么"职业"是最合适不过的了。
% \end{CJK}

% \end{enumerate}

% \end{enumerate}

% Here are questions that are answered correctly in English but wrongly in Chinese:
% \begin{enumerate}
% \item 
% \begin{enumerate}
%     \item 
%     \textbf{Question in English: }What is the relationship between J. J. Abrams and producer? 
    
%     \textbf{The true answer in English: } occupation
    
%     \textbf{ChatGPT's answer in English: } occupation
%     \item 
%     \begin{CJK}{UTF8}{gbsn}
%     \textbf{Question in Chinese: }杰弗里·雅各布·艾布拉姆斯和制作人之间的关系是什么?
    
%     \textbf{The true answer in Chinese: } 职业
    
%     \textbf{ChatGPT's answer in Chinese: }杰弗里·雅各布·艾布拉姆斯（J.J. Abrams）和制作人之间的关系最佳答案是：|导演|
%     \end{CJK}
% \end{enumerate}

% \item 
% \begin{enumerate}
%     \item 
%     \textbf{Question in English: }What is the relationship between Victor Ambartsumian and German Academy of Sciences Leopoldina?
    
%     \textbf{The true answer in English: } member of
    
%     \textbf{ChatGPT's answer in English: } member of
%     \item 
%     \begin{CJK}{UTF8}{gbsn}
%     \textbf{Question in Chinese: }维克托·安巴楚勉和利奥波第那科学院之间的关系是什么?
    
%     \textbf{The true answer in Chinese: } 所属组织
    
%     \textbf{ChatGPT's answer in Chinese: }维克托·安巴楚勉和利奥波第那科学院之间的关系最合适的选择是|创作者|。
%     \end{CJK}
% \end{enumerate}

% \item 
% \begin{enumerate}
%     \item 
%     \textbf{Question in English: }What is the relationship between Theodor W. Adorno and composer?
    
%     \textbf{The true answer in English: } occupation
    
%     \textbf{ChatGPT's answer in English: } occupation
%     \item 
%     \begin{CJK}{UTF8}{gbsn}
%     \textbf{Question in Chinese: }狄奧多·阿多诺和作曲家之间的关系是什么?
    
%     \textbf{The true answer in Chinese: } 职业
    
%     \textbf{ChatGPT's answer in Chinese: }狄奧多·阿多诺和作曲家之间的关系最佳选项是：**研究对象**。阿多诺是一位德国哲学家、社会学家、音乐学家和作曲家，对20世纪音乐，特别是十二音音乐和其背后的哲学和社会基础有深入研究，因此作曲家是他研究的主要对象之一。
%     \end{CJK}
% \end{enumerate}

% \item 
% \begin{enumerate}
%     \item 
%     \textbf{Question in English: }What is the relationship between Sri Lanka and International Centre for Settlement of Investment Disputes?
    
%     \textbf{The true answer in English: } member of
    
%     \textbf{ChatGPT's answer in English: } member of
%     \item 
%     \begin{CJK}{UTF8}{gbsn}
%     \textbf{Question in Chinese: }斯里兰卡和国际投资争端解决中心之间的关系是什么?
    
%     \textbf{The true answer in Chinese: } 所属组织
    
%     \textbf{ChatGPT's answer in Chinese: }斯里兰卡和国际投资争端解决中心之间的关系最恰当的选项是“法律形式”。这是因为国际投资争端解决中心（ICSID）主要处理投资者和国家间的争端解决，是根据法律协议和程序来操作的。当投资者认为他们的投资受到了违反国际投资协定的待遇时，他们可以选择通过ICSID对相关国家（比如斯里兰卡）提起诉讼。因此，在斯里兰卡与国际投资争端解决中心之间的关系中，法律形式是关键的组成部分。
%     \end{CJK}
% \end{enumerate}
% \end{enumerate}