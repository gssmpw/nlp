\label{proof_section}

\begin{wrapfigure}[14]{}{0.6\textwidth}
\vspace{-10pt}
    \centering
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=3.8cm]{figures/heatmaptrix.pdf}
        \caption{\ourmethod.}
        \label{fig:heatmaptrix}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=3.8cm]{figures/heatmapultra.pdf}
        \caption{ULTRA.}
        \label{fig:heatmapultra}
    \end{subfigure}
    \caption{\revisionlog{Heatmaps of cosine similarities of relation embeddings on NELL995 with \ourmethod and ULTRA. \ourmethod gets more diverse relation embeddings than that of ULTRA.}}
    \label{fig:heatmapscompare}
    \vspace{-15pt}
\end{wrapfigure}

The expressive power of a GNN refers to its ability of distinguishing non-isomorphic graphs. Previous works~\cite{morris2019weisfeiler} have shown that the expressive power of GNN is bounded by Weisfeiler–Leman tests. Recent works~\cite{barcelo2022weisfeiler} also extend Weisfeiler–Leman tests on relational graphs. The expressive power in link prediction is limited due to the automorphic node problem~\cite{chamberlain2022graph}. Labeling trick~\cite{zhang2021labeling, zhu2021neural} breaks the symmetry of entity embedding in link prediction by assigning each node a unique feature vector based on its structural properties. 

\revisionlog{\Cref{sec:expressivity} shows that \ourmethod is more expressive than existing relation-graph based double-equivariant models. Before going into the details, we discuss the implications of this expressivity increase, such that the relation embeddings that become more distinguishable. \Cref{fig:heatmapscompare} shows the heatmaps of cosine similarity of relation embeddings of ULTRA and \ourmethod on the NELL995 dataset. The heatmap of \ourmethod shows more shade ranges than that of ULTRA, which indicates that the relation embeddings of \ourmethod are more distinct.
}

In the following part we discuss the expressive power of \ourmethod. 
We start by formally introducing the four intermediate relation adjacency matrices. Specifically, recall that the entity
adjacency matrix $\mA_V \in \mathbb{R}^{|V| \times |V| \times |R|}$ is obtained by stacking four relation adjacency matrices capturing the four roles $\mA_R^{hh}$ (head-head), $\mA_R^{tt}$ (tail-tail), $\mA_R^{ht}$ (head-tail), and $\mA_R^{th}$ (tail-head). These can be directly obtained by leveraging $\mE_h$ and $\mE_t$, that is:
\begin{align*}
\mA_R^{hh}[r_i, r_j, v_k] &= \mE_h[v_k, r_i] * \mE_h[v_k, r_j], \qquad \mA_R^{tt}[r_i, r_j, v_k] = \mE_t[v_k, r_i] * \mE_t[v_k, r_j],\\
\mA_R^{ht}[r_i, r_j, v_k] &= \mE_h[v_k, r_i] * \mE_t[v_k, r_j], \qquad
\mA_R^{th}[r_i, r_j, v_k] = \mE_t[v_k, r_i] * \mE_h[v_k, r_j].
\end{align*}

Next we will discuss the details of the proof.

\asexpr*

\begin{proof} 
\revision{
In this proof we consider the task of entity prediction. 

\revisionlog{\textbf{\ourmethod is at least as expressive as ULTRA.}} Suppose ULTRA first updates relation embedding with a $k_1$-layer GNN and then updates entity embedding with another $k_2$-layer GNN. We will show that there is a \ourmethod with $(k_1+k_2)$ rounds of iterative updates that will get the same embeddings as ULTRA. \ourmethod uses the same initial relation entity embedding as ULTRA and uses the all-one matrix as initial entity embedding. \revisionlog{To be consistent with ULTRA, \ourmethod uses NBFNet as the GNNLayer.}

For the first $k_1$ rounds, ULTRA only updates relation embedding with message passing on relation graphs. ULTRA's message passing function on relation graph is
\begin{align*}
    % \mX_{h,r}^{(i+1)}(u) &= \revision{\text{UPDATE}\left(\mX_{h,r}^{(i-1)}(u), \text{AGG}\left(\text{MESSAGE}(\mX_{h,r}^{(i-1)}(v), \mZ_{h,r}^{(i-1)}(r^\prime))|(u, r^\prime, v) \in \mA_V\right)\right)} \\
    \mZ_{h,r}^{(i)}(r^\prime) &= \revision{\text{UP}_{R, \text{ULTRA}}^{(i)}\left(\mZ_{h,r}^{(i-1)}(r^\prime), \text{AGG}_{R, \text{ULTRA}}^{(i)}\left(\text{MSG}_{R, \text{ULTRA}}^{(i)}(\mZ_{h,r}^{(i-1)}(r^{\prime\prime})|(r^\prime,r^{\prime\prime})\in \mA^{\text{ULTRA}}_R\right)\right)}
\end{align*}

For \ourmethod, we choose the message passing functions on relation graph as follows: 
\begin{align*}
    % \mX_{h,r}^{(i+1)}(u) &= \revision{\text{UPDATE}\left(\mX_{h,r}^{(i-1)}(u), \text{AGG}\left(\text{MESSAGE}(\mX_{h,r}^{(i-1)}(v), \mZ_{h,r}^{(i-1)}(r^\prime))|(u, r^\prime, v) \in \mA_V\right)\right)} \\
    \mX_{h,r}^{(i)}(u) &= \text{UP}_{V, \text{\ourmethod}}^{(i)}\left(\mX_{h,r}^{(i-1)}(u), \right. \\
    &\left.\text{AGG}_{V, \text{\ourmethod}}^{(i)}\left(\text{MSG}_{V, \text{\ourmethod}}^{(i)}(\mX_{h,r}^{(i-1)}(v), \mZ_{h,r}^{(i-1)}(r^\prime))|(u, r^\prime, v) \in \mA^{\text{\ourmethod}}_V\right)\right) \\
    &= \mX_{h,r}^{(i-1)}(u) = \mathbf{1}^{d}\\
    \mZ_{h,r}^{(i)}(r^\prime) &= \text{UP}_{R, \text{\ourmethod}}^{(i)}\left(\mZ_{h,r}^{(i-1)}(r^\prime),\right.\\ &\left.\text{AGG}_{R, \text{\ourmethod}}^{(i)}\left(\text{MSG}_{R, \text{\ourmethod}}^{(i)}(\mZ_{h,r}^{(i-1)}(r^{\prime\prime}), \mX_{h,r}^{(i)}(u))|(r^\prime,u,r^{\prime\prime})\in \mA^{\text{\ourmethod}}_R\right)\right)
\end{align*}

Both ULTRA and \ourmethod use the non-parametric DistMult as the message function. If $\mX_{h,r}^{(i)}$ is an all-one matrix, we have
\begin{align*}
&\text{MSG}_{R, \text{ULTRA}}^{(i)}(\mZ_{h,r}^{(i-1)}(r^{\prime\prime})|(r^\prime,r^{\prime\prime})\in \mA^{\text{ULTRA}}_R)\\
=&\text{MSG}_{R, \text{\ourmethod}}^{(i)}(\mZ_{h,r}^{(i-1)}(r^{\prime\prime}), \mX_{h,r}^{(i)}(u))|(r^\prime,u,r^{\prime\prime})\in \mA^{\text{\ourmethod}}_R)
\end{align*}

Both ULTRA and \ourmethod use sum as the aggregation function. If $\text{UP}_{R, \text{ULTRA}}^{(i)}$ and $\text{UP}_{R, \text{\ourmethod}}^{(i)}$ always have the same parameter, it will end up with the same $\mZ_{h,r}^{(k_1)}$.

For the last $k_2$ layers, ULTRA only updates entity embedding. ULTRA marks the head entity with the query relation embedding. ULTRA's message passing function on the original knowledge graph is:
\begin{align*}
\mX_{h,r}^{(i)}(u) &= \text{UP}_{V, \text{ULTRA}}^{(i)}\left(\mX_{h,r}^{(i-1)}(u), \right. \\
&\left. \text{AGG}_{V, \text{ULTRA}}^{(i)}\left(\text{MSG}_{V, \text{ULTRA}}^{(i)}(\mX_{h,r}^{(i-1)}(v), \mZ_{h,r}^{(k_1)}(r^\prime))|(u, r^\prime, v) \in \mA^{\text{ULTRA}}_V\right)\right)
\end{align*}

For \ourmethod, it also marks the head entity with the query relation embedding as ULTRA does. We choose the message passing functions on relation graph as follows: 
\begin{align*}
    \mX_{h,r}^{(i+k_1)}(u) &= \text{UP}_{V, \text{\ourmethod}}^{(i+k_1)}\left(\mX_{h,r}^{(i+k_1-1)}(u),\right.\\
    &\left.\text{AGG}_{V, \text{\ourmethod}}^{(i+k_1)}\left(\text{MSG}_{V, \text{\ourmethod}}^{(i+k_1)}(\mX_{h,r}^{(i+k_1-1)}(v), \mZ_{h,r}^{(i+k_1-1)}(r^\prime))|(u, r^\prime, v) \in \mA^{\text{\ourmethod}}_V\right)\right) \\
    \mZ_{h,r}^{(i+k_1)}(r^\prime) &= \text{UP}_{R, \text{\ourmethod}}^{(i+k_1)}\left(\mZ_{h,r}^{(i+k_1-1)}(r^\prime),\right.\\ 
    &\left.\text{AGG}_{R, \text{\ourmethod}}^{(i+k_1)}\left(\text{MSG}_{R, \text{\ourmethod}}^{(i+k_1)}(\mZ_{h,r}^{(i+k_1-1)}(r^{\prime\prime}), \mX_{h,r}^{(i+k_1)}(u))|(r^\prime,u,r^{\prime\prime})\in \mA^{\text{\ourmethod}}_R\right)\right) \\
    &= \mZ_{h,r}^{(i+k_1-1)}(r^\prime) = \mZ_{h,r}^{(k_1)}
\end{align*}

If $\text{UP}_{V, \text{ULTRA}}^{(i)}$ and $\text{UP}_{V, \text{\ourmethod}}^{(i+k_1)}$ always have the same parameter, the relation and entity embedding from ULTRA and \ourmethod are exactly the same. Since the embeddings are the same, any non-isomorphic triplet that can be distinguished by ULTRA can also be distinguished by \ourmethod.
}

\revisionlog{\textbf{\ourmethod is at least as expressive as InGram.} Suppose InGram first updates relation embedding with a $k_1$-layer GNN and then updates entity embedding with another $k_2$-layer GNN. We will show that there is a \ourmethod with $(k_1+k_2)$ rounds of iterative updates that will get the same embeddings as InGram. \ourmethod uses the same random initial relation entity embedding as InGram and uses the all-one matrix as initial entity embedding. To be consistent with InGram, \ourmethod uses an extension of GATv2 as the GNNLayer.

% DEq-InGram is a variant of InGram by applying Monte Carlo sampling in inference~\cite{lee2023ingram}. Each sampling of DEq-InGram is actually a forward pass of InGram. So for simplicity in this proof we will show that \ourmethod can reproduce the message passing of InGram in one forward pass, then by applying Monte Carlo sampling of initial embeddings to both methods, \ourmethod can reproduce the message passing of DEq-InGram.

For the first $k_1$ rounds, InGram only updates relation embedding with message passing on relation graphs. InGram's message passing function on relation graph is
\begin{align*}
    \mZ_{h,r}^{(i)}(r^\prime) &= \text{UP}_{R, \text{InGram}}^{(i)}\left(\mZ_{h,r}^{(i-1)}(r^\prime), \text{AGG}_{R, \text{InGram}}^{(i)}\left(\text{MSG}_{R, \text{InGram}}^{(i)}(\mZ_{h,r}^{(i-1)}(r^{\prime\prime})|(r^\prime,r^{\prime\prime})\in \mA^{\text{InGram}}_R\right)\right)
\end{align*}

For \ourmethod, we choose the message passing functions on relation graph as follows. Here with a slight abuse of notation, $\mA^{\text{\ourmethod}}_{R,hh}$ and $\mA^{\text{\ourmethod}}_{R,tt}$
are the relation adjacency matrices for the head-head and tail-tail connections respectively.
\begin{align*}
    \mX_{h,r}^{(i)}(u) &= \text{UP}_{V, \text{\ourmethod}}^{(i)}\left(\mX_{h,r}^{(i-1)}(u), \right. \\
    &\left.\text{AGG}_{V, \text{\ourmethod}}^{(i)}\left(\text{MSG}_{V, \text{\ourmethod}}^{(i)}(\mX_{h,r}^{(i-1)}(v), \mZ_{h,r}^{(i-1)}(r^\prime))|(u, r^\prime, v) \in \mA^{\text{\ourmethod}}_V\right)\right) \\
    &= \mX_{h,r}^{(i-1)}(u) = \mathbf{1}^{d}\\
    \mZ_{h,r}^{(i)}(r^\prime) &= \text{UP}_{R, \text{\ourmethod}}^{(i)}\left(\mZ_{h,r}^{(i-1)}(r^\prime),\right.\\ &\left.\text{AGG}_{R, \text{\ourmethod}}^{(i)}\left(\text{MSG}_{R, \text{\ourmethod}}^{(i)}(\mZ_{h,r}^{(i-1)}(r^{\prime\prime}), \mX_{h,r}^{(i)}(u))|(r^\prime,u,r^{\prime\prime})\in \left(\mA^{\text{\ourmethod}}_{R,hh} \cup \mA^{\text{\ourmethod}}_{R,tt}\right) \right)\right)
\end{align*}

\ourmethod picks the same message function as InGram. If $\mX_{h,r}^{(i)}$ is an all-one matrix, we have
\begin{align*}
&\text{MSG}_{R, \text{InGram}}^{(i)}(\mZ_{h,r}^{(i-1)}(r^{\prime\prime})|(r^\prime,r^{\prime\prime})\in \mA^{\text{InGram}}_R)\\
=&\text{MSG}_{R, \text{\ourmethod}}^{(i)}(\mZ_{h,r}^{(i-1)}(r^{\prime\prime}), \mX_{h,r}^{(i)}(u))|(r^\prime,u,r^{\prime\prime})\in \left(\mA^{\text{\ourmethod}}_{R,hh} \cup \mA^{\text{\ourmethod}}_{R,tt}\right))
\end{align*}

Both InGram and \ourmethod use sum as the aggregation function. If $\text{UP}_{R, \text{InGram}}^{(i)}$ and $\text{UP}_{R, \text{\ourmethod}}^{(i)}$ always have the same parameter, it will end up with the same $\mZ_{h,r}^{(k_1)}$.

For the last $k_2$ layers, InGram only updates entity embedding. InGram's message passing function on the original knowledge graph is:
\begin{align*}
\mX_{h,r}^{(i)}(u) &= \text{UP}_{V, \text{InGram}}^{(i)}\left(\mX_{h,r}^{(i-1)}(u), \right. \\
&\left. \text{AGG}_{V, \text{InGram}}^{(i)}\left(\text{MSG}_{V, \text{InGram}}^{(i)}(\mX_{h,r}^{(i-1)}(v), \mZ_{h,r}^{(k_1)}(r^\prime))|(u, r^\prime, v) \in \mA^{\text{InGram}}_V\right)\right)
\end{align*}

For \ourmethod, we choose the message passing functions on relation graph as follows and change the entity embedding to the same initial entity embedding as InGram: 
\begin{align*}
    % \mX_{h,r}^{(i+1)}(u) &= \revision{\text{UPDATE}\left(\mX_{h,r}^{(i-1)}(u), \text{AGG}\left(\text{MESSAGE}(\mX_{h,r}^{(i-1)}(v), \mZ_{h,r}^{(i-1)}(r^\prime))|(u, r^\prime, v) \in \mA_V\right)\right)} \\
    \mX_{h,r}^{(i+k_1)}(u) &= \text{UP}_{V, \text{\ourmethod}}^{(i+k_1)}\left(\mX_{h,r}^{(i+k_1-1)}(u),\right.\\
    &\left.\text{AGG}_{V, \text{\ourmethod}}^{(i+k_1)}\left(\text{MSG}_{V, \text{\ourmethod}}^{(i+k_1)}(\mX_{h,r}^{(i+k_1-1)}(v), \mZ_{h,r}^{(i+k_1-1)}(r^\prime))|(u, r^\prime, v) \in \mA^{\text{\ourmethod}}_V\right)\right) \\
    \mZ_{h,r}^{(i+k_1)}(r^\prime) &= \text{UP}_{R, \text{\ourmethod}}^{(i+k_1)}\left(\mZ_{h,r}^{(i+k_1-1)}(r^\prime),\right.\\ 
    &\left.\text{AGG}_{R, \text{\ourmethod}}^{(i+k_1)}\left(\text{MSG}_{R, \text{\ourmethod}}^{(i+k_1)}(\mZ_{h,r}^{(i+k_1-1)}(r^{\prime\prime}), \mX_{h,r}^{(i+k_1)}(u))|(r^\prime,u,r^{\prime\prime})\in \mA^{\text{\ourmethod}}_R\right)\right) \\
    &= \mZ_{h,r}^{(i+k_1-1)}(r^\prime) = \mZ_{h,r}^{(k_1)}
\end{align*}

If $\text{UP}_{V, \text{InGram}}^{(i)}$ and $\text{UP}_{V, \text{\ourmethod}}^{(i+k_1)}$ always have the same parameter, the relation and entity embedding from InGram and \ourmethod are exactly the same. Since the embeddings are the same, any non-isomorphic triplet that can be distinguished by InGram can also be distinguished by \ourmethod.
}
\end{proof} 

\triplets*
\begin{proof}
\revisionlog{\textbf{\ourmethod is more expressive than ULTRA.}} Consider the two triplets ($v_{21}$, $r_3$, $v_{24}$) and ($v_{21}$, $r_3$, $v_{25}$) in \Cref{fig:example}, which are non-isomorphic because $r_1 \neq r_2$, and assume they are not observable (i.e., they do not exist in the input graph). Assume, however, we want to predict the existence of these two, with the first one having label 1 (exists) and the second one having label 0 (does not exist). In practice, this means that the first one represents a missing link in an incomplete KG, while the other does not exist and should not be predicted as missing. The proof shows the impossibility of ULTRA to distinguish these two and therefore to make different predictions for the two.

In the relation graph of ULTRA, $r_1$ and $r_2$ are \revisionlog{automorphic}. This implies that ULTRA will give $r_1$ and $r_2$ the same relation embedding. Since this embedding is then used by ULTRA to obtain the entity embeddings, ULTRA will assign $v_{24}$ and $v_{25}$ the same entity embedding. Therefore, ULTRA will assign ($v_{21}$, $r_3$, $v_{24}$) and ($v_{21}$, $r_3$, $v_{25}$) the same representation, and therefore it will necessarily make the same prediction for these two (predict that they both exist or that they both do not exist, even though the ground truth tell us only one exist).

On the contrary, in the relation graph of \ourmethod, $r_1$ and $r_2$ are not \revisionlog{automorphic}. This implies that \ourmethod can give $r_1$ and $r_2$ different relation embeddings, and consequently $v_{24}$ and $v_{25}$ can get different entity embeddings. Therefore, \ourmethod can assign ($v_{21}$, $r_3$, $v_{24}$) and ($v_{21}$, $r_3$, $v_{25}$) different representations, and therefore it can make different predictions for these two (predict that only the first exists, as the ground truth). Therefore, we have just shown that there exist two non-isomorphic triplets that TRIX can distinguish but ULTRA cannot. 

\revisionlog{
\textbf{\ourmethod is more expressive than InGram.} Consider the two triplets ($v_{7}$, $r_3$, $v_{10}$) and ($v_{7}$, $r_3$, $v_{11}$) in \Cref{fig:exampleingram}, which are non-isomorphic because $r_1 \neq r_2$, and assume they are not observable (i.e., they do not exist in the input graph). Assume, however, we want to predict the existence of these two, with the first one having label 1 (exists) and the second one having label 0 (does not exist). In practice, this means that the first one represents a missing link in an incomplete KG, while the other does not exist and should not be predicted as missing. The proof shows the impossibility of InGram to distinguish these two and therefore to make different predictions for the two.

In the relation graph of InGram, $r_1$ and $r_2$ are \revisionlog{automorphic}. This implies that InGram will give $r_1$ and $r_2$ the same relation embedding. Since this embedding is then used by InGram to obtain the entity embeddings, InGram will assign $v_{10}$ and $v_{11}$ the same entity embedding. Therefore, InGram will assign ($v_{7}$, $r_3$, $v_{10}$) and ($v_{7}$, $r_3$, $v_{11}$) the same representation, and therefore it will necessarily make the same prediction for these two (predict that they both exist or that they both do not exist, even though the ground truth tell us only one exist).

On the contrary, in the relation graph of \ourmethod, $r_1$ and $r_2$ are not \revisionlog{automorphic}. This implies that \ourmethod can give $r_1$ and $r_2$ different relation embeddings, and consequently $v_{10}$ and $v_{11}$ can get different entity embeddings. Therefore, \ourmethod can assign ($v_{7}$, $r_3$, $v_{10}$) and ($v_{7}$, $r_3$, $v_{11}$) different representations, and therefore it can make different predictions for these two (predict that only the first exists, as the ground truth). Therefore, we have just shown that there exist two non-isomorphic triplets that TRIX can distinguish but InGram cannot. 

}
\end{proof}

% \begin{figure}[t]
% % \begin{wrapfigure}[14]{}{0.6\textwidth}
% \vspace{-10pt}
%     \centering
%     \begin{subfigure}[t]{0.48\linewidth}
%         \centering
%         \includegraphics[width=3.8cm]{figures/heatmaptrix.pdf}
%         \caption{\ourmethod.}
%         \label{fig:heatmaptrix}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.48\linewidth}
%         \centering
%         \includegraphics[width=3.8cm]{figures/heatmapultra.pdf}
%         \caption{ULTRA.}
%         \label{fig:heatmapultra}
%     \end{subfigure}
%     \caption{\revisionlog{Heatmaps of cosine similarities of relation embeddings on NELL995 with \ourmethod and ULTRA. \ourmethod gets more diverse relation embeddings than that of ULTRA.}}
%     \label{fig:heatmapscompare}
%     \vspace{-15pt}
% % \end{wrapfigure}
% \end{figure}

\label{second_proof}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\textwidth]{figures/example.pdf}
\caption{A counter example \revisionlog{graph with 10 disconnected components} where there exist non-isomorphic triplets that can be distinguished by \ourmethod but that cannot be distinguished by ULTRA. In the figure, solid lines are edges observed in the knowledge graph and dashed lines are edges we want to predict whether existing or not. As discussed in \Cref{second_proof}, ULTRA always gives two dashed edges the same embedding while \ourmethod can distinguish them.}
\label{fig:example}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{figures/exampleingram.pdf}
\caption{\revisionlog{A counter example graph with 3 disconnected components where there exist non-isomorphic triplets that can be distinguished by \ourmethod but that cannot be distinguished by InGram. In the figure, solid lines are edges observed in the knowledge graph and dashed lines are edges we want to predict whether existing or not. As discussed in \Cref{second_proof}, InGram always gives two dashed edges the same embedding while \ourmethod can distinguish them.}}
\label{fig:exampleingram}
\end{figure}


\moreexpr*


\revision{\Cref{theo:asexpr} shows that any non-isomorphic triplet that can be distinguished by ULTRA \revisionlog{or InGram} can also be distinguished by \ourmethod. \Cref{theo:triplets} shows that there exist non-isomorphic triplets that can be distinguished by \ourmethod but that cannot be distinguished by ULTRA \revisionlog{or InGram}}. From \Cref{theo:asexpr,theo:triplets} directly follows that \ourmethod is more expressive than ULTRA \revisionlog{and InGram}.

\begin{proposition}
Define $\alpha=\max\limits_i (\max (\sum\limits_j \mathbbm{1}_{\mE_h[i, j] > 0}, \sum\limits_j \mathbbm{1}_{\mE_t[i, j] > 0}))$ which means $\alpha$ is the maximum number of unique relations one single entity connects to as the head or the tail. Then the relation adjacency matrix has $|R|$ nodes, $O(|V|\alpha^2)$ edges and $|V|$ relations.
\end{proposition}
\begin{proof}
    The dimension of relation adjacency matrix is $\mathbb{R}^{|R| \times |R| \times |V| \times 4}$. Since each node is affiliated with at most $\alpha$ relations and any two of these relations may generate four non-zero entries (one for head-to-head; one for tail-to-tail; one for head-to-tail; one for tail-to-head) in the relation graph, each node creates at most $4\alpha^2$ non-zero entries in the relation adjacency matrix, implying that the number of nodes is $O(|V|\alpha^2)$. Moreover, the total number of edges in the relation adjacency matrix is at most $4|V|\alpha^2$.
\end{proof}

Now, let us suppose \revision{there are $L$ rounds of iterative updates} and embedding dimension is $d$. The time complexity of \revision{entity embedding update} in \ourmethod is $O(L|E|d+L|V|d^2)$. The time complexity of \revision{relation embedding update} in \ourmethod is $O(L|V|\alpha^2d+L|R|d^2)$. Since $L$ and $d$ are always $O(1)$, the time complexity of \ourmethod is $O(|E|+|V|\alpha^2)$ for each query.

 The time complexity of relation feature propagation in ULTRA is $O(L|R|^2d+L|R|d^2)$. The time complexity of entity feature propagation in ULTRA is $O(L|E|d+L|V|d^2)$. Since $d$ and $L$ are always $O(1)$, the time complexity of ULTRA is $O(|E|+|V|+|R|^2)$ for each entity prediction query. However, it goes to $O(|E||R|+|V||R|+|R|^3)$ for each relation prediction query.

 For completeness, we report in the following the statistics of the original graph and the relation graph in three exemplary datasets as shown in \Cref{alpha}. Empirically, the number of edges in the relation graph is usually 4 to 10 times the number of edges in the original graph. This means for entity prediction query, the complexity of ULTRA is up to 10 times better than \ourmethod. However, in relation prediction, the complexity of \ourmethod is up to 20 times better than that of ULTRA since ULTRA needs to perform one forward pass for each relation in the relation set. 

 \begin{table*}[t]
\caption{Statistics of relation graph in pre-training datasets.}
\centering
\label{alpha}
\begin{tabular}{l|rrrr}
\toprule
 & $\alpha$ & \# relation & \# edge in original graph & \# edge in relation graph  \\
\midrule
WN18RR & 7 & 11 & 86835 & 314481 \\
FB15k237 & 53 & 237 & 272115 & 2247123\\ 
CoDExMedium & 19 & 51 & 185584 & 706534\\
\bottomrule
\end{tabular}
\end{table*}