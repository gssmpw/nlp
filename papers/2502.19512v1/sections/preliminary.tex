A Knowledge Graph $G$ is a tuple $(V, E, R)$ where $V$ is a finite set of entities, $R$ is a finite set of relations and $E \in (V \times R \times V)$ is a finite set of edges representing relations between entities. We denote by $G_{\text{train}}=(V_{\text{train}}, E_{\text{train}}, R_{\text{train}})$ the training graph, and by $G_{\text{inf}}=(V_{\text{inf}}, E_{\text{inf}}, R_{\text{inf}})$ the test (or inference) graph. \revision{Since we focus on inductive settings, where the test graph comes from a different domain, the training and the test graphs have disjoint entity and relation sets, that is, $V_{\text{inf}} \not\subset V_{\text{train}}$ and $R_{\text{inf}} \not\subset R_{\text{train}}$.} Despite being unseen, it is however assumed that the test graph shares certain structural patterns with the training graph. These structural similarities imply the existence of invariances sufficient for accurate predictions. Consequently, models trained on the training graph can transfer the learned knowledge to the test graph, leveraging the shared structural patterns for effective predictions.

Since KGs are often incomplete~\citep{rossi2021knowledge}, KGC has been widely utilized to infer missing information by predicting missing triplets. KGC comprises two key tasks: entity prediction\cite{bordes2013translating} addresses queries $(h, r, ?)$ which predicts the tail entity given a head entity $h$ and a relation $r$ (or, equivalently, predict the head); relation prediction\cite{teru2020inductive}, on the other hand, focuses on queries $(h, ?, t)$, aiming to predict the existence of a link between a head entity $h$ and a tail entity $t$, and determining the relation type.


To perform entity and relation predictions in inductive settings, \citet{gao2023double} recently identified the concept of double-equivariance, i.e., equivariance to both entity and relation ids permutations, as a necessary property that \revision{fully inductive models} must possess. \revision{This property ensures that the architecture does not use relation and entity IDs, but, instead, captures the interactions among them. Indeed, even if relations and entities vary across datasets, the interactions between them may be similar and transferable.} Existing \revision{fully inductive models} achieve double equivariance either by designing architectures that are intrinsically equivariant to both permutation groups~\citep{gao2023double}, or by constructing a relation graph that captures relation interactions regardless of their ids or semantics~\citep{lee2023ingram,galkin2023towards, gao2023double}. Since the latter tends to be a lighter approach, which also yields better results in practice, we will in the following focus on that. In particular, these methods first construct a relation graph where entities are relations and edges represent the number of times two relations share an entity. This relation graph is then passed to a standard graph neural network with labeling trick~\citep{zhang2021labeling} to obtain relation representations conditioned on the query of interest, which can directly be used in inductive settings. We show how \ourmethod extends this approach and constructs a relation graph that does not merely count the number of entities shared among two relations, but identify which entities are shared, an approach that we show to return strictly more expressive representations in \Cref{sec:expressivity}.

Finally, we remark that existing \revision{fully inductive models} typically begin by applying the labeling trick~\citep{zhang2021labeling} to obtain initial relative relation representations, conditioned on the query relation~\citep{galkin2023towards}. However, in the relation prediction ($h$, ?, $t$), there is no specific query relation available for conditioning, as this is unknown and constitutes the target of our query. As a result, existing methods must convert relation prediction into a triplet ranking problem, evaluating the possibility of ($h$, $r$, $t$) for all relations $r$ in the relation set. This implies that the model needs to perform one forward pass for each relation. 
%On the contrary,
In contrast,
we show in the next section that \ourmethod can perform the task in one single forward pass. \label{preliminary}