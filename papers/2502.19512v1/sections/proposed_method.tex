\revision{As discussed in previous sections, existing fully inductive models suffer from limited expressivity due to the way in which they construct the relation graph and inefficiency in relation prediction tasks due to the message passing scheme. We fill in these gaps by proposing the framework of \ourmethod, in which the design of our new relation graph records the entity property to enhance expressive power and to the design of iterative message passing mechanism makes relation prediction in one forward pass possible.}
Then we explore \ourmethod's theoretical properties, such as expressiveness and time complexity. An overview of \ourmethod is illustrated in \Cref{fig:ourmethod}.

\subsection{Relation Adjacency Matrix}
% \begin{figure}[t]
% \label{fig:adj}
% \centering
% \includegraphics[width=1.1\textwidth]{figures/relation.pdf}
% \vspace{-.7in}
% \caption{The construction of our proposed relation graph. The entities in the original graph is the relations in the newly constructed relation graph while the relations in the original graph are entities. \beatrice{make font larger}\bruno{it also seems redundant w.r.t.\ Figure 1.}}
% \vspace{-.2in}
% \end{figure}

\revision{Existing methods have limited expressive power. For instance, the relation graphs of both ULTRA~\citep{galkin2023towards} and InGram~\citep{lee2023ingram} are too invariant: In \Cref{fig:example} we show how the edges of the relation graphs of these existing methods (how many common entities two relations have) are not expressive enough for the entity prediction task. We increase expressiveness by including entity information, that is, edges between two relations contain information of which entities share these relations.
}
Given the graph $G$, with adjacency matrix $\mA_V \in \mathbb{R}^{|V| \times |V| \times |R|}$ (entity adjacency matrix), \ourmethod first constructs a relation graph $G_R$ with adjacency matrix $\mA_R$ (relation adjacency matrix). Entities in $\mA_R$ represent the relations in $\mA_V$, and edges in $\mA_R$ denote entities (in $\mA_V$) that share two relations (in $\mA_V$).
Specifically, for any pair of relations $r_i, r_j \in R$ in the original graph $G$, we count how many times each entity $v_k \in V$ is part of triplets involving these two relations as:
\begin{enumerate*}[label=(\arabic*)]
    \item the head entity in both (that is, how many triplets like ($v_k$, $r_i$, $\star$) and ($v_k$, $r_j$, $\star$) exist, where $\star$ is a placeholder for entities),
    \item the tail entity in both (that is ($\star$, $r_i$, $v_k$) and ($\star$, $r_j$, $v_k$)),
    \item the head in the first and the tail in the other
    (that is ($v_k$, $r_i$, $\star$) and ($\star$, $r_j$, $v_k$)), or
    \item vice-versa
    (that is ($\star$, $r_i$, $v_k$) and ($v_k$, $r_j$, $\star$)).
\end{enumerate*}

Since we keep these four roles (head-head, tail-tail, head-tail, tail-head) separate, and we count the above for each entity $v \in V$, the relation adjacency matrix $\mA_R$ \revision{is a tensor of shape} $|R| \times |R| \times |V| \times 4$. Note that this is in contrast with previous methods~\citep{galkin2023towards,lee2023ingram}, that, despite also maintaining the distinct roles, do not differentiate which entities participate in the role but only how many, a choice that impacts the expressive power as we shall see next.


Mathematically, we first construct two matrices $\mE_h \in \mathbb{R}^{|V| \times |R|}$ and $\mE_t \in \mathbb{R}^{|V| \times |R|}$ capturing how many times each entity 
$v \in V$ is the head of triplets involving relation $r \in R$, or the tail, respectively.
Then, we construct four different intermediate relation adjacency matrices capturing the four roles $\mA_R^{hh}$, $\mA_R^{tt}$, $\mA_R^{ht}$, and $\mA_R^{th} \in \mathbb{R}^{|R| \times |R| \times |V|}$. These can be directly obtained by leveraging $\mE_h$ and $\mE_t$. For instance, the entry $\mA_R^{hh}[r_i, r_j, v_k]$, which counts how many times $v_k \in V$ is part of triplets involving both relation $r_i \in R$ and relation $r_j \in R$ while being the head entity in both, can be obtained by multiplying entries in $\mE_h$ as follows:
\begin{equation}
    \mA_R^{hh}[r_i, r_j, v_k] = \mE_h[v_k, r_i] * \mE_h[v_k, r_j].
\end{equation}
%
The equations for $\mA_R^{tt}$, $\mA_R^{ht}$, $\mA_R^{th}$ are obtained equivalently by substituting $\mE_h$ with $\mE_t$ appropriately and we refer the reader to \Cref{app:expressive-power} for explicit definitions.
These four intermediate adjacency relations are then stacked along the last dimensions, yielding a single $\mA_R \in \mathbb{R}^{|R| \times |R| \times |V| \times 4}$.
\Cref{fig:ourmethod} ((a), (b), (c) and (d)) contains an illustrative example of these four matrices. 
%
%
Finally, we remark that, although the shape $|R| \times |R| \times |V| \times 4$ of $\mA_R$ might look massive, it is in fact just an additional list of edge attributes and takes negligible additional space when relying on sparse matrix representations.

\subsection{Iterative Entity and Relation Embedding Updates}
\revision{Existing fully inductive models~\citep{galkin2023towards, lee2023ingram} sequentially perform message passing on the relation adjacency matrix to derive relation representations and subsequently on the entity adjacency matrix using the derived relation representations. As mentioned in \Cref{preliminary}, this sequential message passing does not align with the labeling trick and leads to inefficiency in relation prediction task. We propose a simultaneous refinement process through iterative updates which aligns well with labeling tricks of both relation and entity prediction task. With the more informative relation adjacency matrix $A_R$ proposed in the last part, this iterative embedding update scheme also makes full use of the entity information in $A_R$ to generate strictly more expressive triplet embeddings.} Specifically, we perform message passing updates on $\mA_R$, employing the entity representations as relation embeddings (since entities in $\mA_V$ correspond to relations in $\mA_R$). Subsequently, we proceed with message passing layers on $\mA_V$, utilizing the recently updated relation representations as the relation embeddings. This iterative process is repeated multiple times, ensuring a cohesive refinement of both relation and entity representations throughout the procedure.

In this subsection, we describe mathematically the iterative updates on the entity adjacency matrix and the relation adjacency matrix for the two tasks of interest, namely entity and relation predictions. \revision{\ourmethod is not jointly trained on both tasks, but the proposed framework can be adapted to either solve entity prediction or relation prediction tasks with different initial embeddings.} \revisionlog{The objective functions for optimizing the model for these tasks are in \Cref{loss_function}.}

Let $\mX^{(i)} \in \mathbb{R}^{|V| \times d}$ and $\mZ^{(i)} \in \mathbb{R}^{|R| \times d}$ denote the entity representations and the relation representations of dimension $d$ at any given layer $i$. Following previous methods~\citep{galkin2023towards,lee2023ingram}, we use \revision{NBFNet layers} with labeling tricks~\citep{zhang2021labeling} to obtain relative representations conditioned to the query of interest. Therefore, we will use subscripts to denote the conditioning set. For example, $\mX^{(i)}_{h,r}$ and $\mZ^{(i)}_{h,r}$ will denote the entity and relation representations conditioned on entity $h$ and relation $r$.
With a slight abuse of notation, we will then refer to the relative representation of entity $u$ conditioned on entity $h$ and relation $r$ as $\mX^{(i)}_{h,r}(u)$, and, similarly, $\mZ^{(i)}_{h,r}(r^\prime)$ will denote the relative representation of relation $r^\prime$ conditioned on entity $h$ and relation $r$. 


\textbf{\revision{Embeddings for} Entity Prediction Tasks.}
For an entity prediction query $(h, r, ?)$, we leverage the labeling trick and initialize the embeddings of entities and relations to be conditioned on $h$ and $r$ as: %follows:
% bold for matrix
% \begin{align}
%     \mX^{(0)}_{h,r}(h^\prime) = \mathbbm{1}_{h=h^\prime} \times  \mathbf{1}^{d}, \qquad \mZ^{(0)}_{h,r}(r^\prime) = \mathbbm{1}_{r=r^\prime} \times  \mathbf{1}^{d},
% \end{align}
\begin{align}
    \mX^{(0)}_{h,r}(u) = \revision{\text{INIT}_{V}(h, u)}, \qquad \mZ^{(0)}_{h,r}(r^\prime) = \revision{\text{INIT}_{R}(r, r^\prime)}
\end{align}

\revision{where \text{INIT} is the initialization function for embeddings. The initial embeddings for entity $h$ and for relation $r$ are set to all-one vectors, while all the other entities and relations receive initial zero vectors.}
% where $\mathbbm{1}$ denotes the indicator function and $\mathbf{1}^{d}$ is an all-one vector of dimension $d$.  
% This means that the initial embeddings for entity $h$ and for relation $r$ are set to all-one vectors, while all the other entities and relations receive initial zero vectors. 
Subsequently, we perform iterative updates as follows:
% \begin{align}
%      \mX_{h,r}^{(i)} &= \text{GNN}_V^{(i)}(\mA_V, \mX_{h,r}^{(i-1)}, \mZ_{h,r}^{(i-1)}), \\
%      \mZ_{h,r}^{(i)} &= \text{GNN}_R^{(i)}(\mA_R, \mZ_{h,r}^{(i-1)}, \mX_{h,r}^{(i)}),
% \end{align}
%
\begin{align}
    \mX_{h,r}^{(i)}(u) 
    &= \text{GNNLayer}_V^{(i)}(\mA_V, \mX_{h,r}^{(i-1)}, \mZ_{h,r}^{(i-1)}, u)  \nonumber \\
    &= \revision{\text{UP}_{V}^{(i)}\left(\mX_{h,r}^{(i-1)}(u), \text{AGG}_{V}^{(i)}\left(\text{MSG}_{V}^{(i)}(\mX_{h,r}^{(i-1)}(v), \mZ_{h,r}^{(i-1)}(r^\prime))|(u, r^\prime, v) \in \mA_V\right)\right)} \label{ee} \\
    \mZ_{h,r}^{(i)}(r^\prime) 
    &= \text{GNNLayer}_R^{(i)}(\mA_R, \mZ_{h,r}^{(i-1)}, \mX_{h,r}^{(i)}, r^\prime) \nonumber \\
    &= \revision{\text{UP}_{R}^{(i)}\left(\mZ_{h,r}^{(i-1)}(r^\prime), \text{AGG}_{R}^{(i)}\left(\text{MSG}_{R}^{(i)}(\mZ_{h,r}^{(i-1)}(r^{\prime\prime}), \mX_{h,r}^{(i)}(u))|(r^\prime,u,r^{\prime\prime})\in \mA_R\right)\right)} \label{er}
\end{align}
%
% where $\text{GNN}_V^{(i)}$ and $\text{GNN}_R^{(i)}$ are two distinct \revision{NBFNet} layers taking as input the corresponding adjacency matrix, the representations of the nodes and the representations of the relations in such adjacency matrix. 
%
\revision{$\text{GNNLayer}_V^{(i)}$ and $\text{GNNLayer}_R^{(i)}$ are NBFNet layers where $\text{UP}^{(i)}$, $\text{AGG}^{(i)}$ and $\text{MSG}^{(i)}$ stand for update, aggregation, and message functions at the $i$-th layer, respectively. Following NBFNet, the message function is DistMult, the aggregation function is sum, and the update function is a multi-layer perceptron. The pseudo code is shown in \Cref{entity_code} in the Appendix.} The final entity embeddings are passed to a multi-layer perceptron for entity prediction. 

% Recall that since nodes in $\mA_R$ are the relations and edges are instead entities, the representations of the nodes and the representations of the relations are swapped with respect to those in $\mA_V$.
%

In knowledge graphs, every relation typically has a corresponding reverse relation. To handle the entity prediction query $(?, r, t)$, we can simply transform it into $(t, r^{-1}, ?)$ by utilizing the reverse relation and treat it as a tail prediction as above.

\textbf{\revision{Embeddings for} Relation Prediction Tasks.}
For a relation prediction query $(h, ?, t)$, we leverage the labeling trick and initialize the embeddings of entities and relations to be conditioned on $h$ and $t$ as follows:
% \begin{align}
%     \mX_{h,t}^{(0)}(h^\prime) = (1 \times \mathbbm{1}_{h^\prime=h} + (-1) \times \mathbbm{1}_{h^\prime=t})\times \mathbf{1}^{d}, \qquad \mZ_{h,t}^{(0)}(r^\prime) = \mathbf{1}^{d},
% \end{align}
\begin{align}
    \mX_{h,t}^{(0)}(u) = \revision{\text{INIT}(h, t, u)}, \qquad \mZ_{h,t}^{(0)}(r^\prime) = \revision{\mathbf{1}^{d}},
\end{align}
\revision{where $\mathbf{1}^{d}$ is an all-one vector of dimension $d$.} 
This means that the initial embedding for entity $h$ is set to all-one vectors, the initial embedding for entity $t$ is set to all-minus-one vectors, while all the other entities receive initial zero vectors. Furthermore, the relations are initialized all to all-one vectors.
Subsequently, we perform iterative embedding updates as follows.
% \begin{align}
%     \mZ_{h,t}^{(i)} &= \text{GNN}_R^{(i)}(\mA_R, \mZ_{h,t}^{(i-1)}, \mX_{h,t}^{(i-1)}) \\
%     \mX_{h,t}^{(i)} &= \text{GNN}_V^{(i)}(\mA_V, \mX_{h,t}^{(i-1)}, \mZ_{h,t}^{(i)})
% \end{align}
Same as entity prediction, $\text{GNNLayer}_V^{(i)}$ and $\text{GNNLayer}_R^{(i)}$ are NBFNet layers:
\begin{align}
    \mZ_{h,t}^{(i)}(r) &= \text{GNNLayer}_R^{(i)}(\mA_R, \mZ_{h,t}^{(i-1)}, \mX_{h,t}^{(i-1)}, r) \nonumber\\
    % \mZ_{h,t}^{(i)}(r) 
    &= \revision{\text{UP}_{R}^{(i)}\left(\mZ_{h,t}^{(i-1)}(r), \text{AGG}_{R}^{(i)}\left(\text{MSG}_{R}^{(i)}(\mZ_{h,t}^{(i-1)}(r^\prime), \mX_{h,t}^{(i-1)}(u))|(r,u,r^\prime)\in \mA_R\right)\right)} \label{rr}\\
    \mX_{h,t}^{(i)}(u) 
    &= \text{GNNLayer}_V^{(i)}(\mA_V, \mX_{h,t}^{(i-1)}, \mZ_{h,t}^{(i)}, u)  \nonumber\\
    &= \revision{\text{UP}_{V}^{(i)}\left(\mX_{h,t}^{(i-1)}(u), \text{AGG}_{V}^{(i)}\left(\text{MSG}_{V}^{(i)}(\mX_{h,t}^{(i-1)}(v), \mZ_{h,t}^{(i)}(r))|(u, r, v) \in \mA_V\right)\right)} \label{re}
    % \text{GNN}_V^{(i)}(\mA_V, \mX_{h,t}^{(i)}, \mZ_{h,t}^{(i+1)}) .
\end{align}
% \revision{where $\text{UP}^{(i)}$, $\text{AGG}^{(i)}$ and $\text{MSG}^{(i)}$ stand for update, aggregation and message functions at the $i$-th layer. Following NBFNet, DistMult is chosen as the message function and the aggregation function is sum. 

The pseudo code is shown in \Cref{relation_code} in the Appendix. The final relation embeddings are passed to a multi-layer perceptron for the prediction. %\revision{Please refer to \Cref{loss_function} for the loss functions of these two tasks.}
% where $\text{GNN}_V^{(i)}$ and $\text{GNN}_R^{(i)}$ are two distinct GNN layers taking as input the corresponding adjacency matrix, the representations of the nodes and the representations of the relations in such adjacency matrix.

\subsection{{\revision{\ourmethod Properties}}}
% lemma go to appendix and cite
\label{sec:expressivity}
% We borrow concepts from expressive community the ability to distinguish triplets

We conduct a theoretical analysis to compare the expressiveness of \ourmethod with \revisionlog{existing relation-graph based fully-inductive models, namely ULTRA~\cite{galkin2023towards}, InGram~\cite{lee2023ingram} and DEq-InGram~\cite{gao2023double}.} More precisely, we aim to investigate the power of both methods to distinguish non-isomorphic triplets.
All proofs can be found in \Cref{app:expressive-power}. 
%
We begin by showing that \ourmethod is at least as expressive as \revisionlog{ULTRA and InGram}, since it can distinguish all non-isomorphic triplets that \revisionlog{they can distinguish. DEq-InGram is a variant of InGram by applying Monte Carlo sampling in inference~\cite{lee2023ingram}. Each sampling of DEq-InGram is actually a forward pass of InGram. So for simplicity in the proof we do not include DEq-InGram.}

\begin{restatable}[\ourmethod at least as powerful as ULTRA \revisionlog{and InGram}]{lemma}{asexpr}\label{theo:asexpr}
Any non-isomorphic triplet that can be distinguished by ULTRA \revisionlog{or InGram} can also be distinguished by \ourmethod\ 
 \revisionlog{with certain choices of hyperparameters and initial embeddings}.
\end{restatable}

We prove this by showing ULTRA \revisionlog{and InGram are actually special cases} of \ourmethod. That is, there exist choices of hyperparameters such that \ourmethod can precisely implement ULTRA \revisionlog{and InGram respectively} to reproduce the message passing process of them and can get the same triplet embeddings.
%
However, the contrary is not true: there exist cases where ULTRA \revisionlog{or InGram} cannot obtain the same triplet embeddings as \ourmethod, regardless of hyperparameter or weight choices. 

% Indeed, we prove in the following that there exist non-isomorphic triplets that ULTRA cannot distinguish, while \ourmethod can. This implies that ULTRA will give these triplets the same representations, while \ourmethod can assign them different representations.

\begin{restatable}[\ourmethod can distinguish triplets ULTRA \revisionlog{and InGram} cannot]{lemma}{triplets}\label{theo:triplets}
There exist non-isomorphic triplets that can be distinguished by \ourmethod but that cannot be distinguished by ULTRA \revisionlog{and InGram}.
\end{restatable}

We prove this by constructing exemplary triplets that are clearly non-isomorphic, and then showing that they can be distinguished by \ourmethod but can not be distinguished by ULTRA \revisionlog{and InGram}. Finally, we can combine \Cref{theo:asexpr,theo:triplets} into \Cref{theo:moreexpr}. 

\begin{restatable}[\ourmethod is more expressive than ULTRA \revisionlog{and InGram}]{theorem}{moreexpr}\label{theo:moreexpr}
\ourmethod is strictly more expressive than ULTRA \revisionlog{and InGram} in distinguishing between non-isomorphic triplets in KGs.
\end{restatable}

We remark here that the additional expressive power comes from including which entities share two relations, at the cost of an additional dimension in the relation adjacency matrix. \revision{Prior relation graphs~\citep{galkin2023towards, lee2023ingram, gao2023double} consider all entities as isomorphic: the feature describing the entities shared by two relations is simply how many entities they have in common. 
% Unfortunately, that imposes a great loss in expressiveness since 
It is not uncommon to have many pairs of relations with similar entity counts when these entities are non-isomorphic. Then the relations incorrectly tend to get similar or even same embedding. \ourmethod distinguish these relations by recording entities in the additional dimension in the relation graph. Through the proposed iterative updates, non-isomorphic entities get diverse embeddings and non-isomorphic relations also get diverse embeddings, thus the expressive power gets boosted.} In the following, we discuss the complexity of \ourmethod.

% few examples about alpha
\textbf{Time Complexity.} Recall that the entity 
adjacency matrix $\mA_V \in \mathbb{R}^{|V| \times |V| \times |R|}$ has $\vert V \vert$ entities and $\vert R \vert$ relations. Denote by $\alpha$ the maximum number of unique relations one single entity connects to as the head or the tail in $\mA_V$. Then, the relation adjacency matrix $\mA_R \in \mathbb{R}^{|R| \times |R| \times |V| \times 4}$ has $\vert R \vert$ entities, $\vert V \vert$ relations and at most $4|V|\alpha^2$ edges. \revision{Assuming there are $L$ rounds of iterative updates} and embedding dimension is $d$, which we consider constant, the time complexity of \ourmethod is $O(|E|+|V|\revision{\alpha}^2)$ while the time complexity of ULTRA is $O(|E|+|V|+|R|^2)$ for each forward pass, which results in $O(|E|+|V|+|R|^2)$ for entity prediction and in $O((|E|+|V|+|R|^2)|R|)$ for relation prediction, as it needs to perform one forward pass for each relation in the relation set. 
We expand on this in \Cref{app:expressive-power}, where we show that, in practice, the number of edges in the relation graph is much smaller than $4|V|\alpha^2$, which therefore results in a complexity $\sim 10\times$ worse than ULTRA in entity prediction, and $\sim 20\times$ better than ULTRA in relation prediction.
