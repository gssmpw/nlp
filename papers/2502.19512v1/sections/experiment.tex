\label{experiment_section}
We perform a comprehensive set of experiments to answer the following questions:
\begin{enumerate*}[label=(\arabic*), leftmargin=*]
    \item How does \ourmethod compare to state-of-the-art \revision{fully inductive models} in inductive entity and relation prediction tasks and, in particular, does the increased expressiveness translate into better performance?
    \item Can the accuracy of \ourmethod increase with more data in the pre-training?
    \item \revisionlog{Can LLMs make inductive inferences based on the structural information in KGs? To be more specific,} are LLMs equivariant to permutations of relation and entity ids? And, can they do inductive tasks where the relation semantic is hidden and the model needs to leverage the \revisionlog{structural information in the input}?
\end{enumerate*}
\revision{In the following, we report our main results and refer to \Cref{appx:detailedresults} for additional experiments, including an ablation study on the importance of the proposed relation adjacency matrix and the iterative updates.}

\textbf{\ourmethod Implementation Details.}
\revision{We train one \ourmethod model for entity prediction and another \ourmethod model for relation prediction. For relation prediction,} \ourmethod updates relation and entity embedding for 3 rounds, while for entity prediction, for $5$ rounds. \revision{$\text{GNNLayer}_R$ and $\text{GNNLayer}_V$ in \Cref{ee,er,rr,re} are NBFNet layers with hidden dimension 32}. In pre-training, the model is trained for 10 epochs with 10000 steps per epoch with early stopping. In fine-tuning, the model is trained for 3 epochs with 1000 steps per epoch with early stopping. We use a batch size of 32, AdamW optimizer and learning rate of 0.0005. 

\subsection{Zero-Shot Inference and Fine-Tuning of \revision{Fully Inductive Models}}

\textbf{Datasets \& Evaluation.} We undertake a comprehensive evaluation across 57 distinct KGs coming from different domains, and split them into three categories: inductive entity and relation ($e$, $r$) datasets, inductive entity ($e$) datasets, and transductive datasets (\Cref{appx:datasets}). We follow the procedure prescribed by ULTRA~\citep{galkin2023towards} and pretrain on 3 datasets (WN18RR,
CoDEx-Medium, FB15k237). \revisionlog{We choose ULTRA as our baseline since it is the state-of-the-art double equivariant model that is capable of doing zero-shot fully inductive inference on the large KGs of our interests.} For entity prediction task, we report Mean Reciprocal Rank (MRR) and Hits@10 as the main performance metrics evaluated against the full entity set of the inference graph. For each triplet, we report the results of predicting both head and tail. Only in three datasets from \citet{lv2020dynamic} we report tail-only metrics, as in the baselines. For relation prediction, we report MRR and Hits@1 as the main performance metrics evaluated against the full relation set of the inference graph. We choose Hits@1 instead of Hits@10 because for some datasets the number of relations is less than 10. 

% \revisionlog{
% \textbf{Baselines.} We choose ULTRA as our baseline since it is the state-of-the-art double equivariant model that is capable of doing zero-shot fully inductive inference on large KGs of our interests. }

% InGram and ISDEA+ are also capable of doing inductive inferences with unseen entities and unseen relations. However, InGram finds the best embedding initialization on the test graph, which violates the definition of zero-shot inference so it is not suitable for our zero-shot experiments. We cannot include ISDEA+ as a baseline as it only measures Hits@10 computed with 50 negative samples instead of considering all possible negatives and it is not scalable to large KGs as it needs to compare shortest distances between entity pairs.


% We consider the following baselines: 
% \begin{enumerate*}[label=(\roman*), leftmargin=*]
%     \item the ULTRA (a SOTA fully inductive model); and
%     \item SOTA long-context LLMs with public APIs (Gemini-1.5-flash and Gemini-1.5-Pro \citep{geminiteam2024gemini}). 
% \end{enumerate*}

\textbf{Entity Prediction Results.}
We present the MRR and Hits@10 results across 57 KGs, along with the corresponding baseline (ULTRA) outcomes in \Cref{entity} \revision{and \Cref{entity_mrr_figure}}. Detailed per-dataset results, including standard deviations, are provided in \Cref{tab:app_zero_shot_ent,tab:app_fine_tune_ent}. In short, \ourmethod outperforms ULTRA by a significant margin ($\sim3\%$ average absolute improvement) in zero-shot scenarios, while demonstrating $\sim0.7\%$ average absolute improvement after fine-tuning in inference tasks. 
%
This demonstrates the importance of the additional expressiveness, which results in better performance. 

%\revision{For ablation study, we compare the performance of (1) \ourmethod, (2) \ourmethod without iterative updates, and (3) \ourmethod without iterative updates and without our relation graph (using the relation graph of ULTRA). Ablation study shows both the proposed relation graph and the iterative update scheme significantly enhance the prediction performance. Please refer to \Cref{appx:detailedresults} for details.}
\revision{
\begin{figure}[t]
% \vspace{-10pt}
\centering
\includegraphics[width=1.0\textwidth]{figures/trix.pdf}
\caption{ Zero-shot MRR (higher is better) in the entity prediction task. \ourmethod outperforms ULTRA on 34 datasets, while being comparable on 14 datasets and being outperformed on 6 datasets. It even outperforms supervised baselines on 30 out of 40 datasets.}
\label{entity_mrr_figure}
\vspace{-18pt}
\end{figure}
}

\begin{table*}[t]
    \centering
    \caption{Average entity prediction MRR and Hits@10 over 57 KGs from distinct domains.  The results over each of the 57 KGs are given in \Cref{appx:detailedresults}.}
    \label{entity}
\begin{adjustbox}{width=\textwidth}
    \begin{tabular}{lcccccccc||ccc}\toprule
    \multirow{3}{*}{\bf{Model}} & \multicolumn{2}{c}{\bf{Inductive} $e, r$} & \multicolumn{2}{c}{\bf{Inductive} $e$} & \multicolumn{2}{c}{\bf{Transductive}} & \multicolumn{2}{c}{\bf{Total Avg}} & \multicolumn{2}{c}{\bf{Pretraining}} \\  
 & \multicolumn{2}{c}{(23 graphs)} & \multicolumn{2}{c}{(18 graphs)} & \multicolumn{2}{c}{(13 graphs)} & \multicolumn{2}{c}{(54 graphs)} & \multicolumn{2}{c}{(3 graphs)} \\ \cmidrule(l){2-3} \cmidrule(l){4-5} \cmidrule(l){6-7} \cmidrule(l){8-9} \cmidrule(l){10-11}
 & \bf{MRR} & \bf{H@10} & \bf{MRR} & \bf{H@10} & \bf{MRR} & \bf{H@10} & \bf{MRR} & \bf{H@10} & \multicolumn{1}{c}{\bf{MRR}} & \bf{H@10} \\ 
    \midrule
    ULTRA zero-shot &0.345 &0.513 &0.431 &0.566 &0.312 &0.458 &0.366 &0.518 & N/A & N/A \\
    % \revisionlog{InGram zero-shot} & & & & & & & & & N/A & N/A \\
    \ourmethod zero-shot & \textbf{0.368} & \textbf{0.540} & \textbf{0.455} & \textbf{0.592} & \textbf{0.339} & \textbf{0.500} & \textbf{0.390} & \textbf{0.548} & N/A & N/A \\ \midrule
    ULTRA fine-tuned & 0.397 & \textbf{0.556} & 0.442 & 0.582 &0.379 & 0.543 & 0.408 & 0.562 & 0.407 & \textbf{0.568} \\
    % \revisionlog{InGram fine-tuned} & & & & & & & & & & \\
    \ourmethod fine-tuned & \textbf{0.401} & \textbf{0.556} & \textbf{0.459} & \textbf{0.594} & \textbf{0.390} & \textbf{0.558} & \textbf{0.418} & \textbf{0.569} & \textbf{0.415} & 0.563 \\
    \bottomrule
    \end{tabular}
\end{adjustbox}
\end{table*}

\textbf{Relation Prediction Results of Graph Models.}
We present the average MRR and Hits@1 results across 57 KGs, alongside the results for ULTRA in \Cref{relation}  (Hits@10 is not chosen because some domains have too few relations). Detailed per-dataset results, including standard deviations, are available in  \Cref{tab:app_zero_shot_rel,tab:app_fine_tune_rel}. \ourmethod outperforms ULTRA significantly in zero-shot inference scenarios, exhibiting substantial advantages, with an average absolute improvement of 7.4\% in Hits@1. Moreover, after fine-tuning, \ourmethod shows an average absolute improvement of 4.7\% in Hits@1 in inference tasks. Notably, the zero-shot inference results obtained by \ourmethod even surpass those of ULTRA after finetuning. This underscores the effectiveness of \ourmethod in relation predictions.

% \vspace{-1.0em}
\begin{table*}[t]
    \centering
    \caption{Average relation prediction MRR and hits@1 over 57 KGs from distinct domains.  The results over each of the 57 KGs are given in \Cref{appx:detailedresults}.}
    \label{relation}
\begin{adjustbox}{width=\textwidth}
    \begin{tabular}{lcccccccc||ccc}\toprule
    \multirow{3}{*}{\bf{Model}} & \multicolumn{2}{c}{\bf{Inductive} $e, r$} & \multicolumn{2}{c}{\bf{Inductive} $e$} & \multicolumn{2}{c}{\bf{Transductive}} & \multicolumn{2}{c}{\bf{Total Avg}} & \multicolumn{2}{c}{\bf{Pretraining}} \\  
 & \multicolumn{2}{c}{(23 graphs)} & \multicolumn{2}{c}{(18 graphs)} & \multicolumn{2}{c}{(13 graphs)} & \multicolumn{2}{c}{(54 graphs)} & \multicolumn{2}{c}{(3 graphs)} \\ \cmidrule(l){2-3} \cmidrule(l){4-5} \cmidrule(l){6-7} \cmidrule(l){8-9} \cmidrule(l){10-11}
 & \bf{MRR} & \bf{H@1} & \bf{MRR} & \bf{H@1} & \bf{MRR} & \bf{H@1} & \bf{MRR} & \bf{H@1} & \multicolumn{1}{c}{\bf{MRR}} & \bf{H@1} \\ 
    \midrule
    ULTRA zero-shot & 0.785 & 0.691 & 0.714 & 0.590 & 0.629 & 0.507 & 0.724 & 0.613 & N/A & N/A \\
    \ourmethod zero-shot & \textbf{0.842} & \textbf{0.770} & \textbf{0.756} & \textbf{0.611} & \textbf{0.752} & \textbf{0.647} & \textbf{0.792} & \textbf{0.687} & N/A & N/A \\ \midrule
    ULTRA fine-tuned  & 0.823 & 0.741 & 0.716 & 0.591 & 0.707 & 0.608 & 0.759 & 0.659 & 0.876 & \textbf{0.817} \\
    \ourmethod fine-tuned  & \textbf{0.850} & \textbf{0.785} & \textbf{0.759} & \textbf{0.615} & \textbf{0.785} & \textbf{0.693} & \textbf{0.804} & \textbf{0.706} & \textbf{0.879} & 0.797 \\
    \bottomrule
    \end{tabular}
\end{adjustbox}
\end{table*}
%\vspace{-1em}

\begin{wrapfigure}[13]{}{0.6\textwidth}
\vspace{-3pt}
    \centering
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=3.8cm]{figures/cos1-2.pdf}
        \caption{Single training domain.}
        \label{fig:heatmap1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=3.8cm]{figures/cos4-2.pdf}
        \caption{Four training domains.}
        \label{fig:heatmap2}
    \end{subfigure}
    \caption{Heatmaps of cosine similarities of relation embeddings in test with varying number of training domains. More domains shows stronger embedding differentiation.}
    \label{fig:heatmaps}
    \vspace{-15pt}
\end{wrapfigure}
%
\textbf{Testing Zero-shot Meta-Learning Ability of \ourmethod.}
In this experiment, we investigate the meta-learning capability of \ourmethod. 
That is, following \citet[pp.\ 4]{thrun1998learning}, we can define that a \revision{fully inductive model} is able to learn-to-learn when increasing the number of learning domains improves the inference performance on new domains. Here, we aim to understand the impact of including more domains in the zero-shot performance of \ourmethod. We use the WikiTopics dataset \cite{gao2023double}, which divides the Wikidata-5M~\cite{wang2021kepler} into 11 distinct, non-overlapping domains, resulting in 11 unique KGs, each containing different and non overlapping relation and entity sets. We then proceed by training \ourmethod on a randomly sampled increasing number of domains (until early-stopped) and evaluate their performance on the remaining domains. \Cref{entity_meta} show that as the number of domains in the training increases, the zero-shot inference capability of \ourmethod improves. Additionally, \Cref{fig:heatmap1,fig:heatmap2} illustrate the cosine similarities between the relation embeddings after training, averaged across different query relations, when training with one or four domains, respectively. By adding domains in the training mixture, the model can see more invariances and the relation embeddings get more distinct and expressive, which then translates into more accurate prediction in unseen domains.

% \begin{wrapfigure}[15]{}{0.6\textwidth}
% \vspace{-10pt}
%     \centering
%     \begin{subfigure}[t]{0.48\linewidth}
%         \centering
%         \includegraphics[width=3.8cm]{figures/cos1-2.pdf}
%         \caption{Single training domain.}
%         \label{fig:heatmap1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.48\linewidth}
%         \centering
%         \includegraphics[width=3.8cm]{figures/cos4-2.pdf}
%         \caption{Four training domains.}
%         \label{fig:heatmap2}
%     \end{subfigure}
%     \caption{Heatmaps of cosine similarities of relation embeddings in test with varying number of training domains. Increasing the number of domains shows stronger embedding differentiation.}
%     \label{fig:heatmaps}
%     \vspace{-15pt}
% \end{wrapfigure}

\begin{table*}[t]
\vspace{-5pt}
\caption{\ourmethod Entity Prediction Hits@10 on WiKiTopics with varying pre-training domains.}
\centering
\label{entity_meta}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{l|c|cccccccc}
\toprule
\# pre-train domains & Average & Art & Award & Edu & Health & Infra & Sci & Sport & Tax \\
\midrule
1 domain & 0.413 & 0.380 & 0.428 & 0.263 & 0.601 & 0.556 & 0.369 & \textbf{0.385} & 0.324 \\
2 domains & 0.459 & \textbf{0.439} & 0.462 & 0.282 & 0.700 & 0.656 & 0.423 & 0.366 & 0.358 \\
3 domains & 0.458 & 0.432 & \textbf{0.472} & \textbf{0.300} & 0.623 & 0.679 & 0.431 & 0.376 & 0.361 \\
4 domains & \textbf{0.483} & 0.432 & 0.464 & 0.294 & \textbf{0.744} & \textbf{0.724} & \textbf{0.448} & 0.382 & \textbf{0.396} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-10pt}
\end{table*}

% \begin{figure}[t]
%     \begin{minipage}[t]{0.6\linewidth}
%         \centering
%         \begin{subfigure}[t]{0.45\linewidth}
%         \centering
%         \includegraphics[width=4cm]{figures/cos1-2.png}
%         \caption{Single training domain.}
%         \label{fig:heatmap1}
%         \end{subfigure}
%         \hfill
%         \begin{subfigure}[t]{0.45\linewidth}
%             \centering
%             \includegraphics[width=4cm]{figures/cos4-2.png}
%             \caption{Four training domains.}
%             \label{fig:heatmap2}
%         \end{subfigure}
%         \caption{Cosine similarities of relation embeddings in test with varying number of training domains. Increasing the number of domains shows stronger test embedding differentiation.}
%         \label{fig:heatmaps}
%     \end{minipage}
%     \begin{minipage}[t]{0.35\linewidth}
%         \centering
%         \includegraphics[width=0.8\linewidth]{figures/barplot.pdf}
%         \caption{Relation prediction Hits@1 of 
%         %SOTA long-context LLM (
%         Gemini-1.5-Pro on three tasks against \revision{fully inductive models}. Double-equivariant graph models always give consistent performance but LLM performance changes drastically with metasyntactic tokens (Task 2) and ID permutations (Task 3).}
%         % \vspace{-22pt}
%         \label{fig:llmplot}
%     \end{minipage}
% \end{figure}

% \begin{wrapfigure}{r}{0.5\textwidth}
%     \centering
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=4cm]{figures/cos1-2.pdf}
%         \caption{Single training domain.}
%         \label{fig:heatmap1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=4cm]{figures/cos4-2.pdf}
%         \caption{Four training domains.}
%         \label{fig:heatmap2}
%     \end{subfigure}
%     \caption{Cosine similarities of relation embeddings in test with varying number of training domains. Increasing the number of domains shows stronger test embedding differentiation.}
%     \label{fig:heatmaps}
%     \vspace{-15pt}
% \end{wrapfigure}

\subsection{Relation and Entity Prediction with LLMs}
To evaluate whether long-context LLMs can make zero-shot relation \revision{and entity} predictions based on the structural patterns of the graph, we design three different tasks
\revision{containing both relation prediction and entity prediction.} In these tasks, all triplets (head entity, relation, tail entity) of the KG are provided in the prompt. The tasks differ in how entities and relations are represented. \revisionlog{The goal of these experiments is to underscore the shortcomings of LLMs in capturing the structural patterns, which is due to being sensitive to permutations of IDs and relying on semantic information.}


\revisionlog{\textbf{Experiment Setup.} Since all the triplets of KGs are provided in the prompt (approximately 700,000 tokens per query), we selected the Gemini Pro models (Gemini-1.5-flash and Gemini-1.5-pro~\citep{geminiteam2024gemini}) as the baseline because they offer the largest token size (1,048,576 tokens for Gemini-1.5-flash), whereas other models, such as GPT-4 (32,000 tokens) and Llama 3.1-70B Instruct (128,000 tokens), have limited token capacities. Due to the cost of the Gemini Pro model, we evaluated on 30 samples from the CoDEx-S dataset~\cite{safavi2020codex}. The prompts used followed previous works~\cite{yao2023exploring, shu2024knowledge}. We report in the following relation prediction results, while entity prediction results and further experimental details, including prompts, can be found in \Cref{appx:LLMexp}.}

\begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-2em}
\centering
\includegraphics[width=0.5\textwidth]{figures/barplot.pdf}
\caption{Relation prediction Hits@1 of 
%SOTA long-context LLM (
Gemini-1.5-Pro on three tasks against \revision{fully inductive models}. Double-equivariant graph models always give consistent performance but LLM performance changes drastically with metasyntactic tokens (Task 2) and ID permutations (Task 3).}
\vspace{-22pt}
\label{fig:llmplot}
\end{wrapfigure}
%
%
% \begin{enumerate}[leftmargin=0.25in] 
%     \item[T1.] Task 1 is an in-domain prediction task. The entities and relations are expressed by \revision{their surface names} in natural language, which means that the long-context LLM is queried in the way it was trained on. This task evaluates the in-domain prediction capacity of long-context LLMs.
%     \item[T2.] Task 2 is an out-of-domain prediction task. The neighbor entities of the head entity in the query and the relations that connect the head entity in the query with its neighbors are replaced with metasyntactic words~\cite{metasyntactic} like \texttt{foo}, \texttt{bar} and \texttt{baz} across the whole KG to simulate information from a new domain (with new entities and relations) \revision{while other entities and relations are still expressed by their surface names in natural language}. This task tests whether long-context LLMs reason on the KG's relational information for the inductive predictions of the metasyntactic words, or if they just simply rely on its pre-trained semantic information.
%     \item[T3.] Task 3 is a double-equivariant predictiontest. All the entities and relations are expressed with IDs (e.g. ``entity 64'', ``relation 16'', ``relation 22'') as in \citet{shu2024knowledge}. The mappings from entities and relations to their IDs are given to the LLM and the IDs are shuffled in each test run. This task tests whether long-context LLMs are (double) equivariant to entity and relation ID permutations.
% \end{enumerate}

\textbf{Task 1: In-domain LLM predictions.}
The entities and relations are expressed by \revision{their names} in natural language, which means that the LLM is queried in the way it was trained on. This task evaluates the in-domain prediction capacity of long-context LLMs. \Cref{fig:llmplot} (and \Cref{appx:T1}) shows that the LLM handles the in-domain relation prediction nearly as well as \ourmethod and better than ULTRA. 

% \vspace{-2pt}
\textbf{Task 2: Out-of-domain LLM predictions.}
The neighbor entities of the head entity and the relations that connect the head entity with its neighbors are replaced with metasyntactic words~\cite{metasyntactic} like \texttt{foo}, \texttt{bar} and \texttt{baz} across the whole KG to simulate information from a new domain (with new entities and relations) \revision{while other entities and relations are still expressed by their names in natural language}. This task tests whether long-context LLMs reason on the KG's structural information for the inductive predictions of the metasyntactic words, or if they just simply rely on its pre-trained semantic information. \Cref{fig:llmplot} (details in \Cref{appx:T2}) shows that with metasyntactic words, Gemini-1.5-pro fails to make consistent accurate predictions. Hence, the good performance in Task 1 likely came from its in-distribution syntax, which means that even SOTA long-context LLMs struggle making relation predictions in new KGs domains using only relational information.

% \vspace{-2pt}
\textbf{Task 3: Double-equivariant LLM predictions.}
All the entities and relations are expressed with IDs (e.g. ``entity 64'', ``relation 22'') as in \citet{shu2024knowledge}. The mappings from entities and relations to their IDs are given to the LLM and the IDs are shuffled in each test run. This task tests whether long-context LLMs are (double) equivariant to entity and relation ID permutations.
\Cref{fig:llmplot} (details in \Cref{appx:T3}) shows that the performance of a long-context LLM changes drastically with the permutation of relation and entity IDs. Only for $38.5\%$ of all queries the model makes consistent predictions across 3 runs, which indicates LLM is quite sensitive to the input permutations of the KG. With the whole graph sent as an edge list, the task resembles long text understanding and retrieval tests akin to ``a needle in a haystack''~\cite{babilong,zeroscrolls,longbench} where LLMs show increasingly better performance on context lengths up to 128k tokens. However, our results indicate that such (often synthetic) benchmarks might be overestimating real long-context reasoning capabilities of LLMs (let alone adding a simple relation prediction task on top of the input context).

