% Problem: \revision{fully inductive models} on KGC with new entity and relations
\revision{
Fully inductive knowledge graph models can perform zero-shot Knowledge Graph Completion (KGC), which predicts missing facts in entirely new domains that were not part of the training data. This is particularly challenging because these new domains may contain just unseen relation types and new entities~\citep{galkin2023towards,gao2023double,lee2023ingram}.
% Fully inductive knowledge graphs modelsperforming zero-shot Knowledge Graph Completion (KGC) (i.e., predict missing (head entity, relation, tail entity) triplets) in new domains not present in the training data, which may contain new types of relations and new entities~\citep{galkin2023towards,gao2023double,lee2023ingram}. 
} %
Fully inductive models are trained on one or multiple Knowledge Graphs (\revision{KG}) with a specific set of relations. Previous work has emphasized the importance of double-equivariance~\cite{gao2023double}, that is, equivariance to permutations of both entity ids and relation ids, as a fundamental property that \revision{fully inductive models} must have to transfer across KG domains. Intuitively, this equivariance allows \revision{models} to focus on the underlying structural invariances, despite semantic differences and variations in identifiers across different KGs.

%% What other methods do and what is missing
However, despite the notable achievements of current \revision{fully inductive models}, several open challenges remain, including:
\begin{enumerate*}[label=(\arabic*)]
    \item Limited expressivity of existing methods;
    \item Insufficient support for relation prediction tasks; and
    \item Underexploration of the abilities of Large Language Models (LLMs) to perform the same tasks.
\end{enumerate*}
More precisely, existing state-of-the-art \revision{fully inductive models}, such as ULTRA~\cite{galkin2023towards}, have expressivity limitations, as we show in \Cref{sec:expressivity}, which implies that certain non-isomorphic triplets inevitably get the same representations, and therefore necessarily the same predictions despite their differences. Since increased expressive power tend to translate into better downstream performances in traditional graph tasks~\citep{bouritsas2020improving,bevilacqua2022equivariant,zhang2023rethinking,puny2023equivariant}, an open question is whether improving the expressivity of \revision{fully inductive models} would also improve their empirical performance. Additionally, existing \revision{fully inductive models} are primarily designed for entity prediction tasks, answering queries such as (?, relation, tail entity) or (head entity, relation, ?), where the goal is to predict the head or tail entity of a given triplet. Consequently, they miss the equally important relation prediction tasks, namely queries such as (head entity, ?, tail entity), where the goal is to predict the missing relation between two entities~\citep{teru2020inductive,cui2021type,liang2024mines,su2024anchoring}. 
Finally, while large-context LLMs have demonstrated notable performance in KGC tasks~\citep{zhang2023making,chen2023dipping,shu2024knowledge,xu2024multi,wei2024kicgpt}, their effectiveness in the inductive setting of our interest, where test KGs come from new domains, remains largely underexplored. Therefore, an evaluation and comparison with \revision{fully inductive graph models} is needed to understand whether \revision{fully inductive graph models} are necessary or if repurposing LLMs would suffice.


%change this picture
% latex
% draw io
\begin{figure}[t]
\centering
\includegraphics[width=1.0\textwidth]{figures/Framework_6.pdf}
\caption{Overview of \ourmethod showcasing how a KG in a given domain is represented in \ourmethod's double-equivariant architecture through entity embedding and relation embedding iterative updates.}
\vspace{-10pt}
\label{fig:ourmethod}
\end{figure}

%% what we do
\textbf{Our approach.} In this paper, we aim to address the open challenges in \revision{fully inductive models}. We first show that the limited expressive power of the state-of-the-art \revision{fully inductive models} ULTRA~\citep{galkin2023towards} arises from its approach in capturing relation interactions, obtained by counting the number of entities sharing a pair of relations, rather than \emph{which} entities share those relations. Then, we propose \ourmethod (\underline{T}ransferable \underline{R}elation-Entity \underline{I}nteractions in crossing patterns (\underline{X}-patterns)), which we show to return \emph{strictly more expressive} triplet representations than existing methods.  As illustrated in \Cref{fig:ourmethod}, given any input KG, \ourmethod first constructs a graph of relations, where each node is a relation from the original graph and edges denote shared entities among those relations. Then, it refines relation and entity representations by iteratively applying graph neural network layers over the graph of relations and the original graph. In this way, \ourmethod obtains representations of relation and entities directly applicable to zero-shot tasks. 

We demonstrate that, by design, \ourmethod efficiently handles relation prediction tasks, a capability lacking in existing state-of-the-art \revision{fully inductive models}. In particular, \ourmethod can answer relation prediction queries in a single forward pass, while existing \revision{fully inductive models} require performing a number of forward passes equal to the number of relations, for a single relation prediction query. 

Finally, we explore the capabilities of LLMs for KGC by designing a comprehensive set of experiments. We demonstrate that, while LLMs can do KGC accurately given enough context about the background knowledge, they rely on the textual information (and its semantics), and therefore fail to utilize the actual graph information, given in the context. We show that this result has several implications, including failure cases when the relation names are not given due to privacy concerns, or simply when they are not known by the LLMs. 


\textbf{Contributions.} Our key contributions are as follows:
\begin{enumerate*}[label=(\arabic*), leftmargin=*]
\item We propose a novel \revision{fully inductive model on KGs}, \ourmethod, which exhibits greater expressive power compared to prior methods and can handle relation predictions efficiently; 
\item We show that the increased expressiveness of \ourmethod allows it to surpass state-of-the-art methods in 57 KG datasets in both entity and relation predictions;
\item We present an experimental study of LLMs on the same tasks and show that existing LLMs have limited capabilities in exploiting graph information, which are needed to perform tasks on new domains.
\end{enumerate*}