\textbf{Fully Inductive Models over KGs.}
Original efforts in \revision{inductive} Knowledge Graph Completion primarily focused on handling new entities at test time, \revisionlog{but not new relations}~\cite{schlichtkrull2018modeling,teru2020inductive,zhu2021neural,yang2017differentiable, sadeghian2019drum,qu2020rnnlogic, cheng2022rlogic, shengyuan2024differentiable, liu2021indigo, tena2022explainable}. However, emerging methodologies tackle inductive learning scenarios with both new entities and new relations in test~\cite{gao2023double,lee2023ingram,galkin2023towards}.
%
\citet{gao2023double} approaches the problem by treating relations as set elements and employing DSS layers~\citep{maron2020learning} to learn representations equivariant to permutations of both node and relation ids. %They utilize equivariant layers composed by two graph neural network layers to perform message passing on edges of the query relation and edges of all other relations, respectively.
On the other hand, \citet{lee2023ingram} and \citet{galkin2023towards} introduce relation graphs to capture relation representations based on their interactions. In this work, we extend the latter approach by proposing a novel design for relation graphs, which allows to capture more expressive structural patterns in KGs.


\textbf{Knowledge Graph Completion with Large Language Models.} Despite the impressive capabilities of pretrained LLMs, their application to KGC has primarily focused on leveraging textual information rather than exploiting the underlying graph structure~\cite{chen2023dipping}. Recent efforts aims to enhance LLM performance in KGC by incorporating neighborhood or path information between entities as part of prompts in an in-context learning approach~\cite{wei2024kicgpt, xu2024multi, shu2024knowledge}. However, these approaches have mainly been applied in transductive settings.
%
\citet{li2024condensed} investigated zero-shot link prediction tasks with LLMs but only considered scenarios involving new relations, not new entities. The effectiveness of LLMs in KGC tasks with both new entities and new relations remains unexplored.
%
Furthermore, the extent to which LLMs exhibit double-equivariance, a crucial property for inductive reasoning on KGs \cite{gao2023double}, has yet to be thoroughly investigated. In this work, we address these gaps by conducting comprehensive experiments to evaluate the double-equivariance property of LLMs in KGC tasks.

% \revisionlog{\textbf{Relation Classification.}}

