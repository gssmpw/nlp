\pdfoutput=1
\documentclass[conference]{IEEEtran}
% \documentclass[final]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[noadjust]{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{adjustbox}
\usepackage{svg}
\usepackage{tabularray}
\usepackage{xcolor}
\usepackage{pdfpages}
\usepackage{url}
\usepackage{afterpage}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\customfont}{\fontsize{6.5}{7.8}\selectfont}
\usepackage{hyperref}
\begin{document}

\title{From Occupations to Tasks: A New Perspective on Automatability Prediction Using BERT\\
}

% \author{\IEEEauthorblockN{Anonymous Author}
% }
\author{\IEEEauthorblockN{Dawei Xu}
\IEEEauthorblockA{\textit{University of Technology Sydney} \\
Sydney, Australia \\
dawei.xu@student.uts.edu.au}
\and
\IEEEauthorblockN{Haoran Yang}
\IEEEauthorblockA{\textit{University of Technology Sydney} \\
Sydney, Australia \\
haoran.yang-2@student.uts.edu.au}
\and
\IEEEauthorblockN{Marian-Andrei Rizoiu}
\IEEEauthorblockA{\textit{University of Technology Sydney} \\
Sydney, Australia \\
marian-andrei.rizoiu@uts.edu.au}
\and
\IEEEauthorblockN{Guandong Xu}
\IEEEauthorblockA{\textit{University of Technology Sydney} \\
Sydney, Australia \\
guandong.xu@uts.edu.au}
}

\maketitle

\begin{abstract}
As automation technologies continue to advance at an unprecedented rate, concerns about job displacement and the future of work have become increasingly prevalent. While existing research has primarily focused on the potential impact of automation at the occupation level, there has been a lack of investigation into the automatability of individual tasks. This paper addresses this gap by proposing a BERT-based classifier to predict the automatability of tasks in the forthcoming decade at a granular level leveraging the context and semantics information of tasks. We leverage three public datasets: O*NET Task Statements, ESCO Skills, and Australian Labour Market Insights Tasks, and perform expert annotation. Our BERT-based classifier, fine-tuned on our task statement data, demonstrates superior performance over traditional machine learning models, neural network architectures, and other transformer models. Our findings also indicate that approximately 25.1\% of occupations within the O*NET database are at substantial risk of automation, with a diverse spectrum of automation vulnerability across sectors. This research provides a robust tool for assessing the future impact of automation on the labor market, offering valuable insights for policymakers, workers, and industry leaders in the face of rapid technological advancement.
\end{abstract}

\begin{IEEEkeywords}
Task automatability prediction, Automated Occupation identification, BERT, Transfer learning
\end{IEEEkeywords}

\section{Introduction}
In the era of rapid technological advancement, the fear of large scale unemployment caused by automation technologies has become a significant concern. This concern is not unfounded; a growing body of research suggests that many occupations could be partially or fully automated in the coming years. According to the research conducted by \cite{freyosborne}, over 47\% of current US employment is at high risk of being automated. Another similar study has been performed using job-level data and concluded a less alarming rate of 9\% of employment is at risk \cite{arntz}. However, these studies have largely focused on the potential for automation at the occupation level, leaving a significant gap in our understanding of how individual tasks within these occupations might be automated and multiple studies have shown the effectiveness of assessing the automatability on tasks level instead of occupations \cite{towards, webb_2019_the}. \par
The automatability of tasks is fundamentally a question of understanding the nature and requirements of the task and this information is often embedded in the contextual and semantic information of textual task statements \cite{towards}. For example, the task "Write a report" might be automatable, but "Write a creative story" might not be. And the word ``run" means differently in "run a program" and "run a marathon" \cite{arntz}. Therefore, our research is based on the assumption that the automatability of tasks can be apprehended through the extraction of contextual and semantic information embedded within textual task statements.\par
Due to this circumstance, our research employ the Bidirectional Encoder Representations from Transformers (BERT) model \cite{devlin2018bert} to understand the context and semantics of words in a task statement allowing for a more sophisticated analysis of task descriptions, potentially leading to more accurate predictions of task automatability. We also utilize three task description datasets: O*NET Task statements \cite{onet}, ESCO Skills \cite{esco}, and Australian Labour Market Insights Tasks \cite{australian_labor_outlook}. We annotate each task with one of three labels: ``Substitution," ``Complementarity," or ``Negligibility," representing its automation susceptibility based on expert assessments. Using these annotations as ground truth, we apply a BERT-based classifier model \cite{devlin2018bert} to predict task automatability, benefiting from BERT's context understanding and semantic capabilities.\par

We extend our analysis to occupation and industry levels, using aggregated task-level predictions to assess overall automation susceptibility. This comprehensive view can reveal trends not observable at the task level, providing additional automation impact insights. Our research holds significant implications for workers, businesses, and policymakers by offering clarity on automation-susceptible tasks and demonstrating BERT's utility in labor economics.\par
% This paper aims to address this gap by investigating the automatability of tasks rather than entire occupations in the forthcoming decade. To achieve this, we utilize three comprehensive datasets: O*NET Task statements \cite{onet}, ESCO Skills \cite{esco}, and Australian Labour Market Insights
% Tasks \cite{australian_labor_outlook} which comprising detailed task descriptions across a wide range of occupations. Each task in the datasets has been anonotated to one of the three classes: ``Substitution", ``Complementarity", or ``Negligibility" indicating its susceptibility to automation based on expert assessments. These annotations serve as the ground truth for training and evaluating our proposed model, enabling us to assess its performance in predicting task automatability. We employ the Bidirectional Encoder Representations from Transformers (BERT) model \cite{devlin2018bert}, a cutting-edge machine learning technique that offers several advantages over traditional methods. BERT's ability to understand the context and semantics of words in a sentence allows for a more sophisticated analysis of task descriptions, potentially leading to more accurate predictions of task automatability.\par
% Finally, we extend our analysis to the occupation and industry levels. We aggregate our task-level predictions to assess the overall susceptibility of different occupations and industries to automation. This broader perspective allows us to identify patterns and trends that may not be apparent at the task level, providing additional insights into the potential impact of automation on the workforce.\par
% The implications of this research are significant. By providing a clearer picture of which tasks are most susceptible to automation, we can help workers, businesses, and policymakers prepare more effectively for the future of work. Furthermore, by demonstrating the utility of BERT in this context, we hope to encourage further application of advanced machine learning techniques in the field of labor economics.\par
The contributions of our research to the domain of automation impact on the labor market can be summarized as follows:\par
\begin{itemize}
\item Presents a novel approach to predict the automatability at task level which provides a more detailed and nuanced understanding of the impact of automation.
\item This work substantiates the hypothesis that task automatability can be discerned through mining the contextual and semantic information of textual task statements by leveraging BERT's contextual and semantic understanding capabilities.
\item Fine-tuned the BERT-based classifier on task statement data, outperforms traditional machine learning and neural network models, offering a robust tool for assessing automation's future impact on the labor market.
\item This research combine and annotates three public datasets: O*NET Task Statements, ESCO Skills, and Australian Labour Market Insights Tasks, creating a valuable resource that underscores the robustness of the BERT-based classifier across diverse data types.
\end{itemize}
% In this era of rapid technological advancements, the prospect of large-scale unemployment due to automation is concerning. Studies suggest potential for significant occupational automation, with one estimating 47\% of US jobs at high risk \cite{freyosborne} and another projecting a more conservative 9\% \cite{arntz}. However, these studies mainly focus on occupation-level automation, ignoring individual tasks within occupations. Task-level automation assessment has been shown to be effective \cite{towards, webb_2019_the}.

% Our research investigates task automatability within occupations over the next decade, utilizing three task description datasets: O*NET Task statements \cite{onet}, ESCO Skills \cite{esco}, and Australian Labour Market Insights Tasks \cite{australian_labor_outlook}. We annotate each task with one of three labels: "Substitution," "Complementarity," or "Negligibility," representing its automation susceptibility based on expert assessments. Using these annotations as ground truth, we apply the BERT model \cite{devlin2018bert} to predict task automatability, benefiting from BERT's context understanding and semantic capabilities.

% We extend our analysis to occupation and industry levels, using aggregated task-level predictions to assess overall automation susceptibility. This comprehensive view can reveal trends not observable at the task level, providing additional automation impact insights. Our research holds significant implications for workers, businesses, and policymakers by offering clarity on automation-susceptible tasks and demonstrating BERT's utility in labor economics.

% Our research seeks to answer the following research questions::

% \begin{itemize}
% \item What is the extent of task, rather than occupation, be automated?
% \item How does the BERT model improve task automatability prediction over traditional methods?
% \item Which occupations/industries are more susceptible to future automation?
% \item What are the task-level automation implications for workers and policymakers?
% \end{itemize}

\section{Related Work}
The body of literature on the automatability of occupations is vast and growing, with numerous studies focusing on predicting the susceptibility of entire occupations to automation. For instance, Frey and Osborne \cite{freyosborne} pioneered the field with their seminal work, using a Gaussian process classifier to estimate the probability of computerisation for 702 detailed occupations and concluded that about 47\% occupations in US is at high risk. Arntz \cite{arntz} further expanded on this by considering the heterogeneity of jobs within occupations and obtained a less alarming rate of 9\%, albeit their focus remained on the occupation level. These studies, while groundbreaking, have been critiqued for their broad-brush approach, which overlooks the granularity of tasks within occupations.\par

In parallel, there has been a surge of interest in the application of BERT-based models for sentence classification tasks. Devlin \cite{devlin2018bert} introduced BERT, a transformer-based model that leverages context in both directions, making it highly effective for understanding the semantics of text. Since then, BERT has been widely used in various natural language processing tasks, including sentence classification \cite{croce2020ganbert, lu2020vgcnbert}, and cross domain applications \cite{lavi2021consultantbert, xue2019finetuning} demonstrating its ability to capture contextual and semantic information effectively. However, this is limited research in automatability prediction leveraging sentences' contextual and semantic information and multiple studies have formulated hypothesis that the automatability of tasks is fundamentally a question of understanding the nature and requirements of the task which is often embedded in the contextual and semantic information of textual task statements \cite{towards, webb_2019_the}.\par

Our work bridges these two streams of literature. While we build upon the existing body of work on occupation automatability, we shift the focus from the occupation level to the task level. We argue that this provides a more nuanced and accurate picture of the potential impact of automation on the workforce. Furthermore, we leverage the power of BERT for sentence classification, applying it to the analysis of task descriptions to predict their automatability. This represents a novel application of BERT, extending its use beyond traditional natural language processing tasks. In doing so, we hope to contribute a new perspective to the ongoing discourse on the future of work in the age of automation.\par
\section{Method}
In this section, we outline the whole framework (Fig. \ref{fig: framework}) employed to predict task level automatability leveraging a BERT-based classifier. Our approach first harnesses the BERT architecture to generate task statement embeddings, then construct a BERT-based classifier with a softmax layer for categorizing automatabilities. The model undergoes fine-tuning, tailored to the downstream classification task, optimizing its performance on the specific domain.

\begin{figure}[htp!]
\centerline{\includegraphics[width=1\columnwidth]{svg-inkscape/framework_svg-raw.pdf}}
\caption{The framework of the proposed method.}
\label{fig: framework}
\end{figure}
\subsection{Dataset Creation}
We used three public datasets: ONET Task Statements, ESCO skills, and AU Labor Market Insights Task statements. They provide a wide range of occupation-specific task statements with varying geographical and vocational coverage. We randomly sampled tasks: 5,060 from ONET, 4,783 from ESCO, and 3,356 from AU Labor Market Insights. This was done to ensure diversity across occupations, sectors, and regions for generalizable findings and to optimize computational resources and model training efficiency due to the high-dimensionality of text data and the large dataset sizes.
\subsubsection{Expert Annotation}
% The selected tasks underwent an expert annotation process where each task was labeled according to one of the three classes: ``Substitution", ``Complementarity", or ``Negligibility", based on the level of automation potential. The ``Substitution" class represents tasks that can be entirely automated, ``Complementarity" implies tasks where human labor can be complemented by automation technology but still need human involvement, and ``Negligibility" refers to tasks that are unlikely to be affected by automation. These classes constitute our ground truth data, providing a categorical understanding of automation susceptibility at a granular task level.\par
% Specifically, our process is grounded in two prominent social-economic hypotheses Skill-Biased Technological Change (SBTC) \cite{card2002skill} and Routine-Biased Technological Change (RBTC) \cite{autor2003rbtc} which explain the impact of technological change on labor markets. The SBTC hypothesis postulate that technological advancements favor tasks that require high-level skills while replacing routine, lower-skilled jobs \cite{kristal2020computerization}. Conversely, the RBTC hypothesis asserts that technologies are primarily replacing jobs that involve routine tasks, irrespective of the skill level \cite{buyst2018job}.\par
% In addition to these hypotheses, we also consider the six significant bottlenecks (Fig \ref{fig: bottlenecks}) to automation identified in previous research: Complex Problem Solving, Social Interaction and Emotional Intelligence, Fine Motor Skills and Dexterity, Creative and Artistic Abilities, Contextual Understanding, and Ethical Decision Making \cite{chui2016where, nedelkoska2018automation}. These bottlenecks encapsulate the areas where humans continue to outperform machines, thus providing additional granularity to our analysis.
Our annotation process employs a voting mechanism involving five experts. For each task and each class, if a class receives more than three votes, it is designated as the final label for that task. This approach ensures a consensus among experts and enhances the reliability of the labels. Our expert annotation process labeled each task as ``Substitution" (fully automatable), ``Complementarity" (partial automation with human involvement), or ``Negligibility" (unlikely to be automated). This provides a categorical understanding of task-level automation susceptibility. We base our process on the Skill-Biased Technological Change (SBTC) \cite{card2002skill} and Routine-Biased Technological Change (RBTC) \cite{autor2003rbtc} hypotheses, which explain technological change's impact on labor markets. SBTC postulates that technology favors high-skill tasks and replaces lower-skilled routine jobs \cite{kristal2020computerization}, while RBTC asserts that technology replaces routine tasks, regardless of skill level \cite{buyst2018job}. We also incorporate six major automation bottlenecks : Complex Problem Solving, Social Interaction and Emotional Intelligence, Fine Motor Skills and Dexterity, Creative and Artistic Abilities, Contextual Understanding, and Ethical Decision Making \cite{chui2016where, nedelkoska2018automation}. These bottlenecks highlight areas where humans still outperform machines.

\subsubsection{Data Augmentation}
The initial exploration of the labeled data revealed a class imbalance issue, a common problem in many real-world classification tasks \cite{krawczyk2016learning}. This imbalance can lead to biased models, overfitting to the majority class and neglecting minority ones. Conventional data augmentation methods suitable for image data aren't applicable to text data due to its sequential and semantic nature \cite{zhang2015character}. Apart from class imbalance issue, effectively increasing the volume of training data is particularly beneficial for deep learning models like BERT that perform better with larger datasets.\par
We addressed this using GPT-4 \cite{openai2023gpt4} to paraphrase sentences, creating new task statements with identical meanings but different wording. GPT-4's ability to generate high-quality and varied paraphrases ensures the augmented data remains relevant and increases the classifier's robustness to different task expressions. Specifically, each statement was fed into GPT-4 for paraphrasing, followed by automated checks and expert reviews to validate the paraphrases for semantic similarity to the original statements:\par 
\noindent\textbf{Original Sentence:}\par
\noindent Generate reports utilizing visual aids such as charts, graphs, and narratives by examining and documenting test data.\par
\noindent\textbf{Paraphrased Sentence:}\par
\noindent \textit{Create} reports that \textit{incorporate} visual \textit{elements} like \textit{diagrams}, \textit{plots}, and descriptive narratives by \textit{scrutinizing} and \textit{recording} the results of tests.

\subsection{BERT-based Classifier}
BERT stands out with its multi-layer bidirectional Transformer encoder, which enables it to capture the deep contextual information embedded in text data \cite{devlin2018bert}. Mathematically, this is achieved by applying the attention mechanism, where the output embeddings \textit{E} for a given input token is a weighted sum of all input token embeddings:
\begin{equation}
E = \sum \text{{\textit{Attention\_Scores}}} \times \text{{\textit{Token\_Embeddings}}}
\end{equation}
For our classification task, we fine-tuned a pre-trained BERT model on our task statement data. Each task statement was fed into BERT, which transformed the text into high-dimensional embeddings.\par

The embeddings were then passed through a softmax function to generate the final class probabilities. If we denote the output of our model before the softmax layer as \(z\), the softmax function can be expressed as:
\begin{equation}
\text{{\textit{Softmax}}}(z_i) = \frac{e^{z_i}}{\sum e^{z_j}}
\end{equation}
where \(z_i\) is the output corresponding to the i-th class and the sum in the denominator runs over all possible classes.\par
The training process involved adjusting the model's parameters to minimize the discrepancy between the predicted and actual class labels, typically quantified using the cross-entropy loss function:
\begin{equation}
L = -\sum \text{{\textit{y\_true}}} \times \log(\text{{\textit{y\_pred}}})
\end{equation}
where \textit{y\_true} is the true label and \textit{y\_pred} is the predicted probability.
\subsection{Attention Mechanism}
A fundamental feature of BERT's ability to capture deep contextual information is its use of the attention mechanism, specifically, the scaled dot-product attention as introduced in the original Transformer model \cite{vaswani2017attention}. This attention mechanism allows the model to weigh the importance of different words in a sentence, providing a powerful tool for understanding language semantics.\par
Mathematically, the attention mechanism can be described as mapping a query and a set of key-value pairs to an output. Given a query (\textit{Q}), keys (\textit{K}), and values (\textit{V}), the output (\textit{O}) of the attention mechanism is calculated as a weighted sum of the values, where the weight assigned to each value is determined by the query's compatibility with the corresponding key:
\begin{equation}
\text{\textit{Attention}}(Q, K, V) = \text{\textit{softmax}}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}
where the denominator \(\sqrt{d_k}\) is used for scaling, with \(d_k\) being the dimension of the key vectors. The softmax function ensures the attention scores are normalized to lie between 0 and 1, thus can be interpreted as probabilities. This results in words that are more important to the meaning of the sentence receiving a higher attention score, while less important words receive a lower score.

The use of attention weights allows us to visualize and interpret the model's decision-making process. By examining these weights, we can understand which parts of the input sentence are most influential in determining the output of the model.

\section{Experiment}
\subsection{Data}
The datasets were partitioned into training, evaluation, and test subsets following an 8:1:1 ratio, providing a comprehensive and balanced basis for our experimental studies. And both the original and augmented datasets would be passed through the model to evaluate and quantify the influence of data augmentation.\par
The table \ref{overviewdata} presented below provides a visual representation of both the initial and augmented datasets. Specifically, during the training phase using the original dataset, we observed that the AU LMI dataset exhibited superior performance compared to the other two datasets. Consequently, during the process of data augmentation, we prioritized augmenting the ONET and ESCO datasets to a greater extent, while applying relatively fewer augmentations to the AU LMI dataset. This decision was based on the recognition that ONET and ESCO present greater challenges in terms of machine learning, warranting additional exposure to augmented instances for effective learning.
\begin{table}[htbp]
\caption{The overview of dataset}
\label{overviewdata}
\centering
\begin{tblr}{
  cell{1}{1} = {c=2}{},
  cell{2}{1} = {r=2}{},
  cell{4}{1} = {r=2}{},
  cell{6}{1} = {r=2}{},
  vline{1,2,3,4,5,6},
  hline{1,2,3,4,5,6,7,8}
}
                &           & O*NET & ESCO & AU LMI \\ 
Substitution    & Original  & 1,594  & 1,435 & 998    \\ 
                & \textbf{Augmented} & \textbf{3,188}  & \textbf{3,157} & \textbf{1,796}   \\
Complementarity & Original  & 2,519  & 2,272 & 1,776   \\ 
                & \textbf{Augmented} & \textbf{3,023}  & \textbf{3,181} & \textbf{1,776}   \\
Negligibility  & Original  & 947   & 1,076 & 582    \\ 
                & \textbf{Augmented} & \textbf{3,030}  & \textbf{3,228} & \textbf{1,764} 
\end{tblr}
\end{table}

\subsection{Evaluation Metrics}
We assessed our model and baselines using precision, recall, and F1 score. Precision evaluates the ratio of correct positive predictions to all positive predictions, indicating the model's false-positive avoidance. Recall measures the fraction of true positives from all actual positives, reflecting the model's ability to recognize all relevant instances. The F1 score, the harmonic mean of precision and recall, balances these aspectsâ€”a high F1 score indicates a model with high precision and recall, and vice versa.\par
In our multi-class problem, these metrics were computed for each class, considering the class in question as positive and the rest as negative. The average of per-class metrics provided a single performance measure across all classes.

\subsection{Baselines}
In our work, we make a comprehensive comparison of our proposed BERT-based model with a range of baseline models, encompassing both traditional machine learning approaches, neural network architectures, and other transformer models. These baselines span different approaches to text classification, allowing us to assess the relative merits of our approach in a broad context.

For traditional classifiers, we employ Logistic Regression \cite{cessie1992ridge}, Random Forest \cite{breiman2001random}, and Support Vector Machines (SVM) \cite{cortes1995support}. These models, with their varying theoretical underpinnings, provide a solid foundation against which to compare our more complex neural model. They represent different forms of linear and non-linear decision boundaries and incorporate different forms of regularization and ensemble learning.

In the realm of neural networks, we utilize Bi-directional Long Short-Term Memory (BiLSTM) \cite{graves2005framewise}, Gated Recurrent Units (GRU) \cite{cho2014learning}, and one-dimensional Convolutional Neural Networks (Conv1D) \cite{yoon2020convolutional}. These architectures demonstrate the potential of deep learning for text classification, and their comparison with our model allows us to quantify the value of pre-training and transformer architecture in this context.

Finally, we include three additional transformer models in our baselines: ALBERT \cite{lan2019albert}, ELECTRA \cite{clark2020electra}, and DistilBERT \cite{sanh2019distilbert}. As close relatives of BERT, they provide a stringent test of the specific benefits of our BERT-based approach. By contrasting with these models, we can highlight the unique strengths of our chosen methodology.

\subsection{Results}
The table \ref{results} summarizes the results for proposed BERT and other baseline models of different datasets. We evaluate them with precision, recall and F1-score.\par

% \usepackage{tabularray}
\begin{table*}
\caption{The results of proposed model and baselines.}
\label{results}
\centering
\begin{tblr}{
  cell{1}{1} = {r=2}{c},
  cell{1}{2} = {c=3}{c},
  cell{1}{5} = {c=3}{c},
  cell{1}{8} = {c=3}{c},
  cell{1}{11} = {c=3}c{},
  cell{16}{12} = {c=2}{c},
  vlines,
  hlines,
  column{1} = {c},
  column{2-13} = {c}
}
\textbf{Model}   & \textbf{O*NET}     &        &        & \textbf{ESCO}      &        &        & \textbf{AU LMI}    &        &        & \textbf{O*NET + ESCO + AU LMI} &        &        \\
        & \textbf{Precision} & \textbf{Recall} & \textbf{F1}     & \textbf{Precision} & \textbf{Recall} & \textbf{F1}     & \textbf{Precision} & \textbf{Recall} & \textbf{F1}     & \textbf{Precision}             & \textbf{Recall} & \textbf{F1}     \\
LR      & 0.6091    & 0.61   & 0.609  & 0.5915    & 0.5915 & 0.5911 & 0.657     & 0.6602 & 0.6566 & 0.5902                & 0.5922 & 0.5908 \\
SVM     & 0.6087    & 0.6072 & 0.6046 & 0.5738    & 0.5719 & 0.5707 & 0.6462    & 0.6473 & 0.6452 & 0.5837                & 0.5867 & 0.5811 \\
RF      & 0.5922    & 0.593  & 0.5921 & 0.5657    & 0.5656 & 0.5652 & 0.6449    & 0.6483 & 0.6399 & 0.5889                & 0.5909 & 0.5891 \\
BiLSTM  & 0.7487    & 0.7562 & 0.7532 & \textbf{0.7973}    & \textbf{0.7898} & \textbf{0.7876} & \textbf{0.8997}    & \textbf{0.8994} & \textbf{0.8995} & 0.7676                & 0.7701 & 0.7665 \\
GRU     & \textbf{0.7712}    & \textbf{0.7676} &\textbf{ 0.7712} & 0.7587    & 0.7612 & 0.7611 & 0.8677    & 0.8701 & 0.8664 & 0.7609                & 0.7624 & 0.7611 \\
CONV1D  & 0.7695    & 0.7742 & 0.7778 & 0.7745    & 0.7812 & 0.7822 & 0.8424    & 0.8402 & 0.8468 & 0.7766                & 0.7798 & 0.7723 \\
ALBERT  & 0.7703    & 0.6989 & 0.6988 & 0.7687    & 0.7723 & 0.7701 & 0.7997    & 0.8033 & 0.7946 & 0.7253                & 0.7114 & 0.6801 \\
ELECTRA & 0.7544    & 0.7621 & 0.7602 & 0.7384    & 0.7381 & 0.7285 & 0.8145    & 0.8093 & 0.8122 & 0.7459                & 0.7419 & 0.7425 \\
\textbf{BERT}    & 0.7698    & 0.7643 & 0.7594 & 0.7701    & 0.7723 & 0.7698 & 0.8241    & 0.8225 & 0.8198 & \textbf{0.7981}                & \textbf{0.7955} & \textbf{0.7944}   
\end{tblr}
\end{table*}

As we can see from the results, the GRU model demonstrates the highest performance on the O*NET dataset, with Precision, Recall, and F1-Score at 0.7712, 0.7676, and 0.7712, respectively. BiLSTM exhibits superior performance on the ESCO dataset, with Precision, Recall, and F1-Score at 0.7973, 0.7898, and 0.7876, respectively. The same model outperforms others on the AU LMI dataset, with all three metrics around 0.8995.\par

When all datasets are combined, our proposed BERT-based classifier emerges as the most effective model. The precision, recall, and F1-score are 0.7981, 0.7955, and 0.7944, respectively, which are the highest among all models. These results indicate the robustness of BERT in handling diverse and large-scale datasets and its superiority over traditional, neural network, and other transformer models for this multi-class classification task. We will use BERT as the best performance model for follow-up experiments and analysis.

\subsection{Data Augmentation Analysis}
In this section, we examine the impact of different augmentation levels on our BERT model's performance. The data augmentation methods include no augmentation (Original), augmenting data to match the class with the largest instances (Balanced), and incremental augmentation of the original data by 1.5, 2, 2.5, 3, 4, and 5 times. The results are shown below as Fig. \ref{fig: augmentation}:\par
\begin{figure}[htp!]
\centerline{\includegraphics[width=0.8\columnwidth]{svg-inkscape/data-comp_svg-raw.pdf}}
\caption{The Comparison of different augmentation.}
\label{fig: augmentation}
\end{figure}
A careful review of the results indicates that augmentation improves the precision of the model across all three datasets (ONET, ESCO, and AU LMI). For the ONET and ESCO datasets, the precision increases modestly from ``Balanced" to ``2 Times" and then plateaus slightly at ``2.5 Times". However, a dramatic rise is seen at ``4 Times", implying potential over-augmentation, which could lead to an overfitted model.\par
Therefore, we choose to augment our data between ``Balanced" and ``2 Times" for the O*NET and ESCO datasets. As for the AU LMI dataset, which already shows good performance, we only balance the classes.

\subsection{Model Robustness on Different Data Split}
To ascertain the robustness of our proposed model for real-world applications, we conducted an experiment with varying training set sizes. The dataset was shuffled, and training sets were created with different proportions of the original dataset, namely 80\%, 70\%, 60\%, 50\%, 40\%, 30\%, and 20\%. The remaining data in each scenario was used for performance evaluation. The results are presented at Fig. \ref{fig: robustness}:\par
\begin{figure}[htp!]
\centerline{\includegraphics[width=0.8\columnwidth]{svg-inkscape/trainingratio_svg-raw.pdf}}
\caption{The performance of model at different data split.}
\label{fig: robustness}
\end{figure}
As can be observed, the proposed BERT model's performance is relatively stable, and it gets better performance with the increase of training data ratio. Besides, our model with only 20\% training data can catch up with the baselines who use 80\% data for training, which validates the effectiveness of BERT in the task of predicting the automatability at task level.

\subsection{Attention Weights Visualization}
For the visualization of the model's attention, we have employed the technique of Wordcloud.
% It's important to note that in order to enhance the interpretability of these word clouds, we have carried out a token filtering process. Certain subword tokens generated by the BERT model, while meaningful in the model's internal representations, can appear as nonsensical when visualized in a word cloud. These include tokens which are fragments of words or specific to the BERT model like '[SEP]', '[CLS]', or 'ing s'. We have removed such tokens from our visualization to focus solely on the meaningful, interpretable tokens that can provide substantive insights into the model's decision-making process.\par
For class Substitution, the prominent terms include ``using system", ``machinery operate", ``data record", ``routine perform", and ``trucks load". These tasks are generally routine and predictable, and hence, are more prone to automation. On the other hand, for class Complementarity and class Negligibility, the key terms hint at tasks that require a higher level of human judgment or interaction like ``information provide", ``educational program", ``research conduct", ``medical procedures", ``human expertise", and ``children care". These tasks correlate with recognized automation bottlenecks, corroborating the model's predictions. The results can be found at Fig. \ref{fig: wordcloud}:
\begin{figure}[htp!]
\centerline{\includegraphics[width=0.8\columnwidth]{svg-inkscape/wordcloud_svg-raw.pdf}}
\caption{The Wordclouds for different categories.}
\label{fig: wordcloud}
\end{figure}

\section{Results and Discussion}
Having established the optimal performance of our BERT model through a series of experiments, we moved to a practical application, leveraging the model for inference on real-world datasets. We selected O*NET task statements, a rich and diverse dataset that encapsulates a broad variety of professional tasks, totalling 19,530 individual tasks. The results showed that among the total tasks, 6664 tasks were labeled as ``Substitution", 10,678 tasks were ``Complementarity" and 2188 tasks were ``Negligibility" which could be visualized as Fig. \ref{fig: distrubution}:\par
\begin{figure}[htp!]
\centerline{\includegraphics[width=0.8\columnwidth]{svg-inkscape/results-distribution_svg-raw.pdf}}
\caption{The Distribution of O*NET Task Automatability.}
\label{fig: distrubution}
\end{figure}
\subsection{Assessment of Automatability at Occupation Level}
We mapped individual task automatability measures to 974 distinct occupations using the O*NET task statement to occupation mapping. This allowed us to assess automatability at an occupation level by aggregating tasks and quantifying task type distributions. The results, summarized in Table \ref{fig: top10}, show the top 10 occupations with the highest substitution and negligibility, indicating the most and least likely automated occupations, respectively.
\begin{figure}[htp!]
\centerline{\includegraphics[width=0.8\columnwidth]{svg-inkscape/top10_svg-raw.pdf}}
\caption{Top 10 occupations that have highest and lowest automatabilities.}
\label{fig: top10}
\end{figure}
As can be seen from the results, occupations with high automation susceptibility are primarily manual labor, repetitive tasks, or data-intensive roles, such as "Dishwashers" and "Packaging and Filling Machine Operators and Tenders". Conversely, roles requiring human interaction, creativity, specialized skills, or unpredictable environments, like "Athletes and Sports Competitors" and "Respiratory Therapists", showed lower automatability. \par
% Conversely, occupations in areas that require high levels of human interaction, creative thinking, specialized skills, or involve unpredictable environments demonstrated lower automatability. Occupations like ``Athletes and Sports Competitors', ``Sales Agents, Financial Services', and ``Respiratory Therapists' are less prone to be substituted by automation, with substitution percentages around 11.1\%, 12.5\%, and 31.8\% respectively.\par
Our research findings indicate that out of 974 ONET occupations, 244 display a substitution score exceeding 50\%. This suggests that approximately 25.1\% of occupations within the ONET database are at substantial risk of automation. This outcome is aligned with the findings put forth in \cite{dawei}, thus adding robustness to our estimations. Furthermore, these results are nested between the estimations reported in \cite{freyosborne} which places 47\% of occupations at risk and \cite{arntz} which estimates the figure at a comparatively lower 9\%. Considering the limitations often associated with automated risk evaluations at the occupation and job level, the median value that our research arrives at seems reasonable. This is due to our methodology which leverages granular task statement data to predict occupation-level risks.\par
Additionally, our results indicate that a majority of occupations, specifically 603 out of 974 which is 61.8\%, face a high risk of complementarities. Conversely, a relatively smaller fraction, constituting 128 occupations or 13.1\%, appear to be comparatively safe from impending automation over the forthcoming decades. These conclusions underscore the nuanced complexities underpinning automation risks and the need for more granular data analysis in this domain.
\subsection{Assessment of Automation Vulnerability Among Industries}
Building on our earlier findings of automatability at the occupation level, we bridge the occupation-industry gap utilizing O*NET's detailed occupation-industry mapping. Our results shown as Fig. \ref{fig: top5} indicate a diverse spectrum of automation vulnerability across sectors. On one end, industries such as ``Accommodation and Food Services", ``Administrative and Support Services", ``Retail Trade", ``Mining, Quarrying, and Oil and Gas Extraction", and ``Manufacturing" emerge as the sectors most susceptible to automation. These industries comprise occupations with high automatability scores, revealing their high likelihood of experiencing significant changes due to automation.\par
On the other hand, industries such as ``Educational Services", ``Arts, Entertainment and Recreation", ``Other Services (Except Public Administration)", ``Real Estate and Rental and Leasing", and ``Health Care and Social Assistance" stand on the less vulnerable end of the automation spectrum. Occupations within these sectors possess low automatability scores, indicating a lower likelihood of their roles being fully automated, largely due to the complexity of tasks or the high level of human judgment, interaction, and creativity required.\par
\begin{figure}[htp!]
\centerline{\includegraphics[width=1\columnwidth]{svg-inkscape/top5_svg-raw.pdf}}
\caption{Industries with highest and lowest automatabilities.}
\label{fig: top5}
\end{figure}
% % \begin{figure}[htbp]
% \centerline{\includegraphics[width=1\columnwidth]{top5l.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

To validate the results, we leveraged the insights from the \cite{mckinsey2017jobs}, and identified 4 sectors showing siginifican susceptibility to automation which are ``Accommodation and Food Services", ``Manufacturing", ``Retail Trade", and ``Transportation and Warehousing". These findings align well with our own results.

\section{Conclusion}
This study presents a unique approach to predict task-level automatability with a BERT-based classifier, utilizing three diverse, public datasets and expert annotations. Rigorous experiments demonstrated our model's efficacy.\par
In practical application, we applied our model to real-world datasets, providing a comprehensive perspective of occupational automatability. Our findings indicate that approximately 25.1\% of occupations within the O*NET database are at substantial risk of automation. Furthermore, our results reveal a diverse spectrum of automation vulnerability across sectors, with industries such as ``Accommodation and Food Services", ``Administrative and Support Services", ``Retail Trade", ``Mining, Quarrying, and Oil and Gas Extraction", and ``Manufacturing" emerging as the sectors most susceptible to automation.\par
These findings have significant implications for workers and policymakers. For workers, understanding the susceptibility of their tasks to automation can help them make informed decisions about their career paths and upskilling opportunities. For policymakers, these insights can guide the development of policies and initiatives aimed at managing the transition to an increasingly automated workforce. This could include strategies for retraining and reskilling workers, as well as measures to support industries and regions that are particularly vulnerable to automation.\par
In conclusion, our research provides a robust and effective approach to predicting task-level automatability, offering valuable insights for policymakers, educators, and workers. By understanding the potential impact of automation on different tasks, occupations, and industries, we can better prepare for the future of work.\par

\section*{Acknowledgment}
This work is supported by the Australian Research Council (ARC) under Grant No. DP220103717, and LE220100078.

\input{conference_101719.bbl}

% \bibliographystyle{IEEEtran}
% \bibliography{bibs}

% \onecolumn
% \begin{appendices}
% \clearpage
% \section{The Results of Occupations' Automatability}
% \includepdf[pages=-]{class_percentage_output.pdf}
% \break
% \section{The Results of Industries' Automatability}
% \includepdf[pages=-]{industry_automatability_final.pdf}

% \end{appendices}

\onecolumn
\begin{appendices}
\clearpage
\includepdf[pages=1, pagecommand={\begin{minipage}{\textwidth}\section{The Results of Occupations' Automatability}\vspace{1cm}\end{minipage}}]{class_percentage_output.pdf}
\includepdf[pages=2-, pagecommand={}]{class_percentage_output.pdf}
\includepdf[pages=1, pagecommand={\begin{minipage}{\textwidth}\section{The Results of Industries' Automatability}\vspace{1cm}\end{minipage}}]{industry_automatability_final.pdf}
\end{appendices}

% \onecolumn
% \begin{appendices}
% \clearpage
% \includepdf[pages=1, pagecommand={\section{The Results of Occupations' Automatability}}]{class_percentage_output.pdf}
% \includepdf[pages=2-24, pagecommand={}]{class_percentage_output.pdf}
% \includepdf[pages=25, pagecommand={\afterpage{ Notes: Your explanation here.}}]{class_percentage_output.pdf}

% \includepdf[pages=1, pagecommand={\section{The Results of Industries' Automatability}\afterpage{\vspace*{5cm} Notes: Your explanation here. \clearpage}}]{industry_automatability_final.pdf}
% \end{appendices}




\end{document}
