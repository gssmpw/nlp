\section{Related Work}
\textbf{Vision-language-Action models for robot control.} Recent research has focused on developing generalist robot policies trained on increasingly expansive robot learning datasets~\cite{fang2020graspnet, grauman2022ego4d, fu2024humanplus, ha2024umi, ha2023scaling, lin2024data, radosavovic2023real, chi2024universal,geiger2013vision}. Vision-language-action models (VLA)~\cite{kim24openvla,[pi0,pertsch2025fast,wen2024tinyvla,rt-2,zhen20243d,zhang2024grape,guo2025improving,belkhale2024rt-h} represent a promising approach for training such generalist policies. VLAs adapt vision-language models, pre-trained on vast internet-scale image and text data, for robotic control~\cite{yen2020learning}. This approach offers several advantages: leveraging large vision-language model backbones, with billions of parameters, provides the necessary capacity for fitting extensive robot datasets. Furthermore, reusing weights pre-trained on internet-scale data enhances the ability of VLAs to interpret diverse language commands and generalize to novel objects and environments. However, current VLA models do not specifically focus on learning dexterous robotic skills by leveraging the parameters of the underlying VLM. While a few works, such as $\pi_{0}$~\cite{[pi0} and TinyVLA~\cite{wen2024tinyvla}, introduce external action experts to facilitate action learning, their training pipelines still rely on the entire model. Another challenge is that even advanced methods like $\pi_{0}$, despite being capable of completing highly dexterous and long-horizon tasks, require the assistance of a high-level policy, such as SayCan~\cite{ahn2022can}, to decompose tasks into sub-goals. This allows the VLA to complete sub-tasks sequentially. We aim to integrate this high-level planning capability directly into the model itself by training each component of the network with data annotated at the sub-step level. Consequently, our method can complete complex tasks, like laundry folding, without requiring an external high-level policy, making the entire framework more end-to-end and demonstrating significant potential.

\textbf{Diffusion models.} Diffusion models~\cite{chen2024yilun,peebles2023scalable,ho2020denoising} have emerged as the dominant approach in visual generation. The Diffusion Policy~\cite{diffusion-policy} successfully applies the diffusion model to robot learning, demonstrating its ability to model multimodal action distributions. Subsequent research has further developed the Diffusion Policy~\cite{aloha_unleashed, wang2024sparse-dp, prasad2024consistencypolicy, multimodal_diffusion_transformer, uehara2024fine, uehara2024feedback, black2023training, black2023zero, dasari2024ingredients, lin2024datascalinglawsimitation, dppo, wang2024inference, liu2022compositional} by applying it to 3D environments~\cite{3d_diffuser_actor, ze20243d, ze2024generalizable, yan2024dnact}, scaling its capabilities~\cite{scaledp}, improving its efficiency~\cite{mail-dp,prasad2024consistencypolicy}, and incorporating architectural innovations. There are a number of works investigating the usage of diffusion VLA~\cite{wen2024tinyvla, [pi0, wen2024diffusionvla}. Although existing models achieve strong performance and generalization on diverse tasks, they predominantly rely on the capabilities of pre-trained vision-language models. This work proposes a paradigm shift towards the diffusion module, demonstrating that a newly designed diffusion-based action expert, coupled with a novel training strategy, enables VLA models to learn from data more efficiently and effectively.
