\clearpage
\newpage
%\onecolumn
\begin{appendix}




\subsection{Visual Generalization}
Visual generalization is a critical aspect of robot learning.  A well-trained model should not only perform well on in-domain tasks but also generalize to different objects within the same category and to novel scenes. This section presents our visual generalization tests. Specifically, we evaluate shirt folding on a bimanual AgileX and drink pouring using a Franka Emika robot with a dexterous hand. The former task is evaluated without task-specific adaptation, while the latter is trained with 100 demonstrations of the new embodiment. These tasks were also the focus of the experiments presented in Section~\ref{sec:no_post_training} and Section~\ref{sec:new_embodiment}, respectively. For both tasks, we assess visual generalization across two dimensions: novel objects and novel scenes.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{images/visual_generalization.pdf}
    \caption{\textbf{Example of visual generalization.} Here lists some visual generalization settings including unseen objects and unseen scenes.}\label{fig:visual_generalization}
\end{figure}

\begin{table*}[t]
  \centering
  \caption{\textbf{Visual Generalizaton for Dex-VLA}. For each evaluation setting, we report the averaged scores across 3 trials.}
  % soap, hang cup,toothpaste,towel
  \label{tbl:visual_generalization}
  \resizebox{0.8\linewidth}{!}{
      \begin{tabular}{c|c|ccc}
        \toprule
        Task / Generalization & Embodiment &Novel Object & Novel Scene & Novel Object \& Scene \\
        \midrule
         Shirt folding &  Bimanual AgileX & 0.78 & 0.78 & 0.56 \\
         Drink pouring & Dexterous hand& 0.83 & 0.67 & 0.67 \\
        \bottomrule
      \end{tabular}
    }
\end{table*}
For shirt folding, we varied the shirt color (while maintaining size) and altered the background and scene.  For drink pouring, we used unseen cups and bottles, also evaluating the task in different scenes and backgrounds.  The results are presented in Table~\ref{tbl:visual_generalization}.  Our experiments demonstrate that Dex-VLA effectively generalizes to novel visual environments.  As shown in the supplementary video, the model successfully handles even challenging cases, such as folding white shirts. The examples are shown in Figure~\ref{fig:visual_generalization}.




\subsection{Training Cost for Stage 1}
As mentioned in Section~\ref{sec:stage1}, training the entire VLA model from scratch results in failure on nearly all tasks.  Therefore, this section compares the training cost of our Stage 1 (training only the diffusion expert) with that of training the entire VLA model. The test reports the number of training epochs completed per hour. The model is trained on 8 Nvidia H100 GPUs. We deliberately keep the same batch size for fair comparison. 

As shown in Table~\ref{tbl:train_cost}, training only the diffusion expert is 2.78 times faster than training the entire VLA model. This is expected, as the VLA model is three times larger than the diffusion expert alone. This highlights that our training strategy is not only effective but also cost-efficient.

\begin{table}[t]
  \centering
  \caption{\textbf{Comparison of training cost for train only diffusion expert versus train entire VLA.} Training cost is measured by the number of training epochs completed per hour.}
  % soap, hang cup,toothpaste,towel
  \label{tbl:train_cost}
  \resizebox{1\linewidth}{!}{
      \begin{tabular}{c|cc}
        \toprule
        Train Method & Train only Diffusion Expert  & Train Entire VLA \\
        \midrule
        Training Cost & 0.89 epoch/hour& 0.32 epoch/hour \\
        \bottomrule
      \end{tabular}
    }
\end{table}

\input{4_sub_ablation_study}



\subsection{Evaluation Protocol}
Each task is evaluated across 10 trials and reported averaged scores. For each task, we list the detailed scoring criterion as follows.
\begin{itemize}
    \item \textbf{Lanudary folding (Bimanual AgileX)}: This task is scored out of 4 and we evaluate 5 shirts in total including 2 middle size and 3 small size. We perform two trials for each item, and the items left to be evaluated starting randomly crumpled in a laundry bin (while previously evaluated items start in a fold). One point is given for picking an item out of the bin and putting it on the table. Another point is given for flattening the shirt or shorts. A third point is granted for folding the shirt or shorts. A final point is given for either placing the item in the corner of the table (if it is the first item evaluated), or stacking it onto an existing stack of folded clothes. This evaluation metric is followed $\pi_{0}$. 
    \item \textbf{Shirt folding (Bimanual AgileX)}: This task is scored out of 3. We perform two trials for each item, and the items are flattened on the table. One point is given for double vertically fold. Another point is granted for a double horizontal fold. A final point is given for pushing the folded shirt to the right blank area.
    \item \textbf{Bussing table (Bimanual AgileX)}: This task is scored out of 3-4 where there are 3-4 objects on the table in both \textbf{easy and hard version}. The main difference is the objects that appeared in the hard version are unseen. A point is given for each correctly sorted object.
    \item \textbf{Dryer unloading (Bimanual AgileX)}: This task is scored out of 2 where there are 2 crumpled shirts in the dryer. A point is given for pick up a shirt and place into the hamper.
    \item \textbf{Sorting (Franka with gripper)}: This task is scored out of 5-8 where there are 5-8 objects on the table. There are four kinds of objects in total, a point is given for each correctly sorted object.
    \item \textbf{Drink pouring (Franka with dexterous hand)}: This task is scored out of 2. A point is given for grab the bottle and pour to the cup. Another point is granted for place down the bottle.
    \item \textbf{Bining picking (Franka with gripper)}: This task is scored out of 4-5 where there are 4-5 objects on the table. The main difference is the objects that appeared in the hard version are unseen. A point is given for each correctly picked and placed object.
    \item \textbf{Packing (Bimanual UR5e)}: This task is scored out of 2 where there are 2 objects on the table. A point is given for each correctly picked and placed object.
\end{itemize}



\subsection{More Implementation Details}
This section gives more details on our model, robot setup, and training method. 

\textbf{Robot setup.} Our setup includes two Franka Emika robots: one equipped with a gripper and the other with a robot hand.  Both Franka robots utilize the same camera configuration, consisting of a ZED 2 camera positioned on both the left and right sides, as well as a ZED Mini wrist camera mounted on the robot itself.  Our bimanual UR5e robot uses a single top-mounted Intel RealSense L515 camera and two Intel RealSense 435i cameras attached to the wrists.  Finally, our mobile AgileX platform has two Intel RealSense 435i wrist cameras and a top-mounted Intel RealSense 457 camera.  Although the mobile AgileX image includes a front camera, it was not used during either training or inference.

\textbf{Sub-step reasoning and data acquisition.}
Training with sub-step reasoning is crucial for Dex-VLA to complete long-horizon tasks without a high-level policy model. We present an ablation study on the importance of sub-step reasoning in Section~\ref{sec:ablation}. Acquiring this data presents two key challenges: obtaining language instructions and segmenting videos with corresponding annotations. We address these challenges with the following strategy.

For object-level tasks (e.g., bin picking, sorting, table bussing), object identification is key. We leverage Grounding-Dino and DINOv2 to annotate object bounding boxes and names, along with the gripper's bounding box.  We then calculate the intersection over union between the gripper and object bounding boxes to determine grasp success. For long-horizon single-object tasks (e.g., fold one shirt), the challenge lies in task segmentation. We created a comprehensive list of potential sub-step reasoning, focusing on major steps lasting at least five seconds each to avoid excessive sub-division. We then used Google Gemini 2.0, providing it with the sub-step list, to segment the videos and select the corresponding reasoning from the list. This proved effective and efficient for labeling.  We only manually checked the Stage 3 training data, as this stage requires higher-quality annotations. This annotation strategy makes our approach feasible.

\textbf{Architectural details.} In this section, we provide a full description of the model architecture. Dex-VLA can be split into two parts, VLM backbone originates from Qwen2-VL~\cite{wang2024qwen2} and diffusion expert. We use Qwen2-VL 2B which is powerful and efficient. Regarding our Diffusion Expert, the total number of parameters for this model is 1 billion parameters. We use 32 layers, with the hidden stage of 1280, and a number of heads of 16. During Stage 1, we only pre-train the diffusion expert with random initialized ResNet-50 to process images and off-the-shelf Distilbert~\cite{sanh2019distilbert} to encode language instructions. Because the original diffusion policy model does not support cross-embodiment training, we adopted a multi-head structure similar to Octo~\cite{octo}. Each embodiment is assigned a unique MLP head. The diffusion expert is trained using the similar settings of our Dex-VLA. In particular, we use the image resolution of 320 $\times$ 240, with three camera views. Each image is processed independently to a ResNet-50. We use the strategy as in RT-1~\cite{brohan2022rt-1} to initialize the FiLM layers. 





\end{appendix}