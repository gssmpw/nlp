\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{images/framework.pdf}
    \caption{\textbf{DexVLA architecture and embodied curriculum learning.} Our model employs a three-stage training process. \textbf{Stage 1 (left)} trains the Diffusion Expert independently, without the VLM. \textbf{Stages 2 and 3 (middle)} integrate the Diffusion Expert with a VLM, discarding the visual and language components within the expert. \textbf{The Diffusion Expert (right)} uses multiple heads for cross-embodiment learning.}\label{fig:frame_work}
    \vspace*{-10pt}
\end{figure*}



\section{Method}
This section gives a detailed introduction to our model architecture and training strategy. The overview is presented in Figure~\ref{fig:frame_work}.
\subsection{Model Architecture}
Our~\methodname~model is primarily based on a transformer language model backbone. Following the common framework of VLM models, we employ image encoders to project the robot's image observations into the same embedding space as the language tokens. For multiple camera views, these visual tokens are concatenated. The VLM component generates two outputs: reasoning tokens and action tokens. The action tokens are passed through a projection module, consisting of two linear layers with LayerNorm. This module is analogous to the connectors designed in vision-language models like LLaVA~\cite{liu2024visual}, and serves to transform the VLM's embedding space to align with the input requirements of the action expert. The reasoning tokens are injected into the policy model using FiLM layers, which scale and shift the parameters of the projection layers within the policy. Consequently, the model can autonomously generate reasoning and leverage this reasoning within the diffusion expert to guide action generation.

Following the
standard late fusion VLM recipe~\cite{liu2024improved, liu2024visual}, image encoders
embed the robot’s image observations into the same embedding space as language tokens. We further augment this
backbone with robotics-specific inputs and outputs — namely,
proprioceptive state and robot actions.

\textbf{Building diffusion expert.} 
Since action experts dominate the learning process of robot's action, it is essential to design good neural architecture for better visuomotor policy learning. We utilized the Scale Diffusion Policy (ScaleDP~\cite{scaledp}), a variant of the Diffusion Policy in Transformer architecture, where the largest version of ScaleDP is up to 1B parameters. However, the naive ScaleDP is not designed for cross-embodiement pre-training. We design a multi-head output to enable pre-training on ScaleDP with various robot configurations. Each head is responsible for a single robot configuration. This setup is similar to Octo. 

\textbf{Training objectives.} Given a batch of input sequences, the overall training loss is defined as a weighted combination of the diffusion loss ($L_{diff}$) and the next-token prediction (NTP) loss ($L_{ntp}$):
\begin{equation}
    L = L_{diff} + \alpha L_{ntp},
\end{equation}
where $\alpha$ is a hyperparameter controlling the relative weight of the NTP loss term, $L_{ntp}$. Empirically, without the stage 1 training, the magnitude of $L_{ntp}$ is consistently an order of magnitude smaller than that of $L_{diff}$. Yet, after we adopted stage 1 training to warm up the policy learning ability of diffusion experts, we found that the magnitude of $L_{ntp}$ becomes slightly smaller than that of $L_{diff}$. Therefore, we set $\alpha = 1$ in all experiments. This adjustment ensures that both the diffusion and next-token prediction objectives are comparably weighted during training, thus enabling the model to learn effectively from both action prediction and language understanding tasks. The following section provides a comprehensive introduction to the training strategy, detailing how it leverages the capabilities of our model architecture.



\subsection{Embodied Curriculum Learning}
Curriculum learning is an training strategy where a system learns tasks in a progression from simple to complex, mirroring how humans acquire skills. Our three-stage training strategy implements an embodied curriculum, where the agent first learns generalizable motor skills from cross-embodiment data (Stage 1), then adapts to its specific physical form (Stage 2), and finally refines task-specific behaviors (Stage 3). This mirrors human skill acquisition, where foundational abilities (e.g., grasping) precede specialized expertise (e.g., folding clothes). By decoupling embodiment-agnostic and embodiment-specific learning, we reduce data needs by 60\% compared to end-to-end training.




A well-designed training strategy is critical for optimizing deep neural networks. Approaches that align with a network's inherent training dynamics ensure more efficient and effective data utilization. Our framework,~\methodname, targets general robotic control by integrating a Vision-Language Model (VLM) with a diffusion expert. Leveraging its modular architecture—which combines two distinct components—we propose a three-stage training strategy that systematically addresses: (1) learning dexterous manipulation skills to enable the model to complete complex tasks; and (2) cross-embodiment learning to adapt the model to diverse robotic platforms.


\subsubsection{Stage 1: Cross-Embodiment Pre-training}
\label{sec:stage1}
The vision-language-action models can be viewed as a composite of two distinct components.  At the top of the architecture lies the vision-language model (VLM), which processes both visual input and language instructions, mapping them into a shared embedding space. This shared space is pre-trained using internet-scale data, enabling a wide range of capabilities, including language understanding, multimodal understanding, and various other vision-text tasks. However, despite its extensive training, the VLM lacks the ability to physically interact with diverse objects in real-world environments.

To effectively pre-train the action expert, we leverage all available data while temporarily decoupling it from the VLM component. This allows us to focus on developing a robust action generation capability independent of language grounding. While CNN architectures, such as ResNet, are commonly used as visual encoders, the visual encoders within VLM models typically employ a Transformer architecture. Therefore, for ease of alignment in subsequent stages, we utilize a ViT architecture for visual encoding. For language embedding extraction, we use DistilBERT~\cite{sanh2019distilbert}. The resulting language embeddings are then integrated into the model using FiLM layers, consistent with previous work. A key insight for pre-training the diffusion expert is the necessity of decomposing long-horizon tasks (e.g., bussing tables, laundry folding) into sub-tasks. These tasks, often spanning beyond 2 minutes, prove challenging for the diffusion expert to learn effectively from a single language instruction. Therefore, we annotate sub-step instruction within these long-horizon tasks to provide a more structured learning signal. Pre-training with sub-steps is crucial for strong performance. We demonstrate the importance of sub-step reasoning in the ablation study (see Appendix).  Our empirical observations show that VLA trained without this pre-training frequently skip critical steps in very long tasks.  Sub-step annotations are typically provided every five seconds of the demonstration.


Pre-training the large-scale robotic data on the diffusion-based action expert, rather than the entire VLA, offers several advantages. First, our diffusion expert has a significantly smaller parameter count compared to VLAs (1 billion vs. 3 billion), resulting in much faster training. Quantitatively, pre-training on the diffusion expert is five times faster than pre-training on the full VLA model.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{images/stage_3_task_description.pdf}
    \caption{\textbf{Example of direct prompting for long-horizon tasks. } The figure shows three tasks, \textbf{laundry folding (top)}, \textbf{dryer unloading (middle)},  \textbf{sorting (bottom)}. Our DexVLA breaks down raw instructions into sub-steps automatically. Success in these tasks necessitates not only dexterity but also the capacity to decompose direct prompts into implicit multi-step reasoning and to comprehend the visual context.}\label{fig:hard_task_suite}
\end{figure*}
\subsubsection{Stage 2: Embodied-Specific Alignment}
While Stage 1 learns basic motor skills from cross-embodied data, this cross-embodiment learning can potentially compromise performance on the target embodiment, making it unsuitable for real-world deployment. Stage 2 addresses this by training the model with embodiment-specific data, aligning the abstract vision-language representations from the VLM with the diffusion expert. Therefore, we filter the dataset to include only embodiment-specific data, ensuring each sample involves a single embodiment. Mirroring techniques employed in vision-language models like LLaVA~\cite{liu2024visual,liu2024improved}, this stage focuses on aligning the target embodiment's action space with its corresponding camera views and accompanying language instructions. Specifically, we jointly train the VLM model, the projection layer, and the diffusion expert on this embodiment-specific data, while freezing the VLM's visual encoder. This joint training allows the diffusion expert to effectively ground the high-level vision-language understanding from the VLM in the specific motor control space of the target robot. Following Stage 2 training, we observe that the model exhibits proficiency in performing a range of tasks on the target embodiment, such as shirt folding, demonstrating the effectiveness of the embodiment-specific training. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{images/robot_examples.pdf}
    \caption{Our experiment includes various robot types: bimanual UR5e, Franka, bimanual AgileX, and Franka with dexterous hands.}\label{fig:example_robots}
\end{figure}

\subsubsection{Stage 3: Task-Specific Adaptation}
\label{sec:task_specific_adaptation}
This phase refines the model's ability to execute downstream tasks skillfully and fluently, analogous to the post-training stage in large language models where the model is fine-tuned on domain-specific data.  For simpler, less generalization-dependent tasks, such as shirt folding, table bussing, or bin picking with trained objects, task-specific training is unnecessary as the model already performs well.  However, complex, dexterity-demanding tasks require the model to learn fine-grained, context-dependent actions.  Therefore, effective post-training relies on a high-quality dataset of expert demonstrations exhibiting consistent and fluent task execution strategies focused on behaviors that promote successful task completion.
It is worth noting that we utilize sub-step annotated language data in both Stage 2 and Stage 3. However, instead of directly using these sub-step reasoning as instructional input, we employ them as intermediate language output, compelling the model to learn and generate these sub-step language descriptions. This approach has proven highly effective, enabling our model to perform complex, long-horizon tasks such as laundry folding. While other VLAs, like $\pi_0$~\cite{[pi0}, can also complete such tasks, they rely on high-level policy models like SayCan~\cite{ahn2022can} to identify the task state and provide next-step instructions. In contrast, our framework leverages the VLM backbone as an implicit high-level policy model. This allows the model to internally interpret the task's state and inject this understanding into the policy to guide action generation, eliminating the need for an external high-level policy module.



\subsection{Implementation Details}
In our experiments, we employ the Qwen-2-VL 2B~\cite{wang2024qwen2} model as our base vision-language model. We utilize up to three camera views. Our diffusion expert comprises 1 billion parameters. Two MLP layers, each followed by LayerNorm, connect the vision-language model to the action experts. We use FiLM to inject the final embedding from the vision-language model into the diffusion expert, applying it to each of the two MLPs. During Stage 1 training, we use our collected dataset. Stage 2 training is restricted to data sharing the same embodiment. Post-training is performed selectively, only on tasks where it is explicitly mentioned. Our pretrained dataset has a total of 100 hours of data with 91 different tasks. The detailed data allocation is referred to the Figure~\ref{fig:data_allocation}. For clarity, we report data quantity in hours rather than trajectories, as the duration of tasks varies significantly (i.e., laundry folding takes considerably longer than other tasks). This method of data quantification is consistent with that used by $\pi_{0}$. The detailed hyperparameters are listed in Table~\ref{tab:hyperparams}. Specific details on implementation can be found in the Appendix. 


\input{table/hyper}
\subsection{Robot System Setup}

Our evaluation is conducted on four different robot configurations across 10 tasks. These setups are summarized in the following list and visualized in the accompanying figure:

\begin{itemize} 
\item \textbf{Franka with gripper.} A Franka Emika robot with 7 degrees of freedom, equipped with a Robotiq parallel jaw gripper. Data is collected at 15Hz. 
\item \textbf{Franka with dexterous hand.} An Inspired Dexterous Hand mounted on a Franka Emika robot. The camera setup is identical to the gripper version, with a total of 12-dimensional configuration. Specifically, SE(3) end-effector pose (3D position + 3D orientation)
plus 6-dimensional hand joint space. Data is collected at 15Hz. 
\item \textbf{Bimanual UR5e.} Two UR5e robots, each with a Robotiq parallel jaw gripper and a wrist-mounted camera. A top-down camera is positioned between the two arms. This setup has a total of three camera views and a 14-dimensional configuration and action space. Data is collected at 10Hz. 
\item \textbf{Bimanual AgileX.} Two 6-DoF AgileX arms, each with a wrist-mounted camera and a base camera. This setup has a 14-dimensional configuration and action space, supported by three cameras in total. Data is collected at 10Hz. 
\end{itemize}




