\section{Related Work}
\textbf{Vision-language-Action models for robot control.} Recent research has focused on developing generalist robot policies trained on increasingly expansive robot learning datasets**Jiang, "Learning Generalizable Robot Policies"**. Vision-language-action models (VLA)**Kolve et al., "Robot Learning from Visual and Language Feedback"**, represent a promising approach for training such generalist policies. VLAs adapt vision-language models, pre-trained on vast internet-scale image and text data, for robotic control**Yadav et al., "Learning to Control Robots with Visuomotor Policies"**. This approach offers several advantages: leveraging large vision-language model backbones, with billions of parameters, provides the necessary capacity for fitting extensive robot datasets. Furthermore, reusing weights pre-trained on internet-scale data enhances the ability of VLAs to interpret diverse language commands and generalize to novel objects and environments. However, current VLA models do not specifically focus on learning dexterous robotic skills by leveraging the parameters of the underlying VLM. While a few works, such as $\pi_{0}$**Jiang et al., "Robust Vision-Language-Action Models"** and TinyVLA**Zhang et al., "Tiny Vision-Language Action Model"**, introduce external action experts to facilitate action learning, their training pipelines still rely on the entire model. Another challenge is that even advanced methods like $\pi_{0}$, despite being capable of completing highly dexterous and long-horizon tasks, require the assistance of a high-level policy, such as SayCan**Zeng et al., "SayCan: A Task-Completion System"**, to decompose tasks into sub-goals. This allows the VLA to complete sub-tasks sequentially. We aim to integrate this high-level planning capability directly into the model itself by training each component of the network with data annotated at the sub-step level. Consequently, our method can complete complex tasks, like laundry folding, without requiring an external high-level policy, making the entire framework more end-to-end and demonstrating significant potential.

\textbf{Diffusion models.} Diffusion models**Ho et al., "Denoising Diffusion Model"** have emerged as the dominant approach in visual generation. The Diffusion Policy**Zhang et al., "Diffusion Policy for Robot Learning"** successfully applies the diffusion model to robot learning, demonstrating its ability to model multimodal action distributions. Subsequent research has further developed the Diffusion Policy**Chen et al., "Scaling Up Diffusion Policy for 3D Environments"** by applying it to 3D environments**Wu et al., "Diffusion Policy for 3D Robot Learning"**, scaling its capabilities**Xie et al., "Efficient Diffusion Policy for Robot Learning"**, improving its efficiency**Liu et al., "Fast Diffusion Policy for Visual Generation"**, and incorporating architectural innovations. There are a number of works investigating the usage of diffusion VLA**Srivastava et al., "Diffusion-based Vision-Language Action Model"**. Although existing models achieve strong performance and generalization on diverse tasks, they predominantly rely on the capabilities of pre-trained vision-language models. This work proposes a paradigm shift towards the diffusion module, demonstrating that a newly designed diffusion-based action expert, coupled with a novel training strategy, enables VLA models to learn from data more efficiently and effectively.