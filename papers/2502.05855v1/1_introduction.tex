\section{Introduction}
Enabling robots to perform diverse tasks across varied environments is a central challenge in robotics. Achieving versatility—the ability to solve a variety of tasks across diverse environments, while adapting to language commands, environmental constraints, and unexpected disruptions—is even more demanding. Imitation learning~\cite{lin2024learning,shi2023robocook, lin2024data, zhang2024learning, zeng2022robotic, qin2022dexmv}, particularly through vision-language-action (VLA) models (e.g., \cite{[pi0, rt-2, kim24openvla, hu2023look}), has shown promise in enabling generalizable skills. 


However, realizing the vision of omnipotent robot foundation models faces persistent challenges. Two key bottlenecks hinder progress: 1) Data scarcity: State-of-the-art models, like OpenVLA~\cite{kim24openvla}/Octo~\cite{octo}, rely on massive datasets like the Open-X Embodiment dataset (4,000 hours)~\cite{o2023open-x} or even larger corpora like the 10,000-hour dataset used by $\pi_{0}$~\cite{[pi0}. Collecting such data through human demonstrations is extremely costly and labor-intensive~\cite{fu2024humanplus, ha2024umi, wu2023tidybot, xiang2020sapien}. (2) Architectural imbalance: current VLA models often prioritize scaling the vision-language model (VLM) component, i.e., OpenVLA uses 7B VLM and $\pi_{0}$ uses 3B VLM. Despite its enhanced visual and linguistic understanding through internet-scale data pretraining, the VLM component remains disconnected from the embodied, sensorimotor context of robotic action.

This paper introduces plug-in \textbf{D}iffusion \textbf{Ex}pert for \textbf{V}ision-\textbf{L}anguage-\textbf{A}ction models, namely \textbf{DexVLA}, a novel framework designed to enhance the efficiency and generalization capabilities of VLA for complex, long-horizon tasks across diverse robot embodiments. We achieve this through two key innovations:


\textbf{1) Billion-Parameter Diffusion Expert}: Recognizing the limitations of conventional action experts, particularly in handling cross-embodiment data, we propose a new diffusion-based action expert. The diffusion expert utilizes a multi-head architecture, with each head corresponding to a specific embodiment, enabling effective learning across diverse morphologies.  Furthermore, we scale the model size of the diffusion expert to one billion parameters, a substantial increase from the conventional multi-million parameter scale. This scaling significantly enhances the model's capacity to learn intricate motor skills and control policies from diverse and extensive data.

\textbf{2) Embodied Curriculum Learning}: A three-stage training strategy that progressively learns harder tasks. This is conceptually similar to how human learn, which starts with simple tasks, and then gradually introduces complexity to avoid overwhelming the learner.

\textbf{Stage 1:} The \textit{cross-embodiment pre-training}  stage focuses on learning low-level, embodiment-agnostic motor skills. In this stage, we pre-train only the diffusion expert using cross-embodiment data, without involving the vision-language models.

\textbf{Stage 2:} The \textit{embodiment-specific alignment} is a stage that analogy to ''adapt to your body". Specifically, it bridges abstract vision-language representations to the physical constraints of a specific robot. Remarkably, this stage alone enables the model to complete a variety of tasks, such as shirt folding and bin picking on in-domain objects.

\textbf{Stage 3:} The \textit{task-specific adaptation} aims to make the robot master complex tasks. These tasks include completing long-horizon tasks and generalizing to novel objects. 


While the model has learned from diverse robot behaviors and progressively developed dexterous skills for complex tasks, it faces limitations in very long-horizon, contact-rich scenarios such as folding crumpled shirts or executing continuous bin-picking. Prior approaches often rely on high-level policy models; for example, $\pi_{0}$ use SayCan to update instructions every two seconds. In contrast, we propose leveraging the innate reasoning abilities of the VLA model to directly guide robot action. We train the model using demonstrations annotated with \textit{substep reasoning} — for instance, breaking “fold the shirt” into “smooth wrinkles,” “align sleeves,” and “secure folds” — enabling it to learn disentangled action representations that map language sub-instructions to precise motor primitives. 


We evaluate DexVLA across diverse embodiments, including single-arm, bimanual, dexterous hand, and mobile bimanual robots, demonstrating its effectiveness on a variety of tasks. DexVLA achieves high success rates on many tasks without task-specific adaptation. For example, it achieves a near full score (0.92) in folding flattened shirts. It can also learn dexterous skills on novel embodiments with fewer than 100 demonstrations, such as pouring drinks with a dexterous hand and packing on a bimanual UR5e. Furthermore, our approach can complete complex, long-horizon tasks like laundry folding using only direct instructions, eliminating the need for a high-level policy model to decompose the task. Importantly, our model is pre-trained on only 100 hours of demonstration data and runs at 60Hz on a single Nvidia A6000 GPU, enabling cost-efficient training and fast inference.

In summary, our contributions are three-fold:
\begin{itemize}
    \item We introduce Vision-Language Diffusion Expert (DexVLA), a novel architecture that scales the action expert to facilitate cross-embodiment learning.
    \item We present an embodied curriculum learning strategy tailored for efficient and effective training of DexVLA. Furthermore, we enhance the VLA model's learning by incorporating demonstrations annotated with sub-step reasoning.
    \item We conduct comprehensive experiments across multiple embodiments, demonstrating DexVLA's adaptability to challenging tasks without task-specific adaptation, its ability to learn dexterous skills on novel embodiments with limited data, and its capacity to complete complex, long-horizon tasks using only direct language prompting.
\end{itemize}
