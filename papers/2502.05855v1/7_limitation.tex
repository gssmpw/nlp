\section{Conclusion}
This work proposes DexVLA, a novel architecture that leverages vision-language models to learn semantic information and employs a billion-parameter diffusion expert to learn robust and generalizable visuomotor policies. We introduce an embodied curriculum learning strategy, enabling the network to progressively learn from embodiment-agnostic motor skills to complex, embodiment-specific dexterous skills through three training stages.  Furthermore, we incorporate sub-step reasoning, allowing the model to perform very long-horizon tasks without relying on a high-level policy model. Our method is evaluated from multiple perspectives, including its ability to perform complex tasks without task-specific adaptation, fine-tune on new embodiments with limited data, and execute extremely complex, long-horizon tasks without the assistance of a high-level policy model.  Across all tasks, our method outperforms state-of-the-art approaches such as OpenVLA, Octo, and Diffusion Policy. We also conduct ablation studies to verify the effectiveness and efficacy of our proposed method.


\textbf{Limitations and future work.} Our method has several limitations and areas for improvement. First, we did not observe a significant transfer of learning from data collected on other embodiments to a specific target embodiment. Second, we observed that complex tasks, such as laundry folding, strongly depend on accurate action recovery and the ability to recognize the correct object state within the image. Prior work, $\pi_{0}$, addresses this by using SayCan, where a high-level policy model frequently assesses the object state and provides updated language instructions to the low-level vision-language-action model. Our current method, relying solely on the vision-language component, struggles with this aspect. A potential solution is to design a mechanism for explicit state checking and integrate it into our end-to-end model, which we plan to investigate in future work. Similar to observations reported in $\pi_{0}$, we also found that models trained with stage 3 exhibit reduced cross-task generalization compared to models trained solely through stages 1 and 2. However, complex tasks like laundry folding necessitate stage 3 training to effectively learn the required actions and object states for successful completion. Addressing this trade-off between task-specific learning and generalization is crucial for enabling models to perform complex tasks effectively without relying on post-training. 

\input{8_acknolwedgement}