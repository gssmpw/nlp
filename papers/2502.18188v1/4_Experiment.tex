
\section{Experiment}

\input{tables/tab_augmentation_GIN_all}
We organized the experiments on full-supervised node classification tasks with structure OOD to answer the following questions: 1) Does the proposed augmentation technique work within the SOTA GNN frameworks? 2) How does the proposed augmentation technique compare with typical graph augmentation methods regarding accuracy? 3) How does the proposed low-weight edge-dropping compare with random edge-dropping methods under different dropping rates? 4) How do different components impact the overall performance?

\input{tables/tab_citation_dataset}

\input{tables/tab_ab_study_edge}

\subsection{Experiment Settings}
In the experiment, we focus on node classification problems with distribution shifts existing among the structure of different citation networks. 
\paragraph{Dataset.}

The Citation benchmark we use comprises three real-world paper citation networks~\cite{shen2020adversarial}, namely ACMv9, Citationv1, and DBLPv7, where ACMv9 network consists of papers published after 2010 from ACM, Citationv1 network is collected from Microsoft Academic Graph with papers published before 2008, and DBLPv7 network contains papers published between 2004 and 2008 on DBLP. Each node corresponds to a paper in the citation networks, and edges signify the citation relationships.  Each node belongs to some of the five categories:  Computer Vision, Databases, Networking, Information Security, and Artificial Intelligence. Table \ref{tab:dataset_citation} has listed the detailed statistics of the citation network dataset.
% and Figure \ref{fig:dblp_all} is the visualization of the DBLP dataset.
For simplicity, we use A, C, and D to represent ACMv9, Citationv1, and DBLPv7, respectively. More details regarding experiments can be found in the appendix.
% \input{images/All_Accuracy_GCN}

\paragraph{Parameter settings.}
1) For low-weight edge dropping, without loss of generality, the sampling strategies use the \textit{threshold cutoff} (Eq.~\eqref{eq:threshold_cutoff_dorpedge}), and the hyper-parameter $\alpha,\rho$ is adjusted via the grid search strategy.
2) For spectral clustering-based edge adding,
The similarity matrix in spectral cluster employs the 
Gaussian kernel function RBF, i.e.
\begin{align}
    \S_{ij}=\exp(-\frac{\|\mbf x_i-\mbf x_j\|_2^2}{2\zeta^2}),
\end{align}
where $\zeta$ is a hyper-parameter that controls the distance flatness.
 The default number of clusters $\mcal K$ is set to $100$, which also can be adjusted during training. 
 We use the \textit{threshold cutoff} (Eq.~\eqref{eq:threshold_cutoff_dropadd}) strategy to generate the sampling probability and use the simple graph merge method (Eq. ~\eqref{eq:simple_new_graph}) to generate the new graph augmentation, which corresponds to the "low weight + clustering" in our experiments.
 The hyper-parameter $\beta, \epsilon, \rho$ is selected by the grid search strategy. 3) For model optimization, we use Adam optimizer with a learning rate of 0.001 for all experiments.



\paragraph{Evalutaion.} We adopt the leave-one-domain-out evaluation protocol in alignment with previous works\cite{xu2021fourier}, i.e. select one domain as the test domain and train the model on all other domains. Thus, three domain generalization tasks can be constructed: AC $\rightarrow$ D, AD $\rightarrow$ C, and CD $\rightarrow$ A.

As the citation networks used are multi-class data, we utilize Micro-F1 and Macro-F1 as the evaluation metrics to showcase the classification performance.
Micro-F1 and Macro-F1 are two  various average forms of the F1 score, a measure that emerges to take balance between the two significant but contradictory measures: precision and recall under the binary classification problem. 


The formula is as follows:
\begin{align}
    \text{F1}=\frac{2\times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}=\frac{2\times TP}{2\times TP+FP+FN},
    \label{eq:f1_score}
\end{align}
where, $TP$, $FP$, and $FN$ are the numbers of true positive samples, false positive samples and false negative samples. In multi-class classification problems, which also can be seen as a combination of multiple binary classifications, the Micro-F1 calculates the overall F1 score of these binary classifications by first averaging each item $\overline{TP}$, $\overline{FP}$, and $\overline{FN}$ in the above Eq.~\eqref{eq:f1_score}, while the Macro-F1 score averages the F1 score obtained from each binary classification problem. 
To put it simply, the Micro-F1 score emphasizes the performance of the classifier on individual samples; whereas the Macro-F1 score accentuates the classifier performance on each class irrespective of the number of instances it has.


\subsection{Comparison with SOTA }
In this part, we combine our proposed augmentation methods with different graph neural network frameworks to evaluate the effectiveness of our methods. We adopt the proposed augmentation techniques in GCN and GIN to compare with the classical models: GCN~\cite{kipf2017semi}, GIN~\cite{xu2018powerful}, GAT~\cite{velivckovicgraph}, SGC~\cite{wu2019simplifying}, and the results are shown in Table \ref{tab:citation_benchmark}. According to the results, we have the observation both GCN and GIN with the proposed augmentation technique exceed largely their counterparts and the other two frameworks. The significant performance improvement demonstrates the effectiveness of our augmented technique in tackling the graph domain generalization problems. This can be attributed to two aspects: (i) the augmentation technique increases the number of training samples; (ii) the proposed augmentation technique generates more samples out of the domain.


\subsection{Comparison with Various Augmentations }
In this experiment, we intend to evaluate the power of our proposed graph augmentation technique with other graph augmentation methods. Since we have not found related works tailored for cross-graph node classification with OOD structure, we adopt several typical graph augmentations, including random \textit{Edge Dropping}, random \textit{Edge Adding}, random \textit{Feature Masking} and random \textit{Feature Dropout}.
\vspace{-2.5mm}
\begin{itemize}
    \item \textbf{Edge Dropping.} The edge dropping is a widely used simple augmentation method that only randomly removes the existing edges in the graph with a certain proportion. 
    
    \item \textbf{Edge Adding.} The edge adding is to randomly add extra edges to the graph with a certain proportion.

    \item \textbf{Feature Masking.} Feature masking aims to set several columns of the feature attribute matrix to be zero randomly and use the remaining attribute for model learning.

    \item \textbf{Feature Dropout.} Feature dropout aims to set a part of the feature attribute to be zero randomly and use the remaining attribute for model learning.
\end{itemize}
\vspace{-2.5mm}
Notably, node-level augmentations cannot be used as baselines, because our task is full-supervised node classification. 

\paragraph{Results.} We report the comparisons with the above typical augmentations upon the GIN backbone in Table \ref{tab:tab:augmentation_GIN_all}. 
From the results, we can know that
our augmentation techniques: low weight dropping and low weight dropping + spectral clustering consistently outperform the considered augmentation methods in all three tasks, regarding the classification Micro F1 score and Macro F1 score, suggesting that our methods achieve start-of-the-art  performance among conventional augmentations.
The results also reveal that in addition to improving the diversity and quantity of training samples, the proposed method is capable of boosting the cross-graph node classification performance of the graph model by investigating domain-invariant information.




\subsection{Ablation Studies}
\paragraph{Comparison with Random Edge Dropping.} We construct ablation studies on the effect of different drop rates in the low-weight edge-dropping augmentation and meanwhile make comparisons with the most related augmentation method: random edge-dropping. The results are shown in Figure \ref{fig:all_acc_gin}.  
The results suggest that the low-wight edge dropping outperforms random edge dropping consistently. When the ratio of dropped edges is within a certain range, the performance increases as the ratio of dropped edges increases, indicating some edges in the original graph involve noise that may hurt the generalization of the learned GNN. Further, the low-weight edge dropping achieves better than random edge dropping  with respect to noise edge removal.

\paragraph{Effect of Different Components.}
We conduct ablation studies to evaluate the performance of the two proposed methods. The results are shown in Table ~\ref{tab:ab_study_edge}. From the table, we have the following observation: \textbf{Observation} (1): Joining low-weight edge dropping and spectral clustering-based edge adding achieves the best performance, suggesting the two strategies can work in tandem. \textbf{Observation} (2): Adding the clustering edges into the original graph without low weight dropping leads to poor performance, implying that the edges in the original graph may contain some edge that will destroy the invariant structure constructed by spectral clustering.
Moreover, when we only use the clustering edge as the graph structure, namely, removing all edges in the original graph, we get a competitive performance. This also suggests that the clustering edge can really improve the GNN's ability to perceive the global node feature distribution. 



% \subsubsection*{Results}



