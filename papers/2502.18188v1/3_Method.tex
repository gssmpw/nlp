
\subsection{Preliminaries}
\paragraph{Notations.}
Let $G(V,E,\A,\X)$ denote a graph with node set $V$, edge set $E$, adjacency matrix $\A \in \R^{|V|\times |V|}$ and node feature matrix $\X = \{\x_1;\x_2; \cdots,\x_{|V|}\} \in\R^{|V|\times d}$. We denote the Hadamard product by $\odot$. Besides, if not specified, we use boldface letter $\x\in \R^n$
 to indicate an $n$-dimensional vector, where $x_i$ is
the $i^\text{th}$ entry of $\x$. We use a boldface capital letter
$\mbf A\in\R^{m\times n}$ to denote an $m$ by $n$ matrix and use $A_{ij}$ to denote its ${ij}^\text{th}$ entry. 
\paragraph{Spectral Clustering.}
Spectral clustering in multivariate statistics involves using the eigenvectors of a matrix based on the similarity matrix (affinity matrix) to cluster data points. The technique conventionally requires computing the $k$ smallest eigenvalues and corresponding eigenvectors of the affinity matrix.
% This process is also known as spectral embedding. 
The eigenvectors are then used to form a new lower-dimensional space in which \textit{k-means} clustering or another clustering algorithm can be applied to separate the data points into clusters.

More specifically, the process of spectral clustering involves finding the $k$ smallest eigenvalues and corresponding eigenvectors of the normalized Laplacian matrix $\L_s = \D_s^{-1/2} \mbf S \D_s^{-1/2}$, where $\S$ is the affinity matrix (similarity matrix) and $\D_s$ is the diagonal matrix with element $\D_s(i,i)=\sum_j \S_{ij}$.
The the $k$ eigenvectors will be Concatenated together to form a matrix $\mbf U' = [\mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_k]$. Ultimately, the rows of $\mbf U'$, in which each row represents a data point, will be clustered by a specific clustering algorithm such as \textit{k-means}.

\iffalse
The reason that spectral clustering uses the eigenvectors corresponding to the smallest eigenvalues is due to the properties of the affinity matrix used in the process. The affinity matrix used in spectral clustering is generally sparse and contains noise, which can result in the smallest eigenvalues being more informative than the larger ones. In spectral clustering, these small eigenvalues and corresponding eigenvectors provide the most discriminating information for clustering.
\fi

\subsection{Graph Augmentation for Cross Graph DG}
In order to make the graph neural  network able to perceive the essential distribution of node features in the feature space, we propose a two-step strategy. First, remove the minor edges and leave the major ones. Intuitively, the removed edges contain potential noise that hinders the GNN  from capturing the node feature distribution while the left edges should mainly maintain the key structure of the graph that embeds the essential information of the data.

Second, add some edges by clustering. The remaining edges in the first step may  only be the partial edges of the key topology, which would be insufficient to reflect the node feature distribution.  Therefore, adding some crucial edges that can reflect the node feature distribution is a natural operation to improve invariant information learning in GNN.

Actually, the second step can be canceled if the edges remaining in the first step is good enough for GNN to perceive the essential information underlying the node features. 


\subsubsection{Low-Weights Edge Dropping} \label{subsec:low-weights}
This part proposes an edge-sampling strategy for sampling the core graph structure. The edge sampling is implemented by a low-weight edge-dropping operation. Intuitively, when dropping edges from a graph, we can use a Bernoulli distribution to determine whether each edge exists or not, based on some probability parameter $\rho$. For the whole graph, we need a compute a weight matrix to assign the sampling probability for each edge. 

\input{images/All_Accuracy_GIN}

\paragraph{Edge Weights Computation.}
The graph Laplacian is commonly used to represent graphs and drive the graph spectral convolution neural networks. 
The elements in the graph Laplacian matrix indicate the connection relations between the vertices in the corresponding graph. 
Specifically,
the Laplacian matrix denote as 
$\L  = \mbf I-\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}$, which can be rewritten as:
\begin{equation}
\L = \mbf I- \A \odot
 \begin{bmatrix} \frac{1}{\sqrt{d_1}\sqrt{d_1}} & \frac{1}{\sqrt{d_1}\sqrt{d_2}} & \cdots & \frac{1}{\sqrt{d_1}\sqrt{d_n}} \\ \frac{1}{\sqrt{d_2}\sqrt{d_1}} & \frac{1}{\sqrt{d_2}\sqrt{d_2}} & \cdots & \frac{1}{\sqrt{d_2}\sqrt{d_n}}\\
\ \vdots & \vdots & \ddots & \vdots 
\\ \frac{1}{\sqrt{d_n}\sqrt{d_1}} & \frac{1}{\sqrt{d_n}\sqrt{d_2}} & \cdots & \frac{1}{\sqrt{d_n}\sqrt{d_n}} \end{bmatrix}.
\end{equation}
The graph Laplacian can be understood from the Total Variation of the graph signal. Given the signal $x\in \R^{|V|}$ on graph $G$, its total variation can be expressed as follows:  

\begin{equation}
\label{eq:laplacian_total_variance}
\begin{aligned}
    x^{\top} \L x&=x^{\top}  x-x^{\top} \mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}} x 
    =\sum \limits _{i=1}^{|V|} x_{i}^{2}-\sum\limits_{i=1}^{|V|} \sum\limits_{j=1}^{|V|} \frac{1}{\sqrt{d_{i}d_j}}  x_{i} x_{j} \\
    &=\frac{1}{2}\left(\sum\limits_{i=1}^{|V|}  x_{i}^{2} -2 \sum\limits_{i=1}^{|V|} \sum\limits_{j=1}^{|V|} \frac{1}{\sqrt{d_{i} d_j}}  x_{i} x_{j}+\sum\limits_{j=1}^{|V|} x_{j}^{2}\right) \\
    % &=\frac{1}{2}\left(\sum\limits_{i=1}^{|V|} \sum\limits_{j=1}^{|V|} \frac{1}{{d_{j}}} x_{i}^{2}-2 \sum\limits_{i=1}^{|V|} \sum\limits_{j=1}^{|V|} \frac{1}{\sqrt{d_{i} d_j}} x_{i} x_{j}+\sum\limits_{i=1}^{|V|} \sum\limits_{j=1}^{|V|} \frac{1}{{d_{i}}}  x_{j}^{2}\right) \\
    &=\frac{1}{2} \sum\limits_{i=1}^{|V|} \sum\limits_{j=1}^{|V|}\left(\frac{x_{i}}{\sqrt{d_{i}}}-\frac{x_{j}}{\sqrt{d_{j}}}\right)^{2}.
\end{aligned}
\end{equation}

In particular, we consider using the Laplacian matrix to determine which edges should be removed from the original graph. 
From  the last line
 in Eq.~\eqref{eq:laplacian_total_variance}, we know the $\frac{1}{\sqrt{d_i}}$ can be viewed as the importance of the signal on node $i$. Assume that the edge connecting two significant nodes is more vital for representing the key structure of the graph. Thus the product of $\frac{1}{\sqrt{d_i}}$ and $\frac{1}{\sqrt{d_j}}$ can be used to measure the criticality of edge $e_{ij}$. Formally, the edge weight matrix is:
 \begin{align}
     \mbf P=\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}},
 \end{align}
 with elements $P_{ij} = \frac{1}{\sqrt{d_id_j}}$ if $A_{ij}>0$, otherwise $0$. The intuition behind it is that the edge is  crucial if its incident nodes have a low degree. In other words, the edges connected to low-degree nodes are more likely to affect the properties of the graph, such as connectivity. This is because, if a graph is disconnected, these low-degree nodes are often the first to be disconnected and thus can be an important indicator of the graph's connectivity.
 % For examp
 In addition, from the GCN perspective, the proposed edge weight matrix $\mbf P$ is actually the message aggregation matrix, thereby elements with relatively large values  in $\mbf P$  indicate the key process of message passing. Remaining the key message passing paths and removing the minor paths are likely to avoid aggregating noise that may hurt the generalization of GNNs. 


\paragraph{Adaptive Sampling Strategies.}
There exist many approaches that can be used to construct sampling strategies \cite{gao2021training} based on the edge weights matrix $\mbf P$. 
The principle of the sampling strategies is the larger-weight edges have a higher probability to remain because those undropped edges represent the key structure of the original graph.
\begin{itemize}
    \item \textbf{Threshold Cutoff:} We determine a threshold $\tau$ for edge weights based on their distribution and use it to decide whether to sample each graph edge with the default probability $\rho$ or to preserve it as an undropped critical edge. In particular, edges with weights below $\tau$ are sampled using the default probability, while those with weights above $\tau$  are critical and preserved to maintain the graph key structure. This can be formulated as:
    \begin{align}
    \label{eq:threshold_cutoff_dorpedge}
        P^{D}_{ij}=\left\{\begin{array}{cl}
	\rho, & \text{if } P_{ij}<\tau \text{ and } (v_i,v_j)\in E, \\
    1, & \text{if } P_{ij}\ge\tau \text{ and } (v_i,v_j)\in E, \\
	0, & \text{otherwise.} 
        \end{array}\right.
    \end{align}    
 
In practice, in order to control the edge dropping rate, we use the $k$th-smallest edge weight to determine the threshold $\tau$, i.e. $\tau = f_{min}(\mbf e^{D}, \alpha |E|)$, where $f_{min}(\cdot,k)$ is the $k$th-smallest value function, $\mbf e^{D} \in \R^{|E|}$ is the edge weight vector unfolded from $\mbf P^D$ and the $\alpha \in [0,1]$ is the edge dropping ratio. 
\item \textbf{Division Normalization:}
We employed a division function to normalize the edge weights to a range of $[0, 1]$, and subsequently used the normalized weights to determine the sampling matrix $\mbf P^D$. To calculate the probability of edge sampling given an edge weight of $\omega$ and a function parameter of $\tau$, we can apply the formulation $P^D_{ij} = 1 - (1 - P_{ij}) * \tau/(\tau + \omega)$.

\item \textbf{CDF Normalization:}
Our approach involves the normalization of edge weights, using the cumulative distribution function (CDF), which transforms them to a range of $[0, 1]$. Next, we determine the sampling matrix $\mbf P^D$ using the normalized weights. By utilizing our method's equation that requires the edge weight $\omega$ and its corresponding CDF $f(\omega)$, we can compute the probability of edge sampling, represented as $P^D_{ij} = P_{ij} + (1 - P_{ij}) * f(\omega)$.
    
\end{itemize}


 \paragraph{Low-weights Edge Dropping Augmentation.} 
The  $\mbf P^{D}$ can be used as the probability matrix in the Bernoulli distribution for generating the binary matrix. Specifically,  the augmenter is represented as:
\begin{align}
    \A' &= \A \odot \mbf M, \\
    \mbf M & \sim  \text{Bernoulli}(\mbf P^D),
\end{align}
where $M_{ij} \sim \text{Bernoulli}(P^D_{ij})$.
Eventually, the new graph can be led by the low-weight edge-dropping augmentation:
\begin{align}
G^{D}(V, E^D, \mbf A^D, \mbf X) = G(V, E, \A', \mbf X).
\end{align}
The augmenter would generate different augmentation $G^{D{(t)}}$ at each epoch $t$, which increases the diversity of the training data.   


Although the low-weight edge dropping remove the potential noise edge that may hurt the generalization of the learned GNNs, the remaining edges are not always adequate to reflect the whole invariant structure that benefits the OOD task. Inspired by this, we consider using a clustering-based method to add some edges into  $G^{D}$ for improving the GNN perceive the global node feature distribution (clustering prior), which is the invariant information on node-level tasks with OOD structure.


\input{tables/tab_citation_benchmark}
\subsubsection{Spectral Clustering Based Edge Adding}
As we all know, the node in the feature space has its own position. The nodes with similar features are usually closed in the feature space. In the node-level tasks with OOD structure, the node feature distribution is invariant (c.f. Figure \ref{fig:node_space})..
We consider generating the invariant structure from node features. A simple and effective approach is to  employ a clustering method to get the essential structure of the homogeneous graph. Using the cluster graph as the training data will promote the  capacity of the GNN to perceive the prototypes underlying the node features, thereby enhancing the learned GNN generalized ability.

\paragraph{Cluster Induced Graph Generation.}
Here we apply spectral clustering.
% \todo{Here we apply spectral clustering since it can filter out the high frequency of the  information}
Let the number of clusters be $\mcal K >1$. 
By spectral clustering of node features, we can obtain which cluster each node belongs to. For each cluster, we assign it a pseudo label $k ~(k=1,2,3,...,\mcal K)$.   Let the pseudo label of node $v_i$  denote as $\mcal C(v_i)$.
After getting the cluster label of each node, the nodes with  the same label can connect to each other, that is to say, each cluster will be transformed into a complete subgraph. 
We first use a matrix $\mbf B\in\R^{|V|\times \mcal K}$ to formulate the cluster information:
\begin{align}
\label{eq:incidence_matrix}
    B_{ij} = \left\{\begin{array}{cl}
	1, &\text{if } \mcal C(v_i) = j, \\
	0, & \text{otherwise.}
 \end{array}\right.
\end{align}
Then the adjacency matrix of the cluster-induced graph $G^G(V,E^G,\A^G,\X)$ can be expressed as:
% complete subgraph 
\begin{align}
    \A^G = \operatorname{sign}(\mathbf B \mbf B^{\top}),
\end{align}
where $sign(\cdot)$ signifies the symbolic function. A simple way to get the edge from $\A^G$ is randomly sampling edges in $G^G$. 

Alternatively, we can assign a weight matrix of edges to $G^G$ so that the sampling can be based on a meaningful probability, thus improving the interpretation of the edge-adding operation.

\paragraph{Edge Weights Computation for Edges Sampling.}
% Let us denote the  $G^C$ as $E^C$.
We can generate the edge weights based on the $\B$ in Eq.~\eqref{eq:incidence_matrix}.  Actually, the element in $\mbf B\mathbf{B}^{\top}$ is $\sum_k B_{ik}B_{jk}$. We can consider involving the number of nodes in each cluster. Assume that the fewer the number of nodes contained in a cluster, the more important the edges formed by the cluster. This is because those small clusters contain more critical topology information, such as connectivity. On the other hand, the big clusters are easily sampled due to the dense connections in the complete subgraph. Thus, define the number of nodes in cluster $j$ as  $\delta_j:= \sum_{i=1}^{\mcal |V|} B_{ij}$ and add it into the edge weight computation, we have 
$P^{G}_{ij} = \sum_k \frac{1}{\delta_k} B_{ik}B_{jk}$, which can be rewritten as matrix form $\mathbf B \mbf D_e^{-1} \mbf B^{\top}$. Formally, the edge weights in $G^G$ can be calculated as follows:
\begin{align}
    \mbf P^{G} = \mathbf B \mbf D_e^{-1} \mbf B^{\top},
\end{align}
where  
$\D_e\in\R^{|\mcal K|\times |\mcal K|}$ is a diagonal matrix with element  $D_e{(j,j)} =\delta_j$. 
The $\mbf P^{G}$ can be viewed as the transition matrix in the hypergraph random walk~\cite{zhang2022hypergraph} (each cluster is a hypergraph edge and $\mbf B$ is the incidence matrix) and the higher transition probability between two nodes means the edge connecting the nodes is more important.   
Next, based on the edge weights $\mbf P^{G}$, we can use the  sampling strategies as subsection ~\ref{subsec:low-weights}  to sample the edges. Taking \textit{threshold cutoff} as an example, given the probability $\epsilon, \rho$ where $\epsilon<\rho$, the sampling probability is:
\begin{align}
\label{eq:threshold_cutoff_dropadd}
    P^{G}_{ij}=\left\{\begin{array}{cl}
	\epsilon, & \text{if } P^G_{ij}<\tau \text{ and } \A^G_{ij} =1, \\
    \rho, & \text{if} P^G_{ij}\ge\tau \text{ and } \A^G_{ij}=1, \\
	0, & \text{otherwise.} 
        \end{array}\right.
\end{align}

\begin{align}
\label{eq:threshold_cutoff_dropad}
    P^{G}_{ij}=\left\{\begin{array}{cl}
    \rho, & \text{if } P^G_{ij}<\tau \text{ and } \A^G_{ij}=1, \\
	1, & \text{otherwise.} 
        \end{array}\right.
\end{align}
where the threshold $\tau$ is determined by the $k$th-largest value, which can be implemented by $k$th-smallest value function, i.e.
$\tau = -f_{min}(-\mbf e^{G}, \beta |E^G|)$, where $f_{min}(\cdot,k)$ is the $k$th-smallest value function, $\mbf e^{G} \in \R^{|E^G|}$ is the edge weight vector and the $\beta \in [0,1]$ is the edge adding ratio. 


The  $\mbf P^{G}$ can be used as the probability matrix in the Bernoulli distribution for generating the binary matrix. Specifically,  the augmenter is represented as:
\begin{align}
    \A'' &= \A^G \odot \mbf M,\\
    \mbf M  \sim & \text{Bernoulli}(\mbf P^G),
\end{align}
where $M_{ij} \sim \text{Bernoulli}(P^{G}_{ij})$.
Ultimately, the amended cluster-induced graph is:
\begin{align}
G^{C}(V^C, E^C, \mbf A^C, \mbf X^C) = G(V, E^G, \A'', \mbf X).
\end{align}
Note that the $G^{C}$ is a separate graph augmentation that can be directly applied to the domain generalization tasks. 


\paragraph{Edge Adding Augmentation.}

A simple way to add edges to $G^D$ is to select edges from $\A^C$ and merge them directly into $\A^D$, i.e.
\begin{align}
\label{eq:simple_new_graph}
    \A^{N} = \operatorname{sign}(\A^C + \A^D).
\end{align}
Alternatively, we also can use other mix-up~\cite{han2022g} methods to export a mix-up augmenter, formally,
\begin{align}
    \A^{N} = \operatorname{mix-up}(\A^C, \A^D),
\end{align}
where $\operatorname{mix-up}$ is mix up function. For example, if $\operatorname{mix-up}$  is a weighted sum function, we have $\A^{N} = \eta \A^C + (1-\eta) \A^D $.

\begin{remark}
Note that our method is a computationally efficient method. In practice, spectral clustering, edge weights matrix, as well as edge weights sorting in \textit{threshold cutoff} can be calculated before training. So our approach does not impose any additional computational burden during training and testing.
\end{remark}



\subsection{Overall Loss.} 
Given the source graphs $\{(G^S_i, \mbf Y_i)\}_{i=1}^{D}$,
% $\{G^S_i(V_i,E_i,\mbf A_i,\X_i)\}_{i=1}^{D}$
and one target graph $(G^T, \Y)$ with different structure distribution from source graphs
% $G^T(V,E,\mbf A,\X_i)$
, the optimized objective of the node classification with OOD structure is: 
\begin{equation}
\begin{aligned}
    \mathop{\arg\min}_{f} & \mcal L(f,\{G^S_i, \mbf Y_i\}_{i=1}^{D}) \\
    &= \mathop{\arg\min}_{f} \frac{1}{D}\sum_{i=1}^D \sum_{v\in G_i^S}  l(f(T(G^S_i))_v, \mbf Y_{iv}),
\end{aligned}
\end{equation}
where $T(\cdot)$ represents the augmenter, $f(\cdot)$ is a GNN that predicts the node class, and $l(\cdot)$ is the Cross-Entropy loss for classification or Binary Cross-Entropy loss for multi-class classification. 