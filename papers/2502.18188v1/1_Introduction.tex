\section{Introduction}
Recently, Graph Neural Networks (GNNs)~\cite{kipf2017semi} have become popular in performing machine learning tasks on graph-structured data, showing superior performance in various application domains, such as social network analysis~\cite{kipf2017semi},
recommendation systems~\cite{wu2022graph}, traffic prediction~\cite{jiang2022graph}, chemical molecules~\cite{yang2019analyzing,zhang2022fine}, and so on.
Despite the remarkable success, due to the heavy reliance on the I.I.D assumption that the training and testing data are independently drawn from an identical distribution\cite{pan2010survey}, GNNs are often brittle and susceptible to \textit{distribution shift}, which widely exists in the real-world scenarios. For example, social networks may differ when collected from different communities, molecules may have different structures, and transition networks change as time goes by. Graph domain generalization, aiming to improve the generalization performance under unseen distribution shifts has become a critical problem~\cite{li2022out}.

Due to the complexity of graph distribution shift type and the diversity of graph learning tasks, domain generalization on graphs have many settings, such as graph classification problem with structure shifts~\cite{chen2022invariance} and single graph node classification problem~\cite{yu2022finding}.
In this paper, we focus on the cross-graph node classification tasks with \textit{Out-Of-Distribution} (OOD) graph structures~\cite{wuhandling}.
Specifically, the graph model will  be trained firstly on several graphs in different domains  by node-level full supervised learning and directly predict the node labels of graphs from an unseen domain. Formally, suppose that the node feature distribution is $p(\mcal X)$, the node label distribution is $p(\mcal Y)$, and the graph structure distribution is $p(\mcal A| \mcal{E} )$, where $\mcal E$ is the environment variable and the cause of shift structure distribution can be considered to be
the environment difference.
Thus, the task can be formulated as follows.
\begin{align}
\label{eq:objective}
    \mathop{\min}_{f} \mathop{\max}_{\varDelta } \mbb{E}_{(\X,Y)\sim p(\mcal X,\mcal Y);\A \sim p(\mcal A| \mcal{E} = \varDelta)} \sum_{v\in V} l(f(G(\X, \A))_v, Y_v),
\end{align}
where  $p(\mcal X,\mcal Y)=p(\mcal X)p(\mcal Y | \mcal X)$, $f$ is a GNN with graph $G$ as input for predicting the class of nodes, and $Y_v$ represents the label of node $v$. $l$ denotes the classification loss function, such as cross-entropy.
The optimal $f$ that is able to well process the graph from new environment can be obtained via  optimizing  the objective Eq.~\eqref{eq:objective}. The task widely exists in the real world~\cite{wuhandling}.
For instance, in citation networks, the node features are in the same distribution, but each network formed by different relationships has its own structure distribution, and the model can be trained on  several citation networks to improve the domain generalization ability. 

In general, there exist three perspectives to handle domain generalization problems on graph data~\cite{li2022out}, including  model-level methods~\cite{li2021disentangled,li2022ood}, model-training-level methods~\cite{liu2022confidence,wu2022discovering} and data-level methods~\cite{rongdropedge,you2020graph}.  For cross-graph node classification with OOD structure, 
from the model-training perspective, \citet{wuhandling} 
proposed an invariant learning method EERM, which can be viewed as the extension of ERM~\cite{arjovsky2020invariant}. They focus on designing a new loss involving risk variance minimization to improve the generalization of the learned GNN. On the other hand, 
 data-level approaches like data augmentation, a simple and
easy-to-implement domain generalization technique, remains under-explored.


In this paper, from the data-level standpoint, we propose a new simple and effective graph augmentation  for node-level tasks with OOD structure. Our goal is to utilize data augmentation to expand the volume and diversity of training samples, and simultaneously enhance the ability of models to acquire invariant information between different environments. 
Based on the fact that the distribution shift is caused by the structure,  we first utilize an edge sampling method to boost the volume and variety of the training data. The edge sampling is implemented by a low-weight edge-dropping technique that will drop out some edges that are potential noise for the OOD tasks, leaving a subset of the edge that represents the essential structure of the original graph. 

Second, we need to explore the domain invariant information among the various structures.  Due to the same distribution of nodes (i.e. $p(\mcal X)$) in the structure OOD task, it is possible to use the Identity Distribution node attributes to capture the invariant structure information across graphs. Normally, the graph structure is commonly applied to reflect the relationship between nodes. Two nodes that are connected usually imply they are similar in the homogeneous graph, in other words, they are closed in the feature space. Hence, one promising augmentation operation is to generate new edges that mark the high similarity of connected nodes with initial features. Because the node features are sampled from the same feature space (Figure~\ref{fig:node_space} ), the added edge reflects the essential invariant topology of the node features, thereby amplifying the  capacity of the learned GNN's ability to perceive the prototypes underlying the node features. In addition, the generated essential topology may not focus on the domain-specific information, on the contrary, it maintains the domain-invariant information that can improve the generalization ability of GNN. 
The experiments
conducted on citation network datasets show that the proposed approach achieves the best performance
among all baselines, validating the effectiveness and generalization ability of our method.


In summary, the contributions of this paper can be highlighted as follows:
   (1) We propose a new edge-dropping method to remove the potential noise edges while retaining the key structure of the initial graph.
(2) A spectral clustering-based edge-adding strategy is developed to boost the learned model to perceive the global node clusters in the feature space, which is also the invariant information in the structure OOD datasets.
(3) The extensive experiments demonstrate the proposed augmentation achieves competitive performance in the cross-graph node classification and state-of-the-art performance among typical graph augmentations.

