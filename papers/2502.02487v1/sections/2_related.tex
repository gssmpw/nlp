\section{Related works}\label{sec:related_works}

\subsection{Egocentric Vision}
Egocentric vision captures human activities from the privileged perspective of the camera wearer, allowing a unique point of view on their actions~\cite{betancourt2015evolution,plizzari2024outlook}.
Recently, the field has seen rapid development thanks to the release of several large-scale egocentric vision datasets~\cite{ek55,egtea,epic_tent,ek100,ego4d,sener2022assembly101}.
The rich annotations of these datasets~\cite{ek100,ego4d} allow to tackle a large number of tasks, including action recognition~\cite{nunez2022egocentric}, action anticipation~\cite{furnari2020rulstm,girdhar2021anticipative,zhong2023anticipative}, next active object prediction~\cite{furnari2017next}, action segmentation~\cite{zhang2022actionformer,huang2020improving}, episodic memory~\cite{ramakrishnan2023spotem} and long-range temporal reasoning tasks~\cite{goletto2025amego,mangalam2023egoschema,jia2022egotaskqa}.
Previous works in egocentric vision have focused on domain adaptation~\cite{Munro_2020_CVPR,yang2022interact,chen2019temporal,plizzari2023can,planamente2024relative}, multimodal learning~\cite{zehua2033human,gao2020listen, yang2022interact} and large-scale video-language pretraining~\cite{lin2022egocentric,pramanick2023egovlpv2,hiervl,zhao2023learning} to learn better representation for downstream tasks.

\subsection{Graph Neural Networks for vision tasks}
Traditional neural networks, including Convolutional Neural Networks (CNNs), have been widely used in computer vision, showing impressive performance on a variety of problems~\cite{li2021survey,khan2020survey,gu2018recent}.
However, these models often assume data lying on a regular domain, such as images that have a grid-like structure.
In recent years, the interest in developing methods able to provide a more general and powerful type of processing has been growing and particular attention has been given to learning methods on graphs.
Graph Neural Networks (GNNs) have the innate ability to effectively handle data that lie on irregular domains, such as 3D data~\cite{simonovsky2017dynamic,wang2019dynamic}, robotics~\cite{pistilli2023graph}, molecular chemistry~\cite{kearnes2016molecular}, and social or financial networks~\cite{fan2019graph}, and to model complex data relations~\cite{sanchez2020learning}.
Recently, transformer-based architectures had a great impact on vision applications.
Despite Transformers and GNNs share some similarities in their ability to handle various data types, they are fundamentally different in their core architectures and the specific ways they process data. GNNs can model the topology of a graph and the relations between nodes while also inheriting all the desirable properties of classic convolutions: locality, hierarchical structures and efficient weights reuse.
In video understanding, GNNs have been applied to action localization~\cite{huang2020improving,zeng2019graph,ghosh2020stacked,rashid2020action}, to build a knowledge graph from human actions~\cite{ghosh2020all}, to model human-object interactions~\cite{dessalene2020egocentric, dessalene2021forecasting} or to build a topological map of the environment~\cite{nagarajan2020ego}.

\subsection{Multi-Task Learning}
MTL~\cite{caruana1997multitask, zhang2021survey} tackles the problem of learning to solve multiple tasks simultaneously.
The development of this strategy is justified by the intuition that complex settings require solving multiple tasks, for instance autonomous driving~\cite{Huang_2023_ICCV}, robotics and natural language processing.
Furthermore, these networks can bring the theoretical advantage of sharing complementary information to improve performance.
Several works have been done in this direction~\cite{kokkinos2017ubernet, huang2020mutual, fifty2021efficiently, chen2022unified, Chen_2023_ICCV, shi2023deep, Huang_2023_ICCV, ci2023unihcp}, focusing on which parameters or tasks is better to share~\cite{kang2011learning, guo2020learning, standley2020tasks, sun2020adashare} and promoting synergies between tasks~\cite{kapidis2019multitask, wang2021interactive}.
Such methods encounter the problem of negative transfer~\cite{kokkinos2017ubernet} and sharing with unrelated tasks~\cite{guo2020learning, standley2020tasks} consequently suffering of task competition and not being able to benefit from information sharing between tasks. To overcome these limitations, several methods have been proposed to balance task-related losses~\cite{kendall2018multi, chen2018gradnorm, sinha2018gradient}, to dynamically prioritize tasks~\cite{guo2018dynamic}, to reduce gradient interference between tasks~\cite{gradient_surgery} or to exploit task interactions at multiple scales~\cite{vandenhende2020mti}.
%
Unfortunately, all these solutions require extensive task-specific tuning, and are not able to build an holistic perception across tasks.

%
Few works have explored MTL in egocentric vision~\cite{kapidis2019multitask,egot2,huang2020mutual,egopack}.
Among these, EgoT2~\cite{egot2} is the first to investigate semantic affinities among high-level egocentric vision tasks and learns how to translate the contributions of different task-specific models to support the learning of a primary task.

\ourscvpr~\cite{egopack} stands as a fundamentally different paradigm with respect to traditional MTL approaches by building a backpack of \emph{task perspectives} to leverage when learning a novel task.
%
This work further enhances the uniqueness of \ourscvpr paradigm by extending the support to tasks with diverse temporal granularities. To do this, we introduce a novel graph-based architecture that foster hierarchical temporal reasoning.
