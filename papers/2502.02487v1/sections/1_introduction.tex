\section{Introduction}\label{sec:intro}
\IEEEPARstart{O}{ur}
daily activities are extremely complex and diverse, yet humans have the extraordinary ability to perceive, reason, and plan their actions almost entirely from visual inputs.
For instance, when observing someone at a kitchen counter with a pack of flour and a jug of water, we can infer they are kneading dough (\textit{reasoning about current activity}).
We might predict that their next step will involve mixing flour with water (\textit{reasoning about the future}) to obtain the dough (\textit{reasoning about implications}), maybe with the ultimate goal of preparing some bread (\textit{reasoning about long-range activities}).
Mastering such \quotes{skills} requires analyzing varying portions of the video and reasoning at different levels of granularity.
Long-term activities require analysis of a broader context over extended clips, while finer details, such as distinguishing when someone shifts from measuring flour to pouring water, call for reasoning at a frame level.
Such holistic reasoning, which is natural for humans, poses a significant challenge for artificial intelligence systems.
The difficulty lies in integrating various levels of reasoning, from low-level actions to high-level activity understanding, into a unified framework, while uncovering and leveraging the underlying semantic relationships between these skills to efficiently learn new ones by building on prior knowledge.

Current research trends in human activity understanding predominantly focus on creating several, hyper-specialized, models. This approach splits the understanding of human activities into distinct skills (\ie, tasks), for which each model is independently trained to rely only on \quotes{task-specific} clues for prediction~\cite{yan2022multiview,zhong2023anticipative,zhang2022actionformer}.
%
However, this approach overlooks that different tasks may share similar or complementary reasoning patterns, \ie, looking at the same video portion from different \textit{perspectives}.

To leverage the interplay between such different task perspectives, a first strategy might involve Multi-Task Learning (MTL), exploiting the intuition that knowledge sharing between tasks may be beneficial for each of them.
However, MTL suffers of some limitations~\cite{kokkinos2017ubernet}, mainly related to negative interferences between tasks, making it difficult to exploit task synergies effectively.
In addition, all task annotations must be available at training time, which hinders the extension of MTL models to novel tasks at a later point in time.

In the context of human behaviour understanding, usually inferred from videos collected in first person view, different tasks typically require closely related reasoning, resulting in a strong correlation between them. Consequently, studying and leveraging these inter-task synergies becomes particularly interesting.

\IEEEpubidadjcol
In this scenario, EgoT2 framework~\cite{egot2} represents an alternative solution to Multi-Task Learning, exploring how various egocentric video tasks can mutually benefit through the translation of task-specific cues across tasks.
However, although this approach fosters positive interactions between tasks, it has significant limitations: i) the primary task should be \quotes{known} at training time and present within the task-specific model collection, ii) it necessitates an extensive pretraining process and iii) it is inefficient as it relies on task-specific models instead of building transferable knowledge abstractions.

We argue that an important key to advancing the learning capabilities of intelligent systems and moving closer to more human-like reasoning lies not only in sharing information across tasks, but also in abstracting task-specific knowledge to make it reusable for learning novel tasks.
To enable this, we recently proposed \ourscvpr \cite{egopack}, a first effort in knowledge abstraction and sharing for egocentric videos understanding.
This method is able to exploit a set of known tasks (\emph{support tasks}), each one able to interpret an input stream according to its own task-perspective, to learn reusable knowledge abstractions that can aid in the learning of a \emph{novel task}. Such task-perspectives are encoded in the form of prototypes, collected in a single step from the pretraining of a multi-task network.
However, \ourscvpr implements limited temporal reasoning and, due to its flat architecture, cannot perform reasoning at different levels of granularity.
Notably, egocentric videos cover a wide range of tasks spanning diverse temporal scales, from sub-second actions to extended, long-range activities. While some tasks, such as action recognition and long-term anticipation, focus on fixed short segments, others, like temporal action localization, demand a more adaptive approach to deal with longer activities. As the temporal span of these tasks increases, developing a robust understanding of the sequential order of events, a concept known as \emph{sense of time}~\cite{bagad2023test}, becomes essential.

To address these challenges, we introduce \ours, an extension of \ourscvpr\cite{egopack}, specifically designed to maximize positive interaction across tasks with different temporal granularity, while still using a unified architecture and minimizing task-specific weights and tuning.
To achieve this, we present a hierarchical architecture that progressively learns more comprehensive representations of the input video, capturing both fine-grained details and broad contextual patterns.
A key aspect of this hierarchical design is effectively reasoning on temporal dependencies and consequentiality of actions, encompassing both past and future contexts.
To address this, we develop a novel GCN layer, hereinafter called Temporal Distance Gated Convolution (TDGC), specifically designed to encode these temporal relationships effectively.

We demonstrate the effectiveness and efficiency of our approach on \egofourd~\cite{ego4d}, a large-scale egocentric vision dataset.
To summarize, our main contributions are:
\begin{enumerate}
    \item We introduce a unified video understanding architecture to learn multiple egocentric vision tasks with different temporal granularity, while requiring minimal task-specific overhead;
    \item We present Temporal Distance Gated Convolution (TDGC), a novel GNN layer for egocentric vision tasks that require a strong \textit{sense of time};
    \item We extend \ourscvpr to the Moment Queries task, which involves the localization of activities that range from a few seconds to several minutes in duration;
    \item \ours achieves strong performance on five \egofourd~\cite{ego4d} benchmarks, using the same architecture and showing the importance of cross-task interaction.
\end{enumerate}
