\section{Conclusions}\label{sec:conclusions}
We presented \ours, an extension of \ourscvpr that enables knowledge sharing between egocentric vision tasks with different temporal granularity.
%
\ours is built on a unified temporal architecture that progressively learns more abstract representations of the input video, using a novel GNN layer specifically designed to incorporate strong temporal reasoning.
We evaluate our approach in a \emph{novel task learning} setting, in which a model is first trained on set of known \emph{support tasks} and then has to leverage the knowledge obtained from such tasks to improve the learning process of a \emph{novel task}.
%
We validate \ours on five \egofourd tasks, covering a wide range of temporal granularities, from sub-second actions to long-range activities. Results show the effectiveness of our approach in knowledge reuse, outperforming single-task and multiple-task baselines, as well as task translation approaches that seek to share knowledge across tasks but lack explicit knowledge abstraction.
% 
Our work emphasizes the importance of prior knowledge and task perspectives in learning novel tasks, focusing on how task-specific knowledge is represented and utilized. Furthermore, through our proposed unified video understanding architecture, we demonstrate that leveraging diverse task perspectives in egocentric vision, even across varying temporal granularities, leads to more comprehensive and human-like video understanding.