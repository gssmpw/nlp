\section{Experiments}\label{sec:experiments}
We first introduce in Sec.~\ref{sec:exp_setting} the tasks addressed in this work and the implementation details for our models and the \emph{Task-Translation} baseline in Sec.~\ref{sec:exp_impl_details}.
We report quantitative results for \ours in Sec.~\ref{sec:exp_quantitative}, evaluate different design choices in Sec.~\ref{sec:exp_ablations} and demonstrate the effectiveness of our approach on the test-set in Sec.~\ref{sec:exp_benchmarks}.
Finally, in Sec.~\ref{sec:exp_qualitative} we show qualitative results demonstrating the interaction process of \ours.

\subsection{Setting}\label{sec:exp_setting}
We validate our approach on \egofourd~\cite{ego4d}, a large scale dataset with 3.6k hours of egocentric videos capturing unscripted daily-life human activities, focusing on five \egofourd benchmarks that cover different temporal granularities.
\emph{Fine-grained tasks} focus on short-term understanding of the video, usually a few seconds long, and include:
%
\begin{itemize}
    \item \emph{Action Recognition (AR)}: given a video segment, predict the verb and noun action labels describing the interaction from a taxonomy of 115 and 478 verb and noun classes respectively. We report verb and noun top-1 accuracy.\footnote{This task is not an official \egofourd~\cite{ego4d} task and was initially introduced by EgoT2~\cite{egot2} using the LTA annotations.}
    \item \emph{Object State Change Classification (OSCC)}: given a video segment, predict the presence (or absence) of an object state change, \eg a glass being filled (transition from \textit{empty} to \textit{full}). We report accuracy.
    \item \emph{Point of No Return (PNR)}: given a video segment containing an object state change, predict the temporal frame when the change happens. Predictions are evaluated using the absolute temporal distance from the ground truth.
    \item \emph{Long Term Anticipation (LTA)}: given a video segment, predict the sequence of Z future actions (verb and noun label pairs) the camera wearer is likely to perform next. Performance is measured in terms of verbs and nouns Edit Distance (ED) between the predicted sequence and the ground truth, for the best sequence out of K predictions. In \egofourd, $Z=20$ and $K=5$.
\end{itemize}
Other tasks may require both short and long term understanding of the input video.
Among these, we analyze the \emph{Moment Queries (MQ)} task, which requires predicting the set of activities performed in the video among 110 labels with the corresponding start and end timestamps.
For all tasks, we use the version \textit{v1} of the annotations.

\subsection{Implementation Details}\label{sec:exp_impl_details}
\ours is built using pre-extracted features from fixed-size video segments. In all experiments the backbone used for feature extraction is kept frozen.
%
We use EgoVLP features pretrained on EgoClip~\cite{lin2022egocentric} and extracted using a window of 16 consecutive frames with an equivalent stride. EgoVLP features have size 256.
For comparison with \ourscvpr in Table~\ref{tab:cvpr}, we use Omnivore Video Swin-L~\cite{omnivore} features pre-trained on Kinetics-400~\cite{quo_vadis}, released as part of \egofourd~\cite{ego4d} and extracted using dense sampling over a window of 32 frames with a stride of 16 frames and features size 1536.
In principle, \ours is agnostic to the features extractor and could adopt other architectures.
%
We train all the single, multi-task and \ours models for 15 epochs, using the Adam optimizer. Learning rate is set to $1\mathrm{e}{-4}$ for all tasks, with the exception of the OSCC and PNR tasks which use $1\mathrm{e}{-5}$, and follow a cosine annealing schedule with a linear warmup of 5 epochs.
We repeat our experiments three times with different random seed and report the average performance.
%
All tasks share the same temporal and cross-task interaction architecture, with minimal task-specific hyper-parameter tuning.
The task prototypes are built using samples from the train split of the AR~task.

\input{tables/egopack_all_tasks.tex}

\subsubsection{Task-specific design choices.}\label{ts-design}
\ourscvpr constructs the input graph differently based on the task, \ie each action or sub-segment is mapped to a different node in AR or OSCC respectively, which may result in inconsistencies in how segments with different temporal granularities are processed by the temporal backbone.
On the contrary, with \ours we standardize the graph construction process for all tasks.
Specifically, features from fixed-length segments are extracted densely from the entire video and each segment is mapped to a node of the graph.
Temporal reasoning is performed on these \textit{dense} temporal graphs, followed by a task-specific projection~$\mathcal{N}_k$ and an optional alignment step.
Depending on the temporal granularity of the downstream task, we take the output processed graphs of the temporal model after the first stage $\mathcal{G}^{(1)}$ (\textit{fine-grained tasks}) or from all the stages $\{\mathcal{G}^{(1)}, \mathcal{G}^{(2)}, \dots, \mathcal{G}^{(L)}\}$ (\textit{variable-resolution tasks}).
%
For tasks in which temporal boundaries are known, features within the boundaries are averaged to obtain a single instance-level embedding as input for the task-specific neck. With the exception of MQ, all tasks fall into this category.
%
For MQ, we predict an action for each segment in the input video and use Non-Maximum Suppression (NMS) to filter predictions, consistently with previous approaches. For NMS, we use the same configuration as ActionFormer~\cite{zhang2022actionformer} and set the $\sigma$ parameter to $2.0$, which was empirically found to reduce the penalty on \textit{near-replicate} predictions~\cite{sui2023nmsthresholdmattersego4d}. Therefore, no specific alignment is needed for this task.

Task-specific necks are implemented as two-layers MLPs. The heads are also implemented as multi-layer projections that map to the output space of the task, with the exception of the LTA task. In this case, we first build \textit{on-the-fly} a graph with $K$ nodes initialized to the output of the temporal model, where K is the number of future actions to predict. We then process this graph with a two layers TDGC, before feeding the features to the verb and noun classifiers.

AR, OSCC and LTA are trained with standard cross entropy loss, while PNR uses binary cross entropy. The classification and regression heads of the MQ task are trained with the focal~\cite{ross2017focal} and DIoU~\cite{zheng2020distance} losses respectively, following the same protocol as ActionFormer~\cite{zhang2022actionformer} to match predictions at different scales with their temporally closest ground truth.

\subsubsection{Task-Translation baseline implementation.}
Due to the differences in the network architecture and training data employed, a comparison between \ours and EgoT2~\cite{egot2} is not straightforward.
Indeed, EgoT2's Single Task are based on SlowFast~\cite{slowfast} for AR and LTA, I3D ResNet-50~\cite{carreira2017quo} for OSCC and PNR and VSGN~\cite{zhao2021video} for MQ.
These models are end-to-end trained on the benchmarks' data, unlike \ours which relies on pre-extracted features and does not train the video feature extractor.
Therefore, we introduce a comparable baseline, which we call \emph{Task Translation}, by adapting the cross-task translation mechanism of EgoT2 to our setting.
As in EgoT2s, \emph{Task Translation} learns a transformer encoder on top of the Single Task models to combine the perspective of the different tasks.
Furthermore, EgoT2 supports only tasks with homogeneous temporal granularity.
With \emph{Task Translation}, we extend the translation mechanism to support tasks with different temporal granularities and include in this analysis the same tasks as \ours.

Formally, \emph{Task Translation} combines a set of $K$ Single Task models trained independently.
Each Single Task model outputs a sequence of $N_k$ task-specific tokens $\mathbf{F}_{k} = [\mathbf{f}^{1}_k, \mathbf{f}^{2}_k, \dots, \mathbf{f}^{N_k}_{k}]$ with $\mathbf{f}^{i}_{k} \in \mathbb{R}^{D}$, along with the position attribute $\mathbf{pe}_{k}\in\mathbb{R}^{N_k}$, as defined in Sec.~\ref{sec:method}.
Task-specific tokens and the position attribute are concatenated on the sequence dimension to obtain the full features $\mathbf{F} \in \mathbb{R}^{N \times D}$ and position attribute $\mathbf{pe} \in \mathbb{R}^{N}$, where $N$ is the total number of tokens across all the tasks. We define the \emph{Task Translation} operation as $\tilde{\mathbf{F}} = \mathtt{ENC}(\mathbf{F}, \mathbf{A})$, where $\mathbf{A}$ is a binary attention mask defined as:
\begin{equation}
    \mathbf{A}_{[ij]} = \begin{cases}
        1 & \quad\text{if}\;\left| \mathbf{pe}_{[i]} - \mathbf{pe}_{[j]} \right| \le 2^{l} \\
        0 & \quad\text{otherwise}                                                          \\
    \end{cases},
\end{equation}
where $l$ is the index of the stage in the hierarchical backbone that produced features $\mathbf{f}_i$. The mask restricts the self-attention operation to tokens that are within the same temporal window.
We parameterize $\mathtt{ENC}$ as a transformer encoder with $l$ layers and $h$ attention heads, with the same output size as the input features.
Finally, we take the slice of the transformer output $\tilde{\mathbf{F}}$ corresponding to the features of the primary task and forward them through the task-specific~head.


\subsection{Quantitative results}\label{sec:exp_quantitative}
We show the main results of \ours in Table~\ref{tab:main_results}, comparing our approach with the \egofourd baselines~\cite{ego4d}, the task-translation framework EgoT2~\cite{egot2} and the previous iteration of our work \ourscvpr~\cite{egopack}.

%
We proceed incrementally from the \emph{Single Task} models, \ie each task is trained separately using our GNN-based hierarchical architecture.
%
Conversely, \egofourd baselines and EgoT2 use SlowFast~\cite{slowfast} for AR and LTA, I3D ResNet-50~\cite{carreira2017quo} for OSCC and PNR and VSGN~\cite{zhao2021video} for MQ, with different configuration and hyper-parameters for each task.
In contrast, our \emph{Single Task} models employ the same architecture and a pair of task-specific neck and head.
\emph{Multi-Task Learning} (MTL) baselines are built following the same approach, \ie sharing the same architecture across all the tasks.
In this setting, we observe suboptimal performance in some tasks, particularly in AR (Verb), OSCC, and MQ. We attribute this to potential negative transfer effects.
%
We also consider a \emph{MTL+FT} baseline in which the MTL model is finetuned on the novel task, and  \emph{MTL+HT} which takes the frozen temporal backbone from the MTL training and learns new task-specific neck~$\mathcal{N}_K$ and head~$\mathcal{H}_K$ for the novel task.
These baselines exhibit comparable performance to the \emph{Single Task} models, showing that fine-tuning multi-task models is not the ideal approach to transfer knowledge across tasks as it does not explicitly exploit the semantic similarities and perspectives offered by different tasks.

\subsubsection{Task-Translation baseline results.}
\emph{Task-Translation} shows consistent improvements compared to both Single Task and Multi-Task models, with the sole exception of AR.
These results prove the effectiveness of the cross-task translation mechanism and show that different tasks learn representations that are partially complimentary to each other.
However, we remark the \emph{Task-Translation} mechanism is inefficient by design as it requires different models for each supported task.
Each single task model in the ensemble looks at a different perspective for the same input, without explicitly recalling the entire knowledge gained by the models.
In contrast, the task prototypes in \ours provide a comprehensive and easy-to-access abstraction of the model’s learned knowledge, enabling the extraction of relevant insights tailored to the specific sample and task.


\subsubsection{Comparison with \ourscvpr}\label{sec:comparisonCVPR}
\input{tables/egopack_cvpr.tex}
We compare \ours with our previous iteration \ourscvpr~\cite{egopack} in Table~\ref{tab:cvpr}, using the same \emph{fine-grained} tasks, \ie AR, OSCC, LTA and PNR, and same pre-extracted features (Omnivore).
\ours, thanks to its novel GNN layer with strong temporal reasoning, has on average better performance compared to the Single Task models from the original \ourscvpr.


\subsection{Ablations}
\label{sec:exp_ablations}
\input{tables/temp_gnn/ablations.tex}
\input{tables/temp_gnn/ablations_gnn_mAP.tex}
We evaluate different design choices for the hierarchical temporal backbone in Tables~\ref{tab:temp_gnn_ablations} and~\ref{tab:temp_gnn_mAP}, focusing on the Moment Queries (MQ) task which requires temporal reasoning at multiple granularities, thus exploiting the hierarchical architecture in its entirety.
%
%

\smallskip
\smallskip
\noindent\textbf{Number of GNN layers.}
The number of convolutional layers at each stage has a mild impact on performance, as it tends to saturate after two layers (Table~\ref{tab:temp_gnn_ablations}-left).
Increasing the number of layers expands the receptive field at each stage, a goal already achieved by our pooling and hierarchical aggregation steps.
Consequently, adding more layers appears redundant given the model’s hierarchical structure.
%
%


\smallskip
\smallskip
\noindent\textbf{Pooling strategy.} We evaluate different approaches to \textit{reduce the temporal resolution of graph nodes} between subsequent layers of the temporal model (Table~\ref{tab:temp_gnn_ablations}-middle).
The \textit{batch} strategy selects alternate nodes from the batch, without considering video boundaries, which results in some noise in the node selection process.
Differently, \textit{video} selects alternate nodes from each video separately.
The \textit{mean} and \textit{max} strategies pool features from all the neighbors of each node, corresponding to past and future segments.
On the MQ task, we observe a noticeable gap between the first two strategies that drop half the nodes and the \textit{mean} and \textit{max} strategies which operate on the neighbors of each node and can better forward task-relevant information to the next layers.
%
%

\smallskip
\smallskip
\noindent\textbf{Temporal threshold.} The $\tau$ parameter controls the number of neighbors at each node in the temporal graph, as we consider the existence of an edge  $e_{ij}$ between two nodes $i$ and $j$ only if their relative temporal distance is less than the threshold $\tau$.
We observe that small values of $\tau$ are sufficient and performance deteriorates quickly with larger values, especially in terms of recall (Table~\ref{tab:temp_gnn_ablations}-right).
Also, the use of a smaller neighborhood is compensated by the hierarchical nature of our temporal backbone.

\smallskip
\smallskip
\noindent\textbf{GNN layer.}
In Table~\ref{tab:temp_gnn_mAP}, we analyze the impact of different GNN layers on MQ performance.
At each layer, the neighborhood of a node is the set of nodes within a fixed relative distance $\tau$ from the root node, which makes the GNN operate on local temporal segments of the video.
We evaluate two different approaches: i) permutation invariant~(PI) layers, which ignore the local temporal ordering of the nodes in the neighborhood, and ii) layers that explicitly incorporate temporal grounding, \ie node ordering, into their processing.
Both strategies achieve reasonable performance.
However, the absence of temporal ordering in the approaches from the first group prevents them from properly aggregating past and future nodes, resulting in subpar performance compared to strategies that include temporal grounding.

We evaluate different strategies to add temporal grounding to the GNN layers.
The simplest approach, \emph{SAGE + PE}, adds an absolute positional encoding to the node embeddings of the input graph.
This method, already used by \ourscvpr, works well in tasks that do not require strong temporal reasoning. Despite its simplicity, it outperforms all PI approaches, underscoring the significance of precise node ordering for more \textit{temporal-aware} tasks, such as the MQ.
A more advanced strategy is SGCN~\cite{8594922}, which extends GCN by using different projections for the node embeddings corresponding to past and future segments in the neighborhood.
To design an effective GNN layer for diverse video understanding tasks, we focus on two key temporal reasoning principles: (i) the ability to distinguish between past and future nodes in the aggregation phase and (ii) the relevance of each node should depend on its relative temporal distance.
SGCN addresses the first point but does not consider the relative temporal distance of the nodes, giving the same importance to close and distant nodes. Also, past and future node embeddings are projected differently despite possibly encoding the same event.
Our intuition is that the relative temporal distance should not affect the semantic content of the nodes, and therefore their projection, but only how nodes are combined in the aggregation phase.
By using our TDGC layer we adopt the same projection for all nodes and encode the temporal distance between the nodes in the aggregation step.

To analyze the impact of the aforementioned key temporal reasoning principles, Table~\ref{tab:temp_gnn_mAP} also presents an ablation study on the design choices for our TDGC. The results clearly show a significant performance drop when the $s_{ij}$ coefficients are removed, as this prevents distinguishing between past and future nodes during aggregation.
Similarly, omitting the relative position attributes $\mathbf{w}_{ij}$, which differentiates between temporally close and distant nodes, results in suboptimal performance in the downstream MQ task.
%

\input{figures/activations.tex}
\input{figures/consensus.tex}

\subsection{Benchmarks}\label{sec:exp_benchmarks}
We compare \ours on the test set of MQ and LTA benchmarks, to validate the improvements and soundness of our approach.
In this setting, a fair comparison between methods is challenging because of the use of different backbones or feature extractors, supervision levels, ensemble strategies, and challenge-specific tuning, such as training also on the validation set.

\smallskip
\smallskip
\noindent\textbf{Moment Queries (MQ).}
We compare different approaches using EgoVLP features and with the official \egofourd baseline in Table~\ref{tab:test_mq}.
VSGN~\cite{zhao2021video} is a two-stages method featuring a pyramid network to exploit cross-scale correlations in the input video.
ActionFormer~\cite{zhang2022actionformer} is a single-stage method that combines a multi-scale transformer encoder with a lightweight convolutional decoder.
ASL~\cite{shao2023action} extends ActionFormer by reweighting the predictions based on their distance from the corresponding ground truth segment.
ASL is a much larger model in terms of trainable parameters than \ours (350.7 vs. 37.1 MParams) and the test-set results are obtained with an ensemble of three models, each trained with different hyperparameters on the combination of the training and validation splits. We include this model in our analysis because of its relevance and use of EgoVLP features, although it is not directly comparable with the other approaches.
\input{tables/challenges/mq}
In particular, \ours significantly outperforms VSGN and ActionFormer, despite having a generic architecture not specifically designed for the task.

\smallskip
\smallskip
\noindent\textbf{Long Term Anticipation (LTA).}
We compare different approaches for the LTA task in Table~\ref{tab:test_lta_oscc}.
In particular, we distinguish between \emph{vision-based} and \emph{LLM-based} approaches, with the former relying only on visual reasoning and the latter integrating LLMs into their pipeline.
\ours achieves SOTA performance on the \emph{noun} and \emph{action} metrics in the \emph{vision-based} category, with similar performance compared to \ourscvpr on the \emph{verb} metric.
\input{tables/challenges/oscc_lta.tex}

\subsection{Qualitative results}
\label{sec:exp_qualitative}
In this section, we analyze how \ours leverages knowledge abstractions from the \emph{support tasks} (collected in the form of prototypes) to aid the learning of a \emph{novel task}.
Specifically, we visualize the \emph{activated prototypes} (\ie the set of prototypes each \emph{support task} looks at) during the interaction process of \ours across different novel tasks and quantify task activation consensus, a measure of the complementarity among support tasks in aiding the learning of a novel task.

\smallskip
\smallskip
\noindent\textbf{Prototypes activations.}
We show in Fig.~\ref{fig:activations} the activation frequency for the task-specific prototypes for a subset of \emph{novel tasks}, considering the Top-20 most activated prototypes.
Due to the large number of prototypes, we aggregate them based on their verb labels to enhance the readability of the plots.
Some tasks, \ie OSCC and LTA, also show more similar activations frequencies for the prototypes corresponding to the same label while Moment Queries have a much larger variability in prototypes activations.

\smallskip
\smallskip
\noindent\textbf{Activations consensus.}
The goal of this analysis is to showcase how a \emph{novel task} can leverage the perspectives from a set of \emph{support tasks}, reusing previously learned knowledge stored in the form of prototypes.
To this end, we expect \ours to extract complementary cues from each \emph{support task}.
We define the \emph{activations consensus} as the degree to which different tasks activate prototypes corresponding to the same label for a given sample of the \emph{novel task}.
A low consensus suggests that the support tasks capture more diverse cues, \ie different tasks activate different prototypes, whereas a high consensus indicates that activations are more coherent across tasks.
We show in Fig.~\ref{fig:consensus} the average activation consensus for different novel tasks.
Fine-grained tasks, \eg AR, LTA and OSCC, have higher average consensus compared to MQ.
We attribute this difference to the implementation of the interaction process for these two groups of tasks.
In fine-grained tasks, the interaction process is applied on the sample-level aligned features. On the contrary, for MQ the interaction is applied to node-level features, without any alignment due to the nature of the task, as previously stated in Sec.~\ref{ts-design}.
%
Therefore, a substantially higher number of nodes per video interact with the task-specific prototypes.
These nodes may correspond to background regions of the video or to segments of an activity that are insufficiently discriminating.
The low average activations consensus (Fig.~\ref{fig:consensus}) and high diversity in prototypes' activations across tasks (Fig.~\ref{fig:activations}) show how \ours is effectively integrating different perspectives for the Moment Queries task.


\smallskip
\smallskip
\noindent\textbf{Activation frequency.}
\input{figures/mq_act.tex}
We show in Fig.~\ref{fig:mq_activations} the most activated prototypes for different \emph{support tasks} when the \emph{novel task} is MQ.
To enhance readability, we select the 50 most predicted labels and 50 most activated prototypes.
Overall, we observe that the activations of the AR task are quite sparse, indicating that the novel task looks at very different perspectives from these tasks.
%
On the contrary, the activations of the OSCC task are more uniform across different MQ labels. This is because these tasks focus on detecting object state changes in the video, which are typically associated with a subset of specific actions, such as \emph{cut} or \emph{mix}.
As a result, only a subset of prototypes from these \emph{support tasks} is actually activated by the novel task, as can be seen from the stripes in the plots.
%

\input{figures/oscc_act.tex}
Similarly, we show in Fig.~\ref{fig:oscc_activations} the most activated prototypes when the \emph{novel task} is OSCC.
We consider separately correctly predicted segments that contain an object state change (\emph{positive}) or not (\emph{negative}).
Positive samples tend to focus more on prototypes whose verb could be associated with an object state change, \eg \emph{take} or \emph{put}, compared to negative samples.
