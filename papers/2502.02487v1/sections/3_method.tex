\section{Method}\label{sec:method}

\begin{figure}[t]
    \centering
    \includegraphics[trim=0 0.25cm 0 0,width=.95\columnwidth]{figures/temp_arch.pdf}
    \caption{
        \textbf{Overview of the \ours architecture.}
        First, the video is converted into a graph representation $\mathcal{G}^{(0)}$ whose node embeddings are extracted using a frozen video features extractor.
        The graph is then processed by the \emph{hierarchical temporal backbone} $\mathcal{M}_t$, shared by all the tasks, to progressively learn higher level representations of the input video~$\{\mathcal{G}^{(1)}, \mathcal{G}^{(2)}, \dots, \mathcal{G}^{(L)}\}$.
        The node embeddings of these graphs are projected by the \emph{task-specific necks}~$\mathcal{N}_i$ in the features space of each task~$\mathcal{T}_i$ and to the corresponding output space with the \emph{task-specific heads}~$\mathcal{H}_i$.
    }\label{fig:architecture}
\end{figure}

We address a cross-task interaction setting, in which an egocentric vision model is trained to reuse previously acquired knowledge from a set of different tasks (\emph{support tasks}) to foster the learning process of any \emph{novel task}.
%
A formal definition of the proposed setting is presented in Sec.~\ref{sec:method_setting}.
% 
This work introduces a unified temporal architecture to model tasks with different temporal granularity and strong \emph{sense of time}, \ie the ability to effectively reason on the order of the events in a video.
%
With this new architecture, we extend \ourscvpr to tasks that require long range temporal reasoning, \eg Temporal Action Localization.
%
We call this approach \ours, emphasizing its ability to learn hierarchical video representations that are well suited to various egocentric vision tasks.

\subsection{Setting: novel task learning}\label{sec:method_setting}
A task~$\mathcal{T}$ in egocentric vision is defined as a mapping between a video~$\mathcal{V}$ and an output space~$\mathcal{Y}$.
%
Classification tasks, such as Action Recognition, are defined as a mapping between a video segment~$v_i \in \mathcal{V}$ and the corresponding discrete label~$y_i \in \mathcal{Y}$. For these tasks, the start and end timestamps of the video segment $v_i$ are known.
% 
Differently, the Temporal Action Localization (TAL) task processes the entire video~$\mathcal{V}$ and predicts a set of temporally grounded activities, each described by its start and end timestamps and the corresponding action label: $$\mathcal{T}: \mathcal{V} \to \{(t_i^s, t_i^e, y_i)\}_i.$$
%
We streamline the processing for different tasks by feeding our temporal backbone with untrimmed input videos and aligning the output to the downstream task at a later stage.
This alignment process is described more in depth in~Sec.~\ref{sec:method_ts}.

%
The cross-task interaction mechanism of \ours follows a two-stages training process.
First, a model $\mathcal{M}$ is trained on a set of $K$ tasks $\{\mathcal{T}_1,\dots,\mathcal{T}_K\}$, which we call \emph{support tasks}, in a Multi-Task Learning setting with hard-parameter sharing~\cite{ruder2017overview}.
The inclusion of multiple tasks in this phase encourages the model to learn more general and task-agnostic representations.
%
Then, the model is presented with a \emph{novel task} $\mathcal{T}_{K+1}$ to learn, without access to the supervision of the \emph{support tasks}.
In this scenario, the novel task can benefit from semantic affinities with the previously seen tasks.
For example, a model that has learned to detect object state changes may apply this knowledge for action recognition and vice-versa, as some actions produce object state changes, \eg \emph{cutting something}, while others do not, \eg \emph{moving an object}.
Our goal is to make these semantic affinities more explicit and exploitable, enabling the novel task to re-purpose these \emph{perspectives} from previous tasks to enhance performance,
a necessary step towards more holistic models that seamlessly share knowledge between~tasks.



\subsection{A unified architecture for Video Understanding}\label{sec:method_arch}
Egocentric vision tasks may provide complementary perspectives but also operate at different temporal granularities, from sub-second interactions to minutes-long activities.
To support all these tasks with a unified architecture, we need a model that can perform temporal reasoning hierarchically, progressively integrating fine-grained temporal representations into a broader and more comprehensive understanding.


Also, reasoning over long temporal horizons requires the ability to precisely ground and order past and future events.
The temporal backbone introduced in \ourscvpr~\cite{egopack} partially meets these constraints: while it supports multiple tasks with a shared architecture, it assumes similar temporal granularity across tasks and lacks a robust \emph{sense of time}, as detailed in Sec.~\ref{sec:exp_ablations}.
%
Indeed, the SAGE GNN convolutional operator used in \ourscvpr is invariant to permutations of the input nodes, and temporal ordering of the nodes is only provided by adding a positional encoding to the node embeddings.
This strategy is insufficient for tasks that require strong temporal reasoning, as we show in Sec.~\ref{sec:exp_ablations}.


We address these challenges by proposing a newly crafted hierarchical GNN-based architecture, specifically designed to support tasks with variable temporal resolution.
At the core we place a novel \textbf{Temporal Distance Gated Convolution (TDGC)} layer, able to explicitly encode past and future information, and a temporal sub-sampling operation that progressively computes a coarsened representation of the input video.
% 
Starting with high resolution input video features, our architecture progressively aggregates the input on the temporal axis, moving from a local view of the video to a more high-level representation, as shown in Fig.~\ref{fig:architecture}.
%
We refer to this architecture as \ours, as it extends \ourscvpr to deal with different time granularities thanks to its hierarchical processing.

\subsubsection{Representing videos as graphs.}
%
A video $\mathcal{V}$ can be seen as a dense sequence of $N$ fixed-length temporal segments encoded as $\mathbf{x} = \{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N \}$, where $\mathbf{x}_i \in \mathbb{R}^D$ represents the features of the corresponding segment $v_i$ computed using a video features extractor $\mathcal{F}$, \eg EgoVLP~\cite{lin2022egocentric}.
%
The video can be interpreted as a graph~$\mathcal{G}$:
\begin{equation}
    \mathcal{G} = (\mathbf{X}, \mathcal{E}, \mathbf{pe})
\end{equation}
where $\mathbf{X} \in \mathbb{R}^{N \times D}$ is a matrix encoding the features of the graph nodes in its rows,
edge~$e_{ij} \in \mathcal{E}$ connects nodes~$i$ and~$j$ with a temporal distance considered relevant when lower than a threshold $\tau$ and the attribute $\mathbf{pe} \in \mathbb{R}^{N}$ encodes the \emph{timestamp} (in seconds).
% 
Encoding videos as graphs enables the use of graph neural networks to learn the complex temporal relations between video segments and to cast different egocentric vision tasks as operations on these graphs. The proposed architecture is built on three components:
\begin{enumerate}
    \item a \emph{temporal} backbone $\mathcal{M}_{t}$, which uses a stack of TDGC layers and subsampling operations to implement hierarchical temporal reasoning;
    \item a set of \emph{task-specific projection necks}~$\mathcal{N}_k$ mapping the node embeddings to the features space of task $\mathcal{T}_k$;
    \item a set of \emph{task-specific heads}~$\mathcal{H}_k$ that map to the output space of each task.
\end{enumerate}
Let~$\mathcal{G}^{(0)}$ represent the initial graph of the input video~$\mathcal{V}$, where each node's position $\mathbf{pe}$ is initialized to the midpoint of the corresponding video segment.
%
At each stage~$l$, the \emph{temporal} backbone $\mathcal{M}_{t}$ performs temporal aggregation on the input graph $\mathcal{G}^{(l)}$ and outputs an updated graph $\mathcal{G}^{(l+1)}$. This is done using a sequence of TDGC layers and temporal subsampling operations to progressively enlarge the temporal extent of the nodes
while reducing the nodes cardinality of the graph.
Subsampling is implemented as a mean/max pooling operation over each node and its neighbors, then removing every alternate node, halving the total number of nodes.
The edges of the graph are recomputed accordingly by scaling the position of each node by a factor $2^l$, where $l$ is the index of the stage of the hierarchical temporal backbone.
Overall, the output of the temporal backbone $\mathcal{M}_{t}$ maps the input graph~$\mathcal{G}^{(0)}$ to a set of graphs:
\begin{equation}
    \mathcal{M}_{t}: \mathcal{G}^{(0)} \to \{\mathcal{G}^{(1)}, \mathcal{G}^{(2)}, \dots, \mathcal{G}^{(L)}\},
\end{equation}
where $L$ is the total number of stages in the backbone and each graph $\mathcal{G}^{(\cdot)}$ is a progressively coarsened representation of the input video. The number of stages $L$ depends on the task: for fine-grained tasks, \eg AR or OSCC, a single stage is enough, while we use multiple stages for tasks that reason over a longer horizon. More details are reported in Sec.~\ref{sec:exp_impl_details}.
%
The architecture of the \emph{temporal} backbone is shown in Fig.~\ref{fig:architecture}.

\subsubsection{Temporal Distance Gated Convolution (TDGC).}
\begin{figure}[t]
    \centering
    \includegraphics[trim=0.3cm 0.3cm 0.3cm 0,width=0.98\columnwidth]{figures/TDGCv2.pdf}
    \caption{\textbf{Temporal Distance Gated Convolution layer (TDGC)}, specifically designed to integrate \emph{past and future events grounding} ($s_{ij}$) and to \emph{reason about the temporal distance} between nodes ($\mathbf{w}_{ij}$) in the aggregation step.}\label{fig:tdgc}
\end{figure}
Each stage of the \emph{temporal} backbone $\mathcal{M}_{t}$ is built as a stack of~$N_l$ GNN layers, which we call Temporal Distance Gated Convolution (TDGC).
These layers are designed to preserve and encode the temporal sequence of information, capturing the relative past and future dependencies between nodes. The proposed graph convolution layer, visualized in Fig.~\ref{fig:tdgc},
is explicitly designed to incorporate the relative positions between the root node and its neighbors in the message passing step.
More specifically, given two nodes $i$ and $j$ at layer $l$, we compute $s_{ij}$ as the sign of the relative temporal distance between the nodes and $\mathbf{w}_{ij}$ as a learnable projection of their relative distance (in absolute value):
\begin{equation}
    s_{ij} = \mathtt{sign}(\mathbf{pe}_{[i]}^{(l)} - \mathbf{pe}_{[j]}^{(l)}), \;\;\; \mathbf{w}_{ij} = \mathtt{MLP}(|\mathbf{pe}_{[i]}^{(l)} - \mathbf{pe}_{[j]}^{(l)}|).
\end{equation}
These two factors are used to re-weight the contribution of each node $j$ in the aggregation step, as follows:
\begin{align}
    \mathbf{x}_j^{'}     & = \mathtt{MLP}\left(\mathbf{x}_j^{(l)}\right) = \phi(\mathbf{W}_n^T \mathbf{x}_j^{(l)} + \mathbf{b}_n),                                                  \\
    \mathbf{x}_i^{(l+1)} & = \mathbf{W}^T_r\mathbf{x}_i^{(l)} + \mean_{j \in \bar{\mathcal{N}}(i)} \left( s_{ij} ( \mathbf{w}_{ij}  \odot \mathbf{x}_j^{'}) \right) + \mathbf{b}_r,
\end{align}
where $\mathbf{x}_i^{(l)}$ are the features of the node $i$ at layer $l$, $\bar{\mathcal{N}}(i)$ is the set of neighbors of node $i$, $\mathbf{W}_n$, $\mathbf{W}_r$ and $\mathbf{b}_n$, $\mathbf{b}_r$ are learnable weights and biases respectively. Subscript $r$ refers to the contribution of the root node.
Our TDGC layer is inspired by previous works on Temporal Action Localization which used 1D temporal convolution~\cite{zhao2021video,zhang2022actionformer}.
However, unlike common 1D convolutions, TDGC employs shared weights to aggregate past and future nodes,
enabling its application to video segments of arbitrary length and to graphs in which the relative temporal distance between nodes is not fixed.


\subsection{Task-specific components}\label{sec:method_ts}
The temporal backbone~$\mathcal{M}_t$ is shared between all downstream tasks and is designed to support task-agnostic temporal reasoning over a stream of fixed-length video segments.
After the backbone, we attach a separate neck~$\mathcal{N}_k$ for each task~$\mathcal{T}_k$ to project the node embeddings into the feature space of the corresponding task and possibly aligning them to the temporal boundaries of the task.
%
Features $\mathbf{X}^{(l)}$ from the temporal backbone are first projected with the task neck~$\mathcal{N}_k$, implemented as a two-layers MLP, to obtain~$\mathbf{X}^{(l)}_k$:
\begin{equation}\label{eq:task-specific-features}
    \mathbf{X}^{(l)}_k = \mathcal{N}_k \left( \mathbf{X}^{(l)} \right) \;\;\;\text{with}\;\;\;\mathcal{N}_k: \mathbb{R}^D \to \mathbb{R}^D.
\end{equation}
%
The neck is shared for all the output graphs of the \emph{temporal} backbone.
Then, for tasks defined on input segments with known temporal boundaries, \eg Action Recognition, we align the node embeddings with the task annotations.
For each video segment~$v_i \in \mathcal{V}$ annotated for the task $\mathcal{T}_k$, we aggregate the node embeddings that are between the start~$s_i$ and end~$e_i$ boundaries of the segment to obtain~$\mathbf{F}_{k,[i]}^{(l)}$:
\begin{equation}
    \mathbf{F}_{k,[i]}^{(l)} = \mathtt{align} (\mathbf{X}^{(l)}_k, s_i, e_i) = \mean_{j: \;s_i<\mathbf{p}^{(l)}_{[j]}<e_i} \mathbf{X}^{(l)}_{k,[j]},
\end{equation}
where~$i$ and~$j$ are row-indices and $\mathbf{F}_{k,[i]}^{(l)}$ are the task-specific features of segment $v_i$ of the video for task $\mathcal{T}_k$.
Other tasks, \eg Temporal Action Localization, operate on the full video and do not require task-specific alignment.
In such case, the task-specific features~$\mathbf{F}_{k}^{(l)}$ are set equal to the output of the task-specific neck $\mathbf{X}^{(l)}_k$.

\subsection{Building a backpack of reusable skills}\label{sec:method_egopack}
\begin{figure}
    \centering
    \includegraphics[trim=0 0cm 1cm 0cm,width=.95\columnwidth]{figures/interaction.pdf}
    \vspace{-.25cm}
    \caption{
        \textbf{Learning a novel task with a backpack.}
        After the Multi-Task training phase, we extract a set of prototypes $\mathbf{P}^k$ that summarize what the network has learned from each \emph{support task} $\mathcal{T}_k$, like a backpack of skills that we can carry over.
        In this \emph{Cross-Tasks Interaction} phase, the network can peek at these different task-perspective to enrich the learning of the novel task.
    }\label{fig:interaction}
\end{figure}
To solve the \emph{novel task} $\mathcal{T}_{K+1}$, the naive approach would be to finetune the model, adding new task-specific neck $\mathcal{N}_{K+1}$ and head $\mathcal{H}_{K+1}$ and possibly updating the temporal backbone $\mathcal{M}_{t}$.
However, finetuning may not fully leverage the insights from other tasks as it could result in the loss of the previously acquired knowledge, as the model adapts to the new task.
Instead, we explicitly model the perspectives of the \emph{support tasks}, \ie the set of tasks the model has learned in the MTL pre-training step, as a set of task-specific prototypes that can be accessed by the novel task.
% 
This approach was originally proposed as part of \ourscvpr\cite{egopack} and we provide an overview in Fig.~\ref{fig:interaction}.
We collect these task-specific prototypes from videos annotated for action recognition, as human actions can be seen as the common thread behind the different tasks.

Practically, we forward these action samples through the temporal backbone, align them based on the action recognition annotations and project their features using the task-specific necks $\mathcal{N}_k$ of each task to obtain the task-specific features $\mathbf{F}_k$ for each task in the MTL pre-training phase. Each row in $\mathbf{F}_k$ encodes the perspective of each task for the same video segment.
%
To summarize these features into prototypes we aggregate them according to the action label of the corresponding action segment, \ie, a \textit{verb} and \textit{noun} pair:
\begin{equation}
    \mathbf{P}^k = \{ \mathbf{p}^{k}_{0}, \mathbf{p}^{k}_{1}, \dots, \mathbf{p}^{k}_{P
    } \} \in \mathbb{R}^{P \times D},
\end{equation}
for each task $\mathcal{T}_k$, where $P$ is the number of unique \textit{(verb, noun)} pairs in the dataset and $D$ is the size of the task-specific features.
These prototypes are frozen and represent a \emph{summary} of what the models has learned during the multi-task pre-training process, creating an abstraction of the gained knowledge. They can be then reused when learning a \emph{novel task}, like a backpack of skills that the model can carry over.
Notably, storing the model's knowledge in the prototypes allows for fine-tuning the temporal backbone, which is especially valuable when the novel task has a different temporal granularity compared to the previous tasks.

\subsection{Learning a novel task with a backpack}\label{sec:method_egopack_learning}
Let us now consider the case in which we want to solve a novel task $\mathcal{T}_{K+1}$.
The model can exploit the perspective of the previously seen tasks by comparing the output of the task-specific necks for tasks $\mathcal{T}_{1,\dots,K}$ with their corresponding prototypes.
When learning the novel task $\mathcal{T}_{K+1}$, the output graphs of the \emph{temporal} backbone are forwarded through all projection necks to obtain the task-specific features $\mathbf{X}^{(l)}_{k}$, as defined in Eq.~\ref{eq:task-specific-features}.
To improve readability we hereinafter omit the superscript indicating the specific stage $l$ at the temporal backbone stage.
These features are used as \emph{queries} to match the corresponding task prototypes $\mathbf{P}^k$, using $k$-NN in the features space to look for the closest prototypes.
Task features and their neighboring prototypes form a \emph{graph-like} structure, on which message passing is performed to enrich the task-specific features $\mathbf{X}^{(l)}_k$, following an iterative refinement approach, using $M$ layers of SAGE convolution.
%

At each layer $m$ of \ours, we update the features~$\mathbf{X}_{k,[i]}$ from stage $l$ of the temporal backbone by combining them with its closest prototypes~$\bar{\mathcal{N}}(i)$:
\begin{equation}
    \mathbf{X}^{(m+1)}_{k,[i]} = \mathbf{W}^{(m)}_{r} \mathbf{X}^{(m)}_{k,[i]} + \mathbf{W}^{(m)} \cdot  \mean_{\mathbf{p}^k_{j} \, \in \, \bar{\mathcal{N}}(i)} \mathbf{p}^k_{j},
\end{equation}\label{eq:egopackgnn}
where~$\mathbf{p}^k_{j} \, \in \, \bar{\mathcal{N}}(i)$ are the \emph{activated prototypes} for the given task, \ie the set of closest task-specific prototypes in $\mathbf{P}^k$ with respect to $\mathbf{X}_{k,[i]}$, and $\mathbf{W}^{(m)}_{r},\mathbf{W}^{(m)}$ are learnable projections of the input features and the aggregated neighbors, respectively. Eq.~\ref{eq:egopackgnn} is applied to features from all $l$ stages of the hierarchical temporal backbone.
Notably, only the task features are refined while the task prototypes remain frozen to preserve the original perspectives seen by the network.
We denote the output of this interaction process as $\tilde{\mathbf{X}}^{(l)}_k$.
These features are then possibly aligned to the boundaries of the novel task to obtain $\tilde{\mathbf{F}}^{(l)}_k$, as discussed in Sec.~\ref{sec:method_ts}.

In this process, the \emph{task-specific necks} of the support tasks $\mathcal{N}_{1,\dots,K}$ are initialized from the multi-task training and updated during the task-specific finetuning process, allowing the model to explore the set of task prototypes and to select the most informative ones for each input sample.
Moreover, to allow the model to learn complementary cues specific to the novel task, we add a new pair of neck $\mathcal{N}_{K+1}$ and head $\mathcal{H}_{K+1}$.
We evaluate different fusion strategies to integrate the novel task with the perspectives gained from the previous tasks.
In \emph{features-level} fusion, we average the task-specific features for the novel task $\mathbf{F}_{K+1}$ with the \textit{refined} perspectives from the previous tasks~$\tilde{\mathbf{F}}_k$.
In \emph{logits-level} fusion, we keep a set of separate heads, one for each task $\mathcal{T}_{1,\dots,K}$, feed the features $\tilde{\mathbf{F}}_k$ to each head separately and sum their outputs, as in the original \ourscvpr implementation.
Intuitively, this approach allows each task to cast a vote on the final prediction, based on its perspective on the same video segment.

\subsection{Training process}
We train our models using only supervision of the known task, for both single and multi-task models. More details are reported in Sec.~\ref{sec:exp_impl_details}.
When training \ours, we finetune the \emph{temporal} backbone, the task-specific projection necks and the heads. Gradient updates from the support tasks are not propagated to the \emph{temporal} backbone.