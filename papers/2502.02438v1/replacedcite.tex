\section{Related Work}
% knowledge distillation
\paragraph{Knowledge distillation.}
Knowledge distillation____ helps transfer the knowledge from a complex and larger ``teacher'' model to a compact and simpler ``student'' model, which is similar to our victim-attacker design. 
However, unlike knowledge distillation, where the student model has the same data distribution as the teacher model's training data, in our problem formulation, the attacker has no prior knowledge of the victim's black-box model, e.g., unknown architecture, data distributions or training parameters. 
Although data-free knowledge distillation____ further assumes the absence of the teacher model's training data, its requirement of white-box access to the teacher model for backpropagation is a major difference to our setup.
% model stealing
\paragraph{Model stealing.}
Model stealing, or model theft, typically has one of two objectives: exact replication of the model or its components, or functionality replication, where the attacker aims to mimic the model's behavior.
The first type focuses on extracting the model's hyperparameters____, architecture____, or learned parameters____. The second type____, involves training a model that mimics the victim's performance without prior knowledge of its training data or architecture.
In this work, we present the first functionality model stealing attack against \acp{MLLM} for radiology report generation. While previous methods like Knockoff Nets____ replicate medical image classifiers using natural images, they deal with a much smaller output space compared to text generation. Bert-Thieves____, another related approach, targets language models, but does not handle images and benefits from publicly available text data that shares a similar distribution with the victim model.
In contrast, medical data is hard to obtain, and only a specific subset of the vocabulary is relevant in this context, making it more challenging for the attacker, who may lack prior knowledge in the medical domain.
% mllm
\paragraph{Security of \aclp{MLLM}.}
With the ability to understand and reason about different data types, \acp{MLLM} are vulnerable to evasion attacks targeting each data modality, such as malicious image and text constructs____.
In contrast to these attacks that are designed to induce erroneous or disallowed responses, the model stealing attack we present aims to mimic the functionality of the victim medical \ac{MLLM} for radiology report generation, using only black-box access to the victim model.

%% ==== threat model / problem statement
\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/method.png} 
\caption{The overview of our proposed approach with three iterative phases: (I) attacker model training, (II) medical report enrichment (in gray dash box), and (III) domain alignment (in green dash box).}
\label{fig: method}
\end{figure*}