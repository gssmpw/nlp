@article{10.1145/3639372,
author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
title = {Explainability for Large Language Models: A Survey},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3639372},
doi = {10.1145/3639372},
abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {20},
numpages = {38},
keywords = {Explainability, interpretability, large language models}
}

@inproceedings{NEURIPS2022_6f1d43d5,
 author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {17359--17372},
 publisher = {Curran Associates, Inc.},
 title = {Locating and Editing Factual Associations in GPT},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@misc{bhattacharya2024understandingroleffnsdriving,
      title={Understanding the role of FFNs in driving multilingual behaviour in LLMs}, 
      author={Sunit Bhattacharya and Ond≈ôej Bojar},
      year={2024},
      eprint={2404.13855},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.13855}, 
}

@inproceedings{dar-etal-2023-analyzing,
    title = "Analyzing Transformers in Embedding Space",
    author = "Dar, Guy  and
      Geva, Mor  and
      Gupta, Ankit  and
      Berant, Jonathan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.893/",
    doi = "10.18653/v1/2023.acl-long.893",
    pages = "16124--16170",
    abstract = "Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning. While most interpretability methods rely on running models over inputs, recent work has shown that a zero-pass approach, where parameters are interpreted directly without a forward/backward pass is feasible for some Transformer parameters, and for two-layer attention networks. In this work, we present a theoretical analysis where all parameters of a trained Transformer are interpreted by projecting them into the embedding space, that is, the space of vocabulary items they operate on. We derive a simple theoretical framework to support our arguments and provide ample evidence for its validity. First, an empirical analysis showing that parameters of both pretrained and fine-tuned models can be interpreted in embedding space. Second, we present two applications of our framework: (a) aligning the parameters of different models that share a vocabulary, and (b) constructing a classifier without training by {\textquotedblleft}translating{\textquotedblright} the parameters of a fine-tuned classifier to parameters of a different model that was only pretrained. Overall, our findings open the door to interpretation methods that, at least in part, abstract away from model specifics and operate in the embedding space only."
}

@inproceedings{ferrando-costa-jussa-2024-similarity,
    title = "On the Similarity of Circuits across Languages: a Case Study on the Subject-verb Agreement Task",
    author = "Ferrando, Javier  and
      Costa-juss{\`a}, Marta R.",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.591/",
    doi = "10.18653/v1/2024.findings-emnlp.591",
    pages = "10115--10125",
    abstract = "Several algorithms implemented by language models have recently been successfully reversed-engineered. However, these findings have been concentrated on specific tasks and models, leaving it unclear how universal circuits are across different settings. In this paper, we study the circuits implemented by Gemma 2B for solving the subject-verb agreement task across two different languages, English and Spanish. We discover that both circuits are highly consistent, being mainly driven by a particular attention head writing a {\textquoteleft}subject number' signal to the last residual stream, which is read by a small set of neurons in the final MLPs. Notably, this subject number signal is represented as a direction in the residual stream space, and is language-independent. Finally, we demonstrate this direction has a causal effect on the model predictions, effectively flipping the Spanish predicted verb number by intervening with the direction found in English."
}

@inproceedings{geva-etal-2022-transformer,
    title = "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space",
    author = "Geva, Mor  and
      Caciularu, Avi  and
      Wang, Kevin  and
      Goldberg, Yoav",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.3/",
    doi = "10.18653/v1/2022.emnlp-main.3",
    pages = "30--45",
    abstract = "Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50{\%}, and for improving computation efficiency with a simple early exit rule, saving 20{\%} of computation on average."
}

@article{goldowsky2023localizing,
  title={Localizing model behavior with path patching},
  author={Goldowsky-Dill, Nicholas and MacLeod, Chris and Sato, Lucas and Arora, Aryaman},
  journal={arXiv preprint arXiv:2304.05969},
  year={2023}
}

@misc{heimersheim2024useinterpretactivationpatching,
      title={How to use and interpret activation patching}, 
      author={Stefan Heimersheim and Neel Nanda},
      year={2024},
      eprint={2404.15255},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.15255}, 
}

@misc{lan2024sparseautoencodersrevealuniversal,
      title={Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models}, 
      author={Michael Lan and Philip Torr and Austin Meek and Ashkan Khakzar and David Krueger and Fazl Barez},
      year={2024},
      eprint={2410.06981},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.06981}, 
}

@inproceedings{mu-etal-2024-revealing,
    title = "Revealing the Parallel Multilingual Learning within Large Language Models",
    author = "Mu, Yongyu  and
      Feng, Peinan  and
      Cao, Zhiquan  and
      Wu, Yuzhang  and
      Li, Bei  and
      Wang, Chenglong  and
      Xiao, Tong  and
      Song, Kai  and
      Liu, Tongran  and
      Zhang, Chunliang  and
      Zhu, JingBo",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.396/",
    doi = "10.18653/v1/2024.emnlp-main.396",
    pages = "6976--6997",
    abstract = "Large language models (LLMs) can handle multilingual and cross-lingual text within a single input; however, previous works leveraging multilingualism in LLMs primarily focus on using English as the pivot language to enhance language understanding and reasoning. Given that multiple languages are a compensation for the losses caused by a single language`s limitations, it`s a natural next step to enrich the model`s learning context through the integration of the original input with its multiple translations. In this paper, we start by revealing that LLMs learn from parallel multilingual input (PMI). Our comprehensive evaluation shows that PMI enhances the model`s comprehension of the input, achieving superior performance than conventional in-context learning (ICL). Furthermore, to explore how multilingual processing affects prediction, we examine the activated neurons in LLMs. Surprisingly, involving more languages in the input activates fewer neurons, leading to more focused and effective neural activation patterns. Also, this neural reaction coincidently mirrors the neuroscience insight about synaptic pruning, highlighting a similarity between artificial and biological {\textquoteleft}brains'."
}

@inproceedings{peng-sogaard-2024-concept,
    title = "Concept Space Alignment in Multilingual {LLM}s",
    author = "Peng, Qiwei  and
      S{\o}gaard, Anders",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.315/",
    doi = "10.18653/v1/2024.emnlp-main.315",
    pages = "5511--5526",
    abstract = "Multilingual large language models (LLMs) seem to generalize somewhat across languages. We hypothesize this is a result of implicit vector space alignment. Evaluating such alignment, we see that larger models exhibit very high-quality linear alignments between corresponding concepts in different languages. Our experiments show that multilingual LLMs suffer from two familiar weaknesses: generalization works best for languages with similar typology, and for abstract concepts. For some models, e.g., the Llama-2 family of models, prompt-based embeddings align better than word embeddings, but the projections are less linear {--} an observation that holds across almost all model families, indicating that some of the implicitly learned alignments are broken somewhat by prompt-based methods."
}

@misc{rai2024practicalreviewmechanisticinterpretability,
      title={A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models}, 
      author={Daking Rai and Yilun Zhou and Shi Feng and Abulhair Saparov and Ziyu Yao},
      year={2024},
      eprint={2407.02646},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.02646}, 
}

@inproceedings{tang-etal-2024-language,
    title = "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models",
    author = "Tang, Tianyi  and
      Luo, Wenyang  and
      Huang, Haoyang  and
      Zhang, Dongdong  and
      Wang, Xiaolei  and
      Zhao, Xin  and
      Wei, Furu  and
      Wen, Ji-Rong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.309/",
    doi = "10.18653/v1/2024.acl-long.309",
    pages = "5701--5715",
    abstract = "Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora.It remains a challenging problem to explain the underlying mechanisms by which LLMs process multilingual texts.In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions.Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs.Based on LAPE, we conduct comprehensive experiments on several representative LLMs, such as LLaMA-2, BLOOM, and Mistral. Our findings indicate that LLMs' proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models' top and bottom layers.Furthermore, we showcase the feasibility to {\textquotedblleft}steer{\textquotedblright} the output language of LLMs by selectively activating or deactivating language-specific neurons. Our research provides important evidence to the understanding and exploration of the multilingual capabilities of LLMs."
}

@inproceedings{wei2024interpreting,
author = {Zhang, Wei and Wan, Chaoqun and Zhang, Yonggang and Cheung, Yiu-ming and Tian, Xinmei and Shen, Xu and Ye, Jieping},
title = {Interpreting and improving large language models in arithmetic calculation},
year = {2025},
publisher = {JMLR.org},
abstract = {Large language models (LLMs) have demonstrated remarkable potential across numerous applications and have shown an emergent ability to tackle complex reasoning tasks, such as mathematical computations. However, even for the simplest arithmetic calculations, the intrinsic mechanisms behind LLMs remains mysterious, making it challenging to ensure reliability. In this work, we delve into uncovering a specific mechanism by which LLMs execute calculations. Through comprehensive experiments, we find that LLMs frequently involve a small fraction (< 5\%) of attention heads, which play a pivotal role in focusing on operands and operators during calculation processes. Subsequently, the information from these operands is processed through multi-layer perceptrons (MLPs), progressively leading to the final solution. These pivotal heads/MLPs, though identified on a specific dataset, exhibit transferability across different datasets and even distinct tasks. This insight prompted us to investigate the potential benefits of selectively fine-tuning these essential heads/MLPs to boost the LLMs' computational performance. We empirically find that such precise tuning can yield notable enhancements on mathematical prowess, without compromising the performance on non-mathematical tasks. Our work serves as a preliminary exploration into the arithmetic calculation abilities inherent in LLMs, laying a solid foundation to reveal more intricate mathematical tasks.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {2477},
numpages = {19},
location = {Vienna, Austria},
series = {ICML'24}
}

@inproceedings{wendler-etal-2024-llamas,
    title = "Do Llamas Work in {E}nglish? On the Latent Language of Multilingual Transformers",
    author = "Wendler, Chris  and
      Veselovsky, Veniamin  and
      Monea, Giovanni  and
      West, Robert",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.820/",
    doi = "10.18653/v1/2024.acl-long.820",
    pages = "15366--15394",
    abstract = "We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language{---}-a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study is based on carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already in middle layers allow for decoding a semantically correct next token, but giving higher probability to its version in English than in the input language; (3) move into an input-language-specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in {\textquotedblright}input space{\textquotedblright}, {\textquotedblright}concept space{\textquotedblright}, and {\textquotedblright}output space{\textquotedblright}, respectively. Crucially, our evidence suggests that the abstract {\textquotedblright}concept space{\textquotedblright} lies closer to English than to other input languages, which may have important consequences regarding the biases embodied by multilingual language models."
}

