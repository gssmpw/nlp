% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath,bm}
\usepackage{amssymb}
\usepackage{CJKutf8}  % 宏包
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}  % 加载 caption 宏包
\usepackage{booktabs}
\usepackage{tabularx} % 自适应宽度表格
\usepackage{multirow}
\usepackage{tcolorbox}
\usepackage{subfig}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{tcolorbox}
\usepackage{colortbl}

\definecolor{myRed}{HTML}{ffcdd2}
\definecolor{myGreen}{HTML}{c8e6c9}
\definecolor{myBlue}{HTML}{c8e6f5}

\newcommand{\compactequal}[1]{\setlength{\fboxsep}{1pt}\colorbox{myBlue! 100}{#1}}
\newcommand{\compactworse}[1]{\setlength{\fboxsep}{1pt}\colorbox{myRed! 100}{#1}}
\newcommand{\compactbetter}[1]{\setlength{\fboxsep}{1pt}\colorbox{myGreen! 100}{#1}}


% some format
\hyphenpenalty=8000
\tolerance=1000
\hyphenation{hy-phen-a-tion}
\setlength{\textfloatsep}{8pt}
\setlength{\abovecaptionskip}{5pt}   % 图像与标题之间的间距
\setlength{\belowcaptionskip}{-2.5pt}   % 标题与下方正文的间距

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{How does Llama translate? Unveiling the internal mechanism of LLM's Translation via Path Patching}
\title{Exploring Translation Mechanism of Large Language Models}
% \title{Analyzing Internal Mechanism of LLM's Translation via Path Patching}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Hongbin Zhang\textsuperscript{\textdagger\textdaggerdbl},Kehai Chen\textsuperscript{\textdagger}\thanks{Corresponding Author},Xuefeng Bai\textsuperscript{\textdagger},Xiucheng Li\textsuperscript{\textdagger},Yang Xiang\textsuperscript{\textdagger},Min Zhang\textsuperscript{\textdagger} \\
\textsuperscript{\textdagger}Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China \\
\textsuperscript{\textdaggerdbl}Peng Cheng Laboratory, Shenzhen, China \\
\texttt{azure.starzhang@gmail.com,\{chenkehai,baixuefeng,lixiucheng,zhangmin2021\}@hit.edu.cn,} \\
\texttt{xiangy@pcl.ac.cn} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Large language models (LLMs) have succeeded remarkably in multilingual translation tasks.
However, the inherent translation mechanisms of LLMs remain poorly understood, largely due to sophisticated architectures and vast parameter scales.
In response to this issue, this study explores the translation mechanism of LLM from the perspective of computational components (e.g., attention heads and MLPs).
Path patching is utilized to explore causal relationships between components, detecting those crucial for translation tasks and subsequently analyzing their behavioral patterns in human-interpretable terms.
Comprehensive analysis reveals that translation is predominantly facilitated by a sparse subset of specialized attention heads (less than 5\%), which extract source language, indicator, and positional features. MLPs subsequently integrate and process these features by transiting towards English-centric latent representations.
Notably, building on the above findings, targeted fine-tuning of only 64 heads achieves translation improvement comparable to full-parameter tuning while preserving general capabilities.
\footnote{\textcolor{blue}{Our code and data will be released once accepted.}}
\end{abstract}

\section{Introduction}
Large language models (LLMs) have succeeded remarkably in multilingual translation tasks~\citep{chen2024llmbasedtranslationinferenceiterative,chen-etal-2024-dual,zhu-etal-2024-multilingual,zhang-etal-2024-paying}, paving the way for a new paradigm in machine translation~\citep{xu2024a,alves2024toweropenmultilinguallarge}. 
Recent advancements have continuously focused on enhancing translation capabilities, bringing them progressively closer to human-level translation~\citep{xu2024contrastive,lu-etal-2024-llamax,xu2024xalmaplugplay}.
Despite the widespread adoption and recent advancements in LLMs, the internal mechanisms by which they perform translation tasks remain poorly understood and pose severe challenges. 
Prior analyses focused on surface-level emergent linguistic phenomena (e.g., neuron activation patterns~\citep{mu-etal-2024-revealing,tang-etal-2024-language} or intermediate representations~\citep{wendler-etal-2024-llamas,zhu-etal-2024-multilingual}), remaining \textit{observational} rather than elucidating the \textit{computational mechanistic basis} underlying translation.
A comprehensive understanding of these functional mechanisms is critical for achieving robust improvements in translation capability and advancing the development of controllable and interpretable LLMs~\citep{wang2023interpretability,wei2024interpreting}.

In this paper, we study the internal mechanism of LLM translation by progressively investigating the following research questions:
\begin{itemize}
    \item \textit{Which components of LLMs crucially contribute to performing translation?}
    \item \textit{What behavioral patterns do these translation-crucial components exhibit?}
    \item \textit{Can fine-tuning these translation-crucial components enhance LLM translation capability?}
\end{itemize}
To this end, we leverage path patching~\citep{goldowsky2023localizing} to examine the causal relationships between computational components (e.g., attention heads and MLP), detecting those crucial for translation tasks. For components judged as crucial, we then systematically analyze their behavioral patterns by (1) characterizing attention head's roles according to the attention contribution to lexical alignment and (2) measuring correlations between MLP representations and translation-relevant token embeddings. 
Our analysis reveals three distinct attention head functional roles: (i) \textit{source heads} that focus on source-language tokens, (ii) \textit{indicator heads} that track translation-initiating signals, and (iii) \textit{positional heads} that maintain sequential coherence.  
Additionally, we demonstrate that MLPs dynamically integrate translation-related features from critical attention heads, iteratively transiting them into English-centric latent representations.

Building on these insights, we design a targeted optimization strategy based on supervised fine-tuning (SFT)~\citep{NEURIPS2022_b1efde53} to selectively fine-tune translation-crucial components, thereby assessing whether fine-tuning these components improves translation performance.
As a result, our findings are as follows:
\begin{itemize}
    \item \textit{Only a sparse subset of heads (less than 5\%) are crucial for LLMs' translation.}
    \item \textit{Crucial heads exhibit specialized functions to process translation-relevant features, with MLPs integrating these features and transiting to English-centric latent representations.}
    \item \textit{Fine-tuning merely 64 heads achieves performance parity with full-parameter fine-tuning.}
\end{itemize}



\section{Related Works}
\paragraph{Mechanistic Interpretability.}
Mechanistic interpretability (MI) elucidates neural network mechanisms by seeking to reverse-engineer and decode their functioning~\citep{NEURIPS2022_6f1d43d5,lan2024sparseautoencodersrevealuniversal,10.1145/3639372,rai2024practicalreviewmechanisticinterpretability}.
Within the broader MI landscape, two key techniques are foundational to this work: (i) \textbf{Path patching}~\citep{goldowsky2023localizing,wang2023interpretability}, derived from activation patching~\citep{heimersheim2024useinterpretactivationpatching,zhang2024towards}, probes causal relationships and analyzes interactions between components in neural networks by tracing effect propagation along network pathways via targeted activation interventions.
(ii) \textbf{Embedding projection}~\citep{geva-etal-2022-transformer,dar-etal-2023-analyzing} maps high-dimensional representations to human-interpretable spaces via dimensionality reduction approaches. 
Recent studies highlight the utility of \textit{path patching} to gain insights into functioning behavior, such as identifying circuits for tasks like indirect object identification\citep{wang2023interpretability} and arithmetic calculations\citep{wei2024interpreting}.


\paragraph{Interpretability in Multilingual LLMs.}

Recent studies have delved deeper into \textit{how} LLMs achieve multilingually by investigating linguistic phenomena emergent in multilingual context~\citep{bhattacharya2024understandingroleffnsdriving, peng-sogaard-2024-concept, ferrando-costa-jussa-2024-similarity, dumas2024how}. Key findings indicate that (i) increased linguistic diversity in inputs leads to reduced neuron activations~\citep{mu-etal-2024-revealing}; (ii) LLMs exhibit language-specific functional regions~\citep{tang-etal-2024-language}; and (iii) English frequently functions as an implicit computational pivot~\citep{wendler-etal-2024-llamas,zhao2024how}. 
Unlike prior research focusing on surface-level emergent linguistic phenomena rather than computational translation mechanisms, this work comprehensively analyzes the functional processes underlying LLMs’ translation abilities. We present a novel method to systematically examine how LLMs execute translation tasks, improving interpretability and practical understanding of their translation functionalities.


\section{Constructing Analysis Dataset}
Our goal is to explore the translation mechanism of LLMs.
Directly leveraging existing sentence-level parallel corpora is challenging due to the lack of one-to-one word alignment between source and target languages. This motivates us to investigate how LLMs perform word-level translation.
Taking inspiration from the prompt design and word selection in \citet{wendler-etal-2024-llamas}, we construct a closed set of word translation analysis datasets across five widely used languages (e.g., English (En), Chinese (Zh), Russian (Ru), German (De), and French (Fr)). 
Taking word translation from English to Chinese as an example, a word translation prompt like 
\begin{CJK*}{UTF8}{gbsn} % 设置环境
``English: book - 中文: '' (``中文'' means ``Chinese'') 
\end{CJK*} % 结束环境
might appear in the analysis datasets.
In this study, we select the samples that LLMs can translate correctly, denoting these sentences successfully prompting LLMs to translate as reference data using the notation of $X_{f}$. 
More details of the construction of word translation datasets can be referred to Appendix~\ref{apdx:data_construction}.

Moreover, to meet the demand for component activation perturbation, we constructed a supplementary dataset comprising counterfactual sentences that exclude translation logic, using the notation of $X_{cf}$. The counterfactual sentences are generated adhering to two core principles: (1) preserving grammatical structures from the original $X_{f}$ sentences and (2) replacing several crucial words responsible for the translation logic with contextually irrelevant terms. For instance, a sentence from $X_{f}$ like
\begin{CJK*}{UTF8}{gbsn} % 设置环境
``English: cloud - 中文: \_'' 
\end{CJK*} % 结束环境
is replaced with the corresponding counterfactual one
\begin{CJK*}{UTF8}{gbsn} % 设置环境
``English: cloud - Nothing: \_''.
\end{CJK*} % 结束环境
This isolates model’s impact on translation tasks from sentence structural or syntactic variables, enabling precise analysis of how LLMs perform translation tasks. 
Various counterfactual templates are considered in this study and provided in Appendix~\ref{apdx:data_template}.
The constructed analysis dataset (including $X_{f}$ and $X_{cf}$) is utilized in subsequent sections, as illustrated in the examples below:
\begin{CJK*}{UTF8}{gbsn} % 设置环境
\begin{tcolorbox}[
    colback=blue!5,
    colframe=blue!50!black,
    title=\textbf{Reference Data (\textit{X\textsubscript{f}})},
    top=1pt,
    bottom=1pt,
    left=1pt,
    right=1pt,
    boxsep=2pt,
    ]
\begin{verbatim}
English: "cloud"  - 中文: "
English: "flower" - 中文: "
English: "snow"   - 中文: "
\end{verbatim}
\end{tcolorbox}

\begin{tcolorbox}[
    colback=red!5,
    colframe=red!50!black,
    title=\textbf{Counterfactual Data (\textit{X\textsubscript{cf}})},
    top=1pt,
    bottom=1pt,
    left=1pt,
    right=1pt,
    boxsep=2pt,
    ]
\begin{verbatim}
English: "cloud"  - Nothing: "
English: "flower" - Nothing: "
English: "snow"   - Nothing: "
\end{verbatim}
\end{tcolorbox}
\end{CJK*} % 结束环境

\section{Overview of Interpretation}
\begin{figure}[!htbp]
\vspace{-5pt} % 通过负值减少上方间距
    \centering
    \includegraphics[width=1\linewidth]{overview.pdf}
    \caption{The overview of the interpretation method: (1) detect crucial components, (2) analyze their behavioral patterns, and (3) selectively fine-tune them.}
    \label{fig:intro}
\vspace{-2pt} % 通过负值减少上方间距
\end{figure}
\begin{figure*}[htbp]
\vspace{-10pt} % 通过负值减少上方间距
	\centering
    \subfloat[Zh $\Rightarrow$ En]{\includegraphics[width=.33\linewidth]{zh-en.pdf}\label{fig:identify:zh-en}}
	\subfloat[Zh $\Rightarrow$ Fr]{\includegraphics[width=.33\linewidth]{zh-fr.pdf}\label{fig:identify:zh-fr}}
	\subfloat[Zh $\Rightarrow$ Ru]{\includegraphics[width=.33\linewidth]{zh-ru.pdf}\label{fig:identify:zh-ru}}\\
    \subfloat[En $\Rightarrow$ Zh]{\includegraphics[width=.33\linewidth]{en-zh.pdf}\label{fig:identify:en-zh}}
	\subfloat[Fr $\Rightarrow$ Zh]{\includegraphics[width=.33\linewidth]{fr-zh.pdf}\label{fig:identify:fr-zh}}
	\subfloat[Ru $\Rightarrow$ Zh]{\includegraphics[width=.33\linewidth]{ru-zh.pdf}\label{fig:identify:ru-zh}}
	\caption{Importance of heads related to translation across different directions. Each square at position $(x,y)$ refers to the $x$-th head in the $y$-th layer. Red (Brown) squares denote heads (MLPs) that have a positive impact on predicting the target token, while grey (purple) squares indicate heads (MLPs) with a negative effect.}
    \label{fig:identify}
\end{figure*}
Our method investigates the internal mechanisms of LLM translation through three steps:
\begin{enumerate}
    \item Detecting, validating translation-crucial components and examining their consistency across training phases (Section \S \ref{method_sec:identify}).
    \item Analyzing the inherent patterns of these components to characterize their behavioral and distinctive features (Section \S \ref{method_sec:analyze}).
    \item Implementing a targeted SFT strategy to fine-tune essential components and improve translation performance (Section \S \ref{method_sec:finetune}).
\end{enumerate}


\section{Crucial Components Detection}
\label{method_sec:identify}
We begin by addressing the first research question: ``\textit{Which components crucially influence LLMs' translation capabilities?}'' Using path patching, we detect components crucial for performing translation tasks (Section \S \ref{sec:detecing}), subsequently validate their importance through knockout (Section \S \ref{sec:validaiton}), and further examine whether these heads exhibit consistency across training stages (Section \S \ref{sec:consistency}).

\subsection{Detecting Crucial Components for Translation Tasks via Path Patching}
\label{sec:detecing}
To determine the causal mechanisms behind the model’s translation, we employ the \textit{path patching}~\citep{wang2023interpretability,wei2024interpreting}. This method systematically analyzes causal relationships between two computation nodes (\textit{Sender} $\rightarrow$ \textit{Receiver}), evaluating whether the \textit{Sender} causally influences the \textit{Receiver}, and whether their connection is functionally crucial for translation tasks. 
We perturb specific activations using counterfactual data $X_{cf}$, while maintaining others at reference data $X_{f}$, measuring the counterfactual effect through output logit comparisons. 
Our method iteratively examines all components, isolates constituent circuits, and quantifies changes in ground-truth token logits. 
Appendix \ref{apdx:path_pathcing} provides more details of the method.

\paragraph{Detection results of crucial heads.}
\label{sec:exp:detection}
We begin by examining the causal impact of logits from path patching each head across layers on LLaMA2-7B~\citep{touvron2023llama}. As a particularly clean case, we focus on two categories of translation directions: Zh $\Rightarrow$ X, and X $\Rightarrow$ Zh, which has many single-token words. We define ``crucial heads'' as those whose magnitude of logit change exceeds $1.0\%$. 
As illustrated in Figure \ref{fig:identify}, we highlight several key findings: 
\begin{enumerate}
    \item \textbf{Only a sparse subset of heads significantly influences translation performance.} For instance, patching the head at position $(31,8)$ results in a substantial decrease in the target token’s logit value, illustrating its critical role in the translation process.
    \item \textbf{Impactful heads are concentrated in the middle and final layers.} Earlier layers lack heads directly influencing target token logits; instead, crucial heads cluster predominantly between layers 12 and 20 and in the final two layers. This pattern remains consistent across all translation directions.
    \item \textbf{Crucial heads exhibit high transferability across translation directions.} A notable finding is the significant overlap of crucial heads across diverse language pairs. Analysis reveals that language pairs sharing the same source or target language exhibit a crucial attention head overlap exceeding 60\%, while bidirectional translation pairs (e.g., Zh $\Leftrightarrow$ En) surpass 70\%. This overlap suggests these heads serve generalizable functions in translation, independent of translation directions. Their consistency across language pairs underscores their importance and transferability, indicating contributions to core translation mechanisms regardless of specific languages.

\end{enumerate}
For robustness, we also conduct additional experiments on detecting crucial heads in other LLMs and other directions (e.g., En $\Rightarrow$ X, and X $\Rightarrow$ En). Details are provided in the Appendix~\ref{apdx:more_llms}.

\paragraph{Detection results of crucial MLPs.}
Similar to crucial heads, most MLPs in earlier layers (0–14) exhibit negligible influence on output logits, with changes confined to approximately $\pm 0.0\%$. Crucial MLPs cluster predominantly after layer 15, exceeding $5.0\%$ logit change, whereas the final layer MLP exhibits a substantial impact—reaching $70.0\%$ on target token logit change. This strong correlation between later MLP layers and logit changes underscores their progressively critical role in shaping translations as processing advances.


\subsection{Validating Crucial Heads Through Knockout}
\label{sec:validaiton}
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.85\linewidth]{validation.pdf}
    \caption{The influence on \textbf{En $\Rightarrow$ Zh} translation accuracy in the analysis dataset when attention heads are progressively knocked out, sorted by their effect on logits (``key heads''), and randomly (``random heads'')}
    \label{fig:valid}
\vspace{-5pt}
\end{figure}
Interpretive analyses of model components risk misleading or non-rigorous \citep{bolukbasi2021interpretabilityillusionbert,wiegreffe-pinter-2019-attention}. To ensure reliability, we validate the significance of detected crucial heads and test the irrelevance of non-crucial ones via \textit{mean ablation} \citep{wang2023interpretability}. This method replaces a component’s activation with average activations across counterfactual data $X_{cf}$, thereby removing task-specific information. Performance decline confirms a component's importance for translation tasks, whereas no significant performance change indicates uncritical.

\paragraph{Validation results on the analysis dataset.}
We examine how incrementally knocking out En $\Rightarrow$ Zh crucial heads affects LLM translation performance on the analysis dataset.
As shown in Figure~\ref{fig:valid}, disabling ``\textit{crucial heads}'' leads to a significant decline in translation accuracy, whereas knocking out ``\textit{random heads}'' causes minor fluctuations, with accuracy remaining stable within 2\%. These results highlight the essential role of the detected key attention heads in sustaining the translation functionality of the LLM.

\subsection{Examine Consistency of Crucial Heads Across Training}
\label{sec:consistency}
\begin{figure}[!htbp]
\vspace{-15pt} % 通过负值减少上方间距
	\centering
	\subfloat[CPT-LLM]{\includegraphics[width=.5\linewidth]{CPT.pdf}\label{fig:identify:cpt}}
	\subfloat[SFT-LLM]{\includegraphics[width=.5\linewidth]{SFT.pdf}\label{fig:identify:sft}}
	\caption{Importance of heads related to \textbf{En$\Rightarrow$ Zh} translation across LLM after CPT or SFT.}
    \label{fig:consistency}
\end{figure}
To investigate whether crucial attention heads remain consistent across distinct training phases, we analyze (1) continued pre-training (CPT)~\citep{xu2024a} on the LLaMA-2-7B base model on 1 billion tokens of OSCAR data~\citep{ortiz-suarez-etal-2020-monolingual} and (2) supervised fine-tuning (SFT)~\citep{NEURIPS2022_b1efde53} on LLaMA-2-7B base model on the WMT17-22 validation dataset. 

\paragraph{Detection results across different training phases.}
Following Section \S \ref{sec:detecing}, we examine the causal impact of logits on different LLM training phases in En $\Rightarrow$ Zh translation of analysis dataset. The results are illustrated in Figure \ref{fig:consistency}, 
compared to the base LLM results in Figure~\ref{fig:identify:en-zh}, LLMs after CPT exhibit significant distributional shifts in translation-crucial heads, whereas changes are minimal after SFT. 
This demonstrates that pre-training stage changes LLMs' core translation capabilities, while supervised fine-tuning primary focuses on localized parameter adjustments without altering their fundamental abilities.
 
\section{Behavioral Patterns Analysis}
\label{method_sec:analyze}
Motivated by the sparse distribution of crucial heads, we now turn to the second research question: ``\textit{What behavioral patterns do translation-crucial components exhibit?}'' by systematically investigating their computational mechanisms through two interpretable diagnostic methods: (1) visualizing attention patterns to characterize the roles of crucial heads (Section \S \ref{sec:characterizing}), and (2) projecting MLP representation to measure correlations with translation-related token embeddings (Section \S \ref{sec:mlp}).

\subsection{Analysis of Attention Head}
\label{sec:characterizing}
Following the findings of \citet{kobayashi-etal-2020-attention}, which demonstrates that attention weights alone fail to explain model behavior, we inspect attention values $\mathbf{O}^{i,j} \in \mathbb{R}^{N \times N}$ (where $N$ denotes the sequence length) to analyze significant token interactions. 
We compute $\mathbf{O}^{i,j} = \sum_{n=1}^{N} \mathbf{A}_n^{i,j}\mathbf{X}_f\mathbf{W}_V^{i,j}$ over reference data $\mathbf{X}_f$ for each analyzed head $(i,j)$, where, $\mathbf{A}^n_{i,j}$ denotes attention weights and $\mathbf{W}_V^{i,j}$ value matrix. 
Each head's role is determined by the salient feature of $\mathbf{O}^{i,j}_{\{END\}} \in \mathbb{R}^{1 \times N}$ between the END position's Query token and all Key tokens.

\paragraph{Characterizing heads.} 
\begin{figure}[htbp]
\vspace{-20pt} % 通过负值减少上方间距
	\centering
    \subfloat[Source Head $(17, 18)$]{\includegraphics[width=.5\linewidth]{source.pdf}\label{fig:attn_vis:source}}
	\subfloat[Positional Head $(15,11)$]{\includegraphics[width=.5\linewidth]{position.pdf}\label{fig:attn_vis:position}}\\
    \subfloat[Indicator Head $(14,27)$]{\includegraphics[width=.5\linewidth]{indicator_0.pdf}\label{fig:attn_vis:indicator_0}}
	\subfloat[Indicator Head $(14,4)$]{\includegraphics[width=.5\linewidth]{indicator_1.pdf}\label{fig:attn_vis:indicator_1}}
	\caption{The attention values visualization of the role-classified key heads, which show different characteristics of different crucial heads.}
    \label{fig:attn_vis}
\end{figure}
To better understand the ``behavior'' of the translation-crucial heads, we first gain an intuitive insight by visualizing their attention values as shown in the case in Figure \ref{fig:attn_vis}. 
Our findings indicate that these heads exhibit distinct focus patterns across different types of input tokens. 
Building on these patterns and following \citet{voita-etal-2019-analyzing}, we further categorize these heads into three distinct functional roles:
\begin{itemize}
    \item \textbf{Source Heads} demonstrate concentrated attention on source-language tokens, specializing in cross-lingual alignment. These heads facilitate lexical transfer by identifying source language tokens among the input sequence.
    \begin{CJK*}{UTF8}{gbsn} % 设置环境
    \item \textbf{Indicator Heads} exhibit spike-shaped attention patterns on translation-specific indicators (e.g., language identifiers like "English" or "中文", and structural cues like colons), assisting translation mode recognition and syntactic boundary detection.
    \end{CJK*} % 结束环境
    \item \textbf{Positional Heads} predominately attend to adjacent tokens, managing contextual dependencies and resolving grammatical agreement.
\end{itemize}

\begin{figure}[htbp]
    \vspace{-17.5pt} % 通过负值减少上方间距
	\centering
	\subfloat[Zh $\Rightarrow$ En]{\includegraphics[width=.5\linewidth]{zh-en_attention_distribution.pdf}\label{fig:attn_dist:zh-en}}
    \subfloat[En $\Rightarrow$ Zh]{\includegraphics[width=.5\linewidth]{en-zh_attention_distribution.pdf}\label{fig:attn_dist:en-zh}}
	\caption{The attention value distribution of different roles of key heads across Zh $\Leftrightarrow$ En translation tasks.}
    \label{fig:attn_dist}
    \vspace{-6pt} % 通过负值减少上方间距
\end{figure}
\paragraph{Distinct attention distribution across heads.}
To quantitatively analyze the distinct patterns of the crucial heads' roles, we randomly selected 100 samples from the analysis dataset and plotted the distribution of averaged attention values for the three key head roles across two translation directions (Zh $\Leftrightarrow$ En).
Figure \ref{fig:attn_dist} demonstrates that these heads exhibit distinct attention distributions, with minimal focus on tokens outside important input tokens. The source heads primarily attend to source input tokens, the positional heads distribute attention uniformly across the input context, and the indicator heads concentrate on translation task indicator tokens.

Overall, these analyses provide a clear, human-interpretable perspective of why deactivating crucial heads significantly impacts LLM translation.


\subsection{Analysis of MLP} 
\label{sec:mlp}
To analyze linguistic content encoded in the inputs ($MLP_{in}$) and outputs ($MLP_{out}$) of MLP layers, particularly for translation-relevant tokens: translation indicator (IND), source (SRC) and target-language (TGT), we employ the unembedding matrix $W_U$ as a diagnostic probe and $W_U[*]$ denotes the unembedding vectors corresponding to a specific token.
For each token $TOK$, we compute cosine similarities (denoted as $\langle MLP, \text{TOK} \rangle$) between $MLP_{in}$, $MLP_{out}$, and $W_U[\{TOK\}]$ to quantify linguistic information propagation through MLP layers. 
Following \citet{geva-etal-2022-transformer}, we isolate MLP contributions evaluating: \resizebox{\columnwidth}{!}{
    $\frac{MLP_{out} - MLP_{in}}{\|MLP_{out} - MLP_{in}\|} \cdot \frac{W_U[\{TOK\}]}{\|W_U[\{TOK\}]\|}, TOK \in \{\text{IND}, \text{SRC}, \text{TGT}\}$
}. 


\paragraph{MLPs integrate and process translation-related features iteratively, yielding target translation.}
\begin{figure}[!htbp]
\vspace{-12.5pt} % 通过负值减少上方间距
	\centering
    \subfloat[\{SRC\}, \{IND\} Reception]{\includegraphics[width=.5\linewidth]{src.pdf}\label{fig:mlp:src}}
	\subfloat[Generation of \{TGT\}]{\includegraphics[width=.5\linewidth]{tgt.pdf}\label{fig:mlp:tgt}}
	\caption{We investigate the correlation between MLP input or output with translation-related tokens.}
    \label{fig:mlp}
\end{figure}
Figure \ref{fig:mlp} investigates MLP interactions with source and target tokens across 100 En $\Rightarrow$ Zh samples.
Figure \ref{fig:mlp:src} presents that in early layers (1–14), $\langle MLP_{in},\text{SRC} \rangle$ values remain near-zero, indicating minimal source token encoding, consistent with the inactive region before layer 14 in Figure~\ref{fig:identify:en-zh}.
A surge in $\langle MLP_{in},\text{SRC} \rangle$ occurs between layers 15–25, aligning with activation of key attention heads (e.g., 15.12 and 16.26), where source information is encoded into MLP representations for downstream processes.
From layers 25–31, $\langle MLP_{in},\text{SRC} \rangle$ declines, signaling a transition to target translation.
Concurrently, ($\langle MLP_{in},\text{IND} \rangle$) rises after layer 12 and peaks in the final layers, enabling coherent target-language generation.
Critically, control comparisons with random English tokens ($\langle MLP_{in},\text{RAND} \rangle$) remain near-zero throughout all layers, confirming the specificity of the observed patterns.
As shown in Figure~\ref{fig:mlp:tgt}, starting at layer 15, where MLPs start processing target token information, $\langle MLP_{out} - MLP_{in}, W_U[{\text{TARGET}}] \rangle$ sharply increases, while $\langle MLP_{out} - MLP_{in}, W_U[\text{RANDOM}] \rangle$ declines.
This indicates that MLPs progressively execute translation across layers.
Parallel trends in other LLMs (Appendix~\ref{apdx:more_llms}) confirm their generality.

\begin{figure}[htbp]
\vspace{-10pt} % 通过负值减少上方间距
	\centering
    \subfloat[De $\Rightarrow$ Zh]{\includegraphics[width=.5\linewidth]{de-zh_latent.pdf}\label{fig:latent:de-zh}}
	\subfloat[Ru $\Rightarrow$ Zh]{\includegraphics[width=.5\linewidth]{ru-zh_latent.pdf}\label{fig:latent:ru-zh}}
	\caption{We investigate the correlation between intermediate representation with different languages tokens unembedding vector.}
    \label{fig:latent}
\end{figure}


\paragraph{MLP intermediate features exhibit a transition to English-centric latent representation.}
We further investigate the detailed translation process between non-English pairs (e.g., German/Russian $\Rightarrow$ Chinese) by analyzing the word ``\textit{book}''. Quantitative comparisons between $MLP_{out} - MLP_{in}$ representations and cross-lingual semantic embeddings (Figure~\ref{fig:latent}) reveal:
in layers 16–26, similarity with English embeddings surpasses other languages, declining in later layers (25–31).
We hypothesize LLMs employ a “bridge-translation” paradigm—akin to humans using their native language as a mental intermediary—where source inputs are first processed into English-centric latent representations before generating target outputs. This aligns with prior work~\citep{wendler-etal-2024-llamas, zhao2024how}, confirming English’s latent intermediary role in multilingual LLM tasks.

Consolidating these findings, we conclude that LLMs employ attention heads to capture source language and translation indicator tokens, which are forwarded to downstream MLPs. 
MLPs integrate and process these features by transiting towards an English-centric latent representation, finally generating the target translation.

\begin{table*}[!htbp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}rccccccc@{}}
\toprule
\multicolumn{1}{l}{} &
   &
   &
  \multicolumn{3}{c}{\textbf{Translation Tasks}} &
  \multicolumn{2}{c}{\textbf{Generic Tasks}} \\ \cmidrule(l){4-8} 
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Models}}} &
  \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Train\\ Speed\end{tabular}}} &
  \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Tuned\\ Params.\end{tabular}}} &
  \textbf{Zh$\Rightarrow$En} &
  \textbf{De$\Rightarrow$En} &
  \textbf{Ru$\Rightarrow$En} &
  \textbf{MMLU} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Commonsense\\ Reasoning\end{tabular}} \\ \cmidrule(l){4-8} 
\multicolumn{1}{c}{} &
   &
   &
  \multicolumn{3}{c}{\textbf{BLEU$\uparrow$/COMET$\uparrow$/BLEURT$\uparrow$}} &
  \textbf{Acc.} &
  \textbf{Acc.} \\ \midrule
\multicolumn{1}{l}{LLaMA2-7B} &
  - &
  - &
  15.6/73.1/56.6 &
  24.8/76.8/62.1 &
  20.2/73.8/60.3 &
  45.9 &
  55.3 \\
+ Full SFT &
  17sam./sec. &
  6.7B &
  20.4/78.7/63.9 &
  35.4/83.4/70.7 &
  25.8/79.8/67.6 &
  42.6 &
  50.2 \\
+ Targeted SFT &
  33sam./sec. &
  0.27B &
  \compactbetter{21.3/79.1/64.3} &
  \compactbetter{37.1/83.7/71.4} &
  \compactbetter{27.8/80.3/68.4} &
  \compactbetter{46.0} &
  \compactbetter{55.7} \\
+ Random SFT &
  33sam./sec. &
  0.27B &
  \compactworse{16.9/76.9/61.1} &
  \compactworse{32.5/81.6/68.1} &
  \compactworse{23.7/78.2/65.3} &
  \compactbetter{45.9} &
  \compactbetter{54.9} \\ \bottomrule
\end{tabular}%
}
\caption{The overall results of \textbf{X $\Rightarrow$ En} translation on WMT'23/24 and generic tasks. Results surpassing Full SFT are highlighted in \compactbetter{green}, inferior outcomes in \compactworse{red}, and comparable performance in \compactequal{blue}.}
\label{tab:sft_results_xx-en}
\end{table*}

\begin{table*}[!htbp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}rccccccc@{}}
\toprule
\multicolumn{1}{l}{} &
   &
   &
  \multicolumn{3}{c}{\textbf{Translation Tasks}} &
  \multicolumn{2}{c}{\textbf{Generic Tasks}} \\ \cmidrule(l){4-8} 
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Models}}} &
  \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Train\\ Speed\end{tabular}}} &
  \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Tuned\\ Params.\end{tabular}}} &
  \textbf{En$\Rightarrow$Zh} &
  \textbf{En$\Rightarrow$De} &
  \textbf{En$\Rightarrow$Ru} &
  \textbf{MMLU} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Commonsense\\ Reasoning\end{tabular}} \\ \cmidrule(l){4-8} 
\multicolumn{1}{c}{} &
   &
   &
  \multicolumn{3}{c}{\textbf{BLEU$\uparrow$/COMET$\uparrow$/BLEURT$\uparrow$}} &
  \textbf{Acc.} &
  \textbf{Acc.} \\ \midrule
\multicolumn{1}{l}{LLaMA2-7B} &
  - &
  - &
  17.0/74.1/55.9 &
  13.0/64.2/49.1 &
  12.8/70.5/52.4 &
  45.9 &
  55.3 \\
+ Full SFT &
  17sam./sec. &
  6.7B &
  30.3/80.7/62.9 &
  27.9/78.3/63.7 &
  19.5/80.0/63.2 &
  40.2 &
  50.0 \\
+ Targeted SFT &
  33sam./sec. &
  0.27B &
  \compactequal{30.7/81.4/64.3} &
  \compactequal{27.6/78.4/63.8} &
  \compactequal{20.1/80.4/63.6} &
  \compactbetter{46.2} &
  \compactbetter{56.0} \\
+ Random SFT &
  33sam./sec. &
  0.27B &
  \compactworse{26.4/79.3/61.6} &
  \compactworse{22.7/76.2/60.3} &
  \compactworse{15.8/77.9/60.7} &
  \compactbetter{46.1} &
  \compactbetter{55.2} \\ \bottomrule
\end{tabular}%
}
\caption{The overall results of \textbf{En $\Rightarrow$ X} translation on WMT'23/24 and generic tasks.}
\label{tab:sft_results_en-xx}
\end{table*}



\section{Targeted Fine-tune}
\label{method_sec:finetune}
Building on the insights given from two previous investigations, we aim to answer the final question: ``\textit{Can fine-tuning these translation-crucial components enhance LLM translation capability?}'' To address this, we propose a method to fine-tune these components selectively, as detailed in Section \S \ref{subsec:finetune}. We then introduce our experimental setup in Section \S \ref{exp:setting} and further carry out three sets of experiments (Section \S \ref{exp:comparison}, \S \ref{exp:generalization}, and \S \ref{exp:ablation}) to comprehensively evaluate the proposed method.
\subsection{Selectively Fine-tune Crucial Components}
\label{subsec:finetune}
SFT is a common technique for improving translation performance in LLMs~\citep{jiao-etal-2023-parrot,xu2024a}. Building on this, our method selectively updates parameters directly tied to translation tasks (those detected as crucial in Section \S \ref{sec:detecing}) while preserving the remaining. This strategy aims to precisely improve the model's translation capabilities without compromising general functionality.
% The detailed procedure is outlined in Algorithm \ref{alg:sft}.
Given crucial translation-related components $\Theta$, our method computes gradients $G$ for $\Theta$ rather than for the entire set of parameters and iteratively adjusts these parameters. Modifying only a subset of parameters reduces training duration and mitigates interference with the model’s pre-existing capabilities.

\subsection{Experimental Setup}
\label{exp:setting}
We examine three approaches: (1) Full-parameter fine-tuning (Full SFT), (2) Targeted fine-tuning of translation-crucial components (Targeted SFT), and (3) Random-component fine-tuning (Random SFT), where random components match the parameter count of Targeted SFT.
For training, we leverage human-parallel corpora (WMT17–WMT22, Flores-200~\citep{guzman-etal-2019-flores}) following~\citet{xu2024a}, evaluating translation accuracy on WMT23/24 and general-domain benchmarks  (MMLU~\citep{hendrycks2021measuring}, ARC~\citep{allenai:arc}, SIQA~\citep{sap-etal-2019-social}). 
Implementation details are in Appendix~\ref{apdx:training_details}.

Our experiments focus on two goals: (1) comparing Full, Targeted, and Random SFT on Llama-2-7B across six bidirectional translation tasks (English $\Leftrightarrow$ Chinese, German, Russian), and (2) assessing generalization by fine-tuning English $\Rightarrow$ Chinese crucial heads and testing performance on English $\Leftrightarrow$ Japanese/Czech translation tasks.

\subsection{Comparison Experimental Results}
\label{exp:comparison}
As shown in Tables~\ref{tab:sft_results_xx-en} and \ref{tab:sft_results_en-xx}, Targeted SFT demonstrates three key advantages: (1) \textbf{Translation performance improvement}—it significantly improves translation performance across all language directions, particularly in X $\Rightarrow$ En, outperforming Full SFT and far exceeding Random SFT; (2) \textbf{General capabilities preservation}-unlike Full SFT, which degrades non-translation task performance, Targeted SFT maintains baseline generalization; (3) \textbf{Training efficiency}-it modifies fewer than 5\% of parameters and halves training time compared to Full SFT, achieving substantial computational cost savings. Additional results for other LLMs are provided in Appendix~\ref{apdx:exp_results}. 

\subsection{Generalization Evaluation Results}
Table~\ref{tab:gen_results} demonstrates that translation-crucial attention heads exhibit cross-lingual generalization: fine-tuning only the \textbf{En $\Rightarrow$ Zh} crucial heads in Llama-2-7B and evaluating them on other translation directions (\textit{En $\Leftrightarrow$ Cs (Czech)} and \textit{En $\Leftrightarrow$ Ja (Japanese)}) achieves performance gains comparable to full-parameter fine-tuning.
\label{exp:generalization}
\begin{table}[!htbp]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}rcccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Models}}} & \textbf{En$\Rightarrow$Cs} & \textbf{En$\Rightarrow$Ja} & \textbf{Cs$\Rightarrow$En} & \textbf{Ja$\Rightarrow$En} \\ \cmidrule(l){2-5} 
\multicolumn{1}{c}{}          & \multicolumn{4}{c}{\textbf{BLEU$\uparrow$/COMET$\uparrow$/BLEURT$\uparrow$}} \\ \midrule
\multicolumn{1}{l}{LLaMA2-7B} & 4.4/63.6/39.7     & 6.1/73.3/47.4     & 23.7/77.9/65.1    & 10.8/72.9/56.6   \\
+ Full SFT                    & 20.2/80.0/66.5    & 15.2/82.4/56.7    & 31.9/83.1/71.7    & 17.4/79.5/64.1   \\
+ Targeted SFT                & \compactequal{20.8/80.3/66.7}    & \compactequal{15.3/81.9/56.7}    & \compactbetter{33.5/83.5/72.3}    & \compactequal{18.7/80.0/64.7}   \\
+ Random SFT                  & \compactworse{15.8/78.5/63.8}    & \compactworse{11.3/79.9/53.7}    & \compactworse{29.1/81.5/68.8}    & \compactworse{14.0/77.9/62.1}   \\ \bottomrule
\end{tabular}%
}
\caption{WMT'23/24 \textit{En $\Leftrightarrow$ Cs} and \textit{En $\Leftrightarrow$ Ja} Results. Targeted SFT fine-tunes \textbf{En $\Rightarrow$ Zh} crucial heads.}
\label{tab:gen_results}
\vspace{-10pt}
\end{table}

\subsection{Ablation Study of Trainable Components} 
\label{exp:ablation}
\begin{table}[!htbp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}ccccc@{}}
\toprule
              &             &        & \textbf{Zh $\Rightarrow$ En} & \textbf{MMLU} \\ \cmidrule(l){4-5} 
\multirow{-2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Ablating \\ Attention Heads\end{tabular}}} &
  \multirow{-2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Train\\ Speed\end{tabular}}} &
  \multirow{-2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Tuned\\ Params.\end{tabular}}} &
  \textbf{BLEU/COMET/BLEURT} &
  \textbf{Acc.} \\ \midrule
top-8 heads   & 58sam./sec. & 0.017B & 18.7/78.1/63.0               & 46.1          \\
top-16 heads  & 52sam./sec. & 0.033B & 20.0/78.4/63.5               & 45.9          \\
top-32 heads  & 50sam./sec. & 0.067B & 20.4/78.6/63.8               & 45.8          \\
\rowcolor[HTML]{C8E6C9} 
top-64 heads  & 40sam./sec. & 0.134B & \textbf{21.3/79.1/64.3}      & 45.9          \\
top-96 heads  & 36sam./sec. & 0.134B & 21.0/79.0/64.2               & 45.7          \\
top-128 heads & 33sam./sec. & 0.268B & 21.1/79.1/64.4               & 45.5          \\
top-160 heads & 30sam./sec. & 0.335B & 21.3/79.1/64.4               & 45.3          \\ \bottomrule
\end{tabular}%
}
\caption{Ablative experiments on the number of heads. The most cost-effective setting is shown in \compactbetter{green}.}
\label{tab:ablation_heads}
\end{table}
We conduct ablation studies in Zh $\Rightarrow$ En translation on WMT'23/24 to examine how varying the number of fine-tuned attention heads and MLPs affects translation performance, generic capabilities, and training efficiency. As shown in Table~\ref{tab:ablation_heads}, fine-tuning 64 attention heads achieves the optimal balance between performance and computational cost. Table~\ref{tab:ablation_mlp} reveals that increasing MLPs enhances translation performance but more significantly degrades generic capabilities and training speed compared to tuning additional heads.
\begin{table}[!htbp]
\vspace{-10pt}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}rcccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Ablating\\ MLPs\end{tabular}}}} &
  \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Train\\ Speed\end{tabular}}} &
  \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Tuned\\ Params.\end{tabular}}} &
  \textbf{Zh $\Rightarrow$ En} &
  \textbf{MMLU} \\ \cmidrule(l){4-5} 
\multicolumn{1}{c}{}             &             &       & \textbf{BLEU/COMET/BLEURT} & \textbf{Acc.} \\ \midrule
\multicolumn{1}{c}{Top-64 heads} & 33sam./sec. & 0.27B & 21.3/79.1/64.3             & 45.8          \\ \midrule
+top-1 MLP                       & 30sam./sec. & 0.41B & 21.8/79.1/64.5             & 45.7          \\
+top-2 MLP                       & 27sam./sec. & 0.54B & 21.8/79.1/64.5             & 45.6          \\
+top-3 MLP                       & 24sam./sec. & 0.68B & 21.9/79.1/64.5             & 45.3          \\
+top-5 MLP                       & 20sam./sec. & 0.95B & 22.1/79.2/64.6             & 44.2          \\
+all MLP                         & 18sam./sec. & 4.62B & 22.5/79.4/64.7             & 42.8          \\ \bottomrule
\end{tabular}%
}
\caption{Ablative experiments on the number of MLPs.}
\label{tab:ablation_mlp}
\vspace{-15pt}
\end{table}

\section{Conclusion}
This study systematically explores the translation mechanism of LLM by progressively addressing three research questions. We begin by detecting components crucial for translation via path patching and find that only a sparse subset of components (less than 5\%) are indispensable for translation. These heads exhibit specialized functions, extracting translation-related features, while MLPs integrate and process by transiting toward English-centric latent representations. Based on these findings, we found that targeted fine-tuning of merely 64 translation-crucial heads achieves performance parity with full-parameter tuning. 
These findings collectively advance the interpretability of the inner translation mechanism of LLMs.

\section*{Limitations}
This study acknowledges two methodological considerations that guide future research directions. While the intentionally simplified lexical translation task provided crucial experimental control to isolate core mechanisms, extending these findings to more ecologically valid sentence-level contexts would strengthen their practical relevance. Furthermore, although our parameter-aware methodology proves effective across open-source architectures, its applicability to closed-source systems remains theoretically constrained—a limitation that simultaneously highlights the urgent need for developing model-agnostic analysis frameworks in this evolving research domain.

\bibliography{acl_latex}

\clearpage

\appendix

\section{Translation Task Templates and Examples}
\label{apdx:demo}
As a clear case study, we first focus on Chinese due to its prevalence of single-token words and lack of spacing. We analyze Llama-2's vocabulary to identify single-token Chinese words (primarily nouns) with direct single-token English translations. This enables direct comparison of the model’s next-token probabilities for correct Chinese words and their English equivalents. For robustness, we replicate experiments in German, Russian, and French, compiling datasets of 139 Chinese, 120 German, 115 Russian, and 118 French words.

\subsection{Dataset Construction}
\label{apdx:data_construction}
To ensure the next token is unambiguously inferable as a single token, we design translation prompts where $x_{n+1}$ is uniquely determined by the preceding context $x_1...x_n$. Each prompt specifies the source language, word, and target language, requiring the model to predict the translated word. 
\begin{CJK*}{UTF8}{gbsn} % 设置环境
Taking English-to-Chinese as an example, a word translation like ``English: flower - 中文: 花'' (``中文'' means ``Chinese'', ``花'' means ``flower'') might naturally appear in the pretraining corpus.
\end{CJK*} % 结束环境

Such prompts explicitly guide Llama-2 to perform translation by leveraging its pretrained linguistic knowledge.

\subsection{Templates}
\label{apdx:data_template}
We formalize counterfactual prompt generation through systematic grammatical preservation and semantic disruption, operating under two core design principles:

\begin{itemize}
    \item \textbf{Structural Isomorphism}: Maintain original syntactic patterns (interrogative formats, placeholder positions, punctuation) while altering semantic content
    \item \textbf{Targeted Lexical Substitution}: Replace critical components through four operation classes
\end{itemize}

\paragraph{Perturbation Taxonomy}
\label{subsec:perturbation_taxonomy}

The perturbation strategies fall into four principal categories, as detailed in Table~\ref{tab:perturbation_types}:

\begin{table}[!htbp]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{>{\raggedright}p{3cm}p{8cm}}
\toprule
\textbf{Operation Type} & \textbf{Implementation Mechanism} \\
\midrule
Target Nullification & 
Replace language identifiers with non-linguistic concepts (\texttt{\{tgt\_lang\}} $\rightarrow$ ``Void''/``Null'') \\
\addlinespace

Action Distortion & 
Substitute translation verbs with irrelevant actions (``translate'' $\rightarrow$ ``eat''/``delete'') \\
\addlinespace

Semantic Obfuscation & 
Alter task-specific nouns to disrupt functionality (``translation'' $\rightarrow$ ``color''/``flavor'') \\
\addlinespace

Paradox Insertion & 
Introduce self-contradictory modifiers (``into \texttt{\{tgt\_lang\}}'' $\rightarrow$ ``into a silent rock'') \\
\bottomrule
\end{tabular}
}
\caption{Taxonomy of Counterfactual Perturbation Operations}
\label{tab:perturbation_types}
\end{table}

\paragraph{Validation Protocol}
\label{subsec:validation}

The constructed templates undergo rigorous verification:

\begin{enumerate}
    \item \textit{Grammatical Integrity Check}: Measure template fluency via language model perplexity scores (threshold: $\leq$15\% deviation from originals)
    \item \textit{Task Disruption Test}: Verify semantic shift through human annotation (success criterion: $\geq$90\% agreement on functionality removal)
\end{enumerate}

\paragraph{Implementation Advantages}
\label{subsec:advantages}

Our methodology provides three key benefits:
\begin{itemize}
    \item Controlled isolation of template components affecting model behavior
    \item Cross-lingual consistency through placeholder-based design
    \item Reproducible taxonomy enabling systematic ablation studies
\end{itemize}

The counterfactual prompts we used are shown in Table \ref{tab:prompts}

\begin{table*}[!htbp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{p{6cm}p{6cm}p{3cm}}
\toprule
\textbf{Normal Prompt} & \textbf{Counterfactual Prompt} & \textbf{Perturbation Type} \\
\midrule

% Original pairs from user-provided examples
\small\texttt{\{src\_lang\}: "\{src\_word\}" - \{tgt\_lang\}: "\{tgt\_word\}} & 
\small\texttt{\{src\_lang\}: "\{src\_word\}" - There is nothing: "\{tgt\_word\}} & 
Target Nullification \\

\small\texttt{Translate "\{src\_word\}" into \{tgt\_lang\}: "} & 
\small\texttt{Translate "\{src\_word\}" into Nothing: "} & 
Target Nullification \\

\small\texttt{Translate the \{src\_lang\} word "\{src\_word\}" to \{tgt\_lang\}: "} & 
\small\texttt{Translate the \{src\_lang\} word "\{src\_word\}" to Null: "} & 
Target Nullification \\

% Generated counterfactual examples
\small\texttt{From \{src\_lang\}: "\{src\_word\}" to \{tgt\_lang\}: "} & 
\small\texttt{From \{src\_lang\}: "\{src\_word\}" to Nowhere: "} & 
Target Nullification \\

\small\texttt{Provide the translation of "\{src\_word\}" from \{src\_lang\} to \{tgt\_lang\}: "} & 
\small\texttt{Provide the color of "\{src\_word\}" from \{src\_lang\} to \{tgt\_lang\}: "} & 
Action Distortion \\

\small\texttt{Q: How do you say "\{src\_word\}" in \{tgt\_lang\}? A: "} & 
\small\texttt{Q: How do you eat "\{src\_word\}" in \{tgt\_lang\}? A: "} & 
Action Distortion \\

\small\texttt{Q: What is the \{tgt\_lang\} translation "\{src\_word\}"? A: "} & 
\small\texttt{Q: What is the \{tgt\_lang\} flavor "\{src\_word\}"? A: "} & 
Semantic Obfuscation \\

\small\texttt{Translate "\{src\_word\}" into \{tgt\_lang\}: "} & 
\small\texttt{Translate "\{src\_word\}" into a silent rock: "} & 
Paradox Insertion \\

\small\texttt{Q: What is "\{src\_word\}" translated into \{tgt\_lang\}? A: "} & 
\small\texttt{Q: What is "\{src\_word\}" erased into \{tgt\_lang\}? A: "} & 
Action Distortion \\

\small\texttt{From \{src\_lang\}: "\{src\_word\}" - \{tgt\_lang\}: "\{tgt\_word\}} & 
\small\texttt{From \{src\_lang\}: "\{src\_word\}" - Disabled: "\{tgt\_word\}} & 
Action Distortion \\

\bottomrule
\end{tabular}
}
\small\raggedright Note: All placeholders (\{src\_lang\}, \{src\_word\}, etc.) follow actual implementation syntax. Counterfactual perturbations preserve original grammatical structures while altering translation semantics through targeted substitutions.
\caption{Examples of some regular translation prompt templates and counterfactual prompt templates.}
\label{tab:prompts}
\end{table*}

\section{Path Patching for Detecting Components Crucial for LLM Translation}
\label{apdx:path_pathcing}
\begin{algorithm}[!htbp]
\small
\captionsetup{font=small}
\caption{Critical Component Detection via Path Patching}
\label{alg:path-patching}
\begin{algorithmic}[1]
\Require Dataset $\mathcal{D}$ containing factual/counterfactual pairs $(X_{f}, X_{cf})$, model $\mathcal{F}$ with components $C$
\Ensure Node importance scores $\Delta=\delta_1,...,\delta_m$
\For{each data pair $(X_{f}^{(i)}, X_{cf}^{(i)}) \in \mathcal{D}$}
        \State Compute reference activations $\mathbf{H}_f \leftarrow \mathcal{F}(X_{f}^{(i)})$
        \State Compute contrastive activations $\mathbf{H}_{cf} \leftarrow \mathcal{F}(X_{cf}^{(i)})$
        
        \For{each component $c^{(j)} \in C$}
            \State Create hybrid activation map $\mathbf{\widetilde{H}}_f$ where:
            \State \quad $\widetilde{H}_f \leftarrow \begin{cases}
                H_{cf}^k & \text{if } k = c^{(j)} \\
                H_f^k & \text{otherwise}
            \end{cases}$
            
            \State Obtain original logit $y_f \leftarrow \mathcal{F}(X_{f}; \mathbf{H}_f)$
            \State Obtain patched logit $\widetilde{y}_f \leftarrow \mathcal{F}(X_{f}; \mathbf{\widetilde{H}}_f)$
            \State Calculate patched effect: $\delta_j^{(i)} \leftarrow \frac{\widetilde{y}_f - y_f}{y_f + \epsilon}$
        \EndFor
    \EndFor

    \For{each importance score $\delta_i \in \Delta$}
        \State Aggregate across dataset: $\delta_i \leftarrow \frac{1}{|\mathcal{D}|}\sum_{j=1}^{|\mathcal{D}|} \delta_i^{(j)}$
    \EndFor
    \State \Return Node importance scores $\Delta$
\end{algorithmic}
\end{algorithm}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{path_patching.pdf}
    \caption{Illustration of the method ``path patching''. It measures the importance of the selected circuit (\textit{i.e.}, the red lines that originate from Head 30 in Layer 0 to Output) for the transformer in completing the task on reference data.}
    \label{fig:path-patching}
\end{figure*}
\begin{figure*}[htbp]
\vspace{-10pt} % 通过负值减少上方间距
	\centering
    \subfloat[LLaMA2-7B]{\includegraphics[width=.33\linewidth]{en-zh.pdf}\label{fig:apdx:llama2-7b}}
	\subfloat[LLaMA2-13B]{\includegraphics[width=.33\linewidth]{llama2-13b.pdf}\label{fig:apdx:llama2-13b}}
	\subfloat[Mistral-7B]{\includegraphics[width=.33\linewidth]{mistral-7B.pdf}\label{fig:apdx:mistral-7b}}
	\caption{Comparison of the results of path patching experiments on LLaMA2-7B, LLaMA2-13B, and Mistral-7B~\citep{jiang2023mistral7b} across Zh $\Rightarrow$ En translation task. Each square at position $(x,y)$ refers to the $x$th-head in the $y$-th layer. Red (Brown) squares denote heads (mlps) that have a positive impact on predicting the target token, while grey (purple) squares indicate heads (mlps) with a negative effect. For each head/MLP, a darker color indicates a larger logit difference from the original model before patching.}
    \label{fig:apdx:identify_more_llms}
\end{figure*}

\begin{figure*}[htbp]
\vspace{-10pt} % 通过负值减少上方间距
	\centering
    \subfloat[En $\Rightarrow$ De]{\includegraphics[width=.33\linewidth]{en-de.pdf}\label{fig:identify:en-de}}
	\subfloat[En $\Rightarrow$ Fr]{\includegraphics[width=.33\linewidth]{en-fr.pdf}\label{fig:identify:en-fr}}
	\subfloat[En $\Rightarrow$ Ru]{\includegraphics[width=.33\linewidth]{en-ru.pdf}\label{fig:identify:en-ru}}\\
    \subfloat[De $\Rightarrow$ En]{\includegraphics[width=.33\linewidth]{de-en.pdf}\label{fig:identify:de-en}}
	\subfloat[Fr $\Rightarrow$ En]{\includegraphics[width=.33\linewidth]{fr-en.pdf}\label{fig:identify:fr-en}}
	\subfloat[Ru $\Rightarrow$ En]{\includegraphics[width=.33\linewidth]{ru-en.pdf}\label{fig:identify:ru-en}}
	\caption{Importance of heads related to translation across different directions. Each square at position $(x,y)$ refers to the $x$-th head in the $y$-th layer. Red (Brown) squares denote heads (MLPs) that have a positive impact on predicting the target token, while grey (purple) squares indicate heads (MLPs) with a negative effect.}
    \label{fig:identify_en}
\end{figure*}

\begin{figure*}[htbp]
\vspace{-10pt} % 通过负值减少上方间距
	\centering
    \subfloat[LLaMA2-7B]{\includegraphics[width=.33\linewidth]{src.pdf}\label{fig:apdx:analyze_mlps_src_llama2-7b}}
	\subfloat[LLaMA2-13B]{\includegraphics[width=.33\linewidth]{13b-src.pdf}\label{fig:apdx:analyze_mlps_src_llama2-13b}}
	\subfloat[Mistral-7B]{\includegraphics[width=.33\linewidth]{mistral_src.pdf}\label{fig:apdx:analyze_mlps_src_mistral-7b}}
	\caption{We investigate the projection of each MLP layer input ($MLP_{in}$) along the direction of the source language, indicator, and random English tokens (\{SRC\},\{IND\}, and \{RAND\}), respectively.}
    \label{fig:apdx:analyze_mlps_src_more_llms}
\end{figure*}

The computation of large language models (LLMs) can be formalized as a directed acyclic graph (DAG) \citep{wang2023interpretability}, where nodes represent computational components (e.g., attention heads, MLP layers) and edges denote directional data flow between them. Mechanistic interpretability seeks to reverse-engineer neural networks into interpretable algorithms, leveraging computational circuits as a framework. A computational circuit is a subgraph of the model’s computational graph $M$, comprising nodes (e.g., embeddings, attention heads) and edges (e.g., residual connections, projections) that collectively implement specific tasks, such as translation.

To analyze causal relationships within these circuits, we employ \textit{path patching} \citep{goldowsky2023localizing,wang2023interpretability,wei2024interpreting}. Algorithm~\ref{alg:path-patching} formalizes path patching as follows: for each component $c^{(j)}$, we (1) compute reference and counterfactual activations $(H_f,H_{cf})$, (2) create hybrid activations by replacing $c^{(j)}$’s activations with $H_{cf}$ while keeping others at $H_f$, (3) compute logit differences ($\delta_j$) between original and patched outputs, and (4) aggregate $\delta_j$ across the dataset to quantify $c^{(j)}$’s task-critical importance. This method isolates the causal effect between a \textit{Sender} node (e.g., Head 30 in Layer 0) and a \textit{Receiver} node (e.g., the output layer) by perturbing the Sender’s activations with $X_{cf}$ while freezing other nodes with $X_{f}$. . As illustrated in Figure~\ref{fig:path-patching}, activations from all nodes are first recorded. A hard intervention replaces the Sender’s activations with those from $X_{cf}$, , propagating the effect through paths $\mathcal{P}$ (residual connections and MLPs). Concurrently, other attention heads are frozen to $X_{f}$ to isolate the Sender’s impact. The resulting logits are compared to quantify the Sender’s causal contribution: significant changes indicate critical paths for task execution.

Since residual streams and MLPs process tokens independently \citep{elhage2021mathematical}, perturbing activations at the END token position suffices to measure effects on next-token prediction.

\section{More Analysis of Other LLMs and Translation Directions}
\label{apdx:more_llms}

\paragraph{Crucial Component Detection.} Figure~\ref{fig:apdx:identify_more_llms} extends key component identification to LLaMA2-13B and Mistral-7B. All three models exhibit sparse localization of translation-critical attention heads (e.g., 17.24, 16.0) in middle layers, despite architectural differences (e.g., LLaMA2-13B’s 40 layers with 40 heads per layer).

Figure~\ref{fig:identify_en} illustrates the detection results for bidirectional translation directions (En $\Rightarrow$ X and X  $\Rightarrow$ En). While the multi-token nature of English tokens results in fewer prominent detection instances, the findings remain consistent with the earlier analysis in Section \S\ref{sec:exp:detection}. Together, these observations support the conclusion that translation mechanisms utilize a sparse subset of attention heads, which are language-agnostic, thereby underscoring their generalization capacity.

\begin{figure*}[htbp]
\vspace{-10pt} % 通过负值减少上方间距
	\centering
    \subfloat[LLaMA2-7B]{\includegraphics[width=.33\linewidth]{tgt.pdf}\label{fig:apdx:analyze_mlps_tgt_llama2-7b}}
	\subfloat[LLaMA2-13B]{\includegraphics[width=.33\linewidth]{13b-tgt.pdf}\label{fig:apdx:analyze_mlps_tgt_llama2-13b}}
	\subfloat[Mistral-7B]{\includegraphics[width=.33\linewidth]{mistral_tgt.pdf}\label{fig:apdx:analyze_mlps_tgt_mistral-7b}}
	\caption{We investigate the projection of each MLP layer ($MLP_{out}-MLP_{in}$) along the direction of the target language, and random English tokens (\{TGT\} (i.e., right translation), and \{RAND\} (i.e., wrong translation)), respectively.}
    \label{fig:apdx:analyze_mlps_tgt_more_llms}
\end{figure*}

\begin{figure*}[htbp]
\vspace{-10pt} % 通过负值减少上方间距
	\centering
    \subfloat[LLaMA2-7B]{\includegraphics[width=.33\linewidth]{ru-zh_latent.pdf}\label{fig:apdx:analyze_mlps_latent_llama2-7b}}
	\subfloat[LLaMA2-13B]{\includegraphics[width=.33\linewidth]{13b-ru-zh_latent.pdf}\label{fig:apdx:analyze_mlps_latent_llama2-13b}}
	\subfloat[Mistral-7B]{\includegraphics[width=.33\linewidth]{mistral_ru-zh_latent.pdf}\label{fig:apdx:analyze_mlps_latent_mistral-7b}}
	\caption{We investigate the projection of each MLP layer ($MLP_{out}-MLP_{in}$) along the direction of the different languages.}
    \label{fig:apdx:analyze_mlps_latent_more_llms}
\end{figure*}
\paragraph{Analysis of Crucial MLPs.} Figures~\ref{fig:apdx:analyze_mlps_src_more_llms} and~\ref{fig:apdx:analyze_mlps_tgt_more_llms} reveal consistent MLP dynamics across models. For MLP input/\{SRC\},\{IND\} similarities, trends follow ascending-descending phases with inflection points at layers (13-18-28) for LLaMA2-7B, (13-18-35) for LLaMA2-13B, and (13-20-28) for Mistral-7B. Similarly, $MLP_{out} - MLP_{in}$ and target token \{TGT\} similarities show stabilization-to-increase patterns with identical inflection layers. This synchronization across models indicates a shared computation mechanism: attention heads initiate translation processing, which MLPs subsequently refine. These results demonstrate robustness across architectures and scales.

\paragraph{Cross-Lingual Bridge Translation.} We extend our analysis to non-English pairs (e.g., French/Japanese  Chinese) by examining token-level dynamics. As shown in Figure~\ref{fig:apdx:analyze_mlps_latent_more_llms}, similarity trends between $MLP_{out} - MLP_{in}$ representations and cross-lingual embeddings align with the bridge-translation hypothesis: in layers 15–24, English-centric latent representations dominate across LLaMA2-13B and Mistral-7B, with similarity declining sharply in layers 25–32. This reinforces the observed paradigm where LLMs internally map source languages to English-like representations before generating target outputs, corroborating findings in multilingual latent alignment studies~\citep{wendler-etal-2024-llamas, zhao2024how}. The consistency across both architectures underscores the generality of English’s intermediary role.




\section{Experimental Setup Details}
\label{apdx:training_details}
Following the gradient rescaling method proposed by~\citep{10.5555/3692070.3694452}, gradients are adjusted by a factor of $\frac{H}{h}$, where $H$ is the total number of attention heads in a layer and $h$ represents the updated heads in the same layer. For model fine-tuning, we use Llama2-7B and Llama2-13B with a learning rate of $2 \times 10^{-5}$, a batch size of 128, and train for 2 epochs. The warm-up ratio is set to 0.02, and weight decay is configured at 0.1. All experiments are conducted on a cluster of 8 NVIDIA A100 80 GB GPUs.

\begin{table*}[!htbp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}rccccccc@{}}
\toprule
\multicolumn{1}{l}{} &
   &
   &
  \multicolumn{3}{c}{\textbf{Translation Tasks}} &
  \multicolumn{2}{c}{\textbf{Generic Tasks}} \\ \cmidrule(l){4-8} 
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Models}}} &
  \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Train\\ Speed\end{tabular}}} &
  \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Tuned\\ Params.\end{tabular}}} &
  \textbf{En$\Rightarrow$Zh} &
  \textbf{En$\Rightarrow$De} &
  \textbf{En$\Rightarrow$Ru} &
  \textbf{MMLU} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Commonsense\\ Reasoning\end{tabular}} \\ \cmidrule(l){4-8} 
\multicolumn{1}{c}{} &
   &
   &
  \multicolumn{3}{c}{\textbf{BLEU$\uparrow$/COMET$\uparrow$/BLEURT$\uparrow$}} &
  \textbf{Acc.} &
  \textbf{Acc.} \\ \midrule
\multicolumn{1}{l}{LLaMA2-7B} &
  - &
  - &
  17.0/74.1/55.9 &
  13.0/64.2/49.1 &
  12.8/70.5/52.4 &
  45.9 &
  55.3 \\
+ Full SFT &
  17sam./sec. &
  6.7B &
  30.3/80.7/62.9 &
  27.9/78.3/63.7 &
  19.5/80.0/63.2 &
  40.2 &
  50.0 \\
+ Targeted SFT &
  33sam./sec. &
  0.27B &
  27.6/80.0/62.5 &
  27.6/78.4/63.8 &
  20.1/80.4/63.6 &
  46.2 &
  56.0 \\
+ Random SFT &
  33sam./sec. &
  0.27B &
  26.4/79.3/61.6 &
  22.7/76.2/60.3 &
  15.8/77.9/60.7 &
  46.1 &
  55.2 \\ \midrule
\multicolumn{1}{l}{LLaMA2-13B} &
  - &
  - &
  23.0/77.5/59.1 &
  17.1/67.7/52.8 &
  15.6/72.9/55.1 &
  55.1 &
  58.4 \\
+ Full SFT &
  12sam./sec. &
  13.0B &
  32.8/81.8/64.4 &
  29.8/80.0/65.8 &
  20.7/81.6/65.0 &
  53.7 &
  56.4 \\
+ Targeted SFT &
  28sam./sec. &
  0.32B &
  33.4/82.2/64.8 &
  30.1/80.1/65.9 &
  21.3/81.8/65.3 &
  54.9 &
  58.1 \\
+ Random SFT &
  28sam./sec. &
  0.32B &
  28.8/80.6/63.3 &
  24.6/78.3/62.9 &
  17.3/80.0/62.8 &
  55.0 &
  58.2 \\ \midrule
\multicolumn{1}{l}{Mistral-7B} &
  - &
  - &
  13.7/68.0/49.6 &
  15.6/63.1/49.3 &
  11.2/65.1/48.1 &
  62.7 &
  59.2 \\
+ Full SFT &
  17sam./sec. &
  6.7B &
  31.1/80.6/63.4 &
  26.5/77.4/62.8 &
  19.6/79.5/62.5 &
  43.0 &
  40.8 \\
+ Targeted SFT &
  33sam./sec. &
  0.27B &
  31.9/82.0/65.1 &
  26.3/78.0/63.2 &
  20.5/79.9/63.1 &
  62.5 &
  59.1 \\
+ Random SFT &
  33sam./sec. &
  0.27B &
  27.5/79.5/61.6 &
  22.2/75.5/59.8 &
  15.6/77.4/60.5 &
  62.4 &
  59.2 \\ \bottomrule
\end{tabular}%
}
\caption{The evaluation results of \textbf{En$\Rightarrow$X} translation (average WMT23 and WMT24 evaluation results) and generic tasks of different SFT strategies.}
\label{apdx-tab:sresults_en-xx}
\end{table*}

\begin{table*}[!htbp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}rccccccc@{}}
\toprule
\multicolumn{1}{l}{} &
   &
   &
  \multicolumn{3}{c}{\textbf{Translation Tasks}} &
  \multicolumn{2}{c}{\textbf{Generic Tasks}} \\ \cmidrule(l){4-8} 
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Models}}} &
  \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Train\\ Speed\end{tabular}}} &
  \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Tuned\\ Params.\end{tabular}}} &
  \textbf{En$\Rightarrow$Zh} &
  \textbf{En$\Rightarrow$De} &
  \textbf{En$\Rightarrow$Ru} &
  \textbf{MMLU} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Commonsense\\ Reasoning\end{tabular}} \\ \cmidrule(l){4-8} 
\multicolumn{1}{c}{} &
   &
   &
  \multicolumn{3}{c}{\textbf{BLEU$\uparrow$/COMET$\uparrow$/BLEURT$\uparrow$}} &
  \textbf{Acc.} &
  \textbf{Acc.} \\ \midrule
\multicolumn{1}{l}{LLaMA2-7B} &
  - &
  - &
  15.6/73.1/56.6 &
  24.8/76.8/62.1 &
  20.2/73.8/60.3 &
  45.9 &
  55.3 \\
+ Full SFT &
  17sam./sec. &
  6.7B &
  20.4/78.7/63.9 &
  35.4/83.4/70.7 &
  25.8/79.8/67.6 &
  42.6 &
  50.2 \\
+ Targeted SFT &
  33sam./sec. &
  0.27B &
  21.7/79.1/64.4 &
  37.1/83.7/71.4 &
  27.8/80.3/68.4 &
  46.0 &
  55.7 \\
+ Random SFT &
  33sam./sec. &
  0.27B &
  16.9/76.9/61.1 &
  32.5/81.6/68.1 &
  23.7/78.2/65.3 &
  45.9 &
  54.9 \\ \midrule
\multicolumn{1}{l}{LLaMA2-13B} &
  - &
  - &
  17.3/74.0/57.8 &
  27.0/78.0/63.8 &
  22.2/74.9/61.5 &
  55.1 &
  58.4 \\
+ Full SFT &
  12sam./sec. &
  13.0B &
  22.4/79.5/65.3 &
  36.9/84.0/71.6 &
  27.8/80.8/68.9 &
  50.0 &
  55.3 \\
+ Targeted SFT &
  28sam./sec. &
  0.32B &
  23.6/80.5/66.5 &
  38.3/84.7/72.7 &
  29.7/81.5/69.3 &
  54.9 &
  58.1 \\
+ Random SFT &
  28sam./sec. &
  0.32B &
  19.0/78.1/63.1 &
  34.2/81.8/68.9 &
  25.3/79.3/66.6 &
  55.5 &
  58.8 \\ \midrule
\multicolumn{1}{l}{Mistral-7B} &
  - &
  - &
  16.9/74.3/58.1 &
  26.6/77.9/63.9 &
  22.6/75.3/62.5 &
  62.7 &
  59.2 \\
+ Full SFT &
  17sam./sec. &
  6.7B &
  19.7/78.4/63.1 &
  32.0/82.2/69.0 &
  24.0/78.7/66.2 &
  40.3 &
  50.3 \\
+ Targeted SFT &
  33sam./sec. &
  0.27B &
  21.2/79.2/64.3 &
  33.7/83.0/70.2 &
  26.4/79.6/66.4 &
  62.9 &
  59.1 \\
+ Random SFT &
  33sam./sec. &
  0.27B &
  16.8/77.1/61.1 &
  29.3/80.6/66.8 &
  21.4/77.1/63.9 &
  62.5 &
  59.3 \\ \bottomrule
\end{tabular}%
}
\caption{The evaluation results of \textbf{X$\Rightarrow$En} translation (average WMT23 and WMT24 evaluation results) and generic tasks of different SFT strategies.}
\label{apdx-tab:sresults_xx-en}
\end{table*}
\section{Comparison Experimental Results on More LLMs}
\label{apdx:exp_results}
We investigate whether our method generalizes to larger LLMs (Llama-2-13B) and diverse architectures (Mistral-7B). As shown in Tables~\ref{apdx-tab:sresults_en-xx} and \ref{apdx-tab:sresults_xx-en}, Targeted SFT exhibits three consistent advantages across LLMs: (1) Enhanced translation performance, particularly in X  En, surpassing Full SFT and significantly outperforming Random SFT; (2) Generalization preservation, maintaining baseline non-translation task performance unlike Full SFT; (3) Training efficiency, modifying fewer than 5\% of parameters and reducing training time by 50\% compared to Full SFT.



\end{document}
