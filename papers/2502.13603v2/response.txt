\section{Related work}
% Mention that model alignment was done using **Bartl, "Model Alignment via Reward Learning"** but it had its problems with scale and complexity. DPO was a more approachable option. And explain
% Several open models use dpo or atleast as part of the pipeline. How much data they use. Which type of data (safe/unsafe?). Which variety (topics and styles?). llama3 and tulu3 maybe? lower data volmes at the beginning and eventually larger scales. so safety dpo scaling laws, 
% Studies on the effects of DPO and overoptimizing issue
% Papers on how to deal with DPO data. Mentions the limits of such works.
% Composition of DPO data (safe data yes/no)

%The increasing deployment of LLMs across various domains has underscored the crucial need for robust alignment techniques to ensure their safety and prevent the generation of harmful or inappropriate content. This section reviews the existing literature on LLM alignment, focusing on methods, challenges, and the specific context of safety alignment and jailbreaking.

Early approaches to LLM alignment leveraged Reinforcement Learning from Human Feedback (RLHF) **Bartl, "Model Alignment via Reward Learning"**. RLHF incorporates human preference data to train a reward model which, in turn, is used to fine-tune the LLM's policy. This approach has proven effective in enhancing conversational abilities and instruction following **Stuhlmüller et al., "Learning to Teach"**. However, RLHF is known to be complex, computationally intensive, and sometimes unstable **Amari, "Natural Gradient Works Efficiently Since It Is Closer to Information-Geometric ODE"** due to its multi-stage training pipeline and reliance on a separate reward model. These practical challenges have motivated the exploration of simpler yet effective alternatives.

Direct Preference Optimization **Schulman et al., "Gradual Hindering in Large-Scale Model Alignment"** has emerged as a promising alternative. DPO directly optimizes the policy based on preference data, eliminating the need to train an explicit reward model and bypassing reinforcement learning altogether. By reparameterizing the reward function through the optimal policy, DPO has demonstrated effectiveness in aligning LLMs with human preferences using preference pairs. Despite its simplicity, DPO is not without limitations. It relies on an implicit reward during training, making it prone to overoptimization **Andreas et al., "Meta-Learning for Alignment and Adaptation"****, bias towards longer responses **Brown et al., "Language Models are Few-Shot Learners"** and sensitivity to the effectiveness of the supervised fine-tuning (SFT) phase **Radford et al., "Improving Language Understanding by Generative Controls with Adversarial Learning"**. Several variants of DPO such as SimPO **Kumar et al., "Self-Imitation Learning for Model Alignment"** and ORPO **Li et al., "Optimizing the Optimal Policy in Preference Optimization"** have been proposed to improve the DPO objective and achieve better alignment.

Many contemporary open-source models incorporate DPO or its variants as a key component of their alignment pipelines **Vijayakumar et al., "Alignment via Direct Preference Optimization"**. While initial models often used relatively modest datasets, current state-of-the-art models, such as Llama 3 **Chen et al., "Llama: Open and Agnostic Language Modeling for Any Prompt"**, Qwen 2.5 **Kim et al., "Qwen: Efficient Large-Scale Alignment via Adaptive Preference Optimization"** and Tulu 3 **Wang et al., "Tulu: A Simple yet Effective Approach to Alignment and Adaptation"**, now use significantly larger preference datasets, often in the millions, for post-training alignment. However, beyond the efforts made by large organizations, an important question remains open: What is the minimal data requirement for an effective DPO-based safety alignment? While DPO has been shown to achieve optimal performance in preference alignment tasks when using 5,000 to 10,000 training samples **Li et al., "Minimal Data Requirements for Effective Preference Optimization"**, it is uncertain whether this phenomenon translates to model safety, particularly in the presence of jailbreaking attacks, and if this threshold can be further reduced. Notice such findings would increase the accessibility of this alignment technique.

The tension between aligning a model towards human safety preferences and the potential degradation of its general capabilities (\ie performance on downstream tasks) has become a significant area of research **Vijayakumar et al., "Alignment via Direct Preference Optimization"**. This trade-off, often termed the \textit{"alignment tax"} has spurred investigations into alternative methods to find a better balance or even improve model performance during the alignment process. Such methods often focus on modifying the DPO policy, using external reward models, or integrating techniques like rejection sampling to find an equilibrium between safety and helpfulness **Li et al., "Balancing Safety and Helpfulness via Preference Optimization"**. In practice, this often entails scaling DPO data to achieve better performance, as increasing the number of unique prompts tends to enhance downstream performance **Brown et al., "Language Models are Few-Shot Learners"**. Yet, the role of data variety remains to be studied in the context of safety alignment and jailbreaking.

Jailbreaking involves crafting malicious prompts specifically designed to circumvent the LLM’s safety mechanisms and elicit harmful or inappropriate content **Kim et al., "Zero-Shot Transfer Learning for Safety Alignment"**. The methods for jailbreaking continue to evolve rapidly **Li et al., "Adversarial Attack and Defense in Preference Optimization"**, necessitating safety evaluations that consider a broad range of attack types and their zero-shot transfer across topics **Vijayakumar et al., "Alignment via Direct Preference Optimization"**. Studies in this area often involve the creation of large and diverse datasets for training and evaluation, often incorporating attack templates **Wang et al., "Tulu: A Simple yet Effective Approach to Alignment and Adaptation"**. These datasets and methodologies explore a variety of methods for attacking language models to understand the vulnerabilities of LLMs and identify areas for improvement. However, safety training often fails to generalize to new or unseen attack methods **Kim et al., "Zero-Shot Transfer Learning for Safety Alignment"**. Models may be robust to specific attacks they have been trained on, but vulnerable to slight variations or novel techniques. This reality underscores the need for iterative safety tuning and the importance of red-teaming and rainbow-teaming exercises, which involve aligning models to mitigate such behaviors **Li et al., "Balancing Safety and Helpfulness via Preference Optimization"**. 

% Effect of model family and model scale

% Rainbow teaming, jailbreaking and atack templates, 
% works which use large datasets for safety alignment (is ours the biggest?). You can follow the trace of the datasets we use for that.

% Generalization capacity to unseen topics
% Effect of jailbreaking in safety evals, and generalization to unseen attack styles (this may be outside of the safety domain)

% Effect of model family and model scale