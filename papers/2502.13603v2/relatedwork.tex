\section{Related work}
% Mention that model alignment was done using RLhF but it had its problems with scale and complexity. DPO was a more approachable option. And explain
% Several open models use dpo or atleast as part of the pipeline. How much data they use. Which type of data (safe/unsafe?). Which variety (topics and styles?). llama3 and tulu3 maybe? lower data volmes at the beginning and eventually larger scales. so safety dpo scaling laws, 
% Studies on the effects of DPO and overoptimizing issue
% Papers on how to deal with DPO data. Mentions the limits of such works.
% Composition of DPO data (safe data yes/no)

%The increasing deployment of LLMs across various domains has underscored the crucial need for robust alignment techniques to ensure their safety and prevent the generation of harmful or inappropriate content. This section reviews the existing literature on LLM alignment, focusing on methods, challenges, and the specific context of safety alignment and jailbreaking.

Early approaches to LLM alignment leveraged Reinforcement Learning from Human Feedback (RLHF)~\cite{ouyang2022traininglanguagemodelsfollow, christiano2023deepreinforcementlearninghuman}. RLHF incorporates human preference data to train a reward model which, in turn, is used to fine-tune the LLM's policy. This approach has proven effective in enhancing conversational abilities and instruction following ~\cite{bai2022traininghelpfulharmlessassistant, stiennon2022learningsummarizehumanfeedback}. However, RLHF is known to be complex, computationally intensive, and sometimes unstable~\cite{ramamurthy2023reinforcementlearningnotnatural} due to its multi-stage training pipeline and reliance on a separate reward model. These practical challenges have motivated the exploration of simpler yet effective alternatives.

Direct Preference Optimization \cite{rafailov2024directpreferenceoptimizationlanguage} has emerged as a promising alternative. DPO directly optimizes the policy based on preference data, eliminating the need to train an explicit reward model and bypassing reinforcement learning altogether. By reparameterizing the reward function through the optimal policy, DPO has demonstrated effectiveness in aligning LLMs with human preferences using preference pairs. Despite its simplicity, DPO is not without limitations. It relies on an implicit reward during training, making it prone to overoptimization~\cite{rafailov2024scalinglawsrewardmodel}, bias towards longer responses~\cite{park2024disentanglinglengthqualitydirect} and sensitivity to the effectiveness of the supervised fine-tuning (SFT) phase~\cite{feng2024analyzingunderstandinglimitationsdpo}. Several variants of DPO such as SimPO~\cite{meng2024simposimplepreferenceoptimization} and ORPO~\cite{hong2024orpomonolithicpreferenceoptimization} have been proposed to improve the DPO objective and achieve better alignment.

Many contemporary open-source models incorporate DPO or its variants as a key component of their alignment pipelines \cite{Intel,zhu2024starlingb,tunstall2023zephyrdirectdistillationlm}. While initial models often used relatively modest datasets, current state-of-the-art models, such as Llama 3~\cite{dubey2024llama3herdmodels}, Qwen 2.5~\cite{qwen2025qwen25technicalreport} and Tulu 3~\cite{lambert2024tulu3pushingfrontiers}, now use significantly larger preference datasets, often in the millions, for post-training alignment. However, beyond the efforts made by large organizations, an important question remains open: What is the minimal data requirement for an effective DPO-based safety alignment? While DPO has been shown to achieve optimal performance in preference alignment tasks when using 5,000 to 10,000 training samples~\cite{saeidi2024insightsalignmentevaluatingdpo}, it is uncertain whether this phenomenon translates to model safety, particularly in the presence of jailbreaking attacks, and if this threshold can be further reduced. Notice such findings would increase the accessibility of this alignment technique.

The tension between aligning a model towards human safety preferences and the potential degradation of its general capabilities (\ie performance on downstream tasks) has become a significant area of research~\cite{wolf2024tradeoffsalignmenthelpfulnesslanguage}. This trade-off, often termed the \textit{"alignment tax"} has spurred investigations into alternative methods to find a better balance or even improve model performance during the alignment process. Such methods often focus on modifying the DPO policy, using external reward models, or integrating techniques like rejection sampling to find an equilibrium between safety and helpfulness~\cite{su2024missionimpossiblestatisticalperspective,anonymous2024safedpo,liu2024enhancingllmsafetyconstrained,gallego2024configurablesafetytuninglanguage, kim2024adversarialdpoharnessingharmful, khaki2024rsdpohybridrejectionsampling}. In practice, this often entails scaling DPO data to achieve better performance, as increasing the number of unique prompts tends to enhance downstream performance~\cite{lambert2024tulu3pushingfrontiers}. Yet, the role of data variety remains to be studied in the context of safety alignment and jailbreaking.

Jailbreaking involves crafting malicious prompts specifically designed to circumvent the LLMâ€™s safety mechanisms and elicit harmful or inappropriate content~\cite{chao2024jailbreakingblackboxlarge}. The methods for jailbreaking continue to evolve rapidly~\cite{chowdhury2024breakingdefensescomparativesurvey,yi2024jailbreakattacksdefenseslarge}, necessitating safety evaluations that consider a broad range of attack types and their zero-shot transfer across topics~\cite{shaikh2023secondthoughtletsthink,li2024deepinceptionhypnotizelargelanguage,wei2024jailbreakguardalignedlanguage,ding2024wolfsheepsclothinggeneralized,chen2024redteaminggpt4vgpt4v}. Studies in this area often involve the creation of large and diverse datasets for training and evaluation, often incorporating attack templates~\cite{liu2024autodangeneratingstealthyjailbreak,yu2024gptfuzzerredteaminglarge,chao2024jailbreakingblackboxlarge,mehrotra2024treeattacksjailbreakingblackbox,fernando2023promptbreederselfreferentialselfimprovementprompt}. These datasets and methodologies explore a variety of methods for attacking language models to understand the vulnerabilities of LLMs and identify areas for improvement. However, safety training often fails to generalize to new or unseen attack methods~\cite{mou2024sgbenchevaluatingllmsafety}. Models may be robust to specific attacks they have been trained on, but vulnerable to slight variations or novel techniques. This reality underscores the need for iterative safety tuning and the importance of red-teaming and rainbow-teaming exercises, which involve aligning models to mitigate such behaviors~\cite{ganguli2022redteaminglanguagemodels,perez2022redteaminglanguagemodels,samvelyan2024rainbowteamingopenendedgeneration}. 

% Effect of model family and model scale

% Rainbow teaming, jailbreaking and atack templates, 
% works which use large datasets for safety alignment (is ours the biggest?). You can follow the trace of the datasets we use for that.

% Generalization capacity to unseen topics
% Effect of jailbreaking in safety evals, and generalization to unseen attack styles (this may be outside of the safety domain)

% Effect of model family and model scale