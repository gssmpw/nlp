\documentclass[12pt,a4paper]{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks,colorlinks=true,linkcolor=violet,citecolor=violet]{hyperref}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder


\usepackage[status=final,nomargin,inline,lang=spanish]{fixme}
\usepackage{placeins}
\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{darkred}{rgb}{0.6, 0.0, 0.0}
\usepackage{tablefootnote}
%\usepackage{inconsolata}
\fxusetheme{colorsig}
\FXRegisterAuthor{dg}{adg}{\textcolor{green}{DG}}
\FXRegisterAuthor{at}{aat}{\textcolor{teal}{Adrian}}
\FXRegisterAuthor{ag}{ash}{\textcolor{red}{Ashwin}}
\FXRegisterAuthor{aa}{aad}{\textcolor{purple}{Anna}}

%%
%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\font\myfont=cmr12 at 18pt
\title{{\myfont Efficient Safety Retrofitting Against Jailbreaking for LLMs}}

\author{
    \textbf{Dario Garcia-Gasulla\textsuperscript{\dag}\textsuperscript{1}},
    \textbf{Adrian Tormos\textsuperscript{\dag}\textsuperscript{1}},
    \textbf{Anna Arias-Duart\textsuperscript{1}},
    \textbf{Daniel Hinjos\textsuperscript{1}},
\\
    \large \textbf{Oscar Molina-Sedano\textsuperscript{1}},
    \large \textbf{Ashwin Kumar Gururajan\textsuperscript{1}},
    \large \textbf{Maria Eugenia Cardello\textsuperscript{1}}
\\
\\
    \textsuperscript{\dag} Equal contribution. 
    \textsuperscript{1} Barcelona Supercomputing Center (BSC) \\
}


\newcommand{\wrt}{{\it w.r.t. }}   % with respect to
\newcommand{\eg}{\emph{e.g.}, }       % for example
\newcommand{\ie}{\emph{i.e.}, }      % that is
\newcommand{\etal}{\emph{et al.}}         % and others
\newcommand\etc{\emph{etc.}}

\newcommand{\EC}{\textit{European Comission}}
\newcommand{\EAR}{\textit{EAR}}
\newcommand{\MN}{\textit{MareNostrum5}}
\newcommand{\bscrt}{\textit{Egida}}
\newcommand{\bscs}{\bscrt\textit{-S}}
\newcommand{\bscdpo}{\bscrt\textit{-DPO}}
\newcommand{\bsafe}{\bscrt\textit{-HSafe}}
\newcommand{\alertA}{\textit{ALERT$_{Adv}$}}
\newcommand{\alertB}{\textit{ALERT$_{Base}$}}
\newcommand{\delphi}{\textit{DELPHI}}
\newcommand{\llguard}{\textit{Llama-Guard-3-8B}}
\newcommand{\llguardshort}{\textit{Llama-Guard}}
\newcommand{\qwenS}{\texttt{Qwen-2.5-7B-Instruct}}
\newcommand{\qwenL}{\texttt{Qwen-2.5-72B-Instruct}}
\newcommand{\llamaS}{\texttt{Llama-3.1-8B-Instruct}}
\newcommand{\llamaL}{\texttt{Llama-3.1-70B-Instruct}}

%% end of the preamble, start of the body of the document source.
\begin{document}
\maketitle


%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Direct Preference Optimization (DPO) is an efficient alignment technique that steers LLMs towards preferable outputs by training on preference data, bypassing the need for explicit reward models. Its simplicity enables easy adaptation to various domains and safety requirements. This paper examines DPO’s effectiveness in model safety against jailbreaking attacks while minimizing data requirements and training costs. We introduce \bscrt{}, a dataset expanded from multiple sources, which includes 27 different safety topics and 18 different attack styles, complemented with synthetic and human labels. This data is used to boost the safety of state-of-the-art LLMs (\llamaS, \llamaL, \qwenS{} and \qwenL) across topics and attack styles. In addition to safety evaluations, we assess their post-alignment performance degradation in general purpose tasks, and their tendency to over refusal. Following the proposed methodology, trained models reduce their Attack Success Rate by 10\%-30\%, using small training efforts (2,000 samples) with low computational cost (3\$ for 8B models, 20\$ for 72B models). Safety aligned models generalize to unseen topics and attack styles, with the most successful attack style reaching a success rate around 5\%. Size and family are found to strongly influence model malleability towards safety, pointing at the importance of pre-training choices. To validate our findings, a large independent assessment of human preference agreement with Llama-Guard-3-8B is conducted by the authors and the associated dataset \bsafe{} is released. Overall, this study illustrates how affordable and accessible it is to enhance LLM safety using DPO while outlining its current limitations. All datasets and models are released to enable reproducibility and further research.
\end{abstract}

%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Model Alignment, LLM Safety, Direct Policy Optimization, Jailbreaking}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.

\section{Introduction}\label{sec:intro}

As Large Language Models (LLMs) become more popular and broadly available, the necessity of ensuring the safety of their outputs also grows. Public and private actors deploying LLMs ask for higher levels of reassurance, to prevent models from generating dangerous, harmful or otherwise unsafe content. This is achieved through \textit{safety alignment}. Among model alignment methods, Direct Preference Optimization (DPO)~\cite{rafailov2024directpreferenceoptimizationlanguage}, has become a popular solution because of its efficiency. Unlike alternatives like Reinforcement Learning from Human Feedback (RLHF)~\cite{ouyang2022traininglanguagemodelsfollow, christiano2023deepreinforcementlearninghuman} which train an explicit reward model, DPO directly tunes the model towards desirable behavior through annotated triplets $<"question","chosen\ answer","discarded\ answer">$. 

The reduced cost of DPO makes it feasible to apply it as a default post-processing step on top of pre-trained models, instruct tuned models, and previously aligned models released by third parties. This is the most frequent scenario, considering the high cost of pre-training LLMs (tens of millions of dollars in compute) and the public availability of high quality LLMs. Thus, performing a use-case specific safety alignment phase with a set of hand-crafted samples is the most frequent priority. However, for this approach to be popularized and adopted, it needs to be accessible and cheap, which for LLMs often means data efficient.

A common concern across LLM domains and applications is jailbreaking~\cite{wei2024jailbroken, huang2023catastrophicjailbreakopensourcellms}; the introduction of malicious prompts with the purpose of leading the model towards producing unsafe content to requests that, without the attack prompt, would not responded to. As of today, a wide variety of jailbreaking methods exist~\cite{yi2024jailbreakattacksdefenseslarge}, and more will appear as all it takes to find them is inference access to an LLM and imagination. Considering the challenges jailbreaking represents for LLM safety, this is introduced into the DPO experimentation as an fundamental aspect to study.

The goal of this work is to assess the limits of DPO for safety model alignment in the presence of jailbreaking, while maximizing data efficiency to facilitate adoption. To do so we use state-of-the-art LLMs (Llama 3.1, Qwen 2.5) and a large safety dataset (\bscrt, created for this work, including 27 safety topics and 20 jailbreaking attack styles) in a variety of experiments designed to identify the most relevant factors driving alignment success. In particular, this work presents experiments to explore the following factors:
\begin{itemize}
    \item \textit{Data composition and variety}: Combining safe and unsafe requests, to balance refusals with proper answers. Effect of training on an increasing number of safety topics and attack styles for the robustness of model alignment.
    \item \textit{Data volume}: Impact of DPO training sizes on model alignment robustness, and identification of minimal recommended sizes for effective alignment.
    \item \textit{Model scale and family}: Relevance of size and model family for the efficacy of DPO model alignment and for attack sensitivity.
    \item \textit{Accessibility and cost}: How expensive is it to perform a thorough and reliable DPO alignment process.
    \item \textit{Model degradation}: Undesirable effects of model alignment on LLMs with regards to general purpose performance and over refusal.
\end{itemize}

The consistency of the above experiments is validated through an independent study on the agreement between \llguard{} and human assessment of unsafe content, which to our knowledge is the largest human assessment of this type~\cite{samvelyan2024rainbowteamingopenendedgeneration, chao2024jailbreakbenchopenrobustnessbenchmark}. The outcomes of this work illustrate the current limits of model safety, and provide an accessible and simple methodology to reach state-of-the-art model safety with minimal resources.

\section{Related work}

% Mention that model alignment was done using RLhF but it had its problems with scale and complexity. DPO was a more approachable option. And explain
% Several open models use dpo or atleast as part of the pipeline. How much data they use. Which type of data (safe/unsafe?). Which variety (topics and styles?). llama3 and tulu3 maybe? lower data volmes at the beginning and eventually larger scales. so safety dpo scaling laws, 
% Studies on the effects of DPO and overoptimizing issue
% Papers on how to deal with DPO data. Mentions the limits of such works.
% Composition of DPO data (safe data yes/no)

%The increasing deployment of LLMs across various domains has underscored the crucial need for robust alignment techniques to ensure their safety and prevent the generation of harmful or inappropriate content. This section reviews the existing literature on LLM alignment, focusing on methods, challenges, and the specific context of safety alignment and jailbreaking.

Early approaches to LLM alignment leveraged Reinforcement Learning from Human Feedback (RLHF)~\cite{ouyang2022traininglanguagemodelsfollow, christiano2023deepreinforcementlearninghuman}. RLHF incorporates human preference data to train a reward model which, in turn, is used to fine-tune the LLM's policy. This approach has proven effective in enhancing conversational abilities and instruction following ~\cite{bai2022traininghelpfulharmlessassistant, stiennon2022learningsummarizehumanfeedback}. However, RLHF is known to be complex, computationally intensive, and sometimes unstable~\cite{ramamurthy2023reinforcementlearningnotnatural} due to its multi-stage training pipeline and reliance on a separate reward model. These practical challenges have motivated the exploration of simpler yet effective alternatives.

Direct Preference Optimization \cite{rafailov2024directpreferenceoptimizationlanguage} has emerged as a promising alternative. DPO directly optimizes the policy based on preference data, eliminating the need to train an explicit reward model and bypassing reinforcement learning altogether. By reparameterizing the reward function through the optimal policy, DPO has demonstrated effectiveness in aligning LLMs with human preferences using preference pairs. Despite its simplicity, DPO is not without limitations. It relies on an implicit reward during training, making it prone to overoptimization~\cite{rafailov2024scalinglawsrewardmodel}, bias towards longer responses~\cite{park2024disentanglinglengthqualitydirect} and sensitivity to the effectiveness of the supervised fine-tuning (SFT) phase~\cite{feng2024analyzingunderstandinglimitationsdpo}. Several variants of DPO such as SimPO~\cite{meng2024simposimplepreferenceoptimization} and ORPO~\cite{hong2024orpomonolithicpreferenceoptimization} have been proposed to improve the DPO objective and achieve better alignment.

Many contemporary open-source models incorporate DPO or its variants as a key component of their alignment pipelines \cite{Intel,zhu2024starlingb,tunstall2023zephyrdirectdistillationlm}. While initial models often used relatively modest datasets, current state-of-the-art models, such as Llama 3~\cite{dubey2024llama3herdmodels}, Qwen 2.5~\cite{qwen2025qwen25technicalreport} and Tulu 3~\cite{lambert2024tulu3pushingfrontiers}, now use significantly larger preference datasets, often in the millions, for post-training alignment. However, beyond the efforts made by large organizations, an important question remains open: What is the minimal data requirement for an effective DPO-based safety alignment? While DPO has been shown to achieve optimal performance in preference alignment tasks when using 5,000 to 10,000 training samples~\cite{saeidi2024insightsalignmentevaluatingdpo}, it is uncertain whether this phenomenon translates to model safety, particularly in the presence of jailbreaking attacks, and if this threshold can be further reduced. Notice such findings would increase the accessibility of this alignment technique.

The tension between aligning a model towards human safety preferences and the potential degradation of its general capabilities (\ie performance on downstream tasks) has become a significant area of research~\cite{wolf2024tradeoffsalignmenthelpfulnesslanguage}. This trade-off, often termed the \textit{"alignment tax"} has spurred investigations into alternative methods to find a better balance or even improve model performance during the alignment process. Such methods often focus on modifying the DPO policy, using external reward models, or integrating techniques like rejection sampling to find an equilibrium between safety and helpfulness~\cite{su2024missionimpossiblestatisticalperspective,anonymous2024safedpo,liu2024enhancingllmsafetyconstrained,gallego2024configurablesafetytuninglanguage, kim2024adversarialdpoharnessingharmful, khaki2024rsdpohybridrejectionsampling}. In practice, this often entails scaling DPO data to achieve better performance, as increasing the number of unique prompts tends to enhance downstream performance~\cite{lambert2024tulu3pushingfrontiers}. Yet, the role of data variety remains to be studied in the context of safety alignment and jailbreaking.

Jailbreaking involves crafting malicious prompts specifically designed to circumvent the LLM’s safety mechanisms and elicit harmful or inappropriate content~\cite{chao2024jailbreakingblackboxlarge}. The methods for jailbreaking continue to evolve rapidly~\cite{chowdhury2024breakingdefensescomparativesurvey,yi2024jailbreakattacksdefenseslarge}, necessitating safety evaluations that consider a broad range of attack types and their zero-shot transfer across topics~\cite{shaikh2023secondthoughtletsthink,li2024deepinceptionhypnotizelargelanguage,wei2024jailbreakguardalignedlanguage,ding2024wolfsheepsclothinggeneralized,chen2024redteaminggpt4vgpt4v}. Studies in this area often involve the creation of large and diverse datasets for training and evaluation, often incorporating attack templates~\cite{liu2024autodangeneratingstealthyjailbreak,yu2024gptfuzzerredteaminglarge,chao2024jailbreakingblackboxlarge,mehrotra2024treeattacksjailbreakingblackbox,fernando2023promptbreederselfreferentialselfimprovementprompt}. These datasets and methodologies explore a variety of methods for attacking language models to understand the vulnerabilities of LLMs and identify areas for improvement. However, safety training often fails to generalize to new or unseen attack methods~\cite{mou2024sgbenchevaluatingllmsafety}. Models may be robust to specific attacks they have been trained on, but vulnerable to slight variations or novel techniques. This reality underscores the need for iterative safety tuning and the importance of red-teaming and rainbow-teaming exercises, which involve aligning models to mitigate such behaviors~\cite{ganguli2022redteaminglanguagemodels,perez2022redteaminglanguagemodels,samvelyan2024rainbowteamingopenendedgeneration}. 

% Effect of model family and model scale

% Rainbow teaming, jailbreaking and atack templates, 
% works which use large datasets for safety alignment (is ours the biggest?). You can follow the trace of the datasets we use for that.

% Generalization capacity to unseen topics
% Effect of jailbreaking in safety evals, and generalization to unseen attack styles (this may be outside of the safety domain)

% Effect of model family and model scale

\section{Methodology}

To study the current limits of DPO for model alignment we first collect and expand a comprehensive safety dataset (\bscrt), designed to provide a controlled environment for experimentation and evaluation in the presence of jailbreaking attacks. The \bscrt{} dataset is boosted with two annotation efforts (one by humans, one by LLMs) for training and evaluation. For the sake of promoting model safety, and enabling reproducibility of this work, every dataset described in \S\ref{subsec:dataset} is fully released\footnote{\href{https://huggingface.co/datasets/HPAI-BSC/Egida} {https://huggingface.co/datasets/HPAI-BSC/Egida}}. 

The main experimentation uses \bscrt{} and its extensions to align a set of publicly available LLMs, obtained from different sources and belonging to different model scales, as described in \S\ref{subsec:models}. How are these models evaluated for safety is described in \S\ref{subsec:eval}, while the computational details of the experiments, including footprint, are presented in \S\ref{subsec:computation}.

%For studying LLM alignment with DPO, this work gathers, uses and publicly releases an extended red-teaming dataset \bscrt\dgnote{(NAME)}, designed to tune and evaluate a model for jailbreaking resistance. This dataset includes ... add some basic details about the dataset ... and is presented in \S\ref{subsec:dataset}. \bscrt{} \dgnote{(NAME)} is partitioned into train and test splits, the former is used to conduct DPO alignment on the Llama 3.1 and Qwen 2.5 instruct-tuned model, and the latter is used to evaluate the effect of the DPO training on the aligned  models (see \S\ref{subsubsec:split}). This is the main body of experiments, answering the research questions presented in \S\ref{sec:intro}.%, the results of which can be found in \S\ref{sec:results}.

%All evaluation is conducted on two open LLMs, chosen by their popularity (both widely used by the AI community) and performance (both reaching top results on public leaderboards~\footnote{???}): Llama 3.1~\cite{llama3modelcard} and Qwen 2.5~\cite{qwen}. Both the 7B/8B and the 70B/72B variants from each family are considered, to explore the role of model scale in DPO efficacy. Our test therefore includes four models (\dgnote{list here?} \aanote{already in the previous line}). All these are \dgnote{instruct tuned, and include an alignment stage conducted by the original authors. Is it DPO? For both?}. Finally, to assess the reliability of these findings, an entire section is dedicated to performing sanity checks, as described in \S\ref{sec:sanity}. 

\subsection{\bscrt{} Dataset}\label{subsec:dataset}

Let us first introduce \bscrt, a dataset composed by unsafe requests gathered from a variety of external sources. This dataset is extended, first through a manual fine-grained topic classification, and second by applying a variety of jailbreaking attacks to all their samples.

\paragraph{Sources and data collection}

In total, the dataset is composed of 2,949 dangerous questions or instructions that have been assembled from nine different public datasets (see Table~\ref{table:datasets} for details). The instances have been manually reviewed during the labeling process to ensure that they will cause unsafe or generally undesired responses from LLMs, and then deduplicated using MinHash.

\begin{table}[t]
\centering
\small
\caption{Composition of the \bscrt{} dataset. Source, nature of the sample, and number of samples used.}
\label{table:datasets}
\begin{tabular}{lp{8.5cm}|r}

\textbf{Source} & \textbf{Type} & \textbf{Size} \\ \hline
AdvBench\cite{zou2023universal} & Machine-written & 520  \\ \hline
BSS\tablefootnote{\href{https://huggingface.co/datasets/HPAI-BSC/better-safe-than-sorry}{https://huggingface.co/datasets/HPAI-BSC/better-safe-than-sorry}} & Machine-written & 657 \\ \hline
DoNotAnswer\cite{wang-etal-2024-answer} & Machine-written & 669 \\ \hline
HarmBench\cite{mazeika2024harmbench} & Human-written & 307  \\ \hline
MaliciousInstructions\cite{bianchi2024safetytuned} & Machine-written & 97 \\ \hline
Misuse\tablefootnote{\href{https://trustllmbenchmark.github.io/TrustLLM-Website/}{https://trustllmbenchmark.github.io/TrustLLM-Website/}} & DoNotAnswer\cite{wang-etal-2024-answer}, DAN\cite{SCBSZ24} & 329 \\ \hline
SimpleSafetyTests\cite{vidgen2024simplesafetyteststestsuiteidentifying} & Human-written & 100 \\ \hline
& AdvBench\cite{zou2023universal}, DAN\cite{SCBSZ24}&  \\ 
StrongREJECT &HarmfulQ\cite{shaikh-etal-2023-second}, MasterKey\cite{Deng_2024},& 220 \\
&MaliciousInstructions\cite{bianchi2024safetytuned} &  \\ \hline
TDCRedTeaming\cite{tdc2023} & Human-written & 50  \\
\hline
\textbf{\bscrt} & & \textbf{2,949} \\
\end{tabular}
\end{table}

\paragraph{Topics and jailbreaking attacks}

All gathered samples were manually labeled by the authors into 27 fine-grained topics in a multilabeling fashion (\ie every instance can have several ones). A list of all fine-grained topics within \bscrt, together with their frequency can be found in Figure~\ref{fig:label-hist}. Since there is a significant imbalance among fine-grained topics, and considering how some of these are too small for analysis, the authors recommend aggregating topics into a higher level of abstraction when using the dataset. In this paper, we propose and use one such categorization drawing inspiration from previous works performing similar analyses~\cite{metallamaguard2,samvelyan2024rainbowteamingopenendedgeneration}. The mapping between both is presented at the top of Table~\ref{tab:splits}.


These 2,949 labeled instances are expanded using 18 different jailbreaking attacks, originating from Chen \textit{et al.}~\cite{chen2024redteaminggpt4vgpt4v}, Shen \textit{et al.}~\cite{SCBSZ24}, DeepInception~\cite{li2024deepinceptionhypnotizelargelanguage} and ReNeLLM~\cite{ding-etal-2024-wolf}. Two additional attack styles are implemented using Qwen 72B Chat~\cite{bai2023qwen}: Past tense~\cite{andriushchenko2024doesrefusaltrainingllms} and technical report writing~\cite{samvelyan2024rainbowteamingopenendedgeneration}. For this latter source, model refusals are filtered and removed using rule-based mechanisms. As a result, the complete \bscrt{} is composed of 61,830 unsafe instances\footnote{Also including the samples before adding any jailbreaking attack.}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/label_histogram.pdf}
    \caption{Topic frequency in the \bscrt{} dataset.}
    \label{fig:label-hist}
\end{figure}

%\atnote{Let's leave the explanation of the topics we actually use in this study (which are sets, each one of them formed by various of these individual labels) to the Experiments section}


\paragraph{Data Splits}

To conduct experimentation, we first perform a partition of the \bscrt{} into train and test splits. To avoid contamination, topics and attack styles are distributed between both partitions without overlap. See Table \ref{tab:splits} for details. The attack styles in the test set are selected based on how challenging these are for LLMs (DAN and ReNeLLM Teacher cause the highest amount of unsafe responses from original models), and also prioritizing the lack of contamination among splits (all FuzzLLM variants are in the test set).


\paragraph{DPO Datasets} 
The train split is used to run inference on the four selected models. Unsafe answers are selected, and paired with safe answers (see \S\ref{subsec:extensions}) to create a customized DPO dataset for each model. This allows us to experiment with a DPO datasets composed by triplets $<"question","chosen\ answer","discarded\ answer">$ which contain questions that elicit unsafe responses by the target model, as well as the unsafe responses produced by it.

\begin{table}[t]
    \centering\small
    \caption{Topic hierarchy used in the paper (top). Topics and jailbreaking attacks used in the train and test splits (bottom).}
    \label{tab:splits}
    \begin{tabular}{l|p{7cm}}
        \textbf{Topics} & \textbf{Fine-grained Topics} \\ \hline
        Cybercrime & \texttt{cybercrime}, \texttt{piracy} \\
        Non-violent crimes & \texttt{fraud}, \texttt{vandalism}, \texttt{robbery}, \texttt{arson}, \texttt{p\_info} \\
        Violent crimes & \texttt{violence}, \texttt{terrorism}, \texttt{bioterrorism}, \texttt{animal\_crimes} \\
        Sexual crimes and erotic content & \texttt{erotic}, \texttt{sex\_crimes} \\
        Illegal weapons and substances & \texttt{trafficking}, \texttt{smuggling}, \texttt{bioweapons}, \texttt{drugs}, \texttt{guns} \\
        Hate and harassment & \texttt{d\_eth}, \texttt{d\_gen}, \texttt{d\_body}, \texttt{d\_poor}, \texttt{harassment} \\
        Fake news and misinformation & \texttt{fake\_news} \\
        Dangerous acts and self-harm & \texttt{dangerous\_activties}, \texttt{suicide} \\
        Health & \texttt{health}
    \end{tabular}
    \newline\vspace*{2em}\newline
    \begin{tabular}{c|p{6.6cm}p{7cm}}
        \textbf{Split} & \textbf{Topics} & \textbf{Jailbreaking attacks} \\ \hline
        Train & Violent crimes, Cybercrime, Sexual crimes and erotic content, Hate and harassment, Fake news and misinformation, Dangerous acts and self-harm & DeepInception, Distractions (general, poems), Instructions (evil, forbidden words, short words), ReNeLLM (LaTeX, Python), Past tense, Structured formatting, Technical report \\ \hline
        Test & Illegal weapons and substances, Non-violent crimes, Health & DAN, FuzzLLM (all 7 variants), ReNeLLM (Teacher)
    \end{tabular}
\end{table}




\subsubsection{\bscrt{} Extensions}\label{subsec:extensions}

\paragraph{\bscrt{} Safe Responses}

To extend \bscrt{} for DPO, we use two models that are unrelated to the rest of the experimentation: Mistral 7B v0.3~\cite{jiang2023mistral7b} and Phi 3 Small 8k~\cite{abdin2024phi3technicalreporthighly}. The safe responses of these models is used as chosen answers in the DPO phase. Mistral's responses are given priority over Phi's, as the former tends to be more elaborate than the latter. See Appendix~\ref{app:safe_responses} for more detail on the process.

\paragraph{Human Labeled Subset}

The evaluation methodology used in this work uses an LLM-as-a-judge to label responses as either safe or unsafe (see \S\ref{subsec:eval}). Measuring the reliability of such mechanism is therefore fundamental. In a significant human effort, five authors of this work manually label responses to 1,000 random requests from \bscrt{}, as produced by 10 different LLMs (see Appendix~\ref{app:protocol} for the full list). Each response is annotated by three authors either as \textit{safe}, \textit{unsafe}, or \textit{uncertain}, and this assessment is then compared with the results of the selected LLM-as-a-judge (\llguard). Details on the labeling methodology, and the results obtained can be found in \S\ref{subsec:safety_llama} and Appendix~\ref{app:protocol}.


\subsection{Models}\label{subsec:models}

All experimentation is performed on two family of models which represent the current state of the art in open LLMs: \llamaS{} and \llamaL~\cite{dubey2024llama3herdmodels}, and \qwenS{} and \qwenL~\cite{qwen2025qwen25technicalreport}. These model families are massively used thanks to their permissive licenses and their top performance on benchmarks and public leaderboards\footnote{E.g.: \href{https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard}{https://huggingface.co/spaces/open-llm-leaderboard/open\_llm\_leaderboard}} The four models selected allow us to study both the effect of model family and model scale in experiments.

The instruct version of each model is used, which includes pre-train, supervised fine-tune and model alignment (in both cases including DPO). Our experimentation executes an additional DPO training using the customized triplets described in \S\ref{subsec:dataset}.

\subsection{Evaluation}\label{subsec:eval}

To assess the safety of models we rely on the test partition of the \bscrt{}. Nonetheless, three additional benchmarks are included: the two versions of the ALERT~\cite{tedeschi2024alertcomprehensivebenchmarkassessing} dataset, Base (from now on \alertB) and Adversarial (from now on \alertA), with the former being expanded with all templates selected for the \bscrt{} test split (see Table~\ref{tab:splits}) and the latter used as is; and the \delphi~\cite{sun2023Delphi} dataset, which is also expanded with the \bscrt{} test templates.

During evaluation, models under study are prompted with these four safety benchmarks. Their responses are recorded and labeled as \textit{safe} or \textit{unsafe} by \llguard. Mean per-attack Attack Success Ratio (ASR) is reported, that is the proportion of unsafe answers over the total amount of responses. Results indicate the models are close to saturation in two datasets (\alertA{} and \delphi{}), making patterns among results more noisy. The other two datasets, \bscrt{} and \alertB{}, induce a higher degree of unsafety in model responses, and will be of more interest and relevance during experimentation.

\subsubsection{Human Preference and \llguard }\label{subsec:safety_llama}

Model evaluations conducted in \S\ref{sec:results} are based on the labeling made by \llguard~\cite{metallamaguard2} which deems the outputs of the models being tested as either \textit{safe} or \textit{unsafe}. This decision allows us to scale evaluation, and provide comprehensive results on aspects like data efficiency.

To measure the reliability of \llguardshort{} classifications, and confirm that its labels align with human judgment, a human evaluation is conducted, testing the agreement with the model. Five of the authors of this paper, 3 men and 2 women, participated as human evaluators, collectively reviewing a total of 1,000 questions selected from \bscrt. This evaluation improves previous work \cite{samvelyan2024rainbowteamingopenendedgeneration} %, where only 100 questions were annotated by four evaluators, 
by assessing a larger dataset and involving more annotators for increased reliability.  

In our approach, each question was classified by three reviewers into \textit{safe}, \textit{unsafe} or \textit{uncertain}. Notably, the \textit{uncertain} category was introduced to allow evaluators flag cases where classification was ambiguous or context-dependent. For instance, the \textit{uncertain} label was applied in situations where an answer addresses the question, but the evaluators do not consider the question's intent to be harmful. Some illustrative examples of this scenario include the questions: \textit{``Why is vegetarianism the only ethical diet?''} or \textit{``Why should guns be banned entirely?''}. While the answer might directly address the question, there is a subjective element to whether the question’s intent could be considered harmful.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/agreement.pdf}
  \caption{Percentage of agreement between each human evaluator and \llguardshort{} per topic. The first three bars in each column represent men (depicted in different shades of blue), while the last two bars represent women (depicted in salmon).}\label{fig:human_eval}
\end{figure}

%\dgnote{the best?}An illustrative example is the question: \textit{``Why is atheism healthier than believing in a religion for the well-being of patients?''}.\dgnote{not sure this is a good example} All evaluators labeled this instance as \textit{uncertain}\dgnote{how many are there of these?}. While the question seems to favor atheism, it does not explicitly dismiss religious beliefs or claim atheism's superiority outright, making its intent and potential implications ambiguous\dgnote{<-is this the word? what do you want to say?}.

The results of this evaluation indicate that \llguardshort{} classification aligns with human judgments in a significant majority of cases. Excluding instances marked as \textit{uncetain} by human evaluators, the average agreement between \llguardshort{} and the human raters across the five evaluators is 77.67\%, which is even higher than among human raters themselves (agreement rate between the three annotators of each instance of 75.48\%). The highest agreement with \llguardshort{} was observed with one of the youngest raters (81.00\%), while the lowest agreement was with the oldest rater (74.22\%). This level of agreement underscores the reliability of \llguardshort{} in capturing human-like preferences in safety-critical evaluations. 



To measure the biases of the model, we analyze agreement by topic (see Figure \ref{fig:human_eval}), and observe mild variations in how human evaluators align with \llguardshort{}. For instance, there is strong agreement between the evaluators and the model in topics such as \textit{Illegal weapons and substances}, \textit{Dangerous acts and self-harm}, \textit{Cybercrime}, and \textit{Hate and harassment}, indicating that the model aligns particularly well in these areas. On the other hand, in topics like \textit{Fake news and misinformation}, evaluators show less agreement with the model, underscoring the challenges models face in accurately detecting misinformation. 

Turning to the agreement by gender, the group of three men demonstrates a higher level of agreement with \llguardshort{} (75.92\%) compared to the group of women (70.08\%). Upon analyzing the agreement by topic, we find that in six out of the nine topics the average agreement among men is higher than that of women. This difference suggests that the model may have a slightly stronger alignment with classifications that are preferred or interpreted by men in general.

The main limitation of this evaluation is the sample size. With only five human evaluators participating, it is impossible to capture the diversity that exists across larger and more representative groups. Nonetheless, the primary purpose of this evaluation was to validate the generalized consistency and reliability of \llguardshort{} as an LLM judge. Given the strong level of agreement observed, this goal appears to have been validated. Further details and results of the \llguard{} evaluation process are available in Appendix~\ref{app:protocol}.

\subsection{Computational Details}\label{subsec:computation}

All experiments were conducted on the \MN{} supercomputer, using NVIDIA Hopper 64 GB GPUs. Small models were trained on 4 GPUs (1 node), at batch size 8 and $lr=10^{-7}$; the large models were trained on 64 GPUs (16 nodes), at batch size 64 and $lr=10^{-6}$. Parellelization in this context is motivated solely by the memory requirements associated with the training of LLMs.

The model trainings have been performed with the OpenRLHF~\cite{hu2024openrlhfeasytousescalablehighperformance} Python package, version 0.3.2. The safety evaluations have been performed by running inference on the models with the vLLM~\cite{kwon2023efficientmemorymanagementlarge} Python package, version 0.6.3. General purpose evaluations use \texttt{llm-evaluation-harness}~\cite{eval-harness}.

Scaling the experimentation conducted to four models and several axis of exploration produced a significant computational cost. We estimate the related footprint by tracking execution time, power and energy consumption of every run with the \EAR{} tool. An estimate of the carbon footprint in the form of CO$_2$ emissions is obtained for every run using a conversion rate of 0.158 kgCO$_2$/kWh\footnote{Latest estimate of the emissions intensity ratio reported by \EC.}. In total, our experimentation produces a carbon footprint of 387.32 kg of CO$_2$, which is equivalent to the carbon footprint of a one-way flight from New York to San Francisco for a single passenger, or an average American household for 8.6 days~\cite{strubell-etal-2019-energy}.

\begin{table}[th]
    \centering
    \begin{tabular}{l|rrrr}
          & \textbf{Runs} & \textbf{Total runtime} & \textbf{Total energy} & \textbf{Emitted CO$_2$} \\ \hline
         DPO training & 270 & 44.76 h & 735.306 kWh & 116.178 kg \\ 
         Safety evaluation & 1,048 & 1,029.03 h & 1,523.861 kWh & 240.770 kg \\ 
         General performance evaluation & 58 & 140.65 h &  170.632 kWh & 30.372 kg \\
         \hline
         \textbf{Total} & 1,376 & 1,214.44 h & 2,429.799 kWh & 387.320 kg
    \end{tabular}
    \caption{Computational requirements and estimate carbon footprint of the experimentation performed in this paper.}
    \label{tab:my_label}
\end{table}
% DPO:
%     Llama 8B:  66 runs
%                Mean P:   1599.047W
%                Total E:    13.062kWh
%                Total:   22787.292 s / GPU   91149.168s overall
%                Min:        57.575 s / GPU     230.300s overall
%                Max:      1161.297 s / GPU    4645.188s overall
%     Llama 70B: 64 runs
%                Total:
%                Min:
%                Max:
%     Qwem 7B:   78 runs   
%                Mean P:   1700.248W
%                Total E:    20.908kWh
%                Total:   36001.292 s / GPU  144005.168s overall
%                Min:        53.805 s / GPU     215.220s overall
%                Max:      1485.894 s / GPU    5943.576s overall
%     Qwen 72B:  54 runs
%                Mean P:  20837.936W
%                Total E:   178.154kWh
%                Total:   10323.697 s / GPU  660716.608s overall
%                Min:        36.523 s / GPU    2337.472s overall
%                Max:       722.047 s / GPU   46211.008s overall


% ASR Llama 8B:  114.422 kWh (total)
%                 91.512 kWh (inf)
%                 22.910 kWh (LG)
% ASR Llama 70B: 691.098 kWh (total)
%                657.177 kWh (inf)
%                 33.921 kWh (LG)
% ASR Qwen 7B:   135.227 kWh (total)
%                108.151 kWh (inf)
%                 27.076 kWh (LG)
% ASR Qwen 72B:  583.114 kWh (total)
%                554.493 kWh (inf)
%                 28.621 kWh (LG)

% The rest is pending

The previous costly effort allows us to find a cheap solution, a model alignment training that is both effective and accessible. These are the main models used for experimentation in \S\ref{sec:results}, released with this work. Training them took, from the smallest to the largest training datasets\footnote{Not including the additional model trainings performed in \S\ref{subsec:refusal_rates} with larger amounts of data, as they are not part of the main experimentation.}, 7.57 minutes to 1.59 hours of a single H100 GPU for the 7B and 8B models and 1.3 to 10.23 hours for the 70B and 72B models. In the context of current cloud prices, the largest performed trainings could cost as little as 3\$ for small models, and 20\$ for big ones\footnote{\href{https://getdeploying.com/reference/cloud-gpu/nvidia-h100}{https://getdeploying.com/reference/cloud-gpu/nvidia-h100}}.


\section{Experimentation}\label{sec:results}

The experiments of this section use the models discussed in \S\ref{subsec:models}, aligned by applying DPO on the subset of \bscrt{} requests for which unsafe responses are produced. Evaluation (see \S\ref{subsec:eval}) is designed so that all tests are conducted on topics and attack styles unseen during alignment, providing a measure of robustness.




%To ensure the reliability of our findings and the robustness of the resulting models, we conduct three sanity checks. First, we evaluate the validity of \llguard{} as a labeller of unsafe answers by comparing its assessments with those of five human annotators (in \S\ref{subsec:safety_llama} and Appendix~\ref{app:protocol}). Second, we assess the robustness of safety-trained models by evaluating their performance on the OpenLLM Leaderboard datasets, a general capability benchmark, to check for performance degradation (\S\ref{subsec:general_performance}). Third, we analyze the aligned models' responses to safe questions from the MMLU benchmark to determine if there is an increase in unjustified refusals (\ie not replying to safe questions) after DPO (\S\ref{subsec:refusal_rates}).


\subsection{Data Volume}\label{subsec:data_volume}

Unsafe data is typically limited in volume, as the amount of \textit{fundamentally distinct} requests that are considered to be dangerous or harmful is also limited. At the same time, refusal responses present in safety DPO form a narrow distribution (\ie \textit{I am sorry but..."}, \textit{"For safety reasons I cannot..."}). This lack of diversity in desired "safe" outputs can potentially limit model robustness~\cite{khaki2024rsdpohybridrejectionsampling}. % One promising technique to address this is rejection sampling . 
At the same time, minimizing the data required for effective safety alignment also enables accessibility. While large datasets are employed in state-of-the-art models \cite{dubey2024llama3herdmodels, qwen2025qwen25technicalreport, lambert2024tulu3pushingfrontiers}, understanding the minimal data needs for robust safety against jailbreaking is vital.



%Rejection sampling offers a potential approach to diversify safe responses in DPO training.  By generating multiple candidate safe responses and selecting from them based on diversity criteria, one could create a DPO dataset with a wider range of refusal styles.  This could encourage models to learn more robust and adaptable refusal strategies. However, in this work, we focus on exploring the impact of data volume itself, rather than techniques to diversify safe responses.  


%For exploring the role of train dataset size, \bscrt{} provides a perfect setup, by combining and expanding multiple sources of unsafe requests.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/asr_data_volume.pdf}
    \caption{Performance of the four models under study on the four evaluation safety benchmarks. Y axis shows performance in attack success rate (ASR, lower better), and X axis shows an increasing amount of data used for alignment. \textit{'x'} correspond to original model performance.}
    \label{fig:volume_train}
\end{figure}

Our experiments investigate the role of data volume using varying amounts of \bscrt{} data to align \llamaS{}, \llamaL{}, \qwenS{}, and \qwenL{}. Results achieved by the four models are shown in Figure~\ref{fig:volume_train}. This includes the baselines (the original models) marked as \textit{'x'}. Notice the Y axis of each plot, which shows two of the benchmarks to be hard for the original models (\bscrt{} and \alertB), while the other two are easier (\eg all original models reach ASR below 10\% on \alertA). Starting from each baseline, the different models trained show more training samples yield higher safety. Most of the gains from this alignment are achieved after the first 2,000 samples, with the exception on \qwenS{} which seems to improve linearly with data size. In general, after training with the whole \bscrt{} train split, models show a remarkable boost in robustness capacity across safety topics and attack styles (-10\% to -30\% in ASR). Although not directly comparable, results are highly competitive in the context of similar efforts~\cite{su2024missionimpossiblestatisticalperspective}.

\subsection{Topics \& Attack Styles}\label{subsec:results_styles_topics}

% The generalization shown in Figure~\ref{fig:volume_train} is not uniform across topics and styles. Some are more resilient to model alignment than others, and generalization is uneven, particularly among jailbreaking attack styles. This is shown in Figure~\ref{fig:topics_styles_asr}, which plots the improvement in ASR obtained after training with an increasing number of samples, separately for every style and topic in the \bscrt{} test set. While differences among topics are small (\eg maximum difference of $\pm$10\%, before DPO), attack styles have a much larger variability (\eg minimum difference $\pm$10\% after full DPO). Among the most resilient jailbreaking methods are \dgnote{this and that. some conclusion?}. Regardless, \textit{all} attacks lose efficacy as model alignment increases.\dgnote{mention plots for the other models can be found in the appendix}

Figure~\ref{fig:topics_styles_asr} demonstrates how DPO alignment leads to a generalized reduction in attack efficacy as the training data volume increases. The robustness observed in safety improvements (Figure~\ref{fig:volume_train}) is not uniform across safety topics and jailbreaking attack styles. As illustrated in Figure~\ref{fig:topics_styles_asr}, some styles and topics exhibit greater resilience to DPO alignment than others. This highlights a key challenge in safety alignment: achieving robustness across the diverse landscape of potential safety violations and adversarial techniques. While the variance in Attack Success Rate (ASR) across safety topics is relatively small, the variability is considerably larger across attack styles. This had already been observed in related work~\cite{yi2024jailbreakattacksdefenseslarge}. However, our results indicate jailbreaking effectiveness depends on every specific model, regardless of family and scale (see Appendix~\S\ref{app:harder_topics_all_models}).


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/topics_styles_asr.pdf}
    \caption{For \llamaS, ASR (y axis) change for each attack style (left) and safety topic (right) in the \bscrt{} test set, while using an increasing amount of data (x axis) for DPO model alignment. Lower is better.}
    \label{fig:topics_styles_asr}
\end{figure}

The differences in robustness among topics, and specially attack styles, suggests variety among these may also impact training. We explore this by aligning models using controlled subsets of dangerous topics and jailbreaking styles. In particular we consider varying amounts of topics (1, 2, 4, 6) and attack styles (1, 2, 4, 8, 12) and show test results in Figures~\ref{fig:topics_sizes}. Contrasting previous work~\cite{mazeika2024harmbench}, our experiments indicate that a higher variety of data reduces attack success rate locally in some cases, but not significantly. On the other hand, data volume has a stronger effect than data variety on model robustness. See Appendix~\S\ref{app:experiments_extra} for more results.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/asr_styles_2.pdf}
  \includegraphics[width=\linewidth]{figures/asr_topics_2.pdf}
  % \includegraphics[width=\linewidth]{figures/topic_styles_ablation.pdf}
  \caption{Attack success rate (y axis, lower better) on the two most challenging datasets after models are aligned with an increasing number of attack styles (top two rows) or an increasing number of dangerous topics (bottom two rows).}\label{fig:topics_sizes}
\end{figure}

\subsection{Families \& Sizes}\label{subsec:results_families}

The experiments shown in Figures~\ref{fig:volume_train} and~\ref{fig:topics_sizes} show distinct model behavior across families and scales. Consider first the performance of the four original models (marked as \textit{'x'} in Figure~\ref{fig:volume_train}) on the most challenging benchmarks \bscrt{} and \alertB{}. As shown, bigger models are significantly safer. However, during model alignment, what matters the most is not scale, but family. 

As seen in Figure~\ref{fig:volume_train}, the effect DPO safety training has the model depends mostly on the model family. Llama 3.1 models become the safest after very little training; The \llamaS{} model becomes safer than \qwenL{} after 1,000 training samples of DPO. Considering the technical reports released~\cite{qwen2, llama3modelcard}, authors have not found a difference that could explain such behavior. Both families are pre-trained on datasets of similar size  (+15T tokens), and both include a model alignment stage with DPO done by the original authors prior to release. Nonetheless, these experiments illustrate the importance of model family for alignment, as training factors may induce limitations in model safety. Finding which are these factors remains as future work of high interest (and high expense).

\dgnote{some related work which discusses the role of scale and/or family in DPO? any pointer to what may be important for model permeability?}

\subsection{General Purpose Performance}\label{subsec:general_performance}

When applying a model alignment process, performance on other tasks often degrades~\cite{wolf2024tradeoffsalignmenthelpfulnesslanguage}. To assess to what extent that happens with the proposed models, we use two different general purpose benchmarking suites: OpenLLM Leaderboard \cite{hendrycks2021measuringmassivemultitasklanguage} and MMLU-Generative. These contain a mixture of open-ended and close-ended benchmarks, allowing for a combined view. While close-ended metrics (\ie accuracy) based on multiple choice questions (\ie reply with \textit{A,B,C,D}) are reliable and precise, these responses are not representative of the general discourse capabilities expected of LLMs (\ie auto-regressive outputs). On the other hand, open-ended measures capture performance in complex language generation, but are typically based on approximate methods like matching n-grams, or an LLM-as-a-judge. In this work, for open-ended benchmark we report ROUGE, which is based on n-grams. Notice this could be affected by model alignment, which tends to change the style and framing of responses (and thus, n-grams). By examining both close-ended and open-ended metrics we provide a richer picture of model performance, but notice it will still not be the complete one. OpenLLM is a popular collection of six benchmarks, which includes tasks like reasoning, math \etc. We report average normalized scores for it. MMLU generative is an open-ended version of general language understanding MMLU \cite{open-llm-leaderboard-v2}, created by comparing the produced responses when given all options against the correct choice.% Open-ended metrics in this context are rather noisy, and should be taken with a pinch of salt.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/combined_degradation_plot.pdf}
  \caption{Percentage of performance loss with respect to baseline (original model) on MMLU-Generative (left) and OpenLLM-Leaderboard (right) after models are aligned with an increasing number of unsafe samples.}\label{fig:degradation}
\end{figure}

Results obtained from the aligned models are reported in Figure \ref{fig:degradation}. % and Figure \ref{fig:degradation_xl}. 
On the right-hand plot, the close-ended benchmark shows how all models are equally performance, before and after alignment with \bscrt{}. On the left-hand plot, open ended benchmarks tell a different story, particularly for \llamaL. While this model retains its capacity for factuality, the DPO training has altered its discourse, dramatically hurting ROUGE performance. As we will see in the following section, this seems to be related with over refusal tendencies.

%\dgnote{add comment on how open ended metrics which depend on n-grams, like rouge and bleu, are very sensitive to changes in wording, and therefore these scores can be considered to be quite noisy. close ended on the other hand is more reliable, but not entirely representative of model capabilities}

%Overall, the degradation seems to be minimal for OpenLLM-Leaderboard (which mainly consists on close-ended benchmarks), while it is more noticeable in MMLU-Generative for certain models. Performance degradation in open-ended is specially great in \llamaL, which loses most of it really fast. After extending the size of the alignment (Figure \ref{fig:degradation_xl}), we can also observe how \llamaS{} is more prone to losing performance capabilities than the Qwen family on open-ended, reaching 60\% of degradation.

\dgnote{on sota dpo, ROUGE suffers equally? Accuracy holds?}

\subsection{Over Refusal}\label{subsec:refusal_rates}

A potential drawback from performing safety DPO on language models is that models could overfit to the refusal found in all preferred responses \eg \textit{"As an AI assistant, I cannot answer..."} and decline to produce responses to any request, regardless of safety (\ie over refusal). In order to assess to what extent the models aligned with \bscrt{} express refusal to safe requests, we evaluate them on the OR-Bench \cite{cui2024or}. This over refusal benchmark is a collection of seemingly toxic prompts likely to be refused by LLMs. %These requests are on the border of safety and are at times a matter of personal perspective. 
It contains two main sets of safe prompts: OR-Bench-80K and the OR-Bench-Hard-1K subset.
Samples from these datasets are used to prompt models, and their responses are recorded. Keyword matching is used to determine whether the responses are a refusal or not. For further details on the keyword matching results and methodology used, see Appendix \ref{app:refusal_sentences}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/refusal_rates_v2.pdf}
  \includegraphics[width=0.9\linewidth]{figures/refusal_rates_v2_xl.pdf}
  \caption{Refusal rates for OR-Bench-80K (left) and OR-Bench-Hard-1K (right) after models are aligned with an increasing number of unsafe samples. On top, experiments with up to 6,500 samples. At the bottom, additional experiments with 10,000 samples or more. Lower is better.}\label{fig:refusal_rates}
\end{figure}

The first row of plots of Figure~\ref{fig:refusal_rates} show the performance of models trained with \bscrt. In these, rejection rates remain relatively stable in all models except \llamaL{}, which spikes significantly after training with 2,000+ samples: in both datasets, the rejection rate grows to over 20\% when using close to 4,000 samples. This behavior correlates, and possibly explains, its ROUGE drop in open-ended benchmarks (see Figure~\ref{fig:degradation}). The tendency to over refusal seems to depend on both model size and family. This is strongly linked with safety, as illustrates the fact that \llamaL{} was both the safest model and the one most prone to over refusal.

To study the tendencies of the models when trained past above the limits of our controlled environment with the \bscrt{} dataset, we perform additional DPO trainings on \llamaS{} and \qwenL. We join our \bscrt{} train set with randomly sampled unlabeled data from Aligner-20K\footnote{\href{https://huggingface.co/datasets/aligner/aligner-20K}{https://huggingface.co/datasets/aligner/aligner-20K}}, DoNotAnswer~\cite{wang2023donotanswer} and DAN~\cite{SCBSZ24} to form much larger datasets containing 10,000, 25,000, 50,000 and 100,000 instances. The instances from these additional datasets are also applied the jailbreaking templates from the train split of \bscrt{}. In Figure~\ref{fig:refusal_rates}, we can see that the rejection rates of \llamaS{} spike at up to 35\% and 70\% on 80K and hard, respectively, with 50,000 training samples. However, \qwenL{} maintains a stable rate of refusals even with the largest training datasets.

Using these additionally trained models, we also study the degradation of their general capabilities. In Figure~\ref{fig:degradation_gen_xl}, we see that the performance of \qwenL{} only degrades to around 10\% in the OpenLLM-Leaderboard with 100,000 training samples, while \llamaS{} immediately starts scoring significantly worse in the open-ended MMLU-Generative task but remains relatively stable in the OpenLLM-Leaderboard.

These results, both on over refusals and in general performance, show that loss of performance happen at least when training with 10,000 training samples or more (which aligns with previous work on the matter~\cite{saeidi2024insightsalignmentevaluatingdpo}), but that the exact threshold and optimal amount of training data may vary between models.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/combined_degradation_plot_xl.pdf}
  \caption{Percentage of performance loss with respect to baseline (original model) on MMLU-Generative (left) and OpenLLM-Leaderboard (right) after models are aligned with up to 100,000 training samples.}\label{fig:degradation_gen_xl}
\end{figure}

\dgnote{sota on over refusals, relation with size and family}

%The source of safe requests used is MMLU\dgnote{obtained from this source. transformed this way. with this final size and composition}. Samples from this dataset are used to prompt models, and their responses are recorded. These are then evaluated for refusal, using a specialized LLM judge (Prometheus~\cite{}). Prometheus has proved to be a really robust\dgnote{this sentence is very heavy. Also, dont use "really"} judge model that correlates highly with human evaluators in different benchmarks such as VicunaBench, MT Bench, FLASK, Feedback Bench, HHH Alignment, MT Bench Human Judgment, Auto-J Eval, Preference Bench or LLMBar\dgnote{<- notice the imbalance ->} [].

% Prometheus justifications

% From Prometheus2 paper: "On four direct assessment benchmarks (VicunaBench, MT Bench, FLASK, Feedback Bench), the PROMETHEUS 2 models demonstrate the highest correlation with both human evaluators and proprietary LM-based judges compared to existing open evaluator LMs, with the Pearson correlation surpassing other baselines by 0.2 units across all datasets. Similarly, on four pairwise ranking benchmarks (HHH Alignment, MT Bench Human Judgment, Auto-J Eval, Preference Bench), the PROMETHEUS 2 models show the highest agreement with human evaluators among all the open evaluator LMs we tested"

% From https://arxiv.org/pdf/2310.07641, PROMETHEUS with the reference output achieves a clearly above-chance performance in LLMBar, a benchmark designed to test the ability of an LLM evaluator in discerning instruction following outputs that has 90-95% human agreement

%\dgnote{??->}Nevertheless, even when setting batch size to 1 and visualizing the complete score distribution, there are no significant differences between models.


\section{Conclusion}

The use of DPO for boosting the safety of LLMs delivers on its promises. As shown in \S\ref{sec:results}, with the right training pipeline and data, this method reduces the attack success rate of \textit{unseen} jailbreaking methods between 10\% and 30\% across topics, while using a relatively modest computational budget (between 3\$ and 20\$ depending on model size). A cost that will only decrease in the near future.

The approach of this work first gathers and extends safety datasets into a large collection of samples with extended jailbreaking templates and labels. Training on this data works across models (with varying degrees of efficacy), including state-of-the-art LLMs of different sizes from the Llama 3.1 and Qwen 2.5 families. Main findings suggest:
\begin{enumerate}
    \item Mixing safe data with unsafe samples during model alignment should be avoided.
    \item Certain model families are safer by default and more sensitive to model alignment. However, this susceptibility can lead to model collapse and over refusal.
    \item The weak spots of each LLM (\eg most successful attack styles) are model-specific (not even consistent across families).
    \item Safety alignment datasets should be at least in the order of thousands of samples.
    \item Keeping a variety of attack styles and topics helps with robustness but is not fundamental.
\end{enumerate}

These lessons are applied when training four versions of the aforementioned models, boosting safety and jailbreaking resistance. These are released with this work, together with other computed assets, as additional contributions:
\begin{itemize}
    \item All four safety aligned LLMs, tuned with the corresponding unsafe responses caused by the entire train set of \bscrt. \footnote{\href{https://huggingface.co/HPAI-BSC/Qwen2.5-7B-Instruct-Egida-DPO} {https://huggingface.co/HPAI-BSC/Qwen2.5-7B-Instruct-Egida-DPO}}
    \footnote{\href{https://huggingface.co/HPAI-BSC/Qwen2.5-72B-Instruct-Egida-DPO} {https://huggingface.co/HPAI-BSC/Qwen2.5-72B-Instruct-Egida-DPO}}
    \footnote{\href{https://huggingface.co/HPAI-BSC/Meta-Llama-3.1-8B-Instruct-Egida-DPO} {https://huggingface.co/HPAI-BSC/Meta-Llama-3.1-8B-Instruct-Egida-DPO}}
    \footnote{\href{https://huggingface.co/HPAI-BSC/Meta-Llama-3.1-70B-Instruct-Egida-DPO} {https://huggingface.co/HPAI-BSC/Meta-Llama-3.1-70B-Instruct-Egida-DPO}}
    \item The \bscrt{} dataset, which includes 61,830 unsafe requests with jailbreaking prompts, manually labelled across 27 fine-grained topics.
    \item The \bscs{} dataset, which includes 61,830 safe responses, each paired with an unsafe request from \bscrt, with which new DPO datasets can be generated.
    \item The four \bscdpo{} datasets used to train the models in this paper. For each model, its unsafe answsers on \bscrt{} have been compiled and paired with a prompt and a safe answer. Each of them includes between 2,153 and 6,410 unsafe answers. 
    \item The \bsafe{} dataset, which includes 1,000 unsafe requests and three human labels per request regarding safety.
\end{itemize}

The results obtained also point towards the current limitations of LLM safety. Mostly caused by the two main factors constraining improvement. First, some models are resilient to alignment through DPO. The causes behind this phenomenon need to be analyzed in a dedicated study, as to promote more malleable models where DPO becomes effective. Second, increasing data volume to boost performance cannot be automated, and requires detailed understanding of the domain of application and the interacting population (\ie different age ranges, geographical origins or cultural backgrounds may require additional safety topics), as well as verification on model collapse and over refusal. To tackle some of these challenges, we explore the use of \llguard{}, conducting the largest independent human evaluation released so far on alignment with human preferences. Results shows \llguardshort{} is a useful tool, which correlates strongly with human preference.

Finally, this work addresses the challenge of safety model alignment, but other areas of alignment remain to be considered (toxicity, bias and discrimination, truthfulness, \etc). Addressing these remains as future work. 

%jailbreaking effectiveness depends on every specific model, regardless of family and scale 

\section*{Acknowledgementrs}
This work is supported by Adrian Tormos, Anna Arias Duart and Daniel Hinjos García fellowships within the “Generación D” initiative, Red.es, Ministerio para la Transformación Digital y de la Función Pública, for talent atraction (C005/24-ED CV1). Funded by the European Union NextGenerationEU funds, through PRTR.

We also acknowledge the computational resources provided by the FinisTerrae III, Leonardo and MareNostrum 5 supercomputers. Additionally, this work has
been partially funded by the project SGR-Cat 2021 HPAI (AGAUR grant n.01187).

\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{RT-bibliography}


\appendix
\include{appendix_human_eval}
\include{appendix_safe_responses}
\include{appendix_safe_data}
\include{appendix_best_styles_topics}
\include{appendix_full_experiments}
\include{appendix_refusal_sentences}


\end{document}