\section{Sanity checks for Meta-Llama-Guard-2-8B}\label{app:protocol}

To assess the reliability of \llguardshort{} Guard, we conducted a human evaluation to determine whether human criteria for \textit{safe} and \textit{unsafe} responses align with the outcomes provided by the LLM judge, in this case, \llguardshort. Specifically, the evaluation involved manually reviewing model-generated responses to potentially harmful questions, categorizing them as either \textit{safe} or \textit{unsafe}. 

%define safe and unsafe

\subsection{\textit{Safe} and \textit{unsafe} criteria definition}

To establish the specific criteria for classifying responses, we used the following two definitions: 

\begin{itemize}
    \item \textbf{\textit{Safe}}: A response that does not contain information that could be used for harm, avoid directly answering and unsafe question, or addresses it in a disarming or educational manner.
    \item \textbf{\textit{Unsafe}}: A response that directly answers a malicious question, contains harmful information, or validates harmful intentions. 
    
\end{itemize}

To guide the evaluators, we organize the criteria into a hierarchical decision tree, see Figure \ref{fig:criteria}. For each question, evaluators should follow this decision tree to classify the answers as either \textit{safe} or \textit{unsafe}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/criteria_eval.png}
    \caption{Criteria used by evaluators to classify responses as \textit{safe} or \textit{unsafe}.}
    \label{fig:criteria} 
\end{figure}

\subsection{Interface}

The interface designed for the evaluators displayed each response alongside the original question, the question with the attack (to provide context), and the model's response (see Figure \ref{fig:interface}). Each evaluator could classify the responses as \textit{safe}, \textit{unsafe} or \textit{uncertain}. The \textit{uncertain} option was included for cases where, for example, the evaluator did not perceive any harm in the question itself and, therefore, deemed that the response could not cause harm either. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/interface.png}
    \caption{Interface used by the evaluators to classify responses as \textit{safe}, \textit{unsafe}, or \textit{uncertain}.}
    \label{fig:interface} 
\end{figure}

\subsection{Experiment set-up}


To conduct the evaluation, we designed the selection of responses as follows. We used the 61,830 prompts from BSCRT, derived from 2,949 base questions by using 20 different attack templates (plus the base question itself). These prompts were answered by 10 different models from 5 model families: (1) Phi-3-mini-4k-instruct, (2) Phi-3-medium-4k-instruct~\cite{abdin2024phi3technicalreporthighly}, (3) Yi-1.5-9B-Chat, (4) Yi-1.5-34B-Chat~\cite{young2024yi}, (5) \llamaS, (6) \llamaL~\cite{llama3modelcard}, (7) \qwenS, (8) \qwenL~\cite{qwen2}, (9) Gemma-2-9b-it and (10) Gemma-2-27b-it~\cite{gemma_2024}.



From these responses, we randomly selected 100 for each model, resulting in a total of 1,000 responses to evaluate. Each response was reviewed and classified by three evaluators.





 

\subsection{Evaluators}

The evaluation was conducted by five human evaluators from diverse demographic backgrounds. The participants represented different age groups (21, 26, 26, 31, 46) and geographical locations, including \MN .
%various regions of Spain (Extremadura, Valencia, and Catalonia), as well as one evaluator from Argentina. 
We also aimed for gender balance, with a composition of two women and three men.


\subsection{Results}

The main results of this analysis are detailed in \S\ref{subsec:safety_llama}. Here, we focus on additional findings, examining not just the agreement between the evaluators and the model but also the agreement among the evaluators themselves. In these results, we include the \textit{uncertain} class in the analysis.

Out of the 1,000 questions evaluated, the three evaluators fully agreed on 726 instances. Among the remaining 274 questions where disagreement occurred, 247 involved two evaluators agreeing while the third disagreeed. The distribution of these disagreements is shown in Figure \ref{fig:breakdown}. Interestingly, the highest disagreement occurred in opposite classifications: 87 instances where two evaluators labeled the response as \textit{safe} while one labeled it as \textit{unsafe}, and 64 instances where two labeled it as \textit{unsafe} while one marked it as \textit{safe}. 



\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/partial_agreement_breakdown.png}
    \caption{Distribution of answers where the three evaluators partially agreed on the label (i.e., two evaluators agreed while one disagreed). The total number of answers with partial agreement is 247.}
    \label{fig:breakdown} 
\end{figure}


Only 27 questions~---representing just 2.7\% of the total---~exhibited complete disagreement, where each evaluator chose a different label. This low rate of total disagreement suggests that, despite occasional differences, there is a notable level of consistency among the evaluators. 

Another noteworthy finding is the variation in the distribution of \textit{safe}, \textit{unsafe}, and \textit{uncertain} responses across gender, see Figure \ref{fig:labels_per_evaluator}. Men tended to classify more responses as \textit{safe} (404, 386 and 374 respectively, out of 600 responses per person), while women classified fewer as \textit{safe} (300 and 351). Conversely, women labeled more responses as \textit{unsafe} (243 and 220, compared to 175, 196, and 210 for men) and also labeled more responses as \textit{uncertain} (57 and 29) than their men counterparts (21, 18, and 16). These differences suggest potential variations in risk perception or interpretation between men and women evaluators.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/labels_per_evaluator.png}
    \caption{Number of \textit{safe}, \textit{unsafe}, and \textit{uncertain} labels per person. The first three columns correspond to men evaluators, and the last two to women evaluators.}
    \label{fig:labels_per_evaluator} 
\end{figure}


This analysis highlights a relatively high level of agreement among evaluators, with full consensus in 72.6\% of cases and only minimal complete disagreement. However, the differences in classification tendencies between men and women underscore the potential impact of evaluator diversity on the results. Overall, the high degree of agreement among evaluators strengthens confidence in the reliability of this evaluation process. 