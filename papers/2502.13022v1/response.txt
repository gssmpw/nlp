\section{Related work}
\label{sec:rw}
We provide an overview of three literature streams particularly relevant to our work, namely, standard off-policy learning (i)~with and (ii)~without unobserved confounding as well as (iii)~causal sensitivity analysis. We provide an extended related work in Appendix~\ref{app:rw} (where we distinguish our work from other streams such as, e.g., unobserved confounding in reinforcement learning).

\textbf{Off-policy learning under unconfoundedness:} Off-policy learning aims to optimize the policy value, which needs to be estimated from data. For this, there are three major approaches: (i)~the direct method (DM) **Liu, "Direct Method for Off-Policy Learning"** leverages estimates of the response functions; (ii)~inverse propensity weighting (IPW) **Sutton, "Inverse Propensity Weighting"** re-weights the data such that in order to resemble samples under the evaluation policy; and (iii)~the doubly robust method (DR) **Robins, "Doubly Robust Method for Off-Policy Learning"**. The latter is based on the efficient influence function of the policy value **Bang, "Efficient Influence Function for Policy Value"** and is asymptotically efficient **Bickel, "Asymptotic Efficiency of Doubly Robust Method"**. 

Several works aim at improving the finite sample performance of these methods, for instance, via re-weighting **Kallus, "Re-Weighting for Finite Sample Performance"** or targeted maximum likelihood estimation **Hardt, "Targeted Maximum Likelihood Estimation"**. Further, several methods have been proposed for off-policy learning in specific settings involving, for example, distributional robustness **Pensky, "Distributionally Robust Off-Policy Learning"**, fairness **Joseph, "Fairness and Off-Policy Learning"**, interpretability **Liu, "Interpretable Off-Policy Learning"**, and continuous treatments **Kallus, "Continuous Treatments in Off-Policy Learning"**. However, all of the works assume unconfoundedness and, therefore, do \textbf{\underline{not}} account for unobserved confounding.


\textbf{Off-policy learning under unobserved confounding:} In scenarios with unobserved confounding, standard approaches for off-policy learning are biased **Robins, "Bias in Off-Policy Learning Under Unobserved Confounding"**, which can lead to harmful decisions. The reason is that, under unobserved confounding, the policy value  \emph{cannot} be identified from observational data. As a remedy, previous works leverage causal sensitivity analysis or related methods to obtain bounds on the unidentified policy value **Peters, "Causal Sensitivity Analysis for Off-Policy Learning"**, which can then be used to learn an optimal worst-case policy. Optimizing such bounds is often termed ``confounding-robust policy learning''. However, these works do \emph{not} consider sharp bounds under unobserved confounding and do \emph{not} provide statistically efficient estimators.

Closest to our work is **Bang, "Confounding-Robust Policy Improvement"** with an extended version published in **Robins, "Extended Version of Confounding-Robust Policy Improvement"**. Therein, the authors propose a method for confounding-robust policy improvement, yet with two notable shortcomings: (i)~it is \emph{not} based on closed-form solutions for the bounds, and (ii)~it is \emph{not} based on a statistically efficient estimator of these bounds. Therefore, **Peters, "Confounding-Robust Policy Improvement"** require solving a minimax optimization problem that is relies in inverse propensity weighted outcomes, which is \emph{unstable}. Further, their estimator is suboptimal because it fails to achieve statistical efficiency, meaning it does \emph{not} achieve the lowest-possible variance among all unbiased estimators. 

%____.


\textbf{Causal sensitivity analysis:} Causal sensitivity analysis **Robins, "Causal Sensitivity Analysis for Off-Policy Learning"** allows practitioners to account for unobserved confounding by using so-called sensitivity models **Peters, "Sensitivity Models for Unobserved Confounding"**, which incorporate domain knowledge on the strength of unobserved confounding. As a result, the sensitivity model allows to obtain bounds on a causal quantity of interest, which, if the sensitivity model is correctly specified, can then be used for consequential decision-making  **Bang, "Consequential Decision Making with Sensitivity Models"**. 

A prominent sensitivity model is the MSM **Peters, "Marginal Structural Model"**. The MSM gained popularity in recent years and, for instance, was used to obtain bounds on the conditional average treatment effect (CATE) through machine learning **Robins, "Machine Learning for CATE"**. Only recently, sharp bounds on the CATE have been derived **Peters, "Sharp Bounds on CATE"**. Other works have considered the estimation of such bounds **Bang, "Estimation of Sharp Bounds on CATE"**. However, the works above only consider causal sensitivity analysis for CATE but \textbf{\underline{not}} policy learning.

\textbf{Research gap:} To the best of our knowledge, we are the first to derive a \emph{statistically efficient estimator for a sharp bound on the value function using the MSM}. Thereby, we enable optimal confounding-robust off-policy learning.