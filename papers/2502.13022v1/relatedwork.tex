\section{Related work}
\label{sec:rw}
We provide an overview of three literature streams particularly relevant to our work, namely, standard off-policy learning (i)~with and (ii)~without unobserved confounding as well as (iii)~causal sensitivity analysis. We provide an extended related work in Appendix~\ref{app:rw} (where we distinguish our work from other streams such as, e.g., unobserved confounding in reinforcement learning).

\textbf{Off-policy learning under unconfoundedness:} Off-policy learning aims to optimize the policy value, which needs to be estimated from data. For this, there are three major approaches: (i)~the direct method (DM)~\cite{Qian.2011} leverages estimates of the response functions; (ii)~inverse propensity weighting (IPW)~\cite{Swaminathan.2015} re-weights the data such that in order to resemble samples under the evaluation policy; and (iii)~the doubly robust method (DR)~\cite{Dudik.2011, Athey.2021}. The latter is based on the efficient influence function of the policy value \cite{Robins.1994b} and is asymptotically efficient \cite{vanderVaart.1998, Chernozhukov.2018}. 

Several works aim at improving the finite sample performance of these methods, for instance, via re-weighting \cite{Kallus.2018, Kallus.2021b} or targeted maximum likelihood estimation \cite{Bibaut.2019}. Further, several methods have been proposed for off-policy learning in specific settings involving, for example, distributional robustness \cite{Kallus.2022}, fairness \cite{Frauen.2024}, interpretability \cite{Tschernutter.2022}, and continuous treatments \cite{Kallus.2018d, Schweisthal.2023}. However, all of the works assume unconfoundedness and, therefore, do \textbf{\underline{not}} account for unobserved confounding.


\textbf{Off-policy learning under unobserved confounding:} In scenarios with unobserved confounding, standard approaches for off-policy learning are biased \cite{Kallus.2018c,Kallus.2021d}, which can lead to harmful decisions. The reason is that, under unobserved confounding, the policy value  \emph{cannot} be identified from observational data. As a remedy, previous works leverage causal sensitivity analysis or related methods to obtain bounds on the unidentified policy value \cite{Namkoong.2020, Bellot.2024, Guerdan.2024, Huang.2024, Joshi.2024,Zhang.2024}, which can then be used to learn an optimal worst-case policy. Optimizing such bounds is often termed ``confounding-robust policy learning''. However, these works do \emph{not} consider sharp bounds under unobserved confounding and do \emph{not} provide statistically efficient estimators.

Closest to our work is \citet{Kallus.2018c} with an extended version published in \citet{Kallus.2021d}. Therein, the authors propose a method for confounding-robust policy improvement, yet with two notable shortcomings: (i)~it is \emph{not} based on closed-form solutions for the bounds, and (ii)~it is \emph{not} based on a statistically efficient estimator of these bounds. Therefore, \citet{Kallus.2018c,Kallus.2021d} require solving a minimax optimization problem that is relies in inverse propensity weighted outcomes, which is \emph{unstable}. Further, their estimator is suboptimal because it fails to achieve statistical efficiency, meaning it does \emph{not} achieve the lowest-possible variance among all unbiased estimators. 

%\cite{Kennedy.2022}.


\textbf{Causal sensitivity analysis:} Causal sensitivity analysis \citet{Cornfield.1959} allows practitioners to account for unobserved confounding by using so-called sensitivity models \cite{Rosenbaum.1987, Jin.2022}, which incorporate domain knowledge on the strength of unobserved confounding. As a result, the sensitivity model allows to obtain bounds on a causal quantity of interest, which, if the sensitivity model is correctly specified, can then be used for consequential decision-making  \cite{Jesson.2021}. 

A prominent sensitivity model is the MSM \cite{Tan.2006}. The MSM gained popularity in recent years and, for instance, was used to obtain bounds on the conditional average treatment effect (CATE) through machine learning \cite{Kallus.2019, Jesson.2021, Yin.2022}. Only recently, sharp bounds on the CATE have been derived \cite{Bonvini.2022, Dorn.2022, Frauen.2023c, Frauen.2024b, Jin.2023}. Other works have considered the estimation of such bounds \cite{Dorn.2024, Oprescu.2023}. However, the works above only consider causal sensitivity analysis for CATE but \textbf{\underline{not}} policy learning.

\textbf{Research gap:} To the best of our knowledge, we are the first to derive a \emph{statistically efficient estimator for a sharp bound on the value function using the MSM}. Thereby, we enable optimal confounding-robust off-policy learning.