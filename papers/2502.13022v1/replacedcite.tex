\section{Related work}
\label{sec:rw}
We provide an overview of three literature streams particularly relevant to our work, namely, standard off-policy learning (i)~with and (ii)~without unobserved confounding as well as (iii)~causal sensitivity analysis. We provide an extended related work in Appendix~\ref{app:rw} (where we distinguish our work from other streams such as, e.g., unobserved confounding in reinforcement learning).

\textbf{Off-policy learning under unconfoundedness:} Off-policy learning aims to optimize the policy value, which needs to be estimated from data. For this, there are three major approaches: (i)~the direct method (DM)____ leverages estimates of the response functions; (ii)~inverse propensity weighting (IPW)____ re-weights the data such that in order to resemble samples under the evaluation policy; and (iii)~the doubly robust method (DR)____. The latter is based on the efficient influence function of the policy value ____ and is asymptotically efficient ____. 

Several works aim at improving the finite sample performance of these methods, for instance, via re-weighting ____ or targeted maximum likelihood estimation ____. Further, several methods have been proposed for off-policy learning in specific settings involving, for example, distributional robustness ____, fairness ____, interpretability ____, and continuous treatments ____. However, all of the works assume unconfoundedness and, therefore, do \textbf{\underline{not}} account for unobserved confounding.


\textbf{Off-policy learning under unobserved confounding:} In scenarios with unobserved confounding, standard approaches for off-policy learning are biased ____, which can lead to harmful decisions. The reason is that, under unobserved confounding, the policy value  \emph{cannot} be identified from observational data. As a remedy, previous works leverage causal sensitivity analysis or related methods to obtain bounds on the unidentified policy value ____, which can then be used to learn an optimal worst-case policy. Optimizing such bounds is often termed ``confounding-robust policy learning''. However, these works do \emph{not} consider sharp bounds under unobserved confounding and do \emph{not} provide statistically efficient estimators.

Closest to our work is ____ with an extended version published in ____. Therein, the authors propose a method for confounding-robust policy improvement, yet with two notable shortcomings: (i)~it is \emph{not} based on closed-form solutions for the bounds, and (ii)~it is \emph{not} based on a statistically efficient estimator of these bounds. Therefore, ____ require solving a minimax optimization problem that is relies in inverse propensity weighted outcomes, which is \emph{unstable}. Further, their estimator is suboptimal because it fails to achieve statistical efficiency, meaning it does \emph{not} achieve the lowest-possible variance among all unbiased estimators. 

%____.


\textbf{Causal sensitivity analysis:} Causal sensitivity analysis ____ allows practitioners to account for unobserved confounding by using so-called sensitivity models ____, which incorporate domain knowledge on the strength of unobserved confounding. As a result, the sensitivity model allows to obtain bounds on a causal quantity of interest, which, if the sensitivity model is correctly specified, can then be used for consequential decision-making  ____. 

A prominent sensitivity model is the MSM ____. The MSM gained popularity in recent years and, for instance, was used to obtain bounds on the conditional average treatment effect (CATE) through machine learning ____. Only recently, sharp bounds on the CATE have been derived ____. Other works have considered the estimation of such bounds ____. However, the works above only consider causal sensitivity analysis for CATE but \textbf{\underline{not}} policy learning.

\textbf{Research gap:} To the best of our knowledge, we are the first to derive a \emph{statistically efficient estimator for a sharp bound on the value function using the MSM}. Thereby, we enable optimal confounding-robust off-policy learning.