\section{Experimental Setup}\label{sec:experiments}
\subsection{Dataset}

We propose to use publicly available egocentric conversational video datasets for evaluation: EasyCom \cite{donley2021easycom} and Ego4D \cite{grauman2022ego4d}.

\paragraph{EasyCom.}
The EasyCom dataset contains egocentric videos of 3-5 participants conversing around a table in a room for about 30 minutes per session. It comprises 12 sessions totaling approximately 5 hours and 18 minutes. We use sessions 1-3 for testing and 4-12 for training. The dataset features human-annotated transcripts with precise timestamps and mono-channel audio.

\paragraph{Ego4D.}
We use the Audio-Visual Diarization benchmark from Ego4D \cite{grauman2022ego4d}, a large-scale, in-the-wild egocentric video dataset. This subset contains 5-minute clips from diverse scenarios, including both indoor and outdoor settings. However, the original test split is mostly limited to indoor settings. To ensure robust evaluation, we randomly split the combined original train and test sets into 346 training clips and 87 test clips.

The EasyCom and Ego4D datasets offer complementary scenarios for evaluating our framework. EasyCom provides a controlled setting with continuous conversations among fixed participants, while Ego4D presents diverse, real-world scenarios with varying numbers of speakers, environments, and intermittent speech patterns. This combination allows us to assess EgoSpeak's performance in both relatively structured and unstructured environments, testing its ability to predict utterance initiation across a range of conversational dynamics.


\input{tables/main_result}
\input{tables/additional_baselines}

\subsection{Baselines \& Models}
We evaluate our framework using three trained models with different architectural backbones: RNN~\cite{an2023miniroad}, Transformer~\cite{xu2021longlstr}, and State-Space-Model~\cite{gu2023mamba}. Additionally, we implement two static baselines: a random baseline and a rule-based algorithm using silence as decision threshold~\cite{bell2001real}. Detailed implementation details including architectural specifications, hyperparameters, training objective and feature extraction for the neural models are provided in \Cref{app:implementation}.


\paragraph{Random Baseline.}
This baseline randomly assigns one of the three possible labels (background, target speaker speaking, or other speaking) to each frame with uniform probability.

\paragraph{Silence-based Algorithm.}
Simulating commercial spoken dialogue agents, this approach triggers speech only after a 600\,ms silence interval following other speakers. Our evaluation likely overestimates its real-world performance, since we use ground-truth labels to detect non-target speech and count the entire subsequent speech segment as correct once the start is identified (only a single timestep is penalized when incorrect).

\paragraph{Transformer-based Model.}
We adopt Long Short-term TRansformer (LSTR) \cite{xu2021longlstr} for temporal modeling. LSTR uses long-term and short-term memory mechanisms to handle sequence data, with an encoder-decoder structure. The encoder leverages long context windows by compressing inputs, while the decoder processes shorter context windows, allowing for flexible temporal modeling.


\paragraph{RNN-based Model.}
Inspired by \citet{an2023miniroad}, we used a simple and effective RNN model containing one GRU layer. This model was chosen for its computational efficiency and strong performance. 

\paragraph{Mamba-based Model.}
We implement a Mamba-based model \cite{gu2023mamba} similar to the RNN architecture. Given the recent success of Mamba across various tasks, we include this model to explore its potential to predict speech initiation in egocentric videos while maintaining computational efficiency.


\subsection{Settings}
\paragraph{Feature Extraction.} \label{para:feature_extraction}
We process features at 5 FPS, predicting every 0.2 seconds to align with typical human response times \cite{skantze2021turnreview}. For RGB features, we use a ResNet-50 \cite{he2016resnet} model pretrained on Kinetics-400 \cite{kay2017kinetics}. Audio features are extracted using wav2vec2 \cite{baevski2020wav2vec}. These features are concatenated to create our multimodal input. Further details on feature extraction are provided in \Cref{app:feature_extraction}.

\paragraph{Evaluation Protocol.}

Most existing turn-taking evaluations rely on offline F1-scores after processing the entire clip \cite{lee2023multimodal, kurata2023multimodal}, or on sample-based F1-scores around turn-taking events using threshold-based detection \cite{ekstedt2022voice, onishi2023multimodalvap}. However, both approaches fail to capture the continuous, overlapping nature of real-world conversations, where a decision must be made at every frame. As \citet{heldner2010pauses} suggested, overlaps occur frequently in human conversation. Consequently, we measure performance per frame to better reflect these natural conversational dynamics.

To address this need, we propose using per-frame mean average precision (mAP), inspired by 
prior work on online tasks \cite{de2016online}. This metric evaluates how well the model 
anticipates the target speakerâ€™s speech up to 10 timesteps (2\,s) into the future. We compute mAP by 1) sorting all frame-level confidence scores in descending order, 2) iteratively using each score as a threshold, 3) calculating precision and recall at each threshold, and 4) averaging all precision values. This procedure is repeated for each class and timestep, then averaged to yield the final mAP.





