\section{EgoSpeak Framework}
\label{sec:method}

\subsection{Framework Overview}
\frameworkname is designed for in-the-wild conversational agents, building on the challenges discussed in \Cref{sec:introduction}, where "in-the-wild" refers to real-world conditions outside controlled environments with unpredictable variables and numerous influencing factors.

\frameworkname is grounded in the intuition that, in the egocentric video, the camera wearer’s speaking moments naturally serve as cues for speech initiation. By predicting these moments from the agent’s perspective, our framework learns natural turn-taking behavior, identifying when to speak even after long silences. Moreover, by anticipating these moments in advance, \frameworkname effectively mirrors human turn-taking, deciding when to begin speaking as a real-world agent would. To achieve this, we train the model with a cross-entropy objective, akin to next-token prediction in language modeling, since it must anticipate speaking before the camera wearer actually speaks.

\subsection{Task Definition}
Guided by this intuition, we formulate the problem of predicting the target speaker's speech in an egocentric streaming video, where the camera wearer is naturally identified as the target speaker. Given the real-time nature of the stream, \frameworkname only analyzes information available up to the current moment. This design allows our system to capture the continuously unfolding context and prepare a speech onset before a turn-shift occurs in complex, dynamic conversations.

Formally, let $X_t = [x_1, \ldots, x_t]$ be an online stream up to timestep $t$, where 
each $x_i$ can include multiple modalities $x^m_i$ including visual frames $x^v_i$ or auditory signals $x^a_i$. We transform each $x^m_i$ into a representation $z^m_i$ via off-the-shelf feature extractors, concatenating them into $z_i$. Next, we define a temporal window $Z_{t-L+1,t} = [z_{t-L+1}, \ldots, z_t]$ of length $L$. 
Given an anticipation length $\alpha$, the model performs a three-way classification (background / target speaker speaking / other speaking) for the future range $t+1$ to $t+\alpha$. This anticipatory modeling gives the system extra time to prepare responses, rather than reacting only after a silence threshold. The final model output is a probability tensor of shape $[\alpha, 3]$, where the dimensions correspond to the anticipated future timesteps and the three classes, respectively.




\paragraph{Prediction vs Detection.}
A naive approach for determining when to speak is detection which occurs based on silence threshold. However, detection offers an inadequate response time of only 200ms for listeners. A psycholinguistic study \cite{levinson2015timing} estimates that actual response time ranges from 600 to 1500ms, as humans begin preparing their responses while the other person is still speaking. Additionally, turn-shifts often occur as overlapping without any gaps \cite{skantze2021turnreview}. The prediction will give conversational systems more time to generate reactions and enable human-like conversation.


\input{figures/fig3_data_annotation}
\paragraph{Frame-level Speech Labeling.}

\Cref{fig:data_annotation} illustrates how transcript timestamps convert into 
per-frame, one-hot encoded labels. As our framework requires per-frame speech labels which are expensive to annotate, we developed a method to convert transcript annotations from egocentric videos into per-frame speech classification labels. At each timestep $t$, we label the datapoint $x_t$ as \textit{target speaker speaking} if the camera wearer is speaking, \textit{other person speaking} if others are speaking, and \textit{no speech} otherwise.

\subsection{YT-Conversation: Dataset for Multimodal Conversation Pretraining}
Existing turn-taking resources often stem from controlled laboratory setups or video calls, which are expensive to annotate and capture only a fraction of the complexity found in real-world interactions, limiting scalability. To address this gap, we introduce YT-Conversation, a novel dataset derived from diverse YouTube content including interviews, podcasts, and casual dialogues.

While YT-Conversation is not fully egocentric, it offers realistic face-to-face and multi-person interactions that can effectively transfer to first-person scenarios in egocentric video understanding \cite{zhang2022actionformer, lin2022egocentric}. By leveraging content from real-world YouTube videos through an automatic pipeline, YT-Conversation aims to provide a more scalable resource for turn-taking pretraining.



\begin{figure}[t]
\centering
\includegraphics[trim={0 0cm 0 0},width=1.0\linewidth]{figures/files/YT_3x3_shuffled.pdf}
\caption{Sample frames from YT-Conversation dataset. The dataset includes a diverse range of conversational scenarios from YouTube, such as podcasts, interviews, and informal dialogues, representing various real-world conversation formats.}
\vspace*{-1em}
\end{figure}

\paragraph{Collecting Conversational Videos.}

We curated our dataset from four manually selected YouTube channels, covering diverse conversational formats including podcasts, interviews, and face-to-face dialogues. Videos were randomly sampled without further filtering, ensuring scalability.  
We preprocessed the videos by downsampling to 20 FPS for video and 16 kHz for audio, and trimmed opening segments. Our final dataset comprises 414 videos totaling 41 hours, with durations ranging from 1 to 60 minutes. 




\paragraph{Pseudo Per-frame Annotation for Collected Videos}

Since manual annotation of each video frame is labor-intensive, we employ voice activity detection (VAD) from Pyannote \cite{Plaquet23powerset, Bredin23pyanote} to generate pseudo-labels for speech activity. Specifically, we remove any speech segments under 200\,ms 
(or non-speech gaps under 200\,ms) to match our 200\,ms resolution. This approach yields 
a speech vs.\ no-speech label per frame, effectively approximating the ground truth 
for large-scale pretraining. \Cref{fig:fig5_YT_video_dist} shows a distribution of 
video durations and illustrates the diversity of conversation styles in YT-Conversation. For the validation of pseudo-annotation quality, see \Cref{app:YTConv_quality_validation}.

\input{figures/fig5_YT_video_dist}
