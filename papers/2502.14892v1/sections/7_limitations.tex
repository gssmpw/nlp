
\section{Limitations}\label{sec:limitations}
Our proposed method relies on pre-encoded features, which can limit both performance and frames per second (FPS). A potential solution is to adopt an end-to-end framework that learns to extract relevant features directly from raw input. For example, E2E-LOAD \cite{cao2023e2e} demonstrates improvements in both state-of-the-art performance and FPS in online action recognition task through such an approach.

While our YT-Conversation dataset provides diverse pretraining data, it may not fully capture the nuances of first-person interactions. Future work could explore methods to augment the dataset with more egocentric conversational data, potentially improving model performance in first-person scenarios.

Lastly, our current approach does not explicitly model speaker-specific behaviors. Future research could incorporate individual speaking patterns and tendencies, by analyzing larger egocentric conversational datasets. By capturing these speaker-specific nuances, future models may better anticipate utterance initiation points, particularly in prolonged conversations with familiar participants.



\section{Ethical Statement}\label{sec:Ethical Statement}

\paragraph{Data Collection and Privacy Considerations}
Although the YT-Conversation, derived from publicly shared YouTube videos, provides natural conversations for training AI models, there is a possibility it may capture the facial features of the participants. However, the YT-Conversation dataset was collected under the principles of informed consent and data anonymization, adhering to the ACM Code of Ethics 1.6 (Respect privacy).

\paragraph{Informed consent} We selected videos that participants are likely aware of and have consented to be recorded and publicly shared, such as podcasts, interviews, and face-to-face conversations.
    
\paragraph{Data Anonymization} We only released the YouTube IDs rather than the raw YouTube videos so that content creators can remove their videos from YouTube anytime, which will automatically exclude them from our dataset. Moreover, our transcripts only include the time ranges for the start and end of the speech, along with the corresponding video frames without personal information.

\section{Acknowledgements}
\label{sec:acknowledgement}

We thank Hyolim Kang and Joungbin An for their valuable discussions and insightful feedback.
This work was supported by Artificial intelligence industrial convergence cluster development project funded by the Ministry of Science and ICT(MSIT, Korea)\&Gwangju Metropolitan City and NCSOFT. It was also partly supported by an IITP grant funded by the Korean Government (MSIT) (No.RS-2020-II201361, Artificial Intelligence Graduate School Program (Yonsei University) and RS-2024-00353131) and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2024-00354218).






