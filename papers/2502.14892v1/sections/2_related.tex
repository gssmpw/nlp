\section{Related Works}\label{sec:relatedworks}
\paragraph{Turn-taking.}
Turn-taking research has evolved from simple audio-based models \cite{duncan1972some, khouzaimi2015optimising} to sophisticated multimodal approaches \cite{maier2017LSTMtowards, lee2023multimodal, mizuno2023next, kurata2023multimodal}. Early offline methods, which process entire clips, often result in unnatural pauses. This prompted the development of continuous (online) methods \cite{skantze2017towards, ekstedt2022voice, li2022can}, including recent multimodal models incorporating non-verbal cues \cite{onishi2023multimodalvap}. However, these approaches typically rely on controlled dyadic conversations, limiting real-world applicability. 
\frameworkname addresses these limitations by adopting a first-person perspective, processing both RGB and audio features, and handling untrimmed video streams, aiming to better align turn-taking models with the complexities of natural conversations.

\input{tables/task_position}

\paragraph{Online Processing.}
Online systems, which predict based only on past and present information continuously, have gained popularity in real-time applications across various fields from computer vision to speech \cite{fan2018online, de2016online, kang2021cag, bewley2016simple, rettig2019online, miao2020transformer}.  \frameworkname applies this approach to turn-taking, enabling real-time prediction of speech initiation points in natural conversations without relying on future information. This capability allows \frameworkname to adapt to dynamic conversational scenarios, making it more suitable for real-world interactions.


















 















