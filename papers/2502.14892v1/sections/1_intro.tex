
\section{Introduction}\label{sec:introduction}

Human-like conversational agents have long been a key objective in artificial intelligence. A critical aspect of human conversation is not only understanding what to say but also when to say it—often framed as the study of turn-taking \cite{duncan1972some}. While most are designed under simplified assumptions where turn boundaries are well-defined or where only audio-based cues are available, real-world conversations can be highly fluid, with overlapping speech, unclear speaker roles, and frequent interruptions \cite{skantze2017towards, skantze2021turnreview}.

To address these complexities, we introduce \frameworkname, a framework that predicts when an agent should begin speaking based on egocentric streaming video. Concretely, \frameworkname models speech initiation from the first-person perspective of the camera wearer, capturing exactly what the agent sees at each moment in real time. 
Unlike a third-person or fixed camera view, the egocentric perspective is especially relevant 
for real-world conversational agents such as social robots that must decide on the fly whether to speak or remain silent. By leveraging the camera wearer’s immediate visual context (e.g., facing another person, noticing body language or gaze direction), \frameworkname can more naturally detect subtle cues that signal an appropriate moment to start speaking. This is particularly crucial for a real-world agent that must not only process inputs in real time, but also respond autonomously in dynamic, multi-speaker environments 
to appear natural and engaging.


\begin{figure}[t]
\centering
\includegraphics[trim={0 0cm 0 0},width=1.0\linewidth]{figures/files/Egospeak_Teaser_Figure.pdf}

\caption{\frameworkname\ models speech initiation in real time from the camera wearer’s (camera icon) egocentric video stream, mirroring how a real-world agent would perceive and engage in dynamic, multi-speaker environments.}


\label{fig:new_teaser}
\vspace*{-1.5em}
\end{figure}

\input{figures/fig2_method_desc}

\frameworkname incorporates four key capabilities: (1) first-person perspective: aligns closely with real-world interactions for conversational agents, (2) RGB feature processing: handles scenarios where audio or non-verbal cues may be unreliable, (3) dynamic real-time turn-taking: enables more natural and fluid conversations, and (4) continuous untrimmed video stream processing: captures periods of silence and sporadic interactions. These four collectively enable \frameworkname to handle the complexities of real-world conversations more effectively than previous methods (see \Cref{tab:task_position}). \Cref{fig:method_desc} provides an overview of our real-time pipeline, illustrating how \frameworkname processes continuous video streams to decide when to speak. \frameworkname outputs a continuous speak-probability that a conversational agent can leverage in real time (e.g., by triggering speech once the probability surpasses a threshold).


We validate \frameworkname on two distinct datasets: EasyCom and Ego4D, demonstrating its effectiveness across various conversational contexts. Additionally, we introduce the YT-Conversation dataset, a collection of in-the-wild conversation videos including interviews and casual conversations from YouTube, designed for scalable pretraining.  

By addressing the critical challenge of when to speak in a natural, human-like manner, \frameworkname advances the field of conversational AI, offering a robust solution for dynamic, intermittent conversations with varying numbers of speakers.  

In summary, our key contributions are:
\begin{enumerate}[nosep, leftmargin=*]

\item \frameworkname, a novel framework for speech initiation prediction from egocentric streaming video in real time.
\item YT-Conversation, a large-scale corpus of in-the-wild conversational videos, suitable for 
pretraining multimodal turn-taking models.
\item Experimental results on EasyCom and Ego4D demonstrate effectiveness in real-world scenarios and provide a comprehensive analysis of the role of multimodal inputs and context length. 

\end{enumerate}



























