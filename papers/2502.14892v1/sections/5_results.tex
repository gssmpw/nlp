\section{Results and Analysis}\label{sec:results}
\subsection{Quantitative Results}\label{subsec:quant}
\Cref{tab:main_result,tab:avg_perframe_ap} present comprehensive comparisons of our models across different modalities, datasets, and baselines. To ensure the robustness of our findings, we report performance over five random seeds with error bars in \Cref{app:stat_results}.
\paragraph{Model Performance Across Modalities}
As shown in \Cref{tab:main_result}, the multimodal (A+V) approach generally outperforms unimodal inputs on EasyCom, with the Transformer model achieving 58.7\% mAP (compared to 56.9\% for audio-only and 51.0\% for visual-only). The GRU model performs 
best with A+V (60.6\% mAP), while Mamba sees moderate improvements (57.4\% mAP). On Ego4D, the Transformer with A+V attains 69.0\% mAP, which is roughly on par with its audio-only counterpart. Interestingly, GRU and Mamba actually do better with audio alone, at 69.0\% and 67.9\% mAP respectively.




\paragraph{Pretraining Effects}
Our YT-Conversation pretraining results show that overall gains are modest. However, 
we do observe a small but consistent improvement in detecting \emph{other person speaking} class, which is especially valuable in egocentric scenarios. In contrast, GRU and Mamba show 
little or no net gain, aligning with prior work that certain recurrent/state-space models 
often struggle with large-scale pretraining \cite{wang2023pretraining}. 
We attribute these results to domain mismatch and the inherently noisier nature of real-world conversational videos. For a per-class breakdown and further discussion, refer to \Cref{app:error_analysis}.


\paragraph{Comparison with Baselines}
\Cref{tab:avg_perframe_ap} compares our best Transformer (A+V) model with both baselines 
on EasyCom and Ego4D. Even though the silence-based approach benefits from an evaluation bias, 
our predictive model still achieves significantly higher AP (52.7\% vs.\ 26.6\% on EasyCom, and 66.8\% vs.\ 27.7\% on Ego4D). Moreover, the silence-based method performs similarly to random, indicating that requiring a fixed silence interval fails to accommodate the fluid, overlapping speech found in real-world conversations.

\input{tables/ablation_optical_flow}

\input{figures/fig6_context_length}

\input{figures/fig7_qualitative_results}


\subsection {Motion Inputs Contribute to Turn-Taking Prediction}
Since many non-verbal cues involve motion, we hypothesized that incorporating optical flow could improve utterance initiation prediction.
To test this, we extracted optical flow using the Denseflow toolkit \cite{wang2020denseflow} with the TV-L1 algorithm \cite{zach2007dualityTVL1}, following a similar process to our RGB feature extraction. Optical flow is a computer vision technique that estimates object motion between consecutive video frames by calculating the apparent motion of brightness patterns. This is useful for tracking movement and analyzing dynamic scenes.


\Cref{tab:ablation_optical_flow} presents the results of our experiment on EasyCom. Incorporating optical flow consistently improved performance across all model types and input combinations.
These results suggest that motion information provides valuable cues for predicting utterance initiation, complementing static visual and audio features to enable more accurate predictions of speech onset.

\subsection{Models Do Not Exploit Short-Term Information Well}
Our framework uses online processing models that rely on context length to capture historical information. Because dialogue context is crucial in turn-taking \cite{skantze2021turnreview}, 
the choice of context can strongly affect utterance initiation. \Cref{fig:context_length} shows how the Transformer modelâ€™s performance varies with different long-term and short-term window sizes. While extending the long-term window helps, increasing the short-term window unexpectedly degrades performance. This suggests that although a broader context provides valuable cues, an overly large short-term window may introduce noise or irrelevant data, reducing accuracy. These findings highlight the importance of balancing long-term and short-term context in untrimmed videos to optimize turn-taking predictions.



\subsection{Runtime Analysis}
We evaluated the computational efficiency of our models by measuring their frames per second (FPS), parameter counts, and floating-point operations (GFLOPs) on a single RTX3090 GPU using the EasyCom dataset, as shown in \Cref{tab:fps}. The RNN-based model achieved the highest throughput with 13,939.5 FPS, while maintaining a relatively low parameter count of 34.6M and requiring 206.52 GFLOPs. The Mamba-based model followed with 12,009.3 FPS, though it has the highest parameter count (83.1M) and the largest computational requirement (610.93 GFLOPs). The Transformer-based model ran at 99.8 FPS with 67.21M parameters and 129.48 GFLOPs, achieving real-time processing but at a lower frame rate than RNN and Mamba. Overall, these results indicate that all three architectures are capable of real-time processing.



\input{tables/fps.tex}

\subsection {Qualitative Results}
\Cref{fig:qualitative_results} shows the qualitative results based on Transformer. Our observations indicate that the model using only RGB features struggles to effectively distinguish between speaking and non-speaking segments, leading to frequent misclassifications. In contrast, the model utilizing audio input shows notable improvement in predicting the target speaker's speech. However, the audio-only model often assigns high probabilities to the speech of other individuals, resulting in less accurate turn-taking. Notably, the model that integrates both audio and visual inputs demonstrates superior performance. This multimodal model effectively distinguishes the target speaker from others, accurately identifying speaking segments while minimizing false positives from other speakers.





