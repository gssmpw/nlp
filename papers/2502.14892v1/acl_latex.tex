\pdfoutput=1
\PassOptionsToPackage{prologue,dvipsnames}{xcolor}
\documentclass[11pt]{article}

\usepackage[final]{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}


\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{enumitem}
\usepackage{float}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} %
\usepackage{multirow} %
\usepackage{array} %
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{xspace}
\usepackage{adjustbox}

\usepackage{lipsum}
\usepackage{bbm}
\usepackage{stmaryrd}
\usepackage{makecell}
\usepackage{courier}
\usepackage{bbm}
\usepackage{algorithm, algpseudocode}
\usepackage{setspace}
\usepackage{threeparttable}
\usepackage{cancel}
\usepackage{latexsym}
\usepackage{dirtytalk}
\usepackage{csquotes}
\usepackage{pgfplots}
\usepackage{pifont}
\usepackage[dvipsnames]{xcolor}
\usepackage{geometry}
\usepackage{adjustbox}
\usepackage{lineno}

\usepackage{sections/package}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\definecolor{mygreen}{RGB}{34,139,34}
\definecolor{myred}{RGB}{178,34,34}

\newcommand{\bfsection}[1]{\noindent\textbf{#1}.}

\newcommand{\modelnamefancy}{\textbf{EgoSpeak}\xspace}
\newcommand{\modelname}{EgoSpeak\xspace}
\newcommand{\frameworkname}{EgoSpeak\xspace}
\newcommand{\frameworknamefancy}{\textbf{EgoSpeak}\xspace}


\title{\modelname: Learning When to Speak\\ for Egocentric Conversational Agents in the Wild}



\author{
Junhyeok Kim$^{\clubsuit}$ \quad
Min Soo Kim$^{\clubsuit}$ \quad
Jiwan Chung$^{\clubsuit}$ \quad
Jungbin Cho$^{\clubsuit}$\\
\textbf{Jisoo Kim}$^{\clubsuit}$ \quad
\textbf{Sungwoong Kim}$^{\clubsuit}$ \quad
\textbf{Gyeongbo Sim}$^{\diamondsuit}$ \quad
\textbf{Youngjae Yu}$^{\clubsuit}$\\
\small{$\clubsuit$ Yonsei University} \quad
\small{$\diamondsuit$ Multimodal AI Lab., NC Research, NCSOFT Corporation} \\
\texttt{junhyeok@yonsei.ac.kr}
}


\begin{document}
\maketitle
\input{sections/0_abstract}
\input{sections/1_intro}
\input{sections/2_related}
\input{sections/3_method}
\input{sections/4_experiments}
\input{sections/5_results}
\input{sections/6_conclusion}
\input{sections/7_limitations}


\bibliography{custom}
% \bibliography{anthology,custom}
% \bibliographystyle{acl_natbib}

\clearpage
\appendix
\section{Implementation Details}
\label{app:implementation}
\subsection{Architecture \& Hyperparameters}
\label{app:arch_hyper}
This section provides detailed information on the architectures and hyperparameters used for each model in our experiments. We set the anticipation length to 10 timesteps for all models, predicting up to 2 seconds into the future. All experiments were done with a single RTX3090 GPU within one day.

\paragraph{Transformer-based Model (LSTR)}
For the transformer model, we configured 16 attention heads and 1024-dimensional hidden units in the transformer blocks. The LSTR encoder processes long context windows up to 2048 frames, while the decoder handles shorter context windows up to 32 frames. We trained this model using the Adam optimizer \cite{kingma2014adam} with a weight decay of $5 \times 10^{-5}$. The learning rate was scheduled to increase linearly from zero to $7 \times 10^{-5}$ during the first 40\% of training iterations, then decrease to zero following a cosine function. We trained the transformer model for 50 epochs with a batch size of 16.

\paragraph{RNN-based Model}
For the RNN model, we used 2048-dimensional embeddings and 1024-dimensional hidden units. This model was trained for 30 epochs with a batch size of 64. We used the same optimizer and learning rate schedule as the transformer model.

\paragraph{Mamba-based Model}
The Mamba-based model builds upon the RNN architecture, replacing the GRU layer with a Mamba block. We set the SSM state factor to 16, local convolution width to 4, and block expansion factor to 2. The training settings were kept consistent with the RNN model.

\subsection{Training Objective}
For training, we use cross-entropy loss between predicted confidence scores $s_T$ at time $T$ and the ground-truth label $y_T \in \{0, 1, \ldots, K\}$. $K$ is the number of classes and $s^k_T$ is the $k$-th element of the probability vector $s_T$. For Transformer-based model, $\alpha_T$ is always 1. For RNN-based and Mamba-based models, $\alpha_T$ is used to modulate the contribution of intermediate time steps during the computation of the loss. Specifically, $\alpha_T$ takes the value 1 only at a designated step $t = L$ and 0 otherwise.

We also define a temporal window of length $L$, which determines the final step contributing to the objective function:

$$J(y_T, s_T; T) = - \sum_{k=0}^K \alpha_T\delta(k - y_T) \log s_T^k,$$

\subsection{Feature Extraction}
\label{app:feature_extraction}
\paragraph{RGB Features} As mentioned in \Cref{para:feature_extraction}, videos are downsampled to 20 FPS and processed in 4-frame chunks, resulting in a 5 FPS prediction rate. We use ResNet-50 \cite{he2016resnet} initialized with weights from a video action recognition model \cite{wang2016temporal}, implemented via MMAction2 \cite{2020mmaction2}. The center frame of each chunk is sampled for feature extraction. For the EasyCom dataset, we cropped all clips in each session to remain only video frames and merged them to make one video per session.
\paragraph{Audio Features} We use wav2vec2's \cite{baevski2020wav2vec} multi-layer convolutional feature encoder, as noted in \Cref{para:feature_extraction}. Every 10 encoded audio features are concatenated temporally to match the 5 FPS RGB features.

\input{figures/fig8_attn_heatmap}
\section {Importance of Recent Frames}

\Cref{fig:attn_heatmap} shows the distribution of attention weights across the encoder layers of a transformer model in the context of predicting utterance initiation in real-world conversations \cite{wang2021oadtr}. The attention weights of the test set were averaged with respect to the layers, multi-heads, and batch and then normalized.  These weights reveal the significance assigned to each frame in the sequence during prediction. Our analysis shows that the model focuses predominantly on the recent frames, with attention weights diminishing notably as the distance from the current frame increases. This pattern indicates that recent frames have a greater impact on the model's predictions for utterance initiation. 

\input{tables/pretraining_per_class}
\input{figures/fig9_error_analysis}

\section{Error Analysis}
\label{app:error_analysis}
\paragraph{Pretraining on YT-Conversation}

We observed that YT-Conversation pretraining yields modest overall gains, but a notable improvement for \emph{other person speaking} class (+0.7\% on EasyCom, +1.5\% on Ego4D). 
\Cref{tab:pretrain_perclass} lists the per-class average precision (AP) for the Transformer 
model with and without pretraining. Although this benefit can be crucial in egocentric 
scenarios—where identifying others’ speech fosters smoother turn-taking—gains for 
\textit{Background} and \textit{Target Speaker} remain unchanged or slightly negative. 
We attribute this to domain mismatch (YouTube interviews vs.\ dynamic ego footage) 
and noisy data from real-world conversational videos such as visual effects or subtitles. Future work might address these limitations by bridging domain gaps—e.g., with domain adaptation—or introducing video filtering to obtain higher-quality conversational clips.

\paragraph{Backchannels}
While our method aims to predict any utterance initiation point, there is a short and brief response that occurs when one participant is speaking and the listener reacts to signify the listener's attention, understanding, or emotion rather than take turns and speak. This behavior is referred to as ``backchannels'' \cite{yngve1970getting, skantze2021turnreview}. We observed that prediction scores usually do not increase before backchanneling. \Cref{fig:fig9_error_analysis} illustrates this phenomenon, showing how the model's prediction scores do not significantly increase before a backchanneling event, in contrast to regular speaking turns.


\section{Descriptive Statistics of Experimental Results}
\label{app:stat_results}

We evaluated each model with five random seeds \{0, 10, 20, 29, 42\} to measure performance variance. \Cref{tab:different_seed_results} shows the multi-seed mean average precision (mAP) on EasyCom and Ego4D, while \Cref{tab:main_result_error_bar_easycom,tab:main_result_error_bar_ego4d} provide per-timestep results (mean $\pm$ standard error). These tables complement the main text figures (\Cref{tab:main_result,tab:avg_perframe_ap}) by offering a full breakdown of multi-seed performance at each time step, ensuring transparency and robustness in our results.


\input{tables/different_seed_results.tex}

\section{YT-Conversation Pseudo Annotation Quality Validation}
\label{app:YTConv_quality_validation}
To validate the quality of pseudo-annotations in our YT-Conversation dataset, we conducted a human evaluation study on 100 segments randomly sampled from 10 videos, excluding the first five segments of each (typically non-conversational teasers). Each segment received a label alignment score on a 5-point scale: 
(1) completely misaligned, with timestamps far off from actual speech; 
(2) poor alignment, missing large portions, or labeling silence as speech; 
(3) adequate but potentially off by 0.5–1 second; 
(4) good alignment, within about 0.5 second of true boundaries; and 
(5) excellent alignment, nearly matching human labels. 
Across all evaluated segments, the average alignment score was 2.147. We want to note that as ASR models continue to advance \cite{zusag2024crisperwhisper}, the pseudo-label will be precise as well. We also use these pseudo-labels only for pretraining, ensuring the evaluations remain robust with human-annotated labels.


\input{tables/timesteps_different_seeds_EasyCom.tex}
\input{tables/timesteps_different_seeds_Ego4D.tex}


\section{Use of AI Assistants}
We used Claude 3.5 Sonnet to revise the paper and code, and GitHub Copilot to write the code.










\end{document}
