\begin{table}[t]
\centering
\footnotesize
\begin{adjustbox}{width=\columnwidth, center}
\begin{tabular}{llccccc}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Model} &
\multirow{2}{*}{\shortstack{Avg\\mAP}} & \multicolumn{3}{c}{Per-Class AP (\%)} \\
\cmidrule(lr){4-6}
& & & Background & Target Speaker & Other Speaker \\
\midrule
\multirow{2}{*}{EasyCom} 
  & Transformer & 58.79 & 43.17 & 52.74 & 80.46 \\
  & Transformer\textsuperscript{P} & 59.01 & 42.94 & 52.91 & 81.17 \\
\midrule
\multirow{2}{*}{Ego4D} 
  & Transformer & 69.61 & 73.50 & 66.78 & 68.56 \\
  & Transformer\textsuperscript{P} & 68.79 & 71.80 & 64.46 & 70.11 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Per-class average precision (AP) for the Transformer with and without 
(\textsuperscript{P}) YT-Conversation pretraining on EasyCom and Ego4D. Although overall 
gains are modest, we observe a notable improvement for \emph{Other Speaker} detection.}
\label{tab:pretrain_perclass}
\end{table}
