\section{Overview} \label{4.1}
This chapter introduces an algorithm that addresses the two limitations of the vanilla IFM, caused by the adoption of 3D-to-2D projection, as introduced in Section \ref{twofourfour}. The overview of the improvements to the vanilla IFM is shown in Fig. \ref{mov}. First, as seen in Fig. \ref{mov}(a), the proposed algorithm extends thin-sheet LFM localization from single-view point clouds to 3D LiDAR maps. One might consider such a solution to localize 3D fiducials in the map: incorporating marker detection into the front end of SLAM and detecting fiducials during SLAM. For example, \cite{map3} applies this solution by integrating AprilTag \cite{ap3} detection into a visual SLAM method (Vins-mono \cite{vins}). The feasibility of this solution is not denied. However, the concern is that it involves low-level modifications to the front end of the SLAM method. Therefore, it is not generalizable, as different LiDAR-based SLAM frameworks \cite{traj,sdk} have various front-end pipelines. Consequently, every time the baseline SLAM method changes, the modification must be made again. In contrast, as shown in Fig. \ref{mov}(a), the proposed algorithm is a generalizable method that can be applied to maps built using different methods. \begin{figure}[H] 
	\centering
\includegraphics[width=0.8\linewidth]{figs/chthree/mov.png}
	\caption{An overview of the improvements to the vanilla IFM.}
	\label{mov}
\end{figure}
Second, compared to the vanilla IFM, which directly projects the 3D point cloud onto an image plane, as shown in Fig. \ref{mov}(b), the proposed method jointly analyzes the point cloud from both intensity and geometry perspectives to extract potential candidate clusters. By introducing an intermediate plane to project these clusters, higher-quality intensity images are obtained. As a result, the proposed method improves the range over which markers can be detected. In this example, the distance is 30 meters, and the marker size is 69.2 cm $\times$ 69.2 cm, whereas the proposed method can localize the marker, but the vanilla IFM cannot.
%
Third, as seen in Fig. \ref{mov}(c), the extension of LFM localization to 3D maps enables downstream tasks such as 3D map merging. These low-overlap 3D maps are merged using marker localization results from the proposed method and the algorithm in Section \ref{multi}.
\section{Joint Analysis of Point Clouds from Intensity and Geometry Perspectives} \label{4.2}
\subsection{Downsampling Based on 3D Intensity Gradients} \label{ic} 
A synthesis point cloud (see Fig. \ref{sub}) with a simple scene is utilized to explicitly illustrate the design purpose and results of each operation in our pipeline. The two presenters are holding two different AprilTags \cite{ap3}. As illustrated in the top view, observing along the X-axis of the global coordinate system, the back subpoint cloud is totally blocked by the front one. Thus, although this is not a 3D LiDAR map, it possesses the most important feature of a 3D LiDAR map in this research: the spherical projection (Eq. (\ref{pro})) is not applicable. Note that Fig. \ref{sub} is only used to explicitly present the results and explain design purposes due to its simplicity. The ultimate objective is to localize the fiducial markers on a 3D map, which represents a larger and more complex scene. 
\begin{figure}[ht] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chthree/sub.png}
	\caption{The example used to explain the design purpose and result of each step.}
	\label{sub}
\end{figure}\par
The LFMs are sheets of thin paper that are not spatially distinguishable from the attached planes. Thus, it is infeasible to adopt previous geometric features-based 3D object detection methods \cite{single, rangenet} to find the LFMs. Therefore, a new feature extraction solution is developed to address this problem. 
%
The analysis is given as follows. A fiducial marker is composed of a black-and-white pattern and, as a result, presents as a high-intensity contrast object in the view of a LiDAR (see the zoomed views of Fig. \ref{mov}, Fig. \ref{occ}, and Fig. \ref{sub}). This indicates that the point cloud can first be analyzed from the intensity perspective. \par
In particular, downsampling is conducted on the raw point cloud based on the 3D intensity gradients. The intensity is taken as a function $I(\mathbf{p})$ of the 3D coordinates $\mathbf{p} = [x,y,z]^{T}$. Suppose that the given 3D point/location is $\mathbf{{p}_{0}}=[x_{0},y_{0},z_{0}]^{T}$, the point set composed of the neighbouring $n$ points around $\mathbf{{p}_{0}}$ is defined as $\mathcal{P}_{I}=\{\mathbf{{p}_{1}},\cdots,\mathbf{{p}_{n}}\}$. 
In practice, the following equation is used to approximate $I(\mathbf{p})$:
\begin{equation}
\hat{I}(\mathbf{p}) = \mathbf{C}^{T}\mathbf{p} + b, \label{eq4.1}
\end{equation}
where $\mathbf{C} \in \mathbb{R}^{3 \times 1}$ is the coefficient vector and $b\in \mathbb{R}$ is the intercept. Since the intensity values of points in $\mathcal{P}_{I}$ are known, $\mathbf{C}$ can be estimated by solving the following model fit (least square) problem:
\begin{equation}	
\underset{\mathbf{C}^{*}, b^{*}}{\arg \min } \sum_{i=1}^{n}\left\|\hat{I}(\mathbf{p}_{i})-I(\mathbf{p}_{i})\right\|^{2}\label{least2}.
\end{equation} \par
Coefficient regression \cite{barfoot} is performed to solve this problem. Let the design matrix be: 
\begin{equation}	
		\begin{aligned}
			& \mathbf{D} =\left[\begin{array}{cccc}  1 &\Delta x_{1} & \Delta y_{1} & \Delta z_{1} \\ 
  1 &\Delta x_{2} & \Delta y_{2} & \Delta z_{2} \\  
 \cdot   & \cdot  & \cdot & \cdot \\ 
 \cdot   & \cdot  & \cdot & \cdot \\  
 \cdot   & \cdot  & \cdot & \cdot \\  
  1 &\Delta x_{n} & \Delta y_{n} & \Delta z_{n} \\  
   \end{array}\right],
		\end{aligned}
	\end{equation} 
where $\mathbf{D}\in \mathbb{R}^{n \times 4}$. $\Delta x_{i}=(x_i-x_{0})$, $\Delta y_{i}=(y_i-y_{0})$, $\Delta z_{i}=(z_i-z_{0})$. The first column is the constant term, representing the intercept. Centering is applied to the design matrix, as the focus is on the local gradients of intensity value at a given 3D position within $\mathcal{P}_{I}$. Since the goal is to fit the intensity value as a function of 3D positions, the response variable vector would be:
\begin{equation}	
\mathbf{I}_{in} =\left[ \Delta I_{1}, \Delta I_{2}, \cdots, \Delta I_{n} \right]^{T},
\end{equation}
where $\Delta I_{i}=(I_i-\bar I)$ with $\bar I \in \mathbb{R}$ being the mean of the intensity values of the points in $\mathcal{P}_{I}$. Again, centering is performed as the focus is on the local gradients. Suppose that the regression coefficient vector $\mathbf{E}\in \mathbb{R}^{4 \times 1}$ is defined as:
\begin{equation}	
\mathbf{E} = \left[\begin{array}{c}  \mathbf{C}^{*}  \\ 
b^{*}
   \end{array}\right].
\end{equation} \par
Following \cite{barfoot}, the regression coefficient vector can be calculated by:
\begin{equation}	
\mathbf{E} =(\mathbf{D}^{T}\mathbf{D})^{-1}\mathbf{D}^{T}\mathbf{I}_{in}.
\end{equation} \par
$\hat{I}(\mathbf{x})$ is obtained by substituting the elements of $\mathbf{E}$ into Eq. (\ref{eq4.1}). Denote the 3D intensity gradients at the given 3D position as 
$\nabla I \in \mathbb{R}^{1 \times 3}$:
\begin{equation}
\begin{aligned}
 \nabla I &= \frac{\partial \hat{I}(\mathbf{p})}{\partial \mathbf{p}}\\
 &=\frac{\partial \mathbf{C}^{* T}\mathbf{p}}{\partial \mathbf{p}} + \frac{\partial b^{*}}{\partial \mathbf{p}}\\
 &=\mathbf{C}^{* T}
 \end{aligned}
\end{equation} \par
% \par
% Since $\hat{I}(\mathbf{x})$ is a first-order fit of intensity, the 3D intensity gradients, $\nabla I \in \mathbb{R}^{3 \times 1}$, are represented by the coefficient vector, $\mathbf{C}^{*} \in \mathbb{R}^{3 \times 1}$:
% \begin{equation}	
%  \mathbf{C}^{*} \Leftrightarrow \nabla I
% \end{equation} \par

The direction of $\nabla I$  of a given point indicates the direction where the intensity has the fastest decline and the norm, $|\nabla I|$, implies the rate of descent. 
%
Inspired by LOAM \cite{loam}, which selects a point as a feature if its geometric curvature is larger than a threshold, a 3D point is preserved if its $|\nabla I|$ is larger than a threshold in the downsampling procedure. The result following downsampling execution is depicted in Fig. \ref{step1}. As seen, the majority of unnecessary points are filtered out.\par
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chthree/step1.png}
	\caption{The effect of applying downsampling from the intensity perspective.}
	\label{step1}
\end{figure}\par


\subsection{Spatial Distribution Analysis of Downsampling Result} \label{clu}
The downsampling preserves the points belonging to the outlines of all objects with high intensity-contrast. In this section, the point cloud is analyzed from a geometric perspective.
% are preserved after the feature extraction. To further analyze if a feature belongs to the LiDAR fiducial marker or to a naturally occurring object, we conduct clustering on the point cloud of the features. 
The foundation of doing so is the fact that the points belonging to the fiducial markers will be isolated from those of the other objects after the downsampling (See the zoomed view of  Fig. \ref{step1}). This is due to the design of the marker's pattern as shown in Fig. \ref{td}. 
\begin{figure}[ht] 
	\centering
\includegraphics[width=0.8\linewidth]{figs/chthree/td.png}
	\caption{A diagram to illustrate the design of a typical square fiducial marker \cite{ap3}. }
	\label{td}
\end{figure}\par
In particular, a square fiducial marker is a combination of the prototype marker (a black frame inside a white frame) and the encoding area. The white regions of the prototype marker naturally have higher intensity values than the black regions, and thus, the prototype marker after the downsampling is rendered as a square double-ring that isolates the points inside the coding area from the environment. The isolation makes the points belonging to the markers spatially distinguishable from those of the other objects. Thus, we employ the method introduced in \cite{rusu} to further segment the downsampling result into clusters. Each cluster is represented by an Oriented Bounding Box (OBB) in Fig. \ref{step2}. 
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chthree/step2.png}
	\caption{The effect of clustering on the downsampling result.}
	\label{step2}
\end{figure}\par

\subsection{Filtering Out Unwanted Clusters} 
As depicted in Fig. \ref{step2}, there are many OBBs after clustering. In this section, the geometric characteristics of each OBB are analyzed to verify if it has the potential to be a fiducial marker. Specifically, the bounding box of a cluster needs to satisfy two criteria to be recognized as a valid candidate. \par
%
\noindent\textbf{Criterion 1.} The first criterion is subject to the marker size: 
\begin{equation}	
		\sqrt{2a^{2}+t_{M}^{2}}\leq L_{OBB} \leq \sqrt{4a^{2}+t_{M}^{2}},
		\label{first}
	\end{equation} 
where $L_{OBB}=\sqrt{l^{2}+w^{2}+h^{2}}$ is the cuboid diagonal of the OBB with $l$, $w$, and $h$ ($h \leq t_{M}$) being the length, width, and height, respectively. $a$ denotes the side length of the marker. $t_{M}$ is the trifling thickness of the marker. \par
%
\noindent\textbf{Explanation of Criterion 1.} The trivial height is neglected for now, and the possible size range of the OBB is considered in 2D space. Fig. \ref{proof} shows the 2D geometric relations.
\begin{figure}[ht] 
	\centering
\includegraphics[width=0.7\linewidth]{figs/chthree/proofnew.png}
	\caption{A diagram of the possible OBB size for a given marker.}
	\label{proof}
\end{figure} \par
Principal Component Analysis (PCA) is performed on the distribution of points belonging to the marker.
%
Suppose that $O_{M}-X_{M}-Y_{M}$ is the marker coordinate system and $\theta$ is the angle between the first Primary Component (PC) and the x-axis. On account of the fact that the first PC and second PC are perpendicular to each other \cite{bounding}, the angle between the second PC and the y-axis is also $\theta$. In addition, $\angle DBC=\theta$. Thus, the side length of the OBB, expressed as $a (\mathrm{cos}\theta +\mathrm{sin}\theta)$, falls within the range $[a, \sqrt{2}a]$, given that $\theta \in [0, 2\pi]$. Furthermore, the area of the 2D OBB, $S_{OBB}$, is in the following range:
	\begin{equation}	
		a^{2}\leq S_{OBB} \leq 2a^{2}.
		\label{range}
	\end{equation} \par
Returning to 3D space, the marker's thickness, $\delta$, is also taken into account. The first criterion becomes Eq. (\ref{first}).
\par
\noindent\textbf{Criterion 2.} The second criterion is shown in Eq. (\ref{second}): 
	\begin{equation}	
		 1/1.5 \leq l/w \leq 1.5.
		\label{second}
	\end{equation} \par

\noindent\textbf{Explanation of Criterion 2.} This criterion is based on the fact that the shape of the marker is square and the OBB projection on the plane of length and width is also square. Ideally, $l \approx w$. Whereas in the real world, the marker cannot be perfectly scanned by LiDAR, and LiDAR also has ranging noise. 
As a result, the shape of the OBB on the length and width plane could be distorted. Hence, the requirement on $l \approx w$ is relaxed as shown in Eq. (\ref{second}). It is presented in Fig. \ref{step3} that the unwanted OBBs are filtered out after the operation introduced in this section.
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chthree/step3.png}
	\caption{The effect of filtering out the unwanted clusters.}
	\label{step3}
\end{figure} \par

\section{Marker Localization via Intermediate Plane} \label{4.3}
After analyzing the point cloud from the 3D intensity gradients and 3D geometric perspectives, locations of objects with high intensity-contrast and shapes/sizes similar to the fiducial marker are obtained. 
%
These locations (OBBs) are then inspected in the raw point cloud. When extracting points falling into the OBBs, a buffer is adopted to extend the OBBs, preserving more regions around an OBB in case it does not completely cover the fiducial marker. This indicates that, for each OBB, the pose (position and orientation), $\mathbf{T}_{OBB}$ \footnote{$\mathbf{T}_{OBB}$ denotes the transmission from the world coordinate system $\{G\}$ to the OBB frame. The OBB frame refers to a coordinate system whose X, Y, and Z axes are parallel to the length, width, and height of the OBB. }, is kept while the size is enlarged by multiplying the length, width, and height with an amplification factor, $t_{b}$. The recommended value of $t_{b}$ is twice the marker's side length based on our experiment. \par
\subsection{Motivation for Adopting the Intermediate Plane}
An intermediate plane-based method is proposed to determine if an OBB contains a fiducial marker. There are two reasons for adopting the intermediate plane. First, as seen in Fig. \ref{step4}, although the points belonging to the candidate markers are extracted from the raw point cloud, unfortunately, the spherical projection (Eq. (\ref{pro})) cannot be applied in this case, as occlusion still exists when observing the point cloud from the origin. (Review Fig. \ref{sub} if needed). 
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chthree/step4.png}
	\caption{The result of extracting points falling into the preserved OBBs from the raw point cloud.}
	\label{step4}
\end{figure} 
In contrast, as depicted in Fig. \ref{inter1}, the adoption of the intermediate plane resolves this problem by transferring the clusters onto the intermediate plane one by one, thereby addressing the occlusion issue.
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chthree/inter1.png}
	\caption{An illustration of how the intermediate plane helps solve the occlusion issue.}
	\label{inter1}
\end{figure} 
Second, as shown in Fig. \ref{mov}(b), when the marker is far from the LiDAR, directly applying the spherical projection will result in a projection that is too small to be detected. However, as presented in Fig. \ref{inter2}, the introduction of the intermediate plane addresses this challenge. Specifically, transferring the point cloud of a fiducial marker onto the intermediate plane physically reduces the distance between the point cluster and the LiDAR, thereby resulting in a projection of better quality.
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chthree/inter2.png}
	\caption{An illustration of how the intermediate plane helps solve the occlusion issue.}
	\label{inter2}
\end{figure} 
%
\subsection{Utilization of the Intermediate Plane}
Given that the pose of each OBB ($\mathbf{T}_{OBB}$) is known, the perspective for observing these candidate clusters can be adjusted using $\mathbf{T}_{OBB}$. 
%
In particular, the viewpoint is fixed at the origin, and the points extracted from each OBB are transferred one by one to the intermediate plane (denoted by $\mathbf{P}$), as shown in Fig. \ref{inter1}.
%
The detailed transmission process is as follows. Define the point set of the $j$-th OBB as $\mathcal{P}_{j}$ and a point of $\mathcal{P}_{j}$ as $\mathbf{{p}_{j}}\in \mathbb{R}^{3}$. $\mathbf{{p}_{j}}$ is first transmitted to the origin of the world coordinate system $\{G\}$ through the inverse of $\mathbf{T}_{OBB}$:
	\begin{equation}	
			\mathbf{{p}_{G}}  = \mathbf{T}_{OBB}^{-1} \cdot \mathbf{{p}_{j}}= \mathbf{R}^{-1} \cdot \mathbf{{p}_{j}}-\mathbf{R}^{-1}\mathbf{t}, 
		\label{trans}
	\end{equation}
where $\mathbf{{p}_{G}}$ is the point transmitted to the origin of $\{G\}$. $\mathbf{T}_{OBB} = [\mathbf{R}|\mathbf{t}]$ where $\mathbf{R}$ is the $3 \times 3$ orthogonal rotation matrix and $\mathbf{t}$ is the $3 \times 1$ translation vector. After transmitting all the points in $\mathcal{P}_{j}$ using Eq. (\ref{trans}), a new point set $\mathcal{P}^{G}_{j}$ is obtained. Then, all points belonging to $\mathcal{P}^{G}_{j}$ are transferred to the intermediate plane $\mathbf{P}$:
	\begin{equation}	
			\textbf{p}^{\prime}_{j} = \mathbf{T}_{in} \cdot \textbf{p}_{G}=\mathbf{R}_{in} \cdot \textbf{p}_{G}+\mathbf{t}_{in}, \\
		\label{trans2}
	\end{equation}
where $\textbf{p}^{\prime}_{j}$ is the point transmitted to the intermediate plane. $\mathbf{T}_{in} = [\mathbf{R}_{in}|\mathbf{t}_{in}]$ where $\mathbf{R}_{in} = \mathbf{I}_{3 \times 3}$ is an identity matrix and $\mathbf{t}_{in} = [1 \mathrm{m}, 0, 0]^{T} $ since the plane equation is $x=1 \mathrm{m}$. Define the point set on the intermediate plane as $\mathcal{P}^{\prime}_{j}$. Given that $\mathcal{P}^{\prime}_{j}$ is a point cloud with no occlusions, marker localization in it is straightforward using the vanilla IFM. IFM returns the marker ID and the locations of the vertices labeled by index if the marker is found within the candidate OBB. Then, the locations of vertices can be transferred back to the original positions in the 3D map using the reverse processes of Eqs. (\ref{trans}-\ref{trans2}), and the poses of markers (from $\{G\}$ to the marker coordinate system) are obtained by solving SVD. The final marker localization result is presented in Fig. \ref{step5}. 
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chthree/step5.png}
	\caption{The fiducial marker localization result after applying the intermediate plane method. The vertices of the localized fiducial marker are rendered red.}
	\label{step5}
\end{figure} 
\par
Note that as depicted in Fig. \ref{add}, the rotation of $\mathbf{T}_{OBB}$ may be ambiguous, potentially causing a flipped intensity image due to the symmetry of the 3D OBB. In practice, both the raw intensity image and the flipped intensity image are checked, as a pattern in the coding library will not have its flipped version present \cite{ap3,aruco}.
\begin{figure}[H] 
	\centering
\includegraphics[width=0.4\linewidth]{figs/chthree/add.png}
	\caption{An illustration of the potential rotational ambiguity issue of $\mathbf{T}_{OBB}$.}
	\label{add}
\end{figure} 

\section{3D LiDAR Map Construction} \label{new4.4}
This section briefly introduces how the 3D LiDAR maps tested in Section \ref{4.4} are constructed. In particular, two methods—Livox Mapping \cite{sdk} and Traj LO \cite{traj}—are utilized in this dissertation for constructing large-scale 3D maps.
\subsection{Livox Mapping}
Livox Mapping \cite{sdk} is the official implementation of LiDAR-based SLAM released by Livox, which is built on LOAM \cite{loam}. Suppose that the set of sampled data of the current scan is:
\begin{equation}
\mathcal{P}_{loam} = \{ \{x_{1},y_{1},z_{1},I_{1}\}, \{x_{2},y_{1},z_{2},I_{2}\}, \cdots, \{x_{n},y_{n},z_{n},I_{n}\} \}. \label{loamdata}
\end{equation} \par
To eliminate motion blur effects, the raw data is preprocessed by applying motion compensation. This involves either piecewise processing, where a frame is divided into subframes and processed in parallel to reduce the time interval, or linear interpolation, which modifies 3D point positions using the constant velocity assumption. Then, corner and plane feature points are extracted based on the curvature and normal of each point:
\begin{equation}
\begin{aligned}
&\mathcal{C}_{loam} = \{ \mathbf{p}_{i} \in \mathcal{P}_{loam} | \ \ \kappa(\mathbf{p}_{i})> t_{cor} \}, \\
&\mathcal{S}_{loam} = \{ \mathbf{p}_{i} \in \mathcal{P}_{loam} | \ \ \mathbf{N}(\mathbf{p}_{i})< t_{sur} \},
\end{aligned}
\end{equation} 
where $\mathcal{C}_{loam}$ and $\mathcal{S}_{loam}$ are the sets of corner features and surface features, respectively. $\kappa(\mathbf{p}_{i})$ and $\mathbf{N}(\mathbf{p}_{i})$ are the functions to calculate the curvature and normal of $\mathbf{p}_{i}$, respectively. $t_{cor}$ and $t_{sur}$ are the predefined thresholds. Then, voxel grid filtering is applied to downsample the feature points. The downsampled features are matched and registered with the features in the previous scan to estimate the relative pose between the current scan and the previous scan. Moreover, to eliminate the effect of dynamic objects, the features with too large residuals are removed. Finally, the preserved points are accumulated using the estimated poses to construct the 3D map.
\subsection{Traj LO}
Traj LO \cite{traj} is the state-of-the-art pure LiDAR-based SLAM method. The set of sampled data of the current scan is:
\begin{equation}
\mathcal{P}_{Traj} = \{ \{x_{1},y_{1},z_{1},I_{1},t_{1}\}, \{x_{2},y_{1},z_{2},I_{2},t_{2}\}, \cdots, \{x_{n},y_{n},z_{n},I_{n},t_{n}\} \},
\end{equation}
where $t_{i}$ is the sampling time of $\mathbf{p}_{i}$. As seen, compared to Eq. (\ref{loamdata}), Traj LO also utilizes the temporal information. First, both piecewise processing and linear interpolation are performed for motion compensation. In particular, the current scan (time window) is divided into $K$ equidistant segments of small resolution. Thus, the time interval becomes tiny by choosing a large value of $K$, enhancing the constant velocity assumption and allowing the method to handle rapid motion effectively. Traj LO proposes that the LiDAR trajectory (poses) is a function of time. Specifically, the LiDAR trajectory at the current time window, denoted by $\mathbf{T}(t)$, consists of $K$ piecewise functions: 
\begin{equation}
\mathbf{T}(t) = \{ \tau(t_{1}), \tau(t_{2}), \cdots, \tau(t_{K})  \},
\end{equation}
where $\tau(t_{i})$ is the function representing the LiDAR trajectory at the $i$-th time segment. Traj LO applies point-to-plane ICP for registration without selecting feature. $\mathbf{T}(t)$ is estimated by jointly minimizing the following energy function:
\begin{equation}
\underset{\mathbf{T}(t)^{*}}{\arg \min } \left( \mathcal{E}_{reg} + \mathcal{E}_{kine} + \mathcal{E}_{marg} \right),
\end{equation}
where $\mathcal{E}_{reg}$, $\mathcal{E}_{kine}$, and $\mathcal{E}_{marg}$ represent the registration energy, kinematics energy, and marginalization energy, respectively. Since each 3D point is associated with a sampling timestamp, its location in the world coordinate system can be determined using $\mathbf{T}(t)$. The 3D map is constructed by transforming the observed 3D points into the world coordinate system.

\section{Experimental Validation} \label{4.4}
\subsection{Extending LFM Localization to 3D Maps:\\ Tackling the First Limitation} \label{321}
As a reminder, the first limitation of the vanilla IFM is its restriction to single-view point clouds, rather than 3D maps. This section demonstrates how the proposed method addresses this limitation.
%
In particular, as presented in Table \ref{imtab1}, the proposed method is tested on eight 3D maps built from rosbags collected using a Livox MID 40 LiDAR \cite{sdk}. There are 2 to 4 ArUco markers, measuring 69.2 cm × 69.2 cm, in each map. The number of localized markers out of the total in each map is also included in Table \ref{imtab1}. The error in Table \ref{imtab1} is the average Euclidean distance between the estimated fiducial locations and the ground truth, which is obtained manually using CloudCompare \cite{cloudcompare}, a 3D annotation tool. 
\begin{table}[H]
\caption{Quantitative evaluation of marker localization on 3D maps. }
	\centering
	% \resizebox{0.8\columnwidth}{!}{
 
	\begin{center}
		\begin{tabular}{c|c|c}
			\hline \hline
				Scene & Localized/Total & Error (m) \\ \hline
    Fig. \ref{mov}(a)--Traj LO \cite{traj} & 2/2 & 0.016 \\ \hline
   Fig. \ref{mov}(a)--Livox Mapping \cite{sdk} & 2/2 & 0.013 \\ 
 \hline
   Fig. \ref{map2}-Traj LO \cite{traj} & 3/3 & 0.015  \\ \hline
   Fig. \ref{map2}-Livox Mapping \cite{sdk} & \textbf{2/3} & 0.020  \\ \hline 
Fig. \ref{mov}(c)--Map 1 & 2/2 & 0.026 \\ \hline
Fig. \ref{mov}(c)--Map 2 & 4/4 & 0.021 \\ \hline
Fig. \ref{mov}(c)--Map 3 & 2/2 & 0.017 \\
\hline
 Fig. \ref{map3} & 4/4 & 0.013
 \\ \hline \hline
			
		\end{tabular}
	\end{center}
		\label{imtab1}
\end{table} \par
The results in Fig. \ref{mov}(a) (outdoor office park, two maps) and Fig. \ref{map2} (indoor small parking lot, two maps) demonstrate the generalizability of the proposed algorithm to maps constructed using various methods \cite{traj,sdk}. 
%
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chthree/map2.png}
	\caption{The LiDAR scans the indoor parking lot from right to left, moving rapidly when scanning marker ID 0.}
	\label{map2}
\end{figure} 
%
Each scene uses the same rosbag as input for different SLAM methods, including Traj LO \cite{traj} and Livox Mapping \cite{sdk}, to build 3D maps. As shown, the proposed method successfully localizes all markers in these maps except for one marker in the map built by Livox Mapping in Fig. \ref{map2}. This is because Traj LO, as the state-of-the-art pure LiDAR-based SLAM method, is more robust to rapid motion \cite{traj} than Livox Mapping, and therefore produces maps with less motion blur. For this reason, all the remaining 3D maps in this section are built using Traj LO \cite{traj}. Thereafter, four additional scenes are presented, as shown in Fig. \ref{mov}(c) (maps 1-3) and Fig. \ref{map3}. As seen, all the fiducial markers are successfully localized. 
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chthree/map3.png}
	\caption{Traj LO \cite{traj} is utilized to create this 3D map.}
	\label{map3}
\end{figure}

\subsection{Extending the Valid LFM Localization Range:\\ Addressing the Second Limitation} \label{322}
The second limitation of the vanilla IFM is that the projection of the marker is too small to be detected when the distance between the marker and LiDAR is far. This section reports experimental results that demonstrate the improvements achieved by the proposed method in this regard. Specifically, as depicted in Fig. \ref{imgs}, we fix the position of a 69.2 cm $\times$ 69.2 cm ArUco marker and move the LiDAR to increase the relative distance. The intensity images obtained by the vanilla IFM and the proposed method at different distances are also presented in Fig. \ref{imgs}. 
\begin{figure}[ht] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chthree/imgs.png}
	\caption{Comparison of the intensity images generated by the vanilla IFM and the proposed method at different distances.}
	\label{imgs}
\end{figure}

The quantitative evaluation results are presented in Table \ref{imtab2}. As seen, the vanilla IFM fails to localize the marker when the distance reaches 20 meters, whereas the proposed method remains valid even when the distance extends to 50 meters. The intensity images shown in Fig. \ref{imgs} illustrate the reason: as the distance increases, the projection of the marker becomes smaller and smaller using the vanilla IFM. In contrast, the proposed method extracts the point cluster belonging to the marker and transforms it onto the intermediate plane, thereby maintaining a high-quality projection where the marker remains detectable. However, it was observed that at a distance of 50 meters, although the marker is localized, the method returns a false ID number. Upon observing the intensity image at 50 meters, we believe this occurs because the captured points become sparser and noisier as the distance increases, leading to poorer quality of the point cloud for the marker. \par

\begin{table}[H]
\caption{Quantitative evaluation of marker localization at various distances. }
	\centering
	% \resizebox{1.0\columnwidth}{!}{
 % {
	\begin{center}
		\begin{tabular}{c|c|c|c|c|c|c}
			\hline \hline
				Method/Distance (m) & 5 & 10 & 20 & 30 & 40 & 50 \\ \hline
Vanilla IFM Error (m) & 0.018 & 0.033 & fails & fails & fails & fails \\ \hline
			Ours Error (m) & 0.009 & 0.016 & 0.019 & 0.023 & 0.031 & 0.037 (false ID)  \\ \hline \hline
		\end{tabular}
  \end{center}
	% }
		\label{imtab2}
\end{table}
\subsection{Marker Localization Accuracy Improvement}
In this section, the same experimental setup shown in Fig. \ref{setup} is used to demonstrate that the proposed method boosts pose estimation accuracy compared to the vanilla IFM. In particular, a 16.4 cm $\times$ 16.4 cm AprilTag \cite{ap3} is placed in front of the LiDAR, and the ground truth pose is provided by a motion capture system. In addition to the vanilla IFM, a comparison is also made with the widely-used AprilTag 3 \cite{ap3}. When testing AprilTag 3, the adopted sensor is a camera. As shown in Table \ref{imtab3}, both the accuracy of the vanilla IFM and AprilTag 3 degrades as the distance from the sensor to the marker increases, but the degradation in accuracy is less severe for IFM compared to AprilTag 3. The accuracy of the proposed approach is slightly better than the vanilla IFM at a distance of 2m. However, unlike the vanilla IFM, the proposed approach maintains decent accuracy as the distance increases. The reason is illustrated in Fig. \ref{imgs}: the proposed method generates higher-quality intensity images thanks to cluster extraction and the adoption of the intermediate plane.
% \begin{table}[ht]
% \caption{Comparison of the proposed approach, the vanilla IFM, and AprilTag3 \cite{ap3} with respect to pose estimation accuracy.}
% \begin{center}

% \resizebox{1.0\columnwidth}{!}{

% \begin{tabular}{c|c|c|c|c|c|c|c}
% \hline \hline
% %&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
%  Distance& Method & X error (m) & Y error (m) &Z error (m)&  Roll error (deg)&  Pitch error (deg) & Yaw error (deg) \\
% \cline{1-8} 
% \multirow{3}{*}{ 2m} & AprilTag3 \cite{ap3} & 0.016 & 0.034 &0.016& 0.807&  3.166 &  1.244 \\ \cline{2-8} 
% & Vanilla IFM & 0.003 & 0.006 &0.017& 0.423&  0.457 &  0.399 \\ \cline{2-8} 
% & \textbf{Ours} &\textbf{ 0.002} &\textbf{ 0.005}  &\textbf{0.011}&\textbf{ 0.315}&  \textbf{0.305} & \textbf{ 0.391} \\ \cline{1-8}

% \multirow{3}{*}{ 3m} & AprilTag3 \cite{ap3} & 0.058 & 0.124 &0.044& 1.369&  7.963 & 2.904 \\ \cline{2-8} 
% & Vanilla IFM & 0.026  & 0.021 &0.093& 0.930&  1.182 &  0.859 \\ \cline{2-8} 
% & \textbf{Ours} &\textbf{ 0.006} &\textbf{ 0.009} &\textbf{0.015}&\textbf{ 0.343}& \textbf{ 0.322} & \textbf{ 0.455} \\ \cline{1-8}

% \multirow{3}{*}{ 4m} & AprilTag3 \cite{ap3} & 0.072 & 0.407 &0.233& 1.433&  9.292 &  13.343 \\ \cline{2-8} 
% & Vanilla IFM & 0.024 & 0.024 &0.107& 0.342&  2.020 &  1.111 \\ \cline{2-8} 
% & \textbf{Ours} &\textbf{ 0.008} & \textbf{0.014} &\textbf{0.016}&\textbf{ 0.302}&  \textbf{0.389} &\textbf{ 0.478} \\ \cline{1-8}
% \hline \hline
% \end{tabular}
% \label{imtab3}
% }
% \end{center}
% \end{table}
\begin{table}[H]
\begin{center}
\caption{Comparison of the proposed approach, the vanilla IFM, and AprilTag3 \cite{ap3} with respect to pose estimation accuracy.}
\begin{tabular}{c|c|c|c|c}
\hline \hline
%&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
 Distance& Method & X error (m) & Y error (m) &Z error (m) \\
\cline{1-5} 
\multirow{3}{*}{ 2m} & AprilTag3 \cite{ap3} & 0.016 & 0.034 &0.016 \\ \cline{2-5} 
& Vanilla IFM & 0.003 & 0.006 &0.017 \\ \cline{2-5} 
& \textbf{Ours} &\textbf{ 0.002} &\textbf{ 0.005}  &\textbf{0.011} \\ \cline{1-5}

\multirow{3}{*}{3m} & AprilTag3 \cite{ap3} & 0.058 & 0.124 &0.044 \\ \cline{2-5} 
& Vanilla IFM & 0.026  & 0.021 &0.093 \\ \cline{2-5} 
& \textbf{Ours} &\textbf{ 0.006} &\textbf{ 0.009} &\textbf{0.015} \\ \cline{1-5}

\multirow{3}{*}{ 4m} & AprilTag3 \cite{ap3} & 0.072 & 0.407 &0.233 \\ \cline{2-5} 
& Vanilla IFM & 0.024 & 0.024 &0.107 \\ \cline{2-5} 
& \textbf{Ours} &\textbf{ 0.008} & \textbf{0.014} &\textbf{0.016} \\ \cline{1-5}

Distance& Method & Roll error (deg)&  Pitch error (deg) & Yaw error (deg) \\ \hline
\multirow{3}{*}{ 2m} & AprilTag3 \cite{ap3} & 0.807&  3.166 &  1.244 \\ \cline{2-5} 
& Vanilla IFM & 0.423&  0.457 &  0.399 \\ \cline{2-5} 
& \textbf{Ours} &\textbf{ 0.315}&  \textbf{0.305} & \textbf{ 0.391} \\ \cline{1-5}

\multirow{3}{*}{ 3m} & AprilTag3 \cite{ap3} & 1.369&  7.963 & 2.904 \\ \cline{2-5} 
& Vanilla IFM & 0.930&  1.182 &  0.859 \\ \cline{2-5} 
& \textbf{Ours} &\textbf{ 0.343}& \textbf{ 0.322} & \textbf{ 0.455} \\ \cline{1-5}

\multirow{3}{*}{ 4m} & AprilTag3 \cite{ap3} & 1.433&  9.292 &  13.343 \\ \cline{2-5} 
& Vanilla IFM & 0.342&  2.020 &  1.111 \\ \cline{2-5} 
& \textbf{Ours} &\textbf{ 0.302}&  \textbf{0.389} &\textbf{ 0.478} \\ 
\hline \hline
\end{tabular}
\label{imtab3}

\end{center}
\end{table}



\subsection{Application and Discussion} 
The localized fiducials on the 3D maps are beneficial for downstream tasks, such as map merging. Specifically, as presented in Fig. \ref{mov}(c), multiple large-scale 3D maps built by LiDAR-based SLAM can be merged using the method introduced in Section \ref{multi}, utilizing the fiducials provided by the proposed method on 3D maps.\par
As illustrated by the experiments conducted in Sections \ref{321} and \ref{322}, the intention behind improving the vanilla IFM is to handle large-scale outdoor scenes and cases where the distance between the LiDAR and the marker is significant. Nevertheless, the improvements come at the cost of increased computational time. Thus, unless faced with challenging large-scale scenarios, the vanilla IFM is capable of handling regular small-scale scenes and is preferred for its simplicity and efficiency.
