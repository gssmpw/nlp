\section{Motivation}
Light Detection and Ranging (LiDAR) sensors are crucial ranging sensors for autonomous systems \cite{loam,traj,rangenet}. In contrast to another vital sensor, the camera, the development and application of LiDAR fiducial markers (LFMs) are rare. Specifically, research on Visual Fiducial Marker (VFM) systems \cite{wang,olson,ap3,aruco,cctag}, fiducial marker systems developed for cameras, has a long history, with extensive experience and significant research achievements accumulated. VFMs provide environments with controllable artificial features, making feature extraction and matching simpler and more reliable. VFM systems have been applied in various camera-based applications, including Augmented Reality (AR) \cite{ar}, human-robot interaction \cite{wang,olson}, navigation \cite{yibo1,yibo2,liao}, multi-sensor calibration \cite{kalibr,lt2}, Structure-from-Motion (SfM) \cite{munoz,qingdao}, and visual Simultaneous Localization and Mapping (SLAM) \cite{munoz2019,shuo,shuo2}. \par
LiDAR-based mapping and localization \cite{loam,lloam,sdk,traj,sghr,multiway,mdgd} are fundamental in robotics and computer vision. Similar to visual SLAM \cite{shuo,shuo2,shuo3} and SfM \cite{munoz,qingdao} in camera-based mapping and localization, there are two main categories of solutions. The first category is LiDAR-based SLAM approaches \cite{loam,lloam,sdk,traj}, which process consecutive LiDAR scans in real time to map the scene and localize the LiDAR sensor simultaneously. The second category is multiview point cloud registration methods \cite{sghr,multiway,mdgd}. Compared with SLAM approaches that process LiDAR scans sequentially, these methods handle a set of unaligned point clouds all at once in an offline manner. The mapping is achieved by registering the multiview point clouds into one complete point cloud, and localization is accomplished by finding the relative pose between point clouds.
To clarify the motivation, this dissertation will answer three questions in this field:
\begin{itemize}
\item Given the success of VFM in camera-based applications, why is it still desirable to exploit the LiDAR sensor and LFM?

\item Considering the existence of previous LiDAR fiducial objects, why is it favorable to develop a new type of LiDAR fiducial marker system?

\item Given that previous LiDAR-based mapping and localization methods do not rely on fiducial markers, why is it advantageous to exploit LFM in this dissertation?
\end{itemize}
\par
\noindent\textbf{Response to the First Question.} There are two reasons why LiDAR sensors and LFM are irreplaceable by cameras and VFM. 
First, as a ranging sensor, the role of LiDAR in autonomous systems \cite{waabi,waymo,cd} cannot be replaced by the camera. While Red Green Blue-Depth (RGB-D) cameras can also measure depth, their effective range is usually less than 10 meters \cite{3dmatch,eth,scan}, whereas LiDAR can scan objects several hundred meters away \cite{loam,traj,rangenet}.
Additionally, due to the unique modality of LiDAR point clouds, data captured by LiDAR cannot be replaced by data acquired from cameras. Extensive learning-based models \cite{rangenet,cd,mending} rely on LiDAR data for training.
%
Second, unlike VFM detection, which is sensitive to ambient light \cite{ap3,aruco,cctag}, LiDAR-based object detection is robust to unideal illumination conditions \cite{lt,iilfm}, such as purely dark or overexposed environments. Third, VFM detection suffers from rotational ambiguity \cite{munoz2018,qingdao,munoz2019} and requires discarding ambiguous measurements, whereas LiDAR-based planar object pose estimation is free from this issue \cite{lt}.
\par

\noindent\textbf{Response to the Second Question.}
There have been some fiducial objects developed for LiDAR sensors \cite{cal,cal2,a4,lt} for calibration purposes. There are three reasons why it is desired to develop a new type of LFM system. First, most existing LiDAR fiducial objects \cite{cal,cal2,lt,a4} are designed for specific LiDAR models and are not generalizable to a wide range of solid-state and mechanical LiDARs. Second, unlike popular VFMs such as AprilTag \cite{ap3} and ArUco \cite{aruco}, which are thin-sheet objects attached to surfaces without affecting the 3D geometry of the environment, most existing LiDAR fiducial objects \cite{cal,cal2,lt} are thick boards placed on tripods, acting as additional 3D objects that alter the environment. These extra 3D objects are undesirable in applications such as scene reconstruction and data collection. Third, a typical calibration board \cite{cal,cal2,a4} does not have complex patterns generated from elaborately designed encoding-decoding algorithms like VFMs \cite{wang,olson,ap3,aruco,cctag}, which are robust against false positives and negatives. 
\par
Thus, the first major objective of this dissertation is to develop a unified LFM system that is applicable to a variety of LiDAR sensors, while being as convenient to use as VFMs: printed on sheets of paper/board and attached to surfaces without affecting the 3D environment. The unified system is desirable because we do not need to redesign the algorithm/marker every time the sensor is changed, and it is beneficial for sensor fusion of different models. Additionally, the system should incorporate a reliable encoding-decoding algorithm, like VFMs \cite{wang,olson,ap3,aruco,cctag}, for robust detection. 
\par
\noindent\textbf{Response to the Third Question.}
% There are two reasons it is beneficial to integrate LFM into mapping and localization. First, the introduction of LFM makes feature extraction and matching simpler and more reliable, thereby boosting the efficiency and accuracy of downstream tasks such as 3D asset collection and training data collection. 
The existing point cloud registration methods, including geometry-based methods \cite{teaser,kiss,lloam,random} and learning-based methods \cite{sghr,pre,mdgd,se3et,geotransformer}, rely on shared geometric features between point clouds to align them.
%
Hence, these methods struggle in scenes with scarce shared features, such as extremely low overlap cases and degraded scenes \cite{degradation}. 
%
Consequently, the existing methods \cite{mdgd,sghr,se3et,geotransformer,teaser} are unsuitable for tasks such as capturing the complete 3D shape of a novel object from a sparse set of scans taken from dramatically different viewpoints, gathering data from unseen scenarios for training, reconstructing degraded scenes, or merging large-scale 3D maps with low overlap.
%
In contrast, the introduction of LFM makes feature extraction and matching simpler and more reliable. Thanks to this, the proposed method is robust to any unseen scenarios with extremely low overlap, making it a convenient, efficient, and low-cost tool for diverse applications that pose significant challenges to existing methods.
%
To help readers understand this point, a case with similar motivation in a camera-based application is presented: Although marker-free SfM methods \cite{colmap,dust3r} exist, when constructing the Omniobject3D \cite{omni} dataset—a collection of posed multiview images of real-world objects—VFM-based SfM is employed to efficiently and robustly estimate the relative pose between images using a chessboard composed of AprilTags \cite{ap3}. 
%
% Second, applying LFMs helps tackle challenging textureless scenes or cases with dramatic viewpoint changes. 
One may consider adding external sensors, such as Inertial Measurement Units (IMUs), to the LiDAR sensor to tackle challenging textureless scenes or cases with dramatic viewpoint changes. However, this involves multi-sensor calibration and synchronization. In comparison, LFMs made from sheets of paper or board are low-cost, easy to use, and reliable. This motivation is the same as that of VFM-based SfM \cite{munoz,qingdao} and visual SLAM \cite{munoz2019,shuo} studies.
\par
In summary, the main objective of this dissertation is to develop a framework for mapping and localization using LFMs. This framework will serve as a fundamental tool for robotics and computer vision, enabling a variety of real-world applications, including AR, 3D asset collection from sparse scans, training data collection in unseen scenes, reconstruction of degraded scenes, localization in GPS-denied environments, and merging large-scale low overlap maps. In particular, considering that training data collection for point cloud registration is time-consuming and expensive, and existing datasets \cite{3dmatch,eth,scan} are limited to indoor scenes captured using RGB-D cameras, it is desirable to construct a new training dataset in a straightforward and low-cost manner to benefit learning-based point cloud registration methods \cite{sghr,mdgd}. 
\section{Related Work and Challenges}
\label{Challenges}
\subsection{Visual Fiducial Markers}
VFMs \cite{wang,olson,ap3,aruco,cctag,arc,arc2} are particular objects, commonly planar, that provide the environment with controllable artificial features. They were originally developed for augmented reality \cite{ar}, but have been widely applied in various robotics and computer vision applications, such as human-robot interaction \cite{wang,olson}, navigation \cite{yibo1,yibo2,liao}, multi-sensor calibration \cite{kalibr,lt2}, SfM \cite{munoz,qingdao}, and visual SLAM \cite{munoz2019,shuo,shuo2}. The previous research on the VFM system mainly focuses on the following four aspects: (1) A higher detection rate. Line segmentation methods have been continuously upgraded alongside VFM systems \cite{olson,wang}. In addition to line segmentation, non-square shape detection methods, such as ellipse detection algorithms \cite{arc,arc2,cctag}, have also emerged and made valuable contributions. These advancements aim to improve the detection of line segments, candidate quads \cite{ap3,aruco}, or circles \cite{arc,arc2,cctag}, thereby enhancing the marker detection rate under varying ambient light and/or motion blur. (2) A lower false positive rate. The encoding-decoding algorithms of the latest VFM systems \cite{ap3,aruco} have been upgraded compared to earlier generations \cite{wang,olson}, making the identification of candidate markers more reliable and reducing false positives during decoding. (3) A lower computational time. Again, through the amelioration of methods for graphic segmentation and algorithms adopted in encoding-decoding algorithms, the VFM systems are accelerated. For instance, the speed of the third generation of Apirltag \cite{ap3} is almost 5 times faster than that of the second generation \cite{wang}. (4) Resolving the rotation ambiguity problem. The rotational ambiguity means a planar marker could project onto the same pixels from two different poses when the perspective effect is weak \cite{ippe}. Much research \cite{yibo,ch2020} has been conducted to solve this problem by adding external sensors or changing the maker from 2D to 3D. Unfortunately, rotational ambiguity is still an unresolved problem for 2D VFM detection without external information.
\par
However, the aforementioned VFM algorithms were developed for 2D images, not 3D LiDAR point clouds. Given the differences between 2D image and 3D point cloud modalities, it is challenging—though beneficial and desirable—to transfer the advancements made in VFM research to LiDAR-based applications \cite{lt}.
%
\subsection{LiDAR Fiducial Objects}
\subsubsection{LiDAR Fiducial Object Patterns}
Most existing LiDAR fiducial objects \cite{cal,cal2,a4} are designed for calibration purposes. Fig. \ref{tags}(a) shows a typical calibration board, a thick board with holes and/or regions covered by high-intensity materials. While a calibration board can provide fiducials (holes, corners, and high-intensity regions) and it is feasible to assign specific indexes to the fiducials, a calibration board is fundamentally different from fiducial marker systems \cite{ap3,aruco}. This is because the design of calibration board patterns does not incorporate the core element of a fiducial marker system—the encoding-decoding algorithm. Consequently, calibration boards \cite{cal,cal2} do not support rich patterns with unique identifications (IDs) and the robust detection provided by encoding-decoding algorithms \cite{wang,olson,ap3,aruco,cctag,arc,arc2} as shown in Figs. \ref{tags}(b)(c).
%
The development of VFMs and LiDAR fiducial objects had been disjointed until the proposal of LiDARTag \cite{lt}, as illustrated in Fig. \ref{tags}(d).
LiDARTag, as the first fiducial marker system for LiDAR sensors, integrates the encoding-decoding algorithm of AprilTag 3 \cite{ap3} into LiDAR fiducial object detection. Consequently, it inherits the rich patterns and robust detection features of AprilTag 3. However, LiDARTag only studies square markers with patterns belonging to AprilTag 3 and does not support square markers with different pattern sources, for instance, ArUco \cite{aruco}, or non-square markers, such as CCTag \cite{cctag}. Developing a general algorithm that fully incorporates the achievements of VFM research while not requiring a specific marker pattern and shape is challenging. As shown in Fig. \ref{tags}(e), the other widely used LiDAR fiducial objects are prisms \cite{station}, which are designed to reflect laser beams and produce very high intensity, making them easy to detect. However, the cost of prisms is much higher than that of VFMs, and they also do not contain the encoding-decoding algorithm to support rich patterns that are robust to false positives/negatives.
%
\subsubsection{LiDAR Fiducial Object Placement}
3D LiDAR point clouds are sparse and unstructured compared to 2D images, making it challenging to develop an LFM system \cite{lt}. Existing LiDAR fiducial objects \cite{cal1,cal2}, including LiDARTag \cite{lt}, use conventional geometry-based clustering methods \cite{edge,single,rangenet}, requiring spatial distinction. 
%
Thus, as seen in Fig. \ref{tags}(a) and (d), they are often placed on tripods, introducing extra 3D objects that impact the environment. 
%
For instance, LiDARTag adopts the single-linkage clustering algorithm \cite{single} and requires that the marker must have at least $t_{L}\sqrt{2}/4$ clearance around it, where $t_{L}$ represents the marker's size. If the marker is attached to a wall or a box, it is necessary to ensure that $\omega > t_{L}\sqrt{2}/4$, where $\omega$ is the thickness of the marker's 3D object. Now, consider the case where ten LiDARTags are placed in an indoor environment. This situation would result in a room crowded with tripods and large markers. 
%
Although the sizes of prisms \cite{station} (see Fig. \ref{tags}(e)) are smaller than those of calibration boards or LiDARTags, they still act as additional objects.
%
Moreover, in applications such as training data collection for learning-based point cloud registration methods \cite{sghr,mdgd}, these additional 3D objects are undesirable as they wreck the 3D geometry of the scene. However, considering the different modalities of 3D LiDAR point clouds and 2D images, it is challenging to develop an LFM system as convenient as VFMs (see Fig. \ref{tags}(c)). 
%
\begin{figure}[H] 
	\centering
	\includegraphics[width=0.8\linewidth]{figs/intro/alltags.png}
	\caption{ Comparison of (a) a typical calibration board, (b)(c) VFM, (d) LiDARTag, and (e) prisms in terms of object patterns and placement. 
 }
	\label{tags}
\end{figure}

\subsection{LiDAR-based Mapping and Localization}
\subsubsection{LiDAR-based SLAM}
LiDAR Odometry and Mapping (LOAM) \cite{loam} is a milestone work in LiDAR-based SLAM, inspiring numerous follow-up studies \cite{floam,lloam,aloam,sdk,kiss,traj}. It splits the SLAM problem into two parallel tasks: LiDAR Odometry (LO) and mapping. LO is performed on the front end at high frequency to extract feature points (\textit{e.g.} corners, surfaces, \textit{etc}.) and use them to estimate the LiDAR pose in real-time. Mapping operates on the back end at a lower frequency to refine the odometry results and construct a more accurate map. LiDAR-based SLAM methods \cite{loam,floam,lloam,aloam,sdk,kiss,traj} focus on sequentially processing consecutive LiDAR scans and are designed for use cases that particularly require real-time response. Applications with the following three features are not use cases for LiDAR-based SLAM: (1) require processing a set of point clouds all at once, (2) involve an unordered set and/or point clouds with low overlap (dramatic viewpoint changes), and (3) allow for offline operation. In contrast, multiview point cloud registration methods are designed for these cases.


\subsubsection{Multiview Point Cloud Registration}
Multiview point cloud registration \cite{sghr,multiview,multiway,mdgd} is also a popular solution for LiDAR-based mapping and localization. It directly processes a set of point clouds, which can be unordered and have low overlap, in an offline manner. In particular, mapping is achieved by registering the multiview point clouds into a complete point cloud, while localization is done by determining the relative pose between them. The mapping and localization framework introduced in this dissertation belongs to this category.
\par
Geometry-based point cloud registration methods, such as variants of Iterative Closest Point (ICP) methods \cite{kiss,lloam} and Teaser++ \cite{teaser}, mainly focus on designing efficient and robust algorithms for describing geometry and extracting geometric features (\textit{e.g.}, points, lines, and surfaces/normals) to find correspondence between point clouds and align them. In learning-based point cloud registration methods like Predator \cite{pre} and SGHR \cite{sghr}, neural networks are designed to learn features representing the geometry \cite{features,multiview,3dmatch,se3et,geotransformer}, and then learn to utilize these features for registering the point clouds. However, most existing methods \cite{kiss,lloam,features,3dmatch} apply only to high overlap scenarios, making them less robust in practical applications \cite{pre}. Some recent learning-based research proposes multiview registration methods \cite{sghr,multiview,multiway} and studies low overlap cases \cite{pre,sghr}. Despite their promising performance on benchmarks \cite{3dmatch,eth,scan}, the generalization of learning-based methods to unseen scenarios (\textit{i.e.}, out-of-distribution cases of training data) remains problematic. Moreover, the existing methods \cite{mdgd,sghr,se3et,geotransformer,teaser} face challenges in handling degraded scenes, such as repetitive structures and textureless environments. In camera-based applications \cite{munoz,qingdao,munoz2019}, VFMs have been utilized to tackle challenging degraded scenes and low-overlap cases, while exploiting LFMs for multiview point cloud registration remains an open problem.
\subsection{Training Data Collection for Point Cloud Registration}
Training data is crucial for the development of learning-based point cloud registration methods \cite{sghr,mdgd}, but collecting it is time-consuming and expensive. For instance, in previous dataset collections \cite{3dmatch,eth,scan}, external sensors such as cameras, IMUs, and GPS are employed to obtain ground truth poses among point clouds. This requires careful multi-sensor calibration and synchronization, which can be time-consuming and labor-intensive. A common approach \cite{sghr,pre} to evaluating a learning-based point cloud registration model is to train it on 3DMatch \cite{3dmatch} and test it on various benchmarks, including 3DMatch \cite{3dmatch}, ETH \cite{eth}, and ScanNet \cite{scan}. However, the 3DMatch benchmark is mainly constructed from RGB-D camera captures of indoor scenes \cite{3dmatch}. Collecting a new training dataset with outdoor scenes and point clouds captured by a LiDAR sensor for point cloud registration is beneficial for improving the performance of learning-based methods. Unfortunately, this is challenging due to the lack of an efficient, reliable, and easy-to-use tool for the task.

\section{Contributions}
This dissertation makes several contributions to addressing the technological gap in utilizing LFMs for mapping and localization introduced in Section \ref{Challenges}. Specifically, the main contributions of this dissertation are as follows:
\begin{itemize}
     

\item Intensity Image-Based LiDAR Fiducial Marker (IFM) System. A new IFM system is introduced that supports various LiDAR models and integrates seamlessly with existing VFM patterns \footnote{Please note that the focus of our research is enabling LiDAR sensors to detect thin-sheet markers with patterns from VFM research, rather than proposing new patterns.}. This system offers a practical and cost-effective solution by allowing users to generate markers with standard printing methods, making the process as convenient as current VFM systems.

\item Advanced LFM Detection Method. A novel approach is developed to detect 3D fiducial markers through intensity images. This enables the integration of VFM systems into the IFM system, enhancing its flexibility compared to existing systems like LiDARTag, which are limited to specific marker patterns.

\item Improved Pose Estimation. A new pose estimation technique for LiDAR is proposed, achieving higher accuracy than traditional VFM-based camera systems. The method also eliminates rotation ambiguity and provides robustness against lighting variations.

\item Enhanced Localization of Thin-Sheet LFMs. Limitations of the vanilla IFM are addressed. A method is introduced to extend LFM localization to 3D LiDAR maps and increase the detection distance of LFMs.

\item Joint Analysis of Intensity and Geometry. A novel pipeline is proposed for detecting planar fiducial markers by jointly analyzing 3D point clouds from both intensity and geometry perspectives. This differs from conventional methods, which rely solely on geometric features and are ineffective for detecting planar objects indistinguishable from their background.

\item Framework for Mapping and Localization. A comprehensive framework for mapping and localization using LFMs is developed. This framework registers unordered multiview point clouds with low overlap, providing a reliable tool for various applications, such as 3D asset collection from sparse scans, training data collection in unseen scenes, degraded scene reconstruction, GPS-denied environment localization, and 3D map merging.

\item Livox-3DMatch Dataset. A new training dataset, Livox-3DMatch, is created, augmenting the original 3DMatch data. This expanded dataset improves the performance of state-of-the-art learning-based methods across multiple benchmarks.

\item Adaptive Threshold Detection Algorithm. A viewpoint-robust LFM detection algorithm is developed, ensuring reliable detection in varying real-world conditions.
\end{itemize}
These contributions collectively advance the use of LFMs in robust and efficient LiDAR-based mapping and localization systems, addressing critical challenges and unlocking new possibilities in robotics and computer vision.



% \subsection{Intensity Image-based LiDAR Fiducial Marker System}
% \begin{itemize}
% \item The development of a novel fiducial marker system for the LiDAR, the vanilla intensity image-based LiDAR fiducial marker (IFM) system. Unlike LiDARTag \cite{lt} which requires extra 3D objects to be added to the environment, the usage of the IFM is as convenient and easy as the VFM systems \cite{ap3,aruco}. Namely, the users can produce the marker by printing the VFM on regular letter-size paper with a regular printer and then place the marker anywhere they like.
% \item The proposal of a novel marker detection method to detect 3D fiducials through the intensity image. Thanks to this, the VFM systems proposed in the past, present, and even future can be easily embedded into the IFM system. This is a superiority of the proposed system over LiDARTag \cite{lt} which only supports square markers with patterns from AprilTag 3 \cite{ap3}.
% \item The introduction of a pose estimation approach for the LiDAR via the proposed IFM, which has better accuracy than the VFM-based pose estimation for the camera. In addition, unlike the VFM system, the proposed pose estimation is free from the rotation ambiguity problem \cite{ippe,yibo} and is robust to changes in ambient light. 
% % \item {\color{blue}The release of the open-source implementation based on ROS and C++. Please refer to https://github.com/York-SDCNLab/IILFM. The proposed system adopts many user-friendly designs, and thus the users who are familiar with the VFM systems can quickly get started with the proposed system.}
% \end{itemize}

% \subsection{Improvements to Vanilla IFM Localization}
% Due to the adoption of 3D-to-2D spherical projection, the vanilla IFM exhibits two limitations: (1) IFM can only detect fiducials in a single-view point cloud and does not apply to a 3D LiDAR map, and (2) as the distance between the tag and the LiDAR increases, the projection size of the tag decreases until it is too small to be detected. The following contributions are made to address the limitations of the vanilla IFM.


% \begin{itemize}
% 	\item The development of a novel method improves the localization of thin-sheet LFMs by extending marker detection from the single-view point cloud to the 3D LiDAR map and enhancing the detectable distance of LFMs.
% 	\item The proposal of a new pipeline for jointly analyzing a 3D point cloud from both intensity and geometry perspectives, given that fiducial tags are planar objects with high-intensity contrast and are indistinguishable from the plane to which they are attached. This differs from conventional 3D object detection methods that rely merely on 3D geometric features and can only detect spatially distinguishable objects.
% \end{itemize}

% \subsection{Exploiting LFMs for Mapping and Localization}
% \begin{itemize}
% 	\item The design of a novel framework for mapping and localization using LFMs, where mapping and localization are achieved by registering unordered multiview point clouds, even with low overlap. The proposed framework serves as an efficient and reliable tool for diverse robotics and computer vision applications, including 3D asset collection, training data collection, reconstruction of degraded scenes, localization in GPS-denied environments, and 3D map merging.
% 	\item The collection of a new training dataset called Livox-3DMatch using the proposed framework, which augments the original 3DMatch training data \cite{3dmatch} from 14,400 pairs to 17,700 pairs (a \textbf{22.91\%} increase). By training on this augmented dataset, the performance of the SOTA learning-based method SGHR \cite{sghr} is improved by \textbf{2.90\%} on 3DMatch \cite{3dmatch}, \textbf{4.29\%} on ETH \cite{eth}, and \textbf{22.72\%} (translation) / \textbf{11.19\%} (rotation) on ScanNet \cite{scan}.
% \item The development of an adaptive threshold LFM detection algorithm that is robust to viewpoint changes in the wild.
 
% \end{itemize}

\section{Dissertation Organization}
This dissertation is organized into five chapters, in addition to the Introduction.
%
% Section \ref{threedimen} introduces the definitions and notation of three-dimensional transmission. Section \ref{lie} introduces the definitions and notations of Lie groups and Lie algebras, as well as their transformation into each other. 
Chapter \ref{IFM} is about the development of a general fiducial marker system for the LiDAR sensor, named IFM. Section \ref{3.1} presents an overview of the IFM system, including its overall pipeline and advantages compared to closely related prior work LiDARTag \cite{lt}. Chapter \ref{pre} mainly introduces the preliminary knowledge of three-dimensional transformation, Lie group and Lie algebra, and LiDAR technology. Section \ref{3.2} introduces a method for detecting 3D fiducials in a point cloud through the intensity image. Section \ref{3.3} presents the estimation of the 6-degree-of-freedom (6-DOF) LiDAR pose using the IFM. Section \ref{3.4} reports the qualitative and quantitative experimental results and analysis regarding the efficiency and limitations of the vanilla IFM. The developments listed in this chapter, and related materials have been published in the journal article \cite{iilfm}. The open-source implementation, based on C++ and Robot Operating System (ROS), as well as the datasets, are available at \url{https://github.com/York-SDCNLab/IILFM}.
\par
Chapter \ref{improve} discusses an algorithm that addresses the two limitations of the vanilla IFM: its inapplicability to 3D LiDAR maps and its limited detectable range. Section \ref{4.1} presents an overview of the proposed algorithm, including a comparison to the vanilla IFM in terms of the pipeline for processing point clouds and applicable scenes. Section \ref{4.2} discusses a new method that jointly analyzes point clouds from both intensity and geometry perspectives to find point clusters that cloud contain fiducial markers.
Section \ref{4.3} presents a method for localizing fiducials within the point clusters via a designed intermediate plane. Section \ref{new4.4} introduces how the 3D maps are constructed. Section \ref{4.4} reports the experimental results demonstrating the improvements brought by the proposed method over the vanilla IFM. The developments listed in this chapter, and related materials have been published in the journal article \cite{access}. The open-source implementation based on C++ is available at \url{https://github.com/York-SDCNLab/Marker-Detection-General}.
\par
Chapter \ref{multi} is about the development of a framework exploiting LFMs for mapping and localization.
%
Section \ref{5.1} provides an overview of the proposed framework, including its overall pipeline and the rich downstream robotic and computer vision applications. Section \ref{new5.2} introduces the methodology.
% including an adaptive threshold LFM detection method to address the unstable detection of the vanilla IFM when the viewpoint changed dramatically in the wild. Section \ref{5.3} presents the formulation of the mapping and localization problem as a MAP problem. Section \ref{1level} introduces the design of the first-level graph in the framework, which handles the unordered set of multiview point clouds and estimates the initial values of poses between them.
% Section \ref{5.5} proposes the second-level graph of the proposed framework, through which the MAP problem is addressed by globally optimizing the variables on it. 
Section \ref{dataset} introduces the construction of a new training dataset for point cloud registration, named Livox-3DMatch. Section \ref{5.7} reports the experimental results that validate the proposed framework and demonstrate its rich applications in robotic and computer vision. The developments listed in this chapter and related materials are accepted in the journal article \cite{lpr}. 
%
The open-source implementation, based on C++ and Python, as well as the datasets, are available at \url{https://github.com/yorklyb/L-PR}.
\par
Finally, the conclusions and future work are presented in Chapter \ref{conclusion}.