\section{Overview} \label{5.1}
This chapter introduces a framework for mapping and localization using LFMs. Specifically, mapping is performed by registering multiview point clouds, while localization is achieved by estimating the relative poses between them. Fig. \ref{mov2} provides an overview of the proposed framework. As shown, the LFMs are thin sheets of board or paper attached to surfaces, with no impact on the environment's geometry, making them suitable for tasks such as training dataset collection for point cloud registration. 
%
Given an unordered set of low-overlap point clouds, the proposed method efficiently registers them in a global frame. 
%
Due to the use of a constant threshold for processing intensity images (See Fig. \ref{dis}(c)), the vanilla IFM becomes unstable when the viewpoint varies significantly. To address this issue, an adaptive threshold marker detection method, robust to viewpoint changes, is proposed.
%
The multiview point cloud registration problem is formulated as a MAP problem, and tackled using two-level graphs.
%
The first-level graph, constructed as a weighted graph, efficiently processes the unordered point clouds and estimates the initial poses between them. The weights represent the pose estimation error of each marker observation, and the optimal initial poses are obtained by finding the shortest path from an anchor scan to each non-anchor scan.
\begin{figure}[H] 
	\centering
\includegraphics[width=0.8\linewidth]{figs/chfour/mov3.png}
	\caption{An overview of the proposed framework for mapping and localization using LFMs.}
	\label{mov2}
\end{figure}
Using the initial values, the second-level graph, a factor graph, resolves the MAP problem by globally optimizing the poses of point clouds, markers, and marker corner positions. We conduct both qualitative and quantitative experiments to demonstrate the superiority of the proposed method over competitors \cite{mdgd,sghr,se3et,geotransformer,teaser,qingdao,kiss}. Especially, the proposed method is robust to any unseen scenarios with extremely low overlap, making it a convenient, efficient, and low-cost tool for diverse applications that pose significant challenges to existing methods. As shown in Fig. \ref{mov2}, the downstream applications include 3D asset collection from sparse scans, training dataset collection for point cloud registration in unseen scenes, reconstruction of degraded scenes, navigation in GPS-denied environments, and merging of large-scale low overlap 3D maps.
%
\section{Methodology} \label{new5.2}
\subsection{Adaptive Threshold LFM Detection} \label{5.2}
In the vanilla IFM method, binarization is applied to the raw intensity image due to the imaging noise (See the zoomed views of Fig. \ref{threshold}). As shown in Fig. \ref{threshold}, the effect is determined by the threshold $\lambda$.
%
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chfour/threshold.png}
	\caption{The raw intensity image binarized with different threshold values.}
	\label{threshold}
\end{figure}
%
In Section \ref{IFM}, it is found that $\lambda$ can be selected as a constant if the viewpoint does not change drastically. However, with significant changes in the scene, the value of $\lambda$ needs adjustment. 
%
As an example, consider the case shown in Fig. \ref{threshold}, where three markers are placed. Markers identification (ID) 1 and 4 are detectable when $\lambda$=13, while marker ID 3 is detectable when $\lambda$=70. Thus, $\lambda$=13 and $\lambda$=70 are the optimal thresholds compared to other values for markers ID 1 and 4 and marker ID 3, respectively, denoted by $\lambda^{*}$.
%
Therefore, unless an appropriate $\lambda$ is carefully selected for each point cloud (viewpoint) and for each marker, the LFM detection will fail.
%
Unlike the vanilla IFM, which determines $\lambda^{*}$ based on experience or experimentation, an algorithm to automatically seek $\lambda^{*}$ is developed in this section.
\begin{algorithm}[th]
% \small
    \KwIn{Raw intensity image, $\mathcal{I}$}
    \KwOut{The optimal threshold, $\lambda^{*}$.}
    
    % \vspace{2mm}
    \textbf{Initialize parameters:} Search scope, $S$. step size, $\delta$. 
    queue for saving detected markers, $Q=[ \ ]$. length of $Q$, $Q_l=0$.
    temporary queue, $Q_{temp}=[\ ]$. length of  $Q_{temp}$,  $Q_{temp,l}=0$. the optimal threshold, $\lambda^{*}=0$. search step, $i=0$.\\
    % \vspace{2mm}
    Define the binarization operation as $\Psi(\mathcal{I},\lambda)$. \\ Define the marker detector operation as $\Gamma(\mathcal{I})$. \\
    \While{$i < S$}{
    $\lambda = \delta\times i$\\
        $\mathcal{I} = \Psi(\mathcal{I},\lambda)$\\
         $\Gamma(\mathcal{I}) \rightarrow Q_{temp}$\\
        \If{$Q_{temp,l} \geq Q_{l}$}{
        \For{marker \textbf{in} $Q_{temp}$}{
        \If{marker \textbf{not in} $Q$}{append \textit{marker} to $Q$}
        }
        $\lambda^{*}=\lambda$}
        $i = i+1$\\     
    }
    Return the image binarized with $\lambda^{*}$ and $Q$.
    \caption{ Search for the optimal threshold, $\lambda^{*}$. } \label{algo1}
\end{algorithm}
Given that the focus is on marker detection rather than image denoising, \textbf{Algorithm \ref{algo1}} is designed to utilize the detection result as feedback for the automatic search of $\lambda^{*}$.
%
In particular, the core of the algorithm is to maximize the length of a memory queue for saving detected markers (denoted by $Q$) by gradually increasing $\lambda$.
%
Namely, the aim is to detect as many markers as possible by finding the optimal threshold for each marker in the scene.
%
After applying \textbf{Algorithm \ref{algo1}}, the 2D fiducials located on the image binarized with $\lambda^{*}$ are projected into 3D fiducials using the subsequent steps of IFM. The proposed adaptive threshold LFM detection algorithm addresses the problem of a constant threshold being inapplicable to all point clouds and markers, especially when LiDAR moves in the wild and scenes undergo significant changes.
\subsection{Problem Formulation} \label{5.3}
In this section, the problem formulation is introduced. Suppose that the marker size is $a$. In the marker coordinate system $\{M\}_{j}$, the 3D coordinates of the four corners of the $j$-th marker, $^{j}\mathbf{p}^{j,1},^{j}\mathbf{p}^{j,2},^{j}\mathbf{p}^{j,3},$ and $^{j}\mathbf{p}^{j,4}$, are $[-a/2,-a/2,0]^{T}, [a/2,-a/2,0]^{T}, [a/2,a/2,0]^{T},$ and $[-a/2,a/2,0]^{T}$, respectively. LFM detection returns the 3D coordinates of the corners expressed in $\{F\}_{i}$ (the local coordinate system of the $i$-th scan $f_{i}$).
%
Thus, the 6-DOF transmission from $\{M\}_{j}$ to $\{F\}_{i}$, denoted as $\mathbf{T}^{j}_{i}$, can be found by solving the following least square problem:
 \begin{equation}	
	\mathbf{T}^{j,*}_{i}=\underset{\mathbf{T}^{j}_{i}}{\arg \min } \sum_{s=1}^{4}\left\| \mathbf{T}^{j}_{i} \ \cdot \ ^{j}\mathbf{p}^{j,s}-_{i}\mathbf{p}^{j,s}\right\|^{2}.\label{least3}
\end{equation}

The SVD method \cite{barfoot} is employed to compute $\mathbf{T}^{j,*}_{i}$. The set of measurements, including 
\begin{enumerate}[label=(\roman*)]
	 \item the corner positions w.r.t. $\{M\}_{j}$,
	\item the corner positions w.r.t. $\{F\}_{i}$,
	\item the marker poses w.r.t. $\{F\}_{i}$,
 
\end{enumerate}
are denoted as $\mathcal{Z}$. 
%
To register the multiview point clouds, it is necessary to find a globally consistent pose for each scan so that the point clouds can be transformed into a complete point cloud in the world coordinate system $\{G\}$. 
%
To this end, the following variables are considered: 
\begin{enumerate}[label=(\roman*)]
	 \item the poses of point clouds w.r.t. $\{G\}$,
	\item the poses of markers w.r.t. $\{G\}$,
	\item the marker corner positions w.r.t. $\{G\}$.
\end{enumerate}
\par
The set of variables is represented by $\Theta$. Finally, a MAP inference problem is formulated: given the measurements, $\mathcal{Z}$, the goal is to find the optimal variable set $\Theta^{*}$ that maximizes the posterior probability $P(\Theta \mid \mathcal{Z})$:
\begin{equation}
\
	\Theta^*=\underset{\Theta}{\arg \max } \ P(\Theta \mid \mathcal{Z}).\label{map}
\end{equation} 
\par
In the following, a framework consisting of two-level graphs is designed to solve this MAP problem. Inspired by the coarse-to-fine pipeline of SGHR \cite{sghr}, a first-level graph is developed to efficiently and exhaustively determine the relative poses among point clouds, while a second-level graph is used to globally optimize the variables.

\subsection{First-level Graph} \label{1level}
As aforementioned, the variables in $\Theta$ to be optimized cannot be directly obtained through the measurements in $\mathcal{Z}$. Moreover, deriving the initial values of the variables requires both efficiency and accuracy, given that the input is an unordered set of low overlap point clouds. 
%
The first-level graph is designed to address this challenge. First, the computation of the relative pose between two point clouds is considered. Suppose the $j$-th marker appears in the scenes of two scans $f_{i}$ and $f_{m}$, $\mathbf{T}^{j}_{i}$ and $\mathbf{T}^{j}_{m}$ can be calculated using Eq. (\ref{least3}). Consequently, the relative pose between $f_{i}$ and $f_{m}$, denoted as $\mathbf{T}_{i,m}$, is available from
 \begin{equation}	
	\mathbf{T}_{i,m}=(\mathbf{T}^{j}_{i})^{-1} \mathbf{T}^{j}_{m},\label{relative}
\end{equation}
where $(\mathbf{T}^{j}_{i})^{-1}$ indicates the inverse of $\mathbf{T}^{j}_{i}$. $(\mathbf{T}^{j}_{i})^{-1}$ indicates the pose that transforms 3D points from $\{F\}_{i}$ to $\{M\}_{j}$. Although the method introduced in Eq. (\ref{relative}) is straightforward, it cannot be directly applied because the input in this work is an unordered set of point clouds that do not follow a temporal or spatial sequence.
%
Specifically, for scans (point clouds) with no shared marker observations, their relative pose has to be calculated through pose propagation among other scans.
\par
However, multiple alternative paths could exist. Even for scans that share marker observations, there could be more than one overlapped marker. 
\par
Hence, to accurately and efficiently estimate relative poses among scans, it is necessary to design an algorithm to determine which scans and markers to apply Eq. (\ref{relative}). Specifically, the objective is to infer the relative poses with the highest possible accuracy using only the necessary low-dimensional information through a simple process. 
%
Thus, as shown in Fig. \ref{firstlevel}, the first-level graph is constructed in the form of a weighted graph. 
\begin{figure}[H] 
	\centering
\includegraphics[width=0.75\linewidth]{figs/chfour/first.png}
	\caption{An illustration of the first-level graph.
    %
    After applying the proposed adaptive marker detection to all scans, an exhaustive weighted graph is constructed, with the scans and markers as nodes and the point-to-point errors as edge weights.
    %
    The aim is to derive the relative pose between each non-anchor scan and the anchor scan with optimal accuracy.
    %
    However, for a given non-anchor scan, such as $f_{2}$ in this simple case, there may be multiple possible paths in the exhaustive graph leading to the anchor scan ($f_{1}$).
    %
    Thus, Dijkstra’s algorithm \cite{dij} is employed to find the optimal path with the minimum accumulation of point-to-point errors (weights).}
	\label{firstlevel}
\end{figure}

In particular, when processing the point clouds with the proposed adaptive threshold method one by one, if a marker is detected in a point cloud, the corresponding scan node and marker node are added to the graph along with a weighted edge. The edge weight is the pose estimation point-to-point error ${e}_{pp}$:
\begin{equation}	
	{e}_{pp}=\sum_{s=1}^{4}\left\| \mathbf{T}^{j,*}_{i} \ \cdot \ ^{j}\mathbf{p}^{j,s}-_{i}\mathbf{p}^{j,s}\right\|^{2}.\label{epp}
\end{equation}
\par
Eq. (\ref{epp}) indicates the substitution of the result given by SVD back into the right side of Eq. (\ref{least3}). ${e}_{pp} \in \mathbb{R}^{+}$ is employed as the metric to evaluate the quality of pose estimation of the marker in the corresponding point cloud. The first scan is defined as the anchor scan. The local coordinate system of the anchor scan is set as the world coordinate system. 
%
Namely, $\{F\}_{1}=\{G\}$. Only the relative poses between the anchor scan and each non-anchor scan are considered. Although there could be multiple paths from the given scan to the anchor scan (see Fig. \ref{first}), the information on pose estimation quality has been saved by constructing the first-level graph as a weighted graph. Therefore, Dijkstra’s algorithm \cite{dij} is employed to obtain the shortest path. Then, the relative pose is computed along the shortest path iteratively using Eq. (\ref{relative}). Given that the relative pose computation is achieved with the lowest accumulation of $e_{pp}$, the estimation result achieves the highest possible accuracy. Moreover, the search for the optimal path to propagate poses is based merely on the one-dimensional $e_{pp}$, without incorporating any 6-DOF poses or 3D locations.
\par
In this way, the point cloud poses w.r.t. $\{G\}$ are obtained. Since the marker detection provides the marker poses and the 3D positions of marker corners w.r.t. the local coordinate system of corresponding scan, the initial values of the marker poses w.r.t. $\{G\}$ and the corner positions w.r.t. $\{G\}$ can be derived through the point cloud poses w.r.t. $\{G\}$.

\subsection{Second-level Graph} \label{5.5}
% In this section, a factor graph is constructed to optimize the variables, ultimately addressing the MAP inference problem introduced in Eq. (\ref{map}). Suppose that the $j$-th marker is detected in the $i$-th scan, the following nodes are added to the second graph. The variable nodes include 
% \begin{enumerate}[label=(\arabic*)]
% 	 \item $\mathbf{T}^{j}$ (6-DOF pose of the $j$-th marker w.r.t. $\{G\}$),
% 	\item $\mathbf{T}_{i}$ (6-DOF pose of the $i$-th scan w.r.t. $\{G\}$),
% 	\item $\{ \mathbf{p}^{j,1}, \mathbf{p}^{j,2}, \mathbf{p}^{j,3}, \mathbf{p}^{j,4} \}$ (3D coordinates of the corners of the $j$-th marker w.r.t. $\{G\}$).
 
% \end{enumerate}
% The factor nodes include 
% \begin{enumerate}[label=(\arabic*)]
% 	 \item $\mathbf{T}^{j}_{i}$ (6-DOF pose of the $j$-th marker w.r.t. $\{F\}_{i}$),
% 	\item $\{ ^{j}\mathbf{p}^{j,1}, \ ^{j}\mathbf{p}^{j,2}, \ ^{j}\mathbf{p}^{j,3}, \ ^{j}\mathbf{p}^{j,4} \}$ (3D coordinates of the $j$-th marker's corners w.r.t. $\{M\}_{j}$),
% 	\item $\{ _{i}\mathbf{p}^{j,1}, \ _{i}\mathbf{p}^{j,2},$\ $ _{i}\mathbf{p}^{j,3},$ $_{i}\mathbf{p}^{j,4} \}$ (3D coordinates of the $j$-th marker's corners w.r.t. $\{F\}_{i}$).
% \end{enumerate}
% Note that by adding variables representing the 3D coordinates of the corners, the fiducial localization results can also be optimized at this stage. \par
% For clarity, the first stage in Fig. \ref{secondlevel} shows the added nodes and edges when a marker is detected in a scan. The graph becomes the one depicted in the second stage of Fig. \ref{secondlevel} after integrating all the marker detection results.
% \begin{figure}[H] 
% 	\centering
% \includegraphics[width=1.0\linewidth]{figs/chfour/second.png}
% 	\caption{The procedures for formulating the second graph. The variable nodes are represented by circles and the factor nodes are represented by squares.}
% 	\label{secondlevel}
% \end{figure}
% Subsequently, a prior factor is added to connect the first (anchor) point cloud pose node, as $\{F\}_{1}$ is set to $\{G\}$.
% %
% Finally, factor nodes representing the relative poses between the anchor scan and each non-anchor scan are added. Up to this point, the factor graph is completed, as shown in stage 3 of Fig. \ref{second}. Following \cite{isam2,gtsam}, this indicates that the factorization of the posterior probability $P(\Theta \mid \mathcal{Z})$ has been defined:
% \begin{equation}
% P(\Theta \mid \mathcal{Z})=\prod_k P^{(k)}(\Theta),
% \label{obj}
% \end{equation}
% where $P^{(k)}$ are the factors in the second graph and we follow the standard pipeline \cite{isam2} to model them as Gaussians:
% \begin{equation}
%  P^{(k)}(\Theta) \propto \exp \left(-\frac{1}{2}\left\|h_k\left(\Theta\right)\ominus z_k\right\|_{\Sigma_k}^2\right),
% \end{equation}
% where $h_k\left(\Theta \right)$ is the measurement function and $z_k$ is a measurement.
% %
% $\|e\|_{\Sigma}^2 \triangleq e^T \Sigma^{-1} e$ denotes the squared Mahalanobis distance with $\Sigma$ being the covariance matrix. 
% %
% Following \cite{tagslam}, if $z_k\in\mathbb{R}^{3\times1}$ is a 3D position, $\ominus$ refers to straight subtraction for elements. 
% %
% If $z_k\in SE(3)$ is a 6-DOF pose, $\ominus$ generates 6-dimensional Lie algebra coordinates:

% \begin{equation}{\mathbf{T}_{A} \ominus \mathbf{T}_{B} = [[\log(\mathrm{Rot}(\mathbf{T}_{B}^{-1}\mathbf{T}_{A}))]^{T}_{\vee}, \mathrm{Trans}(\mathbf{T}_{B}^{-1}\mathbf{T}_{A})^{T} ]^{T},
% } \end{equation} 
% where for a $\mathbf{T} \in SE(3)$, $\mathrm{Rot}(\mathbf{T})$ denotes the rotation matrix $\mathbf{R} \in SO(3)$ and $\mathrm{Trans}(\mathbf{T})$ denotes translation vector $\mathbf{t \in \mathbb{R}^{3\times1}}$. 
% %
% $\log(\cdot)$ represents the matrix logarithm. $\vee$ is the \textit{vee} map operator that finds the unique vector $\xi \in \mathbb{R}^{3\times1}$ (Lie algebra coordinates) corresponding to a given skew-symmetric matrix $\log(\mathbf{R})\in \mathbb{R}^{3 \times 3}$ \cite{barfoot,tagslam} (Refer to Section \ref{lie} if needed). With the Gaussian modeling, the objective function in Eq. (\ref{obj}) is transformed into a least square problem by applying the negative logarithm:
% \begin{equation}
% {\arg \min _{\Theta}(-\log \prod_k P^{(k)}(\Theta))= 
% \arg \min _{\Theta} \frac{1}{2} \sum_k\left\|h_k\left(\Theta\right)\ominus z_k\right\|_{\Sigma_k}^2.}
% \end{equation} \par
% The Levenberg-Marquardt (LM) algorithm \cite{gtsam} is utilized to solve this problem. The acquisition of initial values is introduced in Section \ref{1level}. The noise covariance matrices are determined by the quantitative experiments conducted in Section \ref{IFM}. 
To address the MAP problem introduced in Eq. (\ref{map}), we construct the second-level graph as a factor graph to globally optimize the variables using the initial values obtained from the first-level graph. Specifically, we create the second-level graph in three stages.\par
\noindent\textbf{Stage One.} Suppose that the $j$-th marker is detected in the $i$-th scan, the following six types of nodes are added to the second graph. The variable nodes include: 
\begin{itemize}
    \item [(1)] Node $\mathbf{T}^{j}$, which refers to the 6-DOF pose of the $j$-th marker \textit{w.r.t.} $\{G\}$;
    \item [(2)] Node $\mathbf{T}_{i}$, which refers to the 6-DOF pose of the $i$-th scan \textit{w.r.t.} $\{G\}$;
    \item [(3)] Nodes $\{ \mathbf{p}^{j,1}, \mathbf{p}^{j,2}, \mathbf{p}^{j,3}, \mathbf{p}^{j,4} \}$, which refer to 3D coordinates of the corners of the $j$-th marker \textit{w.r.t.} $\{G\}$. 
\end{itemize}
\noindent The factor nodes include: 
\begin{itemize}
    \item [(4)] Node $\mathbf{T}^{j}_{i}$, which refers to the measurement of the 6-DOF pose of the $j$-th marker \textit{w.r.t.} $\{F\}_{i}$;
    \item [(5)] Nodes $\{ ^{j}\mathbf{p}^{j,1}, \ ^{j}\mathbf{p}^{j,2}, \ ^{j}\mathbf{p}^{j,3}, \ ^{j}\mathbf{p}^{j,4} \}$, which refer to the measurement of the 3D coordinates of the corners of the $j$-th marker \textit{w.r.t.} $\{M\}_{j}$;

    \item [(6)] Nodes $\{ _{i}\mathbf{p}^{j,1}, \ _{i}\mathbf{p}^{j,2},$\ $ _{i}\mathbf{p}^{j,3},$ $_{i}\mathbf{p}^{j,4} \}$, which refer to 3D coordinates of the corners of the $j$-th marker \textit{w.r.t.} $\{F\}_{i}$.
\end{itemize}
By adding variables representing the 3D coordinates of the corners, we can also optimize the fiducial localization results. The first stage in Fig. \ref{secondgraph} shows the added nodes and edges when a marker is detected in a scan. \par
\noindent\textbf{Stage Two.} Thereafter, we traverse all the marker detection results and conduct the operation from Stage One for each detected marker. 
After processing all the detected markers, the second-level graph becomes the one shown in Stage Two of Fig. \ref{secondgraph}. Since the operation in this stage essentially consists of repetitions of Stage One, the node types are no different from those in the previous stage. \par
\noindent\textbf{Stage Three.} Considering that the local coordinate system of the first scan, $\{F\}_{1}$, is set as the global coordinate system, $\{G\}$, we add a prior factor that connects to the first (anchor) scan node,  $\mathbf{T}^{1}$. Finally, we add factor nodes representing the relative poses between the anchor scan and each non-anchor scan. Up to this point, the factor graph is completed, as shown in Stage Three of Fig. \ref{secondgraph}. \par
\begin{figure}[H]
	\centering
	\includegraphics[width=5.5in]{figs/chfour/second.png}

	\caption{The procedures for formulating the second-level graph. The variable nodes are represented by circles and the factor nodes are represented by squares. In Stage One, when a marker is detected in a scan, six types of nodes are added to the graph, including (1) scan pose in $\{G\}$, (2) marker pose in   $\{G\}$, (3) corner positions in $\{G\}$, (4) marker pose from $\{M\}$ to $\{G\}$, (5) corner positions in $\{F\}$, and (6) corner positions in $\{M\}$, along with their corresponding edges. In Stage Two, all the marker detection results are traversed, and the operation from Stage One is repeated for each detected marker.  In Stage Three, a prior factor connecting the anchor scan is added, along with factors representing the relative poses between the anchor scan and non-anchor scans.} \label{secondgraph}

\end{figure} 
Following \cite{isam2,gtsam}, since the factor graph is determined, the posterior probability $P(\Theta \mid \mathcal{Z})$ is factorized as:
\begin{equation}
P(\Theta \mid \mathcal{Z})=\prod_k P^{(k)}(\Theta),
\label{obj}
\end{equation}
where $P^{(k)}$ are the factors in the second graph. We follow the standard pipeline \cite{isam2} to model them as Gaussians:
\begin{equation}
 P^{(k)}(\Theta) \propto \exp \left(-\frac{1}{2}\left\|h_k\left(\Theta\right)\ominus z_k\right\|_{\Sigma_k}^2\right),
\end{equation}
where $h_k\left(\Theta \right)$ is the measurement function and $z_k$ is a measurement.
%
$\|e\|_{\Sigma}^2 \triangleq e^T \Sigma^{-1} e$ denotes the squared Mahalanobis distance with $\Sigma$ being the covariance matrix. 
%
Following \cite{tagslam}, if $z_k\in\mathbb{R}^{3\times1}$ is a 3D position, $\ominus$ refers to straight subtraction for elements. 
%
If $z_k\in SE(3)$ is a 6-DOF pose, $\ominus$ generates 6-dimensional Lie algebra (refer to Section \ref{lie}) coordinates:
\begin{equation}{\resizebox{0.6\hsize}{!}{$\mathbf{T}_{A} \ominus \mathbf{T}_{B} = [[\log(\mathrm{Rot}(\mathbf{T}_{B}^{-1}\mathbf{T}_{A}))]^{T}_{\vee}, \mathrm{Trans}(\mathbf{T}_{B}^{-1}\mathbf{T}_{A})^{T} ]^{T},
$} } \end{equation} 
where for a $\mathbf{T} \in SE(3)$, $\mathrm{Rot}(\mathbf{T})$ denotes the rotation matrix $\mathbf{R} \in SO(3)$ and $\mathrm{Trans}(\mathbf{T})$ denotes translation vector $\mathbf{t \in \mathbb{R}^{3\times1}}$. 
%
$\log(\cdot)$ represents the matrix logarithm. $\vee$ is the \textit{vee} map operator. The detailed introduction of $\vee$ is provided in Section \ref{lie}.  With the Gaussian modeling, the objective function in Eq. (\ref{obj}) is transformed into a least square problem by applying the negative logarithm:
\begin{equation}
\resizebox{0.7\hsize}{!}{$\arg \min _{\Theta}(-\log \prod_k P^{(k)}(\Theta))= 
\arg \min _{\Theta} \frac{1}{2} \sum_k\left\|h_k\left(\Theta\right)\ominus z_k\right\|_{\Sigma_k}^2.$}
\end{equation} \par

We utilize the Levenberg-Marquardt algorithm \cite{gtsam} to solve this problem. The acquisition of initial values is introduced in Section \ref{1level}. The noise covariance matrices are determined by the quantitative experiments conducted in \cite{iilfm}.
\section{Livox-3DMatch Dataset} \label{dataset}
A common approach \cite{sghr,pre} to evaluating a learning-based point cloud registration model is to train it on 3DMatch \cite{3dmatch} and test it on various benchmarks, including 3DMatch \cite{3dmatch}, ETH \cite{eth}, and ScanNet \cite{scan}. However, the 3DMatch benchmark is mainly constructed from RGB-D camera captures of indoor scenes \cite{3dmatch}. In this dissertation, a new dataset, named Livox-3DMatch, is collected to enrich the training data for learning-based methods. The enrichment contains two key components. 
\par
First, the sensor we adopt is a Livox MID-40 LiDAR, which has different sampling patterns compared to the RGB-D camera (See Fig. \ref{livox3d}(a) and (b)). Thus, it adds point clouds with new features to the training data. 
\par
Second, scenes that are absent or rare in 3DMatch are selectively sampled, thereby enriching the scenes in the training data. For example, the valid depth range of an RGB-D camera is usually less than ten meters, making it unsuitable for sampling outdoor scenes. In contrast, some outdoor scenes (See Fig. \ref{livox3d}(c)) are collected for Livox-3DMatch, considering that a LiDAR can sample objects a few hundred meters away. Moreover, some challenging cases (See Fig. \ref{livox3d}(d)) are gathered where the overlapping regions are mainly planes, which are rare in 3DMatch \cite{3dmatch}. A more detailed introduction to the scenes in Livox-3DMatch is given in Sections \ref{test1} and \ref{test2}. In Section \ref{testadd}, we demonstrate that the proposed Livox-3DMatch can boost the performance of the state-of-the-art learning-based methods \cite{mdgd,sghr} on various benchmarks \cite{3dmatch,eth,scan}.
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chfour/livox3dmatch.png}
	\caption{Comparison of 3DMatch and Livox-3DMatch. (a): A random sample from 3DMatch. The point cloud sampled by an RGB-D camera has a regular pattern and less noise. (b): A random sample from Livox-3DMatch. The Livox LiDAR point cloud has an irregular pattern and more noise. (c): An example of an outdoor scene in Livox-3DMatch. (d): A selectively sampled challenging case in which the overlapping regions are mainly planes.}
	\label{livox3d}
\end{figure}

\section{Experimental Validation} \label{5.7}

\subsection{Experimental Setup}
In this section, both the solid-state LiDAR (Livox Mid-40) and mechanical LiDAR (RS-Ruby-128) are employed to validate the proposed framework. In particular, the point clouds are sampled from various indoor and outdoor scenes, including offices, a meeting room, lounges, a kitchen, office buildings, and thickets. In indoor scenes, letter-sized AprilTags are utilized as LFMs. In outdoor scenes, poster-sized ArUcos are employed to provide LiDAR fiducials. For experiments concerning registration accuracy, the ground truth poses are obtained by manually registering through CloudCompare \cite{cloudcompare}, a popular 3D annotation tool. The ground truth for evaluating the quality of 3D asset collection is a high-fidelity 3D model acquired from a 3D assets website. For experiments involving localization accuracy, the ground truth trajectory is provided by the MoCap system for the indoor scene and by Real-Time Kinematic (RTK) for the outdoor scene.

\subsection{Evaluation of the Adaptive Marker Detection Method} \label{test0}
\noindent\textbf{Data.}
The Livox MID-40 LiDAR and 69.2 cm $\times$ 69.2 cm ArUco marker(s) are placed in three different scenes, as shown in Fig. \ref{threshold2}. The three scenes are: between groups of buildings, in open outdoor areas, and in large indoor parking lots. In each scene, we collect point clouds and test the proposed adaptive threshold method at different relative positions.
\begin{figure}[ht] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chfour/threshold2.png}
	\caption{Setup for testing the adaptive threshold marker detection algorithm: (a) between groups of buildings, (b) in an open outdoor area, and (c) in a large indoor parking lot.}
	\label{threshold2}
\end{figure} \par
\noindent\textbf{Results and Analysis.}
Table \ref{tabthreshold} presents the results, where x and y indicate the relative position of the LiDAR in the marker (ID 4) coordinate system. Specifically, in scene Fig. \ref{threshold2}(c), there are two markers (ID 4 and ID 1), so the column for $\lambda^{*}$ contains two values. As seen in the table, $\lambda^{*}$ varies significantly as the relative position changes, demonstrating the necessity of the proposed adaptive marker detection method. In addition, the results shown in Fig. \ref{threshold2}(c) also illustrate the necessity of the memory queue design, considering that a single $\lambda^{*}$ might not be applicable to all markers in the same scene. Please note that the intention of reporting the values of the optimal threshold in Table \ref{tabthreshold} is to demonstrate that a constant threshold, as adopted in \cite{iilfm}, is not applicable to this task. However, the major concern of the adaptive threshold marker detection algorithm is to detect all markers in a scene. Namely, the marker detection results saved in the memory queue are the most important output rather than the thresholds.

\begin{table}[H]
\caption{Demonstration of the necessity of the proposed adaptive marker detection algorithm}
\begin{center}

\resizebox{1.0\columnwidth}{!}{

\begin{tabular}{c|c|c|c | c|c|c|c| c|c|c|c}
\hline\hline
Scene & x (m) & y (m) & $\lambda^{*}$ &Scene & x (m) & y (m) & $\lambda^{*}$ &Scene & x (m) & y (m) & $\lambda^{*}$ \\ \hline
 \multirow{21}{*}{\shortstack{Fig. \ref{threshold2}(a)\\Between\\groups of\\buildings}} &0 &4   & 36  & \multirow{21}{*}{\shortstack{Fig. \ref{threshold2}(b)\\In an open\\outdoor\\area}} &0 &4   & 7 & \multirow{21}{*}{\shortstack{Fig. \ref{threshold2}(c)\\In a large\\indoor\\parking lot}} &4 &4   & 20/6   \\  \cline{2-4} \cline{6-8} \cline{10-12} 
 &4 &4  & 9 &  &4 &4  & 8 &  &4 &4  & 8/8 \\  \cline{2-4} \cline{6-8} \cline{10-12} 
&-4 &4   & 6 & &-4 &4  & 6 &  &-4 &4  & 6/8   \\  \cline{2-4} \cline{6-8} \cline{10-12} 
 &0 &5  & 20 &  &0 &5  & 9 &  &0 &5  & 15/10 \\  \cline{2-4} \cline{6-8} \cline{10-12} 
  &5 &5 & 8 & &5 &5  & 8&  &5 &5  & 8/12\\  \cline{2-4} \cline{6-8} \cline{10-12} 
   &-5 &5  & 7&  &-5 &5  & 9 &  &-5 &5  & 7/13 \\  \cline{2-4} \cline{6-8} \cline{10-12} 
    &0 &6  & 19 &  &0 &6  & 9 &  &0 &6  & 8/13\\  \cline{2-4} \cline{6-8} \cline{10-12} 
  &6 &6 & 10& &6 &6  & 6 &  &6 &6  & 11/11\\  \cline{2-4} \cline{6-8} \cline{10-12} 
   &-6 &6  & 11 & &-6 &6  & 8 &  &-6 &6  & 11/10 \\  \cline{2-4} \cline{6-8} \cline{10-12} 
     &0 &7  & 17 &&0 &7  & 9 &  &0 &7  & 19/20 \\  \cline{2-4} \cline{6-8} \cline{10-12} 
  &7 &7 & 8 &&7 &7  & 8 &  &7 &7  & 15/14\\  \cline{2-4} \cline{6-8} \cline{10-12} 
   &-7 &7  & 9  &&-7 &7  & 9&  &-7 &7  & 28/16  \\  \cline{2-4} \cline{6-8} \cline{10-12} 
   &0 &8  & 15  &&0 &8  & 13&  &0 &8  & 10/16 \\  \cline{2-4} \cline{6-8} \cline{10-12} 
  &8 &8 & 9 &&8 &8  & 11 &  &8 &8  & 11/17\\  \cline{2-4} \cline{6-8} \cline{10-12} 
   &-8 &8  & 10  &&-8 &8  & 15&  &-8 &8  & 12/15 \\  \cline{2-4} \cline{6-8} \cline{10-12} 
    &0 &9  & 17  &&0 &9  & 10 &  &0 &9  & 13/11\\  \cline{2-4} \cline{6-8} \cline{10-12} 
  &9 &9 & 8 &&9 &9  & 12 &  &9 &9  & 20/26\\  \cline{2-4} \cline{6-8} \cline{10-12} 
   &-9 &9  & 9  &&-9 &9  & 12  &  &-9 &9  & 16/12\\ \cline{2-4} \cline{6-8} \cline{10-12} 
   &0 &10  & 14  &&0 &10  & 13&  &0 &10  & 24/28\\  \cline{2-4} \cline{6-8} \cline{10-12} 
  &10 &10 & 11 &&10 &10  &10 &  &10 &10  & 10/19\\  \cline{2-4} \cline{6-8} \cline{10-12} 
   &-10 &10  & 12   &&-10 &10  & 15 &  &-10 &10  & 15/19 \\ \cline{2-4} \cline{6-8} \cline{10-12} 
 \hline\hline
\end{tabular}

}
\label{tabthreshold}
\end{center}
\end{table}

\subsection{Evaluation of Point Cloud Registration Accuracy} \label{test1}
\noindent\textbf{Data.} Given that the existing point cloud registration benchmarks \cite{3dmatch,eth,scan} lack fiducial markers in the scenes, a new dataset is constructed with the Livox MID-40, as shown in Fig. \ref{newbench}. As illustrated in the caption of Fig. \ref{newbench}, the newly collected dataset covers various indoor and outdoor scenes. Indoors, multiple 16.4 cm × 16.4 cm AprilTags \cite{ap3} are positioned in the environment. Outdoors, multiple 69.2 cm × 69.2 cm ArUcos \cite{aruco} are placed in the environment. Note that since the LFMs in this work are thin sheets of objects attached to other surfaces, they are almost invisible in the point clouds. This is infeasible if LiDARTags \cite{lt} or calibration boards \cite{cal,cal2,a4} are adopted to provide fiducials. These scenes are challenging due to low overlap, and the overlap rate \cite{pre} of each scene is also presented in Table \ref{tabnew}. The ground truth poses between scans are obtained manually using CloudCompare \cite{cloudcompare}. Note that manually aligning point clouds is a labor-intensive and time-consuming process, highlighting the benefits of developing an automatic tool like the proposed framework. The inference time of the methods is compared using an AMD Ryzen 7 5800X CPU.\\
\noindent\textbf{Competitors and Metrics.}
Competitors include the latest state-of-the-art (SOTA) multiview point cloud registration methods, MDGD \cite{mdgd} (RA-L$^{\prime}$24), SGHR \cite{sghr} (CVPR$^{\prime}$23), and the SOTA pairwise methods, SE3ET \cite{se3et} (RA-L$^{\prime}$24), GeoTrans \cite{geotransformer} (TPAMI$^{\prime}$23), and Teaser++ \cite{teaser} (T-RO$^{\prime}$20). All the competitors are learning-based methods, except for Teaser++, which is a geometry-based method. For all pairwise methods, we manually select pairs of point clouds that have overlap as the inputs.
%
The root-mean-square errors (RMSEs) \cite{shuo} are employed as the metric: 

\begin{equation}
\begin{aligned}
&\operatorname{RMSE}_{T}=\sqrt{\sum_{n=0}^{N_s} e_{T, n}^2 /(N_s+1)}, 
\\
& \operatorname{RMSE}_{R}=\sqrt{\sum_{n=0}^{N_s} e_{R, n}^2 /(N_s+1)},
\end{aligned} \label{rmse}
\end{equation}
where $e_{T, n}^2$ and $e_{R, n}^2$ represent the squared Euclidean distances between the ground truth and the estimates of translation and rotation, respectively. $N_s$ denotes the number of samples. We also compare the inference time of the methods with an AMD Ryzen 7 5800X CPU.
\par
\noindent\textbf{Results and Analysis.}
The qualitative and quantitative results are presented in Fig. \ref{newbench} and Table \ref{tabnew}, respectively. 
The pairwise methods \cite{se3et,geotransformer,teaser}, although provided manually selected pairs of point clouds with overlap, struggle in all scenes.
%
This is because they are tailored for extracting geometric features either in a learning-based manner \cite{se3et,geotransformer} or through conventional geometry \cite{teaser}. Thus, when the overlap ratio between point clouds is low and the overlapped regions lack sufficient geometric features, these methods struggle to find features and utilize them to align point clouds. 
%
In contrast, the multiview point cloud registration methods \cite{mdgd,sghr} show some successful cases. In particular, SGHR \cite{sghr} successfully registers the point clouds in scenes 1 to 3 and one point cloud pair in scene 4. MDGD \cite{mdgd}, built upon the framework of SGHR \cite{sghr} but developing a new matching distance-based overlap estimation module, aligns scenes 1 and 2 with decent accuracy and also successfully registers one pair of point clouds in scenes 4, 5, 7, and 10. The reason is that, in addition to learning pairwise registration, the multiview point cloud registration methods \cite{mdgd,sghr} also learn to further optimize or refine the pose graph constructed using pairwise registration results.
%
Despite this, they fail in other scenes. Reviewing their success cases, it is found that these indoor scenes are similar to those in their training dataset \cite{3dmatch}. While failure cases, such as scenes 4 to 10, are rare or lacking in the training dataset. This indicates that their generalization to unseen scenarios is limited due to the out-of-distribution problem. The proposed method achieves the best performance. Specifically, our method does not require the overlapping regions to have explicit geometric features, thanks to the thin-sheet format of our LiDAR fiducial markers. Moreover, by virtue of the proposed adaptive marker detection algorithm, our method can robustly align any novel indoor or outdoor scenes with thin-sheet markers.
The proposed method also yields the best efficiency as it focuses on registering point clouds through LiDAR fiducial markers rather than analyzing the entire point clouds. The process of our method takes several dozen seconds, while other methods need several minutes on the AMD Ryzen 7 5800X CPU.
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chfour/newdata3.png}
	\caption{A comparison with SOTA methods, including MDGD \cite{mdgd}, SGHR \cite{sghr}, SE3ET \cite{se3et}, GeoTrans \cite{geotransformer}, and Teaser++ \cite{teaser}, on ten scenes. The scenes include the office (1-3), the meeting room (4), the lounge (5,6), the kitchen (7), the office building (8,9), and the thicket (10). Each scene consists of three scans, except scenes 2 and 9, which are composed of two scans.}
	\label{newbench}
\end{figure}
% \begin{table}[H]
% % \begin{table}[htbp]
% \caption{Quantitative comparison with SGHR \cite{sghr} and Teaser++ \cite{teaser} on our dataset}
% \centering
% 	\resizebox{1.0\columnwidth}{!}{
% \begin{tabular}{c|c|c|c|c|c|c}
% \hline\hline

% \multicolumn{2}{c|}{Scene $\#$ } & 1 (Office 1)  &2 (Office 2)& 3 (Office 3)  & 4 (Meeting Room)  &5 (Lounge 1) \\ 
% \multicolumn{2}{c|}{Avg/Min Overlap Rate (\%)} & 27.17/14.29  &46.88/46.88 &44.66/22.26 & 25.80/1.40 &43.82/15.48 \\
% \hline
% \multirow{3}{*}{Teaser++\cite{teaser}} &$\mathrm{RMSE}_{R}$ (rad) & 1.504  & 1.795 & 2.394 & 2.277 & 2.493 \\
% &$\mathrm{RMSE}_{T}$ (m) & 1.724 & 1.890 & 2.075 & 2.252& 1.911 \\
% & Time (s) & 313.1 & 298.3 & 336.4 & 322.3& 388.9 \\
% \hline
% \multirow{3}{*}{SGHR\cite{sghr}} & $\mathrm{RMSE}_{R}$ (rad)  & 0.152  & \textbf{0.060}  & 1.198 & 1.825 & 1.311  \\
% & $\mathrm{RMSE}_{T}$ (m) & 0.020 & \textbf{0.010} & 0.094 & 0.156& 0.231  \\
% & Time (s) & 846.1 & 785.0 & 886.5 & 825.3& 851.7 \\
% \hline
% \multirow{3}{*}{Ours} & $\mathrm{RMSE}_{R}$ (rad) & \textbf{0.036} & 0.068 & \textbf{0.089} & \textbf{0.065}& \textbf{0.088} \\
% & $\mathrm{RMSE}_{T}$ (m) &   \textbf{0.017} &  0.011 &  \textbf{0.028} &  \textbf{0.031}& \textbf{ 0.032} \\  
% & Time (s) & \textbf{31.1}   & \textbf{21.2}  & \textbf{35.7}   & \textbf{32.6 }& \textbf{36.2}  \\
% \hline
% \multicolumn{2}{c|}{Scene $\#$ } & 6 (Lounge 2)  &7 (Kitchen)& 8 (Office Building 1) & 9 (Office Building 2) &10 (Thickets)\\
% \multicolumn{2}{c|}{Avg/Min Overlap Rate (\%)} & 50.66/27.24&  31.31/4.19 & 19.06/12.20 &  44.65/44.65 & 12.16/5.42 \\ 
% \hline
% \multirow{3}{*}{Teaser++\cite{teaser}} &$\mathrm{RMSE}_{R}$ (rad) &  1.946& 1.632&  1.818 & 1.779 & 1.876 \\
% &$\mathrm{RMSE}_{T}$ (m) &  1.733& 2.089&  1.861 & 1.848 & 1.891 \\
% & Time (s) &  419.8& 441.5&  397.1 & 327.2 & 401.2 \\
% \hline
% \multirow{3}{*}{SGHR\cite{sghr}} & $\mathrm{RMSE}_{R}$ (rad) &  1.696& 4.42 &  2.792 & 3.034 &2.810 \\
% & $\mathrm{RMSE}_{T}$ (m) &  0.274&  0.949&  2.171 & 0.686 & 1.787 \\
% & Time (s) &  979.3& 983.6&  909.9 & 794.6 &950.4 \\
% \hline
% \multirow{3}{*}{Ours} & $\mathrm{RMSE}_{R}$ (rad) & \textbf{0.078}&\textbf{0.067}&  \textbf{0.069} & \textbf{0.087} & \textbf{0.101} \\
% & $\mathrm{RMSE}_{T}$ (m) & \textbf{0.043}&  \textbf{0.019}&  \textbf{0.082} &  \textbf{0.069} & \textbf{0.077} \\
% & Time (s) &  \textbf{43.5}& \textbf{53.8} &  \textbf{39.9} & \textbf{24.9}  &\textbf{42.2} \\
% \hline \hline

% \end{tabular}
% }
% \label{tabnew}

% % \end{center}
% \end{table}
\begin{table*}[ht]
% \begin{table}[htbp]
\caption{Quantitative comparison with MDGD \cite{mdgd}, SGHR \cite{sghr}, SE3ET \cite{se3et}, GeoTrans \cite{geotransformer}, and Teaser++ \cite{teaser} on our dataset.}
\centering
	\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c}
\hline\hline

\multicolumn{2}{c|}{Scene $\#$ } & 1 (Office)  &2 (Office) & 3 (Office)  & 4 (Meeting Room) &5 (Lounge)   \\ 

\multicolumn{2}{c|}{Avg/Min Overlap Rate (\%)} & 27.17/14.29  &46.88/46.88 &44.66/22.26 & 25.80/1.40 &43.82/15.48  \\ 
\hline
\multirow{3}{*}{Teaser++ \cite{teaser}} &$\mathrm{RMSE}_{R}$ (rad) & 1.504  & 1.795 & 2.394 & 2.277 & 2.493 \\
&$\mathrm{RMSE}_{T}$ (m) & 1.724 & 1.890 & 2.075 & 2.252& 1.911 \\
(T-RO$^{\prime}$20)& Time (s) & 313.1 & 298.3 & 336.4 & 322.3& 388.9  \\
\hline
%
%
\multirow{3}{*}{GeoTrans \cite{geotransformer}} &$\mathrm{RMSE}_{R}$ (rad) & 1.331  & 1.451 & 1.935 & 1.632 & 1.532 \\
&$\mathrm{RMSE}_{T}$ (m) & 1.502 & 1.671 & 1.912 & 1.552& 1.325 \\
(TPAMI$^{\prime}$23)& Time (s) & 91.8 & 87.3 & 95.1 & 93.9& 105.3  \\
\hline
%
%
\multirow{3}{*}{SE3ET \cite{se3et}} &$\mathrm{RMSE}_{R}$ (rad) & 0.925  & 0.834 & 1.776 & 1.358 & 1.381 \\
&$\mathrm{RMSE}_{T}$ (m) & 0.996 & 1.113 & 1.736 & 1.273& 1.207 \\
(RA-L$^{\prime}$24)& Time (s) & 116.8 & 94.6 & 99.5 & 95.7& 115.7 \\
\hline
%
%
\multirow{3}{*}{SGHR \cite{sghr}} & $\mathrm{RMSE}_{R}$ (rad)  & 0.152  & \textbf{0.060}  & 1.198 & 1.825 & 1.311   \\
& $\mathrm{RMSE}_{T}$ (m) & 0.020 & 0.010 & 0.094 & 0.156& 0.231 \\
(CVPR$^{\prime}$23)& Time (s) & 846.1 & 785.0 & 886.5 & 825.3& 851.7 \\
\hline
%
%
\multirow{3}{*}{MDGD \cite{mdgd}} & $\mathrm{RMSE}_{R}$ (rad)  & \textbf{0.032}  & 0.062  & 1.457 & 0.933 & 0.679   \\
& $\mathrm{RMSE}_{T}$ (m) & \textbf{0.015} & \textbf{0.009} & 0.352 & 1.035& 0.113  \\
(RA-L$^{\prime}$24)& Time (s) & 626.1 & 573.3 & 645.5 & 619.2& 612.0 \\
\hline
%
%
\multirow{3}{*}{\textbf{Ours}} & $\mathrm{RMSE}_{R}$ (rad) & 0.036 & 0.068 & \textbf{0.089} & \textbf{0.065}& \textbf{0.088} \\
& $\mathrm{RMSE}_{T}$ (m) &   0.017 &  0.011 &  \textbf{0.028} &  \textbf{0.031}& \textbf{ 0.032}  \\
        
& Time (s) & \textbf{31.1}   & \textbf{21.2}  & \textbf{35.7}   & \textbf{32.6 }& \textbf{36.2}  \\
\hline
\multicolumn{2}{c|}{Scene $\#$ } & 6 (Lounge)&  7 (Kitchen)& 8 (Building)&  9 (Building) & 10 (Thicket) \\ 

\multicolumn{2}{c|}{Avg/Min Overlap Rate (\%)} & 50.66/27.24&  31.31/4.19 & 19.06/12.20 &  44.65/44.65 & 12.16/5.42 \\ 
\hline
\multirow{3}{*}{Teaser++ \cite{teaser}} &$\mathrm{RMSE}_{R}$ (rad) &  1.946& 1.632&  1.818 & 1.779 & 1.876 \\
&$\mathrm{RMSE}_{T}$ (m) & 1.733& 2.089&  1.861 & 1.848 & 1.891 \\
(T-RO$^{\prime}$20)& Time (s) & 419.8& 441.5&  397.1 & 327.2 & 401.2 \\
\hline
%
%
\multirow{3}{*}{GeoTrans \cite{geotransformer}} &$\mathrm{RMSE}_{R}$ (rad) & 1.471& 1.198&  1.355 & 1.377 & 1.526 \\
&$\mathrm{RMSE}_{T}$ (m) &1.235& 1.132&  1.730 & 1.554 & 1.531 \\
(TPAMI$^{\prime}$23)& Time (s) & 122.3& 127.3&  110.5 & 91.1 & 121.4 \\
\hline
%
%
\multirow{3}{*}{SE3ET \cite{se3et}} &$\mathrm{RMSE}_{R}$ (rad) & 1.352& 1.077&  1.156 & 1.232 & 1.376 \\
&$\mathrm{RMSE}_{T}$ (m) & 1.130& 1.095&  1.469 & 1.413 & 1.392 \\
(RA-L$^{\prime}$24)& Time (s) & 136.0& 139.3&  123.5 & 97.3 & 132.0 \\
\hline
%
%
\multirow{3}{*}{SGHR \cite{sghr}} & $\mathrm{RMSE}_{R}$ (rad)  &  1.696& 3.42 &  2.792 & 3.034 &2.810 \\
& $\mathrm{RMSE}_{T}$ (m) & 0.274&  0.949&  2.171 & 0.686 & 1.787 \\
(CVPR$^{\prime}$23)& Time (s) & 979.3& 983.6&  909.9 & 794.6 &950.4 \\
\hline
%
%
\multirow{3}{*}{MDGD \cite{mdgd}} & $\mathrm{RMSE}_{R}$ (rad)  & 1.715& 0.856 &  0.722 & 0.651 &0.779 \\
& $\mathrm{RMSE}_{T}$ (m) & 0.422&  0.337&  1.553 & 0.686 & 0.939 \\
(RA-L$^{\prime}$24)& Time (s) & 726.4 & 730.5&  689.6 & 590.6 &717.4 \\
\hline
%
%
\multirow{3}{*}{\textbf{Ours}} & $\mathrm{RMSE}_{R}$ (rad) & \textbf{0.078}& \textbf{0.067}&  \textbf{0.069} & \textbf{0.087} & \textbf{0.101} \\
& $\mathrm{RMSE}_{T}$ (m) & \textbf{0.043}&  \textbf{0.019}&  \textbf{0.082} &  \textbf{0.069} & \textbf{0.077} \\
        
& Time (s) & \textbf{43.5}& \textbf{53.8} &  \textbf{39.9} & \textbf{24.9}  &\textbf{42.2} \\

\hline\hline
\end{tabular}
}
\label{tabnew}

% \end{center}
\end{table*} 
\par
\clearpage
\subsection{Application 1: 3D Asset Collection from Sparse Scans} \label{test2}
Collecting the complete shape of a novel object \cite{mending} from sparse observations is advantageous, as sparse scans require less labor and offer better efficiency. However, it is also a challenging task due to the low overlap between scans. In this test, we evaluate the instance reconstruction quality of the proposed method. \par
\noindent\textbf{Data.}
The experimental setup is depicted in Fig. \ref{glbtraj}. Four 69.2 cm $\times$ 69.2 cm ArUco \cite{aruco} markers are placed in the environment. 
% The Livox MID-40 LiDAR is employed to scan the vehicle from five significantly different viewpoints. As shown in Fig. \ref{glbtraj}, 
%
As depicted by the yellow trajectory, the Livox MID-40 LiDAR follows a looping path to scan the vehicle, pausing at five viewpoints for a few seconds to collect relatively dense point clouds due to its sparse scanning pattern \cite{lloam}. 
%
The rostopic of the point cloud published by the LiDAR sensor is recorded as a rosbag throughout the sampling process. Then, the same rosbag is provided to all competitors. 
%
However, point cloud registration methods \cite{mdgd,sghr,se3et,geotransformer,teaser}, including ours, utilize only a portion of the rosbag, \textit{i.e.}, five point clouds with significant viewpoint changes, as shown in Fig. \ref{glbtraj}. This is also a challenging low-overlap case, with the average and minimum overlap rates being 13.39\% and 1.02\%, respectively. To evaluate the instance reconstruction quality, \textit{Supervisely} \cite{super}, a popular 3D annotation tool, is used to extract the vehicle's point cloud from the reconstruction result.
%
Since the manufacture and model (Mercedes-Benz GLB 250) of the vehicle to be reconstructed are known, a high-fidelity 3D model acquired from a 3D assets website acts as the ground truth shape.
\\
\noindent\textbf{Competitors and Metrics.}
Competitors include the state-of-the-art point cloud registration methods (MDGD \cite{mdgd}, SGHR \cite{sghr}, SE3ET \cite{se3et}, GeoTrans \cite{geotransformer}, and Teaser++ \cite{teaser}) and LOAM methods (Traj LO \cite{traj}, Livox Mapping \cite{sdk}, and LOAM Livox \cite{lloam}). Pairs of point clouds with overlapping regions are manually provided to the pairwise methods \cite{se3et,geotransformer,teaser}. MDGD \cite{mdgd}, SGHR \cite{sghr}, and our method directly process the set of point clouds. The LOAM methods take the entire rosbag as input. Following \cite{cd}, the Chamfer Distance (CD) and Recall are computed between the ground truth shape and the reconstruction result. In particular, CD is defined as 
\begin{equation}
CD\left(\boldsymbol{X}, \boldsymbol{Y}\right)=\sum_{x \in \boldsymbol{X}} \min _{y \in \boldsymbol{Y}}\|x-y\|_2^2+\sum_{y \in \boldsymbol{Y}} \min _{x \in \boldsymbol{X}}\|x-y\|_2^2. \label{cd}
\end{equation} 
where $\boldsymbol{X}$ and $\boldsymbol{Y}$ are two point sets with $x$ and $y$ being the 3D points belonging to them, respectively. Recall is defined as follows:
\begin{equation}
\operatorname{Recall}(\boldsymbol{X}, \boldsymbol{Y})=\frac{1}{|\boldsymbol{X}|} \sum_{x \in \boldsymbol{X}}\left[\min _{y \in \boldsymbol{Y}}\|x-y\|_2^2<=thr\right],
\end{equation}
where $thr$ is a predefined threshold \cite{cd}.
\\
\noindent\textbf{Results and Analysis.}
The qualitative results for point cloud registration methods and LOAM methods are shown in Fig. \ref{glbtraj} and Fig. \ref{glb}, respectively. The quantitative results are presented in Table \ref{tabglb}. As shown in Fig. \ref{glbtraj}, MDGD \cite{mdgd} and SGHR \cite{sghr} successfully align the third and fifth scans. In particular, the third and fifth scans have the highest overlap ratio of 59.33\% among all pairs as they are captured from similar perspectives. However, when dealing with point clouds that have lower overlap ratios, the competitors \cite{mdgd,sghr,se3et,geotransformer,teaser} struggle to register them. This comparison illustrates that, although these existing methods can handle some unseen scenarios with high overlap, they are not generalizable to novel low overlap cases.
Moreover, the failure of these existing methods in the instance reconstruction task implies that they are not suitable for efficient 3D asset collection. By contrast, the proposed method successfully registers the unordered multiview point clouds. 
\par
In terms of instance reconstruction quality, as depicted in Fig. \ref{glb}, our reconstructed shape preserves intricate details well compared to the ground truth shape. The Traj LO \cite{traj} result shows almost no drift, though the surface is noisy. The Livox Mapping \cite{sdk} result exhibits noticeable drift and surface noise, while LOAM Livox \cite{lloam} demonstrates severe drift. Moreover, these LOAM methods require the entire rosbag instead of just five sparse scans. As shown in Table \ref{tabglb}, the proposed method achieves the highest reconstruction quality. Specifically, the CD and Recall of the reconstructed result are \textbf{0.003} and \textbf{96.22\%}, respectively. 
%
The decent performance in terms of reconstruction quality metrics \cite{mending,cd} indicates that our method can serve as a convenient, efficient, and low-cost tool for collecting high-fidelity 3D assets using the LiDAR sensor. 
In particular, we consider the proposed method efficient, as it only requires four or five scans from dramatically changed viewpoints to reconstruct the complete shape.


\begin{figure}[H] 
	\centering
\includegraphics[width=0.8\linewidth]{figs/chfour/glbtraj.png}
	\caption{An illustration of the experimental setup and a visual comparison of the proposed method against the SOTA methods (MDGD \cite{mdgd}, SGHR \cite{sghr}, SE3ET \cite{se3et}, GeoTrans \cite{geotransformer}, and Teaser++ \cite{teaser}) regarding instance reconstruction from sparse scans.}
	\label{glbtraj}
\end{figure}
%
\par
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chfour/glb.png}
	\caption{Visual comparison of the instance reconstruction. From top to bottom: ground truth, Ours, Traj LO \cite{traj}, Livox Mapping \cite{sdk}, and LOAM Livox \cite{lloam}.}
	\label{glb}
\end{figure}

\begin{table}[ht]
\caption{Comparison with LOAM methods regarding reconstruction }
	\centering
 % {
	% \begin{center}
		\begin{tabular}{c|c|c}
			\hline\hline
				Method $\backslash$ Metric & CD $\downarrow$ & Recall ($\%$) $\uparrow$ \\ \hline
   Livox Mapping \cite{sdk} & 0.0106 & 75.27 \\ \hline
   LOAM Livox \cite{lloam} & 0.0335 & 78.82  \\ \hline
   Traj LO \cite{traj} & 0.0107 & 82.88  \\ \hline
   \textbf{Ours}  & \textbf{0.0030} & \textbf{96.22}
\\ \hline  \hline	
		\end{tabular}
		\label{tabglb}
\end{table}


\subsection{Application 2: Training Data Collection and Enhancement of\\ Existing Learning-Based Methods} \label{testadd}
Training data is crucial for learning-based methods. Most existing methods \cite{sghr,mdgd} are trained on the 3DMatch dataset \cite{3dmatch}. Augmenting the training data is beneficial for improving the generalization ability of learning-based methods. However, the existing methods cannot be utilized for collecting training data in unseen scenes, as unseen scenes imply out-of-distribution cases and the limited effectiveness of the existing methods.
% As demonstrated in Sections \ref{test1} and \ref{test2}, L-PR can automatically and efficiently register multiview low overlap 3D point clouds. Thus,
The proposed method can serve as a valuable tool for collecting training data. 
In this test, we demonstrate that training data collected using our method can enhance the performance of the state-of-the-art methods across various benchmarks.\\
\noindent\textbf{Data.} Using the proposed method, all the point clouds shown in Figs. \ref{newbench} and \ref{glbtraj}, comprising 11 scenes with 33 scans, are aligned and processed into the format required for training by MDGD \cite{mdgd} and SGHR \cite{sghr}. The newly collected data is named Livox-3DMatch. We train the models \cite{mdgd,sghr} from scratch using only 3DMatch and a combination of 3DMatch and Livox-3DMatch.
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chfour/livox3dmatchfig.png}
	\caption{An illustration of the scenes in Livox-3DMatch.}
	\label{livox3dmatchfig}
\end{figure}
\noindent\textbf{Benchmarks and Metrics.}
We compare the performance of the models \cite{mdgd,sghr} trained with and without our data on three popular benchmarks: 3DMatch \cite{3dmatch}, ETH \cite{eth}, and ScanNet \cite{scan}. Following \cite{sghr}, Registration Recall (RR) is used to evaluate performance on 3DMatch \cite{3dmatch} and ETH \cite{eth}, while RMSEs are used to report performance on ScanNet \cite{scan}. RR is defined as follows:
\begin{equation}
{\rm RR} =\frac{N_{success}}{N_{total}}.
\end{equation}
where $N_{success}$ refers to the number of successful registrations, and $N_{total}$ be the total number of registration attempts. For the registration to be considered successful, the RMSEs (See Eq. (\ref{rmse})) must be below a predefined threshold.\par
\noindent\textbf{Results and Analysis.} First, the number of pairs in the training data increases from 14,400 to \textbf{17,700}, a boost of \textbf{22.91\%}, thanks to the introduction of our data. 
%
The quantitative results are reported in Table \ref{tabsghr}. 
%
As seen, the RR of SGHR \cite{sghr} is increased by \textbf{2.90\%} on 3DMatch \cite{3dmatch} and \textbf{4.29\%} on ETH \cite{eth}. The translation error and rotation error on ScanNet \cite{scan} are decreased by \textbf{22.72\%} and \textbf{11.19\%}, respectively.
%
The RR of MDGD \cite{mdgd} is improved by \textbf{1.71\%} on 3DMatch \cite{3dmatch} and \textbf{2.89\%} on ETH \cite{eth}. The translation error and rotation error on ScanNet \cite{scan} are decreased by \textbf{22.45\%} and \textbf{7.80\%}, respectively.
%
The reason behind these improvements is mentioned in Section \ref{dataset}: the introduction of our data enriches the learnable features during training, which benefits the generalizability of the models.

\begin{table*}[htb]
% \begin{table}[htbp]

\caption{Quantitative evaluation of the enhancement of SGHR \cite{sghr} and MDGD \cite{mdgd} due to the addition of Livox-3DMatch to the training.}

\centering
	\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c}
\hline\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Dataset} & RR $\uparrow$  & \multirow{2}{*}{Dataset} &RR $\uparrow$ & \multirow{2}{*}{Dataset} &  $\mathrm{RMSE}_{T}$ (m) $\downarrow$ & $\mathrm{RMSE}_{R}$ (deg) $\downarrow$ \\ 
  & & (\%) &  &(\%) &  &  Mean/Med & Mean/Med\\ \hline
SGHR \cite{sghr} & \multirow{4}{*}{3DMatch \cite{3dmatch}}  & 92.68 & \multirow{4}{*}{ETH \cite{eth}} &93.74 & \multirow{4}{*}{ScanNet \cite{scan}} &  0.66/0.51 & 23.50/22.08 \\ 
SGHR \cite{sghr} + \textbf{Livox-3DMatch (our data)} &  & \textbf{95.58} &  &\textbf{98.03} &  &  \textbf{0.51/0.45} & \textbf{20.87/17.21} \\ 
MDGD \cite{mdgd}  &  & 94.26 &  & 96.06 &  &  0.49/0.44 & 20.51/19.82 \\ 
MDGD \cite{mdgd} + \textbf{Livox-3DMatch (our data)} &  & \textbf{95.97} &  &\textbf{98.95} &  &  \textbf{0.38/0.31} & \textbf{18.91/17.36} \\ \hline

\hline\hline
\end{tabular}
}
\label{tabsghr}
% \end{center}

\end{table*}

\subsection{Application 3: Reconstructing a Degraded Scene} \label{test3}
An important application of fiducial markers in the real world is enhancing the robustness of reconstruction and localization in degraded scenes when additional sensors are unavailable. Therefore, in this test, we evaluate our method in a textureless, degraded scene.  \par
\noindent\textbf{Data.}
The setup is shown in the reference of Fig. \ref{labpic}. This scenario has repetitive structures and weak geometric features. We attach thirteen 16.4 cm $\times$ 16.4 cm AprilTags to the wall. The LiDAR scans the scene from 11 viewpoints. We also captured 72 images with an iPhone 13 to use as input for SfM-M \cite{qingdao}. The ground truth trajectories are given by an OptiTrack Motion Capture system. 
\par
\noindent\textbf{Competitors and Metrics.}
The competitors are the SOTA point cloud registration methods \cite{mdgd,sghr,se3et,geotransformer,teaser}. However, considering these competitors struggle in the degraded scene, we add SfM-M \cite{qingdao}, the SOTA marker-based SfM method, as a competitor. This addition enables a comprehensive and meaningful comparison. We employ the RMSEs as the metric.\\
%
\noindent\textbf{Results and Analysis.}
The qualitative results are presented in Fig. \ref{labpic}. As seen, none of the point cloud registration methods \cite{mdgd,sghr,se3et,geotransformer,teaser} can align a single pair of point clouds. 
%
This is because these cases only have planar overlap regions and thus are even more challenging than those in Fig. \ref{newbench} and Fig. \ref{glbtraj}. 
%
In comparison, the proposed method and SfM-M \cite{qingdao} successfully reconstruct this degraded scene as they utilize the fiducial markers. The comparison of these two methods in terms of sensor trajectories is shown in Fig. \ref{labtraj}. The quantitative results shown in Table \ref{427} demonstrate that the proposed approach achieves better localization accuracy, which is expected given that LiDAR is a ranging sensor. 
\par
\begin{figure}[H] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chfour/labpic2.png}
	\caption{An illustration of the experimental setup and a visual comparison of the proposed method against the SOTA methods (MDGD \cite{mdgd}, SGHR \cite{sghr}, SE3ET \cite{se3et}, GeoTrans \cite{geotransformer}, Teaser++ \cite{teaser}, and SfM-M \cite{qingdao}) in a degraded scene.}
	\label{labpic}
\end{figure}

\begin{figure}[H] 
	\centering
\includegraphics[width=0.6\linewidth]{figs/chfour/labtraj.png}
	\caption{A comparison of the sensor trajectories obtained from the proposed method and SfM-M \cite{qingdao}. G.T. refers to the ground truth.}
	\label{labtraj}
\end{figure}

\begin{table}[th]
\caption{Comparison with SfM-M \cite{qingdao} regarding localization accuracy in a degraded scene. }
	\centering
	% \resizebox{0.8\columnwidth}{!}{
 % {
	% \begin{center}
		\begin{tabular}{c|c|c}
			\hline\hline
				Method $\backslash$ Metric & $\mathrm{RMSE}_{T}$ (m) $\downarrow$ & $\mathrm{RMSE}_{R}$ (rad) $\downarrow$\\ \hline
   SfM-M \cite{qingdao} & 0.0551 & 0.0491  \\ \hline
   \textbf{Ours} & 0.0490 & 0.0384
\\ \hline  \hline
			
		\end{tabular}
	% }
		\label{427}
  
\end{table}

\subsection{Application 4: Localization in a GPS-denied Environment} \label{test4}
Localization is also a crucial application of point cloud registration methods. Fiducial markers are a popular tool for providing localization information in GPS-denied environments, such as indoor parking lots. In this test, we evaluate the proposed method in this context. \par
\noindent\textbf{Data.}
The experimental setup is shown in Fig. \ref{rob}(a): four 69.2 cm $\times$ 69.2 cm ArUco \cite{aruco} markers are deployed in the environment. The vehicle, equipped with an RS-Ruby 128-beam mechanical LiDAR, follows an 8-shaped trajectory without pausing and samples 364 LiDAR scans. We conduct the experiment on the roof of a large parking lot to acquire the ground truth trajectory from the Real-Time Kinematic.  \par
\noindent\textbf{Competitor and Metric.}
Note that most LO methods are limited to specific LiDAR models, and modifying the method to accommodate the features of a particular LiDAR model is not trivial \cite{lloam}. 
%
Considering this, the choice is made to compare with KISS-ICP \cite{kiss}, the state-of-the-art general pure LO method, which can be directly applied to the RS-Ruby 128 LiDAR without requiring modifications. Again, RMSEs are employed as the metric.\par
\noindent\textbf{Results and Analysis.}
The visual comparison and quantitative comparison of localization results are presented in Fig. \ref{rob}(b) and Table \ref{outtab}, respectively. Our method exhibits less drift in the middle of the trajectory and demonstrates better overall localization accuracy.
\begin{figure}[H] 
	\centering
\includegraphics[width=0.8\linewidth]{figs/chfour/rob2.png}
	\caption{(a): An illustration of the experimental setup. (b): Comparison of the trajectories given by different methods.}
	\label{rob}
\end{figure}

\begin{table}[th]

\caption{Comparison with KISS-ICP \cite{kiss} regarding localization. }
	\centering
	% \resizebox{0.8\columnwidth}{!}{
 % {
	% \begin{center}
		\begin{tabular}{c|c|c}
			\hline\hline
				Method $\backslash$ Metric & $\mathrm{RMSE}_{T}$ (m) $\downarrow$ & $\mathrm{RMSE}_{R}$ (rad) $\downarrow$\\ \hline
   KISS-ICP \cite{kiss} & 0.1976 & 0.1617  \\ \hline
   \textbf{Ours} & 0.1715 & 0.1394
\\ \hline  \hline
		\end{tabular}
	% }
		\label{outtab}
\end{table}

\subsection{Application 5: 3D Map Merging} \label{test5}
To validate the performance of the proposed method in large-scale scenarios, we apply the proposed method to the 3D map merging task in this test, which involves merging multiple large-scale, low overlap 3D maps into a single frame.
\\
\noindent\textbf{Data.} We collected three large-scale LiDAR maps. They are constructed using the SOTA LiDAR-based SLAM method, Traj-LO \cite{traj}, by scanning the York University campus with a Livox MID-40 LiDAR. The ground truth poses between the maps are manually obtained using \textit{CloudCompare} \cite{cloudcompare}. \\
\noindent\textbf{Competitors and Metrics.}
The competitors are SOTA multiview point cloud registration methods, including MDGD \cite{mdgd} and SGHR \cite{sghr}. Moreover, unlike previous tests, the LiDAR fiducial markers on the 3D maps are localized using the algorithm from our previous work \cite{mapmerge}, and the marker detection results serve as input for the our pipeline.
We employ the RMSEs as the metric.
\\
\noindent\textbf{Results and Analysis.}
The visual and quantitative comparisons are shown in Fig. \ref{merge} and Table \ref{tabmap}, respectively. As shown in Fig. \ref{merge}, neither MDGD nor SGHR can address this challenging task. This is due to the fact that the overlap in large-scale maps is too scarce. Specifically, the overlap rates \cite{pre} are 4.87\% between map 1 and map 2, 3.96\% between map 1 and map 3, and 2.59\% between map 2 and map 3. On the other hand, both MDGD and SGHR start by analyzing the features of each individual point cloud. As the scale of the point cloud becomes larger, the portion of the features belonging to the overlap regions decreases, making them unsuitable for large-scale, low overlap map merging. In addition, as the scale of point clouds becomes larger, the absolute error values of MDGD and SGHR also increase. In comparison, even though the scales of the point clouds in this test are much larger than those in the previous test, the proposed method successfully merges these large-scale, low overlap 3D maps. This stems from the observation that the proposed method focuses on utilizing the thin-sheet LiDAR fiducial markers and is not sensitive to scale changes. As introduced in \cite{mapmerge}, the accuracy of marker localization degrades as the scale increases. Thus, the absolute error values of our method also increase compared to previous tests.
% \begin{figure}[H] 
% 	\centering
% \includegraphics[width=1.0\linewidth]{figs/chfour/merge.png}
% 	\caption{An illustration of 3D map merging using the proposed method.}
% 	\label{merge}
% \end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=6.0in]{figs/chfour/merge.png}
\color{blue}
	\caption{A comparison of the 3D map merging results of MDGD \cite{mdgd}, SGHR \cite{sghr}, and our method. }
	\label{merge}

\end{figure}

\begin{table}[H]

\caption{Comparison of 3D map merging results between MDGD \cite{mdgd}, SGHR \cite{sghr}, and our method.}
	\centering
	\resizebox{0.6\columnwidth}{!}{
 % {
	% \begin{center}
		\begin{tabular}{c|c|c}
			\hline\hline
				Method $\backslash$ Metric & $\mathrm{RMSE}_{T}$ (m) $\downarrow$ & $\mathrm{RMSE}_{R}$ (rad) $\downarrow$\\ \hline
   MDGD \cite{mdgd} & 2.7883 & 1.9210  \\ \hline
   SGHR \cite{sghr} & 4.0933 & 2.3925  \\ \hline
   \textbf{Ours} & 0.2305 & 0.1771
\\ \hline  \hline
			
		\end{tabular}
	}
		\label{tabmap}
\end{table}

\subsection{Ablation Studies} \label{ab}
The proposed framework is composed of two levels of graphs. To demonstrate the necessity of this overall architecture, we conduct ablation studies in this section. In particular, we study the effects of removing the first and second graphs in two cases: Fig. \ref{glbtraj} and the kitchen scene in Fig. \ref{newbench}. The visual comparison and quantitative results are shown in Fig. \ref{abstudy} and Table \ref{abtab}, respectively. When the first graph is removed, the factors representing the relative poses between frames in Fig. \ref{second} have to be set to identity due to the lack of initial values provided by the first graph. 
%
However, the role of the second graph is to further optimize the variables based on their initial values rather than finding the optimal solution from scratch.
%
Consequently, as shown in Fig. \ref{abstudy} and Table \ref{abtab}, severe misalignment or degradation in registration accuracy occurs.
%
When the second graph is removed, the multiview point clouds are directly registered using the initial values obtained from the first graph, without any further refinement. 
%
As shown in Fig. \ref{abstudy} and Table \ref{abtab}, removing the second graph causes a slight degradation in registration accuracy.
%
Moreover, the degradation in the kitchen case of Fig. \ref{newbench} is slighter than that in Fig. \ref{glbtraj}.
%
This is because the effect of the second graph is case-by-case, determined by the quality of the initial values provided by the first graph. 
%
Namely, the better the initial values are, the less important the second graph becomes. However, in the real world, pose estimation of markers cannot be perfect. In addition, as seen in Fig. \ref{secondgraph}, we also add the marker corners to the second graph so that the pose estimation of each individual marker can be further optimized along with other variables in the graph.
%
Therefore, it is beneficial to apply the second graph in practice. In summary, the proposed framework adopts a coarse-to-fine pipeline, where the first graph corresponds to the coarse stage, while the second graph is the fine stage. Removing any of them will cause performance degradation.
\begin{figure}[h]
	\centering
  
	\includegraphics[width=6.0in]{figs/chfour/abstudy.png}

	\caption{ A comparison of the point cloud registration results without the first graph, without the second graph, and with both graphs for two cases. }
	\label{abstudy}

\end{figure}

\begin{table}[h]
\caption{Ablation studies on the first and second graphs.}
	\centering
	\resizebox{0.7\columnwidth}{!}{
 % {
	% \begin{center}
		\begin{tabular}{c|c|c|c}
			\hline\hline
				 Scene & Framework & $\mathrm{RMSE}_{T}$ (m) $\downarrow$ & $\mathrm{RMSE}_{R}$ (rad) $\downarrow$\\ \hline
\multirow{3}{*}{Fig. \ref{glbtraj}} & without first graph & 0.431 & 0.459 \\
& without second graph & 0.078 & 0.084 \\
& with both graphs & 0.066 & 0.072 \\
\hline
\multirow{3}{*}{Fig. \ref{newbench}} & without first graph & 0.088 & 0.112 \\
& without second graph & 0.021 & 0.073 \\
(Kitchen) & with both graphs & 0.019 & 0.067 \\
   % MDGD \cite{mdgd} & 2.7883 & 1.9210  \\ \hline
   % SGHR \cite{sghr} & 4.0933 & 2.3925  \\ \hline
   % \textbf{Ours} & 0.2305 & 0.1771
\hline  \hline
			
		\end{tabular}
	}
		\label{abtab}
\end{table}

\subsection{Limitations} 
Despite the promising results of the proposed algorithm, there are potential limitations. Firstly, although LiDARs are robust to unideal illumination conditions, adverse weather phenomena such as rain, snow, and fog can affect LiDAR measurements, thereby reducing the effectiveness of the proposed method. Secondly, the adopted low-cost LFMs, made of thin-sheet paper or boards, may deform after long-term use in the wild. However, this is not a concern for one-time applications such as data collection. Finally, deploying the LFMs requires some labor, but their value is demonstrated in this dissertation.