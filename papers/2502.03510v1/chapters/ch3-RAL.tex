\section{Overview} \label{3.1}
This chapter introduces the vanilla IFM system. As illustrated in Fig. \ref{flex}, the proposed IFM is a general system with ample flexibility. As seen, unlike previous LiDAR fiducial objects, the markers are thin-sheet objects attached to other surfaces without affecting the 3D geometry of the environment. In particular, Figs. \ref{flex}(a,b,c) show that different VFM systems, such as (a) AprilTag \cite{ap3}, (b) CCTag \cite{cctag}, and (c) ArUco \cite{aruco}, can be easily embedded. Moreover, the system is applicable to both solid-state LiDAR Figs. \ref{flex}(a, b) and mechanical LiDAR Fig. \ref{flex}(c). An AR demo using the proposed system is shown in Fig. \ref{flex}(d): the teapot point cloud is transmitted to the location of the marker in the LiDAR's point cloud based on the pose provided by the IFM system. \par
\begin{figure}[H] 
	\centering
	\includegraphics[width=0.8\linewidth]{figs/chtwo/flex.png}
	\caption{An illustration of the flexibility and generalizability of the proposed IFM.
 }
	\label{flex}
\end{figure}
LiDARTag \cite{lt} is a highly relevant prior work. Fig. \ref{tagoverview} shows a comprehensive comparison between LiDARTag and the proposed system. The comparison regards three aspects. 
\begin{enumerate}[label=(\roman*)]
	 \item Convenience of usage. The LiDARTag \cite{lt} system adopts the single-linkage clustering in the tag detection stage that brings restrictions on marker placement. Namely, there must be adequate clearance around the marker's object, which makes the marker an extra object added to the spatial environment.  Furthermore, according to the implementation of LiDARTag, the method to find the boundaries of the markers in the tag decoding stage requires that the marker be perpendicular to the ground. In contrast, owing to the fact that the IFM system does not adopt the clustering method based on spatial features, it has no restrictions on marker placement. That is, the user can place the marker as they do with the conventional VFM.
	\item Extensibility. The current version of LiDARTag only supports markers with patterns belonging to the AprilTag 3 family \cite{ap3}. In the IFM system, however, different VFM systems (square and non-square) function directly since the marker detection is executed on the 2D intensity image.
	\item User-friendly design. The LiDARTag system \cite{lt}, though definitely well-programmed, adopts many settings that are different from the implementations of VFM systems. For example, users have to utilize the marker's local frame as the inertial coordinate system by default to define the sensor (LiDAR) pose. However, the development of the proposed system follows the successful VFM systems, such as AprilTag \cite{ap3} and ArUco \cite{aruco}. For instance, we leave the authority of defining the inertia coordinate system to the users as \cite{ap3,aruco} do. As a result, users who are familiar with the popular VFM systems can commence with our system comfortably.
\end{enumerate}

\begin{figure}[H] 
	\centering
	\includegraphics[width=1.0\linewidth]{figs/chtwo/newoverview.png}
	\caption{Comparison of LiDARTag \cite{lt} (top) and the proposed IFM (bottom).
 }
	\label{tagoverview}
\end{figure}
%    

\section{Preliminaries} \label{pre}
\subsection{Three-dimensional Transformation} \label{threedimen}
Suppose that $\mathbf{p}_{a} = [x,y,z]^{T}$ is a 3D point expressed in the coordinate system $\{a\}$. To express the point in another coordinate system $\{b\}$ as $\mathbf{p}_{b} = [x^{\prime},y^{\prime},z^{\prime}]^{T}$, translation and rotation transformations need be applied to $\mathbf{p}_{a}$. In particular, the extrinsic matrix, $\mathbf{T}\in\mathbb{R}^{4\times4}$, is employed to describe the 6-DOF pose:

\begin{equation}
\mathbf{T} = \left[\begin{array}{cc}
\mathbf{R} & \mathbf{t} \\
\mathbf{0}^{1 \times 3} & 1 \label{transform}
\end{array}\right],
\end{equation} 
where $\mathbf{t}\in\mathbb{R}^{3\times1}$ denotes the translation vector and $\mathbf{R}\in \mathbb{R}^{3\times3}$ denotes the rotation matrix. Afterwards, $\mathbf{p}_{b}$ is obtained as follows:
\begin{equation}
\left[\begin{array}{c}
\mathbf{p}_b \\
1
\end{array}\right]=\mathbf{T}\left[\begin{array}{c}
\mathbf{p}_a \\
1
\end{array}\right]. \label{dot}
\end{equation} \par
To simplify the notation, the operator ($\cdot$) is introduced to denote:
\begin{equation}
\mathbf{p}_b=\mathbf{T} \cdot \mathbf{p}_a. 
\end{equation} \par
Please note that ($\cdot$) is not a dot product as $\mathbf{T}$ is $4 \times 4$ and $\mathbf{p}$ is $3\times 1$. So it is different from a regular dot product. Eq. (\ref{dot}) shows what it means.

\subsection{Lie Group and Lie Algebra} \label{lie}
The rotation matrix $\mathbf{R}\in SO(3)$, which is the special orthogonal group representing rotations \cite{barfoot}:
\begin{equation}
SO(3) = \{ \mathbf{R}\in\mathbb{R}^{3\times3} | \ \mathbf{R}\mathbf{R}^{T}=\mathbf{1},det(\mathbf{R}) =1 \}.
\end{equation} \par
The pose matrix $\mathbf{T} \in SE(3)$, which is the special Euclidean group representing poses \cite{barfoot}:
\begin{equation}
SE(3) = \{ \mathbf{T} = \left[\begin{array}{cc}
\mathbf{R} & \mathbf{t} \\
0 & 1 
\end{array}\right] \in \mathbb{R}^{4 \times 4} | \ \mathbf{R} \in SO(3), \mathbf{t}\in \mathbb{R}^{3 \times 1}\}.
\end{equation} \par
$SO(3)$ and $SE(3)$ are two specific Lie groups. Every matrix Lie group is associated with a Lie algebra \cite{barfoot}. The Lie algebra $\mathfrak{so(\mathrm{3})}$ that is associated with $SO(3)$ is defined as follows:
\begin{equation}
\begin{aligned}
& SO(3) \rightarrow \mathfrak{so(\mathrm{3})}: \\
& \log(\mathbf{R}) = \frac{\theta}{2\mathrm{sin}(\theta)}(\mathbf{R}-\mathbf{R}^{T}),  \\ 
& \xi = \log(\mathbf{R})_{\vee}, 
\end{aligned}
\end{equation}
where $\theta= \arccos \frac{1}{2}(Trace(\mathbf{R})-1)$. $\log(\cdot)$ is the matrix logarithm and $\xi \in \mathbb{R}^{3\times1}$ is the Lie algebra coordinates. $\vee$ is the \textit{vee} map operator that finds the unique vector $\xi \in \mathbb{R}^{3\times1}$ corresponding to a given skew-symmetric matrix $\log(\mathbf{R})\in \mathbb{R}^{3 \times 3}$ \cite{barfoot,tagslam}.

\subsection{LiDAR Technology}
LiDAR is a technology that uses laser pulses to measure the distance between the sensor and objects in the environment. It creates precise three-dimensional representations of the scanned scene. 
%
The LiDAR sensor emits laser beams, and by measuring the time it takes for the reflected light to return to the sensor, the positions of 3D points in space are calculated.
%
In addition to the geometric data, a LiDAR sensor can also capture intensity values, which reflect the strength of the returned signal. These intensity values are influenced by factors such as surface material, texture, and angle of incidence. The inclusion of intensity information enhances the ability to distinguish between different types of surfaces, improving tasks like object detection, scene understanding, and feature recognition. Fig.~\ref{lidarwork} illustrates the schematic diagram of the working principle of LiDAR.
\begin{figure}[H] 
	\centering
	\includegraphics[width=1.0\linewidth]{figs/intro/lidarwork.png}
	\caption{ The schematic diagram of the working principle of LiDAR.
 }
	\label{lidarwork}
\end{figure}






\section{Marker Detection} \label{3.2}
\label{marker-detection}
\subsection{Generation of the Intensity Image} \label{twotwoone}
As introduced in LiDARTag \cite{lt}, the reason LiDAR fiducial objects act as additional 3D objects, abandoning the free-placement advantage of VFMs, is due to the gap between structured images and unstructured point clouds.
%
Nevertheless, this gap is not insurmountable. In particular, there is a notable hot trend in LiDAR-based 3D object detection/segmentation \cite{rangenet,lasernet}: Neural Networks that are originally developed for 2D object detection/segmentation can be utilized to detect/segment the objects in the range/intensity image(s) of the 3D LiDAR point cloud. This indicates that the range/intensity image(s) generated from the LiDAR point cloud can be a pathway to transfer the research accomplishments on the 2D image to the 3D point cloud. Following this inspiration, and considering that the black-and-white marker is explicitly visible in the point cloud rendered by intensity, the intensity image is utilized for LFM development.
\par
The generation of an intensity image from a given unstructured point cloud can be summarized as transferring all the 3D points in the point cloud onto a 2D image plane by spherical projection and rendering the corresponding pixels with intensity values. Fig. \ref{notation} shows the coordinate systems and notations. $\mathbf{p_{L}}=[x_{L},y_{L},z_{L},i]^{T}$ is an observed point in the 3D point cloud, with ${[x_{L},y_{L},z_{L}]^{T}}$ denoting its Cartesian coordinates w.r.t. the LiDAR coordinate system $\{L\}$ and $i$ being the intensity.  $\mathbf{u}$ is the projection of $\mathbf{p_{L}}$ onto the image plane, which has coordinates $[u,v]^{T}$ w.r.t. the image coordinate system, $\{I\}$. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/chtwo/notation.png}	
	\caption{An illustration of the coordinate systems and notations.}
	\label{notation}
\end{figure}
Following \cite{barfoot}, the Cartesian coordinates of $\mathbf{p_{L}}$ are first transformed to spherical coordinates $[\theta,\phi,r]^{T}$:
\begin{equation}	
	\begin{aligned}
		& \theta=\arctan(\frac{y_{L}}{x_{L}}),\\
		& \phi=\arctan(\frac{z_{L}}{\sqrt{x_{L}^{2}+y_{L}^2}}),\\
		& r=\sqrt{x_{L}^2+y_{L}^2+z_{L}^2},
	\end{aligned}\label{pro}
\end{equation}
where $\theta$ and $\phi$ denote the azimuth and inclination, respectively. $r$ is the range from $\mathbf{p_{L}}$ to the origin of $\{L\}$. Then, the image coordinates $[u,v]^{T}$ of  $\mathbf{u}$ are given by:
\begin{equation}	
	u = \lceil \frac{\theta}{\Theta_{a}}\rfloor + u_{o},\; v = \lceil \frac{\phi}{\Theta_{i}}\rfloor + v_{o} \label{uv}
\end{equation}
where $\lceil \; \rfloor$ represents rounding a value to the nearest integer. 
% This rounding can cause information loss if the pixels are non-integer values at the beginning; however, this approximation is not fatal as can be seen in Section \ref{exp}. 
$\Theta_{a}$ and $\Theta_{i}$ are the angular resolutions in $u$ (azimuth) and $v$ (inclination) directions, respectively. $u_{o}$ and $v_{o}$ are the offsets: 
\begin{equation}	
	\theta =\Theta_{a}(u - u_{o}), \; \phi =\Theta_{i}(v - v_{o}).  \label{test11}
\end{equation} \par
Assume it is desired that the point with zero-azimuth and zero-inclination to be projected to the center of the image, then the offsets will be: $u_{o}=I_{w}/2$ and $v_{o}=I_{h}/2$, where $I_{w}$ and $I_{h}$ being the image width and height which are determined by the maximum angular width $P_{w}$ and height $P_{h}$ of the point cloud $I_{w} = \lceil \frac{P_{w}}{\Theta_{a}}\rfloor, \; I_{h} = \lceil \frac{P_{h}}{\Theta_{i}}\rfloor$. The pixel $[u,v]^{T}$ is then rendered by a specific color corresponding to the intensity value $i$. Refer to \cite{colormap,PCL} to see how the correspondence between color and intensity value is generated. For each pixel, the range information $r$ is also saved for the sake of the following pose estimation. Thereafter, we step through the point cloud and repeat the above process. The pixels that are not mapped to any points will remain unobserved and are rendered by a unique predefined value. Note that if these pixels are visited later on in the marker detection process, they will not return any 3D points because they represent unobserved regions.

\subsection{Selections of the Angular Resolutions} \label{select}

The selections of $\Theta_{a}$ and $\Theta_{i}$ are crucial as they affect the quality of the intensity image directly. However, the solution to this problem is straightforward. Suppose that the horizontal angular resolution and vertical angular resolution given by the user manual of the employed LiDAR are $\Theta_{h}$ and $\Theta_{v}$. We should set $\Theta_{a}=\Theta_{h}$ and $\Theta_{i}=\Theta_{v}$. Fig. \ref{newreso} illustrates the effect of choosing different values of $\Theta_{a}$ and $\Theta_{i}$ on the intensity image, with the unobserved regions rendered in light green.
\par
Figs. \ref{newreso}(a,b,c) are intensity images generated from the same point cloud given by one LiDAR scan of a Velodyne ULTRA Puck (mechanical LiDAR, $\Theta_{h}=0.4^{\circ}$ and $\Theta_{v}=0.33^{\circ}$). The LiDAR scan is extracted from the dataset provided by \cite{lt}. Moreover, Figs. \ref{newreso}(a,b,c)
are cropped to display.
Figs. \ref{newreso}(d,e,f) are intensity images generated from the same point cloud which is the integration of multiple LiDAR scans of a Livox Mid-40 (solid-state LiDAR, $\Theta_{h}=\Theta_{v}=0.05^{\circ}$). The detailed settings for each subimage are as follows. (a): $\Theta_{a}=0.05^{\circ}$ and $\Theta_{i}=0.05^{\circ}$; raw image size = $7200\times800$. (b): $\Theta_{a}=0.4^{\circ}$ and $\Theta_{i}=0.33^{\circ}$; raw image size = $900\times121$. (c): $\Theta_{a}=0.6^{\circ}$ and $\Theta_{i}=1.0^{\circ}$; raw image size = $600\times40$. (d): $\Theta_{a}=\Theta_{i}=0.025^{\circ}$; raw image size = $1538\times1178$. (e): $\Theta_{a}=\Theta_{i}=0.05^{\circ}$; raw image size = $771\times591$. (f): $\Theta_{a}=\Theta_{i}=0.5^{\circ}$; raw image size = $81\times62$.

\begin{figure}[H] 
	\centering
	\includegraphics[width=0.7\linewidth]{figs/chtwo/newreso.png}
 \caption{The intensity images generated under different angular resolution settings.
 }
	\label{newreso}
\end{figure} 

In summary, when  $\Theta_{a}<\Theta_{h}$ and $\Theta_{i}<\Theta_{v}$, too many unobserved regions appear around or inside the marker's area (See Fig. \ref{newreso}(a) and (d)). When  $\Theta_{a}>\Theta_{h}$ and $\Theta_{i}>\Theta_{v}$, too many points overlap for the same pixels based on Eq.~(\ref{uv}), such that the marker's pattern could disappear (See Fig. \ref{newreso}(c) and (f)). \par
%
%
Nevertheless, as shown in Fig. \ref{samplecom}, the LiDAR cannot sample perfectly evenly in the inclination/azimuth space. Specifically, Fig. \ref{samplecom}(a) shows the general sampling pattern of a mechanical LiDAR expressed in the azimuth/inclination coordinate system. This is a general schematic that does not correspond to any LiDAR model. Fig. \ref{samplecom}(b) presents the sampling pattern of the Livox Mid-40 LiDAR after one-second integration. Note that the sampling patterns vary when it comes to different solid-state LiDAR models. But generally, the unscanned regions within the valid Field of View (FoV) appear as spots.
%
Thus, even if the user manual of the employed LiDAR is followed to set $\Theta_{a}=\Theta_{h}$ and $\Theta_{i}=\Theta_{v}$, the unwanted unobserved regions around or inside the marker's area and the points overlapping issue are still inevitable on account the fact that $\Theta_{h}$ and $\Theta_{v}$ are approximate values themselves. Based on the experiments, the points overlapping issue under $\Theta_{a}=\Theta_{h}$ and $\Theta_{i}=\Theta_{v}$ could have an acceptable influence on pose estimation while the unobserved regions impede marker detection.
\begin{figure}[H] 
	\centering
	\includegraphics[width=1.0\linewidth]{figs/chtwo/samplecom.png}
	\caption{Sampling patterns of the mechanical LiDAR and solid-state LiDAR, with sampling points represented by red scatter plots.
 }
	\label{samplecom}
\end{figure} \par

To remove the image noise caused by unobserved regions, image preprocessing is carried out as shown in Fig. \ref{dis} before inputting the raw intensity image (Fig.~\ref{dis}(a)) into the embedded VFM system. The preprocessing includes grayscale conversion (Fig.~\ref{dis}(b)), naive thresholding (Fig.~\ref{dis}(c)), and optional Gaussian blurring \cite{gb} (Fig.~\ref{dis}(d)). 
In particular, the raw intensity image (Fig.~\ref{dis}(a)) is first converted to a grayscale image (Fig.~\ref{dis}(b)). Then it becomes the binary image (Fig.~\ref{dis}(c)) using naive thresholding. After the naive thresholding, the image noise caused by varying low-intensity values is removed. Gaussian blurring \cite{gb} is recommended when the embedded VFM system, such as CCTag \cite{cctag}, does not contain it. Fig.~\ref{dis}(d) shows the intensity image of a CCTag after the naive thresholding while the detector cannot detect the marker in it. Fig.~\ref{dis}(e) presents the intensity image after applying Gaussian Blur \cite{gb} on Fig.~\ref{dis}(d). Now the marker is detectable.
%
The methods utilized in the preprocessing are simple but not trivial, which implies that without them the marker detection will fail.
\begin{figure}[H] 
	\centering
	\includegraphics[width=0.8\linewidth]{figs/chtwo/dis.png}
	\caption{An illustration of image preprocessing in the system.
 }
	\label{dis}
\end{figure} \par

\subsection{3D Fiducials Estimation} \label{twotwothree}
The preprocessed image introduced in the previous section is then inputted into the embedded VFM system. Thereafter, the VFM system provides the detection information. In this section, we introduce how to project the 2D fiducials given by the detection information back to the 3D space, such that they become 3D fiducials expressed in the LiDAR coordinate system. As for the square markers, the fiducials refer to the four vertices of the quad. While some of the VFM systems adopt the non-square design, the proposed method has no restriction on the marker shape.
\par
As mentioned in Section~\ref{twotwoone}, the range information is stored for each pixel in the intensity image as well. Thus, a 2D pixel with range information ($\mathbf{{u}^{r}}=[u,v,r]^{T}$) can be projected back to the 3D Cartesian coordinate system by solving the inverse of Eqs. (\ref{pro}-\ref{uv}) to find the corresponding $[x_{L},y_{L},z_{L}]^{T}$. However, in the real world, it cannot be guaranteed that the fiducials of the marker are exactly scanned by the LiDAR. Namely, the detected 2D features in Fig.~\ref{dis}(c) could correspond to the unobserved regions in the raw intensity image. As a matter of fact, this occurs frequently in our experiments, as well as in the dataset provided by \cite{lt}. Hence, when these detected but unscanned features are checked, there will be no range value $r$ returned and it is not feasible to solve the inverse of Eqs. (\ref{pro}-\ref{uv}).
\par
To resolve this problem, we propose an algorithm as illustrated in Fig.~\ref{collinear}. The algorithm is based on the fact that the markers are planar. Suppose that there is a 2D feature point $\mathbf{{u}_{k}}=[u_{k},v_{k}]^{T}$ that is detected but corresponds to an unobserved region in the raw intensity image. The azimuth $\theta_{k}$ of $\mathbf{{u}_{k}}$ is determined by Eq. (\ref{uv}) through $\theta_{k}=\Theta_{a}(u_{k}-u_{0})$. Define the unknown 3D point corresponding to $\mathbf{{u}_{k}}$ as $\mathbf{{p}_{k}}=[x_{k},y_{k},z_{k}]^{T}$ (the yellow point in Fig.~\ref{collinear}).  Hereafter, suppose that in the same column as $\mathbf{{u}_{k}}$, there is a pair of observed pixels that are symmetric about $\mathbf{{u}_{k}}$, and define their corresponding points as $\mathbf{{p}_{u}}=[x_{u},y_{u},z_{u}]^{T}$ and $\mathbf{{p}_{d}}=[x_{d},y_{d},z_{d}]^{T}$ (the black point in Fig.~\ref{collinear}). As illustrated in Eq. (\ref{uv}), pixels in the same column approximately share the same azimuth. 
%
Thus, $\mathbf{{p}_{u}}$, $\mathbf{{p}_{k}}$, and $\mathbf{{p}_{d}}$ are on the same plane, $\alpha$, which is specified by fixing the azimuth ($\theta=\theta_{k}$). 
%
After that, define the plane where the marker is located as $\beta$. Obviously, the intersection of $\alpha$ and $\beta$ is a straight line $\textbf{\textit{l}}$. Under the assumption that $\mathbf{{p}_{u}}$ and $\mathbf{{p}_{d}}$  are on the marker plane $\beta$, it can be shown that  $\mathbf{{p}_{u}}$, $\mathbf{{p}_{k}}$, and $\mathbf{{p}_{d}}$ are collinear and they all fall on $\textbf{\textit{l}}$. To introduce the algorithm more clearly, Fig.~\ref{side} shows the side view of plane $\alpha$.

\begin{figure}[H] 
	\centering
	\includegraphics[width=1.0\linewidth]{figs/chtwo/collinear.png}
	\caption{An illustration of the algorithm to estimate the 3D coordinates of a detected but unscanned 2D feature point.} \label{collinear}
\end{figure} \par

\begin{figure}[H] 
	\centering
	\includegraphics[width=0.8\linewidth]{figs/chtwo/side.png}
	\caption{The side view of $\alpha$. $\phi_{d}$, $\phi_{k}$, and $\phi_{u}$ are the inclinations of $\mathbf{{p}_{d}}$, $\mathbf{{p}_{k}}$, and $\mathbf{{p}_{u}}$, respectively. }
	\label{side}
\end{figure}
Undoubtedly, $\mathbf{{O}_{L}}\mathbf{{p}_{k}}$ is the angle bisector of $\angle\mathbf{{p}_{u}}\mathbf{{O}_{L}}\mathbf{{p}_{d}}$ owing to $\phi_{d}-\phi_{k}=\phi_{k}-\phi_{u}$. Hence, in the light of the angle bisector properties, we have 
	$\mathbf{{p}_{k}}\mathbf{{p}_{d}}/\mathbf{{p}_{u}}\mathbf{{p}_{k}}=\mathbf{{O}_{L}}\mathbf{{p}_{d}}/\mathbf{{O}_{L}}\mathbf{{p}_{u}}$. Note that $\mathbf{{O}_{L}}\mathbf{{p}_{d}}$ and $\mathbf{{O}_{L}}\mathbf{{p}_{u}}$ are the ranges of $\mathbf{{p}_{d}}$ and $\mathbf{{p}_{u}}$ which can be obtained if they are scanned. Thus, the unknown 3D coordinates of $\mathbf{{p}_{k}}$ are estimated by $\mathbf{{p}_{k}}=\mathbf{{M}_{1}}\mathbf{{p}_{d}}+\mathbf{{M}_{2}}\mathbf{{p}_{u}}$, where $\mathbf{{M}_{1}}=\mathrm{diag}(\frac{\mu}{1+\mu} ,\frac{\mu}{1+\mu} ,\frac{\mu}{1+\mu} ) $ and $\mathbf{{M}_{2}}=\mathrm{diag} ( \frac{1}{1+\mu}, \frac{1}{1+\mu}, \frac{1}{1+\mu})$ with $\mu$ being the ratio $\mathbf{{O}_{L}}\mathbf{{p}_{d}}/\mathbf{{O}_{L}}\mathbf{{p}_{u}}$. So far, the 2D fiducials in $\{I\}$, observed and unobserved by the LiDAR, are projected back to ${\{L\}}$.

\section{LiDAR Pose Estimation} \label{3.3}
The aim of LiDAR pose estimation is to seek the Euclidean transformation, $\mathbf{T}=[\mathbf{R}| \mathbf{t}]$, from the world (inertia) coordinate system $\{G\}$ to the LiDAR coordinate system $\{L\}$.  $\mathbf{R}$ is a $3\times3$ orthogonal matrix that represents the rotation. $\mathbf{t}\in\mathbb{R}^{3}$ is the translation vector. Suppose that $\mathbf{f} \in \mathbb{R}^{3}$ are the 3D coordinates of a feature point, the operation of $\mathbf{T}$ on $\mathbf{f} \in \mathbb{R}^{3}$ is $\mathbf{T} \cdot \mathbf{f} = \mathbf{R}\mathbf{f}+\mathbf{t}$ (Refer to Section \ref{threedimen} if needed). LiDAR pose estimation can be resolved through optimally aligning two point sets while in real-world applications, such as SLAM and perception, the point correspondences between the two point sets are unknown, such that the correspondences are also needed to be optimally and iteratively searched \cite{nicp}. However, as seen in the following, with the help of the fiducial marker system, the search for correspondences can be skipped in LiDAR pose estimation, which is a vital benefit brought by using the fiducial marker system.

Thus far two sets of 3D points are obtained. 1) $n$ fiducials w.r.t. $\{L\}$, denoted by $\mathcal{{P}_{L}} = \{ \mathbf{{f}_{1}}, \ \cdots, \ \mathbf{{f}_{n}}\}$, which are given by the 3D fiducials estimation introduced in Section~\ref{twotwothree}; 2) $n$ fiducials w.r.t. $\{G\}$, denoted by $\mathcal{{P}_{W}}= \{ \mathbf{{f}_{1}}^{\prime}, \ \cdots, \ \mathbf{{f}_{n}}^{\prime} \}$, which are predefined. Furthermore, the points in $\mathcal{{P}_{L}} $ and $\mathcal{{P}_{W}}$ are matched based on the ID number and vertex index given by the marker detection. Hence, the LiDAR pose estimation can be transformed into finding $[\mathbf{R}|\mathbf{t}]$ that optimally align $\mathcal{{P}_{L}} $ and $\mathcal{{P}_{W}}$. This is inherently a least square problem:
\begin{equation}	
	\mathbf{R}^{*},\  \mathbf{t}^{*}=\underset{\mathbf{R},\  \mathbf{t}}{\arg \min } \sum_{j=1}^{n}\left\|\mathbf{f}_{j}-\left(\mathbf{R} \mathbf{f}_{j}^{\prime}+\mathbf{t}\right)\right\|^{2}. \label{least}
\end{equation}
	
Considering that the point-correspondence between $\mathcal{{P}_{L}} $ and $\mathcal{{P}_{W}}$ is quite reliable thanks to the embedded VFM system, $\mathbf{R}^{*}$ and $\mathbf{t}^{*}$ can be calculated in closed form by the Singular Value Decomposition (SVD) method \cite{barfoot,icp}. The centroids of $\mathcal{{P}_{L}} = \{ \mathbf{{f}_{1}}, \ \cdots, \ \mathbf{{f}_{n}}\}$ and $\mathcal{{P}_{W}}= \{ \mathbf{{f}_{1}}^{\prime}, \ \cdots, \ \mathbf{{f}_{n}}^{\prime} \}$ are defined as follows:
\begin{equation}
\begin{aligned}
&\mathbf{f}=\frac{1}{\mathrm{n}} \sum_{\mathrm{j}=1}^{\mathrm{n}}\left(\mathbf{f}_{j}\right), \\
&\mathbf{f}^{\prime}=\frac{1}{\mathrm{n}} \sum_{\mathrm{j}=1}^{\mathrm{n}}\left(\mathbf{f}_{j}^{\prime}\right). \label{centroid}
\end{aligned}
\end{equation}
\par
Then, Eq. (\ref{least}) can be reorganized as:
\begin{equation}
\begin{aligned}
	 &\sum_{j=1}^{n}\left\|\mathbf{f}_{j}-\left(\mathbf{R} \mathbf{f}_{j}^{\prime}+\mathbf{t}\right)\right\|^{2} \\ 
  = & \sum_{j=1}^{n}\left\| \mathbf{f}_{j} -\mathbf{R} \mathbf{f}_{j}^{\prime}-\mathbf{t} - \mathbf{f} +\mathbf{R}\mathbf{f}^{\prime} + \mathbf{f} - \mathbf{R}\mathbf{f}^{\prime} \right\|^{2} \\
  = & \sum_{j=1}^{n}\left\| \left(\mathbf{f}_{j} - \mathbf{f} -\mathbf{R}\left(\mathbf{f}_{j}^{\prime}-\mathbf{f}^{\prime}\right) \right) + \left(\mathbf{f}- \mathbf{R}\mathbf{f}^{\prime}-\mathbf{t}\right)
\right\|^{2} \\
  = & \sum_{j=1}^{n}\left(
  \left\| \mathbf{f}_{j} - \mathbf{f} -\mathbf{R}\left(\mathbf{f}_{j}^{\prime}-\mathbf{f}^{\prime}\right) \right\|^{2} +  \left\|\mathbf{f}- \mathbf{R}\mathbf{f}^{\prime}-\mathbf{t} \right\|^{2} + 2 \left(\mathbf{f}_{j} - \mathbf{f} -\mathbf{R}\left(\mathbf{f}_{j}^{\prime}-\mathbf{f}^{\prime}\right) \right)\left(\mathbf{f}- \mathbf{R}\mathbf{f}^{\prime}-\mathbf{t}\right) 
 \right). \label{eq3.6}
  \end{aligned}
\end{equation}
\par
Following the definitions of $\mathbf{f}$ and $\mathbf{f}^{\prime}$ shown in Eq. (\ref{centroid}), we have:
\begin{equation}	
\sum_{j=1}^{n}\left(\mathbf{f}_{j} - \mathbf{f} -\mathbf{R}\left(\mathbf{f}_{j}^{\prime}-\mathbf{f}^{\prime}\right) \right)\left(\mathbf{f}- \mathbf{R}\mathbf{f}^{\prime}-\mathbf{t}\right) =0.
\end{equation} \par
Namely, the last term of Eq. (\ref{eq3.6}) is zero. Thus, based on the derivation given in Eq. (\ref{eq3.6}), the problem introduced in Eq. (\ref{least}) is transformed into the following form:
\begin{equation}	
	\mathbf{R}^{*},\  \mathbf{t}^{*}=\underset{\mathbf{R},\  \mathbf{t}}{\arg \min } \sum_{j=1}^{n}\left(
  \left\| \mathbf{f}_{j} - \mathbf{f} -\mathbf{R}\left(\mathbf{f}_{j}^{\prime}-\mathbf{f}^{\prime}\right) \right\|^{2} +  \left\|\mathbf{f}- \mathbf{R}\mathbf{f}^{\prime}-\mathbf{t} \right\|^{2}\right) \label{newproblem}
\end{equation}\par
As seen in Eq. (\ref{newproblem}), the rotation ($\mathbf{R}$) and translation ($\mathbf{t}$) have been decoupled. In particular, the first term only involves $\mathbf{R}$, so $\mathbf{R}$ can be computed first and then substituted into the second term to obtain $\mathbf{t}$. Define the centroid-aligned coordinates as:
\begin{equation}
\begin{aligned}
&\mathbf{q}_{j} = \mathbf{f}_{j} - \mathbf{f} \\
&\mathbf{q}^{\prime}_{j} = \mathbf{f}_{j}^{\prime}-\mathbf{f}^{\prime}
\end{aligned}
\end{equation}
\par
By substituting the centroid-aligned coordinates into the first term of Eq. (\ref{newproblem}), the computation of rotation is described as follows:
\begin{equation}	
	\mathbf{R}^{*}=\underset{\mathbf{R}}{\arg \min } \sum_{j=1}^{n}\left\|\mathbf{q}_{j}-\mathbf{R} \mathbf{q}_{j}^{\prime}\right\|^{2}. \label{problemr}
\end{equation} \par
Expanding Eq. (\ref{problemr}) yields:
\begin{equation}
\begin{aligned}
    &\sum_{j=1}^{n}\left\|\mathbf{q}_{j}-\mathbf{R} \mathbf{q}_{j}^{\prime}\right\|^{2} \\ 
=&\sum_{j=1}^{n}\left(\mathbf{q}_{j}^{T}\mathbf{q}_{j}+\mathbf{q}_{j}^{\prime}\mathbf{R}^{T}\mathbf{R}\mathbf{q}_{j}-2\mathbf{q}_{j}^{T}\mathbf{R}\mathbf{q}_{j}^{\prime}\right). \label{eq3.11}
 \end{aligned}
\end{equation} \par
Note that, since $\mathbf{R}\in SO(3)$, $\mathbf{R}^{T}\mathbf{R}= \mathbf{I}$. Hence, only the third term in Eq. (\ref{eq3.11}) involves 
$\mathbf{R}$. As a result, the problem described in Eq. (\ref{eq3.11}) is transformed into:
\begin{equation}
\begin{aligned}
    &\sum_{j=1}^{n}-\mathbf{q}_{j}^{T}\mathbf{R}\mathbf{q}_{j}^{\prime}\\
    =&\sum_{j=1}^{n}-Trace\left(\mathbf{R}\mathbf{q}_{j}^{\prime}\mathbf{q}_{j}^{T}\right)\\
    =&-Trace\left(\mathbf{R}\sum_{j=1}^{n}\mathbf{q}_{j}^{\prime}\mathbf{q}_{j}^{T} \right).
 \end{aligned}
\end{equation} \par
Let
\begin{equation}
\mathbf{H}=\sum_{j=1}^{n}\mathbf{q}_{j}^{\prime}\mathbf{q}_{j}^{T}.
\end{equation} \par
The computation of $\mathbf{R}$ is transformed from Eq. (\ref{problemr}) to:
\begin{equation}
\underset{\mathbf{R}}{\min } \ \ -Trace\left(\mathbf{R}\mathbf{H} \right) \Leftrightarrow \underset{\mathbf{R}}{\max } \ \ Trace\left(\mathbf{R}\mathbf{H} \right) \label{eq3.14}
\end{equation}\par
% To solve this problem, a \textbf{\textit{Lemma}} is introduced as follows. 
\noindent\textbf{\textit{Lemma}} For any positive definite matrix $\mathbf{A}\mathbf{A}^{T}$, and any orthonormal matrix $\mathbf{B}$
\begin{equation}
Trace\left(\mathbf{A}\mathbf{A}^{T} \right) \geq Trace\left(\mathbf{B}\mathbf{A}\mathbf{A}^{T} \right).
\end{equation}\par
The detailed proof of this Lemma is provided in \cite{icp}. Find the SVD of $\mathbf{H}$:
\begin{equation}
\mathbf{H} = \mathbf{U}\mathbf{\Lambda}\mathbf{V}^{T},
\end{equation}
where $\mathbf{U}$ and $\mathbf{V}$ are $3\times3$ orthonormal matrices, and $\mathbf{\Lambda}$ is a $3\times3$
diagonal matrix with non-negative elements.  Define an orthonormal matrix as follows:
\begin{equation}
\mathbf{X} = \mathbf{V}\mathbf{U}^{T}.
\end{equation} \par
Then, the following equation is obtained:
\begin{equation}
\begin{aligned}
\mathbf{X}\mathbf{H} =& \mathbf{V}\mathbf{U}^{T}\mathbf{U}\mathbf{\Lambda}\mathbf{V}^{T} \\
=& \mathbf{V}\mathbf{\Lambda}\mathbf{V}^{T},
\end{aligned}
\end{equation}
which is symmetrical and positive definite. Consequently, based on the
Lemma, for any $3\times3$ orthonormal matrix $\mathbf{B}$,
\begin{equation}
Trace\left(\mathbf{X}\mathbf{H} \right) \geq Trace\left(\mathbf{B}\mathbf{X}\mathbf{H} \right).
\end{equation}\par
Namely, among all $3\times3$ orthonormal matrices, $\mathbf{X}$ maximizes $Trace\left( \mathbf{R}\mathbf{H}\right)$. Therefore,
\begin{equation}
\mathbf{R}^{*} = \mathbf{X} = \mathbf{V}\mathbf{U}^{T}
\end{equation}
is the solution to the problem described in Eq. (\ref{eq3.14}). Finally, $\mathbf{t}^{*}$ can be computed by substituting $\mathbf{R}^{*}$ into the second term of Eq. (\ref{newproblem}). In this research, the points in $\mathcal{{P}_{W}}$ are coplanar but not collinear. According to \cite{icp}, the solution of Eq. (\ref{least}) is unique. This is the reason why the proposed system is free from the multi-solution problem, which is unlike the VFM systems troubled by the rotation ambiguity problem \cite{munoz2019,ippe,yibo}. This is a superiority of the proposed system over the planar VFM systems \cite{ap3,aruco}.
\par
Note that the LiDAR pose is specified by the definitions of $\{G\}$ and $\{L\}$. It is a common method to define $\{G\}$ by predefining the vertices \cite{ap3}. However, the predefined vertices are optional in both the AprilTag system \cite{ap3} and the proposed system. Without the predefined vertices, the definition of  $\{G\}$ is missing. Consequently, the AprilTag system \cite{ap3} will only output the image coordinates of the vertices in the image plane and the proposed system only outputs the 3D features w.r.t. $\{L\}$. In the implementation, the predefinition of vertices is customizable since we want to leave the authority of defining $\{G\}$ to the users as the AprilTag system \cite{ap3} does.

\section{Experimental Validation} \label{3.4}

\subsection{Experimental Setup}
To qualitatively evaluate IFM, two LiDAR models—Livox Mid-40 (solid-state LiDAR) and VLP-16 (mechanical LiDAR)—are employed, and patterns from three popular VFM systems, AprilTag \cite{ap3}, CCTag \cite{cctag}, and ArUco \cite{aruco}, are tested. To further verify the pose estimation accuracy of the IFM system, we compare the pose estimation result given by our system with the ground truth provided by the OptiTrack Motion Capture (MoCap) system (See Fig.~\ref{setup}). The MoCap system is composed of 16 OptiTrack cameras and provides the 6-DOF pose information of the predefined rigid body at 100 Hz. The low-cost solid-state LiDAR, Livox Mid-40, is employed to scan a marker pasted on the wall. The marker (ID = 0), with the size of 17.2 cm$\times$17.2 cm, belongs to the \textit{tag36h11} family of AprilTag 3 \cite{ap3} and is printed on letter-sized paper. The location of the marker's center is at (3.620, 0.00, 0.485) m w.r.t. $\{G\}$. Moreover, the rosbag provided by LiDARTag \cite{lt} is utilized to quantitatively evaluate IFM using the Velodyne ULTRA Puck LiDAR (mechanical LiDAR) with the AprilTag marker.
\par
\begin{figure}[H] 
	\centering
	\includegraphics[width=1.0\linewidth]{figs/chtwo/setup.png}
	\caption{An illustration of the experimental setup.}
	\label{setup}
\end{figure}

\subsection{Qualitative Evaluation}
To qualitatively demonstrate the flexibility of the proposed IFM system, Fig.~\ref{result} is presented. 
Specifically, Fig. \ref{result}(a) corresponds to the scenario shown in Fig.~\ref{flex}(a): a Livox Mid-40 is scanning an AprilTag grid and an ArUco grid. All the markers, 35 AprilTags (family: tag36h11, marker size: 17.2 cm$\times$17.2 cm), and 4 ArUcos (family: 4$\times$4, marker size: 16 cm$\times$16 cm), are detected. Fig. \ref{result}(b) corresponds to Fig.~\ref{flex}(b): a Livox Mid-40 is scanning three CCTag (family: 3 rings, marker radius: 7.2 cm) attached to the wall. All the markers are detected. Fig. \ref{result}(c) corresponds to Fig.~\ref{flex}(c): a VLP-16 is scanning an ArUco (family: original, maker size: 40 cm$\times$40 cm). The marker is detected.
\par
As seen in Fig. \ref{result}, the usage of the IFM system is as convenient as the VFM systems. In particular, the user can place the letter-size markers \cite{ap3,aruco} densely to compose a marker grid, as shown in Fig.~\ref{result}(a), as well as attach some non-square markers \cite{cctag} to the wall freely, as shown in Fig.~\ref{result}(b). In summary, there is no spatial restriction on marker placement. It should be noted that if the angular resolution of the LiDAR is relatively large, large marker size and simpler marker pattern are recommended for the sake of the intensity image quality. For instance, the vertical angular resolution of the VLP-16 is 1.33$^{\circ}$, thus a 40 cm$\times$40 cm original ArUco \cite{aruco} is adopted, as shown in Fig.~\ref{result}(c).
\begin{figure}[H] 
	\centering
	\includegraphics[width=1.0\linewidth]{figs/chtwo/result.png}
	\caption{Marker detection results on the preprocessed intensity images.}
	\label{result}
\end{figure}

\subsection{Quantitative Evaluation}
As a reminder, the experimental setup is shown in Fig. \ref{setup}. Firstly, the orientation of the LiDAR (Livox Mid-40) is fixed, with the roll, pitch, and yaw angles approximately being zeros. Only the distance from the marker's plane to the LiDAR's $O_{L}Y_{L}\mbox{-}O_{L}Z_{L}$ plane is changed. Then, to compare the conventional VFM system with the proposed system, we test AprilTag 3 \cite{ap3} with a camera (Omnivision OV7251) under the same experimental setup. To intuitively demonstrate the changing trend of accuracy w.r.t. the distance, we present the histogram of errors in Fig.~\ref{bar1}. In particular, the error refers to the absolute difference between the measurement and the ground truth. 
A detailed table containing all the measurements and ground truth from Fig.~\ref{bar1} is available in Table \ref{imtab3}, where the vanilla IFM refers to the approach introduced in this section.
\begin{figure}[H] 
	\centering
	\includegraphics[width=1.0\linewidth]{figs/chtwo/bar1.png}
	\caption{Pose estimation accuracy of the IFM system and the AprilTag 3 system at different distances.}
	\label{bar1}
\end{figure}
Three issues are illustrated in Fig.~\ref{bar1}: (1) When more LiDAR scans are utilized, the pose estimation accuracy is slightly boosted. The reason behind it is that more LiDAR scans indicate a higher coverage percentage in the FoV \cite{loam}, which implies better intensity image quality. (2) The IFM system shows comparable accuracy as the VFM system. (3) Unlike the VFM system \cite{wang,olson,ap3}, the pose estimation accuracy does not degrade evidently as the distance increases.
\par
Thereafter, the distance from the marker to the LiDAR is set as 2 meters and only the rotation of the LiDAR is adjusted (See Table~\ref{tab2} where the number of scans = 40). 
\begin{table}[H]
	\caption{Pose estimation accuracy of the IFM system with different Euler angles.}
	\begin{center}
		\begin{tabular}{c|c|c|c|c}
			\hline\hline
			Setup & Term & Ground Truth &\textbf{IFM}&Error\\ \hline
			\multirow{6}{*}{ pitch $\approx -15^{\circ}$} &x (m) &1.629   & 1.661   & -0.032  \\  \cline{2-5} 
			&y (m) &-0.066  & -0.011  & -0.055  \\  \cline{2-5} 
			&z (m) &0.618   & 0.699   & -0.081  \\  \cline{2-5} 
			&roll (deg) &-2.673  & -4.936  & 2.263  \\  \cline{2-5} 
			&pitch (deg) &\textbf{-15.060} & -21.145 & 6.085 \\  \cline{2-5} 
			&yaw (deg) &-0.171  & 0.546   & -0.717 \\  \hline
			\multirow{6}{*}{ pitch $\approx 15^{\circ}$} &x (m) &1.684   & 1.622   & 0.062    \\  \cline{2-5} 
			&y (m) &-0.063  & -0.088  & 0.026  \\  \cline{2-5} 
			&z (m) &0.590   & 0.660   & -0.070  \\  \cline{2-5} 
			&roll (deg) &3.657   & 0.900   & 2.757 \\  \cline{2-5} 
			&pitch (deg) &\textbf{14.849}  & 10.961  & 3.888 \\  \cline{2-5} 
			&yaw (deg) &1.136   & -1.921  & 3.057 \\  \hline
			\multirow{6}{*}{ yaw $\approx -15^{\circ}$} &x (m) &1.638   & 1.612   & 0.027   \\  \cline{2-5} 
			&y (m) &-0.618  & -0.237  & -0.381 \\  \cline{2-5} 
			&z (m) &0.610   & 0.585   & 0.025  \\  \cline{2-5} 
			&roll (deg) &-0.009  & -0.792  & 0.783 \\  \cline{2-5} 
			&pitch (deg) &-0.662  & -2.088  & 1.426\\  \cline{2-5} 
			&yaw (deg) &\textbf{-15.387} & -17.937 & 2.550 \\  \hline
			\multirow{6}{*}{ yaw $\approx 15^{\circ}$} &x (m) &1.627   & 1.632   & -0.005  \\  \cline{2-5} 
			&y (m) &-0.194  & -0.146  & -0.048  \\  \cline{2-5} 
			&z (m) &0.609   & 0.553   & 0.057   \\  \cline{2-5} 
			&roll (deg) &0.490   & -1.994  & 2.484 \\  \cline{2-5} 
			&pitch (deg) &-0.388  & -0.359  & -0.028 \\  \cline{2-5} 
			&yaw (deg) &\textbf{14.924}  & 10.678  & 4.246 \\  \hline\hline
		\end{tabular}
		\label{tab2}
	\end{center}
\end{table}
By comparing the errors in Fig.~\ref{bar1} and Table~\ref{tab2}, it is seen that when the plane of the LiDAR is angled towards the marker plane, the pose estimation performance of the IFM system is as good as that when the LiDAR is perpendicular to the marker.
\par
Although the results in Fig.~\ref{bar1} and Table~\ref{tab2} might be inferior to other high-accuracy solutions, such as total stations and prisms \cite{station}, the merit of the proposed framework lies in its low cost, flexibility, and convenience.
\par
To validate the pose estimation accuracy of our system on the mechanical LiDAR as well as to compare the proposed system with the state-of-the-art LFM system, LiDARTag \cite{lt}, which is also the only existing fiducial marker system for the LiDAR as far as we know, Table~\ref{tab3} is presented. Specifically, we use the rosbag (ccw\_10m.bag) provided by \cite{lt} as the benchmark on which we conduct the comparison. ccw\_10m.bag records the raw data of a 32-Beam Velodyne ULTRA Puck LiDAR scanning a 1.22 m$\times$1.22 m AprilTag (tag16h6), from a distance of 10 meters while the relative angle between the LiDAR plane and the marker is around 45$^{\circ}$. Only the vertices estimation is compared on account that \cite{lt} solely provides the ground truth of the vertices and the vertices estimation is a follow-up process after the pose estimation in the LiDARTag system \cite{lt,lt2}. 
\par
Table~\ref{tab3} illustrates that the IFM system is slightly inferior to LiDARTag in terms of accuracy. This is mainly because the LiDARTag system estimates the pose by finding the transmission that projects points inside the marker cluster into a predefined template (bounding box) at the origin of LiDAR \cite{lt,lt2}, while our system adopts the vertices to compute the pose directly just as the AprilTag \cite{ap3} and ArUco \cite{aruco} systems do. \par
\begin{table}[H]
	\caption{Comparison of the IFM system and LiDARTag.}
	\begin{center}
		\begin{tabular}{c|c|c|c|c|c}
			\hline\hline
				System & Vertex & x (m) &y (m)& z (m) & Error (m)\\ \hline
				\multirow{4}{*}{Ground Truth } &1 &9.739 & 0.758 & -0.161 & --  \\  \cline{2-6} 
				&2 &9.940 & 0.072 & -0.732 & -- \\  \cline{2-6} 
				&3 &10.272 & -0.414 & -0.032 & -- \\  \cline{2-6} 
				\cite{lt} &4 &10.072 & 0.271 & 0.539 & --\\  \hline
				\multirow{4}{*}{LiDARTag } &1 & 9.736 & 0.762 & -0.174 & 0.015\\  \cline{2-6} 
				&2 &9.944 & 0.066 & -0.730 & 0.009 \\  \cline{2-6} 
				&3 &10.271 & -0.405 & -0.016 & 0.019 \\  \cline{2-6} 
				\cite{lt} &4 & 10.063 & 0.291 & 0.539 & 0.022\\  \hline
				\multirow{4}{*}{IFM } &1 &9.728 & 0.766 & -0.184 & 0.013\\  \cline{2-6} 
				&2 & 9.963 & 0.052 & -0.705 & 0.033 \\  \cline{2-6} 
				&3 &10.307 & -0.432 & -0.016 & 0.044 \\  \cline{2-6} 
				&4 &10.045 & 0.263 & 0.564 & 0.041 \\  \hline\hline
		\end{tabular}
		\label{tab3}
	\end{center}
\end{table}
Our system utilizes fewer correspondences compared to \cite{lt,lt2}, and thus the ranging noise on a single point could have more effects on the pose estimation of our system. It should be noted that the intention of proposing the LFM system is to improve the real-world applications, such as SLAM \cite{munoz2019}, multi-sensor calibration \cite{lt2}, and AR \cite{ar}. As shown in Fig. \ref{bar1}, the proposed system outperforms the AprilTag system \cite{ap3}, which is already widely used in the above-mentioned applications. Moreover, the VFM systems \cite{ap3,aruco} use only four vertices of a marker (or a limited number of points if the marker is non-square). Note that it is definitely feasible to acquire more feature points from the inner coding area of the marker, however, this will increase the complexity of the VFM systems. Hence, following a simple but effective design concept, we also use only four vertices of a marker in our proposed system. If higher accuracy is needed, it is feasible to use the proposed marker detection information to find the point clustering of the marker in the point cloud and then input the point clustering into the pose estimation block of LiDARTag. However, as mentioned previously, the superiority of the IFM system is the flexibility and extensibility. For example, marker detection is infeasible for the LiDARTag system \cite{lt} in the scenarios shown in Fig.~\ref{flex} and Fig.~\ref{setup} since the marker placement does not satisfy the requirement of LiDARTag, and in addition, the current version of LiDARTag does not support any non-square marker, such as CCTag \cite{cctag}.

\subsection{Computational Time Analysis}
A computational time analysis is conducted on a desktop with Intel Xeon W-1290P Central Processing Unit (CPU). The LiDARTag \cite{lt} runs at around 100 Hz on it. The time consumption of the marker detection of our system, which includes intensity image generation, preprocessing, 2D features detection, and computation of 3D features, mainly depends on the size of the intensity image and the embedded VFM system. Suppose that the embedded VFM system is AprilTag 3 \cite{ap3}, the marker detection takes around 7 ms (approx 143 Hz) in the case shown in Fig.~\ref{result}(c) where $\Theta_{a}=0.3^{\circ}$, $\Theta_{i}=1.33^{\circ}$, and intensity image size = $1201\times27$. For the case shown in  Fig.~\ref{result}(a), the marker detection takes around 25 ms (approx 40 Hz), where intensity  $\Theta_{a}=\Theta_{i}=0.05^{\circ}$ and image size = $771\times591$. The time consumption of the following pose estimation process is around 1.8 $\mu$\textit{s} as the closed-form solution can be obtained directly through SVD \cite{icp} (Refer to Section~\ref{3.3}).
\subsection{Limitations Analysis} \label{twofourfour}
There are some limitations to the application of LFM systems. First, the distance from the object (marker) to the LiDAR must exceed the minimum detectable range. This limitation is caused by the hardware attributes and it affects all the LiDAR applications not only the LFM systems. Secondly, to utilize the solid-state LiDAR, it is required to wait for the growth of the scanned area inside the FoV to obtain a relatively dense point cloud. Again this is a limitation caused by the hardware attributes. In contrast, the mechanical LiDAR only requires one LiDAR scan to work, however, a larger and simpler marker is recommended if the angular resolution is large.  Finally, the false positives are occasionally found in the experiments while the issue of wrong ID detection (not exactly the same as the false positive but similar) is also reported in LiDARTag \cite{lt}. For the proposed system, the false positive rate is mainly determined by the embedded VFM system, thus novel VFM systems with lower false positive rates, such as \cite{ap3,aruco}, are preferred. Moreover, due to the adoption of 3D-to-2D spherical projection, the vanilla IFM exhibits two limitations:
\par
(1) IFM can only detect fiducials in a single-view point cloud and is not applicable to a 3D LiDAR map, as spherical projection only applies to a single-view point cloud \cite{rangenet}. Take the 3D map shown in Fig. \ref{occ} as an example. Unless we manually adjust the perspective to view the map as shown in Fig. \ref{occ}(a), the four tags are not simultaneously visible due to occlusion, as seen in Fig. \ref{occ}(b).
\par
(2) As the distance between the tag and the LiDAR increases, the tag's projection size decreases until it becomes too small to be detected.
\begin{figure}[t] 
	\centering
\includegraphics[width=1.0\linewidth]{figs/chthree/occ.png}
	\caption{An illustration of the limitation of spherical projection for 3D maps. This map is constructed using Traj LO \cite{traj}.}
	\label{occ}
\end{figure} \par
The next section addresses these two limitations caused by the 3D-to-2D spherical projection. Moreover, the vanilla IFM employs a constant threshold to process intensity images (See Fig. \ref{dis}(c)), which is sufficient for static scenes with trivial or no viewpoint changes. However, the marker detection of the vanilla IFM is not robust when the viewpoints change in a wide range, which hinders its application to in-the-wild multiview point cloud registration. An adaptive threshold marker detection method is developed to address this problem in Section \ref{5.2}.

