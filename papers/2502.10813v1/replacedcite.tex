\section{Related work}
\label{sec:Related work}
Traditionally, Self-reports, observational checklists and rating scales, and automatic recognition are the three categories into which the student involvement recognition techniques are divided ____. The former two categories do not suit the online environment as they heavily rely on students or external observers and have a coarser temporal resolution ____. On the contrary, automatic student engagement recognition using sensory data does not require direct and continuous input from students. Therefore, automated engagement recognition becomes the most relevant 
 subject and natural choice for researchers. Student engagement analysis is studied based on their actions in online learning  (from log data) ____. Although, these techniques depend on limited evaluation metrics and develop biases when used in the wild. 
 \par Automated student engagement recognition is also explored using various physiological sensor signals such as EEG, blood pressure, heart rate, and galvanic skin response ____. However, the physiological sensors are obtrusive and costly, which makes it difficult to use for student engagement recognition. On the other hand, computer vision-based methods due to their unobtrusive nature are the most popular for student engagement recognition. In particular, face-based methods have been explored largely as the face depicts an emotional state that is intuitively related to engagement. Among the preliminary studies Grafsgaard et al. ____ utilized Computer Expression Recognition Toolbox (CERT) for the estimation of more frequent Action Units (AUs) based on the AUs activation. Bosch et al. ____ estimated students' affective states based on AUs, orientation, and position features of the face, whereas, Saneiro et al. ____ exploited 2D facial points, 3D head poses, and movement along with AUs for student engagement. Although the AU-based methods can estimate the emotional state of students, they are less robust in the wild environment, thereby making it tedious to predict the detailed engagement of students. In another aspect, appearance-based features have been extracted and fed to the machine learning classifiers for feature learning and classification. Whitehill et al. ____ first annotated samples for four engagement levels and then trained several classifiers using different facial features. Kamath et al. ____ used SVM based on facial Histogram of Oriented Gradient (HOG) features to enhance the prediction performance. Monkaresi et al. ____ used an ensemble of features derived from AUs, Local Binary Pattern on Three Orthogonal Planes (LBP-TOP), and heart rate to train the automatic student engagement detector. Although the algorithm generalizes well with hand-crafted features. however, designing hand-crafted features for different environmental conditions is cumbersome.

Deep learning's overwhelming performance in computer vision tasks has motivated the research community to use it for automatic student engagement recognition. Among the initial studies, Nezami et al. ____ utilized a multilayer CNN trained on a custom dataset for student engagement recognition. In another study, Schulc et al. ____ proposed a CNN-LSTM-based model to predict if a person is attentive or non-attentive. The aforementioned studies utilized a small private dataset to model a deep neural network and thereby making it difficult to make a comparison between their performances and intrinsic challenges. To overcome such a challenge and set a benchmark for comparative evaluation among the state-of-the-arts for student engagement detection, Gupta et al. ____  proposed a database namely Dataset for Affective States in E-Environment (DAiSEE) with five baselines for engagement estimation. Among the first, Zhang et al. ____ utilized DAiSEE to perform binary classification and achieved the highest accuracy on student engagement classification based on Inflated 3D convolution (I3D). Similarly, Huang et al. ____ proposed a Deep Engagement Recognition Network (DERN) trained on DAiSEE for engagement classification. Their study achieved an accuracy of  60\% on four class engagement predictions. 
Another dataset was proposed by Dhall et al. ____ called EmotiW  is widely used in the field of affective computing for student engagement prediction. Several studies have been carried out utilizing EmotiW dataset to estimate student engagement including the study of Niu et al. ____. Niu et al. proposed a GRU-based model trained on novel Gaze-AU-Pose (GAP) features. A similar study was conducted by Yang et al. ____ which used an LSTM model as a multi-model regressor  and outperformed all the previous benchmarks on the EmotiW dataset. To improve the detection performance,  Lio et al. ____ and Mehta et al. ____ incorporated attention modules in addition to employing deep learning models in order to focus on the most relevant features responsible for student engagement. Whereas the former employed Se-ResNet-50 and LSTM for spatial and temporal feature learning, the latter utilized 3D DenseNet to model engagement using DAiSEE and EmotiW datasets. Although most of these methods have shown satisfactory performance on an individual dataset, their robustness can not be proved. Furthermore, the student engagement video datasets are very few in number and small in scale.
% Also, the EmotiW dataset is not accessible at the time of writing this manuscript.

Most of the existing methods use CNNs for spatial feature extraction which suffer due to local receptive field and image-specific inductive bias ____.
 On the contrary, transformers learn global dependencies by employing a module called self-attention. The self-attention induces the importance of other frames into the one being processed. Furthermore, it is hard to estimate beforehand at what frequency and duration the affective states will appear in the learning context ____. Considering the limitations of CNNs and the innate challenges intrinsic to student engagement estimation. We propose EngageFormer, a pure multi-view transformer ____ based architecture for spatial and temporal modeling to efficiently estimate student engagement from the videos. The EngageFormer overcomes the problem of modeling the affect states appearing at different frequencies and duration by capturing the slowly and rapidly varying features using three views of the proposed model.