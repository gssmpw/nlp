[
  {
    "index": 0,
    "papers": [
      {
        "key": "whitehill2014faces",
        "author": "Whitehill, Jacob and Serpell, Zewelanji and Lin, Yi-Ching and Foster, Aysha and Movellan, Javier R",
        "title": "The faces of engagement: Automatic recognition of student engagementfrom facial expressions"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "d2006predicting",
        "author": "D'Mello, Sidney K and Craig, Scotty D and Sullins, Jeremiah and Graesser, Arthur C",
        "title": "Predicting affective states expressed through an emote-aloud procedure from AutoTutor's mixed-initiative dialogue"
      },
      {
        "key": "o2010development",
        "author": "O'Brien, Heather L and Toms, Elaine G",
        "title": "The development and evaluation of a survey to measure user engagement"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "5518758",
        "author": "Cocea, Mihaela and Weibelzahl, Stephan",
        "title": "Disengagement Detection in Online Learning: Validation Studies and Perspectives"
      },
      {
        "key": "aluja2019measuring",
        "author": "Aluja-Banet, Tom{\\`a}s and Sancho, Maria-Ribera and Vukic, Ivan",
        "title": "Measuring motivation from the virtual learning environment in secondary education"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "monkaresi2016automated",
        "author": "Monkaresi, Hamed and Bosch, Nigel and Calvo, Rafael A and D'Mello, Sidney K",
        "title": "Automated detection of engagement using video-based estimation of facial expressions and heart rate"
      },
      {
        "key": "fairclough2006prediction",
        "author": "Fairclough, Stephen H and Venables, Louise",
        "title": "Prediction of subjective states from psychophysiology: A multivariate approach"
      },
      {
        "key": "khedher2019tracking",
        "author": "Khedher, Asma Ben and Jraidi, Im{\\`e}ne and Frasson, Claude and others",
        "title": "Tracking students\u2019 mental engagement using EEG signals during an interaction with a virtual learning environment"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "6681424",
        "author": "Grafsgaard, Joseph F. and Wiggins, Joseph B. and Boyer, Kristy Elizabeth and Wiebe, Eric N. and Lester, James C.",
        "title": "Automatically Recognizing Facial Indicators of Frustration: A Learning-centric Analysis"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "bosch2015automatic",
        "author": "Bosch, Nigel and D'Mello, Sidney and Baker, Ryan and Ocumpaugh, Jaclyn and Shute, Valerie and Ventura, Matthew and Wang, Lubin and Zhao, Weinan",
        "title": "Automatic detection of learning-centered affective states in the wild"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "saneiro2014towards",
        "author": "Saneiro, Mar and Santos, Olga C and Salmeron-Majadas, Sergio and Boticario, Jesus G",
        "title": "Towards emotion detection in educational scenarios from facial expressions and body movements through multimodal approaches"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "whitehill2014faces",
        "author": "Whitehill, Jacob and Serpell, Zewelanji and Lin, Yi-Ching and Foster, Aysha and Movellan, Javier R",
        "title": "The faces of engagement: Automatic recognition of student engagementfrom facial expressions"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kamath2016crowdsourced",
        "author": "Kamath, Aditya and Biswas, Aradhya and Balasubramanian, Vineeth",
        "title": "A crowdsourced approach to student engagement recognition in e-learning environments"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "monkaresi2016automated",
        "author": "Monkaresi, Hamed and Bosch, Nigel and Calvo, Rafael A and D'Mello, Sidney K",
        "title": "Automated detection of engagement using video-based estimation of facial expressions and heart rate"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "mohamad2020automatic",
        "author": "Mohamad Nezami, Omid and Dras, Mark and Hamey, Len and Richards, Deborah and Wan, Stephen and Paris, C{\\'e}cile",
        "title": "Automatic recognition of student engagement using deep learning and facial expression"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "schulc2019automatic",
        "author": "Schulc, Attila and Cohn, Jeffrey F and Shen, Jie and Pantic, Maja",
        "title": "Automatic measurement of visual attention to video content using deep learning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "gupta2016daisee",
        "author": "Gupta, Abhay and D'Cunha, Arjun and Awasthi, Kamal and Balasubramanian, Vineeth",
        "title": "Daisee: Towards user engagement recognition in the wild"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhang2019novel",
        "author": "Zhang, Hao and Xiao, Xiaofan and Huang, Tao and Liu, Sanya and Xia, Yu and Li, Jia",
        "title": "An novel end-to-end network for automatic student engagement recognition"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "huang2019fine",
        "author": "Huang, Tao and Mei, Yunshan and Zhang, Hao and Liu, Sanya and Yang, Huali",
        "title": "Fine-grained engagement recognition in online learning environment"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "dhall2019emotiw",
        "author": "Dhall, Abhinav",
        "title": "Emotiw 2019: Automatic emotion, engagement and cohesion prediction tasks"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "niu2018automatic",
        "author": "Niu, Xuesong and Han, Hu and Zeng, Jiabei and Sun, Xuran and Shan, Shiguang and Huang, Yan and Yang, Songfan and Chen, Xilin",
        "title": "Automatic engagement prediction with GAP feature"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "yang2018deep",
        "author": "Yang, Jianfei and Wang, Kai and Peng, Xiaojiang and Qiao, Yu",
        "title": "Deep recurrent multi-instance learning with spatio-temporal features for engagement intensity prediction"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "liao2021deep",
        "author": "Liao, Jiacheng and Liang, Yan and Pan, Jiahui",
        "title": "Deep facial spatiotemporal network for engagement prediction in online learning"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "mehta2022three",
        "author": "Mehta, Naval Kishore and Prasad, Shyam Sunder and Saurav, Sumeet and Saini, Ravi and Singh, Sanjay",
        "title": "Three-dimensional DenseNet self-attention neural network for automatic detection of student\u2019s engagement"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "dosovitskiy2020image",
        "author": "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others",
        "title": "An image is worth 16x16 words: Transformers for image recognition at scale"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "dewan2019engagement",
        "author": "Dewan, M and Murshed, Mahbub and Lin, Fuhua",
        "title": "Engagement detection in online learning: a review"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "yan2022multiview",
        "author": "Yan, Shen and Xiong, Xuehan and Arnab, Anurag and Lu, Zhichao and Zhang, Mi and Sun, Chen and Schmid, Cordelia",
        "title": "Multiview transformers for video recognition"
      }
    ]
  }
]